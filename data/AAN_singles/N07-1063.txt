Proceedings of NAACL HLT 2007, pages 500?507,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsAn Efficient Two-Pass Approach to Synchronous-CFG Driven Statistical MTAshish Venugopal and Andreas Zollmann and Stephan VogelSchool of Computer Science, Carnegie Mellon University, PittsburghinterACT Lab, University of Karlsruhe{ashishv,zollmann,vogel+}@cs.cmu.eduAbstractWe present an efficient, novel two-passapproach to mitigate the computationalimpact resulting from online intersectionof an n-gram language model (LM) anda probabilistic synchronous context-freegrammar (PSCFG) for statistical machinetranslation.
In first pass CYK-style decod-ing, we consider first-best chart item ap-proximations, generating a hypergraph ofsentence spanning target language deriva-tions.
In the second stage, we instantiatespecific alternative derivations from thishypergraph, using the LM to drive thissearch process, recovering from search er-rors made in the first pass.
Model searcherrors in our approach are comparable tothose made by the state-of-the-art ?CubePruning?
approach in (Chiang, 2007) un-der comparable pruning conditions evalu-ated on both hierarchical and syntax-basedgrammars.1 IntroductionSyntax-driven (Galley et al, 2006) and hierarchi-cal translation models (Chiang, 2005) take advan-tage of probabilistic synchronous context free gram-mars (PSCFGs) to represent structured, lexical re-ordering constraints during the decoding process.These models extend the domain of locality (overphrase-based models) during decoding, represent-ing a significantly larger search space of possibletranslation derivations.
While PSCFG models areoften induced with the goal of producing grammati-cally correct target translations as an implicit syntax-structured language model, we acknowledge thevalue of n-gram language models (LM) in phrase-based approaches.Integrating n-gram LMs into PSCFGs based de-coding can be viewed as online intersection of thePSCFG grammar with the finite state machine rep-resented by the n-gram LM, dramatically increasingthe effective number of nonterminals in the decodinggrammar, rendering the decoding process essentiallyinfeasible without severe, beam-based lossy prun-ing.
The alternative, simply decoding without then-gram LM and rescoring N-best alternative transla-tions, results in substantially more search errors, asshown in (Zollmann and Venugopal, 2006).Our two-pass approach involves fast, approximatesynchronous parsing in a first stage, followed by asecond, detailed exploration through the resultinghypergraph of sentence spanning derivations, usingthe n-gram LM to drive that search.
This achievessearch errors comparable to a strong ?Cube Pruning?
(Chiang, 2007), single-pass baseline.
The first passcorresponds to a severe parameterization of CubePruning considering only the first-best (LM inte-grated) chart item in each cell while maintaining un-explored alternatives for second-pass consideration.Our second stage allows the integration of long dis-tance and flexible history n-gram LMs to drive thesearch process, rather than simply using such mod-els for hypothesis rescoring.We begin by discussing the PSCFG model forstatistical machine translation, motivating the need500for effective n-gram LM integration during decod-ing.
We then present our two-pass approach anddiscuss Cube Pruning as a state-of-the-art baseline.We present results in the form of search error analy-sis and translation quality as measured by the BLEUscore (Papineni et al, 2002) on the IWSLT 06 texttranslation task (Eck and Hori, 2005)1, comparingCube Pruning with our two-pass approach.2 Synchronous Parsing for SMTProbabilistic Synchronous Context Free Grammar(PSCFG) approaches to statistical machine transla-tion use a source terminal set (source vocabulary)TS , a target terminal set (target vocabulary) TT anda shared nonterminal set N and induce rules of theformX ?
?
?, ?,?, w?where (i) X ?
N is a nonterminal, (ii) ?
?
(N ?TS)?
is a sequence of nonterminals and source ter-minals, (iii) ?
?
(N ?TT )?
is a sequence of nonter-minals and target terminals, (iv) the number cnt(?
)of nonterminal occurrences in ?
is equal to the num-ber cnt(?)
of nonterminal occurrences in ?, (v)?
: {1, .
.
.
, cnt(?)}
?
{1, .
.
.
, cnt(?)}
is a one-to-one mapping from nonterminal occurrences in ?
tononterminal occurrences in ?, and (vi) w ?
[0,?
)is a non-negative real-valued weight assigned to therule.
We will assume ?
to be implicitly defined byindexing the NT occurrences in ?
from left to rightstarting with 1, and by indexing the NT occurrencesin ?
by the indices of their corresponding counter-parts in ?.
Syntax-oriented PSCFG approaches typ-ically ignore source structure, instead focussing ongenerating syntactically well formed target deriva-tions.
(Galley et al, 2006) use syntactic constituentsfor the PSCFG nonterminal set and (Zollmann andVenugopal, 2006) take advantage of CCG (Steed-man, 1999) categories, while (Chiang, 2005) usesa single generic nonterminal.
PSCFG derivationsfunction analogously to CFG derivations.
Givena source sentence f , the translation task under aPSCFG grammar can be expressed as1While IWSLT represents a limited resource translation task(120K sentences of training data for Chinese-English), the prob-lem of efficient n-gram LM integration is still critically impor-tant to efficient decoding, and our contributions can be expectedto have an even more significant impact when decoding withgrammars induced from larger corpora.e?
= argmax{e | ?D.
src(D)=f,tgt(D)=e}P (D)where tgt(D) refers to the target terminal symbolsgenerated by the derivation D and src(D) refers tothe source terminal symbols spanned by D. Thescore (also laxly called probability, since we neverneed to compute the partition function) of a deriva-tion D under a log-linear model, referring to therules r used in D, is:P (D) =1ZPLM (tgt(D))?LM ?
?i?r?D?i(r)?iwhere ?i refers to features defined on each rule,and PLM is a g-gram LM probability applied to thetarget terminal symbols generated by the derivationD.
Introducing the LM feature defines dependen-cies across adjacent rules used in each derivation,and requires modifications to the decoding strategy.Viewing the LM as a finite-state machine, the de-coding process involves performing an intersectionbetween the PSCFG grammar and the g-gram LM(Bar-Hillel et al, 1964).
We present our work underthe construction in (Wu, 1996), following notationfrom (Chiang, 2007), extending the formal descrip-tion to reflect grammars with an arbitrary number ofnonterminals in each rule.2.1 Decoding StrategiesIn Figure 1, we reproduce the decoding algorithmfrom (Chiang, 2007) that applies a PSCFG totranslate a source sentence in the same notation (asa deductive proof system (Shieber et al, 1995)),generalized to handle more than two non-terminalpairs.
Chart items [X, i, j, e] : w span j ?
i wordsin the source sentence f1 ?
?
?
fn, starting at positioni + 1, and have weight w (equivalent to P (D)), ande ?
(TT ?
{?})?
is a sequence of target terminals,with possible elided parts, marked by ?.
Functionsp, q whose domain is TT ?
{?}
are defined in(Chiang, 2007) and are repeated here for clarity.p(a1 ?
?
?
am) =Yg?i?m,?/?ai?g+1??
?ai?1PLM (ai|ai?g+1 ?
?
?
ai?1)q(a1 ?
?
?
am) =(a1 ?
?
?
ag?1 ?
am?g+2 ?
?
?
am if m ?
ga1 ?
?
?
am elseThe function q elides elements from a target lan-guage terminal sequence, leaving the leftmost andrightmost g ?
1 words, replacing the elided words501X ?
?
?, ??
: w(X ?
?
?, ?, w?)
?
GX ?
?f ji+1, ??
: w[X, i, j; q(?)]
: wp(?
)Z ?
?f i1i+1(X1)1fi2j1+1?
?
?
(Xm?1)m?1fimjm?1+1(Xm)mfjjm+1, ??
: w?X1, i1, j1; e1?
: w1 ?
?
?
[Xm, im, jm; em] : wm[Z, i, j, q(??)]
: ww1 ?
?
?wmp(??)
(where ??
= ?
[e1/(X1)1, .
.
.
, em/(Xm)m])Goal item:?S, 0, n; ?s?g?1 ?
?\s?g?1?Figure 1.
CYK parsing with integrated g-gram LM.
The inference rules are explored in ascending order of j ?
i.
Here?
[e/Yi] is the string ?
where the NT occurrence Yi is replaced by e. Functions q and p are explained in the text.with a single ?
symbol.
The function p returns g-gram LM probabilities for target terminal sequences,where the ?
delineates context boundaries, prevent-ing the calculation from spanning this boundary.
Weadd a distinguished start nonterminal S to gener-ate sentences spanning target translations beginningwith g ?
1 ?s?
symbols and ending with g ?
1 ?\s?symbols.
This can e.g.
be achieved by adding foreach nonterminal X a PSCFG ruleS ?
?X, ?s?g?1X?\s?g?1, 1?We are only searching for the derivation of highestprobability, so we can discard identical chart itemsthat have lower weight.
Since chart items are de-fined by their left-hand side nonterminal production,span, and the LM contexts e, we can safely discardthese identical items since q has retained all contextthat could possibly impact the LM calculation.
Thisprocess is commonly referred to as item recombina-tion.
Backpointers to antecedent cells are typicallyretained to allow N -Best extraction using an algo-rithm such as (Huang and Chiang, 2005).The impact of g-gram LM intersection during de-coding is apparent in the final deduction step.
Gen-erating the set of consequent Z chart items involvescombining m previously produced chart cells.
Sinceeach of these chart cells with given source span [i, j]is identified by nonterminal symbol X and LM con-text e, we have at worst |N | ?
|TT |2(g?1) such chartcells in a span.
The runtime of this algorithm is thusO(n3[|N ||TT |2(g?1)]K)where K is the maximum number of NT pairs perrule and n the source sentence length.
Without se-vere pruning, this runtime is prohibitive for even thesmallest induced grammars.
Traditional pruning ap-proaches that limit the number of consequents afterthey are produced are not effective since they first re-quire that the cost of each consequent be computed(which requires calls to the g-gram LM).Restrictions to the grammar afford alternative de-coding strategies to reduce the runtime cost of syn-chronous parsing.
(Zhang et al, 2006) ?binarize?grammars into CNF normal form, while (Watan-abe et al, 2006) allow only Griebach-Normal formgrammars.
(Wellington et al, 2006) argue that theserestrictions reduce our ability to model translationequivalence effectively.
We take an agnostic viewon the issue; directly addressing the question of effi-cient LM intersection rather than grammar construc-tion.3 Two-pass LM IntersectionWe propose a two-pass solution to the problem ofonline g-gram LM intersection.
A naive two-passapproach would simply ignore the LM interactionsduring parsing, extract a set of N derivations fromthe sentence spanning hypergraph and rescore thesederivations with the g-gram LM.
In practice, this ap-proach performs poorly (Chiang, 2007; Zollmannand Venugopal, 2006).
While parsing time is dra-matically reduced (and N -best extraction time isnegligible), N is typically significantly less than thecomplete number of possible derivations and sub-stantial search errors remain.
We propose an ap-proach that builds upon the concept of a second passbut uses the g-gram LM to search for alternative,better translations.5023.1 First pass: parsingWe begin by relaxing the criterion that determineswhen two chart items are equivalent during parsing.We consider two chart items to be equivalent (andtherefore candidates for recombination) if they havematching left-hand side nonterminals, and span.
Weno longer require them to have the same LM con-text e. We do however propagate the e, w for thechart item with highest score, causing the algorithmto still compute LM probabilities during parsing.
Asa point of notation, we refer to such a chart item byannotating its e, w as e1, w1, and we refer to themas approximate items (since they have made a first-best approximation for the purposes of LM calcula-tion).
These approximate items labeled with e1, w1are used in consequent parse calculations.The parsing algorithm under this approximationstays unchanged, but parsing time is dramatically re-duced.
The runtime complexity of this algorithm isnow O(n3|N |K)at the cost of significant searcherrors (since we ignored most LM contexts that weencountered).This relaxation is different from approaches thatdo not use the LM during parsing.
The sentencespanning item does have LM probabilities associ-ated with it (but potentially valuable chart itemswere not considered during parsing).
Like in tra-ditional parsing, we retain the recombined items inthe cell to allow us to explore new derivations in asecond stage.3.2 Second pass: hypergraph searchThe goal item of the parsing step represents a sen-tence spanning hypergraph of alternative deriva-tions.
Exploring alternatives from this hyper-graph and updating LM probabilities can now revealderivations with higher scores that were not consid-ered in the first pass.
Exploring the whole space ofalternative derivations in this hypergraph is clearlyinfeasible.
We propose a g-gram LM driven heuris-tic search ?H.Search?
of this space that allows the g-gram LM to decide which section of the hypergraphto explore.
By construction, traversing a particularderivation item from the parse chart in target-sideleft-to-right, depth-first order yields the correctly or-dered sequence of target terminals that is the transla-tion represented by this item.
Now consider a partialtraversal of the item in that order, where we gener-ate only the first M target terminals, leaving the restof the item in its original backpointer form.
We in-formally define our second pass algorithm based onthese partial derivation items.Consider a simple example, where we have parseda source sentence, and arrived at a sentence spanningitem obtained from a rule with the following targetside:NP2 VP3 PP1and that the item?s best-score estimate is w. A par-tial traversal of this item would replace NP2 withone of the translations available in the chart cell un-derlying NP2 (called ?unwinding?
), and recalculatethe weights associated with this item, taking intoaccount the alternative target terminals.
Assuming?the nice man?
was the target side of the best scoringitem in NP2, the respective traversal would main-tain the same weight.
An alternative item at NP2might yield ?a nice man?.
This partial traversal rep-resents a possible item that we did not consider dur-ing parsing, and recalculating LM probabilities forthis new item (based on approximate items VP3 andPP1) yields weight w2:the nice man VP3 PP1 : w1 = wa nice man VP3 PP1 : w2Alternative derivation items that obtain a higherscore than the best-score estimates represent recov-ery from search errors.
Our algorithm is based onthe premise that these items should be traversed fur-ther, with the LM continuing to score newly gener-ated target words.
These partially traversed itemsare placed on an agenda (sorted by score).
At eachstep of the second pass search, we select those itemsfrom the agenda that are within a search beam of Zfrom the best item, and perform the unwind opera-tion on each of these items.
Since we unwind partialitems from left-to-right the g-gram LM is able to in-fluence the search through the space of alternativederivations.Applying the g-gram LM on partial items withleading only-terminal symbols allows the integra-tion of high- / flexible-order LMs during this sec-ond stage process, and has the advantage of explor-ing only those alternatives that participate in sen-tence spanning, high scoring (considering both LMand translation model scores) derivations.
While503we do not evaluate such models here, we note thatH.Search was developed specifically for the integra-tion of such models during search.We further note that partial items that have gen-erated translations that differ only in the word po-sitions up to g ?
1 words before the first nonter-minal site can be recombined (for the same rea-sons as during LM intersected parsing).
For exam-ple, when considering a 3-gram LM, the two par-tial items above can be recombined into one equiv-alence class, since partial item LM costs resultingfrom these items would only depend on ?nice man?,but not on ?a?
vs. ?the?.
Even if two partial itemsare candidates for recombination due to their termi-nal words, they must also have identical backpoint-ers (representing a set of approximate parse deci-sions for the rest of the sentence, in our exampleVP3PP1 ).
Items that are filed into existing equiv-alence classes with a lower score are not put ontothe agenda, while those that are better, or have cre-ated new equivalence classes are scheduled onto theagenda.
For each newly created partial derivation,we also add a backpointer to the ?parent?
partialderivation that was unwound to create it.This equivalence classing operation transformsthe original left-hand side NT based hypergraph intoan (ordinary) graph of partial items.
Each equiva-lence class is a node in this new graph, and recom-bined items are the edges.
Thus, N -best extractioncan now be performed on this graph.
We use theextraction method from (Huang and Chiang, 2005).The expensive portion of our algorithm lies in theunwinding step, in which we generate a new par-tial item for each alternative at the non-terminal sitethat we are ?unwinding?.
For each new partial item,we factor out LM estimates and rule weights thatwere used to score the parent item, and factor inthe LM probabilities and rule weights of the alter-native choice that we are considering.
In addition,we must also update the new item?s LM estimatesfor the remaining non-terminal and terminal sym-bols that depend on this new left context of termi-nals.
Fortunately, the number of LM calculationsper new item is constant, i.e., does not dependent onthe length of the partial derivation, or how unwoundit is.
Only (g ?
1) ?
2 LM probabilities have to bere-evaluated per partial item.
We now define this?unwind-recombine?
algorithm formally.3.2.1 The unwind-recombine algorithmGoing back to the first-pass parsing algorithm(Figure 1), remember that each application of agrammar rule containing nonterminals correspondsto an application of the third inference rule of thealgorithm.
We can assign chart items C created bythe third inference rule a back-pointer (BP) targetside as follows: When applying the third inferencerule, each nonterminal occurrence (Xk)k in the cor-responding Z ?
?
?, ??
grammar rule correspondsto a chart cell [Xk, ik, jk] used as an antecedent forthe inference rule.
We assign a BP target side for Cby replacing NT occurrences in ?
(from the rule thatcreated C) with backpointers to their correspondingantecedent chart cells.
Further we define the distin-guished backpointer PS as the pointer to the goalcell [S, 0, n] : w?.The deductive program for our second-pass al-gorithm is presented in Figure 2.
It makes use oftwo kind of items.
The first, {P ?
?
; e1} : w,links a backpointer P to a BP target side, storingcurrent-item vs. best-item correction terms in formof an LM context e1 and a relative score w. Thesecond item form [[e;?]]
in this algorithm corre-sponds to partial left-to-right traversal states as de-scribed above, where e is the LM context of the tra-versed and unwound translation part, and ?
the partthat is yet to be traversed and whose backpointersare still to be unwound.
The first deduction rulepresents the logical axioms, creating BP items foreach backpointer used in a NT inference rule appli-cation during the first-pass parsing step.
The sec-ond deduction rule represents the unwinding stepas discussed in the example above.
These deduc-tions govern a search for derivations through the hy-pergraph that is driven by updates of rule weightsand LM probabilities when unwinding non-first-besthypotheses.
The functions p and q are as definedin Section 2, except that the domain of q is ex-tended to BP target sides by first replacing eachback-pointer with its corresponding chart cell?s LMcontext and then applying the original q on the re-sulting sequence of target-terminals and ?
symbols.2Note that w?, which was computed by the first de-duction rule, adjusts the current hypothesis?
weight2Note also that p(?s?g?1 ?
?\s?g?1) = 1 as the product overthe empty set is one.504{P ?
?
; e1} : w?/w(P back-points to 1st-pass cell [X, i, j; e1] : w; ?
and w?
are BP-target-side and weight of one of that cell?s items)[[e;P?end]] : w?P ?
?lex?mid; e1?
: w?
[[q(e?lex);?mid?end]] : ww?p[eq(?lex?mid)]p[q(e?lex?mid)q(?end)]/p(ee1)/p[q(ee1)q(?end)]?
?lex contains no BPs and?mid = P ???
or ?mid = ?
?Figure 2.
Left-to-right LM driven hypergraph search of the sentence spanning hypergraph; ?
denotes the empty word.Non-logical (Start) axiom: [[?
;PS ]] : w?
; Goal item: [[?s?g?1 ?
?\s?g?1; ?]]
: wthat is based on the first-best instance of P to theactually chosen instance?s weight.
Further, the ra-tio p(eq(?lex?mid))/p(ee1) adjusts the LM prob-abilities of P ?s instantiation given its left context,and p[q(e?lex?mid)q(?end)]/p[q(ee1)q(?end)] ad-justs the LM probabilities of the g ?
1 words rightof P .4 Alternative ApproachesWe evaluate our two pass hypergraph search?H.Search?
against the strong single pass CubePruning (CP) baseline as mentioned in (Chiang,2005) and detailed in (Chiang, 2007).
In the latterwork, the author shows that CP clearly outperformsboth the naive single pass solution of severe prun-ing as well as the naive two-pass rescoring approach.Thus, we focus on comparing our approach to CP.CP is an optimization to the intersected LM pars-ing algorithm presented in Figure 1.
It addressesthe creation of the?Kk=1 | [Xk, ik, jk, ?]
| chart itemswhen generating consequent items.
CP amounts toan early termination condition when generating theset of possible consequents.
Instead of generatingall consequents, and then pruning away the poor per-formers, CP uses the K-Best extraction approach of(Huang and Chiang, 2005) to select the best K con-sequents only, at the cost of potential search errors.CP?s termination condition can be defined in termsof an absolute number of consequents to generate, orby terminating the generation process when a newlygenerated item is worse (by ?)
than the current bestitem for the same left-hand side and span.
To sim-ulate comparable pruning criteria we parameterizeeach method with soft-threshold based criteria only(?
for CP and Z for H.Search) since counter basedlimits like K have different effects in CP (selectinge labeled items) vs H.Search (selecting rules sinceitems are not labeled with e).5 Experimental FrameworkWe present results on the IWSLT 2006 Chinese toEnglish translation task, based on the Full BTECcorpus of travel expressions with 120K parallel sen-tences (906K source words and 1.2m target words).The evaluation test set contains 500 sentences withan average length of 10.3 source words.Grammar rules were induced with the syntax-based SMT system ?SAMT?
described in (Zoll-mann and Venugopal, 2006), which requires ini-tial phrase alignments that we generated with?GIZA++?
(Koehn et al, 2003), and syntactic parsetrees of the target training sentences, generated bythe Stanford Parser (D. Klein, 2003) pre-trained onthe Penn Treebank.
All these systems are freelyavailable on the web.We experiment with 2 grammars, one syntax-based (3688 nonterminals, 0.3m rules), and onepurely hierarchical (1 generic nonterminal, 0.05mrules) as in (Chiang, 2005).
The large number ofnonterminals in the syntax based systems is due tothe CCG extension over the original 75 Penn Tree-bank nonterminals.
Parameters ?
used to calculateP (D) are trained using MER training (Och, 2003)on development data.6 Comparison of ApproachesWe evaluate each approach by considering bothsearch errors made on the development data for afixed set of model parameters, and the BLEU metricto judge translation quality.6.1 Search Error AnalysisWhile it is common to evaluate MT quality using theBLEU score, we would like to evaluate search errorsmade as a function of ?effort?
made by each algo-rithm to produce a first-best translation.
We con-sider two metrics of effort made by each algorithm.505We first evaluate search errors as a function of novelqueries made to the g-gram LM (since LM calls tendto be the dominant component of runtime in largeMT systems).
We consider novel queries as thosethat have not already been queried for a particularsentence, since the repeated calls are typically effi-ciently cached in memory and do not affect runtimesignificantly.
Our goal is to develop techniques thatcan achieve low search error with the fewest novelqueries to the g-gram LM.To appreciate the practical impact of each algo-rithm, we also measure search errors as a function ofthe number of seconds required to translate a fixedunseen test set.
This second metric is more sensitiveto implementation and, as it turned out, even com-piler memory management decisions.We define search errors based on the weight ofthe best sentence spanning item.
Treating weights asnegative log probabilities (costs), we accumulate thevalue of the lowest cost derivation for each sentencein the testing data as we vary pruning settings ap-propriate to each method.
Search errors are reducedwhen we are able to lower this accumulated modelcost.
We prefer approaches that yield lowmodel costwith the least number of LM calls or number of sec-onds spent decoding.It is important to note that model cost(?
log(P (D))) is a function of the parameters?
which have been trained using MER training.
Theparameters used for these experiments were trainedwith the CP approach; in practice we find that eitherapproach is effective for MER training.6.2 ResultsFigure 3 and Figure 4 plot model cost as a functionof LM cache misses for the IWSLT Hierarchical andSyntax based systems, while Figure 5 plots decod-ing time.
The plots are based on accumulated modelcost, decoding time and LM cache misses over theIWSLT Test 06 set.
For H.Search, we vary the beamparameter Z for a fixed value of ?
= 5 during pars-ing while for CP, we vary ?.
We also limit the totalnumber of items on the agenda at any time to 1000for H.Search as a memory consideration.
We ploteach method until we see no change in BLEU scorefor that method.
BLEU scores for each parametersetting are also noted on the plots.For both the hierarchical and syntax based gram-mars we see that the H.Search method achieves agiven model cost ?earlier?
in terms of novel LMcalls for most of the plotted region, but ultimatelyfails to achieve the same lowest model cost as theCP method.3 While the search beam of Z mit-igates the impact of the estimated scores duringH.Search?s second pass, the score is still not an ad-missible heuristic for error-free search.
We suspectthat simple methods to ?underestimate?
the score ofa partial derivation?s remaining nonterminals couldbridge this gap in search error.
BLEU scores inthe regions of lowest model cost tend to be reason-ably stable and reflect comparable translation per-formance for both methods.Under both H.Search and CP, the hierarchicalgrammar ultimately achieves a BLEU score of19.1%, while the syntactic grammar?s score is ap-proximately 1.5 points higher at 20.7%.
The hierar-chical grammar demonstrates a greater variance ofBLEU score for both CP and H.Search comparedto the syntax-based grammar.
The use of syntac-tic structure serves as an additional model of targetlanguage fluency, and can explain the fact that syn-tax based translation quality is more robust to differ-ences in the number of g-gram LM options explored.Decoding time plots shows a similar result,but with diminished relative improvement for theH.Search method.
Profiling analysis of the H.Searchmethod shows that significant time is spent simplyon allocating and deallocating memory for partialderivations on top of the scoring times for theseitems.
We expect to be able to reduce this overheadsignificantly in future work.7 ConclusionWe presented an novel two-pass decoding approachfor PSCFG-based machine translation that achievessearch errors comparable to the state of the art CubePruning method.
By maintaining comparable, sen-tence spanning derivations we allow easy integrationof high or flexible order LMs as well as sentencelevel syntactic features during the search process.We plan to evaluate the impact of these more power-ful models in future work.
We also hope to addressthe question of how much search error is tolerable to3Analysis of total LM calls made by each method (not pre-sented here) shows the H.Search makes significantly fewer (1/2)total LM calls than CP to achieve each model cost.506IWSLT - LM Cache Misses Hierarchical-3200-3100-3000-2900-2800-2700-2600-25000.0E+00 2.5E+06 5.0E+06 7.5E+06Number of LM MissesModel CostCPH.Search0.1750.1780.1810.1910.1880.1770.1800.1820.1860.1910.1910.174Figure 3.
LM caches misses forIWSLT hierarchical grammar andBLEU scores for varied pruning pa-rametersIWSLT - LM Cache Misses Syntax37400374253745037475375002.0E+05 7.0E+05 1.2E+06Number of LM MissesModel CostCPH.Search0.2050.2060.2060.20.20.2060.2070.2070.207Figure 4.
LM caches missesfor IWSLT syntactic grammar andBLEU scores for varied pruning pa-rametersIWSLT - Syntax Decoding Time3740037420374403746037480375009.0E+02 9.8E+02 1.1E+03 1.1E+03 1.2E+03Decoding Time (s)Model CostCPH.Search0.2050.2060.2070.2050.2060.2060.2060.207 0.207Figure 5.
Decoding time (s)for IWSLT syntactic grammar andBLEU scores for varied pruning pa-rametersrun MER training and still generate parameters thatgeneralize well to test data.
This point is particularlyrelevant to evaluate the use of search error analysis.ReferencesBar-Hillel, M.Perles, and E.Shamir.
1964.
An efficientcontext-free parsing algorithm.
Communications ofthe Assocation for Computing Machinery.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proc.
of ACL.David Chiang.
2007.
Hierarchical phrase based transla-tion.
To Appear in the Journal of Computational Lin-guistics.C.
Manning D. Klein.
2003.
Accurate unlexicalizedparsing.
In Proc.
of ACL.Matthias Eck and Chiori Hori.
2005.
Overview of theIWSLT 2005 evaluation campaign.
In Proc.
of Inter-national Workshop on Spoken Language Translation,pages 11?17.Michael Galley, M. Hopkins, Kevin Knight, and DanielMarcu.
2006.
Scalable inferences and training ofcontext-rich syntax translation models.
In Proc.
ofNAACL-HLT.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proc.
of the 9th International Workshop onParsing Technologies.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.of HLT/NAACL.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
of ACL, Sap-poro, Japan, July 6-7.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalua-tion of machine translation.
In Proc.
of ACL.Stuart Shieber, Yves Schabes, and Fernando Pereira.1995.
Principles and implementation of deductiveparsing.
Journal of Logic Programming, 24:3?15.Mark Steedman.
1999.
Alternative quantifier scope inCCG.
In Proc.
of ACL.Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.2006.
Left-to-right target generation for hierarchicalphrase based translation.
In ACL.Benjamin Wellington, Sonjia Waxmonsky, and I. DanMelamed.
2006.
Empirical lower bounds on the com-plexity of translational equivalence.
In ACL.Dekai Wu.
1996.
A polynomial time algorithm for statis-tical machine translation.
In Proc.
of the Associationfor Computational Linguistics.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for machinetranslation.
In Proc.
of HLT/NAACL.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProc.
of the Workshop on Statistical Machine Transla-tion, HLT/NAACL, New York, June.507
