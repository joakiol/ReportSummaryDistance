A t ra inab le  method  fo r  ext rac t ing  Ch inese  ent i ty  namesand  the i r  re la t ionsYimin Zhang & Zhou Joe FIntel China Research CenterKerry Center 6F1No.
1 Guanghua Road, Chaoyang DistrictBeijing 100020, P.R.
ChinaAbstractIn this paper we propose a trainablemethod for extracting Chinese entity namesand their relations.
We view the entireproblem as series of classification problemsand employ memory-based learning (MBL)to resolve them.
Preliminary results showthat this method is efficient, flexible andpromising to achieve better performancethan other existing methods.1.
IntroductionEntity names and their relations formthe main content of a document.
By graspingthe entity names and their relations from adocument, we will be able to understand thedocument to some extent.In the field of information extraction,much work has been done to automaticallylearn patterns \[1\] from training corpus inorder to extract entity names and theirrelations from Engfish documents.
But forChinese, researchers primarily use man-made rules and keyword sets to identifyentity names \[2\].
Building rules is oftencomplex, error-prone and time-consuming,and usually requires detailed knowledge ofsystem internals.
Some researchers also usestatistical method \[3\], but the training needslots of human annotated data and only localcontext information can be used.
With this inthe view, we have sought a more efficientand flexible trainable method to resolve thisproblem.Section 2 first gives a general outline ofthe trainable method we have defined toextract Chinese entity names and theirrelations, then describes person nameextraction, entity name classification andrelation extraction in detail.
Section 3describes the preliminary experimentalresults of our method.
Section 4 contains ourremarks and discussion on possibleextensions of the proposed work.2.
General Outline of the Methodwe view the problem of Chinese entitynames and their relations as a series ofclassification problems, such as person ameboundary classification, entity nameclassification, noun phrase boundaryclassification and relation classification.
Ifthose classification problems are clarified,then we will be able to resolve the relatedextraction problem.
For example, if we cancorrectly classify (or identify) the beginningor the ending boundaries of a person nameappeared in a document, we will be able toextract the person ame.The process can be divided into twostages.
The first stage is the learning processin which several classifiers are built from thetraining data.
The second stage is theextracting process in which Chinese entitynames and their relations are extracted usingthe classifiers learned.
The learningalgorithm used in the learning process ismemory-based learning (MBL) \[4\].
MBLentails a classification based supervisedlearning approach.
The approach as beennamed differently in a variety of contexts,such as similarity-based, example-based,analogical, case-based, instance-based, andlazy learning, we  selected MBL as ourlearning algorithm because it suits well thedomains with a large number of features66from heterogeneous ources and canremember exceptional and low frequencycases that are useful to extrapolate from \[5\].In addition, we can customize the learnerusing different weighting functionsaccording to linguistic bias.The main steps for the learning processare:Step 1: Prepare training data in which allnoun phrases, entity names and theirrelations are manually annotated.Step 2: Segmenting, tagging, and partialparsing the training data.Step 3: Extract the training sets (instancebase) from the parsed training data.
Fourtraining sets are extracted for different askswith each related to Chinese person names,entity names, noun phrases, or relationsbetween entity names in the training data.The main features used in an example can beeither local context features, e.g.
dependencyrelation feature, or global context features,e.g.
the features of a word in the wholedocument, or surface linguistic features, e.g.character feature and word feature, or deeplinguistic features like semantic feature.Step 4: Use MBL algorithm to obtain IG-Tree \[4\] for the four training sets.
IG-Tree isa compressed representation f the trainingset that can be processed quickly inclassification process.
In our case, theresulted IG-Trees are PersonName-IG-Tree,EntityName-IG-Tree, NP-IG-Tree, andRelation-IG-Tree.The main steps for extracting process are:Step 1: Segmenting, tagging and partialparsing the input Chinese documents.Step 2: Identify Chinese people names usingPersonName-IG-Tree.Step 3: Identify Chinese organization amesusing the same method as described in \[2\].Step 4: Identify other entity names (location,time, number) using the same method asdescribed in \[2\].Step 5: Identify Chinese noun phrases (NPchunking) using NP-IG-Tree.Step 6: Use entity names and noun phrasesextracted to perform partial parsing again tofix the parsing errors.Step 7: Use EntityName-IG-tree to classifythe noun phrases extracted.
This step willidentify entity names that are missed in theprevious teps.Step 8: Use Relafion-IG-Tree to identifyrelations between the extracted entity names.For a better understanding of thealgorithm, we will describe in detail theperson name extraction, the entity nameclassification, and the relation extraction inthe next subsection.
Please note that we arenot going to discuss NP chunking furthersince it is beyond the main theme of thispaper.2.1 Person  Name ext ract ionChinese person names can be dividedinto two categories, local Chinese personnames that consist of Chinese surnames andgiven names and transliterated person namesthat are sound translations of foreign names.The length of a local person name rangesfrom 2 to 6 characters, while the length of atransliterated person name is unrestricted.After segmentation, person names areusually divided into several words.
The taski s  to extract the word sequences that areperson name components.
With this in view,we convert the person name extractionproblem to an equivalent classificationproblem, i.e.
classifying word sequencesexisting in the results of segmentation i totwo classes, namely Person-Name and Not-Person-Name.To classify a word sequence we need touse a number of features.
(1) Word features: the beginningword/tag of the sequence, the endingword/tag of the sequence.
(2) Local context features: the n-th(n<=3) word/tag before/after the sequence;the verb before/after the sequence.
(3) Context dependency features: thedependency relations of the word sequenceand the dependency relations of the first67word before/after the sequence.
The maindependency relations include verb-object(the relation between a verb and its nounobject), subject-verb (the relation between averb and its subject), subject-adj (therelation between an adjective and itssubject), adv-verb (the relation between averb and its adverbial modifier), adv-adj (therelation between an adjective and itsadverbial modifier), modifier-head (therelation between a noun and its modifier.
InChinese, the modifier can be adjective,noun, verb or other phrases).The word features and local contextfeatures can be directly extracted from theparsing results of the training data.
Theextraction of dependency features needsmore explanation.
We employ thecollocation information obtained from alarge corpus \[6\] to help the Chinese partialparser do the parsing.
In most cases,dependency relations can be taken directlyfrom the parsing results.
But, there are someinstances that the parser does not functionwell resulting in flat parsing trees.
Underthese circumstances, we resort to somesimple heuristics such as linear order fordependency relations, thus making ourmethod robust enough to extract most of thedependency relations.To make the learning process moreefficient, we use Boolean features in thetralning set, so every feature describedabove is translated into several Booleanfeatures.
For example, for the feature lth-Next-Word (the first word after the wordsequence), its value is the top 500 words(ordered by frequency) that can appear aftera person name.
We translate it into 500features with every feature name like lth-Next-Word-XX in which XX is one of the500 words.
The feature value 1 means thatthe XX appear next to the word sequence inthe instance.
The translated examples haveabout several thousands of Boolean features,which will be a big challenge for machinelearning algorithms like C4.5 and CN2, butfor MBL this is not a big problem.Furthermore, we use sparse arrayrepresentation to make the storagerequirement much lower.For every word sequence in the trainingdata that meets with one of the followingthree requirements, we extract that wordsequence, including its class and all itsfeatures described above:(1) Begin with a surnarne, plus 1 or 2characters.
(2) Begin with two surnames, plus 1 or 2characters.
(3) Begin with a character included inthe first character set of transliterated personnames (extracted from training data), plusseveral characters.
The name may notsurpass a normal word (that is included in alist of 5000 most frequently used Chinesewords), because these normal words rarelyoccurred in a transliterated name.For example, if in the training data, threewords "W1 W2 W3" are annotated as aperson name, then we will extract a Person-Name "Wl W2 W3" and Not-Person-Name"W1 W2".After all the examples are extracted fromthe training data, they are fed to MBLLearner to get the PersonName-IG-Tree.In the extracting process, we do the sameas in the learning process to extract allexamples, but the class of every example isunknown to us in advance.
With thePersonName-IG-Tree, we can derive theclass of every example, and then all wordsequences classified as PersonName areextracted.When a person name appears more thanonce in a document, we can rely on cachemechanism, similar to those described in \[2\],to solve ambiguous cases.
For example, aperson name "~:J~3~" appears more than oncein a document.
We first erroneously extract"~!~gj.~" from a sentence "~ i~1~-~~ 5~ ~-~ - - 'q" ;~ ~i~ ' t~  I~!1 ~ i~ ~ 131 ", thencorrectly extract "~31U' from anothersentence "~3,~-~- - :~" .
Now "~:\]~gj-~"and "~!5~" are both in the cache, the cachemechanism will be able to correct the first68error based on the heuristics that, if anextracted person name is a substring ofanother extracted person name and they donot appear in one sentence, then only thesubstring is the correct person name.To better understand the process ofextracting person names, we describe a fewintuitive xamples below.
(1) Sample 1The sentence: "~, .~:~~.
-~.
-~,~"The segmentation result: "~, .~  ~!Here both "~\]~-~-" and "~.~"  areperson name candidates for extraction.Because no dependency relations are foundfor the next word "-~-" and most trainingexamples with this feature are classified asNot-Person-Name in training data, so "~:\]~.~"is also classified as Not-Person-Name.
Boththe previous word -,~,~.~:.~.~e~ ~:rm,, and the nextword "$.,k~" are positive evidences thatmake it certain that ""~!~..~_" shduld beclassified as a person name, therefore ouralgorithm correctly extracted it from thissentence.
(2) Sample 2The sentence: ' "~ , : i~ l J~\ ]~.
i l~ l \ ] "The segmentation result: " .~:~ :~ l JOur a lgor i thmcorrect ly  classified"\]t~Jl~ll" as not a person name.
Thereason is that in the training data all wordsequences whose previous word is "~"  isa not a person name.These examples how that our methodperforms disambiguation well thanks to theMBL learner ' s  capability of catchingexceptions.2.2 Entity Name ClassificationThe task of entity name classification isto classify the given noun phrases intoseveral categories, such as organizationname, product name, location, etc., as wellas person names that are missed in theprevious extraction.In addition to the features used in personname extraction, more features are needed.Some of these features are equivalent to thefeatures used in Crystal \[8\], such as subject-auxiliary-noun, e.g.
the relation between"I~I~I~'~N\] '' and "~.~"  in the sentenceSome features are specific to Chinese.Semantic features are also included to makethe learned classifier more powerful.
Thesemantic features of a word can be takenfrom a widely used Chinese thesaurus \[7\]that classifies Chinese words into 12 broadcategories, 94 middle categories, and 1428small categories.
There are about 70thousands words in the thesaurus.Unlike other inductive learning systemsin the field of information extraction, such asCrystal, we use a general machine learningalgorithm to do the learning.
The mostrelevant earlier work is the experimentdescribed in \[8\] using the machine learningalgorithm C4.5.
Though their experimentshowed that the performance of C4.5 basedmethod was comparable to Crystal, theyabandoned this method due to the timecomplexity of C4.5 when ,dealing with largenumber of features.
MBL is similar to C4.5in that both are general machine learningalgorithms.
Their differences lie in that?
MBL is a lazy learning algorithm that keepsall training data in memory and onlyabstracts at classification time byextrapolating a class from the most similaritems in memory, therefore, its timecomplexity is much lower than C4.5,especially when training data contains largenumber of features and examples.
Actually,in Soderland's analysis \[8\], MBL's timecomplexity is only slightly greater than thatof Crystal.
Though the instance-basedalgorithm like MBL may require largememory, the advanced hardware technologyavailable today can overcome this problem.A sparse vector representation will alsolower the memory requirement.
Taken allthese into consideration, MBL is well suitedfor entity name extraction and relationextraction.
Furthermore, the simple instancerepresentation and weight function make69MBL-based method more flexible andextensible.
Any useful features can be addedto the system without any modification tothe algorithm.
In Crystal, however, addingmore features may affect the correctness ofweight function used in finding similarexamples.
Our method can employ globalfeatures, i.e.
features beyond the sentencelevel.
We treat a NP and all its occurrences(including its anaphofical references) in onetext as one single example and all contextwords that are in some dependency relationsto this NP as this example's features.
Thus,we can resolve more complicated cases thanCrystal.
For example, if a NP is in subj-verbrelation with verb "~,~" , it can be a personname or an organization ame.
But, if weknow all verbs that have subj-verb relationswith this NP, then we will know the exactclass this NP belongs to.The steps for entity name classificationare similar to the steps in person nameextraction.
Our method is quite impressivein that it can learn a lot of context features toclassify the entity names, e.g.
it correctlyclassifies "/~ 1\]~" in "~ t\]~ l~?J.~.gj-~" (Qiming'sfather) as a person name.
Such a personname cannot be recognized in person nameextraction because it does not begin with asurname or first character of transliteratedperson names.2.3 Relation extractionThis task is to identify relation classesbetween entity names.
Our current classesinclude employee-of, location-of, product-of, and no-relation.
The relations we canextract are by no means restricted to this set.We can expand the set if training data areprovided.The features for this task include featuresused in Soderland's experiment \[8\].
Thesefeatures are equivalent to the syntactic-lexical or syntactic-semantic constraintsused in Crystal.
The feature name beginswith the name of the syntax position (SUB J,OBJ, PP-OBJ etc.
), followed by the name ofthe constraint and the actual term or classname.
For example, "\]l~,~,~,~ '' in thesubject position would include the features:SUBJ-Terms-\]\[~l~ i,SUBJ-Terms-~,~SUBJ-Mod-Terms-Ii~l~ //the terms in themodifier of the subjectSUB J-Head-Terms-,e~, ~SUB-Classes-Employee // the semanticcategories of the subjectSUB-Mod-Classes-OrganizationSUB-Head-Classes-OrganizationMore features are introduced in ourmethod, such as the linear order of entitynames, the word(s) between the entitynames, the relative position of the entitynames (in one sentence or in neighboringsentences), etc.
These features will make ourmethod more robust han Crystal.For every two related entity names in thetraining data, we identify a training exampleand extract it.
After all the examples areextracted from the training data, they are fedto MBL Learner to get the Relation-IG-Tree.In the extracting process, we do the sameas in the learning process to extract all pairsof entity names.
Then using the Relation-IG-Tree, we can derive the relation betweenevery pair of entity names.To better understand the processof relation extraction, we describe a coupleof examples below.
(1) Sample 1The input text: jl~j~,l\]~l~\[\]t~ff3 \[ \]  ~ l~ lC JIT ~ i i~:~,~-$U~,  ..-In the entity extraction, we have extracted"~ l~ l~ l~"  as a company name and"IT~lj~'f@~.~,:~-" as a product name.
In thetraining data, some training examples havesimilar sentence patterns, e.g.
"CompanyName (~J / ;~)  ...Product Name ~J~\ ]~" ,and most of the time there are product-ofrelation between the two entity names.Based on this evidence, a product-ofrelation can be identified between"~\ ] :~1~"  and "IT~i~'~t:~-~-".
(2) Sample 270The input text: ~':t~;,~/~t~j~J2~,~'~k~t~~l~.~..,,~.
:;i~:t~_, ~L~tIJ!
:~I,~TCL~I~\[\]In the entity extraction, we haveextracted "~: t :~"  as a person name and"TCL~,~I~" as a company name.
Now wewant to test if these two entity names havean employee-of relation.
As can be seen inthe training data, if a person name and acompany name appear in neighbonngsentences, and no other person names andcompany names are found in between, theytend to have a employee-of relation.
Basedon this evidence, an employee-of relationcan be identified between "~: : \ [ :~"  and"TCL~\ [ \ ] " .
Current systems, such asCrystal, would find it difficult to resolvebecause these two entity names appear indifferent sentences.3.
System EvaluationTo test our method we prepare amanually annotated corpus comprised ofabout 200 business news.
All the entitynames (about 500 person names and 300organization names), noun phrases, andrelations (i.e.
employee-of, product-of,location-of) in the corpus were manuallyannotated.
Ten pairs of training set andtesting set were randomly selected from thecorpus with each set equivalent to half sizeof the entire corpus.
We ran our learning andextracting processes on all the data sets andcalculated the mean recall and precisionrates.
The results are showed in Table.
1.Table 1: Evaluation for extracting Chinese ntitynames and their relationsPerson NameOrganizationNameEmployee-OfRecall86.3%73.4%75.6%Precision83.2%89.3%92.3%Product-Of 56.2% 87.1%Location-Of 67.2% 75.6%As can been seen, our performance inperson name and organization nameextraction is  comparable to  other systems\[2,3\] considenng the relatively small size ofthe training corpus.
Based on our survey, ourwork on extracting entity relations isunprecedented for Chinese, therefore we areunable to establish a benchmark.
But, theextraction of emloyee-of relation looks quitegood.
Detailed analysis reveals that ourmethod can handle well some instanceswhere co-reference resolution is neededbecause we introduced cross-sentencefeatures.
The method did poorly on product-of relation extraction due to the errors innoun phrases chunking.
With a better NPchunking module, the performance can beimproved.4.
ConclusionIn this paper we presented a trainablemethod for extracting Chinese entity namesand their relations.
The method provides aunified framework based on MBL.
Ourpreliminary experiment demonstratesthat this trainable method is efficient andflexible.
Any linguistic features, eithersurface or deep, can be easily added intothe system.
Preliminary experimentshave shown that our performance iscomparable to or better than otherexisting trainable methods, such asHMM and Crystal.
Our work, however,is still in its preliminary stage.
Morethorough evaluation is required usinglarger testing corpora.
Some algorithmicextensions are also expected so as toimprove the performance, includingautomatic feature selection, coreferenceresolution, etc.Reference\[1\] S. Soderland, D. Fisher, J. Aseltine, and W.Lehnert.
CRYSTAL: Inducing a conceptualdictionary.
In Proceedings of the FourteenthInternational Joint Conference on ArtificialIntelligence, Montreal, Canada, August 1995.\[2\] H.-H. Chen, Y.-W. Ding, S.-C. Tsai, G.-W.Bian, Description of the NTU System used forMET-2, Message Understanding Conference71Proceedings(MUC-7), Washington, DC.Available at http://www.muc.saic.com/proceedings/muc_7 proceedings/ntumet2 .pdf.1998.\[3\] Shihong Yu, Shuanhu Bai and Paul Wu,Description o f the  Kent Ridge Digital LabsSystem Used for MUC-7, MessageUnderstanding Conference Proceedings,Washington, DC.
Available athttp://www.muc.saic.com/proceedings/muc_7_p?
roceedings/kent_ridge.pdf.
1998.\[4\] Walter Daelemans, Jakub Zavrel, Ko van derSloot, and Antal van den Bosch.
TiMBL:Tilburg Memory Based Learner, version 3.0,Reference Guide Reference: ILK TechnicalReport 00-01.
Available athttp:llilk.kub.nll-ilklpaperslilkOOO l.ps.gz., 2000.\[5\] Walter Daelemans, Antal van den Bosch,Jakub Zavrel, Jorn Veenstra, Sabine Buchholz,and Bertjan Busser.
Rapid development of NLPmodules with memory-based learning, InProceedings of ELSNET in Wonderland, pp.105-113.
Utrecht: ELSNET, 1998.\[6\] D. Lin.
Extracting Collocations from TextCorpora.
First Workshop on ComputationalTerminology, Montreal, Canada, August, 1998.\[7\] Mei Jiaju, ((Tong Yi Ci Ci Lin)> (Chinese),Shanghai Dictionary Publishing Press, Shanghai,1983.\[8\] S. Soderland.
"CRYSTAL: LearningDomain-specific Text Analysis Rules",Technical Report, Center for IntelligentInformation Retrieval, University ofMassachusetts, Available from http://www-nlp.cs.umass.edu/ciir-pubs/te-43.pdf, 1996.72
