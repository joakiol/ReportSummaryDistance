Sorting Texts by ReadabilityKumiko Tanaka-Ishii?Satoshi TezukaHiroshi TeradaGraduate School of Information Scienceand Technology, University of TokyoThis article presents a novel approach for readability assessment through sorting.
A comparatorthat judges the relative readability between two texts is generated through machine learning, anda given set of texts is sorted by this comparator.
Our proposal is advantageous because it solvesthe problem of a lack of training data, because the construction of the comparator only requirestraining data annotated with two reading levels.
The proposed method is compared with regres-sion methods and a state-of-the art classification method.
Moreover, we present our application,called Terrace, which retrieves texts with readability similar to that of a given input text.1.
IntroductionReadability assessment is an important NLP issue with much application in the domainof language education.
The capability to automatically judge the readability of a textwould greatly help language teachers and learners, who currently spend a great deal oftime skimming through texts looking for a text at an appropriate reading level.Substantial previous work has been done over the past decades (Klare 1963, DuBay2004a, 2004b).
Early work generated measures based on simple text statistics by as-suming that these reflect the text reading level.
For example, Kincaid, Fishburne, andRodgers (1975) assumed that the lengths of words and sentences represent their re-spective difficulty.
Chall and Dale (1995) used a manually constructed list of wordsassumed to capture the difficulty of vocabulary.
These measures are easy to use butdifficult to apply to languages other than English, because some features, such as wordlength, are specific to alphabetic writing.
Such methods, however, do not compete withrecent methods based on more sophisticated handling of language statistics.
Collins-Thompson and Callan (2004) proposed a classification model by constructing differentlanguage models for different school grades (Si and Callan 2001), and Schwarm andOstendorf (2005) applied a support vector machine (SVM).
Both of these methodsoutperform classical methods and are less language-dependent.These new methods, however, have a serious problem when implementation is at-tempted for multiple languages: the lack of training corpora.
Large amounts of trainingdata annotated with 12 school grades have not been at all easy to obtain on a reasonablescale.
Another possibility might have been to manually construct such training data,?
University of Tokyo Cross Field, 13F Akihabara Daibiru, 1-18-13 Kanda Chiyoda-ku, Tokyo, Japan.E-mail: kumiko@kumish.net.Submission received: 7 October 2008; revised submission received: 17 September 2009; accepted forpublication: 19 December 2009.?
2010 Association for Computational LinguisticsComputational Linguistics Volume 36, Number 2but humans are generally unable to precisely judge the level of a given text among 12arbitrary levels.
The corpora therefore have to be constructed from academic texts usedin schools.
The amount of such data, however, is limited, and its use is usually strictlylimited by copyrights.
Thus, it is crucial to devise a new method or approach that allowsreadability assessment by using only generally available corpora, such as newspapers.Given a single text, it is hard to attribute an absolute readability level from among12 levels, but given two texts, there should be a better chance of judging which of themis more difficult.
This intuition led to the new model presented in this article.
Our ideais based on sorting, which is implemented in two stages: A comparator is generated by an SVM.
This comparator judges the relativereadability of two given texts. Given a set of texts, the texts are sorted by the comparator with a sortingalgorithm.
In our case, we used a robust binary insertion sort, as explainedin further detail later in this article.The first step requires a training corpus, but because the comparator only judges whichof two texts is more difficult, the texts of a corpus need only be labeled according totwo different levels.
Two sets of texts?one difficult, the other easy?are far easier toobtain than a training corpus annotated for 12 different levels.
Overall, our model ofreadability thus differs from previous regression or classification models.Applying this new method, we also present an application, called Terrace, which is asystem that retrieves a text with readability similar to that of a given input text.
Terracewas originally motivated by a faculty request made by teachers of multiple foreignlanguages.
The system currently works for English and Japanese, and the languageswill be extended to include Chinese and French.Note that we do not claim that our model and method is better than existing meth-ods.
Although our method does compete well with previous methods, the classificationapproach used in any given scenario should remain the most natural, relevant method.The intention of this article is simply to propose an alternative way of handling read-ability assessment, especially when adequate training corpora annotated with multiplelevels are not available.2.
Related WorkReadability, in general, describes the ease with which a text document can be read andunderstood.
Readability is studied in at least two different domains, those of coherence(Barzilay and Lapata 2008) and language learning.
Readability in this article signifiesthe latter, for both a mother tongue and a second language.Even within this domain, substantial previous work has been done (Klare 1963;DuBay 2004a, 2004b).
DuBay (2004a) writes that:By the 1980s, there were 200 formulas and over a thousand studies published on thereadability formulas attesting to their strong theoretical and statistical validity (p. 2).Every method of readability assessment extracts some features from a text and mapsthe feature space to some readability norm.
There are the two viewpoints regardingfeatures and the mapping of feature values to readability, and correspondingly there aretwo kinds of work in this domain.204Tanaka-Ishii, Tezuka, and Terada Sorting Texts by ReadabilityRegarding the first type, many researchers have reported how various featuresaffect the readability of text in terms of vocabulary, syntax, and discourse relations.Recently, Pitler and Nenkova (2008) presented an impressive verification of the effects ofeach kind of feature and found that vocabulary and discourse relations are prominent,although other features are not negligible.The focus of the current work, however, is not on what feature set to consider, so weuse the same features throughout the article, as explained further in Section 3.1.
Rather,the focus of this article is on mapping the extracted feature values to a readability norm.So far, two models have been used for this: regression and classification.In regression, readability is given by a score based on a linearly weighted sumof feature values.
Early methods, from the Wannetka formula (Washburne and Vogel1928), to the recent methods of Flesch?Kincaid (Kincaid, Fishburne, and Rodgers 1975)and Dale?Chall (Chall and Dale 1995), are of this kind.
Elaboration of such regressionmethods in a more modern context could proceed through a generalized linear modelbased on estimation of the weights by machine learning, although we have not foundsuch an approach within the literature of readability assessment for language learning.Our proposal is compared with such an enhanced version of regression in Section 8.In classification, readability is segmented by academic grades, and the assessmentis conducted as a classification task.
The first is implemented by means of statisticalclassification modeling, as reported in Collins-Thompson and Callan (2004) and Siand Callan (2001).
The authors used a language model (unigrams) and a naive Bayesclassifier by presuming different language models for each reading level.
A languagemodel Mi is constructed for each level of readability i by using different corpora foreach level.
The readability of a given text T is assessed using the formula L(Mi|T) =?w?TC(w) log Pr(w|Mi), where w denotes a word in text T, C(w) denotes the frequencyof w, and Pr(w|Mi) denotes the probability of w under Mi.
The second is based on anSVM (Schwarm and Ostendorf 2005) and the authors also studied the effect of statisticalfeatures, such as n-grams and syntactic features.In these papers, the readability norms are represented by means of scores andclasses of readability.
That is, given a single text, the system assigns a value correspond-ing to a school grade.
The result is easy to understand, and various applications havebeen constructed with this type of scoring.
This solution only works, however, whena sufficient amount of training data with annotations regarding multiple levels is pro-vided.
Usually, the availability of training data in readability assessment is limited, evenfor school grading.
This is due to the inherent difficulty of classifying the readability of atext into 12 grades, making it difficult to uniformly construct a large set of training data.Moreover, the copyright issue is more serious for academic texts.1 Given this situation,when readability assessment is modeled by regression or classification, a research teamwanting to apply these previous methods faces the problem of assembling training data,as we did for over a year.In this article, the readability norm is designed in a completely different way: Giventwo texts, a comparator judges which is more difficult.
By applying this comparator, aset of texts is sorted.
The readability of a text is assessed by searching for its position1 We asked the authors of previous studies based on the classification approach to share their trainingdata, but they could not because of the copyright issue.
This is a serious issue in Japanese, as well:These copyrights are more tightly protected than those of normal texts, and publishing companiesrefused to provide us with electronic files.
We thus had to scan texts and use OCR to obtain the testdata utilized in this work.
This experience demonstrates how difficult it is to obtain large-scaletraining data with multiple levels.205Computational Linguistics Volume 36, Number 2within the sorted texts.
The norm is thus considered as the location of a text among anordered set of texts.
Our approach linguistically enhances assessment of the readability ofa text as the relative ease compared to other texts, not as the absolute difficulty of the text.The root of this idea has been presented in two articles of which we are aware.
InInui and Yamamoto (2001), the readability of sentences for deaf people is judged by acomparator generated by an SVM.
In addition, Pitler and Nenkova (2008) presented acomparison of texts in terms of difficulty by using an SVM.
Similarly to what we presentin Section 3.1, those authors propose constructing a comparator by using an SVM tocompare two sentences or texts with multiple features.
However, neither further appliedthis approach to obtain readability assessment based on sorting.
Our contribution inthis study is therefore that we show how a machine learning method can be used as acomparator and applied to sort texts.Our method can be situated more generally among machine learning methods forranking, where the methods learn so that they rank a set of elements given a set ofordered training data.
Various methods have been proposed so far.
In one of the earliestattempts, Cohen, Schapire, and Singer (1998) obtain a function that scores the probabil-ity that an element is ranked higher than another, and rank all elements by maximizingthe sum of the pairwise probabilities.
In another, Joachims (2002) applies an SVM torank elements, by devising the input vector by subtraction of feature values.
In morerecent studies, such as Xia et al (2008), an attempt is made to directly obtain the rankingfunction for the whole ordered training data, not as a composition of pairwise functionapplication between elements.
Among these methods, our proposal is unique in twoways.
First, none of the previous methods, as far as we know, proposed discretizedranking based on sorting.
Second and most importantly, all previous methods assumethe existence of fully ordered training data.
In contrast, as emphasized by our problemdescribed in this section, such training data are difficult to acquire in the readabilitydomain, and we have to devise a method which works even when very limited trainingdata are all that is available.
Our contribution lies in our study of the possibility ofusing a learning-to-rank method even when learning data are only partially available.Such an approach can be further considered for learning-to-rank methods in general infuture work.3.
The MethodIn our method, a comparator of the level of difficulty of two texts is generated by usingmachine learning and then the comparator is applied to sort a set of texts.
The methodhas two parts: construction of the readability relation <, and sorting and searching texts by using the relation.These tasks are explained in the following sections.3.1 Readability ComparatorThe readability comparator is constructed by applying machine learning.
Given textsa, b ?
S, where S is a set of texts, feature vectors Va and Vb are constructed.
By applyingan operator ?, Vab is constructed as Va ?
Vb.
When Vab is entered into the comparator, thecomparator outputs 1 when a > b (i.e., a is more difficult) and ?1 when a < b. Because206Tanaka-Ishii, Tezuka, and Terada Sorting Texts by ReadabilityFigure 1Construction of a comparator using an SVM.the output is binary, we use an SVM to construct the comparator (see Figure 1).
Notethat Vab and Vba are not the same.
Reversibility of a feature vector is thus not obvious inour work, but ideally, when the judgment for Vab is 1, that for Vba will be ?1.In terms of constructing a feature vector Va for a text a, substantial features havebeen proposed (Klare 1963; DuBay 2004a, 2004b; Schwarm and Ostendorf 2005; Pitlerand Nenkova 2008).
In this work, we only utilize the most basic features of vocabularyin terms of word frequencies for three reasons.
First, as stated in Section 2, becausethe focus of this article is not to study the set of features, it is best to set the featureissue aside and use only the most fundamental features.
Even then, there are manyviewpoints to be verified, as will be seen in Sections 7?9.
Furthermore, we have to takeinto account the pros and cons of various features, because naive features only capturecoarse, default trends and could degrade performance.
For example, the text lengthin our data tends to be longer for more difficult texts, thus having a bad influence onshort, higher-grade texts and long, lower-grade texts.
Therefore, in this article, we onlyconsider simple features regarding vocabulary.
Second, previous work has argued forthe fundamental nature of vocabulary as a factor in readability (Alderson 1984; Laufer1991; Pitler and Nenkova 2008).
Third, some features other than word frequencies arelanguage-dependent in terms of the writing system, corpus availability, and perfor-mance of NLP analysis systems.
For example, average word length, used in the Flesch?Kincaid approach, cannot be applied to Japanese.
Thus, we have focused on featuresthat are available for any language, so that the performance becomes comparable acrosslanguages.The features we use are as follows.
There are two factors within vocabulary: the localand global factors.
The local factor is what words are used and how frequently they ap-pear within a text, whereas the global factor indicates the degree of readability of thesewords among the overall vocabulary.
Both are considered within this article, as follows:Local: the frequency of each word divided by the frequency of the number of words inthe text (this is called relative frequency here).Global: the log frequency of words obtained from a large corpus.Local frequency is the most basic statistic used in machine learning methods.
Relativefrequency is used to avoid the influence of text length, as mentioned above.Regarding global frequency, psychological studies have shown that every word hasa level of familiarity that is fairly commonly understood among people.
For exam-ple, the verb meet is more familiar than the verb encounter.
Such levels of familiarity207Computational Linguistics Volume 36, Number 2attributed to all words should affect the difficulty levels of texts.
In Chall and Dale(1995), the authors counted the number of words not appearing in a list of the mostbasic 3,000 words, in order to judge vocabulary difficulty, but the plausibility of such alist is difficult to evaluate and such lists might not exist for languages other than English.An alternative is to use word lists with familiarity scores, such as the MRC list in English(Database 2006) or the Amano list in Japanese (Amano and Kondo 2000), but such listsare costly to generate and the word coverage is limited.
Instead, we decided to use thelog frequency obtained from several terabytes of corpora, because the psychologicalfamiliarity score is known to correlate strongly with the log frequency obtained fromcorpora, as reported in Tanaka-Ishii and Terada (2009) and Amano and Kondo (1995).Tanaka-Ishii and Terada show that log frequency correlates better for larger corpora.Thus, as will be explained in Section 6, Web corpora consisting of 6 terabytes forEnglish and 2 terabytes for Japanese?possibly the largest corpora available for the twolanguages?were used to obtain the log frequencies.Local and global features are extracted for all words of the two texts a and b tobe compared.
The final vector is generated by applying an operator ?, such that Vab =Va ?
Vb.
Two simple possibilities for the operation ?
are the following:Concatenation: concatenate Vb?s elements after the elements of Va.Conjunction: produce a new value for the ith element of Vab from the ith elements of Vaand Vb by some function.
Typical possibilities for such a function are subtractionand division.The two possible vectors for Vab are explained by the illustration in Figure 2.
The upperhalf of the figure shows Vab generated by concatenation.
Because the comparator isFigure 2Composition of the vectors fed into the SVM.208Tanaka-Ishii, Tezuka, and Terada Sorting Texts by Readabilityconstructed by machine learning, the dimension of the vectors is four times the numberof word tokens that appear in the training data.
Each kind of word has a location ina vector, which appears four times within Vab: twice in each Va and Vb for local andglobal features.
The first half denotes Va, and the second half denotes Vb.
In the first halfof Va, the vector values are the relative frequencies for words appearing in text a, or zerootherwise.
For the second half of Va, the values are log frequencies measured in a largecorpus for the words appearing in a, or zero otherwise.
Vb is constructed in a similarmanner, and the two vectors are concatenated.In contrast, in the case of conjunction, the dimension of the vector remains twicethe number of word tokens appearing in the training data, as illustrated in the lowerhalf of Figure 2.
The ith dimension of Vab is calculated as a function of the ith values ofVa and Vb.
Pitler and Nenkova (2008) are likely to have used subtraction within theirexperiment.2 For this reason, among various other possible functions for the conjunctionof two vector values, we also used subtraction.Construction of a comparator requires training data.
Because the comparator judgesthe difficulty between two texts, the training data contains two sets of texts, a set Ld ofa relatively difficult level and a set Le of a relatively easy level.
The SVM is trainedon a combination of texts taken from Le and Ld.
The training data can combine up to2 ?
|Le| ?
|Ld|.
For example, if |Le| = |Ld| = 600, the number of data points for trainingcould amount to 720,000.
Whether learning can fully exploit such combined trainingdata is an interesting issue, which is examined in Section 8.After training, for two given texts a?
and b?, the relative difficulty is judged byinputting Va?b?
into the SVM.
In this phase, if a new word which was not in the trainingcorpora appears in either a?
or b?, the word is ignored for the purposes of this work.3.2 Sorting and Searching TextsWith a comparator thus constructed, the texts of S are sorted.
Further, readabilityassessment is performed by searching for the position of a text within the sorted texts.The sorting and searching algorithms can be chosen from among the basic onesalready established within computer science.
There are further requirements, however,as follows: Searching must be as fast as possible for the actual application. Sorting and searching should be implemented with the same procedure,given the nature of the application, as will be explained in Section 5.Moreover, sorting should be done incrementally to facilitate the additionof texts. Sorting must be made robust to overcome judgment errors made by thecomparator.Given the first two requirements, we chose binary insertion sort and binary sorting fromamong various sorting and searching algorithms, because it provides one of the fastestsearch methods.2 This is no more than our guess.
The precise calculation method unfortunately cannot be understood fromtheir paper, because the description is very brief.209Computational Linguistics Volume 36, Number 2To meet the last requirement, the algorithm is made robust by performing multiplecomparisons at one time, as explained here.
In binary insertion sort, given a new text,the insertion position is searched for among the previously sorted texts (from the easiesttext to the most difficult text), and the text is inserted between two successive texts iand i + 1, where the text is more difficult than the ith easiest text and it is easier thanthe i + 1th text.
Through repetition of this operation, all texts are sorted.
Because ourcomparator is generated using an SVM, one sole comparison could lead to an erroneousresult.
Moreover, one comparison error could lead to disastrous sorting results with abinary sort, because binary search changes the insertion position significantly, especiallyat the beginning of the search procedure.To avoid such comparison mistakes, the comparison judgment is made throughmultiple comparisons with texts located at the proximity of each position.
The numberof multiple comparisons is called the width and is denoted as M = 2K + 1, with Kbeing a positive natural number, as illustrated in Figure 3.
M is dynamically changedduring the search procedure by narrowing it down to indicate the exact position.
Theprocedure starts with a given set of texts S and an empty set SS.
At each judgment, atext to be inserted is removed from S and inserted into SS.
The procedure proceeds asfollows:1.
Obtain a text x from S. If SS is empty, the text x is put into SS and anothertext x is obtained from S.2.
The texts of SS are already sorted and are indexed by i = 1 .
.
.
n, wherei = 1 is the easiest and i = n is the most difficult.
Searching is done usingthree variables, which are initialized as follows: bottom ?
1; top ?
n;middle ?
(top + bottom)/2.
The width M is set as follows: M = 2 ?
K + 1(K ?
N ).3.
If M > (top ?
bottom)/2, then K ?
(top ?
bottom)/2.
A change in K alsochanges the value of M, which is always set to 2 ?
K + 1.4.
Compare x with texts at i = middle ?
K...middle + K. If more than K textsare judged as difficult, then top ?
middle ?
1.
Otherwise, bottom ?middle + 1.5.
If top < bottom, then insert x before the bottom.
Otherwise, go to step 3.Figure 3Robust comparison.210Tanaka-Ishii, Tezuka, and Terada Sorting Texts by Readability6.
Insert x before bottom.7.
If |S| > 1, then go to step 1.
Otherwise, the procedure is complete.The complexity for sorting a set |S| amounts to O(C|S|2), where C is the maximum valueof M. Because the time complexity required for insertion is log n, it might seem thatthe total time complexity is O(C|S| log |S|), but this is not the case: Random access forbinary insertion requires the whole data structure to be implemented by an array, whichrequires copying of the whole SS at every insertion.
Note that the sorting speed at thispoint does not affect the usability of our application, since the text collection is sortedoffline (as explained in Section 5).On the other hand, the searching speed does affect the usability.
For sorted texts,the readability of a text is assessed by searching for its insertion position.
The searchingis done by binary search, through the same procedure in steps 2 to 6.
The computationalcomplexity is O(C log |SS|), where C is the maximum value of M.4.
Pros and Cons of the Proposed MethodNow that we have explained our method, we compare it with the previous models ofregression and classification mentioned in Section 2.
The comparison is summarized inTable 1.In previous work that we mentioned, the readability of a text is represented by ascore or class, which typically has been indicated in terms of school grades (third row).In contrast, readability in our method is presented as a position among texts, indicatingthe ranking of a text situated globally among the other texts of SS.
Considering thenature of the output of assessment, the regression method is continuous, in that featurevalues are mapped to scores within a continuous range, whereas classification and ourmethod are both discrete, in that the former gives a class and the latter gives a rank(fourth row).This fundamental difference gives rise to pros and cons.
Above all, the advantageof our method is that it facilitates the construction of training data (fifth row), because itrequires only two sets of typically difficult and easy texts.
Here, what kinds of corporathe regression method requires in the modern machine learning context has not beenclarified because of the lack of previous work in machine learning regression.
However,our empirical results, shown in Section 8, suggest that the two sets of difficult andeasy training data will not be sufficient, and machine learning regression requires textslabeled with scores for different levels.Table 1Qualitative comparison of our method with previous methods.previous methods our methodmodel regression classification orderingreadability score class rankingoutput continuous discrete discreterequired levels of training data multiple multiple twocomprehensiveness high high questionablespeed of assessment fast fast slow211Computational Linguistics Volume 36, Number 2At the same time, there are disadvantages to our model.
First, because the methodonly outputs a relative ranking, applications must be re-designed differently from thosein the previous work (sixth row).
It could be said that when a > b for documents a andb, then a contains more unfamiliar words as tokens.
Even if two texts are next to eachother, however, their readability could be very different.
For example, texts of grades 1and 11 could be next to each other in a collection if it lacks texts from grade 2 to 10.
Anapplication system generated using our model must cope with this new problem causedby this lack of absoluteness for the readability norm.
In the next section, we show howthis problem is dealt with in Terrace through the use of graphical representation.Second, the scores and classes of regression and classification are used to hash theposition of texts within the readability norm.
On the other hand, because our methodlacks scores, the location of a text must always be searched for.
Thus, the previousmethods require only O(1) time for searching, whereas our method requires a certainamount of time before a response is obtained.
Therefore, it must be verified that an ap-plication works within a reasonable response time when handling a large text collection.In Section 9, we show that the response time is indeed within a reasonable time.5.
Terrace: An ApplicationAs mentioned in Section 1, creation of this system was motivated by a request fromfaculty members who teach multiple foreign languages.
These teachers must look forup-to-date reading materials with appropriate reading levels every day.
This requiresscanning through newspapers and Web resources.
The teachers?
request was that weconstruct a system that would facilitate this text search task.
More precisely, given asample text used within the classroom, the system should return texts of similar levelsof readability from among newspaper articles and other on-line archives.At the beginning of this project, we tried to apply Schwarm and Ostendorf (2005)and Collins-Thompson and Callan (2004).
Because the request covered multiple lan-guages, though, we faced the corpus construction problem in different languages.
Thisproblem was serious, to the extent that training corpora used in published work inEnglish were unavailable.
We were forced to look for another path towards buildingthe requested system without annotated corpora having academic grades.The problem of returning texts of similar readability does not necessarily requireintermediating a score/class, because both the input and output are texts.
Thus, chang-ing our way of thinking, we looked for another readability norm, which led to theframework presented so far.Terrace is a Web-based system in which the user uploads a text, and the systemreturns texts of similar readability from among a collection of texts usable as teachingmaterials, which are crawled for and obtained from the Web.
Currently, Terrace worksfor English and Japanese.
The number of languages is currently being increased.Figure 4 shows an example of giving an input text to Terrace.
After the user uploadsthe text and clicks the Terrace search button, a page like that shown in Figure 5 isdisplayed.After showing the Terrace banner, the system presents the ranking of the text withinthe text collection, which is crawled from the Web site.
The number of texts in thecollection is 14,877 and each is taken from CNN (CNN 2008).
The ranking in thisexample is 8,174th from the easiest text.
A horizontal bar is shown below, indicatingthe location among the 14,877 texts, with triangular indicators showing the locationsof texts with annotation in terms of grades.
These indicators are meant to help usersunderstand the location of the input text.
They are generated by sorting texts with212Tanaka-Ishii, Tezuka, and Terada Sorting Texts by ReadabilityFigure 4Screen shot of Terrace input.Figure 5Screen shot of Terrace result for the given input.annotations together with the text collection.
For example, the leftmost indicator G1shows where a text with an annotation of grade one is located within the bar.
Note thatsuch indicators are easily generated, because the number of annotated texts does notneed to be large.213Computational Linguistics Volume 36, Number 2Figure 6Terrace system.The texts with the closest readability are shown below, and then easier texts arelisted.
Further down, more difficult texts are listed.
By clicking on any of these texts, theuser can obtain texts with the desired readability.This functionality of Terrace is implemented via two modules, one each for search-ing and for crawling, as shown in Figure 6.
The crawler collects texts from news sites andother related archive sites every day, and the module incrementally sorts the collecteddocuments.
These texts are searched upon a user request.6.
Data for EvaluationIn the rest of this article, the proposed method and the Terrace system are evaluated.The key question to be considered through the evaluation is whether the comparatorcan discern slight differences in the readability levels of test data from only two sets oftraining data that are roughly different.The proposed method was tested for English and Japanese.
The data is summarizedin Table 2, where the upper block corresponds to English and the lower to Japanese.
ForTable 2Training and test data.English Datalabel corpus levels # of Texts # of WordsTraining & TD1-E Time normal 600 623,203children 600 259,163TD2-M-E AtoZ 27 levels (5 grades) 674 1,060,557TD2-F-E English textbook 5 levels (linear) 153 114,054Japanese Datalabel corpus levels # of Texts # of WordsTraining & TD1-J Asahi newspaper normal 600 841,289children 600 533,568TD2-M-J Japanese Textbook 6 levels (linear) 58 121,610TD2-F-J Japanese Proficiency Test 4 levels 44 87,846214Tanaka-Ishii, Tezuka, and Terada Sorting Texts by Readabilityboth languages, there were training data and test data.
The test data consisted of twosorts:TD1: A collection of texts taken from the same kind of data as the training data.TD2: A collection of texts unrelated to the training data and originally assigned levelsor a linear ordering.
These levels and ordering were used as the correct orderingin our work.TD2 further consisted of two kinds of data in each language: data for learners of theirmother tongue (i.e., children and students) and data for learners of a foreign language.These are labeled as TD2-M and TD2-F, respectively.For English, the training data were taken from Time (Time 2008) and Time For Kids(TimeForKids 2008).
We downloaded 600 articles (that is, |Ld| = |Le| = 600), of which 100were used as TD1-E.
The total number of different words in TD1-E is 22,736, which isthe dimension of the feature vector of a text when TD2 is used as the text data.3 Whenusing subtraction as ?, the dimension is doubled (for local and global), and when usingconcatenation, the dimension is four times this value.
TD2-M-E consisted of the data setcalled AtoZ, which can be purchased (ReadingA-Z.com 2008).
Each of the texts in thisdata set is labeled by 27 levels and graded by 5 levels.
TD2-F-E consisted of the Englishtextbooks used in Japanese junior high and high schools (Morizumi 2007; Yoneyama2007).
These texts are classified into five grades and also linearly ordered; that is, thetexts become more difficult in their order of appearance in the textbooks.
These levelsand orders originally attached to the data were used as the gold standard in this study.For the global frequency, we used the log frequency of each word as measured fromalmost 6 terabytes of Web data in English, scanned in the autumn of 2006 (Tanaka-Ishiiand Terada 2009).For Japanese, the training data and TD1-J were taken from Asahi newspapers(AsahiNewspaper 2008; KodomoAsahi 2008).
Six hundred (600) articles were acquired,and 100 of these were used as TD1-J.
The total number of different words in this trainingdata is 48,762.4 TD2-M-J consisted of Japanese junior high and high school textbookswith six grades, which also appeared in a linear order (Miyaji 2008).
TD2-F-J consistedof the texts used in the Japanese language proficiency test (JEES 2008).
The texts in thisdata set are classified into four levels and not linearly ordered.
For the global frequency,we used the log frequency of each word as measured from almost 2 terabytes of Webdata in Japanese, scanned in the autumn of 2006.The other evaluation settings were as follows.
As the SVM (Joachims 1998, 1999), weused LIBSVM (Chang and Lin 2001).
The SVM training was done using the parametersof cost = 0.1 and gamma = 0.00001 with a Gaussian kernel.
The value of K used in robustsorting (Section 3.2) was K = 2.7.
Evaluation of the ComparatorThe basic performance was first tested using TD1.
Because |Ld| = |Le| = 600, 500 textswere chosen and paired randomly, and 2 ?
500 = 1,000 pairs were used for training.
Thefactor of 2 is necessary, because a pair can be used twice by exchanging the comparisonorder Ved and Vde.
The remaining 100 texts were randomly paired and tested.
The results3 Words are transformed into their standard forms using the Lancaster algorithm (Paice and Husk 2008).Moreover, we do not use feature selection, because it has been reported that the distribution of functionwords also counts with respect to readability (Collins-Thompson and Callan 2004).4 Words are transformed into their standard forms using MeCab (Kudo 2009).215Computational Linguistics Volume 36, Number 2Table 3Accuracy of the comparator for different operators and features tested on TD1.EnglishConcatenation 91.67%Subtraction 94.27%JapaneseConcatenation 95.47%Subtraction 95.47%presented here were produced through six-fold cross-validation, and each fold wasfurther repeated five times by changing the pairing of training and testing (i.e., totalexecution was done 30 times to obtain one performance value).Table 3 shows the accuracy for English (upper block) and Japanese (lower block).The rows of a block represent the different operators explained in Section 3.1.
Overall,the scores were above 90%.The comparator performance for TD2 was also investigated (Table 4).
Here, thetraining data amount was set to 2 ?
600.
The accuracy was measured for all pairs oftexts with different levels.
For TD2-M-E, all five levels were considered, whereas forthe linearly ordered TD2-F-E and TD2-M-J, the levels were considered by grade (thatis, five levels for TD2-F-E and six levels for TD2-M-J).
The accuracy reported here is theaverage of execution done five times by changing the random pairing for the trainingdata.
The performance was, in general, lower than that for TD1, but still stayed closeto 90%.
For the operator ?, whether concatenation or subtraction was used made nodifference.
Therefore, from here on, the operator ?
is set to concatenation.To evaluate the classification performance in more detail, the accuracy for everytwo-class combination was determined for TD2-M-E in English, as shown in Table 5.Because TD2-M-E has five grades, the columns indicate the 1st to 4th grades, whereasthe rows indicate the 2nd to 5th grades.
The evaluation thus forms a 4 ?
4 table, whereeach cell indicates the accuracy of distinguishing the two?class pairs for the row andcolumn.Table 4Accuracy of the comparator (TD2).EnglishConcatenation SubtractionTD2-M-E 97.23 % 97.14 %TD2-F-J 90.06 % 90.06 %JapaneseConcatenation SubtractionTD2-M-J 88.18 % 88.18 %TD2-F-J 94.49 % 94.49 %216Tanaka-Ishii, Tezuka, and Terada Sorting Texts by ReadabilityTable 5Classification results between two classes, tested for TD2-M-E in English.Grade 4 Grade 3 Grade 2 Grade 1Grade 5 84.28% 98.18% 100.00% 100.00%Grade 4 ?
94.16% 99.97% 100.00%Grade 3 ?
?
96.79% 100.00%Grade 2 ?
?
?
99.01%The closer to the diagonal, the more difficult the classification task was, becausethe levels to be distinguished became closer to each other.
The results reflect this ten-dency, with lower values for cells closer to the diagonal.
In particular, the performancefor discerning grades between pairs of the 4th and 5th grades was poor.
Distinctionbetween the 1st/2nd and 2nd/3rd grades was more successful than that between the3rd/4th and 4th/5th grades, since the lower the grades the easier it is to discern twogiven successive school levels.Before going on to actually sort text using the comparator, we verified how abnor-mal our generated comparator was.
Ideally, we want a complete ordering of the set,and for this the comparator must obey certain laws in the sense of mathematical sets.
Acomparator is considered abnormal if it does not obey two laws:Reversibility: Texts a, b are defined as reversible if b < a and a > b both hold.
Thiscorresponds to the law that when Vab?s value is +1, then Vba?s value must be ?1and vice versa.Transitivity: If a < b and b < c, then a < c.Especially for transitivity, in an ordered set this law is the primary requirement thatmust be fulfilled among ordered elements.
If transitivity does not hold in many triples,we have to introduce partial ordering instead of the total ordering considered thus far.The anomalies were measured by using the four TD2 data sets.
For all pairs andtriples of TD2, we tested the reversibility and transitivity for all possible pairs and triplesby changing the random pairing of test data (TD1) five times.
For reversibility, 10 pairs out of 226,801 for TD2-M-E were non-reversible once, 1 pair out of 11,628 for TD2-F-E was non-reversible once, and all pairs for all other data and random pairings of training data werereversible.Such strong results were obtained because the training was done by reversing theorder of the pairs (thus, the SVM learned both Ved as +1 and Vde as ?1.)
Similarly, fortransitivity, one triple out of 50,803,424 of TD2-M-E was non-transitive once, and all triples for all other data and random pairings of training data weretransitive.Such results show how rarely these anomalies occur in our method.
Therefore, ourchoice of total ordering seems relevant.217Computational Linguistics Volume 36, Number 28.
Evaluation of SortingSince the basic results have been clarified thus far, we will now report the results forconcatenation for a more global evaluation of sorting and searching.The TD2 data were sorted by the method explained in Section 3.2, and the correla-tion with the correct order was investigated.
Three methods were used for comparison: Flesch?Kincaid Dale?Chall Support vector regression (SVR [Drucker et al 1996])In these three methods, the readability level is obtained as a value, whereas our methodpresents an order.
Therefore, the results of the three methods were sorted according tothe values.
The resulting orders for the three methods and for ours were then comparedwith the correct order in the test data.
We used the finest annotation for correct ordering;for example, 27 levels for TD2-M-E, linear ordering for TD2-F-E, linear ordering forTD2-M-J, and 4 levels for TD2-F-J.Here, SVR was trained by labeling the Time/Asahi newspaper texts as +1.0 andthe texts of Time/Asahi for children as ?1.0, and then using the 600 texts for training.The LIBSVM package was used with the same kernel and parameter settings given inSection 6.We used Spearman?s correlation to evaluate the ordering.
Spearman?s basic correla-tion formula isrs = 1 ?6?ni=1 d2in3 ?
nwhere n is the number of texts, and di is the difference in ranking between the correctand obtained results for text i.
This formula has an extended version to cope withmultiple elements having the same ranking.
Given x and y as ordered sequences withthe same ranking, the correlation is given as follows:rs =Tx + Ty ?
?ni=1 d2i2?TxTywhereTx = (n3 ?
n) ?nx?i=1(t3i ?
ti), Ty = (n3 ?
n) ?ny?j=1(t3j ?
tj)Here, nx and ny are the numbers of the rankings for equivalently ranked elements inx and y, respectively, and ti, tj denote the number of elements with the same rankingas elements which are indexed as i, j, respectively.
For example, given an order x =[1,2,3,3,4], nx = 1, because only 3 had the same ranking, ti = 2 for i = 1, because thereare two 3s.The results are shown in Figure 7.
The horizontal axis represents the four data setsof TD2, and the vertical axis represents the correlation value.
Note that for the Japanese218Tanaka-Ishii, Tezuka, and Terada Sorting Texts by ReadabilityCorrelationTest DataTD2-M-E TD2-F-E TD2-M-J TD2-F-JFlesch-KincaidDale-ChallSVROur MethodFigure 7Correlation with the test data.data, there are only two bars, because Flesch?Kincaid and Dale?Chall are inapplicable.Moreover, note that the vertical heights are comparable within a data set but not amongdata sets, because the number of levels for each data set is different.Comparing within each block, our method performed better than any other, havinga correlation of more than 0.8 for all cases.
Flesch?Kincaid performed quite well, havinga high correlation of more than 0.6 for the TD2-M-E data.
The use of SVR was lesseffective, with the correlation being lower than that with our method.
This shows thatthe performance of the regression method is limited, even with a machine learningmethod, when training data for two levels only are available.
In contrast, our methodshows that even with rough two-level training data, high correlation is achievable.
Suchperformance is enabled by comparison among texts even in the middle range betweendifficult and easy texts.How this performance compares to that of previous classification methods is diffi-cult to say.
Above all, our method cannot be fairly compared with previous classificationmethods from the viewpoint of classification, because in order to transform our sortedresults into classes, we would have to give the number of texts in each class.
Becausethis information is not provided to the classification methods, the comparison would beunfair, thus favoring our method.
Therefore, the comparison must be made by means ofcorrelation.
Another problem is that because we do not possess the training data usedin previous work, we could only test with what we have listed in Table 2.Therefore, we compared the performance using TD2-E-M as training data.
Becausethe amount of training data was small, the number of classes considered here was five.Slightly less than half of the data (67 texts) was taken from each of five different levels,and the remaining texts (which differed in number at each level) were used for testing:a sufficient amount of test data is needed, too, since our evaluation is done through219Computational Linguistics Volume 36, Number 2correlation measured on sorted results.
By exchanging the halves, the result reportedhere is the average of two-fold cross-validation.We compared three methods:1.
A classification method using part of TD2-E-M as training data.2.
The sorting method using the same part of TD2-E-M as training data.3.
The sorting method with TD1-E as training data.The first classification method followed that of Schwarm and Ostendorf (2005).
Theamount of training data used to build a classifier for each class was 67 for +1 and 4 ?
(67)for ?1.
The parameters used for the SVM were the same as those used for our method.For the second method, the training data consisted of 1,340 (2 ?
5C2 [pairs among 5classes] ?
67 texts) random pairs of two successive levels.
Each fold of two-fold cross-validation was done five times by changing the pairs randomly.
For the third method,TD1-E was used as in the previous evaluation, but this time the verification was donewith five levels (whereas the first block in Figure 7 was evaluated with 27 levels).
Theamount of training data was the same as TD1 (i.e., 1,200) and the experiment was donefive times by changing the pairs randomly.The results are shown in Figure 8.
Three bars are shown from first to third, cor-responding to each of the three methods.
For classification, the correlation was 0.925,whereas the correlations of the second and third bars were 0.946, 0.941, respectively.
Ourmethods thus slightly outperformed the classification method.
Moreover, the differencebetween the second and third methods showed that two-level training data couldperform similarly to multiple-level training data.
This shows the strength of our methodCorrelationMethodsSVM Our Method(AtoZ)Our Method(Time)Figure 8Comparison of classification methods and our model on TD2-M-E.220Tanaka-Ishii, Tezuka, and Terada Sorting Texts by Readabilityin that it can complement a limited amount of training data through relative comparisonwithin the set.It is not our intention, though, to assert that our method is better than the classifica-tion method based only on this experiment.
Classification has a much better chance ofachieving better performance with large-scale training data, especially when featuresare studied further.
The point here is to show the potential of the sorting model,especially when sufficient amounts of corpus data annotated in multiple classes areunavailable.Finally, we investigated the effect of the amount of training data as mentioned inSection 3.1.
Given a set of relatively difficult texts Ld and a set of easy texts Le, themaximum number of training pairs will amount to 2 ?
|Ld| ?
|Le|, with |L| indicatingthe number of elements in set L. Two effects related to the amount of data shouldbe considered.
First, there is the effect of the absolute amount of data used; that is,the amounts of |Ld| and |Le|.
We let N = |Ld| = |Le| and measured the correlation shiftby changing N from 100 to 600.
For each N, the number of training pairs was 2N,constructed by randomly sampling N documents from Ld and Le and forming N pairs,and then constructing Vde and Ved for each pair.Second, the effect of combination should be considered; that is, the number of pairswhose maximum number is 2 ?
|Ld| ?
|Le| for a given N. When N = 600, this amountsto 720,000 training pairs, which is too large in terms of the time required for training.Therefore, by fixing N = 100, we tested the learning effect for numbers of training pairsup to 2 ?
100 ?
{1, 5, 10, 50, 100}.The results are shown in Figure 9, where the first graph shows the effect of theamount of data and the second graph shows the effect of combination.
The horizontalaxes represent the amount of training data (namely, 2N for the first graph and 2 ?
100 ?
{1, 5, 10, 50, 100} for the second graph), and the vertical axes represent the correlation.In the second graph, the horizontal axis is in log scale.
Each graph has four lines, eachcorresponding to a subset of data from TD2.
Every plot was obtained by averaging fiverepetitions of the random pairing of learning data.In the first graph, the increase in the amount of local data led to only a slightincrease in performance, which is almost invisible.
In the second graph, the increasein combination did not lead to higher performance, but rather to drastically decreasedperformance for the English data.
This decrease was especially prominent with TD2-F-E, the English textbook for Japanese students.
This must have been due to the differentnatures of the training and test data.
On the other hand, the texts of the Asahi newspaperarticles and TD2-{M,F}-J are controlled under a similar standard (in terms of vocabu-lary, syntax, and so forth), which would account for the difference from the case inEnglish.These results suggest that even if our method has the possibility of obtaining largeamounts of training data by combination, this would not lead to higher performance.Moreover, the graphs suggest the importance of obtaining a sufficient amount of train-ing data from two levels which match the target domain.9.
Evaluation of the Terrace System9.1 Evaluation of SearchingThe search performance using the algorithm presented in Section 3.2 was evaluated viathe average positional error when a text is searched.
The search performance of our221Computational Linguistics Volume 36, Number 2     TD2-M-ETD2-F-ETD2-M-JTD2-F-JCorrelationData Amount     CorrelationData AmountTD2-M-ETD2-F-ETD2-M-JTD2-F-JFigure 9Effect of increasing the amount of data (left: corpus data amount; right: training data amountincreased by combination).method was compared with that of the three methods presented at the beginning ofSection 8.
Because the sorting performance already differed among the methods, weevaluated the performance on correctly sorted texts.
One text was removed from thecorrectly sorted texts, its position was searched for with each method, and the difference222Tanaka-Ishii, Tezuka, and Terada Sorting Texts by Readabilitybetween the resulting and correct positions was measured.
This procedure was repeatedfor all texts of TD2, and the average difference was obtained.
The finest levels for TD2were used as the correct annotation.
The execution was again performed five times bychanging the random pairing within the training data.The results are shown in Figure 10.
The horizontal axis represents blocks of TD2data, and the vertical axis represents the average locational error of searching dividedby the total number of levels of each data.
Unlike the correlation figures seen so far, thesmaller the value of each result, the better the precision of the search results.
As was thecase in Figure 7, the heights of bars are comparable within a data block, whereas barsacross blocks are not comparable because of the different numbers of levels.Naturally, the results are reversed from those of Figure 7.
Methods with higherperformance in sorting had smaller errors.
Overall, our method had the smallest errorsamong all methods.Because Terrace is different from previous systems based on the absolute scoresof Flesch?Kincaid or Dale?Chall, it was presented to several language teachers whooriginated the Terrace project (as mentioned in Section 5).
They reacted positively toTerrace, even though the system does not show any absolute readability scores.
Thereare two main reasons for the favorable response.
First, as mentioned, because theyneed texts rather than scores, teachers liked the fact that Terrace returns texts directlywithout outputting the values.
With previous systems, teachers themselves had to inputone candidate text after another to find one of an appropriate level.
Second, for for-eign language teachers, especially of languages other than English, scores are not wellstandardized, and teachers are often puzzled by some readability scores.
The teachinglevels do not necessarily correspond to standard school grades for natives.
Therefore,they prefer that a system outputs texts directly as in Terrace, because this means theyAverageError of SearchTD2-M-E TD2-F-E TD2-M-J TD2-F-JTest DataFlesch-KincaidDale-ChallSVROur MethodFigure 10Search comparison.223Computational Linguistics Volume 36, Number 2do not have to interpret some score which is difficult to assess.
Nevertheless, theyfound that the graphical indicators are helpful for locating themselves among numerouscollections.9.2 Response TimeIn Section 4, qualitative analysis showed that a disadvantage of our model is thegreater computational complexity in readability assessment.
This section reports on theresponse time when using Terrace.The response time of Terrace is determined by the addition of three factors: extraction of features from a text searching for the text?s position within a text collection system overhead time (server access, Web page generation, etc.
)The time needed for feature vector construction is linear with respect to the text length.This process is conducted once before a search.
The text?s position within the textcollection is then searched for.
Because comparison is performed multiple times in asearch, this factor dominates the response time.
As mentioned in Section 3.2, this timeis logarithmic with respect to the number of texts in a collection.
The last factor isconstant.We evaluated the response time for TD2-M-E, since we currently have a larger textcollection in English and TD2-M-E is a larger data set than TD-F-E. As noted in Section 5,this collection amounts to 14,877 texts taken from CNN (CNN 2008).
The input textswere all texts of TD2-M-E. Because K = 2, which makes M = 2 ?
K + 1 = 5 (e.g., thewidth of comparison was 5), the total number of comparisons was about 70 for onesearch.
The computational environment was as follows: CPU: Intel(R) Core(TM)2 QuadCPU Q9550 (2.83 GHz); main memory: 3.5 GB; OS: Debian GNU/Linux 4.0.The results are plotted in Figure 11.
Because the key factor for response time isthe length of a text, the horizontal axis represents the text length and the vertical axisrepresents the response time.
Each plot corresponds to one text in TD2-M-E.
The longerthe text, the slower the response time will be.
The plotted points are separated intotwo clusters, with one at the upper right and the other just below.
This disjunctioncorresponds to texts which were judged as more difficult and those judged as easier,respectively, at the first comparison within the binary search.
Because the Terrace systemstores its feature vectors in a MySQL database, when the number of non-zero featuresis large, access becomes slower, even when all vector dimensions are the same.
Fortexts judged as more difficult at the first comparison, the vector length in the string(which is how it is implemented in MySQL) becomes longer, and the overall proceduretakes a longer time.
In any case, for all texts the response time was less than a second.Therefore, although our method does have a complexity disadvantage, the results ofthis evaluation suggest this is not a serious problem.10.
ConclusionWe have described a new model of readability assessment that uses sorting.
Ourapproach makes it possible to assess the readability of texts in terms of relative ease224Tanaka-Ishii, Tezuka, and Terada Sorting Texts by Readability         ResponseTime(s)Number of WordsFigure 11System response time for texts in TD2-M-E.compared to other texts, rather than based on an indicator of the absolute difficultyof the text.
In our method, a comparator is constructed using an SVM, which judgeswhich of two texts is more difficult.
This comparator is then used to sort the texts of agiven set using a binary insertion sort algorithm.
Our method differs from traditionalreadability assessment methods, most of which are based on linear regression, and italso differs from recent methods of statistical readability assessment, which are basedon multi-value classification.
The advantage of our model is that it solves the problemof assembling training data annotated in multiple classes, because it only requirestwo classes of training data: easy and difficult.
At the same time, our model has thedisadvantages of the resulting norm being somewhat incomprehensible as a rankingand greater computational complexity compared to previous methods.After performing a fundamental evaluation, we compared the overall performanceof our method with those of other methods.
Our method achieved a higher correlationwith the gold standard, more than 0.8, in sorting text collections in both English andJapanese; this was higher than both traditional methods and an SVR trained withtwo-class corpora.
Another comparison with a state-of-the-art classification methodconfirmed the potential of our method.We also presented an actual application, called Terrace: a system that retrieves atext of similar readability from a text collection when given an input text.
We showedthat Terrace can overcome the disadvantages of the proposed model, by introducinga graphical representation of the positions of texts with known levels, and also byverifying that the response time is always less than a second.Currently, we are extending the number of languages for Terrace.
In addition, weare investigating the effects of introducing other features.225Computational Linguistics Volume 36, Number 2ReferencesAlderson, J. C. 1984.
Reading in a ForeignLanguage: A Reading Problem or a LanguageProblem.
London; New York: Longman.Amano, S. and T. Kondo.
1995.
Modalitydependency of familiarity ratings ofJapanese words.
Perception &Psychophysics, 57(5):598?603.Amano, S. and T. Kondo.
2000.
On theNTT psycholinguistic databases?lexical properties of Japanese?.Journal of the Phonetic Society of Japan,4(2):44?50.AsahiNewspaper.
2008. http://www.asahi.com/, accessed in June 2008.Barzilay, R. and M. Lapata.
2008.
Modelinglocal coherence: An entity-based approach.Computational Linguistics, 34(1):1?34.Chall, J. S. and E. Dale.
1995.
ReadabilityRevisited: The New Dale?Chall ReadabilityFormula.
Cambridge University Press,Cambridge.Chang, C. and C. Lin.
2001.
LIBSVM: ALibrary for Support Vector Machines.Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.CNN.
2008. http://edition.cnn.com,accessed in September 2008.Cohen, W. W., R. E. Schapire, and Y. Singer.1998.
Learning to order things.
Journal ofArtificial Intelligence Research, volume 10,pages 243?270.Collins-Thompson, K. and J. Callan.
2004.A language modeling approach topredicting reading difficulty.
InProceedings of HLT?NAACL,pages 193?200, Boston, MA.Database, MRC Psycholinguistic.
2006.http://www.psy.uwa.edu.au/mrcdatabase/uwa mrc.htm, accessed inDecember 2006.Drucker, H., Burges C., J. C. Kaufman,A.
J. Smola, and V. Vapnik, 1996.Support Vector Regression Machines.MIT Press, Cambridge, MA.DuBay, W. H. 2004a.
The Principles ofReadability.
Impact Information, CostaMesa, CA.DuBay, W. H. 2004b.
Unlocking Language:The Classic Studies in Readability.
BookSurgePublishing, Charlestown, South Carolina.Inui, K. and S. Yamamoto.
2001.Corpus-based acquisition of sentencereadability ranking models for deafpeople.
In Natural LanguageProcessing Pacific Rim Symposium,pages 205?212, Tokyo, Japan.JEES.
2008.
Japanese Proficiency Testhttp://www.jees.or.jp/jlpt/en/provided by Japanese EducationalExchange and Services, accessed inNovember 2008.Joachims, T. 1998.
Text categorizationwith support vector machines.
InEuropean Conference on Machine Learning,pages 137?142, Chemnitz, Germany.Joachims, T. 1999.
Making large-scalesupport vector machine learning practical.In Bernhard Scho?lkopf, Christopher J. C.Burges, and Alexander J. Smola, editors,Advances in Kernel Methods: Support VectorLearning.
MIT Press, Cambridge, MA,pages 169?184.Joachims, T. 2002.
Optimizing searchengines using clickthrough data.In International Conference onKnowledge Discovery and DataMining, pages 133?142, Edmonton,Alberta, Canada.Kincaid, J. P., R. P. Fishburne, and R. L.Rodgers.
1975.
Derivation of newreadability formulas for Navy enlistedpersonnel.
Research Branch Report 8?75,U.S.
Naval Air Station.Klare, G. R. 1963.
The Measurement ofReadability.
Iowa State University Press,Ames.KodomoAsahi.
2008.
Asahi Newspaperfor Junior School Students.
http://www.asagaku.com/, accessed in June 2008.Kudo, T. 2009.
Mecab, yet anotherpart-of-speech and morphologicalanalyzer.
Available fromhttp://mecab.sourceforge.net/.Laufer, B.
1991.
How Much Lexis is Necessaryfor Reading Comprehension?
Vocabularyand Applied Linguistics.
Macmillan,Basingstoke.Miyaji, H. 2008.
Junior and Junior HighSchool Japanese.
Mitsumura Tosho, Tokyo.From the 3rd grade to the 9th grade, inJapanese.Morizumi, M. 2007.
New Crown EnglishSeries 1, 2, 3.
Sanseido, Tokyo.Paice, C. and G. Husk.
2008.
The lancasterstemming algorithm.
Available fromhttp://www.comp.lancs.ac.uk/computing/research/stemming/index.htm.Pitler, E. and A. Nenkova.
2008.
Revisitingreadability: A unified framework forpredicting text quality.
In Proceedings ofEMNLP, pages 186?195, Honolulu,Hawaii.ReadingA-Z.com.
2008.
Reading A?ZLeveling and Correlation Charthttp://www.readinga-z.com/, accessedin July 2008.226Tanaka-Ishii, Tezuka, and Terada Sorting Texts by ReadabilitySchwarm, S. E. and M. Ostendorf.
2005.Reading level assessment using supportvector machines and statistical languagemodels.
In Annual Meeting of the ACL,pages 523?530, Ann Arbor, Michigan.Si, I. and J. Callan.
2001.
A statistical modelfor scientific readability.
In CIKM,pages 574?576, Atlanta, Georgia.Tanaka-Ishii, K. and H. Terada.
2009.
Wordfamiliarity and frequency.
StudiaLinguistica.
accepted in 2009, in press, toappear in 2010.Time.
2008. http://www.time.com, accessedin July 2008.TimeForKids.
2008. http://www.timeforkids.com/, accessed inAugust 2008.Washburne, C. and M. Vogel.
1928.
WinnetkaGraded Book List.
Chicago, AmericanLibrary Association.Xia, F., T. Liu, J. Wang, W. Zhang, and H. Li.2008.
Listwise approach to learningto rank: theory and algorithm.
InInternational Conference on MachineLearning, pages 1192?1199, Helsinki,Finland.Yoneyama, A.
2007.
Genius English Course@I,II, Reading.
Taishukan, Tokyo.227
