Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 397?408,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsParser Evaluation overLocal and Non-Local Deep Dependencies in a Large CorpusEmily M.
Bender?, Dan Flickinger?, Stephan Oepen?, Yi Zhang?
?Dept of Linguistics, University of Washington, ?CSLI, Stanford University?Dept of Informatics, Universitetet i Oslo, ?Dept of Computational Linguistics, Saarland Universityebender@uw.edu, danf@stanford.edu, oe@ifi.uio.no, yzhang@coli.uni-sb.deAbstractIn order to obtain a fine-grained evaluation ofparser accuracy over naturally occurring text,we study 100 examples each of ten reason-ably frequent linguistic phenomena, randomlyselected from a parsed version of the En-glish Wikipedia.
We construct a correspond-ing set of gold-standard target dependenciesfor these 1000 sentences, operationalize map-pings to these targets from seven state-of-the-art parsers, and evaluate the parsers againstthis data to measure their level of success inidentifying these dependencies.1 IntroductionThe terms ?deep?
and ?shallow?
are frequently usedto characterize or contrast different approaches toparsing.
Inevitably, such informal notions lack aclear definition, and there is little evidence of com-munity consensus on the relevant dimension(s) ofdepth, let alne agreement on applicable metrics.
Atits core, the implied dichotomy of approaches al-ludes to differences in the interpretation of the pars-ing task.
Its abstract goal, on the one hand, couldbe pre-processing of the linguistic signal, to enablesubsequent stages of analysis.
On the other hand, itcould be making explicit the (complete) contributionthat the grammatical form of the linguistic signalmakes to interpretation, working out who did whatto whom.
Stereotypically, one expects correspond-ing differences in the choice of interface representa-tions, ranging from various levels of syntactic anal-ysis to logical-form representations of semantics.In this paper, we seek to probe aspects of variationin automated linguistic analysis.
We make the as-sumption that an integral part of many (albeit not all)applications of parsing technology is the recovery ofstructural relations, i.e.
dependencies at the level ofinterpretation.
We suggest a selection of ten linguis-tic phenomena that we believe (a) occur with reason-ably high frequency in running text and (b) have thepotential to shed some light on the depths of linguis-tic analysis.
We quantify the frequency of these con-structions in the English Wikipedia, then annotate100 example sentences for each phenomenon withgold-standard dependencies reflecting core proper-ties of the phenomena of interest.
This gold standardis then used to estimate the recall of these dependen-cies by seven commonly used parsers, providing thebasis for a qualitative discussion of the state of theart in parsing for English.In this work, we answer the call by Rimell etal.
(2009) for ?construction-focused parser evalua-tion?, extending and complementing their work inseveral respects: (i) we investigate both local andnon-local dependencies which prove to be challeng-ing for many existing state-of-the-art parsers; (ii) weinvestigate a wider range of linguistic phenomena,each accompanied with an in-depth discussion ofrelevant properties; and (iii) we draw our data fromthe 50-million sentence English Wikipedia, whichis more varied and a thousand times larger than thevenerable WSJ corpus, to explore a more level andambitious playing field for parser comparison.2 BackgroundAll parsing systems embody knowledge about possi-ble and probable pairings of strings and correspond-ing linguistic structure.
Such linguistic and proba-bilistic knowledge can be hand-coded (e.g., as gram-mar rules) or automatically acquired from labeled or397unlabeled training data.
A related dimension of vari-ation is the type of representations manipulated bythe parser.
We briefly review some representativeexamples along these dimensions, as these help toposition the parsers we subsequently evaluate.12.1 Approaches to parsingSource of linguistic knowledge At one end of thisdimension, we find systems whose linguistic knowl-edge is encoded in hand-crafted rules and lexical en-tries; for English, the ParGram XLE system (Rie-zler et al, 2002) and DELPH-IN English ResourceGrammar (ERG; Flickinger (2000))?each reflect-ing decades of continuous development?achievebroad coverage of open-domain running text, for ex-ample.
At the other end of this dimension, we findfully unsupervised approaches (Clark, 2001; Kleinand Manning, 2004), where the primary source oflinguistic knowledge is co-occurrence patterns ofwords in unannotated text.
As Haghighi and Klein(2006) show, augmenting this knowledge with hand-crafted prototype ?seeds?
can bring strong improve-ments.
Somewhere between these poles, a broadclass of parsers take some or all of their linguisticknowledge from annotated treebanks, e.g.
the PennTreebank (PTB), which encodes ?surface grammati-cal analysis?
(Marcus et al, 1993).
Such approachesinclude those that directly (and exclusively) use theinformation in the treebank (e.g.
Charniak (1997),Collins (1999), Petrov et al (2006), inter alios) aswell as those that complement treebank structureswith some amount of hand-coded linguistic knowl-edge (e.g.
O?Donovan et al (2004), Miyao et al(2004), Hockenmaier and Steedman (2007), interalios).
Another hybrid in terms of its acquisition oflinguistic knowledge is the RASP system of Briscoeet al (2006), combining a hand-coded grammar overPoS tag sequences with a probabilistic tagger andstatistical syntactic disambiguation.Design of representations Approaches to parsingalso differ fundamentally in the style of represen-tation assigned to strings.
These vary both in their1Additional sources of variation among extant parsing tech-nologies include (a) the behavior with respect to ungrammaticalinputs and (b) the relationship between probabilistic and sym-bolic knowledge in the parser, where parsers with a hand-codedgrammar at their core typically also incorporate an automati-cally trained probabilistic disambiguation component.formal nature and the ?granularity?
of linguistic in-formation (i.e.
the number of distinctions assumed),encompassing variants of constituent structure, syn-tactic dependencies, or logical-form representationsof semantics.
Parser interface representations rangebetween the relatively simple (e.g.
phrase structuretrees with a limited vocabulary of node labels as inthe PTB, or syntactic dependency structures with alimited vocabulary of relation labels as in Johanssonand Nugues (2007)) and the relatively complex, asfor example elaborate syntactico-semantic analysesproduced by the ParGram or DELPH-IN grammars.There tends to be a correlation between themethodology used in the acquisition of linguisticknowledge and the complexity of representations: inthe creation of a mostly hand-crafted treebank likethe PTB, representations have to be simple enoughfor human annotators to reliably manipulate.
Deriv-ing more complex representations typically presup-poses further computational support, often involv-ing some hand-crafted linguistic knowledge?whichcan take the form of mappings from PTB-like repre-sentations to ?richer?
grammatical frameworks (asin the line of work by O?Donovan et al (2004), andothers; see above), or can be rules for creating theparse structures in the first place (i.e.
a computa-tional grammar), as for example in the treebanks ofvan der Beek et al (2002) or Oepen et al (2004).2In principle, one might expect that richer repre-sentations allow parsers to capture complex syntac-tic or semantic dependencies more explicitly.
At thesame time, such ?deeper?
relations may still be re-coverable (to some degree) from comparatively sim-ple parser outputs, as demonstrated for unboundeddependency extraction from strictly local syntacticdependency trees by Nivre et al (2010).2.2 An armada of parsersStanford Parser (Klein and Manning, 2003) is aprobabilistic parser which can produce both phrasestructure trees and grammatical relations (syntacticdependencies).
The parsing model we evaluate is the2A noteworthy exception to this correlation is the annotatedcorpus of Zettlemoyer and Collins (2005), which pairs sur-face strings from the realm of natural language database inter-faces directly with semantic representations in lambda calculus.These were hand-written on the basis of database query state-ments distributed with the original datasets.398English factored model which combines the prefer-ences of unlexicalized PCFG phrase structures andof lexical dependencies, trained on sections 02?21of the WSJ portion of the PTB.
We chose StanfordParser from among the state-of-the-art PTB-derivedparsers for its support for grammatical relations asan alternate interface representation.Charniak&Johnson Reranking Parser (Char-niak and Johnson, 2005) is a two-stage PCFG parserwith a lexicalized generative model for the first-stage, and a discriminative MaxEnt reranker for thesecond-stage.
The models we evaluate are alsotrained on sections 02?21 of the WSJ.
Top-50 read-ings were used for the reranking stage.
The outputconstituent trees were then converted into StanfordDependencies.
According to Cer et al (2010), thiscombination gives the best parsing accuracy in termsof Stanford dependencies on the PTB.Enju (Miyao et al, 2004) is a probabilistic HPSGparser, combining a hand-crafted core grammar withautomatically acquired lexical types from the PTB.3The model we evaluate is trained on the same ma-terial from the WSJ sections of the PTB, but thetreebank is first semi-automatically converted intoHPSG derivations, and the annotation is enrichedwith typed feature structures for each constituent.In addition to HPSG derivation trees, Enju also pro-duces predicate argument structures.C&C (Clark and Curran, 2007) is a statisticalCCG parser.
Abstractly similar to the approach ofEnju, the grammar and lexicon are automaticallyinduced from CCGBank (Hockenmaier and Steed-man, 2007), a largely automatic projection of (theWSJ portion of) PTB trees into the CCG framework.In addition to CCG derivations, the C&C parser candirectly output a variant of grammatical relations.RASP (Briscoe et al, 2006) is an unlexicalizedrobust parsing system, with a hand-crafted ?tag se-quence?
grammar at its core.
The parser thus anal-yses a lattice of PoS tags, building a parse forestfrom which the most probable syntactic trees andsets of corresponding grammatical relations can beextracted.
Unlike other parsers in our mix, RASPdid not build on PTB data in either its PoS tagging3This hand-crafted grammar is distinct from the ERG, de-spite sharing the general framework of HPSG.
The ERG is notincluded in our evaluation, since it was used in the extraction ofthe original examples and thus cannot be fairly evaluated.or syntactic disambiguation components.MSTParser (McDonald et al, 2005) is a data-driven dependency parser.
The parser uses an edge-factored model and searches for a maximal span-ning tree that connects all the words in a sentenceinto a dependency tree.
The model we evaluateis the second-order projective model trained on thesame WSJ corpus, where the original PTB phrasestructure annotations were first converted into de-pendencies, as established in the CoNLL shared task2009 (Johansson and Nugues, 2007).XLE/ParGram (Riezler et al, 2002, see alsoCahill et al, 2008) applies a hand-built LexicalFunctional Grammar for English and a stochasticparse selection model.
For our evaluation, we usedthe Nov 4, 2010 release of XLE and the Nov 25,2009 release of the ParGram English grammar, withc-structure pruning turned off and resource limita-tions set to the maximum possible to allow for ex-haustive search.
In particular, we are evaluating thef-structures output by the system.Each parser, of course, has its own requirementsregarding preprocessing of text, especially tokeniza-tion.
We customized the tokenization to each parser,by using the parser?s own internal tokenization orpre-tokenizing to match the parser?s desired input.The evaluation script is robust to variations in tok-enization across parsers.3 PhenomenaIn this section we summarize the ten phenomena weexplore and our motivations for choosing them.
Ourgoal was to find phenomena where the relevant de-pendencies are relatively subtle, such that more lin-guistic knowledge is beneficial in order to retrievethem.
Though this set is of course only a sampling,these phenomena illustrate the richness of structure,both local and non-local, involved in the mappingfrom English strings to their meanings.
We discussthe phenomena in four sets and then briefly reviewtheir representation in the Penn Treebank.3.1 Long distance dependenciesThree of our phenomena can be classified as involv-ing long-distance dependencies: finite that-less rel-atives clauses (?barerel?
), tough adjectives (?tough?
)and right node raising (?rnr?).
These are illustrated399in the following examples:4(1) barerel: This is the second time in a row Aus-tralia has lost their home tri-nations?
series.
(2) tough: Original copies are very hard to find.
(3) rnr: Ilu?vatar, as his names imply, exists beforeand independently of all else.While the majority of our phenomena involve lo-cal dependencies, we include these long-distancedependency types because they are challenging forparsers and enable more direct comparison with thework of Rimell et al (2009), who also address rightnode raising and bare relatives.
Our barerel categorycorresponds to their ?object reduced relative?
cate-gory with the difference that we also include adverbrelatives, where the head noun functions as a modi-fier within the relative clause, as does time in (1).
Incontrast, our rnr category is somewhat narrower thanRimell et al (2009)?s ?right node raising?
category:where they include raised modifiers, we restrict ourcategory to raised complements.Part of the difficulty in retrieving long-distancedependencies is that the so-called extraction site isnot overtly marked in the string.
In addition to thisbaseline level of complication, these three construc-tion types present further difficulties: Bare relatives,unlike other relative clauses, do not carry any lexi-cal cues to their presence (i.e., no relative pronouns).Tough adjective constructions require the presenceof specific lexical items which form a subset of alarger open class.
They are rendered more difficultby two sources of ambiguity: alternative subcatego-rization frames for the adjectives and the purposiveadjunct analysis (akin to in order to) for the infiniti-val VP.
Finally, right node raising often involves co-ordination where one of the conjuncts is in fact nota well-formed phrase (e.g., independently of in (3)),making it potentially difficult to construct the correctcoordination structure, let alne associate the raisedelement with the correct position in each conjunct.3.2 Non-dependenciesTwo of our phenomena crucially look for the lack ofdependencies.
These are it expletives (?itexpl?)
andverb-particle constructions (?vpart?
):4All examples are from our data.
Words involved in the rel-evant dependencies are highlighted in italics (dependents) andboldface (heads).
(4) itexpl: Crew negligence is blamed, and it is sug-gested that the flight crew were drunk.
(5) vpart: He once threw out two baserunners athome in the same inning.The English pronoun it can be used as an ordi-nary personal pronoun or as an expletive: a place-holder for when the language demands a subject (oroccasionally object) NP but there is no semantic rolefor that NP.
The expletive it only appears when itis licensed by a specific construction (such as ex-traposition, (4)) or selecting head.
If the goal ofparsing is to recover from the surface string the de-pendencies capturing who did what to whom, exple-tive it should not feature in any of those dependen-cies.
Likewise, instances of expletive it should bedetected and discarded in reference resolution.
Wehypothesize that detecting expletive it requires en-coding linguistic knowledge about its licensers.The other non-dependency we explore is betweenthe particle in verb-particle constructions and thedirect object.
Since English particles are almostalways homophonous with prepositions, when theobject of the verb-particle pair follows the par-ticle, there will always be a competing analysiswhich analyses the sequence as V+PP rather thanV+particle+NP.
Furthermore, since verb-particlepairs often have non-compositional semantics (Saget al, 2002), misanalyzing these constructions couldbe costly to downstream components.3.3 Phrasal modifiersOur next category concerns modifier phrases:(6) ned: Light colored glazes also have softeningeffects when painted over dark or bright images.
(7) absol: The format consisted of 12 games, eachteam facing the other teams twice.The first, (?ned?
), is a pattern which to our knowl-edge has not been named in the literature, where anoun takes the typically verbal -ed ending, is modi-fied by another noun or adjective, and functions as amodifier or a predicate.
We believe this phenomenonto be interesting because its unusual morphology islikely to lead PoS-taggers astray, and because theoften-hyphenated Adj+N-ed constituent has produc-tive internal structure constraining its interpretation.The second phrasal modifier we investigate is theabsolutive construction.
An absolutive consists of an400NP followed by a non-finite predicate (such as couldappear after the copula be).
The whole phrase mod-ifies a verbal projection that it attaches to.
Absolu-tives may be marked with with or unmarked.
Here,we focus on the unmarked type as this lack of lexicalcue can make the construction harder to detect.3.4 Subtle argumentsOur final three phenomena involve ways in whichverbal arguments can be more difficult to identifythan in ordinary finite clauses.
These include de-tecting the arguments of verbal gerunds (?vger?
), theinterleaving of arguments and adjuncts (?argadj?)
andraising/control (?control?)
constructions.
(8) vger: Accessing the website without the ?www?subdomain returned a copy of the main site for?EP.net?.
(9) argadj: The story shows, through flashbacks, thedifferent histories of the characters.
(10) control: Alfred ?retired?
in 1957 at age 60 butcontinued to paint full time.In a verbal gerund, the -ing form a verb retainsverbal properties (e.g., being able to take NP com-plements, rather than only PP complements) butheads a phrase that fills an NP position in the syn-tax (Malouf, 2000).
Since gerunds have the samemorphology as present participle VPs, their role inthe larger clause is susceptible to misanalysis.The argadj examples are of interest because En-glish typically prefers to have direct objects directlyadjacent to the selecting verb.
Nonetheless, phe-nomena such as parentheticals and heavy-NP shift(Arnold et al, 2000), in which ?heavy?
constituentsappear further to the right in the string, allow foradjunct-argument order in a minority of cases.
Wehypothesize that the relative infrequency of this con-struction will lead parsers to prefer incorrect analy-ses (wherein the adjunct is picked up as a comple-ment, the complement as an adjunct or the structurediffers entirely) unless they have access to linguis-tic knowledge providing constraints on possible andprobable complementation patterns for the head.Finally, we turn to raising and control verbs (?con-trol?)
(e.g., Huddleston and Pullum (2002, ch.
14)).These verbs select for an infinitival VP complementand stipulate that another of their arguments (sub-ject or direct object in the examples we explore) isidentified with the unrealized subject position of theinfinitival VP.
Here it is the dependency betweenthe infinitval VP and the NP argument of the ?up-stairs?
verb which we expect to be particularly sub-tle.
Getting this right requires specific lexical knowl-edge about which verbs take these complementationpatterns.
This lexical knowledge needs to be repre-sented in such a way that it can be used robustly evenin the case of passives, relative clauses, etc.53.5 Penn Treebank representationsWe investigated the representation of these 10 phe-nomena in the PTB (Marcus et al, 1993) in twosteps: First we explored the PTB?s annotation guide-lines (Bies et al, 1995) to determine how the rele-vant dependencies were intended to be represented.We then used Ghodke and Bird?s (2010) TreebankSearch to find examples of the intended annotationsas well as potential examples of the phenomena an-notated differently, to get a sense of the consistencyof the annotation from both precision and recall per-spectives.
In this study, we take the phrase structuretrees of the PTB to represent dependencies based onreasonable identification of heads.The barerel, vpart, and absol phenomena are com-pletely unproblematic, with their relevant dependen-cies explicitly and reliably represented.
In addition,the tough construction is reliably annotated, thoughone of the dependencies we take to be central is notdirectly represented: The missing argument is linkedto a null wh head at the left edge of the comple-ment of the tough predicate, rather than to its sub-ject.
Two further phenomena (rnr and vger) are es-sentially correctly represented: the representationsof the dependencies are explicit and mostly but notentirely consistently applied.
Two out of a sample of20 examples annotated as containing rnr did not, andtwo out of a sample of 35 non-rnr-annotated coordi-nations actually contained rnr.
For vger the primaryproblem is with the PoS tagging, where the gerundis sometimes given a nominal tag, contrary to PTBguidelines, though the structure above it conforms.The remaining four constructions are more prob-lematic.
In the case of object control, while the guide-5Distinguishing between raising and control requires fur-ther lexical knowledge and is another example of a ?non-dependency?
(in the raising examples).
We do not draw thatdistinction in our annotations.401lines specify an analysis in which the shared NP isattached as the object of the higher verb, the PTBincludes not only structures conforming to that anal-ysis but also ?small clause?
structures, with the latterobscuring the relationship of the shared argument tothe higher verb.
In the case of itexpl, the adjoined(S(-NONE- *EXP*)) indicating an expletive use ofit is applied consistently for extraposition (as pre-scribed in the guidelines).
However, the set of lex-ical licensers of the expletive is incomplete.
For ar-gadj we run into the problem that the PTB does notexplicitly distinguish between post-verbal modifiersand verbal complements in the way that they are at-tached.
The guidelines suggest that the function tags(e.g., PP-LOC, etc.)
should allow one to distinguishthese, but examination of the PTB itself suggeststhat they are not consistently applied.
Finally, thened construction is not mentioned in the PTB guide-lines nor is its internal structure represented in thetreebank.
Rather, strings like gritty-eyed are left un-segmented and tagged as JJ.We note that the PTB representations of many ofthese phenomena (barerel, tough, rnr, argadj, control,itexpl) involve empty elements and/or function tags.Systems that strip these out before training, as iscommon practice, will not benefit from the informa-tion that is in the PTB.Our purpose here is not to criticize the PTB,which has been a tremendously important resourceto the field.
Rather, we have two aims: The first isto provide context for the evaluation of PTB-derivedparsers on these phenomena.
The second is to high-light the difficulty of producing consistent annota-tions of any complexity as well as the hurdles facedby a hand-annotation approach when attempting toscale a resource to more complex representationsand/or additional phenomena (though cf.
Vadas andCurran (2008) on improving PTB representations).4 Methodology4.1 Data extractionWe processed 900 million tokens of Wikipedia textusing the October 2010 release of the ERG, follow-ing the work of the WikiWoods project (Flickingeret al, 2010).
Using the top-ranked ERG deriva-tion trees as annotations over this corpus and sim-ple patterns using names of ERG-specific construc-Phenomenon Frequency Candidatesbarerel 2.12% 546tough 0.07% 175rnr 0.69% 1263itexpl 0.13% 402vpart 4.07% 765ned 1.18% 349absol 0.51% 963vger 5.16% 679argadj 3.60% 1346control 3.78% 124Table 1: Relative frequencies of phenomena matches inWikipedia, and number of candidate strings vetted.tions or lexical types, we randomly selected a setof candidate sentences for each of our ten phenom-ena.
These candidates were then hand-vetted in se-quence by two annotators to identify, for each phe-nomenon, 100 examples that do in fact involve thephenomenon in question and which are both gram-matical and free of typos.
Examples that were ei-ther deemed overly basic (e.g.
plain V+V coordi-nation, which the ERG treats as rnr) or inappropri-ately complex (e.g.
non-constituent coordination ob-scuring the interleaving of arguments and adjuncts)were also discarded at this step.
Table 1 summarizesrelative frequencies of each phenomenon in about47 million parsed Wikipedia sentences, as well asthe total size of the candidate sets inspected.
Forthe control and tough phenomena hardly any filteringfor complexity was applied, hence these can serveas indicators of the rate of genuine false positives.For phenomena that partially overlap with those ofRimell et al (2009), it appears our frequency es-timates are comparable to what they report for theBrown Corpus (but not the WSJ portion of the PTB).4.2 Annotation formatWe annotated up to two dependency triples per phe-nomenon instance, identifying the heads and depen-dents by the surface form of the head words in thesentence suffixed with a number indicating word po-sition (see Table 2).6 Some strings contain morethan one instance of the phenomenon they illustrate;in these cases, multiple sets of dependencies are6As the parsers differ in tokenization strategies, our evalua-tion script treats these position IDs as approximate indicators.402Item ID Phenomenon Polarity Dependency1011079100200 absol 1 having-2|been-3|passed-4 ARG act-11011079100200 absol 1 withdrew-9 MOD having-2|been-3|passed-41011079100200 absol 1 carried+on-12 MOD having-2|been-3|passed-4Table 2: Sample annotations for sentence # 1011079100200: The-0 act-1 having-2 been-3 passed-4 in-5 that-6 year-7Jessop-8 withdrew-9 and-10 Whitworth-11 carried-12 on-13 with-14 the-15 assistance-16 of-17 his-18 son-19.Phenomenon Head Type Dependent DistanceBare relatives gapped predicate in relative ARG2/MOD modified noun 3.0 (8)(barerel) modified noun MOD top predicate of relative 3.3 (8)Tough adjectives tough adjective ARG2 to-VP complement 1.7 (5)(tough) gapped predicate in to-VP ARG2 subject/modifiee of adjective 6.4 (21)Right Node Raising verb/prep2 ARG2 shared noun 2.8 (9)(rnr) verb/prep1 ARG2 shared noun 6.1 (12)Expletive It it-subject taking verb !ARG1 it 1.2 (3)(itexpl) raising-to-object verb !ARG2 it ?Verb+particle constructions particle !ARG2 complement 2.7 (9)(vpart) verb+particle ARG2 complement 3.7 (10)Adj/Noun2 + Noun1-ed head noun MOD Noun1-ed 2.4 (17)(ned) Noun1-ed ARG1/MOD Adj/Noun2 1.0 (1.5)Absolutives absolutive predicate ARG1 subject of absolutive 1.7 (12)(absol) main clause predicate MOD absolutive predicate 9.8 (26)Verbal gerunds selecting head ARG[1,2] gerund 1.9 (13)(vger) gerund ARG2/MOD first complement/modifier of gerund 2.3 (8)Interleaved arg/adj selecting verb MOD interleaved adjunct 1.2 (7)(argadj) selecting verb ARG[2,3] displaced complement 5.9 (26)Control ?upstairs?
verb ARG[2,3] ?downstairs?
verb 2.4 (23)(control) ?downstairs?
verb ARG1 shared argument 4.8 (17)Table 3: Dependencies labeled for each phenomenon type, including average and maximum surface distances.recorded.
In addition, some strings evince more thanone of the phenomena we are studying.
However,we only annotate the dependencies associated withthe phenomenon the string was selected to repre-sent.
Finally, in examples with coordinated heads ordependents, we recorded separate dependencies foreach conjunct.
In total, we annotated 2127 depen-dency triples for the 1000 sentences, including 253negative dependencies (see below).
Table 3 outlinesthe dependencies annotated for each phenomenon.To allow for multiple plausible attachment sites,we give disjunctive values for heads or dependentsin several cases: (i) with auxiliaries, (ii) with com-plementizers (that or to, as in Table 2), (iii) in casesof measure or classifier nouns or partitives, (iv) withmulti-word proper names and (v) where there isgenuine attachment ambiguity for modifiers.
Asthese sets of targets are disjunctive, these conven-tions should have the effect of increasing measuredparser performance.
580 (27%) of the annotated de-pendencies had at least one disjunction.4.3 Annotation and reconciliation processThe entire data set was annotated independently bytwo annotators.
Both annotators were familiar withthe ERG, used to identify these sentences in theWikiWoods corpus, but the annotation was donewithout reference to the ERG parses.
Before begin-ning annotation on each phenomenon, we agreed onwhich dependencies to annotate.
We also communi-cated with each other about annotation conventionsas the need for each convention became clear.
Theannotation conventions address how to handle co-ordination, semantically empty auxiliaries, passivesand similar orthogonal phenomena.Once the entire data set was dual-annotated, wecompared annotations, identifying the followingsources of mismatch: typographical errors, incom-pletely specified annotation conventions, inconsis-tent application of conventions (101 items, droppingin frequency as the annotation proceeded), and gen-uine disagreement about what to annotate, either dif-ferent numbers of dependencies of interest identified403in an item (59 items) or conflicting elements in a de-pendency (54 items).7 Overall, our initial annotationpass led to agreement on 79% of the items, and ahigher per-dependency level of agreement.
Agree-ment could be expected to approach 90% with moreexperience in applying annotation conventions.We then reconciled the annotations, using thecomparison to address all sources of difference.
Inmost cases, we readily agreed which annotation wascorrect and which was in error.
In a few cases, wedecided that both annotations were plausible alter-natives (e.g., in terms of alternative attachment sitesfor modifiers) and so created a single merged anno-tation expressing the disjunction of both (cf.
?
4.2).5 EvaluationWith the test data consisting of 100 items for each ofour ten selected phenomena, we ran all seven pars-ing systems and recorded their dependency-styleoutputs for each sentence.
While these outputsare not directly comparable with each other, wewere able to associate our manually-annotated tar-get dependencies with parser-specific dependencies,by defining sets of phenomenon-specific regular ex-pressions for each parser.
In principle, we allow thismapping to be somewhat complex (and forgiving tonon-contentful variation), though we require that itwork deterministically and not involve specific lexi-cal information.
An example set is given in Fig.
2.
"absol" =>{?ARG1?
=> [?\(ncsubj \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+) _\)?,?\(ncmod _ \W*{W2}\W*_(\d+) \W*{W1}\W*_(\d+)\)?],?ARG?
=> [?\(ncsubj \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+) _\)?,?\(ncmod _ \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+)\)?],?MOD?
=> [?\(xmod _ \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+)\)?,?\(ncmod _ \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+)\)?,?\(cmod _ \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+)\)?
]}Figure 2: Regexp set to evaluate C&C for absol.These expressions fit the output that we got from theC&C parser, illustrated in Fig.
3 with a relevant por-tion of the dependencies produced for the examplein Table 2.
Here the C&C dependency (ncsubjpassed 4 Act 1 ) matches the first target in the7We do not count typographical errors or incompletely spec-ified conventions as failures of inter-annotator agreement.gold-standard (Table 2), but no matching C&C de-pendency is found for the other two targets.
(xmod _ Act_1 passed_4)(ncsubj passed_4 Act_1 _)(ncmod _ withdrew,_9 Jessop_8)(dobj year,_7 withdrew,_9)Figure 3: Excerpts of C&C output for item in Table 2.The regular expressions operate solely on the de-pendency labels and are not lexically-specific.
Theyare specific to each phenomenon, as we did not at-tempt to write a general dependency converter, butrather to discover what patterns of dependency rela-tions describe the phenomenon when it is correctlyidentified by each parser.
Thus, though we did nothold out a test set, we believe that they would gener-alize to additional gold standard material annotatedin the same way for the same phenomena.8In total, we wrote 364 regular expressions to han-dle the output of the seven parsers, allowing someleeway in the role labels used by a parser for anygiven target dependency.
The supplementary mate-rials for this paper include the test data, parser out-puts, target annotations, and evaluation script.Fig.
1 provides a visualization of the results of ourevaluation.
Each column of points represents onedependency type.
Dependency types for the samephenomenon are represented by adjacent columns.The order of the columns within a phenomenon fol-lows the order of the dependency descriptions inTable 3: For each pair, the dependency type withthe higher score for the majority of the parsers isshown first (to the left).
The phenomena them-selves are also arranged according to increasing (av-erage) difficulty.
itexpl only has one column, as weannotated just one dependency per instance here.
(The two descriptions in Table 3 reflect different,mutually-incompatible instance types.)
Since ex-pletive it should not be the semantic dependent ofany head, the targets are generalized for this phe-nomenon and the evaluation script counts as incor-8In the case of the XLE, our simplistic regular-expressionapproach to the interpretation of parser outputs calls for muchmore complex patterns than for the other parsers.
This is owedto the rich internal structure of LFG f-structures and highergranularity of linguistic analysis, where feature annotations onnodes as well as reentrancies need to be taken into account.Therefore, our current results for the XLE admit small amountsof both over- and under-counting.4040102030405060708090100vgervpartcontrolargadjbarerelrnr toughned itexplabsolenjuxlec&jc&cstanfordmstraspFigure 1: Individual dependency recall for seven parsers over ten phenomena.rect any dependency involving referential it.We observe fairly high recall of the dependenciesfor vpart and vger (with the exception of RASP), andhigh recall for both dependencies representing con-trol for five systems.
While Enju, Stanford, MST,and RASP all found between 70 and 85% of the de-pendency between the adjective and its complementin the tough construction, only Enju and XLE rep-resented the dependency between the subject of theadjective and the gap inside the adjective?s comple-ment.
For the remaining phenomena, each parserperformed markedly worse on one dependency type,compared to the other.
The only exceptions hereare XLE and C&C?s (and to a lesser extent, C&J?s)scores for barerel.
No system scored higher than33% on the harder of the two dependencies in rnrorabsol, and Stanford, MST, and RASP all scored be-low 25% on the harder dependency in barerel.
OnlyXLE scored higher than 10% on the second depen-dency for ned and higher than 50% for itexpl.6 DiscussionFrom the results in Fig.
1, it is clear that even the bestof these parsers fail to correctly identify a large num-ber of relevant dependencies associated with linguis-tic phenomena that occur with reasonable frequencyin the Wikipedia.
Each of the parsers attemptswith some success to analyze each of these phe-nomena, reinforcing the claim of relevance, but theyvary widely across phenomena.
For the two long-distance phenomena that overlap with those studiedin Rimell et al (2009), our results are comparable.9Our evaluation over Wikipedia examples thus showsthe same relative lack of success in recovering long-distance dependencies that they found for WSJ sen-tences.
The systems did better on relatively well-studied phenomena including control, vger, and vpart,but had less success with the rest, even though all buttwo of those remaining phenomena involve syntac-tically local dependencies (as indicated in Table 3).Successful identification of the dependencies inthese phenomena would, we hypothesize, benefitfrom richer (or deeper) linguistic information whenparsing, whether it is lexical (tough, control, itexpl,and vpart), or structural (rnr, absol, vger, argadj, andbarerel), or somewhere in between, as with ned.
Inthe case of treebank-trained parsers, for the informa-tion to be available, it must be consistently encodedin the treebank and attended to during training.
As9Other than Enju, which scores 16 points higher in the eval-uation of Rimell et al, our average scores for each parser acrossthe dependencies for these phenomena are within 12 points ofthose reported by Rimell et al (2009) and Nivre et al (2010).405noted in Sections 2.1 and 3.5, there is tension be-tween developing sufficiently complex representa-tions to capture linguistic phenomena and keepingan annotation scheme simple enough that it can bereliably produced by humans, in the case of hand-annotation.7 Related WorkThis paper builds on a growing body of work whichgoes beyond (un)labeled bracketing in parser evalua-tion, including Lin (1995), Carroll et al (1998), Ka-plan et al (2004), Rimell et al (2009), and Nivre etal.
(2010).
Most closely related are the latter two ofthe above, as we adopt their ?construction-focusedparser evaluation methodology?.There are several methodological differences be-tween our work and that of Rimell et al First, wedraw our evaluation data from a much larger andmore varied corpus.
Second, we automate the com-parison of parser output to the gold standard, and wedistribute the evaluation scripts along with the anno-tated corpus, enhancing replicability.
Third, whereRimell et al extract evaluation targets on the basisof PTB annotations, we make use of a linguisticallyprecise broad-coverage grammar to identify candi-date examples.
This allows us to include both localand non-local dependencies not represented or notreliably encoded in the PTB, enabling us to evalu-ate parser performance with more precision over awider range of linguistic phenomena.These methodological innovations bring two em-pirical results.
The first is qualitative: Where previ-ous work showed that overall Parseval numbers hidedifficulties with long-distance dependencies, our re-sults show that there are multiple kinds of reason-ably frequent local dependencies which are also dif-ficult for the current standard approaches to pars-ing.
The second is quantitative: Where Rimell etal.
found two phenomena which were virtually un-analyzed (recall below 10%) for one or two parserseach, we found eight phenomena which were vir-tually unanalyzed by at least one system, includ-ing two unanalyzed by five and one by six.
Everysystem had at least one virtually unanalyzed phe-nomenon.
Thus we have shown that the dependen-cies being missed by typical modern approaches toparsing are more varied and more numerous thanpreviously thought.8 ConclusionWe have presented a detailed construction-focusedevaluation of seven parsers over 10 phenomena,with 1000 examples drawn from English Wikipedia.Gauging recall of such ?deep?
dependencies, in ourview, can serve as a proxy for downstream pro-cessing involving semantic interpretation of parseroutputs.
Our annotations and automated evaluationscript are provided in the supplementary materials,for full replicability.
Our results demonstrate thatsignificant opportunities remain for parser improve-ment, and highlight specific challenges that remaininvisible in aggregate parser evaluation (e.g.
Parse-val or overall dependency accuracy).
These resultssuggest that further progress will depend on train-ing data that is both more extensive and more richlyannotated than what is typically used today (seeing,for example, that a large part of more detailed PTBannotation remains ignored in much parsing work).There are obvious reasons calling for diversity inapproaches to parsing and for different trade-offsin, for example, the granularity of linguistic analy-sis, average accuracy, cost of computation, or easeof adaptation.
Our proposal is not to substituteconstruction-focused evaluation on Wikipedia datafor widely used aggregate metrics and reference cor-pora, but rather to augment such best practices inthe spirit of Rimell et al (2009) and expand therange of phenomena considered in such evaluations.Across frameworks and traditions (and in principlelanguages), it is of vital importance to be able toevaluate the quality of parsing (and grammar induc-tion) algorithms in a maximally informative manner.AcknowledgmentsWe are grateful to Tracy King for her assistance insetting up the XLE system and to three anonymousreviewers for helpful comments.
The fourth authorthanks DFKI and the DFG funded Excellence Clus-ter on MMCI for their support of the work.
Datapreparation on the scale of Wikipedia was made pos-sible through access to large-scale HPC facilities,and we are grateful to the Scientific Computing staffat UiO and the Norwegian Metacenter for Computa-tional Science.406ReferencesJennifer E. Arnold, Thomas Wasow, Anthony Losongco,and Ryan Ginstrom.
2000.
Heaviness vs. newness:The effects of structural complexity and discourse sta-tus on constituent ordering.
Language, 76(1):28?55.Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-Intyre.
1995.
Bracketing guidelines for treebank IIstyle Penn treebank project.
Technical report, Univer-sity of Pennsylvania.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Proceed-ings of the COLING/ACL 2006 Interactive Presenta-tion Sessions, pages 77?80, Sydney, Australia.Aoife Cahill, John T. Maxwell III, Paul Meurer, Chris-tian Rohrer, and Victoria Rose?n.
2008.
Speedingup LFG parsing using c-structure pruning.
In Coling2008: Proceedings of the workshop on Grammar En-gineering Across Frameworks, pages 33?40, Manch-ester, England, August.
Coling 2008 Organizing Com-mittee.John Carroll, Ted Briscoe, and Antonio Sanfilippo.
1998.Parser evaluation: A survey and a new proposal.In Proceedings of the International Conference onLanguage Resources and Evaluation, pages 447?454,Granada.Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-sky, and Christopher D. Manning.
2010.
Parsing toStanford dependencies: Trade-offs between speed andaccuracy.
In 7th International Conference on Lan-guage Resources and Evaluation (LREC 2010), Malta.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics (ACL?05),pages 173?180, Ann Arbor, Michigan.Eugene Charniak.
1997.
Statistical parsing with acontext-free grammar and word statistics.
In Proceed-ings of the Fourteenth National Conference on Artifi-cial Intelligence, pages 598 ?
603, Providence, RI.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCG andlog-linear models.
Computational Linguistics,33(4):493?552.Alexander Clark.
2001.
Unsupervised inductionof stochastic context-free grammars using distribu-tional clustering.
In Proceedings of the 5th Confer-ence on Natural Language Learning, pages 105?112.Toulouse, France.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Dan Flickinger, Stephan Oepen, and Gisle Ytrest?l.2010.
WikiWoods.
Syntacto-semantic annotation forEnglish Wikipedia.
In Proceedings of the 6th Interna-tional Conference on Language Resources and Evalu-ation, Valletta, Malta.Dan Flickinger.
2000.
On building a more efficientgrammar by exploiting types.
Natural Language En-gineering, 6 (1) (Special Issue on Efficient Processingwith HPSG):15 ?
28.Sumukh Ghodke and Steven Bird.
2010.
Fast query forlarge treebanks.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Linguis-tics, pages 267?275, Los Angeles, California, June.Association for Computational Linguistics.Aria Haghighi and Dan Klein.
2006.
Prototype-drivengrammar induction.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand 44th Annual Meeting of the Association for Com-putational Linguistics, pages 881?888, Sydney, Aus-tralia, July.
Association for Computational Linguistics.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A Corpus of CCG Derivations and DependencyStructures Extracted from the Penn Treebank.
Com-putational Linguistics, 33(3):355?396.Rodney Huddleston and Geoffrey K. Pullum.
2002.
TheCambridge Grammar of the English Language.
Cam-bridge University Press, Cambridge, UK.Richard Johansson and Pierre Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InIn Proceedings of NODALIDA 2007, pages 105?112,Tartu, Estonia.Ron Kaplan, Stefan Riezler, Tracy H King, John TMaxwell III, Alex Vasserman, and Richard Crouch.2004.
Speed and accuracy in shallow and deepstochastic parsing.
In Susan Dumais, Daniel Marcu,and Salim Roukos, editors, HLT-NAACL 2004: MainProceedings, pages 97?104, Boston, Massachusetts,USA, May.
Association for Computational Linguis-tics.Dan Klein and Christopher D. Manning.
2003.
Fast exactinference with a factored model for natural languageparsing.
In Advances in Neural Information Process-ing Systems 15, pages 3?10, Cambridge, MA.
MITPress.Dan Klein and Christopher Manning.
2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
In Proceedings of the42nd Meeting of the Association for ComputationalLinguistics, pages 478?485, Barcelona, Spain.Dekang Lin.
1995.
A dependency-based method forevaluating broad-coverage parsers.
In Proceedings ofIJCAI-95, pages 1420?1425, Montreal, Canada.Robert Malouf.
2000.
Verbal gerunds as mixed cate-gories in HPSG.
In Robert Borsley, editor, The Nature407and Function of Syntactic Categories, pages 133?166.Academic Press, New York.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19:313?330.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic.
2005.
Non-Projective Dependency Pars-ing using Spanning Tree Algorithms.
In Proceedingsof the 2005 Conference on Empirical Methods in Natu-ral Language Processing, pages 523?530, Vancouver,Canada.Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii.2004.
Corpus-oriented grammar development for ac-quiring a Head-driven Phrase Structure Grammar fromthe Penn Treebank.
In Proceedings of the 1st Interna-tional Joint Conference on Natural Language Process-ing, pages 684?693, Hainan Island, China.Joakim Nivre, Laura Rimell, Ryan McDonald, and CarlosGo?mez Rodr??guez.
2010.
Evaluation of dependencyparsers on unbounded dependencies.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics, pages 833?841, Beijing, China.Ruth O?Donovan, Michael Burke, Aoife Cahill, JosefVan Genabith, and Andy Way.
2004.
Large-scale in-duction and evaluation of lexical resources from thepenn-ii treebank.
In Proceedings of the 42nd Meet-ing of the Association for Computational Linguistics,pages 367?374, Barcelona, Spain.Stephan Oepen, Daniel Flickinger, Kristina Toutanova,and Christopher D. Manning.
2004.
LinGO Red-woods.
A rich and dynamic treebank for HPSG.Journal of Research on Language and Computation,2(4):575 ?
596.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand 44th Annual Meeting of the Association for Com-putational Linguistics, pages 433?440, Sydney, Aus-tralia.Stefan Riezler, Tracy H. King, Ronald M. Kaplan,Richard Crouch, John T. Maxwell III, and Mark John-son.
2002.
Parsing the Wall Street Journal using aLexical-Functional Grammar and discriminative esti-mation techniques.
In Proceedings of the 40th Meet-ing of the Association for Computational Linguistics,Philadelphia, PA.Laura Rimell, Stephen Clark, and Mark Steedman.
2009.Unbounded dependency recovery for parser evalua-tion.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing,pages 813?821, Singapore.
Association for Computa-tional Linguistics.Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-take, and Dan Flickinger.
2002.
Multiword expres-sions.
A pain in the neck for NLP.
In Alexander Gel-bukh, editor, Computational Linguistics and Intelli-gent Text Processing, volume 2276 of Lecture Notes inComputer Science, pages 189?206.
Springer, Berlin,Germany.David Vadas and James R. Curran.
2008.
Parsing nounphrase structure with CCG.
In Proceedings of ACL-08: HLT, pages 335?343, Columbus, Ohio, June.
As-sociation for Computational Linguistics.L.
van der Beek, Gosse Bouma, Robert Malouf, and Gert-jan van Noord.
2002.
The Alpino Dependency Tree-bank.
In Mariet Theune, Anton Nijholt, and Hen-dri Hondorp, editors, Computational Linguistics in theNetherlands, Amsterdam, The Netherlands.
Rodopi.Luke Zettlemoyer and Michael Collins.
2005.
Learn-ing to map sentences to logical form: Structured clas-sification with probabilistic categorial grammars.
InProceedings of the Twenty-First Annual Conference onUncertainty in Artificial Intelligence, pages 658?666,Arlington, Virginia.
AUAI Press.408
