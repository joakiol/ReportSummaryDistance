Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 53?61,Sydney, July 2006. c?2006 Association for Computational LinguisticsPhrasetable Smoothing for Statistical Machine TranslationGeorge Foster and Roland Kuhn and Howard JohnsonNational Research Council CanadaOttawa, Ontario, Canadafirstname.lastname@nrc.gc.caAbstractWe discuss different strategies for smooth-ing the phrasetable in Statistical MT, andgive results over a range of translation set-tings.
We show that any type of smooth-ing is a better idea than the relative-frequency estimates that are often used.The best smoothing techniques yield con-sistent gains of approximately 1% (abso-lute) according to the BLEU metric.1 IntroductionSmoothing is an important technique in statisticalNLP, used to deal with perennial data sparsenessand empirical distributions that overfit the trainingcorpus.
Surprisingly, however, it is rarely men-tioned in statistical Machine Translation.
In par-ticular, state-of-the-art phrase-based SMT relieson a phrasetable?a large set of ngram pairs overthe source and target languages, along with theirtranslation probabilities.
This table, which maycontain tens of millions of entries, and phrases ofup to ten words or more, is an excellent candidatefor smoothing.
Yet very few publications describephrasetable smoothing techniques in detail.In this paper, we provide the first system-atic study of smoothing methods for phrase-basedSMT.
Although we introduce a few new ideas,most methods described here were devised by oth-ers; the main purpose of this paper is not to in-vent new methods, but to compare methods.
Inexperiments over many language pairs, we showthat smoothing yields small but consistent gains intranslation performance.
We feel that this paperonly scratches the surface: many other combina-tions of phrasetable smoothing techniques remainto be tested.We define a phrasetable as a set of sourcephrases (ngrams) s?
and their translations t?, alongwith associated translation probabilities p(s?|t?)
andp(t?|s?).
These conditional distributions are derivedfrom the joint frequencies c(s?, t?)
of source/targetphrase pairs observed in a word-aligned parallelcorpus.Traditionally, maximum-likelihood estimationfrom relative frequencies is used to obtain con-ditional probabilities (Koehn et al, 2003), eg,p(s?|t?)
= c(s?, t?)/?s?
c(s?, t?)
(since the estimationproblems for p(s?|t?)
and p(t?|s?)
are symmetrical,we will usually refer only to p(s?|t?)
for brevity).The most obvious example of the overfitting thiscauses can be seen in phrase pairs whose con-stituent phrases occur only once in the corpus.These are assigned conditional probabilities of 1,higher than the estimated probabilities of pairs forwhich much more evidence exists, in the typicalcase where the latter have constituents that co-occur occasionally with other phrases.
During de-coding, overlapping phrase pairs are in direct com-petition, so estimation biases such as this one infavour of infrequent pairs have the potential to sig-nificantly degrade translation quality.An excellent discussion of smoothing tech-niques developed for ngram language models(LMs) may be found in (Chen and Goodman,1998; Goodman, 2001).
Phrasetable smoothingdiffers from ngram LM smoothing in the follow-ing ways:?
Probabilities of individual unseen events arenot important.
Because the decoder onlyproposes phrase translations that are in thephrasetable (ie, that have non-zero count), itnever requires estimates for pairs s?, t?
having53c(s?, t?)
= 0.1 However, probability mass isreserved for the set of unseen translations,implying that probability mass is subtractedfrom the seen translations.?
There is no obvious lower-order distributionfor backoff.
One of the most important tech-niques in ngram LM smoothing is to com-bine estimates made using the previous n?
1words with those using only the previous n?iwords, for i = 2 .
.
.
n. This relies on thefact that closer words are more informative,which has no direct analog in phrasetablesmoothing.?
The predicted objects are word sequences(in another language).
This contrasts to LMsmoothing where they are single words, andare thus less amenable to decomposition forsmoothing purposes.We propose various ways of dealing with thesespecial features of the phrasetable smoothingproblem, and give evaluations of their perfor-mance within a phrase-based SMT system.The paper is structured as follows: section 2gives a brief description of our phrase-based SMTsystem; section 3 presents the smoothing tech-niques used; section 4 reviews previous work; sec-tion 5 gives experimental results; and section 6concludes and discusses future work.2 Phrase-based Statistical MTGiven a source sentence s, our phrase-based SMTsystem tries to find the target sentence t?
that isthe most likely translation of s. To make searchmore efficient, we use the Viterbi approximationand seek the most likely combination of t and itsalignment a with s, rather than just the most likelyt:t?
= argmaxtp(t|s) ?
argmaxt,ap(t,a|s),where a = (s?1, t?1, j1), ..., (s?K , t?K , jK); t?k are tar-get phrases such that t = t?1 .
.
.
t?K ; s?k are sourcephrases such that s = s?j1 .
.
.
s?jK ; and s?k is thetranslation of the kth target phrase t?k.1This is a first approximation; exceptions occur when dif-ferent phrasetables are used in parallel, and when rules areused to translate certain classes of entities.To model p(t,a|s), we use a standard loglinearapproach:p(t,a|s) ?
exp[?i?ifi(s, t,a)]where each fi(s, t,a) is a feature function, andweights ?i are set using Och?s algorithm (Och,2003) to maximize the system?s BLEU score (Pa-pineni et al, 2001) on a development corpus.
Thefeatures used in this study are: the length of t;a single-parameter distortion penalty on phrasereordering in a, as described in (Koehn et al,2003); phrase translation model probabilities; andtrigram language model probabilities log p(t), us-ing Kneser-Ney smoothing as implemented in theSRILM toolkit (Stolcke, 2002).Phrase translation model probabilities are fea-tures of the form:log p(s|t,a) ?K?k=1log p(s?k|t?k)ie, we assume that the phrases s?k specified by aare conditionally independent, and depend only ontheir aligned phrases t?k.
The ?forward?
phraseprobabilities p(t?|s?)
are not used as features, butonly as a filter on the set of possible translations:for each source phrase s?
that matches some ngramin s, only the 30 top-ranked translations t?
accord-ing to p(t?|s?)
are retained.To derive the joint counts c(s?, t?)
from whichp(s?|t?)
and p(t?|s?)
are estimated, we use the phraseinduction algorithm described in (Koehn et al,2003), with symmetrized word alignments gener-ated using IBM model 2 (Brown et al, 1993).3 Smoothing TechniquesSmoothing involves some recipe for modifyingconditional distributions away from pure relative-frequency estimates made from joint counts, in or-der to compensate for data sparsity.
In the spirit of((Hastie et al, 2001), figure 2.11, pg.
38) smooth-ing can be seen as a way of combining the relative-frequency estimate, which is a model with highcomplexity, high variance, and low bias, with an-other model with lower complexity, lower vari-ance, and high bias, in the hope of obtaining bet-ter performance on new data.
There are two mainingredients in all such recipes: some probabilitydistribution that is smoother than relative frequen-cies (ie, that has fewer parameters and is thus less54complex) and some technique for combining thatdistribution with relative frequency estimates.
Wewill now discuss both these choices: the distribu-tion for carrying out smoothing and the combina-tion technique.
In this discussion, we use p?
() todenote relative frequency distributions.Choice of Smoothing DistributionOne can distinguish between two approaches tosmoothing phrase tables.
Black-box techniques donot look inside phrases but instead treat them asatomic objects: that is, both the s?
and the t?
in theexpression p(s?|t?)
are treated as units about whichnothing is known except their counts.
In contrast,glass-box methods break phrases down into theircomponent words.The black-box approach, which is the sim-pler of the two, has received little attention inthe SMT literature.
An interesting aspect ofthis approach is that it allows one to implementphrasetable smoothing techniques that are analo-gous to LM smoothing techniques, by treating theproblem of estimating p(s?|t?)
as if it were the prob-lem of estimating a bigram conditional probabil-ity.
In this paper, we give experimental resultsfor phrasetable smoothing techniques analogousto Good-Turing, Fixed-Discount, Kneser-Ney, andModified Kneser-Ney LM smoothing.Glass-box methods for phrasetable smoothinghave been described by other authors: see sec-tion 3.3.
These authors decompose p(s?|t?)
into aset of lexical distributions p(s|t?)
by making inde-pendence assumptions about the words s in s?.
Theother possibility, which is similar in spirit to ngramLM lower-order estimates, is to combine estimatesmade by replacing words in t?
with wildcards, asproposed in section 3.4.Choice of Combination TechniqueAlthough we explored a variety of black-box andglass-box smoothing distributions, we only triedtwo combination techniques: linear interpolation,which we used for black-box smoothing, and log-linear interpolation, which we used for glass-boxsmoothing.For black-box smoothing, we could have used abackoff scheme or an interpolation scheme.
Back-off schemes have the form:p(s?|t?)
={ph(s?|t?
), c(s?, t?)
?
?pb(s?|t?
), elsewhere ph(s?|t?)
is a higher-order distribution,pb(s?|t?)
is a smooth backoff distribution, and ?
isa threshold above which counts are considered re-liable.
Typically, ?
= 1 and ph(s?|t?)
is version ofp?(s?|t?)
modified to reserve some probability massfor unseen events.Interpolation schemes have the general form:p(s?|t?)
= ?
(s?, t?)p?(s?|t?)
+ ?
(s?, t?)pb(s?|t?
), (1)where ?
and ?
are combining coefficients.
Asnoted in (Chen and Goodman, 1998), a keydifference between interpolation and backoff isthat the former approach uses information fromthe smoothing distribution to modify p?(s?|t?)
forhigher-frequency events, whereas the latter usesit only for low-frequency events (most often 0-frequency events).
Since for phrasetable smooth-ing, better prediction of unseen (zero-count)events has no direct impact?only seen events arerepresented in the phrasetable, and thus hypoth-esized during decoding?interpolation seemed amore suitable approach.For combining relative-frequency estimateswith glass-box smoothing distributions, we em-ployed loglinear interpolation.
This is the tradi-tional approach for glass-box smoothing (Koehnet al, 2003; Zens and Ney, 2004).
To illustrate thedifference between linear and loglinear interpola-tion, consider combining two Bernoulli distribu-tions p1(x) and p2(x) using each method:plinear(x) = ?p1(x) + (1?
?
)p2(x)ploglin(x) =p1(x)?p2(x)p1(x)?p2(x) + q1(x)?q2(x)where qi(x) = 1 ?
pi(x).
Setting p2(x) = 0.5to simulate uniform smoothing gives ploglin(x) =p1(x)?/(p1(x)?
+ q1(x)?).
This is actually lesssmooth than the original distribution p1(x): it pre-serves extreme values 0 and 1, and makes inter-mediate values more extreme.
On the other hand,plinear(x) = ?p1(x) + (1 ?
?
)/2, which has theopposite properties: it moderates extreme valuesand tends to preserve intermediate values.An advantage of loglinear interpolation is thatwe can tune loglinear weights so as to maximizethe true objective function, for instance BLEU; re-call that our translation model is itself loglinear,with weights set to minimize errors.
In fact, a lim-itation of the experiments described in this paperis that the loglinear weights for the glass-box tech-niques were optimized for BLEU using Och?s al-gorithm (Och, 2003), while the linear weights for55black-box techniques were set heuristically.
Ob-viously, this gives the glass-box techniques an ad-vantage when the different smoothing techniquesare compared using BLEU!
Implementing an al-gorithm for optimizing linear weights according toBLEU is high on our list of priorities.The preceding discussion implicitly assumes asingle set of counts c(s?, t?)
from which conditionaldistributions are derived.
But, as phrases of differ-ent lengths are likely to have different statisticalproperties, it might be worthwhile to break downthe global phrasetable into separate phrasetablesfor each value of |t?| for the purposes of smooth-ing.
Any similar strategy that does not split up{s?|c(s?, t?)
> 0} for any fixed t?
can be applied toany smoothing scheme.
This is another idea weare eager to try soon.We now describe the individual smoothingschemes we have implemented.
Four of themare black-box techniques: Good-Turing and threefixed-discount techniques (fixed-discount inter-polated with unigram distribution, Kneser-Neyfixed-discount, and modified Kneser-Ney fixed-discount).
Two of them are glass-box techniques:Zens-Ney ?noisy-or?
and Koehn-Och-Marcu IBMsmoothing.
Our experiments tested not only theseindividual schemes, but also some loglinear com-binations of a black-box technique with a glass-box technique.3.1 Good-TuringGood-Turing smoothing is a well-known tech-nique (Church and Gale, 1991) in which observedcounts c are modified according to the formula:cg = (c + 1)nc+1/nc (2)where cg is a modified count value used to replacec in subsequent relative-frequency estimates, andnc is the number of events having count c. Anintuitive motivation for this formula is that it ap-proximates relative-frequency estimates made bysuccessively leaving out each event in the corpus,and then averaging the results (Na?das, 1985).A practical difficulty in implementing Good-Turing smoothing is that the nc are noisy for largec.
For instance, there may be only one phrasepair that occurs exactly c = 347, 623 times in alarge corpus, and no pair that occurs c = 347, 624times, leading to cg(347, 623) = 0, clearly notwhat is intended.
Our solution to this problemis based on the technique described in (Churchand Gale, 1991).
We first take the log of the ob-served (c, nc) values, and then use a linear leastsquares fit to log nc as a function of log c. To en-sure that the result stays close to the reliable valuesof nc for large c, error terms are weighted by c, ie:c(log nc?
log n?c)2, where n?c are the fitted values.Our implementation pools all counts c(s?, t?)
to-gether to obtain n?c (we have not yet tried separatecounts based on length of t?
as discussed above).
Itfollows directly from (2) that the total count massassigned to unseen phrase pairs is cg(0)n0 = n1,which we approximate by n?1.
This mass is dis-tributed among contexts t?
in proportion to c(t?
),giving final estimates:p(s?|t?)
= cg(s?, t?
)?s cg(s?, t?)
+ p(t?
)n?1,where p(t?)
= c(t?)/?t?
c(t?
).3.2 Fixed-Discount MethodsFixed-discount methods subtract a fixed discountD from all non-zero counts, and distribute the re-sulting probability mass according to a smoothingdistribution (Kneser and Ney, 1995).
We use aninterpolated version of fixed-discount proposed by(Chen and Goodman, 1998) rather than the origi-nal backoff version.
For phrase pairs with non-zero counts, this distribution has the general form:p(s?|t?)
= c(s?, t?)?D?s?
c(s?, t?
)+ ?(t?)pb(s?|t?
), (3)where pb(s?|t?)
is the smoothing distribution.
Nor-malization constraints fix the value of ?(t?):?(t?)
= D n1+(?, t?
)/?s?c(s?, t?
),where n1+(?, t?)
is the number of phrases s?
forwhich c(s?, t?)
> 0.We experimented with two choices for thesmoothing distribution pb(s?|t?).
The first is a plainunigram p(s?
), and the second is the Kneser-Neylower-order distribution:pb(s?)
= n1+(s?, ?
)/?s?n1+(s?, ?
),ie, the proportion of unique target phrases that s?
isassociated with, where n1+(s?, ?)
is defined anal-ogously to n1+(?, t?).
Intuitively, the idea is thatsource phrases that co-occur with many different56target phrases are more likely to appear in newcontexts.For both unigram and Kneser-Ney smoothingdistributions, we used a discounting coefficient de-rived by (Ney et al, 1994) on the basis of a leave-one-out analysis: D = n1/(n1 + 2n2).
For theKneser-Ney smoothing distribution, we also testedthe ?Modified Kneser-Ney?
extension suggestedin (Chen and Goodman, 1998), in which specificcoefficients Dc are used for small count valuesc up to a maximum of three (ie D3 is used forc ?
3).
For c = 2 and c = 3, we used formu-las given in that paper.3.3 Lexical DecompositionThe two glass-box techniques that we consideredinvolve decomposing source phrases with inde-pendence assumptions.
The simplest approach as-sumes that all source words are conditionally in-dependent, so that:p(s?|t?)
=J??j=1p(sj|t?
)We implemented two variants for p(sj|t?)
thatare described in previous work.
(Zens and Ney,2004) describe a ?noisy-or?
combination:p(sj |t?)
= 1?
p(s?j |t?)?
1?I??i=1(1?
p(sj |ti))where s?j is the probability that sj is not in thetranslation of t?, and p(sj|ti) is a lexical proba-bility.
(Zens and Ney, 2004) obtain p(sj|ti) fromsmoothed relative-frequency estimates in a word-aligned corpus.
Our implementation simply usesIBM1 probabilities, which obviate further smooth-ing.The noisy-or combination stipulates that sjshould not appear in s?
if it is not the translationof any of the words in t?.
The complement of this,proposed in (Koehn et al, 2005), to say that sjshould appear in s?
if it is the translation of at leastone of the words in t?:p(sj|t?)
=?i?Ajp(sj |ti)/|Aj |where Aj is a set of likely alignment connectionsfor sj .
In our implementation of this method,we assumed that Aj = {1, .
.
.
, I?
}, ie the set ofall connections, and used IBM1 probabilities forp(s|t).3.4 Lower-Order CombinationsWe mentioned earlier that LM ngrams have anaturally-ordered sequence of smoothing distribu-tions, obtained by successively dropping the lastword in the context.
For phrasetable smoothing,because no word in t?
is a priori less informativethan any others, there is no exact parallel to thistechnique.
However, it is clear that estimates madeby replacing particular target (conditioning) wordswith wildcards will be smoother than the originalrelative frequencies.
A simple scheme for combin-ing them is just to average:p(s?|t?)
=?i=I?c?i (s?, t?)?s?
c?i (s?, t?
)/I?where:c?i (s?, t?)
=?tic(s?, t1 .
.
.
ti .
.
.
tI?
).One might also consider progressively replacingthe least informative remaining word in the targetphrase (using tf-idf or a similar measure).The same idea could be applied in reverse, byreplacing particular source (conditioned) wordswith wildcards.
We have not yet implementedthis new glass-box smoothing technique, but it hasconsiderable appeal.
The idea is similar in spirit toCollins?
backoff method for prepositional phraseattachment (Collins and Brooks, 1995).4 Related WorkAs mentioned previously, (Chen and Goodman,1998) give a comprehensive survey and evalua-tion of smoothing techniques for language mod-eling.
As also mentioned previously, there isrelatively little published work on smoothing forstatistical MT.
For the IBM models, alignmentprobabilities need to be smoothed for combina-tions of sentence lengths and positions not encoun-tered in training data (Garc?
?a-Varea et al, 1998).Moore (2004) has found that smoothing to cor-rect overestimated IBM1 lexical probabilities forrare words can improve word-alignment perfor-mance.
Langlais (2005) reports negative resultsfor synonym-based smoothing of IBM2 lexicalprobabilities prior to extracting phrases for phrase-based SMT.For phrase-based SMT, the use of smoothing toavoid zero probabilities during phrase induction isreported in (Marcu and Wong, 2002), but no de-tails are given.
As described above, (Zens and57Ney, 2004) and (Koehn et al, 2005) use two dif-ferent variants of glass-box smoothing (which theycall ?lexical smoothing?)
over the phrasetable, andcombine the resulting estimates with pure relative-frequency ones in a loglinear model.
Finally, (Cet-tollo et al, 2005) describes the use of Witten-Bellsmoothing (a black-box technique) for phrasetablecounts, but does not give a comparison to othermethods.
As Witten-Bell is reported by (Chen andGoodman, 1998) to be significantly worse thanKneser-Ney smoothing, we have not yet tested thismethod.5 ExperimentsWe carried out experiments in two different set-tings: broad-coverage ones across six Europeanlanguage pairs using selected smoothing tech-niques and relatively small training corpora; andChinese to English experiments using all im-plemented smoothing techniques and large train-ing corpora.
For the black-box techniques,the smoothed phrase table replaced the originalrelative-frequency (RF) phrase table.
For theglass-box techniques, a phrase table (either theoriginal RF phrase table or its replacement afterblack-box smoothing) was interpolated in loglin-ear fashion with the smoothing glass-box distribu-tion, with weights set to maximize BLEU on a de-velopment corpus.To estimate the significance of the results acrossdifferent methods, we used 1000-fold pairwisebootstrap resampling at the 95% confidence level.5.1 Broad-Coverage ExperimentsIn order to measure the benefit of phrasetablesmoothing for relatively small corpora, we usedthe data made available for the WMT06 sharedtask (WMT, 2006).
This exercise is conductedopenly with access to all needed resources andis thus ideal for benchmarking statistical phrase-based translation systems on a number of languagepairs.The WMT06 corpus is based on sentences ex-tracted from the proceedings of the European Par-liament.
Separate sentence-aligned parallel cor-pora of about 700,000 sentences (about 150MB)are provided for the three language pairs hav-ing one of French, Spanish and German with En-glish.
SRILM language models based on the samesource are also provided for each of the four lan-guages.
We used the provided 2000-sentence dev-sets for tuning loglinear parameters, and tested onthe 3064-sentence test sets.Results are shown in table 1 for relative-frequency (RF), Good-Turing (GT), Kneser-Neywith 1 (KN1) and 3 (KN3) discount coefficients;and loglinear combinations of both RF and KN3phrasetables with Zens-Ney-IBM1 (ZN-IBM1)smoothed phrasetables (these combinations aredenoted RF+ZN-IBM1 and KN3+ZN-IBM1).It is apparent from table 1 that any kind ofphrase table smoothing is better than using none;the minimum improvement is 0.45 BLEU, andthe difference between RF and all other meth-ods is statistically significant.
Also, Kneser-Ney smoothing gives a statistically significant im-provement over GT smoothing, with a minimumgain of 0.30 BLEU.
Using more discounting co-efficients does not appear to help.
Smoothingrelative frequencies with an additional Zens-Neyphrasetable gives about the same gain as Kneser-Ney smoothing on its own.
However, combiningKneser-Ney with Zens-Ney gives a clear gain overany other method (statistically significant for alllanguage pairs except en?es and en?de) demon-strating that these approaches are complementary.5.2 Chinese-English ExperimentsTo test the effects of smoothing with largercorpora, we ran a set of experiments forChinese-English translation using the corporadistributed for the NIST MT05 evaluation(www.nist.gov/speech/tests/mt).
These are sum-marized in table 2.
Due to the large size ofthe out-of-domain UN corpus, we trained onephrasetable on it, and another on all other parallelcorpora (smoothing was applied to both).
We alsoused a subset of the English Gigaword corpus toaugment the LM training material.corpus use sentencesnon-UN phrasetable1 + LM 3,164,180UN phrasetable2 + LM 4,979,345Gigaword LM 11,681,852multi-p3 dev 993eval-04 test 1788Table 2: Chinese-English CorporaTable 3 contains results for the Chinese-Englishexperiments, including fixed-discount with uni-gram smoothing (FDU), and Koehn-Och-Marcusmoothing with the IBM1 model (KOM-IBM1)58smoothing method fr ??
en es ??
en de ??
en en ??
fr en ??
es en ??
deRF 25.35 27.25 20.46 27.20 27.18 14.60GT 25.95 28.07 21.06 27.85 27.96 15.05KN1 26.83 28.66 21.36 28.62 28.71 15.42KN3 26.84 28.69 21.53 28.64 28.70 15.40RF+ZN-IBM1 26.84 28.63 21.32 28.84 28.45 15.44KN3+ZN-IBM1 27.25 29.30 21.77 29.00 28.86 15.49Table 1: Broad-coverage resultsas described in section 3.3.
As with thebroad-coverage experiments, all of the black-boxsmoothing techniques do significantly better thanthe RF baseline.
However, GT appears to workbetter in the large-corpus setting: it is statisticallyindistinguishable from KN3, and both these meth-ods are significantly better than all other fixed-discount variants, among which there is little dif-ference.Not surprisingly, the two glass-box methods,ZN-IBM1 and KOM-IBM1, do poorly when usedon their own.
However, in combination with an-other phrasetable, they yield the best results, ob-tained by RF+ZN-IBM1 and GT+KOM-IBM1,which are statistically indistinguishable.
In con-strast to the situation in the broad-coverage set-ting, these are not significantly better than thebest black-box method (GT) on its own, althoughRF+ZN-IBM1 is better than all other glass-boxcombinations.smoothing method BLEU scoreRF 29.85GT 30.66FDU 30.23KN1 30.29KN2 30.13KN3 30.54ZN-IBM1 29.55KOM-IBM1 28.09RF+ZN-IBM1 30.95RF+KOM-IBM1 30.10GT+ZN-IBM1 30.45GT+KOM-IBM1 30.81KN3+ZN-IBM1 30.66Table 3: Chinese-English ResultsA striking difference between the broad-coverage setting and the Chinese-English settingis that in the former it appears to be beneficialto apply KN3 smoothing to the phrasetable thatgets combined with the best glass-box phrasetable(ZN), whereas in the latter setting it does not.
Totest whether this was due to corpus size (as thebroad-coverage corpora are around 10% of thosefor Chinese-English), we calculated Chinese-English learning curves for the RF+ZN-IBM1 andKN3-ZN-IBM1 methods, shown in figure 1.
Theresults are somewhat inconclusive: although theKN3+ZN-IBM1 curve is perhaps slightly flatter,the most obvious characteristic is that this methodappears to be highly sensitive to the particular cor-pus sample used.0.250.2550.260.2650.270.2750.280.2850.290.2950.30  10  20  30  40  50  60  70  80BLEUproportion of corpusLearning curves for smoothing methodsRF+ZN-IBM1KN3+ZN-IBM1Figure 1: Learning curves for two glass-box com-binations.6 Conclusion and Future WorkWe tested different phrasetable smoothing tech-niques in two different translation settings: Eu-ropean language pairs with relatively small cor-pora, and Chinese to English translation with largecorpora.
The smoothing techniques fall into two59categories: black-box methods that work only onphrase-pair counts; and glass-box methods that de-compose phrase probabilities into lexical proba-bilities.
In our implementation, black-box tech-niques use linear interpolation to combine relativefrequency estimates with smoothing distributions,while glass-box techniques are combined in log-linear fashion with either relative-frequencies orblack-box estimates.All smoothing techniques tested gave statisti-cally significant gains over pure relative-frequencyestimates.
In the small-corpus setting, the besttechnique is a loglinear combination of Kneser-Ney count smoothing with Zens-Ney glass-boxsmoothing; this yields an average gain of 1.6BLEU points over relative frequencies.
In thelarge-corpus setting, the best technique is a log-linear combination of relative-frequency estimateswith Zens-Ney smoothing, with a gain of 1.1BLEU points.
Of the two glass-box smoothingmethods tested, Zens-Ney appears to have a slightadvantage over Koehn-Och-Marcu.
Of the black-box methods tested, Kneser-Ney is clearly bet-ter for small corpora, but is equivalent to Good-Turing for larger corpora.The paper describes several smoothing alterna-tives which we intend to test in future work:?
Linear versus loglinear combinations (in ourcurrent work, these coincide with the black-box versus glass-box distinction, making itimpossible to draw conclusions).?
Lower-order distributions as described in sec-tion 3.4.?
Separate count-smoothing bins based onphrase length.7 AcknowledgementsThe authors would like to thank their colleagueMichel Simard for stimulating discussions.
Thefirst author would like to thank all his colleaguesfor encouraging him to taste a delicacy that wasnew to him (shredded paper with maple syrup).This material is based upon work supported bythe Defense Advanced Research Projects Agency(DARPA) under Contract No.
HR0011-06-C-0023.
Any opinions, findings and conclusions orrecommendations expressed in this material arethose of the author(s) and do not necessarily re-flect the views of the Defense Advanced ResearchProjects Agency (DARPA).ReferencesPeter F. Brown, Stephen A. Della Pietra, VincentDella J. Pietra, and Robert L. Mercer.
1993.
Themathematics of Machine Translation: Parameter es-timation.
Computational Linguistics, 19(2):263?312, June.M.
Cettollo, M. Federico, N. Bertoldi, R. Cattoni, andB.
Chen.
2005.
A look inside the ITC-irst SMTsystem.
In Proceedings of MT Summit X, Phuket,Thailand, September.
International Association forMachine Translation.Stanley F. Chen and Joshua T. Goodman.
1998.
Anempirical study of smoothing techniques for lan-guage modeling.
Technical Report TR-10-98, Com-puter Science Group, Harvard University.K.
Church and W. Gale.
1991.
A comparison of theenhanced Good-Turing and deleted estimation meth-ods for estimating probabilities of English bigrams.Computer speech and language, 5(1):19?54.M.
Collins and J. Brooks.
1995.
Prepositional phraseattachment through a backed-off model.
In Proceed-ings of the 3rd ACL Workshop on Very Large Cor-pora (WVLC), Cambridge, Massachusetts.Ismael Garc?
?a-Varea, Francisco Casacuberta, and Her-mann Ney.
1998.
An iterative, DP-based search al-gorithm for statistical machine translation.
In Pro-ceedings of the 5th International Conference on Spo-ken Language Processing (ICSLP) 1998, volume 4,pages 1135?1138, Sydney, Australia, December.Joshua Goodman.
2001.
A bit of progress in languagemodeling.
Computer Speech and Language.Trevor Hastie, Robert Tibshirani, and Jerome Fried-man.
2001.
The Elements of Statistical Learning.Springer.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Pro-ceedings of the International Conference on Acous-tics, Speech, and Signal Processing (ICASSP) 1995,pages 181?184, Detroit, Michigan.
IEEE.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Ed-uard Hovy, editor, Proceedings of the Human Lan-guage Technology Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics, pages 127?133, Edmonton, Alberta,Canada, May.
NAACL.P.
Koehn, A. Axelrod, A.
B. Mayne, C. Callison-Burch,M.
Osborne, D. Talbot, and M. White.
2005.
Ed-inburgh system description for the 2005 NIST MTevaluation.
In Proceedings of Machine TranslationEvaluation Workshop.Philippe Langlais, Guihong Cao, and Fabrizio Gotti.2005.
RALI: SMT shared task system description.60In Proceedings of the 2nd ACL workshop on Build-ing and Using Parallel Texts, pages 137?140, Uni-versity of Michigan, Ann Arbor, June.Daniel Marcu and William Wong.
2002.
A phrase-based, joint probability model for statistical machinetranslation.
In Proceedings of the 2002 Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP), Philadelphia, PA.Robert C. Moore.
2004.
Improving IBM word-alignment model 1.
In Proceedings of the 42th An-nual Meeting of the Association for ComputationalLinguistics (ACL), Barcelona, July.Hermann Ney, Ute Essen, and Reinhard Kneser.1994.
On structuring probabilistic dependencies instochastic language modelling.
Computer Speechand Language, 10:1?38.Arthur Na?das.
1985.
On Turing?s formula forword probabilities.
IEEE Transactions on Acous-tics, Speech and Signal Processing (ASSP), ASSP-33(6):1415?1417, December.Franz Josef Och.
2003.
Minimum error rate trainingfor statistical machine translation.
In Proceedingsof the 41th Annual Meeting of the Association forComputational Linguistics (ACL), Sapporo, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: A method for automaticevaluation of Machine Translation.
Technical Re-port RC22176, IBM, September.Andreas Stolcke.
2002.
SRILM - an extensi-ble language modeling toolkit.
In Proceedings ofthe 7th International Conference on Spoken Lan-guage Processing (ICSLP) 2002, Denver, Colorado,September.WMT.
2006.
The NAACL Workshop on StatisticalMachine Translation (www.statmt.org/wmt06), NewYork, June.Richard Zens and Hermann Ney.
2004.
Improve-ments in phrase-based statistical machine transla-tion.
In Proceedings of Human Language Technol-ogy Conference / North American Chapter of theACL, Boston, May.61
