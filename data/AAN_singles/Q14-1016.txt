Discriminative Lexical Semantic Segmentation with Gaps:Running the MWE GamutNathan Schneider Emily Danchik Chris Dyer Noah A. SmithSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USA{nschneid,emilydan,cdyer,nasmith}@cs.cmu.eduAbstractWe present a novel representation, evaluationmeasure, and supervised models for the task ofidentifying the multiword expressions (MWEs)in a sentence, resulting in a lexical seman-tic segmentation.
Our approach generalizesa standard chunking representation to encodeMWEs containing gaps, thereby enabling effi-cient sequence tagging algorithms for feature-rich discriminative models.
Experiments on anew dataset of English web text offer the firstlinguistically-driven evaluation of MWE iden-tification with truly heterogeneous expressiontypes.
Our statistical sequence model greatlyoutperforms a lookup-based segmentation pro-cedure, achieving nearly 60% F1 for MWEidentification.1 IntroductionLanguage has a knack for defying expectations whenput under the microscope.
For example, there is thenotion?sometimes referred to as compositionality?that words will behave in predictable ways, with indi-vidual meanings that combine to form complex mean-ings according to general grammatical principles.
Yetlanguage is awash with examples to the contrary:in particular, idiomatic expressions such as awashwith NP, have a knack for VP-ing, to the contrary, anddefy expectations.
Thanks to processes like metaphorand grammaticalization, these are (to various degrees)semantically opaque, structurally fossilized, and/orstatistically idiosyncratic.
In other words, idiomaticexpressions may be exceptional in form, function,or distribution.
They are so diverse, so unruly, so1.
MW named entities: Prime Minister Tony Blair2.
MW compounds: hot air balloon, skinny dip3.
conventionally SW compounds: somewhere4.
verb-particle: pick up, dry out, take over, cut short5.
verb-preposition: refer to, depend on, look for6.
verb-noun(-preposition): pay attention (to)7. support verb: make decisions, take pictures8.
other phrasal verb: put up with, get rid of9.
PP modifier: above board, at all, from time to time10.
coordinated phrase: cut and dry, more or less11.
connective: as well as, let alone, in spite of12.
semi-fixed VP: pick up where <one> left off13.
fixed phrase: scared to death, leave of absence14.
phatic: You?re welcome.
Me neither!15.
proverb: Beggars can?t be choosers.Figure 1: Some of the classes of idioms in English.The examples included here contain multiple lexicalizedwords?with the exception of those in (3), if the conven-tional single-word (SW) spelling is used.difficult to circumscribe, that entire theories of syn-tax are predicated on the notion that constructionswith idiosyncratic form-meaning mappings (Fillmoreet al., 1988; Goldberg, 1995) or statistical properties(Goldberg, 2006) offer crucial evidence about thegrammatical organization of language.Here we focus on multiword expressions(MWEs): lexicalized combinations of two or morewords that are exceptional enough to be consideredas single units in the lexicon.
As figure 1 illus-trates, MWEs occupy diverse syntactic and semanticfunctions.
Within MWEs, we distinguish (a) propernames and (b) lexical idioms.
The latter have provedthemselves a ?pain in the neck for NLP?
(Sag et al.,2002).
Automatic and efficient detection of MWEs,though far from solved, would have diverse appli-193Transactions of the Association for Computational Linguistics, 2 (2014) 193?206.
Action Editor: Joakim Nivre.Submitted 12/2013; Revised 1/2014; Published 4/2014.
c?2014 Association for Computational Linguistics.cations including machine translation (Carpuat andDiab, 2010), information retrieval (Newman et al.,2012), opinion mining (Berend, 2011), and secondlanguage learning (Ellis et al., 2008).It is difficult to establish any comprehensive tax-onomy of multiword idioms, let alone develop lin-guistic criteria and corpus resources that cut acrossthese types.
Consequently, the voluminous litera-ture on MWEs in computational linguistics?see ?7,Baldwin and Kim (2010), and Ramisch (2012) forsurveys?has been fragmented, looking (for exam-ple) at subclasses of phrasal verbs or nominal com-pounds in isolation.
To the extent that MWEs havebeen annotated in existing corpora, it has usuallybeen as a secondary aspect of some other scheme.Traditionally, such resources have prioritized certainkinds of MWEs to the exclusion of others, so theyare not appropriate for evaluating general-purposeidentification systems.In this article, we briefly review a shallow formof analysis for MWEs that is neutral to expressiontype, and that facilitates free text annotation with-out requiring a prespecified MWE lexicon (?2).
Thescheme applies to gappy (discontinuous) as well ascontiguous expressions, and allows for a qualitativedistinction of association strengths.
In Schneideret al.
(2014) we have applied this scheme to fully an-notate a 55,000-word corpus of English web reviews(Bies et al., 2012a), a conversational genre in whichcolloquial idioms are highly salient.
This article?smain contribution is to show that the representation?constrained according to linguistically motivated as-sumptions (?3)?can be transformed into a sequencetagging scheme that resembles standard approachesin named entity recognition and other text chunkingtasks (?4).
Along these lines, we develop a discrim-inative, structured model of MWEs in context (?5)and train, evaluate, and examine it on the annotatedcorpus (?6).
Finally, in ?7 and ?8 we comment onrelated work and future directions.2 Annotated CorpusTo build and evaluate a multiword expression ana-lyzer, we use the MWE-annotated corpus of Schnei-der et al.
(2014).
It consists of informal English webtext that has been specifically and completely anno-tated for MWEs, without reference to any particularlexicon.
To the best of our knowledge, this corpusis the first to be freely annotated for many kinds ofMWEs (without reference to a lexicon), and is alsothe first dataset of social media text with MWE an-notations beyond named entities.
This section givesa synopsis of the annotation conventions used to de-velop that resource, as they are important to under-standing our models and evaluation.Rationale.
The multiword expressions communityhas lacked a canonical corpus resource comparableto benchmark datasets used for problems such asNER and parsing.
Consequently, the MWE litera-ture has been driven by lexicography: typically, thegoal is to acquire an MWE lexicon with little or nosupervision, or to apply such a lexicon to corpusdata.
Studies of MWEs in context have focused onvarious subclasses of constructions in isolation, ne-cessitating special-purpose datasets and evaluationschemes.
By contrast, Schneider et al.
?s (2014) cor-pus creates an opportunity to tackle general-purposeMWE identification, such as would be desirable foruse by high-coverage downstream NLP systems.
It isused to train and evaluate our models below.
The cor-pus is publicly available as a benchmark for furtherresearch.1Data.
The documents in the corpus are online userreviews of restaurants, medical providers, retailers,automotive services, pet care services, etc.
Markedby conversational and opinionated language, thisgenre is fertile ground for colloquial idioms (Nunberget al., 1994; Moon, 1998).
The 723 reviews (55,000words, 3,800 sentences) in the English Web Tree-bank (WTB; Bies et al., 2012b) were collected byGoogle, tokenized, and annotated with phrase struc-ture trees in the style of the Penn Treebank (Marcuset al., 1993).
MWE annotators used the sentence andword tokenizations supplied by the treebank.2Annotation scheme.
The annotation scheme itselfwas designed to be as simple as possible.
It consistsof grouping together the tokens in each sentence thatbelong to the same MWE instance.
While annotationguidelines provide examples of MWE groupings ina wide range of constructions, the annotator is not1http://www.ark.cs.cmu.edu/LexSem/2Because we use treebank data, syntactic parses are availableto assist in post hoc analysis.
Syntactic information was notshown to annotators.194# of constituent tokens2 3 ?4 totalstrong 2257 595 172 3024weak 269 121 69 4592526 716 241 3483# of gaps0 1 22626 394 4322 135 22948 529 6Table 1: Counts in the MWE corpus.tied to any particular taxonomy or syntactic structure.This simplifies the number of decisions that have tobe made for each sentence, even if some are difficult.Further instructions to annotators included:?
Groups should include only the lexically fixed partsof an expression (modulo inflectional morphology);this generally excludes determiners and pronouns:made the mistake, pride themselves on.?
Multiword proper names count as MWEs.?
Misspelled or unconventionally spelled tokens areinterpreted according to the intended word if clear.?
Overtokenized words (spelled as two tokens, butconventionally one word) are joined as multiwords.Clitics separated by the tokenization in the corpus?negative n?t, possessive ?s, etc.
?are joined if func-tioning as a fixed part of a multiword (e.g., T ?sCafe), but not if used productively.Gaps.
There are, broadly speaking, three reasonsto group together tokens that are not fully contigu-ous.
Most commonly, gaps contain internal modifiers,such as good in make good decisions.
Syntactic con-structions such as the passive can result in gaps thatmight not otherwise be present: in good decisionswere made, there is instead a gap filled by the pas-sive auxiliary.
Finally, some MWEs may take internalarguments: they gave me a break.
Figure 1 has addi-tional examples.
Multiple gaps can occur even withinthe same expression, though it is rare: they agreed togive Bob a well-deserved break.Strength.
The annotation scheme has two?strength?
levels for MWEs.
Clearly idiomatic ex-pressions are marked as strong MWEs, while mostlycompositional but especially frequent collocations/phrases (e.g., abundantly clear and patently obvious)are marked as weak MWEs.
Weak multiword groupsare allowed to include strong MWEs as constituents(but not vice versa).
Strong groups are required tocohere when used inside weak groups: that is, a weakgroup cannot include only part of a strong group.For purposes of annotation, there were no constraintshinging on the ordering of tokens in the sentence.Process.
MWE annotation proceeded one sentenceat a time.
The 6 annotators referred to and improvedthe guidelines document on an ongoing basis.
Everysentence was seen independently by at least 2 an-notators, and differences of opinion were discussedand resolved (often by marking a weak MWE as acompromise).
See Schneider et al.
(2014) for details.Statistics.
The annotated corpus consists of 723documents (3,812 sentences).
MWEs are frequentin this domain: 57% of sentences (72% of sentencesover 10 words long) and 88% of documents containat least one MWE.
8,060/55,579=15% of tokensbelong to an MWE; in total, there are 3,483 MWEinstances.
544 (16%) are strong MWEs containing agold-tagged proper noun?most are proper names.
Abreakdown appears in table 1.3 Representation and Task DefinitionWe define a lexical segmentation of a sentence as apartitioning of its tokens into segments such that eachsegment represents a single unit of lexical meaning.A multiword lexical expression may contain gaps,i.e.
interruptions by other segments.
We impose tworestrictions on gaps that appear to be well-motivatedlinguistically:?
Projectivity: Every expression filling a gap mustbe completely contained within that gap; gappyexpressions may not interleave.?
No nested gaps: A gap in an expression may befilled by other single- or multiword expressions, solong as those do not themselves contain gaps.Formal grammar.
Our scheme corresponds to thefollowing extended CFG (Thatcher, 1967), where Sis the full sentence and terminals w are word tokens:S ?
X+X ?
w+ (Y+ w+)?Y ?
w+Each expression X or Y is lexicalized by the words inone or more underlined variables on the right-handside.
An X constituent may optionally contain one ormore gaps filled by Y constituents, which must notcontain gaps themselves.33MWEs with multiple gaps are rare but attested in data: e.g.,putting me at my ease.
We encountered one violation of the gapnesting constraint in the reviews data: I have21 nothing21 but21fantastic things2 to21 say21 .
Additionally, the interrupted phrase195Denoting multiword groupings with subscripts, Mywife had taken1 her ?072 Ford2 Fusion2 in1 for aroutine oil3 change3 contains 3 multiword groups?
{taken, in}, {?07, Ford, Fusion}, {oil, change}?and7 single-word groups.
The first MWE is gappy (ac-centuated by the box); a single word and a contiguousmultiword group fall within the gap.
The projectivityconstraint forbids an analysis like taken1 her ?072Ford1 Fusion2, while the gap nesting constraint for-bids taken1 her2 ?07 Ford2 Fusion2 in1.3.1 Two-level Scheme: Strong vs. Weak MWEsOur annotated data distinguish two strengths ofMWEs as discussed in ?2.
Augmenting the gram-mar of the previous section, we therefore designatenonterminals as strong (X , Y ) or weak (X?
, Y?
):S ?
X?+X?
?
X+ (Y?+ X+)?X ?
w+ (Y?+ w+)?Y?
?
Y+Y ?
w+A weak MWE may be lexicalized by single wordsand/or strong multiwords.
Strong multiwords cannotcontain weak multiwords except in gaps.
Further, thecontents of a gap cannot be part of any multiwordthat extends outside the gap.4For example, consider the segmentation: he waswilling to budge1 a2 little2 on1 the price whichmeans4 a43 lot43 to4 me4.
Subscripts denote strongMW groups and superscripts weak MW groups; un-marked tokens serve as single-word expressions.
TheMW groups are thus {budge, on}, {a, little}, {a, lot},and {means, {a, lot}, to, me}.
As should be evidentfrom the grammar, the projectivity and gap-nestingconstraints apply here just as in the 1-level scheme.3.2 EvaluationMatching criteria.
Given that most tokens do notbelong to an MWE, to evaluate MWE identificationwe adopt a precision/recall-based measure from thecoreference resolution literature.
The MUC criterion(Vilain et al., 1995) measures precision and recallgreat gateways never1 before1 , so23 far23 as23 Hudson knew2 ,seen1 by Europeans was annotated in another corpus.4This was violated 6 times in our annotated data: modifierswithin gaps are sometimes collocated with the gappy expression,as in on12 a12 tight1 budget12 and have12 little1 doubt12.of links in terms of groups (units) implied by thetransitive closure over those links.5 It can be definedas follows:Let a ?
b denote a link between two elementsin the gold standard, and a?
?b denote a link in thesystem prediction.
Let the ?
operator denote the tran-sitive closure over all links, such that ?a??b?
is 1 ifa and b belong to the same (gold) set, and 0 other-wise.
Assuming there are no redundant6 links withinany annotation (which in our case is guaranteed bylinking consecutive words in each MWE), we canwrite the MUC precision and recall measures as:P = ?a,b?a?
?b ?a??b??a,b?a?
?b 1 R =?a,b?a?b ?a???b?
?a,b?a?b 1This awards partial credit when predicted and goldexpressions overlap in part.
Requiring full MWEs tomatch exactly would arguably be too stringent, over-penalizing larger MWEs for minor disagreements.We combine precision and recall using the standardF1 measure of their harmonic mean.
This is the link-based evaluation used for most of our experiments.For comparison, we also report some results witha more stringent exact match evaluation where thespan of the predicted MWE must be identical to thespan of the gold MWE for it to count as correct.Strength averaging.
Recall that the 2-levelscheme (?3.1) distinguishes strong vs. weak links/groups, where the latter category applies to reason-ably compositional collocations as well as ambigu-ous or difficult cases.
If where one annotation usesa weak link the other has a strong link or no link atall, we want to penalize the disagreement less thanif one had a strong link and the other had no link.To accommodate the 2-level scheme, we thereforeaverage F?1 , in which all weak links have been con-verted to strong links, and F?1 , in which they havebeen removed: F1 = 12(F?1 +F?1 ).7 If neither annota-tion contains any weak links, this equals the MUC5As a criterion for coreference resolution, the MUC measurehas perceived shortcomings which have prompted several othermeasures (see Recasens and Hovy, 2011 for a review).
It is notclear, however, whether any of these criticisms are relevant toMWE identification.6A link between a and b is redundant if the other links alreadyimply that a and b belong to the same set.
A set of N elements isexpressed non-redundantly with exactly N ?1 links.7Overall precision and recall are likewise computed by aver-aging ?strengthened?
and ?weakened?
measurements.196no gaps,1-levelheOwasOwillingOtoObudgeOaBlittleIonOtheOpriceOwhichOmeansBaIlotItoImeI.O(O?BI+)+no gaps,2-levelheOwasOwillingOtoObudgeOaBlittleI?onOtheOpriceOwhichOmeansBaI?lotI?toI?meI?.O(O?B[I?I?]+)+gappy,1-levelheOwasOwillingOtoObudgeBablittleionItheOpriceOwhichOmeansBaIlotItoImeI.O(O?B(o?bi+?I)?I+)+gappy,2-levelheOwasOwillingOtoObudgeBablittle??onI?theOpriceOwhichOmeansBaI?lotI?toI?meI?.O(O?B(o?b[????]+?[I?I?])?[I?I?
]+)+Figure 2: Examples and regular expressions for the 4 tagging schemes.
Strong links are depicted with solid arcs, andweak links with dotted arcs.
The bottom analysis was provided by an annotator; the ones above are simplifications.score because F1 = F?1 = F?1 .
This method appliesto both the link-based and exact match evaluationcriteria.4 Tagging SchemesFollowing (Ramshaw and Marcus, 1995), shallow an-alysis is often modeled as a sequence-chunking task,with tags containing chunk-positional information.The BIO scheme and variants (e.g., BILOU; Ratinovand Roth, 2009) are standard for tasks like namedentity recognition, supersense tagging, and shallowparsing.The language of derivations licensed by the gram-mars in ?3 allows for a tag-based encoding of MWEanalyses with only bigram constraints.
We describe4 tagging schemes for MWE identification, startingwith BIO and working up to more expressive variants.They are depicted in figure 2.No gaps, 1-level (3 tags).
This is the standard con-tiguous chunking representation from Ramshaw andMarcus (1995) using the tags {O B I}.
O is for to-kens outside any chunk; B marks tokens beginninga chunk; and I marks other tokens inside a chunk.Multiword chunks will thus start with B and then I.B must always be followed by I; I is not allowed atthe beginning of the sentence or following O.No gaps, 2-level (4 tags).
We can distinguishstrength levels by splitting I into two tags: I?
forstrong expressions and I?
for weak expressions.
Toexpress strong and weak contiguous chunks requires4 tags: {O B I?
I?}.
(Marking B with a strength as wellwould be redundant because MWEs are never length-one chunks.)
The constraints on I?
and I?
are the sameas the constraints on I in previous schemes.
If I?
andI?
occur next to each other, the strong attachment willreceive higher precedence, resulting in analysis ofstrong MWEs as nested within weak MWEs.Gappy, 1-level (6 tags).
Because gaps cannotthemselves contain gappy expressions (we do notsupport full recursivity), a finite number of additionaltags are sufficient to encode gappy chunks.
We there-fore add lowercase tag variants representing tokenswithin a gap: {O o B b I i}.
In addition to the con-straints stated above, no within-gap tag may occur atthe beginning or end of the sentence or immediatelyfollowing or preceding O.
Within a gap, b, i, and obehave like their out-of-gap counterparts.Gappy, 2-level (8 tags).
8 tags are required to en-code the 2-level scheme with gaps: {O o B b I?
??
I?
??
}.Variants of the inside tag are marked for strength ofthe incoming link?this applies gap-externally (capi-talized tags) and gap-internally (lowercase tags).
If I?or I?
immediately follows a gap, its diacritic reflectsthe strength of the gappy expression, not the gap?scontents.5 ModelWith the above representations we model MWE iden-tification as sequence tagging, one of the paradigmsthat has been used previously for identifying con-tiguous MWEs (Constant and Sigogne, 2011, see?7).8 Constraints on legal tag bigrams are sufficientto ensure the full tagging is well-formed subject tothe regular expressions in figure 2; we enforce these8Hierarchical modeling based on our representations is leftto future work.197constraints in our experiments.9In NLP, conditional random fields (Lafferty et al.,2001) and the structured perceptron (Collins, 2002)are popular techniques for discriminative sequencemodeling with a convex loss function.
We choosethe second approach for its speed: learning and in-ference depend mainly on the runtime of the Viterbialgorithm, whose asymptotic complexity is linear inthe length of the input and (with a first-order Markovassumption) quadratic in the number of tags.
Below,we review the structured perceptron and discuss ourcost function, features, and experimental setup.5.1 Cost-Augmented Structured PerceptronThe structured perceptron?s (Collins, 2002) learn-ing procedure, algorithm 1, generalizes the classicperceptron algorithm (Freund and Schapire, 1999) toincorporate a structured decoding step (for sequences,the Viterbi algorithm) in the inner loop.
Thus, train-ing requires only max inference, which is fast with afirst-order Markov assumption.
In training, featuresare adjusted where a tagging error is made; the pro-cedure can be viewed as optimizing the structuredhinge loss.
The output of learning is a weight vectorthat parametrizes a feature-rich scoring function overcandidate labelings of a sequence.To better align the learning algorithm with ourF-score?based MWE evaluation (?3.2), we use acost-augmented version of the structured perceptronthat is sensitive to different kinds of errors duringtraining.
When recall is the bigger obstacle, we canadopt the following cost function: given a sentencex, its gold labeling y?, and a candidate labeling y?,cost(y?,y?,x) = ?y??
?j=1c(y?j ,y?j) wherec(y?,y?)
= ?y?
?
y??+??y?
?
{B,b}?y?
?
{O,o}?A single nonnegative hyperparameter, ?
, controlsthe tradeoff between recall and accuracy; higher ?biases the model in favor of recall (possibly hurt-ing accuracy and precision).
This is a slight variantof the recall-oriented cost function of Mohit et al.(2012).
The difference is that we only penalizebeginning-of-expression recall errors.
Preliminary9The 8-tag scheme licenses 42 tag bigrams: sequences suchas B O and o ??
are prohibited.
There are also constraints on theallowed tags at the beginning and end of the sequence.Input: data ??x(n),y(n)?
?Nn=1; number of iterations Mw?
0w?
0t ?
1for m = 1 to M dofor n = 1 to N do?x,y??
?x(n),y(n)?y??
argmaxy?
(w?g(x,y?
)+cost(y,y?,x))if y?
?
y thenw?w+g(x,y)?g(x, y?
)w?w+ tg(x,y)?
tg(x, y?
)endt ?
t +1endendOutput: w?
(w/t)Algorithm 1: Training with the averaged perceptron.
(Adapted from Daum?, 2006, p.
19.
)experiments showed that a cost function penalizingall recall errors?i.e., with ??y?
?
O?y?
= O?
as thesecond term, as in Mohit et al.
?tended to appendadditional tokens to high-confidence MWEs (suchas proper names) rather than encourage new MWEs,which would require positing at least two new non-outside tags.5.2 FeaturesBasic features.
These are largely based on thoseof Constant et al.
(2012): they look at word unigramsand bigrams, character prefixes and suffixes, and POStags, as well as lexicon entries that match lemmas10of multiple words in the sentence.
Appendix A liststhe basic features in detail.Some of the basic features make use of lexicons.We use or construct 10 lists of English MWEs: allmultiword entries in WordNet (Fellbaum, 1998); allmultiword chunks in SemCor (Miller et al., 1993);all multiword entries in English Wiktionary;11 theWikiMwe dataset mined from English Wikipedia(Hartmann et al., 2012); the SAID database ofphrasal lexical idioms (Kuiper et al., 2003); thenamed entities and other MWEs in the WSJ corpuson the English side of the CEDT (Hajic?
et al., 2012);10The WordNet API in NLTK (Bird et al., 2009) was used forlemmatization.11http://en.wiktionary.org; data obtained fromhttps://toolserver.org/~enwikt/definitions/enwikt-defs-20130814-en.tsv.gz198LOOKUP SUPERVISED MODELpreexising lexicons entries max gaplengthP R F1 ?
P R F1 ?none 0 74.39 44.43 55.57 2.19WordNet + SemCor 71k 0 46.15 28.41 35.10 2.44 74.51 45.79 56.64 1.906 lexicons 420k 0 35.05 46.76 40.00 2.88 76.08 52.39 61.95 1.6710 lexicons 437k 0 33.98 47.29 39.48 2.88 75.95 51.39 61.17 2.30best configuration within-domain lexicon1 46.66 47.90 47.18 2.31 76.64 51.91 61.84 1.652 lexicons + MWtypes(train)?1 6 lexicons + MWtypes(train)?2Table 2: Use of lexicons for lookup-based vs. statistical segmentation.
Supervised learning used only basic featuresand the structured perceptron, with the 8-tag scheme.
Results are with the link-based matching criterion for evaluation.Top: Comparison of preexisting lexicons.
?6 lexicons?
refers to WordNet and SemCor plus SAID, WikiMwe, Phrases.net,and English Wiktionary; ?10 lexicons?
adds MWEs from CEDT, VNC, LVC, and Oyz.
(In these lookup-basedconfigurations, allowing gappy MWEs never helps performance.
)Bottom: Combining preexisting lexicons with a lexicon derived from MWEs annotated in the training portion of eachcross-validation fold at least once (lookup) or twice (model).All precision, recall, and F1 percentages are averaged across 8 folds of cross-validation on train; standard deviationsare shown for the F1 score.
In each column, the highest value using only preexisting lexicons is underlined, and thehighest overall value is bolded.
The boxed row indicates the configuration used as the basis for subsequent experiments.the verb-particle constructions (VPCs) dataset of(Baldwin, 2008); a list of light verb constructions(LVCs) provided by Claire Bonial; and two idiomswebsites.12 After preprocessing, each lexical entryconsists of an ordered sequence of word lemmas,some of which may be variables like <something>.Given a sentence and one or more of the lexicons,lookup proceeds as follows: we enumerate entrieswhose lemma sequences match a sequence of lemma-tized tokens, and build a lattice of possible analysesover the sentence.
We find the shortest path (i.e.,using as few expressions as possible) with dynamicprogramming, allowing gaps of up to length 2.13Unsupervised word clusters.
Distributional clus-tering on large (unlabeled) corpora can produce lexi-cal generalizations that are useful for syntactic andsemantic analysis tasks (e.g.
: Miller et al., 2004; Kooet al., 2008; Turian et al., 2010; Owoputi et al., 2013;Grave et al., 2013).
We were interested to see whethera similar pattern would hold for MWE identification,given that MWEs are concerned with what is lexi-cally idiosyncratic?i.e., backing off from specificlexemes to word classes may lose the MWE-relevantinformation.
Brown clustering14 (Brown et al., 1992)12http://www.phrases.net/ and http://home.postech.ac.kr/~oyz/doc/idiom.html13Each top-level lexical expression (single- or multiword)incurs a cost of 1; each expression within a gap has cost 1.25.14With Liang?s (2005) implementation: https://github.com/percyliang/brown-cluster.
We obtain 1,000 clusterson the 21-million-word Yelp Academic Dataset15(which is similar in genre to the annotated web re-views data) gives us a hard clustering of word types.To our tagger, we add features mapping the previ-ous, current, and next token to Brown cluster IDs.The feature for the current token conjoins the wordlemma with the cluster ID.Part-of-speech tags.
We compared three PTB-style POS taggers on the full REVIEWS subcor-pus (train+test).
The Stanford CoreNLP tagger16(Toutanova et al., 2003) yields an accuracy of 90.4%.The ARK TweetNLP tagger v. 0.3.2 (Owoputi et al.,2013) achieves 90.1% with the model17 trained on theTwitter corpus of Ritter et al.
(2011), and 94.9% whentrained on the ANSWERS, EMAIL, NEWSGROUP, andWEBLOG subcorpora of WTB.
We use this third con-figuration to produce automatic POS tags for trainingand testing our MWE tagger.
(A comparison condi-tion in ?6.3 uses oracle POS tags.
)5.3 Experimental SetupThe corpus of web reviews described in ?2 is usedfor training and evaluation.
101 arbitrarily chosendocuments (500 sentences, 7,171 words) were heldfrom words appearing at least 25 times.15https://www.yelp.com/academic_dataset16v.
3.2.0, with english-bidirectional-distsim17http://www.ark.cs.cmu.edu/TweetNLP/model.ritter_ptb_alldata_fixed.20130723199LINK-BASED EXACT MATCHconfiguration M ?
?w?
P R F1 P R F1base model 5 ?
1,765k 69.27 50.49 58.35 60.99 48.27 53.85+ recall cost 4 150 1,765k 61.09 57.94 59.41 53.09 55.38 54.17+ clusters 3 100 2,146k 63.98 55.51 59.39 56.34 53.24 54.70+ oracle POS 4 100 2,145k 66.19 59.35 62.53 58.51 57.00 57.71Table 3: Comparison of supervised models on test (using the 8-tag scheme).
The base model corresponds to the boxedresult in table table 2, but here evaluated on test.
For each configuration, the number of training iterations M and (exceptfor the base model) the recall-oriented hyperparameter ?
were tuned by cross-validation on train.out as a final test set.
This left 3,312 sentences/48,408 words for training/development (train).
Fea-ture engineering and hyperparameter tuning wereconducted with 8-fold cross-validation on train.
The8-tag scheme is used except where otherwise noted.In learning with the structured perceptron (algo-rithm 1), we employ two well-known techniques thatcan both be viewed as regularization.
First, we usethe average of parameters over all timesteps of learn-ing.
Second, within each cross-validation fold, we de-termine the number of training iterations (epochs) Mby early stopping?that is, after each iteration, we usethe model to decode the held-out data, and when thataccuracy ceases to improve, use the previous model.The two hyperparameters are the number of iterationsand the value of the recall cost hyperparameter (?
).Both are tuned via cross-validation on train; we usethe multiple of 50 that maximizes average link-basedF1.
The chosen values are shown in table 3.
Experi-ments were managed with the ducttape tool.186 ResultsWe experimentally address the following questionsto probe and justify our modeling approach.6.1 Is supervised learning necessary?Previous MWE identification studies have foundbenefit to statistical learning over heuristic lexiconlookup (Constant and Sigogne, 2011; Green et al.,2012).
Our first experiment tests whether this holdsfor comprehensive MWE identification: it comparesour supervised tagging approach with baselines ofheuristic lookup on preexisting lexicons.
The base-lines construct a lattice for each sentence using thesame method as lexicon-based model features (?5.2).If multiple lexicons are used, the union of their en-18https://github.com/jhclark/ducttape/tries is used to construct the lattice.
The resultingsegmentation?which does not encode a strengthdistinction?is evaluated against the gold standard.Table 2 shows the results.
Even with just the la-beled training set as input, the supervised approachbeats the strongest heuristic baseline (that incorpo-rates in-domain lexicon entries extracted from thetraining data) by 30 precision points, while achievingcomparable recall.
For example, the baseline (but notthe statistical model) incorrectly predicts an MWE inplaces to eat in Baltimore (because eat in, meaning?eat at home,?
is listed in WordNet).
The supervisedapproach has learned not to trust WordNet too muchdue to this sort of ambiguity.
Downstream applica-tions that currently use lexicon matching for MWEidentification (e.g., Ghoneim and Diab, 2013) likelystand to benefit from our statistical approach.6.2 How best to exploit MWE lexicons(type-level information)?For statistical tagging (right portion of table 2), usingmore preexisting (out-of-domain) lexicons generallyimproves recall; precision also improves a bit.A lexicon of MWEs occurring in the non-held-outtraining data at least twice19 (table 2, bottom right) ismarginally worse (better precision/worse recall) thanthe best result using only preexisting lexicons.6.3 Variations on the base modelWe experiment with some of the modeling alterna-tives discussed in ?5.
Results appear in table 3 underboth the link-based and exact match evaluation cri-teria.
We note that the exact match scores are (asexpected) several points lower.19If we train with access to the full lexicon of trainingset MWEs, the learner credulously overfits to relying on thatlexicon?after all, it has perfect coverage of the training data!
?which proves fatal for the model at test time.200Recall-oriented cost.
The recall-oriented costadds about 1 link-based F1 point, sacrificing precisionin favor of recall.Unsupervised word clusters.
When combinedwith the recall-oriented cost, these produce a slightimprovement to precision/degradation to recall, im-proving exact match F1 but not affecting link-basedF1.
Only a few clusters receive high positive weight;one of these consists of matter, joke, biggie, pun,avail, clue, corkage, frills, worries, etc.
These wordsare diverse semantically, but all occur in collocationswith no, which is what makes the cluster coherentand useful to the MWE model.Oracle part-of-speech tags.
Using human-annotated rather than automatic POS tags improvesMWE identification by about 3 F1 points on test(similar differences were observed in development).6.4 What are the highest-weighted features?An advantage of the linear modeling framework isthat we can examine learned feature weights to gainsome insight into the model?s behavior.In general, the highest-weighted features are thelexicon matching features and features indicative ofproper names (POS tag of proper noun, capitalizedword not at the beginning of the sentence, etc.
).Despite the occasional cluster capturing colloca-tional or idiomatic groupings, as described in theprevious section, the clusters appear to be mostlyuseful for identifying words that tend to belong (ornot) to proper names.
For example, the cluster withstreet, road, freeway, highway, airport, etc., as wellas words outside of the cluster vocabulary, weighin favor of an MWE.
A cluster with everyday desti-nations (neighborhood, doctor, hotel, bank, dentist)prefers non-MWEs, presumably because these wordsare not typically part of proper names in this corpus.This was from the best model using non-oracle POStags, so the clusters are perhaps useful in correct-ing for proper nouns that were mistakenly taggedas common nouns.
One caveat, though, is that it ishard to discern the impact of these specific featureswhere others may be capturing essentially the sameinformation.6.5 How heterogeneous are learned MWEs?On test, the final model (with automatic POS tags)predicts 365 MWE instances (31 are gappy; 23 arePOS pattern # examples (lowercased lemmas)NOUN NOUN 53 customer service, oil changeVERB PREP 36 work with, deal with, yell atPROPN PROPN 29 eagle transmission, comfort zoneADJ NOUN 21 major award, top notch, mental healthVERB PART 20 move out, end up, pick up, pass upVERB ADV 17 come back, come in, come by, stay awayPREP NOUN 12 on time, in fact, in cash, for instanceVERB NOUN 10 take care, make money, give crapVERB PRON 10 thank you, get itPREP PREP 8 out of, due to, out ta, in betweenADV ADV 6 no matter, up front, at all, early onDET NOUN 6 a lot, a little, a bit, a dealVERB DET NOUN 6 answer the phone, take a chanceNOUN PREP 5 kind of, care for, tip on, answer toTable 4: Top predicted POS patterns and frequencies.weak).
There are 298 unique MWE types.Organizing the predicted MWEs by their coarsePOS sequence reveals that the model is not too preju-diced in the kinds of expressions it recognizes: the298 types fall under 89 unique POS+strength patterns.Table 4 shows the 14 POS sequences predicted 5 ormore times as strong MWEs.
Some of the examples(major award, a deal, tip on) are false positives, butmost are correct.
Singleton patterns include PROPNVERB (god forbid), PREP DET (at that), ADJ PRON(worth it), and PREP VERB PREP (to die for).True positive MWEs mostly consist of (a) namedentities, and (b) lexical idioms seen in training and/orlisted in one of the lexicons.
Occasionally the sys-tem correctly guesses an unseen and OOV idiombased on features such as hyphenation (walk - in) andcapitalization/OOV words (Chili Relleno, BIG MIS-TAKE).
On test, 244 gold MWE types were unseenin training; the system found 93 true positives (wherethe type was predicted at least once), 109 false posi-tives, and 151 false negatives?an unseen type recallrate of 38%.
Removing types that occurred in lexi-cons leaves 35 true positives, 61 false positives, and111 false negatives?a unseen and OOV type recallrate of 24%.6.6 What kinds of mismatches occur?Inspection of the output turns up false positives dueto ambiguity (e.g., Spongy and sweet bread); falsenegatives (top to bottom); and overlap (get high qual-ity service, gold get high quality service; live up to,gold live up to).
A number of the mismatches turn201scheme ?Y ?
?
M ?w?
P R F1no gaps, 1-level 3 100 2.1 733k 73.33 55.72 63.20no gaps, 2-level 4 150 3.3 977k 72.60 59.11 65.09gappy, 1-level 6 200 1.6 1,466k 66.48 61.26 63.65gappy, 2-level 8 100 3.5 1,954k 73.27 60.44 66.15Table 5: Training with different tagging schemes.
Resultsare cross-validation averages on train.
All schemes areevaluated against the full gold standard (8 tags).out to be problems with the gold standard, like hav-ing our water shut off (gold having our water shutoff ).
This suggests that even noisy automatic taggersmight help identify annotation inconsistencies anderrors for manual correction.6.7 Are gappiness and the strength distinctionlearned in practice?Three quarters of MWEs are strong and contain nogaps.
To see whether our model is actually sensi-tive to the phenomena of gappiness and strength,we train on data simplified to remove one or bothdistinctions?as in the first 3 labelings in figure 2?and evaluate against the full 8-tag scheme.
For themodel with the recall cost, clusters, and oracle POStags, we evaluate each of these simplifications ofthe training data in table 5.
The gold standard forevaluation remains the same across all conditions.If the model was unable to recover gappy expres-sions or the strong/weak distinction, we would expectit to do no better when trained with the full tagset thanwith the simplified tagset.
However, there is someloss in performance as the tagset for learning is sim-plified, which suggests that gappiness and strengthare being learned to an extent.7 Related WorkOur annotated corpus (Schneider et al., 2014) joinsseveral resources that indicate certain varieties ofMWEs: lexicons such as WordNet (Fellbaum, 1998),SAID (Kuiper et al., 2003), and WikiMwe (Hartmannet al., 2012); targeted lists (Baldwin, 2005, 2008;Cook et al., 2008; Tu and Roth, 2011, 2012); web-sites like Wiktionary and Phrases.net; and large-scalecorpora such as SemCor (Miller et al., 1993), theFrench Treebank (Abeill?
et al., 2003), the Szeged-ParalellFX corpus (Vincze, 2012), and the PragueCzech-English Dependency Treebank (C?mejrek et al.,2005).
The difference is that Schneider et al.
(2014)pursued a comprehensive annotation approach ratherthan targeting specific varieties of MWEs or relyingon a preexisting lexical resource.
The annotationsare shallow, not relying explicitly on syntax (thoughin principle they could be mapped onto the parses inthe Web Treebank).In terms of modeling, the use of machine learn-ing classification (Hashimoto and Kawahara, 2008;Shigeto et al., 2013) and specifically BIO sequencetagging (Diab and Bhutada, 2009; Constant and Si-gogne, 2011; Constant et al., 2012; Vincze et al.,2013) for contextual recognition of MWEs is notnew.
Lexical semantic classification tasks like namedentity recognition (e.g., Ratinov and Roth, 2009), su-persense tagging (Ciaramita and Altun, 2006; Paa?and Reichartz, 2009), and index term identification(Newman et al., 2012) also involve chunking of cer-tain MWEs.
But our discriminative models, facili-tated by the new corpus, broaden the scope of theMWE identification task to include many varieties ofMWEs at once, including explicit marking of gapsand a strength distinction.
By contrast, the afore-mentioned identification systems, as well as someMWE-enhanced syntactic parsers (e.g., Green et al.,2012), have been restricted to contiguous MWEs.However, Green et al.
(2011) allow gaps to be de-scribed as constituents in a syntax tree.
Gimpel andSmith?s (2011) shallow, gappy language model al-lows arbitrary token groupings within a sentence,whereas our model imposes projectivity and nest-ing constraints (?3).
Blunsom and Baldwin (2006)present a sequence model for HPSG supertagging,and evaluate performance on discontinuous MWEs,though the sequence model treats the non-adjacentcomponent supertags like other labels?it cannot en-force that they mutually require one another, as wedo via the gappy tagging scheme (?3.1).
The lexiconlookup procedures of Bejc?ek et al.
(2013) can matchgappy MWEs, but are nonstatistical and extremelyerror-prone when tuned for high oracle recall.Another major thread of research has pursued un-supervised discovery of multiword types from rawcorpora, such as with statistical association measures(Church et al., 1991; Pecina, 2010; Ramisch et al.,2012, inter alia), parallel corpora (Melamed, 1997;Moir?n and Tiedemann, 2006; Tsvetkov and Wint-ner, 2010), or a combination thereof (Tsvetkov and202Wintner, 2011); this may be followed by a lookup-and-classify approach to contextual identification(Ramisch et al., 2010).
Though preliminary experi-ments with our models did not show benefit to incor-porating such automatically constructed lexicons, wehope these two perspectives can be brought togetherin future work.8 ConclusionThis article has presented the first supervised modelfor identifying heterogeneous multiword expressionsin English text.
Our feature-rich discriminative se-quence tagger performs shallow chunking with anovel scheme that allows for MWEs containing gaps,and includes a strength distinction to separate highlyidiomatic expressions from collocations.
It is trainedand evaluated on a corpus of English web reviewsthat are comprehensively annotated for multiwordexpressions.
Beyond the training data, its features in-corporate evidence from external resources?severallexicons as well as unsupervised word clusters; weshow experimentally that this statistical approach isfar superior to identifying MWEs by heuristic lexiconlookup alone.
Future extensions might integrate addi-tional features (e.g., exploiting statistical associationmeasures computed over large corpora), enhance thelexical representation (e.g., by adding semantic tags),improve the expressiveness of the model (e.g., withhigher-order features and inference), or integrate themodel with other tasks (such as parsing and transla-tion).Our data and open source software are released athttp://www.ark.cs.cmu.edu/LexSem/.AcknowledgmentsThis research was supported in part by NSF CA-REER grant IIS-1054319, Google through the Read-ing is Believing project at CMU, and DARPA grantFA8750-12-2-0342 funded under the DEFT program.We are grateful to Kevin Knight, Martha Palmer,Claire Bonial, Lori Levin, Ed Hovy, Tim Baldwin,Omri Abend, members of JHU CLSP, the NLP groupat Berkeley, and the Noah?s ARK group at CMU, andanonymous reviewers for valuable feedback.A Basic FeaturesAll are conjoined with the current label, yi.Label Features1.
previous label (the only first-order feature)Token FeaturesOriginal token2.
i = {1,2}3. i = ?w??{0,1}4.
capitalized ?
?i = 0?5.
word shapeLowercased token6.
prefix: [wi]k1 ?4k=17.
suffix: [wi]?w?j ??w?j=?w??38.
has digit9.
has non-alphanumeric c10.
context word: w j ?i+2j=i?211.
context word bigram: w j+1j ?i+1j=i?2Lemma Features12.
lemma + context lemma if one of them is a verb and the otheris a noun, verb, adjective, adverb, preposition, or particle: ?i ??
j ?i+2j=i?2Part-of-speech Features13.
context POS: pos j ?i+2j=i?214.
context POS bigram: pos j+1j ?i+1j=i?215.
word + context POS: wi?posi?116.
context word + POS: wi?1?posiLexicon Features (unlexicalized)WordNet only17.
OOV: ?i is not in WordNet as a unigram lemma ?
posi18.
compound: non-punctuation lemma ?i and the {previous,next} lemma in the sentence (if it is non-punctuation; an inter-vening hyphen is allowed) form an entry in WordNet, possiblyseparated by a hyphen or space19.
compound-hyphen: posi = HYPH ?
previous and next tokensform an entry in WordNet, possibly separated by a hyphen orspace20.
ambiguity class: if content word unigram ?i is in WordNet,the set of POS categories it can belong to; else posi if not acontent POS ?
the POS of the longest MW match to which ?ibelongs (if any) ?
the position in that match (B or I)For each multiword lexicon21.
lexicon name ?
status of token i in the shortest path segmen-tation (O, B, or I) ?
subcategory of lexical entry whose matchincludes token i, if matched ?
whether the match is gappy22.
the above ?
POS tags of the first and last matched tokens inthe expressionOver all multiword lexicons23.
at least k lexicons contain a match that includes this token (ifn ?
1 matches, n active features)24. at least k lexicons contain a match that includes this token,starts with a given POS, and ends with a given POS203ReferencesAnne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.2003.
Building a treebank for French.
In Anne Abeill?and Nancy Ide, editors, Treebanks, volume 20 of Text,Speech and Language Technology, pages 165?187.Kluwer Academic Publishers, Dordrecht, The Nether-lands.Timothy Baldwin.
2005.
Looking for prepositional verbsin corpus data.
In Proc.
of the Second ACL-SIGSEMWorkshop on the Linguistic Dimensions of Prepositionsand their Use in Computational Linguistics Formalismsand Applications, pages 115?126.
Colchester, UK.Timothy Baldwin.
2008.
A resource for evaluating thedeep lexical acquisition of English verb-particle con-structions.
In Proc.
of MWE, pages 1?2.
Marrakech,Morocco.Timothy Baldwin and Su Nam Kim.
2010.
Multiwordexpressions.
In Nitin Indurkhya and Fred J. Damerau,editors, Handbook of Natural Language Processing,Second Edition.
CRC Press, Taylor and Francis Group,Boca Raton, Florida, USA.Eduard Bejc?ek, Pavel Stran?
?k, and Pavel Pecina.
2013.Syntactic identification of occurrences of multiwordexpressions in text using a lexicon with dependencystructures.
In Proc.
of the 9th Workshop on MultiwordExpressions, pages 106?115.
Atlanta, Georgia, USA.G?bor Berend.
2011.
Opinion expression mining by ex-ploiting keyphrase extraction.
In Proc.
of 5th Interna-tional Joint Conference on Natural Language Process-ing, pages 1162?1170.
Chiang Mai, Thailand.Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.2012a.
English Web Treebank.
Technical ReportLDC2012T13, Linguistic Data Consortium, Philadel-phia, Pennsylvania, USA.Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.2012b.
English Web Treebank.
Technical ReportLDC2012T13, Linguistic Data Consortium, Philadel-phia, Pennsylvania, USA.Steven Bird, Ewan Klein, and Edward Loper.
2009.
Natu-ral Language Processing with Python: Analyzing Textwith the Natural Language Toolkit.
O?Reilly Media,Inc., Sebastopol, California, USA.Phil Blunsom and Timothy Baldwin.
2006.
Multilingualdeep lexical acquisition for HPSGs via supertagging.In Proc.
of EMNLP, pages 164?171.
Sydney, Australia.Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18(4):467?479.Marine Carpuat and Mona Diab.
2010.
Task-based eval-uation of multiword expressions: a pilot study in sta-tistical machine translation.
In Proc.
of NAACL-HLT,pages 242?245.
Los Angeles, California, USA.Kenneth Church, William Gale, Patrick Hanks, and Don-ald Hindle.
1991.
Using statistics in lexical analysis.In Uri Zernik, editor, Lexical acquisition: exploitingon-line resources to build a lexicon, pages 115?164.Lawrence Erlbaum Associates, Hillsdale, New Jersey,USA.Massimiliano Ciaramita and Yasemin Altun.
2006.
Broad-coverage sense disambiguation and information extrac-tion with a supersense sequence tagger.
In Proc.
ofEMNLP, pages 594?602.
Sydney, Australia.Michael Collins.
2002.
Discriminative training methodsfor Hidden Markov Models: theory and experimentswith perceptron algorithms.
In Proc.
of EMNLP, pages1?8.
Philadelphia, Pennsylvania, USA.Matthieu Constant and Anthony Sigogne.
2011.
MWU-aware part-of-speech tagging with a CRF model andlexical resources.
In Proc.
of the Workshop on Multi-word Expressions: from Parsing and Generation to theReal World, pages 49?56.
Portland, Oregon, USA.Matthieu Constant, Anthony Sigogne, and Patrick Watrin.2012.
Discriminative strategies to integrate multiwordexpression recognition and parsing.
In Proc.
of ACL,pages 204?212.
Jeju Island, Korea.Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.
2008.The VNC-Tokens dataset.
In Proc.
of MWE, pages19?22.
Marrakech, Morocco.Hal Daum?, III.
2006.
Practical structured learning tech-niques for natural language processing.
Ph.D. disserta-tion, University of Southern California, Los Angeles,California, USA.
URL http://hal3.name/docs/daume06thesis.pdf.Mona Diab and Pravin Bhutada.
2009.
Verb noun con-struction MWE token classification.
In Proc.
of MWE,pages 17?22.
Suntec, Singapore.Nick C. Ellis, Rita Simpson-Vlach, and Carson Maynard.2008.
Formulaic language in native and second lan-guage speakers: psycholinguistics, corpus linguistics,and TESOL.
TESOL Quarterly, 42(3):375?396.Christiane Fellbaum, editor.
1998.
WordNet: an elec-tronic lexical database.
MIT Press, Cambridge, Mas-sachusetts, USA.Charles J. Fillmore, Paul Kay, and Mary CatherineO?Connor.
1988.
Regularity and idiomaticity in gram-matical constructions: the case of ?let alone?.
Language,64(3):501?538.Yoav Freund and Robert E. Schapire.
1999.
Large marginclassification using the perceptron algorithm.
MachineLearning, 37(3):277?296.Mahmoud Ghoneim and Mona Diab.
2013.
Multiwordexpressions in the context of statistical machine trans-204lation.
In Proc.
of IJCNLP, pages 1181?1187.
Nagoya,Japan.Kevin Gimpel and Noah A. Smith.
2011.
Generativemodels of monolingual and bilingual gappy patterns.In Proc.
of WMT, pages 512?522.
Edinburgh, Scotland,UK.Adele E. Goldberg.
1995.
Constructions: a constructiongrammar approach to argument structure.
Universityof Chicago Press, Chicago, Illinois, USA.Adele E. Goldberg.
2006.
Constructions at work: thenature of generalization in language.
Oxford UniversityPress, Oxford, UK.Edouard Grave, Guillaume Obozinski, and Francis Bach.2013.
Hidden Markov tree models for semantic classinduction.
In Proc.
of CoNLL, pages 94?103.
Sofia,Bulgaria.Spence Green, Marie-Catherine de Marneffe, John Bauer,and Christopher D. Manning.
2011.
Multiword expres-sion identification with tree substitution grammars: aparsing tour de force with French.
In Proc.
of EMNLP,pages 725?735.
Edinburgh, Scotland, UK.Spence Green, Marie-Catherine de Marneffe, and Christo-pher D. Manning.
2012.
Parsing models for identify-ing multiword expressions.
Computational Linguistics,39(1):195?227.Jan Hajic?, Eva Hajic?ov?, Jarmila Panevov?, Petr Sgall,Silvie Cinkov?, Eva Fuc?
?kov?, Marie Mikulov?, PetrPajas, Jan Popelka, Jir??
Semeck?, Jana ?indlerov?, Jan?te?p?nek, Josef Toman, Zden?ka Ure?ov?, and Zdene?k?abokrtsk?.
2012.
Prague Czech-English DependencyTreebank 2.0.
Technical Report LDC2012T08, Linguis-tic Data Consortium, Philadelphia, Pennsylvania, USA.URL http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2003T10.Silvana Hartmann, Gy?rgy Szarvas, and Iryna Gurevych.2012.
Mining multiword terms from Wikipedia.
InMaria Teresa Pazienza and Armando Stellato, editors,Semi-Automatic Ontology Development.
IGI Global,Hershey, Pennsylvania, USA.Chikara Hashimoto and Daisuke Kawahara.
2008.
Con-struction of an idiom corpus and its application to id-iom identification based on WSD incorporating idiom-specific features.
In Proc.
of EMNLP, pages 992?1001.Honolulu, Hawaii, USA.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple semi-supervised dependency parsing.
In Proc.of ACL-08: HLT, pages 595?603.
Columbus, Ohio.Koenraad Kuiper, Heather McCann, Heidi Quinn,Therese Aitchison, and Kees van der Veer.
2003.SAID.
Technical Report LDC2003T10, LinguisticData Consortium, Philadelphia, Pennsylvania, USA.URL http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2003T10.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: probabilisticmodels for segmenting and labeling sequence data.
InProc.
of ICML, pages 282?289.Percy Liang.
2005.
Semi-supervised learning for nat-ural language.
Master?s thesis, Massachusetts In-stitute of Technology, Cambridge, Massachusetts,USA.
URL http://people.csail.mit.edu/pliang/papers/meng-thesis.pdf.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19(2):313?330.I.
Dan Melamed.
1997.
Automatic discovery of non-compositional compounds in parallel data.
In Proc.of EMNLP, pages 97?108.
Providence, Rhode Island,USA.George A. Miller, Claudia Leacock, Randee Tengi, andRoss T. Bunker.
1993.
A semantic concordance.
InProc.
of HLT, pages 303?308.
Plainsboro, New Jersey,USA.Scott Miller, Jethran Guinness, and Alex Zamanian.
2004.Name tagging with word clusters and discriminativetraining.
In Proc.
of HLT-NAACL, pages 337?342.Boston, Massachusetts, USA.Behrang Mohit, Nathan Schneider, Rishav Bhowmick, Ke-mal Oflazer, and Noah A. Smith.
2012.
Recall-orientedlearning of named entities in Arabic Wikipedia.
InProc.
of EACL, pages 162?173.
Avignon, France.Begona Villada Moir?n and J?rg Tiedemann.
2006.
Iden-tifying idiomatic expressions using automatic word-alignment.
In Proc.
of the EACL 2006 Workshopon Multi-word Expressions in a Multilingual Context,pages 33?40.
Trento, Italy.Rosamund Moon.
1998.
Fixed expressions and idiomsin English: a corpus-based approach.
Oxford Stud-ies in Lexicography and Lexicology.
Clarendon Press,Oxford, UK.David Newman, Nagendra Koilada, Jey Han Lau, andTimothy Baldwin.
2012.
Bayesian text segmentationfor index term identification and keyphrase extraction.In Proc.
of COLING 2012, pages 2077?2092.
Mumbai,India.Geoffrey Nunberg, Ivan A.
Sag, and Thomas Wasow.
1994.Idioms.
Language, 70(3):491?538.Olutobi Owoputi, Brendan O?Connor, Chris Dyer, KevinGimpel, Nathan Schneider, and Noah A. Smith.
2013.Improved part-of-speech tagging for online conversa-tional text with word clusters.
In Proc.
of NAACL-HLT,pages 380?390.
Atlanta, Georgia, USA.Gerhard Paa?
and Frank Reichartz.
2009.
Exploiting205semantic constraints for estimating supersenses withCRFs.
In Proc.
of the Ninth SIAM International Confer-ence on Data Mining, pages 485?496.
Sparks, Nevada,USA.Pavel Pecina.
2010.
Lexical association measures andcollocation extraction.
Language Resources and Evalu-ation, 44(1):137?158.Carlos Ramisch.
2012.
A generic and openframework for multiword expressions treatment:from acquisition to applications.
Ph.D. disser-tation, University of Grenoble and Federal Uni-versity of Rio Grande do Sul, Grenoble, France.URL http://www.inf.ufrgs.br/~ceramisch/download_files/thesis-getalp.pdf.Carlos Ramisch, Vitor De Araujo, and Aline Villavicencio.2012.
A broad evaluation of techniques for automaticacquisition of multiword expressions.
In Proc.
of ACL2012 Student Research Workshop, pages 1?6.
Jeju Is-land, Korea.Carlos Ramisch, Aline Villavicencio, and Christian Boitet.2010.
mwetoolkit: a framework for multiword expres-sion identification.
In Proc.
of LREC, pages 662?669.Valletta, Malta.Lance A. Ramshaw and Mitchell P. Marcus.
1995.
Textchunking using transformation-based learning.
In Proc.of the Third ACL Workshop on Very Large Corpora,pages 82?94.
Cambridge, Massachusetts, USA.Lev Ratinov and Dan Roth.
2009.
Design challenges andmisconceptions in named entity recognition.
In Proc.of CoNLL, pages 147?155.
Boulder, Colorado, USA.Marta Recasens and Eduard Hovy.
2011.
BLANC: Im-plementing the Rand index for coreference evaluation.Natural Language Engineering, 17(04):485?510.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011.Named entity recognition in tweets: an experimentalstudy.
In Proc.
of EMNLP, pages 1524?1534.
Edin-burgh, Scotland, UK.Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-take, and Dan Flickinger.
2002.
Multiword expressions:a pain in the neck for NLP.
In Alexander Gelbukh,editor, Computational Linguistics and Intelligent TextProcessing, volume 2276 of Lecture Notes in ComputerScience, pages 189?206.
Springer, Berlin, Germany.Nathan Schneider, Spencer Onuffer, Nora Kazour, EmilyDanchik, Michael T. Mordowanec, Henrietta Conrad,and Noah A. Smith.
2014.
Comprehensive annotationof multiword expressions in a social web corpus.
InProc.
of LREC.
Reykjav?k, Iceland.Yutaro Shigeto, Ai Azuma, Sorami Hisamoto, ShuheiKondo, Tomoya Kouse, Keisuke Sakaguchi, AkifumiYoshimoto, Frances Yung, and Yuji Matsumoto.
2013.Construction of English MWE dictionary and its appli-cation to POS tagging.
In Proc.
of the 9th Workshopon Multiword Expressions, pages 139?144.
Atlanta,Georgia, USA.James W. Thatcher.
1967.
Characterizing derivation treesof context-free grammars through a generalization offinite automata theory.
Journal of Computer and SystemSciences, 1(4):317?322.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Proc.of HLT-NAACL, pages 173?180.
Edmonton, Alberta,Canada.Yulia Tsvetkov and Shuly Wintner.
2010.
Extraction ofmulti-word expressions from small parallel corpora.In Coling 2010: Posters, pages 1256?1264.
Beijing,China.Yulia Tsvetkov and Shuly Wintner.
2011.
Identificationof multi-word expressions by combining multiple lin-guistic information sources.
In Proc.
of EMNLP, pages836?845.
Edinburgh, Scotland, UK.Yuancheng Tu and Dan Roth.
2011.
Learning Englishlight verb constructions: contextual or statistical.
InProc.
of the Workshop on Multiword Expressions: fromParsing and Generation to the Real World, pages 31?39.Portland, Oregon, USA.Yuancheng Tu and Dan Roth.
2012.
Sorting out the mostconfusing English phrasal verbs.
In Proc.
of *SEM,pages 65?69.
Montr?al, Quebec, Canada.Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word representations: a simple and generalmethod for semi-supervised learning.
In Proc.
of ACL,pages 384?394.
Uppsala, Sweden.Martin C?mejrek, Jan Cur?
?n, Jan Hajic?, and Jir??
Havelka.2005.
Prague Czech-English Dependency Treebank:resource for structure-based MT.
In Proc.
of EAMT,pages 73?78.
Budapest, Hungary.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoreticcoreference scoring scheme.
In Proc.
of MUC-6, pages45?52.
Columbia, Maryland, USA.Veronika Vincze.
2012.
Light verb constructions in theSzegedParalellFX English-Hungarian parallel corpus.In Proc.
of LREC.
Istanbul, Turkey.Veronika Vincze, Istv?n Nagy T., and J?nos Zsibrita.
2013.Learning to detect English and Hungarian light verbconstructions.
ACM Transactions on Speech and Lan-guage Processing, 10(2):6:1?6:25.206
