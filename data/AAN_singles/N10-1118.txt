Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 769?776,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsOptimal Parsing Strategies for Linear Context-Free Rewriting SystemsDaniel GildeaComputer Science DepartmentUniversity of RochesterRochester, NY 14627AbstractFactorization is the operation of transforminga production in a Linear Context-Free Rewrit-ing System (LCFRS) into two simpler produc-tions by factoring out a subset of the nontermi-nals on the production?s righthand side.
Fac-torization lowers the rank of a production butmay increase its fan-out.
We show how toapply factorization in order to minimize theparsing complexity of the resulting grammar,and study the relationship between rank, fan-out, and parsing complexity.
We show that itis always possible to obtain optimum parsingcomplexity with rank two.
However, amongtransformed grammars of rank two, minimumparsing complexity is not always possible withminimum fan-out.
Applying our factorizationalgorithm to LCFRS rules extracted from de-pendency treebanks allows us to find the mostefficient parsing strategy for the syntactic phe-nomena found in non-projective trees.1 IntroductionG?mez-Rodr?guez et al (2009a) recently examinedthe problem of transforming arbitrary grammars inthe Linear Context-Free Rewriting System (LCFRS)formalism (Vijay-Shankar et al, 1987) in order toreduce the rank of a grammar to 2 while minimiz-ing its fan-out.
The work was motivated by thedesire to develop efficient chart-parsing algorithmsfor non-projective dependency trees (Kuhlmann andNivre, 2006) that do not rely on the independenceassumptions of spanning tree algorithms (McDon-ald et al, 2005).
Efficient parsing algorithms forgeneral LCFRS are also relevant in the context ofSynchronous Context-Free Grammars (SCFGs) as aformalism for machine translation, as well as the de-sire to handle even more general synchronous gram-mar formalisms which allow nonterminals to coverdiscontinuous spans in either language (Melamed etal., 2004; Wellington et al, 2006).
LCFRS providesa very general formalism which subsumes SCFGs,the Multitext Grammars of Melamed et al (2004),as well as mildly context-sensitive monolingual for-malisms such as Tree Adjoining Grammar (Joshiand Schabes, 1997).
Thus, work on transforminggeneral LCFRS grammars promises to be widely ap-plicable in both understanding how these formalismsinterrelate, and, from a more practical viewpoint, de-riving efficient parsing algorithms for them.In this paper, we focus on the problem of trans-forming an LCFRS grammar into an equivalentgrammar for which straightforward application ofdynamic programming to each rule yields a tabularparsing algorithm with minimum complexity.
Thisis closely related, but not equivalent, to the prob-lem considered by G?mez-Rodr?guez et al (2009a),who minimize the fan-out, rather than the parsingcomplexity, of the resulting grammar.
In Section 4,we show that restricting our attention to factorizedgrammars with rank no greater than 2 comes at nocost in parsing complexity.
This result may be sur-prising, as G?mez-Rodr?guez et al (2009a) com-ment that ?there may be cases in which one has tofind an optimal trade-off between rank and fan-out?in order to minimize parsing complexity ?
in fact,no such trade-off is necessary, as rank 2 is alwayssufficient for optimal parsing complexity.
Giventhis fact, we show how to adapt the factorization al-gorithm of G?mez-Rodr?guez et al (2009a) to re-turn a transformed grammar with minimal parsingcomplexity and rank 2.
In Section 5, we give a769counterexample to the conjecture that minimal pars-ing complexity is possible among binarizations withminimal fan-out.2 BackgroundA linear context-free rewriting system (LCFRS) isdefined as a tuple G = (VN , VT , P, S), where VT isa set of terminal symbols, VN is a set of nonterminalsymbols, P is a set of productions, and S ?
VN isa distinguished start symbol.
Associated with eachnonterminal B is a fan-out ?
(B), which tell howmany discontinuous spans B covers.
Productionsp ?
P take the form:p : A?
g(B1, B2, .
.
.
, Br) (1)where A,B1, .
.
.
Br ?
VN , and g is a functiong : (V ?T )?
(B1) ?
.
.
.?
(V ?T )?
(Br) ?
(V ?T )?
(A)which specifies how to assemble the?ri=1 ?
(Bi)spans of the righthand side nonterminals into the?
(A) spans of the lefthand side nonterminal.
Thefunction g must be linear, non-erasing, whichmeans that if we writeg(?x1,1, .
.
.
, x1,?
(B1)?, .
.
.
, ?x1,1, .
.
.
, x1,?(Br)?
)= ?t1, .
.
.
, t?
(A)?the tuple of strings ?t1, .
.
.
, t?(A)?
on the righthandside contains each variable xi,j from the lefthandside exactly once, and may also contain terminalsfrom VT .We call r, the number of nonterminals on therighthand side of a production p, the rank of p, ?
(p).The fan-out of a production, ?
(p) is the fan-out of itslefthand side, ?(A).
The rank of a grammar is themaximum rank of its rules,?
(G) = maxp?P?
(p)and similarly the fan-out of a grammar is the maxi-mum fan-out of its rules, or equivalently, of its non-terminals:?
(G) = maxB?VN?
(B)3 Parsing LCFRSA bottom-up dynamic programming parser can beproduced from an LCFRS grammar by generaliz-ing the CYK algorithm for context-free grammars.We convert each production of the LCFRS into adeduction rule with variables for the left and rightendpoints of each of the ?
(Bi) spans of each of thenonterminals Bi, i ?
[r] in the righthand side of theproduction.The computational complexity of the resultingparser is polynomial in the length of the input string,with the degree of the polynomial being the numberof distinct endpoints in the most complex produc-tion.
Thus, for input of length n, the complexityis O(nc) for some constant c which depends on thegrammar.For a given rule, each of the r nonterminals has?
(Bi) spans, and each span has a left and right end-point, giving an upper bound of c ?
2?ri=1 ?
(Bi).However, some of these endpoints may be sharedbetween nonterminals on the righthand side.
Theexact number of distinct variables for the dynamicprogramming deduction rule can the writtenc(p) = ?
(A) +r?i=1?
(Bi) (2)where c(p) is the parsing complexity of a produc-tion p of the form of eq.
1 (Seki et al, 1991).
Tosee this, consider counting the left endpoint of eachspan on the lefthand side of the production, and theright endpoint of each span on the righthand side ofthe production.
Any variable corresponding to theleft endpoint of a span of a righthand side nonter-minal will either be shared with the right endpointof another span if two spans are being joined by theproduction, or, alternatively, will form the left end-point of a span of A.
Thus, each distinct endpoint inthe production is counted exactly once by eq.
2.The parsing complexity of a grammar, c(G), isthe maximum parsing complexity of its rules.
Fromeq.
2, we see that c(G) ?
(?
(G) + 1)?(G).
Whilewe focus on the time complexity of parsing, it is in-teresting to note the space complexity of the DP al-gorithm is O(n2?
(G)), since the DP table for eachnonterminal is indexed by at most 2?
(G) positionsin the input string.7704 Binarization Minimizes ParsingComplexityAn LCFRS production of rank r can be factorizedinto two productions of the form:p1 : A?
g1(B1, .
.
.
, Br?2, X)p2 : X ?
g2(Br?1, Br)This operation results in new productions that havelower rank, but possibly higher fan-out, than theoriginal production.If we examine the DP deduction rules correspond-ing to the original production p, and the first newproduction p1 we find thatc(p1) ?
c(p)regardless of the function g of the original produc-tion, or the fan-out of the production?s nonterminals.This is because?
(X) ?
?
(Br?1) + ?
(Br)that is, our newly created nonterminal X may joinspans from Br?1 and Br, but can never introducenew spans.
Thus,c(p1) = ?
(A) +(r?2?i=1?
(Bi))+ ?(X)?
?
(A) +r?i=1?
(Bi)= c(p)As similar result holds for the second newly cre-ated production:c(p2) ?
c(p)In this case, the fan-out of the newly created nonter-minal, ?
(X) may be greater than ?(A).
Let us con-sider the left endpoints of the spans of X .
Each leftendpoint is either also the left endpoint of a span ofA, or is the right endpoint of some nonterminal notincluded in X , that is, one of B1, .
.
.
Br?2.
Thus,?
(X) ?
?
(A) +r?2?i=1?
(Bi)and applying this inequality to the definition of c(p2)we have:c(p2) = ?
(X) + ?
(Br?1) + ?(Br?2)?
?
(A) +r?i=1?
(Bi)= c(p)For notational convenience, we have defined thefactorization operation as factoring out the last twononterminals of a rule; however, the same operationcan be applied to factor out any subset of the orig-inal nonterminals.
The same argument that parsingcomplexity cannot increase still applies.We may apply the factorization operation repeat-edly until all rules have rank 2; we refer to the re-sulting grammar as a binarization of the originalLCFRS.
The factorization operation may increasethe fan-out of a grammar, but never increases itsparsing complexity.
This guarantees that, if we wishto find the transformation of the original grammarhaving the lowest parsing complexity, it is sufficientto consider only binarizations.
This is because anytransformed grammar having more than two nonter-minals on the righthand side can be binarized with-out increasing its parsing complexity.5 The relationship between fan-out andparsing complexityG?mez-Rodr?guez et al (2009a) provide an algo-rithm for finding the binarization of an LCFRS hav-ing minimal fan-out.
The key idea is to search overways of combining subsets of a rule?s righthand sidenonterminals such that subsets with low fan-out areconsidered first; this results in an algorithm withcomplexity polynomial in the rank of the input rule,with the exponent depending on the resulting mini-mum fan-out.This algorithm can be adapted to find the binariza-tion with minimum parsing complexity, rather thanminimum fan-out.
We simply use c(p) rather than?
(p) as the score for new productions, controllingboth which binarizations we prefer and the order inwhich they are explored.An interesting question then arises: does the bina-rization with minimal parsing complexity also haveminimal fan-out?
A binarization into a grammar of771A?
g(B1, B2, B3, B4)g(?x1,1, x1,2?, ?x2,1, x2,2, x2,3?, ?x3,1, x3,2, x3,3, x3,4, x3,5?, ?x4,1, x4,2, x4,3?)
=?x4,1x3,1, x2,1, x4,2x1,1x2,2x4,3x3,2x2,3x3,3, x1,2x3,4, x3,5?Figure 2: A production for which minimizing fan-out and minimizing parsing complexity are mutually exclusive.
{B3}{B4}{B3, B4}{B3, B4}{B1}{B1, B3, B4}{B1, B3, B4}{B2}{B1, B2, B3, B4}Figure 3: The binarization of the rule from Figure 2 that minimizes parsing complexity.
In each of the three steps,we show the spans of each of the two subsets of the rule?s righthand-side nonterms being combined, with the spans oftheir union (corresponding to a nonterminal created by the binarization) below.7721: function MINIMAL-BINARIZATION(p,?
)2: workingSet?
?
;3: agenda?
priorityQueue(?
);4: for i from 1 to ?
(p) do5: workingSet?
workingSet ?
{Bi};6: agenda?
agenda ?
{Bi};7: while agenda 6= ?
do8: p?
?
pop minimum from agenda;9: if nonterms(p?)
= {B1, .
.
.
B?
(p)} then10: return p?
;11: for p1 ?
workingSet do12: p2 ?
newProd(p?, p1);13: find p?2 ?
workingSet :14: nonterms(p?2) = nonterms(p2);15: if p2 ?
p?2 then16: workingSet?
workingSet ?
{p2}\{p?2};17: push(agenda, p2);Figure 1: Algorithm to compute best binarization accord-ing to a user-specified ordering ?
over productions.fan-out f ?
cannot have parsing complexity higherthan 3f ?, according to eq.
2.
Thus, minimizing fan-out puts an upper bound on parsing complexity, butis not guaranteed to minimize it absolutely.
Bina-rizations with the same fan-out may in fact varyin their parsing complexity; similarly binarizationswith the same parsing complexity may vary in theirfan-out.
It is not immediately apparent whether, inorder to find a binarization of minimal parsing com-plexity, it is sufficient to consider only binarizationsof minimal fan-out.To test this conjecture, we adapted the algorithmof G?mez-Rodr?guez et al (2009a) to use a prior-ity queue as the agenda, as shown in Figure 1.
Thealgorithm takes as an argument an arbitrary partialordering relation on productions, and explores pos-sible binarized rules in the order specified by this re-lation.
In Figure 1, ?workingSet?
is a set of single-ton nonterminals and binarized productions whichare guaranteed to be optimal for the subset of non-terminals that they cover.
The function ?nonterms?returns, for a newly created production, the subsetof the original nonterminals B1, .
.
.
Br that it gen-erates, and returns subsets of singleton nonterminalsdirectly.To find the binarization with the minimum fan-outf ?
and the lowest parsing complexity among bina-rizations with fan-out f ?, we use the following com-parison operation in the binarization algorithm:p1 ?
?c p2 iff ?
(p1) < ?
(p2) ?(?
(p1) = ?
(p2) ?
c(p1) < c(p2))guaranteeing that we explore binarizations withlower fan-out first, and, among binarizations withequal fan-out, those with lower parsing complexityfirst.
Similarly, we can search for the binarizationwith the lowest parsing complexity c?
and the lowestfan-out among binarizations with complexity c?, weusep1 ?c?
p2 iff c(p1) < c(p2) ?
(c(p1) = c(p2) ?
?
(p1) < ?
(p2))We find that, in fact, it is sometimes necessary tosacrifice minimum fan-out in order to achieve mini-mum parsing complexity.
An example of an LCFRSrule for which this is the case is shown in Figure 2.This production can be binarized to produce a set ofproductions with parsing complexity 14 (Figure 3);among binarizations with this complexity the mini-mum fan-out is 6.
However, an alternative binariza-tion with fan-out 5 is also possible; among binariza-tions with this fan-out, the minimum parsing com-plexity is 15.
This binarization (not pictured) firstjoins B1 and B2, then adds B4, and finally adds B3.Given the incompatibility of optimizing timecomplexity and fan-out, which corresponds to spacecomplexity, which should we prefer?
In some sit-uations, it may be desirable to find some trade-offbetween the two.
It is important to note, however,that if optimization of space complexity is the soleobjective, factorization is unnecessary, as one cannever improve on the fan-out required by the origi-nal grammar nonterminals.6 A Note on Generative CapacityRambow and Satta (1999) categorize the genera-tive capacity of LCFRS grammars according to theirrank and fan-out.
In particular, they show thatgrammars can be arranged in a two-dimensionalgrid, with languages of rank r and fan-out f havinggreater generative capacity than both grammars ofrank r and fan-out f ?1 and grammars of rank r?1773nmod sbj root vc pp nmod np tmpA hearing is scheduled on the issue todaynmod?
g1 g1 = ?A ?sbj?
g2(nmod, pp) g2(?x1,1?, ?x2,1?)
= ?x1,1 hearing , x2,1?root?
g3(sbj, vc) g3(?x1,1, x1,2?, ?x2,1, x2,2?)
= ?x1,1 is x2,1x1,2x2,2?vc?
g4(tmp) g4(?x1,1?)
= ?
scheduled , x1,1?pp?
g5(tmp) g5(?x1,1?)
= ?
on x1,1?nmod?
g6 g6 = ?
the ?np?
g7(nmod) g7(?x1,1?)
= ?x1,1 issue ?tmp?
g8 g8 = ?
today ?Figure 4: A dependency tree with the LCFRS rules extracted for each word (Kuhlmann and Satta, 2009).and fan-out f , with two exceptions: with fan-out 1,all ranks greater than one are equivalent (context-free languages), and with fan-out 2, rank 2 and rank3 are equivalent.This classification is somewhat unsatisfying be-cause minor changes to a grammar can change bothits rank and fan-out.
In particular, through factor-izing rules, it is always possible to decrease rank,potentially at the cost of increasing fan-out, until abinarized grammar of rank 2 is achieved.Parsing complexity, as defined above, also pro-vides a method to compare the generative capacityof LCFRS grammars.
From Rambow and Satta?sresult that grammars of rank two and increasingfan-out provide an infinite hierarchy of increasinggenerative capacity, we see that parsing complexityalso provides such an infinite hierarchy.
Compar-ing grammars according to the parsing complexityamounts to specifying a normalized binarization forgrammars of arbitrary rank and fan-out, and compar-ing the resulting binarized grammars.
This allows usto arrange LCFRS grammars into total ordering overgenerative capacity, that is a one-dimensional hier-archy, rather than a two-dimensional grid.
It alsogives a way of categorizing generative capacity thatis more closely tied to algorithmic complexity.It is important to note, however, that parsing com-plexity as calculated by our algorithm remains afunction of the grammar, rather than an intrinsicfunction of the language.
One can produce arbitrar-ily complex grammars that generate the simple lan-guage a?.
Thus the parsing complexity of a gram-mar, like its rank and fan-out, can be said to catego-rize its strong generative capacity.7 ExperimentsA number of recent papers have examined dynamicprogramming algorithms for parsing non-projectivedependency structures by exploring how well vari-ous categories of polynomially-parsable grammarscover the structures found in dependency treebanksfor various languages (Kuhlmann and Nivre, 2006;G?mez-Rodr?guez et al, 2009b).Kuhlmann and Satta (2009) give an algorithm forextracting LCFRS rules from dependency structures.One rule is extracted for each word in the depen-dency tree.
The rank of the rule is the number ofchildren that the word has in the dependency tree,as shown by the example in Figure 4.
The fan-outof the symbol corresponding to a word is the num-ber of continuous intervals in the sentence formedby the word and its descendants in the tree.
Projec-774complexity arabic czech danish dutch german port swedish20 118 116 115 113 112 2 311 1 1 110 2 6 16 39 7 4 18 4 7 129 65 107 3 12 89 30 186 178 11 362 1811 492 595 48 1132 93 411 1848 172 2014 250 18269 1026 6678 18124 2643 17363 10942 265202 18306 39362 154948 41075 41245Table 1: Number of productions with specified parsing complexitytive trees yield LCFRS rules of fan-out one and pars-ing complexity three, while the fan-out and parsingcomplexity from non-projective trees are in princi-ple unbounded.Extracting LCFRS rules from treebanks allows usto study how many of the rules fall within certainconstraints.
Kuhlmann and Satta (2009) give an al-gorithm for binarizing LCRFS rules without increas-ing the rules?
fan-out; however, this is not alwayspossible, and the algorithm does not succeed even insome cases for which such a binarization is possible.Kuhlmann and Satta (2009) find that all but 0.02%of productions in the CoNLL 2006 training data,which includes various languages, can be binarizedby their algorithm, but they do not give the fan-outor parsing complexity of the resulting rules.
In re-lated work, G?mez-Rodr?guez et al (2009b) definethe class of mildly ill-nested dependency structuresof varying gap degrees; gap degree is essentially fan-out minus one.
For a given gap degree k, this class ofgrammars can be parsing in time O(n3k+4) for lexi-calized grammars.
G?mez-Rodr?guez et al (2009b)study dependency treebanks for nine languages andfind that all dependency structures meet the mildlyill-nested condition in the dependency treebanks forsome gap degree.
However, they do not report themaximum gap degree or parsing complexity.We extracted LCFRS rules from dependency tree-banks using the same procedure as Kuhlmann andSatta (2009), and applied the algorithm of Figure 1directly to calculate their minimum parsing com-plexity.
This allows us to characterize the pars-ing complexity of the rules found in the treebankwithout needing to define specific conditions onthe rules, such as well-nestedness (Kuhlmann andNivre, 2006) or mildly ill-nestedness, that may notbe necessary for all efficiently parsable grammars.The numbers of rules of different complexities areshown in Table 1.As found by previous studies, the vast major-ity of productions are context-free (projective trees,parsable in O(n3)).
Of non-projective rules, thevast majority can be parsed in O(n6), including thewell-nested structures of gap degree one defined byKuhlmann and Nivre (2006).
The single most com-plex rule had parsing complexity of O(n20), and wasderived from a Swedish sentence which turns out tobe so garbled as to be incomprehensible (taken fromthe high school essay portion of the Swedish tree-bank).
It is interesting to note that, while the bina-rization algorithm is exponential in the worst case, itis practical for real data: analyzing all the rules ex-tracted from the various treebanks takes only a fewminutes.
We did not find any cases in rules extractedfrom Treebank data of rules where minimizing pars-ing complexity is inconsistent with minimizing fan-775out, as is the case for the rule of Figure 2.8 ConclusionWe give an algorithm for finding the optimum pars-ing complexity for an LCFRS among grammars ob-tained by binarization.
We find that minimum pars-ing complexity is always achievable with rank 2, butis not always achievable with minimum fan-out.
Byapplying the binarization algorithm to productionsfound in dependency treebanks, we can completelycharacterize the parsing complexity of the extractedLCFRS grammar.Acknowledgments This work was funded by NSFgrants IIS-0546554 and IIS-0910611.
We are grate-ful to Joakim Nivre for assistance with the Swedishtreebank.ReferencesCarlos G?mez-Rodr?guez, Marco Kuhlmann, GiorgioSatta, and David Weir.
2009a.
Optimal reduction ofrule length in linear conext-free rewriting systems.
InProceedings of the 2009 Meeting of the North Ameri-can chapter of the Association for Computational Lin-guistics (NAACL-09), pages 539?547.Carlos G?mez-Rodr?guez, David Weir, and John Car-roll.
2009b.
Parsing mildly non-projective depen-dency structures.
In Proceedings of the 12th Confer-ence of the European Chapter of the ACL (EACL-09),pages 291?299.A.K.
Joshi and Y. Schabes.
1997.
Tree-adjoining gram-mars.
In G. Rozenberg and A. Salomaa, editors,Handbook of Formal Languages, volume 3, pages 69?124.
Springer, Berlin.Marco Kuhlmann and Joakim Nivre.
2006.
Mildlynon-projective dependency structures.
In Proceed-ings of the International Conference on ComputationalLinguistics/Association for Computational Linguistics(COLING/ACL-06), pages 507?514.Marco Kuhlmann and Giorgio Satta.
2009.
Treebankgrammar techniques for non-projective dependencyparsing.
In Proceedings of the 12th Conference of theEuropean Chapter of the ACL (EACL-09), pages 478?486.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedings ofHuman Language Technology Conference and Confer-ence on Empirical Methods in Natural Language Pro-cessing (HLT/EMNLP).I.
Dan Melamed, Giorgio Satta, and Ben Wellington.2004.
Generalized multitext grammars.
In Proceed-ings of the 42nd Annual Conference of the Associationfor Computational Linguistics (ACL-04), Barcelona,Spain.Owen Rambow and Giorgio Satta.
1999.
Independentparallelism in finite copying parallel rewriting sys-tems.
Theor.
Comput.
Sci., 223(1-2):87?120.H.
Seki, T. Matsumura, M. Fujii, and T. Kasami.
1991.On multiple context-free grammars.
Theoretical Com-puter Science, 88:191?229.K.
Vijay-Shankar, D. L. Weir, and A. K. Joshi.
1987.Characterizing structural descriptions produced byvarious grammatical formalisms.
In Proceedings ofthe 25th Annual Conference of the Association forComputational Linguistics (ACL-87).Benjamin Wellington, Sonjia Waxmonsky, and I. DanMelamed.
2006.
Empirical lower bounds on thecomplexity of translational equivalence.
In Proceed-ings of the International Conference on Computa-tional Linguistics/Association for Computational Lin-guistics (COLING/ACL-06), pages 977?984, Sydney,Australia.776
