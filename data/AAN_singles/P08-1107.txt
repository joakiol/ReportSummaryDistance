Proceedings of ACL-08: HLT, pages 941?949,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsResolving Personal Names in Email Using Context ExpansionTamer Elsayed,?Douglas W. Oard,?
and Galileo Namata?Human Language Technology Center of Excellence andUMIACS Laboratory for Computational Linguistics and Information Processing (CLIP)University of Maryland, College Park, MD 20742{telsayed, oard, gnamata}@umd.eduAbstractThis paper describes a computational ap-proach to resolving the true referent of anamed mention of a person in the body of anemail.
A generative model of mention gener-ation is used to guide mention resolution.
Re-sults on three relatively small collections indi-cate that the accuracy of this approach com-pares favorably to the best known techniques,and results on the full CMU Enron collectionindicate that it scales well to larger collections.1 IntroductionThe increasing prevalence of informal text fromwhich a dialog structure can be reconstructed (e.g.,email or instant messaging), raises new challenges ifwe are to help users make sense of this cacophony.Large collections offer greater scope for assemblingevidence to help with that task, but they pose addi-tional challenges as well.
With well over 100,000unique email addresses in the CMU version of theEnron collection (Klimt and Yang, 2004), commonnames (e.g., John) might easily refer to any one ofseveral hundred people.
In this paper, we associatenamed mentions in unstructured text (i.e., the bodyof an email and/or the subject line) to modeled iden-tities.
We see at least two direct applications for thiswork: (1) helping searchers who are unfamiliar withthe contents of an email collection (e.g., historians orlawyers) better understand the context of emails thatthey find, and (2) augmenting more typical socialnetworks (based on senders and recipients) with ad-ditional links based on references found in unstruc-tured text.Most approaches to resolving identity can be de-composed into four sub-problems: (1) finding a ref-erence that requires resolution, (2) identifying can-didates, (3) assembling evidence, and (4) choosing?Department of Computer Science?College of Information Studiesamong the candidates based on the evidence.
Forthe work reported in this paper, we rely on the userto designate references requiring resolution (whichwe model as a predetermined set of mention-queriesfor which the correct referent is known).
Candidateidentification is a computational expedient that per-mits the evidence assembly effort to be efficientlyfocused; we use only simple techniques for that task.Our principal contributions are the approaches wetake to evidence generation (leveraging three waysof linking to other emails where evidence might befound: reply chains, social interaction, and topicalsimilarity) and our approach to choosing among can-didates (based on a generative model of referenceproduction).
We evaluate the effectiveness of ourapproach on four collections, three of which havepreviously reported results for comparison, and onethat is considerably larger than the others.The remainder of this paper is as follows.
Sec-tion 2 surveys prior work.
Section 3 then describesour approach to modeling identity and ranking can-didates.
Section 4 presents results, and Section 5concludes.2 Related WorkThe problem of identity resolution in email is a spe-cial case of the more general problem referred to as?Entity Resolution.?
Entity resolution is genericallydefined as a process of determining the mappingfrom references (e.g., names, phrases) observed indata to real-world entities (e.g., persons, locations).In our case, the problem is to map mentions in emailsto the identities of the individuals being referred to.Various approaches have been proposed for en-tity resolution.
In structured data (e.g., databases),approaches have included minimizing the numberof ?matching?
and ?merging?
operations (Benjel-loun et al, 2006), using global relational informa-tion(Malin, 2005; Bhattacharya and Getoor, 2007;Reuther, 2006) and using a probabilistic generative941model (Bhattacharya and Getoor, 2006).
None ofthese approaches, however, both make use of con-versational, topical, and time aspects, shown impor-tant in resolving personal names (Reuther, 2006),and take into account global relational informa-tion.
Similarly, approaches in unstructured data(e.g., text) have involved using clustering techniquesover biographical facts (Mann and Yarowsky, 2003),within-document resolution (Blume, 2005), and dis-criminative unsupervised generative models (Li etal., 2005).
These too are insufficient for our prob-lem since they suffer from inability scale or to han-dle early negotiation.Specific to the problem of resolving mentions inemail collections, Abadi (Abadi, 2003) used emailorders from an online retailer to resolve productmentions in orders and Holzer et al (Holzer et al,2005) used the Web to acquire information aboutindividuals mentioned in headers of an email col-lection.
Our work is focused on resolving personalname references in the full email including the mes-sage body; a problem first explored by Diehl et al(Diehl et al, 2006) using header-based traffic anal-ysis techniques.
Minkov et al(Minkov et al, 2006)studied the same problem using a lazy graph walkbased on both headers and content.
Those two re-cent studies reported results on different test collec-tions, however, making direct comparisons difficult.We have therefore adopted their test collections inorder to establish a common point of reference.3 Mention Resolution ApproachThe problem we are interested in is the resolutionof a personal-name mention (i.e., a named referenceto a person) m, in a specific email em in the givencollection of emails E, to its true referent.
We as-sume that the user will designate such mention.
Thiscan be formulated as a known-item retrieval problem(Allen, 1989) since there is always only one right an-swer.
Our goal is to develop a system that provides alist of potential candidates, ranked according to howstrongly the system believes that a candidate is thetrue referent meant by the email author.
In this pa-per, we propose a probabilistic approach that ranksthe candidates based on the estimated probability ofhaving been mentioned.
Formally, we seek to esti-mate the probability p(c|m) that a potential candi-date c is the one referred to by the given mention m,over all candidates C.We define a mention m as a tuple < lm, em >,where lm is the ?literal?
string of characters that rep-resentsm and em is the email wherem is observed.1We assume that m can be resolved to a distinguish-able participant for whom at least one email addressis present in the collection.2The probabilistic approach we propose is moti-vated by a generative scenario of mentioning peoplein email.
The scenario begins with the author of theemail em, intending to refer to a person in that email.To do that s/he will:1.
Select a person c to whom s/he will refer2.
Select an appropriate context xk to mention c3.
Select a specific lexical reference lm to refer toc given the context xk.For example, suppose ?John?
is sending an emailto ?Steve?
and wants to mention a common friend?Edward.?
?John?
knows that he and Steve know2 people named Edward, one is a friend of bothknown by ?Ed?
and the other is his soccer trainer.If ?John?
would like to talk about the former, hewould use ?Ed?
but he would likely use ?Edward?plus some terms (e.g., ?soccer?, ?team?, etc) for thelatter.
?John?
relies on the social context, or the topi-cal context, for ?Steve?
to disambiguate the mention.The steps of this scenario impose a certain struc-ture to our solution.
First, we need to have arepresentational model for each candidate identity.Second, we need to reconstruct the context of thequeried mention.
Third, it requires a computationalmodel of identity that supports reasoning about iden-tities.
Finally, it requires a resolution technique thatleverages both the identity models and the contextto rank the potential candidates.
In this section,we will present our resolution approach within thatstructure.
We first discuss how to build both repre-sentational and computational models of identity insection 3.1.
Next, we introduce a definition of thecontextual space and how we can reconstruct it in1The exact position in em where lm is observed should alsobe included in the definition, but we ignore it assuming that allmatched literal mentions in one email refer to the same identity.2Resolving mentions that refer to non-participants is outsidethe scope of this paper.942section 3.2.
Finally, we link those pieces togetherby the resolution algorithm in section 3.3.3.1 Computational Model of IdentityRepresentation: In a collection of emails, indi-viduals often use different email addresses, multi-ple forms of their proper names, and different nick-names.
In order to track references to a person overa large collection, we need to capture as many aspossible of these referential attributes in one rep-resentation.
We extend our simple representationof identity proposed in (Elsayed and Oard, 2006)where an identity is represented by a set of pair-wise co-occurrence of referential attributes (i.e., co-occurrence ?associations?
), and each extracted as-sociation has a frequency of occurrence.
The at-tributes are extracted from the headers and saluta-tion and signature lines.
For example, an ?address-nickname?
association < a, n > is inferred when-ever a nickname n is usually observed in signaturelines of emails sent from email address a. Threetypes of referential attributes were identified in theoriginal representation: email addresses, names, andnicknames.
We add usernames as well to accountfor the absence of any other type of names.
Names,nicknames, and usernames are distinguishable basedon where each is extracted: email addresses andnames from headers, nicknames from salutationand signature lines, and usernames from email ad-dresses.
Since (except in rare cases) an email ad-dress is bound to one personal identity, the modelleverages email addresses as the basis by mandat-ing that at least one email address must appear inany observed association.
As an off-line preprocess-ing step, we extract the referential attributes from thewhole collection and build the identity models.
Thefirst step in the resolution process is to determine thelist of identity models that are viable candidates asthe true referent.
For the experiments reported in thispaper, any identity model with a first name or nick-name that exactly matches the mention is considereda candidate.Labeling Observed Names: For the purpose of re-solving name mentions, it is necessary to computethe probability p(l|c) that a person c is referred to bya given ?literal?
mention l. Intuitively, that probabil-ity can be estimated based on the observed ?name-type?
of l and how often that association occurs inthe represented model.
We define T as the set of3 different types of single-token name-types: first,last, and nickname.
We did not handle middle namesand initials, just for simplicity.
Names that are ex-tracted from salutation and signature lines are la-beled as nicknames whereas full names extractedfrom headers are first normalized to ?First Last?form and then each single token is labeled based onits relative position as being the first or last name.Usernames are treated similarly to full names if theyhave more than one token, otherwise they are ig-nored.
Note that the same single-token name mayappear as a first name and a nickname.Figure 1: A computational model of identity.Reasoning: Having tokenized and labeled allnames, we propose to model the association of asingle-token name l of type t to an identity c by asimple 3-node Bayesian network illustrated in Fig-ure 1.
In the network, the observed mention l isdistributed conditionally on both the identity c andthe name-type t. p(c) is the prior probability of ob-serving the identity c in the collection.
p(t|c) is theprobability that a name-type t is used to refer to c.p(l|t, c) is the probability of referring to c by l oftype t. These probabilities can be inferred from therepresentational model as follows:p(c) =|assoc(c)|?c?
?C |assoc(c?
)|p(t|c) =freq(t, c)?t?
?T freq(t?
, c)p(l|t, c) =freq(l, t, c)?l?
?assoc(c) freq(l?
, t, c)where assoc(c) is the set of observed associations ofreferential attributes in the represented model c.The probability of observing a mention l giventhat it belongs to an identity c, without assuming aspecific token type, can then be inferred as follows:p(l|c) =?t?Tp(t|c) p(l|t, c)In the case of a multi-token names (e.g., JohnSmith), we assume that the first is either a first name943or nickname and the last is a last name, and computeit accordingly as follows:p(l1l2|c) = {?t?
{f,n}p(t|c) p(l1|t, c)} ?
p(l2|last, c)where f and n above denotes first name and nick-name respectively.Email addresses are also handled, but in a differ-ent way.
Since we assume each of them uniquelyidentifies the identity, all email addresses for oneidentity are mapped to just one of them, which thenhas half of the probability mass (because it appearsin every extracted co-occurrence association).Our computational model of identity can bethought of as a language model over a set of per-sonal references and thus it is important to accountfor unobserved references.
If we know that a spe-cific first name often has a common nickname (by adictionary of commonly used first to nickname map-pings (e.g., Robert to Bob)), but this nickname wasnot observed in the corpus, we will need to applysmoothing.
We achieve that by assuming the nick-name would have been observed n times where n issome fraction (0.75 in our experiments) of the fre-quency of the observed name.
We repeat that foreach unobserved nickname and then treat them as ifthey were actually observed.3.2 Contextual SpaceFigure 2: Contextual SpaceIt is obvious that understanding the context of anambiguous mention will help with resolving it.Fortunately, the nature of email as a conversa-tional medium and the link-relationships betweenemails and people over time can reveal clues that canbe exploited to partially reconstruct that context.We define the contextual space X(m) of a men-tion m as a mixture of 4 types of contexts with ?k asthe mixing coefficient of context xk.
The four con-texts (illustrated in Figure 2) are:(1) Local Context: the email em where the namedperson is mentioned.
(2) Conversational Context: emails in the broaderdiscussion that includes em, typically the thread thatcontains it.
(3) Social Context: discussions that some or all ofthe participants (sender and receivers) of em joinedor initiated at around the time of the mention-email.These might bear some otherwise-undetected rela-tionship to the mention-email.
(4) Topical Context: discussions that are topicallysimilar to the mention-discussion that took place ataround the time of em, regardless of whether the dis-cussions share any common participants.These generally represent a growing (although notstrictly nested) contextual space around the queriedmention.
We assume that all mentions in an emailshare the same contextual space.
Therefore, we cantreat the context of a mention as the context of itsemail.
However, each email in the collection hasits own contextual space that could overlap with an-other email?s space.3.2.1 Formal DefinitionWe define K as the set of the 4 types of contexts.A context xk is represented by a probability distri-bution over all emails in the collection.
An email ejbelongs to the kth context of another email ei withprobability p(ej |xk(ei)).
How we actually representeach context and estimate the distribution dependsupon the type of the context.
We explain that in de-tail in section 3.2.2.3.2.2 Context ReconstructionIn this section, we describe how each context isconstructed.Local Context: Since this is simply em, all of theprobability mass is assigned to it.Conversational Context: Threads (i.e., replychains) are imperfect approximations of focuseddiscussions, since people sometimes switch topicswithin a thread (and indeed sometimes within thesame email).
We nonetheless expect threads to ex-hibit a useful degree of focus and we have there-fore adopted them as a computational representationof a discussion in our experiments.
To reconstructthreads in the collection, we adopted the techniqueintroduced in (Lewis and Knowles, 1997).
Thread944reconstruction results in a unique tree containing themention-email.
Although we can distinguish be-tween different paths or subtrees of that tree, weelected to have a uniform distribution over all emailsin the same thread.
This also applies to threads re-trieved in the social and topical contexts as well.Social Context: Discussions that share commonparticipants may also be useful, though we expecttheir utility to decay somewhat with time.
To recon-struct that context, we temporally rank emails thatshare at least one participant with em in a time pe-riod around em and then expand each by its thread(with duplicate removal).
Emails in each thread arethen each assigned a weight that equals the recip-rocal of its thread rank.
We do that separately foremails that temporally precede or follow em.
Fi-nally, weights are normalized to produce one distri-bution for the whole social context.Topical Context: Identifying topically-similar con-tent is a traditional query-by-example problem thathas been well researched in, for example, the TRECrouting task (Lewis, 1996) and the Topic Detectionand Tracking evaluations (Allan, 2002).
Individualemails may be quite terse, but we can exploit theconversational structure to obtain topically relatedtext.
In our experiments, we tracked back to theroot of the thread in which em was found and usedthe subject line and the body text of that root emailas a query to Lucene3 to identify topically-similaremails.
Terms found in the subject line are dou-bled in the query to emphasize what is sometimesa concise description of the original topic.
Subse-quent processing is then similar to that used for thesocial context, except that the emails are first rankedby their topical, rather than temporal, similarity.The approaches we adopted to reconstruct the so-cial and topical contexts were chosen for their rel-ative simplicity, but there are clearly more sophis-ticated alternatives.
For example, topic modelingtechniques (McCallum et al, 2005) could be lever-aged in the reconstruction of the topical context.3.3 Mention ResolutionGiven a specific mention m and the set of identitymodels C, our goal now is to compute p(c|m) foreach candidate c and rank them accordingly.3http://lucene.apache.org3.3.1 Context-Free Mention ResolutionIf we resolve m out of its context, then we cancompute p(c|m) by applying Bayes?
rule as follows:p(c|m) ?
p(c|lm) =p(lm|c) p(c)?c?
?C p(lm|c?)
p(c?
)All the terms above are estimated as discussed ear-lier in section 3.1.
We call this approach ?backoff?since it can be used as a fall-back strategy.
It is con-sidered the baseline approach in our experiments.3.3.2 Contextual Mention ResolutionWe now discuss the more realistic situation inwhich we use the context to resolve m. By expand-ing the mention with its context, we getp(c|m) = p(c|lm, X(em))We then apply Bayes?
rule to getp(c|lm, X(em)) =p(c, lm, X(em))p(lm, X(em))where p(lm, X(em)) is the probability of observ-ing lm in the context.
We can ignore this probabil-ity since it is constant across all candidates in ourranking.
We now restrict our focus to the numera-tor p(c, lm, X(em)), that is the probability that thesender chose to refer to c by lm in the contextualspace.
As we discussed in section 3.2, X is definedas a mixture of contexts therefore we can further ex-pand it as follows:p(c, lm, X(em)) =?k?k p(c, lm, xk(em))Following the intuitive generative scenario we intro-duced earlier, the context-specific probability can bedecomposed as follows:p(c, lm, xk(em)) = p(c)?
p(xk(em)|c)?
p(lm|xk(em), c)where p(c) is the probability of selecting a can-didate c, p(xk(em)|c) is the probability of select-ing xk as an appropriate context to mention c, andp(lm|xk(em), c) is the probability of choosing tomention c by lm given that xk is the appropriate con-text.Choosing person to mention: p(c) can be estimatedas discussed in section 3.1.Choosing appropriate context: By applying Bayes?rule to compute p(xk(em)|c) we getp(xk(em)|c) =p(c|xk(em)) p(xk(em))p(c)945p(xk(em)) is the probability of choosing xk to gen-erally mention people.
In our experiments, weassumed a uniform distribution over all contexts.p(c|xk(em)) is the probability of mentioning c inxk(em).
Given that the context is defined as a distri-bution over emails, this can be expanded top(c|xk(em)) =?ei?Ep(ei|xk(em) p(c|ei))where p(c|ei) is the probability that c is mentionedin the email ei.
This, in turn, can be estimated us-ing the probability of referring to c by at least oneunique reference observed in that email.
By assum-ing that all lexical matches in the same email refer tothe same person, and that all lexically-unique refer-ences are statistically independent, we can computethat probability as follows:p(c|ei) = 1?
p(c is not mentioned in ei)= 1??m??M(ei)(1?
p(c|m?
))where p(c|m?)
is the probability that c is the truereferent of m?.
This is the same general problemof resolving mentions, but now concerning a relatedmention m?found in the context of m. To handlethis, there are two alternative solutions: (1) break thecycle and compute context-free resolution probabil-ities for those related mentions, or (2) jointly resolveall mentions.
In this paper, we will only consider thefirst, leaving joint resolution for future work.Choosing a name-mention: To estimatep(lm|xk(em), c), we suggest that the email au-thor would choose either to select a reference (or amodified version of a reference) that was previouslymentioned in the context or just ignore the context.Hence, we estimate that probability as follows:p(lm|xk(em), c) = ?
p(lm ?
xk(em)|c)+(1?
?)
p(lm|c)where ?
?
[0, 1] is a mixing parameter (set at 0.9in our experiments), and p(lm|c) is estimated as insection 3.1. p(lm ?
xk(em)|c) can be estimated asfollows:p(lm ?
xk(em)|c) =?m??xkp(lm|lm?
)p(lm?|xk) p(c|lm?
)where p(lm|lm?)
is the probability of modifying lm?into lm.
We assume all possible mentions of care equally similar to m and estimate p(lm|lm?)
by1|possible mentions of c| .
p(lm?|xk) is the probability ofobserving lm?in xk, which we estimate by its rel-ative frequency in that context.
Finally, p(c|lm?)
isagain a mention resolution problem concerning thereference ri which can be resolved as shown earlier.The Aho-Corasick linear-time algorithm (Ahoand Corasick, 1975) is used to find mentions ofnames, using a corpus-based dictionary that includesall names, nicknames, and email addresses extractedin the preprocessing step.4 Experimental EvaluationWe evaluate our mention resolution approach usingfour test collections, all are based on the CMU ver-sion of the Enron collection; each was created by se-lecting a subset of that collection, selecting a set ofquery-mentions within emails from that subset, andcreating an answer key in which each query-mentionis associated with a single email address.The first two test collections were created byMinkov et al(Minkov et al, 2006).
These test col-lections correspond to two email accounts, ?sager-e?
(the ?Sager?
collection) and ?shapiro-r?
(the?Shapiro?
collection).
Their mention-queries andanswer keys were generated automatically by iden-tifying name mentions that correspond uniquely toindividuals referenced in the cc header, and elimi-nating that cc entry from the header.The third test collection, which we call the?Enron-subset?
is an extended version of the testcollection created by Diehl at al (Diehl et al, 2006).Emails from all top-level folders were includedin the collection, but only those that were bothsent by and received by at least one email addressof the form <name1>.<name2>@enron.com wereretained.
A set of 78 mention-queries were manu-ally selected and manually associated with the emailaddress of the true referent by the third author usingan interactive search system developed specificallyto support that task.
The set of queries was lim-ited to those that resolve to an address of the form<name1>.<name2>@enron.com.
Names found insalutation or signature lines or that exactly match<name1> or <name2> of any of the email partic-ipants were not selected as query-mentions.
Those78 queries include the 54 used by Diehl et al946Table 1: Test collections used in the experiments.Test Coll.
Emails IDs Queries CandidatesSager 1,628 627 51 4 (1-11)Shapiro 974 855 49 8 (1-21)Enron-sub 54,018 27,340 78 152 (1-489)Enron-all 248,451 123,783 78 518 (3-1785)For our fourth test collection (?Enron-all?
), weused the same 78 mention-queries and the answerkey from the Enron-subset collection, but we usedthe full CMU version of the Enron collection (withduplicates removed).
We use this collection to as-sess the scalability of our techniques.Some descriptive statistics for each test collectionare shown in Table 1.
The Sager and Shapiro col-lections are typical of personal collections, whilethe other two represent organizational collections.These two types of collections differ markedly inthe number of known identities and the candidatelist sizes as shown in the table (the candidate listsize is presented as an average over that collection?smention-queries and as the full range of values).4.1 Evaluation MeasuresThere are two commonly used single-valued eval-uation measures for ?known item?-retrieval tasks.The ?Success @ 1?
measure characterizes the ac-curacy of one-best selection, computed as the meanacross queries of the precision at the top rank foreach query.
For a single-valued figure of merit thatconsiders every list position, we use ?Mean Recip-rocal Rank?
(MRR), computed as the mean acrossqueries of the inverse of the rank at which the cor-rect referent is found.4.2 ResultsThere are four basic questions which we address inour experimental evaluation: (1) How does our ap-proach perform compared to other approaches?, (2)How is it affected by the size of the collection andby increasing the time period?, (3) Which contextmakes the most important contribution to the resolu-tion task?
and (4) Does the mixture help?In our experiments, we set the mixing coefficients?k and the context priors p(xk) to a uniform distri-bution over all reconstructed contexts.To compare our system performance with resultsTable 2: Accuracy results with different time periods.Period MRR Success @ 1(days) Prob.
Minkov Prob.
Minkov10 0.899 0.889 0.843 0.804Sager 100 0.911 0.889 0.863 0.804200 0.911 0.889 0.863 0.80410 0.913 0.879 0.857 0.779Shapiro 100 0.910 0.879 0.837 0.779200 0.911 0.837 0.878 0.77910 0.878 - 0.821 -Enron-sub 100 0.911 - 0.846 -200 0.911 - 0.846 -10 0.890 - 0.821 -Enron-all 100 0.888 - 0.821 -200 0.888 - 0.821 -previously reported, we experimented with differ-ent (symmetric) time periods for selecting threadsin the social and topical contexts.
Three represen-tative time periods, in days, were arbitrarily chosen:10 (i.e., +/- 5) days, 100 (i.e., +/- 50) days, and 200(i.e., +/- 100) days.
In each case, the mention-emaildefines the center of this period.A summary of the our results (denoted by ?Prob.?
)are shown in Table 2 with the best results for eachtest collection highlighted in bold.
The table also in-cludes the results reported in Minkov et al(Minkovet al, 2006) for the small collections for comparisonpurposes.4 Each score for our system was the bestover all combinations of contexts for these collec-tions and time periods.
Given these scores, our re-sults compare favorably with the previously reportedresults for both Sager and Shapiro collections.Another notable thing about our results is thatthey seem to be good enough for practical appli-cations.
Specifically, our one-best selection (overall tried conditions) is correct at least 82% of thetime over all collections, including the largest one.Of course, the Enron-focused selection of mention-queries in every case is an important caveat on theseresults; we do not yet know how well our techniqueswill hold up with less evidence, as might be the casefor mentions of people from outside Enron.It is encouraging that testing on the largest col-4For the ?Enron-subset?
collection, we do not know which54 mention-queries Diehl et alused in (Diehl et al, 2006)947lection (with all unrelated and thus noisy data) didnot hurt the effectiveness much.
For the three differ-ent time periods we tried, there was no systematiceffect.Figure 3: Individual contexts, period set to 100 days.Individual Contexts: Our choice of contexts wasmotivated by intuition rather than experiments, sowe also took this opportunity to characterize thecontribution of each context to the results.
Wedid that by setting some of the context mixing-coefficients to zero and leaving the others equally-weighted.
Figure 3 shows the MRR achieved witheach context.
In that figure, the ?backoff?
curve in-dicates how well the simple context-free resolutionwould do.
The difference between the two small-est and the two largest collections is immediatelyapparent?this backoff is remarkably effective for thesmaller collections, and almost useless for the largerones, suggesting that the two smaller collections areessentially much easier.
The social context is clearlyquite useful, more so than any other single context,for every collection.
This tends to support our ex-pectation that social networks can be as informativeas content networks in email collections.
The topicalcontext also seems to be useful on its own.
The con-versational context is moderately useful on its ownin the larger collections.
The local context alone isnot very informative for the larger collections.Mixture of Contexts: The principal motivation forcombining different types of contexts is that differ-ent sources may provide complementary evidence.To characterize that effect, we look at combinationsof contexts.
Figure 4 shows three such context com-binations, anchored by the social context alone, witha 100-day window (the results for 10 and 200 dayperiods are similar).
Reassuringly, adding more con-texts (hence more evidence) turns out to be a rea-Figure 4: Mixture of contexts, period set to 100 days.sonable choice in most cases.
For the full combi-nation, we notice a drop in the effectiveness fromthe addition of the topical context.5 This suggeststhat the construction of the topical context may needmore careful design, and/or that learned ?k?s couldyield better evidence combination (since these re-sults were obtained with equal ?k?s).5 ConclusionWe have presented an approach to mention resolu-tion in email that flexibly makes use of expandingcontexts to accurately resolve the identity of a givenmention.
Our approach focuses on four naturallyoccurring contexts in email, including a message,a thread, other emails with senders and/or recipi-ents in common, and other emails with significanttopical content in common.
Our approach outper-forms previously reported techniques and it scaleswell to larger collections.
Moreover, our resultsserve to highlight the importance of social contextwhen resolving mentions in social media, which isan idea that deserves more attention generally.
In fu-ture work, we plan to extend our test collection withmention queries that must be resolved in the ?longtail?
of the identity distribution where less evidenceis available.
We are also interested in exploring iter-ative approaches to jointly resolving mentions.AcknowledgmentsThe authors would like to thank Lise Getoor for herhelpful advice.5This also occurs even when topical context is combinedwith only social context.948ReferencesDaniel J. Abadi.
2003.
Comparing domain-specific andnon-domain-specific anaphora resolution techniques.Cambridge University MPhil Dissertation.Alfred V. Aho and Margaret J. Corasick.
1975.
Effi-cient string matching: an aid to bibliographic search.In Communications of the ACM.James Allan, editor.
2002.
Topic detection and tracking:event-based information organization.
Kluwer Aca-demic Publishers, Norwell, MA, USA.Bryce Allen.
1989.
Recall cues in known-item retrieval.JASIS, 40(4):246?252.Omar Benjelloun, Hector Garcia-Molina, Hideki Kawai,Tait Eliott Larson, David Menestrina, Qi Su, Sut-thipong Thavisomboon, and Jennifer Widom.
2006.Generic entity resolution in the serf project.
IEEEData Engineering Bulletin, June.Indrajit Bhattacharya and Lise Getoor.
2006.
A latentdirichlet model for unsupervised entity resolution.
InThe SIAM International Conference on Data Mining(SIAM-SDM), Bethesda, MD, USA.Indrajit Bhattacharya and Lise Getoor.
2007.
Collectiveentity resolution in relational data.
ACM Transactionson Knowledge Discovery from Data, 1(1), March.Matthias Blume.
2005.
Automatic entity disambigua-tion: Benefits to NER, relation extraction, link anal-ysis, and inference.
In International Conference onIntelligence Analysis, May.Chris Diehl, Lise Getoor, and Galileo Namata.
2006.Name reference resolution in organizational emailarchives.
In Proceddings of SIAM International Con-ference on Data Mining, Bethesda, MD , USA, April20-22.Tamer Elsayed and Douglas W. Oard.
2006.
Modelingidentity in archival collections of email: A prelimi-nary study.
In Proceedings of the 2006 Conferenceon Email and Anti-Spam (CEAS 06), pages 95?103,Mountain View, California, July.Ralf Holzer, Bradley Malin, and Latanya Sweeney.
2005.Email alias detection using social network analysis.
InLinkKDD ?05: Proceedings of the 3rd internationalworkshop on Link discovery, pages 52?57, New York,NY, USA.
ACM Press.Bryan Klimt and Yiming Yang.
2004.
Introducing theEnron corpus.
In Conference on Email and Anti-Spam,Mountain view, CA, USA, July 30-31.David D. Lewis and Kimberly A. Knowles.
1997.Threading electronic mail: a preliminary study.
Inf.Process.
Manage., 33(2):209?217.David D. Lewis.
1996.
The trec-4 filtering track.
In TheFourth Text REtrieval Conference (TREC-4), pages165?180, Gaithersburg, Maryland.Xin Li, Paul Morie, and Dan Roth.
2005.
Semantic inte-gration in text: from ambiguous names to identifiableentities.
AI Magazine.
Special Issue on Semantic Inte-gration, 26(1):45?58.Bradley Malin.
2005.
Unsupervised name disambigua-tion via social network similarity.
In Workshop onLink Analysis, Counter-terrorism, and Security, inconjunction with the SIAM International Conferenceon Data Mining, Newport Beach, CA, USA, April 21-23.Gideon S. Mann and David Yarowsky.
2003.
Unsuper-vised personal name disambiguation.
In Proceedingsof the seventh conference on Natural language learn-ing at HLT-NAACL 2003, pages 33?40, Morristown,NJ, USA.
Association for Computational Linguistics.Andrew McCallum, Andres Corrada-Emmanuel, andXueruiWang Wang.
2005.
Topic and role discoveryin social networks.
In IJCAI.Einat Minkov, William W. Cohen, and Andrew Y. Ng.2006.
Contextual search and name disambiguation inemail using graphs.
In SIGIR ?06: Proceedings ofthe 29th annual international ACM SIGIR conferenceon Research and development in information retrieval,pages 27?34, New York, NY, USA.
ACM Press.Patric Reuther.
2006.
Personal name matching: New testcollections and a social network based approach.949
