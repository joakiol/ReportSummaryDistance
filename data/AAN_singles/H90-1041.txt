Algorithms for an Optimal A* Search andLinearizing the Search in the Stack Decoder.Douglas B. PaulLincoln Laboratory, MITLexington, Ma.
02173AbstractThe stack decoder is an attractive algorithm for con-trolling the acoustic and language model matching in acontinuous speech recognizer.
It implements a best-firsttree search of the language to find the best match toboth the language model and the observed speech.
Thispaper describes a method for performing the optimal A*search which guarantees tofind the most likely path (rec-ognized sentence) while extending the minimum numberof stack entries.
A tree search, however, is exponentialin the number of words.
A second algorithm is presentedwhich linearizes the search at the cost of approximatingsome of the path likelihoods.IntroductionSpeech recognition may be treated as a tree networksearch problem.
As one proceeds from the root towardthe leaves, the branches leaving each junction representthe set of words which may be appended to the cur-rent partial sentence.
Each of the branches leaving ajunction has a probability and each word has a likeli-hood of being produced by the observed acoustic data.The recognition problem is to identify the most likelypath (word sequence) from the root (beginning) to a leaf(end) taking into account he junction probabilities (thestochastic language model) and the optimum acousticmatch (including time alignment) given that path.This paper is concerned with the network search prob-lem and therefore correct recognition is defined as out-putting the most likely sentence given the languagemodel, the acoustic models, and the observed acousticdata.
If the most likely sentence is not the one spoken,that is a modeling error--not a search error.
This pa-per will assume for simplicity that an isolated sentenceis the object to be recognized.
(The stack decoder canbe extended to recognize continuous sentences.
)The stack decoder \[4\], as used in speech, is an imple-mentation of a best-first ree search.
The basic operationof a sentence decoder is as follows \[1, 2\]:1.
Initialize the stack with a null theory.
*This  work was sponsored by the Defense Advanced ResearchProjects Agency.2.
Pop the best (highest score) theory off the stack.3.
if(end-of-sentence) output sentence and terminate.4.
Perform acoustic and language-model fast matchesto obtain a short list of candidate word extensionsof the theory.5.
For each word on the candidate list:(a) Perform acoustic and language-model detailedmatches and add the log-likelihoods to the the-ory log-likelihood.i.
if(not end-of-sentence) insert into stack.ii.
if(end-of-sentence) insert into stack withend-of-sentence flag = TRUE.
(note: end-of-sentence may be optional)6.
Go to 2.The fast matches \[2\] are computationally cheap meth-ods for reducing the number of word extensions whichmust be checked by the more accurate, but computa-tionally expensive detailed matches) (The fast matchesmay also be considered a predictive component for thedetailed matches.)
Top-N mode is achieved by delayingtermination until N sentences have been output.The stack itself is just a sorted list which supports theoperations: pop the best entry, insert new entries ac-cording to their scores, and (in some implementations)discard the worst entry.
The following must be con-tained in each stack entry: the stack score used to or-der the entries, the word history (path or theory iden-tification), an output log-likelihood istribution, and anend-of-sentence flag.
Since the time of exiting the wordcannot be uniquely determined during a forward-decoderpass, the output log-likelihood as a function of time mustbe contained in each entry.
This distribution is the in-put to the next word model.
The end-of-sentence flagidentifies the theories which are candidates to end thesentence.This exposition will assume discrete observation hid-den Markov model (HMM) word models \[9, 10\] with the1The following discussion concerns the basic stack decoder andtherefore it will be assumed that  the correct word will a lways be onthe fast match l i s t .
Th is  can be guaranteed by the scheme outl inedin reference \[2\].200observation log-pdfs identified as the B = bj,k matrix,where j identifies the arc (transition between states) andk identifies the observation symbol.
(This can be triv-ially extended to continuous observation, mixture, andtied-mixture HMM systems.)
However, it should ap-ply to any stochastic word model which outputs a log-likelihood.
Similarly, a stochastic language model whichoutputs a partial sentence log-likelihood is assumed.
Anaccept-reject language model will also work--its outputis either zero or minus infinity.The A* Stack CriterionA key issue in the stack decoder is the scoring crite-rion.
(All scores used here are log-likelihoods or log-probabilities.)
If one uses the raw log-likelihoods asthe stack score, a uniform search \[7\] will result.
Thissearch will result in a prohibitive amount of computa-tion and a very large stack for any large-vocabulary high-perplexity speech recognition task because the score de-creases rapidly with path length and thus short pathswill be "carried along".
A better scoring criterion isthe A* criterion \[7\].
The A* criterion is the differencebetween the actual log-likelihood of reaching a point intime on a path and an upper bound on the log-likelihoodof any path reaching that point in time:hi(t) = Li(t) - ubL(t) (1)where Ai(t) is the scoring function, Li(t) is the outputlog-likelihood, t denotes time, i denotes the path (treebranch or left sentence fragment) and ubL(t) is an upperbound on Li(t).
The stack entry score isStSci =max hi(t).
(2)tIf ubn(t) >_ lubL(t), where lubL(t) is the least upperbound on L, the stack search is guaranteed to be ad-missible, i.e., the first output sentence will be the cor-rect (highest log-likelihood) one \[7\] and, in addition,the following sentences in top-N mode will be outputin log-likelihood order.
In general, the closer ubL(t) isto lubL(t), the less computation.
If ubL(t) = lubL(t),the search is guaranteed to be optimal \[7\], i.e., a mini-mum number of stack entries will have to be popped andextended.
If ubL(t) becomes less than lubL(t), longerpaths will be favored excessively and the first outputsentence may not have the highest log-likelihood, i.e., asearch error may occur.
(Note that ubL(t) is constantfor any t and therefore does not affect the relative scoresof the paths at any fixed time--it only affects the com-parison of paths of differing lengths and the resultantorder of path expansion.
)The basic problem is obtaining a good estimate ofubL(t) in a time-asynchronous decoder.
(Note thatlubL,tat~(t) over the states is easily computed in a time-synchronous decoder and that Astate(t) is the value com-pared to the pruning threshold in a beam search \[6\].
)One estimate of ubL(t) isubL(t) = -a t .
(3)where a is some constant greater than zero.
This ap-proach attempts to cancel out the average log-likelihoodper time step.
If a is too large, it will underestimatethe bound and risk recognition errors.
If a is small thesearch will be admissible, but will require an excessiveamount of computation.
(In fact, a = 0 is the uni-form search mentioned above.)
An intermediate value ofa will achieve a balance between the two extremes andproduce the winner with reduced computation.
Unfortu-nately no single value of a is optimum for all sentences orall parts of a single sentence.
Thus a conservative valueis generally chosen.
One way of altering the tradeoff isto run in top-N mode and sort the output sentences byscore.
If a is slightly high, the sentences may be outputout of order, but the sort provides a recovery mechanism.This scheme may also require additional computation toproduce the additional output sentences.The criterion of equation 3 can be improved by nor-malizing the observation probabilities by the A* crite-rion:tubL(t) = ~ n~ax bj,o r - at (4)r= lwhere ot is the observation at time t. This helps, butbasic problems of equation 3 still remain.
Both of thesecorrections can be precomputed by modifying the B ma-trix:Bj,k = Bj,k-- max bj,k + a.
(5)aThis stack criterion allows estimation of the most-probable partial-path exit time.
Ai(t) now exhibits apeak whose location is the estimate of the exit time ofthe word.
(The stack decoder only implements the for-ward decoder--finding the exact most-probable exit timerequires information from the decode of the remainderof the sentence.)
Therefore the estimated exit time is:tmaz,i =argmax Ai(t).
(6)tThe Opt imal  A* Stack DecoderThe upper bounds of equations 3 and 4 are fixed ap-proximations to the least upper bound and thereforeforce a tradeoff of computation and probability of searcherror.
It is, however, possible to compute the ezact leastupper bound and so perform an optimal A* search.
Theprimary difficulty is that only the "lub so far" can becomputed, i.e., only the upper bound of the currentlycomputed paths can be obtained.
This creates two diffi-culties:1.
Since the estimate of the lub (lub) is changing, thestack order may change as a result of an update oflubL(t).2.
A degeneracy in determining the best path may oc-cur since the current bound may equal Li(t) formore than one i (path).Problem 1 is easily cured by reevaluating score StScevery time lubL(t) is updated and reorganizing the stack.201This is easily accomplished if the stack is stored as aheapL The reorganization is accomplished by scanningthe heap in addressing order and, at each entry, reevalu-ating the score, and if the score is greater than that of itsparent, successively swapping the entry with its parentuntil the next parent has a higher score than the currententry.
Once the first sentence is output, lubL(t) will bestable.Problem 2 occurs because two or more theories maydominate different parts of the current upper bound.Thus all of these theories will have a score of zero.
Ifthe longest heory is extended, its descendents will alsodominate the bound and will in turn be extended.
Thiswill, of course, result in search errors because the shortertheories will never have a chance to be extended.
Thecure is to extend the shortest heory.
One could choosethe shorter theory in the case of a tie or a simpler wayof doing this is to use a slightly modified criterion:Ai(t) = Li(t)  - lubL(t) - et (7)where lt~bL(t) is the least upper bound so far and e isa very small number greater than zero.
(The value ofe need only be large enough to avoid being lost in thesystem quantization error and therefore the loss of op-timality of the search criterion can be ignored.)
The etterm serves as a tie-breaker in favor of the shorter theoryin a manner which is compatible with equation 2.
Notethat this criterion completely accounts for all factors:the language model, the reduction of the log-likelihoodas paths grow, any B matrix normalization (equation5), and any effects due to the restrictions on the HMMstate sequences in the word models.
(In fact, this cri-terion makes Ai(t) invariant o any fixed normalizationssuch as those of equation 5--a fact that will allow boththe A* search and the above definition of tm~, be usedin the search linearizing algorithm described below.
)Reorganizing the stack immediately preceding eachpop if the least upper bound has been updated, adding aminiscule length dependent penalty, and using the max-imum of the normalized log-likelihood as the stack scorefor each theory results in a computationally-optimal ad-missible implementation of the stack decoder.
Further-more, it guarantees that when the stack decoder is usedin top-N mode, the sentences will come out in decreasinglog-likelihood order.What is the advantage of the A* stack decoder overthe time-synchronous beam search?
Both, after all, endup using a least upper bound to control the search.
Thetwo lub's are different--the time synchronous search gen-erally computes its lub over all states and the pruningwill be affected by the location in the network in whichthe language-model log-likelihoods are added (at the be-ginning of the word, at the end, or spread out along2 A heap is a row-wise l inearly-addressed binary-tree data  struc-ture, whose tree representat ion resembles a pyramid.
The  score ofa parent is greater than  or equal to the scores of its chi ldren andeach parent  is located in the row above its children.
A parent al-ways has a lower address than  its child.
The  highest scoring entryresides at the root of the tree (top of the pyramid)  which is storedin the first locat ion of the array \[5\].the word model).
In contrast, the stack decoder onlycomputes its lub at the end of the words and all placesfor adding the language-model log-likelihoods are equiv-alent.
The stack decoder lub can also be better (butnever worse) than the time-synchronous lub because itis computed only for the word ends.
(On the other hand,a time-synchronous decoder can prune parts of words--the stack decoder, as described here, treats words as in-divisible units.)
Finally, the effective pruning thresholdfor the A* stack is continuously adaptive-- it  only ex-tends theories which have a chance of winning while thepruning threshold for a time-synchronous decoder is de-termined by the worst case (which may be rare).
Anunbounded stack has been assumed--stack size will bediscussed later.
(Note that typically only a relativelysmall number of theories are actually popped from thestack and extended--the majority are abandoned whendecoder terminates.
)Linearizing the SearchThe basic stack decoder is exponential in the length ofthe input because it is a tree search with the consequencethat identical subtrees must be searched as many timesas there are distinct histories.
(This section will initiallydeal with the acoustic matching part of the decode andwill therefore assume no language model--adding lan-guage models will be discussed once the acoustic issueshave been described.)
There is, however, a method forcombining identical subtrees which depends only on thelast word of the theory to be extended and tm~ (equa-tion 6).
This limits the maximum number of theories toVT where V is the number of vocabulary words and T isthe length of the input.
Thus the search will be O(T)  orlinear in the length of the input.
This method involves aminor approximation for the lower likelihood of the twojoined paths.Given that:1. the last word of the history for each of the two the-ories is the same2.
and the trnax'S of equation 6 are the samethe lower likelihood theory can be pruned because it, as-suming the approximation to be correct, can never beatthe higher likelihood path.
(Entensions to top-N will begiven later.)
This is true because probability as a func-tion of time of transitioning between the the left word(last word of the history for the theory) and the nextword is only a very weak function of words precedingthe left word.
(Any two words having the same acous-tic model are considered equal herc so if, for instance,stress is being modeled, then the stressed and unstressedword models are considered ifferent.)
The assumptionthat the weak function is not a function of the wordspreceding the left word is the approximation.
Obviously,this approximation is more accurate for longer left wordsor can be made more accurate if left word groups are usedas the matching context.202This algorithm is easily implemented in the stack de-coder.
For any new theory about to be inserted onto thestack (comparisons for a match are made by the abovetwo criteria):1.
If the new theory matches a theory on a list of pasttheories popped from the stack, discard it.
It, bydefinition, has a lower likelihood than the theory onthe list.2.
If the new theory matches a theory currently on thestack, save the more likely theory, discard the lesslikely theory and, if necessary, reorganize the stack.This algorithm saves both stack space and computation.
(It is the analog of a path join in a time-synchronousdecoder.
)To extend this theory to a top-N decoder, instead ofdiscarding the less likely theory, attach a link from themore likely theory to the less likely theory, record thedifference in the likelihoods, and store the less likely the-ory off the stack.
Whenever an end-of-sentence theory ispopped from the stack, follow the links and:1. apply the likelihood differences to reconstruct thelikelihoods,2.
reconstruct the word sequences, and3.
place the word sequences on a secondary heap or-dered by their likelihoods.Once reconstructed theories have been placed on the sec-ondary heap, the pop operation must check both thestack and the secondary heap for the most likely theoryto pop.
This algorithm will be exact for theories whichstay on the stack, but the likelihoods will be approxi-mate for the reconstructed theories from the secondaryheap.This algorithm can also be extended to include lan-guage models.
Unigram and bigram language models \[1\]are trivial--the unigram model is not a function of theleft context and the bigram model is a function of thesame word which was used in determining a match.
Thusboth can be inserted directly into the linearizing algo-rithm without modification.
A trigram language model\[1\] would insert directly if the left matching context wastwo words rather than one word.Language models with left contexts too long to beefficiently incorporated irectly into the linearizing al-gorithm could be handled by by caching the acousticmatches according to the above path joining criterion.Whenever a cache match is found, use the cached nextword output likelihoods to avoid recomputation.
(Notethat the acoustic detailed match must use only the wordlist from the acoustic fast match.
Otherwise the cachedset of word extensions will be limited by the language-model context in effect at the time of caching.
Thelanguage-model fast match can then be applied after theacoustic detailed match.)
This will make the acousticcomputation linear, but will not reduce the language-model computation.
(Some language-model algorithmsmay also be able to use similar mechanisms to avoid re-peating computations.)
This approach extends triviallyto top-N mode without the use of the links or the sec-ondary heap.A final method of using language models with a longleft-context dependency is to simply operate the lin-earized stack decoder with a bigram, unigram or no lan-guage model, and output the top-N sentences to a de-coupled language-model analyzer.
This is the decoupledmode of reference \[8\].Limiting the Stack SizeOne difficulty of the stack decoder is the stack size.The A* search algorithm reduces the stack size, but itcan still grow exponentially.
The first linearizing algo-rithm places an upper bound of VT (or V2T for thetrigram language model) on the stack size, but the sec-ondary heap can grow exponentially.
To minimize theseproblems, a pruning threshold can be applied to thestack insertion operations and the secondary heap gener-ation.
Any theory whose StSc from equation 2 is below athreshold is discarded.
(Since lubn(t) can only increaseas the decoder operates, no discarded theory would beaccepted by a later value of StSc.
Note also that thestack reorganizing operation can also discard any entriesthat fall below the threshold according the the new val-ues of StSc.)
While this pruning threshold may seemsimilar to the time-synchronous decoder pruning thresh-old, a conservative value only increases the stack size,but not the computation in the basic stack decoder.
Aconservative value would also increase the link tracingcomputation and the secondary heap size, but the com-putation is minor compared to the basic stack computa-tion.A second method of limiting the stack size is, a-priorichoosing some reasonable size and when it is about tooverflow, discard the worst theory.
This, in effect, dy-namically chooses a pruning threshold.
(It can be viewedas a fail-soft stack overflow.)
The standard heap doesnot support efficient location of the worst theory.
(Lo-cating the worst theory in a full heap requires earchingthe bottom row or about half the heap.)
It is possibleto modify a heap from its usual pyramid shape to a di-amond shape by attaching an upside down pyramid tothe bottom of the first pyramid.
This structure wouldhave the best theory at the top and the worst theory atthe bottom.
This would complicate the stack operationsomewhat, but it would probably be as efficient as mostother data structures.ConclusionTwo algorithms have been presented for acceleratingthe operation of a stack decoder.
The first is a methodfor computing the true least upper bound so that an op-timal admissible A* search can be performed.
The sec-ond is a set of methods for linearizing the computationrequired by a stack decoder.
The A* search has been203implemented in a continuous peech recognizer simula-tor and has demonstrated a significant speedup.
Thelinearizing algorithm has been partially implemented inthe simulator and has also shown significant computa-tional savings.AddendumJim Baker conjectured that the optimal A* search asdescribed above might not be admissible when a lan-guage model is used due to an interaction between theacoustic and the language model likelihoods \[3\] whichcan prevent lubL(t) from becoming the true lub.
Sucha loss of admissibility can result in the sentences beingoutput out of likelihood order.
This was tested by run-ning the simulator in top-N mode with and without alanguage model.
The test used 600 sentences from theDARPA Resource Management task and the word-pairlanguage model.
No recognition errors (the most likelysentence was not the first output) were observed for theno language model case and two errors were observed inthe word-pair language model case.
This verifies Baker'sconjecture, but suggests that the problem may be rela-tively rare.
It also offers empirical evidence that the nolanguage model case is admissible.\[9\] D. B. Paul, "Speech Recognition using HiddenMarkov Models," Lincoln Laboratory Journal, Vol.3, no.
1, Spring 1990.\[10\] A.
B. Poritz, "Hidden Markov Models: A GuidedTour," Proc.
ICASSP 88, April 1988.References\[1\] L. R. Bahl, F. 3elinek, and R. L. Mercer, "A Max-imum Likelihood Approach to Continuous SpeechRecognition," IEEE Trans.
Pattern Analysis andMachine Intelligence, PAMI-5, March 1983.\[2\] L. Bahl, P. S. Gopalakrishnam, D. Kanevsky, D.Nahamoo, "Matrix Fast Match: A Fast Method forIdentifying a Short List of Candidate Words for De-coding," ICASSP 89, Glasgow, May 1989.\[3\] J. K. Baker, personal communication, 25June 1990.\[4\] F. Jelinek, "A Fast Sequential Decoding AlgorithmUsing a Stack," IBM J. Res.
Develop., vol.
13,November 1969.\[5\] D. E. Knuth, "The Art of Computer Programming:Sorting and Searching,", Vol.
3., Addison-Wesly,Menlo Park, California, 1973.\[6\] B. T. Lowerre, "The HARPY Speech RecognitionSystem," PhD thesis, Computer Science Depart-ment, Carnegie Mellon University, April 1976.\[7\] N. 3.
Nilsson, "Problem-Solving Methods of Artifi-cial Intelligence," McGraw-Hill, New York, 1971.\[8\] D. B. Paul, "A CSR-NL Interface Specification,"Proceedings October, 1989 DARPA Speech andNatural Language Workshop, Morgan KaufmannPublishers, October, 1989.204
