Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1953?1964,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsLatent-Variable Synchronous CFGs for Hierarchical TranslationAvneesh Saluja and Chris DyerCarnegie Mellon UniversityPittsburgh, PA, 15213, USA{avneesh,cdyer}@cs.cmu.eduShay B. CohenUniversity of EdinburghEdinburgh EH8 9AB, UKscohen@inf.ed.ac.ukAbstractData-driven refinement of non-terminalcategories has been demonstrated to bea reliable technique for improving mono-lingual parsing with PCFGs.
In this pa-per, we extend these techniques to learnlatent refinements of single-category syn-chronous grammars, so as to improvetranslation performance.
We compare twoestimators for this latent-variable model:one based on EM and the other is a spec-tral algorithm based on the method of mo-ments.
We evaluate their performance on aChinese?English translation task.
The re-sults indicate that we can achieve signifi-cant gains over the baseline with both ap-proaches, but in particular the moments-based estimator is both faster and performsbetter than EM.1 IntroductionTranslation models based on synchronous context-free grammars (SCFGs) treat the translation prob-lem as a context-free parsing problem.
A parserconstructs trees over the input sentence by pars-ing with the source language projection of a syn-chronous CFG, and each derivation induces trans-lations in the target language (Chiang, 2007).However, in contrast to syntactic parsing, wherelinguistic intuitions can help elucidate the ?right?tree structure for a grammatical sentence, no suchintuitions are available for synchronous deriva-tions, and so learning the ?right?
grammars is acentral challenge.Of course, learning synchronous grammarsfrom parallel data is a widely studied problem(Wu, 1997; Blunsom et al., 2008; Levenberg etal., 2012, inter alia).
However, there has beenless exploration of learning rich non-terminal cat-egories, largely because previous efforts to learnsuch categories have been coupled with effortsto learn derivation structures?a computationallyformidable challenge.
One popular approach hasbeen to derive categories from source and/or targetmonolingual grammars (Galley et al., 2004; Zoll-mann and Venugopal, 2006; Hanneman and Lavie,2013).
While often successful, accurate parsersare not available in many languages: a more ap-pealing approach is therefore to learn the categorystructure from the data itself.In this work, we take a different approach toprevious work in synchronous grammar induc-tion by assuming that reasonable tree structuresfor a parallel corpus can be chosen heuristically,and then, fixing the trees (thereby enabling us tosidestep the worst of the computational issues), welearn non-terminal categories as latent variables toexplain the distribution of these synchronous trees.This technique has a long history in monolingualparsing (Petrov et al., 2006; Liang et al., 2007;Cohen et al., 2014), where it reliably yields state-of-the-art phrase structure parsers based on gen-erative models, but we are the first to apply it totranslation.We first generalize the concept of latent PCFGsto latent-variable SCFGs (?2).
We then followby a presentation of the tensor-based formulationfor our parameters, a representation that makes itconvenient to marginalize over latent states.
Sub-sequently, two methods for parameter estimationare presented (?4): a spectral approach based onthe method of moments, and an EM-based likeli-hood maximization.
Results on a Chinese?Englishevaluation set (?5) indicate significant gains overbaselines and point to the promise of using latent-variable synchronous grammars in conjunctionwith a smaller, simpler set of rules instead of un-wieldy and bloated grammars extracted via exist-ing heuristics, where a large number of context-independent but un-generalizable rules are uti-lized.
Hence, the hope is that this work pro-1953motes the move towards translation models thatdirectly model the conditional likelihood of trans-lation rules via (potentially feature-rich) latent-variable models which leverage information con-tained in the synchronous tree structure, insteadof relying on a heuristic set of features based onempirical relative frequencies (Koehn et al., 2003)from non-hierarchical phrase-based translation.2 Latent-Variable SCFGsBefore discussing parameter learning, we in-troduce latent-variable synchronous context-freegrammars (L-SCFGs) and discuss an inference al-gorithm for marginalizing over latent states.We extend the definition of L-PCFGs (Mat-suzaki et al., 2005; Petrov et al., 2006) to syn-chronous grammars as used in machine transla-tion (Chiang, 2007).
A latent-variable SCFG (L-SCFG) is a 6-tuple (N ,m, ns, nt, pi, t) where:?
N is a set of non-terminal (NT) symbols in thegrammar.
For hierarchical phrase-based transla-tion (HPBT), the set consists of only two sym-bols, X and a goal symbol S.?
[m] is the set of possible hidden states associ-ated with NTs.
Aligned pairs of NTs across thesource and target languages share the same hid-den state.?
[ns] is the set of source side words, i.e., thesource-side vocabulary, with [ns] ?N = ?.?
[nt] is the set of target side words, i.e., thetarget-side vocabulary, with [nt] ?N = ?.?
The synchronous production rules compose asetR = R0?R1?R2:?
Arity 2 (binary) rules (R2):a(h1)?
?
?1b(h2)?2c(h3)?3, ?1b(h2)?2c(h3)?3?ora(h1)?
?
?1b(h2)?2c(h3)?3, ?1c(h2)?2b(h3)?3?where a, b, c ?
N , h1, h2, h3?
[m],?1, ?2, ?3?
[ns]?and ?1, ?2, ?3?
[nt]?.?
Arity 1 (unary) rules (R1):a(h1)?
?
?1b(h2)?2, ?1b(h2)?2?where a, b ?
N , h1, h2?
[m], ?1, ?2?
[ns]?and ?, ?2?
[nt]?.?
Pre-terminal rules (R0): a(h1) ?
?
?, ?
?where a ?
N , ?
?
[nt]?and ?
?
[ns]?.Each of these rules is associated with a proba-bility t(a(h1) ?
?|a, h1) where ?
is the right-hand side (RHS) of the rule.?
For a ?
N , h ?
[m], pi(a, h) is a parameterspecifying the root probability of a(h).A skeletal tree (s-tree) for a sentence is the setof rules in the synchronous derivation of that sen-tence, without any additional latent state informa-tion or decoration.
A full tree consists of an s-tree r1, .
.
.
, rNtogether with values h1, .
.
.
, hNfor every NT in the tree.
An important point tokeep in mind in comparison to L-PCFGs is thatthe right-hand side (RHS) non-terminals of syn-chronous rules are aligned pairs across the sourceand target languages.In this work, we refine the one-category gram-mar introduced by Chiang (2007) for HPBT in or-der to learn additional latent NT categories.
Thus,the following discussion is restricted to these kindsof grammars, although the method is equally ap-plicable in other scenarios, e.g., the extended tree-to-string transducer (xRs) formalism (Huang etal., 2006; Graehl et al., 2008) commonly used insyntax-directed translation, and phrase-based MT(Koehn et al., 2003).Marginal Inference with L-SCFGs.
For a pa-rameter t of rule r, the latent state h1attached tothe left-hand side (LHS) NT of r is associated withthe outside tree for the sub-tree rooted at the LHS,and the states attached to the RHS NTs are asso-ciated with the inside trees of that NT.
Since wedo not assume conditional independence of thesestates, we need to consider all possible interac-tions, which can be compactly represented as a3rd-order tensor in the case of a binary rule, a ma-trix (i.e., a 2nd-order tensor) for unary rules, anda vector for pre-terminal (lexical) rules.
Prefer-ences for certain outside-inside tree combinationsare reflected in the values contained in these tensorstructures.
In this manner, we intend to capture in-teractions between non-local context of a phrase,which can typically be represented via features de-fined over outside trees of the node spanning thephrase, and the interior context, correspondinglydefined via features over the inside trees.
We re-fer to these tensor structures collectively as Crforrules r ?
R, which encompass the parameters t.For r ?
R0: Cr?
Rm?1; similarly forr ?
R1: Cr?
Rm?mand r ?
R2: Cr?Rm?m?m.
We also maintain a vector CS?
R1?mcorresponding to the parameters pi(S, h) for the1954Inputs: Sentence f1.
.
.
fN, L-SCFG (N , S,m, n), param-eters Cr?
R(m?m?m), ?
R(m?m), or ?
R(m?1)for allr ?
R, CS?
R(1?m), hypergraphH.Data structures:For each node q ?
H:?
?
(q) ?
Rm?1 is a column vector of inside terms.?
?
(q) ?
R1?m is a row vector of outside terms.?
For each incoming edge e ?
B(q) to node q, ?
(e) is amarginal probability for edge (rule) e.Algorithm:.
Inside ComputationFor nodes q in topological order inH,?
(q) = 0For each incoming edge e ?
B(q),tail = t(e), rule = r(e)if |tail| = 0, then ?
(q) = ?
(q) + C ruleelse if |tail| = 1, then ?
(q) = ?
(q) +Crule?1?
(tail0)else if |tail| = 2, then ?
(q) = ?
(q) +Crule?2?(tail1)?1?(tail0).
Outside ComputationFor q ?
H,?
(q) = 0?
(goal) = CSFor q in reverse topological order inH,For each incoming edge e ?
B(q),tail = t(e), rule = r(e)if |tail| = 1, then?
(tail0) = ?
(tail0) + ?
(q)?0Cruleelse if |tail| = 2, then?
(tail0) = ?
(tail0) +?(q)?0Crule?2?(tail1)?
(tail1) = ?
(tail1) +?(q)?0Crule?1?
(tail0).Edge MarginalsSentence probability g = ?(goal)?
?
(goal)For edge e ?
H,head = h(e), tail = t(e), rule = r(e)if |tail| = 0, then ?
(e) = (?
(head)?0Crule)/gelse if |tail| = 1, then ?
(e) = (?
(head) ?0Crule?1?
(tail0))/gelse if |tail| = 2, then ?
(e) = (?
(head) ?0Crule?2?(tail1)?1?
(tail0))/gFigure 1: The tensor form of the hypergraph inside-outside algorithm, for calculation of rule marginals ?(e).
Aslight simplification in the marginal computation yields NTmarginals for spans ?
(X, i, j).
B(q) returns the incoming hy-peredges for node q, and h(e), t(e), r(e) return the head node,tail nodes, and rule for hyperedge e.goal node (root).
These parameters participate intensor-vector operations: a 3rd-order tensor Cr2can be multiplied along each of its three modes(?0,?1,?2), and if multiplied by an m ?
1 vec-tor, will produce an m?m matrix.1Note that ma-trix multiplication can be represented by ?1whenmultiplying on the right and ?0when multiplyingon the left of the matrix.
The decoder computesmarginal probabilities for each skeletal rule in the1This operation is sometimes called a contraction.parse forest of a source sentence by marginaliz-ing over the latent states, which in practice corre-sponds to simple tensor-vector products.
This op-eration is not dependent on the manner in whichthe parameters were estimated.Figure 1 presents the tensor version of theinside-outside algorithm for decoding L-SCFGs.The algorithm takes as input the parse forest ofthe source sentence represented as a hypergraph(Klein and Manning, 2001), which is computedusing a bottom-up parser with Earley-style rulessimilar to the algorithm in Chiang (2007).
Hyper-graphs are a compact way to represent a forest ofmultiple parse trees.
Each node in the hypergraphcorresponds to an NT span, and can have multipleincoming and outgoing hyperedges.
Hyperedges,which connect one or more tail nodes to a singlehead node, correspond exactly to rules, and tail orhead nodes correspond to children (RHS NTs) orparent (LHS NT).
The function B(q) returns all in-coming hyperedges to a node q, i.e., all rules suchthat the LHS NT of the rule corresponds to the NTspan of the node q.
The algorithm computes insideand outside probabilities over the hypergraph us-ing the tensor representations, and converts theseprobabilities to marginal rule probabilities.
It issimilar to the version presented in Cohen et al.
(2014), but adapted to hypergraph parse forests.The complexity of this decoding algorithm isO(n3m3|G|) where n is the length of the inputsentence, m is the number of latent states, and |G|is the number of production rules in the grammarwithout latent-variable annotations (i.e., m = 1).2The bulk of the computation is a series of tensor-vector products of relatively small size (each di-mension is of length m), which can be computedvery quickly and in parallel.
The tensor computa-tions can be significantly sped up using techniquesdescribed by Cohen and Collins (2012), so thatthey are linear in m and not cubic.3 Derivation Trees for Parallel SentencesTo estimate the parameters t and pi of an L-SCFG (discussed in detail in the next section),we assume the existence of a dataset composedof synchronous s-trees, which can be acquiredfrom word alignments.
Normally in phrase-basedtranslation models, we consider all possible phrase2In practice, the term m3|G| can be replaced with asmaller term, which separates the rules inG by the number ofNTs on the RHS.
This idea relates to the notion of ?effectivegrammar size?
which we discuss in ?5.1955pairs consistent with the word alignments and es-timate features based on surface statistics associ-ated with the phrase pairs or rules.
The weights ofthese features are then learned using a discrimina-tive training algorithm (Och, 2003; Chiang, 2012,inter alia).
In contrast, in this work we restrictthe number of possible synchronous derivationsfor each sentence pair to just one; thus, derivationforests do not have to be considered, making pa-rameter estimation more tractable.3To achieve this objective, for each sentence inthe training data we extract the minimal set ofsynchronous rules consistent with the word align-ments, as opposed to the composed set of rules(Galley et al., 2006).
Composed rules are ones thatcan be formed from smaller rules in the grammar;with these rules, there are multiple synchronoustrees consistent with the alignments for a givensentence pair, and thus the total number of applica-ble rules can be combinatorially larger than if wejust consider the set of rules that cannot be formedfrom other rules, namely the minimal rules.
Therule types across all sentence pairs are combinedto form a minimal grammar.4To extract a set ofminimal rules, we use the linear-time extractionalgorithm of Zhang et al.
(2008).
We give a roughdescription of their method below, and refer thereader to the original paper for additional details.The algorithm returns a complete minimalderivation tree for each word-aligned sentencepair, and generalizes an approach for finding allcommon intervals (pairs of phrases such that noword pair in the alignment links a word insidethe phrase to a word outside the phrase) betweentwo permutations (Uno and Yagiura, 2000) to se-quences with many-to-many alignment links be-tween the two sides, as in word alignment.
Thekey idea is to encode all phrase pairs of a sen-tence alignment in a tree of size proportional tothe source sentence length, which they call thenormalized decomposition tree.
Each node cor-responds to a phrase pair, with larger phrase spansrepresented by higher nodes in the tree.
Construct-ing the tree is analogous to finding common in-tervals in two permutations, a property that theyleverage to propose a linear-time algorithm for tree3For future work, we will consider efficient algorithms forparameter estimation over derivation forests, since there maybe multiple valid ways to explain the sentence pair via a syn-chronous tree structure.4Table 2 presents a comparison of grammar sizes for ourexperiments (?5.1).extraction.
Converting the tree to a set of minimalSCFG rules for the sentence pair is straightfor-ward, by replacing nodes corresponding to spanswith lexical items or NTs in a bottom-up manner.5By using minimal rules as a starting pointinstead of the traditional heuristically-extractedrules (Chiang, 2007) or arbitrary compositions ofminimal rules (Galley et al., 2006), we are alsoable to explore the transition from minimal rulesto composed ones in a principled manner by en-coding contextual information through the latentstates.
Thus, a beneficial side effect of our re-finement process is the creation of more context-specific rules without increasing the overall sizeof the baseline grammar, instead holding this in-formation in our parameters Cr.4 Parameter Estimation for L-SCFGsWe explore two methods for estimating the param-eters Crof the model: a likelihood-maximizationapproach based on EM (Dempster et al., 1977),and a spectral approach based on the method ofmoments (Hsu et al., 2009; Cohen et al., 2014),where we identify a subspace using a singularvalue decomposition (SVD) of the cross-productfeature space between inside and outside trees andestimate parameters in this subspace.Figure 2 presents a side-by-side comparison ofthe two algorithms, which we discuss in this sec-tion.
In the spectral approach, we base our pa-rameter estimates on low-rank representations ofmoments of features, while EM explicitly maxi-mizes a likelihood criterion.
The parameter es-timation algorithms are relatively similar, but inlieu of sparse feature functions in the spectral case,EM uses partial counts estimated with the currentset of parameters.
The nature of EM allows it tobe susceptible to local optima, while the spectralapproach comes with guarantees on obtaining theglobal optimum (Cohen et al., 2014).
Lastly, com-puting the SVD and estimating parameters in thelow-rank space is a one-shot operation, as opposedto the iterative procedure of EM, and therefore ismuch more computationally efficient.4.1 Estimation with Spectral MethodWe generalize the parameter estimation algorithmpresented in Cohen et al.
(2013) to the syn-5We filtered rules with arity 3 and above (i.e., containingmore than 3 NTs on the RHS).
While the L-SCFG formalismis perfectly capable of handling such cases, it would have re-sulted in higher order tensors for our parameter structures.1956Inputs:Training examples (r(i), t(i,1), t(i,2), t(i,3), o(i), b(i))for i ?
{1 .
.
.M}, where r(i)is a context free rule;t(i,1), t(i,2), and t(i,3)are inside trees; o(i)is an out-side tree; and b(i)= 1 if the rule is at the root of tree,0 otherwise.
A function ?
that maps inside trees t tofeature-vectors ?
(t) ?
Rd.
A function ?
that mapsoutside trees o to feature-vectors ?
(o) ?
Rd?.Algorithm:.
Step 0: Singular Value Decomposition?
Compute the SVD of Eq.
1 to calculate matri-ces?U ?
R(d?m)and?V ?
R(d??m)..
Step 1: ProjectionY (t) = U>?
(t)Z(o) = ??1V>?(o).
Step 2: Calculate Correlations?Er=???????
?o?QrZ(o)|Qr|if r ?
R0?
(o,t)?QrZ(o)?Y (t)|Qr|if r ?
R1?
(o,t2,t3)?QrZ(o)?Y (t2)?Y (t3)|Qr|if r ?
R2Qris the set of outside-inside tree triples for binaryrules, outside-inside tree pairs for unary rules, andoutside trees for pre-terminals..
Step 3: Compute Final Parameters?
For all r ?
R,?Cr=count(r)M??Er?
For all r(i)?
{1, .
.
.
,M} such that b(i)is 1,?CS=?CS+Y (t(i,1))|QS|QSis the set of trees at the root.
(a) The spectral learning algorithm for estimating pa-rameters of an L-SCFG.Inputs:Training examples (r(i), t(i,1), t(i,2), t(i,3), o(i), b(i)) for i ?
{1 .
.
.M}, where r(i)is a context free rule; t(i,1), t(i,2), andt(i,3)are inside trees; o(i)is an outside tree; b(i)= 1 if the ruleis at the root of tree, 0 otherwise; and MAX ITERATIONS.Algorithm:.
Step 0: Parameter InitializationFor rule r ?
R,?
if r ?
R0: initialize?Cr?
Rm?1?
if r ?
R1: initialize?CrRm?m?
if r ?
R2: initialize?CrRm?m?mInitialize?CS?
Rm?1?Cr0=?Cr,?CS0=?CSFor iteration t = 1, .
.
.
,MAX ITERATIONS,?
Expectation Step:.
Estimate Y and ZCompute partial counts and total tree probabili-ties g for all t and o using Fig.
1 and parameters?Crt?1,?CSt?1..
Calculate Correlations?Er=?????????
?o,g?QrZ(o)gif r ?
R0?
(o,t,g)?QrZ(o)?Y (t)gif r ?
R1?
(o,t2,t3,g)?QrZ(o)?Y (t2)?Y (t3)gif r ?
R2.
Update ParametersFor all r ?
R,?Crt=?Crt?1?ErFor all r(i)?
{1, .
.
.
,M} such that b(i)is 1,?CSt=?CSt+ (?CSt?1Y (r(i)))/gQSis the set of trees at the root.?
Maximization Stepif r ?
R0: ?h1:?Cr(h1) =?Cr(h1)?r?=r?h1?Cr?
(h1)if r ?
R1: ?h1, h2:?Cr(h1, h2) =?Cr(h1,h2)?r?=r?h2?Cr?
(h1,h2)if r ?
R2: ?h1, h2, h3:?Cr(h1, h2, h3) =?Cr(h1,h2,h3)?r?=r?h2,h3?Cr?
(h1,h2,h3)if LHS(r) = S: ?h1:?Cr(h1) =?Cr(h1)?r?=r?h1?Cr?
(h1)(b) The EM-based algorithm for estimating parameters of an L-SCFG.Figure 2: The two parameter estimation algorithms proposed for L-SCFGs; (a) method of moments; (b) expectation maxi-mization.
is the element-wise multiplication operator.chronous or bilingual case.
The central conceptof the spectral parameter estimation algorithm isto learn an m-dimensional representation of in-side and outside trees by defining these trees interms of features, in combination with a projectionstep (SVD), with the hope being that the lower-dimensional space captures the syntactic and se-mantic regularities among rules from the sparsefeature space.
Every NT in an s-tree has an as-sociated inside and outside tree; the inside treecontains the entire sub-tree at and below the NT,and the outside tree is everything else in the syn-chronous s-tree except the inside tree.
The insidefeature function ?
maps the domain of inside tree1957fragments to a d-dimensional Euclidean space,and the outside feature function ?
maps the do-main of outside tree fragments to a d?-dimensionalspace.
The specific features we used are discussedin ?5.2.Let O be the set of all tuples of inside-outsidetrees in our training corpus, whose size is equiva-lent to the number of rule tokens (occurrences inthe corpus)M , and let ?
(t) ?
Rd?1, ?
(o) ?
Rd?
?1be the inside and outside feature functions for in-side tree t and outside tree o.
By computing theouter product ?
between the inside and outsidefeature vectors for each pair and aggregating, weobtain the empirical inside-outside feature covari-ance matrix:??
=1|O|?(o,t)?O?
(t) (?
(o))>(1)If m is the desired latent space dimension, wecompute an m-rank truncated SVD of the empir-ical covariance matrix??
?
U?V>, where U ?Rd?mand V ?
Rd?
?mare the matrices containingthe left and right singular vectors, and ?
?
Rm?mis a diagonal matrix containing the m-largest sin-gular values along its diagonal.Figure 2a provides the remaining steps in thealgorithm.
The M training examples are obtainedby considering all nodes in all of the synchronouss-trees given as input.
In step 1, for each insideand outside tree, we project its high-dimensionalrepresentation to the m-dimensional latent space.Using the m-dimensional representations for in-side and outside trees, in step 2 for each rule type rwe compute the covariance between the inside treevectors and the outside tree vector using the ten-sor product, a generalized outer product to com-pute covariances between more than two randomvectors.
For binary rules, with two child insidevectors and one outside vector, the result?Eris a3-mode tensor; for unary rules, a regular matrix,and for pre-terminal rules with no right-hand sidenon-terminals, a vector.
The final parameter es-timate is then the associated tensor/matrix/vector,scaled by the maximum likelihood estimate of therule r, as in step 3.The corresponding theoretical guarantees fromCohen et al.
(2014) can also be generalized tothe synchronous case.??
is an empirical esti-mate of the true covariance matrix ?, and if ?has rank m, then the marginals computed usingthe spectrally-estimated parameters will convergeto the true marginals, with the sample complexityfor convergence inversely proportional to a poly-nomial function of the mthlargest singular valueof ?.4.2 Estimation with EMA likelihood maximization approach can also beused to learn the parameters of an L-SCFG.
Pa-rameters are initialized by sampling each param-eter value?Cr(h1, h2, h3) from the interval [0, 1]uniformly at random.6We first decode the train-ing corpus using an existing set of parameters tocompute the inside and outside probability vectorsassociated with NTs for every rule in each s-tree,constrained to the tree structure of the training ex-ample.
These probabilities can be computed us-ing the decoding algorithm in Figure 1 (where ?and ?
correspond to the inside and outside proba-bilities respectively), except the parse forest con-sists of a single tree only.
These vectors repre-sent partial counts over latent states.
We then de-fine functions Y and Z (analogous to the spectralcase) which map inside and outside tree instancesto m-dimensional vectors containing these partialcounts.
In the spectral case, Y and Z are estimatedjust once, while in the case of EM they have to bere-estimated at each iteration.The expectation step thus consists of comput-ing the partial counts of inside and outside trees tand o, i.e., recovering the functions Y and Z, andupdating parameters Crby computing correla-tions, which involves summing over partial counts(across all occurrences of a rule in the corpus).Each partial count?s contribution is divided by anormalization factor g, which is the total probabil-ity of the tree which t or o is part of.
Note thatunlike the spectral case, there is a specific normal-ization factor for each inside-outside tuple.
Lastly,the correlations are scaled by the existing parame-ter estimates.To obtain the next set of parameters, in the max-imization step we normalize?Crfor r ?
R suchthat for every h1,?r?=r,h2,h3?Cr?
(h1, h2, h3) = 1for r ?
R2,?r?=r,h2?Cr?
(h1, h2) = 1 for r ?
R1,and?r?=r,h2?Cr?
(h2) = 1 for r ?
R0.
Wealso normalize the root rule parameters?CrwhereLHS(r) = S. It is also possible to add sparse,overlapping features to an EM-based estimation6In our experiments, we also tried the initializationscheme described in Matsuzaki et al.
(2005), but found that itprovided little benefit.1958procedure (Berg-Kirkpatrick et al., 2010) and weleave this extension for future work.5 ExperimentsThe goal of the experimental section is to evalu-ate the performance of the latent-variable SCFGin comparison to a baseline without any additionalNT annotations (MIN-GRAMMAR), and to com-pare the performance of the two parameter esti-mation algorithms.
We also compare L-SCFGs toa HIERO baseline (Chiang, 2007).
The languagepair of evaluation is Chinese?English (ZH-EN).We score translations using BLEU (Papineniet al., 2002).
The latent-variable model is inte-grated into the standard MT pipeline by comput-ing marginal probabilities for each rule in the parseforest of a source sentence using the algorithm inFigure 1 with the parameters estimated throughthe algorithms in Figure 2, and is added as a fea-ture for the rule during MERT (Och, 2003).
Theseprobabilities are conditioned on the LHS (X), andare thus joint probabilities for a source-target RHSpair.
We also write out as features the condi-tional relative frequencies?P (e|f) and?P (f |e) asestimated by our latent-variable model, i.e., con-ditioned on the source and target RHS.Overall, we find that both the spectral andthe EM-based estimators improve upon a mini-mal grammar baseline with only a single cate-gory, but the spectral approach does better.
In fact,it matches the performance of the standard HI-ERO baseline, despite learning on top of a minimalgrammar.5.1 Data and BaselinesThe ZH-EN data is the BTEC parallel corpus(Paul, 2009); we combine the first and seconddevelopment sets in one, and evaluate on the thirddevelopment set.
The development and test setsare evaluated with 16 references.
Statistics forthe data are shown in Table 1.
We used the CDECdecoder (Dyer et al., 2010) to extract word align-ments and the baseline hierarchical grammars,MERT tuning, and decoding.
We used a 4-gramlanguage model built from the target-side of theparallel training data.
The Python-based imple-mentation of the tensor-based decoder, as well asthe parameter estimation algorithms is available atgithub.com/asaluja/spectral-scfg/.The baseline HIERO system uses a grammar ex-tracted by applying the commonly used heuris-ZH-ENTRAIN (SRC) 334KTRAIN (TGT) 366KDEV (SRC) 7KDEV (TGT) 7.6KTEST (SRC) 3.8KTEST (TGT) 3.9KTable 1: Corpus statistics (in words).
For the target DEV andTEST statistics, we take the first reference.tics (Chiang, 2007).
Each rule is decorated withtwo lexical and phrasal features corresponding tothe forward (e|f) and backward (f |e) conditionallog frequencies, along with the log joint frequency(e, f), the log frequency of the source phrase (f),and whether the phrase pair or the source phraseis a singleton.
Weights for the language model(and language model OOV), glue rule, and wordpenalty are also tuned.
The MIN-GRAMMARbaseline7maintains the same set of weights.Grammar Number of RulesHIERO 1.69MMIN-GRAMMAR 59KLV m = 1 27.56KLV m = 8 3.18MLV m = 16 22.22MTable 2: Grammar sizes for the different systems; for thelatent-variable models, effective grammar sizes are provided.Grammar sizes are presented in Table 2.
Forthe latent-variable models, we provide the effec-tive grammar size, where the number of NTs onthe RHS of a rule is taken into account when com-puting the grammar size, by assuming each possi-ble latent variable configuration amongst the NTsgenerates a different rule.
Furthermore, all single-tons are mapped to the OOV rule, while we in-clude singletons in MIN-GRAMMAR.8Hence, ef-fective grammar size can be computed as m(1 +|R>10|) +m2|R1|+m3|R2|, whereR>10is the setof pre-terminal rules that occur more than once.5.2 Spectral FeaturesWe use the following set of sparse, binary featuresin the spectral learning process:7Code to extract the minimal derivation trees is availableat www.cs.rochester.edu/u/gildea/mt/.8This OOV mapping is done so that the latent-variablemodel can handle unknown tokens.1959?
Rule Indicator.
For the inside features, we con-sider the rule production containing the currentnon-terminal on the left-hand side, as well asthe rules of the children (distinguishing betweenleft and right children for binary rules).
Forthe outside features, we consider the parent ruleproduction along with the rule production of thesibling (if it exists).?
Lexical.
for both the inside and outside fea-tures, any lexical items that appear in the ruleproductions are recorded.
Furthermore, we con-sider the first and last words of spans (left andright child spans for inside features, distinguish-ing between the two if both exist, and siblingspan for outside features).
Source and targetwords are treated separately.?
Length.
the span length of the tree and eachof its children for inside features, and the spanlength of the parent and sibling for outside fea-tures.In our experiments, we instantiated a total of170,000 rule indicator features, 155,000 lexicalfeatures, and 80 length features.5.3 Chinese?English ExperimentsTable 3 presents a comprehensive evaluation of theZH-EN experimental setup.
The first section con-sists of the various baselines we consider.
In ad-dition to the aforementioned baselines, we eval-uated a setup where the spectral parameters sim-ply consist of the joint maximum likelihood esti-mates of the rules.
This baseline should performen par with MIN-GRAMMAR, which we see is thecase on the development set.
The performanceon the test set is better though, primarily becausewe also include the reverse log relative frequency(f |e) computed from the latent-variable model asan additional feature in MERT.
Furthermore, inline with previous work (Galley et al., 2006) whichcompares minimal and composed rules, we findthat minimal grammars take a hit of more than 2.5BLEU points on the development set, compared tocomposed (HIERO) grammars.
The m = 1 spec-tral baseline with only rule indicator features per-forms slightly better than the minimal grammarbaseline, since it overtly takes into account inside-outside tree combination preferences in the param-eters, but improvement is minimal with one latentstate naturally and the performance on the test setis in line with the MLE baseline.On top of the baselines, we looked at a numberBLEUSetup Dev TestBaselinesHIERO 46.08 55.31MIN-GRAMMAR43.38 51.78MLE 43.24 52.80Spectralm = 1 RI 44.18 52.62m = 8 RI 44.60 53.63m = 16 RI 46.06 55.83m=16 RI+Lex+Sm 46.08 55.22m=16 RI+Lex+Len 45.70 55.29m=24 RI+Lex 43.00 51.28m=32 RI+Lex 43.06 52.16EMm = 8 40.53 (0.2) 49.78 (0.5)m = 16 42.85 (0.2) 52.93 (0.9)m = 32 41.07 (0.4) 49.95 (0.7)Table 3: Results for the ZH-EN corpus, comparing acrossthe baselines and the two parameter estimation techniques.RI, Lex, and Len correspond to the rule indicator, lexical,and length features respectively, and Sm denotes smoothing.For the EM experiments, we selected the best scoring iter-ation by tuning weights for parameters obtained after 25 it-erations and evaluating other parameters with these weights.Results for EM are averaged over 5 starting points, with stan-dard deviation given in parentheses.
Spectral, EM, and MLEperformances compared to the MIN-GRAMMAR baseline arestatistically significant (p < 0.01).of feature combinations and latent states for thespectral and EM-estimated latent-variable models.For the spectral models, we tuned MERT parame-ters separately for each rank on a set of parametersestimated from rule indicator features only; subse-quent variations within a given rank, e.g., the ad-dition of lexical or length features or smoothing,were evaluated with the same set of rank-specificweights from MERT.
For EM, we ran parame-ter estimation with 5 randomly initialized startingpoints for 50 iterations; we tuned the MERT pa-rameters with EM parameters obtained after 25thiterations.
Similar to the spectral experiments,we fixed the MERT weight values and evaluatedBLEU performance with parameters after every 5iterations and chose the iteration with the highestscore on the development set.
The results are av-eraged over the 5 initializations, with standard de-viation in parentheses.Firstly, we can see a clear dependence on rank,with peak performance for the spectral and EMmodels occurring at m = 16.
In this instance, thespectral model roughly matches the performanceof the HIERO baseline, but it only uses rules ex-tracted from a minimal grammar, whose size is afraction of the HIERO grammar.
The gains seemto level off at this rank; additional ranks seem toadd noise to the parameters.
Feature-wise, addi-tional lexical and length features add little, prob-1960ably because much of this information is encap-sulated in the rule indicator features.
For EM,m = 16 outperforms the minimal grammar base-line, but is not at the level of the spectral results.All EM, spectral, and MLE results are statisticallysignificant (p < 0.01) with respect to the MIN-GRAMMAR baseline (Zhang et al., 2004), and theimprovement over the HIERO baseline achieved bythem = 16 rule indicator configuration is also sta-tistically significant.The two estimation algorithms differ signifi-cantly in their estimation time.
Given a featurecovariance matrix, the spectral algorithm (SVD,which was done with Matlab, and correlation com-putation steps) for m = 16 took 7 minutes, whilethe EM algorithm took 5 minutes for each iterationwith this rank.5.4 AnalysisFigure 3 presents a comparison of the non-terminal span marginals for two sentences in thedevelopment set.
We visualize these differencesthrough a heat map of the CKY parse chart, wherethe starting word of the span is on the rows, andthe span end index is on the columns.
Each cell isshaded to represent the marginal of that particularnon-terminal span, with higher likelihoods in blueand lower likelihoods in red.For the most part, marginals at the leaves (i.e.,pre-terminal marginals) tend to score relativelysimilarly across different setups.
Higher up in thechart, the latent SCFG marginals look quite dif-ferent than the MLE parameters.
Most noticeably,spans starting at the beginning of the sentence aremuch more favored.
It is these rules that allowthe right translation to be preferred since the MLEchooses not to place the object of the sentence inthe subject?s span.
However, the spectral param-eters seem to discriminate between these higher-level rules better than EM, which scores spansstarting with the first word uniformly highly.
An-other interesting point is that the range of likeli-hoods is much larger in the EM case compared tothe MLE and spectral variants.
For the second sen-tence (row), the 1-best hypothesis produced by allsystems are the same, but the heat map accentuatesthe previous observation.6 Related WorkThe goal of refining single-category HPBT gram-mars or automatically learning the NT categoriesin a grammar, instead of relying on noisy parseroutputs, has been explored from several differentangles in the MT literature.
Blunsom et al.
(2008)present a Bayesian model for synchronous gram-mar induction, and place an appropriate nonpara-metric prior on the parameters.
However, theirstarting point is to estimate a synchronous gram-mar with multiple categories from parallel data(using the word alignments as a prior), while weaim to refine a fixed grammar with additional la-tent states.
Furthermore, their estimation proce-dure is extremely expensive and is restricted tolearning up to five NT categories, via a series ofmean-field approximations.Another approach is to explicitly attach a real-valued vector to each NT: Huang et al.
(2010) usean external source-language parser for this pur-pose and score rules based on the similarity be-tween a source sentence parse and the informationcontained in this vector, which explicitly requiresthe integration of a good-quality source-languageparser.
The EM-based algorithm that we proposehere is similar to what they propose, except that weneed to handle tensor structures.
Mylonakis andSima?an (2011) select among linguistically moti-vated non-terminal labels with a cross-validatedversion of EM.
Although they consider a restrictedhypothesis space, they do marginalize over dif-ferent derivations therefore their inside-outside al-gorithm is O(n6).
In the syntax-directed trans-lation literature, there have been efforts to relaxor coarsen the hard labels provided by a syntacticparser in an automatic manner to promote param-eter sharing (Venugopal et al., 2009; Hannemanand Lavie, 2013), which is the complement of ouraim in this paper.The idea of automatically learned grammar re-finements comes from the monolingual parsing lit-erature, where phenomena like head lexicalizationcan be modeled through latent variables.
Mat-suzaki et al.
(2005) look at a likelihood-basedmethod to split the NT categories of a gram-mar into a fixed number of sub-categories, whilePetrov et al.
(2006) learn a variable number ofsub-categories per NT.
The latter?s extension maybe useful for finding the optimal number of latentstates from the data in our case.The question of whether we can incorporate ad-ditional contextual information in minimal rulegrammars in MT via auxiliary models instead ofusing longer, composed rules has been investi-gated before as well.
n-gram translation mod-19610 1 2 3 4Span EndSpanstarting at word:1.050.900.750.600.450.300.150.00ln(sum)I go away .
(a) MLE0 1 2 3 4Span EndSpanstarting at word:2.001.751.501.251.000.750.500.250.00ln(sum)I ?ll bring it .
(b) Spectral m = 16 RI0 1 2 3 4Span EndSpanstarting at word:9876543210ln(sum)I ?ll bring it .
(c) EM m = 160 1 2 3 4 5 6 7Span EndSpanstarting at word:2.001.751.501.251.000.750.500.250.00ln(sum)I ?d like a shampoo and style .
(d) MLE0 1 2 3 4 5 6 7Span EndSpanstarting at word:3.22.82.42.01.61.20.80.40.0ln(sum)I ?d like a shampoo and style .
(e) Spectral m = 16 RI0 1 2 3 4 5 6 7Span EndSpanstarting at word:10.59.07.56.04.53.01.50.0ln(sum)I ?d like a shampoo and style .
(f) EM m = 16Figure 3: A comparison of the CKY charts containing marginal probabilities of non-terminal spans ?
(X, i, j) for the MLE,spectral m = 16 with rule indicator features, and EM m = 16, for the two Chinese sentences.
Higher likelihoods are in blue,lower likelihoods in red.
The hypotheses produced by each setup are below the heat maps.els (Mari?no et al., 2006; Durrani et al., 2011)seek to model long-distance dependencies and re-orderings through n-grams.
Similarly, Vaswaniet al.
(2011) use a Markov model in the contextof tree-to-string translation, where the parametersare smoothed with absolute discounting (Ney etal., 1994), while in our instance we capture thissmoothing effect through low rank or latent states.Feng and Cohn (2013) also utilize a Markov modelfor MT, but learn the parameters through a moresophisticated estimation technique that makes useof Pitman-Yor hierarchical priors.Hsu et al.
(2009) presented one of the initialefforts at spectral-based parameter estimation (us-ing SVD) of observed moments for latent-variablemodels, in the case of Hidden Markov models.This idea was extended to L-PCFGs (Cohen et al.,2014), and our approach can be seen as a bilingualor synchronous generalization.7 ConclusionIn this work, we presented an approach to re-fine synchronous grammars used in MT by in-ferring the latent categories for the single non-terminal in our grammar rules, and proposed twoalgorithms to estimate parameters for our latent-variable model.
By fixing the synchronous deriva-tions of each parallel sentence in the training data,it is possible to avoid many of the computationalissues associated with synchronous grammar in-duction.
Improvements over a minimal grammarbaseline and equivalent performance to a hierar-chical phrase-based baseline are achieved by thespectral approach.
For future work, we will seekto relax this consideration and jointly reason aboutnon-terminal categories and derivation structures.AcknowledgementsThe authors would like to thank Daniel Gildeafor sharing his code to extract minimal derivationtrees, Stefan Riezler for useful discussions, Bren-dan O?Connor for the CKY visualization advice,and the anonymous reviewers for their feedback.This work was supported by a grant from eBayInc.
(Saluja), the U. S. Army Research Laboratoryand the U. S. Army Research Office under con-tract/grant number W911NF-10-1-0533 (Dyer).1962ReferencesTaylor Berg-Kirkpatrick, Alexandre Bouchard-C?ot?e,John DeNero, and Dan Klein.
2010.
Painless un-supervised learning with features.
In Proceedings ofNAACL.Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008.Bayesian Synchronous Grammar Induction.
In Pro-ceedings of NIPS.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228,June.David Chiang.
2012.
Hope and Fear for Dis-criminative Training of Statistical Translation Mod-els.
Journal of Machine Learning Research, pages1159?1187.Shay B. Cohen and Michael Collins.
2012.
Tensordecomposition for fast parsing with latent-variablePCFGs.
In Proceedings of NIPS.Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.Foster, and Lyle Ungar.
2013.
Experiments withspectral learning of latent-variable PCFGs.
In Pro-ceedings of NAACL.Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.Foster, and Lyle Ungar.
2014.
Spectral learningof latent-variable PCFGs: Algorithms and samplecomplexity.
Journal of Machine Learning Research.Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-bin.
1977.
Maximum likelihood from incompletedata via the EM algorithm.
Journal of the Royal Sta-tistical Society, Series B, 39(1):1?38.Nadir Durrani, Helmut Schmid, and Alexander Fraser.2011.
A joint sequence translation model with inte-grated reordering.
In Proceedings of ACL.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JohnathanWeese, Ferhan Ture, Phil Blunsom, Hendra Seti-awan, Vladimir Eidelman, and Philip Resnik.
2010.cdec: A decoder, alignment, and learning frameworkfor finite-state and context-free translation models.In Proceedings of ACL.Yang Feng and Trevor Cohn.
2013.
A Markovmodel of machine translation using non-parametricbayesian inference.
In Proceedings of ACL.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proceedings of HLT-NAACL.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Pro-ceedings of ACL.Jonathan Graehl, Kevin Knight, and Jonathan May.2008.
Training tree transducers.
ComputationalLinguistics, 34(3):391?427, September.Greg Hanneman and Alon Lavie.
2013.
Improvingsyntax-augmented machine translation by coarsen-ing the label set.
In Proceedings of NAACL.Daniel Hsu, Sham M. Kakade, and Tong Zhang.
2009.A Spectral Algorithm for Learning Hidden MarkovModels.
In Proceedings of COLT.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of AMTA.Zhongqiang Huang, Martin?Cmejrek, and BowenZhou.
2010.
Soft syntactic constraints for hierar-chical phrase-based translation using latent syntacticdistributions.
In Proceedings of EMNLP.Dan Klein and Christopher D. Manning.
2001.
Parsingand hypergraphs.
In Proceedings of IWPT.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of NAACL.Abby Levenberg, Chris Dyer, and Phil Blunsom.
2012.A Bayesian model for learning SCFGs with discon-tiguous rules.
In Proceedings of EMNLP-CoNLL.Percy Liang, Slav Petrov, Michael I. Jordan, and DanKlein.
2007.
The infinite PCFG using hierarchicaldirichlet processes.
In Proceedings of EMNLP.Jos?e B. Mari?no, Rafael E. Banchs, Josep M. Crego,Adri`a de Gispert, Patrik Lambert, Jos?e A. R. Fonol-losa, and Marta R. Costa-juss`a.
2006.
N-gram-based machine translation.
Computational Linguis-tics, 32(4):527?549, December.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic CFG with latent annotations.
InProceedings of ACL.Markos Mylonakis and Khalil Sima?an.
2011.
Learn-ing hierarchical translation structure with linguisticannotations.
In Proceedings of ACL.Hermann Ney, Ute Essen, and Reinhard Kneser.1994.
On Structuring Probabilistic Dependencies inStochastic Language Modelling.
Computer Speechand Language, 8:1?38.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proceedingsof ACL.Michael Paul.
2009.
Overview of the IWSLT 2009evaluation campaign.
In Proceedings of IWSLT.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of ACL.1963Takeaki Uno and Mutsunori Yagiura.
2000.
Fast al-gorithms to enumerate all common intervals of twopermutations.
Algorithmica, 26(2):290?309.Ashish Vaswani, Haitao Mi, Liang Huang, and DavidChiang.
2011.
Rule Markov models for fast tree-to-string translation.
In Proceedings of ACL.Ashish Venugopal, Andreas Zollmann, Noah A. Smith,and Stephan Vogel.
2009.
Preference grammars:Softening syntactic constraints to improve statisticalmachine translation.
In Proceedings of NAACL.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403, Septem-ber.Ying Zhang, Stephan Vogel, and Alex Waibel.
2004.Interpreting BLEU/NIST scores: How much im-provement do we need to have a better system.
InIn Proceedings LREC.Hao Zhang, Daniel Gildea, and David Chiang.
2008.Extracting synchronous grammar rules from word-level alignments in linear time.
In Proceedings ofCOLING.Andreas Zollmann and Ashish Venugopal.
2006.
Syn-tax augmented machine translation via chart pars-ing.
In Proceedings of the Workshop on StatisticalMachine Translation, StatMT ?06, pages 138?141.Association for Computational Linguistics.1964
