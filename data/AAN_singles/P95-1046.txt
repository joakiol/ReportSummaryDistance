Knowledge-based Automatic Topic IdentificationChin-Yew LinDepartment of Electrical Engineering/SystemUniversity of Southern CaliforniaLos Angeles, CA 90089-2562, USAchinyew~pollux.usc.eduAbstractAs the first step in an automated text sum-marization algorithm, this work presentsa new method for automatically identi-fying the central ideas in a text basedon a knowledge-based concept countingparadigm.
To represent and generalizeconcepts, we use the hierarchical concepttaxonomy WordNet.
By setting appropri-ate cutoff values for such parameters asconcept generality and child-to-parent fre-quency ratio, we control the amount andlevel of generality of concepts extractedfrom the text.
11 In t roduct ionAs the amount of text available online keeps grow-ing, it becomes increasingly difficult for people tokeep track of and locate the information of inter-est to them.
To remedy the problem of informationoverload, a robust and automated text summarizeror information extrator is needed.
Topic identifica-tion is one of two very important steps in the processof summarizing a text; the second step is summarytext generation.A topic is a particular subject that we write aboutor discuss.
(Sinclair et al, 1987).
To identifythe topics of texts, Information Retrieval (IR) re-searchers use word frequency, cue word, location,and title-keyword techniques (Paice, 1990).
Amongthese techniques, only word frequency counting canbe used robustly across different domains; the othertechniques rely on stereotypical text structure or thefunctional structures of specific domains.Underlying the use of word frequency is the as-sumption that the more a word is used in a text,the more important it is in that text.
This method1This research was funded in part by ARPA under or-der number 8073, issued as Maryland Procurement Con-tract # MDA904-91-C-5224 and in part by the NationalScience Foundation Grant No.
MIP 8902426.recognizes only the literal word forms and noth-ing else.
Some morphological processing may help,but pronominalization a d other forms of coreferen-tiality defeat simple word counting.
Furthermore,straightforward word counting can be misleadingsince it misses conceptual generalizations.
For exam-ple: "John bought some vegetables, fruit, bread, andmilk."
What would be the topic of this sentence?We can draw no conclusion by using word countingmethod; where the topic actually should be: "Johnbought some groceries."
The problem is that wordcounting method misses the important concepts be-hind those words: vegetables, fruit, etc.
relates togroceries at the deeper level of semantics.
In rec-ognizing the inherent problem of the word countingmethod, recently people have started to use artifi-cial intelligence techniques (Jacobs and ttau, 1990;Mauldin, 1991) and statistical techniques (Saltonet al, 1994; Grefenstette, 1994) to incorporate thesementic relations among words into their applica-tions.
Following this trend, we have developed a newway to identify topics by counting concepts insteadof words.2 The  Power  o f  Genera l i za t ionIn order to count concept frequency, we employ aconcept generalization taxonomy.
Figure 1 shows apossible hierarchy for the concept digital computer.According to this hierarchy, if we find iaptop andhand-held computer, in a text, we can infer that thetext is about portable computers, which is their par-ent concept.
And if in addition, the text also men-tions workstation and mainframe, it is reasonable tosay that the topic of the text is related to digitalcomputer.Using a hierarchy, the question is now how to findthe most appropriate generalization.
Clearly we can-not just use the leaf concepts - -  since at this level wehave gained no power from generalization.
On theother hand, neither can we use the very top concept- -  everything is a thing.
We need a method of iden-tifying the most appropriate concepts omewhere inmiddle of the taxonomy.
Our current solution uses308~ m p u t e r  Workstation PC~ er MainframePor t~ktop  computerHand-held computer Laptop computerFigure 1: A sample hierarchy for computerconcept frequency ratio and starting depth.2.1 Branch  Rat io  Thresho ldWe call the frequency of occurrence of a concept Cand it's subconcepts in a text the concept's weight 2.We then define the ratio T~,at any concept C, as fol-lows:7~ = MAX(weight of all the direct children of C)SUM(weight of all the direct children of C)7~ is a way to identify the degree of summarizationinformativeness.
The higher the ratio, the less con-cept C generalizes over many children, i.e., the moreit reflects only one child.
Consider Figure 2.
In case(a) the parent concept's ratio is 0.70, and in case (b),it is 0.3 by the definition of 7~.
To generate a sum-mary for case (a), we should simply choose App leas the main idea instead of its parent concept, sinceit is by far the most mentioned.
In contrast, in case(b), we should use the parent concept ComputerCompany as the concept of interest.
Its small ra-tio, 0.30, tells us that if we go down to its children,we will lose too much important information.
Wedefine the branch ratio threshold (T~t) to serve as acutoff point for the determination of interestingness,i.e., the degree of generalization.
We define that if aconcept's ratio T?
is less than 7~t, it is an interestingconcept.2.2 S tar t ing  DepthWe can use the ratio to find all the possible inter-esting concepts in a hierarchical concept axonomy.If we start from the top of a hierarchy and pro-ceed downward along each child branch wheneverthe branch ratio is greater than or equal to 7~t, wewill eventually stop with a list of interesting con-cepts.
We call these interesting concepts the inter-esting wave front.
We can start another explorationof interesting concepts downward from this interest-ing wavefront resulting in a second, lower, wavefront,and so on.
By repeating this process until we reachthe leaf concepts of the hierarchy, we can get a setof interesting wavefronts.
Among these interesting2According to this, a parent concept always hasweight greater or equal to its maximum weighted irectchildren.
A concept itself is considered as its own directchild.
(io)Toshiba(0) NEC(1) Compaq(1) Apple(7) IBM(l)= ~ ( 1 0 )Toshiba(2) NEC(2) Compaq(3) Apple(2) IBM(l)Figure 2: Ratio and degree of generalizationwavefronts, which one is the most appropriate forgeneration of topics?
It is obvious that using theconcept counting technique we have suggested sofar, a concept higher in the hierarchy tends to bemore general.
On the other hand, a concept lowerin the hierarchy tends to be more specific.
In orderto choose an adequate wavefront with appropriategeneralization, we introduce the parameter startingdepth, l)~.
We require that the branch ratio criteriondefined in the previous section can only take effectafter the wavefront exceeds the starting depth; thefirst subsequent interesting wavefront generated willbe our collection of topic concepts.
The appropri-ate ~Da is determined by experimenting with differentvalues and choosing the best one.3 Exper imentWe have implemented a prototype system to testthe automatic topic identification algorithm.
As theconcept hierarchy, we used the noun taxonomy fromWordNet 3 (Miller et al, 1990).
WordNet has beenused for other similar tasks, such as (Resnik, 1993)For input texts, we selected articles about informa-tion processing of average 750 words each out ofBusiness Weck (93-94).
We ran the algorithm on50 texts, and for each text extracted eight sentencescontaining the most interesting concepts.How now to evaluate the results?
For each text,we obtained a professional's abstract from an onlineservice.
Each abstract contains 7 to 8 sentences onaverage.
In order to compare the system's electionwith the professional's, we identified in the text thesentences that contain the main concepts mentionedin the professional's abstract.
We scored how manysentences were selected by both the system and theprofessional bstracter.
We are aware that this eval-uation scheme is not very accurate, but it serves asa rough indicator for our initital investigation.We developed three variations to score the text3WordNet is a concept taxnonmy which consists ofsynonym sets instead of individual words309sentences on weights of the concepts in the interest-ing wavefront.1.
the weight of a sentence is equal to the sumof weights of parent concepts of words in thesentence.2.
the weight of a sentence is the sum of weightsof words in the sentence.3.
similar to one, but counts only one concept in-stance per sentence.To evaluate the system's performance, we definedthree counts: (1) hits, sentences identified by thealgorithm and referenced by the professional's ab-stract; (2) mistakes, sentences identified by the al-gorithm but not referenced by the professional's ab-stract; (3) misses, sentences in the professional's ab-stract not identified by the algorithm.
We then bor-rowed two measures from Information Retrieval re-search:Reca l l  : hits/(hits + misses)Precision : hits/(hits + mistakes)The closer these two measures are to unity, the bet-ter the algorithm's performance.
The precision mea-sure plays a central role in the text summarizationproblem: the higher the precision score, the higherprobability that the algorithm would identify thetrue topics of a text.
We also implemented a simpleplain word counting algorithm and a random selec-tion algorithm for comparision.The average result of 50 input texts with branchratio threshold 4 0.68 and starting depth 6.
The aver-age scores 5 for the three sentence scoring variationsare 0.32 recall and 0.35 precision when the systemproduces extracts of 8 sentences; while the randomselection method has 0.18 recall and 0.22 precisionin the same experimental setting and the plain wordcounting method has 0.23 recall and 0.28 precision.4 Conc lus ionThe system achieves its current performance withoutusing linguistic tools such as a part-of-speech tag-ger, syntactic parser, pronoun resoultion algorithm,or discourse analyzer.
Hence we feel that the con-cept counting paradigm is a robust method whichcan serve as a basis upon which to build an au-tomated text summarization system.
The currentsystem draws a performance lower bound for futuresystems.4This threshold and the starting depth are deter-mined by running the system through different parame-ter setting.
We test ratio = 0.95,0.68,0.45,0.25 anddepth= 3,6,9,12.
Among them, 7~t = 0.68 and ~D~ = 6 givethe best result.5The recall (R) and precision (P) for the three varia-tions axe: vax1(R=0.32,P=0.37), vax2(R=0.30,P=0.34),and vax3(R=0.28,P=0.33) when the system picks 8sentences.We have not yet been able to compare the perfor-mance of our system against IR and commericallyavailable xtraction packages, but since they do notemploy concept counting, we feel that our methodcan make a significant contribution.We plan to improve the system's extraction re-suits by incgrporating linguistic tools.
Our nextgoal is generating a summary instead of just extract-ing sentences.
Using a part-of-speech tagger andsyntatic parser to distinguish different syntatic at-egories and relations among concepts; we can findappropriate concept ypes on the interesting wave-front, and compose them into summary.
For exam-ple, if a noun concept is selected, we can find itsaccompanying verb; if verb is selected, we find itssubject noun.
For a set of selected concepts, we thengeneralize their matching concepts using the taxon-omy and generate the list of {selected concepts +matching eneralization} pairs as English sentences.There are other possibilities.
With a robust work-ing prototype system in hand, we are encouraged tolook for new interesting results.ReferencesGregory Grefenstette.
1994.
Ezplorations in Au-tomatic Thesaurus Discovery.
Kluwer AcademicPublishers, Boston.Paul S. Jacobs and Lisa F. Rau.
1990.
SCISOR:Extracting information from on-line news.
Com-munication of the A CM, 33(11):88-97, November.Michael L. Mauldin.
1991.
Conceptual InformationRetrieval -- A Case Study in Adaptive PartialParsing.
Kluwer Academic Publishers, Boston.George Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine Miller.
1990.Five papers on wordnet.
CSL Report 43, Congni-tive Science Labortory, Princeton University, NewHaven, July.Chris D. Paice.
1990.
Constructing litera-ture abstracts by computer: Techinques andprospects.
Information Processing and Manage-ment, 26(1):171-186.Philip Stuart Resnik.
1993.
Selection and Informa-tion: A Class-Based Approach to Lezical Relation-ships.
Ph.D. thesis, University of Pennsylvania,University of Pennsylvania.Gerard Salton, James Allan, Chris Buckley, andAmit Singhal.
1994.
Automatic analysis,theme generation, and summarization f machine-readable texts.
Science, 264:1421-1426, June.John Sinclair, Patrick Hanks, Gwyneth Fox,Rosamuna Moon, and Penny Stock.
1987.
CollinsCOBUILD English Language Dictionary.
WilliamCollins Sons & Co. Ltd., Glasgow, UK.310
