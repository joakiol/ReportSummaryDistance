Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 472?479,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsA Computational Model of Text Reuse in Ancient Literary TextsJohn LeeSpoken Language SystemsMIT Computer Science and Articial Intelligence LaboratoryCambridge, MA 02139, USAjsylee@csail.mit.eduAbstractWe propose a computational model of textreuse tailored for ancient literary texts, avail-able to us often only in small and noisy sam-ples.
The model takes into account sourcealternation patterns, so as to be able to aligneven sentences with low surface similarity.We demonstrate its ability to characterizetext reuse in the Greek New Testament.1 IntroductionText reuse is the transformation of a source text into atarget text in order to serve a different purpose.
Pastresearch has addressed a variety of text-reuse appli-cations, including: journalists turning a news agencytext into a newspaper story (Clough et al, 2002); ed-itors adapting an encyclopedia entry to an abridgedversion (Barzilay and Elhadad, 2003); and plagia-rizers disguising their sources by removing surfacesimilarities (Uzuner et al, 2005).A common assumption in the recovery of textreuse is the conservation of some degree of lexi-cal similarity from the source sentence to the de-rived sentence.
A simple approach, then, is to de-fine a lexical similarity measure and estimate a scorethreshold; given a sentence in the target text, if thehighest-scoring sentence in the source text is abovethe threshold, then the former is considered to be de-rived from the latter.
Obviously, the effectiveness ofthis basic approach depends on the degree of lexicalsimilarity: source sentences that are quoted verba-tim are easier to identify than those that have beentransformed by a skillful plagiarizer.The crux of the question, therefore, is how toidentify source sentences despite their lack of sur-face similarity to the derived sentences.
Ancient lit-erary texts, which are the focus of this paper, presentsome distinctive challenges in this respect.1.1 Ancient Literary Texts?Borrowed material embedded in the flow of awriter?s text is a common phenomenon in Antiq-uity.?
(van den Hoek, 1996).
Ancient writers rarelyacknowledged their sources.
Due to the scarcityof books, they often needed to quote from mem-ory, resulting in inexact quotations.
Furthermore,they combined multiple sources, sometimes insert-ing new material or substantially paraphrasing theirsources to suit their purpose.
To compound thenoise, the version of the source text available to ustoday might not be the same as the one originallyconsulted by the author.
Before the age of the print-ing press, documents were susceptible to corruptionsintroduced by copyists.Identifying the sources of ancient texts is use-ful in many ways.
It helps establish their relativedates.
It traces the evolution of ideas.
The materialquoted, left out or altered in a composition providesmuch insight into the agenda of its author.
Amongthe more frequently quoted ancient books are thegospels in the New Testament.
Three of them ?
thegospels of Matthew, Mark, and Luke ?
are calledthe Synoptic Gospels because of the substantial textreuse among them.472Target verses (English translation) Target verses (original Greek) Source verses (original Greek)Luke 9:30-33 Luke 9:30-33 Mark 9:4-5(9:30) And, behold, (9:30) kai idou (9:4) kai o?phthe?
autoisthere talked with him two men, andres duo sunelaloun auto?
E?lias sun Mo?usei kaiwhich were Moses and Elias.
hoitines e?san Mo?use?s kai E?lias e?san sullalountes to?
Ie?sou(9:31) Who appeared in glory, ... (9:31) hoi ophthentes en doxe?
... (no obvious source verse)(9:32) But Peter and they that were with him ... (9:32) ho de Petros kai hoi sun auto?
... (no obvious source verse)(9:33) And it came to pass, (9:33) kai egeneto en to?
diacho?rizesthaias they departed from him, autous ap?
autou eipen ho Petros (9:5) kai apokritheis ho PetrosPeter said unto Jesus, Master, pros ton Ie?soun epistata legei to?
Ie?sou rabbiit is good for us to be here: kalon estin he?mas ho?de einai kalon estin he?mas ho?de einaiand let us make kai poie?so?men ske?nas treis kai poie?so?men treis ske?nasthree tabernacles; one for thee, mian soi kai mian Mo?usei soi mian kai Mo?usei mianand one for Moses, and one for Elias: kai mian E?lia kai E?lia miannot knowing what he said.
me?
eido?s ho legeiTable 1: Luke 9:30-33 and their source verses in the Gospel of Mark.
The Greek words with commonstems in the target and source verses are bolded.
The King James Version English translation is included forreference.
?1.2 comments on the text reuse in these verses.1.2 Synoptic GospelsThe nature of text reuse among the Synoptics spansa wide spectrum.
On the one hand, some reveredverses, such as the sayings of Jesus or the apostles,were preserved verbatim.
Such is the case with Pe-ter?s short speech in the second half of Luke 9:33(see Table 1).
On the other hand, unimportant de-tails may be deleted, and new information weavedin from other sources or oral traditions.
For ex-ample, ?Luke often edits the introductions to newsections with the greatest independence?
(Taylor,1972).
To complicate matters, it is believed by someresearchers that the version of the Gospel of Markused by Luke was a more primitive version, cus-tomarily called Proto-Mark, which is no longer ex-tant (Boismard, 1972).
Continuing our example inTable 1, verses 9:31-32 have no obvious counter-parts in the Gospel of Mark.
Some researchers haveattributed them to an earlier version of Mark (Bo-ismard, 1972) or to Luke?s ?redactional tenden-cies?
(Bovon, 2002).The result is that some verses bear little resem-blance to their sources, due to extensive redaction,or to discrepancies between different versions of thesource text.
In the first case, any surface similarityscore alone is unlikely to be effective.
In the second,even deep semantic analysis might not suffice.1.3 GoalsOne property of text reuse that has not been exploredin past research is source alternation patterns.
Forexample, ?it is well known that sections of Luke de-rived from Mark and those of other origins are ar-ranged in continuous blocks?
(Cadbury, 1920).
Thisnotion can be formalized with features on the blocksand order of the source sentences.
The first goal ofthis paper is to leverage source alternation patternsto optimize the global text reuse hypothesis.Scholars of ancient texts tend to express theiranalyses qualitatively.
We attempt to translate theirinsights into a quantitative model.
To our bestknowledge, this is the first sentence-level, quantita-tive text-reuse model proposed for ancient texts.
Oursecond goal is thus to bring a quantitative approachto source analysis of ancient texts.2 Previous WorkText reuse is analyzed at the document level in(Clough et al, 2002), which classifies newspaperarticles as wholly, partially, or non-derived froma news agency text.
The hapax legomena, andsentence alignment based on N -gram overlap, arefound to be the most useful features.
Considering adocument as a whole mitigates the problem of lowsimilarity scores for some of the derived sentences.4731 2 3 4 5 6 7 8 9 10 1112 1314 15 16123456789101112131415161718192021222324MarkLukeFigure 1: A dot-plot of the cosine similarity mea-sure between the Gospel of Luke and the Gospel ofMark.
The number on the axes represent chapters.The thick diagonal lines reflect regions of high lexi-cal similarity between the two gospels.At the level of short passages or sentences, (Hatzi-vassiloglou et al, 1999) goes beyond N -gram, tak-ing advantage of WordNet synonyms, as well as or-dering and distance between shared words.
(Barzi-lay and Elhadad, 2003) shows that the simple cosinesimilarity score can be effective when used in con-junction with paragraph clustering.
A more detailedcomparison with this work follows in ?4.2.In the humanities, reused material in the writ-ings of Plutarch (Helmbold and O?Neil, 1959) andClement (van den Hoek, 1996) have been manuallyclassified as quotations, reminiscences, referencesor paraphrases.
Studies on the Synoptics have beenlimited to N -gram overlap, notably (Honore?, 1968)and (Miyake et al, 2004).Text Hypothesis Researcher ModelLtrain Ltrain.B (Bovon, 2002) BLtrain.J (Jeremias, 1966) JLtest Ltest.B (Bovon, 2003)Ltest.J (Jeremias, 1966)Table 2: Two models of text reuse of Mark in Ltrainare trained on two different text-reuse hypotheses:The B model is on the hypothesis in (Bovon, 2002),and the J model, on (Jeremias, 1966).
These twomodels then predict the text-reuse in Ltest.3 DataWe assume the Two-Document Theory1, which hy-pothesizes that the Gospel of Luke and the Gospelof Matthew have as their common sources two doc-uments: the Gospel of Mark, and a lost text custom-arily denoted Q.
In particular, we will consider theGospel of Luke2 as the target text, and the Gospel ofMark as the source text.We use a Greek New Testament corpus preparedby the Center for Computer Analysis of Texts at theUniversity of Pennsylvania3, based on the text vari-ant from the United Bible Society.
The text-reusehypotheses (i.e., lists of verses deemed to be de-rived from Mark) of Franc?ois Bovon (Bovon, 2002;Bovon, 2003) and Joachim Jeremias (Jeremias,1966) are used.
Table 2 presents our notations.Luke 1:1 to 9:50 (Ltrain, 458 verses) Chapters 1and 2, narratives of the births of Jesus and Johnthe Baptist, are based on non-Markan sources.Verses 3:1 to 9:50 describe Jesus?
activities inGalilee, a substantial part of which is derivedfrom Mark.Luke Chapters 22 to 24 (Ltest, 179 verses) Thesechapters, known as the Passion Narrative, serveas our test text.
Markan sources were behind38% of the verses, according to Bovon, and 7%according to Jeremias.1This theory (Streeter, 1930) is currently accepted by a ma-jority of researchers.
It guides our choice of experimental data,but our model does not depend on its validity.2We do not consider the Gospel of Matthew or Q in thisstudy.
Verses from Luke 9:51 to the end of chapter 21 arealso not considered, since their sources are difficult to ascertain(Bovon, 2002).3Obtained through Peter Ballard (personal communication)4744 ApproachFor each verse in the target text (a ?target verse?
), wewould like to determine whether it is derived from averse in the source text (a ?source verse?)
and, if so,which one.Following the framework of global linear modelsin (Collins, 2002), we cast this task as learning amapping F from input verses x ?
X to a text-reusehypothesis y ?
Y ?
{?}.
X is the set of verses inthe target text.
In our case, xtrain = (x1, .
.
.
, x458)is the sequence of verses in Ltrain, and xtest is thatof Ltest.
Y is the set of verses in the source text.Say the sequence y = (y1, .
.
.
, yn) is the text-reusehypothesis for x = (x1, .
.
.
, xn).
If yi is ?, then xi isnot derived from the source text; otherwise, yi is thesource verse for xi.
The set of candidates GEN(x)contains all possible sequences for y, and ?
is theparameter vector.
The mapping F is thus:F (x) = arg maxy?GEN(x)?
(x,y) ?
?4.1 FeaturesGiven the small amount of training data available4,the feature space must be kept small to avoid overfit-ting.
Starting with the cosine similarity score as thebaseline feature, we progressively enrich the modelwith the following features:Cosine Similarity [Sim] Treating a target verse asa query to the set of source verses, we com-pute the cosine similarity, weighted with tf.idf,for each pair of source verse and target verse5.This standard bag-of-words approach is appro-priate for Greek, a relatively free word-orderlanguage.
Figure 1 plots this feature on Lukeand Mark.Non-derived verses are assigned a constantscore in lieu of the cosine similarity.
We willrefer to this constant as the cosine threshold(C): when the Sim feature alone is used, theconstant effectively acts as the threshold abovewhich target verses are considered to be de-rived.
If wi, wj are the vectors of words of a4Note that the training set consists of only one xtrain ?the Gospel of Luke.
Luke?s only other book, the Acts of theApostles, contains few identifiable reused material.5A targert verse is also allowed to match two consecutivesource verses.target verse and a candidate source verse, then:sim(i, j) ={ wi?wj?wi???wj?
if derivedC otherwiseNumber of Blocks [Block] Luke can be viewedas alternating between Mark and non-Markanmaterial, and he ?prefers to pick up al-ternatively entire blocks rather than isolatedunits.?
(Bovon, 2002) We will use the termMarkan block to refer to a sequence of versesthat are derived from Mark.
A verse with alow cosine score, but positioned in the mid-dle of a Markan block, is likely to be derived.Conversely, an isolated verse in the middle ofa non-Markan block, even with a high cosinescore, is unlikely to be so.
The heavier theweight of this feature, the fewer blocks are pre-ferred.Source Proximity [Prox] When two derivedverses are close to one another, their respectivesource verses are also likely to be close to oneanother; in other words, derived verses tend toform ?continuous blocks?
(Cadbury, 1920).We define distance as the number of verses sep-arating two verses.
For each pair of consec-utive target verses, we take the inverse of thedistance between their source verses.
This fea-ture is thus intended to discourage a derivedverse from being aligned with a source versethat shares some lexical similarities by chance,but is far away from other source verses in theMarkan block.Source Order [Order] ?Whenever Luke followsthe Markan narrative in his own gospel hefollows painstakingly the Markan order?, andhence ?deviations in the order of the materialmust therefore be regarded as indications thatLuke is not following Mark.?
(Jeremias, 1966).This feature is a binary function on two consec-utive derived verses, indicating whether theirsource verses are in order.
A positive weightfor this feature would favor an alignment thatrespects the order of the source text.In cases where there are no obvious source verses,such as Luke 9:30-31 in Table 1, the source order475and proximity would be disrupted.
To mitigate thisissue, we allow the Prox and Order features theoption of skipping up to two verses within a Markanblock in the target text.
In our example, Luke 9:30can skip to 9:32, preserving the source proximityand order between their source verses, Mark 9:4 and9:5.Another potential feature is the occurrence offunction words characteristic of Luke (Rehkopf,1959), along the same lines as in the study of theFederalist Papers (Mosteller and Wallace, 1964).These stylistic indicators, however, are unlikelyto be as helpful on the sentence level as on thedocument level.
Furthermore, Luke ?reworks [hissources] to an extent that, within his entire composi-tion, the sources rarely come to light in their originalindependent form?
(Bovon, 2002).
The significanceof the presence of these indicators, therefore, is di-minished.4.2 DiscussionThis model is both a simplification of and an ex-tension to the one advocated in (Barzilay and El-hadad, 2003).
On the one hand, we perform no para-graph clustering or mapping before sentence align-ment.
Ancient texts are rarely divided into para-graphs, nor are they likely to be large enough forstatistical methods on clustering.
Instead, we relyon the Prox feature to encourage source verses tostay close to each other in the alignment.On the other hand, our model makes two exten-sions to the ?Micro Alignment?
step in (Barzilayand Elhadad, 2003).
First, we add the Block andProx features to capture source alternation patterns.Second, we place no hard restrictions on the re-ordering of the source text, opting instead for a softpreference for maintaining the source order throughthe Order feature.
In contrast, deviation from thesource order is limited to ?flips?
between two sen-tences in (Barzilay and Elhadad, 2003), an assump-tion that is not valid in the Synoptics6.4.3 Evaluation MetricOur model can make two types of errors: source er-ror, when it predicts a non-derived target verse tobe derived, or vice versa; and alignment error, when6For example, Luke 6:12-19 transposes Mark 3:7-12 andMark 3:13-19 (Bovon, 2002).it correctly predicts a target verse to be derived, butaligns it to the wrong source verse.Correspondingly, we interpret the output of ourmodel at two levels: as a binary output, i.e., thetarget verse is either ?derived?
or ?non-derived?
;or, as an alignment of the target verse to a sourceverse.
We measure the precision and recall of thetarget verses at both levels, yielding two F-measures,Fsource and Falign7.Literary dependencies in the Synoptics are typi-cally expressed as pairs of pericopes (short, coher-ent passages), for example, ?Luke 22:47-53 // Mark14:43-52?.
Likewise, for Falign, we consider theoutput correct if the hypothesized source verse lieswithin the pericope8.5 ExperimentsThis section presents experiments for evaluating ourtext-reuse model.
?5.1 gives some implementa-tion details.
?5.2 describes the training process,which uses text-reuse hypotheses of two different re-searchers (Ltrain.B and Ltrain.J ) on the same train-ing text.
The two resulting models thus representtwo different opinions on how Luke re-used Mark;they then produce two hypotheses on the test text(L?test.B and L?test.J ).Evaluations of these hypotheses follow.
In ?5.3,we compare them with the hypotheses of the sametwo researchers on the test text (Ltest.B and Ltest.J ).In ?5.3, we compare them with the hypotheses ofseven other representative researchers (Neirynck,1973).
Ideally, when the model is trained on a par-ticular researcher?s hypothesis on the train text, itshypothesis on the test text should be closest to theone proposed by the same researcher.5.1 ImplementationSuppose we align the ith target verse to the kthsource verse or to ?.
Using dynamic programming,their score is the cosine similarity score sim(i, k),added to the best alignment state up to the (i ?
1 ?skip)th target verse, where skip can vary from 0 to2 (see ?4.1).
If the jth source verse is the aligned7Note that Falign is never higher than Fsource since it pe-nalizes both source and alignment errors.8A more fine-grained metric is individual verse alignment.This is unfortunately difficult to measure.
As discussed in ?1.2,many derived verses have no clear source verses.476Model B JTrain Hyp Ltrain.B Ltrain.JMetric Fsource Falign Fsource FalignSim 0.760 0.646 0.748 0.635+Block 0.961 0.728 0.977 0.743All 0.985 0.949 0.983 0.936Table 3: Performance on the training text, Ltrain.The features are accumulative; All refers to the fullfeature set.verse in this state, then score(i, k) is:sim(i, k) + maxj,skip{score(i ?
1 ?
skip, j)+wprox ?
prox(j, k) + worder ?
order(j, k)?wblock ?
block(j, k)}If both j and k are aligned (i.e., not ?
), then:prox(j, k) = 1dist(j, k)order(j, k) = 1 if j ?
kblock(j, k) = 1 if starting new blockOtherwise these are set to zero.5.2 Training ResultsThe model takes only four parameters: the weightsfor the Block, Prox and Order features, as wellas the cosine threshold (C).
They are empiricallyoptimized, accurate to 0.01, on the two training hy-potheses listed in Table 2, yielding two models, Band J.Table 3 shows the increasing accuracy of bothmodels in describing the text reuse in Ltrain asmore features are incorporated.
The Block fea-ture contributes most in predicting the block bound-aries, as seen in the jump of Fsource from Sim to+Block.
The Prox and Order features substan-tially improve the alignment, boosting the Falignfrom +Block to All.Both models B and J fit their respective hypothe-ses to very high degrees.
For B, the only significantsource error occurs in Luke 8:1-4, which are derivedverses with low similarity scores.
They are transi-tional verses at the beginning of a Markan block.
ForModel B JTest Hyp Ltest.B Ltest.JMetric Fsource Falign Fsource FalignSim 0.579 0.382 0.186 0.144+Block 0.671 0.329 0.743 0.400All 0.779 0.565 0.839 0.839Table 5: Performance on the test text, Ltest.J, the pericope Luke 6:12-16 is wrongly predicted asderived.Most alignment errors are misalignments to aneighboring pericope, typically for verses locatednear the boundary between two pericopes.
Due totheir low similarity scores, the model was unableto decide if they belong to the end of the precedingpericope or to the beginning of the following one.5.3 Test ResultsThe two models trained in ?5.2, B and J, are intendedto capture the characteristics of text reuse in Ltrainaccording to two different researchers.
When ap-plied on the test text, Ltest, they produce two hy-potheses, L?test.B and L?test.J .
Ideally, they shouldbe similar to the hypotheses offered by the same re-searchers (namely, Ltest.B and Ltest.J ), and dissim-ilar to those by other researchers.
We analyze thefirst aspect in ?5.3, and the second aspect in ?5.3.Comparison with Bovon and JeremiasTable 4 shows the output of B and J on Ltest.
Asmore features are added, their output increasinglyresemble Ltest.B and Ltest.J , as shown in Table 5.Both L?test.B and L?test.J contain the same numberof Markan blocks as the ?reference?
hypotheses pro-posed by the respective scholars.
In both cases, thepericope Luke 22:24-30 is correctly assigned as non-derived, despite their relatively high cosine scores.This illustrates the effect of the Block feature.As for source errors, both B and J mistakenly as-sign Luke 22:15-18 as Markan, attracted by the highsimilarity score of Luke 22:18 with Mark 14:25.B, in addition, attributes another pericope to Markwhere Bovon does not.
Despite the penalty of lowersource proximity, it wrongly aligned Luke 23:37-38to Mark 15:2, misled by a specific title of Jesus thathappens to be present in both.477Chp 22.....................................................................23.....................................................Sim xx--x-x-xxxxxx-xxxxx-xx----------x---xxx-x---xx--x-xxx-xxxxxx-xx-x-xxxxx-x--x--xx-----x--xxx--xx------x-x-xxx---xxxx---xxxxx--All xxxxxxxxxxxxxxxxxx-------------------------------xxxxxxxxxxxxxxxxxxxxxxxxxxxx-----------------------------xxxxxxxxxxxxxxxxx---Bov xxxxxxxxxxxxxx--------------------------------xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-------------------------------------xxxxxxxxxxxxxSim xx--x-x-xxxxxx-xxxxx-xx----------x---xxx-x---xx--x-xxx-xxxxxx-xx-x-xxxxx-x--x--xx-----x--xxx--xx------x-x-xxx---xxxx---xxxxx--All xxxxxxxxxxxxxxxxxx------------------------------------------------------------------------------------------------------------Jer xxxxxxxxxxxxx-----------------------------------------------------------------------------------------------------------------Gru xxxxxxxxxxxxx-------xxx---------xx-----------------xx--------------------x-----------------------------xx--x-----xx-x---xxx---Haw xxxxxxxxxxxxx----x---x-------------------x---xx----xxx------x---------x--------------------x---x-------x---------xxx-----xx---Reh xxxxxxxxxxxxx------x-------------------------------xx------------------------------------------x------------------------------Snd --------------------xxx---------xx-----------------xxxxxxxxxx-------xxx-------------------------------------------------------Srm xxxxxxxxxxxxxx------xxx---------xx--------------------------------------------------------------------------------------------Str xxxxxxxxxxxxx----x---x-------------------x---xx----xxxxxxxxxx---------x--x-----------------x--xx------xx---x-----xxx-----xx---Tay xxxxxxxxxxxxx--------x-----------x-----------x---x-xxxxxxxxxx------------x---------------------x-------x---x-----xx---xxxxxx--Chp 24...................................................Sim xxx---x-xx---------x---x-------xxx-x---x--x-xxx-x---- (Model B Sim)All ----------------------------------------------------- (Model B All)Bov xxxxxxxxxxx------------------------------------------ (Bovon)Sim xxx---x-xx---------x---x-------xxx-x---x--x-xxx-x---- (Model J Sim)All ----------------------------------------------------- (Model J All)Jer ----------------------------------------------------- (Jeremias)Gru -x---x----------------------------------------------- (Grundmann)Haw -----x----------------------------------------------- (Hawkins)Reh ----------------------------------------------------- (Rehkopf)Snd -x---x---x------------------------------------------- (Schneider)Srm ----------------------------------------------------- (Schu?rmann)Str -----x----------------------------------------------- (Streeter)Tay ---------x------------------------------------------- (Taylor)Table 4: Output of models B and J, and scholarly hypotheses on the test text, Ltest.
The symbol ?x?
indicatesthat the verse is derived from Mark, and ?-?
indicates that it is not.
The hypothesis from (Bovon, 2003),labelled ?Bov?, is compared with the Sim (baseline) output and the All output of model B, as detailedin Table 5.
The hypothesis from (Jeremias, 1966), ?Jer?, is similarly compared with outputs of model J.Seven other scholarly hypotheses are also listed.Elsewhere, B is more conservative than Bovon inproposing Markan derivation.
For instance, the peri-cope Luke 24:1-11 is deemed non-derived, an opin-ion (partially) shared by some of the other seven re-searchers.Comparison with Other HypothesesAnother way of evaluating the output of B andJ is to compare them with the hypotheses of otherresearchers.
As shown in Table 6, L?test.B is moresimilar to Ltest.B than to the hypothesis of otherresearchers9.
In other words, when the model istrained on Bovon?s text-reuse hypothesis on the traintext, its prediction on the test text matches mostclosely with that of the same researcher, Bovon.9This is the list of researchers whose opinions on Ltestare considered representative by (Neirynck, 1973).
We havesimplified their hypotheses, considering those ?partially assim-ilated?
and ?reflect the influence of Mark?
to be non-derivedfrom Mark.Hypothesis B (L?test.B) J (L?test.J )Bovon (Ltest.B) 0.838 0.676Jeremias (Ltest.J ) 0.721 0.972Grundmann 0.726 0.866Hawkins 0.737 0.877Rehkopf 0.721 0.950Schneider 0.676 0.782Schu?rmann 0.698 0.950Streeter 0.771 0.821Taylor 0.793 0.821Table 6: Comparison of the output of the modelsB and J with hypotheses by prominent researcherslisted in (Neirynck, 1973).
The metric is the per-centage of verses deemed by both hypotheses to be?derived?, or ?non-derived?.478The differences between Bovon and the next twomost similar hypotheses, Taylor and Streeter, arenot statistically significant according to McNemar?stest (p = 0.27 and p = 0.10 respectively), possi-bly a reflection of the small size of Ltest; the dif-ferences are significant, however, with all other hy-potheses (p < 0.05).
Similar results are observedfor Jeremias and L?test.J .6 Conclusion & Future WorkWe have proposed a text-reuse model for ancientliterary texts, with novel features that account forsource alternation patterns.
These features were val-idated on the Lukan Passion Narrative, an instanceof text reuse in the Greek New Testament.The model?s predictions on this passage are com-pared to nine scholarly hypotheses.
When tunedon the text-reuse hypothesis of a certain researcheron the train text, it favors the hypothesis of thesame person on the test text.
This demonstrates themodel?s ability to capture the researcher?s particularunderstanding of text reuse.While a computational model alone is unlikelyto provide definitive answers, it can serve as a sup-plement to linguistic and literary-critical approachesto text-reuse analysis, and can be especially help-ful when dealing with a large amount of candidatesource texts.AcknowledgementsThis work grew out of a term project in the course?Gospel of Luke?, taught by Professor Franc?oisBovon at Harvard Divinity School.
It has also bene-fited much from discussions with Dr. Steven Lulich.ReferencesR.
Barzilay and N. Elhadad.
2003.
Sentence Align-ment for Monolingual Comparable Corpora.
Proc.EMNLP.M.
E. Boismard.
1972.
Synopse des quatre Evangiles enfranc?ais, Tome II.
Editions du Cerf, Paris, France.F.
Bovon.
2002.
Luke I: A Commentary on the Gospelof Luke 1:1-9:50.
Hermeneia.
Fortress Press.
Min-neapolis, MN.F.
Bovon.
2003.
The Lukan Story of the Passion of Jesus(Luke 22-23).
Studies in Early Christianity.
BakerAcademic, Grand Rapids, MI.H.
J. Cadbury.
1920.
The Style and Literary Methodof Luke.
Harvard Theological Studies, Number VI.George F. Moore and James H. Ropes and KirsoppLake (ed).
Harvard University Press, Cambridge, MA.P.
Clough, R. Gaizauskas, S. S. L. Piao and Y. Wilks.2002.
METER: MEasuring TExt Reuse.
Proc.
ACL.M.
Collins.
2002.
Discriminative Training Methods forHidden Markov Models: Theory and Experiments withPerceptron Algorithms.
Proc.
EMNLP.V.
Hatzivassiloglou, J. L. Klavans and E. Eskin.
1999.Detecting Text Similarity over Short Passages: Ex-ploring Linguistic Feature Combinations via MachineLearning.
Proc.
EMNLP.W.
C. Helmbold and E. N. O?Neil.
1959.
Plutarch?sQuotations.
Philological Monographs XIX, AmericanPhilological Association.A.
M. Honore?.
1968.
A Statistical Study of the SynopticProblem.
Novum Testamentum, Vol.
10, p.95-147.J.
Jeremias.
1966.
The Eucharistic Words of Jesus.Scribner?s, New York, NY.M.
Miyake, H. Akama, M. Sato, M. Nakagawa and N.Makoshi.
2004.
Tele-Synopsis for Biblical Research:Development of NLP based Synoptic Software for TextAnalysis as a Mediator of Educational Technology andKnowledge Discovery.
Proc.
IEEE International Con-ference on Advanced Learning Technologies (ICALT).F.
Mosteller and D. L. Wallace.
1964.
Inference and Dis-puted Authorship: The Federalist.
Addison Wesley,Reading, MA.F.
Neirynck.
1973.
La matie`re marcienne dansl?e?vangile de Luc.
L?E?vangile de Luc, Proble`meslitte?raires et the?ologiques.
Editions Duculot, Belgium.F.
Rehkopf.
1959.
Die lukanische Sonderquelle.
Wis-senschaftliche Untersuchungen zum Neuen Testament,Vol.
5.
Tu?bingen, Germany.B.
H. Streeter.
1930.
The Four Gospels: A Study of Ori-gins.
MacMillan.
London, England.V.
Taylor.
1972.
The Passion Narrative of St. Luke: ACritical and Historical Investigation.
Society for NewTestament Studies Monograph Series, Vol.
19.
Cam-bridge University Press, Cambridge, England.O.
Uzuner, B. Katz and T. Nahnsen.
2005.
Using Syn-tactic Information to Identify Plagiarism.
Proc.
2ndWorkshop on Building Educational Applications usingNLP.
Ann Arbor, MI.A.
van den Hoek.
1996.
Techniques of Quotation inClement of Alexandria ?
A View of Ancient LiteraryWorking Methods.
Vigiliae Christianae, Vol 50, p.223-243.
E. J. Brill, Leiden, The Netherlands.479
