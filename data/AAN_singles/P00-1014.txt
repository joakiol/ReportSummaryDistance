An Unsupervised Approach to Prepositional Phrase Attachmentusing Contextually Similar WordsPatrick Pantel and Dekang LinDepartment of Computing ScienceUniversity of Alberta1Edmonton, Alberta T6G 2H1 Canada{ppantel, lindek}@cs.ualberta.ca1This research was conducted at the University of Manitoba.AbstractPrepositional phrase attachment is acommon source of ambiguity in naturallanguage processing.
We present anunsupervised corpus-based approach toprepositional phrase attachment thatachieves similar performance to supervisedmethods.
Unlike previous unsupervisedapproaches in which training data isobtained by heuristic extraction ofunambiguous examples from a corpus, weuse an iterative process to extract trainingdata from an automatically parsed corpus.Attachment decisions are made using alinear combination of features and lowfrequency events are approximated usingcontextually similar words.IntroductionPrepositional phrase attachment is a commonsource of ambiguity in natural languageprocessing.
The goal is to determine theattachment site of a prepositional phrase in asentence.
Consider the following examples:1.
Mary ate the salad with a fork.2.
Mary ate the salad with croutons.In both cases, the task is to decide whether theprepositional phrase headed by the prepositionwith attaches to the noun phrase (NP) headed bysalad or the verb phrase (VP) headed by ate.
Inthe first sentence, withattaches to the VP sinceMary is using a fork to eat her salad.
In sentence2, with attaches to the NP since it is the saladthat contains croutons.Formally, prepositional phrase attachment issimplified to the following classification task.Given a 4-tuple of the form (V, N1, P, N2), whereV is the head verb, N1 is the head noun of theobject of V, P is a preposition, and N2 is the headnoun of the prepositional complement, the goalis to classify as either adverbial attachment(attaching to V) or adjectival attachment(attaching to N1).
For example, the 4-tuple (eat,salad, with, fork) has target classification V.In this paper, we present an unsupervisedcorpus-based approach to prepositional phraseattachment that outperforms previousunsupervised techniques and approaches theperformance of supervised methods.
Unlikeprevious unsupervised approaches in whichtraining data is obtained by heuristic extractionof unambiguous examples from a corpus, we usean iterative process to extract training data froman automatically parsed corpus.
The attachmentdecision for a 4-tuple (V, N1, P, N2) is made asfollows.
First, we replace V and N2 by theircontextually similar words and compute theaverage adverbial attachment score.
Similarly,the verage adjectival attachment score iscomputed by replacing N1 and N2 by theircontextually similar words.
Attachment scoresare obtained using a linear combination offeatures of the 4-tuple.
Finally, we combine theaverage attachment scores with the attachmentscore of N2 attaching to the original V and theattachment score of N2 attaching to the originalN1.
The proposed classification represents theattachment site that scored highest.1 Previous WorkAltmann and Steedman (1988) showed thatcurrent discourse context is often required fordisambiguating attachments.
Recent work showsthat it is generally sufficient to utilize lexicalinformation (Brill and Resnik, 1994; Collins andBrooks, 1995; Hindle and Rooth, 1993;Ratnaparkhi et al, 1994).One of the earliest corpus-based approaches toprepositional phrase attachment used lexicalpreference by computing co-occurrencefrequencies (lexical associations) of verbs andnouns with prepositions (Hindle and Rooth,1993).
Training data was obtained by extractingall phrases of the form (V, N1, P, N2) from alarge parsed corpus.Supervised methods later improvedattachment accuracy.
Ratnaparkhi et al (1994)used a maximum entropy model consideringonly lexical information from within the verbphrase (ignoring N2).
They experimented withboth word features and word class features, theircombination yielding 81.6% attachmentaccuracy.Later, Collins and Brooks (1995) achieved84.5% accuracy by employing a backed-offmodel to smooth for unseen events.
Theydiscovered that P is the most informative lexicalitem for attachment disambiguation and keepinglow frequency events increases performance.A non-statistical supervised approach by Brilland Resnik (1994) yielded 81.8% accuracy usinga transformation-based approach (Brill, 1995)and incorporating word-class information.
Theyreport that the top 20 transformations learnedinvolved specific prepositions supportingCollins and Brooks?
claim that the preposition isthe most important lexical item for resolving theattachment ambiguity.The state of the art is a supervised algorithmthat employs a semantically tagged corpus(Stetina and Nagao, 1997).
Each word in alabelled corpus is sense-tagged using anunsupervised word-sense disambiguationalgorithm with WordNet (Miller, 1990).
Testingexamples are classified using a decision treeinduced from the training examples.
They report88.1% attachment accuracy approaching thehuman accuracy of 88.2% (Ratnaparkhi et al,1994).The current unsupervised state of the artachieves 81.9% attachment accuracy(Ratnaparkhi, 1998).
Using an extractionheuristic, unambiguous prepositional phraseattachments of the form (V, P, N2) and (N1, P,N2) are extracted from a large corpus.
Co-occurrence frequencies are then used todisambiguate examples with ambiguousattachments.2 ResourcesThe input to our algorithm includes a collocationdatabase and a corpus-based thesaurus, bothavailable on the Internet2.
Below, we brieflydescribe these resources.2.1 Collocation databaseGiven a word w in a dependency relationship(such as subject or object), the collocationdatabase is used to retrieve the words thatoccurred in that relationship with w, in a largecorpus, along with their frequencies (Lin,1998a).
Figure 1 shows excerpts of the entries in2Available at www.cs.ualberta.ca/~lindek/demos.htm.eat:object: almond 1, apple 25, bean 5, beam 1, binge 1,bread 13, cake 17, cheese 8, dish 14,disorder 20, egg 31, grape 12, grub 2, hay 3,junk 1, meat 70, poultry 3, rabbit 4, soup 5,sandwich 18, pasta 7, vegetable 35, ...subject: adult 3, animal 8, beetle 1, cat 3, child 41,decrease 1, dog 24, family 29, guest 7, kid22, patient 7, refugee 2, rider 1, Russian 1,shark 2, something 19, We 239, wolf 5, ...salad:adj-modifier:assorted 1, crisp 4, fresh 13, good 3, grilled5, leftover 3, mixed 4, olive 3, prepared 3,side 4, small 6, special 5, vegetable 3, ...object-of: add 3, consume 1, dress 1, grow 1, harvest 2,have 20, like 5, love 1, mix 1, pick 1, place3, prepare 4, return 3, rinse 1, season 1, serve8, sprinkle 1, taste 1, test 1, Toss 8, try 3, ...Figure 1.
Excepts of entries in the collocation database foreat and salad.Table 1.
The top 20 most similar words of eatand salad asgiven by (Lin, 1998b).WORD SIMILAR WORDS (WITH SIMILARITY SCORE)EAT cook 0.127, drink 0.108, consume 0.101, feed 0.094,taste 0.093, like 0.092, serve 0.089, bake 0.087, sleep0.086, pick 0.085, fry 0.084, freeze 0.081, enjoy0.079, smoke 0.078, harvest 0.076, love 0.076, chop0.074, sprinkle 0.072, Toss 0.072, chew 0.072SALAD soup 0.172, sandwich 0.169, sauce 0.152, pasta0.149, dish 0.135, vegetable 0.135, cheese 0.132,dessert 0.13, entree 0.121, bread 0.116, meat 0.116,chicken 0.115, pizza 0.114, rice 0.112, seafood 0.11,dressing 0.109, cake 0.107, steak 0.105, noodle0.105, bean 0.102the collocation database for the words eat andsalad.
The database contains a total of 11million unique dependency relationships.2.2 Corpus-based thesaurusUsing the collocation database, Lin (1998b) usedan unsupervised method to construct a corpus-based thesaurus consisting of 11839 nouns, 3639verbs and 5658 adjectives/adverbs.
Given aword w, the thesaurus returns a set of similarwords of w along with their similarity to w. F rexample, the 20 most similar words of eat andsalad are shown in Table 1.3 Training Data ExtractionWe parsed a 125-million word newspapercorpus with Minipar3, a descendent of Principar(Lin, 1994).
Minipar outputs dependency trees(Lin, 1999) from the input sentences.
Forexample, the following sentence is decomposedinto a dependency tree:Occasionally, the parser generates incorrectdependency trees.
For example, in the abovesentence, the prepositional phrase headed bywith should attach to saw(as opposed to d g).Two separate sets of training data were thenextracted from this corpus.
Below, we brieflydescribe how we obtained these data sets.3.1 Ambiguous Data SetFor each input sentence, Minipar outputs asingle dependency tree.
For a sentencecontaining one or more prepositions, we use aprogram to detect any alternative prepositionalattachment sites.
For example, in the abovesentence, the program would detect that wi hcould attach to saw.
Using an iterativealgorithm, we initially create a table of co-occurrence frequencies for 3-tuples of th  f rm(V, P, N2) and (N1, P, N2).
For each k possibleattachment site of a preposition P, we incrementthe frequency of the corresponding 3-tuple by1/k.
For example, Table 2 shows the initial co-occurrence frequency table for thecorresponding 3-tuples of the above sentence.3Available at www.cs.ualberta.ca/~lindek/minipar.htm.In the following iterations of the algorithm, weupdate the frequency table as follows.
For each kpossible attachment site of a preposition P, werefine its attachment score using the formulasdescribed in Section 4: VScore(Vk, Pk, N2k) andNScore(N1k, Pk, N2k).
For any tuple (Wk, Pk, N2k),where Wk is either Vk or N2k, we update itsfrequ ncy as:( ) ( )( )?
== ki iiikkkkNkPkW NPWScoreNPWScorefr1 222,,,,,,where Score(Wk, Pk, N2k) = VScore(Wk, Pk, N2k)if Wk = Vk; otherwise Score(Wk, Pk, N2k) =NScore(Wk, Pk, N2k).Suppose that after the initial frequency table isset NScore(man, in, park) = 1.23, VScore(saw,with, telescope) = 3.65, and NScore(dog, with,telescope) = 0.35.
Then, the updated co-occurrence frequencies for (man, in, park) and(saw, with, telescope) are:fr(man, in, park) =    23.123.1 = 1.0fr(saw, with, telescope) = 35.065.365.3+ = 0.913Table 3 shows the updated frequency tableafter the first iteration of the algorithm.
Theresulting database contained 8,900,000 triples.3.2 Unambiguous Data SetAs in (Ratnaparkhi, 1998), we constructed atraining data set consisting of only unambiguousTable 2.
Initial co-occurrence frequency table entries for Aman in the park saw a dog with a telescope.V OR N1 P N2 FREQUENCYman in park 1.0saw with telescope 0.5dog with telescope 0.5Table 3.
Co-occurrence frequency table entries for A manin the park saw a dog with a telescope after one iteration.V OR N1 P N2 FREQUENCYman in park 1.0saw with telescope 0.913dog with telescope 0.087A  man  in  the  park  saw  a  dog  with  a  telescope.det det det detpcomppcompmodsubjobjmodattachments of the form (V, P, N2) and (N1, P,N2).
We only extract a 3-tuple from a sentencewhen our program finds no alternativeattachment site for its preposition.
Eachextracted 3-tuple is assigned a frequency countof 1.
For example, in the previous sentence,(man, in, park) is extracted since it contains onlyone attachment site; (dog, with, telescope) is notextracted since with has an alternativeattachment site.
The resulting databasecontained 4,400,000 triples.4 Classification ModelRoth (1998) presented a unified framework fornatural language disambiguation tasks.Essentially, several language learning algorithms(e.g.
na?ve Bayes estimation, back-offestimation, transformation-based learning) weresuccessfully cast as learning linear separators intheir feature space.
Roth modelled prepositionalphrase attachment as linear combinations offeatures.
The features consisted of all 15possible sub-sequences of the 4-tuple (V, N1, P,N2) shown in Table 4.
The asterix (*) in featuresrepresent wildcards.Roth used supervised learning to adjust theweights of the features.
In our experiments, weonly considered features that contained P sincethe preposition is the most important lexical item(Collins and Brooks, 1995).
Furthermore, weomitted features that included both V and N1since their co-occurrence is independent of theattachment decision.
The resulting subset offeatures considered in our system is shown inbold in Table 4 (equivalent to assigning a weightof 0 or 1 to each feature).Let |head, rel, mod| represent the frequency,obtained from the training data, of the headoccurring in the given relationship rel with themodifier.
We then assign a score to each featureas follows:1.
(*, *, P, *) = log(|*, P, *| / |*, *, *|)2.
(V, *, P, N2) = log(|V, P, N2| / |*, *, *|)3.
(*, N1, P, N2) = log(|N1, P, N2| / |*, *, *|)4.
(V, *, P, *) = log(|V, P, *| / |V, *, *|)5.
(*, N1, P, *) = log(|N1, P, *| / |N1, *, *|)6.
(*, *, P, N2) = log(|*, P, N2| / |*, *, N2|)1, 2, and 3 are the prior probabilities of P, V PN2, and N1 P N2 respectively.
4, 5, and 6represent conditional probabilities P(V, P | V),P(N1, P | N1), and P(P N2 | N2) respectively.We estimate the adverbial and adjectivalattachment scores, VScore(V, P, N2) andNScore(N1, P, N2), as a linear combination ofthese features:VScore(V, P, N2) =(*, *, P, *) + (V, *, P, N2) +(V, *, P, *) + (*, *, P, N2)NScore(N1, P, N2) =(*, *, P, *) + (*, N1, P, N2) +(*, N1, P, *) + (*, *, P, N2)For example, the attachment scores for (eat,salad, with, fork) are VScore(eat, with, fork) =-3.47 and NScore(salad, with, fork) = -4.77.
Themodel correctly assigns a higher score to thedv rbial attachment.5 Contextually Similar WordsThe contextually similar words of a word w arewords similar to the intended meaning of w i  itscontext.
Below, we describe an algorithm forconstructing contextually similar words and wepresent a method for approximating theattachment scores using these words.5.1 AlgorithmFor our purposes, a context of w is simply adependency relationship involving w. Forexample, a dependency relationship for aw inthe example sentence of Section 3 issaw:obj:dog.
Figure 2 gives the data flowdiagram for our algorithm for constructing thecontextually similar words of w. We retrievefrom the collocation database the words thatoccurred in the same dependency relationship asw.
We refer to this set of words as the cohort ofw for the dependency relationship.
Consider thewords eat and salad in the context eat salad.The cohort of eat consists of verbs that appearedTable 4.
The 15 features for prepositional phraseattachment.FEATURES(V, *, *, *) (V, *, P, *) (*, N1, *, N2)(V, N1, *, *) (V, *, *, N2) (*, N1, P, N2)(V, N1, P, *) (V, *, P, N2) (*, *, P, *)(V, N1, *, N2) (*, N1, *, *) (*, *, *, N2)(V, N1, P, N2) (*, N1, P, *) (*, *, P, N2)with object salad in Figure 1 (e.g.
add, consume,cover, ? )
and the cohort of salad consists ofnouns that appeared as object of eat in Figure 1(e.g.
almond, apple, bean,  ?
).Intersecting the set of similar words and thecohort then forms the set of contextually similarwords of w. For example, Table 5 shows thecontextually similar words of eat and salad inthe context eat salad and the contextuallysimilar words of fork in the contexts eat withfork and salad with fork.
The words in the firstrow are retrieved by intersecting the similarwords of eat in Table 1 with the cohort of eatwhile the second row represents the intersectionof the similar words of alad in Table 1 and thecohort of salad.
The third and fourth rows aredetermined in a similar manner.
In thenonsensical context salad with fork (in row 4),no contextually similar words are found.While previous word sense disambiguationalgorithms rely on a lexicon to provide senseinventories of words, the contextually similarwords provide a way of distinguishing betweendifferent senses of words without committing toany particular sense inventory.5.2 Attachment ApproximationOften, sparse data reduces our confidence in theattachment scores of Section 4.
Usingcontextually similar words, we can approximatethese scores.
Given the tuple (V, N1, P, N2),adverbial attachments are approximated asfollows.
We first construct a list CSV containingthe contextually similar words of V in contextV:obj:N1 and a list CSN2V containing thecontextually similar words of N2 in contextV:P:N2 (i.e.
assuming adverbial attachment).
Foreach verb v in CSV, we compute VScore(v, P, N2)and set SV as the average of the largest k of thesescores.
Similarly, for each noun  in CSN2V, wecompute VScore(V, P, n) and set SN2V as theaverage of the largest k of these scores.
Then,the approximated adverbial attachment score,Vscore', is:VScore'(V, P, N2) = max(SV, SN2V)We approximate the adjectival attachmentscore in a similar way.
First, we construct a listCSN1 containing the contextually similar wordsof N1 in context V:obj:N1 and a list CSN2N1containing the contextually similar words of N2in context N1:P:N2 (i.e.
assuming adjectivalattachment).
Now, we compute SN1 as theaverage of the largest k of NScore(n, P, N2) foreach noun  in CSN1 and SN2N1 as the average ofthe largest k of NScore(N1, P, n) for each noun nin CSN2N1.
Then, the approximated adjectivalattachment score, NScore', is:NScore'(N1, P, N2) = max(SN1, SN2N1)For example, suppose we wish to approximatethe attachment score for the 4-tuple (eat, salad,with, fork).
First, we retrieve the contextuallysimilar words of eat and salad in context eatsalad, and the contextually similar words of forkin contexts eat with fork and salad with fork asshown in Table 5.
Let k = 2.
Table 6 shows thecalculation of SV and SN2V while the calculationof SN1 and SN2N1 is shown in Table 7.
Only theFigure 2.
Data flow diagram for identifying thecontextually similar words of a word in a dependencyrelationship.word in dependencyrelationshipSimilar Words CohortsCorpus-BasedThesaurusRetrieveIntersectGet SimilarWordsCollocationDBContextuallySimilar WordsTable 5.
Contextually similar words of eat and salad.WORD CONTEXT CONTEXTUALLY SIMILAR WORDSEAT eat salad consume, taste, like, serve, pick,harvest, love, sprinkle, Toss,?SALAD eat salad soup, sandwich, pasta, dish, cheese,vegetable, bread, meat, cake, bean, ?FORK eat with fork spoon, knife, fingerFORK salad with fork ---top k = 2 scores are shown in these tables.
Wehave:VScore' (eat, with, fork) = max(SV, SN2V)= -2.92NScore' (salad, with, fork) = max(SN1, SN2N1)= -4.87Hence, the approximation correctly prefers theadverbial attachment to the adjectivalattachment.6 Attachment AlgorithmFigure 3 describes the prepositional phraseattachment algorithm.
As in previousapproaches, examples with P = of are alwaysclassified as adjectival attachments.Suppose we wish to approximate theattachment score for the 4-tuple (eat, salad,with, fork).
From the previous section, Step 1returns averageV = -2.92 and averageN1 = -4.87.From Section 4, Step 2 gives aV = -3.47 andaN1 = -4.77.
In our training data, fV = 2.97 andfN1 = 0, thus Step 3 gives f = 0.914.
In Step 4, wecompute:S(V) = -3.42 andS(N1) = -4.78Since S(V) > S(N1), the algorithm correctlyclassifies this example as an adverbialattachment.Given the 4-tuple (eat, salad, with, croutons),the algorithm returns S(V) = -4.31 and S(N1) =-3.88.
Hence, the algorithm correctly attachesthe prepositional phrase to the noun salad.7 Experimental ResultsIn this section, we describe our test data and thebaseline for our experiments.
Finally, we presentour results.7.1 Test DataThe test data consists of 3097 examples derivedfrom the manually annotated attachments in thePenn Treebank Wall Street Journal data(Ratnaparkhi et al, 1994)4.
Each line in the testdata consists of a 4-tuple and a targetclassification: V N1 P N2 target.4Available at ftp.cis.upenn.edu/pub/adwait/PPattachData.The data set contains several erroneous tuplesand attachments.
For instance, 133 examplescontain the word the as N1 or N2.
There are alsoimprobable attachments such as (sing, birthday,to, you) with the target attachment birthday.Table 6.
Calculation of SV and SN2V for (eat, salad, with,fork).4-TUPLE VSCORE(mix, salad, with, fork) -2.60(sprinkle, salad, with, fork) -3.24SV -2.92(eat, salad, with, spoon) -3.06(eat, salad, with, finger) -3.50SN2V -3.28Table 7.
Calculation of SN1 and SN2N1 for (eat, salad, with,fork).4-TUPLE NSCORE(eat, pasta, with, fork) -4.71(eat, cake, with, fork) -5.02SN1 -4.87--- n/a--- n/aSN2N1 n/aInput: A 4-tuple (V, N1, P, N2)Step 1: Using the contextually similar words algorithmand the formulas from Section 5.2 compute:averageV = VScore'(V, P, N2)averageN1 = NScore'(N1, P, N2)Step 2: Compute the adverbial attachment score, av,and the adjectival attachment score, an1:aV = VScore(V, P, N2)aN1 = NScore(N1, P, N2)Step 3: Retrieve from the training data set thefrequency of the 3-tuples (V, P, N2) and(N1, P, N2) ?
fV and fN1, respectively.Let f = (fV + fN1 + 0.2) / (fV + fN1 +0.5)Step 4: Combine the scores of Steps 1-3 to obtain thefinal attachment scores:S(V) = fav + (1 - f)averagevS(N1) = fan1 + (1 - f)averagen1Output:The attachment decision: N1 if S(N1) > S(V) orP = of; V otherwise.Figure 3.
The prepositional phrase attachment algorithm.7.2 BaselineChoosing the most common attachment site, N1,yields an accuracy of 58.96%.
However, weachieve 70.39% accuracy by classifying eachoccurrence of P = of as N1, and V otherwise.Human accuracy, given the full context of asentence, is 93.2% and drops to 88.2% whengiven only tuples of the form (V, N1, P, N2)(Ratnaparkhi et al, 1994).
Assuming that humanaccuracy is the upper bound for automaticmethods, we expect our accuracy to be boundedabove by 88.2% and below by 70.39%.7.3 ResultsWe used the 3097-example testing corpusdescribed in Section 7.1.
Table 8 presents theprecision and recall of our algorithm and Table 9presents a performance comparison between oursystem and previous supervised andunsupervised approaches using the same testdata.
We describe the different classifiers below:clbase: the baseline described in Section 7.2clR1: uses a maximum entropy model(Ratnaparkhi et al, 1994)clBR5: uses transformation-based learning (Brilland Resnik, 1994)clCB: uses a backed-off model (Collins andBrooks, 1995)clSN: induces a decision tree with a sense-taggedcorpus, using a semantic dictionary(Stetina and Nagao, 1997)clHR6: uses lexical preference (Hindle and Rooth,1993)clR2: uses a heuristic extraction of unambiguousattachments (Ratnaparkhi, 1998)clPL: uses the algorithm described in this paperOur classifier outperforms all previousunsupervised techniques and approaches theperformance of supervised algorithm.We reconstructed the two earlier unsupervisedclassifiers clHR and clR2.
Table 10 presents theaccuracy of our reconstructed classifiers.
Theoriginally reported accuracy for lR2 is within the95% confidence interval of our reconstruction.Our reconstruction of clHR achieved slightlyhigher accuracy than the original report.5The accuracy is reported in (Collins and Brooks, 1995).6The accuracy was obtained on a smaller test set but, fromthe same source as our test data.Our classifier used a mixture of the twotraining data sets described in Section 3.
InTable 11, we compare the performance of oursystem on the following training data sets:UNAMB: the data set of unambiguous examplesdescribed in Section 3.2EM0: the data set of Section 3.1 afterfrequency table initializationEM1: EM0 + one iteration of algorithm 3.1EM2: EM0 + two iterations of algorithm 3.1EM3: EM0 + three iterations of algorithm 3.11/8-EM1:one eighth of the data in EM1MIX: The concatenation of UNAMB and EM1Table 11 illustrates a slight but consistentincrease in performance when using contextuallysimilar words.
However, since the confidenceintervals overlap, we cannot claim with certaintyTable 8.
Precision and recall for attachment sites V and N1.CLASS ACTUAL CORRECT INCORRECT PRECISION RECALLV 1203 994 209 82.63% 78.21%N1 1894 1617 277 84.31% 88.55%Table 9.
Performance comparison with other approaches.METHOD LEARNING ACCURACYCLBASE --- 70.39%CLR1 supervised 81.6%CLBR supervised 81.9%CLCB supervised 84.5%CLSN supervised 88.1%CLHR unsupervised 75.8%CLR2 unsupervised 81.91%CLPL unsupervised 84.31%Table 10.
Accuracy of our reconstruction of (Hindle &Rooth, 1993) and (Ratnaparkhi, 1998).METHOD ORIGINALREPORTEDACCURACYRECONSTRUCTEDSYSTEM ACCURACY(95% CONF)CLHR 75.8% 78.40% ?
1.45%CLR2 81.91% 82.40% ?
1.34%that the contextually similar words improveperformance.In Section 7.1, we mentioned some testingexamples contained N1 = the or N2 = the.
Forsupervised algorithms, the is represented in thetraining set as any other noun.
Consequently,these algorithms collect training data for the ndperformance is not affected.
However,unsupervised methods break down on suchexamples.
In Table 12, we illustrate theperformance increase of our system whenremoving these erroneous examples.Conclusion and Future WorkThe algorithms presented in this paper advancethe state of the art for unsupervised approachesto prepositional phrase attachment and drawsnear the performance of supervised methods.Currently, we are exploring different functionsfor combining contextually similar wordapproximations with the attachment scores.
Apromising approach considers the mutualinformation between the prepositionalrelationship of candidate attachments and N2.
Asthe mutual information decreases, ourconfidence in the attachment score decreasesand the contextually similar word approximationis weighted higher.
Also, improving theconstruction algorithm for contextually similarwords would possibly improve the accuracy ofthe system.
One approach first clusters thesimilar words.
Then, dependency relationshipsare used to select the most representativeclusters as the contextually similar words.
Theassumption is that more representative similarwords produce better approximations.AcknowledgementsThe authors wish to thank the reviewers for theirhelpful comments.
This research was partlysupported by Natural Sciences and EngineeringResearch Council of Canada grant OGP121338and scholarship PGSB207797.ReferencesAltmann, G. and Steedman, M. 1988.
Interaction with ContextDuring Human Sentence Processing.
Cognition, 30:191-238.Brill, E. 1995.
Transformation-based Error-driven Learning andNatural Language Processing: A case study in part of speechtagging.
Computational Linguistics, December.Brill, E. and Resnik.
P. 1994.
A Rule-Based Approach toPrepositional Phrase Attachment Disambiguation.
InProceedings of COLING-94.
Kyoto, Japan.Collins, M. and Brooks, J.
1995.
Prepositional Phrase Attachmentthrough a Backed-off Model.
In Proceedings of the ThirdWorkshop on Very Large Corpora, pp.
27-38.
Cambridge,Massachusetts.Hindle, D. and Rooth, M. 1993.
Structural Ambiguity and LexicalRelations.
Computational Linguistics, 19(1):103-120.Lin, D. 1999.
Automatic Identification of Non-CompositionalPhrases.
In Proceedings of ACL-99, pp.
317-324.
College Park,Maryland.Lin, D. 1998a.
Extracting Collocations from Text Corpora.Workshop on Computational Terminology.
Montreal, Canada.Lin, D. 1998b.
Automatic Retrieval and Clustering of SimilarWords.
In Proceedings of COLING-ACL98.
Montreal, Canada.Lin, D. (1994).
Principar - an Efficient, Broad-Coverage,Principle-Based Parser.
In Proceedings of COLING-94.
Kyoto,Japan.Miller, G. 1990.
Wordnet: an On-Line Lexical Database.International Journal of Lexicography, 1990.Ratnaparkhi, A.
1998.
Unsupervised Statistical Models forPrepositional Phrase Attachment.
In Proceedings of COLING-ACL98.
Montreal, Canada.Ratnaparkhi, A., Reynar, J., and Roukos, S. 1994.
A MaximumEntropy Model for Prepositional Phrase Attachment.
InProceedings of the ARPA Human Language TechnologyWorkshop, pp.
250-255.
Plainsboro, N.J.Roth, D. 1998.
Learning to Resolve Natural LanguageAmbiguities: A Unified Approach.
In Proceedings of AAAI-98,pp.
806-813.
Madison, Wisconsin.Stetina, J. and Nagao, M. 1997.
Corpus Based PP AttachmentAmbiguity Resolution with a Semantic Dictionary.
InProceedings of the Fifth Workshop on Very Large Corpora, pp.66-80.
Beijing and Hong Kong.Table 11.
Performance comparison of different data sets.DATABASE ACCURACYWITHOUTSIMWORDS(95% CONF)ACCURACYWITHSIMWORDS(95% CONF)UNAMBIGUOUS 83.15% ?
1.32% 83.60% ?
1.30%EM0 82.24% ?
1.35% 82.69% ?
1.33%EM1 83.76% ?
1.30% 83.92% ?
1.29%EM2 83.66% ?
1.30% 83.70% ?
1.31%EM3 83.20% ?
1.32% 83.20% ?
1.32%1/8-EM1 82.98% ?
1.32% 83.15% ?
1.32%MIX 84.11% ?
1.29% 84.31% ?
1.28%Table 12.
Performance with removal of the asN1 or N2.DATA SET ACCURACYWITHOUTSIMWORDS(95% CONF)ACCURACYWITHSIMWORDS(95% CONF)WITH THE 84.11% ?
1.29% 84.31% ?
1.32%WITHOUT THE 84.44% ?
1.31% 84.65% ?
1.30%
