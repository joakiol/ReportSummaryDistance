Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 22?27,Prague, June 2007. c?2007 Association for Computational LinguisticsCOGEX at RTE3Marta Tatu and Dan MoldovanLanguage Computer CorporationRichardson, Texas, 75080United Statesmarta,moldovan@languagecomputer.comAbstractThis paper reports on LCC?s participationat the Third PASCAL Recognizing TextualEntailment Challenge.
First, we summarizeour semantic logical-based approach whichproved successful in the previous two chal-lenges.
Then we highlight this year?s inno-vations which contributed to an overall accu-racy of 72.25% for the RTE 3 test data.
Thenovelties include new resources, such as eX-tended WordNet KB which provides a largenumber of world knowledge axioms, eventand temporal information provided by theTARSQI toolkit, logic form representationsof events, negation, coreference and context,and new improvements of lexical chain ax-iom generation.
Finally, the system?s perfor-mance and error analysis are discussed.1 IntroductionContinuing a two-year tradition, the PASCAL Net-work organized the Third Recognizing Textual En-tailment Challenge1 (RTE 3) to further the researchon reasoning systems able to decide whether themeaning of one text (the entailed hypothesis, H) canbe inferred from another text (the entailing text, T ).Among this year?s challenges, approximately 15%of the (T, H) pairs contained long texts (more de-tails in Section 5.1).We approach the textual entailment problem as alogical implication between meanings (Fowler et al,2005; Tatu et al, 2006).
Our system transforms the1www.pascal-network.org/Challenges/RTE3two text snippets into three-layered semantically-rich logic form representations, generates an abun-dant set of lexical, syntactic, semantic, and worldknowledge axioms and, iteratively, searches for aproof for the entailment between the text T and apossibly relaxed version of the hypothesis H .
Apair is labeled as positive if the score of the foundproof (reflecting H?s degree of relaxation) is abovea threshold learned on the training data.
Figure 1summarizes our approach to RTE.2 Cogex?s Innovations for RTE 32.1 EXtended WordNet Knowledge BaseeXtended WordNet Knowledge Base (XWN-KB) isthe result of our ongoing research which capturesand stores the rich world knowledge encoded inWordNet?s glosses into a knowledge base.
In XWN-KB, the glosses have been transformed into a setof semantic relations using a semantic parser whoseoutput has been verified by human annotators.
Fig.
2displays the semantic relations derived for Nobellaureate?s definition.
Our system used this represen-tation for QA Dev pair 579 and QA Test pair 582 2.2.2 TARSQI ToolkitThe TARSQI project (Temporal Awareness and Rea-soning Systems for Question Interpretation)3 (Ver-hagen et al, 2005) builds a modular system whichdetects, resolves and normalizes time expressions(both absolute and relative times) - GUTime tag-ger; marks events and their grammatical features -2Table 6 lists the pairs referenced throughout the paper.3http://www.timeml.org/site/tarsqi22NLPAxioms KnowledgeWorld SemanticCalculusTemporalAxiomsXWN LexicalChainsAxioms on demandNamed EntityRecognitionPart?of?SpeechTaggingSyntacticParsingCoreferenceResolutionSemanticParsingTime and EventRepresentationText THypothesis HUsableSet of SupportAbductionBackoffProof, Proof score?HLFTLFKnowledge RepresentationCOGEXFigure 1: Cogex?s ArchitectureThe Pet passport alone can be [used]e1:occurrence to [enter]e2:occurrence the UK, but it will not [suffice]e3:occurrence to[enter]e4:occurrence many countries.
For instance Guatemala, like almost every country, [demands]e5:occurrence that allimported pets have a rabies vaccination, but will not [accept]e6:i action the Pet passport as proof of [said]e7:reporting vacci-nation.modality: (e1:can); tense: (e2:infinitive), (e3:future), (e4:infinitive), (e5:present), (e6:future), (e7:past); polarity:(e3:negative), (e6:negative); slink: modal(e1, e2), modal(e5, e6), factive(e6, e7); tlink: before(e1, e2), before(e5, e6),before(e4, e5), before(e6, e7), before(e2, e3), before(e2, e4)Table 1: TARSQI?s Treatment of IE Dev pair 63?s TISA(Nobel_laureate,winner)THEME(Nobel_prize,winner)Nobelist, Nobel_laureate; synsetId: 06822770gloss: winner of a Nobel_prizeFigure 2: XWN-KB Treatment of Nobel laureateEvita; identifies subordination constructions intro-ducing modality information - Slinket; adds tempo-ral relations between events and temporal expres-sions - GUTenLINK; and computes temporal clo-sures - SputLink.
We used the information providedby the TARSQI toolkit (Run #1) as an alternative toour event detection and temporal expression identifi-cation and normalization modules (Run #2).
Table 1shows TARSQI?s output for IE Dev pair 63?s T .The following sections present innovations re-lated to the logic form knowledge representation.2.3 Logic Representation of EventsFor events, the logic representation of their describ-ing concept was augmented with a special predicate(event EV(e1)).
When we made use of TARSQI?soutput (Run #1), the event predicate was replacedby the class of the event (occurrence EV(e1),state EV(e1), reporting EV(e1), etc.
).2.4 NegationRecently, the logic representation of sentences withnegated concepts was altered to mark as negated theentire scope of the negation.
For example, the logicform of IE Dev pair 90?s H: Kennon did not partici-pate in the WWII, formerly equal to Kennon NN(x1)& -participate VB(e1,x1,x4) & in IN(e1,x2)& WWII NN(x2) & conflict NE(x2) &AGT SR(x1,e1), became Kennon NN(x1)& WWII NN(x2) & conflict NE(x2) &-(exists e1 (participate VB(e1,x1,x3) &in IN(e1,x2) & AGT SR(x1,e1))) which is closerto the meaning of the English text snippet.
ForRun #1 (with TARSQI output), we only used thepolarity information attached to the identified eventsand negated the event?s predicate.2.5 Coreference ResolutionIn order to cope with the long text pairs, we addedin our processing pipeline a dedicated pronomi-nal coreference resolution module which replacedthe inter-sentential resolution processing we useduntil now.
The new tool combines Hobbs al-gorithm (Hobbs, 1978) and the Resolution ofAnaphora Procedure (RAP) algorithm (Lappin andLeass, 1994).
For the RTE task, it is very importantto have tight connections between the predicates of23Semantic Relation Axiom TemplatesISA n1(x1) -> n2(x1); v1(e1,x1,x2) -> v2(e1,x1,x2)DERIVATION n(x1) -> v(e1,x1,x2) & AGENT SR(x1,e1); n(e1) -> v(e1,x1,x2)v(e1,x1,x2) -> n(x1); v(e1,x1,x2) -> n(e1)CAUSE v1(e1,x1,x2) -> v2(e2,x2,x3) & CAUSE SR(e1,e2)AGENT n1(x1) -> n2(x2) & AGENT SR(x1,x2)PERTAIN a(x1,x2) -> n(x1)Table 2: Semantic Relation - Axiom Template mappinglong texts.
For example, for QA Dev pair 409, re-solving the pronoun he to George H.W.
Bush is astep needed to correctly label the pair.
But IE Devpair 92 requires more advanced anaphora resolutionwhich corefers the team and the Kinston Indians.3 Natural Language Axiom Improvements3.1 XWN Lexical ChainsIn order to take advantage of XWN-KB, we im-plemented few changes in our lexical chain ax-ioms generation module.
The most significant re-finement is the one axiom-per-chain relation ap-proach.
Previously, the system was generating oneaxiom for the entire lexical chain, but, given the di-versity of semantic relations which link the Word-Net concepts and the difficulty to reduce an entiresemantically rich chain to one implication whichcaptures its meaning, a remodeling of our axiomgeneration module was required.
Therefore, foreach relation in the best lexical chain found be-tween one of T ?s constituents and one of H?s con-stituents, an axiom is created.
For each seman-tic relation, we created a set of axiom templatesto be used during the axiom generation process.Several examples of axiom templates are shownin Table 2.
Therefore, a lexical chain is brokendown into several axioms whose relations are com-bined by the logic prover as it sees fit.
For in-stance, the chain oil company#n#1 agent??
sell#v#1entailment??
trade#v#1 is translated into the ax-ioms oil company NN(x1) -> sell VB(e1,x1,x2)& AGENT SR(x1,e1) and sell VB(e1,x1,x2) ->trade VB(e1,x1,x2) used to prove the entailmentfor IE Dev pair 196.We also changed the subset of senses consideredwhen lexical chains are built.
Previously, this subsetcontained the first k (k = 3) senses for each con-tent word.
For this year?s challenge, we changed thesense selection mechanism and we used the clusterof WordNet senses to which the fine-grained senseassigned by the Word Sense Disambiguation sys-tem corresponds.
We used the coarse-grained senseinventory for WordNet 2.1 released for Task #7 inSemEval-20074.
This clustering was created auto-matically with the aid of a methodology describedin (Navigli, 2006).
For example, the 10 WordNetsenses for the noun bank are mapped into 3 clusters.3.2 NLP AxiomsIn addition to the syntactic re-writing rules whichbreak down complex syntactic structures, includingcomplex nominals and coordinating conjunctions,we added a new type of NLP axioms which linksa named entity to its set of aliases.
For IE Dev pair35, the link between the Central Intelligence Agencymentioned in T and H?s CIA is very important.We also added a deeper analysis of multi-wordhuman named entities which marks last names(Hawking), rst (male/female) names (Stephen),titles (Prime Minister) and names for humanentities found in WordNet (Tony Blair).
Thisfine classification has three goals: (1) to markhuman entities with the gender information (usedby the pronominal coreference module); (2) toprevent lexical chains to use first names of hu-man entities as their source or target (Elizabethas part of Elizabeth Alexandra Mary should notbe mapped to {Elizabeth#1, Elizabeth II#1} or{Elizabeth#2, Elizabeth I#1} - QA Dev pair 407);(3) to create more precise NLP axioms for humanentities denoting noun compounds.
These axiomsfollow rules such as title(x1) & last name(x2)& nn NNC(x3,x1,x2) -> last name(x3) &title(x3), title(x1) & first name(x2) &last name(x3) & nn NNC(x4,x1,x2,x3) ->nn NNC(x4,x2,x3), etc.
For IR Dev pair 287, the4nlp.cs.swarthmore.edu/semeval24axiom Prime Minister NN(x6) & Giulio NN(x7)& Andreotti NN(x8) & nn NNC(x9,x6,x7,x8) &human NE(x9) -> Andreotti NN(x9) expressesthe equivalence between Prime Minister GiulioAndreotti and Andreotti.
During the processing ofthe development set, the prover used 75 axioms ofthis type.
During testing, 112 axioms proved to beuseful in finding proofs.4 Named Entity CheckBased on the guidelines for judging whether T en-tails or not H , hypotheses that introduce entitieswhich cannot be derived from T are not entailed bythe text (the pair is labeled as NO).
Therefore, wecreated a proof?s score adjustment module whichdeducts points for each pair whose H contains atleast one named entity not-derivable from T .
Oncethe prover used the loaded axioms to derive all thepossible information from the text, this named en-tity check is performed.
We note that the named en-tity heuristic is not equivalent with the removal ofa named entity predicate from the hypothesis in therelaxation stage which can also occur if the syntac-tic constraints in which the named entity participatesare not satisfied.
For instance, for IR Dev pair 387,Puncheon Lama is a new entity introduced by thehypothesis without any connection to the text.5 Experiments and Results5.1 Experimental DataThe RTE 3 data set was derived with four NLP ap-plications in mind: Information Extraction (IE), In-formation Retrieval (IR), Question Answering (QA),and Multi-document Summarization (SUM).
Statis-tics for this year?s dataset are shown in Table 3.
Onaverage, the long texts contain twice the number ofwords found in texts from pairs marked as short.5.2 Cogex?s PerformanceTable 4 details our submission results for Run #1(TARSQI?s events, temporal expressions and event-event and event-time relations) and Run #2 (LCC?sevent, temporal expressions and event-time rela-tions)5.
The two runs do not differ significantly.
The5A, AvgP, P, R and F stand for accuracy, average precision,precision, recall, and f-measure, respectively.Dataset True False OverallIE 105 (8) 95 (11) 200 (19)IR 87 (23) 113 (31) 200 (54)QA 106 (22) 94 (13) 200 (35)SUM 112 (5) 88 (4) 200 (9)Test 410 (58) 390 (59) 800 (117)Development 412 (78) 388 (57) 800 (135)Table 3: Data split between true and false classes.The number of pairs with long text is shown inparenthesis.Task A AvgP P R FRun #1IE 63.50 61.44 59.20 98.10 73.84IR 78.00 78.83 76.54 71.26 73.81QA 87.50 87.81 87.85 88.68 88.26SUM 60.00 61.54 58.99 93.75 72.41Test 72.25 69.42 67.41 88.78 76.63Dev 76.37 72.12 75.17 80.82 77.89Run #2IE 64.50 56.26 60.12 96.19 73.99IR 75.50 77.65 75.00 65.52 69.94QA 85.00 87.40 81.67 92.45 86.73SUM 62.00 58.16 60.47 92.86 73.12Test 71.75 67.97 67.16 87.80 76.11Dev 74.12 71.28 71.95 81.55 76.45Table 4: Results for Run #1 and Run #2extra information captured in the logic representa-tions used in Run #1 (as compared with Run #2)was not the focus of the entailment; the understand-ing it brings was not exercised during the entailmentrecognition process.
For the IR and QA tasks, Run#1 results are better when compared to Run #2?s.
Forthese tasks, the performance of the system is muchhigher when compared with the results obtained forIE and SUM.
Even tough the thresholds learned forthese two tasks best separate the positive from thenegative pairs on the development set, they prove tobe fairly low for the test set.
Almost all positive IEand SUM pairs are identified as such (very high re-call for both tasks), but a lot of negatives are also la-beled as positives (low precision, smaller accuracy).5.3 Named Entity Heuristic ImpactTable 5 details the interaction between the prover(Run #1) and the named entity heuristic6.
The6Coverage shows the number of pairs for which the heuris-tic fired, H?s A, C?s A and C+H?s A indicate, respectively, thenamed entity heuristic accuracy, Cogex?s and the prover?s when25heuristic fires for 167 pairs, while only 154 of themare true negative entailments (92.21% accuracy).The prover?s accuracy for the same subset of pairsis 65.86%.
The maximum overall improvement inaccuracy that the heuristic can bring is 5.5%, but,because of the way the heuristic penalizes the proofscores, its overall improvement is 4.12%.Task Coverage H?s A C?s A C+H?s AIE 19 100.00 42.10 100.00IR 56 94.64 73.21 94.64QA 59 94.91 77.96 94.91SUM 33 78.78 45.45 45.45Test 167 92.21 65.86 85.63Table 5: NE heuristic?s performance for Run #1In theory, the named entity check should not fail.But, in practice, its performance is influenced by theknowledge that the prover collects and, if this infor-mation is not complete, then the heuristic fails.
Forexample, for QA Dev pair 419, H mentions numberthree and because the prover cannot infer it as thecardinality of the elementary particles mentioned inT , the heuristic fires incorrectly.5.4 Error AnalysisSome of the sources of errors are:Lexical chains For IR Test pair 377, black plaguecan be derived from T ?s plague only if we allowlexical chains with more than 2 HYPONYMY re-lations (plague#n#1 hyponymy??
bubonic plague#n#1hyponymy??
black plague#n#1).
This restriction onlexical chains was added last year.
However, in thisyear?s data this restriction was detrimental as shownin the above example.Named entity heuristic Some of the errors intro-duced by the named entity heuristic are debatable.For example, IR Test pair 355?s hypothesis intro-duces the named entity German which cannot be de-rived from the text.
Similarly, for QA Test pairs 495and 496, the name Christian Democratic Union can-not be inferred from the text?s mention of ChristianDemocrat party.
On the other hand, pairs for whichthe score adjustment introduced by the named entityheuristic did not change the label assigned by theprover include SUM Test pair 656 whose hypothesisthe scores it computes are adjusted according to the heuristic.mentions US without it being derivable from the text(unless we consider the adjective domestic).World Knowledge For SUM Test pair 744, thesystem fails to infer nearly half a million dollarsfrom $480,350.
Similarly, the system failed to en-tail died in 1970 from the biographical markings?(1890-1970)?
for QA Test pair 486.High word overlap SUM pairs have a high de-gree of word-overlap between T and H and detec-tion of the non-entailment requires careful process-ing.
SUM Test pair 666?s text contains an extra ad-verbial phrase which changes the label of the pair.Reports and Modality Even though reportingverbs (X said that Y) and modalities (X may Y, Xtried to Y) should influence the validity of the state-ment they modify, most Y clauses are consideredtrue in the RTE data (SUM Dev pair 756, IR Devpair 295, IE Dev pair 148 are just few examples).Therefore, our solutions for representing7 or check-ing these modifiers8 failed to bring any improvementon the development set and were not included in theprocessing of the test set.But, for IE Test pair 172, T ?s main verb is qual-ified by threatened which is not present in H .
ForSUM Test pair 672, cited strong volume gains doesnot entail makes strong prots.6 ConclusionThe XWN-KB is an invaluable resource for recog-nizing textual entailment.
Its impact in RTE 3 wassignificant.
However, we are still exploring waysof fully exploiting this resource.
The use of theTARSQI toolkit did not impact the performance be-cause the temporal knowledge was not exercised inthis year?s task.
Contrary to our expectations, therepresentation of modality had a negative impact onthe performance.
This is perhaps due to incorrectrepresentation.
For our system, the introduction oflong texts did not cause significant problems.
Thesystem is robust enough to handle longer texts.7Our representation for X said Y which prevents the entail-ment that Y is (X(x1) & report CTXT(c1,x1)) -> (Y(e1))8We attempted to penalize proofs which infer the second ar-gument of an MODAL slink without entailing the first.
Pairs IEDev 191 and IR Dev 203 fall in this category, but have differentgold annotation labels.26Id Tag Pair Text and HypothesisD35 YES T : A leading human rights group on Wednesday identified Poland and Romania as the likely locations in eastern Europe of secret prisons where al-Qaeda suspectsare interrogated by the Central Intelligence Agency.H: CIA secret prisons were located in Eastern Europe.D92 YES T : The Kinston Indians are a minor league baseball team in Kinston, North Carolina.
The team, a Class A affiliate of the Cleveland Indians, plays in the CarolinaLeague.H: Kinston Indians participate in the Carolina League.D191 YES T : Though Wilkins and his family settled quickly in Italy, it wasn?t a successful era for Milan, and Wilkins was allowed to leave in 1987 to join French outfit ParisSaint-Germain.H: Wilkins departed Milan in 1987.D196 YES T : Some large Russian oil companies, including Lukoil, Zarubezhneft, the state-owned oil company, and Alpha Eco, the trader, were implicated by the report.H: Zarubezhneft trades in oil.D203 NO T : A decision to allow the exiled Italian royal family to return to Italy may be granted amid the discovery that the head of the family, Prince Vittorio Emmanuele,addressed the president of Italy properly.
He has called President Ciampi ?our president, the president of all Italians?.H: Italian royal family returns home.D287 YES T : Italy?s highest court has upheld a court verdict that partially cleared former Prime Minister Giulio Andreotti of having colluded with the Mafia.H: Andreotti is accused of Mafia collusion.D387 NO T : Thus, China?s President repeatedly sent letters and envoys to the Dalai Lama and to the Tibetan Government asking that Tibet ?join?
the Republic of China.H: Dalai Lama and the government of the People?s Republic of China are in dispute over Panchen Lama?s reincarnation.D409 YES T : George H.W.
Bush served this country not only as President but also as Vice President, Member of Congress, United Nations Ambassador, chief of the U.S. LiaisonOffice to the People?s Republic of China, Director of the Central Intelligence Agency and also, as a naval aviator in World War II.
Coming back from the war, hemarried his sweetheart, Barbara Pierce of Rye, New York, and later that year made his first civilian adult decision when he made the appropriate choice of moving toTexas, where he lived the rest of his life.H: The name of George H.W.
Bush?s wife is Barbara.D419 YES T : Discovery of the top quark, if confirmed, completes one set of subatomic building blocks whose existence is predicted by the prevailing theory, called the StandardModel, of the particles and forces that determine the fundamental nature of matter and energy.
In the whimsical lexicon of modern physics, the elementary particlesare called quarks, leptons and bosons.H: Quarks, leptons, and bosons are the three elementary particles of physics according to the Standard Model.D579 YES T : Salma Hayek drew a crowd in Veracruz, Mexico, at the July 8 premiere of ?Nobody Writes to the Colonel?, a movie based on a short novel by Nobel laureateGabriel Garcia Marquez.H: Gabriel Garcia Marquez is a Nobel prize winner.D756 YES T : The contaminated pills included metal fragments ranging in size from ?microdots?
to portions of wire one-third of an inch long, the FDA said.H: The contaminated pills contained metal fragments.T172 NO T : This year thousands of Hindu Holy Men, also known as sadhus, threatened to boycott festivals during their pilgrimage to the Ganges, where their rituals involvewashing away their sins by bathing in the water.H: Hindu Holy Men boycotted festivals during their pilgrimage to the Ganges.T355 YES T : Before reconstruction began, the Reichstag was wrapped by the Bulgarian artist Christo and his wife Jeanne-Claude in 1995, attracting millions of visitors.H: Christo wraps German Reichstag.T377 YES T : The U.S. enjoyed miraculously long immunity from the dreaded plague that used to sweep Europe.H: Black plague swept Europe.T495 YES T : Former German Chancellor Helmut Kohl said Thursday he will not break pledges he made to campaign contributors by publicly disclosing their names eventhough his Christian Democrat party has directed him to reveal their identities.H: The name of Helmut Kohl?s political party is the Christian Democratic Union.T561 YES T : Pope John Paul II arrived Saturday in the birthplace of the Solidarity movement that he sparked with his first papal visit 20 years ago, offering Poles ?the greetingof a fellow Pole who comes among you to fulfill the needs of his own heart.
?H: Pope John Paul II was born in Poland.T565 NO T : On the domestic tobacco front, operating income rose by 12 per cent to Dollars 914m, with ?slightly higher unit volume?.H: US tobacco income has risen.T666 NO T : Boys and girls will be segregated during sex education in junior high school.H: Boys and girls will be segregated in junior high school.T672 NO T : Philip Morris cited strong volume gains in Germany, Italy, France, Spain, central and eastern Europe, the Far East, Japan, Korea, Argentina and Brazil.H: Philip Morris makes strong profits also in Europe.T725 YES T : After years of battling between oil companies, the Ecuadorian government decided to collaborate with indigenous groups.H: The Ecuadorian government collaborated with indigenous groups.Table 6: Examples of RTE 3 pairs.
D# and T# refer to the # pair from the dev and the test set, respectivelyAcknowledgmentsWe would like to thank Iulia Nica for her helpfulsuggestions (including the named entity check) andMarc Verhagen and James Pustejovsky for present-ing us with TARSQI?s output for the RTE datasets.ReferencesA.
Fowler, B. Hauser, D. Hodges, I. Niles, A. Novischi,and J. Stephan.
2005.
Applying COGEX to Recog-nize Textual Entailment.
In Proceedings of the PAS-CAL Challenges Workshop.J.
Hobbs.
1978.
Resolving Pronoun References.
Lingua,44:311?338.S.
Lappin and H. Leass.
1994.
An Algorithm forPronominal Anaphora Resolution.
ComputationalLinguistics, 20(4):535?561.R.
Navigli.
2006.
Meaningful Clustering of SensesHelps Boost Word Sense Disambiguation Perfor-mance.
In Proceedings of COLING-ACL 2006.M.
Tatu, B. Iles, J. Slavick, A. Novischi, andD.
Moldovan.
2006.
COGEX at the Second Rec-ognizing Textual Entailment Challenge.
In Proceed-ings of the Second PASCAL Challenges Workshop onRecognising Textual Entailment, Venice, Italy.M.
Verhagen, I. Mani, R. Sauri, R. Knippen, J .Littman,and J. Pustejovsky.
2005.
Automating Temporal An-notation with TARSQI.
In Proceedings of ACL 2005.Demo Session.27
