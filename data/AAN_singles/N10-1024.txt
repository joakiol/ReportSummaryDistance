Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 207?215,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsCheap, Fast and Good Enough:Automatic Speech Recognition with Non-Expert TranscriptionScott Novotney and Chris Callison-BurchCenter for Language and Speech ProcessingJohns Hopkins Universitysnovotne@bbn.com ccb@jhu.eduAbstractDeploying an automatic speech recogni-tion system with reasonable performancerequires expensive and time-consumingin-domain transcription.
Previous workdemonstrated that non-professional anno-tation through Amazon?s Mechanical Turkcan match professional quality.
We useMechanical Turk to transcribe conversa-tional speech for as little as one thir-tieth the cost of professional transcrip-tion.
The higher disagreement of non-professional transcribers does not have asignificant effect on system performance.While previous work demonstrated thatredundant transcription can improve dataquality, we found that resources are bet-ter spent collecting more data.
Finally, wedescribe a quality control method withoutneeding professional transcription.1 IntroductionSuccessful speech recognition depends on hugeinvestments in data collection.
Even after train-ing on 2000+ hours of transcribed conversa-tional speech, over a billion words of languagemodeling text, and hand-crafted pronunciationdictionaries, state of the art systems still havean error rate of around 15% for English (Prasadet al, 2005) Transcribing the large volumes ofdata required for Large Vocabulary ContinuousSpeech Recognition (LVCSR) of new languagesappears prohibitively expensive.
Recent workhas shown that Amazon?s Mechanical Turk1 can1http://www.mturk.combe used to cheaply create data for other nat-ural language processing applications (Snow etal., 2008; Zaidan and Callison-Burch, 2009; Mc-Graw et al, 2009).
In this paper we focuson reducing the cost of transcribing conversa-tional telephone speech (CTS) data.
Previousmeasurements of Mechanical Turk stopped atagreement/disagreement with professional an-notation.
We take the next logical step andmeasure performance on systems trained withnon-professional transcription.Mechanical Turk is an online labor mar-ket where workers (or Turkers) perform simpletasks called Human Intelligence Tasks (HITs)for small amounts of money ?
frequently as lit-tle as $0.01 per HIT.
Since HITs can be tasksthat are difficult for computers, but easy for hu-mans, they are ideal for natural language pro-cessing tasks (Snow et al, 2008).
MechanicalTurk has even spawned a business that special-izes in manual speech transcription.2Automatic speech recognition (ASR) of con-versational speech is an extremely difficult prob-lem.
Characteristics like rapid speech, pho-netic reductions and speaking style limit thevalue of non-CTS data, necessitating in-domaintranscription.
Even a few hours of transcrip-tion is sufficient to bootstrap with unsupervisedmethods like self-training (Lamel et al, 2002).The speech community has built effective down-stream solutions for the past twenty years de-spite imperfect recognition.
In topic classifi-cation, 90% accuracy is possible on conversa-tional data even with 80%+ word error rate2http://castingwords.com/207(WER) (Gillick et al, 1993).
Other successfultasks include information retrieval from speech(Miller et al, 2007) and spoken dialogue pro-cessing (Young et al, 2007).
Inexpensive tran-scription would quickly open new languages ordomains (like meeting or lecture data) for auto-matic speech recognition.In this paper, we make the following points:?
Quality control isn?t necessary as a systembuilt with non-professional transcription isonly 6% worse for 130 the cost of professionaltranscription.?
Resources are better spent collecting moredata than improving data quality.?
Transcriber skill can be accurately esti-mated without gold standard data.2 Related WorkResearch into Mechanical Turk by the NLP com-munity has largely focused on comparing thequality of annotations produced by non-expertTurkers against annotations created by experts.Snow et al (2008) conducted a comprehensivestudy across a variety of NLP tasks.
Theyshowed that high agreement could be reachedwith gold-standard expert annotation for thesetasks through a weighted combination of ten re-dundant annotations produced by Turkers.Callison-Burch (2009) showed similar resultsfor machine translation evaluation, and furthershowed that Turkers could accomplish complextasks like translating Urdu or creating readingcomprehension tests.McGraw et al (2009) used Mechanical Turkto improve an English isolated word speech rec-ognizer by having Turkers listen to a word andselect from a list of probable words at a cost of$20 per hour of transcription.Marge et al (2010) collected transcriptions ofverbal instructions to robots with clean speech.By using five duplicate transcriptions, the aver-age transcription disagreement with experts wasreduced from 4% to 2%.Previous efforts at reducing the cost of tran-scription include the EARS Fisher project (Cieriet al, 2004), which collected 2000+ hours of En-glish CTS data ?
an order of magnitude morethan had previously been transcribed.
To speedtranscription and lower costs, Kimball et al(2004) created new transcription guidelines andused automatic segmentation.
These improvedthe speed of transcription from fifty times realtime to six times real time, and made it costeffective to transcribe 2000 hours at an aver-age of $150 per hour.
Models trained on thefaster transcripts exhibited almost no degra-dation in performance, although discrimanitvetraining was sensitive to transcription errrors.3 Experiment Description3.1 CorporaWe conducted most experiments on a twentyhour subset of the English Switchboard corpus(Godfrey et al, 1992) where two strangers con-verse about an assigned topic.
We used two setsof transcription as our gold standard: high qual-ity transcription from the LDC and those fol-lowing the Fisher quick transcription guidelines(Kimball et al, 2004) provided by a professionaltranscription company.
All English ASR modelswere tested with the carefully transcribed threehour Dev04 test set from the NIST HUB5 eval-uation.3 A 75k word lexicon taken from theEARS Fisher training corpus covers the LDCtraining data and has a test OOV rate of 0.18%.We also conducted experiments in Korean andcollected Hindi and Tamil data from the Call-friend corpora 4.
Participants were given a freelong distance phone call to talk with friends orfamily in their native language, although En-glish frequently appears.
Since Callfriend wasoriginally intended for language identification,only the 27 hour Korean portion has been tran-scribed by the LDC.3.2 LVCSR SystemWe used Byblos, a state-of-the-art multi-passLVCSR system with state-clustered Gaussiantied-mixture acoustic models and modifiedKneser-Ney smoothed language models (Prasadet al, 2005).
While understanding the system3http://www.itl.nist.gov/iad/mig/tests/ctr/1998/current-plan.html4http://www.ldc.upenn.edu/CallFriend2/208details is not essential for this work, we providea brief description for completeness.Recognition begins with cepstral feature ex-traction using concatenated frames with cepstralmean subtraction and HLDA to reduce the fea-ture dimension space.
Vocal track length nor-malization follows.
Decoding then requires threepasses: a fast forward pass with coarse one-gaussian-per-phone models and bigram LM fol-lowed by a backward pass with triphone modelsand a trigram LM to generate word confusionlattices.
The lattices are rescored with a morepowerful quinphone cross-word acoustic modeland trigram LM to extract the one best out-put.
These three steps are repeated after un-supervised speaker adaptation with constrainedMLLR.
Decoding is around ten times real time.3.3 Transcription TaskUsing language-independent speaker activity de-tection models, we segmented each ten minuteconversation into five second utterances, greatlysimplifying the transcription task (Roy and Roy,2009).
Utterances were assigned in batches often per HIT and played with a simple flashplayer with a text box for entry.
All non-emptyHITs were approved and we did not awardbonuses except as described in Section 5.1.3.4 Measuring Annotation QualityThe usefullness of the transcribed data is ul-timately measured by how much it benefits aspeech recognition system.
Factors that inflatedisagreement (word error rate) between Turkersand professionals do not necessarily impact sys-tem performance.
These include typographicalmistakes, transcription inconsistencies (like im-properly marking hesitations or the many vari-ations of um) and spelling variations (geez orjeez are both valid spellings).
Additionally, thegold standard is itself imperfect, with typicalestimates of professional disagreement aroundfive percent.
Therefore, we judge the quality ofMechanical Turk data by comparing the perfor-mance of one LVCSR system trained on Turkerannotation and another trained on professionaltranscriptions of the same dataset.Average Turker Transcription Productivity for EnglishTranscription time / Utterance length (xRT)Number of Turkers0 10 20 30 40 50 60020406080100FisherQuickTransSpeedTypical HighQualitySpeedMean Turker Productivity 12xRTFigure 1: Histogram of per-turker transcription ratefor twenty hours of English CTS data.
Historicalestimates for high quality transcription are 50xRT.The 2004 Fisher transcription effort achieved 6xRTand the average here is 11xRT.4 Establishing Best Practices withEnglish SwitchboardAs an initial test to see how cheaply conversa-tional data could be transcribed, we uploadedone hour of test data from Hub5 Dev04.
Wefirst paid $0.20 per HIT ($0.02 per utterance).This test finished quickly, and we measured theaverage disagreement with professionals at 17%.Next, we reduced payment to $0.10 per HITand disagreement was again 17%.
Finally, wepushed the price down to $0.05 per HIT or $5per hour of transcription and again disagree-ment was nearly identical at 18%, although afew Turkers complained about the low pay.Using this price, we then paid for the fulltwenty hours to be redundantly transcribedthree times.
1089 Turkers participated in thetask at an incoming rate of 10 hours of tran-scription per day.
On average, each Turker tran-scribed 30 utterance (earning 15 cents) at anaverage professional disagreement of 23%.
Tran-scribing one minute of audio required an aver-age eleven minutes of effort (denoted 11xRT).63 workers transcribed more than one hundredutterances and one prolific worker transcribed1223 utterances.2094.1 Comparing Non-Professional toProfessional TranscriptionTable 1 details the results of different selectionmethods for redundant transcription.
For eachmethod of selection, we build an acoustic andlanguage model and report WER on the heldouttest set (transcribed at very high accuracy).We first randomly selected one of the threetranscriptions per utterance (as if the data wereonly tanscribed once) and repeated this threetimes with little variance.
Selecting utterancesrandomly by Turker performed similarly.
Per-formance of an LVCSR system trained on thenon-professional transcription degrades by only2.5% absolute (6% relative) despite a disagree-ment of 23%.
This is without any qualitycontrol besides throwing out empty utterances.The degradation held constant as we swept theamount of training data frome one to twentyhours.
Bot the acoustic and language models ex-hibited the log-linear relationship between WERand the amount of training data.
Independent ofthe amount of training data, the acoustic modeldegraded by a nearly constant 1.7% and the lan-guage model by 0.8%.To evaluate the benefit of multiple transcrip-tions, we built two oracle systems.
The Turkeroracle ranks Turkers by the average error rate oftheir transcribed utterances against the profes-sionals and selects utterances by Turker until thetwenty hours is covered (Section 4.3 discusses afair way to rank Turkers).
The utterance oracleselects the best of the three different transcrip-tions per utterance.
The best of the three Turk-ers per utterance wrote the best transcriptiontwo thirds of the time.The utterance oracle only recovered half ofthe degradation for using non-professional tran-scription.
Cutting the disagreement in half(from 23% to 13%) reduced the WER gap byabout half (from 2.5% to 1%).
Using the stan-dard system combination algorithm ROVER(Fiscus, 1997) to combine the three transcrip-tions per utterance only reduced disagreementfrom 23% to 21%.
While previous work bene-fited from combining multiple annotations, thistask shows little benefit.TranscriptionDisagreementASR WERwith LDCRandom Utterance 23% 42.0%Random Turker 20% 41.4%Oracle Utterance 13% 40.9%Oracle Turker 18% 41.1%Contractor < 5% 39.6%LDC - 39.5%Table 1: Quality of Non-Professional Transcriptionon 20 hours of English Switchboard.
Even thoughdisagreement for random selection without qualitycontrol has 23% disagreement with professional tran-scription, an ASR system trained on the data is only2.5% worse than using LDC transcriptions.
The up-per bound for quality control (row 3) recovers only50% of the total loss.4.2 Combining with External SourcesWhile in-domain speech transcription is typi-cally the only effective way to improve the acous-tic model, out-of-domain transcripts tend tobe useful for language models of conversationalspeech.
Broadcast News (BN) transcription isparticularly well suited for English Switchboarddata as the topics tend to cover news itemslike terrorism or politics.
We built a smallone million word language model (to simulate aresource-poor language) and interpolated it withvarying amounts of LDC or Mechanical Turktranscriptions.
Figure 2 details the results.4.3 The Value of Quality ControlWith a fixed transcription budget, should oneeven bother with redundant transcription to im-prove an ASR system?
To find out, we tran-scribed 40 additional hours of Switchboard us-ing Mechanical Turk.
Disagreement to the LDCtranscriptions was 24%, similar to the initial20 hours.
The two percent degradation of testWER when using Mechanical Turk compared toLDC held up with 40 and 60 hours of training.Given a fixed budget of 60 hours of transcrip-tion, we compared the quality of 20 hours tran-scribed three times to 60 hours transcribed once.The best we could hope to recover from the threeredundant transcriptions is the utterance oracle.Oracle and singly transcribed data had 13% and24% disagreement with LDC respectively.
Sys-tem performance was 40.9% with 20 hours of210lllllImproving the Language ModelWords of Transcription for Training (log scale)TestWERl lll lllllll llll10K 20K 40K 80K 160K38%40%42%44%46%48%MTurk onlyLDC OnlyMTurk + 1M BNLDC + 1M BN1M word BN LM Initial WER0.8% Average Degradation0.6% Average Degradation(All decodes with a fixed 16 hour LDC acoustic model)Figure 2: WER with a varied amount of LM trainingdata and a fixed 16hr acoustic model.
MTurk tran-scription degrades WER by 0.8% absolute across LMsize.
When interpolated with 1M words of broadcastnews, this degradation shrinks to 0.6%.the former and 37.6% with 60 hours of the latter.Even though perfect selection cuts disagreementin half, three times as much data helps more.The 2004 Fisher effort averaged a price of $150per hour of English CTS transcription.
Thecompany CastingWords produces high quality(Passy, 2008) English transcription for $90 anhour using Mechanical Turk by a multi-pass pro-cess to collect and clean Turker-provided tran-scripts.
While we did not use their service, weassume it is of comparable quality to the pri-vate contractor used earlier.
The price for LDCtranscription is not comparable here since it wasintended for more precise linguistic tasks.
Ex-trapolating from Figure 3, the entire 2000 Fishercorpus could be transcribed using MechanicalTurk at the same cost of collecting 60 hours ofprofessional transcription.5 Collection in Other LanguagesTo test the feasability of improving low-resourcelanguages, we attempted to collect transcrip-tions for Korean, Hindi, Tamil CTS data.
Webuilt an LVCSR system in Korean since it is theonly one with reference LDC transcriptions touse as a test set.l100 200 500 1000 2000 5000 1000030354045Comparing Cost of Reducing WERTotal Cost (Dollars) to Collect Data (log scale)TestWER lllMTurk ?
$5/hrMturk w/Oracle QC ?
$15/hrCasting Words ?
$90/hrPrivate Contractor ?
$150/hrTest WER with 20?60 hours of Switchboard TranscriptionFigure 3: Historical cost estimates are $150 per hourof transcription (blue cirlces).
The company CastingWords uses Turkers to transcribe English at $90 perhour which we estimated to be high quality (greentriangles).
Transcription without quality control onMechanical Turk (red squares) is drastically cheaperat $5 per hour.
With a fixed budget, it is betterto transcribe more data at lower quality than to im-prove quality.
Contrast the oracle WER for 20 hourstranscribed three times (red diamond) with 60 hourstranscribed once (bottom red square).5.1 KoreanKorean is spoken by roughly 78 million speak-ers world wide and is written in Hangul, a pho-netic orthography, although Chinese charactersfrequently appear in written text.
Since Koreanhas essentially arbitrary spacing (Chong-Woo etal., 2001), we report Phoneme Error Rate (PER)instead of WER, which would be unfairly pe-nalized.
Both behave similarly as system per-formance improves.
For comparison, an EnglishWER of 39.5% has a PER of 34.8%.We uploaded ten hours of audio to be tran-scribed once, again segmented into short snip-pets.
Transcription was very slow at first andwe had to pay $0.20 per HIT to attract work-ers.
We posted a separate HIT to refer Koreantranscribers, paying a 25% bonus of the incomeearned by referrals.
This was quite successfulas two referred Turkers contributed over 80%of the total transcription (at a cost of $25 per211hour instead of $20).
We collected three hoursof transcriptions after five weeks, paying eightTurkers $113 at a transcription rate of 10xRT.Average Turker disagreement to the LDCreference was 17% (computed at the charac-ter level).
Using these transcripts to train anLVCSR system instead of those provided byLDC degraded PER by 0.8% from 51.3% to52.1%.
For comparison, a system trained on theentire 27 hours of LDC data had 41.2% PER.Although performance seems poor, it is suf-ficiently good to bootstrap with acoustic modelself-training (Lamel et al, 2002).
The languagemodel can be improved by finding ?conversa-tional?
web text found with n-gram queries ex-tracted from the three hours of transcripts (Bu-lyko et al, 2003).5.2 Hindi and TamilAs a feasability experiment, we collected onehour of transcription in Hindi and Tamil, pay-ing $20 per hour of transcription.
Hindi andTamil transcription finished in eight days, per-haps due to the high prevalence of Turkers inIndia (Ipeirotis, 2008).
While we did not haveany professional reference, Hindi speaking col-leagues viewed some of the data and pointedout errors in English transliteration, but over-all quality appeared fine.
The true test will beto build an LVCSR system and report WER.6 Quality Control sans Quality DataAlthough we have shown that redundantly tran-scribing an entire corpus gives little gain, thereis value in some amount of quality control.
Wecould improve system performance by only re-jecting Turkers with high disagreement, similarto confidence selection for active learning or un-supervised training (Ma and Schwartz, ).
But ifwe are transcribing a truly new domain, there isno gold-standard data to use as reference, so wemust estimate disagreement against errorful ref-erence.
In this section we provide a practical usefor quality control without gold standard refer-ence data.Distribution of Turker SkillAverage Disagreement of Transcribed Utterances by Each Turker0% 10% 30% 50% 70% 90%NormalizedDensityEstimated Against ProfessionalsEstimated Against Other Turkers23%25%Figure 4: Each Turker was judged against profes-sional and non-professional reference and assignedan overall disagreement.
The distribution of Turkerdisagreement follows a gamma distribution, with atight cluster of average Turkers and a long-tail of badTurkers.
Estimating with non-professionals (eventhough the reference is 23% wrong on average) issurprisingly well matched to professional estimate.Turker estimation over-estimated disagreement byonly 2%.6.1 Estimating Turker SkillUsing the twenty hour English transcriptionsfrom Section 4, we computed disagreement foreach Turker against the professional transcrip-tion for all utterances longer than four words.Note that each utterance was transcribed bythree random turkers, so there is not one set ofutterances which were transcribed by all turk-ers.
Each Turker transcribed a different, par-tially overlapping, subset of the data.For a particular Turker, we estimated the dis-agreement with other Turkers by using the twoother transcripts as reference and taking theaverage.
Figure 4 shows the density estimateof Turker disagreement when calculated againstprofessional and non-professional transcription.On average, the non-professional estimate was3% off from the professional disagreement.Given that non-professional disagreement isa good estimate of professional disagreement212Quickly Estimating DisagreementNumber of Utterances to Estimate Non?Professional DisagreementDifferencefromProfessional EstimationonAllUtterances0%5%10%15%20%25%30%0 5 10 15 20 25 30MinimumFirst QuartileMedianThird QuartileMaximumFigure 5: Boxplot of the difference of non-professional disagreement with a fixed number of ut-terances to professional disagreement over all utter-ances.
While error is expectedly high with one ut-terance, 50% of the estimates are within 3% of thetruth after ten utterances and 75% of the estimatesare within 6% after fifteen utterances.over all of a Turker?s utterances, we wonderedhow few needed to be redundantly transcribedby other non-professionals.
For each Turker,we started by randomly selecting one utter-ance and computed the non-professional dis-agreement.
We compared the estimate to thetrue professional disagreement over all of the ut-terances and repeatedly sample 20 times.
Thenwe increased the number of utterances used toestimate non-professional disagreement until allutterances by that Turker are selected.Figure 5 shows a boxplot of the differences ofnon-professional to professional disagreement onall utterances.
As few as fifteen utterances needto be redundantly transcribed to accurately es-timate three out of four Turkers within 5% ofthe professional disagreement.6.2 Finding the Right TurkersSince we can accurately predict a Turker?s skillwith as few as fifteen utterances on average, wecan rank Turkers by their professional and non-professional disagremeents.
By thresholding ondisagreement, we can either select good turk-++ +++++ ++++++++++++++ +++++ ++ + +++ +++++ ++ +++++ +++++++++++++++++++++++++ +++++++ +++++++ +++++ ++++++++++++++++++++ +++ ++++++++++++++++++++++++++++ +++++++++++ ++ ++++++++++++++++ ++++ ++++++++++++0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Rating Turkers: Professional v. Non?ProfessionalTurker Disagreement against ProfessionalTurkerDisagreement againstotherTurkers++ +++++++++++ +++++ ++++++++++++++++++++++++++++++++++++++++ ++++++++++++++++++++++ ++++++ +++++++++++++++++++++++++++++++++++++++ +++++ +++ ++ +++++++++ +++++++++++ +++++++++++++++++++++++++++++ + +++++++++++++++++++ +++++++ +++++++ ++ ++++++++++++++ ++++++++++++++++ ++ +++++++ ++++Incorrect Reject12.5%Correct Accept57.54%Incorrect Accept4.5%Correct Reject25.46%Threshold at Mean Disagreement of 23.17%Figure 6: Each Turker is a point with professional (Xaxis) plotted against non-professional (Y axis) dis-agreement.
The non-professional disagreement cor-relates surprisingly well with professional disagree-ment even though the transcripts used as referenceare 23% wrong on average.
By setting a selectionthreshold, the space is divided into four quadrants.The bottom left are correctly accepted: both non-professional and professional disagreement are belowthe threshold.
The top left are incorrectly rejected:using their transcripts would have helped, but theydon?t hurt system performance, just waste money.The top right are correctly rejected for having highdisagreement.
The bottom right are the troublesomefalse positives that are included in training but actu-ally may hurt performance.
Luckily, the ratio of falsenegatives to false positives is usually much larger.ers or equivalently reject bad turkers.
We canview the ranking as a precision/recall problem toselect only the ?good?
Turkers below the thresh-old.
Figure 6 plots each Turker where the X axisis the professional disagreement and the Y axisis the non-professional disagreement.
Sweepingthe disagreement threshold from zero to one gen-erates Figure 7, which reports F-score (the har-monic mean of precision and recall).
This sec-tion suggests a concrete qualification test by firsttranscribing 15-30 utterance multiple times tocreate a gold standard.
Using the transcriptionfrom the best Turker as reference, approve newTurkers with a WER less than the average WERfrom the initial set.2130.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Selecting Turkers by Estimated SkillWER Selection ThresholdF?scoreFigure 7: It is difficult to find only good Turkers sincethe false positives outnumber the few good workers.However, rejecting bad Turkers becomes very easyonce past the mean error rate of 23%.
It is better touse disagreement estimation to reject poor workersinstead of finding good workers.7 Experience with Mechanical TurkWe initially expected to invest most of our ef-fort in managing Turker transcription.
But thevast majority of Turkers completed the effort ingood faith with few complaints about pay.
Manyleft positive comments5 despite the very difficulttask.
Indeed, the author?s own disagreement ona few dozen English utterances were 17.7% and26.8% despite an honest effort.Instead, we spent most of our time normaliz-ing the transcriptions for English acoustic modeltraining.
Every single misspelling or new wordhad to be mapped to a pronunciation in orderto be used in training.
We initially discardedany utterance with an out of vocabulary word,but after losing half of the data, we used a setof simple heuristics to produce pronunciations.Even though there were a few thousand of theseerrors, they were all singletons and had littleeffect on performance.
Turkers sometimes leftcomments in the transcription box such as ?no5One Turker left a comment ?You don?t grow pick-les!!?
in regards to the misinformed speakers she wastranscribing.audio?
or ?man1: man2:?.
These errant tran-scriptions could be detected by force aligningthe transcript with the audio and rejecting anywith low scores (Lamel et al, 2000).
Extendingtranscription to thousands of hours will requirerobust methods to automatically deal with er-rant transcripts and additionally run the risk ofexhausting the available pool of workers.Finding Korean transcribers required themost creativity.
We found success in interact-ing with the transcribers, providing feedback,encouragement and paying bonuses for referringother workers.
Cultivating workers for a newlanguage is definitely a ?hands on?
process.For Hindi and Tamil, Turkers sometimes mis-interpreted or ignored instructions and trans-lated into English or transliterated into Romancharacters.
Additionally, some linguistic knowl-edge is required to classify phonemic categories(like fricative or sonorant) required for acousticmodel training.8 ConclusionUnlike previous work which studied the qualityof Mechanical Turk annotations alone, we judgeits value in terms of the real task: improvingsystem performance.
Despite relatively high dis-agreement with professional transcription, datacollected with Mechanical Turk was nearly aseffective for training speech models.
Since thisdegradation is so small, redundant annotation toimprove quality is not worth the cost.
Resourcesare better spent collecting more transcription.In addition to English, we demonstrated similartrends in Korean and also collected transcriptsfor Hindi and Tamil.
Finally, we proposed aneffective procedure to reduce costs by maintain-ing the quality of the annotator pool withoutneeding high quality annotation.AcknowledgmentsThis research was supported by the EuroMa-trixPlus project funded by the European Com-mission by the DARPA GALE program underContract No.
HR0011-06-2-0001, and NSF un-der grant IIS-0713448 and by BBN Technologies.The views and findings are the authors?
alone.214ReferencesIvan Bulyko, Mari Ostendorf, and A. Stolcke.
2003.Getting more mileage from web text sources forconversational speech language modeling usingclass-dependent mixtures.
In HLT-NAACL.Chris Callison-Burch.
2009.
Fast, Cheap, and Cre-ative: Evaluating Translation Quality Using Ama-zons Mechanical Turk.
EMNLP.Seung-Shik Kang Chong-Woo, Chong woo Woo, andKookmin Univerity.
2001.
Automatic segmen-tation of words using syllable bigram statistics.In 6th Natural Language Processing Pacific RimSymposium.Christopher Cieri, David Miller, and Kevin Walker.2004.
The fisher corpus: a resource for the nextgenerations of speech-to-text.
In LREC.Jonathan G. Fiscus.
1997.
A post-processing sys-tem to yield reduced word error rates: Recognizeroutput voting error reduction (rover).L.
Gillick, J. Baker, J. Bridle, M. Hunt, Y. Ito,S.
Lowe, J. Orloff, B. Peskin, R. Roth, and F. Scat-tone.
1993.
Application of large vocabulary con-tinuous speech recognition to topic and speakeridentification using telephone speech.
In ICASSP.Jack Godfrey, Edward Holliman, and Jane Mc-Daniel.
1992.
Switchboard: Telephone speechcorpus for research and development.
In ICASSP.Panos Ipeirotis.
2008.
Mechanical turk: The de-mographics.
http://behind-the-enemy-lines.blogspot.com/2010/03/new-demographics-of-mechanical-turk.html.Owen Kimball, Chai-Lin Kao, Teodoro Arvizo, JohnMakhoul, and Rukmini Iyer.
2004.
Quick tran-scription and automatic segmentation of the fisherconversational telephone speech corpus.
In RT04Workshop.Lori Lamel, Jean luc Gauvain, and Gilles Adda.2000.
Lightly supervised acoustic model training.In ISCA ITRW ASR2000.Lori Lamel, Jean luc Gauvain, and Gilles Adda.2002.
Lightly supervised and unsupervised acous-tic model training.
Computer Speech and Lan-guage, 16(1).Jeff Ma and Rich Schwartz.
Unsupervised versus su-pervised training of acoustic models.
In INTER-SPEECH.Matthew Marge, Satanjeev Banerjee, and Alexan-der Rudnicky.
2010.
Using the amazon me-chanical turk for transcription of spoken language.ICASSP, March.Ian McGraw, Alexander Gruenstein, and AndrewSutherland.
2009.
A self-labeling speech corpus:Collecting spoken words with an online educa-tional game.
In INTERSPEECH.D.
Miller, M. Kleber, C. Kao, O. Kimball,T.
Colthurst, S.A. Lowe, R.M.
Schwartz, andH.
Gish.
2007.
Rapid and Accurate Spoken TermDetection.
In INTERSPEECH.Charles Passy.
2008.
Turning audio into words onthe screen.
http://online.wsj.com/article/SB122351860225518093.html.R.
Prasad, S. Matsoukas, CL Kao, J.Z.
Ma, DX Xu,T.
Colthurst, O. Kimball, R. Schwartz, J.L.
Gau-vain, L. Lamel, et al 2005.
The 2004 BBN/LIMSI20xRT English conversational telephone speechrecognition system.
In INTERSPEECH.Brandon Roy and Deb Roy.
2009.
Fast transcrip-tion of unstructured audio recordings.
In INTER-SPEECH.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast?but isit good?
: evaluating non-expert annotations fornatural language tasks.
In EMNLP.Steve.
Young, Jost.
Schatzmann, Karl.
Weilhammer,and Hui.
Ye.
2007.
The hidden information stateapproach to dialog management.
In ICASSP.Omar F. Zaidan and Chris Callison-Burch.
2009.Feasibility of human-in-the-loop minimum errorrate training.
In EMNLP.215
