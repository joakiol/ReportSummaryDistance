Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 683?687,Dublin, Ireland, August 23-24, 2014.UBham: Lexical Resources and Dependency Parsing for Aspect-BasedSentiment AnalysisViktor PekarSchool of Computer ScienceUniversity of BirminghamBirmingham, UKv.pekar@cs.bham.ac.ukNaveed AfzalFCIT, North BranchKing Abdulaziz UniversityJeddah, KSAnafzal@kau.edu.saBernd BohnetSchool of Computer ScienceUniversity of BirminghamBirmingham, UKb.bohnet@cs.bham.ac.ukAbstractThis paper describes the system devel-oped by the UBham team for the SemEval-2014 Aspect-Based Sentiment Analysistask (Task 4).
We present an approachbased on deep linguistic processing tech-niques and resources, and explore the pa-rameter space of these techniques appliedto the different stages in this task and ex-amine possibilities to exploit interdepen-dencies between them.1 IntroductionAspect-Based Sentiment Analysis (ASBA) is con-cerned with detection of the author?s sentiment to-wards different issues discussed in a document,such as aspects or features of a product in a cus-tomer review.
The specific ASBA scenario we ad-dress in this paper is as follows.
Given a sentencefrom a review, identify (1) aspect terms, specificwords or multiword expressions denoting aspectsof the product; (2) aspect categories, categories ofissues being commented on; (3) aspect term po-larity, the polarity of the sentiment associated witheach aspect term; and (4) aspect category polarity,the polarity associated with each aspect categoryfound in the sentence.
For example, in:I liked the service and the staff, but not the food.aspect terms are service, staff and food, where thefirst two are evaluated positively and the last onenegatively; and aspect categories are SERVICE andFOOD, where the former is associated with pos-itive sentiment and the latter with negative.
Itshould be noted that a given sentence may containThis work is licenced under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/The research was partially supported by FP7 ICT project?Workbench for Interactive Contrastive Analysis of PatentDocumentation?
under grant no.
FP7-SME-606163.one, several, or no aspect terms, one, several, or noaspect categories, and may express either positive,negative, neutral, or conflicted sentiment.While the ASBA task is usually studied in thecontext of documents (e.g., online reviews), pecu-liarities of this scenario are short input texts, com-plex categorization schemas, and a limited amountof annotated data.
Therefore we focused on waysto exploit deep linguistic processing techniques,which we use for both creating complex classifi-cation features and rule-based processing.2 Related Work2.1 Aspect Term ExtractionTo recognize terms that express key notions in aproduct or service review, a common general ap-proach has been to extract nouns and noun phrasesas potential terms and then apply a certain filteringtechnique to ensure only the most relevant termsremain.
These techniques include statistical asso-ciation tests (Yi et al., 2003), associative miningrules with additional rule-based post-processingsteps (Hu and Liu, 2004), and measures of asso-ciation with certain pre-defined classes of words,such as part-whole relation indicators (Popescuand Etzioni, 2005).2.2 Aspect Category RecognitionAspect category recognition is often addressed asa text classification problem, where a classifieris learned from reviews manually tagged for as-pects (e.g., Snyder and Barzilay, 2007, Ganu et al.,2009).
Titov and McDonald (2008) present an ap-proach which jointly detects aspect categories andtheir sentiment using a classifier trained on top-ics discovered via Multi-Grain LDA and star rat-ings available in training data.
Zhai et al.
(2010)presented an approach based on Expectation-Maximization to group aspect expressions intouser-defined aspect categories.6832.3 Sentence SentimentLexicon-based approaches to detecting sentimentin a sentence rely on a lexicon where words andphrases are provided with sentiment labels as wellas on techniques to recognize ?polarity shifters?,phrases causing the polarity of a lexical itemto reverse.
Early work on detection of polarityshifters used surface-level patterns (Yu and Hatzi-vassilouglu, 2003; Hu and Liu, 2004).
Moila-nen and Pulman (2007) provide a logic-orientedframework to compute the polarity of grammaticalstructures, that is capable of dealing with phenom-ena such as sentiment propagation, polarity rever-sal, and polarity conflict.
Several papers looked atdifferent ways to use syntactic dependency infor-mation in a machine learning framework, to betteraccount for negations and their scope (Nakagawaet al., 2010; Socher et al., 2013).To adapt a generic sentiment lexicon to a newapplication domain, previous work exploited se-mantic relations encoded in WordNet (Kim andHovy, 2006), unannotated data (Li et al, 2012), orqueries to a search engine (Taboada et al., 2006).3 Our ApproachIn the following sections, we will describe our ap-proach to each stage of the Shared Task, reportingexperiments on the provided training data using a10-fold cross-validation.3.1 Aspect Term ExtractionDuring pre-processing training data was parsedusing a dependency parser (Bohnet and Nivre,2012), and sentiment words were recognized in itusing a sentiment lexicon (see Section 6.1).
Can-didate terms were extracted as single nouns, nounphrases, adjectives and verbs, enforcing certainexceptions as detailed in the annotation guidelinesfor the Shared Task (Pontiki et al., 2014), namely:?
Sentiment words were not allowed as part ofterms;?
Noun phrases with all elements capitalizedand acronyms were excluded, under the as-sumption they refer to brands rather thanproduct aspects;?
Nouns referring to the product class as awhole (?restaurant?, ?laptop?, etc) were ex-cluded.Candidate terms that exactly overlapped withmanually annotated terms were discarded, whilethose that did not were used as negative examplesof aspect terms.In order to provide the term extraction processwith additional lexical knowledge, from the train-ing data we extracted those manually annotatedterms that corresponded to a single aspect cate-gory.
Then the set of terms belonging to eachcategory was augmented using WordNet: first wedetermined the 5 most prominent hyperonyms ofthese terms in the WordNet hierarchy using Resnik(1992)?s algorithm for learning a class in a seman-tic hierarchy that best represents selectional pref-erences of a verb, additionally requiring that eachhypernym is at least 7 nodes away from the root, tomake them sufficiently specific.
Then we obtainedall lexical items that belong to children synsets ofthese hypernyms, and further extended these lexi-cal items with their meronyms and morphologicalderivatives.
The resulting set of lexical items waslater used as an extended aspect term lexicon.
Weadditionally created a list of all individual lemmasof content words found in this lexicon.For each term, we extracted the following fea-tures to be used for automatic classification:?
Normalized form: the surface form of theterm after normalization;?
Term lemmas: lemmas of content wordsfound in the term;?
Lexicon term: if the term is in the lexicon;?
Lexicon lemmas ratio: the ratio of lexiconlemmas in the term;?
Unigram: 3 unigrams on either side of theterm;?
Bigrams: The two bigrams around the term;?
Adj+term: If an adjective depends on theterm1or related to it via a link verb (?be?,?get?, ?become?, etc);?
Sentiment+term: If a sentiment word de-pends on the term or related via a link verb;?
Be+term: If the term depends on a link verb;?
Subject term: If the term is a subject;1In case the term was a multi-word expression, the rela-tion to the head of the phrase was used.684?
Object term: If the term is an object.We first look at how well the manually designedpatterns extracted potential terms.
We are primar-ily interested in recall at this stage, since after thatpotential terms are classified into terms and non-terms with an automatic classifier.
The recall onthe restaurants was 70.5, and on the laptops ?56.9.
These are upper limits on recall for the over-all task of aspect term recognition.Table 1 and Table 2 compare the performance ofseveral learning algorithms on the restaurants andthe laptops dataset, respectively2.P R FLinear SVM 94.42 95.51 94.96Decision Tree 94.24 92.90 93.56Na?
?ve Bayes 84.97 95.67 89.99kNN (k=5) 82.71 93.50 87.76Table 1: Learning algorithms on the aspect termextraction task, restaurants dataset.P R FLinear SVM 88.14 94.07 91.00Na?
?ve Bayes 93.61 79.46 85.92Decision Tree 83.87 82.99 83.39kNN (k=5) 82.83 83.31 83.03Table 2: Learning algorithms on the aspect termextraction task, laptops dataset.On both datasets, linear SVMs performed best,and so they were used in the subsequent experi-ments on term recognition.
To examine the qual-ity of each feature used for term classification, weran experiments where a classifier was built andtested without that feature, see Tables 3 and 4, forthe restaurants and laptops datasets respectively,where a greater drop in performance compared tothe entire feature set, indicates a more informativefeature.The results show the three most useful featuresare the same in both datasets: the occurrence of thecandidate term in the constructed sentiment lexi-con, the lemmas found in the term, and the nor-malized form of the term account.We ran further experiments manually selectingseveral top-performing features, but none of the2This and the following experiments were run on the traindata supplied by the shared task organizers using 10-foldcross-validation.P R FLexicon term 91.74 95.01 93.33Term lemmas 92.43 95.00 93.69Normalized form 93.45 95.36 94.39Be+term 93.99 95.28 94.63Left bigram 94.21 95.09 94.64All features 94.42 95.51 94.96Table 3: Top 5 most informative features for theterm extraction subtask, restaurants dataset.P R FLexicon term 88.82 88.61 88.69Term lemmas 85.02 95.16 89.79Normalized form 87.79 92.13 89.89Left bigram 87.83 93.62 90.62Term is obj 87.79 94.43 90.97All features 88.14 94.07 91.00Table 4: Top 5 most informative features for theterm extraction subtask, laptops dataset.configurations produced significant improvementson the use of the whole feature set.Table 5 shows the results of evaluation of the as-pect term extraction on the test data of the SharedTask (baseline algorithms were provided by the or-ganizers).
The results correspond to what can beexpected based on the upper limits on recall forthe pattern-based extraction of candidate terms aswell as precision and recall for the classifier.P R FRestaurants 77.9 61.1 68.5Restaurants, baseline 53.9 51.4 52.6Laptops 60.3 39.1 47.5Laptops, baseline 40.1 38.1 39.1Table 5: Aspect term extraction on the test data ofthe Shared Task.3.2 Aspect Category RecognitionTo recognize aspect categories in a sentence, weclassified individual clauses found in it, assumingthat each aspect category would be discussed ina separate clause.
Features used for classificationwere lemmas of content words; to account for thefact that aspect terms are more indicative of aspectcategories than other words, we additionally usedentire terms as features, weighting them twice asmuch as other features.
Table 6 compares the per-685formance of several learning algorithms when au-tomatically recognized aspect terms were not usedas an additional feature; Table 7 shows resultswhen terms were used as features.P R FLinear SVM 66.37 58.07 60.69Decision Tree 58.07 51.22 53.05Na?
?ve Bayes 74.34 46.07 48.63kNN (k=5) 58.65 43.77 46.57Table 6: Learning algorithms on the aspect cate-gory recognition task, aspect terms not weighted.P R FLinear SVM 67.23 59.43 61.90Decision Tree 64.41 55.84 58.36Na?
?ve Bayes 78.02 49.57 52.87kNN (k=5) 67.92 47.91 51.94Table 7: Learning algorithms on the aspect cate-gory recognition task, aspect terms weighted.The addition of aspect terms as separate featuresincreased F-scores for all the learning methods,sometimes by as much as 5%.
Based on these re-sults, we used the linear SVM method for the tasksubmission.
Table 8 reports results achieved onthe test data of the Shared Task.P R FRestaurants 81.8 67.9 74.2Baseline 64.8 52.5 58.0Table 8: Aspect category extraction on the testdata of the Shared Task.3.3 Aspect Term SentimentTo recognize sentiment in a sentence, we take alexicon-based approach.
The sentiment lexiconwe used encodes the lemma, the part-of-speechtag, and the polarity of the sentiment word.
It wasbuilt by combining three resources: lemmas fromSentiWordNet (Baccianella et al., 2010), which donot belong to more than 3 synsets; the GeneralInquirer lexicon (Stone et al., 1966), and a sub-section of the Roget thesaurus annotated for sen-timent (Heng, 2004).
In addition, we added sen-timent expressions that are characteristic of therestaurants and laptop domains, obtained based onmanual analysis of the restaurants corpus used in(Snyder and Barzilay (2007) and the laptop re-views corpus used in (Jindal and Liu, 2008).To detect negated sentiment, we used a list ofnegating phrases such as ?not?, ?never?, etc., andtwo types of patterns to determine the scope of anegation.
The first type detected negations on thesentence level, checking for negative phrases atthe start of the sentence; negations detected on thesentence level were propagated to the clause level.The second type of patterns detected negated sen-timent within a clause, using patterns specific tothe part-of-speech of the sentiment word (e.g.,?AUXV + negation + VB + MAINV?, whereMAINV is a sentiment verb).
The output of thisalgorithm is the sentence split into clauses, witheach clause being assigned one of four sentimentlabels: ?positive?, ?negative?, ?neutral?, ?con-flict?.
Thus, each term was associated with thesentiment of the clause it appeared in.On the test data of the Shared Task, the algo-rithm achieved the accuracy scores of 76.0 (therestaurants data, for the baseline of 64.3) and 63.6(the laptops data, for the baseline of 51.1).3.4 Category SentimentRecall that aspect categories were recognized in asentence by classifying its individual clauses.
Cat-egory sentiment was determined from the senti-ment of the clauses where the category was found.In case more than one clause was assigned to thesame category and at least one clause expressedpositive sentiment and at least one ?
negative,such cases were classified as conflicted sentiment.This method achieved the accuracy of 72.8 (on therestaurants data), with the baseline being 65.65.4 ConclusionOur study has shown that aspect terms can be de-tected with a high accuracy using a domain lexiconderived from WordNet, and a set of classificationfeatures created with the help of deep linguisticprocessing techniques.
However, the overall accu-racy of aspect term recognition is greatly affectedby the extraction patterns that are used to extractinitial candidate terms.
We also found that au-tomatically extracted aspect terms are useful fea-tures in the aspect category recognition task.
Withregards to sentiment detection, our results suggestthat reasonable performance can be achieved witha lexicon-based approach coupled with carefullydesigned rules for the detection of polarity shifts.686ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
SENTIWORDNET 3.0: An EnhancedLexical Resource for Sentiment Analysis and Opin-ion Mining.
Proceedings of LREC-2010.Bernd Bohnet and Joakim Nivre.
2012.
A Transition-Based System for Joint Part-of-Speech Tagging andLabeled Non-Projective Dependency Parsing.
Pro-ceedings of EMNLP-CoNLL.Gayatree Ganu, No?emie Elhadad, and Ameli?e Mar-ian.
2009.
Beyond the Stars: Improving RatingPredictions using Review Text Content.
Proceedingsof Twelfth International Workshop on the Web andDatabases (WebDB 2009).Adrian Heng.
2004.
An exploratory study into the useof faceted classification for emotional words.
Mas-ter Thesis.
Nanyang Technological University, Sin-gapore.Minqing Hu and Bing Liu.
2004.
Mining opinionfeatures in customer reviews.
Proceedings of the9th National Conference on Artificial Intelligence(AAAI-2004).Nitin Jindal and Bing Liu.
2008.
Opinion Spam andAnalysis Proceedings of WWW-2008.Soo-Min Kim and Eduard Hovy.
2006.
Identifyingand analyzing judgment opinions.
Proceedings ofHLT/NAACL-2006.Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang andXiaoyan Zhu.
2012.
Cross-Domain Co-Extractionof Sentiment and Topic Lexicons.
Proceedings ofACL-2012.Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.2010.
Dependency tree-based sentiment classifica-tion using CRFs with hidden variables.
Proceedingsof NAACL/HLT-2010.Karo Moilanen and Stephen Pulman.
2007.
Sentimentcomposition.
Proceedings of the Recent Advancesin Natural Language Processing (RANLP 2007).Maria Pontiki, Dimitrios Galanis, John Pavlopou-los, Haris Papageorgiou, Ion Androutsopoulos, andSuresh Manandhar.
2014.
SemEval-2014 Task 4:Aspect Based Sentiment Analysis.
Proceedings ofthe 8th International Workshop on Semantic Evalu-ation (SemEval 2014).Ana-Maria Popescu and Oren Etzioni.
2005.
Extract-ing product features and opinions from reviews.
Pro-ceedings HLT/EMNLP-2005.Philip Resnik.
1992.
A class-based approach to lexi-cal discovery Proceedings of the Proceedings of the30th Annual Meeting of the Association for Compu-tational Linguists.Benjamin Snyder and Regina Barzilay 2007.
Multi-ple Aspect Ranking using the Good Grief Algorithm.Proceedings of NAACL-2007.Richard Socher, Alex Perelygin, Jean Y. Wu, JasonChuang, Christopher D. Manning, Andrew Y. Ngand Christopher Potts 2013.
Recursive Deep Mod-els for Semantic Compositionality Over a SentimentTreebank.
Proceedings of EMNLP-2013.Philip J.
Stone, Dexter C. Dunphy, Marshall S. Smith,and Daniel M. Ogilvie.
1966.
The General In-quirer: A Computer Approach to Content Analysis.Cambridge, MA: The MIT Press.Maite Taboada, Caroline Anthony, and Kimberly Voll.2006.
Creating semantic orientation dictionariesProceedings of 5th International Conference on Lan-guage Resources and Evaluation (LREC).Ivan Titov and Ryan McDonald.
2008.
A joint modelof text and aspect ratings for sentiment summariza-tion.
Proceedings of ACL-2008.Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen and JunZhao.
2013.
Mining Opinion Words and OpinionTargets in a Two-Stage Framework.
Proceedings ofACL-2013.Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, andWayne Niblack.
2003.
Sentiment analyzer: Ex-tracting sentiments about a given topic using naturallanguage processing techniques.
Proceedings of the3rd IEEE International Conference on Data Mining(ICDM-2003), pp.
423-434.Hong Yu and Vasileios Hatzivassiloglou.
2003.
To-wards Answering Opinion Questions: SeparatingFacts from Opinions and Identifying the Polarity ofOpinion Sentences.
Proceedings of EMNLP-03.Zhongwu Zhai, Bing Liu, Hua Xu and Peifa Jia.
2011.Clustering product features for opinion mining.
Pro-ceedings of the 4th ACM International Conferenceon Web Search and Data Mining, ACM, pp 347354.687
