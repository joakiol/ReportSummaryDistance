The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 116?121,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsJudging Grammaticality with Count-Induced Tree Substitution GrammarsFrancis Ferraro, Matt Post and Benjamin Van DurmeDepartment of Computer Science, and HLTCOEJohns Hopkins University{ferraro,post,vandurme}@cs.jhu.eduAbstractPrior work has shown the utility of syntactictree fragments as features in judging the gram-maticality of text.
To date such fragments havebeen extracted from derivations of Bayesian-induced Tree Substitution Grammars (TSGs).Evaluating on discriminative coarse and finegrammaticality classification tasks, we showthat a simple, deterministic, count-based ap-proach to fragment identification performs onpar with the more complicated grammars ofPost (2011).
This represents a significant re-duction in complexity for those interested inthe use of such fragments in the developmentof systems for the educational domain.1 IntroductionAutomatically judging grammaticality is an im-portant component in computer-assisted education,with potential applications including large-scale es-say grading and helping to interactively improve thewriting of both native and L2 speakers.
While n-gram models have been productive throughout nat-ural language processing (NLP), they are obviouslyinsufficient as models of languages, since they donot model language structure or correspondencesbeyond the narrow Markov context.Context-free grammars (CFGs) address many ofthe problems inherent in n-grams, and are there-fore intuitively much better suited for grammatical-ity judgments.
Unfortunately, CFGs used in practiceare permissive (Och et al, 2004) and make unreal-istic independence and structural assumptions, re-sulting in ?leaky?
grammars that overgenerate andthus serve poorly as models of language.
How-ever, approaches that make use of the CFG produc-tions as discriminative features have performed bet-ter.
Cherry and Quirk (2008) improved upon an n-gram baseline in grammatical classification by ad-justing CFG production weights with a latent SVM,while others have found it useful to use comparisonsbetween scores of different parsers (Wagner et al,2009) or the use of CFG productions in linear clas-sification settings (Wong and Dras, 2010) in classi-fying sentences in different grammaticality settings.Another successful approach in grammaticalitytasks has been the use of grammars with an extendeddomain of locality.
Post (2011) demonstrated thatlarger syntactic patterns obtained from Tree Sub-stitution Grammars (Joshi, 1985) outperformed theCherry and Quirk models.
The intuitions underlyingtheir approach were that larger fragments are morenatural atomic units in modeling grammatical text,and that larger fragments reduce the independenceassumptions of context-free generative models sincethere are fewer substitution points in a derivation.Their grammars were learned in a Bayesian settingwith Dirichlet Process priors, which have simple for-mal specifications (c.f., Goldwater et al (2009, Ap-pendix A)), but which can become quite complicatedin implementation.In this paper, we observe that fragments used forclassification do not require an underlying proba-bilistic model.
Here, we present a simple extractionmethod that elicits a classic formal non-probabilisticgrammar from training data by deterministicallycounting fragments.
Whereas Post parses with hisTSG and extracts the Viterbi derivation, we use an116SBARINforSNP VPTOtoVP(a) A TSG fragment.SBAR?
IN SIN?
forS?
NP VPVP?
TO VPTO?
to(b) Equivalent CFG rules.Figure 1: Equivalent TSG fragment and CFG rules.off-the-shelf parser and pattern match the fragmentsin our grammar against the tree.
With enough pos-itive and negative training data (in the form of au-tomatic parses of good and bad sentences), we canconstruct classifiers that learn which fragments cor-relate with grammaticality.
The resulting model re-sults in similar classification accuracy while doingaway with the complexity of Bayesian techniques.2 Tree Substitution Grammars (TSGs)Though CFGs and TSGs are weakly equivalent,TSGs permit nonterminals to rewrite as tree frag-ments of arbitrary size, whereas CFG rewrites arelimited to depth-one productions.
Figure 1 de-picts an example TSG fragment and equivalent CFGrules; note that the entire internal structure of 1a isdescribed within a single rewrite.Unfortunately, learning probabilistic TSGs is notstraight-forward, in large part because TSG-specificresources (e.g., large scale TSG-annotated tree-banks) do not exist.
Approaches to this problem be-gan by taking all fragments Fall in a treebank (Bod,1993; Goodman, 1996), which resulted in very largegrammars composed mostly of fragments very un-likely to generalize.1 A range of heuristic solutionsreduced these grammar sizes to a much smaller,more compact subset of all fragments (Zollmannand Sima?an, 2005; Zuidema, 2007).
More recently,more principled models have been proposed, takingthe form of inference in Bayesian non-parametricmodels (Post and Gildea, 2009; Cohn et al, 2009).In addition to providing a formal model for TSGs,these techniques address the overfitting problem of1The n-gram analog would be something like storing all 30-grams seen in a corpus.all fragments grammars with priors that discouragelarge fragments unless there is enough evidence towarrant their inclusion in the grammar.
The problemwith such approaches, however, is that the samplingprocedures used to infer them can be complex, dif-ficult to code, and slow to converge.
Although moregeneral techniques have been proposed to better ex-plore the search space (Cohn and Blunsom, 2010;Cohn et al, 2010; Liang et al, 2010), the complex-ity and non-determinism of these samplers remain,and there are no publicly available implementations.The underlying premise behind these grammarlearning approaches was the need for a probabilis-tic grammar for parsing.
Post (2011) showed thatthe fragments extracted from derivations obtainedby parsing with probabilistic TSGs were useful asfeatures in two coarse-grained grammaticality tasks.In such a setting, fragments are needed for classifica-tion, but it is not clear that they need to be obtainedfrom derivations produced by parsing with proba-bilistic TSGs.
In the next section, we describe a sim-ple, deterministic, count-based approach to learn-ing an unweighted TSG.
We will then demonstrate(?4) the effectiveness of these grammars for gram-maticality classification when fragments are pattern-matched against parse trees obtained from a state-of-the-art parser.3 Counting Common SubtreesRather than derive probabilistic TSGs, we employa simple, iterative and deterministic (up to tie-breaking) alternative to TSG extraction.
Our methodextracts F?R,K?, the K most common subtrees ofsize at most R. Though selecting the top K-most-frequent fragments from all fragments is computa-tionally challenging through brute force methods,note that if F ?
F?R,K?, then all subtrees F?
of Fmust also be in F?R,K?.2 Thus, we may incremen-tally build F?R,K?
in the following manner: given r,for 1 ?
r ?
R, maintain a ranking S, by frequency,of all fragments of size r; the key point is that S maybe built from F?r?1,K?.
Once all fragments of sizer have been considered, retain only the top K frag-ments of the ranked set F?r,K?
= F?r?1,K?
?
S.32Analogously, if an n-gram appears K times, then all con-stituent m-grams, m < n, must also appear at least K times.3We found that, at the thresholding stage, ties may be arbi-trarily broken with neglible-to-no effect on results.117Algorithm 1 EXTRACTFRAGMENTS (R,K)Assume: Access to a treebank1: S ?
?2: F?1,K?
?
top K CFG rules used3: for r = 2 to R do4: S ?
S ?
{observed 1-rule extensions of F ?F?r?1,K?
}5: F?r,K?
?
top K elements of F?r?1,K?
?
S6: end forPseudo-code is provided in Algorithm 1.4This incremental approach is appealing for tworeasons.
Firstly, our approach tempers the growthof intermediate rankings F?r,K?.
Secondly, wehave two tunable parameters R and K, which canbe thought of as weakly being related to the basemeasure and concentration parameter of (Post andGildea, 2009; Cohn et al, 2010).
Note that bythresholding at every iteration, we enforce sparsity.4 ExperimentsWe view grammaticality judgment as a binary clas-sification task: is a sequence of words grammaticalor not?
We evaluate on two tasks of differing granu-larity: the first, a coarse-grain classification, followsCherry and Quirk (2008); the other, a fine-grain ana-logue, is built upon Foster and Andersen (2009).4.1 DatasetsFor the coarse-grained task, we use the BLLIP5-inspired dataset, as in Post (2011), which dis-criminates between BLLIP sentences and Kneyser-Ney trigram generated sentences (of equal length).Grammatical and ungrammatical examples are givenin 1 and 2 below, respectively:(1) The most troublesome report may be theAugust merchandise trade deficit due outtomorrow .
(2) To and , would come Hughey Co. may becrash victims , three billion .For the fine-grained task we use a version of theBNC that has been automatically modified to be4Code is available at: cs.jhu.edu/?ferraro.5LDC2000T43ungrammatical, via insertions, deletions or substi-tutions of grammatically important words.
As hasbeen argued in previous work, these automaticallygenerated errors, simulate more realistic errors (Fos-ter and Andersen, 2009).
Example 3 gives an origi-nal sentence, with an italicized substitution error:(3) The league ?s promoters hope retirees andtourists will join die-hard fans like Mr. deCastro and pack then stands to see the seniors .Both sets contain train/dev/test splits with anequal number of positive and negative examples, andall instances have an available gold-standard parse6.4.2 Models and FeaturesAlgorithm 1 extracts common constructions, in theform of count-extracted fragments.
To test the ef-ficacy of these fragments, we construct and experi-ment with various discriminative models.Given count-extracted fragments obtained fromEXTRACTFRAGMENTS(R,K), it is easy to define afeature vector: for each query, there is a binary fea-ture indicating whether a particular extracted frag-ment occurs in its gold-standard parse.
These count-extracted features, along with the sentence length,define the first model, called COUNT.Although our extracted fragments may helpidentify grammatical constructions, capturing un-grammatical constructions may be difficult, sincewe do not parse with our fragments.
Thus,we created two augmented models, COUNT+LEXand COUNT+CFG, which built upon and extendedCOUNT.
COUNT+LEX included all preterminal andlexical items.
For COUNT+CFG, we included a bi-nary feature for every rule that was used in the mostlikely parse of a query sentence, according to aPCFG7.Following Post (2011), we train an `-2 regular-ized SVM using liblinear8 (Fan et al, 2008)per model.
We optimized the models on dev data,letting the smoothing parameter be 10m, for integralm ?
[?4, 2]: 0.1 was optimal for all models.6We parsed all sentences with the Berkeley parser (Petrov etal., 2006).7We used the Berkeley grammar/parser (Petrov et al, 2006)in accurate mode; all other options were their default values.8csie.ntu.edu.tw/?cjlin/liblinear/118Task COUNT COUNT+LEX COUNT+CFGcoarse 86.3 86.8 88.3fine 62.9 64.3 67.0(a) Our count-based models, with R = 15, K = 50k.Task 3 5 10 15coarse 89.2 89.1 88.6 88.3fine 67.9 67.2 67.2 67.0(b) Performance of COUNT+CFG, with K =50k and varying R.Table 1: Development accuracy results.Our three models all have the same two tunableparameters, R and K. While we initially experi-mented with R = 31,K ?
{50k, 100k} ?
in or-der to be comparable to the size of Post (2011)?s ex-tracted TSGs ?
we noticed that very few, if any,fragments of size greater than 15 are able to sur-vive thresholding.
Dev experimentation revealedthat K = 50k and 100k yielded nearly the sameresults; for brevity, we report in Table 1a dev re-sults for all three models, with R = 15,K =50k.
The differences across models was stark, withCOUNT+CFG yielding a two point improvement overCOUNT on coarse, but a four point improvementon fine.
While COUNT+LEX does improve uponCOUNT, on both tasks it falls short of COUNT+CFG.These differences are not completely surprising:one possible explanation is that the PCFG featuresin COUNT+CFG yield useful negatively-biased fea-tures, by providing a generative explanation.
Dueto the supremacy of COUNT+CFG, we solely reportresults on COUNT+CFG.In Table 1b, we also examine the effect of ex-tracted rule depth on dev classification accuracy,where we fix K = 50k and vary R ?
{3, 5, 10, 15},where the best results are achieved with R = 3.We evaluate two versions of COUNT+CFG: one withR = 3 and the other with R = 15 (K = 50k forboth).5 Results and Fragment AnalysisWe build on Post (2011)?s results and compareagainst bigram, CFG and TSG baselines.
Each base-line model is built from the same `-2 regularizedMethod coarse fineCOUNT+CFG, R = 3 89.1 67.2COUNT+CFG, R = 15 88.2 66.6bigram 68.4 61.4CFG 86.3 64.5TSG 89.1 67.0Table 2: Classification accuracy on test portions forboth coarse and fine, with K = 50k.
Chance is 50%for each task.SVM as above, and each is optimized on dev data.For the bigram baseline, the binary features corre-spond with whether a particular bigram appears inan instance, while the CFG baseline is simply theaugmentation feature set used for COUNT+CFG.
Forthe TSG baseline, the binary features correspondwith whether a particular fragment is used in themost probable derivation of each input sentence (us-ing Post?s Bayesian TSGs).
All baselines use thesentence length as a feature as well.The results on the test portions of each dataset aregiven in Table 2.
When coupled with the best parseoutput, our counting method was able to perform onpar with, and even surpass, Post?s TSGs.
The sim-pler model (R = 3) ties TSG performance on coarseand exceeds it by two-tenths on fine; the more com-plex model (R = 15) gets within a point on coarseand four-tenths on fine.
Note that both versions ofCOUNT+CFG surpass the CFG baseline on both sets,indicating that (1) encoding deeper structure, evenwithout an underlying probabilistic model, is use-ful for grammaticality classifications, and (2) thisdeeper structure can be achieved by a simple count-ing scheme.As PCFG output comprises a portion of our fea-ture set, it is not surprising that a number of themost discriminative positive and negative features,such as flat NP and VP rules not frequent enoughto survive thresholding, were provided by the CFGparse.
While this points out a limitation of ournon-adaptive thresholding, note that even amongthe highest weighted features, PCFG and count-extracted features were interspersed.
Further, con-sidering that both versions of COUNT+CFG outper-formed CFGs, it seems our method adds discrimina-tive power to the CFG rules.119(a) Coarse (b) FineGrammatical Ungrammatical Grammatical Ungrammatical1 (S NP VP (.
.))
(S NP (VP (VBP are)PP))10 (SBAR (IN if) S) (SBAR (S VP))2 (S (S (VP VBG NP))VP)(VP VBZ (S VP)) 11 (NP (DT these) NNS) (SBAR DT (S NPVP))3 (SBAR (IN while) S) (SBAR (S VP) ) 12 (VP (VBG being) VP) (S (VP VB NP))4 (VP (VBD called) S) (VP VBN (S VP)) 13 (PP IN (S NP (VPVBG NP)))(S (VP VBZ NP))5 (VP (VB give) NP NP) (NP (NP JJ NN)SBAR)14 (S (VP VBG VP)) (VP VB (S VP))6 (NP NNP NNP NNP(NNP Inc.))(VP NN (PP IN NP)) 15 (PP IN (SBAR (INwhether) S))(S (VP VBP VP))7 (PP (IN with) (S NPVP))(S (VP MD VP)) 16 (VP (VBD had) (VPVBN S))(S NP (VP (VBDsaid)))8 (SBAR (IN for) (S NP(VP (TO to) VP)))(SBAR (S (NP NNS)VP))17 (VP MD (VP VB NP(PP IN NP) PP))*(PP (PP IN NP) (CCand) PP)*9 (PRN (-LRB- -LRB-)NP (-RRB- -RRB-))*(S (ADJP JJ))* 18 (NP (DT no) NNS)* (PP (IN As) NP)*Table 3: Most discriminative count-based features for COUNT+CFG on both tasks.
For comparability to Post(2011), R = 15,K = 50k, are shown.
Asterisks (*) denote fragments hand-selected from the top 30.Table 5 presents top weighted fragments fromCOUNT+CFG on both coarse and fine, respectively.Examining useful grammatical features across tasks,we see a variety of fragments: though our fragmentsheavily weight simple structure such as proper punc-tuation (ex.
1) and parentheticals (ex.
9), they alsocapture more complex phenomena such as lexicalargument descriptions (e.g., give, ex.
5).
Our ex-tracted fragments also describe common construc-tions and transitions (e.g., 3, 8 and 15) and involvedverb phrases (e.g., gerunds in 2 and 14, passives in16, and modals in 17).Though for both tasks some ungrammatical frag-ments easily indicate errors, such as sentence frag-ments (e.g., example 6) or repeated words (ex.
11),in general the analysis is more difficult.
In part, thisis because, when isolated from errors, one may con-struct grammatical sentences that use some of thehighest-weighted ungrammatical fragments.
How-ever, certain errors may force particular rules to beinappropriately applied when acquiring the gold-standard parse.
For instance, example 10 typicallycoordinates with larger VPs, via auxiliary verbs orexpletives (e.g., it).
Affecting those crucial wordscan significantly change the overall parse structure:consider that in ?said it is too early.
.
.
,?
it provides acrucial sentential link; without it, ?is too early?
maybe parsed as a sentence, and then glued on to theformer part.6 ConclusionIn this work, we further examined TSGs as usefuljudges of grammaticality for written English.
Us-ing an iterative, count-based approach, along withthe most likely PCFG parse, we were able to train adiscriminative classifier model ?
COUNT+CFG ?that surpassed the PCFG?s ability to judge gram-maticality, and performed on par with Bayesian-TSGs.
Examining the highest weighted features, wesaw that complex structures and patterns encoded bythe count-based TSGs proved discriminatively use-ful.
This suggests new, simpler avenues for frag-ment learning, especially for grammaticality judg-ments and other downstream tasks.Acknowledgements Thank you to the reviewersfor helpful feedback, and thanks to Johns HopkinsHLTCOE for providing support.
Any opinions ex-pressed in this work are those of the authors.120ReferencesR.
Bod.
1993.
Using an annotated corpus as a stochas-tic grammar.
In Proceedings of the sixth conferenceon European chapter of the Association for Computa-tional Linguistics, pages 37?44.
Association for Com-putational Linguistics.Colin Cherry and Chris Quirk.
2008.
Discrimina-tive, syntactic language modeling through latent svms.Proceeding of Association for Machine Translation inthe America (AMTA-2008).Trevor Cohn and Phil Blunsom.
2010.
Blocked inferencein bayesian tree substitution grammars.
In Proceed-ings of ACL (short papers), pages 225?230, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Trevor Cohn, Sharon Goldwater, and Phil Blunsom.2009.
Inducing compact but accurate tree-substitutiongrammars.
In Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 548?556, Stroudsburg, PA,USA.
Association for Computational Linguistics.Trevor Cohn, Phil Blunsom, and Sharon Goldwater.2010.
Inducing tree-substitution grammars.
Journalof Machine Learning Research, 11:3053?3096, De-cember.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874, June.Jennifer Foster and Oistein E. Andersen.
2009.
Gen-ERRate: generating errors for use in grammatical errordetection.
In Proceedings of the Fourth Workshop onInnovative Use of NLP for Building Educational Ap-plications, pages 82?90.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2009.
A Bayesian framework for word segmen-tation: Exploring the effects of context.
Cognition,112(1):21 ?
54.Joshua Goodman.
1996.
Efficient algorithms for parsingthe dop model.
In Proceedings of EMNLP, pages 143?152.A.K.
Joshi.
1985.
Tree adjoining grammars: How muchcontext-sensitivity is required to provide reasonablestructural descriptions?
Natural language parsing,pages 206?250.Percy Liang, Michael .I.
Jordan, and Dan Klein.
2010.Type-based MCMC.
In Human Language Technolo-gies: The 2010 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics.
Association for Computational Linguistics.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng, et al2004.
A smorgasbord of features for statistical ma-chine translation.
In Proceedings of NAACL.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and in-terpretable tree annotation.
In Proceedings of ACL-ICCL, pages 433?440, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Matt Post and Daniel Gildea.
2009.
Bayesian learn-ing of a tree substitution grammar.
In Proceedingsof ACL-IJCNLP (short papers), pages 45?48, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Matt Post.
2011.
Judging grammaticality with treesubstitution grammar derivations.
In Proceedings ofACL (short papers), pages 217?222, Stroudsburg, PA,USA.
Association for Computational Linguistics.J.
Wagner, J.
Foster, and J. van Genabith.
2009.
Judg-ing grammaticality: Experiments in sentence classifi-cation.
CALICO Journal, 26(3):474?490.Sze-Meng Jojo Wong and Mark Dras.
2010.
Parserfeatures for sentence grammaticality classification.
InProceedings of the Australasian Language TechnologyAssociation Workshop.Andreas Zollmann and Khalil Sima?an.
2005.
A consis-tent and efficient estimator for Data-Oriented Parsing.Journal of Automata, Languages and Combinatorics,10(2/3):367?388.Willem Zuidema.
2007.
Parsimonious data-orientedparsing.
In Proceedings of EMNLP-CoNLL, pages551?560.121
