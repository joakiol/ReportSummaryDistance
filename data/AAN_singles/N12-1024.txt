2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 232?242,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsImplicitly Intersecting Weighted Automata using Dual Decomposition?Michael J. Paul and Jason EisnerDepartment of Computer Science / Johns Hopkins UniversityBaltimore, MD 21218, USA{mpaul,jason}@cs.jhu.eduAbstractWe propose an algorithm to find the best paththrough an intersection of arbitrarily manyweighted automata, without actually perform-ing the intersection.
The algorithm is based ondual decomposition: the automata attempt toagree on a string by communicating about fea-tures of the string.
We demonstrate the algo-rithm on the Steiner consensus string problem,both on synthetic data and on consensus de-coding for speech recognition.
This involvesimplicitly intersecting up to 100 automata.1 IntroductionMany tasks in natural language processing in-volve functions that assign scores?such as log-probabilities?to candidate strings or sequences.Often such a function can be represented compactlyas a weighted finite state automaton (WFSA).
Find-ing the best-scoring string according to a WFSA isstraightforward using standard best-path algorithms.It is common to construct a scoring WFSA bycombining two or more simpler WFSAs, taking ad-vantage of the closure properties of WFSAs.
For ex-ample, consider noisy channel approaches to speechrecognition (Pereira and Riley, 1997) or machinetranslation (Knight and Al-Onaizan, 1998).
Givenan input f , the score of a possible English tran-scription or translation e is the sum of its languagemodel score log p(e) and its channel model scorelog p(f | e).
If each of these functions of e is repre-sented as a WFSA, then their sum is represented asthe intersection of those two WFSAs.WFSA intersection corresponds to constraint con-junction, and hence is often a mathematically natu-ral way to specify a solution to a problem involving?The authors are grateful to Damianos Karakos for provid-ing tools and data for the ASR experiments.
This work wassupported in part by an NSF Graduate Research Fellowship.multiple soft constraints on a desired string.
Unfor-tunately, the intersection may be computationally in-efficient in practice.
The intersection of K WFSAshaving n1, n2, .
.
.
, nK states may have n1?n2 ?
?
?nKstates in the worst case.1In this paper, we propose a more efficient methodfor finding the best path in an intersection withoutactually computing the full intersection.
Our ap-proach is based on dual decomposition, a combina-torial optimization technique that was recently intro-duced to the vision (Komodakis et al, 2007) and lan-guage processing communities (Rush et al, 2010;Koo et al, 2010).
Our idea is to interrogate theseveral WFSAs separately, repeatedly visiting eachWFSA to seek a high-scoring path in each WFSAthat agrees with the current paths found in the otherWSFAs.
This iterative negotiation is reminiscent ofmessage-passing algorithms (Sontag et al, 2008),while the queries to the WFSAs are reminiscent ofloss-augmented inference (Taskar et al, 2005).We remark that a general solution whose asymp-totic worst-case runtime beat that of naive intersec-tion would have important implications for com-plexity theory (Karakostas et al, 2003).
Our ap-proach is not such a solution.
We have no worst-casebounds on how long dual decomposition will take toconverge in our setting, and indeed it can fail to con-verge altogether.2 However, when it does converge,we have a ?certificate?
that the solution is optimal.Dual decomposition is usually regarded as amethod for finding an optimal vector in Rd, sub-ject to several constraints.
However, it is not ob-vious how best to represent strings as vectors?they1Most regular expression operators combine WFSA sizesadditively.
It is primarily intersection and its close relative,composition, that do so multiplicatively, leading to inefficiencywhen two large WFSAs are combined, and to exponentialblowup when many WFSAs are combined.
Yet these operationsare crucially important in practice.2An example that oscillates can be constructed along linessimilar to the one given by Rush et al (2010).232have unbounded length, and furthermore the abso-lute position of a symbol is not usually significant inevaluating its contribution to the score.3 One con-tribution of this work is that we propose a general,flexible scheme for converting strings to feature vec-tors on which the WFSAs must agree.
In principlethe number of features may be infinite, but the setof ?active?
features is expanded only as needed un-til the algorithm converges.
Our experiments use aparticular instantiation of our general scheme, basedon n-gram features.We apply our method to a particular task: findingthe Steiner consensus string (Gusfield, 1997) thathas low total edit distance to a number of given, un-aligned strings.
As an illustration, we are pleased toreport that ?alia?
and ?aian?
are the consensuspopular names for girls and boys born in the U.S. in2010.
We use this technique for consensus decodingfrom speech recognition lattices, and to reconstructthe common source of up to 100 strings corrupted byrandom noise.
Explicit intersection would be astro-nomically expensive in these cases.
We demonstratethat our approach tends to converge rather quickly,and that it finds good solutions quickly in any case.2 Preliminaries2.1 Weighted Finite State AutomataA weighted finite state automaton (WFSA) over thefinite alphabet ?
is an FSA that has a cost or weightassociated with each arc.
We consider the case ofreal-valued weights in the tropical semiring.
This isa fancy way of saying that the weight of a path is thesum of its arc weights, and that the weight of a stringis the minimum weight of all its accepting paths (or?
if there are none).When we intersect two WFSAs F and G, the ef-fect is to add string weights: (F ?G)(x) = F (x) +G(x).
Our problem is to find the x that minimizesthis sum, but without constructing F ?
G to run ashortest-path algorithm on it.2.2 Dual DecompositionThe trick in dual decomposition is to decomposean intractable global problem into two or more3Such difficulties are typical when trying to apply structuredprediction or optimization techniques to predict linguistic ob-jects such as strings or trees, rather than vectors.tractable subproblems that can be solved indepen-dently.
If we can somehow combine the solutionsfrom the subproblems into a ?valid?
solution to theglobal problem, then we can avoid optimizing thejoint problem directly.
A valid solution is one inwhich the individual solutions of each subproblemall agree on the variables which are shared in thejoint problem.
For example, if we are combining aparser with a part-of-speech tagger, the tag assign-ments from both models must agree in the final so-lution (Rush et al, 2010); if we are intersecting atranslation model with a language model, then it isthe words that must agree (Rush and Collins, 2011).More formally, suppose we want to find a globalsolution that is jointly optimized among K sub-problems: argminx?Kk=1 fk(x).
Suppose that xranges over vectors.
Introducing an auxiliary vari-able xk for each subproblem fk allows us to equiv-alently formulate this as the following constrainedoptimization problem:argmin{x,x1,...,xK}K?k=1fk(xk) s.t.
(?k)xk = x (1)For any set of vectors ?k that sum to 0,?Kk=1 ?k =0, Komodakis et al (2007) show that the followingLagrangian dual is a lower bound on (1):4min{x1,...,xK}K?k=1fk(xk) + ?k ?
xk (2)where the Lagrange multiplier vectors ?k can beused to penalize solutions that do not satisfy theagreement constraints (?k)xk = x.
Our goal is tomaximize this lower bound and hope that the resultdoes satisfy the constraints.
The graphs in Fig.
2illustrate how we increase the lower bound overtime, using a subgradient algorithm to adjust the ?
?s.At each subgradient step, (2) can be computed bychoosing each xk = argminxk fk(xk) +?k ?xk sep-arately.
In effect, each subproblem makes an inde-pendent prediction xk influenced by ?k, and if theseoutputs do not yet satisfy the agreement constraints,then the ?k are adjusted to encourage the subprob-lems to agree on the next iteration.
See Sontag et al(2011) for a detailed tutorial on dual decomposition.4The objective in (2) can always be made as small as in (1)by choosing the vectors (x1, .
.
.
xK) that minimize (1) (becausethenPk ?k ?
xk =Pk ?k ?
x = 0 ?
x = 0).
Hence (2) ?
(1).2333 WFSAs and Dual DecompositionGiven K WFSAs, F1, .
.
.
, FK , we are interested infinding the string x which has the best score in theintersection F1?
.
.
.
?FK .
The lowest-cost string inthe intersection of all K machines is defined as:argminx?kFk(x) (3)As explained above, the trick in dual decomposi-tion is to recast (3) as independent problems of theform argminxk Fk(xk), subject to constraints thatall xk are the same.
However, it is not so clear howto define agreement constraints on strings.
Perhapsa natural formulation is that Fk should be urged tofavor strings xk that would be read by Fk?
along asimilar path to that of xk?
.
But Fk cannot keep trackof the state of Fk?
for all k?
without solving the fullintersection?precisely what we are trying to avoid.Instead of requiring the strings xk to be equal asin (1), we will require their features to be equal:(?k) ?
(xk) = ?
(x) (4)Of course, we must define the features.
We will usean infinite feature vector ?
(x) that completely char-acterizes x, so that agreement of the feature vectorsimplies agreement of the strings.
At each subgradi-ent step, however, we will only allow finitely manyelements of ?k to become nonzero, so only a finiteportion of ?
(xk) needs to be computed.5We will define these ?active?
features of a stringx by constructing some unweighted deterministicFSA, G (described in ?4).
The active features of xare determined by the collection of arcs on the ac-cepting path of x in G. Thus, to satisfy the agree-ment constraint, xi and xj must be accepted usingthe same arcs ofG (or more generally, arcs that havethe same features).We relax the constraints by introducing a col-lection ?
= ?1, .
.
.
, ?K of Lagrange multipliers,5The simplest scheme would define a binary feature for eachstring in ??.
Then the nonzero elements of ?k would spec-ify punishments and rewards for outputting various strings thathad been encountered at earlier iterations: ?Try subproblem kagain, and try harder not to output michael this time, as it stilldidn?t agree with other subproblems: try jason instead.?
Thisscheme would converge glacially if at all.
We instead focus onfeaturizations that let subproblems negotiate about substrings:?Try again, avoiding mi if possible and favoring ja instead.
?and defining G?k(x) such that the features of G areweighted by the vector ?k (all of whose nonzero el-ements must correspond to features in G).
As in (2),we assume ?
?
?, where ?
= {?
:?k ?k = 0}.This gives the objective:h(?)
= min{x1,...,xK}?k(Fk(xk) +G?k(xk)) (5)This minimization fully decomposes into K sub-problems that can be solved independently.
The kthsubproblem is to find argminxk Fk(xk)+G?k(xk),which is straightforward to solve with finite-statemethods.
It is the string on the lowest-cost paththrough Hk = Fk ?
G?k , as found with standardpath algorithms (Mohri, 2002).The dual problem we wish to solve ismax???
h(?
), where h(?)
itself is a min over{x1, .
.
.
, xK}.
We optimize ?
via projected subgra-dient ascent (Komodakis et al, 2007).
The updateequation for ?k at iteration t is then:?
(t+1)k = ?
(t)k + ?t(?
(x(t)k )??k?
?(x(t)k?
)K)(6)where ?t > 0 is the step size at iteration t. This up-date is intuitive.
It moves away from the current so-lution and toward the average solution (where theydiffer), by increasing the cost of the former?s fea-tures and reducing the cost of the latter?s features.This update may be very dense, however, since?
(x) is an infinite vector.
So we usually only up-date the elements of ?k that correspond to the smallfinite set of active features (the other elements arestill ?frozen?
at 0), denoted ?.
This is still a validsubgradient step.
This strategy is incorrect only ifthe updates for all active features are 0?in otherwords, only if we have achieved equality of the cur-rently active features and yet still the {xk} do notagree.
In that case, we must choose some inactivefeatures that are still unequal and allow the subgra-dient step to update their ?
coefficients to nonzero,making them active.
At the next step of optimiza-tion, we must expand G to consider this enlarged setof active features.4 The Agreement MachineThe agreement machine (or constraint machine) Gcan be thought of as a way of encoding features of234strings on which we enforce agreement.
There are anumber of different topologies for G that might beconsidered, with varying degrees of efficiency andutility.
Constructing G essentially amounts to fea-ture engineering; as such, it is unlikely that thereis a universally optimal topology of G. Neverthe-less, there are clearly bad ways to build G, as notall topologies are guaranteed to lead to an optimalsolution.
In this section, we lay out some abstractguidelines for appropriateG construction, before wedescribe specific topologies in the later subsections.Most importantly, we should design G so that itaccepts all strings in F1?
.
.
.
?FK .
This is to ensurethat it accepts the string that is the optimal solutionto the joint problem.
If G did not accept that string,then neither would Hk = Fk ?G, and our algorithmwould not be able to find it.Even ifHk can accept the optimal string, it is pos-sible that this string would never be the best path inthis machine, regardless of ?.
For example, supposeG is a single-state machine with self-loops accept-ing each symbol in the alphabet (i.e.
a unigram ma-chine).
Suppose Hk outputs the string aaa in thecurrent iteration, but we would like the machines toconverge to aaaaa.
We would lower the weight of?a to encourage Hk to output more of the symbol a.However, if Hk has a cyclic topology, then it couldhappen that a negative value of ?a could create anegative-weight cycle, in which the lowest-cost paththroughHk is infinitely long.
It might be that adjust-ing ?a can change the best string to either aaa oraaaaaaaaa.
.
.
(depending on whether a cycle af-ter the initial aaa has positive or negative weight),but never the optimal aaaaa.
On the other hand,if G instead encoded 5-grams, this would not be aproblem because a path through a 5-gram machinecould accept aaaaa without traversing a cycle.Finally, agreeing on (active) features does notnecessarily mean that all xk are the same string.
Forexample, if we again use a unigram G (that is, ?
=?, the set of unigrams), then ??
(abc) = ??
(cba),where ??
returns a feature vector where all but theactive features are zeroed out.
In this instance, wesatisfy the constraints imposed by G, even thoughwe have not satisfied the constraint we truly careabout: that the strings agree.To summarize, we will aim to choose ?
such thatG has the following characteristics:1.
The language L(Fk ?G) = L(Fk); i.e.
G doesnot restrict the set of strings accepted by Fk.2.
When ??
(xi) = ??
(xj), typically xi = xj .3.
??
?
?
s.t.
argminx Fk(x) +G?k(x) =argminx?k?
Fk?
(x), i.e., the optimal stringcan be the best path in Fk ?
G.6 This may notbe the case if G is cyclic.The first of these is required during every itera-tion of the algorithm in order to maintain optimalityguarantees.
However, even if we do not satisfy thelatter two points, we may get lucky and the stringsthemselves will agree upon convergence, and no fur-ther work is required.
Furthermore, the unigram ma-chine G used in the above examples, despite break-ing these requirements, has the advantage of beingvery efficient to intersect with F .
This motivatesour ?active feature?
strategy of using a simpleG ini-tially, and incrementally altering it as needed, for ex-ample if we satisfy the constraints but the strings donot yet match.
We discuss this in ?4.2.4.1 N-Gram Construction of GIn principle, it is valid to use any G that satisfies theguidelines above, but in practice, some topologieswill lead to faster convergence than others.Perhaps the most obvious form is a simple vectorencoding of strings, e.g.
?a at position 1?, ?b at po-sition 2?, and so on.
As a WFSA, this would simplyhave one state represent each position, with arcs foreach symbol going from position i to i + 1.
This isessentially a unigram machine where the loops havebeen ?unrolled?
to also keep track of position.However, early experiments showed that withthis topology for G, our algorithm converged veryslowly, if at all.
What goes wrong?
The problemstems from the fact that the strings are unaligned andof varying length, and it is difficult to get the stringsto agree quickly at specific positions.
For example,if two subproblems have b at positions 6 and 8 in thecurrent iteration, they might agree at position 7?butour features don?t encourage this.
The Lagrangianupdate would discourage accepting b at 6 and en-courage b at 8 (and vice versa), without giving credit6It is not always possible to construct a G to satisfy thisproperty, as the Lagrangian dual may not be a tight bound to theoriginal problem.235for meeting in the middle.
Further, these features donot encourage the subproblems to preserve the rela-tive order of neighboring symbols, and strings whichare almost the same but slightly misaligned will bepenalized essentially everywhere.
This is an ineffec-tive way for the subproblems to communicate.In this paper, we focus on the feature set wefound to work the best in our experiments: thestrings should agree on their n-gram features, suchas ?number of occurrences of the bigram ab.?
Evenif we don?t yet know precisely where ab should ap-pear in the string, we can still move toward conver-gence if we try to force the subproblems to agree onwhether and how often ab appears at all.To encode n-gram features in a WFSA, each staterepresents the (n?1)-gram history, and all arcs leav-ing the state represent the final symbol in the n-gram, weighted by the score of that n-gram.
Themachine will also contain start and end states, withappropriate transitions to/from the n-gram states.For example, if the trigram abc has weight ?abc,then the trigram machine will encode this as an arcwith the symbol c leaving the state representing ab,and this arc will have weight ?abc.
If our featureset alo contains 1- and 2-grams, then the arc in thisexample would incorporate the weights of all of thecorresponding features: ?abc + ?bc + ?c.A drawback is that these features give no infor-mation about where in the string the n-grams shouldoccur.
In a long string, we might want to encour-age or discourage an n-gram in a certain ?region?
ofthe string.
Our features can only encourage or dis-courage it everywhere in the string, which may leadto slow convergence.
Nevertheless, in our particularexperimental settings, we find that this works betterthan other topologies we have considered.Sparse N-Gram Encoding A full n-gram lan-guage model requires ?
|?|n arcs to encode as aWFSA.
This could be quite expensive.
Fortunately,large n-gram models can be compacted by usingfailure arcs (?-arcs) to encode backoff (Allauzen etal., 2003).
These arcs act as -transitions that canbe taken only when no other transition is available.They allow us to encode the sparse subset of n-grams that have nonzero Lagrangians.
We encodeGsuch that all features whose ?
value is 0 will back offto the next largest n-gram having nonzero weight.This form of G still accepts ??
and has the sameweights as a dense representation, but could requiresubstantially fewer states.4.2 Incrementally Expanding GAs mentioned above, we may need to alter G as wego along.
Intuitively, we may want to start with fea-tures that are cheap to encode, to move the param-eters ?
to a good part of the solution space, thenincrementally bring in more expensive features asneeded.
Shorter n-grams require a smaller G andwill require a shorter runtime per iteration, but ifthey are too short to be informative, then they mayrequire many more iterations to reach convergence.In an extreme case, we may reach a point wherethe subproblems all agree on n-grams currently in?, but the actual strings still do not match.
Wait-ing until we hit such a point may be unnecessarilyslow.
We experimented with periodically increas-ing n (e.g.
adding trigrams to the feature set if wehaven?t converged with bigrams after a fixed num-ber of iterations), but this is expensive, and it is notclear how to define a schedule for increasing the or-der of n. We instead present a simple and effectiveheuristic for bringing in more features.The idea is that if the subproblem solutions cur-rently disagree on counts of the bigrams ab andbc, then an abc feature may be unnecessary, sincethe subproblems could still make progress with onlythese bigram constraints.
However, once the sub-problems agree on these two bigrams, but disagreeon trigram abc, we bring this into the feature set ?.More generally, we add an (n+ 1)-gram to the fea-ture set if the current strings disagree on its countsdespite agreeing on its n-gram prefix and n-gramsuffix (which need not necessarily be ?).
This se-lectively brings in larger n-grams to target portionsof the strings that may require longer context, whilekeeping the agreement machine small.Algorithm 1 gives pseudocode for our completealgorithm when using n-gram features with this in-cremental strategy.
To summarize, we solve for eachxk using the current ?k, and if all the strings agree,we return them as the optimal solution.
Otherwise,we update ?k and repeat.
At each iteration, we checkfor n-gram agreement, and bring in select (n + 1)-grams to the feature set as appropriate.Finally, there is another instance where we might236Algorithm 1 The dual decomposition algorithmwith n-gram features.Initialize ?
to some initial set of n-gram features.for t = 1 to T dofor k = 1 to K doSolve xk = argminx(Fk ?
G?k)(x) with ashortest-path algorithmend forif (?i, j)xi = xj thenreturn {x1, .
.
.
, xK}else?
= ?
?
{z ?
??
: all xk agree on the featurescorresponding to the length-(|z| ?
1) prefix andsuffix of z, but not on z itself}for k = 1 to K doUpdate ?k according to equation (6)Create G?k to encode the features ?end forend ifend forneed to expand G, which we omit from the pseu-docode for conciseness.
If both Fk and G are cyclic,then there is a chance that there will be a negative-weight cycle inFk?G?k .
(If at least one of these ma-chines is acyclic, then this is not a problem, becausetheir intersection yields a finite set.)
In the case ofa negative-weight cycle, the best path is infinitelylong, and so the algorithm will either return an erroror fail to terminate.
If this happens, then we need tobacktrack, and either decrease the subgradient stepsize to avoid moving into this territory, or alter G toexpand the cycles.
This can be done by unrollingloops to keep track of more information?when en-coding n-gram features with G, this amounts to ex-panding G to encode higher order n-grams.
Whenusing a sparse G with ?-arcs, it may also be neces-sary to increase the minimum n-gram history thatis used for back-off.
For example, instead of al-lowing bigrams to back off to unigrams, we mightforce G to encode the full set of bigrams (not justbigrams with nonzero ?)
in order to avoid cyclesin the lower order states.
Our strategy for avoidingnegative-weight cycles is detailed in ?5.1.5 Experiments with Consensus DecodingTo best highlight the utility of our approach, we con-sider applications that must (implicitly) intersect alarge number of WFSAs.
We will demonstrate that,in many cases, our algorithm converges to an exactsolution on problems involving 10, 25, and even 100machines, all of which would be hopeless to solveby taking the full intersection.We focus on the problem of solving for the Steinerconsensus string: given a set of K strings, find thestring in ??
that has minimal total edit distance toall strings in the set.
This is an NP-hard problemthat can be solved as an intersection of K machines,as we will describe in ?5.2.
The consensus stringalso gives an implicit multiple sequence alignment,as we discuss in ?6.We begin with the application of minimum Bayesrisk decoding of speech lattices, which we show canreduce to the consensus string problem.
We then ex-plore the consensus problem in depth by applying itto a variety of different inputs.5.1 Experimental DetailsWe initialize ?
to include both unigrams and bi-grams, as we find that unigrams alone are not pro-ductive features in these experiments.
As we expand?, we allow it to include n-grams up to length five.We run our algorithm for a maximum of 1000 iter-ations, using a subgradient step size of ?/(t + 500)at iteration t, which satisfies the general propertiesto guarantee asymptotic convergence (Spall, 2003).We initialize ?
to 1 and 10 in the two subsections, re-spectively.
We halve ?
whenever we hit a negative-weight cycle and need to backtrack.
If we still getnegative-weight cycles after ?
?
10?4 then we reset?
and increase the minimum order of n which is en-coded in G. (If n is already at our maximum of five,then we simply end without converging.)
In the caseof non-convergence after 1000 iterations, we selectthe best string (according to the objective) from theset of strings that were solutions to any subproblemat any point during optimization.Our implementation uses OpenFST 1.2.8 (Al-lauzen et al, 2007).5.2 Minimum Bayes Risk Decoding for ASRWe first consider the task of automatic speech recog-nition (ASR).
Suppose x?
is the true transcription(a string) of an spoken utterance, and pi(w) is anASR system?s probability distribution over possi-ble transcriptions w. The Bayes risk of an out-put transcription x is defined as the expectation237?w pi(w) `(x,w) for some loss function ` (Bickeland Doksum, 2006).
Minimum Bayes risk decoding(Goel and Byrne, 2003) involves choosing the x thatminimizes the Bayes risk, rather than simply choos-ing the x that maximizes pi(x) as in MAP decoding.As a reasonable approximation, we will take theexpectation over just the strings w1, .
.
.
, wK that aremost probable under pi.
A common loss functionis the Levenshtein distance because this is generallyused to measure the word error rate of ASR output.Thus, we seek a consensus transcriptionargminxK?k=1pik d(x,wk) (7)that minimizes a weighted sum of edit distances toall of the top-K strings, where high edit distanceto more probable strings is more strongly penal-ized.
Here d(x,w) is the unweighted Levenshteindistance between two strings, and pik = pi(wk).
Ifeach pik = 1/K, then argminx is known as theSteiner consensus string, which is NP-hard to find(Sim and Park, 2003).
Equation (7) is a weightedgeneralization of the Steiner problem.Given an input string wk, it is straightforwardto define our WFSA Fk such that Fk(x) computespik d(x,wk).
A direct construction of Fk is as fol-lows.
First, create a ?straight line?
WFSA whosesingle path accepts (only) wk; each each state corre-sponds to a position in wk.
These arcs all have cost0.
Now add various arcs with cost pik that permitedit operations.
For each arc labeled with a symbola ?
?, add competing ?substitution?
arcs labeledwith the other symbols in ?, and a competing ?dele-tion?
arc labeled with ; these have the same sourceand target as the original arc.
Also, at each state, adda self-loop labeled with each symbol in ?
; these are?insertion?
arcs.
Each arc that deviates from wk hasa cost of pik, and thus the lowest-cost path throughFk accepting x has weight pik d(x,wk).The consensus objective in Equation (7) can besolved by finding the lowest-cost path in F1 ?
.
.
.
?FK , and we can solve this best-path problem usingthe dual decomposition algorithm described above.5.2.1 ExperimentsWe ran our algorithm on Broadcast News data, us-ing 226 lattices produced by the IBM Attila decoder0 <s> I WANT TO BE TAKING A DEEP BREATH NOW </s><s> WE WANT TO BE TAKING A DEEP BREATH NOW </s><s> I DON?T WANT TO BE TAKING A DEEP BREATH NOW </s><s> WELL I WANT TO BE TAKING A DEEP BREATH NOW </s><s> THEY WANT TO BE TAKING A DEEP BREATH NOW </s>300 <s> I WANT TO BE TAKING A DEEP BREATH NOW </s><s> WE WANT TO BE TAKING A DEEP BREATH NOW </s><s> I DON?T WANT TO BE TAKING A DEEP BREATH NOW </s><s> WELL I WANT TO BE TAKING A DEEP BREATH NOW </s><s> WELL WANT TO BE TAKING A DEEP BREATH NOW </s>375 <s> I WANT TO BE TAKING A DEEP BREATH NOW </s><s> I WANT TO BE TAKING A DEEP BREATH NOW </s><s> I DON?T WANT TO BE TAKING A DEEP BREATH NOW </s><s> I WANT TO BE TAKING A DEEP BREATH NOW </s><s> I WANT TO BE TAKING A DEEP BREATH NOW </s>472 <s> I WANT TO BE TAKING A DEEP BREATH NOW </s><s> I WANT TO BE TAKING A DEEP BREATH NOW </s><s> I WANT TO BE TAKING A DEEP BREATH NOW </s><s> I WANT TO BE TAKING A DEEP BREATH NOW </s><s> I WANT TO BE TAKING A DEEP BREATH NOW </s>Figure 1: Example run of the consensus problem onK = 25 strings on a Broadcast News utterance, showingx1, .
.
.
, x5 at the 0th, 300th, 375th, and 472nd iterations.
(Chen et al, 2006; Soltau et al, 2010) on a subsetof the NIST dev04f data, using models trained byZweig et al (2011).
For each lattice, we found theconsensus of the top K = 25 strings.85% of the problems converged within 1000 it-erations, with an average of 147.4 iterations.
Wefound that the true consensus was often the mostlikely string under pi, but not always?this was true70% of the time.
In the Bayes risk objective we areoptimizing in equation (7)?the expected loss?ourapproach averaged a score of 1.59, while always tak-ing the top string gives only a slightly worse averageof 1.66.
8% of the problems encountered negative-weight cycles, which were all resolved either by de-creasing the step size or encoding larger n-grams.5.3 Investigating Consensus Performance withSynthetic DataThe above experiments demonstrate that we can ex-actly find the best path in the intersection of 25machines?an intersection that could not feasibly beconstructed in practice.
However, these experimentsdo not exhaustively explore how dual decompositionbehaves on the Steiner string problem in general.Above, we experimented with only a fixed num-ber of input strings, which were generally similar toone another.
There are a variety of other inputs to theconsensus problem which might lead to different be-havior and convergence results, however.
If we wereto instead run this experiment on DNA sequences(for example, if we posit that the strings are all muta-tions of the same ancestor), the alphabet {A,T,C,G}2380 5 10 15 20Runtime (s)0102030405060708090ScoreK=50,`=10,|?|=10,?=0.2PrimalDual0 10 20 30 40 50 60Runtime (s)01020304050ScoreK=10,`=15,|?|=20,?=0.4PrimalDual0 5 10 15 20 25 30 35Runtime (s)0510152025303540ScoreK=5,`=50,|?|=5,?=0.1PrimalDualFigure 2: The algorithm?s behavior on three specific consensus problems.
The curves show the current values ofthe primal bound (based on the best string at the current iteration) and dual bound h(?).
The horizontal axis showsruntime.
Red upper triangles are placed every 10 iterations, while blue lower triangles are placed for every 10%increase in the size of the feature set ?.K ` |?| ?
Conv.
Iters.
Red.5 100 5 0.1 68% 257 (?110) 24%5 100 5 0.2 0% ?
8%5 50 5 0.1 80% 123 (?
65) 20%5 50 5 0.2 10% 436 (?195) 18%10 50 5 0.1 69% 228 (?164) 18%10 50 5 0.2 0% ?
8%10 50 5 0.4 0% ?
3%10 30 10 0.1 100% 50 (?
69) 13%10 30 10 0.2 93% 146 (?142) 20%10 30 10 0.4 0% ?
16%10 15 20 0.1 100% 26 (?
6) 1%10 15 20 0.2 98% 43 (?
18) 10%10 15 20 0.4 63% 289 (?217) 18%10 15 20 0.8 0% ?
11%25 15 20 0.1 98% 30 (?
5) 0%25 15 20 0.2 92% 69 (?112) 6%25 15 20 0.4 55% 257 (?149) 16%25 15 20 0.8 0% ?
12%50 10 10 0.2 68% 84 (?141) 0%50 10 10 0.4 21% 173 (?
94) 9%100 10 10 0.2 44% 147 (?220) 0%100 10 10 0.4 13% 201 (?138) 6%Table 1: A summary of results for various consensusproblems, as described in ?5.3.is so small that n-grams are likely to be repeated inmany parts of the strings, and the lack of position in-formation in our features could make it hard to reachagreement.
Another interesting case is when the in-put strings have little or nothing in common?canwe still converge to an optimal consensus in a rea-sonable number of iterations?We can investigate many different cases by cre-ating synthetic data, where we tune the number ofinput strings K, the length of the strings, the size ofthe vocabulary |?|, as well as how similar the stringsare.
We do this by randomly generating a base stringx?
?
?` of length `.
We then generate K randomstrings w1, .
.
.
, wK , each by passing x?
through anoisy edit channel, where each position has inde-pendent probability ?
of making an edit.
For eachposition in x?, we uniformly sample once amongthe three types of edits (substitution, insertion, dele-tion), and in the case of the first two, we uniformlysample from the vocabulary (excluding the currentsymbol for substitution).
The larger ?, the more mu-tated the strings will be.
For small ?
or large K, theoptimal consensus ofw1, .
.
.
, wK will usually be x?.Table 1 shows results under various settings.
Eachline presents the percentage of 100 examples thatconverge within the iteration limit, the average num-ber of iterations to convergence (?
standard devi-ation) for those that converged, and the reductionin the objective value that is obtained over a sim-ple baseline of choosing the best string in the inputset, to show how much progress the algorithm makesbetween the 0th and final iteration.As expected, a higher mutation probability slowsconvergence in all cases, as does having longer in-put strings.
These results also confirm our hypothe-sis that a small alphabet would lead to slow conver-gence when using small n-gram features.
For thesetypes of strings, which might show up in biologicaldata, one would likely need more informative con-straints than position-agnostic n-grams.Figure 2 shows example runs on problems gen-erated at three different parameter settings.
We plotthe objective value as a function of runtime, showingboth the primal objective (3) that we hope to mini-mize, which we measure as the quality of the bestsolution among the {xk} that are output at the cur-239rent iteration, and the dual objective (5) that our al-gorithm is maximizing.
The dual problem (which isconcave in ?)
lower bounds the primal.
If the twofunctions ever touch, we know the solution to thedual problem is in the set of feasible solutions to theoriginal primal problem we are attempting to solve,and indeed must be optimal.
The figure shows thatthe dual function always has an initial value of 0,since we initialize each ?k = 0, and then Fk willsimply return the input wk as its best solution (sincewk has zero distance to itself).
As the algorithm be-gins to enforce the agreement constraints, the valueof the relaxed dual problem gradually worsens, untilit fully satisfies the constraints.These plots indicate the number of iterations thathave passed and the number of active features.
Wesee that the time per iteration increases as the num-ber of features increases, as expected, because more(and longer) n-grams are being encoded by G.The three patterns shown are typical of almost allthe trials we examined.
When the solution is in theoriginal input set (a likely occurrence for large K orsmall ?
?
`), the primal value will be optimal fromthe start, and our algorithm only has to prove its op-timality.
For more challenging problems, the primalsolution may jump around in quality at each iterationbefore settling into a stable part of the space.To investigate how different n-gram sizes affectconvergence rates, we experiment with using the en-tire set of n-grams (for a fixed n) for the durationof the optimization procedure.
Figure 3 shows con-vergence rates (based on both iterations and run-time) of different values of n for one set of param-eters.
While bigrams are very fast (average runtimeof 14s among those that converged), this convergedwithin 1000 iterations only 78% of the time, andthe remaining 22% end up bringing down the av-erage speed (with an overall average runtime over aminute).
All larger n-grams converged every time;trigrams had an average runtime of 32s.
Our algo-rithm, which begins with bigrams but brings in morefeatures (up to 5-grams) as needed, had an averageruntime of 19s (with 98% convergence).6 Discussion and Future WorkAn important (and motivating) property of La-grangian relaxation methods is the certificate of op-0 50 100 150 200Number of Iterations0100200300400500Runtime(s)K=10,`=15,|?|=20,?=0.2n=2n=3n=4n=5Figure 3: Convergence rates for a fixed set of n-grams.timality.
Even in instances where approximate algo-rithms perform well, it could be useful to have a trueoptimality guarantee.
For example, our algorithmcan be used to produce reference solutions, whichare important to have for research purposes.Under a sum-of-pairs Levenshtein objective, theexact multi-sequence alignment can be directly ob-tained from the Steiner consensus string and viceversa (Gusfield, 1997).
This implies that our ex-act algorithm could be also used to find exact multi-sequence alignments, an important problem in nat-ural language processing (Barzilay and Lee, 2003)and computational biology (Durbin et al, 2006) thatis almost always solved with approximate methods.We have noted that some constraints are moreuseful than others.
Position-specific information ishard to agree on and leads to slow convergence,while pure n-gram constraints do not work as wellfor long strings where the position may be impor-tant.
One avenue we are investigating is the useof a non-deterministic G, which would allow us toencode latent variables (Dreyer et al, 2008), suchas loosely defined ?regions?
within a string, and toallow for the encoding of alignments between theinput strings.
We would also like to extend thesemethods to other combinatorial optimization prob-lems involving strings, such as inference in graphi-cal models over strings (Dreyer and Eisner, 2009).To conclude, we have presented a general frame-work for applying dual decomposition to implicitWFSA intersection.
This could be applied to a num-ber of NLP problems such as language model andlattice intersection.
To demonstrate its utility on alarge number of automata, we applied it to consen-sus decoding, determining the true optimum in a rea-sonable amount of time on a large majority of cases.240ReferencesCyril Allauzen, Mehryar Mohri, and Brian Roark.
2003.Generalized algorithms for constructing statistical lan-guage models.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Linguis-tics, pages 40?47.Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-ciech Skut, and Mehryar Mohri.
2007.
OpenFst: Ageneral and efficient weighted finite-state transducerlibrary.
In Proceedings of the 12th International Con-ference on Implementation and Application of Au-tomata, CIAA?07, pages 11?23.Regina Barzilay and Lillian Lee.
2003.
Learning toparaphrase: An unsupervised approach using multiple-sequence alignment.
In Proceedings of the 2003 Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics on Human Lan-guage Technology - Volume 1, NAACL ?03, pages 16?23.Peter J. Bickel and Kjell A. Doksum.
2006.
Mathemat-ical Statistics: Basic Ideas and Selected Topics, vol-ume 1.
Pearson Prentice Hall.Stanley F. Chen, Brian Kingsbury, Lidia Mangu, DanielPovey, George Saon, Hagen Soltau, and GeoffreyZweig.
2006.
Advances in speech transcription atIBM under the DARPA EARS program.
IEEE Trans-actions on Audio, Speech & Language Processing,14(5):1596?1608.Markus Dreyer and Jason Eisner.
2009.
Graphical mod-els over multiple strings.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, EMNLP ?09, pages 101?110.
As-sociation for Computational Linguistics.Markus Dreyer, Jason R. Smith, and Jason Eisner.
2008.Latent-variable modeling of string transductions withfinite-state methods.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP), pages 1080?1089, Honolulu, October.R.
Durbin, S. Eddy, A. Krogh, and G. Mitchison.
2006.Biological Sequence Analysis.
Cambridge UniversityPress.Vaibhava Goel and William J. Byrne.
2003.
Mini-mum Bayes risk methods in automatic speech recog-nition.
In Wu Chou and Biing-Hwang Juan, editors,Pattern Recognition in Speech and Language Process-ing.
CRC Press.Dan Gusfield.
1997.
Algorithms on Strings, Trees, andSequences: Computer Science and Computational Bi-ology.
Cambridge University Press.George Karakostas, Richard J Lipton, and Anastasios Vi-glas.
2003.
On the complexity of intersecting finitestate automata and NL versus NP.
Theoretical Com-puter Science, pages 257?274.Kevin Knight and Yaser Al-Onaizan.
1998.
Translationwith finite-state devices.
In AMTA?98, pages 421?437.N.
Komodakis, N. Paragios, and G. Tziritas.
2007.MRF optimization via dual decomposition: Message-Passing revisited.
In Computer Vision, 2007.
ICCV2007.
IEEE 11th International Conference on, pages1?8.Terry Koo, Alexander M. Rush, Michael Collins, TommiJaakkola, and David Sontag.
2010.
Dual decompo-sition for parsing with non-projective head automata.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?10, pages 1288?1298.Mehryar Mohri.
2002.
Semiring frameworks and algo-rithms for shortest-distance problems.
J. Autom.
Lang.Comb., 7:321?350, January.Fernando C. N. Pereira and Michael Riley.
1997.
Speechrecognition by composition of weighted finite au-tomata.
CoRR.Alexander M. Rush and Michael Collins.
2011.
Exactdecoding of syntactic translation models through La-grangian relaxation.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies - Volume1, HLT ?11, pages 72?82.Alexander M. Rush, David Sontag, Michael Collins, andTommi Jaakkola.
2010.
On dual decomposition andlinear programming relaxations for natural languageprocessing.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing,EMNLP ?10, pages 1?11.Jeong Seop Sim and Kunsoo Park.
2003.
The consen-sus string problem for a metric is NP-complete.
J. ofDiscrete Algorithms, 1:111?117, February.H.
Soltau, G. Saon, and B. Kingsbury.
2010.
The IBMAttila speech recognition toolkit.
In Proc.
IEEE Work-shop on Spoken Language Technology, pages 97?102.David Sontag, Talya Meltzer, Amir Globerson, YairWeiss, and Tommi Jaakkola.
2008.
Tightening LPrelaxations for MAP using message-passing.
In 24thConference in Uncertainty in Artificial Intelligence,pages 503?510.
AUAI Press.David Sontag, Amir Globerson, and Tommi Jaakkola.2011.
Introduction to dual decomposition for in-ference.
In Suvrit Sra, Sebastian Nowozin, andStephen J. Wright, editors, Optimization for MachineLearning.
MIT Press.James C. Spall.
2003.
Introduction to Stochastic Searchand Optimization.
John Wiley & Sons, Inc., NewYork, NY, USA, 1 edition.Ben Taskar, Vassil Chatalbashev, Daphne Koller, andCarlos Guestrin.
2005.
Learning structured predictionmodels: A large margin approach.
In Proceedings of241the 22nd international conference on Machine learn-ing, ICML ?05, pages 896?903.Geoffrey Zweig, Patrick Nguyen, Dirk Van Compernolle,Kris Demuynck, Les E. Atlas, Pascal Clark, GregorySell, Meihong Wang, Fei Sha, Hynek Hermansky,Damianos Karakos, Aren Jansen, Samuel Thomas,Sivaram G. S. V. S., Sam Bowman, and Justine T. Kao.2011.
Speech recognition with segmental conditionalrandom fields: A summary of the JHU CLSP 2010Summer Workshop.
In ICASSP.242
