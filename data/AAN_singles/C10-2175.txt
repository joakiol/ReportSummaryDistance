Coling 2010: Poster Volume, pages 1533?1540,Beijing, August 2010All in Strings: a Powerful String-based Automatic MTEvaluation Metric with Multiple GranularitiesJunguo Zhu1, Muyun Yang1, Bo Wang2, Sheng Li1, Tiejun Zhao11 School of Computer Science and Technology, Harbin Institute of Technology{jgzhu; ymy; tjzhao; lish}@mtlab.hit.edu.cn2 School of Computer Science and Technology, Tianjin Universitybo.wang.1979@gmail.comAbstractString-based metrics of automatic ma-chine translation (MT) evaluation arewidely applied in MT research.
Mean-while, some linguistic motivated me-trics have been suggested to improvethe string-based metrics in sentence-level evaluation.
In this work, we at-tempt to change their original calcula-tion units (granularities) of string-basedmetrics to generate new features.
Wethen propose a powerful string-basedautomatic MT evaluation metric, com-bining all the features with variousgranularities based on SVM rank andregression models.
The experimentalresults show that i) the new featureswith various granularities can contri-bute to the automatic evaluation oftranslation quality; ii) our proposedstring-based metrics with multiple gra-nularities based on SVM regressionmodel can achieve higher correlationswith human assessments than the state-of-art  automatic metrics.1 IntroductionThe automatic machine translation (MT) eval-uation has aroused much attention from MTresearchers in the recent years, since the auto-matic MT evaluation metrics can be applied tooptimize MT systems in place of the expensiveand time-consuming human assessments.
Thestate-of-art strategy to automatic MT evalua-tion metrics estimates the system output quali-ty according to its similarity to human refer-ences.
To capture the language variability ex-hibited by different reference translations, atendency is to include deeper linguistic infor-mation into machine learning based automaticMT evaluation metrics, such as syntactic andsemantic information (Amig?
et al, 2005; Al-brecht and Hwa, 2007; Gim?nez and M?rquez,2008).
Generally, such efforts may achievehigher correlation with human assessments byincluding more linguistic features.
Neverthe-less, the complex and variously presented lin-guistic features often prevents the wide appli-cation of the linguistic motivated metrics.Essentially, linguistic motivated metrics in-troduce additional restrictions for accepting theoutputs of translations (Amig?
et al, 2009).With more linguistic features attributed, themodel is actually capturing the sentence simi-larity in a finer granularity.
In this sense, thepractical effect of employing various linguisticknowledge is changing the calculation units ofthe matching in the process of the automaticevaluation.Similarly, the classical string-based metricscan be changed in their calculation units direct-ly.
For example, the calculation granularity inBLEU (Papineni et al, 2002) metric is word:n-grams are extracted on the basis of singleword as well as adjacent multiple words.
Andthe calculation granularity in PosBLEU(Popovi?
and Ney, 2009) metric is Pos tag,which correlate well with the human assess-ments.
Therefore, it is straight forward to applythe popular string-based automatic evaluationmetrics, such as BLEU, to compute the scoresof the systems outputs in the surface or linguis-1533tic tag sequences on various granularities le-vels.In this paper, we attempt to change the orig-inal calculation units (granularities) of string-based metrics to generate new features.
Afterthat, we propose a powerful string-based au-tomatic MT evaluation metric, combining allthe features with various granularities based onSVM rank (Joachims, 2002) and regression(Drucker et al, 1996) models.
Our analysisindicates that: i) the new features with variousgranularities can contribute to the automaticevaluation of translation quality; ii) our pro-posed string-based metrics with multiple gra-nularities based on SVM regression model canachieve higher correlations with human as-sessments than the state-of-art automatic me-trics .The remainder of this paper is organized asfollows: Section 2 reviews the related re-searches on automatic MT evaluation.
Section3 describes some new calculation granularitiesof string-based metrics on sentence level.
InSection 4, we propose string-based metricswith multiple granularities based on SVM rankand regression models.
In Section 5, wepresent our experimental results on differentsets of data.
And conclusions are drawn in theSection 6.2 Related Work on Automatic Ma-chine Translation EvaluationThe research on automatic string-based ma-chine translation (MT) evaluation is targeted ata widely applicable metric of high consistencyto the human assessments.
WER (Nie?en et al,2000), PER (Tillmann et al, 1997), and TER(Snover et al, 2006) focuses on word error rateof translation output.
GTM (Melamed et al,2003) and the variants of ROUGE (Lin andOch, 2004) concentrate on matched longestcommon substring and discontinuous substringof translation output according to the humanreferences.
BLEU (Papineni et al, 2002) andNIST (Doddington, 2002) are both based onthe number of common n-grams between thetranslation hypothesis and human referencetranslations of the same sentence.
BLEU andNIST are widely adopted in the open MT eval-uation campaigns; however, the NIST MTevaluation in 2005 indicates that they can evenerror in the system level (Le and Przybocki,2005).
Callison-Burch et al (2006) detailed thedeficits of the BLEU and other similar metrics,arguing that the simple surface similarity cal-culation between the machines translations andthe human translations suffers from morpho-logical issues and fails to capture what are im-portant for human assessments.In order to attack these problems, some me-trics have been proposed to include more lin-guistic information into the process of match-ing, e.g., Meteor (Banerjee and Lavie, 2005)metric and MaxSim (Chan nad Ng, 2008) me-trics, which improve the lexical level by thesynonym dictionary or stemming technique.There are also substantial studies focusing onincluding deeper linguistic information in themetrics (Liu and Gildea, 2005; Owczarzak etal., 2006; Amig?
et al, 2006; Mehay and Brew,2007; Gim?nez and M?rquez, 2007; Owczar-zak et al, 2007; Popovic and Ney, 2007;Gim?nez and M?rquez, 2008b).A notable trend improving the string-basedmetric is to combine various deeper linguisticinformation via machine learning techniques inthe metrics (Amig?
et al, 2005; Albrecht andHwa, 2007; Gim?nez and M?rquez, 2008).Such efforts are practically amount of intro-ducing additional linguistic restrictions into theautomatic evaluation metrics (Amig?
et al2009), achiving a higher performance at thecost of lower adaptability to other languagesowing to the language dependent linguisticsfeatures.Previous work shows that including the newfeatures into the evaluation metrics may bene-fit to describe nature language accurately.
Inthis sense, the string-based metrics will be im-proved, if the finer calculation granularities areintroduced into the metrics.Our study analyzes the role of the calcula-tion granularities in the performance of metrics.We find that the new features with variousgranularities can contribute to the automaticevaluation of translation quality.
Also we pro-pose a powerful string based automatic MTevaluation metric with multiple granularitiescombined by SVM.
Finally, we seek a finerfeature set of metrics with multiple calculationgranularities.15343 The New Calculation Granularitiesof String-based Metrics on SentenceLevelThe string-based metrics of automatic machinetranslation evaluation on sentence level adopt acommon strategy: taking the sentences of thedocuments as plain strings.
Therefore, whenchanging the calculation granularities of thestring-based metrics we can simplify the in-formation of new granularity with plain strings.In this work, five kinds of available calculationgranularities are defined: ?Lexicon?, ?Letter?,?Pos?, ?Constitute?
and ?Dependency?.Lexicon: The calculation granularity iscommon word in the sentences of the docu-ments, which is popular practice at present.Letter: Split the granularities of ?Lexical?into letters.
Each letter is taken as a matchingunit.Pos: The Pos tag of each ?Lexicon?
is takenas a matching unit in this calculation granulari-ty.Constitute: Syntactic Constitutes in a treestructure are available through the parser tools.We use Stanford Parser (Klein and Manning,2003a; Klein and Manning, 2003b) in thiswork.
The Constitute tree is changed intoplain string, travelling by BFS (Breadth-firstsearch traversal) 1.Dependency: Dependency relations in a de-pendency structure are also available throughthe parser tools.
The dependency structure canalso be formed in a tree, and the sameprocessing of being changed into plain string isadopted as ?Constitute?.The following serves as an example:Sentence:I have a dogPos tag:I/PRON have/V a/ART dog/NConstitute tree:1 We also attempt some other traversal algorithms, in-cluding preorder, inorder and postorder traversal, theperformance are proved to be similar.Dependency tree:Then, we can change the sentence into theplain string in multiple calculation granulari-ties as follows:Lexicon string:I have a dogLetter string:I h a v e a d o gPos string:PRON V ART NConstitute string:PRON V ART N NP NP VP SDependency string:a I dog haveThe translation hypothesis and human refer-ence translations are both changed into thosestrings of various calculation granularities.
Thestrings are taken as inputs of the string-basedautomatic MT evaluation metrics.
The outputsof each metric are calculated on differentmatching units.4 String-based Metrics with MultipleGranularities Combined by SVMIntroducing machine learning methods to es-tablished MT evaluation metric is a populartrend.
Our study chooses rank and regressionsupport vector machine (SVM) as the learningmodel.
Features are important for the SVMmodels.Plenty of scores can be generated from theproposed metrics.
In fact, not all these featuresare needed.
Therefore, feature selection shouldbe a necessary step to find a proper feature setand alleviate the language dependency by us-ing fewer linguistic features.Feature selection is an NP-Complete prob-lem; therefore, we adopt a greedy selectionalgorithm called ?Best One In?
to find a localoptimal feature set.
Firstly, we select the fea-ture among all the features which best corre-lates with the human assessments.
Secondly, afeature among the rest features is added in tothe feature set, if the correlation with the hu-man assessments of the metric using new set is1535the highest among all new metrics and higherthan the previous metric in cross training cor-pus.
The cross training corpus is prepared bydividing the training corpus into five parts.Each four parts of the five are for training andthe rest one for testing; then, we integratescores of the five tests as scores of cross train-ing corpus.
The five-fold cross training canhelp to overcome the overfitting.
At the end,the feature selection stops, if adding any of therest features cannot lead to higher correlationwith human assessments than the current me-tric.5 Experiments5.1 The Impact of the Calculation Granu-larities on String-based MetricsIn this section, we use the data from NISTOpen MT 2006 evaluation (LDC2008E43),which is described in Table 1.
It consists of249 source sentences that were translated byfour human translators as well as 8 MT sys-tems.
Each machine translated sentence wasevaluated by human judges for their adequacyon a 7-point scale.NIST 2002NIST2003NISTOpenMT 2006LDCcorpusLDC2003T17LDC2006T04LDC2008E43Type Newswire Newswire NewswireSource Chinese Chinese ArabicTarget English English English# ofsentences 878 919 249# ofsystems 3 7 8#  ofreferences 4 4 4Score1-5,adequacy& fluency1-5,adequacy& fluency1-7adequacyTable 1: Description of LDC2006T04,LDC2003T17 and LDC2008E43To judge the quality of a metric, we com-pute Spearman rank-correlation coefficient,which is a real number ranging from -1 (indi-cating perfect negative correlations) to +1 (in-dicating perfect positive correlations), betweenthe metric?s scores and the averaged humanassessments on test sentences.We select 21 features in ?lexicon?
calcula-tion granularity and 11?4 in the other calcula-tion granularities.
We analyze the correlationwith human assessments of the metrics in mul-tiple calculation granularities.
Table 2 lists theoptimal calculation granularity of the multiplemetrics on sentence level in the data(LDC2008E43).Metric GranularityBLEU-opt LetterNIST-opt LetterGTM(e=1) DependencyTER LetterPER LexiconWER DependencyROUGE-opt LetterTable 2 The optimal calculation granularity of themultiple metricsThe most remarkable aspect is that not allthe best metrics are based on the ?lexicon?
cal-culation granularities, such as the ?letter?
and?dependency?.
In other words, the granulari-ties-shifted string-based metrics are promisingto contribute to the automatic evaluation oftranslation quality.5.2 Correlation with Human Assessmentsof String-based Metrics with MultipleGranularities Based on SVM FrameWe firstly train the SVM rank and regressionmodels on LDC2008E43 using all the features(21+11?
4 species), without any selection.Secondly, the other two SVM rank and regres-sion models are trained on the same data usingthe feature set via feature selection, which aredescribed in Table 3.
We have four string-based evaluation metrics with multiple granu-larities on rank and regression SVM frame?Rank_All, Regression_All, Rank_Select andRegression_Select?.
Then we apply the fourmetrics to evaluate the sentences of the testdata (LDC2006T04 and LDC2003T17).
Theresults of Spearman correlation with humanassessments is summarized in Table 3.
Forcomparison, the results from some state-of-artmetrics (Papineni et al, 2002; Doddington,15362002; Melamed et al, 2003; Banerjee and La-vie, 2005; Snover et al, 2006; Liu and Gildea,2005) and two machine learning methods (Al-brecht and Hwa, 2007; Ding Liu and Gildea,2007) are also included in Table 3.
Of the twomachine learning methods, both trained on thedata LDC2006T04.
The ?Albrecht, 2007?score reported a result of Spearman correlationwith human assessments on the dataLDC2003T17 using 53 features, while the?Ding Liu, 2007?
score reported that underfive-fold cross validation on the dataLDC2006T04 using 31 features.Feature numberLDC2003T17LDC2006T04Rank_All 65 0.323 0.495Regression_All 65 0.345 0.507Rank_Select 16 0.338 0.491Regression_Select 8 0.341 0.510Albrecht, 2007 53 0.309 --Ding Liu, 2007 31 -- 0.369BLEU-opt2 -- 0.301 0.453NIST-opt -- 0.219 0.417GTM(e=1) -- 0.270 0.375METEOR3 -- 0.277 0.463TER -- -0.250 -0.302STM-opt -- 0.205 0.226HWCM-opt -- 0.304 0.377Table 3: Comparison of Spearman correlations withhuman assessments of our proposed metrics andsome start-of-art metrics and two machine learningmethods?-opt?
stands for the optimum values of the pa-rameters on the metricsTable 3 shows that the string-based meta-evaluation metrics with multiple granularitiesbased on SVM frame gains the much higherSpearman correlation than other start-of-artmetrics on the two test data and, furthermore,our proposed metrics also are higher than themachine learning metrics (Albrecht and Hwa,2007; Ding Liu and Gildea, 2007).The underlining is that our proposed metricsare more robust than the aforementioned two2 The result is computed by mteval11b.pl.3 The result is computed by meteor-v0.7.machine learning metrics.
As shown in Table 1the heterogeneity between the training and testdata in our method is much more significantthan that of the other two machine learningbased methods.In addition, the ?Regression_Select?
metricusing only 8 features can achieve a high corre-lation rate which is close to the metric pro-posed in ?Albrecht, 2007?
using 53 features,?Ding Liu, 2007?
using 31 features, ?Regres-sion_All?
and ?Rank_All?
metrics using  65features and ?Rank_Select?
metric using 16features.
What is more, ?Regression_Select?metric is better than ?Albrecht, 2007?, andslightly lower than ?Regression_All?
on thedata LDC2003T17; and better than both ?Re-gression_All?
and ?Rank_All?
metrics on thedata LDC2006T04.
That confirms that a smallcardinal of feature set can also result in a me-tric having a high correlation with human as-sessments, since some of the features representthe redundant information in different forms.Eliminating the redundant information is bene-fit to reduce complexity of the parametersearching and thus improve the metrics per-formance based on SVM models.
Meanwhile,fewer features can relieve the language depen-dency of the machine learning metrics.
At last,our experimental results show that regressionmodels perform better than rank models in thestring-based metrics with multiple granularitiesbased on SVM frame, since ?Regres-sion_Select?
and ?Regression_All?
achievehigher correlations with human assessmentsthan the others.5.3 Reliability of Feature SelectionThe motivation of feature selection is keepingthe validity of the feature set and alleviatingthe language dependency.
We also look for-ward to the higher Spearman correlation on thetest data with a small and proper feature set.We use SVM-Light (Joachims, 1999) totrain our learning models using NIST OpenMT 2006 evaluation data (LDC2008E43), andtest on the two sets of data, NIST?s 2002 and2003 Chinese MT evaluations.
All the data aredescribed in Table 1.
To avoid the bias in thedistributions of the two judges?
assessments inNIST?s 2002 and 2003 Chinese MT evalua-tions, we normalize the scores following (Blatzet al, 2003).1537We trace the process of the feature selection.The selected feature set of the metric based onSVM rank includes 16 features and that of themetric based on SVM regression includes 8features.
The selected features are listed in Ta-ble 4.
The values in Table 4 are absoluteSpearman correlations with human assess-ments of each single feature score.
The prefix-es ?C_?, ?D_?, ?L_?, ?P_?, and ?W_?represent ?Constitute?, ?Dependency?, ?Let-ter?, ?Pos?
and ?Lexicon?
respectively.Rank spear-man Regressionspear-manC_PER .331 C_PER .331C_ROUGE-W .562 C_ROUGE-W .562D_NIST9 .479 D_NIST9 .479D_ROUGE-W .679 D_ROUGE-L .667L_BLEU6 .702 L_BLEU6 .702L_NIST9 .691 L_NIST9 .691L_ROUGE-W .634 L_ROUGE-W .634P_PER .370 P_ROUGE-W .683P_ROUGE-W .616W_BLEU1_ind .551W_BLEU2 .659W_GTM .360W_METEOR .693W_NIST5 .468W_ROUGE1 .642W_ROUGE-W .683Table 4: Feature sets of SVM rank and regressionTable 4 shows that 8 features are selectedfrom 65 features in the process of feature se-lection based on SVM regression while 16 fea-tures based on SVM rank.
Fewer featuresbased on SVM regression are selected thanSVM rank.
Only one feature in feature setbased on SVM regression does not occur inthat based on SVM rank.
The reason is thatthere are more complementary advantages be-tween the common selected features.Next, we will verify the reliability of ourfeature selection algorithm.
Figure 1 and Fig-ure 2 show the Spearman correlation valuesbetween our SVM-based metrics (regressionand rank) and the human assessments on bothtraining data (LDC2008E43) and test data(LDC2006T04 and LDC2003T17).Figure 1: The Spearman correlation values betweenour SVM rank metrics and the human assessmentson both training data and test data with the exten-sion of the feature setsFigure 2: The Spearman correlation values betweenour SVM regression metrics and the human as-sessments on both training data and test data withthe extension of the feature setsFrom Figure 1 and Figure 2, with the exten-sion of the feature sets, we can find that thetendency of correlation obtained by each me-tric based on SVM rank or regression roughlythe same on both the training data and test data.Therefore, the two feature sets of SVM rankand regression models are reliable.6 ConclusionIn this paper we propose an integrated platformfor automatic MT evaluation by improving thestring based metrics with multiple granularities.Our proposed metrics construct a novel inte-grated platform for automatic MT evaluationbased on multiple features.
Our  key contribu-tion consists of two parts: i) we suggest a strat-egy  of changing the various complex featuresinto plain string form.
According to the strate-gy, the automatic MT evaluation frame are1538much more clarified, and the computation ofthe similarity is much more simple, since thevarious linguistic features may express in theuniform strings with multiple calculation gra-nularities.
The new features have the sameform and are dimensionally homogeneous;therefore, the consistency of the features isenhanced strongly.
ii) We integrate the featureswith machine learning and proposed an effec-tive approach of feature selection.
As a result,we can use fewer features but obtain the betterperformance.In this framework, on the one hand, string-based metrics with multiple granularities mayintroduce more potential features into automat-ic evaluation, with no necessarily of new simi-larity measuring method, compared with theother metrics.
On the other hand, we succeedin finding a finer and small feature set amongthe combinations of plentiful features, keepingor improving the performance.
Finally, weproposed a simple, effective and robust string-based automatic MT evaluation metric withmultiple granularities.Our proposed metrics improve the flexibilityand performance of the metrics based on themultiple features; however, it still has somedrawbacks: i) some potential features are notyet considered, e.g.
the semantic roles; and ii)the loss of information exists in the process ofchanging linguistic information into plainstrings.
For example, the dependency label inthe calculation granularity ?Dependency?
islost when changing information into stringform.
Though the final results obtain the betterperformance than the other linguistic metrics,the performance is promising to be further im-proved if the loss of information can be prop-erly dealt with.AcknowledgementThis work is supported by Natural Sciencefoundation China (Grant No.60773066 &60736014) and National Hi-tech Program(Project No.2006AA010108), and the NaturalScientific Reserach Innovation Foundation inHarbin Institute of Technology (Grant No.HIT.NSFIR.20009070).ReferencesAlbrecht S. Joshua and Rebecca Hwa.
2007.
AReexamination of Machine Learning Approachesfor Sentence-Level MT Evaluation.
In Proceed-ings of the 45th Annual Meeting of the Associa-tion of Computational Linguistics, pages 880-887.Amig?
Enrique, Julio Gonzalo, Anselmo P?nas,and Felisa Verdejo.
2005.
QARLA: a Frameworkfor the Evaluation of Automatic Summarization.In Proceedings of the 43th Annual Meeting ofthe Association for Computational Linguistics.Amig?
Enrique, Jes?s Gim?nez, Julio Gonzalo,Felisa Verdejo.
2009.
The Contribution of Lin-guistic Features to Automatic Machine Transla-tion Evaluation.
In proceedings of the Joint Con-ference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference onNatural Language Processing of the AFNLP.Amig?
Enrique, Jes?s Gim?nez, Julio Gonzalo, andLlu?s M?rquez.
2006.
MT Evaluation: Human-Like vs. Human Acceptable.
In Proceedings ofthe Joint 21st International Conference on Com-putational Linguistics and the 44th Annual Meet-ing of the Association for Computational Lin-guistic, pages 17?24.Banerjee Satanjeev and Alon Lavie.
2005.
ME-TEOR: An automatic metric for MT evaluationwith improved correlation with human judg-ments.
In Proceedings of the ACL Workshop onIntrinsic and Extrinsic Evaluation Measures.Blatz John, Erin Fitzgerald, George Foster, SimonaGandrabur, Cyril Goutte, Alex Kulesza, AlbertoSanchis, and Nicola Ueffing.
2003.
Confidenceestimation for machine translation.
In TechnicalReport Natural Language Engineering WorkshopFinal Report, pages 97-100.Callison-Burch Chris, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluating the Role of BLEU inMachine Translation Research.
In Proceedingsof 11th Conference of the European Chapter ofthe Association for Computational LinguisticsChan S. Yee and Hwee T. Ng.
2008.
MAXSIM: Amaximum similarity metric for machine transla-tion evaluation.
In Proceedings of ACL-08: HLT,pages 55?62.Doddington George.
2002.
Automatic Evaluation ofMachine Translation Quality Using N-gram Co-Occurrence Statistics.
In Proceedings of the 2ndInternational Conference on Human LanguageTechnology, pages 138?145.1539Drucker Harris, Chris J. C. Burges, Linda Kaufman,Alex Smola, Vladimir Vapnik.
1996.
Supportvector regression machines.
In NIPS.Gim?nez Jes?s and Llu?s M?rquez.
2007.
LinguisticFeatures for Automatic Evaluation of Heteroge-neous MT Systems.
In Proceedings of the ACLWorkshop on Statistical Machine Translation.Gim?nez Jes?s and Llu?s M?rquez.
2008a.
Hetero-geneous Automatic MT Evaluation ThroughNon-Parametric Metric Combinations.
In Pro-ceedings of IJCNLP, pages 319?326.Gim?nez Jes?s and Llu?s M?rquez.
2008b.
On theRobustness of Linguistic Features for AutomaticMT Evaluation.Joachims Thorsten.
2002.
Optimizing search en-gines using clickthrough data.
In KDD.Klein Dan and Christopher D. Manning.
2003a.Fast Exact Inference with a Factored Model forNatural Language Parsing.
In Advances inNeural Information Processing Systems 15, pp.3-10.Klein Dan and Christopher D. Manning.
2003b.Accurate Unlexicalized Parsing.
Proceedings ofthe 41st Meeting of the Association for Compu-tational Linguistics, pp.
423-430.Le Audrey and Mark Przybocki.
2005.
NIST 2005machine translation evaluation official results.In Official release of automatic evaluation scoresfor all submission.Lin Chin-Yew and Franz Josef Och.
2004.
Auto-matic Evaluation of Machine Translation Quali-ty Using Longest Common Subsequence andSkip-Bigram Statistics.
Proceedings of the 42ndAnnual Meeting of the Association for Computa-tional Linguistics, pp.
605-612.Liu Ding and Daniel Gildea.
2005.
Syntactic Fea-tures for Evaluation of Machine Translation.
InProceedings of ACL Workshop on Intrinsic andExtrinsic Evaluation Measures for MT and/orSummarization, pages 25?32.Liu Ding and Daniel Gildea.
2007.
Source Lan-guage Features and Maximum CorrelationTraining for Machine Translation Evaluation.
Inproceedings of NAACL HLT 2007, pages 41?48Mehay Dennis and Chris Brew.
2007.
BLEUATRE:Flattening Syntactic Dependencies for MT Eval-uation.
In Proceedings of the 11th Conference onTheoretical and Methodological Issues in Ma-chine Translation.Melamed Dan I., Ryan Green, and Joseph P. Turian.2003.
Precision and Recall of Machine Transla-tion.
In Proceedings of the Joint Conference onHuman Language Technology and the NorthAmerican Chapter of the Association for Com-putational Linguistics.Nie?en Sonja, Franz Josef Och, Gregor Leusch, andHermann Ney.
2000.
An Evaluation Tool forMachine  Translation: Fast Evaluation for MTResearch.
In Proceedings of the 2nd Internation-al Conference on Language Resources and Eval-uation .Owczarzak Karolina, Declan Groves, Josef VanGenabith, and Andy Way.
2006.
Contextual Bi-text- Derived Paraphrases in Automatic MTEvaluation.
In Proceedings of the 7th Confe-rence of the Association for Machine Translationin the Americas, pages 148?155.Owczarzak Karolina, Josef van Genabith, and AndyWay.
2007.
Labelled Dependencies in MachineTranslation Evaluation.
In Proceedings of theACL Workshop on Statistical Machine Transla-tion, pages 104?111.Papineni Kishore, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: a Method for Auto-matic Evaluation of Machine Translation.
InProceedings of 40th Annual Meeting of the As-sociation for Computational Linguistics.Popovi?
Maja and Hermann Ney.
2007.
Word Er-ror Rates: Decomposition over POS classes andApplications for Error Analysis.
In Proceedingsof the Second Workshop on Statistical MachineTranslation, pages 48?55.Popovi?
Maja and Hermann Ney.
2009.
Syntax-oriented evaluation measures for machine trans-lation output.
In Proceedings of the 4th EACLWorkshop on Statistical Machine Translation,pages 29?32.Snover Matthew, Bonnie Dorr, Richard Schwartz,Linnea Micciulla, and John Makhoul.
2006.
Astudy of translation edit rate with targeted hu-man annotation.
In Proceedings of AMTA, pag-es 223?231.Tillmann Christoph, Stefan Vogel, Hermann Ney,A.
Zubiaga, and H. Sawaf.
1997.
AcceleratedDP based Search for Statistical Translation.
InProceedings of European Conference on SpeechCommunication and Technology.1540
