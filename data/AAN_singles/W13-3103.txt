Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 20?28,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsMulti-document multilingual summarization and evaluation tracks inACL 2013 MultiLing WorkshopGeorge GiannakopoulosNCSR Demokritos, GreeceSciFY NPC, Greeceggianna@iit.demokritos.grAbstractThe MultiLing 2013 Workshop of ACL2013 posed a multi-lingual, multi-document summarization task to thesummarization community, aiming toquantify and measure the performance ofmulti-lingual, multi-document summa-rization systems across languages.
Thetask was to create a 240?250 word sum-mary from 10 news articles, describinga given topic.
The texts of each topicwere provided in 10 languages (Arabic,Chinese, Czech, English, French, Greek,Hebrew, Hindi, Romanian, Spanish) andeach participant generated summariesfor at least 2 languages.
The evaluationof the summaries was performed usingautomatic and manual processes.
Theparticipating systems submitted over 15runs, some providing summaries acrossall languages.
An automatic evaluationtask was also added to this year?s setof tasks.
The evaluation task meant todetermine whether automatic measuresof evaluation can function well in themulti-lingual domain.
This paper providesa brief description related to the data ofboth tasks, the evaluation methodology, aswell as an overview of participation andcorresponding results.1 IntroductionThe MultiLing Pilot introduced in TAC 2011was a combined community effort to presentand promote multi-document summarization ap-poraches that are (fully or partly) language-neutral.This year, in the MultiLing 2013 Workshop ofACL 2013, the effort grew to include a total of10 languages in a multi-lingual, multi-documentsummarization corpus: Arabic, Czech, English,French, Greek, Hebrew, Hindi from the old cor-pus, plus Chinese, Romanian and Spanish as newadditions.
Furthermore, the document set in exist-ing languages was extended by 5 new topics.
Wealso added a new track aiming to work on evalu-ation measures related to multi-document summa-rization, similarly to the AESOP task of the recentText Analysis Conferences.This document describes:?
the tasks and the data of the multi-documentmultilingual summarization track;?
the evaluation methodology of the participat-ing systems (Section 2.3);?
the evaluation track of MultiLing (Section 3).?
The document is concluded (Section 4) with asummary and future steps related to this spe-cific task.The first track aims at the real problem of sum-marizing news topics, parts of which may be de-scribed or happen in different moments in time.The implications of including multiple aspects ofthe same event, as well as time relations at a vary-ing level (from consequtive days to years), are stilldifficult to tackle in a summarization context.
Fur-thermore, the requirement for multilingual appli-cability of the methods, further accentuates the dif-ficulty of the task.The second track, summarization evaluation,is related the corresponding, prominent researchproblem of how to automatically evaluate a sum-mary.
While commonly used methods build upona few human summaries to be able to judge au-tomatic summaries (e.g., (Lin, 2004; Hovy et al2005)), there also exist works on fully automaticevaluation of summaries, without human?model?summaries (Louis and Nenkova, 2012; Saggionet al 2010).
The Text Analysis Conference hasa separate track, named AESOP (e.g.
see (Dang20and Owczarzak, 2009)) aiming to test and evaluatedifferent automatic evaluation methods of summa-rization systems.
We perform a similar task, but ina multilingual setting.2 Multi-document multi-lingualsummarization trackIn the next paragraphs we describe the task, thecorpus, the evaluation methodology and the resultsrelated to the summarization track of MultiLing2013.2.1 The summarization taskThis MultiLing task aims to evaluate the appli-cation of (partially or fully) language-independentsummarization algorithms on a variety of lan-guages.
Each system participating in the task wascalled to provide summaries for a range of differentlanguages, based on corresponding corpora.
In theMultiLing Pilot of 2011 the languages used were 7,while this year systems were called to summarizetexts in 10 different languages: Arabic, Chinese,Czech, English, French, Greek, Hebrew, Hindi,Romanian, Spanish.
Participating systems wererequired to apply their methods to a minimum oftwo languages.The task was aiming at the real problem of sum-marizing news topics, parts of which may be de-scribed or may happen in different moments intime.
We consider, similarly to MultiLing 2011(Giannakopoulos et al 2011) that news topics canbe seen as event sequences:Definition 1 An event sequence is a set of atomic(self-sufficient) event descriptions, sequenced intime, that share main actors, location of occurenceor some other important factor.
Event sequencesmay refer to topics such as a natural disaster, acrime investigation, a set of negotiations focusedon a single political issue, a sports event.The summarization task requires to generate asingle, fluent, representative summary from a setof documents describing an event sequence.
Thelanguage of the document set will be within thegiven range of 10 languages and all documents in aset share the same language.
The output summaryshould be of the same language as its source doc-uments.
The output summary should be between240 and 250 words.2.2 Summarization CorpusThe summarization corpus is based on a gath-ered English corpus of 15 topics (10 of whichwere already available fromMultiLing 2011), eachcontaining 10 texts.
Each topic contains at leastone event sequence.
The English corpus was thentranslated to all other languages (see also (Li etal., 2013; Elhadad et al 2013)), trying to gener-ate sentence-parallel translations.The input documents generated are UTF8-encoded, plain text files.
The whole set of trans-lated documents together with the original Englishdocument set will be referred to as the Source Doc-ument Set.
Given the creation process, the SourceDocument Set contains a total of 1350 texts (650more than the corpus of the MultiLing 2011 Pilot):7 languages (Arabic, Czech, English, Greek, ) with15 topics per language and 10 texts per topic for atotal of 1050 texts; 3 languages (Chinese, French,Hindi) with 10 topics per language and 10 texts pertopic for a total of 300 texts.The non-Chinese texts had an average wordlength of approximately 350 words (and a standarddeviation of 224 words).
Since words in Chinesecannot be counted easily, the Chinese text lengthwas based on the byte length of the correspond-ing files.
Thus, Chinese texts had an average bytelength of 1984 bytes (and a standard deviation of1366 bytes).
The ratio of average words in non-Chinese texts to average bytes in Chinese textsshows that on average one may (simplisticly) ex-pect that 6 bytes of Chinese text are adequate toexpress one word from a European language.We note that the measurement of Chinese textlength in words proved a very difficult endeavour.In the future we plan to use specialized Chinesetokenizers, which have an adequately high perfor-mance that will allowmeasuring text and summarylengths in words more accurately.2.3 Evaluation MethodologyThe evaluation of results was perfromed bothautomatically and manually.
The manual evalu-ation was based on the Overall Responsiveness(Dang and Owczarzak, 2008) of a text.
For themanual evaluation the human evaluators were pro-vided the following guidelines:Each summary is to be assigned aninteger grade from 1 to 5, related to theoverall responsiveness of the summary.We consider a text to be worth a 5, if21it appears to cover all the important as-pects of the corresponding document setusing fluent, readable language.
A textshould be assigned a 1, if it is either un-readable, nonsensical, or contains onlytrivial information from the documentset.
We consider the content and thequality of the language to be equally im-portant in the grading.The automatic evaluation was based on human,model summaries provided by fluent speakers ofeach corresponding language (native speakers inthe general case).
ROUGE variations (ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-4) (Lin, 2004)and the AutoSummENG-MeMoG (Giannakopou-los et al 2008; Giannakopoulos and Karkalet-sis, 2011) and NPowER (Giannakopoulos andKarkaletsis, 2013) methods were used to automat-ically evaluate the summarization systems.
Withinthis paper we provide results based on ROUGE-2and MeMoG methods.2.4 Participation and Overview of ResultsThis section provides a per-language overviewof participation and of the evaluation results.For an overview of participation information seeTable 1.
In the table, one can find the mappingbetween participant teams and IDs, as well as perlanguage information.
An asterisk in a cell indi-cates systems of co-organizers for the specific lan-guage.
These systems had early access to the cor-pus for their language and, thus, had an advantageover others on that specific language.Moreover, for the MultiLing pilot we createdtwo systems, one acting as a global baseline (Sys-tem ID6) and the other as a global topline (SystemID61).
These two systems are described briefly inthe following paragraphs.2.5 Baseline/Topline SystemsThe two systems devised as pointers of a stan-dard, simplistic approach and of an approach tak-ing into account human summaries were imple-mented as follows.The global baseline system?ID6?
representsthe documents of a topic in vector space using abag-of-words approach.
Then it determines thecentroidC of the document set in that space.
Giventhe centroid, the system gets the text T that is mostsimilar to the centroid (based on the cosine simi-larity) and uses it in the summary.
If the text ex-ceeds the summary word limit, then only a part ofit is used to provide the summary.
Otherwise, thewhole text is added as summary text.
If the sum-mary is below the lower word limit, the process isrepeated iteratively adding the next most similardocument to the centroid.The global topline system ?
ID61 ?
uses the(human) model summaries as a given (thus cheat-ing).
These documents are represented in the vec-tor space similarly to the global baseline.
Then,an algorithm produces random summaries by com-bining sentences from the original texts.
The sum-maries are evaluated by their cosine similarity tothe centroid of the model summaries.We use the centroid score as a fitness measurein a genetic algorithm process.
The genetic algo-rithm fitness function also penalizes summaries ofout-of-limit length.
Thus, what we do is that wesearch, using a genetic algorithm process, throughthe space of possible summaries, to produce onethat mostly matches (an average representation of)the model summaries.
Of course, using an in-termediate, centroid representation, loses part ofthe information in the original text.
Through thismethod we want to see how well we can createsummaries by knowing a priori what (on average)must be included.Unfortunately, the sentence splitting module ofthe topline, based on the Apache OpenNLP li-brary1 statistical sentence splitted failed due to abug in our code.
This resulted in an interestingphenomenon: the system would maximize sim-ilarity to the centroid, using fragments of sen-tences.
This is actually an excellent way to ex-amine what types of text can cheat n-gram basedmethods that they are good, while remaining just-not-good-enough from a human perspective.
In thesystem performance analysis sections we will seethat this expectation holds.In the Tables of the following sectionwe provideMeMoG and Overall Responsiveness (OR) statis-tics per system and language.
We also provide in-formation on statistically significant performancedifferences (based on Tukey HSD tests).2.6 Language-specific TablesThe tables below illustrate the system per-formances per language.
Each table containsthree columns: ?Group?, ?SysID?
and ?Avg Perf?.The Group column indicates to which statistically1See http://opennlp.apache.org/.22Participant Run IDs Arabic Chinese Czech English French Greek Hebrew Hindi Romanian SpanishMaryland ID1, ID11, ID21 ?
?
?
?
?
?
?
?
?
?CIST ID2 ?
?
?
?
?
?
?
?
?
?
?Lancaster ID3 ?
?
?WBU ID4 ?
?
??
?
?
?
?
?
?
?Shamoon ID5, ID51 ?
?
?
?Baseline ID6 Centroid baseline for all languagesTopline ID61 Using model summaries for all languagesTable 1: Participation per language.
An asterisk indicates a contributor system, with early access tocorpus data.Group SysID Avg Perfa ID61 0.2488ab ID4 0.2235abc ID1 0.2190abc ID11 0.2054abc ID21 0.1875abc ID2 0.1587abc ID5 0.1520bc ID51 0.1450bc ID6 0.1376c ID3 0.1230Table 2: Arabic: Tukey?s HSD test MeMoGgroupsequivalent groups of performance a system be-longs.
If two systems belong to the same group,they do not have statistically significant differ-ences in their performance (95% confidence levelof Tukey?s HSD test).
The SysID column indicatesthe system ID and the ?Avg Perf?
column the av-erage performance of the system in the given lan-guage.
The caption of each table indicates whatmeasure was used to grade performance.
In theOverall Responsiveness (OR) tables we also pro-vide the grades assigned to human summarizers.We note that for two of the languages ?
French,Hindi ?
there were no human evaluations thisyear, thus there are no OR tables for these lan-guages.
At the time of writing of this paper, therewere also no evaluations for human summaries forthe Hebrew and the Romanian languages.
Thesedata are planned to be included in an extendedtechnical report, whichwill bemade available afterthe workshop at the MultiLing Community web-site2, as an addenum to the proceedings.There are several notable findings in the tables:?
In several languages (e.g., Arabic, Spanish)there were systems (notable system ID4) that2See http://multiling.iit.demokritos.gr/pages/view/1256/proceedings-addenum)Group SysID Avg Perfa B 4.07ab C 3.93ab A 3.80ab ID6 3.71ab ID2 3.58ab ID3 3.58ab ID4 3.49ab ID1 3.47abc ID11 3.33bcd ID21 3.11cde ID51 2.78de ID5 2.71e ID61 2.49Table 3: Arabic: Tukey?s HSD test OR groupsGroup SysID Avg Perfa ID4 0.1019ab ID61 0.0927bc ID2 0.0589bc ID1 0.0540bc ID11 0.0537c ID21 0.0256c ID6 0.0200Table 4: Chinese: Tukey?s HSD test MeMoGgroupsGroup SysID Avg Perfa B 4.47a C 4.30a A 4.03b ID2 3.40c ID4 2.43c ID61 2.33c ID21 2.13c ID11 2.13c ID1 2.07d ID6 1.07Table 5: Chinese: Tukey?s HSD test OR groups23Group SysID Avg Perfa ID61 0.2500a ID4 0.2312ab ID11 0.2139ab ID21 0.2120ab ID1 0.2026b ID2 0.1565b ID6 0.1489Table 6: Czech: Tukey?s HSD testMeMoG groupsGroup SysID Avg Perfa B 4.75ab A 4.633ab C 4.613ab D 4.215b E 4.1c ID4 3.129d ID1 2.642d ID11 2.604de ID21 2.453e ID61 2.178e ID2 2.067f ID6 1.651Table 7: Czech: Tukey?s HSD test OR groupsGroup SysID Avg Perfa ID4 0.2220a ID11 0.2129a ID61 0.2103ab ID1 0.2085ab ID21 0.1903ab ID6 0.1798ab ID2 0.1751ab ID5 0.1728b ID3 0.1590b ID51 0.1588Table 8: English: Tukey?s HSD test MeMoGgroupsGroup SysID Avg Perfa A 4.5a C 4.467a B 4.25ab D 4.167ab ID4 3.547b ID11 3.013b ID6 2.776bc ID21 2.639bc ID51 2.571bc ID61 2.388bc ID5 2.245bc ID1 2.244bc ID3 2.208c ID2 1.893Table 9: English: Tukey?s HSD test OR groupsGroup SysID Avg Perfa ID4 0.2661ab ID61 0.2585ab ID1 0.2390ab ID11 0.2353ab ID21 0.2180ab ID6 0.1956b ID2 0.1844Table 10: French: Tukey?s HSD test MeMoGgroupsGroup SysID Avg Perfa ID61 0.2179ab ID11 0.1825ab ID1 0.1783ab ID21 0.1783ab ID4 0.1727b ID2 0.1521b ID6 0.1393Table 11: Greek: Tukey?s HSD test MeMoGgroups24Group SysID Avg Perfa A 3.889a ID4 3.833a B 3.792a C 3.792a D 3.583ab ID11 2.878ab ID6 2.795ab ID1 2.762ab ID21 2.744ab ID61 2.717b ID2 2.389Table 12: Greek: Tukey?s HSD test OR groupsGroup SysID Avg Perfa ID61 0.219ab ID11 0.1888ab ID4 0.1832ab ID21 0.1668ab ID51 0.1659ab ID1 0.1633ab ID5 0.1631b ID6 0.1411b ID2 0.1320Table 13: Hebrew: Tukey?s HSD test MeMoGgroupsGroup SysID Avg Perfa ID11 0.1490a ID4 0.1472a ID2 0.1421a ID21 0.1402a ID61 0.1401a ID1 0.1365a ID6 0.1208Table 14: Hindi: Tukey?s HSD test MeMoGgroupsGroup SysID Avg Perfa ID61 0.2308a ID4 0.2100a ID1 0.2096a ID21 0.1989a ID11 0.1959a ID6 0.1676a ID2 0.1629Table 15: Romanian: Tukey?s HSD test MeMoGgroupsGroup SysID Avg Perfa ID4 4.336ab ID6 4.033bc ID11 3.433c ID1 3.329c ID21 3.207c ID61 3.051c ID2 2.822Table 16: Romanian: Tukey?s HSD test OR groupsGroup SysID Avg Perfa ID4 0.2516a ID61 0.2491ab ID11 0.2399ab ID1 0.2261ab ID21 0.2083ab ID2 0.2075b ID6 0.187Table 17: Spanish: Tukey?s HSD test MeMoGgroupsGroup SysID Avg Perfa C 3.867a ID4 3.844a B 3.778ab A 3.667abc ID6 3.444bc ID2 3.067c ID11 3.022c ID1 2.978c ID21 2.956c ID61 2.844Table 18: Spanish: Tukey?s HSD test OR groups25reached human level performance.?
The centroid baseline performed very well inseveral cases (e.g., Spanish, Arabic), whilerather badly in others (e.g., Czech).?
The cheating topline system did indeed man-age to reveal a blind-spot of automatic evalu-ation, achieving high MeMoG grades, whileperforming badly in terms of OR grade.We note that detailed results related to the per-formances of the participants will be made avail-able via the MultiLing website3.3 Automatic Evaluation trackIn the next paragraphs we describe the task, thecorpus and the evaluation methodology related tothe automatic summary evaluation track of Multi-Ling 2013.3.1 The Evaluation TaskThis task aims to examine how well automatedsystems can evaluate summaries from differentlanguages.
This task takes as input the summariesgenerated from automatic systems and humans inthe Summarization Task.
The output should be agrading of the summaries.
Ideally, we would wantthe automatic evaluation to maximally correlate tohuman judgement.3.2 Evaluation CorpusBased on the Source Document Set, a numberof human summarizers and several automatic sys-tems submitted summaries for the different top-ics in different languages.
The human summarieswere considered model summaries and were pro-vided, together with the source texts and the auto-matic summaries, as input to summary evaluationsystems.
There were a total of 405 model sum-maries and 929 automatic summaries (one systemdid not submit summaries for all the topics).
Eachtopic in each language was mapped to 3 modelsummaries.The question posed in the multi-lingual con-text is whether an automatic measure is enoughto provide a ranking of systems.
In order to an-swer this question we used the ROUGE-2 score,as well as the ?n-gram graph?-based methods (Au-toSummENG, MeMoG, NPowER) to grade sum-maries.
We used ROUGE-2 because it has been ro-bust and highly used for several years in the DUC3See http://multiling.iit.demokritos.grand TAC communities.
There was only one ad-ditional participating measure for the evaluationtrack ?
namely the Coverage measure ?
in ad-dition to the above methods.In order to measure correlation we usedKendall?s Tau, to see whether grading with the au-tomatic or the manual grades would cause differ-ent rankings (and how different).
The results ofthe correlation per language are indicated in Ta-ble 19.
Unfortunately, the Hebrew evaluation datawere not fully available at the time of writing and,thus, they could not be used.
Please check the tech-nical report tha twill be available after the comple-tion of the Workshop for more information4.4 Summary and Future DirectionsOverall, the MultiLing 2013 multi-documentsummarization and summary evaluation tasksaimed to provide a scientifically acceptable bench-mark setting for summarization systems.
Buildingupon previous community effort we managed toachieve two main aims of the MultiLing Pilot of2011 (Giannakopoulos et al 2011): we managedto increase the number of languages included to 10and increase the number of topics per language.We should also note that the addition of Chinesetopics offered a fresh set of requirements, relatedto the differences of writing in this specific lan-guage from writing in the rest of the languages inthe corpus: not even tokenization is easy to transferto Chinese from other,e.g.
European languages.The main lessons learned from the multi-document and evaluation tracks were the follow-ing:?
multi-document summarization is an activedomain of research.?
current systems seem to performwell-enoughto provide more than basic, acceptable ser-vices to humans in a variety of languages.However, there still exist challenging lan-guages.?
there are languages where systems achievedhuman-grade performance.?
automatic evaluation of summaries in differ-ent languages in far from an easy task.
Muchmore effort must be put in this direction, tofacilitate summarization research.4See http://multiling.iit.demokritos.gr/pages/view/1256/proceedings-addenum)26Language R2 to OR MeMoG to OR Coverage to ORArabic -0.11 0.00 -0.07Chinese -0.38 0.46 0.41Czech 0.38 0.30 0.26English 0.22 0.24 0.26Greek 0.07 0.07 0.03Romanian 0.15 0.16 0.12Spanish 0.01 0.05 0.04All languages 0.12 0.18 0.14Table 19: Correlation (Kendall?s Tau) Between Gradings.
Note: statistically significant results, withp-value < 0.05, in bold.The main steps we plan to take, based also onthe future steps inherited from the MultiLing Pilotof 2011 are:?
to find the funds required for the evaluationprocess, in order to support the quality of theendeavour.?
to use the top performing evaluation systemas the main evaluationmeasure in futureMul-tiLing workshops.?
to create a piece of support software that willhelp implement and track all corpus genera-tion processes.?
to study the possibility of breaking down thesummarization process and asking systemsto make individual components available as(web) services to other systems.
This prac-tice aims to allow combinations of differentcomponents into new methods.?
to check the possibility of using the corpus forcross-language summarization.
We can eitherhave the task of generating a summary in adifferent language than the source documents,or/and use multi-language source documentson a single topic to provide a summary in onetarget language.?
to start a track aiming to measure the effec-tiveness of multi-lingual summarization as acommercial service to all the world.
Thistrack would need a common interface, hid-ing the underlying mechanics from the user.The user, in turn, will be requested to judge asummary based on its extrinsic value.
Muchconversation needs to be conducted in orderfor this task to provide a meaningful compar-ison between systems.
The aim of the trackwould be to illustrate the current applicabilityof multilingual multi-document summariza-tion systems in a real-world task, aiming atnon-expert users.Overall, the MultiLing effort enjoys the con-tribution of a flourishing research community onmulti-lingual summarization research.
We need tocontinue building on this contribution, inviting andchallenging more researchers to participate in thecommunity.
So far we have seen the MultiLing ef-fort grow from a pilot to a workshop, encompass-ing more and more languages and research groupsunder a common aim: providing a commonly ac-cepted benchmark setting for current and futuremulti-lingual summarization systems.ReferencesH.
T. Dang and K. Owczarzak.
2008.
Overview of theTAC 2008 update summarization task.
In TAC 2008Workshop - Notebook papers and results, pages 10?23, Maryland MD, USA, November.Hoa Trang Dang and K. Owczarzak.
2009.
Overviewof the tac 2009 summarization track, Nov.Michael Elhadad, Sabino Miranda-Jim?nez, JosefSteinberger, and George Giannakopoulos.
2013.Multi-document multilingual summarization corpuspreparation, part 2: Czech, hebrew and spanish.
InMultiLing 2013 Workshop in ACL 2013, Sofia, Bul-garia, August.George Giannakopoulos and Vangelis Karkaletsis.2011.
Autosummeng and memog in evaluatingguided summaries.
In TAC 2011 Workshop, Mary-land MD, USA, November.George Giannakopoulos and Vangelis Karkaletsis.2013.
Summary evaluation: Together we standnpower-ed.
In Computational Linguistics and Intel-ligent Text Processing, pages 436?450.
Springer.27George Giannakopoulos, Vangelis Karkaletsis, GeorgeVouros, and Panagiotis Stamatopoulos.
2008.
Sum-marization system evaluation revisited: N-gramgraphs.
ACM Trans.
Speech Lang.
Process., 5(3):1?39.G.
Giannakopoulos, M. El-Haj, B. Favre, M. Litvak,J.
Steinberger, and V. Varma.
2011.
TAC 2011MultiLing pilot overview.
In TAC 2011 Workshop,Maryland MD, USA, November.E.
Hovy, C. Y. Lin, L. Zhou, and J. Fukumoto.
2005.Basic elements.Lei Li, Corina Forascu, Mahmoud El-Haj, and GeorgeGiannakopoulos.
2013.
Multi-document multilin-gual summarization corpus preparation, part 1: Ara-bic, english, greek, chinese, romanian.
InMultiLing2013 Workshop in ACL 2013, Sofia, Bulgaria, Au-gust.C.
Y. Lin.
2004.
Rouge: A package for automatic eval-uation of summaries.
Proceedings of the Workshopon Text Summarization Branches Out (WAS 2004),pages 25?26.Annie Louis and Ani Nenkova.
2012.
Automaticallyassessing machine summary content without a goldstandard.
Computational Linguistics, 39(2):267?300, Aug.H.
Saggion, J. M. Torres-Moreno, I. Cunha, and E. San-Juan.
2010.
Multilingual summarization evalu-ation without human models.
In Proceedings ofthe 23rd International Conference on ComputationalLinguistics: Posters, page 1059?1067.28
