Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 305?308,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsPutting the User in the Loop: Interactive Maximal Marginal Relevance forQuery-Focused SummarizationJimmy Lin, Nitin Madnani, and Bonnie J. DorrUniversity of MarylandCollege Park, MD 20742, USAjimmylin@umd.edu, {nmadnani,bonnie}@umiacs.umd.eduAbstractThis work represents an initial attempt tomove beyond ?single-shot?
summarization tointeractive summarization.
We present an ex-tension to the classic Maximal Marginal Rel-evance (MMR) algorithm that places a user?in the loop?
to assist in candidate selec-tion.
Experiments in the complex interac-tive Question Answering (ciQA) task at TREC2007 show that interactively-constructed re-sponses are significantly higher in quality thanautomatically-generated ones.
This novel al-gorithm provides a starting point for futurework on interactive summarization.1 IntroductionDocument summarization, as captured in moderncomparative evaluations such as TAC and DUC, ismostly conceived as a ?one-shot?
task.
However, re-searchers have long known that information seekingis an iterative activity, which suggests that an inter-active approach might be worth exploring.This paper present a simple extension of a well-known algorithm, Maximal Marginal Relevance(MMR) (Goldstein et al, 2000), that places the userin the loop.
MMR is an iterative algorithm, whereat each step a candidate extract c (e.g., a sentence) isassigned the following score:?Rel(q, c)?
(1?
?
)maxs?SSim(s, c)The score consists of two components: the rele-vance of the candidate c with respect to the queryq (Rel) and the similarity of the candidate c to eachextract s in the current summary S (Sim).
The maxi-mum score from these similarity comparisons is sub-tracted from the relevance score, subjected to a tun-ing parameter that controls the emphasis on rele-vance and anti-redundancy.
Scores are recomputedafter each step and the algorithm iterates until a stop-ping criterion has been met (e.g., length quota).We propose a simple extension to MMR: at eachstep, we interactively ask the user to select the bestsentence for inclusion in the summary.
That is, in-stead of the system automatically selecting the can-didate with the highest score, it presents the userwith a ranked list of candidates for selection.2 Complex, Interactive QAOne obstacle to assessing the effectiveness of in-teractive summarization algorithms is the lack of asuitable evaluation vehicle.
Given the convergenceof complex QA and summarization (particularly thequery-focused variant) in recent years, we found anappropriate evaluation vehicle in the ciQA (com-plex, interactive Question Answering) task at TREC2007 (Dang et al, 2007).Information needs in the ciQA task, called top-ics, consist of two parts: the question template andthe narrative.
The question template is a stylized in-formation need that has a fixed structure and freeslots whose instantiation varies across different top-ics.
The narrative is unstructured prose that elabo-rates on the information need.
For the evaluation,NIST assessors developed 30 topics, grouped intofive templates.
See Figure 1 for an example.Participants in the task were able to deploy fully-functional web-based QA systems, with which the305Template: What evidence is there for transport of[drugs] from [Mexico] to [the U.S.]?Narrative: The analyst would like to know of effortsto curtail the transport of drugs from Mexico to theU.S.
Specifically, the analyst would like to know ofthe success of the efforts by local or international au-thorities.Figure 1: Example topic from the TREC 2007 ciQA task.NIST assessors interacted (serving as surrogates forusers).
Upon receiving the topics, participants firstsubmitted an initial run.
During a pre-arranged pe-riod of time shortly thereafter, each assessor wasgiven five minutes to interact with the participant?ssystem, live over the web.
After this interaction pe-riod, participants submitted a final run, which hadpresumably gained the benefit of user interaction.By comparing initial and final runs, it was possibleto quantify the effect of the interaction.The target corpus was AQUAINT-2, which con-sists of around 970k documents totaling 2.5 GB.System responses consisted of multi-line answersand were evaluated using the ?nugget?
methodol-ogy with the ?nugget pyramid?
extension (Lin andDemner-Fushman, 2006).3 Experiment DesignThis section describes our experiments for theTREC 2007 ciQA task.
In summary: the initial runwas generated automatically using standard MMR.The web-based interactions consisted of iterations ofinteractive MMR, where the user selected the bestcandidate extract at each step.
The final run con-sisted of the output of interactive MMR padded withautomatically-generated output.Sentence extracts were used as the basic re-sponse unit.
For each topic, the top 100 documentswere retrieved from the AQUAINT-2 collection withLucene, using the topic template verbatim as thequery.
Neither the template structure nor the narra-tive text were exploited.
All documents were thenbroken into individual sentences, which served asthe pool of candidates.
The relevance of each sen-tence was computed as the sum of the inverse doc-ument frequencies of matching terms from the topictemplate.
Redundancy was computed as the cosinesimilarity between the current answer (consisting ofFigure 2: Screenshot of the interface for interactiveMMR, which shows the current topic (A), the current an-swer (B), and a ranked list of document extracts (C).all previously-selected sentences) and the currentcandidate.
The relevance and redundancy scoreswere then normalized and combined (?
= 0.8).
Forthe initial run, the MMR algorithm iterated until 25candidates had been selected.For interactive MMR, a screenshot of the web-based system is shown in Figure 2.
The interfaceconsists of three elements: at the top (label A) is thecurrent topic; in the middle (label B) is the currentanswer, containing user selections from previous it-erations; the bottom area (label C) shows a rankedlist of candidate sentences ordered by MMR score.At each iteration, the user is asked to select one can-didate by clicking the ?Add to answer?
button nextto that candidate.
The selected candidate is thenadded to the current answer.
Ten answer candidatesare shown per page.
Clicking on a button labeled?Show more candidates?
at the bottom of the page(not shown in the screenshot) displays the next tencandidates.
In the ciQA 2007 evaluation, NIST as-sessors engaged with this interface for the entire al-lotted five minute interaction period.
Note that thissimple interface was designed only to assess the ef-fectiveness of interactive MMR, and not intended torepresent an actual interactive system.To prevent users from seeing the same sentencesrepeatedly once a candidate selection has beenrecorded, we divide the scores of all candidatesranked higher than the selected candidate by two (an3060 5 10 15 20 25 3035 4056  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85Number of Extracts TopicLength of ciQA 2007 Final Answers: Number of Extracts complete answerinteractive MMRmean, interactive MMRFigure 3: Per-topic lengths of final run in terms of num-ber of extracts.
Bars show contribution from interactiveMMR (darker) and ?padding?
(lighter).arbitrary constant).
For example, if the user clickedon candidate five, scores for candidates one throughfour are cut in half.
Previous studies have shownthat users generally examine ranked lists in order, sothe lack of a selection can be interpreted as negativefeedback (Joachims et al, 2007).The answers constructed interactively were sub-mitted to NIST as the final (post-interaction) run.However, since these answers were significantlyshorter than the initial run (given the short interac-tion period), the responses were ?padded?
by run-ning additional iterations of automatic MMR until alength quota of 4000 characters had been achieved.4 Results and DiscussionFirst, we present descriptive statistics of the finalrun submitted to NIST.
Lengths of the answers ona per-topic basis are shown in Figure 3 in terms ofnumber of extracts: darker bars show the number ofmanually-selected extracts for each topic during thefive-minute interaction period (i.e., the number of in-teractive MMR iterations).
The average across alltopics was 6.5 iterations, shown by the horizontalline; the average length of answers (all user selec-tions) was 1186 characters.
The average rank of theuser selection was 4.9, and the user selected the topranking sentence 28% of the time.
Note that the in-teraction period included system processing as wellas delays caused by network traffic.
The number ofextracts contained in the padding is shown by thelighter gray portions of the bars.
For topic 68, thesystem did not record any user interactions (possi-bly resulting from a network glitch).0 0.05 0.1 0.15 0.2 0.25 0.3 0.350.4 0.450  500  1000  1500  2000  2500  3000  3500  4000Weighted Recall Length of Answer (non-whitespace characters)TREC 2007 ciQA: interactive vs. non-interactive MMRnon-interactive MMRinteractive MMRsig., p<0.05Figure 4: Weighted recall at different length increments,comparing interactive and non-interactive MMR.The official metric for the ciQA task was F-measure, but a disadvantage of this single-point met-ric is that it doesn?t account for answers of vary-ing lengths.
An alternative proposed by Lin (2007)and used as the secondary metric in the evalua-tion is recall-by-length plots, which characterizeweighted nugget recall at varying length incre-ments.
Weighted recall captures how much rele-vant information is contained in the system response(weighted by each nugget?s importance, with an up-per bound of one).
Responses that achieve highernugget recall at shorter length increments are desir-able in providing concise, informative answers.Recall-by-length plots for both the initial run(non-interactive MMR) and final run (interactiveMMR with padding) are shown in Figure 4, in lengthincrements of 1000 characters.
The vertical dottedline denotes the average length of interactive MMRanswers (without padding).
Taking length as a proxyfor time, one natural interpretation of this plot is howquickly users are able to ?learn?
about the topic ofinterest under the two conditions.We see that interactive MMR yields higherweighted recall at all length increments.
TheWilcoxon signed-rank test was applied to assess thestatistical significance of the differences in weightedrecall at each length increment.
Solid circles in thegraph represent improvements that are statisticallysignificant (p < 0.05).
Furthermore, in the 700?1000 character range, weighted recall is significantlyhigher for interactive MMR at the 99% level.Viewing weighted recall as a proxy for answerquality, interactive MMR yields responses that aresignificantly better than non-interactive MMR at307a range of length increments.
This is an impor-tant finding, since effective interaction techniquesthat require little training and work well in limited-duration settings are quite elusive.
Often, user in-put actually makes answers worse.
Results fromboth ciQA 2006 and ciQA 2007 show that, overall,F-measure improved little between initial and finalruns.
Although it is widely accepted that user feed-back can enhance interactive IR, effective interac-tion techniques to exploit this feedback are by nomeans obvious.To better understand the characteristics of interac-tive MMR, it is helpful to compare our experimentswith the ciQA task-wide baseline.
As a referencefor all participants, the organizers of the task sub-mitted a pair of runs to help calibrate effectiveness.According to Dang et al (2007), the first run wasprepared by submitting the question template ver-batim as a query to Lucene to retrieve the top 20documents.
These documents were then tokenizedinto individual sentences.
Sentences that containedat least one non-stopword from the question were re-tained and returned as the initial run (up to a quotaof 5,000 characters).
Sentence order within eachdocument and across the ranked list was preserved.The interaction associated with this run asked the as-sessor for relevance judgments on each of the sen-tences.
Three options were given: ?relevant?, ?notrelevant?, and ?no opinion?.
The final run was pre-pared by removing sentences judged not relevant.Other evidence suggests that the task-wide sen-tence retrieval algorithm represents a strong base-line.
Similar algorithms performed well in othercomplex QA tasks?in TREC 2003, a sentence re-trieval variant beat all but one run on definition ques-tions (Voorhees, 2003).
The sentence retrieval base-line also performed well in ciQA 2006.The MMR runs are compared to the task-widereference runs in Figure 5: diamonds denote thesentence retrieval baseline and triangles mark themanual sentence selection final run.
The manualsentence selection run outperforms the sentence re-trieval baseline (as expected), but its weighted recallis still below that of interactive MMR across almostall length increments.
The weighted recall of inter-active MMR is significantly better at 1000 characters(at the 95% level), but nowhere else.
So, the bottomline is: for limited-duration interactions, interactive0 0.05 0.1 0.15 0.2 0.25 0.3 0.350.4 0.450  500  1000  1500  2000  2500  3000  3500  4000Weighted Recall Length of Answer (non-whitespace characters)TREC 2007 ciQA: MMR vs. task-wide baselinenon-interactive MMRinteractive MMRsentence retrieval baselinemanual sentence selectionFigure 5: Weighted recall at different length increments,comparing MMR with the task-wide baseline.MMR is more effective than simply asking for rele-vance judgments, but not significantly so.5 ConclusionWe present an interactive extension of the MaximalMarginal Relevance algorithm for query-focusedsummarization.
Results from the TREC 2007 ciQAtask demonstrate it is a simple yet effective tech-nique for involving users in interactively construct-ing responses to complex information needs.
Theseresults provide a starting point for future work in in-teractive summarization.AcknowledgmentsThis work was supported in part by NLM/NIH.
Thefirst author would like to thank Esther and Kiri fortheir loving support.ReferencesH.
Dang, J. Lin, and D. Kelly.
2007.
Overview of theTREC 2007 question answering track.
TREC 2007.J.
Goldstein, V. Mittal, J. Carbonell, and J. Callan.
2000.Creating and evaluating multi-document sentence ex-tract summaries.
CIKM 2000.T.
Joachims, L. Granka, B. Pan, H. Hembrooke,F.
Radlinski, and G. Gay.
2007.
Evaluating the ac-curacy of implicit feedback from clicks and query re-formulations in Web search.
TOIS, 25(2):1?27.J.
Lin and D. Demner-Fushman.
2006.
Will pyramidsbuilt of nuggets topple over?
HLT/NAACL 2006.J.
Lin.
2007.
Is question answering better than informa-tion retrieval?
Towards a task-based evaluation frame-work for question series.
HLT/NAACL 2007.E.
Voorhees.
2003.
Overview of the TREC 2003 ques-tion answering track.
TREC 2003.308
