Proceedings of the 7th Workshop on Statistical Machine Translation, pages 157?162,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsNon-Linear Models for Confidence EstimationYong Zhuang?Zhejiang University866 Yuhangtang RoadHangzhou, Chinayong.zhuang22@gmail.comGuillaume Wisniewski and Franc?ois YvonUniv.
Paris Sud and LIMSI?CNRSrue John von Neumann91403 Orsay CEDEX, France{firstname.lastname}@limsi.frAbstractThis paper describes our work with the datadistributed for the WMT?12 Confidence Es-timation shared task.
Our contribution istwofold: i) we first present an analysis ofthe data which highlights the difficulty ofthe task and motivates our approach; ii) weshow that using non-linear models, namely ran-dom forests, with a simple and limited featureset, succeeds in modeling the complex deci-sions required to assess translation quality andachieves results that are on a par with the sec-ond best results of the shared task.1 IntroductionConfidence estimation is the task of predicting thequality of a system prediction without knowledgeof the expected output.
It is an important stepin many Natural Language Processing applications(Gandrabur et al, 2006).
In Machine Translation(MT), this task has recently gained interest (Blatzet al, 2004; Specia et al, 2010b; Soricut and Echi-habi, 2010; Bach et al, 2011).
Indeed, professionaltranslators are more and more requested to post-editthe outputs of a MT system rather than to producea translation from scratch.
Knowing in advance thesegments they should focus on would be very help-ful (Specia et al, 2010a).
Confidence estimation isalso of great interest for developers of MT system, asit provides them with a way to analyze the systemsoutput and to better understand the main causes oferrors.Even if several studies have tackled the problemof confidence estimation in machine translation, un-til now, very few datasets were publicly available andcomparing the proposed methods was difficult, if notimpossible.
To address this issue, WMT?12 orga-nizers proposed a shared task aiming at predict the?This work was conducted during an internship at LIMSI?CNRSquality of a translation and provided the associateddatasets, baselines and metrics.This paper describes our work with the data of theWMT?12 Confidence Estimation shared task.
Ourcontribution is twofold: i) we first present an analysisof the provided data that will stress the difficulty ofthe task and motivate the choice of our approach; ii)we show how using non-linear models, namely ran-dom forests, with a simple and limited features setsucceed in modeling the complex decisions requireto assess translation quality and achieve the secondbest results of the shared task.The rest of this paper is organized as follows: Sec-tion 2 summarizes our analysis of the data; in Sec-tion 3, we describe our learning method; our mainresults are finally reported in Section 4.2 Data AnalysisIn this section, we quickly analyze the data dis-tributed in the context of the WMT?12 ConfidenceEstimation Shared Task in order to evaluate the diffi-culty of the task and to find out what predictors shallbe used.
We will first describe the datasets, then thefeatures usually considered in confidence estimationtasks and finally summarize our analyses.2.1 DatasetsThe datasets used in our experiments were releasedfor the WMT?12 Quality Estimation Task.
All thedata provided in this shared task are based on thetest set of WMT?09 and WMT?10 translation tasks.The training set is made of 1, 832 English sen-tences and their Spanish translations as computed bya standard Moses system.
Each sentence pair is ac-companied by an estimate of its translation quality.This score is the average of ordinal grades assignedby three human evaluators.
The human grades are inthe range 1 to 5, the latter standing for a very goodtranslation that hardly requires post-editing, whilethe former stands for a bad translation that does157not deserve to be edited, meaning that the machineoutput useless and that translation should better beproduced from scratch.
The test contains 422 sen-tence pairs, the quality of which has to be predicted.The training set alo contains additional material,namely two references (the reference originally givenby WMT and a human post-edited one), which willallow us to better interpret our results.
No referenceswere provided for the test set.2.2 FeaturesSeveral works have studied the problem of confidenceestimation (Blatz et al, 2004; Specia et al, 2010b) orrelated problems such as predicting readability (Ka-nungo and Orr, 2009) or developing automated essayscoring systems (Burstein et al, 1998).
They all usethe same basic features:IBM 1 score measures the quality of the ?associa-tion?
of the source and the target sentence usingbag-of-word translation models;Language model score accounts for the ?flu-ency?, ?grammaticality?
and ?plausibility?
of atarget sentence;Simple surface features like the sentence length,the number of out-of-vocabulary words or wordsthat are not aligned.
These features are used toaccount for the difficulty of the translation task.More elaborated features, derived, for instance,from parse trees or dependencies analysis have alsobeen used in past studies.
However they are far moreexpensive to compute and rely on the existence of ex-ternal resources, which may be problematic for somelanguages.
That is why we only considered a re-stricted number of basic features in this work1.
An-other reason for considering such a small set of fea-tures is the relatively small size of the training set: inour preliminary experiments, considering more fea-tures, especially lexicalized features that would be ofgreat interest for failure analysis, always resulted inoverfitting.2.3 Data AnalysisThe distribution of the human scores on the trainingset is displayed in Figure 1.
Surprisingly enough,the baseline translation system used to generate thedata seems to be pretty good: 73% of the sentenceshave a score higher than 3 on a 1 to 5 scale.
Italso appears that most scores are very close: morethan half of them are located around the mean.
Asa consequence, it seems that distinguishing betweenthem will require to model subtle nuances.1The complete list of features is given in Appendix A.Figure 1: Distribution of the human scores on the trainset.
(HS?
stands for Human Scores)Figure 2 plots the distribution of quality scoresas a function of the Spanish-to-English IBM 1 scoreand of the probability of the target sentence.
Thesetwo scores were computed with the same models thatwere used to train the MT systems that have gener-ated the training data.
It appears that even if theexamples are clustered by their quality, these clustersoverlap and the frontiers between them are fuzzy andcomplex.
Similar observations were made for othersfeatures.Figure 2: Quality scores as a function of the Spanish-to-English IBM 1 score and of the probability of the targetsentence (HS?
stands for Human Scores)These observations prove that a predictor of thetranslation quality has to capture complex interac-tion patterns in the training data.
Standard resultsfrom machine learning show that such structures canbe described either by a linear model using a largenumber of features or by a non-linear model using a158(potentially) smaller set of features.
As only a smallnumber of training examples is available, we decidedto focus on non-linear models in this work.3 Inferring quality scoresPredicting the quality scores can naturally be castas a standard regression task, as the reference scoresused in the evaluation are numerical (real) values.Regression is the approach adopted in most workson confidence estimation for MT (Albrecht and Hwa,2007; Specia et al, 2010b).
A simpler way to tacklethe problem would be to recast it as binary classi-fication task aiming at distinguishing ?good?
trans-lations from ?bad?
ones (Blatz et al, 2004; Quirk,2004).
It is also possible, as shown by (Soricut andEchihabi, 2010), to use ranking approaches.
How-ever, because the shared task is evaluated by com-paring the actual value of the predictions with thehuman scores, using these last two frameworks is notpossible.In our experiments, following the observations re-ported in the previous section, we use two well-known non-linear regression methods: polynomialregression and random forests.
We also consider lin-ear regression as a baseline.
We will now quicklydescribe these three methods.Linear regression (Hastie et al, 2003) is a simplemodel in which the prediction is defined by a linearcombination of the feature vector x: y?
= ?0 + x>?,where ?0 and ?
are the parameters to estimate.These parameters are usually learned by minimiz-ing the sum of squared deviations on the trainingset, which is an easy optimization problem with aclose-form solution.Polynomial regression (Hastie et al, 2003) is astraightforward generalization of linear regression inwhich the relationship between the features and thelabel is modeled as a n-th order polynomial.
By care-fully extending the feature vector, the model can bereduced to a linear regression model and trained inthe same way.Random forest regressor (Breiman, 2001) is an en-semble method that learns many regression trees andpredicts an aggregation of their result.
In contrastwith standard decision tree, in which each node issplit using the best split among all features, in a ran-dom forest the split is chosen randomly.
In spite ofthis simple and counter-intuitive learning strategy,random forests have proven to be very good ?out-of-the-box?
learners and have achieved state-of-the-art performance in many tasks, demonstrating boththeir robustness to overfitting and their ability totake into account complex interactions between fea-tures.In our experiments, we use the implementationprovided by scikit-learn (Pedregosa et al, 2011).Hyper-parameters of the random forest (the num-ber of trees and the stopping criterion) were chosenby 10-fold cross-validation.4 Experimental Setting4.1 FeaturesIn all our experiments, we considered a simple de-scription of the translation hypotheses relying on31 features.
The complete list of features is givenin Appendix A.
All these features have already beenused in works related to ours and are simple fea-tures that can be easily computed using only a lim-ited number of external resources.A key finding in our preliminary experiments isthe need to re-scale the features by dividing theirvalue by the length of the corresponding sentence(e.g.
the language model score of a source sentencewill be divided by its length of the source sentence,and the one of a target sentence will be done by itslength of the target sentence).
This rescaling makesfeatures that depend on the sentence length (like theLM score) comparable and results in a large improve-ment of the performance of the associated feature.4.2 MetricsThe two metrics used to evaluate prediction perfor-mance are the standard metrics for regression: MeanAbsolute Error (MAE) and Root Mean Squared Er-ror (RMSE) defined by:MAE =1nn?i=1|y?i ?
yi|RMSE =????
1nn?i=1(y?i ?
yi)2where n is the number of examples, yi and y?i the truelabel and predicted label of the ith example.
MAEcan be understood as the averaged error made inpredicting the quality of a translation.
As it is easyto interpret, we will use it to analyze our results.RMSE scores are reported to facilitate comparisonwith other submissions to the shared task.All the reported scores have been computed usingthe tools provided by the Quality Estimation taskorganizers2.2https://github.com/lspecia/QualityEstimation1594.3 ResultsTable 1 details the results achieved by the differentmethods introduced in the previous section.
All ofthem achieve similar performances: their MAE is be-tween 0.64 and 0.66, which is a pretty good result asthe best reported MAE in the shared task is 0.61.Our best model is the second-best when submissionsare ranked according to their MAE.Even if their results are very close (significance ofthe score differences will be investigated in the fol-lowing subsection), all non-linear models outperforma simple linear regression, which corroborates the ob-servations made in Section 2.For the polynomial regression, we tried differentpolynomial orders in order to achieve an optimalsetting.
Even if this method achieves the best re-sults when the model is selected on the test set, it isnot usable in practice: when we tried to select thepolynomial degree by cross-validation, the regressorssystematically overfitted due to the reduction of thenumber of examples.
That is why random forests,which do not suffer from overfitting and can learngood predictor even when features outnumber exam-ples, is our method of choice.4.4 InterpretationTo get a better understanding of the task difficultyand to make interpretation of the error rate easier,we train another regressor using an ?oracle?
feature:the hTER score.
It is clear that this feature can onlybe computed on the training set and that consideringit does not make much sense in a ?real-life?
scenario.However, this feature is supposed to be highly rele-vant to the quality prediction task and should there-fore result in a ?large?
reduction of the error rates.Quantifying what ?large?
means in this context willallow us to analyze the results presented in Table 1.Training a random forest with this additional fea-ture on 1, 400 examples of the train set chosen ran-domly reduces the MAE evaluated on the 432 re-maining examples by 0.10 and the RMSE by 0.12.This small reduction stresses how difficult the taskis.
Comparatively, the 0.02 reduction achieved byreplacing a linear model with a non-linear modelshould therefore be considered noteworthy.
Furtherinvestigations are required to find out whether thedifficulty of the task results from the way humanscores are collected (low inter-annotators agreement,bias in the gathering of the collection, ...) or fromthe impossibility to solve the task using only surfacefeatures.Another important question in the analysis of ourresults concerns the usability of our approach: anerror of 0.6 seems large on a 1 to 5 scale and mayquestion the interest of our approach.
To allow a fine-grained analysis, we report the correlation betweenthe predicted score and the human score (Figure 3)and the distribution of the absolute error (Figure 4).These figures show that the actual error is often quitesmall: for more than 45% of the examples, the erroris smaller than 0.5 and for 23% it is smaller than 0.2.Figure 3 also shows that the correlation between ourpredictions and the true labels is ?substantial?
ac-cording to the established guidelines of (Landis andKoch, 1977) (the Pearson correlation coefficient isgreater than 0.6).
The difference between the meanof the two distributions is however quite large.
Cen-tering the predictions on the mean of the true labelmay improves the MAE.
This observation also sug-gests that we should try to design evaluation metricsthat do not rely on the actual predicted values.Figure 3: Correlation between our predictions and thetrue label (HS?
stands for Human Scores)5 ConclusionIn this work, we have presented, a simple, yet effi-cient, method to predict the quality of a translation.Using simple features and a non-linear model, ourapproach has achieved results close to the best sub-mission to the Confidence Estimation shared task,which supports the results of our analysis of the data.In our future work, we aim at considering more fea-tures, avoiding overfitting thanks to features selec-tion methods.Even if a fine-grained analysis of our results showsthe interest and usefulness of our approach, more re-mains to be done to develop reliable confidence esti-mation methods.
Our results also highlight the needto continue gathering high-quality resources to trainand investigate confidence estimation systems: evenwhen considering only very few features, our systems160Train TestMethods parameters MAE RMSE MAE RMSElinear regression ?
0.58 0.71 0.66 0.82polynomial regressionn=2 0.55 0.68 0.64 0.79n=3 0.54 0.67 0.64 0.79n=4 0.54 0.67 0.65 0.85random forest cross-validated 0.39 0.46 0.64 0.80Table 1: Prediction performance achieved by different regressorsFigure 4: Distribution of the absolute error (|yi ?
y?i|) ofour predictionswere prone to overfitting.
Developing more elabo-rated systems will therefore only be possible if moretraining resource is available.AcknowledgmentThe authors would like to thank Nicolas Usunier forhelpful discussions about ranking and regression us-ing random forest.
This work was partially funded bythe French National Research Agency under projectANR-CONTINT-TRACE.ReferencesJoshua Albrecht and Rebecca Hwa.
2007.
Regression forsentence-level mt evaluation with pseudo references.
InProceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 296?303,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Nguyen Bach, Fei Huang, and Yaser Al-Onaizan.
2011.Goodness: a method for measuring machine transla-tion confidence.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies - Volume 1, HLT?11, pages 211?219, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.John Blatz, Erin Fitzgerald, George Foster, Simona Gan-drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,and Nicola Ueffing.
2004.
Confidence estimation formachine translation.
In Proceedings of the 20th in-ternational conference on Computational Linguistics,COLING ?04, Stroudsburg, PA, USA.
Association forComputational Linguistics.Leo Breiman.
2001.
Random forests.
Mach.
Learn.,45(1):5?32, October.Jill Burstein, Karen Kukich, Susanne Wolff, Chi Lu, Mar-tin Chodorow, Lisa Braden-Harder, and Mary DeeHarris.
1998.
Automated scoring using a hybrid fea-ture identification technique.
In Proceedings of the17th international conference on Computational lin-guistics - Volume 1, COLING ?98, pages 206?210,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Simona Gandrabur, George Foster, and Guy Lapalme.2006.
Confidence estimation for nlp applications.ACM Trans.
Speech Lang.
Process., 3(3):1?29, Octo-ber.T.
Hastie, R. Tibshirani, and J. H. Friedman.
2003.
TheElements of Statistical Learning.
Springer, July.Tapas Kanungo and David Orr.
2009.
Predicting thereadability of short web summaries.
In Proceedingsof the Second ACM International Conference on WebSearch and Data Mining, WSDM ?09, pages 202?211,New York, NY, USA.
ACM.R.
J. Landis and G. G. Koch.
1977.
The measurement ofobserver agreement for categorical data.
Biometrics,33(1):159?174.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos,D.
Cournapeau, M. Brucher, M. Perrot, and E. Duch-esnay.
2011.
Scikit-learn: Machine Learning inPython .
Journal of Machine Learning Research,12:2825?2830.Chris Quirk.
2004.
Training a sentence-level machinetranslation confidence metric.
In Proceedings of the4th International Conference on Language Resourcesand Evaluation (LREC), pages 825?828.Radu Soricut and Abdessamad Echihabi.
2010.Trustrank: Inducing trust in automatic translationsvia ranking.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguistics,pages 612?621, Uppsala, Sweden, July.
Association forComputational Linguistics.161Lucia Specia, Nicola Cancedda, and Marc Dymetman.2010a.
A dataset for assessing machine translationevaluation metrics.
In 7th Conference on Interna-tional Language Resources and Evaluation (LREC-2010), pages 3375?3378, Valletta, Malta.Lucia Specia, Dhwaj Raj, and Marco Turchi.
2010b.
Ma-chine translation evaluation versus quality estimation.Machine Translation, 24(1):39?50, March.A Features ListHere is the whole list of the 31 features we used inour experiments (?
has been used in the baseline ofthe shared task organizer):?
?
Number of tokens in the source sentence?
?
Number of tokens in the target sentence?
?
Average token length in source sentence?
English-Spanish IBM 1 scores?
Spanish-English IBM 1 scores?
English-Spanish IBM 1 scores divided by thelength of source sentence?
English-Spanish IBM 1 scores divided by thelength of target sentence?
Spanish-English IBM 1 scores divided by thelength of source sentence?
Spanish-English IBM 1 scores divided by thelength of target sentence?
Number of out-of-vocabulary in source sentence?
Number of out-of-vocabulary in target sentence?
Out-of-vocabulary rates in source sentence?
Out-of-vocabulary rates in target sentence?
log10(LM probability of source sentence)?
log10(LM probability of target sentence)?
log10(LM probability of source sentence) dividedby the length of source sentence?
log10(LM probability of target sentence) dividedby the length of target sentence?
Ratio of functions words in source sentence?
Ratio of functions words in target sentence?
?
Number of occurrences of the target wordwithin the target hypothesis (averaged for allwords in the hypothesis - type/token ratio)?
?
Average number of translations per sourceword in the sentence (as given by IBM 1 tablethresholded so that prob(t|s) > 0.2)?
?
Average number of translations per sourceword in the sentence (as given by IBM 1 tablethresholded so that prob(t|s) > 0.01) weightedby the inverse frequency of each word in thesource corpus?
?
Percentage of unigrams in quartile 1 of fre-quency (lower frequency words) in a corpus ofthe source language (SMT training corpus)?
?
Percentage of unigrams in quartile 4 of fre-quency (higher frequency words) in a corpus ofthe source sentence?
?
Percentage of bigrams in quartile 1 of fre-quency of source words in a corpus of the sourcelanguage?
?
Percentage of bigrams in quartile 4 of fre-quency of source words in a corpus of the sourcelanguage?
?
Percentage of trigrams in quartile 1 of fre-quency of source words in a corpus of the sourcelanguage?
?
Percentage of trigrams in quartile 4 of fre-quency of source words in a corpus of the sourcelanguage?
?
Percentage of unigrams in the source sentenceseen in a corpus (SMT training corpus)?
?
Number of punctuation marks in the sourcesentence?
?
Number of punctuation marks in the targetsentence162
