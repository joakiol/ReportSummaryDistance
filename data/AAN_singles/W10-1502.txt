Proceedings of the NAACL HLT 2010 Sixth Web as Corpus Workshop, pages 8?16,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsBuilding a Korean Web Corpus for Analyzing Learner LanguageMarkus DickinsonIndiana Universitymd7@indiana.eduRoss IsraelIndiana Universityraisrael@indiana.eduSun-Hee LeeWellesley Collegeslee6@wellesley.eduAbstractPost-positional particles are a significantsource of errors for learners of Korean.
Fol-lowing methodology that has proven effectivein handling English preposition errors, we arebeginning the process of building a machinelearner for particle error detection in L2 Ko-rean writing.
As a first step, however, we mustacquire data, and thus we present a method-ology for constructing large-scale corpora ofKorean from the Web, exploring the feasibil-ity of building corpora appropriate for a giventopic and grammatical construction.1 IntroductionApplications for assisting second language learnerscan be extremely useful when they make learnersmore aware of the non-native characteristics in theirwriting (Amaral and Meurers, 2006).
Certain con-structions, such as English prepositions, are difficultto characterize by grammar rules and thus are well-suited for machine learning approaches (Tetreaultand Chodorow, 2008; De Felice and Pulman, 2008).Machine learning techniques are relatively portableto new languages, but new languages bring issues interms of defining the language learning problem andin terms of acquiring appropriate data for training amachine learner.We focus in this paper mainly on acquiring datafor training a machine learning system.
In partic-ular, we are interested in situations where the taskis constant?e.g., detecting grammatical errors inparticles?but the domain might fluctuate.
This isthe case when a learner is asked to write an essay ona prompt (e.g., ?What do you hope to do in life??
),and the prompts may vary by student, by semester,by instructor, etc.
By isolating a particular domain,we can hope for greater degrees of accuracy; see,for example, the high accuracies for domain-specificgrammar correction in Lee and Seneff (2006).In this situation, we face the challenge of obtain-ing data which is appropriate both for: a) the topicthe learners are writing about, and b) the linguisticconstruction of interest, i.e., containing enough rel-evant instances.
In the ideal case, one could builda corpus directly for the types of learner data toanalyze.
Luckily, using the web as a data sourcecan provide such specialized corpora (Baroni andBernardini, 2004), in addition to larger, more gen-eral corpora (Sharoff, 2006).
A crucial question,though, is how one goes about designing the rightweb corpus for analyzing learner language (see, e.g.,Sharoff, 2006, for other contexts)The area of difficulty for language learners whichwe focus on is that of Korean post-positional parti-cles, akin to English prepositions (Lee et al, 2009;Ko et al, 2004).
Korean is an important languageto develop NLP techniques for (see, e.g., discussionin Dickinson et al, 2008), presenting a variety offeatures which are less prevalent in many Westernlanguages, such as agglutinative morphology, a richsystem of case marking, and relatively free word or-der.
Obtaining data is important in the general case,as non-English languages tend to lack resources.The correct usage of Korean particles relies onknowing lexical, syntactic, semantic, and discourseinformation (Lee et al, 2005), which makes thischallenging for both learners and machines (cf.
En-8glish determiners in Han et al, 2006).
The onlyother approach we know of, a parser-based one, hadvery low precision (Dickinson and Lee, 2009).
Asecondary contribution of this work is thus defin-ing the particle error detection problem for a ma-chine learner.
It is important that the data representthe relationships between specific lexical items: inthe comparable English case, for example, interestis usually found with in: interest in/*with learning.The basic framework we employ is to train a ma-chine learner on correct Korean data and then applythis system to learner text, to predict correct parti-cle usage, which may differ from the learner?s (cf.Tetreault and Chodorow, 2008).
After describing thegrammatical properties of particles in section 2, weturn to the general approach for obtaining relevantweb data in section 3, reporting basic statistics forour corpora in section 4.
We outline the machinelearing set-up in section 5 and present initial resultsin section 6.
These results help evaluate the best wayto build specialized corpora for learner language.2 Korean particlesSimilar to English prepositions, Korean postposi-tional particles add specific meanings or grammat-ical functions to nominals.
However, a particle can-not stand alone in Korean and needs to be attachedto the preceding nominal.
More importantly, par-ticles indicate a wide range of linguistic functions,specifying grammatical functions, e.g., subject andobject; semantic roles; and discourse functions.
In(1), for instance, ka marks both the subject (func-tion) and agent (semantic role), eykey the dative andbeneficiary; and so forth.1(1) Sumi-kaSumi-SBJJohn-eykeyJohn-tochayk-ulbook-OBJilhke-yoread-polite?Sumi reads a book to John.
?Particles can also combine with nominals to formmodifiers, adding meanings of time, location, instru-ment, possession, and so forth, as shown in (2).
Notein this case that the marker ul/lul has multiple uses.21We use the Yale Romanization scheme for writing Korean.2Ul/lul, un/nun, etc.
only differ phonologically.
(2) Sumi-kaSumi-SBJJohn-uyJohn-GENcip-eysehouse-LOCku-lulhe-OBJtwutwosikan-ulhours-OBJkitaly-ess-ta.wait-PAST-END?Sumi waited for John for (the whole) two hours inhis house.
?There are also particles associated with discoursemeanings.
For example, in (3) the topic marker nunis used to indicate old information or a discourse-salient entity, while the delimiter to implies thatthere is someone else Sumi likes.
In this paper, wefocus on syntactic/semantic particle usage for nom-inals, planning to extend to other cases in the future.
(3) Sumi-nunSumi-TOPJohn-toJohn-alsocohahay.like?Sumi likes John also.
?Due to these complex linguistic properties, parti-cles are one of the most difficult topics for Koreanlanguage learners.
In (4b), for instance, a learnermight replace a subject particle (as in (4a)) with anobject (Dickinson et al, 2008).
Ko et al (2004) re-port that particle errors were the second most fre-quent error in a study across different levels of Ko-rean learners, and errors persist across levels (seealso Lee et al, 2009).
(4) a. Sumi-nunSumi-TOPchayk-ibook-SBJphilyohay-yoneed-polite?Sumi needs a book.?b.
*Sumi-nunSumi-TOPchayk-ulbook-OBJphilyohay-yoneed-polite?Sumi needs a book.
?3 Approach3.1 Acquiring training dataDue to the lexical relationships involved, machinelearning has proven to be a good method for sim-ilar NLP problems like detecting errors in En-glish preposition use.
For example Tetreault andChodorow (2008) use a maximum entropy classifierto build a model of correct preposition usage, with7 million instances in their training set, and Lee andKnutsson (2008) use memory-based learning, with10 million sentences in their training set.
In expand-ing the paradigm to other languages, one problem9is a dearth of data.
It seems like a large data set isessential for moving forward.For Korean, there are at least two corpora pub-licly available right now, the Penn Korean Treebank(Han et al, 2002), with hundreds of thousands ofwords, and the Sejong Corpus (a.k.a., The KoreanNational Corpus, The National Institute of KoreanLanguage, 2007), with tens of millions of words.While we plan to include the Sejong corpus in fu-ture data, there are several reasons we pursue a dif-ferent tack here.
First, not every language has suchresources, and we want to work towards a language-independent platform of data acquisition.
Secondly,these corpora may not be a good model for the kindsof topics learners write about.
For example, newstexts are typically written more formally than learnerwriting.
We want to explore ways to quickly buildtopic-specific corpora, and Web as Corpus (WaC)technology gives us tools to do this.33.2 Web as CorpusTo build web corpora, we use BootCat (Baroni andBernardini, 2004).
The process is an iterative algo-rithm to bootstrap corpora, starting with various seedterms.
The procedure is as follows:1.
Select initial seeds (terms).2.
Combine seeds randomly.3.
Run Google/Yahoo queries.4.
Retrieve corpus.5.
Extract new seeds via corpus comparison.6.
Repeat steps #2-#5.For non-ASCII languages, one needs to checkthe encoding of webpages in order to convert thetext into UTF-8 for output, as has been done for,e.g., Japanese (e.g., Erjavec et al, 2008; Baroni andUeyama, 2004).
Using a UTF-8 version of Boot-Cat, we modified the system by using a simple Perlmodule (Encode::Guess) to look for the EUC-KR encoding of most Korean webpages and switchit to UTF-8.
The pages already in UTF-8 do not needto be changed.3.3 Obtaining dataA crucial first step in constructing a web corpus isthe selection of appropriate seed terms for construct-ing the corpus (e.g., Sharoff, 2006; Ueyama, 2006).3Tetreault and Chodorow (2009) use the web to derivelearner errors; our work, however, tries to obtain correct data.In our particular case, this begins the question ofhow one builds a corpus which models native Ko-rean and which provides appropriate data for the taskof particle error detection.
The data should be genre-appropriate and contain enough instances of the par-ticles learners know and used in ways they are ex-pected to use them (e.g., as temporal modifiers).
Alarge corpus will likely satisfy these criteria, but hasthe potential to contain distracting information.
InKorean, for example, less formal writing often omitsparticles, thereby biasing a machine learner towardsunder-guessing particles.
Likewise, a topic with dif-ferent typical arguments than the one in questionmay mislead the machine.
We compare the effec-tiveness of corpora built in different ways in traininga machine learner.3.3.1 A general corpusTo construct a general corpus, we identify wordslikely to be in a learner?s lexicon, using a list of 50nouns for beginning Korean students for seeds.
Thisincludes basic vocabulary entries like the words formother, father, cat, dog, student, teacher, etc.3.3.2 A focused corpusSince we often know what domain4 learner es-says are written about, we experiment with buildinga more topic-appropriate corpus.
Accordingly, weselect a smaller set of 10 seed terms based on therange of topics covered in our test corpus (see sec-tion 6.1), shown in figure 1.
As a first trial, we selectterms that are, like the aforementioned general cor-pus seeds, level-appropriate for learners of Korean.han-kwuk ?Korea?
sa-lam ?person(s)?han-kwuk-e ?Korean (lg.)?
chin-kwu ?friend?kyey-cel ?season?
ga-jok ?family?hayng-pok ?happiness?
wun-tong ?exercise?ye-hayng ?travel?
mo-im ?gathering?Figure 1: Seed terms for the focused corpus3.3.3 A second focused corpusThere are several issues with the quality of datawe obtain from our focused terms.
From an ini-tial observation (see section 4.1), the difficulty stemsin part from the simplicity of the seed terms above,4By domain, we refer to the subject of a discourse.10leading to, for example, actual Korean learner data.To avoid some of this noise, we use a second set ofseed terms, representing relevant words in the samedomains, but of a more advanced nature, i.e., topic-appropriate words that may be outside of a typicallearner?s lexicon.
Our hypothesis is that this is morelikely to lead to native, quality Korean.
For eachone of the simple words above, we posit two moreadvanced words, as given in figure 2.kyo-sa ?teacher?
in-kan ?human?phyung-ka ?evaluation?
cik-cang ?workplace?pen-yuk ?translation?
wu-ceng ?friendship?mwun-hak ?literature?
sin-loy ?trust?ci-kwu ?earth?
cwu-min ?resident?swun-hwan ?circulation?
kwan-kye ?relation?myeng-sang ?meditation?
co-cik ?organization?phyeng-hwa ?peace?
sik-i-yo-pep ?diet?tham-hem ?exploration?
yen-mal ?end of a year?cwun-pi ?preparation?
hayng-sa ?event?Figure 2: Seed terms for the second focused corpus3.4 Web corpus parametersOne can create corpora of varying size and general-ity, by varying the parameters given to BootCaT.
Weexamine three parameters here.Number of seeds The first way to vary the typeand size of corpus obtained is by varying the numberof seed terms.
The exact words given to BootCaT af-fect the domain of the resulting corpus, and utilizintga larger set of seeds leads to more potential to createa bigger corpus.
With 50 seed terms, for example,there are 19,600 possible 3-tuples, while there areonly 120 possible 3-tuples for 10 seed terms, limit-ing the relevant pages that can be returned.For the general (G) corpus, we use: G1) all 50seed terms, G2) 5 sets of 10 seeds, the result of split-ting the 50 seeds randomly into 5 buckets, and G3)5 sets of 20 seeds, which expand the 10-seed sets inG2 by randomly selecting 10 other terms from theremaining 40 seeds.
This breakdown into 11 sets (1G1, 5 G2, 5 G3) allows us to examine the effect ofusing different amounts of general terms and facili-tates easy comparison with the first focused corpus,which has only 10 seed terms.For the first focused (F1) corpus, we use: F11) the10 seed terms, and F12) 5 sets of 20 seeds, obtainedby combining F11 with each seed set from G2.
Thissecond group provides an opportunity to examinewhat happens when augmenting the focused seedswith more general terms; as such, this is a first steptowards larger corpora which retain some focus.
Forthe second focused corpus (F2), we simply use theset of 20 seeds.
We have 7 sets here (1 F11, 5 F12, 1F2), giving us a total of 18 seed term sets at this step.Tuple length One can also experiment with tuplelength in BootCat.
The shorter the tuple, the morewebpages that can potentially be returned, as shorttuples are likely to occur in several pages (e.g., com-pare the number of pages that all of person happi-ness season occur in vs. person happiness seasonexercise travel).
On the other hand, longer tuples aremore likely truly relevant to the type of data of inter-est, more likely to lead to well-formed language.
Weexperiment with tuples of different lengths, namely3 and 5.
With 2 different tuple lengths and 18 seedsets, we now have 36 sets.Number of queries We still need to specify howmany queries to send to the search engine.
The max-imum number is determined by the number of seedsand the tuple size.
For 3-word tuples with 10 seedterms, for instance, there are 10 items to choose 3objects from:(103)= 10!3!(10?3)!
= 120 possibilities.Using all combinations is feasible for small seedsets, but becomes infeasible for larger seed sets, e.g.,(505)= 2, 118, 760 possibilities.
To reduce this, weopt for the following: for 3-word tuples, we generate120 queries for all cases and 240 queries for the con-ditions with 20 and 50 seeds.
Similarly, for 5-wordtuples, we generate the maximum 252 queries with10 seeds, and both 252 and 504 for the other condi-tions.
With the previous 36 sets (12 of which have10 seed terms), evenly split between 3 and 5-wordtuples, we now have 60 total corpora, as in table 1.# of seedstuple # of General F1 F2len.
queries 10 20 50 10 20 203 120 5 5 1 1 5 1240 n/a 5 1 n/a 5 15 252 5 5 1 1 5 1504 n/a 5 1 n/a 5 1Table 1: Number of corpora based on parameters11Other possibilities There are other ways to in-crease the size of a web corpus using BootCaT.
First,one can increase the number of returned pages for aparticular query.
We set the limit at 20, as anythinghigher will more likely result in non-relevant datafor the focused corpora and/or duplicate documents.Secondly, one can perform iterations of search-ing, extracting new seed terms with every iteration.Again, the concern is that by iterating away from theinitial seeds, a corpus could begin to lose focus.
Weare considering both extensions for the future.Language check One other constraint we use is tospecify the particular language of interest, namelythat we want Korean pages.
This parameter is setusing the language option when collecting URLs.We note that a fair amount of English, Chinese, andJapanese appears in these pages, and we are cur-rently developing our own Korean filter.4 Corpus statisticsTo gauge the properties of size, genre, and degree ofparticle usage in the corpora, independent of appli-cation, basic statistics of the different web corporaare given in table 2, where we average over multiplecorpora for conditions with 5 corpora.5There are a few points to understand in the table.First, it is hard to count true words in Korean, ascompounds are frequent, and particles have a de-batable status.
From a theory-neutral perspective,we count ejels, which are tokens occurring betweenwhite spaces.
Secondly, we need to know about thenumber of particles and number of nominals, i.e.,words which could potentially bear particles, as ourmachine learning paradigm considers any nominal atest case for possible particle attachment.
We use aPOS tagger (Han and Palmer, 2004) for this.Some significant trends emerge when comparingthe corpora in the table.
First of all, longer queries(length 5) result in not only more returned uniquewebpages, but also longer webpages on average thanshorter queries (length 3).
This effect is most dra-matic for the F2 corpora.
The F2 corpora also exhibita higher ratio of particles to nominals than the otherweb corpora, which means there will be more pos-5For the 252 5-tuple 20 seed General corpora, we averageover four corpora, due to POS tagging failure on the fifth corpus.itive examples in the training data for the machinelearner based on the F2 corpora.4.1 Qualitative evaluationIn tandem with the basic statistics, it is also impor-tant to gauge the quality of the Korean data froma more qualitative perspective.
Thus, we examinedthe 120 3-tuple F1 corpus and discovered a numberof problems with the data.First, there are issues concerning collecting datawhich is not pure Korean.
We find data extractedfrom Chinese travel sites, where there is a mixture ofnon-standard foreign words and unnatural-soundingtranslated words in Korean.
Ironically, we also findlearner data of Korean in our search for correct Ko-rean data.
Secondly, there are topics which, whileexhibiting valid forms of Korean, are too far afieldfrom what we expect learners to know, including re-ligious sites with rare expressions; poems, whichcommonly drop particles; gambling sites; and soforth.
Finally, there are cases of ungrammatical usesof Korean, which are used in specific contexts notappropriate for our purposes.
These include newspa-per titles, lists of personal names and addresses, andincomplete phrases from advertisements and chats.In these cases, we tend to find less particles.Based on these properties, we developed theaforementioned second focused corpus with moreadvanced Korean words and examined the 240 3-tuple F2 corpus.
The F2 seeds allow us to capture agreater percentage of well-formed data, namely datafrom news articles, encyclopedic texts, and blogsabout more serious topics such as politics, literature,and economics.
While some of this data might beabove learners?
heads, it is, for the most part, well-formed native-like Korean.
Also, the inclusion oflearner data has been dramatically reduced.
How-ever, some of the same problems from the F1 corpuspersist, namely the inclusion of poetry, newspapertitles, religious text, and non-Korean data.Based on this qualitative analysis, it is clear thatwe need to filter out more data than is currently be-ing filtered, in order to obtain valid Korean of a typewhich uses a sufficient number of particles in gram-matical ways.
In the future, we plan on restrict-ing the genre, filtering based on the number of rarewords (e.g., religious words), and using a trigramlanguage model to check the validity.12Ejel Particles NominalsCorpus Seeds Len.
Queries URLs Total Avg.
Total Avg.
Total Avg.Gen.
10 3 120 1096.2 1,140,394.6 1044.8 363,145.6 331.5 915,025 838.75 252 1388.2 2,430,346.4 1779.9 839,005.8 618.9 1,929,266.0 1415.320 3 120 1375.2 1,671,549.2 1222.1 540,918 394.9 1,350,976.6 988.63 240 2492.4 2,735,201.6 1099.4 889,089 357.3 2,195,703 882.45 252 1989.6 4,533,642.4 2356 1,359,137.2 724.5 3,180,560.6 1701.55 504 3487 7,463,776 2193.5 2,515,235.8 741.6 5,795,455.8 1709.750 3 120 1533 1,720,261 1122.1 584,065 380.9 1,339,308 873.63 240 2868 3,170,043 1105.3 1,049,975 366.1 2,506,995 874.15 252 1899.5 4,380,684.2 2397.6 1,501,358.7 821.5 3,523,746.2 1934.65 504 5636 5,735,859 1017.7 1,773,596 314.6 4,448,815 789.3F1 10 3 120 1315 628,819 478.1 172,415 131.1 510,620 388.35 252 1577 1,364,885 865.4 436,985 277.1 1,069,898 678.420 3 120 1462.6 1,093,772.4 747.7 331,457.8 226.8 885,157.2 604.9240 2637.2 1,962,741.8 745.2 595,570.6 226.1 1,585,730.4 602.15 252 2757.6 2,015,077.8 730.8 616,163.8 223.4 1,621,306.2 588504 4734 3,093,140.4 652.9 754,610 159.8 1,993,104.4 422.1F2 20 3 120 1417 1,054,925 744.5 358,297 252.9 829,416 585.3240 2769 1,898,383 685.6 655,757 236.8 1,469,623 530.75 252 1727 4,510,742 2611.9 1,348,240 780.7 2,790,667 1615.9504 2680 6,916,574 2580.8 2,077,171 775.1 4,380,571 1634.5Table 2: Basic statistics of different web corporaNote that one might consider building even largercorpora from the start and using the filtering step towinnow down the corpus for a particular application,such as particle error detection.
However, while re-moving ungrammatical Korean is a process of re-moving noise, identifying whether a corpus is abouttraveling, for example, is a content-based decision.Given that this is what a search engine is designedto do, we prefer filtering based only on grammaticaland genre properties.5 ClassificationWe describe the classification paradigm used to de-termine how effective each corpus is for detectingcorrect particle usage; evaluation is in section 6.5.1 Machine learning paradigmBased on the parallel between Korean particles andEnglish prepositions, we use preposition error de-tection as a starting point for developing a classifier.For prepositions, Tetreault and Chodorow (2008) ex-tract 25 features to guess the correct preposition (outof 34 selected prepositions), including features cap-turing the lexical and grammatical context (e.g., thewords and POS tags in a two-word window aroundthe preposition) and features capturing various rel-evant selectional properties (e.g., the head verb andnoun of the preceding VP and NP).We are currently using TiMBL (Daelemans et al,2007) for development purposes, as it provides arange of options for testing.
Given that learnerdata needs to be processed instantaneously and thatmemory-based learning can take a long time to clas-sify, we will revisit this choice in the future.5.2 Defining features5.2.1 Relevant properties of KoreanAs discussed in section 2, Korean has major dif-ferences from English, leading to different features.First, the base word order of Korean is SOV, whichmeans that the following verb and following nouncould determine how the current word functions.However, since Korean allows for freer word orderthan English, we do not want to completely disre-gard the previous noun or verb, either.Secondly, the composition of words is differentthan English.
Words contain a stem and an arbitrarynumber of suffixes, which may be derivational mor-13phemes as well as particles, meaning that we mustconsider sub-word features, i.e., segment words intotheir component morphemes.Finally, particles have more functions than prepo-sitions, requiring a potentially richer space of fea-tures.
Case marking, for example, is even more de-pendent upon the word?s grammatical function ina sentence.
In order to ensure that our system cancorrectly handle all of the typical relations betweenwords without failing on less frequent constructions,we need (large amounts of) appropriate data.5.2.2 Feature setTo begin with, we segment and POS tag the text,using a hybrid (trigram + rule-based) morphologicaltagger for Korean (Han and Palmer, 2004).
This seg-mentation phase means that we can define subwordfeatures and isolate the particles in question.
For ourfeatures, we break each word into: a) its stem and b)its combined affixes (excluding particles), and eachof these components has its own POS, possibly acombined tag (e.g., EPF+EFN), with tags from thePenn Korean Treebank (Han et al, 2002).The feature vector uses a five word window thatincludes the target word and two words on eitherside for context.
Each word is broken down into fourfeatures: stem, affixes, stem POS, and affixes POS.Given the importance of surrounding noun and verbsfor attachment in Korean, we have features for thepreceding as well as the following noun and verb.For the noun/verb features, only the stem is used, asthis is largely a semantically-based property.In terms of defining a class, if the target word?saffixes contain a particle, it is removed and used asthe basis for the class; otherwise the class is NONE.We also remove particles in the context affixes, aswe cannot rely on surrounding learner particles.As an example, consider predicting the particlefor the word Yenge (?English?)
in (5a).
We gener-ate the instance in (5b).
The first five lines referto the previous two words, the target word, and thefollowing two words, each split into stem and suf-fixes along with their POS tags, and with particlesremoved.
The sixth line contains the stems of thepreceding and following noun and verb, and finally,there is the class (YES/NO).
(5) a. Mikwuk-eyseAmerica-insal-myenselive-whileYenge-man-ulEnglish-only-OBJcip-eysehome-atss-ess-eyo.use-Past-Decl?While living in America, (I/she/he) used onlyEnglish at home.?b.
Mikwuk NPR NONE NONEsal VV myense ECSYenge NPR NONE NONEcip NNC NONE NONEss VV ess+eyo EPF+EFNsal Mikwuk ss cipYESFor the purposes of evaluating the different cor-pora, we keep the task simple and only guess YESor NO for the existence of a particle.
We envisionthis as a first pass, where the specific particle canbe guessed later.
This is also a practical task, inthat learners can benefit from accurate feedback onknowing whether or not a particle is needed.6 EvaluationWe evaluate the web corpora for the task of predict-ing particle usage, after describing the test corpus.6.1 Learner CorpusTo evaluate, we use a corpus of learner Korean madeup of essays from college students (Lee et al, 2009).The corpus is divided according to student level (be-ginner, intermediate) and student background (her-itage, non-heritage),6 and is hand-annotated for par-ticle errors.
We expect beginners to be less accuratethan intermediates and non-heritage less accuratethan heritage learners.
To pick a middle ground, thecurrent research has been conducted on non-heritageintermediate learners.
The test corpus covers a rangeof common language classroom topics such as Ko-rean language, Korea, friends, family, and traveling.We run our system on raw learner data, i.e, un-segmented and with spelling and spacing errors in-cluded.
As mentioned in section 5.2.2, we use a POStagger to segment the words into morphemes, a cru-cial step for particle error detection.76Heritage learners have had exposure to Korean at a youngage, such as growing up with Korean spoken at home.7In the case of segmentation errors, we cannot possibly getthe particle correct.
We are currently investigating this issue.14Seeds Len.
Quer.
P R FGen.
10 3 120 81.54% 76.21% 78.77%5 252 82.98% 77.77% 80.28%20 3 120 81.56% 77.26% 79.33%3 240 82.89% 78.37% 80.55%5 252 83.79% 78.17% 80.87%5 504 84.30% 79.44% 81.79%50 3 120 82.97% 77.97% 80.39%3 240 83.62% 80.46% 82.00%5 252 82.57% 78.45% 80.44%5 504 84.25% 78.69% 81.36%F1 10 3 120 81.41% 74.67% 77.88%5 252 83.82% 77.09% 80.30%20 3 120 82.23% 76.40% 79.20%240 82.57% 77.19% 79.78%5 252 83.62% 77.97% 80.68%504 81.86% 75.88% 78.73%F2 20 3 120 81.63% 76.44% 78.93%240 82.57% 78.45% 80.44%5 252 84.21% 80.62% 82.37%504 83.87% 81.51% 82.67%Table 3: Results of guessing particle existence, trainingwith different corporaThe non-heritage intermediate (NHI) corpus givesus 3198 words, with 1288 particles and 1836 nom-inals.
That is, about 70% of the nominals in thelearner corpus are followed by a particle.
This is amuch higher average than in the 252 5-tuple F2 cor-pus, which exhibits the highest average of all of theweb corpora at about 48% ( 7811616 ; see table 2).6.2 ResultsWe use the default settings for TiMBL for all the re-sults we report here.
Though we have obtained 4-5%higher F-scores using different settings, the compar-isons between corpora are the important measure forthe current task.
The results are given in table 3.The best results were achieved when trainingon the 5-tuple F2 corpora, leading to F-scores of82.37% and 82.67% for the 252 tuple and 504 tu-ple corpora, respectively.
This finding reinforces ourhypothesis that more advanced seed terms result inmore reliable Korean data, while staying within thedomain of the test corpus.
Both longer tuple lengthsand greater amounts of queries have an effect on thereliability of the resulting corpora.
Specificaly, 5-tuple corpora produce better results than similar 3-tuple corpora, and corpora with double the amountof queries of n-length perform better than smallercomparable corpora.
Although larger corpora tendto do better, it is important to note that there is nota clear relationship.
The general 50/5/252 corpus,for instance, is similarly-sized to the F2 focused20/5/252 corpus, with over 4 million ejels (see ta-ble 2).
The focused corpus?based on fewer yetmore relevant seed terms?has 2% better F-score.7 Summary and OutlookIn this paper, we have examined different ways tobuild web corpora for analyzing learner languageto support the detection of errors in Korean parti-cles.
This type of investigation is most useful forlesser-resourced languages, where the error detec-tion task stays constant, but the topic changes fre-quently.
In order to develop a framework for testingweb corpora, we have also begun developing a ma-chine learning system for detecting particle errors.The current web data, as we have demonstrated, isnot perfect, and thus we need to continue improvingthat.
One approach will be to filter out clearly non-Korean data, as suggested in section 4.1.
We mayalso explore instance sampling (e.g., Wunsch et al,2009) to remove many of the non-particle nominal(negative) instances, which will reduce the differ-ence between the ratios of negative-to-positive in-stances of the web and learner corpora.
We still feelthat there is room for improvement in our seed termselection, and plan on constructing specific web cor-pora for each topic covered in the learner corpus.We will also consider adding currently available cor-pora, such as the Sejong Corpus (The National Insti-tute of Korean Language, 2007), to our web data.With better data, we can work on improving themachine learning system.
This includes optimizingthe set of features, the parameter settings, and thechoice of machine learning algorithm.
Once the sys-tem has been optimized, we will need to test the re-sults on a wider range of learner data.AcknowledgmentsWe would like to thank Marco Baroni and JanPomika?lek for kindly providing a UTF-8 version ofBootCat; Chong Min Lee for help with the POS tag-ger, provided by Chung-Hye Han; and Joel Tetreaultfor useful discussion.15ReferencesAmaral, Luiz and Detmar Meurers (2006).
Wheredoes ICALL Fit into Foreign Language Teach-ing?
Talk given at CALICO Conference.
May19, 2006.
University of Hawaii.Baroni, Marco and Silvia Bernardini (2004).
Boot-CaT: Bootstrapping Corpora and Terms from theWeb.
In Proceedings of LREC 2004. pp.
1313?1316.Baroni, Marco and Motoko Ueyama (2004).
Re-trieving Japanese specialized terms and corporafrom the World Wide Web.
In Proceedings ofKONVENS 2004.Daelemans, Walter, Jakub Zavrel, Ko van der Sloot,Antal van den Bosch, Timbl Tilburg and Memorybased Learner (2007).
TiMBL: Tilburg Memory-Based Learner - version 6.1 - Reference Guide.De Felice, Rachele and Stephen Pulman (2008).
Aclassifier-baed approach to preposition and deter-miner error correction in L2 English.
In Proceed-ings of COLING-08.
Manchester.Dickinson, Markus, Soojeong Eom, YunkyoungKang, Chong Min Lee and Rebecca Sachs (2008).A Balancing Act: How can intelligent computer-generated feedback be provided in learner-to-learner interactions.
Computer Assisted LanguageLearning 21(5), 369?382.Dickinson, Markus and Chong Min Lee (2009).Modifying Corpus Annotation to Support theAnalysis of Learner Language.
CALICO Journal26(3).Erjavec, Irena Srdanovic`, Tomaz Erjavec and AdamKilgarriff (2008).
A Web Corpus and WordSketches for Japanese.
Information and MediaTechnologies 3(3), 529?551.Han, Chung-Hye, Na-Rare Han, Eon-Suk Ko andMartha Palmer (2002).
Development and Eval-uation of a Korean Treebank and its Applicationto NLP.
In Proceedings of LREC-02.Han, Chung-Hye and Martha Palmer (2004).
A Mor-phological Tagger for Korean: Statistical Tag-ging Combined with Corpus-Based Morphologi-cal Rule Application.
Machine Translation 18(4),275?297.Han, Na-Rae, Martin Chodorow and Claudia Lea-cock (2006).
Detecting Errors in English Arti-cle Usage by Non-Native Speakers.
Natural Lan-guage Engineering 12(2).Ko, S., M. Kim, J. Kim, S. Seo, H. Chung and S. Han(2004).
An analysis of Korean learner corporaand errors.
Hanguk Publishing Co.Lee, John and Ola Knutsson (2008).
The Role ofPP Attachment in Preposition Generation.
In Pro-ceedings of CICLing 2008.
Haifa, Israel.Lee, John and Stephanie Seneff (2006).
Auto-matic Grammar Correction for Second-LanguageLearners.
In INTERSPEECH 2006.
Pittsburgh,pp.
1978?1981.Lee, Sun-Hee, Donna K. Byron and Seok Bae Jang(2005).
Why is Zero Marking Important in Ko-rean?
In Proceedings of IJCNLP-05.
Jeju Island,Korea.Lee, Sun-Hee, Seok Bae Jang and Sang kyu Seo(2009).
Annotation of Korean Learner Corporafor Particle Error Detection.
CALICO Journal26(3).Sharoff, Serge (2006).
Creating General-PurposeCorpora Using Automated Search EngineQueries.
In WaCky!
Working papers on the Webas Corpus.
Gedit.Tetreault, Joel and Martin Chodorow (2008).
TheUps and Downs of Preposition Error Detectionin ESL Writing.
In Proceedings of COLING-08.Manchester.Tetreault, Joel and Martin Chodorow (2009).
Exam-ining the Use of Region Web Counts for ESL Er-ror Detection.
In Web as Corpus Workshop (WAC-5).
San Sebastian, Spain.The National Institute of Korean Language (2007).The Sejong Corpus.Ueyama, Motoko (2006).
Evaluation of JapaneseWeb-based Reference Corpora: Effects of SeedSelection and Time Interval.
In WaCky!
Workingpapers on the Web as Corpus.
Gedit.Wunsch, Holger, Sandra Ku?bler and RachaelCantrell (2009).
Instance Sampling Methods forPronoun Resolution.
In Proceedings of RANLP2009.
Borovets, Bulgaria.16
