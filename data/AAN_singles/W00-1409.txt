An Extended Architecture for Robust  Generation*T i lman Becker ,  Anne  K i lger ,  Pat r i ce  Lopez ,  Peter  Po l le rDFK I  GmbHStuh lsatzenhausweg 3D-66123 Saarbr i i cken ,  Germany{becker, kilger, lopez, poller}@dfki, deAbst rac tBased on our experiences in VERBMOBIL, a largescale speech-to-speech translation system, we iden-tify two types of problems that a generation com-ponent must address in a realistic implementationand present relevant examples.
As an extension tothe architecture ofa translation system, we present amodule for robustness preprocessing on the interfacebetween translation and generation.1 In t roduct ionBased on our experiences with VERBMOBIL, a largescale speech-to-speech translation system, we iden-tify two types of problems that a generation com-ponent must address in a comprehensive implemen-tation.
Besides general task-inherent problems like,e.g., the processing of spontaneous speech input, thetranslation step itself, and real-time processing, wefound that an implementation of such a large scalesystem additionally exhibits technical problems thatare caused by various faults in the steps prior togeneration.The task of VERBMOBIL is the multi-lingual (Ger-man, English, Japanese) speaker-independent trans-lation of spontaneous peech input that enablesusers to converse about the scheduling of a busi-ness appointment including travel, accommodation,and leisure time planning in a multi-lingual dia-logue.
The system covers 10,000 words in eachlanguage with the corresponding knowledge bases(grammars, translation rules, etc.).
In contrast toa text translation system, the processing of spon-taneous speech requires extended functionalities inalmost ever:,- module because the system has to beable do deal with, e.g., ill-formed and disfluent (hes-Due to the high complexity of this task, the sys-tem is subdivided into 24 separate subtasks (imple-mented modules).For the translation step the system containsfour different parallel translation "tracks" consist-ing Of three "shallow" (case based, statistical, anddialogue-act based (Reithinger, 1999)) and one"deep" translation track (see figure 1) for each lan-guage.
The individual translation results are partlyassociated with confidence values reflecting theirquality and then sent to a special selection compo-nent to choose the most appropriate one.
Our prac-tical experience shows that there are cases in whichthe input to the generation component is impossibleto process.
Here the shallow translation paths serveas a fall-back in order to fulfill the strong necessityof a translation result as far as possible.Although some parts of the analysis task (e.g., re-solving scopal ambiguities) can actually be left un-solved when they are not necessary for the transla-tion task, in general, problems in some module resultin an accumulated inadequacy of the final transla-tion.Since the translation task is distributed to a setof cooperating modules, there is a choice of solvingthe task inherent and technical problems either lo-cally inside the individual modules or handing themto problem specific correction modules.
We foundthat robustness must be included in every module.For the architecture of the generation module, wehave devised a submodule for robustness that pre-processes the input data.
This proved an elegantand promising extension to achieve the required localmodule robustness without touching tile core gener-ation module directly.
A similar module also ex-ists for analysis (see 'Robust Semantics' in figure 1),itations, repetitions, repairs) speech input.
In a dia- (Worm, 1998).logue system, there is also.anapparently simp!
ebut :.
?
In.this paper~ we,foeus..on the generation eompo-very strong constraint on the system to achieve itstask: For each user input the system has to producea translation result."
The research within VERBMOBIL presented here is fundedby the German Ministry of Research and q~mhnology undergrant 011\.
'101K/1.nent of our system.
Besides the general robustnessrequirements, the mentioned inadequacy accunmla-tion reaches its maxinmm since generation is posi-tioned at the end of the translation process.
In thefollowing sections, we show how the strong robust-ness requirement influenced the architecture of our63User 1Language AUser 2Language BSpeech777 W .....Figure 1: Simplified system architecture of the speech-to-speech translation system VERBMOBIL.generation module.
We classify the above mentionedproblems from the point of view of generation andpresent our solutions to these problems, mainly un-der the aspect of robust generation with problematicinput.2 Task - inherent  and  Techn ica lProb lemsThe problems for generation that arise in a speech-to-speech translation system fall into two mainclasses: as in any large-scale system, there willbe software-engineering problems which we will calltechnical problems and there are task-inherent prob-lems that are particular to the translation task andthe highly variable input in spontaneous peech.Since it is impossible to draw a clear line be-tween technical and task-inherent problems, we willpresent a short classification and then go into moredetail without much discussion whether a certainproblem should be viewed as technical or task-inherent.One would hope to be able to eliminate technicalproblems completely.
However, in a large system,where development is distributed over many mod-ules (implemented at different sites), some robust-ness against certain technical problems can becomea necessity, as our experiences have shown.
This iseven more important during the development phase-which a research system never leaves.
Most technicalproblems have to do with violations of the interfacedefinitions.
Thisranges.
from simple ~things uch asusing unknown predicates in the semantic represen-tation to complex constructions that cannot be gen-erated (the generation gap).
We actually regard thelatter as a task-inherent problem.Secondly, tile task-inherent problems can be di-vided into problems that are caused by (i) spon-taneous speech input and (ii) insufficiencies in theanalysis and translation steps.2.1 Robustness  in Ana lys i sThe problems in (i) are quite varied and many casesare dealt with in analysis (and translation), somecases are dealt with in our robustness preprocess-ing submodule, a few in the classical submodules ofgeneration.
For example, there is a separate mod-ule on the level of speech recognition which dealswith hesitations and self-corrections.
Phenomenalike ellipsis, phrasal and other incomplete utterancesare handled by analysis, so generation must be ableto deal with the corresponding semantic representa-tions too.
Agreement errors are handled (i.e., cor-rected) in analysis.
But some serious syntactic errorscannot be corrected.
However, at least the maxi-mal analyzable segments are determined so that un-grammatical utterances are translated as sequencesof several meaningful segments.2.2 Robustness  in Generat ionThe problems in (ii) are caused by an accunmla-tion of problems which result in (semantic) input tothe generator that cannot be processed.
Robustnessin our system concentrates on this type of problenlwhich is and should be handled as a separate stepbetween analysis/transfer and generation.
(See thediscussion of the architecture in section 3.
)The list below contains some examples that arepicked up again in section 4.
* Problems with the structure of the semantic rep-resentation:- unconnected subgraphs- multiple predicates referring to the sameobject64- omission of obligatory arguments?
Problems with the content of the semantic rep-resentation:- contradicting information- missing information (e.g.
agreement infor-mation)3 Arch i tectureAs described in section t, the  deep processing inVERBMOBIL is based on a pipeline of modules whichuse a unique interface language (VIT 1) that incorpo-rates a semantic representation.
Since this seman-tic representation is obviously grammar-independentand could reflect the effects of spoken, spontaneouslanguage, we have no guarantee that the gram-mar covers the semantic representation given by thetransfer module.
Consequently we have chosen toextend the classical generation architecture with anew module dedicated to robust preprocessing.
Wefirst present our classical generator architecture (seealso (Becker et al, 1998; Becker et al, 2000)) interms of the RAGS architecture and then discuss itsextension to the task-inherent problems.The RAGS architecture (Cahill et al, 1999) is areference architecture for natural language genera-tion systems.
Reflecting the common parts of natu-ral language generation systems, this proposal aimsto provide a standard architecture allowing the iden-tification of some important generation subtasks andresources.
By presenting our system in the light ofthe RAGS specifications, our goal is to propose gen-eral solutions that could be used by other researcherswho need to extend their own generation architec-ture to similar tasks.While the macro-planning task is important andmandatory in text generation, it is limited in dia-logue translation.
Most of the related problems, forinstance the sentence segmentation a d the pronounchoices, have been solved by the user in the sourcelanguage.
Considering the RAGS architecture, con-ceptual and rhetorical evels of representation arealso outside the scope of our system.
Our architec-ture consists of four main modules (see figure 2).For an easy adaptation to other domains and lan-guages, we have emphasized an organization basedon a general kernel system and the declarativity ofknowledge sources (Becker et al, 1998).
All but thefirst modules are captured by the RAGS architec-ture.
However, the first module is dedicated solelyto robustness in the specific speech-to-speech trans-lation task and will be presented and discussed lastin this section.
It can easily be added to a RAGS-like system whose whiteboard is perfectly suited forlVerbmobil Interface Term, (Bos et al, 1996; Dorna,1996)the transformations that the robustness preprocess-ing module performs.RobustnessPreprocessingModuleStandardGenerationModuleRepairing Struturalkx~.~ss ing  GHae~risfics for Generation J(%e::::::Selecting Planning Ru les~Checking Lexical Choice JC0nstraints .
- ~ .
.e Selecting LTAG Treese Tree Combination?
Inflectione Synthesis AnnotationFigure 2: An extended generation system architec-tureM ic rop lann ing  Modu le  At the level of sentencegeneration, the quality of the planning process de-pends on the interdependencies between conceptualsemantics, predicative semantics and syntax.
A par-ticular lexical choice can imply constraints on otherlexical items.
The role of the microplanner is to re-alize lexical choices in a way that a syntactic realiza-tion is possible and costly backtracking is prevented.The microplanning task can be viewed as a con-straint solving problem and implemented using anadapted constraint solving mechanism in order toachieve efficiency, flexibility, and declarativity ofknowledge.
The microplanner produces a depen-dency tree representation i dicating for each nodea set of syntactical constraints to be fulfilled bythe corresponding lexical syntactic units (predicate,tense, aspect, mood, etc.
).Syntact ic  Rea l izat ion Modu le  This module isin charge of the concrete syntax generation.
Theprocessing is .based ,on a fully lexicatized Tree-Adjoining Grammar derived from the HPSG gram-mar used in the deep-processing parser module(Kasper~et aL, 1995; Becker, 1998).S u r f a c e  Real izat ion  Modu le  The syntactic re-alization module produces a derived tree from whichtile output string is extracted.
The morphologicalfeatures in preterminal nodes are used for inflection.The surface string is also annotated by syntactic in-formation (phrase boundary, aspect, sentence mood)65that are exploited by the speech synthesis module.Robustness Preprocess ing  Modu le  We havedescribed three modules corresponding to classicaltasks of generation systems and pointed out at thebeginning of this section the necessity for robustness.Where can we integrate the required robustness insuch a generation architecture?
One approach couldbe the relaxation of constraints during the syntac-tic realization (relaxing word order or/and depen-dency relations).
One can argue against this ap-proach that:clearly separated from the microplanning rules, jus-tifying our presentation of robustness as a separatemodule.4.2 Conforming  to the Interface LanguageDefinitionThe definition of the interface language 2 comprisesonly its syntax and some semantic constraints.There is an implementation of expressions in the in-terface language as an abstract data type which canat least check syntactic conformity (Dorna, 1996).But we also have to deal with semantic faults.-*.
There is no .straightf~r~ard~Way~t~Aimi.t~he.J~e~.
:.~.~`-~,;~T~f~rs~e~amp~e~i~''minating>r`0bust~pre~r~ess-laxation of syntactic onstraints only to the ro-bustness problematic structures.?
We must be sure that the microplanning modulecan deal with problematic semantic input.These points suggest to check and repair theinconsistencies of the semantic representation asearly as possible, i.e., before sentence microplanning.Moreover we show in the next section that most ofthe problems presented in section 2 can be identifiedbased on the microplanner input.We now present more concretely the robust pre-processing module.4 RobustnessIn this section we describe the types of problemsdefined in section 2 using examples from our systemand discuss how our module is made robust enoughto handle a lot of these problems.Before the semantic representation is handed tomicroplanning, the robustness preproeessing moduleof the generator checks the input, inspecting its partsfor known problems.
For each problem found, thepreprocessor lowers a confidence value for the gen-eration output which measures the reliability of ourresult.
In a number of cases, we use heuristics to fixproblems, aiming at improved output.As discussed in section 2, problems in the inputto the generator can be technical or task-inherent.Technical problems manifest themselves as faultswrt.
the interface language definition, whereas thetask-inherent problems concern mismatches betweena specific semantic expression and the coverage ofthe natural language grammar used in the genera-tor.
These mismatches are known as the generationgap (Meteer, 1990).4.1 Dec la ra t iv i tyIn..our implementation; the  :robustness module ispartly integrated into the constraint solving ap-proach of the microplanning module.
Using the con-straint solving approach allows for a strict separa-tion of algorithms (i.e., some constraint solving al-gorithln) and declarative knowledge sources.
On thislevel, the rules (constraints) for robustness can being is on the connectedness of the semantic inputgraph.
Our interface language describes an interfaceterm to contain a connected semantic graph plus anindex pointing to the root of the graph.
Two typesof problems can occur according to this definition:Disconnectedness of the Graph: The robust-ness preprocessor checks whether the inputgraph is in fact connected.
If there are severaldisconnected parts, a distinct generation callis made for each subgraph.
In the end, allsub-results are connected to produce a globalresult.
We are currently working on a betterheuristic to order the sub-results, taking intoaccount information about the word order inthe source language.Wrong Index:  The robustness preprocessor testswhether the index points to the root of thegraph or one of the subgraphs.
For each sub-graph without an adequate index, we computea local root pointer which is used for furtherprocessing.
This turned out to be an easy andreliable heuristic, leading to good results.There are several types of technical problemswhich cannot be repaired well.
Minimally, thesecases are detected, warning messages are produced,and the confidence value is lowered.
We applyheuristics where possible.
Examples are unique-ness of labels (every semantic predicate must havea unique identifier), the use of undefined predicatenames, and contradicting information (e.g., the useof  a DEFINITE and an INDEFINITE quantifier for thesame object).
In the case of incorrect predicateclasses, i.e., where a predicate is used with an unde-fined-argument frame, only those parts of the inputare handled which are analyzed as correct.4.3 Fal l ing into the Generat ion  GapThe robustness preprocessor even does more thanchecking for structural contradictions between in-put and interface language.
Based on analyses of2A further complication in a research system like ourss tems from the fact that the interface language itself is de-veloped, i.e., changed over time.66a large amount of test-suites it is fed with someheuristics which help to bridge the generation gapthat reflects the unpredictability, whether_a specificsemantic structure can be mapped to an acceptableutterance in the target language.
Some examples ofheuristics used in our system are as follows:Conf l ic t ing In fo rmat ion :  Often it is inconsistentto allow several predicates to include the samedepending structure in their argument frames,e.g., two predicates describing different prepo-sitions should not point to the same entity.
Wehave to pick one-,possibitity~heuristically: ........Gaps  in Generat ion  Knowledge:  There are in-put configurations that have no reflectionwithin the generator's knowledge bases, e.g.,the DISCOURSE predicate defining a sequenceof otherwise unrelated parts of the input.
Therobustness preprocessor removes this predicate,thereby subdividing the connected graph intoseveral unconnected ones and continuing as fordisconnected graphs described above.Other examples for generation constraints thatcan conflict with the input are the occurrenceof some specific cyclic subparts of graphs, self-referring predicates, and chains of predicateswhich are not realizable in generation.Robustness  inside the  Microp lanner  and theSyntact i c  Generator  additionally helps to get ridof some generation gap problems:Cont rad ic t ions  to Generat ion  Constra ints :The knowledge bases of the generator (mi-eroplanning rules, grammar and lexicon)describe constraints on the structure of theoutput utterance that might conflict with theinput.
A common problem occuring in oursystem is the occurrence of subordinatingpredicates with empty obligatory arguments.Here the microplanner relaxes the constraintfor argument completeness and hands over astructure to the syntactic generator that doesnot fulfill all syntactic constraints or containselliptical arguments.
In these cases, the gram-mar constraints for obligatory arguments arerelaxed in the syntactic generator and ellipticalarguments are allowed.beyond the constraintsof the grammar.
The result is often output thatreflects the spontaneous speech input which weaccept for the sake of robustness.M iss ing  a t t r ibutes :  Often there are obligatory at-tributes for the semantic predicates missing inthe input, e.g., statements about the direction-ality of prepositions, agreement information,etc.
The generator uses heuristics to choose avalue for its own.Cont rad ic t ions  on  the  Semant ic  Level: Someattributes may lead to conflicts during genera-tion,.e.g:, i f~ pronoun is given:as SORT~HUMANand TYPE-~SPEAKER.
The generator uses aheuristics to set the value of SORT in this case.So lv ing Part  o f  the  Ana lys i s  Task: Sometimesthe input to the generator is underspecified ina way that it can be improved easily by usingsimple heuristics to "continue analysis."
Acommon example in.
our system is an inputexpression like "on the third" which often is.... .. ~analyzed..as.. (ABSTR.,-NOM .A  OPal)(.3.).
), e~-e,,..anelliptical noun with ordinal number 3.
Weadd the sort TIME_DOFM 3 to the attributes ofABSTR_NOM SO that, e.g., a semantic relationTEMPORAL_OR_LOCAL is correctly mapped tothe German preposition "an.
"4.4 How much robustness?There is a limit to the power of heuristics that wehave determined using a large corpus of test data.Some examples for possible pitfalls:?
When realizing "empty nominals" ABSTR_NOMas elliptical nouns, guessing the gender cancause problems: "Thursday is my free day ti "as FREE A DAY A ABSTR_NOM (with a readingas in "day job") might result in "*Donnerstagist mein freies Tag ti.
"o Conflicts between sort and gender of a pronounmight be resolved incorrectly: "Es (English:'it') trifft sich ganz hervorragend" with PRON(GENDER:NTR, SORT~HUMAN) should not betranslated as "#He is really great.
"Although the boundary beyond which deep trans-lation cannot be achieved even with heuristics issomewhat arbitrary, the big advantage of deep pro-cessing lies in the fact that the system 'knows' itsboundaries and actually fails when a certain level ofquality cannot be guaranteed.
As discussed in sec-tion 1, in a dialogue system, a bad translation fightstill be better than none at all, so one of the shallowmodules can be selected when deep processing fails.5 Re la ted  Work  and  Conc lus ionsVERB~;IOBIL also contains a component hat au-tomatically generates dialogue scripts and resultsummaries of the dialogues in all target languages(Alexandersson and Poller, 1998; Alexandersson and::Poller~ 2000 )~:: ~This component , uses the'generationmodules of VERB1V\[OBIL for sentence generation aswell as the translation system itself to achieve multi-linguality.
To some extend, this task also benefits3DOFM: day of the month.
Note that in this paper, thepresentation of the semantic representation language is highlyabstracted from tile actual interface language.67from the task inherent robustness features of theoverall system and its modules which we describedin this paper.Our problem classification also shows up in othergeneration systems.
There is a multi-lingual gen-eration project (Uchida et al, 1999) that utilizesan interlingua-based semantic representation to gen-erate web-documents in different output languagesfrom one common representation.
Although techni-cal problems are less prominent, the task-inherentproblems are almost he same.
Again, the genera-R. Kasper, B. Kiefer, K. Netter, and K. Vijay-Shanker.
1995.
Compilation of hpsg to tag.
In.
.
.
.
.
Proceedings o f  .the:..33rd ~Aunual: Meeting :of theAssociation for Computational Linguistics, pages92-99, Cambridge, Mass.M.W.
Meteer.
t990.
The "Generation Gap" - TheProblem of Expressibility in Text Planning.
Ph.D.thesis, Amherst, MA.
BBN Report No.
7347.Norbert Reithinger.
1999.
Robust information ex-traction in a speech translation system.
In Pro-ceedings of EuroSpeech-99, pages 2427-2430.tot has to able to deal with, e.g., disconnected or Hiroshi Uchida, Meiying Zhu, and Tarcisio Della?
contradicting inpu't graphs: .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~" ~ Sieiiit~2 "1999.-UNL::~I~S;" U/iitei:l ~Na~i6hg"Ufii-~~ ::: -sity, Tokyo, Japan, November.Re ferencesJan Alexandersson and Peter Poller.
1998.
Towardmultilingual protocol generation for spontaneousspeech dialogues.
In Proceedings of the Ninth In-ternational Workshop on Natural Language Gen-eration, Niagara-on-the-Lake, Ontario, Canada,August.Jan Alexandersson and Peter Poller.
2000.
Multi-lingual summary generation i a speech-to-speechtranslation system for multilingual negotiation di-alogues.
In Proceedings of INLG 2000, Mitzpe Ra-mon, Israel, June.T.
Becker, W. Finkler, A. Kilger, and P. Poller.1998.
An efficient kernel for multilingual genera-tion in speech-to-speech dialogue translation.
InProceedings of COLING/A CL-98, Montreal, Que-bec, Canada.Tilman Becker, Anne Kilger, Patrice Lopez, andPeter Poller.
2000.
Multilingual generation fortranslation in speech-to-speech dialogues and itsrealization in verbmobil.
In Proceedings of ECAI2000, Berlin, Germany, August.Tihnan Becker.
1998.
Fully lexicalized head-driven syntactic generation.
In Proceedings of theNinth International Workshop on Natural Lan-guage Generation, Niagara-on-the-Lake, Ontario,Canada, August.Johan Bos, Bj6rn Gambfi.ck, Christian Lieske,Yoshiki Mori, Manfred Pinkal, and KarstenWorm.
1996.
Compositional semantics in verb-mobil.
In Proceedings of Coling '96, Copenhagen,Denmark.Lynne Cahill, Christy Doran, Roger Evans, ChrisMellish, Daniel Paiva, Mike Reape, Donia Scott,and Neil Tipper.
1999.
Towards a Reference Ar-chitecture for Natural Language: Generation Sys-tems.
Technical Report ITRI-99-14, InformationTechnology Research Institute (ITRI), Universityof Brighton.Michael Dorna.
1996.
The adt package for the verb-mobil interface term.
Verbmobil-lrleport 104, Uni-versity Stuttgart, April.Karsten Worm.
1998.
A model for robust processingof spontaneous speech by integrating viable frag-ments.
In Proceedings of COLING-ACL '98, Mon-treal, Canada.68
