An Evaluation Exercise for Romanian Word Sense DisambiguationRada MihalceaDepartment of Computer ScienceUniversity of North TexasDallas, TX, USArada@cs.unt.eduVivi Na?staseSchool of Computer ScienceUniversity of OttawaOttawa, ON, Canadavnastase@site.uottawa.caTimothy ChklovskiInformation Sciences InstituteUniversity of Southern CaliforniaMarina del Rey, CA, USAtimc@isi.eduDoina Ta?tarDepartment of Computer ScienceBabes?-Bolyai UniversityCluj-Napoca, Romaniadtatar@ubb.roDan Tufis?Romanian Academy Centerfor Artificial IntelligenceBucharest, Romaniatufis@racai.roFlorentina HristeaDepartment of Computer ScienceUniversity of BucharestBucharest, Romaniafhristea@mailbox.roAbstractThis paper presents the task definition, resources,participating systems, and comparative results for aRomanian Word Sense Disambiguation task, whichwas organized as part of the SENSEVAL-3 evaluationexercise.
Five teams with a total of seven systemswere drawn to this task.1 IntroductionSENSEVAL is an evaluation exercise of the lat-est word-sense disambiguation (WSD) systems.
Itserves as a forum that brings together researchers inWSD and domains that use WSD for various tasks.It allows researchers to discuss modifications thatimprove the performance of their systems, and an-alyze combinations that are optimal.Since the first edition of the SENSEVAL competi-tions, a number of languages were added to the orig-inal set of tasks.
Having the WSD task prepared forseveral languages provides the opportunity to testthe generality of WSD systems, and to detect dif-ferences with respect to word senses in various lan-guages.This year we have proposed a Romanian WSDtask.
Five teams with a total of seven systems havetackled this task.
We present in this paper the dataused and how it was obtained, and the performanceof the participating systems.2 Open Mind Word ExpertThe sense annotated corpus required for this taskwas built using the Open Mind Word Expert system(Chklovski and Mihalcea, 2002), adapted to Roma-nian1.To overcome the current lack of sense taggeddata and the limitations imposed by the creation ofsuch data using trained lexicographers, the OpenMind Word Expert system enables the collectionof semantically annotated corpora over the Web.Sense tagged examples are collected using a Web-based application that allows contributors to anno-tate words with their meanings.The tagging exercise proceeds as follows.
Foreach target word the system extracts a set of sen-tences from a large textual corpus.
These examplesare presented to the contributors, who are asked toselect the most appropriate sense for the target wordin each sentence.
The selection is made using check-boxes, which list all possible senses of the currenttarget word, plus two additional choices, ?unclear?and ?none of the above.?
Although users are en-couraged to select only one meaning per word, theselection of two or more senses is also possible.
Theresults of the classification submitted by other usersare not presented to avoid artificial biases.3 Sense inventoryFor the Romanian WSD task, we have chosen a setof words from three parts of speech - nouns, verbsand adjectives.
Table 1 presents the number ofwords under each part of speech, and the averagenumber of senses for each class.The senses were (manually) extracted from a Ro-manian dictionary (Dict?ionarul EXplicativ al limbiiroma?ne - DEX (Coteanu et al, 1975)).
These senses1Romanian Open Mind Word Expert can be accessed athttp://teach-computers.org/word-expert/romanianAssociation for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of SystemsNumber Avg senses Avg sensesClass words (fine) (coarse)Nouns 25 8.92 4.92Verbs 9 8.7 4.6Adjectives 5 9 4Total 39 8.875 4.725Table 1: Sense inventoryand their dictionary definitions were incorporated inthe Open Mind Word Expert.
For each annotationtask, the contributors could choose from this list of39 words.
For each chosen word, the system dis-plays the associated senses, together with their def-initions, and a short (1-4 words) description of thesense.
After the user gets familiarized with thesesenses, the system displays each example sentence,and the list of senses together with their short de-scription, to facilitate the tagging process.For the coarse grained WSD task, we had the op-tion of using the grouping provided by the dictio-nary.
A manual analysis however showed that someof the senses in the same group are quite distinguish-able, while others that were separated were verysimilar.For example, for the word circulatie (roughly, cir-culation).
The following two senses are grouped inthe dictionary:2a.
movement, travel along a communicationline/way2b.
movement of the sap in plants or the cytoplasminside cellsSense 2a fits better with sense 1 of circulation:1. the event of moving aboutwhile sense 2b fits better with sense 3:3. movement or flow of a liquid, gas, etc.
within acircuit or pipe.To obtain a better grouping, a linguist clusteredthe similar senses for each word in our list of forty.The average number of senses for each class is al-most halved.Notice that Romanian is a language that uses dia-critics, and the the presence of diacritics may be cru-cial for distinguishing between words.
For examplepeste without diacritics may mean fish or over.
Inchoosing the list of words for the Romanian WSDtask, we have tried to avoid such situations.
Al-though some of the words in the list do have dia-critics, omitting them does not introduce new ambi-guities.4 CorpusExamples are extracted from the ROCO corpus, a400 million words corpus consisting of a collectionof Romanian newspapers collected on the Web overa three years period (1999-2002).The corpus was tokenized and part-of-speechtagged using RACAI?s tools (Tufis, 1999).
The to-kenizer recognizes and adequately segments variousconstructs: clitics, dates, abbreviations, multiwordexpressions, proper nouns, etc.
The tagging fol-lowed the tiered tagging approach with the hiddenlayer of tagging being taken care of by ThorstenBrants?
TNT (Brants, 2000).
The upper level ofthe tiered tagger removed from the assigned tags allthe attributes irrelevant for this WSD exercise.
Theestimated accuracy of the part-of-speech tagging isaround 98%.5 Sense Tagged DataWhile several sense annotation schemes have beenpreviously proposed, including single or dual anno-tations, or the ?tag until two agree?
scheme used dur-ing SENSEVAL-2, we decided to use a new schemeand collect four tags per item, which allowed usto conduct and compare inter-annotator agreementevaluations for two-, three-, and four-way agree-ment.
The agreement rates are listed in Table 3.The two-way agreement is very high ?
above 90%?
and these are the items that we used to build theannotated data set.
Not surprisingly, four-way agree-ment is reached for a significantly smaller number ofcases.
While these items with four-way agreementwere not explicitly used in the current evaluation,we believe that this represents a ?platinum standard?data set with no precedent in the WSD research com-munity, which may turn useful for a range of futureexperiments (for bootstrapping, in particular).Agreement type Total (%)TOTAL ITEMS 11,532 100%At least two agree 10,890 94.43%At least three agree 8,192 71.03%At least four agree 4,812 41.72%Table 3: Inter-agreement rates for two-, three-, andfour-way agreementTable 2 lists the target words selected for this task,together with their most common English transla-tions.
For each word, we also list the number ofsenses, as defined in the DEX sense inventory (col-locations included), and the number of annotated ex-amples made available to task participants.Word Main English senses senses Train Test Word Main English senses senses Train Testtranslation (fine) (coarse) size size translation (fine) (coarse) size sizeNOUNSac needle 16 7 127 65 accent accent 5 3 172 87actiune action 10 7 261 128 canal channel 6 5 134 66circuit circuit 7 5 200 101 circulatie circulation 9 3 221 114coroana crown 15 11 252 126 delfin doplhin 5 4 31 15demonstratie demonstration 6 3 229 115 eruptie eruption 2 2 54 27geniu genius 5 3 106 54 nucleu nucleus 7 5 64 33opozitie opposition 12 7 266 134 perie brush 5 3 46 24pictura painting 5 2 221 111 platforma platform 11 8 226 116port port 7 3 219 108 problema problem 6 4 262 131proces process 11 3 166 82 reactie reaction 7 6 261 131stil style 14 4 199 101 timbru stamp 7 3 231 116tip type 7 4 263 131 val wave 15 9 242 121valoare value 23 9 251 125VERBScistiga win 5 4 227 115 citi read 10 4 259 130cobori descend 11 6 252 128 conduce drive 7 6 265 134creste grow 14 6 209 103 desena draw 3 3 54 27desface untie 11 5 115 58 fierbe boil 11 4 83 43indulci sweeten 7 4 19 10ADJECTIVESincet slow 6 3 224 113 natural natural 12 5 242 123neted smooth 7 3 34 17 oficial official 5 3 185 96simplu simple 15 6 153 82Table 2: Target words in the SENSEVAL-3 Romanian Lexical Sample taskTeam System name Reference (this volume)Babes-Bolyai University, Cluj-Napoca (1) ubb nbc ro (Csomai, 2004)Babes-Bolyai University, Cluj-Napoca (2) UBB (Serban and Tatar, 2004)Swarthmore College swat-romanian (Wicentowski et al, 2004a)Swarthmore College / Hong Kong Polytechnic University swat-hk-romanian (Wicentowski et al, 2004b)Hong Kong University of Science and Technology romanian-swat hk-boUniversity of Maryland, College Park UMD SST6 (Cabezas et al, 2004)University of Minnesota, Duluth Duluth-RomLex (Pedersen, 2004)Table 4: Teams participating in the SENSEVAL-3 Romanian Word Sense Disambiguation taskIn addition to sense annotated examples, partici-pants have been also provided with a large numberof unlabeled examples.
However, among all partici-pating systems, only one system ?
described in (Ser-ban and Ta?tar 2004) ?
attempted to integrate this ad-ditional unlabeled data set into the learning process.6 Participating SystemsFive teams participated in this word sense disam-biguation task.
Table 4 lists the names of the par-ticipating systems, the corresponding institutions,and references to papers in this volume that providedetailed descriptions of the systems and additionalanalysis of their results.There were no restrictions placed on the numberof submissions each team could make.
A total num-ber of seven submissions was received for this task.Table 5 shows all the submissions for each team, andgives a brief description of their approaches.7 Results and DiscussionTable 6 lists the results obtained by all participatingsystems, and the baseline obtained using the ?mostfrequent sense?
(MFS) heuristic.
The table lists pre-cision and recall figures for both fine grained andcoarse grained scoring.The performance of all systems is significantlyhigher than the baseline, with the best system per-forming at 72.7% (77.1%) for fine grained (coarsegrained) scoring, which represents a 35% (38%) er-ror reduction with respect to the baseline.The best system (romanian-swat hk-bo) relies ona Maximum Entropy classifier with boosting, usinglocal context (neighboring words, lemmas, and theirpart of speech), as well as bag-of-words features forsurrounding words.Not surprisingly, several of the top perform-ing systems are based on combinations of multi-ple sclassifiers, which shows once again that votingSystem Descriptionromanian-swat hk-bo Supervised learning using Maximum Entropy with boosting, using bag-of-wordsand n-grams around the head word as featuresswat-hk-romanian The swat-romanian and romanian-swat hk-bo systems combined with majority voting.Duluth-RLSS An ensemble approach that takes a vote among three bagged decision trees,based on unigrams, bigrams and co-occurrence featuresswat-romanian Three classifiers: cosine similarity clustering, decision list, and Naive Bayes,using bag-of-words and n-grams around the head word as featurescombined with a majority voting scheme.UMD SST6 Supervised learning using Support Vector Machines, using contextual features.ubb nbc ro Supervised learning using a Naive Bayes learning scheme, and features extractedusing a bag-of-words approach.UBB A k-NN memory-based learning approach, with bag-of-words features.Table 5: Short description of the systems participating in the SENSEVAL-3 Romanian Word Sense Disam-biguation task.
All systems are supervised.Fine grained Coarse grainedSystem P R P Rromanian-swat hk-bo 72.7% 72.7% 77.1% 77.1%swat-hk-romanian 72.4% 72.4% 76.1% 76.1%Duluth-RLSS 71.4% 71.4% 75.2% 75.2%swat-romanian 71.0% 71.0% 74.9% 74.9%UMD SST6 70.7% 70.7% 74.6% 74.6%ubb nbc ro 71.0% 68.2% 75.0% 72.0%UBB 67.1% 67.1% 72.2% 72.2%Baseline (MFS) 58.4% 58.4% 62.9% 62.9%Table 6: System results on the Romanian Word Sense Disambiguation task.schemes that combine several learning algorithmsoutperform the accuracy of individual classifiers.8 ConclusionA Romanian Word Sense Disambiguation taskwas organized as part of the SENSEVAL-3 eval-uation exercise.
In this paper, we presentedthe task definition, and resources involved, andshortly described the participating systems.
Thetask drew the participation of five teams, and in-cluded seven different systems.
The sense an-notated data used in this exercise is availableonline from http://www.senseval.org andhttp://teach-computers.org.AcknowledgmentsMany thanks to all those who contributed to theRomanian Open Mind Word Expert project, mak-ing this task possible.
Special thanks to Bog-dan Harhata, from the Institute of Linguistics Cluj-Napoca, for building a coarse grained sense map.We are also grateful to all the participants in thistask, for their hard work and involvement in thisevaluation exercise.
Without them, all these com-parative analyses would not be possible.ReferencesT.
Brants.
2000.
Tnt - a statistical part-of-speechtagger.
In Proceedings of the 6th Applied NLPConference, ANLP-2000, Seattle, WA, May.T.
Chklovski and R. Mihalcea.
2002.
Building asense tagged corpus with Open Mind Word Ex-pert.
In Proceedings of the Workshop on ?WordSense Disambiguation: Recent Successes and Fu-ture Directions?, ACL 2002, Philadelphia, July.I.
Coteanu, L. Seche, M. Seche, A. Burnei,E.
Ciobanu, E.
Contras?, Z. Cret?a, V. Hristea,L.
Mares?, E.
St?
?ngaciu, Z.
S?tefa?nescu, T. T?ugulea,I.
Vulpescu, and T. Hristea.
1975.
Dict?ionarulExplicativ al Limbii Roma?ne.
Editura AcademieiRepublicii Socialiste Roma?nia.D.
Tufis.
1999.
Tiered tagging and combined classi-fiers.
In Text, Speech and Dialogue, Lecture Notesin Artificial Intelligence.
