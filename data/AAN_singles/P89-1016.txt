THE EFFECTS OF INTERACTION ON SPOKEN DISCOURSESharon L. OviattPhilip It.
CohenArtificial Intelligence CenterSItI International333 Ravenswood AvenueMenlo Park, California 94025-3493ABSTRACTNear-term spoken language systems will likelybe limited in their interactive capabilities.
Todesign them, we shall need to model how thepresence or absence of speaker interaction in-fluences spoken discourse patterns in differenttypes of tasks.
In this research, a comprehensiveexamination is provided of the discourse struc-ture and performance efficiency of both interac-tive and noninteractive spontaneous speech in aseriated assembly task.
More specifically, tele-phone dialogues and audiotape monologues arecompared, which represent opposites in terms ofthe opportunity for confirmation feedback andclarification subdialognes.
Keyboard communi-cation patterns, upon which most natural lan-guage heuristics and algorithms have been based,also are contrasted with patterns observed in thetwo speech modalities.
Finally, implications arediscussed for the design of near-term limited-interaction spoken language systems.INTRODUCTIONMany basic issues need to be addresssed be-fore technology will be able to leverage suc-cessfully from the natural advantages of speech.First, spoken interfaces will need to be struc-tured to reflect the realities of speech insteadof text.
Historically, language norms have beenbased on written modalities, even though spo-ken and written communication differ in majorways (Chafe, 1982; Chapanis, Parrish, Ochsman,& Weeks, 1977).
Furthermore, it has becomeclear that the algorithms and heuristics needed todesign spoken language systems will be differentfrom those required for keyboard system s (Co-hen, 1984; Hindle, 1983; Oviatt & Cohen, 1988 ~:1989; Ward, 1989).
Among other things, speechunderstanding systems tend to have considerabledifficulty with the indirection, confirmations andreaffirmations, nonword fillers, false starts andoverall wordiness of human speech (van Katwijk,van Nes, Bunt, Muller & Leopold, 1979).
Todate, however, esearch as not yet provided ac-curate models of spoken language to serve as abasis for designing future spoken language sys-tems.People experience speech as a very rapid,direct, and tightly interactive communicationmodality, one that is governed by an array ofconversational rules and is rewarding in its so-cial effectiveness.
Although afull.
y interactive ex-change that includes confirmatory feedback andclarification subdialo~mes is the prototypical ornetural form of speech, near-term spoken lan-guage systems are likely to provide only limitedinteractive capabilities.
For example, lack of ad-equate confirmatory feedback, variable delays ininteractive processing, and limited prosodic anal-ysis all can be expected to constrain interactionswith initial systems.
Other speech technology,such as voice mail and automatic dictation de-vices (Gould, Conti & Hovanyecz, 1983; Jelinek,1985), isdesigned specifically for noninteractivespeech input.
Therefore, to the extent hat inter-active and noninteractive spoken language differ,future SLSs may require tailoring to handle phe-nomena typical of noninteractive speech.
Thatis, at least for the near term, the goal of design-ing SLSs based on models of fully interactive di-alogne may be inappropriate.
Instead, buildingaccurate speech models for SLSs may depend on126an examination ofthe discourse and performancecharacteristics of both interactive and noninter-active spoken language in different types of tasks.Unfortunately, little is known about how theopportunity for interactive feedback actually in-fluences a spoken discourse.
To begin exam-ining the influence of speaker interaction, thepresent research aimed to investigate the maindistinctions between interactive and noninterac-rive speech in a hands-on assembly task.
Morespecifically, it explored the discourse and perfor-mance features of telephone dialogues and audio-tape monologues, which represent opposites onthe spectrum of speaker interaction.
Since key-board is the modality upon which most currentnatural anguage heuristics and algorithms arebased, the discourse and performance patternsobserved in the two speech modalities also werecontrasted with those of interactive keyboard.Modality comparisons were performed for teamsin which an expert instructed a novice on how toassemble a hydraulic water pump.
A hands-onassembly task was selected since it has been con-jectured that speech may demonstrate a specialefficiency advantage for this type of task.One purpose of this research was to providea comprehensive analysis of differences betweenthe interactive and noninteractive speech modal-ities in discourse structure, referential charac-teristics, and performance efficiency.
Of these,the present paper will focus on the predominantreferential differences between the two speechmodes.
A fuller treatment of modality distinc-tions is provided elsewhere (Oviatt & Cohen,1988).
Another goal involved outlining patternsin common between the two speech modalitiesthat differed from keyboard.
A further objectivewas to consider the implications of any observedcontrasts among these modalities for the designof prospective speech systems that are habitable,high quality, and relatively enduring.
Since fu-ture SLSs will depend in part on adequate modelsof spoken discourse, a final goal of this researchwas to begin constructing a theoretical modelfrom which several principal features of interac-tive and noninteractive speech could be derived.For a discussion of the theoretical model, whichis beyond the scope of the present research sum-mary, see Oviatt & Cohen (1988).METHODThe data upon which the present manuscript isbased were originally collected as part of a largerstudy on modality differences in task-orientedcommunication.
This project collected exten-sive audio and videotape data on the commu-nicative exchanges and task assembly in five dif-ferent modalities.
It has provided the basis for aprevious research report (Cohen, 1984) that com-pared communicative indirection and illocution-ary style in the keyboard and telephone condi-tions.
As indicated above, the present researchfocused on a comprehensive assessment of thediscourse and performance features of speech.More specifically, it compares noninteractive au-diotape and interactive telephone.Thirty subjects, fifteen experts and fifteennovices, were included in the analysis for thepresent study.
The fifteen novices were ran-domly assigned to experts to form a total of fif-teen expert-novice pairs.
For five of the pairs,the expert related instructions by telephone andan interactive dialogue ensued as the pump wasassembled.
For another five pairs, the expert'sspontaneous spoken instructions were recordedby audiotape, and the novice later assembled thepump as he or she listened to the taped mono-logue.
In this condition, there was no oppor-tunity for the audiotape speakers and listenersto confirm their understanding as the task pro-gressed, or to engage in clarification subdialogueswith one another.
For the last five pairs, theexpert typed instructions on a keyboard, and atyped interactive exchange then took place be-tween the participants on linked CRTs.
All threecommunication modalities involved spatial dis-placement of the participants, and participationin the noninteractive audiotape mode also wasdisjoint temporally.
The fifteen pairs of partici-pants were randomly assigned to the telephone,audiotape, and keyboard conditions.Each expert participated in the experimenton two consecutive days, the first for training127and the second for instructing the novice part-ner.
During training, experts were informed thatthe purpose of the experiment was to investigatemodality differences in the communication f in-structions.
They were given a set of assemblydirections for the hydraulic pump kit, along witha diagram of the pump's labeled parts.
Approxi-mately twenty minutes was permitted for the ex-pert to practice putting the pump together usingthese materials, after which the expert practicedadministering the instructions to a research as-sistant.
During the second session, the expertwas informed of a modality assignment.
Thenthe expert was asked to explain the task to anovice partner, and to make sure that the part-ner built the pump so that it would function cor-rectly when completed.
The novice received sim-ilar instructions regarding the purpose of the ex-periment, and was supplied with all of the pumpparts and a tray of water for testing.Written transcriptions were available as ahard copy of the keyboard exchanges, and werecomposed from audio-cassette r cordings of themonologues and coordinated dialogues, the latterof which had been synchronized onto one audiochannel.
Signal distortion was not measured forthe two speech modalities, although no subjectsreported difficulty with inaudible or unintelligi-ble instructions, and < 0.2% or 1 in 500 of therecorded words were undecipherable to the tran-scriber and experimenter.
All dependent mea-sures described in this research had interraterreliabilities ranging above .86, and all discourseand performance differences reported among themodal\]ties were statistically significant based oneither apriori t or Fisher's exact probability tests(Siegel, 1956).RESULTS AND DISCUSSIONwell as averaging significantly longer.
In ad-dition, repetitions were significantly more com-mon in the audiotape modality, in comparisonwith interactive telephone and keyboard.
Al-though noninteractive speech was more elab-orated and repetitive than interactive speech,these two speech modes did not differ in the totalnumber of words used to convey instructions.Noninteractive monologues also displayed anumber of unusual elaborative patterns.
In thetelephone modality, the prototypical pattern ofpresentation i volved escribing one pump piece,a second piece, and then the action required toassemble them.
In contrast, an initial audiotapepiece description often continued to be elabo-rated even after the expert had described themain action for assembling the piece.
The follow-ing two examples illustrate this audiotape pat-tern of perseverative piece description:"So the first thing to do is to take themetal rod with the red thing on one endand the green cap on the other end.Take that and then look in the other parts - -there are three small red pieces.Take the smallest one.It looks like a nail -- a little red nail --and put that into the holein the end of the green cap.There's a green cap on the end of thesilver ~hing.
~"...Now, the curved tube that you justput in that should be pointing up stillTake that, uh m Take the the cylinder that'sleft over m it's the biggest piece that's left over mand place that on top of that, fit that intothat curved tube that you just put on.This piece tha~ I'm talking about is hasa blue base on it and it's a round tube..."Compared to interactive telephone dialoguesand keyboard exchanges, the principal referen-tial distinction of the noninteractive monologueswas profuse elaborative description.
Audiotapeexperts' elaborations of piece and action descrip-tions, which formed the essence of these task in-structions, were significantly more frequent, asThese piece elaborations that followed themain assembly action were significantly morecommon in the audiotape modality.
However,the frequency of piece elaborations in the moreprototypical location preceding specification ofthe action did not differ significantly between theaudiotape and telephone modes.128Another phenomenon observed in noninterac-tive audiotape discourse that did not occur atall in interactive speech or keyboard was elab-orative reversion.
Audiotape experts habituallyused a direct and definite style when instruct-ing novices on the assembly of pump pieces.
Forexample, they used significantly more definitedeterminers during first reference to new pumppieces (88% in audiotape, compared with 48% intelephone).
However, after initially introducinga piece in a definite and direct manner, in somecases there was downshifting to an indefinite andindirect elaboration of the same piece.
All casesof reverted elaborations were presented as exis-tential statements, in which part or all of thesame phrase used to describe the piece was pre-sented again with an indefinite determiner.
Thefollowing are two examples of audiotape rever-sions:"...You take the L-shaped clear plastic tube,another tube, there's an L-shaped onewith a big base...""...you are going to insert that intothe long clear tube with two holes on the side.Okay.
There's a tube about one inch indiameter and about four inches long.Two holes on the side.
~These reversions gave the impression of beingout-of-sequence parenthetical additions which,together with other audiotape dysflueneies likeperseverative piece descriptions, tended to dis-rupt the flow of noninteractive spoken discourse.Partly due to phenomena such as these, thereferential descriptions provided during audio-taped speech simply were less well integrated andpredictably sequenced than descriptions in tele-phone dialogue.
To "begin with, the high rateof audiotape laborations introduced more in-formation for the novice to integrate about apiece.
In addition, perseverative piece descrip-tions required the novice to integrate informationfrom two separate locations in the discourse.
Assuch, they created unpredictability with respectto where piece information was located, and vio-lated expectations for the prototypical placementof piece information.
In the case of both per-severative and reverted piece elaborations, thenovice had to decide whether the reference wasanaphoric, or whether a new piece was being re-ferred to, since these elaborations were eitherdiscontinuous from the initial piece descriptionor began with an indefinite article.
Once estab-lished as anaphoric, the novice then had to suc-cessfully integrate the continued or reverted e-scription with the appropriate arlier one.
Forexample, did it refine or correct the earlier de-scription?
All of these characteristics producedmore inferential strain in the audiotape modality.An evaluation of total assembly time indi-cated that the audiotape novices functioned sig-nificantly less efficiently than telephone novices.Furthermore, the length of novice assembly timedemonstrated a strong positive correlation withthe frequency of expert elaborations, implicat-ing the inefficiency of this particular discoursefeature.
Evidently, experts who elaborated theirdescriptions most extensively were the ones mostlikely to be part of a team in which novice assem-bly time was lengthy.The different patterns observed between inter-active and noninteractive speech may be drivenby the presence or absence of confirmation feed-back.
The literature indicates that access to con-firmation feedback is associated with increaseddialogue efficiency in the form of shorter nounphrases with repeated reference (Krauss & Wein-heimer, 1966).
During the present hands-onassembly interactions, all interactive telephoneteams produced a high and stable rate of con-firmations, with 18% of the total verbal inter-action spent eliciting and issuing confirmations,and a confirmation forthcoming every 5.6 sec-onds.
Confirmations were clearly a major vehi-cle available for the telephone listener to signal tothe expert hat the expert's communicative goalshad been achieved and could now be discharged.Since audiotape xperts had to operate withoutconfirmation feedback from the novice, they hadno metric for gauging when to finish a descriptionand inhibit their elaborations.
Therefore, it wasnot possible for audiotape xperts to tailor a de-129scription to meet the information needs of theirparticular partner most efficiently.
In this sense,their extensive and perseverative elaborating wasan understandably conservative strategy.In spite of the fact that instructions in the twospeech modalities were almost three-fold wordierthan keyboard, novices who received spoken in-structions nonetheless averaged pump assemblytimes that were three times faster than keyboardnovices (cf.
Chapanis, Parrish, Ochsman, &Weeks, 1977).
These data confirm that speechinterfaces may be a particularly apt choice for usewith hands-on assembly tasks, as well as provid-ing some calibration of the overall efficiency ad-vantage.
For a more detailed account of the simi-larities and differences between the keyboard andspeech modalities, see 0viatt & Cohen (1989).IMPL ICAT IONS FOR INTERACTIVESPOKEN LANGUAGE SYSTEMS 1A long-term goal for many spoken languagesystems i the development of fully interactive ca-pabilities.
In practice, of course, speech applica-tions currently being developed are ill equippedto handle spontaneous human speech, and areonly capable of interactive dialogue in a very lim-ited sense.
One example of an ihteractional limi-tation is the fact that system responses typicallyare more delayed than the average human conver-sant.
While the natural speed of human dialoguecreates an efficiency advantage in tasks, it simul-taneously challenges current computing technol-ogy to produce more consistently rapid responsetimes.
In research on telephone conversations,transmission and access delays 2of as little as .25to 1.8 seconds have been found to disrupt thenormal temporal pattern of conversation and toreduce referential efficiency (Krauss ~z Bricker,1967; Krauss, Garlock, Bricker, & McMahon,x For a discussion of the implications of this researchfor non/nteractive speech technology, see Oviatt ~ Cohen(198S).2A transmission delay refers to a relatively pure delayof each speaker's utterances for some defined time period.By contrast, an access delay prevents imultaneous speechby the listener, and then delays circuit access for a definedtime period after the primary speaker ceases talking.1977).
These data reveal that the threshold foran acceptable time lag can be a very brief in-terval, and that even these minimal delays canalter the organization and efficiency of spokendiscourse.Preliminary research on human-computer di-alogue has indicated that, beyond a certainthreshold, language systems slower than real-time will elicit user input that has characteristicsin common with noninteractive speech.
For ex-ample, when system response is slow and promptconfirmations to support user-system interactionare not forthcoming, users will interrupt he sys-tem to elaborate and repeat themselves, whichultimately results in a negative appraisal of thesystem (van Katwijk, van Nes, Bunt, Muller, &Leopold, 1979).
For practical purposes, then,people typically are unable to distinguish be-tween a slow response and no response at all, sotheir strategy for coping with both situations issimilar.
Unfortunately, since system delays typ-ically vary in length, their duration is not pre-dictable from the user's viewpoint.
Under thesecircumstances, it seems unrealistic to expect hatusers will learn to anticipate and accommodatethe new dialogue pace as if it had been reducedby some constant amount.Apart from system delay, another currentlimitation that will influence future interac-tive speech systems is the unavailability of fullprosodic analysis.
Since an interactive systemmust be able to analyze prosodic meaning in or-der to deliver appropriate and timely confirma-tions of received messages, limited prosodic anal-ysis may make the design of an effective confir-mation system more difficult.
In spoken interac-tion, speakers typically convey requests for con-firmation prosodically, and such requests occurmid-sentence as well as at sentence nd.
For ex-ample:130Expert:Novice:Expert:Novice:"Put that on the hol~on the side of that tube --" (pause)"Yeah.""
-- that is nearest to the top ornearest to the green handle.""Okay.
"For a system to analyze and respond to re-quests for confirmation, it would need to detectrising intonation, pausing, and other characteris-tics of the speech signal which, although elemen-tary in appearance, cannot yet be performed ina reliable manner automatically (Pierrehumbert,1983; Walbel, 1988).
A system also would needto derive the contextually appropriate meaningfor a given intonation pattern, by mapping theprosodic structure of an utterance onto a rep-resentation of the speaker's intentions at a par-ticular moment.
Since the pragmatic analysisof prosody barely has begun (Pierrehumbert &Hirschberg, 1989; Waibel, 1988), this importantcapability is unlikely to be present in initial ver-sions of interactive speech systems.
Therefore,the typical prosodic vehicles that speakers useto request confirmation will remain unanalyzedsuch that confirmations are likely to be omitted.. k This may be especially true of rind-sentence on-firmation requests that lack redundant grammat-ical cues to their function.
To the extent thatconfirmation feedback is omitted, speakers' dis-course can be expected to become more elabo-rative, repetitive, and generally similar to mono-logue as they attempt to engage in dialogue withlimited-interaction systems.If supplying apt and precisely timed confir-mations for near-term spoken language systemswill be difficult, then consideration is in orderof the difficulties posed by noninteractive dis-course phenomena for the design of preliminarysystems.
For one thing, the discourse phenom-ena of noninteractive speech differ substantiallyfrom the keyboard discourse upon which cur-rent natural anguage processing algorithms arebased.
Keyboard-based algorithms will requirealteration, especially with respect o referentialfeatures and discourse macrostructure, if design-ers expect future systems to handle spontaneoushuman speech input.
With respect to refer-ence resolution, the system will have to iden-tify whether a perseverative elaboration refersto a new part or a previously mentioned one,whether the initial descriptive expression is beingfurther expanded, qualified, or corrected, and soforth.
The potential difficulty of tracking nounphrases throughout a repetitive and elaborativediscourse, espedally segments that include perse-verative descriptions displaced from one anotherand definite descriptions that revert to indefiniteelaborations about the same part, is illustratedin the following brief monologue segment:"and then you take the L-shaped clear plas-tic tube, another tube, there's an L-shapedone with a big base, and that big base hap-pens to fit over the top of this hole that youjust put the red piece on.
Okay.
So there'sone hole with a blue piece and one with ared piece and you take the one with the redpiece and put the L-shaped instrument ontop of this, so that..."For example, a system must distinguishwhether "another tube" is a new tube or whetherit co-refers with "the L-shaped clear plastic tube"uttered previously, or with the other two itali-cized phrases.
In cases where description of apart persists beyond that of the basic assemblyaction, the system also must determine whethera new discourse assembly segment has been ini-tiated and whether a new action now is beingdescribed.
In the above illustration, the systemmust determine whether "and you take the onewith the red piece and put the L-shaped instru-ment on top of this" refers to a new action, orwhether it refers back to the previously describedaction in "that big base happens to fit over thetop of this hole..." The system's ability to re-solve such co-reference relations will determinethe accuracy with which it interprets the basicassembly actions underway.
To optimize the in-terpretation ofspoken monologues, a system willhave to continually reexamine whether furtherdescriptive information supports or refutes cur-131rent beliefs about part identity and action perfor-mance.
That is, the system's orientation shouldbe geared more toward frequent cross-checking ofprevious information, rather than automaticallypositing new entities and actions.In order to see how current algorithms willneed to be altered to process noninteractivespeech phenomena, we consider how recent di-alogue and text processing systems would fare ifconfronted with such data.
The ability to rec-ognize when and how utterances elaborate uponprevious discourse is a special case of recogniz-ing how speakers intend discourse segments tobe related.
The ARGOT dialogue system (Lit-man & Allen, 1989) takes one important step to-ward recognizing discourse structures by distin-guishing the speaker's domain plan, such as forassembling parts, from his or her discourse plan,such as to clarify which domain plans are beingperformed.
Although there are technical diffi-culties, its "identify parameter" discourse planis designed to process elaborations that furtherspecify the arguments ofrequested actions duringinteractive dialogue.
However, ARGOT wouldhave to be extended to include.a number of newtypes of discourse p\]anA before it would be ableto aa~lyze noninteractive speech phenomena cor-rectly.
For one thing, ARGOT does not distin-guish different ypes of elaboration such that in-formation in the two segments of discourse couldbe integrated correctly.
Also, instead of hav-ing a discourse plan for self-correction, ARGOTfocuses exclusively on a strategy for correctingother agents' plans by means of requesting themto perform remedial actions.
In addition, AR-GOT's current processing scheme is not gearedto handle elaborative requests.
Briefly, ARGOTperforms an action once a sufficiently precise re-quest o perform that action has been recognized.However, since monologue speakers tend to per-sist in attempting to achieve their goals, they es-sentially issue multiple requests for the listenerto perform a particular action.
For example, inthe above audiotape fragment, he speaker triedtwice to get the listener to put the L-shaped pieceover the outlet containing the red valve.
Any sys-tem unable to recognize that the second requestis an elaboration of the first would likely makethe fundamental error of positing the existenceof two separate actions to be performed.Although text processing systems are explic-itly designed to analyze noninteractive discourse,they fail to provide the needed solutions for an-alyzing noninteractive speech.
These systemscurrently have no means for identifying basicdiscourse laborations and, to date, they havenot incorporated discourse structural cues whichcould be helpful in signaling the relationship ofdiscourse segments (Grosz & Sidner, 1986; Lit-man & Allen, 1989; Oviatt & Cohen, 1989; Re-ichman, 1978).
In addition, they are restrictedto declarative sentences.One recent ext analysis ystem called Tacitus(Hobbs, Stickel, Martin & Edwards, 1988) ap-pears uniquely capable of handling some of theelaborative phenomena found in our corpus.
Inselecting the best analysis of a text, Tacitus usesan abductive strategy to search for an interpre-tation that minimizes the overall cost of the setof assu.mptions needed to prove that the text istrue.
The interpretive cost is a weighted func-tion of the individual costs of the assumptionsneeded to derive that interpretation.
Depend-ing on the assignment of costs, it is possible forTacitus to adopt a non-minimal individual as-sumption as part of a globally optimal discourseinterpretation.
Applying this general strategyto noun phrase interpretation, Tacitus' heuristicsfor referring expressions include a higher cost forassuming that a definite noun phrase refers to anew discourse ntity than to a previously intro-duced one, as well as a higher cost for assumingthat an indefinite noun phrase refers to a previ-ously introduced entity than to a new one.
Theseheuristics could handle the prevalent noninterac-tive speech phenomenon of definite first referenceto new pump parts, as well as elaborative re-versions, although both would entail higher-costindividual assumptions.
That is, if it makes themost global sense, the system could interpret def-inite first references and reversions as referring to"new" and "old" entities, respectively, contrary132to the usual preferences in computational linguis-tics.Although such an interpretation strategy maysometimes be sufficient o establish the neededco-reference r lations in elaborative discourses,due to the nature of Tacitus' global optimizationapproach one cannot be certain that any par-ticular case of elaboration will be resolved cor-rectly without first weighing all other local dis-course specifics.
It is neither clear what percent-age of the phenomena would be handled correctlyat present, nor whether Tacitus' heuristics couldbe extended to arrive at consistently correct in-terpretations.
Furthermore, since Tacitus' usualstrategy for determining what should be provenis simply to conjoin the meaning representationsof two utterances, it would fail to provide correctinterpretations for certain types of elaborations,such as corrections in which the latter descrip-tion supercedes an earlier one.
Hobbs (1979) hasrecognized and attempted to define elaborationas a coherence relation in previous work, and iscurrently refining Tacitus' computational meth-ods in a manner that may yield improvements inthe processing of elaborations.CONCLUSIONSIn summary, the present results imply thatnear-term spoken language systems that are un-able to provide meaningful and timely confirma-tions may not be able to curtail speakers' elab-orations effectively, or the related iscourse con-volutions typical of noninteractive speech.
Cur-rent dialogue and text processing systems are notprepared to handle this type of elaborative dis-course.
Clearly, new heuristics will need to bedeveloped to accomodate speakers who try morethan once to achieve their communicative goals,in the process using multiple utterances and var-ied speech acts.
Under these circumstances,models of noninteractive speech may provide amore appropriate basis for designing near-termspoken language systems than either keyboardmodels or models of fully interactive dialogue.To model discourse accurately for interactiveSLSs, further research will be needed to estab-lish the generality of these noninteractive speechphenomena cross different asks and applica-tions, and to determine whether speakers canbe trained to alter these patterns.
In addition,research also will be needed on the extent towhich human-computer ask-oriented speech dif-fers from that between humans.
At present, hereis no well developed iscourse theory of human-machine communication, and the few studiescomparing human-machine with human-humancommunication have focused on the keyboardmodality, with the exception of Hauptmann &Rudnicky (1988).
These studies also have reliedexclusively on the Wizard of Oz paradigm, al-though this technique ntails unavoidable feed-back delays due to the inherent deception, and itwas never intended to simulate the interactionalcoverage of any particular system.
Further workideally would examine human-computer speechpatterns as prototypes of interactive SLSs be-come available.In short, our present research findings implythat designers of future spoken language sys-tems should be vigilant to the possibility thattheir selected application may elicit noninterac-tive speech phenomena, nd that these patternsmay have adverse consequences for the technol-ogy proposed.
By anticipating or at least recog-nizing when they occur, designers will be betterprepared to develop speech systems based on ac-curate discourse models, as well as ones that areviable ergonomically.ACKNOWLEDGMENTSThis research was supported by the NationalInstitute of Education under contract US-NIE-C-400-76-0116 to the Center for the Study of l~ead-ing at the University of Illinois and Bolt Beranekand Newman, Inc., and by a contract from ATRInternational to SPd International.References\[11 Chapanis A., R. N. Parrish, R. B. Ochsman, andG.
D. Weeks.
Studies ifi interactive communi-cation: If.
The effects of four communicationmodes on the linguistic performance of teams133during cooperative problem solving.
Human Fac.tots, 19(2):101-125, 1977.\[2\] W. L. Chafe.
Integration and involvement inspeaking, writing, and oral literature.
In D. Tan-nun, editor, Spoken and Written Language: Ez-ploring Oralit~/ and Literacy, chapter 3, pages35-53.
Ablex Publishing Corp., Norwood, NewJersey, 1982.\[3\] P. R. Cohen.
The pragmatics of referring andthe modality of communication.
ComputationalLinguistics, 10(2):97-146, 1984.\[4\] J. D. Gould, J. Conti, and T. Hovanyeez.
Com-posing letters with a simulated listening type-writer.
Communications of the ACM, 26(4):295-308, April 1983.\[5\] B. J. Grosz and C. L. Sidner.
Attention,intentions, and the structure of discourse.Computational Linguistics, 12(3):175--204, July-September 1986.\[6\] A. G. Hauptmann and A. I. Rudnicky.
Talking tocomputers: An empirical investigation.
Interna-tional Journal of Man-Machine Studies, 28:583-604, 1988.\[7\] D. Hindle.
Deterministic parsing of syntacticnon-flueneies.
In Proceedings of the ~1s1.
An-nual Meeting of the Association for Computa-tional Linguistics, pages 123-128, Cambridge,Massachusetts, June 1983.\[8\] J. Hobbs.
Coherence and coreferenee.
CognitiveScience, 3(I):67-90, 1979.\[9\] J. R. Hobbs, M. Stickel, P. Martin, and D. Ed-wards.
Interpretation asabduction.
In Proceed-ings of the ~6th Annual Meeting of the Associ-ation for Computational Linguistics, pages 95-103, Buffalo, New York, 1988.\[10\] F. Jelinek.
The development of an experimentaldiscrete dictation recognizer.
Proceedings of theIEEE, 73(11):1616-1624, November 1985.\[11\] R. M. Krauss and P. D. Bricker.
Effects oftransmission delay and access delay on the ef-ficiency of verbal communication.
The Journalof the Acoustical Societ~/ of America, 41(2):286-292, 1967.\[12\] R. M. Krauss, C. M. Garlock, P. D. Bricker, andL.
E. McMahon.
The role of audible and visibleback-channel responses in interpersonal commu-nication.
Journal of Personality and Social Pupchology, 35(7):523-529, 1977.\[13\] R. M. Krauss and S. Weinheimer.
Concur-rent feedback, confirmation, and the encodingof referents in verbal communication.
Journalof Personality and Social Psychology, 4(3):343-346, 1966.\[14\] D. J. Litman and J. F. Alien.
Discourse pro-ceasing and commonsense plans.
In P. R. Co-hen, J. Morgan, and M. E. Pollack, editors, In-tentions in Communication.
M.I.T.
Press, Cam-bridge, Massachusetts, 1989.\[15\] S. L. Oviatt and P. R. Cohen.
Discourse struc-ture and performance efficiency in interactiveand noninteractive spoken modalities.
TechnicalReport 454, Artificial Intelligence Center, SRIInternational, Menlo Park, California, 1988.\[16\] S. L. Oviatt and P. R. Cohen.
The contribut-ing influence of speech and interaction on humandiscourse patterns.
In J. W. Sullivan and S. W.Tyler, editors, Architectures for Intelligent Inter-faces: Elements and Prototypes.
Addison-WesleyPublishing Co., Menlo Park, California, 1989.\[17\] J. Pierrehumbert.
Automatic recognition of in-tonation patterns.
In Proceedings of the 21stAnnual Meeting of the Association for Compu-tational Linguistics, pages 85-90, Cambridge,Massachusetts, June 1983.\[18\] J. Pierrehumbert and J. Hirschberg.
The mean-in 8 of intonational contours in the interpretata-tion of discourse.
In Intentions in Communica-tion.
Bradford Books, M.I.T.
Press, Cambridge,Massachusetts, 1989.\[19\] R. Reichman.
Conversational coherency.
Cogni-tive Science, 2(4):283-328, 1978.\[20\] S. Siegel.
Nonparametric Methods for the Be-havioral Sciences.
McGraw-Hill Publishing Co.,New York, New York, 1956.\[21\] A. F. VanKatwijk, F. L. VanNes, H. C. Bunt,H.
F. Muller, and F. F. Leopold.
Naive subjectsinteracting with a conversing information sys-tem.
IPO Annual Progress Report, Eindhoven,Netherlands, 14:105--112, 1979.\[22\] A. Waibel.
Prosody and Speech Recognition.
Pit-man Publishing, Ltd., London, U. K., 1988.\[23\] W. Ward.
Understanding spontaneous speech.In Proceedings of the Darpa Speech and Natu-ral Language Workshop, February 1989, MorganKaufman Publishers, Inc., Los Altos, California.134
