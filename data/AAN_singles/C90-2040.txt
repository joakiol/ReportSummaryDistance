FINITE-STATE PARSING AND DISAMBIGUATIONKimmo Koskenn iemiUniversity of HelsinkiDepartment of General LinguisticsHallituskatu 1100100 Helsinki, FinlandASS ,ACrA language-independent method of finite-state surface syntactic parsing and word-dis-ambiguation is discussed.
Input sentences arerepresented as finite-state networks alreadycontaining all possible roles and interpretationsof its units.
Also syntactic constraint rules arerepresented as finite-state machines whereeach constraint excludes certain types of un-grammatical readings.
The whole grammar isan intersection of its constraint rules and ex-cludes all ungrammatical possibilities leavingthe correct interpretation(s) of the sentence.The method is being tested for Finnish, Swedishand English.INTRODUCTIONThe present approach is surface oriented andshallow, and it does not aim to uncover seman-tically oriented distinctions.
An important sourceof inspiration has been Fred Karlsson's syntacticparser for Finnish, FPARSE (1985).
The presentapproach tries to formalize the underlying ideasof that parser in a finite-state framework (cf.Karlsson 1989a,b).
The finite-state formalism at-tacks the very basic things in syntax such as:what are the correct readings of ambiguouswords, what are the clauses in a complex sen-tence, how the words form constituents, andwhat are the syntactic roles of the constituents.Let us consider the full framework of automaticsyntactic parsing.
One possible partition of thewhole process is given in the following figure 1.The morphological analysis is done eg.
byusing the two-level model (Koskenniemi 1983).Comprehensive systems exist now for Finnish,Swedish, English and Russian (about 30-40,000root entries in each), and some twenty smallerones.IIr .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.L .
.
.
.
.
.
.
.
.
.
.
.
_nor _malize_cl_ sentences .
.
.
.
.
.
.
.
.
.
.
.
.
_~IMORPHOLOGICAL ANALYSIS (TWOL)\]Ir .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1: analyzed words with all interpretations (and alJ :i .
?
, i possible syntactic funct=ons) ,u .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
JI\[LOCAL DISAMBIGUATION !Ir .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.\[ analyzed words with feasible interpretations iL__ !nonp\[efe/red a_nd_im _proba_ble o_ ne_ s d) s_car_d_e._d_)__ jiIr .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1i disambiguated sentence (clause boundaries, :, correct senses and syntactic functions elected) :L .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
JFigure 1.The local disambiguation is an essential stepeg.
in Swedish, because many longer word-forms have several possible interpretations.
Inpart, the local disambiguation supplements thetwo-level description by imposing more sophis-t icated restrictions on eg.
compounds, and byreducing redundant or duplicate analyses (eg,in case a derived word both exists as a givenlexicalized entry and is productively generatedfrom its root), The remaining logic concernsweighing various alternatives and excludingreadings which are significantly less probablethan the best ones.i 229FINITE-SI'ATE SYNTAXThe actual finite-state syntax consists of threecomponents:?
Syntactic disambiguation of word-formswhich have multiple interpretations.?
Determination of clause boundaries.?
Determining the head-modifier elations ofwords and the surface syntactic functionsof the heads.These components are well defined but theydepend on each other in a nontrivial way.
It ismore convenient to write constraint rules fordisambiguation and head-modifier elations ifone can assume that the clause boundaries arealready there.
And conversely, the clauseboundaries are easier to determine if we havethe correct readings of words available.
Theapproach adopted in this paper shows onesolution where one may describe the con-straints freely, ie.
one may act as if the othermodules had already done their work.Representation of sentencesThe way we have chosen in order to solve thisinterdependence, relies on the representationof sentences and the constraint rules.
Each sen-tence is represented as a finite-state machine(fsm) that accepts all possible readings of thesentence.
The task of the grammar is to acceptthe correct reading(s) and exclude incorrectones.
In a reading we include:?
One interpretation of each word-form.?
One possible type of clause boundary or itsabsence for each word boundary.?
One possible syntactic tag for each word.An example of a sentence in this repre-sentation is given in figure 2 on the next page.In the input sentence each word is representedas an analysis given by the morphological ana-lyzer.
The representation consists of one or mo;einterpretations, and each interpretation, in turn,of a base form and a set of morphosyntacticfeatures, eg.
"katto" N ELA SG.Word and clause boundariesFor word boundaries we have four possibilities:@@ A sentence boundary, which occurs onlyat the very beginning and end of thesentence (and is the only possibility there).
@ A normal word boundary (where there isno clause boundary).
@/ A clause boundary separating twoclauses, where one ends and the otherstarts.
@< Beginning of a center embedding, wherethe preceding clause continues after theembedding has been completed.
@> End of a center embedding.Each word is assumed to belong to exactlyone clause.
This is taken strictly as a formal basisand implies that words in a subordinate clauseonly belong to the subordinate clause, not totheir main clause.
Furthermore, this implies avery flat structure to sentences.
Tail recursion istreated as iteration.There has been a long dispute on the finite-state property of natural languages.
We haveobserved that one level of proper center em-bedding is fairly common in our corpuses andthat these instances also represent normal andunmarked language usage.
We do not insist onthe absence of a second or third level of centerembedding.
We only notice that there are veryfew examples of these in the corpuses, andeven these are less clear examples of normalusage.The present version of the finite-state syntaxaccepts exactly one level of center embed-ding.
The formalism and the implementationcan be extended to handle a fixed number ofrecursive center-embeddings, but we will notpursue it further here.Grammatical tagsOne grammatical tag is attached with eachword.
Tags for heads indicate the syntactic roleof the constituent, eg.
MAIN-PRED, SUBJ, OBJ,ADV, PRED-COMP, and tags for modifiers reflectthe part of speech of the head and the direc-tion where it is located, eg.
No, <-N.This kind of simple tagging induces a kind of aconstituent structure to the sentence closelyresembling classical parsing.GRAMMARThe proposed grammar constructs no analysisfor input sentences.
Instead, the grammar ex-cludes the incorrect readings.
The ultimate re-suit of the parsing is already present as onereading in the initial representation of the sen-tence which acts as an input to the parser.
Theresult is just hidden among a large number ofincorrect readings.Input sentencesThe following is an example of a sentence"kalle voisi uida paljonkin" (English glosses 'Char-230 2les could swim much+also') to be input to thefinite-state syntax:(@@"ka l le"  PROP N NOM SG (/ SUBJ  OBJPRED-COMP )@ @< @> @I)( "vo ida"  VCHAIN V COND ACTSG3 MAIN-PRED)("vo:Lda" VCHAIN V COND ACT NEG))@ 0< @> @t)(/ ( "u ida"  V INF I  NOM)( "u ida"  V PRES PSS  NEG))(I @ @< @> 01)(/ ( "pa l jon"  ADV kin)( "pa l jon"  AD-A  k in) )@0)Figure 2This is an expression standing for a finite-statenetwork.
Alternatives are denoted by lists of theform:(/ (alterr~,at ive - \ ]  )(a l te rnat \ ]ve -2)o .
.
)The input expression lists thus some 256 distinctreadings in spite of its concise appearance.
(The input is here still simplified because of theomission of the syntactic function tags.
)Constraint rulesEach constraint is formulated as a readablestatement expressing some necessity in allgrammatical sentences, eg.
:NEG .... > NEGV ..This constraint says that if we have an occur-rence of a feature NEG (denoting a negativeform of a verb), then we must also have afeature NEGV (denoting a negation) in thesame clause.
".." denotes arbitrary features andstems, excluding clause boundaries except forfull embeddings.Types of constraint rulesSeveral types of constraint rules are needed:.
Tect~nical constraints for feasible clausebracketing.- Disambiguation rules (eg.
imperatives onlyin sentence initial positions, negative formsrequire a negation, AD-A requires an adjec-tive or adverb to follow; etc.)?
Clause boundary constraints (relative pro-nouns and certain conjunct ions arepreceded by a boundary, even otherboundaries need some explicit clue to jus-tify their possibility).o Even/clause may have at most one finiteverb and (roughlyspeaking) also must haveone finite verb.Examples of constraint rulesThe following rule constrains the occurrenceinfinitives by requiring that they must bepreceded by a verb taking an infinitive comple-ment (signalled by the feature VCHAIN).INF I  NOM : :>  VCHAIN .oImperatives should occur only at the begin-ning of a sentence.
A coordination of two ormore imperatives is also permitted (if the firstimperative is sentence initial):IMPV :=>\[@@ I IMPV .
@/ , \[ COMMAI COORD\ ]  \] .
(Here COMMA is a feature associated with thepunctuation token, and COORD a feature pres-ent in coordinating conjunctions.
)The following disambiguation rule requires thatmodifiers of adjectives and adverbs must havetheir head present:AD-A  :=> .
@ , \[A I ADV\]For clause boundaries we need a small set ofconstraint rules.
Part of them specify that in cer-tain contexts (such as before relative pronounsor subjunctions) there must be a boundary.
Theremaining rules specify converse constraints, ie.what kinds of clues must be present in order fora clause boundary to be present.All these constraints are ult imately im-plemented as finite-state machines which dis-card he corresponding ungrammatical read-ings.
All constraint-automata together leave(hopefully) exactly one grammatical reading,the correct one.
The grammar as a whole islogically an intersection of all constraints where-as the process of syntactic analysis correspondsto the intersection of the grammar and the inputsentence.OutputWith a very small grammar consisting of abouta dozen constraint rules, the input sentencegiven in the above example is reduCed into thefollowing result:(@@ "ka l le"  PROP N NOM SG SUBJ@ "vo ida  I' VCHAIN V COND ACTSG3 MAIN-PRED@ "u ida"  V INF I  NOM@ "pa l jon"  AD k in@@)3 231MonotonicityThe formalism and implementation proposedfor the finite-state syntax is monotonic in thesense that no information is ever changed.
Eachconstraint simply adds something to the discrimi-nating power of the whole grammar.
No con-straint rule may ever forbid something thatwould later on be accepted as an exception.This, maybe, puts more strain for the grammarwriter but gives us better hope of understandingthe grammar we write.IMPLEMENTATIONThe constraint rules are implemented by usingRan Kaptan's finite-state package.
In the pre-liminary phase constraints are hand-coded intoexpressions which are then converted into fsm's.We have planned to construct a compilerwhich would automatically translate rules in theproposed formalism into automata like the oneused for morphological two-level rules (Kart-tunen et al 1987).The actual run-time system needs only a veryrestricted set of finite-state operations, intersec-tion of the sentence and the grammar.
Thegrammar itself might be represented as onelarge intersection or as several smaller oneswhich are intersected in parallel.
The sentenceas a fsm is of a very restricted class of finite-statenelworks which simplifies the run-time process.An alternative and obvious framework for im-plementing constraint rules is Prolog whichwould be convenient for the testing phase.
Pro-log would, perhaps, have certain limitations forthe production use of such parsers.ACKNOWLEDGEMENTSThe work has been funded bythe Academy ofFinland and it is a part of the activities of theResearch Unit for Computational Linguistics atthe University of Helsinki.
Special thanks are dueto Ran Kaplan and Lauri Karttunen for the use ofthe finite-state package.REFERENCESKarttunen, L., K. Koskenniemi, and R. Kaplan1987.
A compiler for two-level phonologicalrules.
In: Dalrymple et al Tools for morpho-logical analysis.
Report No.
CSU-87-108, Cen-ter for the Study of Language and Informa-tion, Stanford University,Karlsson, F. 1985.
Parsing Finnish in terms of aprocess grammar.
In: Computational Mor-phosyntax: Report on Research 1981-84.(Ed.)
F. Karlsson, University of Helsinki, Depart-ment of General Linguistics, Publications, No.13.
pp.
137-176.--- 1989a.
Parsing and constraint grammar.
Re-search Unit for Computational Linguistics,University of Helsinki.
Manuscript, 45 pp.--- 1989b.
Constraint grammar as a frameworkfor parsing running text.
Paper submitted toColing'90.Koskenniemi, K. 1983.
Two-level morphology: Ageneral computational model for word-formrecognition and production.
University of Hel-sinki, Department of General Linguistics, Pub-lications, No.
11.232 4
