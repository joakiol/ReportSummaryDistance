Structural Disambiguat ion Based on ReliableEst imat ion of Strength of Associat ionHaodong Wu Eduardo de Paiva AlvesTeiji FurugoriDepartment  of Computer  ScienceUniversity of E lectro-Communicat ions1-5-1, Chofugaoka, Chofu, Tokyo 1828585, JAPAN{wu, ea lves ,  fu rugor i  }@phaeton.
cs .
uec .
ac.
j pAbst rac tThis paper proposes a new class-based methodto estimate the strength of association in wordco-occurrence for the purpose of structural dis-ambiguation.
To deal with sparseness of data,we use a conceptual dictionary as the sourcefor acquiring upper classes of the words relatedin the co-occurrence, and then use t-scores todetermine a pair of classes to be employed forcalculating the strength of association.
We haveapplied our method to determining dependencyrelations in Japanese and prepositional phraseattachments in English.
The experimental re-sults show that the method is sound, effectiveand useful in resolving structural ambiguities.1 In t roduct ionThe strength of association between words pro-vides lexical preferences for ambiguity resolu-tion.
It is usually estimated from statistics onword co-occurrences in large corpora (Hindleand Rooth, 1993).
A problem with this ap-proach is how to estimate the probability ofword co-occurrences that are not observed inthe training corpus.
There are two main ap-proaches to estimate the probability: smoothingmethods (e.g., Church and Gale, 1991; Jelinekand Mercer, 1985; Katz, 1987) and class-basedmethods (e.g., Brown et al, 1992; Pereira andTishby, 1992; Resnik, 1992; Yarowsky, 1992).Smoothing methods estimate the probabil-ity of the unobserved co-occurrences by usingfrequencies of the individual words.
For exam-pie, when eat and bread do not co-occur, theprobability of (eat, bread) would be estimatedby using the frequency of (eat) and (bread).A problem with this approach is that it paysno attention to the distributional characteris-tics of the individual words in question.
Usingthis method, the probability of (eat, bread> and(eat, cars) would become the same when breadand cars have the same frequency.
It is unac-ceptable from the linguistic point of view.Class-based methods, on the other hand, es-timate the probabihties by associating a classwith each word and collecting statistics on wordclass co-occurrences.
For instance, instead ofcalculating the probability of (eat, bread) di-rectly, these methods associate eat with theclass \[ingest\] and bread with tile class \[food\]and collect statistics on the classes \[ingest\] and\[food\].
The accuracy of the estimation dependson the choice of classes, however.
Some class-based methods (e.g., Yarowsky, 1992) associateeach word with a single class without considcr-ing the other words in the co-occurrence.
How-ever, a word may need to be replaced by differ-ent class depending on the co-occurrence.
Someclasses may not have enough occurrences to al-low a reliable estimation, while other classesmay be too general and include too many wordsnot relevant o the estimation.
An alternative isto obtain various classes associated in a taxon-omy with the words in question and select theclasses according to a certain criteria.There are a number of ways to select theclasses used in the estimation.
Weischedel et al(1993) chose the lowest classes in a taxonomy1416for which the association for the co-occurrencecan be estimated.
This approach may result inunreliable estimates, since some of the class co-occurrences used may be attr ibuted to chance.Resnik (1993) selected all pairs of classes corre-sponding to the head of a prepositional phraseand weighted them to bias the computationof the association in favor of higher-frequencyco-occurrences which he considered "more reli-able."
Contrary to this assumption, high fre-quency co-occurrences axe unreliable when theprobabil ity that the co-occurrence may be at-tr ibuted to chance is high.In this paper we propose a class-basedmethod that selects the lowest classes in a tax-onomy for which the co-occurrence confidenceis above a threshold.
We subsequently applythe method to solving structural ambiguitiesin Japanese dependency structures and Englishprepositional phrase attachments.2 Class-based Est imat ion ofSt rength  of Assoc iat ionThe strength of association (SA) may bemeasured using the frequencies of word co-occurrences in large corpora.
For instance,Church and Hanks (1990) calculated SA interms of mutual information between two wordswl and w2:N * f(wl,w2) I(wl, w2) = log2 (1) f(wl)f(w2)here N is the size of the corpus used in the es-timation, f(Wl, w2) is the frequency of the co-occurrence, f(wl) and f(w2) that of each word.When no co-occurrence is observed, SA maybe estimated using the frequencies of wordclasses that contain the words in question.
Themutual information in this case is estimated by:I(CI, C2) = log2 N * f(Cl, C2) (2)f(Cl )f(C2)here Cl and C2 are the word classes that respec-tively contain Wl and w2, f(C1) and f(C2) thenumbers of occurrences of all the words includedin the word classes C1 and C2, and f(C1, C2) isthe number of co-occurrences of the word classesC1 and C2.Normally, the estimation using word classesneeds to select classes, from a taxonomy, forwhich co-occurrences are significant.
We use t-scores for this purpose 1.For a class co-occurrence (C1,C2), the t-score may be approximated by:~ f(C,,C2) - -~f(Cl)f(C2) (3)J/(c,,c2)We use the lowest class co-occurrence forwhich the confidence measured with t-scores isabove a threshold 2.
Given a co-occurrence con-taining the word w, our method selects a classfor w in the following way:Step 1: Obtain the classes C 1, C 2 .... , C n associ-ated with w in a taxonomy.Step 2: Set i to 0.Step 3: Set i to i q- 1.Step 4: Compute t using formula (3).Step 5: If t < threshold.If i ~ n goto step 3.Otherwise xit.Step 6: Select the class C i to replace w.Let us see what this means with an ex-ample.
Suppose we try to estimate SA for(produce, telephone) 3.
See Table 1.
Here f (v) ,f(n) and f(vn) axe the frequencies for the verbproduce, classes for the noun telephone, and co-occurrences between the verb and the classes fortelephone, respectively; and t is the t-score 4.
'The t-score (Church and Mercer, 1993) compares thehypothesis that a co-occurrence is significan~ against henull hypothesis that the co-occurrence an be attributedto chance.2The default threshold for t-score is 1.28 which cor-responds to a confidence l vel of 90%.
t-scores are ofteninflated due to certain violations of assumptions.aThe data was obtained from 68,623 verb-noun pairsin EDR Corpus (EDR, 1993).4In our theory, we are to use each pair of (C i, Ci),where i=l,2,...m, j - l ,2, .
.
.
,n,  to calculate strengths oflexical associations.
But our experiments show that up-per classes of a verb are very unreliable to be used tomeasure the strengths.
The reason may be that, unlikenouns, the verbs would not have a "neat" hierarchy orthat the upper classes of a verb become too general asthey contain too many concepts underneath t em.
Be-cause of this observation, we use, for the classes of a1417verb classes for telephone f(v) f(n) f(vn) t-scoreproduce concrete thing 671 18926 100 -4.6produce inanimate object 671 5593 69 0.83produce implement/tool  671 2138 35 1.91produce machine 671 664 19 2.86produce communication machine 671 83 1 0.25produce telephone 671 24 0 -Table 1 Estimation of (produce telephone)The lowest class co-occurrence (produce,communication machine) has a low t-score andproduces a bad estimation.
The most frequentco-occurrence (produce, concrete thing) has alow t-score also reflecting the fact that it may beattr ibuted to chance.
The t-scores for (produce,machine) and (produce, implement/tooO arehigh and show that these co-occurrences are sig-nificant.
Among them, our method selects thelowest class co-occurrence for which the t-scoreis above the threshold: (produce, machine).3 Disambiguation UsingClass-Based EstimationWe now apply our method to estimate SA fortwo different types of syntactic constructionsand use the results in resolving structural am-biguities.3.1 D isambiguat ion  o f  DependencyRe la t ions  in  JapaneseIdentifying the dependency structure of aJapanese sentence is a difficult problem sincethe language allows relatively free word or-ders.
A typical dependency relation inJapanese appears in the form of modifier-particle-modificand triplets.
When a modifier isfollowed by a number of possible modificands,verb, the verb itself or, when it does not give us a goodresult, only the lowest class of the verb in calculating thestrength of association (SA).
Thus, for an example, theverb eat has a sequence of eat ~ ingest ~ put somethinginto body --%... --" event -" concept in the class hierarchy,but we use only eat and ingest for the verb eat whencalculating SA for (eat, apple).there arise situations in which syntactic rolesmay be unable to determine the dependency re-lation or the modifier-modificand relation.
Forinstance, in' ~ 0 '(vigorous) may modify either ' q~~'  (middle aged) o r '  t l l~  ' ( health care).But which one is the modiflcand o f '  ~ ~ ~ 0 ' ?We solve the ambiguity comparing the strengthof association for the two or more possible de-pendency relations.Calculation of Strength of Association We cal-culate the Strength of Association (SA) scorefor modi f ier  - particle - modi f icand by:SA(rn / ;p .
.
.
m.) = log2 \ /(C., l i .r)/(p..trn.)
\](a)where Cmfie r stands for the classes that in-clude the modifier word, Part is the particle fol-lowing the modifier, mc the content word in themodificand phrase, and f the frequency.Let us see the process of obtaining SA scorein an example ( ~ - ?
)~- ~ ( ) (literally: profes-sor - subject.marker - work).
To calculate thefrequencies for the classes associated with ' ~', we obtain from the Co-occurrence Dictionary(COD) 5 the number of occurrences for (w- 3 ?-SCOD and CD are provided by Japan Electronic Dic-tionary Research Institute (EDR, 1993).
COD containsthe frequencies of individual words and of the modifier-1418< ), where w can be any modifier.
We thenobtain from the Concept Dictionary (CD) 6 thecloses that include ' $~'  and then sum up allthe occurrences ofwords included in the classes.The relevant portion of CD for ' $~'  in ( ~-$~-~ < ) is shown in Figure 1.
The numbersin parenthesis here indicate the summed-up fre-quencies.We then calculate the t-score between ' $~-< ' and all the classes that include' ~ '.
SeeTable 2.Classes for the t- particle-modifier ~ score modificandA ~ ~ $ # ~  4.57 h?~<A~ 5.14 $~<~ O ~ A ~  1.74 ~<~ ~ A ~  0.74 ~<Table 2 t-scores for ( ~ - ~-  ~ < )The t-score for the co-occurrence of themodifier and particle-modificand pair, '~}~'and '~)~-~ < ', is higher than the thresholdwhen ' ~ '  is replaced with \[~J~C~_t~)kr~\].Using (4), the strength of ~sociat ion for the co-occurrence of ( ~ - ~)~ - ~ < ) is calculated fromthe SA between the c l~s \[~R~lJ'C~_?cgk~\] and, ~_~<.
'When the word in question has more thanone sense, we estimate SA corresponding to eachsense and choose the one that results in thehighest SA score.
For instance, we estimate SAbetween ' ~ '  and the various senses of ' ~ <', and choose the highest value: in this case theone corresponding to the sense 'to be employed.
'Determination of Most Strongly AssociatedStructure After calculating SA for each possibleconstruction, we choose the construction withhighest SA score as the most probable struc-pm-ticle-modificand triplets in a corpus that includes220,000 parsed Japanese sentences.6 CD provides a hierarchical structure of concepts cor-responding to all the words in COD.
The number of con-cepts in CD is about 400,000.ture.
See the following example:?
?
.
~ ? )
~  ~'C  ~< )kc )~b~:~.
?
?.technic:al progress work people stress |nnovatlonHere, the arrows show possible dependencyrelations, the numbers on the arrows the esti-mated SA, and the thick arrows the dependencywith highest mutual  information that means themost probable dependency relation.
In the ex-ample, ' ~d:~ ~' modifies ' j~A.
'C ' and ' ~ <' modifes ' A '.
The estimated mutual  informa-tion for ( ~g~#~,  ~A,~C ) is 2.79 and that for( ff~ i ,  A ) is 6.13.
Thus, we choose ' ~_/,~C ' asthe modificand for ' ~ $ ?  '
and '  ,k ' as thatfor ' ~ i 'In the example shown in Figure 2, ourmethod selects the most likely modifier-modificand relation.Experiment Disambiguation of dependency re-lations was done using 75 anlbiguous con-structions from Fukumoto (1992).
Solvingthe ambiguity in the constructions involveschoosing among two or more modifier-particle-modificand relations.
The training data con-sists of all 568,000 modifier-particle-modificandtriplets in COD.Evaluation We evaluated the performance ofour method comparing its results with those ofother methods using the same test and trainingdata.
Table 3 shows the various results (suc-cess rates).
Here, (1) indicates the performanceobtained using the principle of Closest Attach-ment (Kimball, 1973); (2) shows the perfor-mance obtained using the lowest observed classco-occurrence (Weischedel et al, 1993); (3) isthe result from the maximum mutual informa-tion over all pairs of classes corresponding tothe words in the co-occurrence (Resnik, 1993;Alves, 1996); and (4) shows the performance ofour method 7.7The precision is for the 1.28 default threshold.
Theprecision was 81.2% and 84.1% when we set the thresholdto .84 and .95.
In all these cases the coverage was 92.0%.1419(3) person (3)I human or similar (42) IAM(39) humandefined by race or origin(3) Japanese (2) worker(5) person defined by role(I) person defined by position..?.
.......(I) slave (0) professorFigure 1 An Extract of CD\[~ 9.19 \[ 4.48F- ' )  I tnational investigation based cause prompt study expectFigure 2 An example of parsing a Japanese sentencemethod precision(1) closest attachment 70.6%(2) lowest classes 81.2%(3) maximum MI 82.6%(4) our method 87.0%Table 3 Results for determining dependencyrelationsClosest attachment (1) has a low perfor-mance since it fails to take into considerationthe identity of the words involved in the deci-sion.
Selecting the lowest classes (2) often pro-duces unreliable stimates and wrong decisionsdue to data sparseness.
Selecting the classeswith highest mutual information (3) results inovergeneralization that may lead to incorrect at-tachments.
Our method avoids both estimatingfrom unreliable classes and overgeneralizationand results in better estimates and a better per-formance.A qualitative analysis of our results showstwo causes of errors, however.
Some errors oc-curred when there were not enough occurrencesof the particle-modificand pattern to estimateany of the strength of association necessaryfor resolving ambiguity.
Other errors occurredwhen the decision could not be made withoutsurrounding context.3.2 P repos i t iona l  Phrase  At tachmentin Eng l i shPrepositional phrase (PP) attachment is aparadigm case of syntactic ambiguity.
The mostprobable attachment may be chosen comparingthe SA between the PP and the various attach-ment elements.
Here SA is measured by:S A( v_attachlv, p, n2) = log2 \ - \ ] - (C~ ~',2 ) )(5)SA(n_attachln,,p, n ) -- log, \ 7-(C-~,~-C,--~2 ) \](6)where Cw stands for the class that includesthe word w and f is the frequency in a trainingdata containing verb-nounl-preposition-noun2constructions.Our method selects from a taxonomy theclasses to be used to calculate the SA score and1420then chooses the attachment with highest SAscore as the most probable.Experiment We performed a PP attachmentexperiment on the data that consists of allthe 21,046 semantically annotated verb-noun-preposition-noun constructions found in EDREnglish Corpus.
We set aside 500 constructionsfor test and used the remaining 20,546 as train-ing data.
We first performed the experimentusing various values for the threshold.
Table4 shows the results.
The first line here showsthe default which corresponds to the most likelyattachment for each preposition.
For instance,the preposition of is attached to the noun, re-flecting the fact that PP's led by of are mostlyattached to nouns in the training data.
The'confidence' values correspond to a binomial dis-tribution and are given only as a reference s.confidence t coverage precision success100% 68.0% 68.0%50% .00 82% 82.2% 79.4%70% .52 75% 87.3% 83.4%80% .84 65% 88.6% 84.2%85% .95 57% 89.6% 84.8%90% 1.28 50% 91.3% 85.6%Table 4 Results for PP attachment withvarious thresholds for t-scoreThe precision grows with t-scores, whilecoverage decreases.
In order to improve cov-erage, when the method cannot find a classco-occurrence for which the t-score is abovethe threshold, we recursivcly tried to find aco-occurrence using the threshold immediatelysmaller (see Table 4).
When the method couldnot find co-occurrences with t-score above thesmallest hreshold, the default was used.
Theoverall success rates are shown in "success" col-umn in Table 4.SAs another way of reducing the sparse data problem,we clustered prepositions using the method escribed in"~Vu and Furugori (1996).
Prepositions like synonymsand antonyms are clustered into groups and replaced bya representative preposition (e.g., till and pending arereplaced by until; amongst, amid and amidst are replacedby among.
).Evaluation We evaluated the performance ofour method comparing its results with those ofother methods with the same test and trainingdata.
The results are given in Table 5.
Here, (5)shows the performance of two native speakerswho were just presented quadruples of four headwords without surrounding contexts.Method Success Rate(1)closest Attachment 59.6%(2)lowest classes 80.2%(3)maximum MI 79.0%(4)our method 85.6%(5)human (head words only) 87.0%Table 5 Comparison with other methodsThe lower bound and the upper bound onthe performance of our method seem to be59.6% scored by the simple heuristic of closestattachment (1) and 87.0% by human beings (4).Obviously, the success rate of closest attach-ment (1) is low as it always attaches a word tothe noun without considering the words in ques-tion.
The unanticipated low success rate of hu-man judges is partly due to the fact that some-times constructions were inherently ambiguousso that their choices differed from the annota-tion in the corpus.Our method (4) performed better than thelowest classes method (2) and maximum MImethod (3).
It owes mainly to the fact thatour method makes the estimation from class co-occurrences that are more reliable.4 Concluding RemarksWe proposed a class-based method that selectsclasses to be used to estimate the strength of as-sociation for word co-occurrences.
The classesselected by our method can be used to estimatevarious types of strength of association i differ-ent applications.
The method iffers from otherclass-based methods in that it allows identifica-tion of a reliable and specific class for each co-occurrence in consideration and can deal withdate sparseness problem more efficiently.
It1421overcame the shortcomings from other meth-ods: overgeneralization and employment of un-reliable class co-occurrences.We applied our method to two structuraldisambiguation experiments.
In both exper-iments the performance is significantly betterthan those of others.References\[1\] Alves, E. 1996.
"The Selection of the MostProbable Dependency Structure in JapaneseUsing Mutual Information."
In Proc.
of the34th ACL, pages 372-374.\[2\] Brown, P., Della Pietra, V. and Mercer,R.
(1992).
"Word Sense Disambiguation Us-ing Statistical Methods."
Proceedings of the30th ACL, pages 264-270.\[3\] Church, K., and Mercer, R. 1993.
"Introduc-tion to the Special Issue on ComputationalLinguistics Using Large Corpora."
Compu-tational Linguistics, 19 (1): 1-24.\[4\] Church, K., and Hanks, P. 1990.
"Word As-sociation Norms, Mutual Information andLexicography."
Computational Linguistics,16(1):22-29.\[5\] Church, K., and Gale, W. 1991.
"A Com-parison of the Enhanced Good-Turing andDeleted Estimation Methods for Estimat-ing Probabilities of English Bigrams."
Com-puter Speech and Language, 5:19-54.\[6\] Fukumoto, F., Sano, H., Saitoh, Y. andFukumoto J.
1992.
"A Framework for De-pendency Grammar Based on the Word'sModifiability Level - Restricted DependencyGrammar."
Trans.
IPS Japan, 33(10):1211-1223 (in Japanese).\[7\] Hindle, D., and Rooth, M. 1993.
"StructuralAmbiguity and Lexical Relations."
Compu-tational Linguistics, 19(1):103-120.\[8\] Japan Electronic Dictionary Research Insti-tute, Ltd. 1993.
EDR Electronic DictionarySpecifications Guide (in Japanese).\[9\] Jelinek, F., and Mercer, R. 1985.
"Proba-bility Distribution Estimation from SparseData."
IBM Technical Disclosure Bulletin,28:2591-2594.\[10\] Katz, S. 1987.
"Estimation of Probabili-ties from Sparse Data for Language ModelComponent of a Speech Recognizer."
IEEETransactions on Acoustics, Speech and Sig-nal Processing, ASSP-35(3):400-401.\[11\] Kimball, J.
1973.
"Seven Principles ofSurface Structure Parsing in Natural Lan-guage."
Cognition, 2:15-47.\[12\] Pereira, F. and Tishby, N. 1992.
"Distribu-tional Similarity, Phrase Transitions and Hi-erarchical Clustering."
In Proc.
of the 30thACL, pages 183-190.\[13\] Resnik, P. 1992.
"Wordnet and Distribu-tional Analysis: A Class-Based Approachto Lexical Discovery."
AAAI Workshop onStatistically-based Natural Language Pro-cessing Techniques, pages 56-64.\[14\] Resnik, P. 1993.
"Selection and Informa-tion: A Class-Based Approach to LexicalRelationships."
PhD.
thesis, University ofPennsylvania.\[15\] Weischedel, R., Meteer, M., Schwartz, R.,Ramshaw, L., and Palmucci, J.
1993.
"Cop-ing with Ambiguity and Unknown WordsThrough Probabilistic Models."
Computa-tional Linguistics, 19(2):359-382.\[16\] Wu, H. and Furugori, T. 1996.
"A Hy-brid Disambiguation Model for Preposi-tional Phrase Attachment."
Literary andLinguistic Computing.
11(4):187-192.\[17\] Yarowsky, D. 1992.
"Word Sense Disam-biguation using Statistical Models of Roget'sCategories Trained on Large Corpora."
Pro-ceedings of COLING-92, pages 454-460.1422
