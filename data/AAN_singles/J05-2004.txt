A Mathematical Model of HistoricalSemantics and the Grouping of WordMeanings into ConceptsMartin C. Cooper?University of Toulouse IIIA statistical analysis of polysemy in sixteen English and French dictionaries has revealedthat, in each dictionary, the number of senses per word has a near-exponential distribution.A probabilistic model of historical semantics is presented which explains this distribution.
Thismathematical model also provides a means of estimating the average number of distinct conceptsper word, which was found to be considerably less than the average number of senses listed perword.
The grouping of word senses into concepts is based on whether they could inspire the samenew senses (by metaphor, metonymy, etc.
), that is, their potential future rather than their history.1.
IntroductionAmbiguity is ubiquitous in natural language.
It is most dramatic when it concerns theparsing of a sentence in examples such asThe High Court judges rape and murder suspects.I heard a giant swallow after seeing a horse fly.La petite brise la glace.
(?The girl breaks the mirror.
?/?The little breeze chills her.?)
(from Fuchs1996)However, the most common form of ambiguity concerns the meanings of individualwords, as in the following examples:The minister decided to leave the party.
(?church minister?/?government minister?, ?drinks party?/?political party?
)He?s a curious individual.
(?odd?/?nosey?
)Je suis un imbe?cile.
(?I?m following an idiot.
?/?I am an idiot.?)?
IRIT, Universite?
de Toulouse III, 118 route de Narbonne, 31062 Toulouse, France.
E-mail: cooper@irit.fr.Submission received: 12th September 2003; Revised submission received: 29th April 2004; Accepted forpublication: 7th December 2004?
2005 Association for Computational LinguisticsComputational Linguistics Volume 31, Number 2The last example involves homographs (different words which happen to be spelledthe same).
However, it should be noted that only a small percentage of word senseambiguity is due to homography.
(We obtained an estimate of approximately 2% byrandom sampling of English and French dictionaries [12, 21, 22, 24].)
Many words havegained multiple senses by metonymy or by figurative or metaphorical uses.
The result-ing senses are sufficiently different to be considered by lexicographers as distinct con-cepts (e.g., political party/drinks party).
In information retrieval systems with naturallanguage interfaces (Mandala, Tokunaga, and Tanaka 1999; Stevenson and Wilks 1999)or in models of human language processing via networks of semantic links (Fellbaum1998; Hayes 1999; Vossen 2001), a fundamental question is what should correspond to abasic semantic concept.
Is it a word, a word sense, or a group of word senses?This article presents a stochastic model of the evolution of language which allowsus to answer this question.
Applying the model to statistics obtained from a largenumber of monolingual and bilingual dictionaries provides convincing evidence thatneither words nor individual word senses (as identified by lexicographers) correspondto concepts, but rather groups of word senses.
Our model demonstrates that each wordrepresents, on average, about 1.3 distinct concepts.
This can be compared with theaverage 2.0 distinct senses per word listed in the dictionaries.
This model also allows usto propose a novel and formal definition of the word concept.
There are clear applicationsin artificial intelligence (Mandala, Tokunaga, and Tanaka 1999; Stevenson and Wilks1999), cognitive science (Cruse 1995), lexicography, and historical linguistics (Algeo1998; Antilla 1989; Schendl 2001; Geeraerts 1997).2.
The Genesis of Word SensesWord origin and the evolution of spelling, pronunciation, and meaning have long beenstudied by etymologists.
Etymology tells us that many words in everyday use have ahistory that can be traced back thousands of years (Onions 1966; Picoche 1992).
Thiscan be contrasted with the hundreds of new entries which lexicographers add to eachnew edition of a dictionary.
These new entries are not only neologisms, but also newsenses for existing words.
The history of the variations in spelling and pronunciation ofparticular words are not of direct concern here.
We are interested in how ?word, sense?pairs enter (or leave) a language.
For each such semantic change, we can try to identifythe originator, the reason, and the mechanism by which it occurs.2.1 Origins of Semantic ChangePicoche (1992) states that the majority of words in French have a scholarly origin andwere introduced by clerics, jurists, intellectuals, and scientists directly from Latin andGreek.
However, it is clear that many words are of popular origin (e.g., bike, trainers, andOK in English or ve?lo, baskets, and OK in French) and have become accepted terms as theresult of common use.The principal reason why new ?word, sense?
pairs are introduced is to adapt lan-guage to new communicative requirements (Schendl 2001).
Discoveries and inventionscan give rise to neologisms (e.g., kangaroo, quark, Internet) or new senses for existingwords (e.g., the ?armoured vehicle?
sense of tank which coexists with the earlier ?largecontainer?
sense).
Another driving force in historical semantics is the human tendencytoward efficiency of communication, by, for example, shortening words or expres-sions (e.g., clipping of omnibus to bus), ignoring unnecessary semantic distinctions, andinventing new words to replace long expressions.
Other reasons for semantic change228Cooper A Mathematical Model of Historical Semanticshave more to do with human psychology than with practical necessity.
Taboo leadsto the introduction of slang words or euphemisms, such as to terminate for ?to kill?
orsenior for ?old?.
Litotes is a special case in which a word is replaced by the negationof its opposite (e.g., not bad for ?good?).
New words may be employed to make anold product sound more modern, exotic, or appetizing (e.g., the old-fashioned Britishword chips is often replaced by french fries or frites on menus).
The human tendencyto emphasize or exaggerate leads to the replacement of severe by horrific or very byawfully (Schendl 2001).2.2 Mechanisms of Semantic ChangeWe can divide the mechanisms for neologism into three categories (Algeo 1998; Antilla1989; Chalker and Weiner 1994; Gramley 2001; Schendl 2001; Stockwell and Minkova2001):1.
Word-creation from no previous etymon.
This is rare but is the most likelyexplanation for echoic words such as vroom, cuckoo, oh!
(Bloomfield 1933).2.
Borrowing from another language.
This includes loan words (e.g., strudelfrom German, pizza from Italian) and loan translations in which eachelement of a word is translated (e.g., spring roll from Chinese, dreamtimefrom the Australian aboriginal alcheringa (Gramley 2001), and chien-chaud,which is the French Quebec version of hot dog).3.
Word formation from existing etyma (words or word components).
Thisincludes(a) compounding (e.g., bookcase, bushfire),(b) blending (e.g., brunch, motel),(c) affixation (e.g., overcook, international, likeness, privatize),(d) shortening (e.g., petrol(eum), radar, telly, AIDS),(e) eponyms (e.g., kleenex, sandwich, jersey, casanova),(f) internal derivations (Gramley 2001) (e.g., extend/extent or sing/song),(g) reduplication (Stockwell and Minkova 2001) (e.g., fifty-fifty,dum-dum),(h) morphological reanalysis (Schendl 2001) (e.g., the nonexistent verbto edit was formed from the noun editor; the word cheeseburger wasderived from hamburger even though this word comes from theproper name Hamburg).Mechanisms 1 and 3(f)?
(h) are rare compared to 2 and 3(a)?
(e) (Algeo 1998).
Clearly,the above mechanisms are not exclusive.
Borrowing and word formation are obviouslyboth at play in examples such as blitz, which is a clipping of the German word blitzkreig,and the French word tennisman, which is a compound of two English words.Word creation, borrowing, and word formation generally produce a new word witha single sense, except when by coincidence the word being created, borrowed, or formedalready exists with a different sense.
In the rest of the article, we consider homographsproduced by such coincidences to be different words.
In most dictionaries, homographshave distinct entries.
For example, the term bug, meaning ?error in a computer program,?was borrowed into French as bogue (by assimilation with the already existing word withthe unrelated meaning ?husk?
), but these two meanings of bogue are listed in Frenchdictionaries as two distinct words.229Computational Linguistics Volume 31, Number 2Nevertheless, we should mention three cases in which a neologism is often notrecognized as a genuinely new word: ellipsis (Antilla 1989) (e.g., daily (newspaper)), zeroderivation (Nevalainen 1999) (also known as conversion [Gramley 2001; Schendl 2001])(e.g., to cheat > a cheat), and borrowing of an already-existing word with a related sense(e.g., to control was borrowed into French as contro?ler, thus giving an extra sense to thisFrench word meaning ?to verify?
).The following is a list of mechanisms which can create a new sense for an already-existing word (adapted from Algeo [1998]):1. referential shift (e.g., to print now also refers to laser printers).2. generalization (e.g., chap used to mean ?a customer?)
or abstraction (e.g.,zest denoted orange or lemon peel used for flavoring before being used inthe abstract sense of ?gusto?).3.
specialization (e.g., in Old English fowl meant any kind of bird and meatany kind of food [Onions 1966; Schendl 2001]) or concretion.4.
metaphor (e.g., kite, meaning ?bird of prey,?
applied to a toy).5. metonymy (literally, ?name change?
), that is, naming something by any ofits parts, accompaniments, or indexes (e.g., the crown for the sovereign, theCity for the people who work there, tin for the container made of thatmetal, cognac for the drink originating from that region) (Traugott andDasher 2002).6. clang association or folk etymology (e.g., belfry meant ?a movable towerused in attacking walled positions,?
but the first syllable was associatedwith bell, and now the basic meaning is ?bell tower?
[Antilla 1989]).7. embellishment of language by using words which are more acceptable,attractive, or flattering than existing terms (hyperbole, litotes,euphemisms, etc., as discussed above).We use the general term association to cover all these cases.3.
The Near-Exponential RuleIn order to study the relative importance of neologism, obsolescence, and the creation ofnew meanings for existing words, we counted the number of senses listed per word inseveral different monolingual dictionaries.
We observed the following general empiricalrule satisfied to within a fairly high degree of accuracy by all the dictionaries studied[9, 12, 19, 20, 21, 23, 24, 28, 31, 32]:Near-Exponential Rule: The number of senses per word in a monolingual dictionary has an approximatelyexponential distribution.One way of testing this rule is by plotting log(Ns) against s, where Ns is the numberof words in the dictionary with exactly s senses.
If the near-exponential rule is satisfied,then the resulting plot should be very close to a straight line with a negative slope.
Thisis indeed the case for the dictionaries tested, with varying values of the slope dependingon the dictionary.
Figures 1 and 2 show the plot of Ns, on a logarithmic scale, against s230Cooper A Mathematical Model of Historical SemanticsFigure 1Plot of Ns (number of words with s senses) against s for various monolingual Englishdictionaries.Figure 2Plot of Ns (number of words with s senses) against s for various monolingual French dictionaries.231Computational Linguistics Volume 31, Number 2Figure 3Plot of Ns (number of entries with s senses) for four different dictionaries, each showing anonexponential distribution.for four English [19, 24, 28, 31] and five French [9, 12, 20, 21, 23] dictionaries.
Only thosepoints (s, Ns) for which Ns > 12 are plotted in the figures.For each dictionary, the values of Ns were obtained by sampling a random set ofpages of the dictionary.
Sampling was performed independently for each dictionary,meaning that the random sample of words was different in each case.
We excludedentries corresponding to proper names, foreign words, spelling variants, derived words(such as past participles), regional words, abbreviations, and expressions.
We allowedhyphens within words but not spaces.
Thus cat-o?-nine-tails counted as a word, buttower block and phrasal verbs such as give up did not.
Only words forming part ofBritish English or French spoken in metropolitan France were considered.
All wordswere treated equally irrespective of their relative frequencies.
Thus the words get andfloccinaucinihilipilification were given the same importance.
The size of each dictionarythat was sampled is given in the reference section.
The dictionaries sampled vary in sizefrom 20,000 to 80,000 words.To ensure that the near-exponential rule was not simply an artifact of our choice ofexperimental procedure or of lexicographical practice, we performed the same analysison a dictionary of abbreviations and acronyms [7], a dictionary of scientific terms [1], abilingual dictionary of slang words [17], and a dictionary of French synonyms [10].
Theresulting curves, shown in Figure 3, are far from straight lines.We performed similar counts for bilingual dictionaries.
Figures 4 and 5 show thenumber of words NTt with t translations plotted against t for several pairs of languages.The NTt scale is again logarithmic.
Although the near-exponential rule could also besaid to hold for certain bilingual dictionaries, the curvature of the log NTt against tcurve varies considerably depending on the distance between the two languages.
Forpairs of languages with strong etymological connections (such as French and Spanish),the average curvature is positive (Figure 4), but for pairs of distant languages (suchas Japanese and English) the average curvature is negative (Figure 5).
A theoreticalexplanation of this phenomenon is outside the scope of the present article, but it is232Cooper A Mathematical Model of Historical SemanticsFigure 4Number NTt of words in French with t translations in English, Spanish, Italian, and Portuguese.Figure 5Number NTt of words in language A with t translations in language B, for distant languages Aand B.probably due to the greater differences in the segmentation of semantic space by distantlanguages (see Resnik and Yarowsky [2000] for some illustrative examples).
It will betreated in detail in a follow-up article.4.
Words, Senses, and ConceptsIn the following section we present a mathematical model which explains the near-exponential distribution of word senses observed in English and French dictionaries.Not only do the curves of Figures 1 and 2 share the property of being close to straightlines (i.e., having curvature close to zero), but in each case, the curvature that theydo exhibit is positive rather than negative.
Although barely discernible for some ofthe curves, this positive curvature cannot be ignored.
We fitted a straight line to thecurves and then used a chi-square test to judge the closeness of fit of this straight lineto the data.
For each curve the chi-square test demonstrated a significant discrepancy233Computational Linguistics Volume 31, Number 2between the model and the data.
For example, the significance level was 15 standarddeviations for the Longman Dictionary of Contemporary English (LCDE) [24].
In order tofind a satisfactory model to explain this slight but consistently positive curvature, westudy in more detail the process by which words gain new senses.The word panel provides a good example of a word whose number of meaningshas grown since its introduction into English from Old French in the 13th century.
Itsoriginal meaning was a piece of cloth placed under a saddle.
Over the centuries it gainedmany meanings, by extension of this original sense, which can be grouped together inthe following concept:(C1) an often rectangular-shaped part of a surface (of a wall, fence, cloth, etc.
),possibly decorated or with controls fastened to it.Concept (C1) covers four of the meanings of panel listed in the LDCE.
However,during the 14th century panel also gained the following meaning: piece of parchment(attached to a writ) on which names of jurors were written (hence by metonymy) list ofjurymen: jury (Onions 1966).
Four of the meanings of panel listed in the LDCE can beconsidered to be covered by the following general concept:(C2) a group of people (or the list of their names) brought together to answerquestions, make judgements, etc.If panel were to gain new meanings, such as1.
a side of a tower block2.
a school disciplinary committeethen these would be by association with the two concepts listed above, (C1) and (C2),respectively.
Note that neither of these potential new meanings would constitute a trulynew concept, since they can be considered to be covered by the existing concepts (C1)and (C2).If, on the other hand, panel were to gain the following new meanings3.
a wall which divides a large room into smaller units but which does notreach the ceiling4.
a combined table and bench that can be used, for example, by a panel ofexpertsby association with concepts (C1) and (C2), respectively, then these new meaningscould be considered as corresponding to new concepts.
These meanings are sufficientlydifferent from the existing meanings listed in the LCDE that they themselves could giverise to further new meanings by metonymy, metaphor, etc., which would simply not bepossible by direct association with the existing meanings.
For example, the followingmeanings could theoretically be derived from the meanings 3 and 4, respectively, above(but not directly from concepts (C1) and (C2), respectively):5. any division of something into smaller units6.
a combined desk and bench for a single person234Cooper A Mathematical Model of Historical SemanticsWe continue with another example, this time from French.
The word toilette haseight meanings listed in Le petit Robert [21], which we can translate and paraphrase asfollows:1. a small piece of cloth (from toile = ?piece of cloth?)
and, in particular, onethat was used in the past to wrap up objects2.
a membrane used by butchers to wrap up certain pieces of meat3.
clothes, jewelry, comb, etc.
(objects necessary to prepare one?s appearancebefore going out, which used to be laid out on a small cloth)4. the action of combing, making up, dressing5.
a woman?s style of dressing6.
the cleaning of one?s body before dressing7.
a washroom, toilet8.
the cleaning, preparation of an object, text, etc.We can group these meanings into three concepts:(D1) a small piece of material (meanings 1, 2)(D2) the objects used for, the action of or the style of dressing, making-up,cleaning of a person or an object (meanings 3, 4, 5, 6, 8)(D3) a washroom, toilet (meaning 7)We have grouped meanings together in this way because we consider it likely thatnew meanings for toilette which could enter the French language by association withan existing meaning would be very similar for those meanings grouped into the sameconcept, but very different for those corresponding to different concepts.This discussion leads us naturally to the following technical definition of concept:DefinitionTwo meanings of a given word correspond to the same concept if and only if they couldinspire the same new meanings by association.We suggest grouping together different senses of a word, not only according to theirparts of speech or to their etymology (i.e., the history) of word senses, but also accordingto their potential future: whether or not they could inspire the same new meanings byassociation.
This can be compared with the biological definition of species in terms ofthe ability to breed together to produce viable offspring rather than in terms of historyor physical characteristics.5.
A Mathematical Model of Word Sense GenesisThis section describes a stochastic model of the creation of word senses.
This modelnot only explains the near-exponential rule but also provides a deeper insight into theprocess of naming.
Let LD be a language as defined by the set of ?word, sense?
pairs in a235Computational Linguistics Volume 31, Number 2dictionary D. We consider the evolution of the language LD over time.
We must alwaysbear in mind that LD is, of course, only an approximate representation of the semanticsof the corresponding natural language.
For example, the compiler of a dictionary maychoose to include archaic words as a historical record or to exclude whole categories ofwords such as slang or technical terms.Consider the evolution of LD as a stochastic process in which each step is either(a) the elimination of a word sense (by obsolescence), (b) the introduction of a newword (by creation, borrowing, word-formation, or any other mechanism), or (c) theaddition of a new sense for an existing word (by association with an existing sense).Let t be the probability of a step of type (a), u the probability of a step of type (b), andv the probability of a step of type (c).
Note that t + u + v = 1.
The parameters of ourmodel t, u, v are unknowns which will be estimated from the observed values of Ns (thenumber of words with s senses).We make the following simplifying assumptions:1.
New-word single-sense assumption: When a neologism enters thelanguage LD, it has a single sense.2.
Independence of obsolescence and number of senses: The probabilitythat a ?word, sense?
pair leaves the language LD by obsolescence isindependent of the number of senses this word has in LD.The new-word single-sense assumption is an essential part of our model.
To testit we require two editions of the same dictionary.
The 1994 edition of the Dictionnairede l?acade?mie franc?aise indicates which words are new compared to the 1935 edition.Less than 17% of these words are polysemic.
Furthermore, this corresponds, accordingto our model and to within-sample error, to the proportion of originally monosemicwords entering the language that can be expected to acquire new senses during theperiod between the publication of the two editions.
Assumption 2 above is not asimportant as assumption 1, since later we restrict ourselves to a no-obsolescencemodel.As discussed in the previous section, the set of s senses of an ambiguous wordmay correspond to a number c of essentially distinct concepts, where c is some numberbetween one and s. For example, the plumbing and anatomy senses of joint correspondto the same concept, since they could inspire the same new senses by association.
The?cigarette containing cannabis?
sense of joint clearly corresponds to a different concept,since it could inspire a very different set of new senses by association.
Associationsinspired by distinct concepts are assumed to occur independently.
We assume that aword with s senses in LD represents on average 1 + ?
(s ?
1) concepts.
We call ?
theconcept creation factor (since, in a no-obsolescence model, ?
is simply the probabilitythat a new sense for a word w can be considered a new concept compared to the existingsenses for w).
We can now state a third assumption:3.
Associations are with concepts: The probability that a concept gives riseto a new sense for a word w by association is proportional to the numberof concepts represented by w in LD, which is assumed to be on average1 + ?
(s ?
1), where s is the number of senses of w and ?
is a constant.The concept creation factor ?
is another unknown which will be estimated from thevalues of Ns.236Cooper A Mathematical Model of Historical SemanticsTable 1Number Ns of words with s senses in samples from the 1933 and the 1993 edition of the ShorterOxford English Dictionary.N1 N2 N3 N4 N5 N6 N7 N8 N91933 427 186 104 49 24 15 22 6 81993 403 176 86 44 32 16 14 7 1We make a fourth hypothesis in order to render the problem mathematicallytractable:4.
Stationary-state hypothesis: LD considered as a stochastic process is in astationary state, in the sense that the probability P(s) that an arbitraryword of LD has exactly s senses does not change as LD evolves.To test the validity of the stationary-state hypothesis, we compared the 1933 and1993 editions of the Shorter Oxford English Dictionary (SOED) [32, 28].
In the space of60 years, the number of words in the SOED increased by 24%.
Nevertheless the valuesof P(s) (s = 1, 2, .
.
.
, 9) remained almost constant.
A chi-square test revealed that thedifferences in the values of P(s) (s = 1, 2, .
.
.)
could be accounted for by sampling error.The corresponding values of Ns are given in Table 1.The results of further experiments carried out to test the validity of the assumptionson which our model is based are given in a later section, so as not to clutter up thepresentation of the model in this section.Let m be the expected number of senses per word in LD.
Sincem =?
?s=1s P(s) (1)and the values of P(s) are constant by the stationary-state hypothesis, m is also aconstant.The expected net increase in the number of word senses in LD during one step ofthe process is ?t + (1 ?
t) = 1 ?
2t, since the probability that a word sense is lost byobsolescence is t and the probability that a word sense is gained is 1 ?
t. If r denotes theexpected net increase in the number of words in LD during one step of the process, thenwe must have 1?2tr = m, since m is a constant.
Thusr = (1 ?
2t)/m (2)Note that the number of words in LD would be constant if and only if t = 0.5.Let pout(s) represent the probability that the next change in the language LD is thata word with s senses loses one of its senses by obsolescence.
Let pin(s) represent theprobability that the next change in LD is that a word with s senses gains a new sense.Note that?
?s=1 pout(s) = t and?
?s=1 pin(s) = v, by the definitions of t and v.By the stationary-state hypothesis, the expected net increase in Ns (the numberof words in LD with exactly s senses) during one step must be proportional to P(s).Denote the expected net increase in Ns by ?s = dP(s), for some constant d. We then have237Computational Linguistics Volume 31, Number 2?
?s=1 ?s = d, since?
?s=1 P(s) = 1.
But?
?s=1 ?s = r, since the total expected increase inthe number of words is r. Thus ?s = rP(s) = (1 ?
2t)P(s)/m (by equation (2)).We can also express ?s, the expected net increase in Ns, in terms of the probabilitiespin(s) and pout(s), which gives the following equation:(1 ?
2t)P(s)/m = ?pin(s) ?
pout(s) + pin(s ?
1) + pout(s + 1) (3)since Ns is decremented when a word with s senses gains or loses a sense and Ns isincremented when a word with s ?
1 senses gains a sense or a word with s + 1 sensesloses a sense.From the assumption of the independence of obsolescence and number of senses,it follows directly that pout(s) is proportional to sP(s).
Let pout(s) = KsP(s), for someconstant K. Then, since?
?s=1 pout(s) = t, we have t =?
?s=1 KsP(s) = Km by equation(1).
Thus K = t/m andpout(s) =tsP(s)mUnder the assumption that associations are with concepts, pin(s) is proportional to both1 + ?
(s ?
1) and P(s).
Suppose that pin(s) = K?
P(s)(1+ ?
(s ?
1)).
Since?
?s=1 pin(s) =v,?
?s=1 P(s) = 1, and?
?s=1 sP(s) = m, we have v =?
?s=1 K?P(s)(1+ ?
(s ?
1)) =K?
(1 ?
?)
+ K??m.
Thus K?
= v/(1 ?
?+ ?m), and hence, for s = 1, 2, .
.
.pin(s) =v(1 + ?
(s ?
1))P(s)1 ?
?+ ?mNote that the creation of a new word with a single sense is a special case.
By definitionof u as the probability that the next step of the process is the creation of a new word,pin(0) = uSumming equation (3), for s = 1, 2, .
.
., gives1 ?
2tm = ?pout(1) + pin(0) =?tP(1)m + uThusu =1 ?
2t + tP(1)m (4)and, since by definition v = 1 ?
t ?
u,v = 1 ?
t ?
1 ?
2t + tP(1)mPlugging in the formulas for pin(s), pout(s), and v, our basic equation (3) becomes, aftersimplification, for s > 1:t(s + 1)(1 ?
?+ ?m)P(s+ 1) ?
{(m ?
mt ?
1 + 2t ?
tP(1))(1 ?
?+ ?s)+(1 ?
2t + ts)(1 ?
?+ ?m)}P(s)+(m ?
mt ?
1 + 2t ?
tP(1))(1 ?
2?+ ?s)P(s ?
1) = 0 (5)238Cooper A Mathematical Model of Historical SemanticsAs observed in the previous section, empirical evidence indicates that P(s) is anear-exponential function.
In fact, if P(s) were an exponential function, then since?
?s=1 P(s) = 1 and?
?s=1 sP(s) = m, we can easily deduce that P(s) would be equal tom?1(1 ?
m?1)s?1.
The proof of the following result is simple but rather tedious andhence is omitted:PropositionThe solution P(s) to the set of equations (5) is the exponential function P(s) =m?1(1 ?
m?1)s?1 if and only if?
= tm ?
2tm + 2tSince the relationship between ?, t, and m given by the above proposition did notseem to have any theoretical foundation, and since the observed values of P(s) did not,in fact, follow a perfectly exponential distribution, we decided to estimate the valuesof the parameters m, ?, and t which would best explain the actual near-exponentialdistributions.
We first set m =?
?s=1 sPobs(s), where Pobs(s) are the observed values ofP(s) calculated from the values of Ns.
Then we calculated the values of ?
and t whichminimized the sum of the squares of the errors in equation (5).
For six out of the tendictionaries tested, the best-fit value occurred when t = 0.
The average of the best-fitvalues of t was 0.04.
These results led us to examine different editions of the samedictionaries in order to obtain an alternative estimate of t. We discovered that whilehundreds or even thousands of words were added between two different editions ofthe same dictionary [32, 28], very few words were removed due to obsolescence.
Forexample, the number of words in the Dictionnaire de l?Acade?mie Franc?aise [9] increasedby 28% in 59 years, whereas the total number of word senses marked as obsolete in thelatest edition is less than 1%.
Our conclusion is that the English and French languages, asdefined by dictionaries, are in a state of continual expansion, with an almost negligibleloss of word senses by obsolescence.We therefore study in more detail the special case in which t = 0.
The followingresult follows immediately from equation (4) by setting t = 0:PropositionWhen t = 0, the probability that the next addition to LD is the creation of a new word isu = 1/m.TheoremWhen assumptions 1, 2, 3 and 4 hold and furthermore LD is subject to no obsolescence,then the probability that an arbitrary word in LD has s senses is given byP(1) = 1 + ?m ?
?m + ?m ?
?P(s) = 1 + ?m ?
?m + ?m ?
?s?i=2(m ?
1)(1 ?
2?+ ?i)m + m?i ?
?i (s > 1) (6)ProofWhen t = 0, equation (5) becomes, for s > 1P(s)(m+ m?s ?
?s) = P(s ?
1)(m ?
1)(1 ?
2?+ ?s) (7)239Computational Linguistics Volume 31, Number 2Summing equation (7) for s = 2, 3, .
.
.
, gives(1 ?
P(1))m + (m ?
P(1))(m??
?)
= (m ?
1)(1 ?
?)
+ m(m ?
1)?since?
?s=1 P(s) = 1,?
?s=2 P(s) = 1 ?
P(1),?
?s=1 sP(s) = m, and?
?s=2 sP(s) =m ?
P(1).
Solving for P(1) givesP(1) = 1 + ?m ?
?m + ?m ?
?The closed-form solution for P(s) in the statement of the theorem then follows by aneasy induction using equation (7).
6.
Applying the Model to Experimental DataWe make the no-obsolescence assumption throughout this section, that is, that t = 0.Knowing that u = 1/m allows us to estimate that, in French, approximately 60% of newword senses correspond to the creation of a new word and approximately 40% to theintroduction of a new sense for an existing word.
In English the split is approximately50-50.
There are, however, quite large variations (between 55% and 65% in French)depending on the dictionary consulted.
Variations are inevitable, since different lexi-cographers have different interpretations of what constitutes distinct senses of a word.We conjecture that similar percentages exist for all natural languages, although therewill be variations among languages depending, among other things, on the ease withwhich new words can be created.The curves in Figure 1 are approximately straight lines, but all have a slight positivecurvature.
This curvature can be explained by the fact that ?
> 0.
Note that, under theassumption t = 0, the concept creation factor ?
is simply the probability that a new sensefor an existing word is sufficiently different from previous senses for it to correspondto a new concept (capable of inspiring associations different from those that could beinspired by the existing senses).
When ?
= t = 0, it follows from the results proved inthe previous section that P(s) is an exponential function.
For ?
> 0, however, the plot oflog Ns against s does indeed have a positive curvature.In order to evaluate visually the influence of the value of ?
on the predicted valuesof Ns, we generated the values of Ns using equation (6) for various values of ?.
Theresults are plotted in Figure 6 (with the average number m of meanings per word set tobe the same as that for the LDCE [24] in order to provide a concrete comparison).
Theobserved values of Ns (for the LDCE) coincide so closely with those predicted by ourmodel with ?
= 0.31 that the curves of observed and predicted values would be barelydistinguishable if drawn in the same figure.For each dictionary we studied, we calculated the value of ?
which provided thebest fit, in a least squares sense, between the observed values of Ns and those calculatedfrom the values of P(s) given by equation (6).
These best-fit values of ?
are given inthe second column of Table 2 for each dictionary we examined.
The values of ?
varybetween 0.22 and 0.41 for the English dictionaries and between 0.28 and 0.47 for theFrench dictionaries.
Our conclusion is that, although nearly half of the words in adictionary are ambiguous in the sense that they require more than one definition, onlyapproximately one-third of this ambiguity corresponds to ambiguity in the underlyingconcept (as defined in section 4).240Cooper A Mathematical Model of Historical SemanticsFigure 6Plots of the predicted values of Ns for ?
= 0, ?
= 0.31, and ?
= 1.0.The value of the concept creation factor ?
found for different dictionaries dependson the number of divisions into different senses the lexicographer chooses to list foreach word.
We can nevertheless calculate the average number of concepts per wordin a dictionary.
This number should be more independent of lexicographic choices.Table 2 also lists c, the average number of concepts per word, which is given byc = 1 + ?
(m ?
1), for each of the dictionaries studied.
The average number of conceptsper word is not the same, even for dictionaries of the same language.
Variations are tobe expected as a result of different lexicographical choices of which words and sensesto include in the dictionary.
We can note, in particular, that technical terms do not havethe same distribution of number of senses per word as everyday words.
Furthermore,many derived words do not have their own entries but are simply listed at the end ofthe entry for the root word.
For example, in the LDCE [24], solidly and solidness have nosenses listed and were hence ignored in our study, even though solid has 15 senses inthe same dictionary.Table 2Average number m of meanings listed per word, concept-creation factor ?, and average numberc of concepts per word for various dictionaries.Dictionary m ?
cLarousse (English) [19] 1.67 0.41 1.27Shorter Oxford English Dictionary (English) [32] 2.26 0.22 1.28New Shorter Oxford English Dictionary (English) [28] 2.26 0.24 1.30Longman Dictionary of Contemporary English (English) [24] 2.04 0.31 1.32Oxford Illustrated (English) [31] 2.46 0.22 1.32Le Robert Junior (French) [23] 1.34 0.47 1.16Acade?mie Franc?aise (French) [9] 1.64 0.29 1.19Hachette (French) [12] 1.53 0.38 1.20Le Petit Robert (French) [21] 1.83 0.28 1.23Le Grand Robert (French) [20] 1.79 0.46 1.36241Computational Linguistics Volume 31, Number 2Despite these interdictionary variations, we can nevertheless conclude that theaverage number of concepts per word (as defined in section 4) is approximately 1.3for English dictionaries and a little less for French dictionaries.7.
Further Experiments to Validate the ModelAs with any scientific theory, if our theory is correct, we should be able to put it to thetest by means of experiment.
Playing the devil?s advocate, we invented several exper-iments which, if unsuccessful, would demonstrate the invalidity of our mathematicalmodel.First, we performed a chi-square test to compare the observed values of Ns andthe values of Ns predicted by our model (as calculated from equation (6)).
For nineout of the ten dictionaries tested, the ?2 value was less than ?20.10 (the value whichshould be exceeded in only 10% of random trials).
In the one remaining case, ?2 wasonly marginally greater than ?20.10.
These results are consistent with the hypothesisthat the difference between the observed and predicted values of Ns is due to ran-dom sampling and that Es = Nobss ?
Npreds (for s = 1, 2, .
.
.)
is an independent normallydistributed random variable with mean zero (Hoel 1984).
It is interesting to note thatthe difference between the observed values of Ns and those predicted by our modelwith ?
= 0 (corresponding to the hypothesis that associations are with words) or ?
= 1(corresponding to the hypothesis that associations are with senses) are both statisticallyhighly significant (at levels of 15 and 28 standard deviations, respectively, in the case ofthe LCDE [24]).In order to test the validity of the stationary-state hypothesis, we simulated thegeneration of a dictionary using the stochastic process model described in section 5.We used a random number generator to decide whether the next step should be thecreation of a new word or the creation of a new sense for an existing word.
Figure 7 is agraphical summary of one such simulation, for the particular values m = 0.6, t = 0, and?
= 0.3.
The values of P(1), P(2), P(3), P(4), and P(5) are plotted against the number ofwords generated.
After the generation of only 1,000 senses (which corresponds to lessFigure 7Values of P(1), P(2), P(3), P(4), and P(5) against the number of words generated in the simulationof the evolution of a dictionary.242Cooper A Mathematical Model of Historical Semanticsthan 600 words), the values of P(1), P(2), P(3), P(4), and P(5) are practically constant.
Wecan deduce that a steady state has been attained long before the simulation generates adictionary of size comparable to those studied (several tens of thousands of senses).We conclude that the stationary-state hypothesis is, in fact, for dictionaries of anyreasonable size, simply a mathematical consequence of our other assumptions.To check the validity of our assumption that the average number of concepts corre-sponding to a word with s senses is 1 + ?
(s ?
1), we tested a more general linear modelb + ?
(s ?
1) for a constant b.
The best-fit values of b for each dictionary were all foundto be between 0.98 and 1.08, thus confirming our assumption b = 1.Our conclusion that there is only a negligible loss of word senses from dictionariesthrough obsolescence contrasts with the fact that 22% of the words in the Oxford EnglishDictionary (OED) [30] are marked as obsolete.
Nevalainen (1999) points out that many ofthese obsolete words were abortive attempts by pre-17th-century writers to introducenew words which simply never caught on.
Garner (1982) attributes 1,700 neologisms toShakespeare alone.
Before the publication of the first monolingual English dictionariesin the early 17th century, both vocabulary and spelling were more a matter of personaltaste than convention.
Standardization occurred only after the publication of SamuelJohnson?s dictionary [8] in the 18th century.
We should mention in passing that the veryexhaustiveness of the OED makes it completely unsuitable (in the present context) asan accurate representation of the English language, since 90% of the senses listed areunknown to the majority of educated native English speakers (Winchester 2003).
Thusour model cannot be expected to provide a faithful prediction of the evolution of theOED, since we assume that the set of word senses in a dictionary is an approximationof those available to people who create new senses for existing words.
Instead ofattempting to list all English words ever used, most dictionaries aim simply to list aset of words that an educated person might reasonably encounter during his or herlifetime, which is more in keeping with the assumptions of our model.
Not surprisingly,therefore, fitting our model to values of Ns obtained from the OED gave incoherentvalues of the parameters (?
= 1.51 when, by assumption, we should have 0 ?
?
?
1).We obtained a similar anomalous best-fit value ?
= 1.16 for Webster?s Third InternationalDictionary [34], no doubt because this dictionary is again so exhaustive.It is worth going back to the counts of the number of senses per entry in specializeddictionaries [1, 7, 10, 17], plotted in Figure 3, to explain why these do not fit our model.The number of translations of a French word w in English slang [17] is related to thenumber of synonyms of w [10], since they both concern the onomasiological questionof the different ways the same concept can be expressed in a language.
This is theconverse of the semasiological question of the development of different meanings ofa given word, which is the problem our model addresses.The number of meanings of abbreviations and acronyms [7] is closely related tothe question of the distribution of homographs in a language, since abbreviationsand acronyms almost invariably obtain new meanings by coincidence rather than byassociation with existing meanings.
For example, the ?temperature?
and ?temporary?meanings of the abbreviation temp were clearly not derived by some direct semanticassociation between the notions of temperature and temporary (as would be required byour model).The distribution of the number of meanings of scientific and technical terms [1] can,on the other hand, be partly explained by our model.
The reason that the distributionof these types of terms is so far from satisfying the near-exponential rule is simply that75% of the terms listed in scientific and technical dictionaries are composed of at leasttwo words.
When we count only single-word entries (as we did for all dictionaries in243Computational Linguistics Volume 31, Number 2Table 3Average number m of meanings per word, concept-creation factor ?, and average number c ofconcepts per word for three monolingual Basque dictionaries.Dictionary m ?
cBasque School [14] 1.35 0.48 1.17Basque Learner?s [16] 1.36 0.50 1.18Basque Modern [15] 1.38 0.55 1.21Figures 1 and 2), we obtain a distribution which can be explained by our model.
Wefound that, although the average number of senses listed per word for the scientificand technical dictionary we examined [1] was much less than for English dictionariesof everyday language [19, 24, 28, 31, 32] (1.35 compared to 2.0), the number of conceptsper word was approximately the same at 1.32.In order to test the universality of the near-exponential rule, we also studied threemonolingual Basque dictionaries [14, 15, 16].
Basque is a well-known language isolate.The curves of log Ns against s were again nearly straight lines with a slight positivecurvature, and the values of Ns predicted by our model provided a very good fit to theobserved values of Ns.
The corresponding values of m, ?, and c are given in Table 3.
Thenumber of concepts per word was approximately 1.2 for all three dictionaries.Our model assumes that no ambiguity arises in deciding what constitutes a word.However, such ambiguity is clearly present in fusional languages.
In this article, wehave chosen the pragmatically simple definition that the words of a language can be ap-proximated by those sequences of characters without spaces whose meanings are listedin a given dictionary.
Applying this definition to a German monolingual dictionary [13],we observed the usual near-exponential distribution in Ns.
The best-fit values of theparameters of our model were m = 1.20, ?
= 0.80, and c = 1.16.
The average numberof meanings per word m and the average number of concepts per word c are low, nodoubt because many specialized terms which are expressed by a sequence of words inother languages count, according to our definition, as a single word in German.
Furtherresearch is required to test our model on other languages with complex morphology.Finally, we were surprised that the number of concepts per word was almostidentical for the five English dictionaries tested (see Table 2).
However, we found thatthis was not always the case, since further trials on six other English dictionaries gave alarger range of values, shown in Table 4, varying from 1.23 to 1.55.Table 4Average number m of meanings per word, concept-creation factor ?, and average number c ofconcepts per word for six English dictionaries.Dictionary m ?
cOxford Advanced Learner?s [29] 1.56 0.41 1.23Collins Concise [4] 2.22 0.24 1.29Collins Learner?s [3] 1.74 0.39 1.29Nelson [26] 1.72 0.43 1.31Collins School [5] 1.64 0.56 1.36Johnson [8] 1.55 1.00 1.55244Cooper A Mathematical Model of Historical Semantics8.
Relevance to Computational LinguisticsOne application of our model is a simple method for testing whether an attempt togroup word senses into distinct concepts (as defined in section 4) has been successful.The number NWi of words representing i concepts should demonstrate a distributionwith ?
close to one, whereas the number NCj of concepts covering j dictionary meaningsshould demonstrate a distribution with ?
close to zero (i.e., an exponential distribution,as illustrated in Figure 6).
Such a grouping of word senses into concepts is clearly usefulnot only in computer models of natural languages, but also in lexicography and histor-ical linguistics.
In lexicography, different rules have been proposed for identifying pol-ysemy, based on etymology, statistical analysis of colocations in corpora, the existenceof zeugma (such as *there is a pen on the table and one outside for the sheep), the existenceof different synonyms (such as present?now, present?gift), antonyms (right?wrong, right?left), or paronyms (race?racing, race?racist), and the existence of ambiguous questions(such as the canine/male ambiguity of the word dog brought out by the question ?Is it adog??)
(Robins 1987; Ayto 1983; Cruse 1986).
In the context of computational linguistics,Mihalcea and Moldovan (2001) relaxed these rules in order to find a more coarse-grained representation in WordNet, by grouping meanings based on similar synsetstogether with the existence of a common hypernym, antonym, or pertainym.
The pos-sible translations of a word w into several foreign languages is another useful practicaltool for the grouping of the meanings of w into concepts (Resnik and Yarowsky 2000).We have introduced an equivalence relation between word meanings: S1 ?
S2 if thesenses S1, S2 of a word w could give rise to the same new senses for w by metaphor,metonymy, etc.
The grouping of meanings into the corresponding equivalence classescould be an essential part of an automatic system for the interpretation of nonstandarduses of words.
Words are often used with a meaning which is not explicitly listed in adictionary.
Metaphor and metonymy are obvious examples, but we can also mentionmeanings which are too specialized or too new to be listed in a general-purpose dictio-nary (such as the Internet meanings of the words provider, home, and portal, for example).Analysis of the plot of log Ns against s provides a method for identifying the criteriaused in the compilation of a dictionary.
A large positive curvature is characteristicof a dictionary whose aim is exhaustiveness.
The OED [30] and Webster?s [34] areexamples, and perhaps to a lesser extent Johnson?s dictionary [8].
A small positivecurvature indicates a general-purpose dictionary whose aim is to list those words andmeanings that an educated person can reasonably be expected to encounter during hisor her lifetime.
Machine-readable dictionaries play an important role in many natural-language-processing systems, and the choice of dictionary is a critical one.
In manyapplications, an exhaustive dictionary is inappropriate.
Finding the best-fit value of theconcept creation factor ?
has allowed us to identify such dictionaries.
Our model couldalso be used to estimate performance characteristics of systems which use machine-readable dictionaries, since we have given a formula for the expected number of wordswith s senses.
For example, this might help us judge which is the best data structure touse to store a dictionary.An important aspect of the present work is the apparently universal nature of thenear-exponential rule (with a slightly positive curvature when plotted on a logarithmicscale) for the number of words Ns with s dictionary meanings.
This provides an insightinto language in general rather than any one language in particular.
Our mathematicalmodel of historical semantics provides a very plausible explanation for this generalrule.
Various mathematical models of the evolution of networks have been proposedin recent years which explain other statistical phenomena in linguistics, such as the245Computational Linguistics Volume 31, Number 2small-worlds property of semantic nets (Gaume et al 2002).
It is worth pointing outthat Price?s (1976) classical model for the number of journal articles with s citations ismathematically identical to our predicted value of Ns if we set ?
= 1.
Since our model isa strict generalization of Price?s model, it may find applications, both within and beyondthe frontiers of linguistics, as a more general model for the prediction of network growth(Newman 2003).9.
ConclusionEmpirical evidence indicates that the number of senses per word in a dictionary hasan approximately exponential distribution.
We have shown that a stochastic model ofhistorical semantics not only can explain this near-exponential phenomenon but canalso use the distance from an exponential distribution to estimate the average numberof distinct concepts per word as well as the concept creation factor (the percentage ofnew senses for existing words which can be considered genuinely new concepts).Further research is required to determine whether refinements to the mathematicalmodel presented in this article can produce a more accurate model by, for example,distinguishing among different parts of speech, distinguishing between everyday andtechnical terms, or introducing the extra parameter word-frequency.
The introduction tothe OED states that prepositions generally have more senses than verbs and adjectives,which in turn have more senses than nouns.
Pagel (2000) emphasizes the fact that theevolution of language is not identical for all words.
Fundamental vocabulary, includingbody parts, seasons, and cosmological terms, are more stable than less basic words(Swadesh 1952).
Zipf (1949) was the first to notice a correlation between word frequencyand number of senses listed in a dictionary, in the form of a hyperbolic distribution.
Thiscan be summarized by saying that a word with frequency rank r has on average twiceas many meanings as a word with frequency rank 10r.
A sophisticated stochastic modelalong the lines of the one presented in this article but taking into account frequencywould require information on the relative frequency of each different sense of eachword.
Unfortunately this is beyond the scope of this preliminary article, whose aim issimply to show that a simple stochastic model (based on grouping senses into distinctconcepts liable to give rise to different new senses) can explain a universal property ofmonolingual dictionaries.A more intriguing avenue of future research is the investigation of the possibility ofusing statistical analysis of dictionaries to model synonymy rather than (or as well as)polysemy.
Indeed, it is an open question whether a similar approach to that followed inthis work can be used to group together senses of different words which correspond tothe same concept.ReferencesAcademic Press Dictionary of Science andTechnology, ed.
C. Morris, Academic Press,London, 1992 (c. 100,000 entries).Basque-English Dictionary, G. Aulestia,University of Nevada Press, Reno and LasVegas, 1989 (c. 30,000 words).Collins Cobuild Learner?s Dictionary, HarperCollins, London, 1996 (c. 24,000 words).Collins Concise Dictionary of the EnglishLanguage, 2nd edition, Collins, London,1988 (c. 37,000 words).Collins School Dictionary, Collins, Glasgow,1989 (c. 17,000 words).Diciona?rio de France?s Portugue?s, Ol?
?vio daCosta Carvalho, Porto Editora, Porto, 1997(c. 53,000 words).Dictionary of Abbreviations and Acronyms, 2ndedition, Tec & Doc?Lavoisier, Paris, 1992(c. 50,000 entries).A Dictionary of the English Language, SamuelJohnson, facsimile edition, Times Books,London, 1979 (original edition published1755) (c. 40,000 words).246Cooper A Mathematical Model of Historical SemanticsDictionnaire de l?Acade?mie Franc?aise A-Enz,Editions Julliard, Paris, 1994 (c. 56,000words in the complete dictionary).Dictionnaire des synonymes de la languefranc?aise, R. Bailly, Librairie Larousse,Paris, 1971 (c. 2,600 words).Dictionnaire ge?ne?ral Franc?ais-Italien, Larousse,Paris, 1994 (c. 34,000 words).Dictionnaire universel de poche, Hachette,Paris, 2000 (c. 32,000 words).Duden Deutsches Universalwo?rterbuch,Dudenverlag, Mannheim, 1996(c. 122,000 words).Europa Hiztegia?Eskola berrirakoa, Adorez 6,Bilbao, 1993 (c. 24,000 words).Euskal Hiztegi Modernoa, Elhuyar KulturElkarten/Elkar SL, San Sebastian, 1994(c. 38,000 words).Euskara Ikaslearen Hiztegia, Ibon Sarasola,Vox, Barcelona, 1999 (c. 26,000 words).Harrap?s English-French Slang Dictionary,Harrap, London, 1984 (c. 10,000 words).Larousse Dictionnaire Franc?ais-Espagnol,Larousse, Paris, 1989 (c. 47,000 words).Larousse English Dictionary, Larousse-Bordas,Paris, 1997 (c. 29,000 words).Le Grand Robert de la langue franc?aise,2nd edition, Dictionnaires Le Robert, Paris(9 volumes), 1992 (c. 80,000 words).Le Petit Robert, Dictionnaires Le Robert, Paris,2000 (c. 46,000 words).Le Robert & Collins English-French Dictionary,HarperCollins, Glasgow, 1978 (c. 31,000words).Le Robert Junior, Dictionnaires Le Robert,Paris, 1999 (c. 20,000 words).Longman Dictionary of Contemporary English,Longman, Harlow, UK, 1978 (c. 38,000words).Mounged de poche franc?ais-arabe, 10th edition,Dar El-Machreq, Beirut, 1983 (c. 15,000words).Nelson Contemporary English Dictionary,ed.
W. T. Cunningham, Nelson,Walton-on-Thames, Surrey, UK, 1977(c. 20,000 words).New Crown Japanese-English Dictionary,Sanseido, Tokyo, 1968 (c. 55,000 words).New Shorter Oxford English Dictionary,Clarendon Oxford, 1993 (c. 78,000 words).Oxford Advanced Learner?s EncyclopedicDictionary, Oxford University Press,Oxford, 1992 (c. 33,000 words).Oxford English Dictionary, 2nd edition,Clarendon, Oxford, 1989 (c. 290,000words).Oxford Illustrated Dictionary, 2nd edition,Oxford University Press, Oxford, 1975(c. 50,000 words).Shorter Oxford English Dictionary, Clarendon,Oxford, 1933 (c. 63,000 words).Turkish-English Dictionary, H. C. Hony, 2ndedition, Oxford University Press, Oxford,1957 (c. 16,000 words).Webster?s Third International Dictionary,Encyclopaedia Britannica, Chicago, 1986(c. 156,000 words).Algeo, John.
1998.
Vocabulary.
In SuzanneRomaine, editor, The Cambridge Historyof the English Language, Vol.
4.,Cambridge University Press, Cambridge,pages 57?91.Antilla, Raimo.
1989.
Historical andComparative Linguistics, 2nd ed., volume 6of Current Issues in Linguistic Theory.Benjamins, Amsterdam/Philadelphia.Ayto, John R. 1983.
On specifying meaning:Semantic analysis and dictionarydefinitions.
In Reinhard R. K. Hartmann,editor, Lexicography: Principles andPractice.
Academic Press, London,pages 89?98.Bloomfield, Leonard.
1933.
Language.
HenryHolt, New York.Chalker, Sylvia and Edmund Weiner.
1994.The Oxford Dictionary of English Grammar.BCA, London.Cruse, D. Alan.
1986.
Lexical Semantics,Cambridge University Press, Cambridge.Cruse, D. Alan.
1995.
Polysemy and relatedphenomena from a cognitive linguisticviewpoint.
In P. Saint-Dizier and E. Viegas,editors, Computational Lexical Semantics.Cambridge University Press, Cambridge,pages 33?49.Fellbaum, Christiane.
1998.
WordNet, AnElectronic Lexical Database.
MIT Press,Cambridge, MA.Fuchs, Catherine.
1996.
Les ambigu?
?te?s dufranc?ais.
Collection l?Essentiel Franc?ais.Ophrys, Paris.Garner, Bryan A.
1982.
Shakespeare?s latinateneologisms.
Shakespeare Studies, 15:149?170.Gaume, Bruno, Karine Duvignau, Oliver.Gasquet, and Marie-Dominique Gineste.2002.
Forms of meaning, meaning offorms.
Journal of Experimental andTheoretical Artificial Intelligence, 14(1):61?74.Geeraerts, Dirk.
1997.
Diachronic PrototypeSemantics: A Contribution to HistoricalLexicology.
Clarendon, Oxford.Gramley, Stephan.
2001.
The Vocabulary ofWorld English.
Arnold, London.Hayes, Brian.
1999.
The web of words.American Scientist (March?April):108?112.247Computational Linguistics Volume 31, Number 2Hoel, Paul G. 1984.
Introduction toMathematical Statistics, 5th ed.
Wiley,New York.Mandala, Rila, Takenobu Tokunaga, andHozumi Tanaka.
1999.
Combininghand-made and automatically constructedthesauri for information retrieval.In Proceedings of the International JointConference on Artificial Intelligence,Stockholm, pages 920?925.Mihalcea, Rada and Dan I. Moldovan.
2001.EZ.WordNet: Principles for automaticgeneration of a coarse grained WordNet.In Proceedings of the FLAIRS Conference,pages 454?458.Nevalainen, Terttu.
1999.
Early ModernEnglish lexis and semantics.
In Roger Lass,editor, The Cambridge History of the EnglishLanguage, Vol.
3, Cambridge UniversityPress, Cambridge, pages 332?458.Newman, Mark E. J.
2003.
The structure andfunction of complex networks.
SIAMReview 45:169?256.Onions, C. T., editor.
1966, The OxfordDictionary of English Etymology.
OxfordUniversity Press, Oxford.Pagel, Mark.
2000.
The history, rate andpattern of world linguistic evolution.
InChris Knight, Michael Studdert-Kennedy,and James R. Hurford editors, TheEvolutionary Emergence of Language.Cambridge University Press, Cambridge,pages 391?416.Picoche, Jacqueline.
1992.
Dictionnaireetymologique du Franc?ais.
Dictionnaires LeRobert, Paris.Price, Derek de S. 1976.
A general theory ofbibliometric and other cumulativeadvantage processes.
Journal of theAmerican Society Information Science, 27:292?306.Resnik, Philip and David Yarowsky.
2000.Distinguishing systems and distinguishingsenses: New evaluation methods for wordsense disambiguation.
Natural LanguageEngineering 5(3):113?133.Robins, Robert H. 1987.
Polysemy and thelexicographer.
In Robert Burchfield, editor,Studies in Lexicography.
Oxford UniversityPress, Oxford, pages 52?75.Schendl, Herbert.
2001.
Historical Linguistics.Oxford University Press, Oxford.Stevenson, Mark and Yorick Wilks.
1999.Combining weak knowledge sources forsense disambiguation.
In Proceedings of theInternational Joint Conference on ArtificialIntelligence, Stockholm, pages 884?889.Stockwell, Robert and Donka Minkova.2001.
English Words: History andStructure.
Cambridge University Press,Cambridge.Swadesh, Morris.
1952.
Lexico-statisticdating of prehistoric ethnic contacts.Proceedings of the American PhilosophicalSociety, 96:452?463.Traugott, Elizabeth Cross and Richard B.Dasher.
2002.
Regularity in Semantic Change.Cambridge University Press, Cambridge.Vossen, Piek.
2001.
Condensed meaning inEuroWordNet.
In Pierrette Bouillonand Federica Busa, editors, The Language ofWord Meaning.
Cambridge UniversityPress, Cambridge, pages 363?383.Winchester, Simon.
2003.
The Meaning ofEverything: The Story of the Oxford EnglishDictionary.
Oxford University Press,Oxford.Zipf, Goerge K. 1972.
Human Behaviour andthe Principle of Least Effort: An Introductionto Human Ecology.
Hafner, New York(facsimile of 1949 edition,Addison-Wesley).248
