Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 55?60,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsInteractive Annotation Learning with Indirect Feature VotingShilpa Arora and Eric NybergLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{shilpaa,ehn}@cs.cmu.eduAbstractWe demonstrate that a supervised annotationlearning approach using structured featuresderived from tokens and prior annotations per-forms better than a bag of words approach.We present a general graph representation forautomatically deriving these features from la-beled data.
Automatic feature selection basedon class association scores requires a largeamount of labeled data and direct voting canbe difficult and error-prone for structured fea-tures, even for language specialists.
We showthat highlighted rationales from the user canbe used for indirect feature voting and sameperformance can be achieved with less labeleddata.We present our results on two annotationlearning tasks for opinion mining from prod-uct and movie reviews.1 IntroductionInteractive Annotation Learning is a supervised ap-proach to learning annotations with the goal of min-imizing the total annotation cost.
In this work, wedemonstrate that with additional supervision per ex-ample, such as distinguishing discriminant features,same performance can be achieved with less anno-tated data.
Supervision for simple features has beenexplored in the literature (Raghavan et al, 2006;Druck et al, 2008; Haghighi and Klein, 2006).
Inthis work, we propose an approach that seeks super-vision from the user on structured features.Features that capture the linguistic structure intext such as n-grams and syntactic patterns, referredto as structured features in this work, have beenfound to be useful for supervised learning of annota-tions.
For example, Pradhan et al (2004) show thatusing features like syntactic path from constituentto predicate improves performance of a semanticparser.
However, often such features are ?hand-crafted?
by domain experts and do not generalize toother tasks and domains.
In this work, we proposea general graph representation for automatically ex-tracting structured features from tokens and prior an-notations such as part of speech, dependency triples,etc.
Gamon (2004) shows that an approach usinga large set of structured features and a feature selec-tion procedure performs better than an approach thatuses a few ?handcrafted?
features.
Our hypothesisis that structured features are important for super-vised annotation learning and can be automaticallyderived from tokens and prior annotations.
We testour hypothesis and present our results for opinionmining from product reviews.Deriving features from the annotation graph givesus a large number of very sparse features.
Fea-ture selection based on class association scores suchas mutual information and chi-square have oftenbeen used to identify the most discriminant features(Manning et al, 2008).
However, these scores arecalculated from labeled data and they are not verymeaningful when the dataset is small.
Supervisedfeature selection, i.e.
asking the user to vote for themost discriminant features, has been used as an al-ternative when the training dataset is small.
Ragha-van et al (2006) and Druck et al (2008) seek feed-back on unigram features from the user for docu-ment classification tasks.
Haghighi and Klein (2006)ask the user to suggest a few prototypes (examples)for each class and use those as features.
These ap-proaches ask the annotators to identify globally rel-55evant features, but certain features are difficult tovote on without the context and may take on verydifferent meanings in different contexts.
Also, allthese approaches have been demonstrated for uni-gram features and it is not clear how they can beextended straightforwardly to structured features.We propose an indirect approach to interactivefeature selection that makes use of highlighted ra-tionales from the user.
A rationale (Zaidan et al,2007) is the span of text a user highlights in supportof his/her annotation.
Rationales also allow us toseek feedback on features in context.
Our hypothe-sis is that with rationales, we can achieve same per-formance with lower annotation cost and we demon-strate this for opinion mining from movie reviews.In Section 2, we describe the annotation graphrepresentation and motivate the use of structuredfeatures with results on learning opinions from prod-uct reviews.
In Section 3, we show how rationalescan be used for identifying the most discriminantfeatures for opinion classification with less trainingdata.
We then list the conclusions we can draw fromthis work, followed by suggestions for future work.2 Learning with Structured FeaturesIn this section, we demonstrate that structured fea-tures help in improving performance and propose aformal graph representation for deriving these fea-tures automatically.2.1 Opinions and Structured FeaturesUnigram features such as tokens are not sufficientfor recognizing all kinds of opinions.
For example,a unigram feature good may seem useful for identi-fying opinions, however, consider the following twocomments in a review: 1) This camera has good fea-tures and 2) I did a good month?s worth of researchbefore buying this camera.
In the first example,the unigram good is a useful feature.
However, inthe second example, good is not complementing thecamera and hence will mislead the classifier.
Struc-tured features such as part-of-speech, dependencyrelations etc.
are needed to capture the languagestructure that unigram features fail to capture.2.2 Annotation Graph and FeaturesWe define the annotation graph as a quadruple: G =(N,E,?, ?
), where N is the set of nodes, E is theset of edges E ?
N ?
N , ?
= ?N ?
?E is aset of labels for nodes and edges.
?
is the label-ing function ?
: N ?
E ?
?, that assigns labels tonodes and edges.
In this work, we define the set oflabels for nodes, ?N as tokens, part of speech anddependency annotations and set of labels for edges,?E as relations, ?E = {leftOf, parentOf, restricts}.The leftOf relation is defined between two adjacentnodes.
The parentOf relation is defined between thedependency type and its attributes.
For example, forthe dependency triple ?nsubj perfect camera?, thereis a parentOf relation between the dependency type?nsubj?
and tokens ?perfect?
and ?camera?.
The re-stricts relation exists between two nodes a and b iftheir textual spans overlap completely and a restrictshow b is interpreted.
For a word with multiple sensesthe restricts relation between the word and its part ofspeech, restricts the way the word is interpreted, bycapturing the sense of the word in the given context.The Stanford POS tagger (Toutanova and Manning,2000) and the Stanford parser (Klein and Manning,2003) were used to produce the part of speech anddependency annotations.Features are defined as subgraphs, G?
=(N ?, E?,?
?, ??)
in the annotation graph G, such thatN ?
?
N ,E?
?
N ?
?N ?
andE?
?
E, ??
= ??N??
?Ewhere ?
?N ?
?N and ?
?E ?
?E and ??
: N ??E?
???.
For a bag of words approach that only uses to-kens as features, ?
?N = T , where T is the tokenvocabulary and E = ?
and ?E = ?
(where ?
is thenull set).
We define the degree of a feature subgraphas the number of edges it contains.
For example, theunigram features are the feature subgraphs with noedges i.e.
degree = 0.
Degree?
1 features are thefeature subgraphs with two nodes and an edge.
Inthis paper, we present results for feature subgraphswith degree = 0 and degree = 1.Figure 1 shows the partial annotation graph fortwo comments discussed above.
The feature sub-graph that captures the opinion expressed in 1(a),can be described in simple words as ?camera hasfeatures that are good?.
This kind of subject-objectrelationship with the same verb, between the ?cam-era?
and what?s being modified by ?good?, is notpresent in the second example (1(b)).
A slight modi-fication of 1(b), I did a month?s worth of research be-fore buying this good camera does express an opin-ion about the camera.
A bag of words approach thatuses only unigram features will not be able to differ-56entiate between these two examples; structured fea-tures like dependency relation subgraphs can capturethis linguistic distinction between the two examples.P24:amod[16,29]P23:JJ[16,20]P22:dobj[12,29]P21:nsubj[5,15]restrictsparentOfparentOfparentOf(a)(b)Figure 1: The figure shows partial annotation graphs for two examples.Only some of the nodes and edges are shown for clarity.
Spans of nodesin brackets are the character spans.2.3 Experiments and ResultsThe dataset we used is a collection of 244 Amazon?scustomer reviews (2962 comments) for five products(Hu and Liu, 2004).
A review comment is annotatedas an opinion if it expresses an opinion about an as-pect of the product and the aspect is explicitly men-tioned in the sentence.
We performed 10-fold crossvalidation (CV) using the Support Vector Machine(SVM) classifier in MinorThird (Cohen, 2004) withthe default linear kernel and chi-square feature se-lection to select the top 5000 features.
As can beseen in Table 1, an approach using degree ?
0 fea-tures, i.e.
unigrams, part of speech and dependencytriples together, outperforms using any of those fea-tures alone and this difference is significant.
Us-ing degree ?
1 features with two nodes and anedge improves performance further.
However, usingdegree?0 features in addition to degree?1 featuresdoes not improve performance.
This suggests thatwhen using higher degree features, we may leave outthe features with lower degree that they subsume.Features Avg F1 Outperformsunigram [uni] 65.74 pos,deppos-unigram [pos] 64 depdependency [dep] 63.18 -degree-0 [deg-0] 67.77 uni,pos,depdegree-1 [deg-1] 70.56 uni,pos,dep,deg-0, deg-*(deg-0 + deg-1) [deg-*] 70.12 uni,pos,dep,deg-0Table 1: The table reports the F-measure scores averaged over ten crossvalidation folds.
The value in bold in the Avg F1 column is the bestperforming feature combination.
For each feature combination in therow, outperforms column lists the feature combinations it outperforms,with significant differences highlighted in bold (paired t-test with p <0.05 considered significant).3 Rationales & Indirect Feature votingWe propose an indirect feature voting approach thatuses user-highlighted rationales to identify the mostdiscriminant features.
We present our results onMovie Review data annotated with rationales.3.1 Data and Experimental SetupThe data set by Pang and Lee (2004) consists of2000 movie reviews (1000-pos, 1000-neg) from theIMDb review archive.
Zaidan et al (2007) providerationales for 1800 reviews (900-pos, 900-neg).
Theannotation guidelines for marking rationales are de-scribed in (Zaidan et al, 2007).
An example of arationale is: ?the movie is so badly put togetherthat even the most casual viewer may notice the mis-erable pacing and stray plot threads?.
For a testdataset of 200 reviews, randomly selected from 1800reviews, we varied the training data size from 50 to500 reviews, adding 50 reviews at a time.
Trainingexamples were randomly selected from the remain-ing 1600 reviews.
During testing, information aboutrationales is not used.We used tokens1, part of speech and dependencytriples as features.
We used the KStem stemmer(Krovetz, 1993) to stem the token features.
In or-der to compare the approaches at their best perform-ing feature configuration, we varied the total num-ber of features used, choosing from the set: {1000,2000, 5000, 10000, 50000}.
We used chi-squarefeature selection (Manning et al, 2008) and theSVM learner with default settings from the Minor-third package (Cohen, 2004) for these experiments.We compare the following approaches:Base Training Dataset (BTD): We train a modelfrom the labeled data with no feature voting.1filtering the stop words using the stop word list: http://www.cs.cmu.edu/?shilpaa/stop-words-ial-movie.txt57Rationale annotated Training Dataset (RTD):We experimented with two different settings for in-direct feature voting: 1) only using features thatoverlap with rationales (RTD(1, 0)); 2) featuresfrom rationales weighted twice as much as featuresfrom other parts of the text (RTD(2, 1)).
In general,R(i, j) describes an experimental condition wherefeatures from rationales are weighted i times andother features are weighted j times.
In Minorthird,weighing a feature two times more than other fea-tures is equivalent to that feature occurring twice asmuch.Oracle voted Training Data (OTD): In order tocompare indirect feature voting to direct voting onfeatures, we simulate the user?s vote on the featureswith class association scores from a large dataset(all 1600 documents used for selecting training doc-uments).
This is based on the assumption that theclass association scores, such as chi-square, from alarge dataset can be used as a reliable discriminatorof the most relevant features.
This approach of sim-ulating the oracle with large amount of labeled datahas been used previously in feature voting (Ragha-van et al, 2006).3.2 Results and DiscussionIn Table 2, we present the accuracy results for thefour approaches described in the previous section.We compare the best performing feature configura-tions for three approaches - BTD, RTD(1, 0) andRTD (2,0).
As can be seen, RTD(1, 0) always per-forms better than BTD.
As expected, improvementwith rationales is greater and it is significant whenthe training dataset is small.
The performance ofall approaches converge as the training data size in-creases and hence we only present results up to train-ing dataset size of 500 examples in this paper.Since our goal is to evaluate the use of rationalesindependently of how many features the model uses,we also compared the four approaches in terms ofthe accuracy averaged over five feature configura-tions.
Due to space constraints, we do not includethe table of results.
On average RTD(1, 0) signif-icantly outperforms BTD when the total trainingdataset is less than 350 examples.
When the train-ing data has fewer than 400 examples, RTD(1, 0)also significantly outperforms RTD(2, 1).OTD with simulated user is an approximate up-#Ex Approach Number of Features1000 2000 5000 10000 5000050OTD 67.63 66.30 62.90 52.17 55.03BTD 58.10 57.47 52.67 51.80 55.03RTD(1,0)* 55.43 55.93 61.63 61.63 61.63RTD(2,1) 57.77 57.53 52.73 52.30 56.33100OTD 71.97 71.07 70.27 69.37 64.33BTD 64.17 64.43 62.70 56.63 64.37RTD(1,0)* 65.43 63.27 65.13 67.23 67.23RTD(2,1) 64.27 63.93 62.47 56.10 63.77150OTD 73.83 74.83 74.20 74.00 63.83BTD 66.17 67.77 68.60 64.33 60.47RTD(1,0)* 69.30 68.30 67.27 71.30 71.87RTD(2,1) 68.00 67.07 68.43 63.57 58.90200OTD 74.83 75.87 75.70 75.10 56.97BTD 71.63 71.37 72.57 71.53 58.90RTD(1,0) 72.23 72.63 71.63 73.80 73.93RTD(2,1) 71.20 71.10 73.03 70.77 57.87250OTD 75.63 76.90 77.70 77.67 62.20BTD 72.60 73.57 74.73 75.20 58.93RTD(1,0) 73.00 73.57 73.57 74.70 76.70RTD(2,1) 72.87 73.90 74.63 75.40 57.43300OTD 76.57 77.67 78.93 78.43 68.17BTD 72.97 74.13 74.93 76.57 63.83RTD(1,0) 74.43 74.83 74.67 74.73 77.67RTD(2,1) 72.67 74.53 74.37 76.53 61.30350OTD 76.47 78.20 80.20 79.80 71.73BTD 74.43 74.30 74.73 77.27 66.80RTD(1,0) 75.07 76.20 75.80 75.20 78.53RTD(2,1) 74.63 75.70 74.80 78.23 64.93400OTD 77.97 78.93 80.53 80.60 75.27BTD 75.83 76.77 76.47 78.93 70.63RTD(1,0) 75.17 76.40 75.83 76.00 79.23RTD(2,1) 75.73 76.07 76.80 78.50 68.20450OTD 77.67 79.20 80.57 80.73 77.13BTD 75.73 76.80 77.80 78.80 74.37RTD(1,0)* 74.83 76.50 76.23 76.47 80.40RTD(2,1) 75.87 76.87 77.87 78.87 71.80500OTD 78.03 80.10 81.27 81.67 79.87BTD 75.27 77.33 79.37 80.30 75.73RTD(1,0) 75.77 77.63 77.47 77.27 81.10RTD(2,1) 75.83 77.47 79.50 79.70 74.50Table 2: Accuracy performance for four approaches, five feature con-figurations and increasing training dataset size.
Accuracy reported isaveraged over five random selection of training documents for three ran-domly selected test datasets.
The numbers in bold in a row representsthe best performing feature configuration for a given approach and train-ing dataset size.
The approach in bold represents the best performingapproach among BTD, RTD(1, 0) and RTD(2, 1) for a given train-ing dataset size.
?*?
indicates significant improvement in performanceover BTD (paired t-test with p < 0.05 considered significant).per bound for rationale based approaches.
It tellsus how far we are from direct supervision on struc-tured features.
On average, OTD significantly out-performed RTD(1, 0) for training data size of 100,150, 400, 450 and 500 examples but not always.As can be seen from Table 2, difference betweenOTD and RTD(1, 0) reduces with more trainingdata, since with more data and hence more rationaleswe get better feature coverage.Results presented here show that for a given train-ing dataset, we can boost the performance by ask-58ing the user to label rationales.
However, there isan additional cost associated with the rationales.
Itis important to evaluate how much total annotationcost rationales can save us while achieving the de-sired performance.
In Figure 2, we compare thenumber of training examples an approach needs toachieve a given level of performance.
As can beseen, RTD(1, 0) needs fewer training examples toachieve the same performance as BTD.
The differ-ence is large initially when the total number of train-ing examples is small (50 forRTD(1, 0) and 150 forBTD to achieve a performance between 66?
67).Figure 2: The Figure shows the number of examples needed by thetwo approaches, RTD(1, 0) and BTD, to achieve an accuracy in thegiven range.Comparison with Zaidan et al (2007): Zaidanet al (2007) conclude that using only features fromrationales performs worse than both: 1) using all thefeatures in the documents, and 2) using features thatdo not overlap with the rationales.
The results pre-sented in this paper seem to contradict their results.However, they only experimented with unigram fea-tures and only one approach to using features fromrationales, RTD(1, 0) and not RTD(2, 1).
In orderto compare our work directly with theirs, we exper-imented with an equivalent set of unigram features.In Table 3, we present the results using same num-ber of total features (17744) as Zaidan et al (2007).As can be seen from the table, when only unigramfeatures are used,RTD(2, 1) outperformsBTD butRTD(1, 0) performs worse than BTD.
Thus, ourresults are consistent with (Zaidan et al, 2007) i.e.using unigram features only from the rationales doesnot boost performance.From Table 3, we also analyze the improvementin performance when part of speech and depen-dency features are used in addition to the unigramfeatures i.e.
using all degree ?
0 subgraph fea-#Ex Approach uni uni-pos uni-pos-dep100OTD 68.6 68.8 61.6BTD 68.6 68.8 52.2RTD(1,0) 68.2 68.1 69.0*RTD(2,0) 70.0 67.0 51.7200OTD 73.6 73.8 75.3BTD 73.6 73.8 67.1RTD(1,0) 73.9 73.2 73.9*RTD(2,0) 75.3* 70.3 65.2300OTD 76.2 76.1 79.1BTD 76.2 76.1 73.7RTD(1,0) 75.0 74.9 77.1*RTD(2,0) 77.5* 73.3 74.8400OTD 77.4 76.8 79.9BTD 77.4 76.8 76.2RTD(1,0) 75.9 75.9 77.0RTD(2,0) 78.0 74.7 77.7*500OTD 78.1 78.1 80.0BTD 78.1 78.1 78.4RTD(1,0) 76.3 76.2 77.6RTD(2,0) 78.2 75.4 79.0Table 3: The Table reports accuracy for four approaches in a settingsimilar to (Zaidan et al, 2007).
Accuracy reported is averaged over tenrandom selection of training documents for two randomly selected testdatasets.The numbers in bold are the best among BTD, RTD(1, 0),RTD(2, 1) for a given feature combination.
?*?
highlights the signif-icant improvement in performance over BTD (using paired t-test, withp < 0.05 considered significant).tures.
For RTD(1, 0), adding these features im-proves performance for all data sizes with signifi-cant improvement for dataset size of 300 and 500 ex-amples.
RTD(1, 0) also significantly outperformsBTD when all three features are used.
For directvoting on features (OTD), a significant improve-ment with these structured features is seen when thetraining dataset size is greater than 200 examples.For BTD and RTD(2, 1) approaches, there is nosignificant improvement with these additional fea-tures.
In the future, we plan to investigate furtherthe benefit of using higher degree subgraph featuresfor opinion mining from the movie review data.Comparing ranking of features:We also com-pared the features that the rationales capture to whatthe oracle will vote for as the most relevant features.Features are ranked based on chi-square scores usedin feature selection.
We compare the ranked list offeatures from RTD(1, 0), BTD and OTD and usea weighted F-measure score for evaluating the top100 ranked features by each approach.
This measureis inspired by the Pyramid measure used in Summa-rization (Nenkova and Passonneau, 2004).
Insteadof using counts in calculating F-measure, we usedthe chi-square score assigned to the features by theoracle dataset, in order to give more weight to themore discriminant features.
As can be seen from59Table 4, RTD(1, 0) outperforms BTD in captur-ing the important features when the datasize set issmall (< 300) and this difference is significant.
Be-yond 300 examples, as the data size increases,BTDoutperforms RTD(1, 0).
This implies that the ra-tionales alone are able to capture the most relevantfeatures when the dataset is small.100 200 300 400 500 600 700RO 47.70 53.80 57.68 59.54 62.13 60.86 61.56TO 31.22 44.43 52.98 60.57 64.61 67.10 70.39Table 4: Weighted F-measure performance comparison of ranked listof features from RTD(1, 0) & OTD(RO) and BTD & OTD(TO).Results are averaged over ten random selections of the training data fora randomly selected test dataset.
Significant differences are highlightedin bold (paired t-test with p < 0.05 considered significant).4 Conclusion and Future WorkIn this work, we demonstrated that using structuredfeatures boosts performance of supervised annota-tion learning.
We proposed a formal annotationgraph representation that can be used to derive thesefeatures automatically.
However, the space of pos-sible feature subgraphs can grow very large withmore prior annotations.
Standard feature selectiontechniques based on class association scores are lesseffective when the dataset is small.
Feature votingfrom the user for identifying the relevant featuresis limited to simple features.
Supplementary inputfrom the user in terms of highlighted rationales canbe used instead to prune the feature space.
The pro-posed approach is general and can be applied to avariety of problems and features.In this work, we presented our results withdegree ?
0 and degree ?
1 feature subgraphs.We will extend our algorithm to automatically ex-tract higher degree features from the annotationgraph.
For the rationale annotated training data(RTD(i, j)), we experimented with two possiblevalues for i and j.
We aim to learn these weightsempirically using a held out dataset.
Rationales areassociated with an additional cost per example andhence two approaches, with and without the ratio-nales, are not directly comparable in terms of thenumber of examples.
In the future, we will conductan annotation experiment with real users to evaluatethe usefulness of rationales in terms of clock time.AcknowledgmentsWe would like to thank Dr. Carolyn P. Rose forher help with statistical analysis of the results.
Wewould also like to thank all the anonymous review-ers for their helpful comments.ReferencesCohen W. Minorthird: Methods for Identifying Namesand Ontological Relations in Text using Heuristics forInducing Regularities from Data.
2004.
(http://minorthird.sourceforge.net/).Druck G., Mann G. and McCallum A.
Learning from la-beled features using generalized expectation criteria.In Proceedings of the ACM SIGIR, 2008.Michael Gamon.
2004.
Sentiment classification on cus-tomer feedback data: noisy data, large feature vectors,and the role of linguistic analysis.
In Proceedings ofCOLING, 2005.Haghighi A. and Klein D. Prototype-driven learning forsequence models.
In Proceedings of the NAACL HLT2006.Minqing Hu and Bing Liu.
2004.
Mining and Summariz-ing Customer Reviews.
In Proc.
of the ACM SIGKDDInternational Conference on Knowledge Discovery &Data Mining.Klein D. and Manning C. Accurate Unlexicalized Pars-ing.
In Proceedings of ACL 2003.Krovetz R. Viewing Morphology as an Infer-ence Process.
http://ciir.cs.umass.edu/pubfiles/ir-35.pdfManning C., Raghavan P. and Schu?tze H. Introduction toInformation Retrieval.
Cambridge University Press.2008.Nenkova A. and Passonneau R. Evaluating Content Se-lection In Summarization: The Pyramid Method.
InProceedings of HLT-NAACL 2004.Pang B. and Lee L. ?A Sentimental Education: Sen-timent Analysis Using Subjectivity SummarizationBased on Minimum Cuts?
In Proceedings of the ACL,2004.Sameer S. Pradhan, Wayne Ward, Kadri Hacioglu, JamesH.
Martin, Daniel Jurafsky.
2004.
Shallow Seman-tic Parsing using Support Vector Machines.
In Pro-ceedings of HLT/NAACL-2004,Boston, MA, May 2-7, 2004Raghavan H., Madani O. and Jones R. Active Learningwith Feedback on Both Features and Instances.
Jour-nal of Machine Learning Research, 2006.Toutanova K. and Manning C. Enriching the KnowledgeSources Used in a Maximum Entropy Part-of-SpeechTagger.
In Proceedings of EMNLP/VLC-2000.Zaidan O., Eisner J. and Piatko C. Using ?annotator ra-tionales?
to improve machine learning for text catego-rization.
In Proceedings of NAACL-HLT 2007.Zaidan O. and Eisner J.
Modeling Annotators: A Genera-tive Approach to Learning from Annotator Rationales.In Proceedings of EMNLP 2008.60
