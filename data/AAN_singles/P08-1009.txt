Proceedings of ACL-08: HLT, pages 72?80,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsCohesive Phrase-based Decoding for Statistical Machine TranslationColin Cherry?Microsoft ResearchOne Microsoft WayRedmond, WA, 98052colinc@microsoft.comAbstractPhrase-based decoding produces state-of-the-art translations with no regard for syntax.
Weadd syntax to this process with a cohesionconstraint based on a dependency tree forthe source sentence.
The constraint allowsthe decoder to employ arbitrary, non-syntacticphrases, but ensures that those phrases aretranslated in an order that respects the sourcetree?s structure.
In this way, we target thephrasal decoder?s weakness in order model-ing, without affecting its strengths.
To fur-ther increase flexibility, we incorporate cohe-sion as a decoder feature, creating a soft con-straint.
The resulting cohesive, phrase-baseddecoder is shown to produce translations thatare preferred over non-cohesive output in bothautomatic and human evaluations.1 IntroductionStatistical machine translation (SMT) is complicatedby the fact that words can move during translation.If one assumes arbitrary movement is possible, thatalone is sufficient to show the problem to be NP-complete (Knight, 1999).
Syntactic cohesion1 isthe notion that all movement occurring during trans-lation can be explained by permuting children in aparse tree (Fox, 2002).
Equivalently, one can saythat phrases in the source, defined by subtrees inits parse, remain contiguous after translation.
Early?Work conducted while at the University of Alberta.1We use the term ?syntactic cohesion?
throughout this paperto mean what has previously been referred to as ?phrasal cohe-sion?, because the non-linguistic sense of ?phrase?
has becomeso common in machine translation literature.methods for syntactic SMT held to this assump-tion in its entirety (Wu, 1997; Yamada and Knight,2001).
These approaches were eventually super-seded by tree transducers and tree substitution gram-mars, which allow translation events to span sub-tree units, providing several advantages, includingthe ability to selectively produce uncohesive transla-tions (Eisner, 2003; Graehl and Knight, 2004; Quirket al, 2005).
What may have been forgotten duringthis transition is that there is a reason it was once be-lieved that a cohesive translation model would work:for some language pairs, cohesion explains nearlyall translation movement.
Fox (2002) showed thatcohesion is held in the vast majority of cases forEnglish-French, while Cherry and Lin (2006) haveshown it to be a strong feature for word alignment.We attempt to use this strong, but imperfect, char-acterization of movement to assist a non-syntactictranslation method: phrase-based SMT.Phrase-based decoding (Koehn et al, 2003) is adominant formalism in statistical machine transla-tion.
Contiguous segments of the source are trans-lated and placed in the target, which is constructedfrom left to right.
The process iterates within a beamsearch until each word from the source has beencovered by exactly one phrasal translation.
Candi-date translations are scored by a linear combinationof models, weighted according to Minimum ErrorRate Training or MERT (Och, 2003).
Phrasal SMTdraws strength from being able to memorize non-compositional and context-specific translations, aswell as local reorderings.
Its primary weakness isin movement modeling; its default distortion modelapplies a flat penalty to any deviation from source72order, forcing the decoder to rely heavily on its lan-guage model.
Recently, a number of data-driven dis-tortion models, based on lexical features and relativedistance, have been proposed to compensate for thisweakness (Tillman, 2004; Koehn et al, 2005; Al-Onaizan and Papineni, 2006; Kuhn et al, 2006).There have been a number of proposals to in-corporate syntactic information into phrasal decod-ing.
Early experiments with syntactically-informedphrases (Koehn et al, 2003), and syntactic re-ranking of K-best lists (Och et al, 2004) producedmostly negative results.
The most successful at-tempts at syntax-enhanced phrasal SMT have di-rectly targeted movement modeling: Zens et al(2004) modified a phrasal decoder with ITG con-straints, while a number of researchers have em-ployed syntax-driven source reordering before de-coding begins (Xia and McCord, 2004; Collins etal., 2005; Wang et al, 2007).2 We attempt some-thing between these two approaches: our constraintis derived from a linguistic parse tree, but it is usedinside the decoder, not as a preprocessing step.We begin in Section 2 by defining syntactic cohe-sion so it can be applied to phrasal decoder output.Section 3 describes how to add both hard and softcohesion constraints to a phrasal decoder.
Section 4provides our results from both automatic and humanevaluations.
Sections 5 and 6 provide a qualitativediscussion of cohesive output and conclude.2 Cohesive Phrasal OutputPrevious approaches to measuring the cohesion ofa sentence pair have worked with a word align-ment (Fox, 2002; Lin and Cherry, 2003).
This align-ment is used to project the spans of subtrees fromthe source tree onto the target sentence.
If a modifierand its head, or two modifiers of the same head, haveoverlapping spans in the projection, then this indi-cates a cohesion violation.
To check phrasal trans-lations for cohesion violations, we need a way toproject the source tree onto the decoder?s output.Fortunately, each phrase used to create the targetsentence can be tracked back to its original sourcephrase, providing an alignment between source and2While certainly both syntactic and successful, we considerHiero (Chiang, 2007) to be a distinct approach, and not an ex-tension to phrasal decoding?s left-to-right beam search.target phrases.
Since each source token is used ex-actly once during translation, we can transform thisphrasal alignment into a word-to-phrase alignment,where each source token is linked to a target phrase.We can then project the source subtree spans ontothe target phrase sequence.
Note that we never con-sider individual tokens on the target side, as theirconnection to the source tree is obscured by thephrasal abstraction that occurred during translation.Let em1 be the input source sentence, and f?p1 be theoutput target phrase sequence.
Our word-to-phrasealignment ai ?
[1, p], 1 ?
i ?
m, maps a sourcetoken position i to a target phrase position ai.
Next,we introduce our source dependency tree T .
Eachsource token ei is also a node in T .
We define T (ei)to be the subtree of T rooted at ei.
We define a localtree to be a head node and its immediate modifiers.With this notation in place, we can define our pro-jected spans.
Following Lin and Cherry (2003), wedefine a head span to be the projection of a singletoken ei onto the target phrase sequence:spanH (ei, T, am1 ) = [ai, ai]and the subtree span to be the projection of the sub-tree rooted at ei:spanS (ei, T, am1 ) =[min{j|ej?T (ei)}aj , max{k|ek?T (ei)}ak]Consider the simple phrasal translation shown inFigure 1 along with a dependency tree for the En-glish source.
If we examine the local tree rooted atlikes , we get the following projected spans:spanS (nobody , T, a) = [1, 1]spanH (likes, T, a) = [1, 1]spanS (pay , T, a) = [1, 2]For any local tree, we consider only the head span ofthe head, and the subtree spans of any modifiers.Typically, cohesion would be determined bychecking these projected spans for intersection.However, at this level of resolution, avoiding inter-section becomes highly restrictive.
The monotonetranslation in Figure 1 would become non-cohesive:nobody intersects with both its sibling pay and withits head likes at phrase index 1.
This complica-tion stems from the use of multi-word phrases that73nobody likes to pay taxespersonne n ' aime payer des imp?ts(nobody likes) (paying taxes)1 2Figure 1: An English source tree with translated Frenchoutput.
Segments are indicated with underlined spans.do not correspond to syntactic constituents.
Re-stricting phrases to syntactic constituents has beenshown to harm performance (Koehn et al, 2003), sowe tighten our definition of a violation to disregardcases where the only point of overlap is obscured byour phrasal resolution.
To do so, we replace spanintersection with a new notion of span innersection.Assume we have two spans [u, v] and [x, y] thathave been sorted so that [u, v] ?
[x, y] lexicograph-ically.
We say that the two spans innersect if andonly if x < v. So, [1, 3] and [2, 4] innersect, while[1, 3] and [3, 4] do not.
One can think of innersectionas intersection, minus the cases where the two spansshare only a single boundary point, where x = v.When two projected spans innersect, it indicates thatthe second syntactic constituent must begin beforethe first ends.
If the two spans in question corre-spond to nodes in the same local tree, innersectionindicates an unambiguous cohesion violation.
Un-der this definition, the translation in Figure 1 is co-hesive, as none of its spans innersect.Our hope is that syntactic cohesion will help thedecoder make smarter distortion decisions.
An ex-ample with distortion is shown in Figure 2.
In thiscase, we present two candidate French translationsof an English sentence, assuming there is no entryin the phrase table for ?voting session.?
Because theproper French construction is ?session of voting?,the decoder has to move voting after session using adistortion operation.
Figure 2 shows two methods todo so, each using an equal numbers of phrases.
Theprojected spans for the local tree rooted at beginsin each candidate are shown in Table 1.
Note theinnersection between the head begins and its modi-fier session in (b).
Thus, a cohesion-aware systemwould receive extra guidance to select (a), whichmaintains the original meaning much better than (b).Span (a) (b)spanS (session, T, a) [1,3] [1,3]*spanH (begins, T, a) [4,4] [2,2]*spanS (tomorrow , T, a) [4,4] [4,4]Table 1: Spans of the local trees rooted at begins fromFigures 2 (a) and (b).
Innersection is marked with a ?
*?.2.1 K-best List FilteringA first attempt at using cohesion to improve SMToutput would be to apply our definition as a filter onK-best lists.
That is, we could have a phrasal de-coder output a 1000-best list, and return the highest-ranked cohesive translation to the user.
We testedthis approach on our English-French developmentset, and saw no improvement in BLEU score.
Er-ror analysis revealed that only one third of the un-cohesive translations had a cohesive alternative intheir 1000-best lists.
In order to reach the remain-ing two thirds, we need to constrain the decoder?ssearch space to explore only cohesive translations.3 Cohesive DecodingThis section describes a modification to standardphrase-based decoding, so that the system is con-strained to produce only cohesive output.
This willtake the form of a check performed each time a hy-pothesis is extended, similar to the ITG constraintfor phrasal SMT (Zens et al, 2004).
To create asuch a check, we need to detect a cohesion viola-tion inside a partial translation hypothesis.
We can-not directly apply our span-based cohesion defini-tion, because our word-to-phrase alignment is notyet complete.
However, we can still detect viola-tions, and we can do so before the spans involvedare completely translated.Recall that when two projected spans a and b(a < b) innersect, it indicates that b begins before aends.
We can say that the translation of b interruptsthe translation of a.
We can enforce cohesion by en-suring that these interruptions never happen.
Be-cause the decoder builds its translations from left toright, eliminating interruptions amounts to enforcingthe following rule: once the decoder begins translat-ing any part of a source subtree, it must cover all74the voting session begins tomorrowla session de  vote d?bute  demain2 3 41(the) (session) (of voting) (begins tomorrow)(a) (b)1 2the voting session begins tomorrow34la session  commence ?
voter demain(the) (session begins) (to vote) (tomorrow)Figure 2: Two candidate translations for the same parsed source.
(a) is cohesive, while (b) is not.the words under that subtree before it can translateanything outside of it.For example, in Figure 2b, the decoder translatesthe , which is part of T (session) in f?1.
In f?2, it trans-lates begins , which is outside T (session).
Since wehave yet to cover voting , we know that the projectedspan of T (session) will end at some index v > 2,creating an innersection.
This eliminates the hypoth-esis after having proposed only the first two phrases.3.1 AlgorithmIn this section, we formally define an interruption,and present an algorithm to detect one during de-coding.
During both discussions, we represent eachtarget phrase as a set that contains the English tokensused in its translation: f?j = {ei|ai = j}.
Formally,an interruption occurs whenever the decoder wouldadd a phrase f?h+1 to the hypothesis f?h1 , and:?r ?
T such that:?e ?
T (r) s.t.
e ?
f?h1 (a.
Started)?e?
/?
T (r) s.t.
e?
?
f?h+1 (b.
Interrupted)?e??
?
T (r) s.t.
e??
/?
f?h+11 (c. Unfinished)(1)The key to checking for interruptions quickly isknowing which subtrees T (r) to check for qualities(1:a,b,c).
A na?
?ve approach would check every sub-tree that has begun translation in f?h1 .
Figure 3a high-lights the roots of all such subtrees for a hypotheticalT and f?h1 .
Fortunately, with a little analysis that ac-counts for f?h+1, we can show that at most two sub-trees need to be checked.For a given interruption-free f?h1 , we call subtreesthat have begun translation, but are not yet complete,open subtrees.
Only open subtrees can lead to inter-ruptions.
We can focus our interruption check onf?h, the last phrase in f?h1 , as any open subtree T (r)must contain at least one e ?
f?h.
If this were not theAlgorithm 1 Interruption check.?
Get the left and right-most tokens used to createf?h, call them eL and eR?
For each of e ?
{eL, eR}:i. r?
?
e, r ?
nullWhile ?e?
?
f?h+1 such that e?
/?
T (r?
):r ?
r?, r?
?
parent(r)ii.
If r 6= null and ?e??
?
T (r) such thate??
/?
f?h+11 , then f?h+1 interrupts T (r).case, then the open T (r)must have began translationsomewhere in f?h?11 , and T (r) would be interruptedby the placement of f?h.
Since our hypothesis f?h1is interruption-free, this is impossible.
This leavesthe subtrees highlighted in Figure 3b to be checked.Furthermore, we need only consider subtrees thatcontain the left and right-most source tokens eL andeR translated by f?h.
Since f?h was created from acontiguous string of source tokens, any distinct sub-tree between these two endpoints will be completedwithin f?h.
Finally, for each of these focus pointseL and eR, only the highest containing subtree T (r)that does not completely contain f?h+1 needs to beconsidered.
Anything higher would contain all off?h+1, and would not satisfy requirement (1:b) of ourinterruption definition.
Any lower subtree would bea descendant of r, and therefore the check for thelower subtree is subsumed by the check for T (r).This leaves only two subtrees, highlighted in ourrunning example in Figure 3c.With this analysis in place, an extension f?h+1 ofthe hypothesis f?h1 can be checked for interruptionswith Algorithm 1.
Step (i) in this algorithm findsan ancestor r?
such that T (r?)
completely contains75f hf h+1fh1f hf h+1fh1f hf h+1fh1a)b) c)Figure 3: Narrowing down the source subtrees to be checked for completeness.f?h+1, and then returns r, the highest node that doesnot contain f?h+1.
We know this r satisfies require-ments (1:a,b).
If there is no T (r) that does not con-tain f?h+1, then e and its ancestors cannot lead to aninterruption.
Step (ii) then checks the coverage vec-tor of the hypothesis3 to make sure that T (r) is cov-ered in f?h+11 .
If T (r) is not complete in f?h+11 , thenthat satisfies requirement (1:c), which means an in-terruption has occurred.For example, in Figure 2b, our first interruptionoccurs as we add f?h+1 = f?2 to f?h1 = f?11 .
The de-tection algorithm would first get the left and rightboundaries of f?1; in this case, the is both eL andeR.
Then, it would climb up the tree from the untilit reached r?
= begins and r = session .
It wouldthen check T (session) for coverage in f?21 .
Sincevoting ?
T (session) is not covered in f?21 , it woulddetect an interruption.Walking up the tree takes at most linear time,and each check to see if T (r) contains all of f?h+1can be performed in constant time, provided thesource spans of each subtree have been precom-puted.
Checking to see if all of T (r) has been cov-ered in Step (ii) takes at most linear time.
Thismakes the entire process linear in the size of thesource sentence.3.2 Soft ConstraintSyntactic cohesion is not a perfect constraint fortranslation.
Parse errors and systematic violationscan create cases where cohesion works against thedecoder.
Fox (2002) demonstrated and countedcases where cohesion was not maintained in hand-aligned sentence-pairs, while Cherry and Lin (2006)3This coverage vector is maintained by all phrasal decodersto track how much of the source sentence has been covered bythe current partial translation, and to ensure that the same tokenis not translated twice.showed that a soft cohesion constraint is superior toa hard constraint for word alignment.
Therefore, wepropose a soft version of our cohesion constraint.We perform our interruption check, but we do notinvalidate any hypotheses.
Instead, each hypothe-sis maintains a count of the number of extensionsthat have caused interruptions during its construc-tion.
This count becomes a feature in the decoder?slog-linear model, the weight of which is trained withMERT.
After the first interruption, the exact mean-ing of further interruptions becomes difficult to in-terpret; but the interruption count does provide auseful estimate of the extent to which the translationis faithful to the source tree structure.Initially, we were not certain to what extent thisfeature would be used by the MERT module, asBLEU is not always sensitive to syntactic improve-ments.
However, trained with our French-Englishtuning set, the interruption count received the largestabsolute feature weight, indicating, at the very least,that the feature is worth scaling to impact decoder.3.3 ImplementationWe modify the Moses decoder (Koehn et al, 2007)to translate head-annotated sentences.
The decoderstores the flat sentence in the original sentence datastructure, and the head-encoded dependency tree inan attached tree data structure.
The tree structurecaches the source spans corresponding to each ofits subtrees.
We then implement both a hard checkfor interruptions to be used before hypotheses areplaced on the stack,4 and a soft check that is used tocalculate an interruption count feature.4A hard cohesion constraint used in conjunction with a tra-ditional distortion limit also requires a second linear-time checkto ensure that all subtrees currently in progress can be finishedunder the constraints induced by the distortion limit.76Set Cohesive UncohesiveDev-Test 1170 330Test 1563 437Table 2: Number of sentences that receive cohesive trans-lations from the baseline decoder.
This property also de-fines our evaluation subsets.4 ExperimentsWe have adapted the notion of syntactic cohesion sothat it is applicable to phrase-based decoding.
Thisresults in a translation process that respects source-side syntactic boundaries when distorting phrases.In this section we will test the impact of such infor-mation on an English to French translation task.4.1 Experimental DetailsWe test our cohesion-enhanced Moses decodertrained using 688K sentence pairs of EuroparlFrench-English data, provided by the SMT 2006Shared Task (Koehn and Monz, 2006).
Word align-ments are provided by GIZA++ (Och and Ney,2003) with grow-diag-final combination, with in-frastructure for alignment combination and phraseextraction provided by the shared task.
We decodewithMoses, using a stack size of 100, a beam thresh-old of 0.03 and a distortion limit of 4.
Weights forthe log-linear model are set using MERT, as imple-mented by Venugopal and Vogel (2005).
Our tuningset is the first 500 sentences of the SMT06 develop-ment data.
We hold out the remaining 1500 develop-ment sentences for development testing (dev-test),and the entirety of the provided 2000-sentence testset for blind testing (test).
Since we require sourcedependency trees, all experiments test English toFrench translation.
English dependency trees areprovided by Minipar (Lin, 1994).Our cohesion constraint directly targets sentencesfor which an unmodified phrasal decoder producesuncohesive output according to the definition in Sec-tion 2.
Therefore, we present our results not only oneach test set in its entirety, but also on the subsetsdefined by whether or not the baseline naturally pro-duces a cohesive translation.
The sizes of the result-ing evaluation sets are given in Table 2.Our development tests indicated that the soft andhard cohesion constraints performed somewhat sim-ilarly, with the soft constraint providing more sta-ble, and generally better results.
We confirmed thesetrends on our test set, but to conserve space, we pro-vide detailed results for only the soft constraint.4.2 Automatic EvaluationWe first present our soft cohesion constraint?s ef-fect on BLEU score (Papineni et al, 2002) for bothour dev-test and test sets.
We compare against anunmodified baseline decoder, as well as a decoderenhanced with a lexical reordering model (Tillman,2004; Koehn et al, 2005).
For each phrase pair inour translation table, the lexical reordering modeltracks statistics on its reordering behavior as ob-served in our word-aligned training text.
The lex-ical reordering model provides a good comparisonpoint as a non-syntactic, and potentially orthogonal,improvement to phrase-based movement modeling.We use the implementation provided in Moses, withprobabilities conditioned on bilingual phrases andpredicting three orientation bins: straight, invertedand disjoint.
Since adding features to the decoder?slog-linear model is straight-forward, we also experi-ment with a combined system that uses both the co-hesion constraint and a lexical reordering model.The results of our experiments are shown in Ta-ble 3, and reveal some interesting phenomena.
Firstof all, looking across columns, we can see that thereis a definite divide in BLEU score between our twoevaluation subsets.
Sentences with cohesive base-line translations receive much higher BLEU scoresthan those with uncohesive baseline translations.This indicates that the cohesive subset is easier totranslate with a phrase-based system.
Our definitionof cohesive phrasal output appears to provide a use-ful feature for estimating translation confidence.Comparing the baseline with and without the softcohesion constraint, we see that cohesion has only amodest effect on BLEU, when measured on all sen-tence pairs, with improvements ranging between 0.2and 0.5 absolute points.
Recall that the majority ofbaseline translations are naturally cohesive.
The co-hesion constraint?s effect is much more pronouncedon the more difficult uncohesive subsets, showingabsolute improvements between 0.5 and 1.1 points.Considering the lexical reordering model, we seethat its effect is very similar to that of syntactic co-hesion.
Its BLEU scores are very similar, with lex-77Dev-Test TestSystem All Cohesive Uncohesive All Cohesive Uncohesivebase 32.04 33.80 27.46 32.35 33.78 28.73lex 32.19 33.91 27.86 32.71 33.89 29.66coh 32.22 33.82 28.04 32.88 34.03 29.86lex+coh 32.45 34.12 28.09 32.90 34.04 29.83Table 3: BLEU scores with an integrated soft cohesion constraint (coh) or a lexical reordering model (lex).
Any systemsignificantly better than base has been highlighted, as tested by bootstrap re-sampling with a 95% confidence interval.ical reordering also affecting primarily the uncohe-sive subset.
This similarity in behavior is interesting,as its data-driven, bilingual reordering probabilitiesare quite different from our cohesion flag, which isdriven by monolingual syntax.Examining the system that employs both move-ment models, we see that the combination (lex+coh)receives the highest score on the dev-test set.
A largeportion of the combined system?s gain is on the co-hesive subset, indicating that the cohesion constraintmay be enabling better use of the lexical reorderingmodel on otherwise cohesive translations.
Unfor-tunately, these same gains are not born out on thetest set, where the lexical reordering model appearsunable to improve upon the already strong perfor-mance of the cohesion constraint.4.3 Human EvaluationWe also present a human evaluation designed to de-termine whether bilingual speakers prefer cohesivedecoder output.
Our comparison systems are thebaseline decoder (base) and our soft cohesion con-straint (coh).
We evaluate on our dev-test set,5 as ithas our smallest observed BLEU-score gap, and wewish to determine if it is actually improving.
Our ex-perimental set-up is modeled after the human evalu-ation presented in (Collins et al, 2005).
We providetwo human annotators6 a set of 75 English sourcesentences, along with a reference translation and apair of translation candidates, one from each sys-tem.
The annotators are asked to indicate which ofthe two system translations they prefer, or if they5The cohesion constraint has no free parameters to optimizeduring development, so this does not create an advantage.6Annotators were both native English speakers who speakFrench as a second language.
Each has a strong comprehensionof written French.Annotator #2Annotator #1 base coh equal sum (#1)base 6 7 1 14coh 8 35 4 47equal 7 4 3 14sum (#2) 21 46 8Table 4: Confusion matrix from human evaluation.consider them to be equal.
To avoid bias, the com-peting systems were presented anonymously and inrandom order.
Following (Collins et al, 2005), weprovide the annotators with only short sentences:those with source sentences between 10 and 25 to-kens long.
Following (Callison-Burch et al, 2006),we conduct a targeted evaluation; we only draw ourevaluation pairs from the uncohesive subset targetedby our constraint.
All 75 sentences that meet thesetwo criteria are included in the evaluation.The aggregate results of our human evaluation areshown in the bottom row and right-most column ofTable 4.
Each annotator prefers coh in over 60% ofthe test sentences, and each prefers base in less than30% of the test sentences.
This presents strong evi-dence that we are having a consistent, positive effecton formerly non-cohesive translations.
A completeconfusion matrix indicating agreement between thetwo annotators is also given in Table 4.
There are afew more off-diagonal points than one might expect,but it is clear that the two annotators are in agree-ment with respect to coh?s improvements.
A com-bination annotator, which selects base or coh onlywhen both human annotators agree and equal oth-erwise, finds base is preferred in only 8% of cases,compared to 47% for coh.78(1+) creating structures that do not currently exist and reducing .
.
.base de cre?er des structures qui existent actuellement et ne pas re?duire .
.
.to create structures that actually exist and do not reduce .
.
.coh de cre?er des structures qui n ?
existent pas encore et re?duire .
.
.to create structures that do not yet exist and reduce .
.
.(2?)
.
.
.
repealed the 1998 directive banning advertisingbase .
.
.
abroge?e l?interdiction de la directive de 1998 de publicite?.
.
.
repealed the ban from the 1998 directive on advertisingcoh .
.
.
abroge?e la directive de 1998 l?interdiction de publicite?.
.
.
repealed the 1998 directive the ban on advertisingTable 5: A comparison of baseline and cohesion-constrained English-to-French translations, with English glosses.5 DiscussionExamining the French translations produced by ourcohesion constrained phrasal decoder, we can drawsome qualitative generalizations.
The constraint isused primarily to prevent distortion: it provides anintelligent estimate as to when source order must berespected.
The resulting translations tend to be moreliteral than unconstrained translations.
So long asthe vocabulary present in our phrase table and lan-guage model supports a literal translation, cohesiontends to produce an improvement.
Consider the firsttranslation example shown in Table 5.
In the base-line translation, the language model encourages thesystem to move the negation away from ?exist?
andtoward ?reduce.?
The result is a tragic reversal ofmeaning in the translation.
Our cohesion constraintremoves this option, forcing the decoder to assem-ble the correct French construction for ?does not yetexist.?
The second example shows a case where ourresources do not support a literal translation.
In thiscase, we do not have a strong translation mapping toproduce a French modifier equivalent to the English?banning.?
Stuck with a noun form (?the ban?
), thebaseline is able to distort the sentence into some-thing that is almost correct (the above gloss is quitegenerous).
The cohesive system, even with a softconstraint, cannot reproduce the same movement,and returns a less grammatical translation.We also examined cases where the decoder over-rides the soft cohesion constraint and produces anuncohesive translation.
We found this was done veryrarely, and primarily to overcome parse errors.
Onlyone correct syntactic construct repeatedly forced thedecoder to override cohesion: Minipar?s conjunctionrepresentation, which connects conjuncts in parent-child relationships, is at times too restrictive.
A sib-ling representation, which would allow conjuncts tobe permuted arbitrarily, may work better.6 ConclusionWe have presented a definition of syntactic cohesionthat is applicable to phrase-based SMT.
We haveused this definition to develop a linear-time algo-rithm to detect cohesion violations in partial decoderhypotheses.
This algorithm was used to implementa soft cohesion constraint for the Moses decoder,based on a source-side dependency tree.Our experiments have shown that roughly 1/5 ofour baseline English-French translations contain co-hesion violations, and these translations tend to re-ceive lower BLEU scores.
This suggests that co-hesion could be a strong feature in estimating theconfidence of phrase-based translations.
Our softconstraint produced improvements ranging between0.5 and 1.1 BLEU points on sentences for which thebaseline produces uncohesive translations.
A humanevaluation showed that translations created using asoft cohesion constraint are preferred over uncohe-sive translations in the majority of cases.Acknowledgments Special thanks to Dekang Lin,Shane Bergsma, and Jess Enright for their usefulinsights and discussions, and to the anonymous re-viewers for their comments.
The author was fundedby Alberta Ingenuity and iCORE studentships.79ReferencesY.
Al-Onaizan and K. Papineni.
2006.
Distortion modelsfor statistical machine translation.
In COLING-ACL,pages 529?536, Sydney, Australia.C.
Callison-Burch, M. Osborne, and P. Koehn.
2006.
Re-evaluating the role of BLEU in machine translation re-search.
In EACL, pages 249?256.C.
Cherry and D. Lin.
2006.
Soft syntactic constraintsfor word alignment through discriminative training.
InCOLING-ACL, Sydney, Australia, July.
Poster.D.
Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2):201?228, June.M.
Collins, P. Koehn, and I. Kucerova.
2005.
Clause re-structuring for statistical machine translation.
In ACL,pages 531?540.J.
Eisner.
2003.
Learning non-ismorphic tree mappingsfor machine translation.
In ACL, Sapporo, Japan.Short paper.H.
J.
Fox.
2002.
Phrasal cohesion and statistical machinetranslation.
In EMNLP, pages 304?311.J.
Graehl and K. Knight.
2004.
Training tree transducers.In HLT-NAACL, pages 105?112, Boston, USA, May.K.
Knight.
1999.
Squibs and discussions: Decod-ing complexity in word-replacement translation mod-els.
Computational Linguistics, 25(4):607?615, De-cember.P.
Koehn and C. Monz.
2006.
Manual and automaticevaluation of machine translation.
In HLT-NACCLWorkshop on Statistical Machine Translation, pages102?121.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In HLT-NAACL, pages 127?133.P.
Koehn, A. Axelrod, A. Birch Mayne, C. Callison-Burch, M. Osborne, and David Talbot.
2005.
Edin-burgh system description for the 2005 IWSLT speechtranslation evaluation.
In International Workshop onSpoken Language Translation.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkit forstatistical machine translation.
In ACL.
Demonstra-tion.R.
Kuhn, D. Yuen, M. Simard, P. Paul, G. Foster, E. Joa-nis, and H. Johnson.
2006.
Segment choice models:Feature-rich models for global distortion in statisticalmachine translation.
In HLT-NAACL, pages 25?32,New York, NY.D.
Lin and C. Cherry.
2003.
Word alignment with co-hesion constraint.
In HLT-NAACL, pages 49?51, Ed-monton, Canada, May.
Short paper.D.
Lin.
1994.
Principar - an efficient, broad-coverage,principle-based parser.
In COLING, pages 42?48, Ky-oto, Japan.F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?52.F.
J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,V.
Jain, Z. Jin, and D. Radev.
2004.
A smorgasbordof features for statistical machine translation.
In HLT-NAACL 2004: Main Proceedings, pages 161?168.F.
J. Och.
2003.
Minimum error rate training for statisti-cal machine translation.
In ACL, pages 160?167.K.
Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In ACL, pages 311?318.C.
Quirk, A. Menezes, and C. Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal SMT.
In ACL, pages 271?279, Ann Arbor,USA, June.C.
Tillman.
2004.
A unigram orientation model for sta-tistical machine translation.
In HLT-NAACL, pages101?104.
Short paper.A.
Venugopal and S. Vogel.
2005.
Considerations inmaximum mutual information and minimum classifi-cation error training for statistical machine translation.In EAMT.C.
Wang, M. Collins, and P. Koehn.
2007.
Chinese syn-tactic reordering for statistical machine translation.
InEMNLP, pages 737?745.D.
Wu.
1997.
Stochastic inversion transduction gram-mars and bilingual parsing of parallel corpora.
Com-putational Linguistics, 23(3):377?403.F.
Xia and M. McCord.
2004.
Improving a statistical mtsystem with automatically learned rewrite patterns.
InProceedings of Coling 2004, pages 508?514.K.
Yamada and K. Knight.
2001.
A syntax-based statis-tical translation model.
In ACL, pages 523?530.R.
Zens, H. Ney, T. Watanabe, and E. Sumita.
2004.Reordering constraints for phrase-based statistical ma-chine translation.
In COLING, pages 205?211,Geneva, Switzerland, August.80
