Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 32?40,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPClassifying Japanese Polysemous Verbs based on Fuzzy C-meansClusteringYoshimi SuzukiInterdisciplinary Graduate School ofMedicine and EngineeringUniversity of Yamanashi, Japanysuzuki@yamanashi.ac.jpFumiyo FukumotoInterdisciplinary Graduate School ofMedicine and EngineeringUniversity of Yamanashi, Japanfukumoto@yamanashi.ac.jpAbstractThis paper presents a method for classify-ing Japanese polysemous verbs using analgorithm to identify overlapping nodeswith more than one cluster.
The algo-rithm is a graph-based unsupervised clus-tering algorithm, which combines a gener-alized modularity function, spectral map-ping, and fuzzy clustering technique.
Themodularity function for measuring clusterstructure is calculated based on the fre-quency distributions over verb frames withselectional preferences.
Evaluations aremade on two sets of verbs including pol-ysemies.1 IntroductionThere has been quite a lot of research concernedwith automatic clustering of semantically simi-lar words or automatic retrieval of collocationsamong them from corpora.
Most of this work isbased on similarity measures derived from the dis-tribution of words in corpora.
However, the factsthat a single word does have more than one senseand that the distribution of a word in a corpus is amixture of usages of different senses of the sameword often hamper such attempts.
In general, re-striction of the subject domain makes the problemof polysemy less problematic.
However, even intexts from a restricted domain such as economicsor sports, one encounters quite a large number ofpolysemous words.
Therefore, semantic classifi-cation of polysemies has been an interest since theearliest days when a number of large scale corporahave become available.In this paper, we focus on Japanese polysemousverbs, and present a method for polysemous verbclassification.
We used a graph-based unsuper-vised clustering algorithm (Zhang, 2007).
Thealgorithm combines the idea of modularity func-tion Q, spectral relaxation and fuzzy c-means clus-tering method to identify overlapping nodes withmore than one cluster.
The modularity functionmeasures the quality of a cluster structure.
Spec-tral mapping performs a dimensionality reductionwhich makes it possible to cluster in the very highdimensional spaces.
The fuzzy c-means allows forthe detection of nodes with more than one cluster.We applied the algorithm to cluster polysemousverbs.
The modularity function for measuring thequality of a cluster structure is calculated basedon the frequency distributions over verb frameswith selectional preferences.
We collected seman-tic classes from IPAL Japanese dictionary (IPAL,1987), and used them as a gold standard data.IPAL lists about 900 Japanese basic verbs, and cat-egorizes each verb into multiple senses.
Moreover,the categorization is based on verbal syntax withrespect to the choice of its arguments.
Therefore,if the clustering algorithm induces a polysemousverb classification on the basis of verbal syntax,then the resulting classification should agree theIPAL classes.
We used a large Japanese newspapercorpus and EDR (Electronic Dictionary Research)dictionary (EDR, 1986) to obtain verbs and theirsubcategorization frames with selectional prefer-ences 1.
The results obtained using two data setswere better than the baseline, EM algorithm.The rest of the paper is organized as follows.The next section presents related work.
Afterdescribing Japanese verb with selectional pref-erences, we present a distributional similarity inSection 4, and a graph-based unsupervised clus-tering algorithm in Section 5.
Results using twodata sets are reported in Section 6.
We give ourconclusion in Section 7.1We did not use IPAL, but instead EDR sense dictionary.Because IPAL did not have senses for the case filler whichwere used to create selectional preferences.322 Related WorkGraph-based algorithms have been widely usedto classify semantically similar words (Jannink,1999; Galley, 2003; Widdows, 2002; Muller,2006).
Sinha and Mihalcea proposed a graph-based algorithm for unsupervised word sensedisambiguation which combines several seman-tic similarity measures including Resnik?s metric(Resnik, 1995), and algorithms for graph central-ity (Sinha, 2007).
They reported that the resultsusing the SENSEVAL-2 and SENSEVAL-3 En-glish all-words data sets lead to relative error ratereductions of 5 - 8% as compared to the previsouswork (Mihalcea, 2005).
More recently, Matsuoet al (2006) presented a method of word clus-tering based on Web counts using a search en-gine.
They applied Newman clustering (New-man, 2004) for identifying word clusters.
Theyreported that the results obtained by the algorithmwere better than those obtained by average-linkagglomerative clustering using 90 Japanese nounwords.
However, their method relied on hard-clustering models, and thus have largely ignoredthe issue of polysemy that word belongs to morethan one cluster.In contrast to hard-clustering algorithms, softclustering allows that words to belong to morethan one cluster.
Much of the previous work onword classification with soft clustering is basedon the EM algorithm (Pereira, 1993).
Torisawaet al, (2002) presented a method to detect asso-ciative relationships between verb phrases.
Theyused the EM algorithm to calculate the likelihoodof co-occurrences, and reported that the EM is ef-fective to produce associative relationships witha certain accuracy.
More recent work in this di-rection is that of Schulte et al, (2008).
Theyproposed a method for semantic verb classifica-tion based on verb frames with selectional prefer-ences.
They combined the EM training with theMDL principle.
The MDL principle is used toinduce WordNet-based selectional preferences forarguments within subcategorization frames.
Theresults showed the effectiveness of the method.Our work is similar to their method in the use ofverb frames with selectional preferences.
Korho-nen et al (2003) used verb?frame pairs to clus-ter verbs into Levin-style semantic classes (Ko-rhonen, 2003).
They used the Information Bottle-neck, and classified 110 test verbs into Levin-styleclasses.
They had a focus on the interpretation ofverbal polysemy as represented by the soft clus-ters: they interpreted polysemy as multiple-hardassignments.In the context of Japanese taxonomy of verbsand their classes, Utsuro et al (1995) proposed aclass-based method for sense classification of ver-bal polysemy in case frame acquisition from paral-lel corpora (Utsuro, 1995).
A measure of bilingualclass/class association is introduced and used fordiscovering sense clusters in the sense distributionof English predicates and Japanese case elementnouns.
They used the test data consisting of 10 En-glish and Japanese verbs taken from Roget?s The-saurus and BGH (Bunrui Goi Hyo) (BGH, 1989).They reported 92.8% of the discovered clusterswere correct.
Tokunaga et al (1997) presenteda method for extending an existing thesaurus byclassifying new words in terms of that thesaurus.New words are classified on the basis of relativeprobabilities of a word belonging to a given wordclass, with the probabilities calculated using noun-verb co-occurrence pairs.
Experiments using theJapanese BGH thesaurus showed that new wordscan be classified correctly with a maximum accu-racy of more than 80%, while they did not reportin detail whether the clusters captured polysemies.3 Selectional PreferencesA major approach on word clustering task is to usedistribution of a word in a corpus, i.e., words areclassified into classes based on their distributionalsimilarity.
Similarity measures based on distribu-tional hypothesis compare a pair of weighted fea-ture vectors that characterize two words (Hindle,1990; Lin, 1998; Dagan, 1999).Like previous work on verb classification, weused subcategorization frame distributions withselectional preferences to calculate similarity be-tween verbs (Schulte, 2008).
We used the EDRdictionary of selectional preferences consisting of5,269 basic Japanese verbs and the EDR conceptdictionary (EDR, 1986).
For selectional prefer-ences, the dictionary has each concept of a verb,the group of possible co-occurrence surface-levelcase particles, the types of concept relation labelthat correspond to the surface-level case as wellas the range of possible concepts that may fill thedeep-level case.
Figure 1 illustrates an example ofa verb ?taberu (eat)?.In Figure 1, ?Sentence pattern?
refers to the co-occurrence pattern between a verb and a noun33[Sentence pattern] <word1> ga <word2> wo taberu (eat)[Sense relation] agent object[Case particle] ga (nominative) wo (accusative)[Sense identifier] 30f6b0 (human);30f6bf (animal) 30f6bf(animal);30f6ca(plants);30f6e5(parts of plants);3f9639(food and drink);3f963a(feed)Figure 1: An example of a verb ?taberu (eat)?with a case marker.
?Sense relation?
expresses thedeep-level case, while ?Case particle?
shows thesurface-level case.
?Sense identifier?
refers to therange of possible concepts for the case filler.
Thesubcategorization frame pattern of a sentence (1),for example consists of two arguments with selec-tional preferences and is given below:(1) Nana ga apple wo taberu.
?Nana eats an apple.
?taberu 30f6b0 ga 3f9639 woeat human nom entity accIn the above frame pattern, x of the argument?x y?
refers to sense identifier and y denotes caseparticle.4 Distributional SimilarityVarious similarity measures have been proposedand used for NLP tasks (Korhonen, 2002).
Inthis paper, we concentrate on three distance-based,and entropy-based similarity measures.
In the fol-lowing formulae, x and y refer to the verb vec-tors, their subscripts to the verb subcategorizationframe values.1.
The Cosine measure (Cos): The cosinemeasures the similarity of the two vectors xand y by calculating the cosine of the an-gle between vectors, where each dimensionof the vector corresponds to each frame withselectional preferences patterns of verbs andeach value of the dimension is the frequencyof each pattern.2.
The Cosine measure based on probabilityof relative frequencies (rfCos): The differ-ences between the cosine and the value basedon relative frequencies of verb frames withselectional preferences are the values of eachdimension, i.e., the former are frequencies ofeach pattern and the latter are the fraction ofthe total number of verb frame patterns be-longing to the verb.3.
L1Norm (L1): The L1Norm is a mem-ber of a family of measures known as theMinkowski Distance, for measuring the dis-tance between two points in space.
The L1distance between two verbs can be written as:L1(x, y) =n?i=1| xi?
yi| .4.
Kullback-Leibler (KL): Kullback-Leibler isa measure from information theory that deter-mines the inefficiency of assuming a modelprobability distribution given the true distri-bution.KL(x, y) =n?i=1P (xi) ?
logP (xi)P (yi).where P (xi) =xi|x|.
KL is not defined incase yi= 0.
So, the probability distribu-tions must be smoothed (Korhonen, 2002).We used two smoothing methods, i.e., Add-one smoothing and Witten and Bell smooth-ing (Witten, 1991).2 Moreover, two variantsof KL, ?-skew divergence and the Jensen-Shannon, were used to perform smoothing.5.
?-skew divergence (?
div.
): The ?-skew di-vergence measure is a variant of KL, and isdefined as:?div(x, y) = KL(y, ?
?
x + (1 ?
?)
?
y).Lee (1999) reported the best results with ?
=0.9.
We used the same value.6.
The Jensen-Shannon (JS): The Jensen-Shannon is a measure that relies on the as-sumption that if x and y are similar, they areclose to their average.
It is defined as:2We report Add-one smoothing results in the evaluation,as it was better than Witten and Bell smoothing.34JS(x, y) =12[KL(x,x + y2) + KL(y,x + y2)].All measures except Cos and rfCos showed thatsmaller values indicate a closer relation betweentwo verbs.
Thus, we used inverse of each value.5 Clustering MethodThe clustering algorithm used in this study was agraph-based unsupervised clustering reported by(Zhang, 2007).
This algorithm detects overlap-ping nodes by the combination of a modularityfunction based on Newman Girvan?s Q function(Newman, 2004), spectral mapping that maps in-put nodes into Euclidean space, and fuzzy c-meansclustering which allows node to belong to morethan one cluster.
They evaluated their method byapplying several data including the American col-lege football team network, and found that the al-gorithm successfully detected overlapping nodes.We thus used the algorithm to cluster verbs.Here are the key steps of the algorithm: Givena set of input verbs V = {v1, v2, ?
?
?
vn}, an up-per bound K of the number of clusters, the adja-cent matrix A = (aij)n?nof an input verbs and athreshold ?
that can convert a soft assignment intofinal clustering, i.e., the value of ?
decreases, eachverb is distributed into larger number of clusters.We calculated the adjacent matrix A by using oneof the similarity measures mentioned in Section 4,i.e., the value of the edge between viand vj.
aijrefers to the similarity value between them.1.
Form a diagonal matrix D = (dii), where dii=?kaik.2.
Form the eigenvector matrix EK=[e1, e2, ?
?
?
, eK] by calculating the top Keigenvectors of the generalized eigensystemAx = tDx.3.
For each value of k, 2 ?
k ?
K:(a) Form the matrix Ek= [e2, ?
?
?
, ek] whereekrefers to the top k-th eigenvector.
(b) Normalize the rows of Ekto unit lengthusing Euclidean distance norm.
(c) Cluster the row vectors of Ekusingfuzzy c-means to obtain a soft assign-ment matrix Uk.
Fuzzy c-means iscarried out through an iterative opti-mization (minimization) of the objectivefunction Jmwith the update of member-ship degree uijand the cluster centerscj.
Jmis defined as:Jm=n?i=1k?j=1umij|| vi?
cj||2,where uijis the membership degree ofviin the cluster j, and?juij= 1. m ?[1,?]
is a weight exponent controllingthe degree of fuzzification.
cjis the d-dimensional center of the cluster j.|| vi?
cj|| is defined as:|| vi?
cj||2= (vi?
cj)E(vi?
cj)T.where E denotes an unit matrix.
Theprocedure converges to a saddle point ofJm.4.
Pick the k and the corresponding n ?
ksoft assignment matrix Ukthat maximizesthe modularity function ?Q(Uk).
Here Uk=[u1, ?
?
?uk] with 0 ?
uic?
1 for each c = 1,?
?
?, k, and?k1uic= 1 for each i = 1, ?
?
?, n.A modularity function of a soft assignmentmatrix is defined as:?Q(Uk) =k?c=1[A(?Vc,?Vc)A(V, V )?
(A(?Vc, V )A(V, V ))2],whereA(?Vc,?Vc) =?i??Vc,j?
?Vc{(uic+ ujc)2}aij,A(?Vc, V ) = A(?Vc,?Vc) +?i?
?Vc,j?V \?Vc{(uic+ (1 ?
ujc))2}aij,A(V, V ) =?i?V,j?Vaij.
?Q(Uk) shows comparison of the actual val-ues of internal or external edges with its re-spective expectation value under the assump-tion of equally probable links and given datasizes.356 Experiments6.1 Experimental setupWe created test verbs using two sets of JapaneseMainichi newspaper corpus.
One is a set con-sisting one year (2007) newspapers (We call it aset from 2007), and another is a set of 17 years(from 1991 to 2007) Japanese Mainichi newspa-pers (We call it a set from 1991 2007).
For eachset, all Japanese documents were parsed using thesyntactic analyzer Cabocha (Kudo, 2003).
Weselected verbs, each frequency f(v) is, 500 ?f(v) ?
10,000.
As a result, we obtained 279verbs for a set from 2007 and 1,692 verbs fora set from 1991 2007.
From these verbs, wechose verbs which appeared in the machine read-able dictionary, IPAL.
This selection resulted ina total of 81 verbs for a set from 2007, and 170verbs, for a set from 1991 2007.
We obtainedJapanese verb frames with selectional preferencesusing these two sets.
We extracted sentence pat-terns with their frequencies.
Noun words withineach sentence were tagged sense identifier by us-ing the EDR Japanese sense dictionary.
As a re-sult, we obtained 56,400 verb frame patterns for aset from 2007, and 300,993 patterns for a set from1991 2007.We created the gold standard data, verb classes,using IPAL.
IPAL lists about 900 Japanese verbsand categorizes each verb into multiple senses,based on verbal syntax and semantics.
It alsolisted synonym verbs.
Table 1 shows a fragment ofthe entry associated with the Japanese verb taberu.The verb ?taberu?
has two senses, ?eat?
and?live?.
?pattern?
refers to the case frame(s) associ-ated with each verb sense.
According to the IPAL,we obtained verb classes, each class correspondsto a sense of each verb.
There are 87 classes fora set from 2007, and 152 classes for a set from1991 2007.
The examples of the test verbs andtheir senses are shown in Table 2.For evaluation of verb classification, we usedthe precision, recall, and F-score, which were de-fined by (Schulte, 2000), especially to capturehow many verbs does the algorithm actually de-tect more than just the predominant sense.For comparison against polysemies, we utilizedthe EM algorithm which is widely used as a softclustering technique (Schulte, 2008).
We followedthe method presented in (Rooth, 1999).
We useda probability distribution over verb frames withselectional preferences.
The initial probabilitiesTable 3: Results for a set from 2007Method m ?
C Prec Rec FFCM 2.0 0.09 74 .815 .483 .606FCM(none) 1.5 0.07 74 .700 .477 .567EM ?
?
87 .308 .903 .463Table 4: Results against each measureMeasure m ?
C Prec Rec Fcos 3.0 0.02 74 .660 .517 .580rfcos 2.0 0.04 74 .701 .488 .576L12.0 0.04 74 .680 .500 .576KL 2.0 0.09 74 .815 .483 .606?
div.
2.0 0.04 74 .841 .471 .604JS 1.5 0.03 74 .804 .483 .603EM ?
?
87 .308 .903 .463were often determined randomly.
We set the ini-tial probabilities by using the result of the standardk-means.
For k-means, we used 50 random repli-cations of the initialization, each time initializingthe cluster center with k randomly chosen.
Weused up to 20 iterations to learn the model prob-abilities.6.2 Basic resultsThe results using a set from 2007 are shown inTable 3.
We used KL as a similarity measure inFCM.
?FCM(none)?
shows the result not applyinga spectral mapping, i.e., we applied fuzzy c-meansto each vector of verb, where each dimension ofthe vector corresponds to each frame with selec-tional preferences.
?m?
and ???
refer to the pa-rameters used by Fuzzy C-means.
?C?
refers tothe number of clusters obtained by each method.
?m?, ???
and ?C?
in Table 3 denote the value thatmaximized the F-score.
?C?
in the EM is fixedin advance.
The result of EM shows the best re-sult among 20 iterations.
As can be seen clearlyfrom Table 3, the result obtained by fuzzy c-meanswas better to the result by EM algorithm.
Table3 also shows that a dimensionality reduction, i.e.,spectral mapping improved overall performance,especially we have obtained better precision.
Theresult suggests that a dimensionality reduction iseffective for clustering.
Table 4 shows the resultsobtained by using each similarity measure.
As wecan see from Table 4, the overall results obtainedby information theory based measures, KL, ?
div.,and JS were slightly better to the results obtainedby other distance based measures.We note that the fuzzy c-means has two param-eters ?
and m, where ?
is a threshold of the as-36Table 1: A fragment of the entry associated with the Japanese verb ?taberu?Sense id Pattern Synonyms1 kare(he) ga(nominative) soba(noodles) wo(accusative) kuu (eat)2 kare (he) ga(nominative) fukugyo(a part-time job) de(accusative) kurasu (live)Table 2: Examples of test verbs and their polysemic gold standard sensesId Sense Verb Classes Id Sense Verb Classes1 treat {ashirau, atsukau} 11 tell {oshieru, shimesu, shiraseru}2 prey {negau, inoru} 12 persuade {oshieru, satosu}3 wish {negau, nozomu} 13 congratulate {iwau, syukufukusuru}4 ask {negau, tanomu} 14 accept {uketoru, ukeru, morau, osameru}5 leave {saru, hanareru} 15 take {uketoru, toru, kaisyakusuru, miru}6 move {saru, utsuru} 16 lose {ushinau, nakusu}7 pass {saru, kieru, sugiru} 17 miss {ushinau, torinogasu, itusuru}8 go {saru, sugiru, iku} 18 survive, lose {ushinau, nakusu, shinareru}9 remove {saru, hanareru, toozakeru 19 give {kubaru, watasu, wakeru}torinozoku}10 lead {oshieru, michibiku, tugeru} 20 arrange {kubaru, haichisuru}Figure 2: F-score against ?signment in the fuzzy c-means, and m is a weightcontrolling the degree of fuzzification.
To exam-ine how these parameters affect the overall per-formance of the algorithm, we performed exper-iments by varying these parameters.
Figure 2 il-lustrates F-score of polysemies against the valueof ?.
We used KL as a similarity measure, m = 2,and C = 74.As shown in Figure 2, the best result was ob-tained when the value of ?
was 0.09.
When ?value was larger than 0.09, the overall perfor-mance decreased, and when it exceeded 1.2, noverbs were assigned to multiple sense.
Figure 3illustrates F-score against the value of m. As il-lustrated in Figure 3, we could not find effects onaccuracy against the value of m. It is necessary toinvestigate on the influence of the parameter m byperforming further quantitative evaluation.Figure 3: F-score against m6.3 Error analysis against polysemyWe examined whether 46 polysemous verbs ina set from 2007 were correctly classified intoclasses.
We manually analyzed clustering resultsobtained by running fuzzy c-means with KL as asimilarity measure.
They were classified into threetypes of error.1.
Partially correct: Some senses of a poly-semous verb were correctly identified, butothers were not.
The first example of thispattern is that ?nigiru?
has at least twosenses, ?motsu (have)?
and ?musubu (dou-ble)?.
However, only one sense was identi-fied correctly.
The second example is that oneof the senses of the verb ?watasu?
was clas-sified correctly into the class ?ataeru (give)?,while it was classified incorrectly into theclass ?uru (sell)?.
This was the most frequenterror type.37{nigiru, motsu (have)}?
{watasu, ataeru (give)}{watasu, uru (sell)}2.
Polysemous verbs classified into only onecluster: ?hakobu?
has two senses ?carry?,and ?progress?.
However, it was classifiedinto one cluster including verbs ?motuteiku(carry)?, and ?susumu (progress)?.
Becauseit often takes the same nominative subjectssuch as ?human?
and accusative object suchas ?abstract?.
{hakobu (carry, progress),motuteiku (carry), susumu (progress)}3.
Polysemous verb incorrectly classified intoclusters: The polysemous verb ?hataraku?has two senses, ?work?, and ?operate?.
How-ever, it was classified incorrectly into ?ochiru(fall)?
and ?tsukuru (make)?.
{hataraku (work, operate), ochiru (fall),tsukuru (make)}Apart from the above error analysis, we foundthat we should improve the definition and demar-cation of semantic classes by using other exist-ing thesaurus, e.g., EDR or BGH (Bunrui GoiHyo) (BGH, 1989).
We recall that we createdthe gold standard data by using synonymous infor-mation.
However, the algorithm classified someantonymous words such as ?uketoru?
(receive) and?watasu?
(give) into one cluster.
Similarly, transi-tive and intransitive verbs are classified into thesame cluster.
For example, intransitive verb of theverb ?ochiru?
(drop) is ?otosu?.
They were clas-sified into one cluster.
It would provide furtherpotential, i.e., not only to improve the accuracyof classification, but also to reveal the relationshipbetween semantic verb classes and their syntacticbehaviors.An investigation of the resulting clusters re-vealed another interesting direction of the method.We found that some senses of a polysemous verbTable 5: Results for a set from 1991 2007Method m ?
C Prec Rec FFCM 2.0 0.24 152 .792 .477 .595FCM(none) 2.0 0.07 147 .687 .459 .550EM ?
?
152 .284 .722 .408which is not listed in the IPAL are correctly identi-fied by the algorithm.
For example, ?ukeireru?
and?yurusu?
(forgive) were correctly classified intoone cluster.
Figure 4 illustrates a sample of verbframes with selectional preferences extracted byour method.?ukeireru?
and ?yurusu?
in Table 4 have the sameframe pattern, and the sense identifiers of the casefiller ?wo?, for example, are ?a human being?
(0f0157) and ?human?
(30f6b0).
However, theseverbs are not classified into one class in the IPAL:?ukeireru?
is not listed in the IPAL as a synonymverb of ?yurusu?.
The example illustrates thatthese verbs within a cluster are semantically re-lated, and that they share obvious verb frames withintuitively plausible selectional preferences.
Thisindicates that we can extend the algorithm to solvethis resource scarcity problem: semantic classifi-cation of words which do not appear in the re-source, but appear in corpora.6.4 Results for a set of verbs from 1991 2007corpusOne goal of this work was to develop a cluster-ing methodology with respect to the automaticrecognition of Japanese verbal polysemies cover-ing large-scale corpora.
For this task, we tested aset of 170 verbs including 82 polysemies.
The re-sults are shown in Table 5.
We used KL as a simi-larity measure in FCM.
Each value of the parame-ter shows the value that maximized the F-score.As shown in Table 5, the result obtained by fuzzyc-means was as good as for the smaller set, a setof 78 verbs.
Moreover, we can see that the fuzzyc-means is better than the EM algorithm and themethod not applying a spectral mapping, as an in-crease in the F-score of 18.7% compared with theEM, and 4.5% compared with a method withoutspectral mapping.
This shows that our method iseffective for a size of the input test data consisting178 verbs.One thing should be noted is that when the al-gorithm is applied to large data, it is computation-ally expensive.
There are at least two ways to ad-dress the problem.
One is to use several methods38[Sentence pattern] <word1> ga <word2> wo ukeireru / yurusu (forgive)[Concept relation] agent object[Case particle] ga (nominative) wo (accusative)[Sense identifier] 0ee0de; 0f58b4; 0f98ee 0f0157; 30f6b00ee0de: the part of a something written that makes reference to a particular matter0f58b4: a generally-held opinion0f98ee: the people who citizens of a nation0f0157: a human being30f6b0: humanFigure 4: Extracted Verb frames of ?ukeireru?
and ?yurusu?
(forgive)of fuzzy c-means acceleration.
Kelen et al (2002)presented an efficient implementation of the fuzzyc-means algorithm, and showed that the algorithmhad the worse-case complexity of O(nK2), wheren is the number of nodes, and K is the number ofeigenvectors.
Another approach is to parallelizethe algorithm by using the Message Passing Inter-face (MPI) to estimate the optimal number of k (2?
k ?
K).
This is definitely worth trying with ourmethod.7 ConclusionWe have developed an approach for classifyingJapanese polysemous verbs using fuzzy c-meansclustering.
The results were comparable to otherunsupervised techniques.
Future work will assessby a comparison against other existing soft clus-tering algorithms such as the Clique Percolationmethod (Palla, 2005).
Moreover, it is necessaryto apply the method to other verbs for quantitativeevaluation.
New words including polysemies aregenerated daily.
We believe that classifying thesewords into semantic classes potentially enhancesmany semantic-oriented NLP applications.
It isnecessary to apply the method to other verbs, espe-cially low frequency of verbs to verify that claim.AcknowledgmentsThis work was supported by the Grant-in-aid forthe Japan Society for the Promotion of Science(JSPS).ReferencesE.
Iwabuchi.
1989.
Word List by Semantic Principles,National Language Research Institute Publications,Shuei Shuppan.I.
Dagan and L. Lee and F. C. N. Pereira.
1999.Similarity-based Models of Word CooccurrenceProbabilities.
Machine Learning, 34(1-3), pages43?69.Japan Electronic Dictionary Research Institute, Ltd.http://www2.nict.go.jp/r/r312/EDR/index.htmlM.
Galley and K. McKeown.
2003.
Improving WordSense Disambiguation in Lexical Chaining, In Proc.of 19th International Joint Conference on ArtificialIntelligence, pages 1486?1488.D.
Hindle.
1990.
Noun Classification from Predicate-Argument Structures, In Proc.
of 28th Annual Meet-ing of the Association for Computational Linguis-tics, pages 268?275.GSK2007-D. http://www.gsk.or.jp/catalog/GSK2007-D/catalog.htmlJ.
Jannink and G. Wiederhold.
1999.
Thesaurus EntryExtraction from an On-line Dictionary, In Proc.
ofFusion?99.J.
F. Kelen and T. Hutcheson.
2002.
Reducing theTime Complexity of the Fuzzy C-means Algorithm,In Trans.
of IEEE Fuzzy Systems, 10(2), pages 263?267.A.
Korhonen and Y. Krymolowski.
2002.
On theRobustness of Entropy-based Similarity Measuresin Evaluation of Subcategorization Acquisiton Sys-tems.
In Proc.
of the 6th Conference on NaturalLanguage Learning, pages 91?97.A.
Korhonen and Y. Krymolowski and Z. Marx.
2003.Clustering Polysemic Subcategorization Frame Dis-tributions Semantically.
In Proc.
of the 41st AnnualMeeting of the Association for Computational Lin-guistics, pages 64?71.T.Kudo and Y.Matsumoto.
2003.
Fast Methods forKernel-based Text Analysis.
In Proc.
of 41th ACL,pages 24?31.L.
Lee.
1999.
Measures of Distributional Similarity.In Proc.
of the 37th Annual Meeting of the Associa-tion for Computational Linguistics, pages 25?32.D.
Lin.
1998.
Automatic Retrieval and Clusteringof Similar Words, In Proc.
of 36th Annual Meet-ing of the Association for Computational Linguis-tics and 17th International Conference on Compu-tational Linguistics, pages 768?773.39Y.
Matsuo and T. Sakaki and K. Uchiyama and M.Ishizuka.
2006.
Graph-based Word Clustering us-ing a Web Search Engine, In Proc.
of 2006 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP2006), pages 542?550.R.
Mihalcea.
2005.
Unsupervised Large VocabularyWord Sense Disambiguation with Graph-based Al-gorithms for Sequence Data Labeling, In Proc.
ofthe Human Language Technology / Empirical Meth-ods in Natural Language Processing Conference,pages 411?418.P.
Muller and N. Hathout and B. Gaume.
2006.
Syn-onym Extraction Using a Semantic Distance on aDictionary, In Proc.
of the Workshop on TextGraphs,pages 65?72.M.E.J.Newman.
2004.
Fast Algorithm for DetectingCommunity Structure in Networks, Physical Re-view, E 2004, 69, 066133.G.
Palla and I. Dere?nyi and I. Farkas and T. Vic-sek.
2005.
Uncovering the Overlapping Commu-nity Structure of Complex Networks in Nature andSociety, Nature.
435(7043), 814?8.F.
Pereira and N. Tishby and L. Lee.
1993.
Distribu-tional Clustering of English Words.
In Proc.
of the31st Annual Meeting of the Association for Compu-tational Linguistics, pages 183?190.P.
Resnik.
1995.
Using Information Content to Eval-uate Semantic Similarity in a Taxonomy.
In Proc.of 14th International Joint Conference on ArtificialIntelligence, pages 448?453.M.
Rooth et al 1999.
Inducing a Semantically Anno-tated Lexicon via EM-Based Clustering, In Proc.
of37th ACL, pages 104?111.R.
Sinha and R. Mihalcea.
2007.
Unsupervised Graph-based Word Sense Disambiguation Using Measuresof Word Semantic Similarity.
In Proc.
of the IEEEInternational Conference on Semantic Computing,pages 46?54.S.
Schulte im Walde.
2000.
Clustering Verbs Seman-tically according to their Alternation Behaviour.
InProc.
of the 18th COLING, pages 747?753.S.
Schulte im Walde et al 2008.
Combining EMTraining and the MDL Principle for an AutomaticVerb Classification Incorporating Selectional Pref-erences.
In Proc.
of the 46th ACL, pages 496?504.T.
Tokunaga and A. Fujii and M. Iwayama and N. Saku-rai and H. Tanaka.
1997.
Extending a thesaurusby classifying words.
In Proc.
of the ACL-EACLWorkshop on Automatic Information Extraction andBuilding of Lexical Semantic Resources, pages 16?21.K.
Torisawa.
2002.
An Unsupervised LearningMethod for Associative Relationships between VerbPhrases, In Proc.
of 19th International Confer-ence on Computational Linguistics (COLING2002),pages 1009?1015.T.
Utsuro.
1995.
Class-based sense classification ofverbal polysemy in case frame acquisition from par-allel corpora.
In Proc.
of the 3rd Natural LanguageProcessing Pacific Rim Symposium, pages 671?677.D.
Widdows and B. Dorow.
2002.
A Graph Model forUnsupervised Lexical Acquisition.
In Proc.
of 19thInternational conference on Computational Linguis-tics (COLING2002), pages 1093?1099.I.
H. Witten and T. C. Bell.
1991.
The Zero-Frequency Problem: Estimating the Probabilities ofNovel Events in Adaptive Text Compression.
IEEETransactions on Information Theory, 37(4), pages1085?1094.S.
Zhang et al 2007.
Identification of OverlappingCommunity Structure in Complex Networks usingFuzzy C-means Clustering.
PHYSICA A, 374, pages483?490.40
