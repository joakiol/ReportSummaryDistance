Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 17?24,New York City, June 2006. c?2006 Association for Computational LinguisticsTerm Generalization and Synonym Resolution for Biological Abstracts:Using the Gene Ontology for Subcellular Localization PredictionAlona FysheDepartment of Computing ScienceUniversity of AlbertaEdmonton, Alberta T6G 2E8alona@cs.ualberta.caDuane SzafronDepartment of Computing ScienceUniversity of AlbertaEdmonton, Alberta T6G 2E8duane@cs.ualberta.caAbstractThe field of molecular biology is growingat an astounding rate and research findingsare being deposited into public databases,such as Swiss-Prot.
Many of the over200,000 protein entries in Swiss-Prot 49.1lack annotations such as subcellular lo-calization or function, but the vast major-ity have references to journal abstracts de-scribing related research.
These abstractsrepresent a huge amount of informationthat could be used to generate annotationsfor proteins automatically.
Training clas-sifiers to perform text categorization onabstracts is one way to accomplish thistask.
We present a method for improvingtext classification for biological journalabstracts by generating additional text fea-tures using the knowledge represented ina biological concept hierarchy (the GeneOntology).
The structure of the ontology,as well as the synonyms recorded in it, areleveraged by our simple technique to sig-nificantly improve the F-measure of sub-cellular localization text classifiers by asmuch as 0.078 and we achieve F-measuresas high as 0.935.1 IntroductionCan computers extract the semantic content of aca-demic journal abstracts?
This paper explores the useof natural language techniques for processing bio-logical abstracts to answer this question in a specificdomain.
Our prototype method predicts the subcel-lular localization of proteins (the part of the biolog-ical cell where a protein performs its function) byperforming text classification on related journal ab-stracts.In the last two decades, there has been explosivegrowth in molecular biology research.
Molecular bi-ologists organize their findings into a common setof databases.
One such database is Swiss-Prot, inwhich each entry corresponds to a protein.
As ofversion 49.1 (February 21, 2006) Swiss-Prot con-tains more than 200,000 proteins, 190,000 of whichlink to biological journal abstracts.
Unfortunately, amuch smaller percentage of protein entries are anno-tated with other types of information.
For example,only about half the entries have subcellular localiza-tion annotations.
This disparity is partially due tothe fact that humans annotate these databases manu-ally and cannot keep up with the influx of data.
If acomputer could be trained to produce annotations byprocessing journal abstracts, proteins in the Swiss-Prot database could be curated semi-automatically.Document classification is the process of cate-gorizing a set of text documents into one or moreof a predefined set of classes.
The classificationof biological abstracts is an interesting specializa-tion of general document classification, in that sci-entific language is often not understandable by, norwritten for, the lay-person.
It is full of specializedterms, acronyms and it often displays high levelsof synonymy.
For example, the ?PAM complex?,which exists in the mitochondrion of the biologi-cal cell is also referred to with the phrases ?pre-sequence translocase-associated import motor?
and17?mitochondrial import motor?.
This also illustratesthe fact that biological terms often span word bound-aries and so their collective meaning is lost whentext is whitespace tokenized.To overcome the challenges of scientific lan-guage, our technique employs the Gene Ontology(GO) (Ashburner et al 2000) as a source of expertknowledge.
The GO is a controlled vocabulary ofbiological terms developed and maintained by biol-ogists.
In this paper we use the knowledge repre-sented by the GO to complement the informationpresent in journal abstracts.
Specifically we showthat:?
the GO can be used as a thesaurus?
the hierarchical structure of the GO can be usedto generalize specific terms into broad concepts?
simple techniques using the GO significantlyimprove text classificationAlthough biological abstracts are challengingdocuments to classify, solving this problem willyield important benefits.
With sufficiently accuratetext classifiers, the abstracts of Swiss-Prot entriescould be used to automatically annotate correspond-ing proteins, meaning biologists could more effi-ciently identify proteins of interest.
Less time spentsifting through unannotated proteins translates intomore time spent on new science, performing impor-tant experiments and uncovering fresh knowledge.2 Related WorkSeveral different learning algorithms have been ex-plored for text classification (Dumais et al 1998)and support vector machines (SVMs) (Vapnik,1995) were found to be the most computationally ef-ficient and to have the highest precision/recall break-even point (BEP, the point where precision equalsrecall).
Joachims performed a very thorough evalu-ation of the suitability of SVMs for text classifica-tion (Joachims, 1998).
Joachims states that SVMsare perfect for textual data as it produces sparsetraining instances in very high dimensional space.Soon after Joachims?
survey, researchers startedusing SVMs to classify biological journal abstracts.Stapley et al (2002) used SVMs to predict the sub-cellular localization of yeast proteins.
They createda data set by mining Medline for abstracts contain-ing a yeast gene name, which achieved F-measuresin the range [0.31,0.80].
F-measure is defined asf =2rpr + pwhere p is precision and r is recall.
They expandedtheir training data to include extra biological infor-mation about each protein, in the form of amino acidcontent, and raised their F-measure by as much as0.05.
These results are modest, but before Stapleyet al most localization classification systems werebuilt using text rules or were sequence based.
Thiswas one of the first applications of SVMs to bio-logical journal abstracts and it showed that text andamino acid composition together yield better resultsthan either alone.Properties of proteins themselves were again usedto improve text categorization for animal, plant andfungi subcellular localization data sets (Ho?glundet al 2006).
The authors?
text classifiers werebased on the most distinguishing terms of docu-ments, and they included the output of four pro-tein sequence classifiers in their training data.
Theymeasure the performance of their classifier usingwhat they call sensitivity and specificity, thoughthe formulas cited are the standard definitions ofrecall and precision.
Their text-only classifier forthe animal MultiLoc data set had recall (sensitivity)in the range [0.51,0.93] and specificity (precision)[0.32,0.91].
The MultiLocText classifiers, whichinclude sequence-based classifications, have recall[0.82,0.93] and precision [0.55,0.95].
Their overalland average accuracy increased by 16.2% and 9.0%to 86.4% and 94.5% respectively on the PLOC an-imal data set when text was augmented with addi-tional sequence-based information.Our method is motivated by the improvementsthat Stapley et al and Ho?glund et al saw when theyincluded additional biological information.
How-ever, our technique uses knowledge of a textual na-ture to improve text classification; it uses no infor-mation from the amino acid sequence.
Thus, our ap-proach can be used in conjunction with techniquesthat use properties of the protein sequence.In non-biological domains, external knowledgehas already been used to improve text categoriza-tion (Gabrilovich and Markovitch, 2005).
In their18research, text categorization is applied to news docu-ments, newsgroup archives and movie reviews.
Theauthors use the Open Directory Project (ODP) as asource of world knowledge to help alleviate prob-lems of polysemy and synonymy.
The ODP is ahierarchy of concepts where each concept node haslinks to related web pages.
The authors mined theseweb pages to collect characteristic words for eachconcept.
Then a new document was mapped, basedon document similarity, to the closest matching ODPconcept and features were generated from that con-cept?s meaningful words.
The generated features,along with the original document, were fed into anSVM text classifier.
This technique yielded BEP ashigh as 0.695 and improvements of up to 0.254.We use Gabrilovich and Markovitch?s (2005) ideato employ an external knowledge hierarchy, in ourcase the GO, as a source of information.
It hasbeen shown that GO molecular function annotationsin Swiss-Prot are indicative of subcellular localiza-tion annotations (Lu and Hunter, 2005), and that GOnode names made up about 6% of a sample Medlinecorpus (Verspoor et al 2003).
Some consider GOterms to be too rare to be of use (Rice et al 2005),however we will show that although the presence ofGO terms is slight, the terms are powerful enough toimprove text classification.
Our technique?s successmay be due to the fact that we include the synonymsof GO node names, which increases the number ofGO terms found in the documents.We use the GO hierarchy in a different way thanGabrilovich et al use the ODP.
Unlike their ap-proach, we do not extract additional features from allarticles associated with a node of the GO hierarchy.Instead we use synonyms of nodes and the namesof ancestor nodes.
This is a simpler approach, asit doesn?t require retrieving all abstracts for all pro-teins of a GO node.
Nonetheless, we will show thatour approach is still effective.3 MethodsThe workflow used to perform our experiments isoutlined in Figure 1.3.1 The Data SetThe first step in evaluating the usefulness of GO asa knowledge source is to create a data set.
This pro-Set ofProteinsRetrieveAbstractsSet ofAbstractsProcessAbstractsData Set 1 Data Set 2 Data Set 3abFigure 1: The workflow used to create data sets usedin this paper.
Abstracts are gathered for proteinswith known localization (process a).
Treatments areapplied to abstracts to create three Data Sets (pro-cess b).cess begins with a set of proteins with known sub-cellular localization annotations (Figure 1).
For thiswe use Proteome Analyst?s (PA) data sets (Lu et al2004; Szafron et al 2004).
The PA group used thesedata sets to create very accurate subcellular classi-fiers based on the keyword fields of Swiss-Prot en-tries for homologous proteins.
Here we use PA?scurrent data set of proteins collected from Swiss-Prot (version 48.3) and impose one further crite-rion: the subcellular localization annotation may notbe longer than four words.
This constraint is in-troduced to avoid including proteins where the lo-calization category was incorrectly extracted from along sentence describing several aspects of localiza-tion.
For example, consider the subcellular anno-tation ?attached to the plasma membrane by a lipidanchor?, which could mean the protein?s functionalcomponents are either cytoplasmic or extracellular(depending on which side of the plasma membranethe protein is anchored).
PA?s simple parsing schemecould mistake this description as meaning that theprotein performs its function in the plasma mem-brane.
Our length constraint reduces the chances ofincluding mislabeled training instances in our data.19Class Number of NumberName Proteins of Abstractscytoplasm 1664 4078endoplasmicreticulum 310 666extracellular 2704 5655golgi a 41 71lysosome 129 599mitochondrion 559 1228nucleus 2445 5589peroxisome 108 221plasmamembrane a 15 38Total 7652 17175aClasses with less than 100 abstracts were considered tohave too little training data and are not included in our experi-ments.Table 1: Summary of our Data Set.
Totals are lessthan the sum of the rows because proteins may be-long to more than one localization class.PA has data sets for five organisms (animal, plant,fungi, gram negative bacteria and gram positive bac-teria).
The animal data set was chosen for our studybecause it is PA?s largest and medical research hasthe most to gain from increased annotations for an-imal proteins.
PA?s data sets have binary labeling,and each class has its own training file.
For exam-ple, in the nuclear data set a nuclear protein appearswith the label ?+1?, and non-nuclear proteins ap-pear with the label ??1?.
Our training data includes317 proteins that localize to more than one location,so they will appear with a positive label in more thanone data set.
For example, a protein that is both cyto-plasmic and peroxisomal will appear with the label?+1?
in both the peroxisomal and cytoplasmic sets,and with the label ??1?
in all other sets.
Our dataset has 7652 proteins across 9 classes (Table 1).
Totake advantage of the information in the abstracts ofproteins with multiple localizations, we use a one-against-all classification model, rather than a ?singlemost confident class?
approach.3.2 Retrieve AbstractsNow that a set of proteins with known localiza-tions has been created, we gather each protein?sabstracts and abstract titles (Figure 1, process a).We do not include full text because it can be dif-ficult to obtain automatically and because usingfull text does not improve F-measure (Sinclair andWebber, 2004).
Abstracts for each protein are re-trieved using the PubMed IDs recorded in the Swiss-Prot database.
PubMed (http://www.pubmed.gov) is a database of life science articles.
It shouldbe noted that more than one protein in Swiss-Protmay point to the same abstract in PubMed.
Becausethe performance of our classifiers is estimated us-ing cross-validation (discussed in Section 3.4) it isimportant that the same abstract does not appear inboth testing and training sets during any stage ofcross-validation.
To address this problem, all ab-stracts that appear more than once in the completeset of abstracts are removed.
The distribution of theremaining abstracts among the 9 subcellular local-ization classes is shown in Table 1.
For simplicity,the fact that an abstract may actually be discussingmore than one protein is ignored.
However, becausewe remove duplicate abstracts, many abstracts dis-cussing more than one protein are eliminated.In Table 1 there are more abstracts than proteinsbecause each protein may have more than one asso-ciated abstract.
Classes with less than 100 abstractswere deemed to have too little information for train-ing.
This constraint eliminated plasma membraneand golgi classes, although they remained as nega-tive data for the other 7 training sets.It is likely that not every abstract associated witha protein will discuss subcellular localization.
How-ever, because the Swiss-Prot entries for proteins inour data set have subcellular annotations, some re-search must have been performed to ascertain local-ization.
Thus it should be reported in at least oneabstract.
If the topics of the other abstracts are trulyunrelated to localization than their distribution ofwords may be the same for all localization classes.However, even if an abstract does not discuss local-ization directly, it may discuss some other propertythat is correlated with localization (e.g.
function).In this case, terms that differentiate between local-ization classes will be found by the classifier.3.3 Processing AbstractsThree different data sets are made by processing ourretrieved abstracts (Figure 1, process b).
An ex-20We studied theeffect of p123 onthe regulation ofosmotic pressure."studi?:1,?effect?:1,?p123?:1,?regul?:1,"osmot?:1,"pressur?:1"studi?:1,?effect?:1,?p123?:1,?regul?:1,"osmot?:1,"pressur?:1,"osmoregulation":1"studi?:1,?effect?:1,?p123?:1,?regul?:1,"osmot?:1,"pressur?
:1,"osmoregulation":1,"GO_homeostasis":1,"GO_physiologicalprocess":1,"GO_biological process":1Dataset1Dataset2Dataset3Figure 2: A sentence illustrating our three meth-ods of abstract processing.
Data Set 1 is our base-line, Data Set 2 incorporates synonym resolutionand Data Set 3 incorporates synonym resolution andterm generalization.
Word counts are shown here forsimplicity, though our experiments use TFIDF.ample illustrating our three processing techniques isshown in Figure 2.In Data Set 1, abstracts are tokenized and eachword is stemmed using Porter?s stemming algo-rithm (Porter, 1980).
The words are then trans-formed into a vector of <word,TFIDF> pairs.TFIDF is defined as:TFIDF (wi) = f(wi) ?
log(nD(wi))where f(wi) is the number of times word wi ap-pears in documents associated with a protein, n isthe total number of training documents and D(wi)is the number of documents in the whole trainingset that contain the word wi.
TFIDF was first pro-posed by Salton and Buckley (1998) and has beenused extensively in various forms for text catego-rization (Joachims, 1998; Stapley et al 2002).
Thewords from all abstracts for a single protein areamalgamated into one ?bag of words?
that becomesthe training instance which represents the protein.3.3.1 Synonym ResolutionThe GO hierarchy can act as a thesaurus forwords with synonyms.
For example the GO encodesthe fact that ?metabolic process?
is a synonym for?metabolism?
(see Figure 3).
Data Set 2 uses GO?s?exact synonym?
field for synonym resolution andadds extra features to the vector of words from DataSet 1.
We search a stemmed version of the abstractsregulation ofosmotic pressurebiologicalprocessphysiologicalprocesshomeostasis metabolismgrowththermo-regulationosmo-regulationmetabolicprocessFigure 3: A subgraph of the GO biological processhierarchy.
GO nodes are shown as ovals, synonymsappear as grey rectangles.for matches to stemmed GO node names or syn-onyms.
If a match is found, the GO node name(deemed the canonical representative for its set ofsynonyms) is associated with the abstract.
In Fig-ure 2 the phrase ?regulation of osmotic pressure?appears in the text.
A lookup in the GO synonymdictionary will indicate that this is an exact synonymof the GO node ?osmoregulation?.
Therefore we as-sociated the term ?osmoregulation?
with the traininginstance.
This approach combines the weight of sev-eral synonyms into one representative, allowing theSVM to more accurately model the author?s intent,and identifies multi-word phrases that are otherwiselost during tokenization.
Table 2 shows the increasein average number of features per training instanceas a result of our synonym resolution technique.3.3.2 Term GeneralizationIn order to express the relationships betweenterms, the GO hierarchy is organized in a directedacyclic graph (DAG).
For example, ?thermoregula-tion?
is a type of ?homeostasis?, which is a ?phys-iological process?.
This ?is a?
relationship is ex-pressed as a series of parent-child relationships (seeFigure 3).
In Data Set 3 we use the GO for synonymresolution (as in Data Set 2) and we also use its hi-erarchical structure to generalize specific terms intobroader concepts.
For Data Set 3, if a GO node name(or synonym) is found in an abstract, all names ofancestors to the match in the text are included in the21Class Data Data DataSet 1 Set 2 Set 3cytoplasm 166 177 203endoplasmicreticulum 162 171 192extracellular 148 155 171lysosome 244 255 285mitochondrion 155 163 186nucleus 147 158 183peroxisome 147 156 182Overall Average 167 176 200Table 2: Average number of features per traininginstance for 7 subcellular localization categories inanimals.
Data Set 1 is the baseline, Data Set 2 in-corporates synonym resolution and Data Set 3 usessynonym resolution and term generalization.training instance along with word vectors from DataSet 2 (see Figure 2).
These additional node namesare prepended with the string ?GO ?
which allowsthe SVM to differentiate between the case where aGO node name appears exactly in text and the casewhere a GO node name?s child appeared in the textand the ancestor was added by generalization.
Termgeneralization increases the average number of fea-tures per training instance (Table 2).Term generalization gives the SVM algorithm theopportunity to learn correlations that exist betweengeneral terms and subcellular localization even ifthe general term never appears in an abstract andwe encounter only its more specific children.
With-out term generalization the SVM has no concept ofthe relationship between child and parent terms, norbetween sibling terms.
For some localization cate-gories more general terms may be the most informa-tive and in other cases specific terms may be best.Because our technique adds features to training in-stances and never removes any, the SVM can as-sign lower weights to the generalized terms in caseswhere the localization category demands it.3.4 EvaluationEach of our classifiers was evaluated using 10 foldcross-validation.
In 10 fold cross-validation eachData Set is split into 10 stratified partitions.
For thefirst ?fold?, a classifier is trained on 9 of the 10 par-titions and the tenth partition is used to test the clas-sifier.
This is repeated for nine more folds, holdingout a different tenth each time.
The results of all10 folds are combined and composite precision, re-call and F-measures are computed.
Cross-validationaccurately estimates prediction statistics of a classi-fier, since each instance is used as a test case at somepoint during validation.The SVM implementation libSVM (Chang andLin, 2001) was used to conduct our experiments.
Alinear kernel and default parameters were used in allcases; no parameter searching was done.
Precision,recall and F-measure were calculated for each ex-periment.4 Results and DiscussionResults of 10 fold cross-validation are reported inTable 3.
Data Set 1 represents the baseline, whileData Sets 2 and 3 represent synonym resolution andcombined synonym resolution/term generalizationrespectively.
Paired t-tests (p=0.05) were done be-tween the baseline, synonym resolution and termgeneralization Data Sets, where each sample is onefold of cross-validation.
Those classifiers with sig-nificantly better performance over the baseline ap-pear in bold in Table 3.
For example, the lysosomeclassifiers trained on Data Set 2 and 3 are both sig-nificantly better than the baseline, and results forData Set 3 are significantly better than results forData Set 2, signified with an asterisk.
In the caseof the nucleus classifier no abstract processing tech-nique was significantly better, so no column appearsin bold.In six of the seven classes, classifiers trained onData Set 2 are significantly better than the base-line, and in no case are they worse.
In Data Set3, five of the seven classifiers are significantly bet-ter than the baseline, and in no case are they worse.For the lysosome and peroxisome classes our com-bined synonym resolution/term generalization tech-nique produced results that are significantly betterthan synonym resolution alone.
The average resultsof Data Set 2 are significantly better than Data Set1 and the average results of Data Set 3 are signifi-cantly better than Data Set 2 and Data Set 1.
On av-erage, synonym resolution and term generalizationcombined give an improvement of 3%, and synonym22ClassData Set 1 Data Set 2 Data Set 3Baseline Synonym Resolution Term GeneralizationF-measure F-Measure ?
F-Measure ?cytoplasm 0.740 (?0.049) 0.758 (?0.042) +0.017 0.761 (?0.042) +0.021endoplasmicreticulum 0.760 (?0.055) 0.779 (?0.068) +0.019 0.786 (?0.072) +0.026extracellular 0.931 (?0.009) 0.935 (?0.009) +0.004 0.935 (?0.010) +0.004lysosome 0.746 (?0.107) 0.787 (?
0.100) +0.041 0.820* (?0.089) +0.074mitochondrion 0.840 (?0.041) 0.848 (?0.038) +0.008 0.852 (?0.039) +0.012nucleus 0.885 (?0.014) 0.885 (?
0.016) +0.001 0.887 (?0.019) +0.003peroxisome 0.790 (?0.054) 0.823 (?0.042) +0.033 0.868* (?0.046) +0.078Average 0.815 (?0.016) 0.832 (?0.012) +0.017 0.845* (?0.009) +0.030Table 3: F-measures for stratified 10 fold cross-validation on our three Data Sets.
Results deemed signifi-cantly improved over the baseline (p=0.05) appear in bold, and those with an asterisk (*) are significantlybetter than both other data sets.
Change in F-measure compared to baseline is shown for Data Sets 2 and 3.Standard deviation is shown in parentheses.resolution alone yields a 1.7% improvement.
Be-cause term generalization and synonym resolutionnever produce classifiers that are worse than syn-onym resolution alone, and in some cases the resultis 7.8% better than the baseline, Data Set 3 can beconfidently used for text categorization of all sevenanimal subcellular localization classes.Our baseline SVM classifier performs quite wellcompared to the baselines reported in relatedwork.
At worst, our baseline classifier has F-measure 0.740.
The text only classifier reportedby Ho?glund et al has F-measure in the range[0.449,0.851] (Ho?glund et al 2006) and the textonly classifiers presented by Stapley et al begin witha baseline classifier with F-measure in the range[0.31,0.80] (Stapley et al 2002).
Although theirapproaches gave a greater increase in performancetheir low baselines left more room for improvement.Though we use different data sets than Ho?glundet al (2006), we compare our results to theirs on aclass by class basis.
For those 7 localization classesfor which we both make predictions, the F-measureof our classifiers trained on Data Set 3 exceed the F-measures of the Ho?glund et al text only classifiersin all cases, and our Data Set 3 classifier beats the F-measure of the MutliLocText classifier for 5 classes(see supplementary material http://www.cs.ualberta.ca/?alona/bioNLP).
In addition,our technique does not preclude using techniquespresented by Ho?glund et al and Stapley et al, andit may be that using a combination of our approachand techniques involving protein sequence informa-tion may result in an even stronger subcellular local-ization predictor.We do not assert that using abstract text alone isthe best way to predict subcellular localization, onlythat if text is used, one must extract as much fromit as possible.
We are currently working on incorpo-rating the classifications given by our text classifiersinto Proteome Analyst?s subcellular classifier to im-prove upon its already strong predictors (Lu et al2004), as they do not currently use any informationpresent in the abstracts of homologous proteins.5 Conclusion and Future workOur study has shown that using an external informa-tion source is beneficial when processing abstractsfrom biological journals.
The GO can be used as areference for both synonym resolution and term gen-eralization for document classification and doing sosignificantly increases the F-measure of most sub-cellular localization classifiers for animal proteins.On average, our improvements are modest, but theyindicate that further exploration of this technique iswarranted.We are currently repeating our experiments forPA?s other subcellular data sets and for function pre-diction.
Though our previous work with PA is not23text based, our experience training protein classifiershas led us to believe that a technique that works wellfor one protein property often succeeds for othersas well.
For example our general function classifierhas F-measure within one percent of the F-measureof our Animal subcellular classifier.
Although wetest the technique presented here on subcellular lo-calization only, we see no reason why it could not beused to predict any protein property (general func-tion, tissue specificity, relation to disease, etc.).
Fi-nally, although our results apply to text classificationfor molecular biology, the principle of using an on-tology that encodes synonyms and hierarchical re-lationships may be applicable to other applicationswith domain specific terminology.The Data Sets used in these experiments areavailable at http://www.cs.ualberta.ca/?alona/bioNLP/.6 AcknowledgmentsWe would like to thank Greg Kondrak, Colin Cherry,Shane Bergsma and the whole NLP group at theUniversity of Alberta for their helpful feedback andguidance.
We also wish to thank Paul Lu, Rus-sell Greiner, Kurt McMillan and the rest of theProteome Analyst team.
This research was madepossible by financial support from the Natural Sci-ences and Engineering Research Council of Canada(NSERC), the Informatics Circle of Research Excel-lence (iCORE) and the Alberta Ingenuity Centre forMachine Learning (AICML).ReferencesMichael Ashburner et al 2000.
Gene ontology: tool forthe unification of biology the gene ontology consor-tium.
Nature Genetics, 25(1):25?29.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIB-SVM: a library for support vector machines.
Soft-ware available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Susan T. Dumais et al 1998.
Inductive learning al-gorithms and representations for text categorization.In Proc.
7th International Conference on Informationand Knowledge Management CIKM, pages 148?155.Evgeniy Gabrilovich and Shaul Markovitch.
2005.
Fea-ture generation for text categorization using worldknowledge.
In IJCAI-05, Proceedings of the Nine-teenth International Joint Conference on Artificial In-telligence, pages 1048?1053.Annette Ho?glund et al 2006.
Significantly improvedprediction of subcellular localization by integratingtext and protein sequence data.
In Pacific Symposiumon Biocomputing, pages 16?27.Thorsten Joachims.
1998.
Text categorization with su-port vector machines: Learning with many relevantfeatures.
In ECML ?98: Proceedings of the 10th Eu-ropean Conference on Machine Learning, pages 137?142.Zhiyong Lu and Lawrence Hunter.
2005.
GO molecularfunction terms are predictive of subcellular localiza-tion.
volume 10, pages 151?161.Zhiyong Lu et al 2004.
Predicting subcellular local-ization of proteins using machine-learned classifiers.Bioinformatics, 20(4):547?556.Martin F. Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137.Simon B Rice et al 2005.
Mining protein function fromtext using term-based support vector machines.
BMCBioinformatics, 6:S22.Gail Sinclair and Bonnie Webber.
2004.
Classificationfrom full text: A comparison of canonical sectionsof scientific papers.
In COLING 2004 InternationalJoint workshop on Natural Language Processing inBiomedicine and its Applications (NLPBA/BioNLP)2004, pages 69?72.B.
J. Stapley et al 2002.
Predicting the sub-cellular lo-cation of proteins from text using support vector ma-chines.
In Pacific Symposium on Biocomputing, pages374?385.Duane Szafron et al 2004.
Proteome analyst: Custompredictions with explanations in a web-based tool forhigh-throughput proteome annotations.
Nucleic AcidsResearch, 32:W365?W371.Vladimir N Vapnik.
1995.
The nature of statisticallearning theory.
Springer-Verlag New York, Inc., NewYork, NY, USA.Cornelia M. Verspoor et al 2003.
The gene ontology as asource of lexical semantic knowledge for a biologicalnatural language processing application.
Proceedingsof the SIGIR?03 Workshop on Text Analysis and Searchfor Bioinformatics.24
