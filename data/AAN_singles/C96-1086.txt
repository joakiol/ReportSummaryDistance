Inherited Feature-based Similarity Measure Based on LargeSemantic Hierarchy and Large Text CorpusHideki HirakawaTosh iba  I{.&D Center1 Komuka i  Tosh iba-cho ,  Sa iwai -ku,Kawasak i  210, JAPANh i rakawa?ee: l .
,  rdc .
tosh iba ,  co .
j pZhonghui Xu, Kenneth HaaseMIT  Med ia  Laboratory20 Ames St reetCambr idge ,  MA 02139 USA{xu, haase} @media.
mit.
eduAbstractWe describe a similarity calculationmodel called IFSM (Inherited FeatureSimilarity Measure) between objects(words/concepts) based on their com-mon and distinctive features.
We pro-pose an implementation method for ob-taining features based on abstractedtriples extracted fi'om a large text eorpusutilizing taxonomical knowledge.
Thismodel represents an integration of tradi-tional methods, i.e,.
relation b~used sin>itarity measure and distribution basedsimilarity measure.
An experiment, us-ing our new concept abstraction methodwhich we <'all the fiat probability group-ing method, over 80,000 surface triples,shows that the abstraction level of 3000is a good basis for feature description.
'1 IntroductionDetermination of semantic similarity betweenwords is an important component of linguis-tic tasks ranging from text retrieval and filter-ing, word sense disambiguation or text match-ing.
In the past five years, this work has evolvedin conjunction with the availability of powerfulcomputers and large linguistic resources uch asWordNet (Miller,90), the EDR concept dictionary(EDR,93), and large text corpora.Similarity methods can be broadly divided into"relation based" methods which use relations inan ontology to determine similarity and "distribu-tion based" methods which use statistical analysisas the basis of similarity judgements.
This articledescribes a new method of similarity nmtehing, in-herited feature based similarity matching (IFSM)which integrates these two approaches.Relation based methods include both depthbased and path based measures of similarity.The Most Specific Common Abstraction (MSCA)method compares two concepts based on the tax-onomic depth of their common parent; for exam-ple, "dolphin" and "human" are more similar than"oak" and "human" because the common concept"mammal" is deeper in the taxonomy than "livingthing".Path-length similarity methods are based oncounting the links between nodes in a semanticnetwork.
(Rada,89) is a widely adopted approachto such matching and (Sussna,93) combines itwith WordNet to do semantic disambiguation.The chief problems with relation-b~sed similar-ity methods lie in their sensitivity to artifacts inthe coding of the ontology, l;or instance, MSCAalgorithms are sensitive to the relative deplh anddetail of different parts of the concept taxon-omy.
If one conceptual domain (say plants) issketchily represented while another conceptual do-main (say,, animals) is richly represented, similar-ity comparisons within the two domains will be in-commensurable.
A similar problem plagues path-length based algorithms, causing nodes in richlystructured parts of the ontology to be consistentlyjudged less similm" to one another than nodes inshallower or hess complete parts of the ontology.Distribution-based methods are based on theidea that the similarity of words can be derivedfrorn the similarity of the contexts in which theyoccur.
These methods difl'er most significantlyin the way they characterize contexts and thesimilarity of contexts.
Word Space (Schutze,93)uses letter 4-grams to characterize both words andthe contexts in which they appear.
Similarity isbased on 4-grams in common between the con-texts.
Church and tlanks ('89) uses a word win-dow of set size to characterize the context of aword based on the immediately adjacent words.Other methods include the use of expensive-to-derive features uch as subject-verb-object (SVO)relations (Hindle,90) or other grammatical rela-tions (Grefenstette,94).
These choices are not sim-ply iml)lemelltational but imply ditferent similar-ity judgements.
The chief problem with distribu-tion based methods is that they only permit theformation of first-order concepts definable directlyin terms of the original text.
Distribution basedmethods can acquire concepts b~sed on recurringpatterns of words but not on recurring patternsof concepts.
\[,'or instance, a distributional sys-tem could easily identify that an article involveslawyers based on recurring instances of words like"sue" or "court".
But it could not use the oc~currence of these concepts as conceptual cues for508<lewfloping coneel)ts like "lit igadon" or "l)\]eading"in connection with the "lawyer" eoncel)t.One.
notable integration of relation t)ased anddistri lmtional methods is l lesnik's annotation ofa relational ontology wil h distributional in fornla-lion (l{esnik,95a,95b).
\]lesnik inLroduees a "classprobabil ity" associated with nodes (synset.s) inWoMNet and uses these to determiue similarity.Given these probabilities, he eOltlptttes tile simi-larit.y of concepts I+)ased on the "inl'on nation" thatwou\](l be necessary to distinguish them, tneasuredttsing iMbrmalion-theoretie calculations+The Feature-based SimilarityMeasureThe Inherited Feature Similarity Measure (IFSM)is another integrated approach to measuring simi-la.rity.
It uses a semantic knowledge base whereconcepts are annotated wit\]\] disli<qlli.sbiW\] fi'a-ltu'es and i)ases similarity on (:otnl>aril~.g these selsof feal;ures.
In our exl)erime\]tts, we deriw>d thefeature sets I) 3, a distJ'ilmtiona\] analysis of +t larget :Ol: l) tiN.Most existing relation-hase(l similarity methodsdirectly use l,he relat:iotl ~Ol)O/ogy of the seman-tic network to derive similarity, either by strate-gies like link counting (f~a(la,89) or tim determina-tion of the depth <)f <:otnmon al)slra<:lions (Kolod:net,g9).
\[FSM, in eontrasl., uses the I:Ol)O\]Ogy toderive (leseril)lions whose (:omparisotl yields a sim-ilarity measure.
In l)arti(:ular, it aSSlllnes art Otl-I:o\[ogy where:I.
Each (:once\])l; has a set of features2.
Each concept inherits Features from its get>erMizations (hypernyms)3.
\]!
;;u:h concept has one or more "(listinctiw~features" which are not inherite(l ft:om its hy-\])el:nylllS.Note that we neidter claim nor require t:hat thefeatures eonq>letely charaelerize their (:(mcepts orlhat inh<'.ritan<:e of feal m:es is sound.
We only re-quire dlat there I)e some set of feal;ul:es we use forsimilarity judgcmettts.
For instance, a similarity.iudgenle31t betwe(+m a penguin and a rot)in will t)epartial ly based on the fe++ture "ean-\[ly" assignedto the concel)t bird, ewm though it (toes not apl)lyit~dividually to t)et\]guins.Fig I shows a Siml)le exatnple o\[' a fragment ofa (:oncel~ttud taxonomy wiLl~ associated featttres.Inherited features are in italh: while disliuctivellalcnl(< h~vu-ch i id :>)falhcl(< male >< \]lave child >) iflothel(< female >< hove-chihl >1Fig.
1 Fragment ot' c(mccptual taxonomy\[Salutes are in bold.
In our model, features havea weight l)ased otl the importance o1' the featureto the eolleel)t.We \[laV(~ chosel\] to alltOlIla, tieally gel\]erate {'ea-tures (listril)utionally by analyzing a large eOrl)US.We (leseribe lids geueration process below, but wewill \[irst ttlrtl to the e\qthlgti()tl of similarity basedon feat ural analysis.2.1 At) i ) roaehes to Featm'e Mate l f ing'l'here are a variety of similarity measures awu\]-able for sets of \[;~atm'es, biLL all make their eom-l)arisons t)ase(l on some combination of shared\['etltlH;es, disLilleL \['eal ttres, altd sharect ttl)sellL l'ea-.tures (e.g., neither X or Y is red).
For example,Tversky ('77) proposes a mode\] (based on huntansimilarity judgements) where similarity is a linearcombination of shared and distinct features whereeach f('atm'e is weighted 1)ased on its itnl)ortatme+'l'w>rsky's experiment showed the highesl eorrela-lion with hunmn subjects ~ feelings when weightedshared and dislinet features are taken into consi(l-eration.HI~X'I'ANT ((~reii:nstette,94) introduce(1 the\Veighted 3aeeard Measure which combitms theJaeeard Measure with weights derive(l froth aninh)rmation theoreti<: anMysis of %ature occur-fences+ '\]'he we:ight of a feature is com\[mte(l froma global weight (based on the nmuber of glohaloccurrences of the, wor(l or concept) and a \[()(:atweight (based Oil the \['re(lllellcy Of tlt+> Features at-laehed to the word).\]n our (:urrent work.
we have adol)te(t theWeighted .laeeard Measure for prelimit,ary ewJ-tmti(m of otu" al)lJroaeh.
'l'he clistinetiw" featureof our apl):roach is the rise of the ontology I.o (|e+rive features rather than assuming atomic sets ofRmtures.2.2 P roper t ies  o f  I FSM/u this section we compare IFSM's similarityjudgements to those generated by other tneth-()<Is.
In our diseltssiou, we will consider the sim-ple netwoH?
o~' Fig 2.
We will use 1he expressionsim.
(ci, cj' ) to denote the similarity of eoncel)ts (harid e2.Given lhe situation of Fig 2, both MS(L,\an(t tlesnik's M ISM (Most In formative Sul>stmtorMethod) asse,'t .s ' im(Ct,C2) = sirn(C2, C3).MSCA makes the sitnilarit.y the satile because theyhave the sante (nearest) eotmnon abstraction CO.MISM holds the similarity Io be the same 1)eeause( :ll( ' l  ( :2~"( "3I"ig.2 I"xanal)le of  a h ie rardt i ca l  strttctul 'e509H i ~ h ~F ig .3  l son lo l -ph ic  suhst rac turesin  h igher / lower  leve ls  o f  h ie rarchythe assertion of C2 adds no information given theassertion of C3.
Path-length methods, in contrast,assert sire(C1, C2) < sire(C2, C3) since the num-ber of links between the concepts is quite different.Because IFSM depends on the features derivedfrom the network rather than on the network it-self, judgements of similarity depend on the ex-act features assigned to C1, C2, and C3.
BecauseIFSM assumes that some distinctive features ex-ist for C3, sire(el, 62) and sire(el, C3) are un-likely to be identical.
In fact, unless the distinc-tive features of C3 significantly overlap the dis-tinctive feature of C1, it will be the case thatsi,~(C1, C2) < si,~(C2, C3).IFSM differs from the path length model be-cause it is sensitive to depth.
If we assume a rel-atively uniform distribution of features, the totalnumber of features increases with depth in the hi-erarchy.
This means that sim(C0,C1) located inhigher part of the hierarchy is expected to be lessthan sim(C2,C3) located in lower part of the hi-erarchy.3 Components  o f  I FSM mode lIFSM consists of a hierarchical conceptual the-saurus, a set of distinctive features assigned toeach object and weightings of the features.
Wecan use, for example, WordNet or the EDR con-cept dictionary as a hierarchical conceptual the-saurus.
Currently, there are no explicit methodsto determine sets of distinctive features and theirweightings of each object (word or concept).Here we adopt an automatic extraction of fea-tures and their weightings from a large text cor-pus.
This is the same approach as that of the dis-tribdted semantic models.
However, in contrastto those models, here we hope to make the levelof the representation of features high enough tocapture semantic behaviors of objects.For example, if one relation and one object canbe said to describe the features of object, we candefine one feature Of "human" as "agent of walk-ing".
If more context is allowed, we can definea feature of "human" as "agent of utilizing fire".A wider context gives a precision to the contentsof the features.
However, a wider context expo-nentially increases the possible number of featureswhich will exceed current limitations of computa-tional resources.
In consideration of these factors,we adopts triple relations uch as "dog chase cat","cut paper with scissors" obtained from the cot-"k dog chases a cat" "k hound chases a cat" "A dog chases a kitty"("chase" "dog" "cat") (*'chase" "hound" "cat") ("chase" "dog" "kitty").... .............. o' o .............. o / ....... o ...........' "1 I / / | .
(so ~ ~ ,,.0.J ~0, lo/ / -<so  ~,~ h,~.~ ki,,.51.... ..................... i,,,,,il .............. i ............... W ............ i .......... O-iso ~,,,,~c Jo~ ?,~, -, ) 0 0 (9  0 i.- .e%ed .
.r,ooe Tri,l s ............ N .............. N .............. .......i O O O?
O O .O O O O O OL.
Deep Triples .
~ ~  .. \[ .... ~ ../ / / /(SO v228 n5 n9 ... 5.3 32 ("dll, Se" nm after")("dog" "hound"~{"cat" ki ty")):~0 0 O" 0 0 0 0 0i , , ,  I I  1 1 ?
Abstracted Tr ip les  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
I I II F i l te r ing  Heur i s t i cs  I0 0,,, Filtered Abstracted Triples .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Fig.4 Abstracted triple extraction from corpuspus as a resource of features, and apply class basedabstraction (Resnik 95a) to triples to reduce thesize of the possible feature space.As mentioned above, features extracted fi'omthecorpus will be represented using synsets/conceptsin IFSM.
Since no large scale corpus data withsemantic tags is available, the current implemen-tation of IFSM has a word sense disambiguationproblem in obtaining class probabilities.
Our cur-rent basic strategy to this problem is similar to(Resnik,95a) in the sense that synsets associatedwith one word are assigned uniform frcquency or"credit" when that word appears in the corpus.We call this strategy the "brute-force" approach,like Resnik.
On top of this strategy, we introducefiltering heuristics which sort out unreliable flatausing heuristics based on the statistical propertiesof the data.4 The  feature  ext ract ion  processThis section describes the feature extraction pro-cedure.
If a sentence "a dog chased a cat" ap-pears in the corpus, features representing "chasecat" and "dog chase" may be attached to "dog"and "cat" respectively.
Fig 4 shows the overallprocess used to obtain a set of abstracted tripleswhich are sources of feature and weighting sets forsynsets.4.1 Ext ract ion  of  surface typed tr ip lesf rom the corpusTyped surface triples are triples of surface wordsholding some fixed linguistic relations (Hereaftercall this simply "surface triples").
The current im-plementation has one type "SO" which represents510"subject - verb - object" relation.
A set of typedsurface triples are extracted from a corpus withtheir frequencies.Surface triple set(TYPE VERB NOUN1 NOUN2 FREQUENCY)Fx.
(SO "ch~se" <log" "cat" 10)4.2 Expans ion  of  sin-face tr ip les to deeptriplesSurface triples are expanded to correspondingdeep triples (triples of synset IDs) by expandingeach surface word to its corresponding synsets.The frequency of the surface triples is divided bythe number of generated eep triples and it is as-signed to each deep triple.
The frequency is alsopreserved ~ it is as an occurrence count.
Surfacewords are also reserved for later processings.Deep triple collection(TYPE V-SYNSE'F N1-SYNSET N2-SYNSEq' FREQENCYOCCUttRENCE V-WORD NI-WORD N2-WORI))Ex.
(SO v123 n5 n9 0.2 10 "chase" "<log" "cat")"v123" and "n5" are synset IDs correspondingto word "chase" and "dog" respectively, Thesedeep triples are sorted and merged.
The frequen-cies and the occurrence counts are summed uprespectively.
The surface words are merged intosurface word lists as the following example shows.Deep triple set(TYPE V-SYNSET N1-SYNSEq' N2-SYNSET FREQUENCYOCCURRENCE V-WOttDS N1-WORDS N2-WORDS)gx.
(SO v123 n5 n9 0.7 15(" ch msc" )(" dog" "hou nd ") ("cat "))In this example, "dog" and "hound" have samesynset ID "n9".4.3 Synset abstraction methodThe purpose of the following phases is to extractfeatm:e sets for each synset in an abstracted form.In an abstracted form, the size of each lhaturespace becomes tractable.Abstraction of a syuset can be done by divid~ing whole synsets into the appropriate number ofsynset groups and determining a representative ofeach group to which each member is abstracted.There are several methods to decide a set of synsetgroups using a hierarchical structure.
One of thesimplest methods is to make groups by cutting thehierarchy structure at some depth from the root.We call this the flat-depth grouping method.
An-other method tries to make the nmnber of synsetsin a group constant, i.e., the upper/lower boundfor a number of concepts is given as a criteria(ttearst,93).
We call this the flat-size groupingmethod.
In our implementation, we introduce anew grouping method called the flat-probabilitygrouping method in which synset groups are speci-fied such that every group has the same class prob-abilities.
One of the advantages of this method isthat it is expected to give a grouping based onthe quantity of information which will be suitablefor the target task, i.e., semantic abstraction oftriples.
The degree of abstraction, i.e., the num-ber of groups, is one of the principal factors indeciding the size of the feature space and the pre-ciseness of the features (power of description).4.4 Deep triple abstractionEach synset of deep triples is abstracted basedon the flat-probability grouping method.
Theseabstracted triples are sorted and merged.
Originalsynset IDs are maintained in this processing forfeature extraction process.
The result is calledthe abstracted eep triple set.Abstracted deep triple set(TYPE V-ABS-SYNSET NJ-ABS-SYNSET N2-ABS-SYNSFTV-SYNSEq'-LISq' N1-SYNSE'r-LIS'f N2-SYNSET-LISTSYN-FREQUENCY OCCURRENCEV-WORDS NI-WORDS N2-WORDS)Ex.
(SO v28 n5 n9(v123 v224) (n5) (n9 n8) 5.3 32C c!
.... " "ru n "'after")C dog" "hound") C cat" "kit ty"))Synset "v28" is an abstraction of synset "v123"and synset "v224" which corresponds to "chase"and "run_after" respectively.
Synset "ng" con:e-sponding to "cat" is an abstraction of synset "nS"corresponding to "kitty".4.5 Filtering abstracted triples byheuristicsSince the current implementation adepts the"brute-force" approach, almost all massively gen-erated deep triples are fake triples.
The filter-ing process reduces the number of abstractedtriples using heuristics based on statistical dataattached to the abstracted triples.
There arethree types of statistical data available; i.e., es-timated frequency, estimated occurrences of ab-stracted triples and lists of surface words.\[ler% the length of a surface word list associ-ated with an abstracted synset is called a surfacesupport of the abstracted synset.
A heuristics ruleusing some fixed frequency threshold and a surfacesupport bound are adopted in the current imple-mentation.4.6 Common feature  ext rac t ion  f romabstracted triple setThis section describes a method for obtainingfeatures of each synset.
Basically a featureis typed binary relation extracted from an ab-stracted triple.
From the example triple,(SO v28 115 n9(v12a v224) (,,5) (,,9 ns) ,~,a a~(" chase" "run "'after")(" dog" "hound") (" cat" "kitty"))the following features are extracted for three ofthe synsets contained in the above data.n5 (ov v28 n9 5.3 32 ("chase" "run"'after")("cat" "kitty"))i19 (sv v2S n5 5.3 32 ("chase" "run "'after" )(" dog" "hound" ))n8 (sv v28 n5 5.3 32 ("chase" "run"'after")( 'dog" "hound"))An abstracted triple represents a set of ex-mnples in the text corpus and each sentence inthe corpus usually describes ome specific event.This means that the content of each abstracted511triple cannot be treated as generally or univer-sally true.
For example, even if a sentence "aman bit a dog" exists in the corpus, we cannotdeclare that "biting dogs" is a general propertyof "man".
Metaphorical expressions are typicalexamples.
Of course, the distributional semanticsapproach assumes that such kind of errors or noiseare hidden by the accumulation of a large numberof examples.However, we think it might be a more seriousproblem because many uses of nouns seem to havean anaphoric aspect, i.e., the synset which best fitsthe real world object is not included in the set ofsynsets of the noun which is used to refer to thereal world object.
"The man" can be used to ex-press any descendant of the concept "man".
Wecall this problem the word-referent disambigua-tion problem.
Our approach to this problem willbe described elsewhc're.Preliminary experiments onfeature extract ion using 1010corpusIn this section, our preliminary experiments ofthe feature extraction process are described.
Inthese experiments, we examine the proper gran-ularity of abstracted concepts.
We also discuss acriteria for evaluating filtering heuristics.
Word-Net 1.4, 1010 corpus and Brown corpus are uti-lized through the exI)eriments.
The 1010 corpusis a multiqayered structured corpus constructedon top of the FRAMEIX-D knowledge represen-tation language.
More than 10 million words ofnews articles have been parsed using a multi-scaleparser and stored in the corpus with mutual ref-erences to news article sources, parsed sentencestructures, words and WordNet synsets.5.1 Exper iment  on f ia t -p robab i l i tygroup ingTo examine the appropriate number of abstractedsynsets, we calculated three levels of abstractedsynset sets using the fiat probability group-ing method.
Class probabilities for noun andverb synsets are calculated using the brute forcemethod based on 280K nouns and 167K verbs ex-tracted fl'om the Brown eortms (1 million words).We selected 500, 1500, 3000 synset groups forcandidates of feature description level.
The 500node level is considered to be a lowest boundaryand the 3000 node level is expected to be the tar-I)epth 1 2 3 4 5 6 7 8Synsets 611 122 966 2949 5745 12293 8384 7408Depth 9 10 11 12 13 14 15 16Synsets 5191 3068 1417 812 314 94 36 6Table 1.
Depth/Noun_Synsels in WordNet 1.4Level 500 (518 synsets)1 (structure construction\](72\]9.47 4): a thing constructed; acon~.plex eonstruetioI'l or entity2 {time_period period period_of_tilne\](6934 3): a length eftime; "government services began during the colonial period"3 {organization\](6469.94 4):a group of people who work together4 {action}(6370.54 9): something done;5 {natural_object}(6277.26 3): an object occurring naturally;Level 3000 (3001 synsets)1 {natural language tongue mother tongue\](678.7 6):the language of a community~2 {weapon arm weapon_system\](673.7(~ 6):used in fighting or hunting3 {head chief top_dog}(671.55 ):4 {capitalist}(669.45 ):a person who believes in the capitalistic system5 {point point_in_~ime}(669.29 8): a parti.cular clock time;Table 2: Synsets I)y f lat-probal)i l lty grouping metho(1get abstraction level.
This expectation is based onthe observation that 3000 node granularity is em-pirically sulficient for deseribing the translationpatterns for selecting the proper target Fmglishverb for one Japanese verb(lkehara,93).Table 1 shows the average synset node depthand the distribution of synset node depth of Word-Net1.4.
Table 2 lists the top five noun synsetsin the fiat probability groupings of 500 and 3000synsets.
"{}" shows synset.
The first and the sec-ond number in "0"  shows the class frequency andthe depth of synset respectively.Level 500 grout)ings contain a very abs|ractedlevel of synsets such as "action", "time_period"and "natural_object".
This level seems to betoo general for describing the features of objects.In contrast, the level 3000 groupings contains"natural_language", "weapotf' ,  "head,chief', and"point_in_time" which seems to be a reasonablebasis for feature description.There is a relatively big depth gap betweensynsets in the abstracted synset group.
F, ven inthe 500 level synset group, there is a two-depthgap.
In the 3000 level synset group, there is4 depth gap between "capitalist" (depth 4:) and"point_in_time" (depth 8).
The interesting pointhere is that "point_in_time" seems to be more at).stract than "capitalist, " inluitively speaking.The actual synset numbers of each level ofsynset groups are 518, 15%8, and 3001. '
fhusthe fiat probability grouping method can preciselycontrol the lew'J of abstraction.
Considering thepossible abstraction levels available by the fiat-depth method, i.e., depth 2 (122 synsets), depth3 (966 synsets), depth 4 (2949 synsets), this is agreat advantage over the flat probability grouping.5.2 Exper iment :  Abst rac ted  t r ip les  f rom1010 corpusA preliminary experiment for obtaining abstracttriples as a basis of features of synsets was con-ducted.
82,703 surface svo triples are extractedfrom the 101.0 corpus.
Polarities of abstractedtriple sets for 500, 1500, 3000 level abstractionare 1.20M, 2.03M and 2.30M respectively.
Each512Level 15001 {organization}{talk sp,:ak utter mouth verbulize vurbi~v}{ organization\] (70,4.24,1|)8)2 {organization){talk spcal~ utt<r inottth v, rl>aliz< ver/-,ify}{action}(5(;,'La5,112)3 {organization} {change ttndergo~a_change becozlLe_(liitk:rent}{l>ossession } (60,2.83,188)4 torgauization} {talk speak utter mouth vez'lmlizu vcrbi~,\]{ ....... ~t) (48,175,a4)5 {cn'ganiza*ion} {move displac ....... ke ......... } {action}(5(), ~ .s4,82)L,wel 3000I {c?pcrL}{greet z'(:cogniz~.
:)/"cxpr<sa grc( ring:; up(m nlcc!ing .
"{ due_process due_process_of_law}2 { jmT}/"a  body of citi'zcns sworn to give a true verdict .
"~l>ronoun(:e IM)el judge)/'})t'Ol'~Otll~('c, jttdgm, nt on"{capit~alist} (4,11.09,4)3 {police police_force constal)ulary law}{allege aver say}/" l t (  alleged ltlat w( was the victim "{female fen:~alc_pel'son} (4,\].,3)4 {assc'nfl)ly}"a body that holds formal n~(x.lings"{refuse l't~jccL l),'/sS_tll3 Ltlrll_(Iown (h<:\[in(!
}/" rcfllsO I;0 ~t(:('c\])t;"{r,:quest petition solicitation} ((;,0 25,G)5 {animal animate_b<:ing /)~;ml I)rttt0 Cl'c~tlttl'u ft~ll l lH}/{win gaitL}/'win somvthiug dlrough one's; ,.ll\~vls"{ contest comt)etit ion\] (5,0 ..19,6)"()" .+I ...... (# ,,f s,,,,r ......... s,,pl,,,~+~ ,l',.,,q,,,.,,y:o .
.
.
.
.
.
.
.i,,,, ....... )TaMe 3: ~xamph!
of abst rac ted  triple.
'+abstract triple holds ft:equeu(:y, oc<:llJ'lX)llO(, lltl/ll-.be G and woful list, which mqq~(a't.s each of thce(~al)st ra(:ted sy nsel:s.A lilt ering heuristic that elin~htates al:+sll'a<:ttrit,les whose stlr\['ac(; Sul)pOrl is three (i.e., sup-ported })y only one sm'face \])~I~LC\]:II) iS al>plicd toeach set of al)sLracLed Iril)les , ;111(1 l;(.','-.
;tl\[I.s iu theR)llowing sizes of at)stract:ed triple sets in the 379K(level 500), 150b: (level 1500) and 561,: (\]ewq 3000)respectively.
F, ach triple is assiglted a evaluationscore which is a snt|, of m)rnmlized surface SUl)l)(~rLscore  (:: Sll.l:f3e('.
Sllt)l)orl; s<:ore/tl-taXilHtl l l l  ,qlll'I'~/ceSUl)l)orL score)  ;+tim normalized \[\]:e(luet~(;y (~ fre+( luency  / nmxi /unm f i 'equency) .  '
l 'at) le 3 showsthe top \ [be abstra<'ted tril)les with respect o dw+irewduaLiot~ scores, ltetns in the talJe shows subjectsyl lseL,  ver t )Sy l lSe l ; ,  oh j0e l ;  synseL ,  sttrfa<:e sup-l)Orl;, f r<'qt le l tcy ~tll(\] oc('.ll\])re\]lC(~ IlllIlI\])0I!S.All the sul<iccl;s in the top five al)sLract triplesof  level  500  are "organ iza l ; io t f ' .
This seems to be.r0asonal)le bee;rose the COlll;eltl;s of the 10\] 0 corpusare news articles ~tt(t l:hese triples seem to showsome highly abstract, briefing of the cont.ent, s ofthe corpus.The clfcclAveness of the filtering ;re(l/or scoringhcuri6dcs ca, n bc tl:l(~a,,stlr(;(l ttsilt~ tv?
(~ ch)scly re-.lated criteria.
One measm:es the l)lausitfility o\['al)stract.ed triple,s i.e., the r(x'all and l)cecision |'a-+I;io of the l)\]ausible at)straeted Lriples.
'l'he othercriteria shows the correctness of the mappings ofdie surface t;riple I)atLerns to abstracted tr i l ) les.varsity \[htiled_Nations t:ealn subsidiary State state staff so+vier school l 'ol itburo police patrol party palml OrganizationOI'(\[CI" operation lle!
'vVSl)~/,})Vl' li issioll Ministry lll(:II21)t'F lll\[tg\[~-zine lin(: law_firm law hind 3u:~tice_l)epartmcnt jury industryhOL|S(?
h<~ztd(tual't,.
'I'S govut ' i l l I / tHlt  g?t l lg I:tlA division COlllq ,:'OllH-try co/lllcil Collf(!l'eltc(!
<:(Jllll)tllly ( 'ommitlct (:ollcge (:ht\]2 Cabi-net business board associ~tion Association airlineTable  4.
SIIrflk(?
( ~.
Snplmrt,  s of  "o rgmdzat ion"This is measured I)y counting the eon:ect surfacesupports of each absl.racted triple, l"or example,considering a set of sm:l';u:e words sut~port.ing "o>ganization" of Lhe I o f  leve l  ,500 shown in table 4,the word "panel" rnight loe used as "panel board".
'l'his abilily is also measm:ed by developing theword sense dismnbiguator whic.h inputs the sur-fa(:e tril)le and select:s lhe most l~\[ausil)le deepIril)le based ou abstracted triple scores matchedwith the deep triple, 'Flm surface SUlh~octs iu 'l';t--hie 4 show the intuitNe tendency that a suftlcientnumber of triple data will generate solid results.6 ConclusionsThis paper described a simil+~rity calculaik)nmodel between ol),je+cl.s based on commoz~ and dis-l inctiwe feal, ures ;-mcl prol)oses an hnplementationl>rocedu re \[br obtaining feat;ures based on al>stractlriples extracted l}om a large text <:orpus (1010corpu,~) utilizing taxonomical km)wle(lge (Word-Net).
The exl)eritt|ettL , which used around 801{SLlrfaee triples, shows l,}lal; t, he abstraction level3000 l)rovi(le.s a good basis for \['eal;ttre.
(les<-zit)-l i on .
A feal;m'e extra(:tion eXl)erhnent based (ml a rge  tr i i ) \]c ( \ [a~a is ()tic next.
goal+l {e ferences\](~:nneth (Thurch mid Pal;rick Hanks.
t98,9, l/Vo~d assoc2-at~on norms, ~nnt~al inj'orm,ttio.n, and h?.rlcogvaphy , \]n Pro-cccdings of th(.
'27th Annual Me+~tittg of A(:I,E I)IL 1995.
>'.umma~?/ fo r  the I';I)\]~ Ele<:tronw l)ict'~o?~argVersion l Technical <:'lz~de El) I f  T1~2-005, Japan I';\]ectronicI )ictionary II.cs~ arch Iiml:il ul % 'l'okyo.
(h'egory Grctbnstettc 1994.
I';xptovations in A~ztotnatic7'k+saurus l)'~scovc*y, /( luwur Academic Publishers.Marti.
A. I lcarst and Him'lob S<hutze.
\]9.q'l.
(:~zs~om+znl9a Lc:!
:~con to Hettcr .%tzt a Coml)utat 'wnal  Task.
Proco.dmgsof the ACf, SIGI,I.
;X Workshop,  (h Jmnbus,  Ohio.\])cmald l l indh.
19q0.
No~z*t classzJicativn ~(r;;m pred'u:alei~<q~t?ncn~ st?ltCttZ?t:.% Proceedings of life 28th Anntt~d Meetingof A( :1,S.
Ikthm';t, M. Miyazaki, A. Yokoo.
1993.
('lass~firat'~o~of J)a~.jtitt~je \[(llo2ulrtdfJ(!
fo~' Mc a'lr*ny .Anahjs'ls Jn A.hz(h.zneTtamsJat~on, Transactions of h/lk)z'mal;ion t'rocesMng Societyof Jal>aP, , Vol 34, No.8, pl>s.
\]692-1704.J.
Kolodner and C. H.iesbeck.
Cast>l\]ased Rtasott~n~htutorial t .xtbook of \] l th I JCAI.
(;,org( A. Miller, ILichard ileckwith, Christiane t"el\[bmtm,I)tl'C\]( (\]I'ONS I Kad~crine Millet< 19!)0.
l"~ve Papers on Word-N,t, (k)gnilivc Sciunce \],M)or;~tory ILeport 43, Princeton tJni-vcrsity.lloy Rada, Hafeclh Mill, \]"llen Bicknell, Maria Bh'ttner.1989.
Z)eualvpme'nt alzd Apphcat+oa of a Metr ic  on 5rc;tlaltltcN+ts, \]I';I';E qYansacl:ions on Systems~ MmG :rod ('yberncti,'s,\%1.
tg, No.
I.Phi|ip |tt:,nnik.
l!10,gzt.
?i,'sZnCl info?~nat+o~t ( 'o~ttcr l t  to l",val-tizzY+ >'cm~u~tic Szmi lar i ty  i~t a Taxonomy,  Pz'occedings of1J(:A\[-95.Philip liesnik.
1!
)!151) I)'lsamb~guating No~t~ (,'roul.ulgsw~th \]{espcct o I/VordNet 5'e'as+~s, Proceedings of AnnualM(.cting of  ACt,.lIinz'ich Schutze.
1993.
Adva'~wes in Neural lTzforn~at~ontbocessing ,<'gste*ns 5, Stephen ,/.
\[\[anson, Jack D. Cowan,(:.i,ce Files editors, Morgmt Kaufmmm, San Marco (?A.Michael SklSSll/t.
1993.
Wo~d :'ensc k)'~sambiguation fo rt"rce-temt hzdccqnq Using a Mass ive Semant ic  Network,  P\['o-cccdings of the S,,?
(:on(\[ \[nterlmtional Conf~rcn(:e ,:)n \[nfornla-don and l(zzowl,.dge Managument (CIKM-93)Amos Twrsky  1977.
\["aat+tres Of 5'imHa~ity.
l'sychologicalt{-view, Vol.
84, Number 4.513
