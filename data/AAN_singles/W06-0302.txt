Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 9?14,Sydney, July 2006. c?2006 Association for Computational LinguisticsToward Opinion Summarization: Linking the SourcesVeselin Stoyanov and Claire CardieDepartment of Computer ScienceCornell UniversityIthaca, NY 14850, USA{ves,cardie}@cs.cornell.eduAbstractWe target the problem of linking sourcementions that belong to the same entity(source coreference resolution), which isneeded for creating opinion summaries.
Inthis paper we describe how source coref-erence resolution can be transformed intostandard noun phrase coreference resolu-tion, apply a state-of-the-art coreferenceresolution approach to the transformeddata, and evaluate on an available corpusof manually annotated opinions.1 IntroductionSentiment analysis is concerned with the extrac-tion and representation of attitudes, evaluations,opinions, and sentiment from text.
The area ofsentiment analysis has been the subject of muchrecent research interest driven by two primary mo-tivations.
First, there is a desire to provide appli-cations that can extract, represent, and allow theexploration of opinions in the commercial, gov-ernment, and political domains.
Second, effec-tive sentiment analysis might be used to enhanceand improve existing NLP applications such as in-formation extraction, question answering, summa-rization, and clustering (e.g.
Riloff et al (2005),Stoyanov et al (2005)).Several research efforts (e.g.
Riloff and Wiebe(2003), Bethard et al (2004), Wilson et al (2004),Yu and Hatzivassiloglou (2003), Wiebe and Riloff(2005)) have shown that sentiment informationcan be extracted at the sentence, clause, or indi-vidual opinion expression level (fine-grained opin-ion information).
However, little has been done todevelop methods for combining fine-grained opin-ion information to form a summary representa-tion in which expressions of opinions from thesame source/target1 are grouped together, multi-ple opinions from a source toward the same tar-get are accumulated into an aggregated opinion,and cumulative statistics are computed for eachsource/target.
A simple opinion summary2 isshown in Figure 1.
Being able to create opinionsummaries is important both for stand-alone ap-plications of sentiment analysis as well as for thepotential uses of sentiment analysis as part of otherNLP applications.In this work we address the dearth of ap-proaches for summarizing opinion information.In particular, we focus on the problem of sourcecoreference resolution, i.e.
deciding which sourcementions are associated with opinions that belongto the same real-world entity.
In the example fromFigure 1 performing source coreference resolutionamounts to determining that Stanishev, he, and herefer to the same real-world entities.
Given theassociated opinion expressions and their polarity,this source coreference information is the criticalknowledge needed to produce the summary of Fig-ure 1 (although the two target mentions, Bulgariaand our country, would also need to be identifiedas coreferent).Our work is concerned with fine-grained ex-pressions of opinions and assumes that a systemcan rely on the results of effective opinion andsource extractors such as those described in Riloffand Wiebe (2003), Bethard et al (2004), Wiebeand Riloff (2005) and Choi et al (2005).
Presentedwith sources of opinions, we approach the prob-lem of source coreference resolution as the closely1We use source to denote an opinion holder and target todenote the entity toward which the opinion is directed.2For simplicity, the example summary does not containany source/target statistics or combination of multiple opin-ions from the same source to the same target.9?
[Target Delaying of Bulgaria?s accession to the EU] wouldbe a serious mistake?
[Source Bulgarian Prime MinisterSergey Stanishev] said in an interview for the German dailySuddeutsche Zeitung.
?
[Target Our country] serves as amodel and encourages countries from the region to followdespite the difficulties?, [Source he] added.
[Target Bulgaria] is criticized by [Source the EU] because ofslow reforms in the judiciary branch, the newspaper notes.Stanishev was elected prime minister in 2005.
Since then,[Source he] has been a prominent supporter of [Target hiscountry?s accession to the EU].Stanishev AccessionEUBulgariaDelaying+?
?+Figure 1: Example of text containing opinions(above) and a summary of the opinions (below).In the text, sources and targets of opinions aremarked and opinion expressions are shown initalic.
In the summary graph, + stands for positiveopinion and - for negative.related task of noun phrase coreference resolu-tion.
However, source coreference resolution dif-fers from traditional noun phrase (NP) coreferenceresolution in two important aspects discussed inSection 4.
Nevertheless, as a first attempt at sourcecoreference resolution, we employ a state-of-the-art machine learning approach to NP coreferenceresolution developed by Ng and Cardie (2002).Using a corpus of manually annotated opinions,we perform an extensive evaluation and obtainstrong initial results for the task of source coref-erence resolution.2 Related WorkSentiment analysis has been a subject of much re-cent research.
Several efforts have attempted toautomatically extract opinions, emotions, and sen-timent from text.
The problem of sentiment ex-traction at the document level (sentiment classifi-cation) has been tackled as a text categorizationtask in which the goal is to assign to a documenteither positive (?thumbs up?)
or negative (?thumbsdown?)
polarity (e.g.
Das and Chen (2001), Panget al (2002), Turney (2002), Dave et al (2003),Pang and Lee (2004)).
In contrast, the problem offine-grained opinion extraction has concentratedon recognizing opinions at the sentence, clause,or individual opinion expression level.
Recentwork has shown that systems can be trained to rec-ognize opinions, their polarity, and their strengthat a reasonable degree of accuracy (e.g.
Dave etal.
(2003), Riloff and Wiebe (2003), Bethard etal.
(2004), Pang and Lee (2004), Wilson et al(2004), Yu and Hatzivassiloglou (2003), Wiebeand Riloff (2005)).
Additionally, researchers havebeen able to effectively identify sources of opin-ions automatically (Bethard et al, 2004; Choi etal., 2005; Kim and Hovy, 2005).
Finally, Liu et al(2005) summarize automatically generated opin-ions about products and develop interface that al-lows the summaries to be vizualized.Our work also draws on previous work in thearea of coreference resolution, which is a rela-tively well studied NLP problem.
Coreferenceresolution is the problem of deciding what nounphrases in the text (i.e.
mentions) refer to the samereal-world entities (i.e.
are coreferent).
Generally,successful approaches have relied machine learn-ing methods trained on a corpus of documentsannotated with coreference information (such asthe MUC and ACE corpora).
Our approach tosource coreference resolution is inspired by thestate-of-the-art performance of the method of Ngand Cardie (2002).3 Data setWe begin our discussion by describing the data setthat we use for development and evaluation.As noted previously, we desire methods thatwork with automatically identified opinions andsources.
However, for the purpose of developingand evaluating our approaches we rely on a corpusof manually annotated opinions and sources.
Moreprecisely, we rely on the MPQA corpus (Wilsonand Wiebe, 2003)3, which contains 535 manu-ally annotated documents.
Full details about thecorpus and the process of corpus creation can befound in Wilson and Wiebe (2003); full detailsof the opinion annotation scheme can be found inWiebe et al (2005).
For the purposes of the dis-cussion in this paper, the following three pointssuffice.First, the corpus is suitable for the domains andgenres that we target ?
all documents have oc-curred in the world press over an 11-month period,between June 2001 and May 2002.
Therefore, the3The MPQA corpus is available athttp://nrrc.mitre.org/NRRC/publications.htm.10corpus is suitable for the political and governmentdomains as well as a substantial part of the com-mercial domain.
However, a fair portion of thecommercial domain is concerned with opinion ex-traction from product reviews.
Work described inthis paper does not target the genre of reviews,which appears to differ significantly from news-paper articles.Second, all documents are manually annotatedwith phrase-level opinion information.
The an-notation scheme of Wiebe et al (2005) includesphrase level opinions, their sources, as well asother attributes, which are not utilized by our ap-proach.
Additionally, the annotations contain in-formation that allows coreference among sourcementions to be recovered.Finally, the MPQA corpus contains no corefer-ence information for general NPs (which are notsources).
This might present a problem for tradi-tional coreference resolution approaches, as dis-cussed throughout the paper.4 Source Coreference ResolutionIn this Section we define the problem of sourcecoreference resolution, describe its challenges,and provide an overview of our general approach.We define source coreference resolution as theproblem of determining which mentions of opin-ion sources refer to the same real-world entity.Source coreference resolution differs from tradi-tional supervised NP coreference resolution in twoimportant aspects.
First, sources of opinions donot exactly correspond to the automatic extrac-tors?
notion of noun phrases (NPs).
Second, duemainly to the time-consuming nature of corefer-ence annotation, NP coreference information is in-complete in our data set: NP mentions that are notsources of opinion are not annotated with coref-erence information (even when they are part ofa chain that contains source NPs)4.
In this pa-per we address the former problem via a heuris-tic method for mapping sources to NPs and givestatistics for the accuracy of the mapping process.We then apply state-of-the-art coreference resolu-tion methods to the NPs to which sources were4This problem is illustrated in the example of Figure 1The underlined Stanishev is coreferent with all of the Stan-ishev references marked as sources, but, because it is usedin an objective sentence rather than as the source of an opin-ion, the reference would be omitted from the Stanishev sourcecoreference chain.
Unfortunately, this proper noun might becritical in establishing coreference of the final source refer-ence he with the other mentions of the source Stanishev.Single Match Multiple Matches No MatchTotal 7811 3461 50Exact 6242 1303 0Table 1: Statistics for matching sources to nounphrases.mapped (source noun phrases).
The latter prob-lem of developing methods that can work with in-complete supervisory information is addressed ina subsequent effort (Stoyanov and Cardie, 2006).Our general approach to source coreference res-olution consists of the following steps:1.
Preprocessing: We preprocess the corpus by runningNLP components such as a tokenizer, sentence split-ter, POS tagger, parser, and a base NP finder.
Sub-sequently, we augment the set of the base NPs foundby the base NP finder with the help of a named en-tity finder.
The preprocessing is done following the NPcoreference work by Ng and Cardie (2002).
From thepreprocessing step, we obtain an augmented set of NPsin the text.2.
Source to noun phrase mapping: The problemof mapping (manually or automatically annotated)sources to NPs is not trivial.
We map sources to NPsusing a set of heuristics.3.
Coreference resolution: Finally, we restrict our atten-tion to the source NPs identified in step 2.
We extracta feature vector for every pair of source NPs from thepreprocessed corpus and perform NP coreference reso-lution.The next two sections give the details of Steps 2and 3, respectively.
We follow with the results ofan evaluation of our approach in Section 7.5 Mapping sources to noun phrasesThis section describes our method for heuristicallymapping sources to NPs.
In the context of sourcecoreference resolution we consider a noun phraseto correspond to (or match) a source if the sourceand the NP cover the exact same span of text.
Un-fortunately, the annotated sources did not alwaysmatch exactly a single automatically extracted NP.We discovered the following problems:1.
Inexact span match.
We discovered that often (in3777 out of the 11322 source mentions) there is nonoun phrase whose span matches exactly the source al-though there are noun phrases that overlap the source.In most cases this is due to the way spans of sourcesare marked in the data.
For instance, in some casesdeterminers are not included in the source span (e.g.
?Venezuelan people?
vs. ?the Venezuelan people?).
Inother cases, differences are due to mistakes by the NPextractor (e.g.
?Muslims rulers?
was not recognized,while ?Muslims?
and ?rulers?
were recognized).
Yet inother cases, manually marked sources do not match thedefinition of a noun phrase.
This case is described inmore detail next.11Measure Overall Method and Instance B3 MUC Positive Identification Actual Pos.
Identificationrank parameters selection score Prec.
Recall F1 Prec.
Recall F1B3 1 svm C10 ?0.01 none 81.8 71.7 80.2 43.7 56.6 57.5 62.9 60.2400 5 ripper asc L2 soon2 80.7 72.2 74.5 45.2 56.3 55.1 62.1 58.4Training MUC Score 1 svm C10 ?0.01 soon1 77.3 74.2 67.4 51.7 58.5 37.8 70.9 49.3Documents 4 ripper acs L1.5 soon2 78.4 73.6 68.3 49.0 57.0 40.0 69.9 50.9Positive 1 svm C10 ?0.05 soon1 72.7 73.9 60.0 57.2 58.6 37.8 71.0 49.3identification 4 ripper acs L1.5 soon1 78.9 73.6 68.8 48.9 57.2 40.0 69.9 50.9Actual pos.
1 svm C10 ?0.01 none 81.8 71.7 80.2 43.7 56.6 57.5 62.9 60.2identification 2 ripper asc L4 soon2 73.9 69.9 81.1 40.2 53.9 69.8 52.5 60.0B3 1 ripper acs L4 none 81.8 67.8 91.4 32.7 48.2 72.0 52.5 60.69 svm C10 ?0.01 none 81.4 70.3 81.6 40.8 54.4 58.4 61.6 59.9200 MUC Score 1 svm C1 ?0.1 soon1 74.8 73.8 63.2 55.2 58.9 32.1 74.4 44.9Training 5 ripper acs L1 soon1 77.9 0.732 71.4 46.5 56.3 37.7 69.7 48.9Documents Positive 1 svm C1 ?0.1 soon1 74.8 73.8 63.2 55.2 58.9 32.1 74.4 44.9identification 4 ripper acs L1 soon1 75.3 72.4 69.1 48.0 56.7 33.3 72.3 45.6Actual pos.
1 ripper acs L4 none 81.8 67.8 91.4 32.7 48.2 72.0 52.5 60.6identification 10 svm C10 ?0.01 none 81.4 70.3 81.6 40.8 54.4 58.4 61.6 59.9Table 2: Performance of the best runs.
For SVMs, ?
stands for RBF kernel with the shown ?
parameter.2.
Multiple NP match.
For 3461 of the 11322 sourcementions more than one NP overlaps the source.
Inroughly a quarter of these cases the multiple match isdue to the presence of nested NPs (introduced by theNP augmentation process introduced in Section 3).
Inother cases the multiple match is caused by source an-notations that spanned multiple NPs or included morethan only NPs inside its span.
There are three gen-eral classes of such sources.
First, some of the markedsources are appositives such as ?the country?s new pres-ident, Eduardo Duhalde?.
Second, some sources con-tain an NP followed by an attached prepositional phrasesuch as ?Latin American leaders at a summit meeting inCosta Rica?.
Third, some sources are conjunctions ofNPs such as ?Britain, Canada and Australia?.
Treat-ment of the latter is still a controversial problem inthe context of coreference resolution as it is unclearwhether conjunctions represent entities that are distinctfrom the conjuncts.
For the purpose of our current workwe do not attempt to address conjunctions.3.
No matching NP.
Finally, for 50 of the 11322 sourcesthere are no overlapping NPs.
Half of those (25 tobe exact) included marking of the word ?who?
suchas in the sentence ?Carmona named new ministers,including two military officers who rebelled againstChavez?.
From the other 25, 19 included markings ofnon-NPs including question words, qualifiers, and ad-jectives such as ?many?, ?which?, and ?domestically?.The remaining six are rare NPs such as ?lash?
and?taskforce?
that are mistakenly not recognized by theNP extractor.Counts for the different types of matches ofsources to NPs are shown in Table 1.
We deter-mine the match in the problematic cases using aset of heuristics:1.
If a source matches any NP exactly in span, match thatsource to the NP; do this even if multiple NPs overlapthe source ?
we are dealing with nested NP?s.2.
If no NP matches matches exactly in span then:?
If a single NP overlaps the source, then map thesource to that NP.Most likely we are dealing withdifferently marked spans.?
If multiple NPs overlap the source, determinewhether the set of overlapping NPs include anynon-nested NPs.
If all overlapping NPs arenested with each other, select the NP that iscloser in span to the source ?
we are still dealingwith differently marked spans, but now we alsohave nested NPs.
If there is more than one setof nested NPs, then most likely the source spansmore than a single NP.
In this case we select theoutermost of the last set of nested NPs before anypreposition in the span.
We prefer: the outermostNP because longer NPs contain more informa-tion; the last NP because it is likely to be the headNP of a phrase (also handles the case of expla-nation followed by a proper noun); NP?s beforepreposition, because a preposition signals an ex-planatory prepositional phrase.3.
If no NP overlaps the source, select the last NP beforethe source.
In half of the cases we are dealing with theword who, which typically refers to the last precedingNP.6 Source coreference resolution ascoreference resolutionOnce we isolate the source NPs, we apply corefer-ence resolution using the standard combination ofclassification and single-link clustering (e.g.
Soonet al (2001) and Ng and Cardie (2002)).We compute a vector of 57 features for everypair of source noun phrases from the preprocessedcorpus.
We use the training set of pairwise in-stances to train a classifier to predict whether asource NP pair should be classified as positive (theNPs refer to the same entity) or negative (differententities).
During testing, we use the trained clas-sifier to predict whether a source NP pair is pos-itive and single-link clustering to group togethersources that belong to the same entity.7 EvaluationFor evaluation we randomly split the MPQA cor-pus into a training set consisting of 400 documents12and a test set consisting of the remaining 135 doc-uments.
We use the same test set for all evalua-tions, although not all runs were trained on all 400training documents as discussed below.The purpose of our evaluation is to create astrong baseline utilizing the best settings for theNP coreference approach.
As such, we try thetwo reportedly best machine learning techniquesfor pairwise classification ?
RIPPER (for Re-peated Incremental Pruning to Produce Error Re-duction) (Cohen, 1995) and support vector ma-chines (SVMs) in the SVM light implementation(Joachims, 1998).
Additionally, to exclude pos-sible effects of parameter selection, we try manydifferent parameter settings for the two classifiers.For RIPPER we vary the order of classes and thepositive/negative weight ratio.
For SVMs we varyC (the margin tradeoff) and the type and parameterof the kernel.
In total, we use 24 different settingsfor RIPPER and 56 for SVM light.Additionally, Ng and Cardie reported better re-sults when the training data distribution is bal-anced through instance selection.
For instanceselection they adopt the method of Soon et al(2001), which selects for each NP the pairs withthe n preceding coreferent instances and all in-tervening non-coreferent pairs.
Following Ng andCardie (2002), we perform instance selection withn = 1 (soon1 in the results) and n = 2 (soon2).With the three different instance selection algo-rithms (soon1, soon2, and none), the total numberof settings is 72 for RIPPER and 168 for SVMa.However, not all SVM runs completed in the timelimit that we set ?
200 min, so we selected halfof the training set (200 documents) at random andtrained all classifiers on that set.
We made sureto run to completion on the full training set thoseSVM settings that produced the best results on thesmaller training set.Table 2 lists the results of the best performingruns.
The upper half of the table gives the re-sults for the runs that were trained on 400 docu-ments and the lower half contains the results forthe 200-document training set.
We evaluated us-ing the two widely used performance measures forcoreference resolution ?
MUC score (Vilain et al,1995) and B3 (Bagga and Baldwin, 1998).
In ad-dition, we used performance metrics (precision,recall and F1) on the identification of the posi-tive class.
We compute the latter in two differentways ?
either by using the pairwise decisions asthe classifiers outputs them or by performing theclustering of the source NPs and then consideringa pairwise decision to be positive if the two sourceNPs belong to the same cluster.
The second option(marked actual in Table 2) should be more repre-sentative of a good clustering, since coreferencedecisions are important only in the context of theclusters that they create.Table 2 shows the performance of the best RIP-PER and SVM runs for each of the four evaluationmetrics.
The table also lists the rank for each runamong the rest of the runs.7.1 DiscussionThe absolute B3 and MUC scores for sourcecoreference resolution are comparable to reportedstate-of-the-art results for NP coreference resolu-tions.
Results should be interpreted cautiously,however, due to the different characteristics of ourdata.
Our documents contained 35.34 source NPsper document on average, with coreference chainsconsisting of only 2.77 NPs on average.
The lowaverage number of NPs per chain may be produc-ing artificially high score for the B3 and MUCscores as the modest results on positive class iden-tification indicate.From the relative performance of our runs, weobserve the following trends.
First, SVMs trainedon the full training set outperform RIPPER trainedon the same training set as well as the correspond-ing SVMs trained on the 200-document trainingset.
The RIPPER runs exhibit the opposite be-havior ?
RIPPER outperforms SVMs on the 200-document training set and RIPPER runs trainedon the smaller data set exhibit better performance.Overall, the single best performance is observedby RIPPER using the smaller training set.Another interesting observation is that the B3measure correlates well with good ?actual?
perfor-mance on positive class identification.
In contrast,good MUC performance is associated with runsthat exhibit high recall on the positive class.
Thisconfirms some theoretical concerns that MUCscore does not reward algorithms that recognizewell the absence of links.
In addition, the resultsconfirm our conjecture that ?actual?
precision andrecall are more indicative of the true performanceof coreference algorithms.138 ConclusionsAs a first step toward opinion summarization wetargeted the problem of source coreference resolu-tion.
We showed that the problem can be tackledeffectively as noun coreference resolution.One aspect of source coreference resolution thatwe do not address is the use of unsupervised infor-mation.
The corpus contains many automaticallyidentified non-source NPs, which can be used tobenefit source coreference resolution in two ways.First, a machine learning approach could use theunlabeled data to estimate the overall distributions.Second, some links between sources may be real-ized through a non-source NPs (see the exampleof figure 1).
As a follow-up to the work describedin this paper we developed a method that utilizesthe unlabeled NPs in the corpus using a structuredrule learner (Stoyanov and Cardie, 2006).AcknowledgementsThe authors would like to thank Vincent Ng and Art Munsonfor providing coreference resolution code, members of theCornell NLP group (especially Yejin Choi and Art Munson)for many helpful discussions, and the anonymous reviewersfor their insightful comments.
This work was supported bythe Advanced Research and Development Activity (ARDA),by NSF Grants IIS-0535099 and IIS-0208028, by gifts fromGoogle and the Xerox Foundation, and by an NSF GraduateResearch Fellowship to the first author.ReferencesA.
Bagga and B. Baldwin.
1998.
Entity-based cross-document coreferencing using the vector space model.
InProceedings of COLING/ACL.S.
Bethard, H. Yu, A. Thornton, V. Hativassiloglou, andD.
Jurafsky.
2004.
Automatic extraction of opinionpropositions and their holders.
In 2004 AAAI Spring Sym-posium on Exploring Attitude and Affect in Text.Y.
Choi, C. Cardie, E. Riloff, and S. Patwardhan.
2005.
Iden-tifying sources of opinions with conditional random fieldsand extraction patterns.
In Proceedings of EMNLP.W.
Cohen.
1995.
Fast effective rule induction.
In Proceed-ings of ICML.S.
Das and M. Chen.
2001.
Yahoo for amazon: Extractingmarket sentiment from stock message boards.
In Proceed-ings of APFAAC.K.
Dave, S. Lawrence, and D. Pennock.
2003.
Mining thepeanut gallery: Opinion extraction and semantic classifi-cation of product reviews.
In Proceedings of IWWWC.T.
Joachims.
1998.
Making large-scale support vectormachine learning practical.
In A. Smola B. Scho?lkopf,C.
Burges, editor, Advances in Kernel Methods: SupportVector Machines.
MIT Press, Cambridge, MA.S.
Kim and E. Hovy.
2005.
Identifying opinion holders forquestion answering in opinion texts.
In Proceedings ofAAAI Workshop on Question Answering in Restricted Do-mains.B.
Liu, M. Hu, and J. Cheng.
2005.
Opinion observer: An-alyzing and comparing opinions on the web.
In Proceed-ings of International World Wide Web Conference.V.
Ng and C. Cardie.
2002.
Improving machine learningapproaches to coreference resolution.
In Proceedings ofACL.B.
Pang and L. Lee.
2004.
A sentimental education: Senti-ment analysis using subjectivity summarization based onminimum cuts.
In Proceedings of ACL.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
Sentiment classification using machine learning tech-niques.
In Proceedings of EMNLP.E.
Riloff and J. Wiebe.
2003.
Learning extraction patternsfor subjective expressions.
In Proceesings of EMNLP.E.
Riloff, J. Wiebe, and W. Phillips.
2005.
Exploiting sub-jectivity classification to improve information extraction.In Proceedings of AAAI.W.
Soon, H. Ng, and D. Lim.
2001.
A machine learningapproach to coreference resolution of noun phrases.
Com-putational Linguistics, 27(4).V.
Stoyanov and C. Cardie.
2006.
Partially supervisedcoreference resolution for opinion summarization throughstructured rule learning.
In Proceedings of EMNLP.V.
Stoyanov, C. Cardie, and J. Wiebe.
2005.
Multi-Perspective question answering using the OpQA corpus.In Proceedings of EMNLP.P.
Turney.
2002.
Thumbs up or thumbs down?
Semantic ori-entation applied to unsupervised classification of reviews.In Proceedings of ACL.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1995.
A model-theoretic coreference scor-ing scheme.
In Proceedings of MUC-6.J.
Wiebe and E. Riloff.
2005.
Creating subjective and objec-tive sentence classifiers from unannotated texts.
In Pro-ceedings of CICLing.J.
Wiebe, T. Wilson, and C. Cardie.
2005.
Annotating ex-pressions of opinions and emotions in language.
Lan-guage Resources and Evaluation, 1(2).T.
Wilson and J. Wiebe.
2003.
Annotating opinions in theworld press.
4th SIGdial Workshop on Discourse and Di-alogue (SIGdial-03).T.
Wilson, J. Wiebe, and R. Hwa.
2004.
Just how mad areyou?
Finding strong and weak opinion clauses.
In Pro-ceedings of AAAI.H.
Yu and V. Hatzivassiloglou.
2003.
Towards answer-ing opinion questions: Separating facts from opinions andidentifying the polarity of opinion sentences.
In Proceed-ings of EMNLP.14
