A Re laxat ion  Method  for Unders tand ingSpeech  Ut terances  1Stephanie SeneffSpoken Language Systems GroupLaboratory  for  Computer  Sc ienceMassachuset ts  Ins t i tu te  of Techno logyCambr idge ,  Massachuset ts  02139 U.S.A.SpontaneousABSTRACTThis paper describes an extension to the MIT ATIS (AirTravel Information Service) system, which allows it to answera question when a full linguistic analysis fails.
This "robust"parsing capability was achieved through minor extensions ofpre-existing components already in place for the full linguis-tic analysis component.
Robust parsing is apphed only aftera full analysis has failed, and it involves the two stages of1) parsing a set of phrases and clauses, and 2) gluing themtogether to obtain a single semantic frame encoding the fullmeaning of the sentence.
We have assessed the degree of suc-cess of the robust parsing mechanism through a breakdown ofthe performance ofrobustly parsed vs. fully parsed sentenceson the October '91 "dry-run" test set.
It was clear that therobust parser allowed us to answer many more questions cor-rectly, as over a third of the sentences were not covered bythe grammar.
We also report here on the performance of thesystem on the February '92 test sentences, and discuss omeissues with regard to the evaluation methodology.INTRODUCTIONCurrent approaches to the language understandingaspect of spoken language systems tend to fall into twocategories.
In syntax-driven formulations \[1,4,10\], a com-plete syntactic analysis is performed which attempts toaccount for all words in an utterance.
While provid-ing strong linguistic constraints to the speech recogni-tion component and a useful structure for further lin-guistic analysis, such an approach can break down in thepresence of unknown words, novel linguistic constructs,recognition errors, and some spontaneous speech eventssuch as false starts.
In contrast, semantic-driven ap-proaches \[2,5,9\] tend to derive their understanding byspotting key words and phrases in the utterance.
Whilethis approach can potentially provide better coverage anddeal with ill-formed sentences, it provides less constraintfor the speech recognizer, and may not be able to ade-quately interpret complex linguistic constructs.This paper describes our efforts to develop a languageunderstanding component that combines the advantagesof both of these approaches.
Our strategy has been to1This research was supported by DARPA under ContractN00014-89-J-1332, monitored through the Office of Naval Research.299relax the constraint that the syntactic analysis must ac-count for all of the words in an utterance.
Our currentimplementation is a two stage process.
In the first step,our parser \[7\] searches for a complete linguistic analysis.Failing that, constraints of the parser are relaxed to per-mit the recovery of parsable phrases and clauses withinthe sentence.
These fragments are fused together usinga mechanism that closely resembles our discourse historymechanism \[8\].
Thus the robust parser is able to leverageoff of existing components to a large degree.ROBUST PARSINGMECHANISMThe natural language component of the MIT ATISsystem makes use of a semantic frame representation fthe meaning which serves as the input for database ac-cess, spoken response generation, and history manage-ment.
The frame design is flexible enough to be readilyJextended to other domains.
Domain-dependent aspectsof the system are entered mainly through table-drivenmechanisms that seek certain patterns in the frame, withvery little explicit programming required.Because the semantic frame is so central to our sys-tem, we felt it was appropriate to integrate the fragmentsprovided by partial parse analysis at the frame level.Whenever a full linguistic analysis fails, a set of parsetrees accounting for key phrases and clauses is recovered.Each parse tree is individually converted to a semanticframe, and the set of frames are combined to form a sin-gle semantic frame encoding the meaning of the entiresentence.
This frame is then ready for integration intothe existing mechanisms of the back-end component.The ability to provide partial parses was achieved bymodifying the parser and the grammar in minor ways.The grammar is written as a set of context free rewriterules with constraints, and is converted automatically toa network form, where each node in the network repre-sents a particular category (which might be a semanticname such as a-place or a syntactic one such as predi-cate).
In full-sentence analysis mode, only the sentencecategory is allowed to terminate, and only at the end ofthe sentence.
In the relaxed mode, on the other hand,a set of categories representing important clauses andphrases are allowed to terminate, and such terminationcan occur anywhere in the sentence.W:hen operating in robust mode, the parser proceedsleft-to-right, initially producing an exhaustive set of pos-sible parses beginning at the first word of the sentence.The parse that consumes the most words is then selected 2.The parser begins again at the first subsequent word, re-peating the procedure.
Whenever no parses are returned,the parser advances by one word and tries again.
Even-tually a set of parsed phrases are returned.In order to combine parsed fragments, we need an in-heritance mechanism that is similar in many respects toour discourse model.
Since we already have the capabilityof responding appropriately to sentence fragments uchas 'aircraft" or "first class," we surmised that the samemechanism could be utilized effectively to fuse togetherparsed fragments within a single sentence.
The only im-portant distinction between such a sentence-internal his-tory mechanism and the existing sentence-ezternal his-tory mechanism is that nothing from the internal historycan be overwritten, since answers have not yet been pro-vided to the previous parsed fragments.In the standard history mechanism, the presence ofcertain attributes in the new frame masks inheritance ofcertain other attributes from the history.
Furthermore,whenever a value for a given attribute occurs in the cur-rent frame and also in the history frame, the value of thatattribute from the history is overwritten.
The sentence-internal history mechanism remembers everything, how-ever, since none of the pieces have as yet been answered.Whenever the frames are judged to be too disjoint, thesystem spawns additional top-level clauses, essentiallyproducing a compound sentence.
This would be the case,for example, for the input: "I'll take flight twelve oh nine.What ground transportation is available in Denver?
"An example, shown in Figure 1, will help to explainthe difference between the two history mechanisms.
Thesentence, "What are the meals and aircraft for flight twoeighty one and also for flight two oh one," is treated bythe parser as three sequential entries: "What are themeals," "aircraft for flight 281," and "flight 201."
If thissequence were delivered to the sentence-external historymechanism, the last phrase would be interpreted as "air-craft for flight 201."
Sentence internally, however, theresult would become "meals and aircraft for flights 281and 201."
Once the sentence is fully fused, the externalhistory is brought in, and the sentence may inherit fur-ther constraints from the dialogue context, as shown inthe figure, where it picks up a source and destination.Further examples of robust parsing on sentences spo-ken by actual users are shown in Figure 2.
In all threecases, we believe the system produced reasonable answersto the questions.
The tables are omitted ue to space lim-2In a more sophist icated form, the score may t~ke into accountN-best outputs  and/or  parse probabil it ies.300itations, but the verbal response gives a clear indicationof the system's interpretation.Re jec t ion  Cr i te r ionBecause the DARPA evaluation mechanism currentlypenalizes ystems for incorrect answers, we augmentedthe robust parser with a capability for detecting certainkey words, such as "between," which, if not properly un-derstood, would most likely lead to an incorrect answer.Another heuristic, most relevant when a speech recog-nizer is included, was to refuse to answer if an unknownflight number was detected in the sentence.
We usedthese sentences to update the discourse context, but gavea NO ANSWEa response for evaluation.
In addition, whenthe input was judged overall to be sufficiently unreliabledue to recognition errors, we used a more conservativerejection criterion that excluded answers for sentencesthat did not receive a full parse and were suspected torequire context.
We used a simple algorithm (flights withno source and destination) to distinguish this set.EVALUATION PROCEDUREThe DARPA community has been developing an eval-uation scheme over the past year and a half, based on acomparison between an answer produced by the systemand a set of two "rain/max" answers provided by trainedannotators, specifying the minimum and maximum re-quirement for expected entries from the database, wherethe maximum table addresses the overgeneration issue.The sentences for a given dialogue are presented in orderto the system being tested, and it must deal with the sen-tence in context o come up with an appropriate answer 3.No partial credit is given for a "nearly correct" answer,and systems are penalized for wrong answers, so that thescore is defined as the difference between percent correctand percent incorrect.The DARPA community have also been involved ina joint effort to collect a large body of common trainingand testing data within the ATIS domain.
The data havebeen collected at five sites (AT~T,  BBN, MIT, CMU,and SRI) under a variety of different conditions, some-times including a speech recognizer (SRI), sometimes al-lowing only verbal feedback (AT&T), sometimes typedverbatim to the site-dependent system (MIT), and some-times including a "wizard" who could rephrase the ques-tion before submitting it to the data collection system(BBN, CMU).
The sentences have been labelled accord-ing to four distinct categories: Class A (no context re-quired), Class D1 (context-dependent o  only one pre-ceding sentence), Class D (context dependent on multiplesentences) ~,and Class X (unanswerable).Thus far, two sets of data have been released as test-ing material for this "min/max" whole-dialogue evalua-tion.
The first set, the so-called October '91 "dry-run"3Sentence categories are no~ known to the system.4Class D and D1 have now been merged to a single class D.INPUT: "(WHAT ARE THE MEALS) AND (AIRCRAFT FOR FLIGHT TWO EIGHTY ONE) AND ALSO FOR(FLIGHT TWO OH ONE)"FRAME: \[Existential clauseTopic: \[(aircraft meal) for;\[(flight) number:(281, 201)\]\]\]RESPONSE: Here are meals for flights 281 and 201 from boston to denver.AIRLINE NUMBER FROM TO MEALS SERVEDUNITED 201 BOS DEN BREAKFASTUNITED 281 BOS DEN DINNERHere are the aircraft for flights 281 and 201 from boston to denver.AIRLINE NUMBER FROM TO AIRCRAFT COMPANYUNITED 201 BOS DEN DC8 MCDONNELL DOUGLASUNITED 281 BOS DEN DC8 MCDONNELL DOUGLASFigure I: Examplesentencetoillustr~erobust-parsing mechanism.
Parenthesesin thesentenceindicate parsed phr~es.INPUT: "(WHAT ARE THE CHEAPEST FLIGHTS FROM BOSTON TO ATLANTA) WITH (CONNECTING FLIGHTS INPHILADELPHIA)"RESPONSE: Here are the cheapest connecting flights from Boston to Atlanta connecting inPhiladelphia.
<shows table>INPUT: "(I WOULD LIKE TO FLY FROM BOSTON TO ATLANTA AND STOP IN PHILADELPHIA) (ARE THEREANY PLANES) (THAT) DO (THAT)"RESPONSE: Here are the aircraft for the flights with stops from Boston to Atlanta connectingin Philadelphia.
<shows table>INPUT: "(HOW LONG) DOES (FLIGHT TWO EIGHTY ONE) REMAIN (IN DENVER)RESPONSE: Here are the connection times for flight 281 from Boston to San Francisco connectingin Denver.
<shows table>Figure 2: Three examples ofrobustly parsed sentences, taken ~om the tr~ning data.
Tables are omitted in theinterest ofconserving space.test set, served as a testbed to confirm that the evalua-tion procedure can be executed and has merit.
Out of atotal of 362 sentences in this set, 290 were "answerable,"(Class A, D, or D1).
The second set is the February '92test set, released just prior to this meeting.
The datainclude nearly 1000 sentences, distributed equally overcontributions from all five collecting sites.
A subset of687 sentences were considered evaluable.
This test setis associated with a set of "official" results for all of theparticipating sites, mediated through NIST.RESULTSWe report here on the results for the two DARPA testreleases, and on three different systems: (1) The MIT NL(text input) system, (2) The MIT Spoken Language Sys-tem (recognizer included), and (3) The MIT-SRI system(MIT NL component operating on outputs from a rec-ognizer developed at SRI \[3\]).
For the October '91 NL-only experiment, we give a breakdown of performancefor those sentences that required robust parsing against301those that received a full parse, in order to assess howmuch robust parsing helped.
For the February '92 testset, we provide a detailed discussion of the errors for thetext-input condition.
We use the MIT-SRI results in anexperiment to address the question of whether it is validto penalize systems one-to-one for incorrect answers.October '91 Test ResultsA breakdown of the results for our system on textinput on the October '91 test set, with robust parsingincluded, is given in Figure 3.
All of the columns under"robust" mode would have given a NO ANSWER responsewithout the robust parser.
Over half of the answers mustbe correct in order to yield a net gain in score.
For theClass A and Class D1 sentences, this requirement wasmet with a comfortable margin.
Although the Class D,robustly parsed sentences yielded a greater number ofincorrect answers than correct ones, this result is mis-leading, because the majority of the errors were not dueto failures in the robust parsing algorithm.
For instance,140120'100'0o .0!I=QEZ 40\ [ \ ]  NO Answer\ [ \ ]  FalseD Tn,m200Full Parse Robust Full Parse Robust Full Parse RobustClass A Class D1 Class DFigure 3: Results for the October '91 test set, text in-put, broken down by sentence type.
Class A: Context In-dependent; Class DI: Context Dependent on a Single Query;Class D: Context Dependent on Multiple Queries.
The robustparser is used only when a full parse fails.\[ Correct \[IncorrectText Input 80% 13%MIT-SLS 61% 14%MIT-SRI 69% 19%No Answer \[ Error7% 32.5%25% 52.8%12% 50.7%Table 1: Performance r sults for three systems for the Febru-ary '92 test set.five sentences concerned a fare "less than one thousanddollars."
A minor bug in the number interpretation rou-tine led to an incorrect answer to all of these questions.An additional four sentences failed due to a minor prob-lem in the external history mechanism.
Overall, we werequite encouraged by the result of this evaluation, whichindicates that the robust parsing mechanism provides apowerful enhancement of the system's capabilities.February  '92  Test  Resu l tsTable 1 gives performance results for the Feb '92 testset.
For the text-input condition, 80% of the queries wereanswered correctly, and this number dropped to 61% forspeech-input mode.
The number of incorrect answers re-mained almost constant at 13%, with a correspondinglarge increase in unanswered questions from 7% to 25%.This is a direct result of our change in rejection strat-egy in going from text-input o speech-input mode.
Weexamined in detail all of the sentences for which our text-input system produced an incorrect answer, categorizingthe errors in the hopes of assessing how far away we arefrom the ideal goal of an error-free system.A breakdown of the categorizations i  given in Ta-ble 2.
Seventeen answers fell in the category, "correct,"302which is to say the system produced the answer we ex-pected it to produce, and we feel that, although the an-swer does not match the comparator's requirement, it isnonetheless also a reasonable answer.
For instance, theanswer we gave to the question, "what is the stopover,"was the location of the stop, whereas the comparator ex-pects, inexplicably, the number of stops instead.
Therewere five essentially identical sentences asking for thenumber of Delta flights in differing fare classes.
In allcases our count was off by one, because we included aconnecting flight one of whose legs was a Delta flight.This was a consequence of a misunderstanding on ourpart of the rules, so we feel that the system did the rightthing in this case.
Two sentences were a result of thecomparator refusing to accept "NIL" and a null string asthe same thing.
Other "correct" sentences involved aninterpretation of the context, often for cases where thesubject is speaking "computerese," where we think ourinterpretation is a valid one.
Given that 20% of the er-rors are in this category, we believe that the comparatorevaluation is probably overly rigid.
It might make senseto allow some flexibility in overruling the comparator'sresult on a case-by-case basis.There were 32 sentences in the category "easily fixed.
"It took two day's time to correct the mistakes for thesesentences, although they were distributed over all aspectsof the system (parse failure, meaning representation, dis-course mechanism, and query generation).
Some of themwere clearly bugs, whereas others were simply due to in-complete understanding (such as generalizing "this after-noon" to mean "today" as well as "in the afternoon.")
Sixof the sentences failed due to a deficiency in our discoursemechanism specific to questions about airlines.
These in-volved an anaphoric reference to a set of phantom flights,implied because of a preceding question about an airline.The system understood "those flights" only in the con-text of an existing set of flights that had been generatedthrough a call to the database.
Thus, in the sequence,"Does Delta fly between Boston and Denver," followedby "Show me ~hose fligMs," the system was unable tounderstand which flights were intended.
This was an in-teresting discourse situation, and we were happy to un-cover this inadequacy in our system.
Overall, while it isencouraging that it was easy to correct so many errors, itis also problematic that we continue to uncover such "mi-nor" problems in unseen data.
It is unclear how manymore sets of 1000 sentences will be necessary before newbugs and inadequacies are no longer encountered.Twenty seven sentences were judged as more difficultto correct, and their problems are about equally dividedbetween the categories "complex meaning" and "difficultcontext."
A particularly troublesome set for context arethe sentences spoken by subjects who tend to chronicallyspeak a staccato computerese which is difficult to distin-guish from normal fragments.
We are not as concernedabout these sentences, because these subjects would getfeedback from our system were they using it interactively,Table 2: Breakdown of 87 errors in the M!T Text-Input February '92 test set.which would serve to communicate o them quite clearlyhow the system is interpreting their staccato sentences,thus keeping the dialogue coherent.
Another set of sen-tences that are very difficult yet probably not fruitful tocorrect, are "stage setting" sentences that tend to askfor too much information, such as the test-set sentence,"Please give me flight information from Denver to Pitts-burgh to Atlanta and return to Denver."
Our systemprovides a large subset of the flights requested, whichis surely information overload anyway, leading the sub-ject, in an interactive mode, to follow up with a sentenceasking for information about only one leg of the trip.The eleven remaining errors were distributed amongthree categories.
Three were due to an incorrect anal-ysis of a context-setting query.
False starts that weredeadly for the robust parser accounted for four errors.For instance, a stutter on the word "a" ("A a flight")produced the interpretation "AA" (American Airlines).Such problems are very difficult to repair, and we seeno near-term solutions.
An additional four errors werelabelled "uninteresting," either because our system willnever see such a sentence in actual operation (a requestfor a definition of a code like "DDEN," which is neverdisplayed to the user by our system) or because the sen-tence is hopelessly obscure, such that a similar sentencewould never reoccur.The  MIT -SR I  Sys temThe SRI researchers have provided us with their rec-ognizer's outputs for three sets of data: a training datasubset, the October '91 test set, and the February '92test set.
We used the training data to develop an ap-propriate rejection mechanism, and then we applied theresults to both test sets.
We decided to use the samerejection criterion for this test as for the NL-input test,without screening context dependent sentences requiringa robust parse, as we had done for the MIT recognizerinputs.Interestingly, the error for the "MIT-SRI" system onthe October '91 test set was only ten percentage pointshigher than that for text input, whereas the performancedrop was much greater for the February '92 test set (18.2points).
We don't fully understand this difference, butapparently the recognition errors were more disruptivefor the February '92 test set than for the October '91 testset.
Although the SRI recognizer has a significantly bet-ter SPREC performance than the MIT recognizer (11.0%Error vs. 18.0%), our SLS system was apparently notable to take advantage of this performance improvement.The error for the MIT-only system was only 2% higher303than that of the MIT-SRI system.
We can think of atleast two factors that may account for this surprising re-sult.
The first is that our recognizer results were obtainedthrough filtering by TINA on 10 N-best outputs from ourrecognizer.
IfTINA could find a parsable hypothesis, thenthat one would be selected as the recognizer output.
Thismeant hat small errors in prepositions and the like weremore likely to be corrected.
The second factor is the morerigid rejection criterion used for the MIT recognizer.
Alarger percentage of the MIT-SRI sentences were incor-rect (19% vs. 14%), and we suspect hat had we used thesame rejection criterion for the SRI recognizer as for theMIT recognizer the performance would have improved.We strongly suspect hat the algorithm of penalizingsentences one-to-one for incorrect answers is too steep apenalty.
Because current system capabilities generally in-clude a good discourse model as well as an ability to han-dle sentence fragments, it is often the case that a partiallyunderstood query provides valid information that the sys-tem can make use of in a follow-up query.
For instance,if the user said: "Show me all flights from Boston to Dal-las leaving Tuesday morning before ten" and the systemmisunderstood "Tuesday" as "Thursday," the user couldsimply say in a follow-up query, "On Tuesday," and thesystem would be able to deliver a completely correct an-swer.
On the other hand, if the system instead refused toanswer the first question (so as to maximize score), theuser would have to repeat he entire sentence in order toretain the other conditions.The only way to clearly assess whether or not sys-tems should err in the direction of answering too muchis to compare user satisfaction tests on A/B conditions.Short of this, however, it is still possible to devise anexperiment to assess the degree of correctness for thoseanswers that the recognizer misunderstood.
To do this,we selected a subset of 62 utterances from the Febru-ary '92 test material, representing all queries which hadbeen correctly answered (according to the comparator)by our NL system, but incorrectly answered by the jointMIT-SRI SLS system.
We have available to us a frame-based evaluation procedure that we make use of inter-nally for comparing semantic frames generated by therecognizer against hose generated from the true orthog-raphy.
The scoring involves comparing a set of key/valuepairs representing the set of attributes mentioned in thesentence, things like "source," "departure-time," "fare-code," "flight-number," etc.
The score is computed as(correct - insertion) / (correct -t- substitution -b dele-tion), where "correct" means that both the key and thevalue are identical between the hypothesis (NL answer)\] Nsentences I Nkeys INcorrect I Nsub I Ndel I Nins I Score I162 \]213 l 163 122 128 121 \]67% \]Table 3: Results of an experiment on a subset of February '92test sentences whose orthography was correctly understood bythe NL component but whose SRI recognizer outputs wereincorrect.
See text for further details.and the reference (recognizer answer).The result is shown in Table 3.
There were on averageabout 3.5 attributes per sentence to be identified.
Thesystem identified correctly more than 3 out of every 4attributes, with an insertion rate (recognizing additionalfalse attributes) of 10%.
This suggests to us that userswould be better served if the system answered most ofthese questions than if the system simply said a cannedphrase such as, "I'm sorry, I didn't understand you,"requiring the user to reinstantiate even those attributesthat had been correctly recognized.CONCLUSIONSThrough examining a large body of speech materialcollected from a general population of naive users, wehave reached the conclusion that it is not feasible to de-sign a grammar that can always achieve a complete lin-guistic analysis of every input sentence.
We have simul-taneously become aware that a system that could recovera partial analysis would also be valuable for overcomingsome recognition errors.
We have described in this papera capability to produce a partial analysis whenever a fullparse fails, and have reported substantial performanceimprovements on test material as a direct consequenceof this robust mechanism.
We were able to leverage offof existing system components to a large extent, leadingto a rapid development of the new robust parsing mecha-nism.
This capability allowed the system to answer manymore sentences than had previously been possible.We have begun to explore some possibilities for mak-ing use of a set of N-best recognizer outputs, by parsinga network of paths generated through an intelligent joinof the top-N candidates.
We can use the frequency ofoccurrence of a word in the top-N candidates as a mea-sure of its robustness, and then select a path throughthe network that maximizes the selection of linguisticallymeaningful phrases that recurred among the top-N sen-tences.We have just begun to incorporate robust parsing intoour data-collection procedure.
We have collected ata for4 scenarios from each of 15 subjects, where the systemwas toggled between robust and non-robust modes halfway through each subject's episode.
Subjects were askedto solw~ the scenarios, all of which had a unique answer.Interestingly, subjects were able to find the correct an-swer in robust mode 90% of the time, v.s.
only 70% inthe non-robust mode.
We take this as a clear indicatorthat robust mode is effective in real usage.
For a furtherdiscussion of this experiment see \[11\].ACKNOWLEDGEMENTSI would like to thank Hy Murveit and John Butzbergerof SRI for providing recognizer outputs for some train-ing data and the October '91 and February '92 test set.Joe Polifroni helped diagnose the incorrect answers in theFebruary '92 test results.
Discussions with J im Glass onrobust parsing algorithms have been fruitful, and he hasalso played a major role in the network parsing experi-ments.REFERENCES\[1\] Bobrow, R.,Ingria, R., and Stallard, R., "Syntacticand Semantic Knowledge in the DELPHI UnificationGrammar," Proc.
DARPA Speech and Natural LanguageWorkshop: 230-236, June 1990.\[2\] Jackson, E., Appelt, D., Bear, J., Moore, R., and Pod-lozny, A., "A Template Matcher for Robust NL Inter-pretation,",Proc.
DARPA Speech and Natural LanguageWorkshop: 190-194, February, 1991.\[3\] Murveit, H., "Speech Recognitin in SRI's Resource Man-agement and ATIS Systems,"Proc.
DARPA Speech andNatural Language Workshop: 94-100, February, 1991.\[4\] Norton, L., Linebarger, M., Dahl, D. and Nguyen, N."Augmented Role Filling Capabilities for Semantic In-terpretation ofSpoken Language," Proc.
DARPA Speechand Natural Language Workshop: 125-133, February,1991.\[5\] Pieraccini, R., Levin, E. and Lee, C.H., "Stochastic Rep-resentation of Conceptual Structure in the ATIS Task,"Proc.
DARPA Speech and Natural Language Workshop:121-124, February, 1991.\[6\] Price, P., "Evaluation of Spoken Language Systems: TheAIIS Domain," Proc.
Third Darpa Speech and NaturalLanguage Workshop: 91-95, June 1990.\[7\] Seneff, S., "TINA: A Natural Language System for Spo-ken Language Applications," J.
Association for Compu-tational Linguistics, To Appear, March, 1992.\[8\] Seneff, S., Hirschman, L., and Zue, V., "Interactive Prob-lem Solving and Dialogue in the ATIS Domain," Proc.Fourth Darpa Speech and Natural Language Workshop:354-359, February 1991.\[9\] Ward, W., "The CMU Air Travel Information Service:Understanding Spontaneous Speech,", Proc.
DARPASpeech and Natural Language Workshop: 127-129, June,1990.\[10\] Zue, V., Glass, J., Goodine, D., Leung, H., Phillips,M., Po!ifroni , J., and.
Seneff, S., "Integration of SpeechRecognition and Natural Language Processing in theMIT VOYAGER System," Proc.
ICASSP 91: 713-716,May 1991.\[11\] Polifroni, J., Hirschman, L., Seneff, S., and Zue, V. "Ex-periments in Evaluating Interactive Spoken LanguageSystems," These Proceedings.\[12\] Zue, V., Glass, J., Goodine, D., Hirschman, L., Leung,H., Phillips, M., Polifroni, J., and.
Seneff, S., "The MITATIS System: Preliminary Development, SpontaneousSpeech Data Collection, and Performance Evaluation,"Proc.
Eurospeech 91: 537-540, September 1991.304
