Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1875?1885, Dublin, Ireland, August 23-29 2014.Syntactic Parsing and Compound Recognition via Dual Decomposition:Application to FrenchJoseph Le Roux1and Matthieu Constant2and Antoine Rozenknop1(1) LIPN, Universit?
Paris 13 ?
Sorbonne Paris Cit?, CNRS(2) LIGM, Universit?
Paris Est, CNRSleroux@univ-paris13.fr, mconstan@univ-mlv.fr, antoine.rozenknop@lipn.univ-paris13.frAbstractIn this paper we show how the task of syntactic parsing of non-segmented texts, including com-pound recognition, can be represented as constraints between phrase-structure parsers and CRFsequence labellers.
In order to build a joint system we use dual decomposition, a way to com-bine several elementary systems which has proven successful in various NLP tasks.
We evaluatethis proposition on the French SPMRL corpus.
This method compares favorably with pipelinearchitectures and improves state-of-the-art results.1 IntroductionDual decomposition (DD), which can be used as a method to combine several elementary systems, hasalready been successfully applied to many NLP tasks, in particular syntactic parsing, see (Rush et al.,2010; Koo et al., 2010) inter alia.
Intuitively, the principle can be described quite simply: at decodingtime, the combined systems seek for a consensus on common subtasks, in general the prediction of someparts of the overall structure, via an iterative process imposing penalties where the systems disagree.
Ifthe systems converge to a solution, it is formally guaranteed to be optimal.
Besides, this approach isquite flexible and easy to implement.
One can add or remove elementary systems without rebuildingthe architecture from the ground up.
Moreover, the statistical models for the subsystems can often beestimated independently at training time.In this paper we show how syntactic parsing of unsegmented texts, integrating compound recognition,can be represented by constraints between phrase-structure parsers and sequence labellers, either forcompound recognition or part-of-speech (POS) tagging, and solved using DD.
We compare this approachexperimentally with pipeline architectures: our system demonstrates state-of-the-art performance.
Whilethis paper focuses on French, the approach is generic and can be applied to any treebank with compoundinformation, and more generally to tasks combining segmentation and parsing.This paper is structured as follows.
First, we describe the data we use to build our elementary systems.Second, we present related work in compound recognition, in particular for French, and the type ofinformation one is able to incorporate in tag sets.
Third, we show how CRF-based sequence labellers withthese different tag sets can be combined using DD to obtain an efficient decoding algorithm.
Fourth, weextend our method to add phrase-structure parsers in the combination.
Finally, we empirically evaluatethese systems and compare them with pipeline architectures.2 DataWe use the phrase-structure treebank released for the SPMRL 2013 shared task (Seddah et al., 2013).This corresponds to a new version of the French Treebank (Abeill?
et al., 2003).
One of the key dif-ferences between French data and other treebanks of the shared task is the annotation of compounds.Compounds are sequences of words with a certain degree of semantic non-compositionality.
They formThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/1875a single lexical unit to which one can assign a single POS.
In the SPRML corpus 15% of the tokens be-long to a compound, or 12.7% if we omit numerals: the training, development and test sets respectivelycomprise 23658, 2120 and 4049 compounds.In the treebank, compounds are annotated as subtrees whose roots are labelled with the POS of thecompounds with a + suffix.
Each leaf under a compound is the daughter of its own POS tag, which is inturn the daughter of the root of the compound.
For example, the tree in Figure 1 contains a subtree withthe compound adverb pour l?instant (so far) whose category ADV+ dominates the preposition pour, thedeterminer l?, and the noun instant.SENTPONCT.VNVPbloqu?eADVcompl?tementVestNP-SUJNCsituationDETlaPONCT,ADV+NCinstantDETl?PPourFigure 1: Syntatic annotation in the SPRML FTB: So far, the situation has been completely blocked.The sequence labellers used in the experiments are able to exploit external lexical resources that willhelp coping with data missing from the training corpus.
These resources are dictionaries, consisting oftriplets (flexed form, lemma, POS tag), where form and lemmamay be compound or simple.Several such dictionaries exist for French.
We use:?
DELA (Courtois et al., 1997) contains a million entries, among which 110,000 are compounds;?
Lefff (Sagot, 2010) contains 500,000 entries, among which 25,000 are compounds;?
Prolex (Piton et al., 1999) is a toponym dictionary with approximately 100,000 entries.The described resources are additional to the SPMRL shared task data (Seddah et al., 2013), but werealso used in (Constant et al., 2013a) for the shared task.3 Compound Recognition3.1 Related WorkThe compound recognition traditionally relies on 2 types of information: lexical and syntactic clues.
Astrong lexical association between the tokens of a compound can be detected using a compound dictio-nary or by measuring a degree of relatedness, which can be learnt on a corpus.
Some recent approachesuse sequence labellers.
The linear chain CRF model (Lafferty et al., 2001) is widely used, see for ex-ample (Vincze et al., 2011; Constant and Tellier, 2012).
It has proven to be a very adequate model: itis flexible enough to incorporate information from labelled data and external resources (POS taggers,compound lexicons or named entity recognisers).The compound recognition may also be directly performed by syntactic parsers learnt from corporawhere compounds are marked, such as the one we use in this paper1(Arun and Keller, 2005; Green et al.,2011; Green et al., 2013; Constant et al., 2013b), but these results are contradictory.
Green et al.
(2011)experimentally show that a lexicalised model is better than an unlexicalised one.
On the other hand,Constant et al.
(2013b) show that, using products of PCFG-LA (Petrov, 2010), unlexicalised models canbe more accurate.
They obtain performance on a par with a linear chain CRF system without externalinformation.
But such information is difficult to incorporate directly in a PCFG-LA model.
Constant etal.
(2012) resort to a reranker to add arbitrary features in the parse selection process, but their systemshowed inferior performance compared with a CRF model with access to the same external information.1Such an approach has been used already for joint named entity recognition and parsing based on CRF (Finkel and Manning,2009).18763.2 Annotation schemesCompound recognition can be seen as a segmentation task which consists in assigning to each token alabel with segmentation information.
We use label B if the token is the beginning of a word (single orcompound), and label I if the token is inside a compound, but not in initial position.
This lexical seg-mentation can be enriched with additional information, for example POS tags of compounds or tokens incompounds, and gives a variety of tag sets.
This leads us to define 5 simple tag sets for our problem, eachwith very simple information, that will be combined in the next section.
These tag sets are exemplifiedon a sentence with the compound vin rouge (red wine).1.
(basic) recognition with two labels (B and I)Luc/B aime/B le/B vin/B rouge/I (Luc likes red wine)2.
(partial) recognition with compound POS tags: [BI]-POS for tokens in compounds; B for othersLuc/B aime/B le/B vin/B-NC+ rouge/I-NC+3.
(partial-internal) recognition with token POS tags in compoundsLuc/B aime/B le/B vin/B-NC rouge/I-ADJ4.
(complete) recognition with POS tags for all tokens; in compounds use compound POS tagsLuc/B-NPP aime/B-V le/B-DET vin/B-NC+ rouge/I-NC+5.
(complete-internal) recognition with POS tags for all tokens; in compounds use token POS tagsLuc/B-NPP aime/B-V le/B-DET vin/B-NC rouge/I-ADJ4 Dual decomposition for compound recognition using CRFs4.1 A maximisation problem4.1.1 CRFA conditional random field (Lafferty et al., 2001), or CRF, is a tuple c = (?,Lc, wc, {fcp}p) whichdefines the conditional probability of a sequence of labels y ?
L?cgiven a sequence of words of the samelength x ?
?
?as a logistic regression of the form:Pc(y|x) =exp(?p?P(x)wc?
fcp(x, yp))Z(wc, x), where (1)?
wc?
Rdis a d-dimensional weight vector, where d is the number of features of the system,?
Z is the partition function?
P(x) is a set of places, in our case the set of unigram and bigram decompositions of sequencesof words.
A place p is of the form [i]xfor unigrams and [i, i + 1]xfor bigrams.
We omit x whencontext is unambiguous.?
ypis the restriction of y to the place p, and we will write yifor y[i]and yiyi+1for y[i,i+1]?
fcpis the feature function for the place p that projects (x, yp) on Rd.Our goal is to find the best labelling, i.e.
the one that maximises the conditional probability given asequence of tokens.
One can observe that this labelling also maximises the numerator of Equation 1, asZ(wc, x) does not depend on y.
We therefore write:y?c= argmaxy?c(x, y) = argmaxy?p?P(x)wc?
fcp(x, yp) (2)18774.1.2 Viterbi Algorithm for CRFsSince our combination of CRF systems relies on the Viterbi algorithm, we review it briefly.
For a giveninput sentence x = x1.
.
.
xn, we represent the problem of finding the best labelling with a CRF c as abest path algorithm in a directed acyclic graph Gc= (V, E) built from a set of nodes V and a set of edgesE .
Nodes are pairs (xi, l) where xiis an input token and l is an admissible label for xi.2Edges connectnodes of the form (xi, l) to nodes of the form (xi+1, l?)
and the weights of these arcs are given by c. Inorder to find the weight of the best path in this graph, that corresponds to the score of the best labelling,we use Algorithm 1.3One can remark that the score of a node decomposes as a score s1, computed froma vector of unigram features, written fc[i](x, l), and a score s2computed from a vector of bigram features,written fc[i?1,i](x, l?, l).4The Viterbi algorithm has a time complexity linear in the length of the sentenceand quadratic in the number of labels of the CRF.Algorithm 1 Viterbi Algorithm for CRFs with unigram and bigram features1: Viterbi(Gc, wc, {fcp}p,?BI,?IB):2: for all node v do3: ?
[v] = ?
?4: end for5: ?
[ (<s>,START)] = w ?
fp0(x, START)6: for all non initial node v = (xi, l) in topological order do7: s1?
wc?
fc[i](x, l)8: s2?
?
?9: for all incoming edge v?= (xi?1, l?)?
v do10: t?
?[v?]
+ wc?
fc[i?1,i](x, l?, l)11: t?
t?
b(l?
)i(l) ?
?BI[i]?
i(l?
)b(l) ?
?IB[i] ?
only for DD: we ignore this line otherwise12: if t > s2then13: s2= t14: end if15: end for16: ?[v]?
s1+ s217: end for18: return the best scoring path ?
[ (</s>,STOP) ]4.2 Dual decomposition for CRF combinationsIn Section 3.2 we described several annotation schemes that lead to different CRF models.
Theseschemes give the same lexical segmentation information but they use more or less rich part-of-speechtag sets.
It is not clear a priori if the richness of the tag set has a beneficial effect over segmentationprediction: there is a compromise between linguistic informativeness and data sparseness.
Instead oftrying to find the best annotation scheme, we propose a consensus-based joint system between severalCRF-based sequence labellers for the task of text segmentation relying on dual decomposition (Rushet al., 2010).
This system maximises the sum of the scores of combined CRFs, while enforcing globalconsistency between systems in terms of constraints over the admissible solutions.
These constraints arespecifically realised as reparametrisations of the elementary CRFs until a consensus is reached.
Sincewe deal with several annotation schemes, we will use predicates to abstract from them:?
b(l) is true if l starts with B;?
i(l) is true if l starts with I;?
bi(i, y) is true if b(yi?1) and i(yi) are true;?
ib(i, y) is true if i(yi?1) and b(yi) are true.For a labelling y, we define 2 boolean vectors that indicate where the compounds begin and end:2We also include two additional nodes: an initial state (<s>,START) and a final state (</s>, STOP).3Algorithm 1 calculates the score and backpointers must be added to retrieve the corresponding path.4This algorithm takes as input 2 vectors that will be used for DD and will be explained in ?
4.2.
They can be ignored now.1878?
D(y), such that D(y)[i] = 1 if bi(i, y), and 0 otherwise;?
F (y), such that F (y)[i] = 1 if ib(i, y), and 0 otherwise.As we want to combine CRFs, the solution of our system will be a tuple of label sequences with thesame compound segmentation.
For an input sequence x, this new maximisation problem is:(P ) : find max(y1,...,yn)n?c=1?c(yc) =n?c=1?p?P(x)wc?
fcp(x, ycp) (3)s.t.
?u1, u2?c ?
J1, nK, D(yc) = u1, F (yc) = u2(4)Objective (3) indicates that we seek for a tuple for which the sum of the scores of its elements ismaximal.
Constraints (4) imply that the compound frontiers ?
transitions B to I and I to B ?
must bethe same for each element of the tuple.
There are several ways to tackle this problem.
The first oneis by swapping the sum signs in (3) and noticing that the problem could then be represented by a jointsystem relying on dynamic programming ?
a CRF for which labels would be elements of the productL = L1?
?
?
?
?
Ln?
and for which it is straightforward to define a weight vector and feature functions.We can therefore reuse the Viterbi algorithm but the complexity is quadratic in the size of L, which isimpractical5.In any case, this approach would be inadequate for inclusion of parsers, and we therefore rely onlagrangian relaxation.
We modify the objective by introducing Lagrange multipliers, two real vectors?BIcand ?IBcindexed by bigram places6for each CRF c of the combination.
We obtain a new problemwith the same solution as the previous one, since constraints (4) are garanteed to be satisfied at optimality:max(y1,...,yn,u1,u2)min(?BI,?IB)n?c=1?c(yc) ?n?c=1[(D(yc) ?
u1) ?
?BIc+ (F (yc) ?
u2) ?
?IBc](5)The next step is dualisation, which gives an upper bound of our problem:min(?BI,?IB)max(y1,...,yn,u1,u2)n?c=1?c(yc)?n?c=1D(yc) ?
?BIc+u1n?c=1?BIc?n?c=1F (yc) ?
?IBc+u2n?c=1?IBc(6)We then remark that?nc=1?BIcand?nc=1?IBcmust be zeros at optimum (if the problem is feasible).7It is convenient to convert this remark to hard constraints in order to remove any reference to vectors ui?
and therefore to the coupling contraints ?
from the objective.
We obtain the constrained problem withthe same optimal solution :(Du) : find min(?BI,?IB)n?c=1maxyc[?c(yc) ?
D(yc) ?
?BIc?
F (yc) ?
?IBc](7)s.t.n?c=1?BIc= 0 andn?c=1?IBc= 0 (8)In order to solve (Du) we use the projected subgradient descent method that has already been usedin many problems, for example MRF decoding (Komodakis et al., 2007).
For the problem at hand, thismethod gives Algorithm 2.
This iterative algorithm consists in reparametrising the elementary CRFs ofthe system, by modifying the weights associated with the bigram features in places that correspond tocompound frontiers, penalising them on CRFs that diverge from the average solution.
This is performed5One could object that some combinations are forbidden.
It remains that the number of labels still grows exponentially.6Bigram places are identified by their second position.7Otherwise the sum expressions would be unbounded and their maximum is +?
for an appropriate value of ui.1879by amending the vectors ?BIcand ?IBcthat are updated at each iteration proportionally to the differencebetween the feature vectors for c and the average values of these vectors.
Hence the farther a solutionis from the consensus, the more penalised it gets at the next iteration.
This algorithm stops when theupdates are null for all CRFs: in this case the consensus is reached.Algorithm 2 Best segmentation with combined CRF system via subgradient descentRequire: n CRF c = (?,Lc, wc, {fcp}p), an input sentence x, a maximum number of iterations ?
, stepsizes {?t}0?t?
?1: for all CRF c, bigram end position i, bigram label pair (l,m) do2: ?BIc[i, l,m](0)= 0; ?IBc[i, l,m](0)= 03: end for4: for t = 0?
?
do5: for all CRF c do6: yc(t)= V iterbi(Gc, wc, fc,?BI(t)c,?IB(t)c)7: end for8: for all CRF c do9: ?BI(t)c?
?t(D(yc(t))?
?1?d?nD(yd(t))n); ?IB(t)c?
?t(F(yc(t))?
?1?d?nF(yd(t))n)10: ?BI(t+1)c?
?BI(t)c+ ?BI(t)c; ?IB(t+1)c?
?IB(t)c+ ?IB(t)c11: end for12: if ?BI(t)c= 0 and ?IB(t)c= 0 for all c then13: Exit loop14: end if15: end for16: return (y1(t), ?
?
?
, yn(t))We set the maximum number of iteration ?
to 1000.
For the step size, we use a common heuristic:?t=11+kwhere k is the number of times that (Du) has increased between two successive iterations.4.3 Experimental results for CRF combinationsWe modified the wapiti software (Lavergne et al., 2010) with Algorithm 2.
Table 1 reports segmen-tation results on the development set with the different tag sets, the best DD combination, and the bestvoting system.8Tag Set CRF / combination Recall Precision F-scorepartial-internal 79.59 85.49 82.44partial 78.98 85.57 82.14basic 79.74 84.65 82.12complete 79.69 83.10 81.36complete-internal 79.03 82.66 80.80MWE basic complete partial-internal 80.82 86.07 83.36vote (basic complete partial-internal) 80.49 85.46 82.90Table 1: Segmentation scores of CRF systems (dev)System F-score (all) F-score (compounds)complete 94.29 78.32MWE 94.59 80.00Table 2: Segmentation + POS tagging (dev)We see that the best system is a combination of 3 CRFs (tag sets basic, complete and partial-internal)with DD, that we call MWE in the remaining of the paper.
The subgradient descent converges on allinstances in 2.14 iterations on average.
The DD combination is better than the voting system.We can also evaluate the POS tagging accuracy of the system for systems including the completetag set.
We compare the results of the complete CRF with the MWE combination on Table 2.
The secondcolumn gives the F-score of the complete task, segmentation and POS tagging.
The third column restrictsthe evaluation to compounds.
Again, the MWE combination outperforms the single system.8Each system has one vote and in case of a draw, we pick the best system?s decision.1880In some preliminary experiments, the weights of the CRF systems were based on unigram featuresmainly ?
i.e.
those described in (Constant et al., 2012).
As our CRFs are constrained on transitions fromB to I and I to B, penalising systems resulted in modifying (low) bigram weights and had only a minoreffect on the predictions and consequently the projected gradient algorithm was slow to converge.
Wetherefore added bigram templates for some selected unigram templates, so that our system can convergein a reasonable time.
Adding these bigram features resulted in slower elementary CRFs.
On average theenriched CRFs were 1.8 times slower that their preliminary counterparts.5 Dual Decomposition to combine parsers and sequence labellersWe now present an extension of the previous method to incorporate phrase-structure parsers in the com-bination.
Our approach relies on the following requirement for the systems to agree: if the parser predictsa compound between positions i and j, then the CRFs must predict compound frontiers at the same po-sitions.
In this definition, like in previous CRF combinations, only the positions are taken into account,not the grammatical categories.
From a parse tree a, we define two feature vectors:?
D(a), such that D(a)[i] = 1 if a contains a subtree for a compound starting at position i ?
1?
F (a), such that F (a)[i] = 1 if a contains a subtree for a compound ending at position i ?
1In other words, D(a)[i] indicates whether the CRFs should label position i ?
1 with B and position iwith I, while F (a)[i] indicates whether the CRFs should label position i ?
1 with I and position i withB.
See Figure 2 for an example.NPADJvol?eBNC+ADJbleueINCcarteBDETuneBFigure 2: Parser and CRF alignments (A stolen credit card)5.1 Parsing with probabilistic context-free grammarsWe follow the type of reasoning we used in ?
4.2.
With a PCFG g, we can define the score of a parse foran input sentence x as the logarithm of the probability assigned to this parse by g. Finding the best parsetakes a form analogous to the one in Equation 2, and we can write the CKY algorithm as a best pathalgorithm with penalties on nodes, as we did for the Viterbi algorithm previously.
This is closely relatedto the PCFG combinations of (Le Roux et al., 2013).
We introduce penalties through two real vectors?BIand ?IBindexed by compound positions.
The modified CKY is presented in Algorithm 39wherethe parse forest F is assumed to be already available and we note w the vector of rule log-probabilities.5.2 System combinationAs in ?
4.2, our problem amounts to finding a tuple that now consists of a parse tree and several labellings.All systems must agree on compound frontiers.
Our objective is:(P?)
: find max(a,y1,...,yn)?p(a) + ?n?c=1?c(yc) (9)s.t.
?u1, u2?c ?
J1, nK, D(yc) = u1, F (yc) = u2, D(a) = u1, F (a) = u2(10)9Without loss of generality, only binary rules are taken into account.1881Algorithm 3 CKY with node penalties for compound start/end positions1: CKY(F , w,?BI,?IB):2: for all node v in the forest F do3: ?
[v] = ?
?4: end for5: for all leaf node x do6: ?
[x] = 07: end for8: for all non-terminal node (A, i, j) in topological order do9: for all incoming hyperedge u = (B, i, k)(C, k + 1, j)?
(A, i, j) do10: s?
?
[(B, i, k)] + ?
[(C, k + 1, j)] + wA?BC?
wA?BCis the score for rule A?
BC11: if A is a compound label then12: s?
s?
?BI[i]?
?IB[j + 1]13: end if14: if s > ?
[(A, i, j)] then15: ?
[(A, i, j)]?
s16: end if17: end for18: end for19: return hyperpath with score ?
[(ROOT, 1, n)]We use ?
to set the relative weights of the CRFs and the PCFG.
It will be tuned on the develop-ment set.
We then reuse the same procedure as before: lagrangian relaxation, dualisation, and projectedsubgradient descent.
Algorithm 4 presents the function we derive from these operations.Algorithm 4 Find the best segmentation with a PCFG and CRFsRequire: a PCFG parser p, n CRFs, an input sentence x, a bound ?1: set Lagrange multipliers (penalty vectors) to zero2: for t = 0?
?
do3: for all CRF c do4: yc(t)?
V iterbi(Gc, wc, fc,?BI(t)c,?IB(t)c)5: end for6: a(t)?
CKY (F , w,?BI(t)p,?IB(t)p)7: for all CRF c and parser p do8: Update penalty vectors proportionally to the difference between corresponding solution and average solution9: end for10: if update is null for all c and p then11: Exit loop12: end if13: end for14: return (a(t), y1(t), ?
?
?
, yn(t))Algorithm 4 follows the method used in ?
4 and simply adds the PCFG parser as another subsystem.This method can then be extended further: for instance, we can add a POS tagger (Rush et al., 2010) ormultiple PCFG parsers (Le Roux et al., 2013).
Due to lack of space, we omit the presentation of thesesystems, but we experiment with them in the following section.6 ExperimentsFor this series of experiments, we used wapiti as in ?
4.3 and the LORG PCFG-LA parser in theconfiguration presented in (Le Roux et al., 2013) that we modified by implementing Algorithm 4.
Thisparser already implements a combination of parsers based on DD, a very competitive baseline.For parse evaluation, we used the EVALB tool, modified by the SPMRL 2013 organisers, in orderto compare our results with the shared task participants.
We evaluated several configurations: (i) theLORG parser alone, a combination of 4 PCFG-LA parsers as in (Le Roux et al., 2013), (ii) a pipelineof POS, a CRF-based POS tagger, and LORG, (iii) joint LORG and POS, using DD as in (Rush et al.,2010), (iv) joint LORG and MWE (our best CRF combination for compound segmentation) using DD, and(v) joint LORG, POS et MWE using DD.
We also compare these architectures with 2 additional pipelines,in which we first run MWE and then merge compounds as single tokens.
The converted sentences arethen sent to a version of LORG learnt on this type of corpus.
After parsing, compounds are unmerged,1882replaced with the corresponding subtree.
In one of these two architectures, we add a POS tagger.The evaluations for the parsing task of all these configurations are summarised in Table 3.
The bestsystem is the DD joint system combining the POS tagger, the parser and the compound recognisers.System Recall Precision Fscore EX TagLORG 82.01 82.37 82.19 18.06 97.35pipeline POS?
LORG 82.36 82.59 82.47 19.22 97.73DD POS + LORG 82.48 82.73 82.61 19.19 97.84DD MWE + LORG 82.91 83.07 82.99 19.19 97.41DD POS + MWE + LORG 83.38 83.42 83.40 20.73 97.85pipeline MWE MERGE?
LORG?
UNMERGE 82.56 82.63 82.59 18.79 97.39pipeline MWE MERGE/POS?
LORG?
UNMERGE 82.73 82.64 82.69 20.02 97.57Table 3: Parse evaluation on dev set (recall, precision and F-score, exactness and POS tagging).Table 4 shows evaluation results of our best system and comparisons with baseline or alternativeconfigurations on the SPMRL 2013 test set.Parsing The DD method performs better than our baseline, and better than the best system in theSPMRL 2013 shared task (Bj?rkelund et al., 2013).
This system is a pipeline consisting of amorpho-syntactic tagger with a very rich and informative tag set, a product of PCFG-LAs, anda parse reranker.
Although this approach is quite different from ours, we believe our system is moreaccurate overall because our method is more resilient to an error from one of its components.Compound recognition and labelling For the task of recognition alone, where only the frontiers areevaluated, the DD combinations of CRFs performs better than the best single CRF which itselfperforms better than the parser alone, but the complete architecture is again the best system.
If wealso evaluate compound POS tags, we get similar results.
The DD combination is always beneficial.System Recall Precision Fscore EX TagLORG 82.79 83.06 82.92 22.00 97.39(Bj?rkelund et al., 2013) ?
?
82.86 ?
?DD POS + MWE + LORG 83.74 83.85 83.80 23.81 97.87compound recognition LORG 78.03 78.63 78.49 ?
?compound recognition best single CRF (partial-internal) 78.27 82.84 80.49 ?
?compound recognition MWE 79.68 83.50 81.54 ?
?compound recognition DD POS + MWE + LORG 80.76 84.19 82.44 ?
?compound recognition + POS tagging LORG 75.43 75.71 75.57 ?
?compound recognition + POS tagging MWE 76.49 80.10 78.28 ?
?compound recognition + POS tagging DD POS + MWE + LORG 77.92 81.23 79.54 ?
?Table 4: Evaluation on SPMRL 2013 test set: parsing (first 3 lines), and compound recognition.7 ConclusionWe have presented an original architecture for the joint task of syntactic parsing and compound recogni-tion.
We first introduced a combination of recognisers based on linear chain CRFs, and a second systemthat adds in a phrase-structure parser.
Our experimental prototype improves state-of-the-art on the FrenchSPMRL corpus.In order to derive decoding algorithms for these joint systems, we used dual decomposition.
Thisapproach, leading to simple and efficient algorithms, can be extended further to incorporate additionalcomponents.
As opposed to pipeline approaches, a component prediction can be corrected if its solutionis too far from the general consensus.
As opposed to joint systems relying on pure dynamic programmingto build a complex single system, the search space does not grow exponentially, so we can avoid usingpruning heuristics such as beam search.
The price to pay is an iterative algorithm.Finally, this work paves the way towards component-based NLP software systems that perform com-plex processing based on consensus between components, as opposed to previous pipelined approaches.1883AcknowledgementsWe would like to thank Nadi Tomeh and Davide Buscaldi for their feedback on an early draft of thispaper, as well as the three anonymous reviewers for their helpful comments.
This work is supported bya public grant overseen by the French National Research Agency (ANR) as part of the Investissementsd?Avenir program (ANR-10-LABX-0083).ReferencesAnne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.
2003.
Building a treebank for French.
In Anne Abeill?,editor, Treebanks.
Kluwer, Dordrecht.Abhishek Arun and Frank Keller.
2005.
Lexicalization in crosslinguistic probabilistic parsing: The case of French.In Proceedings of the Annual Meeting of the Association For Computational Linguistics (ACL?05), pages 306?313.Anders Bj?rkelund, Rich?rd Farkas, Thomas M?ller, and Wolfgang Seeker.
2013.
(re) ranking meets morphosyn-tax: State-of-the-art results from the spmrl 2013 shared task.
In Proceedings of the 4th Workshop on StatisticalParsing of Morphologically Rich Languages: Shared Task.Matthieu Constant and Isabelle Tellier.
2012.
Evaluating the impact of external lexical resources into a CRF-basedmultiword segmenter and part-of-speech tagger.
In Proceedings of the 8th conference on Language Resourcesand Evaluation.Matthieu Constant, Anthony Sigogne, and Patrick Watrin.
2012.
Discriminative strategies to integrate multi-word expression recognition and parsing.
In Proceedings of the 50th Annual Meeting of the Association forComputational Linguistics (ACL?12), pages 204?212.Matthieu Constant, Marie Candito, and Djam?
Seddah.
2013a.
The LIGM-Alpage Architecture for the SPMRL2013 Shared Task: Multiword Expression Analysis and Dependency Parsing.
In Proceedings of the 4th Work-shop on Statistical Parsing of Morphologically Rich Languages: Shared Task, Seattle, WA.Matthieu Constant, Joseph Le Roux, and Anthony Sigogne.
2013b.
Combining compound recognition and PCFG-LA parsing with word lattices and conditional random fields.
ACM Transaction in Speech and Language Pro-cessing, 10(3).Blandine Courtois, Myl?ne Garrigues, Gaston Gross, Maurice Gross, Ren?
Jung, Michel Mathieu-Colas, AnneMonceaux, Anne Poncet-Montange, Max Silberztein, and Robert Viv?s.
1997.
Dictionnaire ?lectroniqueDELAC : les mots compos?s binaires.
Technical Report 56, University Paris 7, LADL.Jenny Rose Finkel and Christopher D. Manning.
2009.
Joint parsing and named entity recognition.
In Proceed-ings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of theAssociation for Computational Linguistics, NAACL ?09, pages 326?334, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Spence Green, Marie-Catherine de Marneffe, John Bauer, and Christopher D. Manning.
2011.
Multiword expres-sion identification with tree substitution grammars: A parsing tour de force with french.
In Proceedings of theconference on Empirical Method for Natural Language Processing (EMNLP?11), pages 725?735.Spence Green, Marie-Catherine de Marneffe, and Christopher D Manning.
2013.
Parsing models for identifyingmultiword expressions.
Computational Linguistics, 39(1):195?227.Nikos Komodakis, Nikos Paragios, and Georgios Tziritas.
2007.
MRF optimization via dual decomposition:Message-passing revisited.
In Computer Vision, 2007.
ICCV 2007.
IEEE 11th International Conference on,pages 1?8.
IEEE.Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag.
2010.
Dual decompositionfor parsing with non-projective head automata.
In Proceedings of the Conference on Empirical Methods inNatural Language Processing.John Lafferty, Andrew McCallum, and Fernando Pereira.
2001.
Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
In Proceedings of the Eighteenth International Conference onMachine Learning.1884Thomas Lavergne, Olivier Capp?, and Fran?ois Yvon.
2010.
Practical very large scale CRFs.
In Proceedings the48th Annual Meeting of the Association for Computational Linguistics (ACL?10), pages 504?513.Joseph Le Roux, Antoine Rozenknop, and Jennifer Foster.
2013.
Combining PCFG-LA models with dual de-composition: A case study with function labels and binarization.
In Association for Computational Linguistics,editor, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.
Associationfor Computational Linguistics, October.Slav Petrov.
2010.
Products of random latent variable grammars.
In Proceedings of the Annual Conference of theNorth American Chapter of the Association for Computational Linguistics - Human Language Technologies,pages 19?27.Odile Piton, Denis Maurel, and Claude Belleil.
1999.
The Prolex Data Base : Toponyms and gentiles forNLP.
In Proceedings of the Third International Workshop on Applications of Natural Language to Data Bases(NLDB?99), pages 233?237.Alexander Rush, David Sontag, Michael Collins, and Tommi Jaakola.
2010.
On dual decomposition and linearprogramming relaxations for natural language processing.
In ACL, editor, Proceedings of the Conference onEmpirical Methods in Natural Language Processing.
Association for Computational Linguistics.Beno?t Sagot.
2010.
The lefff, a freely available, accurate and large-coverage lexicon for french.
In Proceedingsof the 7th International Conference on Language Resources and Evaluation.Djam?
Seddah, Reut Tsarfaty, Sandra K?
?ubler, Marie Candito, Jinho Choi, Rich?rd Farkas, Jennifer Foster, IakesGoenaga, Koldo Gojenola, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann, Wolfgang Maier,Joakim Nivre, Adam Przepiorkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, Veronika Vincze, MarcinWoli?nski, Alina Wr?blewska, and Eric Villemonte de la Cl?rgerie.
2013.
Overview of the spmrl 2013 sharedtask: A cross-framework evaluation of parsing morphologically rich languages.
In Proceedings of the 4thWorkshop on Statistical Parsing of Morphologically Rich Languages, Seattle, WA.Veronica Vincze, Istv?n Nagy, and G?bor Berend.
2011.
Multiword expressions and named entities in the wiki50corpus.
In Proceedings of the conference on Recent Advances in Natural Language Processing (RANLP?11),pages 289?295.1885
