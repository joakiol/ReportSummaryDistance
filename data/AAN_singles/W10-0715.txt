Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 93?98,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsAn Enriched MT Grammar for Under $100Omar F. Zaidan and Juri GanitkevitchDept.
of Computer Science, Johns Hopkins UniversityBaltimore, MD 21218, USA{ozaidan,juri}@cs.jhu.eduAbstractWe propose a framework for improving out-put quality of machine translation systems, byoperating on the level of grammar rule fea-tures.
Our framework aims to give a boost togrammar rules that appear in the derivationsof translation candidates that are deemed tobe of good quality, hence making those rulesmore preferable by the system.
To that end, weask human annotators on Amazon MechanicalTurk to compare translation candidates, andthen interpret their preferences of one candi-date over another as an implicit preference forone derivation over another, and therefore asan implicit preference for one or more gram-mar rules.
Our framework also allows us togeneralize these preferences to grammar rulescorresponding to a previously unseen test set,namely rules for which no candidates havebeen judged.1 IntroductionWhen translating between two languages, state-of-the-art statistical machine translation sys-tems (Koehn et al, 2007; Li et al, 2009) generatecandidate translations by relying on a set of relevantgrammar (or phrase table) entries.
Each of thoseentries, or rules, associates a string in the sourcelanguage with a string in the target language, withthese associations typically learned by examininga large parallel bitext.
By the very nature of thetranslation process, a target side sentence e canbe a candidate translation for a source sentence fonly if e can be constructed using a small subsetof the grammar, namely the subset of rules withsource side sequences relevant to the word sequenceof f .
However, even this limited set of candidates(call it E(f)) is quite large, with |E(f)| growingexponentially in the length of f .
The system is ableto rank the translations within E(f) by assigning ascore s(e) to each candidate translation.
This scoreis the dot product:s(e) = ~?
(e) ?
~w (1)where ~?
(e) is a feature vector characterizing e, and~w is a system-specific weight vector characterizingthe system?s belief of how much the different fea-tures reflect translation quality.
The features of acandidate e are computed by examining the way e isconstructed (or derived), and so if we let d(e) be thederivation of e, the feature vector can be denoted:1~?
(d(e)) = ?
?1(d(e)), .
.
.
, ?m(d(e))?
(2)where ?i(d(e)) is the value of ith feature functionof d(e) (with a corresponding weight wi in ~w).To compute the score for a candidate, we examineits derivation d(e), enumerating the grammar rulesused to construct e: d(e) = (r1, .
.
.
, rk).
Typically,each of the rules will itself have a vector of m fea-tures, and we calculate the value of a derivation fea-ture ?i(d(e)) as the sum of the ith feature over allrules in the derivation:?i(d(e)) =?r?d(e)?i(r) (3)1There are other features computed directly, without ex-amining the derivation (e.g.
candidate length, language modelscore), but we omit these features from the motivation discus-sion for clarity.93These features are usually either relative frequen-cies estimated from the training corpus, relating therule?s source and target sides, or features that char-acterize the structure of the rule itself, independentlyfrom the corpus.Either way, the weightwi is chosen so as to reflectsome belief regarding the correlation between the ithfeature and translation quality.
This is usually doneby choosing weights that maximize performance ona tuning set separate from the training bitext.
Un-like system weights, the grammar rule feature val-ues are fixed once extracted, and are not modifiedduring this tuning phase.
In this paper, we propose aframework to augment the feature set to incorporateadditional intuition about how likely a rule is to pro-duce a translation preferred by a human annotator.This knowledge is acquired by directly asking hu-man judges to compare candidate translations, there-fore determining which subset of grammar rules an-notators seem to prefer over others.
We also seek togeneralize this intuition to rules for which no can-didates were judged, hence allowing us to impact amuch larger set of rules than just those used in trans-lating the tuning set.The paper is organized as follows.
We first givea general description of our framework.
We thendiscuss our data collection efforts on Amazon Me-chanical Turk for an Urdu-English translation task,and make explicit the type of judgments we col-lect and how they can be used to augment grammarrules.
Before concluding, we propose a frameworkfor generalizing judgments to unseen grammar rules,and analyze the data collection process.2 The General FrameworkAs initially mentioned, when tuning a SMT systemon a development set, we typically only performhigh-level optimization of the system weights.
Inthis section we outline an approach that could allowfor lower-level optimization, on the level of individ-ual grammar rules.We kick off the process by soliciting judgmentsfrom human annotators regarding the quality of asubset of candidates (the following section outlineshow candidates are chosen).
The resulting judg-ments on sentences are interpreted to be judgmentson individual grammar rules used in the derivationsof these candidates.
And so, if an annotator declaresa candidate to be of high quality, this is considereda vote of confidence on the individual rules givingrise to this candidate, and if an annotator declares acandidate to be of lowl quality, this is considered avote of no confidence on the individual rules.To make use of the collected judgments, we ex-tend the set of features used in the decoder by a newfeature ?:~??
= ?
?1, .
.
.
, ?m, ??
(4)This feature is the cornerstone of our framework,as it will hold the quantified and cumulated judg-ments for each rule, and will be used by the systemat decoding time, in addition to the existing m fea-tures, incorporating the annotators?
judgments intothe translation process.2The range of possible values for this feature, andhow the feature is computed, depends on how onechooses to ask annotators to score candidates, andwhat form those judgments assume (i.e.
are thosejudgments scores on a scale?
Are they ?better?vs.
?worse?
judgments, and if so, compared to howmany other possibilities?).
At this point, we willonly emphasize that the value of ?
should reflectthe annotators?
preference for the rule, and that itshould be computed from the collected judgments.We will propose one such method of computing ?
inSection 4, after describing the type of judgments wecollected.3 Data CollectionWe apply our approach to an Urdu-to-English trans-lation task.
We used a syntactically rich SAMTgrammar (Venugopal and Zollmann, 2006), whereeach rule in the grammar is characterized by 12 fea-tures.
The grammar was provided by Chris Callison-Burch (personal communication), and was extractedfrom a parallel corpus of 88k sentence pairs.3 Onesystem using this grammar produced significantlyimproved output over submissions to the NIST 2009Urdu-English task (Baker et al, 2009).We use the Joshua system (Li et al, 2009)as a decoder, with system weights tuned using2In fact, the collected judgments can only cover a small por-tion of the grammar.
We address this coverage problem in Sec-tion 4.3LDC catalog number LDC2009E12.94Z-MERT (Zaidan, 2009) on a tuning set of 981 sen-tences, a subset of the 2008 NIST Urdu-English testset.4 We choose candidates to be judged from the300-best candidate lists.5Asking a worker to make a quantitative judgmentof the quality of a particular candidate translation(e.g.
on a 1?7 scale) is a highly subjective andannotator-dependent process.
Instead, we presentworkers with pairs of candidates, and ask them tojudge which candidate is of better quality.How are candidate pairs chosen?
We would likea judgment to have the maximum potential for be-ing informative about specific grammar rules.
In es-sense, we prefer a pair of candidates if they havehighly similar derivations, yet differ noticeably interms of how the decoder ranks them.
In otherwords, if a relatively minimal change in derivationcauses a relatively large difference in the score as-signed by the decoder, we are likely to attribute thedifference to very few rule comparisons (or perhapsonly one), hence focusing the comparison on indi-vidual rules, all the while shielding the annotatorsfrom having to compare grammar rules directly.Specifically, each pair of candidates (e, e?)
is as-signed a potential score pi(e, e?
), defined as:pi(e, e?)
=s(e)s(e?)lev(d(e),d(e?
)), (5)where s(e) is the score assigned by the decoder,and lev(d,d?)
is a distance measure between twoderivations which we will now descibe in more de-tail.
In Joshua, the derivation of a candidate is cap-tured fully and exactly by a derivation tree, and sowe define lev(d,d?)
as a tree distance metric as fol-lows.
We first represent the trees as strings, using thefamiliar nested string representation, then computethe word-based Levenshtein edit distance betweenthe two strings.
An edit has a cost of 1 in general, butwe assign a cost of zero to edit operations on termi-nals, since we want to focus on the structure of thederivation trees, rather than on terminal-level lexi-cal choices.6 Furthermore, we ignore differences in4LDC catalog number LDC2009E11.5We exclude source sentences shorter than 4 words long orthat have fewer than 4 candidate translations.
This eliminatesroughly 6% of the development set.6This is not to say that lexical choices are not important, butlexical choice is heavily influenced by context, which is not cap-?pure?
pre-terminal rules, that only have terminalsas their right-hand side.
These decisions effectivelyallow us to focus our efforts on grammar rules withat least one nonterminal in their right-hand side.We perform the above potential computation onall pairs formed by the cross product of the top 10candidates and the top 300 candidates, and choosethe top five pairs ranked by potential.Our HIT template is rather simple.
Each HITscreen corresponds to a single source sentence,which is shown to the worker along with the fivechosen candidate pairs.
To aid workers who are notfluent in Urdu7 better judge translation quality, theHIT also displays one of the available referencesfor that source sentence.
To eliminate potential biasassociated with the order in which candidates arepresented (an annotator might be biased to choos-ing the first presented candidate, for example), wepresent the two candidates in random or- der.
Fur-thermore, for quality assurance, we embed a sixthcandidate pair to be judged, where we pair up a ran-domly chosen candidate with another reference forthat sentence.8 Presumably, a faithful worker wouldbe unlikely to prefer a random candidate over thereference, and so this functions as an embedded self-verification test.
The order of this test, relative to thefive original pairs, is chosen randomly.4 Incorporating the Judgements4.1 Judgement QuantificationThe judgments we obtain from the procedure de-scribed in the previous section relate pairs of can-didate translations.
However, we have defined theaccumulation feature ?
as a feature for each rule.Thus, in order to compute ?, we need to project thejudgments onto the rules that tell the two candidatesapart.
A simple way to do this is the following: fora judged candidate pair (e, e?)
let U(e) be the set oftured well by grammar rules.
Furthermore, lexical choice is aphenomenon already well captured by the score assigned to thecandidate by the language model, a feature typically includedwhen designing ~?.7We exclude workers from India and restrict the task toworkers with an existing approval rating of 90% or higher.8The tuning set contains at least three different human refer-ences for each source sentence, and so the reference ?candidate?shown to the worker is not the same as the sentence alreadyidentified as a reference.95rules that appear in d(e) but not in d(e?
), and viceversa.9 We will assume that the jugdment obtainedfor (e, e?)
applies for every rule pair in the cartesianproduct of U(e) and U(e?).
This expansion yieldsa set of judged grammar rule pairs J = {(a, b)}with associated vote counts va>b and vb>a, captur-ing how often the annotators preferred a candidatethat was set apart by a over a candidate containingb, and vice versa.So, following our prior definiton as an expressionof the judges?
preference, we can calculate the valueof ?
for a rule r as the relative frequency of favorablejudgements:?
(r) =?
(r,b)?J vr>b?
(r,b)?J vb>r + vr>b(6)4.2 Generalization to Unseen RulesThis approach has a substantial problem: ?, com-puted as given above, is undefined for a rule thatwas never judged (i.e.
a rule that never set aparta pair of candidates presented to the annotators).Furthermore, as described, the coverage of the col-lected judgments will be limited to a small subsetof the entire grammar, meaning that when the sys-tem is asked to translate a new source sentence, itis highly unlikely that the relevant grammar ruleswould have already been judged by an annotator.Therefore, it is necessary to generalize the collectedjudgments/votes and propagate them to previouslyunexamined rules.In order to do this, we propose the following gen-eral approach: when observing a judgment for a pairof rules (a, b) ?
J , we view that judgement not asa vote on one of them specifically, but rather as acomparison of rules similar to a versus rules similarto b.
When calculating ?
(r) for any rule r we usea distance measure over rules, ?, to estimate howeach judgment in J projects to r. This leads to thefollowing modified computatio of ?(r):?
(r) =?(a,b)?J?
(a, r)v?b>a + ?
(b, r)v?a>b?
(a, r) + ?
(b, r)(7)9The way we select candidate pairs ensures that U(e) andU(e?)
are both small and expressive in terms of impact on thedecoder ranking.
On our data U(e) contained an average of 4rules.where v?a>b (and analogously v?b>a) is defined as therelative frequency of a being preferred over b:v?a>b =va>bva>b + vb>a4.3 A Vector Space RealizationHaving presented a general framework for judgmentgeneralization, we will now briefly sketch a concreterealization of this approach.In order to be able to use the common distancemetrics on rules, we define a rule vector space.
Thebasis of this space will be a new set of rule featuresdesigned specifically for the purpose of describingthe structure of a rule, ~?
= ?
?1, .
.
.
, ?k?.
Providedthe exact features chosen are expressive and well-distributed over the grammar, we expect any con-ventional distance metric to correlate with rule sim-ilarity.We deem a particular ?i good if it quantifies aquality of the rule that describes the rule?s naturerather than the particular lexical choices it makes,i.e.
a statistic (such as the rule length, arity, numberof lexical items in the target or source side or the av-erage covered span in the training corpus), informa-tion relevant to the rule?s effect on a derivation (suchas nonterminals occuring in the rule and wheter theyare re-ordered) or features that capture frequent lex-ical cues that carry syntactic information (such asthe co-occurrence of function words in source andtarget language, possibly in conjunction with certainnonterminal types).5 Results and AnalysisThe judgments were collected over a period ofabout 12 days (Figure 1).
A total of 16,374 labelswere provided (2,729 embedded test labels + 13,645?true?
labels) by 658 distinct workers over 83.1 hours(i.e.
each worker completed an average of 4.2 HITsover 7.6 minutes).
The reward for each HIT was$0.02, with an additional $0.005 incurred for Ama-zon Fees.
Since each HIT provides five labels, weobtain 200 (true) labels on the dollar.
Each HITtook an average of 1.83 minutes to complete, for alabeling rate of 164 true labels/hour, and an effec-tive wage of $0.66/hour.
The low reward does notseem to have deterred Turkers from completing ourHITs faithfully, as the success rate on the embedded960 10 20 30 40 50 60 70 80 90 1001  2  3  4  5  6  7  8  9  10  11% Completed Time (Days)Figure 1: Progress of HIT submission over time.
Therewas a hiatus of about a month during which we collectedno data, which we are omitting for clairty.True Questions Validation QuestionsPreferred % Preferred %High-Ranked40.0% Reference 83.7%Low-Ranked24.1% RandomCandidate11.7%NoDifference35.9% NoDifference4.65%Table 1: Distributions of the collected judgments overthe true questions and over the embedded test questions.?High-Ranked?
(resp.
?Low-Ranked?)
refers to whetherthe decoder assigned a high (low) score to the candidate.And so, annotators agreed with the decoder 40.0% of thetime, and disagreed 24.1% of the time.questions was quite high (Table 1).10 From our setof comparatively judged candidate translations weextracted competing rule pairs.
To reduce the in-fluence of lexical choices and improve comparabil-ity, we excluded pure preterminal rules and limitedthe extraction to rules covering the same span in theUrdu source.
Figure 3 shows an interesting exampleof one such rule pair.
While the decoder demon-strates a clear preference for rule (a) (including itinto its higher-ranked translation 100% of the time),the Turkers tend to prefer translations generated us-ing rule (b), disagreeing with the SMT system 60%of the time.
This indicates that preferring the secondrule in decoding may yield better results in terms ofhuman judgment, in this case potentially due to the10It should be mentioned that the human references them-selves are of relatively low quality.0510152025301 2 3 4 5 6 7 8 9 10Candidate Rank%TimeChosenforComparisonFigure 2: Histogram of the rank of the higher-rankedcandidate chosen in pair comparisons.
For instance, inabout 29% of chosen pairs, the higher-ranked candidatewas the top candidate (of 300) by decoder score.
(a)  [NP] ! "
[NP] [NN+IN] !"
# the [NN+IN] [NP] $(b)  [NP] ! "
[NP] !"
[NN] !"
# [NN] of [NP] $Figure 3: A example pair of rules for which judgementswere obtained.
The first rule is preferred by the decoder,while human annotators favor the second rule.cleaner separation of noun phrases from the prepo-sitional phrase.We also examine the distribution of the chosencandidates.
Recall that each pair consists of a high-ranked candidate from the top-ten list, and a low-ranked candidate from the top-300 list.
The His-togram of the higher rank (Figure 2) shows that thehigh-ranked candidate is in fact a top-three candi-date over 50% of the time.
We also see (Figure 4)that the low-ranked candidate tends to be either closein rank to the top-ten list, or far away.
This againmakes sense given our definition of potential for apair: potential is high if the derivations are veryclose (left mode) or if the decoder scores differ con-siderably (right mode).Finally, we examine inter-annotator agreement,since we collect multiple judgments per query.
Wefind that there is full agreement among the anno-tators in 20.6% of queries.
That is, in 20.6% ofqueries, all three annotators answering that querygave the same answer (out of the three providedanswers).
This complete agreement rate is signif-icantly higher than a rate caused by pure chance(11.5%).
This is a positive result, especially given97051015201-2021-4041-6061-8081-100101-120121-140141-160161-180181-200201-220221-240241-260261-280281-300Candidate Rank%TimeChosenforComparisonFigure 4: Histogram of the rank of the lower-ranked can-didate chosen in pair comparisons.
For instance, in about16% of chosen candidate pairs, the lower-ranked candi-date was ranked in the top 20.how little diversity usually exists in n-best lists,a fact (purposely) exacerbated by our strategy ofchoosing highly similar pairs of candidates.
On theother hand, we observe complete disagreement inonly 14.9% of queries, which is significantly lowerthan a rate caused by pure chance (which is 22.2%).One thing to note is that these percentages arecalculated after excluding the validation questions,where the complete agreement rate is an expectedlyeven higher 64.9%, and the complete disagreementrate is an expectedly even lower 3.60%.6 Conclusions and OutlookWe presented a framework that allows us to ?tune?MT systems on a finer level than system-level fea-ture weights, going instead to the grammar rule leveland augmenting the feature set to reflect collectedhuman judgments.
A system relying on this new fea-ture during decoding is expected to have a slightlydifferent ranking of translation candidates that takeshuman judgment into account.
We presented oneparticular judgment collection procedure that relieson comparing candidate pairs (as opposed to eval-uating a candidate in isolation) and complementedit with one possible method of propagating humanjudgments to cover grammar rules relevant to newsentences.While the presented statistics over the collecteddata suggest that the proposed candidate selectionprocedure yields consistent and potentially informa-tive data, the quantitative effects on a machine trans-lation system remain to be seen.Additionally, introducing ?
as a new featuremakes it necessary to find a viable weight for it.While this can be done trivially in running MERTon arbitrary development data, it may be of interestto extend the weight optimization procedure in or-der to preserve the partial ordering induced by thejudgments as best as possible.AcknowledgmentsThis research was supported by the EuroMatrix-Plus project funded by the European Commission,by the DARPA GALE program under Contract No.HR0011-06-2-0001, and the NSF under grant IIS-0713448.ReferencesKathy Baker, Steven Bethard, Michael Bloodgood, RalfBrown, Chris Callison-Burch, Glen Coppersmith,Bonnie Dorr, Wes Filardo, Kendall Giles, Anni Irvine,Mike Kayser, Lori Levin, Justin Martineau, Jim May-field, Scott Miller, Aaron Phillips, Andrew Philpot,Christine Piatko, Lane Schwartz, and David Zajic.2009.
Semantically informed machine translation(SIMT).
In SCALE 2009 Summer Workshop Final Re-port, pages 135?139.Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,Chris Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Chris Dyer, Ondr?ej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machinetranslation.
In Proc.
of ACL, Demonstration Session,pages 177?180, June.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-itkevitch, Sanjeev Khudanpur, Lane Schwartz, WrenN.
G. Thornton, Jonathan Weese, and Omar F. Zaidan.2009.
Joshua: An open source toolkit for parsing-based machine translation.
In Proc.
of the FourthWorkshop on Statistical Machine Translation, pages135?139.Ashish Venugopal and Andreas Zollmann.
2006.
Syn-tax augmented machine translation via chart parsing.In Proc.
of the NAACL 2006 Workshop on StatisticalMachine Translation, pages 138?141.
Association forComputational Linguistics.Omar F. Zaidan.
2009.
Z-MERT: A fully configurableopen source tool for minimum error rate training ofmachine translation systems.
The Prague Bulletin ofMathematical Linguistics, 91:79?88.98
