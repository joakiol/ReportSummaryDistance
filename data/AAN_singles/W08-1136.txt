Referring Expression Generation Challenge 2008 DIT System DescriptionsJ.D.
KelleherSchool of ComputingDublin Institute of Technologyjohn.kelleher@comp.dit.ieB.
Mac NameeSchool of ComputingDublin Institute of Technologybrian.macnamee@comp.dit.ie1 Task 1: Attribute SelectionThis section describes the two systems developed atDIT for the attribute selection track of the REG 2008challenge.
Both of theses systems use an incremen-tal greedy search to generate descriptions, similarto the incremental algorithm described in (Dale andReiter, 1995).
The output of these incremental algo-rithms are, to a large extent, determined by the orderin which the algorithm tests the target object?s at-tributes for inclusion in the description.
Indeed, themajor difference between the two systems describedin this section is the mechanism used to order theattributes for inclusion.1.1 DIT-FBI SystemThe DIT-FBI system selects the next attribute to betested for inclusion in the description by orderingeach attribute based on its frequency in the subset ofthe training corpus that is defined by the test trial?sdomain (i.e., furniture versus people) and condition(+LOC versus -LOC).
Attributes are selected in de-scending order of frequency (i.e.
the attribute thatoccurred most frequently in the relevant subset ofthe training corpus is selected first).
Where two ormore attributes have the same frequency of occur-rence the first attribute found with that frequency isselected.
The type attribute is always included in thedescription.
Other attributes are included in the de-scription if they exclude at least 1 distractor from theset of distractors that fulfil the description generatedprior to that attribute?s selection.The mapping from qualitative linguistics descrip-tions, such as middle or centre, to the TUNA cor-pus?
quantitative location attribute values, i.e, x-dimension and y-dimension, can result in both the x-dimension and y-dimension attributes being includedin the target set.
These cases, where both the dimen-sional attributes are required, are difficult to capturebecause each of the dimensional attributes would besufficiently discriminative to result in a distinguish-ing description simply by their lone inclusion.
As aresult, a rule was put in place whereby if we haveincluded either of the dimensional attributes we in-clude the other dimensional attribute if the includedone refers to the center of the display (i.e., x=3,y=2).The algorithm terminates when a distinguishingdescription has been generated (i.e., all the distrac-tors have been excluded) or when all of the target?sattributes have been tested for inclusion in the de-scription.
Table 1 lists the results for the system.Furniture People BothDice 0.816 0.702 0.763MASI 0.606 .452 0.535Accuracy 37 17 54Minimality 80 68 148Uniqueness 0 0 0Table 1: Results for furniture, people and both domains.1.2 DIT-TVAS SystemIn the DIT-TVAS system the selection of the nexttarget attribute to test for inclusion in the descriptionis based on the prior probability of a target?s attributewith a particular value being used to describe thetarget, given that the target has a particular attributewith that particular value.
This prior is computedby counting the number of trials in the training cor-pus where the target description included a partic-221ular attribute-value pair and dividing this count bythe number of trials in the training corpus where thetarget?s properties listed a particular attribute-valuepair.For example, there are 143 trials in the Reg-08-Challenge training corpus where the target?s type at-tribute had the value chair, and in 134 of these tri-als the description of the target included the attributevalue pair type-chair.
As a result, the atribute-valuepair type-chair has a prior probability of being usedto describe the target, given that the target propertiescontain this attribute-value combination, of 134143?0.937.
Table 2 lists the priors for each attribute-value combination in the furniture corpus and Table3 lists the priors for each attribute-value combina-tion in the people corpus (for space reasons thesetables do not include the priors for the x-dimension,y-dimension and other attributes-value pairs).
Giventhese tables and a test trial, the next attribute-valuepair to be tested for inclusion in the description ofthe test target is the attibute-value with the highestprior that has not already been tested for inclusionand that the target object fulfils.As is evident from Table 2 and Table 3, there isno significant difference between the priors of someattribute-value pairs.
For example, in the furnituredomain orientation=left and hasShirt=true have pri-ors of 0.023 and 0.022 respectively.
In order toavoid situations where a non-significant differencein priors unduly biases the system toward the inclu-sion of a particular attribute-value pair, each timean attribute-vaule pair has been selected for testingthe DIT-TVAS system checks whether there are anyother attribute-value pairs that have not been previ-ously tested for inclusion in the description of thetarget and whose prior is within 5% of the prior ofthe attribute-value that has been selected for testing.In cases where this test returns one or more attribute-value pairs, the system uses the attribute-value pairwhose inclusion would exclude the most amount ofdistractors.
Finally, if there is a tie between one ormore attribute-value pairs with respect to distractorexclusion this is resolved by slecting the attribute-value pair with the highest prior.
Table 4 lists theresults for the system.Attribute VALUE Sel Occur PriorTYPE fan 41 42 0.976TYPE chair 134 143 0.937TYPE sofa 43 48 0.896COLOUR green 35 40 0.875COLOUR blue 75 86 0.872COLOUR red 82 96 0.854TYPE desk 73 86 0.849COLOUR grey 81 97 0.835ORIENTATION back 25 51 0.490SIZE small 56 130 0.431ORIENTATION front 31 86 0.360SIZE large 61 189 0.324ORIENTATION left 22 86 0.256ORIENTATION right 28 96 0.292Table 2: Prior?s for each attribute-value pair in the furni-ture domain.
Sel: how often an attribute-value pair wasincluded in a description; Occur: how often an attribute-vaule pair appeared in targets in the training corpus.Prior?s listed to three decimal places.Attribute VALUE Sel Occur PriorTYPE person 225 274 0.821hasBeard true 123 181 0.680hasGlasses true 117 184 0.636hasSuit true 4 94 0.43hasHair true 36 233 0.155hasHair false 6 41 0.146AGE old 15 132 0.114ORIENTATION right 2 44 0.045ORIENTATION front 4 143 0.028ORIENTATION left 2 87 0.023hasShirt true 3 136 0.022hasTie true 2 94 0.021AGE young 2 142 0.014hasShirt false 0 138 0hasBeard false 0 93 0hasGlasses false 0 90 0hasTie false 0 180 0hasSuit false 0 180 0Table 3: Prior?s for each attribute-value pair in the furni-ture domain.
Sel: how often an attribute-value pair wasincluded in a description; Occur: how often an attribute-vaule pair appeared in targets in the training corpus.Prior?s listed to three decimal places.222Furniture People BothDice 0.778 0.709 0.746MASI 0.540 .426 0.488Accuracy 33 15 48Uniqueness 80 68 148Minimality 0 0 0Table 4: Results for furniture, people and both domains.2 Task 2: RealisationThis section describes the two systems developed atDIT for the realisation track of the REG 2008 chal-lenge.
The DIT-CBSR system, Section 2.1, uses acase-based reasoning approach to realization, which(Daelemans and van den Bosch, 2005) have recentlyargued is an appropriate machine learning approachto natural language processing.
The DIT-RBR sys-tem, Section 2.2, uses a set of hand-crafted domain-specific rules to generate descriptions.2.1 DIT-CBSR SystemCased-Based Reasoning attempts to use a historyof past problems and their solutions to solve newlyarising problems.
The solution to a new problem isgenerated by finding the problem in the set of train-ing problems the system has previously seen (i.e.
thecase base) which most closely matches it and adapt-ing its solution.The DIT-CBSR system uses a relatively simplecase matching algorithm.
When a new trial requiressentence generation it?s attribute set is matchedagainst all of the cases in the training set to deter-mine which cases is matches most closely.
Thismatching firstly considers only attributes and theirvalues.
There are three kinds of matches that canarise from this process:?
Perfect match: the attributes used in both thequery case and the case from the case-basematch perfectly as do their values.?
Partial match: the attribute used by both thequery case and the case from the case-basematch perfectly, but the attribute values do notmatch.?
No match: no member of the case-base has alist of attributes used that match those requiredby the query case.Slightly different actions are taken depending onthe type of match achieved.
These are as follows.Perfect Match Perfect matches are the easiest todeal with as little effort is required in order to pro-duce a useful sentence.
In fact, if only one per-fect match is found then that trial?s word string isused, unedited, as the generated sentence.
However,things become a little more interesting if more thanone case in the case-base matches the query caseperfectly (remembering that the match is only basedon the attributes used and their values).
In this case,the list of matches is first trimmed of any cases thatare not based on the same image as the query case,as long as this does not remove all cases.
If thisdoes remove all cases we revert to the original set ofmatching cases.
In either instance, the word stringsin the set of remaining matching cases are consid-ered to determine if there are any duplicates.
If thereare, the word string that appears most frequently isused as the generated sentence.
If there are no dupli-cates, the shortest word string in the set is chosen.Partial Match Partial matches occur when thereis no example in the case-base for which all of theattribute values are the same as those of the querycase.
However, there are some case whose attributesmatch the query, but whose attribute values are dif-ferent.
This set of cases is sub-divided based on thenumber of attribute values in each case that matchthose in the query case.
This results in a set of casesthat share the highest match score.
From this set alof those cases that are not based on the same im-age as the query case are removed, as long as thisdoes not completely empty the set.
If this trimmingwould completely empty the set it is not performed.The trial with the shortest word string from the set ofremaining candidate matches is selected.
The wordstring associated with the selected trial needs to bemodified to account for the disparity between its at-tribute values and those of the query case.
This mod-ification is done by replacing all substrings in theselected case?s word string that arise from attributevalues not matching those in the query case with thesubstring that is most commonly associated in thetraining corpus with the query case?s attribute value.All of these substrings are identified using the anno-tated word string element present in each trial.223No Match When no match is found a simple rule-based realiser is used to construct a sentence match-ing the attribute value set of the query case.
Therules used by the realiser are based on the most com-mon strings found in the corpus for each attributevalue pair.Table 5 lists the results for the system.Furniture People BothString-edit distance 3.95 4.81 4.34Accuracy score 12 6 18Table 5: Results for furniture, people and both domains.2.2 DIT-RBR SystemIn Section 2.1 we noted that if no match was found inthe case-base a simple rule-based realiser was used.This rule-based realiser, the DIT-RBR system, uses asequence of IF-THEN rules based on a study of thefrequencies and order of the phrases used to realisespecific attribute-value pairs in the training corpus.Theses phrase are easily extracted from the anno-tated word-string xml element.
The great advantageof this algorithm is that it always able to return astring given an input.
However, the rule-set is spe-cific to the task and would not generalise as well asthe DIT-CBSR system.
Due to space restrictions wedo not list the rules used by the system.
Table 6 liststhe results for the system.Furniture People BothString edit distance 3.613 4.132 3.851Accuracy 11 3 14Table 6: Results for furniture, people and both domains.3 Task 3: Referring ExpressionGenerationThis section describes describes our approach to task3 of REG Challenge 2008.
Each of the systems de-scribed in this section simply chains together DITsolutions to task 1 and task 2.3.1 DIT-FBI-CBSR SystemThe DIT-FBI-CBSR system chains together the DIT-FBI attribute selection system, described in 1.1, andthe DIT-CBSR system, described in Section 2.1.
Ta-ble 7 lists the results for the system.Furniture People BothString-edit distance 4.45 5.162 4.777Accuracy score 7 1 8Table 7: Results for furniture, people and both domains.3.2 DIT-TVAS-RBR Task 3 SystemThe DIT-TVAS-RBR system chains together theDIT-TVAS attribute selection system, described inSection 1.2, and the DIT-RBR realiser, described inSection 2.2.
Table 8 lists the results for the system.Furniture People BothString-edit distance 4.725 5.178 4.905Accuracy score 4 0 4Table 8: Results for furniture, people and both domains.ReferencesWalter Daelemans and Antal van den Bosch.
2005.Memory-Based Language Processing.
CambridgeUniversity Press.R.
Dale and E. Reiter.
1995.
Computatinal interpreta-tions of the gricean maxims in the generation of refer-ring expressions.
Cognitive Science, 18:233?263.224
