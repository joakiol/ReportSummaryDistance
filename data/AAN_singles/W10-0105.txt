Proceedings of the NAACL HLT 2010 Workshop on Active Learning for Natural Language Processing, pages 33?41,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsParallel Active Learning: Eliminating Wait Time with Minimal StalenessRobbie Haertel, Paul Felt, Eric Ringger, Kevin SeppiDepartment of Computer ScienceBrigham Young UniversityProvo, Utah 84602, USArah67@cs.byu.edu, pablofelt@gmail.com,ringger@cs.byu.edu, kseppi@cs.byu.eduhttp://nlp.cs.byu.edu/AbstractA practical concern for Active Learning (AL)is the amount of time human experts must waitfor the next instance to label.
We propose amethod for eliminating this wait time inde-pendent of specific learning and scoring al-gorithms by making scores always availablefor all instances, using old (stale) scores whennecessary.
The time during which the ex-pert is annotating is used to train models andscore instances?in parallel?to maximize therecency of the scores.
Our method can be seenas a parameterless, dynamic batch AL algo-rithm.
We analyze the amount of stalenessintroduced by various AL schemes and thenexamine the effect of the staleness on perfor-mance on a part-of-speech tagging task on theWall Street Journal.
Empirically, the parallelAL algorithm effectively has a batch size ofone and a large candidate set size but elimi-nates the time an annotator would have to waitfor a similarly parameterized batch scheme toselect instances.
The exact performance of ourmethod on other tasks will depend on the rel-ative ratios of time spent annotating, training,and scoring, but in general we expect our pa-rameterless method to perform favorably com-pared to batch when accounting for wait time.1 IntroductionRecent emphasis has been placed on evaluating theeffectiveness of active learning (AL) based on re-alistic cost estimates (Haertel et al, 2008; Settleset al, 2008; Arora et al, 2009).
However, to ourknowledge, no previous work has included in thecost measure the amount of time that an expert an-notator must wait for the active learner to provide in-stances.
In fact, according to the standard approachto cost measurement, there is no reason not to use thetheoretically optimal (w.r.t.
a model, training proce-dure, and utility function) (but intractable) approach(see Haertel et al, 2008).In order to more fairly compare complex andtime-consuming (but presumably superior) selec-tion algorithms with simpler (but presumably in-ferior) algorithms, we describe ?best-case?
(mini-mum, from the standpoint of the payer) and ?worst-case?
(maximum) cost scenarios for each algorithm.In the best-case cost scenario, annotators are paidonly for the time they spend actively annotating.
Theworst-case cost scenario additionally assumes thatannotators are always on-the-clock, either annotat-ing or waiting for the AL framework to provide themwith instances.
In reality, human annotators work ona schedule and are not always annotating or waiting,but in general they expect to be paid for the timethey spend waiting for the next instance.
In somecases, the annotator is not paid directly for wait-ing, but there are always opportunity costs associ-ated with time-consuming algorithms, such as timeto complete a project.
In reality, the true cost usuallylies between the two extremes.However, simply analyzing only the best-casecost, as is the current practice, can be misleading,as illustrated in Figure 1.
When excluding waitingtime for a particular selection algorithm1 (?AL An-notation Cost Only?
), the performance is much bet-1We use the ROI-based scoring algorithm (Haertel et al,2008) and the zero-staleness technique, both described below.330.780.80.820.840.860.880.90.920.940.960.98100000  200000  300000  400000  500000  600000  700000  800000  900000  1e+06AccuracyTime in SecondsAL Annotation Cost OnlyRandom Total CostAL Total CostFigure 1: Accuracy as a function of cost (time).Side-by-side comparison of best-case and worst-case cost measurement scenarios reveals that not ac-counting for the time required by AL to select in-stances affects the evaluation of an AL algorithm.ter than the cost of random selection (?Random TotalCost?
), but once waiting time is accounted for (?ALTotal cost?
), the AL approach can be worse than ran-dom.
Given only the best-case cost, this algorithmwould appear to be very desirable.
Yet, practition-ers would be much less inclined to adopt this al-gorithm knowing that the worst-case cost is poten-tially no better than random.
In a sense, waiting timeserves as a natural penalty for expensive selectionalgorithms.
Therefore, conclusions about the use-fulness of AL selection algorithms should take bothbest-case and worst-case costs into consideration.Although it is current practice to measure onlybest-case costs, Tomanek et al (2007) mention as adesideratum for practical AL algorithms the need forwhat they call fast selection time cycles, i.e., algo-rithms that minimize the amount of time annotatorswait for instances.
They address this by employingthe batch selection technique of Engleson and Da-gan (1996).
In fact, most AL practitioners and re-searchers implicitly acknowledge the importance ofwait time by employing batch selection.However, batch selection is not a perfect solution.First, using the tradtional implementation, a ?good?batch size must be specified beforehand.
In research,it is easy to try multiple batch sizes, but in practicewhere there is only one chance with live annotators,specifying a batch size is a much more difficult prob-lem; ideally, the batch size would be set during theprocess of AL.
Second, traditional methods use thesame batch size throughout the entire learning pro-cess.
However, in the beginning stages of AL, mod-els have access to very little training data and re-training is often much less costly (in terms of time)than in the latter stages of AL in which models aretrained on large amounts of data.
Intuitively, smallbatch sizes are acceptable in the beginning stages,whereas large batch sizes are desirable in the latterstages in order to mitigate the time cost of training.In fact, Haertel et al (2008) mention the use of anincreasing batch size to speed up their simulations,but details are scant and the choice of parameters fortheir approach is task- and dataset-dependent.
Also,the use of batch AL causes instances to be chosenwithout the benefit of all of the most recently anno-tated instances, a phenomenon we call staleness andformally define in Section 2.
Finally, in batch AL,the computer is left idle while the annotator is work-ing and vice-verse.We present a parallel, parameterless solution thatcan eliminate wait time irrespective of the scoringalogrithm and training method.
Our approach isbased on the observation that instances can alwaysbe available for annotation if we are willing to serveinstances that may have been selected without thebenefit of the most recent annotations.
By havingthe computer learner do work while the annotator isbusy annotating, we are able to mitigate the effectsof using these older annotations.The rest of this paper will proceed as follows:Section 2 defines staleness and presents a progres-sion of four AL algorithms that strike different bal-ances between staleness and wait time, culminat-ing in our parallelized algorithm.
We explain ourmethodology and experimental parameters in Sec-tion 3 and then present experimental results andcompare the four AL algorithms in Section 4.
Con-clusions and future work are presented in Section 5.2 From Zero Staleness to Zero WaitWe work within a pool- and score-based AL settingin which the active learner selects the next instancefrom an unlabeled pool of data U .
A scoring func-tion ?
(aka scorer) assigns instances a score usinga model ?
trained on the labeled data A; the scoresserve to rank the instances.
Lastly, we assume that34Input: A seed set of annotated instances A, a set ofpairs of unannotated instances and theirinitial scores S, scoring function ?, thecandidate set size N , and the batch size BResult: A is updated with the instances chosen bythe AL process as annotated by the oraclewhile S 6= ?
do1?
?
TrainModel(A)2stamp?
|A|3C ?
ChooseCandidates(S,N)4K ?
{(c[inst], ?
(c[inst], ?))
| c ?
C}5S ?
S ?
C ?K6T ?
pairs from K with c[score] in the top B7scoresfor t ?
T do8S ?
S ?
t9staleness?
|A| ?
stamp ; // unused10A ?
A?
Annotate(t)11end12end13Algorithm 1: Pool- and score-based active learner.an unerring oracle provides the annotations.
Theseconcepts are demonstrated in Algorithm 1.In this section, we explore the trade-off betweenstaleness and wait time.
In order to do so, it is bene-ficial to quantitatively define staleness, which we doin the context of Algorithm 1.
After each model ?is trained, a stamp is associated with that ?
that indi-cates the number of annotated instances used to trainit (see line 3).
The staleness of an item is definedto be the difference between the current number ofitems in the annotated set and the stamp of the scorerthat assigned the instance a score.
This concept canbe applied to any instance, but it is particularly in-formative to speak of the staleness of instances atthe time they are actually annotated (we will simplyrefer to this as staleness, disambiguating when nec-essary; see line 10).
Intuitively, an AL scheme thatchooses instances having less stale scores will tendto produce a more accurate ranking of instances.2.1 Zero StalenessThere is a natural trade-off between staleness andthe amount of time an annotator must wait for aninstance.
Consider Algorithm 1 when B = 1 andN = ?
(we refer to this parameterization as ze-rostale).
In line 8, a single instance is selected forannotation (|T | = B = 1); the staleness of this in-stance is zero since no other annotations were pro-vided between the time it was scored and the time itwas removed.
Therefore, this algorithm will neverselect stale instances and is the only way to guaran-tee that no selected instances are stale.However, the zero staleness property comes witha price.
Between every instance served to the an-notator, a new model must be trained and every in-stance scored using this model, inducing potentiallylarge waiting periods.
Therefore, the following op-tions exist for reducing the wait time:1.
Optimize the learner and scoring function (in-cluding possible parallelization)2.
Use a different learner or scoring function3.
Parallelize the scoring process4.
Allow for stalenessThe first two options are specific to the learning andscoring algorithms, whereas we are interested in re-ducing wait time independent of these in the generalAL framework.
We describe option 3 in section 2.4;however, it is important to note that when train-ing time dominates scoring, the reduction in waitingtime will be minimal with this option.
This is typi-cally the case in the latter stages of AL when modelsare trained on larger amounts of data.We therefore turn our attention to option 4: in thiscontext, there are at least three ways to decrease thewait time: (A) train less often, (B) score fewer items,or (C) allow old scores to be used when newer onesare unavailable.
Strategies A and B are the batch se-lection scheme of Engelson and Dagan (1996); analgorithm that allows for these is presented as Al-gorithm 1, which we refer to as ?traditional?
batch,or simply batch.
We address the traditional batchstrategy first and then address strategy C.2.2 Traditional BatchIn order to train fewer models, Algorithm 1 can pro-vide the annotator with several instances scored us-ing the same scorer (controlled by parameter B);consequently, staleness is introduced.
The first itemannotated on line 11 has zero staleness, having beenscored using a scorer trained on all available anno-tated instances.
However, since a model is not re-trained before the next item is sent to the annotator,35the next items have staleness 1, 2, ?
?
?
, B?1.
By in-troducing this staleness, the time the annotator mustwait is amortized across allB instances in the batch,reducing the wait time by approximately a factor ofB.
The exact effect of staleness on the quality ofinstances selected is scorer- and data-dependent.The parameter N , which we call the candidate setsize, specifies the number of instances to score.
Typ-ically, candidates are chosen in round-robin fash-ion or with uniform probability (without replace-ment) from U .
If scoring is expensive (e.g., if itinvolves parsing, translating, summarizing, or someother time-consuming task), then reducing the can-didate set size will reduce the amount of time spentscoring by the same factor.
Interestingly, this param-eter does not affect staleness; instead, it affects theprobability of choosing the same B items to includein the batch when compared to scoring all items.Intuitively, it affects the probability of choosing B?good?
items.
As N approaches B, this probabil-ity approaches uniform random and performance ap-proaches that of random selection.2.3 Allowing Old ScoresOne interesting property of Algorithm 1 is that line 7guarantees that the only items included in a batch arethose that have been scored in line 5.
However, if thecandidate set size is small (because scoring is expen-sive), we could compensate by reusing scores fromprevious iterations when choosing the best items.Specifically, we change line 7 to instead be:T ?
pairs from S with c[score] in the top B scoresWe call this allowold, and to our knowledge, it is anovel approach.
Because selected items may havebeen scored many ?batches?
ago, the expected stale-ness will never be less than in batch.
However, ifscores do not change much from iteration to itera-tion, then old scores will be good approximationsof the actual score and therefore not all items nec-essarily need to be rescored every iteration.
Con-sequently, we would expect the quality of instancesselected to approach that of zerostale with less wait-ing time.
It is important to note that, unlike batch,the candidate set size does directly affect staleness;smaller N will increase the likelihood of selectingan instance scored with an old model.2.4 Eliminating Wait TimeThere are portions of Algorithm 1 that are triviallyparallelizable.
For instance, we could easily split thecandidate set into equal-sized portions across P pro-cessors to be scored (see line 5).
Furthermore, it isnot necessary to wait for the scorer to finish trainingbefore selecting the candidates.
And, as previouslymentioned, it is possible to use parallelized trainingand/or scoring algorithms.
Clearly, wait time willdecrease as the speed and number of processors in-crease.
However, we are interested in parallelizationthat can guarantee zero wait time independent of thetraining and scoring algorithms without precludingthese other forms of parallelization.All other major operations of Algorithm 1 haveserial dependencies, namely, we cannot score untilwe have trained the model and chosen the candi-dates, we cannot select the instances for the batchuntil the candidate set is scored, and we cannot startannotating until the batch is prepared.
These depen-dencies ultimately lead to waiting.The key to eliminating this wait time is to ensurethat all instances have scores at all times, as in al-lowold.
In this way, the instance that currently hasthe highest score can be served to the annotator with-out having to wait for any training or scoring.
Ifthe scored instances are stored in a priority queuewith a constant time extract-max operation (e.g., asorted list), then the wait time will be negligible.Even a heap (e.g., binary or Fibonacci) will oftenprovide negligible overhead.
Of course, eliminatingwait time comes at the expense of added staleness asexplained in the context of allowold.This additional staleness can be reduced by allow-ing the computer to do work while the oracle is busyannotating.
If models can retrain and score most in-stances in the amount of time it takes the oracle toannotate an item, then there will be little staleness.2Rather than waiting for training to complete be-fore beginning to score instances, the old scorer canbe used until a new one is available.
This allowsus to train models and score instances in parallel.Fast training and scoring procedures result in moreinstances having up-to-date scores.
Hence, the stale-2Since the annotator requests the next instance immediatelyafter annotating the current instance, the next instance is virtu-ally guaranteed to have a staleness factor of at least 1.36ness (and therefore quality) of selected instances de-pends on the relative time required to train and scoremodels, thereby encouraging efficient training andscoring algorithms.
In fact, the other forms of par-allelization previously mentioned can be leveragedto reduce staleness rather than attempting to directlyreduce wait time.These principles lead to Algorithm 2, which wecall parallel (for clarity, we have omitted steps re-lated to concurrency).
AnnotateLoop representsthe tireless oracle who constantly requests instances.The call to Annotate is a surrogate for the actualannotation process and most importantly, the timespent in this method is the time required to provideannotations.
Once an annotation is obtained, it isplaced on a shared buffer B where it becomes avail-able for training.
While the annotator is, in effect,a producer of annotations, TrainLoop is the con-sumer which simply retrains models as annotated in-stances become available on the buffer.
This bufferis analagous to the batch used for training in Algo-rithm 1.
However, the size of the buffer changesdynamically based on the relative amounts of timespent annotating and training.
Finally, ScoreLoopendlessly scores instances, using new models assoon as they are trained.
The set of instances scoredwith a given model is analagous to the candidate setin Algorithm 1.3 Experimental DesignBecause the performance of the parallel algorithmand the ?worst-case?
cost analysis depend on waittime, we hold computing resources constant, run-ning all experiments on a cluster of Dell PowerEdgeM610 servers equipped with two 2.8 GHz quad-coreIntel Nehalem processors and 24 GB of memory.All experiments were on English part of speech(POS) tagging on the POS-tagged Wall Street Jour-nal text in the Penn Treebank (PTB) version 3 (Mar-cus et al, 1994).
We use sections 2-21 as initiallyunannotated data and randomly select 100 sentencesto seed the models.
We employ section 24 as the seton which tag accuracy is computed, but do not countevaluation as part of the wait time.
We simulate an-notation costs using the cost model from Ringger etal.
(2008): cost(s) = (3.80 ?
l + 5.39 ?
c+ 12.57),where l is the number of tokens in the sentence, andInput: A seed set of annotated instances A, a set ofpairs of unannotated instances and theirinitial scores S, and a scoring function ?Result: A is updated with the instances chosen bythe AL process as annotated by the oracleB ?
?, ?
?
nullStart(AnnotateLoop)Start(TrainLoop)Start(ScoreLoop)procedure AnnotateLoop()while S 6= ?
dot?
c from S having max c[score]S ?
S ?
tB ?
B ?
Annotate(t)endendprocedure TrainLoop()while S 6= ?
do?
?
TrainModel(A)A ?
A?
BB ?
?endendprocedure ScoreLoop()while S 6= ?
doc?
ChooseCandidate(S)S ?S ?
{c} ?
{(c[inst], ?
(c[inst], ?
))|c ?
S}endendAlgorithm 2: parallelc is the number of pre-annotated tags that need cor-rection, which can be estimated using the currentmodel.
We use the same model for pre-annotationas for scoring.We employ the return on investment (ROI) ALframework introduced by Haertel et.
al (2008).This framework requires that one define both a costand benefit estimate and selects instances that max-imize benefit(x)?cost(x)cost(x) .
For simplicity, we esti-mate cost as the length of a sentence.
Our bene-fit model estimates the utility of each sentence asfollows: benefit(s) = ?
log (maxt p(t|s)) wherep(t|s) is the probability of a tagging given a sen-tence.
Thus, sentences having low average (in thegeometric mean sense) per-tag probability are fa-vored.
We use a maximum entropy Markov modelto estimate these probabilities, to pre-annotate in-stances, and to evaluate accuracy.37Figure 2: Staleness of the allowold algorithm overtime for different candidate set sizes4 ResultsTwo questions are pertinent regarding staleness:how much staleness does an algorithm introduce?and how detrimental is that staleness?
For zerostaleand batch, the first question was answered analyti-cally in a previous section.
We proceed by address-ing the answer empirically for allowold and parallelafter which we examine the second question.Figure 2 shows the observed staleness of instancesselected for annotation over time and for varyingcandidate set sizes for allowold.
As expected, smallcandidate sets induce more staleness, in this case invery high amounts.
Also, for any given candidateset size, staleness decreases over time (after the be-ginning stages), since the effective candidate set in-cludes an increasingly larger percentage of the data.Since parallel is based on the same allow-old-scores principle, it too could potentially see highlystale instances.
However, we found the average per-instance staleness of parallel to be very low: 1.10; itwas never greater than 4 in the range of data that wewere able to collect.
This means that for our task andhardware, the amount of time that the oracle takes toannotate an instance is high enough to allow newmodels to retrain quickly and score a high percent-age of the data before the next instance is requested.We now examine effect that staleness has onAL performance, starting with batch.
As we haveshown, higher batch sizes guarantee more stalenessso we compare the performance of several batchsizes (with a candidate set size of the full data) to ze-rostale and random.
In order to tease out the effectsthat the staleness has on performance from the ef-fects that the batches have on wait time (an elementof performance), we purposely ignore wait time.The results are shown in Figure 3.
Not surprisingly,zerostale is slightly superior to the batch methods,and all are superior to random selection.
Further-more, batch is not affected much by the amount ofstaleness introduced by reasonable batch sizes: forB < 100 the increase in cost of attaining 95% accu-racy compared to zerostale is 3% or less.Recall that allowold introduces more stalenessthan batch by maintaining old scores for each in-stance.
Figure 4 shows the effect of differentcandidate set sizes on this approach while fixingbatch size at 1 (wait time is excluded as before).Larger candidate set sizes have less staleness, sonot surprisingly performance approaches zerostale.Smaller candidate set sizes, having more staleness,perform similarly to random during the early stageswhen the model is changing more drastically eachinstance.
In these circumstances, scores producedfrom earlier models are not good approximations tothe actual scores so allowing old scores is detrimen-tal.
However, once models stabilize and old scoresbecome better approximations, performance beginsto approach that of zerostale.Figure 6 compares the performance of allowoldfor varying batch sizes for a fixed candidate set size(5000; results are similiar for other settings).
Asbefore, performance suffers primarily in the earlystages and for the same reasons.
However, a batchexcerbates the problem since multiple instances withpoor scores are selected simultaneously.
Neverthe-less, the performance appears to mostly recover oncethe scorers become more accurate.
We note thatbatch sizes of 5 and 10 increase the cost of acheiving95% accuracy by 3% and 10%, respectively, com-pared to zerostale.
The implications for parallelare that stalness may not be detrimental, especiallyif batch sizes are small and candidate set sizes arelarge in the beginning stages of AL.Figure 5 compares the effect of staleness on allfour algorithms when excluding wait time (B = 20,N = 5000 for the batch algorithms).
After achiev-ing around 85% accuracy, batch and parallel arevirtually indistinguishable from zerostale, implyingthat the staleness in these algorithms is mostly ignor-able.
Interestingly, allowold costs around 5% morethan zerostale to acheive an accuracy of 95%.
Weattribute this to increased levels of staleness whichparallel combats by avoiding idle time.380.80.820.840.860.880.90.920.940.96100000  200000  300000  400000  500000  600000  700000  800000  900000  1e+06AccuracyTime in SecondsZero StalenessBatch size 5Batch size 50Batch size 500Batch size 5000RandomFigure 3: Effect of staleness due to batch size forbatch, N =?0.80.820.840.860.880.90.920.940.96100000  200000  300000  400000  500000  600000  700000  800000  900000  1e+06AccuracyTime in SecondsZero StalenessCandidate Set Size 5000Candidate Set Size 500Candidate Set Size 100Candidate Set Size 50RandomFigure 4: Effect of staleness due to candidate set sizefor allowold, B = 10.80.820.840.860.880.90.920.940.96100000  200000  300000  400000  500000  600000  700000  800000  900000  1e+06AccuracyTime in SecondsZero StalenessParallelTraditional BatchAllow Old ScoresRandomFigure 5: Comparison of algorithms (not includingwait time)0.80.820.840.860.880.90.920.940.96100000  200000  300000  400000  500000  600000  700000  800000  900000  1e+06AccuracyTime in SecondsZero StalenessBatch Size 5Batch Size 10Batch Size 50Batch Size 100Batch Size 500RandomFigure 6: Effect of staleness due to batch size forallowold, N = 50000500001000001500002000002500000  2000  4000  6000  8000  10000  12000InstancesScoredModel NumberFigure 7: Effective candidate set size of parallelover time0.80.820.840.860.880.90.920.940.96100000  200000  300000  400000  500000  600000  700000  800000  900000  1e+06AccuracyTime in SecondsZero StalenessParallelTraditional BatchAllow Old ScoresRandomFigure 8: Comparison of algorithms (including waittime)39Since the amount of data parallel uses to trainmodels and score instances depends on the amountof time instances take to annotate, the ?effective?candidate set sizes and batch sizes over time is of in-terest.
We found that the models were always trainedafter receiving exactly one instance, within the datawe were able to collect.
Figure 7 shows the numberof instances scored by each successive scorer, whichappears to be very large on average: over 75% of thetime the scorer was able to score the entire dataset.For this task, the human annotation time is muchgreater than the amount of time it takes to train newmodels (at least, for the first 13,000 instances).
Thenet effect is that under these conditions, parallel isparameterized similar to batch with B = 1 and Nvery high, i.e., approaching zerostale, and thereforehas very low staleness, yet does so without incurringthe waiting cost.Finally, we compare the performance of the fouralgorithms using the same settings as before, but in-clude wait time as part of the cost.
The results arein Figure 8.
Importantly, parallel readily outper-forms zerostale, costing 40% less to reach 95% ac-curacy.
parallel also appears to have a slight edgeover batch, reducing the cost to acheive 95% accu-racy by a modest 2%; however, had the simulationcontinued, we we may have seen greater gains giventhe increasing training time that occurs later on.
Itis important to recognize in this comparison that thepurpose of parallel is not necessarily to significantlyoutperform a well-tuned batch algorithm.
Instead,we aim to eliminate wait time without requiring pa-rameters, while hopefully maintaining performance.These results suggest that our approach successfullymeets these criteria.Taken as a whole, our results appear to indicatethat the net effect of staleness is to make selectionmore random.
Models trained on little data tend toproduce scores that are not reflective of the actualutility of instances and essentially produce a ran-dom ranking of instances.
As more data is collected,scores become more accurate and performance be-gins to improve relative to random selection.
How-ever, stale scores are by definition produced usingmodels trained with less data than is currently avail-able, hence more staleness leads to more random-like behavior.
This explains why batch selectiontends to perform well in practice for ?reasonable?batch sizes: the amount of staleness introduced bybatch (B?12 on average for a batch of size B) intro-duces relatively little randomness, yet cuts the waittime by approximately a factor of B.This also has implications for our parallel methodof AL.
If a given learning algorithm and scoringfunction outperform random selection when usingzerostale and excluding wait time, then any addedstaleness should cause performance to more closelyresemble random selection.
However, once wait-ing time is accounted for, performance could ac-tually degrade below that of random.
In parallel,more expensive training and scoring algorithms arelikely to introduce larger amounts of staleness, andwould cause performance to approach random selec-tion.
However, parallel has no wait time, and henceour approach should always perform at least as wellas random in these circumstances.
In contrast, poorchoices of parameters in batch could perform worsethan random selection.5 Conclusions and Future WorkMinimizing the amount of time an annotator mustwait for the active learner to provide instances is animportant concern for practical AL.
We presented amethod that can eliminate wait time by allowing in-stances to be selected on the basis of the most re-cently assigned score.
We reduce the amount ofstaleness this introduces by allowing training andscoring to occur in parallel while the annotator isbusy annotating.
We found that on PTB data us-ing a MEMM and a ROI-based scorer that our pa-rameterless method performed slightly better than ahand-tuned traditional batch algorithm, without re-quiring any parameters.
Our approach?s parallel na-ture, elimination of wait time, ability to dynamicallyadapt the batch size, lack of parameters, and avoid-ance of worse-than-random behavior, make it an at-tractive alternative to batch for practical AL.Since the performance of our approach dependson the relative time spent annotating, training, andscoring, we wish to apply our technique in futurework to more complex problems and models thathave differing ratios of time spent in these areas.
Fu-ture work could also draw on the continual compu-tation framework (Horvitz, 2001) to utilize idle timein other ways, e.g., to predict annotators?
responses.40ReferencesS.
Arora, E. Nyberg, and C. P. Rose?.
2009.
Estimatingannotation cost for active learning in a multi-annotatorenvironment.
In Proceedings of the NAACL HLT 2009Workshop on Active Learning for Natural LanguageProcessing, pages 18?26.S.
P. Engelson and I. Dagan.
1996.
Minimizing manualannotation cost in supervised training from corpora.
InProceedings of the 34th annual meeting on Associa-tion for Computational Linguistics, pages 319?326.R.
A. Haertel, K. D. Seppi, E. K. Ringger, and J. L. Car-roll.
2008.
Return on investment for active learning.In NIPS Workshop on Cost Sensitive Learning.E.
Horvitz.
2001.
Principles and applications of con-tinual computation.
Artificial Intelligence Journal,126:159?96.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1994.
Building a large annotated corpus of en-glish: The penn treebank.
Computational Linguistics,19:313?330.E.
Ringger, M. Carmen, R. Haertel, K. Seppi, D. Lond-sale, P. McClanahan, J. Carroll, and N. Ellison.
2008.Assessing the costs of machine-assisted corpus anno-tation through a user study.
In Proc.
of LREC.B.
Settles, M. Craven, and L. Friedland.
2008.
Activelearning with real annotation costs.
In Proceedings ofthe NIPS Workshop on Cost-Sensitive Learning, pages1069?1078.K.
Tomanek, J. Wermter, and U. Hahn.
2007.
An ap-proach to text corpus construction which cuts annota-tion costs and maintains reusability of annotated data.Proc.
of EMNLP-CoNLL, pages 486?495.41
