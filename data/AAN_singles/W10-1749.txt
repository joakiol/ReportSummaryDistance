Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 327?332,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsLRscore for Evaluating Lexical and Reordering Quality in MTAlexandra BirchUniversity of EdinburghUnited Kingdoma.c.birch-mayne@s0454866.ed.ac.ukMiles OsborneUniversity of EdinburghUnited Kingdommiles@inf.ed.ac.ukAbstractThe ability to measure the quality of wordorder in translations is an important goalfor research in machine translation.
Cur-rent machine translation metrics do notadequately measure the reordering perfor-mance of translation systems.
We presenta novel metric, the LRscore, which di-rectly measures reordering success.
Thereordering component is balanced by alexical metric.
Capturing the two most im-portant elements of translation success ina simple combined metric with only oneparameter results in an intuitive, shallow,language independent metric.1 IntroductionThe main purpose of MT evaluation is to de-termine ?to what extent the makers of a systemhave succeeded in mimicking the human transla-tor?
(Krauwer, 1993).
But machine translationhas no ?ground truth?
as there are many possi-ble correct translations.
It is impossible to judgewhether a translation is incorrect or simply un-known and it is even harder to judge the the degreeto which it is incorrect.
Even so, automatic met-rics are necessary.
It is nearly impossible to collectenough human judgments for evaluating incre-mental improvements in research systems, or fortuning statistical machine translation system pa-rameters.
Automatic metrics are also much fasterand cheaper than human evaluation and they pro-duce reproducible results.Machine translation research relies heavilyupon automatic metrics to evaluate the perfor-mance of models.
However, current metrics relyupon indirect methods for measuring the qualityof the word order, and their ability to capture re-ordering performance has been demonstrated to bepoor (Birch et al, 2010).
There are two main ap-proaches to capturing reordering.
The first wayto measure the quality of word order is to countthe number of matching n-grams between the ref-erence and the hypothesis.
This is the approachtaken by the BLEU score (Papineni et al, 2002).This method discounts any n-gram which is notidentical to a reference n-gram, and also does notconsider the relative position of the strings.
Theycan be anywhere in the sentence.
Another com-mon approach is typified by METEOR (Banerjeeand Lavie, 2005) and TER (Snover et al, 2006).They calculate an ordering penalty for a hypoth-esis based on the minimum number of chunks thetranslation needs to be broken into in order to alignit to the reference.
The disadvantage of the secondapproach is that aligning sentences with very dif-ferent words can be inaccurate.
Also there is nonotion of how far these blocks are out of order.More sophisticated metrics, such as the RTE met-ric (Pado?
et al, 2009), use higher level syntactic oreven semantic analysis to determine the quality ofthe translation.
These approaches are useful, butcan be very slow, require annotation, they are lan-guage dependent and their parameters are hard totrain.
For most research work shallow metrics aremore appropriate.Apart from failing to capture reordering perfor-mance, another common criticism of most cur-rent automatic MT metrics is that a particularscore value reported does not give insights intoquality (Przybocki et al, 2009).
This is becausethere is no intrinsic significance of a differencein scores.
Ideally, the scores that the metrics re-port would be meaningful and stand on their own.However, the most one can say is that higher isbetter for accuracy metrics and lower is better forerror metrics.We present a novel metric, the LRscore, whichexplicitly measures the quality of word order inmachine translations.
It then combines the re-ordering metric with a metric measuring lexicalsuccess.
This results in a comprehensive met-327ric which measures the two most fundamental as-pects of translation.
We argue that the LRscoreis intuitive and meaningful because it is a simple,decomposable metric with only one parameter totrain.The LRscore has many of the properties that aredeemed to be desirable in a recent metric eval-uation campaign (Przybocki et al, 2009).
TheLRscore is language independent.
The reorder-ing component relies on abstract alignments andword positions and not on words at all.
The lex-ical component of the system can be any mean-ingful metric for a particular target language.
Inour experiments we use 1-gram BLEU and 4-gramBLEU, however, if a researcher was interested inmorphologically rich languages, a different met-ric which scores partially correct words might bemore appropriate.
The LRscore is a shallow met-ric, which means that it is reasonably fast to run.This is important in order to be useful for train-ing of the translation model parameters.
A finaladvantage is that the LRscore is a sentence levelmetric.
This means that human judgments can bedirectly compared to system scores and helps re-searchers to understand what changes they are see-ing between systems.In this paper we start by describing the reorder-ing metrics and then we present the LRscore.
Fi-nally we discuss related work and conclude.2 Reordering MetricsThe relative ordering of words in the source andtarget sentences is encoded in alignments.
Wecan interpret algnments as permutations.
Thisallows us to apply research into metrics for or-dered encodings to our primary tasks of measur-ing and evaluating reorderings.
A word alignmentover a sentence pair allows us to transcribe thesource word positions in the order of the alignedtarget words.
Permutations have already beenused to describe reorderings (Eisner and Tromble,2006), primarily to develop a reordering modelwhich uses ordering costs to score possible per-mutations.
Here we use permutations to evaluatereordering performance based on the methods pre-sented in (Birch et al, 2010).The ordering of the words in the target sentencecan be seen as a permutation of the words in thesource sentence.
The source sentence s of lengthN consists of the word positions s0 ?
?
?
si ?
?
?
sN .Using an alignment function where a source wordat position i is mapped to a target word at positionj with the function a : i ?
j, we can reorder thesource word positions to reflect the order of thewords in the target.
This gives us a permutation.A permutation is a bijective function from a setof natural numbers 1, 2, ?
?
?
, N to itself.
We willname our permutations pi and ?.
The ith symbolof a permutation pi will be denoted as pi(i), andthe inverse of the permutation pi?1 is defined sothat if pi(i) = j then pi?1(j) = i.
The identity, ormonotone, permutation id is the permutation forwhich id(i) = i for all i.
Table 1 shows the per-mutations associated with the example alignmentsin Figure 1.
The permutations are calculated byiterating over the source words, and recording theordering of the aligned target words.Permutations encode one-one relations,whereas alignments contain null alignments andone-many, many-one and many-many relations.For now, we make some simplifying assumptionsto allow us to work with permutations.
Sourcewords aligned to null (a(i) ?
null) are assignedthe target word position immediately after thetarget word position of the previous source word(pi(i) = pi(i ?
1) + 1).
Where multiple sourcewords are aligned to the same target word orphrase, a many-to-one relation, the target orderingis assumed to be monotone.
When one sourceword is aligned to multiple target words, a one-to-many relation, the source word is assumed to bealigned to the first target word.A translation can potentially have many validword orderings.
However, we can be reason-ably certain that the ordering of reference sentencemust be acceptable.
We therefore compare the or-dering of a translation with that of the referencesentence.
The underlying assumption is that mostreasonable word orderings should be fairly similarto the reference.
The assumption that the referenceis somehow similar to the translation is necessaryfor all automatic machine translation metrics.
Wepropose using permutation distance metrics to per-form the comparison.There are many different ways of measuringdistance between two permutations, with differentsolutions originating in different domains (statis-tics, computer science, molecular biology, .
.
.
).Real numbered data leads to measures such as Eu-clidean distance, binary data to measures such asHamming distance.
But for ordered sets, thereare many different options, and the best one de-328t1t2t3t4t5t6t7t8t9t10s1 s2 s3 s4 s5 s6 s7 s8 s9 s10(a)t1t2t3t4t6t5t7t8t9t10s1 s2 s3 s4 s5 s6 s7 s8 s9 s10(b)t6t7t8t9t10t1t2t3t4t5s1 s2 s3 s4 s5 s6 s7 s8 s9 s10(c)t10t1t2t3t4t5t6t7t8t9s1 s2 s3 s4 s5 s6 s7 s8 s9 s10(d)Figure 1: Synthetic examples: a translation and three reference scenarios.
(a) is a monotone translation,(b) is a reference with one short distance word order difference, (c) is a reference where the order of thetwo halves has been swapped, and (d) is a reference with a long distance reordering of the first targetword.pends on the task at hand.
We choose a fewmetrics which are widely used, efficient to calcu-late and capture certain properties of the reorder-ing.
In particular, they are sensitive to the num-ber of words that are out of order.
Three of themetrics, Kendall?s tau, Spearman?s rho and Spear-man?s footrule distances also take into account thedistance between positions in the reference andtranslation sentences, or the size of the reordering.An obvious disadvantage of this approach is thefact that we need alignments, either between thesource and the reference, and the source and thetranslation, or directly between the reference andthe translation.
If accuracy is paramount, the testset could include manual alignments and the sys-tems could directly output the source-translationalignments.
Outputting the alignment informa-tion should require a trivial change to the decoder.Alignments can also be automatically generatedusing the alignment model that aligns the trainingdata.Distance metrics increase as the quality of trans-lation decreases.
We invert the scale of the dis-(a) (1 2 3 4 5 6 7 8 9 10)(b) (1 2 3 4 ?6 ?5 ?7 8 9 10)(c) (6 7 8 9 10 ?1 2 3 4 5)(d) (2 3 4 5 6 7 8 9 10 ?1)Table 1: Permutations extracted from the sentencepairs shown in Figure 1: (a) is a monotone permu-tation and (b), (c) and (d) are permutations withdifferent amounts of disorder, where bullet pointshighlight non-sequential neighbors.tance metrics in order to easily compare them withother metrics where increases in the metrics meanincreases in translation quality.
All permutationdistance metrics are thus subtracted from 1.
Notethat the two permutations we refer to pi and ?
arerelative to the source sentence, and not to the ref-erence: the source-reference permutation is com-pared to the source-translation permutation.2.1 Hamming DistanceThe Hamming distance (Hamming, 1950) mea-sures the number of disagreements between two329permutations.
The Hamming distance for permu-tations was proposed by (Ronald, 1998) and is alsoknown as the exact match distance.
It is definedas follows:dH(pi, ?)
= 1?
?ni=1 xinwhere xi ={0 if pi(i) = ?
(i)1 otherwiseWhere pi, ?
are the two permutations and thenormalization constant Z is n, the length of thepermutation.
We are interested in the Hammingdistance for its ability to capture the amount of ab-solute disorder that exists between two permuta-tions.
The Hamming distance is widely utilized incoding theory to measure the discrepancy betweentwo binary sequences.2.2 Kendall?s Tau DistanceKendall?s tau distance is the minimum numberof transpositions of two adjacent symbols nec-essary to transform one permutation into an-other (Kendall, 1938; Kendall and Gibbons,1990).
This is sometimes known as the swap dis-tance or the inversion distance and can be inter-preted as a function of the probability of observingconcordant and discordant pairs (Kerridge, 1975).It is defined as follows:d?
(pi, ?)
= 1?
?ni=1?nj=1 zijZwhere zij ={1 if pi(i) < pi(j) and ?
(i) > ?
(j)0 otherwiseZ =(n2 ?
n)2The Kendall?s tau metric is possibly the most in-teresting for measuring reordering as it is sensitiveto all relative orderings.
It consequently measuresnot only how many reordering there are but alsothe distance that words are reordered.In statistics, Spearman?s rho and Kendall?s tauare widely used non-parametric measures of as-sociation for two rankings.
In natural languageprocessing research, Kendall?s tau has been usedas a means of estimating the distance betweena system-generated and a human-generated gold-standard order for the sentence ordering task (La-pata, 2003).
Kendall?s tau has also been usedin machine translation as a cost function in a re-ordering model (Eisner and Tromble, 2006) andan MT metric called ROUGE-S (Lin and Och,2004) is similar to a Kendall?s tau metric on lexicalitems.
ROUGE-S is an F-measure of ordered pairsof words in the translation.
As far as we know,Kendall?s tau has not been used as a reorderingmetric before.3 LRscoreThe goal of much machine translation research iseither to improve the quality of the words used inthe output, or their ordering.
We use the reorderingmetrics and combine them with a measurement oflexical performance to produce a comprehensivemetric, the LRscore.
The LRscore is a linear in-terpolation of a reordering metric with the BLEUscore.
If we use the 1-gram BLEU score, BLEU1,then the LRscore relies purely upon the reorder-ing metric for all word ordering evaluation.
Wealso use the 4-gram BLEU score, BLEU4, as it isan important baseline and the values it reports arevery familiar to machine translation researchers.BLEU4 also contains a notion of word orderingbased on longer matching n-grams.
However, itis aware only of very local orderings.
It does notmeasure the magnitude of the orderings like thereordering metrics do, and it is dependent on ex-act lexical overlap which does not affect the re-ordering metric.
The two components are there-fore largely orthogonal and there is a benefit incombining them.
Both the BLEU score and thereordering distance metric apply a brevity penaltyto account for translations of different lengths.The formula for calculating the LRscore is asfollows:LRscore = ?
?R+ (1?
?
)BLEUWhere the reordering metricR is calculated as fol-lows:R = d ?BPWhere we either take the Hamming distance dHor the Kendall?s tau distance d?
as the reorderingdistance d and then we apply the brevity penaltyBP .
The brevity penalty is calculated as:BP ={1 if t > re1?r/t if t ?
rwhere t is the length of the translation, and r isthe closest reference length.
R is calculated at thesentence level, and the scores are averaged over atest set.
This average is then combined with the330system level lexical score.
The Lexical metric isthe BLEU score which sums the log precision ofn-grams.
In our paper we set the n-gram length toeither be one or four.The only parameter in the metric ?
balances thecontribution of reordering and the lexical compo-nents.
There is no analytic solution for optimizingthis parameter, and we use greedy hillclimbing inorder to find the optimal setting.
We optimize thesentence level correlation of the metric to humanjudgments of accuracy as provided by the WMT2010 shared task.
As hillclimbing can end up in alocal minima, we perform 20 random restarts, andretaining only the parameter value with the bestconsistency result.
Random-restart hill climbing isa surprisingly effective algorithm in many cases.
Itturns out that it is often better to spend CPU timeexploring the space, rather than carefully optimiz-ing from an initial condition.The brevity penalty applies to both the reorder-ing metric and the BLEU score.
We do not seta parameter to regulate the impact of the brevitypenalty, as we want to retain BLEU scores that arecomparable with BLEU scores computed in pub-lished research.
And as we do not regulate thebrevity penalty in the BLEU score, we do not wishto do so for the reordering metric either.
It there-fore impacts on both the reordering and the lexicalcomponents equally.4 Correlation with Human JudgmentsIt has been common to use seven-point fluencyand adequacy scores as the main human evalua-tion task.
These scores are intended to be absolutescores and comparable across sentences.
Seven-point fluency and adequacy judgements are quiteunreliable at a sentence level and so it seems du-bious that they would be reliable across sentences.However, having absolute scores does have the ad-vantage of making it easy to calculate the correla-tion coefficients of the metric with human judge-ments.
Using rank judgements, we do not haveabsolute scores and thus we cannot compare trans-lations across different sentences.We therefore take the method adopted in the2009 workshop on machine translation (Callison-Burch et al, 2009).
We ascertained how consis-tent the automatic metrics were with the humanjudgements by calculating consistency in the fol-lowing manner.
We take each pairwise compari-son of translation output for single sentences by aMetric de-en es-en fr-en cz-enBLEU4 58.72 55.48 57.71 57.24LR-HB1 60.37 60.55 58.59 53.70LR-HB4 60.49 58.88 58.80 57.74LR-KB1 60.67 58.54 58.46 54.20LR-KB4 61.07 59.86 58.59 58.92Table 2: The percentage consistency between hu-man judgements of rank and metrics.
The LRscorevariations (LR-*) are optimised for consistency foreach language pair.particular judge, and we recorded whether or notthe metrics were consistent with the human rank.Ie.
we counted cases where both the metric and thehuman judged agree that one system is better thananother.
We divided this by the total umber of pair-wise comparisons to get a percentage.
There weremany ties in the human data, but metrics rarelygive the same score to two different translations.We therefore excluded pairs that the human anno-tators ranked as ties.
The human ranking data andthe system outputs from the 2009 Workshop onMachine Translation (Callison-Burch et al, 2009)have been used to evaluate the LRscore.We optimise the sentence level consistency ofthe metric.
As hillclimbing can end up in a localminima, we perform 20 random restarts, and re-taining only the parameter value with the best con-sistency result.
Random-restart hill climbing is asurprisingly effective algorithm in many cases.
Itturns out that it is often better to spend CPU timeexploring the space, rather than carefully optimis-ing from an initial condition.Table 2 reports the optimal consistency of theLRscore and baseline metrics with human judge-ments for each language pair.
The table alsoreports the individual component results.
TheLRscore variations are named as follows: LRrefers to the LRscore, ?H?
refers to the Hammingdistance and ?K?
to Kendall?s tau distance.
?B1?and ?B4?
refer to the smoothed BLEU score withthe 1-gram and 4-gram scores.
The LRscore is themetric which is most consistent with human judge-ment.
This is an important result which showsthat combining lexical and reordering informationmakes for a stronger metric.5 Related Work(Wong and Kit, 2009) also suggest a metric whichcombines a word choice and a word order com-331ponent.
They propose a type of F-measure whichuses a matching function M to calculate precisionand recall.
M combines the number of matchedwords, weighted by their tfidf importance, withtheir position difference score, and finally sub-tracting a score for unmatched words.
Includ-ing unmatched words in the in M function un-dermines the interpretation of the supposed F-measure.
The reordering component is the averagedifference of absolute and relative word positionswhich has no clear meaning.
This score is not intu-itive or easily decomposable and it is more similarto METEOR, with synonym and stem functional-ity mixed with a reordering penalty, than to ourmetric.6 ConclusionWe propose the LRscore which combines a lexi-cal and a reordering metric.
This results in a met-ric which is both meaningful and accurately mea-sures the word order performance of the transla-tion model.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
Meteor: Anautomatic metric for MT evaluation with improvedcorrelation with human judgments.
In Workshop onIntrinsic and Extrinsic Evaluation Measures for MTand/or Summarization.Alexandra Birch, Phil Blunsom, and Miles Osborne.2010.
Metrics for MT Evaluation: Evaluating Re-ordering.
Machine Translation (to appear).Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
InProceedings of the Fourth Workshop on StatisticalMachine Translation, pages 1?28, Athens, Greece,March.
Association for Computational Linguistics.Jason Eisner and Roy W. Tromble.
2006.
Local searchwith very large-scale neighborhoods for optimalpermutations in machine translation.
In Proceed-ings of the HLT-NAACL Workshop on Computation-ally Hard Problems and Joint Inference in Speechand Language Processing, pages 57?75, New York,June.Richard Hamming.
1950.
Error detecting and errorcorrecting codes.
Bell System Technical Journal,26(2):147?160.M.
Kendall and J. Dickinson Gibbons.
1990.
RankCorrelation Methods.
Oxford University Press,New York.Maurice Kendall.
1938.
A new measure of rank corre-lation.
Biometrika, 30:81?89.D Kerridge.
1975.
The interpretation of rank correla-tions.
Applied Statistics, 2:257?258.S.
Krauwer.
1993.
Evaluation of MT systems: a pro-grammatic view.
Machine Translation, 8(1):59?66.Mirella Lapata.
2003.
Probabilistic text structur-ing: Experiments with sentence ordering.
Compu-tational Linguistics, 29(2):263?317.Chin-Yew Lin and Franz Josef Och.
2004.
Auto-matic evaluation of machine translation quality us-ing longest common subsequence and skip-bigramstatistics.
In Proceedings of the 42nd Meetingof the Association for Computational Linguistics(ACL?04), Main Volume, pages 605?612, Barcelona,Spain, July.Sebastian Pado?, Daniel Cer, Michel Galley, Dan Juraf-sky, and Christopher D. Manning.
2009.
Measur-ing machine translation quality as semantic equiva-lence: A metric based on entailment features.
Ma-chine Translation.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof the Association for Computational Linguistics,pages 311?318, Philadelphia, USA.Mark Przybocki, Kay Peterson, Se?bastien Bronsart,and Gregory Sanders.
2009.
The nist 2008 metricsfor machine translation challengeoverview, method-ology, metrics, and results.
Machine Translation.S Ronald.
1998.
More distance functions for order-based encodings.
In the IEEE Conference on Evolu-tionary Computation, pages 558?563.Matthew Snover, Bonnie Dorr, R Schwartz, L Micci-ulla, and J Makhoul.
2006.
A study of translationedit rate with targeted human annotation.
In AMTA.B.
Wong and C. Kit.
2009.
ATEC: automatic eval-uation of machine translation via word choice andword order.
Machine Translation, pages 1?15.332
