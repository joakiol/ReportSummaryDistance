iA Max imum Ent ropy  Mode l  for  Par t -Of -Speech  Tagg ingAdwait RatnaparkhiUnivers i ty  of Pennsy lvan iaDept .
of Computer  and In format ion  Scienceadwai t~grad ient ,  c i s .
upenn,  eduAbst ractThis paper presents a statistical model whichtrains from a corpus annotated with Part-Of-Speech tags and assigns them to previously unseentext with state-of-the-art accuracy(96.6%).
Themodel can be classified as a Maximum Entropymodel and simultaneously uses many contextual"features" to predict the POS tag.
Furthermore,this paper demonstrates the use of specialized fea-tures to model difficult tagging decisions, discussesthe corpus consistency problems discovered uringthe implementation f these features, and proposesa training strategy that mitigates these problems.In t roduct ionMany natural anguage tasks require the accurateassignment of Part-Of-Speech (POS) tags to pre-viously unseen text.
Due to the availability oflarge corpora which have been manually annotatedwith POS information, many taggers use anno-tated text to "learn" either probability distribu-tions or rules and use them to automatically assignPOS tags to unseen text.The experiments in this paper were conductedon the Wall Street Journal corpus from the PennTreebank project(Marcus et al, 1994), althoughthe model can trai~n from any large corpus anno-tated with POS tags.
Since most realistic natu-ral language applications must process words thatwere never seen before in training data, all exper-iments in this paper are conducted on test datathat include unknown words.Several recent papers(Brill, 1994,Magerman, 1995) have reported 96.5% tag-ging accuracy on the Wall St. Journal corpus.The experiments in this paper test the hy-pothesis that better use of context will improvethe accuracy.
A Maximum Entropy model iswell-suited for such experiments ince it corn-bines diverse forms of contextual informationin a principled manner, and does not imposeany distributional assumptions on the train-ing data.
Previous uses of this model includelanguage modeling(Lau et al, 1993), machinetranslation(Berger et al, 1996), prepositionalphrase attachment(Ratnaparkhi et al, 1994), andword morphology(Della Pietra et al, 1995).
Thispaper briefly describes the maximum entropyand maximum likelihood properties of the model,features used for POS tagging, and the experi-ments on the Penn Treebank Wall St. Journalcorpus.
It then discusses the consistency problemsdiscovered uring an attempt to use specializedfeatures on the word context.
Lastly, the resultsin this paper are compared to those from previouswork on POS tagging.The Probability ModelThe probability model is defined over 7-/x 7-, where7t is the set of possible word and tag contexts, or"histories", and T is the set of allowable tags.
Themodel's probability of a history h together with atag t is defined as:kp(h, t) = Ill -J- jj(h,t)j= l(1)where ~" is a normalization constant,{tt, ~1,..., ak} are the positive model parametersand { f l , .
.
- , f k}  are known as "features", wherefj(h,t) E {O, 1}.
Note that each parameter ajcorresponds to a feature fj.Given a sequence of words {wl , .
.
.
,  Wn} andtags {tl,...t,~} as training data, define hi as thehistory available when predicting ti.
The param-eters {p, al  .
.
.
.
.
ak} are then chosen to maximize133the likelihood of the training data using p:r(p) = 1-\[  p(h.t,) = 1-\[i=1 i=1 j= lThis model also can be interpreted under theMaximum Entropy formalism, in which the goal isto maximize the entropy of a distribution subjectto certain constraints.
Here, the entropy of thedistribution p is defined as:H(p) = - E p(h,t) logp(h,t)hE74,tETand the constraints are given by:Efj = Efj, l < j _< k (2)where the model's feature expectation isEf j=  E p(h,t)fj(h,t)hET.l,tETand the observed feature expectation isi=1and where iS(hi, ti) denotes the observed probabil-ity of (hi,ti) in the training data.
Thus the con-straints force the model to match its feature xpec-tations with those observed in the training data.In practice, 7-/ is very large and the model's ex-pectation Efj cannot be computed irectly, so thefollowing approximation(Lau et al, 1993) is used:nE fj ,~ E15(hi)p(tilhi)fj(hi,ti)i=1where fi(hi) is the observed probability of the his-tory hi in the training set.It can be shown (Darroch and Ratcliff, 1972)that if p has the form (1) and satisfies the kconstraints (2), it uniquely maximizes the en-tropy H(p) over distributions that satisfy (2), anduniquely maximizes the likelihood L(p) over dis-tributions of the form (1).
The model parametersfor the distribution p are obtained via GeneralizedIterative Scaling(Darroch and Ratcliff, 1972).Features  fo r  POS Tagg ingThe joint probability of a history h and tag tis determined by those parameters whose corre-sponding features are active, i.e., those o~j such134that fj(h,t) = 1.
A feature, given (h,t), may ac-tivate on any word or tag in the history h, andmust encode any information that might help pre-dict t, such as the spelling of the current word, orthe identity of the previous two tags.
The specificword and tag context available to a feature is givenin the following definition of a history hi:hi  {wi, wi+I ,  wi-}-2, wi-1, wi-2, t i -1 ,  t i -2}For example,1 if suffix(w/) = "ing" & ti = VBGf j(hl,t i)= 0 otherwiseIf the above feature exists in the feature set ofthe model, its corresponding model parameter willcontribute towards the joint probability p(hi,ti)when wi ends with "?ng" and when ti =VBG 1.Thus a model parameter aj effectively serves asa "weight" for a certain contextual predictor, inthis case the suffix " ing",  towards the probabilityof observing a certain tag, in this case a VBG.The model generates the space of features byscanning each pair (hi, ti) in the training data withthe feature "templates" given in Table 1.
Givenhi as the current history, a feature always askssome yes/no question about hi, and furthermoreconstrains ti to be a certain tag.
The instantia-tions for the variables X, Y, and T in Table 1 areobtained automatically from the training data.The generation of features for tagging un-known words relies on the hypothesized distinctionthat "rare" words 2 in the training set are similarto unknown words in test data, with respect tohow their spellings help predict their tags.
Therare word features in Table 1, which look at theword spellings, will apply to both rare words andunknown words in test data.For example, Table 2 contains an excerpt fromtraining data while Table 3 contains the featuresgenerated while scanning (ha, t3), in which the cur-rent word is about, and Table 4 contains featuresgenerated while scanning (h4, t4), in which the cur-rent word, we l l -hee led ,  occurs 3 times in train-ing data and is therefore classified as "rare".The behavior of a feature that occurs verysparsely in the training set is often difficult to pre-dict, since its statistics may not be reliable.
There-fore, the model uses the heuristic that any feature1VBG is the Treebank POS tag for Verb Gerund.2A "rare" word here denotes a word which occursless than 5 times in the training set.
The count of 5was chosen by subjective inspection of words in thetraining data.Condition Featuresw i is not rare wi = X & ti = Twi is rare?
wiX is prefix of wi, IXI ~ 4 &t i=TX is suffix of wi, IXI < 4 & l i   Twi conta ins  number  ~z ti = Twi conta ins  uppercase  character  ~ ti = Twi contains hyphen ~t i=Tt i -1 = X & ti  = Tt i_2t i_ t  = XY  ~ ti = Twi -1 = X & li = Twi -2  = X ~ ti = Twi+ 1 = X ~s t i = Twi+2 = X & t i  = TTable h Features on the current history hiWord:Tag:Pos i t ion :the stories about wel l -heeled communit ies and developersDT NNS IN JJ NNS CC NNS1 2 3 4 5 6 7Table 2: Sample Datawi ---- aboutwi-i -~ storieswi-2 ---- theWi+l = wel l -heeledwi+2 ---- communit iest i -  I = NNSt i -2 t i -1  = DT NNSTable 3: Features Generated From h3 (for& ti = INg5 ti = IN& ti = INti = IN~2 ti = IN~: ti = INti = INtagging about) from Table 2Wi-1 -~ about   ti = J Jwi-2  = stor ies  ~z ti = J JWi+l = communi t ies  & ti = J Jwi+2 = and ~ ti = J Jt i -1  = IN ~ ti = JJti-2ti-I ---- I~IS IN ~z ti = JJpre f ix (w i )=w ~ ii = J Jprefix(wi)----we & ti = J Jp re f ix (w i )=wel  ~z li = J Jprefix(wi)=well & ti = JJsu l f f i x (w i )=d ~ ti = JJsu f f ix (w i )=ed & ti = J Jsu fx (w i )= led  ~ ti = JJsuffix(wi)=eled & ti = JJwi conta ins  hyphen & ti = JJTable 4: Features Generated From h4 (for tagging wel l -heeled)  from Table 2135which occurs less than 10 times in the data is un-reliable, and ignores features whose counts are lessthan 10.
3 While there are many smoothing algo-rithms which use techniques more rigorous than asimple count cutoff, they have not yet been inves-tigated in conjunction with this tagger.Tes t ing  the  ModelThe test corpus is tagged one sentence at a time.The testing procedure requires a search to enumer-ate the candidate tag sequences for the sentence,and the tag sequence with the highest probabilityis chosen as the answer.Search  A lgor i thmThe search algorithm, essentially a "beam search",uses the conditional tag probabilityp(h,t)p( t lh )  - p(h,t')and maintains, as it sees a new word, the Nhighest probability tag sequence candidates upto that point in the sentence.
Given a sentencea tag sequence candidatehas conditional probability:P(tl  .
.tnlwl ..wn) = I I  p(tilh,)i= lIn addition the search procedure optionallyconsults a Tag Dictionary, which, for each knownword, lists the tags that it has appeared with inthe training set.
If the Tag Dictionary is in effect,the search procedure, for known words, generatesonly tags given by the dictionary entry, while forunknown words, generates all tags in the tag set.Without the Tag Dictionary, the search proceduregenerates all tags in the tag set for every word.Let W = {wl.. .w,~} be a test sentence, andlet sij be the j th  highest probability tag sequenceup to and including word wi.
The search is de-scribed below:1.
Generate tags for wl, find top N, set 81j , 1 _<j < N, accordingly.2.
Initialize i = 2(a) Initialize j = 13Except for features that look only at the cur-rent word, i.e., features of the form wl ----<word> andtl :<TAG>.
The count of 10 was chosen by inspectionof Training and Development data.136(b) Generate tags for wi, given s(i-1)j as previoustag context, and append each tag to s(i-1)j tomake a new sequence(c) j = j + 1, Repeat from (b) i f j  _< g3.
Find N highest probability sequences generatedby above loop, and set sij, 1 < j _< N, accord-ingly.4.
i = i + 1, Repeat from (a) if i _< n5.
Return highest probability sequence, s~lExperimentsIn order to conduct tagging experiments, theWall St. Journal data has been split into threecontiguous ections, as shown in Table 5.
Thefeature set and search algorithm were tested anddebugged only on the Training and Developmentsets, and the official test result on the unseen Testset is presented in the conclusion of the paper.The performances of the "baseline" model on theDevelopment Set, both with and without the TagDictionary, are shown in Table 6.All experiments use a beam size of N = 5;further increasing the beam size does not signifi-cantly increase performance on the DevelopmentSet but adversely affects the speed of the tagger.Even though use of the Tag Dictionary gave an ap-parently insignificant (.
12%) improvement in accu-racy, it is used in further experiments since it sig-nificantly reduces the number of hypotheses andthus speeds up the tagger.The running time of the parameter estimationalgorithm is O(NTA), where N is the training setsize, T is the number of allowable tags, and A isthe average number of features that are active for agiven event (h, t).
The running time of the searchprocedure on a sentence of length N is O(NTAB),where T, A are defined above, and B is the beamsize.
In practice, the model for the experimentshown in Table 6 requires approximately 24 hoursto train, and 1 hour to test 4 on an IBM RS/6000Model 380 with 256MB of RAM.Specialized Features  andCons is tencyThe Maximum Entropy model allows arbitrarybinary-valued features on the context, so it can useadditional specialized, i.e., word-specific, features4The search procedure has not been optimized andthe author expects it to run 3 to 5 times faster afteroptimizations.DataSet Sentences Words Unknown WordsTraining 40000 962687Development 8000 192826 6107Test 5485 133805 3546Table 5: WSJ Data SizesTag DictiOnaryNo Tag DictionaryTotal Word Accuracy Unknown Word Accuracy96.43% 86.23%96.31% 86.28%Table 6: Baseline Performance on Development SetSentence Accuracy47.55%47.38%Table 7:WordaboutthatmoreupthatasupmorethataboutthatoutthatmuchyenchiefupagomuchoutCorrect Tag Model's Tag FrequencyRB IN 393DT IN 389RBR J JR 221IN RB 187WDT IN 184RB IN 176IN RP 176J JR RBR 175IN WDT 159IN i RB 144IN DT 127RPDTINWDT126123J J  RB 118NN NNS 117NN I J J  116RP IN 114IN RB 112RB !
J J  111IN RP  109Top Tagging Mistakes on Training Set for Baseline Model137to correctly tag the "residue" that the baselinefeatures cannot model.
Since such features typ-ically occur infrequently, the training set consis-tency must be good enough to yield reliable statis-tics.
Otherwise the specialized features will modelnoise and perform poorly on test data.Such features can be designed for those wordswhich are especially problematic for the model.The top errors of the model (over the training set)are shown in Table 7; clearly, the model has trou-ble with the words that  and about, among others.As hypothesized in the introduction, better fea-tures on the context surrounding that  and aboutshould correct the tagging mistakes for these twowords, assuming that the tagging errors are due toan impoverished feature set, and not inconsistentdata.Specialized features for a given word are con-structed by conjoining certain features in the base-line model with a question about the word itself.The features which ask about previous tags andsurrounding words now additionally ask about theidentity of the current word, e.g., a specialized fea-ture for the word about in Table 3 could be:1 if wi : about ~ t i-2ti-1 = DT NNSf j  (hi, ti) = & ti = IN0 otherwiseTable 8 shows the results of an experimentin which specialized features are constructed for"difficult" words, and are added to the baselinefeature set.
Here, "difficult" words are those thatare mistagged a certain way at least 50 times whenthe training set is tagged with the baseline model.Using the set of 29 difficult words, the model per-forms at 96.49% accuracy on the Development Set,an insignificant improvement from the baseline ac-curacy of 96.43%.
Table 9 shows the change in er-ror rates on the Development Set for the frequentlyoccurring "difficult" words.
For most words, thespecialized model yields little or no improvement,and for some, i.e., more and about, the specializedmodel performs worse.The lack of improvement implies that eitherthe feature set is still impoverished, or that thetraining data is inconsistent.
A simple consistencytest is to graph the POS tag assignments for agiven word as a function of the article in whichit occurs.
Consistently tagged words should haveroughly the same tag distribution as the articlenumbers vary.
Figure 1 represents each POS tagwith a unique integer and graphs the POS annota-tion of about in the training set as a function of the138articles (the points are "scattered" to show den-sity).
As seen in figure 1, about is usually anno-tated with tag#l ,  which denotes IN (preposition),or tag#9, which denotes RB (adverb), and the ob-served probability of either choice depends heavilyon the current article-~.
Upon further examina-tion 5, the tagging distribution for about changesprecisely when the annotator changes.
Figure 2,which again uses integers to denote POS tags,shows the tag distribution of about as a function ofannotator, and implies that the tagging errors forthis word are due mostly to inconsistent data.
Thewords ago, ch ie f ,  down, execut ive ,  o f f ,  out, upand yen also exhibit similar bias.Thus specialized features may be less effectivefor those words affected by inter-annotator bias.A simple solution to eliminate inter-annotator in-consistency is to train and test the model on datathat has been created by the same annotator.
Theresults of such an experiment 6 are shown in Ta-ble 10.
The total accuracy is higher, implyingthat the singly-annotated training and test setsare more consistent, and the improvement due tothe specialized features is higher than before (.1%)but still modest, implying that either the featuresneed further improvement or that intra-annotatorinconsistencies xist in the corpus.Compar i son  Wi th  P rev ious  WorkMost of the recent corpus-based POS taggers inthe literature are either statistically based, anduse Markov Model(Weischedel t al., 1993,Merialdo, 1994) or Statistical DecisionTree(Jelinek et al, 1994, Magerman, 1995)(SDT)techniques, or are primarily rule based,such as Drill's Transformation BasedLearner(Drill, 1994)(TBL).
The MaximumEntropy (MaxEnt) tagger presented in this papercombines the advantages of all these methods.
Ituses a rich feature representation, like TBL andSDT, and generates a tag probability distributionfor each word, like Decision Tree and MarkovModel techniques.5The mapping from article to annotator is in thefile doc/wsj .wht on the Treebank CDROM.6The single-annotator training data was obtainedby extracting those articles tagged by "maryann" inthe Treebank v.5 CDROM.
This training data doesnot overlap with the Development and Test set usedin the paper.
The single-annotator Development Setis the portion of the Development Set which has alsobeen annotated by "maryann".
The word vocabularyand tag dictionary are the same as in the baselineexperiment.Number of "Difficult" Words I Development Set Performance29 \] 96.49%Table 8: Performance of Baseline Model with Specialized FeaturesWord ~ Baseline Model Errors # Specialized Model Errorsthat 246 207up 186 169about 110 120out 104 97more 88 89down 81 84off 73 78as 50 38much 47 40chief 46 47in 39 39executive 37 33most 23 34ago 22 18yen 18 17Table 9: Errors on Development Set with Baseline and Specialized Models35302520POS Tag15105?
II I I I I I I I I. ,  , .
.
o ,'~$1r~.
.
?
mmL.up~ ~ .'mNNmn~.
~ gtPd l= |&.al lm~WI.LqlfIDW,t~ l IO ,  r I ~ " 1 ~ ~  ~ II, M lmulm,  ?
IP, i l~  ,,lllb, l~  ~I I I I I I I I I0 200 400 600 800 1000 1200 1400 1600 1800Article#2000Figure 1: Distribution of Tags for the word "about" vs. Article#Training Size(w?rds)I Test571190 Size(w?rds) IBasel ine44478 97.0 % Specialized 197.13%Table 10: Performance of Baseline ~ Specialized Model When Tested on Consistent Subset of DevelopmentSet139POS Tag3530252O1510501Io.
Oho?mI IIB ~ m MI I I2 3 4AnnotatorFigure 2: Distribution of Tags for the word "about" vs. Annotator(Weischedel et al, 1993) provide the resultsfrom a battery of "tri-tag" Markov Model exper-iments, in which the probability P(W,T) of ob-serving a word sequence W = {wl,w2,.. .
,wn}together with a tag sequence T = {tl,t2,.. .
,tn}is given by:P(TIW)P(W) p(tl)p(t21tl) ?H P(tilti-lti-2) p(wiltii=3Furthermore, p(wilti) for unknown words is com-puted by the following heuristic, which uses a setof 35 pre-determined ndings:p(wilti) p(unknownwordlti ) xp(capitalfeature\[ti) xp(endings, hypenationlti )This approximation works as well as theMaxEnt model, giving 85% unknown wordaccuracy(Weischedel et al, 1993) on the Wall St.Journal, but cannot be generalized to handle morediverse information sources.
Multiplying togetherall the probabilities becomes less convincing ofan approximation as the information sources be-come less independent.
In contrast, the Max-Ent model combines diverse and non-local infor-mation sources without making any independenceassumptions.140A POS tagger is one component in theSDT based statisticM parsing system describedin (Jelinek et al, 1994, Magerman, 1995).
Thetotal word accuracy on Wall St. Jour-nal data, 96.5%(Magerman, 1995), is similar tothat presented in this paper.
However, theaforementioned SDT techniques require wordclasses(Brown et al, 1992) to help prevent datafragmentation, and a sophisticated smoothing al-gorithm to mitigate the effects of any fragmenta-tion that occurs.
Unlike SDT, the MaxEnt train-ing procedure does not recursively split the data,and hence does not suffer from unreliable countsdue to data fragmentation.
As a result, no wordclasses are required and a trivial count cutoff suf-rices as a smoothing procedure in order to achieveroughly the same level of accuracy.TBL is a non-statistical approach to POStagging which also uses a rich feature repre-sentation, and performs at a total word accu-racy of 96.5% and an unknown word accuracy of85%.
(Bri11, 1994).
The TBL representation of thesurrounding word context is almost he same 7andthe TBL representation of unknown words is asuperset s of the unknown word representation ithis paper.
However, since TBL is non-statistical,it does not provide probability distributions and7(Brill, 1994) looks at words ?3 away from the cur-rent, whereas the feature set in this paper uses a win-dow of ?2.8(Brill, 1994) uses prefix/suffix additions and dele-tions, which are not used in this paper.
!unlike MaxEnt, cannot be used as a probabilis-tic component in a larger model.
MaxEnt canprovide a probability for each tagging decision,which can be used in the probability calculationof any structure that is predicted over the POStags, such as noun phrases, or entire parse trees,as in (Jelinek et al, 1994, Magerman, 1995).Thus MaxEnt has at least one advantage overeach of the reviewed POS tagging techniques.
It isbetter able to use diverse information than MarkovModels, requires less supporting techniques thanSDT, and unlike TBL, can be used in a prob-abilistic framework.
However, the POS taggingaccuracy on the Penn Wall St. Journal corpusis roughly the same for all these modelling tech-niques.
The convergence of the accuracy rateimplies that either all these techniques are miss-ing the right predictors in their representationto get the "residue", or more likely, that anycorpus based algorithm on the Penn TreebankWall St. Journal corpus will not perform muchhigher than 96.5% due to consistency problems.ConclusionThe Maximum Entropy model is an extremelyflexible technique for linguistic modelling, since itcan use a virtually unrestricted and rich featureset in the framework of a probability model.
Theimplementation in this paper is a state-of-the-artPOS tagger, as evidenced by the 96.6% accuracyon the unseen Test set, shown in Table 11.The model with specialized features does notperform much better than the baseline model, andfurther discovery or refinement of word-based fea-tures is difficult given the inconsistencies in thetraining data.
A model trained and tested on datafrom a single annotator performs at .5% higheraccuracy than the baseline model and should pro-duce more consistent input for applications thatrequire tagged text.References\[ARP, 1994\] ARPA.
1994.
Proceedings of the Hu-man Language Technology Workshop.\[Berger et al, 1996\] Adam Berger, StephenA.
Della Pietra, and Vincent J. Della Pietra.1996.
A Maximum Entropy Approach toNatural Language Processing.
ComputationalLinguistics, 22(1):39-71.\[Brill, 1994\] Eric Brill.
1994.
Some Advances inTransformation-Based Part of Speech Tagging.In Proceedings off the Twelfth National Confer-ence on Artificial Intelligence, volume 1, pages722-727.\[Brown et al, 1992\] Peter F Brown, Vincent Del-laPietra, Peter V deSouza, Jennifer C Lai, andRobert L Mercer.
1992.
Class-Based n-gramModels of Natural Language.
ComputationalLinguistics, 18(4).\[Darroch and Ratcliff, 1972\] J. N. Darroch andD.
Ratcliff.
1972.
Generalized Iterative Scalingfor Log-Linear Models.
The Annals of Mathe-matical Statistics, 43(5) :1470-1480.\[Della Pietra et al, 1995\] Steven Della Pietra,Vincent Della Pietra, and John Lafferty.
1995.Inducing Features of Random Fields.
Techni-cal Report CMU-CS95-144, School of ComputerScience, Carnegie-Mellon University.\[Jelinek et al, 1994\] F Jelinek, J Lafferty,D Magerman, R Mercer, A Ratnaparkhi, andS Roukos.
1994.
Decision Tree Parsing usinga Hidden Derivational Model.
In Proceedingsof the Human Language Technology Workshop(ARP, 1994), pages 272-277.\[Lau et al, 1993\] Ray Lau, Ronald Rosenfeld,and Salim Roukos.
1993.
Adaptive LanguageModeling Using The Maximum Entropy Prin-ciple.
In Proceedings of the Human LanguageTechnology Workshop, pages 108-113.
ARPA.\[Magerman, 1995\] David M. Magerman.
1995.Statistical Decision-Tree Models for Parsing.
InProceedings of the 33rd Annual Meeting of theACL.\[Marcus et al, 1994\] Mitchell P. Marcus, BeatriceSantorini, and Mary Ann Mareinkiewicz.
1994.Building a large annotated corpus of English:the Penn Treebank.
Computational Linguistics,19(2):313-330.\[Merialdo, 1994\] Bernard Merialdo.
1994.
Tag-ging English Text with a Probabilistic Model.Computational Linguistics, 20(2):155-172.\[Ratnaparkhi et al, 1994\] Adwait Ratnaparkhi,Jeff Reynar, and Salim Roukos.
1994.
A Maxi-mum Entropy Model for Prepositional PhraseAttachment.
In Proceedings of the HumanLanguage Technology Workshop (ARP, 1994),pages 250-255.141T?tal W?rd Accuracy I Unkn?wn W?rd Accuracy 1 9 6 .
6 3 %  85.56% Sentence AccuracY47.51% ITable 11: Performance ofSpecialized Model on Unseen Test Data\[Weischedel et al, 1993\] Ralph Weischedel, MarieMeteer, Richard Schwartz, Lance Ramshaw,and Jeff Palmucci.
1993.
Coping With Ambigu-ity and Unknown Words through ProbabilisticModels.
Computational Linguistics, 19(2):359-382.142
