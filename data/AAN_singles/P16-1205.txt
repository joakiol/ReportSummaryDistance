Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2174?2184,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsModeling Stance in Student EssaysIsaac Persing and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{persingq,vince}@hlt.utdallas.eduAbstractEssay stance classification, the task of de-termining how much an essay?s authoragrees with a given proposition, is animportant yet under-investigated subtaskin understanding an argumentative essay?soverall content.
We introduce a new cor-pus of argumentative student essays an-notated with stance information and pro-pose a computational model for automati-cally predicting essay stance.
In an evalu-ation on 826 essays, our approach signif-icantly outperforms four baselines, one ofwhich relies on features previously devel-oped specifically for stance classificationin student essays, yielding relative errorreductions of at least 11.3% and 5.3%, inmicro and macro F-score, respectively.1 IntroductionState-of-the-art automated essay scoring enginessuch as E-rater (Attali and Burstein, 2006) donot grade essay content, focusing instead on pro-viding diagnostic trait feedback on categoriessuch as grammar, usage, mechanics, style andorganization.
Hence, persuasiveness and othercontent-dependent dimensions of argumentativeessay quality are largely ignored in existing auto-mated essay scoring research.
While full-fledgedcontent-based essay scoring is still beyond thereach of state-of-the-art essay scoring engines, re-cent work has enabled us to move one step closerto this ambitious goal by analyzing essay content,attempting to determine the argumentative struc-ture of student essays (Stab and Gurevych, 2014)and the persuasiveness of the arguments made inthese essays (Persing and Ng, 2015).Stance classification is an important first step indetermining how persuasive an argumentative stu-dent essay is because persuasiveness depends onhow well the author argues w.r.t.
the stance shetakes using the supporting evidence she provides.For instance, if her stance is Agree Somewhat,a persuasive argument would involve explainingwhat reservations she has about the given propo-sition.
As another example, an argumentative es-say in which the author takes a neutral stance orthe author presents evidence that does not supportthe stance she claims to take should receive a lowpersuasiveness score.Given the important role played by stanceclassification in determining an essay?s persua-siveness, our goal in this paper is to examinestance classification in argumentative student es-says.
While there is a large body of work on stanceclassification1, stance classification in argumen-tative essays is largely under-investigated and isdifferent from previous work in several respects.First, in automated essay grading, the majority ofthe essays to be assessed are written by studentswho are learners of English.
Hence our stanceclassification task could be complicated by the au-thors?
lack of fluency in English.
Second, essaysare longer and more formally written than the texttypically used in previous stance classification re-search (e.g., debate posts).
In particular, a studentessay writer typically expresses her stance on theessay?s topic in a thesis sentence/clause, while adebate post?s author may never even explicitly ex-press her stance.
Although the explicit expressionof stance in essays seems to make our task easier,1Previous approaches to stance classification have fo-cused on three discussion/debate settings, namely congres-sional floor debates (Thomas et al, 2006; Bansal et al, 2008;Balahur et al, 2009; Yessenalina et al, 2010; Burfoot et al,2011), company-internal discussions (Agrawal et al, 2003;Murakami and Raymond, 2010), and online social, political,and ideological debates (Wang and Ros?e, 2010; Biran andRambow, 2011; Walker et al, 2012; Abu-Jbara et al, 2013;Hasan and Ng, 2013; Boltu?zi?c and?Snajder, 2014; Sobhani etal., 2015; Sridhar et al, 2015).2174Prompt Prompt PartsMost university degrees are theoretical anddo not prepare students for the real world.They are therefore of very little value.1) Most university degrees are theoretical.2) Most university degrees do not prepare students for the real world.3) Most university degrees are of very little value.The prison system is outdated.
No civilizedsociety should punish its criminals: it shouldrehabilitate them.1) The prison system is outdated.2) No civilized society should punish its criminals.3) Civilized societies should rehabilitate criminals.Table 1: Some examples of essay prompts and their associated parts.identifying stancetaking text in the midst of non-stancetaking sentences in a potentially long essay,as we will see, is by no means a trivial task.To our knowledge, the essay stance classifica-tion task has only been attempted by Faulkner(2014).
However, the version of the task weaddress is different from his.
First, Faulkneronly performed two-class stance classification:while his corpus contains essays labeled withFor (Agree), Against (Disagree), and Neither, hesimplified the task by leaving out the arguablymost difficult-to-identify stance, Neither.
In con-trast, we perform fine-grained stance classifica-tion, where we allow essay stance to take oneof six values: Agree Strongly, Agree Somewhat,Neutral, Disagree Somewhat, Disagree Strongly,and Never Addressed, given the practical need toperform fine-grained stance classification in stu-dent essays, as discussed above.
Second, giventhat many essay prompts are composed of multiplesimpler propositions (e.g., the prompt ?Most uni-versity degrees are theoretical and do not preparestudents for the real world?
has two parts, ?Mostuniversity degrees are theoretical?
and ?Most uni-versity degrees do not prepare students for thereal world.?
), we manually split such prompts intoprompt parts and determine the stance of the au-thor w.r.t.
each part, whereas Faulkner assignedan overall stance to a given prompt regardless ofwhether it is composed of multiple propositions.The distinction is important because an analysisof our annotations described in Section 2 showsthat essay authors take different stances w.r.t.
dif-ferent prompt parts in 49% of essays, and in 39%of essays, authors even take stances with differentpolarities w.r.t.
different prompt parts.In sum, our contributions in this paper are two-fold.
First, we propose a computational modelfor essay stance classification that outperformsfour baselines, including our re-implementation ofFaulkner?s approach.
Second, in order to stimulatefurther research on this task, we make our anno-tations publicly available.
Since progress on thistask is hindered in part by the lack of a publiclyannotated corpus, we believe that our data set willbe a valuable resource for the NLP community.2 CorpusWe use as our corpus the 4.5 million word Interna-tional Corpus of Learner English (ICLE) (Grangeret al, 2009), which consists of more than 6000essays on a variety of different topics written byuniversity undergraduates from 16 countries and16 native languages who are learners of English asa Foreign Language.
91% of the ICLE texts arewritten in response to prompts that trigger argu-mentative essays, and thus are expected to take astance on some issue.
We select 11 such prompts,and from the subset of argumentative essays writ-ten in response to them, we select 826 essays toannotate for training and testing our stance clas-sification system.2Table 1 shows two of the 11topics selected for annotation.We pair each of the 826 essays with each ofthe prompt parts to which it responds, resulting in1,593 instances.3We then familiarize two humanannotators, both of whom are native speakers ofEnglish, with the stance definitions in Table 2 andask them to assign each instance the stance labelthey believe the essay?s author would have chosenif asked how strongly she agrees with the promptpart.
We additionally furnish the annotators withdescriptions of situations that might cause an au-thor to select the more ambiguous classes.
For ex-ample, an author might choose Agree Somewhat ifshe appears to mostly agree with the prompt part,but qualifies her opinion in a way that is not cap-tured by the prompt part?s bluntness (e.g.
an au-thor who claims the prison system in a lot of coun-tries is outdated would Agree Somewhat with thefirst part of Table 1?s second prompt).
Or she maychoose Disagree Somewhat if she appears to dis-2See our website at http://www.hlt.utdallas.edu/?persingq/ICLE/ for the complete list of essaystance annotations.3We do not segment the essays?
texts according to whichprompt part is being responded to.
Each (entire) essay isviewed as a response to all of its associated prompt parts.2175Stance DefinitionAgreeStrongly(885)The author seems to agree with andcare about the claim.AgreeSomewhat(148)The author generally agrees withthe claim, but might be hesitant tochoose ?Agree Strongly?.Neutral (28) The author agrees with the claim asmuch as s/he disagrees with it.DisagreeSomewhat(91)The author generally disagrees withthe claim, but might be hesitant tochoose ?Disagree Strongly?.DisagreeStrongly (416)The author seems to disagree withand care about the claim.NeverAddressed(25)A stance cannot be inferred be-cause the proposition was never ad-dressed.Table 2: Stance label counts and definitions.agree with the prompt part, but mentions the dis-agreement only in passing because she does notcare much about the topic.To ensure consistency in annotation, we ran-domly select 100 essays (187 instances) for anno-tation by both annotators.
Their labels agree in84.5% of the instances, yielding a Cohen?s (1960)Kappa of 0.76.
Each case of disagreement is re-solved through discussion between the annotators.3 Baseline Stance Classification SystemsIn this section, we describe four baseline systems.3.1 Agree Strongly BaselineGiven the imbalanced stance distribution shownin Table 2, we create a simple but by no meansweak baseline, which predicts that every instancehas most frequent class label (Agree Strongly), re-gardless of the prompt part or the essay?s contents.3.2 N-Gram BaselinePrevious work on stance classification, which as-sumes that stance-annotated training data is avail-able for every topic for which stance classifica-tion is performed, has shown that the N-Grambaseline is a strong baseline.
Not only is thisassumption unrealistic in practice, but it has ledto undesirable consequences.
For instance, theproposition ?feminists have done more harm tothe cause of women than good?
elicits much moredisagreement than normal.
So, if instances fromthis proposition appeared in both the training andtest sets, the unigram feature ?feminist?
would bestrongly correlated with the disagreement classeseven though intuitively it tells us nothing aboutstance.
This partly explains why the N-Gram base-line was strong in previous work (Somasundaranand Wiebe, 2010).
In light of this problem, weperform leave-one-out cross validation where wepartition the instances by prompt, leaving the in-stances created for one prompt out in each test set.To understand how strong n-grams are whenevaluated in our leave-one-prompt-out cross-validation setting, we employ them as features inour second baseline.
Specifically, we train a mul-ticlass classifier on our data set using a feature setcomposed solely of unigram, bigram, and trigramfeatures, each of which indicates the number oftimes the corresponding n-gram is present in theassociated essay.3.3 Duplicated Faulkner BaselineWhile it is true that no system exists for solv-ing our exact problem, the system proposed byFaulkner (2014) comes fairly close.
Hence, as ourthird baseline, we train a multiclass classifier onour data set for fine-grained essay stance classifi-cation using the two types of features proposed byFaulkner, as described below.Part-of-speech (POS) generalized dependencysubtrees.
Faulkner first constructs a lexicon ofstance words in the style of Somasundaran andWiebe (2010).
The lexicon consists of (1) the setof stemmed first unigrams appearing in all stance-annotated text spans in the Multi-PerspectiveQuestion Answering (MPQA) corpus (Wiebe etal., 2005), and (2) the set of boosters (clearly, de-cidedly), hedges (claim, estimate), and engage-ment markers (demonstrate, evaluate) from the ap-pendix of Hyland (2005).
He then manually re-moves from this list any words that appear not tobe stancetaking, resulting in a 453 word lexicon.Stance words target propositions, whichFaulkner notes, usually contain some opinion-bearing language that can serve as a proxy for thetargeted proposition.
In order to find the locationsin an essay where a stance is being taken, hefirst finds each stance word in the essay.
Then hefinds the shortest path from the stance word to anopinion word in the sentence?s dependency tree,using the MPQA subjectivity lexicon of opinionwords (Wiebe et al, 2005).
If this nearest opinionword appears in the stance word?s immediate orembedded clause, he creates a binary feature byconcatenating all the words in the dependencypath, POS generalizing all words other than thestance and opinion word, and finally prepending2176?not?
if the stance word is adjacent to a negatorin the dependency tree.
Thus given the sentence?I can only say that this statement is completelytrue.?
he would add the feature can-V-true, whichsuggests agreement with the prompt.Prompt topic words.
Recall that for the previ-ous feature type, a feature was generated wheneveran opinion word occurred in a stance word?s im-mediate or embedded clause.
Each content wordin this clause is used as a binary feature if itssimilarity with one of the prompt?s content wordsmeets an empirically determined threshold.3.4 N-Gram+Duplicated Faulkner BaselineTo build a stronger baseline, we employ as ourfourth baseline a classifier trained on both n-gramfeatures and duplicated Faulkner?s features.4 Our ApproachOur approach to stance classification is a learning-based approach where we train a multiclass clas-sifier using four types of features: n-gram fea-tures (Section 3.2), duplicated Faulkner?s features(Section 3.3), and two novel types of features,stancetaking path-based features (Section 4.1) andknowledge-based features (Section 4.2).4.1 Stancetaking Path-Based FeaturesRecall that, in order to identify his POS general-ized dependency subtrees, Faulkner relies on twolexica, a lexicon of stancetaking words and a lex-icon of opinion-bearing words.
He then extractsa feature any time words from the two lexica aresyntactically close enough.
A major problem withthis approach is that the lexica are so broad thatnearly 80% of sentences in our corpus contain textthat can be identified as stancetaking using thismethod.
Intuitively, an essay may state its stancew.r.t.
a prompt part in a thesis or conclusion sen-tence, but most of essay?s text will be at most tan-gentially related to any particular prompt part.
Forthis reason, we propose to identifying stancetakingtext to target only text that appears directly relatedto the prompt part.
Below we first show how weidentify and stance-labeling relevant stancetakingdependency paths, and then describe the featureswe derive from these paths.4.1.1 Identifying relevant stancetaking pathsAs noted above, we first identify stancetaking textthat appears directly related to the prompt part.Figure 1: Automatic dependency parse of aprompt part.To begin, we note that the prompt parts them-selves must express a stance on a topic if they canbe agreed or disagreed with.
By examining the de-pendency parses4of the prompt parts, we can rec-ognize elements of how stancetaking text is struc-tured.
From the prompt part shown in Figure 1,for example, we notice that the important wordsthat express a stance in the sentence are ?money?,?root?, and ?evil?.
By analyzing the dependencystructure in this and other prompt parts, we dis-covered that stancetaking text often consists of (1)a subject word, which is the child in an nsubj ornsubjpass relation, (2) a governor word which isthe subject?s parent, and (3) an object, which isa content word from which there is a (not alwaysdirect) dependency path from the governor.
Wetherefore abstract a stance in an essay as a depen-dency path from a subject to an object that passesthrough the governor.
Thus, the stancetaking de-pendency path we identify from the prompt partshown in Figure 1 could be represented as money-root-evil.The obvious problem with identifying stanc-etaking text in this way is that nearly all sentencescontain this kind of stancetaking structure, and justas with Faulkner?s dependency paths, there is lit-tle reason to believe that any particular path isrelevant to an instance?s prompt part.
Does thismean that nearly all sentences are stancetaking?We would argue that they can be, as even sen-tences that appear on their face to be mere state-ments of fact with no apparent value judgment canbe viewed as taking a stance on the factuality ofthe statement, and people often disagree about thefactuality of statements.
For this reason, after wehave identified a stancetaking path, we must de-termine whether the stance being expressed is rel-evant to the prompt part before extracting featuresfrom it.4Dependency parsing, POS tagging, and lemmatizationare performed automatically using the Stanford CoreNLPsystem (Manning et al, 2014)2177Figure 2: Automatic dependency parse of an essaysentence.For this reason, we ignore all stancetaking pathsthat do not meet the following three relevance con-ditions.
First, the lemma of the path?s governormust match the lemma of a governor in the promptpart.
Second, the lemma of the path?s object mustmatch the lemma of some content word5in theprompt part.
Finally, the containing sentence mustnot contain a question mark or a quotation, as suchsentences are usually rhetorical in nature.
We donot require that the subject word match the promptpart?s subject word because this substantially re-duces coverage for various reasons.
For one, of thethree words (subject, governor, object), the sub-ject is the word most likely to be replaced withsome other word like a pronoun, and possibly be-cause the essays were written by non-native En-glish speakers, automatic coreference resolutioncannot reliably identify these cases.
We also donot fully trust that the subject identified by the de-pendency parser will reliably match the subject weare looking for.
Given these constraints, we canautomatically identify the ?itself-root-of-evil?
de-pendency path in Figure 2 as a relevant stancetak-ing path.4.1.2 Stance-labeling the pathsNext, we determine whether a stancetaking pathidentified in the previous step appears to agree ordisagree with the prompt part.To begin, we count the number of negations oc-curring in the prompt part.
Any word like ?no?,?not?, or ?none?
counts as a negation unless it be-gins a non-negation phrase like ?no doubt?
or ?notonly?.6Thus, the count of negations in the promptpart in Figure 1 is 0.After that, we count the number of times theidentified stancetaking path is negated.
Because5For our purpose, a content word (1) is a noun, pronoun,verb, adjective, or adverb, (2) is not a stopword, and (3) is atthe root, is a child in a dobj or pobj relation, or is the childin a conj relation whose parent is the child in a dobj or pobjrelation in the dependency tree.6See our website at http://www.hlt.utdallas.edu/?persingq/ICLE/ for our list of manually con-structed negation words and non-negation phrases.these paths occur in student essays and are there-fore often not as simply-stated as the prompt parts,this is a little bit more complicated than just count-ing the containing sentence?s negations since thesentence may contain a lot of additional material.To do this, we construct a list of all the depen-dency nodes in the stancetaking path as well as allof their dependency tree children.
We then removefrom this list any node that, in the sentence, occursafter the last node in the stancetaking path.
The to-tal negation count we are looking for is the num-ber of nodes in this list that correspond to negationwords (unless the negation word begins a negationphrase).
Thus, because the word ?not?
is the childof ?root?
in the path ?itself-root-of-evil?
we iden-tified in Figure 2, we consider this path to havebeen negated one time.Finally, we sum the prompt part negations andthe stancetaking path negations.
If this sum iseven, we believe that the relevant stancetakingpath agrees with the prompt part in the instance.
Ifit is odd, however (as in the case of the prompt partand stancetaking text in the dependency tree fig-ures), we believe that it disagrees with the promptpart.
To illustrate why we are concerned withwhether this sum is even, consider the followingexamples.
If both the prompt part and the stanc-etaking text are negated, both disagree with theopposite of the prompt part?s stance.
Thus, theyagree with each other, and their negation sum iseven (2).
If the stancetaking path was negatedtwice, however, the sum would be odd (3) due tothe stance path?s double negations canceling eachother out, and the stancetaking path would dis-agree with the prompt part.4.1.3 Deriving path-based featuresWe extract four features from the relevant stanc-etaking dependency paths identified and stance-labeled so far, as described below.The first feature encodes the count of relevantstancetaking paths that appear to agree with theprompt part.
The second feature encodes the countof relevant stancetaking paths that appear to dis-agree with the prompt part.
While we expectthese first two features to be correlated with theagreement and disagreement classes, respectively,they may not be sufficient to distinguish betweenagreeing and disagreeing instances.
It is possi-ble, for example, that both features may be greaterthan zero in a single instance if we have identi-fied one stancetaking path that appears to agree2178with the prompt part and another stancetaking paththat appears to disagree with the prompt part.
Itis not clear whether this situation is indicative ofonly the Neutral class, or perhaps it indicates par-tial (Somewhat) (Dis)Agreement, or maybe ourmethod of detecting disagreement is not reliableenough, and it therefore makes sense, when we getthese conflicting signals, to ignore them entirelyand just assign the instance to the most frequent(Agree Strongly) class.
For that matter, if neitherfeature is greater than zero, does this mean thatthe instance Never Addressed the prompt part, ordoes it instead mean that our method for identify-ing stancetaking paths doesn?t have high enoughrecall to work on all instances?
We let our learnersort these problems out by adding two more binaryfeatures to our instances, one which indicates thatboth of the first two features are zero, and one thatindicates whether both are greater than zero.4.2 Knowledge-Based FeaturesOur second feature type is composed of five lin-guistically informed binary features that corre-spond to five of the six classes in our fine-grainedstance classification task.
Intuitively, if an instancehas one of these features turned on, it should be as-signed to the feature?s corresponding class.1.
Neutral.
Stancetaking text indicating neutral-ity tends to be phrased somewhat differently thanstancetaking text indicating any other class.
Inparticular, neutral text often makes claims that areabout the prompt part?s subject, but which are tan-gential to the proposition expressed in the promptpart.
For this reason, we search the essay forwords that match the prompt part?s subject lem-matically.After identifying a sentence that is about theprompt part?s subject in this way, we checkwhether the sentence begins with any neutral indi-cating phrase.7If we find a sentence that both be-gins with a neutral phrase and is about the promptpart?s subject, we turn the Neutral feature on.Thus, sentences like the following can be cap-tured: ?In all probability university students won-der whether or not they spend their time uselesslyin studying through four or five years in order totake their degree.
?7We construct a list of neutral phrases for introduc-ing another person?s ideas from a writing skills website(http://www.myenglishteacher.eu/question/other-ways-to-say-according-to/).2.
(Dis)Agree Somewhat.
In order to set thevalues of the features associated with the Some-what classes, we first identify relevant stancetak-ing paths as described above.
We then trim the listof paths by removing any path whose governor orsubject does not have a hedge word as an adverbmodifier child in the dependency tree.8Thus, weare able to determine that the essay containing thesentence ?There is nearly no place left for dreamand imagination?
is likely to belong to one of theSomewhat classes w.r.t.
the prompt part ?There isno longer a place for dreaming and imagination.
?The question now is how to determine which (ifany) of the Somewhat classes it should belong to.We analyze all the paths from the list for nega-tion in much the same way we described above,but with one major difference.
We hypothesizethat when taking a Somewhat stance, students aremore likely to explicitly state that the stance beingtaken is their opinion rather than stating the stancebluntly without attribution.
For example, one Dis-agree Somewhat essay includes the sentence, ?Inever believed these people were honest if sayingthat money is just the root of all evil.?
In order todetermine that this sentence contains an indicationof the Disagree Somewhat class, we need to ac-count for the negation that occurs at the beginning,far away from the stancetaking path (money-root-of-evil).
To do this, we semantically parse the sen-tence using SEMAFOR (Das et al, 2010).
Each ofthe semantic frames detected by SEMAFOR de-scribes an event that occurs in a sentence, and theevent?s frame elements may be the people or otherentities that participate in the event.
One of thesemantic frames detected in this example sentencedescribes a Believer (I) and the content of his orher belief (all the text after ?believed?).
Becausethe sentence includes a semantic frame that (1)contains a first person (I, we) Cognizer, Speaker,Perceiver, or Believer element, (2) contains an el-ement that covers all the text in the dependencypath (a Content frame element, in this case), and(3) the word that triggers the frame (?believed?
)has a negator child in the dependency tree, we addone to this relevant stancetaking path?s negationcount.
This makes this hedged stancetaking path?snegation count odd, so we believe that this sen-tence likely disagrees with its instance?s promptpart somewhat.
If we find a hedged stancetaking8See our website at http://www.hlt.utdallas.edu/?persingq/ICLE/ for our manually constructedlist of hedge words.2179path with an odd negation count, we turn on theDisagree Somewhat feature.
Similarly, if we finda hedged stancetaking path with an even negationcount, we turn on the Agree Somewhat feature.3.
(Dis)Agree Strongly.
When we believe thereis strong evidence that an instance should belongto one of the Strongly classes, we turn on the cor-responding (Dis)Agree Strongly feature.
In par-ticular, if we find a relevant stancetaking path thatappears to agree with the prompt part (as describedin Section 4.1.2), but do not find any such path thatappears to disagree with it, we turn on the AgreeStrongly feature.
Similarly, if we find a relevantstancetaking path that appears to disagree with theprompt part, but do not find a relevant stancetak-ing path that appears to agree with it, we turn onthe Disagree Strongly feature.5 Evaluation5.1 Experimental SetupData partition.
All our results are obtainedvia leave-one-prompt-out cross-validation experi-ments.
So, in each fold experiment, we partitionthe instances from our 11 prompts into a trainingset (10 prompts) and a test set (1 prompt).Evaluation metrics.
We employ two metrics toevaluate our systems: (1) micro F-score, whichtreats each instance as having equal weight; and(2) macro F-score, which treats each class as hav-ing equal weight.9To gain insights into how dif-ferent systems perform on different classes, we ad-ditionally report per-class F-scores.Training.
We train the baselines and our ap-proach using two learning algorithms, MAL-LET?s (McCallum, 2002) implementation of max-imum entropy (MaxEnt) classification and ourown implementation of the one nearest neighbor(1NN) algorithm using the cosine similarity met-ric.
Note that these two learners have their ownstrengths and weaknesses: in comparison to 1NN,MaxEnt is better at exploiting high-dimensionalfeatures but less robust to skewed class distri-butions.
For the baseline systems, we selectthe learner by performing cross validation on thetraining folds to maximize the average of microand macro F-scores in each fold experiment.When training our approach, we perform ex-haustive feature selection to determine which sub-9Since stance classification is a multiclass, single-labeltask, micro F-score, precision, recall, and accuracy are allequivalent.set of the four sets of features (i.e., n-gram, dupli-cated Faulkner, path-based, and knowledge-basedfeatures) should be used.
Specifically, we selectthe feature groups and learner jointly by perform-ing cross validation on the training folds, choos-ing the combination yielding the highest averageof micro and macro F-scores in each fold experi-ment.
To prevent any feature type from dominat-ing the others, to each feature we apply a weightof one divided by the number of features havingits type.Testing.
In case of a tie when applying 1NN,the tie is broken by selecting the class appearinghigher in Table 2.5.2 Results and DiscussionResults on fine-grained essay stance classificationare shown in Table 3.
The first four rows showour baselines?
performances.
Among the fourbaselines, Always Agree Strongly performs bestw.r.t.
micro F-score, obtaining a score of 55.6%,whereas Duplicated Faulkner performs best w.r.t.macro F-score, obtaining a score of 15.6%.
De-spite its poor performance, Duplicated Faulkner isa state-of-the-art approach on this task.
Its poorperformance can be attributed to three major fac-tors.
First, it was intended to identify only Agreeand Disagree instances (note that Faulkner simplyremoved neutral instances from his experimentalsetup), which should not prevent them from per-forming well w.r.t.
micro F-score.
Second, it is fartoo permissive, generating features from a largemajority of sentences while relevant sentences arefar rarer.
Third, while it does succeed at predictingDisagree Strongly far more frequently than eitherof the other baselines that excludes the Faulknerfeature set, the problem?s class skewness meansthat a learner is much more likely to be punishedfor predicting minority classes, which are moredifficult to predict with high precision.The fact that it makes an attempt to solve theproblem rather than relying on class skewness forgood performance makes Duplicated Faulkner amore interesting baseline than either N-Gram orAlways Agree Strongly, even though both tech-nically outperform it w.r.t.
micro F-score.
Simi-larly, the statistically significant improvements inmicro and macro F-score our approach achievesover the best baselines are more impressive whentaking the skewness problem into consideration.The results of our approach, which has access2180System Micro-F Macro-F A+ A?
Neu D?
D+ Nev1 Always Agree Strongly 55.6 11.9 71.4 .0 .0 .0 .0 .02 N-Gram 55.4 12.0 71.3 .0 .0 .0 .5 .03 Duplicated Faulkner 50.8 15.6 66.8 4.0 .0 .0 22.9 .04 N-Gram + Duplicated Faulkner 53.4 15.4 69.1 2.5 .0 .0 20.6 .05 Our approach 60.6 20.1 73.6 .0 .0 2.1 44.8 .0Table 3: Cross-validation results for fine-grained essay stance classification, including per-class F-scoresfor Agree Strongly (A+), Agree Somewhat (A?
), Neutral (Neu), Disagree Somewhat (D?
), DisagreeStrongly (D+), and Never Addressed (Nev).to all four feature groups, are shown in row 5 ofthe table.
It obtains micro and macro F-scores of60.6% and 20.1%, which correspond to statisti-cally significant relative error reductions over thebest baselines of 11.3% and 5.3%, respectively.10Recall that we turned on one of our knowledge-based features only when we believed there wasstrong evidence that an instance belonged to itsassociated class.
To get an idea of how use-ful these features are, we calculate the preci-sion, recall, and F-score that would be obtainedfor each class if we treated our knowledge-basedfeatures as heuristic classifiers.
The respec-tive precisions, recalls, and F-scores we obtainedare: 0.66/0.28/0.40 (A+), 0.50/0.02/0.04 (A?
),0.00/0.00/0.00 (Neu), 0.50/0.01/0.02 (D?
), and0.63/0.31/0.42 (D+).
Since the rule predictions areencoded as features for the learner, they may notnecessarily be used by the learner even if the un-derlying rules are precise.
For instance, despitethe rule?s high precision on the Agree Somewhatclass, the learner did not make use of its predic-tions due to its low coverage.5.3 Additional ExperimentsSince all the systems we examined fared poorlyon identifying Somewhat classes, one may won-der how these systems would perform if we con-sidered a simplified version of the task where wemerged each Somewhat class with the correspond-ing Strongly class.
In particular, since Faulkner?sapproach was originally not designed to distin-guish between Strongly and Somewhat classes, itmay seem fairer to compare our approach againstDuplicated Faulkner on the four-class essay stanceclassification task, where stance can take one offour values: Agree (created by merging Agree10All significance tests are approximate randomizationtests with p < 0.01.
Boldfaced results are significant w.r.t.micro F-score for the Always Agree Strongly baseline, andmacro F-score w.r.t.
the Duplicated Faulkner baseline.Strongly and Agree Somewhat), Disagree (cre-ated by merging Disagree Strongly and DisagreeSomewhat), Neutral, and Never Addressed.In the results for the different systems on thisfour-class stance classification task, shown in Ta-ble 4, we see that the same patterns we noticed inthe six-class version of the task persist.
The ap-proaches?
relative order w.r.t.
micro and macro F-score remains the same, though they are adjustedupwards due to the problem?s increased simplicity.Our approach?s performance on Agree increases(compared to Agree Strongly) because Agree isa bigger class, making predictions of the classsafer.
Our approach?s performance decreases onDisagree (compared to Disagree Strongly) since itis not good at predicting Disagree Somewhat in-stances which are part of the class.5.4 Error AnalysisTo gain additional insights into our approach, weanalyze its six major sources of error below.Stances not presented in a straightforwardmanner.
As an example, consider ?To my opin-ion this technological progress triggers off theimagination in a certain way.?
To identify thissentence as strongly disagreeing with the propo-sition ?there is no longer a place for dreaming andimagination?, we need to understand (1) the worldknowledge that technological progress is occur-ring, (2) that ?triggers off the imagination in a cer-tain way?
means that the technological progresscoincides with imagination occurring, (3) that ifimagination is occurring, there must be ?a placefor dreaming and imagination?, and (4) that theprompt part is negated.
In general, in order to con-struct reliable features to increase our coverage ofessays that express their stance like this, we wouldneed additional world knowledge and a deeper un-derstanding of the text.Rhetorical statements occasionally misidenti-fied as stancetaking.
For example, our method2181System Micro-F Macro-F A Neu D Nev1 Always Agree Strongly 64.8 19.7 78.7 .0 .0 .02 N-Gram 64.3 19.7 78.2 .0 .8 .03 Duplicated Faulkner 62.3 25.1 75.1 .0 25.2 .04 N-Gram + Duplicated Faulkner 62.6 23.7 75.8 .0 19.0 .05 Our approach 67.6 29.1 78.5 .0 38.0 .0Table 4: Cross-validation results for four-class essay stance classification for Agree (A), Neutral (Neu),Disagree (D), and Never Addressed (Nev).for identifying stancetaking paths misidentifies ?Iam going to discuss the topic that television is theopium of the masses in modern society?
as stanc-etaking.
To handle this, we need to incorporatemore sophisticated methods for detecting rhetori-cal statements than those we are using (e.g., ignor-ing sentences ending in question marks).Negation expressed without negation words.Our techniques for capturing negation are un-able to detect when negation is expressed with-out the use of simple negation words.
For ex-ample, ?In this sense money is the root of life?should strongly disagree with ?money is the rootof all evil?.
The author replaced ?life?
with ?evil?,and detecting that this constitutes something likenegation would require semantic knowledge aboutwords that are somehow opposite of each other.Insufficient feature/heuristic coverage of theDisagree Strongly class.
Our stancetaking path-based features that we identified as intuitively hav-ing a connection to the Disagree Strongly class to-gether cover only 51% of Disagree Strongly in-stances, meaning that it is in principle impossi-ble for our system to identify the remaining 49%.However, our decision to incorporate only fea-tures that are expected to have fairly high preci-sion for some class was intentional, as the lessonwe learned from the Faulkner-based system is thatit is difficult to learn a good classifier for stanceclassification using a large number of weakly ornon-predictive features.
To solve this problem, wewould therefore need to exploit other aspects ofstrongly disagreeing essays that act as reliable pre-dictors of the class.Rarity of minority class instances.
It is per-haps not surprising that our learning-based ap-proach performs poorly on the minority classes.Even though the knowledge-based features weredesigned in part to improve the prediction of mi-nority classes, our results suggest that the result-ing features were not effectively exploited by thelearners.
To address this problem, one could em-ploy a hybrid rule-based and learning-based ap-proach where we use our machine-learned clas-sifier to classify an instance only if it cannot beclassified by any of these rules.Lack of obvious similarity between instancesof the same class.
For example, if the moststraightforward stancetaking sentence in an AgreeSomewhat instance reads something like this, ?Inconclusion, I will not go to such extremes as todeclare nihilistically that university does not pre-pare me for the real world in the least?, (given theprompt part ?Most university degrees do not pre-pare us for real life?
), and we somehow managedto identify the instance?s class as Agree Some-what, what would the instance have in commonwith other Agree Somewhat instances?
Given thenumerous ways of expressing a stance, we believea deeper understanding of essay text is required inorder automatically detect how instances like thisare similar to instances of the same class, and suchsimilarities are required for learning in general.6 ConclusionWe examined the new task of fine-grained es-say stance classification, in which we determinestance for each prompt part and allow stance totake one of six values.
We addressed this taskby proposing two novel types of features, stanc-etaking path-based features and knowledge-basedfeatures.
In an evaluation on 826 argumentativeessays, our learning-based approach, which com-bines our novel features with n-gram features andFaulkner?s features, significantly outperformedfour baselines, including our re-implementation ofFaulkner?s system.
Compared to the best base-lines, our approach yielded relative error reduc-tions of 11.3% and 5.3%, in micro and macro F-score, respectively.
Nevertheless, accurately pre-dicting the Somewhat, Neutral, and Never Ad-dressed stances remains a challenging task.
Tostimulate further research on this task, we makeall of our stance annotations publicly available.2182AcknowledgmentsWe thank the three anonymous reviewers for theirdetailed comments.
This work was supported inpart by NSF Grants IIS-1219142 and IIS-1528037.Any opinions, findings, conclusions or recommen-dations expressed in this paper are those of the au-thors and do not necessarily reflect the views or of-ficial policies, either expressed or implied, of NSF.ReferencesAmjad Abu-Jbara, Ben King, Mona Diab, andDragomir Radev.
2013.
Identifying opinion sub-groups in arabic online discussions.
In Proceed-ings of the 51st Annual Meeting of the Associationfor Computational Linguistics (Volume 2: Short Pa-pers), pages 829?835.Rakesh Agrawal, Sridhar Rajagopalan, RamakrishnanSrikant, and Yirong Xu.
2003.
Mining newsgroupsusing networks arising from social behavior.
In Pro-ceedings of the 12th International Conference onWorld Wide Web, pages 529?535.Yigal Attali and Jill Burstein.
2006.
Automated essayscoring with E-rater v.2.0.
Journal of Technology,Learning, and Assessment, 4(3).Alexandra Balahur, Zornitsa Kozareva, and Andr?esMontoyo.
2009.
Determining the polarity andsource of opinions expressed in political debates.
InProceedings of the 10th International Conference onComputational Linguistics and Intelligent Text Pro-cessing, pages 468?480.Mohit Bansal, Claire Cardie, and Lillian Lee.
2008.The power of negative thinking: Exploiting labeldisagreement in the min-cut classification frame-work.
In Proceedings of the 22nd InternationalConference on Computational Linguistics: Com-panion volume: Posters, pages 15?18.Or Biran and Owen Rambow.
2011.
Identifying justi-fications in written dialogs.
In Proceedings of the2011 IEEE Fifth International Conference on Se-mantic Computing, pages 162?168.Filip Boltu?zi?c and Jan?Snajder.
2014.
Back up yourstance: Recognizing arguments in online discus-sions.
In Proceedings of the First Workshop on Ar-gumentation Mining, pages 49?58.Clinton Burfoot, Steven Bird, and Timothy Baldwin.2011.
Collective classification of congressionalfloor-debate transcripts.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies,pages 1506?1515.Jacob Cohen.
1960.
A coefficient of agreementfor nominal scales.
Educational and PsychologicalMeasurement, 20(1):37.Dipanjan Das, Nathan Schneider, Desai Chen, andNoah A. Smith.
2010.
Probabilistic frame-semanticparsing.
In Human Language Technologies: The2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 948?956.Adam Faulkner.
2014.
Automated classification ofstance in student essays: An approach using stancetarget information and the Wikipedia link-basedmeasure.
In Proceedings of the Twenty-Seventh In-ternational Florida Artificial Intelligence ResearchSociety Conference.Sylviane Granger, Estelle Dagneaux, Fanny Meunier,and Magali Paquot.
2009. International Corpus ofLearner English (Version 2).
Presses universitairesde Louvain.Kazi Saidul Hasan and Vincent Ng.
2013.
Stanceclassification of ideological debates: Data, mod-els, features, and constraints.
In Proceedings ofthe Sixth International Joint Conference on NaturalLanguage Processing, pages 1348?1356.Ken Hyland.
2005.
Metadiscourse: Exploring interac-tion in writing.
Continuum Discourse.
Continuum,London.Andrew Kachites McCallum.
2002.
MALLET: AMachine Learning for Language Toolkit.
http://mallet.cs.umass.edu.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David Mc-Closky.
2014.
The Stanford CoreNLP natural lan-guage processing toolkit.
In Proceedings of 52ndAnnual Meeting of the Association for Computa-tional Linguistics: System Demonstrations, pages55?60.Akiko Murakami and Rudy Raymond.
2010.
Supportor oppose?
classifying positions in online debatesfrom reply activities and opinion expressions.
InProceedings of the 23rd International Conference onComputational Linguistics: Posters, pages 869?875.Isaac Persing and Vincent Ng.
2015.
Modeling ar-gument strength in student essays.
In Proceedingsof the 53rd Annual Meeting of the Association forComputational Linguistics and the 7th InternationalJoint Conference on Natural Language Processing(Volume 1: Long Papers), pages 543?552.Parinaz Sobhani, Diana Inkpen, and Stan Matwin.2015.
From argumentation mining to stance clas-sification.
In Proceedings of the 2nd Workshop onArgumentation Mining, pages 67?77.Swapna Somasundaran and Janyce Wiebe.
2010.
Rec-ognizing stances in ideological on-line debates.
InProceedings of the NAACL HLT 2010 Workshop onComputational Approaches to Analysis and Genera-tion of Emotion in Text, pages 116?124.2183Dhanya Sridhar, James Foulds, Bert Huang, LiseGetoor, and Marilyn Walker.
2015.
Joint modelsof disagreement and stance in online debate.
In Pro-ceedings of the 53rd Annual Meeting of the Associ-ation for Computational Linguistics and the 7th In-ternational Joint Conference on Natural LanguageProcessing (Volume 1: Long Papers), pages 116?125.Christian Stab and Iryna Gurevych.
2014.
Identify-ing argumentative discourse structures in persuasiveessays.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Process-ing, pages 46?56.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Get outthe vote: Determining support or opposition fromcongressional floor-debate transcripts.
In Proceed-ings of the 2006 Conference on Empirical Methodsin Natural Language Processing, pages 327?335.Marilyn Walker, Pranav Anand, Rob Abbott, and RickyGrant.
2012.
Stance classification using dialogicproperties of persuasion.
In Proceedings of the 2012Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 592?596.Yi-Chia Wang and Carolyn P. Ros?e.
2010.
Makingconversational structure explicit: Identification ofinitiation-response pairs within online discussions.In Human Language Technologies: The 2010 An-nual Conference of the North American Chapterof the Association for Computational Linguistics,pages 673?676.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions and emo-tions in language.
Language Resources and Evalu-ation, 39(2?3):165?210.Ainur Yessenalina, Yisong Yue, and Claire Cardie.2010.
Multi-level structured models for document-level sentiment classification.
In Proceedings of the2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 1046?1056.2184
