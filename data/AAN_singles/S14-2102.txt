Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 585?589,Dublin, Ireland, August 23-24, 2014.SSMT: A Machine Translation Evaluation View to Paragraph-to-SentenceSemantic SimilarityPingping HuangDepartment of Linguistic EngineeringSchool of Software and MicroelectronicsPeking University, Chinagirlhpp@163.comBaobao ChangKey Laboratory of ComputationalLinguistics, Ministry of EducationInstitute of Computational LinguisticsPeking University, Chinachbb@pku.edu.cnAbstractThis paper presents the system SSMTmeasuring the semantic similarity betweena paragraph and a sentence submitted tothe SemEval 2014 task3: Cross-level Se-mantic Similarity.
The special difficultyof this task is the length disparity betweenthe two semantic comparison texts.
Weadapt several machine translation evalua-tion metrics for features to cope with thisdifficulty, then train a regression model forthe semantic similarity prediction.
Thissystem is straightforward in intuition andeasy in implementation.
Our best run gets0.808 in Pearson correlation.
METEOR-derived features are the most effectiveones in our experiment.1 IntroductionCross level semantic similarity measures the simi-larity between different levels of text unit, for ex-ample, between a document and a paragraph, orbetween a phrase and a word.Paragraph and sentence are the natural languageunits to convey opinions or state events in dailylife.
We can see posts on forums, questions andanswers in Q&A communities and customer re-views on E-commerce websites, are mainly organ-ised in these two units.
Better similarity measure-ment across them will be helpful in clustering sim-ilar answers or reviews.The paragraph-to-sentence semantic similaritysubtask in SemEval2014 task3 (Jurgens et al.,2014) is the first semantic similarity competitionacross these two language levels.
The specialdifficulty of this task is the length disparity be-tween the compared pair: a paragraph containsThis work is licenced under a Creative Commons Attribution4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/3.67 times the words of a sentence on average inthe training set.Semantic similarity on different levels, for ex-ample, on word level (Mikolov et al., 2013), sen-tences level (B?ar et al., 2012), document level(Turney and Pantel, 2010), have been well studied,yet methods on one level can hardly be applied toa different level, let alone be applied for the cross-level tasks.
The work of Pilehvar et al.
(2013) wasan exception.
They proposed a unified method forsemantic comparison at multi-levels all the wayfrom comparing word senses to comparing textdocumentsOur work is inspired by automatic machinetranslation(MT) evaluation, in which differentmetrics are designed to compare the adequacy andfluency of a MT system?s output, called hypothe-sis, against a gold standard translation, called ref-erence.
As MT evaluation metrics measure sen-tence pair similarity, it is a natural idea to general-ize them for paragraph-sentence pair.In this paper, we follow the motivations of sev-eral MT evaluation metrics yet made adaption tocope with the length disparity difficulty of thistask, and combine these features in a regressionmodel.
Our system SSMT (Semantic Similarity inview of Machine Translation evaluation) involvesno extensive resource or strenuous computation,yet gives promising result with just a few simplefeatures.2 Regression FrameworkIn our experiment, we use features adapted fromsome MT evaluation metrics and combine themin a regression model for the semantic similaritymeasurement.
We exploit the following two sim-ple models:A linear regression model is presented as:y = w1xi+ w2xi..+ wnxn+ ?585A log-linear model is presented as:y = xw11?
xw22.. ?
xwnn?
e?Where y is the similarity score, {x1, x2.., xn} arethe feature values.We can see that in a log-linear model, if anyfeature xiget a value of 0, the output y will suck in0 forever no matter what the values other featuresget.
In our experiment we resort to smoothing toavoid this ?0-trap?
for some features (Section 4.3).3 FeaturesMT evaluation metrics vary from lexical level tosyntactic level to semantic level.
We consider onlylexical ones to avoid complicated steps like pars-ing or semantic role labelling, which are computa-tional expensive and may bring extra noise.But instead of directly using the MT evaluationmetrics, we use the factors in them as features, theidea is that the overall score of the original metricis highly related to the length of both of the com-pared pair, but its factors are often related to thelength of just one side yet still carry useful simi-larity information.3.1 BLEU-Derived FeaturesAs the most wildly used MT evaluation metric,BLEU (Papineni et al., 2002) uses the geomet-ric mean of n-gram precisions to measure the hy-potheses against references.
It is a corpus-basedand precision-based metric, and uses ?brevitypenalty?
as a replacement for recall.
Yet thispenalty is meaningless on sentence level.
There-fore we considers only the precision factors inBLEU:PnBLEU=Ngramref?NgramhyoNgramrefWe use the modified n-gram precision here andregard ?paragraph?
as ?reference?, and ?sentence?as the ?hypothesis?.
N= 1,2,3,4.
We call thesefour features BLEU-derived features.3.2 ROUGE-L-Derived FeaturesROUGE-L (Lin and Och, 2004) measures thelargest common subsequence(LCS) between acompared pair.
BLEU implies the n-gram to beconsecutive, yet ROUGE-L allows for gaps be-tween them.
By considering only in-sequencewords, ROUGE-L captures sentence level struc-ture in a natural way, then:Rlcs=LCS(ref, hyo)length(hyo)Plcs=LCS(ref, hyo)length(ref)Flcs=(1 + ?2)RlcsPlcs)Rlcs+ ?2PlcsWhere LCS(ref, hoy) is the length of LCS of thecompared pair.
We set ?
= 1, which means wedon?t want to make much distinction between the?reference?
and ?hypothesis?
here.
We call thesethree features ROUGE-L-derived features.3.3 ROUGE-S-Derived FeaturesROUGE-S (Lin and Och, 2004) uses skip-bigramco-occurrence statistics for similarity measure-ment.
One advantage of skip-bigram over BLEUis that it does not require consecutive matches butis still sensitive to word order.
Given the referenceof length n, and hypothesis of length m, then:Pskip2=skip2(ref, hyo)C(m, 2)Rskip2=skip2(ref, hyo)C(n, 2)Fskip2=(1 + ?2)Pskip2Rskip2Rskip2+ ?2Pskip2Where C is combination, and skip2(ref, hyo) isthe number of common skip-bigrams.
We alsoset ?
= 1 here, and call these three indicatorsROUGE-S-derived features.3.4 METEOR-Derived FeaturesMETEOR (Banerjee and Lavie, 2005) evaluatesa hypothesis by aligning it to a reference trans-lation and gives sentence-level similarity scores.It uses a generalized concept of unigram mappingthat matches words in the following types: ex-act match on words surface forms , stem matchon words stems, synonym match according to thesynonym sets in WordNet, and paraphrase match(Denkowski and Lavie, 2010).METEOR also makes distinction between con-tent words and function words.
Each type ofmatchmiis weighted by wi, let (mi(hc),mi(hf))be the number of content and function wordscovered by this type in the hypothesis, and586(mi(rc),mi(rf)) be the counts in the reference,then:P =?i=1wi?
(?
?mi(hi) + (1?
?)
?mi(hf))??
| hc| +(1?
?)?
| hf|R =?i=1wi?
(?
?mi(ri) + (1?
?)
?mi(rf))??
| rc| +(1?
?)?
| rf|Fmean=P ?R?P + (1?
?
)RTo account for word order difference, the frag-mentation penalty is calculated using the totalnumber of matched words(m) and the number ofchunks1(ch) in the hypothesis:Pen = ?
?
(chm)?And the final METEOR score is:Score = (1?
Pen) ?
FmeanParameters ?, ?, ?, ?and wi...wnare tuned tomaximize correlation with human judgements(Denkowski and Lavie, 2014).
We use Meteor1.5system2for scoring.
Parameters are tuned onWMT12, and the paraphrase table is extracted onthe WMT data.We use the p, r, frag(frag = ch/m) andscore as features and call them METEOR-derivedfeatures.4 Experiment and Discussion4.1 Data SetThe SemEval2014 task3 subtask gives a train-ing set of 500 paragraph-sentence pairs, with hu-man annotated continuous score of 0 ?
4.
Thesepairs are labelled with genres of ?Newswire/ cqa3/metaphoric/ scientific/ travel/ review?.
Systemsare asked to predict the similarity scores for 500pairs in the test set.
Performance is evaluated inPearson correlation and Spearman correlation.4.2 Data ProcessingTo avoid meaningless n-gram match ?the a?, orwords surface form difference, we employ verysimple data processings here: for features derivedfrom BLEU, ROUGE-L and ROUGE-S, we re-move stop words and stem the sentences with1Chunk is defined as a series of matched unigrams that iscontiguous and identically ordered in both sentences2https://www.cs.cmu.edu/ alavie/METEOR/3cqa:Community Question Answering site textcoreNLP4.
For METEOR-derived features, we usethe tool?s option for text normalization beforematching.4.3 ResultThough texts with different genres may have dif-ferent regression parameters, we just train onemodel for all for simplicity.
Table 1 comparesthe result.
Run1 is submitted as SSMT in theofficial evaluation.
It?s a log-linear model.
Wechoose more dense features for log-linear modeland use smoothing to avoid the ?0-trap?
men-tioned in (Section 2).
The features includeP1,2BLEU,PROUGE?L,PROUGE?S4 features, and4 METEOR-derived features, altogether 8 fea-tures.
When calculation the first 4 features, weplus 1 to both numerator and denominator assmoothing.
Run2 is a linear-regression model withthe same features as Run1.
Run3 is a simple linearregression model, which is free from the ?0-trap?,thus we use all the 14 features without smoothing.We use Matlab for regression.
The baseline is of-ficially given using LCS.Run Regression Pearson SpearmanBaseline LCS 0.527 0.613run1 log-linear 0.789 0.777run2 linear 0.794 0.777run3 linear 0.808 0.792Table 1: System Performance.4.4 System AnalysisWe compares the effectiveness of different fea-tures in a linear regression model.
Table 2shows the result.
?All?
refers to all the fea-tures, ?-METEOR?
means the feature set ex-cludes METEOR-derived features.
We can see theMETEOR-derived features are the most effectiveones here.Figure 1 shows the performance of our sys-tem submitted as SSMT in the SemEval2014 task3competition.
It shows quite good correlation withthe gold standard.A well predicted example is the #trial-p2s-5 pairin the trial set:Paragraph: Olympic champion Usain Bolt re-gained his 100m world title and won a fourth in-dividual World Championships gold with a sea-son?s best of 9.77 seconds in Moscow.
In heavy4http://nlp.stanford.edu/software/corenlp.shtml587Feature Pearson SpearmanAll 0.808 0.792- METEOR 0.772 0.756- ROUGE-L 0.802 0.789- ROUGE-S 0.807 0.793- BLEU 0.807 0.790Table 2: Effectiveness of Different Features.?-METEOR?
means the feature set excludingMETEOR-derived features.Figure 1: Result Scatter of SSMT.rain, the 26-year-old Jamaican made amends forhis false start in Daegu two years ago and fur-ther cemented his status as the greatest sprinterin history.
The six-time Olympic champion over-took Justin Gatlin in the final stages, forcing theAmerican to settle for silver in 9.85.
Bolt?s com-patriot Nesta Carter (9.95) claimed bronze, whileBritain?s James Dasaolu was eighth (10.21).Sentence: Germany?s Robert Harting beatsIran?s Ehsan Hadadi and adds the Olympic discustitle to his world crown.The system gives a prediction of 1.253 againstthe gold standard 1.25.
We can see that topicwords like ?Olympic?
, ?world crown?, ?beats?
inthe short text correspond to expressions of ?worldtitle ?
, ?champion?
across several sentences in thelong text, but this pair of texts are not talking aboutthe same event.
The model captures and modelsthis commonness and difference very well .But Figure 1 also reveals an interesting phe-nomenon: the system seldom gives the boundaryscores of 0 or 4.
In other words, it tends to over-score or underscore the boundary conditions.
Anexample in point is the #trial-p2s-17 pair in thetrial data, it is actually the worst predicted pair byour system in the trail set:Paragraph: A married couple who met at workis not a particularly rare thing.
Three in ten work-ers who have dated a colleague said in a recentsurvey by CareerBuilder.com that their office ro-mance eventually led to marriage.Sentence: Marrying a coworker isn?t uncom-mon given that 30% of workers who dated acoworker ended up marrying them.The system gives a 1.773 score against the goldstandard of 4.
It should fail to detect the equalityof expressions between ?three in ten?
and ?30%?.Thus better detection of phrase similarity is de-sired.
We think this is the main reason to under-score the similarity.
For test pairs with the genre of?Metaphoric?, the system almost underscores allof them.
This failure has been expected, though.Because ?Metaphoric?
pairs demand full under-standing of the semantic meaning and paragraphstructure, which is far beyond the reach of lexicalmatch metrics.5 ConclusionMT evaluation metrics have been directly usedas features in paraphrase (Finch et al., 2005) de-tection and sentence pair semantic comparison(Souza et al., 2012).
But paragraph-to-sentencepair faces significant length disparity, we try a wayout to alleviate this impact yet still follow the mo-tivations underlying these metrics.
By factorizingdown the original metrics, the linear model canflexibly pick out factors that are not sensitive tothe length disparity problem.We derive features from BLEU, ROUGE-L, ROUGE-S and METEOR, and show thatMETEOR-derived features make the most signifi-cant contributions here.
Being easy and light, oursubmitted SSMT achieves 0.789 in Pearson and0.777 in Spearman correlation, and ranks 11 outof the 34 systems in this subtask.
Our best tryachieves 0.808 in Pearson and 0.786 in Spearmancorrelation.AcknowledgementsThis work is supported by National Natu-ral Science Foundation of China under GrantNo.61273318 and National Key Basic ResearchProgram of China 2014CB340504.588ReferencesAndrew Finch, Yong S. Hwang, Eiichiro Sumita.
Us-ing machine translation evaluation techniques to de-termine sentence-level semantic equivalence.
Pro-ceedings of the Third International Workshop onParaphrasing(IWP2005), 2005: 17-24.Chin Y. Lin,Franz J. Och.
Automatic evaluation of ma-chine translation quality using longest common sub-sequence and skip-bigram statistics.
Proceedings ofthe 42nd Annual Meeting on Association for Com-putational Linguistics.
ACL, 2004: 605.Daniel B?ar, Chris Biemann, Iryna Gurevych, et al.Ukp: Computing semantic textual similarity bycombining multiple content similarity measures.Proceedings of the First Joint Conference on Lexicaland Computational Semantics-Volume 1: Proceed-ings of the main conference and the shared task, andVolume 2: Proceedings of the Sixth InternationalWorkshop on Semantic Evaluation.
ACL, 2012: 435-440.David Jurgens, Mohammad Taher Pilehvar, andRoberto Navigli.
SemEval-2014 Task 3: Cross-Level Semantic Similarity.
In Proceedings of the8th International Workshop on Semantic Evaluation(SemEval-2014)., August 23-24, 2014, Dublin, Ire-land.George Miller,Christiane Fellbaum.
WordNet.http://wordnet.princton.edu/, 2007.Jos?e G C de Souza, Matteo Negri, Yashar Mehdad.FBK: machine translation evaluation and word sim-ilarity metrics for semantic textual similarity.
Pro-ceedings of the First Joint Conference on Lexicaland Computational Semantics-Volume 1: Proceed-ings of the main conference and the shared task, andVolume 2: Proceedings of the Sixth InternationalWorkshop on Semantic Evaluation.
ACL, 2012: 624-630.Kishore Papineni, Salim Roukos, Todd Ward, et al.BLEU: a method for automatic evaluation of ma-chine translation.
Proceedings of the 40th annualmeeting on association for computational linguis-tics.
ACL, 2002: 311-318.Michael Denkowski, Alon Lavie.
Extending the ME-TEOR machine translation evaluation metric to thephrase level.
Human Language Technologies: The2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics.
ACL, 2010: 250-253.Michael Denkowski, Alon Lavie.
Meteor Univer-sal: Language Specific Translation /Evaluation forAny Target Language translation.
Proceedings of theEACL 2014 Workshop on Statistical Machine Trans-lation, 2014.Mohammad T Pilehvar, David Jurgens, Roberto Nav-igli.
Align, Disambiguate and Walk: A UnifiedApproach for Measuring Semantic Similarity Pro-ceedings of the 51st Annual Meeting of the Asso-ciation for Computational Linguistics,ACL, 2013:1341-1351.Peter D. Turney and Patrick Pantel.
From frequency tomeaning: Vector space models of semantics Artifi-cial Intelligence Research, 2010.
37(1): 141-188Satanjeev Banerjee, Alon Lavie.
METEOR: an auto-matic metric for MT Evaluation with improved cor-relation with human judgements.
Proceedings of theACL Workshop on Intrinsic and Extrinsic EvaluationMeasures for Machine Translation and/or Summa-rization., 2005: 65-72.Tomas Mikolov, Kai Chen, Greg Corrado, et al.
Ef-ficient estimation of word representations in vectorspace.
2013. arXiv preprint arXiv:1301.3781,589
