Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 1?4,New York, June 2006. c?2006 Association for Computational LinguisticsFactored Neural Language ModelsAndrei AlexandrescuDepartment of Comp.
Sci.
Eng.University of Washingtonandrei@cs.washington.eduKatrin KirchhoffDepartment of Electrical EngineeringUniversity of Washingtonkatrin@ee.washington.eduAbstractWe present a new type of neural proba-bilistic language model that learns a map-ping from both words and explicit wordfeatures into a continuous space that isthen used for word prediction.
Addi-tionally, we investigate several ways ofderiving continuous word representationsfor unknown words from those of knownwords.
The resulting model significantlyreduces perplexity on sparse-data taskswhen compared to standard backoff mod-els, standard neural language models, andfactored language models.1 IntroductionNeural language models (NLMs) (Bengio et al,2000) map words into a continuous representationspace and then predict the probability of a wordgiven the continuous representations of the preced-ing words in the history.
They have previously beenshown to outperform standard back-off models interms of perplexity and word error rate on mediumand large speech recognition tasks (Xu et al, 2003;Emami and Jelinek, 2004; Schwenk and Gauvain,2004; Schwenk, 2005).
Their main drawbacks arecomputational complexity and the fact that only dis-tributional information (word context) is used togeneralize over words, whereas other word prop-erties (e.g.
spelling, morphology etc.)
are ignoredfor this purpose.
Thus, there is also no principledway of handling out-of-vocabulary (OOV) words.Though this may be sufficient for applications thatuse a closed vocabulary, the current trend of portingsystems to a wider range of languages (esp.
highly-inflected languages such as Arabic) calls for dy-namic dictionary expansion and the capability of as-signing probabilities to newly added words withouthaving seen them in the training data.
Here, we in-troduce a novel type of NLM that improves gener-alization by using vectors of word features (stems,affixes, etc.)
as input, and we investigate derivingcontinuous representations for unknown words fromthose of known words.2 Neural Language ModelsP(w  | w    ,w     )t?2t?1tMihoWih Whod columns|V| rowsd = continuous space sizeV = vocabularyn?2wn?1wFigure 1: NLM architecture.
Each word in the context mapsto a row in the matrix M .
The output is next word?s probabilitydistribution.A standard NLM (Fig.
1) takes as input the previ-ous n ?
1 words, which select rows from a continu-ous word representation matrix M .
The next layer?sinput i is the concatenation of the rows in M cor-responding to the input words.
From here, the net-work is a standard multi-layer perceptron with hid-den layer h = tanh(i ?
Wih + bh) and output layero = h ?
Who + bo.
where bh,o are the biases on therespective layers.
The vector o is normalized by thesoftmax function fsoftmax(oi) = eoiP|V |k=1 eok.
Back-propagation (BKP) is used to learn model parame-1ters, including the M matrix, which is shared acrossinput words.
The training criterion maximizes theregularized log-likelihood of the training data.3 Generalization in Language ModelsAn important task in language modeling is to pro-vide reasonable probability estimates for n-gramsthat were not observed in the training data.
Thisgeneralization capability is becoming increasinglyrelevant in current large-scale speech and NLP sys-tems that need to handle unlimited vocabularies anddomain mismatches.
The smooth predictor func-tion learned by NLMs can provide good generaliza-tion if the test set contains n-grams whose individ-ual words have been seen in similar context in thetraining data.
However, NLMs only have a simplis-tic mechanism for dealing with words that were notobserved at all: OOVs in the test data are mappedto a dedicated class and are assigned the singletonprobability when predicted (i.e.
at the output layer)and the features of a randomly selected singletonword when occurring in the input.
In standard back-off n-gram models, OOVs are handled by reserv-ing a small fixed amount of the discount probabil-ity mass for the generic OOV word and treating itas a standard vocabulary item.
A more powerfulbackoff strategy is used in factored language models(FLMs) (Bilmes and Kirchhoff, 2003), which viewa word as a vector of word features or ?factors?
:w = ?f1, f2, .
.
.
, fk?
and predict a word jointlyfrom previous words and their factors: A general-ized backoff procedure uses the factors to provideprobability estimates for unseen n-grams, combin-ing estimates derived from different backoff paths.This can also be interpreted as a generalization ofstandard class-based models (Brown et al, 1992).FLMs have been shown to yield improvements inperplexity and word error rate in speech recogni-tion, particularly on sparse-data tasks (Vergyri etal., 2004) and have also outperformed backoff mod-els using a linear decomposition of OOVs into se-quences of morphemes.
In this study we use factorsin the input encoding for NLMs.4 Factored Neural Language ModelsNLMs define word similarity solely in terms of theircontext: words are assumed to be close in the contin-uous space if they co-occur with the same (subset of)words.
But similarity can also be derived from wordshape features (affixes, capitalization, hyphenationetc.)
or other annotations (e.g.
POS classes).
Theseallow a model to generalize across classes of wordsbearing the same feature.
We thus define a factoredneural language model (FNLM) (Fig.
2) which takesas input the previous n ?
1 vectors of factors.
Dif-ferent factors map to disjoint row sets of the ma-trix.
The h and o layers are identical to the standardNLM?s.
Instead of predicting the probabilities forn?1f2f 1n?1n?1f3?
|V  | rowsMihoWih Whon?2f13n?2ffn?22P(c   | c    ,c      ) t t?1 t?2P(w  |c   )t td columnsd = continuous space sizekkkV  =vocabulary of factor kFigure 2: FNLM architecture.
Input vectors consisting ofword and feature indices are mapped to rows in M. The finalmultiplicative layer outputs the word probability distribution.all words at the output layer directly, we first groupwords into classes (obtained by Brown clustering)and then compute the conditional probability of eachword given its class: P (wt) = P (ct) ?
P (wt|ct).This is a speed-up technique similar to the hierarchi-cal structuring of output units used by (Morin andBengio, 2005), except that we use a ?flat?
hierar-chy.
Like the standard NLM, the network is trainedto maximize the log-likelihood of the data.
We useBKP with cross-validation on the development setand L2 regularization (the sum of squared weightvalues penalized by a parameter ?)
in the objectivefunction.5 Handling Unknown Factors in FNLMsIn an FNLM setting, a subset of a word?s factors maybe known or can be reliably inferred from its shapealthough the word itself never occurred in the train-ing data.
The FNLM can use the continuous repre-sentation for these known factors directly in the in-put.
If unknown factors are still present, new contin-uous representations are derived for them from thoseof known factors of the same type.
This is done byaveraging over the continuous vectors of a selectedsubset of the words in the training data, which placesthe new item in the center of the region occupied by2the subset.
For example, proper nouns constitute alarge fraction of OOVs, and using the mean of therows in M associated with words with a proper nountag yields the ?average proper noun?
representationfor the unknown word.
We have experimented withthe following strategies for subset selection: NULL(the null subset, i.e.
the feature vector componentsfor unknown factors are 0), ALL (average of allknown factors of the same type); TAIL (averagingover the least frequently encountered factors of thattype up to a threshold of 10%); and LEAST, i.e.
therepresentation of the single least frequent factors ofthe same type.
The prediction of OOVs themselvesis unaffected since we use a factored encoding onlyfor the input, not for the output (though this is a pos-sibility for future work).6 Data and Baseline SetupWe evaluate our approach by measuring perplex-ity on two different language modeling tasks.
Thefirst is the LDC CallHome Egyptian Colloquial Ara-bic (ECA) Corpus, consisting of transcriptions ofphone conversations.
ECA is a morphologicallyrich language that is almost exclusively used in in-formal spoken communication.
Data must be ob-tained by transcribing conversations and is thereforevery sparse.
The present corpus has 170K wordsfor training (|V | = 16026), 32K for development(dev), 17K for evaluation (eval97).
The data waspreprocessed by collapsing hesitations, fragments,and foreign words into one class each.
The corpuswas further annotated with morphological informa-tion (stems, morphological tags) obtained from theLDC ECA lexicon.
The OOV rates are 8.5% (de-velopment set) and 7.7% (eval97 set), respectively.Model ECA (?102) Turkish (?102)dev eval dev evalbaseline 3gram 4.108 4.128 6.385 6.438hand-optimized FLM 4.440 4.327 4.269 4.479GA-optimized FLM 4.325 4.179 6.414 6.637NLM 3-gram 4.857 4.581 4.712 4.801FNLM-NULL 5.672 5.381 9.480 9.529FNLM-ALL 5.691 5.396 9.518 9.555FNLM-TAIL 10% 5.721 5.420 9.495 9.540FNLM-LEAST 5.819 5.479 10.492 10.373Table 1: Average probability (scaled by 102) of known wordswith unknown words in order-2 contextThe second corpus consists of Turkish newspa-per text that has been morphologically annotated anddisambiguated (Hakkani-Tu?r et al, 2002), thus pro-viding information about the word root, POS tag,number and case.
The vocabulary size is 67510(relatively large because Turkish is highly aggluti-native).
400K words are used for training, 100Kfor development (11.8% OOVs), and 87K for test-ing (11.6% OOVs).
The corpus was preprocessed byremoving segmentation marks (titles and paragraphboundaries).7 Experiments and ResultsWe first investigated how the different OOV han-dling methods affect the average probability as-signed to words with OOVs in their context.
Ta-ble 1 shows that average probabilities increase com-pared to the strategy described in Section 3 aswell as other baseline models (standard backoff tri-grams and FLM, further described below), with thestrongest increase observed for the scheme using theleast frequent factor as an OOV factor model.
Thisstrategy is used for the models in the following per-plexity experiments.We compare the perplexity of word-based andfactor-based NLMs with standard backoff trigrams,class-based trigrams, FLMs, and interpolated mod-els.
Evaluation was done with (the ?w/unk?
columnin Table 2) and without (the ?no unk?
column) scor-ing of OOVs, in order to assess the usefulness of ourapproach to applications using closed vs. open vo-cabularies.
The baseline Model 1 is a standard back-off 3-gram using modified Kneser-Ney smoothing(model orders beyond 3 did not improve perplex-ity).
Model 2 is a class-based trigram model withBrown clustering (256 classes), which, when inter-polated with the baseline 3-gram, reduces the per-plexity (see row 3).
Model 3 is a 3-gram word-basedNLM (with output unit clustering).
For NLMs,higher model orders gave improvements, demon-strating their better scalability: for ECA, a 6-gram(w/o unk) and a 5-gram (w/unk) were used; for Turk-ish, a 7-gram (w/o unk) and a 5-gram (w/unk) wereused.
Though worse in isolation, the word-basedNLMs reduce perplexity considerably when interpo-lated with Model 1.
The FLM baseline is a hand-optimized 3-gram FLM (Model 5); we also testedan FLM optimized with a genetic algorithm as de-3# Model ECA dev ECA eval Turkish dev Turkish evalno unk w/unk no unk w/unk no unk w/unk no unk w/unk1 Baseline 3-gram 191 176 183 172 827 569 855 5862 Class-based LM 221 278 219 269 1642 1894 1684 19303 1) & 2) 183 169 178 167 790 540 814 5554 Word-based NLM 208 341 204 195 1510 1043 1569 10675 1) & 4) 178 165 173 162 758 542 782 5576 Word-based NLM 202 194 204 192 1991 1369 2064 13867 1) & 6) 175 162 173 160 754 563 772 5808 hand-optimized FLM 187 171 178 166 827 595 854 6149 1) & 8) 182 167 174 163 805 563 832 58110 genetic FLM 190 188 181 188 761 1181 776 117911 1) & 10) 183 166 175 164 706 488 720 49812 factored NLM 189 173 190 175 1216 808 1249 83213 1) & 12) 169 155 168 155 724 487 744 50014 1) & 10) & 12) 165 155 165 154 652 452 664 461Table 2: Perplexities for baseline backoff LMs, FLMs, NLMs, and LM interpolationscribed in (Duh and Kirchhoff, 2004) (Model 6).Rows 7-10 of Table 2 display the results.
Finally, wetrained FNLMs with various combinations of fac-tors and model orders.
The combination was opti-mized by hand on the dev set and is therefore mostcomparable to the hand-optimized FLM in row 8.The best factored NLM (Model 7) has order 6 forboth ECA and Turkish.
It is interesting to note thatthe best Turkish FNLM uses only word factors suchas morphological tag, stem, case, etc.
but not theactual words themselves in the input.
The FNLMoutperforms all other models in isolation except theFLM; its interpolation with the baseline (Model 1)yields the best result compared to all previous inter-polated models, for both tasks and both the unk andno/unk condition.
Interpolation of Model 1, FLMand FNLM yields a further improvement.
The pa-rameter values of the (F)NLMs range between 32and 64 for d, 45-64 for the number of hidden units,and 362-1024 for C (number of word classes at theoutput layer).8 ConclusionWe have introduced FNLMs, which combine neu-ral probability estimation with factored word repre-sentations and different ways of inferring continuousword features for unknown factors.
On sparse-dataArabic and Turkish language modeling task FNLMswere shown to outperform all comparable models(standard backoff 3-gram, word-based NLMs) ex-cept FLMs in isolation, and all models when inter-polated with the baseline.
These conclusions applyto both open and closed vocabularies.AcknowledgmentsThis work was funded by NSF under grant no.
IIS-0326276 and DARPA under Contract No.
HR0011-06-C-0023.
Any opinions, findings and conclusionsor recommendations expressed in this material arethose of the author(s) and do not necessarily reflectthe views of these agencies.ReferencesY.
Bengio, R. Ducharme, and P. Vincent.
2000.
A neuralprobabilistic language model.
In NIPS.J.A.
Bilmes and K. Kirchhoff.
2003.
Factored lan-guage models and generalized parallel backoff.
InHLT-NAACL.P.
F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai,and R. L. Mercer.
1992.
Class-based n-gram modelsof natural language.
Computational Linguistics, 18(4).K.
Duh and K. Kirchhoff.
2004.
Automatic learning oflanguage model structure.
In COLING 2004.A.
Emami and F. Jelinek.
2004.
Exact training of a neu-ral syntactic language model.
In ICASSP 2004.D.
Hakkani-Tu?r, K. Oflazer, and G. Tu?r.
2002.
Statisticalmorphological disambiguation for agglutinative lan-guages.
Journal of Computers and Humanities, 36(4).F.
Morin and Y. Bengio.
2005.
Hierarchical probabilisticneural network language model.
In AISTATS.H.
Schwenk and J.L.
Gauvain.
2004.
Neural networklanguage models for conversational speech recogni-tion.
In ICSLP 2004.H.
Schwenk.
2005.
Training neural network languagemodels on very large corpora.
In HLT/EMNLP.D.
Vergyri, K. Kirchhoff, K. Duh, and A. Stolcke.2004.
Morphology-based language modeling for ara-bic speech recognition.
In ICSLP.P.
Xu, A. Emami, and F. Jelinek.
2003.
Training connec-tionist models for the structured language model.
InEMNLP 2003.4
