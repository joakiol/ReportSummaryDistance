Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 113?117,Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational LinguisticsIllinois-Coref: The UI System in the CoNLL-2012 Shared TaskKai-Wei Chang Rajhans Samdani Alla Rozovskaya Mark Sammons Dan RothUniversity of Illinois at Urbana-Champaign{kchang10|rsamdan2|rozovska|mssammon|danr}@illinois.eduAbstractThe CoNLL-2012 shared task is an extensionof the last year?s coreference task.
We partici-pated in the closed track of the shared tasks inboth years.
In this paper, we present the im-provements of Illinois-Coref system from lastyear.
We focus on improving mention detec-tion and pronoun coreference resolution, andpresent a new learning protocol.
These newstrategies boost the performance of the systemby 5% MUC F1, 0.8% BCUB F1, and 1.7%CEAF F1 on the OntoNotes-5.0 developmentset.1 IntroductionCoreference resolution has been a popular topic ofstudy in recent years.
In the task, a system requiresto identify denotative phrases (?mentions?)
and tocluster the mentions into equivalence classes, so thatthe mentions in the same class refer to the same en-tity in the real world.Coreference resolution is a central task in theNatural Language Processing research.
Both theCoNLL-2011 (Pradhan et al, 2011) and CoNLL-2012 (Pradhan et al, 2012) shared tasks focus onresolving coreference on the OntoNotes corpus.
Wealso participated in the CoNLL-2011 shared task.Our system (Chang et al, 2011) ranked first in twoout of four scoring metrics (BCUB and BLANC),and ranked third in the average score.
This year,we further improve the system in several respects.In Sec.
2, we describe the Illinois-Coref systemfor the CoNLL-2011 shared task, which we take asthe baseline.
Then, we discuss the improvementson mention detection (Sec.
3.1), pronoun resolu-tion (Sec.
3.2), and learning algorithm (Sec.
3.3).Section 4 shows experimental results and Section 5offers a brief discussion.2 Baseline SystemWe use the Illinois-Coref system from CoNLL-2011as the basis for our current system and refer to it asthe baseline.
We give a brief outline here, but fo-cus on the innovations that we developed; a detaileddescription of the last year?s system can be found in(Chang et al, 2011).The Illinois-Coref system uses a machine learn-ing approach to coreference, with an inference pro-cedure that supports straightforward inclusion of do-main knowledge via constraints.The system first uses heuristics based on NamedEntity recognition, syntactic parsing, and shallowparsing to identify candidate mentions.
A pair-wise scorer w generates compatibility scores wuvfor pairs of candidate mentions u and v using ex-tracted features ?
(u, v) and linguistic constraints c.wuv = w ?
?
(u, v) + c(u, v) + t, (1)where t is a threshold parameter (to be tuned).
Aninference procedure then determines the optimal setof links to retain, incorporating constraints that mayoverride the classifier prediction for a given mentionpair.
A post-processing step removes mentions insingleton clusters.Last year, we found that a Best-Link decodingstrategy outperformed an All-Link strategy.
TheBest-Link approach scans candidate mentions in adocument from left to right.
At each mention, if cer-tain conditions are satisfied, the pairwise scores ofall previous mentions are considered, together withany constraints that apply.
If one or more viable113links is available, the highest-scoring link is selectedand added to the set of coreference links.
After thescan is complete, the transitive closure of edges istaken to generate the coreference clusters, each clus-ter corresponding to a single predicted entity in thedocument.The formulation of this best-link solution is as fol-lows.
For two mentions u and v, u < v indicatesthat the mention u precedes v in the document.
Letyuv be a binary variable, such that yuv = 1 only ifu and v are in the same cluster.
For a document d,Best-Link solves the following formulation:argmaxy?u,v:u<vwuvyuvs.t?u<vyuv ?
1 ?v,yuw ?
{0, 1}.(2)Eq.
(2) generates a set of connected componentsand the set of mentions in each connected compo-nent constitute an entity.
Note that we solve theabove Best-Link inference using an efficient algo-rithm (Bengtson and Roth, 2008) which runs in timequadratic in the number of mentions.3 Improvements over the Baseline SystemBelow, we describe improvements introduced to thebaseline Illinois-Coref system.3.1 Mention DetectionMention detection is a crucial component of an end-to-end coreference system, as mention detection er-rors will propagate to the final coreference chain.Illinois-Coref implements a high recall and lowprecision rule-based system that includes all nounphrases, pronouns and named entities as candidatementions.
The error analysis shows that there aretwo main types of errors.Non-referential Noun Phrases.
Non-referentialnoun phrases are candidate noun phrases, identifiedthrough a syntactic parser, that are unlikely to re-fer to any entity in the real world (e.g., ?the sametime?).
Note that because singleton mentions are notannotated in the OntoNotes corpus, such phrases arenot considered as mentions.
Non-referential nounphrases are a problem, since during the coreferencestage they may be incorrectly linked to a valid men-tion, thereby decreasing the precision of the system.To deal with this problem, we use the training datato count the number of times that a candidate nounphrase happens to be a gold mention.
Then, we re-move candidate mentions that frequently appear inthe training data but never appear as gold mentions.Relaxing this approach, we also take the predictedhead word and the words before and after the men-tion into account.
This helps remove noun phrasesheaded by a preposition (e.g., the noun ?fact?
in thephrase ?in fact?).
This strategy will slightly degradethe recall of mention detection, so we tune a thresh-old learned on the training data for the mention re-moval.Incorrect Mention Boundary.
A lot of errors inmention detection happen when predicting mentionboundaries.
There are two main reasons for bound-ary errors: parser mistakes and annotation incon-sistencies.
A mistake made by the parser may bedue to a wrong attachment or adding extra wordsto a mention.
For example, if the parser attachesthe relative clause inside of the noun phrase ?Pres-ident Bush, who traveled to China yesterday?
to adifferent noun, the algorithm will predict ?PresidentBush?
as a mention instead of ?President Bush, whotraveled to China yesterday?
; thus it will make an er-ror, since the gold mention also includes the relativeclause.
In this case, we prefer to keep the candi-date with a larger span.
On the other hand, we maypredict ?President Bush at Dayton?
instead of ?Pres-ident Bush?, if the parser incorrectly attaches theprepositional phrase.
Another example is when ex-tra words are added, as in ?Today President Bush?.A correct detection of mention boundaries is cru-cial to the end-to-end coreference system.
The re-sults in (Chang et al, 2011, Section 3) show that thebaseline system can be improved from 55.96 avg F1to 56.62 in avg F1 by using gold mention boundariesgenerated from a gold annotation of the parsing treeand the name entity tagging.
However, fixing men-tion boundaries in an end-to-end system is difficultand requires additional knowledge.
In the currentimplementation, we focus on a subset of mentionsto further improve the mention detection stage of thebaseline system.
Specifically, we fix mentions start-ing with a stop word and mentions ending with apunctuation mark.
We also use training data to learnpatterns of inappropriate mention boundaries.
Themention candidates that match the patterns are re-114moved.
This strategy is similar to the method usedto remove non-referential noun phrases.As for annotation inconsistency, we find that in afew documents, a punctuation mark or an apostropheused to mark the possessive form are inconsistentlyadded to the end of a mention.
The problem resultsin an incorrect matching between the gold and pre-dicted mentions and downgrades the performance ofthe learned model.
Moreover, the incorrect mentionboundary problem also affects the training phase be-cause our system is trained on a union set of the pre-dicted and gold mentions.
To fix this problem, inthe training phase, we perform a relaxed matchingbetween predicted mentions and gold mentions andignore the punctuation marks and mentions that startwith one of the following: adverb, verb, determiner,and cardinal number.
For example, we successfullymatch the predicted mention ?now the army?
to thegold mention ?the army?
and match the predictedmention ?Sony ?s?
to the gold mention ?Sony.?
Notethat we cannot fix the inconsistency problem in thetest data.3.2 Pronoun ResolutionThe baseline system uses an identical model forcoreference resolution on both pronouns and non-pronominal mentions.
However, in the litera-ture (Bengtson and Roth, 2008; Rahman and Ng,2011; Denis and Baldridge, 2007) the featuresfor coreference resolution on pronouns and non-pronouns are usually different.
For example, lexi-cal features play an important role in non-pronouncoreference resolution, but are less important forpronoun anaphora resolution.
On the other hand,gender features are not as important in non-pronouncoreference resolution.We consider training two separate classifiers withdifferent sets of features for pronoun and non-pronoun coreference resolution.
Then, in the decod-ing stage, pronoun and non-pronominal mentionsuse different classifiers to find the best antecedentmention to link to.
We use the same features fornon-pronoun coreference resolution, as the baselinesystem.
For the pronoun anaphora classifier, we usea set of features described in (Denis and Baldridge,2007), with some additional features.
The aug-mented feature set includes features to identify if apronoun or an antecedent is a speaker in the sen-Algorithm 1 Online Latent Structured Learning forCoreference ResolutionLoop until convergence:For each document Dt and each v ?
Dt1.
Let u?
= maxu?y(v)wT?
(u, v), and2.
u?
= maxu?{u<v}?{?}wT?
(u, v) + ?
(u, v, y(v))3.
Let w?
w + ?wT (?
(u?, v)?
?
(u?, v)).tence.
It also includes features to reflect the docu-ment type.
In Section 4, we will demonstrate the im-provement of using separate classifiers for pronounand non-pronoun coreference resolution.3.3 Learning Protocol for Best-Link InferenceThe baseline system applies the strategy in (Bengt-son and Roth, 2008, Section 2.2) to learn the pair-wise scoring functionw using the Averaged Percep-tron algorithm.
The algorithm is trained on mentionpairs generated on a per-mention basis.
The exam-ples are generated for a mention v as?
Positive examples: (u, v) is used as a positiveexample where u < v is the closest mention tov in v?s cluster?
Negative examples: for all w with u < w < v,(w, v) forms a negative example.Although this approach is simple, it suffers froma severe label imbalance problem.
Moreover, it doesnot relate well to the best-link inference, as the deci-sion of picking the closest preceding mention seemsrather ad-hoc.
For example, consider three men-tions belonging to the same cluster: {m1: ?Presi-dent Bush?, m2: ?he?, m3:?George Bush?}.
Thebaseline system always chooses the pair (m2,m3)as a positive example because m2 is the closet men-tion of m3.
However, it is more proper to learn themodel on the positive pair (m1,m3), as it providesmore information.
Since the best links are not givenbut are latent in our learning problem, we use an on-line latent structured learning algorithm (Connor etal., 2011) to address this problem.We consider a structured problem that takes men-tion v and its preceding mentions {u | u < v} asinputs.
The output variables y(v) is the set of an-tecedent mentions that co-refer with v. We definea latent structure h(v) to be the bestlink decisionof v. It takes the value ?
if v is the first mention115MethodWithout Separating Pronouns With Separating PronounsMD MUC BCUB CEAF AVG MD MUC BCUB CEAF AVGBinary Classifier (baseline) 70.53 61.63 69.26 43.03 57.97 73.24 64.57 69.78 44.95 59.76Latent-Structured Learning 73.02 64.98 70.00 44.48 59.82 73.95 65.75 70.25 45.30 60.43Table 1: The performance of different learning strategies for best-link decoding algorithm.
We show the resultswith/without using separate pronoun anaphora resolver.
The systems are trained on the TRAIN set and evaluated onthe CoNLL-2012 DEV set.
We report the F1 scores (%) on mention detection (MD) and coreference metrics (MUC,BCUB, CEAF).
The column AVG shows the averaged scores of the three coreference metrics.System MD MUC BCUB CEAF AVGBaseline 64.58 55.49 69.15 43.72 56.12New Sys.
70.03 60.65 69.95 45.39 58.66Table 2: The improvement of Illinois-Coref.
We reportthe F1 scores (%) on the DEV set from CoNLL-2011shared task.
Note that the CoNLL-2011 data set does notinclude corpora of bible and of telephone conversation.in the equivalence class, otherwise it takes valuesfrom {u | u < v}.
We define a loss function?
(h(v), v, y(v)) as?
(h(v), v, y(v)) ={0 h(v) ?
y(v),1 h(v) /?
y(v).We further define the feature vector ?
(?, v) to be azero vector and ?
to be the learning rate in Percep-tron algorithm.
Then, the weight vectorw in (1) canbe learned from Algorithm 1.
At each step, Alg.
1picks a mention v and finds the Best-Link decisionu?
that is consistent with the gold cluster.
Then, itsolves a loss-augmented inference problem to findthe best link decision u?
with current model (u?
= ?if the classifier decides that v does not have coref-erent antecedent mention).
Finally, the model w isupdated by the difference between the feature vec-tors ?
(u?, v) and ?
(u?, v).Alg.
1 makes learning more coherent with infer-ence.
Furthermore, it naturally solves the data im-balance problem.
Lastly, this algorithm is fast andconverges very quickly.4 Experiments and ResultsIn this section, we demonstrate the performance ofIllinois-Coref on the OntoNotes-5.0 data set.
A pre-vious experiment using an earlier version of this datacan be found in (Pradhan et al, 2007).
We first showthe improvement of the mention detection system.Then, we compare different learning protocols forcoreference resolution.
Finally, we show the overallperformance improvement of Illinois-Coref system.First, we analyze the performance of mention de-tection before the coreference stage.
Note that sin-gleton mentions are included since it is not possibleto identify singleton mentions before running coref-erence.
They are removed in the post-processingstage.
The mention detection performance of theend-to-end system will be discussed later in this sec-tion.
With the strategy described in Section 3.1, weimprove the F1 score for mention detection from55.92% to 57.89%.
Moreover, we improve the de-tection performance on short named entity mentions(name entity with less than 5 words) from 61.36 to64.00 in F1 scores.
Such mentions are more impor-tant because they are easier to resolve in the corefer-ence layer.Regarding the learning algorithm, Table 1 showsthe performance of the two learning protocolswith/without separating pronoun anaphora resolver.The results show that both strategies of using a pro-noun classifier and training a latent structured modelwith a online algorithm improve the system perfor-mance.
Combining the two strategies, the avg F1score is improved by 2.45%.Finally, we compare the final system with thebaseline system.
We evaluate both systems on theCoNLL-11 DEV data set, as the baseline systemis tuned on it.
The results show that Illinois-Corefachieves better scores on all the metrics.
The men-tion detection performance after coreference resolu-tion is also significantly improved.116Task MD MUC BCUB CEAF AVGEnglish (Pred.
Mentions) 74.32 66.38 69.34 44.81 60.18English (Gold Mention Boundaries) 75.72 67.80 69.75 45.12 60.89English (Gold Mentions) 100.00 85.74 77.46 68.46 77.22Chinese (Pred Mentions) 47.58 37.93 63.23 35.97 45.71Table 3: The results of our submitted system on the TEST set.
The systems are trained on a collection of TRAIN andDEV sets.4.1 Chinese Coreference ResolutionWe apply the same system to Chinese coreferenceresolution.
However, because the pronoun proper-ties in Chinese are different from those in English,we do not train separate classifiers for pronoun andnon-pronoun coreference resolution.
Our Chinesecoreference resolution on Dev set achieves 37.88%MUC, 63.37% BCUB, and 35.78% CEAF in F1score.
The performance for Chinese coreference isnot as good as the performance of the coreferencesystem for English.
One reason for that is that weuse the same feature set for both Chinese and En-glish systems, and the feature set is developed forthe English corpus.
Studying the value of strong fea-tures for Chinese coreference resolution system is apotential topic for future research.4.2 Test ResultsTable 3 shows the results obtained on TEST, usingthe best system configurations found on DEV.
Wereport results on both English and Chinese coref-erence resolution on predicted mentions with pre-dicted boundaries.
For English coreference resolu-tion, we also report the results when using gold men-tions and when using gold mention boundaries1.5 ConclusionWe described strategies for improving mention de-tection and proposed an online latent structure al-gorithm for coreference resolution.
We also pro-posed using separate classifiers for making Best-Link decisions on pronoun and non-pronoun men-tions.
These strategies significantly improve theIllinois-Coref system.1Note that, in Ontonotes annotation, specifying gold men-tions requires coreference resolution to exclude singleton men-tions.
Gold mention boundaries are provided by the task orga-nizers and include singleton mentions.Acknowledgments This research is supported by theDefense Advanced Research Projects Agency (DARPA)Machine Reading Program under Air Force ResearchLaboratory (AFRL) prime contract no.
FA8750-09-C-0181 and the Army Research Laboratory (ARL) underagreement W911NF-09-2-0053.
Any opinions, findings,and conclusion or recommendations expressed in this ma-terial are those of the author(s) and do not necessarilyreflect the view of the DARPA, AFRL, ARL or the USgovernment.ReferencesE.
Bengtson and D. Roth.
2008.
Understanding the valueof features for coreference resolution.
In EMNLP.K.
Chang, R. Samdani, A. Rozovskaya, N. Rizzolo,M.
Sammons, and D. Roth.
2011.
Inference proto-cols for coreference resolution.
In CoNLL.M.
Connor, C. Fisher, and D. Roth.
2011.
Online latentstructure training for language acquisition.
In IJCAI.P.
Denis and J. Baldridge.
2007.
A ranking approach topronoun resolution.
In IJCAI.S.
Pradhan, L. Ramshaw, R. Weischedel, J. MacBride,and L. Micciulla.
2007.
Unrestricted Coreference:Identifying Entities and Events in OntoNotes.
InICSC.S.
Pradhan, L. Ramshaw, M. Marcus, M. Palmer,R.
Weischedel, and N. Xue.
2011.
CoNLL-2011shared task: Modeling unrestricted coreference inOntoNotes.
In CoNLL.S.
Pradhan, A. Moschitti, N. Xue, O. Uryupina, andY.
Zhang.
2012.
CoNLL-2012 shared task: Modelingmultilingual unrestricted coreference in OntoNotes.
InCoNLL.A.
Rahman and V. Ng.
2011.
Narrowing the modelinggap: a cluster-ranking approach to coreference resolu-tion.
Journal of AI Research, 40(1):469?521.117
