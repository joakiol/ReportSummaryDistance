Constraint Satisfaction Inference:Non-probabilistic Global Inference for Sequence LabellingSander Canisius and Antal van den BoschILK / Language and Information ScienceTilburg UniversityTilburg, The Netherlands{S.V.M.Canisius,Antal.vdnBosch@uvt.nl}@uvt.nlWalter DaelemansCNTS, Department of LinguisticsUniversity of AntwerpAntwerp, BelgiumWalter.Daelemans@ua.ac.beAbstractWe present a new method for performingsequence labelling based on the idea of us-ing a machine-learning classifier to gen-erate several possible output sequences,and then applying an inference proce-dure to select the best sequence amongthose.
Most sequence labelling methodsfollowing a similar approach require thebase classifier to make probabilistic pre-dictions.
In contrast, our method canbe used with virtually any type of clas-sifier.
This is illustrated by implement-ing a sequence classifier on top of a (non-probabilistic) memory-based learner.
Ina series of experiments, this method isshown to outperform two other methods;one naive baseline approach, and anothermore sophisticated method.1 IntroductionIn machine learning for natural language process-ing, many diverse tasks somehow involve pro-cessing of sequentially-structured data.
For ex-ample, syntactic chunking, grapheme-to-phonemeconversion, and named-entity recognition are allusually reformulated as sequence labelling tasks:a task-specific global unit, such as a sentence or aword, is divided into atomic sub-parts, e.g.
wordor letters, each of which is separately assigned alabel.
The concatenation of those labels forms theeventual output for the global unit.More formally, we can define a sequence la-belling task as a tuple (x,y, `).
The goal is to mapan input vector x = ?x1, x2, .
.
.
, xn?
of tokens toan output sequence y = ?y1, y2, .
.
.
, yn?
of labels.The possible labels for each token are specified bya finite set `, that is, yi ?
`,?i.In most real-world sequence labelling tasks, thevalues of the output labels are sequentially cor-related.
For machine learning approaches to se-quence labelling this implies that classifying eachtoken separately without considering the labels as-signed to other tokens in the sequence may leadto sub-optimal performance.
Ideally, the complexmapping of the entire input sequence to its corre-sponding output sequence is considered one clas-sification case; the classifier then has access to allinformation stored in the sequence.
In practise,however, both input and output sequences are fartoo sparse for such classifications to be performedreliably.A popular approach to circumvent the issuesraised above is what we will refer to as the clas-sification and inference approach, covering tech-niques such as hidden markov models and condi-tional random fields (Lafferty et al, 2001).
Ratherthan having a token-level classifier make local de-cisions independently of the rest of the sequence,the approach introduces an inference procedure,operating on the level of the sequence, using classlikelihoods estimated by the classifier to optimisethe likelihood of the entire output sequence.A crucial property of most of the classificationand inference techniques in use today is that theclassifier used at the token level must be able toestimate the likelihood for each potential class la-bel.
This is in contrast with the more commonview of a classifier having to predict just one classlabel for an instance which is deemed most opti-mal.
Maximum-entropy models, which are used inmany classification and inference techniques, havethis property; they model the conditional class dis-tribution.
In general, this is the case for all prob-abilistic classification methods.
However, manygeneral-purpose machine learning techniques are9not probabilistic.
In order to design inference pro-cedures for those techniques, other principles thanprobabilistic ones have to be used.In this paper, we propose a non-probabilistic in-ference procedure that improves performance of amemory-based learner on a wide range of natural-language sequence processing tasks.
We startfrom a technique introduced recently by Van denBosch and Daelemans (2005), and reinterpret it asan instance of the classification and inference ap-proach.
Moreover, the token-level inference pro-cedure proposed in the original work is replacedby a new procedure based on principles of con-straint satisfaction that does take into account theentire sequential context.The remainder of this paper is structured as fol-lows.
Section 2 introduces the theoretical back-ground and starting point of the work presented inthis paper: the trigram method, and memory-basedlearning.
Next, the new constraint-satisfaction-based inference procedure for class trigrams ispresented in Section 3.
Experimental comparisonsof a non-sequence-aware baseline classifier, theoriginal trigram method, and the new classificationand inference approach on a number of sequencelabelling tasks are presented in Section 4 and dis-cussed in Section 5.
Finally, our work is comparedand contrasted with some related approaches inSection 6, and conclusions are drawn in Section 7.2 Theoretical background2.1 Class TrigramsA central weakness of approaches consideringeach token of a sequence as a separate classifica-tion case is their inability to coordinate labels as-signed to neighbouring tokens.
Due to this, invalidlabel sequences, or ones that are highly unlikelymay result.
Van den Bosch and Daelemans (2005)propose to resolve parts of this issue by predict-ing trigrams of labels as a single atomic class la-bel, thereby labelling three tokens at once, ratherthan classifying each token separately.
Predict-ing sequences of three labels at once makes surethat at least these short subsequences are known tobe syntactically valid sequences according to thetraining data.Applying this general idea, Van den Bosch andDaelemans (2005) label each token with a com-plex class label composed of the labels for the pre-ceding token, the token itself, and the one follow-ing it in the sequence.
If such class trigrams areassigned to all tokens in a sequence, the actual la-bel for each of those is effectively predicted threetimes, since every token but the first and last iscovered by three class trigrams.
Exploiting thisredundancy, a token?s possibly conflicting predic-tions are resolved by voting over them.
If two outof three trigrams suggest the same label, this labelis selected; in case of three different candidate la-bels, a classifier-specific confidence metric is usedto break the tie.Voting over class trigrams is but one possibleapproach to taking advantage of the redundancyobtained with predicting overlapping trigrams.
Adisadvantage of voting is that it discards one ofthe main benefits of the class trigram method: pre-dicted class trigrams are guaranteed to be syntac-tically correct according to the training data.
Thevoting technique splits up the predicted trigrams,and only refers to their unigram components whendeciding on the output label for a token; no attemptis made to keep the trigram sequence intact in thefinal output sequence.
The alternative to votingpresented later in this paper does try to retain pre-dicted trigrams as part of the output sequence.2.2 Memory-based learningThe name memory-based learning refers to a classof methods based on the k-nearest neighbour rule.At training time, all example instances are storedin memory without attempting to induce an ab-stract representation of the concept to be learned.Generalisation is postponed until a test instance isclassified.
For a given test instance, the class pre-dicted is the one observed most frequently amonga number of most-similar instances in the instancebase.
By only generalising when confronted withthe instance to be classified, a memory-basedlearner behaves as a local model, specificallysuited for that part of the instance space that thetest instance belongs to.
In contrast, learners thatabstract at training time can only generalise glob-ally.
This distinguishing property makes memory-based learners especially suited for tasks wheredifferent parts of the instance space are structuredaccording to different rules, as is often the case innatural-language processing.For the experiments performed in this study weused the memory-based classifier as implementedby TiMBL (Daelemans et al, 2004).
In TiMBL,similarity is defined by two parameters: a feature-level similarity metric, which assigns a real-valued10score to pairs of values for a given feature, and aset of feature weights, that express the importanceof the various features for determining the simi-larity of two instances.
Further details on both ofthese parameters can be found in the TiMBL man-ual.
To facilitate the explanation of our inferenceprocedure in Section 3, we will formally definesome notions related to memory-based classifica-tion.The function Ns,w,k(x) maps a given instancex to the set of its nearest neighbours; here, theparameters s, w, and k are the similarity metric,the feature weights, and the number k of nearestneighbours, respectively.
They will be consideredgiven in the following, so we will refer to thisspecific instantiation simply as N(x).
The func-tion wd(c,N(x)) returns the weight assigned toclass c in the given neighbourhood according tothe distance metric d; again we will use the nota-tion w(c,N(x)) to refer to a specific instantiationof this function.
Using these two functions, we canformulate the nearest neighbour rule as follows.argmaxcw(c,N(x))The class c maximising the above expression isreturned as the predicted class for the instance x.3 Constraint Satisfaction InferenceA strength of the class trigram method is the guar-antee that any trigram that is predicted by the baseclassifier represents a syntactically valid subse-quence of length three.
This does not necessar-ily mean the trigram is a correct label assignmentwithin the context of the current classification, butit does reflect the fact that the trigram has beenobserved in the training data, and, moreover, isdeemed most likely according to the base classi-fier?s model.
For this reason, it makes sense to tryto retain predicted trigrams in the output label se-quence as much as possible.The inference method proposed in this sectionseeks to attain this goal by formulating the classtrigram disambiguation task as a weighted con-straint satisfaction problem (W-CSP).
Constraintsatisfaction is a well-studied research area with ap-plications in numerous fields both inside and out-side of computer science.
Weighted constraint sat-isfaction extends the traditional constraint satis-faction framework with soft constraints; such con-straints are not required to be satisfied for a so-lution to be valid, but constraints a given solutiondoes satisfy, are rewarded according to weights as-signed to them.Formally, a W-CSP is a tuple (X,D,C,W ).Here, X = {x1, x2, .
.
.
, xn} is a finite set of vari-ables.
D(x) is a function that maps each variableto its domain, that is, the set of values that variablecan take on.
C is the set of constraints.
Whilea variable?s domain dictates the values a singlevariable is allowed to take on, a constraint spec-ifies which simultaneous value combinations overa number of variables are allowed.
For a tradi-tional (non-weighted) constraint satisfaction prob-lem, a valid solution would be an assignment ofvalues to the variables that (1) are a member of thecorresponding variable?s domain, and (2) satisfyall constraints in the set C .
Weighted constraintsatisfaction, however, relaxes this requirement tosatisfy all constraints.
Instead, constraints are as-signed weights that may be interpreted as reflect-ing the importance of satisfying that constraint.Let a constraint c ?
C be defined as a func-tion that maps each variable assignment to 1 if theconstraint is satisfied, or to 0 if it is not.
In addi-tion, let W : C?
IR+ denote a function that mapseach constraint to a positive real value, reflectingthe weight of that constraint.
Then, the optimal so-lution to a W-CSP is given by the following equa-tion.x?
= argmaxx?cW (c)c(x)That is, the assignment of values to its variablesthat maximises the sum of weights of the con-straints that have been satisfied.Translating the terminology introduced earlierin this paper to the constraint satisfaction domain,each token of a sequence maps to a variable, thedomain of which corresponds to the three candi-date labels for this token suggested by the trigramscovering the token.
This provides us with a defini-tion of the function D, mapping variables to theirdomain.
In the following, yi,j denotes the candi-date label for token xj predicted by the trigramassigned to token xi.D(xi) = {yi?1,i, yi,i, yi+1,i}Constraints are extracted from the predicted tri-grams.
Given the goal of retaining predicted tri-grams in the output label sequence as much as pos-sible, the most important constraints are simply11the trigrams themselves.
A predicted trigram de-scribes a subsequence of length three of the entireoutput sequence; by turning such a trigram into aconstraint, we express the wish to have this trigramend up in the final output sequence.
(xi?1, xi, xi+1) = (yi,i?1, yi,i, yi,i+1),?iNo base classifier is flawless though, and there-fore not all predicted trigrams can be expected tobe correct.
Nevertheless, even an incorrect trigrammay carry some useful information regarding theoutput sequence: one trigram also covers two bi-grams, and three unigrams.
An incorrect trigrammay still contain smaller subsequences, of lengthone or two, that are correct.
Therefore, all of theseare also mapped to constraints.
(xi?1, xi) = (yi,i?1, yi,i), ?i(xi, xi+1) = (yi,i, yi,i+1), ?ixi?1 = yi,i?1, ?ixi = yi,i, ?ixi+1 = yi,i+1, ?iWith such an amount of overlapping con-straints, the satisfaction problem obtained eas-ily becomes over-constrained, that is, no vari-able assignment exists that can satisfy all con-straints without breaking another.
Only one in-correctly predicted class trigram already leads totwo conflicting candidate labels for one of the to-kens at least.
Yet, without conflicting candidatelabels no inference would be needed to start with.The choice for the weighted constraint satisfactionmethod always allows a solution to be found, evenin the presence of conflicting constraints.
Ratherthan requiring all constraints to be satisfied, eachconstraint is assigned a certain weight; the optimalsolution to the problem is an assignment of valuesto the variables that optimises the sum of weightsof the constraints that are satisfied.Constraints can directly be traced back to a pre-diction made by the base classifier.
If two con-straints are in conflict, the one which the classi-fier was most certain of should preferably be sat-isfied.
In the W-CSP framework, this preferencecan be expressed by weighting constraints accord-ing to the classifier confidence for the originatingtrigram.
For the memory-based learner, we definethe classifier confidence for a predicted class cias the weight assigned to that class in the neigh-bourhood of the test instance, divided by the totalweight of all classes.w(ci,N(x))?c w(c,N(x))Let x denote a test instance, and c?
its pre-dicted class.
Constraints derived from this classare weighted according to the following rules.?
for a trigram constraint, the weight is simplythe base classifier?s confidence value for theclass c??
for a bigram constraint, the weight is the sumof the confidences for all trigram classes inthe nearest-neighbour set of x that assign thesame label bigram to the tokens spanned bythe constraint?
for a unigram constraint, the weight is thesum of the confidences for all trigram classesin the nearest-neighbour set of x that assignthe same label to the token spanned by theconstraint4 ExperimentsTo thoroughly evaluate our new inference proce-dure, and to show that it performs well over awide range of natural-language sequence labellingtasks, we composed a benchmark set consisting ofsix different tasks, covering four areas in naturallanguage processing: syntax (syntactic chunking),morphology (morphological analysis), phonology(grapheme-to-phoneme conversion), and informa-tion extraction (general, medical, and biomedicalnamed-entity recognition).
Below, the six data setsused for these tasks are introduced briefly.CHUNK is the task of splitting sentences intonon-overlapping syntactic phrases or constituents.The data set, extracted from the WSJ Penn Tree-bank, and first used in the CoNLL-2000 sharedtask (Tjong Kim Sang and Buchholz, 2000), con-tains 211,727 training examples and 47,377 testinstances.NER, named-entity recognition, involves iden-tifying and labelling named entities in text.
Weemploy the English NER shared task data setused in the CoNLL-2003 conference (Tjong KimSang and De Meulder, 2003).
This data set dis-criminates four name types: persons, organisa-tions, locations, and a rest category of ?miscellanynames?.
The data set is a collection of newswire12articles from the Reuters Corpus, RCV11.
Thegiven training set contains 203,621 examples; astest set we use the ?testb?
evaluation set whichcontains 46,435 examples.MED is a data set extracted from a semantic an-notation of (parts of) two Dutch-language medi-cal encyclopedias.
On the chunk-level of this an-notation, there are labels for various medical con-cepts, such as disease names, body parts, and treat-ments, forming a set of twelve concept types in to-tal.
Chunk sizes range from one to a few tokens.The data have been split into training and test sets,resulting in 428,502 training examples and 47,430test examples.The GENIA corpus (Tateisi et al, 2002) is a col-lection of annotated abstracts taken from the Na-tional Library of Medicine?s MEDLINE database.Apart from part-of-speech tagging information,the corpus annotates a subset of the substancesand the biological locations involved in reactionsof proteins.
Using a 90%?10% split for producingtraining and test sets, there are 458,593 trainingexamples and 50,916 test examples.PHONEME refers to grapheme-to-phoneme con-version for English.
The sequences to be la-belled are words composed of letters (rather thansentences composed of words).
We based our-selves on the English part of the CELEX-2 lexi-cal data base (Baayen et al, 1993), from whichwe extracted 65,467 word-pronunciation pairs.This pair list has been aligned using expectation-maximisation to obtain sensible one-to-one map-pings between letters and phonemes (Daelemansand Van den Bosch, 1996).
The classes to pre-dict are 58 different phonemes, including somediphones such as [ks] needed to keep the letter-phoneme alignment one-to-one.
The resultingdata set has been split into a training set of 515,891examples, and a test set of 57,279 examples.MORPHO refers to morphological analysis ofDutch words.
We collected the morphologi-cal analysis of 336,698 Dutch words from theCELEX-2 lexical data base (Baayen et al, 1993),and represented the task such that it captures thethree most relevant elements of a morphologicalanalysis: (1) the segmentation of the word intomorphemes (stems, derivational morphemes, andinflections), (2) the part-of-speech tagging infor-mation contained by each morpheme; and (3) all1Reuters Corpus, Volume 1, English language, 1996-08-20 to 1997-08-19.Task Baseline Voting CSInf OracleCHUNK 91.9 92.7 93.1 95.8NER 77.2 80.2 81.8 86.5MED 64.7 67.5 68.9 74.9GENIA 55.8 60.1 61.8 70.6PHONEME 79.0 83.4 84.5 98.8MORPHO 41.3 46.1 51.9 62.2Table 1: Performances of the baseline method, andthe trigram method combined both with majorityvoting, and with constraint satisfaction inference.The last column shows the performance of the (hy-pothetical) oracle inference procedure.spelling changes due to compounding, derivation,or inflection that would enable the reconstructionof the appropriate root forms of the involved mor-phemes.For CHUNK, and the three information extrac-tion tasks, instances represent a seven-token win-dow of words and their (predicted) part-of-speechtags.
Each token is labelled with a class using theIOB type of segmentation coding as introduced byRamshaw and Marcus (1995), marking whetherthe middle word is inside (I), outside (O), or at thebeginning (B) of a chunk, or named entity.
Per-formance is measured by the F-score on correctlyidentified and labelled chunks, or named entities.Instances for PHONEME, and MORPHO consistof a seven-letter window of letters only.
The labelsassigned to an instance are task-specific and havebeen introduced above, together with the tasksthemselves.
Generalisation performance is mea-sured on the word accuracy level: if the entirephonological transcription of the word is predictedcorrectly, or if all three aspects of the morpholog-ical analysis are predicted correctly, the word iscounted correct.4.1 ResultsFor the experiments, memory-based learners weretrained and automatically optimised with wrappedprogressive sampling (Van den Bosch, 2004) topredict class trigrams for each of the six tasks in-troduced above.
Table 1 lists the performances ofconstraint satisfaction inference, and majority vot-ing applied to the output of the base classifiers, andcompares them with the performance of a naivebaseline method that treats each token as a sepa-rate classification case without coordinating deci-sions over multiple tokens.Without exception, constraint satisfaction infer-13ence outperforms majority voting by a consider-able margin.
This shows that, given the samesequence of predicted trigrams, the global con-straint satisfaction inference manages better to re-cover sequential correlation, than majority voting.On the other hand, the error reduction attained bymajority voting with respect to the baseline is inall cases more impressive than the one obtainedby constraint satisfaction inference with respect tomajority voting.
However, it should be empha-sised that, while both methods trace back their ori-gins to the work of Van den Bosch and Daelemans(2005), constraint satisfaction inference is not ap-plied after, but instead of majority voting.
Thismeans, that the error reduction attained by major-ity voting is also attained, independently by con-straint satisfaction inference, but in addition con-straint satisfaction inference manages to improveperformance on top of that.5 DiscussionThe experiments reported upon in the previoussection showed that by globally evaluating thequality of possible output sequences, the con-straint satisfaction inference procedure manages toattain better results than the original majority vot-ing approach.
In this section, we attempt to fur-ther analyse the behaviour of the inference pro-cedure.
First, we will discuss the effect that theperformance of the trigram-predicting base classi-fier has on the maximum performance attainableby any inference procedure.
Next, we will con-sider specifically the effect of base classifier accu-racy on the performance of constraint satisfactioninference.5.1 Base classifier accuracy and inferenceprocedure upper-boundsAfter trigrams have been predicted, for each token,at most three different candidate labels remain.
Asa result, if the correct label is not among them, thebest inference procedure cannot correct that.
Thissuggests that there is an upper-bound on the per-formance attainable by inference procedures oper-ating on less than perfect class trigram predictions.To illustrate what performance is still possible af-ter a base classifier has predicted the trigrams fora sequence, we devised an oracle inference proce-dure.An oracle has perfect knowledge about the truelabel of a token; therefore it is able to select this la-bel if it is among the three candidate labels.
If thecorrect label is absent among the candidate labels,no inference procedure can possibly predict thecorrect label for the corresponding token, so theoracle procedure just selects randomly among thecandidate labels, which will be incorrect anyway.Table 1 compares the performance of majority vot-ing, constraint satisfaction inference, and the ora-cle after an optimised base classifier has predictedclass trigrams.5.2 Base classifier accuracy and constraintsatisfaction inference performanceThere is a subtle balance between the quality ofthe trigram-predicting base classifier, and the gainthat any inference procedure for trigram classescan reach.
If the base classifier?s predictions areperfect, all three candidate labels will agree for alltokens in the sequence; consequently the inferenceprocedure can only choose from one potential out-put sequence.
On the other extreme, if all threecandidate labels disagree for all tokens in the se-quence, the inference procedure?s task is to selectthe best sequence among 3n possible sequences,where n denotes the length of the sequence; it islikely that such a huge amount of candidate labelsequences cannot be dealt with appropriately.Table 2 collects the base classifier accuracies,and the average number of potential output se-quences per sentence resulting from its predic-tions.
For all tasks, the number of potential se-quences is manageable; far from the theoreticalmaximum 3n, even for GENIA, that, comparedwith the other tasks, has a relatively large num-ber of potential output sequences.
The factors thathave an effect on the number of sequences arerather complex.
One important factor is the accu-racy of the trigram predictions made by the baseclassifier.
To illustrate this, Figure 1 shows thenumber of potential output sequences as a functionof the base classifier accuracy for the PHONEMEtask.
There is an almost linear decrease of thenumber of possible sequences as the classifier ac-curacy improves.
This shows that it is importantto optimise the performance of the base classifier,since it decreases the number of potential outputsequences to consider for the inference procedure.Other factors affecting the number of potentialoutput sequences are the length of the sequence,and the number of labels defined for the task.
Un-like classifier accuracy, however, these two factors14123456789101165  70  75  80  85  90  95#sequencesbase classifier accuracyFigure 1: Average number of potential output se-quences as a function of base classifier accuracyon the PHONEME task.Task Base acc.
Avg.
# seq.CHUNK 88.8 38.4NER 91.6 9.0MED 77.1 9.3GENIA 71.8 1719.3PHONEME 91.7 1.8MORPHO 80.9 2.8Table 2: The average number of potential outputsequences that result from class trigram predic-tions made by a memory-based base classifier.are inherent properties of the task, and cannot beoptimised.While we have shown that improved base clas-sifier accuracy has a positive effect on the num-ber of possible output sequences; we have not yetestablished a positive relation between the num-ber of possible output sequences and the perfor-mance of constraint satisfaction inference.
Fig-ure 2 illustrates, again for the PHONEME task, thatthere is indeed a positive, even linear relation be-tween the accuracy of the base classifier, and theperformance attained by inference.
This relationexists for all inference procedures: majority vot-ing, as well as constraint satisfaction inference,and the oracle procedure.
It is interesting to seehow the curves for those three procedure comparewith each other.The oracle always outperforms the other twoprocedures by a wide margin.
However, its in-crease is less steep.
Constraint satisfaction in-ference consistently outperforms majority voting,though the difference between the two decreasesas the base classifier?s predictions improve.
Thisis to be expected, since more accurate predictionsmeans more majorities will appear among candi-455055606570758085909565  70  75  80  85  90  95sequenceaccuracybase classifier accuracyoracleconstraint satisfaction inferencemajority votingFigure 2: Performance of majority voting, con-straint satisfaction inference, and the oracle infer-ence procedure as a function of base classifier ac-curacy on the PHONEME task.date labels, and the predictive quality of such ma-jorities improves as well.
In the limit ?with a per-fect base classifier?
all three curves will meet.6 Related workMany learning techniques specifically designedfor sequentially-structured data exist.
Given ourgoal of developing a method usable with non-probabilistic classifiers, we will not discuss theobvious differences with the many probabilisticmethods.
In this section, we will contrast our workwith two other approaches that also apply prin-ciples of constraint satisfaction to sequentially-structured data.Constraint Satisfaction with Classifiers (Pun-yakanok and Roth, 2001) performs the somewhatmore specific task of identifying phrases in a se-quence.
Like our method, the task of coordinatinglocal classifier decisions is formulated as a con-straint satisfaction problem.
The variables encodewhether or not a certain contiguous span of tokensforms a phrase.
Hard constraints enforce that notwo phrases in a solution overlap.Similarly to our method, classifier confidenceestimates are used to rank solutions in order ofpreference.
Unlike in our method, however, boththe domains of the variables and the constraintsare prespecified; the classifier is used only to esti-mate the cost of potential variable assignments.
Inour approach, the classifier predicts the domainsof the variables, the constraints, and the weightsof those.Roth and Yih (2005) replace the Viterbi algo-15rithm for inference in conditional random fieldswith an integer linear programming formulation.This allows arbitrary global constraints to be in-corporated in the inference procedure.
Essentially,the method adds constraint satisfaction function-ality on top of the inference procedure.
In ourmethod, constraint satisfaction is the inferenceprocedure.
Nevertheless, arbitrary global con-straints (both hard and soft) can easily be incor-porated in our framework as well.7 ConclusionThe classification and inference approach is a pop-ular and effective framework for performing se-quence labelling in tasks where there is stronginteraction between output labels.
Most existinginference procedures expect a base classifier thatmakes probabilistic predictions, that is, rather thanpredicting a single class label, a conditional proba-bility distribution over the possible classes is com-puted.
The inference procedure presented in thispaper is different in the sense that it can be usedwith any classifier that is able to estimate a confi-dence score for its (non-probabilistic) predictions.Constraint satisfaction inference builds uponthe class trigram method introduced by Van denBosch and Daelemans (2005), but reinterprets itas a strategy for generating multiple potential out-put sequences, from which it selects the sequencethat has been found to be most optimal accordingto a weighted constraint satisfaction formulationof the inference process.
In a series of experi-ments involving six sequence labelling task cover-ing several different areas in natural language pro-cessing, constraint satisfaction inference has beenshown to improve substantially upon the perfor-mance achieved by a simpler inference procedurebased on majority voting, proposed in the originalwork on the class trigram method.The work presented in this paper shows there ispotential for alternative interpretations of the clas-sification and inference framework that do not relyon probabilistic base classifiers.
Future work maywell be able to further improve the performanceof constraint satisfaction inference, for example,by using more optimised constraint weightingschemes.
In addition, alternative ways of formu-lating constraint satisfaction problems from classi-fier predictions may be explored; not only for se-quence labelling, but also for other domains thatcould benefit from global inference.ReferencesR.
H. Baayen, R. Piepenbrock, and H. van Rijn.
1993.The CELEX lexical data base on CD-ROM.
Lin-guistic Data Consortium, Philadelphia, PA.W.
Daelemans and A.
Van den Bosch.
1996.Language-independent data-oriented grapheme-to-phoneme conversion.
In J. P. H. Van Santen,R.
W. Sproat, J. P. Olive, and J. Hirschberg, edi-tors, Progress in Speech Processing, pages 77?89.Springer-Verlag, Berlin.W.
Daelemans, J. Zavrel, K. Van der Sloot, and A. Vanden Bosch.
2004.
TiMBL: Tilburg memory basedlearner, version 5.1.0, reference guide.
TechnicalReport ILK 04-02, ILK Research Group, TilburgUniversity.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fi elds: Probabilistic models for seg-menting and labeling sequence data.
In Proceed-ings of the 18th International Conference on Ma-chine Learning, Williamstown, MA.V.
Punyakanok and D. Roth.
2001.
The use of classi-fi ers in sequential inference.
In NIPS-13; The 2000Conference on Advances in Neural Information Pro-cessing Systems, pages 995?1001.
The MIT Press.L.A.
Ramshaw and M.P.
Marcus.
1995.
Text chunk-ing using transformation-based learning.
In Pro-ceedings of the 3rd ACL/SIGDAT Workshop on VeryLarge Corpora, Cambridge, Massachusetts, USA,pages 82?94.D.
Roth andW.
Yih.
2005.
Integer linear programminginference for conditional random fi elds.
In Proc.
ofthe International Conference on Machine Learning(ICML), pages 737?744.Yuka Tateisi, Hideki Mima, Ohta Tomoko, and JunichiTsujii.
2002.
Genia corpus: an annotated researchabstract corpus in molecular biology domain.
InHu-man Language Technology Conference (HLT 2002),pages 73?77.E.
Tjong Kim Sang and S. Buchholz.
2000.
Introduc-tion to the CoNLL-2000 shared task: Chunking.
InProceedings of CoNLL-2000 and LLL-2000, pages127?132.E.
Tjong Kim Sang and F. De Meulder.
2003.
In-troduction to the conll-2003 shared task: Language-independent named entity recognition.
In W. Daele-mans and M. Osborne, editors, Proceedings ofCoNLL-2003, pages 142?147.
Edmonton, Canada.A.
Van den Bosch and W. Daelemans.
2005.
Improv-ing sequence segmentation learning by predictingtrigrams.
In I. Dagan and D. Gildea, editors, Pro-ceedings of the Ninth Conference on ComputationalNatural Language Learning.A.
Van den Bosch.
2004.
Wrapped progressivesampling search for optimizing learning algorithmparameters.
In R. Verbrugge, N. Taatgen, andL.
Schomaker, editors, Proceedings of the 16thBelgian-Dutch Conference on Artificial Intelligence,pages 219?226, Groningen, The Netherlands.16
