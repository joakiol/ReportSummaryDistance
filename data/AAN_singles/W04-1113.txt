Using Synonym Relations In Chinese Collocation ExtractionWanyin LiDepartment of Computing,The Hong Kong Polytechnic University,Hung Hom, Kowloon, Hong Kongcswyli@comp.polyu.edu.hkQin LuDepartment of Computing,The Hong Kong Polytechnic University,Hung Hom, Kowloon, Hong Kongcsluqin @comp.polyu.edu.hkRuifeng XuDepartment of Computing, The Hong Kong Polytechnic University,Hung Hom, Kowloon, Hong Kongcsrfxu@comp.polyu.edu.hkAbstractA challenging task in Chinese collocationextraction is to improve both the precision andrecall rate.
Most lexical statistical methodsincluding Xtract face the problem of unable toextract  collocations with lower frequencies thana given threshold.
This paper presents a methodwhere HowNet is used to find synonyms using asimilarity function.
Based on such synonyminformation, we have successfully extractedsynonymous collocations which normally cannotbe extracted using the lexical statisticalapproach.
We applied synonyms mapping toeach headword to extract more synonymousword bi-grams.
Our evaluation over 60MBtagged corpus shows that we can extractsynonymous collocations that occur with verylow frequency, sometimes even for collocationsthat occur only once in the training set.Comparing to a collocation extraction systembased on Xtract, we have reached the precisionrate of 43% on word bi-grams for a set of 9headwords, almost 50% improvement fromprecision rate of 30% in the Xtract system.Furthermore, it  improves the recall rate of wordbi-gram collocation extraction by 30%.1 IntroductionA Chinese collocation is a recurrent andconventional expression of words which  holdssyntactic and semantic relations.
A widely adopteddefinition given by Benson (Benson 1990) statedthat ?a collocation is an arbitrary and recurrentword combination.?
For example, we say ?warmgreetings?
rather than ?hot greetings?, ?broaddaylight?
rather than ?bright daylight?.
Similarly,in Chinese ?
?
?
?
?
?
are three nounswith similar meanings, however, we  say?
?
rather than ?
?,?
?rather than ?
?.Study in collocation extraction using lexicalstatistics has gained some insights to the issuesfaced in collocation extraction (Church and Hanks1990, Smadja 1993, Choueka 1993, Lin 1998).
Asthe lexical statistical approach is developed basedon the ?recurrence?
property of collocations, onlycollocations with reasonably good recurrence canbe extracted.
Collocations with low occurrencefrequency cannot be extracted, thus affecting therecall rate.
The precision rate using the lexicalstatistics approach can reach around 60% if bothword bi-gram extraction and n-gram extractionsare taking into account (Smadja 1993, Lin 1997and Lu et al 2003).
The low precision rate ismainly due to the low precision rate of word bi-gram extractions as only about 30% - 40%precision rate can be achieved for word bi-grams.In this paper, we propose a different approach tofind collocations with low recurrences.
The mainidea is to make use of synonym relations to extractsynonymous collocations.
Lin (Lin 1997)described a distributional hypothesis that if twowords have similar set of collocations, they areprobably similar.
In HowNet, Liu Qun (Liu et al2002) defined the word similarity as two wordsthat can substitute each other in the context andkeep the sentence consistent in syntax andsemantic structure.
That means, naturally, twosimilar words are very close to each other and theycan be used in place of the other in certain context.For example, we may either say  ?
?or ?
?as  and are semantically close to eachother.
We apply this lexical phenomenal after thelexical statistics based extractor to find the lowfrequency synonymous collocations, thusincreasing recall rate.The rest of this paper is organized as follows.Section 2 describes related existing collocationextraction techniques based on both lexicalstatistics and synonymous collocation.
Section 3describes our approach on collocation extraction.Section 4 evaluates the proposed method.
Section 5draws our conclusion and presents possible futurework.2 Related WorkMethods have proposed to extract collocationsbased on lexical statistics.
Choueka (Choueka1993) applied quantitative selection criteria basedon frequency threshold to extract adjacent n-grams(including bi-grams).
Church and Hanks (Churchand Hanks 1990) employed mutual information toextract both adjacent and distant bi-grams that tendto co-occur within a fixed-size window.
But themethod did not extend to extract n-grams.
Smadja(Smadja 1993) proposed a statistical model bymeasuring the spread of the distribution of co-occurring pairs of words with higher strength.
Thismethod successfully extracted both adjacent anddistant bi-grams and n-grams.
However, themethod failed to extract bi-grams with lowerfrequency.
The precision rate on bi-gramscollocation is very low, only around high 20% andlow 30%.
Even though, it is difficult to measurerecall rate in collocation extraction (almost noreport on recall estimation), It is understood thatlow occurrence collocations cannot be extracted.Our research group has further applied the Xtractsystem to Chinese (Lu et al 2003) by adjusting theparameters to optimize the algorithm for Chineseand a new weighted algorithm was developedbased on mutual information to acquire word bi-grams with one higher frequency word and onelower frequency word.
The result has achieved anestimated 5% improvement in recall rate and a15% improvement in precision comparing to theXtract system.All of the above techniques do not takeadvantage of the wide range of lexical resourcesavailable including synonym information.
Pearce(Pearce 2001) presented a collocation extractiontechnique that relies on a mapping from a word toits synonyms for each of its senses.
The underlyingintuitions is that if the difference between theoccurrence counts of one synonyms pair withrespect to a particular word was at least two, thenthis was deemed sufficient to consider them as acollocation.
To apply this approach, knowledge inword (concept) semantics and relations to otherwords must be available such as the use ofWordNet.
Dagan (Dagan 1997) applied similarity-based smoothing method to solve the problem ofdata sparseness in statistical natural languageprocessing.
The experiments conducted in his laterworks showed that this method achieved muchbetter results than back-off smoothing methods inword sense disambiguation.
Similarly, Hua Wu(Wu and Zhou 2003) applied synonymsrelationship between two different languages toautomatically acquire English synonymouscollocation.
This is the first time that the conceptsynonymous collocation is proposed.
A sideintuition raised here is that nature language is fullof synonymous collocations.
As many of themhave low occurrences, they are failed to beretrieved by lexical statistical methods.
Eventhough there are Chinese synonym dictionaries,such as  ( Tong Yi Ci Lin), thedictionaries lack structured knowledge andsynonyms are too loosely defined to be used forcollocation extraction.HowNet developed by Dong et al(Dong andDong 1999) is the best publicly available resourceon Chinese semantics.
By making use of semanticsimilarities of words, synonyms can be defined bythe closeness of their related concepts and thecloseness can be calculated.
In Section 3, wepresent our method to extract synonyms fromHowNet and using synonym relations to furtherextract collocations.Sun (Sun 1997) did a preliminary Quantitativeanalysis on Chinese collocations based on theirarbitrariness, recurrence and the syntax structure.The purpose of this study is to help differentiate ifa collocation is true or not according to thequantitative factors.
By observing the existence ofsynonyms information in natural language use, weconsider it possible to identify different types ofcollocations using more semantic and syntacticinformation available.
We discuss the basic ideasin section 5..3 Our ApproachOur method of extracting Chinese collocationsconsists of three steps.Step 1: Take the output of any lexical statisticalalgorithm which extracts word bi-gramcollocations.
The data is then sortedaccording to each headword , Wh, with its co-word, Wc, listed.Step 2: For each headword Wh used to extract bi-grams, we acquire its synonyms based on asimilarity function using HowNet.
Any wordin HowNet having similarity value over athreshold value is chosen as a synonymheadword Ws for additional extractions.Step 3: For each synonym headword, Ws, and theco-word Wc of Wh, as its synonym, if the bi-gram (Ws , Wc) is not in the output of thelexical statistical algorithm in Step one, takethis bi-gram (Ws , Wc) as a collocation if thepair co-occurs in the corpus by additionalsearch to the corpus.3.1 Structure of HowNetDifferent from WordNet or other synonymsdictionary, HowNet describes words as a set ofconcepts  and each concept is described by aset of primitives .
The following lists for theword , one of its corresponding conceptsIn the above record, DEF is where the primitivesare specified.
DEF contains up to four types  ofprimitives: the basic independent primitive, the other independentprimitive , the relation primitive, and the symbol primitive ,where the basic independent primitive and theother independent primitive are used to indicate thesemantics of a concept and the others are used toindicate syntactical relationships.
The similaritymodel described in the next subsection willconsider both of these relationships.The primitives are linked by a hierarchical treeto indicate the parent-child relationships of theprimitives as shown in the following example:This hierarchical structure provides a way to linkone concept with any other concept in HowNet,and the closeness of concepts can be simulated bythe distance between two concepts.3.2 Similarity Model Based on HowNetLiu Qun (Liu 2002) defined word similarity astwo words which can substitute each other in thesame context and still maintain the sentenceconsistent syntactically and semantically.
This isvery close to our definition of synonyms.
Thus wedirectly used their similarity function, which isstated as follows.A word in HowNet is defined as a set ofconcepts and each concept is represented byprimitives.
Thus, HowNet can be described by W,a collection of n words, as:W = { w1, w2, ?
wn}Each word wi is, inturn, described by a set of concepts S as:Wi = { Si1, Si2,?Six},And, each concept Si  is, in turn, described by aset of primitives:Si  = { pi1, pi2 ?piy }For each word pair, w1 and w2, the similarityfunction is defined by)1(),(max),( 21...1,..121 jimjni SSSimwwSim ===where S1i is the list of concepts associated with W1and S2j is the list of concepts associated with W2.As any concept Si is presented by its primitives,the similarity of primitives for any p1, and  p2 ofthe same type, can be expressed by the followingformula:?
?+= ),(),( 2121 ppDisppSim     (2)where ?
is an adjustable parameter set to 1.6,and ),( 21 ppDis is the path length between p1 andp2 based on the semantic tree structure.
The aboveformula where ?
is a constant does not indicateexplicitly the fact that the depth of a pair of nodesin the tree affects their similarity.
For two pairs ofnodes (p1 ,  p2) and  (p3 ,  p4) with the same distance,the deeper the depth is, the more commonly sharedancestros they would have which should besemantically closer to each other.
In following twotree structures, the pair of nodes (p1, p2) in the lefttree should be more similar than (p3 ,  p4)  in theright tree.rootp2p1rootP3P4To indicate this observation,  ?
is modified as afunction of tree depths of the nodes using theformula  ?
=min(d(p1), d(p2)) .
Consequently, theformula (2) is rewritten as formular (2?)
during theexperiment.
))(),(min(),())(),(min(),(21212121 pdpdppDispdpdppSim +=(2?
)where d(pi) is the depth of node pi  in the tree .
Thecomparison of calculating the word similarity byapplying the formula (2) and  (2?)
is shown inSection 4.4.Based on the DEF description in HowNet,different primitive types play different roles onlysome are directly related to semantics.
To makeuse of both the semantic and syntactic informationincluded in HowNet to describe a word, thesimilarity of two concepts should take intoconsideration of all primitive types with weightedconsiderations and thus the formula is defined as)3(),(),( 2114121 jjijjii ppSimSSSim ?
?=== ?where ?i is a weighting factor given in (Liu2002) with the sum of ?1 + ?2 + ?3 + ?4 being 1?and ?1 ?
?2 ?
?3 ?
?4.
The distribution of theweighting factors is given for each concept a prioriin HowNet to indicate the importance of primitivepi in defining the corresponding concept S.3.3 Collocation ExtractionIn order to extract collocations from a corpus,and to obtain result for Step 1 of our algorithm, weused the collocation extraction algorithmdeveloped by the research group at the Hong KongPolytechnic University(Lu et al 2003).
Theextraction of bi-gram collocation is based on theEnglish Xtract(Smaja 1993) with improvements.Based on the three Steps mentioned earlier, we willpresent the extractions in each step in thesubsections.3.3.1 Bi-gram ExtractionBased on the lexical statistical model proposedby Smadja in Xtract on extracting Englishcollocations, an improved algorithm wasdeveloped for Chinese collocation by our researchgroup and the system is called CXtract.
For easy ofunderstanding, we will explain the algorithmbriefly here.
According to Xtract, wordcooccurence is denoted by a tripplet (wh, wi, d)where wh is a given headword, wi is a co-wordappeared in the corpus in a distance d within thewindow of [-5, 5].
The frequency fi of the co-wordwi   in the window of [-5, 5] is defined as:?
?==55,jjii ff    (4)where  fi, j   is the frequency of the co-word at distancej in the corpus within the window.
The averagefrequency of  fi , denoted by if , is given by10/55,?
?==jjii ff    (5)Then, the average frequency f , and the standarddeviation ?
are defined by?==niifnf11;  21)(1?=?=nii ffn?
(6)The Strength of the co-occurrence for the pair(wh, wi,), denoted by ki, is defined by?ffk ii?= ?
(7)Furthermore, the Spread of (wh, wi,),, denoted asUi, which characterizes the distribution of  wiaround  wh is define as:10)( 2,?
?=ijiiffU ;    (8)To eliminate the bi-grams with unlikely co-occurrence, the following sets of threshold valuesis defined:0:1 KffkC ii ?
?= ?
(9)0:2 UUC i ?
(10))(:3 1, iiji UKffC ?+?
(11)However, the above statistical model given bySmadja fails to extract the bi-grams with a muchhigher frequency of wh but a relatively lowfrequency word of wi,,  For example,  in the bi-gram , freq ( ) is much lower than thefreq ( ).
Therefore, we further defined aweighted mutual information to extract this kind ofbi-grams:,)(),w(0h RwfwfRiii ?=      (12)As a result, the system should return a list oftriplets (wh, wi, d), where  (wh, wi,) is consideredcollocations.3.3.2 Synonyms SetFor each given headword wh, before taking it asan input to extract its bi-grams directly, we fistapply the similarity formula described in Equation(1) to generate a set of synonyms headwords Wsyn:}),(:{ ?>= shssyn wwSimwW                             (13)Where 0 <?
<1 is an algorithm parameter whichis adjusted based on experience.
We set it as 0.85from the experiment because we would like tobalance the strength of the synonyms relationshipand the coverage of the synonyms set.
The settingof the parameter ?
< 0.85 weaks the similaritystrength of the extracted synonyms.
For example,for a given collocation ?
?, that is unlikelyto include the candidates ?
?, ?
?,?
?.
On the other hand, by setting theparameter ?
> 0.85 will limit the coverage of thesynonyms set and hence lose valuable synonyms.For example, for a given bi-gram ?
?, wehope to include the candidate synonymouscollocations such as  ?
?, ?
?,?
?.
We will show the test of ?
in thesection 4.2.This synonyms headwords set provides thepossibility to extract the synonymous collocationwith the lower frequency that failed to be extractedby lexical statistic.3.3.3 Synonymous CollocationsA phenomenal among the collocations in naturallanguage is that there are many synonymouscollocations exist.
For example, ?switch on light?and ?turn on light?, ?
?
and ?
?.Due to the domain specification of the corpus,some of the synonymous collocations may fail tobe extracted by the lexical statistic model becauseof their lower frequency.
Based on thisobservation, this paper takes a further step.
Thebasic idea is for a bi-gram collocation (wh, wc, d )we select the synonyms ws of wh with themaximum similarity respect to all the conceptscontained by wh, we deem (ws, wc, d ) as acollocation if its occurrence is greater than 1 in thecorpus.
There are similar works discussed byPearce (Pearce 2001).
.For a given collocation (ws, wc,, d), if ws ?
Wsyn,then we deem the triple (ws, wc,, d) as asynonymous collocation with respect to thecollocation (wh, wc,, d) if the co-occurrence of (ws,wc, , d) in the corpus is greater than one.
Therefore,we define the collection of synonymouscollocations Csyn as:}1),,(:),,{( >= dwwFreqdwwC cscssyn           (14)where  ws ?
Wsyn.4 EvaluationThe performance of collocation is normallyevaluated by precision and recall as defined below.nsCollocatioextractedofnumbertotalnsCollocatioExtractedcorrectofnumberprecision= (15)nsCollocatioactualofnumbertotalnsCollocatioExtractedcorrectofnumberrecall=  (16)To evaluate the performance of our approach, weconducted a set of experiments based on 9 selectedheadwords.
A baseline system using only lexicalstatistics given in 3.3.1 is used to get a set ofbaseline data called Set A.
The output using ouralgorithm is called Set B.
Results are checked byhand for validation on what is true collocation andwhat is not a true collocation.Table 1.
Sample table for the true collocationwith headword ?
?Table 2.
Sample table for the bi-grams that arenot true collocationsTable 1 shows samples of extracted word bi-gramsusing our algorithm that are consideredsynonymous collocations for the headword ?
?.Table 2 shows extracted bi-grams by our algorithmthat are not considered true collocations.4.1 Test SetOur experiment is based on a corpus of sixmonths tagged People Daily with 11 millionsnumber of words.
For word bi-gram extractions,we consider only content words, thus headwordsare selected from noun, verb and adjective only.For evaluation purpose, we selected randomly 3nouns, 3 verbs and 3 adjectives with frequency oflow, medium and high.
Thus, in Step 1 of thealgorithm, 9 headwords were  used to extract bi-gram collocations from the corpus, and 253 pairsof collocations were extracted.
Evaluation by handhas identified 77 true collocations in Set A test set.The overall precision rate is 30% (see Table 3).Noun+Verb+AdjectiveHeadword 9Extracted Bi-grams 253True collocations usinglexical statistics only77Precision rate 30%Table 3: Statistics in test set for set AUsing Step 2 of our algorithm, where ?=0.85 isused, we have obtained 55 synonym headwords(include the 9 headwords).
Out of these 55synonyms, 614 bi-gram pairs were then extractedfrom the lexical statistics based algorithm, inwhich 179 are consider true collocations.
Then, byapplying Step 3 of our algorithm, we extracted anadditional 201 bi-gram pairs, among them, 178 areconsidered true collocations.
Therefore, using ouralgorithm, the overall precision rate has achieved43%, an improvement of almost 50%.
The data issummarized in Table 4.n., v, and adj.Synonyms headword 55Bi-grams (lexical statistics) 614Non-synonym collocations(lexical statistics only)179Extracted synonymcollocations Step 2201True synonym collocationsusing Step 2178Overall precision rate 43%Table 4: Statistics in test set for mode B4.2 The choice of ?We also conducted a set of experiments tochoose the best value for the similarity function?sthreshold ?.
We tested the best value of ?
with boththe precision rate and the estimated recall rateusing the so called remainder bi-grams.
Theremainder bi-grams is the total number of bi-gramsextracted by the algorithm.
When precision goesup, the size of the result is smaller, which in a wayis an indicator of less recalled collocations.
Figure1 shows the precision rate and the estimated recallrate in testing the value of ?.Figure 1.
Precision Rate vs. value of ?From Figure 1, it is obvious that at ?=0.85 therecall rate starts to drop more drastically withoutmuch incentive for precision.Extracted Bi-grams usinglexicalstatisticsExtractedSynonymsCollocationsusing Step 2(1.2,1.4,12) 465 328(1.4,1.4,12) 457 304(1.4,1.6,12) 394 288(1.2,1.2,12) 513 382(1.2,1.2,14) 503 407(1.2,1.2,16) 481 413Table 5: Value of (K0, K1, U0).4.3 The test of (K0, K1, U0)The original threshold for CXtract is (1.2, 1.2, 12)for the parameters (K0, K1, U0).
However, withsynonyms collocations, we have also conductedsome experiments to see whether the parametersshould be adjusted.
Table 5 shows the statistics totest the value of (K0, K1, U0).
The similaritythreshold ?
was fixed at 0.85 throughout theexperiments.The experimental shows that varying the value of(k0, k1) does not bring any benefit to our algorithm.However, increasing the value of u0 did improvethe extraction of synonymous collocations.
Figure2 shows that U0 =14 is a good trade-off for theprecision rate and the remainder Bi-grams.
The basicmeaning behind the result is reasonable.
According toSmadja, U0 defined in the formula (8) represents theco-occurrence distribution of the candidatecollocation (wh, wc) in the position of d (-5 ?
d ?5).
For a true collocation (wh, wc,, d), its co-occurrence  in the position d is much higher than inother positions which leads to a peak in the co-occurrence distribution.
Therefore, it is selected bythe statistical algorithm based on the formula (10).Based on the physical meaning behind, one way toimprove the precision rate is to increase the value ofthe threshold U0.
A side effect to an increased  valueof U0  is that the recall is decreased because sometrue collocations do not meet the condition of co-occurrence greater than U0.
Step 2 of the newalgorithm regains some  true collocations lostbecause of a higher U0.
in Step 1.Figure 2.
Precision Rate vs. Value of U04.4 The comparison of similarity calculationbased on formula  (2) and (2?
)Table 6 shows the similarity value given byformula (2) where ?
is a constant given the value1.6 and by formula (2?)
where ?
is replaced by afunction of the depths of the nodes.
Results showthat (2?)
is more fine tuned and reflects the natureof the data better.
For example, andare more similar than and .and are much similar but not the same.Table 6: comparison of similarity calculation5 Conclusion and Further WorkIn this paper, we have presented a method toextract bi-gram collocations using lexical statisticsmodel with synonyms information.
Our methodreaches the precision rate of 43% for the tested data.Comparing to the precision of 30% using lexicalstatistics only, our improvement is close to 50%.
Inadditional, the recall improved 30%.
The contributionis that we have made use of synonym informationwhich is plentiful in the natural language use and itworks well to supplement the shortcomings of lexicalstatistical method.Manning claimed that the lack of validsubstitution for a synonym is a characteristics ofcollocations in general (Manning and Schutze1999).
To extend our work, we consider the use ofsynonym information can be further applied tohelp identify collocations of different types.Our preliminary study has suggested thatcollocation can be classified into 4 types:Type 0 Collocation:  Fully fixed collocationwhich include some idioms, proverbs and sayingssuch as ?
?
?
?
and so on.Type 1 Collocation:  Fixed collocation in whichthe appearance of one word implies the co-occurrence of another one such as ?
?.Type 2 Collocation: Strong collocation whichallows very limited substitution of the components,for example, ?
?, ?
?,?
?
and so on.Type 3 Collocation: Normal collocation whichallows more substitution of the components,however a limitation is still required.
For example,?
?
?
?
?
??
?
.By using synonym information and definesubstitutability, we can validate whethercollocations are fixed collocations, strongcollocations with very limited substitutions, orgeneral collocations that can be substituted morefreely.6 AcknowledgementsOur great thanks to Dr. Liu Qun of the ChineseLanguage Research Center of Peking University forletting us share their data structure in the SynonymsSimilarity Calculation.
This work is partiallysupported by the Hong Kong PolytechnicUniversity (Project Code A-P203) and CERGGrant (Project code 5087/01E)ReferencesM.
Benson, 1990.
Collocations and GeneralPurpose Dictionaries.
International Journal ofLexicography, 3(1): 23-35Y.
Choueka, 1993.
Looking for Needles in aHaystack or Locating Interesting CollocationExpressions in Large Textual Database.Proceedings of RIAO Conference on User-oriented Content-based Text and ImageHandling: 21-24, Cambridge.K.
Church, and P. Hanks, 1990.
Word AssociationNorms, Mutual Information,and Lexicography.Computational Linguistics, 6(1): 22-29.I.
Dagan, L. Lee, and F. Pereira.
1997.
Similarity-based method for word sense disambiguation.Proceedings of the 35th Annual Meeting ofACL: 56-63, Madrid, Spain.Z.
D. Dong and Q. Dong.
1999.
Hownet,http://www.keenage.comD.
K. Lin, 1997.
Using Syntactic Dependency asLocal Context to Resolve Word Sense Ambiguity.Proceedings of ACL/EACL-97: 64-71, Madrid,SpainQ.
Liu, 2002.
The Word Similarity Calculation on<<HowNet>>.
Proceedings of 3rd Conferenceon Chinese lexicography, TaiBeiQ.
Lu, Y. Li, and R. F. Xu, 2003.
Improving Xtractfor Chinese Collocation Extraction.
Proceedingsof IEEE International Conference on NaturalLanguage Processing and KnowledgeEngineering, BeijingC.
D. Manning and H. Schutze, 1999.
Foundationsof Statistical Natural Language Processing.
TheMIT Press, Cambridge, MassachusettsD.
Pearce, 2001.
Synonymy in CollocationExtraction.
Proceedings of NAACL'01Workshop on Wordnet and Other LexicalResources: Applications, Extensions andCustomizationsF.
Smadja, 1993.
Retrieving collocations from text:Xtract.
Computational Linguistics, 19(1): 143-177H.
Wu, and M. Zhou, 2003.
SynonymousCollocation Extraction Using TranslationInformation.
Proceeding of the 41st AnnualMeeting of ACLD.
K. Lin, 1998.
Extracting collocations from textcorpora.
In Proc.
First Workshop onComputational Terminology, Montreal, Canada.M.
S. Sun, C. N. Huang and J. Fang, 1997.Preliminary Study on Quantitative Study onChinese Collocations.
ZhongGuoYuWen, No.1,29-38, (in Chinese).
