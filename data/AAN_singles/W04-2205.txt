Building and sharing multilingual speech resources,using ERIM generic platformsGeorges FAFIOTTEGETA, CLIPS, IMAG-campus (UJF, Grenoble 1 Univ.
)385 rue de la Biblioth?que, BP 53F-38041 Grenoble cedex 9Francegeorges.fafiotte@imag.frAbstractIn the framework of projects ChinFaDial andERIM we have developed in recent yearsseveral platforms allowing to handle variousaspects of bilingual spoken dialogues on theweb ?mainly, spontaneous speech corpuscollection through distant human interpreting.Current development of the core ERIM-Interpand ERIM-Collect platforms now includesmultimodal user interaction, integration ofsome machine aids (such as speech turn logsthrough speech recognition, or tentativelyspeech machine translation, both based onserver-grounded market products), and next,online aids to speakers and/or interpreters.First collected data should be made availableon the web in fall 2004 (DistribDial) alongwith, as soon as available, a robust version ofthe collecting platform, in order to promotecollaborative building, and sharing, of "raw"unannotated multilingual speech corpora.A variant of the ERIM environment is toextend to distant e-training in interpreting,possibly creating situations which should inturn, in our view, foster larger-scale datacollection and sharing in open access mode.KeywordsBilingual speech corpora, collaborative corpuscollection, spontaneous dialogues, Web-basedinterpreting, multilingual communication,open-access resources, resource mutualization.IntroductionOngoing burst in the development of both portabletelecommunications tools open to Internettransactions, and videoconferencing means, iscreating rapid expansion of teleservicing andtelebusiness applications with spontaneousdialogue, information inquiry, distant negotiation,etc.
Multilingualism, now in spoken transaction asit has been in written one, appears as a key issue indistant communication, with sensitive questions,both in supporting the diversity of the native ororigin language of conversing users (particularlywithin the opening European economic area), andin bringing some kind of balance between main"linguae francae" (common languages).
Thus newstakes arise in enhancing distant web-based on-lineinterpreting services.Meanwhile, Speech Machine Translation (SMT)steadily takes steps towards style spontaneity andmultilingualism.
In this context though, we face anotorious lack of large open-access corpora ofbilingual spoken dialogues.This led us to study, to model and propose a set ofgeneric platforms, aiming at enhancing distantmultilingual multimodal oral communication withfull recording and collecting facilities, alsoaddressing expectations from the MT systemsengineering community.The paper first looks over project motivation, thenintroduces the interpreting and collecting platformspresently available in the ERIM family, withcurrent variants.
It then reports on their first use incollecting domain-oriented spontaneously spokenFrench-Chinese dialogues.
Finally we presentongoing or planned development, advocating forcollaborative building and voluntary sharing ofresulting multilingual resources.1.
Motivations, early prototyping1.1 Developing multilingual linguisticresourcesIt is widely recognized that realistic and largecorpora are key resources for building SpeechRecognition (SR) and Speech MT systems.
If theWeb has recently been put to use as the largestpossible corpus, modeling casual spontaneousspoken language requires transcribed speechcorpora of hundreds of hours.Speech translation systems thus need large paralleltranslation corpora of transcribed and alignedspontaneous utterances in dialogue context, ideallywith complete sets of parse trees.
However, fewsuch corpora have been developed (by NEC, ATRand a few others), and these are not publiclyavailable.
Why not?
Because these corpora arevery expensive to transcribe once collected, and toannotate.
After so much time has been spent incompiling a corpus, giving it away seemsunreasonable.Besides, a future research objective is to usecollected corpora for studying and modeling reallife spontaneous spoken language and dialogues,and possibly to investigate if and how specificlinguistic traits can be expected depending onspecific dialogue situations, translation processsettings, or various multimodal interaction means.For instance, two speakers in a bilingual dialoguemay hear one another's original speech or not, theymay use video or fixed images, etc.
Their linguisticbehavior is expected to vary accordingly: thenumber of clarification sub-dialogues may vary;third person use or indirect speech may be usedmore in the presence of a speech translation systemthan with a human interpreter; the use of deicticand anaphoric elements may turn out to depend onthe use of visible markable objects on whiteboards,maps, images.With these considerations in mind, we thusendeavoured to propose open-acces corpusresources ?and therefore open-access collectingresources?, in order to ease collaborative buildingof "raw" unannotated multilingual translatedspeech corpora, likely taking advantage of newweb-based interpreting situations or scenarios.1.2 Enhancing multilingual communicationon the WebSome companies have already developedproprietary network-oriented interpreter's cubicles,which are the counterparts of existing fixedinstallations for interpreting in multilingualmeetings (for example at the UN or EU).
However,the associated code is not available for research.Furthermore, our typical scenario is somewhatdifferent from that of classical interpreting, whereinterpreters are available for the entire duration ofthe conversations.
We rather allow two situations:?
"conference call": speakers establish a schedule,and book a time slot with an interpreter,?
"on demand interpretation": interlocutors try toconverse using whatever knowledge they mayhave of their interlocutor's language, or of athird common language.
When the languagebarrier impedes communication, they ask anavailable interpreter to jump in to help.Apart from these practical motivations, we alsowish to conduct experimental studies on the effectof combining multimodal resources on bilingual ormultilingual conversations.
Thus, full recordingfacilities were required anyhow.1.3 Pre-ERIM platformsOther studies of human "consecutive"interpretation have employed multimodal Wizardof Oz platforms (e.g.
the EMMI plateform, that weexperienced at ATR-ITL for bilingual pilot-experiments [Fafiotte & Boitet, 1994] [Loken-Kim& al., 1994]), or monolingual multi-Wizardarchitectures have been modelled in a multimodalsetting (NEIMO [Coutaz & al., 1996]).
Thus ourfirst objective was to produce a simulator ofautomatic speech translation systems in the samespirit, to gain experience and collect data.We first built prototypes of a Speech MT Wizardof Oz simulator, Sim* [Fafiotte & Zhai, 1999] (tobe read as "Sim-Star", since being a parallelplatform to the C-STAR II CLIPS environment).They were designed to run on the Internet, andwere originally used on the intranet of CLIPS-GETA.
Network-based communications werehandled by a client-server communication moduledeveloped in Tcl/Tk.
Participants could see andhear each other and share an electronic whiteboard,using MBone resources.The idea of using Wizard of Oz techniques in thiscontext proved quite impractical, and thus wasabandoned.
Even if an acoustic filter was used todeform the interpreter?s voice, participantsperceived that a human was speaking.
In the end,we realized that, even for true automatic highquality interpretation, there actually might well bea real human "warm body" in the loop anyway.Thus a realistic design for online interpretationcould integrate both human and machineinterpretation for "partially automatic" Speech MT.The successive ERIM platforms have beenimplemented on this basis, in parallel at CLIPSwith integrating the French language intomultilingual Speech Machine Translation withinC-STAR and NESPOLE!
international projects.ERIM stands in French for Network-basedEnvironment for Multimodal Interpreting.2.
Distant human interpreting, as a collectingscheme for multilingual spoken dialogues2.1 ContextAt CLIPS-GETA, one of the ultimate researchgoals in Speech MT is to build systems forautomatic or partially automatic SpeechInterpretation (i.e.
"synergic" user-aided translationof speech).
Much progress has been made in thisarea over the past twelve years.
NEC produced thefirst speech translation demo in September 1992,within the tourist domain, but the most widelyknown coordinated research efforts to date includethe C-STAR projects (now a 7-languageinternational Consortium for Speech TranslationAdvanced Research) [http://www.c-star.org], theEuropean NESPOLE!
project [http://nespole.itc.it],the German Verbmobil [http://verbmobil.dfki.de]project, the US DARPA Communicator programwith the Galaxy Communicator SoftwareInfrastructure [http://fofoca.mitre.org/doc.html][http://www.darpa.mil/ito/research/com/index.html][http://www.sls.lcs.mit.edu/sls/whatwedo/architecture.html].All have demonstrated platforms enhancingspontaneous speech processing in multilingualperson-person or person-system communication,always in restricted domains.
CLIPS is firmlyinvolved in this action, while being in charge forintegrating the French language in the C-STARand NESPOLE!
environments.At the same time, we strongly believe that humaninterpreters will remain vital, both as irreplaceablesuppliers of relevant nuances and as models forautomatic or partially automatic systems.Human interpreting, too, will inevitably be carriedout through the Web and its raising applications.Thus we foresee a continuing need for research onWeb-based interpreting, and for data collection ofrealistic general-purpose or domain-oriented Web-based interpreting sessions.2.2 Functionals of the ERIM humanInterpreting platformThe ERIM-Interp network-based environmentconsists of a central communication server, twospeaker stations, one interpreter station (cf.
Fig.
1),with a multimodality server (exchange of shorttyped messages, whiteboard with shared picturesor files, and shared pointing and marking).
Toavoid complex problems due to turn overlap, wehave adopted a push-to-talk discipline up to now.The current implementation of ERIM-Interp, inTcl/Tk, is platform independent (and runs onWindows, MacOS, eventually Linux), and uses anadapted version of the CommSwitch written byCMU for the CSTAR-II project.It is also flexible: the CommServer can be hostedon a dedicated station or on any user workstation,two speakers may share the same station (in a"visit" situation), the scenario can be extended toinclude more than two interlocutors, more than oneinterpreter (in "one-way" interpreting situations),and hence possibly more than two languages.3.
Bilingual spontaneous speech collection3.1 As the next step taken then, the ERIMCollecting platformWe have then developed the ERIM-Collect variant,intended to collect corpora (cf.
Fig.
1), moreover toenhance collaborative generation and use ofbilingual speech corpora; namely to:?
collect only "raw" data (web-based spontaneousdialogues in any language pairs), as multimodalas possible ?with no built-in annotationscheme intended yet,?
motivate volunteers to produce the data,?
induce volunteering by offering free service (onone of the ERIM variants described here), inexchange for free data (users should agree to"donate their speech to science"),?
distribute the data as freeware (via GPLlicensing) on the Web, in a "re-playable" form:for each dialogue, descriptors indicate essential(anonymous) facts about the participants, alongwith the list of turns, indications of files,speakers, and time stamps for each turn,?
make it possible for other researchers to enrichthe corpora by adding annotations in parallelfiles, again sharable through the web; theymight use an extended version of the "Replay"facility (cf.
Fig.
3), with consensus on a sharedfile structure and XML descriptors format,?
develop the collection platform so that it canitself be offered as freeware on the Web.Accordingly, ERIM-Collect (currently 350 Kbytesof code in Tcl/Tk) was defined as an extension ofERIM-Interp:?
ERIM-Collect is language-independent,?
data is recorded locally during the dialogue;speech files are in PCM 22kHz-16bit-monoformat,?
session and speech turns descriptor files arenow in XML format,?
after the conversation, local descriptors andfiles are transferred then structured in corpusbases on a Collection Server,?
everything possible should be recorded: speech,short texts, whiteboard events, video, objectswhich the speakers refer to (e.g.
file names andurls).
In the current version 3 of ERIM-Collect,voice and short texts are collected; whiteboardactions and video are currently added.Speaker 1COR PUStranslationtranslationinto FrenchFrenchturnCommunicationServer+InterpreterSpeaker 2Chineseturntranslationinto ChineseCOR PUSCORPUStranslationWhiteboardWhiteboard2a4a31a5a5b1b2b4b6Figure 1: ERIM-Interp / ERIM-CollectWe describe here (cf.
Fig.
1) a basic exchangewithin a French-Chinese collection session.
First(1), the French interlocutor takes a turn of one ormore utterances.
This turn (speech, descriptors) isrecorded locally (1a), and transmitted (1b) to theInterpreter and the CommServer which broadcastsit across the virtual room established for theconversation.
The interpreter listens to the turn and(2) translates it into Chinese.
The translated turn isrecorded locally (2a) and broadcast (2b).
TheChinese participant listens to the translation (3)and then answers (4).
Again, his answer is storedlocally and broadcast (4a and 4b).
The interpreterthen translates it into French (5) and the translationis stored locally (5a) and broadcast (5b).In order to create various experimental settings, wemay unlock the reception of some messages forsome participants.
For instance in (1b) the Frenchvoice could be made audible for the Chineseparticipant.Figure 2 shows the screen which is presented to aconversational partner, as presently prototyped forthe ERIM-Collect platform.Figure 2: Speaker's screenAs for playback of apreviously recordedbilingual dialogue, afull reconstruction isavailable.
Simplifiedvisual tracking isprovided as shown inFigure 3.
One canextract monolingualvers ions of  thedialogues.A first version of theDistribDial / Replaycomponent (and website) for such replaysh a s  j u s t  b e e ncompleted.Figure 3: Playback of client, interpreter, and agent utterancesSuccessive versions of ERIM-Collect have beenused for collecting first domain-orientedspontaneous speech corpora (hotel reservation) inGrenoble and Beijing (cf.
4.2).3.2 Providing online aid to interpretersand/or speakersIn our "on demand interpretation?
scenario,interpreters may be asked to jump from oneconversation to another, and thus from one topic toanother.
This conversation switching is likely to bequite difficult, and stressful.
Thus machine aidscould be welcome: communication aids andlanguage aids.
We also envisage providingmachine aids for the conversational partners, tohelp them do without interpreters so far aspossible, if necessary.The currently implemented "communication aids"include facilities to?
see and hear others (participants andinterpreters),?
share data, possibly modifiable, markable, and"pointable" through the whiteboard,?
access an agenda for scheduling rendezvous.Possible "language aids", to both the humaninterpreter and the speakers, are of three kinds:?
access to dictionaries via typed or voicedrequests, and via automatic word spottingfollowed by filtering, dictionary look-up, andpresentation in a dedicated window,?
speech recognition, to alleviate difficulties oforal understanding when not using theinterpreter, and to produce a log of theconversation (which can additionally help aninterpreter jump in), after possible reduction,?
fully or partially automatic speech translation.At this time most communication aids have beenimplemented.
The scheduling agenda is global foran ERIM site, but each user handles it through apersonalized view (cf.
Fig.
4).Figure 4: Window of user agendaLanguage aids are the next step.
An interface toexisting free dictionary resources on the Papillonsite [http://www.papillon-dictionary.org] should beadded soon.
A speech recognizer has beenconnected to the platform in another ERIM variant(the automatic interpretation pilot setup ERIM-paST).
This Speech-To-Text facility could help aswell to issue some draft transcripts during thedialogue.3.3 Adding partially automatic Speech MTAn ERIM-paST (partially automated SpeechTranslation) platform is in progress at CLIPS inGrenoble, originally in cooperation with SpokenTranslation Inc. (Berkeley).
It aims at eventuallyproviding some languge aids to speakers who"converse by themselves", and at allowing datarecording of partially automatic interpreteddialogues (as a testing ground for Speech MTsystems development, testing or tuning, at CLIPS).Experimentation with interactive disambiguationmethods derived from the LIDIA project [Boitet &Blanchon, 1994] is also expected.The detailed description of this ERIM variant isbeyond the scope of this paper.
Briefly stated, thegoal is here a generic modular integration, throughplug-in, of Speech MT modules (speechrecognizers, text-to-text translators, speechsynthesizers), either research components (for theirfine testing and tuning) or off-the-shelf products.Objective is to carry out comparative assessmentof their results, or possibly contrastive evaluationwith the human production of an interpreter "warmbody".A first version of ERIM-paST is currently beingprototyped, while integrating server-based (Philips,Linguatec, Scansoft) market components.4 .
First corpus collection, towards acollaborative building/sharing scheme4.1 Platform assessment: distant collectionDistant collection is also being tested, but in ourfirst experiments Voice/IP still proved problematicwhen two turns overlapped.
New efficient basicsoftware and connection improvements are underevaluation.
Record-then-send or record-while-sending (streaming) modes are available.We may retain facilities for transmitting soundthrough phone lines.
These might be used inoperational contexts by telephone operators, suchas Prosodie in France: since this company is alsoan Internet service provider, it can merge bothtracks into a single communication.Distant connection data is summarized in Figure 5.Experiments(Gradesfrom 0 to 5)textvoice:recordthensendvoice:record &send(streaming)voice: samewithoverlappingStreaming ?
?
+ +Connexion:Internet100Mbit = = =Receptionquality 5 5 3 1Speed ofexchange 5 2 4 5Reliability 5 5 4 1Specialproblems /phenomenaNoneUserwary(tooslow)Somemicro-cuts,but goodoverallqualityUnusable,bandwidthtoo largeFigure 5:  Oral communication over the web4.2 The ChinFaDial project, French-Chinesespeech corporaThe system has been used in the ChinFaDialproject for collecting bilingual French-Chineseinterpreted spontaneous spoken dialogues, in thehotel reservation domain.
This 3-year project wasfunded by LIAMA, a joint French-Chineselaboratory under both French INRIA and ChineseCAS and MOST supervision.
Our partner is theChinese Information Processing group at NLPR(National Laboratory for Pattern Recognition), aresearch team within the Institute of Automation,Chinese Academy of Sciences (CAS-IA).In ChinFaDial we have used intranets in Grenobleor in Beijing, with 3 participants using headsets,located in one or in 2 different buildings.
It waspossible for 2 speakers to share the sameworkstation, but we have mainly used the regular3-station setting for the French-Chinese datacollection.
Some 10 hours of spontaneoustranslated spoken dialogues on "hotel informationand reservation" have been recorded thus far.
Theyproduce about 43kBits per second.Figure 5 shows a dialogue fragment transcription.We do not plan currently to transcribe or annotatecorpora, but others will be very welcome to do so.Participants to this first data collection have beenat this time:Chinese French TotalFr-ChInterpreters2 2 4Interlocutors 3 3 6There are 65 recorded dialogues with thesecharacteristics:Minimum Average MaximumDuration (sec) 457 635 874Number of turns 28 52 78Turn length (sec) 4 12 57Figure 5: Dialogue between a French hotel manager and a Chinese client (manual transcript)4.3 Ongoing developments, to promotecollaborative corpus buildingA website with a small ?DistribDial?
server hasbeen prototyped to freely distribute the sound filesand their descriptors, and a Replay module.
Ourgoal is to extend it to allow other groups tocontribute to the site whatever annotations theymay have created, and to share them under thesame conditions (GPL).
They should only agree toshare a common file base structure and a flexibleXML desciptor format for each annotation file.Corpus collection in French-Chinese will extend.Further data collection using ERIM-Collect juststarted (spontaneous dialogues in French andVietnamese, Tamil, Hindi), under support of AUF(University Agency for French-SpeakingCommunities), within the VTH-Fra.Dial project.We are also considering distributing an ERIM-Collect "hardened" version on DistribDial, afterstrengthening robustness and usability, so thatothers can use it to do their own spoken dialoguecollection.4.4 Planned e-Training extensions: use of theplatform to involve volunteer interpretersData collection being time-consuming all the same,our goal is not to do too much of it for its ownsake, but to get it as byproduct of some"mutualized" use of the platform, in the openaccess mode.Professional interpreters are unlikely to help on anon-profit basis, since interpreting is theirlivelihood.
Improving junior interpreters or evenadvanced student interpreters, however, may findWeb-based cooperation to be a good way oflearning or perfecting their trade in real lifesituations.We aim to induce volunteer interpreters or studentsof interpretation to translate bilingual dialoguesonline, by exchanging this on-line help for free useof our Web-based lab for e-training ininterpretation.We plan to develop an ERIM-Training variantplatform, an e-training extension, with fullrecording of all speech interaction and anymultimodal event.
Actually we already simulatedthe functional architecture of it, using the currentERIM-Collect in a multi-interpreter setting.Different scenarios and settings can be envisaged.For example, in a distant training or practicesituation, for a student interpreter: the studentmight be alone, gaining experience, or might bewith an instructor, who could supervise or takeover.At the 2008 Olympic Games in Beijing, as anotherexample, good student interpreters could be askedto aid bilingual communication in exchange foracademic credit, and free tickets.
Assume, forinstance, that a French speaker and a Chinesespeaker want to converse.
They could then go to aPC, activate ERIM-Interp with ERIM-Assist forFrench-Chinese, click on the icon of an availableinterpreter, and begin a mediated conversation,which would be recorded if participants agreewhile using the service free of charge.4.5 Building and sharing multilingual speechresourcesWe advocate and expect ERIM-Collect, onceproposed in an open-access mode on the Web, tobe willingly and freely operated by otherresearchers, under an agreed collaborativeframework to be set up, with minimal method andtechnical consent on collecting procedures andcorpus characteristic profiles, in order to bringbuilding and sharing of raw multilingual speechcorpora to a more rapid expansion.Collaborative annotation work could take place aswell, again with simple agreed procedures oncontent and descriptor files formats, and on apublic use scheme.Such tools, and their open use, could as wellunderlie valuable action towards supportiveprotection of "smaller languages", among othersminor European languages, while for instancefostering distant learning of interpreting, and whileeasing the use of low-cost or even free interpretingfacilities over the net.5.
Unification of ERIM platform variantsWork is now beginning on the integration of thedifferent platforms presented here into one singlemultifunctional ERIMM system [Fafiotte & Boitet,2003], for enhancing free multilingual multimodalnetwork-based communication with distantinterpreting and corpus collection.Numerous technical issues arise in this effort.
Forinstance, it is not immediately clear how theCommServer will accommodate server-basedinteractive lexical disambiguation duringtranslation; or how to secure efficient streamingdata transmission in a multicast scheme.
Even so,the platform independence and plug-and-playgeneric architecture of ERIM set components makethis integration effort quite realistic, in spite of thenumber and diversity of functions to be integrated.ConclusionWe have presented several platforms developed inthe long-range ERIM project.
Each platform canaid in the study of spontaneous cross-lingualcommunication on the Web.
The core platform isERIM-Interp for Web-based human interpretation.ERIM-Collect is a deliberate development of thelatter, dedicated to multilingual "raw" speechcorpus building, and intended to alleviate thecurrent scarcity of data ?particularly open data?,and which can also support the construction ofspeech translation systems.ERIM-Assist will add various machine aids forinterpreters and conversational partners, whileERIM-paST (only briefly mentioned here) includescomponents for partially automatic speechtranslation.We then reported on a first collection ofspontaneous bilingual interpreted spoken dialoguesfor French-Chinese.
This data, along with thecollecting framework itself, will be distributed inthe near future on the Web as shareware or GPL-ware, under a DistribDial component.We are looking for funding to create ERIM-Training ?a further extension of ERIM-Interp?which could serve as a valuable "Web-basedlanguage lab for interpreting" for distant e-training,while also providing new facilities for languagelearning.We plan to continue research in the ERIMframework by collecting and distributing more dataconcerning more languages (Vietnamese, Tamil,Hindi to French).
Data collection should beenhanced by a unified version of ERIM, offeringall the functionalities of the platform variants.More specifically, we hope that junior interpretersor advanced students in interpreting will volunteerto interpret and to practice with ERIM-Training,while users would agree to give their dialogues toscience in exchange of using ERIM-Interp for free.AcknowledgementsThis work has been supported by CLIPS-IMAG(UJF University Grenoble 1, CNRS, INPG) andfunded in part by the LIAMA French-ChineseLaboratory (ChinFaDial project), and by theRh?ne-Alpes Region (ERIM project).
Corpuscollecting action is currently supported by AUF-LTT (University Agency for French-SpeakingCommunities, VTH-Fra.Dial project).Our thanks go to Zhai JianShe (NanjingUniversity, China) for early prototyping, to JulienLamboley (at INSA, Lyon, France) for platformdevelopment, to members of the GETA andNLPR/CASIA-Beijing teams and to BrigitteMeillon at CLIPS-MultiCom, for theirparticipation in data collection and relatedexperiments.ReferencesBoitet C. & Blanchon H., 1994.
MultilingualDialogue-Based MT for Monolingual Authors:the LIDIA Project and a First Mockup.
MachineTranslation 9/2/94, pp.
99-132.Coutaz J., Salber D., Carraux E. & Portolan N.,1996.
NEIMO, a Multiwork station Usability Labfor Observing and Analyzing MultimodalInteraction.
Proc.
CHI'96 companion.Fafiotte G. & Boitet C., 1994.
Report on firstEMMI Experiments for the MIDDIM project inthe context of Interpreting Telecommunications.MIDDIM report TR-IT-0074 GETA-IMAG &ATR-ITL, Aug. 94, 11 p.Fafiotte G. & Boitet C., 2003.
ERIMM, a platformfor supporting and collecting multimodalspontaneous bilingual dialogues.
IEEE NLP-KE2003, Beijing, 26-29/10/03, 6 p.Fafiotte G. & Zhai J.-S., 1999.
A Network-basedSimulator for Speech Translation.
Proc.NPLRS?99, Beijing, 5-7/11/99, B. Yuan, T.Huang & X. Tang ed., pp.
511-514.Furuse O., Sobashima Y., Takezama T. & UrataniN., 1994.
Bilingual corpus for speechtranslation.
Proc.
AAAI-94 Workshop onIntegration of Natural Language and SpeechProcessing, Seattle, Washington, USA, 31/7-1/8/94, ATR Interpreting Telecommunications.Loken-Kim K.-H., Yato F. & Morimoto T., 1994.A Simulation Environment for MultimodalInterpreting Telecommunications.
Proc.
IPSJ-AV workshop, March 94, 5 p.<url> C-STAR.
http://www.c-star.org<url> DARPA sites.http://www.darpa.mil/ito/research/com/index.html,http://fofoca.mitre.org/doc.html<url> GALAXY system architecture site.http://www.sls.lcs.mit.edu/sls/whatwedo/ architecture.html<url> site web NESPOLE!
http://nespole.itc.it<url> site web PAPILLON.http://www.papillon-dictionary.org[<url> VERBMOBIL site.
http://verbmobil.dfki.de
