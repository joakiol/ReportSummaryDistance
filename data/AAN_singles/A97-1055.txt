Automatic Selection of Class Labels from a Thesaurus for an EffectiveSemantic Tagging of Corpora.Alessandro Cucchiarelli t Paola Velardi ~1JrUniversit~ diAncona - alex@inform.unian.it :~Universit/I di Roma 'La Sapienza' - velardi@dsi.uniromal.itAbstractIt is widely accepted that tagging text with se-mantic information would improve the quality oflexical learning in corpus-based NLP methods.However available on-line taxonomies arerather entangled and introduce an unnecessarylevel of ambiguity.
The noise produced by the re-dundant number of tags often overrides the ad-vantage of semantic tagging.
In this paper wepropose an automatic method to select fromWordNet a subset of domain-appropriate cate-gories that effectively reduce the overambiguityof WordNet, and help at identifying and cate-gorise relevant language patterns in a more com-pact way.
The method is evaluated against amanually tagged corpus, SEMCOR.1 IntroductionIt is well known that statistically-based approaches tolexical knowledge acquisition are faced with the problemof low counts.
Many language patterns (from simple co-occurrences to more complex syntactic associationsamong words) occur very rarely, or are never encoun-tered, in the learning corpus.
Since rare patterns are themajority, the quality and coverage of lexical earning mayresult severely affected.The obvious trategy to reduce this problem is to gener-alise word patterns according to some clustering tech-niques.
In the literature, two generalisation strategieshave been adopted:Distributional approaches: Several papers adopt distribu-tional techniques to identify clusters of words accordingto some defined measure of similarity.
Among these, in(Grishman and Sterling, 1994) a method is proposed tocluster syntactic triples, while in (Pereira and Tishby1992, 1993), (Dagan et al, 1994) pure bigrams are anal-ysed.The most intuitive evaluation of the effectiveness of dis-tributional approaches tothe problem of word general-ization is presented in (Grishman and Sterling, 1994).
Inthis paper it is argued that distributional (called alsosmoothing) techniques introduce a certain degree of addi-tional error, because co-occurrences may be erroneouslyconflated in a cluster, and some of the co-occurrences be-ing generalized are themselves incorrect.
In general theeffect is a higher ecall at the price of a lower precision.Another drawback of these methods i that, since clustershave only a numeric description, they are often hard toevaluate on a linguistic ground.Semantic tagging: Another adopted solution is to gener-380alise the observed word patterns by grouping patterns inwhich words have the same semantic tag.
Semantic tagsare assigned from on-line thesaura like WordNet (Basiliet al 1996) (Resnik, 1995), Roget's categories (Yarowsky1992) (Chen and Chen, 1996), the Japanese BGH (Utsuroet al 1993), or assigned manually (Basili et al 1992) 1.The obvious advantage ofsemantic tags is that words areclustered according to an intuitive principle (they belongto the same concept) rather than to some probabilisticmeasure.
Semantic tagging has been proven useful forlearning and categorising interesting relations amongwords, and for systematic lexical learning in sublan-guages, as shown in (Basili et al 1996) and (Basili et al1996b).On the other hand, semantic tagging has a serious draw-back, which is not solely due to the limited availabilityof on-line resources, but rather to the entangled structureof thesaura.
Wordnet and Roget's thesaura have not beenconceived, espite their success among researchers in lex-ical statistics, as tools for automatic language processing.The purpose was rather to provide the linguists with avery refined, general purpose, linguistically motivatedsource of taxonomic knowledge.As a consequence, in most on-line thesaura words are ex-tremely ambiguous, with very subtle distinctions amongsenses .
(Dolan, 1994) and (Krovetz and Croft, 1992) claim thatfine-grained semantic distinctions are unlikely to be ofpractical value for many applications.
Our experiencesupports this claim: often, what matters is to be able todistinguish among contrastive (Pustejowsky, 1995) ambi-guities of the bank_river bank__organisation flavour.High ambiguity, entangled nodes, and asymmetry have al-ready been emphasised in (Hearst and Shutze, 1993) asbeing an obstacle to the effective use of on-line thesaurain corpus linguistics.
In most cases, the noise introducedby overambiguity almost overrides the positive ffect ofsemantic lustering.
For example, in (Brill and Resnik,1994) clustering PP heads according to WordNet synsetsproduced only a 1% improvement in a PP disambiguationtask, with respect to the non-clustered method.
There arereported cases in which the use of WordNet worsenedthe performance ofan automatic indexing method.
Evencontext-based sense disambiguation becomes a pro-hibitive task on a wide-scale basis, because when wordsin the context of an ambiguous word are replaced by1 Manually assigning semantic tags if of course rathertime-consuming, however on-line thesaura re not avail-able in many languages, like Italian.their synsets, there is a multiplication of possible con-texts, rather than a generalization.
In (Agirre and Rigau,1996) a method called Conceptual Distance is proposedto reduce this problem, but the reported performance indisambiguation still do not reach 50%.A possible alternative is to manually select a set of high-level tags from the thesaurus.
This approach is adoptedin (Chen and Chen, 1996) and in (Basili et al 1996)where only a dozen categories are used.
As discussed inthe latter paper, high-level tags reduce the problem ofoverambiguity and allow the detection of more regularbehaviours in the analysis of lexical patterns.
On theother hand, high-level tags may be overgeneral, and theacquired lexical rules, while usually perform well in thetask of selecting the correct word associations (for ex-ample in PP disambiguation, or sense interpretation), areless capable of filtering out the noise.
Overgeneral cate-gories may even fail to capture contrastive ambiguities ofwords.So far the manual selection of an appropriate set of se-mantic tags has been a matter of personal intuitions, butwe believe that this task should be performed in a moreprincipled, and automatic, way.In this paper, we present a method for the selection of the"best-set" of WordNet categories for an effective, domain-tailored, semantic tagging of a corpus.
The purpose of themethod is to automatically select:?
A domain-appropriate s t of categories, that wellrepresent the semantics of the domain.?
A "right" level of abstraction, so as to mediate at bestbetween overambiguity and overgenerality.?
A balanced (for the domain) set of categories, i.e.words should be evenly distributed among cate-gories.The second feature is the most important, since as we re-marked so far, assigning semantic characteristics towords is very useful in lexical learning tasks, but over-ambiguity is the major obstacle to an effective use of the-saura in semantic tagging.In the following sections, we define a method for the au-tomatic selection of the '"oest-set" of WordNet categories,for nouns given an application corpus.First, an iterative method is used to create alternativesets of balanced categories.
Sets have an increasing levelof generality.
Second, a scoring function is applied to al-ternative sets to identify the "best" set.
The best set ismodelled as the linear function of four performance fac-tors: generality, coverage of the domain, average ambigu-ity, and discrimination power.
An interpolation method isadopted to estimate the parameters of the model against areference, correctly tagged, corpus (SEMCOR).
The per-formance of the selected set of categories i evaluated interms of effective reduction of overambiguity.The described method only requires a medium-range(stemmed) application corpus and a thesaurus.
The modelparameters are tuned against a reference correctly taggedcorpus, but this is not strictly necessary if correctlytagged corpora are not available.3812 Se lec t ion  o f  A l te rnat ive  Sets  o f  Se -mant ic  Categor ies  f rom WordNetThe first step of the method is generating alternative setsof WordNet categories.
Alternative sets are selected ac-cording to the following principles:?
Balanced categories: words must be uniformly dis-tribut~z~d among categories of a set;?
Increasing level of generality: alternative sets are se-lected by uniformly increasing the level of generalityof the categories belonging to a set;?
Domain-appropriateness: selected categories in a setare those pointed by (an increasingly large numberof) words of the application domain, weighted bytheir frequency in the corpus.The set-generation algorithm is an iterative applicationof the algorithm proposed in (Hearst and Shutze, 1993)for creating WordNet categories of a fixed average size.In its modified version, the algorithm is as follows2:Let S be a set of WordNet synsets , W the set of differentwords (nouns) in the corpus, P(s) the number of words inW that are instances of s, weighted by their frequency,LIB and LB the upper and lower bound for P(s), N, h andk constant values.i=1UB=NLB=UB*h;do{ initialise S with the set of WordNet opmost;initialise the set of categories C i withthe empty set;new_cat(S);ifi=l or Ci~C.i_ 1 then add C i to the set of Cat.i=i+l;UB=UB+k;LB=UB*h;Iuntil C i is not an empty set;where:newcat(S):for any category s of S{if s does not belong to C i thenIif P(s) <= LIB and P(s) >= LBthen put s in the set C ielse if P(s) > UBthen{let S' be the set of direct descendents of snew_cat(S')}else add s to SCT(C i)}}2The procedure new_cat(S) is almost the same as in(Hears-t and Shutze, 1993).
For sake of brevity, the algo-rithm is not further explained here.N, h and k are the initial parameters of the algorithm.
Weexperimentally observed that only h (the ratio betweenlower and upper bound) significantly modifies the result-ing sets of categories (Ci): we established that a goodcompromise is h=0.4.
SCT(C i) is the set of "smaller"WordNet categories with P(s)<LB that do not belong tothe Ci set (see next section).3 Scoring Alternative Sets of Cate-goriesThe algorithm of section 2 creates alternative sets of bal-anced and increasingly general categories Ci.
We nowneed a scoring function to evaluate these alternatives.The following performance factors have been selected toexpress the scoring function:Generality: In principle, we would like to represent thesemantics of the domain using the highest possible level ofgeneralisation.
We can express the generality G'(Ci) as1/DM(Ci), being DM(C i) the average distance betweenthe categories of C i and the WordNet topmost synsets.Due to the graph structure of WordNet, different pathsmay connect each element cij of Ci with different topmosts,therefore we compute DM(Ci) as:nDM(Ci) = 1 ,  ~.adm(cij)/1where dm(cij) is the average distance of each cij from thetopmosts.
Figure I illustrates a possible sysnsets hierar-chical in which, for Ci=\[cil ci2}, being dm(Cil)=(4+3)/2=3.5 and dm(ci2)=3, DM(Ci)=(3+3.5)/2=3.25Cil Ci2?
Topmost Synset?
General SynsetFigure 1 - An example of synsets hierarchyAs defined, G'(C i) is a linear function (low values forlow generality, high value for high generality), whilstour goal is to mediate at best between overspecificity andovergenerality.
Therefore, we model the generality asG(Ci)=G'(Ci)*Gauss(G'(Ci)), where Gauss(G'(Ci)) is aGauss distribution function computed by using the aver-age and the variancy of G'(C i) values over the set of allcategories Ci, selected by the algorithm in section 2, nor-malised in the \[0,1\] interval.Coverage: the algorithm of section 2, for any set C i , doesnot allow a full coverage of the nouns in the domain.Given a selected pair <UB, LB>, it may well be the case382that several words are not assigned to any category, be-cause when branching from an overpopulated category toits descendants, ome of the descendants may be under-populated.
Each iterative step that creates a C i also cre-ates a set of underpopulated categories SCT(Ci).
To en-sure full coverage, these categories may be added to Ci, oralternatively, they can be replaced by their direct ances-tors, but clearly a "good" selection of C i is one that mini-mizes this problem.
The coverage CO(Ci) is therefore de-fined as the ratio Nc(Ci)/W, where Nc(Ci) is the numberof words that reach at least one category of C iDiscrimination Power: a certain selection of categoriesmay not allow a full discrimination of the lowest-levelsenses for a word (leaves-synsets hereafter).
Figure 2 il-lustrates an example.
If C i = {Cil ci2 ci3 ci4}, w2 cannot befully disambiguated by any sense selection algorithm, be-cause two of its leaves-synsets belong to the same cate-gory ci2- With respect o w2, ci2 is overgeneral (thoughnothing can be said about the actual importance of dis-criminating between such two synsets).We measure the discrimination power DP(C i) as the ratio(Nc(Ci)-Npc(Ci))/Nc(Ci), where Nc(Ci) is the number ofwords that reach at least one category of C i, and Npc(Ci)is the number of words that have at least two leaves-synsets that reach the same category cij of C i.
For the ex-ample of figure 2 DP1, DP(C i) =(3-1 )/ 3=0.66.Cil c i2 c i3 ci4w I w 2 w 3ca General 5ynset0 LeafSynset\[\] Corpus WordFigure 2 - An example of distribution of leaves-synsets among categoriesAverage Ambiguity: each choice of C i in general re-duces the initial ambiguity of the corpus.
In part, becausethere are leaves-synsets that converge into a single cate-gory of the set, in part because there are leaves-synsets ofa word that do not reach any of these categories.
Thisphenomenon is accounted for by the inverse of the aver-age ambiguity A(Ci).
The A(Ci) is measured as:I Nc(Ci)a (Ci) = * ~_, Cwj(Ci)Nc(Ci) j=lwhere Nc(C i) is the number of words that reach at leastone category of Ci and, for each word wj in this set,Cwj(C i) is the number of categories of C i reached.In figure 2, the average ambiguity is 2 for the set C i = {cilci2 ci3 ci4}, and is 5/3=1.66 for C i = |Cil ci2 ci3}.1-, .DP_ / " \ \  , /e,  _ ~ .
, / '  0,90,8  - ~ / /~- .
-  .
, , / \~/ ;I - - - - - -~- / : .
.
.
.
.
.
.
.
/- / '~- ' - - .1 /A  ~ ~ I " ' -  / \ 0.7 .
.
.
.
.
.
.
.
- -  .
.
.
.
./ ~ / '~  "" 7 - -_ .
ok0,6  -O,S -0,4  -0 ,3  -0,:~ -0,10o0Ni.--..., ~ / - -L  ~.
~ ,  '~-~.
- _~- ,  / ,  .
, JCO /.
~ '.
'.
'.~ '.
'.
'.
'.
'.'.
'.
~ ', ~ ~ ~ ~ ~ ~ ~ ' ,~ '.
,, ,, ,, ,, I, , , ~  ~ ~ j0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ~ 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0Figure 3 - Performance factors G, CO, DP and l/A, for each generated set of categoriesThe scoring function for a set of categories Ci is definedas the linear combination of the performance parametersdescribed above:(1) Score (C i ) = ~G(C i )+ 13CO(C i )+ xDP(C i )+ 8(1/A(C i ))Notice that we assigned a positive effect on the score(modelled by l /A )  to the ability of eliminating certainleaves-synsets and a negative ffect (modelled by DP) tothe inability of discriminating among certain otherleaves-synsets.
This is reasonable in general, because ouraim is to control overgenerality while reducing overam-biguity.
However, nothing can be said on the appropri-ateness of a specific sense aggregation and/or sense lim-ination for a word.
It may well be the case that mergingtwo senses in a single category is a reasonable thing todo, if the senses do not draw interesting (for the domain)distinctions.
Therefore liminating a priori a sense of aword may be inappropriate in the domain.The (1) is computed for all the generated sets of categoriesC i, and then normalised in the \[0,1\] interval.
The effec-tiveness of this model is estimated in the following sec-tion.4 Evaluation Experiments and Discus-sion of the DafaThe algorithm was applied to the 10,235 different nounsof the Wall Street Journal (hereafter WSJ) corpus that areclassified in WordNet.
Categories are generated with h=0.4 and k=l,000.
The cardinality of each set varies, butnot uniformly, from 456 categories for UB--2000(remember that words are frequency-weighted), to I cate-gory (i.e.
the topmost entity) for UB=264,000.
Medium-high level categories (those between 50,000 and 100,000maximum words) range between 10-20 members for each383Figure 3 plots the values of G, CO, DP and 1/A for thedifferent sets of categories generated by the algorithm ofSection 2.
Alternative sets of categories are identified bytheir upperbound 3.
The figure shows that DP(Ci) has aregular decreasing behaviour, while 1/A(Ci) is less regu-lar.
The coverage CO(C i) has a rather unstable behaviourdue to the entangled structure of WordNet.
We attemptedslight changes in the definitions and computation of CO,DP and 1/A (for example, weighting words with theirfrequency), but globally, the behaviour remain as those infigure 3.To compute the score of each set Ci.
, the parameters (x,13,;?and 8 in (1) must be estimated.
To perform this task, weadopted a linear interpolation method, using SEMCOR(the semantically tagged Brown Corpus) as a referencecorpus.
In SEMCOR every word is unambiguously taggedwith its leaf-synset.To build a reference scoring function against which toevaluate our model parameters, we proceeded as follows:?
Since our categories are generated for an economicdomain (WSJ) while SEMCOR is a tagged balancedcorpus (the Brown Corpus), we extracted only thefragment of the corpus dealing with economic and fi-nancial texts.
We obtained a reference corpus in-cluding 475 of the 1,235 nouns of the WSJ corpus.?
For each set of categories Ci generated by the algo-rithm in section 2, we computed on the reference cor-pus the following two l:~rformance figures:Precision: For each Ci, let W(C i) be the set of words inthe reference corpus covered by the met C i.
For each w k in3Remember that words are weighted by their frequency inthe corpus.
This seems reasonable, but in any case we ob-served that our results do not vary when counting eachword only once.W(Ci), let S(w k) be the total set of leaves-synsets of w k inWordNet,  SR(w k) the subset of leaves-synsets of w kfound in the reference corpus, SC(w k) the subset ofleaves-synsets that reach some of the categories of C i. LetWR(Ci) ~ W(C i) be the set of w k having SC(w k) c S(Wk).Following the algorithm:for any w k in WR(C i){for any s i in SR(w k){i fs i E SC(wk) then N + =N + + freqi(w k)NtOt = NtOt + freq(w k)}where freq(w k) is the number of occurrences of w k in thereference corpus, the precision Precision(C i) is then de-fined as N+/N -t?t.
The precision measures the ability ofeach set C i at correctly pruning out some of the senses ofW(Ci).Global  reduction of ambiguity: For each C i, let S(W i)be the total number of WordNet leaves-synsets reachedby the words in WR(Ci), and SCON i) ~ S(W i) the set ofthese synsets that reach some category in C i.
By taggingthe corpus with C i, we obtain a reduction of ambiguitymeasured by:GRAmb(C i ) = (card (S(Wi))- card (SC(Wi)))/card(S(Wi))where card (X) is the number of elements in the set XStarting from these two performance figures, the globalperformance function Perf(C i ) is measured by:(2) Perf(C i ) = Precision(Q) + GRAmb(C i )The (2) is computed for all the generated sets of categoriesCi, and then normalised in the \[0,1\] interval.
The obtainedplot is the reference against which we apply a standardlinear interpolation method to estimate the values of themodel parameters c?,~,X and 8 that minimize the differ-ence between the values of the two functions for each C i.In figure 4a the (not normalised) Precision and GRAmbare plotted for the test corpus.
In figure 4b the normalisedreference performance function and the "best fitting"scoring function are shown, with the estimated values ofa,~,X and 8.While the reference function has a peak on the class set Cjwith UB--55,000 and the score function assigns the max-imum value to the class set C k with UB=62,000, the per-formance of the sets in the range j-k is very similar.
Table1 shows the values of precision and global reduction ofambiguity in the range \[~k\].UB Precision GRAmb0.338 55,00056,00059,00060,00061,00062,0000.7520.758 0.3470.748 0.3600.734 0.3780.731 0.3860.733  0.368Table 1 - Values of precision and global reduction ofambiguity for UB \[55,000-62,000\]In evaluating the method, few aspects are worth un-0,80,7  - ) - " - ~ ~ ~ ' ~iG.
.
l~a~b , - .
_, , .
.
.
_ 0,6  - ,"0,5  -0,4 - Precis ion ?
,  .
.
.
,  .
,, , ' \I ?
*~  ?
j I  ?
A tie \ I # t I I e 0,3  - ",.0 ,2  , :  = i ~ !
!
!
'.
'.'. '
. '
,~  = i \] ~ ~!
!
!'.
'.
'.
'.
~ ~ { i ~ !
!
i '. "
: " "  ~= ~\ [~!
!
! "
" " "" ~ ~ ~ = ~ " " " " " ""O O O O O O O (D C'  O (D O O O O O O O O O (~ O O O~ ~ 8 C,  C,  C,  O (D, C '  <D' C '  O O O CD' O O 'O' C '  ~ O 'O ~Figure 4a.
- Performance figures of sets C i against the reference corpus38410,90,80,70,60,50,40,30,20,10OOO?
~# I I '," ".'
~=1.o29 v 5 / / , -1  /13=0"412 \[~/: I X _ _ A .
O ~  ~ :.V,,= ?
: /I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I \[ I I I I I I I I I I I I |~P ' I?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
0o?
8 8 8 8 ?
~ = 88ooo oooo0ooo ooo o o o o o o o o o o o o o o o o oI',..- 0q  OlD ~ ?O ~ ,O~ ~ N ~ ~ ~ ~ O?%1Figure 4b - Reference function and best-fitting Score function, with estimated parameters.derlining?
The test corpus includes only 475 words of the over10,000 in our learning corpus.
This may well cause ashift of the reference scoring function, as comparedwith the "real" scoring function.?
In any case, figure 4a shows that the sets C~ havepeak performances in the range 50,000-100,d00.
Inthis range, the precision is around 73-76%, and thereduction of ambiguity is around 35%, which areboth valuable results.
We also experimented that, bychanging slightly the model parameters and/or thedefinitions of the four performance figures in the (1),in any case the peak performance of the obtainedscoring function falls in the 50,000-100,000 interval,and the function stays high around the peak, withlocal maxima.?
In other domains (see a brief summary in the conclud-ing remarks) for which we did not have a referencetagged corpus, we used (a=l ~--0,5 z=l 5=1) as modelparameters in the (1), and still observed a scoringfunction similar in shape to that of figure 4b.
Se-lected categories vary according to the domains, butthe size of the best set stays around the 10-20 cate-gories.
Evaluation is of course more problematic dueto the absence of a tagged reference corpus.Therefore, we may conclude that the method is "robust",in the sense that it correctly identifies a range of reason-able choices for the set of categories to be used, eventuallyleaving the final choice to a linguist.As for the WSJ corpus, a short analysis of the linguisticdata may be useful.
In figure 5 the 14 '"oest" selected cate-gories for nouns are listed.
Figure 6 shows four veryfrequent and very ambiguous words in the domain: bank,business, market and stock, with attached list of synsets as385generated by WordNet, ordered from left to right by theincreasing level of generality (leaf-sysnset leftmost).
Thesenses marked with '*' are those that reach some of thecategories (marked in bold in the figure) of the best-performing set, selected by the scoring function (1).
Forbank and market, we observed that the less plausible (forthe domain) senses ridge and market_grocery_store arepruned out.
The word stock retains only 5 out of 16senses.
Of these, the gunstock and progenitor sensesshould have been further dropped out, but there are 11senses that are correctly pruned, like liquid, caudex,plant, etc.
The word business till keeps its ambiguity, butthe 9 subtle distinctions of WordNet are reduced to 7senses .i.
person, individual, someone,mortal, human, soul2.
instrumentality, instrumentat ion3.
attribute4.
written_communication,written_language5.
message, content, subject_matter,substance6.
measure, quantity, amount, quantum7.
action8.
activity9.
group_act ioni0.
organizat ionii.
psychological_feature12.
possession13.
state14.
locationFigure 5 - List of best-performing categoriesWord: bankSense n.l: bank, side -> slope,incline,side ->.. .Sense n.2 (*): depository_f inancial_ inst itut ion,bank,banking_concern,banking_company -> f inan-cial institution, f inancial_organization -> institution,establishment -> organizat ion -> ...Sense n3:  bank -> ridge ->.
.
.Sense n.4: bank -> array -> ...Sense n~5 (*): bank  -> reserve,backlog, stockpile -> accumulation -> asset -> possessionSense n.6 (*): bank  -> funds, f inances,monetary_resource, cash in hand,pecuniary_resource -> asset -> possessionSense n.7" bank, cant,camber -> slope, incline,side -> ...Sense n.8 (*): savings_bank, coin_bank, money_box,bank ->container -> inst rumenta l i ty ,  ins t rumentat ion  -> ...Sense n.9: bank, bank_bui ld ing -> depository, deposit,repository -> ...Word: bus inessSense n.1 (*): business, concern,business_concern,business_organization -> enterprise -> organizat ion -> ...Sense n2 (*): commercial_enterprise, business_enterprise,business -> commerce,commercialism,mercantil ism ->group_action -> ...Sense n.3 (*): occupation,business, l ine of work, l ine -> activity ->.
.
.Sense n.4 (*): business -> concern, worry, headache, vexation -> negative_stimulus -> stimula-tion, stimulus, st imulant, input ->information -> cognition, knowledge -> psychological_featureSense n~5 (*): business -> aim, object,objective, target -> goal,end -> content,cognitive_content,mental_object -> cogni-t ion,knowledge -> psychological_featureSense n.6: business,business_sector -> sector -> ...Sense n.7 (*): business -> business_activity, commercial_activity -> activity -> ...Sense n.8: clientele, patronage, business -> people ->.. .Sense n.9 (*): business, tage_business,byplay -> acting,playing, playacting,performing -> activity -> ...Word: marketSense n.l: market -> class,social_class, ocio-economic_class -> ...Sense n.2: grocery_store,grocery, market -> marketplace, mart ->.. .Sense n,3 (*): market, marketplace -> activity ->.. .Sense n.4 (*): market,securities industry -> industry -> commercial_enterprise -> nterprise -> organizat ion -> ...Word: stockSense n.1 (*): stock -> capital,working_capital -> asset -> possessionSense n.2 (*): stock,gunstock -> support -> device -> instrumenta l i ty ,  ins t rumentat ion  -> ...Sense n.3: stock, inventory -> merchandise,wares,product -> ...Sense n.4 (*): stockcertif icate, stock -> security,certificate -> le-gal_document, legal_instnm~ent,official_document,instrument -> document,wr i t tendocument,  papers -> writ-ing, written material  -> wr i t ten_communicat ion ,wr i t ten_ language -> ...Sense n,5 (*): store,stock,fund ->accumulat ion -> asset -> possessionSense n.6 (*): stock -> progenitor,primogenitor -> ancestor,ascendant,ascendent,antecedent -> relative,relation ->person,individual,someone,mortal,human, soul -> ...Sense n.7: broth,stock -> soup -> ...Sense n.8: stock,caudex -> stalk, stem ->.. .Sense n.9: stock-> plant_part -> ...Sense n.lO: stock, gil lyflower -> flower -> ...Sense n.11: Malcolm_stock, stock -> flower -> ...Sense n.12: l ineage, line,line_of descent,descent,bloodline,blood_line,blood,pedigree, ancestryorigin, parent-age, stock -> genealogy, family_tree ->...Sense n.13: breed,strain,stock,variety -> animal_group -> ...Sense n.14: stock -> lumber, t imber -> ...Sense n.15: stock-> handle, grip,hold -> ...Sense n.16: neckcloth,stock -> cravat -> ...F igure 6.
Selected synsets for the words bank, business, market and stock.3865 Concluding RemarksIt has already been demonstrated in (Basili et al 1996)that tagging a corpus with semantic ategories triggers amore effective lexical earning.
However, overambiguityof on-line thesaura is known as the major obstacle to au-tomatic semantic tagging of corpora.
The method pre-sented in this paper allows an efficient and simple selec-tion of a fiat set of domain-tuned categories, that dramati-cally reduce the initial overambiguity of the thesaurus.We measured a 73% precision in reducing the initial am-biguity, and a 37% global reduction of ambiguity.Significantly, our method selects a limited number ofcategories (10-20, depending upon the learning corpusand the model parameters), out of the initial 47,110 leaf-synsets of WordNet 4.We remark that our experiment is on large, meaning thatwe automatically evaluated the performance of the modelon a large set of nouns taken from the Wall StreetJournal.
Most sense disambiguation r semantic taggingmethods evaluate their performances manually, againstfew very ambiguous cases, with clear distinctions amongsenses.
Instead, WordNet draws very subtle and fine-grained distinctions among words.
We believe that ourresults are very encouraging.The model parameters for category selection has beentuned on SEMCOR, but a correctly tagged corpus is notstrictly necessary.
In our experiments, we applied a scor-ing function similar to that obtained for the Wall StreetJournal to two other domains, acorpus of Airline reser-vations and the Unix handbook.
We do not discuss thedata here for the sake of space.
The method constantlyselects a set of categories at the medium-high level ofgenerality, different for each domain.
The selection"seems" good according to our linguistic intuition of thedomains, but the absence of a correctly tagged corpusdoes not allow a large-scale evaluation.In the future, we plan to demonstrate hat the methodproposed in this paper, besides reducing the overambigu-ity of on-line thesaura, improves the performance of lexi-cal learning methods that are based on semantic tagging,such as PP disambiguation, case frame acquisition andand sense selection, with respect to a non-optimal choiceof semantic ategories.6 AcknowledgementsThe method presented in this paper has been developedwithin the context of the ECRAN project LE 2110, fundedby the European Community.
One of the main researchobjectives of ECRAN is lexical tuning, being semantictagging and sense disambiguation two important andpreliminary objectives.
This paper approached the prob-lem of domain-appropriate semantic tagging.We thank Christian Pavoni who developed much of the4We used WordNet in this experiment, because it is nowveryppo  ular among scholars in.
lexical statistics, butclearly our method could be apphed to any on-line tax-onomy or lattice.387software used in this experiment, as well as all our part-ners in the ECRAN project.References(Agirre and Rigau, 1996) E. Agirre and G. Rigau, WordSense Disambiguation using Conceptual Density,proc.
of COLING 1996(Basili et al 1992) Basili, R., Pazienza, M.T., Velardi, P.,"Computational Lexicons: the Neat Examples and theOdd Exemplars", Proc.
of Third Int.
Conf.
on AppliedNatural Language Processing, Trento, Italy, 1-3 April,1992.
(Basili et al 1996) Basili, R., M.T.
Pazienza, P. Velardi,An Empyrical Symbolic Approach to Natural Lan-guage Processing, Artificial Intelligence, August 1996(Basili et al 1996b) R. Basili, R., M.T.
Pazienza, P. Ve-lardi, Integrating eneral purpose and corpus-basedverb classification, Computational Linguistics, 1996(Brill and Resnik, 1994) E. Brill and P. Resnik, A trans-formation-based approach to prepositional phraseattachment disambiguation, proc.
of COLING 1994(Chen and Chen, 1996) K. Chen and C. Chen, A rule-basedand MT-oriented Approach to Prepositional PhraseAttachment, proc.
of COLING 1996(Dagan et al 1994)Dagan I., Pereira F., Lee L., Similarly-based Estimation of Word Co-occurrences Probabili-ties, Proc.
of ACL, Las Cruces, New Mexico, USA,1994.
(Dolan, 1994) W. Dolan, Word Sense Ambiguation: Clus-tering Related Senses, Proc.
of Coling 1994(Hearst and Schuetze, 1993) M. Hearst and H. Schuetze,Customizing a Lexicon to Better Suite a Computa-tional Task, ACL SIGLEX, Workshop on Lexical Ac-quisition from Text, Columbus, Ohio, USA, 1993.
(Krovetz and Croft, 1992) R. Krovetz and B. Croft, Lexi-cal Ambiguity and Information Retrieval, in ACMtrans, on Information Systems, 10:2, 1992(Grishman and Sterling, 1994) R. Grishman, J. Sterling,Generalizing Automatically Generated SelectionalPatterns, Proc.
of COLING '94, Kyoto, August 1994.
(Pereira et al, 1993) Pereira F., N. Tishby, L. Lee, Distri-butional Clustering of English Verbs, Proc.
of ACL,Columbus, Ohio, USA, 1993.
(Pustejovsky, 1995) J. Pustejovsky, The generative Lexi-con, MIT Press, 1995(Resnik, 1995) P. Resnik, Disambiguating Noun Group-ings with respect o Wordnet Senses, proc.
of 3rdWorkshop on Very Large Corpora, 1995(Yarowsky, 1992) Yarowsky D., Word-sense disam-biguation using statistical models of Roget's cate-gories trained on large corpora, Proc.
of COLING 92,Nantes, July 1992.COtsuro et al 1993) T. Utsuro, Y. Matsumoto and M. Na-gao, "Verbal case frame acquisition from BilingualCorpora" Proc.
of IJCAI, 1993
