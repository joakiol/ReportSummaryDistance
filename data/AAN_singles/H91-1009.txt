DRAGON SYSTEMS RESOURCE MANAGEMENTBENCHMARK RESULTS FEBRUARY 19911James Baker, Janet Baker, Pard Bamberg, Larry Gillick,Lori LameI, Robert Roth, Francesco Scattone, Dean Sturtevant,Ousmane Ba, Richard BenedictDragon Systems, Inc.320 Nevada StreetNewton, Massachusetts 02160DRAGON@A.ISI.EDUTEL: (617) 965-5200FAX: (617) 527-0372ABSTRACTIn this paper we present preliminary results obtained at Dragon Systems onthe Resource Maaaagernent benchmark task.
The basic conceptual units ofour system are Phonemes-m-Context (PICs), which are represented asHidden Mmkov Models, each of which is eapressed as a sequence ofPhonetic Elements (PELs).
The PELs corresponding to a given phonemeconstitute a kind of alphabet for the representation of PICs.For the speaker-dependent tests, two basic methods of training the acousticmodels were investigated.
'nac first method of training the Resouro~Managemera models is to ~e-estimate the models for each test speaker fromthat speaker's training data, keeping the PEL spellings of the PICs fixed.
Thesecond approach is to use the re-estimated models from the first melhod toderive a segmentation f the training data, then to respall the PICs in a hrgelyspeaker-depmdmt anner in order to improve the representation f speakerdifferences.
A full explanation of these methods i  given, as are results usingeach method.In addition to repotting on two different training slrategies, we disoass N-Best results.
The N-Best algorithm is a modification of the algorithmproposed by Soong and Huang at the Jtme 1990 workshop.
This algorithmruns as a post-processing step and uses an A*-search (an algorithm alsoknown as a 'stack decoder').1.
INTRODUCTIONIn this paper we report on some preliminary work done atDragon Systems' on the Resource Management benchmarktask.
First, a brief overview of Dragon Systems speaker-dependent, continuous peech recognition system is given.Next, the modifications necessary to evaluate this system onthe RM task are described.
Our goal has been to make changesto the standard continuous speech recognition system in waysthat are in line with Dragon's long term aims.
The primarymodifications so far have been in the areas of signal processingand speaker-dependent training.
The speaker-dependenttraining is described in detail in Section 4.Recognition results are given for the RM1 speaker-dependent development test data and for the Feb91 evaluationtest material.
In presenting these results, we make a start atevaluating the transfer characteristics of our system whenresponding to changes in the speaker, the hardware, and thesignal processing algorithm.
Our experimentation wasperformed using the speaker-dependent development test~ta~ and these dam arc used to compare system configurationsin this paper.
Since we befieve that we are still on a steeplearning curve, the February 1991 evaluation lest material wastun through the system only one time, and thus comparativeresults using the evaluation dam are not yet available.2.
OVERVIEW OF THE DRAGONCSR SYSTEMDragon Systems' continuous peech recognition systemwas presented at the June 1990 DARPA meeting \[1,2,3\].
Thesystem is speaker-dependent a d was demonstrated to becapable of near real-time performance on an 844 word task(mammography reports), when running on a 486-based PC.The signal processing is performed by an additionalTMS32010-based board.
The speech is sampled at 12 kHz andthe signal representation is quite simple: there are only eightparameters - - 7 spectral comlxments covering the region upto 3 kHz and an overall energy ~ameter - -  a complete set ofwhich are computed every 20 ms and used as input to theHMM-based recognizer.The fundamental concepaml unit used in the system is the"phoneme-in-context" or PIC, where the word "context" in1.
This work was sponsored by the Defense Advanced Research Projects Agency and was monitored by the Space and NavalWarfare Systems Command under Conlract N000-39-86-C-0307.59principle refers to as much information about the surroundingphonetic environment as is nec~ to determine the acousticcharacter of the phoneme in question.
Several relatedalternative approaches have appeared in the literature \[5,6,7\].Currently, context for our models includes the identity of theprec~ing and succeeding phonemes as well as whether thephoneme is in a prepausally lengthened segment PICs aremodeled as a sequence of PELs (phonetic elements), each ofwhich represents a "slate" in an HMM.
PE~ may be sharedamong PIC models representing the same phoneme.
Adetailed description of models for PICs and how they aretrained may be found in \[2\].
Modifications made to the PICtraining procedure are presented in Section 4.Recognition uses frame-synchronous dynamicprogramming to extend the sentence hypotheses subject o thebeam pruning used to eliminate poor paths.
Another importantcomponent of the system is the rapid matcher, described in \[3\],which limits the number of word candidates that can behypothesized to start at any given frame.
Some alternativeapproaches tothe rapid match problem have ~so been outlinedby others \[8,9,10\].A lexicon for the RM task had to be specified beforemodels could be built.
Pronunciations were supplied for eachentry in the SNOR leficon by extracting them from ourstandard lexicon.
Any entries not found in Dragon's currentgeneral English lexicon were added by hand.
The set ofphonemes used for English contains 24 consonants, 17 vowels(each of which may have 3 degrees of stress), and 3 syllabicconsonants.
Approximately 22% of the entries in the SNORlexicon have been given multiple pronunciations.
Thesepronunciations may reflect stress differences, uch as stressedand unstressed versions of function words, and expectedpronunciation alternatives.Roughly 39,000 PICs are used in modeling the vocabularyfor this task.
The set of PICs was deXermined by finding all ofthe PICs that can occur given the constraint hat sentencesmust conform to the word pair grammar.
The Iralning dataused to build PIC models for the reference speaker comesprimarily fi'om general Engli~ isolated words and phrases,supplemented by a few hundred phrases from the RM1training sentences.
The generation andAraining of PICs isdis~ssed in more detail in the next section.3.
MODIFICATIONS TO THE SYSTEM FORUSE WITH THE RM TASKIn order to be able to run the RM benchmark task on theDragon speaker-dependent continuous peech recognitionsystem, several modifications were necessary.
Thesemodifications primarily concerned the signal acquisition andpreprocessing stages.
Prior to this evaluation, the system hadonly been evaluated on data obtained from Dragon's ownacquisition hardware.The signal processing, as described above, has alwaysbeen performed by the signal acquisition board.
Thus it wasthought possible that the performance of the system would behighly tuned to the hardware.
In order to run the RM clamthrough the system, software was written to emulate thehardware.
One question to be addressed is how well the signalprocessing software does in fact emulate the hardware.
Toassess this, a small test was performed using new dam fromDragon's reference speaker.
The speaker ecorded, using theDragon hardware, three sets of 100 sentences selected fromthe development test texts (those of BEF, CMR, and DAS).Recognition was performed, using the reference speaker'sbase models after adapting to the standard training sentences,and an average word error rate of 3.5% was recorded.
The factthat the rate is comparable to error rates of some of the betterRM1 speakers uggests that we have errl!dated our slandardsignal proc~ng reasonably well.
An explicit comparison ofperformance on the reference speaker using our standardhardware and our software mulation will be available soon.The language model used in the CSR system returns a logprobability indicating the score of the candidate word.
Thiswas modified to return a fixed score if the word is allowed bythe word-pair grammar or a flag denoting that the sequence isimpermissible.The standard rapid match module was used in all of theexperiments reported in this paper, in order to reduce processingtime.
We have not focused on the issue of proc~ing time inthe current phase of our research, and have therefore modifiedour standard rapid match parameter settings to be suffidentlyconservative so as to insure that only a small proportion of theerrors are due to rapid match mistakes.4.
TRAINING ALGORITHMS FOR THESPEAKER-DEPENDENT MODELSDragon's strategy for phoneme-based training wasdescribed in detail in an earlier eport\[2\].
We have used a fullyautomatic version of the same strategy to build speaker-dependent models for each of the RM1 speakers, using thereference speaker's models to provide an initial segmentation.The goal was to build models in which the acoustic parametersand duration estimates were based almost entirely on the 600training utterances for each speaker, using the referencespeaker's models only in rare cases for which no relevanttraining ~ta is available.The recognition model for a word (or sentence) is obtainedby concatenating a sequence of PICs, each of which is, in turn,were selected in the course of the semi-automatic labeling of60a large amount of data acqtfired horn the reference speaker.about 9000 isolated words and 6000 short phrases.
In changingto the Resource Management task, an additional set of task-spccitic Iralning utterances flora lhe reference speaker wereadded.
Although less than 10% of the training data was drawnfrom the Resource Management task, most of the PICs that arelegal according to the word-pair grammar are representeclsomewhere in lhe total training set.
Legal PICs missing fromthe training set are typically like the sequence "ah-uh-ee" thatwould occur in "WICHITA A EAST': for the most part, theydo not occur in the training sentences and seem unlikely tooccur in evaluation sentences.The reference speaker's models are speakerMependent ithree dk~nct ways:1.
The parameters of the PELs depend on the spectralcharacteristics of the reference speaker's voice..
The durations for the PEI.~ in each Markov model fora PIC depend on the reference speaker's peaking roteand other features of his speech..
The sequence of PELs used in the Markov model fora PIC depends on what allophone the reference speakeruses in a given contextWe report on two techniques for creating speaker-dependent PICs starting with the reference speaker's models.The first is a straightforward adaptation algorithm, in which anew speaker's training utterances are segmented into PICsand PELs using a set of base models, and the segments are thenused to re-estimate the immmeters of the PELs and of theduration models.
This algorithm is typically run multipletimes.
This ~ 'oach  is very effective in dealing with (1),since the 600 training sentences include dam for almost all ofthe PELs.
This strategy is less effecdve in dealing with (2),since only about 6000 of the 30000 PICs occur in the trainingscripts.
Adaptation alone, however, can do nothing to change(3) the "spelling" of each PIC in terms of PELs.The first technique uses the following two steps:Step 1: The data from all 12 ofthe spe,~ers were used toadapt the reference speaker's models.
Three passes ofadaplafion weze performed with these data.
Since Dragon'salgorithm does not yet use mixture distributions, this has theeffect of averaging together spectra for male and femaletalkers and generally "washing out" formants in PELs forvowels.
The resulting "multiple speaker" models are not goodenough to do speaker-independent recognition, but they serveas a better basis for speaker adaptation than do the referencespeaker's models.Step 2: For a given speaker, a maximum of six passes ofadaptation are carried out, starting from the multiple-speakermodels.
The resulting models are used to segment the utterancesinto phonemes.
At this point we have a good speaker-dependent set of PEL models, and a set of segmentations withwhich to proceed further.The second technique begins with the models produced bythe first technique together with the segmentation of thetraining data into phonemes done using those same models.Using this automatic labeling, speaker-dependent training isperformed for each of the RM1 speakers, to produce a newspeaker-dependent set of PIC models - -  with new PELspellings and duration models.
The algorithm is as follows:Step 1: For each phoneme in turn, all the labeled trainingdam for that phoneme are extracted from the training sentences.For each PIC that involves the phoneme, an appropriateweighted average of these data is taken to create a spectralmodel (a sequence of expected values for each frame) for thePIC.
Details of this averaging process may be found in ourearlier eport\[2\], but the key idea is to take a weighted averageof phoneme tokens that represent the PIC to be modeled orclosely related PICs.The number of PICs to be constructed for each phonemeis of the same order of magnitude as the number of examplesof the phoneme in the 600 trairfing sentences.
Since there areexamples of only about 6000 PICs in the RM1 trainingsentences, for most PICs the models must be based entirely ondata with either the left or right context incorrect.
For aboutone-fifth of the 30000 PICs, therewere insufficient relateddata to construct a spectlal model (using the usual criteria for"relatedness").
This is frequently the case when a diphonecorresponding to a legal word pair fails to occur in the trainingsentences.Step 2: Dynamic programming is used to construct hesequence of PELs that best represents the spectral model foreach PIC, thereby 'Yespelling" the PIC in terms of PELs.
Thisresults in a speaker-dependent PEL spelling for each PIC.
Inthe process, speaker-dependent durations for each PEL in aPIC are also computed.Step 3: Step 2 results in respelled PICs for those PICs forwhich sufficient raining data are available.
For the remainingapproximately 6000 PICs, the adapted PIC models of thereference speaker are used (as in technique 1).
Merging thesePies results in a model for every legal PIC in the word-pairgrammar.61Table 1: Comparison of recognition results for RM1 speakers using the two methods of speaker Iraining: speaker dependentmodels (SD-PELs) and speaker-dependent r spelling of PICs (SD PICs).
Word error rates are repo~d as percentages for theRM1 development test clam and the Feb91 evaluation data.SpeakerBEFCMR(0DAS(f)DMS(0DTBDTD(OERSHXS(0JWSPGHRKMTABAverageSD-PELsDevelopment10.56.94.34.17.65.612.43.16.35.313.93.6SD-PICsDevelopment7.26.82.93A3.64.410.52.5?
4.75.59.84.37.0 5.4SD-PICsEvaluation6.315.01.93.67.27.812.65.64.59.19.95.37.5Step 4: A final pass of adaptation consists of resegmentingthe training data into PELs and then re-estimating theparameters of the speaker-dependent PELs.
In the process,duration distributions are also re-estimated.The above algorithm to create speaker-dependent PICmodels provides two sets of models with which we haveexperimented.
The first set is referred to as speaker-dependentRM models.
The second set is the output of the final stage, andis referred to as the respeHed speaker-dependent RM models.Both sets of speaker-dependent models may contain unchangedPICs from the original reference speakex when no trainingdata was available - -  mainly unchanged uration models,since most PELs are used in a variety of PICs.5.
RECOGNITION EXPERIMENTSAND DISCUSSIONIn this section we present results making use of the two setsof speaker dependent models, as well as results on postprocessing with the N-best algorithm.5.1 Comparison of two methods for speaker-dependent trainingThe error rates using each of the training strategies areshown in Table 1.
In this table we display the word error rateson the 100 development test sentences for each of the 12 RM1speakers, and we also display the performance of the respelledmodels on the Feb91 evaluation data, which consisted of 25sentences for each speaker.Table 2: Cumulative percentage of ccorrect sentences on thechoice list using the N-Best algorithm.Choice # Cumulative %5678910111213141572838788909192929393939393939462Analysis of Errors for Speaker-DependentRespelled PICsIn the course of our research it has been enlightening toinvestigate the errors.
We will now focus our discussion onthe performance of the respelled models when recognizing thedevelopment data: The word error rates are seen to range froma low of 2.5% for speaker HXS to 10.5% for ERS, with anoverall average error rate of 5A%.
When the very samesystem is run without he rapid match module, the amount ofcompmafon is vastly increased, but there is only a smallreduction of the observed overall error rate from 5.4% to5.1%.
Roughly 62% of the errors involve function wordsonly, and the remaining 38% involve a content word (and mayalso include a function word error).
Function words have anerror rate of 7.6% compa~d to 2.5% for content words.
Themost common content word error is "SPS-40" which is oftenmisrecognized as"SPS-48".
Other content word errors ofteninvolve homophones ( uch as "ships+s" --~ "ships").
Functionword deletions are more common than insertions, andsubstitutions may be symmetric ?
'and" --> "in" are as frequentas "in" --> "and") or asymmetric C'theh ~' --> "the" but thereverse confusion does not occur).
Other common errorsinvolve contractions: "what is" -> ''what+s" and ''when will"- -> "when+ll".Use of alternate pronur~iafionsApproximately 22% of the lexical entries have alternatepronunciations.
These variants are used to express expectedpronunciation alternations and/or stress differences.5.2 N-Best Algorithm Test.A recognition pass using an N-Best algorithm wasperformed on the development test data.
The N-Best algorithrnwhich we have implemented is similar to the one proposed bySoong and Huang\[4\].
It runs as a post-processing step and isessentially a stack decoder which processes the speech inreverse time.
Computational results saved during the forwardpass are used to provide very close approximations to the bestscore of a full transcription which extends a reverse partialtranscription.
Although a more complete description of thealgorithm is beyond the scope of the paper, we note that a keydifference between the algorithm we use and that of Soongand Huang is that we do a full acoustic match in the reversepass (i.e., we process the speech dam).
Also, the reason ourextension sc~es are only approximate is that in our currentimplemenlation, the forward and reverse acoustic match scoresare different.The test was run on the 1200 utterances from the RM1development sentences, 100 each from the 12 RM1 speakers.The parameters conlrolling the N-Best were set conservatively.With high confidence, the 100 best alternative sentencetranscriptions were delivered (slowing down the recognitionby about a factor of six).
These transcriptions included onesdiffering only in placement of internal pauses and/or alternativepronunciations.
If such transcriptions are considered i entical,17 choices were delivered on average.
The results given belowdo consider such transcriptions as being identical.The forward algorithm determined the correct ranscription70% of the time, and the N-Best algorithm delivered it as achoice 94% of the time (almost always as one of the top 15).That is, for around 80% of the misrecognitions, the correctionwas on the choice list A cumulative count (based on the 1200test utterances) i  given in Table 2.
For instance, the ccorrecttranscription was one of the top 5 choices 90% of the time.7.
REFERENCES1.
P. Bamberg, Y.L.
Chow, L. GiUick, R. Roth, andD.
Sturtevant, "The Dragon Continuous Speech RecognitionSystem: A Real-Time Implementation," Proceedings ofDARPA Speech and Natural Language Workshop, June 1990,Hidden Valley, Pennsylvania, pp.
78-81.2.
P. Bamberg and L. Gillick, "Phoneme-in-Context Modelingfor Dragon's Continuous Speech Recognizer," Proceea~ngsof DARPA Speech and Natural Language Workshop, June1990, Hidden Valley, Pennsylvania, pp.
163-169.3.
L. Gillick and R. Roth, "A Rapid Match Algorithm forContinuous Speech Recognition," Proceedings of DARPASpeech and Natural Language Workshop, June 1990 HiddenValley, Pennsylvania, pp.
170-172.4.
F.K.
Soong and E.-F. Huang, "A Tree-Trellis Based FastSearch for Finding the N-Best Sentence Hypotheses inContinuous Speech Recognition," Proceedings of the DARPASpeech and Natural Language Workshop, June 1990, HiddenValley, Pennsylvania, pp.
12-19.5.
R. Schwartz et al, "Context-Dependent Modeling forAcoustic-Phonetic Recognition of Continuous Speech", IEEEInternational Conference on Acouaics, Speech, and SignalProcessing, April 1985.6.
Bahl et al, "Large Vocabulary Natural LanguageContinuous Speech Recognition", IEEE InternationalConference on Acoustics, Speech, and Signal Processing,May 1989.637.
K.F.
Lee a at, "The Sphinx Speech Recognition System",IEEIq International Conference on Acoustics, Speech, andSignal Processing, May 1989.8.
Lalit Bahl, Raimo Bakis, Peter V. de Souza and Robert L.Mercer, "Obtaining Candidate Words by Polling in a LargeVocabulary Speech Recognition System", ICASSP 88, NewYork City, April 1988.9.
Xavier L Aubert, "Fast Look-Ahead Pruning Strategies inContinuous Speech Recognition", ICASSP 89, Glasgow, May1989.10.Lalit Bahl, P. S. Gopalakrishnan, D. Kanevsky,D.
Nahamoo, "Matrix Fast Match: A Fast Method forIdentifying a Short List of Candidate Words for Decoding",ICASSP 89, Glasgow, May 1989.64
