Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1069?1080,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsLearning the Information Status of Noun Phrases in Spoken DialoguesAltaf Rahman and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{altaf,vince}@hlt.utdallas.eduAbstractAn entity in a dialogue may be old, new,or mediated/inferrable with respect to thehearer?s beliefs.
Knowing the informationstatus of the entities participating in a dia-logue can therefore facilitate its interpreta-tion.
We address the under-investigated prob-lem of automatically determining the informa-tion status of discourse entities.
Specifically,we extend Nissim?s (2006) machine learningapproach to information-status determinationwith lexical and structured features, and ex-ploit learned knowledge of the informationstatus of each discourse entity for coreferenceresolution.
Experimental results on a set ofSwitchboard dialogues reveal that (1) incor-porating our proposed features into Nissim?sfeature set enables our system to achieve state-of-the-art performance on information-statusclassification, and (2) the resulting informa-tion can be used to improve the performanceof learning-based coreference resolvers.1 IntroductionInformation status is not a term unfamiliar to re-searchers working on discourse processing prob-lems.
It describes the extent to which a discourse en-tity, which is typically a noun phrase (NP), is avail-able to the hearer given the speaker?s assumptionsabout the hearer?s beliefs.
According to Nissim etal.
(2004), a discourse entity can be new, old, or me-diated.
Informally, a discourse entity is (1) old tothe hearer if it is known to the hearer and has pre-viously been referred to in the dialogue, (2) new ifit is unknown to her and has not been previously re-ferred to; and (3) mediated if it is newly mentionedin the dialogue but she can infer its identity froma previously-mentioned entity.
Information statusis a subject that has received a lot of attention intheoretical linguistics (Halliday, 1976; Prince, 1981;Hajic?ova?, 1984; Vallduv?
?, 1992; Steedman, 2000).Knowing the information status of discourse enti-ties can potentially benefit many NLP applications.One such task is anaphora resolution.
While there isgeneral belief that definite descriptions are mostlyanaphoric, Vieira and Poesio (2000) empiricallyshow that only 30% of these NPs are anaphoric.Without being able to determine whether an NP isanaphoric, an anaphora resolver will attempt to re-solve every NP, potentially damaging its precision.Since new entities are by definition new to the hearerand therefore cannot refer to a previously-introducedNP, knowledge of information status could be usedto improve anaphora resolution.Despite the potential usefulness of informationstatus in NLP tasks, there has been little work onlearning the information status of discourse entities.To investigate the plausibility of learning informa-tion status, Nissim et al (2004) annotate a set ofSwitchboard dialogues with such information1 , andsubsequently present a rule-based approach and alearning-based approach to acquiring such knowl-edge from the manual annotations (Nissim, 2006).Our goals in this paper are two-fold.
First, wedescribe a learning approach to the under-studiedproblem of determining the information status ofdiscourse entities that extends Nissim?s (2006) fea-ture set with two novel types of features: lexicalfeatures and structured features based on syntacticparse trees.
Second, we employ the automatically1These and other linguistic annotations on the Switchboarddialogues were later released by the LDC as part of the NXTcorpus, which is described in detail in Calhoun et al (2010).1069acquired knowledge of information status for coref-erence resolution.
Experimental results on Nissim etal.
?s (2004) corpus of Switchboard dialogues showthat (1) adding our linguistic features to Nissim?sfeature set enables our system to outperform her sys-tem by 8.1% in F-measure, and (2) learned knowl-edge of information status can be used to improvecoreference resolvers by 1.1?2.6% in F-measure.The rest of this paper is organized as follows.
Wefirst illustrate with examples the concepts of new,old, and mediated entities.
Then, we describe thedataset and the feature set that Nissim (2006) usedin her approach.
After that, we introduce our lexi-cal and structured features.
Finally, we evaluate thedetermination of information status as a standalonetask and in the context of coreference resolution.2 Old, New, and Mediated EntitiesSince the concepts of old, new, and mediated entitiesare not widely known to researchers working outsidethe area of discourse processing, in this section wewill explain them in more detail.The terms old and new information have meanta variety of things over the years (Allerton, 1978;Prince, 1981; Horn, 1986).
Since we use Nissimet al?s (2004) corpus for training and evaluation,the definitions of these concepts we present here arethose that Nissim et al used to annotate their cor-pus.
According to Nissim et al, their definitions arebuilt upon Prince?s (1981), and the categorizationinto old, new, and mediated entities resemble thoseof Strube (1998) and Eckert and Strube (2001).Old.
As mentioned before, an entity is old if it isboth known to the hearer and has been mentioned inthe conversation.
More precisely, an entity is old if(1) it is coreferential with an entity introduced ear-lier, (2) it is a generic pronoun, or (3) it is a personalpronoun referring to the dialogue participants.
Toexemplify, consider the following sentences.
(1) I was angry that he destroyed my tent.
(2) You cannot leave until the test is over.In Example 1, my is an old entity because it iscoreferent with I.
In Example 2, You is an old entitybecause it is a generic pronoun.Mediated.
An entity is mediated if it has not beenpreviously introduced in the conversation, but can beinferred from already-mentioned entities or is gener-ally known to the hearer.
More specifically, an entityis mediated if (1) it is a generally known entity (e.g.,the Earth, China, and most proper names), (2) it isa bound pronoun, or (3) it is an instance of bridging(i.e., an entity that is inferrable from a related entitymentioned earlier in the dialogue).
As an example,consider the following sentences.
(3a) He passed by the door of Mary?s house andsaw that the door was painted purple.
(3b) He passed by Mary?s house and saw thatthe door was painted purple.In Example 3a, by the time the hearer processesthe second occurrence of the door, she has alreadyhad a mental entity corresponding to the door (af-ter processing the first occurrence).
As a result, thesecond occurrence of the door is an old entity.
InExample 3b, on the other hand, the hearer is not as-sumed to have any mental representation of the doorin question, but she can infer that the door she sawwas part of Mary?s house.
Hence, this occurrence ofthe door is a mediated entity.
In general, an entitythat is related to an earlier entity via a part-wholerelation or a set-subset relation is mediated.New.
An entity is new if it has not been introducedin the dialogue and the hearer cannot infer it frompreviously mentioned entities.In case more than one class is appropriate fora given entity, Nissim et al employ additional tie-breaking rules.
Suppose, for instance, that we havetwo occurrences of China in a dialogue.
The secondoccurrence can be labeled as old (because it is coref-erential with an earlier entity) or mediated (becauseit is a generally known entity).
According to Nissimet al?s rules, the entity will be labeled as old.3 DatasetWe employ Nissim et al?s (2004) dataset, whichcomprises 147 Switchboard dialogues.
A total of68,992 NPs are annotated with information status:51.2% of them are labeled as old, 34.5% as mediated(henceforth med), and 14.3% as new.
Nissim (2006)randomly split the instances created from these NPsinto a training set (for classifier training), a develop-ment set (for feature development), and an evalua-tion set (for testing).
Hence, the NPs from the same1070Training Testold 31358 (51.7%) 3931 (47.4%)med 20778 (34.2%) 3036 (36.6%)new 8567 (14.1%) 1322 (16.0%)total 60703 (100%) 8289 (100%)Table 1: Information status distribution of NPs.document may be split across different sets.Unlike Nissim (2006), we partition the 147 dia-logues (rather than the instances) into a training set(117 dialogues) and a test set (30 dialogues).
Inother words, we do not randomize the instances, aswe believe that it represents an unrealistic evalua-tion setting, for the following reasons.
First, in prac-tice, the test dialogues may not be available until testtime.
Second, we may want to examine how a sys-tem performs on a given dialogue.
Finally, random-izing the instances does not allow us to apply learnedknowledge of information status to coreference res-olution, which needs to be performed for each dia-logue.
The information status distribution of the NPsin the training and test sets are shown in Table 1.4 Baseline SystemIn this section, we describe our baseline system,which adopts a machine learning approach to deter-mining the information status of a discourse entity.Building SVM classifiers for information-statusdetermination.
We employ the support vectormachine (SVM) learner as implemented in theSVMlight package (Joachims, 1999) to train threebinary classifiers, one for predicting each of thethree possible classes (i.e., new, old, and med), us-ing a linear kernel in combination with the one-versus-all training scheme.2 Each training instancerepresents a single NP and consists of the sevenmorpho-syntactic features that Nissim (2006) usedin her learning-based approach (see Table 2 for anoverview).
Following Nissim, we extract the NPsdirectly from the gold-standard annotations, but thefeatures are computed entirely automatically.2SVM was chosen because it provides the option to employkernels.
The reason why we train three binary classifiers ratherthan just one multi-class classifier (using SVMmulticlass) is thatSVMmulticlass does not permit the use of a non-linear kernel,which we will need to incorporate structured features later on.Feature Valuesfull prev mention numericmention time {first,second,more}partial prev mention {yes,no,NA}determiner {bare,def,dem,indef,poss,NA}NP type {pronoun,common,proper,other}NP length numericgrammatical role {subject,subjpass,pp,other}Table 2: Nissim?s feature set.The seven features are all intuitively useful fordetermining information status.
For instance, if anNP, NPk, and a discourse entity that appears beforeit have the same string (full prev mention), then NPkis likely to be an old entity.
Mention time is the cat-egorical version of full prev mention and thereforeserves to detect old entities.
Partial prev mentionis useful for detecting mediated entities, especiallythose that have a set-subset relation with a precedingentity.
For instance, your dogs would be considereda partial previous mention of my dogs or my threedogs.
The value ?NA?
stands for ?not applicable?,and is used for pronouns.
Determiners and NP typeare likely to be helpful for all three categories.
Forinstance, indefinite NPs and pronouns are likely tobe new and old, respectively.
The ?NP length?
fea-ture is motivated by the observation that old entitiestend to contain less lexical materials than new enti-ties.
For instance, subsequent references to BarackObama may simply be Obama.Applying the classifiers.
To determine the infor-mation status of an NP in a test dialogue, we createan instance for it as during training and present itindependently to the three binary SVM classifiers,each of which returns a real value representing thesigned distance of the instance from the hyperplane.We assign the instance to the class that is associatedwith the most positive classification value.5 Our FeaturesWe propose to extend Nissim?s (2006) feature setwith two types of features.5.1 Lexical FeaturesAs discussed, an entity should be labeled as med if ithas not been introduced in the dialogue but is gener-1071ally known to a human.
Whether an entity is ?gener-ally known?
may be easily determined by a humanbut not by a machine, since world knowledge is in-volved in the decision process.
In particular, Nis-sim?s feature set does not contain any features thatencode the notion of a ?generally known?
entity.Hence, it would be desirable to augment Nissim?sfeature set with features that indicate whether an en-tity is generally known or not.
One way to do this isto (1) create a list of generally known entities, andthen (2) create a binary feature that has the valueTrue if and only if the entity under consideration ap-pears in this list.
The question, then, is: how canwe obtain the list of generally known entities?
Wemay manually assemble this list, but this could bea labor-intensive task.
As a result, we propose toacquire this kind of world knowledge automaticallyfrom annotated data.Specifically, we augment Nissim?s feature setwith the set of unigrams that appear in the trainingdata.
Given a training/test instance (i.e., discourseentity), we compute the values of its unigram fea-tures (henceforth lexical features) as follows.
Foreach unigram, we check if it appears in the stringrepresenting the discourse entity.
If so, its featurevalue is 1; otherwise, its value is 0.
For instance, ifthe entity is the red hat, then all of its lexical featuresexcept the, red, and hat will have a value of 0.It should perhaps not be too difficult to see whythese lexical features are useful for the information-status classifier: these features enable the SVMlearner to determine the extent to which a unigramcorrelates with each class.
For instance, from the an-notated data, the learner will learn that any instanceof China cannot be labeled as new, and the deci-sion of whether it should be an old entity or a medentity depends on whether it is coreferential with apreviously-mentioned entity.
Hence, the use of lex-ical features allows the learner to implicitly acquiresome world knowledge.We believe that lexicalization is an important steptowards building high-performance text-processingsystems.
In fact, lexicalized models have demon-strated their effectiveness in other areas of languageprocessing, such as syntactic and semantic parsing.While lexicalized models may be less portable tonew genres and domains than their unlexicalizedcounterparts, we believe that this issue can be han-dled via domain adaptation techniques and shouldnot be a reason against lexicalization.5.2 Structured FeaturesIn Nissim?s (2006) feature set, there are a couple offeatures that capture NP-internal information, suchas determiner, NP length, and NP type.
However,there is only one feature that captures the syntacticcontext of an NP, grammatical role, which is com-puted based on the parse tree in which the NP re-sides.
This is arguably a very shallow representationof its syntactic context.
We hypothesize that we cantrain more accurate information-status classifiers ifwe have access to a richer representation of syntac-tic context.
This motivates us to employ syntacticparse trees directly as features.Before describing how this can be done, recallthat in a traditional learning setting, the feature setemployed by an off-the-shelf learning algorithm typ-ically consists of flat features (i.e., features whosevalues are discrete- or real-valued, as the ones de-scribed in the previous section).
Advanced machinelearning algorithms such as SVMs, on the otherhand, have enabled the use of structured features(i.e., features whose values are structures such asparse trees), owing to their ability to employ ker-nels to efficiently compute the similarity betweentwo potentially complex structures.Perhaps the main advantage of employing struc-tured features is simplicity.
To understand this ad-vantage, consider learning in a setting where we canonly employ flat features.
If we want to use informa-tion from a parse tree as features in this setting, wewill need to design heuristics to extract the desiredparse-based features from parse trees.
For certaintasks, designing a good set of heuristics can be time-consuming and sometimes difficult.
On the otherhand, SVMs enable a parse tree to be employed di-rectly as a structured feature, obviating the need todesign such heuristics.Given two parse trees (as features), we com-pute their similarity using a convolution tree ker-nel (Collins and Duffy, 2001), which efficiently enu-merates the number of common substructures in thetwo trees via dynamic programming.
Note, however,that while we want to use a parse tree directly as afeature, we do not want to use the entire parse tree asa feature.
Specifically, while using the entire parse1072tree enables a richer representation of the syntacticcontext than using a partial parse tree, the increasedcomplexity of the tree also makes it more difficultfor the SVM learner to make generalizations.To strike a better balance between having a richrepresentation of the context and improving thelearner?s ability to generalize, we extract a substruc-ture from a parse tree and use it as the value of thestructured feature of an instance.
Specifically, givenan instance corresponding to discourse entity e, weextract the substructure from the parse tree contain-ing e as follows.
Let n(e) be the root of the sub-tree that spans all and only the words in e, and letParent(n(e)) be its immediate parent node.
We (1)take the subtree rooted at Parent(n(e)), (2) replaceeach leaf node in this subtree with a node labeledX, (3) replace the subtree rooted at n(e) with a leafnode labeled Y, and (4) use the subtree rooted atParent(n(e)) as the structured feature for the in-stance corresponding to e. Intuitively, the first threesteps aim to provide generalizations by simplifyingthe tree.
For instance, step (1) allows us to focus onusing a small window as the context.
Steps (2) and(3) help generalization by ignoring the words withine and its context.
Note that using two labels, X andY, enables the kernel to distinguish the discourse en-tity under consideration from its context within thissubstructure.
In addition, we simply use a singlenode (Y) to represent the discourse entity, since anyNP-internal information has presumably been cap-tured by the flat features.
We compute these struc-tured features using hand-annotated parse trees.While structured features have been employed fora multitude of tasks in syntax, semantics, and in-formation extraction such as syntactic parsing (e.g.,Collins (2002)), semantic parsing (e.g., Moschitti(2004)), named entity recognition (e.g., Cumby andRoth (2003), and relation extraction (e.g., Zelenkoet al (2003)), the same is not true for discourseprocessing tasks.
We hope that our use of struc-tured features for information-status classificationcan promote their use in discourse processing.5.3 Combining KernelsRecall that the flat features are computed using alinear kernel, while the structured features are com-puted using a tree kernel.
If we want our learner tomake use of more than one of these types of features,we need to employ a composite kernel to combinethem.
Specifically, we define and employ the fol-lowing composite kernel:Kc(F1, F2) = K1(F1, F2) + K2(F1, F2),where F1 and F2 are the full set of features that rep-resent the two entities under consideration, and K1and K2 are the kernels we are combining.
To ensurethat both kernels contribute equally to the compos-ite kernel, we normalize the values they return to therange [0,1].6 EvaluationNext, we evaluate the effectiveness of our featuresin improving information-status classification.6.1 Results and DiscussionResults of four information-status classification sys-tems are shown in Table 3.
Under Original Nis-sim, we have the results copied verbatim from Nis-sim?s (2006) paper.
Baseline is the aforementionedbaseline system, which is trained using Nissim?s fea-ture set.
Baseline+Lexical is the system trained us-ing Nissim?s feature set augmented with lexical fea-tures.
Finally, Baseline+Both is the system trainedusing Nissim?s feature set augmented with both lex-ical and structured features.
For each system, weshow the recall (R), precision (P), and F-measure (F)of each of the three classes: old, new, and med.
Be-fore we describe the results, two points deserve men-tion.
First, as noted earlier, Nissim partitioned thedialogues into training and test folds in a differentway than us.
In particular, Original Nissim and theremaining three systems were not evaluated on thesame set of test instances.
Hence, the Original Nis-sim results are not directly comparable with those ofthe other systems.
We show them here just to pro-vide another point of reference.
Second, the resultsof the remaining three systems were obtained by ag-gregating the results of three binary SVM classifiers,as described earlier.Comparing Baseline and Baseline+Lexical, wesee that augmenting Nissim?s feature set with lexicalfeatures improves the F-measure scores on all threeclasses.
In particular, the F-measure and recall formed rise considerably by 3.0 and 7.8, respectively.This provides indirect empirical support for our ear-lier hypothesis that the med class can benefit from1073Original Nissim Baseline Baseline+Lexical Baseline+BothR P F R P F R P F R P Fold 91.5 94.1 92.8 91.2 85.8 88.5 88.7 91.7 90.2 93.0 95.2 94.1med 87.6 68.1 76.6 84.7 62.7 72.1 92.5 63.2 75.1 89.1 70.9 79.0new 22.3 56.3 32.0 30.2 66.4 41.5 32.1 68.3 43.7 34.4 71.5 46.5Accuracy 79.5 74.1 76.3 82.2Table 3: Per-class performance of four information-status classifiers.the shallow world knowledge that these lexical fea-tures help to ?extract?
from annotated data.Comparing Baseline+Lexical and Baseline+Both,we see that the addition of structured features en-ables a further boost to performance: F-measure in-creases by 2.8?3.9 for the three classes.
These re-sults substantiate our hypothesis that employing aricher representation of syntactic context is benefi-cial to information-status classification.Comparing Baseline and Baseline+Both, we seethat F-measure improves considerably by 5?6.9 forthe three classes.
Overall, these results provide sug-gestive evidence that both types of features are ef-fective at improving an information-status classifierthat employs Nissim?s features.For further comparison, we show the classifica-tion accuracies of the four systems in the last rowof Table 3.
As we can see, adding lexical featuresto the baseline features improves accuracy by 2.2%,and adding structured features further improves ac-curacy by 5.9%.
Our two types of features, whenused in combination with Nissim?s features, improvethe baseline substantially by an accuracy of 8.1%.Note that while our results and Original Nissim?sare not directly comparable, the two systems areconsistent in terms of the relative performance forthe three classes: best for old and worst for new.
Thepoor performance for new is largely a consequenceof its low recall, which can in turn be attributed to itslower representation in the dataset.
Since many newinstances are misclassified, a natural question is: arethese instances misclassified as old or med?
Simi-lar questions can be raised for old and med, despitetheir substantially higher recall values than new.To answer these questions, we need to betterunderstand the kind of errors made by our ap-proach.
Consequently, we show in Table 4 the con-fusion matrix generated from the test set for ourC?
old med newG ?old 3656 257 18med 167 2706 163new 17 850 455Table 4: Confusion matrix for the Baseline+Bothclassifier.
C=Classifier tag; G=Gold tagbest-performing information-status classifier, Base-line+Both.
The rows and the columns correspondto the gold tags and the classifier tags, respectively.As we can see, these numbers seem to suggest the?in-between?
nature of mediated entities: when anold or new entity is misclassified, it is typically mis-classified as med (rows 1 and 3); however, when amed entity is misclassified, it is equally likely to bemisclassified as old and new (row 2).These results are perhaps not surprisingly, sinceintuitively med entities bear some resemblance toboth old and new entities.
For instance, the simi-larity between med and old stems from the fact thatdifferent instances of the same entity (e.g., China)can receive one of these two labels, with the deci-sion dependent on whether the entity was previouslymentioned in the dialogue.
On the other hand, medand new are similar in that it may sometimes be dif-ficult even for a human to determine whether certainentities should be labeled as med or new, since thedecision depends on whether she believes these en-tities are generally known or not.6.2 Relation to Anaphoricity DeterminationAnaphoricity determination refers to the task of de-termining whether an NP is anaphoric or not, wherean NP is considered anaphoric if it is part of a (non-singleton) coreference chain but is not the head ofthe chain (Ng and Cardie, 2002).
In other words, an1074Anaphoricity Baseline+Ana Baseline+Lexical+Ana Baseline+Both+AnaR P F R P F R P F R P Fold 91.4 86.6 88.9 91.3 87.3 89.3 90.8 91.7 91.3 92.8 94.9 93.9med 84.3 63.1 72.2 84.9 64.1 73.1 92.3 64.7 76.1 88.7 71.1 78.9new 30.8 66.4 42.1 31.1 66.9 42.5 32.9 68.7 44.5 34.1 71.7 46.2Accuracy 74.7 75.1 77.6 82.0Table 5: Impact of knowledge of anaphoricity on the information-status classifiers.NP is anaphoric if and only if it has an antecedent.Given this definition, anaphoricity determinationbears resemblance to information-status classifica-tion.
For instance, an old entity is anaphoric, since ithas been introduced earlier in the conversation andtherefore have an antecedent.
Similarly, a new ormed entity is non-anaphoric, since the entity has notbeen previously introduced in the conversation andtherefore cannot have an antecedent.There has been a lot of recent work on anaphoric-ity determination (e.g., Bean and Riloff (1999),Uryupina (2003), Ng (2004), Denis and Baldridge(2007), Versley et al (2008), Ng (2009), Zhou andKong (2009)).
Given the similarity between this taskand information-status classification, a natural ques-tion is: will the anaphoricity features previously de-veloped by coreference researchers be helpful forinformation-status classification?
To answer thisquestion, we (1) assemble a feature set composedof the 26 anaphoricity features previously used byRahman and Ng (2009),3 and then (2) repeat the ex-periments in Table 3, except that we augment thefeature set used in each of these experiments withthe anaphoricity features we assembled in step (1).Results with the anaphoricity features are shownin Table 5.
Under Anaphoricity, we have the resultsobtained using only the 29 anaphoricity features.
Aswe can see, these results are comparable to thoseobtained using the Baseline features.
Comparingeach of Baseline+Ana and Baseline+Lexical+Anawith the corresponding experiments in Table 3, wesee that the addition of anaphoricity features yieldsa mild performance improvement, which is consis-tent over all three classes.
However, comparing thelast column of the two tables, we can see that in the3These 26 features are derived from those employed by Ngand Cardie?s (2002) anaphoricity determination system.
SeeFootnote 2 of Rahman and Ng (2009) for details.presence of the structured features, the anaphoricityfeatures do not contribute positively to overall per-formance.
Hence, in the coreference experiments inthe next section, we will not employ anaphoricityfeatures for information-status classification.7 Application to Coreference ResolutionSince the significance of information-status classi-fication stems in part from the potential benefits itbrings to higher-level NLP applications, we deter-mine whether our information-status classificationsystems can offer benefits to learning-based coref-erence resolution.
Since the 147 information-statusannotated dialogues are also coreference annotated,we use them in our coreference evaluation.
To ourknowledge, our work represents the first attempt toreport coreference results on this dataset.7.1 Coreference ModelsWhile the so-called mention-pair coreference modelhas dominated coreference research for more thana decade since its appearance in the mid-1990s, anumber of new coreference models have been pro-posed in recent years.
To investigate whether thesenewer, presumably more sophisticated, coreferencemodels can better exploit the automatically acquiredinformation-status information, we will evaluate theusefulness of information-status information whenused in combination with two different coreferencemodels, the aforementioned mention-pair model andthe recently-developed cluster-ranking model.7.1.1 Mention-Pair ModelThe mention-pair (MP) model, proposed by Aoneand Bennett (1995) and McCarthy and Lehnert(1995), is a classifier that determines whether twoNPs are co-referring or not.
Each instance i(NPj ,NPk) corresponds to two NPs, NPj and NPk , and isrepresented by 39 features.
Table 1 of Rahman and1075Ng (2009) contains a detailed description of thesefeatures.
Linguistically, they can be divided intofour categories: string-matching, grammatical, se-mantic, and positional.
They can also be categorizedbased on whether they are relational or not.
Specifi-cally, relational features capture the relationship be-tween NPj and NPk , whereas non-relational featurescapture the linguistic property of one of these NPs.We follow Soon et al?s (2001) method for cre-ating training instances.
Specifically, we create (1)a positive instance for each anaphoric NP NPk andits closest antecedent NPj ; and (2) a negative in-stance for NPk paired with each of the interveningNPs, NPj+1, NPj+2, .
.
., NPk?1.
The classificationassociated with a training instance is either positiveor negative, depending on whether the two NPs arecoreferent.
To train the MP model, we use the SVMlearner from SVMlight (Joachims, 1999).4After training, the classifier is used to identify anantecedent for an NP in a test text.
Specifically,each NP, NPk, is compared in turn to each preced-ing NP, NPj , from right to left, and selects NPj as itsantecedent if the pair is classified as coreferent.
Theprocess terminates as soon as an antecedent is foundfor NPk or the beginning of the text is reached.Despite its popularity, the MP model has twomajor weaknesses.
First, since each candidate an-tecedent for an NP to be resolved (henceforth an ac-tive NP) is considered independently of the others,this model only determines how good a candidateantecedent is relative to the active NP, but not howgood a candidate antecedent is relative to other can-didates.
So, it fails to answer the critical question ofwhich candidate antecedent is most probable.
Sec-ond, it has limitations in its expressiveness: the in-formation extracted from the two NPs alone may notbe sufficient for making a coreference decision.7.1.2 Cluster-Ranking ModelThe cluster-ranking (CR) model, proposed byRahman and Ng (2009), addresses the two weak-nesses of the MP model by combining the strengthsof the entity-mention model (e.g., Luo et al (2004),Yang et al (2008)) and the mention-ranking model(e.g., Denis and Baldridge (2008)).
Specifically,the CR model ranks the preceding clusters for an4For this and subsequent uses of the SVM learner in ourexperiments, we set al parameters to their default values.active NP so that the highest-ranked cluster is theone to which the active NP should be linked.
Em-ploying a ranker addresses the first weakness, asa ranker allows all candidates to be compared si-multaneously.
Considering preceding clusters ratherthan antecedents as candidates addresses the secondweakness, as cluster-level features (i.e., features thatare defined over any subset of NPs in a precedingcluster) can be employed.Since the CR model ranks preceding clusters, atraining instance i(cj , NPk) represents a precedingcluster cj and an anaphoric NP NPk.
Each instanceconsists of features that are computed based solelyon NPk as well as cluster-level features, which de-scribe the relationship between cj and NPk .
Mo-tivated in part by Culotta et al (2007), we createcluster-level features from the relational features inour feature set using four predicates: NONE, MOST-FALSE, MOST-TRUE, and ALL.
Specifically, for eachrelational feature X, we first convert X into an equiv-alent set of binary-valued features if it is multi-valued.
Then, for each resulting binary-valued fea-ture Xb, we create four binary-valued cluster-levelfeatures: (1) NONE-Xb is true when Xb is false be-tween NPk and each NP in cj ; (2) MOST-FALSE-Xbis true when Xb is true between NPk and less than half(but at least one) of the NPs in cj ; (3) MOST-TRUE-Xb is true when Xb is true between NPk and at leasthalf (but not all) of the NPs in cj ; and (4) ALL-Xb istrue when Xb is true between NPk and each NP in cj .We train a cluster ranker to jointly learnanaphoricity determination and coreference reso-lution using SVMlight?s ranker-learning algorithm.Specifically, for each NP, NPk, we create a train-ing instance between NPk and each preceding clus-ter cj using the features described above.
Since weare learning a joint model, we need to provide theranker with the option to start a new cluster by creat-ing an additional training instance that contains fea-tures that solely describes NPk.
The rank value ofa training instance i(cj , NPk) created for NPk is therank of cj among the competing clusters.
If NPk isanaphoric, its rank is HIGH if NPk belongs to cj , andLOW otherwise.
If NPk is non-anaphoric, its rank isLOW unless it is the additional training instance de-scribed above, which has rank HIGH.After training, the cluster ranker processes theNPs in a test text in a left-to-right manner.
For each1076active NP, NPk , we create test instances for it by pair-ing it with each of its preceding clusters.
To allowfor the possibility that NPk is non-anaphoric, we cre-ate an additional test instance that contains featuresthat solely describe the active NP (similar to whatwe did in the training step above).
All these test in-stances are then presented to the ranker.
If the addi-tional test instance is assigned the highest rank valueby the ranker, then NPk is classified as non-anaphoricand will not be resolved.
Otherwise, NPk is linked tothe cluster that has the highest rank.7.2 Coreference Experiments7.2.1 Experimental SetupThe training/test split we use in the coreferenceexperiments is the same as that in the information-status experiments.
Specifically, we use the train-ing set to train both the information-status classifierand our coreference models, apply the information-status classifier to each discourse entity in the testset, and have the coreference models resolve alland only those NPs that are labeled as old by theinformation-status classifier.
Our decision to allowthe coreference models to resolve only the old enti-ties is motivated by the fact that med and new entitieshave not been previously introduced in the conversa-tion and therefore do not have antecedents.
The NPsused by the coreference models are the same as thoseaccessible to the information-status classifier.We employ two scoring programs, B3 (Bagga andBaldwin, 1998) and ?3-CEAF (Luo, 2005), to scorethe output of a coreference model.
Given a gold-standard (i.e., key) partition, KP , and a system-generated (i.e., response) partition, RP , B3 com-putes the recall and precision of each NP and av-erages these values at the end.
Specifically, for eachNP, NPj , B3 first computes the number of NPs thatappear in both KPj and RPj , the clusters containingNPj in KP and RP , respectively, and then dividesthis number by |KPj| and |RPj| to obtain the re-call and precision of NPj , respectively.
On the otherhand, CEAF finds the best one-to-one alignmentbetween the key clusters and the response clustersusing the Kuhn-Munkres algorithm (Kuhn, 1955),where the weight of an edge connecting two clustersis equal to the number of NPs that appear in bothclusters.
Precision and recall are equal to the sum ofthe weights of the edges in the alignment divided bythe total number of NPs in the response and the key,respectively.7.2.2 Results and DiscussionAs our baseline, we employ our coreference mod-els to generate NP partitions on the test documentswithout using any knowledge of information status.Results, reported in terms of recall (R), precision(P), and F-measure (F) using B3 and ?3-CEAF, areshown in row 1 of Table 6.5 As we can see, thebaseline achieves B3 F-measures of 69.2 (MP) and74.5 (CR) and CEAF F-measures of 61.6 (MP) and68.5 (CR).
These results suggest that the CR modelis stronger than the MP model, corroborating previ-ous empirical findings (Rahman and Ng, 2009).Next, we examine the impact of learned knowl-edge of information status on the performance of acoreference model.
Since knowledge of informationstatus enables a coreference model to focus on re-solving only the old entities, we hypothesize that theresulting model will have a higher precision than onethat does not employ such knowledge.
An equallyimportant question is: will the F-measure of the re-sulting model improve?
Since we are employingknowledge of information status in a pipeline coref-erence architecture where information-status classi-fication is performed prior to coreference resolution,errors made by the (upstream) information-statusclassifier may propagate to the (downstream) coref-erence system.
Given this observation, we hypoth-esize that the answer to the aforementioned ques-tion depends in part on the accuracy of information-status classification.
In particular, the higher theaccuracy of information-status classification is, themore likely the F-measure of the downstream coref-erence model will improve.
To test this hypothe-sis, we conduct experiments where we employ theknowledge provided by the three information-statusclassifiers which, as discussed earlier, perform atvarying levels of accuracy ?
the first one using onlyNissim?s features, the second one using both lexicaland Nissim?s features, and the last one using Nis-sim?s features in combination with lexical and parse-based features ?
for our coreference models.5Since gold-standard NPs are used in our experiments,CEAF precision is always equal to CEAF recall.
For brevity,we only report F-measure scores for CEAF in the table.1077Mention-Pair Model Cluster-Ranking ModelB3 CEAF B3 CEAFSystem R P F F R P F FNo knowledge of information status 78.6 61.8 69.2 61.6 78.2 71.1 74.5 68.5Nissim features only 73.4 67.3 70.2 62.1 73.6 77.4 75.4 69.7Nissim+Lexical features 71.0 69.5 70.2 61.9 73.7 77.3 75.4 69.9Nissim+Lexical+Parse features 74.1 66.8 70.3 62.3 77.3 74.0 75.6 71.1Perfect information status 76.7 68.1 72.1 66.4 77.1 79.5 78.3 74.2Table 6: B3 and CEAF coreference results.Results of the coreference models employingknowledge provided by the three information-statusclassifiers are shown in rows 2?4 of Table 6.
As ex-pected, B3 precision increases in comparison to thebaseline, regardless of the coreference model and thescoring program.
In addition, employing knowledgeof information status always improves coreferenceperformance: F-measure scores increase by 1.0?1.1% (B3) and 0.3?0.7% (CEAF) for the MP model,and by 0.9?1.1% (B3) and 1.2?2.6% (CEAF) forthe CR model.
These results suggest that the threeinformation-status classifiers have achieved the levelof accuracy needed for the coreference models toimprove.
On the other hand, it is somewhat surpris-ing that the three information-status classifiers haveyielded coreference systems that perform at essen-tially the same level of performance.To understand why better information-status clas-sification results do not necessarily yield bettercoreference performance, we take a closer look atthe results of the coreference resolver employingNissim?s features (henceforth NISSIM) and the re-solver employing our Nissim+Lexical+Parse fea-tures (henceforth FULL-FEATURE).
Among the oldentities that were correctly classified using our fea-tures and incorrectly classified by Nissim?s features,we found that the precision of the FULL-FEATUREsystem suffered (since in many cases the corefer-ence models identified wrong antecedents for theseold entities) whereas the NISSIM system remainedunaffected (since the entities were misclassified andwould not be resolved by the models).
In addition,although many med and new entities were correctlyclassified using our features and incorrectly classi-fied (as old) using Nissim?s features, we found thatin many cases no antecedents were identified forthese misclassified entities and hence the precisionof the NISSIM system was not adversely affected.Finally, we investigate whether our coreferencesystem could be improved if it had access to per-fect knowledge of information status (taken directlyfrom the gold-standard annotations).
This experi-ment will allow us to determine whether the useful-ness of knowledge of information status for coref-erence resolution is limited by the accuracy in com-puting such knowledge.
Results are shown in thelast row of Table 6.
As we can see, using per-fect information-status knowledge yields a corefer-ence system that improves those that employs auto-matically acquired information-status knowledge by1.8?4.1% (MP) and 2.7?3.1% (CR) in F-measure.This indicates that the accuracy in computing suchknowledge does play a role in determining its use-fulness for coreference resolution.8 ConclusionsWe examined the problem of automatically deter-mining the information status of discourse entities inspoken dialogues.
In particular, we augmented Nis-sim?s feature set with two types of features: lexicalfeatures, which capture in a shallow manner worldknowledge implicitly encoded in the annotated data;and syntactic parse trees, which provide a richer rep-resentation of the syntactic context in which a dis-course entity appears than grammatical roles.
Re-sults on 147 Switchboard dialogues demonstratedthe effectiveness of these features: we obtained asignificant improvement of 8.1% in accuracy overa information-status classifier trained on Nissim?sfeature set.
In addition, we evaluated information-status classification in the context of coreferenceresolution, and showed that automatically acquiredknowledge of information status can be profitablyused to improve coreference systems.1078AcknowledgmentsWe thank the three reviewers for their invaluablecomments on an earlier draft of the paper.
This workwas supported in part by NSF Grant IIS-0812261.ReferencesDavid J. Allerton.
1978.
The notion of ?givenness?
andits relations to presupposition and to theme.
Lingua,44:133?168.Chinatsu Aone and Scott William Bennett.
1995.
Eval-uating automated and manual acquisition of anaphoraresolution strategies.
In Proceedings of the 33rd An-nual Meeting of the Association for ComputationalLinguistics, pages 122?129.Amit Bagga and Breck Baldwin.
1998.
Algorithmsfor scoring coreference chains.
In Proceedings of theLinguistic Coreference Workshop at the First Interna-tional Conference on Language Resources and Evalu-ation, pages 563?566.David Bean and Ellen Riloff.
1999.
Corpus-based iden-tification of non-anaphoric noun phrases.
In Proceed-ings of the 37th Annual Meeting of the Association forComputational Linguistics, pages 373?380.Sasha Calhoun, Jean Carletta, Jason Brenier, Neil Mayo,Dan Jurafsky, Mark Steedman, and David Beaver.2010.
The NXT-format Switchboard corpus: A richresource for investigating the syntax, semantics, prag-matics and prosody of dialogue.
Language Resourcesand Evaluation, 44(4):387?419.Michael Collins and Nigel Duffy.
2001.
Convolutionkernels for natural language.
In Advances in NeuralInformation Processing Systems 14, pages 625?632.Michael Collins.
2002.
Ranking algorithms for named-entity extraction: Boosting and the voted perceptron.In Proceedings of the 40th Annual Meeting of the As-sociation for Computational Linguistics, pages 489?496.Aron Culotta, Michael Wick, and Andrew McCallum.2007.
First-order probabilistic models for coreferenceresolution.
In Human Language Technologies 2007:The Conference of the North American Chapter of theAssociation for Computational Linguistics; Proceed-ings of the Main Conference, pages 81?88.Chad Cumby and Dan Roth.
2003.
On kernel methodsfor relational learning.
In Proceedings of the 20th In-ternational Conference on Machine Learning, pages107?114.Pascal Denis and Jason Baldridge.
2007.
Global, jointdetermination of anaphoricity and coreference reso-lution using integer programming.
In Human Lan-guage Technologies 2007: The Conference of theNorth American Chapter of the Association for Com-putational Linguistics; Proceedings of the Main Con-ference, pages 236?243.Pascal Denis and Jason Baldridge.
2008.
Specializedmodels and ranking for coreference resolution.
In Pro-ceedings of the 2008 Conference on Empirical Meth-ods in Natural Language Processing, pages 660?669.Miriam Eckert and Michael Strube.
2001.
Dialogue acts,synchronising units and anaphora resolution.
Journalof Semantics, 17(1):51?89.Eva Hajic?ova?.
1984.
Topic and focus.
In Contributionsto Functional Syntax, Semantics, and Language Com-prehension (LLSEE 16), pages 189?202.
John Ben-jamins, Amsterdam.Michael A. K. Halliday.
1976.
Notes on transitivity andtheme in English.
Journal of Linguistics, 3(2):199?244.Laurence R. Horn.
1986.
Presupposition, theme, andvariations.
In A. Farley et al, editor, Papers from theParasession on Pragmatics and Grammatical Theoryat the 22nd Regional Meeting, pages 168?192.
CLS.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In Bernhard Scholkopf and Alexan-der Smola, editors, Advances in Kernel Methods - Sup-port Vector Learning, pages 44?56.
MIT Press.Harold W. Kuhn.
1955.
The Hungarian method for theassignment problem.
Naval Research Logistics Quar-terly, 2(83).Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, NandaKambhatla, and Salim Roukos.
2004.
A mention-synchronous coreference resolution algorithm basedon the Bell tree.
In Proceedings of the 42nd AnnualMeeting of the Association for Computational Linguis-tics, pages 135?142.Xiaoqiang Luo.
2005.
On coreference resolution perfor-mance metrics.
In Proceedings of Human LanguageTechnology Conference and Conference on EmpiricalMethods in Natural Language Processing, pages 25?32.Joseph McCarthy and Wendy Lehnert.
1995.
Using de-cision trees for coreference resolution.
In Proceedingsof the Fourteenth International Conference on Artifi-cial Intelligence, pages 1050?1055.Alessandro Moschitti.
2004.
A study on convolution ker-nels for shallow statistic parsing.
In Proceedings of the42nd Annual Meeting of the Association for Computa-tional Linguistics, pages 335?342.Vincent Ng and Claire Cardie.
2002.
Identifyinganaphoric and non-anaphoric noun phrases to improvecoreference resolution.
In Proceedings of the 19th In-ternational Conference on Computational Linguistics,pages 730?736.1079Vincent Ng.
2004.
Learning noun phrase anaphoricityto improve conference resolution: Issues in represen-tation and optimization.
In Proceedings of the 42ndAnnual Meeting of the Association for ComputationalLinguistics, pages 151?158.Vincent Ng.
2009.
Graph-cut-based anaphoricity deter-mination for coreference resolution.
In Proceedingsof Human Language Technologies: The 2009 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 575?583.Malvina Nissim, Shipra Dingare, Jean Carletta, and MarkSteedman.
2004.
An annotation scheme for infor-mation status in dialogue.
In Proceedings of the 4thInternational Conference on Language Resources andEvaluation.Malvina Nissim.
2006.
Learning information status ofdiscourse entities.
In Proceedings of the 2006 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 94?102.Ellen Prince.
1981.
Toward a taxonomy of given-newinformation.
In P. Cole, editor, Radical Pragmatics,pages 223?255.
New York, N.Y.: Academic Press.Altaf Rahman and Vincent Ng.
2009.
Supervised mod-els for coreference resolution.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 968?977.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to corefer-ence resolution of noun phrases.
Computational Lin-guistics, 27(4):521?544.Mark Steedman.
2000.
The Syntactic Process.
The MITPress, Cambridge, MA.Michael Strube.
1998.
Never look back: An alternativeto centering.
In Proceedings of the 36th Annual Meet-ing of the Association for Computational Linguisticsand the 17th International Conference on Computa-tional Linguistics, pages 1251?1257.Olga Uryupina.
2003.
High-precision identification ofdiscourse new and unique noun phrases.
In Proceed-ings of the ACL Student Research Workshop, pages 80?86.Enric Vallduv??.
1992.
The Informational Component.Garland, New York.Yannick Versley, Alessandro Moschitti, Massimo Poe-sio, and Xiaofeng Yang.
2008a.
Coreference systemsbased on kernels methods.
In Proceedings of the 22ndInternational Conference on Computational Linguis-tics, pages 961?968.Renata Vieira and Massimo Poesio.
2000.
Anempirically-based system for processing definite de-scriptions.
Computational Linguistics, 26(4):539?593.Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, andSheng Li.
2008.
An entity-mention model for coref-erence resolution with inductive logic programming.In Proceedings of the 46th Annual Meeting of the As-sociation for Computational Linguistics: Human Lan-guage Technologies, pages 843?851.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella.
2003.
Kernel methods for relationextraction.
Journal of Machine Learning Research,3:1083?1106.GuoDong Zhou and Fang Kong.
2009.
Global learn-ing of noun phrase anaphoricity in coreference reso-lution via label propagation.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 978?986.1080
