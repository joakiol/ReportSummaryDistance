Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 158?161,Sydney, July 2006. c?2006 Association for Computational LinguisticsWord Segmentation and Named Entity Recognition for SIGHANBakeoff3Zhang SuxiangCISTR,Beijing University ofPosts andTelecommunicationszsuxiang@163.comQin YingCISTR,Beijing University ofPosts andTelecommunicationsqinyingmail@163.comWen JuanCISTR,Beijing University ofPosts andTelecommunicationsmystery999@163.comWang XiaojieCISTR,Beijing University ofPosts andTelecommunicationsxjwang@bupt.edu.cnAbstractWe have participated in three open tracksof Chinese word segmentation andnamed entity recognition tasks ofSIGHAN Bakeoff3.
We take aprobabilistic feature based MaximumEntropy (ME) model as our basic frameto combine multiple sources ofknowledge.
Our named entity recognizerachieved the highest F measure forMSRA, and word segmenter achieved themedium F measure for MSRA.
We findeffective combining of the externalmulti-knowledge is crucial to improveperformance of word segmentation andnamed entity recognition.1 IntroductionWord Segmentation (WS) and Named EntityRecognition (NER) are two basic tasks forChinese Processing.
The main difficulty isambiguities widely exist in these two tasks.
Oursystem is thus pay special attentions on variousambiguities resolution.
After preprocessing wetake Maximum Entropy (ME) model as theunified frame for WS and NER.
ME is aeffective model which often used to combinemultiple sources of knowledge into variousfeatures.
For finer-grain utilization of features,we use probabilistic features instead of binaryfeatures normally used.
By exploring some oftenused features and some new features, our systemperforms well in this SIGHAN contest.In the rest sections of this paper, we give abrief introduction to our system sequentially.Section 2 describes the preprocessing in thesystem, including rough segmentation andfactoid identification.
Section 3 is on ambiguityresolution of WS.
NER is introduced in Section 4.We give some experimental results in Section 5.Finally we draw some conclusions.2 PreprocessingThe first step in preprocessing is to do a roughsegmentation.
By using both Forward MaximumMatching (FMM) and Backward MaximumMatching (BMM) approaches, we get an initialsegmentation simultaneously detecting some ofsegmentation ambiguities in text.
We use twodifferent wordlists in this step.
One is a basicwordlists with about 60 thousands words.
Wethink this wordlist is relatively steady in Chinese.Another includes some words from specialtraining corpus.We then cope with factoid recognition byusing automata.
Four automata are built toidentify time, date, number and other (liketelephone number and model of product)respectively.
For covering some exceptionalstructures, we use some templates to post-process some outputs from automata.Overlapping and combination ambiguitiesdetected in preprocessing will be treated in nextround of our system.
It is the topic of nextsection.3 Disambiguation3.1 Overlapping ambiguityWe only detect overlapping ambiguity withlength of chain no more than 3 because thesekinds of overlapping account for over 98% of alloccurrences according to (Yan, 2000).
The class-based bigram model trained on tagged corpus ofPeople?s Daily 2000 (about 12 million Chinesecharacters) is applied to resolve the ambiguities.In class-based bigram, all named entities, allpunctuation and factoids is one class separatelyand each word is one class.
For MSRA test we158evaluate the performance of our overlappingdisambiguation with precision of 84.1%.3.2 Combination ambiguityWe use some templates to describe the POSproperties of combination ambiguity and theirsegmentation words.
In our system there are 155most frequent combination words.
Due to thefact that instances of combination ambiguity isdeficient in given training corpus, to enlargetraining examples we convert the People Daily2000 to meet the standard of different guidelinesthen extract examples for training besides thegiven training corpora.
For example, ??
is acombination ambiguity according to theguideline of MSRA whereas it is always one unit?
?in People Daily 2000.
Noticing that whentakes the sense of result, it is always tagged as anoun and a verb when it takes the meaning offructification, we can easily enlarge the trainingexamples of  ?
?.We then use ME model to combinationambiguity resolution.
There are six features usedin the model as below.
(1) Contextual words;(2) Contextual characters;(3) Bigram collocations;(4) If the transfer probability of adjacentwords to the target word exists;(5) If keywords indicate segmentation exists;(6) The most frequent segmentation from priordistribution4 Named entity recognition4.1 Personal name recognitionWe propose a probabilistic feature basedmaximum entropy approach to NER.
Where,probabilistic feature functions are used instead ofbinary feature functions, it is one of the severaldifferences between this model and the most ofthe previous ME based model.
We also exploreseveral new features in our model, whichincludes confidence functions, position offeatures etc.
Like those in some previous works,we use sub-models to model Chinese PersonNames, Foreign Names respectively, but webring some new techniques in these sub-models.In standard ME, feature function is a binaryfunction, for example, if we use CPN denotes theChinese person Name, SN denotes Surname, atypical feature is:)1(),(???
?
?=otherwiseSNxandCPNyyxfi ?????
?But in Chinese, firstly, most of words used assurname are also used as normal words.
Theprobabilities are different for them to be used assurname.
Furthermore, a surname is not alwaysfollowed by a given name, both cases are notbinary.
To model these phenomena, we giveprobability values to features, instead of binaryvalues.For example, a feature function can be setvalue as follows:)2(0andCPNyif0.985),(???
??=otherwisexyxf?Or)3(0xCPNyif0.01805),(???
?
?=otherwiseandyxf?Chinese characters used for translating foreignpersonal name are different from those inChinese personal name.
We built the foreignname model by collecting suffixes, prefixes,frequently-used characters, estimate theirprobabilities used in foreign personal name.These probabilities also used in model asprobability features.We also design a confidence function for acharacter sequence nCCCW ...21=  to help model toestimate the probability of W as a person name.iC  may be a character or a word.
Let Ff1 isprobability of the C1, iMf is the probability of theiC , nEf  is the probability of the Cn.
So theconfidence function is)4(),(121 nEniiMF fffPERSONwK ++= ?
?<=<=This function is included in ME frame as afeature.Candidate person name collection is the firststep of NER.
Since the ambiguity of Chineseword segmentation always exists.
We proposesome patterns for model different kind ofsegmentation ambiguity.
Some labels are used toexpress specific roles of Chinese characters inperson names.We have seven patterns as follows; first twopatterns are non-ambiguity, while the othersmodel some possible ambiguity in Chineseperson name brought by word segmenter.
(1) BCD: the Chinese personal name iscomposed of three Hanzi ((Chinese character).B: Surname of a Chinese personal name.159C: Head character of 2-Hanzi given names.D: Tail character of 2-Hanzi of given names.
(2) BD: the Chinese personal name iscomposed of two Hanzi (Chinese character).
(3) BCH:H: the last given name and its next context arecomposed of a word.
(4) UCD:U: the surname and its previous context arecomposed of a word.
(5) BE:E: the first given name and the last given nameare composed of a word.
(6) UD:U: the surname and the first given name arecomposed of a word.
(7) CD?The Chinese personal name is onlycomposed of two given names.Based on the People?s Daily corpus andmaximum entropy, we achieve models ofChinese personal name and transliteratedpersonal name respectively.Here, How can we know whether a personname is composed of two or three Hanzi, weused another technology to limit boundary.
Wethink out the co-appearing about the last givenname and its next context, now, we have made astatistics about personal name and its nextcontext to decide the length of the Chinesepersonal name.
For example:???????????
?,In this sentence, we collect a candidateChinese person name ????
?, but the lastgiven name ???
is a specific character, it hasdifferent meaning, now, we make a decisionwhether ???
is belong to personal name or not.?????
3)()( NRnumberNRnumber <So, ???
is not included in the personal name,????
is a correct choice.Another problem we have met is to recognizetransliterated personal name, because manytransliterated personal characters has includedthe Chinese surname, however, the condition thatwe can recognize the Chinese personal name isChinese surname, therefore, a section of thetransliterated personal name will often berecognized a Chinese personal name.In our system, we design a dynamitic prioritymethod to check ambiguous character, when weexamine a ambiguous character like ???
or ??
?,we will search different characters which maybebelong to Chinese personal name or transliteratedpersonal name with forward and backwarddirection.
According to the collection result, wewill decide to use Chinese personal model ortransliterated personal model to recognizepersonal name.For example:??/?/???/?/??/??/?/??/?/?/?/?/?/?/?/?/?/?/?/??/??/??/??/?/?/??/?/?/?/?/??/???/?
?The correct candidate personal name is ?????????
and not ????
?.4.2 Location recognitionWe collect 196 keywords such like ??,?,?,?,?,?
?, when the system search these keywordsin a string, it will collect some characters orwords which maybe belong to a location withbackward direction, and the candidate locationcan be inputted into location model to recognize.The approach is similar to the personal namerecognition, the difference is its contextual, thecontextual used for location is2112 ++??
iiiii wwwww , which always can beused as feature during location entity recognition.We trained model based on the People?s Daily.We design some rules to help rectify wrongresult, when a transliterated location name is lackof keyword like ??
?, it maybe recognized as atransliterated personal name.
We collect somespecific words list such as ???,?,??,??
?to correct the wrong personal name.
If thecurrent word is in the list, the following wordsare accepted as candidate location entity.4.3 Organization recognitionOrganization name recognition is very differentfrom other kinds of entities.
An organization isoften composed of several characters or words,and its length is dynamitic.
According tostatistical result about People?s Daily and MSRcorpus, we decided the maximum length of anorganization is 7 in a sentence.We computed the probability of every word orcharacter of an organization, and defined theprobability threshold.According to the different keyword, wedesigned sixteen classifiers; every classifier hasits knowledge base, the different classifier canachieve organization recognition goal.We computed the probability threshold (>0.02)of a candidate organization.Combined the BIO-tagged method and theprobability threshold, the organization can berecognized.1604.4 Combination of Knowledge fromVarious SourcesHuman knowledge is very useful for NER.Knowledge from various sources can beincorporated into ME model, which are shown asfollows.1.
Chinese family name list (including 925items) and given names list (including 2453items):2.
Transliterated character list (including 1398items).3.
Location keyword list (including 607 items):If the word belongs to the list, 2~6 words beforethe salient word are accepted as candidateLocation.4.
Abbreviated location like ?
?/Beijing?, ??/Tianjin?
name list.
Moreover, on Microsoftcorpus, the word ???
of ??????
is alsolabeled as location ???/China?.5.
Organization keyword list (including 875items): If the current word is in organizationkeyword list, 2~6 words before keywords areaccepted as the candidate Organization.6.
A location name dictionary.
Somefrequently used locations are included in thedictionary, like ??
?/United States?
and ????/Singapore?.7.
An organization name dictionary.
Somefrequently used organization names are includedin the dictionary, like ???
?/State Council?and ???
?/United Nations?.8.
Person name list: we collect some personnames which come from the MSR train corpus.Moreover, the famous person name are includedin the list such as ????,???
?.5 Evaluation resultWe evaluated our word segmenter and namedentity recognizer on the SIGHAN MicrosoftResearch Asia (MSRA) corpus in open track.The Table 1 is the official result of wordsegmentation by our system.Corpus OOV-RateOOV-RecallIVRecall-rateFmeasureMSR 0.034 0.804 0.976 0.97UPUC 0.087 0.593 0.957 0.911Table 1 Official SIGHAN evaluation result for wordsegmentation in the open trackTable 2 shows the official result of entityrecognition.Type R P FPerson 95.39% 96.71% 96.04%Location 87.77% 93.06% 90.34%Organization 87.68% 84.20% 85.90%Table2 Official SIGHAN evaluation result for entityrecognition in the open track6 ConclusionsA probabilistic feature based ME model is usedto Chinese word segmentation and named entityrecognition tasks.
Our word segmenter achievedthe medium result in the open word segmentationtrack of MSRA corpus, while entity recognitionachieved the top one performance.AcknowledgementThe research work is supported by ChinaMinistry Of Education funded project (MZ115-022): ?Tools for Chinese and Minority LanguageProcessing?ReferencesA L Berger.
1996.
A Maximum Entropy Approach toNatural Language Processing.
Computational Linguistic,22 (1): 39- 71.Yan Yintang, Zhou XiaoQiang.
2000.12 Study ofSegmentation Strategy on Ambiguous Phrases ofOverlapping Type  Journal of The China Society ForScientific and Technical Information  Vol.
19 , ?6Liang NanYuan.
1987 A Written Chinese Segmentationsystem?
CDWS.
Journal of Chinese Information Processing,Vol.2: 44-52ZHANG Hua-ping and Liu Qun.
2004 AutomaticRecognition of Chinese Personal Name Based on RoleTagging.
CHINESE JOURNAL OF COMPUTERS Vol (27)pp 85-91.Lv YaJuan, ZhaoTie-jun et al 2001.
Leveled unknownChinese Words resolution by dynamic programming.Journal Information Processing, 15(1): 28-33.Borthwick .A 1999.
Maximum Entropy Approach to NamedEntity Recognition.
hD Dissertation.161
