Biomedical Named Entity Recognition UsingConditional Random Fields and Rich Feature SetsBurr SettlesDepartment of Computer SciencesDepartment of Biostatistics and Medical InformaticsUniversity of Wisconsin-MadisonMadison, WI, USAbsettles@cs.wisc.edu1 IntroductionAs the wealth of biomedical knowledge in theform of literature increases, there is a rising needfor effective natural language processing toolsto assist in organizing, curating, and retrievingthis information.
To that end, named entityrecognition (the task of identifying words andphrases in free text that belong to certain classesof interest) is an important first step for manyof these larger information management goals.In recent years, much attention has been fo-cused on the problem of recognizing gene andprotein mentions in biomedical abstracts.
Thispaper presents a framework for simultaneouslyrecognizing occurrences of PROTEIN, DNA, RNA,CELL-LINE, and CELL-TYPE entity classes us-ing Conditional Random Fields with a varietyof traditional and novel features.
I show thatthis approach can achieve an overall F1 mea-sure around 70, which seems to be the currentstate of the art.The system described here was developed aspart of the BioNLP/NLPBA 2004 shared task.Experiments were conducted on a training andevaluation set provided by the task organizers.2 Conditional Random FieldsBiomedical named entity recognition can bethought of as a sequence segmentation prob-lem: each word is a token in a sequence tobe assigned a label (e.g.
PROTEIN, DNA, RNA,CELL-LINE, CELL-TYPE, or OTHER1).
ConditionalRandom Fields (CRFs) are undirected statisti-cal graphical models, a special case of which is alinear chain that corresponds to a conditionallytrained finite-state machine.
Such models arewell suited to sequence analysis, and CRFs in1More accurately, the data is in IOB format.
B-DNAlabels the first word of a DNA mention, I-DNA labels allsubsequent words (likewise for other entities), and O la-bels non-entities.
For simplicity, this paper only refersto the entities, not all the IOB label variants.particular have been shown to be useful in part-of-speech tagging (Lafferty et al, 2001), shallowparsing (Sha and Pereira, 2003), and named en-tity recognition for newswire data (McCallumand Li, 2003).
They have also just recently beenapplied to the more limited task of finding geneand protein mentions (McDonald and Pereira,2004), with promising early results.Let o = ?o1, o2, .
.
.
, on?
be an sequence ofobserved words of length n. Let S be a setof states in a finite state machine, each corre-sponding to a label l ?
L (e.g.
PROTEIN, DNA,etc.).
Let s = ?s1, s2, .
.
.
, sn?
be the sequenceof states in S that correspond to the labels as-signed to words in the input sequence o. Linear-chain CRFs define the conditional probability ofa state sequence given an input sequence to be:P (s|o) =1Zoexp?
?n?i=1m?j=1?jfj(si?1, si, o, i)?
?where Zo is a normalization factor of all statesequences, fj(si?1, si, o, i) is one of m functionsthat describes a feature, and ?j is a learnedweight for each such feature function.
This pa-per considers the case of CRFs that use a first-order Markov independence assumption withbinary feature functions.
For example, a fea-ture may have a value of 0 in most cases, butgiven the text ?the ATPase?
it has the value 1along the transition where si?1 corresponds toa state with the label OTHER, si corresponds to astate with the label PROTEIN, and fj is the fea-ture function Word=ATPase ?
o at positioni in the sequence.
Other feature functions thatcould have the value 1 along this transition areCapitalized, MixedCase, and Suffix=ase.Intuitively, the learned feature weight ?jfor each feature fj should be positive for fea-tures that are correlated with the target label,negative for features that are anti-correlatedwith the label, and near zero for relativelyuninformative features.
These weights are104set to maximize the conditional log likelihoodof labeled sequences in a training set D ={?o, l?
(1), .
.
.
, ?o, l?
(n)}:LL(D) =n?i=1log(P (l(i)|o(i)))?m?j=1?2j2?2.When the training state sequences are fullylabeled and unambiguous, the objective func-tion is convex, thus the model is guaranteedto find the optimal weight settings in terms ofLL(D).
Once these settings are found, the la-beling for an new, unlabeled sequence can bedone using a modified Viterbi algorithm.
CRFsare presented in more complete detail by Laf-ferty et al (2001).These experiments use the MALLET imple-mentation of CRFs (McCallum, 2002), whichuses a quasi-Newton method called L-BFGS tofind these feature weights efficiently.3 Feature SetOne property that makes feature based statisti-cal models like CRFs so attractive is that theyreduce the problem to finding an appropriatefeature set.
This section outlines the two maintypes of features used in these experiments.3.1 Orthographic FeaturesThe simplest and most obvious feature set is thevocabulary from the training data.
Generaliza-tions over how these words appear (e.g.
capital-ization, affixes, etc.)
are also important.
Thepresent model includes training vocabulary, 17orthographic features based on regular expres-sions (e.g.
Alphanumeric, HasDash, Ro-manNumeral) as well as prefixes and suffixesin the character length range [3,5].Words are also assigned a generalized ?wordclass?
similar to Collins (2002), which replacescapital letters with ?A?, lowercase letters with?a?, digits with ?0?, and all other characterswith ?
?.
There is a similar ?brief word class?feature which collapses consecutive identicalcharacters into one.
Thus the words ?IL5?and ?SH3?
would both be given the featuresWC=AA0 and BWC=A0, while ?F-actin?
and?T-cells?
would both be assigned WC=A aaaaaand BWC=A a.To model local context simply, neighboringwords in the window [-1,1] are also added asfeatures.
For instance, the middle token in thesequence ?human UDG promoter?
would havefeatures Word=UDG, Neighbor=human andNeighbor=promoter.3.2 Semantic FeaturesIn addition to orthography, the model could alsobenefit from generalized semantic word groups.If training sequences contain ?PML/RAR al-pha,?
?beta 2-M,?
and ?kappa B-specific DNAbinding protein?
all labeled with PROTEIN, themodel might learn that the words ?alpha,??beta,?
and ?kappa?
are indicative of pro-teins, but cannot capture the fact that theyare all semantically related because they areGreek letters.
Similarly, words with the featureWC=Aaa are often part of protein names, suchas ?Rab,?
?Alu,?
and ?Gag.?
But the modelmay have a difficult time setting the weightsfor this feature when confronted with words like?Phe,?
?Arg,?
and ?Cys,?
which are amino acidabbreviations and not often labeled as part of aprotein name.This sort of semantic domain knowledge canbe provided in the form of lexicons.
I pre-pared a total of 17 such lexicons, which include7 that were entered by hand (Greek letters,amino acids, chemical elements, known viruses,plus abbreviations of all these), and 4 corre-sponding to genes, chromosome locations, pro-teins, and cell lines, drawn from online publicdatabases (Cancer GeneticsWeb,2 BBID,3 Swis-sProt,4 and the Cell Line Database5).
Featurefunctions for the lexicons are set to 1 if theymatch words in the input sequence exactly.
Forlexicon entries that are multi-word, all wordsare required to match in the input sequence.Since no suitable database of terms for theCELL-TYPE class was found online, a lexicon wasconstructed by utilizing Google Sets,6 an onlinetool which takes a few seed examples and lever-ages Google?s web index to return other termsthat appear in similar formatting and contextas the seeds on web pages across the Internet.Several examples from the training data (e.g.?lymphocyte?
and ?neutrophil?)
were used asseeds and new cell types (e.g.
?chondroblast,?which doesn?t even occur in the training data),were returned.
The process was repeated untilthe lexicon grew to roughly 50 entries, thoughit could probably be more complete.With all this information at the model?s dis-posal, it can still be difficult to properly dis-ambiguate between these entities.
For exam-2http://www.cancerindex.org/geneweb/3http://bbid.grc.nia.nih.gov/bbidgene.html4http://us.expasy.org/sprot/5http://www.biotech.ist.unige.it/interlab/cldb.html6http://labs.google.com/sets105ple, the acronym ?EPC?
appears in these staticlexicons both as a protein (?eosinophil cationicprotein?
[sic]) and as a cell line (?epitheliomapapulosum cyprini?).
Furthermore, a singleword like ?transcript?
is sometimes all thatdisambiguates between RNA and DNA mentions(e.g.
?BMLF1 transcript?).
The CRF can learnweights for these individual words, but it mayhelp to build general, dynamic keyword lexi-cons that are associated with each label to assistin disambiguating between similar classes (andperhaps boost performance on low-frequency la-bels, such as RNA and CELL-LINE, for whichtraining data are sparse).These keyword lexicons are generated auto-matically as follows.
All of the labeled terms areextracted from the training set and separatedinto five lists (one for each entity class).
Stopwords, Greek letters, and digits are filtered, andremaining words are tallied for raw frequencycounts under each entity class label.
These fre-quencies are then subjected to a ?2 test, wherethe null hypothesis is that a word?s frequency isthe same for a given entity as it is for any otherentity of interest (i.e.
PROTEIN vs. DNA + RNA+ CELL-LINE + CELL-TYPE, such that there isonly one degree of freedom).
All words for whichthe null hypothesis is rejected with a p-value< 0.005 are added to the keyword lexicon forits majority class.
Some example keywords arelisted in table 1.Keyword ?2 value Lexiconprotein 1121.5 PROTEINgene 984.3 DNAline 618.1 CELL-LINEpromoter 613.4 DNAfactor 563.2 PROTEINsite 399.8 DNAreceptor 338.7 PROTEINcomplex 312.8 PROTEINmRNA 292.2 RNAsequence 196.5 DNAperipheral 57.8 CELL-TYPElineage 56.1 CELL-TYPEjurkat 45.2 CELL-LINEculture 41.3 CELL-LINEtranscript 40.9 RNAclone 38.1 CELL-LINEmononuclear 30.2 CELL-TYPEmessenger 12.3 RNATable 1: A sample of high-ranking semantic key-words and the lexicons to which they belong.Orthographic Features OnlyEntity R P F1 L-F1 R-F1PROTEIN 76.3 68.4 72.1 77.4 79.2DNA 62.4 68.2 65.2 68.5 73.8RNA 61.9 62.9 62.4 64.9 75.2CELL-LINE 53.8 54.0 53.9 58.5 65.1CELL-TYPE 63.6 78.5 70.3 72.6 80.4Overall 70.3 69.3 69.8 74.2 77.9Complete Feature SetEntity R P F1 L-F1 R-F1PROTEIN 76.1 68.2 72.0 77.3 79.2DNA 62.1 67.9 64.9 67.7 74.1RNA 65.3 64.2 64.7 66.4 73.9CELL-LINE 57.4 54.1 55.7 59.2 64.2CELL-TYPE 61.7 78.4 69.1 71.3 79.7Overall 70.0 69.0 69.5 73.7 77.7Table 2: Detailed performance of the two fea-tures sets.
Relaxed F1-scores using left- andright-boundary matching are also reported.4 Results and DiscussionTwo experiments were completed in the timeallotted: one CRF model using only the ortho-graphic features described in section 3.1, and asecond system using all the semantic lexiconsfrom 3.2 as well.
Detailed results are presentedin table 2.
The orthographic model achieves anoverall F1 measure of 69.8 on the evaluation set(88.9 on the training set), converging after 230training iterations and approximately 18 hoursof computation.
The complete model, however,only reached an overall F1 of 69.5 on the evalu-ation set (86.7 on the training set), convergingafter 152 iterations in approximately 9 hours.The deleterious effect of the semantic lexi-cons is surprising and puzzling.7 However, eventhough semantic lexicons slightly decrease over-all performance, it is worthwhile to note thatadding lexicons actually improves both recalland precision for the RNA and CELL-LINE en-tities.
These happen to be the two lowest fre-quency class labels in the data, together com-prising less than 10% of the mentions in eitherthe training or evaluation set.
Error analysisshows that several of the orthographic model?sfalse negatives for these entities are of the form?messenger accumulation?
(RNA) or ?nonadher-ent culture?
(CELL-LINE).
It may be that key-word lexicons contributed to the model identify-ing these low frequency terms more accurately.7Note, however, that these figures are on a singletraining/evaluation split without cross-validation, so dif-ferences are likely not statistically significant.106Also of note is that, in both experiments, theCRF framework achieves somewhat comparableperformance across all entities.
In a previousattempt to use a Hidden Markov Model to si-multaneously recognize multiple biomedical en-tities (Collier et al, 2000), HMM performancefor a particular entity seemed more or less pro-portional to its frequency in the data.
The ad-vantage of the CRF here may be due to thefact that HMMs are generative models trainedto learn the joint probability P (o, l) ?
wheredata for l may be sparse ?
and use Bayes ruleto predict the best label.
CRFs are discrimina-tive models trained to maximize P (l|o) directly.5 Conclusions and Future WorkIn short, I have presented in detail a frame-work for recognizing multiple entity classesin biomedical abstracts with Conditional Ran-dom Fields.
I have shown that a CRF-basedmodel with only simple orthographic featurescan achieve performance near the current stateof the art, while using semantic lexicons (aspresented here) do not positively affect perfor-mance.8While the system presented here showspromise, there is still much to be explored.Richer syntactic information such as shallowparsing may be useful.
The method introducedin section 3.2 to generate semantic keywords canalso be adapted to generate features for entity-specific morphology (e.g.
affixes) and context,both linearly (e.g.
neighboring words) and hi-erarchically (e.g.
from a parse).Most interesting, though, might be to inves-tigate why the lexicons do not generally help.One explanation is simply an issue of tokeniza-tion.
While one abstract refers to ?IL12,?
oth-ers may write ?IL-12?
or ?IL 12.?
Similarly,the generalization of entities to groups (e.g.
?xantibody?
vs. ?x antibodies?)
can cause prob-lems for these rigid lexicons that require exactmatching.
Enumerating all such variants for ev-ery entry in a lexicon is absurd.
Perhaps relax-ing the matching criteria and standardizing to-kenization for both the input and lexicons willimprove their utility.8More recent work (not submitted for evaluation) in-dicates that lexicons are indeed useful, but mainly whentraining data are limited.
I have also found that usingorthographic features with part-of-speech tags and onlythe RNA and CELL-LINE (rare class) lexicons can boostoverall F1 to 70.3 on the evaluation data, with particu-lar improvements for the RNA and CELL-LINE entities.AcknowledgementsI would like to thank my advisor Mark Craven forhis advice and guidance, as well as Andrew McCal-lum and Aron Culotta for answering my questionsabout the MALLET system.
This work is supportedby NLM training grant 5T15LM007359-02 and NIHgrant R01 LM07050-01.ReferencesNigel Collier, Chikashi Nobata, and Jun ichi Tsu-jii.
2000.
Extracting the names of genes and geneproducts with a hidden markov model.
In Pro-ceedings of the International Conference on Com-putational Linguistics, pages 201?207.
Saarbru-ucken, Germany.Michael Collins.
2002.
Ranking algorithms fornamed-entity extraction: Boosting and the votedperceptron.
In Proceedings of the Associationfor Computational Linguistics Conference, pages489?496.
Philadelphia, USA.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of the InternationalConference on Machine Learning.
Williamstown,MA, USA.Andrew McCallum and Wei Li.
2003.
Early resultsfor named entity recognition with conditional ran-dom fields, feature induction and web-enhancedlexicons.
In Proceedings of the Conference on Nat-ural Language Learning, pages 188?191.
Edmon-ton, Canada.Andrew McCallum.
2002.
Mallet: Amachine learning for language toolkit.http://mallet.cs.umass.edu.Ryan McDonald and Fernando Pereira.
2004.
Iden-tifying gene and protein mentions in text us-ing conditional random fields.
In Proceedings ofBioCreative: Critical Assessment for InformationExtraction in Biology.
Grenada, Spain.Fei Sha and Fernando Pereira.
2003.
Shallow pars-ing with conditional random fields.
In Proceedingsof the Human Language Technology and NorthAmerican Association for Computational Linguis-tics Conference.
Edmonton, Canada.107
