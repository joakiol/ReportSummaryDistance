What?s in a translation rule?Michel GalleyDept.
of Computer ScienceColumbia UniversityNew York, NY 10027galley@cs.columbia.eduMark HopkinsDept.
of Computer ScienceUniversity of CaliforniaLos Angeles, CA 90024mhopkins@cs.ucla.eduKevin Knight and Daniel MarcuInformation Sciences InstituteUniversity of Southern CaliforniaMarina Del Rey, CA 90292{knight,marcu}@isi.eduAbstractWe propose a theory that gives formal seman-tics to word-level alignments defined over par-allel corpora.
We use our theory to introduce alinear algorithm that can be used to derive fromword-aligned, parallel corpora the minimal setof syntactically motivated transformation rulesthat explain human translation data.1 IntroductionIn a very interesting study of syntax in statistical machinetranslation, Fox (2002) looks at how well proposed trans-lation models fit actual translation data.
One such modelembodies a restricted, linguistically-motivated notion ofword re-ordering.
Given an English parse tree, childrenat any node may be reordered prior to translation.
Nodesare processed independently.
Previous to Fox (2002), ithad been observed that this model would prohibit certainre-orderings in certain language pairs (such as subject-VP(verb-object) into verb-subject-object), but Fox car-ried out the first careful empirical study, showing thatmany other common translation patterns fall outside thescope of the child-reordering model.
This is true evenfor languages as similar as English and French.
Forexample, English adverbs tend to move outside the lo-cal parent/children in environment.
The English word?not?
translates to the discontiguous pair ?ne ...
pas.
?English parsing errors also cause trouble, as a normallywell-behaved re-ordering environment can be disruptedby wrong phrase attachment.
For other language pairs,the divergence is expected to be greater.In the face of these problems, we may choose amongseveral alternatives.
The first is to abandon syntax instatistical machine translation, on the grounds that syn-tactic models are a poor fit for the data.
On this view,adding syntax yields no improvement over robust phrase-substitution models, and the only question is how muchdoes syntax hurt performance.
Along this line, (Koehnet al, 2003) present convincing evidence that restrictingphrasal translation to syntactic constituents yields poortranslation performance ?
the ability to translate non-constituent phrases (such as ?there are?, ?note that?, and?according to?)
turns out to be critical and pervasive.Another direction is to abandon conventional Englishsyntax and move to more robust grammars that adapt tothe parallel training corpus.
One approach here is that ofWu (1997), in which word-movement is modeled by rota-tions at unlabeled, binary-branching nodes.
At each sen-tence pair, the parse adapts to explain the translation pat-tern.
If the same unambiguous English sentence were toappear twice in the corpus, with different Chinese trans-lations, then it could have different learned parses.A third direction is to maintain English syntax andinvestigate alternate transformation models.
After all,many conventional translation systems are indeed basedon syntactic transformations far more expressive thanwhat has been proposed in syntax-based statistical MT.We take this approach in our paper.
Of course, the broadstatistical MT program is aimed at a wider goal thanthe conventional rule-based program ?
it seeks to under-stand and explain human translation data, and automati-cally learn from it.
For this reason, we think it is impor-tant to learn from the model/data explainability studies ofFox (2002) and to extend her results.
In addition to beingmotivated by rule-based systems, we also see advantagesto English syntax within the statistical framework, suchas marrying syntax-based translation models with syntax-based language models (Charniak et al, 2003) and otherpotential benefits described by Eisner (2003).Our basic idea is to create transformation rules thatcondition on larger fragments of tree structure.
It iscertainly possible to build such rules by hand, and wehave done this to formally explain a number of human-translation examples.
But our main interest is in collect-ing a large set of such rules automatically through corpusanalysis.
The search for these rules is driven exactly bythe problems raised by Fox (2002) ?
cases of crossingand divergence motivate the algorithms to come up withbetter explanations of the data and better rules.
Section2 of this paper describes algorithms for the acquisitionof complex rules for a transformation model.
Section 3gives empirical results on the explanatory power of theacquired rules versus previous models.
Section 4 presentsexamples of learned rules and shows the various types oftransformations (lexical and nonlexical, contiguous andnoncontiguous, simple and complex) that the algorithmsare forced (by the data) to invent.
Section 5 concludes.Due to space constraints, all proofs are omitted.2 Rule AcquisitionSuppose that we have a French sentence, its translationinto English, and a parse tree over the English translation,as shown in Figure 1.
Generally one defines an alignmentas a relation between the words in the French sentenceand the words in the English sentence.
Given such analignment however, what kinds of rules are we entitledto learn from this instance?
How do we know when it isvalid to extract a particular rule, especially in the pres-ence of numerous crossings in the alignment?
In this sec-tion, we give principled answers to these questions, byconstructing a theory that gives formal semantics to wordalignments.2.1 A Theory of Word AlignmentsWe are going to define a generative process throughwhich a string from a source alphabet is mapped to arooted tree whose nodes are labeled from a target alha-bet.
Henceforth we will refer to symbols from our sourcealphabet as source symbols and symbols from our targetalphabet as target symbols.
We define a symbol tree overan alphabet ?
as a rooted, directed tree, the nodes ofwhich are each labeled with a symbol of ?.We want to capture the process by which a symbol treeover the target language is derived from a string of sourcesymbols.
Let us refer to the symbol tree that we want toderive as the target tree.
Any subtree of this tree will becalled a target subtree.
Furthermore, we define a deriva-tion string as an ordered sequence of elements, each ofwhich is either a source symbol or a target subtree.Now we are ready to define the derivation process.Given a derivation string S, a derivation step replacesa substring S?
of S with a target subtree T that has thefollowing properties:1.
Any target subtree in S ?
is a subtree of T .2.
Any target subtree in S but not in S ?
does not sharenodes with T .SNP VPPRP RBAUX VBhe notdoes goil vane pasFigure 1: A French sentence aligned with an Englishparse tree.il ne va pasne va pashePRPNPne pashePRPNPSNP VPPRP RBAUX VBhe notdoes goVBgoil ne va pasne va pasRBnotne heRBnotSNP VPPRP RBAUX VBhe notdoes goil ne va pasSNP VPPRP RBAUX VBhe notdoes goNP VPPRP RBAUX VBhe notdoes goFigure 2: Three alternative derivations from a source sen-tence to a target tree.Moreover, a derivation from a string S of source sym-bols to the target tree T is a sequence of derivation stepsthat produces T from S.Moving away from the abstract for a moment, let usrevisit the example from Figure 1.
Figure 2 shows threederivations of the target tree from the source string ?ilne va pas?, which are all consistent with our defini-tions.
However, it is apparent that one of these deriva-tions seems much more ?wrong?
than the other.
Specif-ically, in the second derivation, ?pas?
is replaced by theEnglish word ?he,?
which makes no sense.
Given the vastspace of possible derivations (according to the definitionabove), how do we distinguish between good ones andbad ones?
Here is where the notion of an alignment be-comes useful.Let S be a string of source symbols and let T be a targettree.
First observe the following facts about derivationsfrom S to T (these follow directly from the definitions):1.
Each element of S is replaced at exactly one step ofthe derivation.SNP VPPRP RBAUX VBhe notdoes goil vane pasSNP VPPRP RBAUX VBhe notdoes goil vane pasSNP VPPRP RBAUX VBhe notdoes goil vane pasFigure 3: The alignments induced by the derivations inFigure 22.
Each node of T is created at exactly one step of thederivation.Thus for each element s of S, we can definereplaced(s, D) to be the step of the derivation D duringwhich s is replaced.
For instance, in the leftmost deriva-tion of Figure 2, ?va?
is replaced by the second step of thederivation, thus replaced(va, D) = 2.
Similarly, for eachnode t of T , we can define created(t, D) to be the stepof derivation D during which t is created.
For instance,in the same derivation, the nodes labeled by ?AUX?
and?VP?
are created during the third step of the derivation,thus created(AUX, D) = 3 and created(VP, D) = 3.Given a string S of source symbols and a target treeT , an alignment A with respect to S and T is a relationbetween the leaves of T and the elements of S. Choosesome derivation D from S to T .
The alignment A in-duced by D is created as follows: an element s of S isaligned with a leaf node t of T iff replaced(s, D) =created(t, D).
In other words, a source word is alignedwith a target word if the target word is created during thesame step in which the source word is replaced.
Figure 3shows the alignments induced by the derivations of Fig-ure 2.Now, say that we have a source string, a target tree,and an alignment A.
A key observation is that the setof ?good?
derivations according to A is precisely the setof derivations that induce alignments A?
such that A isa subalignment of A?.
By subalignment, we mean thatA ?
A?
(recall that alignments are simple mathematicalrelations).
In other words, A is a subalignment of A?
if Aaligns two elements only if A?
also aligns them.We can see this intuitively by examining Figures 2 and3.
Notice that the two derivations that seem ?right?
(thefirst and the third) are superalignments of the alignmentgiven in Figure 1, while the derivation that is clearlywrong is not.
Hence we now have a formal definitionof the derivations that we are interested in.
We say thata derivation is admitted by an alignment A if it induces asuperalignment of A.
The set of derivations from sourcestring S to target tree T that are admitted by alignment Acan be denoted ?A(S, T ).
Given this, we are ready to ob-tain a formal characterization of the set of rules that canne pashePRPNPVBgoNP VPPRP RBAUX VBhe notdoes goDerivationstep: Inducedrule:input: ne VB?pasoutput: VPRBAUX x2notdoesSNP VPPRP RBAUX VBhe notdoes goinput: NP?VPoutput: Sx1 x2Figure 4: Two derivation steps and the rules that are in-duced from them.be inferred from the source string, target tree, and align-ment.2.2 From Derivations to RulesIn essence, a derivation step can be viewed as the applica-tion of a rule.
Thus, compiling the set of derivation stepsused in any derivation of ?A(S, T ) gives us, in a mean-ingful sense, all relevant rules that can be extracted fromthe triple (S, T, A).
In this section, we show in concreteterms how to convert a derivation step into a usable rule.Consider the second-last derivation step of the firstderivation in Figure 2.
In it, we begin with a source sym-bol ?ne?, followed by a target subtree rooted at V B, fol-lowed by another source symbol ?pas.?
These three ele-ments of the derivation string are replaced with a targetsubtree rooted at V P that discards the source symbolsand contains the target subtree rooted at V B.
In general,this replacement process can be captured by the rule de-picted in Figure 4.
The input to the rule are the rootsof the elements of the derivation string that are replaced(where we define the root of a symbol to be simply thesymbol itself), whereas the output of the rule is a symboltree, except that some of the leaves are labeled with vari-ables instead of symbols from the target alhabet.
Thesevariables correspond to elements of the input to the rule.For instance, the leaf labeled x2 means that when this ruleis applied, x2 is replaced by the target subtree rooted atV B (since V B is the second element of the input).
Ob-serve that the second rule induced in Figure 4 is simplya CFG rule expressed in the opposite direction, thus thisrule format can (and should) be viewed as a strict gener-alization of CFG rules.SNP VPPRP RBAUX VBhe notdoes goil vane pas{ il, ne, va,pas}{ ne, va,pas}{ il }{ il }{ il }{ il }{ne,pas} {ne,pas}{ne,pas} {ne,pas}{ va }{ ne } { va }{ va }{pas}Figure 5: An alignment graph.
The nodes are annotatedwith their spans.
Nodes in the frontier set are boldfacedand italicized.Every derivation step can be mapped to a rule in thisway.
Hence given a source string S, a target tree T , andan alignment A, we can define the set ?A(S, T ) as the setof rules in any derivation D ?
?A(S, T ).
We can regardthis as the set of rules that we are entitled to infer fromthe triple (S, T, A).2.3 Inferring Complex RulesNow we have a precise problem statement: learn the set?A(S, T ).
It is not immediately clear how such a set canbe learned from the triple (S, T, A).
Fortunately, we caninfer these rules directly from a structure called an align-ment graph.
In fact, we have already seen numerous ex-amples of alignment graphs.
Graphically, we have beendepicting the triple (S, T, A) as a rooted, directed, acyclicgraph (where direction is top-down in the diagrams).
Werefer to such a graph as an alignment graph.
Formally,the alignment graph corresponding to S, T , and A is justT , augmented with a node for each element of S, andedges from leaf node t ?
T to element s ?
S iff A alignss with t. Although there is a difference between a nodeof the alignment graph and its label, we will not make adistinction, to ease the notational burden.To make the presentation easier to follow, we assumethroughout this section that the alignment graph is con-nected, i.e.
there are no unaligned elements.
All of theresults that follow have generalizations to deal with un-aligned elements, but unaligned elements incur certainprocedural complications that would cloud the exposi-tion.It turns out that it is possible to systematically con-vert certain fragments of the alignment graph into rulesof ?A(S, T ).
We define a fragment of a directed, acyclicgraph G to be a nontrivial (i.e.
not just a single node) sub-graph G?
of G such that if a node n is in G?
then either nis a sink node of G?
(i.e.
it has no children) or all of itschildren are in G?
(and it is connected to all of them).
InVPRBAUX VBnotdoesne pasSNP VPinput: ne VB?pasoutput: VPRBAUX x2notdoesinput: NP?VPoutput: Sx1 x2{ ne } {pas}{ va }{ ne, va,pas}{ il } { ne, va,pas}{ il, ne, va,pas}Figure 6: Two frontier graph fragments and the rules in-duced from them.
Observe that the spans of the sinknodes form a partition of the span of the root.Figure 6, we show two examples of graph fragments ofthe alignment graph of Figure 5.The span of a node n of the alignment graph is thesubset of nodes from S that are reachable from n. Notethat this definition is similar to, but not quite the sameas, the definition of a span given by Fox (2002).
Wesay that a span is contiguous if it contains all elementsof a contiguous substring of S. The closure of span(n)is the shortest contiguous span which is a superset ofspan(n).
For instance, the closure of {s2, s3, s5, s7}would be {s2, s3, s4, s5, s6, s7} The alignment graph inFigure 5 is annotated with the span of each node.Take a look at the graph fragments in Figure 6.
Thesefragments are special: they are examples of frontiergraph fragments.
We first define the frontier set of analignment graph to be the set of nodes n that satisfy thefollowing property: for every node n?
of the alignmentgraph that is connected to n but is neither an ancestor nora descendant of n, span(n?)
?
closure(span(n)) = ?.We then define a frontier graph fragment of an align-ment graph to be a graph fragment such that the root andall sinks are in the frontier set.
Frontier graph fragmentshave the property that the spans of the sinks of the frag-ment are each contiguous and form a partition of the spanof the root, which is also contiguous.
This allows the fol-lowing transformation process:1.
Place the sinks in the order defined by the partition(i.e.
the sink whose span is the first part of the spanof the root goes first, the sink whose span is the sec-ond part of the span of the root goes second, etc.
).This forms the input of the rule.2.
Replace sink nodes of the fragment with a variablecorresponding to their position in the input, thentake the tree part of the fragment (i.e.
project thefragment on T ).
This forms the output of the rule.Figure 6 shows the rules derived from the given graphfragments.
We have the following result.Theorem 1 Rules constructed according to the aboveprocedure are in ?A(S, T ).Rule extraction: Algorithm 1.
Thus we now have asimple method for extracting rules of ?A(S, T ) from thealignment graph: search the space of graph fragments forfrontier graph fragments.Unfortunately, the search space of all fragments of agraph is exponential in the size of the graph, thus thisprocedure can also take a long time to execute.
To ar-rive at a much faster procedure, we take advantage of thefollowing provable facts:1.
The frontier set of an alignment graph can be identi-fied in time linear in the size of the graph.2.
For each node n of the frontier set, there is a uniqueminimal frontier graph fragment rooted at n (ob-serve that for any node n?
not in the frontier set,there is no frontier graph fragment rooted at n?, bydefinition).By minimal, we mean that the frontier graph fragmentis a subgraph of every other frontier graph fragment withthe same root.
Clearly, for an alignment graph with knodes, there are at most k minimal frontier graph frag-ments.
In Figure 7, we show the seven minimal frontiergraph fragments of the alignment graph of Figure 5.
Fur-thermore, all other frontier graph fragments can be cre-ated by composing 2 or more minimal graph fragments,as shown in Figure 8.
Thus, the entire set of frontier graphfragments (and all rules derivable from these fragments)can be computed systematically as follows: compute theset of minimal frontier graph fragments, compute the setof graph fragments resulting from composing 2 minimalfrontier graph fragments, compute the set of graph frag-ments resulting from composing 3 minimal graph frag-ments, etc.
In this way, the rules derived from the min-imal frontier graph fragments can be regarded as a ba-sis for all other rules derivable from frontier graph frag-ments.
Furthermore, we conjecture that the set of rulesderivable from frontier graph fragments is in fact equiva-lent to ?A(S, T ).Thus we have boiled down the problem of extractingcomplex rules to the following simple problem: find theset of minimal frontier graph fragments of a given align-ment graph.The algorithm is a two-step process, as shown below.Rule extraction: Algorithm 21.
Compute the frontier set of the alignment graph.2.
For each node of the frontier set, compute the mini-mal frontier graph fragment rooted at that node.VPRBAUX VBnotdoesne pasSNP VPNPPRPPRPheVBgogovaheilFigure 7: The seven minimal frontier graph fragments ofthe alignment graph in Figure 5VPRBAUX VBnotdoesne pasVBgo+ =VPRBAUX VBnotdoesne pasgoSNP VP+ + =NPPRPPRPheSNP VPPRPheFigure 8: Example compositions of minimal frontiergraph fragments into larger frontier graph fragments.Step 1 can be computed in a single traversal of thealignment graph.
This traversal annotates each node withits span and its complement span.
The complement spanis computed as the union of the complement span of itsparent and the span of all its siblings (siblings are nodesthat share the same parent).
A node n is in the frontierset iff complement span(n) ?
closure(span(n)) = ?.Notice that the complement span merely summarizes thespans of all nodes that are neither ancestors nor descen-dents of n. Since this step requires only a single graphtraversal, it runs in linear time.Step 2 can also be computed straightforwardly.
Foreach node n of the frontier set, do the following: expandn, then as long as there is some sink node n?
of the result-ing graph fragment that is not in the frontier set, expandn?.
Note that after computing the minimal graph frag-ment rooted at each node of the frontier set, every nodeof the alignment graph has been expanded at most once.Thus this step also runs in linear time.For clarity of exposition and lack of space, a couple ofissues have been glossed over.
Briefly:?
As previously stated, we have ignored here the is-sue of unaligned elements, but the procedures canbe easily generalized to accommodate these.
Theresults of the next two sections are all based on im-plementations that handle unaligned elements.?
This theory can be generalized quite cleanly to in-clude derivations for which substrings are replacedby sets of trees, rather than one single tree.
Thiscorresponds to allowing rules that do not require theoutput to be a single, rooted tree.
Such a general-ization gives some nice power to effectively explaincertain linguistic phenomena.
For instance, it allowsus to immediately translate ?va?
as ?does go?
in-stead of delaying the creation of the auxiliary word?does?
until later in the derivation.3 Experiments3.1 Language ChoiceWe evaluated the coverage of our model of transforma-tion rules with two language pairs: English-French andEnglish-Chinese.
These two pairs clearly contrast bythe underlying difficulty to understand and model syntac-tic transformations among pairs: while there is arguablya fair level of cohesion between English and French,English and Chinese are syntactically more distant lan-guages.
We also chose French to compare our study withthat of Fox (2002).
The additional language pair providesa good means of evaluating how our transformation ruleextraction method scales to more problematic languagepairs for which child-reordering models are shown not toexplain the data well.3.2 DataWe performed experiments with two corpora, the FBISEnglish-Chinese Parallel Text and the Hansard French-English corpus.We parsed the English sentences witha state-of-the-art statistical parser (Collins, 1999).
Forthe FBIS corpus (representing eight million Englishwords), we automatically generated word-alignments us-ing GIZA++ (Och and Ney, 2003), which we trained ona much larger data set (150 million words).
Cases otherthan one-to-one sentence mappings were eliminated.
Forthe Hansard corpus, we took the human annotation ofword alignment described in (Och and Ney, 2000).
Thecorpus contains two kinds of alignments: S (sure) forunambiguous cases and P (possible) for unclear cases,e.g.
idiomatic expressions and missing function words(S ?
P ).
In order to be able to make legitimate com-parisons between the two language pairs, we also usedGIZA++ to obtain machine-generated word alignmentsfor Hansard: we trained it with the 500 sentences andadditional data representing 13.7 million English words(taken from the Hansard and European parliament cor-pora).3.3 ResultsFrom a theoretical point of view, we have shown that ourmodel can fully explain the transformation of any parsetree of the source language into a string of the target lan-guage.
The purpose of this section is twofold: to pro-vide quantitative results confirming the full coverage ofour model and to analyze some properties of the trans-formation rules that support these derivations (linguisticanalyses of these rules are presented in the next section).Figure 9 summarizes the coverage of our model withrespect to the Hansard and FBIS corpora.
For the for-mer, we present results for the three alignments: S align-ments, P alignments, and the alignments computed byGIZA++.
Each plotted value represents a percentage ofparse trees in a corpus that can be transformed into a tar-get sentence using transformation rules.
The x-axis rep-resents different restrictions on the size of these rules: ifwe use a model that restrict rules to a single expansionof a non-terminal into a sequence of symbols, we are inthe scope of the child-reordering model of (Yamada andKnight, 2001; Fox, 2002).
We see that its explanatorypower is quite poor, with only 19.4%, 14.3%, 16.5%, and12.1% (for the respective corpora).
Allowing more ex-pansions logically expands the coverage of the model,until the point where it is total: transformation rules nolarger than 17, 18, 23, and 43 (in number of rule expan-sions) respectively provide enough coverage to explainthe data at 100% for each of the four cases.It appears from the plot that the quality of alignmentsplays an important role.
If we compare the three kinds ofalignments available for the Hansard corpus, we see thatmuch more complex transformation rules are extractedfrom noisy GIZA++ alignments.
It also appears that thelanguage difference produces quite contrasting results.Rules acquired for the English-Chinese pair have, on av-erage, many more nodes.
Note that the language differ-ence in terms of syntax might be wider than what the plotseems to indicate, since word alignments computed forthe Hansard corpus are likely to be more errorful than theones for FBIS because the training data used to induce thelatter is more than ten times larger than for the former.In Figure 10, we show the explanatory power of ourmodel at the node level.
At each node of the frontierset, we determine whether it is possible to extract a rulethat doesn?t exceed a given limit k on its size.
The plot-ted values represent the percentage of frontier set inter-nal nodes that satisfy this condition.
These results appearmore promising for the child-reordering model, with cov-erage ranging from 72.3% to 85.1% of the nodes, but weshould keep in mind that many of these nodes are low inthe tree (e.g.
base NPs); extraction of 1-level transfor-mation rules generally present no difficulties when childnodes are pre-terminals, since any crossings can be re-solved by lexicalizing the elements involved in it.
How-0.10.20.30.40.50.60.70.80.911 2 3 4 5 6 7 8 9 10 15 20 25 30 35 40 4550ParsetreecoverageMaximum number of rule expansions"Hansard-S""Hansard-P""Hansard-GIZA""FBIS"Figure 9: Percentage of parse trees covered by the modelgiven different constraints on the maximum size of thetransformation rules.0.70.750.80.850.90.9511 2 3 4 5 6 7 8 9 10 15 20 25 30 35 40 4550NodecoverageMaximum number of rule expansions"Hansard-S""Hansard-P""Hansard-GIZA""FBIS"Figure 10: Same as Figure 9, except that here coverage isevaluated at the node level.ever, higher level syntactic constituents are more prob-lematic for child-reordering models, and the main rea-sons they fail to provide explanation of the parses at thesentence level.Table 1 shows that the extraction of rules can be per-formed quite efficiently.
Our first algorithm, which has anexponential running time, cannot scale to process largecorpora and extract a sufficient number of rules that asyntax-based statistical MT system would require.
Thesecond algorithm, which runs in linear time, is on theother hand barely affected by the size of rules it extracts.k=1 3 5 7 10 20 50I 4.1 10.2 57.9 304.2 - - -II 4.3 5.4 5.9 6.4 7.33 9.6 11.8Table 1: Running time in seconds of the two algorithmson 1000 sentences.
k represent the maximum size of rulesto extract.NPBDTNNRBthatGovernmentsimplytellsADVPVBZNPBDTNNSthepeoplewhatisthemgoodforWPVBZJJINPRPNPBADJPVPSG-ASBAR-AVHPNVPSlegouvernementdittoutsimplement?lesgenscequiestbonpoureuxinput:VBZADVP?NPBSBAR-Soutput:SVPx2x1x3x4Figure 11: Adverb-verb reordering.4 DiscussionsIn this section, we present some syntactic transformationrules that our system learns.
Fox (2002) identified threemajor causes of crossings between English and French:the ?ne ... pas?
construct, modals and adverbs, which achild-reordering model doesn?t account for.
In section 2,we have already explained how we learn syntactic rulesinvolving ?ne ... pas?.
Here we describe the other twoproblematic cases.Figure 11 presents a frequent cause of crossings be-tween English and French: adverbs in French often ap-pear after the verb, which is less common in English.Parsers generally create nested verb phrases when ad-verbs are present, thus no child reordering can allow averb and an adverb to be permuted.
Multi-level reoderingas the rule in the figure can prevent crossings.
Fox?s solu-tion to the problem of crossings is to flatten verb phrases.This is a solution for this sentence pair, since this ac-counts for adverb-verb reorderings, but flattening the treestructure is not a general solution.
Indeed, it can only ap-ply to a very limited number of syntactic categories, forwhich the advantage of having a deep syntactic structureis lost.Figure 12 (dotted lines are P alignments) shows an in-teresting example where flattening the tree structure can-not resolve all crossings in node-reordering models.
Inthese models, a crossing remains between MD and AUXno matter how VPs are flattened.
Our transformation rulemodel creates a lexicalized rule as shown in the figure,where the transformation of ?will be?
into ?sera?
is theonly way to resolve the crossing.In the Chinese-English domain, the rules extracted byour algorithm often have the attractive quality that theyare the kind of common-sense constructions that are usedin Chinese language textbooks to teach students.
For in-stance, there are several that illustrate the complex re-orderings that occur around the Chinese marker word?de.
?NPBDTJJNNthefullreportwillMDAUXVBbecominginbeforethefallRBINDTNNNPBPPVP-AADVPVPSlerapportcompletserad?pos?deicileautomneprochaininput:seraVP-Aoutput:VPVP-Awill/MDbe/AUXVP-Ax2Figure 12: Crossing due to a modal.5 ConclusionThe fundamental assumption underlying much recentwork in statistical machine translation (Yamada andKnight, 2001; Eisner, 2003; Gildea, 2003) is that lo-cal transformations (primarily child-node re-orderings)of one-level parent-children substructures are an adequatemodel for parallel corpora.
Our empirical results suggestthat this may be too strong of an assumption.
To explainthe data in two parallel corpora, one English-French, andone English-Chinese, we are often forced to learn rulesinvolving much larger tree fragments.
The theory, algo-rithms, and transformation rules we learn automaticallyfrom data have several interesting aspects.1.
Our rules provide a good, realistic indicator of thecomplexities inherent in translation.
We believe thatthese rules can inspire subsequent developments ofgenerative statistical models that are better at ex-plaining parallel data than current ones.2.
Our rules put at the fingertips of linguists a veryrich source of information.
They encode translationtransformations that are both syntactically and lex-ically motivated (some of our rules are purely syn-tactic; others are lexically grounded).
A simple sorton the counts of our rules makes explicit the trans-formations that occur most often.
A comparison ofthe number of rules extracted from parallel corporaspecific to multiple language pairs provide a quanti-tative estimator of the syntactic ?closeness?
betweenvarious language pairs.3.
The theory we proposed in this paper is independentof the method that one uses to compute the word-level alignments in a parallel corpus.4.
The theory and rule-extraction algorithm are alsowell-suited to deal with the errors introduced bythe word-level alignment and parsing programs oneuses.
Our theory makes no a priori assumptionsabout the transformations that one is permitted tolearn.
If a parser, for example, makes a systematicerror, we expect to learn a rule that can neverthe-less be systematically used to produce correct trans-lations.In this paper, we focused on providing a well-foundedmathematical theory and efficient, linear algorithmsfor learning syntactically motivated transformation rulesfrom parallel corpora.
One can easily imagine a rangeof techniques for defining probability distributions overthe rules that we learn.
We suspect that such probabilis-tic rules could be also used in conjunction with statisticaldecoders, to increase the accuracy of statistical machinetranslation systems.AcknowledgementsThis work was supported by DARPA contract N66001-00-1-9814 and MURI grant N00014-00-1-0617.ReferencesE.
Charniak, K. Knight, and K. Yamada.
2003.
Syntax-based language models for machine translation.
InProc.
MT Summit IX.M.
Collins.
1999.
Head-driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, University ofPennsylvania, Philadelphia.J.
Eisner.
2003.
Learning non-isomorphic tree mappingsfor machine translation.
In Proc.
of the 41st Meetingof the Association for Computational Linguistics.H.
Fox.
2002.
Phrasal cohesion and statistical machinetranslation.
In Proc.
of Conference on Empirical Meth-ods in Natural Language Processing (EMNLP).D.
Gildea.
2003.
Loosely tree-based alignment for ma-chine translation.
In Proc.
of the 41th Annual Confer-ence of the Association for Computational Linguistics.P.
Koehn, F. Och, and D. Marcu.
2003.
Statistical phrase-based translation.
In Proceedings of HLT/NAACL.F.
Och and H. Ney.
2000.
Improved statistical alignmentmodels.
Proc.
of the 38th Annual Meeting of the Asso-ciation for Computational Linguistics.F.
Och and H Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?51.D.
Wu.
1997.
Stochastic inversion transduction gram-mars and bilingual parsing of parallel corpora.
Com-putational Linguistics, 23(3):377?404.K.
Yamada and K. Knight.
2001.
A syntax-based statis-tical translation model.
In ACL, pages 523?530.
