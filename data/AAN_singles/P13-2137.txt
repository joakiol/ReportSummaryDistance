Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 784?789,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsBuilding and Evaluating a Distributional Memory for CroatianJan S?najder?
Sebastian Pado??
Z?eljko Agic??
?University of Zagreb, Faculty of Electrical Engineering and ComputingUnska 3, 10000 Zagreb, Croatia?Heidelberg University, Institut fu?r Computerlinguistik69120 Heidelberg, Germany?University of Zagreb, Faculty of Humanities and Social SciencesIvana Luc?ic?a 3, 10000 Zagreb, Croatiajan.snajder@fer.hr pado@cl.uni-heidelberg.de zagic@ffzg.hrAbstractWe report on the first structured dis-tributional semantic model for Croatian,DM.HR.
It is constructed after the modelof the English Distributional Memory (Ba-roni and Lenci, 2010), from a dependency-parsed Croatian web corpus, and coversabout 2M lemmas.
We give details on thelinguistic processing and the design prin-ciples.
An evaluation shows state-of-the-art performance on a semantic similaritytask with particularly good performance onnouns.
The resource is freely available.1 IntroductionMost current work in lexical semantics is basedon the Distributional Hypothesis (Harris, 1954),which posits a correlation between the degree ofwords?
semantic similarity and the similarity ofthe contexts in which they occur.
Using this hy-pothesis, word meaning representations can be ex-tracted from large corpora.
Words are typically rep-resented as vectors whose dimensions correspondto context features.
The vector similarities, whichare interpreted as semantic similarities, are used innumerous applications (Turney and Pantel, 2010).Most vector spaces in current use are either word-based (co-occurrence defined by surface window,context words as dimensions) or syntax-based (co-occurrence defined syntactically, syntactic objectsas dimensions).
Syntax-based models have sev-eral desirable properties.
First, they are model tofine-grained types of semantic similarity such aspredicate-argument plausibility (Erk et al 2010).Second, they are more versatile ?
Baroni and Lenci(2010) have presented a generic framework, theDistributional Memory (DM), which is applicableto a wide range of tasks beyond word similarity.Third, they avoid the ?syntactic assumption?
in-herent in word-based models, namely that contextwords are relevant iff they are in an n-word windowaround the target.
This property is particularly rele-vant for free word order languages with many longdistance dependencies and non-projective structure(Ku?bler et al 2009).
Their obvious problem, ofcourse, is that they require a large parsed corpus.In this paper, we describe the construction ofa Distributional Memory for Croatian (DM.HR),a free word order language.
To do so, we parsehrWaC (Ljubes?ic?
and Erjavec, 2011), a 1.2B-tokenCroatian web corpus.
We evaluate DM.HR on asynonym choice task, where it outperforms thestandard bag-of-word model for nouns and verbs.2 Related WorkVector space semantic models have been appliedto a number of Slavic languages, including Bul-garian (Nakov, 2001a), Czech (Smrz?
and Rychly?,2001), Polish (Piasecki, 2009; Broda et al 2008;Broda and Piasecki, 2008), and Russian (Nakov,2001b; Mitrofanova et al 2007).
Previous workon distributional semantic models for Croatiandealt with similarity prediction (Ljubes?ic?
et al2008; Jankovic?
et al 2011) and synonym detec-tion (Karan et al 2012), however using only word-based and not syntactic-based models.So far the only DM for a language other thanEnglish is the German DM.DE by Pado?
and Utt(2012), who describe the process of buildingDM.DE and the evaluation on a synonym choicetask.
Our work is similar, though each languagehas its own challenges.
Croatian, like other Slaviclanguages, has rich inflectional morphology andfree word order, which lead to errors in linguisticprocessing and affect the quality of the DM.7843 Distributional MemoryDM represents co-occurrence information in a gen-eral, non-task-specific manner, as a tensor, i.e., athree-dimensional matrix, of weighted word-link-word tuples.
Each tuple is mapped onto a numberby scoring function ?
: W ?
L ?W ?
R+, thatreflects the strength of the association.
When a par-ticular task is selected, a vector space for this taskcan be generated from the tensor by matricization.Regarding the examples from Section 1, synonymdiscovery would use a word by link-word space(W ?
LW ), which contains vectors for words wrepresented by pairs ?l, w?
of a link and a contextword.
Analogy discovery would use a word-wordby link space (WW ?
L), which represents wordpairs ?w1, w2?
by vectors over links l.The links can be chosen to model any relationof interest between words.
However, as noted byPado?
and Utt (2012), dependency relations are themost obvious choice.
Baroni and Lenci (2010) in-troduce three dependency-based DM variants: De-pDM, LexDM, and TypeDM.
DepDM uses linksthat correspond to dependency relations, with sub-categorization for subject (subj tr and subj intr)and object (obj and iobj).
Furthermore, all prepo-sitions are lexicalized into links (e.g., ?sun, on,Sunday?).
Finally, the tensor is symmetrized: foreach tuple ?w1, l, w2?, its inverse ?w2, l?1, w1?
isincluded.
The other two variants are more complex:LexDM uses more lexicalized links, encoding, e.g.,lexical material between the words, while TypeDMextends LexDM with a scoring function based onlexical variability.Following the work of Pado?
and Utt (2012), webuild a DepDM variant for DM.HR.
Although Ba-roni and Lenci (2010) show that TypeDM can out-perform the other two variants, DepDM often per-forms at a comparable level, while being muchsimpler to build and more efficient to compute.4 Building DM.HRTo build DM.HR, we need to collect co-occurrencecounts from a corpus.
Since no sufficiently largesuitable corpus exists for Croatian, we first explainhow we preprocessed, tagged, and parsed the data.Corpus and preprocessing.
We adopted hrWaC,the 1.2B-token Croatian web corpus (Ljubes?ic?
andErjavec, 2011), as starting point.
hrWaC was builtwith the aim of obtaining a cleaner-than-usual webcorpus.
To this end, a conservative boilerplate re-moval procedure was used; Ljubes?ic?
and Erjavec(2011) report a precision of 97.9% and a recall of70.7%.
Nonetheless, our inspection revealed that,apart from the unavoidable spelling and grammati-cal errors, hrWaC still contains non-textual content(e.g., code snippets and formatting structure), en-coding errors, and foreign-language content.
Asthis severely affects linguistic processing, we addi-tionally filtered the corpus.First, we removed from hrWaC the contentcrawled from main discussion forum and blog web-sites.
This content is highly ungrammatical andcontains a lot of non-diacriticized text, typical foruser-generated content.
This step alone removedone third of the data.
We processed the remainingcontent with a tokenizer and a sentence segmenterbased on regular expressions, obtaining 66M sen-tences.
Next, we applied a series of heuristic filtersat the document- and sentence-level.
At the doc-ument level, we discard all documents (1) whoselength is below a specified threshold, (2) containno diacritics, (3) contain no words from a list of fre-quent Croatian words, or (4) contain a single wordfrom lists of distinctive foreign-language words(for Serbian).
The last two steps serve to eliminateforeign-language content.
In particular, the laststep serves to filter out the text in Serbian, which atthe sentence-level is difficult to automatically dis-criminate from Croatian.
At the sentence-level, wediscard sentences that are (1) shorter than a speci-fied threshold, (2) contain non-standard symbols,(3) contain non-diacriticized Croatian words, or(4) contain too many foreign words from a list offoreign-language words (for English and Slovene).The last step filters out specifically the sentencesin English and Slovene, as we found that these of-ten occur mixed with text in Croatian.
The finalfiltered version of hrWaC contains 51M sentencesand 1.2B tokens.
The corpus is freely available fordownload, along with a more detailed descriptionof the preprocessing steps.1Tagging, lemmatization, and parsing.
For mor-phosyntactic (MSD) tagging, lemmatization, anddependency parsing of hrWaC, we use freely avail-able tools with models trained on the new SETimesCorpus of Croatian (SETIMES.HR), based on theCroatian part of the SETimes parallel corpus.2 SE-TIMES.HR and the derived tools are prototypes1http://takelab.fer.hr/data2http://www.nljubesic.net/resources/corpora/setimes/785SETIMES.HR WikipediaHunPos (POS only) 97.1 94.1HunPos (full MSD) 87.7 81.5CST lemmatizer 97.7 96.5MSTParser 77.5 68.8Table 1: Tagging, lemmatization, and parsing accu-racythat are about to be released as parts of anotherwork.
Here we give a general description and are-evaluation that we consider relevant for buildingDM.HR.SETIMES.HR consists of 90K tokens and 4Ksentences, manually lemmatized and MSD-taggedaccording to Multext East v4 tagset (Erjavec, 2012),with the help of the Croatian Lemmatization Server(Tadic?, 2005).
It is used also as a basis for a novelformalism for syntactic annotation and dependencyparsing of Croatian (Agic?
and Merkler, 2013).On the basis of previous evaluation for Croa-tian (Agic?
et al 2008; Agic?
et al 2009; Agic?,2012) and availability and licensing considerations,we chose HunPos tagger (Hala?csy et al 2007),CST lemmatizer (Ingason et al 2008), and MST-Parser (McDonald et al 2006) to process hrWaC.We evaluated the tools on 100-sentence test setsfrom SETIMES.HR and Wikipedia; performanceon Wikipedia should be indicative of the perfor-mance on a cross-domain dataset, such as hrWaC.In Table 1 we show lemmatization and tagging ac-curacy, as well as dependency parsing accuracyin terms of labeled attachment score (LAS).
Theresults show that lemmatization, tagging and pars-ing accuracy improves on the state of the art forCroatian.
The SETIMES.HR dependency parsingmodels are publicly available.3Syntactic patterns.
We collect the co-occur-rence counts of tuples using a set of syntactic pat-terns.
The patterns effectively define the link types,and hence the dimensions of the semantic space.Similar to previous work, we use two sorts of links:unlexicalized and lexicalized.For unlexicalized links, we use ten syntactic pat-terns.
These correspond to the main dependency re-lations produced by our parser: Pred for predicates,Atr for attributes, Adv for adverbs, Atv for verbalcomplements, Obj for objects, Prep for preposi-tions, and Pnom for nominal predicates.
We sub-categorized the subject relation into Sub tr (sub-3http://zeljko.agic.me/resources/Link P (%) R (%) F1 (%)UnlexicalizedAdv 57.3 52.7 54.9Atr 85.0 89.3 87.1Atv 75.3 70.9 73.1Obj 71.4 71.7 71.5Pnom 55.7 50.8 53.1Pred 81.8 70.6 75.8Prep 50.0 28.6 36.4Sb tr 67.8 73.8 70.7Sb intr 64.5 64.8 64.7Verb 61.6 73.6 67.1LexicalizedPrepositions 67.2 67.9 67.5Verbs 61.6 73.6 67.1All links 73.7 75.5 74.6Table 2: Tuple extraction performance on SE-TIMES.HRjects of transitive verbs) and Sub intr (subject ofintransitive verbs).
The motivation for this is bettermodeling of verb semantics by capturing diathe-sis alternations.
In particular, for many Croatianverbs reflexivization introduces a meaning shift,e.g., predati (to hand in/out) vs. predati se (tosurrender).
With subject subcategorization, re-flexive and irreflexive readings will have differ-ent tensor representations; e.g., ?student, Subj tr,zadac?a?
(?student, Subj tr, homework?)
vs. ?trupe,Subj intr, napadac??
(?troops, Subj intr, invadors?
).Finally, similar to Pado?
and Utt (2012), we useVerb as an underspecified link between subjectsand objects linked by non-auxiliary verbs.For lexicalized links, we use two more extractionpatterns for prepositions and verbs.
Prepositionsare directly lexicalized as links; e.g., ?mjesto, na,sunce?
(?place, on, sun?).
The same holds for non-auxiliary verbs linking subjects to objects; e.g.,?drz?ava, kupiti, kolic?ina?
(?state, buy, amount?
).Tuple extraction and scoring.
The overall qual-ity of the DM.HR depends on the accuracy of ex-tracted tuples, which is affected by all preprocess-ing steps.
We computed the performance of tu-ple extraction by evaluating a sample of tuplesextracted from a parsed version of SETIMES.HRagainst the tuples extracted from the SETIMES.HRgold annotations (we use the same sample as fortagging and parsing performance evaluation).
Ta-ble 2 shows Precision, Recall, and F1 score.
Over-all, we achieve the best performance on the Atrlinks, followed by Pred links.
The performance isgenerally higher on unlexicalized links than on lex-icalized links (note that performance on unlexical-786Link Word LMI Link Word LMIAtv moc?i 225107 Adv moguc?e 9669Atv z?eljeti 22049 Atv namjeravati 9095Obj stan 19997 Obj karta 8936po cijena 18534 prije godina 8584Pred kada 14408 Adv nedavno 7842Obj dionica 13720 Atv odluc?iti 7578Atv morati 12097 Adv godina 7496Obj ulaznica 11126 Obj zemljis?te 7180Table 3: Top 16 LMI-scored tuples for the verbkupiti (to buy)ized Verb links is identical to overall performanceon lexicalized verb links).
The overall F1 score oftuple extraction is 74.6%.Following DM and DM.DE, we score eachextracted tuple using Local Mutual Information(LMI) (Evert, 2005):LMI(i, j, k) = f(i, j, k) log P (i, j, k)P (i)P (j)P (k)For a tuple (w1, l, w2), LMI scores the associationstrength between word w1 and word w2 via link lby comparing their joint distribution against the dis-tribution under the independence assumption, mul-tiplied with the observed frequency f(w1, l, w2) todiscount infrequent tuples.
The probabilities arecomputed from tuple counts as maximum likeli-hood estimates.
We exclude from the tensor alltuples with a negative LMI score.
Finally, we sym-metrize the tensor by introducing inverse links.Model statistics.
The resulting DM.HR tensorconsists of 2.3M lemmas, 121M links and 165Klink types (including inverse links).
On average,each lemma has 53 links.
This makes DM.HRmore sparse than English DM (796 link types), butless sparse than German DM (220K link types; 22links per lemma).
Table 3 shows an example ofthe extracted tuples for the verb kupiti (to buy).DM.HR tensor is freely available for download.45 Evaluating DM.HRTask.
We present a pilot evaluation DM.HR on astandard task from distributional semantics, namelysynonym choice.
In contrast to tasks like predict-ing word similarity We use the dataset created byKaran et al(2012), with more than 11,000 syn-onym choice questions.
Each question consists ofone target word (nouns, verbs, and adjectives) with4http://takelab.fer.hr/dmhrAccuracy (%) Coverage (%)Model N A V N A VDM.HR 70.0 66.3 63.2 99.9 99.1 100BOW-LSA 67.2 68.9 61.0 100 100 100BOW baseline 59.9 65.7 55.9 99.9 99.7 100Table 4: Results on synonym choice taskfour synonym candidates (one is correct).
The ques-tions were extracted automatically from a machine-readable dictionary of Croatian.
An example itemis tez?ak (farmer): poljoprivrednik (farmer), um-jetnost (art), radijacija (radiation), bod (point).We sampled from the dataset questions for nouns,verbs, and adjectives, with 1000 questions each.5Additionally, we manually corrected some errorsin the dataset, introduced by the automatic extrac-tion procedure.
To make predictions, we computepairwise cosine similarities of the target word vec-tors with the four candidates and predict the can-didate(s) with maximal similarity (note that theremay be ties).Evaluation.
Our evaluation follows the schemedeveloped by Mohammad et al(2007), who defineaccuracy as the average number of correct predic-tions per covered question.
Each correct predictionwith a single most similar candidate receives a fullcredit (A), while ties for maximal similarity arediscounted (B: two-way tie, C: three-way tie, D:four-way tie): A+ 12B+ 13C+ 14D.
We consider aquestion item to be covered if the target and at leastone answer word are modeled.
In our experiments,ties occur when vector similarities are zero for allword pairs (due to vector sparsity).
Note that arandom baseline would perform at 0.25 accuracy.As baseline to compare against the DM.HR, webuild a standard bag-of-word model from the samecorpus.
It uses a ?5-word within-sentence con-text window, and the 10,000 most frequent contextwords (nouns, adjectives, and verbs) as dimensions.We also compare against BOW-LSA, a state-of-the-art synonym detection model from Karan etal.
(2012), which uses 500 latent dimensions andparagraphs as contexts.
We determine the signifi-cance of differences between the models by com-puting 95% confidence intervals with bootstrap re-sampling (Efron and Tibshirani, 1993).Results.
Table 4 shows the results for the threeconsidered models on nouns (N), adjectives (A),5Available at: http://takelab.fer.hr/crosyn787and verbs (V).
The performance of BOW-LSAdiffers slightly from that reported by Karan et al(2012), because we evaluate on a sample of theirdataset.
DM.HR outperforms the baseline BOWmodel for nouns and verbs (differences are sig-nificant at p < 0.05).
Moreover, on these cate-gories DM.HR performs slightly better than BOW-LSA, but the differences are not statistically sig-nificant.
Conversely, on adjectives BOW-LSA per-forms slightly better than DM.HR, but the differ-ence is again not statistically significant.
All mod-els achieve comparable and almost perfect cov-erage on this dataset (BOW-LSA achieves com-plete coverage because of the way how the originaldataset was filtered).Overall, the biggest improvement over the base-line is achieved for nouns.
Nouns occur as headsand dependents of many link types (unlexicalizedand lexicalized), and are thus well represented inthe semantic space.
On the other hand, adjectivesseem to be less well modeled.
Although the major-ity of adjectives occur as heads or dependents ofthe Atr relation, for which extraction accuracy isthe highest (cf.
Table 2), it is likely that a single linktype is not sufficient.
As noted by a reviewer, moreinsight could perhaps be gained by comparing thepredictions of BOW-LSA and DM.HR models.
Thegenerally low performance on verbs suggests thattheir semantic is not fully covered in word- andsyntax-based spaces.6 ConclusionWe have described the construction of DM.HR, asyntax-based distributional memory for Croatianbuilt from a dependency-parsed web corpus.
To thebest of our knowledge, DM.HR is the first freelyavailable distributional memory for a Slavic lan-guage.
We have conducted a preliminary evalua-tion of DM.HR on a synonym choice task, whereDM.HR outperformed the bag-of-word model andperformed comparable to an LSA model.This work provides a starting point for a sys-tematic study of dependency-based distributionalsemantics for Croatian and similar languages.
Ourfirst priority will be to analyze how corpus prepro-cessing and the choice of link types relates to modelperformance on different semantic tasks.
Bettermodeling of adjectives and verbs is also an impor-tant topic for future research.AcknowledgmentsThe first author was supported by the CroatianScience Foundation (project 02.03/162: ?Deriva-tional Semantic Models for Information Retrieval?
).We thank the reviewers for their constructive com-ments.
Special thanks to Hiko Schamoni, Tae-GilNoh, and Mladen Karan for their assistance.ReferencesZ?eljko Agic?
and Danijela Merkler.
2013.
Three syn-tactic formalisms for data-driven dependency pars-ing of Croatian.
Proceedings of TSD 2013, LectureNotes in Artificial Intelligence.Z?eljko Agic?, Marko Tadic?, and Zdravko Dovedan.2008.
Improving part-of-speech tagging accuracyfor Croatian by morphological analysis.
Informat-ica, 32(4):445?451.Z?eljko Agic?, Marko Tadic?, and Zdravko Dovedan.2009.
Evaluating full lemmatization of Croatiantexts.
In Recent Advances in Intelligent InformationSystems, pages 175?184.
EXIT Warsaw.Z?eljko Agic?.
2012.
K-best spanning tree dependencyparsing with verb valency lexicon reranking.
In Pro-ceedings of COLING 2012: Posters, pages 1?12,Bombay, India.Marco Baroni and Alessandro Lenci.
2010.
Dis-tributional memory: A general framework forcorpus-based semantics.
Computational Linguistics,36(4):673?721.Bartosz Broda and Maciej Piasecki.
2008.
Superma-trix: a general tool for lexical semantic knowledgeacquisition.
In Speech and Language Technology,volume 11, pages 239?254.
Polish Phonetics Asso-cation.Bartosz Broda, Magdalena Derwojedowa, Maciej Pi-asecki, and Stanis?aw Szpakowicz.
2008.
Corpus-based semantic relatedness for the construction ofPolish WordNet.
In Proceedings of LREC, Mar-rakech, Morocco.Bradley Efron and Robert J. Tibshirani.
1993.
AnIntroduction to the Bootstrap.
Chapman and Hall,New York.Tomaz?
Erjavec.
2012.
MULTEXT-East: Morphosyn-tactic resources for Central and Eastern Europeanlanguages.
Language Resources and Evaluation,46(1):131?142.Katrin Erk, Sebastian Pado?, and Ulrike Pado?.
2010.A Flexible, Corpus-driven Model of Regular and In-verse Selectional Preferences.
Computational Lin-guistics, 36(4):723?763.788Stefan Evert.
2005.
The statistics of word cooccur-rences.
Ph.D. thesis, PhD Dissertation, StuttgartUniversity.Pe?ter Hala?csy, Andra?s Kornai, and Csaba Oravecz.2007.
HunPos: An open source trigram tagger.
InProceedings of ACL 2007, pages 209?212, Prague,Czech Republic.Zelig S. Harris.
1954.
Distributional structure.
Word,10(23):146?162.Anton Karl Ingason, Sigru?n Helgado?ttir, Hrafn Lofts-son, and Eir?
?kur Ro?gnvaldsson.
2008.
A mixedmethod lemmatization algorithm using a hierarchyof linguistic identities (HOLI).
In Proceedings ofGoTAL, pages 205?216.Vedrana Jankovic?, Jan S?najder, and Bojana DalbeloBas?ic?.
2011.
Random indexing distributional se-mantic models for Croatian language.
In Proceed-ings of Text, Speech and Dialogue, pages 411?418,Plzen?, Czech Republic.Mladen Karan, Jan S?najder, and Bojana Dalbelo Bas?ic?.2012.
Distributional semantics approach to detect-ing synonyms in Croatian language.
In Proceedingsof the Language Technologies Conference, Informa-tion Society, Ljubljana, Slovenia.Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.2009.
Dependency Parsing.
Synthesis Lectureson Human Language Technologies.
Morgan & Clay-pool.Nikola Ljubes?ic?
and Tomaz?
Erjavec.
2011. hrWaCand slWac: Compiling web corpora for Croatian andSlovene.
In Proceedings of Text, Speech and Dia-logue, pages 395?402, Plzen?, Czech Republic.Nikola Ljubes?ic?, Damir Boras, Nikola Bakaric?, and Jas-mina Njavro.
2008.
Comparing measures of seman-tic similarity.
In Proceedings of the ITI 2008 30thInternational Conference of Information TechnologyInterfaces, Cavtat, Croatia.Ryan McDonald, Kevin Lerman, and Fernando Pereira.2006.
Multilingual dependency analysis with atwo-stage discriminative parser.
In Proceedings ofCoNLL-X, pages 216?220, New York, NY.Olga Mitrofanova, Anton Mukhin, Polina Panicheva,and Vyacheslav Savitsky.
2007.
Automatic wordclustering in Russian texts.
In Proceedings of Text,Speech and Dialogue, pages 85?91, Plzen?, CzechRepublic.Saif Mohammad, Iryna Gurevych, Graeme Hirst, andTorsten Zesch.
2007.
Cross-lingual distributionalprofiles of concepts for measuring semantic distance.In Proceedings of EMNLP/CoNLL, pages 571?580,Prague, Czech Republic.Preslav Nakov.
2001a.
Latent semantic analysisfor Bulgarian literature.
In Proceedings of SpringConference of Bulgarian Mathematicians Union,Borovets, Bulgaria.Preslav Nakov.
2001b.
Latent semantic analysis forRussian literature investigation.
In Proceedings ofthe 120 years Bulgarian Naval Academy Confer-ence.Sebastian Pado?
and Jason Utt.
2012.
A distributionalmemory for German.
In Proceedings of the KON-VENS 2012 workshop on lexical-semantic resourcesand applications, pages 462?470, Vienna, Austria.Maciej Piasecki.
2009.
Automated extraction of lexi-cal meanings from corpus: A case study of potential-ities and limitations.
In Representing Semantics inDigital Lexicography.
Innovative Solutions for Lexi-cal Entry Content in Slavic Lexicography, pages 32?43.
Institute of Slavic Studies, Polish Academy ofSciences.Pavel Smrz?
and Pavel Rychly?.
2001.
Finding semanti-cally related words in large corpora.
In Text, Speechand Dialogue, pages 108?115.
Springer.Marko Tadic?.
2005.
The Croatian LemmatizationServer.
Southern Journal of Linguistics, 29(1):206?217.Peter D. Turney and Patrick Pantel.
2010.
Fromfrequency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37:141?188.789
