Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics?System Demonstrations, pages 103?108,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsJigg: A Framework for an Easy Natural Language Processing PipelineHiroshi NojiGraduate School of Information ScienceNara Institute of Science and Technology8916-5 Takayama, Ikoma, Nara, Japannoji@is.naist.jpYusuke MiyaoNational Institute of Informatics2-1-2 Hitotsubashi, Chiyoda-ku,Tokyo, Japanyusuke@nii.ac.jpAbstractWe present Jigg, a Scala (or JVM-based) NLP annotation pipeline frame-work, which is easy to use and is exten-sible.
Jigg supports a very simple in-terface similar to Stanford CoreNLP, themost successful NLP pipeline toolkit, buthas more flexibility to adapt to new typesof annotation.
On this framework, systemdevelopers can easily integrate their down-stream system into a NLP pipeline from araw text by just preparing a wrapper of it.1 IntroductionA common natural language processing systemworks as a component in a pipeline.
For example,a syntactic parser typically requires that an inputsentence is correctly tokenized or assigned part-of-speech (POS) tags.
The syntactic trees given bythe parser may be required in further downstreamtasks such as coreference resolution and semanticrole labelling.
While this pipeline-based approachhas been quite successful due to its modularity, itsuffers from several drawbacks from a viewpointof software use and development:?
For a user, building a pipeline connecting ex-isting tools and aggregating the outputs arepainful, since often each system outputs theresults in a different format;?
For researchers or tool developers of down-stream tasks, supporting the full pipelinefrom an input text in their software is boringand time consuming.For example, two famous dependency parsing sys-tems, MaltParser (Nivre et al, 2006) and MST-Parser (McDonald et al, 2005), both assume thatan input sentence is already tokenized and as-signed POS tags, and encoded in a specific format,such as the CoNLL format.POS$TaggingBerkeley$Parser Stanford$CoreNLPParsingBerkeley$Parser Stanford$CoreNLPNew$TaggerTokeniza:on Stanford$CoreNLPSentence$Spli=ng Stanford$CoreNLPDown$stream$tasksCoreference$Resolu:on,$$Seman:c$Role$Labelling,$etcScala$XML$ObjectInput:$Raw$textOutput:$Annotated$XML$fileFigure 1: In a pipeline, annotations are performedon a Scala XML object.
A pipeline is built bychoosing annotator tools at each step, e.g., the boldor dotted lines in the figure.
Each component isimplemented as a wrapper, which manipulates theXML object.
If we prepare a new wrapper of somecomponent, one can integrate it in a pipeline (e.g.,the POS tagger in the dotted lines).In this paper, we present Jigg, which aims tomake it easy to incorporate an existing or newtool (component) in an NLP pipeline.
Figure 1describes the overview.
Using Jigg, a user caneasily construct a pipeline by choosing a tool ateach step on a command-line interface.
Jigg iswritten in Scala, and can easily be extended withJVM languages including Java.
A new tool canbe incorporated into this framework by writing awrapper of that to follow the common API of Jigg(Scala XML object), which requires typically sev-eral dozes of lines of code.The software design of Jigg is highly inspiredby the success of Stanford CoreNLP (Manning etal., 2014), which is now the most widely used NLPtoolkit supporting pipeline processing from rawtexts.
One characteristic of Stanford CoreNLP is103its simplicity of API, which allows wider users toeasily get linguistic annotations for a text.
Follow-ing this strategy, Jigg is also quite simple to use;all the basic components are included into one jarfile, so a user need not install the external depen-dencies.
The basic usage of Jigg is command-lineinterface, and the behavior can be customized witha Java properties file.
On the other hand, it fo-cuses just on processing of a single document ona single machine, and does not provide the solu-tion to more complex scenarios such as distributedprocessing or visualization, which UIMA and re-lated projects (Ferrucci and Lally, 2004; Kano etal., 2011) may provide.The largest difference between Jigg and Stan-ford CoreNLP is the focused NLP components.Stanford CoreNLP is basically a collection of NLPtools developed by the Stanford NLP group, e.g.,Stanford POS tagger (Toutanova et al, 2003) andStanford parser (Socher et al, 2013).
Jigg, on theother hand, is an integration framework of vari-ous NLP tools developed by various groups.
Thismeans that adding a new component in Jigg iseasier than Stanford CoreNLP.
Also as indicatedin Figure 1, Jigg provides a wrapper to StanfordCoreNLP itself, so a user can enjoy combinationof Stanford CoreNLP and other tools, e.g., Berke-ley parser (Petrov and Klein, 2007) (see Section2).
This difference essentially comes from the un-derlying object annotated on each step, which isCoreMap object in Stanford CoreNLP, and ScalaXML object in Jigg, which gives more flexibilityas we describe later (Section 5).
Before that, inthe following, we first describes the concrete us-age (Section 2), the core software design (Section3), and a way to add a new component (Section 4).The code is open-source under the Apache Li-cense Version 2.0.
Followings are the pointers tothe related websites:?
Github: https://github.com/mynlp/jigg?
Maven: http://mvnrepository.com/artifact/com.github.mynlp/jiggJigg is also available from Maven, so it can eas-ily be incorporated into another JVM project.
SeeREAME on the project Github for this usage.2 Basic UsagesAs an example, let us consider the scenario to runthe Berkeley parser on a raw text.
This parser isstate-of-the-art but it requires that the input is cor-$ cat sample.txtThis is a cat.
That is a dog.$ echo sample.txt | java -cp "*" \jigg.pipeline.Pipeline\-annotators "corenlp[tokenize,ssplit],berkeleyparser"\-berkeleyparser.grFileName ./eng_sm6.gr > sample.xmlFigure 2: A command-line usage to run the Berke-ley parser on sentences tokenized and splitted byStanford CoreNLP.Figure 3: The output of the command in Figure 2(sample.xml).rectly tokenized and splitted on sentences.
Fig-ure 2 shows a concrete command-line to build apipeline, on which tokenization and sentence split-ting are performed using the components in Stan-ford CoreNLP.
This pipeline corresponds to thebold lines in Figure 1. jigg.pipeline.Pipeline isthe path to the main class.
?annotators argu-ment is essential, and specifies which components(tools) one wishes to apply.
In the command-line,corenlp[tokenize, ssplit] is an abbreviation of twocomponents, corenlp[tokenize] (tokenization) andcorenlp[ssplit] (sentence splitting by CoreNLP).1The last argument ?berkeleyparser.grFileNameis necessary and specifies the path to the parsermodel (learned grammar).XML output In the current implementation, theoutput format of annotations is always XML.
Fig-ure 3 shows the output for this example.
Inthis output, parse element specifies a (constituent)parse tree with a collection of spans, each ofwhich consists of a root symbol (e.g., S) and childnodes (ids).
This format is intended to be eas-ily processed with a computer, and differs in sev-eral points from the outputs of Stanford CoreNLP,which we describe more in Section 5.1Precisely, the two commands have different meaningsand the former abbreviated form is recommended.
In the lat-ter separated form, transformation between CoreMap objectand Scala XML is performed at each step (twice), while itoccurs once in the former one after ssplit.104import jigg.pipeline.Pipelineimport scala.xml.Nodeimport java.util.Propertiesobject ScalaExample {def main(args: Array[String]): Unit = {val props = new Properties()props.setProperty("annotators","corenlp[tokenize,ssplit],berkeleyparser")props.setProperty("berkeleyparser.grFileName","eng_sm6.gr")val pipeline = new Pipeline(props)val annotation: Node = pipeline.annotate("This is a cat.
That is a dog")// Find all sentence elements recursively,// and get the first one.val firstSentence = (annotation \\ "sentence")(0)// All tokens on the sentenceval tokens = firstSentence \\ "token"println("POS tags on the first sentence: " +(tokens map (_ \@ "pos") mkString " "))// Output "DT VBZ DT NN .
"}}Figure 4: A programmatic usage from Scala.Properties As in Stanford CoreNLP, these argu-ments can be customized through a Java propertiesfile.
For example, the following properties file cus-tomizes the behavior of corenlp besides the parser:$ cat sample.propertiesannotators: corenlp[tokenize,ssplit],berkeleyparserberkeleyparser.grFileName: ./eng_sm6.grcorenlp.tokenize.whitespace: truecorenlp.ssplit.eolonly: trueThis file can be used as follows:jigg.pipeline.Pipeline -props sample.propertiesEach annotator-specific argument has the formannotator name.key.
In the case of corenlp, allkeys of the arguments prefixed with that are di-rectly transferred to the CoreNLP object, so theall arguments defined in Stanford CoreNLP can beused to customize the behavior.
The setting aboveyields tokenization on white spaces, and sentencesplitting on new lines only (i.e., the input text isassumed to be properly preprocessed beforehand).Programmatic usage Jigg can also be used asa Scala library, which can be called on JVM lan-guages.
Figure 4 shows an example on a Scalacode.
The annotate method of Pipeline objectperforms annotations on the given input, and re-turns the annotated XML object (Node class).
Theexample also shows how we can manipulate theScala XML object, which can be searched withmethods similar to XPath, e.g., \\.
\@ key returnsthe attribute value for the key if exists.
Figure 5shows that Jigg can also be used via a Java code.Another example Jigg is a growing project, andthe supported tools are now increasing.
Histori-Properties props = new Properties();props.setProperty("annotators","corenlp[tokenize,ssplit],berkeleyparser");props.setProperty("berkeleyparser.grFileName","eng_sm6.gr");Pipeline pipeline = new Pipeline(props);Node annotation = pipeline.annotate("This is a cat.
That is a dog");// Though the search methods such as \\ cannot be// used on Java, we provide utilities to support// Java programming.List<Node> sentences = jigg.util.XMLUtil.findAllSub(annotation, "sentence");Node firstSentence = sentences.get(0);List<Node> tokens = jigg.util.XMLUtil.findAllSub(firstSentence, "token");System.out.print("POS tags on the first sentence: ");for (Node token: tokens) {String pos = XMLUtil.find(token, "@pos").toString();System.out.print(pos + " ");}Figure 5: Jigg also supports Java programming.cally, Jigg has been started as a pipeline frame-work focusing on Japanese language processing.Jigg thus supports many Japanese processing toolssuch as MeCab (Kudo et al, 2004), a famous mor-phological analyzer, as well as a Japanese CCGparser based on the Japanese CCGBank (Uematsuet al, 2013).
For English, currently the core toolis Stanford CoreNLP.
Here we present an inter-esting application to integrate Berkeley parser intothe full pipeline of Stanford CoreNLP:-annotators "corenlp[tokenize,ssplit],berkeleyparser,corenlp[lemma,ner,dcoref]"where dcoref is a coreference resolution systemrelying on constituent parse trees (Recasens etal., 2013).
This performs annotation of corefer-ence resolution based on the parse trees given bythe Berkeley parser instead of the Stanford parser.Using Jigg, a user can enjoy these combinationsof existing tools quite intuitively.
Also if a userhas her own (higher-performance) system on thepipeline, one can replace the existing componentwith that in a minimal effort, by writing a wrapperof that tool in JVM languages (see Section 4).3 DesignWe now describe the internal mechanisms of Jigg,which comprise of two steps: the first is a checkfor correctness of the given pipeline, and the sec-ond is annotations on a raw text with the con-structed pipeline.
We describe the second anno-tation step first (Section 3.1), and then discuss thefirst pipeline check phase (Section 3.2).3.1 Annotation on Scala XMLAs shown in Figure 1, each annotator (e.g., the to-kenizer in Stanford CoreNLP) communicates with105the Scala XML object.
Basically, each annotatoronly adds new elements or attributes into the re-ceived XML.2For example, the Berkeley parserreceives an XML, on which each sentence elementis annotated with tokens elements lacking pos at-tribute on each token.
Then, the parser (i.e., thewrapper of the parser) adds the predicted syntactictree and POS tags on each sentence XML (see Fig-ure 3).
Scala XML (Node object) is an immutabledata structure, but it is implemented as an im-mutable tree, so a modification can be performedefficiently (in terms of memory and speed).3.2 Requirement-based Pipeline CheckOn this process, the essential point for the pipelineto correctly work is to guarantee that all the re-quired annotations for an annotator are provided ateach step.
For example, the berkeleyparser anno-tator assumes each sentence element in the XMLhas the following structure:<sentence id="...">sentence text<tokens><token form="..." id="..."/><token form="..." id="..."/>...</tokens></sentence>where form means the surface form of a token.How do we guarantee that the XML given toberkeleyparser satisfies this form?Currently, Jigg manages these dependen-cies between annotators using the concept ofRequirement, which we also borrowed from Stan-ford CoreNLP.
Each annotator has a field calledrequires, which specifies the type of necessary an-notations that must be given before running it.
Inberkeleyparser it is defined as follows:override def requires:Set[Requirement] =Set(Tokenize, Ssplit)where Ssplit is an object (of Requirement type),which guarantees that sentences element (a col-lection of sentence elements) exists on the currentannotation, while Tokenize guarantees that eachsentence element has tokens element (a collec-tion of token elements), and each token has fourattributes: id, form, characterOffsetBegin, andcharacterOffsetEnd.Each annotator also has requirementsSatisfiedfield, which declares which Requirements will besatisfied (annotated).
In the above requirements,2One exception in the current implementation is ssplit incorenlp, which breaks the result of tokenize (one very longtokenized sentence) into several sentences.Ssplit is given by corenlp[ssplit] while Tokenize isgiven by corenlp[tokenize].
In berkeleyparser, it isPOS and Parse; POS guarantees that each tokenelement has pos attribute.
Before running annota-tion, Jigg checks whether the constructed pipelinecorrectly works by checking that all elements inrequires for each annotator are satisfied by (in-cluded in) the requirementsSatisfied elements ofthe previous annotators.
For example, if we runthe pipeline with ?annotators berkeleyparser ar-gument, the program fails with an error messagesuggesting missing Requirements.Note that currently Requirement is somethingjust like a contract on the structure of annotatedXML, and it is the author?s responsibility to im-plement each annotator to output the correct XMLstructure.
Currently the correspondence betweeneach Requirement and the satisfied XML structureis managed with a documentation on the wiki ofthe project Github.
We are seeking a more sophis-ticated (safe) mechanism to guarantee these corre-spondences in a code; one possible solution mightbe to define the skeletal XML structure for eachRequirement, and test in each annotator whetherthe annotated object follows the defined structure.4 Adding New AnnotatorHere we describe how to implement a new annota-tor and integrate it into the Jigg pipeline.
We alsodiscuss a way to distribute a new system in Jigg.Implementing new annotator We focus on im-plementation of Berkeley parser as an example toget intuition into what we should do.
Annotator isthe base trait3of all annotator classes, which de-fines the following basic methods:?
def annotate(annotation : Node) : Node?
def requires : Set[Requirement]?
def requirementsSatisfied : Set[Requirement]We have already seen the roles of requires andrequirementsSatisfied in Section 3.2.
Note thatin many cases including the Berkeley parser, an-notation is performed on each sentence indepen-dently.
For this type of annotation, we providea useful trait SentenceAnnotator, which replacesthe method to be implemented from annotate tonewSentenceAnnotation, which has the same sig-nature as annotate.43Trait is similar to interface in Java.4This trait implements annotate to traverse all sentencesand replace them using newSentenceAnnotation method.106package jigg.pipelineimport ...// By supporting a constructor with signature// (String, Properties), the annotator can be// instantiated dynamically using reflection.class BerkeleyParserAnnotator(override val name: String,override val props: Properties) extends SentenceAnnotator {// Instantiate a parser by reading the gramar file.val parser: CoarseToFineMaxRuleParser = ...override def newSentenceAnnotation(sentence: Node): Node = {val tokens: Node = (sentence \ "tokens").headval tokenSeq: Seq[Node] = tokens \ "token"// (1) Get a list of surface forms.val formSeq: Seq[String] = tokenSeq.map(_ \@ "form")// (2) Parse the sentence by calling the API.val binaryTree: Tree[String] = parser.getBestConstrainedParse(formSeq.asJava, null, null)val tree =TreeAnnotations.unAnnotateTree(binaryTree, true)// (3) Convert the output tree into annotation.val taggedTokens = addPOSToTokens(tree, tokens)val parse = treeToNode(tree, tokenSeq)// (4) Return a new sentence node with updated// child elements.XMLUtil.addOrOverrideChild(sentence, Seq(newTokens, parseNode))}// Return the new tokens element on which each element has// pos attributes.def addPOSToTokens(tree: Tree[String], tokens: Node): Node= { ... }// Convert the Tree object in Berkeley parser into XML.def treeToNode(tree: Tree[String], tokenSeq: Seq[Node]): Node = { ... }override def requires = Set(Tokenize)override def requirementsSatisfied = Set(POS, Parse)}Figure 6: Core parts in BekeleyParserAnnotator.Figure 6 shows an excerpt of essential partsin BerkeleyParserAnnotator.
It creates a parserobject in the constructor, and then in eachnewSentenceAnnotation, it first extracts a se-quence of (yet annotated) tokens (1), gets a treeobject from the parser (2), converts the tree intoScala XML object (3), and returns the updatedsentence XML object (4).
This workflow to en-code to and decode from the API-specific objectsis typical when implementing new annotators.Calling with reflection The class in Fig-ure 6 has a constructor with the signature(String, Properties), which allows us to instanti-ate the class dynamically using reflection.
To dothis, a user has to add a new property prefixedwith customAnnotatorClass (the same as StanfordCoreNLP).
In the case above, the propertycustomAnnotatorClass.berkeleyparser : jigg.pipeline.BerkeleyParserAnother advantage of this trait is that annotations are auto-matically performed in parallel if the code is thread-safe.
Onecan also prohibit this behavior by overriding nThreads vari-able by 1 in the annotator class.makes it possible to load the implemented annota-tor with the name berkeleyparser.Distributing new annotators An ultimate goalof Jigg is that the developers of a new tool ina pipeline distribute their system along with thewrapper (Jigg annotator) when releasing the soft-ware.
If the system is JVM-based, the most stableway to integrate it is releasing the annotator (alongwith the software) into Maven repositories.
Then,a user can build an extended Jigg by adding thedependency to it.
For example, now the annotatorfor the MST parser is implemented, but is not in-cluded in Jigg, as it is a relatively old system.
Oneway to extend Jigg with this tool is to prepare an-other project, on which its build.sbt may containthe following lines:5libraryDependencies ++= Seq("com.github.mynlp" % "jigg" % "VVV","com.github.mynlp" % "jigg-mstparser" % "0.1-SNAPSHOT")Jigg itself focuses more on the central NLP toolsfor wider users, but one can obtain the customizedJigg in this way.Tools beyond JVM So far we have only dealtwith JVM softwares such as Stanford CoreNLP,but Jigg can also wraps the softwares written inother languages such as C++ and python.
In fact,many existing tools for Japanese are implementedin C or C++, and Jigg provides wrappers for thosesoftwares.
One problem of these languages is thatinstallation is sometimes hard due to complex de-pendencies to other libraries.
We thus put a pri-ority on supporting the tool written in JVM lan-guages in particular on Maven first, which can besafely incorporated in general.5 Comparison to Stanford CoreNLPAs we have seen so far, Jigg follows the softwaredesign of Stanford CoreNLP in many respects.
Fi-nally, in this section, we highlight the importantdifferences between two approaches.Annotated objects Conceptually this is themost crucial difference as we mentioned in Sec-tion 1.
In Stanford CoreNLP, each annotator ma-nipulates an object called CoreMap.
A clear ad-vantage of this data structure is that one can takeout a typed data structure, such as a well imple-mented Sentence or Graph object, which is easy5To call a new annotator, a user have to give a class pathto the annotator with the property.
Note that the mappings forthe built-in annotators such as berkeleyparser are preservedin the Jigg package, so they can be used without any settings.107to use.
In Jigg?s XML, on the other hand, one ac-cesses the fields through literals (e.g., \@??pos?
?toget the POS attribute of a token).
This may sug-gests Jigg needs more careful implementation foreach annotator.
However, we note that the prob-lem can be alleviated by adding a simple unit test,which we argue is important as well in other plat-forms.The main advantage of using Scala XML as aprimary object is its flexibility for adapting to newtypes of annotations.
It is just an XML object,so there is no restriction on the allowed structure.This is not the case in Stanford CoreNLP, whereeach element in CoreMap must be a proper datastructure defined in the library, which means thatthe annotation that goes beyond the assumption ofStanford CoreNLP is difficult to support.
Even ifwe define a new data structure in CoreMap, an-other problem occurs when outputting the annota-tion into other formats such as XML.
In StanfordCoreNLP, this output component is hard-coded inthe outputter class, which is difficult to extend.This is the problem that we encountered when weexplored an extension to Stanford CoreNLP forJapanese processing pipeline as our initial attempt.Historically in Japanese NLP, the basic analyzingunit is called bunsetsu, which is a kind of chunk; asyntactic tree is often represented as a dependencytree on bunsetsu.
Jigg is preferable to handle thesenew data structures, which go beyond the assump-tion on typical NLP focusing primarily on English,and we believe this flexibility make Jigg suitablefor an integration framework, which has no restric-tions on the applicable softwares and languages.Output format Another small improvement isthat our XML output format (Figure 3) is (we be-lieve) more machine-friendly.
For example, inStanford CoreNLP, the parse element is just aLisp-style tree like (S (NP (DT This)) ((VBZ is)(NP (DT a) (NN cat))) (.
.
)), which is parsable el-ements in Jigg.
For some attribute names we em-ploy different names, e.g., surface form is calledform in Jigg instead of word in Stanford CoreNLP.We decide these names basically following thenaming convention found in Universal Dependen-cies6, which we expect becomes the standard in fu-ture NLP.
Finally, now we implement each wrap-per so that each id attribute is unique across theXML, which is not the case in Stanford CoreNLP.This makes search of elements more easier.6http://universaldependencies.org/docs/6 ConclusionWe presented Jigg, an open source framework foran easy natural language processing pipeline bothfor system developers and users.
We hope that thisplatform facilitates distribution of a new high qual-ity system on the pipeline to wider users.AcknowledgmentsThis work was supported by CREST, JST.ReferencesDavid A. Ferrucci and Adam Lally.
2004.
Uima: an archi-tectural approach to unstructured information processingin the corporate research environment.
Natural LanguageEngineering, 10(3-4):327?348.Y.
Kano, M. Miwa, K. B. Cohen, L. E. Hunter, S. Ananiadou,and J. Tsujii.
2011.
U-compare: A modular nlp work-flow construction and evaluation system.
IBM Journal ofResearch and Development, 55(3):11:1?11:10, May.Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004.Applying conditional random fields to japanese morpho-logical analysis.
In Dekang Lin and Dekai Wu, editors,EMNLP, pages 230?237, Barcelona, Spain, July.Christopher Manning, Mihai Surdeanu, John Bauer, JennyFinkel, Steven Bethard, and David McClosky.
2014.
Thestanford corenlp natural language processing toolkit.
InACL: System Demonstrations, pages 55?60, Baltimore,Maryland, June.Ryan McDonald, Fernando Pereira, Kiril Ribarov, and JanHajic.
2005.
Non-projective dependency parsing usingspanning tree algorithms.
In HLT-EMNLP, October.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.
Malt-parser: a data-driven parser-generator for dependencyparsing.
In LREC.Slav Petrov and Dan Klein.
2007.
Improved inference forunlexicalized parsing.
In HLT-NAACL, pages 404?411,Rochester, New York, April.Marta Recasens, Marie-Catherine de Marneffe, and Christo-pher Potts.
2013.
The life and death of discourse entities:Identifying singleton mentions.
In NAACL: HLT, pages627?633, Atlanta, Georgia, June.Richard Socher, John Bauer, Christopher D. Manning, andNg Andrew Y.
2013.
Parsing with compositional vectorgrammars.
In ACL, pages 455?465, Sofia, Bulgaria, Au-gust.Kristina Toutanova, Dan Klein, Christopher D. Manning, andYoram Singer.
2003.
Feature-rich part-of-speech tag-ging with a cyclic dependency network.
In NAACL: HLT,pages 173?180, Morristown, NJ, USA.Sumire Uematsu, Takuya Matsuzaki, Hiroki Hanaoka,Yusuke Miyao, and Hideki Mima.
2013.
Integratingmultiple dependency corpora for inducing wide-coveragejapanese ccg resources.
In ACL, pages 1042?1051, Sofia,Bulgaria, August.108
