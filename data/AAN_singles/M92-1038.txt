University of Massachusetts : Description of the CIRCUS System a sUsed for MUC-4W.
Lehnert, C. Cardie, D. Fisher, J. McCarthy, E. Riloff, & S. SoderlandUniversity of MassachusettsDepartment of Computer ScienceAmherst, MA 0100 3lehnert@cs.umass.eduTHE CIRCUS SENTENCE ANALYZERCIRCUS is a conceptual analyzer that produces semantic case frame representations for input sentences .Although space does not permit us to give a full technical description of CIRCUS, we will attempt to convey som esense of sentence analysis via CIRCUS .
For more details, please consult [2] and [1].CIRCUS uses no syntactic grammar and produces no parse tree as it analyzes a sentence .
Rather, it useslexically-indexed syntactic knowledge to segment incoming text into noun phrases, prepositional phrases, and ver bphrases.
These constituents are stored in global buffers that track the subjects, verbs, direct objects, and prepositiona lphrases of a sentence.
Because we restrict the buffer contents to simple constituents with a highly local sense of th esentence, larger constituents like clauses are not explicitly stored by the syntactic component of CIRCUS .While syntactic buffers are being bound to sentence fragments, a mechanism for handling predictive semanticsis responsible for establishing case role assignments .
Semantic case frames are activated by concept node (CN)definitions, and each CN defmition can be triggered by one or more lexical items .
Associated with each slot in a CNare both hard and soft constraints .
A hard constraint is a predicate that must be satisfied, while a soft constraintdefines apreference rather than an absolute requirement When a CN instantiation meets certain criteria established bythe CN definition, CIRCUS freezes that case frame and passes it along as output from the sentence analyzer.
Asingle sentence can generate an arbitrary number of case frame instantiations depending on the conceptual complexit yof the sentence and the availability of relevant CN definitions in the dictionary.Because CIRCUS is designed to generate case frame representations in response to sentence fragments ,ungrammatical sentences or sentences with highly complicated syntactic structures are often navigated withoutdifficulty.
CIRCUS was designed to maximize robust processing in the face of incomplete knowledge .
It does notrequire complete dictionary coverage with respect to CN defmition or even part-of-speech recognition, so CIRCUSis especially well-suited for text extraction applications from unconstrained text .
The first serious evaluation ofCIRCUS took place with MUC-3, where CIRCUS posted the highest combined scores for recall and precision of al lthe participating sites [3, 4, 5] .MEMORY-BASED CONSOLIDATIO NConsolidation refers to the problem of mapping CN instantiations produced by CIRCUS into even tdescriptions appropriate for target template instantiations .
Since information pertaining to a single event can bedistributed across multiple sentences, problems associated with consolidation are challenging, especially when a textdescribes multiple events.
It is necessary to know when different noun phrases point to the same referent and whenthe topic shifts from one event to another.The UMass,/MUC-3 system used a rule based consolidation module which was largely dominated by rules designed t omerge appropriate structures.
Because the rule base was large (168 rules), it was difficult to pinpoint weak spots i nthe rule base and it became increasingly difficult to make reliable adjustments as needed .
Because of ourdissatisfaction with last year's approach, we decided to design a new consolidation module for MUC-4 .Our new consolidation module is "memory-based " in the sense that it assumes a specific memor yorganization strategy, and all processing is motivated by a small number of memory manipulations .
The basicstructure of memory-based consolidation (MBC) is a simple stack of incident structures, along with two associate d282stacks that track human targets and physical targets .
At the end of each consolidation run, the number of incidentstructures on the incident stack usually corresponds to the number of templates we will instantiate, with eac hincident structure containing all the information needed to fill at least one template .The incident-structure serves as the basic data type inside MBC as well as the data type that is output fro mMBC.
An incident structure is a frame consisting of slots for a date, location, perpetrators, and subevents .
Eachsubevent consists of a specific incident type (murder, bombing, robbery, etc .)
along with victims, physical targets,instruments, and effects .
Although multiple subevents are permitted in an incident-structure to handle combinedevents like a arson/robbery combination, most incident structures contain only one subevent .
When a new incident-structure is input to MBC, it will either merge with an existing incident structure already on the incident stack, or i twill be added to the incident stack as a separate incident .
When target templates are eventually generated from inciden tstructures on the incident stack, each subevent within an incident structure will spawn its own templat einstantiation .In comparing MBC with the rule-based consolidation module in UMass/MUC-3, we find that MBC tends t ogenerate fewer spurious templates without sacrificing significant recall .
However, we have seen test sets where MBCdoes lag behind in recall .
In general, the two modules seem quite comparable in terms of overall performance ,although MBC is easier to understand, maintain, and scale-up .
Most of the merging rules used by rule-basedconsolidation were incorporated into MBC, so it makes sense that the two modules exhibit similar behavior .
Ourdecision to run MBC for MUC-4 was largely motivated by use of the All Templates metric as the official scorin gmetric for MUC-4 .
Because All Templates is maximally sensitive to all types of precision loss, it is generall yadvantageous to minimize spurious templates for this metric .
MBC seemed consistently better at eliminatingspurious templates, so we decided to risk a possible loss of some recall for the sake of maximizing our precision .A SHORT WALK THROUGH TST2-MUC4-004 8In order to illustrate the behavior of UMass/MUC-4 in operation, we will trace the processing of a sampletext that contains two separate bombing incidents .
In general, CIRCUS generates multiple CN instantiations i nresponse to each sentence, while Memory-Based Consolidation (MBC) extracts information from the CNs an dorganizes it within incident structures .
CIRCUS and MBC work in a serial fashion : CIRCUS analyzes the entire tex trust, and then MBC works on the resulting concept node instantiations .
But for the sake of this presentation, wewill examine the effects of CIRCUS and MBC working together on a sentence-by-sentence basis.Because our CN definitions extract information on the basis of phrase fragments, we will underline thos eportions of the input sentences that are important to relevant CN 's .
Any remaining segments of the input sentencesthat are not underlined are effectively ignored during semantic processing by CIRCUS .
We will also show th epreprocessed version of each input sentence, to indicate which items have been recognized by the phrasal lexico n(these will be catenated), and other minor transformations to the original source text .
Abbreviations preceded by "> "represent punctuation marks.
For example, >CO is a comma .The first job of MBC is to partition multiple CNs into event structures which are then restructured int oincident structures.
As a rule, all CNs generated from a single sentence tend to fall into the same partition, so w ewill omit any detailed discussion of this preliminary conversion .
But it is important to note that essential mergin goperations can take place during the creation of initial incident structures.
For example, S l illustrates how an accusedperpetrator is linked to a murder because their associated CNs fall into a single partition:Si: ($ATVADORANPRRSTDENT-RTRCTATFRF,AO CRTSTTANI CONDEMNED THE TRRRORISTKTT.T .TNA OFATTORNRY,0F,NFRAT .
RORERTO CJRCTA ALVARADO AND ACCUSRD THE FARABUNQp MARTINATTONAT .
LTRRRATTONFRONT(FMLN) OF THE CRIME >PE )CIRCUS triggers a murder CN from "KILLING" which picks up a target = "ATTORNEY GENERA LROBERTO GARCIA ALVARADO ."
The subject of the sentence has been recognized as such but does not ente rinto the murder CN .
When CIRCUS encounters the verb "ACCUSED", a clause boundary is recognized.
Thi sallows CIRCUS to reset syntactic buffers and pick up " ACCUSED " as a new verb while retaining the previoussubject buffer .
"ACCUSED" triggers a perpetrator CN with confidence = SUSPECTED_OR_ACCUSED, accuser ="SALVADORAN PRESIDENT-ELECT ALFREDO CRISTIANI", and perpetrator =FARABUNDO_MARTI_NATIONAL LIBERATION_FRONT .
Note that the FMLN is recognized as a terroris t283organization, thereby satisfying a soft constraint in the perpetrator CN.
"ACCUSED" tells us to assume a less thanfactual confidence level within the perpetrator CN, but CIRCUS does not connect the perpetrator CN with any even tdescription.
In particular, no attempt is made by CIRCUS to resolve a referent for "the crime ."
The two resultingCN instantiations look like:TYPE = MURDERVICTIM = WS-GOVT-OFFICIAL,.
.
.
noun group = (ATTORNEY GENERAL ROBERTO GARCIA ALVARADO )TYPE = PERPETRATORCONFIDENCE = SUSPECTED OR_ACCUSED BY AUTHORITIFSACCUSER = WS-GOVT-OFFICIALnoun group = (PRESIDENT-ELECT ALFREDO CRISTIANI )predicates = (SALVADORAN )PERPETRATOR = WS-ORGANIZATION, .
.
.
noun group = (FARABUNDQMARTI NATIONAL LIBERATION FRONT )MBC's preprocessing and partitioning merges these two CNs into a single event structure before any high-levelmemory integration is attempted .
Incident structures are designed to collapse multiple events (subevents) associate dwith a single perpetrator into a single structure .
The incident structure for S l looks like:INCIDENTDATE =NILLOCATION =NILPERPS = (#S(PERPETRATO RID NILORG (FARABUNDO MARTI NATIONALL1BERATION_FRONT )WORD-SENSES (WS-TERRORIST WS-ORGANIZATION)CONFIDENCE (SUSPECTED OR ACCUSED_BY AUTHORITIES )NEW-INFO NILSENTENCE 1))NEW =NILPLURAL =NILDISCOURSE-MODE = NILSUBEVENT: NI LTARGETS: NILEFFECTS: NILINSTRUMENT: NILVICTIMS : (#s(VICTI MID (ROBERTO GARCIA ALVARADO )TITLE (ATTORNEY GENERAL)NATIONALITY NILNUM 1TYPE (WS-GOVT-OFFICIAL WS-LEGAL-OR-JUDICIALWS-PROPER-NAME)EFFECTS (DEATH)SENTENCE 1))Because MBC has no incident structures on its incident stack, this new incident structure is added to the stack ,and the victim description is added to the victim stack.S2: (we omit this sentence from the discussion - no alterations to memory are made )S3: (GARCIA ALVARADO >CO &&56 >CO WAS KILLED WHEN A BOMB PLACED BY URBAN GUERRILLAS ON HISVEHICLE )XPLODED AS IT CAME TO A HALT AT AN INTERSECTION IN DOWNTOWN SAN SALVADOR >PE )CIRCUS generates 5 CNs in response to this sentence.
A simple CN describing a weapon is generated b y"BOMB ."
More complicated CNs are triggered by "KILLED," "PLACED," and "EXPLODED .
"The trigger "KILLED" creates a murder CN with victim = "GARCIA ALVARADO.
"The trigger "PLACED" creates a location CN with instrument = "BOMB," and actor = "URBA NGUERRILLAS."
This same CN also looks for a physical target inside a prepositional phrase, but it misses "ONHIS VEHICLE" because "on" is not one of the prepositions that it predicts .
If the sentence had said "outside "," inside" , "by", "near", "in", "under", "opposite", "across_from", or "in_front_of', instead of "on", we would have284picked up this physical target.
The omission of "on" was a simple oversight in an otherwise legitimate CNdefinition.
This particular CN is specifically predicting a bomb since bombs are frequently the object of the verb "t oplace" in this domain .The trigger "EXPLODED" creates a bombing CN with instrument = "A BOMB .
"Note that we miss the location San Salvador in S3.
Although we have a bottom-up mechanism designed tofind dates and locations, it doesn't always work.
All 5 CNs are placed in a single partition which generates a newincident structure containing a single subevent:SUBEVENT : BOMBINGTARGETS : NILEFFECTS : NILINSTRUMENT: (#S(INSTRUMEN TVICTIMS: (#S(VICTIMID (BOMB )ID (GARCIA ALVARADO)TYPE WS-BOMB))TITLE NILNATIONALITY NILNUM 1TYPE (WS-GOVT-OFFICIAL WS-LEGAL-OR-JUDICIA LWS-PROPER-NAME)EFFECTS (DEATH)SENTENCE 3))When MBC receives this new incident structure, it runs a memory integration test for compatibletarget/victim descriptions, and determines that this new subevent is compatible with the incident structure already i nmemory.
MBC therefore merges the two incidents, and memory acquires the fact that Alvarado was killed by abomb .S4-7: (we omit these sentences from the discussion - no alterations to memory are made )S8: (VICE PRESIDENT?ELECT FRANCISCO MERINO SAID THAT WHEN THE ATTORNEY @GENERAL@S CAR STOPPE DAT A LIGHT ON A STREET IN DOWNTOWN SAN SALVADOR >CO ANINDIVIDUALPLACEDABOMB ON THE ROOFOF THE ARMORED VFRTCLE >PE )CIRCUS generates two CNs here.
One fairly complicated CN is triggered by "PLACED ."
This CN picks upnot just the bomb as a weapon, but also the individual as the responsible party, and the vehicle as a target .
Thesecond CN describes the bomb as a weapon and its link to the targeted vehicle (as before) .
These two CNs are largelyredundant, and they are merged into a single incident structure because they share the same partition .
This incidentstructure contains a perpetrator id = "AN INDIVIDUAL" along with the following subeven tSUBEVENT: BOMBINGTARGETS : (M(PHYS-OBJVICTIMS : NILID (ARMORED VEHICLE)EFFECTS : NI LNUM 1INSTRUMENT: (#S(INSTRUMENTTYPE (WS-TRANSPORT-VEHICLE)ID (BOMB)EFFECTS NILTYPE WS-BOMB))SENTENCE 8))MBC checks this incident structure against the incident structure already in memory and determines that the yshould be merged, thereby picking up a physical target for the first time .
Had we picked up this physical target fromS3 as well, the target integration test would have merged the two vehicle descriptions at this point as well .
Note thatMBC merges the description of the perpetrator as "an individual" with the previously encountered descriptor "urbanguerrillas" because the earlier description is recognized to be more specific .S9-10: (we omit these sentences from the discussion - no alterations to memory are made )Si!
:(GUERRTLLAS ATTACKF,D (dMF.RTNOPS HOMR TN SAN SALVADOR ON APR 14 89 >CO &&5 DAYS AGO >COWITH EXPLOSIVES >PE)CIRCUS generates 7 highly redundant CNs in response to S11 .
The most comprehensive CN instantiates a nattack with actor = "GUERRILLAS," target = "MERINO'S HOME," and instrument = "EXPLOSIVES."
This same285CN also picks up the location (San Salvador) and date (April 14) by the bottom-up attachment mechanism .Locations and dates are normally not predicted by CN definitions, but they can be inserted into available CNs vi abottom-up attachment.
All of this information is incorporated into a single incident structure containing a bombin gsubevent (an attack using explosives is understood to be a bombing) .
The resulting incident structure is then passedto the memory integration portion of MBC .Just as before, MBC checks to see if the new incident can be merged into the lone incident structure currentlystored in memory .
But this time the new structure fails to match the existing structure because of incompatibl etargets .
MBC cannot merge a home with a vehicle.
When MBC fails to merge the new bombing incident with th eold bombing incident, it moves down the target stack to see if there is another incident structure that might merge ,but there are no other physical targets in memory .
MBC adds the new incident to the top of the incident stack, andmemory now contains two bombing incidents .S12:(THERE WERE &&7 CHILDREN >CO INCLUDING &&4 OF THE VICE @PRESIDENT@S CHILDREN >CO IN TH EHOME AT THE TIME >PE)CIRCUS produces no output for this sentence because no CN triggers are encountered .
We sometimes missinformation in sentences where the only verb is a form of "to be .
"S13:(A 75?YEAR?OLD NIECE OF @MERINO@S WAS INJURED >PE )CIRCUS generates an injury CN with victim = "A 15-YEAR-OLD NIECE ."
This results in a subevent o funknown type with a victim id = "A 15-YEAR-OLD NIECE."
When MBC receives this incident, it examines thefirst incident on the top of its stack to see if a merge is possible.
Since no incompatible victims are found i nmemory for this incident (the latest bombing incident specifies no victims), a merging occurs .S14-S17 : [we omit these sentences from our discussion - no alterations are made to memory .
]S18: (RICARDO VALDIVIESO >CO PRESIDENT OF THE LEGISLATIVE ASSEMBLY AND AN ARENA LEADER >COSAID THE FMLN AND ITS FRONT GROUPS ARE RESPONSIBLE FOR THE "IRRATIONAL VIOLENCE THAT JTLLEDATTORNRY GRNF.RAT, GARCIA >DQ >PE )CIRCUS produces a murder CN with victim = "Attorney General Garcia" and actor = "irrational violence .
"This CN has a soft constraint on the actor slot which specifies a human or organization, but the CN survives theCN filter because its other variable slot has a filler that does meet the required soft constraints (the filter errs on theside of spurious information if one slot looks good and the other slot looks bad) .
MBC is careful to check availablesoft constraints when it integrates information into its preliminary incident structures .
Any slot fill that violates asoft constraint is discarded at that time .When MBC attempts to integrate this incident into memory, it locates a compatible target in the victi mstack, and merges the new incident structure with the existing structure that describes Garcia as a victim .
Because wehave now merged new information into an incident that was not at the top of the incident stack, we have to reorde rthe incident stack by moving the most recently referenced incident to the top of the stack .
This effectively identifiesthe first incident as the current topic once again .
Ideally, this would set us up to correctly integrate informationcontained later in S21 and S22 where new information is presented about the vehicle bombing, but CIRCUS fails topick up the additional human targets from those sentences, so the topic shift that we 've successfully recognized atS18 goes unrewarded .When MBC completes its analysis, the two bombing incident structures are converted into two templateinstantiations, along with a third threat incident picked up from additional sentences near the end of the text .
In orderto instantiate the final templates, we rely on semantic features in our dictionary to recognize a home as a civilia nresidence and an armored vehicle as a transport vehicle.
When instantiating response templates, we attempt to fill allslots with the exception of phys-tgt-total-num and hum-tgt-total-num .We did fairly well on the first template (see Figure 1) .
We missed San Salvador as the location within E lSalvador, we said the vehicle was destroyed instead of damaged, and we missed 3 human targets (the driver who wa snot hurt, and the 2 bodyguards, one of whom was injured) .
All the other slots were correctly filled .
On the secondtemplate, we fail in three places.
We have no perpetrator organization, we miss the physical target type for Merino' s2860 .
MESSAGE: ID TST2-MUC4-004 81 .
MESSAGE: TEMPLATE 1 ;correc t2 .
INCIDENT: DATE - 19 APR 89 ;correc t3 .
INCIDENT: LOCATION EL SALVADOR :partia l4 .
INCIDENT : TYPE BOMBING ;correc t5 .
INCIDENT: STAGE OF EXEC.
ACCOMPLISHED ;correc t6 .
INCIDENT: INSTRUMENT ID "BOMB" ;correc t7 .
INCIDENT: INSTRUMENT TYPE BOMB: "BOMB" ;correc t8 .
PERP: INCIDENT CATEGORY TERRORIST ACT ;correc t9 .
PERP: INDIVIDUAL ID "URBAN GUERRILLAS" ;correc t10 : PERP: ORGANIZATION ID "FARABUNDO MARTI NATIONAL LIBERATION ;correc tFRONT'11 : PERP: ORG CONFIDENCE SUSPECTED OR ACCUSED BY AUTHORITIES : ;correct"FARABUNDO MARTI NATIONA LLIBERATION FRONT '12 : PHYS TGT : ID "ARMORED VEHICLE" ;correct13 : PHYS TGT: TYPE TRANSPORT VEHICLE : "ARMORED VEHICLE" ;correc t14 : PHYS TGT: NUMBER 1 : "ARMORED VEHICLE " ;correct15 : PHYS TGT: FOREIGN NATION - ;N/A16 .
PHYS TGT: EFFECT DESTROYED: "ARMORED VEHICLE" ;partia l17 : PHYS TGT: TOTALNUMBER ;N/A18: HUM TGT: NAME "ROBERTO GARCIA ALVARADO" ;correc t19 : HUM TGT : DESCRIPTION "ATTORNEY GENERAL": "ROBERTO GARCIA ;correct/missingALVARADO"20 : HUM TGT: TYPE GOVERNMENT OFFICIAL: "ROBERTO GARCIA ;correct/missingALVARADO"21 : HUM TGT: NUMBER 1 : "ROBERTO GARCIA ALVARADO " ;correct/missin g22 .
HUM TGT: FOREIGN NATION ;N/A23 : HUM TGT : EFFECT DEATH : "ROBERTO GARCIA ALVARADO" ;correct/missing24 : HUM TGT: TOTAL NUMBER ;N/AFigure 1 : Our response template for the first bombing inciden thome (it should have been GOVERNMENT OFFICE OR RESIDENCE), and we are missing the 7 children tha twere human targets (this is one of the few texts where a hum-tgt-total-num slot should receive a value) .Overall, TST2-MUC4-0048 showed the UMass/MUC-4 system working fairly well and not making an ymajor errors.
Most of our recall loss resulted from a failure to recognize relevant information in S12 (the 7 children) ,S21 and S22 (the driver and 2 bodyguards).
As we saw in this message, we can recover from some failures i nsentence analysis when a text provides redundant descriptions (e .g.
we missed the physical target in S3, but picked itup correctly in S8).
When memory-based consolidation responds correctly to topic transitions, the output tha tCIRCUS generates usually makes it into the correct places in the response templates .
TST2-MUC4-0048 showshow MBC was able to correctly recognize two topic transitions : first from an old incident to a new incident, and thenback again to the earlier incident .
Given that the errors encountered for TST2-MUC4-0048 were relatively minor (onecould even argue that the third template was valid and should have been covered by an optional key template), thereis nothing here that illustrates the more damaging problems that impacted our TST3 and TST4 score reports .Figure 2 shows score reports for the two templates that mapped to TST2-MUC4-0048 answer keys, along wit hthe score report for the entire message which averages in the spurious template that we generated for the threat .This final score report for the whole message illustrates how much negative impact spurious templates have o nprecision if a system is generating one spurious template for every two good templates .
If we had generated asummary score report based on only two templates instead of three, our All Templates precision would have been94 .
With the third template averaged in, our All Templates precision drops to 76 .28 7Vehicle Bombing Templat eP06 ACT COR PAR INC ACR IPA SPU MIS NON REC PRE OVGinc-total665100000092920perp-total44400000001001000phys-tgt-total443100000288880hum-tgt-total14550000092361000TOTAL281917200009464950Home Bombing TemplatePO6 ACT COR PAR INC ACR IPA SPU MIS NON REC PRE OVGInc-total66600000001001000perp-total4220000020SO1000phys-tgt-total332010000367670hum-tgt-total11440000072361000TOTAL241514010009558930Total Scores forTST2-MUC4-0048P08 ACT COR PAR INC ACR IPA- SPU MIS NON REC PRE OVGinc-total 12 16 11 1 0 0 0 4 0 2 96 72 25perp-total 8 7 6 0 0 0 0 1 2 3 75 86 14phys-tgt-total 7 7 5 1 1 0 0 0 0 11 78 78 0hum-tgt-total 25 12 9 0 0 0 0 3 16 8 36 75 25MATCHED/MISSING 52 34 31 2 1 0 0 0 18 9 62 94 0MATCHED/SPURIOUS 52 42 31 2 1 0 0 8 18 24 62 76 1 9MATCHED ONLY 52 34 31 2 1 0 0 0 18 9 62 94 0ALL TEMPLATES 52 42 31 2 1 0 0 8 18 24 62 76 19SET FILLS ONLY 23 16 14 1 1 0 0 0 7 5 63 91 0STRING FILLS ONLY 15 10 10 0 0 0 0' 0 5 1 67 100 0P&R2P&RP&2R68 .2972 .726437Figure 2 : Partial and Overall Scores for TST2-MUC4-004 8In a domain that is characterized by complicated domain guidelines, and lots of grey areas, answer keys canno tbe trusted to give encodings that are necessarily superior to the output of a high performance extraction system.
Ifthis is the case, it may be very difficult to attain 85% precision under all templates, and optimal precision level smay be closer to the 70-80% range .BIBLIOGRAPHY[1] Cardie, C. and Lehnert, W .
(1991) "A Cognitively Plausible Approach to Understanding Complex Syntax" inProceedings of the Ninth National Conference on Artificial Intelligence .
Anaheim, CA .
pp.
117-124.
[2] Lehnert, W. (1991) "Symbolic/Subsymbolic Sentence Analysis: Exploiting the Best of Two Worlds" i nAdvances in Connectionist and Neural Computation Theory.
Vol 1 .
(eds: J .
Pollack and J .
Barnden).
pp.
135-164 .
Ablex Publishing .
Norwood, NJ.
pp.
135-164.
[3] Lehnert, W., Cardie, C., Fisher, D ., Riloff, E ., Williams, R .
(1991a) "University of Massachusetts: MUC- 3Test Results and Analysis" in Proceedings of the Third Message Understanding Conference .
Morgan Kaufman .San Mateo, CA .
pp.
116-119.
[4] Lehnert, W., Cardie, C., Fisher, D., Riloff, E ., Williams, R .
(1991b) "University of Massachusetts :Description of the CIRCUS System as Used for MUC-3" in Proceedings of the Third Message UnderstandingConference.
Morgan Kaufman.
San Mateo, CA.
pp .
223-233.
[5] Lehnert, W., Williams, R., Cardie, C, Riloff, E ., and Fisher, D.(1991c) "The CIRCUS System as Used inMUC-3," Technical Report No .
91-59, Department of Computer and Information Science, University o fMassachusetts.
1991 .28 8
