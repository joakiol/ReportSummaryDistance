Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 67?74,Prague, June 2007. c?2007 Association for Computational LinguisticsThe Best of Two Worlds: Cooperation of Statisticaland Rule-Based Taggers for CzechDrahom?
?ra ?johanka?
Spoustova?Jan Hajic?Jan VotrubecInstitute of Formal and Applied LinguisticsFaculty of Mathematics and Physics,Charles University Prague, Czech Republic{johanka,hajic,votrubec}@ufal.mff.cuni.czPavel KrbecIBM Czech Republic,Voice Technologies and Systems,Prague, Czech Republic,pavel krbec@cz.ibm.comPavel Kve?ton?Institute of the Czech Language,Academy of Sciences of the Czech RepublicPavel.Kveton@seznam.czAbstractSeveral hybrid disambiguation methods aredescribed which combine the strength ofhand-written disambiguation rules and sta-tistical taggers.
Three different statistical(HMM, Maximum-Entropy and AveragedPerceptron) taggers are used in a taggingexperiment using Prague Dependency Tree-bank.
The results of the hybrid systems arebetter than any other method tried for Czechtagging so far.1 IntroductionInflective languages pose a specific problem in tag-ging due to two phenomena: highly inflective na-ture (causing sparse data problem in any statisticallybased system), and free word order (causing fixed-context systems, such as n-gram HMMs, to be evenless adequate than for English).The average tagset contains about 1,000 ?
2,000distinct tags; the size of the set of possible and plau-sible tags can reach several thousands.
There havebeen attempts at solving this problem for some ofthe highly inflective European languages, such as(Daelemans, 1996), (Erjavec, 1999) for Slovenianand (Hajic?, 2000) for five Central and Eastern Euro-pean languages.Several taggers already exist for Czech, e.g.(Hajic?
et al, 2001b), (Smith, 2005), (Hajic?
et al,2006) and (Votrubec, 2006).
The last one reachesthe best accuracy for Czech so far (95.12 %).
Henceno system has reached ?
in the absolute terms ?
aperformance comparable to English tagging (such as(Ratnaparkhi, 1996)), which stands above 97 %.We are using the Prague Dependency Treebank(Hajic?
et al, 2006) (PDT) with about 1.8 millionhand annotated tokens of Czech for training and test-ing.
The tagging experiments in this paper all usethe Czech morphological (pre)processor, which in-cludes a guesser for ?unknown?
tokens and which isavailable from the PDT website (PDT Guide, 2006)to disambiguate only among those tags which aremorphologically plausible.The meaning of the Czech tags (each tag has 15positions) we are using is explained in Table 1.
Thedetailed linguistic description of the individual posi-tions can be found in the documentation to the PDT(Hajic?
et al, 2006).67Name Description1 POS Part of Speech2 SUBPOS Detailed POS3 GENDER Gender4 NUMBER Number5 CASE Case6 POSSGENDER Possessor?s Gender7 POSSNUMBER Possessor?s Number8 PERSON Person9 TENSE Tense10 GRADE Degree of comparison11 NEGATION Negation12 VOICE Voice13 RESERVE1 Unused14 RESERVE2 Unused15 VAR VariantTable 1: Czech Morphology and the Positional Tags2 Components of the hybrid system2.1 The HMM taggerThe HMM tagger is based on the well known for-mula of HMM tagging:T?
= arg maxTP (T )P (W | T ) (1)whereP (W |T ) ?
?ni=1 P (wi | ti, ti?1)P (T ) ?
?ni=1 P (ti | ti?1, ti?2).
(2)The trigram probability P (W | T ) in formula 2replaces (Hajic?
et al, 2001b) the common (and lessaccurate) bigram approach.
We will use this taggeras a baseline system for further improvements.Initially, we change the formula 1 by introduc-ing a scaling mechanism1: T?
= arg maxT (?T ?logP (T ) + logP (W | T )).We tag the word sequence from right to left, i.e.we change the trigram probability P (W | T ) fromformula 2 to P (wi | ti, ti+1).Both the output probability P (wi | ti, ti+1) andthe transition probability P (T ) suffer a lot due tothe data sparseness problem.
We introduce a com-ponent P (endingi | ti, ti+1), where ending con-sists of the last three characters of wi.
Also, we in-troduce another component P (t?i | t?i+1, t?i+2) basedon a reduced tagset T ?
that contains positions POS,GENDER, NUMBER and CASE only (chosen onlinguistic grounds).1The optimum value of the scaling parameter ?T can betuned using held-out data.We upgrade all trigrams to fourgrams; thesmoothing mechanism for fourgrams is history-based bucketing (Krbec, 2005).The final fine-tuned HMM tagger thus uses allthe enhancements and every component contains itsscaling factor which has been computed using held-out data.
The total error rate reduction is 13.98 %relative on development data, measured against thebaseline HMM tagger.2.2 Morc?eTheMorc?e2 tagger assumes some of the HMMprop-erties at runtime, namely those that allow the Viterbialgorithm to be used to find the best tag sequence fora given text.
However, the transition weights are notprobabilities.
They are estimated by an AveragedPerceptron described in (Collins, 2002).
AveragedPerceptron works with features which describe thecurrent tag and its context.Features can be derived from any information wealready have about the text.
Every feature can betrue or false in a given context, so we can regardcurrent true features as a description of the currenttag context.For every feature, the Averaged Perceptron storesits weight coefficient, which is typically an integernumber.
The whole task of Averaged Perceptron isto sum all the coefficients of true features in a givencontext.
The result is passed to the Viterbi algorithmas a transition weight for a given tag.
Mathemati-cally, we can rewrite it as:w(C, T ) =n?i=1?i.
?i(C, T ) (3)where w(C, T ) is the transition weight for tag T incontext C, n is number of features, ?i is the weightcoefficient of ith feature and ?
(C, T )i is evaluationof ith feature for context C and tag T .Weight coefficients (?)
are estimated on trainingdata, cf.
(Votrubec, 2006).
The training algorithmis very simple, therefore it can be quickly retrainedand it gives a possibility to test many different sets offeatures (Votrubec, 2005).
As a result, Morc?e givesthe best accuracy from the standalone taggers.2The name Morc?e stands for ?MORfologie C?Es?tiny?
(?Czech morphology?
).682.3 The Feature-Based TaggerThe Feature-based tagger, taken also from the PDT(Hajic?
et al, 2006) distribution used in our exper-iments uses a general log-linear model in its basicformulation:pAC(y | x) =exp(?ni=1 ?ifi(y, x))Z(x)(4)where fi(y, x) is a binary-valued feature of the eventvalue being predicted and its context, ?i is a weightof the feature fi, and the Z(x) is the natural normal-ization factor.The weights ?i are approximated by MaximumLikelihood (using the feature counts relative to allfeature contexts found), reducing the model essen-tially to Naive Bayes.
The approximation is nec-essary due to the millions of the possible featureswhich make the usual entropy maximization infeasi-ble.
The model makes heavy use of single-categoryAmbiguity Classes (AC)3, which (being indepen-dent on the tagger?s intermediate decisions) can beincluded in both left and right contexts of the fea-tures.2.4 The rule-based componentThe approach to tagging (understood as a stand-alone task) using hand-written disambiguation ruleshas been proposed and implemented for the firsttime in the form of Constraint-Based Grammars(Karlsson, 1995).
On a larger scale, this aproach wasapplied to English, (Karlsson, 1995) and (Samuels-son, 1997), and French (Chanod, 1995).
Also (Bick,2000) uses manually written disambiguation rulesfor tagging Brazilian Portuguese, (Karlsson, 1985)and (Koskenniemi, 1990) for Finish and (Oflazer,1997) reports the same for Turkish.2.4.1 OverviewIn the hybrid tagging system presented in this pa-per, the rule-based component is used to further re-duce the ambiguity (the number of tags) of tokensin an input sentence, as output by the morphologicalprocessor (see Sect.
1).
The core of the componentis a hand-written grammar (set of rules).Each rule represents a portion of knowledge ofthe language system (in particular, of Czech).
The3If a token can be a N(oun), V(erb) or A(djective), its (majorPOS) Ambiguity Class is the value ?ANV?.knowledge encoded in each rule is formally definedin two parts: a sequence of tokens that is searchedfor in an input sentence and the tags that can bedeleted if the sequence of tokens is found.The overall strategy of this ?negative?
grammar isto keep the highest recall possible (i.e.
100 %) andgradually improve precision.
In other words, when-ever a rule deletes a tag, it is (almost) 100% safe thatthe deleted tag is ?incorrect?
in the sentence, i.e.
thetag cannot be present in any correct tagging of thesentence.Such an (virtually) ?error-free?
grammar can par-tially disambiguate any input and prevent the subse-quent taggers (stochastic, in our case) to choose tagsthat are ?safely incorrect?.2.4.2 The rulesFormally, each rule consists of the description ofthe context (sequence of tokens with some specialproperty), and the action to be performed given thecontext (which tags are to be discarded).
The lengthof context is not limited by any constant; however,for practical purposes, the context cannot cross oversentence boundaries.For example: in Czech, two finite verbs cannotappear within one clause.
This fact can be used todefine the following disambiguation rule:?
context: unambiguous finite verb, fol-lowed/preceded by a sequence of tokenscontaining neither a comma nor a coordinat-ing conjunction, at either side of a word xambiguous between a finite verb and anotherreading;?
action: delete the finite verb reading(s) at theword x.It is obvious that no rule can contain knowledgeof the whole language system.
In particular, eachrule is focused on at most a few special phenomenaof the language.
But whenever a rule deletes a tagfrom a sentence, the information about the sentencestructure ?increases?.
This can help other rules to beapplied and to delete more and more tags.For example, let?s have an input sentence with twofinite verbs within one clause, both of them ambigu-ous with some other (non-finite-verbal) tags.
In thissituation, the sample rule above cannot be applied.69On the other hand, if some other rule exists in thegrammar that can delete non-finite-verbal tags fromone of the tokens, then the way for application of thesample rule is opened.The rules operate in a loop in which (theoreti-cally) all rules are applied again whenever a ruledeletes a tag in the partially disambiguated sentence.Since deletion is a monotonic operation, the algo-rithm is guaranteed to terminate; effective imple-mentation has also been found in (Kve?ton?, 2006).2.4.3 Grammar used in testsThe grammar is being developed since 2000 asa standalone module that performs Czech morpho-logical disambiguation.
There are two ways of ruledevelopment:?
the rules developed by syntactic introspection:such rules are subsequently verified on the cor-pus material, then implemented and the imple-mented rules are tested on a testing corpus;?
the rules are derived from the corpus by intro-spection and subsequently implemented.In particular, the rules are not based on examina-tion of errors of stochastic taggers.The set of rules is (manually) divided into two(disjoint) reliability classes ?
safe rules (100% re-liable rules) and heuristics (highly reliable rules, butobscure exceptions can be found).
The safe rules re-flect general syntactic regularities of Czech; for in-stance, no word form in the nominative case can fol-low an unambiguous preposition.
The less reliableheuristic rules can be exemplified by those account-ing for some special intricate relations of grammati-cal agreement in Czech.The grammar consists of 1727 safe rules and 504heuristic rules.
The system has been used in twoways:?
safe rules only: in this mode, safe rules are ex-ecuted in the loop until some tags are beingdeleted.
The system terminates as soon as norule can delete any tag.?
all rules: safe rules are executed first (see saferules only mode).
Then heuristic rules startto operate in the loop (similarly to the saferules).
Any time a heuristic rule deletes a tag,the safe rules only mode is entered as a sub-procedure.
When safe rules?
execution termi-nates, the loop of heuristic rules continues.
Thedisambiguation is finished when no heuristicrule can delete any tag.The rules are written in the fast LanGR formalism(Kve?ton?, 2006) which is a subset of more generalLanGR formalism (Kve?ton?, 2005).
The LanGR for-malism has been developed specially for writing andimplementing disambiguation rules.3 Methods of combination3.1 Serial combinationThe simplest way of combining a hand-written dis-ambiguation grammar with a stochastic tagger is tolet the grammar reduce the ambiguity of the tagger?sinput.
Formally, an input text is processed as fol-lows:1. morphological analysis (every input token getsall tags that are plausible without looking atcontext);2. rule-based component (partially disambiguatesthe input, i.e.
deletes some tags);3. the stochastic tagger (gets partially disam-biguated text on its input).This algorithm was already used in (Hajic?
etal., 2001b), only components were changed ?
theruled-based component was significantly improvedand two different sets of rules were tried, as wellas three different statistical taggers.
The best resultwas (not surprisingly) achieved with set of safe rulesfollowed by the Morc?e tagger.An identical approach was used in (Tapanainen,1994) for English.3.2 Serial combination with SUBPOSpre-processingManual inspection of the output of the application ofthe hand-written rules on the development data (asused in the serial combination described in the pre-vious section) discovered that certain types of dead-locked (?cross-dependent?)
rules prevent successfuldisambiguation.70Cross-dependence means that a rule A can notapply because of some remaining ambiguity, whichcould be resolved by a ruleB, but the operation ofBis still dependent on the application of A.
In particu-lar, ambiguity in the Part-of-Speech category is veryproblematic.
For example, only a few safe rules canapply to a three-word sentence where all three wordsare ambiguous between finite verbs and somethingelse.If the Part-of-Speech ambiguity of the input is al-ready resolved, precision of the rule-based compo-nent and also of the final result after applying any ofthe statistical taggers improves.
Full Part-of-Speechinformation is represented by the first two categoriesof the Czech morphology tagset ?
POS and SUB-POS, which deals with different types of pronouns,adverbs etc.
As POS is uniquely determined bySUBPOS (Hajic?
et al, 2006), it is sufficient to re-solve the SUBPOS ambiguity only.All three taggers achieve more than 99% accuracyin SUBPOS disambiguation.
For SUBPOS disam-biguation, we use the taggers in usual way (i.e.
theydetermine the whole tag) and then we put back alltags having the same SUBPOS as the tag chosen bythe tagger.Thus, the method with SUBPOS pre-processingoperates in four steps:1. morphological analysis;2.
SUBPOS disambiguation (any tagger);3. rule-based component;4. final disambiguation (the same tagger4).The best results were again achieved with the tag-ger Morc?e and set of safe rules.3.3 Combining more taggers in parallelThis method is quite different from previous ones,because it essentially needs more than one tagger.
Itconsists of the following steps:1. morphological analysis;4This limitation is obviously not necessary, but we treat thiscombination primarily as a one-tagger method.
Results of em-ploying two different taggers are only slightly better, but stillmuch worse than results of other methods presented later be-low.2.
running N taggers independently;3. merging the results from the previous step ?each token ends up with between 1 and N tags,a union of the taggers?
outputs;4.
(optional: the rule-based component;)5. final disambiguation (single tagger).The best results were achieved with two taggersin Step 1 (Feature-based and Morc?e), set of all rulesin Step 3 and the HMM tagger in Step 4.This method is based on an assumption that dif-ferent stochastic taggers make complementary mis-takes, so that the recall of the ?union?
of taggersis almost 100 %.
Several existing language mod-els are based on this assumption ?
(Brill, 1998)for tagging English, (Borin, 2000) for tagging Ger-man and (Vidova?-Hladka?, 2000) for tagging inflec-tive languages.
All these models perform some kindof ?voting?
?
for every token, one tagger is selectedas the most appropriate to supply the correct tag.The model presented in this paper, however, entruststhe selection of the correct tag to another tagger thatalready operates on the partially disambiguated in-put.4 ResultsAll the methods presented in this paper have beentrained and tested on the PDT version 2.05.
Tag-gers were trained on PDT 2.0 training data set(1,539,241 tokens), the results were achieved onPDT 2.0 evaluation-test data set (219,765 tokens),except Table 6, where PDT 2.0 development-testdata set (201,651 tokens) was used.
The morpholog-ical analysis processor and all the taggers were usedin versions from April 2006 (Hajic?
et al, 2006), therule-based component is from September 2006.For evaluation, we use both precision and recall(and the corresponding F-measure) and accuracy,since we also want to evaluate the partial disam-biguation achieved by the hand-written rules alone.Let t denote the number of tokens in the test data,let c denote the number of tags assigned to all to-kens by a disambiguation process and let h denote5The results cannot be simply (number-to-number) com-pared to previous results on Czech tagging, because differenttraining and testing data (PDT 2.0 instead of PDT 1.0) are usedsince 2006.71the number of tokens where the manually assignedtag is present in the output of the process.?
In case of the morphological analysis processorand the standalone rule-based component, theoutput can contain more than one tag for ev-ery token.
Then precision (p), recall (r) and F-measure (f ) characteristics are defined as fol-lows:p = h/c r = h/t f = 2pr/(p + r).?
The output of the stochastic taggers contains al-ways exactly one tag for every token ?
thenp = r = f = h/t holds and this ratio is de-noted as accuracy.Table 2 shows the performance of the morpholog-ical analysis processor and the standalone rule-basedcomponent.
Table 3 shows the performance of thestandalone taggers.
The improvement of the combi-nation methods is presented in Table 4.Table 5 shows the relative error rate reduction.The best method presented by this paper (parallelcombination of taggers with all rules) reaches therelative error rate decrease of 11.48 % in compari-son with the tagger Morc?e (which achieves the bestresults for Czech so far).Table 6 shows error rate (100 % ?
accuracy) ofvarious methods6 on particular positions of the tags(13 and 14 are omitted).
The most problematic posi-tion is CASE (5), whose error rate was significantlyreduced.5 ConclusionWe have presented several variations of a novelmethod for combining statistical and hand-writtenrule-based tagging.
In all cases, the rule-basedcomponent brings an improvement ?
the smallerthe involvement of the statistical component(s) is,the bigger.
The smallest gain can be observedin the case of the parallel combination of taggers(which by itself brings an expected improvement).The best variation improved the accuracy of thebest-performing standalone statistical tagger by over6F-b stands for feature-based taggeer, Par for parallel com-bination without rules and Par+Rul for parallel combinationwith rules.11 % (in terms of relative error rate reduction), andthe inclusion of the rule-component itself improvedthe best statistical-only combination by over 3.5 %relative.This might actually lead to pessimism regardingthe rule-based component.
Most other inflective lan-guages however have much smaller datasets avail-able than Czech has today; in those cases, we expectthat the contribution of the rule-based component(which does not depend on the training data size, ob-viously) will be much more substantial.The LanGR formalism, now well-developed,could be used for relatively fast development forother languages.
We are, of course, unable to giveexact figures of what will take less effort ?
whetherto annotate more data or to develop the rule-basedcomponent for a particular language.
Our feeling isthat the jury is actually still out on this issue, de-spite some people saying that annotation is alwayscheaper: annotation for morphologically complex(e.g., inflective) languages is not cheap, and rule-based development efforts have not been previouslyusing (unannotated) corpora so extensively (whichis what LanGR supports for ?testing?
the developedrules, leading to more reliable rules and more effec-tive development cycle).On the other hand, the rule-based component hasalso two obvious and well-known disadvantages: itis language dependent, and the application of therules is slower than even the baseline HMM taggerdespite the ?fast?
version of the LanGR implemen-tation we are using7.In any case, our experiments produced a softwaresuite which gives the all-time best results in Czechtagging, and we have offered to apply it to re-tag theexisting 200 mil.
word Czech National Corpus.
Itshould significantly improve the user experience (forsearching the corpus) and allow for more precise ex-periments with parsing and other NLP applicationsthat use that corpus.7In the tests presented in this paper, the speed of the op-eration of each stochastic tagger (and the parallel combinationwithout rules) is several hundreds of tokens processed per sec-ond (running on a 2.2GHz Opteron processor).
The operation ofthe standalone rule-based component, however, is cca 10 timesslower ?
about 40 tokens per second.
The parallel combinationwith all rules processes about 60 tokens per second ?
the rulesoperate faster here because their input in parallel combinationis already partially disambiguated.72Method p r fMorphology 25.72 % 99.39 % 40.87 %Safe rules 57.90 % 98.83 % 73.02 %All rules 66.35 % 98.03 % 79.14 %Table 2: Evaluation of rules aloneTagger accuracyFeature-based 94.04 %HMM 94.82 %Morc?e 95.12 %Table 3: Evaluation of the taggers aloneCombination method accuracySerial (safe rules+Morc?e) 95.34 %SUBPOS serial (safe rules+Morc?e) 95.44 %Parallel without rules 95.52 %Parallel with all rules 95.68 %Table 4: Evaluation of the combinationsMethod Morc?e ParallelwithoutrulesParallel without rules 8.20 % ?Parallel with all rules 11.48 % 3.57 %Table 5: Relative error rate reductionF-b HMM Morc?e Par Par+Rul1 0.61 0.70 0.66 0.57 0.572 0.69 0.78 0.75 0.64 0.643 1.82 1.49 1.66 1.39 1.374 1.56 1.30 1.38 1.18 1.155 4.03 3.53 3.08 2.85 2.626 0.02 0.03 0.03 0.02 0.027 0.01 0.01 0.01 0.01 0.018 0.06 0.07 0.08 0.06 0.059 0.05 0.08 0.07 0.05 0.0410 0.29 0.28 0.30 0.26 0.2711 0.29 0.31 0.33 0.28 0.2812 0.05 0.08 0.06 0.05 0.0415 0.31 0.31 0.31 0.28 0.29Table 6: Error rate [%] on particular positions of tagsAcknowledgementsThe research described here was supported by theprojects MSM0021620838 and LC536 of Ministry ofEduation, Youth and Sports of the Czech Republic,GA405/06/0589 of the Grant Agency of the CzechRepublic and 1ET100610409 Diagnostic and Eval-uation Tools for Linguistic Software of the Informa-tion Society Programme of the National ResearchProgramme of the Czech Republic.ReferencesEckhard Bick.
2000.
The parsing system ?Palavras??
automatic grammatical analysis of Portuguese in aconstraint grammar framework.
In: Proceedings of the2nd International Conference on Language Resourcesand Evaluation, TELRI.
AthensLars Borin.
2000.
Something borrowed, something blue:Rule-based combination of POS taggers.
In: Proceed-ings of the 2nd International Conference on LanguageResources and Evaluation, Vol.
1, pp.
21?26.
AthensEric Brill and Jun Wu.
1998.
Classifier combinationfor improved lexical disambiguation.
In: Proceedingsof the 17th international conference on Computationallinguistics, Vol.
1, pp.
191?195.
Montreal, QuebecJean-Pierre Chanod and Pasi Tapanainen.
1995.
TaggingFrench ?
comparing a statistical and a constraint-based method.
In: Proceedings of EACL-95, pp.
149?157.
DublinMichael Collins.
2002.
Discriminative Training Meth-ods for Hidden Markov Models: Theory and Experi-ments with Perceptron Algorithms.
In: Proceedingsof EMNLP?02, July 2002, pp.
1?8.
PhiladelphiaW.
Daelemans and Jakub Zavrel and Peter Berck andSteven Gillis.
1996.
MBT: A memory-based part ofspeech tagger-generator.
In: Proceedings of the 4thWVLC, pp.
14?27.
CopenhagenTomaz Erjavec and Saso Dzeroski and Jakub Zavrel.1999.
Morphosyntactic Tagging of Slovene: Evaluat-ing PoS Taggers and Tagsets.
Technical Report, Dept.for Intelligent Systems, Jozef Stefan Institute.
Ljubl-janaJan Hajic?
and Barbora Hladka?.
1997.
Tagging of in-flective languages: a comparison.
In: Proceedings ofANLP ?97, pp.
136?143.
Washington, DC.Jan Hajic?
2000.
Morphological tagging: Data vs. dic-tionaries.
In: Proceedings of the 6th ANLP / 1stNAACL?00, pp.
94?101.
Seattle, WA73Jan Hajic?, Pavel Krbec, Pavel Kve?ton?, Karel Oliva andVladim?
?r Petkevic?.
2001.
Serial Combination ofRules and Statistics: A Case Study in Czech Tag-ging.
In: Proceedings of the 39th Annual Meeting ofthe Association for Computational Linguistics.
CNRS?
Institut de Recherche en Informatique de Toulouseand Universite?
des Sciences Sociales, pp.
260?267.ToulouseJan Hajic?, Eva Hajic?ova?, Jarmila Panevova?, PetrSgall, Petr Pajas, Jan S?te?pa?nek, Jir???
Havelkaand Marie Mikulova?.
2006.
Prague De-pendency Treebank v2.0.
CDROM.
Linguis-tic Data Consortium, Cat.
LDC2006T01.
Philadel-phia.
ISBN 1-58563-370-4.
Documentation also athttp://ufal.ms.mff.cuni.cz/pdt2.0.Fred Karlsson.
1985.
Parsing Finnish in terms of a pro-cess grammar.
In: Fred Karlsson (ed.
): ComputationalMorphosyntax: Report on Research 1981-84, Univer-sity of Helsinki, Department of General LinguisticsPublications No.
13, pp.
137?176.Fred Karlsson and Atro Voutilainen and Juha Heikkila?and Arto Anttila (eds.).
1995.
Constraint Grammar: alanguage-independent system for parsing unrestrictedtext.
Natural Language Processing.
Vol.
4, Moutonde Gruyter, Berlin and New York.Kimmo Koskenniemi.
1990.
Finite-State Parsing andDisambiguation.
In: Proceedings of Coling-90, Uni-versity of Helsinki, 1990, pp.
229?232.
HelsinkiPavel Krbec.
2005.
Language Modelling for SpeechRecognition of Czech.
PhD Thesis, MFF, Charles Uni-versity Prague.Pavel Kve?ton?.
2005.
Rule-based Morphological Dis-ambiguation.
PhD Thesis, MFF, Charles UniversityPrague.Pavel Kve?ton?.
2006.
Rule-based morphological dis-ambiguation: On computational complexity of theLanGR formalism.
In: The Prague Bulletin of Mathe-matical Linguistics, Vol.
85, pp.
57?72.
PragueKemal Oflazer and Go?khan Tu?r.
1997.
Morphologicaldisambiguation by voting constraints.
In: Proceedingsof the 8th conference on European chapter of the As-sociation for Computational Linguistics, pp.
222?229.MadridKarel Oliva, Milena Hna?tkova?, Vladim?
?r Petkevic?
andPavel Kve?ton?.
2000.
The Linguistic Basis of a Rule-Based Tagger of Czech.
In: Sojka P., Kopec?ek I.,Pala K.
(eds.
): Proceedings of the Conference ?Text,Speech and Dialogue 2000?, Lecture Notes in Artifi-cial Intelligence, Vol.
1902.
Springer-Verlag, pp.
3?8.Berlin-HeidelbergPDTGuide.
http://ufal.ms.mff.cuni.cz/pdt2.0A.
Ratnaparkhi.
1996.
A maximum entropy model forpart-of-speech tagging.
In: Proceedings of the 1stEMNLP, May 1996, pp.
133?142.
PhiladelphiaChrister Samuelsson and Atro Voluntainen.
1997.
Com-paring a linguistic and a stochastic tagger.
In: Pro-ceedings of ACL/EACL Joint Converence, pp.
246?252.
MadridNoah A. Smith and David A. Smith and Roy W.Tromble.
2005.
Context-Based Morphological Dis-ambiguation with Random Fields.
In: Proceedings ofHLT/EMNLP, pp.
475?482.
VancouverDrahom?
?ra ?johanka?
Spoustova?.
in prep.
Kombino-vane?
statisticko-pravidlove?
metody znac?kova?n??
c?es?tiny.
(Combining Statistical and Rule-Based Approaches toMorphological Tagging of Czech Texts).
PhD Thesis,MFF UK, in prep.Pasi Tapanainen and Atro Voutilainen.
1994.
Taggingaccurately: don?t guess if you know.
In: Proceedingsof the 4th conference on Applied Natural LanguageProcessing, pp.
47?52.
StuttgartBarbora Vidova?-Hladka?.
2000.
Czech Language Tag-ging.
PhD thesis, U?FAL MFF UK.
PragueJan Votrubec.
2005.
Volba vhodny?ch rysu?
pro morfolog-icke?
znac?kova?n??
c?es?tiny.
(Feature Selection for Mor-phological Tagging of Czech.)
Master thesis, MFF,Charles University, Prague.Jan Votrubec.
2006.
Morphological Tagging Based onAveraged Perceptron.
In: WDS?06 Proceedings ofContributed Papers, MFF UK, pp.
191?195.
Prague74
