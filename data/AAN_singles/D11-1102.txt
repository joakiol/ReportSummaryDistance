Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1104?1115,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsHypotheses Selection Criteria in a Reranking Framework for SpokenLanguage UnderstandingMarco DinarelliLIMSI-CNRSB.P.
133, 91403 Orsay CedexFrancemarcod@limsi.frSophie RossetLIMSI-CNRSB.P.
133, 91403 Orsay CedexFrancerosset@limsi.frAbstractReranking models have been successfully ap-plied to many tasks of Natural Language Pro-cessing.
However, there are two aspects ofthis approach that need a deeper investiga-tion: (i) Assessment of hypotheses generatedfor reranking at classification phase: baselinemodels generate a list of hypotheses and theseare used for reranking without any assess-ment; (ii) Detection of cases where rerank-ing models provide a worst result: the besthypothesis provided by the reranking modelis assumed to be always the best result.
Insome cases the reranking model provides anincorrect hypothesis while the baseline besthypothesis is correct, especially when base-line models are accurate.
In this paper wepropose solutions for these two aspects: (i)a semantic inconsistency metric to select pos-sibly more correct n-best hypotheses, from alarge set generated by an SLU basiline model.The selected hypotheses are reranked apply-ing a state-of-the-art model based on PartialTree Kernels, which encode SLU hypothe-ses in Support Vector Machines with com-plex structured features; (ii) finally, we applya decision strategy, based on confidence val-ues, to select the final hypothesis between thefirst ranked hypothesis provided by the base-line SLU model and the first ranked hypothe-sis provided by the re-ranker.
We show the ef-fectiveness of these solutions presenting com-parative results obtained reranking hypothe-ses generated by a very accurate ConditionalRandom Field model.
We evaluate our ap-proach on the French MEDIA corpus.
The re-sults show significant improvements with re-spect to current state-of-the-art and previousre-ranking models.1 IntroductionDiscriminative reranking is a widely used approachfor several Natural Language Processing (NLP)tasks: Syntactic Parsing (Collins, 2000), Named En-tity Recognition (Collins, 2000; Collins and Duffy,2001), Semantic Role Labelling (Moschitti et al,2008), Machine Translation (Shen et al, 2004),Question Answering (Moschitti et al, 2007).
Re-cently reranking approaches have been successfullyapplied also to Spoken Language Understanding(SLU) (Dinarelli et al, 2009b).Discriminative Reranking combines two models:a first SLU model is used to generate a ranked listof n-best hypotheses; a reranking model sorts thelist based on a different score and the final resultis the new top ranked hypothesis.
The advantage ofreranking approaches is in the possibility to learn di-rectly complex dependencies in the output domain,as this is provided in the hypotheses generated bythe baseline model.In previous approaches complex features are ex-tracted from the hypotheses for both training andclassification phase, but there are very few stud-ies on approaches that can be applied to search inthe hypotheses space generated by the baseline SLUmodel.
Moreover, to keep overall computationalcost reasonable, the size of the n-best list is typicallysmall (few tens).
This is a limitation since the largeris the hypotheses space generated, the more likely isto find a better hypothesis.
On the other hand, re-ranking a large set of hypotheses is computationally1104expensive, thus a strategy to select the best hypothe-ses to be re-ranked would overcome this problem.Another aspect of reranking that deserves to bedeeper studied is its applicability.
Although areranking model improves the baseline model in theoverall performance, in some cases the reranked besthypotheses can contain more mistakes than the base-line best hypothesis.
A strategy to decide when thereranking model should be applied and when thefirst hypothesis of the baseline model is more accu-rate would improve reranking performances.In this paper, we propose two new models forimproving discriminative reranking: (a) a seman-tic inconsistency metric that can be applied to SLUhypotheses to select those that are more likely tobe correct; (b) a model selection strategy based onthe confidence scores provided by the baseline SLUmodel and the reranker.
This provides a decisionfunction that detects if the original top ranked hy-pothesis is more accurate than the reranked best hy-pothesis.Our re-ranking strategies turn out to be effectiveon very accurate baseline models based on state-of-the-art Conditinal Random Fields (CRF) implemen-tation (Lavergne et al, 2010).
We evaluate our ap-proach on the well-known French MEDIA corpusfor SLU (Bonneau-Maynard et al, 2006).
The re-sults show that our approach significantly improvesboth ?traditional?
reranking approaches and state-of-the-art SLU models.The remainder of the paper is organized as fol-lows: in Section 2 we introduce the SLU task.
Sec-tion 3 describes our discriminative reranking frame-work for SLU, in particular the baseline modeladopted, in sub-section 3.1, and the rerankingmodel, in sub-section 3.2.
Section 4 describesthe two strategies proposed in this paper for SLUreranking, whereas the experiments to evaluate ourapproaches are described in Section 5.
Finally, aftera discussion in Section 6, in Section 7 we draw someconclusions.2 Spoken Language UnderstandingSpoken Language Understanding is the task of rep-resenting and extracting the meaning of natural lan-guage sentences.
Designing a general meaning rep-resentation which can capture the semantics of aspoken language is very complex.
Therefore, inpractice, the meaning representations depend on thespecific application domain being modeled.For the corpus used in this work, the semantic rep-resentation is defined in an ontology described in(Bonneau-Maynard et al, 2006).
As an example,given the following natural language sentence trans-lated from the MEDIA corpus:?Good morning I would like to book an hotel room in Lon-don?The semantic representation extraction for theSLU task is performed in two steps:1.
Automatic Concept LabelingNull{Good morning} command-task{I would like to book}object-bd{an hotel room} localization-city{in London}2.
Attribute-Value Extractioncommand-task[reservation] object-bd[hotel] localization-city[London]command-task, object-bd and localization-cityare three domain concepts, called also ?attributes?,defined in the ontology and Null is the concept forwords not associated to any concept.
As shown inthe example, Null concepts are removed from the fi-nal output since they don?t bring any semantic con-tent with respect to the application domain.
reserva-tion, hotel and London are the normalized attributevalues, defined also in the application ontology.
Thisrepresentation is usually called attribute-value repre-sentation.In the last decade several probabilistic modelshave been proposed for the Automatic Concept La-beling step: in (Raymond et al, 2006) a conceptuallanguage model encoded in Stochastic Finite StateTransducers (SFST) is proposed.
In (Raymond andRiccardi, 2007), the SFST-based model is comparedwith Support Vector Machines (SVM) (Vapnik,1998) and Conditional Random Fields (CRF) (Laf-ferty et al, 2001).
Moreover, in (Hahn et al, 2008a)two more models are applied to SLU: a MaximumEntropy (EM) model and a model coming fromthe Statistical Machine Translation (SMT) commu-nity (it is actually a log-linear combination of SMTmodels).
Among these models, CRF has shown ingeneral superior performances on sequence labelingtasks like Named Entity Recognition (NER) (TjongKim Sang and De Meulder, 2003), Grapheme-to-Phoneme transcription (Sejnowski and Rosenberg,11051987) and also Spoken Language Understanding(Hahn et al, 2008a).In addition to individual systems, more recentlyalso some system combination approaches havebeen tried on SLU.
In (Hahn et al, 2010), two suchapproaches are compared, one based on weightedROVER (Fiscus, 1997) while the other is the rerank-ing approach proposed in (Dinarelli et al, 2009b).Both system combination approaches are applied onthe MEDIA corpus, thus we will refer to (Hahn etal., 2010) for a comparison with our approach.Like the other tasks mentioned above, SLU is usu-ally a supervised learning task, this means that mod-els are learned from annotated data.
This is an im-portant aspect to take into account when designingSLU systems.
In this respect accurate SLU modelscan in part alleviate the problem of manually anno-tating data.The second step of SLU, that is Attribute ValueExtraction (from now on AVE) is performed withtwo approaches: a) Rule-based approaches applyRegular Expressions (RE) to map the words realiz-ing a concept into a normalized value.
Regular ex-pressions are defined for each attribute-value pair.Given a concept and its realizing surface form, if aRE for that concept matches the surface, the corre-sponding value is returned.An example of surfaces that can be mapped intothe value hotel given the concept object-bd is:1. an hotel room2.
a hotel room3.
the hotel...Note that these surfaces share the same keywordfor the concept object-bd, which is ?hotel?.
Thus,a possible rule extracted from data, for the conceptobject-bd can be:Robject?bd(S) =if S = ?an hotel room?
orS = ?a hotel room?
orS = ?the hotel?
thenreturn ?hotel?endThis kind of rules can be easily refined using reg-ular expressions, so that they can capture all possiblelinguistic patterns containing the triggering keyword(?hotel?
in the example).b) The other approach used for attribute value ex-traction is based on probabilistic models.
In this casethe model learns from data the conditional probabil-ity of values V , given the concept C and the cor-responding sequence of words W realizing the con-cept: P (V |W,C).The most meaningful work about AVE ap-proaches in SLU tasks is (Hahn et al, 2010).The model used in this work for Automatic Con-cept Labeling is based on CRF.
For the Attribute-Value Extraction phase we use a combination ofrule based and probabilistic approaches.
The firstis made of regular expressions, as explained above.The probabilistic approach is based again on CRF.3 Reranking FrameworkThis section describes the different models involvedin the pipeline realising our reranking framework:?
Conditional Random Fields?
Semantic Inconsistency Metric for hypothesesselection, which is optional and is applied onlyat the classification phase?
Support Vector Machines with Partial Tree Ker-nel?
Decision Strategy to detect when the top rankedhypothesis of the baseline model is more accu-rate than the reranked best hypothesisIt is important to underline that the phases in-volved in the reranking framewrok are distinguishedfor a matter of clarity.
In principle, the phasesfrom the hypotheses selection to the last, the deci-sion strategy, can be thought of as a whole rerankingmodel.In the next two subsection we describe the twomodels used for hypotheses generation and forreranking: CRF and SVM with kernel methods.
Thetwo improvements proposed in this paper and listedabove are presented in a dedicated section (4).3.1 Conditional Random FieldsCRFs have been proposed for the first time for se-quence segmentation and labeling tasks in (Laffertyet al, 2001).
This model belongs to the family ofexponential or log-linear models.
Its main charac-teristics are the possibility to include a huge number1106of features, like the Maximum Entropy (ME) model,but computing global conditional probabilities nor-malized at sentence level, instead of position levellike in ME.
In particular this last point results veryeffective since it solves the label bias problem, aspointed out in (Lafferty et al, 2001).Given a sequence of N words WN1 = w1, ..., wNand its corresponding sequence of concepts CN1 =c1, ..., cN , CRF trains the conditional probabilitiesP (CN1 |WN1 ) =1ZN?n=1exp( M?m=1?m ?
hm(cn?1, cn, wn+2n?2))(1)where ?m are the training parameters.hm(cn?1, cn, wn+2n?2) are the feature functionscapturing conditional dependencies of concepts andwords.
Z is a probability normalization factor inorder to model well defined probability distribution:Z =?c?N1N?n=1H(c?n?1, c?n, wn+2n?2) (2)here c?n?1 and c?n are the concepts hypoth-esized for the previous and current words,H(c?n?1, c?n, wn+2n?2) is an abbreviation for?Mm=1 ?m ?
hm(cn?1, cn, wn+2n?2).The CRF model used for the Attribute-Value Ex-traction phase learns in the same way the conditionalprobability P (V N1 |CN1 ,WN1 ).
In particular we useattributes-words concatenations on the source sideand attribute values on the target side.Two particular effective implementations of CRFshave been recently proposed.
One is described in(Hahn et al, 2009) and uses a margin based criterionfor probabilities estimation.
The other is describedin (Lavergne et al, 2010) and has been implementedin the software wapiti1.
The latter solution in partic-ular trains the model using two different regulariza-tion factors at the same time:Gaussian prior, used as l2 regularization and usedin many softwares to avoid overfitting;Laplacian prior, used as l1 regularization (Riezlerand Vasserman, 2010), which has the effect to filterout features with very low scores.1available at http://wapiti.limsi.frThe two regularization parameters are used to-gether in the model implementing the so-called elas-tic net regularization (Zou and Hastie, 2005):l(?)
+ ?1??
?1 +?22 ??
?22 (3)?
is the set of parameters of the model introducedin equation 1, l(?)
is the minus-logarithm of equa-tion 1, used as loss function for training CRF.
??
?1and ??
?2 are the l1 and l2 regularization, respec-tively, while ?1 and ?2 are two parameters that canbe optimized as usual on development data or withcross validation.As explained in (Lavergne et al, 2010), using l1regularization is an effective way for feature selec-tion in CRF at training time.
Note that other ap-proaches have been proposed for feature selection,e.g.
in (McCallum, 2003).
This type of features se-lection, performed directly at training time, yieldsvery accurate models, since only the most meaning-ful features are kept in the final model, which guar-antee a strong robustness on unseen data.In this work we refer in particular to the CRF im-plementation described in (Lavergne et al, 2010).3.2 SVM and Kernel MethodsOur reranking model is based on SVM (Vapnik,1998) with the use of the Partial Tree Kernel definedin (Moschitti, 2006).SVMs are well-known machine learning algo-rithms belonging to the class of maximal-margin lin-ear classifiers (Vapnik, 1998).
The model representsa hyperplane which separates the training exampleswith a maximum margin.
The hyperplane is learnedusing optimization theory and is represented in thedual form as a linear combination of training exam-ples:?i=1..l yi?i ~xi~x + b = 0,where ~xi, i ?
[1, .., l] are training examples rep-resenting objects oi and o in any feature space, yi isthe label associated with ~xi and ?i are the lagrangemultipliers.
The dual form of the hyperplane showsthat SVM training depends on the inner product be-tween instances.
Kernel methods theory (Shawe-Taylor and Cristianini, 2004), allows us to substitutethe inner product with a so-called kernel function,computing the same result: K(oi, o) = ~xi ?
~x.1107The interesting aspect of using such formulationis the possibility to compare objects in arbitrar-ily complex feature spaces implicitly, i.e.
withoutknowing the features to be used.
Since in real worldscenarios data cannot be classified using a simplelinear classifier, kernel methods can be used to carryout learning in complex feature spaces.
In this workwe use the Partial Tree Kernel (PTK) (Moschitti,2006).3.3 Reranking ModelIn order to give an effective representation to SLUhypotheses in SVM, since we are using PTK, weneed to represent as trees SLU hypotheses like theone described in section 2.This problem is easily solved by transforming thehypotheses into trees like the one depicted in fig-ure 1.
Although there may be more formal solutionsto represent semantic information of SLU hypothe-ses as trees, we would like to remark that the treestructure shown in figure 1 contains all the key in-formation needed for our purposes: the first level ofthe tree represents the concept sequence annotatedon surface form.
The second level of the tree al-low to implicitly represent the segmentation of eachconcept, while the third level, i.e.
the leaves, are theinput words.
Moreover, from figure 1 we removedword categories associated to words in order to keepthe figure readable.
Word categories are providedtogether with the corpus as an application knowl-edge base.
They comprise domain categories likecity names, hotel names, street names etc., and somedomain independent categories like numbers, dates,months etc.
The categories are used at the same levelof words, they provide a generalization over wordsand alleviate the effect of Out-of-Vocabulary (OOV)words.The CRF model used as baseline generates then most likely conceptual annotations for each inputsentence.
These are ranked by the global conditionalprobability of the concept sequence, given the inputword sequence of CRF.
The n-best list produced bythe baseline model is the list of candidate hypothesesH1, H2, .., Hn used in the reranking step.The candidate hypotheses are organized intopairs, e.g.
(H1, H2) or (H1, H3).
We build train-ing pairs such that a reranker can learn to select thebest one between the two hypotheses in a pair, i.e.the more correct hypothesis with respect to a refer-ence annotation and a given metric.
In particular,we compute the edit distance of each hypothesis inthe list, with respect to the manual annotation takenfrom the corpus.
The best hypothesis Hb is usedto build positive instances for the reranker as pairs(Hb, Hi) for i ?
[1..n] and i 6= b, negative instancesare built as (Hi, Hb), with same constraints on indexi.
This means that, if n hypotheses are generated fora sentence, 2 ?
n instances are generated from them.Note that by construction of pairs the model is sym-metric, this provides a property that will be exploitedat classification phase, as described in (Shen et al,2003b).Hypotheses are then converted into trees like theone shown in figure 1.
Pairs of trees ek = (ti,k, tj,k),for k varying along all the training or classificationinstances, are given as input to the SVM model totrain the reranker using the following reranking ker-nel:KR(e1, e2) = PTK(t1,1, t1,2) + PTK(t2,1, t2,2) (4)?
PTK(t1,1, t2,2)?
PTK(t2,1, t1,2),where e1 and e2 are two pairs of trees to be com-pared.The reranking kernel in equation 4, consisting insumming four different kernels, has been proposedin (Shen et al, 2003b) for syntactic parsing rerank-ing, where the basic kernel was a Tree Kernel, andthe idea was taken in turn from (Heibrich et al,2000), where pairs where used to learn preferenceranking.
The same idea appears also, in a slightlydifferent form, in early work about reranking, e.g.
(Collins and Duffy, 2002).
The same rerankingschema has been used also in (Shen et al, 2004)for reranking different candidate hypotheses for ma-chine translation.For classification, observing that the model issymmetric and exploiting kernel properties, we canuse, as classification instances, simple hypothesesinstead of pairs.
More precisely we use pairs wherethe second hypothesis is empty, i.e.
(Hi, 0), fori ?
[1..n].
This simplification allow a relatively fastclassification phase, since only n instances are gen-erated for each sentence, instead of n2.
This simpli-fication has been proposed in (Shen et al, 2003b).1108Figure 1: An example of semantic tree constructed from an SLU hypothesis from the MEDIA corpus and used in PTK4 Hypotheses Selection CriteriaThis section describes the main contribution of ourwork: first, a semantic inconsistency metric basedon the AVE phase of SLU and allowing to select hy-potheses generated by the baseline model; second, astrategy to decide, after the reranking phase, if it ismore likely that the baseline best hypothesis is moreaccurate than the best reranked hypothesis and al-lowing to recover the mistake.
Similar ideas havebeen proposed in (Dinarelli et al, 2010), here wepropose a significant evolution and we give a muchwider description and evaluation.4.1 Hypotheses Selection via Attribute ValueExtraction (AVE)In previous reranking approaches (Collins, 2000;Collins and Duffy, 2002; Shen et al, 2003a; Shenet al, 2003b; Shen et al, 2004; Collins and Koo,2005; Kudo et al, 2005; Dinarelli et al, 2009b), fewhypotheses are generated with the baseline model,ranked by the model probability.
These are thenused for the reranking model.
An interesting strat-egy to improve reranking performance is the selec-tion of the best set of hypotheses to be reranked.In this work we propose a semantic inconsistencymetric (SIM) based on the attribute-value extractionphase that allows to select better n-best hypotheses.We combine the scores provided by the rule basedapproach and the CRF approach for AVE, comput-ing a confidence measure.The rule-based approach for AVE is defined bya set of rules that map concepts and their realiz-ing words into the corresponding value.
The rulesare extracted from the training data, thus they aredefined to extract correct values from well formedphrases annotated with correct concepts.
This meansthat when the corresponding words are annotatedwith a wrong concept, the extracted value will prob-ably be wrong.
We use this property to compute asemantic inconsistency value for hypotheses, whichin turn allows to select hypotheses with higher prob-abilities to be correct.We show the application of SIM using the sameexample of Section 2.
For space issues we ab-breviate command-task with com-task, object-bdwith obj-bd and localization-city with loc-city.
Wealso suppose to have already removedNull concepts.From the same sentence, the three first hypothesesthat may be generated by the baseline model are:1. obj-bd{I would like to book} obj-bd{an hotel room} loc-city{in London}2. com-task{I would like to book} obj-bd{an hotel room} loc-city{in London}3. com-task{I would like to book} obj-bd{an hotel} obj-bd{room} loc-city{in London}Two of these annotations show typical errors of anSLU model:(i) wrong concepts annotation: in the first hypothe-sis the phrase ?I would like to book?
is erroneouslyannotated as obj-bd;(ii) wrong concept segmentation: in the third hy-pothesis the phrase ?an hotel room?
is split in twoconcepts.If we apply the AVE module to these hypothesesthe result is:1. obj-bd[] obj-bd[hotel] loc-city[london]2. cmd-task[reservation] obj-bd[hotel] loc-city[london]3. cmd-task[reservation] obj-bd[hotel] obj-bd[] loc-city[london]As we can see the first concept obj-bd in the firsthypothesis has an empty value since it was incor-rectly annotated and, therefore, it is not supported1109MEDIA training dev test# sentences 12,908 1,259 3,005words concepts words concepts words concepts# tokens 94,466 43,078 10,849 4,705 25,606 11,383# vocabulary 2,210 99 838 66 1,276 78# singletons 798 16 338 4 494 10# OOV rate [%] ?
?
1.33 0.02 1.39 0.04Table 1: Statistics of the MEDIA training and evaluation sets used for all experiments.by words from which the AVE module can extracta correct value.
In this case, the output of AVE isempty.
In the same way, in the third hypothesis, theAVE module cannot extract a correct value from thephrase ?room?
since it doesn?t contain any keywordfor a obj-bd concept.For each hypothesis, our SIM simply counts thenumber of wrong (or empty) values.
In the exampleabove, we have 1, 0 and 1 for the three hypothe-sis, respectively.
Accordingly, the most accurate hy-pothesis under SIM is the second, which is also thecorrect one.In order to combine the SIM score computed bythe rule-based AVE module with the score providedby the CRF AVE model, we consider per-conceptscores from both approaches.
In particular, we for-malize the definition of the SIM metric above on aconcept ci as SIM(ci, w1,...,mi ).
The value of SIMis simply 0 if the rule-based AVE module can extracta value from the surface form w1,...,mi realizing theconcept ci.
1 otherwise.
For each concept in a hy-pothesis, we compute its semantic consistency s(ci)ass(ci) =P (vi|ci, w1,...,mi )SIM(ci, w1,...,mi ) + 1(5)where P (vi|ci, w1,...,mi ) is the conditional prob-ability output by the CRF model for the value vi,given the concept ci and its realizing surfacew1,...,mi .Equation 5 means that the CRF score provided for agiven value is halved if SIM returns 1, i.e.
if theAVE module cannot extract any value.
Otherwisethe score output by the CRF AVE model is keptunchanged.
The semantic inconsistency metric ofan hypothesis Hk containing the concept sequenceCN1 = c1, ..., cN is then defined asS(Hk) =N?i=1s(ci) (6)Using S(Hk) as semantic inconsistency metric,we generate a huge number of hypotheses with thebaseline model and we select only the top n-best.
Weuse these hypotheses in the discriminative rerankingmodel, instead of the original n-best generated bythe CRF model.
For simplicity, in general contextwe denote S(Hk) as SIM.4.2 Wrong Rerank RejectionAfter the reranking model is applied, the first hy-pothesis is selected as final result.
This choice as-sumes that the new hypothesis is more accurate thanthe one provided by the baseline model.
In gen-eral this assumption is not true.
Indeed, a rerankingmodel must be carefully tuned in order to correctlyrerank wrong first best hypotheses but keeping theoriginal baseline best for correct hypotheses.
Whenthe baseline model is relatively accurate, the lattercase occurs in most of the cases.
In this situation itbecomes hard to train an accurate reranking model.Our idea to overcome this problem is to apply thereranking model and then post-process results to de-tect when the original best hypothesis is actually bet-ter than the reranked best.For this purpose we propose a simple strategybased on the scores computed by the two models in-volved in reranking: CRF for the baseline and SVMwith PTK for reranking.Let Hcrf and HRR be the best hypothesis of theCRF and reranking (RR) models, respectively.
LetScrf (Hcrf ) and Scrf (HRR) be the scores of theCRF model for Hcrf and HRR.
In the same way,let SRR(Hcrf ) and SRR(HRR) be the scores of thereranking model on the same hypotheses.
We definethe confidence margin of the CRF model the quan-tity: Mcrf = Scrf (Hcrf )?
Scrf (HRR).In the same way we define the confidence mar-gin of the RR model: MRR = SRR(Hcrf ) ?SRR(HRR).We compute two thresholds Tcrf and TRR for the1110Average score Feature type0.0528186 Pref20.044189 CATEGORY-20.0355579 CATEGORY0.0354006 Pref3-20.0338949 Pref4-20.0332647 Suff3-20.0314831 Suff20.030613 Suff4-2... ...0.0165602 Suff10.000579602 Pref1Table 2: Ranks of average score given by the CRF model to featuretypestwo margins with respect to error rate minimization(with a ?line search?
algorithm).We select the final best interpretation hypothesisfor a given sentence with the decision function:BestHypothesis ={ HRR if Mcrf ?
Tcrf and MRR ?
TRRHcrf otherwise.Since this strategy allows to recover from rerank-ing mistakes, we call it Wrong Rerank Rejection(WRR).5 ExperimentsThe data used in our experiments are taken fromthe French MEDIA corpus (Bonneau-Maynard etal., 2006).
The corpus is made of 1.250 Human-Machine dialogs acquired with a Wizard-of-Oz ap-proach in the domain of informtation and reservationof French hotels.
The data are split into training, de-velopment and test set.
Statistics of the corpus arepresented in table 1.For our CRFmodels, both Automatic Concept An-notation and Attribute Value Extraction SLU phases,we used wapiti2 (Lavergne et al, 2010).
The CRFmodel for the first SLU phase integrates a tradi-tional set of features like word prefixes and suffixes(of length up to 5), plus some Yes/No features like?Does the word start with capital letter ?
?, ?Doesthe word contain non alphanumeric characters ?
?,?Is the word preceded by non alphanumeric char-acteris ??
etc.
The CRF model for AVE integratesonly words, prefixes and suffixes (length 3 and 4)concatenated with concepts.
Since in this case la-bels are attribute values, which are a huge set with2available at http://wapiti.limsi.frMEDIA Text Input DEV TESTModel Attr Attr+Val Attr Attr+ValCRF 12.1% 14.8% 11.5% 13.8%CRF+RR 12.0% 14.6% 11.5% 13.7%CRF+RRSIM 11.7% 13.9% 11.3% 13.4%CRF+RRWRR 11.2% 13.4% 11.3% 13.0%Table 3: Results of baseline CRF model and reranking models onMEDIA text inputrespect to concepts (7?00 VS 99), using a lot of fea-tures would make model training problematic.
De-spite the reduced set of features, training error rateat both token and sentence level is under 1%.
Wedidn?t carry out optimization for parameters ?1 and?2 of the elastic net (see section 3.1), default valueslead in most cases to very accurate models.Reranking models based on SVM and PTK havebeen trained with ?SVM-Light-TK?3.
Kernel param-eters M and SVM parameter C have been optimizedon the development set, as well as thresholds for theWRR (see section 4.2).Concerning hypotheses generation, for trainingwe generate 100 hypotheses, we select the best withrespect to the edit distance and the reference anno-tation and we keep a total of 10 hypotheses to buildpairs.
For classification, with the ?standard?
rerank-ing approach we generate and we keep the 10 besthypotheses.
While using SIM for hypotheses selec-tion, we generate 1.000 hypotheses and we keep the10 best with respect to SIM.
1.000 is the best thresh-old between oracle accuracy and computational costfor evaluating the hypotheses.Experiments have been performed on both man-ual and automatic transcriptions of dialog turns.
Forautomatic transcriptions the WER of the ASR is30.3% on development set and 31.4% on test set.All results are reported in terms of Concept Er-ror Rate (CER), which is the same as WER, but it iscomputed on concept sequences.
In all cases we giveresults for both attributes only and attributes and val-ues extraction5.1 ResultsIn order to understand feature relevance, in table 2we report feature types ranked by the average scoregiven by the CRF model.
Each type correspond tofeatures at any position with respect to the target3available at http://disi.unitn.it/moschitti/Tree-Kernel.htm1111(a) M kernel parameter VS CER (onattribute-value extraction)(b) C SVM parameter VS training time (c) C SVM parameter VS CER (onattribute-value extraction)Figure 2: Optimization of the PTK M parameter and C parameter of SVMMEDIA Speech Input DEV TESTModel Attr Attr+Val Attr Attr+ValCRF 24.1% 29.1% 23.7% 27.6%CRF+RR 23.9% 29.1% 23.5% 27.6%CRF+RRSIM 23.9% 28.3% 23.2% 26.8%CRF+RRWRR 23.3% 27.5% 22.7% 26.1%Table 4: Results of baseline CRF model and reranking models onMEDIA speech inputword, with label unigrams.
In contrast observationunigrams are distinguished from bigrams using suf-fixes -1 and -2 respectively.
Feature types wrd arewords converted to lower case, Wrd are words keptwith original capitalization.
Feature types Pren areword prefixes of length n, Sufn are word suffixes oflength n. CATEGORY features are word categories(see section 3.3).
As we can see from the table,although feature relevance depends of course fromthe task, surprisingly word prefixes of length 2 arethe most meaningful features.
As expected, CATE-GORY features are also very relevant features, sincethey provide a strong generalization over words.
An-other expected outcome is the fact that prefixes andsuffixes of length 1 are the least relevant features.In figure 2(a), 2(b) and 2(c) we show the curvesresulting from optimization of parameters of rerank-ing models.
In particular we optimized the M kernelparameter (?
decay factor, see (Moschitti, 2006) fordetails), and the C SVM parameter, i.e.
the scalefactor for the soft margin (please refer to (Vapnik,1998) for SVM details).
Figure 2(b) shows the learn-ing time as a function of the C SVM parameter.
Thisgives an idea of how long takes training our rerank-ing models.In table 3 and 4 we report comparative resultsover the baseline CRF model, the baseline rerank-ing model (CRF+RR) and the reranking models ob-tained applying the two improvements proposed inthis work (CRF+RRSIM and CRF+RRWRR).
Aswe can see, the baseline reranking model does notimprove significantly the baseline CRF model.
Thisoutcome is expected since we don?t use any other in-formation in the reranking model than the semantictree shown in figure 1.
Previous approaches like forexample (Collins and Duffy, 2002), use the baselinemodel score as feature, as that the reranking modelcannot do worst than the baseline model.
As wepointed out in section 4.2, this solution require a finetuning of the reranking model, especially when thebaseline model is relatively accurate.
In our case,the CRF model has a Sentence Error Rate of 25.0%on the MEDIA test set.
This means that 75% ofthe times the best hypothesis of CRF is correct.
Inturn this implies that the reranking model must notrerank 75% of times and rerank the other 25% oftimes, someway contrasting the evidence providedby the baseline model score.
In contrast, using ourWRR strategy, we can tune the reranking model tomaximize reranking effect and recover from rerank-ing errors applying WRR.
As shown in tables 3 and4, we consistently improve CRF baseline as wellas reranking baseline CRF+RR, especially applyingboth SIM and WRR (CRF+RRWRR).
Comparingour results with those reported in (Hahn et al, 2010),we can see that our model reaches, and even im-1112MEDIA Test set OER[%] correct found/presentModelCRF 9.5 2359/2657CRF+RR 9.5 2375/2657CRF+RRSIM 7.5 2381/2758CRF+RRWRR 7.5 2444/2758Table 5: Analysis over 10-best hypotheses for CRF baseline and thereranking models showing the effect of hypotheses selectionMEDIA Text Input DEV TESTModel Pair Attr+Val Attr+ValCRF vs. CRF+RR 0.2235 0.4075CRF vs. CRF+RRSIM 0.0299 0.065CRF vs. CRF+RRWRR 0.0044 1.9998E-4CRF+RR vs. CRF+RRSIM 0.002 5.9994E-4CRF+RR vs. CRF+RRWRR 4.9995E-4 9.999E-5CRF+RRSIM vs. CRF+RRWRR 0.1355 0.0031Table 6: Significance tests on results of models described in thiswork.
The significance test is based on computationally-intensive ran-domizations as described in (Yeh and Church, 2000).proves in some cases, state-of-the-art performance.This is particularly meaningful since best results re-ported in (Hahn et al, 2010) are obtained combining6 different SLU models.In table 5 we report some statistics to show theeffect of SIM on the 10-best hypotheses list.
It isparticularly interesting to see that when hypothe-ses selection is applied, oracle error rate (OER)drops of 2% points from an already accurate OERof 9.5%.
This is reflected also by the number of ora-cles present in the 10-best list without applying andapplying SIM.
We pass from 2657 without SIM to2758 applying our hypotheses selection metric.Finally, in table 6 we report statistical signifi-cance tests over the models described in this work.We used the significance test described in (Yehand Church, 2000), it is based on computationally-intensive randomizations of data and tests the nullhypothesis, i.e.
the lower the score, the higher thestatistical significance of results difference.
Scoresin table 5 reflect results given in terms of CER.
Wecan see that when the difference between results issmall, this is not statistically significant, when thescore is above 0.05, the difference between the twocorresponding models is not significant.
We can thusconclude that the reranking model we propose, usinghypotheses selection and reranking errors recover,significantly improves baseline CRFmodel and ?tra-ditional?
reranking models.6 DiscussionAlthough the new ideas proposed in this paper areeffective and interesting, an important issue is theirapplicability to other tasks and domains.
In this re-spect, it is sufficient to note that our ideas comesfrom the multi-stage nature of the task and of theproposed reranking framework.
SLU is performedin two intertwined steps, since attribute values areextracted from syntactic chunks annotated with con-cept in the first step.
This allows to use the model forthe second step to validate the output of the first step,and vice versa, which is the principle of our hypothe-ses selection metric.
There are many other tasks,in NLP and in other domains, that can be modeledwith multiple steps and thus the same idea of ?val-idation?
of the output of one step with the other?smodel output can be applied.
An example is syntac-tic parsing, where in most cases parsing is performedupon POS tagging output.7 ConclusionsIn this paper we propose two improvements forreranking models to be integrated in a rerankingframework for Spoken Language Understanding.The reranking model is based on a CRF baselinemodel and Support Vector Machines with the Par-tial Tree Kernel for the reraning model.
The twoimprovements we propose are: i) hypotheses selec-tion criteria, used before applying reranking to selectbetter hypotheses amongst those generated by CRF.ii) a strategy to recover from reranking errors calledWrong Rerank Rejection.We presented a full set of comparative resultsshowing the viability of our approach.
We can reachperformances of state-of-the-art models, improvingthem in some cases, especially on automatic tran-scriptions coming from ASR (speech input).In particular, the effectiveness of hypotheses se-lection is shown reporting the improvement of theOracle Error Rate on the 10-best hypotheses list.AcknowledgmentsThis work has been funded by OSEO under theQuaero program.1113ReferencesErik F. Tjong Kim Sang and Fien De Meulder.
2003.Introduction to the conll-2003 shared task: language-independent named entity recognition.
In Proceedingsof the seventh conference on Natural language learn-ing at HLT-NAACL 2003 - Volume 4, pages 142?147,Morristown, NJ, USA.
Association for ComputationalLinguistics.Ellen M. Voorhees.
2001.
The trec question answeringtrack.
Nat.
Lang.
Eng., 7:361?378, December.X.
Carreras and Lluis Marquez.
2005.
Introduction tothe conll-2005 shared task: Semantic role labeling.R.
De Mori, F. Bechet, D. Hakkani-Tur, M. McTear,G.
Riccardi, and G. Tur.
2008.
Spoken language un-derstanding: A survey.
IEEE Signal Processing Mag-azine, 25:50?58.Sylvain Galliano, Guillaume Gravier, and MauraChaubard.
2009.
The ester 2 evaluation campaignfor the rich transcription of french radio broadcasts.In Proceedings of the International Conference ofthe Speech Communication Assosiation (Interspeech),Brighton, U.K.E.
Charniak and M. Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative reranking.
InProceedings of the Conference of the Association forComputational Linguistics (ACL), page 363370, AnnArbor, MI.Michael Collins and Terry Koo.
2005.
Discriminative re-ranking for natural language parsing.
ComputationalLinguistic (CL), 31(1):25?70.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedings ofthe Eighteenth International Conference on MachineLearning (ICML), pages 282?289, Williamstown,MA, USA, June.Brigitte Krenn and Christer Samuelsson.
1997.
The lin-guist?s guide to statistics - don?t panic.Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.2010.
Practical very large scale CRFs.
In Proceed-ings the 48th Annual Meeting of the Association forComputational Linguistics (ACL), pages 504?513.
As-sociation for Computational Linguistics, July.Stefan Hahn, Patrick Lehnen, Georg Heigold, and Her-mann Ney.
2009.
Optimizing crfs for slu tasks in vari-ous languages using modified training criteria.
In Pro-ceedings of the International Conference of the SpeechCommunication Assosiation (Interspeech), Brighton,U.K.Stefan Riezler and Alexander Vasserman.
2010.
Incre-mental feature selection and l1 regularization for re-laxed maximum-entropy modeling.Hui Zou and Trevor Hastie.
2005.
Regularization andvariable selection via the Elastic Net.
Journal of theRoyal Statistical Society B, 67:301?320.Andrew McCallum.
2003.
Efficiently inducing featuresof conditional random fields.
In 19th Conference onUncertainty in Artificial Intelligence.Lawrence R. Rabiner.
1989.
A tutorial on hidden markovmodels and selected applications in speech recogni-tion.
Proceedings of the IEEE, 77(2):257?286.Mark Johnson.
1998.
Pcfg models of linguistic tree rep-resentations.
Computational Linguistics, 24:613?632.Olivier Galibert, Ludovic Quintard, Sophie Rosset, PierreZweigenbaum, Claire Ndellec, Sophie Aubin, Lau-rent Gillard, Jean-Pierre Raysz, Delphine Pois, XavierTannier, Louise Delger, and Dominique Laurent.2010.
Named and specific entity detection in var-ied data: The quro named entity baseline evalu-ation.
In Nicoletta Calzolari (Conference Chair),Khalid Choukri, Bente Maegaard, Joseph Mariani,Jan Odijk, Stelios Piperidis, Mike Rosner, and DanielTapias, editors, Proceedings of the Seventh conferenceon International Language Resources and Evaluation(LREC?10), Valletta, Malta, may.
European LanguageResources Association (ELRA).Olivier Galibert.
2009.
Approches et me?thodologies pourla re?ponse automatique a` des questions adapte?es uncadre interactif en domaine ouvert.
Ph.D. thesis, Uni-versit Paris Sud, Orsay.G.
Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,S.
Strassel, and R. Weischedel.
2004.
The AutomaticContent Extraction (ACE) Program?Tasks, Data, andEvaluation.
Proceedings of LREC 2004, pages 837?840.He?le`ne Bonneau-Maynard, Christelle Ayache, F. Bechet,A Denis, A Kuhn, Fabrice Lefe`vre, D. Mostefa,M.
Qugnard, S. Rosset, and J. Servan, S. Vilaneau.2006.
Results of the french evalda-media evaluationcampaign for literal understanding.
In LREC, pages2054?2059, Genoa, Italy, May.Christian Raymond, Frdric Bchet, Renato De Mori, andGraldine Damnati.
2006.
On the use of finite statetransducers for semantic interpretation.
Speech Com-munication, 48(3-4):288?304, March-April.Christian Raymond and Giuseppe Riccardi.
2007.
Gen-erative and discriminative algorithms for spoken lan-guage understanding.
In Proceedings of the Interna-tional Conference of the Speech Communication As-sosiation (Interspeech), pages 1605?1608, Antwerp,Belgium, August.Vladimir N. Vapnik.
1998.
Statistical Learning Theory.John Wiley and Sons.T.
J. Sejnowski and C. S. Rosenberg.
1987.
Parallel net-works that learn to pronounce English text.
ComplexSystems, 1:145?168.1114Stefan Hahn, Marco Dinarelli, Christian Raymond, Fab-rice Lefe`vre, Patrick Lehen, Renato De Mori, Alessan-dro Moschitti, Hermann Ney, and Giuseppe Riccardi.2010.
Comparing stochastic approaches to spokenlanguage understanding in multiple languages.
IEEETransactions on Audio, Speech and Language Pro-cessing (TASLP), 99.Marco Dinarelli, Alessandro Moschitti, and GiuseppeRiccardi.
2009b.
Re-ranking models based on smalltraining data for spoken language understanding.
InConference of Empirical Methods for Natural Lan-guage Processing, pages 11?18, Singapore, August.Marco Dinarelli, Alessandro Moschitti, and GiuseppeRiccardi.
2010.
Hypotheses Selection for RerankingSemantic Annotation.
In IEEE Workshop of SpokenLanguage Technology (SLT), Berkeley, USA.Alessandro Moschitti.
2006.
Efficient Convolution Ker-nels for Dependency and Constituent Syntactic Trees.In Proceedings of ECML 2006, pages 318?329, Berlin,Germany.M.
Collins and N. Duffy.
2002.
New Ranking Algo-rithms for Parsing and Tagging: Kernels over Discretestructures, and the voted perceptron.
In Proceedings ofthe Association for Computational Linguistics, pages263?270.Libin Shen, Anoop Sarkar, and Aravind K. Joshi.
2003.Using LTAG Based Features in Parse Reranking.
InProceedings of EMNLP?06.Herbrich, Ralf and Graepel, Thore and Obermayer,Klaus.
2000.
Large Margin Rank Boundaries for Or-dinal Regression.
In Advances in Large Margin Clas-sifiers.Libin Shen, and Aravind K. Joshi.
2003.
An SVM BasedVoting Algorithm with Application to Parse Rerank-ing.
In Proceedings of CoNLL 2003.Libin Shen, Anoop Sarkar, and Franz J. Och.
2004.
Dis-criminative reranking for machine translation.
In HLT-NAACL, pages 177?184.Taku Kudo, Jun Suzuki, and Hideki Isozaki.
2005.Boosting-based parse reranking with subtree features.In Proceedings of ACL?05.Stefan Hahn, Patrick Lehnen, and Hermann Ney.
2008a.System combination for spoken language understand-ing.
In Proceedings of the International Conference ofthe Speech Communication Assosiation (Interspeech),pages 236?239, Brisbane, Australia.J.
G. Fiscus.
1997.
A post-processing system to yieldreduced word error rates: Recogniser output voting er-ror reduction (ROVER).
In Proceedings 1997 IEEEWorkshop on Automatic Speech Recognition and Un-derstanding (ASRU), pages 347?352, Santa Barbara,CA, December.John Shawe-Taylor and Nello Cristianini.
2004.
KernelMethods for Pattern Analysis.
Cambridge UniversityPress.Michael Collins.
2000.
Discriminative reranking for nat-ural language parsing.
In ICML, pages 175?182.Michael Collins and Nigel Duffy.
2001.
Convolutionkernels for natural language.
In Advances in NeuralInformation Processing Systems 14, pages 625?632.MIT Press.Rush, Alexander M. and Sontag, David and Collins,Michael and Jaakkola, Tommi.
2010.
On dual decom-position and linear programming relaxations for nat-ural language processing.
In Empirical Methods forNatural Language Processing (EMNLP).
Cambridge,Massachusetts, USA.Alessandro Moschitti, Daniele Pighin, and RobertoBasili.
2008.
Tree kernels for semantic role labeling.Computational Linguistics, 34(2):193?224.Alessandro Moschitti, Silvia Quarteroni, Roberto Basili,and Suresh Manandhar.
2007.
Exploiting syntacticand shallow semantic kernels for question/answer clas-sification.
In Proceedings of ACL?07, Prague, CzechRepublic.Alexander Yeh and Kelmeth Church.
2000.
More accu-rate tests for the statistical significance of result differ-ences.1115
