Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),pages 80?88, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational LinguisticsRecognizing Arguing Subjectivity and Argument TagsAlexander Conrad, Janyce Wiebe, and Rebecca HwaDepartment of Computer ScienceUniversity of PittsburghPittsburgh PA, 15260, USA{conrada,wiebe,hwa}@cs.pitt.eduAbstractIn this paper we investigate two distincttasks.
The first task involves detecting ar-guing subjectivity, a type of linguistic sub-jectivity on which relatively little work hasyet to be done.
The second task involveslabeling instances of arguing subjectivitywith argument tags reflecting the concep-tual argument being made.
We refer tothese two tasks collectively as ?recogniz-ing arguments?.
We develop a new anno-tation scheme and assemble a new anno-tated corpus to support our learning ef-forts.
Through our machine learning ex-periments, we investigate the utility of asentiment lexicon, discourse parser, andsemantic similarity measures with respectto recognizing arguments.
By incorpo-rating information gained from these re-sources, we outperform a unigram baselineby a significant margin.
In addition, we ex-plore a two-phase approach to recognizingarguments, with promising results.1 IntroductionSubjectivity analysis is a thriving field withinnatural language processing.
However, mostresearch into subjectivity has focused on sen-timent with respect to concrete things suchas product debates (e.g., (Somasundaran andWiebe, 2009), (Yu et al, 2011)) and movie re-views (e.g., (He et al, 2011), (Maas et al, 2011),(Pang and Lee, 2004)).
Analysis often followsthe opinion-target paradigm, in which expres-sions of sentiment are assessed with respect tothe aspects of the object(s) under considerationtowards which they are targeted.
For example,in the domain of smartphone reviews, aspectscould include product features such as the key-board, screen quality, and battery life.Although sentiment analysis is interestingand important in its own right, this paradigmdoes not seem to be the best match for fine-grained analysis of ideological domains.
Whilesentiment is also present in documents fromthis domain, previous work (Somasundaran andWiebe, 2010) has found that arguing subjec-tivity, a less-studied form of subjectivity, ismore frequently employed and more relevantfor a robust assessment of ideological positions.Whereas sentiment conveys the polarity of awriter?s affect towards a topic, arguing subjec-tivity is a type of linguistic subjectivity in whicha person expresses a controversial belief aboutwhat is true or what action ought to be takenregarding a central contentious issue (Somasun-daran, 2010).
For example, consider this sen-tence about health care reform:(1) Almost everyone knows that wemust start holding insurance compa-nies accountable and give Americans agreater sense of stability and securitywhen it comes to their health care.In a traditional opinion-target or sentiment-topic paradigm, perhaps this sentence could belabeled as containing a negative sentiment to-wards a topic representing ?insurance compa-nies?, or a positive sentiment towards a topicrepresenting ?stability?
or ?security?.
However,a reader of a political editorial or blog may bemore interested in why the author is negative to-80wards insurers, and how the author proposes toimprove stability of the healthcare system.
Byfocusing on the arguments conveyed through ar-guing subjectivity, we aim to capture these kindof conceptual reasons an author provides whenarguing for his or her position.However, identifying when someone is arguingis only part of the challenge.
Since arguing sub-jectivity is used to express arguments, the nextnatural step is to identify the argument beingexpressed through each instance of arguing sub-jectivity.
To illustrate this distinction, considerthe following three example spans:(2) the bill is a job destroyer(3) President Obamas signature do-mestic policy will throw 100,000 peo-ple out of work come January(4) he can?t expand his business be-cause he can?t afford the burden ofObamacareEach of these examples contains arguingsubjectivity, but more importantly, each ex-presses roughly the same idea, namely, that therecently-passed health care reform bill will causeeconomic harm.
This latent, shared idea givingrise to each of the three spans is what we meanby ?argument tag?.However, although all three are related, exam-ple spans (2) and (3) are more similar than (4)in terms of the notions they convey: while thefirst two explicitly are concerned with the lossof jobs, the last focuses on business expansionand the economy as a whole.
If we were to tagthese three spans with respect to the argumentthat each is making, should they all receive thesame tag, or should (4)?s tag be different?To address these challenges, we propose in thiswork a new annotation scheme for identifyingarguing subjectivity and a hierarchical model fororganizing ?argument tags?.
In our hierarchicalmodel, (4) would receive a different tag from (2)and (3), but because of the tags?
relatedness allwould share the same parent tag.In addition to presenting this new scheme forlabeling arguing subjectivity, we also exploresentiment, discourse, and distributional similar-ity as tools to enhance identification and classi-fication of arguing subjectivity.
Finally, we alsoinvestigate splitting the arguing subjectivity de-tection task up into two distinct phases: iden-tifying expressions of arguing subjectivity, andlabelling each such expression with an appropri-ate argument tag.Since no corpora annotated for arguing sub-jectivity yet exist, we gather and annotate a cor-pus of blog posts and op-eds about a contro-versial topic, namely, the recently-passed ?Oba-maCare?
health care reform bill.2 Annotation SchemeWe designed our annotation scheme with twogoals in mind: identifying all spans of text whichexpress arguing subjectivity, and labelling eachsuch span with an argument tag.
To addressthe first goal, our annotators manually identifiedand annotated spans of text containing arguingsubjectivity using the GATE environment1.
An-notators were instructed to identify spans of 1sentence or less in which a writer ?conveys acontroversial private state concerning what shebelieves to be true or what action she believesshould be taken?
concerning the health care re-form debate.
To train our annotators to recog-nize arguing subjectivity, we performed severalrounds of practice on a separate dataset.
Be-tween each round, our annotators met to discusstheir annotations and resolve disagreements.As a heuristic to help distinguish between bor-derline sentences, we advised our annotators toimagine disputants from each side writing thesentence in isolation.
If a disputant from eitherside could conceivably write the sentence, thenthe sentence is likely objective.
For example,statements of accepted facts and statistics gen-erally fall into this category.
However, if onlyone side could conceivably be the author of thesentence, it is highly likely that the sentence ex-presses a controversial belief relevant to the de-bate and thus should be labeled as subjective.Next, the annotators labeled each arguingspan with an argument tag.
As illustrated inearlier examples, an argument tag represents a1http://gate.ac.uk/81controversial abstract belief expressed througharguing subjectivity.
Since the meanings ofmany tags may be related, we organize thesetags in a hierarchical ?stance structure?.
Astance structure is a tree-based data structurecontaining all of the argument tags associatedwith a particular debate, organizing those tagsusing ?is-a?
relationships.
Our stance structurecontains two levels of argument tags: upper-level ?primary?
argument tags and lower-level?secondary?
tags.
Each primary tag has one ofthe stances (either ?pro?
or ?anti?
in our case)as its parent, while each secondary tag has aprimary tag as its parent2.Political science ?arguing dimension?
ap-proaches to debate framing analysis served, inpart, as an inspiration for our stance structure(Baumgartner et al, 2008).
Also, as illustratedin Section 1, this approach permits us additionalflexibility, supporting classification at differentlevels of specificity depending on the task athand and the amount of data available.
We en-vision a future scenario in which a community ofusers collaboratively builds a stance structure torepresent a new topic or debate, or in which an-alysts build a stance structure to categorize theissues expressed towards a proposed law, suchas in the context of e-rulemaking (Cardie et al,2008).Because each stance contains a large numberof argument tags, we back-off from each sec-ondary argument tag to its primary argumentparent for the classification experiments.
Wechose to do this in order to ensure that we havea sufficient amount of data with which to trainthe classifier.3 DatasetFor this study, we chose to focus on online ed-itorials and blog posts concerning the ongoingdebate over health insurance reform legislationin the United States.
Our intuition is that blogsand editorials represent a genre rich in both2Our stance structure contains an additional ?aspect?level consisting of a-priori categories adopted from politi-cal science research.
However, we do not utilize this levelof the stance structure in this work.?pro?
documents 37?pro?
sentences 1,222?anti?
documents 47?anti?
sentences 1,456total documents 84total sentences 2,678Table 1: Dataset summary statistics.arguing subjectivityobjective 683subjective 588argument labelsno label 683improves healthcare access 130improves healthcare affordability 104people dont know truthabout bill75controls healthcare costs 54improves quality of healthcare 52helps economy 51bill should be passed 43other argument 79Table 2: Arguing and argument label statistics forthe ?pro?
stance.subjectivity and arguments.
We collected docu-ments written both before and after the passageof the final ?Patient Protection and AffordableCare Act?
bill using the ?Google Blog Search?3and ?Daily Op Ed?4 search portals.
By choosinga relatively broad time window, from early 2009to late 2011, we aimed to capture a wide rangeof arguments expressed throughout the debate.The focus of this paper is on sentence-levelargument detection rather than document-levelstance classification (e.g., (Anand et al, 2011),(Park et al, 2011), (Somasundaran and Wiebe,2010), (Burfoot et al, 2011)).
We treat stanceclassification as a separate step preceding argu-ing subjectivity detection, and thus provide or-acle stance labels for our data.We treat documents written from the ?pro?3http://www.google.com/blogsearch4http://www.dailyoped.com/82arguing subjectivityobjective 913subjective 575argument labelsno label 913diminishes quality of care 122too expensive 67unpopular 60hurts economy 55expands govt 52bill is politically motivated 44other reforms more appropriate 35other argument 140Table 3: Arguing and argument label statistics forthe ?anti?
stance.stance and documents written from the ?anti?stance as separate datasets.
Being written fromdifferent positions, the two stances will have dif-ferent argument labels and may employ differentstyles of arguing subjectivity.
Table 1 providesan overview of the size of this dataset.
Summarystatistics concerning the density of arguing andargument labels in the two sides of the datasetis presented in Tables 2 and 3.
However, sinceit can be difficult to summarize a complex ar-gument in a short phrase, many of these labelsby themselves do not clearly convey the meaningthey are meant to represent.
To better illustratethe meanings of some of the more ambiguous la-bels, Table 4 presents several annotated examplespans for some of the more unclear ambiguousargument labels.4 Agreement StudyOne of our authors performed annotation of ourcorpus, the broad outlines of which are sketchedin the previous section.
However, to assess inter-annotator agreement for this annotation scheme,we recruited a non-author to independently an-notate a subset of our corpus consisting of 384sentences across 10 documents.
This non-authorboth identified spans of arguing subjectivity andassigned argument tags.
She was given a stancestructure from which to select argument tags.improves healthcare access?Our reform will prohibit insurance compa-nies from denying coverage because of yourmedical history.?
?Let?s also not overlook the news from lastweek about the millions of younger Americanswho are getting coverage thanks to consumerprotections that are now in place.
?improves healthcare affordability?
new health insurance exchanges will offercompetitive, consumer-centered health insur-ance marketplaces...?
?Millions of seniors can now afford medicationthey would otherwise struggle to pay for.
?people dont know truth about bill?...the cynics and the naysayers will continueto exploit fear and concerns for political gain.?
?Republican leaders, who see opportunitiesto gain seats in the elections, have madeclear that they will continue to peddle fictionsabout a government takeover of the healthcare system and about costs too high to bear.
?unpopular?The 1,000-page monstrosity that emerged invarious editions from Congress was done in bywidespread national revulsion...?
?Support for ObamaCare?s repeal is broad,and includes one group too often overlookedduring the health care debate: America?s doc-tors.
?expands govt?...the real goal of the health care overhaulwas to enact the largest entitlement programin history...?
?the new bureaucracy the health care legisla-tion creates is so complex and indiscriminatethat its size and cost is ?currently unknow-able.?
?bill is politically motivated?...tawdry backroom politics were used to selloff favors in exchange for votes.?
?From the wildly improper gifts to senatorslike Nebraska?s Ben Nelson to this week?sbackroom deals for unions...?Table 4: Example annotated spans for several argu-ment labels.83metric recall precision f-measureagr 0.677 0.690 0.683kappa for overlapping annotations 0.689Table 5: Inter-annotator span agr (top) and argu-ment label kappa on overlapping spans (bottom).In assessing inter-annotator agreement on thissubset of the corpus, we must address two levelsof agreement, arguing spans and argument tags.At first glance, how to assess agreementof annotated arguing spans is not obvious.Because our annotation scheme did not enforcestrict boundaries, we hypothesized that bothannotators would both frequently see an in-stance of arguing subjectivity within a localregion of text, but would disagree with respectto where the arguing begins and ends.
Thus, weadopt from (Wilson and Wiebe, 2003) the agrdirectional agreement metric to measure thedegree of annotation overlap.
Given two setsof spans A and B annotated by two differentannotators, this metric measures the fractionof spans in A which at least partially overlapwith any spans in B.
Specifically, agreement iscomputed as:agr(A B) = A matching BAWhen A is the gold standard set of annota-tions, agr is equivalent to recall.
Similarly, whenB is the gold standard, agr is equivalent to pre-cision.
For this evaluation, we treat the datasetannotated by our primary annotator as the goldstandard.
Table 5 presents these agr scores andf-measures for the arguing spans.Second, we measure agreement with respectto the argument tags assigned by the two an-notators.
Continuing to follow the methodol-ogy of (Wilson and Wiebe, 2003), we look ateach pair of annotations, one from each anno-tator, which share at least a partial overlap.For each such pair, we assess whether the twospans share the same primary argument tag.Scores for primary argument label agreement interms of Cohen?s kappa are also presented in Ta-ble 5.
Since this kappa score falls within therange of 0.67 ?
K ?
0.8, according to Krippen-dorf?s scale (Krippendorff, 2004) this allows usto draw tentative conclusions concerning a sig-nificant level of tag agreement.5 MethodsAs discussed earlier, recognizing arguments canbe thought of in terms of two related but dif-ferent tasks: recognizing a type of subjectivity,and labeling instances of that subjectivity withtags.
We refer to the binary arguing subjectiv-ity detection task as ?arg?, and to the multi-class argument labeling task as ?tag?.
For the?tag?
task, we create eight classes: one for eachof the seven most-frequent labels, and an eighthinto which we agglomerate the remaining less-frequent labels.
We only consider the sentencesknown to be subjective (via oracle information)for the ?tag?
task.We also perform a ?combined?
task.
Thisthird task is conceptually similar to the ?tag?task, except that all sentences are consideredrather than only the subjective sentences.
In ad-dition to the eight classes used by ?tag?, ?com-bined?
adds an additional class for non-arguingsentences.
Finally, we also perform a two-stage?arg+tag?
task.
In this two-stage task, the in-stances labeled as subjective by the ?arg?
clas-sifier are passed as input to the ?tag?
classifier.The intuition behind this two-phase approach isthat the features most useful for identifying ar-guing subjectivity may not be the most usefulfor discriminating between argument tags, andvice versa.
For all of our classification tasks,we treat both the ?pro?
and ?anti?
stancesseparately, building separate classifiers for eachstance for each of the above tasks.In general, we perform single-label classifi-cation at the sentence level.
However, sen-tences containing multiple labels pose a chal-lenge.
Since this was an early exploratory workon a very difficult task, we decided to handlethis situation by splitting sentences containingmultiple labels into separate instances for thepurpose of learning, assigning a single label toeach instance.
However, only about 3% of thesentences in our corpus contained multiple la-84bels.
Thus, replacing this splitting step in thefuture with another method that does not re-quire oracle information, such as choosing thelabel which covers the most words in the sen-tence, is a reasonable simplification of the task.Since discourse actions, such as contrasting,restating, and identifying causation, play a sub-stantial role in arguing, we hypothesize that in-formation about the discourse roles played bya span of text will help improve classification.Although discourse parsers historically haven?tbeen found to be effective for subjectivity anal-ysis, a new parser (Lin et al, 2010) trained onthe Penn Discourse TreeBank (PDTB) tagset(Prasad et al, 2008) has recently been released.Previous work has demonstrated that this parsercan reliably detect discourse relationships be-tween adjacent sentences (Lin et al, 2011), andthe PDTB tagset, being relatively flat, is con-ducive to feature engineering for our task.To give a feeling for the kind of discourse re-lations identified by this parser, the followingexample illustrates a concession relation identi-fied in the corpus by the parser.
The italicizedtext represents the concession, while the boldedtext indicates the overall point that the authoris making.
The underlined word was identifiedby the parser as an explicit concessionary clue.
(7) the health care reform legisla-tion that President Obama now seemslikely to sign into law , while anunlovely mess , will be rememberedas a landmark accomplishment .Using this automatic information, we definefeatures indicating the discourse relationships bywhich the instance is connected to surroundingtext.
Specifically, the class of discourse rela-tionship connecting the target instance to theprevious instance, the relationship connecting itto the following instance, and any internal dis-course relationships by which the parts of theinstance are connected to each other are eachadded as features.
Since PDTB contains manyfine-grained discourse relations, we replace eachdiscourse relationship type inferred by the dis-course parser with the parent top-level PDTBdiscourse relationship class.
We arrive at a totalof 15 binary discourse relationship features: (4top-level classes + ?other?)
x (connects to pre-vious + connects to following + internal connec-tion) = 15.
We refer to these features as ?rels?.As illustrated in our earlier examples, whilearguing subjectivity is different from sentiment,the two types of subjectivity are often related.Thus, we investigate incorporating sentimentinformation based on the presence of unigramclues from a publically-available sentiment lexi-con5 (Wilson, 2005).
Each clue in the lexicon ismarked as being either ?strong?
or ?weak?.We found that this lexicon was producingmany false hits for positive sentiment.
Thus, aspan containing a minimum of two positive cluesof which at least one is marked as ?strong?, orthree positive ?weak?
clues, is augmented with afeature indicating positive sentiment.
For nega-tive sentiment the threshold is slightly lower, atone ?strong?
clue or two ?weak?
clues.
Thesefeatures are referred to as ?senti?.A challenge to argument tag assignment is thebroad diversity of language through which in-dividual entities or specific actions may be ref-erenced, as illustrated in Examples (2-4) fromSection 1.
To address this problem, we in-vestigate expanding each instance with termsthat are most similar, according to a distribu-tional model generated from Wikipedia articles,to the nouns and verbs present within the in-stance (Pantel et al, 2009).
We refer to thesefeatures as ?expn?, where n is the number ofmost-similar terms with which to expand the in-stance for each noun or verb.
We experimentwith values of n = 5 and n = 10.Subjectivity classification of small units oftext, such as individual microblog posts (Jianget al, 2011) and sentences (Riloff et al, 2003),has been shown to benefit from additional con-text.
Thus, we augment the feature representa-tion of each target sentence with features fromthe two preceding and two following sentences.These additional features are modified so thatthey do not fall within the same feature space5downloaded from http://www.cs.pitt.edu/mpqa/subj_lexicon.html85feat.abbrev.elaborationunigramsenti 2 binary features indicating posi-tive or negative sentiment based onpresence of lexicon cluesrels 15 binary features indicating kindsof discourse relationships and howthey connect instance to surround-ing textexp5 for each noun and verb in instance,expand instance with top 5 mostdistributionally similar wordsexp10 for each noun and verb in instance,expand instance with top 10 mostdistributionally similar wordsTable 6: Overview of features used in the arguingand argument experiments.as the features representing the target sentence.Using the Naive Bayes classifier within theWEKA machine learning toolkit (Hall et al,2009), we explore the impact of the features de-scribed above on our four experiment configu-rations.
We perform our experiments using k-fold cross-validation, where k equals the num-ber of documents within the stance.
The testset for each fold consists of a single document?sinstances.
For the ?pro?
dataset k = 37, whilefor the ?anti?
dataset k = 47.6 ResultsTable 7 presents the accuracy scores from each ofour stand-alone classifiers across combinationsof feature sets.
Each feature set consists ofunigrams augmented with the designated addi-tional features, as described in Section 5.
Toevaluate the ?tag?
classifier in isolation, we useoracle information to provide this classifier withonly the subjective instances.
To assess signif-icance of the performance differences betweenfeature sets, we used the Pearson Chi-squaredtest with Yates continuity correction.Expansion of nouns and verbs withdistributionally-similar terms (?exp5?, ?exp10?
)plays the largest role in improving classifierfeatures arg tag comb.unigram baseline 0.610 0.425 0.458senti 0.614 0.426 0.459rels 0.614 0.422 0.462senti, rels 0.618 0.424 0.465exp5 0.635 0.522 0.482exp5, senti 0.638 0.515 0.486exp5, rels 0.640 0.522 0.484exp5, senti, rels 0.643 0.516 0.484exp10 0.645 0.517 0.488exp10, senti 0.647 0.515 0.489exp10, rels 0.642 0.512 0.490exp10, senti, rels 0.644 0.513 0.490Table 7: Classifier accuracy for differing feature sets.Significant improvement (p < 0.05) over baseline isboldfaced (0.05 < p < 0.1 italicized).
Underline in-dicates best performance per column.performance.
While differences between con-figurations using ?exp5?
versus ?exp10?
weregenerally not significant, all of the configu-rations incorporating some version of termexpansion outperformed the unigram baselineby either a statistically significant margin(p < 0.05) or by a margin that approachedsignificance (0.05 < p < 0.1).Sentiment features consistently produce im-provements in accuracy for the ?arg?
and ?com-bined?
tasks.
While these improvements arepromising, the lack of a significant margin of im-provement when incorporating sentiment is sur-prising.
Since sentiment lexicons are known tobe highly domain-dependent (Pan et al, 2010),it may be the case that, having been learnedfrom a general news corpus, the sentiment lexi-con employed in this work is not the best matchfor the domain of ?ObamaCare?
blogs and edito-rials.
Similarly, the discourse features also fail toproduce significant improvements in accuracy.Finally, we aim to test our hypothesis thatseparating the ?arg?
and ?tag?
phases results inimprovement beyond treating the two in a single?combined?
phase.
The first step of our hierar-chy involves normal classification of all sentencesusing the ?arg?
classifier.
Next, all sentencesjudged to contain arguing subjectivity by ?arg?86arg features tag features acc.exp5, senti, relsexp5 0.506exp5, rels 0.506exp10 0.501exp10exp5 0.514exp5, rels 0.513exp10 0.512exp10, sentiexp5 0.514exp5, rels 0.513exp10 0.512Table 8: Accuracies of two-stage classifiers across dif-ferent combinations of feature sets for the ?arg?
and?tag?
phases.
Italics indicate improvement over thetop ?combined?
configuration which approaches sig-nificance (0.05 < p < 0.1).
Underline indicates bestoverall performance.are passed to the ?tag?
classifier to have an ar-gument tag assigned.
We choose three promis-ing feature sets for the ?arg?
and ?tag?
phases,based on best performance in isolation.Results of this hierarchical experiment arepresented in Table 8.
We evaluate the hi-erarchical system against the best-performing?combined?
single-phase systems from Table 7.While all of the hierarchical configurations beatthe best ?combined?
classifier, none beats thetop combined classifier by a significant margin,although the best configurations approach sig-nificance (0.05 < p < 0.1).7 Related WorkMuch recent work in ideological subjectivitydetection has focused on detecting a writer?sstance in domains of varying formality, such asonline forums, debating websites, and op-eds.
(Anand et al, 2011) demonstrates the usefulnessof dependency relations, LIWC counts (Pen-nebaker et al, 2001), and information about re-lated posts for this task.
(Lin et al, 2006) ex-plores relationships between sentence-level anddocument-level classification for a stance-likeprediction task.Among the literature on ideological subjectiv-ity, perhaps most similar to our work is (Soma-sundaran and Wiebe, 2010).
This paper investi-gates the impact of incorporating arguing-basedand sentiment-based features into binary stanceprediction for debate posts.
Also closely relatedto our work is (Somasundaran et al, 2007).
Tosupport answering of opinion-based questions,this work investigates the use of high-precisionsentiment and arguing clues for sentence-levelsentiment and arguing prediction.Another active area of related research focuseson identifying important aspects towards whichsentiment is expressed within a domain.
(Heet al, 2011) approaches this problem throughtopic modeling, extending the joint sentiment-topic (JST) model which aims to simultaneouslylearn sentiment and aspect probabilities for aunit of text.
(Yu et al, 2011) takes a differentapproach, investigating thesaurus methods forlearning aspects based on groups of synonymousnouns within product reviews.8 ConclusionIn this paper, we explored recognizing argu-ments in terms of arguing subjectivity and ar-gument tags.
We presented and evaluated anew annotation scheme to capture arguing sub-jectivity and argument tags, and annotated anew dataset.
Utilizing existing sentiment, dis-course, and distributional similarity resources,we explored ways in which these three formsof knowledge could be used to enhance argu-ment recognition.
In particular, our empiricalresults highlight the important role played bydistributional similarity in all phases of detect-ing arguing subjectivity and argument tags.
Wehave also provided tentative evidence suggestingthat addressing the problem of recognizing argu-ments in two separate phases may be beneficialto overall classification accuracy.9 AcknowledgmentsThis material is based in part upon work sup-ported by National Science Foundation award#0916046.
We would like to thank Patrick Pan-tel for sharing his thesaurus of distributionallysimilar words from Wikipedia with us, AmberBoydstun for insightful conversations about de-bate frame categorization, and the anonymousreviewers for their useful feedback.87ReferencesPranav Anand, Marilyn Walker, Rob Abbott,Jean E. Fox Tree, Robeson Bowmani, and MichaelMinor.
2011.
Cats rule and dogs drool!
: Classi-fying stance in online debate.
In WASSA, pages1?9, Portland, Oregon, June.F.R.
Baumgartner, S.D.
Boef, and A.E.
Boydstun.2008.
The decline of the death penalty and the dis-covery of innocence.
Cambridge University Press.Clinton Burfoot, Steven Bird, and Timothy Bald-win.
2011.
Collective classification of congres-sional floor-debate transcripts.
In ACL, pages1506?1515, Portland, Oregon, USA, June.Claire Cardie, Cynthia Farina, Adil Aijaz, MattRawding, and Stephen Purpura.
2008.
A study inrule-specific issue categorization for e-rulemaking.In DG.O, pages 244?253.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: an update.SIGKDD Explor.
Newsl., 11:10?18, November.Yulan He, Chenghua Lin, and Harith Alani.
2011.Automatically extracting polarity-bearing topicsfor cross-domain sentiment classification.
In ACL,pages 123?131, Portland, Oregon, USA, June.Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, andTiejun Zhao.
2011.
Target-dependent twittersentiment classification.
In ACL, pages 151?160,Portland, Oregon, USA, June.K.
Krippendorff.
2004.
Content analysis: an intro-duction to its methodology.
Sage.Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, andAlexander Hauptmann.
2006.
Which side are youon?
: identifying perspectives at the document andsentence levels.
In CoNLL, pages 109?116.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.A pdtb-styled end-to-end discourse parser.
CoRR.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.Automatically evaluating text coherence using dis-course relations.
In ACL, pages 997?1006, Port-land, Oregon, USA, June.Andrew L. Maas, Raymond E. Daly, Peter T. Pham,Dan Huang, Andrew Y. Ng, and ChristopherPotts.
2011.
Learning word vectors for sentimentanalysis.
In ACL, pages 142?150, Portland, Ore-gon, USA, June.Sinno Jialin Pan, Xiaochuan Ni, Jian tao Sun, QiangYang, and Zheng Chen.
2010.
Cross-domain senti-ment classification via spectral feature alignment.In WWW.Bo Pang and Lillian Lee.
2004.
A sentimentaleducation: Sentiment analysis using subjectivitysummarization based on minimum cuts.
In ACL,pages 271?278, Barcelona, Spain, July.Patrick Pantel, Eric Crestan, Arkady Borkovsky,Ana-Maria Popescu, and Vishnu Vyas.
2009.Web-scale distributional similarity and entity setexpansion.
In EMNLP, pages 938?947, Morris-town, NJ, USA.Souneil Park, Kyung Soon Lee, and Junehwa Song.2011.
Contrasting opposing views of news arti-cles on contentious issues.
In ACL, pages 340?349,Portland, Oregon, USA, June.James W Pennebaker, Roger J Booth, and Martha EFrancis.
2001.
Linguistic inquiry and word count(liwc): Liwc2001.
Linguistic Inquiry, (Mahwah,NJ):0.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and BonnieWebber.
2008.
The penn discourse treebank 2.0.In LREC, May.Ellen Riloff, Janyce Wiebe, and Theresa Wilson.2003.
Learning subjective nouns using extractionpattern bootstrapping.
In CoNLL, pages 25?32.Swapna Somasundaran and Janyce Wiebe.
2009.Recognizing stances in online debates.
In ACL-AFNLP, pages 226?234.Swapna Somasundaran and Janyce Wiebe.
2010.Recognizing stances in ideological on-line debates.In CAAGET, pages 116?124.Swapna Somasundaran, Theresa Wilson, JanyceWiebe, and Veselin Stoyanov.
2007.
Qa with atti-tude: Exploiting opinion type analysis for improv-ing question answering in on-line discussions andthe news.
In ICWSM.Swampa Somasundaran.
2010.
Discourse-Level Re-lations for Opinion Analysis.
Ph.D. thesis, Uni-versity of Pittsburgh, USA.Theresa Wilson and Janyce Wiebe.
2003.
Annotat-ing opinions in the world press.
In SIGdial, pages13?22.Theresa Wilson.
2005.
Recognizing contextualpolarity in phrase-level sentiment analysis.
InEMNLP, pages 347?354.Jianxing Yu, Zheng-Jun Zha, Meng Wang, and Tat-Seng Chua.
2011.
Aspect ranking: Identifyingimportant product aspects from online consumerreviews.
In ACL, pages 1496?1505, Portland, Ore-gon, USA, June.88
