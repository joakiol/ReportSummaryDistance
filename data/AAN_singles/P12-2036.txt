Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 181?186,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsCorpus-based interpretation of instructions in virtual environmentsLuciana Benotti1 Mart?
?n Villalba1 Tessa Lau2 Julia?n Cerruti31 FaMAF, Medina Allende s/n, Universidad Nacional de Co?rdoba, Co?rdoba, Argentina2IBM Research ?
Almaden, 650 Harry Road, San Jose, CA 95120 USA3IBM Argentina, Ing.
Butty 275, C1001AFA, Buenos Aires, Argentina{benotti,villalba}@famaf.unc.edu.ar, tessalau@us.ibm.com, jcerruti@ar.ibm.comAbstractPrevious approaches to instruction interpre-tation have required either extensive domainadaptation or manually annotated corpora.This paper presents a novel approach to in-struction interpretation that leverages a largeamount of unannotated, easy-to-collect datafrom humans interacting with a virtual world.We compare several algorithms for automat-ically segmenting and discretizing this datainto (utterance, reaction) pairs and training aclassifier to predict reactions given the next ut-terance.
Our empirical analysis shows that thebest algorithm achieves 70% accuracy on thistask, with no manual annotation required.1 Introduction and motivationMapping instructions into automatically executableactions would enable the creation of natural lan-guage interfaces to many applications (Lau et al,2009; Branavan et al, 2009; Orkin and Roy, 2009).In this paper, we focus on the task of navigation andmanipulation of a virtual environment (Vogel andJurafsky, 2010; Chen and Mooney, 2011).Current symbolic approaches to the problem arebrittle to the natural language variation present in in-structions and require intensive rule authoring to befit for a new task (Dzikovska et al, 2008).
Currentstatistical approaches require extensive manual an-notations of the corpora used for training (MacMa-hon et al, 2006; Matuszek et al, 2010; Gorniak andRoy, 2007; Rieser and Lemon, 2010).
Manual anno-tation and rule authoring by natural language engi-neering experts are bottlenecks for developing con-versational systems for new domains.This paper proposes a fully automated approachto interpreting natural language instructions to com-plete a task in a virtual world based on unsupervisedrecordings of human-human interactions perform-ing that task in that virtual world.
Given unanno-tated corpora collected from humans following otherhumans?
instructions, our system automatically seg-ments the corpus into labeled training data for a clas-sification algorithm.
Our interpretation algorithm isbased on the observation that similar instructions ut-tered in similar contexts should lead to similar ac-tions being taken in the virtual world.
Given a previ-ously unseen instruction, our system outputs actionsthat can be directly executed in the virtual world,based on what humans did when given similar in-structions in the past.2 Corpora situated in virtual worldsOur environment consists of six virtual worlds de-signed for the natural language generation sharedtask known as the GIVE Challenge (Koller et al,2010), where a pair of partners must collaborate tosolve a task in a 3D space (Figure 1).
The ?instruc-tion follower?
(IF) can move around in the virtualworld, but has no knowledge of the task.
The ?in-struction giver?
(IG) types instructions to the IF inorder to guide him to accomplish the task.
Each cor-pus contains the IF?s actions and position recordedevery 200 milliseconds, as well as the IG?s instruc-tions with their timestamps.We used two corpora for our experiments.
TheCm corpus (Gargett et al, 2010) contains instruc-tions given by multiple people, consisting of 37games spanning 2163 instructions over 8:17 hs.
The181Figure 1: A screenshot of a virtual world.
The worldconsists of interconnecting hallways, rooms and objectsCs corpus (Benotti and Denis, 2011), gathered usinga single IG, is composed of 63 games and 3417 in-structions, and was recorded in a span of 6:09 hs.
Ittook less than 15 hours to collect the corpora throughthe web and the subjects reported that the experi-ment was fun.While the environment is restricted, people de-scribe the same route and the same objects in ex-tremely different ways.
Below are some examples ofinstructions from our corpus all given for the sameroute shown in Figure 1.1) out2) walk down the passage3) nowgo [sic] to the pink room4) back to the room with the plant5) Go through the door on the left6) go through opening with yellow wall paperPeople describe routes using landmarks (4) orspecific actions (2).
They may describe the sameobject differently (5 vs 6).
Instructions also differin their scope (3 vs 1).
Thus, even ignoring spellingand grammatical errors, navigation instructions con-tain considerable variation which makes interpretingthem a challenging problem.3 Learning from previous interpretationsOur algorithm consists of two phases: annotationand interpretation.
Annotation is performed onlyonce and consists of automatically associating eachIG instruction to an IF reaction.
Interpretation isperformed every time the system receives an instruc-tion and consists of predicting an appropriate reac-tion given reactions observed in the corpus.Our method is based on the assumption that a re-action captures the semantics of the instruction thatcaused it.
Therefore, if two utterances result in thesame reaction, they are paraphrases of each other,and similar utterances should generate the same re-action.
This approach enables us to predict reactionsfor previously-unseen instructions.3.1 Annotation phaseThe key challenge in learning from massive amountsof easily-collected data is to automatically annotatean unannotated corpus.
Our annotation method con-sists of two parts: first, segmenting a low-level in-teraction trace into utterances and corresponding re-actions, and second, discretizing those reactions intocanonical action sequences.Segmentation enables our algorithm to learn fromtraces of IFs interacting directly with a virtual world.Since the IF can move freely in the virtual world, hisactions are a stream of continuous behavior.
Seg-mentation divides these traces into reactions that fol-low from each utterance of the IG.
Consider the fol-lowing example starting at the situation shown inFigure 1:IG(1): go through the yellow openingIF(2): [walks out of the room]IF(3): [turns left at the intersection]IF(4): [enters the room with the sofa]IG(5): stopIt is not clear whether the IF is doing ?3, 4?
be-cause he is reacting to 1 or because he is beingproactive.
While one could manually annotate thisdata to remove extraneous actions, our goal is to de-velop automated solutions that enable learning frommassive amounts of data.We decided to approach this problem by experi-menting with two alternative formal definitions: 1) astrict definition that considers the maximum reactionaccording to the IF behavior, and 2) a loose defini-tion based on the empirical observation that, in sit-uated interaction, most instructions are constrainedby the current visually perceived affordances (Gib-son, 1979; Stoia et al, 2006).We formally define behavior segmentation (Bhv)as follows.
A reaction rk to an instruction uk begins182right after the instruction uk is uttered and ends rightbefore the next instruction uk+1 is uttered.
In theexample, instruction 1 corresponds to ?2, 3, 4?.
Weformally define visibility segmentation (Vis) as fol-lows.
A reaction rk to an instruction uk begins rightafter the instruction uk is uttered and ends right be-fore the next instruction uk+1 is uttered or right afterthe IF leaves the area visible at 360?
from where ukwas uttered.
In the example, instruction 1?s reactionwould be limited to ?2?
because the intersection isnot visible from where the instruction was uttered.The Bhv and Vis methods define how to segmentan interaction trace into utterances and their corre-sponding reactions.
However, users frequently per-form noisy behavior that is irrelevant to the goal ofthe task.
For example, after hearing an instruction,an IF might go into the wrong room, realize the er-ror, and leave the room.
A reaction should not in-clude such irrelevant actions.
In addition, IFs mayaccomplish the same goal using different behaviors:two different IFs may interpret ?go to the pink room?by following different paths to the same destination.We would like to be able to generalize both reactionsinto one canonical reaction.As a result, our approach discretizes reactions intohigher-level action sequences with less noise andless variation.
Our discretization algorithm uses anautomated planner and a planning representation ofthe task.
This planning representation includes: (1)the task goal, (2) the actions which can be taken inthe virtual world, and (3) the current state of thevirtual world.
Using the planning representation,the planner calculates an optimal path between thestarting and ending states of the reaction, eliminat-ing all unnecessary actions.
While we use the clas-sical planner FF (Hoffmann, 2003), our techniquecould also work with classical planning (Nau et al,2004) or other techniques such as probabilistic plan-ning (Bonet and Geffner, 2005).
It is also not de-pendent on a particular discretization of the world interms of actions.Now we are ready to define canonical reaction ckformally.
Let Sk be the state of the virtual worldwhen instruction uk was uttered, Sk+1 be the state ofthe world where the reaction ends (as defined by Bhvor Vis segmentation), and D be the planning domainrepresentation of the virtual world.
The canonicalreaction to uk is defined as the sequence of actionsreturned by the planner with Sk as initial state, Sk+1as goal state and D as planning domain.3.2 Interpretation phaseThe annotation phase results in a collection of (uk,ck) pairs.
The interpretation phase uses these pairs tointerpret new utterances in three steps.
First, we fil-ter the set of pairs into those whose reactions can bedirectly executed from the current IF position.
Sec-ond, we group the filtered pairs according to theirreactions.
Third, we select the group with utterancesmost similar to the new utterance, and output thatgroup?s reaction.
Figure 2 shows the output of thefirst two steps: three groups of pairs whose reactionscan all be executed from the IF?s current position.Figure 2: Utterance groups for this situation.
Coloredarrows show the reaction associated with each group.We treat the third step, selecting the most similargroup for a new utterance, as a classification prob-lem.
We compare three different classification meth-ods.
One method uses nearest-neighbor classifica-tion with three different similarity metrics: Jaccardand Overlap coefficients (both of which measure thedegree of overlap between two sets, differing onlyin the normalization of the final value (Nikravesh etal., 2005)), and Levenshtein Distance (a string met-ric for measuring the amount of differences betweentwo sequences of words (Levenshtein, 1966)).
Oursecond classification method employs a strategy inwhich we considered each group as a set of pos-sible machine translations of our utterance, usingthe BLEU measure (Papineni et al, 2002) to selectwhich group could be considered the best translationof our utterance.
Finally, we trained an SVM clas-sifier (Cortes and Vapnik, 1995) using the unigrams183Corpus Cm Corpus CsAlgorithm Bhv Vis Bhv VisJaccard 47% 54% 54% 70%Overlap 43% 53% 45% 60%BLEU 44% 52% 54% 50%SVM 33% 29% 45% 29%Levenshtein 21% 20% 8% 17%Table 1: Accuracy comparison between Cm and Cs forBhv and Vis segmentationof each paraphrase and the position of the IF as fea-tures, and setting their group as the output class us-ing a libSVM wrapper (Chang and Lin, 2011).When the system misinterprets an instruction weuse a similar approach to what people do in orderto overcome misunderstandings.
If the system exe-cutes an incorrect reaction, the IG can tell the systemto cancel its current interpretation and try again us-ing a paraphrase, selecting a different reaction.4 EvaluationFor the evaluation phase, we annotated both the Cmand Cs corpora entirely, and then we split them inan 80/20 proportion; the first 80% of data collectedin each virtual world was used for training, whilethe remaining 20% was used for testing.
For eachpair (uk, ck) in the testing set, we used our algorithmto predict the reaction to the selected utterance, andthen compared this result against the automaticallyannotated reaction.
Table 1 shows the results.Comparing the Bhv and Vis segmentation strate-gies, Vis tends to obtain better results than Bhv.
Inaddition, accuracy on the Cs corpus was generallyhigher than Cm.
Given that Cs contained only oneIG, we believe this led to less variability in the in-structions and less noise in the training data.We evaluated the impact of user corrections bysimulating them using the existing corpus.
In caseof a wrong response, the algorithm receives a secondutterance with the same reaction (a paraphrase of theprevious one).
Then the new utterance is tested overthe same set of possible groups, except for the onewhich was returned before.
If the correct reactionis not predicted after four tries, or there are no ut-terances with the same reaction, the predictions areregistered as wrong.
To measure the effects of usercorrections vs. without, we used a different evalu-ation process for this algorithm: first, we split thecorpus in a 50/50 proportion, and then we movedcorrectly predicted utterances from the testing set to-wards training, until either there was nothing moreto learn or the training set reached 80% of the entirecorpus size.As expected, user corrections significantly im-prove accuracy, as shown in Figure 3.
The worstalgorithm?s results improve linearly with each try,while the best ones behave asymptotically, barelyimproving after the second try.
The best algorithmreaches 92% with just one correction from the IG.5 Discussion and future workWe presented an approach to instruction interpreta-tion which learns from non-annotated logs of hu-man behavior.
Our empirical analysis shows thatour best algorithm achieves 70% accuracy on thistask, with no manual annotation required.
Whencorrections are added, accuracy goes up to 92%for just one correction.
We consider our resultspromising since state of the art semi-unsupervisedapproaches to instruction interpretation (Chen andMooney, 2011) reports a 55% accuracy on manuallysegmented data.We plan to compare our system?s performanceagainst human performance in comparable situa-tions.
Our informal observations of the GIVE cor-pus indicate that humans often follow instructionsincorrectly, so our automated system?s performancemay be on par with human performance.Although we have presented our approach in thecontext of 3D virtual worlds, we believe our tech-nique is also applicable to other domains such as theweb, video games, or Human Robot Interaction.Figure 3: Accuracy values with corrections over Cs184ReferencesLuciana Benotti and Alexandre Denis.
2011.
CL system:Giving instructions by corpus based selection.
In Pro-ceedings of the Generation Challenges Session at the13th European Workshop on Natural Language Gener-ation, pages 296?301, Nancy, France, September.
As-sociation for Computational Linguistics.Blai Bonet and He?ctor Geffner.
2005. mGPT: a proba-bilistic planner based on heuristic search.
Journal ofArtificial Intelligence Research, 24:933?944.S.R.K.
Branavan, Harr Chen, Luke Zettlemoyer, andRegina Barzilay.
2009.
Reinforcement learning formapping instructions to actions.
In Proceedings ofthe Joint Conference of the 47th Annual Meeting ofthe ACL and the 4th International Joint Conferenceon Natural Language Processing of the AFNLP, pages82?90, Suntec, Singapore, August.
Association forComputational Linguistics.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM:A library for support vector machines.
ACM Transac-tions on Intelligent Systems and Technology, 2:27:1?27:27.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.David L. Chen and Raymond J. Mooney.
2011.
Learn-ing to interpret natural language navigation instruc-tions from observations.
In Proceedings of the 25thAAAI Conference on Artificial Intelligence (AAAI-2011), pages 859?865, August.Corinna Cortes and Vladimir Vapnik.
1995.
Support-vector networks.
Machine Learning, 20:273?297.Myroslava O. Dzikovska, James F. Allen, and Mary D.Swift.
2008.
Linking semantic and knowledge repre-sentations in a multi-domain dialogue system.
Journalof Logic and Computation, 18:405?430, June.Andrew Gargett, Konstantina Garoufi, Alexander Koller,and Kristina Striegnitz.
2010.
The GIVE-2 corpusof giving instructions in virtual environments.
In Pro-ceedings of the 7th Conference on International Lan-guage Resources and Evaluation (LREC), Malta.James J. Gibson.
1979.
The Ecological Approach to Vi-sual Perception, volume 40.
Houghton Mifflin.Peter Gorniak and Deb Roy.
2007.
Situated languageunderstanding as filtering perceived affordances.
Cog-nitive Science, 31(2):197?231.Jo?rg Hoffmann.
2003.
The Metric-FF planning sys-tem: Translating ?ignoring delete lists?
to numericstate variables.
Journal of Artificial Intelligence Re-search (JAIR), 20:291?341.Alexander Koller, Kristina Striegnitz, Andrew Gargett,Donna Byron, Justine Cassell, Robert Dale, JohannaMoore, and Jon Oberlander.
2010.
Report on the sec-ond challenge on generating instructions in virtual en-vironments (GIVE-2).
In Proceedings of the 6th In-ternational Natural Language Generation Conference(INLG), Dublin.Tessa Lau, Clemens Drews, and Jeffrey Nichols.
2009.Interpreting written how-to instructions.
In Proceed-ings of the 21st International Joint Conference on Ar-tificial Intelligence, pages 1433?1438, San Francisco,CA, USA.
Morgan Kaufmann Publishers Inc.Vladimir I. Levenshtein.
1966.
Binary codes capable ofcorrecting deletions, insertions, and reversals.
Techni-cal Report 8.Matt MacMahon, Brian Stankiewicz, and BenjaminKuipers.
2006.
Walk the talk: connecting language,knowledge, and action in route instructions.
In Pro-ceedings of the 21st National Conference on Artifi-cial Intelligence - Volume 2, pages 1475?1482.
AAAIPress.Cynthia Matuszek, Dieter Fox, and Karl Koscher.
2010.Following directions using statistical machine trans-lation.
In Proceedings of the 5th ACM/IEEE inter-national conference on Human-robot interaction, HRI?10, pages 251?258, New York, NY, USA.
ACM.Dana Nau, Malik Ghallab, and Paolo Traverso.
2004.Automated Planning: Theory & Practice.
MorganKaufmann Publishers Inc., California, USA.Masoud Nikravesh, Tomohiro Takagi, Masanori Tajima,Akiyoshi Shinmura, Ryosuke Ohgaya, Koji Taniguchi,Kazuyosi Kawahara, Kouta Fukano, and AkikoAizawa.
2005.
Soft computing for perception-baseddecision processing and analysis: Web-based BISC-DSS.
In Masoud Nikravesh, Lotfi Zadeh, and JanuszKacprzyk, editors, Soft Computing for InformationProcessing and Analysis, volume 164 of Studies inFuzziness and Soft Computing, chapter 4, pages 93?188.
Springer Berlin / Heidelberg.Jeff Orkin and Deb Roy.
2009.
Automatic learningand generation of social behavior from collective hu-man gameplay.
In Proceedings of The 8th Interna-tional Conference on Autonomous Agents and Mul-tiagent SystemsVolume 1, volume 1, pages 385?392.International Foundation for Autonomous Agents andMultiagent Systems, International Foundation for Au-tonomous Agents and Multiagent Systems.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th Annual Meeting on Association for Computa-tional Linguistics, ACL ?02, pages 311?318, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Verena Rieser and Oliver Lemon.
2010.
Learning hu-man multimodal dialogue strategies.
Natural Lan-guage Engineering, 16:3?23.Laura Stoia, Donna K. Byron, Darla Magdalene Shock-ley, and Eric Fosler-Lussier.
2006.
Sentence planning185for realtime navigational instructions.
In Proceedingsof the Human Language Technology Conference of theNAACL, Companion Volume: Short Papers, NAACL-Short ?06, pages 157?160, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Adam Vogel and Dan Jurafsky.
2010.
Learning to fol-low navigational directions.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, ACL ?10, pages 806?814, Stroudsburg,PA, USA.
Association for Computational Linguistics.186
