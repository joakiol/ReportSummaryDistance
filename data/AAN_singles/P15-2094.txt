Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 573?577,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsImproving Pivot Translation by Remembering the PivotAkiva Miura, Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi NakamuraGraduate School of Information ScienceNara Institute of Science and Technology8916-5 Takayama-cho, Ikoma-shi, Nara, Japan{miura.akiba.lr9, neubig, ssakti, tomoki, s-nakamura}@is.naist.jpAbstractPivot translation allows for translation oflanguage pairs with little or no paralleldata by introducing a third language forwhich data exists.
In particular, the trian-gulation method, which translates by com-bining source-pivot and pivot-target trans-lation models into a source-target model,is known for its high translation accuracy.However, in the conventional triangulationmethod, information of pivot phrases isforgotten and not used in the translationprocess.
In this paper, we propose a novelapproach to remember the pivot phrases inthe triangulation stage, and use a pivot lan-guage model as an additional informationsource at translation time.
Experimen-tal results on the Europarl corpus showedgains of 0.4-1.2 BLEU points in all testedcombinations of languages1.1 IntroductionIn statistical machine translation (SMT) (Brown etal., 1993), it is known that translation with mod-els trained on larger parallel corpora can achievegreater accuracy (Dyer et al., 2008).
Unfor-tunately, large bilingual corpora are not readilyavailable for many language pairs, particularlythose that don?t include English.
One effective so-lution to overcome the scarceness of bilingual datais to introduce a pivot language for which paralleldata with the source and target languages exists(de Gispert and Mari?no, 2006).Among various methods using pivot languages,the triangulation method (Cohn and Lapata, 2007;Utiyama and Isahara, 2007; Zhu et al., 2014),which translates by combining source-pivot andpivot-target translation models into a source-target1Code to replicate the experiments can be found athttps://github.com/akivajp/acl2015		 	(a) Triangulation (de-en-it) 	 	(b) Traditional Triangulated Phrases	 		 		 (c) Proposed Triangulated PhrasesFigure 1: An example of (a) triangulation and theresulting phrases in the (b) traditional method offorgetting pivots and (c) our proposed method ofremembering pivots.model, has been shown to be one of the most effec-tive approaches.
However, word sense ambiguityand interlingual differences of word usage causedifficulty in accurately learning correspondencesbetween source and target phrases.Figure 1 (a) shows an example of three wordsin German and Italian that each correspond to theEnglish polysemic word ?approach.?
In such acase, finding associated source-target phrase pairsand estimating translation probabilities properlybecomes a complicated problem.
Furthermore, inthe conventional triangulation method, informa-tion about pivot phrases that behave as bridges be-tween source and target phrases is lost after learn-ing phrase pairs, as shown in Figure 1 (b).To overcome these problems, we propose anovel triangulation method that remembers thepivot phrase connecting source and target in therecords of phrase/rule table, and estimates a jointtranslation probability from the source to target573and pivot simultaneously.
We show an examplein Figure 1 (c).
The advantage of this approachis that generally we can obtain rich monolingualresources in pivot languages such as English, andSMT can utilize this additional information to im-prove the translation quality.To utilize information about the pivot languageat translation time, we train a Multi-SynchronousContext-free Grammar (MSCFG) (Neubig et al.,2015), a generalized extension of synchronousCFGs (SCFGs) (Chiang, 2007), that can gener-ate strings in multiple languages at the same time.To create the MSCFG, we triangulate source-pivotand pivot-target SCFG rule tables not into a singlesource-target SCFG, but into a source-target-pivotMSCFG rule table that remembers the pivot.
Dur-ing decoding, we use language models over boththe target and the pivot to assess the naturalness ofthe derivation.
We perform experiments on pivottranslation of Europarl proceedings, which showthat our method indeed provide significant gains inaccuracy (of up to 1.2 BLEU points), in all com-binations of 4 languages with English as a pivotlanguage.2 Translation Formalisms2.1 Synchronous Context-free GrammarsFirst, we cover SCFGs, which are widely usedin machine translation, particularly hierarchicalphrase-based translation (Hiero; Chiang (2007)).In SCFGs, the elementary structures are rewriterules with aligned pairs of right-hand sides:X ?
?s, t?
(1)whereX is the head of the rewrite rule, and s and tare both strings of terminals and non-terminals inthe source and target side respectively.
Each stringin the right side tuple has the same number of in-dexed non-terminals, and identically indexed non-terminals correspond to each-other.
For example,a synchronous rule could take the form of:X ?
?X0of the X1, X1?
X0?
.
(2)In the SCFG training method proposed byChiang (2007), SCFG rules are extracted basedon parallel sentences and automatically obtainedword alignments.
Each extracted rule is scoredwith phrase translation probabilities in both direc-tions ?
(s|t) and ?
(t|s), lexical translation proba-bilities in both directions ?lex(s|t) and ?lex(t|s),a word penalty counting the terminals in t, and aconstant phrase penalty of 1.At translation time, the decoder searches forthe target sentence that maximizes the derivationprobability, which is defined as the sum of thescores of the rules used in the derivation, and thelog of the language model probability over the tar-get strings.
When not considering an LM, it is pos-sible to efficiently find the best translation for aninput sentence using the CKY+ algorithm (Chap-pelier et al., 1998).
When using an LM, the ex-panded search space is further reduced based on alimit on expanded edges, or total states per span,through a procedure such as cube pruning (Chi-ang, 2007).2.2 Multi-Synchronous CFGsMSCFGs (Neubig et al., 2015) are a generalizationof SCFGs that are be able to generate sentences inmultiple target languages simultaneously.
The sin-gle target side string t in the SCFG production ruleis extended to have strings for N target languages:X ?
?s, t1, ..., tN?.
(3)Performing multi-target translation withMSCFGs is quite similar to translating usingstandard SCFGs, with the exception of the ex-panded state space caused by having one LMfor each target.
Neubig et al.
(2015) propose asequential search method, that ensures diversity inthe primary target search space by first expandingwith only primary target LM, then additionallyexpands the states for other LMs, a strategy wealso adopt in this work.In the standard training method for MSCFGs,the multi-target rewrite rules are extracted frommultilingual line-aligned corpora by applying anextended version of the standard SCFG rule ex-traction method, and scored with features that con-sider the multiple targets.
It should be noted thatthis training method requires a large amount ofline-aligned training data including the source andall target languages.
This assumption breaks downwhen we have little parallel data, and thereby wepropose a method to generate MSCFG rules bytriangulating 2 SCFG rule tables in the followingsection.3 Pivot Translation MethodsSeveral methods have been proposed for SMT us-ing pivot languages.
These include cascade meth-ods that consecutively translate from source to574pivot then pivot to target (de Gispert and Mari?no,2006), synthetic data methods that machine-translate the training data to generate a pseudo-parallel corpus (de Gispert and Mari?no, 2006),and triangulation methods that obtain a source-target phrase/rule table by merging source-pivotand pivot-target table entries with identical pivotlanguage phrases (Cohn and Lapata, 2007).
In par-ticular, the triangulation method is notable for pro-ducing higher quality translation results than otherpivot methods (Utiyama and Isahara, 2007), so weuse it as a base for our work.3.1 Traditional Triangulation MethodIn the triangulation method by Cohn and Lapata(2007), we first train source-pivot and pivot-targetrule tables, then create rules:X ?
?s, t?
(4)if there exists a pivot phrase p such that the pair?s, p?
is in source-pivot table TSPand the pair?p, t?is in pivot-target table TPT.
Source-targettable TSTis created by calculation of the trans-lation probabilities using phrase translation prob-abilities ?(?)
and lexical translation probabilities?lex(?)
for all connected phrases according to thefollowing equations (Cohn and Lapata, 2007):?(t|s)=?p?TSP?TPT?(t|p)?
(p|s) , (5)?(s|t)=?p?TSP?TPT?
(s|p)?
(p|t), (6)?lex(t|s)=?p?TSP?TPT?lex(t|p)?lex(p|s) , (7)?lex(s|t)=?p?TSP?TPT?lex(s|p)?lex(p|t).
(8)The equations (5)-(8) are based on the memo-ryless channel model, which assumes ?
(t|p, s)=?
(t|p)and ?
(s|p, t)= ?
(s|p).
Unfortunately,these equations are not accurate due to polysemyand disconnects in the grammar of the languages.As a result, pivot translation is significantly moreambiguous than standard translation.3.2 Proposed Triangulation MethodTo help reduce this ambiguity, our proposed tri-angulation method remembers the correspondingpivot phrase as additional information to be uti-lized for disambiguation.
Specifically, instead ofmarginalizing over the pivot phrase p, we create anMSCFG rule for the tuple of the connected source-target-pivot phrases such as:X ?
?s, t, p?.
(9)The advantage of translation with these rules isthat they allow for incorporation of additional fea-tures over the pivot sentence such as a strong pivotLM.In addition to the equations (5)-(8), we also es-timate translation probabilities ?
(t, p|s), ?
(s|p, t)that consider both target and pivot phrases at thesame time according to:?
(t, p|s)= ?(t|p)?
(p|s) , (10)?
(s|p, t)= ?
(s|p) .
(11)Translation probabilities between source and pivotphrases ?
(p|s), ?
(s|p), ?lex(p|s), ?lex(s|p) canalso be used directly from the source-pivot rule ta-ble.
This results in 13 features for each MSCFGrule: 10 translation probabilities, 2 word penaltiescounting the terminals in t and p, and a constantphrase penalty of 1.It should be noted that remembering the pivotresults in significantly larger rule tables.
To savecomputational resources, several pruning methodsare conceivable.
Neubig et al.
(2015) show that aneffective pruning method in the case of a main tar-get T1with the help of target T2is the T1-pruningmethod, namely, using L candidates of t1with thehighest translation probability ?
(t1|s) and select-ing t2with highest ?
(t1, t2|s) for each t1.
We fol-low this approach, using the L best t, and the cor-responding 1 best p .4 Experiments4.1 Experimental SetupWe evaluate the proposed triangulation methodthrough pivot translation experiments on the Eu-roparl corpus, which is a multilingual corpus in-cluding 21 European languages (Koehn, 2005)widely used in pivot translation work.
In ourwork, we perform translation among German (de),Spanish (es), French (fr) and Italian (it), with En-glish (en) as the pivot language.
To prepare thedata for these 5 languages, we first use the Gale-Church alignment algorithm (Gale and Church,1993) to retrieve a multilingual line-aligned cor-pus of about 900k sentences, then hold out 1,500sentences each for tuning and test.
In our basic575Source TargetBLEU Score [%]Direct CascadeTri.
SCFG(baseline)Tri.
MSCFG-PivotLMTri.
MSCFG+PivotLM 100kTri.
MSCFG+PivotLM 2Mes 27.10 25.05 25.31 25.38 25.52 ?
25.75de fr 25.65 23.86 24.12 24.16 24.25 ?
24.58it 23.04 20.76 21.27 21.42 ?
21.65 ?
22.29de 20.11 18.52 18.77 18.97 19.08 ?
19.40es fr 33.48 27.00 29.54 ?
29.87 ?
29.91 ?
29.95it 27.82 22.57 25.11 25.01 25.18 ?
25.64de 19.69 18.01 18.73 18.77 18.87 ?
19.19fr es 34.36 27.26 30.31 30.53 ?
30.73 ?
31.00it 28.48 22.73 25.31 25.50 ?
25.72 ?
26.22de 19.09 14.03 17.35 ?
17.99 ?
18.17 ?
18.52it es 31.99 25.64 28.85 28.83 29.01 ?
29.31fr 31.39 25.87 28.48 28.40 28.63 ?
29.02Table 1: Results for each method.
Bold indicates the highest BLEU score in pivot translation, anddaggers indicate statistically significant gains over Tri.
SCFG (?
: p < 0.05, ?
: p < 0.01)training setup, we use 100k sentences for train-ing both the TMs and the target LMs.
We as-sume that in many situations, a large amount ofEnglish monolingual data is readily available andtherefore, we train pivot LMs with different datasizes up to 2M sentences.As a decoder, we use Travatar (Neubig, 2013),and train SCFG TMs with its Hiero extractioncode.
Translation results are evaluated by BLEU(Papineni et al., 2002) and we tuned to maxi-mize BLEU scores using MERT (Och, 2003).
Fortrained and triangulated TMs, we use T1rule prun-ing with a limit of 20 rules per source rule.
Fordecoding using MSCFG, we adopt the sequentialsearch method.We evaluate 6 translation methods:Direct: Translating with a direct SCFG trained onthe source-target parallel corpus (not using apivot language) for comparison.Cascade: Cascading source-pivot and pivot-target translation systems.Tri.
SCFG: Triangulating source-pivot andpivot-target SCFG TMs into a source-targetSCFG TM using the traditional method.Tri.
MSCFG: Triangulating source-pivot andpivot-target SCFG TMs into a source-target-pivot MSCFG TM in our approach.-PivotLM indicates translating without apivot LM and +PivotLM 100k/2M indicatesa pivot LM trained using 100k/2M sentencesrespectively.4.2 Experimental ResultsThe result of experiments using all combinationsof pivot translation tasks for 4 languages via En-glish is shown in Table 1.
From the results, we cansee that the proposed triangulation method consid-ering pivot LMs outperforms the traditional trian-gulation method for all language pairs, and trans-lation with larger pivot LMs improves the BLEUscores.
For all languages, the pivot-rememberingtriangulation method with the pivot LM trainedwith 2M sentences achieves the highest score ofthe pivot translation methods, with gains of 0.4-1.2 BLEU points from the baseline method.
Thisshows that remembering the pivot and using itto disambiguate results is consistently effective inimproving translation accuracy.We can also see that the MSCFG triangulatedmodel without using the pivot LM slightly outper-forms the standard SCFG triangulation method forthe majority of language pairs.
It is conceivablethat the additional scores of translation probabil-ities with pivot phrases are effective features thatallow for more accurate rule selection.Finally, we show an example of a translated sen-tence for which pivot-side ambiguity is resolved inthe proposed triangulation method:Input (German): ich bedaure , da?
es keinegemeinsame ann?aherung gegeben hat .Reference (Italian): sono spiacente del mancatoapproccio comune .Tri.
SCFG: mi rammarico per il fatto che non siravvicinamento comune .
(BLEU+1: 13.84)Tri.
MSCFG+PivotLM 2M:576mi dispiace che non esiste un approccio co-mune .
(BLEU+1: 25.10)i regret that there is no common approach .
(Generated English Sentence)The derivation uses an MSCFG rule connecting?approccio?
to ?approach?
in the pivot, and wecan consider that appropriate selection of Englishwords according to the context contributes to se-lecting relevant vocabulary in Italian.5 ConclusionIn this paper, we have proposed a method for pivottranslation using triangulation of SCFG rule ta-bles into an MSCFG rule table that remembers thepivot, and performing translation with pivot LMs.In experiments, we found that these models areeffective in the case when a strong pivot LM ex-ists.
In the future, we plan to explore more refinedmethods to devising effective intermediate expres-sions, and improve estimation of probabilities fortriangulated rules.AcknowledgementsThe authors thank anonymous reviewers for help-ful suggestions.
This work was supported in partby the Microsoft CORE project.ReferencesPeter F. Brown, Vincent J.Della Pietra, StephenA.
Della Pietra, and Robert L. Mercer.
1993.The Mathematics of Statistical Machine Translation:Parameter Estimation.
Computational Linguistics,19:263?312.Jean-C?edric Chappelier, Martin Rajman, et al.
1998.
AGeneralized CYK Algorithm for Parsing StochasticCFG.
TAPD, 98(133-137):5.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Trevor Cohn and Mirella Lapata.
2007.
MachineTranslation by Triangulation: Making Effective Useof Multi-Parallel Corpora.
In Proc.
ACL, pages 728?735, June.Adri`a de Gispert and Jos?e B. Mari?no.
2006.
Catalan-English Statistical Machine Translation without Par-allel Corpus: Bridging through Spanish.
In Proc.of LREC 5th Workshop on Strategies for developingmachine translation for minority languages.Christopher Dyer, Aaron Cordova, Alex Mont, andJimmy Lin.
2008.
Fast, easy, and cheap: construc-tion of statistical machine translation models withMapReduce.
In Proc.
WMT, pages 199?207.William A Gale and Kenneth W Church.
1993.
Aprogram for aligning sentences in bilingual corpora.Computational linguistics, 19(1):75?102.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In MT summit, vol-ume 5, pages 79?86.Graham Neubig, Philip Arthur, and Kevin Duh.2015.
Multi-Target Machine Translation withMulti-Synchronous Context-free Grammars.
In Proc.NAACL.Graham Neubig.
2013.
Travatar: A Forest-to-StringMachine Translation Engine based on Tree Trans-ducers.
In Proc.
ACL Demo Track, pages 91?96.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proc.
ACL,pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proc.
ACL,pages 311?318.Masao Utiyama and Hitoshi Isahara.
2007.
A Com-parison of Pivot Methods for Phrase-Based Statis-tical Machine Translation.
In Proc.
NAACL, pages484?491.Xiaoning Zhu, Zhongjun He, Hua Wu, Conghui Zhu,Haifeng Wang, and Tiejun Zhao.
2014.
ImprovingPivot-Based Statistical Machine Translation by Piv-oting the Co-occurrence Count of Phrase Pairs.
InProc.
EMNLP.577
