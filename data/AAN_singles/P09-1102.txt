Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 905?913,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPRobust Approach to Abbreviating Terms:A Discriminative Latent Variable Model with Global InformationXu Sun?, Naoaki Okazaki?, Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo,Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, Japan?School of Computer Science, University of Manchester, UK?National Centre for Text Mining, UK{sunxu, okazaki, tsujii}@is.s.u-tokyo.ac.jpAbstractThe present paper describes a robust ap-proach for abbreviating terms.
First, inorder to incorporate non-local informa-tion into abbreviation generation tasks, wepresent both implicit and explicit solu-tions: the latent variable model, or alter-natively, the label encoding approach withglobal information.
Although the two ap-proaches compete with one another, wedemonstrate that these approaches are alsocomplementary.
By combining these twoapproaches, experiments revealed that theproposed abbreviation generator achievedthe best results for both the Chinese andEnglish languages.
Moreover, we directlyapply our generator to perform a very dif-ferent task from tradition, the abbreviationrecognition.
Experiments revealed that theproposed model worked robustly, and out-performed five out of six state-of-the-artabbreviation recognizers.1 IntroductionAbbreviations represent fully expanded forms(e.g., hidden markov model) through the use ofshortened forms (e.g., HMM).
At the same time,abbreviations increase the ambiguity in a text.For example, in computational linguistics, theacronym HMM stands for hidden markov model,whereas, in the field of biochemistry, HMM is gen-erally an abbreviation for heavy meromyosin.
As-sociating abbreviations with their fully expandedforms is of great importance in various NLP ap-plications (Pakhomov, 2002; Yu et al, 2006;HaCohen-Kerner et al, 2008).The core technology for abbreviation disam-biguation is to recognize the abbreviation defini-tions in the actual text.
Chang and Schu?tze (2006)reported that 64,242 new abbreviations were intro-duced into the biomedical literatures in 2004.
Assuch, it is important to maintain sense inventories(lists of abbreviation definitions) that are updatedwith the neologisms.
In addition, based on theone-sense-per-discourse assumption, the recogni-tion of abbreviation definitions assumes senses ofabbreviations that are locally defined in a docu-ment.
Therefore, a number of studies have at-tempted to model the generation processes of ab-breviations: e.g., inferring the abbreviating mech-anism of the hidden markov model into HMM.An obvious approach is to manually designrules for abbreviations.
Early studies attemptedto determine the generic rules that humans useto intuitively abbreviate given words (Barrett andGrems, 1960; Bourne and Ford, 1961).
Sincethe late 1990s, researchers have presented var-ious methods by which to extract abbreviationdefinitions that appear in actual texts (Taghvaand Gilbreth, 1999; Park and Byrd, 2001; Wrenand Garner, 2002; Schwartz and Hearst, 2003;Adar, 2004; Ao and Takagi, 2005).
For example,Schwartz and Hearst (2003) implemented a simplealgorithm that mapped all alpha-numerical lettersin an abbreviation to its expanded form, startingfrom the end of both the abbreviation and its ex-panded forms, and moving from right to left.These studies performed highly, especially forEnglish abbreviations.
However, a more extensiveinvestigation of abbreviations is needed in order tofurther improve definition extraction.
In addition,we cannot simply transfer the knowledge of thehand-crafted rules from one language to another.For instance, in English, abbreviation charactersare preferably chosen from the initial and/or cap-ital characters in their full forms, whereas some905p o l y g l y c o l i c a c i dP S S S P S S S S S S S S P S S S [PGA]???
?
??
?S P P S S S P [???
]Institute of History and Philology at Academia Sinica(b): Chinese Abbreviation Generation(a): English Abbreviation GenerationFigure 1: English (a) and Chinese (b) abbreviationgeneration as a sequential labeling problem.other languages, including Chinese and Japanese,do not have word boundaries or case sensitivity.A number of recent studies have investigatedthe use of machine learning techniques.
Tsuruokaet al (2005) formalized the processes of abbrevia-tion generation as a sequence labeling problem.
Inthe present study, each character in the expandedform is tagged with a label, y ?
{P,S}1, wherethe label P produces the current character andthe label S skips the current character.
In Fig-ure 1 (a), the abbreviation PGA is generated fromthe full form polyglycolic acid because the under-lined characters are tagged with P labels.
In Fig-ure 1 (b), the abbreviation is generated using the2nd and 3rd characters, skipping the subsequentthree characters, and then using the 7th character.In order to formalize this task as a sequentiallabeling problem, we have assumed that the la-bel of a character is determined by the local in-formation of the character and its previous label.However, this assumption is not ideal for model-ing abbreviations.
For example, the model can-not make use of the number of words in a fullform to determine and generate a suitable num-ber of letters for the abbreviation.
In addition, themodel would be able to recognize the abbreviat-ing process in Figure 1 (a) more reasonably if itwere able to segment the word polyglycolic intosmaller regions, e.g., poly-glycolic.
Even thoughhumans may use global or non-local informationto abbreviate words, previous studies have not in-corporated this information into a sequential label-ing model.In the present paper, we propose implicit andexplicit solutions for incorporating non-local in-formation.
The implicit solution is based on the1Although the original paper of Tsuruoka et al (2005) at-tached case sensitivity information to the P label, for simplic-ity, we herein omit this information.y1 y2 ymxmx2x1h1 h2 hmxmx2x1ymy2y1CRF DPLVMFigure 2: CRF vs. DPLVM.
Variables x, y, and hrepresent observation, label, and latent variables,respectively.discriminative probabilistic latent variable model(DPLVM) in which non-local information is mod-eled by latent variables.
We manually encode non-local information into the labels in order to providean explicit solution.
We evaluate the models on thetask of abbreviation generation, in which a modelproduces an abbreviation for a given full form.
Ex-perimental results indicate that the proposed mod-els significantly outperform previous abbreviationgeneration studies.
In addition, we apply the pro-posed models to the task of abbreviation recogni-tion, in which a model extracts the abbreviationdefinitions in a given text.
To the extent of ourknowledge, this is the first model that can per-form both abbreviation generation and recognitionat the state-of-the-art level, across different lan-guages and with a simple feature set.2 Abbreviator with Non-localInformation2.1 A Latent Variable AbbreviatorTo implicitly incorporate non-local information,we propose discriminative probabilistic latentvariable models (DPLVMs) (Morency et al, 2007;Petrov and Klein, 2008) for abbreviating terms.The DPLVM is a natural extension of the CRFmodel (see Figure 2), which is a special case of theDPLVM, with only one latent variable assigned foreach label.
The DPLVM uses latent variables tocapture additional information that may not be ex-pressed by the observable labels.
For example, us-ing the DPLVM, a possible feature could be ?thecurrent character xi = X, the label yi = P, andthe latent variable hi = LV.?
The non-local infor-mation can be effectively modeled in the DPLVM,and the additional information at the previous po-sition or many of the other positions in the pastcould be transferred via the latent variables (seeFigure 2).906Using the label set Y = {P,S}, abbreviationgeneration is formalized as the task of assigninga sequence of labels y = y1, y2, .
.
.
, ym for agiven sequence of characters x = x1, x2, .
.
.
, xmin an expanded form.
Each label, yj , is a mem-ber of the possible labels Y .
For each sequence,we also assume a sequence of latent variablesh = h1, h2, .
.
.
, hm, which are unobservable intraining examples.We model the conditional probability of the la-bel sequence P (y|x) using the DPLVM,P (y|x,?)
=?hP (y|h,x,?
)P (h|x,?).
(1)Here, ?
represents the parameters of the model.To ensure that the training and inference are ef-ficient, the model is often restricted to have dis-jointed sets of latent variables associated with eachlabel (Morency et al, 2007).
Each hj is a memberin a set Hyj of possible latent variables for the la-bel yj .
Here, H is defined as the set of all possi-ble latent variables, i.e., H is the union of all Hyjsets.
Since the sequences having hj /?
Hyj will,by definition, yield P (y|x,?)
= 0, the model isrewritten as follows (Morency et al, 2007; Petrovand Klein, 2008):P (y|x,?)
=?h?Hy1?...
?HymP (h|x,?).
(2)Here, P (h|x,?)
is defined by the usual formula-tion of the conditional random field,P (h|x,?)
= exp??f(h,x)?
?h exp?
?f(h,x), (3)where f(h,x) represents a feature vector.Given a training set consisting of n instances,(xi,yi) (for i = 1 .
.
.
n), we estimate the pa-rameters ?
by maximizing the regularized log-likelihood,L(?)
=n?i=1logP (yi|xi,?)?R(?).
(4)The first term expresses the conditional log-likelihood of the training data, and the second termrepresents a regularizer that reduces the overfittingproblem in parameter estimation.2.2 Label Encoding with Global InformationAlternatively, we can design the labels such thatthey explicitly incorporate non-local information.?
?
?
?
?
?
?
?
?
?
?
?
?
?S S P S S S S S S P S P S SS0 S0 P1 S1 S1 S1 S1 S1 S1 P2 S2 P3 S3 S3Management office of the imports and exports of endangered speciesOrig.GIFigure 3: Comparison of the proposed label en-coding method with global information (GI) andthe conventional label encoding method.In this approach, the label yi at position i at-taches the information of the abbreviation lengthgenerated by its previous labels, y1, y2, .
.
.
, yi?1.Figure 3 shows an example of a Chinese abbre-viation.
In this encoding, a label not only con-tains the produce or skip information, but also theabbreviation-length information, i.e., the label in-cludes the number of all P labels preceding thecurrent position.
We refer to this method as labelencoding with global information (hereinafter GI).The concept of using label encoding to incorporatenon-local information was originally proposed byPeshkin and Pfeffer (2003).Note that the model-complexity is increasedonly by the increase in the number of labels.
Sincethe length of the abbreviations is usually quiteshort (less than five for Chinese abbreviations andless than 10 for English abbreviations), the modelis still tractable even when using the GI encoding.The implicit (DPLVM) and explicit (GI) solu-tions address the same issue concerning the in-corporation of non-local information, and thereare advantages to combining these two solutions.Therefore, we will combine the implicit and ex-plicit solutions by employing the GI encoding inthe DPLVM (DPLVM+GI).
The effects of thiscombination will be demonstrated through experi-ments.2.3 Feature DesignNext, we design two types of features: language-independent features and language-specific fea-tures.
Language-independent features can be usedfor abbreviating terms in English and Chinese.
Weuse the features from #1 to #3 listed in Table 1.Feature templates #4 to #7 in Table 1 are usedfor Chinese abbreviations.
Templates #4 and #5express the Pinyin reading of the characters, whichrepresents a Romanization of the sound.
Tem-plates #6 and #7 are designed to detect characterduplication, because identical characters will nor-mally be skipped in the abbreviation process.
On907#1 The input char.
xi?1 and xi#2 Whether xj is a numeral, for j = (i?
3) .
.
.
i#3 The char.
bigrams starting at (i?
2) .
.
.
i#4 The Pinyin of char.
xi?1 and xi#5 The Pinyin bigrams starting at (i?
2) .
.
.
i#6 Whether xj = xj+1, for j = (i?
2) .
.
.
i#7 Whether xj = xj+2, for j = (i?
3) .
.
.
i#8 Whether xj is uppercase, for j = (i?
3) .
.
.
i#9 Whether xj is lowercase, for j = (i?
3) .
.
.
i#10 The char.
3-grams starting at (i?
3) .
.
.
i#11 The char.
4-grams starting at (i?
4) .
.
.
iTable 1: Language-independent features (#1 to#3), Chinese-specific features (#4 through #7), andEnglish-specific features (#8 through #11).the other hand, such duplication detection featuresare not so useful for English abbreviations.Feature templates #8?#11 are designed for En-glish abbreviations.
Features #8 and #9 encode theorthographic information of expanded forms.
Fea-tures #10 and #11 represent a contextual n-gramwith a large window size.
Since the number ofletters in Chinese (more than 10K characters) ismuch larger than the number of letters in English(26 letters), in order to avoid a possible overfittingproblem, we did not apply these feature templatesto Chinese abbreviations.Feature templates are instantiated with valuesthat occur in positive training examples.
We usedall of the instantiated features because we foundthat the low-frequency features also improved theperformance.3 ExperimentsFor Chinese abbreviation generation, we used thecorpus of Sun et al (2008), which contains 2,914abbreviation definitions for training, and 729 pairsfor testing.
This corpus consists primarily of nounphrases (38%), organization names (32%), andverb phrases (21%).
For English abbreviation gen-eration, we evaluated the corpus of Tsuruoka etal.
(2005).
This corpus contains 1,200 alignedpairs extracted from MEDLINE biomedical ab-stracts (published in 2001).
For both tasks, weconverted the aligned pairs of the corpora into la-beled full forms and used the labeled full forms asthe training/evaluation data.The evaluation metrics used in the abbreviationgeneration are exact-match accuracy (hereinafteraccuracy), including top-1 accuracy, top-2 accu-racy, and top-3 accuracy.
The top-N accuracy rep-resents the percentage of correct abbreviations thatare covered, if we take the top N candidates fromthe ranked labelings of an abbreviation generator.We implemented the DPLVM in C++ and op-timized the system to cope with large-scale prob-lems.
We employ the feature templates defined inSection 2.3, taking into account these 81,827 fea-tures for the Chinese abbreviation generation task,and the 50,149 features for the English abbrevia-tion generation task.For numerical optimization, we performed agradient descent with the Limited-Memory BFGS(L-BFGS) optimization technique (Nocedal andWright, 1999).
L-BFGS is a second-orderQuasi-Newton method that numerically estimatesthe curvature from previous gradients and up-dates.
With no requirement on specialized Hes-sian approximation, L-BFGS can handle large-scale problems efficiently.
Since the objectivefunction of the DPLVM model is non-convex,different parameter initializations normally bringdifferent optimization results.
Therefore, to ap-proach closer to the global optimal point, it isrecommended to perform multiple experiments onDPLVMs with random initialization and then se-lect a good start point.
To reduce overfitting,we employed a L2 Gaussian weight prior (Chenand Rosenfeld, 1999), with the objective function:L(?)
= ?ni=1 logP (yi|xi,?)?||?||2/?2.
Dur-ing training and validation, we set ?
= 1 for theDPLVM generators.
We also set four latent vari-ables for each label, in order to make a compro-mise between accuracy and efficiency.Note that, for the label encoding withglobal information, many label transitions (e.g.,P2S3) are actually impossible: the label tran-sitions are strictly constrained, i.e., yiyi+1 ?{PjSj,PjPj+1,SjPj+1,SjSj}.
These con-straints on the model topology (forward-backwardlattice) are enforced by giving appropriate featuresa weight of ?
?, thereby forcing all forbidden la-belings to have zero probability.
Sha and Pereira(2003) originally proposed this concept of imple-menting transition restrictions.4 Results and Discussion4.1 Chinese Abbreviation GenerationFirst, we present the results of the Chinese abbre-viation generation task, as listed in Table 2.
Toevaluate the impact of using latent variables, wechose the baseline system as the DPLVM, in whicheach label has only one latent variable.
Since this908Model T1A T2A T3A TimeHeu (S08) 41.6 N/A N/A N/AHMM (S08) 46.1 N/A N/A N/ASVM (S08) 62.7 80.4 87.7 1.3 hCRF 64.5 81.1 88.7 0.2 hCRF+GI 66.8 82.5 90.0 0.5 hDPLVM 67.6 83.8 91.3 0.4 hDPLVM+GI (*) 72.3 87.6 94.9 1.1 hTable 2: Results of Chinese abbreviation gener-ation.
T1A, T2A, and T3A represent top-1, top-2, and top-3 accuracy, respectively.
The systemmarked with the * symbol is the recommendedsystem.special case of the DPLVM is exactly the CRF(see Section 2.1), this case is hereinafter denotedas the CRF.
We compared the performance of theDPLVM with the CRFs and other baseline sys-tems, including the heuristic system (Heu), theHMM model, and the SVM model described inS08, i.e., Sun et al (2008).
The heuristic methodis a simple rule that produces the initial characterof each word to generate the corresponding abbre-viation.
The SVM method described by Sun et al(2008) is formalized as a regression problem, inwhich the abbreviation candidates are scored andranked.The results revealed that the latent variablemodel significantly improved the performanceover the CRF model.
All of its top-1, top-2,and top-3 accuracies were consistently better thanthose of the CRF model.
Therefore, this demon-strated the effectiveness of using the latent vari-ables in Chinese abbreviation generation.As the case for the two alternative approachesfor incorporating non-local information, the la-tent variable method and the label encodingmethod competed with one another (see DPLVMvs.
CRF+GI).
The results showed that the la-tent variable method outperformed the GI encod-ing method by +0.8% on the top-1 accuracy.
Thereason for this could be that the label encoding ap-proach is a solution without the adaptivity on dif-ferent instances.
We will present a detailed discus-sion comparing DPLVM and CRF+GI for the En-glish abbreviation generation task in the next sub-section, where the difference is more significant.In contrast, to a larger extent, the results demon-strate that these two alternative approaches arecomplementary.
Using the GI encoding furtherimproved the performance of the DPLVM (with+4.7% on top-1 accuracy).
We found that major?
?
?
?
?
?
?P S P S P S PP1 S1 P2 S2 S2 S2 P3State Tobacco Monopoly Administration DPLVM DPLVM+GI ????
[Wrong]???
[Correct]Figure 4: An example of the results.010203040506070800  1  2  3  4  5  6Percentage(%)Length of Produced Abbr.Gold TrainGold TestDPLVMDPLVM+GIFigure 5: Percentage distribution of Chineseabbreviations/Viterbi-labelings grouped by length.improvements were achieved through the more ex-act control of the output length.
An example isshown in Figure 4.
The DPLVM made correct de-cisions at three positions, but failed to control theabbreviation length.2 The DPLVM+GI succeededon this example.
To perform a detailed analysis,we collected the statistics of the length distribution(see Figure 5) and determined that the GI encod-ing improved the abbreviation length distributionof the DPLVM.In general, the results indicate that all of the se-quential labeling models outperformed the SVMregression model with less training time.3 In theSVM regression approach, a large number of neg-ative examples are explicitly generated for thetraining, which slowed the process.The proposed method, the latent variable modelwith GI encoding, is 9.6% better with respect tothe top-1 accuracy compared to the best system onthis corpus, namely, the SVM regression method.Furthermore, the top-3 accuracy of the latent vari-able model with GI encoding is as high as 94.9%,which is quite encouraging for practical usage.4.2 English Abbreviation GenerationIn the English abbreviation generation task, werandomly selected 1,481 instances from the gen-2The Chinese abbreviation with length = 4 should havea very low probability, e.g., only 0.6% of abbreviations withlength = 4 in this corpus.3On Intel Dual-Core Xeon 5160/3 GHz CPU, excludingthe time for feature generation and data input/output.909Model T1A T2A T3A TimeCRF 55.8 65.1 70.8 0.3 hCRF+GI 52.7 63.2 68.7 1.3 hCRF+GIB 56.8 66.1 71.7 1.3 hDPLVM 57.6 67.4 73.4 0.6 hDPLVM+GI 53.6 63.2 69.2 2.5 hDPLVM+GIB (*) 58.3 N/A N/A 3.0 hTable 3: Results of English abbreviation genera-tion.somatosensory evoked potentials(a) P1P2 P3 P4 P5 SMEPS(b) P P P P SEPS(a): CRF+GI with p=0.001 [Wrong](b): DPLVM with p=0.191 [Correct]Figure 6: A result of ?CRF+GI vs. DPLVM?.
Forsimplicity, the S labels are masked.eration corpus for training, and 370 instances fortesting.
Table 3 shows the experimental results.We compared the performance of the DPLVMwith the performance of the CRFs.
Whereas theuse of the latent variables still significantly im-proves the generation performance, using the GIencoding undermined the performance in this task.In comparing the implicit and explicit solutionsfor incorporating non-local information, we cansee that the implicit approach (the DPLVM) per-forms much better than the explicit approach (theGI encoding).
An example is shown in Figure 6.The CRF+GI produced a Viterbi labeling with alow probability, which is an incorrect abbrevia-tion.
The DPLVM produced the correct labeling.To perform a systematic analysis of thesuperior-performance of DPLVM compare toCRF+GI, we collected the probability distribu-tions (see Figure 7) of the Viterbi labelings fromthese models (?DPLVM vs. CRF+GI?
is high-lighted).
The curves suggest that the data sparse-ness problem could be the reason for the differ-ences in performance.
A large percentage (37.9%)of the Viterbi labelings from the CRF+GI (ENG)have very small probability values (p < 0.1).For the DPLVM (ENG), there were only a few(0.5%) Viterbi labelings with small probabilities.Since English abbreviations are often longer thanChinese abbreviations (length < 10 in English,whereas length < 5 in Chinese4), using the GIencoding resulted in a larger label set in English.4See the curve DPLVM+GI (CHN) in Figure 7, whichcould explain the good results of GI encoding for the Chi-nese task.010203040500  0.2  0.4  0.6  0.8  1Percentage(%)Probability of Viterbi labelingCRF (ENG)CRF+GI (ENG)DPLVM (ENG)DPLVM+GI (ENG)DPLVM+GI (CHN)Figure 7: For various models, the probability dis-tributions of the produced abbreviations on the testdata of the English abbreviation generation task.mitomycin CDPLVM P P MC [Wrong]DPLVM+GI P1 P2 P3 MMC [Correct]Figure 8: Example of abbreviations composedof non-initials generated by the DPLVM and theDPLVM+GI.Hence, the features become more sparse than inthe Chinese case.5 Therefore, a significant numberof features could have been inadequately trained,resulting in Viterbi labelings with low probabili-ties.
For the latent variable approach, its curvedemonstrates that it did not cause a severe datasparseness problem.The aforementioned analysis also explains thepoor performance of the DPLVM+GI.
However,the DPLVM+GI can actually produce correct ab-breviations with ?believable?
probabilities (highprobabilities) in some ?difficult?
instances.
InFigure 8, the DPLVM produced an incorrect la-beling for the difficult long form, whereas theDPLVM+GI produced the correct labeling con-taining non-initials.Hence, we present a simple voting method tobetter combine the latent variable approach withthe GI encoding method.
We refer to this newcombination as GI encoding with ?back-off?
(here-inafter GIB): when the abbreviation generated bythe DPLVM+GI has a ?believable?
probability(p > 0.3 in the present case), the DPLVM+GIthen outputs it.
Otherwise, the system ?backs-off?5In addition, the training data of the English task is muchsmaller than for the Chinese task, which could make the mod-els more sensitive to data sparseness.910Model T1A TimeCRF+GIB 67.2 0.6 hDPLVM+GIB (*) 72.5 1.4 hTable 4: Re-evaluating Chinese abbreviation gen-eration with GIB.Model T1AHeu (T05) 47.3MEMM (T05) 55.2DPLVM (*) 57.5Table 5: Results of English abbreviation genera-tion with five-fold cross validation.to the parameters trained without the GI encoding(i.e., the DPLVM).The results in Table 3 demonstrate that theDPVLM+GIB model significantly outperformedthe other models because the DPLVM+GI modelimproved the performance in some ?difficult?
in-stances.
The DPVLM+GIB model was robusteven when the data sparseness problem was se-vere.By re-evaluating the DPLVM+GIB model forthe previous Chinese abbreviation generation task,we demonstrate that the back-off method also im-proved the performance of the Chinese abbrevia-tion generators (+0.2% from DPLVM+GI; see Ta-ble 4).Furthermore, for interests, like Tsuruoka et al(2005), we performed a five-fold cross-validationon the corpus.
Concerning the training time inthe cross validation, we simply chose the DPLVMfor comparison.
Table 5 shows the results of theDPLVM, the heuristic system (Heu), and the max-imum entropy Markov model (MEMM) describedby Tsuruoka et al (2005).5 Recognition as a Generation TaskWe directly migrate this model to the abbrevia-tion recognition task.
We simplify the abbrevia-tion recognition to a restricted generation problem(see Figure 9).
When a context expression (CE)with a parenthetical expression (PE) is met, therecognizer generates the Viterbi labeling for theCE, which leads to the PE or NULL.
Then, if theViterbi labeling leads to the PE, we can, at thesame time, use the labeling to decide the full formwithin the CE.
Otherwise, NULL indicates that thePE is not an abbreviation.For example, in Figure 9, the recognition is re-stricted to a generation task with five possible la-... cannulate for arterial pressure (AP)...(1) P P AP(2) P P AP(3) P P AP(4) P P AP(5) SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS NULLFigure 9: Abbreviation recognition as a restrictedgeneration problem.
In some labelings, the S la-bels are masked for simplicity.Model P R FSchwartz & Hearst (SH) 97.8 94.0 95.9SaRAD 89.1 91.9 90.5ALICE 96.1 92.0 94.0Chang & Schu?tze (CS) 94.2 90.0 92.1Nadeau & Turney (NT) 95.4 87.1 91.0Okazaki et al (OZ) 97.3 96.9 97.1CRF 89.8 94.8 92.1CRF+GI 93.9 97.8 95.9DPLVM 92.5 97.7 95.1DPLVM+GI (*) 94.2 98.1 96.1Table 6: Results of English abbreviation recogni-tion.belings.
Other labelings are impossible, becausethey will generate an abbreviation that is not AP.If the first or second labeling is generated, AP isselected as an abbreviation of arterial pressure.
Ifthe third or fourth labeling is generated, then APis selected as an abbreviation of cannulate for ar-terial pressure.
Finally, the fifth labeling (NULL)indicates that AP is not an abbreviation.To evaluate the recognizer, we use the corpus6of Okazaki et al (2008), which contains 864 ab-breviation definitions collected from 1,000 MED-LINE scientific abstracts.
In implementing therecognizer, we simply use the model from the ab-breviation generator, with the same feature tem-plates (31,868 features) and training method; themajor difference is in the restriction (according tothe PE) of the decoding stage and penalizing theprobability values of the NULL labelings7.For the evaluation metrics, following Okazakiet al (2008), we use precision (P = k/m), re-call (R = k/n), and the F-score defined by6The previous abbreviation generation corpus is improperfor evaluating recognizers, and there is no related research onthis corpus.
In addition, there has been no report of Chineseabbreviation recognition because there is no data available.The previous generation corpus (Sun et al, 2008) is improperbecause it lacks local contexts.7Due to the data imbalance of the training corpus, wefound the probability values of the NULL labelings are ab-normally high.
To deal with this imbalance problem, we sim-ply penalize all NULL labelings by using p = p?
0.7.911Model P R FCRF+GIB 94.0 98.9 96.4DPLVM+GIB 94.5 99.1 96.7Table 7: English abbreviation recognition withback-off.2PR/(P + R), where k represents #instances inwhich the system extracts correct full forms, mrepresents #instances in which the system extractsthe full forms regardless of correctness, and n rep-resents #instances that have annotated full forms.Following Okazaki et al (2008), we perform 10-fold cross validation.We prepared six state-of-the-art abbreviationrecognizers as baselines: Schwartz and Hearst?smethod (SH) (2003), SaRAD (Adar, 2004), AL-ICE (Ao and Takagi, 2005), Chang and Schu?tze?smethod (CS) (Chang and Schu?tze, 2006), Nadeauand Turney?s method (NT) (Nadeau and Turney,2005), and Okazaki et al?s method (OZ) (Okazakiet al, 2008).
Some methods use implementationson the web, including SH8, CS9, and ALICE10.The results of other methods, such as SaRAD, NT,and OZ, are reproduced for this corpus based ontheir papers (Okazaki et al, 2008).As can be seen in Table 6, using the latent vari-ables significantly improved the performance (seeDPLVM vs. CRF), and using the GI encodingimproved the performance of both the DPLVMand the CRF.
With the F-score of 96.1%, theDPLVM+GI model outperformed five of six state-of-the-art abbreviation recognizers.
Note that allof the six systems were specifically designed andoptimized for this recognition task, whereas theproposed model is directly transported from thegeneration task.
Compared with the generationtask, we find that the F-measure of the abbrevia-tion recognition task is much higher.
The majorreason for this is that there are far fewer classifi-cation candidates of the abbreviation recognitionproblem, as compared to the generation problem.For interests, we also tested the effect of theGIB approach.
Table 7 shows that the back-offmethod further improved the performance of boththe DPLVM and the CRF model.8http://biotext.berkeley.edu/software.html9http://abbreviation.stanford.edu/10http://uvdb3.hgc.jp/ALICE/ALICE index.html6 Conclusions and Future ResearchWe have presented the DPLVM and GI encod-ing by which to incorporate non-local informationin abbreviating terms.
They were competing andgenerally the performance of the DPLVM was su-perior.
On the other hand, we showed that the twoapproaches were complementary.
By combiningthese approaches, we were able to achieve state-of-the-art performance in abbreviation generationand recognition in the same model, across differ-ent languages, and with a simple feature set.
Asdiscussed earlier herein, the training data is rela-tively small.
Since there are numerous unlabeledfull forms on the web, it is possible to use a semi-supervised approach in order to make use of suchraw data.
This is an area for future research.AcknowledgmentsWe thank Yoshimasa Tsuruoka for providing theEnglish abbreviation generation corpus.
We alsothank the anonymous reviewers who gave help-ful comments.
This work was partially supportedby Grant-in-Aid for Specially Promoted Research(MEXT, Japan).ReferencesEytan Adar.
2004.
SaRAD: A simple and robust ab-breviation dictionary.
Bioinformatics, 20(4):527?533.Hiroko Ao and Toshihisa Takagi.
2005.
ALICE: Analgorithm to extract abbreviations from MEDLINE.Journal of the American Medical Informatics Asso-ciation, 12(5):576?586.June A. Barrett and Mandalay Grems.
1960.
Abbrevi-ating words systematically.
Communications of theACM, 3(5):323?324.Charles P. Bourne and Donald F. Ford.
1961.
A studyof methods for systematically abbreviating englishwords and names.
Journal of the ACM, 8(4):538?552.Jeffrey T. Chang and Hinrich Schu?tze.
2006.
Abbre-viations in biomedical text.
In Sophia Ananiadouand John McNaught, editors, Text Mining for Biol-ogy and Biomedicine, pages 99?119.
Artech House,Inc.Stanley F. Chen and Ronald Rosenfeld.
1999.
A gaus-sian prior for smoothing maximum entropy models.Technical Report CMU-CS-99-108, CMU.Yaakov HaCohen-Kerner, Ariel Kass, and Ariel Peretz.2008.
Combined one sense disambiguation of ab-breviations.
In Proceedings of ACL?08: HLT, ShortPapers, pages 61?64, June.912Louis-Philippe Morency, Ariadna Quattoni, and TrevorDarrell.
2007.
Latent-dynamic discriminative mod-els for continuous gesture recognition.
Proceedingsof CVPR?07, pages 1?8.David Nadeau and Peter D. Turney.
2005.
A super-vised learning approach to acronym identification.In the 8th Canadian Conference on Artificial Intelli-gence (AI?2005) (LNAI 3501), page 10 pages.Jorge Nocedal and Stephen J. Wright.
1999.
Numeri-cal optimization.
Springer.Naoaki Okazaki, Sophia Ananiadou, and Jun?ichi Tsu-jii.
2008.
A discriminative alignment model forabbreviation recognition.
In Proceedings of the22nd International Conference on ComputationalLinguistics (COLING?08), pages 657?664, Manch-ester, UK.Serguei Pakhomov.
2002.
Semi-supervised maximumentropy based approach to acronym and abbreviationnormalization in medical texts.
In Proceedings ofACL?02, pages 160?167.Youngja Park and Roy J. Byrd.
2001.
Hybrid text min-ing for finding abbreviations and their definitions.
InProceedings of EMNLP?01, pages 126?133.Leonid Peshkin and Avi Pfeffer.
2003.
Bayesian in-formation extraction network.
In Proceedings of IJ-CAI?03, pages 421?426.Slav Petrov and Dan Klein.
2008.
Discriminative log-linear grammars with latent variables.
Proceedingsof NIPS?08.Ariel S. Schwartz and Marti A. Hearst.
2003.
A simplealgorithm for identifying abbreviation definitions inbiomedical text.
In the 8th Pacific Symposium onBiocomputing (PSB?03), pages 451?462.Fei Sha and Fernando Pereira.
2003.
Shallow pars-ing with conditional random fields.
Proceedings ofHLT/NAACL?03.Xu Sun, Houfeng Wang, and Bo Wang.
2008.
Pre-dicting chinese abbreviations from definitions: Anempirical learning approach using support vector re-gression.
Journal of Computer Science and Tech-nology, 23(4):602?611.Kazem Taghva and Jeff Gilbreth.
1999.
Recogniz-ing acronyms and their definitions.
InternationalJournal on Document Analysis and Recognition (IJ-DAR), 1(4):191?198.Yoshimasa Tsuruoka, Sophia Ananiadou, and Jun?ichiTsujii.
2005.
A machine learning approach toacronym generation.
In Proceedings of the ACL-ISMB Workshop, pages 25?31.Jonathan D. Wren and Harold R. Garner.
2002.Heuristics for identification of acronym-definitionpatterns within text: towards an automated con-struction of comprehensive acronym-definition dic-tionaries.
Methods of Information in Medicine,41(5):426?434.Hong Yu, Won Kim, Vasileios Hatzivassiloglou, andJohn Wilbur.
2006.
A large scale, corpus-based ap-proach for automatically disambiguating biomedicalabbreviations.
ACM Transactions on InformationSystems (TOIS), 24(3):380?404.913
