Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
181?189, Prague, June 2007. c?2007 Association for Computational LinguisticsImproving Query Spelling CorrectionUsing Web Search ResultsQing ChenNatural Language Processing LabNortheastern UniversityShenyang, Liaoning, China, 110004chenqing@ics.neu.edu.cnMu LiMicrosoft Research Asia5F Sigma CenterZhichun Road, Haidian  DistrictBeijing, China, 100080muli@microsoft.comMing ZhouMicrosoft Research Asia5F Sigma CenterZhichun Road, Haidian  DistrictBeijing, China, 100080mingzhou@microsoft.comAbstractTraditional research on spelling correctionin natural language processing and infor-mation retrieval literature mostly relies onpre-defined lexicons to detect spelling er-rors.
But this method does not work wellfor web query spelling correction, becausethere is no lexicon that can cover the vastamount of terms occurring across the web.Recent work showed that using searchquery logs helps to solve this problem tosome extent.
However, such approachescannot deal with rarely-used query termswell due to the data sparseness problem.
Inthis paper, a novel method is proposed foruse of web search results to improve theexisting query spelling correction modelssolely based on query logs by leveragingthe rich information on the web related tothe query and its top-ranked candidate.
Ex-periments are performed based on real-world queries randomly sampled fromsearch engine?s daily logs, and the resultsshow that our new method can achieve16.9% relative F-measure improvementand 35.4% overall error rate reduction incomparison with the baseline method.1 IntroductionNowadays more and more people are using Inter-net search engine to locate information on the web.Search engines take text queries that users type asinput, and present users with information of rankedweb pages related to users?
queries.
During thisprocess, one of the important factors that lead topoor search results is misspelled query terms.
Ac-tually misspelled queries are rather commonly ob-served in query logs, as shown in previous investi-gations into the search engine?s log data thataround 10%~15% queries were misspelled (Cucer-zan and Brill, 2004).Sometimes misspellings are due to simple typo-graphic errors such as teh for the.
In many casesthe spelling errors are more complicated cognitiveerrors such as camoflauge for camouflage.
As amatter of fact, correct spelling is not always aneasy task ?
even many Americans cannot exactlyspell out California governor?s last name: Schwar-zenegger.
A spelling correction tool can help im-prove users?
efficiency in the first case, but it ismore useful in the latter since the users cannot fig-ure out the correct spelling by themselves.There has been a long history of general-purposespelling correction research in natural languageprocessing and information retrieval literature(Kukich, 1992), but its application to web search181query is still a new challenge.
Although there aresome similarities in correction candidate genera-tion and selection, these two settings are quite dif-ferent in one fundamental problem: How to deter-mine the validity of a search term.
Traditionally,the measure is mostly based on a pre-defined spel-ling lexicon ?
all character strings that cannot befound in the lexicon are judged to be invalid.
How-ever, in the web search context, there is little hopethat we can construct such a lexicon with idealcoverage of web search terms.
For example, evenmanually collecting a full list of car names andcompany names will be a formidable task.To obtain more accurate understanding of thisproblem, we performed a detailed investigationover one week?s MSN daily query logs, amongwhich found that 16.5% of search terms are out ofthe scope of our spelling lexicon containing around200,000 entries.
In order to get more specific num-bers, we also manually labeled a query data set thatcontains 2,323 randomly sampled queries and6,318 terms.
In this data set, the ratio of out-of-vocabulary (OOV) terms is 17.4%, which is verysimilar to the overall distribution.
However, only25.3% of these OOV terms are identified to bemisspelled, which occupy 85% of the overall spel-ling errors.
All these statistics indicate that accu-rate OOV term classification is of crucial impor-tance to good query spelling correction perfor-mance.Cucerzan and Brill (2004) first investigated thisissue and proposed to use query logs to infer cor-rect spellings of misspelled terms.
Their principlecan be summarized as follows: given an inputquery string q, finding a more probable query cthan q within a confusion set of q, in which the editdistance between each element and q is less than agiven threshold.
They reported good recall formisspelled terms, but without detailed discussionson accurate classification of valid out-of-vocabulary terms and misspellings.
In Li?s work,distributional similarity metrics estimated fromquery logs were proposed to be used to discrimi-nate high-frequent spelling errors such as massen-ger from valid out-of-vocabulary terms such asbiocycle.
But this method suffers from the datasparseness problem: sufficient amounts of occur-rences of every possible misspelling and validterms are required to make good estimation of dis-tributional similarity metrics; thus this methoddoes not work well for rarely-used out-of-vocabulary search terms and uncommon misspel-lings.In this paper we propose to use web search re-sults to further improve the performance of queryspelling correction models.
The key contribution ofour work is to identify that the dynamic onlinesearch results can serve as additional evidence todetermine users?
intended spelling of a given term.The information in web search results we used in-cludes the number of pages matched for the query,the term distribution in the web page snippets andURLs.
We studied two schemes to make use of thereturning results of a web search engine.
The firstone only exploits indicators of the input query?sreturning results, while the other also looks at otherpotential correction candidate?s search results.
Weperformed extensive evaluations on a query setrandomly sampled from search engines?
dailyquery logs, and experimental results show that wecan achieve 35.4% overall error rate reduction and18.2% relative F-measure improvement on OOVmisspelled terms.The rest of the paper is structured as follows.Section 2 details other related work of spelling cor-rection research.
In section 3, we show the intuitivemotivations to use web search results for the queryspelling correction.
After presenting the formalstatement of the query spelling correction problemin Section 4, we describe our approaches that usemachine learning methods to integrate statisticalfeatures from web search results in Section 5.
Wepresent our evaluation methods for the proposedmethods and analyze their performance in Section6.
Section 7 concludes the paper.2 Related WorkSpelling correction models in most previous workwere constructed based on conventional task set-tings.
Based on the focus of these task settings, twolines of research have been applied to deal withnon-word errors and real-word errors respectively.Non-word error spelling correction is focused onthe task of generating and ranking a list of possiblespelling corrections for each word not existing in aspelling lexicon.
Traditionally candidate ranking isbased on manually tuned scores such as assigningalternative weights to different edit operations orleveraging candidate frequencies (Damerau, 1964;Levenshtein, 1966).
In recent years, statisticalmodels have been widely used for the tasks of nat-182ural language processing, including spelling cor-rection task.
(Brill and Moore, 2000) presented animproved error model over the one proposed by(Kernighan et al, 1990) by allowing genericstring-to-string edit operations, which helps withmodeling major cognitive errors such as the confu-sion between le and al.
Via explicit modeling ofphonetic information of English words, (Toutanovaand Moore, 2002) further investigated this issue.Both of them require misspelled/correct word pairsfor training, and the latter also needs a pronuncia-tion lexicon, but recently (Ahmad and Kondrak,2005) demonstrated that it is also possible to learnsuch models automatically from query logs withthe EM algorithm, which is similar to work of(Martin, 2004), learning from a very large corpusof raw text for removing non-word spelling errorsin large corpus.
All the work for non-word spellingcorrection focused on the current word itself with-out taking into account contextual information.Real-word spelling correction is also referred tobe context sensitive spelling correction (CSSC),which tries to detect incorrect usage of valid wordsin certain contexts.
Using a pre-defined confusionset is a common strategy for this task, such as inthe work of (Golding and Roth, 1996) and (Manguand Brill, 1997).
Opposite to non-word spellingcorrection, in this direction only contextual evi-dences were taken into account for modeling byassuming all spelling similarities are equal.The complexity of query spelling correction taskrequires the combination of these types of evidence,as done in (Cucerzan and Brill, 2004; Li et al,2006).
One important contribution of our work isthat we use web search results as extended contex-tual information beyond query strings by takingadvantage of application specific knowledge.
Al-though the information used in our methods can allbe accessed in a search engine?s web archive, sucha strategy involves web-scale data processingwhich is a big engineering challenge, while ourmethod is a light-weight solution to this issue.3 MotivationWhen a spelling correction model tries to make adecision whether to make a suggestion c to a queryq, it generally needs to leverage two types of evi-dence: the similarity between c and q, and the va-lidity plausibility of c and q.
All the previous workestimated plausibility of a query based on thequery string itself ?
typically it is represented asthe string probability, which is further decomposedinto production of consecutive n-gram probabilities.For example, both the work of (Cucerzan and Brill,2004; Li et al, 2006) used n-gram statistical lan-guage models trained from search engine?s querylogs to estimate the query string probability.In the following, we will show that the searchresults for a query can serve as a feedback mechan-ism to provide additional evidences to make betterspelling correction decisions.
The usefulness ofweb search results can be two-fold:First, search results can be used to validatequery terms, especially those not popular enoughin query logs.
One case is the validation for navi-gational queries (Broder, 2004).
Navigational que-ries usually contain terms that are key parts of des-tination URLs, which may be out-of-vocabularyterms since there are millions of sites on the web.Because some of these navigational terms are veryrelatively rare in query logs, without knowledge ofthe special navigational property of a term, a queryspelling correction model might confuse them withother low-frequency misspellings.
But such infor-mation can be effectively obtained from the URLsof retrieved web pages.
Inferring navigational que-ries through term-URL matching thus can help re-duce the chance that the spelling correction modelchanges an uncommon web site name into popularsearch term, such as from innovet to innovate.Another example is that search results can be usedin identifying acronyms or other abbreviations.
Wecan observe some clear text patterns that relate ab-breviations to their full spellings in the search re-sults as shown in Figure 1.
But such mappingscannot easily be obtained from query logs.Figure 1.
Sample search results for SARSSecond, search results can help verify correctioncandidates.
The terms appearing in search results,both in the web page titles and snippets, provideadditional evidences for users intention.
For exam-ple, if a user searches for a misspelled query vac-cum cleaner on a search engine, it is very likelythat he will obtain some search results containingthe correct term vacuum as shown in Figure 2.
This183can be attributed to the collective link text distribu-tion on the web ?
many links with misspelled textpoint to sites with correct spellings.
Such evi-dences can boost the confidence of a spelling cor-rection model to suggest vacuum as a correction.Figure 2.
Sample search resultsfor vaccum cleanerThe number of matched pages can be used tomeasure the popularity of a query on the web,which is similar to term frequencies occurring inquery logs, but with broader coverage.
Poor cor-rection candidates can usually be verified by asmaller number of matched web pages.Another observation is that the documents re-trieved with correctly-spelled query and misspelledones are similar to some extent in the view of termdistribution.
Both the web retrieval results of va-cuum and vaccum contain terms such as cleaner,pump, bag or systems.
We can take this similarityas an evidence to verify the spelling correction re-sults.4 Problem StatementGiven a query q, a spelling correction model is tofind a query string c that maximizes the posteriorprobability of c given q within the confusion set ofq.
Formally we can write this as follows:??
= ???????????(?|?)
(1)where C is the confusion set of q.
Each querystring c in the confusion set is a correction candi-date for q, which satisfies the constraint that thespelling similarity between c and q is within giventhreshold ?.In this formulation, the error detection and cor-rection are performed in a unified way.
The queryq itself always belongs to its confusion set C, andwhen the spelling correction model identifies amore probable query string c in C which is differ-ent from q, it claims a spelling error detected andmakes a correction suggestion c.There are two tasks in this framework.
One ishow to learn a statistical model to estimate theconditional probability ??
?
?
, and the other ishow to generate confusion set C of a given query q4.1 Maximum Entropy Model for QuerySpelling CorrectionWe take a feature-based approach to model theposterior probability ??
?
?
.
Specifically we usethe maximum entropy model (Berger et al, 1996)for this task:??
?
?
=exp  ????
?, ??
?=1exp( ????
(?, ?)?
?=1 )?
(2)where exp( ????
(?, ?)?
?=1 )?
is the normalizationfactor; ??
?, ?
is a feature function defined overquery q and correction candidate c , while ??
is thecorresponding feature weight.
??
can be optimizedusing the numerical optimization algorithms suchas the Generalized Iterative Scaling (GIS) algo-rithm (Darroch and Ratcliff, 1972) by maximizingthe posterior probability of the training set whichcontains a manually labeled set of query-truth pairs:??
= argmax ?
,?
log???(?|?)
(3)The advantage of maximum entropy model isthat it provides a natural way and unified frame-work to integrate all available information sources.This property is well fit for our task in which weare using a wide variety of evidences based on lex-icon, query log and web search results.4.2 Correction Candidate GenerationCorrection candidate generation for a query q canbe decomposed into two phases.
In the first phase,correction candidates are generated for each termin the query from a term-base extracted from querylogs.
This task can leverage conventional spellingcorrection methods such as generating candidatesbased on edit distance (Cucerzan and Brill, 2004)or phonetic similarity (Philips, 1990).
Then thecorrection candidates of the entire query are gener-ated by composing the correction candidates ofeach individual term.
Let  ?
= ?1???
, and theconfusion set of ??
is  ??
?
, then the confusion setof q is ??1???2??????1.
For example, for aquery  ?
= ?1?2 , ?1  has candidates ?11  and ?12 ,while ?2 has candidates ?21and ?22, then the con-fusion set C is {?11?21 , ?11?22 , ?12?21 , ?12?22}.1 For denotation simplicity, we do not cover compound andcomposition errors here.184The problem of this method is the size of confu-sion set C may be huge for multi-term queries.
Inpractice, one term may have hundreds of possiblecandidates, then a query containing several termsmay have millions.
This might lead to impracticalsearch and training using the maximum entropymodeling method.
Our solution to this problem isto use candidate pruning.
We first roughly rank thecandidates based on the statistical n-gram languagemodel estimated from query logs.
Then we onlychoose a subset of C that contains a specifiednumber of top-ranked (most probable) candidatesto present to the maximum entropy model for of-fline training and online re-ranking, and the num-ber of candidates is used as a parameter to balancetop-line performance and run-time efficiency.
Thissubset can be efficiently generated as shown in (Liet al, 2006).5 Web Search Results based Query Spel-ling CorrectionIn this section we will describe in detail the me-thods for use of web search results in the queryspelling correction task.
In our work we studiedtwo schemes.
The first one only employs indicatorsof the input query?s search results, while the otheralso looks at the most probable correction candi-dates?
search results.
For each scheme, we extractadditional scheme-specific features from the avail-able search results, combine them with baselinefeatures and construct a new maximal model toperform candidate ranking.5.1 Baseline modelWe denote the maximum entropy model based onbaseline model feature set as M0 and the featureset S0 derived from the latest state of the art worksof (Li et al, 2006), where S0 includes the featuresmostly concerning the statistics of the query termsand the similarities between query terms and theircorrection candidates.5.2 Scheme 1: Using search results for inputquery onlyIn this scheme we build more features for each cor-rection candidate (including input query q itself)by distilling more evidence from the search resultsof the query.
S1 denotes the augmented feature set,and M1 denotes the maximum entropy modelbased on S1.
The features are listed as follows:1.
Number of pages returned: the number ofweb search pages retrieved by a web searchengine, which is used to estimate the popu-larity of query.
This feature is only for q.2.
URL string: Binary features indicatingwhether the combination of terms of eachcandidate is in the URLs of top retrieveddocuments.
This feature is for all candidates.3.
Frequency of correction candidate term:the number of occurrences of modifiedterms in the correction candidate found inthe title and snippet of top retrieved docu-ments based on the observation that correc-tion terms possibly co-occur with theirmisspelled ones.
This feature is invalid for q.4.
Frequency of query term: the number ofoccurrences of each term of q found in thetitle or snippet of the top retrieved docu-ments, based on the observation that the cor-rect terms always appear frequently in theirsearch results.5.
Abbreviation pattern: Binary features indi-cating whether inputted query terms mightbe abbreviations according to text patterns insearch results.5.3 Scheme 2: Using both search results ofinput query and top-ranked candidateIn this scheme we extend the use of search resultsboth for query q and for top-ranked candidate cother than q determined by M1.
First we submit aquery to a search engine for the initial retrieval toobtain one set of search results ??
, then use M1 tofind the best correction candidate c other than q.Next we perform a second retrieval with c to ob-tain another set of search results ??
.
Finally addi-tional features are generated for each candidatebased on ??
, then a new maximum entropy modelM2 is built to re-rank the candidates for a secondtime.
The entire process can be schematicallyshown in Figure 3.Figure 3.
Relations of models and featuresLexicon / queryLogs SpellingSimilarity?
?
???
?
?
?S0 featuresS1 specificfeaturesS2 specificfeaturesModel M1Model M2Model M0185where ??
is the web search results of query q; ??
isthe web search results of c which is the top-rankedcorrection of q suggested by model M1.The new feature set denoted with S2 is a set ofdocument similarities between ??
and ??
, whichincludes different similarity estimations betweenthe query and its correction at the document levelusing merely cosine measure based on term fre-quency vectors of ??
and ??
.6 Experiments6.1 Evaluation MetricsIn our work, we consider the following four typesof evaluation metrics:?
Accuracy: The number of correct outputsproposed by the spelling correction model di-vided by the total number of queries in the testset?
Recall: The number of correct suggestions formisspelled queries by the spelling correctionmodel divided by the total number of miss-pelled queries in the test set?
Precision: The number of correct suggestionsfor misspelled queries proposed by the spel-ling correction model divided by the totalnumber of suggestions made by the system?
F-measure: Formula ?
= 2??/(?
+ ?)
usedfor calculating the f-measure, which is essen-tially the harmonic mean of recall and preci-sionAny individual metric above might not be suffi-cient to indicate the overall performance of a queryspelling correction model.
For example, as in mostretrieval tasks, we can trade recall for precision orvice versa.
Although intuitively F might be in ac-cordance with accuracy, there is no strict theoreti-cal relation between these two numbers ?
there areconditions under which accuracy improves whileF-measure may drop or be unchanged.6.2 Experimental SetupWe used a manually constructed data set as goldstandard for evaluation.
First we randomly sam-pled 7,000 queries from search engine?s dailyquery logs of different time periods, and had themmanually labeled by two annotators independently.Each query is attached to a truth, which is eitherthe query itself for valid queries, or a spelling cor-rection for misspelled ones.
From the annotationresults that both annotators agreed with each other,we extracted 2,323 query-truth pairs as training setand 991 as test set.
Table 1 shows the statistics ofthe data sets, in which Eq denotes the error rate ofquery and Et denotes the error rate of term.# queries # termsqEtETraining set 2,323 6,318 15.0% 5.6%Test set 991 2,589 12.8% 5.2%Table 1.
Statistics of training set and test setIn the following experiments, at most 50 correc-tion candidates were used in the maximum entropymodel for each query if there is no special explana-tion.
The web search results were fetched fromMSN?s search engine.
By default, top 100 re-trieved items from the web retrieval results wereused to perform feature extraction.
A set of querylog data spanning 9 months are used for collectingstatistics required by the baseline.6.3 Overall ResultsFollowing the method as described in previous sec-tions, we first ran a group of experiments to eva-luate the performance of each model we discussedwith default settings.
The detailed results areshown in Table 2.Model Accuracy Recall Precision FM0 91.8% 60.6% 62.6% 0.616M1 93.9% 64.6% 77.4% 0.704M2 94.7% 66.9% 78.0% 0.720Table 2.
Overall ResultsFrom the table we can observe significant per-formance boosts on all evaluation metrics of M1and M2 over M0.We can achieve 25.6% error rate reduction and23.6% improvement in precision, as well as 6.6%relative improvement in recall, when adding S1 toM1.
Paired t-test gives p-value of 0.002, which issignificant to 0.01 level.M2 can bring additional 13.1% error rate reduc-tion and moderate improvement in precision, aswell as 3.6% improvement in recall over M1, withpaired t-test showing that the improvement is sig-nificant to 0.01 level.1866.4 Impact of Candidate numberTheoretically the number of correction candidatesin the confusion set determines the accuracy andrecall upper bounds for all models concerned inthis paper.
Performance might be hurt if we use atoo small candidate number, which is because thecorrections are separated from the confusion sets.These curves shown in Figure 4 and 5, includeboth theoretical bound (oracle) and actual perfor-mance of our described models.
From the chart wecan see that our models perform best when ??
isaround 50, and when ??
> 15 the oracle recall andaccuracy almost stay unchanged, thus the actualmodels?
performance only benefits a little fromlarger ??
values.
The missing part of recall islargely due to the fact that we are not able to gen-erate truth candidates for some weird query termsrather than insufficient size of confusion set.Figure 4.
Recall versus candidate numberFigure 5.
Accuracy versus candidate number6.5 DiscussionsWe also studied the performance difference be-tween in-vocabulary (IV) and out-of-vocabulary(OOV) terms when using different spelling correc-tion models.
The detailed results are shown in Ta-ble 3 and Table 4.Accuracy Precision  Recall FM0 88.2% 77.1% 67.3% 0.718M1 92.4% 88.5% 77.3% 0.825M2 93.2% 91.6% 79.1% 0.849Table 3.
OOV Term ResultsAccuracy Precision  Recall FM0 98.8% 44.0% 45.8% 0.449M1 99.0% 62.5% 20.8% 0.313M2 99.1% 75.0% 37.5% 0.500Table 4.
IV Term ResultsThe results show that M1 is very powerful toidentify and correct OOV spelling errors comparedwith M0.
Actually M1 is able to correct spellingerrors such as guiness, whose frequency in querylog is even higher than its truth spelling guinness.Since most spelling errors are OOV terms, this ex-plains why the model M1 can significantly outper-form the baseline.
But for IV terms things are dif-ferent.
Although the overall accuracy is better, theF-measure of M1 is far lower than M0.
M2 per-forms best for the IV task in terms of both accura-cy and F-measure.
However, IV spelling errors isso small a portion of the total misspelling (only17.4% of total spelling errors in our test set) thatthe room for improvement is very small.
This helpsto explain why the performance gap between M1and M0 is much larger than the one between M2and M1, and shows the tendency that M1 prefer toidentify and correct OOV misspellings in compari-son to IV ones, which causes F-measure drop fromM0 to M1; while by introducing more useful evi-dence, M2 outperforms better for both OOV andIV terms over M0 and M1.Another set of statistics we collected from theexperiments is the performance data of low-frequency terms when using the models proposedin this paper, since we believe that our approachwould help make better classification of low-frequency search terms.
As a case study, we identi-fied in the test set al terms whose frequencies inour query logs are less than 800, and for theseterms we calculated the error reduction rate ofmodel M1 over the baseline model M0 at each in-0%10%20%30%40%50%60%70%80%90%1 2 3 4 5 6 7 8 9 10 15 20 25 30 35 40 45 50RecallCandidate numberM0 M1M2 Oracle86%88%90%92%94%96%98%100%1 2 3 4 5 6 7 8 9 10 15 20 25 30 35 40 45 50AccuracyCandidate numberM0 M1M2 Oracle187terval of 50.
The detailed results are shown in Fig-ure 6.
The clear trend can be observed that M1 canachieve larger error rate reduction over baseline forterms with lower frequencies.
This is because theperformance of baseline model drops for theseterms when there are no reliable distributional si-milarity estimations available due to data sparse-ness in query logs, while M1 can use web data toalleviate this problem.Figure 6.
Error rate reduction of M1 over baselinefor terms in different frequency ranges7 Conclusions and Future WorkThe task of query spelling correction is very differ-ent from conventional spelling checkers, and posesspecial research challenges.
In this paper, we pre-sented a novel method for use of web search re-sults to improve existing query spelling correctionmodels.We explored two schemes for taking advantageof the information extracted from web search re-sults.
Experimental results show that our proposedmethods can achieve statistically significant im-provements over the baseline model which onlyrelies on evidences of lexicon, spelling similarityand statistics estimated from query logs.There is still further potential useful informationthat should be studied in this direction.
For exam-ple, we can work on page ranking information ofreturning pages, because trusted or well-knownsites with high page rank generally contain fewwrong spellings.
In addition, the term co-occurrence statistics on the returned snippet textare also worth deep investigation.ReferencesAhmad F. and Grzegorz Kondrak G. Learning a spellingerror model from search query logs.
Proceedings ofEMNLP 2005, pages 955-962, 2005Berger A. L., Della Pietra S. A., and Della Pietra V. J.A maximum entropy approach to natural languageprocessing.
Computation Linguistics, 22(1):39-72,1996Brill E. and Moore R. C. An improved error model fornoisy channel spelling correction.
Proceedings of38th annual meeting of the ACL, pages 286-293,2000.Broder, A.
A taxonomy of web search.
SIGIR ForumFall 2002, Volume 36 Number 2, 2002.Church K. W. and Gale W. A. Probability scoring forspelling correction.
In Statistics and Computing, vo-lume 1, pages 93-103, 1991.Cucerzan S. and Brill E. Spelling correction as an itera-tive process that exploits the collective knowledge ofweb users.
Proceedings of EMNLP?04, pages 293-300, 2004.Damerau F. A technique for computer detection andcorrection of spelling errors.
Communication of theACM 7(3):659-664, 1964.Darroch J. N. and Ratcliff D. Generalized iterative scal-ing for long-linear models.
Annals of MathematicalStatistics, 43:1470-1480, 1972.Efthimiadis, N.E., Query Expansion, In Annual Reviewof Information Systems and Technology, Vol.
31, pp.121-187 , 1996.Golding A. R. and Roth D. Applying winnow to con-text-sensitive spelling correction.
Proceedings ofICML 1996, pages 182-190, 1996.J.
Lafferty and C. Zhai.
Document language models,query models, and risk minimization for informationretrieval.
In Proceedings of SIGIR?2001, pages 111-119, Sept 2001.J.
Xu and W. Croft.
Query expansion using local andglobal document analysis.
In Proceedings of the SI-GIR 1996, pages 4-11, 1996Kernighan M. D., Church K. W. and Gale W. A.
A spel-ling correction program based on a noisy channelmodel.
Proceedings of COLING 1990, pages 205-210, 1990.Kukich K. Techniques for automatically correctingwords in text.
ACM Computing Surveys.
24(4): 377-439, 1992.Levenshtein V. Binary codes capable of correcting dele-tions, insertions and reversals.
Soviet Physice ?
Dok-lady 10: 707-710, 1966.Li M., Zhu M. H., Zhang Y. and Zhou M. Exploringdistributional similarity based models for query spel-5%10%15%20%25%30%35%50 150 250 350 450 550 650 750errorratereductionterm frequency188ling correction.
Proceedings of COLING-ACL 2006,pages 1025-1032, 2006Mangu L. and Eric Brill E. Automatic rule acquisitionfor spelling correction.
Proceedings of ICML 1997,pages 734-741, 1997.Martin Reynaert.
Text induced spelling correction.
Pro-ceedings of COLING 2004,pages 834-840, 2004.Mayes E., Damerau F. and Mercer R. Context basedspelling correction.
Information processing andmanagement, 27(5): 517-522, 1991.Philips L. Hanging on the metaphone.
Computer Lan-guage Magazine, 7(12): 39, 1990.Toutanova K. and Moore R. Pronunciation modeling forimproved spelling correction.
Proceedings of the40th annual meeting of ACL, pages 144-151, 2002.189
