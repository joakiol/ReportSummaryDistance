Proceedings of NAACL-HLT 2013, pages 416?425,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsLearning to Relate Literal and Sentimental Descriptions of Visual PropertiesMark YatskarComputer Science & EngineeringUniversity of WashingtonSeattle, WAmy89@cs.washington.eduSvitlana VolkovaCenter for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, MDsvitlana@jhu.eduAsli CelikyilmazConversational Understanding SciencesMicrosoftMountain View, CAasli@ieee.orgBill DolanNLP GroupMicrosoft ResearchRedmond, WAbilldol@microsoft.eduLuke ZettlemoyerComputer Science & EngineeringUniversity of WashingtonSeattle, WAlsz@cs.washington.eduAbstractLanguage can describe our visual world atmany levels, including not only what is lit-erally there but also the sentiment that it in-vokes.
In this paper, we study visual language,both literal and sentimental, that describes theoverall appearance and style of virtual char-acters.
Sentimental properties, including la-bels such as ?youthful?
or ?country western,?must be inferred from descriptions of the moreliteral properties, such as facial features andclothing selection.
We present a new dataset,collected to describe Xbox avatars, as well asmodels for learning the relationships betweenthese avatars and their literal and sentimen-tal descriptions.
In a series of experiments,we demonstrate that such learned models canbe used for a range of tasks, including pre-dicting sentimental words and using them torank and build avatars.
Together, these re-sults demonstrate that sentimental languageprovides a concise (though noisy) means ofspecifying low-level visual properties.1 IntroductionLanguage can describe varied aspects of our visualworld, including not only what is literally there butalso the social, cultural, and emotional sentiment itinvokes.
Recently, there has been a growing effortto study literal language that describes directly ob-servable properties, such as object color, shape, orThis is a light tan young manwith short and trim haircut.
Hehas straight eyebrows and largebrown eyes.
He has a neat andtrim appearance.State of mind: angry, upset,determined.
Likes: countrywestern, rodeo.
Occupation:cowboy, wrangler, horse trainer.Overall: youthful, cowboy.Figure 1: (A) Literal avatar descriptions and (B) sen-timental descriptions of four avatar properties, in-cluding possible occupations and interests.category (Farhadi et al 2009; Mitchell et al 2010;Matuszek et al 2012).
Here, we add a focus onsentimental visual language, which compactly de-scribes more subjective properties such as if a personlooks determined, if a resume looks professional, orif a restaurant looks romantic.
Such models enablemany new applications, such as text editors that au-tomatically select properties including font, color, ortext alignment to best match high level descriptionssuch as ?professional?
or ?artistic.
?416In this paper, we study visual language, both lit-eral and sentimental, that describes the overall ap-pearance and style of virtual characters, like those inFigure 1.
We use literal language as feature norms, atool used for studying semantic information in cog-nitive science (Mcrae et al 2005).
Literal words,such ?black?
or ?hat,?
are annotated for objects to in-dicate how people perceive visual properties.
Suchfeature norms provide our gold-standard visual de-tectors, and allow us to focus on learning to modelsentimental language, such as ?youthful?
or ?goth.
?We introduce a new corpus of descriptions ofXbox avatars created by actual gamers.
Each avataris specified by 19 attributes, including clothing andbody type, allowing for more than 1020 possibil-ities.
Using Amazon Mechanical Turk,1 we col-lected literal and sentimental descriptions of com-plete avatars and many of their component parts,such as the cowboy hat in Figure 1(B).
In all, thereare over 100K descriptions.
To demonstrate poten-tial for learning, we also report an A/B test whichshows that native speakers can use sentimental de-scriptions to distinguish the labeled avatars fromrandom distractors.
This new data will enable studyof the relationships between the co-occurring literaland sentimental text in a rich visual setting.2We describe models for three tasks: (i) classify-ing when words match avatars, (ii) ranking avatarsgiven a description, and (iii) constructing avatars tomatch a description.
Each model includes literal partdescriptions as feature norms, enabling us to learnwhich literal and sentinel word pairs best predictcomplete avatars.Experiments demonstrate the potential for jointlymodeling literal and sentimental visual descriptionson our new dataset.
The approach outperforms sev-eral baselines and learns varied relationships be-tween the sentimental and literal descriptions.
Forexample, in one experiment ?nerdy student?
is pre-dictive of an avatar with features indicating its shirtis ?plaid?
and glasses are ?large?
and faces that arenot ?bearded.?
We also show that individual sen-timental words can be predicted but that multipleavatars can match a single sentimental description.Finally, we use our model to build complete avatars1www.mturk.com2Data available at http://homes.cs.washington.edu/?my89/avatar.and show that we can accurately predict the senti-mental terms annotators ascribe to them.2 Related WorkTo the best of our knowledge, our focus on learn-ing to understand visual sentiment descriptions isnovel.
However, visual sentiment has been stud-ied from other perspectives.
Jrgensen (1998) pro-vides examples which show that visual descriptionscommunicate social status and story information inaddition to literal object and properties.
Tousch etal.
(2012) draw the distinction between ?of-ness?
(objective and concrete) and ?about-ness?
(subjec-tive and abstract) in image retrieval, and observethat many image queries are abstract (for example,images about freedom).
Finally, in descriptions ofpeople undergoing emotional distress, Fussell andMoss (1998) show that literal descriptions co-occurfrequently with sentimental ones.There has been significant work on more lit-eral aspects of grounded language understand-ing, both visual and non-visual.
The Words-Eye project (Coyne and Sproat, 2001) generates3D scenes from literal paragraph-length descrip-tions.
Generating literal textual descriptions of vi-sual scenes has also been studied, including bothcaptions (Kulkarni et al 2011; Yang et al 2011;Feng and Lapata, 2010) and descriptions (Farhadiet al 2010).
Furthermore, Chen and Dolan (2011)collected literal descriptions of videos with thegoal of learning paraphrases while Zitnick andParikh (2013) describe a corpus of descriptions forclip art that supports the discovery of semantic ele-ments of visual scenes.There has also been significant recent work on au-tomatically recovering visual attributes, both abso-lute (Farhadi et al 2009) and relative (Kovashka etal., 2012), a challenge that we avoid having to solvewith our use of feature norms (Mcrae et al 2005).Grounded language understanding has also re-ceived significant attention, where the goal is tolearn to understand situated non-visual languageuse.
For example, there has been work on learningto execute instructions (Branavan et al 2009; Chenand Mooney, 2011; Artzi and Zettlemoyer, 2013),provide sports commentary (Chen et al 2010), un-derstand high level strategy guides to improve game417Figure 2: The number of assets per category and ex-ample images from the hair, shirt and hat categories.play (Branavan et al 2011; Eisenstein et al 2009),and understand referring expression (Matuszek etal., 2012).Finally, our work is similar in spirit to sentimentanalysis (Pang et al 2002), emotion detection fromimages and speech (Zeng et al 2009), and metaphorunderstanding (Shutova, 2010a; Shutova, 2010b).However, we focus on more general visual context.3 Data CollectionWe gathered a large number of natural language de-scriptions from Mechanical Turk (MTurk).
They in-clude: (1) literal descriptions of specific facial fea-tures, clothing or accessories and (2) high level sub-jective descriptions of human-generated avatars.3Literal Descriptions We showed annotators a sin-gle image of clothing, a facial feature or an acces-sory and asked them to produce short descriptions.Figure 2 shows the distribution over object types.We restricted descriptions to be between 3 and 15words.
In all, we collected 33.2K descriptions andhad on average 7 words per descriptions.
The ex-ample annotations with highlighted overlapping pat-terns are in Table 1.Sentimental Descriptions We also collected 1913gamer-created avatars from the web.
The avatarswere filtered to contain only items from the set of665 for which we gathered literal descriptions.
Thegender distribution is 95% male.3(2) also has phrases describing emotional reactions.
Wealso collected (3) multilingual literal, (4) relative literal and (5)comprehensive full-body descriptions.
We do not use this data,but it will be included in the public release.LITERAL DESCRIPTIONSfull-sleeved executive blue shirtblue , long-sleeved button-up shirtmens blue button dress shirt with dark blue stripesmulti-blue striped long-sleeve button-up dressshirt with cuffs and breast pocketTable 1: Literal descriptions of shirt in Figure 2.To gather high level sentimental descriptions, an-notators were presented with an image of an avatarand asked to list phrases in response to the followdifferent aspects:- State of mind of the avatar.- Things the avatar might care about.- What the avatar might do for a living.- Overall appearance of the avatar.6144 unique vocabulary items occurred in thesedescriptions, but only 1179 occurred more than 10times.
Figure 1 (B) shows an avatar and its corre-sponding sentimental descriptions.Quality Control All annotations in our dataset areproduced by non-expert annotators.
We relied onmanual spot checks to limit poor annotations.
Overtime, we developed a trusted crowd of annotatorswho produced only high quality annotations duringthe earliest stage of data collection.4 FeasibilityOur hypothesis is that sentimental language does notuniquely identify an avatar, but instead summarizesor otherwise describes its overall look.
In general,there is a trade off between concise and precise de-scriptions.
For example, given a single word youmight be able to generally describe the overall lookof an avatar, but a long, detailed, literal descriptionwould be required to completely specify their ap-pearance.To demonstrate that the sentimental descriptionswe collected are precise enough to be predictiveof appearance, we conducted an experiment thatprompts people to judge when avatars match de-scriptions.
We created an A/B test where we showEnglish speakers two avatars and one sentimentaldescription.
They were asked to select which avataris better matched by the description and how dif-ficult they felt, on a scale from 1 to 4, it was tojudge.
For 100 randomly selected descriptions, we4180 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 1.211.522.533.5datadifficulty less than XKappa vsCumulative Difficultygamer is majority label kappaportion of dataFigure 3: Judged task difficulty versus agreement,gamer avatar preference, and percentage of data cov-ered.
The difficulty axis is cumulative.asked 5 raters to compare the gamer avatars to ran-domly generated ones (where each asset is selectedindependently according to a uniform distribution).Figure 3 shows a plot of Kappa and the percent ofthe time a majority of the raters selected the gameravatar.
The easiest 20% of the data pairs had thestrongest agreement, with kappa=.92, and two thirdsof the data has kappa = .70.
While agreement fallsoff to .52 for the full data set, the gamer avatar re-mains the majority judgment 81% of the time.The fact that random avatars are sometimes pre-ferred indicates that it can be difficult to judge sen-timental descriptions.
Consider the avatars in Fig-ure 4.
Neither conforms to a clear sentimental de-scription based on the questions we asked.
Theright one is described with conflicting words andthe words describing the left one are very general(like ?dumb?).
This corresponds to our intuition thatwhile many avatars can be succinctly summarizedwith our questions, some would be more easily de-scribed using literal language.5 Tasks and EvaluationWe formulate three tasks to study the feasibility oflearning the relationship between sentimental andliteral descriptions.
In this section, we first definethe space of possible avatars, followed by the tasks.Avatars Figure 5 summarizes the notation we willdevelop to describe the data.
An avatar is defined bya 19 dimensional vector ~a where each position is anState of mind:playful, happy;Likes: sexOccupation: hoboOverall: dumbState of mind: content, humble, satisfied,peaceful, relaxed, calm.
Likes: fashion,friends, money, cars, music, education.Occupation: teacher, singer, actor,performer, dancer, computer engineer.Overall: nerdy, cool, smart, comfy,easygoing, reservedFigure 4: Avatars rated as difficult.index into a list of possible items~i.
Each dimensionrepresents a position on the avatar, for example, hator nose.
Each possible item is called an asset andis associated with a set of positions it can fill.
Mostassets take up exactly one position, while there area few cases where assets take multiple positions.4An avatar ~a is valid if all of its mandatory positionsare filled, and no two assets conflict on a position.Mandatory positions include hair, eyes, ears, eye-brows, nose, mouth, chin, shirt, pants, and shoes.All other positions are optional.
We refer to this setof valid ~a as A.
Practically speaking, if an avatar isnot valid, it cannot be reliably rendered graphically.Each item i is associated with the literal descrip-tions ~di ?
D where D is the set of literal descrip-tions.
Furthermore, every avatar~a is associated a listof sentimental query words ~q, describing subjectiveaspects of an avatar.5Sentimental Word Prediction We first study in-dividual words.
The word prediction task is to de-cide whether a given avatar can be described with a4For example, long sleeve shirts cover up watches, so theytake up both shirt and wristwear positions.
Costumes tend tospan many more positions, for example there a suit that takesup shirt, pants, wristwear and shoes positions.5We do not distinguish which prompt (e.g., ?state of mind?or ?occupation?)
a word in ~q came from, although the vocabu-laries are relatively disjoint.419Figure 5: Avatars, queries, items, literal descriptions.particular sentimental word q?.
We evaluate perfor-mance with F-score.Avatar Ranking We also consider an avatar re-trieval task, where the goal is to rank the set ofavatars in our data, ?j=1...n ~aj , according to whichone best matches a sentimental description, ~qi.
Asan automated evaluation, we report the average per-centile position assigned to the true ~ai for each ex-ample.
However, in general, many different avatarscan match each ~qi, an interesting phenomena we willfurther study with human evaluation.Avatar Generation Finally, we consider the prob-lem of generating novel, previously unseen avatars,by selecting a set of items that best embody somesentimental description.
As with ranking, we aim toconstruct the avatar ~ai that matches each sentimen-tal description ~qi.
We evaluate by considering theitem overlap between ~ai and the output avatar ~a?,discounting for empty positions:6f =?| ~a?|j=1 I( ~a?j = ~aij)max(numparts( ~a?
), numparts(~ai)), (1)where numparts returns the number of non-emptyavatar positions.
The score is a conservative measurebecause some items are significantly more visuallysalient than others.
For instance, shirts and pants oc-cupy a large portion of the physical realization of theavatar, while rings are small and virtually unnotice-able.
We additionally perform a human evaluationin Section 8 to better understand these challenges.6Optional items are infrequently used.
Therefore not pre-dicting them at all offers a strong baseline.
Yet doing thisdemonstrates nothing about an algorithm?s ability to predictitems which contribute to the sentimental qualities of an avatar.6 MethodsWe present two different models: one that considerswords in isolation and another that jointly modelsthe query words.
This section defines the modelsand how we learn them.6.1 Independent Sentimental Word ModelThe independent word model (S-Independent) as-sumes that each word independently describes theavatar.
We construct a separate linear model for eachword in the vocabulary.To train these model, we transform the data toform a binary classification problem for each word,where the positive data includes all avatars the wordwas seen with, (q, ~ai, 1) for all i and q ?
~qi, and therest are negative, (q, ~ai, 0) for all i and q /?
~qi.We use the following features:?
an indicator feature for the cross product of asentiment query word q, a literal descriptionword w ?
D, and the avatar position index j(for example, q = ?angry?
with w = ?pointy?and j = eyebrows):I(q ?
~qi, w ?
~daij , j)?
a bias feature for keeping a position empty:I(q ?
~qi, aij = empty, j)These features will allow the model to capturecorrelations between our feature norms which pro-vide descriptions of visual attributes, like black, andsentimental words, like gothic.420S-Independent is used for both word predictionand ranking.
For prediction, we train a linear modelusing averaged binary perceptron.
For ranking, wetry to rank all positive instances above negative in-stances.
We use an averaged structured perceptronto train the ranker (Collins, 2002).
To rank with re-spect to an entire query ~qi, we sum the scores of eachword q ?
~qi.6.2 Joint Sentimental ModelThe second approach (S-Joint) jointly models thequery words to learn the relationships between lit-eral and sentimental words with score s:s(~a|~q,D) =|~a|?i=1|~q|?j=1?T f(~ai, ~qj , ~dai)Where every word in the query has a separate factorand every position is treated independently subjectto the constraint that ~a is valid.
The feature functionf uses the same features as the word independentmodel above.This model is used for ranking and generation.For ranking, we try to rank the avatar ai for queryqi above all other avatars in the candidate set.
Forgeneration, we try to score ai above all other validavatars given the query qi.
In both cases, we trainwith averaged structured perceptron (Collins, 2002)on the original data, containing query, avatar pairs(~qi, ~ai).7 Experimental SetupRandom Baseline For the ranking and avatar gen-eration tasks, we report random baselines.
For rank-ing, we randomly order the avatars.
In the genera-tion case, we select an item randomly for every posi-tion.
This baseline does not generate optional assetsbecause they are rare in the real data.Sentimental-Literal Overlap (SL-Overlap) Wealso report a baseline that measures the overlap be-tween words in the sentiment query ~qi and words inthe literal asset descriptions D. In generation, foreach position in the avatar, ~ai, SL-Overlap selectsthe item whose literal description has the most wordsin common with ~qi.
If no item had overlap with thequery, we backoff to a random choice.
In the case ofranking, it orders avatars by the sum over every po-sition of the number of words in common betweenWord F-Score Precision Recall Nhappi 0.84 0.89 0.78 149student 0.78 0.82 0.74 129friend 0.76 0.84 0.70 153music 0.74 0.89 0.63 148confid 0.74 0.82 0.76 157sport 0.69 0.62 0.76 76casual 0.63 0.6 0.67 84youth 0.6 0.57 0.64 88waitress 0.59 0.42 1 5smart 0.57 0.54 0.6 88fashion 0.54 0.54 0.54 70monei 0.54 0.52 0.56 76cool 0.54 0.52 0.56 84relax 0.53 0.52 0.56 90game 0.51 0.44 0.62 61musician 0.51 0.44 0.61 66parti 0.51 0.43 0.62 58content 0.5 0.47 0.53 75friendli 0.49 0.42 0.6 56smooth 0.49 0.4 0.63 57Table 2: Top 20 words (stemmed) for classification.N is the number of occurances in the test set.the literal description and the query, ~qi.
This base-line tests the degree to which literal and sentimentaldescriptions overlap lexically.Feature Generation For all models that use lexi-cal features, we limited the number of words.
6144unique vocabulary items occur in the query set, and3524 in the literal description set.
There are over400 million entries in the full set of features that in-clude the cross product of these sets with all possibleavatar positions, as described in Section 6.
Since thiswould present a challenge for learning, we prune intwo ways.
We stem all words with a Porter stemmer.We also filter out all features which do not occur atleast 10 times in our training set.
The final modelhas approximately 700k features.8 ResultsWe present results for the tasks described in Sec-tion 5 with the appropriate models from Section 6.8.1 Word Prediction ResultsThe goal of our first experiment is to study whenindividual sentiment words can be accurately pre-dicted.
We computed sentimental word classifica-tion accuracy for 1179 word classes with 10 or more421Algorithm Percentile RankS-joint 77.3S-independant 73.5SL-overlap 60.4Random 48.8Table 3: Automatic evaluation of ranking.
The aver-age percentile that a test avatar was ranked given itssentimental description.mentions.
Table 2 shows the top 20 words orderedby F-score.7 Many common words can be predictedwith relatively high accuracy.
Words with strongindividual cues like happy (a smiling mouth), andconfidence (wide eyes) and nerdi (particular glasses)can be predicted well.The average F-score among all words was .085.33.2% of words have an F-score of zero.
These zerosinclude words like: unusual, bland, sarcastic, trust,prepared, limber, healthy and poetry.
Some of thesewords indicate broad classes of avatars (e.g., unusualavatars) and others indicate subtle modifications tolooks that without other words are not specific (e.g.,a prepared surfer vs. a prepared business man).
Fur-thermore, evaluation was done assuming that whena word is not mentioned, it is should be predicted asnegative.
This fails to account for the fact that peo-ple do not mention everything that?s true, but insteadmake choices about what to mention based on themost relevant qualities.
Despite these difficulties,the classification performance shows that we can ac-curately capture usage patterns for many words.8.2 Ranking ResultsRanking allows us to test the hypothesis that multi-ple avatars are valid for a high level description.
Fur-thermore, we consider the differences between S-Joint and S-Independent, showing that jointly mod-elings all words improves ranking performance.Automatic Evaluation The results are shown inTable 3.
Both S-Independent and S-Joint outperformthe SL-overlap baseline.
SL-Overlap?s poor perfor-mance can be attributed to low direct overlap be-tween sentimental words and literal words.
S-Jointalso outperforms the S-Independent.7Accuracy numbers are inappropriate in this case becausethe number of negative instances, in most cases, is far largerthan the number of positive ones.Inspection of the parameters shows that S-Jointdoes better than S-Independent in modeling wordsthat only relate to a subset of body positions.
Forexample, in one case we found that for the word?puzzled?
nearly 50% of the weights were on fea-tures that related to eyebrows and eyes.
This typeof specialization was far more pronounced for S-Joint.
The joint nature of the learning allows the fea-tures for individual words to specialize for specificpositions.
In contrast, S-Independent must indepen-dently predict all parts for every word.Human Evaluation We report human relevancyjudgments for the top-5 returned results from S-Joint.
On average, 56.2% were marked to be rele-vant.
This shows that S-Joint is performing betterthan automatic numbers would indicate, confirmingour intuition that there is a one-to-many relationshipbetween a sentimental description and avatars.
Sen-timental descriptions, while having significant sig-nal, are not exact.
These results also indicate thatrelying on automatic measures of accuracy that as-sume a single reference avatar underestimates per-formance.
Figure 6 shows the top ranked resultsreturned by S-Joint for a sentimental descriptionwhere the model performs well.8.3 Generation ResultsFinally we evaluate three models for avatar genera-tion: Random, SL-Overlap and S-Joint using auto-matic measures and human evaluation.Automatic Evaluation Table 4 presents resultsfor automatic evaluation.
The Random baseline per-forms badly, on average assigning items correctly toless than 1 position in the generated avatar.
The SL-Overlap baseline improves, but still performs quitepoorly.
The S-Joint model performs significantlybetter, correctly guessing 2-3 items for each outputavatar.
However, as we will see in the manual eval-uation, many of the non-matching parts it producesare still a good fit for the query.Human Evaluation As before, there are manyreasonable avatars that could match as well as thereference avatars.
Therefore, we also evaluated gen-eration with A/B tests, much like in Section 4.
An-notators were asked to judge which of two avatarsbetter matched a sentimental description.
They422pensive,confrontational; music,socializing; musician,bar tending,club owner; smart,cool.Figure 6: A sentimental description paired with the highest ranked avatars found by S-Joint.Model OverlapRandom 0.041SL-Overlap 0.049S-Joint 0.126Table 4: Automatic generation evaluation results.The item overlap metric is defined in Section 5.Kappa Majority Random Sys.SL-Overlap 0.20 0.25 0.34 0.32S-Joint 0.52 0.90 0.07 0.81Gamer 0.52 0.81 0.08 0.77Table 5: Human evaluation of automatically gener-ated avatars.
Majority represents the percentage oftime the system output is preferred by a majority ofraters.
Random and System (Sys.)
indicate the per-centage of time each was preferred.could rate System A or System B as better, or re-port that they were equal or that neither matchedthe description.
We consider two comparisons: SL-Overlap vs. Random and S-Joint vs Random.
Fiveannotators performed each condition, rating 100 ex-amples with randomly ordered avatars.We report the results for human evaluation includ-ing kappa, majority judgments, and a distributionover judgments in Table 5.
The SL-Overlap baselineis indistinguishable from a random avatar.
This con-trasts with the ranking case, where this simple base-line showed improvement, indicating that generationis a much harder problem.
Furthermore, agreementis low; people felt the need to make a choice butwere not consistent.We also see in Table 5 that people prefer the S-Joint model outputs to random avatars as often asthey prefer gamer to random.
While this does notnecessarily imply that S-Joint creates gamer-qualityavatars, it indicates substantial progress by learninga mapping between literal and sentimental words.Qualitative Results Table 6 presents the highestand lowest weighted features for different sentimen-tal query words.
Figure 7 shows four descriptionsthat were assigned high quality avatars.In general, many of the weaker avatars had as-pects of the descriptions but lacked such distinctiveoverall looks.
This was especially true when thedescriptions contained seemingly contradictory in-formation.
For example, one avatar was describedas being both nerdy and popular.
We generated alook that had aspects of both of these descriptions,including a head that contained both conservative el-ements (like glasses) and less conservative elements(like crazy hair and earrings).
However, the combi-nation would not be described as nerdy or popular,because of difficult to predict global interactions be-tween the co-occurring words and items.
This is animportant area for future work.9 ConclusionsWe explored how visual language, both literal andsentimental, maps to the overall physical appearanceand style of virtual characters.
While this paper fo-cused on avatar design, our approach has implica-tions for a broad class of natural language-driven423Ambition; business,fashion, success;salesman; smooth,professional.Capable, confident, firm; heavy metal,extreme sports, motorcycles; engineer,mechanic, machinist; aggressive,strong, protective.Stressed, bored,discontent; emo music;works at a record store;goth, dark, drab.Happy, content, confident,home, career, family,secretary,student,classy,clean,casualFigure 7: Avatars automatically generated with the S-Joint model.Sentiment positive features negative featureshappi mouth:thick, mouth:smilei, mouth:make, mouth:open mouth:tight, mouth:emotionless, mouth:brownish, mouth:attractgothic shoes:brown, shirt:black, pants:hot, shirt:band shirt:half, shirt:tight, pants:sexi, hair:brownishretro eyebrows:men, eyebrows:large, hair:round, pants:light eyebrows:beauti, pants:side; eyebrows:trim, pants:cutbeach pants:yello, pants:half, nose:narrow, pants:white shirt:brown, shirt:side; shoes:long, pants:jeanTable 6: Most positive and negative features for a word stem.
A feature is [position]:[literal word].dialog scenarios.
In many situations, a user maybe perfectly able to formulate a high-level descrip-tion of their intent (?Make my resume look cleaner?
?Buy me clothes for a summer wedding,?
or ?Playsomething more danceable?)
while having little orno understanding of the complex parameter spacethat the underlying software must manipulate in or-der to achieve this result.We demonstrated that these high-level sentimen-tal specifications can have a strong relationship toliteral aspects of a problem space and showed thatsentimental language is a concise, yet noisy, wayof specifying high level characteristics.
Sentimen-tal language is an unexplored avenue for improvingnatural language systems that operate in situated set-tings.
It has the potential to bridge the gap betweenlay and expert understandings of a problem domain.AcknowledgmentsThis work is partially supported by the DARPACSSG (N11AP20020) and the NSF (IIS-1115966).The authors would like to thank Chris Brockett,Noelle Sophy, Rico Malvar for helping with collect-ing and processing the data.
We would also liketo thank Tom Kwiatkowski and Nicholas FitzGer-ald and the anonymous reviewers for their helpfulcomments.ReferencesYoav Artzi and Luke Zettlemoyer.
2013.
Weakly su-pervised learning of semantic parsers for mapping in-structions to actions.
Transactions of the Associationfor Computational Linguistics, 1(1):49?62.SRK Branavan, H. Chen, L.S.
Zettlemoyer, and R. Barzi-lay.
2009.
Reinforcement learning for mapping in-structions to actions.
In Proceedings of the Joint Con-ference of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on Natural Lan-guage Processing of the AFNLP, pages 82?90.SRK Branavan, David Silver, and Regina Barzilay.
2011.Learning to win by reading manuals in a monte-carloframework.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies-Volume 1, pages 268?277.David L. Chen and William B. Dolan.
2011.
Collectinghighly parallel data for paraphrase evaluation.
In Pro-ceedings of the 49th Annual Meeting of the Associationfor Computational Linguistics, pages 190?200.D.L.
Chen and R.J. Mooney.
2011.
Learning to interpretnatural language navigation instructions from observa-424tions.
In Proceedings of the 25th AAAI Conference onArtificial Intelligence (AAAI-2011), pages 859?865.David L. Chen, Joohyun Kim, and Raymond J. Mooney.2010.
Training a multilingual sportscaster: Using per-ceptual context to learn language.
Journal of ArtificialIntelligence Research, 37:397?435.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: theory and experi-ments with perceptron algorithms.
In Proceedings ofthe ACL-02 conference on Empirical methods in natu-ral language processing, pages 1?8.B.
Coyne and R. Sproat.
2001.
Wordseye: an automatictext-to-scene conversion system.
In Proceedings of the28th annual conference on Computer graphics and in-teractive techniques, pages 487?496.J.
Eisenstein, J. Clarke, D. Goldwasser, and D. Roth.2009.
Reading to learn: Constructing features fromsemantic abstracts.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 958?967.Ali Farhadi, Ian Endres, Derek Hoiem, and DavidForsyth.
2009.
Describing objects by their attributes.In Proceedings of the IEEE Computer Society Confer-ence on Computer Vision and Pattern Recognition.Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi,Peter Young, Cyrus Rashtchian, Julia Hockenmaier,and David Forsyth.
2010.
Every picture tells a story:generating sentences from images.
In Proceedings ofthe 11th European conference on Computer Vision,ECCV?10, pages 15?29.Yansong Feng and Mirella Lapata.
2010.
Topic modelsfor image annotation and text illustration.
In HumanLanguage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, pages 831?839.Susan R Fussell and Mallie M Moss.
1998.
Figura-tive language in emotional communication.
Social andcognitive approaches to interpersonal communication,page 113.Corinne Jrgensen.
1998.
Attributes of images in describ-ing tasks.
Information Processing & Management,34(23):161 ?
174.Adriana Kovashka, Devi Parikh, and Kristen Grauman.2012.
Whittlesearch: Image search with relative at-tribute feedback.
In Computer Vision and PatternRecognition (CVPR), pages 2973?2980.G.
Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A.C.Berg, and T.L.
Berg.
2011.
Baby talk: Understandingand generating simple image descriptions.
In Com-puter Vision and Pattern Recognition (CVPR), pages1601?1608.Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-moyer, Liefeng Bo, and Dieter Fox.
2012.
A JointModel of Language and Perception for Grounded At-tribute Learning.
In Proc.
of the 2012 InternationalConference on Machine Learning.Ken Mcrae, George S. Cree, Mark S. Seidenberg, andChris Mcnorgan.
2005.
Semantic feature productionnorms for a large set of living and nonliving things.Behavior Research Methods, 37(4):547?559.Margaret Mitchell, Kees van Deemter, and Ehud Reiter.2010.
Natural reference to objects in a visual domain.In Proceedings of the 6th International Natural Lan-guage Generation Conference, INLG ?10, pages 95?104.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
: sentiment classification using machine learningtechniques.
In Proceedings of the ACL-02 conferenceon Empirical methods in natural language processing,pages 79?86.Ekaterina Shutova.
2010a.
Automatic metaphor inter-pretation as a paraphrasing task.
In Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, HLT ?10, pages 1029?1037.Ekaterina Shutova.
2010b.
Models of metaphor in nlp.In Proceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics, ACL ?10, pages688?697.Anne-Marie Tousch, Stphane Herbin, and Jean-Yves Au-dibert.
2012.
Semantic hierarchies for image annota-tion: A survey.
Pattern Recognition, 45(1):333 ?
345.Yezhou Yang, Ching Lik Teo, Hal Daume?
III, and Yian-nis Aloimonos.
2011.
Corpus-guided sentence gen-eration of natural images.
In Empirical Methods inNatural Language Processing.Z.
Zeng, M. Pantic, G.I.
Roisman, and T.S.
Huang.
2009.A survey of affect recognition methods: Audio, vi-sual, and spontaneous expressions.
Pattern Analy-sis and Machine Intelligence, IEEE Transactions on,31(1):39?58.C Lawrence Zitnick and Devi Parikh.
2013.
Bringingsemantics into focus using visual abstraction.
In Com-puter Vision and Pattern Recognition (To Appear).425
