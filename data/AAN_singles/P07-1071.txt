Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 560?567,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsFast Semantic Extraction Using a Novel Neural Network ArchitectureRonan CollobertNEC Laboratories America, Inc.4 Independence WaySuite 200, Princeton, NJ 08540collober@nec-labs.comJason WestonNEC Laboratories America, Inc.4 Independence WaySuite 200, Princeton, NJ 08540jasonw@nec-labs.comAbstractWe describe a novel neural network archi-tecture for the problem of semantic role la-beling.
Many current solutions are compli-cated, consist of several stages and hand-built features, and are too slow to be appliedas part of real applications that require suchsemantic labels, partly because of their useof a syntactic parser (Pradhan et al, 2004;Gildea and Jurafsky, 2002).
Our method in-stead learns a direct mapping from sourcesentence to semantic tags for a given pred-icate without the aid of a parser or a chun-ker.
Our resulting system obtains accuraciescomparable to the current state-of-the-art ata fraction of the computational cost.1 IntroductionSemantic understanding plays an important role inmany end-user applications involving text: for infor-mation extraction, web-crawling systems, questionand answer based systems, as well as machine trans-lation, summarization and search.
Such applicationstypically have to be computationally cheap to dealwith an enormous quantity of data, e.g.
web-basedsystems process large numbers of documents, whilstinteractive human-machine applications require al-most instant response.
Another issue is the cost ofproducing labeled training data required for statisti-cal models, which is exacerbated when those modelsalso depend on syntactic features which must them-selves be learnt.To achieve the goal of semantic understanding,the current consensus is to divide and conquer the[The company]ARG0 [bought]REL [sugar]ARG1 [on the worldmarket]ARGM-LOC [to meet export commitments]ARGM-PNCFigure 1: Example of Semantic Role Labeling fromthe PropBank dataset (Palmer et al, 2005).
ARG0is typically an actor, REL an action, ARG1 an ob-ject, and ARGM describe various modifiers such aslocation (LOC) and purpose (PNC).problem.
Researchers tackle several layers of pro-cessing tasks ranging from the syntactic, such aspart-of-speech labeling and parsing, to the semantic:word-sense disambiguation, semantic role-labeling,named entity extraction, co-reference resolution andentailment.
None of these tasks are end goals inthemselves but can be seen as layers of feature ex-traction that can help in a language-based end ap-plication, such as the ones described above.
Un-fortunately, the state-of-the-art solutions of many ofthese tasks are simply too slow to be used in the ap-plications previously described.
For example, state-of-the-art syntactic parsers theoretically have cubiccomplexity in the sentence length (Younger, 1967)1and several semantic extraction algorithms use theparse tree as an initial feature.In this work, we describe a novel type of neuralnetwork architecture that could help to solve someof these issues.
We focus our experimental study onthe semantic role labeling problem (Palmer et al,2005): being able to give a semantic role to a syn-1Even though some parsers effectively exhibit linear be-havior in sentence length (Ratnaparkhi, 1997), fast statisticalparsers such as (Henderson, 2004) still take around 1.5 secondsfor sentences of length 35 in tests that we made.560tactic constituent of a sentence, i.e.
annotating thepredicate argument structure in text (see for exam-ple Figure 1).
Because of its nature, role labelingseems to require the syntactic analysis of a sentencebefore attributing semantic labels.
Using this intu-ition, state-of-the-art systems first build a parse tree,and syntactic constituents are then labeled by feed-ing hand-built features extracted from the parse treeto a machine learning system, e.g.
the ASSERT sys-tem (Pradhan et al, 2004).
This is rather slow, tak-ing a few seconds per sentence at test time, partlybecause of the parse tree component, and partly be-cause of the use of Support Vector Machines (Boseret al, 1992), which have linear complexity in test-ing time with respect to the number of training ex-amples.
This makes it hard to apply this method tointeresting end user applications.Here, we propose a radically different approachthat avoids the more complex task of building a fullparse tree.
From a machine learning point of view, ahuman does not need to be taught about parse treesto talk.
It is possible, however, that our brains mayimplicitly learn features highly correlated with thoseextracted from a parse tree.
We propose to developan architecture that implements this kind of implicitlearning, rather than using explicitly engineered fea-tures.
In practice, our system also provides semantictags at a fraction of the computational cost of othermethods, taking on average 0.02 seconds to label asentence from the Penn Treebank, with almost noloss in accuracy.The rest of the article is as follows.
First, we de-scribe the problem of shallow semantic parsing inmore detail, as well as existing solutions to this prob-lem.
We then detail our algorithmic approach ?
theneural network architecture we employ ?
followedby experiments that evaluate our method.
Finally,we conclude with a summary and discussion of fu-ture work.2 Shallow Semantic ParsingFrameNet (Baker et al, 1998) and the PropositionBank (Palmer et al, 2005), or PropBank for short,are the two main systems currently developed forsemantic role-labeling annotation.
We focus hereon PropBank.
PropBank encodes role labels by se-mantically tagging the syntactic structures of handannotated parses of sentences.
The current versionof the dataset gives semantic tags for the same sen-tences as in the Penn Treebank (Marcus et al, 1993),which are excerpts from theWall Street Journal.
Thecentral idea is that each verb in a sentence is la-beled with its propositional arguments, where theabstract numbered arguments are intended to fill typ-ical roles.
For example, ARG0 is typically the actor,and ARG1 is typically the thing acted upon.
Theprecise usage of the numbering system is labeled foreach particular verb as so-called frames.
Addition-ally, semantic roles can also be labeled with one of13 ARGM adjunct labels, such as ARGM-LOC orARGM-TMP for additional locational or temporalinformation relative to some verb.Shallow semantic parsing has immediate applica-tions in tasks such as meta-data extraction (e.g.
fromweb documents) and question and answer based sys-tems (e.g.
call center systems), amongst others.3 Previous WorkSeveral authors have already attempted to build ma-chine learning approaches for the semantic role-labeling problem.
In (Gildea and Jurafsky, 2002)the authors presented a statistical approach to learn-ing (for FrameNet), with some success.
They pro-posed to take advantage of the syntactic tree struc-ture that can be predicted by a parser, such as Char-niak?s parser (Charniak, 2000).
Their aim is, givena node in the parse tree, to assign a semantic rolelabel to the words that are the children of that node.They extract several key types of features from theparse tree to be used in a statistical model for pre-diction.
These same features also proved crucial tosubsequent approaches, e.g.
(Pradhan et al, 2004).These features include:?
The parts of speech and syntactic labels ofwords and nodes in the tree.?
The node?s position (left or right) in relation tothe verb.?
The syntactic path to the verb in the parse tree.?
Whether a node in the parse tree is part of anoun or verb phrase (by looking at the parentnodes of that node).561?
The voice of the sentence: active or passive(part of the PropBank gold annotation);as well as several other features (predicate, headword, verb sub-categorization, .
.
.
).The authors of (Pradhan et al, 2004) used asimilar structure, but added more features, notablyhead word part-of-speech, the predicted named en-tity class of the argument, word sense disambigua-tion of the verb and verb clustering, and others (theyadd 25 variants of 12 new feature types overall.
)Their system also uses a parser, as before, and then apolynomial Support Vector Machine (SVM) (Boseret al, 1992) is used in two further stages: to clas-sify each node in the tree as being a semantic ar-gument or not for a given verb; and then to clas-sify each semantic argument into one of the classes(ARG1, ARG2, etc.).
The first SVM solves a two-class problem, the second solves a multi-class prob-lem using a one-vs-the-rest approach.
The final sys-tem, called ASSERT, gives state-of-the-art perfor-mance and is also freely available at: http://oak.colorado.edu/assert/.
We compareto this system in our experimental results in Sec-tion 5.
Several other competing methods exist, e.g.the ones that participated in the CONLL 2004 and2005 challenges (http://www.lsi.upc.edu/?srlconll/st05/st05.html).
In this paperwe focus on a comparison with ASSERT becausesoftware to re-run it is available online.
This alsogives us a timing result for comparison purposes.The three-step procedure used in ASSERT (calcu-lating a parse tree and then applying SVMs twice)leads to good classification performance, but hasseveral drawbacks.
First in speed: predicting aparse tree is extremely demanding in computing re-sources.
Secondly, choosing the features necessaryfor SVM classification requires extensive research.Finally, the SVM classification algorithm used in ex-isting approaches is rather slow: SVM training is atleast quadratic in time with respect to the numberof training examples.
The number of support vec-tors involved in the SVM decision function also in-creases linearly with the number of training exam-ples.
This makes SVMs slow on large-scale prob-lems, both during training and testing phases.To alleviate the burden of parse tree computation,several attempts have been made to remove the fullparse tree information from the semantic role label-ing system, in fact the shared task of CONLL 2004was devoted to this goal, but the results were notcompletely satisfactory.
Previously, in (Gildea andPalmer, 2001), the authors tried to show that theparse tree is necessary for good generalization byshowing that segments derived from a shallow syn-tactic parser or chunker do not perform as well forthis goal.
A further analysis of using chunkers, withimproved results was also given in (Punyakanok etal., 2005), but still concluded the full parse tree ismost useful.4 Neural Network ArchitectureIdeally, we want an end-to-end fast learning systemto output semantic roles for syntactic constituentswithout using a time consuming parse tree.Also, as explained before, we are interesting inexploring whether machine learning approaches canlearn structure implicitly.
Hence, even if there is adeep relationship between syntax and semantics, weprefer to avoid hand-engineered features that exploitthis, and see if we can develop a model that can learnthese features instead.
We are thus not interestedin chunker-based techniques, even though they arefaster than parser-based techniques.We propose here a neural network based architec-ture which achieves these two goals.4.1 Basic ArchitectureThe type of neural network that we employ is aMultiLayer Perceptron (MLP).
MLPs have been used formany years in the machine learning field and slowlyabandoned for several reasons: partly because ofthe difficulty of solving the non-convex optimizationproblems associated with learning (LeCun et al,1998), and partly because of the difficulty of theirtheoretical analysis compared to alternative convexapproaches.An MLP works by successively projecting thedata to be classified into different spaces.
Theseprojections are done in what is called hidden lay-ers.
Given an input vector z, a hidden layer appliesa linear transformation (matrix M ) followed by asquashing function h:z 7?
Mz 7?
h(Mz) .
(1)562A typical squashing function is the hyperbolic tan-gent h(?)
= tanh(?).
The last layer (the outputlayer) linearly separates the classes.
The composi-tion of the projections in the hidden layers could beviewed as the work done by the kernel in SVMs.However there is a very important difference: thekernel in SVM is fixed and arbitrarily chosen, whilethe hidden layers in an MLP are trained and adaptedto the classification task.
This allows us to createmuch more flexible classification architectures.Our method for semantic role labeling classifieseach word of a sentence separately.
We do not useany semantic constituent information: if the modelis powerful enough, words in the same semanticconstituent should have the same class label.
Thismeans we also do not separate the problem intoan identification and classification phase, but rathersolve in a single step.4.1.1 NotationWe represent words as indices.
We consider a fi-nite dictionary of words D ?
N. Let us represent asentence of nw words to be analyzed as a functions(?).
The ith word in the sentence is given by theindex s(i):1 ?
i ?
nw s(i) ?
D .We are interested in predicting the semantic role la-bel of the word at position posw, given a verb at po-sition posv (1 ?
posw, posv ?
nw).
A mathemati-cal description of our network architecture schemat-ically shown in Figure 2 follows.4.1.2 Transforming words into feature vectorsOur first concern in semantic role labeling is thatwe have to deal with words, and that a simple in-dex i ?
D does not carry any information specificto a word: for each word we need a set of featuresrelevant for the task.
As described earlier, previousmethods construct a parse tree, and then computehand-built features which are then fed to a classi-fication algorithm.
In order to bypass the use of aparse tree, we convert each word i ?
D into a par-ticular vector wi ?
Rd which is going to be learntfor the task we are interested in.
This approach hasalready been used with great success in the domainof language models (Bengio and Ducharme, 2001;Schwenk and Gauvain, 2002).        Lookup Table d...dLinear Layer with sentence?adapted columnsdC(position w.r.t.
cat, position w.r.t.
sat)Softmax Squashing Layer...ARG1 ARG2 ARGMLOC ARGMTMPClassical Linear LayerTanh Squashing Layernhu Ciws(6)ws(2)s(1)w...C1 C2 C6Classical Linear Layerws(6)...ws(2)s(1)ws(1)  s(2)   ...                   s(6)sattheInput Sentenceon the matcatFigure 2: MLP architecture for shallow semanticparsing.
The input sequence is at the top.
The out-put class probabilities for the word of interest (?cat?
)given the verb of interest (?sat?)
are given at the bot-tom.The first layer of our MLP is thus a lookup tablewhich replaces the word indices into a concatenationof vectors:{s(1), .
.
.
, s(nw)}7?
(ws(1) .
.
.
ws(nw)) ?
Rnw d .
(2)The weights {wi | i ?
D} for this layer are consid-ered during the backpropagation phase of the MLP,and thus adapted automatically for the task we areinterested in.4.1.3 Integrating the verb positionFeeding word vectors alone to a linear classifica-tion layer as in (Bengio and Ducharme, 2001) leads563to very poor accuracy because the semantic classifi-cation of a given word also depends on the verb inquestion.
We need to provide the MLP with infor-mation about the verb position within the sentence.For that purpose we use a kind of linear layer whichis adapted to the sentence considered.
It takes theform:(ws(1) .
.
.
ws(nw)) 7?
M???wTs(1)...wTs(nw)???
,where M ?
Rnhu?nw d, and nhu is the number ofhidden units.
The specific nature of this layer isthat the matrix M has a special block-column formwhich depends on the sentence:M = (C1| .
.
.
|Cnw) ,where each column Ci ?
Rnhu?d depends on theposition of the ith word in s(?
), with respect to theposition posw of the word of interest, and with re-spect to the position posv of the verb of interest:Ci = C(i?
posw, i?
posv) ,where C(?, ?)
is a function to be chosen.In our experiments C(?, ?)
was a linear layer withdiscretized inputs (i ?
posw, i ?
posv) which weretransformed into two binary vectors of size wsz,where a bit is set to 1 if it corresponds to the po-sition to encode, and 0 otherwise.
These two binaryvectors are then concatenated and fed to the linearlayer.
We chose the ?window size?
wsz = 11.
Ifa position lies outside the window, then we still setthe leftmost or rightmost bit to 1.
The parameters in-volved in this function are also considered during thebackpropagation.
With such an architecture we al-low our MLP to automatically adapt the importanceof a word in the sentence given its distance to theword we want to classify, and to the verb we are in-terested in.This idea is the major novelty in this work, and iscrucial for the success of the entire architecture, aswe will see in the experiments.4.1.4 Learning class probabilitiesThe last layer in our MLP is a classical linearlayer as described in (1), with a softmax squashingfunction (Bridle, 1990).
Considering (1) and givenz?
= Mz, we havehi(z?)
=exp z?i?j exp z?j.This allows us to interpret outputs as probabilitiesfor each semantic role label.
The training of thewhole system is achieved using a normal stochasticgradient descent.4.2 Word representationAs we have seen, in our model we are learning oned dimensional vector to represent each word.
If thedataset were large enough, this would be an elegantsolution.
In practice many words occur infrequentlywithin PropBank, so (independent of the size of d)we can still only learn a very poor representation forwords that only appear a few times.
Hence, to con-trol the capacity of our model we take the originalword and replace it with its part-of-speech if it isa verb, noun, adjective, adverb or number as deter-mined by a part-of-speech classifier, and keep thewords for all other parts of speech.
This classifier isitself a neural network.
This way we keep linkingwords which are important for this task.
We do notdo this replacement for the predicate itself.5 ExperimentsWe used Sections 02-21 of the PropBank datasetversion 1 for training and validation and Section23 for testing as standard in all our experiments.We first describe the part-of-speech tagger we em-ploy, and then describe our semantic role labelingexperiments.
Software for our method, SENNA (Se-mantic Extraction using a Neural Network Archi-tecture), more details on its implementation, an on-line applet and test set predictions of our systemin comparison to ASSERT can be found at http://ml.nec-labs.com/software/senna.Part-Of-Speech Tagger The part-of-speech clas-sifier we employ is a neural network architecture ofthe same type as in Section 4, where the functionCi = C(i ?
posw) depends now only on the wordposition, and not on a verb.
More precisely:Ci ={0 if 2 |i?
posw| > wsz ?
1Wi?posw otherwise ,564where Wk ?
Rnhu?d and wsz is a window size.We chose wsz = 5 in our experiments.
Thed-dimensional vectors learnt take into account thecapitalization of a word, and the prefix and suf-fix calculated using Porter-Stemmer.
See http://ml.nec-labs.com/software/senna formore details.
We trained on the training set of Prop-Bank supplemented with the Brown corpus, result-ing in a test accuracy on the test set of PropBank of96.85% which compares to 96.66% using the Brilltagger (Brill, 1992).Semantic Role Labeling In our experiments weconsidered a 23-class problem of NULL (no la-bel), the core arguments ARG0-5, REL, ARGA, andARGM- along with the 13 secondary modifier labelssuch as ARGM-LOC and ARGM-TMP.
We simpli-fied R-ARGn and C-ARGn to be written as ARGn,and post-processed ASSERT to do this as well.We compared our system to the freely availableASSERT system (Pradhan et al, 2004).
Both sys-tems are fed only the input sentence during testing,with traces removed, so they cannot make use ofmany PropBank features such as frameset identiti-fier, person, tense, aspect, voice, and form of theverb.
As our algorithm outputs a semantic tag foreach word of a sentence, we directly compare thisper-word accuracy with ASSERT.
Because ASSERTuses a parser, and because PropBank was built by la-beling the nodes of a hand-annotated parse tree, per-node accuracy is usually reported in papers such as(Pradhan et al, 2004).
Unfortunately our approachis based on a completely different premise: we tagwords, not syntactic constituents coming from theparser.
We discuss this further in Section 5.2.The per-word accuracy comparison results can beseen in Table 5.
Before labeling the semantic rolesof each predicate, one must first identify the pred-icates themselves.
If a predicate is not identified,NULL tags are assigned to each word for that pred-icate.
The first line of results in the table takes intoaccount this identification process.
For the neuralnetwork, we used our part-of-speech tagger to per-form this as a verb-detection task.We noticed ASSERT failed to identify relativelymany predicates.
In particular, it seems predicatessuch as ?is?
are sometimes labeled as AUX bythe part-of-speech tagger, and subsequently ignored.We informed the authors of this, but we did not re-ceive a response.
To deal with this, we consideredthe additional accuracy (second row in the table)measured over only those sentences where the pred-icate was identified by ASSERT.Timing results The per-sentence compute time isalso given in Table 5, averaged over all sentences inthe test set.
Our method is around 250 times fasterthan ASSERT.
It is not really feasible to run AS-SERT for most applications.Measurement NN ASSERTPer-word accuracy(all verbs) 83.64% 83.46%Per-word accuracy(ASSERT verbs) 84.09% 86.06%Per-sentencecompute time (secs) 0.02 secs 5.08 secsTable 1: Experimental comparison with ASSERT5.1 Analysis of our MLPWhile we gave an intuitive justification of the archi-tecture choices of our model in Section 4, we nowgive a systematic empirical study of those choices.First of all, providing the position of the word andthe predicate in function C(?, ?)
is essential: the bestmodel we obtained with a window around the wordonly gave 51.3%, assuming correct identification ofall predicates.
Our best model achieves 83.95% inthis setting.If we do not cluster the words according to theirpart-of-speech, we also lose some performance, ob-taining 78.6% at best.
On the other hand, clusteringall words (such as CC, DT, IN part-of-speech tags)also gives weaker results (81.1% accuracy at best).We believe that including all words would give verygood performance if the dataset was large enough,but training only on PropBank leads to overfitting,many words being infrequent.
Clustering is a wayto fight against overfitting, by grouping infrequentwords: for example, words with the label NNP, JJ,RB (which we cluster) appear on average 23, 22 and72 times respectively in the training set, while CC,DT, IN (which we do not cluster) appear 2420, 5659and 1712 times respectively.565Even though some verbs are infrequent, one can-not cluster all verbs into a single group, as each verbdictates the types of semantic roles in the sentence,depending on its frame.
Clustering all words intotheir part-of-speech, including the predicate, givesa poor 73.8% compared with 81.1%, where every-thing is clustered apart from the predicate.Figure 3 gives some anecdotal examples of test setpredictions of our final model compared to ASSERT.5.2 Argument Classification AccuracySo far we have not used the same accuracy measuresas in previous work (Gildea and Jurafsky, 2002;Pradhan et al, 2004).
Currently our architecture isdesigned to label on a per-word basis, while existingsystems perform a segmentation process, and thenlabel segments.
While we do not optimize our modelfor the same criteria, it is still possible to measure theaccuracy using the same metrics.
We measured theargument classification accuracy of our network, as-suming the correct segmentation is given to our sys-tem, as in (Pradhan et al, 2004), by post-processingour per-word tags to form a majority vote over eachsegment.
This gives 83.18% accuracy for our net-work when we suppose the predicate must also beidentified, and 80.53% for the ASSERT software.Measuring only on predicates identified by ASSERTwe instead obtain 84.32% accuracy for our network,and 87.02% for ASSERT.6 DiscussionWe have introduced a neural network architecturethat can provide computationally efficient semanticrole tagging.
It is also a general architecture thatcould be applied to other problems as well.
Becauseour network currently outputs labels on a per-wordbasis it is difficult to assess existing accuracy mea-sures.
However, it should be possible to combineour approach with a shallow parser to enhance per-formance, and make comparisons more direct.We consider this work as a starting point for dif-ferent research directions, including the followingareas:?
Incorporating hand-built features Currently,the only prior knowledge our system encodescomes from part-of-speech tags, in stark con-trast to other methods.
Of course, performanceTRUTH: He camped out at a high-tech nerve centeron the floor of [the Big Board, where]ARGM-LOC [he]ARG0[could]ARGM-MOD [watch]REL [updates on prices and pend-ing stock orders]ARG1.ASSERT (68.7%): He camped out at a high-tech nervecenter on the floor of the Big Board, [ where]ARGM-LOC[he]ARG0 [could]ARGM-MOD [watch]REL [updates]ARG1 onprices and pending stock orders.NN (100%): He camped out at a high-tech nerve centeron the floor of [the Big Board, where]ARGM-LOC [he]ARG0[could]ARGM-MOD [watch]REL [updates on prices and pend-ing stock orders]ARG1.TRUTH: [United Auto Workers Local 1069, which]ARG0[represents]REL [3,000 workers at Boeing?s helicopter unitin Delaware County, Pa.]ARG1 , said it agreed to extend itscontract on a day-by-day basis, with a 10-day notificationto cancel, while it continues bargaining.ASSERT (100%): [United Auto Workers Local 1069,which]ARG0 [represents]REL [3,000 workers at Boeing?shelicopter unit in Delaware County, Pa.]ARG1 , said it agreedto extend its contract on a day-by-day basis, with a 10-daynotification to cancel, while it continues bargaining.NN (89.1%): [United Auto Workers Local 1069,which]ARG0 [represents]REL [3,000 workers at Boeing?shelicopter unit]ARG1 [ in Delaware County]ARGM-LOC , Pa. ,said it agreed to extend its contract on a day-by-day basis,with a 10-day notification to cancel, while it continuesbargaining.Figure 3: Two examples from the PropBank test set,showing Neural Net and ASSERT and gold standardlabelings, with per-word accuracy in brackets.
Notethat even though our labeling does not match thehand-annotated one in the second sentence it stillseems to make some sense as ?in Delaware County?is labeled as a location modifier.
The complete setof predictions on the test set can be found at http://ml.nec-labs.com/software/senna.would improve with more hand-built features.For example, simply adding whether each wordis part of a noun or verb phrase using the hand-annotated parse tree (the so-called ?GOV?
fea-ture from (Gildea and Jurafsky, 2002)) im-proves the performance of our system from83.95% to 85.8%.
One must trade the gener-ality of the model with its specificity, and alsotake into account how long the features take tocompute.?
Incorporating segment information Our systemhas no prior knowledge about segmentation intext.
This could be encoded in many ways:most obviously by using a chunker, but also by566designing a different network architecture, e.g.by encoding contiguity constraints.
To showthe latter is useful, using hand-annotated seg-ments to force contiguity by majority vote leadsto an improvement from 83.95% to 85.6%.?
Incorporating known invariances via virtualtraining data.
In image recognition problemsit is common to create artificial training data bytaking into account invariances in the images,e.g.
via rotation and scale.
Such data improvesgeneralization substantially.
It may be possibleto achieve similar results for text, by ?warp-ing?
training data to create new sentences, orby constructing sentences from scratch using ahand-built grammar.?
Unlabeled data.
Our representation of wordsis as d dimensional vectors.
We could try toimprove this representation by learning a lan-guage model from unlabeled data (Bengio andDucharme, 2001).
As many words in Prop-Bank only appear a few times, the representa-tion might improve, even though the learning isunsupervised.
This may also make the systemgeneralize better to types of data other than theWall Street Journal.?
Transductive Inference.
Finally, one can alsouse unlabeled data as part of the supervisedtraining process, which is called transductionor semi-supervised learning.In particular, we find the possibility of using un-labeled data, invariances and the use of transduc-tion exciting.
These possibilities naturally fit intoour framework, whereas scalability issues will limittheir application in competing methods.ReferencesC.F.
Baker, C.J.
Fillmore, and J.B. Lowe.
1998.
TheBerkeley FrameNet project.
Proceedings of COLING-ACL, 98.Y.
Bengio and R. Ducharme.
2001.
A neural probabilis-tic language model.
In Advances in Neural Informa-tion Processing Systems, NIPS 13.B.E.
Boser, I.M.
Guyon, and V.N.
Vapnik.
1992.
A train-ing algorithm for optimal margin classifiers.
Proceed-ings of the fifth annual workshop on Computationallearning theory, pages 144?152.J.S.
Bridle.
1990.
Probabilistic interpretation of feed-forward classification network outputs, with relation-ships to statistical pattern recognition.
In F. FogelmanSoulie?
and J.
He?rault, editors, Neurocomputing: Al-gorithms, Architectures and Applications, pages 227?236.
NATO ASI Series.E.
Brill.
1992.
A simple rule-based part of speech tag-ger.
Proceedings of the Third Conference on AppliedNatural Language Processing, pages 152?155.E.
Charniak.
2000.
A maximum-entropy-inspired parser.Proceedings of the first conference on North Americanchapter of the Association for Computational Linguis-tics, pages 132?139.D.
Gildea and D. Jurafsky.
2002.
Automatic label-ing of semantic roles.
Computational Linguistics,28(3):245?288.D.
Gildea and M. Palmer.
2001.
The necessity of pars-ing for predicate argument recognition.
Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, pages 239?246.J.
Henderson.
2004.
Discriminative training of a neuralnetwork statistical parser.
In Proceedings of the 42ndMeeting of Association for Computational Linguistics.Y.
LeCun, L. Bottou, G. B. Orr, and K.-R. Mu?ller.
1998.Efficient backprop.
In G.B.
Orr and K.-R. Mu?ller, ed-itors, Neural Networks: Tricks of the Trade, pages 9?50.
Springer.M.P.
Marcus, M.A.
Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of En-glish: the penn treebank.
Computational Linguistics,19(2):313?330.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
Theproposition bank: An annotated corpus of semanticroles.
Comput.
Linguist., 31(1):71?106.S.
Pradhan, W. Ward, K. Hacioglu, J. Martin, and D. Ju-rafsky.
2004.
Shallow semantic parsing using supportvector machines.
Proceedings of HLT/NAACL-2004.V.
Punyakanok, D. Roth, and W. Yih.
2005.
The ne-cessity of syntactic parsing for semantic role labeling.Proceedings of IJCAI?05, pages 1117?1123.A.
Ratnaparkhi.
1997.
A linear observed time statisticalparser based on maximum entropy models.
Proceed-ings of EMNLP.H.
Schwenk and J.L.
Gauvain.
2002.
Connection-ist language modeling for large vocabulary continu-ousspeech recognition.
Proceedings of ICASSP?02.D.
H. Younger.
1967.
Recognition and parsing ofcontext-free languages in time n3.
Information andControl, 10.567
