Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1797?1807,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsEvent Schema Induction with a Probabilistic Entity-Driven ModelNathanael ChambersUnited States Naval AcademyAnnapolis, MD 21402nchamber@usna.eduAbstractEvent schema induction is the task of learninghigh-level representations of complex events(e.g., a bombing) and their entity roles (e.g.,perpetrator and victim) from unlabeled text.Event schemas have important connections toearly NLP research on frames and scripts,as well as modern applications like templateextraction.
Recent research suggests eventschemas can be learned from raw text.
In-spired by a pipelined learner based on namedentity coreference, this paper presents the firstgenerative model for schema induction that in-tegrates coreference chains into learning.
Ourgenerative model is conceptually simpler thanthe pipelined approach and requires far lesstraining data.
It also provides an interestingcontrast with a recent HMM-based model.
Weevaluate on a common dataset for templateschema extraction.
Our generative modelmatches the pipeline?s performance, and out-performs the HMM by 7 F1 points (20%).1 IntroductionEarly research in language understanding focusedon high-level semantic representations to drive theirmodels.
Many proposals, such as frames and scripts,used rich event schemas to model the situations de-scribed in text.
While the field has since focused onmore shallow approaches, recent work on schemainduction shows that event schemas might be learn-able from raw text.
This paper continues the trend,addressing the question, can event schemas be in-duced from raw text without prior knowledge?
Wepresent a new generative model for event schemas,and it produces state-of-the-art induction results, in-cluding a 7 F1 point gain over a different generativeproposal developed in parallel with this work.Event schemas are unique from most work in in-formation extraction (IE).
Current relation discovery(Banko et al 2007a; Carlson et al 2010b) focuseson atomic facts and relations.
Event schemas buildrelations into coherent event structures, often calledtemplates in IE.
For instance, an election templatejointly connects that obama won a presidential elec-tion with romney was the defeated, the election oc-curred in 2012, and the popular vote was 50-48.
Theentities in these relations fill specific semantic roles,as in this template schema:Template Schema for Elections(events: nominate, vote, elect, win, declare, concede)Date: TimestampWinner: PersonLoser: PersonPosition: OccupationVote: NumberTraditionally, template extractors assume fore-knowledge of the event schemas.
They know a Win-ner exists, and research focuses on supervised learn-ing to extract winners from text.
This paper focuseson the other side of the supervision spectrum.
Thelearner receives no human input, and it first inducesa schema before extracting instances of it.Our proposed model contributes to a growingline of research in schema induction.
The majorityof previous work relies on ad-hoc clustering algo-rithms (Filatova et al 2006; Sekine, 2006; Cham-bers and Jurafsky, 2011).
Chambers and Jurafskyis a pipelined approach, learning events first, andlater learning syntactic patterns as fillers.
It requires1797several ad-hoc metrics and parameters, and it lacksthe benefits of a formal model.
However, central totheir algorithm is the use of coreferring entity men-tions to knit events and entities together into an eventschema.
We adapt this entity-driven approach to asingle model that requires fewer parameters and farless training data.
Further, experiments show state-of-the-art performance.Other research conducted at the time of this pa-per also proposes a generative model for schema in-duction (Cheung et al 2013).
Theirs is not entity-based, but instead uses a sequence model (HMM-based) of verb clauses.
These two papers thus pro-vide a unique opportunity to compare two very dif-ferent views of document structure.
One is entity-driven, modeling an entity?s role by its coreferencechain.
The other is clause-driven, classifying indi-vidual clauses based on text sequence.
Each modelmakes unique assumptions, providing an interest-ing contrast.
Our entity model outperforms by 7 F1points on a common extraction task.The rest of the paper describes in detail ourmain contributions: (1) the first entity-based gen-erative model for schema induction, (2) a directpipeline/formal model comparison, (3) results im-proving state-of-the-art performance by 20%, and(4) schema induction from the smallest amount oftraining data to date.2 Previous WorkUnsupervised learning for information extractionusually learns binary relations and atomic facts.Models can learn relations like Person is married toPerson without labeled data (Banko et al 2007b), orrely on seed examples for ontology induction (dog isa mammal) and attribute extraction (dogs have tails)(Carlson et al 2010b; Carlson et al 2010a; Huangand Riloff, 2010; Durme and Pasca, 2008).
These donot typically capture the deeper connections mod-eled by event schemas.Algorithms that do focus on event schema extrac-tion typically require both the schemas and labeledcorpora, such as rule-based approaches (Chinchoret al 1993; Rau et al 1992) and modern super-vised classifiers (Freitag, 1998; Chieu et al 2003;Bunescu and Mooney, 2004; Patwardhan and Riloff,2009; Huang and Riloff, 2011).
Classifiers rely onthe labeled examples?
surrounding context for fea-tures (Maslennikov and Chua, 2007).
Weakly su-pervised learning removes some of the need for la-beled data, but most still require the event schemas.One common approach is to begin with unlabeled,but clustered event-specific documents, and extractcommon word patterns as extractors (Riloff andSchmelzenbach, 1998; Sudo et al 2003; Riloff etal., 2005; Filatova et al 2006; Patwardhan andRiloff, 2007; Chen et al 2011).
Bootstrapping withseed examples of known slot fillers has been shownto be effective (Yangarber et al 2000; Surdeanu etal., 2006).Shinyama and Sekine (2006) presented unre-stricted relation discovery to discover relations inunlabeled documents.
Their algorithm used redun-dant documents (e.g., all describe Hurricane Ivan)to observe repeated proper nouns.
The approach re-quires many documents about the exact same eventinstance, and relations are binary (not schemas) overrepeated named entities.
Our model instead learnsschemas from documents with mixed topics thatdon?t describe the same event, so repeated propernouns are less helpful.Chen et al(2011) perform relation extractionwith no supervision on earthquake and finance do-mains.
Theirs is a generative model that representsrelations as predicate/argument pairs.
As with oth-ers, training data is pre-clustered by event type andthere is no schema connection between relations.This paper builds the most on Chambers and Ju-rafsky (2011).
They learned event schemas with athree-stage clustering algorithm that included a re-quirement to retrieve extra training data.
This paperremoves many of these complexities.
We presenta formal model that uniquely models coreferencechains.
Advantages include a joint clustering ofevents and entities, and a formal probabilistic inter-pretation of the resulting schemas.
We achieve betterperformance, and do so with far less training data.Cheung et al(2013) is most related as a genera-tive formulation of schema induction.
They proposean HMM-based model over latent event variables,where each variable generates the observed clauses.Latent schema variables generate the event vari-ables (in the spirit of preliminary work by O?Connor(2012)).
There is no notion of an entity, so learninguses text mentions and relies on the local HMM win-1798message: id dev-muc3-0112 (bellcore, mitre)incident: date 10 mar 89incident: location peru: huanuco, ambo (town)incident: type bombingincident: stage accomplishedincident: instrument explosive: ?-?perp: individual ?shining path members?perp: organization ?shining path?Figure 1: A subset of the slots in a MUC-4 template.dow for event transitions.
Their model was createdin parallel with our work, and provides a nice con-trast in both approach and results.
Ours outperformstheir model by 20% on a MUC-4 evaluation.In summary, this paper extends most previouswork on event schema induction by removing thesupervision.
Of the recent ?unsupervised?
work, wepresent the first entity-driven generative model, andwe experiment on a mixed-domain corpus.3 Dataset: The MUC-4 CorpusThe corpus from the Message Understanding Con-ference (MUC-4) serves as the challenge text (Sund-heim, 1991), and will ground discussion of ourmodel.
MUC-4 is also used by the closest previ-ous work.
It contains Latin American newswireabout terrorism events, and it provides a set ofhand-constructed event schemas that are tradition-ally called template schemas.
It also maps labeledtemplates to the text, providing a dataset for tem-plate extraction evaluations.
Until very recently,only extraction has been evaluated.
We too evalu-ate our model through extraction, but we also com-pare our learned schemas to the hand-created tem-plate schemas.
An example of a filled in MUC-4template is given in Figure 1.The MUC-4 corpus defines six template types:Attack, Kidnapping, Bombing, Arson, Robbery,and Forced Work Stoppage.
Documents are oftenlabeled with more than one template and type.
Manyinclude multiple events at different times in differentlocations.
The corpus is particularly challenging be-cause template schemas are inter-mixed and entitiescan play multiple roles across instances.The training corpus contains 1300 documents,733 of which are labeled with at least one schema.567 documents are not labeled with any schemas.These unlabeled documents are articles that reporton non-specific political events and speeches.
Theymake the corpus particularly challenging.
The de-velopment and test sets each contain 200 documents.4 A Generative Model for Event SchemasThis paper?s model is an entity-based approach, sim-ilar in motivation to Haghighi and Klein (2010) andthe pipelined induction of Chambers and Jurafsky(2011).
Coreference resolution guides the learningby providing a set of pre-resolved entities.
Eachentity receives a schema role label, so it allows allmentions of the entity to inform that role choice.This important constraint links coreferring mentionsto the same schema role, and distinguishes our ap-proach from others (Cheung et al 2013).4.1 IllustrationThe model represents a document as a set of enti-ties.
An entity is a set of entity mentions clusteredby coreference resolution.
We will use the followingtwo sentences for illustration:A truck bomb exploded near the embassy.Three militia planted it, and then they fled.This text contains five entity mentions.
A perfectcoreference resolution system will resolve these fivementions into three entities:Entity Mentions Entities Rolesa truck bomb (a truck bomb, it) Instrumentthe embassy (the embassy) Targetthree militia (three militia, they) PerpetratorittheyThe schema roles, or template slots, are the typeof target knowledge we want to learn.
Each en-tity will be labeled with both a slot variable s anda template variable t (e.g., the s=perpetrator of at=bombing).
The lexical context of the entity men-tions guides the learning model to this end.4.2 DefinitionsA document d ?
D is represented as a set of entitiesEd.
Each entity e ?
Ed is a triple: e = (h,M,F )1. he is the canonical word for the entity (typicallythe first mention?s head word)1799TextA truck bomb exploded near the embassy.Three militia planted it, and then they fled.Entity Representationentity 1: h = bomb, F = {PHYS-OBJ},M = { (p=explode, d=subject-explode)(p=plant, d=object-plant) }entity 2: h = militia, F = {PERSON, ORG},M = { (p=plant, d=subject-plant),(p=flee, subject-flee) }entity 3: h = embassy, F = {PHYS-OBJ, ORG},M = { (p=explode, d=prep near-explode) }Figure 2: Example text mapped to our entities.2.
Me is a set of entity mentions m ?
Me.
Eachmention is a pairm = (p, d): the predicate, andthe typed dependency from the predicate to themention (e.g., push and subject-push).3.
Fe is a set of binary entity features.
This paperonly uses named entity types as features, butgeneralizes to other features as well.A document is thus reduced to its entities, theirgrammatical contexts, and entity features.
Figure 2continues our example using this formulation.
he ischosen to be e?s longest non-pronoun mention m ?Me.
Mentions are labeled with NER and WordNetsynsets to create an entity?s features Fe ?
{Person,Org, Loc, Event, Time, Object, Other}.
We use theStanford NLP toolkit to parse, extract typed depen-dencies, label with NER, and run coreference.4.3 The Generative ModelsSimilar to topics in LDA, each document d in ourmodel has a corresponding multinomial over schematypes ?d, drawn from a Dirichlet.
For each entity inthe document, a hidden variable t is drawn accord-ing to ?d.
These t variables represent the high levelschema types, such as bombing or kidnapping.
Thepredicates associated with each of the entity?s men-tions are then drawn from the schema?s multinomialover predicates Pt.
The variable t also generatesa hidden variable s from its distribution over slots,such as perpetrator and victim.
Finally, the entity?scanonical head word is generated from ?s, all entitymentions?
typed dependencies from ?s, and namedentity types from ?s.The most important characteristic of this modelis the separation of event words from the lexicalproperties of specific entity mentions.
The schematype variables t only model the distribution of eventwords (bomb, plant, defuse), but the slot variabless model the syntax (subject-bomb, subject-plant,object-arrest) and entity words (suspect, terrorist,man).
This allows the high-level schemas to first se-lect predicates, and then forces predicate argumentsto prefer slots that are in the parent schema type.Formally, a document d receives a labeling Zdwhere each entity e ?
Ed is labeled Zd,e = (t, s)with a schema type t and a slot s. The joint distribu-tion of a document and labeling is then as follows:P (d, Zd) =?e?EdP (t|?)?
P (s|t)?
?e?EdP (he|s)?
?e?Ed?f?FeP (f |s)?
?e?Ed?m?MeP (dm|s) ?
P (pm|t) (1)The plate diagram for the model is given in Fig-ure 3.
The darker circles correspond to the observedentity components in Figure 2.
We assume the fol-lowing generative process for a document d:Generate ?d from Dir(?
)for each schema type t = 1...m doGenerate Pt from Dir(?
)for each slot st = 1...k doGenerate ?s from Dir(?
)Generate ?s from Dir(?
)Generate ?s from Dir(?
)for each entity e ?
Ed doGenerate schema type t from Multinomial(?d)Generate slot s from UniformDist(k)Generate head word h from Multinomial(?s)for each mention m ?Me doGenerate predicate token p from Multinomial(Pt)Generate typed dependency d from Multinomial(?s)for each entity type i = 1..|Fe| doGenerate entity type f from Multinomial(?s)The number of schema types m and the numberof slots per schema k are chosen based on trainingset performance.1800sh d?f?
?
?
kEMFtvV mkxmDEpPNAMED ENTITIESENTITY MENTIONSENTITY FEATURESENTITY HEADDOCUMENTSFigure 3: The full plate diagram for the event schemamodel.
Hyper-parameters are omitted for readability.The Flat Relation ModelWe also experiment with a Flat Relation Model thatremoves the hidden t variables, ignoring schematypes.
Figure 4 visually compares this flat modelwith the full model.
We found that the predicatedistribution Pt hurts performance in a flat model.Predicates are more informative at the higher level,but less so for slots where syntax is more important.We thus removed Pt from the model, and everythingelse remains the same.
This flat model now learnsa large set of k slots S that aren?t connected by ahigh-level schema variable.
Each slot s ?
S has acorresponding triple of multinomials (h,M,F ) sim-ilar to above: (1) a multinomial over the head men-tions ?s, (2) a multinomial over the grammatical re-lations of the entity mentions ?s, and (3) a multino-mial over the entity features ?s.
For each entity ina document, a hidden slot s ?
S is first drawn from?, and then the observed entity (h,M,F ) is drawnaccording to the multinomials (?s, ?s, ?s).
We laterevaluate this flat model to show the benefit of addedschema structure.4.4 InferenceWe use collapsed Gibbs sampling for inference,sampling the latent variables te,d and se,d in se-shd?
k EMFf?s hd?
k EMFf??
mxm?tFigure 4: Simplified plate diagrams comparing the flatrelation model to the full template model.
The observedf ?
F variables are not included for clarity.quence conditioned on a full setting of all the othervariables (Griffiths and Steyvers, 2004).
Initial pa-rameter values are set by randomly setting t and svariables from the uniform distribution over schematypes and slots, then computing the other parametervalues based on these initial settings.
The hyperpa-rameters for the dirichlet distributions were chosenfrom a small grid search (see Experiments).Beyond standard inference, we added one con-straint to the model that favors grammatical distri-butions ?s that do not contain conflicts.
The subjectand direct object of a verb should not both receivehigh probability mass under the same schema slot?s.
For instance, the victim of a kidnapping shouldnot favor both the subject and object of a single verb.Semantic roles should (typically) select one syntac-tic slot, so this constraint encourages that behavior.During sampling of se,d, we use a penalty factor ?to make conflicting relations less likely.
Formally,P (se,d = s|?, he, Fe,Me) = ?
iff there exists anm ?
Me such that P (m|?s) < P (inv(m)|?s) andP (inv(m)|?s) > 0.1, where inv(m) = object ifm = subject and vice versa.
Otherwise, the proba-bility is computed as normal.
We normalize the dis-tributions after penalties are computed.4.5 Entity Extraction for Template FillingInducing event schemas is only one benefit of themodel.
The learned model can also extract spe-cific instances of the learned schemas without ad-1801ditional complexity.
To evaluate the effectiveness ofthe model, we apply the model to perform standardtemplate extraction on MUC-4.
Previous MUC-4induction required an extraction algorithm separatefrom induction because induction created hard clus-ters (Chambers and Jurafsky, 2011).
Cluster scoresdon?t have a natural interpretation, so extraction re-quired several parameters/thresholds to tune.
Ourmodel instead simply relies on model inference.We run inference as described above and each en-tity receives a template label te,d and a template slotlabel se,d.
These labels are the extractions, and it re-quires no other parameters.
The model thus requiresfar less machinery than a pipeline, and the exper-iments below further show that this simpler modeloutperforms the pipeline.Beyond parameters, the question of ?irrelevant?documents is a concern in MUC-4.
Approximatelyhalf the corpus are documents that are not labeledwith a template, so past algorithms required extraprocessing stages to filter out these irrelevant doc-uments.
Patwardhan and Riloff (2009) and Cham-bers and Jurafsky (2011) make initial decisions as towhether they should extract or not from a document.Huang and Riloff (2011) use a genre detector for thisproblem.
Even the generative HMM-based model ofCheung et al(Cheung et al 2013) requires an ex-tra filtering parameter.
Our formal model is uniquein not requiring additional effort.
Ours is the onlyapproach that doesn?t require document filtering.5 Evaluation SetupEvaluating on MUC-4 has a diverse history thatcomplicates comparison.
The following balancescomparison against previous work and enables fu-ture comparison to our results.5.1 Template Schema SlotsMost systems do not evaluate performance on allMUC-4 template slots.
They instead focus on fourmain slots, ignoring the parameterized slots that in-volve deeper reasoning (such as ?stage of execution?and ?effect of incident?).
The four slots and exampleentity fillers are shown here:Perpetrator: Shining Path membersVictim: Sergio HornaTarget: public facilitiesInstrument: explosivesWe also focus only on these four slots.
We mergedMUC?s two perpetrator slots (individuals and orgs)into one gold Perpetrator.
Previous work has bothsplit the two and merged the two.
We merge thembecause the distinction between an individual andan organization is often subtle and not practicallyimportant to analysts.
This is also consistent withthe most recent event schema induction in Chambersand Jurafsky (2011) and Cheung et al(2013).One peculiarity in MUC-4 is that some templatesare labeled as optional (i.e., all its slots are optional),and some required templates contain optional slots(i.e., a subset of slots are optional).
We ignoreboth optional templates and specific optional slotswhen computing recall, as in previous work (Pat-wardhan and Riloff, 2007; Patwardhan and Riloff,2009; Chambers and Jurafsky, 2011).Comparison between the extracted strings and thegold template strings uses head word scoring.
Wedo not use gold parses for the text, so head wordsare defined simply as the rightmost word in the nounphrase.
The exception is when the extracted phraseis of the form ?A of B?, then the rightmost word in?A?
is used as the head.
This is again consistent withprevious work1.
The standard evaluation metrics areprecision, recall, and F1 score.5.2 Mapping Learned SlotsInduced schemas need to map to gold schemas be-fore evaluation.
Which learned slots correspond toMUC-4 slots?
There are two methods of mapping.The first ignores the schema type variables t, andsimply finds the best performing s variable for eachgold template slot2.
We call this the slot-only map-ping evaluation.
The second approach is to map eachtemplate variable t to the best gold template type g,and limit the slot mapping so that only the slots un-der t can map to slots under g. We call this the tem-plate mapping evaluation.
The slot-only mappingcan result in higher scores since it is not constrainedto preserve schema structure in the mapping.Chambers and Jurafsky (2011) used templatemapping in their evaluation.
Cheung et al(2013)used slot-only mapping.
We run both evaluations inthis paper and separately compare both.1Personal communications with Patwardhan and Riloff2bombing-victim is a template slot distinct from kidnap-victim.
Both need to be mapped.18026 ExperimentsWe use the Stanford CoreNLP toolkit for text pro-cessing and parsing.
We developed the models onthe 1300 document MUC-4 training set.
We thenlearned once on the entire 1700 training/dev/test set,and report extraction numbers from the inferred la-bels on the 200 document test set.
Each experimentwas repeated 10 times.
Reported numbers are aver-aged across these runs.There are two structure variables for the model:the number of schema types and the number of slotsunder each type.
We searched for the optimal valueson the training set before evaluating on test.
Thehyperparameters for all evaluations were set to ?
=?
= ?
= ?
= 1, ?
= .1 based on a grid search.6.1 Template Schema InductionThe first evaluation compares the learned schemasto the gold schemas in MUC-4.Since most previous work assumes this knowl-edge ahead of time, we align our schemas with themain MUC-4 template types to measure quality.
Weinspected the learned event schemas that mapped toMUC-4 schemas based on the template mapping ex-traction evaluation.Figure 5 shows some of the learned distribu-tions for two mapped schemas: kidnappings andbombings.
The predicate distribution for each eventschema is shown, as well as the top 5 head wordsand grammatical relations for each slot.
The wordsand events that were jointly learned in these exam-ples appear quite accurate.
The bombing and kidnapschemas learned all of the equivalent MUC-4 goldslots.
Interestingly, our model also learned Loca-tions and Times as important entities that appear inthe text.
These entities are not traditionally includedin the MUC-4 extraction task.Figure 6 lists the MUC-4 slots that we did anddid not learn for the four most prevelant types.
Wereport 71% recall, with almost all errors due to themodel?s failure to learn about arsons.
Arson tem-plates only occur in 40 articles, much less than the200 bombing and over 400 attack.
We show belowthat overall extraction performs well despite this.The learned distributions for Attack end up extract-ing Arson perpetrators and Arson victims in the ac-tual extraction evaluation.Bomb Kidnap Attack ArsonPerpetrator X X X xVictim X X X XTarget X - X xInstrument X - x xLocation X X X XDate/Time X X X xFigure 6: The MUC-4 gold slots that were learned.
Thebottom two are not in the traditional evaluation, but werelearned by our model nonetheless.Evaluation: Template MappingPrec Recall F1C & J 2011 .48 .25 .33Formal Template Model .42 .27 .33Table 1: MUC-4 extraction with template mapping.
Alearned schema first maps to a gold MUC template.Learned slots can then only map to slots in that template.6.2 Extraction ExperimentsWe now present the full extraction experiment thatis traditionally used for evaluating MUC-4 per-formance.
Although our learned schemas closelymatch gold schemas, extraction depends on howwell the model can extract from diverse lexical con-texts.
We ran inference on the full training and testsets, and used the inferred labels as schema labels.These labels were mapped and evaluated against thegold MUC-4 labels as discussed in Section 5.Performance is compared to two state-of-the-artinduction systems.
Since these previous two mod-els used different methods to map their learnedschemas, we compare separately.
Table 1 shows thetemplate mapping evaluation with Chambers and Ju-rafsky (C&J).
Table 2 shows the slot-only mappingevaluation with Cheung et alOur model achieves an F1 score comparable toC&J, and 20% higher than Cheung et alPart of thegreater increase over Cheung et alis the mappingdifference.
For each MUC-4 type, such as bombing,any four learned slots can map to the four MUC-4 bombing slots.
There is no constraint that thelearned slots must come from the same schema type.The more strict template mapping (Table 1) ensuresthat entire schema types are mapped together, and itreduces our performance from .41 to .33.1803Kidnapping EntitiesVictim (Person 88%)businessman object-kidnapcitizen object-releaseSoares prep of-kidnappingKent possessive-releasehostage object-foundPerpetrator (Person 62%, Org 30%)guerrilla subject-kidnapELN subject-holdgroup subject-attackextraditables subject-demandman subject-announceDate (TimeDate 89%)TIME tmod-kidnapFebruary prep on-kidnaphours tmod-releasemorning prep on-releasenight tmod-takeBombing EntitiesVictim (Person 86%, Location 8%)person object-killguerrilla object-woundsoldier subject-dieman subject-blow upcivilian subject-tryPhysical Target (Object 65%, Event 42%)building object-destroyoffice object-damageexplosive object-usestation and-officevehicle prep of-numberInstrument (Event 56%, Object 39%)bomb subject-explodeexplosion subject-occurattack object-causecharge object-placedevice subject-destroyFigure 5: Select distributions for two learned events.
Left columns are head word distributions ?, right columns aresyntactic relation distributions ?, and entity types in parentheses are the learned ?.
Most probable words are shown.Evaluation: Slot-Only MappingPrec Recall F1Cheung et al2013 .32 .37 .34Flat Relation Model .26 .45 .33Formal Template Model .41 .41 .41Table 2: MUC-4 extraction with slot-only mapping.
Anylearned slot is allowed to map to any gold slot.Entity Role PerformancePrec Recall F1Perpetrator .40 .20 .26Victim .42 .31 .34Target .38 .28 .31Instrument .57 .39 .45Table 3: Results for each MUC-4 template slot using thetemplate-mapping evaluation.The macro-level F1 scores can be broken downinto individual slot performance.
Table 3 showsthese results ranging from .26 to .45.
The Instrumentrole proves easiest to learn, consistent with C&J.A large portion of MUC-4 includes irrelevantdocuments.
Cheung et al(2013) evaluated theirmodel without irrelevant documents in the test setthat to see how performance is affected.
We com-pare against their numbers in Table 4.
Results arecloser now with ours outperforming .46 to .43 F1.This suggests that the HMM-based approach stum-bles more on spurious documents, but performs bet-ter on relevant ones.Gold Document EvaluationPrec Recall F1Cheung et al2013 .41 .44 .43Formal Template Model .49 .43 .46Table 4: Full MUC-4 extraction with gold document clas-sification.
These results ignore false positives extractedfrom ?irrelevant?
documents in the test set.6.3 Model AblationTable 2 shows that the flat relation model (no latenttype variables t) is inferior to the full schema model.F1 drops 20% without the explicit modeling of bothschema types t and their entity slots s. The entityfeatures Fe are less important.
Experiments with-out them show a slight drop in performance (2 F1points), small enough that they could be removed forefficiency.
However, it is extremely useful to learnslots with NER labels like Person or Location.Finally, we experimented without the sub-ject/object constraint (Section 4.4).
Performancedrops 5-10% depending on the number of schemaslearned.
Anecdotally, it merges too many schemaslots that should be separate.
We recommend usingthis constraint as it has little impact on CPU time.6.4 Extension: Reduce Training SizeOne of the main benefits of this generative modelappears to be the reduction in training data.
Thepipelined approach in C&J required an informationretrieval stage to bring in hundreds of other docu-1804ments from an external corpus.
This paper?s genera-tive model doesn?t require such a stage.We thus attempted to induce and extract eventschemas from just the 200 test set documents, withno training or development data.
We repeated thisexperiment 30 times and averaged the results, settingthe number of templates t = 20 and slots s = 10 asin the main experiment.
The resulting F1 score forthe template-mapping evaluation fell to 0.27 fromthe full data experiment of 0.33 F1.
Adding moretraining documents in another experiment did notsignificantly increase performance over 0.27 untilall training and development documents were in-cluded.
This could be explained by the develop-ment set being more similar to the test set than train-ing.
We did not investigate further to prevent over-experimentation on test.7 DiscussionOur model is one of the first generative formula-tions of schema induction.
It produces state-of-the-art performance on a traditional extraction task, andperforms with less training data as well as a morecomplex pipelined approach.
Further, our uniqueentity-driven approach outperforms an HMM-basedmodel developed in parallel to this work.Our entity-driven proposal is strongly influencedby the ideas in the pipeline model of Chambers andJurafsky (2011).
Coreference chains have been usedin a variety of learning tasks, such as narrative learn-ing and summarization.
Here we are the first to showhow it can be used for schema induction in a proba-bilistic model, connecting predicates across a docu-ment in a way that is otherwise difficult to represent.The models perform similarly, but ours also includessignificant benefits like a reduction in complexity,reproducibility, and a large reduction in training datarequirements.This paper also implies that learning and ex-traction need not be independent algorithms.
Ourmodel?s inference procedure to learn schemas is thesame one that labels text for extraction.
C&J re-quired 3-4 separate pipelined steps.
Cheung et al(2013) required specific cutoffs for document classi-fication before extraction.
Not only does our modelperform well, but it does so without these steps.Highlighted here are key differences between thisproposal and the HMM-based model of Cheung etal.
(2013).
One of the HMM strengths is the in-clusion of sequence-based knowledge.
Each slot la-bel is influenced by the previous label in the text,encouraging syntactic arguments of a predicate tochoose the same schema.
This knowledge is onlyloosely present in our document distribution ?.
Che-ung et alalso include a hidden event variable be-tween the template and slot variables.
Our modelcollapses this event variable and makes fewer depen-dency assumptions.
This difference requires furtherinvestigation as it is unclear if it provides valuableinformation, or too much complexity.We also note a warning for future work on properevaluation methodology.
This task is particularlydifficult to compare to other models due to itscombination of both induction and then extraction.There are many ways to map induced schemas togold answers, and this paper illustrates how ex-traction performance is significantly affected by thechoice.
We suggest the template-mapping evalua-tion to preserve learned structure.Finally, these induced results are far behind su-pervised learning (Huang and Riloff, 2011).
Thereis ample room for improvement and future researchin event schema induction.AcknowledgmentsThis work was partially supported by a grant fromthe Office of Naval Research.
It was also sup-ported, in part, by the Johns Hopkins Human Lan-guage Technology Center of Excellence.
Any opin-ions, findings, and conclusions or recommendationsexpressed in this material are those of the author.Thanks to Eric Wang for his insights into Bayesianmodeling, Brendan O?Connor for his efforts on nor-malizing MUC-4 evaluation details, Frank Ferraroand Benjamin Van Durme for helpful conversations,and to the reviewers for insightful feedback.1805ReferencesMichele Banko, Michael J Cafarella, Stephen Soderland,Matt Broadhead, and Oren Etzioni.
2007a.
Learningrelations from the web.
In Proceedings of the Interna-tional Joint Conferences on Artificial Intelligence (IJ-CAI).Michele Banko, Michael J Cafarella, Stephen Soderland,Matt Broadhead, and Oren Etzioni.
2007b.
Open in-formation extraction from the web.
In Proceedings ofthe International Joint Conferences on Artificial Intel-ligence (IJCAI).Razvan Bunescu and Raymond Mooney.
2004.
Collec-tive information extraction with relational markov net-works.
In Proceedings of the Association for Compu-tational Linguistics (ACL), pages 438?445.Andrew Carlson, J. Betteridge, B. Kisiel, B. Settles,E.R.
Hruschka Jr., and T.M.
Mitchell.
2010a.
To-ward an architecture for never-ending language learn-ing.
In Proceedings of the Conference on ArtificialIntelligence (AAAI).Andrew Carlson, J. Betteridge, R.C.
Wang, E.R.
Hr-uschka Jr., and T.M.
Mitchell.
2010b.
Coupled semi-supervised learning for information extraction.
In Pro-ceedings of the ACM International Conference on WebSearch and Data Mining (WSDM).Nathanael Chambers and Dan Jurafsky.
2011.
Template-based information extraction without the templates.
InProceedings of the Association for Computational Lin-guistics.Harr Chen, Edward Benson, Tahira Naseem, and ReginaBarzilay.
2011.
In-domain relation discovery withmeta-constraints via posterior regularization.
In Pro-ceedings of the Association for Computational Lin-guistics (ACL).Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-derwende.
2013.
Probabilistic frame induction.
InProceedings of the North American Chapter of the As-sociation for Computational Linguistics: Human Lan-guage Technologies.Hai Leong Chieu, Hwee Tou Ng, and Yoong Keok Lee.2003.
Closing the gap: Learning-based informationextraction rivaling knowledge-engineering methods.In Proceedings of the Association for ComputationalLinguistics (ACL).Nancy Chinchor, David Lewis, and Lynette Hirschman.1993.
Evaluating message understanding systems: ananalysis of the third message understanding confer-ence.
Computational Linguistics, 19:3:409?449.Benjamin Van Durme and Marius Pasca.
2008.
Findingcars, goddesses and enzymes: Parametrizable acquisi-tion of labeled instances for open-domain informationextraction.
In Proceedings of the 23rd Annual Con-ference on Artificial Intelligence (AAAI-2008), pages1243?1248.Elena Filatova, Vasileios Hatzivassiloglou, and KathleenMcKeown.
2006.
Automatic creation of domain tem-plates.
In Proceedings of the Association for Compu-tational Linguistics (ACL).Dayne Freitag.
1998.
Toward general-purpose learningfor information extraction.
In Proceedings of the As-sociation for Computational Linguistics (ACL), pages404?408.T.
L. Griffiths and M. Steyvers.
2004.
Finding scien-tific topics.
In Proceedings of the National Academy ofSciences of the United States of America, pages 5228?5235.Aria Haghighi and Dan Klein.
2010.
An entity-level ap-proach to information extraction.
In Proceedings ofthe Association for Computational Linguistics (ACL).Ruihong Huang and Ellen Riloff.
2010.
Inducingdomain-specific semantic class taggers from (almost)nothing.
In Proceedings of the Association for Com-putational Linguistics (ACL).Ruihong Huang and Ellen Riloff.
2011.
Peeling back thelayers: Detecting event role fillers in secondary con-texts.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics (ACL).Mstislav Maslennikov and Tat-Seng Chua.
2007.
Auto-matic acquisition of domain knowledge for informa-tion extraction.
In Proceedings of the Association forComputational Linguistics (ACL).Brendan O?Connor.
2012.
Learning frames from textwith an unsupervised latent variable model.
Technicalreport, Carnegie Mellon University.Siddharth Patwardhan and Ellen Riloff.
2007.
Effectiveie with semantic affinity patterns and relevant regions.In Proceedings of the Conference on Empirical Meth-ods on Natural Language Processing (EMNLP).Siddharth Patwardhan and Ellen Riloff.
2009.
A unifiedmodel of phrasal and sentential evidence for informa-tion extraction.
In Proceedings of the Conference onEmpirical Methods on Natural Language Processing(EMNLP).Lisa Rau, George Krupka, Paul Jacobs, Ira Sider, andLois Childs.
1992.
Ge nltoolset: Muc-4 test resultsand analysis.
In Proceedings of the Message Under-standing Conference (MUC-4), pages 94?99.Ellen Riloff and Mark Schmelzenbach.
1998.
An em-pirical approach to conceptual case frame acquisition.In Proceedings of the Sixth Workshop on Very LargeCorpora.Ellen Riloff, Janyce Wiebe, and William Phillips.
2005.Exploiting subjectivity classification to improve infor-mation extraction.
In Proceedings of AAAI-05.Satoshi Sekine.
2006.
On-demand information extrac-tion.
In Proceedings of the Joint Conference of the1806International Committee on Computational Linguis-tics and the Association for Computational Linguis-tics, pages 731?738.Yusuke Shinyama and Satoshi Sekine.
2006.
Preemptiveie using unrestricted relation discovery.
In Proceed-ings of NAACL.Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.2003.
An improved extraction pattern representationmodel for automatic ie pattern acquisition.
In Pro-ceedings of the Association for Computational Lin-guistics (ACL), pages 224?231.Beth M. Sundheim.
1991.
Third message understand-ing evaluation and conference (muc-3): Phase 1 statusreport.
In Proceedings of the Message UnderstandingConference.Mihai Surdeanu, Jordi Turmo, and Alicia Ageno.
2006.A hybrid approach for the acquisition of informationextraction patterns.
In Proceedings of the EACL Work-shop on Adaptive Text Extraction and Mining.Roman Yangarber, Ralph Grishman, Pasi Tapanainen,and Silja Huttunen.
2000.
Automatic acquisi-tion of domain knowledge for information extraction.In Proceedings of the 18th International Conferenceon Computational Linguistics (COLING), pages 940?946.1807
