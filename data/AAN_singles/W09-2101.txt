Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 1?9,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsAutomatic Assessment of Spoken Modern Standard ArabicJian Cheng, Jared Bernstein, Ulrike Pado, Masanori SuzukiPearson Knowledge Technologies299 California Ave, Palo Alto, CA 94306jian.cheng@pearson.comAbstractProficiency testing is an important ingredientin successful language teaching.
However, re-peated testing for course placement, over thecourse of instruction or for certification can betime-consuming and costly.
We present thedesign and validation of the Versant ArabicTest, a fully automated test of spoken ModernStandard Arabic, that evaluates test-takers' fa-cility in listening and speaking.
Experimentaldata shows the test to be highly reliable (test-retest r=0.97) and to strongly predict perform-ance on the ILR OPI (r=0.87), a standard in-terview test that assesses oral proficiency.1 IntroductionTraditional high-stakes testing of spoken profi-ciency often evaluates the test-taker's ability to ac-complish communicative tasks in a conversationalsetting.
For example, learners may introduce them-selves, respond to requests for information, or ac-complish daily tasks in a role-play.Testing oral proficiency in this way can betime-consuming and costly, since at least onetrained interviewer is needed for each student.
Forexample, the standard oral proficiency test used bythe United States government agencies (the Inter-agency Language Roundtable Oral ProficiencyInterview or ILR OPI) is usually administered bytwo certified interviewers for approximately 30-45minutes per candidate.The great effort involved in oral proficiency in-terview (OPI) testing makes automated testing anattractive alternative.
Work has been reported onfully automated scoring of speaking ability (e.g.,Bernstein & Barbier, 2001; Zechner et al, 2007,for English; Balogh & Bernstein, 2007, for Englishand Spanish).
Automated testing systems do notaim to simulate a conversation with the test-takerand therefore do not directly observe interactivehuman communication.
Bernstein and Barbier(2001) describe a system that might be used inqualifying simultaneous interpreters; Zechner et al(2007) describe an automated scoring system thatassesses performance according to the TOEFL iBTspeaking rubrics.
Balogh and Bernstein (2007) fo-cus on evaluating facility in a spoken language, aseparate test construct that relates to oral profi-ciency.
?Facility in a spoken language?
is defined as?the ability to understand a spoken language oneveryday topics and to respond appropriately andintelligibly at a native-like conversational pace?
(Balogh & Bernstein, 2007, p. 272).
This ability isassumed to underlie high performance in commu-nicative settings, since learners have to understandtheir interlocutors correctly and efficiently in realtime to be able to respond.
Equally, learners haveto be able to formulate and articulate a comprehen-sible answer without undue delay.
Testing for oralproficiency, on the other hand, conventionally in-cludes additional aspects such as correct interpreta-tion of the pragmatics of the conversation, sociallyand culturally appropriate wording and content andknowledge of the subject matter under discussion.In this paper, we describe the design and valida-tion of the Versant Arabic Test (VAT), a fullyautomated test of facility with spoken ModernStandard Arabic (MSA).
Focusing on facilityrather than communication-based oral proficiencyenables the creation of an efficient yet informativeautomated test of listening and speaking ability.The automated test can be administered over thetelephone or on a computer in approximately 17minutes.
Despite its much shorter format and con-strained tasks, test-taker scores on the VAT1strongly correspond to their scores from an ILROral Proficiency Interview.The paper is structured as follows: After re-viewing related work, we describe Modern Stan-dard Arabic and introduce the test construct (i.e.,what the test is intended to measure) in detail (Sec-tion 3).
We then describe the structure and devel-opment of the VAT in Section 4 and presentevidence for its reliability and validity in Section 5.2 Related WorkThe use of automatic speech recognition appearedearliest in pronunciation tutoring systems in thefield of language learning.
Examples include SRI'sAUTOGRADER (Bernstein et al, 1990), the CMUFLUENCY system (Eskenazi, 1996; Eskenazi &Hansma, 1998) and SRI's commercial EduSpeaksystem (Franco et al, 2000).
In such systems,learner speech is typically evaluated by comparingfeatures like phone duration, spectral characteris-tics of phones and rate-of-speech to a model ofnative speaker performances.
Systems evaluatelearners?
pronunciation and give some feedback.Automated measurement of more comprehen-sive speaking and listening ability was first re-ported by Townshend et al (1998), describing theearly PhonePass test development at Ordinate.
ThePhonePass tests returned five diagnostic scores,including reading fluency, repeat fluency and lis-tening vocabulary.
Ordinate?s Spoken Spanish Testalso included automatically scored passage re-tellings that used an adapted form of latent seman-tic analysis to estimate vocabulary scores.More recently at ETS, Zechner et al (2007) de-scribe experiments in automatic scoring of test-taker responses in a TOEFL iBT practice environ-ment, focusing mostly on fluency features.
Zechnerand Xi (2008) report work on similar algorithms toscore item types with varying degrees of responsepredictability, including items with a very re-stricted range of possible answers (e.g., readingaloud) as well as item types with progressively lessrestricted answers (e.g., describing a picture ?
rela-tively predictable, or stating an opinion ?
less pre-dictable).
The scoring mechanism in Zechner andXi (2008) employs features such as the averagenumber of word types or silences for fluency esti-mation, the ASR HMM log-likelihood for pronun-ciation or a vector-based similarity measure toassess vocabulary and content.
Zechner and Xipresent correlations of machine scores with humanscores for two tasks: r=0.50 for an opinion task andr=0.69 for picture description, which are compara-ble to the modest human rater agreement figures inthis data.Balogh and Bernstein (2007) describe opera-tional automated tests of spoken Spanish and Eng-lish that return an overall ability score and fourdiagnostic subscores (sentence mastery, vocabu-lary, fluency, pronunciation).
The tests measure alearner's facility in listening to and speaking a for-eign language.
The facility construct can be testedby observing performance on many kinds of tasksthat elicit responses in real time with varying, butgenerally high, predictability.
More predictableitems have two important advantages: As with do-main restricted speech recognition tasks in general,the recognition of response content is more accu-rate, but a higher precision scoring system is alsopossible as an independent effect beyond thegreater recognition accuracy.
Scoring is based onfeatures like word stress, segmental form, latencyor rate of speaking for the fluency and pronuncia-tion subscores, and on response fidelity with ex-pected responses for the two content subscores.Balogh and Bernstein report that their tests arehighly reliable (r>0.95 for both English and Span-ish) and that test scores strongly predict humanratings of oral proficiency based on CommonEuropean Framework of Reference language abil-ity descriptors (r=0.88 English, r=0.90 Spanish).3 Versant Arabic Test: Facility in Mod-ern Standard ArabicWe describe a fully operational test of spokenMSA that follows the tests described in Balogh andBernstein (2007) in structure and method, and inusing the facility construct.
There are two impor-tant dimensions to the test's construct: One is thedefinition of what comprises MSA, and the otherthe definition of facility.3.1 Target Language: Modern StandardArabicModern Standard Arabic is a non-colloquial lan-guage used throughout the Arabic-speaking worldfor writing and in spoken communication withinpublic, literary, and educational settings.
It differsfrom the colloquial dialects of Arabic that are spo-ken in the countries of North Africa and the Mid-2dle East in lexicon and in syntax, for example inthe use of explicit case and mood marking.Written MSA can be identified by its specificsyntactic style and lexical forms.
However, sinceall short vowels are omitted in normal printed ma-terial, the word-final short vowels indicating caseand mood are provided by the speaker, even whenreading MSA aloud.
This means that a text that issyntactically and lexically MSA can be read in away that exhibits features of the regional dialect ofthe speaker if case and mood vowels are omitted orphonemes are realized in regional pronunciations.Also, a speaker's dialectal and educational back-ground may influence the choice of lexical itemsand syntactic structures in spontaneous speech.The MSA spoken on radio and television in theArab world therefore shows a significant variationof syntax, phonology, and lexicon.3.2 FacilityWe define facility in spoken MSA as the ability tounderstand and speak contemporary MSA as it isused in international communication for broadcast,for commerce, and for professional collaboration.Listening and speaking skills are assessed by ob-serving test-taker performance on spoken tasks thatdemand understanding a spoken prompt, and for-mulating and articulating a response in real time.Success on the real-time language tasks de-pends on whether the test-taker can process spokenmaterial efficiently.
Automaticity is an importantunderlying factor in such efficient language proc-essing (Cutler, 2003).
Automaticity is the ability toaccess and retrieve lexical items, to build phrasesand clause structures, and to articulate responseswithout conscious attention to the linguistic code(Cutler, 2003; Jescheniak et al, 2003; Levelt,2001).
If processing is automatic, the lis-tener/speaker can focus on the communicative con-tent rather than on how the language code isstructured.
Latency and pace of the spoken re-sponse can be seen as partial manifestation of thetest-taker?s automaticity.Unlike the oral proficiency construct that coor-dinates with the structure and scoring of OPI tests,the facility construct does not extend to socialskills, higher cognitive functions (e.g., persuasion),or world knowledge.
However, we show belowthat test scores for language facility predict almostall of the reliable variance in test scores for an in-terview-based test of language and communication.4 Versant Arabic TestThe VAT consists of five tasks with a total of 69items.
Four diagnostic subscores as well as anoverall score are returned.
Test administration andscoring is fully automated and utilizes speechprocessing technology to estimate features of thespeech signal and extract response content.4.1 Test DesignThe VAT items were designed to represent coresyntactic constructions of MSA and probe a widerange of ability levels.
To make sure that the VATitems used realistic language structures, texts wereadapted from spontaneous spoken utterances foundin international televised broadcasts with the vo-cabulary altered to contain common words that alearner of Arabic may have encountered.Four educated native Arabic speakers wrote theitems and five dialectically distinct native Arabicspeakers (Arabic linguist/teachers) independentlyreviewed the items for correctness and appropri-ateness of content.
Finally, fifteen educated nativeArabic speakers (eight men and seven women)from seven different countries recorded the vetteditems at a conversational pace, providing a rangeof native accents and MSA speaking styles in theitem prompts.4.2 Test Tasks and StructureThe VAT has five task types that are arranged insix sections (Parts A through F): Readings, Repeats(presented in two sections), Short Answer Ques-tions, Sentence Builds, and Passage Retellings.These item types provide multiple, fully independ-ent measures that underlie facility with spokenMSA, including phonological fluency, sentenceconstruction and comprehension, passive and ac-tive vocabulary use, and pronunciation of rhythmicand segmental units.Part A: Reading (6 items) In this task, test-takers read six (out of eight) printed sentences, oneat a time, in the order requested by the examinervoice.
Reading items are printed in Arabic scriptwith short vowels indicated as they would be in abasal school reader.
Test-takers have the opportu-nity to familiarize themselves with the readingitems before the test begins.
The sentences arerelatively simple in structure and vocabulary, sothey can be read easily and fluently by people edu-3cated in MSA.
For test-takers with little facility inspoken Arabic but with some reading skills, thistask provides samples of pronunciation and oralrearlyautthedemding fluency.Parts B and E: Repeats (2x15 items) Test-takers hear sentences and are asked to repeat themverbatim.
The sentences were recorded by nativespeakers of Arabic at a conversational pace.
Sen-tences range in length from three words to at mosttwelve words, although few items are longer thannine words.
To repeat a sentence longer than aboutseven syllables, the test-taker has to recognize thewords as produced in a continuous stream ofspeech (Miller & Isard, 1963).
Generally, the abil-ity to repeat material is constrained by the size ofthe linguistic unit that a person can process in anautomatic or nearly automatic fashion.
The abilityto repeat longer and longer items indicates moreand more advanced language skills ?
particulaomaticity with phrase and clause structures.Part C: Short Answer Questions (20 items)Test-takers listen to spoken questions in MSA andanswer each question with a single word or shortphrase.
Each question asks for basic information orrequires simple inferences based on time, se-quence, number, lexical content, or logic.
Thequestions are designed not to presume any special-ist knowledge of specific facts of Arabic culture orother subject matter.
An English example1 of aShort Answer Question would be ?Do you get milkfrom a bottle or a newspaper??
To answer thequestions, the test-taker needs to identify the wordsin phonological and syntactic context, inferand proposition and formulate the answer.Part D: Sentence Building (10 items) Test-takers are presented with three short phrases.
Thephrases are presented in a random order (excludingthe original, naturally occurring phrase order), andthe test-taker is asked to respond with a reasonablesentence that comprises exactly the three givenphrases.
An English example would be a prompt of?was reading - my mother - her favorite maga-zine?, with the correct response: ?My mother wasreading her favorite magazine.?
In this task, thetest-taker has to understand the possible meaningsof each phrase and know how the phrases might becombined with the other phrasal material, bothwith regard to syntax and semantics.
The lengthand complexity of the sentence that can be built is(e.g., a syllable, a wordorly,scored in this test.e withinmpleted.of facility with spoken MSA.
The sub-sc s?phrases and clauses in?ontext and?tructing, reading and re-?in a native-like manner1 See Pearson (2009) for Arabic example items.constrained by the size of the linguistic units withwhich the test-taker represents the prompt phrasesin verbal working memorya multi-word phrase).Part F: Passage Retelling (3 items) In this fi-nal task, test-takers listen to a spoken passage(usually a story) and then are asked to retell thepassage in their own words.
Test-takers are en-couraged to retell as much of the passage as theycan, including the situation, characters, actions andending.
The passages are from 19 to 50 wordslong.
Passage Retellings require listening compre-hension of extended speech and also provide addi-tional samples of spontaneous speech.
Currentthis task is not automatically4.3 Test AdministrationAdministration of the test takes about 17 minutesand the test can be taken over the phone or via acomputer.
A single examiner voice presents all thespoken instructions in either English or Arabic andall the spoken instructions are also printed verba-tim on a test paper or displayed on the computerscreen.
Test items are presented in Arabic by na-tive speaker voices that are distinct from the exam-iner voice.
Each test administration contains 69items selected by a stratified random draw from alarge item pool.
Scores are available onlina few minutes after the test is co4.4 Scoring DimensionsThe VAT provides four diagnostic subscores thatindicate the test-taker's ability profile over variousdimensionsore  areSentence Mastery: Understanding, recalling,and producing MSAcomplete sentences.Vocabulary: Understanding common wordsspoken in continuous sentence cproducing such words as needed.Fluency: Appropriate rhythm, phrasing andtiming when conspeating sentences.Pronunciation: Producing consonants, vow-els, and lexical stressin sentence context.4The VAT also reports an Overall score, whichis a weighted average of the four subscores (Sen-, Vocabulary 20%,tion 20%).m was trainedonentvalonse networks for eachitewoanswers with ob-serthe can-didlinearmoSentenceBuilding items and Vocabulary is based on re-n rt Answer Questions.inconsistent measure-metence Mastery contributes 30%Fluency 30%, and Pronuncia4.5 Automated ScoringThe VAT?s automated scoring systenative and non-native responses to the test itemsas well as human ability judgments.Data Collection For the development of theVAT, a total of 246 hours of speech in response tothe test items was collected from natives and learn-ers and was transcribed by educated native speak-ers of Arabic.
Subsets of the response data werealso rated for proficiency.
Three trained nativespeakers produced about 7,500 judgments for eachof the Fluency and the Pronunciation subscores (ona scale from 1-6, with 0 indicating missing data).The raters agreed well with one another at r?0.8(r=0.79 for Pronunciation, r=0.83 for Fluency).
Alltest administrations included in the concurridation study (cf.
Section 5 below) were ex-cluded from the training of the scoring system.Automatic Speech Recognition Recognition isperformed by an HMM-based recognizer built us-ing the HTK toolkit (Young et al, 2000).
Three-state triphone acoustic models were trained on 130hours of non-native and 116 hours of native MSAspeech.
The expected respm were induced from the transcriptions of nativeand non-native responses.Since standard written Arabic does not markshort vowels, the pronunciation and meaning ofwritten words is often ambiguous and words do notshow case and mood markings.
This is a challengeto Arabic ASR, since it complicates the creation ofpronunciation dictionaries that link a word's soundto its written form.
Words were represented withtheir fully voweled pronunciation (cf., Vergyri etal., 2008; Soltau et al, 2007).
We relied on hand-corrected automatic diacritization of the standardwritten transcriptions to create fully-voweledrds from which phonemic representations wereautomatically created.The orthographic transcript of a test-taker utter-ance in standard, unvoweled form is still ambigu-ous with regard to the actual words uttered, sincethe same consonant string can have different mean-ings depending on the vowels that are inserted.Moreover, the different words written in this wayare usually semantically related, making them po-tentially confusable for language learners.
There-fore, for system development, we transcribedwords with full vowel marks whenever a vowelchange would cause a change of meaning.
Thispartial voweling procedure deviates from the stan-dard way of writing, but it facilitated system-internal comparison of targetved test-taker utterances since the target pro-nunciation was made explicit.Scoring Methods The Sentence Mastery andVocabulary scores are derived from the accuracyof the test-taker's response (in terms of number ofwords inserted, deleted, or substituted byate), and the presence or absence of expectedwords in correct sequences, respectively.The Fluency and Pronunciation subscores arecalculated by measuring the latency of the re-sponse, the rate of speaking, the position andlength of pauses, the stress and segmental forms ofthe words, and the pronunciation of the segmentsin the words within their lexical and phrasal con-text.
The final subscores are based on a non-linearcombination of these features.
The non-del is trained on feature values and humanjudgments for native and non-native speech.Figure 1 shows how each subscore draws on re-sponses from the different task types to yield a sta-ble estimate of test-taker ability.
The Pronunciationscore is estimated from responses to Reading, Re-peat and Sentence Build items.
The Fluency scoreuses the same set of responses as for Pronuncia-tion, but a different set of acoustic features are ex-tracted and combined in the score.
SentenceMastery is derived from Repeat andspo ses to the Sho5 EvaluationFor any test to be meaningful, two properties arecrucial: Reliability and validity.
Reliability repre-sents how consistent and replicable the test scoresare.
Validity represents the extent to which one canjustify making certain inferences or decisions onthe basis of test scores.
Reliability is a necessarycondition for validity, sincents cannot support inferences that would justifyreal-world decision making.To investigate the reliability and the validity ofthe VAT, a concurrent validation study was con-ducted in which a group of test-takers took both5the VAT and the ILR OPI.
If the VAT scores areparable t  traditionalhis will be ative functioning in the target language.Thning Arabicco .S., and at least 11 were gradu-ter for Arabic Studies Abroadbetween one rater andthee taker took the VAT twice, we cancom o scores from a reliablemeasure of oral proficiency in MSA, tpiece of evidence that the VAT indeed capturesimportant aspects of test-takers' abilities in usingspoken MSA.As additional evidence to establish the validityof the VAT, we examined the performance of thenative and non-native speaker groups.
Since thetest claims to measure facility in understanding andspeaking MSA, most educated native speakersshould do quite well on the test, whereas the scoresof the non-native test-takers should spread out ac-cording to their ability level.
Furthermore, onewould also expect that educated native speakerswould perform equally well regardless of specificnational dialect backgrounds and no importantscore differences among different national groupsof educated native speakers should be observed.5.1 Concurrent Validation StudyILR OPIs.
The ILR Oral Proficiency Interview isa well-established test of spoken language per-formance, and serves as the standard evaluationtool used by United States government agencies(see www.govtilr.org).
The test is a structured in-terview that elicits spoken performances that aregraded according to the ILR skill levels.
Theselevels describe the test-taker?s ability in terms ofcommunicae OPI test construct is therefore different fromthat of the VAT, which measures facility with spo-ken Arabic, and not communicative ability, assuch.Concurrent Sample.
A total of 118 test-takers(112 non-natives and six Arabic natives) took twoVATs and two ILR OPIs.
Each test-taker com-pleted all four tests within a 15 day window.
Themean age of the test-takers was 27 years old (SD =7) and the male-to-female split was 60-to-58.
Ofthe non-native speakers in this concurrent testingsample, at least 20 test-takers were learat a llege in the Uates from the Cenprogram.
Nine test-takers were recruited at a lan-guage school in Cairo, Egypt, and the remainderwere current or former students of Arabic recruitedin the US.Seven active government-certified oral profi-ciency interviewers conducted the ILR OPIs overthe telephone.
Each OPI was administered by twointerviewers who submitted the performance rat-ings independently after each interview.
The aver-age inter-rater correlationaverage score given by the other two ratersadministering the same test-taker's other interviewwas 0.90.The test scores used in the concurrent study arethe VAT Overall score, reported here in a rangefrom 10 to 90, and the ILR OPI scores with levels{0, 0+, 1, 1+, 2, 2+, 3, 3+, 4, 4+, 5}2.5.2 ReliabilitySinc each test-estimate the VAT?s reliability using the test-retestmethod (e.g., Crocker & Algina, 1986: 133).
The2 All plus ratings (e.g., 1+, 2+, etc) were converted with 0.5(e.g, 1.5, 2.5, etc) in the analysis reported in this paper.Figure 1: Relation of subscores to item types.6correlation between the scores from the first ad-ministration and the scores from the second ad-miare reliable at r=0.91(thased test of oral proficiency inM .the V  MSAsp khwith ILR OPI scores, despite the difference in con-dict native performance)score distributions of test-takernistration was found to be at r=0.97, indicatinghigh reliability of the VAT test.
The scores fromone test administration explain 0.972=94% of thescore variance in another test administration to thesame group of test-takers.We also compute the reliability of the ILR OPIscores for each test taker by correlating the aver-ages of the ratings for each of the two test admini-strations.
The OPI scoresus 83% of the variance in the test scores areshared by the scores of another administration).This indicates that the OPI procedure implementedin the validation study was relatively consistent.5.3 ValidityEvidence here for VAT score validity comes fromtwo sources: the prediction of ILR OPI scores (as-sumed for now to be valid) and the performancedistribution of native and non-native test takers.Prediction of ILR OPI Test Scores.
For thecomparison of the VAT to the ILR OPI, a scaledaverage OPI score was computed for each test-taker from all the available ILR OPI ratings.
Thescaling was performed using a computer program,FACETS, which takes into account rater severityand test-taker ability and therefore produces afairer estimate than a simple average (Linacre etal., 1990; Linacre, 2003).Figure 2 is a scatterplot of the ILR OPI scoresand VAT scores for the concurrent validation sam-ple (N=118).
IRT scaling of the ILR scores allowsa mapping of the scaled OPI scores and the VATscores onto the original OPI levels, which aregiven on the inside of the plot axes.
The correlationcoefficient of the two test scores is r=0.87.
This isroughly in the same range as both the ILR OPI re-liability and the average ILR OPI inter-rater corre-lation.
The test scores on the VAT account for 76%of the variation in the ILR OPI scores (in contrastto 83% accounted for by another ILR OPI test ad-ministration and 81% accounted for by one otherILR OPI interviewer).The VAT accounts for most of the variance inthe interview-bSA  This is one form of confirming evidence thatAT captures important aspects ofea ing and listening ability.T e close correspondence of the VAT scoresstruct, may come about because candidates easilytransfer basic social and communicative skills ac-quired in their native language, as long as they areable to correctly and efficiently process (i.e., com-prehend and produce) the second language.
Also,highly proficient learners have most likely ac-quired their skills at least to some extent in socialinteraction with native speakers of their secondlanguage and therefore know how to interact ap-propriately.Group Performance.
Finally, we examine thescore distributions for different groups of test-takers to investigate whether three basic expecta-tions are met:?
Native speakers all perform well, while non-natives show a range of ability levels?
Non-native speakers spread widely acrossthe scoring scale (the test can distinguishwell between a range of non-native abilitylevels)?
Native speakers from different countries per-form similarly (national origin does not pre-We compare thegroups in the training data set, which contains1309 native and 1337 non-native tests.
For eachtest in the data set, an Overall score is computed bythe trained scoring system on the basis of the re-corded responses.
Figure 3 presents cumulativedistribution functions of the VAT overall scores,showing for each score which percentage of test-takers performs at or below that level.
This figurecompares two speaker groups: Educated nativespeakers of Arabic and learners of Arabic.
TheFigure 2: Test-takers' ILR OPI scores as a functionof VAT scores (r=0.87; N=118).7score distributions of the native speakers and thelearner sample are clearly different.
For example,fewer than 5% of the native speakers score below70, while fewer than 10% of the learners scoreabove 70.
Further, the shape of the learner curvebution of scores, suggestingthat the VAT discriminates well in the range of.
The Mo-ccan speakers are slightly separate from the othernative speakers, but only a negligible number ofthem scores lower than 70, a score that less than10% of learners achieve.
This finding supports thenotion that the VAT scores reflect a speaker's facil-ity in spoken MSA, irrespective of the speaker'scountry of origin.6 ConclusionWe have presented an automatically scored test offacility with spoken Modern Standard Arabic(MSA).
The test yields an ability profile over foursubscores, Fluency and Pronunciation (manner-of-speaking) as well as Sentence Mastery and Vo-cabulary (content), and generates a single Overallscore as the weighted average of the subscores.
Wehave presented data from a validation study withnative and non-native test-takers that shows theVAT to be highly reliable (test-retest r=0.97).
Wealso have presented validity evidence for justifyingthe use of VAT scores as a measure of oral profi-ciency in MSA.
While educated native speakers ofArabic can score high on the test regardless of theircountry of origin because they all possess high fa-cility in spoken MSA, learners of Arabic score dif-ferently according to their ability levels; the VATtest scores account for most of the variance in theinterview-based ILR OPI for MSA, indicating thatthe VAT captures a major feature of oral profi-ciency.In summary, the empirical validation data sug-gests that the VAT can be an efficient, practicalalternative to interview-based proficiency testingin many settings, and that VAT scores can be usedto inform decisions in which a person?s listeningand speaking ability in Modern Standard Arabicshould play a part.AcknowledgmentsThe reported work was conducted under contractW912SU-06-P-0041 from the U.S. Dept.
of theArmy.
The authors thank Andy Freeman for pro-viding diacritic markings, and to Waheed Samy,Naima Bousofara Omar, Eli Andrews, MohamedAl-Saffar, Nazir Kikhia, Rula Kikhia, and LindaIstanbulli for support with item development anddata collection/transcription in Arabic.Figure 4: Score distributions for native speakersof different countries of origin.indicates a wide distriabilities of learners of Arabic as a foreign lan-guage.Figure 4 is also a cumulative distribution func-tions, but it shows score distributions for nativespeakers by country of origin (showing only coun-tries with at least 40 test-takers).
The curves forEgyptian, Syrian, Iraqi, Palestinian, Saudi andYemeni speakers are indistinguishableroFigure 3: Score distributions for native and non-native speakers.8ReferencesJennifer Balogh and Jared Bernstein.
2007.
Workablemodels of standard performance in English andSpanish.
In Y. Matsumoto, D. Oshima, O. Robinson,and P. Sells, editors, Diversity in Language: Per-spectives and Implications (CSLI Lecture Notes,176), 271-292.
CSLI, Stanford, CA.Jared Bernstein and Isabella Barbier.
2001.
Design anddevelopment parameters for a rapid automaticscreening test for prospective simultaneous inter-preters.
Interpreting, International Journal of Re-search and Practice in Interpreting, 5(2): 221-238.Jared Bernstein, Michael Cohen, Hy Murveit, DmitryRtischev, and Mitch Weintraub.
1990.
Automaticevaluation and training in English pronunciation.
InProceedings of ICSLP, 1185-1188.Linda Crocker and James Algina.
1986.
Introduction toClassical & Modern Test Theory.
Harcourt BraceJovanovich, Orland, FL.Anne Cutler.
2003.
Lexical access.
In L. Nadel, editor,Encyclopedia of Cognitive Science, volume 2, pp.858-864.
Nature Publishing Group.Maxine Eskenazi.
1996.
Detection of foreign speakers?pronunciation errors for second language training ?preliminary results.
In Proceedings of ICSLP ?96.Maxine Eskenazi and Scott Hansma.
1998.
The fluencypronunciation trainer.
In Proceedings of the STiLLWorkshop.Horacio Franco, Victor Abrash, Kristin Precoda, HarryBratt, Raman Rao, John Butzberger, Romain Ross-ier, and Federico Cesar.
2000.
The SRI EduSpeaksystem: Recognition and pronunciation scoring forlanguage learning.
In Proceedings of InSTiLL, 123-128.J?rg Jescheniak, Anja Hahne, and Herbert Schriefers.2003.
Information flow in the mental lexicon duringspeech planning: Evidence from event-related poten-tials.
Cognitive Brain Research, 15(3):858-864.Willem Levelt.
2001.
Spoken word production: A the-ory of lexical access.
Proceedings of the NationalAcademy of Sciences, 98(23):13464-13471.John Linacre.
2003.
FACETS Rasch measurement com-puter program.
Winstep, Chicago, IL.John Linacre, Benjamin Wright, and Mary Lunz.
1990.A Facets model for judgmental scoring.
Memo 61.MESA Psychometric Laboratory.
University of Chi-cago.
Retrieved April 14, 2009, from http://http://www.rasch.org/memo61.htm.George Miller and Stephen Isard.
1963.
Some percep-tual consequences of linguistic rules.
Journal ofVerbal Learning and Verbal Behavior, 2:217-228.Pearson.
2009.
Versant Arabic test ?
test descriptionand validation summary.
Pearson.
Retrieved April14, 2009, fromhttp://www.ordinate.com/technology/VersantArabicTestValidation.pdf.Hagen Soltau, George Saon, Daniel Povy, Lidia Mangu,Brian Kingsbury, Jeff Kuo, Mohamed Omar, andGeoffrey Zweig.
2007.
The IBM 2006 GALE ArabicASR system.
In Proceedings of ICASSP 2007, 349-352.Brent Townshend, Jared Bernstein, Ognjen Todic &Eryk Warren.
1998.
Estimation of Spoken LanguageProficiency.
In STiLL: Speech Technology in Lan-guage Learning, 177-180.Dimitra Vergyri, Arindam Mandal, Wen Wang, An-dreas Stolcke, Jing Zheng, Martin Graciarena, DavidRybach, Christian Gollan, Ralf Schl?ter, KarinKirchhoff, Arlo Faria, and Nelson Morgan.
2008.Development of the SRI/Nightingale Arabic ASRsystem.
In Proceedings of Interspeech 2008, 1437-1440.Steve Young, Dan Kershaw, Julian Odell, Dave Ol-lason, Valtcho Valtchev, and Phil Woodland.
2000.The HTK Book Version 3.0.
Cambridge UniversityPress, Cambridge, UK.Klaus Zechner and Xiaoming Xi.
2008.
Towards auto-matic scoring of a test of spoken language with het-erogeneous task types.
In Proceedings of the ThirdWorkshop on Innovative Use of NLP for BuildingEducational Applications, 98-106.Klaus Zechner, Derrick Higgins, and Xiaoming Xi.2007.
SpeechRater?
: A construct-driven approachto score spontaneous non-native speech.
In Proceed-ings of the Workshop of the ISCA SIG on Speech andLanguage Technology in Education.9
