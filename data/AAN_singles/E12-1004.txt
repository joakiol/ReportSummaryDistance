Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 23?32,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsEntailment above the word level in distributional semanticsMarco BaroniRaffaella BernardiUniversity of Trentoname.surname@unitn.itNgoc-Quynh DoFree University of Bozen-Bolzanoquynhdtn.hut@gmail.comChung-chieh ShanCornell UniversityUniversity of Tsukubaccshan@post.harvard.eduAbstractWe introduce two ways to detect entail-ment using distributional semantic repre-sentations of phrases.
Our first experimentshows that the entailment relation betweenadjective-noun constructions and their headnouns (big cat |= cat), once represented assemantic vector pairs, generalizes to lexicalentailment among nouns (dog |= animal).Our second experiment shows that a classi-fier fed semantic vector pairs can similarlygeneralize the entailment relation amongquantifier phrases (many dogs|=some dogs)to entailment involving unseen quantifiers(all cats|=several cats).
Moreover, nominaland quantifier phrase entailment appears tobe cued by different distributional corre-lates, as predicted by the type-based viewof entailment in formal semantics.1 IntroductionDistributional semantics (DS) approximates lin-guistic meaning with vectors summarizing thecontexts where expressions occur.
The successof DS in lexical semantics has validated the hy-pothesis that semantically similar expressions oc-cur in similar contexts (Landauer and Dumais,1997; Lund and Burgess, 1996; Sahlgren, 2006;Schu?tze, 1997; Turney and Pantel, 2010).
For-mal semantics (FS) represents linguistic mean-ings as symbolic formulas and assemble them viacomposition rules.
FS has successfully modeledquantification and captured inferential relationsbetween phrases and between sentences (Mon-tague, 1970; Thomason, 1974; Heim and Kratzer,1998).
The strengths of DS and FS have beencomplementary to date: On one hand, DS has in-duced large-scale semantic representations fromcorpora, but it has been largely limited to thelexical domain.
On the other hand, FS has pro-vided sophisticated models of sentence meaning,but it has been largely limited to hand-coded mod-els that do not scale up to real-life challenges bylearning from data.Given these complementary strengths, we nat-urally ask if DS and FS can address each other?slimitations.
Two recent strands of research arebringing DS closer to meeting core FS chal-lenges.
One strand attempts to model compo-sitionality with DS methods, representing bothprimitive and composed linguistic expressionsas distributional vectors (Baroni and Zamparelli,2010; Grefenstette and Sadrzadeh, 2011; Gue-vara, 2010; Mitchell and Lapata, 2010).
Theother strand attempts to reformulate FS?s notionof logical inference in terms that DS can cap-ture (Erk, 2009; Geffet and Dagan, 2005; Kotler-man et al 2010; Zhitomirsky-Geffet and Dagan,2010).
In keeping with the lexical emphasis ofDS, this strand has focused on inference at theword level, or lexical entailment, that is, discover-ing from distributional vectors of hyponyms (dog)that they entail their hypernyms (animal).This paper brings these two strands of researchtogether by demonstrating two ways in which thedistributional vectors of composite expressionsbear on inference.
Here we focus on phrasal vec-tors harvested directly from the corpus rather thanobtained compositionally.
In a first experiment,we exploit the entailment properties of a classof composite expressions, namely adjective-nounconstructions (ANs), to harvest training data foran entailment recognizer.
The recognizer is thensuccessfully applied to detect lexical entailment.In short, since almost all ANs entail the noun theycontain (red car entails car), the distributionalvectors of AN-N pairs can train a classifier to de-tect noun pairs that stand in the same relation (dog23entails animal).
With almost no manual effort,we achieve performance nearly identical with thestate-of-the-art balAPinc measure that Kotlermanet al(2010) crafted, which detects feature inclu-sion between the two nouns?
occurrence contexts.Our second experiment goes beyond lexical in-ference.
We look at phrases built from a quanti-fying determiner1 and a noun (QNs) and use theirdistributional vectors to recognize entailment re-lations of the form many dogs |= some dogs, be-tween two QNs sharing the same noun.
It turnsout that a classifier trained on a set of Q1N |=Q2Npairs can recognize entailment in pairs with a newquantifier configuration.
For example, we cantrain on many dogs |= some dogs then correctlypredict all cats|=several cats.
Interestingly, on theQN entailment task, neither our classifier trainedon AN-N pairs nor the balAPinc method beatbaseline methods.
This suggests that our success-ful QN classifiers tap into vector properties be-yond such relations as feature inclusion that thosemethods for nominal entailment rely upon.Together, our experiments show that corpus-harvested DS representations of composite ex-pressions such as ANs and QNs contain suffi-cient information to capture and generalize theirinference patterns.
This result brings DS closerto the central concerns of FS.
In particular, theQN study is the first to our knowledge to showthat DS vectors capture semantic properties notonly of content words, but of an important class offunction words (quantifying determiners) deeplystudied in FS but of little interest until now in DS.Besides these theoretical implications, our re-sults are of practical import.
First, our AN studypresents a novel, practical method for detect-ing lexical entailment that reaches state-of-the-art performance with little or no manual interven-tion.
Lexical entailment is in turn fundamentalfor constructing ontologies and other lexical re-sources (Buitelaar and Cimiano, 2008).
Second,our QN study demonstrates that phrasal entail-ment can be automatically detected and thus pavesthe way to apply DS to advanced NLP tasks suchas recognizing textual entailment (Dagan et al2009).1In the sequel we will simply refer to a ?quantifying de-terminer?
as a ?quantifier?.2 Background2.1 Distributional semantics above the wordlevelDS models such as LSA (Landauer and Dumais,1997) and HAL (Lund and Burgess, 1996) ap-proximate the meaning of a word by a vector thatsummarizes its distribution in a corpus, for exam-ple by counting co-occurrences of the word withother words.
Since semantically similar wordstend to share similar contexts, DS has been verysuccessful in tasks that require quantifying se-mantic similarity among words, such as synonymdetection and concept clustering (Turney and Pan-tel, 2010).Recently, there has been a flurry of interestin DS to model meaning composition: How canwe derive the DS representation of a compositephrase from that of its constituents?
Although thegeneral focus in the area is to perform algebraicoperations on word semantic vectors (Mitchelland Lapata, 2010), some researchers have also di-rectly examined the corpus contexts of phrases.For example, Baldwin et al(2003) studied vec-tor extraction for phrases because they were inter-ested in the decomposability of multiword expres-sions.
Baroni and Zamparelli (2010) and Gue-vara (2010) look at corpus-harvested phrase vec-tors to learn composition functions that should de-rive such composite vectors automatically.
Ba-roni and Zamparelli, in particular, showed qual-itatively that directly corpus-harvested vectors forAN constructions are meaningful; for example,the vector of young husband has nearest neigh-bors small son, small daughter and mistress.
Fol-lowing up on this approach, we show here quanti-tatively that corpus-harvested AN vectors are alsouseful for detecting entailment.
We find moreoverdistributional vectors informative and useful notonly for phrases made of content words (such asANs) but also for phrases containing functionalelements, namely quantifying determiners.2.2 Entailment from formal to distributionalsemanticsEntailment in FS To characterize the condi-tions under which a sentence is true, FS beginswith the lexical meanings of the words in the sen-tence and builds up the meanings of larger andlarger phrases until it arrives at the meaning of thewhole sentence.
The meanings throughout this24compositional process inhabit a variety of seman-tic domains, depending on the syntactic categoryof the expressions: typically, a sentence denotes atruth value (true or false) or truth conditions,a noun such as cat denotes a set of entities, and aquantifier phrase (QP) such as all cats denotes aset of sets of entities.The entailment relation (|=) is a core notion oflogic: it holds between one or more sentences anda sentence such that it cannot be that the former(antecedent) are true and the latter (consequent)is false.
FS extends this notion from formal-logicsentences to natural-language expressions.
By as-signing meanings to parts of a sentence, FS allowsdefining entailment not only among sentences butalso among words and phrases.
Each semanticdomain A has its own entailment relation |=A.The entailment relation |=S among sentences isthe logical notion just described, whereas the en-tailment relations |=N and |=QP among nounsand quantifier phrases are the inclusion relationsamong sets of entities and sets of sets of entitiesrespectively.
Our results in Section 5 show thatDS needs to treat |=N and |=QP differently as well.Empirical, corpus-based perspectives on en-tailment Until recently, the corpus-based re-search tradition has studied entailment mostly atthe word level, with applied goals such as clas-sifying lexical relations and building taxonomicWordNet-like resources automatically.
The mostpopular approach, first adopted by Hearst (1992),extracts lexical relations from patterns in largecorpora.
For instance, from the pattern N1 suchas N2 one learns that N2 |=N1 (from insects suchas beetles, derive beetles |= insects).
Several stud-ies have refined and extended this approach (Pan-tel and Ravichandran, 2004; Snow et al 2005;Snow et al 2006; Turney, 2008).While empirically very successful, the pattern-based method is mostly limited to single contentwords (or frequent content-word phrases).
We areinterested in entailment between phrases, where itis not obvious how to use lexico-syntactic patternsand cope with data sparsity.
For instance, it seemshard to find a pattern that frequently connects oneQP to another it entails, as in all beetles PATTERNmany beetles.
Hence, we aim to find a more gen-eral method and investigate whether DS vectors(whether corpus-harvested or compositionally de-rived) encode the information needed to accountfor phrasal entailment in a way that can be cap-tured and generalized to unseen phrase pairs.Rather recently, the study of sentential entail-ment has taken an empirical turn, thanks to the de-velopment of benchmarks for entailment systems.The FS definition of entailment has been modifiedby taking common sense into account.
Instead ofa relation from the truth of the consequent to thetruth of the antecedent in any circumstance, theapplied view looks at entailment in terms of plau-sibility: ?
|= ?
if a human who reads (and trusts)?
would most likely infer that ?
is also true.
En-tailment systems have been compared under thisnew perspective in various evaluation campaigns,the best known being the Recognizing Textual En-tailment (RTE) initiative (Dagan et al 2009).Most RTE systems are based on advanced NLPcomponents, machine learning techniques, and/orsyntactic transformations (Zanzotto et al 2007;Kouleykov and Magnini, 2005).
A few systemsexploit deep FS analysis (Bos and Markert, 2006;Chambers et al 2007).
In particular, the FS re-sults about QP properties that affect entailmenthave been exploited by Chambers et alwho com-plement a core broad-coverage system with a Nat-ural Logic module to trade lower recall for higherprecision.
For instance, they exploit the mono-tonicity properties of no that cause the follow-ing reversal in entailment direction: some bee-tles |= some insects but no insects |= no beetles.To investigate entailment step by step, we ad-dress here a much simpler and clearer type ofentailment than the more complex notion takenup by the RTE community.
While RTE is out-side our present scope, we do focus on QP entail-ment as Natural Logic does.
However, our eval-uation differs from Chambers et als, since werely on general-purpose DS vectors as our onlyresource, and we look at phrase pairs with differ-ent quantifiers but the same noun.
For instance,we aim to predict that all beetles |= many beetlesbut few beetles 6|=all beetles.
QPs, of course, havemany well-known semantic properties besides en-tailment; we leave their analysis to future study.Entailment in DS Erk (2009) suggests that itmay not be possible to induce lexical entailmentdirectly from a vector space representation, but itis possible to encode the relation in this space af-ter it has been derived through other means.
Onthe other hand, recent studies (Geffet and Dagan,252005; Kotlerman et al 2010; Weeds et al 2004)have pursued the intuition that entailment is theasymmetric ability of one term to ?substitute?
foranother.
For example, baseball contexts are alsosport contexts but not vice versa, hence baseballis ?narrower?
than sport and baseball |=sport.
Onthis view, entailment between vectors correspondsto inclusion of contexts or features, and can becaptured by asymmetric measures of distributionsimilarity.
In particular, Kotlerman et al(2010)carefully crafted the balAPinc measure (see Sec-tion 3.5 below).
We adopt this measure becauseit has been shown to outperform others in severaltasks that require lexical entailment information.Like Kotlerman et al we want to capture theentailment relation between vectors of features.However, we are interested in entailment not onlybetween words but also between phrases, and weask whether the DS view of entailment as fea-ture inclusion, which captures entailment betweennouns, also captures entailment between QPs.
Tothis end, we complement balAPinc with a moreflexible supervised classifier.3 Data and methods3.1 Semantic spaceWe construct distributional semantic vectors fromthe 2.83-billion-token concatenation of the BritishNational Corpus (http://www.natcorp.ox.ac.uk/), WackyPedia and ukWaC (http://wacky.sslmit.unibo.it/).
We tok-enize and POS-tag this corpus, then lemmatizeit with TreeTagger (Schmid, 1995) to merge sin-gular and plural instances of words and phrases(some dogs is mapped to some dog).We process the corpus in two steps to computesemantic vectors representing our phrases of in-terest.
We use phrases of interest as a generalterm to refer to both multiword phrases and sin-gle words, and more precisely to: those AN andQN sequences that are in the data sets (see nextsubsections), the adjectives, quantifiers and nounscontained in those sequences, and the most fre-quent (9.8K) nouns and (8.1K) adjectives in thecorpus.
The first step is to count the contentwords (more precisely, the most frequent 9.8Knouns, 8.1K adjectives, and 9.6K verbs in the cor-pus) that occur in the same sentence as phrasesof interest.
In the second step, following standardpractice, the co-occurrence counts are convertedinto pointwise mutual information (PMI) scores(Church and Hanks, 1990).
The result of this stepis a sparse matrix (with both positive and negativeentries) with 48K rows (one per phrase of interest)and 27K columns (one per content word).3.2 The AN |= N data setTo characterize entailment between nouns usingtheir semantic vectors, we need data exemplifyingwhich noun entails which.
This section introducesone cheap way to collect such a training data setexploiting semantic vectors for composed expres-sions, namely AN sequences.
We rely on the lin-guistic fact that ANs share a syntactic categoryand semantic type with plain common nouns (bigcat shares syntactic category and semantic typewith cat).
Furthermore, most adjectives are re-strictive in the sense that, for every noun N, theAN sequence entails the N alone (every big catis a cat).
From a distributional point of view, thevector for an N should by construction include theinformation in the vector for an AN, given that thecontexts where the AN occurs are a subset of thecontexts where the N occurs (cat occurs in all thecontexts where big cat occurs).
This ideal inclu-sion suggests that the DS notion of lexical entail-ment as feature inclusion (see Section 2.2 above)should be reflected in the AN |= N pattern.Because most ANs entail their head Ns, we cancreate positive examples of AN |= N without anymanual inspection of the corpus: simply pair upthe semantic vectors of ANs and Ns.
Furthermore,because an AN usually does not entail another N,we can create negative examples (AN1 6|=N2) justby randomly permuting the Ns.
Of course, suchunsupervised data would be slightly noisy, espe-cially because some of the most frequent adjec-tives are not restrictive.To collect cleaner data and to be sure that weare really examining the phenomenon of entail-ment, we took a mere few moments of man-ual effort to select the 256 restrictive adjectivesfrom the most frequent 300 adjectives in the cor-pus.
We then took the Cartesian product of these256 adjectives with the 200 concrete nouns in theBLESS data set (Baroni and Lenci, 2011).
Thosenouns were chosen to avoid highly polysemouswords.
From the Cartesian product, we obtain atotal of 1246 AN sequences, such as big cat, thatoccur more than 100 times in the corpus.
TheseAN sequences encompass 190 of the 256 adjec-26tives and 128 of the 200 nouns.The process results in 1246 positive instancesof AN |= N entailment, which we use as trainingdata.
To create a comparable amount of negativedata, we randomly permuted the nouns in the pos-itive instances to obtain pairs of AN1 6|= N2 (e.g.,big cat 6|=dog).
We manually double-checked thatall positive and negative examples are correctlyclassified (2 of 1246 negative instances were re-moved, leaving 1244 negative training examples).3.3 The lexical entailment N1 |= N2 data setFor testing data, we first listed all WordNet nounsin our corpus, then extracted hyponym-hypernymchains linking the first synsets of these nouns.
Forexample, pope is found to entail leader becauseWordNet contains the chain pope ?
spiritualleader ?
leader.
Eliminating the 20 hypernymswith more than 180 hyponyms (mostly very ab-stract nouns such as entity, object, and quality)yields 9734 hyponym-hypernym pairs, encom-passing 6402 nouns.
Manually double-checkingthese pairs leaves us with 1385 positive instancesof N1 |= N2 entailment.We created the negative instances of again 1385pairs by inverting 33% of the positive instances(from pope|=leader to leader 6|=pope), and by ran-domly shuffling the words across the positive in-stances.
We also manually double-checked thesepairs to make sure that they are not hyponym-hypernym pairs.3.4 The Q1N |= Q2N data setWe study 12 quantifiers: all, both, each, either,every, few, many, most, much, no, several, some.We took the Cartesian product of these quantifierswith the 6402 WordNet nouns described in Sec-tion 3.3.
From this Cartesian product, we obtaina total of 28926 QN sequences, such as every cat,that occur at least 100 times in the corpus.
Theseare our QN phrases of interest to which the proce-dure in Section 3.1 assigns a semantic vector.Also, from the set of quantifier pairs (Q1,Q2)where Q1 6= Q2, we identified 13 clear caseswhere Q1 |=Q2 and 17 clear cases where Q1 6|=Q2.These 30 cases are listed in the first column ofTable 1.
For each of these 30 quantifier pairs(Q1,Q2), we enumerate those WordNet nouns Nsuch that semantic vectors are available for bothQ1N and Q2N (that is, both sequences occur inat least 100 times).
Each such noun then givesQuantifier pair Instances Correctall |= some 1054 1044 (99%)all |= several 557 550 (99%)each |= some 656 647 (99%)all |= many 873 772 (88%)much |= some 248 217 (88%)every |= many 460 400 (87%)many |= some 951 822 (86%)all |= most 465 393 (85%)several |= some 580 439 (76%)both |= some 573 322 (56%)many |= several 594 113 (19%)most |= many 463 84 (18%)both |= either 63 1 (2%)Subtotal 7537 5804 (77%)some 6|= every 484 481 (99%)several 6|= all 557 553 (99%)several 6|= every 378 375 (99%)some 6|= all 1054 1043 (99%)many 6|= every 460 452 (98%)some 6|= each 656 640 (98%)few 6|= all 157 153 (97%)many 6|= all 873 843 (97%)both 6|= most 369 347 (94%)several 6|= few 143 134 (94%)both 6|= many 541 397 (73%)many 6|= most 463 300 (65%)either 6|= both 63 39 (62%)many 6|= no 714 369 (52%)some 6|= many 951 468 (49%)few 6|= many 161 33 (20%)both 6|= several 431 63 (15%)Subtotal 8455 6690 (79%)Total 15992 12494 (78%)Table 1: Entailing and non-entailing quantifier pairswith number of instances per pair (Section 3.4) andSVMpair-out performance breakdown (Section 5).rise to an instance of entailment (Q1N |= Q2N ifQ1 |=Q2; example: many dogs |= several dogs) ornon-entailment (Q1N 6|=Q2N if Q1 6|=Q2; example:many dogs 6|=most dogs).
The number of QN pairsthat each quantifier pair gives rise to in this way islisted in the second column of Table 1.
As shownthere, we have a total of 7537 positive instancesand 8455 negative instances of QN entailment.3.5 Classification methodsWe consider two methods to classify candidatepairs as entailing or non-entailing, the balAPincmeasure of Kotlerman et al(2010) and a standardSupport Vector Machine (SVM) classifier.27balAPinc As discussed in Section 2.2, balAP-inc is optimized to capture a relation of featureinclusion between the narrower (entailing) andbroader (entailed) terms, while capturing other in-tuitions about the relative relevance of features.balAPinc averages two terms, APinc and LIN.APinc is given by:APinc(u |= v) =?|Fu|r=1(P (r) ?
rel?
(fr))|Fu|APinc is a version of the Average Precisionmeasure from Information Retrieval tailored tolexical inclusion.
Given vectors Fu and Fv rep-resenting the dimensions with positive PMI val-ues in the semantic vectors of the candidate pairu |= v, the idea is that we want the features (thatis, vector dimensions) that have larger values inFu to also have large values in Fv (the oppositedoes not matter because it is u that should be in-cluded in v, not vice versa).
The Fu features areranked according to their PMI value so that fris the feature in Fu with rank r, i.e., r-th high-est PMI.
Then the sum of the product of the twoterms P (r) and rel?
(fr) across the features in Fuis computed.
The first term is the precision at r,which is higher when highly ranked u features arepresent in Fv as well.
The relevance term rel?
(fr)is higher when the feature fr in Fu also appearsin Fv with a high rank.
(See Kotlerman et alforhow P (r) and rel?
(fr) are computed.)
The result-ing score is normalized by dividing by the entail-ing vector size |Fu| (in accordance with the ideathat having more v features should not hurt be-cause the u features should be included in the vfeatures, not vice versa).To balance the potentially excessive asymmetryof APinc towards the features of the antecedent,Kotlerman et alaverage it with LIN, the widelyused symmetric measure of distributional similar-ity proposed by Lin (1998):LIN(u, v) =?f?Fu?Fv [wu(f) + wv(f)]?f?Fu wu(f) +?f?Fv wv(f)LIN essentially measures feature vector overlap.The positive PMI values wu(f) and wv(f) of afeature f in Fu and Fv are summed across thosefeatures that are positive in both vectors, normal-izing by the cumulative positive PMI mass in bothvectors.
Finally, balAPinc is the geometric aver-age of APinc and LIN:balAPinc(u|=v) =?APinc(u |= v) ?
LIN(u, v)To adapt balAPinc to recognize entailment, wemust select a threshold t above which we classifya pair as entailing.
In the experiments below, weexplore two approaches.
In balAPincupper, we op-timize the threshold directly on the test data, bysetting t to maximize the F-measure on the testset.
This gives us an upper bound on how well bal-APinc could perform on the test set (but note thatoptimizing F does not necessarily translate into agood accuracy performance, as clearly illustratedby Table 3 below).
In balAPincAN |= N, we use theAN |= N data set as training data and pick the tthat maximizes F on this training set.We use the balAPinc measure as a refer-ence point because, on the evidence provided byKotlerman et al it is the state of the art in varioustasks related to lexical entailment.
We recognizehowever that it is somewhat complex and specifi-cally tuned to capturing the relation of feature in-clusion.
Consequently, we also experiment witha more flexible classifier, which can detect othersystematic properties of vectors in an entailmentrelation.
We present this classifier next.SVM Support vector machines are widely usedhigh-performance discriminative classifiers thatfind the hyperplane providing the best separationbetween negative and positive instances (Cristian-ini and Shawe-Taylor, 2000).
Our SVM classifiersare trained and tested using Weka 3 and LIBSVM2.8 (Chang and Lin, 2011).
We use the defaultpolynomial kernel ((u ?v/600)3) with  (toleranceof termination criterion) set to 1.6.
This value wastuned on the AN |=N data set, which we never usefor testing.
In the same initial tuning experimentson the AN |=N data set, SVM outperformed deci-sion trees, naive Bayes, and k-nearest neighbors.We feed each potential entailment pair to SVMby concatenating the two vectors representing theantecedent and consequent expressions.2 How-ever, for efficiency and to mitigate data sparse-ness, we reduce the dimensionality of the seman-tic vectors to 300 columns using Singular ValueDecomposition (SVD) before feeding them to theclassifier.3 Because the SVD-reduced semantic2We have tried also to represent a pair by subtracting andby dividing the two vectors.
The concatenation operationgave more successful results.3To keep a manageable parameter space, we picked 300columns without tuning.
This is the best value reported inmany earlier studies, including classic LSA.
Since SVDsometimes improves the semantic space (Landauer and Du-28vectors occupy a 300-dimensional space, the en-tailment pairs occupy a 600-dimensional space.An SVM with a polynomial kernel takes intoaccount not only individual input features but alsotheir interactions (Manning et al 2008, chapter15).
Thus, our classifier can capture not just prop-erties of individual dimensions of the antecedentand consequent pairs, but also properties of theircombinations (e.g., the product of the first dimen-sions of the antecedent and the consequent).
Weconjecture that this property of SVMs is funda-mental to their success at detecting entailment,where relations between the antecedent and theconsequent should matter more than their inde-pendent characteristics.4 Predicting lexical entailment fromAN |= N evidenceSince the contexts of AN must be a subset of thecontexts of N, semantic vectors harvested fromAN phrases and their head Ns are by construc-tion in an inclusion relation.
The first experimentshows that these vectors constitute excellent train-ing data to discover entailment between nouns.This suggests that the vector pairs representingentailment between nouns are also in an inclusionrelation, supporting the conjectures of Kotlermanet al(2010) and others.Table 2 reports the results we obtained withbalAPincupper, balAPincAN |= N (Section 3.5) andSVMAN |= N (the SVM classifier trained on theAN |= N data).
As an upper bound for meth-ods that generalize from AN |= N, we also re-port the performance of SVM trained with 10-foldcross-validation on the N1 |= N2 data themselves(SVMupper).
Finally, we tried two baseline classi-fiers.
The first baseline (fq(N1)< fq(N2)) guessesentailment if the first word is less frequent thanthe second.
The second (cos(N1, N2)) applies athreshold (determined on the test set) to the co-sine similarity of the pair.
The results of thesebaselines shown in Table 2 use SVD; those with-out SVD are similar.
Both baselines outperformedmore trivial methods such as random guessing orfixed response, but they performed significantlyworse than SVM and balAPinc.Both methods that generalize entailment fromAN |= N to N1 |= N2 perform well, with 70%mais, 1997; Rapp, 2003; Schu?tze, 1997), we tried balAPincon the SVD-reduced vectors as well, but results were consis-tently worse than with PMI vectors.P R F Accuracy(95% C.I.
)SVMupper 88.6 88.6 88.5 88.6 (87.3?89.7)balAPincAN |= N 65.2 87.5 74.7 70.4 (68.7?72.1)balAPincupper 64.4 90.0 75.1 70.1 (68.4?71.8)SVMAN |= N 69.3 69.3 69.3 69.3 (67.6?71.0)cos(N1, N2) 57.7 57.6 57.5 57.6 (55.8?59.5)fq(N1)< fq(N2) 52.1 52.1 51.8 53.3 (51.4?55.2)Table 2: Detecting lexical entailment.
Results rankedby accuracy and expressed as percentages.
95% con-fidence intervals around accuracy calculated by bino-mial exact tests.accuracy on the test set, which is balanced be-tween positive and negative instances.
Interest-ingly, the balAPinc decision thresholds tuned onthe AN |= N set and on the test data are veryclose (0.26 vs. 0.24), resulting in very similar per-formance for balAPincAN |= N and balAPincupper.This suggests that the relation captured by bal-APinc on the phrasal entailment training data isindeed the same that the measure captures whenapplied to lexical entailment data.The success of this first experiment shows thatthe entailment relation present in the distribu-tional representation of AN phrases and theirhead Ns transfers to lexical entailment (entailmentamong Ns).
Most importantly, this result demon-strates that the semantic vectors of composite ex-pressions (such as ANs) are useful for lexical en-tailment.
Moreover, the result is in accordancewith the view of FS, that ANs and Ns have thesame semantic type, and thus they enter entail-ment relations of the same kind.
Finally, the hy-pothesis that entailment among nouns is reflectedby distributional inclusion among their semanticvectors (Kotlerman et al 2010) is supported bothby the successful generalization of the SVM clas-sifier trained on AN |= N pairs and by the goodperformance of the balAPinc measure.5 Generalizing QN entailmentThe second study is somewhat more ambitious,as it aims to capture and generalize the entailmentrelation between QPs (of shape QN) using onlythe corpus-harvested semantic vectors represent-ing these phrases as evidence.
We are thus firstand foremost interested in testing whether thesevectors encode information that can help a power-29P R F Accuracy(95% C.I.
)SVMpair-out 76.7 77.0 76.8 78.1 (77.5?78.8)SVMquantifier-out 70.1 65.3 68.0 71.0 (70.3?71.7)SVMQpair-out 67.9 69.8 68.9 70.2 (69.5?70.9)SVMQquantifier-out 53.3 52.9 53.1 56.0 (55.2?56.8)cos(QN1, QN2) 52.9 52.3 52.3 53.1 (52.3?53.9)balAPincAN |= N 46.7 5.6 10.0 52.5 (51.7?53.3)SVMAN |= N 2.8 42.9 5.2 52.4 (51.7?53.2)fq(QN1)<fq(QN2) 51.0 47.4 49.1 50.2 (49.4?51.0)balAPincupper 47.1 100 64.1 47.2 (46.4?47.9)Table 3: Detecting quantifier entailment.
Resultsranked by accuracy and expressed as percentages.95% confidence intervals around accuracy calculatedby binomial exact tests.ful classifier, such as SVM, to detect entailment.To abstract away from lexical or other effectslinked to a specific quantifier, we consider twochallenging training and testing regimes.
In thefirst (SVMpair-out), we hold out one quantifier pairas testing data and use the other 29 pairs in Table 1as training data.
Thus, for example, the classifiermust discover all dogs |= some dogs without see-ing any all N |= some N instance in the trainingdata.
In the second (SVMquantifier-out), we hold outone of the 12 quantifiers as testing data (that is,hold out every pair involving a certain quantifier)and use the rest as training data.
For example,the quantifier must guess all dogs |= some dogswithout ever seeing all in the training data.
Weexpect the second training regime to be more dif-ficult, not just because there is less training data,but also because the trained classifier is tested ona quantifier that it has never encountered withinany training QN sequence.4Table 3 reports the results for SVMpair-out andSVMquantifier-out, as well as for the methods wetried in the lexical entailment experiments.
(Asin the first study, the frequency- and cosine-based4In our initial experiments, we added negative entail-ment instances by blindly permuting the nouns, under theassumption that Q1N1 typically does not entail Q2N2 whenQ1 6= Q2 and N1 6= N2.
These additional instances turnedout to be much easier to classify: adding an equal proportionof them to the training data and testing data, such that thenumber of instances where N1 = N2 and where N1 6= N2is equal, reduced every error rate roughly by half.
The re-ported results do not involve these additional instances.baselines are only slightly better overall than moretrivial baselines.)
We consider moreover an alter-native approach that ignores the noun altogetherand uses vectors for the quantifiers only (e.g., thedecision about all dogs |=some dogs considers thecorpus-derived all and some vectors only).
Themodels resulting from this Q-only strategy aremarked with the superscript Q in the table.The results confirm clearly that semantic vec-tors for QNs contain enough information to allowa classifier to detect entailment: SVMquantifier-outperforms as well as the lexical entailment classi-fiers of our first study, and SVMpair-out does evenbetter.
This success is especially impressive givenour challenging training and testing regimes.In contrast to the first study, now SVMAN |= N,the classifier trained on the AN |= N data set,and balAPinc perform no better than the base-lines.
(Here balAPincupper and balAPincAN |= Npick very different thresholds: the first settlingon a very low t = 0.01, whereas for the sec-ond t = 0.26.)
As predicted by FS (see Section2.2 above), noun-level entailment does not gen-eralize to quantifier phrase entailment, since thetwo structures have different semantic types, cor-responding to different kinds of entailment rela-tions.
Moreover, the failure of balAPinc suggeststhat, whatever evidence the SVMs rely upon, it isnot simple feature inclusion.Interestingly, even the Q vectors alone encodeenough information to capture entailment abovechance.
Still, the huge drop in performance fromSVMQpair-out to SVMQquantifier-out suggests that the Q-only method learned ad-hoc properties that do notgeneralize (e.g., ?all entails every Q2?
).Tables 1 and 4 break down the SVM results by(pairs of) quantifiers.
We highlight the remark-able dichotomy in Table 4 between the good per-formance on the universal-like quantifiers (each,every, all, much) and the poor performance on theexistential-like ones (some, no, both, either).In sum, the QN experiments show that seman-tic vectors contain enough information to detecta logical relation such as entailment not only be-tween words, but also between phrases contain-ing quantifiers that determine their entailment re-lation.
While a flexible classifier such as SVMperforms this task well, neither measuring fea-ture inclusion nor generalizing nominal entail-ment works.
SVMs are evidently tapping intoother properties of the vectors.30Quantifier Instances Correct|= 6|= |= 6|=each 656 656 649 637 (98%)every 460 1322 402 1293 (95%)much 248 0 216 0 (87%)all 2949 2641 2011 2494 (81%)several 1731 1509 1302 1267 (79%)many 3341 4163 2349 3443 (77%)few 0 461 0 311 (67%)most 928 832 549 511 (60%)some 4062 3145 1780 2190 (55%)no 0 714 0 380 (53%)both 636 1404 589 303 (44%)either 63 63 2 41 (34%)Total 15074 16910 9849 12870 (71%)Table 4: Breakdown of results with leaving-one-quantifier-out (SVMquantifier-out) training regime.6 ConclusionOur main results are as follows.1.
Corpus-harvested semantic vectors repre-senting adjective-noun constructions andtheir heads encode a relation of entailmentthat can be exploited to train a classifierto detect lexical entailment.
In particular,a relation of feature inclusion between thenarrower antecedent and broader consequentterms captures both AN |= N and N1 |= N2entailment.2.
The semantic vectors of quantifier-noun con-structions also encode information sufficientto learn an entailment relation that general-izes to QNs containing quantifiers that werenot seen during training.3.
Neither the entailment information encodedin AN |= N vectors nor the balAPinc mea-sure generalizes well to entailment detectionin QNs.
This result suggests that QN vectorsencode a different kind of entailment, as alsosuggested by type distinctions in Formal Se-mantics.In future work, we want first of all to conductan analysis of the features in the Q1N |=Q2N vec-tors that are crucially exploited by our success-ful entailment recognizers, in order to understandwhich characteristics of entailment are encoded inthese vectors.Very importantly, instead of extracting vectorsrepresenting phrases directly from the corpus, weintend to derive them by compositional operationsproposed in the literature (see Section 2.1 above).We will look for composition methods producingvector representations of composite expressionsthat are as good as (or better than) vectors directlyextracted from the corpus at encoding entailment.Finally, we would like to evaluate our entail-ment detection strategies for larger phrases andsentences, possibly containing multiple quanti-fiers, and eventually embed them as core compo-nents of an RTE system.AcknowledgmentsWe thank the Erasmus Mundus EMLCT Programfor the student and visiting scholar grants to thethird and fourth author, respectively.
The firsttwo authors are partially funded by the ERC 2011Starting Independent Research Grant supportingthe COMPOSES project (nr.
283554).
We aregrateful to Gemma Boleda, Louise McNally, andthe anonymous reviewers for valuable comments,and to Ido Dagan for important insights into en-tailment from an empirical point of view.ReferencesTimothy Baldwin, Colin Bannard, Takaaki Tanaka,and Dominic Widdows.
2003.
An empirical modelof multiword expression decomposability.
In Pro-ceedings of the ACL 2003 Workshop on MultiwordExpressions, pages 89?96.Marco Baroni and Alessandro Lenci.
2011.
Howwe BLESSed distributional semantic evaluation.
InProceedings of the Workshop on Geometrical Mod-els of Natural Language Semantics.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of EMNLP, pages 1183?1193, Boston,MA.Johan Bos and Katja Markert.
2006.
When logicalinference helps determining textual entailment (andwhen it doesn?t.
In Proceedings of the Second PAS-CAL Challenges Workshop on Recognising TextualEntailment.Paul Buitelaar and Philipp Cimiano.
2008.
Bridgingthe Gap between Text and Knowledge.
IOS, Ams-terdam.Nathanael Chambers, Daniel Cer, Trond Grenager,David Hall, Chloe Kiddon, Bill MacCartney, Marie-Catherine de Marneffe, Daniel Ramage, Eric Yeh,31and Christopher D. Manning.
2007.
Learningalignments and leveraging natural logic.
In ACL-PASCAL Workshop on Textual Entailment and Para-phrasing.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
ACMTransactions on Intelligent Systems and Technol-ogy, 2(3):27:1?27:27.Kenneth Church and Peter Hanks.
1990.
Word associ-ation norms, mutual information, and lexicography.Computational Linguistics, 16(1):22?29.Nello Cristianini and John Shawe-Taylor.
2000.
Anintroduction to Support Vector Machines and otherkernel-based learning methods.
Cambridge Univer-sity Press, Cambridge.Ido Dagan, Bill Dolan, Bernardo Magnini, and DanRoth.
2009.
Recognizing textual entailment: ratio-nal, evaluation and approaches.
Natural LanguageEngineering, 15:459?476.Katrin Erk.
2009.
Supporting inferences in semanticspace: representing words as regions.
In Proceed-ings of IWCS, pages 104?115, Tilburg, Netherlands.Maayan Geffet and Ido Dagan.
2005.
The distribu-tional inclusion hypotheses and lexical entailment.In Proceedings of ACL, pages 107?114, Ann Arbor,MI.Edward Grefenstette and Mehrnoosh Sadrzadeh.2011.
Experimental support for a categorical com-positional distributional model of meaning.
In Pro-ceedings of EMNLP, pages 1395?1404, Edinburgh.Emiliano Guevara.
2010.
A regression modelof adjective-noun compositionality in distributionalsemantics.
In Proceedings of the ACL GEMS Work-shop, pages 33?37, Uppsala, Sweden.Marti Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings ofCOLING, pages 539?545, Nantes, France.Irene Heim and Angelika Kratzer.
1998.
Semantics inGenerative Grammar.
Blackwell, Oxford.Lili Kotlerman, Ido Dagan, Idan Szpektor, andMaayan Zhitomirsky-Geffet.
2010.
Directionaldistributional similarity for lexical inference.
Natu-ral Language Engineering, 16(4):359?389.Milen Kouleykov and Bernardo Magnini.
2005.
Treeedit sistance for textual entailment.
In Proceed-ings of RALNP-2005, International Conference onRecent Advances in Natural Language Processing,pages 271?278.Thomas Landauer and Susan Dumais.
1997.
Asolution to Plato?s problem: The latent semanticanalysis theory of acquisition, induction, and rep-resentation of knowledge.
Psychological Review,104(2):211?240.Dekang Lin.
1998.
An information-theoretic defini-tion of similarity.
In Proceedings of ICML, pages296?304, Madison, WI, USA.Kevin Lund and Curt Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, 28:203?208.Chris Manning, Prabhakar Raghavan, and HinrichSchu?tze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press, Cambridge.Jeff Mitchell and Mirella Lapata.
2010.
Composi-tion in distributional models of semantics.
Cogni-tive Science, 34(8):1388?1429.Richard Montague.
1970.
Universal Grammar.
Theo-ria, 36:373?398.Patrick Pantel and Deepak Ravichandran.
2004.
Au-tomatically labeliing semantic classes.
In Proceed-ings of HLT-NAACL 2004, pages 321?328.Reinhard Rapp.
2003.
Word sense discovery based onsense descriptor dissimilarity.
In Proceedings of the9th MT Summit, pages 315?322, New Orleans, LA.Magnus Sahlgren.
2006.
The Word-Space Model.Dissertation, Stockholm University.Helmut Schmid.
1995.
Improvements in part-of-speech tagging with an application to German.In Proceedings of the EACL-SIGDAT Workshop,Dublin, Ireland.Hinrich Schu?tze.
1997.
Ambiguity Resolution in Nat-ural Language Learning.
CSLI, Stanford, CA.Rion Snow, Daniel Juravsky, and Andrew Y. Ng.2005.
Learning syntactic patterns for automatic hy-pernym discovery.
In Proceedings of NIPS 17.Rion Snow, Daniel Juravsky, and Andrew Y. Ng.2006.
Semantic taxonomy induction from het-erogenous evidence.
In Proceedings of ACL 2006,pages 801?808.Richmond H. Thomason, editor.
1974.
Formal Phi-losophy: Selected Papers of Richard Montague.Yale University Press, New York.Peter Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37:141?188.Peter Turney.
2008.
A uniform approach to analogies,synonyms, antonyms and associations.
In Proceed-ings of COLING, pages 905?912, Manchester, UK.Julie Weeds, David Weir, and Diana McCarthy.
2004.Characterising measures of lexical distributionalsimilarity.
In Proceedings of the 20th Interna-tional Conference of Computational Linguistics,COLING-2004, pages 1015?1021.Fabio M. Zanzotto, Marco Pennacchiotti, and Alessan-dro Moschitti.
2007.
Shallow semantics in fast tex-tual entailment rule learners.
In Proceedings of theACL-PASCAL Workshop on Textual Entailment andParaphrasing.Maayan Zhitomirsky-Geffet and Ido Dagan.
2010.Bootstrapping distributional feature vector quality.Computational Linguistics, 35(3):435?461.32
