Context Modeling for Language and Speech GenerationKees  van  DeemterPhil ips Research Laborator ies,  WY 2.54Prof.
Holst laan 4, 5656 AA Eindhoven, The Nether landsdeemter@natlab.research.phi l ips.com1 In t roduct ionIt is well known that some of the most importantissues in the design of a dialogue system involve themodeling of linguistic context.
The present paperhighlights a number of these issues, focusing on thelanguage and speech generation components ofsuchsystems, and discusses their implications for the wayin which context has to be modeled in a spoken dia-logue system.
We will compare the 'dedicated' con-text models that have been proposed in theoreticaland computational linguistics with the more gen-eral models proposed in artificial intelligence.
Ourmain examples of a 'dedicated' context model willbe the context model of the 'Dial Your Disc' (DYD)music information system (Collier and Landsber-gen, 1995), (van Deemter and Odijk, 1997) and thebetter-known Discourse Representation Theory (e.g.
(Kamp and Reyle, 1993)) of which this model is avariant.
Our main example of a 'general' contextmodel is provided by the so-called 'Ist' formalism(McCarthy, 1993).2 A sketch  o f  the  dyd  sys temThe DYD system produces poken monologues de-rived from information stored in a general-purposedatabase about W.A.Mozart's instrumental compos-itions.
The goal of the monologue generator is togenerate from these data a large variety of spokentexts.
A generator like this could be part of an elec-tronic shopping system, where the system providesinformation and 'sales talk'.
The way in which userscan indicate their areas of interest will not be dis-cussed in this paper, which focuses on language andspeech generation.
A (highly simplified) example ofa database representation f a recording is:\[KV\] 32\[DATE\] 03/1766 - 04/1766\ [SORT\] quodlibet\[TITLE\] Galimathias MusicumA teleshopping system has to be entertaining.Therefore, an important system requirement is thata large variety of texts can be produced from thesame database structures.
Presentations are gener-ated on the basis of database information by makinguse of syntactic sentence templates (Henceforth, S-template): structured sentences with variables, i.e.,open slots for which expressions can be substituted.These syntactically structured sentence templatesindicate how the information provided by a databaseobject can be expressed in natural language.
The re-quired variety is achieved by having many differenttemplates for the same information and by havinga flexible mechanism for combining the generatedsentences into texts.
A template can be used, inprinciple, if there is enough information i  the data-base to fill its slots.
However, there are extra condi-tions to guard the welLformedness and effectivenessof presentations.
For example, certain points in thediscourse are more appropriate for the expression ofa certain bit of information.
Thus, it is important forthe system to maintain a record showing which in-formation has been expressed and when it has beenexpressed.
This record, which is called the Know-ledge State, will be part of DYD's Context Model.Many variations of the above presentation are pos-sible.
The system can, for instance, start mentioningthe date of composition, or information could be ad-ded that contrasts this composition with a previousone.
Also, there are various ways of referring to thecomposition being discussed, for instance by nameK.
300, with a definite noun phrase or with a pro-noun.
The appropriateness of a referring expressiondepends, among other things, on the existence andkinds of references to the referred object in previoussentences.
Therefore, it is important o maintaina record of which objects have been introduced inthe text, and how and when they have been referredto.
This record will be called the Discourse Model,which is also a part of the Context Model.48As was mentioned above, templates in our systemare structured sentences with slots for variable parts.For brevity, we will not represent syntactic structurebut only the terminals of templates:(composition) was/were written by (com-poser) (date)Slots are to be filled with structured expressionsthat contain database information.
This is donewith other, smaller, S-templates.
The system hasthree modules: Generation, Prosody and Speech.The module Generation generates syntax trees onthe basis of the Mozart database, a collection of S-templates, and the Context Model.
Conversely, itupdates the Context Model whenever a phrase hasbeen generated.
The module Prosody transforms asyntax tree into a sequence of annotated words, theannotations specifying accents and prosodic bound-aries (e.g.
pauses).
The module Speech transformsa sequence of annotated words into a speech signal(Collier and Landsbergen, 1995).3 Text  Generat ionAs explained in the previous ection, sentences aregenerated by means of S-templates.
An S-templateindicates how the meaning of a database recordcan be put into words.
Given the information rep-resented about the composition K.32 in the data-base, example sentences derived from the above-mentiuoned S-templates include: K.32 was writtenby W.A.Mozart in 1766 and This quodlibet was writ-ten by the composer in March 1766.
The fact that S-templatesare syntactically structured objects makesit possible to formulate various conditions on theform of variable parts.
In this way, it is possible toavoid the generation ofincorrect sentences such as Itwere written by him when Mozart was only ten yearsold.
Since S-templates are structured objects, con-ditions guaranteeing the appropriate choice for thevariable parts of the templates can refer to inform-ation contained in these structures.
For instance, itcan be read off the syntactic structure that the pro-noun 'it' is the singular subject of the second sen-tence and that therefore the finite verb should be'was'.Which sentences should be used in a given situation?First, it has to be determined what is going to besaid.
This is determined during the dialogue, wherethe user can indicate a preference for less or moreelaborate monologues.
This preference is stored inthe Dialogue State, a part of the Context Model inwhich all those properties of the dialogue history arerecorded that are relevant for monologue generation.Secondly, a selection has to be made from all S-templates in such a way that the text generated con-veys all and only the required information.
Onlythose S-templates are selected which are able to con-vey the relevant information; moreover, under nor-mal circumstances, the same information is presen-ted not more than once.
These requirements havebeen incorporated in the text generator, which alsopresents the sentences in such a way that the textshows a certain coherence.
Information should begrouped into convenient clusters and presented ina natural order.
Clustering is achieved by meansof the so-called Topic State.
For each paragraph ofthe monologue, the Topic State, which is anotherpart of the Context Model, keeps track of the topicof the paragraph, which is defined as a set of at-tributes from the (music) database.
For example, aparagraph may have 'place and date of performance'as its topic and then only those S-templates can beused that are associated with the attributes 'date'and 'place'.The text generator operates as follows: Each S-template 'attempts' to get a sentence generated fromit into the text.
Whether this succeeds depends onthe information conveyed by the sentence, which in-formation has been conveyed earlier, and whetherthe sentence can find a place in a natural groupingof sentences in paragraphs.
Only local conditions onthe Context Model and the properties of the cur-rent S-template determine whether a sentence is ap-propriate at a certain point in the text.
No globalproperties of the text are considered and no explicitplanning is involved.As we have seen, an important part of the ContextModel is a Discourse Model.
Starting with an emptyDiscourse Model, each candidate sentence adds dis-course referents and relevant associated informationto this model.
For example, the Discourse Modelmay record that a certain description (e.g., 'thiscomposition') has occurred as the x-th and x-t-l-stword of the y-th sentence of paragraph number zof the u-th monologue that has occurred during agiven user-system interaction.
Rules for anaphoraestablish the antecedents for anaphora, and after-wards it is checked whether the resulting DiscourseModel is well-formed (e.g., by checking whether eachpronoun has an antecedent, whether definite descrip-tions have been used appropriately, etc.).
If the Dis-course Model is found to be well-formed, the can-didate sentence can be used as an actual sentence.If not, a different candidate sentence is subjected toexamination, etc.
We will see that very similar rules,which are also based on the information in the Dis-course Model, are used to determine which words in49the sentence are to be accented.4 P rosody  and  speechGenerating acceptable speech requires yntactic andsemantic information that is hard to extract fromunazmotated text.
In the present setting, however,speech generation is helped by the availability ofsyntactic and semantic information.
When the gen-eration module outputs a sentence, the generatedstructure contains all the syntactic information thatwas present in the S-template from which it results.Moreover, the Discourse Model, as we have seen,contains emantic information about the sentence.Both kinds of information are used to find the properlocations for pitch accents.Existing speech synthesis ystems (e.g., Bell Labs'Newspeak program) have typically de-stressed allcontent words that had occurred in the recent past.Yet, these systems still stress too many words(Hirschberg, 1990).
To remedy this defect, wehave redefined givenness and newness as propertiesnot of individual words, but of entire phrases (vanDeemter, 1994).
These definitions are combinedwith a version of Focus-Accent theory to determinethe exact word at which the accent must land.Inspection of the relevant facts suggests stronglythat words of very different forms may cause a wordto have 'given' status.
For example, an occurrence of'K.32' or of 'this composition' may become 'given',and hence de-stressed ( e-accented) ue to an earlierreference to K.32:You have selected K.32.You will now hear K.32/this composition.De-stressing and pronominalization ccur in roughlythe same environments, namely those in which anexpression contains 'given' information.
This sug-gests that both may be viewed as reduction phenom-ena that are caused by semantic redundancy (VanDeemter 1994).
The Discourse Model presents itselfas a natural candidate to implement this idea, sinceit contains all the relevant information.
In particu-lar, it says, for each referentially used Noun Phrase,whether and where in the discourse the object thatit refers to was described earlier.
If such an 'ante-cedent' for an expression is found earlier in the sameparagraph, the expression is considered 'given' in-formation (i.e., it is not 'in focus').
If not, it is con-sidered 'new' (i.e., it is 'in focus').The basic insight of Focus-Accent (e.g.
(Ladd,1980)) is the idea that the syntactic structure of asentence determines its 'metrical' structure.
Met-rical structure is most conveniently represented bybinary trees, in which one daughter of each node ismarked as 'strong' and the other as 'weak'.
Met-rical structure determines which leaves of the treeare most suitable to carry an accent on syntacticgrounds.
Roughly, these are the leaves that can bereached through a path that starts from an expres-sion that is 'in focus', and that does not contain weaknodes (Dirksen, 1992).
More exactly, if a given ma-jor phrase is 'in focus', it is also marked as accented,and so is each strong node that is the daughter ofa node that is marked as accented.
Accent is real-ized on those leaves that are marked as accented.However, several obstacles may prevent his fromhappening.
For example,(a) A major phrase is marked -A if it is notin focus.
(b) A leaf x is marked -A if there is a re-cent occurrence of an expression y which issemantically subsumed by x.
(c) A leaf is marked -A if it is lexicallymarked as unfit to carry an accent thatis due to informational status.
(Examples:'the', 'a', some prepositions.
)The result of an -A marking is that the so-calledDefault Accent rule (cf.
Ladd 1980) is triggered,which transforms one metrical tree into another:Default Accent rule: If a strong node nl ismarked -A, while its weak sister n2 is not,then the strong/weak labeling of the sistersis reversed: nl is now marked weak, and n2is marked strong.In English, it is usually, but not always, the rightdaughter of a mother node that is strong.
Thus, themetrical tree for our earlier example looks as follows:/ \you,w will now hear,s/ \will,w now hear K.32,s/ \now,w hear K.32,s/ \hear,w K.32,sAssume that the Verb Phrase is 'in focus' and there-fore labeled as accented.
If semantic factors wouldnot intervene, K.32 would carry an accent.
But sinceK.32 is also referred to in the previous entence ofthe discourse, K.32 represents 'given' information,and is marked -A.
As a result, the Default Accentrule swaps the strong/weak (S/W) labeling between'hear' and 'K.32' before the 'accented' labels are as-signed.
Consequently, the sentence accent rickles50down along a path of strong nodes and ends up on'hear'.5 Context  mode l ingWe have seen how the Knowledge State, the TopicState, the Context State, and the Dialogue State to-gether form one large Context Model which is used(and maintained) by the DYD system to generate itsspoken monologues.
But context models have alsocome up in other settings.
Wouldn't it have beenpossible to re-use these context models for our pur-poses?5.1 Context Modeling in AIOne might try to use a general-purpose theory ofcontext o formalize DYDs Context Model.
The so-called 'Ist' theory (McCarthy, 1993}, (Burnt, 1996)can be used for this purpose.
Ist(c,p) can be readas saying that p is true with respect o c. Now let cbe the context hat obtains after the sentence 'Moz-art composed K.280' has been generated.
We cannow say various things about c, and then use theIst-formaiism to say that a second sentence (for in-stance, 'It is a sonata') is expressed in c. The nota-tion DE(c) stands for the set of 'discourse ntities'(roughly: earlier-introduced in ividuals) associatedwith c:Text(c) -- Mozart composed K.280Speaker(c) = dydPrevious entence(c) = ...DE(c) = {x, y}Conditions(c) ={ x=W.A.Mozart,y=K.280,x composed y }Ist(c,It is a sonata}, etc.The 'DE' predicate plays the role of DYD's so-calledDiscourse Model, noting which objects in the data-base have been referred to in the monologue.
Thisinformation can be exploited when the second ut-terance, It is a sonata, is interpreted 'in the con-text of' c. This suggests that important parts ofDYD's Context Model may be mirrored in the Ist-formalism.
But linguistic contexts have a peculiar-ity: they change during processing: discourse n-tities are added, objects and expressions move intoand out of focus as sentences are generated or inter-preted.
This requires extensions of the Ist formal-ism.
For example, one need an 'update' operator '?
'to say how a context c is changed when the sentenceS has been processed in c:c+S=c'Also, one needs several operators to compare con-texts.
Thus, one might writec\[x,y\]c'to express that c and c' are alike, except'for thediscourse ntities x and y.
Using such extensions,Discourse Representation Theory can be mirroredin the Ist formalism.
This is a useful exercise, whichleads to a better understanding of the peculiaritiesof linguistic context.
But it also raises the questionof whether we might have used DRT as a backbonefor DYD's Context Model.5.2 Context Model ing in DRTIn the setting of DYD, DRT could take the form of acontext model containing a series of sub-DRSs, thefirst of which contains information extracted fromthe dialogue that has led up to the selection of thefirst composition plus the monologue following it,and so on.
However, setting up structures of thiskind would have required a tremendous amount ofwork since generation requires many kinds of inform-ation that are neither outinely represented in ex-isting versions of DRT, nor trivial to calculate onthe basis of them.
For example, DRSs do not nor-really contain a representation f their subject mat-ter (their 'topic') and it would not be a trivial matterto deduce this information from the truth conditionsof the DRS (Demolombe and Jones, 1995).
Fur-thermore, standard versions of DRT do not containinformation about the exact place of occurrence ofexpressions, nor do they contain information aboutparagraph structure.
Of course, information of allthese kinds might be added.
The result would be anew, extended version of DRT, which would complic-ate drastically the formal basis of this theory (Mus-kens et al, 1996).
Moreover, conventional DRSs con-tain plenty of semantic information that is not im-mediately relevant for current (i.e., generative) pur-poses.
DRSs contain both less and more than whatis needed for language generation.The conclusion seems unavoidable: Language gen-eration requires a specific kind of context modelswhich are suitable to formalize the notion of a lin-guistic context.
DYD's Context Model was designedto be such a context model.
It might be viewed asa modest, computationally feasible version of DRT.This context model, with all its diverse components,may not be as elegant as some of the context modelsdiscussed in the present section.
But it is difficult osee how the requirements of high-quality languageand speech generation can be reconciled with formalelegance.51ReferencesS.
Buva~.
1996.
Resolving lexical ambiguity using aformal theory of context.
In K.van Deemter andS.Peters, editors, Semantic Ambiguity and Under-specification.
CSLI Publications.Rend Collier and Jan Landsbergen.
1995.
Languageand speech generation.
Philips Journal of Re-search, 49(4):419-437.Demolombe and Jones.
1995.
Reasoning about top-ics: Towards a formal theory.
In Working Notesof Workshop on Formalizing Context.
AAAI.A.
Dirksen.
1992.
Accenting and deaccenting: a de-clarative approach.
In Proc.
off COLING, Nantes,France.J.
Hirschberg.
1990.
Accent and discourse con-text: assigning pitch accent in synthetic speech.In Proc.
of AAAI, page 953.Hans Kamp and Uwe Reyle.
1993.
From Dis-course to Logic, volume 42 of Studies in Linguist-ics and Philosophy.
Kluwer Academic Publishers,Dordrecht.D.R.
Ladd.
1980.
The Structure of IntonationalMeaning: Evidence from English.
Indiana Uni-versity Press.John McCarthy.
1993.
Notes of formalizing context.In Proceedings of IJCAI.Reinhard Muskens, Johan van Benthem, and AlbertVisser.
1996.
Dynamics.
In J.van Benthem andA.ter Meulen, editors, Handbook of Logic and Lan-guage.
Elsevier Science Publishers.Kees van Deemter and Jan Odijk.
1997.
Contextmodeling and the generation of spoken discourse.Speech Communication, 21(1/2):101-121.K.
van Deemter.
1994.
What's new?
A semanticperspective on sentence accent.
Journal of Se-mantics, 11:1-31.32
