Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 212?216,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsCreating a Bi-lingual Entailment Corpus through Translations withMechanical Turk: $100 for a 10-day RushMatteo Negri1 and Yashar Mehdad1,2FBK-Irst1, University of Trento2Trento, Italy{negri,mehdad}@fbk.euAbstractThis paper reports on experiments in the cre-ation of a bi-lingual Textual Entailment cor-pus, using non-experts?
workforce under strictcost and time limitations ($100, 10 days).
Tothis aim workers have been hired for transla-tion and validation tasks, through the Crowd-Flower channel to Amazon Mechanical Turk.As a result, an accurate and reliable corpus of426 English/Spanish entailment pairs has beenproduced in a more cost-effective way com-pared to other methods for the acquisition oftranslations based on crowdsourcing.
Focus-ing on two orthogonal dimensions (i.e.
relia-bility of annotations made by non experts, andoverall corpus creation costs), we summarizethe methodology we adopted, the achieved re-sults, the main problems encountered, and thelessons learned.1 IntroductionTextual Entailment (TE) (Dagan and Glickman,2004) has been proposed as a generic framework formodelling language variability.
Given a text T andan hypothesis H, the task consists in deciding if themeaning of H can be inferred from the meaning ofT.
At the monolingual level, the great potential ofintegrating TE recognition (RTE) components intoNLP architectures has been demonstrated in severalareas, including question answering, information re-trieval, information extraction, and document sum-marization.
In contrast, mainly due to the absence ofcross-lingual TE (CLTE) recognition components,similar improvements have not been achieved yetin any cross-lingual application.
Along such di-rection, focusing on feasibility and architectural is-sues, (Mehdad et al, 2010) recently proposed base-line results demonstrating the potential of a simpleapproach that integrates Machine Translation andmonolingual TE components.As a complementary research problem, this paperaddresses the data collection issue, focusing on thedefinition of a fast, cheap, and reliable methodologyto create CLTE corpora.
The main motivation is that,as in many other NLP areas, the availability of largequantities of annotated data represents a critical bot-tleneck in the systems?
development/evaluation cy-cle.
Our first step in this direction takes advantageof an already available monolingual corpus, castingthe problem as a translation one.
The challenge con-sists in taking a publicly available RTE dataset ofEnglish T-H pairs (i.e.
the PASCAL-RTE3 dataset1),and create its English-Spanish CLTE equivalent bytranslating the hypotheses into Spanish.
To thisaim non-expert workers have been hired throughthe CrowdFlower2 channel to Amazon MechanicalTurk3 (MTurk), a crowdsourcing marketplace re-cently used with success for a variety of NLP tasks(Snow et al, 2008; Callison-Burch, 2009; Mihalceaand Strapparava, 2009; Marge et al, 2010; Ambatiet al, 2010).The following sections overview our experiments,carried out under strict time (10 days) and cost($100) limitations.
In particular, Section 2 describesour data acquisition process; Section 3 summarizes1Available at: http://www.nist.gov/tac/data/RTE/index.html2http://crowdflower.com/3https://www.mturk.com/mturk/212the successive approximations that led to the defini-tion of our methodology, and the lessons learned ateach step; Section 4 concludes the paper and pro-vides directions for future work.2 Corpus creation cyclesStarting from the RTE3 Development set (800 En-glish T-H pairs), our corpus creation process hasbeen organized in sentence translation-validationcycles, defined as separate ?jobs?
routed to Crowd-Fower?s workforce.
At the first stage of each cycle,the original English hypotheses are used to create atranslation job for collecting their Spanish equiva-lents.
At the second stage, the collected translationsare used to create a validation job, where multiplejudges are asked to check the correctness of eachtranslation, given the English source.
Translated hy-potheses that are positively evaluated by the major-ity of trustful validators (i.e.
those judged correctwith a confidence above 0.8) are retained, and di-rectly stored in our CLTE corpus together with thecorresponding English texts.
The remaining onesare used to create a new translation job.
The proce-dure is iterated until substantial agreement for eachtranslated hypothesis is reached.As regards the first phase of the cycle, we definedour translation HIT as follows:In this task you are asked to:?
First, judge if the Spanish sentence is a correcttranslation of the English sentence.
If the En-glish sentence and its Spanish translation are blank(marked as -), you can skip this step.?
Then, translate the English sentence above the textbox into Spanish.Please make sure that your translation is:1.
Faithful to the original phrase in both meaning andstyle.2.
Grammatically correct.3.
Free of spelling errors and typos.Don?t use any automatic (machine) translation tool!
Youcan have a look at any on-line dictionary or referencefor the meaning of a word.This HIT asks workers to first check the qual-ity of an English-Spanish translation (used as a goldunit), and then write the Spanish translation of anew English sentence.
The quality check allowsto collect accurate translations, by filtering outjudgments made by workers missing more than20% of the gold units.As regards the second phase of the cycle, ourvalidation HIT has been defined as follows:Su tarea es verificar si la traduccio?n dada de unafrase del Ingle?s al espaol es correcta o no.
La traduccio?nes correcta si:1.
El estilo y sentido de la frase son fieles a los de laoriginal.2.
Es gramaticalmente correcta.3.
Carece de errores ortogra?ficos y tipogra?ficos.Nota: el uso de herramientas de traduccio?n automa?tica(ma?quina) no esta?
permitido!This HIT asks workers to take binary decisions(Yes/No) for a set of English-Spanish translationsincluding gold units.
The title and the descriptionare written in Spanish in order to weed out untrustedworkers (i.e.
those speaking only English), andattract the attention of Spanish speakers.In our experiments, both the translation and vali-dation jobs have been defined in several ways, tryingto explore different strategies to quickly collect reli-able data in a cost effective way.
Such cost reductioneffort led to the following differences between ourwork and similar related approaches documented inliterature (Callison-Burch, 2009; Snow et al, 2008):?
Previous works built on redundancy of the col-lected translations (up to 5 for each sourcesentence), thus resulting in more costly jobs.For instance, adopting a redundancy-based ap-proach to collect 5 translations per sentence atthe cost of $0.01 each, and 5 validations pertranslation at the cost of $0.002 each, would re-sult in $80 for 800 sentences.Assuming that the translation process is com-plex and expensive, our cycle-based techniquebuilds on simple and cheap validation mech-anisms that drastically reduce the amount oftranslations required.
In our case, 1 translationper sentence at the cost of $0.01, and 5 valida-tions per translation at the cost of $0.002 each,213would result in $32 for 800 sentences, makinga conservative assumption of up to 8 iterationswith 50% wrong translations at each cycle (i.e.800 sentences in the first cycle, 400 in the sec-ond, 200 in the third, etc.).?
Previous works involving validation of the col-lected data are based on ranking/voting mecha-nisms, where workers are asked to order a num-ber of translations, or select the best one giventhe source.
Our approach to validation is basedon asking workers to take binary decisions oversource-target pairs.
This results in an easier,faster, and eventually cheaper task.?
Previous works did not use any specific methodto qualify the workers?
knowledge, apart frompost-hoc agreement computation.
Our ap-proach systematically includes gold units to fil-ter out untrusted workers during the process.As a result we pay only for qualified judgments.3 Experiments and lessons learnedThe overall methodology, and the definition of theHITs described in Section 2, are the result of suc-cessive approximations that took into account twocorrelated aspects: the quality of the collected trans-lations, and the current limitations of the Crowd-Flower service.
On one side, simpler, cheaper, andfaster jobs launched in the beginning of our experi-ments had to be refined to improve the quality of theretained translations.
On the other side, ad-hoc solu-tions had to be found to cope with the limited qualitycontrol functionalities provided by CrowdFlower.
Inparticular, the lack of regional qualifications of theworkers, and of any qualification tests mechanism(useful features of MTurk) raised the need of defin-ing more controlled, but also more expensive jobs.Table 1 and the rest of this section summarize theprogress of our work in defining the methodologyadopted, the main improvements experimented ateach step, the overall costs, and the lessons learned.Step 1: a na?
?ve approach.
Initially, transla-tion/validation jobs were defined without using qual-ification mechanisms, giving permission to anyworker to complete our HITs.
In this phase, our goalwas to estimate the trade-off between the requireddevelopment time, the overall costs, and the qual-ity of translations collected in the most na?
?ve condi-tions.As expected, the job accomplishment time wasnegligible, and the overall cost very low.
Morespecifically, it took about 1 hour for translating the800 hypotheses at the cost of $12, and less than 6hours to obtain 5 validations per each translation atthe same cost of $12.Nevertheless, as revealed by further experimentswith the introduction of gold units, the quality of thecollected translations was poor.
In particular, 61% ofthem should have been rejected, often due to grossmistakes.
As an example, among the collected mate-rial several translations in languages other than En-glish revealed a massive and defective use of on-linetranslation tools by untrusted workers, as also ob-served by (Callison-Burch, 2009).Step 2: reducing validation errors.
A first im-provement addressed the validation phase, wherewe introduced gold units as a mechanism to qual-ify the workers, and consequently prune the un-trusted ones.
To this aim, we launched the valida-tion HIT described in Section 2, adding around 50English-Spanish control pairs.
The pairs (equallydistributed into positive and negative samples) havebeen extracted from the collected data, and manuallychecked by a Spanish native speaker.The positive effect of using gold units has beenverified in two ways.
First, we checked the qualityof the translations collected in the first na?
?ve transla-tion job, by counting the number of rejections (61%)after running the improved validation job.
Then, wemanually checked the quality of the translations re-tained with the new job.
A manual check on 20% ofthe retained translations was carried out by a Span-ish native speaker, resulting in 97% Accuracy.
The3% errors encountered are equally divided into mi-nor translation errors, and controversial (but sub-stantially acceptable) cases due to regional Spanishvariations.The considerable quality improvement observedhas been obtained with a small increase of 25% inthe cost (less than $3).
However, as regards the ac-complishment time, adding the gold units to qualifyworkers led to a considerable increase in duration(about 4 days for the first iteration).
This is mainly214due to the high number of automatically rejectedjudgments, obtained from untrusted workers miss-ing the gold units.
Because of the discrepancy be-tween trusted and untrusted judgments, we faced an-other limitation of the CrowdFlower service, whichfurther delayed our experiments.
Often, in fact, therapid growth of untrusted judgments activates auto-matic pausing mechanisms, based on the assumptionthat gold units are not accurate.
This, however, is astrong assumption which does not take into accountthe huge amount of non-qualified workers accepting(or even just playing with) the HITs.
For instance,in our case the vast majority of errors came fromworkers located in specific regions where the nativelanguage is not Spanish nor English.Step 3: reducing translation errors.
The ob-served improvement obtained by introducing goldunits in the validation phase, led us to the definitionof a new translation task, also involving a similarqualification mechanism.
To this aim, due to lan-guage variability, it was clearly impossible to usereference translations as gold units.
Taking into ac-count the limitations of the CrowdFlower interface,which does not allow to set qualification tests orsplit the jobs into sequential subtasks (other effec-tive and widely used features of MTurk), we solvedthe problem by defining the translation HITs as de-scribed in Section 2.
This solution combines a va-lidity check and a translation task, and proved to beeffective with a decrease in the translations eventu-ally rejected (45%).Step 4: reducing time.
Considering the extra timerequired by using gold units, we decided to spendmore money on each HIT to boost the speed of ourjobs.
In addition, to overcome the delays caused bythe automatic pausing mechanism, we obtained fromCrowdFlower the possibility to pose regional quali-fication, as commonly used in MTurk.As expected, both solutions proved to be effective,and contributed to the final definition of our method-ology.
On one side, doubling the payment for eachtask (from $0.01 to $0.02 for each translation andfrom from $0.002 to $0.005 for each validation), wehalved the required time to finish each job.
On theother side, by imposing the regional qualification,we eventually avoided unexpected automatic pauses.4 Conclusion and future workWe presented a set of experiments targeting the cre-ation of bi-lingual Textual Entailment corpora bymeans of non experts?
workforce (i.e.
the Crowd-Flower channel to Amazon Mechanical Turk).As a first step in this direction, we took advantageof an already existing monolingual English RTE cor-pus, casting the problem as a translation task whereSpanish translations of the hypotheses are collectedand validated by the workers.
Strict time and costlimitations on one side, and the current limitationsof the CrowdFlower service on the other side, ledus to the definition of an effective corpus creationmethodology.
As a result, less than $100 were spentin 10 days to define such methodology, leading tocollect 426 pairs as a by-product.
However, it?sworth remarking that applying this technique to cre-ate the full corpus would cost about $30.The limited costs, together with the short time re-quired to acquire reliable results, demonstrate theeffectiveness of crowdsourcing services for simplesentence translation tasks.
However, while MTurk isalready a well tested, stable, and rich of functional-ities platform, some limitations emerged during ourexperience with the more recent CrowdFlower ser-vice (currently the only one accessible to non-UScitizens).
Some of these limitations, such as theregional qualification mechanism, have been over-come right after the end of our experimentation withthe introduction of new functionalities provided as?Advanced Options?.
Others (such as the lack ofother qualification mechanisms, and the automaticpausing of the HITs in case of high workers?
errorrates on the gold units) at the moment still representa possible complication, and have to be carefullyconsidered when designing experiments and inter-preting the results4.In light of this positive experience, next stepsin our research will further explore crowdsourcing-based data acquisition methods to address the com-plementary problem of collecting new entailmentpairs from scratch.
This will allow to drastically re-duce data collection bottlenecks, and boost researchboth on cross-lingual and mono-lingual Textual En-4However, when asked through the provided support ser-vice, the CrowdFlower team proved to be quite reactive in pro-viding ad-hoc solutions to specific problems.215Elapsed time Running cost Focus Lessons learned1 day $24 Approaching CrowdFlower,defining a na?
?ve methodologyNeed of qualification mechanism,task definition in Spanish.7 days $58 Improving validation Qualification mechanisms (gold unitsand regional) are effective, need ofpayment increase to boost speed.9 days $99.75 Improving translation Combined HIT for qualification, pay-ment increase worked!10 days $99.75 Obtaining bi-lingual RTE corpus Fast, cheap, and reliable method.Table 1: $100 for a 10-day rush (summary and lessons learned)tailment.AcknowledgmentsWe would like to thank MTurk and CrowdFlowerfor providing the $100 credit used for the experi-ments, and our colleague Silvana Bernaola Biggio,who kindly accepted to validate our results.The research leading to these results has re-ceived funding from the European Community?sSeventh Framework Programme (FP7/2007-2013)under Grant Agreement n. 248531.ReferencesV.
Ambati, S. Vogel and J. Carbonell 2010.
ActiveLearning and Crowd-Sourcing for Machine Transla-tion.
To appear in Proceedings of LREC 2010.C.
Callison-Burch 2009.
Fast, Cheap, and Creative:Evaluating Translation Quality Using Amazon?s Me-chanical Turk.
In Proceedings of EMNLP 2009.I.
Dagan and O. Glickman 2004.
Probabilistic TextualEntailment: Generic Applied Modeling of LanguageVariability.
In Proceedings of the PASCAL Workshopof Learning Methods for Text Understanding and Min-ing.M.
Marge, S. Banerjee and A. Rudnicky 2010.
Using theAmazon Mechanical Turk for Transcription of SpokenLanguage.
In Proceedings of the 2010 IEEE Interna-tional Conference on Acoustics, Speech and SpokenLanguage (ICASSP 2010).Y.
Mehdad, M. Negri, and M. Federico 2010.
TowardsCross-Lingual Textual Entailment.
To appear in Pro-ceedings of NAACL HLT 2010.R.
Mihalcea and C. Strapparava 2009.
The Lie Detector:Explorations in the Automatic Recognition of Decep-tive Language.
In Proceedings of ACL 2009.R.
Snow, B. O?Connor, D. Jurafsky, and A. Y. Ng 2008.Cheap and Fast - but is it Good?
Evaluating Non-expert Annotations for Natural Language Tasks.
InProceedings of EMNLP 2008.216
