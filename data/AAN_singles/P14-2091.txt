Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 557?562,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsA joint inference of deep case analysis and zero subject generation forJapanese-to-English statistical machine translationTaku Kudo, Hiroshi Ichikawa, Hideto KazawaGoogle Japan{taku,ichikawa,kazawa}@google.comAbstractWe present a simple joint inference ofdeep case analysis and zero subject gener-ation for the pre-ordering in Japanese-to-English machine translation.
The detec-tion of subjects and objects from Japanesesentences is more difficult than that fromEnglish, while it is the key process to gen-erate correct English word orders.
In addi-tion, subjects are often omitted in Japanesewhen they are inferable from the context.We propose a new Japanese deep syntac-tic parser that consists of pointwise proba-bilistic models and a global inference withlinguistic constraints.
We applied our newdeep parser to pre-ordering in Japanese-to-English SMT system and show substantialimprovements in automatic evaluations.1 IntroductionJapanese to English translation is known to be oneof the most difficult language pair for statisticalmachine translation (SMT).
It has been widely be-lieved for years that the difference of word or-ders, i.e., Japanese is an SOV language, while En-glish is an SVO language, makes the English-to-Japanese and Japanese-to-English translation dif-ficult.
However, simple, yet powerful pre-orderingtechniques have made this argument a thing of thepast (Isozaki et al, 2010b; Komachi et al, 2006;Fei and Michael, 2004; Lerner and Petrov, 2013;Wu et al, 2011; Katz-Brown and Collins, 2008;Neubig et al, 2012; Hoshino et al, 2013).
Pre-ordering processes the source sentence in such away that word orders appear closer to their finalpositions on the target side.While many successes of English-to-Japanesetranslation have been reported recently, the qualityimprovement of Japanese-to-English translation isstill small even with the help of pre-ordering (Gotoet al, 2013).
We found that there are two ma-jor issues that make Japanese-to-English transla-tion difficult.
One is that Japanese subject and ob-ject cannot easily be identified compared to En-glish, while their detections are the key processto generate correct English word orders.
Japanesesurface syntactic structures are not always corre-sponding to their deep structures, i.e., semanticroles.
The other is that Japanese is a pro-drop lan-guage in which certain classes of pronouns maybe omitted when they are pragmatically inferable.In Japanese-to-English translation, these omittedpronouns have to be generated properly.There are several researches that focused on thepre-ordering with Japanese deep syntactic analysis(Komachi et al, 2006; Hoshino et al, 2013) andzero pronoun generation (Taira et al, 2012) forJapanese-to-English translation.
However, thesetwo issues have been considered independently,while they heavily rely on one another.In this paper, we propose a simple joint infer-ence which handles both Japanese deep structureanalysis and zero pronoun generation.
To the bestof our knowledge, this is the first study that ad-dresses these two issues at the same time.This paper is organized as follows.
First, we de-scribe why Japanese-to-English translation is dif-ficult.
Second, we show the basic idea of thiswork and its implementation based on pointwiseprobabilistic models and a global inference withan integer linear programming (ILP).
Several ex-periments are employed to confirm that our newmodel can improve the Japanese to English trans-lation quality.2 What makes Japanese-to-Englishtranslation difficult?Japanese syntactic relations between argumentsand predicates are usually specified by particles.There are several types of particles, but we focuson ?
(ga), ?
(wo) and ?
(wa) for the sake of557Table 1: An example of difficult sentence for pars-ingSentence: ??
?
??
?
??
?.Gloss: today wa TOP liquor ga NOM can drink.Translation: (I) can drink liquor today.simplicity1.?
ga is usually a subject marker.
However, itbecomes an object marker if the predicate hasa potential voice type, which is usually trans-lated into can, be able to, want to, or wouldlike to.?
wo is an object marker.?
wa is a topic case marker.
The topic can beanything that a speaker wants to talk about.
Itcan be subject, object, location, time or anyother grammatical elements.We cannot always identify Japanese subject andobject only by seeing the surface case markers ga,wo and wa.
Especially the topic case marker isproblematic, since there is no concept of topic inEnglish.
It is necessary to get a deep interpretationof topic case markers in order to develop accurateJapanese-to-English SMT systems.Another big issue is that Japanese subject (oreven an object) can be omitted when they canpragmatically be inferable from the context.
Sucha pronoun-dropping is not a unique phenomenonin Japanese actually.
For instance, Spanish alsoallows to omit pronouns.
However, the inflec-tional suffix of Spanish verbs include a hint of theperson of the subject.
On the other hand, infer-ring Japanese subjects is more difficult than Span-ish, since Japanese verbs usually do not have anygrammatical cues to tell the subject type.Table 1 shows an example Japanese sentencewhich cannot be parsed only with the surfacestructure.
The second token wa specifies the rela-tion between??
(today) and???
(can drink).Human can easily tell that the relation of them isnot a subject but an adverb (time).
The topic casemarker wa implies that the time when the speakerdrinks liquor is the focus of this sentence.
The4th token ga indicates the relation between ??
(liquor) and ???
(can drink).
Since the predi-cate has a potential voice (can drink), the ga par-ticle should be interpreted as an object here.
In1Other case markers are less frequent than these threemarkersthis sentence, the subject is omitted.
In general, itis unknown who speaks this sentence, but the firstperson is a natural interpretation in this context.Another tricky phenomenon is that detectingvoice type is not always deterministic.
Thereare several ways to generate a potential voice inJapanese, but we usually put the suffix word??
(reru) or ???
(rareru) after predicates.
How-ever, these suffix words are also used for a passivevoice.In summary, we can see that the followingfour factors are the potential causes that make theJapanese parsing difficult.?
Japanese voice type detection is not straight-forward.
reru or rareru are used either forpassive or potential voice.?
surface case ga changes its interpretationfrom subject to object when the predicate hasa potential voice.?
topic case marker wa is used as a topic casemarker which doesn?t exist in English.
Topicis either subject, object or any grammaticalelements depending on the context.?
Japanese subject is often omitted when it isinferable from the context.
There is no cue totell the subject person in verb suffix (inflec-tion) like in Spanish verbsWe should note that they are not always inde-pendent issues.
For instance, the deep case detec-tion helps to tell the voice type, and vice versa.Another note is that they are unique issuesobserved only in Japanese-to-English translation.In English-to-Japanese translation, it is accept-able to generate Japanese sentences that do notuse Japanese topic markers wa.
Also, generatingJapanese pronoun from English pronoun is accept-able, although it sounds redundant and unnaturalfor native speakers.3 A joint inference of deep case analysisand zero subject generation3.1 Probabilistic model overpredicate-argument structuresOur deep parser runs on the top of a dependencyparse tree.
First, it extracts all predicates and theirarguments from a dependency tree by using man-ual rules over POS tags.
Since our pre-orderingsystem generates the final word orders from alabeled dependency tree, we formalize our deep558parsing task as a simple labeling problem over de-pendency links, where the label indicates the deepsyntactic roles between head and modifier.We here define a joint probability over a predi-cate and its arguments as follows:P (p, z, v, A, S,D) (1)where?
p: a predicate?
z: a zero subject candidate for p. z ?
Z ={I, you, we, it, he/she, imperative, already exists}?
v: voice type of the predicate p. v ?
V ={active, passive, potential}?
ak?
A: k-th argument which modifies or ismodified by the predicate2.?
dk?
D: deep case label which represents adeep relation between akand p. d ?
{ sub-ject, object, other }, where other means thatdeep case is neither subject nor object.?
sk?
S: surface relation (surface casemarker) between akand p.We assume that a predicate p is independentfrom other predicates in a sentence.
This assump-tion allows us to estimate the deep structures of pseparately, with no regard to which decisions aremade in other predicates.An optimal zero subject label z, deep cases D,and voice type v for a given predicate p can besolved as the following optimization problem.
?z?, v?,?D?
= argmaxz,v,DP (p, z, v, A, S,D)Since the inference of this joint probability is diffi-cult, we decompose P (p, z, v, A, S,D) into smallindependent sub models:P (p, z, v, A, S,D) ?Pz(z|p,A, S)Pv(v|p,A, S)Pd(D|p, v, A, S)P (p,A, S) (2)We do not take the last term P (p,A, S) into con-sideration, since it is constant for the optimization.In the next sections, we describe how these proba-bilities Pz, Pd, and Pvare computed.2Generally, an argument modifies a predicate, but in rela-tive clauses, a predicate modifies an argument3.1.1 Zero subject model: Pz(z|p,A, S)This model estimates the syntactic zero subject3of the predicate p. For instance, z= Imeans that thesubject of p is omitted and its type is first person.z=imperative means that we do not need to aug-ment a subject because the predicate is imperative.z=already exists means that a subject already ap-pears in the sentence.
A maximum entropy classi-fier is used in our zero subject model, which takesthe contextual features extracted from p, A, and S.3.1.2 Voice type model: Pv(v|p,A, S)This model estimates the voice type of a predicate.We also use a maximum entropy classifier for thismodel.
This classifier is used only when the predi-cate has the ambiguous suffix reru or rareru.
If thepredicate does not have any ambiguous suffix, thismodel returns pre-defined voice types with withvery high probabilities.3.1.3 Deep case model: Pd(D|p, v,A, S)This model estimates the deep syntactic role be-tween a predicate p and its arguments A. Thismodel helps to resolve the deep cases when theirsurface cases are topic.
We define Pdas followsafter introducing an independent assumption overpredicate-argument structures:P (D|p, v, A, S) ?
?i[max(p(di|ai, p) ?
m(si, di, v), ?
)].p(d|a, p) models the deep relation between p anda.
We use a maximum likelihood estimation forp(d|a, p):p(d = subj|a, p) =freq(s = ga, a, active form of p)freq(a, active form of p)p(d = obj|a, p) =freq(s = wo, a, active form of p)freq(a, active form of p),where freq(s = ga, a, active form of p) is thefrequency of how often an argument a and p ap-pears with the surface case ga.
The frequenciesare aggregated only when the predicate appear inactive voice.
If the voice type is active, we cansafely assume that the surface cases ga and wocorrespond to subject and object respectively.
Wecompute the frequencies from a large amount ofauto-parsed data.m(s, d, v) is a non-negative penalty variable de-scribing how the deep case d generates the sur-face case s depending on the voice type v. Since3Here syntactic subject means the subject which takes thevoice type into account.559the number of possible surface cases, deep cases,and voice types are small, we define this penaltymanually by referring to the Japanese grammarbook (descriptive grammar research group, 2009).We use these manually defined penalties in orderto put more importance on syntactic preferencesrather than those of semantics.
Even if a predicate-augment structure is semantically irrelevant, wetake this structure as long as it is syntactically cor-rect in order to avoid SMT from generating liberaltranslations.?
is a very small positive constant to avoid zeroprobability.3.2 Joint inference with linguistic constraintsOur initial model (2) assumes that zero subjectsand deep cases are generated independently.
How-ever, this assumption does not always capturereal linguistic phenomena.
English is a subject-prominent language in which almost all sentences(or predicates) must have a subject.
This impliesthat it is more reasonable to introduce strong lin-guistic constraints to the final solution for pre-ordering, which are described as follows:?
Subject is a mandatory role.
A subject mustbe inferred either by zero subject or deep casemodel4.
When the voice type is passive, anobject role in D is considered as a syntacticsubject.?
A predicate can not have multiple subjectsand objects respectively.These two constraints avoid the model from in-ferring syntactically irrelevant solutions.In order to find the result with the constraintsabove, we formalize our model as an integer lin-ear programming, ILP.
Let {x1, , ..., xn} be bi-nary variables, i.e., xi?
{0, 1}.
xicorrespondsto the binary decisions in our model, e.g., xk=1 if di= subj and v = active.
Let {p1, ..., pn} beprobability vector corresponding to the binary de-cisions.
ILP can be formalized as a mathematicalproblem, in which the objective function and theconstraints are linear:{x?1, ..., x?n} = argmax{x1,...,xn}?{0,1}nn?i=1log(pi)xis.t.
linear constraints over {x1, .., xn}.After taking the log of (2), our optimization modelcan be converted into an ILP.
Also, the constraints4imperative is also handled as an invisible subjectdescribed above can be represented as linear equa-tions over binary variablesX .
We leave the detailsof the representations to (Punyakanok et al, 2004;Iida and Poesio, 2011).3.3 Japanese pre-ordering with deep parserWe use a simple rule-based approach to make pre-ordered Japanese sentences from our deep parsetrees, which is similar to the algorithms describedin (Komachi et al, 2006; Katz-Brown and Collins,2008; Hoshino et al, 2013).
First, we naively re-verse all the bunsetsu-chunks5.
Then, we movea subject chunk just before its predicate.
Thisprocess converts SOV to SVO.
When the subjectis omitted, we generate a subject with our deepparser and insert it to a subject position in thesource sentence.
There are three different waysto generate a subject.1.
Generate real Japanese words (Insert ?
(I),???
(you).. etc)2.
Generate virtual seed Japanese words (Insert1st person, 2nd person..., which are not inthe Japanese lexicon.)3.
Generate only a single virtual seed Japaneseword regardless of the subject type.
(Insertzero subject)1) is the most aggressive method, but it causescompletely incorrect translations if the detectionof subject type fails.
2) and 3) is rather conser-vative, since they leave SMT to generate Englishpronouns.We decided to use the following hybrid ap-proach, since it shows the best performance in ourpreliminary experiments.?
In the training of SMT, use 3).?
In decoding, use 1) if the input sentence onlyhas one predicate.
Otherwise, use 3).3.4 Examples of parsing resultsTable 2 shows examples of our deep parser output.It can be seen that our parser can correctly identifythe deep case of topic case markers wa.5bunsetsu is a basic Japanese grammatical unit consistingof one content word and functional words.560Table 2: Examples of deep parser output???
(today wa) {d=other} ??
(liquor ga) {d=obj} ???
(can drink) {v=potential, z=I}?????
(news ga) {d=subj} ?????
(was broadcast) {v=passive, z=already exist}????
(pasta wa) {d=obj} ??????
(ate+question) {v=active, z=you}????
(you wa) {d=subj} ??????
(ate+question) {v=active, z=already exist}4 Experiments4.1 Experimental settingsWe carried out all our experiments using a state-of-the-art phrase-based statistical Japanese-to-English machine translation system (Och, 2003)with pre-ordering.
During the decoding, weuse the reordering window (distortion limit) to 4words.
For parallel training data, we use an in-house collection of parallel sentences.
These comefrom various sources with a substantial portioncoming from the web.
We trained our system onabout 300M source words.
Our test set containsabout 10,000 sentences randomly sampled fromthe web.The dependency parser we apply is an imple-mentation of a shift-reduce dependency parserwhich uses a bunsetsu-chunk as a basic unit forparsing (Kudo and Matsumoto, 2002).The zero subject and voice type models weretrained with about 20,000 and 5,000 manually an-notated web sentences respectively.
In order tosimplify the rating tasks for our annotators, we ex-tracted only one candidate predicate from a sen-tence for annotations.We tested the following six systems.?
baseline: no pre-ordering.?
surface reordering : pre-ordering only withsurface dependency relations.?
independent deep reordering: pre-orderingusing deep parser without global linguisticconstraints.?
independent deep reordering + zero sub-ject: pre-ordering using deep parser and zerosubject generation without global linguisticconstraints.?
joint deep reordering: pre-ordering usingour new deep parser with global linguisticconstraints.?
joint deep reordering + zero-subject: pre-ordering using deep parser and zero subjectgeneration with global linguistic constraints.Table 3: Results for different reordering methodsSystem BLEU RIBESbaseline (no reordering) 16.15 52.67surface reordering 19.39 60.30independent deep reordering 19.68 61.27independent deep reordering + zero subj.
19.81 61.67joint deep reordering 19.76 61.43joint deep reordering + zero subj.
19.90 61.89As translation metrics, we used BLEU (Pap-ineni et al, 2002), as well as RIBES (Isozaki etal., 2010a), which is designed for measuring thequality of distant language pairs in terms of wordorders.4.2 ResultsTable 3 shows the experimental results for six pre-reordering systems.
It can be seen that the pro-posed method with deep parser outperforms base-line and naive reordering with surface syntactictrees.
The zero subject generation can also im-prove both BLEU and RIBES scores, but the im-provements are smaller than those with reordering.Also, joint inference with global linguistics con-straints outperforms the model which solves deepsyntactic analysis and zero subject generation in-dependently.5 ConclusionsIn this paper, we proposed a simple joint inferenceof deep case analysis and zero subject generationfor Japanese-to-English SMT.
Our parser consistsof pointwise probabilistic models and a global in-ference with linguistic constraints.
We applied ournew deep parser to pre-ordering in Japanese-to-English SMT system and showed substantial im-provements in automatic evaluations.Our future work is to enhance our deep parser sothat it can handle other linguistic phenomena, in-cluding causative voice, coordinations, and objectellipsis.
Also, the current system is built on thetop of a dependency parser.
The final output of ourdeep parser is highly influenced by the parsing er-rors.
It would be interesting to develop a full jointinference of dependency parsing and deep syntac-tic analysis.561ReferencesJapan descriptive grammar research group.
2009.
Con-temporary Japanese grammar book 2.
Part 3.
Caseand Syntax, Part 4.
Voice.
Kuroshio Publishers.Xia Fei and McCord Michael.
2004.
Improving a sta-tistical mt system with automatically learned rewritepatterns.
In Proc.
of ACL.Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, andBenjamin K Tsou.
2013.
Overview of the patentmachine translation task at the ntcir-10 workshop.In Proc.
of NTCIR.Sho Hoshino, Yusuke Miyao, Katsuhito Sudoh, andMasaaki Nagata.
2013.
Two-stage pre-orderingfor japanese-to-english statistical machine transla-tion.
In Proc.
IJCNLP.Ryu Iida and Massimo Poesio.
2011.
A cross-lingualilp solution to zero anaphora resolution.
In Proc.
ofACL.Hideki Isozaki, Tsutomu Hirao, Kevin Duh, KatsuhitoSudoh, and Hajime Tsukada.
2010a.
Automaticevaluation of translation quality for distant languagepairs.
In Proc.
of EMNLP.
Association for Compu-tational Linguistics.Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, andKevin Duh.
2010b.
Head finalization: A simple re-ordering rule for sov languages.
In Proc.
of the JointFifth Workshop on Statistical Machine Translationand MetricsMATR.Jason Katz-Brown and Michael Collins.
2008.
Syntac-tic reordering in preprocessing for japanese ?
en-glish translation: Mit system description for ntcir-7 patent translation task.
In Proc.
of the NTCIR-7Workshop Meeting.Mamoru Komachi, Masaaki Nagata, and Yuji Mat-sumoto.
2006.
Phrase reordering for statistical ma-chine translation based on predicate-argument struc-ture.
In Proc.
of the International Workshop on Spo-ken Language Translation.Taku Kudo and Yuji Matsumoto.
2002.
Japanesedependency analysis using cascaded chunking.
InProc.
of CoNLL.Uri Lerner and Slav Petrov.
2013.
Source-side classi-fier preordering for machine translation.
In Proc.
ofEMNLP.Graham Neubig, Taro Watanabe, and Shinsuke Mori.2012.
Inducing a discriminative parser to optimizemachine translation reordering.
In Proc.
of EMNLP.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
of ACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proc.
of ACL.Vasin Punyakanok, Dan Roth, Wen-tau Yih, and DavZimak.
2004.
Semantic role labeling via integerlinear programming inference.
In Proc.
of ACL.Hirotoshi Taira, Katsuhito Sudoh, and Masaaki Na-gata.
2012.
Zero pronoun resolution can improvethe quality of je translation.
In Proc.
of Workshop onSyntax, Semantics and Structure in Statistical Trans-lation.Xianchao Wu, Katsuhito Sudoh, Kevin Duh, HajimeTsukada, and Masaaki Nagata.
2011.
Extractingpre-ordering rules from predicate-argument struc-tures.
In Proc.
of IJCNLP.562
