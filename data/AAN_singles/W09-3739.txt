Proceedings of the 8th International Conference on Computational Semantics, pages 338?341,Tilburg, January 2009. c?2009 International Conference on Computational SemanticsAn Application of Lexical Semantics Annotation toQuestion-Answering in e-FarmingMukda Suktarachan (1), Patrick Saint-Dizier (2)(1) NAIST, Kasetsart University, Bangkok, Thailand(2) IRIT, Toulouse, Francenaist da da@yahoo.com, stdizier@irit.frAbstractIn this poster we present an approach to responding to complexquestions in the agriculture domain, from specifications given by ex-perts.
We present in particular a semantic annotation procedure thatwould allow us to define accurate and domain dedicated forms of lexicalsemantics inference, in order to be able to match non factoid questions(i.e.
questions whose response is a significant text portion) with doc-uments on a large scale.
This project is designed to help farmers toget advices via question answering on SMS in order to improve ricefarming.1 Challenges and GoalsQuestion answering (Moldovan 2000, Maybury 2004) operates on top ofsearch engines of classical textual database querying tools, by providing alayer that has natural language understanding and generation as well somereasoning capabilities in order to provide users with responses which aremuch more accurate and cooperative than what search engines provide ingeneral.
This is particularly crucial when responses are not straightforward,e.g.
when they require some form of elaboration (synthesis of data, consis-tency checking, etc.
), reasoning or when the response is not a simple item,but a well-formed fragment of text, e.g.
a chain of events leading to a con-sequence, a procedure, etc.The project we present here emerged from a need from the Thai Ministryof Agriculture.
The main goal is to develop tools for e-Farming, in particularrice farming, so that farmers can easily get information on farming rice andrice diseases, for example via SMS.
The Thai Ministry of Agriculture has338large text databases on the way rice can be planted, on how to prepare andfertilize soils and on the numerous diseases rice may be subject to, the ef-fects, the treatments, etc.
Question answering is a particularly well-adaptedapproach to allow farmers to query in Thai (via SMS short messages) suchdatabases.The NAIST lab at the university of Kasetsart has basic tools to parseThai (stemmer, morphological analysis, part of speech recognition, and sim-ple syntactic analysis).
These tools were designed for machine translationpurposes, but they turn out to be appropriate to parse queries and to retreiveinformation in technical texts.2 Outline of the Project and MethodologyThe needs of the Thai Ministry of Agriculture have been specified in a sim-ple way via a corpus composed of (1) questions raised in real life by farmers(about 1000 questions), (2) the responses which have been provided by ex-perts based on existing documents (possibly several responses per question)and (3) the texts they originate from.
In general the response is found ina unique text: there are no multiple answers since most texts are not re-dundant, although some responses, in particular complex or indirect ones,may involve the taking into account of several independent texts.
We willnot address here the problem of message length reduction so that it fits intoSMS format (althought this is also an important semantic problem).A few examples of question-answer pairs are (glosses from Thai, butstructures are in fact quite similar to English):Q: How to prevent the Weedy rice?R1: Skip some seasons when growing rice,R2: Grow hydrotonics plants.Q: How to control the Bacterial Leaf Streak Disease?R: Do not put too much Nitrogen.Q: How to eradicate the rice thrips?R: Spray with Malathion or Carbaryl every week, add fertilizer and waterevery two days.Questions are essentially factoid questions (e.g.
products to use, best pe-riods for plantations, varieties to use, symptoms of a desease), why questionswhere responses are chains of events (reasons for something to happen) anda large number of procedural questions (Delpech et al 2008) in particularfor treating deseases.
There are no comparative or evaluative questions asin other domains.339In most cases, questions do not have responses which can be immediatelyfound in the texts.
For example: How does the Sheath Blight affects the ricegrowth?
has the following response: Plants heavily infected at these stagesproduce poorly filled grain, particularly in the lower portion of the panicle.Additional losses result from ....
Therefore some lexical semantics devicesare needed to allow appropriate question-text matching.
The second aspectof this problem is to be able to extract the complete text portion thatresponds to the question.
For that purpose we are developing an annotationmethodology whose goal is to identify the different processes at stake andthe needed resources.3 The Question-Answering Process AnnotationSince the task is quite large (a large group of students are annotating the setof 1000 questions and related texts), we need to establish norms and anno-tation guidelines.
Based on the research conducted at IRIT on annotatingprocedural questions and instructions based on semantic roles (TextCoopproject), we first annotate the questions and their corresponding responsesas provided by the ministry of agriculture.
There are many attempts toannotate arguments by means of primitives, our approach is here orientedtowards the task and the specific actions.
Therefore roles are not as stan-dard as they are in general (see IWCS 2009 Dautriche et al this volume).An earlier attempt for language generation is e.g.
(Delin 1994).
Seman-tic tags are either close to thematic roles (instrument, location, etc.)
orborrowed from the primitive systems of the Lexical Conceptual Structure(LCS), in particular to establish links between arguments, which thematicroles cannot do.
For example, in the first Thai university we have a linkbetween ?first?
and ?Thai university?
which is either loctempor loc+char+identdepending on the interpretation of first (oldest or the best).
Then, the textin which the response occurs is annotated (so that the boundaries and sur-rounding elements of the response can be characterized) and the underlyinglexical semantics mechanisms at stake are given.Let us present here an illustrative example:Q: How can thrips destroy the rice ?annotation:<question type=?manner?
> How can <agent> thrips </agent> <action>destroy <theme> the rice </theme> </action> ?
</question>The response is annotated as follows:<response> <agent> The rice thrips <action> sucks the sap <source>340from the young plant.
</source> </action> </response>To match the action ?destroy?
in the question with the text portion fromwhich the response is extracted, it is then necessary to identify the inference:<lex inference> <action> Suck sap of X </action> <entail> <modality>probably </modality> <action> destroy X </action> </entail>,<type> X : plant </type><part-of> sap : X </part-of > </lex inference>This example shows that (1) in the question and in the answer, anno-tations are used to identify the different components, arguments, adjuncts,but also some other components (e.g.
temporal adverbs), and (2) the an-notation developed to characterize the matching between the question andthe answer is used to induce and develop forms of lexical inference (or otherphenomena like synonymy, lexical equivalence, etc.).
The types and lexi-cal functions which are introduced contribute to the process of induction ofgeneralizations over some semantic categories (plants, products, etc.
), andverb classes.At this level, the inferences which may be drawn are directly attachedto the terms which are tagged.
This is obviously too limited.
We are nowexprimenting different generalization levels in order to tune lexical inferencerules.
This process involves (1) generalization principles over different typesand categories (via a domain ontology), and (2) a set of principles thatlimit these generalizations via, for example, the taking into account of thesemantics restrictions imposed by lexical items, in particular verbs.
Thistask needs to be developed and evaluated gradually,; so far it is too early toevaluate the quality of the rules we get, but we believe this is a simple andclose to the data approach and it should be reproducible to other areas.This approach, and the principles we have briefly outlined, allow usto introduce a working method for the development of question-answeringsystems for concrete applications, for non-factoid questions.References[1] Delpech, E., Saint-Dizier, P., 2008, Investigating the Structure of Pro-cedural Texts for Answering How-to Questions, LREC 2008, Marrakech.
[2] Delin, J., Hartley, A., Paris, C., Scott, D., Vander Linden, K., Express-ing Procedural Relationships in Multilingual Instructions, Proc.
7th Int.Workshop on Natural Language Generation, pp.
61-70, Maine, USA, 1994.341[3] Maybury, M., New Directions in Question Answering, The MIT Press,Menlo Park, 2004.
[4] Moldovan, D., Harabagiu, S., Pasca, M., Milhacea, R., Goodrum, R.,Grju, R., Rus, V., The Structure and Performance of an Open-DomainQuestion Answering System, Proc.
38th Meeting of the ACL, Hong Kong,2000.
[5] Takechi, M., Tokunaga, T., Matsumoto, Y., Tanaka, H., Feature Se-lection in Categorizing Procedural Expressions, The Sixth InternationalWorkshop on Information Retrieval with Asian Languages (IRAL2003),pp.49-56, 2003.342
