Beyond Class A: A Proposalfor Automatic Evaluation of DiscourseLynette Hirschman, Deborah A. Dahl, Donald P. McKay,Lewis M. Norton, and Marcia C. LinebargerUnisys Defense SystemsCenter for Advanced Information TechnologyPO Box 517Paoli, PA  19301IntroductionThe DARPA Spoken Language community has justcompleted the first trial evaluation of spontaneousquery/response pairs in the Air Travel (ATIS) domain.
1Our goal has been to find a methodology for evaluatingcorrect responses to user queries.
To this end, we agreed,for the first trial evaluation, to constrain the problem inseveral ways:Database  App l i ca t ion :  Constrain the applicationto a database query application, to ease the burden of a)constructing the back-end, and b) determining correctresponses;Canon ica l  Answer :  Constrain answer comparisonto a minimal "canonical answer" that imposes the fewestconstraints on the form of system response displayed toa user at each site;Typed Input :  Constrain the evaluation to typed in-put only;Class A: Constrain the test set to single unambiguousintelligible utterances taken without context that havewell-defined atabase answers ("class A" sentences).These were reasonable constraints to impose on thefirst trial evaluation.
However, it is clear that we needto loosen these constraints to obtain a more realistic eval-uation of spoken language systems.
The purpose of thispaper is to suggest how we can move beyond evaluationof class A sentences to an evaluation of connected ia-logue, including out-of-domain queries.Analysis of the Training DataThe training data consisted of almost 800 sentences, ap-proximately 60% of which could be evaluated completelyindependent of context.
Of  the remaining sentences, ap-proximately half of them (19%) require context, and al-most that many do not have a unique database answer(17%).
Table 1 shows these figures for the four sets ofATIS training data; note that the total adds up to morethan 100% because some sentences belonged to multipleclasses.
21This work was supported by DARPA contract N000014-89-C0171, administered by the Office of Naval Research.2 This table counts the so-called context-removable sentences ascontext dependent, because the answer to such sentences changesdepending on whether context is used or not.CLASSIFICATION # %Total Sentences 774 100Pure Class A 490 63Context 145 19Unanswerable 129 17Ambiguous 42 5Ungrammatical 31 3Table 1: Classification of ATIS Training DataA Modest ProposalWe originaUy postponed evaluation of non-class A sen-tences because there was no consensus on automatedevaluation techniques for these sentences.
We wouldlike here to propose a methodology for both "unanswer-able" sentences and for automated evaluation of context-dependent sentences.
By capturing these two additionalclasses in the evaluation, we can evaluate on more than90% of the data; in addition, we can evaluate ntire (well-formed) dialogues, not just isolated query/answer pairs.Unanswerab le  Quer iesFor unanswerable queries, we propose that the systemrecognize that the query is unanswerable and generate(for evaluation purposes) a canonical answer such asUNANSWERABLE_QUERY.
This would be scoredcorrect in exactly those cases where the query is in factunanswerable.
The use of a canonical message side-stepsthe tricky issue of exactly what kind of error messageto issue to the user.
This solution is proposed in thegeneral spirit of the Canonical Answer Specification \[1\]which requires only a minimal answer, in order to im-pose the fewest constraints on the exact nature of thesystem's answer to the user.
This must be distinguishedfrom the use of NO_ANSWER,  which flags cases wherethe system does not attempt o formulate a query.
TheNO.ANSWER response allows the system to admitthat it doesn't understand something.
By contrast, theUNANSWERABLE_QUERY answer actually diag-noses the cases where the system understands the queryand determines that the query cannot be answered bythe database.109###01 Utterance: What are the flights from Atlanta to Denver on mid-day on the 5th of July?>>>D1 Display to the User:FLT CODE FLT DAY FRM TO DEPT ARRV AL FLT# CLASSES EQP MEAL STOP DC DURA102122 1234567 ATL DEN 840102123 1234567 ATL DEN 934955 DL 445 FYBM0 757 B 0 N 1951054 EA 821FYHOK 725 B 0 N 200###02 Utterance: Okay, now I would like to find flights going on to San Francisco on Mondaythe 9th of July.
*** 02 needs info from 01: Leaving from Denver.>>>D2 Display to the User:FLT CODE FLT DAY FRM TO DEPT112516 1234567 DEN SFO 1200112519 12345-7 DEN SFO 1220. .
,ARRV AL FLT# CLASSES EOP MEAL STOP DC DURA1336 UA 343 FYBMQ D8S L 0 N 1561416 CO 1295 FYqHK 733 L 0 N 176###Q3 Utterance: What would be the fare on United 343?
*** 03 needs information from previous display D2.>>>D3 Display to the User:FARE CODE FRM TO CLASS FA RESTRICT ONE WAY RND TRIP7100247 DEN SFO F $488.00 $976.00 ...###04 Utterance: What about Continental 12957*** 04 needs display from D2 and query from q3.Figure 1: Using Context to Understand QueriesCapturing the ContextThe major obstacle to evaluation of context-dependentsentences i how to provide the context required for un-derstanding the sentences.
If each system were ableto replicate the context in which the data is collected,it should be possible to evaluate context-dependentqueries.
This context (which we will call the "canoni-cal context") consists of the query-answer pairs seen bythe subject up to that point during data collection.
Fig-ure 1 shows the kind of context dependencies that arefound in the ATIS corpus.These examples how how contextual information isused.
Query 2 (...
I would like to find flights going on toSan Francisco on Monda~t he 9th of July) requires theprevious query Q1 to determine that the starting pointof this leg is Denver.
Query 3 (What would be the fareon United 3~37) refers to an entity mentioned in theanswer of Query 2, namely United 343.
United 343 maywell include several egs, flying from Chicago to Den-ver to San Francisco, for example, with three fares forthe different segments (Chicago to Denver, Chicago toSan Francisco, and Denver to San Francisco).
However,Query 3 depends on context from the previous display tofocus only on the fare from Denver to San Francisco.
Fi-nally, Query 4 (What about Continental 1~g57) requiresthe previous query Q3 and its contezt o establish what isbeing asked about (fare from Denver to San Francisco);it also refers to an entity mentioned in the display D2associated with Query 2 (Continental 1295).
By  build-ing up a context using information from both the queryand the answer, it is possible to interpret these queriescorrectly.
This is shown schematically in Figure 2.Keeping in SynchIn Figure 3, we show an example of what can happenwhen context is not properly taken into account.
Thisi i 0SL SYSTEM DB~ ~ ~ ~ CAS\[ CONTE ~i~ i !
\ [~ i i i~~ " UsSERA y j lPROCESSING THE USER QUERYFigure 2: Current Handling of Context in PUNDITpoints out an additional difficulty in evaluating sentencesdependent on context, namely the possibility of "gettingout of synch".
In this example, the system misprocessesthe original request, saying that there are no flights fromAtlanta to Denver leaving before 11.
When the follow-upquery asks Show me the cheapest one, there is an appar-ent incoherence, since there is no "cheapest" one in theempty set.
However, if the canonical query/answer pairsare provided during evaluation, the system can "resyn-chronize" to the information originally displayed to theuser and thus recognize that it should chose the cheapestflight from the set given in the canonical answer.Providing the Canonical ContextThe above examples illustrate what information isneeded in order to understand queries in context.
Thenext question is how to provide this "canonical context"(consisting of the query/answer pairs generated uringdata collection) for purposes of automated evaluation.Providing the set of queries is, of course, not a prob-lem: this is exactly the set of input data.
a Providingthe canonical answers is more of a problem, because itrequires each system to reproduce the answer displayedduring data gathering.
Since there is no agreement asto what constitutes the best way to display the data, re-quiring that each system reproduce the original displayseems far too constraining.
However, we can provide,for evaluation purposes, the display seen by the subjectduring data collection.
The log file in the training datacontains this information in human-readable form.
Itcan be provided in more convenient form for automaticprocessing by representing the display as a list of lists,where the first element in the list is the set of columnheadings, and the remaining elements are the rows ofdata.
This "canonical display format" is illustrated inFigure 4.For evaluation, the canonical (transcribed) query andthe canonical display would be furnished with each30f  course, if the input is speech data, then the system couldmisunderstand the speech data; therefore, to preserve synchroniza-tion as much as possible, we propose that the transcribed input beprovided for evaluation of speech input.DISPLAY SHOWN TO USER:FLT CODE FLT DAY FRM TO DEPT .
.
.102122 1234567 ATL DEN 840 .
.
.102123 1234567 ATL DEN 934 .
.
.
* * .CANONICAL DISPLAY(('FLT CODE' 'FLT DAY' 'FRM' 'TO' 'DEPT'( 102122 1234567 ATL DEN 840. .
.
)Figure 4: Canonical Display FormatD O II O Iquery, to provide the full context o the system, allowingit to "resynchronize" at each step in the dialogue.
4 Thesystem could then process the query (which creates anycontext associated with the query) and answer the query(producing the usual CAS output).
It would then resetits context o the state before query processing and addthe "canonical context" from the canonical query andfrom the canonical display, leaving the system with theappropriate context to handle the next query.
This isillustrated in Figure 5.This methodology allows the processing of an entiredialogue, even when the context may not be from the di-rectly preceding query, but from a few queries back.
AtUnisys, we have already demonstrated the feasibility ofsubstituting an "external" DB answer for the internallygenerated answer \[3\].
We currently treat the display(that is, the set ofDB tuples returned) as an entity avail-able for reference, in order to capture answer/questiondependencies, as illustrated in Figure 3.4There is still the possibility that the system mlslnterprets hequery and then needs to use the query as context for a subsequentquery.
In thls case, providing the answer may not help, unlessthere is some redundancy between the query and the answer.i i iUSER:  Show me al l  f l ights  f rom At lanta  to  Denver  leav ing  be fore  11.SYSTEM ANSWER (Wrong) :NO INFORMATION SATISFIES YOUR REQUESTCORRECT ANSWER:FLT CODE FLT DAY FRM TO DEPT AREV AL FLT# CLASSES EQP MEAL STOP DC DURA.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.102122 1234567 ATL DEN 840 955 DL 445 FYBMQ 757 S/B 0 N 195102123 1234567 ATL DEN 934 1054 EA 821 FYEQK 72S S/B 0 g 200. o oFol low-up Query:USER:  Show me the cheapest one.Synchronization lost; can regain with canonical display!Figure 3: Example of Losing SynchronizationAmbiguous  Quer iesIn addition to the suggestions for handling unanswerablequeries and context-dependent queries, there seems tobe an emerging consensus that ambiguous queries canbe handled by allowing any of several possible answersto be counted as correct.
The system would then beresynchronized as described above, to use the canonicalanswer furnished during data collection.Eva luat ion  FormatTaking the need for context into consideration and theneed to allow systems to resynchronize as much as pos-sible, the proposed form of test input for each utterancein a dialogue is:?
INPUT during TESTING- Digitized speech- Canonical query for synchronization- Canonical display for synchronization?
OUTPUT during TEST ING- Transcription- CAS (with UNANSWERABLE responses)For evaluation, the system still outputs a transcrip-tion and an answer in CAS format; these are evaluatedagainst he SNOR transcription and the reference answerin CAS, as is done now.With each utterance, the system processes the utter-ance, then is allowed to "resynchronize" against he cor-rect question-answer pair, provided as part of the evalu-ation input data before evaluating the next utterance.Is It Too Easy To Cheat.
*One obvious drawback of this proposal is that it makesit extremely easy to cheat - the user is provided withthe transcription and the database display.
It is clearlyeasy to succumb to the temptation to look at the answer- but it is easy to look at the input sentences underthe current system; only honesty prevents us from doingthat.
Providing a canonical display raises the possibilityof deriving the correct answer by a simple reformattingof the canonical display.
However, it would be easy toprevent this simple kind of cheating by inserting extratuples or omitting a required tuple from the canonicaldisplay answer.
This would make any answer derivedfrom the display not compare correctly to the canonicalanswer.
In short, the issue of cheating does not seem likean insurmountable obstacle: we are now largely on thehonor system, and if we wished to make it more difficultto cheat, it is not difficult to think of minor alterationsthat would protect he system from obvious mappings ofinput to correct answer.Evaluating Whole  DiscoursesThere are several arguments in favor of moving beyondclass A queries:?
Yield is increased from 60% to over 90%;?
Data categorization is easier (due to elimination ofthe context-removable c ass);?
Data validation is easier (no need to rerun context-removable queries);?
Data from different data collection paradigms canbe used by multiple sites;?
We address a realistic problem, not just an artificialsubset.This is particularly important in light of the resultsfrom the June evaluation.
In general, systems performedin the 50-60% range on class A sentences.
This meansthat the coverage of the data was in the 30-40% range.112ERY }SL SYSTEMCONTEXT ~QIFD11DB%SYNCHRONIZING THE CONTEXTFigure 5: Updating the Context via Canonical Query and DisplayIf we move on to include unanswerable queries and con-text dependent queries, we are at least looking at morethan 90% of the data.
Given that several sites alreadyhave the ability to process context-dependent material(\[4\], \[6\], \[3\]), this should enable contractors to reportsignificantly better overall coverage of the corpus.Subjective Evaluation CriteriaIn addition to these fully automated evaluation criteria,we also propose that we include some subjective valua-tion criteria, specifically:?
User Satisfaction?
Task Completion Quality and TimeAt the previous meeting, the MIT group reported onresults using outside evaluators to assess system perfor-mance (\[5\]).
We report on a similar experiment at thismeeting(\[2\]), in which three evaluators howed good re-liability in scoring correct system answers.
This indi-cates that subjective black box evaluation is a feasibleapproach to system evaluation.
Out suggestion is thatsubjective valuation techniques be used to supplementand complement the various automated techniques un-der development.ConclusionThis proposal does not address everal important issues.For example, clearly a useful system would move towardsan expert system, and not remain restricted to a DB in-terface.
We agree that this is an important direction,but have not addressed it here.
We also agree with ob-servations that the Canonical Answer hides or conflatesinformation.
It does not capture the notion of focus, forexample.
And we have explicitly side-stepped the dif-ficult issues of what kind of detailed error messages asystem should provide, how it should handle failed pre-supposition, how it should respond to queries outsidethe DB.
For the next round, we are suggesting that itis sufficient to recognize the type of problem the sys-tem has, and to supplement the objective measures withsome subjective measures of how actual users react tothe system.References\[1\] Sean Boisen, Lance Ramshaw, Damaris Ayuso, andMadeleine Bates.
A Proposal for SLS EvaluationIn Proceedings of the DARPA Speech and NaturalLanguage Workshop, Cape Cod, MA, October 1989.\[2\] Deborah A. DaM, Lynette Hirschman, Lewis M.Norton, Marcia C. Linebarger, David Magerman,Nghi Nguyen, and Catherine N. Ball.
Training andevaluation of a language understanding system fora spoken language application.
In Proceedings ofthe Darpa Speech and Language Workshop, HiddenValley, PA, June 1990.\[3\] Lewis M. Norton, Deborah A. Dahl, LynetteHirschman, Marcia C. Linebarger, and Catherine N.Ball.
Management and evaluation of interactive di-aiog in the air travel domain.
In Proceedings of theDarpa Speech and Language Workshop, Hidden Val-ley, PA, June 1990.\[4\] Wayne Ward.
The CMU Air Travel Informa-tion Service: Understanding Spontaneous SpeechIn Proceedings of the Darpa Speech and LanguageWorkshop, Hidden Valley, PA, June 1990.\[5\] Victor Zue, James Glass, David Goodine, HongLeung, Michael Phillips, Joseph Polifroni, andStephanie Seneff.
Preliminary evaluation of the voy-ager spoken language system.
In Proceedings of theDARPA Speech and Natural Language Workshop,Cape Cod, MA, October 1989.\[6\] Victor Zue, James Glass, David Goodine, HongLeung, Michael Phillips, Joseph Polifroni, andStephanie Seneff.
Preliminary ATIS Development aMIT In Proceedings of the DARPA Speech and Nat-ural Language Workshop, Hidden Valley, PA, June,1990.113
