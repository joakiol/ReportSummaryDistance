Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 71?81, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsGenerative Goal-Driven User Simulation for Dialog ManagementAciel EshkyILCCSchool of InformaticsUniversity of Edinburgha.eshky@sms.ed.ac.ukBen AllisonILCCSchool of InformaticsUniversity of Edinburgh{ballison, steedman}@inf.ed.ac.ukMark SteedmanILCCSchool of InformaticsUniversity of EdinburghAbstractUser simulation is frequently used to trainstatistical dialog managers for task-orienteddomains.
At present, goal-driven simula-tors (those that have a persistent notion ofwhat they wish to achieve in the dialog) re-quire some task-specific engineering, makingthem impossible to evaluate intrinsically.
In-stead, they have been evaluated extrinsicallyby means of the dialog managers they are in-tended to train, leading to circularity of argu-ment.
In this paper, we propose the first fullygenerative goal-driven simulator that is fullyinduced from data, without hand-crafting orgoal annotation.
Our goals are latent, and takethe form of topics in a topic model, clusteringtogether semantically equivalent and phoneti-cally confusable strings, implicitly modellingsynonymy and speech recognition noise.
Weevaluate on two standard dialog resources,the Communicator and Let?s Go datasets, anddemonstrate that our model has substantiallybetter fit to held out data than competing ap-proaches.
We also show that features derivedfrom our model allow significantly greater im-provement over a baseline at distinguishingreal from randomly permuted dialogs.1 IntroductionAutomatically simulating user behaviour in human-machine dialogs has become vital for training sta-tistical dialog managers in task-oriented domains.These managers are often trained with some vari-ant of reinforcement learning (Sutton and Barto,1998), where optimal behaviour is sought or learntthrough the exploration of the space of possible di-alogs.
Although learning by interacting with humansubjects is a possibility (Gas?ic?
et al2011), it hasbeen argued that user simulation avoids the expen-sive, labour intensive, and error-prone experience ofexposing real humans to fledgling dialog systems(Eckert et al1997).Training effective dialog managers should benefitfrom exposure to properties exhibited by real users.Table 1 shows an example dialog in a domain suchas we consider, where the objective is to simulate atthe semantic level.
In such task oriented domains,the user has a goal (in this case, to book a flightfrom New York to Osaka), and the machine is taskedwith fulfilling it.
Notice that the user is consistentwith this goal throughout the dialog, in that they donot provide contradictory information (although anASR error is present), but that every mention of theirdestination city uses a different string.
This moti-vates our first desideratum: that simulation be con-sistent over the course of a dialog.
Furthermore, onecan imagine users not always responding identicallyin identical situations: we thus additionally requirevariability.
In this paper we demonstrate a fully gen-erative, latent variable probability model exhibitingboth of these properties.Thus far, consistent simulators have been par-tially deterministic and have required some hand-engineering.
As a result, it has only been possible toevaluate them extrinsically using dialog managers.This is circular because we need simulators to trainmanagers, but need managers to evaluate simulators.The issue is that judgements of quality of each de-pend on the specifics of the other and that a properevaluation of one depends on the correct function-ing of the other.
Furthermore, there is little reason toassume that because a simulator performs well witha certain dialog manager, it would perform similarly71Speech Semantic RepresentationM: Hello, How Can I help?
M: GREETINGM: META REQUEST INFOU: A trip from New York City to Osaka, U: PROVIDE orig city New York Cityplease.
U: PROVIDE dest city Salt Lake CityM: Leaving from New York City to Salt Lake M: IMPLICIT CONFIRM orig dest cityCity.
What day would you like to travel?
M: REQUEST depart dateU: No, no.
Leaving from New York to Osaka U: NO ANSWER null noin Japan.
U: PROVIDE orig city New YorkU: PROVIDE dest city Osaka JapanM: Leaving from New York to Osaka Japan, M: EXPLICIT CONFIRM orig citycorrect?
M: EXPLICIT CONFIRM dest cityU: Yes.
U: YES ANSWER null yesTable 1: An example of a dialog in speech and its semantic equivalent.
M and U denote machine and user utterancesrespectively.
Note how a single speech utterance is split by the semantic parser into multiple logical utterances, eachof which is broken down to an ACT, slot, and value.
We consider resources where gold standard transcriptions are notavailable; thus there will be speech recognition noise, e.g.
Osaka rendered as Salt Lake City, something our modelis able to capture.well with other managers.
In contrast, a probabilisticformulation such as we propose allows us to evalu-ate our models intrinsically using standard machinelearning metrics, and without reference to a specificmanager, thus breaking the circularity, and guardingagainst such experimental biases.We demonstrate the efficacy of our model ontwo tasks, and compare it to two other approaches.Firstly we use a standard bigram model as conceivedby Eckert et al1997) and Levin and Pieraccini(2000); secondly we compare to a probabilistic goal-based simulator where the goals are string literals,as envisaged by Scheffler and Young (2002) andSchatzmann et al2007b).
We demonstrate sub-stantial improvement over these models in terms ofpredicting heldout data on two standard dialog re-sources: DARPA Communicator (Levin et al2000;Georgila et al2005b) and Let?s Go (Black and Es-kenazi, 2009).2 Related Work2.1 Related Work on User SimulationUser simulation as a stochastic process was first en-visioned by Eckert et al1997): their Bigram modelconditions user utterances exclusively on the pre-ceding machine utterance.
This was extended byLevin and Pieraccini (2000), who manually restrictthe model to estimating ?sensible?
pairs of user andmachine utterances by assigning all others probabil-ity zero.Bigram models ensure that a locally sensibleresponse to a machine utterance is provided bythe simulator; however, they do not ensure thatit provides responses consistent with one anotherthroughout the dialog.
Several approaches have at-tempted to overcome this problem.
Pietquin (2004),for example, explicitly models a user goal as a setof slot-value pairs randomly generated once per dia-log.
He then hand selects parameters to ensure thatthe user?s actions are in accordance with their goal.Jung et al2009) use large amounts of dialogstate annotations (e.g.
what information has beenprovided so far) to learn Conditional Random Fieldsover the user utterances, and assume that those fea-tures ensure user consistency.
Georgila et al2005a)instead consider only act-slot pairs, and thus incon-sistency is not a factor.Scheffler and Young (2002) simulate user be-haviour by introducing rules for actions that dependon the user goal, and probabilistic modelling for ac-tions that are not goal-dependent.
They then mapout a decision network that determines user actionsat every node prior to the start of the dialog.
Agenda-based user simulation, another approach from the lit-erature, assumes a probability distribution over the72user goal which is either induced from data (Schatz-mann et al2007b), or is manually set when no datais available (Schatzmann et al2007a).
An agenda,which is a stack-like structure of utterances to beproduced given the goal, is then devised determin-istically.
Keizer et al2010) combine the decisionnetwork with the agenda and goal to allow for somevariability for some actions.
These models ensureconsistency but restrict the variability in user be-haviour that can be accommodated.
Furthermore,because these approaches do not define a completeprobability distribution over user behaviour, they re-strict possibilities for their evaluation, a point towhich we now turn.2.2 Related Work on Simulator EvaluationNo standardised metric of evaluation has been estab-lished for user simulators largely because they havebeen so inextricably linked to dialog managers.
Themost popular method of evaluation relies on gener-ating synthetic dialogs through the interaction of theuser simulator with some dialog manager.
Schatz-mann et al2005) hand-craft a simple determin-istic dialog manager based on finite automata, andcompute similarity measures between these synthet-ically produced dialogs and real dialogs.
Georgilaet al2006) use a scoring function to evaluate syn-thetic dialogs using accuracy, precision, recall, andperplexity, while Schatzmann et al2007b) relyon dialog completion rates.
Williams (2008) usea Cramer?von Mises test, a hypothesis test to de-termine whether simulated and real dialogs are sig-nificantly different, while Janarthanam and Lemon(2009) use Kullback Leibler Divergence between theempirical distributions over acts in real and simu-lated dialogs.
Singh et al2000) and Ai and Lit-man (2008) judge the consistency of human qualityranked synthetic dialogs generated by different sim-ulators interacting with the IT-SPOKE dialog sys-tem.Schatzmann et al2007b) use a simulator to traina statistical dialog manager and then evaluate thelearned policy.
Because this only indirectly evalu-ates the simulator, it is inappropriate as a sole mea-sure of quality.There has been far less evaluation of simulatorswithout a dialog manager.
The main approach isto compute precision and recall on an utterance ba-sis, which is intended to measure the similarity be-tween real user responses in the corpora and simu-lated user responses produced under similar circum-stances (Schatzmann et al2005; Georgila et al2006).
However, this is a harsh evaluation as it as-sumes a correct or ?best?
answer, and penalises validvariability in user behaviour.3 Dialog as a Statistical ProcessWe consider a dialog to be a series of turns, com-prised of multiple utterances.
Each Utterance con-sists of an ACT, a slot, and a value, as shown in Ta-ble 1.
Dialogs proceed by the user and the machinealternating turns.
Because the dialogs are of mixedinitiative, there is no restriction on the number ofcontiguous machine or user utterances.Our aim is to model the user, and are interestedin the conditional distribution of the user utterancesgiven the dialog up to that point.
In other words, weare interested in the distribution p (ui|d1 .
.
.
di?1),where dn is either a machine utterance mn or a userutterance un.4 Models of Users in DialogsThis section describes several models of increas-ing complexity: a Bigram model, which serves asa baseline; an upper-bound on String-Goal models,which we design to mimic the behaviour of previousgoal-based approaches, but with a probabilistic for-mulation; and finally our approach, the Topic-Goalmodel.4.1 Bigram ModelThe simplest model we define over dialogs is the bi-gram model of Eckert et al1997):p (ui|m) = p (ui|mi?1) (1)p (u|m) =?ip (ui|m) (2)The probability of each user utterance ui (the com-plete {ACT, slot, value} triple) is dependent only onthe machine utterance immediately preceding it (theslight abuse of notationmi?1 here does not mean theutterance at i?1 in the machine utterance list, but theutterance immediately preceding the i-th), and utter-ances in the dialog are conditionally independent of73one another.
(Georgila et al2006) found no bene-fit from increasing the Markov horizon).
Since eachutterance is generated independently of others in thedialog with the same context, there is no enforcedconsistency between utterances.Since we require a distribution over all possibleutterances, assigning non?zero probability to casesoutside of the training data, our bigram model is in-terpolated with a unigram model, which itself is in-terpolated with a smoothing model which assumesindependence between the act, slot, and value el-ements of the utterance.
Interpolation weights areset to maximise probability of a development set ofdialogs.
Each sub-model uses the maximum like-lihood estimator (the relative frequency of the ut-terance), and unseen machine utterances place fullweight on the unigram/smoothed model (ignoringthe bigram probability since it has no meaning ifmi?1 is unobserved).
We label this model the Bi-gram model in subsequent experiments.4.2 Goal-Based ModelsOne way to ensure consistency and more realisticbehaviour is to have a goal for the user in the dia-log, which corresponds to values for slots required inthe problem.
For instance, they might be the originand destination cities in a flight booking domain.
Instandard machine learning terms, the goal becomesa latent variable g in a probability model.
We canthen define a distribution over utterances as:p (ui|m, g) = p (ui|mi?1, g) (3)p (u|m) =?gp (g)?ip (ui|mi?1, g) (4)4.3 An Upper-Bound on String-Goal ModelsThe simplest variant of g has string values for eachof the slots the user is required to provide in orderfor the dialog to succeed.
Thus we may have:g = [orig city: New York; dest city: Osaka]as presented in Schatzmann et al2005) and Schatz-mann et al2007b).
However, in these simulators,while the goal is probabilistic, there is no distribu-tion over utterances given the goal because utter-ances are assembled deterministically from a seriesof rule applications.
There is also no marginalisationover the goal as in (4) above.The issue with a model of user goals as stringsin this fashion is that users describe the same val-ues in multiple ways (Osaka Japan, Osaka), andspeech recognition errors corrupt consistent userinput (Osaka mis-recognised as Salt Lake City).Users also might legitimately switch their goals mid-dialog.
Inference in the model would have allowfor these possibilities: we would have to marginaliseover all possible goal switches.For the sake of comparison, we compute an upper-bound on string-goal models, which gives a flavourfor how such models would perform optimistically.The upper-bound assigns probability to dialogs asfollows: for each utterance ui if the correspondingvalue vi has been seen before in the dialog, the prob-ability used for that utterance is just p (ai, si|mi?1),that is, the probability of the act ai and slot si only;there is no penalty for repetition of the value.
If thevalue is unseen in the dialog, we use the full proba-bility of the utterance from the bigram model as de-scribed above.
This is optimistic because there is nopenalty for repeated goal changes besides that im-posed by the bigram model itself, and no penalty isimposed for choosing between previously sampledgoals as would be necessary in a probability model.Any string-based model necessarily assigns lowerprobabilities to data than the upper bound, becauseit would penalise goal changes (in a probabilisticsense; that is, there would be a term to reflect theprobability of some new goal given the old) to al-low for the discrepancy in values present in dialogs.In contrast, our upper bound does not include sucha term.
Furthermore, once multiple goal values hadbeen uttered in the dialog, we would have to sampleone to use for the next utterance, which would againincur some cost: again, we do not have such a costin our upper bound.We could in theory use an external model of noiseto account for these value discrepancies (and theASR errors we model in the next section).
However,this would further decrease the probability, as someprobability mass currently assigned to the heldoutdata would have to be reserved for the possibility ofstring renderings other than those we observe.It bears reiterating that our upper bound on string-goals is not a generative model: however, it allowsus to assign probabilities to unseen data (albeit op-timistically), and thus provides us with a point of74comparison.
Although not technically a model, werefer to this as the String-Goal model for the remain-der of the paper.4.4 Topic-Goal ModelTo motivate our proposal, consider that over thecourse of a dialog one could look at the set of allvalues used for some slot, for example the destina-tion city, as a count vector:vdest city = Salt Lake:1; Osaka:2; Osaka Japan:1The above vector may arise because the user actu-ally wants to go to Osaka, but the destination isinitially mis-recognised as Salt Lake, and the userfinally disambiguates with the addition of the coun-try.
Such situations are common in the noisy dia-log resources from which simulators are induced?however, any string-based goal will necessarily con-sider these different renderings to be different goals,and will require resampling or smoothing terms todeal with them.Our approach instead treats the count vector assamples from a topic model; that is, a mixture overmultinomial distributions.
Whilst by far the mostpopular topic model is LDA (Blei et al2003), itprovides too flexible a distribution over count vec-tors to be used with such small samples (we con-firmed the poor suitability of this model in pre-liminary experiments).
Instead we use the simplerMixture-of-Multinomials model, where the latenttopic is sampled once per dialog instead of once pervalue uttered.
We describe below how parameters tothis model are estimated, and focus for now on howthe resulting model assigns probability to dialogs.In this formulation, the latent goal for each slot,which was previously a string, now becomes an in-dicator for a topic in a topic model.
Each topic canin theory generate any string (so the model is inher-ently smoothed), but most strings in most topics willhave only the smoothing weight and most probabil-ity mass will be on a small number of highly corre-lated strings.
We treat the slots as being independentof one another in the goal, and thus:p(g) =?sp (zs) (5)Where zs is the topic indicator for some slot s. If slots has associated with it a count vector of values vs,each looking like the example above, then the distri-bution over the values used for each slot becomes:p (vs) =?zsp (zs) p (vs|zs) (6)We then define a bigram-based Act model to de-scribe the probabilities of the {ACT, slot} pairs towhich these values belong, so that:p (u|m) =?sp (zs) ?
?ip (ai, si|mi?1) p (vi|zsi)(7)In reality, some slots will not have correspondingvalues, or will be slots whose values are not appro-priate to model in the above way.
Dates and times,for example, have ordinal and structural relations be-tween them, and a model which treats them as dis-connected entities is inappropriate.
For utterancesdefined over such slots we use a standard bigrammodel as in (1), and for appropriate utterances weuse a topic-goal model as in (7).
This constitutesthe only domain knowledge necessary to adapt themodel for a new resource.
We refer to this model asthe Topic-Goal model.4.4.1 Topic Model Parameter EstimationOur topic model is a Bayesian version of theMixture-of-Multinomials model.
Under this model,each dialog has associated with it a latent variablezs for each slot s in the goal, which indicates whichtopic is used to draw the values for that slot.
Con-ditioned on z, independent samples are drawn fromthe distribution over words to which that value ofz corresponds?however, the effect in the marginaldistribution over words is to strongly prefer setswhich have co-occurred in training as these are as-signed to the same topic.Bayesian inference in mixture models has beendescribed in detail in Neal (1991) and Griffiths andSteyvers (2004), so we give only a brief account herefor our particular model.
We take r appropriately-spaced samples from a Gibbs?
sampler over the pos-terior mixture parameters ?, ?
: ?
are the word-topicparameters and ?
are the mixture proportions.
Weassume a uniform Dirichlet prior on ?
and ?, lead-ing to Dirichlet posteriors which we integrate out inthe predictive distribution over v using the standardDirichlet integral.
For each of our r samples we have75components z parameterised by ?rz (the Dirichletparameter for the z-th mixture component in the r-th sample) and ?rzj for each word j in the z-th topicfor the r-th sample.
The ?
notation indicates a sumover the corresponding index, i.e.
?r?
=?z ?rz .Then:p (v) =1|r|?r?z?rz?r?p (v|?rz) (8)p (v|?)
=?
(??)?
(??
+ v?)?j?
(vj + ?j)?
(?j)(9)This states that each of the r samples has topics zwhich are multinomial distributions with posteriorsgoverned by parameters ?rz .
For any of these top-ics, the distribution over v is as given in Equation(9) (we suppress the subscripting of ?
here for thedifferent samples and topics, since this holds what-ever its value).
The final predictive probability givenin Equation (8) averages over the samples r and thetopics z (with topics weighted by their parameters?rz).5 Experimental SetupOur experiments use two standard corpora, thefirst of which is DARPA Communicator (DC), aflight booking domain collected between 2000-2001through the interaction of real users with 10 differentsystems (Levin et al2000).
It was later automati-cally annotated by Georgila et al2005b) to includesemantic information.
The second corpora is Let?sGo (LG), years 2007, 2008, and 2009, distributed aspart of the Spoken Dialog Challenge (Black and Es-kenazi, 2009).
Let?s Go is a bus routing domain inPittsburgh collected by having the general public in-teract with the CMU dialog system to find their waythrough the city.
The dialogs in both corpora are ofmixed-initiative, having a free number of contiguoussystem and user responses.We preprocessed the corpora, converting Com-municator XML-tagged files and Let?s Go systemlog files into sequences of ACT, slot, and value ut-terances.
Table 2 gives examples.
We then dividedthe corpora into training, development and test setsas follows: Communicator contains 2285 dialogs intotal, and Let?s Go contains 17992, and in each casewe selected 80% of dialogs at random for training,10% for development, and 10% for testing.DC: PROVIDE INFO orig city BostonLG: INFORM place [departure place CMU,arrival place airport]Table 2: Example utterances from the two corpora.
Notehow in addition to the value, the Let?s Go utterances con-tain properties (departure place and arrival place).Let?s Go is a noisy corpus that contains far morespeech recognition errors than Communicator.
Inaddition, users tend to be more flexible with theirbus routes than they are with their flight destinations,and so values are a lot more varied throughout thecourse of Let?s Go dialogs than Communicator ones.Furthermore, Let?s Go semantic parses contain am-biguity not present in Communicator; the parserfails to distinguish departure from arrival places over90% of the time, and instead assigns them a genericSingle Place property.
Our current model assumesthe decisions made by the semantic parser are cor-rect.
In reality however, a better model would in-corporate potential noise in the semantic parse in ajoint model.
We defer this more complex treatmentfor future work.Free model parameters are set by a simple searchon the development set, where the objective islikelihood?for the bigram model the parameters arethe interpolation weights, and for the topic model wesearch for the number of topics and smoothing con-stant for the topic distributions.
For Let?s Go, sincewe can have multiple places provided in a single act,we treat each utterance as containing a set of valuesand build the count vector for the topic model as theunion of these sets over the whole dialog.
The slotsover which the topic model is defined for Commu-nicator are dest city and orig city (this takes into ac-count PROVIDE and REPROVIDE acts).
For Let?s Gowe derive the model over the three properties: sin-gle place, arrival place and departure place, as op-posed to the less informative slot place.6 Evaluating the SimulatorsWe evaluate each of the models in terms of the prob-ability they assign to the test data.
This metric ismore suitable than the precision and recall metricswhich have been previously used, because it ac-knowledges that, rather than each user response be-ing ?correct?
at the point which it is observed, there76Model DC(A) DC(P) LG(A) LG(P)Topic 252.78 860.2 113.45 1417.06String 270.09 1286.03 169.87 4578.23Bigram 347.88 5979.53 223.23 10125.87Act 9.56 5.2 2.77 2.34Table 3: The mean per-utterance perplexity on heldoutdata.
DC-A is all acts for Communicator, while DC-P isthe calculated on PROVIDE acts alone (the acts on whichour model is designed to improve prediction).
LG-A andLG-P have the same meaning for Let?s Go.is a distribution over possible responses.
Becausethe models we define are full probability models, weare able to compute this metric and do not need touse an arbitrarily selected dialog manager for evalu-ation.The heldout probability metric should be under-stood as a means of comparing the relative viabil-ity of different models of the same data.
Note thatwe are reporting the probability of unobserved data,rather than data from which the models were in-duced, and are thus measuring the generalisabilityof the models (in contrast, maximising the proba-bility of the training data would simply encourageoverfitting).
The absolute numbers are hard to inter-pret, as there is no hard upper bound; while it maybe appealing to think of an upper bound of 1, this isincorrect as it would imply that there was no vari-ability in the data.
However, it should be understoodthat assigning particular behaviour higher probabil-ity means that the model is more likely to exhibitit when run in simulation mode?and since the userbehaviour in question has not been seen at trainingtime, this measures the extent to which the modelshave generalised beyond the training data relative toone another.We report the mean per-utterance log probabilityof unseen data, that is, the probability of the wholeheldout corpus divided by the number of user utter-ances.6.1 ResultsFigure 1 shows the results of our evaluation.
We seethat the Bigram model is weak on both resources.The results of the String Goal model suggest that,even using the generous evaluation we do here, there20 40 60 80 100?8.0?7.5?7.0?6.5?6.0?5.5?5.0?4.5Percent of Training ExamplesMean Per?UtteranceLogProbabilityTopic (DC)String (DC)Bigram (DC)Topic (LG)String (LG)Bigram (LG)Figure 1: Heldout probability of the two resources forvarying percentages of training dialogs.
Note that whilethe percentages match across resources, Let?s Go is muchlarger and thus the absolute numbers of dialogs are differ-ent, which explains the better performance on Let?s Go.is much variability due to synonymy and recognitionerrors which string goals are unable to capture (incontrast to our Topic Goal model).
The Topic Goalmodel explains this much more easily by groupingcommonly co?occurring values into the same topic.Table 3 shows the perplexities corresponding to theperformances with 100% training data for all actsand just PROVIDE acts (perplexity is 2?lp where lp isthe log probability).
Improvements are more appar-ent when we compute the probability over PROVIDEacts alone, which the models are designed to handle.And since perplexity is not on a log scale, the differ-ences are more pronounced.
The Act model, whichis a bigram model over {ACT, slot} pairs alone ex-cluding the values, demonstrates the vast discrep-ancy in uncertainty between the full problem and thevalueless prediction problem.
We note that the per-plexity of our Act model on Communicator is com-parable to that of Georgila et al2006).6.2 Example Simulator BehaviourIn this section we give examples of our TopicGoal model simulator in generation mode, whichcorresponds to sampling from the induced model.77d zdest city [probability] proportion user utterance given topic zdest city andof samples machine utterance REQUEST INFO dest cityNorfolk Virginia [0.562] 0.264 PROVIDE INFO dest city Norfolk VirginiaNorfolk [0.234] 0.111 PROVIDE INFO dest city Norfolk1 Newark Virginia [0.088] 0.039 PROVIDE INFO dest city Newark VirginiaVirginia Beach [0.0412] 0.028 PROVIDE INFO orig city Las Vegas NevadaNewark [0.040] 0.028 NO ANSWER null no0.025 COMMAND start over start overChicago [0.350] 0.164 PROVIDE INFO dest city ChicagoChicago Illinois [0.182] 0.082 PROVIDE INFO dest city Chicago Illinois2 Duluth Minnesota [0.124] 0.057 PROVIDE INFO dest city New OrleansNew Orleans [0.122] 0.055 PROVIDE INFO dest city Duluth MinnesotaNew Orleans Louisiana [0.085] 0.039 PROVIDE INFO dest city New Orleans Louisiana0.028 NO ANSWER null noAnchorage [0.539] 0.252 PROVIDE INFO dest city AnchorageAnchorage Alaska [0.148] 0.072 PROVIDE INFO dest city Anchorage Alaska3 Jacksonville Florida [0.124] 0.056 PROVIDE INFO dest city Jacksonville FloridaGreat Anchorage Alaska [0.098] 0.048 PROVIDE INFO dest city Great Anchorage AlaskaDuluth Minnesota [0.057] 0.047 PROVIDE INFO orig city Hartford Connecticut0.026 PROVIDE INFO dest city Duluth MinnesotaTable 4: Examples of sampling from the topic goal model.
Left: top 5 strings (with probabilities) sampled from topicsfor three different dialogs d. Right: top 6 utterances (plus fraction of samples in 10,000) generated in response to themachine utterance ?REQUEST INFO dest city?
and conditioned on the topic zdest city .Our examples are drawn from the model inducedfor the Communicator data.
Sampling from stan-dard distributions can be implemented followingthe algorithms in Bishop (2006) and other statisti-cal resources.
Utterances are sampled by samplingACT, slot pairs from the distribution p (ai, si|mi?1)(drawing a value from a multinomial distribution).
Ifwe sample a PROVIDE INFO act, we check whetherwe have sampled a topic for the corresponding slotthus far in the dialog.
If not, we sample one bydrawing a topic indicator from p(zs) =?rz??
?and thendrawing a multinomial distribution over strings fromthe Dirichlet posterior corresponding to z.
Once thetopic for the slot is set, we sample values as drawsfrom the fixed multinomial and add these to the ACT,slot pair.Table 4 shows some examples drawn from themodel.
For each row in the table (corresponding to anew dialog d), we sample a topic for the dest cityand orig city as needed, and sample 10000 utter-ances given that topic.
The left hand side of the ta-ble shows the top five strings in the sampled topic,while the right hand side shows the top six utter-ances in response to REQUEST INFO dest city.
Notethat the proportion of utterances on the right doesnot match the probability of the values on the leftbecause of the presence of other user acts besidesPROVIDE dest city.7 Evaluating Model ConsistencyHaving shown in the previous section that our TopicGoal model is a much better predictor of heldoutdata than the String Goal model or Bigram model,we now turn to a demonstration of the model?s cap-turing of consistency.In the face of value synonymy and ASR errors,we define inconsistent dialogs to be ones that are lo-cally coherent but lack the structure of a real dialogfrom one turn to the next.
We then suggest that anappropriate task for consistent models is distinguish-ing between consistent and inconsistent dialogs.To test this hypothesis, we devise the followingclassification problem: can we discriminate between78Baseline Dialog length (turns)Mean, standard deviation, min and max acts per turnPresence of special machine acts (flight offer and confirm)Presence of user acts (provide a dest city and arrival city)Proportion of acts which were providesString Consistency Did the user provide inconsistent information about dest city?Did the user provide inconsistent information about orig city?Topic Model Ranked list of posterior probabilities of top 50 topicsNormalised probability of dialog for topic modelTable 5: Feature sets for consistency experimentsreal dialogs and those generated by randomly sam-pling turns from different dialogs?
In this section weinduce classifiers over various feature sets to demon-strate that we can, and that the Topic Goal modelcontains far more useful information in this regardthan string-based consistency features.
(The bigrammodel by definition provides no help here, since theunits of which dialogs consist contain the entire win-dow of context used for the bigram model).We take our training and development data fromthe Communicator corpus in the previous section,and create a classification problem as follows: realdialogs form positive examples in the classificationproblem.
To create negative examples, we sample{machine, user} turns at random from the appropri-ate resource.
We keep a histogram over real dia-log lengths, and sample a number of turns for our?fake?
dialogs proportional to this histogram.
Wethen sample this many turns from the frequency dis-tribution over turns in the real data, and create ex-actly as many dialogs in this fashion as real dialogsin the data.
The result is an equal number of dialogscomprised of real turns, of (expected) real length,but where the sequence of turns is highly unlikely tobe coherent given the random sampling.
The classi-fication problem is thus far from trivial.
We do thisfrom our training data to produce data with which totrain the classifier, and from our development datato provide test instances.
This gives rise to 2500training instances, and 500 test instances.We learn linear SVMs with various features de-scribed in Table 6.
These feature sets are designedto capture different aspects of consistency: the base-line features are intended to capture surface levelfeatures of the dialogs, inspired by (Schatzmann etal., 2005) where they provide trivial separation ofreal from simulated dialogs.
However, our settingis different: we do not seek to tell real dialogs fromfully simulated ones, but real dialogs from scram-bled versions of real dialogs.
In addition to length-based features, we add binary presence indicator forseveral user and machine acts highly correlated withthe completion of dialogs, as well as for acts whichindicate the provision of information and the propor-tion of all acts occupied by these.
The table gives acomplete list of these Baseline (B) features.We derive a second set of features intended toreplicate the utility of string-based goals: we set upbinary features to fire if contradictory information isprovided for the slots over the course of the dialog.These are our String Consistency (SC) features.Finally, we use our topic-model simulator to de-rive consistency features.
Our features are the pos-terior distribution over topics for each slot given thatdialog.
Our topics are induced from the real trainingdialogs, and their posterior probabilities computedfor all dialogs relative to this model.
We take poste-rior probabilities of the fifty most probable topics foreach of the dest city and orig city slots as features,as well as the normalised log probability of the di-alog (the log probability divided by the number ofuser utterances).
These form our Topic Model (TM)features.Our classifiers are linear SVMs, and we use lib-svm (Chang and Lin, 2011), scaling features to therange [0 ?
1].
All other parameters are left at theirdefaults.79Feature Set AccuracyBaseline (B) 74.34 ?3.77String Consistency (SC) 63.60 ?4.27B + SC 77.63 ?3.58Topic Model (TM) 79.61 ?3.44B + SC + TM 85.96 ?2.89Table 6: Performances for the classifiers.
Errors are 95%intervals to the accuracies assuming they are parametersto a binomial distribution7.1 ResultsThe results of the classifiers are shown in Table 5.Since we have an equally balanced binary classifi-cation task, accuracy is the most appropriate metric.Here we see that the baseline and string consistencyfeatures have roughly the same discriminatory po-tential, and their union produces a slight improve-ment.
The topic model features are far superior tothis, and the union of all three sets gives a furtherimprovement.These results demonstrate that our model encodesnotions of consistency which go substantially be-yond those defined at the level of strings.
Featuresdefined over the latent topic goal space substantiallyimprove performance in a difficult discriminationtask, demonstrating that our model captures an im-portant notion of how real dialogs appear that is notshared by the other models we consider.8 Concluding Remarks and Future WorkThis paper presents a fully generative goal drivenuser simulator, the first to merge both consistencyand variability within a fully probabilistic frame-work.
We evaluate our model on two task-based di-alog domains, Let?s Go and Communicator, and findit to outperform both a simple bigram model and anupper bound on probability models where the stringsare represented as goals, in terms of the probabilitythe model assigns to heldout dialogs.We then move on to show that features derivedfrom the model lead to substantial improvement indetecting real dialogs from those where the turnshave been selected at random from all turns in thetraining data: this is a fairly difficult task, but ourmodel allows significant improvement over strongand sensible baselines.Our model could be extended in a number ofways.
It could be improved to incorporate noiseresulting from the decisions made by the semanticparser.
Another possible improvement is to explorethe effects of introducing dependency between theslots in the user goal, which would enforce moreplausible values pairings and would potentially im-prove the simulator?s performance.
The effects of adependence assumption between the different utter-ances occurring in a single user turn under the actmodel can also be explored.
We would also like touse our simulator to train a POMDP-based dialogmanager using a form of reinforcement learning.ReferencesHua Ai and Diane J. Litman.
2008.
Assessing dialog sys-tem user simulation evaluation measures using humanjudges.
In Proceedings of ACL-08: HLT.Christopher M. Bishop.
2006.
Pattern Recognition andMachine Learning (Information Science and Statis-tics).
Springer-Verlag New York, Inc.Alan W. Black and Maxine Eskenazi.
2009.
The Spo-ken Dialogue Challenge.
In Proceedings of SIGDIAL2009, SIGDIAL ?09.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alation.
Journal of MachineLearning Research, 3:993?1022, March.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM:A library for support vector machines.
ACM Transac-tions on Intelligent Systems and Technology, 2:27:1?27:27.Wieland Eckert, Esther Levin, and Roberto Pieraccini.1997.
User modeling for spoken dialogue system eval-uation.
In Proceedings of IEEE Workshop on Auto-matic Speech Recognition and Understanding.M.
Gas?ic?, F. Jurcicek, B. Thomson, K. Yu, and S. Young.2011.
On-line policy optimisation of spoken dia-logue systems via live interaction with human subjects.In Automatic Speech Recognition and Understanding,2011 IEEE Workshop on, Hawaii, December.Kallirroi Georgila, James Henderson, and Oliver Lemon.2005a.
Learning user simulations for information stateupdate dialogue systems.
In Proceedings InterSpeech2005.Kallirroi Georgila, Oliver Lemon, and James Henderson.2005b.
Automatic annotation of communicator dia-logue data for learning dialogue strategies and usersimulations.
In Proceedings Ninth Workshop on theSemantics and Pragmatics of Dialogue.Kallirroi Georgila, James Henderson, and Oliver Lemon.2006.
User Simulation for Spoken Dialogue Systems:80Learning and Evaluation.
In Proceedings InterSpeech2006.Thomas L. Griffiths and Mark Steyvers.
2004.
Findingscientific topics.
PNAS, 101(suppl.
1):5228?5235.Srinivasan Janarthanam and Oliver Lemon.
2009.
A two-tier user simulation model for reinforcement learn-ing of adaptive referring expression generation poli-cies.
In Proceedings of SIGDIAL 2009, SIGDIAL ?09,pages 120?123.Sangkeun Jung, Cheongjae Lee, Kyungduk Kim, Min-woo Jeong, and Gary Geunbae Lee.
2009.
Data-driven user simulation for automated evaluation ofspoken dialog systems.
Computer Speech & Lan-guage, 23(4):479?509.Simon Keizer, Milica Gas?ic?, Filip Jurc??
?c?ek, Franc?oisMairesse, Blaise Thomson, Kai Yu, and Steve Young.2010.
Parameter estimation for agenda-based usersimulation.
In Proceedings of SIGDIAL 2010.Esther Levin and Roberto Pieraccini.
2000.
A stochasticmodel of human-machine interaction for learning di-alog strategies.
In IEEE Transactions on Speech andAudio Processing.E.
Levin, S. Narayanan, R. Pieraccini, K. Biatov,E.
Bocchieri, G. Di Fabbrizio, W. Eckert, S. Lee,A.
Pokrovsky, M. Rahim, P. Ruscitti, and M. Walker.2000.
The AT&T-DARPA communicator mixed-initiative spoken dialog system.
In In ICSLP.Radford Neal.
1991.
Bayesian Mixture Modeling byMonte Carlo Simulation.
Technical report, Universityof Toronto.Olivier Pietquin.
2004.
A Framework for UnsupervisedLearning of Dialogue Strategies.
Ph.D. thesis, Faculte?Polytechnique de Mons, TCTS Lab (Belgique), apr.Jost Schatzmann, Kallirroi Georgila, and Steve Young.2005.
Quantitative evaluation of user simulation tech-niques for spoken dialogue systems.
In Proceeings of6th SIGDIAL Workshop.Jost Schatzmann, Blaise Thomson, Karl Weilhammer,Hui Ye, and Steve Young.
2007a.
Agenda-based usersimulation for bootstrapping a POMDP dialogue sys-tem.
In HLT-NAACL (Short Papers), NAACL-Short?07.Jost Schatzmann, Blaise Thomson, and Steve Young.2007b.
Statistical User Simulation with a HiddenAgenda.
In Proceedings 8th SIDdial Workshop onDiscourse and Dialogue, September.Konrad Scheffler and Steve Young.
2002.
Automaticlearning of dialogue strategy using dialogue simula-tion and reinforcement learning.
In Proceedings ofHLT 2002.Satinder P. Singh, Michael J. Kearns, Diane J. Litman,and Marilyn A. Walker.
2000.
Empirical evaluation ofa reinforcement learning spoken dialogue system.
InProceedings of the Seventeenth National Conferenceon Artificial Intelligence and Twelfth Conference onInnovative Applications of Artificial Intelligence.Richard S. Sutton and Andrew G. Barto.
1998.
Re-inforcement Learning: An Introduction.
MIT Press,Cambridge, MA.Jason D. Williams.
2008.
Evaluating user simulationswith the Cramer-von Mises divergence.
Speech Com-munication, 50(10):829?846, October.81
