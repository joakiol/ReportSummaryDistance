A Sense-Topic Model for Word Sense Inductionwith Unsupervised Data EnrichmentJing Wang?
Mohit Bansal?
Kevin Gimpel?
Brian D. Ziebart?
Clement T.
Yu?
?University of Illinois at Chicago, Chicago, IL, 60607, USA{jwang69,bziebart,cyu}@uic.edu?Toyota Technological Institute at Chicago, Chicago, IL, 60637, USA{mbansal,kgimpel}@ttic.eduAbstractWord sense induction (WSI) seeks to automat-ically discover the senses of a word in a cor-pus via unsupervised methods.
We proposea sense-topic model for WSI, which treatssense and topic as two separate latent vari-ables to be inferred jointly.
Topics are in-formed by the entire document, while sensesare informed by the local context surroundingthe ambiguous word.
We also discuss unsu-pervised ways of enriching the original cor-pus in order to improve model performance,including using neural word embeddings andexternal corpora to expand the context of eachdata instance.
We demonstrate significant im-provements over the previous state-of-the-art,achieving the best results reported to date onthe SemEval-2013 WSI task.1 IntroductionWord sense induction (WSI) is the task of automat-ically discovering all senses of an ambiguous wordin a corpus.
The inputs to WSI are instances of theambiguous word with its surrounding context.
Theoutput is a grouping of these instances into clusterscorresponding to the induced senses.
WSI is gen-erally conducted as an unsupervised learning task,relying on the assumption that the surrounding con-text of a word indicates its meaning.
Most previouswork assumed that each instance is best labeled witha single sense, and therefore, that each instance be-longs to exactly one sense cluster.
However, recentwork (Erk and McCarthy, 2009; Jurgens, 2013) hasshown that more than one sense can be used to inter-pret certain instances, due to context ambiguity andsense relatedness.To handle these characteristics of WSI (unsuper-vised, senses represented by token clusters, multiplesenses per instance), we consider approaches basedon topic models.
A topic model is an unsupervisedmethod that discovers the semantic topics underly-ing a collection of documents.
The most popular islatent Dirichlet allocation (LDA; Blei et al., 2003),in which each topic is represented as a multinomialdistribution over words, and each document is rep-resented as a multinomial distribution over topics.One approach would be to run LDA on the in-stances for an ambiguous word, then simply inter-pret topics as induced senses (Brody and Lapata,2009).
However, while sense and topic are related,they are distinct linguistic phenomena.
Topics areassigned to entire documents and are expressed byall word tokens, while senses relate to a single am-biguous word and are expressed through the localcontext of that word.
One possible approach wouldbe to only keep the local context of each ambigu-ous word, discarding the global context.
However,the topical information contained in the broader con-text, though it may not determine the sense directly,might still be useful for narrowing down the likelysenses of the ambiguous word.Consider the ambiguous word cold.
In the sen-tence ?His reaction to the experiments was cold?,the possible senses for cold include cold tempera-ture, a cold sensation, common cold, or a negativeemotional reaction.
However, if we know that thetopic of the document concerns the effects of lowtemperatures on physical health, then the negativeemotional reaction sense should become less likely.Therefore, in this case, knowing the topic helps nar-row down the set of plausible senses.59Transactions of the Association for Computational Linguistics, vol.
3, pp.
59?71, 2015.
Action Editor: Hwee Tou Ng.Submission batch: 10/2014; Revision batch 12/2014; Revision batch 1/2015; Published 1/2015.
c?2015 Association for Computational Linguistics.At the same time, knowing the sense can also helpdetermine possible topics.
Consider a set of textsthat all include the word cold.
Without further in-formation, the texts might discuss any of a numberof possible topics.
However, if the sense of cold isthat of cold ischemia, then the most probable topicswould be those related to organ transplantation.In this paper, we propose a sense-topic model forWSI, which treats sense and topic as two separatelatent variables to be inferred jointly (?4).
When re-lating the sense and topic variables, a bidirectionaledge is drawn between them to represent their cyclicdependence (Heckerman et al., 2001).
We performinference using collapsed Gibbs sampling (?4.2),then estimate the sense distribution for each instanceas the solution to the WSI task.
We conduct exper-iments on the SemEval-2013 Task 13 WSI dataset,showing improvements over several strong baselinesand task systems (?5).We also present unsupervised ways of enrichingour dataset, including using neural word embed-dings (Mikolov et al., 2013) and external Web-scalecorpora to enrich the context of each data instanceor to add more instances (?6).
Each data enrich-ment method gives further gains, resulting in sig-nificant improvements over existing state-of-the-artWSI systems.
Overall, we find gains of up to 22%relative improvement in fuzzy B-cubed and 50% rel-ative improvement in fuzzy normalized mutual in-formation (Jurgens and Klapaftis, 2013).2 Background and Related WorkWe discuss the WSI task, then discuss several areasof research that are related to our approach, includ-ing applications of topic modeling to WSI as wellas other approaches that use word embeddings andclustering algorithms.WSD and WSI: WSI is related to but distinctfrom word sense disambiguation (WSD).
WSDseeks to assign a particular sense label to eachtarget word instance, where the sense labels areknown and usually drawn from an existing senseinventory like WordNet (Miller et al., 1990).
Al-though extensive research has been devoted to WSD,WSI may be more useful for downstream tasks.WSD relies on sense inventories whose construc-tion is time-intensive, expensive, and subject to poorinter-annotator agreement (Passonneau et al., 2010).Sense inventories also impose a fixed sense gran-ularity for each ambiguous word, which may notmatch the ideal granularity for the task of interest.Finally, they may lack domain-specific senses andare difficult to adapt to low-resource domains or lan-guages.
In contrast, senses induced by WSI are morelikely to represent the task and domain of interest.Researchers in machine translation and informationretrieval have found that predefined senses are of-ten not well-suited for these tasks (Voorhees, 1993;Carpuat and Wu, 2005), while induced senses canlead to improved performance (Ve?ronis, 2004; Vick-rey et al., 2005; Carpuat and Wu, 2007).Topic Modeling for WSI: Brody and Lapata(2009) proposed a topic model that uses a weightedcombination of separate LDA models based on dif-ferent feature sets (e.g.
word tokens, parts of speech,and dependency relations).
They only used smallerunits of text surrounding the ambiguous word, dis-carding the global context of each instance.
Yao andVan Durme (2011) proposed a model based on a hi-erarchical Dirichlet process (HDP; Teh et al., 2006),which has the advantage that it can automaticallydiscover the number of senses.
Lau et al.
(2012) de-scribed a model based on an HDP with positionalword features; it formed the basis for their submis-sion (unimelb, Lau et al., 2013) to the SemEval-2013 WSI task (Jurgens and Klapaftis, 2013).Our sense-topic model is distinct from this priorwork in that we model sense and topic as two sepa-rate latent variables and learn them jointly.
We com-pare to the performance of unimelb in ?5.For word sense disambiguation, there also existseveral approaches that use topic models (Cai et al.,2007; Boyd-Graber and Blei, 2007; Boyd-Graber etal., 2007; Li et al., 2010); space does not permit afull discussion.Word Representations for WSI: Another ap-proach to solving WSI is to use word representationsbuilt by distributional semantic models (DSMs;Sahlgren, 2006) or neural net language models(NNLMs; Bengio et al., 2003; Mnih and Hinton,2007).
Their assumption is that words with similardistributions have similar meanings.
Akkaya et al.
(2012) use word representations learned from DSMsdirectly for WSI.
Each word is represented by a co-60occurrence vector, and the meaning of an ambigu-ous word in a specific context is computed throughelement-wise multiplication applied to the vector ofthe target word and its surrounding words in the con-text.
Then instances are clustered by hierarchicalclustering based on their representations.Word representations trained by NNLMs, oftencalled word embeddings, capture information viatraining criteria based on predicting nearby words.They have been useful as features in many NLPtasks (Turian et al., 2010; Collobert et al., 2011;Dhillon et al., 2012; Hisamoto et al., 2013; Bansalet al., 2014).
The similarity between two words canbe computed using cosine similarity of their embed-ding vectors.
Word embeddings are often also usedto build representations for larger units of text, suchas sentences, through vector operations (e.g., sum-mation) applied to the vector of each token in thesentence.
In our work, we use word embeddingsto compute word similarities (for better modeling ofour data distribution), to represent sentences (to findsimilar sentences in external corpora for data enrich-ment), and in a product-of-embeddings baseline.Baskaya et al.
(2013) represent the context of eachambiguous word by using the most likely substitutesaccording to a 4-gram LM.
They pair the ambigu-ous word with likely substitutes, project the pairsonto a sphere (Maron et al., 2010), and obtain finalsenses via k-means clustering.
We compare to theirSemEval-2013 system AI-KU (?5).Other Approaches to WSI: Other approaches in-clude clustering algorithms to partition instancesof an ambiguous word into sense-based clus-ters (Schu?tze, 1998; Pantel and Lin, 2002; Purandareand Pedersen, 2004), or graph-based methods to in-duce senses (Dorow and Widdows, 2003; Ve?ronis,2004; Agirre and Soroa, 2007).3 Problem SettingIn this paper, we induce senses for a set of wordtypes, which we refer to as target words.
For eachtarget word, we have a set of instances.
Each in-stance provides context for a single occurrence ofthe target word.1 For our experiments, we use the1The target word token may occur multiple times in an in-stance, but only one occurrence is chosen as the target wordoccurrence.Figure 1: Proposed sense-topic model in plate notation.There are MD instances for the given target word.
Inan instance, there are Ng global context words (wg) andN` local context words (w`), all of which are observed.There is one latent variable (?topic?
tg) for the wg andtwo latent variables (?topic?
t` and ?sense?
s`) for thew`.
Each instance has topic mixing proportions ?t andsense mixing proportions ?s.
For clarity, not all variablesare shown.
The complete figure with all variables is givenin Appendix A.
This is a dependency network, not a di-rected graphical model, as shown by the directed arrowsbetween t` and s`; see text for details.dataset released for SemEval-2013 Task 13 (Jur-gens and Klapaftis, 2013), collected from the OpenAmerican National Corpus (OANC; Ide and Suder-man, 2004).2 It includes 50 target words: 20 verbs,20 nouns, and 10 adjectives.
There are a total of4,664 instances across all target words.
Each in-stance contains only one sentence, with a minimumlength of 22 and a maximum length of 100.
The goldstandard for the dataset was prepared by multipleannotators, where each annotator labeled instancesbased on the sense inventories in WordNet 3.1.
Foreach instance, they rated all senses of a target wordon a Likert scale from one to five.4 A Sense-Topic Model for WSIWe now present our sense-topic model, shown inplate notation in Figure 1.
It generates the words inthe set of instances for a single target word; we runthe model separately for each target word, sharingno parameters across target words.
We treat senseand topic as two separate latent variables to be in-ferred jointly.
To differentiate sense and topic, weuse a window around the target word in each in-stance.
Word tokens inside the window are local2?Word Sense Induction for Graded and Non-Graded Senses,?
http://www.cs.york.ac.uk/semeval-2013/task1361context words (w`), while tokens outside the win-dow are global context words (wg).
The number ofwords in the window is fixed to 21 in all experiments(10 words before the target word and 10 after).Generating global context words: As shown inthe left part of Figure 1, each global context wordwg is generated from a latent topic variable tg for theinstance, which follows the same generative storyas LDA.
The corresponding probability of the ithglobal context word w(i)g within instance d is:3Pr(w(i)g |d,?t, ?t)=T?j=1P?tj(w(i)g |t(i)g =j)P?t(t(i)g =j|d)(1)where T is the number of topics, P?tj (w(i)g |t(i)g = j)is the multinomial distribution over words for topicj (parameterized by ?tj ) and P?t(t(i)g = j|d) is themultinomial distribution over topics for instance d(parameterized by ?t).Generating local context words: A local contextword w` is generated from a topic variable t` and asense variable s`:Pr(w`|d, ?t, ?t, ?s, ?s, ?s|t, ?t|s, ?st) =T?j=1S?k=1Pr(w`|t`=j, s`=k) Pr(t`=j, s`=k|d)(2)where S is the number of senses, Pr(w`|t` = j, s` =k) is the probability of generating word w` giventopic j and sense k, and Pr(t` = j, s` = k|d) isthe joint probability over topics and senses for d.4Unlike in Eq.
(1), we do not use multinomialparameterizations for the distributions in Eq.
(2).When parameterizing them, we make several de-partures from purely-generative modeling.
All ourchoices result in distributions over smaller eventspaces and/or those that condition on fewer vari-ables.
This helps to mitigate data sparsity is-sues arising from attempting to estimate high-dimensional distributions from small datasets.
Asecondary benefit is that we can avoid biases causedby particular choices of generative directionality in3We use Pr() for generic probability distributions withoutfurther qualifiers and P?
() for distributions parameterized by ?.4For clarity, we drop the (i) superscripts in these and thefollowing equations.the model.
We later include an empirical compari-son to justify some of our modeling choices (?5).First, when relating the sense and topic variables,we avoid making a single decision about generativedependence.
Taking inspiration from dependencynetworks (Heckerman et al., 2001), we use the fol-lowing factorization:Pr(t` = j, s` = k|d) =1ZdPr(s` = k|d, t` = j) Pr(t` = j|d, s` = k)(3)where Zd is a normalization constant.We factorize further by using redundant proba-bilistic events, then ignore the normalization con-stants during learning, a concept commonly calleddeficiency (Brown et al., 1993).
Deficient modelinghas been found to be useful for a wide range of NLPtasks (Klein and Manning, 2002; May and Knight,2007; Toutanova and Johnson, 2007).
In particular,we factor the conditional probabilities in Eq.
(3) intoproducts of multinomial probabilities:Pr(s` = k|d, t` = j) =P?s(s`=k|d)P?s|tj(s`=k|t`=j)P?st(t`=j, s`=k)Zd,tjPr(t`=j|d, s`=k)=P?t(t`=j|d)P?t|sk(t`=j|s`=k)Zd,skwhere Zd,tj and Zd,sk are normalization factors andwe have introduced new multinomial parameters?s, ?s|tj , ?st, and ?t|sk .We use the same idea to factor the word genera-tion distribution:Pr(w`|t`=j, s`=k)=P?tj(w`|t`=j)P?sk(w`|s`=k)Ztj ,skwhere Ztj ,sk is a normalization factor, and we havenew multinomial parameters ?sk for the sense-worddistributions.
One advantage of this parameteriza-tion is that we naturally tie the topic-word distribu-tions across the global and local context words byusing the same parameters ?tj .4.1 Generative StoryWe now give the full generative story of our model.We describe it for generating a set of instances ofsize MD, where all instances contain the same tar-get word.
We use symmetric Dirichlet priors for62all multinomial distributions mentioned above, us-ing the same fixed hyperparameter value (?)
forall.
We use ?
to denote parameters of multinomialdistributions over words, and ?
to denote parame-ters of multinomial distributions over topics and/orsenses.
We leave unspecified the distributions overN` (number of local words in an instance) and Ng(number of global words in an instance), as we onlyuse our model to perform inference given fixed in-stances, not to generate new instances.The generative story first follows the steps de-scribed in Algo.
1 to generate parameters that areshared across all instances; then for each instance d,it follows Algo.
2 to generate global and local words.Algorithm 1 Generative story for instance set1: for each topic j ?
1 to T do2: Choose topic-word params.
?tj ?
Dir(?
)3: Choose topic-sense params.
?s|tj ?
Dir(?
)4: for each sense k ?
1 to S do5: Choose sense-word params.
?sk ?
Dir(?
)6: Choose sense-topic params.
?t|sk ?
Dir(?
)7: Choose topic/sense params.
?st ?
Dir(?
)Algorithm 2 Generative story for instance d1: Choose topic proportions ?t ?
Dir(?
)2: Choose sense proportions ?s ?
Dir(?
)3: Choose Ng and N` from unspecified distributions4: for i?
1 to Ng do5: Choose a topic j ?
Mult(?t)6: Choose a word wg ?
Mult(?tj )7: for i?
1 to N` do8: repeat9: Choose a topic j ?
Mult(?t)10: Choose a sense k ?
Mult(?s)11: Choose a topic j?
?
Mult(?t|sk)12: Choose a sense k?
?
Mult(?s|tj )13: Choose topic/sense ?j?
?, k???
?
Mult(?st)14: until j = j?
= j??
and k = k?
= k?
?15: repeat16: Choose a word w` ?
Mult(?tj )17: Choose a word w?` ?
Mult(?sk)18: until w` = w?`4.2 InferenceWe use collapsed Gibbs sampling (Geman and Ge-man, 1984) to obtain samples from the posterior dis-tribution over latent variables, with all multinomialparameters analytically integrated out before sam-pling.
Then we estimate the sense distribution ?s foreach instance using maximum likelihood estimationon the samples.
These sense distributions are theoutput of our WSI system.We note that deficient modeling does not ordinar-ily affect Gibbs sampling when used for computingposteriors over latent variables, as long as parame-ters (the ?
and ?)
are kept fixed.
This is the caseduring the E step of an EM algorithm, which is theusual setting in which deficiency is used.
Only theM step is affected; it becomes an approximate Mstep by assuming the normalization constants equal1 (Brown et al., 1993).However, here we use collapsed Gibbs samplingfor posterior inference, and the analytic integrationis disrupted by the presence of the normalizationconstants.
To bypass this, we employ the standardapproximation of deficient models that all normal-ization constants are 1, permitting us to use stan-dard formulas for analytic integration of multino-mial parameters with Dirichlet priors.
Empirically,we found this ?collapsed deficient Gibbs sampler?to slightly outperform a more principled approachbased on EM, presumably due to the ability of col-lapsing to accelerate mixing.During the sampling process, each sampler is runon the full set of instances for a target word, iteratingthrough all word tokens in each instance.
If the cur-rent word token is a global context word, we samplea new topic for it conditioned on all other latent vari-ables across instances.
If the current word is a localcontext word, we sample a new topic/sense pair for itagain conditioned on all other latent variable values.We write the conditional posterior distributionover topics for global context word token i in in-stance d as Pr(t(i)g = j|d, t?i, s, ?
), where t(i)g = jis the topic assignment of token i, d is the currentinstance, t?i is the set of topic assignments of allword tokens aside from i for instance d, s is theset of sense assignments for all local word tokensin instance d, and ???
stands for all other observed orknown information, including all words, all Dirich-let hyperparameters, and all latent variable assign-ments in other instances.
The conditional posterior63can be computed by:Pr(t(i)g = j|d, t?i, s, ?
)?CDTdj + ?
?Tk=1 CDTdk + T??
??
?Pr(t=j|d,t?i,s,?
)CWTij + ?
?Wtk?=1 CWTk?j +Wt??
??
?Pr(w(i)g |t=j,t?i,s,?
)(4)where we use the superscriptDT as a mnemonic for?instance/topic?
when counting topic assignments inan instance and WT for ?word/topic?
when count-ing topic assignments for a word.
CDTdj contains thenumber of times topic j is assigned to some wordtoken in instance d, excluding the current word to-ken w(i)g ; CWTij is the number of times word w(i)g isassigned to topic j, across all instances, excludingthe current word token.
Wt is the number of dis-tinct word types in the full set of instances.
We showthe corresponding conditional posterior probabilitiesunderneath each term; the count ratios are obtainedusing standard Dirichlet-multinomial collapsing.The conditional posterior distribution overtopic/sense pairs for a local context word token w(i)`can be computed by:Pr(t(i)` = j, s(i)` = k|d, t?i, s?i, ?)
?CDTdj + ?
?Tk?=1 CDTdk?
+ T??
??
?Pr(t=j|d,t?i,s,?
)CWTij + ?
?Wtk?=1 CWTk?j +Wt??
??
?Pr(w(i)` |t=j,t?i,s,?
)CDSdk + ?
?Sk?=1 CDSdk?
+ S??
??
?Pr(s=k|d,s?i,?
)CWSik + ?
?Wsk?=1 CWSk?k +Ws??
??
?Pr(w(i)` |s=k,s?i,?
)CSTkj + ?
?Sk?=1 CSTk?j + S??
??
?Pr(s=k|t=j,t?i,s?i,?
)CSTkj + ?
?Tk?=1 CSTkk?
+ T??
??
?Pr(t=j|s=k,t?i,s?i,?
)CSTkj + ?
?Sk?=1?Tj?=1 CSTk?j?
+ ST??
??
?Pr(s=k,t=j|t?i,s?i,?
)(5)where CDSdk contains the number of times sense k isassigned to some local word token in instance d, ex-cluding the current word token; CWSik contains thenumber of time word w(i)` is assigned to sense k, ex-cluding the current time; CSTkj contains the numberof times sense k and topic j are assigned to some lo-cal word tokens.
Ws is the number of distinct localcontext word types across the collection.Decoding After the sampling process, we obtain afixed-point estimate of the sense distribution (?s) foreach instance d using the counts from our samples.Where we use ?ks to denote the probability of sensek for the instance, this amounts to:?ks =CDSdk?Sk?=1 CDSdk?
(6)This distribution is considered the final sense assign-ment distribution for the target word in instance d forthe WSI task; the full distribution is fed to the eval-uation metrics defined in the next section.To inspect what the model learned, we similarlyobtain the sense-word distribution (?s) from thecounts as follows, where ?isk is the probability ofword type i given sense k:?isk =CWSik?Wsi?=1 CWSi?k(7)5 Experimental ResultsIn this section, we evaluate our sense-topic modeland compare it to several strong baselines and state-of-the-art systems.Evaluation Metrics To evaluate WSI systems, Ju-rgens and Klapaftis (2013) propose two metrics:fuzzy B-cubed and fuzzy normalized mutual infor-mation (NMI).
They are each computed separatelyfor each target word, then averaged across targetwords.
Fuzzy B-cubed prefers labeling all instanceswith the same sense, while fuzzy NMI prefers theopposite extreme of labeling all instances with dis-tinct senses.
Hence, we report both fuzzy B-cubed(%) and fuzzy NMI (%) in our evaluation.
For easeof comparison, we also report the geometric meanof the 2 metrics, which we denote by AVG.5SemEval-2013 Task 13 also provided a trialdataset (TRIAL) that consists of eight target ambigu-ous words, each with 50 instances (Erk et al., 2009).We use it for preliminary experiments of our modeland for tuning certain hyperparameters, and evalu-ate final performance on the SemEval-2013 dataset(TEST) with 50 target words.5We do not use an arithmetic mean because the effectiverange of the two metrics is substantially different.64S B-cubed(%) NMI(%) AVG2 42.9 4.18 13.393 31.9 6.50 14.405 22.3 8.60 13.857 15.4 8.72 11.6110 12.5 10.91 11.67Table 1: Performance on TRIAL for the sense-topicmodel with different numbers of senses (S).
Best scorein each column is bold.Hyperparameter Tuning We use TRIAL to ana-lyze performance of our sense-topic model underdifferent settings for the numbers of senses (S) andtopics (T ); see Table 1.
We always set T = 2Sfor simplicity.
We find that small S values workbest, which is unsurprising considering the relativelysmall number of instances and small size of each in-stance.
When evaluating on TEST, we use S = 3(which gives the best AVG results on TRIAL).
Later,when we add larger context or more instances (see?6), tuning on TRIAL chooses a larger S value.During inference, the Gibbs sampler was run for4,000 iterations for each target word, setting the first500 iterations as the burn-in period.
In order to get arepresentative set of samples, every 13th sample (af-ter burn-in) is saved to prevent correlations amongsamples.
Due to the randomized nature of the in-ference procedure, all reported results are averagescores over 5 runs.
The hyperparameters (?)
for allDirichlet priors in our model are set to the (untuned)value of 0.01, following prior work on topic model-ing (Griffiths and Steyvers, 2004; Heinrich, 2005).Baselines We include two na?
?ve baselines corre-sponding to the two extremes (biases) preferred byfuzzy B-cubed and NMI, respectively: 1 sense (la-bel each instance with the same single sense) and alldistinct (label each instance with its own sense).We also consider two baselines based on LDA.We run LDA for each target word in TEST, using theset of instances as the set of documents.
We treat thelearned topics as induced senses.
When setting thenumber of topics (senses), we use the gold-standardnumber of senses for each target word, making thisbaseline unreasonably strong.
We run LDA bothwith full context (FULL) and local context (LOCAL),using the same window size as above (10 words be-fore and after the target word).We also present results for the two best systemsin the SemEval-2013 task (according to fuzzy B-cubed and fuzzy NMI, respectively): unimelb andAI-KU.
As described in Section 2, unimelb useshierarchical Dirichlet processes (HDPs).
It extracts50,000 extra instances for each target word as train-ing data from the ukWac corpus?a web corpus ofapproximately 2 billion tokens.6 Among all systemsin the task, it performs best according to fuzzy B-cubed.
AI-KU is based on a lexical substitutionmethod; a language model is built to identify lexicalsubstitutes for target words from the dataset and theukWac corpus.
It performed best among all systemsaccording to fuzzy NMI.Results In Table 2, we present results for thesesystems and compare them to our basic (i.e., withoutany data enrichment) sense-topic model with S = 3(row 9).
According to both fuzzy B-cubed and fuzzyNMI, our model outperforms the other WSI systems(LDA, AI-KU, and unimelb).
Hence, we are ableto achieve state-of-the-art results on the SemEval-2013 task even when only using the single sentenceof context given in each instance (while AI-KU andunimelb use large training sets from ukWac).
Wefound similar performance improvements when onlytested on instances labeled with a single sense.Bidirectionalilty Analysis To measure the impactof the bidirectional dependency between the topicand sense variables in our model, we also evalu-ate the performance of our sense-topic model whendropping one of the directions.
In Table 3, wecompare their performance with our full sense-topicmodel on TEST.
Both unidirectional models performworse than the full model, and dropping t?
s hurtsmore.
This result verifies our intuition that topicswould help narrow down the set of likely senses, andsuggests that bidirectional modeling between topicand sense is desirable for WSI.In subsequent sections, we investigate severalways of exploiting additional data to build better-performing sense-topic models.6 Unsupervised Data EnrichmentThe primary signal used by our model is word co-occurrence information across instances.
If we en-6http://wacky.sslmit.unibo.it/doku.php?id=corpora65Model Data Enrichment Fuzzy B-cubed % Fuzzy NMI % AVG1 1 sense ?
62.3 0 ?2 all distinct ?
0 7.09 ?3 unimelb add 50k instances 48.3 6.0 17.024 AI-KU add 20k instances 39.0 6.5 15.925 LDA (LOCAL) none 47.1 5.93 16.716 LDA (FULL) none 47.3 5.79 16.557 LDA (FULL) add actual context (?6.1) 43.5 6.41 16.708 word embedding product (?6.3) none 33.3 7.24 15.53THIS PAPER9Sense-Topic Modelnone 53.5 6.96 19.3010 add ukWac context (?6.1) 54.5 9.74 23.0411 add actual context (?6.1) 59.1 9.39 23.5612 add instances (?6.2) 58.9 6.01 18.8113 weight by sim.
(?6.3) 55.4 7.14 19.89Table 2: Performance on TEST for baselines and our sense-topic model.
Best score in each column is bold.Model B-cubed(%) NMI(%) AVGDrop s?
t 52.1 6.84 18.88Drop t?
s 51.1 6.78 18.61Full 53.5 6.96 19.30Table 3: Performance on TEST for the sense-topic modelwith ablation of links between sense and topic variables.rich the instances, we can have more robust co-occurrence statistics.
The SemEval-2013 datasetmay be too small to induce meaningful senses, sincethere are only about 100 instances for each targetword, and each instance only contains one sentence.This is why most shared task systems added in-stances from external corpora.In this section, we consider three unsupervisedways of enriching data and measure their impact onperformance.
In ?6.1 we augment the context ofeach instance in our original dataset while keepingthe number of instances fixed.
In ?6.2 we collectmore instances of each target word from ukWac,similar to the AI-KU and unimelb systems.
In?6.3, we change the distribution of words in each in-stance based on their similarity to the target word.Throughout, we make use of word embeddings(see ?2).
We trained 100-dimensional skip-gramvectors (Mikolov et al., 2013) on English Wikipedia(tokenized/lowercased, resulting in 1.8B tokens oftext) using window size 10, hierarchical softmax,and no downsampling.77We used a minimum count cutoff of 20 during training,6.1 Adding ContextThe first way we explore of enriching data is to adda broader context for each instance while keepingthe number of instances unchanged.
This will intro-duce more word tokens into the set of global con-text words, while keeping the set of local contextwords mostly unchanged, as the window size we useis typically smaller than the length of the original in-stance.
With more global context words, the modelhas more evidence to learn coherent topics, whichcould also improve the induced senses via the con-nection between sense and topic.The ideal way of enriching context for an instanceis to add its actual context from the corpus fromwhich it was extracted.
To do this for the SemEval-2013 task, we find each instance in the OANC andretrieve three sentences before the instance and threesentences after.
While not provided for the SemEvaltask, it is reasonable to assume this larger context inmany real-world applications, such as informationretrieval and machine translation of documents.However, in other settings, the corpus may onlyhave a single sentence containing the target word(e.g., search queries or machine translation of sen-tences).
To address this, we find a semantically-similar sentence from the English ukWac corpus andappend it to the instance as additional context.
Foreach instance in the original dataset, we extract itsthen only retained vectors for the most frequent 100,000 wordtypes, averaging the rest to get a vector for unknown words.66most similar sentence that contains the same targetword and add it to increase its set of global contextwords.
To compute similarity, we first represent in-stances and ukWac sentences by summing the wordembeddings across their word tokens, then computecosine similarity.
The ukWac sentence (s?)
with thehighest cosine similarity to each original instance(d) is appended to that instance:s?
= argmaxs?ukWac sim(d, s)Results Since the vocabulary has increased, weexpect we may need larger values for S and T .
OnTRIAL, we find best performance for S = 10, sowe run on TEST with this value.
Performance isshown in Table 2 (rows 10 and 11).
These two meth-ods have higher AVG scores than all others.
Boththeir fuzzy B-cubed and NMI improvements overthe baselines and previous WSI systems are statisti-cally significant, as measured by a paired bootstraptest (p < 0.01; Efron and Tibshirani, 1994).It is unsurprising that we find best performancewith actual context.
Interestingly, however, wecan achieve almost the same gains when automati-cally finding relevant context from a different cor-pus.
Thus, even in real-world settings where we onlyhave a single sentence of context, we can inducesubstantially better senses by automatically broad-ening the global context in an unsupervised manner.As a comparative experiment, we also evaluatethe performance of LDA when adding actual con-text (Table 2, row 7).
Compared with LDA withfull context (FULL) in row 6, performance is slightlyimproved, perhaps due to the fact that longer con-texts induce more accurate topics.
However, thosetopics are not necessarily related to senses, whichis why LDA with only local context actually per-forms best among all three LDA models.
Thus wesee that merely adding context does not necessarilyhelp topic models for WSI.
Importantly, since ourmodel includes both sense and topic, we are able toleverage the additional context to learn better top-ics while also improving the quality of the inducedsenses, leading to our strongest results.Examples We present examples to illustrate oursense-topic model?s advantage over LDA and thefurther improvement when adding actual context.Consider instances (1) and (2) below, with targetword occurrences in bold:(1) Nigeria then sent troops to challenge the coup, evi-dently to restore the president and repair Nigeria?scorrupt image abroad.
(image%1:07:01::/4)8(2) When asked about the Bible?s literal account ofcreation, as opposed to the attractive concept ofdivine creation, every major Republican presiden-tial candidate?even Bauer?has squirmed, ducked,and tried to steer the discussion back to ?faith,??morals,?
and the general idea that humans ?werecreated in the image of God.?
(image%1:06:00::/2image%1:09:02::/4)Both instances share the common word stem pres-ident.
LDA uses this to put these two instancesinto the same topic (i.e., sense).
In our sense-topicmodel, president is a local context word in instance(1) but a global context word in instance (2).
Sothe effect of sharing words is decreased, and thesetwo instances are assigned to different senses by ourmodel.
According to the gold standard, the two in-stances are annotated with different senses, so oursense-topic model provides the correct prediction.Next, consider instances (3), (4), and (5):(3) I have recently deliberately begun to use variationsof ?kick ass?
and ?bites X in the ass?
because theyare colorful, evocative phrases; because, thanksto South Park, ass references are newly familiarand hilarious and because they don?t evoke partic-ularly vivid mental image of asses any longer.
(im-age%1:09:00::/4)(4) Also, playing video games that require rapid mentalrotation of visual image enhances the spatial testscores of boys and girls alike.
(image%1:06:00::/4)(5) Practicing and solidifying modes of representa-tion, Piaget emphasized, make it possible for thechild to free thought from the here and now; cre-ate larger images of reality that take into accountpast, present, and future; and transform those im-age mentally in the service of logical thinking.
(im-age%1:09:00::/4)In the gold standard, instances (3) and (4) have dif-ferent senses while (3) and (5) have the same sense.However, sharing the local context word ?mental?8This is the gold standard sense label, where im-age%1:07:01:: indexes the wordnet senses, and 4 is the scoreassigned by the annotators.The possible range of a score is [1,5].67triggers both LDA and our sense-topic model to as-sign them to the same sense label with high proba-bility.
When augmenting the instances by their realcontexts, we have a better understanding about thetopics.
Instance (3) is about phrase variations, in-stance (4) is about enhancing boys?
spatial skills,while instance (5) discusses the effect of make-believe play for children?s development.When LDA is run with the actual context, it leaves(4) and (5) in the same topic (i.e., sense), while as-signing (3) into another topic with high probability.This could be because (4) and (5) both relate to childdevelopment, and therefore LDA considers them assharing the same topic.
However, topic is not thesame as sense, especially when larger contexts areavailable.
Our sense-topic model built on the ac-tual context makes correct predictions, leaving (3)and (5) into the same sense cluster while labeling(4) with a different sense.6.2 Adding InstancesWe also consider a way to augment our dataset withadditional instances from an external corpus.
Wehave no gold standard senses for these instances, sowe will not evaluate our model on them; they aremerely used to provide richer co-occurrence statis-tics about the target word so that we can performbetter on the instances on which we evaluate.If we added randomly-chosen instances (contain-ing the target word), we would be concerned that thelearned topics and senses may not reflect the distri-butions of the original instance set.
So we only addinstances that are semantically similar to instances inour original set (Moore and Lewis, 2010; Chambersand Jurafsky, 2011).
Also, to avoid changing theoriginal sense distribution by adding too many in-stances, we only add a single instance for each orig-inal instance.
As in ?6.1, for each instance in theoriginal dataset, we find the most similar sentencein ukWac for each instance using word embeddingsand add it into the dataset.
Therefore, the number ofinstances is doubled, and we use the enriched datasetfor our sense-topic model.Results Similarly to ?6.1, on TRIAL, we find bestperformance for S = 10, so we run on TEST withthis value.
As shown in Table 2 (row 12), thisimproves fuzzy B-cubed by 5.4%, but fuzzy NMIis lower, making the AVG worse than the originalmodel.
A possible reason for this is that the sensedistribution in the added instances disturbs that inthe original set of instances, even though we pickedthe most semantically similar ones to add.6.3 Weighting by Word SimilarityAnother approach is inspired by the observation thateach local context token is treated equally in termsof its contribution to the sense.
However, our intu-ition is that certain tokens are more indicative thanothers.
Consider the target word window.
Sinceglass evokes a particular sense of window, we wouldlike to weight it more highly than, say, day.To measure word relatedness, we use cosine sim-ilarity of word embeddings.
We (softly) replicateeach local context word according to its exponenti-ated cosine similarity to the target word.9 The resultis that the local context in each instance has beenmodified to contain fewer occurrences of unrelatedwords and more occurrences of related words.
Ifeach cosine similarity is 0, we obtain our originalsense-topic model.
During inference, the posteriorsense distribution for instance d is now given by:Pr(s = k|d, ?)
=?w?d` exp(sim(w,w?
))1sw=k + ??w?
?d` exp(sim(w?, w?))
+ S?
(8)where d` is the set of local context tokens in d,sim(w,w?)
is the cosine similarity between w andtarget word w?, and 1sw=k is an indicator returning1 when w is assigned to sense k and 0 otherwise.The posterior distribution of sampling a token ofword wi from sense k becomes:CWSik exp(sim(wi, w?))
+ ?
?Wsi?=1 CWSi?k exp(sim(wi?
, w?))
+Ws?
(9)where CWSik counts the number of times wi is as-signed to sense k.Results We again use TRIAL to tune S (and stilluse T = 2S).
We find best TRIAL performance atS = 3; this is unsurprising since this approach doesnot change the vocabulary.
In Table 2, we present re-sults on TEST with S = 3 (row 13).
We also report9Cosine similarities range from -1 to 1, so we use exponen-tiation to ensure we always use positive counts.68Sense Top-5 terms per senseSense-Topic Model1 include, depict, party, paint, visual2 zero, manage, company, culture, figure3 create, clinton, people, american, popular+weight by similarity (?6.3)1 depict, create, culture, mental, include2 picture, visual, pictorial, matrix, movie3 public, means, view, american, storyTable 4: Top 5 terms for each sense induced for the nounimage by the sense-topic model and when weighting localcontext words by similarity.
S = 3 for both.an additional baseline: ?word embedding product?
(row 8), where we represent each instance by mul-tiplying (element-wise) the word vectors of all lo-cal context words, and then feed the instance vectorsinto the fuzzy c-means clustering algorithm (Pal andBezdek, 1995), c = 3.
Compared to this baseline,our approach improves 4.36% on average; comparedwith results for the original sense-topic model (row9), this approach improves 0.69% on average.In Table 4 we show the top-5 terms for each senseinduced for image, both for the original sense-topicmodel and when additionally weighting by similar-ity.
We find that the original model provides lessdistinguishable senses, as it is difficult to derive sep-arate senses from these top terms.
In contrast, senseslearned from the model with weighted similaritiesare more distinct.
Sense 1 relates to mental repre-sentation; sense 2 is about visual representation pro-duced on a surface; and sense 3 is about the generalimpression that something presents to the public.7 Conclusions and Future WorkWe presented a novel sense-topic model for theproblem of word sense induction.
We consideredsense and topic as distinct latent variables, defininga model that generates global context words usingtopic variables and local context words using bothtopic and sense variables.
Sense and topic are re-lated using a bidirectional dependency with a robustparameterization based on deficient modeling.We explored ways of enriching data using wordembeddings from neural language models and exter-nal corpora.
We found enriching context to be mosteffective, even when the original context of the in-stance is not available.
Evaluating on the SemEval-2013 WSI dataset, we demonstrate that our modelyields significant improvements over current state-of-the-art systems, giving 59.1% fuzzy B-cubed and9.39% fuzzy NMI in our best setting.
Moreover, wefind that modeling both sense and topic is criticalto enable us to effectively exploit broader context,showing that LDA does not improve when each in-stance is enriched by actual context.In future work, we plan to further explore thespace of sense-topic models, including non-deficientmodels.
One possibility is to use ?switching vari-ables?
(Paul and Girju, 2009) to choose whether togenerate each word from a topic or sense, with astronger preference to generate from senses closer tothe target word.
Another possibility is to use locally-normalized log-linear distributions and include fea-tures pairing words with particular senses and topics,rather than redundant generative steps.Appendix AThe plate diagram for the complete sense-topicmodel is shown in Figure 2.Figure 2: Plate notation for the proposed sense-topicmodel with all variables (except ?, the fixed Dirichlethyperparameter used as prior for all multinomial distri-butions).
Each instance has topic mixing proportions ?tand sense mixing proportions ?s.
The instance set sharessense/topic parameter ?st, topic-sense distribution ?s|t,sense-topic distribution ?t|s, topic-word distribution ?t,and sense-word distribution ?s.AcknowledgmentsWe thank the editor and the anonymous reviewersfor their helpful comments.
This research was par-tially supported by NIH LM010817.
The opinionsexpressed in this work are those of the authors anddo not necessarily reflect the views of the fundingagency.69ReferencesE.
Agirre and A. Soroa.
2007.
SemEval-2007 Task 02:Evaluating word sense induction and discriminationsystems.
In Proc.
of SemEval, pages 7?12.C.
Akkaya, J. Wiebe, and R. Mihalcea.
2012.
Utilizingsemantic composition in distributional semantic mod-els for word sense discrimination and word sense dis-ambiguation.
In Proc.
of ICSC, pages 45?51.M.
Bansal, K. Gimpel, and K. Livescu.
2014.
Tailoringcontinuous word representations for dependency pars-ing.
In Proc.
of ACL, pages 809?815.O.
Baskaya, E. Sert, V. Cirik, and D. Yuret.
2013.
AI-KU: Using substitute vectors and co-occurrence mod-eling for word sense induction and disambiguation.
InProc.
of SemEval, pages 300?306.Y.
Bengio, R. Ducharme, P. Vincent, and C. Janvin.2003.
A neural probabilistic language model.
J.Mach.
Learn.
Res., 3:1137?1155.D.
M. Blei, A. Y. Ng, and M. I. Jordan.
2003.
La-tent Dirichlet allocation.
J. Mach.
Learn.
Res., 3:993?1022.J.
Boyd-Graber and D. M. Blei.
2007.
PUTOP: Turningpredominant senses into a topic model for word sensedisambiguation.
In Proc.
of SemEval, pages 277?281.J.
Boyd-Graber, D. M. Blei, and X. Zhu.
2007.
A topicmodel for word sense disambiguation.
In Proc.
ofEMNLP-CoNLL, pages 1024?1033.S.
Brody and M. Lapata.
2009.
Bayesian word senseinduction.
In Proc.
of EACL, pages 103?111.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Computa-tional Linguistics, 19(2):263?311.J.
F. Cai, W. S. Lee, and Y. W. Teh.
2007.
Improvingword sense disambiguation using topic features.
InProc.
of EMNLP-CoNLL, pages 1015?1023.M.
Carpuat and D. Wu.
2005.
Word sense disambigua-tion vs. statistical machine translation.
In Proc.
ofACL, pages 387?394.M.
Carpuat and D. Wu.
2007.
Improving statistical ma-chine translation using word sense disambiguation.
InProc.
of EMNLP-CoNLL, pages 61?72.N.
Chambers and D. Jurafsky.
2011.
Template-basedinformation extraction without the templates.
In Proc.of ACL, pages 976?986.R.
Collobert, J. Weston, L. Bottou, M. Karlen,K.
Kavukcuoglu, and P. Kuksa.
2011.
Natural lan-guage processing (almost) from scratch.
J. Mach.Learn.
Res., 12:2493?2537.P.
Dhillon, J. Rodu, D. Foster, and L. Ungar.
2012.
TwoStep CCA: A new spectral method for estimating vec-tor models of words.
In ICML, pages 1551?1558.B.
Dorow and D. Widdows.
2003.
Discovering corpus-specific word senses.
In Proc.
of EACL, pages 79?82.B.
Efron and R. J. Tibshirani.
1994.
An introduction tothe bootstrap, volume 57.
CRC press.K.
Erk and D. McCarthy.
2009.
Graded word sense as-signment.
In Proc.
of EMNLP, pages 440?449.K.
Erk, D. McCarthy, and N. Gaylord.
2009.
Investi-gations on word senses and word usages.
In Proc.
ofACL, pages 10?18.S.
Geman and D. Geman.
1984.
Stochastic relax-ation, Gibbs distributions, and the Bayesian restorationof images.
IEEE Trans.
Pattern Anal.
Mach.
Intell.,6(6):721?741.T.
L. Griffiths and M. Steyvers.
2004.
Finding scien-tific topics.
Proc.
of the National Academy of Sciencesof the United States of America, 101(Suppl 1):5228?5235.D.
Heckerman, D. M. Chickering, C. Meek, R. Roun-thwaite, and C. Kadie.
2001.
Dependency networksfor inference, collaborative filtering, and data visual-ization.
J. Mach.
Learn.
Res., 1:49?75.G.
Heinrich.
2005.
Parameter estimation for text analy-sis.
Technical report.S.
Hisamoto, K. Duh, and Y. Matsumoto.
2013.
An em-pirical investigation of word representations for pars-ing the web.
In ANLP.N.
Ide and K. Suderman.
2004.
The American NationalCorpus first release.
In Proc.
of LREC, pages 1681?1684.D.
Jurgens and I. Klapaftis.
2013.
SemEval-2013 Task13: Word sense induction for graded and non-gradedsenses.
In Proc.
of SemEval, pages 290?299.D.
Jurgens.
2013.
Embracing ambiguity: A comparisonof annotation methodologies for crowdsourcing wordsense labels.
In Proc.
of NAACL, pages 556?562.D.
Klein and C. D. Manning.
2002.
A generativeconstituent-context model for improved grammar in-duction.
In Proc.
of ACL, pages 128?135.J.
H. Lau, P. Cook, D. McCarthy, D. Newman, andT.
Baldwin.
2012.
Word sense induction for novelsense detection.
In Proc.
of EACL, pages 591?601.J.
H. Lau, P. Cook, and T. Baldwin.
2013. unimelb:Topic modelling-based word sense induction.
In Proc.of SemEval, pages 307?311.L.
Li, B. Roth, and C. Sporleder.
2010.
Topic modelsfor word sense disambiguation and token-based idiomdetection.
In Proc.
of ACL, pages 1138?1147.Y.
Maron, E. Bienenstock, and M. James.
2010.
Sphereembedding: An application to part-of-speech induc-tion.
In Advances in NIPS 23.J.
May and K. Knight.
2007.
Syntactic re-alignmentmodels for machine translation.
In Proc.
of EMNLP-CoNLL, pages 360?368.70T.
Mikolov, K. Chen, G. Corrado, and J.
Dean.
2013.Efficient estimation of word representations in vectorspace.
In Proc.
of ICLR.G.
A. Miller, R. Beckwith, C. Fellbaum, D. Gross, andK.
J. Miller.
1990.
WordNet: An on-line lexicaldatabase.
International Journal of Lexicography, 3(4).A.
Mnih and G. Hinton.
2007.
Three new graphicalmodels for statistical language modelling.
In Proc.
ofICML, pages 641?648.R.
C. Moore and W. Lewis.
2010.
Intelligent selection oflanguage model training data.
In Proc.
of ACL, pages220?224.N.
R. Pal and J. C. Bezdek.
1995.
On cluster validity forthe fuzzy c-means model.
Trans.
Fuz Sys., 3:370?379.P.
Pantel and D. Lin.
2002.
Discovering word sensesfrom text.
In Proc.
of KDD, pages 613?619.R.
J. Passonneau, A. Salleb-Aoussi, V. Bhardwaj, andN.
Ide.
2010.
Word sense annotation of polysemouswords by multiple annotators.
In Proc.
of LREC.M.
Paul and R. Girju.
2009.
Cross-cultural analysis ofblogs and forums with mixed-collection topic models.In Proc.
of EMNLP, pages 1408?1417.A.
Purandare and T. Pedersen.
2004.
Word sense dis-crimination by clustering contexts in vector and simi-larity spaces.
In Proc.
of CoNLL, pages 41?48.M.
Sahlgren.
2006.
The word-space model: Us-ing distributional analysis to represent syntagmaticand paradigmatic relations between words in high-dimensional vector spaces.
Ph.D. dissertation, Stock-holm University.H.
Schu?tze.
1998.
Automatic word sense discrimination.Comput.
Linguist., 24(1):97?123.Y.
W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei.
2006.Hierarchical Dirichlet processes.
Journal of the Amer-ican Statistical Association, 101:1566?1581.K.
Toutanova and M. Johnson.
2007.
A Bayesian LDA-based model for semi-supervised part-of-speech tag-ging.
In Advances in NIPS 20.J.
Turian, L. Ratinov, and Y. Bengio.
2010.
Word rep-resentations: A simple and general method for semi-supervised learning.
In Proc.
of ACL, pages 384?394.J.
Ve?ronis.
2004.
Hyperlex: lexical cartography for in-formation retrieval.
Computer Speech & Language,18(3):223?252.D.
Vickrey, L. Biewald, M. Teyssier, and D. Koller.
2005.Word-sense disambiguation for machine translation.In Proc.
of HLT-EMNLP, pages 771?778.E.
M. Voorhees.
1993.
Using WordNet to disambiguateword senses for text retrieval.
In Proc.
of SIGIR, pages171?180.X.
Yao and B.
Van Durme.
2011.
Nonparamet-ric Bayesian word sense induction.
In Proc.
ofTextGraphs-6: Graph-based Methods for Natural Lan-guage Processing, pages 10?14.7172
