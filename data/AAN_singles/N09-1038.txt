Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 335?343,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsMinimal-length linearizations for mildly context-sensitive dependency treesY.
Albert ParkDepartment of Computer Science and Engineering9500 Gilman DriveLa Jolla, CA 92037-404, USAyapark@ucsd.eduRoger LevyDepartment of Linguistics9500 Gilman DriveLa Jolla, CA 92037-108, USArlevy@ling.ucsd.eduAbstractThe extent to which the organization of nat-ural language grammars reflects a drive tominimize dependency length remains littleexplored.
We present the first algorithmpolynomial-time in sentence length for obtain-ing the minimal-length linearization of a de-pendency tree subject to constraints of mildcontext sensitivity.
For the minimally context-sensitive case of gap-degree 1 dependencytrees, we prove several properties of minimal-length linearizations which allow us to im-prove the efficiency of our algorithm to thepoint that it can be used on most naturally-occurring sentences.
We use the algorithmto compare optimal, observed, and randomsentence dependency length for both surfaceand deep dependencies in English and Ger-man.
We find in both languages that anal-yses of surface and deep dependencies yieldhighly similar results, and that mild context-sensitivity affords very little reduction in min-imal dependency length over fully projectivelinearizations; but that observed linearizationsin German are much closer to random and far-ther from minimal-length linearizations thanin English.1 IntroductionThis paper takes up the relationship between twohallmarks of natural language dependency structure.First, there seem to be qualitative constraints on therelationship between the dependency structure of thewords in a sentence and their linear ordering.
In par-ticular, this relationship seems to be such that anynatural language sentence, together with its depen-dency structure, should be generable by a mildlycontext-sensitivity formalism (Joshi, 1985), in par-ticular a linear context-free rewrite system in whichthe right-hand side of each rule has a distinguishedhead (Pollard, 1984; Vijay-Shanker et al, 1987;Kuhlmann, 2007).
This condition places strong con-straints on the linear contiguity of word-word de-pendency relations, such that only limited classes ofcrossing context-free dependency structures may beadmitted.The second constraint is a softer preference forwords in a dependency relation to occur in closeproximity to one another.
This constraint is perhapsbest documented in psycholinguistic work suggest-ing that large distances between governors and de-pendents induce processing difficulty in both com-prehension and production (Hawkins, 1994, 2004;Gibson, 1998; Jaeger, 2006).
Intuitively there isa relationship between these two constraints: con-sistently large dependency distances in a sentencewould require many crossing dependencies.
How-ever, it is not the case that crossing dependenciesalways mean longer dependency distances.
For ex-ample, (1) below has no crossing dependencies, butthe distance between arrived and its dependent Yes-terday is large.
The overall dependency length of thesentence can be reduced by extraposing the relativeclause who was wearing a hat, resulting in (2), inwhich the dependency Yesterday?arrived crossesthe dependency woman?who.
(1) Yesterday a woman who was wearing a hat arrived.
(2) Yesterday a woman arrived who was wearing a hat.335There has been some recent work on dependencylength minimization in natural language sentences(Gildea and Temperley, 2007), but the relationshipbetween the precise constraints on available lin-earizations and dependency length minimization re-mains little explored.
In this paper, we introducethe first efficient algorithm for obtaining lineariza-tions of dependency trees that minimize overall de-pendency lengths subject to the constraint of mildcontext-sensitivity, and use it to investigate the rela-tionship between this constraint and the distributionof dependency length actually observed in naturallanguages.2 Projective and mildly non-projectivedependency-tree linearizationsIn the last few years there has been a resurgenceof interest in computation on dependency-tree struc-tures for natural language sentences, spurred bywork such as McDonald et al (2005a,b) show-ing that working with dependency-tree syntacticrepresentations in which each word in the sen-tence corresponds to a node in the dependency tree(and vice versa) can lead to algorithmic benefitsover constituency-structure representations.
The lin-earization of a dependency tree is simply the linearorder in which the nodes of the tree occur in a sur-face string.
There is a broad division between twoclasses of linearizations: projective linearizationsthat do not lead to any crossing dependencies in thetree, and non-projective linearizations that involveat least one crossing dependency pair.
Example (1),for example, is projective, whereas Example (2) isnon-projective due to the crossing between the Yes-terday?arrived and woman?who dependencies.Beyond this dichotomy, however, the homomor-phism from headed tree structures to dependencystructures (Miller, 2000) can be used together withwork on the mildly context-sensitive formalism lin-ear context-free rewrite systems (LCFRSs) (Vijay-Shanker et al, 1987) to characterize various classesof mildly non-projective dependency-tree lineariza-tions (Kuhlmann and Nivre, 2006).
The LCFRSs arean infinite sequence of classes of formalism for gen-erating surface strings through derivation trees in arule-based context-free rewriting system.
The i-thLCFRS class (for i = 0, 1, 2, .
.
. )
imposes the con-Figure 1: Sample dependency subtree for Figure 2straint that every node in the derivation tree maps toto a collection of at most i+1 contiguous substrings.The 0-th class of LCFRS, for example, correspondsto the context-free grammars, since each node in thederivation tree must map to a single contiguous sub-string; the 1st class of LCFRS corresponds to Tree-Adjoining Grammars (Joshi et al, 1975), in whicheach node in the derivation tree must map to at mosta pair of contiguous substrings; and so forth.
Thedependency trees induced when each rewrite rule inan i-th order LCFRS distinguish a unique head cansimilarly be characterized by being of gap-degree i,so that i is the maximum number of gaps that mayappear between contiguous substrings of any subtreein the dependency tree (Kuhlmann and Mo?hl, 2007).The dependency tree for Example (2), for example,is of gap-degree 1.
Although there are numerousdocumented cases in which projectivity is violatedin natural language, there are exceedingly few doc-umented cases in which the documented gap degreeexceeds 1 (though see, for example, Kobele, 2006).3 Finding minimal dependency-lengthlinearizationsEven under the strongest constraint of projectivity,the number of possible linearizations of a depen-dency tree is exponential in both sentence lengthand arity (the maximum number of dependenciesfor any word).
As pointed out by Gildea and Tem-perley (2007), however, finding the unconstrainedminimal-length linearization is a well-studied prob-lem with an O(n1.6) solution (Chung, 1984).
How-ever, this approach does not take into account con-straints of projectivity or mild context-sensitivity.Gildea and Temperley themselves introduced anovel efficient algorithm for finding the minimizeddependency length of a sentence subject to the con-straint that the linearization is projective.
Their al-gorithm can perhaps be most simply understood bymaking three observations.
First, the total depen-336Figure 2: Dependency length factorization for efficientprojective linearization, using the dependency subtree ofFigure 1dency length of a projective linearization can bewritten as?wi??
?D(wi, Ei) +?wjdep?wiD(wi, Ej)???
(1)where Ei is the boundary of the contiguous substringcorresponding to the dependency subtree rooted atwi which stands between wi and its governor, andD(wi, Ej) is the distance from wi to Ej , with thespecial case of D(wroot, Eroot) = 0 (Figures 1and 2).
Writing the total dependency length thisway makes it clear that each term in the outer sumcan be optimized independently, and thus one canuse dynamic programming to recursively find op-timal subtree orderings from the bottom up.
Sec-ond, for each subtree, the optimal ordering can beobtained by placing dependent subtrees on alternat-ing sides of w from inside out in order of increas-ing length.
Third, the total dependency lengths be-tween any words withing an ordering stays the samewhen the ordering is reversed, letting us assume thatD(wi, Ei) will be the length to the closest edge.These three observations lead to an algorithm withworst-case complexity of O(n log m) time, wheren is sentence length and m is sentence arity.
(Thelog m term arises from the need to sort the daugh-ters of each node into descending order of length.
)When limited subclasses of nonprojectivity areadmitted, however, the problem becomes more diffi-cult because total dependency length can no longerbe written in such a simple form as in Equation (1).Intuitively, the size of the effect on dependencylength of a decision to order a given subtree discon-tiguously, as in a woman.
.
.
who was wearing a hatin Example (2), cannot be calculated without con-sulting the length of the string that the discontiguouskh|c1| |c2|hd12 d11 d21 d22d31d32Figure 3: Factorizing dependency length at node w i ofa mildly context-sensitive dependency tree.
This partiallinearization of head with dependent components makesc1 the head component and leads to l = 2 links crossingbetween c1 and c2.subtree would be wrapped around.
Nevertheless, forany limited gap degree, it is possible to use a dif-ferent factorization of dependency length that keepscomputation polynomial in sentence length.
We in-troduce this factorization in the next section.4 Minimization with limited gap degreeWe begin by defining some terms.
We use the wordcomponent to refer to a full linearization of a sub-tree in the case where it is realized as a single con-tiguous string, or to refer to any of of the contigu-ous substrings produced when a subtree is realizeddiscontiguously.
We illustrate the factorization forgap-degree 1, so that any subtree has at most twocomponents.
We refer to the component contain-ing the head of the subtree as the head component,the remaining component as the dependent compo-nent, and for any given (head component, depen-dent component) pair, we use pair component to re-fer to the other component in the pair.
We refer tothe two components of dependent dj as dj1 and dj2respectively, and assume that dj1 is the head com-ponent.
When dependencies can cross, total depen-dency length cannot be factorized as simply as inEquation (1) for the projective case.
However, wecan still make use of a more complex factorizationof the total dependency length as follows:?wi??
?D(wi, Ei) +?wjdep?wi[D(wi, Ej) + ljkj]???
(2)where lj is the number of links crossing between thetwo components of dj , and kj is the distance addedbetween these two components by the partial lin-earization at wi.
Figure 3 illustrates an example of337such a partial linearization, where k2 is |d31|+ |d32|due to the fact that the links between d21 and d22have to cross both components of d3.
The factor-ization in Equation (2) allows us to use dynamicprogramming to find minimal-length linearizations,so that worst-case complexity is polynomial ratherthan exponential in sentence length.
However, theadditional term in the factorization means that weneed to track the number of links l crossing betweenthe two components of the subtree Si headed by wiand the component lengths |c1| and |c2|.
Addition-ally, the presence of crossing dependencies meansthat Gildea and Temperley?s proof that ordering de-pendent components from the inside out in orderof increasing length no longer goes through.
Thismeans that at each node wi we need to hold on to theminimal-length partial linearization for each combi-nation of the following quantities:?
|c2| (which also determines |c1|);?
the number of links l between c1 and c2;?
and the direction of the link between wi and itsgovernor.We shall refer to a combination of these factorsas a status set.
The remainder of this section de-scribes a dynamic-programming algorithm for find-ing optimal linearizations based on the factorizationin Equation (2), and continues with several furtherfindings leading to optimizations that make the al-gorithm tractable for naturally occurring sentences.4.1 Algorithm 1Our first algorithm takes a tree and recursively findsthe optimal orderings for each possible status set ofeach of its child subtrees, which it then uses to cal-culate the optimal ordering of the tree.
To calcu-late the optimal orderings for each possible statusset of a subtree S, we use the brute-force methodof choosing all combinations of one status set fromeach child subtree, and for each combination, we tryall possible orderings of the components of the childsubtrees, calculate all possible status sets for S, andstore the minimal dependency value for each appear-ing status set of S. The number of possible lengthpairings |c1|, |c2| and number of crossing links lare each bounded above by the sentence length n,so that the maximum number of status sets at eachnode is bounded above by n2.
Since the sum of thestatus sets of all child subtrees is also bounded byn2, the maximum number of status set combinationsis bounded by (n2m )m (obtainable from the inequal-ity of arithmetic and geometric means).
There are(2m+1)!m possible arrangements of head word anddependent components into two components.
Sincethere are n nodes in the tree and each possible com-bination of status sets from each dependent sub treemust be tried, this algorithm has worst-case com-plexity of O((2m + 1)!mn(n2m )m).
This algorithmcould be generalized for mildly context-sensitivelinearizations polynomial in sentence length for anygap degree desired, by introducing additional l termsdenoting the number of links between pairs of com-ponents.
However, even for gap degree 1 this boundis incredibly large, and as we show in Figure 7, al-gorithm 1 is not computationally feasible for batchprocessing sentences of arity greater than 5.4.2 Algorithm 2We now show how to speed up our algorithm byproving by contradiction that for any optimal or-dering which minimizes the total dependency lengthwith the two-cluster constraint, for any given sub-tree S and its child subtree C , the pair componentsc1 and c2 of a child subtree C must be placed onopposite sides of the head h of subtree S.Let us assume that for some dependency treestructure, there exists an optimal ordering where c1and c2 are on the same side of h. Let us refer to theordered set of words between c1 and c2 as v. None ofthe words in v will have dependency links to any ofthe words in c1 and c2, since the dependencies of thewords in c1 and c2 are either between themselves orthe one link to h, which is not between the two com-ponents by our assumption.
There will be j1 ?
0links from v going over c1, j2 ?
0 dependency linksfrom v going over c2, and l ?
1 links between c1 andc2.
Without loss of generality, let us assume that h ison the right side of c2.
Let us consider the effect ontotal dependency length of swapping c1 with v, sothat the linear ordering is v c1 c2 ?
h. The total de-pendency length of the new word ordering changesby?j1|c1|?l|v|+j2|c1| if c2 is the head component,and decreases by another |v| if c1 is the head com-ponent.
Thus the total change in dependency length338is less than or equal to(j2 ?
j1)|c1| ?
l ?
|v| < (j2 ?
j1)|c1| (3)If instead we swap places of v with c2 instead of c1so that we have c1 c2 v ?
h, we find that the totalchange in dependency length is less than or equal to(j1 ?
j2)|c2| ?
(l ?
1)|v| ?
(j1 ?
j2)|c2| (4)It is impossible for the right-hand sides of (3) and (4)to be positive at the same time, so swapping v witheither c1 or c2 must lead to a linearization with loweroverall dependency length.
But this is a contradic-tion to our original assumption, so we see that forany optimal ordering, all split child subtree compo-nents c1 and c2 of the child subtree of S must beplaced on opposite sides of the head h.This constraint allows us to simplify our algo-rithm for finding the minimal-length linearization.Instead of going through all logically possible or-derings of components of the child subtrees, we cannow decide on which side the head component willbe on, and go through all possible orderings for eachside.
This changes the factorial part of our algorithmrun time from (2m + 1)!m to 2m(m!
)2m, giving usO(2m(m!
)2mn(n2m )m), greatly reducing actual pro-cessing time.4.3 Algorithm 3We now present two more findings for further in-creasing the efficiency of the algorithm.
First, welook at the status sets which need to be stored for thedynamic programming algorithm.
In the straightfor-ward approach we first presented, we stored the op-timal dependency lengths for all cases of possiblestatus sets.
We now know that we only need to con-sider cases where the pair components are on op-posite sides.
This means the direction of the linkfrom the head to the parent will always be towardthe inside direction of the pair components, so wecan re-define the status set as (p, l) where p is againthe length of the dependent component, and l is thenumber of links between the two pair components.If the p values for sets s1 and s2 are equal, s1 hasa smaller number of links than s2 (ls1 ?
ls2) ands1 has a smaller or equal total dependency lengthto s2, then replacing the components of s2 with s1will always give us the same or more optimal totalFigure 4: Initial setup for latter part of optimization proofin section 4.4.
To the far left is the head h of subtree S.The component pair C1 and C2 makes up S, and g is thegovernor of h. The length of the substring v between C 1and C2 is k. ci and ci+1 are child subtree components.dependency length.
Thus, we do not have to storeinstances of these cases for our algorithm.Next, we prove by contradiction that for any twostatus sets s1 and s2, if ps1 > ps2 > 0, ls1 = ls2 , andthe TOTAL INTERNAL DEPENDENCY LENGTH t1 ofs1?defined as the sum in Equation (2) over onlythose words inside the subtree headed by h?is lessthan or equal to t2 of s2, then using s1 will be at leastas good as s2, so we can ignore s2.
Let us supposethat the optimal linearization can use s2 but not s1.Then in the optimal linearization, the two pair com-ponents cs2,1 and cs2,2 of s2 are on opposite sidesof the parent head h. WLOG, let us assume thatcomponents cs1,1 and cs2,1 are the dependent com-ponents.
Let us denote the total number of links go-ing over cs2,1 as j1 and the words between cs2,1 andcs2,2 as v (note that v must contain h).
If we swapcs2,1 with v, so that cs2,1 lies adjacent to cs2,2, thenthere would be j2+1 links going over cs2,1.
By mov-ing cs2,1 from opposite sides of the head to be rightnext to cs2,2, the total dependency length of the sen-tence changes by?j1|cs2,1|?
ls2|v|+(j2+1)|cs2,1|.Since the ordering was optimal, we know that(j2 ?
j1 + 1)|cs2,1| ?
ls2 |v| ?
0Since l > 0, we can see that j1 ?
j2 ?
0.
Now, in-stead of swapping v with cs2,1, let us try substitutingthe components from s1 instead of s2.
The changeof the total dependency length of the sentence willbe:j1 ?
(|cs1,1| ?
|cs2,1|) + j2 ?
(|cs1,2|?|cs2,2|) + t1 ?
t2= (j1 ?
j2)?
(ps1 ?
ps2) + (t1 ?
t2)Since j1 ?
j2 ?
0 and ps1 > ps2 , the first termis less than or equal to 0 and since t1 ?
t2 ?
0, thetotal dependency length will have been be equal or339Figure 5: Moving ci+1 to C1Figure 6: Moving ci to C2have decreased.
But this contradicts our assumptionthat only s2 can be part of an optimal ordering.This finding greatly reduces the number of sta-tus sets we need to store and check higher up inthe algorithm.
The worst-case complexity remainsO(2mm!2mn(n2m )m), but the actual runtime is re-duced by several orders of magnitude.4.4 Algorithm 4Our last optimization is on the ordering among thechild subtree components on each side of the sub-tree head h. The initially proposed algorithm wentthrough all combinations of possible orderings tofind the optimal dependency length for each statusset.
By the first optimization in section 4.2 we haveshown that we only need to consider the orderingsin which the components are on opposite sides ofthe head.
We now look into the ordering of the com-ponents on each side of the head.
We first define therank value r for each component c as follows:|c|# links between c and its pair component+I(c)where I(c) is the indicator function having value 1 ifc is a head component and 0 otherwise .
Using thisdefinition, we prove by contradiction that the order-ing of the components from the head outward mustbe in order of increasing rank value.Let us suppose that at some subtree S headed byh and with head component C1 and dependent com-ponent C2, there is an optimal linearization in whichthere exist two components ci and ci+1 of immedi-ate subtrees of S such that ci is closer to h, the com-1 2 3 4 5 6 7100102104106maximum number of dependencies per headtime(ms)Execution times for algorithms 1 & 4Algorithm 1Algorithm 4Figure 7: Timing comparison of first and fully optimizedalgorithmsponents have rank values ri and ri+1 respectively,ri > ri+1, and no other component of the imme-diate subtrees of S intervenes between ci and ci+1.We shall denote the number of links between eachcomponent and its pair component as li, li+1.
Letl?i = li + I(ci) and l?i+1 = li+1 + I(ci+1).
Thereare two cases to consider: either (1) ci and ci+1 arewithin the same component of S, or (2) ci is at theedge of C1 nearest C2 and ci+1 is at the edge of C2neareast C1.Consider case 1, and let us swap ci with ci+1; thisaffects only the lengths of links involving connec-tions to ci or ci+1.
The total dependency length ofthe new linearization will change by?l?i+1|ci|+ l?i|ci+1| = ?l?il?i+1(ri ?
ri+1) < 0This is a contradiction to the assumption that we hadan optimal ordering.Now consider case 2, which is illustrated in Fig-ure 4.
We denote the number of links going overci and ci+1, excluding links to ci, ci+1 as ?1 and?2 respectively, and the length of words betweenthe edges of C1 and C2 as k. Let us move ci+1to the outermost position of C1, as shown in Fig-ure 5.
Since the original linearization was optimal,we have:?
?2|ci+1|+ ?1|ci+1| ?
l?i+1k ?
0(?1 ?
?2)|ci+1| ?
l?i+1k(?1 ?
?2)ri+1 ?
kLet us also consider the opposite case of mov-ing ci to the inner edge of C2, as shown in Fig-ure 6.
Once again due to optimality of the originallinearization, we have340DLA English GermanSurface Deep Surface DeepOptimal with one crossing dependency 32.7 33.0 24.5 23.3Optimal with projectivity constraint 34.1 34.4 25.5 24.2Observed 46.6 48.0 43.6 43.1Random with projectivity constraint 82.4 82.8 50.6 49.2Random with two-cluster constraint 84.0 84.3 50.7 49.5Random ordering with no constraint 183.2 184.2 106.9 101.1Table 1: Average sentence dependency lengths(with max arity of 10)?
?1|ci|+ ?2|ci|+ l?ik ?
0(?2 ?
?1)|ci| ?
?l?ik(?1 ?
?2)ri ?
kBut this is a contradiction, since ri > ri+1.
Com-bining the two cases, we can see that regardless ofwhere the components may be split, in an optimalordering the components going outwards from thehead must have an increasing rank value.This result allows us to simplify our algorithmgreatly, because we no longer need to go throughall combinations of orderings.
Once it has been de-cided which components will come on each side ofthe head, we can sort the components by rank valueand place them from the head out.
This reduces thefactorial component of the algorithm?s complexityto m log m, and the overall worst-case complexityto O(nm2 log m(2n2m )m).
Although this is still ex-ponential in the arity of the tree, nearly all sentencesencountered in treebanks have an arity low enoughto make the algorithm tractable and even very effi-cient, as we show in the following section.5 Empirical resultsUsing the above algorithm, we calculated minimaldependency lengths for English sentences from theWSJ portion of the Penn Treebank, and for Germansentences from the NEGRA corpus.
The English-German comparison is of interest because word or-der is freer, and crossing dependencies more com-mon, in German than in English (Kruijff and Va-sishth, 2003).
We extracted dependency trees fromthese corpora using the head rules of Collins (1999)for English, and the head rules of Levy and Man-ning (2004) for German.
Two dependency treeswere extracted from each sentence, the surface treeextracted by using the head rules on the context-free tree representation (i.e.
no crossing dependen-cies), and the deep tree extracted by first return-ing discontinuous dependents (marked by *T* and*ICH* in WSJ, and by *T* in the Penn-format ver-sion of NEGRA) before applying head rules.
Fig-ure 7 shows the average time it takes to calculatethe minimal dependency length with crossing depen-dencies for WSJ sentences using the unoptimized al-gorithm of Section 4.1 and the fully optimized al-gorithm of Section 4.4.
Timing tests were imple-mented and performed using Java 1.6.0 10 on a sys-tem running Linux 2.6.18-6-amd64 with a 2.0 GHzIntel Xeon processor and 16 gigs of memory, run ona single core.
We can see from Figure 7 that thestraight-forward dynamic programming algorithmtakes many more magnitudes of time than our op-timized algorithm, making it infeasible to calculatethe minimal dependency length for larger sentences.The results we present below were obtained with thefully optimized algorithm from the sentences witha maximum arity of 10, using 49,176 of the 49,208WSJ sentences and 20,563 of the 20,602 NEGRAsentences.Summary results over all sentences from each cor-pus are shown in Table 1.
We can see that for bothcorpora, the oberved dependency length is smallerthan the dependency length of random orderings,even when the random ordering is subject to theprojectivity constraint.
Relaxing the projectivityconstraint by allowing crossing dependencies intro-duces a slightly lower optimal dependency length.The average sentence dependency lengths for thethree random orderings are significantly higher thanthe observed values.
It is interesting to note that therandom orderings given the projectivity constraintand the two-cluster constraint have very similar de-pendency lengths, where as a total random ordering3410 10 20 30 40 500100200300400English/SurfaceSentence lengthAveragesentenceDLUnconstrained Random2?component RandomProjective RandomObservedProjective Optimal2?component Optimal0 10 20 30 40 500100200300400English/DeepSentence lengthAveragesentenceDLUnconstrained Random2?component RandomProjective RandomObservedProjective Optimal2?component Optimal0 10 20 30 40 500100200300400German/SurfaceSentence lengthAveragesentenceDLUnconstrained Random2?component RandomProjective RandomObservedProjective Optimal2?component Optimal0 10 20 30 40 500100200300400German/DeepSentence lengthAveragesentenceDLUnconstrained Random2?component RandomProjective RandomObservedProjective Optimal2?component OptimalFigure 8: Average sentence DL as a function of sentence length.
Legend is ordered top curve to bottom curve.1 2 3 4 5 6 7 80100200300400English/SurfaceSentence ArityAveragesentenceDLUnconstrained Random2?component RandomProjective RandomObservedProjective Optimal2?component Optimal1 2 3 4 5 6 7 80100200300400English/DeepSentence ArityAveragesentenceDLUnconstrained Random2?component RandomProjective RandomObservedProjective Optimal2?component Optimal1 2 3 4 5 6 7 80100200300400German/SurfaceSentence ArityAveragesentenceDLUnconstrained Random2?component RandomProjective RandomObservedProjective Optimal2?component Optimal1 2 3 4 5 6 7 80100200300400German/DeepSentence ArityAveragesentenceDLUnconstrained Random2?component RandomProjective RandomObservedProjective Optimal2?component OptimalFigure 9: Average sentence DL as a function of sentence arity.
Legend is ordered top curve to bottom curve.increases the dependency length significantly.NEGRA generally has shorter sentences thanWSJ, so we need a more detailed picture of depen-dency length as a function of sentence length; thisis shown in Figure 8.
As in Table 1, we see thatEnglish, which has less crossing dependency struc-tures than German, has observed DL closer to opti-mal DL and farther from random DL.
We also seethat the random and observed DLs behave very sim-ilarly across different sentence lengths in Englishand German, but observed DL grows faster in Ger-man.
Perhaps surprisingly, optimal projective DLand gap-degree 1 DL tend to be very similar evenfor longer sentences.
The picture as a function ofsentence arity is largely the same (Figure 9).6 ConclusionIn this paper, we have presented an efficient dynamicprogramming algorithm which finds minimum-length dependency-tree linearizations subject toconstraints of mild context-sensitivity.
For the gap-degree 1 case, we have proven several properties ofthese linearizations, and have used these propertiesto optimize our algorithm.
This made it possible tofind minimal dependency lengths for sentences fromthe English Penn Treebank WSJ and German NE-GRA corpora.
The results show that for both lan-guages, using surface dependencies and deep de-pendencies lead to generally similar conclusions,but that minimal lengths for deep dependencies areconsistently slightly higher for English and slightlylower for German.
This may be because Germanhas many more crossing dependencies than English.Another finding is that the difference between aver-age sentence DL does not change much between op-timizing for the projectivity constraint and the two-cluster constraint: projectivity seems to give nat-ural language almost all the flexibility it needs tominimize DL.
For both languages, the observed lin-earization is much closer in DL to optimal lineariza-tions than to random linearizations; but crucially, wesee that English is closer to the optimal linearizationand farther from random linearization than German.This finding is resonant with the fact that Germanhas richer morphology and overall greater variabilityin observed word order, and with psycholinguisticresults suggesting that dependencies of greater lin-ear distance do not always pose the same increasedprocessing load in German sentence comprehensionas they do in English (Konieczny, 2000).342ReferencesChung, F. R. K. (1984).
On optimal linear arrange-ments of trees.
Computers and Mathematics withApplications, 10:43?60.Collins, M. (1999).
Head-Driven Statistical Modelsfor Natural Language Parsing.
PhD thesis, Uni-versity of Pennsylvania.Gibson, E. (1998).
Linguistic complexity: Localityof syntactic dependencies.
Cognition, 68:1?76.Gildea, D. and Temperley, D. (2007).
Optimizinggrammars for minimum dependency length.
InProceedings of ACL.Hawkins, J.
A.
(1994).
A Performance Theory ofOrder and Constituency.
Cambridge.Hawkins, J.
A.
(2004).
Efficiency and Complexity inGrammars.
Oxford University Press.Jaeger, T. F. (2006).
Redundancy and Syntactic Re-duction in Spontaneous Speech.
PhD thesis, Stan-ford University, Stanford, CA.Joshi, A. K. (1985).
How much context-sensitivityis necessary for characterizing structural descrip-tions ?
Tree Adjoining Grammars.
In Dowty,D., Karttunen, L., and Zwicky, A., editors, Nat-ural Language Processing ?
Theoretical, Com-putational, and Psychological Perspectives.
Cam-bridge.Joshi, A. K., Levy, L. S., and Takahashi, M. (1975).Tree adjunct grammars.
Journal of Computer andSystem Sciences, 10(1).Kobele, G. M. (2006).
Generating Copies: An inves-tigation into Structural Identity in Language andGrammar.
PhD thesis, UCLA.Konieczny, L. (2000).
Locality and parsing com-plexity.
Journal of Psycholinguistic Research,29(6):627?645.Kruijff, G.-J.
M. and Vasishth, S. (2003).
Quantify-ing word order freedom in natural language: Im-plications for sentence processing.
Proceedings ofthe Architectures and Mechanisms for LanguageProcessing conference.Kuhlmann, M. (2007).
Dependency Structures andLexicalized Grammars.
PhD thesis, Saarland Uni-versity.Kuhlmann, M. and Mo?hl, M. (2007).
Mildlycontext-sensitive dependency languages.
In Pro-ceedings of ACL.Kuhlmann, M. and Nivre, J.
(2006).
Mildly non-projective dependency structures.
In Proceedingsof COLING/ACL.Levy, R. and Manning, C. (2004).
Deep depen-dencies from context-free statistical parsers: cor-recting the surface dependency approximation.
InProceedings of ACL.McDonald, R., Crammer, K., and Pereira, F.(2005a).
Online large-margin training of depen-dency parsers.
In Proceedings of ACL.McDonald, R., Pereira, F., Ribarov, K., and Hajic?,J.
(2005b).
Non-projective dependency parsingusing spanning tree algorithms.
In Proceedings ofACL.Miller, P. (2000).
Strong Generative Capacity: TheSemantics of Linguistic Formalism.
Cambridge.Pollard, C. (1984).
Generalized Phrase StructureGrammars, Head Grammars, and Natural Lan-guages.
PhD thesis, Stanford.Vijay-Shanker, K., Weir, D. J., and Joshi, A. K.(1987).
Characterizing structural descriptionsproduced by various grammatical formalisms.
InProceedings of ACL.343
