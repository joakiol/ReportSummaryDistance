Proceedings of NAACL-HLT 2013, pages 41?51,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsMulti-faceted Event Recognition with Bootstrapped DictionariesRuihong Huang and Ellen RiloffSchool of ComputingUniversity of UtahSalt Lake City, UT 84112{huangrh, riloff}@cs.utah.eduAbstractIdentifying documents that describe a specifictype of event is challenging due to the highcomplexity and variety of event descriptions.We propose a multi-faceted event recognitionapproach, which identifies documents aboutan event using event phrases as well as defin-ing characteristics of the event.
Our researchfocuses on civil unrest events and learns civilunrest expressions as well as phrases cor-responding to potential agents and reasonsfor civil unrest.
We present a bootstrappingalgorithm that automatically acquires eventphrases, agent terms, and purpose (reason)phrases from unannotated texts.
We use thebootstrapped dictionaries to identify civil un-rest documents and show that multi-facetedevent recognition can yield high accuracy.1 IntroductionMany people are interested in following news re-ports about events.
Government agencies are keenlyinterested in news about civil unrest, acts of terror-ism, and disease outbreaks.
Companies want to stayon top of news about corporate acquisitions, high-level management changes, and new joint ventures.The general public is interested in articles aboutcrime, natural disasters, and plane crashes.
We willrefer to the task of identifying documents that de-scribe a specific type of event as event recognition.It is tempting to assume that event keywordsare sufficient to identify documents that discuss in-stances of an event.
But event words are rarely reli-able on their own.
For example, consider the chal-lenge of finding documents about civil unrest.
Thewords ?strike?, ?rally?, and ?riot?
refer to com-mon types of civil unrest, but they frequently refer toother things as well.
A strike can refer to a militaryevent or a sporting event (e.g., ?air strike?, ?bowl-ing strike?
), a rally can be a race or a spirited ex-change (e.g.,?car rally?, ?tennis rally?
), and a riotcan refer to something funny (e.g., ?she?s a riot?
).Event keywords also appear in general discussionsthat do not mention a specific event (e.g., ?37 statesprohibit teacher strikes?
or ?The fine for inciting ariot is $1,000?).
Furthermore, many relevant docu-ments are not easy to recognize because events canbe described with complex expressions that do notinclude event keywords.
For example, ?took to thestreets?, ?walked off their jobs?
and ?stormed par-liament?
often describe civil unrest.The goal of our research is to recognize event de-scriptions in text by identifying event expressions aswell as defining characteristics of the event.
We pro-pose that agents and purpose are characteristics ofan event that are essential to distinguish one type ofevent from another.
The agent responsible for an ac-tion often determines how we categorize the action.For example, natural disasters, military operations,and terrorist attacks can all produce human casual-ties and physical destruction.
But the agent of a nat-ural disaster must be a natural force, the agent ofa military incident must be military personnel, andthe agent of a terrorist attack is never a natural forceand rarely military personnel.
There may be otherimportant factors as well, but the agent is often anessential part of an event definition.The purpose of an event is also a crucial factorin distinguishing between event types.
For exam-41ple, civil unrest events and sporting events both in-volve large groups of people amassing at a specificsite.
But the purpose of civil unrest gatherings is toprotest against socio-political problems, while sport-ing events are intended as entertainment.
As anotherexample, terrorist events and military incidents canboth cause casualties, but the purpose of terrorism isto cause widespread fear, while the purpose of mili-tary actions is to protect national security interests.Our research explores the idea of multi-facetedevent recognition: using event expressions as wellas facets of the event (agents and purpose) to iden-tify documents about a specific type of event.
Wepresent a bootstrapping framework to automaticallycreate event phrase, agent, and purpose dictionaries.The learning process uses unannotated texts, a fewevent keywords, and seed terms for common agentsand purpose phrases associated with the event type.Our bootstrapping algorithm exploits the obser-vation that event expressions, agents, and purposephrases often appear together in sentences that in-troduce an event.
In the first step, we extract eventexpressions based on dependency relations with anagent and purpose phrase.
The harvested event ex-pressions are added to an event phrase dictionary.
Inthe second step, new agent terms are extracted fromsentences containing an event phrase and a purposephrase, and new purpose phrases are harvested fromsentences containing an event phrase and an agent.These harvested terms are added to agent and pur-pose dictionaries.
The bootstrapping algorithm rico-chets back and forth, alternately learning new eventphrases and learning new agent/purpose phrases, inan iterative process.We explore several ways of using these boot-strapped dictionaries.
We conclude that finding atleast two different types of event information pro-duces high accuracy (88% precision) with good re-call (71%) on documents that contain an event key-word.
We also present experiments with documentsthat do not contain event keywords, and obtain 74%accuracy when matching all three types of event in-formation.2 Related WorkEvent recognition has been studied in several dif-ferent contexts.
There has been a lot of researchon event extraction, where the goal is to extractfacts about events from text (e.g., (ACE Evaluations,2006; Appelt et al 1993; Riloff, 1996; Yangar-ber et al 2000; Chieu and Ng, 2002; Califf andMooney, 2003; Sudo et al 2003; Stevenson andGreenwood, 2005; Sekine, 2006)).
Although our re-search does not involve extracting facts, event ex-traction systems can also be used to identify sto-ries about a specific type of event.
For example, theMUC-4 evaluation (MUC-4 Proceedings, 1992) in-cluded ?text filtering?
results that measured the per-formance of event extraction systems at identifyingevent-relevant documents.
The best text filtering re-sults were high (about 90% F score), but relied onhand-built event extraction systems.
More recently,some research has incorporated event region detec-tors into event extraction systems to improve extrac-tion performance (Gu and Cercone, 2006; Patward-han and Riloff, 2007; Huang and Riloff, 2011).There has been recent work on event detectionfrom social media sources (Becker et al 2011;Popescu et al 2011).
Some research identifies spe-cific types of events in tweets, such as earthquakes(Sakaki et al 2010) and entertainment events (Ben-son et al 2011).
There has also been work on eventtrend detection (Lampos et al 2010; Mathioudakisand Koudas, 2010) and event prediction through so-cial media, such as predicting elections (Tumasjanet al 2010; Conover et al 2011) or stock mar-ket indicators (Zhang et al 2010).
(Ritter et al2012) generated a calendar of events mentioned ontwitter.
(Metzler et al 2012) proposed structuredretrieval of historical event information over mi-croblog archives by distilling high quality event rep-resentations using a novel temporal query expansiontechnique.Some text classification research has focused onevent categories.
(Riloff and Lehnert, 1994) usedan information extraction system to generate rele-vancy signatures that were indicative of differentevent types.
This work originally relied on man-ually labeled patterns and a hand-crafted semanticdictionary.
Later work (Riloff and Lorenzen, 1999)eliminated the need for the dictionary and labeledpatterns, but still assumed the availability of rele-vant/irrelevant training texts.Event recognition is also related to Topic Detec-tion and Tracking (TDT) (Allan et al 1998; Allan,42Figure 1: Bootstrapped Learning of Event Dictionaries2002) which addresses event-based organization of astream of news stories.
Event recognition is similarto New Event Detection, also called First Story De-tection, which is considered the most difficult TDTtask (Allan et al 2000a).
Typical approaches re-duce documents to a set of features, either as a wordvector (Allan et al 2000b) or a probability distri-bution (Jin et al 1999), and compare the incomingstories to stories that appeared in the past by com-puting similarities between their feature representa-tions.
Recently, event paraphrases (Petrovic et al2012) have been explored to deal with the diversityof event descriptions.
However, the New Event De-tection task differs from our event recognition taskbecause we want to find all stories describing a cer-tain type of event, not just new events.3 Bootstrapped Learning of EventDictionariesOur bootstrapping approach consists of two stagesof learning as shown in Figure 1.
The process be-gins with a few agent seeds, purpose phrase patterns,and unannotated articles selected from a broad-coverage corpus using event keywords.
In the firststage, event expressions are harvested from the sen-tences that have both an agent and a purpose phrasein specific syntactic positions.
In the second stage,new purpose phrases are harvested from sentencesthat contain both an event phrase and an agent, whilenew agent terms are harvested from sentences thatcontain both an event phrase and a purpose phrase.The new terms are added to growing event dictionar-ies, and the bootstrapping process repeats.
Our workfocuses on civil unrest events.3.1 Stage 1: Event Phrase LearningWe first extract potential civil unrest stories from theEnglish Gigaword corpus (Parker et al 2011) usingsix civil unrest keywords.
As explained in Section 1,event keywords are not sufficient to obtain relevantdocuments with high precision, so the extracted sto-ries are a mix of relevant and irrelevant articles.
Ouralgorithm first selects sentences to use for learning,and then harvests event expressions from them.3.1.1 Event Sentence IdentificationThe input in stage 1 consists of a few agent termsand purpose patterns for seeding.
The agent seedsare single nouns, while the purpose patterns areverbs in infinitive or present participle forms.
Table1 shows the agent terms and purpose phrases used inour experiments.
The agent terms were manually se-lected by inspecting the most frequent nouns in thedocuments with civil unrest keywords.
The purposepatterns are the most common verbs that describe thereason for a civil unrest event.
We identify probableevent sentences by extracting all sentences that con-tain at least one agent term and one purpose phrase.Agents protesters, activists, demonstrators,students, groups, crowd, workers,palestinians, supporters, womenPurpose demanding, to demand,Phrases protesting, to protestTable 1: Agent and Purpose Phrases Used for Seeding433.1.2 Harvesting Event ExpressionsTo constrain the learning process, we requireevent expressions and purpose phrases to match cer-tain syntactic structures.
We apply the Stanford de-pendency parser (Marneffe et al 2006) to the prob-able event sentences to identify verb phrase candi-dates and to enforce syntactic constraints betweenthe different types of event information.Figure 2: Phrasal Structure of Event & Purpose PhrasesFigure 2 shows the two types of verb phrasesthat we learn.
One type consists of a verb pairedwith the head noun of its direct object.
For exam-ple, event phrases can be ?stopped work?
or ?oc-cupied offices?, and purpose phrases can be ?showsupport?
or ?condemn war?.
The second type con-sists of a verb and an attached prepositional phrase,retaining only the head noun of the embedded nounphrase.
For example, ?took to street?
and ?scuffledwith police?
can be event phrases, while ?call forresignation?
and ?press for wages?
can be purposephrases.
In both types of verb phrases, a particle canoptionally follow the verb.Event expressions, agents, and purpose phrasesmust appear in specific dependency relations, as il-lustrated in Figure 3.
An agent must be the syn-tactic subject of the event phrase.
A purpose phrasemust be a complement of the event phrase, specif-ically, we require a particular dependency relation,?xcomp?1, between the two verb phrases.
For ex-ample, in the sentence ?Leftist activists took tothe streets in the Nepali capital Wednesday protest-ing higher fuel prices.
?, the dependency relation1In the dependency parser, ?xcomp?
denotes a general rela-tion between a VP or an ADJP and its open clausal complement.For example, in the sentence ?He says that you like to swim.
?,the ?xcomp?
relation will link ?like?
(head) and ?swim?
(de-pendent).
With our constraints on the verb phrase forms, thedependent verb phrase in this construction tends to describe thepurpose of the verb phrase.?xcomp?
links ?took to the streets?
with ?protest-ing higher fuel prices?.Figure 3: Syntactic Dependencies between Agents, EventPhrases, and Purpose PhrasesGiven the syntactic construction shown in Figure3, with a known agent and purpose phrase, we ex-tract the head verb phrase of the ?xcomp?
depen-dency relation as an event phrase candidate.
Theevent phrases that co-occur with at least two uniqueagent terms and two unique purposes phrases aresaved in our event phrase dictionary.3.2 Stage 2: Learning Agent and PurposePhrasesIn the second stage of bootstrapping, we learn newagent terms and purpose phrases.
Our rationale isthat if a sentence contains an event phrase and oneother important facet of the event (agent or pur-pose), then the sentence probably describes a rele-vant event.
We can then look for additional facetsof the event in the same sentence.
We learn bothagent and purpose phrases simultaneously in paral-lel learning processes.
As before, we first identifyprobable event sentences and then harvest agent andpurpose phrases from these sentences.3.2.1 Event Sentence IdentificationWe identify probable event sentences by extract-ing sentences that contain at least one event phrase(based on the dictionary produced in the first stageof bootstrapping) and an agent term or a purposephrase.
As before, the event information must oc-cur in the sentential dependency structures shown inFigure 3.3.2.2 Harvesting Agent and Purpose PhrasesThe sentences that contain an event phrase andan agent are used to harvest more purpose phrases,while the sentences that contain an event phraseand a purpose phrase are used to harvest moreagent terms.
Purpose phrases are extracted from thephrasal structures shown in Figure 2.
In the learn-ing process for agents, if a sentence has an event44phrase as the head of the ?xcomp?
dependency re-lation and a purpose phrase as the dependent clauseof the ?xcomp?
dependency relation, then the headnoun of the syntactic subject of the event phrase isharvested as a candidate agent term.
We also recordthe modifiers appearing in all of the noun phrasesheaded by an agent term.
Agent candidates that co-occur with at least two unique event phrases and atleast two different modifiers of known agent termsare selected as new agent terms.The learning process for purpose phrases is anal-ogous.
If the syntactic subject of an event phraseis an agent and the event phrase is the head ofthe ?xcomp?
dependency relation, then the depen-dent clause of the ?xcomp?
dependency relation isharvested as a candidate purpose phrase.
Purposephrase candidates that co-occur with at least two dif-ferent event phrases are selected as purpose phrases.The bootstrapping process then repeats, ricochet-ing back and forth between learning event phrasesand learning agent and purpose phrases.3.3 Domain Relevance CriteriaTo avoid domain drift during bootstrapping, we usetwo additional criteria to discard phrases that are notnecessarily associated with the domain.For each event phrase and purpose phrase, we es-timate its domain-specificity as the ratio of its preva-lence in domain-specific texts compared to broad-coverage texts.
The goal is to discard phrases thatare common across many types of documents, andtherefore not specific to the domain.
We define thedomain-specificity of phrase p as:domain-specificity(p) = frequency of p in domain-specific corpusfrequency of p in broad-coverage corpusWe randomly sampled 10% of the Gigaword textsthat contain a civil unrest event keyword to createthe ?domain-specific?
corpus, and randomly sam-pled 10% of the remaining Gigaword texts to cre-ate the ?broad-coverage?
corpus.2 Keyword-basedsampling is an approximation to domain-relevance,but gives us a general idea about the prevalance of aphrase in different types of texts.For agent terms, our goal is to identify people whoparticipate as agents of civil unrest events.
Othertypes of people may be commonly mentioned incivil unrest stories too, as peripheral characters.
For2The random sampling was simply for efficiency reasons.example, police may provide security and reportersmay provide media coverage of an event, but theyare not the agents of the event.
We estimate theevent-specificity of each agent term as the ratio ofthe phrase?s prevalence in event sentences comparedto all the sentences in the domain-specific corpus.We define an event sentence as one that containsboth a learned event phrase and a purpose phrase,based on the dictionaries at that point in time.
There-fore, the number of event sentences increases as thebootstrapped dictionaries grow.
We define the event-specificity of phrase p as:event-specificity(p) = frequency of p in event sentencesfrequency of p in all sentencesIn our experiments we required event and purposephrases to have domain-specificity ?
.33 and agentterms to have event-specificity ?
.01.34 Evaluation4.1 DataWe conducted experiments to evaluate the perfor-mance of our bootstrapped event dictionaries for rec-ognizing civil unrest events.
Civil unrest is a broadterm typically used by the media or law enforce-ment to describe a form of public disturbance thatinvolves a group of people, usually to protest or pro-mote a cause.
Civil unrest events include strikes,protests, occupations, rallies, and similar forms ofobstructions or riots.
We chose six event keywords toidentify potential civil unrest documents: ?protest?,?strike?, ?march?, ?rally?, ?riot?
and ?occupy?.
Weextracted documents from the English Gigawordcorpus (Parker et al 2011) that contain at least oneof these event keywords, or a morphological variantof a keyword.4 This process extracted nearly onemillion documents, which we will refer to as ourevent-keyword corpus.We randomly sampled 400 documents5 from theevent-keyword corpus and asked two annotators todetermine whether each document mentioned a civil3This value is so small because we simply want to filterphrases that virtually never occur in the event sentences, andwe can recognize very few event sentences in the early stagesof bootstrapping.4We used ?marched?
and ?marching?
as keywords but didnot use ?march?
because it often refers to a month.5These 400 documents were excluded from the unannotateddata used for dictionary learning.45unrest event.
We defined annotation guidelines andconducted an inter-annotator agreement study on100 of these documents.
The annotators achieved a?
score of .82.
We used these 100 documents as ourtuning set.
Then each annotator annotated 150 moredocuments to create our test set of 300 documents.4.2 BaselinesThe first row of Table 2 shows event recognition ac-curacy when only the event keywords are used.
Allof our documents were obtained by searching for akeyword, but only 101 of the 300 documents in ourtest set were labeled as relevant by the annotators(i.e., 101 describe a civil unrest event).
This meansthat using only the event keywords to identify civilunrest documents yields about 34% precision.
In asecond experiment, KeywordTitle, we required theevent keyword to be in the title (headline) of the doc-ument.
The KeywordTitle approach produced betterprecision (66%), but only 33% of the relevant docu-ments had a keyword in the title.Method Recall Precision FKeyword AccuracyKeyword - 34 -KeywordTitle 33 66 44Supervised LearningUnigrams 62 66 64Unigrams+Bigrams 55 71 62Bootstrapped Dictionary LookupEvent Phrases (EV) 60 79 69Agent Phrases (AG) 98 42 59Purpose Phrases (PU) 59 67 63All Pairs 71 88 79Table 2: Experimental ResultsThe second section of Table 2 shows the re-sults of two supervised classifiers based on 10-foldcross validation with our test set.
Both classifierswere trained using support vector machines (SVMs)(Joachims, 1999) with a linear kernel (Keerthi andDeCoste, 2005).
The first classifier used unigramsas features, while the second classifier used both un-igrams and bigrams.
All the features are binary.
Theevaluation results show that the unigram classifierhas an F-score of .64.
Using both unigram and bi-gram features increased precision to 71% but recallfell by 7%, yielding a slightly lower F-score of .62.4.3 Event Recognition with BootstrappedDictionariesNext, we used our bootstrapped dictionaries forevent recognition.
The bootstrapping process ranfor 8 iterations and then stopped because no morephrases could be learned.
The quality of boot-strapped data often degrades as bootstrapping pro-gresses, so we used the tuning set to evaluate thedictionaries after each iteration.
The best perfor-mance6 on the tuning set resulted from the dictionar-ies produced after four iterations, so we used thesedictionaries for our experiments.
Table 3 shows theEvent Agent PurposePhrases Terms PhrasesIter #1 145 67 124Iter #2 410 106 356Iter #3 504 130 402Iter #4 623 139 569Table 3: Dictionary Sizes after Several Iterationsnumber of event phrases, agents and purpose phraseslearned after each iteration.
All three lexicons weresignificantly enriched after each iteration.
The finalbootstrapped dictionaries contain 623 event phrases,569 purpose phrases and 139 agent terms.
Table 4shows samples from each event dictionary.Event Phrases: went on strike, took to street,chanted slogans, gathered in capital, formed chain,clashed with police, staged rally, held protest,walked off job, burned flags, set fire, hit streets,marched in city, blocked roads, carried placardsAgent Terms: employees, miners, muslims, unions,protestors, journalists, refugees, prisoners, immigrants,inmates, pilots, farmers, followers, teachers, driversPurpose Phrases: accusing government, voice anger,press for wages, oppose plans, urging end, defying ban,show solidarity, mark anniversary, calling for right,condemning act, pressure government, mark death,push for hike, call attention, celebrating withdrawalTable 4: Examples of Dictionary EntriesThe third section of Table 2 shows the resultswhen using the bootstrapped dictionaries for eventrecognition.
We used a simple dictionary look-upapproach that searched for dictionary entries in eachdocument.
Our phrases were generated based on6Based on the performance for the All Pairs approach.46syntactic analysis and only head words were re-tained for generality.
But we wanted to match dic-tionary entries without requiring syntactic analysisof new documents.
So we used an approximatematching scheme that required each word to appearwithin 5 words of the previous word.
For example,?held protest?
would match ?held a large protest?and ?held a very large political protest?.
In this way,we avoid the need for syntactic analysis when usingthe dictionaries for event recognition.First, we labeled a document as relevant if it con-tained any Event Phrase (EV) in our dictionary.
Theevent phrases achieved better performance than allof the baselines, yielding an F-score of 69%.
Thebest baseline was the unigram classifier, which wastrained with supervised learning.
The bootstrappedevent phrase dictionary produced much higher pre-cision (79% vs. 66%) with only slightly lower recall(60% vs. 62%), and did not require annotated textsfor training.
Statistical significance testing showsthat the Event Phrase lookup approach works signif-icantly better than the unigram classifier (p < 0.05,paired bootstrap (Berg-Kirkpatrick et al 2012)).For the sake of completeness, we also evaluatedthe performance of dictionary look-up using ourbootstrapped Agent (AG) and Purpose (PU) dictio-naries, individually.
The agents terms produced 42%precision with 98% recall, demonstrating that thelearned agent list has extremely high coverage but(unsurprisingly) does not achieve high precision onits own.
The purpose phrases achieved a better bal-ance of recall and precision, producing an F-scoreof 63%, which is nearly the same as the supervisedunigram classifier.Our original hypothesis was that a single type ofevent information is not sufficient to accurately iden-tify event descriptions.
Our goal was high-accuracyevent recognition by requiring that a document con-tain multiple clues pertaining to different facets of anevent (multi-faceted event recognition).
The last rowof Table 2 (All Pairs) shows the results when requir-ing matches from at least two different bootstrappeddictionaries.
Specifically, we labeled a documentas relevant if it contained at least one phrase fromeach of two different dictionaries and these phrasesoccurred in the same sentence.
Table 2 shows thatmulti-faceted event recognition achieves 88% preci-sion with reasonably good recall of 71%, yielding anF-score of 79%.
This multi-faceted approach withsimple dictionary look-up outperformed all of thebaselines, and each dictionary used by itself.
Sta-tistical significance testing shows that the All Pairsapproach works significantly better than the unigramclassifier (p < 0.001, paired bootstrap).
The AllPairs approach is significantly better than the EventPhrase (EV) lookup approach at the p < 0.1 level.Method Recall Precision F-scoreEV + PU 14 100 24EV + AG 47 94 62AG + PU 50 85 63All Pairs 71 88 79Table 5: Analysis of Dictionary CombinationsTable 5 takes a closer look at how each pair ofdictionaries performed.
The first row shows that re-quiring a document to have an event phrase and apurpose phrase produces the best precision (100%)but with low recall (14%).
The second row revealsthat requiring a document to have an event phraseand an agent term yields better recall (47%) and highprecision (94%).
The third row shows that requiringa document to have a purpose phrase and an agentterm produces the best recall (50%) but with slightlylower precision (85%).
Finally, the last row of Ta-ble 5 shows that taking the union of these results(i.e., any combination of dictionary pairs is suffi-cient) yields the best recall (71%) with high preci-sion (88%), demonstrating that we get the best cov-erage by recognizing multiple combinations of eventinformation.Lexicon Recall Precision F-scoreSeeds 13 87 22Iter #1 50 88 63Iter #2 63 89 74Iter #3 68 88 77Iter #4 71 88 79Table 6: All Pairs Lookup Results using only Seeds andthe Lexicons Learned after each Iteration, on the Test SetTable 6 shows the performance of the lexiconlookup approach using the All Pairs criteria dur-ing the bootstrapping process.
The first row showsthe results using only 10 agent seeds and 4 purposeseeds as shown in Table 1.
The following four rowsin the table show the performance of All Pairs using47the lexicons learned after each bootstrapping itera-tion.
We can see that the recall increases steadily andthat precision is maintained at a high level through-out the bootstrapping process.Event recognition can be formulated as an infor-mation retrieval (IR) problem.
As another point ofcomparison, we ran an existing IR system, Terrier(Ounis et al 2007), on our test set.
We used Ter-rier to rank these 300 documents given our set ofevent keywords as the query 7, and then generated arecall/precision curve (Figure 4) by computing theprecisions at different levels of recall, ranging from0 to 1 in increments of .10.
Terrier was run with the0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91RecallPrecisionFigure 4: Comparison with the Terrier IR systemparameter PL2 which refers to an advanced Diver-gence From Randomness weighting model (Amatiand Van Rijsbergen, 2002).
In addition, Terrier usedautomatic query expansion.
We can see that Terrieridentified the first 60 documents (20% recall) with100% precision.
But precision dropped sharply afterthat.
The circle in Figure 4 shows the performanceof our bootstrapped dictionaries using the All Pairsapproach.
At comparable level of precision (88%),Terrier achieved about 45% recall versus 71% recallproduced with the bootstrapped dictionaries.4.4 Supervised Classifiers with BootstrappedDictionariesWe also explored the idea of using the bootstrappeddictionaries as features for a classifier to see if a su-pervised learner could make better use of the dic-7We gave Terrier one query with all of the event keywords.tionaries.
We created five SVM classifiers and per-formed 10-fold cross validation on the test set.Method Recall Precision F-scoreTermLex 66 85 74PairLex 10 91 18TermSets 59 83 69PairSets 68 84 75AllSets 70 84 76Table 7: Supervised classifiers using the dictionariesTable 7 shows the results for the five classifiers.TermLex encodes a binary feature for every phrasein any of our dictionaries.
PairLex encodes a binaryfeature for each pair of phrases from two differentdictionaries and requires them to occur in the samesentence.
The TermLex classifier achieves good per-formance (74% F-score), but is not as effective asour All Pairs dictionary look-up approach (79% F-score).
The PairLex classifier yield higher precisionbut very low recall, undoubtedly due to sparsity is-sues in matching specific pairs of phrases.One of the strengths of our bootstrapping methodis that it creates dictionaries from large volumes ofunannotated documents.
A limitation of supervisedlearning with lexical features is that the classifier cannot benefit from terms in the bootstrapped dictionar-ies that do not appear in its training documents.
Toaddress this issue, we also tried encoding the dic-tionaries as set-based features.
The TermSets clas-sifier encodes three binary features, one for eachdictionary.
A feature gets a value of 1 if a docu-ment contains any word in the corresponding dictio-nary.
The PairSets classifier also encodes three bi-nary features, but each feature represents a differentpair of dictionaries (EV+AG, EV+PU, or AG+PU).A feature gets a value of 1 if a document contains atleast one term from each of the two dictionaries inthe same sentence.
The AllSets classifier encodes 7set-based features: the previous six features and oneadditional feature that requires a sentence to containat least one entry from all three dictionaries.The All Sets classifier yields the best performancewith an F-score of 76%.
However, our straightfor-ward dictionary look-up approach still performs bet-ter (79% F-score), and does not require annotateddocuments for training.484.5 Finding Articles with no Event KeywordThe learned event dictionaries have the potential torecognize event-relevant documents that do not con-tain any human-selected event keywords.
This canhappen in two ways.
First, 378 of the 623 learnedevent phrases do not contain any of the original eventkeywords.
Second, we expect that some event de-scriptions will contain a known agent and purposephrase, even if the event phrase is unfamiliar.We performed an additional set of experimentswith documents in the Gigaword corpus that containno human-selected civil unrest keyword.
Followingour multi-faceted approach to event recognition, wecollected all documents that contain a sentence thatmatches phrases in at least two of our bootstrappedevent dictionaries.
This process retrieved 178,197documents.
The first column of Table 8 shows thenumber of documents that had phrases found in twodifferent dictionaries (EV+AG, EV+PU, AG+PU) orin all three dictionaries (EV+AG+PU).Total Samples AccuracyEV+AG 67,796 50 44%EV+PU 2,375 50 54%AG+PU 101,173 50 18%EV+AG+PU 6,853 50 74%Table 8: Evaluation of articles with no event keywordWe randomly sampled 50 documents from eachcategory and had them annotated.
The accura-cies are shown in the third column.
Finding allthree types of phrases produced the best accuracy,74%.
Furthermore, we found over 6,800 documentsthat had all three types of event information us-ing our learned dictionaries.
This result demon-strates that the bootstrapped dictionaries can recog-nize many event descriptions that would have beenmissed by searching only with manually selectedkeywords.
This experiment also confirms that multi-facted event recognition using all three learned dic-tionaries achieves good accuracy even for docu-ments that do not contain the civil unrest keywords.5 ConclusionsWe proposed a multi-faceted approach to eventrecognition and presented a bootstrapping techniqueto learn event phrases as well as agent terms andpurpose phrases associated with civil unrest events.Our results showed that multi-faceted event recog-nition using the learned dictionaries achieved highaccuracy and performed better than several othermethods.
The bootstrapping approach can be eas-ily trained for new domains since it requires onlya large collection of unannotated texts and a fewevent keywords, agent terms, and purpose phrasesfor the events of interest.
Furthermore, although thetraining phase requires syntactic parsing to learn theevent dictionaries, the dictionaries can then be usedfor event recognition without needing to parse thedocuments.An open question for future work is to investigatewhether the same multi-faceted approach to eventrecognition will work well for other types of events.Our belief is that many different types of events havecharacteristic agent terms, but additional types offacets will need to be defined to cover a broad arrayof event types.
The syntactic constructions used toharvest dictionary items may also vary depending onthe types of event information that must be learned.In future research, we plan to explore these issues inmore depth to design a more general multi-facetedevent recognition system, and we plan to investigatenew ways to use these event dictionaries for eventextraction as well.6 AcknowledgmentsThis research was supported by the Intelligence Ad-vanced Research Projects Activity (IARPA) via De-partment of Interior National Business Center (DoI /NBC) contract number D12PC00285 and by the Na-tional Science Foundation under grant IIS-1018314.The U.S. Government is authorized to reproduceand distribute reprints for Governmental purposesnotwithstanding any copyright annotation thereon.The views and conclusions contained herein arethose of the authors and should not be interpreted asnecessarily representing the official policies or en-dorsements, either expressed or implied, of IARPA,DoI/NBE, NSF, or the U.S. Government.49ReferencesACE Evaluations.
2006.http://www.itl.nist.gov/iad/mig/tests/ace/.J.
Allan, J. Carbonell, G. Doddington, J. Yamron, andY.
Yang.
1998.
Topic Detection and Tracking PilotStudy: Final Report.
In Proceedings of DARPA Broad-cast News Transcription and Understanding Work-shop.J.
Allan, V. Lavrenko, and H. Jin.
2000a.
First StoryDetection in TDT is Hard.
In Proceedings of the 2000ACM CIKM International Conference on Informationand Knowledge Management.J.
Allan, Victor Lavrenko, Daniella Malin, and RussellSwan.
2000b.
Detections, Bounds, and Timelines:UMass and TDT-3.
In Proceedings of Topic Detectionand Tracking Workshop.J.
Allan, 2002.
Topic Detection and Tracking: EventBased Information Organization.
Kluwer AcademicPublishers.G.
Amati and C. J.
Van Rijsbergen.
2002.
ProbabilisticModels of Information Retrieval based on MeasuringDivergence from Randomness.
ACM Transactions onInformation Systems, 20(4):357?389.D.
Appelt, J. Hobbs, J.
Bear, D. Israel, and M. Tyson.1993.
FASTUS: a finite-state processor for informa-tion extraction from real-world text.
In Proceedings ofthe Thirteenth International Joint Conference on Arti-ficial Intelligence.H.
Becker, M. Naaman, and L. Gravano.
2011.
Be-yond trending topics: Real-world event identificationon twitter.
In Proceedings of the Fifth InternationalAAAI Conference on Weblogs and Social Media.E.
Benson, A. Haghighi, and R. Barzilay.
2011.
Eventdiscovery in social media feeds.T.
Berg-Kirkpatrick, D. Burkett, and D. Klein.
2012.
AnEmpirical Investigation of Statistical Significance inNLP.
In Proceedings of the 2012 Conference on Em-pirical Methods in Natural Language Processing.M.E.
Califf and R. Mooney.
2003.
Bottom-up RelationalLearning of Pattern Matching rules for InformationExtraction.
Journal of Machine Learning Research,4:177?210.H.L.
Chieu and H.T.
Ng.
2002.
A Maximum En-tropy Approach to Information Extraction from Semi-Structured and Free Text.
In Proceedings of the 18thNational Conference on Artificial Intelligence.M.
D. Conover, J. Ratkiewicz, M. Francisco,B.
Goncalves, A. Flammini, and F. Menczer.
2011.Political Polarization on Twitter.
In Proceedings ofthe Fifth International AAAI Conference on Weblogsand Social Media.Z.
Gu and N. Cercone.
2006.
Segment-Based HiddenMarkov Models for Information Extraction.
In Pro-ceedings of the 21st International Conference on Com-putational Linguistics and 44th Annual Meeting ofthe Association for Computational Linguistics, pages481?488, Sydney, Australia, July.R.
Huang and E. Riloff.
2011.
Peeling Back the Layers:Detecting Event Role Fillers in Secondary Contexts.H.
Jin, R. Schwartz, S. Sista, and F. Walls.
1999.
TopicTracking for Radio, TV broadcast, and Newswire.
InEUROSPEECH.T.
Joachims.
1999.
Making Large-Scale SupportVector Machine Learning Practical.
In A. SmolaB.
Scho?lkopf, C. Burges, editor, Advances in KernelMethods: Support Vector Machines.
MIT Press, Cam-bridge, MA.S.
Keerthi and D. DeCoste.
2005.
A Modified FiniteNewton Method for Fast Solution of Large Scale Lin-ear SVMs.
Journal of Machine Learning Research.V.
Lampos, T. D. Bie, and N. Cristianini.
2010.
FluDetector - Tracking Epidemics on Twitter.
In ECMLPKDD.M.
d. Marneffe, B. MacCartney, and C. D. Manning.2006.
Generating Typed Dependency Parses fromPhrase Structure Parses.
In Proceedings of the FifthConference on Language Resources and Evaluation(LREC-2006).M.
Mathioudakis and N. Koudas.
2010.
TwitterMonitor:trend detection over the twitter stream.
In Proceedingsof the 2010 international conference on Managementof data, page 11551158.
ACM.D.
Metzler, C. Cai, and E. Hovy.
2012.
Structured EventRetrieval over Microblog Archives.
In Proceedings ofThe 2012 Conference of the North American Chapterof the Association for Computational Linguistics: Hu-man Language Technologies.MUC-4 Proceedings.
1992.
Proceedings of the FourthMessage Understanding Conference (MUC-4).
Mor-gan Kaufmann.I.
Ounis, C. Lioma, C. Macdonald, and V. Plachouras.2007.
Research Directions in Terrier.
Novat-ica/UPGRADE Special Issue on Web Information Ac-cess, Ricardo Baeza-Yates et al(Eds), Invited Paper.R.
Parker, D. Graff, J. Kong, K. Chen, and Kazuaki M.2011.
English Gigaword.
In Linguistic Data Consor-tium.S.
Patwardhan and E. Riloff.
2007.
Effective InformationExtraction with Semantic Affinity Patterns and Rele-vant Regions.
In Proceedings of 2007 the Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP-2007).S.
Petrovic, M. Osborne, and V. Lavrenko.
2012.
Us-ing Paraphrases for Improving First Story Detection in50News and Twitter.
In Proceedings of The 2012 Con-ference of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies.A.-M. Popescu, M. Pennacchiotti, and D. A. Paranjpe.2011.
Extracting events and event descriptions fromtwitter.E.
Riloff and W. Lehnert.
1994.
Information Ex-traction as a Basis for High-Precision Text Classifi-cation.
ACM Transactions on Information Systems,12(3):296?333, July.E.
Riloff and J. Lorenzen.
1999.
Extraction-based textcategorization: Generating domain-specific role rela-tionships automatically.
In Tomek Strzalkowski, edi-tor, Natural Language Information Retrieval.
KluwerAcademic Publishers.E.
Riloff.
1996.
Automatically Generating ExtractionPatterns from Untagged Text.
In Proceedings of theThirteenth National Conference on Artificial Intelli-gence, pages 1044?1049.
The AAAI Press/MIT Press.A.
Ritter, Mausam, O. Etzioni, and S. Clark.
2012.
Opendomain event extraction from twitter.
In The Proceed-ings of The 18th ACM SIGKDD Conference on Knowl-edge Discovery and Data Mining.T.
Sakaki, M. Okazaki, and Y. Matsuo.
2010.
Earth-quake shakes twitter users: real-time event detectionby social sensors.S.
Sekine.
2006.
On-demand Information Extrac-tion.
In Proceedings of Joint Conference of the In-ternational Committee on Computational Linguisticsand the Association for Computational Linguistics(COLING/ACL-06).M.
Stevenson and M. Greenwood.
2005.
A Seman-tic Approach to IE Pattern Induction.
In Proceed-ings of the 43rd Annual Meeting of the Association forComputational Linguistics, pages 379?386, Ann Ar-bor, MI, June.K.
Sudo, S. Sekine, and R. Grishman.
2003.
An Im-proved Extraction Pattern Representation Model forAutomatic IE Pattern Acquisition.
In Proceedings ofthe 41st Annual Meeting of the Association for Com-putational Linguistics (ACL-03).A.
Tumasjan, T. O. Sprenger, P. G. Sandner, and I. M.Welpe.
2010.
Predicting Elections with Twitter: What140 Characters Reveal about Political Sentiment.
InProceedings of the 4th International AAAI Conferenceon Weblogs and Social Media.R.
Yangarber, R. Grishman, P. Tapanainen, and S. Hut-tunen.
2000.
Automatic Acquisition of DomainKnowledge for Information Extraction.
In Proceed-ings of the Eighteenth International Conference onComputational Linguistics (COLING 2000).X.
Zhang, H. Fuehres, and P. A. Gloor.
2010.
PredictingStock Market Indicators Through Twitter ?I hope it isnot as bad as I fear?.
In COINs.51
