Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 259?268,Athens, Greece, 30 March ?
31 March 2009. c?2009 Association for Computational LinguisticsFluency, Adequacy, or HTER?Exploring Different Human Judgments with a Tunable MT MetricMatthew Snover?, Nitin Madnani?, Bonnie J. Dorr?
?
& Richard Schwartz?
?
?Laboratory for Computational Linguistics and Information Processing?Institute for Advanced Computer Studies?University of Maryland, College Park?Human Language Technology Center of Excellence?BBN Technologies{snover,nmadnani,bonnie}@umiacs.umd.edu schwartz@bbn.comAbstractAutomatic Machine Translation (MT)evaluation metrics have traditionally beenevaluated by the correlation of the scoresthey assign to MT output with humanjudgments of translation performance.Different types of human judgments, suchas Fluency, Adequacy, and HTER, mea-sure varying aspects of MT performancethat can be captured by automatic MTmetrics.
We explore these differencesthrough the use of a new tunable MT met-ric: TER-Plus, which extends the Transla-tion Edit Rate evaluation metric with tun-able parameters and the incorporation ofmorphology, synonymy and paraphrases.TER-Plus was shown to be one of thetop metrics in NIST?s Metrics MATR2008 Challenge, having the highest aver-age rank in terms of Pearson and Spear-man correlation.
Optimizing TER-Plusto different types of human judgmentsyields significantly improved correlationsand meaningful changes in the weight ofdifferent types of edits, demonstrating sig-nificant differences between the types ofhuman judgments.1 IntroductionSince the introduction of the BLEU metric (Pa-pineni et al, 2002), statistical MT systems havemoved away from human evaluation of their per-formance and towards rapid evaluation using au-tomatic metrics.
These automatic metrics arethemselves evaluated by their ability to generatescores for MT output that correlate well with hu-man judgments of translation quality.
Numer-ous methods of judging MT output by humanshave been used, including Fluency, Adequacy,and, more recently, Human-mediated TranslationEdit Rate (HTER) (Snover et al, 2006).
Fluencymeasures whether a translation is fluent, regard-less of the correct meaning, while Adequacy mea-sures whether the translation conveys the correctmeaning, even if the translation is not fully flu-ent.
Fluency and Adequacy are frequently mea-sured together on a discrete 5 or 7 point scale,with their average being used as a single scoreof translation quality.
HTER is a more complexand semi-automatic measure in which humans donot score translations directly, but rather generatea new reference translation that is closer to theMT output but retains the fluency and meaningof the original reference.
This new targeted refer-ence is then used as the reference translation whenscoring the MT output using Translation Edit Rate(TER) (Snover et al, 2006) or when used withother automatic metrics such as BLEU or ME-TEOR (Banerjee and Lavie, 2005).
One of thedifficulties in the creation of targeted referencesis a further requirement that the annotator attemptto minimize the number of edits, as measured byTER, between the MT output and the targeted ref-erence, creating the reference that is as close aspossible to the MT output while still being ade-quate and fluent.
In this way, only true errors inthe MT output are counted.
While HTER has beenshown to be more consistent and finer grained thanindividual human annotators of Fluency and Ade-quacy, it is much more time consuming and tax-ing on human annotators than other types of hu-man judgments, making it difficult and expensiveto use.
In addition, because HTER treats all editsequally, no distinction is made between serious er-rors (errors in names or missing subjects) and mi-nor edits (such as a difference in verb agreement259or a missing determinator).Different types of translation errors vary in im-portance depending on the type of human judg-ment being used to evaluate the translation.
Forexample, errors in tense might barely affect the ad-equacy of a translation but might cause the trans-lation be scored as less fluent.
On the other hand,deletion of content words might not lower the flu-ency of a translation but the adequacy would suf-fer.
In this paper, we examine these differencesby taking an automatic evaluation metric and tun-ing it to these these human judgments and exam-ining the resulting differences in the parameteri-zation of the metric.
To study this we introducea new evaluation metric, TER-Plus (TERp)1 thatimproves over the existing Translation Edit Rate(TER) metric (Snover et al, 2006), incorporatingmorphology, synonymy and paraphrases, as wellas tunable costs for different types of errors thatallow for easy interpretation of the differences be-tween human judgments.Section 2 summarizes the TER metric and dis-cusses how TERp improves on it.
Correlation re-sults with human judgments, including indepen-dent results from the 2008 NIST Metrics MATRevaluation, where TERp was consistently one ofthe top metrics, are presented in Section 3 to showthe utility of TERp as an evaluation metric.
Thegeneration of paraphrases, as well as the effect ofvarying the source of paraphrases, is discussed inSection 4.
Section 5 discusses the results of tuningTERp to Fluency, Adequacy and HTER, and howthis affects the weights of various edit types.2 TER and TERpBoth TER and TERp are automatic evaluationmetrics for machine translation that score a trans-lation, the hypothesis, of a foreign language text,the source, against a translation of the source textthat was created by a human translator, called areference translation.
The set of possible cor-rect translations is very large?possibly infinite?and any single reference translation is just a sin-gle point in that space.
Usually multiple refer-ence translations, typically 4, are provided to givebroader sampling of the space of correct transla-tions.
Automatic MT evaluation metrics comparethe hypothesis against this set of reference trans-lations and assign a score to the similarity; higher1Named after the nickname??terp?
?of the University ofMaryland, College Park, mascot: the diamondback terrapin.scores are given to hypotheses that are more simi-lar to the references.In addition to assigning a score to a hypothe-sis, the TER metric also provides an alignment be-tween the hypothesis and the reference, enabling itto be useful beyond general translation evaluation.While TER has been shown to correlate well withhuman judgments of translation quality, it has sev-eral flaws, including the use of only a single ref-erence translation and the measuring of similarityonly by exact word matches between the hypoth-esis and the reference.
The handicap of using asingle reference can be addressed by the construc-tion of a lattice of reference translations.
Such atechnique has been used with TER to combine theoutput of multiple translation systems (Rosti et al,2007).
TERp does not utilize this methodology2and instead focuses on addressing the exact match-ing flaw of TER.
A brief description of TER is pre-sented in Section 2.1, followed by a discussion ofhow TERp differs from TER in Section 2.2.2.1 TEROne of the first automatic metrics used to evaluateautomatic machine translation (MT) systems wasWord Error Rate (WER) (Niessen et al, 2000),which is the standard evaluation metric for Au-tomatic Speech Recognition.
WER is computedas the Levenshtein (Levenshtein, 1966) distancebetween the words of the system output and thewords of the reference translation divided by thelength of the reference translation.
Unlike speechrecognition, there are many correct translations forany given foreign sentence.
These correct transla-tions differ not only in their word choice but alsoin the order in which the words occur.
WER isgenerally seen as inadequate for evaluation for ma-chine translation as it fails to combine knowledgefrom multiple reference translations and also failsto model the reordering of words and phrases intranslation.TER addresses the latter failing of WER by al-lowing block movement of words, called shifts.within the hypothesis.
Shifting a phrase has thesame edit cost as inserting, deleting or substitut-ing a word, regardless of the number of wordsbeing shifted.
While a general solution to WERwith block movement is NP-Complete (Lopresti2The technique of combining references in this fashionhas not been evaluated in terms of its benefit when correlatingwith human judgments.
The authors hope to examine andincorporate such a technique in future versions of TERp.260and Tomkins, 1997), TER addresses this by usinga greedy search to select the words to be shifted,as well as further constraints on the words to beshifted.
These constraints are intended to simu-late the way in which a human editor might choosethe words to shift.
For exact details on these con-straints, see Snover et al (2006).
There are otherautomatic metrics that follow the general formu-lation as TER but address the complexity of shift-ing in different ways, such as the CDER evaluationmetric (Leusch et al, 2006).When TER is used with multiple references, itdoes not combine the references.
Instead, it scoresthe hypothesis against each reference individually.The reference against which the hypothesis has thefewest number of edits is deemed the closet refer-ence, and that number of edits is used as the nu-merator for calculating the TER score.
For the de-nominator, TER uses the average number of wordsacross all the references.2.2 TER-PlusTER-Plus (TERp) is an extension of TER thataligns words in the hypothesis and reference notonly when they are exact matches but also whenthe words share a stem or are synonyms.
In ad-dition, it uses probabilistic phrasal substitutionsto align phrases in the hypothesis and reference.These phrases are generated by considering possi-ble paraphrases of the reference words.
Matchingusing stems and synonyms (Banerjee and Lavie,2005) and using paraphrases (Zhou et al, 2006;Kauchak and Barzilay, 2006) have previously beenshown to be beneficial for automatic MT evalu-ation.
Paraphrases have also been shown to beuseful in expanding the number of references usedfor parameter tuning (Madnani et al, 2007; Mad-nani et al, 2008) although they are not used di-rectly in this fashion within TERp.
While all editcosts in TER are constant, all edit costs in TERpare optimized to maximize correlation with humanjudgments.
This is because while a set of constantweights might prove adequate for the purpose ofmeasuring translation quality?as evidenced bycorrelation with human judgments both for TERand HTER?they may not be ideal for maximiz-ing correlation.TERp uses all the edit operations of TER?Matches, Insertions, Deletions, Substitutions andShifts?as well as three new edit operations: StemMatches, Synonym Matches and Phrase Substitu-tions.
TERp identifies words in the hypothesis andreference that share the same stem using the Porterstemming algorithm (Porter, 1980).
Two wordsare determined to be synonyms if they share thesame synonym set according to WordNet (Fell-baum, 1998).
Sequences of words in the referenceare considered to be paraphrases of a sequence ofwords in the hypothesis if that phrase pair occursin the TERp phrase table.
The TERp phrase tableis discussed in more detail in Section 4.With the exception of the phrase substitutions,the cost for all other edit operations is the same re-gardless of what the words in question are.
Thatis, once the edit cost of an operation is determinedvia optimization, that operation costs the same nomatter what words are under consideration.
Thecost of a phrase substitution, on the other hand,is a function of the probability of the paraphraseand the number of edits needed to align the twophrases according to TERp.
In effect, the proba-bility of the paraphrase is used to determine howmuch to discount the alignment of the two phrases.Specifically, the cost of a phrase substitution be-tween the reference phrase, p1 and the hypothesisphrase p2 is:cost(p1, p2) =w1+edit(p1, p2)?
(w2 log(Pr(p1, p2))+ w3 Pr(p1, p2) + w4)where w1, w2, w3, and w4 are the 4 free param-eters of the edit cost, edit(p1, p2) is the edit costaccording to TERp of aligning p1 to p2 (excludingphrase substitutions) and Pr(p1, p2) is the prob-ability of paraphrasing p1 as p2, obtained fromthe TERp phrase table.
The w parameters of thephrase substitution cost may be negative while stillresulting in a positive phrase substitution cost, asw2 is multiplied by the log probability, which is al-ways a negative number.
In practice this term willdominate the phrase substitution edit cost.This edit cost for phrasal substitutions is, there-fore, specified by four parameters, w1, w2, w3and w4.
Only paraphrases specified in the TERpphrase table are considered for phrase substitu-tions.
In addition, the cost for a phrasal substi-tution is limited to values greater than or equal to0, i.e., the substitution cost cannot be negative.
Inaddition, the shifting constraints of TERp are alsorelaxed to allow shifting of paraphrases, stems,and synonyms.261In total TERp uses 11 parameters out of whichfour represent the cost of phrasal substitutions.The match cost is held fixed at 0, so that only the10 other parameters can vary during optimization.All edit costs, except for the phrasal substitutionparameters, are also restricted to be positive.
Asimple hill-climbing search is used to optimize theedit costs by maximizing the correlation of humanjudgments with the TERp score.
These correla-tions are measured at the sentence, or segment,level.
Although it was done for the experimentsdescribed in this paper, optimization could alsobe performed to maximize document level correla-tion ?
such an optimization would give decreasedweight to shorter segments as compared to the seg-ment level optimization.3 Correlation ResultsThe optimization of the TERp edit costs, and com-parisons against several standard automatic eval-uation metrics, using human judgments of Ade-quacy is first described in Section 3.1.
We thensummarize, in Section 3.2, results of the NISTMetrics MATR workshop where TERp was eval-uated as one of 39 automatic metrics using manytest conditions and types of human judgments.3.1 Optimization of Edit Costs andCorrelation ResultsAs part of the 2008 NIST Metrics MATR work-shop (Przybocki et al, 2008), a development sub-set of translations from eight Arabic-to-EnglishMT systems submitted to NIST?s MTEval 2006was released that had been annotated for Ade-quacy.
We divided this development set into anoptimization set and a test set, which we then usedto optimize the edit costs of TERp and compare itagainst other evaluation metrics.
TERp was op-timized to maximize the segment level Pearsoncorrelation with adequacy on the optimization set.The edit costs determined by this optimization areshown in Table 1.We can compare TERp with other metrics bycomparing their Pearson and Spearman corre-lations with Adequacy, at the segment, docu-ment and system level.
Document level Ade-quacy scores are determined by taking the lengthweighted average of the segment level scores.
Sys-tem level scores are determined by taking theweighted average of the document level scores inthe same manner.We compare TERp with BLEU (Papineni et al,2002), METEOR (Banerjee and Lavie, 2005), andTER (Snover et al, 2006).
The IBM version ofBLEU was used in case insensitive mode withan ngram-size of 4 to calculate the BLEU scores.Case insensitivity was used with BLEU as it wasfound to have much higher correlation with Ade-quacy.
In addition, we also examined BLEU usingan ngram-size of 2 (labeled as BLEU-2), insteadof the default ngram-size of 4, as it often has ahigher correlation with human judgments.
Whenusing METEOR, the exact matching, porter stem-ming matching, and WordNet synonym matchingmodules were used.
TER was also used in caseinsensitive mode.We show the Pearson and Spearman correlationnumbers of TERp and the other automatic metricson the optimization set and the test set in Tables 2and 3.
Correlation numbers that are statisticallyindistinguishable from the highest correlation, us-ing a 95% confidence interval, are shown in boldand numbers that are actually not statistically sig-nificant correlations are marked with a ?.
TERphas the highest Pearson correlation in all condi-tions, although not all differences are statisticallysignificant.
When examining the Spearman cor-relation, TERp has the highest correlation on thesegment and system levels, but performs worsethan METEOR on the document level Spearmancorrelatons.3.2 NIST Metrics MATR 2008 ResultsTERp was one of 39 automatic metrics evaluatedin the 2008 NIST Metrics MATR Challenge.
Inorder to evaluate the state of automatic MT eval-uation, NIST tested metrics across a number ofconditions across 8 test sets.
These conditions in-cluded segment, document and system level corre-lations with human judgments of preference, flu-ency, adequacy and HTER.
The test sets includedtranslations from Arabic-to-English, Chinese-to-English, Farsi-to-English, Arabic-to-French, andEnglish-to-French MT systems involved in NIST?sMTEval 2008, the GALE (Olive, 2005) Phase 2and Phrase 2.5 program, Transtac January and July2007, and CESTA run 1 and run 2, covering mul-tiple genres.
The version of TERp submitted tothis workshop was optimized as described in Sec-tion 3.1.
The development data upon which TERpwas optimized was not part of the test sets evalu-ated in the Challenge.262Phrase SubstitutionMatch Insert Deletion Subst.
Stem Syn.
Shift w1 w2 w3 w40.0 0.26 1.43 1.56 0.0 0.0 0.56 -0.23 -0.15 -0.08 0.18Table 1: Optimized TERp Edit CostsOptimization Set Test Set Optimization+TestMetric Seg Doc Sys Seg Doc Sys Seg Doc SysBLEU 0.623 0.867 0.952 0.563 0.852 0.948 0.603 0.861 0.954BLEU-2 0.661 0.888 0.946 0.591 0.876 0.953 0.637 0.883 0.952METEOR 0.731 0.894 0.952 0.751 0.904 0.957 0.739 0.898 0.958TER -0.609 -0.864 -0.957 -0.607 -0.860 -0.959 -0.609 -0.863 -0.961TERp -0.782 -0.912 -0.996 -0.787 -0.918 -0.985 -0.784 -0.914 -0.994Table 2: Optimization & Test Set Pearson Correlation ResultsDue to the wealth of testing conditions, a sim-ple overall view of the official MATR08 results re-leased by NIST is difficult.
To facilitate this anal-ysis, we examined the average rank of each metricacross all conditions, where the rank was deter-mined by their Pearson and Spearman correlationwith human judgments.
To incorporate statisticalsignificance, we calculated the 95% confidence in-terval for each correlation coefficient and foundthe highest and lowest rank from which the cor-relation coefficient was statistically indistinguish-able, resulting in lower and upper bounds of therank for each metric in each condition.
The aver-age lower bound, actual, and upper bound ranks(where a rank of 1 indicates the highest correla-tion) of the top metrics, as well as BLEU and TER,are shown in Table 4, sorted by the average upperbound Pearson correlation.
Full descriptions of theother metrics3, the evaluation results, and the testset composition are available from NIST (Przy-bocki et al, 2008).This analysis shows that TERp was consistentlyone of the top metrics across test conditions andhad the highest average rank both in terms of Pear-son and Spearman correlations.
While this anal-ysis is not comprehensive, it does give a generalidea of the performance of all metrics by syn-thesizing the results into a single table.
Thereare striking differences between the Spearman andPearson correlations for other metrics, in particu-lar the CDER metric (Leusch et al, 2006) had thesecond highest rank in Spearman correlations (af-3System description of metrics are also distributedby AMTA: http://www.amtaweb.org/AMTA2008.htmlter TERp), but was the sixth ranked metric accord-ing to the Pearson correlation.
In several cases,TERp was not the best metric (if a metric was thebest in all conditions, its average rank would be 1),although it performed well on average.
In partic-ular, TERp did significantly better than the TERmetric, indicating the benefit of the enhancementsmade to TER.4 ParaphrasesTERp uses probabilistic phrasal substitutions toalign phrases in the hypothesis with phrases in thereference.
It does so by looking up?in a pre-computed phrase table?paraphrases of phrases inthe reference and using its associated edit cost asthe cost of performing a match against the hy-pothesis.
The paraphrases used in TERp were ex-tracted using the pivot-based method as describedin (Bannard and Callison-Burch, 2005) with sev-eral additional filtering mechanisms to increasethe precision.
The pivot-based method utilizes theinherent monolingual semantic knowledge frombilingual corpora: we first identify English-to-Fphrasal correspondences, then map from Englishto English by following translation units from En-glish to F and back.
For example, if the two En-glish phrases e1 and e2 both correspond to thesame foreign phrase f, then they may be consid-ered to be paraphrases of each other with the fol-lowing probability:p(e1|e2) ?
p(e1|f) ?
p(f |e2)If there are several pivot phrases that link the twoEnglish phrases, then they are all used in comput-263Optimization Set Test Set Optimization+TestMetric Seg Doc Sys Seg Doc Sys Seg Doc SysBLEU 0.635 0.816 0.714?
0.550 0.740 0.690?
0.606 0.794 0.738?BLEU-2 0.643 0.823 0.786?
0.558 0.747 0.690?
0.614 0.799 0.738?METEOR 0.729 0.886 0.881 0.727 0.853 0.738?
0.730 0.876 0.922TER -0.630 -0.794 -0.810?
-0.630 -0.797 -0.667?
-0.631 -0.801 -0.786?TERp -0.760 -0.834 -0.976 -0.737 -0.818 -0.881 -0.754 -0.834 -0.929Table 3: MT06 Dev.
Optimization & Test Set Spearman Correlation ResultsMetric Average Rank by Pearson Average Rank by SpearmanTERp 1.49 6.07 17.31 1.60 6.44 17.76METEOR v0.7 1.82 7.64 18.70 1.73 8.21 19.33METEOR ranking 2.39 9.45 19.91 2.18 10.18 19.67METEOR v0.6 2.42 10.67 19.11 2.47 11.27 19.60EDPM 2.45 8.21 20.97 2.79 7.61 20.52CDER 2.93 8.53 19.67 1.69 8.00 18.80BleuSP 3.67 9.93 21.40 3.16 8.29 20.80NIST-v11b 3.82 11.13 21.96 4.64 12.29 23.38BLEU-1 (IBM) 4.42 12.47 22.18 4.98 14.87 24.00BLEU-4 (IBM) 6.93 15.40 24.69 6.98 14.38 25.11TER v0.7.25 8.87 16.27 25.29 6.93 17.33 24.80BLEU-4 v12 (NIST) 10.16 18.02 27.64 10.96 17.82 28.16Table 4: Average Metric Rank in NIST Metrics MATR 2008 Official Resultsing the probability:p(e1|e2) ?
?f ?p(e1|f ?)
?
p(f ?|e2)The corpus used for extraction was an Arabic-English newswire bitext containing a million sen-tences.
A few examples of the extracted para-phrase pairs that were actually used in a run ofTERp on the Metrics MATR 2008 developmentset are shown below:(brief ?
short)(controversy over?
polemic about)(by using power?
by force)(response?
reaction)A discussion of paraphrase quality is presentedin Section 4.1, followed by a brief analysis of theeffect of varying the pivot corpus used by the auto-matic paraphrase generation upon the correlationperformance of the TERp metric in Section 4.2.4.1 Analysis of Paraphrase QualityWe analyzed the utility of the paraphrase probabil-ity and found that it was not always a very reliableestimate of the degree to which the pair was se-mantically related.
For example, we looked at allparaphrase pairs that had probabilities greater than0.9, a set that should ideally contain pairs that areparaphrastic to a large degree.
In our analysis, wefound the following five kinds of paraphrases inthis set:(a) Lexical Paraphrases.
These paraphrasepairs are not phrasal paraphrases but insteaddiffer in at most one word and may be con-sidered as lexical paraphrases for all practicalpurposes.
While these pairs may not be veryvaluable for TERp due to the obvious overlapwith WordNet, they may help in increasingthe coverage of the paraphrastic phenomenathat TERp can handle.
Here are some exam-ples:(2500 polish troops?
2500 polish soldiers)(accounting firms?
auditing firms)(armed source?
military source)(b) Morphological Variants.
These phrasalpairs only differ in the morphological form264for one of the words.
As the examples show,any knowledge that these pairs may provideis already available to TERp via stemming.
(50 ton?
50 tons)(caused clouds?
causing clouds)(syria deny?
syria denies)(c) Approximate Phrasal Paraphrases.
Thisset included pairs that only shared partial se-mantic content.
Most paraphrases extractedby the pivot method are expected to be of thisnature.
These pairs are not directly beneficialto TERp since they cannot be substituted foreach other in all contexts.
However, the factthat they share at least some semantic contentdoes suggest that they may not be entirelyuseless either.
Examples include:(mutual proposal?
suggest)(them were exiled?
them abroad)(my parents?
my father)(d) Phrasal Paraphrases.
We did indeed finda large number of pairs in this set that weretruly paraphrastic and proved the most usefulfor TERp.
For example:(agence presse?
news agency)(army roadblock?
military barrier)(staff walked out?
team withdrew)(e) Noisy Co-occurrences.
There are also pairsthat are completely unrelated and happento be extracted as paraphrases based on thenoise inherent in the pivoting process.
Thesepairs are much smaller in number than thefour sets described above and are not signif-icantly detrimental to TERp since they arerarely chosen for phrasal substitution.
Exam-ples:(counterpart salam?
peace)(regulation dealing?
list)(recall one?
deported)Given this distribution of the pivot-based para-phrases, we experimented with a variant of TERpthat did not use the paraphrase probability at allbut instead only used the actual edit distance be-tween the two phrases to determine the final costof a phrase substitution.
The results for this exper-iment are shown in the second row of Table 5.
Wecan see that this variant works as well as the fullversion of TERp that utilizes paraphrase probabil-ities.
This confirms our intuition that the proba-bility computed via the pivot-method is not a veryuseful predictor of semantic equivalence for use inTERp.4.2 Varying Paraphrase Pivot CorporaTo determine the effect that the pivot languagemight have on the quality and utility of the ex-tracted paraphrases in TERp, we used paraphrasepairsmade available by Callison-Burch (2008).These paraphrase pairs were extracted from Eu-roparl data using each of 10 European languages(German, Italian, French etc.)
as a pivot languageseparately and then combining the extracted para-phrase pairs.
Callison-Burch (2008) also extractedand made available syntactically constrained para-phrase pairs from the same data that are morelikely to be semantically related.We used both sets of paraphrases in TERp as al-ternatives to the paraphrase pairs that we extractedfrom the Arabic newswire bitext.
The results areshown in the last four rows of Table 5 and showthat using a pivot language other than the one thatthe MT system is actually translating yields resultsthat are almost as good.
It also shows that thesyntactic constraints imposed by Callison-Burch(2008) on the pivot-based paraphrase extractionprocess are useful and yield improved results overthe baseline pivot-method.
The results further sup-port our claim that the pivot paraphrase probabilityis not a very useful indicator of semantic related-ness.5 Varying Human JudgmentsTo evaluate the differences between human judg-ment types we first align the hypothesis to the ref-erences using a fixed set of edit costs, identical tothe weights in Table 1, and then optimize the editcosts to maximize the correlation, without realign-ing.
The separation of the edit costs used for align-ment from those used for scoring allows us to re-move the confusion of edit costs selected for align-ment purposes from those selected to increase cor-relation.For Adequacy and Fluency judgments, theMTEval 2002 human judgement set4 was used.This set consists of the output of ten MT sys-tems, 3 Arabic-to-English systems and 7 Chinese-4Distributed to the authors by request from NIST.265Pearson SpearmanParaphrase Setup Seg Doc Sys Seg Doc SysArabic pivot -0.787 -0.918 -0.985 -0.737 -0.818 -0.881Arabic pivot and no prob -0.787 -0.933 -0.986 -0.737 -0.841 -0.881Europarl pivot -0.775 -0.940 -0.983 -0.738 -0.865 -0.905Europarl pivot and no prob -0.775 -0.940 -0.983 -0.737 -0.860 -0.905Europarl pivot and syntactic constraints -0.781 -0.941 -0.985 -0.739 -0.859 -0.881Europarl pivot, syntactic constraints and no prob -0.779 -0.946 -0.985 -0.737 -0.866 -0.976Table 5: Results on the NIST MATR 2008 test set for several variations of paraphrase usage.Human Phrase SubstitutionJudgment Match Insert Deletion Subst.
Stem Syn.
Shift w1 w2 w3 w4Alignment 0.0 0.26 1.43 1.56 0.0 0.0 0.56 -0.23 -0.15 -0.08 0.18Adequacy 0.0 0.18 1.42 1.71 0.0 0.0 0.19 -0.38 -0.03 0.22 0.47Fluency 0.0 0.12 1.37 1.81 0.0 0.0 0.43 -0.63 -0.07 0.12 0.46HTER 0.0 0.84 0.76 1.55 0.90 0.75 1.07 -0.03 -0.17 -0.08 -0.09Table 6: Optimized Edit Coststo-English systems, consisting of a total, acrossall systems and both language pairs, of 7,452 seg-ments across 900 documents.
To evaluate HTER,the GALE (Olive, 2005) 2007 (Phase 2.0) HTERscores were used.
This set consists of the out-put of 6 MT systems, 3 Arabic-to-English systemsand 3 Chinese-to-English systems, although eachof the systems in question is the product of systemcombination.
The HTER data consisted of a total,across all systems and language pairs, of 16,267segments across a total of 1,568 documents.
Be-cause HTER annotation is especially expensiveand difficult, it is rarely performed, and the onlysource, to the authors?
knowledge, of availableHTER annotations is on GALE evaluation data forwhich no Fluency and Adequacy judgments havebeen made publicly available.The edit costs learned for each of these humanjudgments, along with the alignment edit costs areshown in Table 6.
While all three types of humanjudgements differ from the alignment costs usedin alignment, the HTER edit costs differ most sig-nificantly.
Unlike Adequacy and Fluency whichhave a low edit cost for insertions and a very highcost for deletions, HTER has a balanced cost forthe two edit types.
Inserted words are strongly pe-nalized against in HTER, as opposed to in Ade-quacy and Fluency, where such errors are largelyforgiven.
Stem and synonym edits are also penal-ized against while these are considered equivalentto a match for both Adequacy and Fluency.
Thispenalty against stem matches can be attributed toFluency requirements in HTER that specificallypenalize against incorrect morphology.
The costof shifts is also increased in HTER, strongly penal-izing the movement of phrases within the hypoth-esis, while Adequacy and Fluency give a muchlower cost to such errors.
Some of the differencesbetween HTER and both fluency and adequacycan be attributed to the different systems used.
TheMT systems evaluated with HTER are all highlyperforming state of the art systems, while the sys-tems used for adequacy and fluency are older MTsystems.The differences between Adequacy and Fluencyare smaller, but there are still significant differ-ences.
In particular, the cost of shifts is over twiceas high for the fluency optimized system than theadequacy optimized system, indicating that themovement of phrases, as expected, is only slightlypenalized when judging meaning, but can be muchmore harmful to the fluency of a translation.
Flu-ency however favors paraphrases more stronglythan the edit costs optimized for adequacy.
Thismight indicate that paraphrases are used to gener-ate a more fluent translation although at the poten-tial loss of meaning.2666 DiscussionWe introduced a new evaluation metric, TER-Plus,and showed that it is competitive with state-of-the-art evaluation metrics when its predictions are cor-related with human judgments.
The inclusion ofstem, synonym and paraphrase edits allows TERpto overcome some of the weaknesses of the TERmetric and better align hypothesized translationswith reference translations.
These new edit costscan then be optimized to allow better correlationwith human judgments.
In addition, we have ex-amined the use of other paraphrasing techniques,and shown that the paraphrase probabilities esti-mated by the pivot-method may not be fully ad-equate for judgments of whether a paraphrase ina translation indicates a correct translation.
Thisline of research holds promise as an external eval-uation method of various paraphrasing methods.However promising correlation results for anevaluation metric may be, the evaluation of thefinal output of an MT system is only a portionof the utility of an automatic translation metric.Optimization of the parameters of an MT systemis now done using automatic metrics, primarilyBLEU.
It is likely that some features that make anevaluation metric good for evaluating the final out-put of a system would make it a poor metric for usein system tuning.
In particular, a metric may havedifficulty distinguishing between outputs of an MTsystem that been optimized for that same metric.BLEU, the metric most frequently used to opti-mize systems, might therefore perform poorly inevaluation tasks compared to recall oriented met-rics such as METEOR and TERp (whose tuningin Table 1 indicates a preference towards recall).Future research into the use of TERp and othermetrics as optimization metrics is needed to betterunderstand these metrics and the interaction withparameter optimization.Finally, we explored the difference betweenthree types of human judgments that are oftenused to evaluate both MT systems and automaticmetrics, by optimizing TERp to these humanjudgments and examining the resulting edit costs.While this can make no judgement as to the pref-erence of one type of human judgment over an-other, it indicates differences between these hu-man judgment types, and in particular the differ-ence between HTER and Adequacy and Fluency.This exploration is limited by the the lack of alarge amount of diverse data annotated for all hu-man judgment types, as well as the small num-ber of edit types used by TERp.
The inclusionof additional more specific edit types could leadto a more detailed understanding of which trans-lation phenomenon and translation errors are mostemphasized or ignored by which types of humanjudgments.AcknowledgmentsThis work was supported, in part, by BBN Tech-nologies under the GALE Program, DARPA/IPTOContract No.
HR0011-06-C-0022 and in part bythe Human Language Technology Center of Ex-cellence.. TERp is available on the web for down-load at: http://www.umiacs.umd.edu/?snover/terp/.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
In Pro-ceedings of the ACL 2005 Workshop on Intrinsic andExtrinsic Evaulation Measures for MT and/or Sum-marization.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with Bilingual Parallel Corpora.
In Pro-ceedings of the 43rd Annual Meeting of the Asso-ciation for Computational Linguistics (ACL 2005),pages 597?604, Ann Arbor, Michigan, June.Chris Callison-Burch.
2008.
Syntactic constraints onparaphrases extracted from parallel corpora.
In Pro-ceedings of the 2008 Conference on Empirical Meth-ods in Natural Language Processing, pages 196?205, Honolulu, Hawaii, October.
Association forComputational Linguistics.Christiane Fellbaum.
1998.
WordNet: AnElectronic Lexical Database.
MIT Press.http://www.cogsci.princeton.edu/?wn [2000,September 7].David Kauchak and Regina Barzilay.
2006.
Para-phrasing for Automatic Evaluation.
In Proceedingsof the Human Language Technology Conference ofthe North American Chapter of the ACL, pages 455?462.Gregor Leusch, Nicola Ueffing, and Hermann Ney.2006.
CDER: Efficient MT Evaluation Using BlockMovements.
In Proceedings of the 11th Confer-enceof the European Chapter of the Association forComputational Linguistics (EACL 2006).V.
I. Levenshtein.
1966.
Binary Codes Capable of Cor-recting Deletions, Insertions, and Reversals.
SovietPhysics Doklady, 10:707?710.267Daniel Lopresti and Andrew Tomkins.
1997.
Blockedit models for approximate string matching.
Theo-retical Computer Science, 181(1):159?179, July.Nitin Madnani, Necip Fazil Ayan, Philip Resnik, andBonnie J. Dorr.
2007.
Using paraphrases for pa-rameter tuning in statistical machine translation.
InProceedings of the Workshop on Statistical MachineTranslation, Prague, Czech Republic, June.
Associ-ation for Computational Linguistics.Nitin Madnani, Philip Resnik, Bonnie J. Dorr, andRichard Schwartz.
2008.
Are Multiple ReferenceTranslations Necessary?
Investigating the Valueof Paraphrased Reference Translations in ParameterOptimization.
In Proceedings of the Eighth Confer-ence of the Association for Machine Translation inthe Americas, October.S.
Niessen, F.J. Och, G. Leusch, and H. Ney.
2000.
Anevaluation tool for machine translation: Fast evalua-tion for MT research.
In Proceedings of the 2nd In-ternational Conference on Language Resources andEvaluation (LREC-2000), pages 39?45.Joseph Olive.
2005.
Global Autonomous LanguageExploitation (GALE).
DARPA/IPTO Proposer In-formation Pamphlet.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for AutomaticEvaluation of Machine Traslation.
In Proceedingsof the 40th Annual Meeting of the Association forComputational Linguistics.Martin F. Porter.
1980.
An algorithm for suffic strip-ping.
Program, 14(3):130?137.Mark Przybocki, Kay Peterson, and Sebas-tian Bronsart.
2008.
Official resultsof the NIST 2008 ?Metrics for MAchineTRanslation?
Challenge (MetricsMATR08).http://nist.gov/speech/tests/metricsmatr/2008/results/,October.Antti-Veikko Rosti, Spyros Matsoukas, and RichardSchwartz.
2007.
Improved word-level system com-bination for machine translation.
In Proceedingsof the 45th Annual Meeting of the Association ofComputational Linguistics, pages 312?319, Prague,Czech Republic, June.
Association for Computa-tional Linguistics.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human An-notation.
In Proceedings of Association for MachineTranslation in the Americas.Liang Zhou, Chon-Yew Lin, and Eduard Hovy.
2006.Re-evaluating Machine Translation Results withParaphrase Support.
In Proceedings of the 2006Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP 2006), pages 77?84.268
