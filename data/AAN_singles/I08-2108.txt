MRD-based Word Sense Disambiguation: Further#2 Extending#1 LeskTimothy Baldwin,?
Su Nam Kim,?
Francis Bond,?
Sanae Fujita,?David Martinez?
and Takaaki Tanaka??
CSSEUniversity of MelbourneVIC 3010 Australia?
NICT3-5 Hikaridai, Seika-choSoraku-gun, Kyoto619-0289 Japan?
NTT CS Labs2-4 Hikari-dai, Seika-choSoraku-gun, Kyoto619-0237 JapanAbstractThis paper reconsiders the task of MRD-based word sense disambiguation, in extend-ing the basic Lesk algorithm to investigatethe impact onWSD performance of differenttokenisation schemes, scoring mechanisms,methods of gloss extension and filteringmethods.
In experimentation over the Lex-eed Sensebank and the Japanese Senseval-2 dictionary task, we demonstrate that char-acter bigrams with sense-sensitive gloss ex-tension over hyponyms and hypernyms en-hances WSD performance.1 IntroductionThe aim of this work is to develop and extend wordsense disambiguation (WSD) techniques to be ap-plied to all words in a text.
The goal of WSD isto link occurrences of ambiguous words in specificcontexts to their meanings, usually represented bya machine readable dictionary (MRD) or a similarlexical repository.
For instance, given the followingJapanese input:(1) ?????quiet?dog?ACC???
?want to keep?
(I) want to keep a quiet dog?we would hope to identify each component word asoccurring with the sense corresponding to the indi-cated English glosses.WSD systems can be classified according to theknowledge sources they use to build their models.
Atop-level distinction is made between supervised andunsupervised systems.
The former rely on traininginstances that have been hand-tagged, while the lat-ter rely on other types of knowledge, such as lexicaldatabases or untagged corpora.
The Senseval evalu-ation tracks have shown that supervised systems per-form better when sufficient training data is available,but they do not scale well to all words in context.This is known as the knowledge acquisition bottle-neck, and is the main motivation behind research onunsupervised techniques (Mihalcea and Chklovski,2003).In this paper, we aim to exploit an existing lexicalresource to build an all-words Japanese word-sensedisambiguator.
The resource in question is the Lex-eed Sensebank (Tanaka et al, 2006) and consists ofthe 28,000 most familiar words of Japanese, each ofwhich has one or more basic senses.
The senses takethe form of a dictionary definition composed fromthe closed vocabulary of the 28,000 words containedin the dictionary, each of which is further manuallysense annotated according to the Lexeed sense in-ventory.
Lexeed also has a semi-automatically con-structed ontology.Through the Lexeed sensebank, we investigate anumber of areas of general interest to theWSD com-munity.
First, we test extensions of the Lesk algo-rithm (Lesk, 1986) over Japanese, focusing specif-ically on the impact of the overlap metric and seg-ment representation on WSD performance.
Second,we propose further extensions of the Lesk algorithmthat make use of disambiguated definitions.
In this,we shed light on the relative benefits we can expectfrom hand-tagging dictionary definitions, i.e.
in in-troducing ?semi-supervision?
to the disambiguationtask.
The proposed method is language independent,and is equally applicable to the Extended WordNet1for English, for example.2 Related workOur work focuses on unsupervised and semi-supervised methods that target al words and partsof speech (POS) in context.
We use the term?unsupervised?
to refer to systems that do notuse hand-tagged example sets for each word, inline with the standard usage in the WSD litera-ture (Agirre and Edmonds, 2006).
We blur the su-pervised/unsupervised boundary somewhat in com-bining the basic unsupervised methods with hand-tagged definitions from Lexeed, in order to measurethe improvement we can expect from sense-taggeddata.
We qualify our use of hand-tagged definition1 http://xwn.hlt.utdallas.edu775sentences by claiming that this kind of resource isless costly to produce than sense-annotated open textbecause: (1) the effects of discourse are limited, (2)syntax is relatively simple, (3) there is significant se-mantic priming relative to the word being defined,and (4) there is generally explicit meta-tagging ofthe domain in technical definitions.
In our experi-ments, we will make clear when hand-tagged senseinformation is being used.Unsupervised methods rely on different knowl-edge sources to build their models.
Primarilythe following types of lexical resources have beenused for WSD: MRDs, lexical ontologies, and un-tagged corpora (monolingual corpora, second lan-guage corpora, and parallel corpora).
Althoughearly approaches focused on exploiting a single re-source (Lesk, 1986), recent trends show the bene-fits of combining different knowledge sources, suchas hierarchical relations from an ontology and un-tagged corpora (McCarthy et al, 2004).
In this sum-mary, we will focus on a few representative systemsthat make use of different resources, noting that thisis an area of very active research which we cannotdo true justice to within the confines of this paper.The Lesk method (Lesk, 1986) is an MRD-basedsystem that relies on counting the overlap betweenthe words in the target context and the dictionarydefinitions of the senses.
In spite of its simplicity,it has been shown to be a hard baseline for unsu-pervised methods in Senseval, and it is applicable toall-words with minimal effort.
Banerjee and Peder-sen (2002) extended the Lesk method for WordNet-based WSD tasks, to include hierarchical data fromthe WordNet ontology (Fellbaum, 1998).
They ob-served that the hierarchical relations significantlyenhance the basic model.
Both these methods willbe described extensively in Section 3.1, as our ap-proach is based on them.Other notable unsupervised and semi-supervisedapproaches are those of McCarthy et al (2004), whocombine ontological relations and untagged corporato automatically rank word senses in relation to acorpus, and Leacock et al (1998) who use untaggeddata to build sense-tagged data automatically basedon monosemous words.
Parallel corpora have alsobeen used to avoid the need for hand-tagged data,e.g.
by Chan and Ng (2005).3 BackgroundAs background to our work, we first describe the ba-sic and extended Lesk algorithms that form the coreof our approach.
Then we present the Lexeed lex-ical resource we have used in our experiments, andfinally we outline aspects of Japanese relevant forthis work.3.1 Basic and Extended LeskThe original Lesk algorithm (Lesk, 1986) performsWSD by calculating the relative word overlap be-tween the context of usage of a target word, and thedictionary definition of each of its senses in a givenMRD.
The sense with the highest overlap is then se-lected as the most plausible hypothesis.An obvious shortcoming of the original Lesk al-gorithm is that it requires that the exact words usedin the definitions be included in each usage of thetarget word.
To redress this shortcoming, Banerjeeand Pedersen (2002) extended the basic algorithmfor WordNet-based WSD tasks to include hierarchi-cal information, i.e.
expanding the definitions to in-clude definitions of hypernyms and hyponyms of thesynset containing a given sense, and assigning thesame weight to the words sourced from the differentdefinitions.Both of these methods can be formalised accord-ing to the following algorithm, which also forms thebasis of our proposed method:for each word wi in context w = w1w2...wn dofor each sense si,j and definition di,j of wi doscore(si,j) = overlap(w, di,j)end fors?i = arg maxj score(si,j)end for3.2 The Lexeed SensebankAll our experimentation is based on the LexeedSensebank (Tanaka et al, 2006).
The Lexeed Sense-bank consists of all Japanese words above a certainlevel of familiarity (as defined by Kasahara et al(2004)), giving rise to 28,000 words in all, with a to-tal of 46,000 senses which are similarly filtered forsimilarity.
The sense granularity is relatively coarsefor most words, with the possible exception of lightverbs, making it well suited to open-domain appli-cations.
Definition sentences for these senses wererewritten to use only the closed vocabulary of the28,000 familiar words (and some function words).Additionally, a single example sentence was man-ually constructed to exemplify each of the 46,000senses, once again using the closed vocabulary of theLexeed dictionary.
Both the definition sentences andexample sentences were then manually sense anno-tated by 5 native speakers of Japanese, from which amajority sense was extracted.776In addition, an ontology was induced from theLexeed dictionary, by parsing the first definition sen-tence for each sense (Nichols et al, 2005).
Hy-pernyms were determined by identifying the highestscoping real predicate (i.e.
the genus).
Other rela-tion types such as synonymy and domain were alsoinduced based on trigger patterns in the definitionsentences, although these are too few to be usefulin our research.
Because each word is sense tagged,the relations link senses rather than just words.3.3 Peculiarities of JapaneseThe experiments in this paper focus exclusivelyon Japanese WSD.
Below, we outline aspects ofJapanese which are relevant to the task.First, Japanese is a non-segmenting language, i.e.there is no explicit orthographic representation ofword boundaries.
The native rendering of (1), e.g., is???????????.
Various packages exist toautomatically segment Japanese strings into words,and the Lexeed data has been pre-segmented usingChaSen (Matsumoto et al, 2003).Second, Japanese is made up of 3 basic alpha-bets: hiragana, katakana (both syllabic in nature)and kanji (logographic in nature).
The relevance ofthese first two observations to WSD is that we canchoose to represent the context of a target word byway of characters or words.Third, Japanese has relatively free word order,or strictly speaking, word order within phrases islargely fixed but the ordering of phrases governedby a given predicate is relatively free.4 Proposed ExtensionsWe propose extensions to the basic Lesk algorithmin the orthogonal areas of the scoring mechanism,tokenisation, extended glosses and filtering.4.1 Scoring MechanismIn our algorithm, overlap provides the means toscore a given pairing of context w and definitiondi,j .
In the original Lesk algorithm, overlap wassimply the sum of words in common between thetwo, which Banerjee and Pedersen (2002) modifiedby squaring the size of each overlapping sub-string.While squaring is well motivated in terms of prefer-ring larger substring matches, it makes the algorithmcomputationally expensive.
We thus adopt a cheaperscoring mechanism which normalises relative to thelength of w and di,j , but ignores the length of sub-string matches.
Namely, we use the Dice coefficient.4.2 TokenisationTokenisation is particularly important in Japanesebecause it is a non-segmenting language with a lo-gographic orthography (kanji).
As such, we canchose to either word tokenise via a word splittersuch as ChaSen, or character tokenise.
Charac-ter and word tokenisation have been compared inthe context of Japanese information retrieval (Fujiiand Croft, 1993) and translation retrieval (Baldwin,2001), and in both cases, characters have been foundto be the superior representation overall.Orthogonal to the question of whether to tokeniseinto words or characters, we adopt an n-gram seg-ment representation, in the form of simple unigramsand simple bigrams.
In the case of word tokenisa-tion and simple bigrams, e.g., example (1) would berepresented as {??????
,??
,?????
}.4.3 Extended GlossesThe main direction in which Banerjee and Peder-sen (2002) successfully extended the Lesk algorithmwas in including hierarchically-adjacent glosses (i.e.hyponyms and hypernyms).
We take this a stepfurther, in using both the Lexeed ontology and thesense-disambiguated words in the definition sen-tences.The basic form of extended glossing is the simpleLesk method, where we take the simple definitionsfor each sense si,j (i.e.
without any gloss extension).Next, we replicate the Banerjee and Pedersen(2002) method in extending the glosses to includewords from the definitions for the (immediate) hy-pernyms and/or hyponyms of each sense si,j .An extension of the Banerjee and Pedersen (2002)method which makes use of the sense-annotated def-initions is to include the words in the definition ofeach sense-annotated word dk contained in defini-tion di,j = d1d2...dm of word sense si,j .
That is,rather than traversing the ontology relative to eachword sense candidate si,j for the target word wi,we represent each word sense via the original def-inition plus all definitions of word senses containedin it (weighting each to give the words in the originaldefinition greater import than those from definitionsof those word senses).
We can then optionally adopta similar policy to Banerjee and Pedersen (2002) inexpanding each sense-annotated word dk in the orig-inal definition relative to the ontology, to include theimmediate hypernyms and/or hyponyms.We further expand the definitions (+extdef) byadding the full definition for each sense-tagged wordin the original definition.
This can be combinedwith the Banerjee and Pedersen (2002) method by777also expanding each sense-annotated word dk in theoriginal definition relative to the ontology, to in-clude the immediate hypernyms (+hyper) and/or hy-ponyms (+hypo).4.4 FilteringEach word sense in the dictionary is marked with aword class, and the word splitter similarly POS tagsevery definition and input to the system.
It is nat-ural to expect that the POS tag of the target wordshould match the word class of the word sense, andthis provides a coarse-grained filter for discriminat-ing homographs with different word classes.We also experiment with a stop word-based filterwhich ignores a closed set of 18 lexicographic mark-ers commonly found in definitions (e.g.
?
[ryaku]?an abbreviation for ...?
), in line with those used byNichols et al (2005) in inducing the ontology.5 EvaluationWe evaluate our various extensions over twodatasets: (1) the example sentences in the Lexeedsensebank, and (2) the Senseval-2 Japanese dictio-nary task (Shirai, 2002).All results below are reported in terms of sim-ple precision, following the conventions of Sensevalevaluations.
For all experiments, precision and re-call are identical as our systems have full coverage.For the two datasets, we use two baselines: a ran-dom baseline and the first-sense baseline.
Note thatthe first-sense baseline has been shown to be hardto beat for unsupervised systems (McCarthy et al,2004), and it is considered supervised when, as inthis case, the first-sense is the most frequent sensefrom hand-tagged corpora.5.1 Lexeed Example SentencesThe goal of these experiments is to tag all the wordsthat occur in the example sentences in the LexeedSensebank.
The first set of experiments over theLexeed Sensebank explores three parameters: theuse of characters vs. words, unigrams vs. bigrams,and original vs. extended definitions.
The results ofthe experiments and the baselines are presented inTable 1.First, characters are in all cases superior to wordsas our segment granularity.
The introduction of bi-grams has a uniformly negative impact for both char-acters and words, due to the effects of data sparse-ness.
This is somewhat surprising for characters,given that the median word length is 2 characters,although the difference between character unigramsand bigrams is slight.Extended definitions are also shown to be superiorto simple definitions, although the relative incrementin making use of large amounts of sense annotationsis smaller than that of characters vs. words, suggest-ing that the considerable effort in sense annotatingthe definitions is not commensurate with the finalgain for this simple method.Note that at this stage, our best-performingmethod is roughly equivalent to the unsupervised(random) baseline, but well below the supervised(first sense) baseline.Having found that extended definitions improveresults to a small degree, we turn to our next exper-iment were we investigate whether the introductionof ontological relations to expand the original def-initions further enhances our precision.
Here, wepersevere with the use of word and characters (allunigrams), and experiment with the addition of hy-pernyms and/or hyponyms, with and without the ex-tended definitions.
We also compare our methoddirectly with that of Banerjee and Pedersen (2002)over the Lexeed data, and further test the impactof the sense annotations, in rerunning our experi-ments with the ontology in a sense-insensitive man-ner, i.e.
by adding in the union of word-level hyper-nyms and/or hyponyms.
The results are described inTable 2.
The results in brackets are reproduced fromearlier tables.Adding in the ontology makes a significant dif-ference to our results, in line with the findings ofBanerjee and Pedersen (2002).
Hyponyms are betterdiscriminators than hypernyms (assuming a givenword sense has a hyponym ?
the Lexeed ontologyis relatively flat), partly because while a given wordsense will have (at most) one hypernym, it often hasmultiple hyponyms (if any at all).
Adding in hyper-nyms or hyponyms, in fact, has a greater impact onresults than simple extended definitions (+extdef),especially for words.
The best overall results areproduced for the (weighted) combination of all on-tological relations (i.e.
extended definitions, hyper-nyms and hyponyms), achieving a precision levelabove both the unsupervised (random) and super-vised (first-sense) baselines.In the interests of getting additional insights intothe import of sense annotations in our method, weran both the original Banerjee and Pedersen (2002)method and a sense-insensitive variant of our pro-posed method over the same data, the results forwhich are also included in Table 2.
Simple hy-ponyms (without extended definitions) and word-based segments returned the best results out of allthe variants tried, at a precision of 0.656.
This com-pares with a precision of 0.683 achieved for the best778UNIGRAMS BIGRAMSALL WORDS POLYSEMOUS ALL WORDS POLYSEMOUSSimple DefinitionsCHARACTERS 0.523 0.309 0.486 0.262WORDS 0.469 0.229 0.444 0.201Extended DefinitionsCHARACTERS 0.526 0.313 0.529 0.323WORDS 0.489 0.258 0.463 0.227Table 1: Precision over the Lexeed example sentences using simple/extended definitions and word/characterunigrams and bigrams (best-performing method in boldface)ALL WORDS POLYSEMOUSUNSUPERVISED BASELINE: 0.527 0.315SUPERVISED BASELINE: 0.633 0.460Banerjee and Pedersen (2002) 0.648 0.492Ontology expansion (sense-sensitive)simple (0.469) (0.229)+extdef (0.489) (0.258)+hypernyms 0.559 0.363W +hyponyms 0.655 0.503+def +hyper 0.577 0.386+def +hypo 0.649 0.490+def +hyper +hypo 0.683 0.539simple (0.523) (0.309)+extdef (0.526) (0.313)+hypernyms 0.539 0.334C +hyponyms 0.641 0.481+def +hyper 0.563 0.365+def +hypo 0.671 0.522+def +hyper +hypo 0.671 0.522Ontology expansion (sense-insensitive)+hypernyms 0.548 0.348+hyponyms 0.656 0.503W +def +hyper 0.551 0.347+def +hypo 0.649 0.490+def + hyper +hypo 0.631 0.464+hypernyms 0.537 0.332+hyponyms 0.644 0.485C +def +hyper 0.542 0.335+def +hypo 0.644 0.484+def + hyper +hypo 0.628 0.460Table 2: Precision over the Lexeed exam-ple sentences using ontology-based gloss extension(with/without word sense information) and word(W) and character (C) unigrams (best-performingmethod in boldface)of the sense-sensitive methods, indicating that senseinformation enhances WSD performance.
This rein-forces our expectation that richly annotated lexicalresources improve performance.
With richer infor-mation to work with, character based methods uni-formly give worse results.While we don?t present the results here due to rea-sons of space, POS-based filtering had very little im-pact on results, due to very few POS-differentiatedhomographs in Japanese.
Stop word filtering leadsALLWORDSPOLYSEMOUSBaselinesUnsupervised (random) 0.310 0.260Supervised (first-sense) 0.577 0.555Ontology expansion (sense-sensitive)W +def +hyper +hypo 0.624 0.605C +def +hyper +hypo 0.624 0.605Ontology expansion (sense-insensitive)W +def +hyper +hypo 0.602 0.581C +def +hyper +hypo 0.593 0.572Table 3: Precision over the Senseval-2 datato a very slight increment in precision across theboard (of the order of 0.001).5.2 Senseval-2 Japanese Dictionary TaskIn our second set of experiments we apply our pro-posed method to the Senseval-2 Japanese dictionarytask (Shirai, 2002) in order to calibrate our resultsagainst previously published results for JapaneseWSD.
Recall that this is a lexical sample task,and that our evaluation is relative to Lexeed re-annotations of the same dataset, although the relativepolysemy for the original data and the re-annotatedversion are largely the same (Tanaka et al, 2006).The first sense baselines (i.e.
sense skewing) for thetwo sets of annotations differ significantly, however,with a precision of 0.726 reported for the originaltask, and 0.577 for the re-annotated Lexeed vari-ant.
System comparison (Senseval-2 systems vs. ourmethod) will thus be reported in terms of error ratereduction relative to the respective first sense base-lines.In Table 3, we present the results over theSenseval-2 data for the best-performing systemsfrom our earlier experiments.
As before, we in-clude results over both words and characters, andwith sense-sensitive and sense-insensitive ontologyexpansion.Our results largely mirror those of Table 2, al-though here there is very little to separate wordsand characters.
All methods surpassed both the ran-dom and first sense baselines, but the relative impact779of sense annotations was if anything even less pro-nounced than for the example sentence task.Both sense-sensitiveWSDmethods achieve a pre-cision of 0.624 over all the target words (with onetarget word per sentence), an error reduction rateof 11.1%.
This compares favourably with an errorrate reduction of 21.9% for the best of the WSDsystems in the original Senseval-2 task (Kurohashiand Shirai, 2001), particularly given that our methodis semi-supervised while the Senseval-2 system is aconventional supervised word sense disambiguator.6 ConclusionIn our experiments extending the Lesk algorithmover Japanese data, we have shown that definitionexpansion via an ontology produces a significantperformance gain, confirming results by Banerjeeand Pedersen (2002) for English.
We also exploreda new expansion of the Lesk method, by measuringthe contribution of sense-tagged definitions to over-all disambiguation performance.
Using sense infor-mation doubles the error reduction compared to thesupervised baseline, a constant gain that shows theimportance of precise sense information for error re-duction.Our WSD system can be applied to all words inrunning text, and is able to improve over the first-sense baseline for two separate WSD tasks, usingonly existing Japanese resources.
This full-coveragesystem opens the way to explore further enhance-ments, such as the contribution of extra sense-taggedexamples to the expansion, or the combination ofdifferent WSD algorithms.For future work, we are also studying the in-tegration of the WSD tool with other applicationsthat deal with Japanese text, such as a cross-lingualglossing tool that aids Japanese learners reading text.Another application we are working on is the inte-gration of the WSD system with parse selection forJapanese grammars.AcknowledgementsThis material is supported by the Research Collaboration be-tween NTT Communication Science Laboratories, NipponTelegraph and Telephone Corporation and the University ofMelbourne.
We would like to thank members of the NTT Ma-chine Translation Group and the three anonymous reviewers fortheir valuable input on this research.ReferencesEneko Agirre and Philip Edmonds, editors.
2006.
Word SenseDisambiguation: Algorithms and Applications.
Springer,Dordrecht, Netherlands.Timothy Baldwin.
2001.
Low-cost, high-performance transla-tion retrieval: Dumber is better.
In Proc.
of the 39th AnnualMeeting of the ACL and 10th Conference of the EACL (ACL-EACL 2001), pages 18?25, Toulouse, France.Satanjeev Banerjee and Ted Pedersen.
2002.
An adapted Leskalgorithm for word sense disambiguation using WordNet.
InProc.
of the 3rd International Conference on Intelligent TextProcessing and Computational Linguistics (CICLing-2002),pages 136?45, Mexico City, Mexico.Yee Seng Chan and Hwee Tou Ng.
2005.
Scaling up wordsense disambiguation via parallel texts.
In Proc.
of the 20thNational Conference on Artificial Intelligence (AAAI 2005),pages 1037?42, Pittsburgh, USA.Christiane Fellbaum, editor.
1998.
WordNet: An ElectronicLexical Database.
MIT Press, Cambridge, USA.Hideo Fujii and W. Bruce Croft.
1993.
A comparison of index-ing techniques for Japanese text retrieval.
In Proc.
of 16thInternational ACM-SIGIR Conference on Research and De-velopment in Information Retrieval (SIGIR?93), pages 237?46, Pittsburgh, USA.Kaname Kasahara, Hiroshi Sato, Francis Bond, TakaakiTanaka, Sanae Fujita, Tomoko Kanasugi, and ShigeakiAmano.
2004.
Construction of a Japanese semantic lexicon:Lexeed.
In Proc.
of SIG NLC-159, Tokyo, Japan.Sadao Kurohashi and Kiyoaki Shirai.
2001.
SENSEVAL-2Japanese tasks.
In IEICE Technical Report NLC 2001-10,pages 1?8.
(in Japanese).Claudia Leacock, Martin Chodorow, and George A. Miller.1998.
Using corpus statistics and WordNet relations forsense identification.
Computational Linguistics, 24(1):147?65.Michael Lesk.
1986.
Automatic sense disambiguation usingmachine readable dictionaries: How to tell a pine cone froman ice cream cone.
In Proc.
of the 1986 SIGDOC Confer-ence, pages 24?6, Ontario, Canada.Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, YoshitakaHirano, Hiroshi Matsuda, Kazuma Takaoka, and MasayukiAsahara.
2003.
Japanese Morphological Analysis SystemChaSen Version 2.3.3 Manual.
Technical report, NAIST.Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll.2004.
Finding predominant senses in untagged text.
InProc.
of the 42nd Annual Meeting of the ACL, pages 280?7, Barcelona, Spain.Rada Mihalcea and Timothy Chklovski.
2003.
Open MindWord Expert: Creating Large Annotated Data Collectionswith Web Users?
Help.
In Proceedings of the EACL2003 Workshop on Linguistically Annotated Corpora (LINC2003), pages 53?61, Budapest, Hungary.Eric Nichols, Francis Bond, and Daniel Flickinger.
2005.
Ro-bust ontology acquisition from machine-readable dictionar-ies.
In Proc.
of the 19th International Joint Conferenceon Artificial Intelligence (IJCAI-2005), pages 1111?6, Ed-inburgh, UK.Kiyoaki Shirai.
2002.
Construction of a word sense taggedcorpus for SENSEVAL-2 japanese dictionary task.
In Proc.of the 3rd International Conference on Language Resourcesand Evaluation (LREC 2002), pages 605?8, Las Palmas,Spain.Takaaki Tanaka, Francis Bond, and Sanae Fujita.
2006.
TheHinoki sensebank ?
a large-scale word sense tagged cor-pus of Japanese ?.
In Proc.
of the Workshop on Frontiersin Linguistically Annotated Corpora 2006, pages 62?9, Syd-ney, Australia.780
