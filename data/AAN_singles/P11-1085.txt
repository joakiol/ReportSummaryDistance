Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 846?855,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsLearning to Transform and Select Elementary Trees for ImprovedSyntax-based Machine TranslationsBing Zhao?, and Young-Suk Lee?, and Xiaoqiang Luo?, and Liu Li?IBM T.J. Watson Research?
and Carnegie Mellon University?
{zhaob, ysuklee, xiaoluo}@us.ibm.com and liul@andrew.cmu.eduAbstractWe propose a novel technique of learning how totransform the source parse trees to improve the trans-lation qualities of syntax-based translation mod-els using synchronous context-free grammars.
Wetransform the source tree phrasal structure into aset of simpler structures, expose such decisions tothe decoding process, and find the least expensivetransformation operation to better model word re-ordering.
In particular, we integrate synchronous bi-narizations, verb regrouping, removal of redundantparse nodes, and incorporate a few important fea-tures such as translation boundaries.
We learn thestructural preferences from the data in a generativeframework.
The syntax-based translation system in-tegrating the proposed techniques outperforms thebest Arabic-English unconstrained system in NIST-08 evaluations by 1.3 absolute BLEU, which is sta-tistically significant.1 IntroductionMost syntax-based machine translation models with syn-chronous context free grammar (SCFG) have been re-lying on the off-the-shelf monolingual parse structuresto learn the translation equivalences for string-to-tree,tree-to-string or tree-to-tree grammars.
However, state-of-the-art monolingual parsers are not necessarily wellsuited for machine translation in terms of both labelsand chunks/brackets.
For instance, in Arabic-to-Englishtranslation, we find only 45.5% of Arabic NP-SBJ struc-tures are mapped to the English NP-SBJ with machinealignment and parse trees, and only 60.1% of NP-SBJsare mapped with human alignment and parse trees as in?
2.
The chunking is of more concern; at best only 57.4%source chunking decisions are translated contiguously onthe target side.
To translate the rest of the chunks onehas to frequently break the original structures.
The mainissue lies in the strong assumption behind SCFG-stylenonterminals ?
each nonterminal (or variable) assumes asource chunk should be rewritten into a contiguous chunkin the target.
Without integrating techniques to mod-ify the parse structures, the SCFGs are not to be effec-tive even for translating NP-SBJ in linguistically distantlanguage-pairs such as Arabic-English.Such problems have been noted in previous literature.Zollmann and Venugopal (2006) and Marcu et al (2006)used broken syntactic fragments to augment their gram-mars to increase the rule coverage; while we learn opti-mal tree fragments transformed from the original ones viaa generative framework, they enumerate the fragmentsavailable from the original trees without learning pro-cess.
Mi and Huang (2008) introduced parse forests toblur the chunking decisions to a certain degree, to ex-pand search space and reduce parsing errors from 1-besttrees (Mi et al, 2008); others tried to use the parse treesas soft constraints on top of unlabeled grammar such asHiero (Marton and Resnik, 2008; Chiang, 2010; Huanget al, 2010; Shen et al, 2010) without sufficiently lever-aging rich tree context.
Recent works tried more com-plex approaches to integrate both parsing and decodingin one single search space as in (Liu and Liu, 2010), atthe cost of huge search space.
In (Zhang et al, 2009),combinations of tree forest and tree-sequence (Zhang etal., 2008) based approaches were carried out by addingpseudo nodes and hyper edges into the forest.
Overall,the forest-based translation can reduce the risks from up-stream parsing errors and expand the search space, butit cannot sufficiently address the syntactic divergencesbetween various language-pairs.
The tree sequence ap-proach adds pseudo nodes and hyper edges to the forest,which makes the forest even denser and harder for nav-igation and search.
As trees thrive in the search space,especially with the pseudo nodes and edges being addedto the already dense forest, it is becoming harder to wadethrough the deep forest for the best derivation path out.We propose to simplify suitable subtrees to a reason-able level, at which the correct reordering can be easilyidentified.
The transformed structure should be frequentenough to have rich statistics for learning a model.
In-stead of creating pseudo nodes and edges and make theforest dense, we transform a tree with a few simple oper-ators; only meaningful frontier nodes, context nodes andedges are kept to induce the correct reordering; such oper-ations also enable the model to share the statistics amongall similar subtrees.On the basis of our study on investigating the languagedivergence between Arabic-English with human alignedand parsed data, we integrate several simple statistical op-erations, to transform parse trees adaptively to serve the846translation purpose better.
For each source span in thegiven sentence, a subgraph, corresponding to an elemen-tary tree (in Eqn.
1), is proposed for PSCFG translation;we apply a few operators to transform the subgraph intosome frequent subgraphs seen in the whole training data,and thus introduce alternative similar translational equiv-alences to explain the same source span with enrichedstatistics and features.
For instance, if we regroup twoadjacent nodes IV and NP-SBJ in the tree, we can ob-tain the correct reordering pattern for verb-subject order,which is not easily available otherwise.
By finding a setof similar elementary trees derived from the original ele-mentary trees, statistics can be shared for robust learning.We also investigate the features using the context be-yond the phrasal subtree.
This is to further disambiguatethe transformed subgraphs so that informative neighbor-ing nodes and edges can influence the reordering prefer-ences for each of the transformed trees.
For instance, atthe beginning and end of a sentence, we do not expectdramatic long distance reordering to happen; or underSBAR context, the clause may prefer monotonic reorder-ing for verb and subject.
Such boundary features weretreated as hard constraints in previous literature in termsof re-labeling (Huang and Knight, 2006) or re-structuring(Wang et al, 2010).
The boundary cases were not ad-dressed in the previous literature for trees, and here weinclude them in our feature sets for learning a MaxEntmodel to predict the transformations.
We integrate theneighboring context of the subgraph in our transforma-tion preference predictions, and this improve translationqualities further.The rest of the paper is organized as follows: in sec-tion 2, we analyze the projectable structures using hu-man aligned and parsed data, to identify the problems forSCFG in general; in section 3, our proposed approachis explained in detail, including the statistical operatorsusing a MaxEnt model; in section 4, we illustrate the in-tegration of the proposed approach in our decoder; in sec-tion 5, we present experimental results; in section 6, weconclude with discussions and future work.2 The Projectable StructuresA context-free style nonterminal in PSCFG rules meansthe source span governed by the nonterminal should betranslated into a contiguous target chunk.
A ?projectable?phrase-structure means that it is translated into a con-tiguous span on the target side, and thus can be gener-alized into a nonterminal in our PSCFG rule.
We carriedout a controlled study on the projectable structures usinghuman annotated parse trees and word alignment for 5kArabic-English sentence-pairs.In Table 1, the unlabeled F-measures with machinealignment and parse trees show that, for only 48.71% ofthe time, the boundaries introduced by the source parsesAlignment Parse Labels AccuracyH HNP-SBJ 0.6011PP 0.3436NP 0.4832unlabel 0.5739M HNP-SBJ 0.5356PP 0.2765NP 0.3959unlabel 0.5305M MNP-SBJ 0.4555PP 0.1935NP 0.3556unlabel 0.4871Table 1: The labeled and unlabeled F-measures for projectingthe source nodes onto the target side via alignments and parsetrees; unlabeled F-measures show the bracketing accuracies fortranslating a source span contiguously.
H: human, M: machine.are real translation boundaries that can be explained by anonterminal in PSCFG rule.
Even for human parse andalignment, the unlabeled F-measures are still as low as57.39%.
Such statistics show that we should not blindlylearn tree-to-string grammar; additional transformationsto manipulate the bracketing boundaries and labels ac-cordingly have to be implemented to guarantee the reli-ability of source-tree based syntax translation grammars.The transformations could be as simple as merging twoadjacent nonterminals into one bracket to accommodatenon-contiguity on the target side, or lexicalizing thosewords which have fork-style, many-to-many alignment,or unaligned content words to enable the rest of the spanto be generalized into nonterminals.
We illustrate severalcases using the tree in Figure 1.NP?SBJthe millde east crisisupthat makemnPREP PRON+hA AzmpNOUNAl$rqADJAlAwsTPRON IVAlty ttAlfNOUNWHNP VP PP?CLRSBARSNPFigure 1: Non-projectable structures in an SBAR tree withhuman parses and alignment; there are non-projectable struc-tures: the deleted nonterminals PRON (+hA), the many-to-many alignment for IV(ttAlf) PREP(mn), fork-style alignmentfor NOUN (Azmp).In Figure 1, several non-projectable nodes were illus-847trated: the deleted nonterminals PRON (+hA), the many-to-many alignment for IV(ttAlf) PREP(mn), fork-stylealignment for NOUN (Azmp).
Intuitively, it would begood to glue the nodes NOUN(Al$rq) ADJ(AlAwsT) un-der the node of NP, because it is more frequent for movingADJ before NOUN in our training data.
It should be eas-ier to model the swapping of (NOUN ADJ) using the tree(NP NOUN, ADJ) instead of the original bigger tree of(NP-SBJ Azmp, NOUN, ADJ) with one lexicalized node.Approaches in tree-sequence based grammar (Zhang etal., 2009) tried to address the bracketing problem by us-ing arbitrary pseudo nodes to weave a new ?tree?
backinto the forest for further grammar extractions.
Such ap-proach may improve grammar coverage, but the pseudonode labels would be arguably a worse choice to splitthe already sparse data.
Some of the interior nodes con-necting the frontier nodes might be very informative formodeling reordering.
Also, due to the introduced pseudonodes, it would need exponentially many nonterminals tokeep track of the matching tree-structures for translations.The created pseudo node could easily block the informa-tive neighbor nodes associated with the subgraph whichcould change the reordering nature.
For instance, IV andNP-SBJ tends to swap at the beginning of a sentence, butit may prefer monotone if they share a common parent ofSBAR for a subclause.
In this case, it is unnecessary tocreate a pseudo node ?IV+SBJ?
to block useful factors.We propose to navigate through the forest, via simpli-fying trees by grouping the nodes, cutting the branches,and attaching connected neighboring informative nodesto further disambiguate the derivation path.
We apply ex-plicit translation motivated operators, on a given mono-lingual elementary tree, to transform it into similar butsimpler trees, and expose such statistical preferences tothe decoding process to select the best rewriting rulefrom the enriched grammar rule sets, for generating tar-get strings.3 Elementary Trees to String GrammarWe propose to use variations of an elementary tree, whichis a connected subgraph fitted in the original monolingualparse tree.
The subgraph is connected so that the frontiers(two or more) are connected by their immediate commonparent.
Let ?
be a source elementary tree:?
=< `; vf , vi, E >, (1)where vf is a set of frontier nodes which contain nonter-minals or words; vi are the interior nodes with source la-bels/symbols; E is the set of edges connecting the nodesv = vf+vi into a connected subgraph fitted in the sourceparse tree; ` is the immediate common parent of the fron-tier nodes vf .
Our proposed grammar rule is formulatedas follows:< ?;?;?
; m?
; t?
>, (2)where ?
is the target string, containing the terminalsand/or nonterminals in a target language; ?
is the one-to-one alignment of the nonterminals between ?
and ?
; t?contains possible sequence of transform operations (to beexplained later in this section) associated with each rule;m?
is a function of enumerating the neighborhood of thesource elementary tree ?, and certain tree context (nodesand edges) can be used to further disambiguate the re-ordering or the given lexical choices.
The interior nodesof ?.vi, however, are not necessarily informative for thereordering decisions, like the unary nodes WHNP,VP, andPP-CLR in Figure 1; while the frontier nodes ?.vf are theones directly executing the reordering decisions.
We canselectively cut off the interior nodes, which have no oronly weak causal relations to the reordering decisions.This will enable the frequency or derived probabilitiesfor executing the reordering to be more focused.
We callsuch transformation operators t?.
We specified a few op-erators for transforming an elementary tree ?, includingflattening tree operators such as removing interior nodesin vi, or grouping the children via binarizations.Let?s use the trigram ?Alty ttAlf mn?
in Figure 1 asan example, the immediate common parent for the spanis SBAR: ?.` = SBAR; the interior nodes are ?.vi ={WHNP VP S PP-CLR}; the frontier nodes are ?.vf =(x:PRON x:IV x:PREP).
The edges ?.E (as highlightedin Figure 1) connect ?.vi and ?.vf into a subgraph for thegiven source ngram.For any source span, we look up one elementary tree ?covering the span, then we select an operator t?
?
T , toexplore a set of similar elementary trees t?
(?, m?)
= {??
}as simplified alternatives for translating that source tree(span) ?
into an optimal target string ??
accordingly.
Ourgenerative model is summarized in Eqn.
3:??
= argmaxt?
?T ;???t?(?,m?)pa(??|??)?pb(?
?|t?, ?, m?
)?pc(t?|?, m?).
(3)In our generative scheme, for a given elementary tree?, we sample an operator (or a combination of operations)t?
with the probability of pc(t?|?
); with operation t?, wetransform ?
into a set of simplified versions ??
?
t?
(?, m?
)with the probability of pb(?
?|t?, ?
); finally we select thetransformed version ??
to generate the target string ?
?with a probability of pa(??|??).
Note here, ??
and ?
sharethe same immediate common parent `, but not necessar-ily the frontier, or interior, or even neighbors.
The frontiernodes can be merged, lexicalized, or even deleted in thetree-to-string rule associated with ?
?, as long as the align-ment for the nonterminals are book-kept in the deriva-tions.
To simplify the model, one can choose the operatort?
to be only one level, and the model using a single oper-ator t?
is to be deterministic.
Thus, the final set of models848to learn are pa(??|??)
for rule alignment, and the pref-erence model pb(?
?|t?, ?, m?
), and the operator proposalmodel pc(t?|?, m?
), which in our case is a maximum en-tropy model?
the key model in our proposed approachin this paper for transforming the original elementary treeinto similar trees for evaluating the reordering probabili-ties.Eqn.
3 significantly enriches reordering powers forsyntax-based machine translation.
This is because it usesall similar set of elementary trees to generate the best tar-get strings.
In the next section, we?ll first define the op-erators conceptually, and then explain how we learn eachof the models.3.1 Model pa(??|??
)A log linear model is applied here to approximatepa(??|??)
?
exp(??
?ff) via weighted combination (??)
offeature functions ff(?
?, ??
), including relative frequen-cies in both directions, and IBM Model-1 scores in bothdirections as ??
and ??
have lexical items within them.We also employed a few binary features listed in the fol-lowing table.??
is observed less than 2 times(?
?, ??)
deletes a src content word(?
?, ??)
deletes a src function word(?
?, ??)
over generates a tgt content word(?
?, ??)
over generates a tgt function wordTable 2: Additional 5 Binary Features for pa(??|??
)3.2 Model pb(?
?|t?, ?, m?)pb(?
?|t?, ?, m?)
is our preference model.
For instance us-ing the operator t?
of cutting an unary interior node in?.vi, if ?.vi has more than one unary interior node, likethe SBAR tree in Figure 1, having three unary interiornode: WHNP, VP and PP-CLR, pb(?
?|t?, ?, m?)
specifieswhich one should have more probabilities to be cut.
Inour case, to make model simple, we simply choose his-togram/frequency for modeling the choices.3.3 Model pc(t?|?, m?
)pc(t?|?, m?)
is our operator proposal model.
It ranksthe operators which are valid to be applied for thegiven source tree ?
together with its neighborhood m?.Here, in our approach, we applied a Maximum Entropymodel, which is also employed to train our Arabic parser:pc(t?|?, m?)
?
exp ??
?
ff(t?, ?, m?).
The feature sets weuse here are almost the same set we used to train our Ara-bic parser; the only difference is the future space here isoperator categories, and we check bag-of-nodes for inte-rior nodes and frontier nodes.
The key feature categorieswe used are listed as in the Table 3.
The headtable usedin our training is manually built for Arabic.bag-of-nodes ?.vibag-of-nodes and ngram of ?.vfchunk-level features: left-child, right-child, etc.lexical features: unigram and bigrampos features: unigram and bigramcontextual features: surrounding wordsTable 3: Feature Features for learning pc(t?|?, m?
)3.4 t?
: Tree Transformation FunctionObvious systematic linguistic divergences betweenlanguage-pairs could be handled by some simple oper-ators such as using binarization to re-group contiguouslyaligned children.
Here, we start from the human alignedand parsed data as used in section 2 to explore potentialuseful operators.3.4.1 BinarizationsOne of the simplest way for transforming a tree is via bi-narization.
Monolingual binarization chooses to re-groupchildren into smaller subtree with a suitable label for thenewly created root.
We choose a function mapping to se-lect the top-frequent label as the root for the grouped chil-dren; if such label is not found we simply use the label ofthe immediate common parent for ?.
In decoding time,we need to select trees from all possible binarizations,while in the training time, we restrict the choices allowedwith the alignment constraint, that every grouped chil-dren should be aligned contiguously on the target side.Our goal is to simulate the synchronous binarization asmuch as we can.
In this paper, we applied the four ba-sic operators for binarizing a tree: left-most, right-mostand additionally head-out left and head-out right for morethan three children.
Two examples are given in Table 4,in which we used LDC style representation for the trees.With the proper binarization, the structure becomesrich in sub-structures which allow certain reordering tohappen more likely than others.
For instance for the sub-tree (VP PV NP-SBJ), one would apply stronger statisticsfrom training data to support the swap of NP-SBJ and PVfor translation.3.4.2 Regrouping verbsVerbs are keys for reordering especially for Araic-Englishwith VSO translated into SVO.
However, if the verb andits relevant arguments for reordering are at different lev-els in the tree, the reordering is difficult to model as moreinterior nodes combinations will distract the distributionsand make the model less focused.
We provide the fol-lowing two operations specific for verb in VP trees as inTable 5.3.4.3 Removing interior nodes and edgesFor reordering patterns, keeping the deep tree structuremight not be the best choice.
Sometimes it is not even849Binarization Operations Examplesright-most (NP Xnoun Xadj1 Xadj2) 7?
(NP Xnoun (ADJP Xadj1 Xadj2))left-most (VP Xpv XNP-SBJ XSBAR) 7?
(VP (VP Xpv XNP-SBJ) XSBAR)Table 4: Operators for binarizing the treesOperators for regroup verbs Examplesregroup verb (V P1 Xv (V P2 Y )) 7?
(V P1 (V P2 Xv Y ))regroup verb and remove the top level VP (R (V P1 Xv (R2 Y ))) 7?
(R (R2 XvY ))Table 5: Operators for manipulating the treespossible due to the many-to-many alignment, insertionsand deletions of terminals.
So, we introduce the oper-ators to remove the interior nodes ?.vi selectively; thisway, we can flatten the tree, remove irrelevant nodes andedges, and can use more frequent observations of simpli-fied structures to capture the reordering patterns.
We usetwo operators as shown in Table 6.The second operator deletes all the interior nodes, la-bels and edges; thus reordering will become a Hiero-alike(Chiang, 2007) unlabeled rule, and additionally a spe-cial glue rule: X1X2 ?
X1X2.
This operator is neces-sary, we need a scheme to automatically back off to themeaningful glue or Hiero-alike rules, which may lead to acheaper derivation path for constructing a partial hypoth-esis, at the decoding time.NP*PREPto ignite the situationAlAwDAEDET+NOUNAlAnfjArDET+NOUNdfENOUNNPNPPP*NPAlyFigure 2: A NP tree with an ?inside-out?
alignment.
The nodes?NP*?
and ?PP*?
are not suitable for generalizing into NTsused in PSCFG rules.As shown in Table 1, NP brackets has only 35.56% oftime to be translated contiguously as an NP in machinealigned & parsed data.
The NP tree in Figure 2 happens tobe an ?inside-out?
style alignment, and context free gram-mar such as ITG (Wu, 1997) can not explain this structurewell without necessary lexicalization.
Actually, the Ara-bic tokens of ?dfE Aly AlAnfjAr?
form a combinationand is turned into English word ?ignite?
in an idiomaticway.
With lexicalization, a Hiero style rule ?dfE X AlyAlAnfjAr 7?
to ignite X?
is potentially a better alterna-tive for translating the NP tree.
Our operators allow usto back off to such Hiero-style rules to construct deriva-tions, which share the immediate common parent NP, asdefined for the elementary tree, for the given source span.3.5 m?
: Neighboring FunctionFor a given elementary tree, we use function m?
to checkthe context beyond the subgraph.
This includes lookingthe nodes and edges connected to the subgraph.
Similarto the features used in (Dyer et al, 2009), we check thefollowing three cases.3.5.1 Sentence boundariesWhen the tree ?
frontier sets contain the left-most token,right-most token, or both sides, we will add to the neigh-boring nodes the corresponding decoration tags L (left),R (right), and B (both), respectively.
These decorationsare important especially when the reordering patterns forthe same trees are depending on the context.
For instance,at the beginning or end of a sentence, we do not expectdramatic reordering ?
moving a token too far away in themiddle of the sentences.3.5.2 SBAR/IP/PP/FRAG boundariesWe check siblings of the root for ?
for a few special la-bels, including SBAR, IP, PP, and FRAG.
These labelsindicate a partial sentence or clause, and the reorderingpatterns may get different distributions due to the posi-tion relative to these nodes.
For instance, the PV and SBJnodes under SBAR tends to have more monotone prefer-ence for word reordering (Carpuat et al, 2010).
We markthe boundaries with position markers such as L-PP, to in-dicate having a left sibling PP, R-IP for having a rightsibling IP, and C-SBAR to indicate the elementary tree isa child of SBAR.
These labels are selected mainly basedon our linguistic intuitions and errors in our translationsystem.
A data-driven approach might be more promis-ing for identifying useful markups w.r.t specific reorder-ing patterns.3.5.3 Translation boundariesIn the Figure 2, there are two special nodes under NP:NP* and PP*.
These two nodes are aligned in a ?inside-out?
fashion, and none of them can be generalized intoa nonterminal to be rewritten in a PSCFG rule.
In otherwords, the phrasal brackets induced from NP* and PP*850operators for removing nodes/edges Examplesremove unary nodes (R Xt1(R1 (R2 Xt2))) 7?
(R Xt1(R2 Xt2)))remove all labels (R (R1 Xt1(R2 Xt2))) 7?
(R Xt2Xt1)Table 6: Operators for simplifying the treesare not translation boundaries, and to avoid translationerrors we should identify them by applying a PSCFGrule on top of them.
During training, we label nodeswith translation boundaries, as one additional functiontag; during decoding, we employ the MaxEnt model topredict the translation boundary label probability for eachspan associated with a subgraph ?, and discourage deriva-tions accordingly for using nonterminals over the non-translation boundary span.
The translation boundariesover elementary trees have much richer representationpower.
The previous works as in Xiong et al (2010),defined translation boundaries on phrase-decoder stylederivation trees due to the nature of their shift-reduce al-gorithm, which is a special case in our model.4 DecodingDecoding using the proposed elementary tree to stringgrammar naturally resembles bottom up chart parsing al-gorithms.
The key difference is at the grammar queryingstep.
Given a grammar G, and the input source parse treepi from a monolingual parser, we first construct the ele-mentary tree for a source span, and then retrieve all therelevant subgraphs seen in the given grammar throughthe proposed operators.
This step is called populating,using the proposed operators to find all relevant elemen-tary trees ?
which may have contributed to explain thesource span, and put them in the corresponding cells inthe chart.
There would have been exponential number ofrelevant elementary trees to search if we do not have anyrestrictions in the populating step; we restrict the maxi-mum number of interior nodes |?.vi| to be 3, and the sizeof frontier nodes |?.vf | to be less than 6; additional prun-ing for less frequent elementary trees is carried out.After populating the elementary trees, we constructthe partial hypotheses bottom up, by rewriting the fron-tier nodes of each elementary tree with the probabili-ties(costs) for ?
?
??
as in Eqn.
3.
Our decoder (Zhaoand Al-Onaizan, 2008) is a template-based chart decoderin C++.
It generalizes over the dotted-product operator inEarley style parser, to allow us to leverage many opera-tors t?
?
T as above-mentioned, such as binarizations, atdifferent levels for constructing partial hypothesis.5 ExperimentsIn our experiments, we built our system using most of theparallel training data available to us: 250M Arabic run-ning tokens, corresponding to the ?unconstrained?
condi-tion in NIST-MT08.
We chose the testsets of newswireand weblog genres from MT08 and DEV101.
In partic-ular, we choose MT08 to enable the comparison of ourresults to the reported results in NIST evaluations.
Ourtraining and test data is summarized in Table 5.
For test-ings, we have 129,908 tokens in our testsets.
For lan-guage models (LM), we used 6-gram LM trained with10.3 billion English tokens, and also a shrinkage-basedLM (Chen, 2009) ?
?ModelM?
(Chen and Chu, 2010;Emami et al, 2010) with 150 word-clusters learnt from2.1 million tokens.From the parallel data, we extract phrase pairs(blocks)and elementary trees to string grammar in various con-figurations: basic tree-to-string rules (Tr2str), elementarytree-to-string rules with boundaries t?(elm2str+m?
), andwith both t?
and m?
(elm2str+t?
+ m?).
This is to evalu-ate the operators?
effects at different levels for decoding.To learn our MaxEnt models defined in ?
3.3, we collectthe events during extracting elm2str grammar in trainingtime, and learn the model using improved iterative scal-ing.
We use the same training data as that used in trainingour Arabic parser.
There are 16 thousand human parsetrees with human alignment; additional 1 thousand hu-man parse and aligned sent-pairs are used as unseen testset to verify our MaxEnt models and parsers.
For ourArabic parser, we have a labeled F-measure of 78.4%,and POS tag accuracy 94.9%.
In particular, we?ll evaluatemodel pc(t?|?, m?)
in Eqn.
3 for predicting the translationboundaries in ?
3.5.3 for projectable spans as detailed in?
5.1.Our decoder (Zhao and Al-Onaizan, 2008) supportsgrammars including monotone, ITG, Hiero, tree-to-string, string-to-tree, and several mixtures of them (Leeet al, 2010).
We used 19 feature functions, mainly fromthose used in phrase-based decoder like Moses (Koehnet al, 2007), including two language models (one for a6-gram LM, one for ModelM, one brevity penalty, IBMModel-1 (Brown et al, 1993) style alignment probabil-ities in both directions, relative frequency in both direc-tions, word/rule counts, content/function word mismatch,together with features on tr2str rule probabilities.
Weuse BLEU (Papineni et al, 2002) and TER (Snover etal., 2006) to evaluate translation qualities.
Our base-line used basic elementary tree to string grammar withoutany manipulations and boundary markers in the model,1DEV10 are unseen testsets used in our GALE project.
It was se-lected from recently released LDC data LDC2010E43.v3.851Data Train MT08-NW MT08-WB Dev10-NW Dev10-WB# Sents 8,032,837 813 547 1089 1059# Tokens 349M(ar)/230M(en) 25,926 19,654 41,240 43,088Table 7: Training and test data; using all training parallel training data for 4 test setsand we achieved a BLEUr4n4 55.01 for MT08-NW, ora cased BLEU of 53.31, which is close to the best offi-cially reported result 53.85 for unconstrained systems.2We expose the statistical decisions in Eqn.
3 as the ruleprobability as one of the 19 dimensions, and use Sim-plex Downhill algorithm with Armijo line search (Zhaoand Chen, 2009) to optimize the weight vector for de-coding.
The algorithm moves all dimensions at the sametime, and empirically achieved more stable results thanMER(Och, 2003) in many of our experiments.5.1 Predicting Projectable StructuresThe projectable structure is important for our proposedelementary tree to string grammar (elm2str).
When aspan is predicted not to be a translation boundary, wewant the decoder to prefer alternative derivations out-side of the immediate elementary tree, or more aggres-sive manipulation of the trees, such as deleting inte-rior nodes, to explore unlabeled grammar such as Hi-ero style rules, with proper costs.
We test separatelyon predicting the projectable structures, like predictingfunction tags in ?
3.5.3, for each node in syntactic parsetree.
We use one thousand test sentences with two con-ditions: human parses and machine parses.
There aretotally 40,674 nodes excluding the sentence-level node.The results are shown in Table 8.
It showed our Max-Ent model is very accurate using human trees: 94.5% ofaccuracy, and about 84.7% of accuracy for using the ma-chine parsed trees.
Our accuracies are higher comparedwith the 71+% accuracies reported in (Xiong et al, 2010)for their phrasal decoder.Setups AccuracyHuman Parses 94.5%Machine Parses 84.7%Table 8: Accuracies of predicting projectable structuresWe zoom in the translation boundaries for MT08-NW,in which we studied a few important frequent labels in-cluding VP and NP-SBJ as in Table 9.
According to ourMaxEnt model, 20% of times we should discourage a VPtree to be translated contiguously; such VP trees have anaverage span length of 16.9 tokens in MT08-NW.
Simi-lar statistics are 15.9% for S-tree with an average span of13.8 tokens.2See link: http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/mt08official results v0.htmlLabels total NonProj Percent Avg.lenVP* 4479 920 20.5% 16.9NP* 14164 825 5.8% 8.12S* 3123 495 15.9% 13.8NP-SBJ* 1284 53 4.12% 11.9Table 9: The predicted projectable structures in MT08-NWUsing the predicted projectable structures for elm2strgrammar, together with the probability defined in Eqn.
3as additional cost, the translation results in Table 11 showit helps BLEU by 0.29 BLEU points (56.13 v.s.
55.84).The boundary decisions penalize the derivation paths us-ing nonterminals for non-projectable spans for partial hy-pothesis construction.Setups TER BLEUr4n4Baseline 39.87 55.01right-binz (rbz) 39.10 55.19left-binz (lbz) 39.67 55.31Head-out-left (hlbz) 39.56 55.50Head-out-right (hrbz) 39.52 55.53+all binzation (abz) 39.42 55.60+regroup-verb 39.29 55.72+deleting interior nodes ?.vi 38.98 55.84Table 10: TER and BLEU for MT08-NW, using only t?(?
)5.2 Integrating t?
and m?We carried out a series of experiments to explore the im-pacts using t?
and m?
for elm2str grammar.
We start fromtransforming the trees via simple operator t?(?
), and thenexpand the function with more tree context to include theneighboring functions: t?
(?, m?
).Setups TER BLEUr4n4Baseline w/ t?
38.98 55.84+ TM Boundaries 38.89 56.13+ SENT Bound 38.63 56.46all t?
(?, m?)
38.61 56.87Table 11: TER and BLEU for MT08-NW, using t?
(?, m?
).Experiments in Table 10 focus on testing operators es-pecially binarizations for transforming the trees.
In Ta-ble 10, the four possible binarization methods all improve852Data MT08-NW MT08-WB Dev10-NW Dev10-WBTr2Str 55.01 39.19 37.33 41.77elm2str+t?
55.84 39.43 38.02 42.70elm2str+m?
55.57 39.60 37.67 42.54elm2str+t?
(?, m?)
56.87 39.82 38.62 42.75Table 12: BLEU scores on various test sets; comparing elementary tree-to-string grammar (tr2str), transformation of the trees(elm2str+t?
), using the neighboring function for boundaries ( elm2str+m?
), and combination of all together ( elm2str+t?
(?, m?
)).MT08-NW and MT08-WB have four references; Dev10-WB has three references, and Dev10-NW has one reference.
BLEUn4were reported.over the baseline from +0.18 (via right-most binarization)to +0.52 (via head-out-right) BLEU points.
When wecombine all binarizations (abz), we did not see additivegains over the best individual case ?
hrbz.
Because duringour decoding time, we do not frequently see large numberof children (maximum at 6), and for smaller trees (withthree or four children), these operators will largely gen-erate same transformed trees, and that explains the differ-ences from these individual binarization are small.
Forother languages, these binarization choices might givelarger differences.
Additionally, regrouping the verbs ismarginally helpful for BLEU and TER.
Upon close ex-aminations, we found it is usually beneficial to groupverb (PV or IV) with its neighboring nodes for expressingphrases like ?have to do?
and ?will not only?.
Deletingthe interior nodes helps on shrinking the trees, so that wecan translate it with more statistics and confidences.
Ithelps more on TER than BLEU for MT08-NW.Table 11 extends Table 10 with neighboring functionto further disambiguate the reordering rule using the treecontext.
Besides the translation boundary, the reorder-ing decisions should be different with regard to the posi-tions of the elementary tree relative to the sentence.
Atthe sentence-beginning one might expect more for mono-tone decoding, while in the middle of the sentence, onemight expect more reorderings.
Table 11 shows when weadd such boundary markups in our rules, an improvementof 0.33 BLEU points were obtained (56.46 v.s.
56.13)on top of the already improved setups.
A close checkup showed that the sentence-begin/end markups signifi-cantly reduced the leading ?and?
(from Arabic word w#)in the decoding output.
Also, the verb subject order un-der SBAR seems to be more like monotone with a lead-ing pronoun, rather than the general strong reordering ofmoving verb after subject.
Overall, our results showedthat such boundary conditions are helpful for executingthe correct reorderings.
We conclude the investigationwith full function t?
(?, m?
), which leads to a BLEUr4n4 of56.87 (cased BLEUr4n4c 55.16), a significant improve-ment of 1.77 BLEU point over a already strong baseline.We apply the setups for several other NW and WEBdatasets to further verify the improvement.
Shown in Ta-ble 12, we apply separately the operators for t?
and m?
first,then combine them as the final results.
Varied improve-ments were observed for different genres.
On DEV10-NW, we observed 1.29 BLEU points improvement, andabout 0.63 and 0.98 improved BLEU points for MT08-WB and DEV10-WB, respectively.
The improvementsfor newwire are statistically significant.
The improve-ments for weblog are, however, only marginally better.One possible reason is the parser quality for web genre isreliable, as our training data is all in newswire.
Regardingto the individual operators proposed in this paper, we ob-served consistent improvements of applying them acrossall the datasets.
The generative model in Eqn.
3 leveragesthe operators further by selecting the best transformedtree form for executing the reorderings.5.3 A Translation ExampleTo illustrate the advantages of the proposed grammar, weuse a testing case with long distance word reordering andthe source side parse trees.
We compare the translationfrom a strong phrasal decoder (DTM2) (Ittycheriah andRoukos, 2007), which is one of the top systems in NIST-08 evaluation for Arabic-English.
The translations fromboth decoders with the same training data (LM+TM) arein Table 13.
The highlighted parts in Figure 3 show that,the rules on partial trees are effectively selected and ap-plied for capturing long-distance word reordering, whichis otherwise rather difficult to get correct in a phrasal sys-tem even with a MaxEnt reordering model.6 Discussions and ConclusionsWe proposed a framework to learn models to predicthow to transform an elementary tree into its simplifiedforms for better executing the word reorderings.
Twotypes of operators were explored, including (a) trans-forming the trees via binarizations, grouping or deletinginterior nodes to change the structures; and (b) neighbor-ing boundary context to further disambiguate the reorder-ing decisions.
Significant improvements were observedon top of a strong baseline system, and consistent im-provements were observed across genres; we achieved acased BLEU of 55.16 for MT08-NW, which is signifi-cantly better than the officially reported results in NISTMT08 Arabic-English evaluations.853Src Sent qAl AlAmyr EbdAlrHmn bn EbdAlEzyz nA}b wzyr AldfAE AlsEwdy AlsAbq fy tSryH SHAfy An +hmtfA}l b# qdrp Almmlkp Ely AyjAd Hl l# Alm$klp .Phrasal Decoder prince abdul rahman bin abdul aziz , deputy minister of defense former saudi said in a press statementthat he was optimistic about the kingdom ?s ability to find a solution to the problem .Elm2Str+t?
(?, m?)
former saudi deputy defense minister prince abdul rahman bin abdul aziz said in a press statementthat he was optimistic of the kingdom ?s ability to find a solution to the problem .Table 13: A translation example, comparing with phrasal decoder.Figure 3: A testing case: illustrating the derivations from chart decoder.
The left panel is source parse tree for the Arabic sentence?
the input to our decoder; the right panel is the English translation together with the simplified derivation tree and alignmentfrom our decoder output.
Each ?X?
is a nonterminal in the grammar rule; a ?Block?
means a phrase pair is applied to rewrite anonterminal; ?Glue?
and ?Hiero?
means the unlabeled rules were chosen to explain the span as explained in ?
3.4.3 ; ?Tree?
meansa labeled rule is applied for the span.
For instance, for the source span [1,10], a rule is applied on a partial tree with PV and NP-SBJ;for the span [18,23], a rule is backed off to an unlabeled rule (Hiero-alike); for the span [21,22], it is another partial tree of NPs.Within the proposed framework, we also presentedseveral special cases including the translation boundariesfor nonterminals in SCFG for translation.
We achieveda high accuracy of 84.7% for predicting such bound-aries using MaxEnt model on machine parse trees.
Fu-ture works aim at transforming such non-projectable treesinto projectable form (Eisner, 2003), driven by translationrules from aligned data(Burkett et al, 2010), and infor-mative features form both the source 3 and the target sides(Shen et al, 2008) to enable the system to leverage more3The BLEU score on MT08-NW has been improved to 57.55 sincethe acceptance of this paper, using the proposed technique but with ourGALE P5 data pipeline and setups.isomorphic trees, and avoid potential detour errors.
Weare exploring the incremental decoding framework, like(Huang and Mi, 2010), to improve pruning and speed.AcknowledgmentsThis work was partially supported by the Defense Ad-vanced Research Projects Agency under contract No.HR0011-08-C-0110.
The views and findings contained inthis material are those of the authors and do not necessar-ily reflect the position or policy of the U.S. governmentand no official endorsement should be inferred.We are also very grateful to the three anonymous re-viewers for their suggestions and comments.854ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent.
J.Della Pietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estimation.
InComputational Linguistics, volume 19(2), pages 263?331.David Burkett, John Blitzer, and Dan Klein.
2010.
Joint pars-ing and alignment with weakly synchronized grammars.
InProceedings of HLT-NAACL, pages 127?135, Los Angeles,California, June.
Association for Computational Linguistics.Marine Carpuat, Yuval Marton, and Nizar Habash.
2010.Reordering matrix post-verbal subjects for arabic-to-englishsmt.
In 17th Confrence sur le Traitement Automatique desLangues Naturelles, Montral, Canada, July.Stanley F. Chen and Stephen M. Chu.
2010.
Enhanced wordclassing for model m. In Proceedings of Interspeech.Stanley F. Chen.
2009.
Shrinking exponential language mod-els.
In Proceedings of NAACL HLT,, pages 468?476.David Chiang.
2007.
Hierarchical phrase-based translation.
InComputational Linguistics, volume 33(2), pages 201?228.David Chiang.
2010.
Learning to translate with source andtarget syntax.
In Proc.
ACL, pages 1443?1452.Chris Dyer, Hendra Setiawan, Yuval Marton, and Philip Resnik.2009.
The University of Maryland statistical machine trans-lation system for the Fourth Workshop on Machine Trans-lation.
In Proceedings of the Fourth Workshop on Statisti-cal Machine Translation, pages 145?149, Athens, Greece,March.Jason Eisner.
2003.
Learning Non-Isomorphic tree mappingsfor Machine Translation.
In Proc.
ACL-2003, pages 205?208.Ahmad Emami, Stanley F. Chen, Abe Ittycheriah, HagenSoltau, and Bing Zhao.
2010.
Decoding with shrinkage-based language models.
In Proceedings of Interspeech.Bryant Huang and Kevin Knight.
2006.
Relabeling syntaxtrees to improve syntax-based machine translation quality.
InProc.
NAACL-HLT, pages 240?247.Liang Huang and Haitao Mi.
2010.
Efficient incrementaldecoding for tree-to-string translation.
In Proceedings ofEMNLP, pages 273?283, Cambridge, MA, October.
Asso-ciation for Computational Linguistics.Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.
2010.Soft syntactic constraints for hierarchical phrase-based trans-lation using latent syntactic distributions.
In Proceedings ofthe 2010 EMNLP, pages 138?147.Abraham Ittycheriah and Salim Roukos.
2007.
Direct transla-tion model 2.
In Proc of HLT-07, pages 57?64.Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan,Wade Shen, Christine Moran, Richard Zens, Chris Dyer, On-drej Bojar, Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machine transla-tion.
In ACL, pages 177?180.Young-Suk Lee, Bing Zhao, and Xiaoqian Luo.
2010.
Con-stituent reordering and syntax models for english-to-japanesestatistical machine translation.
In Proceedings of Coling-2010, pages 626?634, Beijing, China, August.Yang Liu and Qun Liu.
2010.
Joint parsing and translation.In Proceedings of COLING 2010,, pages 707?715, Beijing,China, August.Daniel Marcu, Wei Wang, Abdessamad Echihabi, and KevinKnight.
2006.
Spmt: Statistical machine translation withsyntactified target language phraases.
In Proceedings ofEMNLP-2006, pages 44?52.Yuval Marton and Philip Resnik.
2008.
Soft syntactic con-straints for hierarchical phrased-based translation.
In Pro-ceedings of ACL-08: HLT, pages 1003?1011.Haitao Mi and Liang Huang.
2008.
Forest-based translationrule extraction.
In Proceedings of EMNLP 2008, pages 206?214.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-basedtranslation.
In In Proceedings of ACL-HLT, pages 192?199.Franz Josef Och.
2003.
Minimum error rate training in Statis-tical Machine Translation.
In ACL-2003, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-JingZhu.
2002.
Bleu: a method for automatic evaluation of ma-chine translation.
In Proc.
of the ACL-02), pages 311?318,Philadelphia, PA, July.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
A newstring-to-dependency machine translation algorithm with atarget dependency language model.
In Proceedings of ACL-08: HLT, pages 577?585, Columbus, Ohio, June.
Associa-tion for Computational Linguistics.Libin Shen, Bing Zhang, Spyros Matsoukas, Jinxi Xu, andRalph Weischedel.
2010.
Statistical machine translationwith a factorized grammar.
In Proceedings of the 2010EMNLP, pages 616?625, Cambridge, MA, October.
Asso-ciation for Computational Linguistics.Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Mic-ciulla, and John Makhoul.
2006.
A study of translation editrate with targeted human annotation.
In AMTA.W.
Wang, J.
May, K. Knight, and D. Marcu.
2010.
Re-structuring, re-labeling, and re-aligning for syntax-based sta-tistical machine translation.
In Computational Linguistics,volume 36(2), pages 247?277.Dekai Wu.
1997.
Stochastic inversion transduction grammarsand bilingual parsing of parallel corpora.
In ComputationalLinguistics, volume 23(3), pages 377?403.Deyi Xiong, Min Zhang, and Haizhou Li.
2010.
Learn-ing translation boundaries for phrase-based decoding.
InNAACL-HLT 2010, pages 136?144.Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew LimTan, and Sheng Li.
2008.
A tree sequence alignment-basedtree-to-tree translation model.
In ACL-HLT, pages 559?567.Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and Chew LimTan.
2009.
Forest-based tree sequence to string translationmodel.
In Proc.
of ACL 2009, pages 172?180.Bing Zhao and Yaser Al-Onaizan.
2008.
Generalizing localand non-local word-reordering patterns for syntax-based ma-chine translation.
In Proceedings of EMNLP, pages 572?581, Honolulu, Hawaii, October.Bing Zhao and Shengyuan Chen.
2009.
A simplex armijodownhill algorithm for optimizing statistical machine trans-lation decoding parameters.
In Proceedings of HLT-NAACL,pages 21?24, Boulder, Colorado, June.
Association for Com-putational Linguistics.Andreas Zollmann and Ashish Venugopal.
2006.
Syntax aug-mented machine translation via chart parsing.
In Proc.
ofNAACL 2006 - Workshop on SMT, pages 138?141.855
