Coling 2008: Proceedings of the workshop on Knowledge and Reasoning for Answering Questions, pages 1?8Manchester, August 2008Semantic Chunk Annotation for complex questions using ConditionalRandom FieldShixi FanDepartment of computer scienceHarbin Institute of TechnologyShenzhen Graduate School,Shenzhen,518055, chinafanshixi@hit.edu.cnYaoyun ZhangDepartment of computer scienceHarbin Institute of TechnologyShenzhen Graduate School,Shenzhen,518055, chinaXiaoni5122@gmail.comWing W. Y. NgDepartment of computer scienceHarbin Institute of TechnologyShenzhen Graduate School,Shenzhen,518055, chinawing@hitsz.edu.cnXuan WangDepartment of computer scienceHarbin Institute of TechnologyShenzhen Graduate School,Shenzhen,518055, chinawangxuan@insun.hit.edu.cnXiaolong WangDepartment of computer scienceHarbin Institute of TechnologyShenzhen Graduate School,Shenzhen,518055, chinawangxl@insun.hit.edu.cnAbstractThis paper presents a CRF (ConditionalRandom Field) model for SemanticChunk Annotation in a Chinese Questionand Answering System (SCACQA).
Themodel was derived from a corpus of realworld questions, which are collectedfrom some discussion groups on theInternet.
The questions are supposed tobe answered by other people, so some ofthe questions are very complex.
Mutualinformation was adopted for feature se-lection.
The training data collection con-sists of 14000 sentences and the testingdata collection consists of 4000 sentences.The result shows an F-score of 93.07%.?
2008.
Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.1 Introduction1.1 Introduction of Q&A SystemAutomated question answering has been a hottopic of research and development since the ear-liest AI applications (A.M. Turing, 1950).
Sincethen there has been a continual interest in proc-essing knowledge and retrieving it efficiently tousers automatically.
The end of the 1980s saw aboost in information retrieval technologies andapplications, with an unprecedented growth inthe amount of digital information available, anexplosion of growth in the use of computers forcommunications, and the increasing number ofusers that have access to all this information(Diego Moll?and Jose?Luis Vicedo, 2007).Search engines such as Google, Yahoo, Baiduand etc have made a great success for people?sinformation need.Anyhow, search engines are keywords-basedwhich can only return links of relevant webpages, failing to provide a friendly user-interfacewith queries expressed in natural language sen-tences or questions, or to return precise answersto users.
Especially from the end of the 1990s, as1information retrieval technologies and method-ologies became mature and grew more slowly inpace, automated question answering(Q&A) sys-tems which accept questions in free natural lan-guage formations and return exactly the answeror a short paragraph containing relevant informa-tion has become an urgent necessity.
Major in-ternational evaluations such as TREC, CLEF andNTCIR have attracted the participation of manypowerful systems.The architecture of a Q&A system generally in-cludes three modules: question processing, can-didate answer/document retrieval, and answerextraction and re-ranking.1.2 Introduction of Question AnalyzingQuestion Analyzing, as the premise and founda-tion of the latter two modules, is of paramountimportance to the integrated performance of aQ&A system.
The reason is quite intuitive: aquestion contains all the information to retrievethe corresponding answer.
Misinterpretation ortoo much loss of information during the process-ing will inevitably lead to poor precision of thesystem.The early research efforts and evaluations inQ&A were focused mainly on factoid questionsasking for named entities, such as time, numbers,and locations and so on.
The questions in the testcorpus of TREC and other organizations are alsoin short and simple form.
Complex hierarchy inquestion types (Dragomir Radev et al 2001),question templates (Min-Yuh Day et al 2005),question parsing (Ulf Hermjakob, 2001) andvarious machine learning methods (Dell Zhangand Wee Sun Lee, 2003)are used for factoidquestion analysis, aiming to find what namedentity is asked in the question.
There are somequestions which are very complicated or evenneed domain restricted knowledge and reasoningtechnique.
Automatic Q&A system can not dealwith such questions with current technique.In china, there is a new kind of web based Q&Asystem which is a special kind of discussiongroup.
Unlike common discussion group, in theweb based Q&A system one user posts a ques-tion, other users can give answers to it.
It isfound that at least 50% percent questions(Valentin Jijkoun and Maarten de Rijke,2005)posted by users are non-factoid and surelymore complicated both in question pattern andinformation need than those questions in the testset of TREC and other FAQ.
An example is asfollows:This kind of Q&A system can complement thesearch engines effectively.
As the best searchengines in china, Baidu open the Baidu Knowl-edge2 Q&A system from 2003, and now it hasmore than 29 million question-answer pairs.There are also many other systems of this kindsuch as Google Groups, Yahoo Answers andSina Knowledge3.
This kind of system is a bigquestion-answer pair database which can betreated as a FAQ database.
How to search fromthe database and how to analyze the questions inthe database needs new methods and techniques.More deeper and precise capture of the semanticsin those complex questions is required.
This phe-nomenon has also been noticed by some re-searchers and organizations.
The spotlight gradu-ally shifted to the processing and semantic un-derstanding of complex questions.
From 2006,TREC launched a new annually evaluationCIQ&A (complex, interactive Question Answer-ing), aiming to promote the development of in-teractive systems capable of addressing complexinformation needs.
The targets of national pro-grams AQUAINT and QUETAL are all at newinterface and new enhancements to current state-of-the-art Q&A systems to handle more complexinputs and situations.A few researchers and institutions serve as pio-neers in complex questions study.
Different tech-nologies, such as definitions of different sets ofquestion types, templates and sentence patterns(Noriko Tomuro, 2003) (Hyo-Jung Oh et al2005) machine learning methods (Radu Soricutand Eric Brill, 2004), language translation model(Jiwoon Jeon, W et al 2005), composition ofinformation needs of the complex question(Sanda Harabagiu et al 2006) and so on, havebeen experimented on the processing of complexquestion, gearing the acquired information to thefacility of other Q&A modules.Several major problems faced now by researcherof complex questions are stated as follow:First: Unlike factoid questions, it is very dif-ficult to define a comprehensive type hierarchyfor complex questions.
Different domains underresearch may require definitions of different setsof question types, as shown in (Hyo-Jung Oh etal, 2005).
Especially, the types of certain ques-2 http://zhidao.baidu.com/3 http://iask.sina.com.cn/2tions are ambiguous and hard to identify.
Forexample:This question type can be treated as definition,procedure or entity.Second: Lack of recognition of different seman-tic chunks and the relations between them.FAQFinder (Radu Soricut and Eric Brill, 2004)also used semantic measure to credit the similar-ity between different questions.
Nevertheless, thequestion similarity is only a simple summation ofthe semantic similarity between words from thetwo question sentences.
Question pattern are veryuseful and easy to implement, as justified by pre-vious work.
However, just like the problem withquestion types, question patterns have limitationon the coverage of all the variations of complexquestion formation.
Currently, after the questionprocessing step in most systems, the semanticmeaning of large part of complex questions stillremain vague.
Besides, confining user?s inputonly within the selection of provided pattern maylead to unfriendly and unwelcome user interface.
(Ingrid Zukerman and Eric Horvitz, 2001) useddecision tree to model and recognize the infor-mation need, question and answer coverage,topic, focus and restrictions of a question.
Al-though features employed in the experimentswere described in detail, no selection process ofthose feature, or comparison between them wasmentioned.This paper presents a general method for Chinesequestion analyzing.
Our goal is to annotate thesemantic chunks for the question automatically.2 Semantic Chunk AnnotationChinese language differs a lot from English inmany aspects.
Mature methodologies and fea-tures well-justified in English Q&A systems arevaluable sources of reference, but no direct copyis possible.The Ask-Answer system 4  is a Chinese onlineQ&A system where people can ask and answerquestions like other web based Q&A system.
Thecharacteristic of this system is that it can give theanswer automatically by searching from theasked question database when a new question ispresented by people.
The architecture of theautomatically answer system is shown in figure 1.The system contains a list of question-answerpairs on particular subject.
When users input a4 http://haitianyuan.com/qaquestion from the web pages, the question issubmitted to the system and then question-answer pair is returned by searching from thequestions asked before.
The system includes fourmain parts: question pre-processing, questionanalyzing, searching and answer getting.The question pre-processing part will segmentthe input questions into words, label POS tagsfor every word.
Sometimes people ask two ormore questions at one time, the questions shouldbe made into simple forms by conjunctive struc-ture detection.
The question analyzing programwill find out the question type, topic, focus andetc.
The answer getting part will get the answerby computing the similarity between the inputquestion and the questions asked before.
Thequestion analyzing part annotates the semanticchunks for the question.
So that the question canbe mapped into semantic space and the questionsimilarity can be computed semantically.
TheSemantic chunk annotation is the most importantpart of the system.Question Pre- processingSegmentation and  postaggingDetect conjunctive structureQuestion AnalyzingSemantic chunk annotationGet and extend key wordsQuestion pattern and knowledge baseSearch reference question-answer pairs form databaseAnswer gettingScore the constituentanswersOut put the topfive answersFigure 1 the architecture of the automaticallyanswer systemCurrently, no work has been reported yet on thequestion semantic chunk annotation in Chinese.The prosperity of major on-line discussiongroups provides an abundant ready corpus forquestion answering research.
Using questionscollected from on-line discussion groups; wemake a deep research on semantic meanings andbuild a question semantic chunk annotationmodel based on Conditional Random Field.Five types of semantic chunks were defined:Topic, Focus, Restriction, Rubbish informationand Interrogative information.
The topic of a3question which is the topic or subject asked is themost important semantic chunk.
The focus of aquestion is the asking point of the question.
Therestriction information can restrict the question?sinformation need and the answers.
The rubbishinformation is those words in the question thathas no semantic meanings for the question.
Inter-rogative information is a semantic tag set whichcorresponds to the question type.
The interroga-tive information includes interrogative words,some special verbs and nouns words and all thesewords together determine the question type.
Thesemantic chunk information is shown in table 1.Semanticchunk   tagAbbreviation MeaningTopic T The question subjectFocus F The additional informationof topicRestrictRe Such as Time restriction andlocation restrictionRubbishinformationRu Words no meaning for thequestionOther O other information withoutsemantic meaningThe following is interrogative informationQuantity WquaDescription Wdes The answer need descriptionYes/No Wyes The answer should be yes ornoList Wlis The answer should be a listof entityDefinition Wdef The answer is the definitionof topicLocation Wloc The answer is locationReason Wrea The answer can explain thequestionContrast Wcon The answer is the compari-son of the items proposed inthe questionPeople Wwho The answer is about thepeople?s informationChoice Wcho The answer is one of thechoice proposed in the ques-tionTime Wtim The answer is the data ortime length about the eventin the questionEntity Went The answer is the attributeof the topic.Table 1: Semantic chunksAn annotation example question is as follows:This question can be annotated as follows:This kind of annotation is not convenient for CRFmodel, so the tags were transfer into the B I Oform.
(Shown as follows)Then the Semantic chunk annotation can betreated as a sequence tag problem.3 Semantic Chunk Annotation model3.1 Overview of the CRF modelThe conditional random field (CRF) is a dis-criminative probabilistic model proposed by JohnLafferty, et al(2001) to overcome the long-rangedependencies problems associated with genera-tive models.
CRF was originally designed to la-bel and segment sequences of observations, butcan be used more generally.
Let X, Y be randomvariables over observed data sequences and cor-responding label sequences, respectively.
Forsimplicity of descriptions, we assume that therandom variable sequences X and Y have thesame length, and use [ ]mxxxx ......, 21=and [ ]myyyy ......, 21=  to represent instances ofX and Y, respectively.
CRF defines the condi-tional probability distribution P(Y |X) of labelsequences given observation sequences as fol-lows)),(exp()(1)|(1?==niii YXfXZXYP ???
(1)Where  is the normalizing factor thatensures equation 2.)(XZ??
=y xyP 1)|(?
(2)In equation 2 the i?
is a model parameter andis a feature function (often binary-valued) that becomes positive (one for binary-valued feature function) when X contains a cer-tain feature in a certain position and Y takes acertain label, and becomes zero otherwise.Unlike Maximum Entropy model which use sin-gle normalization constant to yield a joint distri-bution, CRFs use the observation-dependentnormalization  for conditional distribu-tions.
So CRFs can avoid the label biased prob-lem.
Given a set of training data),( YXfi)(XZ?
}....2,1),,{( nkyxT kk ==With an empirical distribution , CRF ),(~YXP4determines the model parameters }{ i??
=  bymaximizing the log-likelihood of the training set)|(log),()|(log)(,~1xyPyxPxyPPyxNkkk?????
?=?=                       (3)3.2 Features for the modelThe following features, which are used for train-ing the CRF model, are selected according to theempirical observation and some semantic mean-ings.
These features are listed in the followingtable.Feature type in-dexFeature type name1 Current word2 Current POS tag3 Pre-1 word POS tag4 Pre-2 word POS tag5 Post -1 word POS tag6 Post -2 word POS tag7 Question pattern8 Question type9 Is pattern key word10 Pattern tagTable 2: the Features for the modelCurrent word:The current word should be considered whenadding semantic tag for it.
But there are toomany words in Chinese language and only partof them will contribute to the performance, a setof words was selected.
The word set includessegment note and some key words such as timekey word and rubbish key word.
When the cur-rent word is in the word set the current word fea-ture is the current word itself, and null on theother hand.Current POS tag:Current POS tag is the part of speech tag for thecurrent word.Pre-1 word POS tag:Pre- 1 word POS tag is the POS tag of the firstword before the labeling word in the sentence.
Ifthe Pre-1 word does not exit (the current is thefirst word in the sentence), the Pre- 1 word POStag is set to null.Pre-2 word POS tag:Pre- 2 word POS tag is the POS tag of the secondword before the labeling word in the sentence.
Ifthe Pre-2 word does not exit, the Pre- 2 wordPOS tag is set to null.Post -1 word POS tag:Post - 1 word POS tag is the POS tag of the firstword after the labeling word in the sentence.
Ifthe Post -1 word does not exit (the current is thefirst word in the sentence), the Post - 1 word POStag is set to null.Post -2 word POS tag:Post - 2 word POS tag is the POS tag of the sec-ond word after the labeling word in the sentence.If the Post-2 word does not exit, the Pre- 2 wordPOS tag is set to null.Question pattern:Question pattern which is associated with ques-tion type, can locate question topic, question fo-cus by surface string matching.
For example,(where is <topic>).
The patterns are extractedfrom the training data automatically.
When a pat-tern is matched, it is treated as a feature.
Thereare 1083 question patterns collected manually.Question type:Question type is an important feature for ques-tion analyzing.
The question patterns have theability of deciding the question type.
If there isno question pattern matching the question, thequestion type is defined by a decision tree algo-rithm.Is pattern key word:For each question pattern, there are some keywords.
When the current word belongs to thepattern key word this feature is set to ?yes?, elseit is set to ?no?.Pattern tag:When a pattern is matched, the topic, focus andrestriction can be identified by the pattern.
Wecan give out the tags for the question and the tagsare treated as features.
If there is no pattern ismatched, the feature is set to null.4 Feature Selection experimentFeature selection is important in classifying sys-tems such as neural networks (NNs), MaximumEntropy, Conditional Random Field and etc.
Theproblem of feature selection has been tackled bymany researchers.
Principal component analysis(PCA) method and Rough Set Method are oftenused for feature selection.
Recent years, mutualinformation has received more attention for fea-ture selection problem.According to the information theory, the uncer-tainty of a random variable X can be measuredby its entropy .
For a classifying problem,there are class label set represented by C and fea-ture set represented by F. The conditional en-tropy  measures the uncertainty about)(XH)|( FCH5C when F is known, and the Mutual informationI(C, F) is defined as:F)|(C -(C));( HHFCI =                   (4)The feature set is known; so that the objective oftraining the model is to minimize the conditionalentropy   equally maximize the mutualinformation .
In the feature set F, somefeatures are irrelevant or redundant.
So that thegoal of a feature selection problem is to find afeature S ( ), which achieve the highervalues of .
The set S is a subset of F andits size should be as small as possible.
There aresome algorithms for feature selection problem.The ideal greedy selection algorithm using mu-tual information is realized as follows (NojunKwak and Chong-Ho Choi, 2002):)|( FCH);( FCIFS ?
);( FCIInput:   S- an empty setF- The selected feature setOutput:  a small reduced feature set S which isequivalent to FStep 1: calculate the MI with the Classset C , , compute  Ffi ??
);( ifCIStep 2: select the feature that maximizes ,set);( ifCI}{},{\ ii fSfFF ?
?Step 3: repeat until desired number of featuresare selected.1) Calculate the MI with the Class set C and S,Ffi ??
, compute  ),;( ifSCI2) Select the feature that maximizes ,set),;( ifSCI}{},{\ ii fSfFF ?
?Step 4: Output the set S  that contains the se-lected featuresTo calculate MI the PDFs (Probability Distribu-tion Functions) are required.
When features andclassing types are dispersing, the probability canbe calculated statistically.
In our system, thePDFs are got from the training corpus statistically.The training corpus contains 14000 sentences.The training corpus was divided into 10 parts,with each part 1400 sentences.
And each part isdivided into working set and checking set.
Theworking set, which contains 90% percent data,was used to select feature by MI algorithm.
Thechecking set, which contains 10% percent data,was used to test the performance of the selectedfeature sequence.
When the feature sequence wasselected by the MI algorithm, a sequence of CRFmodels was trained by adding one feature at eachtime.
The checking data was used to test the per-formance of these models.The open test resultSelected featuresequence1 2 3 4 5 6 7 8 9 107, 10, 3, 1, 5, 2,4, 6, 8?90.5104 0.8764 0.8864 0.8918 0.8925 0.8977 0.8992 0.9023 0.9025 0.90187, 10, 1, 3, 5, 2,4?6?8?90.5241 0.8775 0.8822 0.8911 0.8926 0.8956 0.8967 0.9010 0.9005 0.90077, 10, 1, 3, 5, 2,4, 6?8?90.5090 0.8691 0.8748 0.8851 0.8852 0.8914 0.8929 0.8955 0.8955 0.89497, 10, 1, 3, 5, 2,4, 6?9?80.5157 0.8769 0.8823 0.8913 0.8925 0.8978 0.8985 0.9017 0.9018 0.90107, 10, 1, 3, 5, 2,4, 6?8?90.5144 0.8821 0.8856 0.8921 0.8931 0.8972 0.8981 0.9010 0.9009 0.90077, 10, 3, 1, 5, 2,4?6?8?90.5086 0.8795 0.8876 0.8914 0.8919 0.8960 0.8967 0.9016 0.9013 0.90117, 10, 1, 3, 5, 2,4, 6, 8, 90.5202 0.8811 0.8850 0.8920 0.8931 0.8977 0.8980 0.9015 0.9013 0.90097, 10, 1, 3, 5, 2,4, 6?8?90.5015 0.8858 0.8879 0.8948 0.8942 0.8998 0.8992 0.9033 0.9027 0.90237, 10, 1, 3, 5, 2,4, 6?8?90.5179 0.8806 0.8805 0.8898 0.8908 0.8954 0.8958 0.8982 0.8982 0.89867, 10, 1, 3, 5, 2,4, 6, 8?90.5153 0.8921 0.8931 0.9006 0.9012 0.9041 0.9039 0.9071 0.9068 0.9067Table 3: the feature selection result and the test resultIn table 3, each row contains data correspondingto one part of the training corpus so there are tenrows with data in the table.
The third row corre-sponds to the first part and the last row corre-sponds to the tenth part.
There are eleven col-umns in the table, the first columns is the fea-tures sequence selected by the mutual informa-tion algorithm for each part.
The second columnis the open test result with the first feature in thefeature sequence.
The third column is the opentest result with the first two features in the fea-ture sequence and so on.
From the table, it is6clear that the feature 7(Question pattern) and10(Pattern tag) are very important, while the fea-ture 8(Question type) and 9(Is pattern key word)are not necessary.
The explanation about thisphenomenon is that the ?pattern key word?
and?Question type?
information can be covered bythe Question patterns.
So feature 8 and 9 are notused in the Conditional Random Field model.5 Semantic Chunk Annotation Experi-mentThe test and training data used in our system arecollected from the website (Baidu knowledgeand the Ask-Answer system), where people pro-posed questions and answers.
The training dataconsists of 14000 and the test data consists of4000 sentences.
The data set consists of wordtokens, POS and semantic chunk tags.
The POSand semantic tags are assigned to each word to-kens.The performance is measured with three rates:precision (Pre), recall (Rec) and F-score (F1).Pre = Match/Model                     (5)Rec=Match/Manual                    (6)F1=2*Pre*Rec/(Pre+Rec)              (7)Match is the count of the tags that was predictedright.
Model is the count of the tags that was pre-dicted by the model.
Manual is the count of thetags that was labeled manually.Table 4 shows the performance of annotation ofdifferent semantic chunk types.
The first columnis the semantic chunk tag.
The last three columnsare precision, recall and F1 value of the semanticchunk performance, respectively.Label Manual Model Match Pre.
() Rec.
() F1B-T?I-T 17061?78462 16327?80488 14825?76461 90.80?95.00 86.89?97.45 88.80?96.21B-F?I-F 5072?13029 5079?13583 4657?12259 91.69?90.25 91.82?94.09 91.75?92.13B-Ru?I-Ru 775?30 11?0 2?0 18.18?0.00 0.26?0.00 0.51?0.00O 8354 8459 6676 78.92 79.91 79.41B-Wqua?I-Wqua 1363?934 1327?1028 1298?881 97.81?85.70 95.23?94.33 96.51?89.81B-Wyes?I-Wyes 5669?1162 5702?1098 5550?1083 97.33?98.63 97.90?93.20 97.62?95.84B-Wdes?I-Wdes 2907?278 2855?185 2779?184 97.34?99.46 95.60?66.19 96.46?79.48B-Wlis?I-Wlis 603?257 563?248 560?248 99.47?100 92.87?96.50 96.05?98.22B-Wdef?I-Wdef 1420?1813 1430?1878 1280?1695 89.51?90.26 90.14?93.49 89.82?91.85B-Wloc?I-Wloc 683?431 665?395 661?392 99.40?99.24 96.78?90.95 98.07?94.92B-Wrea?I-Wrea 902?159 873?83 843?82 96.56?98.80 93.46?51.57 94.99?67.77B-Wcon?I-Wcon 552?317 515?344 503?291 97.67?84.59 91.12?91.80 94.28?88.05B-Wwho?I-Wwho 420?364 357?350 348?336 97.48?96.00 82.86?92.31 89.58?94.12B-Wcho?I-Wcho 857?85 738?0 686?0 92.95?0.00 80.05?0.00 86.02?0.00B-Wtim?I-Wtim 408?427 401?419 355?380 88.53?90.69 87.01?88.99 87.76?89.83B-Went?I-Went 284?150 95?81 93?80 97.89?98.77 32.75?53.33 49.08?69.26Avg 145577 145577 135488 93.07 93.07 93.07Table 4: the performance of different semantic chunkThe semantic chunk type of ?Topic?
and ?Focus?can be annotated well.
Topic and focus semanticchunks have a large percentage in all the seman-tic chunks and they are important for questionanalyzing.
So the result is really good for thewhole Q&A system.As for ?Rubbish?
semantic chunk, it only has0.51 and 0.0 F1 measure for B-Ru and I-Ru.
Onereason is lacking enough training examples, forthere are only 1031 occurrences in the trainingdata.
Another reason is sometimes restriction iscomplex.6 Conclusion and future workThis paper present a new method for Chinesequestion analyzing based on CRF.
The featuresare selected by using mutual information algo-rithm.
The selected features work effectively forthe CRF model.
The experiments on the test dataset achieve 93.07% in F1 measure.
In the future,new features should be discovered and newmethods will be used.AcknowledgmentThis work is supported by Major Program of Na-tional Natural Science Foundation of China(No.60435020 and No.
90612005) and the HighTechnology Research and Development Programof China (2006AA01Z197).ReferencesA.M.
Turing.
1950.
Computing Machinery andIntelligence.
Mind, 236 (59): 433~460.Diego Moll?, Jose?Luis Vicedo.
2007.
QuestionAnswering in Restricted Domains: An Overview.Computational Linguistics, 33(1),7Dragomir Radev, WeiGuo Fan, Leila Kosseim.
2001.The QUANTUM Question Answering System.TREC.Min-Yuh Day, Cheng-Wei Lee, Shih-Hung WU,Chormg-Shyong Ong,  Wen-Lian Hsu.
2005.
AnIntegrated Knowledge-based and MachineLearning Approach for Chinese QuestionClassification.
Proceedings of the IEEEInternational Conference on Natural LanguageProcessing and Knowledge Engineering, Wuhan,China,:620~625.Ulf Hermjakob.
2001.
Parsing and QuestionClassification for Question Answering.Proceedings of the ACL Workshop on Open-Domain Question Answering, Toulouse,:19~25.Dell Zhang, Wee Sun Lee.
2003.
Questionclassification using support vector machines.Proceedings of the 26th Annual International ACMConference on Research and Development inInformation Retrieval(SIGIR), Toronto, Canada,26~ 32.Valentin Jijkoun, Maarten de Rijke.2005.
RetrievingAnswers from Frequently Asked Questions Pageson the Web.
CIKM?05, Bermen, Germany.Noriko Tomuro.
2003.
Interrogative ReformulationPatterns and Acquisition of Question Paraphrases.Proceeding of the Second International Workshopon Paraphrasing, :33~40.Hyo-Jung Oh, Chung-Hee Lee, Hyeon-Jin Kim,Myung-Gil Jang.
2005.
Descriptive QuestionAnswering in Encyclopedia.
Proceedings of theACL Interactive Poster and Demonstration Sessions,pages 21?24, Ann Arbor.Radu Soricut, Eric Brill.
2004, Automatic QuestionAnswering: Beyond the Factoid.
Proceedings ofHLT-NAACL ,:57~64.Jiwoon Jeon, W. Bruce Croft and Joon Ho Lee.
2005.Finding Similar Questions in Large Question andAnswer Archives.
CIKM?05, Bremen, Germany.Sanda Harabagiu, Finley Lacatusu and Andrew Hickl.2006 .
Answering Complex Questions with RandomWalk Models.
SIGIR?06, Seattle, Washington,USA.pp220-227.Ingrid Zukerman, Eric Horvitz.
2001.
Using MachineLearning Techniques to Interpret WH-questions.ACL.John Lafferty, Andrew McCallum, Fernando Pereira.2001.
Conditional Random Fields: probabilisticModels for Segmenting and Labeling SequenceData.
Proceedings of the Eighteenth InternationalConference on Machine Learning, p.282-289.Nojun Kwak and Chong-Ho Choi.
2002.
Inputfeature selection for classification problems.IEEE Trans on Neural Networks,,13(1):143-1598
