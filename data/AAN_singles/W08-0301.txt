Proceedings of the Third Workshop on Statistical Machine Translation, pages 1?8,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsAn Empirical Study in Source Word Deletionfor Phrase-based Statistical Machine TranslationChi-Ho Li, Dongdong Zhang, Mu Li, Ming ZhouMicrosoft Research AsiaBeijing, Chinachl, dozhang@microsoft.commuli, mingzhou@microsoft.comHailei ZhangNortheastern University of ChinaShenyang, Chinahailei.zh@gmail.comAbstractThe treatment of ?spurious?
words of sourcelanguage is an important problem but oftenignored in the discussion on phrase-basedSMT.
This paper explains why it is impor-tant and why it is not a trivial problem, andproposes three models to handle spurioussource words.
Experiments show that anysource word deletion model can improve aphrase-based system by at least 1.6 BLEUpoints and the most sophisticated modelimproves by nearly 2 BLEU points.
Thispaper also explores the impact of trainingdata size and training data domain/genre onsource word deletion.1 IntroductionIt is widely known that translation is by nomeans word-to-word conversion.
Not only be-cause sometimes a word in some language trans-lates as more than one word in another language,also every language has some ?spurious?
wordswhich do not have any counterpart in other lan-guages.
Consequently, an MT system should beable to identify the spurious words of the sourcelanguage and not translate them, as well as to gen-erate the spurious words of the target language.This paper focuses on the first task and studieshow it can be handled in phrase-based SMT.An immediate reaction to the proposal of inves-tigating source word deletion (henceforth SWD)is: Is SWD itself worth our attention?
Isn?t it atrivial task that can be handled easily by existingtechniques?
One of the reasons why we need topay attention to SWD is its significant improve-ment to translation performance, which will beshown by the experiments results in section 4.2.Another reason is that SWD is not a trivial task.While some researchers think that the spuriouswords of a language are merely function wordsor grammatical particles, which can be handledby some simple heuristics or statistical means,there are in fact some tricky cases of SWD whichneed sophisticated solution.
Consider the follow-ing example in Chinese-to-English translation: inEnglish we have the subordinate clause ?accord-ing to NP?, where NP refers to some source ofinformation.
The Chinese equivalent of thisclause can sometimes be ?ACCORDING-TO/?
?NP EXPRESS/,+?
; that is, in Chinese we couldhave a clause rather than a noun phrase followingthe preposition ACCORDING-TO/??.
There-fore, when translating Chinese into English, thecontent word EXPRESS/,+ should be consid-ered spurious and not to be translated.
Of course,the verb EXPRESS/,+ is not spurious in othercontexts.
It is an example that SWD is not onlyabout a few function words, and that the solu-tion to SWD has to take context-sensitive factorsinto account.
Moreover, the solution needed forsuch tricky cases seems to be beyond the scopeof current phrase-based SMT, unless we have avery large amount of training data which cov-ers all possible variations of the Chinese pattern?ACCORDING-TO/??
NP EXPRESS/,+?.Despite the obvious need for handling spuri-ous source words, it is surprising that phrase-based SMT, which is a major approach to SMT,does not well address the problem.
There aretwo possible ways for a phrase-based system todeal with SWD.
The first one is to allow a source1language phrase to translate to nothing.
How-ever, no existing literature has mentioned sucha possibility and discussed the modifications re-quired by such an extension.
The second way isto capture SWD within the phrase pairs in trans-lation table.
That is, suppose there is a foreignphrase F?
= (fAfBfC) and an English phraseE?
= (eAeC), where fA is aligned to eA and fCto eC , then the phrase pair (F?
, E?)
tacitly deletesthe spurious word fB .
Such a SWD mechanismfails when data sparseness becomes a problem.
Ifthe training data does not have any word sequencecontaining fB , then the spurious fB cannot asso-ciate with other words to form a phrase pair, andtherefore cannot be deleted tacitly in some phrasepair.
Rather, the decoder can only give a phrasesegmentation that treats fB itself as a phrase, andthis phrase cannot translate into nothing, as faras the SMT training and decoding procedure re-ported by existing literature are used.
In sum, thecurrent mechanism of phrase-based SMT is notcapable of handling all cases of SWD.In this paper, we will present, in section 3, threeSWD models and elaborate how to apply eachof them to phrase-based SMT.
Experiment set-tings are described in section 4.1, followed by thereport and analysis of experiment results, usingBLEU as evaluation metric, in section 4.2, whichalso discusses the impact of training data size andtraining data domain on SWD models.
Beforemaking our conclusions, the effect of SWD on an-other evaluation metric, viz.
METEOR, is exam-ined in section 5.2 Literature ReviewResearch work in SMT seldom treats SWD asa problem separated from other factors in trans-lation.
However, it can be found in differ-ent SMT paradigms the mechanism of handlingSWD.
As to the pioneering IBM word-basedSMT models (Brown et al, 1990), IBM mod-els 3, 4 and 5 handle spurious source words byconsidering them as corresponding to a particularEMPTY word token on the English side, and by thefertility model which allows the English EMPTYto generate a certain number of foreign words.As to the hierarchical phrase-based ap-proach (Chiang, 2007), its hierarchical rules aremore powerful in SWD than the phrase pairsin conventional phrase-based approach.
Forinstance, the ?ACCORDING-TO/??
NP EX-PRESS/,+?
example in the last section can behandled easily by the hierarchical ruleX ?<??
X,+, according to X > .In general, if the deletion of a source worddepends on some context cues, then the hier-archical approach is, at least in principle, ca-pable of handling it correctly.
However, it isstill confronted by the same problem as the con-ventional phrase-based approach regarding thosewords whose ?spuriousness?
does not depend onany context.3 Source Word Deletion ModelsThis section presents a number of solutions to theproblem of SWD.
These solutions share the sameproperty that a specific empty symbol ?
on the tar-get language side is posited and any source wordis allowed to translate into ?.
This symbol is in-visible in every module of the decoder except thetranslation model.
That is, ?
is not counted whencalculating language model score, word penaltyand any other feature values, and it is omitted inthe final output of the decoder.
It is only used todelete spurious source words and refine transla-tion model scores accordingly.It must be noted that in our approach phrasescomprising more than one source word are not al-lowed to translate into ?.
This constraint is basedon our subjective evaluation of alignment matrix,which indicates that the un-alignment of a con-tinuous sequence of two or more source words isfar less accurate than the un-alignment of a sin-gle source word lying within aligned neighbors.Consequently, in order to treat a source word asspurious, the decoder must give a phrase segmen-tation that treats the word itself as a phrase.Another important modification to the phrase-based architecture is a new feature added to thelog-linear model.
The new feature, ?-penalty, rep-resents how many source words translate into ?.The purpose of this feature is the same as thatof the feature of word penalty.
As many featuresused in the log-linear model have values of log-arithm of probability, candidate translations withmore words have, in general, lower scores, and2Model 1 P (?
)Model 2 P (?|f)Model 3 PCRF (?|~F (f)Table 1: Summary of the Three SWD Modelstherefore the decoder has a bias towards shortertranslations.
Word penalty (in fact, it should berenamed as word reward) is used to neutralizethis bias.
Similarly, the more source words trans-late into ?, the shorter the translation will be,and therefore the higher score the translation willhave.
The ?-penalty is proposed to neutralize thebias towards shorter translations.The core of the solutions is the SWD model,which calculates P (?|f), the probability distribu-tion of translating some source word f to ?.
ThreeSWD models will be elaborated in the followingsubsections.
They differ from each other by theconditions of the probability distribution, as sum-marized in Table 1.
Model 1 is a uniform prob-ability distribution that does not take the sourceword f into account.
Model 2 is a simple proba-bility distribution conditioned on the lexical formof f only.
Model 3 is a more complicated distribu-tion conditioned on a feature vector of f , and thedistribution is estimated by the method of Condi-tional Random Field.3.1 Model 1: Uniform ProbabilityThe first model assumes a uniform probabilityof translation to ?.
This model is inspired bythe HMM-based alignment model (Och and Ney,2000a), which posits a probability P0 for align-ment of some source word to the empty wordon the target language side, and weighs all otheralignment probabilities by the factor 1 ?
P0.
Inthe same style, SWD model 1 posits a probabilityP (?)
for the translation of any source word to ?.The probabilities of normal phrase pairs shouldbe weighed accordingly.
For a source phrasecontaining only one word, its weight is simplyP (??)
= 1 ?
P (?).
As to a source phrase con-taining more than one word, it implies that everyword in the phrase does not translate into ?, andtherefore the weighing factor P (??)
should be mul-tiplied as many times as the number of words inthe source phrase.
In sum, for any phrase pair< F?
, E?
>, its probability isP (E?|F? )
={P (?)
ifE?
= (?
)P (??)|F?
|PT (E?|F? )
otherwisewhere PT (E?|F? )
is the probability of the phrasepair as registered in the translation table, and |F?
|is the length of the phrase F?
.
The estimation ofP (?)
is done by MLE:P (?)
= number of unaligned source word tokensnumber of source word tokens .3.2 Model 2: EMPTY as Normal WordModel 1 assumes that every word is as likely to bespurious as any other word.
Definitely this is nota reasonable assumption, since certain functionwords and grammatical particles are more likelyto be spurious than other words.
Therefore, in oursecond SWD model the probability of translatinga source word f to ?
is conditioned on f itself.This probability, P (?|f), is in the same form asthe probability of a normal phrase pair, P (E?|F?
),if we consider ?
as some special phrase of the tar-get language and f as a source language phraseon its own.
Thus P (?|f) can be estimated andrecorded in the same way as the probability ofnormal phrase pairs.
During the phase of phraseenumeration, in addition to enumerating all nor-mal phrase pairs, we also enumerate all unalignedsource words f and add phrase pairs of the form< (f), (?)
>.
These special phrase pairs, TO-EMPTY phrase pairs, are fed to the module ofphrase scoring along with the normal phrase pairs.Both types of phrase pairs are then stored in thetranslation table with corresponding phrase trans-lation probabilities.
It can be seen that, since theprobabilities of normal phrase pairs are estimatedin the same procedure as those of TO-EMPTYphrase pairs, they do not need re-weighing as inthe case of SWD model 1.3.3 Model 3: Context-sensitive ModelAlthough model 2 is much more informative thanmodel 1, it is still unsatisfactory if we considerthe problem of SWD as a problem of tagging.The decoder can be conceived as if it carries outa tagging task over the source language sentence:each source word is tagged either as ?spurious?
or?non-spurious?.
Under such a perspective, SWD3model 2 is merely a unigram tagging model, andit uses only one feature template, viz.
the lex-ical form of the source word in hand.
Such amodel can by no means encode any contextualinformation, and therefore it cannot handle the?ACCORDING-TO/??
NP EXPRESS/,+?
ex-ample in section 1.An obvious solution to this limitation is a morepowerful tagging model augmented with context-sensitive feature templates.
Inspired by researchwork like (Lafferty et al, 2001) and (Sha andPereira, 2003), our SWD model 3 uses first-orderConditional Random Field (CRF) to tackle thetagging task.1 The CRF model uses the follow-ing feature templates:1. the lexical form and the POS of the foreignword f itself;2. the lexical forms and the POSs of f?2, f?1,f+1, and f+2, where f?2 and f?1 are the twowords to the left of f , and f+1 and f+2 arethe two words to the right of f ;3. the lexical form and the POS of the headword of f ;4. the lexical forms and the POSs of the depen-dent words of f .The lexical forms are the major source of infor-mation whereas the POSs are employed to allevi-ate data sparseness.
The neighboring words areused to capture local context information.
For ex-ample, in Chinese there is often a comma afterverbs like ?said?
or ?stated?, and such a commais not translated to any word or punctuation inEnglish.
These spurious commas are thereforeidentified by their immediate left neighbors.
Thehead and dependent words are employed to cap-ture non-local context information found by somedependency parser.
For the ?ACCORDING-TO/??
NP EXPRESS/,+?
example in section 1,the Chinese word ACCORDING-TO/??
is thehead word of EXPRESS/,+.
The spurious to-ken of EXPRESS/,+ in this pattern can be dis-tinguished from the non-spurious tokens throughthe feature template of head word.1Maximum Entropy was also tried in our experiments butits performance is not as good as CRF.The training data for the CRF model comprisesthe alignment matrices of the bilingual trainingdata for the MT system.
A source word (token)in the training data is tagged as ?non-spurious?
ifit is aligned to some target word(s), otherwise it istagged as ?spurious?.
The sentences in the train-ing data are also POS-tagged and parsed by somedependency parser, so that each word can be as-signed values for the POS-based feature templatesas well as the feature templates of head word anddependency words.The trained CRF model can then be used toaugment the decoder to tackle the SWD problem.An input source sentence should first be POS-tagged and parsed for assigning feature values.The probability for f being spurious, P (?|f), isthen calculated by the trained CRF model asPCRF (spurious|~F (f)).The probability for f being non-spurious is sim-ply 1 ?
P (?|f).
For a normal phrase pair< F?
, E?
> recorded in the translation table,its phrase translation probability and the lexicalweight should be re-weighed by the probabilitiesof non-spuriousness.
The weighing factor is?fi?F?(1?
P (?|fi)),since the translation of F?
into E?
means the de-coder considers every word in F?
as non-spurious.4 Experiments4.1 Experiment SettingsA series of experiments were run to compare theperformance of the three SWD models against thebaseline, which is the standard phrase-based ap-proach to SMT as elaborated in (Koehn et al,2003).
The experiments are about Chinese-to-English translation.
The bilingual training datais the one for NIST MT-2006.
The GIGAWORDcorpus is used for training language model.
Thedevelopment/test corpora are based on the testsets for NIST MT-2005/6.The alignment matrices of the training data areproduced by the GIZA++ (Och and Ney, 2000b)word alignment package with its default settings.The subsequent construction of translation tablewas done in exactly the same way as explained4in (Koehn et al, 2003).
For SWD model 2,the phrase enumeration step is modified as de-scribed in section 3.2.
We used the Stanfordparser (Klein and Manning, 2003) with its defaultChinese grammar for its POS-tagging as well asfinding the head/dependent words of all sourcewords.
The CRF toolkit used for model 3 isCRF++2.
The training data for the CRF modelshould be the same as that for translation tableconstruction.
However, since there are too manyinstances (every single word in the training datais an instance) with a huge feature space, no pub-licly available CRF toolkit can handle the entiretraining set of NIST MT-2006.3 Therefore, wecan use at most only about one-third of the NISTtraining set (comprising the FBIS, B1, and T10sections) for CRF training.The decoder in the experiments is our re-implementation of HIERO (Chiang, 2007), aug-mented with a 5-gram language model and a re-ordering model based on (Zhang et al, 2007).Note that no hierarchical rule is used with the de-coder; the phrase pairs used are still those usedin conventional phrase-based SMT.
Note also thatthe decoder does not translate OOV at all evenin the baseline case, and thus the SWD modelsdo not improve performance simply by removingOOVs.In order to test the effect of training data size onthe performance of the SWD models, three varia-tions of training data were used:FBIS Only the FBIS section of the NIST trainingset is used as training data (for both transla-tion table and the CRF model in model 3).This section constitutes about 10% of the en-tire NIST training set.
The purpose of thisvariation is to test the performance of eachmodel when very small amount of data areavailable.BFT Only the B1, FBIS, and T10 sections of theNIST training set are used as training data.These sections are about one-third of the en-tire NIST training set.
The purpose of this2http://crfpp.sourceforge.net/3Apart from CRF++, we also tried FLEX-CRF (http://flexcrfs.sourceforge.net) and MALLET(http://mallet.cs.umass.edu).Data baseline model 1 model 2 model 3FBIS 28.01 29.71 29.48 29.64BFT 29.82 31.55 31.61 31.75NIST 29.77 31.39 31.33 31.71Table 2: BLEU scores in Experiment 1: NIST?05 asdev and NIST?06 as testvariation is to test each model when mediumsize of data are available.4NIST All the sections of the NIST training setare used.
The purpose of this variation is totest each model when a large amount of dataare available.
(Case-insensitive) BLEU-4 (Papineni et al,2002) is used as the evaluation metric.
In eachtest in our experiments, maximum BLEU trainingwere run 10 times, and thus there are 10 BLEUscores for the test set.
In the following we willreport the mean scores only.4.2 Experiment Results and AnalysisTable 2 shows the results of the first experiment,which uses the NIST MT-2005 test set as develop-ment data and the NIST MT-2006 test set as testdata.
The most obvious observation is that anySWD model achieves much higher BLEU scorethan the baseline, as there is at least 1.6 BLEUpoint improvement in each case, and in some casethe improvement of using SWD is nearly 2 BLEUpoints.
This clearly proves the importance ofSWD in phrase-based SMT.The difference between the performance of thevarious SWD models is much smaller.
Yet thereare still some noticeable facts.
The first one isthat model 1 gives the best result in the case ofusing only FBIS as training data but it fails todo so when more training data is available.
Thisphenomenon is not strange since model 2 andmodel 3 are conditioned on more information andtherefore they need more training data.The second observation is about the strength ofSWD model 3, which achieves the best BLEUscore in both the BFT and NIST cases.
Whileits improvement over models 1 and 2 is marginalin the case of BFT, its performance in the NIST4Note also that the BFT data set is the largest trainingdata that the CRF model in model 3 can handle.5case is remarkable.
A suspicion to the strength ofmodel 3 is that in the NIST case both models 1and 2 use the entire NIST training set for esti-mating P (?
), while model 3 uses only the BFTsections to train its CRF model.
It may be thatthe BFT sections are more consistent with the testdata set than the other NIST sections, and there-fore a SWD model trained on BFT sections onlyis better than that trained on the entire NIST.
Thisconjecture is supported by the fact that in all foursettings the BLEU scores in the NIST case arelower than those in the BFT case, which suggeststhat other NIST sections are noisy.
While it is im-possible to test model 3 with the entire NIST, it ispossible to restrict the data for the estimation ofP (?|f) in model 1 to the BFT sections only andcheck if such a restriction helps.5 We estimatedthe uniform probability P (?)
from only the BFTsections and used it with the translation table con-structed from the complete NIST training set.
TheBLEU score thus obtained is 31.24, which is evenlower than the score (31.39) of the original caseof using the entire NIST for both translation tableand P (?|f) estimation.
In sum, the strength ofmodel 3 is not simply due to the choice of train-ing data.The test set used in Experiment 1 distinguishesitself from the development data and the trainingdata by its characteristics of combining text fromdifferent genres.
There are three sources of theNIST MT-2006 test set, viz.
?newswire?, ?news-group?, and ?broadcast news?, while our devel-opment data and the NIST training set comprisesonly newswire text and text of similar style.
It isan interesting question whether SWD only worksfor some genres (say, newswire) but not for othergenres.
In fact, it is dubious whether SWD fits thetest set to the same extent as it fits the develop-ment set.
That is, perhaps SWD contributes to theimprovement in Experiment 1 simply by improv-ing the translation of the development set which iscomposed of newswire text only, and SWD maynot benefit the translation of the test data at all.In order to test this conjecture, we ran Experi-ment 2, in which the SWD models were still ap-plied to the development data during training, but5Unfortunately this way does not work for model 2 asthe estimation of P (?|f) and the construction of translationtable are tied together.Data model 1 model 2 model 3FBIS 29.85 29.91 29.95BFT 31.73 31.84 32.08NIST 31.70 31.82 32.05Table 3: BLEU scores in Experiment 2, which is thesame as Experiment 1 but no word is deleted for testcorpus.
Note: the baseline scores are the same as thebaselines in Experiment 1 (Table 2).all SWD models stopped working when translat-ing the test data with the trained parameters.
Theresults are shown in Table 3.
These results arevery discouraging if we compare each cell in Ta-ble 3 against the corresponding cell in Table 2: inall cases SWD seems harmful to the translation ofthe test data.
It is tempting to accept the conclu-sion that SWD works for newswire text only.To scrutinize the problem, we split up the testdata set into two parts, viz.
the newswire sec-tion and the non-newswire section, and ran ex-periments separately.
Table 4 shows the resultsof Experiment 3, in which the development datais still the NIST MT-2005 test set and the testdata is the newswire section of NIST MT-2006test set.
It is confirmed that if test data sharesthe same genre as the training/development data,then SWD does improve translation performancea lot.
It is also observed that more sophisticatedSWD models perform better when provided withsufficient training data, and that model 3 exhibitsremarkable improvement when it comes to theNIST case.Of course, the figures in Table 5, which showsthe results of Experiment 4 where the non-newswire section of NIST MT-2006 test set isused as test data, still leave us the doubt that SWDis useful for a particular genre only.
After all, itis reasonable to assume that a model trained fromdata of a particular domain can give good perfor-mance only to data of the same domain.
On theother hand, the language model is another causeof the poor performance, as the GIGAWORD cor-pus is also of the newswire style.While we cannot prove the value of SWD withrespect to training data of other genres in themean time, we could test the effect of using de-velopment data of other genres.
In our last ex-periment, the first halves of both the newswire6apply SWD for test set no SWD for test setData model 1 model 2 model 3 model 1 model 2 model 3FBIS 30.81 30.81 30.68 29.23 29.61 29.46BFT 33.57 33.74 33.71 31.88 31.87 32.25NIST 33.65 34.01 34.42 32.14 32.59 32.87Table 4: BLEU scores in Experiment 3, which is the same as Experiments 1 and 2 but only the newswire sectionof NIST?06 test set is used.
Note: the baseline scores are the same as the baselines in Experiment 1 (Table 2).apply SWD for test set no SWD for test setData model 1 model 2 model 3 model 1 model 2 model 3FBIS 29.19 28.86 29.16 30.07 29.67 30.08BFT 30.62 30.64 30.86 31.66 31.83 32.00NIST 30.34 30.10 30.46 31.50 31.45 31.66Table 5: BLEU scores in Experiment 4, which is the same as Experiments 1 and 2 but only the non-newswiresection of NIST?06 test set is used.
Note: the baseline scores are the same as the baselines in Experiment 1(Table 2).Data baseline model 1 model 2 model 3FBIS 26.87 27.79 27.51 27.61BFT 29.11 30.38 30.49 30.41NIST 29.34 30.63 30.95 31.00Table 6: BLEU scores in Experiment 5: which is thesame as Experiment 1 but uses half of NIST?06 as de-velopment set and another half of NIST?06 as test set.and non-newswire sections of NIST MT-2006 testset are combined to form the new developmentdata, and the second halves of the two sectionsare combined to form the new test data.
The newdevelopment data is therefore consistent with thenew test data.
If SWD, or at least a SWD modelfrom newswire, is harmful to the non-newswiresection, which constitutes about 60% of the de-velopment/test data, then it will be either that theparameter training process minimizes the impactof SWD, or that the SWD model will make theparameter training process fail to search for goodparameter values.
The consequence of either caseis that the baseline setting should produce similaror even higher BLEU score than the settings thatemploy some SWD model.
Experiment results, asshown in Table 6, illustrate that SWD is still veryuseful even when both development and test setscontain texts of different genres from the trainingtext.
It is also observed, however, that the threeSWD models give rise to roughly the same BLEUscores, indicating that the SWD training data donot fit the test/development data very well as eventhe more sophisticated models are not benefitedfrom more data.5 Experiments using METEORThe results in the last section are all evaluated us-ing the BLEU metric only.
It is dubious whetherSWD is useful regarding recall-oriented metricslike METEOR (Banerjee and Lavie, 2005), sinceSWD removes information in source sentences.This suspicion is to certain extent confirmed byour application of METEOR to the translationoutputs of Experiment 1 (c.f.
Table 7), whichshows that all SWD models achieve lower ME-TEOR scores than the baseline.
However, SWD isnot entirely harmful to METEOR: if SWD is ap-plied to parameter tuning only but not for the testset, (i.e.
Experiment 2), even higher METEORscores can be obtained.
This puzzling observa-tion may be because the parameters of the de-coder are optimized with respect to BLEU score,and SWD benefits parameter tuning by improv-ing BLEU score.
In future experiments, maxi-mum METEOR training should be used insteadof maximum BLEU training so as to examine ifSWD is really useful for parameter tuning.7Experiment 1 Experiment 2SWD for both dev/test SWD for dev onlyData baseline model 1 model 2 model 3 model 1 model 2 model 3FBIS 50.07 47.90 49.83 49.34 51.58 51.08 51.17BFT 52.47 50.55 51.89 52.10 54.72 54.43 54.30NIST 52.12 49.86 50.97 51.59 54.14 53.82 54.01Table 7: METEOR scores in Experiments 1 and 26 Conclusion and Future WorkIn this paper, we have explained why the han-dling of spurious source words is not a trivialproblem and how important it is.
Three solu-tions, with increasing sophistication, to the prob-lem of SWD are presented.
Experiment resultsshow that, in our setting of using NIST MT-2006test set, any SWD model leads to an improvementof at least 1.6 BLEU points, and SWD model 3,which makes use of contextual information, canimprove up to nearly 2 BLEU points.
If onlythe newswire section of the test set is considered,SWD model 3 is even more superior to the othertwo SWD models.The effect of training data size on SWD hasalso been examined, and it is found that moresophisticated SWD models do not outperformunless they are provided with sufficient amountof data.
As to the effect of training data do-main/genre on SWD, it is clear that SWD modelstrained on text of certain genre perform the bestwhen applied to text of the same genre.
Whileit is infeasible for the time being to test if SWDworks well for non-newswire style of trainingdata, we managed to illustrate that SWD based onnewswire text still to certain extent benefits thetraining and translation of non-newswire text.In future, two extensions of our system areneeded for further examination of SWD.
The firstone is already mentioned in the last section: max-imum METEOR training should be implementedin order to fully test the effect of SWD regard-ing METEOR.
The second extension is about theweighing factor in models 1 and 3.
The currentimplementation assumes that all source wordsin a normal phrase pair need to be weighed by1?
P (?).
However, in fact some source words ina source phrase are tacitly deleted (as explainedin the Introduction).
Thus the word alignment in-formation within phrase pairs need to be recordedand the weighing of a normal phrase pair shouldbe done in accordance with such alignment infor-mation.ReferencesBrown, P., J. Cocke, S. Della Pietra, V. Della Pietra,F.
Jelinek, J. Lafferty, R. Mercer, and P. Roossin.1990.
A Statistical Approach to Machine Transla-tion Computational Linguistics, 16(2).Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
Pro-ceedings of Workshop on Evaluation Measures forMT and/or Summarization at ACL 2005.David Chiang.
2007.
Hierarchical Phrase-basedTranslation.
Computational Linguistics, 33(2).Dan Klein and Christopher D. Manning.
2003.
Ac-curate Unlexicalized Parsing.
Proceedings for ACL2003.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical Phrase-based Translation.
Proceedingsfor HLT-NAACL 2003.John Lafferty, Andrew McCallum, and FernandoPereira 2001.
Conditional Random Fields: Prob-abilistic Models for Segmenting and Labeling Se-quence Data.
Proceedings for 18th InternationalConf.
on Machine Learning.Franz J. Och, and Hermann Ney.
2000.
A comparisonof alignment models for statistical machine transla-tion.
Proceedings of COLING 2000.Franz J. Och, and Hermann Ney.
2000.
ImprovedStatistical Alignment Models.
Proceedings for ACL2000.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: a Method for Auto-matic Evaluation of Machine Translation.
Proceed-ings for ACL 2002.Fei Sha, Fernando Pereira.
2003.
Shallow parsingwith conditional random fields.
Proceedings ofNAACL 2003.Dongdong Zhang, Mu Li, Chi-Ho Li and MingZhou.
2007.
Phrase Reordering Model Integrat-ing Syntactic Knowledge for SMT.
Proceedings forEMNLP 2007.8
