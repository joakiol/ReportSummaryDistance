Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 116?123,Baltimore, Maryland USA, June 26, 2014.c?2014 Association for Computational LinguisticsSimilarity-Based Non-Scorable Response Detection for Automated SpeechScoringSu-Youn YoonEducational Testing ServicePrinceton, NJ, USAsyoon@ets.orgShasha XieMicrosoftSunnyvale, CA, USAshxie@microsoft.comAbstractThis study provides a method that iden-tifies problematic responses which makeautomated speech scoring difficult.
Whenautomated scoring is used in the contextof a high stakes language proficiency as-sessment, for which the scores are used tomake consequential decisions, some testtakers may have an incentive to try to gamethe system in order to artificially inflatetheir scores.
Since many automated pro-ficiency scoring systems use fluency fea-tures such as speaking rate as one of theimportant features, students may engagein strategies designed to manipulate theirspeaking rate as measured by the system.In order to address this issue, we de-veloped a method which filters out non-scorable responses based on text similar-ity measures.
Given a test response, themethod generated a set of features whichcalculated the topic similarity with theprompt question or the sample responsesincluding relevant content.
Next, an au-tomated filter which identified these prob-lematic responses was implemented us-ing the similarity features.
This filter im-proved the performance of the baselinefilter in identifying responses with topicproblems.1 IntroductionIn spoken language proficiency assessment, someresponses may include sub-optimal characteristicswhich make it difficult for the automated scor-ing system to provide a valid score.
For instance,some test takers may try to game the system byspeaking in their native languages or by citingmemorized responses for unrelated topics.
Oth-ers may repeat questions or part of questions withmodifications instead of generating his/her ownresponse.
Hereafter, we call these problematicresponses non-scorable (NS) responses.
By us-ing these strategies, test takers can generate flu-ent speech, and the automated proficiency scoringsystem, which utilizes fluency as one of the im-portant factors, may assign a high score.
In or-der to address this issue, the automated proficiencyscoring system in this study used a two-step ap-proach: these problematic responses were filteredout by a ?filtering model,?
and only the remainingresponses were scored using the automated scor-ing model.
By filtering out these responses, therobustness of the automated scoring system can beimproved.The proportion of NS responses, in the assess-ment of which the responses are scored by humanraters, are likely to be low.
For instance, the pro-portion of NS responses in the international En-glish language assessment used in this study was2%.
Despite this low proportion, it is a seriousproblem which has a strong impact on the validityof the test.
In addition, the likelihood of studentsengaging in gaming strategies may increase withthe use of automated scoring.
Therefore, an au-tomated filtering model with a high accuracy is anecessary step to use the automated scoring sys-tem as a sole rater.Both off-topic and copy responses have topic-related problems, although they are at the two ex-tremes in the degree of similarity.
Focusing onthe intermediate levels of similarity, Metzler et al.
(2005) presented a hierarchy of five similarity lev-els: unrelated, on the general topic, on the spe-cific topic, same facts, and copied.
In the auto-mated scoring of spontaneous speech, responsesthat fell into unrelated can be considered as off-topic, while the ones that fell into copied can beconsidered as repetition or plagiarism.
Follow-ing this approach, we developed a non-scorableresponse identification method utilizing similar-116Figure 1: A diagram of the overall architecture ofour method.ity measures.
We will show that this similaritybased method is highly efficient in identifying off-topic or repetition responses.
Furthermore, wewill show that the method can effectively detectNS responses that are not directly related to thetopicality issue (e.g, non-English responses).Figure 1 shows the overall architecture of ourmethod including the automated speech profi-ciency scoring system.
For a given spoken re-sponse, the system performs speech processing in-cluding speech recognition and generates a wordhypotheses and time stamps.
In addition, the sys-tem computes pitch and power; the system calcu-lates descriptive statistics such as the mean andstandard deviation of pitch and power at both theword level and response level.
Given the word hy-potheses and descriptive features of pitch/power,it derives features for automated proficiency scor-ing.
In addition, the similarity features are gener-ated based on the word hypotheses and topic mod-els.
Finally, given both sets of features, the filter-ing model filters out non-scorable responses, andthe remainder of the responses are scored using ascoring model.
A detailed description of the sys-tem is available from Zechner et al.
(2009).
In thisstudy, we will only focus on the filtering model.This paper will proceed as follows: we first re-view previous studies in section 2, then describethe data in section 3, and present the method andexperiment set-up in sections 4 and 5.
The resultsand discussion are presented in section 6, and theconclusions are presented in section 7.2 Related WorkFiltering of NS responses for automated speechscoring has been rarely recognized.
Only a fewpieces of research have focused on this task,and most studies have targeted highly restrictedspeech.
van Doremalen et al.
(2009) and Lo etal.
(2010) used normalized confidence scores ofa speech recognizer in recasting speech.
Theyidentified non-scorable responses with promisingperformances (equal error rates ranged from 10to 20%).
Cheng and Shen (2011) extended thesestudies and combined an acoustic model score, alanguage model score, and a garbage model scorewith confidence scores.
They applied this new fil-ter to less constrained items (e.g., picture descrip-tion) and identified off-topic responses with an ac-curacy rate of 90%with a false positive rate of 5%.Although normalized confidence scoresachieved promising performances in restrictedspeech, they may not be appropriate for the itemsthat elicit unconstrained spontaneous speech.Low confidence scores signal the use of wordsor phrases not covered by the language model(LM) and this is strongly associated with off-topicresponses in restricted speech in which the targetsentence is given.
However, in spontaneousspeech, this is not trivial; it may be associatedwith not only off-topic speech but also mismatchbetween the LM and speech input due to the lowcoverage of the LM.
Due to the latter case, thedecision based on the confidence score may notbe effective in measuring topic similarity.The topic similarity between two documentshas been frequently modeled by relative-frequencymeasures (Hoad and Zobel, 2003; Shivakumar andGarcia-Molina, 1995), document fingerprinting(Brin et al., 1995; Shivakumar and Garcia-Molina,1995; Shivakumar and Garcia-Molina, 1996)), andquery based information retrieval methods usingvector space models or language model (Sander-son, 1997; Hoad and Zobel, 2003).Document similarity measures have been ap-plied in automated scoring.
Foltz et al.
(1999)evaluated the content of written essays using latentsemantic analysis (LSA) by comparing the test es-says with essays of known quality in regard to theirdegree of conceptual relevance and the amount ofrelevant content.
In another approach, the lexicalcontent of an essay was evaluated by comparingthe words contained in each essay to the wordsfound in a sample of essays from each score cat-egory (Attali and Burstein, 2006).
More recently,Xie et al.
(2012) used a similar approach in au-tomated speech scoring; they measured the sim-ilarity using three similarity measures, includinga lexical matching method (Vector Space Model)and two semantic similarity measures (Latent Se-mantic Analysis and Pointwise Mutual Informa-tion).
They showed moderately high correlations117between the similarity features and human profi-ciency scores on even the output of an automaticspeech recognition system.
Similarity measureshave also been used in off-topic detection for non-native speakers?
essays.
Higgins et al.
(2006) cal-culated overlaps between the question and contentwords from the essay and obtained an error rate of10%.Given the promising performance in both auto-mated scoring and off-topic essay detection, wewill expand these similarity measures in NS re-sponse detection for speech scoring.3 DataIn this study, we used a collection of responsesfrom an international English language assess-ment.
The assessment was composed of items inwhich speakers were prompted to provide sponta-neous speech.Approximately 48,000 responses from 8,000non-native speakers were collected and used fortraining the automated speech recognizer (ASRset).
Among 24 items in the ASR set, four itemswere randomly selected.
For these items, a totalof 11,560 responses were collected and used forthe training and evaluation of filtering model (FMset).
Due to the extremely skewed distribution ofNS responses (2% in the ASR set), it was not easyto train and evaluate the filtering model.
In or-der to address this issue, we modified the distribu-tion of NS responses in the FM set.
Initially, wecollected 90,000 responses including 1,560 NS re-sponses.
While maintaining all NS responses, wedownsampled the scorable responses in the FM setto include 10,000 responses.
Finally, the propor-tion of NS responses was 6 times higher in FMset (13%) than ASR set.
This artificial increase ofthe NS responses reduces the current problem ofthe skewed NS distribution and may make the taskeasier.
However, the likelihood of students engag-ing in gaming strategies may increase with the useof automated scoring, and this increased NS dis-tribution may be close to this situation.Each response was rated by trained humanraters using a 4-point scoring scale, where 1 indi-cated a low speaking proficiency and 4 indicated ahigh speaking proficiency.
The raters also labeledresponses as NS, when appropriate.
NS responsesare defined as responses that cannot be given ascore according to the rubrics of the four-pointscale.
NS responses were responses with tech-nical difficulties (TDs) that obscured the contentof the responses or responses that would receivea score of 0 due to participants?
inappropriate be-haviors.
The speakers, item information, and dis-tribution of proficiency scores are presented in Ta-ble 1.
There was no overlap in the sets of speakersin the ASR and FM sets.In addition, 1,560 NS responses from the FMset were further classified into six types by tworaters with backgrounds in linguistics using therubrics presented in Table 2.
This annotation wasused for the purpose of analysis: to identify thefrequent types of NS responses and prioritize theresearch effort.Type Proportionin totalNSsDescriptionNR 73% No response.
Test taker doesn?tspeak.OR 16% Off-topic responses.
The re-sponse is not related to theprompt.TR 5% Generic responses.
The re-sponse only include filler wordsor generic responses such as, ?Idon?t know, it is too difficult toanswer, well?, etc.RE 4% Question copy.
Full or partialrepetition of question.NE 1% Non-English.
Responses is in alanguage other than English.OT 1% OthersTable 2: Types of zero responses and proportionsSome responses belonged to more than onetype, and this increased complexity of the anno-tation task.
For instance, one response was com-prised of a question copy and generic sentences,while another response was comprised of a ques-tion copy and off-topic sentences.
An example ofthis type was presented in Table 3.
This was a re-sponse for the question ?Talk about an interestingbook that you read recently.
Explain why it wasinteresting1.
?For these responses, annotators first segmentedthem into sentences and assigned the type that wasmost dominant.Each rater annotated approximately 1,000 re-sponses, and 586 responses were rated by both1In order to not reveal the real test question administeredin the operational test, we invented this question.
Based onthe question, we also modified a sample response; the ques-tion copy part was changed to avoid disclosure of the testquestion, but the other part remained the same as the originalresponse.118Data set Num.
responses Num.
speakers Num.
items Average score Score distributionNS 1 2 3 4ASR 48,000 8,000 24 2.63 773 1953 16834 23106 53342% 4% 35% 48% 11%FM 11,560 11,390 4 2.15 1560 734 4328 4263 67513% 6% 37% 37% 6%Table 1: Data size and score distributionSentence TypeWell in my opinion are the inter-esting books that I read recentlyis.RETalking about a interesting book.
REOne interesting book oh God in-teresting book that had read re-cently.REOh my God.
TRI really don?t know how to an-swer this question.TRWell I don?t know.
TRSorry.
TRTable 3: Manual transcription of complex-type re-sponseraters.
The Cohen?s kappa between two raters was0.76.
Among five different NS responses, non-response was the most frequent type (73%), fol-lowed by off-topic (16%).
The combination of thetwo types was approximately 90% of the entire NSresponses.4 MethodIn this study, we generated two different types offeatures.
First, we developed similarity features(both chunk-based and response-based) to identifythe responses with problems in topicality.
Sec-ondly, we generated acoustic, fluency, and ASR-confidence features using a state-of-art automatedspeech scoring system.
Finally, using both featuresets, classifiers were trained to make a binary dis-tinction of NS response vs. scorable response.4.1 Chunk-based similarity featuresSome responses in this study included more thantwo different types of the topicality problems.
Forinstance, the first three sentences in Table 3 be-longed to the ?copied?
category, while the othersentences fell into ?unrelated?.
If the similarityfeatures were calculated based on the entire re-sponse, the feature values may fall into neitherthe ?copied?
nor ?unrelated?
range because of thetrade-off between the two types at two extremes.In order to address this issue, we calculated chunk-based similarity features similar to Metzler et al.
(2005)?s sentence-based features.First, the response was split into the chunkswhich were surrounded by long silences with du-rations longer than 0.6 sec.
For each chunk,the proportion of word overlap with the question(WOL) was calculated based on the formula (1).Next, chunks with a WOL higher than 0.5 wereconsidered as question copies.WOL =|S?Q||S|where S is a response and Q is a question,|S ?
Q| is the number of word types that appearboth in S and Q,|S| is the number of word types in S(1)Finally, the following three features were de-rived for each response based on the chunk-basedWOL.?
numwds: the number of word tokens after re-moving question copies, fillers, and typicalgeneric sentences2;?
copyR: the proportion of question copies inthe response in terms of number of word to-kens;?
meanWOL: the mean ofWOLs for all chunksin the response.4.2 Response-based similarity featuresWe implemented three features based on a vectorspace model (VSM) using cosine similarity andterm frequency-inverse document frequency (tf -idf ) weighting to estimate the topic relevance atthe response-level.2Five sentences ?it is too difficult?, ?thank you?, ?I don?tknow?, ?I am sorry?, and ?oh my God?
were stored as typicalsentences and removed from responses119Since the topics of each question were differ-ent from each other, we trained a VSM for eachquestion separately.
For the four items in theFM set, we selected a total of 485 responses (125responses per item) from the ASR set for topicmodel training.
Assuming that the responses withthe highest proficiency scores contain the most di-verse and appropriate words related to the topic,we only selected responses with a score of 4.We obtained the manual transcriptions of the re-sponses, and all responses about the same ques-tion were converted into a single vector.
In thisstudy, the term was a unigram word, and the doc-ument was the response.
idf was trained from theentire set of 48,000 responses in the ASR trainingpartition, while tf was trained from the question-specific topic model training set.In addition to the response-based VSM, wetrained a question-based VSM.
Each question wascomposed of two sentences.
Each question wasconverted into a single vector, and a total of fourVSMs were trained.
idf was trained in the sameway as the response-based VSMs, while tf wastrained only using the question sentences.Using these two different types of VSMs, thefollowing three features were generated for eachresponse.?
sampleCosine: a similarity score based onthe response-based VSM.
Assuming that twodocuments with the same topic shared com-mon words, it measured the similarity in thewords used in a test response and the sampleresponses.
The feature was implemented toidentify off-topic responses (OR);?
qCosine: a similarity score based on thequestion-based VSM.
It measured the simi-larity between a test response and its ques-tion.
The feature was implemented to iden-tify both off-topic responses (OR) and ques-tion copy responses (RE); a low score ishighly likely to be an off-topic response,while a high score signals a full or partialcopy;?
meanIDF : mean of idfs for all word tokensin the response.
Generic responses (TR) tendto include many high frequency words suchas articles and pronouns, and the mean idfvalue of these responses may be low.4.3 Features from the automated speechscoring systemA total of 61 features (hereafter, A/S features)were generated using a state-of-the-art automatedspeech scoring system.
A detailed descriptionof the system is available from (Jeon and Yoon,2012).
Among these features, many features wereconceptually similar but based on different nor-malization methods, and they showed a stronginter-correlation.
For this study, 30 features wereselected and classified into three groups accordingto their characteristics: acoustic features, fluencyfeatures, and ASR-confidence features.The acoustic features were related to power,pitch, and MFCC.
First, power, pitch andMFCC were extracted at each frame usingPraat (Boersma, 2002).
Next, we generatedresponse-level features from these frame-level fea-tures by calculating mean and variation.
Thesefeatures captured the overall distribution of energyand voiced regions in a speaker?s response.
Thesefeatures are relevant since NS responses may havean abnormal distribution in energy.
For instance,non-responses contain very low energy.
In orderto detect these abnormalities in the speech signal,pitch and power related features were calculated.The fluency features measure the length of a re-sponse in terms of duration and number of words.In addition, this group contains features relatedto speaking rate and silences, such as mean du-ration and number of silences.
In particular, thesefeatures are effective in identifying non-responseswhich contain zero or only a few words.The ASR-confidence group contains featurespredicting the performance of the speech recog-nizer.
Low confidence scores signal low speechrecognition accuracy.4.4 Model trainingThree filtering models were trained to investigatethe impact of each feature group: a filtering modelusing similarity features (hereafter, the Similarity-filter), a filtering model using A/S features (here-after, the A/S-filter), and a filtering model using acombination of the two groups of features (here-after, the Combined-filter).5 ExperimentsAnHMM-based speech recognizer was trained us-ing the ASR set.
A gender independent triphoneacoustic model and a combination of bigram, tri-120gram, and four-gram language models were used.A word error rate (WER) of 27% on the held-outtest dataset was observed.For each response in the FM set, the wordhypotheses was generated using this recognizer.From this ASR-based transcription, the six simi-larity features were generated.
In addition, the 30A/S features described in 4.3 were generated.Using these two sets of features, filtering mod-els were trained using the Support Vector Ma-chine algorithm (SVM) with the RBF kernel ofthe WEKA machine-learning toolkit (Hall et al.,2009).
A 10 fold cross-validation was conductedusing the FM dataset.6 Results and discussionFirst, we will report the performance for the sub-set only topic-related NS responses.
The sim-ilarity features were designed to detect NS re-sponses with topicality issues, but the majority inthe FM set were non-response (73%).
The topic-related NS responses (off-topic responses, genericresponses, and question copy responses) were only25%.
In the entire set, the advantage of the simi-larity features over the A/S features might not besalient due to the high proportion of non-response.In order to investigate the performance of the sim-ilarity features in the topic related NS responses,we excluded all responses other than ?OR?, ?TR?,and ?RE?
from the FM set and conducted a 10 foldcross-validation.Table 4 presents the average of the 10 foldcross-validation results in this subset.
In this set,the total number of NS responses is 314, and theaccuracy of the majority voting (to classify all re-sponses as scorable responses) is 0.962.acc.
prec.
recall fscoreSimilarity-filter0.975 0.731 0.548 0.626A/S-filter 0.971 0.767 0.341 0.472Combined-filter0.977 0.780 0.566 0.656Table 4: Performance of filters in topic-related NSdetectionNot surprisingly, the Similarity-filter outper-formed the A/S-filter: the F-score was approxi-mately 0.63 which was 0.15 higher than that ofthe A/S-filter in absolute value.
The lack of fea-tures specialized for detection of topic abnormal-ity resulted in the low recall of the A/S-filter.
Thecombination of the two features achieved a slightimprovement: the F-score was 0.66 and it was 0.03higher than the Similarity-filter.In Metzler et al.
(2005)?s study, the system us-ing both sentence-based features and document-based features did not achieve further improve-ment over the system based on the document-based features alone.
In order to explore the im-pact of chunk-based features, similarity featureswere classified into two groups (chunk-based fea-tures vs. document-based features), and two fil-ters were trained using each group separately.
Ta-ble 5 compares the performance of the two filters(Similarity-chunk and Similarity-doc) with the fil-ter using all similarity features (Similarity).acc.
prec.
recall fscoreSimilarity-chunk0.972 0.700 0.442 0.542Similarity-doc0.971 0.730 0.396 0.514Similarity 0.975 0.731 0.548 0.626Table 5: Comparison of chunk-based anddocument-based similarity featuresIn this study, the chunk-based features werecomparable to the document-based features.
Fur-thermore, combination of the two features im-proved F-score.
The performance improvementmostly resulted from higher recall.Finally, Table 6 presents the results using theentire FM set, including the OR, TR, and RE re-sponses that were not included in the previousexperiment.
The accuracy of the majority classbaseline (classifying all responses as scorable re-sponses) is 0.865.acc.
prec.
recall fscoreSimilarity-filter0.976 0.926 0.895 0.910A/S-filter 0.974 0.953 0.849 0.898Combined-filter0.977 0.941 0.884 0.911Table 6: Performance of filters in all types of NSdetectionBoth the Similarity-filter and the A/S-filter achieved high performance.
Both accuraciesand F-scores were similar and the difference121between the two filters was approximately 0.01.The Similarity-filter achieved better performancethan the A/S-filter in recall: it was 0.89, whichwas substantially higher than the A/S-filter (0.85).It is an encouraging result that the Similarity-filter could achieve a performance comparableto the A/S-filter, which was based on multi-ple resources such as signal processing, forced-alignment, and ASR.
But, the combination of thetwo feature groups did not achieve further im-provement: the increase in both accuracy and F-measure was less than 0.01.7 ConclusionsIn this study, filtering models were implementedas a supplementary module for an automatedspeech proficiency scoring system.
In addition toA/S features, which have shown promising perfor-mance in previous studies, a set of similarity fea-tures were implemented and a filtering model wasdeveloped.
The Similarity-filter was more accu-rate than the A/S-filter in identifying the responseswith topical problems.
This result is encouragingsince the proportion of these responses is likely toincrease when the automated speech scoring sys-tem becomes a sole rater of the assessment.Although the Similarity-filter achieved betterperformance than the A/S-filter, it should be fur-ther improved.
The recall of the system was low,and approximately 45% of NS responses couldnot be identified.
In addition, the model requiressubstantial amount of sample responses for eachitem, and it will cause serious difficulty when it isused the real test situation.
In future, we will ex-plore the similarity features trained only using theprompt question or the additional prompt materi-als such as visual and audio materials.ReferencesYigal Attali and Jill Burstein.
2006.
Automated essayscoring with e?rater R v.2.
The Journal of Technol-ogy, Learning, and Assessment, 4(3).Paul Boersma.
2002.
Praat, a system for doing phonet-ics by computer.
Glot International, 5(9/10):341?345.Sergey Brin, James Davis, and Hector Garcia-Molina.1995.
Copy detection mechanisms for digital docu-ments.
In ACM SIGMOD Record, volume 24, pages398?409.
ACM.Jian Cheng and Jianqiang Shen.
2011.
Off-topic detec-tion in automated speech assessment applications.In Proceedings of InterSpeech, pages 1597?1600.IEEE.Peter W. Foltz, Darrell Laham, and Thomas K. Lan-dauer.
1999.
The Intelligent Essay Assessor: Appli-cations to educational technology.
Interactive mul-timedia Electronic Journal of Computer-EnhancedLearning, 1(2).Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H Witten.2009.
The weka data mining software: an update.ACM SIGKDD Explorations Newsletter, 11(1):10?18.Derrick Higgins, Jill Burstein, and Yigal Attali.
2006.Identifying off-topic student essays without topic-specific training data.
Natural Language Engineer-ing, 12(02):145?159.Timothy C Hoad and Justin Zobel.
2003.
Meth-ods for identifying versioned and plagiarized doc-uments.
Journal of the American society for infor-mation science and technology, 54(3):203?215.Je Hun Jeon and Su-Youn Yoon.
2012.
Acousticfeature-based non-scorable response detection for anautomated speaking proficiency assessment.
In Pro-ceedings of the InterSpeech, pages 1275?1278.Wai-Kit Lo, Alissa M Harrison, and Helen Meng.2010.
Statistical phone duration modeling to filterfor intact utterances in a computer-assisted pronun-ciation training system.
In Proceedings of Acous-tics Speech and Signal Processing (ICASSP), 2010IEEE International Conference on, pages 5238?5241.
IEEE.Donald Metzler, Yaniv Bernstein, W Bruce Croft, Al-istair Moffat, and Justin Zobel.
2005.
Similaritymeasures for tracking information flow.
In Proceed-ings of the 14th ACM international conference on In-formation and knowledge management, pages 517?524.
ACM.Mark Sanderson.
1997.
Duplicate detection in thereuters collection.
?
Technical Report (TR-1997-5)of the Department of Computing Science at the Uni-versity of Glasgow G12 8QQ, UK?.Narayanan Shivakumar and Hector Garcia-Molina.1995.
Scam: A copy detection mechanism for digi-tal documents.Narayanan Shivakumar and Hector Garcia-Molina.1996.
Building a scalable and accurate copy detec-tion mechanism.
In Proceedings of the first ACMinternational conference on Digital libraries, pages160?168.
ACM.Joost van Doremalen, Helmet Strik, and Cartia Cuc-chiarini.
2009.
Utterance verification in languagelearning applications.
In Proceedings of the SLaTE.122Shasha Xie, Keelan Evanini, and Klaus Zechner.
2012.Exploring content features for automated speechscoring.
In Proceedings of the 2012 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 103?111.
Association for Computa-tional Linguistics.Klaus Zechner, Derrick Higgins, Xiaoming Xi, andDavid M. Williamson.
2009.
Automatic scoringof non-native spontaneous speech in tests of spokenEnglish.
Speech Communication, 51(10):883?895.123
