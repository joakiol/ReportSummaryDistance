Tagging with Hidden Markov Models Using Ambiguous TagsAlexis NasrLaTTice - Universite?
Paris 7anasr@linguist.jussieu.frFre?de?ric Be?chetLaboratoire d?Informatiqued?Avignonfrederic.bechet@lia.univ-avignon.frAlexandra VolanschiLaTTice - Universite?
Paris 7avolansk@linguist.jussieu.frAbstractPart of speech taggers based on HiddenMarkov Models rely on a series of hypothe-ses which make certain errors inevitable.The idea developed in this paper consistsin allowing a limited, controlled ambiguityin the output of the tagger in order to avoida number of errors.
The ambiguity takesthe form of ambiguous tags which denotesubsets of the tagset.
These tags are usedwhen the tagger hesitates between the dif-ferent components of the ambiguous tags.They are introduced in an existing lexiconand 3-gram database.
Their lexical andsyntactic counts are computed on the basisof the lexical and syntactic counts of theirconstituents, using impurity functions.
Thetagging process itself, based on the Viterbialgorithm, is unchanged.
Experiments con-ducted on the Brown corpus show a recall of0.982, for an ambiguity rate of 1.233 whichis to be compared with a baseline recall of0.978 for an ambiguity rate of 1.414 usingthe same ambiguous tags and with a recallof 0.955 corresponding to the one best solu-tion of standard tagging (without ambigu-ous tags).1 IntroductionTaggers are commonly used as pre-processorsfor more sophisticated treatments like full syn-tactic parsing or chunking.
Although taggersachieve high accuracy, they still make somemistakes that quite often impede the followingstages.
There are at least two solutions to thisproblem.
The first consists in devising more so-phisticated taggers either by providing the tag-ger with more linguistic knowldge or by refiningthe tagging process, through better probabilityestimation, for example.
The second strategyconsists in allowing some ambiguity in the out-put of the tagger.
It is the second solution thatwas chosen in this paper.
We believe that thisis an instance of a more general problem in se-quential natural language processing chains, inwhich a module takes as input the output ofthe preceding module.
Since we cannot, in mostcases, expect a module to produce only correctsolutions, modules should be able to deal withambiguous input and ambiguous output.
In ourcase, the input is non ambiguous while the out-put is ambiguous.
From this perspective, thequality of the tagger is evaluated by the trade-off it achieves between accuracy and ambiguity.The introduction of ambiguous tags in thetagger output raises the question of the process-ing of these ambiguous tags in the post-taggingstages of the application.
Leaving some ambigu-ity in the output of the tagger only makes senseif these other processes can handle it.
In thecase of a chunker, ambiguous tags can be takeninto account through the use of weighted finitestate machines, as proposed in (Nasr and Volan-schi, 2004).
In the case of a syntactic parser,such a device can usually deal with some ambi-guity and discard the incorrect elements of anambiguous tag when they do not lead to a com-plete analysis of the sentence.
The parser itselfacts, in a sense, as a tagger since, while pars-ing the sentence, it chooses the right tag amonga set of possible tags for each word.
The rea-son why we still need a tagger and don?t let theparser do the job is time and space complexity.Parsers are usually more time and space con-suming than taggers and highly ambiguous tagsassignments can lead to prohibitive processingtime and memory requirements.The tagger described in this paper is basedon the standard Hidden Markov Model archi-tecture (Charniak et al, 1993; Brants, 2000).Such taggers assign to a sequence of wordsW = w1 .
.
.
wn , the part of speech tag sequenceT?
= t?1 .
.
.
t?n which maximizes the joint prob-ability P (T,W ) where T ranges over all possi-ble tag sequences of length n. The probabilityP (T,W ) is itself decomposed into a product of2n probabilities, n lexical probabilities P (wi|ti)(emission probabilities of the HMM) and n syn-tactic probabilites (transition probabilities of theHMM).
Syntactic probabilities model the prob-ability of the occurrence of tag ti given a historywhich is the knowledge of the h preceding tags(ti?1 .
.
.
ti?h).
Increasing the length of the his-tory increases the predictive power of the tag-ger but also the number of parameters to esti-mate and therefore the amount of training dataneeded.
Histories of length 2 constitute a com-mon trade-off for part of speech tagging.We define an ambiguous tag as a tag that de-notes a subset of the original tagset.
In the re-mainder of the paper, tags will be representedas subscripted capitals T : T1, T2 .
.
.. Ambigu-ous tags will be noted with multiple subscripts.T1,3,5 for example, denotes the set {T1, T3, T5}.We define the ambiguity of an ambiguous tag asthe cardinality of the set it denotes.
This notionis extended to non ambiguous tags, which canbe seen as singletons, their ambiguity is there-fore equal to 1.Ambiguous tags are actually new tags whoselexical and syntactic probability distributionsare computed on the basis of lexical and syn-tactic distributions of their constituents.
Thelexical and syntactic probability distributions ofTi1,...,in should be computed in such a way that,when a word in certain context can be taggedas Ti1 , .
.
.
, Tin with probabilities that are closeenough, the tagger should choose the ambiguoustag Ti1,...,in .The idea of changing the tagset in order to im-prove tagging accuracy has already been testedby several researchers.
(Tufis?
et al, 2000) re-ports experiments of POS tagging of Hungarianwith a large tagset (about one thousand differ-ent tags).
In order to reduce data sparsenessproblems, they devise a reduced tagset which isused for tagging.
The same kind of idea is de-veloped in (Brants, 1995).
The major differencebetween these approaches and ours, is that theydevise the reduced tagset in such a way that, af-ter tagging, a unique tag of the extended tagsetcan be recovered for each word.
Our perspectiveis significantly different since we allow unrecov-erable ambiguity in the output of the tagger andleave to the other processing stages the task ofreducing it.
In the HMM based taggers frame-work, our work bears a certain resemblance with(Brants, 2000) who distinguishes between reli-able and unreliable tag assignments using prob-abilities computed by the tagger.
Unreliabletag assignments are those for which the prob-ability is below a given threshold.
He showsthat taking into account only reliable assign-ments can significantly improve the accuracy,from 96.6% to 99.4%.
In the latter case, only64.5% of the words are reliably tagged.
For theremaining 35.5%, the accuracy is 91.6%.
Thesefigures show that taking into account probabil-ities computed by the tagger discriminates wellthese two situations.
The main difference be-tween his work and ours is that he does notpropose a way to deal with unreliable assign-ments, which we treat using ambiguous tags.The paper is structured as follows: section 2describes how the probability distributions ofthe ambiguous tags are estimated.
Section 3presents an iterative method to automaticallydiscover good ambiguous tags as well as an ex-periment on the Brown corpus.
Section 4 con-cludes the paper.2 Computing probabilitydistributions for ambiguous tagsProbabilistic models for part of speech taggersare built in two stages.
In a first stage, countsare collected from a tagged training corpuswhile in the second, probabilities are computedon the basis of these counts.
Two type of countsare collected: lexical counts, noted Cl(w, T )indicating how many times word w has beentagged T in the training corpus and syntacticcounts Cs(T1, T2, T3) indicating how manytimes the tag sequence T1, T2, T3 occurred inthe training corpus.
Lexical counts are storedin a lexicon and syntactic counts in a 3-gramdatabase.These real counts will be used to computefictitious counts for ambiguous tags on the basisof which probability distributions will be esti-mated.
The rationale behind the computationof the counts (lexical as well as syntactic) of anambiguous tag T1...j is that they must reflectthe homogeneity of the counts of {T1 .
.
.
Tj}.
Ifthey are all equal, the count of T1...j should bemaximal.Impurity functions (Breiman et al, 1984) per-fectly model this behavior1: an impurity func-tion ?
is a function defined on the set of all N-tuples of numbers (p1, .
.
.
, pN ) satisfying ?j ?
[1, .
.
.
, N ], pj ?
0 and?Nj=1 pj = 1 with the fol-lowing properties:1Entropy would be another candidate for such compu-tation.
The same experiments have also been conductedusing entropy and lead to almost the same results.?
?
reaches its maximum at the point( 1N , .
.
.
, 1N )?
?
achieves its minimum at the points(1, 0, .
.
.
, 0), (0, 1, .
.
.
, 0), .
.
.
(0, 0, .
.
.
, 1)Given an impurity function ?, we define theimpurity measure of a N-tuple of counts C =(c1, .
.
.
, cN ) as follows :I(c1, .
.
.
, cN ) = ?
(f1, .
.
.
, fN ) (1)where fi is the relative frequency of ci in C:fi =ci?Nk=1 ckThe impurity function we have used is theGini impurity criteria:?
(f1, .
.
.
, fN ) =?i6=jfifjwhose maximal value is equal to N?1N .The impurity measure will be used to com-pute both lexical and syntactic fictitious countsas described in the two following sections.2.1 Lexical countsLexical counts for an ambiguous tag T1,...,n arecomputed using lexical impurity Il(w, T1,...,n)which measures the impurity of the n-tuple(Cl(w, T1), .
.
.
, Cl(w, Tn)):Il(w, T1,...,n) = I(Cl(w, T1), .
.
.
, Cl(w, Tn))A high lexical impurity Il(w, T1,...,n) meansthat w is ambiguous with respect to the differ-ent classes T1, .
.
.
, Tn.
It reaches its maximumwhen w has the same probability to belong toany of them.
The lexical count Cl(w, T1,...,n) iscomputed using the following formula:Cl(w, T1,...,n) = Il(w, T1,...,n)n?i=1Cl(w, Ti)This formula is used to update a lexicon, foreach lexical entry, the counts of the ambiguoustags are computed and added to the entry.
Thetwo entries daily and deals whose original countsare represented below2:daily RB 32 JJ 41deals NNS 1 VBZ 132RB, JJ, NNS and VBZ stand respectively for adverb,adjective, plural noun and verb (3rd person singular,present).are updated to3:daily RB 32 JJ 41 JJ_RB 36deals NNS 1 VBZ 13 NNS_VBZ 22.2 Syntactic countsSyntactic counts of the form Cs(X,Y, T1,...,n)are computed using syntactic impurityIs(X,Y, T1,...,n) which measures the impurity ofthe n-tuple I(Cs(X,Y, T1), .
.
.
, Cs(X,Y, Tn)) :Is(X, Y, T1,...,n) = I(Cs(X, Y, T1), .
.
.
, Cs(X, Y, Tn))A maximum syntactic impurity means thatall the tags T1, .
.
.
, Tn have the same probabil-ity of occurrence after the tag sequence X Y .If any of them has a probability of occurrenceequal to zero after such a tag sequence, the im-purity is also equal to zero.
The syntactic countCs(X,Y, T1,...,n) is computed using the followingformula:Cs(X, Y, T1,...,n) = Is(X, Y, T1,...,n)n?i=1Cs(X, Y, Ti)Such a formula is used to update the 3-gram database in three steps.
First, syntac-tic counts of the form Cs(X,Y, T1,...,n) (withX and Y unambiguous) are computed, thensyntactic counts of the form Cs(X,T1,...,n, Y )(with X unambiguous and Y possibly ambigu-ous) and eventually, syntactic counts of the formCs(T1,...,n, X, Y ) (for X and Y possibly ambigu-ous).
The following four real 3-grams:A A A 100 A A B 100A B A 10 A B B 1000will give rise to following five fictitious ones:A A A_B 100 A A_B A 18A A_B A_B 31 A A_B B 181A B A_B 19which will be added to the 3-gram database.Note that the real 3-grams are not modified dur-ing this operation.Once the lexicon and the 3-gram databasehave been updated, both real and fictitiouscounts are used to estimate lexical and syntacticprobability distribution.
These probability dis-tributions constitute the model.
The taggingprocess itself, based on the Viterbi search algo-rithm, is unchanged.3The fictitious counts were rounded to the nearestinteger.2.3 Data sparsenessThe introduction of new tags in the tagset in-creases the number of states in the HMM andtherefore the number of parameters to be esti-mated.
It is important to notice that even if thenumber of parameters increases, the model doesnot become more sensitive to data sparsenessproblems than the original model was.
The rea-son is that fictitious counts are computed basedon actual counts.
The occurrence, in the train-ing corpus, of an event (as the occurrence ofa sequence of tags or the occurrence of a wordwith a given tag) is used for estimating both theprobability of the event associated to the sim-ple tag and the probabilities of the events asso-ciated with the ambiguous tags which containthe simple tag.
For example, the occurrence ofthe word w with tag T , in the training corpus,will be used to estimate the lexical probabil-ity P (w|T ) as well as the lexical probabilitiesP (w|T ?)
for every ambiguous tag T ?
of which Tmay be a component.3 Learning ambiguous tags fromerrorsSince ambiguous tags are not given a priori,candidates can be selected based on the errorsmade by the tagger.
The idea developed in thissection consists in learning iteratively ambigu-ous tags on the basis of the errors made by atagger.
When a word w tagged T1 in a refer-ence corpus has been wrongly tagged T2 by thetagger, that means that T1 and T2 are lexicallyand syntactically ambiguous, with respect to wand a given context.
Consequently, T1,2 is a po-tential candidate for an ambiguous tag.The process of discovering ambiguous tagsstarts with a tagged training corpus whosetagset is called T0.
A standard tagger, M0,is trained on this corpus.
M0 is used to tagthe training corpus.
A confusion matrix is thencomputed and the most frequent error is se-lected to form an ambiguous tag which is addedto T0 to constitute T1.
M0 is then updatedwith the new ambiguous tag to constitue M1,as described in section 2.
The process is iter-ated : the training corpus is tagged with Mi,the most frequent error is used to constitue Ti+1and a new tagger Mi+1 is built, based on Mi.The process continues until the result of the tag-ging on the development corpus converges or thenumber of iterations has reached a given thresh-old.3.1 ExperimentsThe model described in section 2 has beentested on the Brown corpus (Francis andKuc?era, 1982), tagged with the 45 tags of thePenn treebank tagset (Marcus et al, 1993),which constitute the initial tagset T0.
The cor-pus has been divided in a training corpus of961, 3 K words, a development corpus of 118, 6K words and a test corpus of 115, 6 K words.The development corpus was used to detect theconvergence and the final model was evaluatedon the test corpus.
The iterative tag learningalgorithm converged after 50 iterations.A standard trigram model (without ambigu-ous tags) M0 was trained on the training cor-pus using the CMU-Cambridge statistical lan-guage modeling toolkit (Clarkson and Rosen-feld, 1997).
Smoothing was done through back-off on bigrams and unigrams using linear dis-counting (Ney et al, 1994).The lexical probabilities were estimated onthe training corpus.
Unknown words (words ofthe development and test corpus not present inthe lexicon) were taken into account by a sim-ple technique: the words of the developmentcorpus not present in the training corpus wereused to estimate the lexical counts of unknownwords Cl(UNK, t).
During tagging, if a word isunknown, the probability distribution of wordUNK is used.
The development corpus contains4097 unknown words (3.4% of the corpus) andthe test corpus 3991 (3.3%).3.1.1 Evaluation measuresThe result of the tagging process consists in asequence of ambiguous and non ambiguous tags.This result can no longer be evaluated using ac-curacy alone (or word error rate), as it is usu-ally the case in part of speech tagging, since theintroduction of ambiguous tags allows the tag-ger to assign multiple tags to a word.
This iswhy two measures have been used to evaluatethe output of the tagger with respect to a goldstandard: the recall and the ambiguity rate.Given an output of the tagger T = t1 .
.
.
tn,where ti is the tag associated to word i by thetagger, and a gold reference R = r1 .
.
.
rn wherer1 is the correct tag for word wi, the recall of Tis computed as follows :REC(T ) =?ni=1 ?
(ri ?
ti)nwhere ?
(p) equals to 1 if predicate p is trueand 0 otherwise.
A recall of 1 means that forevery word occurrence, the correct tag is an el-ement of the tag given by the tagger.The ambiguity rate of T is computed as fol-lows :AMB(T ) =?ni=1 AMB(ti)nwhere AMB(ti) is the ambiguity of tag ti.
Anambiguity rate of 1 means that no ambiguoustag has been introduced.
The maximum ambi-guity rate for the development corpus (when allthe possible tags of a word are kept) is equal to2.4.3.1.2 Baseline modelsThe successive modelsMi are based on the dif-ferent tagsets Ti.
Their output is evaluated withthe two measures described above.
But thesefigures by themselves are difficult to interpret ifwe cannot compare them with the output of an-other tagging process based on the same tagset.The only point of comparision at hand is modelM0 but it is based on tagset T0, which does notcontain ambiguous tags.
In order to create sucha point of comparison, a baseline model Bi isbuilt at every iteration.
The general idea is toreplace in the training corpus, all occurrences oftags that appear as an element of an ambigu-ous tag of Ti by the ambiguous tag itself.
Afterthe replacement stage, a model Bi is computedand used to tag the development corpus.
Theoutput of the tagging is evaluated using recalland ambiguity rate and can be compared to theoutput of model Mi.The replacement stage described above is ac-tually too simplistic and gives rise to very poorbaseline models.
There are two problems withthis approach.
The first is that a tag Ti can ap-pear as a member of several ambiguous tags andwe must therefore decide which one to choose.The second, is that a word tagged Ti in the ref-erence corpus might be unambiguous, it wouldtherefore be ?unfair?
to associate to it an am-biguous tag.
This is the reason why the replace-ment step is more elaborate.
At iteration i, foreach couple (wj , Tj) of the training corpus, alookup is done in the lexicon, which gives accessto all the possible non ambiguous tags word wjcan have.
If there is an ambiguous tag T inTi such that all its elements are possible tags ofwj then, couple (wj , Tj) is replaced with (wj , T )in the corpus.
If several ambiguous tags fulfillthis condition, the ambiguous tag which has thehighest lexical count for wj is chosen.Another simple way to build a baseline wouldbe to produce the n best solutions of the tag-ger, then take for each word of the input thetags associated to it in the different solutionsand make an ambiguous tag out of these tags.This solution was not adopted for two reasons.The first is that this method mixes tags fromdifferent solutions of the tagger and can leadto completely incoherent tags sequences.
It isdifficult to measure the influence of this inco-herence on the post-tagging stages of the ap-plication and we didn?t try to measure it em-pirically.
But the idea of potentially producingsolutions which are given very poor probabili-ties by the model is unappealing.
The secondreason is that we cannot control anymore whichambiguous tags will be created (although thisfeature might be desirable in some cases).
Itwill be therefore difficult to compare the resultwith our models (the tagsets will be different).43.1.3 ResultsThe results of the successive models have beenplotted in figure 1 and summarized in table 1,which also shows the results on the test corpus.For each iteration i, recall and ambiguity ratesof modelsMi and Bi on the development corpuswere computed.
The results show, as expected,that recall and ambiguity rate increase with theincrease of the number of ambiguous tags addedto the tagset.
This is true for both models Miand Bi.
The figure also shows that recall of Bi,for a given i, is generally a bit lower than Miwhile its ambiguity is higher.
Figure 2 showsthat for the same recall Bi introduces more am-biguous tags than Mi.The list of the 20 first ambiguous tags createdduring the process is represented below :1 IN_RB 11 IN_WDT_WP2 DT_IN_WDT_WP 12 VBD_VBN3 JJ_VBN 13 JJ_NN_NNP_NNS_RB_VBG4 NN_VB 14 JJ_NN_NNP5 JJ_NN 15 JJ_NN_NNP_NNS_RB6 IN_RB_RP 16 JJR_RBR4As a point of comparison we will nevertheless give afew figures here.
For low values of n, the n best solutionshave better recall for a given value of the ambiguity rate.For instance, the 4 best tagger output yields a recall of0.9767 for an ambiguity rate of 1.12, while, for the sameambiguity rate, the iterative method obtains a 0.9604 re-call.
However, the 0.982 recall value which we attainedat the end of the iterative ambiguous tag learning pro-cedure, corresponding to an ambiguity rate of 1.23, wasalso reached by keeping the 7 best solutions of the tag-ger, with an ambiguity rate of 1.20 (only slightly betterthan ours).0.950.960.970.980.9910  5  10  15  20  25  30  35  40  45  5011.051.11.151.21.251.31.351.41.451.5recallambiguityiterationsrecallambiguityrecall (baseline)ambiguity (baseline)Figure 1: Recall and ambiguity rate of the suc-cessive models on development corpus11.051.11.151.21.251.31.350.955  0.96  0.965  0.97  0.975ambiguityrecallModel MiBaselineFigure 2: Comparing ambiguity rates for a fixedvalue of recall7 NNPS_NNS 17 NN_VBG8 VB_VBP 18 CD_NN9 JJ_RB 19 WDT_WP10 DT_RB 20 JJ_NN_NNP_NNSModel DEV TESTREC AMB REC AMBM0 = B0 0.955 1 0.955 1B40 0.978 1.414 0.979 1.418M40 0.980 1.232 0.982 1.232Table 1: Results on development and test cor-pus3.1.4 Model efficiencyThe original idea of our method consists in cor-recting errors that were made by M0, throughthe introduction of ambiguous tags.
Ideally, wewould like models Mi with i > 0 to introducean ambiguous tag only where M0 made a mis-take.
Unfortunately, it is not always the case.We have classified the use of ambiguous tagsinto four situations function of their influenceon both recall and ambiguity rate as indicatedin table 2, where G stands for the gold standard.In situations 1 and 2 model M0 made a mis-take.
In situation 1, the mistake was correctedby the introduction of the ambiguous tag whilein situation 2 it was not.
In situations 3 and 4,modelM0 did not make a mistake.
In situation3 the introduction of the ambiguous tag did notcreate a mistake while it did in situation 4.Situation G M0 Mi REC AMB1 T1 T2 T1,2 + +2 T3 T4 T1,2 0 +3 T1 T1 T1,2 0 +4 T3 T3 T1,2 ?
+Table 2: Influence of the introduction of an am-biguous tag on recall and ambiguity ratesThe frequency of each situation for some ofthe 20 first ambiguous tags has been reportedin table 3.
The last column of the table indicatesthe frequency of the ambiguous tag (number ofoccurrences of this tag divided by the sum ofoccurrences of all ambiguous tags).
The figuresshow that ambiguous tags are not very efficient:only a moderate proportion of their occurrences(24% on average) actually corrected an error.While we are very rarely confronted with sit-uation 4 which decreases recall and increasesambiguity (0.5% on average), in the vast ma-jority of cases ambiguous tags simply increasethe ambiguity without correcting any mistakes.Ambiguous tags behave quite differently withrespect to the four situations described above.In the best cases (tag 6), 46% of the occurrencescorrected an error, and the tag is used one out often times the tagger selects an ambiguous tag,as opposed to tag 19 , which corrected errors in48% of the cases but is not frequently used.
Theworst configuration is tag 9, which, althoughnot chosen very often, corrects an error in 13%of the occurrences and increases the ambiguityin 85% of its occurrences.A more detailed evaluation of the basic tag-ging mistakes has suggested a better adaptedand more subtle method of using the ambiguoustags which may at the same time constitute a di-rection for future work.
While the vast majorityof mistakes are due to mixing up word classes,such as the -ing forms used as adjectives, asnouns or as verbs, about one third of the mis-takes concern only 25 common words such asthat, out, there, on, off, etc.
Using the ambigu-Tag 1 2 3 4 freq1 0.220 0.026 0.746 0.006 0.1265 0.129 0.014 0.852 0.002 0.1656 0.461 0.000 0.538 0.000 0.1079 0.133 0.012 0.850 0.003 0.08219 0.483 0.064 0.419 0.032 0.012AVG 0.241 0.029 0.722 0.005Table 3: Error analysis of some ambiguous tagsous tags for these words alone has yielded a re-call of 0.965 on the test corpus (25% errors lessthan model M0) while keeping the ambiguityrate very low (1.04).
With this procedure, 35%of the ambiguous tags occurrences corrected anerror made byM0 and 59% increased the ambi-guity.
The result can be improved by designingtwo sets of ambiguous tags: one to be used forthis set of words, and one for the word-classesmost often mistaken.4 Conclusions and Future WorkWe have presented a method for computing theprobability distributions associated to ambigu-ous tags, denoting subsets of the tagset, in anHMM based part of speech tagger.
An iterativemethod for discovering ambiguous tags, basedon the mistakes made by the tagger allowed toreach a recall of 0.982 for an ambiguity rate of1.232.
These figures can be compared to thebaseline model which achieves a recall of 0.979and an ambiguity rate of 1.418 using the sameambiguous tags.
An analysis of ambiguous tagsshowed that they do not always behave in theway expected; some of them introduce a lot ofambiguity without correcting many mistakes.This work will be developed in two directions.The first one concerns the study of the differ-ent behaviour of ambiguous tags which couldbe influenced by computing differently the ficti-tious counts of each ambiguous tag, based on itsbehaviour on a development corpus in order toforce or prevent its introduction during tagging.The second direction concerns experiments onsupertagging (Bangalore and Joshi, 1999) fol-lowed by a parsing stage the tagging stage asso-ciates to each word a supertag.
The supertagsare then combined by the parser to yield a parseof the sentence.
Errors of the supertagger (al-most one out of 5 words is attributed the wrongsupertag) often impede the parsing stage.
Theidea is therefore to allow some ambiguity duringthe supertagging stage, leaving to the parser thetask of selecting the right supertag using syntac-tic constraints that are not available to the tag-ger.
Such experiments will constitute one wayof testing the viability of our approach.ReferencesSrinivas Bangalore and Aravind K. Joshi.
1999.Supertagging: An approach to almost pars-ing.
Computational Linguistics, 25(2):237?265.Thorsten Brants.
1995.
Tagset reduction with-out information loss.
In ACL?95, Cambridge,USA.Thorsten Brants.
2000.
Tnt - a statisticalpart-of-speech tagger.
In Sixth Applied Natu-ral Language Processing Conference, Seattle,USA.L.
Breiman, J. H. Friedman, R. A. Olshen, andC.
J.
Stone.
1984.
Classification and Re-gression Trees.
Wadsworth & Brooks, PacificGrove, California.Eugene Charniak, Curtis Hendrickson, Neil Ja-cobson, and Mike Perkowitz.
1993.
Equa-tions for part-of-speech tagging.
In 11th Na-tional Conference on Artificial Intelligence,pages 784?789.Philip Clarkson and Ronald Rosenfeld.
1997.Statistical language modeling using the cmu-cambridge toolkit.
In Eurospeech.Nelson Francis and Henry Kuc?era.
1982.
Fre-quency Analysis of English Usage: Lexiconand Grammar.
Houghton Mifflin, Boston.Mitchell Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of english: Thepenn treebank.
Computational Linguistics,9(2):313?330, june.
Special Issue on UsingLarge Corpora.Alexis Nasr and Alexandra Volanschi.
2004.Couplage d?un e?tiqueteur morpho-syntaxiqueet d?un analyseur partiel repre?sente?s sousla forme d?automates finis ponde?re?s.
InTALN?2004, pages 329?338, Fez, Morocco.H.
Ney, U. Essen, and R. Kneser.
1994.On structuring probabilistic dependencies instochastic language modelling.
ComputerSpeech and Language, 8:1?38.Dan Tufis?, Pe?ter Dienes, Csaba Oravecz, andTama?s Va?radi.
2000.
Principled hiddentagset design for tiered tagging of hungarian.In LREC, Athens, Greece.
