Reading Comprehension Programs in aStatistical-Language-Processing Class*Eugene Charniak, Yasemin Altun, Rodrigo de Salvo Braz,Benjamin Garrett, Margaret Kosmala, Tomer Moscovich, Lixin Pang,Changhee Pyo, Ye Sun, Wei Wy, Zhongfa Yang, Shawn Zeller, and Lisa ZornBrown UniversityAbstractWe present-some n w results for the readingcomprehension task described in \[3\] that im-prove on the best published results - from36% in \[3\] to 41% (the best of the systemsdescribed herein).
We discuss a variety oftechniques that tend to give small improve-ments, ranging from the fairly simple (giveverbs more weight in answer selection) tothe fairly complex (use specific techniquesfor answering specific kinds of questions).1 IntroductionCS241, the graduate course in statistical lan-guage processing at Brown University, hadas its class project the creation of programsto answer reading-comprehension tests.
Inparticular, we used the Remedia TM readingcomprehension test data as annotated by agroup at MITRE Corporation, henceforthcalled the Deep Read group \[3\].
The class di-vided itself into four groups with sizes rang-ing from two to four students.
In the firsthalf of the semester the goal was to repro-duce the results of Deep Read and of oneaother.
After this learning and debuggingperiod the groups were encouraged to thinkof and implement new ideas.The Deep Read group provided us withan on-line version of the Remedia materialalong with several marked up versions of* This research was supported inpart by NSF grantLIS SBR 9720368.
Thanks to Marc Light and thegroup at MITRE Corporation for providing the on-line versions of the reading comprehension materialand the Brown Laboratory for Linguistic Informa-tion Processing (BLLIP) for providing the parsedand pronoun referenced versions.same.
The material encompasses four gradelevels - -  third through sixth.
Each gradelevels consists of thirty stories plus five ques-tions for each story.
Each story has the formof a newspaper article, including a title anddateline.
Following \[3\], we used grades threeand six as our development corpus and fourand five for testing.The questions on each story are typicallyone each of the "who, what, where, why, andwhen" varieties.
The Deep Read group an-swered these questions by finding the sen-tence in the story that best answers thequestion.
One of the marked up versionsthey provide indicates those sentences Titlesand datelines are also considered possible an-swers to the questions.
In about 10% of thecases Deep Read judged no sentence stand-ing on its own to be a good answer.
In thesecases no answer to the question is consideredcorrect.
In a few cases more than one answeris acceptable and all of them are so marked.Deep Read also provided a version withperson/place/time arkings inserted auto-matically by the Alembic named-entity s s-tem \[4\].
Henceforth we refer to this as NE(named entity) material.
As discussed be-low, these markings are quite useful.
In addi-tion to the mark-ups provided by Deep Read,the groups were also g~ven a machine anno-tated version with full parse trees and pro-noun coreference.The Deep Read group suggests everaldifferent metrics for judging the perfor-mance of reading-comprehension-question-answering programs.
However, their datashow that the performance of theii: programsgoes up and down on all of the metrics inMethods  Resu l ts1 Best of Deep Read 362 BOW Stem Coref Class 373 BOV Stem NE Coref Tfidf Subj Why MainV 384 BOV Stem NE Defaults Coref 385 BOV Stem NE Defaults Qspecific 41BOWBOVCorefClassDefaultsMainVNEQspecificSubjTfidfWhybag-of-wordsbag-of-verbspronoun coreferenceWord-Net class membershipDefaults from Figure 3Extra credit for main verb matchnamed entitySpecific techniques for all question typesPrefer sentences with same subjectterm frequency times inverse document frequencySpecific good words for "why" questionsFigure 1: Some notable resultstandem.
We implemented several of thosemetrics ourselves, but to keep things sim-ple we only report results on one of them -how often (in percent) the program answersa question by choosing a correct sentence (asjudged in the answer mark-ups).
Following\[3\] we refer to this as the "humsent" (hu-man annotated sentence) metric.
Note thatif more than one sentence is marked as ac-ceptable, a program response of any of thosesentences i considered correct.
If no sen-tence is marked, the program cannot get theanswer correct, so there is an upper bound ofapproximately 90% accuracy for this metric.The results were both en- and dis-couraging.
On the encouraging side, threeof the four groups were able to improve, atleast somewhat, on the previous best results.On the other hand, the extra annotationwe provided (machine-generated parses of allthe sentences \[1\] and machine-generated pro-noun coreference information \[2\]) proved oflimited utility.2 Resu l tsFigure 1 shows four of the results that bet-tered those of Deep Read.
In the next sec-tion we discuss the techniques used in theseprograms.The performance of all the programs var-ied widely depending on the type of ques-tion to be answered.
In particular, "why"questions proved the most difficult.
(DeepRead observed the same phenomenon.)
InFigure 2 we break down the results for sys-tem 3 in Figure 1 according to question type.This system was able to answer only 22%of the "why" questions correctly.
Program5, which had the most complicated schemefor handling "why" questions, answered 26%correctly.3 D iscuss ionAs noted above, the early phase of theproject was concerned with replicating theDeep Read results.
This we were able todo, although generally only to about 1.5 sig-nificant digits.
It seems that one can getswings of several percentage points in per-formance just depending on, say, how one2Question Type Percent CorrectWhen 32Where 50Who 57What 32Why 22Figure 2: Results by question typeresolves ties in the bag-of-words cores, orwhether one.
considers capitalized and un-capitalized words the same.
However, thenumbers our groups got were in the sameballpark and, more importantly, the trendswe found in the numbers were the same.
Forexample, stemming helped a little, stop-listsactually hurt a very small amount, and theuse of named-entity data gave the biggestsingle improvement of the various Deep Readtechniques.We found two variations on bag-of-wordsthat improved results both individually andwhen combined.
The first of these is the"bag of verbs" (BOV) technique.
In thisscheme one first measures imilarity by do-ing bag-of-words, but looking only at verbs(obtained from the machine-generated parsetrees we provided).
If two sentences tiedon BOV, then bag-of-words is used as a tie-breaker.
As the usefulness of this techniquewas shown early in the project, all of thegroups tried it.
It seems to provide two orthree percentage-point mprovement in a va-riety of circumstances.
Most of our best re-sults were obtained when using this tech-nique.
A further refinement of this tech-nique is to weight matching main verbs morehighly.
This is used in system 3.One group explored the idea of replacingbag-of-words with a scheme based upon thestandard ocument-retrieval "tfidf" method.Document retrieval has long used a bag-of-words technique, in which the words aregiven different weights.
So if our query haswords wl...wn, the frequency of the word i indocument in question is fi, and the numberof documents that have word i is n, then thescore for this document isL ~i (1)i=l  n iThat is, we take the term frequency (tf = fi)times the inverse document frequency (idf =1/ni) and sum over the words in the query.Of course, our application is sentence re-trieval, not document retrieval, so we defineterm frequency as the number of times theword appears in the candidate sentence, anddocument frequency as the number of sen-tences in which this word appears.
(If weuse stemming, then this applies to stemmedwords.)
Replacing BOW (OR BOV) bytfidf gives a three to six percentage-pointimprovement, depending on the other tech-niques with which it is combined.
This issomewhat surprising because, as stated ear-lier, stop-lists were observed both by DeepRead and ourselves to have a slight negativeimpact on performance.
One might thinkthat the tfidf scheme should have somethinglike the same impact, as the words on thestop-list are exactly those that occur in manysentences on average, and thus ones whoesimpact will be attenuated in tfidL That tfidfis nevertheless uccessful suggests (perhaps)that the words on the stop-lists are usefulfor settling ties, a situation where even theattenuated value provided in tfidf will workjust fine.
It may also be the case that itis useful to distinguish between those wordsthat are more common and those that areless common, even though neither appear onthe stop-list.The best results, however, were obtainedby creating question-answering strategies forspecific question types (who, what, where,why, when).
For example, one simple strat-egy assigns a default answer to each ques-tion type (in case all of the other strategiesproduce a tie) and zero or more sentence lo-cations that should be eliminated from con-sideration (before any of the other strategiesare used).
The particulars of this "Defaults"strategy are shown in Figure 3.There were more complicated question-type strategies as well.
As already noted,3Question Type Default EliminateWho title datelineWhat 1st story line (none)When dateline (none)Where dateline titleWhy 1st story line title,datelineFigure 3: Default and eliminable sentencesin the "Default" strategy"why" questions are the most difficult forbag-of-words.
The reason is fairly intuitive.
"Why" questions are of the form "Why didsuch-and-such happen?"
Bag-of-words typ-ically finds a sentence of the form "Suchand such happened."
The following strategymakes use of the fact that the answer to the"why" question is often either the sentencepreceding or following the sentence that de-scribes the event.If the first NP (noun-phrase) in the sen-tence following the match is a pronoun,choose that sentence:Q: Why did Chris write two booksof his own?match: He has written two booksof his own.A: They tell what it is like to befamous.If that rule does not apply, then if the firstword of the matching sentence is "this","that," "these" or "those", select the pre-vious sentence:Q: Why did Wang once get upset?A: When she was a little girl, herart teacher didn't like her paint-ings.match: This upset Wang.Finally, if neither of the above two rules ap-plies, look for sentences that have the follow-ing words and phrases (and morphologicalvariants) which tend to answer why ques-tions: "show", "explain", "because", "noone knows", and "if so".
If there is morethan one such sentence, use bag-of-~words todecide between them:4Q: Why does Greenland havestrange seasons?A: Because it is far north, it hasfour months of sunlight each year.A lot of the question-type-specific rulesuse the parse of the sentence to select keywords that are more important matches thanother words of the sentence.
For example,"where" questions tended to come in two va-rieties: "Where AUX NP VP" (e.g., "Wheredid Fred find the dog?")
and "Where AUXNP."
(e.g:, "Where is the dog?").
In bothcases the words of the NP are important omatch, and in the first case the (stemmed)main verb of the VP is important.
Also, sen-tences that have PPs (prepositional phrases)with a preposition that often indicates loca-tion (e.g., "in," "near," etc.)
are given aboost by the weighting scheme.4 ConclusionWe have briefly discussed several readingcomprehension systems that are able to im-prove on the results of \[3\].
While these arepositive results, many of the lessons learnedin this exercise are more negative.
In par-ticular, while the NE data clearly helpeda few percent, most of the extra syntacticand semantic annotations (i.e., parsing andcoreference) were either of very small utility,or their utility came about in idiosyncraticways.
For example, probably the biggest im-pact of the parsing data was that it allowedpeople to experiment with the bag-of-verbstechnique.
Also, the parse trees served as thelanguage for describing very question spe-cific techniques, uch as the ones for "where"questions presented in the previous section.Thus our tentative conclusion is that weare still not at a point that a task like chil-dren's reading comprehension tests is a goodtesting ground for NLP techniques.
To theextent that these standard techniques areuseful, it seems to be only in conjunctionwith other methods that are more directlyaimed at the task.Of course, this is not to say that some-one else will not come up with better syntac-tic/semantic annotations that more directlylead to improvements on such tests.
We canonly say that so far we have not been ableto do so.References1.
CHARNIAK, E. A maximum-entropy-inspired parser.
In Proceedings of the2000 Conference of the North AmericanChapter of the Assocation for Computa-tional Linguistics.
ACL, New BrunswickN J, 2000.2.
GE, N.,-HALE, J.
AND CHARNIAK, E.A statistical approach to anaphora reso-lution.
In Proceedings of the Sixth Work-shop on Very Large Corpora.
1998, 161-171.3.
HIRSCHMAN, L., LIGHT, M., BRECK, E.AND BURGER, J. D. Deep read: a read-ing comprehension system.
In Proceedingsof the ACL 1999.
ACL, New Brunswick,N J, 1999, 325-332.4.
VILAIN, M. AND DAY, D. Finite-stateparsing by rule sequences.
In Interna-tional Conferences on ComputationM Lin-guistics (COLING-96).
The InternationalConmmittee on Computational Linguis-tics, 1996.5
