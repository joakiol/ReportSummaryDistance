ANNOTATION OF ATIS DATAKate Hunicke-Smith, Project LeaderJared Bernstein, Principal InvestigatorSRI InternationalMenlo Park, California 94025PROJECT GOALSThe performance of spoken language systems on utterancesfrom the ATI$ domain is evaluated by comparing system-produced responses with hand-crafted (and -verified) stan-dard responses to the same utterances.
The objective ofSRI's annotation project is to provide SLS system develop-ers with the range of correct responses to human utterancesproduced uring experimental sessions with ATIS domaininteractive systems.
These correct responses are then usedin system training and evaluation.RECENT RESULTSSince June 1991, SRI has produced classification andresponse files for about 9,000 utterances of training data(2900 of these since February, 1992).
A dry run systemevaluation and two official evaluations have been heldsince the project began in 1991.
SRI has produced the stan-dard responses for all of these evaluations; in all, about2300 utterances.These tests were performed according to the CommonAnswer Specification (CAS) protocol which is used intraining.
All systems are evaluated on a common set ofdata, with system responses measured against official refer-ence answers produced at SRI in the same m,anner as thetraining da~.In addition to producing the classification and standardresponse files, SRI takes an active role in the adjudicationof test and training data bug reports, initiates nearly all ofthe changes to the Principles oflnterpretation document (abasic set of principles for interpreting the meaning of ATISsentences agreed upon by the DARPA community), andcontinues to support NIST by modifying software and act-ing as a consultant regarding the annotation of data.In 1992 the DARPA community developed a complemen-tary evaluation method, referred to alternately as "end-to-end" or "logfile" evaluation, to better evaluate system-nserinterfaces.
Using an interactive program developed byDavid Goodine at MIT, human evaluators from SRI andNIST used this end-to-end method in a dry run evaluationin December, 1992.
For each query/response pair in 128interactions, the human evaluators judged the correctnessor appropriateness of system responses, and classified thetype of user request and type of system response.The use of human evaluators allowed more flexibility inscoring than an automatic, omparator-based method.
Thismethod allowed partial correctness judgements and theopportunity o score system responses which were not data-base retrievals, uch as diagnostic messages and directivesto the user.
Because human evaluators saw the interactionfrom the point of view of the iJser, the results of the logfileevaluation method aided system developers by identifyingdialogue and user interface problems which werenot indi-cated by the usual CAS evaluation method.PLANS FOR THE COMING YEARIn the next year, SRI will continue to provide MADCOWannotation and other services to the DARPA community.412
