NJFun: A Reinforcement Learning Spoken Dialogue SystemDiane  L i tman,  Sat inder  S ingh ,  M ichae l  Kearns  and  Mar i l yn  WalkerAT&T Labs - -  Research180 Park  AvenueF lo rham Park,  NJ  07932 USA{diane,bavej  a,mkearns,walker} @research.att .comAbst rac tThis paper describes NJFun, a real-time spoken dia-logue systemthat-provides users with informationabout things to d~ in New Jersey.
NJFun auto-matically optimizes its dialogue strategy over time,by using a methodology for applying reinforcementlearning to a working dialogue system with humanusers .1 In t roduct ionUsing the formalism of Markov decision processes(MDPs) and the algorithms of reinforcement learn-ing (RL) has become a standard approach to manyAI problems that involve an agent learning tooptimize reward by interaction with its environ-ment (Sutton and Barto, 1998).
We have adaptedthe methods of RL to the problem of automaticallylearning a good dialogue strategy in a fielded spokendialogue system.
Here is a summary of our proposedmethodology for developing and evaluating spokendialogue systems using R.L:?
Choose an appropriate reward measure for di-alogues, and an appropriate representation fordialogue states.?
Build an initial state-based training system thatcreates an exploratory data set.
Despite beingexploratory, this system should provide the de-sired basic functionality.?
Use these training dialogues to build an empir-ical MDP model on the state space.?
Compute the optimal dialogue policy accordingto this MDF, using RL.?
Reimplement the system using the learned dia-logue policy.In this demonstration session paper, we briefly de-scribe our system, present some sample dialogues,and summarize our main contributions and limita-tions.
Full details of our work (e.g.
our reinforce-ment learning methodology, analysis establishing theveracity of the MDP we learn, a description of anexperimental evaluation of NJFun, analysis of ourlearned ialogue strategy) can be found in two forth-coming technical papers (Singh et al, 2000; Litmanet al, 2000).2 The  N JFun  SystemNJFun is a reM-time spoken dialogue system thatprovides users with information about things to doin New Jersey.
1 An example dialogue with NJFun isshown in Figure 1.
NJFun is built using an internalplatform for spoken dialogue systems.
NJFun usesa speech recognizer with stochastic language modelstrained from example user utterances, and a TTSsystem based on concatenative diphone synthesis.Its database is populated from the nj.
onl ine web-page to contain information about activities.
NJFunindexes this database using three attributes: activitytype, location, and time of day.Informally, the NJFun dialogue manager sequen-tially queries the user regarding the activity, locationand time attributes, respectively.
NJFun first asksthe user for the current attribute (and possibly theother attributes, depending on the initiative).
If thecurrent attribute's value is not obtained, NJFun asksfor the attribute (and possibly the later attributes)again.
If NJFun still does not obtain a value, N J-Fun moves on to the next attribute(s).
WheneverNJFun successfully obtains a value, it can confirmthe value, or move on and attempt o obtain thenext attribute(s)?
When NJFun has finished askingabout the attributes, it queries the database (usinga wildcard for each unobtained attribute value).We use reinforcement learning (RL) to optimizedialogue strategy, lq.L requires that all potential ac-tions for each state be specified.
Note that at somestates it is easy for a human to make the correctaction choice.
We made obvious dialogue strategychoices in advance, and used learning only to opti-mize the difficult choices.
In NJFun, we restrictedthe action choices to 1) the type of initiative to use1This system description section is taken from (Litman etal., 2000) .2Note that it is possible for users to specify multiple at-tributes, in any order, in a single utterance.
However, NJFunwill always process multiple attributes using its predefinedsequential ordering.17SI: Welcome to N JFun .
How may i help you?
._UI :  I 'd like to find um winetas t ing  in Lambertv i l le  in the morn ing .
(ASR output: I 'd like to f ind out wineries the in the Lambertville in the morning.
)$2: Did you say you are interested in Lambertv i l le?U2: Yes.$3: Did you say you want  to go in the morn ing?U3: Yes.$4: I found a winery near  Lambertv i l le  that  is open in the morn ing .
It is \ [ .
.
.
\]P lease give me feedback by say ing 'good ' ,  'so-so',  or  'bad ' .U4: Good.Figure 1: An example dialogue with NJFun (after optimization via RL).Act ionGreetSGreetUReAsk 1 SReAsk  1 MAsk2SAsk2UReAsk2SReAsk2~mWelcome to N JFun.
P lease say an act iv i ty  name or say 'l ist act iv i t ies '  for a list of act iv i t ies I know about .Welcome to N JFun.
How may I help you?I know about  amusement  parks ,  aquar iums,  cruises, histor ic sites, museums,  parks ,  theaters ,  wineries,and  zoos.
P lease say an act iv i ty  name f rom this  list.P lease tell me the act iv i ty  type.You can  also tell me the locat ion  and time.P lease say the name of the town or city that  you are interested in.P lease give me more in format ion.P lease tell me the name of the town or c i ty  that  you are interested in.
"P lease  tell me the locat ion that  you are interested in.
You can  also tell me the t ime.Figure 2: Sample initiative strategy choices.when asking or reasking for an attribute, and 2)whether to confirm an attribute value once obtained.The optimal actions may vary with dialogue state,and are subject o active debate in the literature.The examples in Figure 2 shows that NJFun canask the user about the first 2 attributes 3 using threetypes of initiative, based on the combination of thewording of the system prompt (open versus direc-tive), and the type of grammar NJFun uses duringASR (restrictive versus non-restrictive).
If NJFunuses an open question with an unrestricted gram-mar, it is using user initiative (e.g., GreetU).
If N J-Fun instead uses a directive prompt with a restrictedgrammar, the system is using system initiative (e.g.,GreetS).
If NJFun uses a directive question with anon-restrictive grammar, it is using mixed initiative,because it is giving the user an opportunity to takethe initiative by supplying extra information (e.g.,ReAsklM).NJFun can also vary the strategy used to confirmeach attribute.
If NJFun asks the user to explicitlyverify an attribute, it is using explicit confirmation(e.g., ExpConf2 for the location, exemplified by $2in Figure 1).
If NJFun does not generate any con-firmation prompt, it is using no confirmation (anaction we call NoConf).Solely for the purposes of controlling its operation(as opposed to the learning, which we discuss in amoment), NJFun internally maintains an operationsvector of 14 variables.
2variables track whether thesystem has greeted the user, and which attributethe system is currently attempting to obtain.
Foreach of the 3 attributes, 4 variables track whether'~ "Greet"  is equ iva lent  to  ask ing  for the f i rs t  a t t r ibute .
N J -Fun  a lways  uses  sys tem in i t ia t ive  fo r  the  th i rd  a t t r ibute ,  be -cause  a t  that  po in t  the  user  can  on ly  prov ide  the time of day .the system has obtained the attribute's value, thesystem's confidence in the value (if obtained), thenumber of times the system has asked the user aboutthe attribute, and the type of ASR grammar mostrecently used to ask for the attribute.The formal state space S maintained by NJFunfor the purposes of learning is much simpler thanthe operations vector, due to data sparsity concerns.The dialogue state space $ contains only 7 variables,which are summarized in Figure 3, and is easily com-puted from the operations vector.
The "greet" vari-able tracks whether the system has greeted the useror not (no=0, yes=l).
"Attr" specifies which at-tribute NJFun is currently attempting to obtain orverify (activity=l, location=2, time=3, done withattributes=4).
"Conf" represents the confidencethat NJFun has after obtaining a value for an at-tribute.
The values 0, 1, and 2 represent low,medium and high ASR confidence.
The values 3and 4 are set when ASR hears "yes" or "no" after aconfirmation question.
"Val" tracks whether NJFunhas obtained avalue for the attribute (no=0, yes=l).
"Times" tracks the number of times that NJFun hasasked the user about the attribute.
"Gram" tracksthe type of grammar most recently used to obtainthe attribute (0=non-restrictive, l=restrictive).
Fi-nally, "history" represents whether NJFun had trou-ble understanding the user in the earlier part of theconversation (bad=0, good=l).
We omit the fulldefinition, but as an example, when NJFun is work-ing on the second attribute (location), the historyvariable is set to 0 if NJFun does not have an ac-tivity, has an activity but has no confidence in thevalue, or needed two queries to obtain the activity.In order to apply RL with a limited amount oftraining data, we need to design a small state space18I greet  a t t r  conf  val t imes gram history \[0,1 1,2,3,4 0,1,2,3,4 0,1 0,1,2 0,1 0,1 IFigure 3: State features and values.that makes enough critical distinctions to supportlearning.
The use of S yields a state space of size62.
The state space that we utilize here, althoughminimal, allows us to make initiative decisions basedon the success of earlier exchanges, and confirmationdecisions based on ASR confidence scores and gram-mars.In order to learn a good dialogue strategy via RLwe have to explore the state action space.
Thestate/action mapping representing NJFun's initialexploratory dialog@ strategy EIC (Exploratory forInitiative and Confirmation) is given in Figure 4.Only the exploratory portion of the strategy isshown, namely all those states for which NJFun hasan action choice.
For each such state, we list thetwo choices of actions available.
(The action choicesin boldface are the ones eventually identified as op-timal by the learning process.)
The EIC strategychooses randomly between these two actions when inthe indicated state, in order to maximize xplorationand minimize data sparseness when constructing ourmodel.
Since there are 42 states with 2 choiceseach, there is a search space of 242 potential dia-logue strategies; the goal of the RL is to identify anapparently optimal strategy from this large searchspace.
Note that due to the randomization of theEIC strategy, the prompts are designed to ensurethe coherence of all possible action sequences.Figure 5 illustrates how the dialogue strategy inFigure 4 generates the dialogue in Figure 1.
Eachrow indicates the state that NJFun is in, the ac-tion executed in this state, the corresponding turnin Figure 1, and the reward received.
The initialstate represents hat NJFun will first attempt to ob-tain attribute 1.
NJFun executes GreetU (althoughas shown in Figure 4, Greets is also possible), gen-erating the first utterance in Figure 1.
After theuser's response, the next state represents that N J-Fun has now greeted the user and obtained the ac-tivity value with high confidence, by using a non-restrictive grammar.
NJFun chooses not to confirmthe activity, which causes the state to change but noprompt o be generated.
The third state representsthat NJFun is now working on the second attribute(location), that it already has this value with highconfidence (location was obtained with activity af-ter the user's first utterance), and that the dialoguehistory is good.
This time NJFun chooses to confirmthe attribute with the second NJFun utterance, andthe state changes again.
The processing of time issimilar to that of location, which leads NJFun to thefinal state, where it performs the action "Tell" (cor-StateC V t g0 1 0 0 0 0 01 1 0 0 1 0 01 1 0 1 0 0 01 1 0 1 0 1 01 1 1 1 0 0 01 1 1 1 0 1 01 1 2 I 0 0 01 1 2 1 0 1 01 1 4 0 0 0 01 1 4 0 1 01 2 0 0 0 01 2 0 0 0 01 2 0 0 1 01 2 0 0 1 01 2 0 1 0 01 2 0 1 0 01 2 0 1 0 1I 2 0 I 0 I1 2 i 1 0 01 2 1 1 0 01 2 1 1 0 11 2 1 1 0 11 2 2 1 0 01 2 2 1 0 0I 2 2 1 0-  11 2 2 1 0 1I 2 4 0 0 01 2 4 0 0 01 2 4 0 I 01 2 4 0 1 01 3 0 1 0 01 3 0 1 0 01 3 0 1 0 11 3 0 1 0 11 3 1 1 0 01 3 1 1 0 01 3 1 1 0 1i 3 1 I 0 i1 3 2 1 0 01 3 2 1 0 01 3 2 1 0 1i 3 2 1 0 IAct ion  ChoicesGreetS ,GreetUReAsk  1 S ,ReAsk  1 MNoConf ,  ExpConf lNoConf ,  ExpConf lNoConf ,ExpConf lNoConf ,  ExpConf lNoConf ,  ExpConf lNoConf ,  ExpConf lReAsk lS ,ReAsk lM0 ReAsk lS ,ReAsk lM0 - Ask2S,Ask2U1 Ask2S,Ask2U0 ReAsk2S,ReAsk2M1 ReAsk2S,ReAsk2  M0 NoConf ,  ExpConf21 NoConf, ExpConf20 NoConf, ExpConf21 NoConf ,  ExpConf20 NoConf ,  ExpConf21 NoConf ,ExpConf20 NoConf ,  ExpConf21 NoConf ,  ExpConf20 NoConf ,  ExpConf21 NoConf ,  ExpConf20 NoConf ,  ExpConf21 NoConf, ExpConf20 ReAsk2S,ReAsk2M1 ReAsk2S,ReAsk2M0 ReAsk2S,R .eAsk2M1 ReAsk2S,ReAskSM0 NoConf ,  ExpConf31 NoConf ,  ExpConf30 NoConf, ExpConf3I NoConf, ExpConf30 NoConf, ExpConf31 NoConf ,  ExpConf30 NoConf ,  ExpConf31 NoConf ,  ExpConf30 NoConf ,ExpCon: f31 NoConf ,ExpConf30 NoConf ,  ExpConf3I NoConf, ExpConf3Figure 4: Exploratory portion of EIC strategy.State  Act ion Turn Rewardgaevtgh0100000 GreetU S1 0I 121000 NoConf  01 2 2 1 0 0 1 ExpConf2  $2 01 3 2 1 0 0 1 ExpConf3  $3 01 4 0 0 0 0 0 Tell S4 1Figure 5: Generating the dialogue in Figure 1.responding to querying the database, presenting theresults to the user, and asking the user to provide areward).
Note that in NJFun, the reward is always0 except at the terminal state, as shown in the lastcolumn of Figure 5.3 Sample DialoguesFigures 1 and 6 present two dialogues that were gen-erated using different dialogue strategies in the EICsearch space.
For example, although NJFun beginsboth dialogues in the first state in Figure 4, NJFunexecutes GreetU in one dialogue and GreetS in theother.
Similarly, although the next state of NJFun19SI: Welcome to N JFun.
P lease say an act iv i ty  name or say 'list act iv i t ies '  for  a list of act iv i t ies I know about .U I :  I would like to visit a h istor ic  site in S tanhope this  morn ing.
(ASR output: I would like to the zoo historic sites in stanhope historic)$2: Did you say you are interested in going to a zoo?U2: No.$3: I know about  amusement  parks ,  aquar iums,  cruises, histor ic sites, museums,  parks,  theaters ,  wineries, and  zoos.P lease say an act iv i ty  name f rom this list.U3: I would like to visit a h istor ic  site.
(ASR output: I would like to visit historic sites)$4: Did you say you are interested in going to a h istor ic  site?U4: Yes.$5: P lease give me more in format ion .U5: (user says nothing)S6: P lease tell me the locat ion that  you are interested in.
You can also tell me the time.U6: Visit a histor ic .
(ASR output: What  historic)$7: What  t ime of  the day  do you want  to go?U7: S tanhope.
(no ASR output)$8: Do you want  to go in the morn ing ,  in the a f ternoon,  or  in the evening?US: Morn ing.S9: Did you say you want  to go in the morn ing?U9: Yes.S l0 :  I found 11 histor ic  sites in New Jersey that  are open in the morn ing .
The  first 3 \ [ .
.
.
\] Would you like to  hear  more?UiO: No.$11: Thank~ou for using the system.
P lease give me feedback by say ing 'good ' ,  'so-so',  or  'bad ' .U I I :  Bad'.
_.Figure 6: An example training dialogue with NJFun.is the same in both dialogues ("1 1 2 1 0 0 0"), theactivity is not confirmed in the first dialogue.4 ContributionsThe main contribution of this work is that wehave developed and empirically validated a practi-cal methodology for using RL to build a real dia-logue system that optimizes its behavior from dia-logue data.
Unlike traditional approaches to learn-ing dialogue strategy from data, which are limitedto searching a handful of policies, our RL approachis able to search many tens of thousands of dialoguestrategies.
In particular, the traditional approachis to pick a handful of strategies that experts in-tuitively feel are good, implement each policy as aseparate system, collect data from representative hu-man users for each system, and then use standardstatistical tests on that data to pick the best sys-tem, e.g.
(Danieli and Gerbino, 1995).
In contrast,our use of RL allowed us to explore 242 strategiesthat were left in our search space after we excludedstrategies that were clearly suboptimal.An empirical validation of our approach is de-tailed in two forthcoming technical papers (Singhet al, 2000; Litman et al, 2000).
We obtained 311dialogues with the exploratory (i.e., training) ver-sion of NJFun, constructed an MDP from this train-ing data, used RL to compute the optimal dialoguestrategy in this MDP, reimplemented NJFun suchthat it used this learned dialogue strategy, and ob-tained 124 more dialogues.
Our main result wasthat task completion improved from 52% to 64%from training to test data.
Furthermore, analysisof our MDP showed that the learned strategy wasnot only better than EIC, but also better than otherfixed choices proposed in the literature (Singh et al,2000).5 LimitationsThe main limitation of this effort to automate thedesign of a good dialogue strategy is that our currentframework has nothing to say about how to choosethe reward measure, or how to best represent dia-logue state.
In NJFun we carefully but manually de-signed the state space of the dialogue.
In the future,we hope to develop a learning methodology to auto-mate the choice of state space for dialogue systems.With respect o the reward function, our empiricalevaluation investigated the impact of using a numberof reward measures (e.g., user feedback such as U4 inFigure 1, task completion rate, ASR accuracy), andfound that some rewards worked better than others.We would like to better understand these differencesamong the reward measures, investigate the use ofa learned reward function, and explore the use ofnon-terminal rewards.Re ferencesM.
Danieli and E. Gerbino.
1995.
Metrics for eval-uating dialogue strategies in a spoken languagesystem.
In Proceedings of the 1995 AAA1 SpringSymposium on Empirical Methods in DiscourseInterpretation and Generation, pages 34-39.D.
Litman, M. Kearns, S. Singh, and M. Walker.2000.
Automatic optimization of dialogue man-agement.
Manuscript submitted for publication.S.
Singh, M. Kearns, D. Litman, and M. Walker.2000.
Empirical evaluation of a reinforcementlearning spoken dialogue system.
In Proceedingsof AAAI 2000.R.
S. Sutton and A. G. Barto.
1998.
ReinforcementLearning: An Introduction.
MIT Press.20
