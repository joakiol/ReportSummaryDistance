The GREC Challenges 2010:Overview and Evaluation ResultsAnja Belz Eric KowNatural Language Technology GroupSchool of Computing, Mathematical and Information SciencesUniversity of BrightonBrighton BN2 4GJ, UK{asb,eykk10}@bton.ac.ukAbstractThere were three GREC Tasks at Gen-eration Challenges 2010: GREC-NER re-quired participating systems to identifyall people references in texts; for GREC-NEG, systems selected coreference chainsfor all people entities in texts; and GREC-Full combined the NER and NEG tasks, i.e.systems identified and, if appropriate, re-placed references to people in texts.
Fiveteams submitted 10 systems in total, andwe additionally created baseline systemsfor each task.
Systems were evaluated au-tomatically using a range of intrinsic met-rics.
In addition, systems were assessed byhuman judges using preference strengthjudgements.
This report presents the eval-uation results, along with descriptions ofthe three GREC tasks, the evaluation meth-ods, and the participating systems.1 IntroductionUntil recently, referring expression generation(REG) research focused on the task of selecting thesemantic content of one-off mentions of listener-familiar discourse entities.
In the GREC researchprogramme we have been interested in REG as (i)grounded within discourse context, (ii) embeddedwithin an application context, and (iii) informedby naturally occurring data.In general terms, the GREC tasks are about howto select appropriate references to an entity in thecontext of a piece of discourse longer than a sen-tence.
In GREC?10, there were three subtasks:identification of references to people in free text(GREC-NER); selection of references to people intext (GREC-NEG); and regeneration of referencesto people in text (GREC-Full) which can be thoughtof as combining the NER and NEG tasks.The immediate motivating application contextfor the GREC Tasks is the improvement of referen-tial clarity and coherence in extractive summariesand multiply edited texts (such as Wikipedia ar-ticles) by regenerating referring expressions con-tained in them.
The motivating theoretical inter-est for the GREC Tasks is to discover what kind ofinformation is useful for making choices betweendifferent kinds of referring expressions in context.The GREC?10 tasks used the GREC-People cor-pus which consists of 1,100 Wikipedia texts aboutpeople within which we have annotated all refer-ences to people.Five teams participated in the GREC?10 tasks(see Table 1), submitting 10 systems in total.
Twoof these were created by combining the NER sys-tem of one of the teams with the NEG systemsof two different teams, producing two ?combined?systems for the Full Task.
We also used the corpustexts themselves as ?system?
outputs, and createdbaseline systems for all three tasks.
We evaluatedsystems using a range of intrinsic automaticallycomputed and human-assessed evaluation meth-ods.
This report describes the data (Section 2)and evaluation methods (Section 3) used in thethree GREC?10 tasks, and then presents task defi-nition, participating systems, evaluation methods,and evaluation results for each of the three tasksseparately (Sections 4?
6).2 GREC?10 DataThe GREC?10 data is derived from the GREC-People corpus which (in its 2010 version) con-sists of 1,100 annotated introduction sections fromWikipedia articles in the category People.
An in-troduction section was defined as the textual con-tent of a Wikipedia article from the title up to (andexcluding) the first section heading, the table ofcontents or the end of the text, which ever comesfirst.
Each text belongs to one of six subcategories:inventors, chefs, early music composers, explor-ers, kickboxers and romantic composers.
For theTeam Affiliation NEG systems NER systems Full systemsUDelx University of Delaware UDel-NEG UDel-NER UDel-FullUMUS Universite?
du Maine UMUS ?
?Universita?t StuttgartJUx Jadavpur University JU ?
?Poly-co E?cole Polytechnique de Montre?al ?
Poly-co ?XRCEy Xerox Research Centre Europe XRCE ?
?UDel/UMUS (see above) ?
?
UDel-UMUS-FullUDel/XRCE (see above) ?
?
UDel-XRCE-FullTable 1: GREC-NEG?09 teams and systems (combined teams in last two rows).
x = resubmitted afterfixing character encoding problems and/or software bugs; y = late submission.All Inventors Chefs Early Explorers Kickboxers RomanticComposers ComposersTraining 809 249 248 312 ?
?
?Development 91 28 28 35 ?
?
?Test (NEG) 100 31 30 39 ?
?
?Test (NER/Full) 100 ?
?
?
33 34 33Total 1,100 307 306 387 33 34 33Table 2: Overview of GREC?10 data sets.purposes of the GREC task, the GREC-People cor-pus was divided into training, development andtest data.
The number of texts in the subsets areas shown in Table 2.In the GREC-People annotation scheme, a dis-tinction is made between reference and referentialexpression.
A reference is ?an instance of refer-ring?
which is unique, whereas a referential ex-pression is a word string and each reference can berealised by many different referential expressions.In the GREC corpora, each time an entity is re-ferred to, there is a single reference, but there maybe one or several referring expressions providedwith it: in the training/development data, there isa single RE for each reference (the one found inthe corpus); in the test set, there are four REs foreach reference (the one from the corpus and threeadditional ones selected by subjects in a manualselection experiment).We first manually annotated people mentions inthe GREC-People texts by marking up the wordstrings that function as referential expressions(REs) and annotating them with coreference in-formation as well as semantic category, syntac-tic category and function, and various supplementsand dependents.
Annotations included nested ref-erences, plurals and coordinated REs, certain un-named references and indefinites.
In terminologyand the treatment of syntax used in the annota-tion scheme we relied heavily on The CambridgeGrammar of the English Language by Huddlestonand Pullum (2002).
For full details of the manualannotation please refer to the GREC?10 documen-tation (Belz, 2010).The manual annotations were then automat-ically checked and converted to XML format.In the XML format of the annotations, the be-ginning and end of a reference is indicated by<REF><REFEX>... </REFEX></REF> tags, andother properties mentioned above (e.g.
syntacticcategory) are encoded as attributes on these tags.For the GREC tasks we decided not to transferthe annotations of integrated dependents and rel-ative clauses to the XML format.
Such dependentsare included within <REFEX>...</REFEX> annota-tions where appropriate, but without being markedup as separate constituents.Figure 1 shows one of the XML-annotated textsfrom the GREC data.
For full details of the manualannotations and the XML version, please refer tothe GREC?10 documentation (Belz, 2010).
Herewe provide a brief summary.The REF element indicates a reference, and iscomposed of one REFEX element (the ?selected?referential expression for the given reference; inthe corpus texts it is the referential expressionfound in the corpus).
The attributes of the REFelement are ENTITY (entity identifier), MENTION(mention identifier), SEMCAT (semantic category),SYNCAT (syntactic category), and SYNFUNC (syntac-tic function).
ENTITY and MENTION together con-stitute a unique identifier for a reference within atext; together with the TEXT ID, they constitute aunique identifier for a reference within the entire<?xml version="1.0" encoding="utf-8"?><!DOCTYPE GREC-ITEM SYSTEM "genchal09-grec.dtd"><GREC-ITEM><TEXT ID="15"><TITLE>Alexander Fleming</TITLE><PARAGRAPH> <REF ENTITY="0" MENTION="1" SEMCAT="person" SYNCAT="np" SYNFUNC="subj"><REFEX ENTITY="0" REG08-TYPE="name" CASE="plain">Sir Alexander Fleming</REFEX></REF> (6 August 1881 - 11 March 1955) was a Scottish biologist and pharmacologist.<REF ENTITY="0" MENTION="2" SEMCAT="person" SYNCAT="np" SYNFUNC="subj"><REFEX ENTITY="0" REG08-TYPE="name" CASE="plain">Fleming</REFEX></REF> published many articles on bacteriology, immunology, and chemotherapy.<REF ENTITY="0" MENTION="3" SEMCAT="person" SYNCAT="np" SYNFUNC="subj-det"><REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="genitive">his</REFEX></REF> best-known achievements are the discovery of the enzyme lysozyme in 1922 and the discoveryof the antibiotic substance penicillin from the fungus Penicillium notatum in 1928, for which<REF ENTITY="0" MENTION="4" SEMCAT="person" SYNCAT="np" SYNFUNC="subj"><REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="nominative">he</REFEX></REF> shared the Nobel Prize in Physiology or Medicine in 1945 with<REF ENTITY="1" MENTION="1" SEMCAT="person" SYNCAT="np" SYNFUNC="obj"><REFEX ENTITY="1" REG08-TYPE="name" CASE="plain">Florey</REFEX></REF> and<REF ENTITY="2" MENTION="1" SEMCAT="person" SYNCAT="np" SYNFUNC="obj"><REFEX ENTITY="2" REG08-TYPE="name" CASE="plain">Chain</REFEX></REF>.</PARAGRAPH></TEXT><ALT-REFEX><REFEX ENTITY="0" REG08-TYPE="empty" CASE="no_case">_</REFEX><REFEX ENTITY="0" REG08-TYPE="name" CASE="genitive">Fleming?s</REFEX><REFEX ENTITY="0" REG08-TYPE="name" CASE="genitive">Sir Alexander Fleming?s</REFEX><REFEX ENTITY="0" REG08-TYPE="name" CASE="plain">Fleming</REFEX><REFEX ENTITY="0" REG08-TYPE="name" CASE="plain">Sir Alexander Fleming</REFEX><REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="accusative">him</REFEX><REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="genitive">his</REFEX><REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="nominative">he</REFEX><REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="nominative">who</REFEX><REFEX ENTITY="1" REG08-TYPE="empty" CASE="no_case">_</REFEX><REFEX ENTITY="1" REG08-TYPE="name" CASE="genitive">Florey?s</REFEX><REFEX ENTITY="1" REG08-TYPE="name" CASE="plain">Florey</REFEX><REFEX ENTITY="1" REG08-TYPE="pronoun" CASE="accusative">him</REFEX><REFEX ENTITY="1" REG08-TYPE="pronoun" CASE="genitive">his</REFEX><REFEX ENTITY="1" REG08-TYPE="pronoun" CASE="nominative">he</REFEX><REFEX ENTITY="1" REG08-TYPE="pronoun" CASE="nominative">who</REFEX><REFEX ENTITY="2" REG08-TYPE="empty" CASE="no_case">_</REFEX><REFEX ENTITY="2" REG08-TYPE="name" CASE="genitive">Chain?s</REFEX><REFEX ENTITY="2" REG08-TYPE="name" CASE="plain">Chain</REFEX><REFEX ENTITY="2" REG08-TYPE="pronoun" CASE="accusative">him</REFEX><REFEX ENTITY="2" REG08-TYPE="pronoun" CASE="genitive">his</REFEX><REFEX ENTITY="2" REG08-TYPE="pronoun" CASE="nominative">he</REFEX><REFEX ENTITY="2" REG08-TYPE="pronoun" CASE="nominative">who</REFEX></ALT-REFEX></GREC-ITEM>Figure 1: Example XML-annotated text from the GREC-NEG?09 data.corpus.A REFEX element indicates a referential expres-sion (a word string that can be used to refer to anentity).
The attributes of the REFEX element areREG08-TYPE (name, common, pronoun, empty), andCASE (nominative, accusative, etc.
).We allow arbitrary-depth embedding of refer-ences.
This means that a REFEX element may haveREF element(s) embedded in it.The second (and last) component of aGREC-ITEM is an ALT-REFEX element whichis a list of REFEX elements.
For the GREC tasks,these were obtained by collecting the set of allREFEXs that are in the text, and adding severaldefaults including pronouns and other cases (e.g.genitive) of REs already in the list.REF elements that are embedded in REFEX ele-ments contained in an ALT-REFEX list have an un-specified MENTION id (the ???
value).
Furthermore,such REF elements have had their enclosed REFEXremoved.The two test data sets exist in two versions:1.
Version a: each text has a single human-selected refer-ring expression for each reference (i.e.
the one found inthe original Wikipedia article).2.
Version b: the same subset of texts as in (a); for this setwe did not use the REs in the corpus, but replaced eachof them with human-selected alternatives obtained inan online experiment as described in (Belz and Varges,2007); this version of the test set therefore containsthree versions of each text where all the REFEXs in agiven version were selected by one ?author?.The training, development and test data for theGREC-NEG task is exactly as described above.The training and development data for the GREC-NER/Full tasks comes in two versions.
The first isidentical to the standard XML-annotated version ofthe GREC-People corpus as described above (Sec-tion 2).
The second is in the test data input format.In this format, texts have no REFEX and REF tags,and no ALT-REFEX element.
A further difference isthat in the test data format, a proportion of REFEXword strings have been replaced with standardisednamed references.
All empty references have beenreplaced in this way, whereas (non-relative) pro-nouns, and previously seen named references thatare not identical to the standardised named refer-ence, are replaced with a likelihood of 0.5.The reason for this replacement is to make bothtasks easier (as we are running them for the firsttime) as well as more realistic (in an extractivesummary, reference chains are unlikely to be asgood as in the Wikipedia texts).3 Evaluation ProceduresTable 3 is an overview of the evaluation mea-sures we applied to the three tasks in GREC?10.Version a of the test sets has a single versionof each text, and the scoring metrics that arebased on counting matches (Word String Ac-curacy counts matching word strings, REG08-Type Recall/Precision count matching REG08-Type attribute values) simply count the number ofmatches a system achieves against that single text.Version b, however, has three versions of each text,so the match-based metrics first calculate the num-ber of matches for each of the three versions andthen use (just) the highest number of matches.3.1 Automatic EvaluationsREG08-Type Precision is defined as the proportionof REFEXs selected by a participating system whichmatch the reference REFEXs.
REG08-Type Recallis defined as the proportion of reference REFEXsfor which a participating system has produced amatch.String Accuracy is defined as the proportion ofword strings selected by a participating systemthat match those in the reference texts.
This wascomputed on complete, ?flattened?
word stringscontained in the outermost REFEX i.e.
embeddedREFEX word strings were not considered sepa-rately.We also computed BLEU-3, NIST, string-editdistance and length-normalised string-edit dis-tance, all on word strings defined as for String Ac-curacy.
BLEU and NIST are designed for multipleoutput versions, and for the string-edit metrics wecomputed the mean of means over the three text-level scores (computed against the three versionsof a text).To measure accuracy in the NER task, we ap-plied three commonly used performance measuresfor coreference resolution: MUC-6 (Vilain et al,1995), CEAF (Luo, 2005), and B-CUBED (Baggaand Baldwin, 1998).3.2 Human-assessed evaluationsWe designed the human-assessed intrinsic evalua-tion as a preference-judgement test where subjectsexpressed their preference, in terms of two crite-ria, for either the original Wikipedia text or theversion of it with system-generated referring ex-pressions in it.
For the GREC-NEG systems, the in-trinsic human evaluation involved system outputsfor 30 randomly selected items from the test set.We used a Repeated Latin Squares design whichensures that each subject sees the same numberof outputs from each system and for each test setitem.
There were three 10 ?
10 squares, and atotal of 600 individual judgements in this evalua-tion (60 per system: 2 criteria ?
3 articles ?
10evaluators).
We recruited 10 native speakers ofEnglish from among students currently complet-ing a linguistics-related degree at Kings CollegeLondon and University College London.For the GREC-Full systems, we used 21 ran-domly selected test set items, a design analogousto that for the GREC-NEG experiment, and 7 eval-uators from the same cohort.
This experiment hadthree 7 ?
7 squares, and 294 individual judge-ments.Following detailed instructions, subjects didtwo practice examples, followed by the texts to beevaluated, in random order.
Subjects carried outthe evaluation over the internet, at a time and placeof their choosing.
They were allowed to interruptand resume the experiment (though discouragedfrom doing so).Figure 2 shows what subjects saw during theevaluation of an individual text pair.
The place(left/right) of the original Wikipedia article wasrandomly determined for each individual evalua-tion of a text pair.
People references are high-lighted in yellow/orange, those that are identicalin both texts are yellow, those that are different areorange (in the GREC-Full version, there were onlyyellow highlights).
The evaluator?s task is to ex-press their preference in terms of each quality cri-terion by moving the slider pointers.
Moving theslider to the left means expressing a preference forQuality criterion: Type of evaluation: Task: Evaluation Method(s):Humanlikeness Intrinsic/automatic NEG 1.
REG?08-Type Recall and Precision2.
String Accuracy3.
String-edit distanceNEG, Full 1.
BLEU2.
NIST version of BLEUNER CEAF, MUC-6, B-CUBEDFluency Intrinsic/human NEG, Full Human preference-strength judgementsReferential Clarity Intrinsic/human NEG, Full Human preference-strength judgementsTable 3: Overview of GREC?10 evaluation procedures.Figure 2: Example of text pair presented in human intrinsic evaluation of GREC-NEG systems.the text on the left, moving it to the right meanspreferring the text on the right; the further to theleft/right the slider is moved, the stronger the pref-erence.
The two criteria were explained in the in-troduction as follows (the wording of the first isfrom DUC):1.
Referential Clarity: It should be easy to identify whothe referring expressions are referring to.
If a personis mentioned, it should be clear what their role in thestory is.
So, a reference would be unclear if a personis referenced, but their identity or relation to the storyremains unclear.2.
Fluency: A referring expression should ?read well?,i.e.
it should be written in good, clear English, and theuse of titles and names should seem natural.
Note thatthe Fluency criterion is independent of the ReferentialClarity criterion: a reference can be perfectly clear, yetnot be fluent.It was not evident to the evaluators that slid-ers were associated with numerical values.
Sliderpointers started out in the middle of the scale (nopreference).
The values associated with the pointson the slider ranged from -10.0 to +10.0.4 GREC-NEG4.1 TaskThe GREC-NEG test data inputs are identical tothe training/development data (Figure 1), exceptthat REF elements in the test data do not contain aREFEX element, i.e.
they are ?empty?.
The task forparticipating systems is to select one REFEX fromthe ALT-REFEX list for each REF in each TEXT inthe test sets.
If the selected REFEX contains an em-bedded REF then participating systems also needto select a REFEX for this embedded REF and to setthe value of its MENTION attribute.
The same ap-plies to all further embedded REFEXs, at any depthof embedding.4.2 SystemsNEG-Base-rand, NEG-Base-freq, NEG-Base-1st, NEG-Base-name: We created four baselinesystems each with a different way of selecting aREFEX from those REFEXs in the ALT-REFEX listthat have matching entity IDs.
Base-rand selects aREFEX at random.
Base-1st selects the first REFEX(unless the first is the empty reference in whichcase it selects the second).1 Base-freq selects thefirst REFEX with a REG08-TYPE and CASE combi-nation that is the overall most frequent (as deter-mined from the training/development data) giventhe SYNCAT, SYNFUNC and SEMCAT of the refer-ence.1 Base-name selects the shortest REFEX withattribute REG08-TYPE=name.UMUS: The UMUS system maps REFEXs to classlabels encoding REG08-TYPE, CASE, pronoun type,reflexiveness and recursiveness.
References arerepresented by a set of features encoding the at-tributes given in the corpus, information about in-tervening references to other entities, precedingpunctuation, sentence and paragraph boundaries,surrounding word and POS n-grams, etc.
A Condi-tional Random Fields method is then used to mapfeatures to class labels.
The problem is construedas predicting a sequence of class labels for eachentity, to avoid repetition.
If there is more thanone REFEX available with the predicted label thenthe longest one is chosen the first time, and selec-tion iterates through the list subsequently.UDel: The UDel system is a set of decision-treeclassifiers (separate ones for the main subject andother person entities) using psycholinguisticallyinspired features that predict the REG08-TYPE andCASE of the REFEX to select.
Then the system ap-plies rules governing the length of first and subse-quent mentions.
There are back-off rules for whenthe predicted type/case is not available.
An ambi-guity checker avoids the use of a pronoun if therehas been an intervening reference to a person ofthe same gender.JU: The JU baseline system is similar to ourNEG-Base-freq system described above.
The sub-1Note that this is a change from GREC?09.mitted JU system adds features to the set of REFattributes available from the corpus, including in-dices for paragraph, sentence and word.
It alsoadds features to the REFEX attributes available fromthe corpus, in order to distinguish between severalREFEXs that match the predicted REG08-TYPE andCASE combination.XRCE: The XRCE system uses a conditionalrandom field model in combination with the Sam-pleRank algorithm for learning model parameters.The feature functions used include unary ones(>100 features encoding the attributes provided inthe corpus as well as position within sentence, ad-jacent POS tags, etc.)
and binary ones (distance toprevious mention, distribution of type and case).Some binary feature functions are activated onlyif the previous mention was a name and controloveruse of pronouns.4.3 Evaluation resultsParticipants computed evaluation scores on the de-velopment set, using the geval code provided by uswhich computes Word String Accuracy, REG?08-Type Recall and Precision, string-edit distance andBLEU.
The following is a summary of teams?
self-reported scores:Recall Precision WSAUMUS 0.816 0.829 0.813UMUS?09 0.830 0.830 0.786XRCE 0.771 0.771 0.702UDel 0.758 0.758 0.650JU 0.66 0.63 0.54REG08-Type Recall and Precision results for TestSet NEG-a (version a of the test set with just oneREFEX for each REF) are shown in Table 4.
Aswould be expected, results on the test data aresomewhat worse than on the development data.Also included in this table are results for the 4baseline systems, and it is clear that selecting themost frequent RE type and case combination givenSEMCAT, SYNFUNC and SYNCAT (as done by theBase-freq system) provides a strong baseline, al-though it is a much better predictor for Composerand Inventor texts than Chef texts.The last 6 columns in Table 4 contain Recall (R)and Precision (P) results for the three subdomains.For most of the systems results are slightly betterfor Composers than for Chefs.
A contributing fac-tor to this may be the fact that Chef texts tend tobe much more colloquial.
A striking detail is thecollapse in scores in the Inventors subdomain forSystemREG08-Type Precision and Recall Scores against Corpus (Test Set NEG-a)All Chefs Composers InventorsPrecision Recall P R P R P RUMUS 80.71 A 78.31 A 79.19 75.44 80.88 78.68 81.66 80.05UMUS?09 80.17 A 77.06 A 75.16 70.71 82.25 79.54 80.66 78.08XRCE 74.26 A 71.38 A 68.55 64.50 75.44 72.96 76.84 74.38JU 66.98 A B 64.38 A B 79.56 74.85 84.32 81.55 26.97 26.11Base-freq 61.52 A B C 59.60 A B C 51.86 49.41 65.74 63.95 62.12 60.59UDel-NEG 60.92 A B C 58.56 A B C 55.35 52.07 62.43 60.37 62.85 60.84Base-rand 43.32 B C 42.00 B C 40.43 38.76 43.00 41.77 46.21 45.07Base-name 40.60 C 39.09 C 47.80 44.97 40.32 39.06 35.28 34.24Base-1st 40.25 C 39.64 C 47.88 46.75 39.71 39.20 34.91 34.48Table 4: REG08-Type Precision and Recall scores against corpus version of Test Set for complete set andfor subdomains; homogeneous subsets (Tukey HSD, alpha = .05) for complete set only.SystemREG08-Type Precision and Recall Scores against human topline (Test Set NEG-b)All Chefs Composers InventorsPrecision Recall P R P R P RCorpus 82.67 A 84.01 A 82.25 84.24 83.26 84.47 82.02 83.04UMUS 81.64 A 80.49 A 82.92 80.91 80.59 79.54 82.41 81.80UMUS?09 80.46 A 78.59 A B 80.50 77.58 80.62 79.10 80.15 78.55XRCE 73.76 A B 72.04 A B C 73.58 70.91 74.11 72.71 73.28 71.82UDel-NEG 65.54 A B C 64.01 A B C D 66.04 63.64 66.12 64.88 64.12 62.84Base-freq 65.38 A B C 64.37 A B C D 59.94 58.48 68.97 68.07 63.64 62.84JU 63.73 A B C 62.25 A B C D 76.42 73.64 76.04 74.60 32.32 31.67Base-name 55.22 B C 54.01 B C D 56.29 54.24 58.05 57.04 49.49 48.63Base-1st 54.68 B C 54.68 C D 55.45 55.45 57.68 57.68 48.88 48.88Base-rand 48.46 C 47.75 D 48.77 47.88 47.13 46.44 50.51 49.88Table 5: REG08-Type Recall and Precision scores against human topline version of Test Set for completeset and for subdomains; homogeneous subsets (Tukey HSD, alpha = .05) for complete set only.the JU system.
As a side effect, the resulting vari-ation led to fewer significant differences betweensystems being found in the results than would havebeen the case otherwise.We carried out univariate ANOVAs with Systemas the fixed factor, and REG08-Type Recall as thedependent variable in one ANOVA, and REG08-Type Precision in the other.
The F-ratio for Recallwas F(9,990) = 13.253, p < 0.001.2 The F-ratiofor Precision was F(9,990) = 12.670, p < 0.001.The columns containing single capital letters inTable 4 show the homogeneous subsets of systemsas determined by a post-hoc Tukey HSD analysis.Systems whose scores are not significantly differ-ent (at the .05 level) share a letter.Table 5 shows analogous results computedagainst Test Set NEG-b (which has three versionsof each text).
Table 5 includes results for the cor-pus texts, also computed against the three ver-sions of each text in test set GREC-NEG-b.
Weperformed univariate ANOVAs with System as thefixed factor, and Recall as the dependent variablein one, and Precision in the other.
The result forRecall was F(9,990) = 5.248, p < .001), and forPrecision F(9,990) = 5.038, p < .001.
We againcompared the mean scores with Tukey?s HSD.2We included the corpus texts themselves in the analysis,hence 9 degrees of freedom (10 systems).One would generally expect results on test setNEG-b to be better than on NEG-a.
This is the casefor all baseline systems and some of the participat-ing systems, but not all.
The JU system in particu-lar drops in score (and rank).We also computed Word String Accuracy andthe other string similarity metrics described inSection 3 for the GREC-NEG Task.
The result-ing scores for Test Set NEG-a are shown in Ta-ble 6.
Ranks for peer systems relative to each otherare very similar to the results for REG08-Type re-ported above.We performed a univariate ANOVA with Systemas the fixed factor, and Word String Accuracy asthe dependent variable.
The F-ratio for Systemwas F(9,990) = 41.308, p < 0.001; the homoge-neous subsets resulting from the Tukey HSD post-hoc analysis are shown in columns 3?7 of Table 6.Table 7 shows analogous results for humantopline Test Set NEG-b (which has three versionsof each text).
We carried out the same kind ofANOVA as for Test Set NEG-a; the result for Sys-tem on Word String Accuracy was F(9,990) =35.123, p < 0.001.
System rankings are the sameas for Test Set NEG-a (the differences between JUand Base-freq, which swap ranks, are not signif-icant); scores across the board (again, except forthe JU system) are somewhat higher, because ofthe way scores are computed for version b testSystemString similarity against Corpus (Test Set NEG-a)Word String AccuracyBLEU-3 NIST SE norm.
SEAll Chefs Composers InventorsUMUS 78.51 A 76.42 79.29 78.88 0.7968 7.4986 0.6063 0.2019UMUS?09 75.05 A 69.18 77.66 75.32 0.7615 6.9865 0.6806 0.2233XRCE 65.25 A 61.01 66.12 67.18 0.7031 6.0264 0.8969 0.3131JU 60.71 A 72.96 76.63 23.41 0.5720 5.7264 1.1810 0.3671Base-freq 57.10 A B 50.31 60.65 56.49 0.5913 4.9860 1.2249 0.4191UDel-NEG 38.21 B C 37.42 39.20 37.15 0.5498 5.0211 1.6222 0.5869Base-name 28.48 C D 35.53 27.51 24.43 0.4966 4.9355 1.8017 0.6662Base-rand 8.22 D E 8.49 7.10 9.92 0.1728 1.2501 2.4290 0.8928Base-1st 4.69 E 3.46 5.47 4.33 0.1990 2.4018 2.9906 0.8152Table 6: Word String Accuracy, BLEU, NIST, and string-edit scores, computed on Test Set NEG-a (sys-tems in order of Word String Accuracy); homogeneous subsets (Tukey HSD, alpha = .05) for StringAccuracy only.SystemString similarity against human topline (Test Set NEG-b)Word String AccuracyBLEU-3 NIST SE norm.
SEAll Chefs Composers InventorsCorpus 81.90 A 83.33 82.25 80.15 0.9499 9.1087 0.7082 0.2517UMUS 77.29 A B 79.25 76.48 77.10 0.9296 8.1746 0.8383 0.2906UMUS?09 74.84 A B C 73.58 75.59 74.55 0.8968 7.5005 0.9096 0.3083XRCE 63.95 A B C 66.35 63.02 63.61 0.7960 6.0780 1.1577 0.4060Base-freq 59.84 B C D 55.97 62.72 58.02 0.7393 5.4920 1.3949 0.4717JU 56.31 C D E 68.87 66.86 27.99 0.5765 5.8764 1.5114 0.4720UDel-NEG 41.60 D E 44.34 40.38 41.48 0.6503 5.9571 1.7138 0.6057Base-name 37.27 E 42.14 36.83 34.10 0.6480 6.6551 1.7299 0.6287Base-rand 10.45 F 10.06 9.91 11.70 0.2468 1.4828 2.4869 0.8884Base-1st 8.58 F 5.66 10.95 6.87 0.2824 3.5790 2.9226 0.7868Table 7: Word String Accuracy, BLEU, NIST, and string-edit scores, computed on Test Set NEG-b (sys-tems in order of Word String Accuracy); homogeneous subsets (Tukey HSD, alpha = .05) for StringAccuracy.sets: a score is the highest score a system achieves(at text-level) against any of the three versions ofa test set text that is taken into account.Results for BLEU-3, NIST and the two string-edit distance metrics are shown in the rightmost 4columns of Tables 6 and 7.
With the exception ofBase-freq/Basename on Test Set NEG-b, systemswhose Word String Accuracy scores differ signif-icantly are assigned the same relative ranks by allother string-similarity metrics as by Word StringAccuracy.In the human intrinsic evaluation, evaluatorsrated system outputs in terms of whether they pre-ferred them over the original Wikipedia texts.
Asa result of the experiment we had (for each systemand each evaluation criterion) a set of scores rang-ing from -10.0 to +10.0, where 0 meant no pref-erence, negative scores meant a preference for theWikipedia text, and positive scores a preferencefor the system-produced text.The second column of the left half of Table 8summarises the Clarity scores for each system interms of their mean; if the mean is negative theevaluators overall preferred the Wikipedia texts,if it is positive evaluators overall preferred thesystem.
The more negative the score, the morestrongly evaluators preferred the Wikipedia texts.Columns 8?10 show corresponding counts of howmany times each system was preferred (+), dis-preferred (?
), and neither (0).The other half of Table 8 shows correspondingresults for Fluency.We ran a factorial multivariate ANOVA with Flu-ency and Clarity as the dependent variables.
In thefirst version of the ANOVA, the fixed factors wereSystem, Evaluator and Wikipedia Side (indicatingwhether the Wikipedia text was shown on the leftor right during evaluation).
This showed no signif-icant effect of Wikipedia Side on either Fluency orClarity, and no significant interaction between anyof the factors.
There was also no significant effectof Evaluator on Fluency, and only a weakly sig-nificant effect of Evaluator on Clarity.
We ran theANOVA again, this time with just System as thefixed factor.
The F-ratio for System on Fluencywas F(9,290) = 22.911, p < .001, and for Systemon Clarity it was F(9,290) = 13.051, p < .001.Post-hoc Tukey?s HSD tests revealed the signif-icant pairwise differences indicated by the lettercolumns in Table 8.Correlation between individual Clarity and Flu-ency ratings as estimated with Pearson?s coeffi-cient was r = 0.66, p < 0.01, indicating that thetwo criteria covary to some extent.Clarity FluencySystem Mean + 0 ?
System Mean + 0 ?Corpus 0.000 A 1 28 1 Corpus 0.133 A 1 29 0UMUS -2.023 A B 1 13 16 UMUS -1.640 A B 4 12 14UMUS?09 -2.527 A B C 0 15 15 UMUS?09 -2.130 A B 3 11 16Base-name -2.900 B C 1 7 22 XRCE -3.587 B C 2 8 20Base-1st -3.160 B C 4 3 23 JU -4.057 B C D 0 10 20XRCE -3.500 B C D 1 9 20 Base-freq -4.990 C D 1 3 26JU -3.577 B C D 0 10 20 Base-name -6.620 D E 0 1 29UDel-NEG -5.137 C D E 0 1 29 Base-1st -7.823 E 1 0 29Base-freq -6.190 D E 0 2 28 Base-rand -7.950 E 1 0 29Base-rand -7.663 E 1 0 29 UDel-NEG -7.970 E 0 1 29Table 8: GREC-NEG: Results for Clarity and Fluency preference judgement experiment.
Mean = mean ofindividual scores (where scores ranged from -10.0 to + 10.0); + = number of times system was preferred;?
= number of times corpus text (Wikipedia) was preferred; 0 = number of times neither was preferred.The relative ranks of the peer systems are thesame in terms of both Fluency and Clarity.
How-ever, there are interesting differences in the ranksof the baseline systems.
For Clarity, Base-nameand Base-1st are scored fairly highly (presumablybecause both tend to pick named references whichare clear if not always fluent), but both go backto not being significantly better than Base-rand inthe Fluency rankings.
Base-freq does badly in theClarity scores, but is significantly better than thebottom three systems in terms of Fluency.5 GREC-NER5.1 TaskThe GREC-NER task is a straightforward combinednamed-entity recognition and coreference resolu-tion task, restricted to people entities.
The aim forparticipating systems is to identify all those typesof mentions of people that we have annotated inthe GREC-People corpus, and to insert REF andREFEX tags with coreference IDs into the texts.5.2 SystemsBaselines: We used the coreference resolversincluded in the LingPipe3 and OpenNLP Tools4packages as baseline systems.Poly-co: The Poly-co system starts by applyinga POS tagger to the input text.
A ConditionalRandom Fields classifier (trained on an automat-ically annotated Wikipedia corpus) is then usedto detect named mentions, using word and POSbased features.
Logical rules then detect pro-noun mentions, using named-entity, word and POSfeatures.
Coreference of named mentions is de-termined by clustering with a similarity measurebased on words, POS tags and sentence position,3http://alias-i.com/lingpipe/4http://opennlp.sf.netapplied to mentions in order of their appearance.Coreference of pronouns is determined with theHobbs algorithm for anaphora resolution.UDel-NER: The UDel-NER system starts by (1)parsing the input text with the Stanford Parser,from which it extracts syntactic functions of wordsand relationships between them; and (2) separatelyapplying the Stanford Named Entity Recognizer.Pronoun and common noun mentions are identi-fied using lists of all English pronouns and of com-mon nouns which could conceivably be used torefer to people (occupations like ?painter?, fam-ily relations like ?grandmother?, etc.).
Values forall REF and REFEX attributes except coreference IDare obtained.
Finally, the system applies a coref-erence resolution tool which compares each refer-ence to all previous references in reverse order, onthe basis of case, gender, number, syntactic func-tion, and REG?08-Type.5.3 ResultsThe coreference resolution accuracy scores for theGREC-NER systems are shown in Table 9.
The twoparticipating systems are both significantly betterthan the two baslines in terms of their mean coref-erence resolution accuracy scores.6 GREC-Full6.1 TaskThe aim for GREC-Full systems was to improvethe referential clarity and fluency of input texts.Participants were free to do this in whichever waythey chose.
Participants were encouraged, thoughnot required, to create systems which replace re-ferring expressions as and where necessary to pro-duce as clear and fluent a text as possible.
Thistask could be viewed as composed of three sub-tasks: (1) named entity recognition (as in GREC-Test setMean B-3 CEAF MUCUDel-NER 72.71 A 80.51 77.53 60.09Poly-co 66.99 A 76.92 70.29 53.77LingPipe 58.23 B 71.19 61.58 41.92OpenNLP 54.03 B 67.61 59.17 35.32Table 9: MUC-6, CEAF and B-3 scores for GREC-NER systems.
Systems shown in order of averagescores.NER); (2) a conversion tool to give lists of possi-ble referring expressions for each entity; and (3)named entity generation (as in GREC-NEG).6.2 SystemsAll GREC-Full systems in our evaluations are com-posed of a GREC-NER and a GREC-NEG system.We created three baseline systems.
Two of thesewe created by combining the two GREC-NER base-line systems with the random GREC-NEG base-line system (Base-rand).
For this purpose we cre-ated a simple conversion utility which adds defaultREFEXs.
The third baseline system combines theUDel-NER system with Base-rand.The only team that submitted both a GREC-NERand a GREC-NEG system was UDel.
All otherGREC-Full systems therefore combine the effortsof two teams (for overview of system combina-tions, please refer to Table 1).
The two systemcombinations involving the UDel-NER system didnot require a conversion utility, because UDel-NERalready outputs full GREC-People format.6.3 ResultsNIST and BLEU scores computed against theWikipedia texts for the GREC-Full systems areshown in Table 10.
Note that these have beencomputed on the complete texts, not just the refer-ential expressions (which explains the high BLEUscores).
The scores in the second row (Corpus,test set vers.)
are obtained by comparing the testset versions of the corpus texts (in which some ofthe references have been replaced with standard-ised named references, as explained in Section 2)against the Wikipedia texts.
The two halves of thetable show scores computed against version a ofthe test set (the original Wikipedia texts) on theleft, and against version b of the test set (which hasthree versions of each text with human-selectedREs) on the right.In the human intrinsic evaluation of GREC-Fullsystems, evaluators again rated system outputs interms of whether they preferred them over theoriginal Wikipedia texts.
Table 11 shows the re-sults in the same format as in Table 8 for theGREC-NEG systems.We ran the same two factorial multivariateANOVAs with Fluency and Clarity as the depen-dent variables.
In the first version of the ANOVA,there were no effects of Evaluator (apart froma mild one on Clarity) and Wikipedia Side andno significant interaction between any of the fac-tors.
There was no effect of Evaluator on Fluencyand only a mild effect of Evaluator on Clarity.The second ANOVA just had System as the fixedfactor.
The F-ratio for Fluency was F(6,140) =13.054, p < .001, and for System on Clarity itwas F(6,140) = 14.07, p < .001.
Post-hoc Tukey?sHSD tests revealed the significant pairwise differ-ences indicated by the letter columns in Table 11.Correlation between individual Clarity and Flu-ency ratings as estimated with Pearson?s coeffi-cient was r = 0.696, p < .01, indicating that thetwo criteria covary to some extent.Apart from UDel-Full and OpenNLP/Base-randswitching places, system ranks are the same forFluency and Clarity.
Moreover, system ranksare very similar to those produced by the string-similarity scores above.
UDel-Full is a muchharder task than GREC-NEG and it is a very goodresult indeed for a system to be preferred overWikipedia once or twice and to be rated equallygood as Wikipedia 4?7 times.7 Concluding RemarksGREC?10 has, for the first time, produced systemswhich can do end-to-end named-entity generation,moreover most of which can do it well enough forhuman judges do rate them as good as Wikipediaor better around one third of the time.This was the second time the GREC-NEG Taskwas run, and the first time GREC-NER and GREC-Full were run.
As in 2009, many more teams reg-istered than were able to submit a system by thedeadline, but we hope that the GREC data (whichis now freely available) will lead to many more re-Test Set NEG-Full-a Test Set NEG-Full-bSystem Mean text-level BLEU-4 BLEU-4 NIST System Mean text-level BLEU-4 BLEU-4 NISTCorpus 1.00 A 1.000 13.71 Corpus .991 A 0.985 13.74Corpus (test set vers.)
.941 B 0.923 12.92 Corpus (test set vers.)
.946 B 0.929 13.20UDel/UMUS .934 B C 0.925 13.13 UDel/UMUS .939 B C 0.928 13.29UDel/XRCE .921 B C 0.898 12.98 UDel/XRCE .928 B C 0.907 13.15UDel-Full .905 C 0.870 12.59 UDel-Full .912 C 0.882 12.82UDel/Base-rand .812 D 0.809 12.17 UDel/Base-rand .823 D 0.821 12.43OpenNLP/Base-rand .809 D 0.775 11.49 OpenNLP/Base-rand .817 D 0.785 11.72LingPipe/Base-rand .752 E 0.753 11.48 LingPipe/Base-rand .763 E 0.764 11.70Table 10: GREC-FULL: Mean text-level BLEU-4 scores, system-level BLEU-4 and NIST scores.Clarity FluencySystem Mean + 0 ?
System Mean + 0 ?Corpus -0.033 A 1 20 0 Corpus 0 A 0 30 0UDel/XRCE -2.209 A B 0 6 15 UDel/XRCE -3.424 B 1 4 16UDel/UMUS -2.638 A B 1 6 14 UDel/UMUS -4.057 B C 2 5 14UDel-Full -2.833 B 0 7 14 OpenNLP/Base-rand -4.671 B C 2 4 15OpenNLP/Base-rand -3.486 B 1 7 13 UDel-Full -4.967 B C 0 4 16UDel/Base-rand -4.667 B 0 5 16 UDel/Base-rand -6.800 C D 0 2 19LingPipe/Base-rand -7.829 C 0 0 21 LingPipe/Base-rand -8.405 D 0 0 21Table 11: GREC-FULL: Results for Clarity and Fluency preference judgement experiment.
Mean =mean of individual scores (where scores ranged from -10.0 to + 10.0); + = number of times system waspreferred; ?
= number of times corpus text (Wikipedia) was preferred; 0 = number of times neither waspreferred.sults being produced and reported over time.AcknowledgmentsMany thanks to the members of the Corpora andSIGGEN mailing lists, and Brighton Universitycolleagues who helped with the online RE selec-tion experiments for the b-versions of the test sets.Thanks are also due to the Oxford, Kings CollegeLondon and University College London studentswho helped with the intrinsic evaluation experi-ments.ReferencesA.
Bagga and B. Baldwin.
1998.
Algorithms for scor-ing coreference chains.
In Proceedings of the Lin-guistic Coreference Workshop at LREC?98, pages563?566.A.
Belz and S. Varges.
2007.
Generation of repeatedreferences to discourse entities.
In Proceedings ofENLG?07, pages 9?16.A.
Belz.
2010.
GREC named entity recognition andGREC named entity regeneration challenges 2010:Participants?
Pack.
Technical Report NLTG-10-01,Natural Language Technology Group, University ofBrighton.R.
Huddleston and G. Pullum.
2002.
The CambridgeGrammar of the English Language.
CambridgeUni-versity Press.X.
Luo.
2005.
On coreference resolution performancemetrics.
Proc.
of HLT-EMNLP, pages 25?32.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1995.
A model-theoretic corefer-ence scoring scheme.
Proceedings of MUC-6, pages45?52.
