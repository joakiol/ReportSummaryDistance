A New Model for Lexical Choice for Open-Class WordsEhud Reiter'~Aiken Computation LaboratoryHarvard UniversityCambridge, Mass 02138AbstractThe lexical choice process hould be regarded as a con-straint satisfaction problem: the generation system mustchoose a lexical unit that is accurate (t~mthful), va//d(conveys the necessary information), and preferred (max-irnal under a preference function).
This corts~aint-basedarchitecture allows a clema separation to be madebetween what the system knows of the object or event,and what the system wishes to communicate about heobject or event.
It also allows lexical choices to bebiased towards basic-level (Rosch 1978) and other pre-ferred lexical units.1.
IntroductionLexical choice for open-class words has typically beenregarded as a matching or classification problem.
Thegeneration system is given a semantic structure thatrepresents an object or event, and a dictionary thatrepresents the semantic meanings of the lexical units(Zgusta 1971) of the target language; it then chooses thelexical unit (or set of lexical units) that best matches theobject or event.
This paper proposes an alternative l xi-cal choice architecture, in which the lexical choice pro-cess is regarded as a constraint satisfaction problem: thegeneration system must choose a lexical unit that isaccurate (truthful), valid (conveys the necessary infor-marion), and preferred (maximal under a preferencefunction).
1 This constraint-based architecture is morerobust than classification systems.
In parricular, itallows a clean separation to be made between what thesystem knows of the object or event, and what the sys-tem wishes to communicate about the object or event;and it allows lexical choices to be biased towardsbasic-level (Rosch 1978) and other preferred lexicalunits.Throughout this paper, it will be assumed that bothlexical units and objects/events are represented ast Currently at the Department of Artificial Intelligence,University of Edinburgh, 80 South Bridge, Edinburgh EH1 1HN,Scotland.
E-maih reitel@aitma.edinburgh.ac.ukI This paper does not exmnine the kind of oollocational andselectional constraints discussed by Cumming (1986) and Niren-burg and Nirenburg (1988).classes in a KL-ONE type taxonomy (Brachman andSchmolze 1985).
For example, the lexical unitBachelor might be represented as the generic class(Human with role value restrictions Sex:Male, Age-status:Adult, Married:False); and the object Terry mightbe represented as the individual class (Human with rolefillers Sex:Male, Eye-color:Brown, Birthplace:Chicago,Employer:IBM .
.
.
.
).
Default attributes as well asdefinitional information can be associated with lexicalunits; this is essential for making appropriate lexicalchoices (Section 5).
Figure 1 shows a sample taxonomythat will be used for most of the examples in this paper.Lexical units (e.g., Bachelor) are shown in bold font,while objects (e.g., Terry) are shown in italic font.Role value restrictions (VR's), such as Sex:Male forMan, are fisted textually instead of displayed graphi-cally, to simpfify the complexity of the diagram; defaultattributes (e.g., Can-fly:True for Bird) are listed in italicfont.
Basic-level classes (e.g., Man) are underlined.Section 2 of the paper discusses classification-basedsystems and some of the problems associated with them.Section 3 introduces the proposed constraint-based sys-tem; Section 4 looks in more detail at the lexical prefer-ences used by the system; and Section 5 brieflydiscusses the need for default atlributes in the semanticrepresentations of lexical units.
The constraint-basedlexical choice system has been incorporated into the FNsystem (Reiter 1990), which generates certain kinds ofnatural language object descriptions.
FN uses someadditional preference rules that primarily affect NP for-marion; these rules are not discussed in this paper.2.
Lexical Choice as ClassificationThe two major approaches (to date) for lexical choicehave been discrimination ets and structure mappingsystems.
Both of these approaches can be regarded asclassification/matching architectures, where a classifieris given an object or event, and is asked to find anappropriate l xical unit that fits that object or event.Discrimination ets (e.g., Goldman 1975; Pustejovskyand Nirenburg 1987) are basically decision trees.
Theyare typically used as high-speed 'compiled' classifiersthat select the most specific lexical unit that subsumes23C AnimalVertebrateObjec~MachinetNetworkFishtShark(Dangerous:True) ~tLBreathes:AirHuman ) ( PekingeseBrca~es.-Air(Can-fly:True) jl a r rowEthernet Dam-rate: lOMbiffsec lCircuit-type:Packet \[Physical-medium:Coaxial-cableJ0I Adult Age-status:Adult If \I ~ ,  I \[ ~=ma~o IBachelorMarried:False IcPrimitive 3lass\[ Defined \[Class-.
@(Key)Basic Level Class defining role VRLexical Unit Class (default role filler)Object Classis ~ ycaOStrich n-fly:FalseJFigure 1: ObjectsLexical Units in aTaxonomyand24the target object or event.
For instance, looking atsome of Goldman's examples, the event!ngest(John,Milk027), which can be represented in KL-ONE as (Ingest with VR's actor:John andtheme:Milk027), has as its most specific subsuming lexi-cal unit (Ingest with VR theme:Liquid), and thus is lexi-cally realized as "drink".
Similarly, the actionIngest(BearO36,Fish802), which'can be represented inKL-ONE as (Ingest with VR's actor:Bear036 andtheme:Fish802), has (Ingest with VR's agent:Non-human-animal nd theme:Solid) as its most specific sub-sumer in a taxonomy of German lexical units, and thusis realized, in German, as "fressen".Structure-mapping systems (e.g., Jacobs 1987; Iordan-skaja et al 1988; note that different erminology isused in different papers) take as input a semantic struc-ture that needs to be communicated to the user, searchfor pieces of the input structure that are equivalent tolexical units, and then replace the matched structure bythe corresponding lexical unit.
The matching and sub-sdtution process continues until the semantic structurehas been completely reformulated in terms of lexicalunits.
For example, the structure (Human (:sex male)(:age-status adult) (:wealth high)) might be mapped intothe structure ("man" (:attribute "rich")), and hence lexi-tally realiTcd as "rich man".
In KL-ONE terms, thematching process can be considered to be a search for aclass definition that uses only classes and role VR's thatcan be realized as lexical units; e.g., the above exampleessentially redefines the class (Human with role VR'sSex:Male, Age-status:Adult, Wealth:High) as theequivalent class ("man" with VR "rich"), where thelexical unit "man" represents the class (Human withrole VR's Sex:Male, Age-status:Adult), and the lexicalunit "rich" is equivalent to the role VR Wealth:High.Recently, the machine translation group at CMU hasproposed an alternative lexical choice system that isbased on a variant of nearest neighbor classification(Center for Machine Translation 1989; Nirenburg et al1987).
In the CMU system, both objects and lexicalunits are treated as points or regions in a feature space,and the classifier works by choosing the lexical unit thatis closest o the target object, using a fairly complexdistance (matching) metric (collocation constraints arealso taken into consideration).
For example, the object(Human with VR's Sex:Male and Age:13) would bejudged closest to the lexical unit (Human with VR'sSex:Male and Age:range(2,15)), and thus would be real-ized as "boy'.All of the above classification-based lexical-ehoicearchitectures 2 uffer from two basic flaws:?
they do not allow a clean separation to be madebetween what the system knows, and what it wishesto communicate;?
they do not provide a clean mechanism for allowingthe lexical choice process to be biased towards pre-ferred lexical units.These failures may lead classification-based systems tochoose inappropriate l xical units that carry unwantedconversational implicatures (Grice 1975), and thereforemislead the user.2.1.
One Input vs Two InputsClassification-based systems take as their input a singleset of attributes about he object/event being lexicalized,and use this set of attributes to select a matchingclassification.
However, lexical choice systems houldlook at two input sets of attributes: the set ofobject/event a tributes that are relevant and need to beconveyed to the user, and the set of attributes that con-stitute the system's total knowledge of the object/eventbeing lexicalized.A lexieal choice system that looks only at thesystem's domain knowledge about he object/event, andignores the set of relevant attributes, may choose inap-propriate lexical items that carry unwanted relevanceconversational implicatures.
In particular, a system thatsimply selects the most specific lexical unit that sub-sumes the object/event (as many discrimination et sys-tems do) may mislead the user by choosing lexical unitsthat are too specific.
For example, consider the follow-ing exchange:1) A: "Is Terry a woman?
"2a) B: "No, Terry is a man"2b) B: "No, Terry is a bachelor"B's communicative goal is simply to inform A thatTerry has the attributes {Human, Age-status:Adult,Sex:Male}, so utterance (2a) is an appropriate response.A lexical choice system that simply selected the mostspecific lexical unit that subsumed Terry would generateutterance (2b), however.
Utterance (2b) is inappropri-ate, and would probably lead A to infer the (incorrect)conversational implicature that B thought hat Terry'smarital status was relevant to the conversation.A lexical choice system that looks only at the attri-butes being communicated, and ignores the system's2 Individual lexical-daoice systems can, of course, be aug-mented with special code that addresses some of these issues; theclaim is that the classification-based l xical-choice architeouresdo not easily or naturally deal with these problems.25general domain knowledge about the object/event, mayalso make inappropriate lexical choices that lead tounwanted conversational implicatures.
For example,suppose A wished to communicate o B that XNET wasa Network with the attributes {Data-rate:lOMbit/sec,Circuit-type:Packet-switched}.
Consider three possiblelexicalizations:3a) "XNET is a network"3b) "XNET is a I0 Mbit/sec packet-based network"3c) "XNET is an Ethernet"Utterance (3c) is the most appropriate utterance (assum-ing the user has some domain knowledge about Ether-nets).
Utterance (3a), however, would be generated bya system that simply chose the most specific lexieal unitthat subsumed {Network, Data-rate:lOMbit/sec,Circuit-type:Packet-switched}.
3 This utterance fails tofulfill the communicative goal of informing the readerthat the network has the attributes {Data-rate:lOMbitlsec, Circuit-type:Packet-switched}, and istherefore unacceptable.
Utterance (3b) would be gen-erated by a structure-mapping system that chose a lexi-cal unit according to the above strategy, and then addedexpficit modifiers to communicate attributes that werenot impfied by the lexical class.
4 Tlds utterance success-fully communicates the relevant information, but it alsoimplicates, to the knowledgeable hearer, that XNET isnot an Ethernet m because if it was, the knowledgeablehearer would reason, then the speaker would have usedutterance (3e).2.2.
Preferred Lexical UnitsCertain lexical units, in particular those that representbasic-level classes (Rosch 1978), are preferred andshould be chosen whenever possible.
Cruse (1977) and3 Another possibility is choosing the most general exical unitthat is subsumed by the attributes being communicated.
Howev-er, this cmnot be done by a ~/stma that ignores the object andonly 1oo1~ at the attributes being communicated, because such asystem would not know which le~ical units accurately describedthe object.
For example, if there were two classes Ethernet andApplenet that had the attributes (Network, Data-rate:lOMbitlsec,Circuit.type:Packet-switched}, the system could only decidewhether to generate "Ethemet" or "Applenet" by detexminingwhich of these classes ubsumed the object being described (e.g.,"Etbemet" should be used to describe XNET).
See also exam-pie 5, where the most appropriate l exical unit that informs thehearer that F/do has the attributes {An/ma/, Breathes:Air} is"dog',  not "mmnmal" or "animal'.,t In this example, the 'lexical choice' system is assumed tocapable of forming a complete NP.
In general, it is oftendifficult to separate the task of selecting a single word from thetask of forming a complete phrase.others have suggested that the failure to use a basic-level class in an utterance will conversationally impli-cate that the basic-level class could not have been used.For example, consider the following utterances:4) A: "I want to flood room 16 with carbon dioxide"5a) B: "Wail  there is an animal in the room"5b) B: "Wail there is a dog in the room"5c) B: "Wait, there is a Pekingese in the room"Assume the object in question is Fido, and A's com-municative goal is simply to inform B that Fido has theattributes {An/ma/, Breathes:Air}, and hence would beadversely affected if the room was flooded with carbondioxide.
Utterances (5a), (5b), and (5c) all fulfill thiscommunicative goal (assuming that Breathes:Air is adefault attribute of An/ma/), but utterance (5b) is pre-ferred because Dog is a basic-level class.
Utterance(5a) is odd because the use of the superordinate classAnimal impficates, according to Cruse's hypothesis, thatthe animal in question is not a Dog, Cat, or other com-monly known type of animal (or at least the speakerdoes not know that the animal is a member of one ofthese species); utterance (5c) is odd because the use ofthe subordinate class Pekingese implicates that it issomehow relevant hat the animal is a Pekingese andnot some other kind of dog.
If both of these impfica-tures are incorrect, the speaker should choose the lexicalunit Dog if he wishes to avoid misleading the hearer.It should be pointed out that the strategy of simplyalways picking a basic-level class that subsumes theobject/event will not work, because it ignores thesystem's communicative goals.
For instance, a systemthat followed the basic-level strategy would, in thesituation of example 3, generate utterance (3a) or (3b).Both of these are inappropriate and implicate, to theknowledgeable user, that u t t~ce  (3c) could not havebeen used, i.e., that XNET is not an Ethernet.3.
Lexical Choice as Constraint SatisfactionThe above problems can be avoided by regarding lexi-cal choice as a constraint-satisfaction ask instead of aclassification task.
More precisely, the task of choosingan appropriate open-class lexical unit should be formal-ized as follows:Input.?
Entity: a taxonomy class that represents the system'sknowledge of the object or event being lexicalized.?
To-Communicate: a set of predicates (attributes) thatrepresent he relevant information about the objectthat needs to be communicated to the user.26Output: A lexical unit Lex that is a member of theknowledge-base taxonomy, and that satisfies the follow-ing constraints:?
Accurate: Lex must be a truthful description of Entity.Formally, Lex must subsume Entity.?
Valid: The use of Lex in an utterance must inform theuser that the predicates in To-Communicate hold forEntity.
Formally, every predicate in To-Communicatemust either be inferrable from the definition of Lex(e.g., subsume Lex), or be a default attribute that isassociated with Lex.?
Preferred: Lex must be a maximal element of the setof accurate and valid lexical units under certain lexi-cal preference rules (Section 4).In other words, the lexical choice system is given twoinputs, which represent the system's knowledge of theobject or event, and the relevant information about thatobject or event that needs to be communicated to theuser; and is expected to produce as its output a maximallexical unit (under the lexical preference rules) that istruthful and conveys the relevant information.The constraint-based system makes appropriate lexi-cal choices in each of the previous examples:?
Entity = Terry, To-Communicate = {Human,Sex:Male} (example 2).
Both Man and Bachelor areaccurate and valid lexical units.
Man is chosen,because it is basic-level and therefore preferred.?
Entity = XNET, To-Communicate = {Network, Data-rate :l OMbitlsec, Circuit-type:Packet-switched}(example 3).
Ethernet is chosen, because it is theonly accurate and valid lexical unit.?
Entity = Fido, To-Communicate = {Animal,Breathes:Air} (example 5).
Accurate and valid lexi-cal units include Animal, Mammal, Dog, and Pek-ingese.
Dog is chosen, because it is basic-level.4.
Preferences Among Lexical ClassesIf several exical units are accurate and valid, a set oflexical preferences rules is used to select the lexicalunit the system will utter.
The preference for basic-level classes was previously mentioned (Section 2.2),but it is complicated by entry-level ffects (Section 4.1),Additional exical preferences include the length\[subsetpreference (Section 4.2).
Combined, the lexical prefer-ence rules impose a lexical preference hierarchy on thelexical units in the knowledge base.
Figure 2 showspart of the lexical preference hierarchy that is associatedwith the knowledge base of Figure 1.4.1.
Basic-Level vs Entry-Level PreferencesHirschberg (1985) has suggested that it may be better touse Jolicoeur et al's (1984) notion of entry level classesinstead of Rosch's basic level classes.
The difference isthat under the entry-level hypothesis, which category isunmarked (i.e., which category may be used withoutgenerating a conversational implicature) may depend onhow atypical the object is.
For example, consider:.
(the speaker points to a robin)7a) ``Look at the bird"7b) "Look at the robin"(the speaker points to an ostrich)8a) "Look at the bird"8b) "Look at the ostrich"Under the basic-level hypothesis, a category is eitherbasic-level or it is not, and ff it is basic-level, then it isalways the unmarked way of referring to any object hatbelongs to it.
Therefore, under this hypothesis utter-ances (7a) and (8a) are both unmarked and carry noconversa6onal implicatures, ince Bird is a basic-levelcategory for most urban Americans.
Under the entry-level hypothesis, in contrast, while a basic-levelcategory is the unmarked way of referring to 'normal'members of the category, it may not be the unmarkedway of referring to atypical members.
Instead, a morespecialized category may be the unmarked way of refer-ring to atypical members.
Thus, under the entry-levelhypothesis, even ff utterance (7a) was the unmarkedway of referring to robins (which are typical birds),utterance (8b) could still be the unmarked way of refer-ring to ostriches (which are atypical birds).The lexical-choice system can allow for entry-leveleffects ff it allows any lexical unit to be marked asbasic-level in the taxonomy, but then only considers thelowest such marked class to be a true basic-level (andhence lexically-preferred) class for an object.
Moreprecisely, ff an object has two subsumers A and B thatare both marked as basic-level classes, and A subsumesB, then the system should only treat B as a lexically-preferred class for the object.
For example, in Figure 1Bird and Ostrich are both marked as basic-level.
There-fore, the lexical-choiee system should treat Bird (but notSparrow) as a lexically-preferred class for Tweety (aSparrow), and Ostrich (but not Bird) as a lexically-preferred class for Big-Bird (an Ostrich).4.2.
Length/Subset PreferencesA lexical unit A is almost always preferred over a lexi-cal unit B if A's surface form uses a subset of thewords used by B's surface form (this can be considered27Fish Dog Man ) ~_Bird ) \]x~arro~ CPekingese ")C Bachelor)hark parrow ~Basic Level Preference Word Subset Preference.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.X  ................................................. VFigure 2: Some of the Lexical Preferences from Figure 128to be a consequence of Grice's maxim of quantity(Grice 1975)).
Consider, for example,9a) ``Don't go swimming; there is a shark in the water"9b) "Don't go swimming; there is a tiger shark in thewater"According to the subset lexical, preference rule, lexicalunit Shark is preferred over lexical unit Tiger-shark.Therefore, the use of utterance (9b) carries the conver-sational implicature that utterance (9a) could not beused, i.e., that it was relevant hat the animal was aTiger-shark and not some other kind of Shark.
A hearerwho heard utterance (9b) might infer, for example, thatthe speaker thought that tiger sharks were unusuallydangerous kinds of sharks.
If no such implicature wasintended by the speaker, then he should use utterance(9a), not utterance (9b).A stronger version of this preference rule would be toprefer lexical unit A to lexical unit B if A's surfaceform used fewer open-class words than B's surfaceform.
This would, for example, correctly predict thatDog is preferred over Great-Dane, and that Flower ispreferred over Rocky-Mountain-iris.
This preference isusually accurate, but it does fail in some cases.
Forexample, it is questionable whether Porsche is preferredover Sports-car, and doubtful whether Mammal is pre-ferred over Great-Dane.There are cases where the basic-level preferenceconflicts with (and takes precedence over) both the sub-set and the length preferences.
Such conflicts are prob-ably rare, because psychological nd linguistic findingssuggest hat basic-level classes are almost always lexi-cally realized with single words (Rosch 1978; Berlin etal.
1973).
However, there are a small number of basic-level classes that have multi-word reali7ations, and thiscan lead to conflicts of the above type.
Consider, forexample,10a) "Joe has a mach/ne"10b) ``Joe has an appliance"10c) "Joe has a washing machine"Washing-machine is probably basic-level for mostAmericans.
Therefore, utterance (10c) is preferred overutterances (10a) and (10b), despite the fact that thelength preference suggests that utterances (10a) and(10b) should be preferred over utterance (10c), and thesubset preference suggests that utterance (10a) shouldbe preferred over utterance (10c).4.3.
Other Lexical PreferenceThere are lexical preferences that are not captured byeither the basic-level preference or the subset/lengthpreference.
For example, suppose the speaker wished torefer to two animals, a horse and a cow.
Consider thedifference betweenl la) "Look at the an/mats"1 lb) "Look at the mamma/s"1 lc) "Look at the vertebrates"None of the above are basic-level classes (Horse andCow are basic-level for most urban Americans).
There-fore, neither the basic-level nor the length/subset rulesindicate any preferences among the above.
However, itseems clear that utterance ( l la) is much preferable toutterance (lib), and that utterance (l ib) is probablypreferable to utterance (llc).
In addition, the use ofutterances (l lb) or ( l lc) seems to implicate that utter-unee (l la) could not have been used.5.
Default AttributesOne final point is that the representation f the seman-tics of lexical units must include default attributes aswell as definitional information.
These defaults mayrepresent domain knowledge (e.g., birds typically fly) oruseful conventions that have evolved in a particularenvironment (e.g., most computers at Harvard's AikenComputation Lab run the UNIX operating system).Systems that ignore default attributes may make inap-propriate lexical choices, and therefore generate utter-anoes that carry unwanted conversational implicatures.For example, ff To-Communicate was {Bird, Can-fly:True}, and Entity was Tweety, consider thedifference between12a) "Tweety is a b/rd"12b) ``Tweety is a bird that can fly"If the generation system ignored default attributes, itwould have to generate something like utterance (12b).Utterance (12b) sounds odd, however, and a person whoheard it might infer unwanted and unintended conversa-tional implicatm'es, e.g., that some other bird under dis-cussion was not able to fly.
Utterance (12a) is muchbetter, but it can only be generated by a generation sys-tem that takes into consideration the fact that Can-fly:True is a default attribute of Bird.For another example, sutvose an NLG system wishedto inform a user that a particular computer was a VAXthat ran the UNIX operating system and the Latex textprocessor (i.e., To-Communicate = {VAX, Operating-29system:UNIX, Available-software:Latex}).
Consider twopossible utterances:13a) "Hucl is a VAX that runs Latex"13b) "Hucl is a UNIX VAX that runs Latex"Utterance (13a) is acceptable, and indeed expected, ifthe user thinks that Operating-system:UNIX is a defaultattribute of VAX's in the current environment (e.g., atthe Aiken Computation Lab).
In a different environ-ment, where users by default associate Operating-system:VMS with VAX's, utterance (13a) would bemisleading and unacceptable, and utterance (13b) shouldbe generate&6.
ConclusionThis paper has proposed a lexical choice system thatsearches for lexical units that are accurate, valid, andpreferred with respect o the information the generationsystem wishes to communicate (To-Communicate), andthe object or event being lexicalized (Entity).
This sys-tem is more robust than discrimination ets and otherexisting classification-based l xical choice systems, andin particular is less likely to make inappropriate lexicalchoices that lead human readers to infer unwantedconversational implicatures.
The improved performanceis largely a consequence of the fact that the systemallows a clean separation to be made between what thesystem knows, and what it wishes to communicate; andthe fact that the system allows lexical choice to bebiased towards preferred lexical units.AcknowledgementsMany thanks to Joyce Friedman, Barbara Grosz, ChrisMellish, Stuart Shieber, and Bill Woods for their help.This work was partially supported by a National Sci-ence Foundation Graduate Fellowship, an IBM GraduateFellowship, and a conlract from U S West AdvancedTechnologies.
Any opinions, findings, conclusions, orrecommendations are those of the authors and do notnecessarily reflect the views of the National ScienceFoundation, IBM, or U S West Advanced Technologies.ReferencesBerlin, B.;Brecdlove, D.; and Raven, P. 1973 GeneralPrinciples of Classification and Nomenclature in FolkBiology.
American Anthropologist 75:214-242.Brachman, R. and Schmolze, J.
1985 An overview ofthe KL-ONE knowledge representation system.
Cog-nitive Science 9:171-216.Center for Machine Translation 1989 KBMT-89 ProjectReport.
Carnegie-Mellon University.Cruse, D. 1977 The pragmatics of lexical specificity.Journal of Linguistics 13:153-164.Cumming, S. 1986 The Lexicon in Text Generation.
ISIResearch Report ISI/RR-86-168.
Information SciencesInstitute, University of Southern California.Goldman, N. 1975 Conceptual generation.
In R. Schankand C. Riesbeck (Eds.
), Conceptual Information Pro-cessing~ American Elselvier.
New York.Grice, H. 1975 Logic and conversation.
In P. Cole andJ.
Morgan (Eds.
), Syntax and Semantics: Vol 3,Speech Acts, pg 43-58.
Academic Press: New York.Hirschberg, J.
1985 A Theory of Scalar Implicature.Ph.D thesis.
Report MS-CIS-85-56, LINC LAB 21,Department of Computer and Information Science,University of Pennsylvania.Iordanskaja, L.; Kittredge, R.; Polguere, A.
1988 Imple-menting a Meaning-Text Model for Language Gen-eration.
Presented at COLING 1988 (not in proc.
).Jacobs, P. 1987 Knowledge-Intensive Natural LanguageGeneration.
Artificial Intelligence 33:325-378.Jolicoeur, P.; Gluck, M.; and Kosslyn, S. 1984 Picturesand Names: Making the Connection.
CognitivePsychology 16:243-275.Nirenburg, S. and Nirenburg, I.
1988 A Framework forLexical Selection in Natural Language Generation.Proceedings of the 12th International Conference onComputational Linguistics (2):471-475.Nirenburg, S.; Nyberg, E.; and Kenschaft, E. 1987 Inex-act Frame Matching for Lexical Selection in NaturalLanguage Generation.
Unpublished memo, Center forMachine Translation, Carnegie-Mellon University.Pustejovsky, J. and Nirenburg, S. 1987 Lexical selectionin the process of natural language generation.
InProceedings of the 25th Annual Meeting of the Asso-ciation for Computational Linguistics: pages 201-206.Reiter, E. 1990 Generating Descriptions that Exploit aUser's Domain Knowledge.
In R. Dale, C. Mellish,and M. Zock (Eds.
), Current Research in NaturalLanguage Generation.
Academic Press: London.Forthcoming.Rosch, E. 1978 Principles of Categorization.
In E.Rosch and B. Lloyd (Eds.
), Cognition and Categori-zation.
Lawrence Erlbaum: Hillsdale, NJ.Zgusta, L. 1971 Manual of Lexicography.
AcademiaPress: Prague.30
