IDENT IFY ING RELEVANT PR IOREXPLANATIONSJ ames  A .
Rosenb lumDepar tment  of Computer  ScienceUnivers i ty  of P i t t sburghP i t t sburgh ,  PA 15260, USAInternet: jr@cs.pitt.eduAbst ractWhen human tutors engage in dialogue, they freelyexploit all aspects of the mutually known context,including the previous discourse.
Utterances thatdo not draw on previous discourse seem awkward,unnatural, or even incoherent.
Previous discoursemust be taken into account in order to relate newinformation effectively to recently conveyed mate-rial, and to avoid repeating old material that woulddistract the student from what is new.Producing a system that displays such behaviorinvolves finding an efficient way to identify whichprevious explanations (if any) are relevant to thecurrent explanation task.
Thus, we axe implement-ing a system that uses a case-based reasoning ap-proach to identify previous situations and expla-nations that could potentially affect the explana-tion being constructed.
We have identified heuris-tics for constructing explanations that exploit thisinformation in ways similar to what we have ob-served in human-human tutorial dialogues.I n t roduct ion  and  Mot ivat ionWe are building an explanation component foran existing intelligent raining system, SHERLOCK(Lesgold ei al., 1992), which trains avionics tech-nicians to troubleshoot electronic equipment.
Us-ing SHERLOCK, trainees olve problems with min-imal tutor interaction and then review their trou-bleshooting in a post-problem reflective follolz-up(RFU) session where the tutor replays each stu-dent action and assesses it as "good" (<+>)  oras "could be improved" (<->).
After a step is re-played, the student can ask the tutor to justify itsassessment.As an example of the way in which human tutorsexploit previous discourse, consider the dialoguein Figure 1, taken from our data.
Even thoughthe student has made the same mistake twice, thesecond explanation looks quite different from thefirst.
Yet the two explanations are related to oneanother in an important way.
In the second expla-nation the tutor simply reminds the student hatshe has not determined the status of the main con-trol data signals and that she should do so beforetesting the secondary control data signals.
Thetutor expects the student o be able to make useof the previous explanation once he has indicatedthat it is relevant to the current situation ("for thesame reasons given ..." serves this purpose).
Ac-cordingly, the tutor does not repeat the detailedexplanation of why the main control data signalsshould be tested first.
By generating the secondexplanation in such a way that it 'meshes' with thefirst, not only has the tutor corrected the testingmistake of the student, but has forced the studentto consider how the two situations are similar.
Inpointing out this similarity, he has given the stu-dent a better understanding of the domain.
Wecall an explanation that is later referred to (explic-itly or implicitly) or is integrated into a subsequentexplanation the anchor.Clearly it is desirable for a system to producetext that is sensitive to what has been said previ-ously.
In order to do this, however, a system mustfirst be able to decide what previous explanation(or part thereof) to use as an anchor.
This in-volves deciding, ia an e.~cient uJay, whether thereexist suitable candidates to act as anchor, and ifso, which amongst them would be best to use.
Thispaper concentrates on this task.The  Text  P lannerFor this work, we are extending the text plannerbuilt by Moore and Paris (1989).
Briefly, it worksin the following way.
A commuaicati~e goal (e.g.,"achieve the state where the hearer believes thatan action could be improved") is formed basedupon the student's question.
Using its library ofplan operators that encode knowledge about tu-torial explanations, the system employs a linearplanning mechanism to synthesize a response toachieve this goal.
The result is a tez~ plan for theexplanation.The system then presents the explanation to theuser, retaining the plan that produced it in a di-alogue history.
The dialogue history is a recordof the conversation that has occurred thus far andincludes the user's utterances as well as the textplans that led to the system's responses.
In thissystem, a text plan represents the effect that eachpart of the text is intended to have on the heaxer'smental state, the linguistic strategies that wereused to achieve these effects, and how the completetext achieves the overall communicative goal.277TUTORSTUDENTTUTORTUTORSTUDENTTUTOR<-> You tested pin 28 of the A1A3A15.
\[1\]Why is testing pin 28 not a good step?
\[2\]As explained before, the main data inputs to relay B24 are highly suspect at this time.
\[3\]You have tested the low input to B24 but not the high one.
Since it is more likely thatthere may be a problem with the main data signals to relay B24 than with the secondarydata signals to relays B25-B30, the main data signal should be tested first.
Testing pin28 is not a bad step, it is just more likely (on the average) that the main data signalsare causing the problem.
: One ezchange and one action later.<->You tested pin 36 of the A1A3A15.
\[4\]Don't I need to test pin 36?
\[5\]You might have to, but for the same reasons given when you tested pin 28, it is generally \[6\]more efficient to test the main control data signals first, and then test the secondarycontrol data signals if necessary.Figure 1: Human-Human Advisory Interaction Displaying Contextual EffectsKnowledge  Sources  fo r  F ind ingRe levant  P r io r  Exp lanat ionsThe most straightforward way to find relevantprior explanations i to exhaustively search thesystem's dialogue history looking for explanationsthat have certain features.
For example, when ex-pl~inlng why a step was assessed as "could be im-proved," the system could look for previous expla-nations that justified this type of assessment, andin which the two actions being assessed were sim-ilar (i.e., had the same features).However, this approach is problematic.
Expla-nation plans are large complex structures, andthey will accumulate rapidly as the dialogue pro-gresses.
Exhaustively searching the discourse his-tory for relevant prior explanations i computa-tionally prohibitive.
Thus, we require an indexingstrategy that allows the system to find possiblyrelevant prior explanations in an efficient manner.To satisfy this requirement, we use case-basedreasoning (CBR) to provide a framework in whichprevious tudent actions can be efficiently exam-ined to determine which, if any, are relevant whenproducing an explanation.
This approach as theadditional advantage of allowing the system toconsider what was said as well as what was notsaid when planning an explanation.
For example,the student may have previously performed an ac-tion that displayed some characteristic that the tu-tor decided not to mention at the time and whichwould now be appropriate to discuss.A Case-Based AlgorithmThe following aspect of SHERLOCK's reasoning isextremely important to our work.
SHERLOCK eval-uates each student action by determining whichfacets apply to that action.
The facets repre-sent factors that expert avionics tutors use in as-sessing student's troubleshooting actions (Pokornyand Gott, 1990).
To evaluate an action, SHER-LOCK finds each facet that applies to it and de-termines whether that facet should be consideredgood (g), bad (b), or neutral (n) given the currentproblem-solving context.
For example, the facet"Making a measurement that is off the active cir-cuit path" is considered a b-facet.
The representa-tion of a student action includes the list of facetscharacterizing the action and an assessment (g, b,or r~) for each of those facets.Case-based reasoning eneralizes from cases tosupport indexing and relevance assessment, andcan be used to evaluate a case by comparing it topast cases (Ashley, 1992).
This seems to describeour task when we treat each student action as a"case".
Influenced by the work of Aleven and Ash-ley (1992), we noted certain similarities betweentheir domain and ours that led us to believe thatwe could use CBR techniques to identify similar ac-tions as described below.Our algorithm builds a data structure called asimilarity DAG (Directed A__cyclic Graph) whichindicates the previous tudent actions that are sim-ilar to a given action.
By similar, we mean simi-lar with respect o a certain class of facets (somecombination ofg, b, or n).
For example, when an-swering a question about why the current actionwas assessed as "could be improved," the similar-ity DAG is built so that it indicates which previ-ous actions were similar to the current action withrespect o the b-facets.
The root of the DAG rep-resents the current action and the facets of interest(b-facets in our example) that apply to it.
Eachnode in the DAG, including the root, represents aset of student actions that share the same set ofinteresting facets.
The more facets that a node hasin common with the current action (in the root),the closer it will be to the root node.
Proximityin the DAG corresponds to similarity in facet sets.Basically, the similarity DAG is a partial orderingof the student's actions based on their facet lists.278Similarity DAGDiscourse I "'"HistoryFACETSFI00: Allowed main data signalrelay to remain partially tes~d (b)F101: Tested secondary data signalbefore main data signal (b)~ NAction 12: VDC test, pin 36 toground on A1A3A15 Co)PREVIOUS ACTIONSAction 9: VDC test,pin 28 to~round on A1A3A15(b)TEXT PLAN 1 / ~FACETSF~00: A11ow?d a moiv.data signal retay to remam parUally teste~ (b)\] PREVIOUS ACTIONS~>~ Action 8: VDC test, pin 38 toground on A1A3A15(b)/ /x/,,  ... "I'~'T PL~aN 2 "'" IFigure 2: Data structures when considering how to answer turn 5, Figure 1Figure 2 shows the similarity DAG that is con-structed when the system considers how to answerthe question, "Don't I need to test pin 36?"
(turn5 of Figure 1).
The facets relevant o the actionin question are F100 and F101.
The structureindicates that two previous actions - 9 and to alesser degree 8, are similar to the current situa-tion.
Pointers index the dialogue history's recordof what was said at those times.
At this point,the system has identified candidate situations thatare relevant for planning the current explanation.It can now consider these retrieved situations moreclosely to determine any other facets that they maypossess, and can examine the related explanationsin the dialogue history to determine what was saidabout each of the two previous ituations.
The factthat there are no other nodes in the DAG indicatesthat there are no other suitable prior situations.Initial results using this algorithm seem promis-ing.
In an analysis of 8 student-tutor protocolsinvolving 154 actions and 22 opportunities for in-tegrating a previous explanation into an answer,the algorithm correctly identified the same previ-ous situations that were used by the human tutorin the actual interactions.
In all but 3 cases, whenthe human tutor did not make a reference to a pre-vious explanation, our algorithm reported no sim-ilar prior situation.
In the 3 situations where ouralgorithm identified a similarity not exploited bythe tutor, our expert agreed that they would havebeen useful to incorporate into his explanations.Lastly, this technique will be useful in answeringstudents' direct questions about the similarities ofsituations, e.g., "Why is testing 30 good?
Isn't itlike 36 and 28?"
By constructing and consultinga similarity DAG, the system is able to plan re-sponses uch as: aYes, but now you know the maincontrol data signals on pins 33 and 22 are good soyou need to test the secondary data signals.
"It is important o note that this approach is suc-cessful, in part, because the facets are based on atutor's evaluation of a student's actions, and weare currently addressing only questions that jus-tify these evaluations.
We focused on this type ofquestion because 48% of student's queries duringRFU are of this type.
To answer additional ques-tions in a context-sensitive fashion, we will need toextend our indexing scheme to take the intentionsbehind an explanation into account as well as thedomain content discussed.Conclus ions and Future  WorkWe have indicated that in order to produce textthat is sensitive to the previous discourse, a sys-tem must f irst be able to identify relevant previousexplanations and situations.
To achieve this firststep, a CBR algorithm was introduced that indexesthe dialogue history and supplies the explanationswith a context in which to be considered.
We aredevising techniques that use this information toplan subsequent explanations.ReferencesAhven, V. and Ashley, K. 1992.
Auto-mated generation of examples for a tutorial incase-based argumentation.
In Proc.
of the ?ndInt'l Conference on Intelligent 2~toring S~ls-ter~, Montreal, Canada.Ashley, K. 1992.
Case-based reasoningand its implications for legal expert systems.
Ar-tificial Intelligence and Law 2(1).Lesgold, A.; Lajoie, S.; Bunzo, M.; andEggan, G. 1992.
Sherlock: A coached practiceenvironment for an electronics troubleshootingjob.
In Computer Assisted Instruction and In-telligent Tutoring S~/stems: Shared Goals andComplementary Approaches.
Lawrence ErlbaumAssoc,, NJ.Moore, J. D. and Paris, C. L. 1989.
Plan-ning text for advisory dialogues.
In Proc.
of the?7th Annual Meeting of the ACL, Vancouver,B.C., Canada.
203-211.Pokorny, R. and Gott, S. 1990.
The eval-uation of a real-world instructional system: Us-ing technical experts as raters.
Technical report,Armstrong Laboratories, Brooks AFB.279
