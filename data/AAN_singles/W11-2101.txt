Proceedings of the 6th Workshop on Statistical Machine Translation, pages 1?11,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsA Grain of Salt for the WMT Manual Evaluation?Ondr?ej Bojar, Milos?
Ercegovc?evic?, Martin PopelCharles University in PragueFaculty of Mathematics and PhysicsInstitute of Formal and Applied Linguistics{bojar,popel}@ufal.mff.cuni.czercegovcevic@hotmail.comOmar F. ZaidanDepartment of Computer ScienceJohns Hopkins Universityozaidan@cs.jhu.eduAbstractThe Workshop on Statistical MachineTranslation (WMT) has become one ofACL?s flagship workshops, held annuallysince 2006.
In addition to soliciting pa-pers from the research community, WMTalso features a shared translation task forevaluating MT systems.
This shared taskis notable for having manual evaluation asits cornerstone.
The Workshop?s overviewpaper, playing a descriptive and adminis-trative role, reports the main results of theevaluation without delving deep into ana-lyzing those results.
The aim of this paperis to investigate and explain some interest-ing idiosyncrasies in the reported results,which only become apparent when per-forming a more thorough analysis of thecollected annotations.
Our analysis shedssome light on how the reported resultsshould (and should not) be interpreted, andalso gives rise to some helpful recommen-dation for the organizers of WMT.1 IntroductionThe Workshop on Statistical Machine Translation(WMT) has become an annual feast for MT re-searchers.
Of particular interest is WMT?s sharedtranslation task, featuring a component for man-ual evaluation of MT systems.
The friendly com-petition is a source of inspiration for participatingteams, and the yearly overview paper (Callison-Burch et al, 2010) provides a concise report of thestate of the art.
However, the amount of interest-ing data collected every year (the system outputs?
This work has been supported by the grants EuroMa-trixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003 ofthe Czech Republic), P406/10/P259, MSM 0021620838, andDARPA GALE program under Contract No.
HR0011-06-2-0001.
We are grateful to our students, colleagues, and thethree reviewers for various observations and suggestions.and, most importantly, the annotator judgments)is quite large, exceeding what the WMT overviewpaper can afford to analyze with much depth.In this paper, we take a closer look at the datacollected in last year?s workshop, WMT101, anddelve a bit deeper into analyzing the manual judg-ments.
We focus mainly on the English-to-Czechtask, as it included a diverse portfolio of MT sys-tems, was a heavily judged language pair, and alsoillustrates interesting ?contradictions?
in the re-sults.
We try to explain such points of interest,and analyze what we believe to be the positive andnegative aspects of the currently established eval-uation procedure of WMT.Section 2 examines the primary style of man-ual evaluation: system ranking.
We discuss howthe interpretation of collected judgments, the com-putation of annotator agreement, and documentthat annotators?
individual preferences may rendertwo systems effectively incomparable.
Section 3is devoted to the impact of embedding referencetranslations, while Section 4 and Section 5 discusssome idiosyncrasies of other WMT shared tasksand manual evaluation in general.2 The System Ranking TaskAt the core of the WMT manual evaluation is thesystem ranking task.
In this task, the annotatoris presented with a source sentence, a referencetranslation, and the outputs of five systems overthat source sentence.
The instructions are keptminimal: the annotator is to rank the presentedtranslations from best to worst.
Ties are allowed,but the scale provides five rank labels, allowing theannotator to give a total order if desired.The five assigned rank labels are submitted atonce, making the 5-tuple a unit of annotation.
Inthe following, we will call this unit a block.
Theblocks differ from each other in the choice of the1http://www.statmt.org/wmt101Language Pair Systems Blocks Labels Comparisons Ref ?
others Intra-annot.
?
Inter-annot.
?German-English 26 1,050 5,231 10,424 0.965 0.607 0.492English-German 19 1,407 6,866 13,694 0.976 0.560 0.512Spanish-English 15 1,140 5,665 11,307 0.989 0.693 0.508English-Spanish 17 519 2,591 5,174 0.935 0.696 0.594French-English 25 837 4,156 8,294 0.981 0.722 0.452English-French 20 801 3,993 7,962 0.917 0.636 0.449Czech-English 13 543 2,691 5,375 0.976 0.700 0.504English-Czech 18 1,395 6,803 13,538 0.959 0.620 0.444Average 19 962 4,750 9,471 0.962 0.654 0.494Table 1: Statistics on the collected rankings, quality of references and kappas across language pairs.
Ingeneral, a block yields a set of five rank labels, which yields a set of(52)= 10 pairwise comparisons.Due to occasional omitted labels, the Comparisons/Blocks ratio is not exactly 10.source sentence and the choice of the five systemsbeing compared.
A couple of tricks are introducedin the sampling of the source sentences, to en-sure that a large enough number of judgments isrepeated across different screens for meaningfulcomputation of inter- and intra-annotator agree-ment.
As for the sampling of systems, it is doneuniformly ?
no effort is made to oversample or un-dersample a particular system (or a particular pairof systems together) at any point in time.In terms of the interface, the evaluation utilizesthe infrastructure of Amazon?s Mechanical Turk(MTurk)2, with each MTurk HIT3 containing threeblocks, corresponding to three consecutive sourcesentences.Table 1 provides a brief comparison of the vari-ous language pairs in terms of number of MT sys-tems compared (including the reference), numberof blocks ranked, the number of pairwise com-parisons extracted from the rankings (one blockwith 5 systems ranked gives 10 pairwise compar-isons, but occasional unranked systems are ex-cluded), the quality of the reference (the percent-age of comparisons where the reference was betteror equal than another system), and the ?
statistic,which is a measure of agreement (see Section 2.2for more details).4We see that English-to-Czech, the language pairon which we focus, is not far from the average inall those characteristics except for the number ofcollected comparisons (and blocks), making it thesecond most evaluated language pair.2http://www.mturk.com/3?HIT?
is an acronym for human intelligence task, whichis the MTurk term for a single screen presented to the anno-tator.4We only use the ?expert?
annotations of WMT10, ignor-ing the data collected from paid annotators on MTurk, sincethey were not part of the official evaluation.2.1 Interpreting the Rank LabelsThe description in the WMT overview paper says:?Relative ranking is our official evaluation met-ric.
[Systems] are ranked based on how frequentlythey were judged to be better than or equal toany other system.?
(Emphasis added.)
The WMToverview paper refers to this measure as ??
oth-ers?, with a variant of it called ?> others?
that doesnot reward ties.We first note that this description is somewhatambiguous, and an uninformed reader might in-terpret it in one of two different ways.
For somesystem A, each block in which A appears includesfour implicit pairwise comparisons (against theother presented systems).
How is A?s score com-puted from those comparisons?The correct interpretation is that A is re-warded once for each of the four comparisons inwhich A wins (or ties).5 In other words, A?s scoreis the number of pairwise comparisons in whichA wins (or ties), divided by the total number ofpairwise comparisons involving A.
We will use??
others?
(resp.
?> others?)
to refer to this inter-pretation, in keeping with the terminology of theoverview paper.The other interpretation is that A is rewardedonly if A wins (or ties) all four comparisons.
Inother words, A?s score is the number of blocks inwhichA wins (or ties) all comparisons, divided bythe number of blocks in which A appears.
We willuse ??
all in block?
(resp.
?> all in block?)
torefer to this interpretation.65Personal communication with WMT organizers.6There is yet a third interpretation, due to a literal read-ing of the description, where A is rewarded at most once perblock if it wins (or ties) any one of its four comparisons.
Thisis probably less useful: it might be good at identifying thebottom tier of systems, but would fail to distinguish betweenall other systems.2REF CU-BOJARCU-TECTOEUROTRANSONLINEBPC-TRANSUEDIN?
others 95.9 65.6 60.1 54.0 70.4 62.1 62.2> others 90.5 45.0 44.1 39.3 49.1 49.4 39.6?
all in block 93.1 32.3 30.7 23.4 37.5 32.5 28.1> all in block 81.3 13.6 19.0 13.3 15.6 18.7 10.6Table 2: Sentence-level ranking scores for theWMT10 English-Czech language pair.
The ??others?
and ?> others?
scores reproduced hereexactly match numbers published in the WMT10overview paper.
A boldfaced score marks the bestsystem in a given row (besides the reference).For quality control purposes, the WMT organiz-ers embed the reference translations as a ?system?alongside the actual entries (the idea being that anannotator clicking randomly would be easy to de-tect, since they would not consistently rank thereference ?system?
highly).
This means that thereference is as likely as any other system to ap-pear in a block, and when the score for a system Ais computed, pairwise comparisons with the refer-ence are included.We use the publicly released human judgments7to compute the scores of systems participating inthe English-Czech subtask, under both interpreta-tions.
Table 2 reports the scores, with our ??
oth-ers?
(resp.
?> others?)
scores reproduced exactlymatching those reported in Table 21 of the WMToverview paper.
(For clarity, Table 2 is abbreviatedto include only the top six systems of twelve.
)Our first suggestion is that both measures couldbe reported in future evaluations, since each tellsus something different.
The first interpretationgives partial credit for an MT system, hence distin-guishing systems from each other at a finer level.This is especially important for a language pairwith relatively few annotations, since ??
others?would produce a larger number of data points (fourper system per block) than ??
all in block?
(oneper system per block).
Another advantage of theofficial ??
others?
is greater robustness towardsvarious factors like the number of systems in thecompetition, the number of systems in one blockor the presence of the reference in the block (how-ever, see Section 3).As for the second interpretation, it helps iden-tify whether or not a single system (or a smallgroup of systems) is strongly dominant over theother systems.
For the systems listed in Table 2,7http://statmt.org/wmt10/results.html-10 0102030405060  1020304050607080>= All in Block>=OthersCzech-EnglishEnglish-CzechEnglish-FrenchEnglish-GermanEnglish-SpanishFrench-EnglishGerman-EnglishSpanish-Englisha*x+bFigure 1: ??
all in block?
and ??
others?
providevery similar ordering of systems.
?> all in block?
suggests its potential in the con-text of system combination: CU-TECTO and PC-TRANS win almost one fifth of the blocks in whichthey appear, despite the fact that either a refer-ence translation or a combination system alreadyappears alongside them.
(See also Table 4 below.
)Also, note that if the ranking task were designedspecifically to cater to the ??
all in block?
inter-pretation, it would only have two ?rank?
labels (ba-sically, ?top?
and ?non-top?).
In that case, an-notators would spend considerably less time perblock than they do now, since all they need to dois identify the top system(s) per block, without dis-tinguishing non-top systems from each other.Even for those interested in distinguishing non-state-of-the-art systems from each other, we pointout that the ??
all in block?
interpretation ulti-mately gives a system ordering that is very simi-lar to that of the official ??
others?
interpretation,even for the lower-tier systems (Figure 1).2.2 Annotator AgreementThe WMT10 overview paper reports inter- andintra-annotator agreement over the pairwise com-parisons, to show the validity of the evaluationsetup and the ??
others?
metric.
Agreement isquantified using the following formula:?
=P (A)?
P (E)1?
P (E)(1)where P (A) is the proportion of times two anno-tators are observed to agree, and P (E) is the ex-pected proportion of times two annotators wouldagree by chance.
Note that ?
has a value of at most1, with higher ?
values indicating higher rates ofagreement.
The ?
measure is more meaningful30.10.20.30.40.50.60.70.80.9 105101520253035KappaSource lengthIntra.
incl.
ref.Intra.
excl.
ref.Inter.
incl.
ref.Inter.
excl.
ref.ModerateagreementFigure 2: Intra-/inter-annotator agreementwith/without references, across various sourcesentence lengths (lengths of n and n + 1 are usedto plot the point at x = n).
This figure is based onall language pairs.than reporting P (A) as is, since it takes into ac-count, via P (E), how ?surprising?
it is for annota-tors to agree in the first place.In the context of pairwise comparisons, anagreement between two annotators occurs whenthey compare the same pair of systems (S1,S2),and both agree on their relative ranking: eitherS1 > S2, S1 = S2, or S1 < S2.
P (E) is then:P (E) = P 2(S1>S2)+P 2(S1=S2)+P 2(S1<S2) (2)In the WMT overview paper, all three cate-gories are assumed equally likely, giving P (E) =19 +19 +19 =13 .
For consistency with the WMToverview paper, and unless otherwise noted, wealso use P (E) = 13 whenever a ?
value is re-ported.
(Though see Section 2.2.2 for a discussionabout P (E).
)2.2.1 Observed Agreement for DifferentSentence LengthsIn Figure 2 we plot the ?
values across differentsource sentence lengths.
We see that the inter-annotator agreement (when excluding references)is reasonably high only for sentences up to 10words in length ?
according to Landis and Koch(1977), and as cited by the WMT overview paper,not even ?moderate?
agreement can be assumed if?
is less than 0.4.
Another popular (and controver-sial) rule of thumb (Krippendorff, 1980) is morestrict and says that ?
< 0.67 is not suitable evenfor tentative conclusions.For this reason, and given that a majority of sen-tences are indeed more than 10 words in length(the median is 20 words), we suggest that futureevaluations either include fewer outputs per block,or divide longer sentences into shorter segments(e.g.
on clause boundaries), so these segments aremore easily and reliably comparable.
The lattersuggestions assumes word alignment as a prepro-cessing and presenting the annotators the contextof the judged segment.2.2.2 Estimating P (E), the ExpectedAgreement by ChanceSeveral agreement measures (usually called kap-pas) were designed based on the Equation 1 (seeArtstein and Poesio (2008) and Eugenio and Glass(2004) for an overview and a discussion).
Thosemeasures differ from each other in how to de-fine the individual components of Equation 2, andhence differ in what the expected agreement bychance (P (E)) would be:8?
The S measure (Bennett et al, 1954) assumesa uniform distribution over the categories.?
Scott?s pi (Scott, 1955) estimates the distribu-tion empirically from actual annotation.?
Cohen?s ?
(Cohen, 1960) estimates the dis-tribution empirically as well, and further as-sumes a separate distribution for each anno-tator.Given that the WMT10 overview paper assumesthat the three categories (S1 > S2, S1 = S2, andS1 < S2) are equally likely, it is using the S mea-sure version of Equation 1, though it does not ex-plicitly say so ?
it simply calls it ?the kappa coef-ficient?
(K).Regardless of what the measure should becalled, we believe that the uniform distribution it-self is not appropriate, even though it seems tomodel a ?random clicker?
adequately.
In partic-ular, and given the design of the ranking inter-face, 13 is an overestimate of P (S1 = S2) fora random clicker, and should in fact be 15 : eachsystem receives one of five rank labels, and fortwo systems to receive the same rank label, thereare only five (out of 25) label pairs that satisfyS1 = S2.
Therefore, with P (S1 = S2) = 15 ,8These three measures were later generalized to more thantwo annotators (Fleiss, 1971; Bartko and Carpenter, 1976),Thus, without loss of generality, our examples involve twoannotators.4??
Others?
S piInter incl.
ref.
0.487 0.454excl.
ref.
0.439 0.403Intra incl.
ref.
0.633 0.609excl.
ref.
0.601 0.575Table 3: Summary of two variants of kappa: S(or K as it is reported in the WMT10 paper) andour proposed Scott?s pi.
We report inter- vs. intra-annotator agreement and collected from all com-parisons (?incl.
ref.?)
vs. collected only fromcomparisons without the reference (?excl.
ref.?
)because it is generally easier to agree that the ref-erence is better than the other systems.
This tableis based on all language pairs.we have P (S1 > S2) = P (S1 < S2) = 25 , andtherefore P (E) = 0.36 rather than 0.333.Taking the discussion a step further, we actuallyadvocate following the idea of Scott?s pi, wherebythe distribution of each category is estimated em-pirically from the actual annotation, rather thanassuming a random annotator ?
these frequenciesare easy to compute, and reflect a more meaning-ful P (E).9Under this interpretation, P (S1 = S2) is cal-culated to be 0.168, reflecting the fraction of pair-wise comparisons that correspond to a tie.
(Notethat this further supports the claim that settingP (S1 = S2) = 13 for a random clicker, as usedin the WMT overview paper, is an overestimate.
)This results in P (E) = 0.374, yielding, for in-stance, pi = 0.454 for ??
others?
inter-annotatoragreement, somewhat lower than ?
= 0.487 (re-ported in Table 3).We do note that the difference is rather small,and that our aim is to be mathematically soundabove all.
Carefully defining P (E) would be im-portant when comparing kappas across differenttasks with different P (E), or when attemptingto satisfy certain thresholds (as the cited 0.4 and0.67).
Furthermore, if one is interested in mea-suring agreement for individual annotators, suchas identifying those who have unacceptably lowintra-annotator agreement, the question of P (E) isquite important, since annotation behavior variesnoticeably from one annotator to another.
A ?con-servative?
annotator who prefers to rank systemsas being tied most of the time would have a high9We believe that P (E) should not reflect the chance thattwo random annotators would agree, but the chance that twoactual annotators would agree randomly.
The two sound sub-tly related but are actually quite different.P (E), whereas an annotator using ties moderatelywould have a low P (E).
Hence, two annotatorswith equal agreement rates (P (A)) are not neces-sarily equally proficient, since their P (E) mightdiffer considerably.102.3 The ?
variant vs. the > variantEven within the same interpretation of how sys-tems could be scored, there is a question ofwhether or not to reward ties.
The overview paperreports both variants of its measure, but does notnote that there are non-trivial differences betweenthe two orderings.
Compare for example the ??others?
ordering vs. the ?> others?
ordering ofCU-BOJAR and PC-TRANS (Table 2), showing anunexpected swing of 7.9%:?
others > othersCU-BOJAR 65.6 45.0PC-TRANS 62.1 49.4CU-BOJAR seems better under the?
variant, butloses out when only strict wins are rewarded.
The-oretically, this could be purely due to chance, butthe total number of pairwise comparisons in ??others?
is relatively large (about 1,500 pairwisecomparisons for each system), and ought to can-cel such effects.A similar pattern could be seen under the ?all inblock?
interpretation as well (e.g.
for CU-TECTOand ONLINEB).
Table 4 documents this effect bylooking at how often a system is the sole winnerof a block.
Comparing PC-TRANS and CU-BOJARagain, we see that PC-TRANS is up there with CU-TECTO and DCU-COMBO as the most frequent solewinners, winning 71 blocks, whereas CU-BOJARis the sole winner of only 53 blocks.
This is inspite of the fact that PC-TRANS actually appearedin slightly fewer blocks than CU-BOJAR (385 vs.401).One possible explanation is that the two vari-ants (???
and ?>?)
measure two subtly differentthings about MT systems.
Digging deeper into Ta-ble 2?s values, we find that CU-BOJAR is tied withanother system 65.6 ?
45.0 = 20.4% of the time,while PC-TRANS is tied with another system only62.1?
49.4 = 12.7% of the time.
So it seems thatPC-TRANS?s output is noticeably different fromanother system more frequently than CU-BOJAR,which reduces the number of times that annotators10Who?s more impressive: a psychic who correctly pre-dicts the result of a coin toss 50% of the time, or a psychicwho correctly predicts the result of a die roll 50% of the time?5Blocks Sole Winner305 Reference73 CU-TECTO71 PC-TRANS70 DCU-COMBO57 RWTH-COMBO54 ONLINEB53 CU-BOJAR46 EUROTRANS41 UEDIN41 UPV-COMBO175 One of eight other systems409 No sole winner1395 Total English-to-Czech BlocksTable 4: A breakdown of the 1,395 blocks for theEnglish-Czech task, according to which system (ifany) is the sole winner.
On average, a system ap-pears in 388 blocks.mark PC-TRANS as tied with another system.11 Inthat sense, the ???
ranking is hurting PC-TRANS,since it does not benefit from its small number ofties.
On the other hand, the ?>?
variant would notreward CU-BOJAR for its large number of ties.The ??
others?
score may be artificially boostedif several very similar systems (and thereforelikely to be ?tied?)
take part in the evaluation.12One possible solution is to completely disregardties and calculate the final score as winswins+losses .
Werecommend to use this score instead of ??
others?
( wins+tieswins+ties+losses ) which is biased toward often tiedsystems, and ?> others?
( winswins+ties+losses ) which isbiased toward systems with few ties.2.4 Surprise?
Does the Number ofEvaluations Affect a System?s Score?When examining the system scores for theEnglish-Czech task, we noticed a surprising pat-tern: it seemed that the more times a system issampled to be judged, the lower its ??
others?score (??
all in block?
behaving similarly).
Ascatter plot of a system?s score vs. the number ofblocks in which it appears (Figure 3) makes thepattern obvious.We immediately wondered if the pattern holdsin other language pairs.
We measured Pearson?scorrelation coefficient within each language pair,reported in Table 5.
As it turns out, English-11Indeed, PC-TRANS is a commercial system (manually)tuned over a long period of time and based on resources verydifferent from what other participants in WMT use.12In the preliminary WMT11 results, this seems to hap-pen to four Moses-like systems (UEDIN, CU-BOJAR, CU-MARECEK and CU-TAMCHYNA) which have better ??
oth-ers?
score but worse ?> others?
score than CU-TECTO.Correlation of Block CountSource Target vs.
??
Others?English Czech -0.558English Spanish -0.434Czech English -0.290Spanish English -0.240English French -0.227English German -0.161French English -0.024German English 0.146Overall -0.092Table 5: Pearson?s correlation between the num-ber of blocks where a system was ranked and thesystem?s ??
others?
score.
(The reference itself isnot included among the considered systems).3035404550556065707580  350360370380390400410420>= OthersNumber of judgmentscmu-heafield-combocu-bojarcu-tecto cu-zemandcudcu-comboeurotranskockoc-comboonlineAonlineBpc-transpotsdamrwth-combosfuuedinupv-comboa*x+bFigure 3: A plot of ??
others?
system score vs.times judged, for English-Czech.Czech happened to be the one language pair wherethe ?correlation?
is strongest, with only English-Spanish also having a somewhat strong correla-tion.
Overall, though, there is a consistent trendthat can be seen across the language pairs.
Couldit really be the case that the more often a system isjudged, the worse its score gets?Examining plots for the other language pairsmakes things a bit clearer.
Consider for examplethe plot for English-Spanish (Figure 4).
As onewould hope, the data points actually come togetherto form a cloud, indicating a lack of correlation.The reason that a hint of a correlation exists is thepresence of two outliers in the bottom right cor-ner.
In other words, the very worst systems are,indeed, the ones judged quite often.
We observedthis pattern in several other language pairs as well.The correlation naturally does not imply cau-sation.
We are still not sure how to explain theartifact.
A subtle possibility lies in the MTurkinterface: annotators have the choice to accept aHIT or skip it before actually providing their la-61020304050607080  130135140145150155160165170>= OthersNumber of judgmentscambridgecmu-heafield-combocu-zemandcu dfkijhukockoc-comboonlineAonlineBrwth-combosfuuedin upb-comboupvupv-nnlma*x+bFigure 4: A plot of ??
others?
system score vs.times judged, for English-Spanish.bels.
It might be the case that some annotators aremore willing to accept HITs when there is an ob-viously poor system (since that would make theirtask somewhat easier), and who are more proneto skipping HITs where the systems seem hard todistinguish from each other.
So there might be acausation effect after all, but in the reverse order:a system gets judged more often if it is a bad sys-tem.13 A suggestion from the reviewers is to run apilot annotation with deliberate inclusion of a poorsystem among the ranked ones.2.5 Issues of Pairwise JudgmentsThe WMT overview paper also provides pairwisesystem comparisons: each cell in Table 6 indicatesthe percentage of pairwise comparisons betweenthe two systems where the system in the columnwas ranked better (>) than the system in the row.For instance, there are 81 ranking responses whereboth CU-TECTO and CU-BOJAR were present andindeed ranked14 among the 5 systems in the block.In 37 (45.7%) of the cases, CU-TECTO was rankedbetter, in 29 (35.8%), CU-BOJAR was ranked betterand there was a tie in the remaining 15 (18.5%)cases.
The ties are not explicitly shown in Table 6but they are implied by the total of 100%.
The cellis in bold where there was a win in the pairwisecomparison, so 45.7 is bold in our example.An interesting ?discrepancy?
in Table 6 is thatCU-TECTO wins pairwise comparisons with CU-BOJAR and UEDIN but it scores worse than themin the official ??
others?, cf.
Table 2.
Simi-larly, UEDIN outperformed ONLINEB in the pair-13No pun intended!14The users sometimes did not fill any rank for a system.Such cases are ignored.REFCU-BOJARCU-TECTOEUROTRANSONLINEBPC-TRANSUEDINREF - 4.3 4.3 5.1 3.8 3.6 2.3CU-BOJAR 87.1 - 45.7 28.3 44.4 39.5 41.1CU-TECTO 88.2 35.8 - 38.0 55.8 44.0 36.0EUROTRANS 88.5 60.9 46.8 - 50.7 53.8 48.6ONLINEB 91.2 31.1 29.1 32.8 - 43.8 39.3PC-TRANS 88.0 45.3 42.9 28.6 49.3 - 36.6UEDIN 94.3 39.3 44.2 31.9 32.1 49.5 -Table 6: Pairwise comparisons extracted fromsentence-level rankings of the WMT10 English-Czech News Task.
Re-evaluated to reproduce thenumbers published in WMT10 overview paper.Bold in column A and row B means that systemA is pairwise better than system B.wise comparisons but it was ranked worse in both> and ?
official comparison.In the following, we focus on the CU-BOJAR(B) and CU-TECTO (T) pair because they are in-teresting competitors on their own.
They both usethe same parallel corpus for lexical mapping butoperate very differently: CU-BOJAR is based onMoses while CU-TECTO transfers at a deep syn-tactic layer and generates target text which is moreor less grammatically correct but suffers in lexicalchoice.2.5.1 Different Set of SentencesThe mismatch in the outcomes of ??
others?
andpairwise comparisons could be caused by differentset of sentences.
The pairwise ranking is collectedfrom the set of blocks where both CU-BOJAR andCU-TECTO appeared (and were indeed ranked).Each of the systems however competes in otherblocks as well, which contributes to the official ?
?others?.The set of sentences underlying the comparisonis very different and more importantly that the ba-sis for pairwise comparisons is much smaller thanthe basis of the official ??
others?
interpretation.The outcome of the official interpretation howeverdepends on the random set of systems your systemwas compared to.
In our case, it is impossible todistinguish, whether CU-TECTO had just bad luckon sentences and systems it was compared to whenCU-BOJAR was not in the block and/or whether the81 blocks do not provide a reliable picture.2.5.2 Pairwise Judgments UnreliableTo complement WMT10 rankings for the two sys-tems and avoid the possible lower reliability dueto 5-fold ranking instead of a targeted compari-7Author of B says:both bothB>T T>B fine wrong TotalTsays:B>T 9 - 1 1 11T>B 2 13 - 3 18both fine 2 - 2 3 7both wrong 10 5 1 11 27Total 23 18 4 18 63Table 7: Additional annotation of 63 CU-BOJAR(B) vs. CU-TECTO (T) sentences by two annota-tors.Better BothAnnotator B T fine wrongA 24 23 5 11C 10 12 5 36D 32 20 2 9M 11 18 7 27O 23 18 4 18Z 25 27 2 9Total 125 118 25 110Table 8: Blurry picture of pairwise rankings ofCU-BOJAR vs. CU-TECTO.
Wins in bold.son, we asked the main authors of both CU-BOJARand CU-TECTO to carry out a blind pairwise com-parison on the exact set of 63 sentences appearingacross the 81 blocks in which both systems wereranked.
As the totals in Table 7 would suggest,each author unwittingly recognized his system andslightly preferred it.
The details however reveal asubtler reason for the low agreement: one of theannotators was less picky about MT quality andaccepted 10+5 sentences completely rejected bythe other annotator.
In total, these two annotatorsagreed on 9 + 13 + 2 + 11 = 35 (56%) of casesand their pairwise ?
is 0.387.A further annotation of these 63 sentences byfour more people completes the blurry picture:the pairwise ?
for each pair of our five annota-tors ranges from 0.242 to 0.615 with the aver-age 0.407?0.106.
The multi-annotator ?
(Fleiss,1971) is 0.394 and all six annotators agree on asingle label only in 24% of cases.
The agree-ment is not better even if we merge the categories?Both fine?
and ?Both wrong?
into a single one:The pairwise ?
ranges from 0.212 to 0.620 withthe average 0.405?0.116, the multi-annotator ?
is0.391.
Individual annotations are given in Table 8.Naturally, the set of these 63 sentences is not arepresentative sample.
Even if one of the systemsSRC It?s not completely ideal.REF Nen??
to u?plne?
idea?ln??.
RanksPC-TRANS To nen??
u?plne?
idea?ln??.
2 5CU-BOJAR To nen??
u?plne?
idea?ln??.
5 4Table 9: Two rankings by the same annotator.SRC FCC awarded a tunnel in Slovenia for 64 millionREF FCC byl pr?ide?len tunel ve Slovinsku za 64 milionu?Gloss FCC was awarded a tunnel in Slovenia for 64 millionHYP1 FCC pr?ide?lil tunel ve Slovinsku za 64 milio?nu?HYP2 FCC pr?ide?lila tunel ve Slovinsku za 64 milionu?Gloss FCC awardedmasc/fem a tunnel in Slovenia for 64 millionFigure 5: A poor reference translation confuseshuman judges.
The SRC and REF differ in the ac-tive/passive form, attributing completely differentroles to ?FCC?.actually won, such an observation could not havebeen generalized to other test sets.
The purposeof the exercise was to check whether we are at allable to agree which of the systems translates thisspecific set of sentences better.
As it turns out,even a simple pairwise ranking can fail to pro-vide an answer because different annotators sim-ply have different preferences.Finally, Table 9 illustrates how poor theWMT10 rankings can be.
The exact same stringproduced by two systems was ranked differentlyeach time ?
by the same annotator.
(The hypothe-sis is a plausible translation, only the informationstructure of the sentence is slightly distorted so thetranslation may not fit well it the surrounding con-text.
)3 The Impact of the ReferenceTranslation3.1 Bad Reference TranslationsFigure 5 illustrates the impact of poor referencetranslation on manual ranking as carried out inSection 2.5.2.
Of our six independent annotations,three annotators marked the hypotheses as ?bothfine?
given the match with the source and threeannotators marked them as ?both wrong?
due tothe mismatch with the reference.
Given the con-struction of the WMT test set, this particular sen-tence comes from a Spanish original and it wasmost likely translated directly to both English andCzech.8Correlation ofSource Target Reference vs.
??
others?Spanish English 0.341English French 0.164French English 0.098German English 0.088Czech English -0.041English Czech -0.145English Spanish -0.411English German -0.433Overall -0.107Table 10: Pearson?s correlation of the relative per-centage of blocks where the reference was in-cluded in the ranking and the final ??
others?of the system (the reference itself is not includedamong the considered systems).2530354045505560657075  0.190.20.210.220.230.240.250.26>= OthersRelativepresenceof the referencecmu-heafield-combocu-zemandfkifbkjhukitkockoc-combolimsiliuonlineAonlineBrwthrwth-combo sfuuedinuppsalaupv-comboa*x+bFigure 6: Correlation of the presence of the ref-erence and the official ??
others?
for English-German evaluation.3.2 Reference Can Skew PairwiseComparisonsThe exact set of competing systems in each 5-foldranking in WMT10 evaluation is random.
The ??others?
however is affected by this: a system maysuffer more losses if often compared to the refer-ence, and similarly it may benefit from being com-pared to a poor competitor.To check this, we calculate the correlation be-tween the relative presence of the reference amongthe blocks where a system was judged and thesystem?s official ??
others?
score.
Across lan-guage, there is almost no correlation (Pearson?scoefficient: ?0.107).
However, for some languagepairs, the correlation is apparent, as listed in Ta-ble 10.
Negative correlation means: the more of-ten the system was compared along with the refer-ence, the worse the score of the system.Figure 6 plots the extreme case of English-German evaluation.Source Target Min Avg?StdDev MaxEnglish Czech 40 65?19 115English French 40 66?17 110English German 10 40?16 80English Spanish 30 54?15 85Czech English 5 38?13 60French English 5 37?15 70German English 10 32?12 65Spanish English 35 56?11 70Table 11: The number of post-edits per system foreach language pair to complement Figure 3 (page12) of the WMT10 overview paper.4 Other WMT10 Tasks4.1 Blind Post-Editing UnreliableWMT often carries out one more type of manualevaluation: ?Editing the output of systems withoutdisplaying the source or a reference translation,and then later judging whether edited translationswere correct.?
(Callison-Burch et al, 2010).
Wecall the evaluation ?blind post-editing?
for short.We feel that blind post-editing is more infor-mative than system ranking.
First, it constitutesa unique comprehensibility test, and after all, MTshould aim at comprehensible output in the firstplace.
Second, blind post-editing can be furtheranalyzed to search for specific errors in systemoutput, see Bojar (2011) for a preliminary study.Unfortunately, the amount of post-edits col-lected in WMT10 varied a lot across systems andlanguage pairs.
Table 11 provides the minimum,average and maximum number of post-edits ofoutputs of a particular MT system.
We see thate.g.
while English-to-Czech has many judgmentsof this kind per system, Czech-to-English is one ofthe worst supported directions.It is not surprising that conclusions based on 5observations can be extremely deceiving.
For in-stance CU-BOJAR seems to produce 60% of out-puts comprehensible (and thus wins in Figure 3 onpage 12 in the WMT overview paper), far betterthan CMU.
This is not in line with the ranking re-sults where both rank equally (Table 5 on page 10in the WMT overview paper).
In fact, CU-BOJARwas post-edited 5 times and 3 of these post-editswere acceptable while CMU was post-edited 30times and 5 of these post-edits were acceptable.4.2 A Remark on System Combination TaskOne results of WMT10 not observed in previousyears was that system combinations indeed per-formed better than individual systems.
Previous9Dev Set Test SetSententes 455 2034 DiffGOOGLE 17.32?1.25 16.76?0.60 ?BOJAR 16.00?1.15 16.90?0.61 ?TECTOMT 11.48?1.04 13.19?0.58 ?PC-TRANS 10.24?0.92 10.84?0.46 ?EUROTRAN 9.64?0.92 11.04?0.48 ?Table 12: BLEU scores of sample five systems inEnglish-to-Czech combination task.years failed to show this clearly, because GoogleTranslate used to be included among the combinedsystems, making it hard to improve.
In WMT10,Google Translate was excluded from system com-bination task (except for translations involvingCzech, where it was accidentally included).Our Table 12 provides an additional explanationwhy the presence of Google among combined sys-tems leads to inconclusive results.
While the testset was easier (based on BLEU) than the develop-ment set for most systems, it was much harder forGoogle.
All system combinations were thus likelyto overfit and select Google n-grams most often.Without access to Google powerful language mod-els, the combination systems were likely to under-perform Google in final fluency of the output.5 Further Issues of Manual EvaluationWe have already seen that the comprehensibilitytest by blind post-editing provides a different pic-ture of the systems than the official ranking.
Berkaet al (2011) introduced a third ?quiz-based evalu-ation?.
The quiz-like evaluation used the English-to-Czech WMT10 systems, applied to differenttexts: short text snippets were translated and an-notators were asked to answer three yes/no ques-tions complementing each snippet.
The order ofthe systems was rather different from the officialWMT10 results: CU-TECTO won the quiz-basedevaluation despite being the fourth in WMT10.Because the texts were different in WMT10 andthe quiz-based evaluation, we asked a small groupof annotators to apply the ranking technique on thetext snippets.
While not exactly comparable to theWMT10 ranking, the WMT10 ranking was con-firmed: CU-TECTO was again among the lowest-scoring systems and Google won the ranking.Bojar (2011) applies the error-flagging manualevaluation by Vilar et al (2006) to four systemsof WMT09 English-to-Czech task.
Again, theoverall order of the systems is somewhat differ-ent when ranked by the number of errors flagged.Mireia Farru?s and Fonollosa (2010) use a coarserbut linguistically motivated error classification forCatalan-Spanish and suggest that differences inranking are caused by annotators treating sometypes of errors as more serious.In short, different types of manual evaluationslead to different results even when identical sys-tems and texts are evaluated.6 ConclusionWe took a deeper look at the results of the WMT10manual evaluation, and based on our observations,we have some recommendations for future evalu-ations:?
We propose to use a score which ignoresties instead of the official ??
others?
metricwhich rewards ties and ?> others?
which pe-nalizes ties.
Another score, ??
all in block?,could help identify which systems are moredominant.?
Inter-annotator agreement decreases dramat-ically with sentence length; we recommendincluding fewer sentences per block, at leastfor longer sentences.?
We suggest agreement be measured based onan empirical estimate of P (E), or at least us-ing a more correct random clicking P (E) =0.36.?
There is evidence of a negative correlationbetween the number of times a system isjudged and its score; we recommend a deeperanalysis of this issue.?
We recommend the reference be sampled ata lower rate than other systems, so as to playa smaller role in the evaluation.
We also rec-ommend better quality control over the pro-duction of the references.And to the readers of the WMT overview paper,we point out:?
Pairwise comparisons derived from 5-foldrankings are sometimes unreliable.
Even atargeted pairwise comparison of two systemscan shed little light as to which is superior.?
The acceptability of post-edits is sometimesvery unreliable due to the low number of ob-servations.10ReferencesR.
Artstein and M. Poesio.
2008.
Inter-coder agree-ment for computational linguistics.
ComputationalLinguistics, 34(4):555?596.John J. Bartko and William T. Carpenter.
1976.
On themethods and theory of reliability.
Journal of Ner-vous and Mental Disease, 163(5):307?317.E.
M. Bennett, R. Alpert, and A. C. Goldstein.
1954.Communications through limited questioning.
Pub-lic Opinion Quarterly, 18(3):303?308.Jan Berka, Martin C?erny?, and Ondr?ej Bojar.
2011.Quiz-Based Evaluation of Machine Translation.Prague Bulletin of Mathematical Linguistics, 95:77?86, March.Ondr?ej Bojar.
2011.
Analyzing Error Types inEnglish-Czech Machine Translation.
Prague Bul-letin of Mathematical Linguistics, 95:63?76, March.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Przybocki, and Omar Zaidan.2010.
Findings of the 2010 joint workshop on sta-tistical machine translation and metrics for machinetranslation.
In Proceedings of the Joint Fifth Work-shop on Statistical Machine Translation and Metric-sMATR, pages 17?53, Uppsala, Sweden, July.
Asso-ciation for Computational Linguistics.Jacob Cohen.
1960.
A coefficient of agreementfor nominal scales.
Educational and PsychologicalMeasurement, 20(1):37?46.Barbara Di Eugenio and Michael Glass.
2004.
Thekappa statistic: A second look.
Computational lin-guistics, 30(1):95?101.J.
L. Fleiss.
1971.
Measuring nominal scale agree-ment among many raters.
Psychological Bulletin,76(5):378?382.Klaus Krippendorff.
1980.
Content Analysis: An In-troduction to Its Methodology.
Sage Publications,Beverly Hills, CA.
Chapter 12.J.
Richard Landis and Gary G. Koch.
1977.
The mea-surement of observer agreement for categorical data.Biometrics, 33:159?174.Jose?
B. Marin?o Mireia Farru?s, Marta R. Costa-jussa`and Jose?
A. R. Fonollosa.
2010.
Linguistic-basedevaluation criteria to identify statistical machinetranslation errors.
In Proceedings of the 14th AnnualConference of the Euoropean Association for Ma-chine Translation (EAMT?10), pages 167?173, May.William A. Scott.
1955.
Reliability of content analy-sis: The case of nominal scale coding.
Public Opin-ion Quarterly, 19(3):321?325.David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-mann Ney.
2006.
Error Analysis of Machine Trans-lation Output.
In International Conference on Lan-guage Resources and Evaluation, pages 697?702,Genoa, Italy, May.11
