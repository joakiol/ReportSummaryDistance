Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing, pages 42?50,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsLearning Simple Wikipedia:A Cogitation in Ascertaining Abecedarian LanguageCourtney Napoles and Mark DredzeCenter for Language and Speech ProcessingHuman Language Technology Center of ExcellenceJohns Hopkins UniversityBaltimore, MD 21211courtneyn@jhu.edu, mdredze@cs.jhu.eduAbstractText simplification is the process of changingvocabulary and grammatical structure to cre-ate a more accessible version of the text whilemaintaining the underlying information andcontent.
Automated tools for text simplifica-tion are a practical way to make large corporaof text accessible to a wider audience lackinghigh levels of fluency in the corpus language.In this work, we investigate the potential ofSimple Wikipedia to assist automatic text sim-plification by building a statistical classifica-tion system that discriminates simple Englishfrom ordinary English.
Most text simplifica-tion systems are based on hand-written rules(e.g., PEST (Carroll et al, 1999) and its mod-ule SYSTAR (Canning et al, 2000)), andtherefore face limitations scaling and trans-ferring across domains.
The potential for us-ing Simple Wikipedia for text simplificationis significant; it contains nearly 60,000 ar-ticles with revision histories and aligned ar-ticles to ordinary English Wikipedia.
Us-ing articles from Simple Wikipedia and ordi-nary Wikipedia, we evaluated different classi-fiers and feature sets to identify the most dis-criminative features of simple English for useacross domains.
These findings help furtherunderstanding of what makes text simple andcan be applied as a tool to help writers craftsimple text.1 IntroductionThe availability of large collections of electronictexts is a boon to information seekers, however, ad-vanced texts often require fluency in the language.Text simplification (TS) is an emerging area of text-to-text generation that focuses on increasing thereadability of a given text.
Potential applicationscan increase the accessibility of text, which has greatvalue in education, public health, and safety, and canaid natural language processing tasks such as ma-chine translation and text generation.Corresponding to these applications, TS can bebroken down into two rough categories dependingon the target ?reader.?
The first type of TS aims toincrease human readability for people lacking high-level language skills, either because of age, educa-tion level, unfamiliarity with the language, or dis-ability.
Historically, generating this text has beendone by hand, which is time consuming and expen-sive, especially when dealing with material that re-quires expertise, such as legal documents.
Most cur-rent automatic TS systems rely on handwritten rules,e.g., PEST (Carroll et al, 1999), its SYSTAR mod-ule (Canning et al, 2000), and the method describedby Siddharthan (2006).
Systems using handwrittenrules can be susceptible to changes in domains andneed to be modified for each new domain or lan-guage.
There has been some research into automat-ically learning the rules for simplifying text usingaligned corpora (Daelemans et al, 2004; Yatskar etal., 2010), but these have yet to match the perfor-mance hand-crafted rule systems.
An example ofa manually simplified sentence can be found in ta-ble 1.The second type of TS has the goal of increas-ing the machine readability of text to aid tasks suchas information extraction, machine translation, gen-erative summarization, and other text generation42tasks for selecting and evaluating the best candi-date output text.
In machine translation, the eval-uation tool most commonly used for evaluating out-put, the BLEU score (Papineni et al, 2001), rates the?goodness?
of output based on n-gram overlap withhuman-generated text.
However this metric has beencriticized for not accurately measuring the fluencyof text and there is active research into other met-rics (Callison-Burch et al, 2006; Ye et al, 2007).Previous studies suggest that text simplified for ma-chine and human comprehension are categoricallydifferent (Chae and Nenkova, 2009).
Our researchconsiders text simplified for human readers, but thefindings can be used to identify features that dis-criminate simple text for both applications.The process of TS can be divided into three as-pects: removing extraneous or superfluous text, sub-stituting more complex lexical and syntactic forms,and inserting information to offer further clarifica-tion where needed (Alu?
?sio et al, 2008).
In this re-gard, TS is related to several different natural lan-guage processing tasks such as text summarization,compression, machine translation, and paraphras-ing.While none of these tasks alone directly providea solution to text simplification, techniques can bedrawn from each.
Summarization techniques canbe used to identify the crucial, most informativeparts of a text and compression can be used to re-move superfluous words and phrases.
In fact, in theWikipedia documents analyzed for this research, theaverage length of a ?simple?
document is only 21%the length of an ?ordinary?
English document (al-though this may be an unintentional byproduct ofhow articles were simplified, as discussed in section6.1).In this paper we study the properties of languagethat differentiate simple from ordinary text for hu-man readers.
Specifically, we use statistical learn-ing techniques to identify the most discriminativefeatures of simple English and ?ordinary?
Englishusing articles from Simple Wikipedia and EnglishWikipedia.
We use cognitively motivated featuresas well as statistical measurements of a document?slexical, syntactic, and surface features.
Our studydemonstrates the validity and potential benefits ofusing Simple Wikipedia as a resource for TS re-search.Ordinary textEvery person has the right to a name, in which isincluded a first name and surname.
.
.
.
The aliaschosen for legal activities has the same protectionas given to the name.Same text in simple languageEvery person has the right to have a name, andthe law protects people?s names.
Also, the lawprotects a person?s alias.
.
.
.
The name is madeup of a first name and a surname (name = firstname + surname).Table 1: A text in ordinary and simple language fromAlu?
?sio et al (2008).2 Wikipedia as a CorpusWikipedia is a unique resource for natural lan-guage processing tasks due to its sheer size, acces-sibility, language diversity, article structure, inter-document links, and inter-language document align-ments.
Denoyer and Gallinari (2006) introducedthe Wikipedia XML Corpus, with 1.5 million doc-uments in eight languages from Wikipedia, thatstored the rich structural information of Wikipediawith XML.
This corpus was designed specificallyfor XML retrieval but has uses in natural languageprocessing, categorization, machine translation, en-tity ranking, etc.
YAWN (Schenkel et al, 2007), aWikipedia XML corpus with semantic tags, is an-other example of exploiting Wikipedia?s structuralinformation.
Wikipedia provides XML site dumpsevery few weeks in all languages as well as staticHTML dumps.A diverse array of NLP research in the pastfew years has used Wikipedia, such as for wordsense disambiguation (Mihalcea, 2007), classifica-tion (Gantner and Schmidt-Thieme, 2009), machinetranslation (Smith et al, 2010), coreference resolu-tion (Versley et al, 2008; Yang and Su, 2007), sen-tence extraction for summarization (Biadsy et al,2008), information retrieval (Mu?ller and Gurevych,2008), and semantic role labeling (Ponzetto andStrube, 2006), to name a few.
However, except forvery recent work by Yatskar et al (2010), to ourknowledge there has not been comparable researchin using Wikipedia for text simplification.43Ordinary WikipediaHawking was the Lucasian Professor of Mathe-matics at the University of Cambridge for thirtyyears, taking up the post in 1979 and retiring on 1October 2009.Simple WikipediaHawking was a professor of mathematics at theUniversity of Cambridge (a position that IsaacNewton once had).
He retired on October 1st2009.Table 2: Comparable sentences from the ordinaryWikipedia and Simple Wikipedia entry for ?StephenHawking.
?What makes Wikipedia an excellent resource fortext simplification is the new Simple Wikipediaproject1, a collection of 58,000 English Wikipediaarticles that have been rewritten in Simple English,which uses basic vocabulary and less complex gram-mar to make the content of Wikipedia accessible tostudents, children, adults with learning difficulties,and non-native English speakers.
In addition to be-ing a large corpus, these articles are linked to theirordinary Wikipedia counterparts, so for each articleboth a simple and an ordinary version are available.Furthermore, on inspection many articles in SimpleWikipedia appear to be copied and edited from thecorresponding ordinary Wikipedia article.
This in-formation, together with revision history and flagssignifying unsimplified text, can provide a scale ofinformation on the text-simplification process previ-ously unavailable.
Example sentences from SimpleWikipedia and ordinary Wikipedia are shown in ta-ble 2.We used articles from Simple Wikipedia and or-dinary English Wikipedia to create a large cor-pus of simple and ordinary articles for our exper-iments.
In order to experiment with models thatwork across domains, the corpus includes articlesfrom nine of the primary categories identified inSimple Wikipedia: Everyday Life, Geography, His-tory, Knowledge, Literature, Media, People, Reli-gion, and Science.
A total of 55,433 ordinary and42,973 simple articles were extracted and processedfrom English Wikipedia and Simple Wikipedia, re-1http://simple.wikipedia.org/Coarse Tag Penn Treebank TagsDET DT, PDTADJ JJ, JJR, JJSN NN, NNS, NP, NPS, PRP, FWADV RB, RBR, RBSV VB, VBN, VBG, VBP, VBZ, MDWH WDT, WP, WP$, WRBTable 3: A mapping of the Penn Treebank tags to a coarsetagset used to generate features.spectively.
Each document contains at least two sen-tences.
Additionally, the corpus contains only themain text body of each article and does not con-sider info boxes, tables, lists, external and cross-references, and other structural features.
The exper-iments that follow randomly extract documents andsentences from this collection.Before extracting features, we ran a series of nat-ural language processing tools to preprocess the col-lection.
First, all of the XML and ?wiki markup?was removed.
Each document was split into sen-tences using the Punkt sentence tokenizer (Kiss andStrunk, 2006) in NLTK (Bird and Loper, 2004).
Wethen parsed each sentence using the PCFG parserof Huang and Harper (2009), a modified versionof the Berkeley parser (Petrov et al, 2006; Petrovand Klein, 2007), for the tree structure and part-of-speech tags.3 Task SetupTo evaluate the feasibility of learning simple and or-dinary texts, we sought to identify text propertiesthat differentiated between these classes.
Using thetwo document collections, we constructed a simplebinary classification task: label a piece of text as ei-ther simple or ordinary.
The text was labeled ac-cording to its source: simple or ordinary Wikipedia.From each piece of text, we extracted a set of fea-tures designed to capture differences between thetexts, using cognitively motivated features based ona document?s lexical, syntactic, and surface features.We first describe our features and then our experi-mental setup.444 FeaturesWe began by examining the guidelines for writingSimple Wikipedia pages.2 These guidelines suggestthat articles use only the 1000 most common and ba-sic English words and contain simple grammar andshort sentences.
Articles should be short but can belonger if they need to explain vocabulary words nec-essary to understand the topic.
Additionally, wordsshould appear on lists of basic English words, suchas the Voice of America Special English words list(Voice Of America, 2009) or the Ogden Basic En-glish list (Ogden, 1930).
Idioms should be avoidedas well as compounds and the passive voice as op-posed to a single simple verb.To capture these properties in the text, we createdfour classes of features: lexical, part-of-speech, sur-face, and parse.
Several of our features have previ-ously been used for measuring text fluency (Alu?
?sioet al, 2008; Chae and Nenkova, 2009; Feng et al,2009; Petersen and Ostendorf, 2007).Lexical.
Previous work by Feng et al (2009) sug-gests that the document vocabulary is a good predic-tor of document readability.
Simple texts are morelikely to use basic words more often as opposed tomore complicated, domain-specific words used inordinary texts.
To capture these features we used aunigram bag-of-words representation.
We note thatlexical features are unlikely to be useful unless wehave access to a large training corpus that allowedthe estimation of the relative frequency of words(Chae and Nenkova, 2009).
Additionally, we canexpect lexical features to be very fragile for cross-domain experiments as they are especially suscepti-ble to changes in domain vocabulary.
Nevertheless,we include these features as a baseline in our exper-iments.Parts of speech.
A clear focus of the simple textguidelines is grammar and word type.
One wayof representing this information is by measuringthe relative frequency of different types of partsof speech.
We consider simple unigram part-of-speech tag information.
We measured the nor-malized counts and relative frequency of part-of-speech tags and counts of bigram part-of-speech tags2http://simple.wikipedia.org/wiki/Wikipedia:Simple_English_WikipediaFeature Simple OrdinaryTokens 158 4332Types 100 1446Sentences 10 172Average sentence length 15.80 25.19Type-token ratio 0.63 0.33Percent simple words 0.31 0.08Not BE850 type-token ratio 0.65 0.30BE850 type-token ratio 0.59 0.67Table 4: A comparison of the article ?Stephen Hawking?from Simple and ordinary Wikipedia.in each piece of text.
Since Devlin and Unthank(2006) has shown that word order (subject verb ob-ject (SVO), object verb subject (OVS), etc.)
is cor-related with readability, we also included a reducedtagset to capture grammatical patterns (table 3).
Wealso included normalized counts of these reducedtags in the model.Surface features.
While lexical items may be im-portant, more general properties can be extractedfrom the lexical forms.
We can also include fea-tures that correspond to surface information in thetext.
These features include document length, sen-tence length, word length, numbers of lexical typesand tokens, and the ratio of types to tokens.
Allwords are labeled as basic or not basic accordingto Ogden?s Basic English 850 (BE850) list (Ogden,1930).3 In order to measure the lexical complexityof a document, we include features for the numberof BE850 words, the ratio of BE850 words to totalwords, and the type-token ratio of BE850 and non-BE850 words.
Investigating the frequency and pro-ductivity of words not in the BE850 list will hope-fully improve the flexibility of our model to workacross domains and not learn any particular jargon.We also hope that the relative frequency and pro-ductivity measures of simple and non-simple wordswill codify the lexical choices of a sentence whileavoiding the aforementioned problems with includ-ing specific lexical items.3Wikipedia advocates using words that appear on the BE850list.
Ogden also provides extended Basic English vocabularylists, totaling 2000 Basic English words, but these words tendto be more specialized or domain specific.
For the purposes ofthis study only words in BE850 were used.45Table 4 shows the difference in some surfacestatistics in an aligned document from Simple andordinary Wikipedia.
In this example, nearly one-third of the words in the simple document are fromthe BE850 while less than a tenth of the words in theordinary document are.
Additionally, the productiv-ity of words, particularly non-BE850 words, is muchhigher in the ordinary document.
There are alsoclear differences in the length of the documents, andon average documents from ordinary Wikipedia aremore than four times longer than documents fromSimple Wikipedia.Syntactic parse.
As previously mentioned, anumber of Wikipedia?s writing guidelines focus ongeneral grammatical rules of sentence structure.
Ev-idence of these rules may be captured in the syn-tactic parse of the sentences in the text.
Chae andNenkova (2009) studied text fluency in the contextof machine translation and found strong correlationsbetween parse tree structures and sentence fluency.In order to represent the structural complexity ofthe text, we collected extracted features from theparse trees.
Our features included the frequency andlength of noun phrases, verb phrases, prepositionalphrases, and relative clauses (including embeddedstructures).
We also considered relative ratios, suchas the ratio of noun to verb phrases, prepositional tonoun phrases, and relative clauses to noun phrases.We used the length of the longest noun phrase asa signal of complexity, and we also sought featuresthat measured how typical the sentences were of En-glish text.
We included some of the features fromthe parser reranking work of Charniak and Johnson(2005): the height of the parse tree and the numberof right branches from the root of the tree to the fur-thest right leaf that is not punctuation.5 ExperimentsUsing the feature sets described above, we evalu-ated a simple/ordinary text classifier in several set-tings on each category.
First, we considered the taskof document classification, where a classifier deter-mines whether a full Wikipedia article was fromordinary English Wikipedia or Simple Wikipedia.For each category of articles, we measured accu-racy on this binary classification task using 10-foldcross-validation.
In the second setting, we consid-Category Documents SentencesEveryday Life 15,124 7,392Geography 10,470 5,852History 5,174 1,644Literature 992 438Media 502 429People 4,326 1,562Religion 1,863 1,581Science 25,787 21,054All 64,238 39,952Table 5: The number of examples available in each cate-gory.
To compare experiments in each category we usedat most 2000 instances in each experiment.Feature class FeaturesLexical 522,153Part of speech 2478tags 45tag pairs 1972tags (reduced) 22tag pairs (reduced) 484Parse 11Surface 9Table 6: The number of features in each feature class.ered the performance of a sentence-level classifier.The classifier labeled each sentence as either ordi-nary or simple and we report results using 10-foldcross-validation on a random split of the sentences.For both settings we also evaluated a single classifiertrained on all categories.We next considered cross-category performance:how would a classifier trained to detect differencesbetween simple and ordinary examples from onecategory do when tested on another category.
Inthis experiment, we trained a single classifier on datafrom a single category and used the classifier to labelexamples from each of the other categories.
We re-port the accuracy on each category in these transferexperiments.For learning we require a binary classifier train-ing algorithm.
We evaluated several learning algo-rithms for classification and report results for eachone: a) MIRA?a large margin online learning al-gorithm (Crammer et al, 2006).
Online learningalgorithms observe examples sequentially and up-46date the current hypothesis after each observation; b)Confidence Weighted (CW) learning?a probabilis-tic large margin online learning algorithm (Dredze etal., 2008); c) Maximum Entropy?a log-linear dis-criminative classifier (Berger et al, 1996); and d)Support Vector Machines (SVM)?a large margindiscriminator (Joachims, 1998).For each experiment, we used default settings ofthe parameters and 10 online iterations for the onlinemethods (MIRA, CW).
To create a fair comparisonfor each category, we limited the number of exam-ples to a maximum of 2000.6 ResultsFor the first task of document classification, we sawat least 90% mean accuracy with each of the clas-sifiers.
Using all features, SVM and Maximum En-tropy performed almost perfectly.
The online clas-sifiers, CW and MIRA, displayed similar preferenceto the larger feature sets, lexical and part-of-speechcounts.
When using just lexical counts, both CWand MIRA were more accurate than the SVM andMaximum Entropy (reporting 92.95% and 86.55%versus 75.00% and 78.75%, respectively).
For allclassifiers, the models using the counts of part-of-speech tags did better than classifiers trained on thesurface features and on the parse features.
This issurprising, since we expected the surface features tobe robust predictors of the document class, mainlybecause the average ordinary Wikipedia article inour corpus is about four times longer than the av-erage Simple Wikipedia article.
We also expectedthe syntactic features to be a strong predictor of thedocument class since more complicated parse treescorrespond to more complex sentences.For each classifier, we looked at its performancewithout its less predictive feature categories, andfor CW the inclusion of the surface features de-creased performance noticeably.
The best CWclassifiers used either part-of-speech and lexicalfeatures (95.95%) or just part-of-speech features(95.80%).
The parse features, which by themselvesonly yielded 64.60% accuracy, when combined withpart-of-speech and lexical features showed high ac-curacy as well (95.60%).
MIRA also showed higheraccuracy when surface features were not included(from 97.50% mean accuracy with all features to97.75% with all but surface features).The best SVM classifier used all four featureclasses, but had nearly as good accuracy with justpart-of-speech counts and surface features (99.85%mean accuracy) and with surface and parse features(also 99.85% accuracy).
Maximum Entropy, onthe other hand, improved slightly when the lexicaland parse features were not included (from 99.45%mean accuracy with all feature classes to 99.55%).We examined the weights learned by the classi-fiers to determine the features that were effective forlearning.
We selected the features with the highestabsolute weight for a MIRA classifier trained on allcategories.
The most predictive features for docu-ment classification were the sentence length (shorterfavors Simple), the length of the longest NP (longerfavors ordinary), the number of sentences (more fa-vors ordinary), the average number of prepositionalphrases and noun phrases per sentence, the heightof the parse tree, and the number of adjectives.
Themost predictive features for sentence classificationwere the ratio of different tree non-terminals (VP, S,NP, S-Bar) to the number of words in the sentence,the ratio of the total height of the productions in atree to the height of the tree, and the extent to whichthe tree was right branching.
These features are con-sistent with the rules described above for simple text.Next we looked at a pairwise comparison of howthe classifiers perform when trained on one categoryand tested on another.
Surprisingly, the results wererobust across categories, across classifiers.
Usingthe best feature class as determined in the first task,the average drop in accuracy when trained on eachdomain was very low across all classifiers (the meanaccuracy rate of each cross-category classificationwas at least 90%).
Table 6 shows the mean change inaccuracy from CW models trained and tested on thesame category to the models trained and tested ondifferent categories.
When trained on the EverydayLife category, the model actually showed a mean in-crease in accuracy when predicting other categories.In the final task, we trained binary classifiers toidentify simple sentences in isolation.
The meanaccuracy was lower for this task than for the doc-ument classification task, and we anticipated indi-vidual sentences to be more difficult to classify be-cause each sentence only carries a fraction of the47Classifier All features Lexical POS Surface ParseCW 86.40% 92.95% 95.80% 69.80% 64.60%MIRA 97.50% 86.55% 94.55% 79.65% 66.90%MaxEnt 99.45% 78.75% 96.25% 86.90% 80.70%SVM 99.90% 75.00% 96.60% 89.75% 82.70%Table 7: Mean accuracy of all classifiers on the document classification task.Classifier All features POS Surface ParseCW 73.20% 74.45% 57.40% 62.25%MIRA 71.15% 72.65% 56.50% 56.45%MaxEnt 80.80% 77.65% 71.30% 69.00%SVM 77.00% 76.40% 72.55% 73.00%Table 8: Mean accuracy of all classifiers on the sentence classification task.Category Mean accuracy changeEveryday life +1.42%Geography ?4.29%History ?1.01%Literature ?1.84%Media ?0.56%People ?0.20%Religion ?0.56%Science ?2.50%Table 9: Mean accuracy drop for a CW model trained onone category and tested on all other categories.
Negativenumbers indicate a decrease in performance.information held in an entire document.
It is com-mon to have short, simple sentences as part of ordi-nary English text, although they will not make up thewhole.
However results were still promising, withbetween 72% and 80% mean accuracy.
With CWand MIRA, the classifiers benefited from training onall categories, while MaxEnt and SVM in-categoryand all-category models achieved similar accuracylevels, but the results on cross-category tests weremore variable than in the document classification.There was also no consistency across features andclassifiers with regard to category-to-category clas-sification.
Overall the results of the sentence classi-fication task are encouraging and show promise fordetecting individual simple sentences taken out ofcontext.6.1 DiscussionThe classifiers performed robustly for the document-level classification task, although the corpus itselfmay have biased the model due to the longer aver-age length of ordinary documents, which we triedto address by filtering out articles with only oneor two sentences.
Cursory inspection suggests thatthere is overlap between many Simple Wikipedia ar-ticles and their corresponding ordinary English arti-cles, since a large number of Simple Wikipedia doc-uments appear to be generated directly from the En-glish Wikipedia articles with more complicated sub-sections of the documents omitted from the Simplearticle.The sentence classification task could be im-proved by better labeling of sentences.
In these ex-periments, we assumed that every sentence in an or-dinary document would be ordinary (i.e., not simple)and vice versa for simple documents.
However it isnot the case that ordinary English text contains onlycomplicated sentences.
In future research we canuse human annotated sentences for building the clas-sifiers.
The features we used in this research suggestthat simple text is created from categorical lexicaland syntactic replacement, but more complicated,technical, or detailed oriented text may require morerewriting, and would be of more interest in futureresearch.7 Conclusion and Future WorkWe have demonstrated the ability to automaticallyidentify texts as either simple or ordinary at both48the document and sentence levels using a variety offeatures based on the word usage and grammaticalstructures in text.
Our statistical analysis has identi-fied relevant features for this task accessible to com-putational systems.
Immediate applications of theclassifiers created in this research for text simplifi-cation include editing tools that can identify parts ofa text that may be difficult to understand or for wordprocessors, in order to notify writers of complicatedsentences in real time.Using this initial exploration of SimpleWikipedia, we plan to continue working in anumber of directions.
First, we will explore ad-ditional robust indications of text difficulty.
Forexample, Alu?
?sio et al (2008) claim that sentencesthat are easier to read are also easier to parse, sothe entropy of the parser or confidence in the outputmay be indicative of a text?s difficulty.
Additionally,language models trained on large corpora can assignprobability scores to texts, which may indicatetext difficulty.
Of particular interest are syntacticlanguage models that incorporate some of thesyntactic observations in this paper (Filimonov andHarper, 2009).Our next goal will be to look at parallel sentencesto learn rules for simplifying text.
One of the ad-vantages of the Wikipedia collection is the parallelarticles in ordinary English Wikipedia and SimpleWikipedia.
While the content of the articles can dif-fer, these are excellent examples of comparable textsthat can be useful for learning simplification rules.Such learning can draw from machine translation,which learns rules that translate between languages.The related task of paraphrase extraction could alsoprovide comparable phrases, one of which can beidentified as a simplified version of the other (Ban-nard and Callison-Burch, 2005).
An additional re-source available in Simple Wikipedia is the flaggingof articles as not simple.
By examining the revisionhistory of articles whose flags have been changed,we can discover changes that simplified texts.
Initialwork on this topic has automatically learned whichedits correspond to text simplifications (Yatskar etal., 2010).Text simplification may necessitate the removal ofwhole phrases, sentences, or even paragraphs, as, ac-cording to the writing guidelines for Wikipedia Sim-ple (Wikipedia, 2009), the articles should not exceeda specified length, and some concepts may not beexplainable using the lexicon of Basic English.
Insome situations, adding new text to explain confus-ing but crucial points may serve to aid the reader,and text generation needs to be further investigatedto make text simplification an automatic process.AcknowledgementsThe authors would like to thank Mary Harper for herhelp in parsing our corpus.ReferencesS.M.
Alu?
?sio, L. Specia, T.A.S.
Pardo, E.G.
Maziero, andR.P.M.
Fortes.
2008.
Brazilian portuguese automatictext simplification systems.
In DocEng.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Associa-tion for Computational Linguistics (ACL).A.L.
Berger, V.J.D.
Pietra, and S.A.D.
Pietra.
1996.
Amaximum entropy approach to natural language pro-cessing.
Computational linguistics, 22(1):39?71.F.
Biadsy, J. Hirschberg, E. Filatova, and LLC InforS-ense.
2008.
An unsupervised approach to biographyproduction using Wikipedia.
In Association for Com-putational Linguistics (ACL).S.
Bird and E. Loper.
2004.
NLTK: The natural lan-guage toolkit.
Proceedings of the ACL demonstrationsession, pages 214?217.C.
Callison-Burch, M. Osborne, and P. Koehn.
2006.
Re-evaluating the role of BLEU in machine translation re-search.
In European Conference for ComputationalLinguistics (EACL), volume 2006, pages 249?256.Y.
Canning, J. Tait, J. Archibald, and R. Crawley.
2000.Cohesive generation of syntactically simplified news-paper text.
Lecture notes in computer science, pages145?150.J.
Carroll, G. Minnen, D. Pearce, Y. Canning, S. Devlin,and J. Tait.
1999.
Simplifying text for language-impaired readers.
In European Conference for Com-putational Linguistics (EACL), pages 269?270.J.
Chae and A. Nenkova.
2009.
Predicting the fluencyof text with shallow structural features.
In EuropeanConference for Computational Linguistics (EACL),pages 139?147.E.
Charniak andM.
Johnson.
2005.
Coarse-to-fine n-bestparsing and MaxEnt discriminative reranking.
In As-sociation for Computational Linguistics (ACL), page180.
Association for Computational Linguistics.49Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
Journal of Machine LearningResearch (JMLR).W.
Daelemans, A. Ho?thker, and E Tjong Kim Sang.2004.
Automatic sentence simplification for subtitlingin Dutch and English.
In Conference on Language Re-sources and Evaluation (LREC), pages 1045?1048.Ludovic Denoyer and Patrick Gallinari.
2006.
TheWikipedia XML Corpus.
SIGIR Forum.S.
Devlin and G. Unthank.
2006.
Helping aphasic peo-ple process online information.
In SIGACCESS Con-ference on Computers and Accessibility.Mark Dredze, Koby Crammer, and Fernando Pereira.2008.
Confidence-weighted linear classification.In International Conference on Machine Learning(ICML).L.
Feng, N. Elhadad, and M. Huenerfauth.
2009.
Cog-nitively motivated features for readability assessment.In European Conference for Computational Linguis-tics (EACL).Denis Filimonov and Mary Harper.
2009.
A jointlanguage model with fine-grain syntactic tags.
InEmpirical Methods in Natural Language Processing(EMNLP).Z.
Gantner and L. Schmidt-Thieme.
2009.
Automaticcontent-based categorization of Wikipedia articles.
InAssociation for Computational Linguistics (ACL).Z.
Huang and M. Harper.
2009.
Self-training pcfggrammars with latent annotations across languages.
InEmpirical Methods in Natural Language Processing(EMNLP).T.
Joachims.
1998.
Text categorization with supportvector machines: Learning with many relevant fea-tures.
In European Conference on Machine Learning(ECML).T.
Kiss and J. Strunk.
2006.
Unsupervised multilingualsentence boundary detection.
Computational Linguis-tics, 32(4):485?525.R.
Mihalcea.
2007.
Using Wikipedia for automaticword sense disambiguation.
In North American Chap-ter of the Association for Computational Linguistics(NAACL).C.
Mu?ller and I. Gurevych.
2008.
Using Wikipediaand Wiktionary in domain-specific information re-trieval.
In Working Notes of the Annual CLEF Meet-ing.
Springer.C.K.
Ogden.
1930.
Basic English: A General Introduc-tion with Rules and Grammar.
Paul Treber & Co., Ltd.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
2001.BLEU: a method for automatic evaluation of machinetranslation.
In Association for Computational Linguis-tics (ACL).S.E.
Petersen and M. Ostendorf.
2007.
Text simplifi-cation for language learners: A corpus analysis.
InThe Speech and Language Technology for EducationWorkshop, pages 69?72.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In North American Chap-ter of the Association for Computational Linguistics(NAACL).Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Association for Computa-tional Linguistics (ACL).S.P.
Ponzetto and M. Strube.
2006.
Exploiting semanticrole labeling, WordNet and Wikipedia for coreferenceresolution.
In North American Chapter of the Associ-ation for Computational Linguistics (NAACL).R.
Schenkel, F. Suchanek, and G. Kasneci.
2007.YAWN: A semantically annotated Wikipedia XMLcorpus.
In Proceedings of GI-Fachtagung fu?rDatenbanksysteme in Business, Technologie und Web(BTW2007).A.
Siddharthan.
2006.
Syntactic simplification and textcohesion.
Research on Language & Computation,4(1):77?109.Jason Smith, Chris Quirk, and Kristina Toutanova.
2010.Extracting parallel sentences from comparable corporausing document level alignment.
In North AmericanChapter of the Association for Computational Linguis-tics (NAACL).Y.
Versley, S.P.
Ponzetto, M. Poesio, V. Eidelman,A.
Jern, J. Smith, X. Yang, and A. Moschitti.
2008.BART: A modular toolkit for coreference resolution.In Association for Computational Linguistics (ACL)Demo Session.Voice Of America.
2009.
Word book, 2009 edition.www.voaspecialenglish.com, February.Wikipedia.
2009.
Simple Wikipedia English.http://en.wikipedia.org/wiki/Citing Wikipedia, Octo-ber.X.
Yang and J. Su.
2007.
Coreference resolution usingsemantic relatedness information from automaticallydiscovered patterns.
In Association for ComputationalLinguistics (ACL).Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil, and Lillian Lee.
2010.
For the sake of simplic-ity: Experiments with unsupervised extraction of lexi-cal simplifications.
In North American Chapter of theAssociation for Computational Linguistics (NAACL).Y.
Ye, M. Zhou, and C.Y.
Lin.
2007.
Sentence level ma-chine translation evaluation as a ranking problem: onestep aside from BLEU.
In ACL Workshop on statisti-cal machine translation.50
