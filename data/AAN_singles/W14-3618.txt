Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 137?142,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsCMUQ@QALB-2014: An SMT-based Systemfor Automatic Arabic Error CorrectionSerena Jeblee1, Houda Bouamor2, Wajdi Zaghouani2and Kemal Oflazer21Carnegie Mellon Universitysjeblee@cs.cmu.edu2Carnegie Mellon University in Qatar{hbouamor,wajdiz}@qatar.cmu.edu, ko@cs.cmu.eduAbstractIn this paper, we describe the CMUQ sys-tem we submitted to The ANLP-QALB 2014Shared Task on Automatic Text Correctionfor Arabic.
Our system combines rule-basedlinguistic techniques with statistical languagemodeling techniques and machine translation-based methods.
Our system outperforms thebaseline and reaches an F-score of 65.42% onthe test set of QALB corpus.
This ranks us 3rdin the competition.1 IntroductionThe business of text creation and editing represents alarge market where NLP technologies might be appliednaturally (Dale, 1997).
Today?s users of word proces-sors get surprisingly little help in checking spelling,and a small number of them use more sophisticatedtools such as grammar checkers, to provide help in en-suring that a text remains grammatically accurate aftermodification.
For instance, in the Arabic version of Mi-crosoft Word, the spelling checker for Arabic, does notgive reasonable and natural proposals for many real-word errors and even for simple probable errors (Had-dad and Yaseen, 2007).With the increased usage of computers in the pro-cessing of natural languages comes the need for cor-recting errors introduced at different stages.
Natu-ral language errors are not only made by human op-erators at the input stage but also by NLP systemsthat produce natural language output.
Machine trans-lation (MT), or optical character recognition (OCR),often produce incorrect output riddled with odd lexi-cal choices, grammar errors, or incorrectly recognizedcharacters.
Correcting human/machine-produced er-rors, or post-editing, can be manual or automated.
Formorphologically and syntactically complex languages,such as Modern Standard Arabic (MSA), correctingtexts automatically requires complex human and ma-chine processing which makes generation of correctcandidates a challenging task.For instance, the Automatic Arabic Text CorrectionShared Task is an interesting testbed to develop andevaluate spelling correction systems for Arabic trainedeither on naturally occurring errors in texts written byhumans (e.g., non-native speakers), or machines (e.g.,MT output).
In such tasks, participants are asked toimplement a system that takes as input Modern Stan-dard Arabic texts with various spelling errors and au-tomatically correct them.
In this paper, we describethe CMUQ system we developed to participate in theThe First Shared Task on Automatic Text Correctionfor Arabic (Mohit et al., 2014).
Our system combinesrule-based linguistic techniques with statistical lan-guage modeling techniques and machine translation-based methods.
Our system outperforms the baseline,achieves a better correction quality and reaches an F-score of 62.96% on the development set of QALB cor-pus (Zaghouani et al., 2014) and 65.42% on the test set.The remainder of this paper is organized as follows.First, we review the main previous efforts for automaticspelling correction, in Section 2.
In Section 3, we de-scribe our system, which consists of several modules.We continue with our experiments on the shared task2014 dev set (Section 4).
Then, we give an analysis ofour system output in Section 5.
Finally, we concludeand hint towards future improvement of the system, inSection 6.2 Related WorkAutomatic error detection and correction include auto-matic spelling checking, grammar checking and post-editing.
Numerous approaches (both supervised andunsupervised) have been explored to improve the flu-ency of the text and reduce the percentage of out-of-vocabulary words using NLP tools, resources, andheuristics, e.g., morphological analyzers, languagemodels, and edit-distance measure (Kukich, 1992;Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalanet al., 2003; Haddad and Yaseen, 2007; Hassan et al.,2008; Habash, 2008; Shaalan et al., 2010).
There hasbeen a lot of work on error correction for English (e.g.,(Golding and Roth, 1999)).
Other approaches learnmodels of correction by training on paired examplesof errors and their corrections, which is the main goalof this work.For Arabic, this issue was studied in various direc-tions and in different research work.
In 2003, Shaalanet al.
(2003) presented work on the specification andclassification of spelling errors in Arabic.
Later on,Haddad and Yaseen (2007) presented a hybrid ap-proach using morphological features and rules to fine137tune the word recognition and non-word correctionmethod.
In order to build an Arabic spelling checker,Attia et al.
(2012) developed semi-automatically, a dic-tionary of 9 million fully inflected Arabic words us-ing a morphological transducer and a large corpus.They then created an error model by analyzing errortypes and by creating an edit distance ranker.
Finally,they analyzed the level of noise in different sources ofdata and selected the optimal subset to train their sys-tem.
Alkanhal et al.
(2012) presented a stochastic ap-proach for spelling correction of Arabic text.
They useda context-based system to automatically correct mis-spelled words.
First of all, a list is generated with pos-sible alternatives for each misspelled word using theDamerau-Levenshtein edit distance, then the right al-ternative for each misspelled word is selected stochas-tically using a lattice search, and an n-gram method.Shaalan et al.
(2012) trained a Noisy Channel Modelon word-based unigrams to detect and correct spellingerrors.
Dahlmeier and Ng (2012a) built specialized de-coders for English grammatical error correction.
Morerecently, (Pasha et al., 2014) created MADAMIRA,a system for morphological analysis and disambigua-tion of Arabic, this system can be used to improve theaccuracy of spelling checking system especially withHamza spelling correction.In contrast to the approaches described above, weuse a machine translation (MT) based method to trainan error correction system.
To the best of our knowl-edge, this is the first error correction system for Arabicusing an MT approach.3 Our SystemOur system is a pipeline that consists of several dif-ferent modules.
The baseline system uses a spellingchecking module, and the final system uses a phrase-based statistical machine translation system.
Topreproces the text, we use the provided output ofMADAMIRA (Pasha et al., 2014) and a rule-basedcorrection.
We then do a rule-based post-processingto fix the punctuation.3.1 Baseline SystemsFor the baseline system, we try a common spellingchecking approach.
We first pre-process the data us-ing the features from MADAMIRA (see Feature 14Replacement), then we use a noisy channel model forspelling checking.Feature 14 ReplacementThe first step in the pipeline is to extractMADAMIRA?s 14th feature from the .column fileand replace each word in the input text with this form.MADAMIRA uses morphological disambiguation andSVM analysis to select the most likely fully diacritizedArabic word for the input word.
The 14th featurerepresents the undiacritized form of the most likelyword.
This step corrects many Hamza placement oromission errors, which makes a good base for othercorrection modules.Spelling CorrectionThe spelling checker is based on a noisy channel model- we use a word list and language model to determinethe most probable correct Arabic word that could havegenerated the incorrect form that we have in the text.For detecting spelling errors we use the AraComLexword list for spelling checking (Attia et al., 2012),which contains about 9 million Arabic words.1Welook up the word from the input sentence in this list,and attempt to correct those that are not found in thelist.
We also train a mapping of incorrect words andpossible corrections from the edits in the training data.If the word is in this map, the list of possible correc-tions from the training data becomes the candidate list.If the word is not in the trained map, the candidate listis created by generating a list of words with commoninsertions, substitutions, and deletions, according to thelist in (Attia et al., 2012).
Each candidate is generatedby performing these edits and has a weight according tothe edit distance weights in the list.
We then prune thecandidate list by keeping only the lowest weight words,and removing candidates that are not found in the wordlist.
The resulting sentence is scored with a 3-gram lan-guage model built with KenLM (Heafield et al., 2013)on the correct side of the training data.
The top onesentence is then kept and considerd as the ?corrected?one.This module handles spelling errors of individualwords; it does not handle split/merge errors or wordreordering.
The spelling checker sometimes attemptsto correct words that were already correct, becausethe list does not contain named entities or translitera-tions, and it does not contain all possible correct Arabicwords.
Because the spelling checker module decreasedthe overall performance, it is not included in our finalsystem.3.2 Final SystemFeature 14 ReplacementThe first step in our final system is Feature 14 Replace-ment, as described above.Rule-based Clitic CorrectionWith the resulting data, we apply a set of rules to reat-tach clitics that may have been split apart from the baseword.
After examining the train dataset, we realizedthat 95% of word merging cases involve ???
attach-ment.
When found by themselves, the clitics are at-tached to either the previous word or next word, basedon whether they generally appear as prefixes or suf-fixes.
The clitics handled by this module are specifiedin Table 2.We also remove extra characters by replacing a se-quence of 3 or more of the same character with a single1http://sourceforge.net/projects/arabic-wordlist/138DevExact Match No PunctPrecision Recall F1 Precision Recall F1Feature 14 0.7746 0.3210 0.4539 0.8100 0.5190 0.6326Feature 14 + Spelling checker (baseline) 0.4241 0.3458 0.3810 0.4057 0.4765 0.4382Feature 14 + Clitic Rules 0.7884 0.3642 0.4983 0.8149 0.5894 0.6841Feature 14 + Phrase-based MT 0.7296 0.5043 0.5964 0.7797 0.6397 0.7028Feature 14 + Clitic Rules + Phrase-based MT 0.7571 0.5389 0.6296 0.8220 0.6850 0.7473TestFeature 14 + Clitic Rules + Phrase-based MT 0.7797 0.5635 0.6542 0.7438 0.6855 0.7135Table 1: System results on the dev set (upper part) and on the test set (lower part).Attach clitic to... CliticsBeginning of next word {?, ?
@, H.,?, ?
}End of previous word {?, A?, AK, ?G, ?, ?
?, @}Table 2: Clitics handled by the rule-based module.instance of that character (e.g.
!!!!!!!
would be replacedwith !
).Statistical Phrase-based ModelWe use the Moses toolkit (Koehn et al., 2007) tocreate a statistical phrase-based machine translationmodel built on the best pre-processed data, as describedabove.
We treat this last step as a translation prob-lem, where the source language is pre-processed in-correct Arabic text, and the reference is correct Ara-bic.
Feature 14 extraction, rule-based correction, andcharacter de-duplication are applied to both the trainand dev sets.
All but the last 1,000 sentences of thetrain data are used at the training set for the phrase-based model, the last 1,000 sentences of the train dataare used as a tuning set, and the dev set is used fortesting and evaluation.
We use fast align, the alignerincluded with the cdec decoder (Dyer et al., 2010) asthe word aligner with grow-diag as the symmetrizationheuristic (Och and Ney, 2003), and build a 5-gram lan-guage model from the correct Arabic training data withKenLM (Heafield et al., 2013).
The system is evaluatedwith BLEU (Papineni et al., 2002) and then scored forprecision, recall, and F1 measure against the dev setreference.We tested several different reordering window sizessince this is not a standard translation task, so we maywant shorter distance reordering.
Although 7 is the de-fault size, we tested 7, 5, 4, 3, and 0, and found that awindow of size 4 produces the best result according toBLEU score and F1 measure.4 Experiments and ResultsWe train and evaluate our system with the train-ing and development datasets provided for the sharedtask and the m2Scorer (Dahlmeier and Ng, 2012b).These datasets are extracted from the QALB corpusof human-edited Arabic text produced by native speak-ers, non-native speakers and machines (Zaghouani etal., 2014).We conducted a small scale statistical study on the950K tokens training set used to build our system.
Werealized that 306K tokens are affected by a correctionaction which could be a word edit, insertion, deletion,split or merge.
169K tokens were edited to correct thespelling errors and 99K tokens were inserted (mostlypunctuation marks).
Furthermore, there is a total of6,7K non necessary tokens deleted and 10.6K attachedtokens split and 18.2 tokens merged.
Finally, there areonly 427 tokens moved in the sentence and 1563 mul-tiple correction action.We experiment with different configurations andreach the sweet spot of performance when combiningthe different modules.4.1 ResultsTo evaluate the performance of our system on the de-velopment data, we compare its output to the reference(gold annotation).
We then compute the usual mea-sures of precision, recall and f-measure.
Results forvarious system configurations on the dev and test setsare given in Table 1.
Using the baseline system con-sisting in replacing words by their non diacritized form(Feature 14), we could correct 51.9% of the errors oc-curring in the dev set, when punctuation is not consid-ered.
This result drops when we consider the punctua-tion errors which seem to be more complex to correct:Only 32.1% of the errors are corrected in the dev set.
Itis important to notice that adding the clitic rules to theFeature 14 baseline yields an improvement of + 5.15 inF-measure.
We reach the best F-measure value whenusing the phrase-based MT system after pre-processingthe data and applying the Feature 14 and clitic rules.Using this combination we were able to correct 68.5%of the errors (excluding punctuation) on the develop-ment set with a precision of 82.2% and 74.38% on thetest set.
When we consider the punctuation, 53.89%of the errors of different types were corrected on thedev set and 56.35% on the test set with a precision of75.71% and 77.97%, respectively.1395 Error Analysis and DiscussionWhen building error correction systems, minimizingthe number of cases where correct words are markedas incorrect is often regarded as more important thancovering a high number of errors.
Therefore, a higherprecision is often preferred over higher recall.
In orderto understand what was affecting the performance, wetook a closer look at our system output and translationtables to present some samples of errors that our systemmakes on development set.5.1 Out-of-vocabulary WordsThis category includes words that are not seen by oursystem during the training which is a common problemin machine translation systems.
In our system, most ofout-of-vocabulary words were directly transferred un-changed from source to target.
For example the word?J???????
@ was not corrected to?J??????
@.5.2 Unnecessary EditsIn some cases, our system made some superfluous editssuch as adding the definite article in cases where it isnot required such as :Source?JKY??@?AJ?@Hypothesis?JKY??
@?AJ?B@Reference (unchanged)?JKY??@?AJ?
@Table 3: An example of an unnecessary addition of thedefinite article.5.3 Number NormalizationWe observed that in some cases, the system did not nor-malize the numbers such as in the following case whichrequires some knowledge of the real context to under-stand that these numbers require normalization.SourceH@?A?J?
450000HypothesisH@?A?J?
450000ReferenceH@?A?J?
450Table 4: An example of number normalization.5.4 Hamza SpellingEven though our system corrected most of the Hamzaspelling errors, we noticed that in certain cases theywere not corrected, especially when the words withoutthe Hamza were valid entries in the dictionary.
Thesecases are not always easy to handle since only contextand semantic rules can handle them.5.5 Grammatical ErrorsIn our error analysis we encountered many cases of un-corrected grammatical errors.
The most frequent typeSource?JJ???
@ X@?Hypothesis?JJ???
@ X@?Reference?JJ???
@ X@?Table 5: A sentence where the Hamza was not addedabove the Alif in the first word because both versionsare valid dictionary entries.is the case endings correction such as correcting theverbs in jussive mode when there is a prohibition par-ticle (negative imperative) like the (B) in the followingexamples :Source ??EXAK@???
@?K.Q?
?BHypothesis ??EXAK@???
@?K.Q?
?BReference ??EXAK@?????K.Q?
?BTable 6: An example of a grammatical error.5.6 Unnecessary Word DeletionAccording to the QALB annotation guidelines, ex-tra words causing semantic ambiguity in the sentenceshould be deleted.
The decision to delete a given wordis usually based on the meaning and the understandingof the human annotator, unfortunately this kind of er-rors is very hard to process and our system was not ableto delete most of the unnecessary words.Source Qk@ AJ??
A?
?EYK@ A???
Y?D?J?
?
?Hypothesis Qk@ AJ??
A?
?EYK@ A???
Y?D?J?
?
?Reference Qk@ AJ??
A???
Y?D?J?
?
?Table 7: An example of word deletion.5.7 Adding Extra WordsOur analysis revealed cases of extra words introducedto some sentences, despite the fact that the words addedare coherent with the context and could even improvethe overall readability of the sentence, they are uncred-ited correction since they are not included in the goldstandard.
For example :Source ?P???@?m.?'@?????H.Q?
?Hypothesis Qm?
'@ ?P???@?m.?'@?????H.Q?
?Reference ?P???@?m.?'@?????H.Q?
?Table 8: An example of the addition of extra words.5.8 Merge and Split ErrorsIn this category, we show some sample errors of neces-sary word splits and merge not done by our system.
The140word Y?K.A??
?k should have been split as Y?K.A??
?kand the word YK.B should have been merged to appearas one word as in YK.B.5.9 Dialectal Correction ErrorsDialectal words are usually converted to their ModernStandard Arabic (MSA) equivalent in the QALB cor-pus, since dialectal words are rare, our system is unableto detect and translate the dialectal words to the MSAas in the expression?KP I.?
that is translated in thegold standard to?KPQ?.6 ConclusionWe presented our CMUQ system for automatic Ara-bic text correction.
Our system combines rule-basedlinguistic techniques with statistical language model-ing techniques and a phrase-based machine transla-tion method.
We experiment with different configu-rations.
Our experiments have shown that the systemwe submitted outperforms the baseline and we reachan F-score of 74.73% on the development set fromthe QALB corpus when punctuation is excluded, and65.42% on the test set when we consider the punctu-ation errors .
This placed us in the 3rd rank.
We be-lieve that our system could be improved in numerousways.
In the future, we plan to finalize a current mod-ule that we are developing to deal with merge and spliterrors in a more specific way.
We also want to focus ina deeper way on the word movement as well as punc-tuation problems, which can produce a more accuratesystem.
We will focus as well on learning further errorcorrection models from Arabic Wikipedia revision his-tory, as it contains natural rewritings including spellingcorrections and other local text transformations.AcknowledgementsThis publication was made possible by grants NPRP-09-1140-1-177 and NPRP-4-1058- 1-168 from theQatar National Research Fund (a member of the QatarFoundation).
The statements made herein are solely theresponsibility of the authors.ReferencesMohamed I. Alkanhal, Mohamed Al-Badrashiny, Man-sour M. Alghamdi, and Abdulaziz O. Al-Qabbany.2012.
Automatic Stochastic Arabic Spelling Correc-tion With Emphasis on Space Insertions and Dele-tions.
IEEE Transactions on Audio, Speech & Lan-guage Processing, 20(7):2111?2122.Mohammed Attia, Pavel Pecina, Younes Samih,Khaled Shaalan, and Josef van Genabith.
2012.
Im-proved Spelling Error Detection and Correction forArabic.
In Proceedings of COLING 2012: Posters,pages 103?112, Mumbai, India.Daniel Dahlmeier and Hwee Tou Ng.
2012a.
A Beam-Search Decoder for Grammatical Error Correction.In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 568?578, Jeju Island, Korea.Daniel Dahlmeier and Hwee Tou Ng.
2012b.
Bet-ter Evaluation for Grammatical Error Correction.
InNAACL HLT ?12 Proceedings of the 2012 Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies, pages 568?572.Robert Dale.
1997.
Computer Assistance in Text Cre-ation and Editing.
In Survey of the state of the artin Human Language Technology, chapter 7, pages235?237.
Cambridge University Press.Chris Dyer, Jonathan Weese, Hendra Setiawan, AdamLopez, Ferhan Ture, Vladimir Eidelman, Juri Gan-itkevitch, Phil Blunsom, and Philip Resnik.
2010.cdec: A Decoder, Alignment, and Learning Frame-work for Finite-state and Context-free TranslationModels.
In Proceedings of the ACL 2010 SystemDemonstrations, pages 7?12, Uppsala, Sweden.A.
R. Golding and D. Roth.
1999.
A Winnow BasedApproach to Context-Sensitive Spelling Correction.Machine Learning, 34(1-3):107?130.Nizar Habash.
2008.
Four Techniques for Online Han-dling of Out-of-Vocabulary Words in Arabic-EnglishStatistical Machine Translation.
In Proceedings ofACL-08: HLT, Short Papers, pages 57?60, Colum-bus, Ohio.Bassam Haddad and Mustafa Yaseen.
2007.
Detectionand Correction of Non-words in Arabic: a HybridApproach.
International Journal of Computer Pro-cessing of Oriental Languages, 20(04):237?257.Ahmed Hassan, Sara Noeman, and Hany Hassan.2008.
Language Independent Text Correction usingFinite State Automata.
In Proceedings of the ThirdInternational Joint Conference on Natural LanguageProcessing (IJCNLP 2008), pages 913?918, Hyder-abad, India.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.Clark, and Philipp Koehn.
2013.
Scalable ModfiedKneser-Ney Language Model Estimation.
In In Pro-ceedings of the Association for Computational Lin-guistics, Sofia, Bulgaria.Philipp Koehn, Hieu Hoang, Alexandra Birch, Christo-pher Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Christopher Dyer, Ondrej Bo-jar, Alexandra Constantin, and Evan Herbst.
2007.Moses: Open Source Toolkit for Statistical Ma-chine Translation.
In Proceedings of the 45th An-nual Meeting of the Association for ComputationalLinguistics Companion Volume Proceedings of theDemo and Poster Sessions, pages 177?180, Prague,Czech Republic.141Karen Kukich.
1992.
Techniques for AutomaticallyCorrecting Words in Text.
ACM Computing Surveys(CSUR), 24(4):377?439.Behrang Mohit, Alla Rozovskaya, Nizar Habash, Wa-jdi Zaghouani, and Ossama Obeid.
2014.
The FirstQALB Shared Task on Automatic Text Correctionfor Arabic.
In Proceedings of EMNLP Workshop onArabic Natural Language Processing, Doha, Qatar,October.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
In Computational Linguistics, page 1951.Kemal Oflazer.
1996.
Error-Tolerant Finite-StateRecognition with Applications to MorphologicalAnalysis and Spelling Correction.
ComputationalLinguistics, 22(1):73?89.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A Method for AutomaticEvaluation of Machine Translation.
In Proceed-ings of the Association for Computational Linguis-tics, Philadelphia, Pennsylvania.Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab,Ahmed El Kholy, Ramy Eskander, Nizar Habash,Manoj Pooleery, Owen Rambow, and Ryan Roth.2014.
MADAMIRA: A Fast, Comprehensive Toolfor Morphological Analysis and Disambiguation ofArabic.
In Proceedings of the Ninth InternationalConference on Language Resources and Evaluation(LREC?14), pages 1094?1101, Reykjavik, Iceland.Khaled Shaalan, Amin Allam, and Abdallah Gomah.2003.
Towards Automatic Spell Checking for Ara-bic.
In Proceedings of the 4th Conference on Lan-guage Engineering, Egyptian Society of LanguageEngineering (ELSE), Cairo, Egypt.Khaled Shaalan, Rana Aref, and Aly Fahmy.
2010.
AnApproach for Analyzing and Correcting Spelling Er-rors for Non-native Arabic Learners.
In Proceedingsof The 7th International Conference on Informaticsand Systems, INFOS2010, the special track on Nat-ural Language Processing and Knowledge Mining,pages 28?30, Cairo, Egypt.Khaled Shaalan, Mohammed Attia, Pavel Pecina,Younes Samih, and Josef van Genabith.
2012.Arabic Word Generation and Modelling for SpellChecking.
In Proceedings of the Eighth Inter-national Conference on Language Resources andEvaluation (LREC-2012), pages 719?725, Istanbul,Turkey.Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Os-sama Obeid, Nadi Tomeh, Alla Rozovskaya, NouraFarra, Sarah Alkuhlani, and Kemal Oflazer.
2014.Large Scale Arabic Error Annotation: Guidelinesand Framework.
In Proceedings of the Ninth In-ternational Conference on Language Resources andEvaluation (LREC?14), Reykjavik, Iceland.Chiraz Zribi and Mohammed Ben Ahmed.
2003.
Ef-ficient Automatic Correction of Misspelled ArabicWords Based on Contextual Information.
In Pro-ceedings of the Knowledge-Based Intelligent Infor-mation and Engineering Systems Conference, pages770?777, Oxford, UK.142
