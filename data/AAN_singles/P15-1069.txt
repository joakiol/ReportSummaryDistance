Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 708?718,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsKnowledge Portability with Semantic Expansion of Ontology LabelsMihael Arcan1Marco Turchi2Paul Buitelaar11Insight Centre for Data Analytics, National University of Ireland, Galwayfirstname.lastname@insight-centre.org2FBK- Fondazione Bruno Kessler, Via Sommarive 18, 38123 Trento, Italyturchi@fbk.euAbstractOur research focuses on the multilin-gual enhancement of ontologies that, of-ten represented only in English, need tobe translated in different languages to en-able knowledge access across languages.Ontology translation is a rather differenttask then the classic document translation,because ontologies contain highly specificvocabulary and they lack contextual in-formation.
For these reasons, to improveautomatic ontology translations, we firstfocus on identifying relevant unambigu-ous and domain-specific sentences from alarge set of generic parallel corpora.
Then,we leverage Linked Open Data resources,such as DBPedia, to isolate ontology-specific bilingual lexical knowledge.
Inboth cases, we take advantage of the se-mantic information of the labels to se-lect relevant bilingual data with the aimof building an ontology-specific statisticalmachine translation system.
We evaluateour approach on the translation of a medi-cal ontology, translating from English intoGerman.
Our experiment shows a sig-nificant improvement of around 3 BLEUpoints compared to a generic as well as adomain-specific translation approach.1 IntroductionCurrently, most of the semantically structureddata, i.e.
ontologies or taxonomies, has labels ex-pressed in English only.1On the one hand, theincreasing amount of ontologies offers an excel-lent opportunity to link this knowledge together(G?omez-P?erez et al, 2013).
On the other hand,non-English users may encounter difficulties when1Based on (Gracia et al, 2012), around 80% of ontologylabels indexed in Watson are English.using the ontological knowledge represented onlyin English.
Furthermore, applications in informa-tion retrieval, question answering or knowledgemanagement, that use monolingual ontologies aretherefore limited to the language in which the on-tology labels are stored.
To make the ontologi-cal knowledge language-independent and accessi-ble beyond language borders, these monolingualresources need to be transformed into multilingualknowledge bases.
This multilingual enhancementcan enable queries on documents beyond English,e.g.
for cross-lingual business intelligence in thefinancial domain (O?Riain et al, 2013), provid-ing information related to an ontology label, e.g.other intangible assets,2in Spanish, German orItalian.
The main challenge involved in build-ing multilingual knowledge bases is, however, tobridge the gap between language-specific informa-tion and the language-independent semantic con-tent of ontologies or taxonomies (Gracia et al,2012).Since manual multilingual enhancement of on-tologies is a very time consuming and expensiveprocess, we engage an ontology-specific statisti-cal machine translation (SMT) system to automat-ically translate the ontology labels.
Due to the factthat ontology labels are usually highly domain-specific and stored only in knowledge represen-tations (Chandrasekaran et al, 1999), the labelsappear infrequent in parallel corpora, which areneeded to build a domain-specific translation sys-tem with accurate translation candidates.
Addi-tionally, ambiguous labels built out of only a fewwords do often not express enough semantic orcontextual information to guide the SMT systemto translate a label into the targeted domain.
Thiscan be observed by domain-unadapted SMT sys-tems, e.g.
Google Translate, where ambiguousexpressions, such as vessel stored in an medicalontology, are often translated into a generic do-2ontology label stored in FINREP - FINancial REPorting708main as Schiff3in German (meaning ship or boat),but not into the targeted medical domain as Gef?a?.Since ontologies may change over time, keepingup with these changes can be challenging for a hu-man translator.
Having in place an SMT systemadapted to an ontology can therefore be very ben-eficial.In this work, we propose an approach to selectthe most relevant (parallel) sentences from a poolof generic sentences based on the lexical and se-mantic overlap with the ontology labels.
The goalis to identify sentences that are domain-specific inrespect of the target domain and contain as muchas possible relevant words that can allow the SMTsystem to learn the translations of the monolin-gual ontology labels.
For instance, with the sen-tence selection we aim to retain only parallel sen-tences where the English word injection is trans-lated into the German language as Impfung in themedical domain, but not into Eind?usung, belong-ing to the technical domain.
This selection processaims to reduce the semantic noise in the translationprocess, since we try to avoid learning translationcandidates that do not belong to the targeted do-main.
Nonetheless, some of the domain-specificontology labels may not be automatically trans-latable with SMT, due to the fact that the bilin-gual information is missing and cannot be learnedfrom the parallel sentences.
Therefore we use theinformation contained in the DBpedia knowledgebase (Lehmann et al, 2015) to improve the trans-lation of expressions which are not known to theSMT system.
We tested our approach on the med-ical domain translating from English to German,showing improvements of around 3 BLEU pointscompared to a generic as well as a domain-specifictranslation model.The remainder of this paper is organized asfollows: Section 2 gives an overview of the re-lated work done in the field of ontology translationwithin SMT.
In Section 3, we present the method-ology of parallel data selection and terminologyidentification to improve ontology label transla-tion.
Furthermore we show different methods ofembedding domain-specific knowledge into SMT.In Experimental Setting, Section 4, we describethe ontology to be translated along the trainingdata needed for SMT.
Moreover we introduce ex-isting approaches and give a description of met-rics for automatic translation evaluation.
Section 53Translation performed on 25.02.2015presents the automatic and manual evaluation ofthe translated labels.
Finally, conclusions and fu-ture work are shown in Section 6.2 Related WorkThe task of ontology translation involves the find-ing of an appropriate translation for the lexicallayer, i.e.
labels, of the ontology.
Most of theprevious work tackled this problem by accessingmultilingual lexical resources, e.g.
EuroWordNetor IATE (Declerck et al, 2006; Cimiano et al,2010).
Their work focuses on the identificationof the lexical overlap between the ontology andthe multilingual resource.
Since the replacementof the source and target vocabulary guarantees ahigh precision but a low recall, external transla-tion services, e.g.
BabelFish, SDL FreeTransla-tion tool or Google Translate, were used to over-come this issue (Fu et al, 2009; Espinoza et al,2009).
Additionally, ontology label disambigua-tion was performed by (Espinoza et al, 2009) and(McCrae et al, 2011), where the structure of theontology along with existing multilingual ontolo-gies was used to annotate the labels with their se-mantic senses.
Differently to the aforementionedapproaches, which rely on external knowledge orservices, we focus on how to gain adequate trans-lations using a small, but ontology-specific SMTsystem.
We learned that using external SMT ser-vices often results in wrong translations of la-bels, because the external SMT services are notable to adapt to the specificity of the ontology.Avoiding existing multilingual resources, whichenables a simple replacement of source and targetlabels, showed the possibility of improving labeltranslations without manually generated lexical re-sources, since not every ontology may benefit ofcurrent multilingual resources.Due to the specificity of the labels, previousresearch (Wu et al, 2008; Haddow and Koehn,2012) showed that generic SMT systems, whichmerge all accessible data together, cannot be usedto translate domain-specific vocabulary.
To avoidunsatisfactory translations of specific vocabularywe have to provide the SMT system domain-specific bilingual knowledge, from where it canlearn specific translation candidates.
(Eck et al,2004) used for the language model adaptationwithin SMT the information retrieval techniquetf-idf.
Similarly, (Hildebrand et al, 2005) and(L?u et al, 2007) utilized this approach to select709relevant sentences from available parallel text toadapt translation models.
The results confirmedthat large amounts of generic training data can-not compensate for the requirement of domain-specific training sentences.
Another approach istaken by (Moore and Lewis, 2010), where, basedon source and target language models, the authorscalculated the difference of the cross-entropy val-ues for a given sentence.
(Axelrod et al, 2011)extend this work using the bilingual differenceof cross-entropy on in-domain and out-of-domainlanguage models for training sentence selectionfor SMT.
(Wuebker et al, 2014) reused the cross-entropy approach and applied it to the translationof video lectures.
(Kirchhoff and Bilmes, 2014)introduce submodular optimization using complexfeatures for parallel sentence selection.
In theirexperiments they use the source and target sideof the text to be translated, and show significantimprovements over the widely used cross-entropymethod.
A different approach for sentence se-lection is shown in (Cuong and Sima?an, 2014),where the authors propose a latent domain transla-tion model to distinguish between hidden in- andout-of-domain data.
(Gasc?o et al, 2012) and (Bi-cici and Yuret, 2011) sub-sample sentence pairswhose source has most overlap with the evaluationdataset.
Different from these approaches, we donot embed any specific in-domain knowledge tothe generic corpus, from which sentence selectionis performed.
Furthermore, none of these meth-ods explicitly exploit the ontological hierarchy forlabel disambiguation and are not specifically de-signed to deal with the characteristics of ontologylabels.As a lexical resource, Wikipedia with its richsemantic knowledge was used as a resource forbilingual term identification in the context of SMT.
(Tyers and Pieanaar, 2008) extracts bilingual dic-tionary entries from Wikipedia to support the ma-chine translation system.
Based on exact stringmatching they query Wikipedia with a list ofaround 10,000 noun lemmas to generate the bilin-gual dictionary.
Besides the interwiki link system,(Erdmann et al, 2009) enhance their bilingual dic-tionary by using redirection page titles and anchortext within Wikipedia.
To cast the problem ofambiguous Wikipedia titles, (Niehues and Waibel,2011; Arcan et al, 2014a) use the information ofWikipedia categories and the text of the articles toprovide the SMT system domain-specific bilingualknowledge.
This research showed that using thelexical information stored in this knowledge baseimproves the translation of highly domain-specificvocabulary.
However, we do not rely on cate-gory annotations of Wikipedia articles, but per-form domain-specific dictionary generation basedon the overlap between related words from the on-tology label and the abstract of a Wikipedia article.3 MethodologyWe propose an approach that uses the ontologylabels to be translated to select the most relevantparallel sentences from a generic parallel corpus.Since ontology labels tend to be short (McCraeet al, 2011), we expand the label representationwith its semantically related words.
This expan-sion enables a larger semantic overlap between alabel and the (parallel) sentences, which gives usmore information to distinguish between relatedand unrelated sentences.
Our approach reducesthe ambiguity of expressions in the selected par-allel sentences, which consequently gives morepreference to translation candidates of the targeteddomain.
Furthermore, we access the DBpediaknowledge base to identify bilingual terminologybelonging to the domain of the ontology.
Oncethe domain-specific parallel sentences and lexi-cal knowledge is available, we use different tech-niques to embed this knowledge into the SMT sys-tem.
These methods are detailed in the followingsubsections.3.1 Domain-Specific Parallel SentenceSelectionIn order to generate the best translation system weselect only sentences from the generic parallel cor-pus which are most relevant to the labels to betranslated.
The first criteria for relevance was then-gram overlap between a label and a source sen-tence coming from the generic corpus.
Thereforewe calculate the cosine similarity between the n-grams extracted from a label and the n-grams ofeach source sentence in the generic corpus.
Thesimilarity between the label and the sentence is de-fined as the cosine of the angle between the twovectors.
The calculated similarity score allows usto distinguish between more and less relevant sen-tences.Due to the specificity of ontology labels, the n-gram overlap approach is not able to select use-ful sentences in the presence of short labels.
For710this reason, we improve it by extending the se-mantic information of labels using a technique forcomputing vector representations of words.
Thetechnique is based on a neural network that anal-yses the textual data provided as input and pro-vides as output a list of semantically related words(Mikolov et al, 2013).
Each input string is vector-ized using the surrounding context and comparedto other vectorized sets of words (from the trainingdata) in a multi-dimensional vector space.
For ob-taining the vector representations we used a distri-butional semantic model trained on the Wikipediaarticles,4containing more than 3 billion words.Word relatedness is measured through the cosinesimilarity between two word vectors.
A score of1 would represent a perfect word similarity; e.g.cholera equals cholera, while the medical expres-sion medicine has a cosine distance of 0.678 tocholera.
Since words, which occur in similar con-texts tend to have similar meanings (Harris, 1954),this approach enables to group related words to-gether.
The output of this technique is the analysedlabel with a vector attached to it, e.g.
for the med-ical label cholera it provides related words withits relatedness value, e.g.
typhus (0.869), smallpox(0.849), epidemic (0.834), dysentery (0.808) .
.
.
Inour experiments, this method is implemented bythe use of Word2Vec.5To additionally disambiguate short labels, therelated words of the current label are combinedwith the related words of its direct parent in theontology.
The usage of the ontology hierarchy al-lows us to take advantage of the specific vocabu-lary of the related words in the computation of thecosine similarity.
Given a label and a source sen-tence from the generic corpus, related words andtheir weights are extracted from both of them andused as entries of the vectors passed to the cosinesimilarity.
The most similar source sentence andthe label should share the largest number of relatedwords (largest cosine similarity).3.2 Bilingual Terminology IdentificationThe automatic translation of domain-specific vo-cabulary can be a hard task for a generic SMT sys-tem, if the bilingual knowledge is not present inthe parallel dataset.
To complement the previousapproaches we access DBpedia6as a multilinguallexical resource.4Wikipedia dump id enwiki-201411065https://code.google.com/p/word2vec/6http://wiki.dbpedia.org/Downloads2014We engage the idea of (Arcan et al, 2012)where the authors provide to the SMT system un-ambiguous terminology identified in Wikipedia toimprove the translations of labels in the financialdomain.
To disambiguate Wikipedia entries withtranslations into different domains, they query therepository for analysing the n-gram overlap be-tween the financial labels and the Wikipedia en-tries and store the frequency of categories whichare associated with the matched entry.
In a fi-nal step they extract only bilingual Wikipedia en-tries, which are associated with the most frequentWikipedia categories identified in the previousstep.Since the Wikipedia entries are often associ-ated only with a few categories, this limited vo-cabulary may give only a small contribution forthis disambiguation of different meanings or top-ics of the same Wikipedia entry.
For this reason,we use for each Wikipedia entry the extended ab-stract, which contains more information about theentry compared to the previous approach.
For am-biguous Wikipedia entries, which overlap with amedical label, we therefore calculate the cosinesimilarity between the related words associatedwith the label and the lexical information of theWikipedia abstract.
Among different ambiguousentries, the cosine similarity gives more weight tothe Wikipedia entry, which is closer to our pre-ferred domain.
Finally, if the Wikipedia entry hasan equivalent in the target language, i.e.
German,we use the bilingual information for the lexical en-hancement of the SMT system.3.3 Integration of Domain-SpecificKnowledge into SMTAfter the identification of domain-specific bilin-gual knowledge, it has to be integrated into theworkflow of the SMT system.
The injection ofnew obtained knowledge can be performed by re-training the domain-specific knowledge with thegeneric parallel corpus (Langlais, 2002; Ren et al,2009; Haddow and Koehn, 2012) or by addingnew entries directly to the translation system (Pin-nis et al, 2012; Bouamor et al, 2012).
Thesemethods have the drawback that the bilingual do-main specificity may get lost due to the usuallylarger generic parallel corpora.
Giving more pri-ority to domain-specific translations than genericones, we focus on two techniques, i.e.
the Fill-Upmodel (Bisazza et al, 2011) and the Cache-Based711Model (Bertoldi et al, 2013) approach.The Fill-Up model has been developed to ad-dress a common scenario where a large genericbackground model exists, and only a small quan-tity of domain-specific data can be used to builda translation model.
Its goal is to leverage thelarge coverage of the background model, whilepreserving the domain-specific knowledge com-ing from the domain-specific data.
For this pur-pose the generic and the domain-specific transla-tion models are merged.
For those translation can-didates that appear in both models, only one in-stance is reported in the Fill-Up model with thelargest probabilities according to the translationmodels.
To keep track of a translation candidate?sprovenance, a binary feature is added that givespreference to a translation candidate if it comesfrom the domain-specific translation model.
Weengage the idea of the Fill-Up model to combinethe domain-specific parallel knowledge from theselected sentences with the generic (1.9M) paral-lel corpus.Furthermore, for embedding bilingual lexicalknowledge into the SMT system, we engage theidea of cache-based translation and language mod-els (Bertoldi et al, 2013).
The main idea behindthese models is to combine a large static globalmodel with a small, but dynamic local model.
Thisapproach has already shown its potential of in-jecting domain-specific knowledge into a genericSMT system (Arcan et al, 2014b).
For our exper-iments we inject the bilingual lexical knowledgeidentified in DBpedia and IATE into the cache-based models.
The cache-based model relies ona local translation model (CBTM) and languagemodel (CBLM).
The first is implemented as anadditional table in the translation model provid-ing one score.
All entries are associated with an?age?
(initially set to 1), corresponding to the timewhen they were actually inserted.
Each new in-sertion causes an ageing of the existing translationcandidates and hence their re-scoring; in case ofre-insertion of a phrase pair, the old value is set tothe initial value.
Similarly to the CBTM, the lo-cal language model is built to give preference tothe provided target expressions.
Each entry storedin CBLM is associated with a decaying functionof the age of insertion into the model.
Both mod-els are used as additional features of the log-linearmodel in the SMT system.4 Experimental SettingIn this Section, we give an overview on the datasetand the translation toolkit used in our experiment.Furthermore, we describe the existing approachesand give insights into the SMT evaluation tech-niques, considering the translation direction fromEnglish to German.Evaluation Dataset For our experiments weused the International Classification of Diseases(ICD) ontology as the gold standard,7whereby theconsidered translation direction is from English toGerman.
The ICD ontology, translated into 43 lan-guages, is used to monitor diseases and to reportthe general health situation of the population in acountry.
This stored information also provides anoverview of the national mortality rate and appear-ance of diseases of WHO member countries.For our experiment we used 2000 English labelsfrom the ICD-10 dataset, which were aligned totheir German equivalents (Table 1).
To identify thebest set of sentences we experiment with differ-ent values of ?
, which is the percentage of all thesentences that are considered relevant (domain-specific) by the sentence extraction approach.
Thevalue that allows the SMT system to achieve thebest performance on the development dataset 1 isused on the evaluation set, which is used for thetranslation evaluation of ontology labels reportedin this paper.
The parameters within the SMT sys-tem are optimized on the development dataset 2.Statistical Machine Translation and TrainingDataset For our translation task, we use the sta-tistical translation toolkit Moses (Koehn et al,2007), where the word alignments were built withthe GIZA++ toolkit (Och and Ney, 2003).
TheSRILM toolkit (Stolcke, 2002) was used to buildthe 5-gram language model.For a broader domain coverage of the generictraining dataset necessary for the SMT system,we merged parts of JRC-Acquis 3.08(Steinbergeret al, 2006), Europarl v79(Koehn, 2005) andOpenSubtitles201310(Tiedemann, 2012), obtain-ing a training corpus of 1.9M sentences, con-7http://www.who.int/classifications/icd/en/8https://ec.europa.eu/jrc/en/language-technologies/jrc-acquis9http://www.statmt.org/europarl/10http://opus.lingfil.uu.se/OpenSubtitles2013.php712English GermanGeneric Dataset Sentences 1.9M(out-domain) Running Words 39.8M 37.1MVocabulary 195,912 446,068EMEA Dataset Sentences 1.1M(domain-specific) Running Words 13.8M 12.7MVocabulary 58,935 115,754Development Labels 500Dataset 1 Running Words 3,025 2,908Vocabulary 889 951Development Labels 500Dataset 2 Running Words 3,003 3,020Vocabulary 938 1,027Evaluation Labels 1,000Dataset Running Words 5,677 5,514Vocabulary 1,255 1,489Table 1: Statistics for the bilingual training, de-velopment and evaluation datasets.
(?Vocabulary?denotes the number of unique words in the dataset)taining around 38M running words (Table 1).11The generic SMT system, trained on the con-catenated 1.9 sentences, is used as a baseline,which we compare against the domain-specificmodels generated with different sentence selectionmethods.
Furthermore we use the generic SMTsystem in combination with the smaller domain-specific models to evaluate different approacheswhen combining generic and domain-specific datatogether.We additionally compare our results to an SMTsystem built on an existing domain-specific par-allel dataset, i.e.
EMEA12(Tiedemann, 2009),which holds specific medical parallel data ex-tracted from the European Medicines Agency doc-uments and websites.Comparison to Existing Approaches We com-pare our approach on knowledge expansion forsentence selection with similar methods that dis-tinguish between more important sentences andless important ones.
First, we sort 1.9M sentencesfrom the generic corpus based on the perplexityof the ontology vocabulary.
The perplexity scoregives a notion of how well the probability modelbased on the ontology vocabulary predicts a sam-ple, which is in our case each sentence in thegeneric corpus.Second, we use the method shown in (Hilde-brand et al, 2005), where the authors use a method11For reproducibility and future evaluation we take the firstone-third part of each corpus.12http://opus.lingfil.uu.se/EMEA.phpbased on tf-idf13to select the most relevant sen-tences.
This widely-used method in informationretrieval tells us how important a word is to a doc-ument, whereby each sentence from the genericcorpus is treated as a document.Finally, we compare our approach with the in-frequent n-gram recovery method, described in(Gasc?o et al, 2012).
Their technique consists ofselection of relevant sentences from the genericcorpus, which contain infrequent n-grams basedon their test data.
They consider an n-gram asinfrequent if it appears in the generic corpus lesstimes than an infrequent threshold t.Furthermore we enrich and evaluate our pro-posed ontology-specific SMT system with the lex-ical information coming from the terminologicaldatabase IATE14(Inter-Active Terminology forEurope).
IATE is the institutional terminologydatabase of the EU and is used for the collection,dissemination and shared management of specificterminology and contains approximately 1.4 mil-lion multilingual entries.Evaluation Metrics The automatic translationevaluation is based on the correspondence be-tween the SMT output and reference translation(gold standard).
For the automatic evaluationwe used the BLEU (Papineni et al, 2002) andMETEOR (Denkowski and Lavie, 2014) algo-rithms.15BLEU (Bilingual Evaluation Understudy) iscalculated for individual translated segments (n-grams) by comparing them with a dataset of refer-ence translations.
Considering the shortness of thelabels, we report scores based on the bi-gram over-lap (BLEU-2) and the standard four-gram over-lap (BLEU-4).
Those scores, between 0 and 100(perfect translation), are then averaged over thewhole evaluation dataset to reach an estimate ofthe translation?s overall quality.METEOR (Metric for Evaluation of Transla-tion with Explicit ORdering) is based on the har-monic mean of precision and recall, whereby re-call is weighted higher than precision.
Along withstandard exact word (or phrase) matching it hasadditional features, i.e.
stemming, paraphrasingand synonymy matching.
Differently to BLEU,the metric produces good correlation with humanjudgement at the sentence or segment level.13tf-idf ?
term frequency-inverse document frequency14http://iate.europa.eu/downloadTbx.do15METEOR configuration: exact, stem, paraphrase713The approximate randomization approach inMultEval (Clark et al, 2011) is used to testwhether differences among system performancesare statistically significant with a p-value < 0.05.5 Evaluation of Ontology LabelsIn this Section, we report the translation qualityof ontology labels based on translation systemslearned from different sentence selection methods.Additionally, we perform experiments training anSMT system on the combination of in- and out-domain knowledge.
The final approach enhancesa domain-specific translation system with lexicalknowledge identified in IATE or DBpedia.5.1 Automatic Translation EvaluationWe report the automatic evaluation based onBLEU and METEOR for the sentence selectiontechniques, the combination of in- and out-domaindata and the lexical enhancement of SMT.Sentence Selection Techniques As a first eval-uation, we automatically compare the quality ofthe ICD labels translated with different SMT sys-tems trained on specific sentences by the afore-mentioned selection techniques (Table 2).
Due tothe in-domain bilingual knowledge, the translationsystem trained using the EMEA dataset performsslightly better compared to the large generic base-line system.
Among the different sentence selec-tion approaches, the infrequent n-gram recoverymethod (infreq.
in Table 2) outperforms the base-lines and all the other techniques.
This is due tothe very strict criteria of selecting relevant sen-tences that allows the infrequent n-gram recoverymethod to identify a limited number (20,000) ofhighly ontology-specific bilingual sentences.
Therelated words and the n-gram overlap models per-form slightly better than the baseline, with a usageof 81,000 and 59,000 relevant sentences, and per-form similarly to the in-domain EMEA translationsystem.Further translation quality improvement is pos-sible, if sentence selection methods are combinedtogether (last four rows in Table 2).
The co-sine similarities of the methods are combined to-gether, whereby new thresholds ?
are computedon the development dataset 1 and applied on theICD evaluation dataset.
Each combined methodshowed improvement compared to the stand-alonemethod.
The best overall performance is obtainedDataset Type Size BLEU-2 BLEU-4 METEORGeneric dataset 1.9M 17.2 6.6 24.7EMEA dataset 1.1M 18.5 7.0 25.8(1) perplexity 89K 17.5 6.8 24.8(2) tf-idf 21K 12,6 4.9 18,7(3) infreq.
20K 19.1 8.1 25.3(4) related w. 81K 18.9 7.0 25.8(5) n-gram 59K 17.7 7.1 23.3(5) ?
(3) 22K 18.9 8.2* 25.1(5) ?
(4) 24K 17.3 7.3 23.9(3) ?
(4) 24K 18.4 8.4* 25.5*(5) ?
(4) ?
(3) 30K 20.1 8.9* 27.2*Table 2: Automatic translation evaluation on theevaluation dataset of the ICD ontology (Size =amount of selected sentences from the generic par-allel corpus.
bold results = best performance; *sta-tistically significant compared to baseline)when combining the n-gram overlap, the seman-tic related words and infrequent n-gram recoverymethods.
With this combination, we reduce theamount of parallel sentences by 98% comparedto the generic corpus and significantly outperformthe baseline by 2.3 BLEU score points.
Thesetwo factors confirm the capability of the combinedapproach of selecting only few ontology-specificbilingual sentences (30,000) that allows the SMTsystem to identify the correct translations in thetarget ontology domain.
This is due to the fact thatthe three combined methods are quite complemen-tary.
In fact, the n-gram overlap method selects arelatively large amount of bilingual sentences withfew words in common with the label, the relatedwords approach identifies bilingual sentences inthe ontology target domain, and the infrequent n-gram recovery technique selects few bilingual sen-tences with only specific n-grams in common withthe labels balancing the effect of the n-gram over-lap method.Combining In- and Out-Domain Data Con-sidering the relatively small amount of paralleldata extracted with the sentence selecting meth-ods for the SMT community, we evaluate dif-ferent approaches that combine a large generictranslation model with domain-specific data.
Forthis purpose, we use the sentences selected bythe best approach ((5)?(4)?
(3)) in the previousexperiments and combine them with the genericparallel dataset.
We evaluate the translation per-formance when (i) concatenating the selecteddomain-specific parallel dataset with the generic714Dataset Type BLEU-2 BLEU-4 METEORGeneric dataset 17.2 6.6 24.7(5)?(4)?
(3) sent.
selec.
20.1 8.9* 27.2*Data Concatenation (i) 18.1 6.8 24.1Log-linear Models (ii) 18.9 8.1* 25.3Fill-Up Model (iii) 17.7 7.0 24.7(5)?(4)?
(3) + IATE 19.8 9.0* 27.8*(5)?(4)?
(3) + DBpedia(1)20.6 9.1* 27.3*(5)?(4)?
(3) + DBpedia(2)21.0 9.6*328.2*3Table 3: Evaluation of the ICD ontology eval-uation dataset combining domain-specific withgeneric parallel knowledge and lexical enhance-ment of SMT using IATE and DBpedia (boldresults = best performance; *statistically signifi-cant compared to baseline;3statistically signifi-cant compared to best sentence selection model)parallel one, (ii) combining the generated transla-tion models from the selected domain-specific par-allel dataset and the generic corpus and (iii) apply-ing the Fill-Up model to emphasise the domain-specific data in a single translation model.
Thetranslation performance of the combination meth-ods are shown in Table 3.
It is interesting tonotice that none of them benefits from the useof the additional generic parallel data showingtranslation performance smaller than the domain-specific model.
Although all methods outperformthe generic translation model, only the log-linearapproach, keeping in- and out-domain translationmodels separated, shows significant improvement.Comparing it to the combined sentence selec-tion technique ((5)?(4)?
(3)) does not show anystatistical significant differences between the ap-proaches.
We conclude that the generic corpusis too large compared to the selected in-domaincorpus, nullifying the influence of the extracteddomain-specific parallel knowledge.Lexical enhancement for SMT Since the out-of-vocabulary problem can be only mitigatedwith sentence selection, we accessed lexical re-sources IATE and DBpedia to further improvethe translations of the medical labels.
Based onthe word overlap between labels and entries inIATE we extracted 11,641 English lexical entrieswith its equivalent in German.
The DBpedia(1)approach, which disambiguates DBpedia entriesbased on the (Wikipedia article) categories (Ar-can et al, 2012), identified 7,911 English-Germanexpression for the targeted domain, while the ab-stract based disambiguation approach, marked asDBpedia(2)in Table 3 identified 3,791 bilingualentries.
The lexical enhanced models further im-proved the translations of the medical labels (lastthree rows in Table 3) due to the additional bilin-gual information from the lexical resources, whichis missing in the standalone sentence selectionmodel.
Comparing the ICD evaluation datasetand the translations generated with the DBpedia(2)lexical enhanced model we observed that morethan 80 labels benefit from the additional lexi-cal knowledge, e.g.
correcting the mistranslated?adrenal gland?
into ?Nebenniere?.
The lexicalextraction and disambiguation of bilingual knowl-edge based on the abstract of the article comparedto the article categories further improves the lex-ical choice, helping SMT systems to improve thetranslation of ontology labels.5.2 Manual Evaluation of Translated LabelsSince ontologies store specific vocabulary about adomain, this vocabulary is adapted to a concretelanguage and culture community (Cimiano et al,2010).
In order to investigate to what extent theautomatically generated translations differ from atranslator?s adapted point of view, we manually in-spected the translations produced by the sentenceselection approaches described in Section 5.1.While analysing the English and German part ofthe ICD ontology gold standard we noticed signif-icant differences in the translations of the medicallabels.
As a result of the language and culturaladaptation, many labels in the ICD ontology werenot always translated literally, i.e.
parts of a la-bel were semantically merged, omitted or new in-formation was added while crossing the languageborder.
For example, the ICD label ?acute kid-ney failure and chronic kidney disease?
is storedin the German part of the ontology as ?Nierenin-suffizienz?.16Although none of the translationsystems can generate the compounded medicalexpression for German, the SMT system gener-ated nevertheless an acceptable translation, i.e.
?akutes Nierenversagen und chronischer Nieren-erkrankungen?.17A more extreme example is theEnglish label ?slipping, tripping, stumbling andfalls?, in the German ICD ontology represented as16Niereninsuffizienz?kidney insufficiency17akutes?acute, Nierenversagen?kidney failure,und?and, chronischer?chronic,Nierenerkrankungen?kidney disease715?sonstige St?urze auf gleicher Ebene?.18The lan-guage and cultural adaptation is very active for thisexample, where the whole English label is seman-tically merged into the word ?St?urze?, meaning?falls?.
Additionally, the German part holds moreinformation within the label, i.e.
?auf gleicherEbene?
(en.
?at the same level?
), which is notrepresented on the English side.
Since the SMTsystem will always try to translate every phrase(word or word segments) into the target language,an automatic translation evaluation cannot reflectthe overall SMT performance.Further we detected a large error class caused bycompounding, a common linguistic feature of Ger-man.
Although the phrase ?heart diseases?
with itsreference translation ?Herzkrankheiten?
appearsfrequent in the generic training dataset, the SMTsystem prefers to translate it word by word into?Herz Krankheiten?.19Similar observations weremade with ?upper arm?
(German ?Oberarm?)
withthe SMT word to word translation ?oberen Arm?.Finally, we analysed the impact of the seman-tically enriched sentence selection with relatedwords coming from Word2Vec compared to thesurface based sentence selection, e.g.
preplex-ity, infrequent n-gram recovery or n-gram overlap.Since semantically enriched selection stored themost relevant sentences, we observed the correcttranslation of the label ?blood vessels?
into ?Blut-gef?a?e?.
The generic and other surface based se-lections translated the expression individually into?Blut Schiffe?, where ?Schiffe?
refers to the morecommon English word ?ship?, but not to ?partof the system transporting blood throughout ourbody?.
The last example illustrates further the se-mantic mismatch between the training domain andthe test domain.
Using the generic model, builtmainly out of European laws and parliament dis-cussions (JRC-Acquis/Europarl) the word ?head?inside the label ?injury of head?
is wrongly trans-lated into the word ?Leiter?, meaning ?leader?
inthe legal domain.
Nevertheless, the additional se-mantic information prevents storing wrong paral-lel sentences and guides the SMT to the correcttranslation, i.e.
?Sch?adigung des Kopfes?.2018sonstige?other, St?urze?falls, auf?on,gleicher?same, Ebene?level19Herz?heart, Krankheiten?diseases20Sch?adigung?injury, des?of, Kopfes?head6 ConclusionIn this paper we presented an approach to identifythe most relevant sentences from a large genericparallel corpus, giving the possibility to translatehighly specific ontology labels without particularin-domain parallel data.
We enhanced furthermorethe translation system build on the in-domain par-allel knowledge with additional lexical knowledgeaccessing DBpedia.
With the aim to better se-lect relevant bilingual knowledge for SMT, we ex-tend previous sentence and lexical selection tech-niques with additional semantic knowledge.
Ourproposed ontology-specific SMT system showed astatistical significant improvement (up to 3 BLEUpoints) of ontology label translation over the com-pared translation approaches.In future, we plan to integrate a larger diversityof surface, semantic and linguistic information forrelevant sentence selection.
Although the SMTsystem is capable of translating several words intoa compound word, the small amount of the se-lected sentences limits this capability.
To improvethe ontology label translations, we therefore seethe need to focus more on the German compoundfeature.
Additionally we observed that more than25% of the identified lexical knowledge consistsof multi-word-expressions, e.g.
?fatal familial in-somnia?.
For this reason, our ongoing work fo-cuses on the alignment of nested knowledge insidethose expressions.
To move further in this direc-tion, we plan to focus on exploiting morphologicalterm variations taking advantage of the alternativeterms provided by DBpedia.AcknowledgmentsThis publication has emanated from research sup-ported in part by a research grant from ScienceFoundation Ireland (SFI) under Grant NumberSFI/12/RC/2289 (Insight) and the European Unionsupported projects LIDER (ICT-2013.4.1-610782)and MixedEmotions (H2020-644632).ReferencesArcan, M., Federmann, C., and Buitelaar, P. (2012).Experiments with term translation.
In Proceedingsof the 24th International Conference on Computa-tional Linguistics, Mumbai, India.Arcan, M., Giuliano, C., Turchi, M., and Buite-laar, P. (2014a).
Identification of Bilingual Terms716from Monolingual Documents for Statistical Ma-chine Translation.
In Proceedings of the 4th Inter-national Workshop on Computational Terminology(Computerm), Dublin, Ireland.Arcan, M., Turchi, M., Tonelli, S., and Buitelaar, P.(2014b).
Enhancing statistical machine translationwith bilingual terminology in a cat environment.
InProceedings of the 11th Conference of the Associa-tion for Machine Translation in the Americas, Van-couver, Canada.Axelrod, A., He, X., and Gao, J.
(2011).
Domainadaptation via pseudo in-domain data selection.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?11,Stroudsburg, PA, USA.Bertoldi, N., Cettolo, M., and Federico, M. (2013).Cache-based Online Adaptation for Machine Trans-lation Enhanced Computer Assisted Translation.
InProceedings of Machine Translation Summit XIV,Nice, France.Bicici, E. and Yuret, D. (2011).
Instance selection formachine translation using feature decay algorithms.In Proceedings of the Sixth Workshop on StatisticalMachine Translation, Edinburgh, Scotland.Bisazza, A., Ruiz, N., and Federico, M. (2011).
Fill-upversus Interpolation Methods for Phrase-based SMTAdaptation.
In Proceedings of IWSLT.Bouamor, D., Semmar, N., and Zweigenbaum, P.(2012).
Identifying bilingual multi-word expres-sions for statistical machine translation.
In Proceed-ings of the Eight International Conference on Lan-guage Resources and Evaluation, Istanbul, Turkey.Chandrasekaran, B., Josephson, J. R., and Benjamins,V.
R. (1999).
What are ontologies, and why do weneed them?
IEEE Intelligent Systems, 14(1):20?26.Cimiano, P., Montiel-Ponsoda, E., Buitelaar, P., Es-pinoza, M., and G?omez-P?erez, A.
(2010).
A note onontology localization.
Appl.
Ontol., 5(2):127?137.Clark, J., Dyer, C., Lavie, A., and Smith, N. (2011).Better Hypothesis Testing for Statistical MachineTranslation: Controlling for Optimizer Instability .In Proceedings of the Association for ComputationalLingustics.Cuong, H. and Sima?an, K. (2014).
Latent domaintranslation models in mix-of-domains haystack.
InProceedings of the 25th International Conference onComputational Linguistics, Dublin, Ireland.Declerck, T., P?erez, A. G., Vela, O., Gantner, Z., Man-zano, D., and D-Saarbr?ucken (2006).
Multilinguallexical semantic resources for ontology translation.In In Proceedings of the 5th International Confer-ence on Language Resources and Evaluation.Denkowski, M. and Lavie, A.
(2014).
Meteor univer-sal: Language specific translation evaluation for anytarget language.
In Proceedings of the EACL 2014Workshop on Statistical Machine Translation.Eck, M., Vogel, S., and Waibel, A.
(2004).
Languagemodel adaptation for statistical machine translationbased on information retrieval.
In Proc.
of LREC.Erdmann, M., Nakayama, K., Hara, T., and Nishio, S.(2009).
Improving the extraction of bilingual ter-minology from wikipedia.
ACM Trans.
MultimediaComput.
Commun.
Appl., 5(4).Espinoza, M., Montiel-Ponsoda, E., and G?omez-P?erez,A.
(2009).
Ontology localization.
In Proceedingsof the Fifth International Conference on KnowledgeCapture, K-CAP ?09, New York, NY, USA.
ACM.Fu, B., Brennan, R., and O?Sullivan, D. (2009).
Cross-lingual ontology mapping - an investigation of theimpact of machine translation.
In G?omez-P?erez, A.,Yu, Y., and Ding, Y., editors, ASWC, volume 5926of Lecture Notes in Computer Science.
Springer.Gasc?o, G., Rocha, M.-A., Sanchis-Trilles, G., Andr?es-Ferrer, J., and Casacuberta, F. (2012).
Does moredata always yield better translations?
In Proceed-ings of the 13th Conference of the European Chap-ter of the Association for Computational Linguistics,EACL ?12, Stroudsburg, PA, USA.G?omez-P?erez, A., Vila-Suero, D., Montiel-Ponsoda,E., Gracia, J., and Aguado-de Cea, G. (2013).Guidelines for multilingual linked data.
In Proceed-ings of the 3rd International Conference on Web In-telligence, Mining and Semantics.
ACM.Gracia, J., Montiel-Ponsoda, E., Cimiano, P., G?omez-P?erez, A., Buitelaar, P., and McCrae, J.
(2012).Challenges for the multilingual web of data.
Web Se-mantics: Science, Services and Agents on the WorldWide Web, 11.Haddow, B. and Koehn, P. (2012).
Analysing the Ef-fect of Out-of-Domain Data on SMT Systems.
InProceedings of the Seventh Workshop on StatisticalMachine Translation, Montr?eal, Canada.Harris, Z.
(1954).
Distributional structure.
Word,10(23).Hildebrand, A. S., Eck, M., Vogel, S., and Waibel, A.(2005).
Adaptation of the translation model for sta-tistical machine translation based on information re-trieval.
In Proceedings of the 10th Conference ofthe European Association for Machine Translation(EAMT), Budapest.Kirchhoff, K. and Bilmes, J.
(2014).
Submodularityfor data selection in machine translation.
In Em-pirical Methods in Natural Language Processing(EMNLP).717Koehn, P. (2005).
Europarl: A Parallel Corpus for Sta-tistical Machine Translation.
In Conference Pro-ceedings: the tenth Machine Translation Summit,pages 79?86.
AAMT.Koehn, P., Hoang, H., Birch, A., Callison-Burch, C.,Federico, M., Bertoldi, N., Cowan, B., Shen, W.,Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin,A., and Herbst, E. (2007).
Moses: Open sourcetoolkit for statistical machine translation.
In Pro-ceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,Stroudsburg, PA, USA.Langlais, P. (2002).
Improving a general-purposestatistical translation engine by terminological lex-icons.
In Proceedings of the 2nd InternationalWorkshop on Computational Terminology (COM-PUTERM) ?2002, Taipei, Taiwan.Lehmann, J., Isele, R., Jakob, M., Jentzsch, A., Kon-tokostas, D., Mendes, P. N., Hellmann, S., Morsey,M., van Kleef, P., Auer, S., and Bizer, C. (2015).DBpedia - a large-scale, multilingual knowledgebase extracted from wikipedia.
Semantic Web Jour-nal, 6(2):167?195.L?u, Y., Huang, J., and Liu, Q.
(2007).
Improving sta-tistical machine translation performance by trainingdata selection and optimization.
In Proceedings ofthe 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL).McCrae, J., Espinoza, M., Montiel-Ponsoda, E.,Aguado-de Cea, G., and Cimiano, P. (2011).
Com-bining statistical and semantic approaches to thetranslation of ontologies and taxonomies.
In Fifthworkshop on Syntax, Structure and Semantics in Sta-tistical Translation (SSST-5).Mikolov, T., Chen, K., Corrado, G., and Dean, J.(2013).
Efficient estimation of word representationsin vector space.
CoRR, abs/1301.3781.Moore, R. C. and Lewis, W. (2010).
Intelligent se-lection of language model training data.
In Pro-ceedings of the ACL 2010 Conference Short Papers,ACLShort ?10, Stroudsburg, PA, USA.Niehues, J. and Waibel, A.
(2011).
Using Wikipediato Translate Domain-specific Terms in SMT.
In In-ternational Workshop on Spoken Language Transla-tion, San Francisco, CA, USA.Och, F. J. and Ney, H. (2003).
A systematic compari-son of various statistical alignment models.
Compu-tational Linguistics, 29.O?Riain, S., Coughlan, B., Buitelaar, P., Declerck, T.,Krieger, U., and Thomas, S. M. (2013).
Cross-lingual querying and comparison of linked financialand business data.
In Cimiano, P., Fern?andez, M.,Lopez, V., Schlobach, S., and V?olker, J., editors,ESWC (Satellite Events), volume 7955 of LectureNotes in Computer Science.
Springer.Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.(2002).
BLEU: a method for automatic evaluationof machine translation.
In Proceedings of the 40thAnnual Meeting on Association for ComputationalLinguistics, ACL ?02, pages 311?318.Pinnis, M., Ljube?si?c, N., S?tef?anescu, D., Skadin?a, I.,Tadi?c, M., and Gornostay, T. (2012).
Term extrac-tion, tagging, and mapping tools for under-resourcedlanguages.
In Proceedings of the Terminology andKnowledge Engineering (TKE2012) Conference.Ren, Z., L?u, Y., Cao, J., Liu, Q., and Huang, Y.
(2009).Improving statistical machine translation using do-main bilingual multiword expressions.
In Proceed-ings of the Workshop on Multiword Expressions:Identification, Interpretation, Disambiguation andApplications, MWE ?09, Stroudsburg, PA, USA.Steinberger, R., Pouliquen, B., Widiger, A., Ignat, C.,Erjavec, T., Tufis, D., and Varga, D. (2006).
TheJRC-Acquis: A multilingual aligned parallel corpuswith 20+ languages.
In Proceedings of the 5th In-ternational Conference on Language Resources andEvaluation (LREC?2006).Stolcke, A.
(2002).
SRILM - An extensible lan-guage modeling toolkit.
In Proceedings Interna-tional Conference on Spoken Language Processing.Tiedemann, J.
(2009).
News from OPUS - A collectionof multilingual parallel corpora with tools and inter-faces.
In Nicolov, N., Bontcheva, K., Angelova, G.,and Mitkov, R., editors, Recent Advances in NaturalLanguage Processing, volume V. John Benjamins,Amsterdam/Philadelphia, Borovets, Bulgaria.Tiedemann, J.
(2012).
Parallel data, tools and inter-faces in opus.
In Chair), N. C. C., Choukri, K., De-clerck, T., Do?gan, M. U., Maegaard, B., Mariani,J., Odijk, J., and Piperidis, S., editors, Proceedingsof the Eight International Conference on LanguageResources and Evaluation, Istanbul, Turkey.Tyers, F. M. and Pieanaar, J.
A.
(2008).
Extractingbilingual word pairs from wikipedia.
In Collabora-tion: interoperability between people in the creationof language resources for less-resourced languages(A SALTMIL workshop).Wu, H., Wang, H., and Zong, C. (2008).
Domain adap-tation for statistical machine translation with domaindictionary and monolingual corpora.
In Proceedingsof the 22nd International Conference on Computa-tional Linguistics - Volume 1, COLING ?08.Wuebker, J., Ney, H., Mart?
?nez-Villaronga, A.,Gim?enez, A., , Juan, A., Servan, C., Dymetman, M.,and Mirkin, S. (2014).
Comparison of Data Selec-tion Techniques for the Translation of Video Lec-tures.
In Proc.
of the Eleventh Biennial Conf.
of theAssociation for Machine Translation in the Ameri-cas (AMTA-2014), Vancouver (Canada).718
