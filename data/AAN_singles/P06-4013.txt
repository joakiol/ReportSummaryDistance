Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 49?52,Sydney, July 2006. c?2006 Association for Computational LinguisticsArchivus: A multimodal system for multimedia meeting browsing andretrievalMarita Ailomaa, Miroslav Melichar,Martin RajmanArtificial Intelligence Laboratory?Ecole Polytechnique Fe?de?rale de LausanneCH-1015 Lausanne, Switzerlandmarita.ailomaa@epfl.chAgnes Lisowska,Susan ArmstrongISSCO/TIM/ETIUniversity of GenevaCH-1211 Geneva, Switzerlandagnes.lisowska@issco.unige.chAbstractThis paper presents Archivus, a multi-modal language-enabled meeting brows-ing and retrieval system.
The prototypeis in an early stage of development, andwe are currently exploring the role of nat-ural language for interacting in this rela-tively unfamiliar and complex domain.
Webriefly describe the design and implemen-tation status of the system, and then focuson how this system is used to elicit usefuldata for supporting hypotheses about mul-timodal interaction in the domain of meet-ing retrieval and for developing NLP mod-ules for this specific domain.1 IntroductionIn the past few years, there has been an increasinginterest in research on developing systems for effi-cient recording of and access to multimedia meet-ing data1.
This work often results in videos ofmeetings, transcripts, electronic copies of docu-ments referenced, as well as annotations of variouskinds on this data.
In order to exploit this work, auser needs to have an interface that allows them toretrieve and browse the multimedia meeting dataeasily and efficiently.In our work we have developed a multimodal(voice, keyboard, mouse/pen) meeting browser,Archivus, whose purpose is to allow users to ac-cess multimedia meeting data in a way that is mostnatural to them.
We believe that since this is a newdomain of interaction, users can be encouraged to1The IM2 project http://www.im2.ch, the AMI projectwww.amiproject.org, The Meeting Room Project at CarnegieMellon University, http://www.is.cs.cmu.edu/mie, and richtranscription of natural and impromptu meetings at ICSI,Berkeley, http://www.icsi.berkeley.edu/Speech/EARS/rt.htmltry out and consistently use novel input modalitiessuch as voice, including more complex natural lan-guage, and that in particular in this domain, suchmultimodal interaction can help the user find in-formation more efficiently.When developing a language interface for an in-teractive system in a new domain, the Wizard ofOz (WOz) methodology (Dahlba?ck et al, 1993;Salber and Coutaz, 1993) is a very useful tool.The user interacts with what they believe to be afully automated system, when in fact another per-son, a ?wizard?
is simulating the missing or incom-plete NLP modules, typically the speech recogni-tion, natural language understanding and dialoguemanagement modules.
The recorded experimentsprovide valuable information for implementing orfine-tuning these parts of the system.However, the methodology is usually appliedto unimodal (voice-only or keyboard-only) sys-tems, where the elicitation of language data is nota problem since this is effectively the only type ofdata resulting from the experiment.
In our case, weare developing a complex multimodal system.
Wefound that when the Wizard of Oz methodologyis extended to multimodal systems, the number ofvariables that have to be considered and controlledfor in the experiment increases substantially.
Forinstance, if it is the case that within a single inter-face any task that can be performed using naturallanguage can also be performed with other modal-ities, for example a mouse, the user may preferto use the other ?
more familiar ?
modality fora sizeable portion of the experiment.
In order togather a useful amount of natural language data,greater care has to be taken to design the systemin a way that encourages language use.
But, ifthe goal of the experiment is also to study whatmodalities users find more useful in some situa-49Figure 1: The Archivus Interfacetions compared to others, language use must beencouraged without being forced, and finding thisbalance can be very hard to achieve in practice.2 Design and implementationThe Archivus system has been designed to sat-isfy realistic user needs based on a user require-ment analysis (Lisowska, 2003), where subjectswere asked to formulate queries that would enablethem to find out ?what happened at a meeting?.The design of the user interface is based on themetaphor of a person interacting in an archive orlibrary (Lisowska et al, 2004).Furthermore, Archivus is flexibly multimodal,meaning that users can interact unimodally choos-ing one of the available modalities exclusively,or multimodally, using any combination of themodalities.
In order to encourage natural lan-guage interaction, the system gives textual and vo-cal feedback to the user.
The Archivus Interfaceis shown in Figure 1.
A detailed description of allof the components can be found in Lisowska et al(2004).Archivus was implemented within a softwareframework for designing multimodal applicationswith mixed-initiative dialogue models (Cenek etal., 2005).
Systems designed within this frame-work handle interaction with the user througha multimodal dialogue manager.
The dialoguemanager receives user input from all modalities(speech, typing and pointing) and provides mul-timodal responses in the form of graphical, textualand vocal feedback.The dialogue manager contains only linguisticknowledge and interaction algorithms.
Domainknowledge is stored in an SQL database and is ac-cessed by the dialogue manager based on the con-straints expressed by the user during interaction.The above software framework provides sup-port for remote simulation or supervision ofsome of the application functionalities.
This fea-ture makes any application developed under thismethodology well suited for WOz experiments.
Inthe case of Archivus, pilot experiments stronglysuggested the use of two wizards ?
one supervisingthe user?s input (Input Wizard) and the other con-trolling the natural language output of the system(Output Wizard).
Both wizards see the user?s in-put, but their actions are sequential, with the Out-put Wizard being constrained by the actions of theInput Wizard.The role of the Input Wizard is to assure thatthe user?s input (in any modality combination)is correctly conveyed to the system in the formof sets of semantic pairs.
A semantic pair (SP)is a qualified piece of information that the dia-50logue system is able to understand.
For exam-ple, a system could understand semantic pairs suchas date:Monday or list:next.
A user?sutterance ?What questions did this guy ask inthe meeting yesterday??
combined with point-ing on the screen at a person called ?Raymond?could translate to dialogact:Question,speaker:Raymond, day:Monday.In the current version of Archivus, user clicksare translated into semantic pairs automatically bythe system.
Where written queries are concerned,the wizard sometimes needs to correct automat-ically generated pairs due to the currently lowperformance of our natural language understand-ing module.
Finally since the speech recognitionengine has not been implemented yet, the user?sspeech is fully processed by a wizard.
The InputWizard also assures that the fusion of pairs comingfrom different modalities is done correctly.The role of the Output Wizard is to monitor, andif necessary change the default prompts that aregenerated by the system.
Changes are made forexample to smooth the dialogue flow, i.e.
to bet-ter explain the dialogue situation to the user or tomake the response more conversational.
The wiz-ard can select a prompt from a predefined list, ortype a new one during interaction.All wizards?
actions are logged and afterwardsused to help automate the correct behavior of thesystem and to increase the overall performance.3 Collecting natural language dataIn order to obtain a sufficient amount of languagedata from the WOz experiments, several meanshave been used to determine what encouragesusers to speak to the system.
These include givingusers different types of documentation before theexperiment ?
lists of possible voice commands, auser manual, and step-by-step tutorials.
We foundthat the best solution was to give users a tutorialin which they worked through an example usingvoice alone or in combination with other modali-ties, explaining in each step the consequences ofthe user?s actions on the system.
The drawback ofthis approach is that the user may be biased by theexamples and continue to interact according to theinteraction patterns that are provided, rather thandeveloping their own patterns.
These influencesneed to be considered both in the data analysis,and in how the tutorials are written and structured.The actual experiment consists of two parts inwhich the user gets a mixed set of short-answerand true-false questions to solve using the system.First they are only allowed to use a subset of theavailable modalities, e.g.
voice and pen, and thenthe full set of modalities.
By giving the users dif-ferent subsets in the first part, we can compare ifthe enforcement of certain modalities has an im-pact on how they choose to use language when allmodalities are available.On the backend, the wizards can also to someextent have an active role in encouraging languageuse.
The Input Wizard is rather constrained interms of what semantic pairs he can produce, be-cause he is committed to selecting from a set ofpairs that are extracted from the meeting data.For example if ?Monday?
is not a meeting datein the database, the input is interpreted as having?no match?, which generates the system prompt?I don?t understand?.
Here, the Output Wizardcan intervene by replacing that prompt by one thatmore precisely specifies the nature of the problem.The Output Wizard can also decide to replacedefault prompts in situations when they are toogeneral in a given context.
For instance, whenthe user is browsing different sections of a meetingbook (cover page, table of contents, transcript andreferenced documents) the default prompt givesgeneral advice on how to access the different partsof the book, but can be changed to suggest a spe-cific section instead.4 Analysis of elicited language dataThe data collected with Archivus through WOzexperiments provide useful information in severalways.
One aspect is to see the complexity of thelanguage used by users ?
for instance whether theyuse more keywords, multi-word expressions orfull-sentence queries.
This is important for choos-ing the appropriate level of language processing,for instance for the syntactic analysis.
Another as-pect is to see the types of actions performed us-ing language.
On one hand, users can manipulateelements in the graphical interface by expressingcommands that are semantically equivalent withpointing, e.g.
?next page?.
On the other hand,they can freely formulate queries relating to theinformation they are looking for, e.g.
?Did theydecide to put a sofa in the lounge??.
Commandsare interface specific rather than domain specific.From the graphical interface the user can easilypredict what they can say and how the system will51Part 1 condition Pointing LanguageExperiment set 1voice only 91% 9%voice+keyboard 88% 12%keyboard+pointing 66% 34%voice+keyb.+pointing 79% 21%Experiment set 2voice only 68% 32%voice+pointing 62% 38%keyboard+pointing 39% 61%pointing 76% 24%Table 1: Use of each modality in part 2.respond.
Queries depend on the domain and thedata, and are more problematic for the user be-cause they cannot immediately see what types ofqueries they can ask and what the coverage ofthe data is.
But, using queries can be very use-ful, because it allows the user to express them-selves in their own terms.
An important goal of thedata analysis is to determine if the language inter-face enables the user to interact more successfullythan if they are limited to pointing only.
In addi-tion, the way in which the users use language inthese two dimensions has important implicationsfor the dialogue strategy and for the implementa-tion of the language processing modules, for in-stance the speech recognition engine.
A speechrecognizer can be very accurate when trained on asmall, fixed set of commands whereas it may per-form poorly when faced with a wide variety of lan-guage queries.Thus far, we have performed 3 sets of pilotWOz experiments with 40 participants.
The pri-mary aim was to improve and finetune the systemand the WOz environment as a preparation for thedata-collection experiments that we plan to do inthe future.
In these experiments we compared howfrequently users used voice and keyboard in rela-tion to pointing as we progressively changed fea-tures in the system and the experimental setup toencourage language use.
The results between thefirst and the third set of experiments can be seenin table 1, grouped by the subset of modalities thatthe users had in the first part of the experiment.From the table we can see that changes madebetween the different iterations of the systemachieved their goal ?
by the third experiment setwe were managing to elicit larger amounts of nat-ural language data.
Moreover, we noticed that themodality conditions that are available to the userin the first part play a role in the amount of use oflanguage modalities in the second part.5 Conclusions and future workWe believe that the work presented here (both thesystem and the WOz environment and experimen-tal protocol) has now reached a stable stage thatallows for the elicitation of sufficient amounts ofnatural language and interaction data.
The nextstep will be to run a large-scale data collection.The results from this collection should provideenough information to allow us to develop and in-tegrate fairly robust natural language processinginto the system.
Ideally, some of the componentsused in the software framework will be made pub-licly available at the end of the project.ReferencesPavel Cenek, Miroslav Melichar, and Martin Rajman.2005.
A Framework for Rapid Multimodal Appli-cation Design.
In Va?clav Matous?ek, Pavel Mautner,and Toma?s?
Pavelka, editors, Proceedings of the 8thInternational Conference on Text, Speech and Dia-logue (TSD 2005), volume 3658 of Lecture Notesin Computer Science, pages 393?403, Karlovy Vary,Czech Republic, September 12-15.
Springer.Nils Dahlba?ck, Arne Jo?nsson, and Lars Ahrenberg.1993.
Wizard of Oz Studies ?
Why and How.
InDianne Murray Wayne D. Gray, William Hefley, ed-itor, International Workshop on Intelligent User In-terfaces 1993, pages 193?200.
ACM Press.Agnes Lisowska, Martin Rajman, and Trung H. Bui.2004.
ARCHIVUS: A System for Accessing theContent of Recorded Multimodal Meetings.
InIn Procedings of the JOINT AMI/PASCAL/IM2/M4Workshop on Multimodal Interaction and RelatedMachine Learning Algorithms, Bourlard H. & Ben-gio S., eds.
(2004), LNCS, Springer-Verlag, Berlin.,Martigny, Switzerland, June.Agnes Lisowska.
2003.
Multimodal interface designfor the multimodal meeting domain: Preliminary in-dications from a query analysis study.
Project re-port IM2.MDM-11, University of Geneva, Geneva,Switzerland, November.Daniel Salber and Joe?lle Coutaz.
1993.
Applyingthe wizard of oz technique to the study of multi-modal systems.
In EWHCI ?93: Selected papersfrom the Third International Conference on Human-Computer Interaction, pages 219?230, London, UK.Springer-Verlag.52
