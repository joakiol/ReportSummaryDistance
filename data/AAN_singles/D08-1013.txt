Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 117?126,Honolulu, October 2008. c?2008 Association for Computational LinguisticsAdding Redundant Features for CRFs-based Sentence SentimentClassificationJun Zhao, Kang Liu, Gen WangNational Laboratory of Pattern RecognitionInstitute of Automation, Chinese Academy of Sciences, Beijing, 100190, China{jzhao, kliu, gwang}@nlpr.ia.ac.cnAbstractIn this paper, we present a novel methodbased on CRFs in response to the two specialcharacteristics of ?contextual dependency?and ?label redundancy?
in sentence sentimentclassification.
We try to capture the contextualconstraints on sentence sentiment using CRFs.Through introducing redundant labels into theoriginal sentimental label set and organizingall labels into a hierarchy, our method can addredundant features into training for capturingthe label redundancy.
The experimentalresults prove that our method outperforms thetraditional methods like NB, SVM, MaxEntand standard chain CRFs.
In comparison withthe cascaded model, our method caneffectively alleviate the error propagationamong different layers and obtain betterperformance in each layer.1 Introduction*There are a lot of subjective texts in the web, suchas product reviews, movie reviews, news,editorials and blogs, etc.
Extracting thesesubjective texts and analyzing their orientationsplay significant roles in many applications such aselectronic commercial, etc.
One of the mostimportant tasks in this field is sentiment* Contact: Jun ZHAO, jzhao@nlpr.ia.ac.cnclassification, which can be performed in severallevels: word level, sentence level, passage level,etc.
This paper focuses on sentence level sentimentclassification.Commonly, sentiment classification containsthree layers of sub-tasks.
From upper to lower, (1)Subjective/Objective classification: the subjectivetexts are extracted from the corpus teeming withboth subjective and objective texts.
(2) Polarityclassification: a subjective text is classified into?positive?
or ?negative?
according to thesentimental expressions in the text.
(3) Sentimentalstrength rating: a subjective text is classified intoseveral grades which reflect the polarity degree of?positive?
or ?negative?.
It is a special multi-classclassification problem, where the classes areordered.
In machine learning, this kind of problemis also regarded as an ordinal regression problem(Wei Wu et al 2005).
In this paper, we mainlyfocus on this problem in sentiment classification.Sentiment classification in sentence level has itsspecial characteristics compared with traditionaltext classification tasks.
Firstly, the sentiment ofeach sentence in a discourse is not independent toeach other.
In other words, the sentiment of eachsentence is related to those of other adjacentsentences in the same discourse.
The sentiment ofa sentence may vary in different contexts.
If wedetach a sentence from the context, its sentimentmay not be inferred correctly.
Secondly, there isredundancy among the sentiment classes,117especially in sentimental strength classes.
Forexample:?I love the scenario of ?No country for old man?very much!!?
?This movie sounds good.
?The first sentence is labeled as ?highly praised?class and the second one is labeled as ?somethinggood?
class.
Both the sentences express positivesentiment for the movie, but the former expressesstronger emotion than the latter.
We can see thatboth ?highly praised?
and ?something good?belong to an implicit class ?positive?, which can beregarded as the relation between them.
If we addthese implicit classes in the label set, the sentimentclasses will form a hierarchical structure.
Forexample, ?positive?
can be regarded as the parentclass of ?highly praised?
and ?something good?,?subjective?
can be regarded as the parent class of?positive?
and ?negative?.
This implicithierarchical structure among labels should not beneglected because it may be beneficial forimproving the accuracy of sentiment classification.In the paper, we call this characteristic ofsentiment classification as ?label redundancy?.Unfortunately, in our knowledge most of thecurrent research treats sentiment classification as atraditional multi-classification task or an ordinalregression task, which regard the sentimentalclasses being independent to each other and eachsentence is also independent to the adjacentsentences in the context.
In other words, theyneglect the contextual information and theredundancy among sentiment classes.In order to consider the contextual information inthe process of the sentence sentiment classification,some research defines contextual features andsome uses special graph-based formulation, like(Bo Pang, et al 2005).
In order to consider thelabel redundancy, one potential solution is to use acascaded framework which can combinesubjective/objective classification, polarityclassification and sentimental strengthclassification together, where the classificationresults of the preceding step will be the input of thesubsequent one.
However, the subsequentclassification cannot provide constraint andcorrection to the results of the preceding step,which will lead to the accumulation andpropagation of the classification errors.
As a result,the performance of sentiment analysis of sentencesis often not satisfactory.This paper focuses on the above two specialcharacteristics of the sentiment classificationproblem in the sentence level.
To the firstcharacteristic, we regard the sentimentclassification as a sequence labeling problem anduse conditional random field (CRFs) model tocapture the relation between two adjacentsentences in the context.
To the secondcharacteristic, we propose a novel method based ona CRF model, in which the original task is mappedto a classification on a hierarchical structure, whichis formed by the original label set and someadditional implicit labels.
In the hierarchicalclassification framework, the relations between thelabels can be represented as the additional featuresin classification.
Because these features are relatedto the original labels but unobserved, we namethem as ?redundant features?
in this paper.
Theycan be used to capture the redundant andhierarchical relation between different sentimentclasses.
In this way, not only the performance ofsentimental strength rating is improved, theaccuracies of subjective/objective classificationand polarity classification are also improvedcompared with the traditional sentimentclassification method.
And in comparison with thecascaded method, the proposed approach caneffectively alleviate error propagation.
Theexperimental results on movie reviews prove thevalidity of our method.2 Capturing Contextual Influence forSentiment ClassificationFor capturing the influence of the contexts to thesentiment of a sentence, we treat original sentimentclassification as a sequence labeling problem.
Weregard the sentiments of all the sentencesthroughout a paragraph as a sequential flow ofsentiments, and we model it using a conditionalmodel.
In this paper, we choose ConditionalRandom Fields (CRFs) (Lafferty et al 2001)because it has better performance than othersequence labeling tools in most NLP applications.CRFs are undirected graphical models used tocalculate the conditional probability of a set oflabels given a set of input variables.
We cite thedefinitions of CRFs in (Lafferty et al 2001).
Itdefines the conditional probability proportional tothe product of potential functions on cliques of thegraph,118exp ( , )( | )( )F Y XP Y XZ X??
?=      (1)where X is a set of input random variables and Y isa set of random labels.
( , )F Y X is an arbitraryfeature function over its arguments, ?
is a learnedweight for each feature function and( ) exp( ( , ))yXZ F Y X?= ??
.The training of CRFs is based on MaximumLikelihood Principle (Fei Sha et al 2003).
The loglikelihood function is[ ]( ) ( , ) log ( )k k kkL F Y X Z X??
?= ?
?
?Therefore, Limited-memory BFGS (L-BFGS)algorithm is used to find this nonlinearoptimization parameters.3 Label Redundancy in SentimentClassificationIn this section, we explain the ?label redundancy?in sentiment classification mentioned in the firstsection.
We will analyze the effect of the labelredundancy on the performance of sentimentclassification from the experimental view.We conduct the experiments of polarityclassification and sentimental strength rating on thecorpus which will be introduced in section 5 later.The class set is also illustrated in that section.Polarity classification is a three-class classificationprocess, and sentimental strength rating is a five-class classification process.
We use first 200reviews as the training set which contains 6,079sentences, and other 49 reviews, totally 1,531sentences, are used as the testing set.
Both thethree-class classification and the five-classclassification use standard CRFs model with thesame feature set.
The results are shown in Table 1,2 and 3, where ?Answer?
denotes the results givenby human, ?Results?
denotes the results given by?CRFs model ?Correct?
denotes the number ofcorrect samples which is labeled by CRFs model.We use precision, recall and F1 value as theevaluation metrics.Table 1 gives the result of sentimental strengthrating.
Table 2 shows the polarity classificationresults extracted from the results of sentimentalstrength rating in Table 1.
The extraction process isas follows.
In the sentimental strength ratingresults, we combine the sentences with ?PP?
classand the sentences with ?P?
class into ?Pos?
class,and the sentences with ?NN?
class and thesentences with ?N?
class into ?Neg?
class.
So theresults of five-class classification are transformedinto the results of three-class classification.
Table 3is the results of performing polarity classificationin the data set by CRFs directly.Label Answer Results Correct Precision Recall F1PP 51 67 5 0.0746 0.0980 0.0847P 166 177 32 0.1808 0.1928 0.1866Neu 1190 1118 968 0.8658 0.81.34 0.8388N 105 140 25 0.1786 0.2381 0.2041NN 19 29 1 0.0345 0.0526 0.0417Total 1531 1531 1031 0.67.34 0.6734 0.6734Table 1.
Result of Sentimental Strength RatingLabel Answer Results Correct Precision Recall F1Pos 217 244 79 0.3238 0.3641 0.3427Neu 1190 1118 968 0.8658 0.8134 0.8388Neg 124 169 41 0.2426 0.3306 0.2799Total 1531 1531 1088 0.7106 0.7106 0.7106Table 2.
Result of Polarity Classification Extracted from Table 1.Label Answer Results Correct Precision Recall F1Pos 217 300 108 0.3600 0.4977 0.4178Neu 1190 1101 971 0.8819 0.8160 0.8477Neg 124 130 40 0.3077 0.3226 0.3150Total 1531 1531 1119 0.7309 0.7309 0.7309Table 3.
Result of Polarity Classification119From the results we can find the followingphenomena.
(1) The corpus is severely unbalanced, theobjective sentences take the absolute majority inthe corpus, which leads to the poor accuracy forclassifying subjective sentences.
The experiment inTable 1 puts polarity classification and sentimentalstrength rating under a unique CRFs model,without considering the redundancy andhierarchical structure between different classes.
Asa result, the features for polarity classification willusually cover the features for sentimental strengthrating.
These reasons can explain why there is onlyone sample labeled as ?NN?
correctly and only 5samples labeled as ?PP?
correctly.
(2) Comparing Table 2 with 3, we can find that,the F1 value of the polarity classification resultsextracted from sentimental strength rating results islower than that of directly conducting polarityclassification.
That is because the redundancybetween sentimental strength labels makes theclassifier confused to determine the polarity of thesentence.
Therefore, we should deal with thesentiment analysis in a hierarchical frame whichcan consider the redundancy between the differentclasses and make full use of the subjective andpolarity information implicitly contained insentimental strength classes.4 Capturing Label Redundancy for CRFsvia Adding Redundant FeaturesAs mentioned above, it?s important for a classifierto consider the redundancy between differentlabels.
However, from the standard CRFsdescribed in formula (1), we can see that thetraining of CRFs only maximizes the probabilitiesof the observed labels Y  in the training corpus.Actually, the redundant relation between sentimentlabels is unobserved.
The standard CRFs still treatseach class as an isolated item so that itsperformance is not satisfied.In this section, we propose a novel method forsentiment classification, which can capture theredundant relation between sentiment labelsthrough adding redundant features.
In thefollowing, we firstly show how to add theseredundant features, then illustrate thecharacteristics of this method.
After that, for thesentiment analysis task, the process of featuregeneration will be presented.4.1 Adding Redundant Features for CRFsAdding redundant features has two steps.
Firstly,an implicit redundant label set is designed, whichcan form a multi-layer hierarchical structuretogether with the original labels.
Secondly, in thehierarchical classification framework, the implicitlabels, which reflect the relations between theoriginal labels, can be used as redundant featuresin the training process.
We will use the followingexample to illustrate the first step for sentimentalstrength rating task.For the task of sentimental strength rating, theoriginal label set is {?PP (highly praised)?, ?P(something good)?, ?Neu (objective description)?,?N (something that needs improvement)?
and ?NN(strong aversion)?}.
In order to introduceredundant labels, the 5-class classification task isdecomposed into the following three layers shownin Figure 1.
The label set in the first layer is{?subjective?, ?objective?
}, The label set in thesecond layer is for polarity classification{?positive?, ?objective?, ?negative?
}, and the labelset in the third layer is the original set.
Actually,the labels in the first and second layers areunobserved redundant labels, which will not bereflected in the final classification result obviously.Figure 1.
The hierarchical structure of sentimental labelsIn the second step, with these redundant labels,some implicit features can be generated for CRFs.So the standard CRFs can be rewritten as follows.The first layerThe third layerThe second layerSentiment AnalysisSubjective ObjectivePositive NegativeP PP N NNObjectiveObjective12011exp( ( , ) )( | )( )exp( ( , ) )exp( ( , ) )Tmj j jjmj j jT jF X TP T XZ XF X YF X Y???==?=?=???
?
(2)where 1 2( ), ,... ...,j mT Y Y Y Y= , and jY denotes thelabel sequence in the jth layer.
( , )j jF X Y denotesthe arbitrary feature function in the jth layer.From the formula (2), we can see that theoriginal label set is rewritten as1 2( ), ,... ...,j mT Y Y Y Y= , which contains implicitlabels in the hierarchical structure shown in Figure1.
The difference between our method and thestandard chain CRFs is that we make some implicitredundant features to be active when training.
Theoriginal feature function ( , )F Y X is replaced by1( , )mj jjF X Y=?
.
We use an example to illustrate theprocess of feature generation.
When a sentenceincluding the word ?good?
is labeled as ?PP?, ourmodel not only generate the state feature (good,?PP?
), but also two implicit redundant state feature(good, ?positive?)
and (good, ?subjective?
).Through adding larger-granularity labels ?positive?and ?negative?
into the model, our method canincrease the probability of ?positive?
and decreasethe probability of ?negative?.
Furthermore, ?P?
and?PP?
will share the probability gain of ?positive?,therefore the probability of ?P?
will be larger thanthat of ?N?.
For the transition feature, the samestrategy is used.
Therefore the complexity of itstraining procedure is ( )mjjO M N F l?
?
??
where Mis the number of the training samples, N is theaverage sentence length, jF  is the average numberof activated features in the jth layer, l  is thenumber of the original labels and m is the numberof the layers.
For the complexity of the decodingprocedure, our method has ( )mjjO N F l?
??
.It?s worth noting that, (1) transition features areextracted in each layer separately rather thanacross different layers.
For example, feature (good,?subjective?, ?positive?)
will never be extractedbecause ?subjective?
and ?positive?
are fromdifferent layers; (2) if one sentence is labeled as?Neu?, no implicit redundant features will begenerated.4.2 The Characteristics of Our MethodOur method allows that the label sets aredependent and redundant.
As a result, it canimprove the performance of not only the classifierfor the original sentimental strength rating task, butalso the classifiers for other tasks in thehierarchical frame, i.e.
polarity classification andsubjective/objective classification.
This kind ofdependency and redundancy can lead to twocharacteristics of the proposed method forsentiment classification compared with traditionalmethods, such as the cascaded method.
(1) Error-correction: Two dependent tasks in theneighboring layers can correct the errors of eachother relying on the inconsistent redundantinformation.
For example, if in the first layer, thefeatures activated by ?objective?
get larger scoresthan the features activated by ?subjective?, and inthe second layer the features activated by?positive?
get larger scores than the featuresactivated by ?objective?, then inconsistencyemerges.
At this time, our method can globallyselect the label with maximum probability.
Thischaracteristic can make up the deficiency of thecascaded method which may induce errorpropagation.
(2) Differentiating the ordinal relation amongsentiment labels: Our method organizes the ordinalsentiment labels into a hierarchy throughintroducing redundant labels into standard chainCRFs, in this way the degree of classificationerrors can be controlled.
In the different layers ofsentiment analysis task, the granularities ofclassification are different.
Therefore, when anobservation cannot be correctly labeled on asmaller-granularity label set, our method will usethe larger-granularity labels in the upper layer tocontrol the final classification labels.4.3 Feature Selection in Different LayersFor feature selection, our method selects differentfeatures for each layer in the hierarchical frame.In the top layer of the frame shown in Figure 1,for subjective/objective classification task, we use121not only adjectives and the verbs which containsubjective information (e.g., ?believe?, ?think?)
asthe features, but also the topic words.
The topicwords are defined as the nouns or noun phaseswhich frequently appear in the corpus.
We believethat some topic words contain subjectiveinformation.In the middle and bottom layers, we not only usethe features in the first layer, but also some specialfeatures as follows.
(1) The prior orientation scores of the sentimentwords: Firstly, a sentiment lexicon is generated byextending the synonymies and antonyms inWordNet2 from a positive and negative seed list.Then, the positive score and the negative score of asentiment word are individually accumulated andweighted according to the polarity of itssynonymies and antonyms.
At last we scale thenormalized distance of the two scores into 5 levels,which will be the prior orientation of the word.When there is a negative word, like {not, no, can?t,merely, never, ?
}, occurring nearby the featureword in the range of 3 words size window, theorientation of this word will be reversed and ?NO?will be added in front of the original feature wordfor creating a new feature word.
(2) Sentence transition features: We consider twotypes of sentence transition features.
The first typeis the conjunctions and the adverbs occurring in thebeginning of this sentence.
These conjunctions andadverbs are included in a word list which ismanually selected, like {and, or, but, though,however, generally, contrarily, ?}.
The secondtype of the sentence transition feature is theposition of the sentence in one review.
The reasonlies in that: the reviewers often follow somewriting patterns, for example some reviewersprefer to concede an opposite factor beforeexpressing his/her real sentiment.
Therefore, wedivide a review into five parts, and assign eachsentence with the serial number of the part whichthe sentence belongs to.5 Experiments5.1 Data and BaselinesIn order to evaluate the performance of our method,we conducted experiments on a sentence level2 http://wordnet.princeton.edu/annotation corpus obtained from Purdue University,which is also used in (Mao and Lebanon 07).
Thiscorpus contains 249 movie reviews and 7,610sentences totally, which is randomly selected fromthe Cornell sentence polarity dataset v1.0.
Eachsentence was hand-labeled with one of five classes:PP (highly praised), P (something good), Neu(objective description), N (something that needsimprovement) and NN (strong aversion), whichcontained the orientation polarity of each sentence.Based on the 5-class manually labeled resultsmentioned above, we also assigned each sentencewith one of three classes: Pos (positive polarity),Neu (objective description), Neg (negativepolarity).
Data statistics for the corpus are given inTable 4.Pos Neu NegLabelPP P Neu N NNTotal5 classes 383 860 5508 694 165 76103 classes 1243 5508 859 7610Table 4.
Data Statistics for Movies ReviewsCorpusThere is a problem in the dataset that more than70% of the sentences are labeled as ?Neu?
andlabels are seriously unbalanced.
As a result, the?Neu?
label is over-emphasized.
For this problem,Mao and Lebanon (2007) made a balanced data set(equal number sentences for different labels) whichis sampled in the original corpus.
Since randomlysampling sentences from the original corpus willbreak the intrinsic relationship between twoadjacent sentences in the context, we don?t createbalanced label data set.For the evaluation of our method, we chooseaccuracy as the evaluation metrics and someclassical methods as the baselines.
They are Na?veBayes (NB), Support Vector Machine (SVM),Maximum Entropy (MaxEnt) (Kamal Nigam et al1999) and standard chain CRFs (Fei et al 2003).We also regard cascaded-CRFs as our baseline forcomparing our method with the cascaded-basedmethod.
For NB, we use Laplace smoothingmethod.
For SVM, we use the LibSVM3  with alinear kernel function4.
For MaxEnt, we use theimplementation in the toolkit Mallet5.
For CRFs,3 http://www.csie.ntu.tw/~cjlin/libsvm4 http://svmlight.joachims.org/5 http://mallet.cs.umass.edu/index.php/Main_Page122Label NB SVM MaxEnt Standard CRF Cascaded CRF Our MethodPP 0.1745 0.2219 0.2055 0.2027 0.2575 0.2167P 0.2049 0.2877 0.2353 0.2536 0.2881 0.3784Neu 0.8083 0.8685 0.8161 0.8273 0.8554 0.8269N 0.2636 0.3014 0.2558 0.2981 0.3092 0.4204NN 0.0976 0.1162 0.1148 0.1379 0.1510 0.2967Total 0.6442 0.6786 0.6652 0.6856 0.7153 0.7521Table 5.
The accuracy of Sentimental Strength RatingLabel NB SVM MaxEnt Standard CRF Cascaded-CRF Our MethodPos 0.4218 0.4743 0.4599 0.4405 0.5122 0.6008Neu 0.8147 0.8375 0.8424 0.8260 0.8545 0.8269Neg 0.3217 0.3632 0.2739 0.3991 0.4067 0.5481Total 0.7054 0.7322 0.7318 0.7327 0.7694 0.7855Table 6?The Results of Polarity ClassificationLabel NB SVM MaxEnt Standard CRF Our MethodSubjective 0.4743 0.5847 0.4872 0.5594 0.6764Objective 0.8170 0.8248 0.8212 0.8312 0.8269Total 0.7238 0.7536 0.7518 0.7561 0.8018Table 7.
The accuracy of Subjective/Objective Classificationwe use the implementation in Flex-CRFs6.
We setthe iteration number to 120 in the training processof the method based on CRFs.
In the cascadedmodel we set 3 layers for sentimental strengthrating, where the first layer is subjective/objectiveclassification, the second layer is polarityclassification and the last layer is sentimentalstrength classification.
The upper layer passes theresults as the input to the next layer.5.2 Sentimental Strength RatingIn the first experiment, we evaluate theperformance of our method for sentimentalstrength rating.
Experimental results for eachmethod are given in Table 5.
We not only give theoverall accuracy of each method, but also theperformance for each sentimental strength label.All baselines use the same feature space mentionedin section 4.3, which combine all the features inthe three layers together, except cascaded CRFsand our method.
In cascaded-CRFs and our method,we use different features in different layersmentioned in section 4.3.
These results weregathered using 5-fold cross validation with onefold for testing and the other 4 folds for training.From the results, we can obtain the followingconclusions.
(1) The three versions of CRFsperform consistently better than Na?ve Bayes,6 http://flexcrfs.sourceforge.netSVM and MaxEnt methods.
We think that isbecause CRFs model considers the contextualinfluence of each sentence.
(2) Comparing theperformance of cascaded CRFs with that ofstandard sequence CRFs, we can see that not onlythe overall accuracy but also the accuracy for eachsentimental strength label are improved, where theoverall accuracy is increased by 3%.
It proves thattaking the hierarchical relationship between labelsinto account is very essential for sentimentclassification.
The reason is that: the cascadedmodel performs sentimental strength rating in threehierarchical layers, while standard chain CRFsmodel treats each label as an independentindividual.
So the performance of the cascadedmodel is superior to the standard chain CRFs.
(3)The experimental results also show that ourmethod performs better than the Cascaded CRFs.The classification accuracy is improved from71.53% to 75.21%.
We think that is because ourmethod adds the label redundancy among thesentimental strength labels into considerationthrough adding redundant features into the featuresets, and the three subtasks in the cascaded modelare merged into a unified model.
So the outputresult is a global optimal result.
In this way, theproblem of error propagation in the cascaded framecan be alleviated.1235.3 Sentiment Polarity ClassificationIn the second experiment, we evaluate theperformance of our method for sentiment polarityclassification.
Our method is based on ahierarchical frame, which can perform differenttasks in different layers at the same time.
Forexample, it can determine the polarity of sentenceswhen sentimental strength rating is performed.Here, the polarity classification results of ourmethod are extracted from the results of thesentimental strength rating mentioned above.
In thesentimental strength rating results, we combine thesentences with PP label and the sentences with Plabel into one set, and the sentences with NN labeland the sentences with N label into one set.
So theresults of 5-class classification are transformed intothe results of 3-class classification.
Other methodslike NB, SVM, MaxEnt, standard chain CRFsperform 3-class classification directly, and theirlabel sets in the training corpus is {Pos, Neu, Neg}.The parameter setting is the same as sentimentalstrength rating.
For the cascaded-CRFs method, wefirstly perform subjective/objective classification,and then determine the polarity of the sentencesbased on the subjective sentences.
Theexperimental results are given in Table 6.From the experimental results, we can obtain thefollowing conclusion for sentiment polarityclassification, which is similar to the conclusionfor sentimental strength rating mentioned insection 5.2.
That is both our model and thecascaded model can get better performance thanother traditional methods, such as NB, SVM,MaxEnt, etc.
But the performance of the cascadedCRFs (76.94%) is lower than that of our method(78.55%).
This indicates that because our methodexploits the label redundancy in the different layers,it can increase the accuracies of both polarityclassification and sentimental strength rating at thesame time compared with other methods.5.4 Subjective/Objective ClassificationIn the last experiment, we test our method forsubjective/objective classification.
Thesubjective/objective label of the data is extractedfrom its original label like section 5.3.
As the sameas the experiment for polarity classification, allbaselines perform subjective/objectiveclassification directly.
It?s no need to perform thecascaded-based method because it?s a 2-class task.The results of our method are extracted from theresults of the sentimental strength rating too.
Theresults are shown in Table 7.
From it, we canobtain the similar conclusion, i.e.
our methodoutperforms other methods and has the 80.18%classification accuracy.
Our method, whichintroduces redundant features into training, canincrease the accuracies of all tasks in the differentlayers at the same time compared with otherbaselines.
It proves that considering labelredundancy are effective for promoting theperformance of a sentimental classifier.6 Related WorksRecently, many researchers have devoted into theproblem of the sentiment classification.
Most ofresearchers focus on how to extract useful textualfeatures (lexical, syntactic, punctuation, etc.)
fordetermining the semantic orientation of thesentences using machine learning algorithm (Bo etal.
2002; Kim and Hovy, 2004; Bo et al 2005, Huet al 2004; Alina et al2008; Alistair et al2006).But fewer researchers deal with this problem usingCRFs model.For identifying the subjective sentences, thereare several research, like (Wiebe et al 2005).
Forpolarity classification on sentence level, (Kim andHovy, 2004) judged the sentiment by classifying apseudo document composed of synonyms ofindicators in one sentence.
(Pang and Lee, 04)proposed a semi-supervised machine learningmethod based on subjectivity detection andminimum-cut in graph.Cascaded models for sentiment classificationwere studied by (Pang and Lee, 2005).
Their workmainly used the cascaded frame for determiningthe orientation of a document and the sentences.
Inthat work, an initial model is used to determine theorientation of each sentence firstly, then the topsubjective sentences are input into a document -level model to determine the document?sorientation.The CRFs has previously been used forsentiment classification.
Those methods based onCRFs are related to our work.
(Mao et al 2007)used a sequential CRFs regression model tomeasure the polarity of a sentence in order todetermine the sentiment flow of the authors inreviews.
However, this method must manually124select a word set for constraints, where eachselected word achieved the highest correlation withthe sentiment.
The performance of isotonic CRFsis strongly related to the selected word set.
(McDonald et al2007; Ivan et al2008) proposed astructured model based on CRFs for jointlyclassifying the sentiment of text at varying levelsof granularity.
They put the sentence level anddocument level sentiment analysis in an integratedmodel and employ the orientation of the documentto influence the decision of sentence?s orientation.Both the above two methods didn?t consider theredundant and hierarchical relation betweensentimental strength labels.
So their methodscannot get better results for the problem mentionedin this paper.Another solution to this problem is to use a jointmulti-layer model, such as dynamic CRFs, multi-layer CRFs, etc.
Such kind of models can treat thethree sub-tasks in sentiment classification as amulti-task problem and can use a multi-layer orhierarchical undirected graphic to model thesentiment of sentences.
The main differencebetween our method and theirs is that we considerthe problem from the feature representation view.Our method expands the feature set according tothe number of layers in the hierarchical frame.
Sothe complexity of its decoding procedure is lowerthan theirs, for example the complexity of themulti-layer CRFs is ( )jjlO N F?
??
whendecoding and our method only has ( )jjFO N l?
??
,where N is the average sentence length, jF  is theaverage number of activated features in the jth layer,l  is the number of the original labels.7 Conclusion and Future WorkIn the paper, we propose a novel method forsentiment classification based on CRFs in responseto the two special characteristics of ?contextualdependency?
and ?label redundancy?
in sentencesentiment classification.
We try to capture thecontextual constraints on the sentence sentimentusing CRFs.
For capturing the label redundancyamong sentiment classes, we generate ahierarchical framework through introducingredundant labels, under which redundant featurescan be introduced.
The experimental results provethat our method outperforms the traditionalmethods (like NB, SVM, ME and standard chainCRFs).
In comparison with cascaded CRFs, ourmethod can effectively alleviate error propagationamong different layers and obtain betterperformance in each layer.For our future work, we will explore otherhierarchical models for sentimental strength ratingbecause the experiments presented in this paperprove this hierarchical frame is effective forordinal regression.
We would expand the idea inthis paper  into other models, such as Semi-CRFsand Hierarchical-CRFs.AcknowledgmentsThe work is supported by the National NaturalScience Foundation of China under Grants no.60673042, the Natural Science Foundation ofBeijing under Grants no.
4073043 and the NationalHigh Technology Development 863 Program ofChina under Grants no.
2006AA01Z144.ReferencesAlina A. and Sabine B.
2008.
When Specialists andGeneralists Work Together: Overcoming DomainDependence in Sentiment Tagging.
In Proc.
of ACL-08Alistair Kennedy and Diana Inkpen.
2006.
SentimentClassification of Movie Reviews Using ContextualValence Shifters.
Computational Intelligence, 22(2),pages 110-125Bo Pang, Lillian Lee and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification usingmachine learning techniques.
In Proceedings ofEMNLP 2002, pp.79-86.Bo Pang and Lillian Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proceedings of ACL 2004,pp.271-278.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploitingclass relationships for sentiment categorization withrespect to rating scales.
In Proceedings of ACL 2005,pp.115-124.Ivan Titov and Ryan McDonald.
2008.
A Joint Model of Text and Aspect Ratings of SentimentSummarization.
In Proceedings of ACL-08, pages308-316125Janyce Webie, Theresa Wilson and Claire Cardie.
2005.Annotating expressions of opinions and emotions inlauguage.
Language Resources and Evaluation 2005Fei Sha and Fernando Pereira, 2003 Shallow Parsingwith Conditional Random Fields, In ProceedingsofHLT-NAACL 2003, Edmonton, Canada, pp.
213-220.Kim, S and Edward H. Hovy.
2004.
Determining theSentiment of Opinions.
In Proceedings of COLING-04.Kamal Nigam, John Lafferty and Andrew McCallum.1999.
Using Maximum Entropy for TextClassification.
In Proceedings of IJCAI Workshop onMachine Learning for Information Filtering, pages61-67.J Lafferty, A McCallum, F Pereira.
2001.
Conditionalrandom fields: Probabilistic models for segmentingand labeling sequence data.
In Proceedings ofICML-01,  pages 282.289.L.
Zhuang, F. Jing, and X.Y.
Zhu.
2006.
Movie reviewmining and summarization.
In Proceedings of the15th ACM international conference on Informationand knowledge management (CIKM), pages 43-50.M.
Hu and B. Liu.
2004a.
Mining and summarizingcustomer reviews.
In Proceedings of the 2004 ACMSIGKDD international conference on Knowledgediscovery and data mining, pages 168-177.Ryan McDonald, Kerry Hannan and Tyler Neylon et alStructured Models for Fine-to-Coarse SentimentAnalysis.
In Proceedings of ACL 2007, pp.
432-439.Wei Wu, Zoubin Ghahraman, 2005.
GaussianProcesses for Oridinal Regression.
The Journal ofMachine learning Research, 2005Y.
Mao and G. Lebanon, 2007.
Isotonic ConditionalRandom Fields and Local Sentiment Flow.
Advancesin Neural Information Processing Systems 19, 2007126
