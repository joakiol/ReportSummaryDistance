Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
132?140, Prague, June 2007. c?2007 Association for Computational LinguisticsProbabilistic Models of Nonprojective Dependency TreesDavid A. SmithDepartment of Computer ScienceCenter for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, MD 21218 USAdasmith@cs.jhu.eduNoah A. SmithLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213 USAnasmith@cs.cmu.eduAbstractA notable gap in research on statistical de-pendency parsing is a proper conditionalprobability distribution over nonprojectivedependency trees for a given sentence.
Weexploit the Matrix Tree Theorem (Tutte,1984) to derive an algorithm that efficientlysums the scores of all nonprojective treesin a sentence, permitting the definition ofa conditional log-linear model over trees.While discriminative methods, such as thosepresented in McDonald et al (2005b), ob-tain very high accuracy on standard de-pendency parsing tasks and can be trainedand applied without marginalization, ?sum-ming trees?
permits some alternative tech-niques of interest.
Using the summing al-gorithm, we present competitive experimen-tal results on four nonprojective languages,for maximum conditional likelihood estima-tion, minimum Bayes-risk parsing, and hid-den variable training.1 IntroductionRecently dependency parsing has received renewedinterest, both in the parsing literature (Buchholzand Marsi, 2006) and in applications like translation(Quirk et al, 2005) and information extraction (Cu-lotta and Sorensen, 2004).
Dependency parsing canbe used to provide a ?bare bones?
syntactic struc-ture that approximates semantics, and it has the addi-tional advantage of admitting fast parsing algorithms(Eisner, 1996; McDonald et al, 2005b) with a neg-ligible grammar constant in many cases.The latest state-of-the-art statistical dependencyparsers are discriminative, meaning that they arebased on classifiers trained to score trees, given asentence, either via factored whole-structure scores(McDonald et al, 2005a) or local parsing decisionscores (Hall et al, 2006).
In the works cited, thesescores are not intended to be interpreted as proba-bilistic quantities.Here we consider weighted dependency parsingmodels that can be used to define well-formed con-ditional distributions p(y | x), for dependencytrees y and a sentence x.
Conditional distribu-tions over outputs (here, trees) given inputs (here,sentences) have certain advantages.
They per-mit marginalization over trees to compute poste-riors of interesting sub-events (e.g., the probabil-ity that two noun tokens bear a relation, regard-less of which tree is correct).
A probability modelpermits alternative decoding procedures (Goodman,1996).
Well-motivated hidden variable trainingprocedures (such as EM and conditional EM) arealso readily available for probabilistic models.
Fi-nally, probability models can be chained together (asin a noisy channel model), mixed, or combined in aproduct-of-experts.Sequence models, context-free models, and de-pendency models have appeared in several guises;a cross-model comparison clarifies the contributionof this paper.
First, there were generative, stochas-tic models like HMMs, PCFGs, and Eisner?s (1996)models.
Local discriminative classifiers were pro-posed by McCallum et al (2000) for sequence mod-eling, by Ratnaparkhi et al (1994) for constituentparsing, and by Hall et al (2006) (among others) for132dependencies.
Large-margin whole-structure mod-els were proposed for sequence labeling by Al-tun et al (2003), for constituents by Taskar et al(2004), and for dependency trees by McDonald etal.
(2005a).
In this paper, we propose a modelmost similar to the conditional random fields?interpretable as log-linear models?of Lafferty et al(2001), which are now widely used for sequence la-beling.
Log-linear models have been used in pars-ing by Riezler et al (2000) (for constraint-basedgrammars) and Johnson (2001) and Miyao and Tsu-jii (2002) (for CFGs).
Like McDonald et al, we usean edge-factored model that permits nonprojectivetrees; like Lafferty et al, we argue for an alternativeinterpretation as a log-linear model over structures,conditioned on the observed sentence.In Section 2 we point out what would be required,computationally, for conditional training of nonpro-jective dependency models.
The solution to the con-ditionalization problem is given in Section 3, using awidely-known but newly-applied Matrix Tree Theo-rem due to Tutte (1984), and experimental results arepresented with a comparison to the MIRA learningalgorithm used by McDonald et al (2005a).
We goon to describe and experiment with two useful appli-cations of conditional modeling: minimum Bayes-risk decoding (Section 4) and hidden-variable train-ing by conditional maximum likelihood estimation(Section 5).
Discussion in Section 6 considers theimplications of our experimental results.Two indepedent papers, published concurrentlywith this one, report closely related results to ours.Koo et al (2007) and McDonald and Satta (2007)both describe how the Matrix Tree Theorem can beapplied to computing the sum of scores of edge-factored dependency trees and the edge marginals.Koo et al compare conditional likelihood training(as here) to the averaged perceptron and a max-imum margin model trained using exponentiated-gradient (Bartlett et al, 2004); the latter requiresthe same marginalization calculations as conditionallog-linear estimation.
McDonald and Satta discuss avariety of applications (including minimum Bayes-risk decoding) and give complexity results for non-edge-factored models.
Interested readers are re-ferred to those papers for further discussion.2 Conditional Training for NonprojectiveDependency ModelsLet x = ?x1, ..., xn?
be a sequence of words (possi-bly with POS tags, lemmas, and morphological in-formation) that are the input to a parser.
y will referto a directed, unlabeled dependency tree, which is amap y : {1, ..., n} ?
{0, ..., n} from child indicesto parent indices; x0 is the invisible ?wall?
symbol.Let Yx be the set of valid dependency trees for x. Inthis paper, Yx is equivalent to the set of all directedspanning trees over x.1A conditional model defines a family of probabil-ity distributions p(y | x), for all x and y ?
Yx.
Wepropose that this model take a log-linear form:p~?
(y | x) =e~??~f(x,y)?y??Yxe~??~f(x,y?)=e~??~f(x,y)Z~?
(x)(1)where ~f is a feature vector function on parsed sen-tences and ~?
?
Rm parameterizes the model.
Fol-lowing McDonald et al (2005a), we assume that thefeatures are edge-factored:~f(x,y) =n?i=1~f(x, xi, xy(i)) (2)In other words, the dependencies between words inthe tree are all conditionally independent of eachother, given the sequence x and the fact that theparse is a spanning tree.
Despite the constraints theyimpose on features, edge-factored models have theadvantage of tractable O(n3) inference algorithmsor, with some trickery, O(n2) maximum a posteriori(?best parse tree?)
inference algorithms in the non-projective case.
Exact nonprojective inference andestimation become intractable if we break edge fac-toring (McDonald and Pereira, 2006).We wish to estimate the parameters ~?
by maxi-mizing the conditional likelihood (like a CRF) rather1To be precise, every word has in-degree 1, with the soleedge pointing from the word?s parent, xy(i) ?
xi.
x0 has in-degree 0.
By definition, trees are acyclic.
The edges need notbe planar and may ?cross?
in the plane, since we do not have aprojectivity constraint.
In some formulations, exactly one nodein x can attach to x0; here we allow multiple nodes to attachto x0, since this occurs with some frequency in many existingdatasets.
Summation over trees where x0 has exactly one childis addressed directly by Koo et al (2007).133than the margin (McDonald et al, 2005a).
For anempirical distribution p?
given by a set of training ex-amples, this means:max~??x,yp?(x,y)(~?
?
~f(x,y))??xp?
(x) logZ~?
(x)(3)This optimization problem is typically solved us-ing a quasi-Newton numerical optimization methodsuch as L-BFGS (Liu and Nocedal, 1989).
Such amethod requires the gradient of the objective func-tion, which for ?k is given by the following differ-ence in expectations of the value of feature fk:??
?k= (4)Ep?
(X,Y) [fk(X,Y)] ?Ep?(X)p~?
(Y|X) [fk(X,Y)]The computation of Z~?
(x) and the sufficientstatistics (second expectation in Equation 4) are typ-ically the difficult parts.
They require summing thescores of all the spanning trees for a given sentence.Note that, in large-margin training, and in standardmaximum a posteriori decoding, only a maximumover spanning trees is called for?it is conditionaltraining that requires Z~?(x).
In Section 3, we willshow how this can be done exactly in O(n3) time.3 Exploiting the Matrix Tree Theorem forZ~?
(x)We wish to apply conditional training to estimateconditional models of nonprojective trees.
This re-quires computing Z~?
(x) for each training example(as an inner loop to training).
In this section we showhow the summation can be computed and how con-ditional training performs.3.1 Kirchoff MatrixRecall that we defined the unnormalized probability(henceforth, score) of a dependency tree as a combi-nation of edge-factored scores for the edges presentin the tree (Eq.
2):exp ~?
?~f(x,y) =n?i=1e~?
?~f(x,xi,xy(i)) =n?i=1sx,~?
(i,y(i))(5)where y(i) denotes the parent of xi in y.
sx,~?
(i, j),then, denotes the (multiplicative) contribution of theedge from child i to parent j to the total score ofthe tree, if the edge is present.
Define the KirchoffmatrixKx,~?
?
Rn?n by[Kx,~?
]mom,kid= (6)????sx,~?
(kid ,mom) if mom 6= kid?j?
{0,...n}:j 6=momsx,~?
(kid , j) if mom = kid .where mom indexes a parent node and kid a childnode.Kx ~?
can be regarded as a special weighted adja-cency matrix in which the ith diagonal entry is thesum of edge-scores directed into vertex i (i.e., xi isthe child)?note that the sum includes the score ofattaching xi to the wall x0.In our notation and in one specific form, the Ma-trix Tree Theorem (Tutte, 1984) states:2Theorem 1 The determinant of the Kirchoff matrixKx,~?
is equal to the sum of scores of all directedspanning trees in Yx rooted at x0.
Formally:???Kx,~????
= Z~?
(x).A proof is omitted; see Tutte (1984).To compute Z~?
(x), we need only take the deter-minant of Kx,~?, which can be done in O(n3) timeusing the standard LU factorization to compute thematrix inverse.
Since all of the edge weights usedto construct the Kirchoff matrix are positive, it is di-agonally dominant and therefore non-singular (i.e.,invertible).3.2 GradientThe gradient of Z~?
(x) (required for numerical opti-mization; see Eqs.
3?4) can be efficiently computedfrom the same matrix inverse.
While ?
logZ~?
(x)equates to a vector of feature expectations (Eq.
4),we exploit instead some facts from linear algebra2There are proven generalizations of this theorem (Chen,1965; Chaiken, 1982; Minoux, 1999); we give the most specificform that applies to our case, originally proved by Tutte in 1948.Strictly speaking, ourKx,~?
is not the Kirchoff matrix, but rathera submatrix of the Kirchoff matrix with a leftmost column ofzeroes and a topmost row [0,?sx,~?
(1, 0), ...,?sx,~?
(n, 0)] re-moved.
Farther afield, Jaakkola et al (1999) used an undirectedmatrix tree theorem for learning tree structures for graphicalmodels.134Kx,~?
=?????????????j?
{0,...,n}:j 6=1sx,~?
(1, j) ?sx,~?
(2, 1) ?
?
?
?sx,~?
(n, 1)?sx,~?
(1, 2)?j?
{0,...,n}:j 6=2sx,~?
(2, j) ?
?
?
?sx,~?
(n, 2)....... .
....?sx,~?
(1, n) ?sx,~?
(2, n) ?
?
??j?
{0,...,n}:j 6=nsx,~?
(n, j)???????????
?and the chain rule.
First, note that, for any weight?k,?
logZ~?(x)??k=?
log |Kx,~?|??k=1|Kx,~?|?|Kx,~?|??k=1|Kx,~?|n?i=1n?j=0?|Kx,~?|?sx,~?
(i, j)?sx,~?
(i, j)??k=1|Kx,~?|n?i=1n?j=0sx,~?
(i, j)fk(x, xi, xj)??|Kx,~?|?sx,~?
(i, j)(7)(We assume sx,~?
(i, i) = 0, for simplicity of nota-tion.)
The last line follows from the definition ofsx,~?
(i, j) as exp~??
~f(x, xi, xj).
Now, since sx,~?
(i, j)affects the Kirchoff matrix in at most two cells?
(i, i) and (j, i), the latter only when j > 0?weknow that?|Kx,~?|?sx,~?
(i, j)=?|Kx,~?|?[Kx,~?]i,i?[Kx,~?]i,i?sx,~?
(i, i)??|Kx,~?|?[Kx,~?]j,i?[Kx,~?]j,i?sx,~?
(i, j)=?|Kx,~?|?[Kx,~?]i,i??|Kx,~?|?[Kx,~?
]j,i(8)We have now reduced the problem of the gradientto a linear function of ?|Kx,~?| with respect to thecells of the matrix itself.
At this point, we simplifynotation and consider an arbitrary matrixA.The minor mj,i of a matrix A is the determi-nant of the submatrix obtained by striking out rowj and column i of A; the cofactor cj,i of A is then(?1)i+jmj,i.
Laplace?s formula defines the deter-minant as a linear combination of matrix cofactorsof an arbitrary row j:|A| =n?i=1[A]j,icj,i (9)It should be clear that any cj,k is constant with re-spect to the cell [A]j,i (since it is formed by remov-ing row j of A) and that other entries of A are con-stant with respect to the cell [A]j,i.
Therefore:?|A|?
[A]j,i= cj,i (10)The inverse matrixA?1 can also be defined in termsof cofactors:[A?1]i,j =cj,i|A|(11)Combining Eqs.
10 and 11, we have:?|A|?
[A]j,i= |A|[A?1]i,j (12)Plugging back in through Eq.
8 to Eq.
7, we have:?
logZ~?(x)??k=n?i=1n?j=0sx,~?
(i, j)fk(x, xi, xj)?([K?1x,~?]i,i?[K?1x,~?
]i,j)(13)where [K?1]i,0 is taken to be 0.
Note that the cofac-tors do not need to be computed directly.
We pro-posed in Section 3.1 to get Z~?
(x) by computing theinverse of the Kirchoff matrix (which is known toexist).
Under that procedure, the marginalization isa by-product of the gradient.135decode train Arabic Czech Danish Dutchmap MIRA 79.9 81.4 86.6 90.0CE 80.4 80.2 87.5 90.0 (Section 3)mBr MIRA 79.4 80.3 85.0 87.2 (Section 4)CE 80.5 80.4 87.5 90.0 (Sections 3 & 4)Table 1: Unlabeled dependency parsing accuracy (on test data) for two training methods (MIRA, as inMcDonald et al (2005b), and conditional estimation) and with maximum a posteriori (map) and minimumBayes-risk (mBr) decoding.
Boldface scores are best in their column on a permutation test at the .05 level.3.3 ExperimentWe compare conditional training of a nonprojectiveedge-factored parsing model to the online MIRAtraining used by McDonald et al (2005b).
Four lan-guages with relatively common nonprojective phe-nomena were tested: Arabic (Hajic?
et al, 2004),Czech (Bo?hmova?
et al, 2003), Danish (Kromann,2003), and Dutch (van der Beek et al, 2002).
TheDanish and Dutch datasets were prepared for theCoNLL 2006 shared task (Buchholz and Marsi,2006); Arabic and Czech are from the 2007 sharedtask.
We used the same features, extracted by Mc-Donald?s code, in both MIRA and conditional train-ing.
In this paper, we consider only unlabeled de-pendency parsing.Our conditional training used an online gradient-based method known as stochastic gradient descent(see, e.g., Bottou, 2003).
Training with MIRA andconditional estimation take about the same amountof time: approximately 50 sentences per second.Training proceeded as long as an improvement onheld-out data was evident.
The accuracy of the hy-pothesized parses for the two models, on each lan-guage, are shown in the top two rows of Tab.
1 (la-beled ?map?
for maximum a posteriori, meaningthat the highest-weighted tree is hypothesized).The two methods are, not surprisingly, close inperformance; conditional likelihood outperformedMIRA on Arabic and Danish, underperformedMIRA on Czech, and the two tied on Dutch.
Resultsare significant at the .05 level on a permutation test.Conditional estimation is in practice more prone toover-fitting than maximum margin methods, thoughwe did not see any improvement using zero-meanGaussian priors (variance 1 or 10).These experiments serve to validate conditionalestimation as a competitive learning algorithm forparsing models, and the key contribution of the sum-ming algorithm that permits conditional estimation.4 Minimum Bayes-Risk DecodingA second application of probability distributionsover trees is the alternative decoding algorithmknown as minimum Bayes-risk (mBr) decoding.The more commonly used maximum a posterioridecoding (also known as ?Viterbi?
decoding) thatwe applied in Section 3.3 sought to minimize the ex-pected whole-tree loss:y?
= argmaxyp~?
(y | x) = argminyEp~?
(Y|x) [??
(y,Y)](14)Minimum Bayes-risk decoding generalizes this ideato an arbitrary loss function ` on the proposed tree:y?
= argminyEp~?
(Y|x) [`(y,Y)] (15)This technique was originally applied in speechrecognition (Goel and Byrne, 2000) and translation(Kumar and Byrne, 2004); Goodman (1996) pro-posed a similar idea in probabilistic context-freeparsing, seeking to maximize expected recall.
Formore applications in parsing, see Petrov and Klein(2007).The most common loss function used to evaluatedependency parsers is the number of attachment er-rors, so we seek to decode using:y?
= argminyEp~?(Y|x)[n?i=1??
(y(i),Y(i))]= argmaxyn?i=1p~?
(Y(i) = y(i) | x) (16)To apply this decoding method, we make use ofEq.
13, which gives us the posterior probabilities136of edges under the model, and the same Chiu-Liu-Edmonds maximum directed spanning tree al-gorithm used for maximum a posteriori decoding.Note that this decoding method can be applied re-gardless of how the model is trained.
It merely re-quires assuming that the tree scores under the trainedmodel (probabilistic or not) can be treated as unnor-malized log-probabilities over trees given the sen-tence x.We applied minimum Bayes-risk decoding to themodels trained using MIRA and using conditionalestimation (see Section 3.3).
Table 1 shows that,across languages, minimum Bayes-risk decodinghurts slightly the performance of a MIRA-trainedmodel, but helps slightly or does not affect the per-formance of a conditionally-trained model.
SinceMIRA does not attempt to model the distributionover trees, this result is not surprising; interpretingweights as defining a conditional log-linear distribu-tion is questionable under MIRA?s training criterion.One option, which we do not test here, is touse minimum Bayes-risk decoding inside of MIRAtraining, to propose a hypothesis tree (or k-besttrees) at each training step.
Doing this would moreclosely match the training conditions with the test-ing conditions; however, it is unclear whether thereis a formal interpretation of such a combination, forexample its relationship to McDonald et al?s ?fac-tored MIRA.
?Minimum Bayes-risk decoding, we believe, willbecome important in nonprojective parsing withnon-edge-factored models.
Note that minimiumBayes-risk decoding reduces any parsing problem tothe maximum directed spanning tree problem, evenif the original model is not edge-factored.
All thatis required are the marginals p~?
(Y(i) = y(i) | x),which may be intractable to compute exactly, thoughit may be possible to develop efficient approxima-tions.5 Hidden VariablesA third application of probability distributions overtrees is hidden-variable learning.
The Expectation-Maximization (EM) algorithm (Baum and Petrie,1966; Dempster et al, 1977; Baker, 1979), forexample, is a way to maximum the likelihood oftraining data, marginalizing out hidden variables.This has been applied widely in unsupervised pars-ing (Carroll and Charniak, 1992; Klein and Man-ning, 2002).
More recently, EM has been used tolearn hidden variables in parse trees; these can behead-child annotations (Chiang and Bikel, 2002), la-tent head features (Matsuzaki et al, 2005; Prescher,2005; Dreyer and Eisner, 2006), or hierarchically-split nonterminal states (Petrov et al, 2006).To date, we know of no attempts to apply hid-den variables to supervised dependency tree mod-els.
If the trees are constrained to be projective, EMis easily applied using the inside-outside variant ofthe parsing algorithm described by Eisner (1996) tocompute the marginal probability.
Moving to thenonprojective case, there are two difficulties: (a) wemust marginalize over nonprojective trees and (b)we must define a generative model over (X,Y).We have already shown in Section 3 how to solve(a); here we avoid (b) by maximizing conditionallikelihood, marginalizing out the hidden variable,denoted z:max~??x,yp?
(x,y) log?zp~?
(y, z | x) (17)This sort of conditional training with hidden vari-ables was carried out by Koo and Collins (2005),for example, in reranking; it is related to the infor-mation bottleneck method (Tishby et al, 1999) andcontrastive estimation (Smith and Eisner, 2005).5.1 Latent Dependency LabelsNoting that our model is edge-factored (Eq.
2), wedefine our hidden variables to be edge-factored aswell.
We can think of the hidden variables as clusterson dependency tokens, and redefine the score of anedge to be:sx,~?
(i, j) =?z?Ze~?
?~f(x,xi,xj ,z) (18)where Z is a set of dependency clusters.Note that keeping the model edge-factored meansthat the cluster of each dependency in a tree is con-ditionally independent of all the others, given thewords.
This is computationally advantageous (wecan factor out the marginalization of the hidden vari-able by edge), and it permits the use of any cluster-ing method at all.
For example, if an auxiliary clus-tering model q(z | x,y)?perhaps one that did not137make such independence assumptions?were used,the posterior probability q(Zi = z | x,y) couldbe a feature in the proposed model.
On the otherhand, we must consider carefully the role of thedependency clusters in the model; if clusters arelearned extrinsic to estimation of the parsing model,we should not expect them to be directly advanta-geous to parsing accuracy.5.2 ExperimentsWe tried two sets of experiments with clustering.
Inone case, we simply augmented all of McDonaldet al?s edge features with a cluster label in hopesof improved accuracy.
Models were initialized nearzero, with Gaussian noise added to break symmetryamong clusters.Under these conditions, performance stayed thesame or changed slightly (see Table 2); none of theimprovements are significant.
Note that three de-coders were applied: maximum a posteriori (map)and minimum Bayes-risk (mBr) as described in Sec-tion 4, and ?max-z,?
in which each possible edgewas labeled and weighted only with its most likelycluster (rather than the sum over all clusters), beforefinding the most probable tree.3 For each of the threelanguages tested, some number of clusters and somedecoding method gave small improvements over thebaseline.More ambitiously, we hypothesized that manylexicalized features on edges could be ?squeezed?through clusters to reduce the size of the feature set.We thus removed all word-word and lemma-lemmafeatures and all tag fourgrams.
Although this re-duced our feature set by a factor of 60 or more (priorto taking a cross-product with the clusters), the dam-age of breaking the features was tremendous, andperformance even with a thousand clusters barelybroke 25% accuracy.6 DiscussionNoting that adding latent features to nonterminalsin unlexicalized context-free parsing has been verysuccessful (Chiang and Bikel, 2002; Matsuzaki etal., 2005; Prescher, 2005; Dreyer and Eisner, 2006;Petrov et al, 2006), we were surprised not to see a3Czech experiments were not done, since the number of fea-tures (more than 14 million) was too high to multiply out byclusters.# cl.
decoding Arabic Danish Dutchnone map=max-z 80.4 87.5 90.0mBr 80.5 87.5 90.02 map 80.4 87.5 89.5mBr 80.6 87.3 89.7max-z 80.4 86.3 89.416 map 80.4 87.6 90.1mBr 80.4 87.6 90.1max-z 80.4 87.6 90.232 map 80.0 87.6 ?mBr 80.4 87.5 ?max-z 80.0 87.5 ?Table 2: Augmenting edge features with clusters re-sults in similar performance to conditional trainingwith no clusters (top two lines).
Scores are unla-beled dependency accuracy on test data.more substantial performance improvement throughlatent features.
We propose several interpretations.First, it may simply be that many more clusters maybe required.
Note that the label-set sizes for the la-beled versions of these datasets are larger than 32(e.g., 50 for Danish).
This has the unfortunate effectof blowing up the feature space beyond the mem-ory capacity of our machines (hence our attemptsat squeezing high-dimensional features through theclusters).Of course, improved clustering methods mayalso improve performance.
In particular, a cluster-learning algorithm that permits clusters to splitand/or merge, as in Petrov et al (2006) or in Pereiraet al (1993), may be appropriate.Given the relative simplicity of clustering meth-ods for context-free parsing to date (gains werefound just by using Expectation-Maximization), webelieve the fundamental reason clustering was notparticularly helpful here is a structural one.
Incontext-free parsing, the latent features are (in pub-lished work to date) on nonterminal states, which arethe stuctural ?bridge?
between context-free rules.Adding features to those states is a way of pushinginformation?encoded indirectly, perhaps?fartheraround the tree, and therefore circumventing thestrict independence assumptions of probabilisticCFGs.In an edge-factored dependency model, on the138other hand, latent features on the edges seem to havelittle effect.
Given that they are locally ?summedout?
when we compute the scores of possible at-tachments, it should be clear that the edge clustersdo not circumvent any independence assumptions.Three options appear to present themselves.
First,we might attempt to learn clusters in tandem withestimating a richer, non-edge-factored model whichwould require approximations to Z~?
(x), if condi-tional training were to be used.
Note that the approx-imations to maximizing over spanning trees withsecond-order features, proposed by McDonald andPereira (2006), do not permit estimating the clustersas part of the same process as weight estimation (atleast not without modification).
In the conditionalestimation case, a variational approach might be ap-propriate.
The second option is to learn clusters of-fline, before estimating the parser.
(We suggestedhow to incorporate soft clusters into our model inSection 5.1.)
This option is computationally ad-vantageous but loses sight of the aim of learningthe clusters specifically to improve parsing accuracy.Third, noting that the structural ?bridge?
betweentwo coincident edges is the shared vertex (word), wemight consider word token clustering.We also believe this structural locality issue helpsexplain the modesty of the gains using minimumBayes-risk decoding with conditional training (Sec-tion 4).
In other dependency parsing scenarios, min-imum Bayes-risk decoding has been found to offersignificant advantages?why not here?
MinimumBayes-risk makes use of global statistical dependen-cies in the posterior when making local decisions.But in an edge-factored model, the edges are all con-ditionally independent, given that y is a spanningtree.As a post hoc experiment, we comparedpurely greedy attachment (attach each word to itsmaximum-weighted parent, without any tree con-straints).
Edge scores as defined in the model werecompared to minimum Bayes-risk posterior scores,and the latter were consistently better (though thisalways under-performed optimal spanning-tree de-coding, unsurprisingly).
This comparison servesonly to confirm that minimum Bayes-risk decodingis a way to circumvent independence assumptions(here made by a decoder), but only when the trainedmodel does not make those particular assumptions.7 ConclusionWe have shown how to carry out exact marginaliza-tion under an edge-factored, conditional log-linearmodel over nonprojective dependency trees.
Themethod has cubic runtime in the length of the se-quence, but is very fast in practice.
It can be usedin conditional training of such a model, in minimumBayes-risk decoding (regardless of how the model istrained), and in training with hidden variables.
Wedemonstrated how each of these techniques gives re-sults competitive with state-of-the-art existing de-pendency parsers.AcknowledgmentsThe authors thank the anonymous reviewers, JasonEisner, Keith Hall, and Sanjeev Khudanpur for help-ful comments, and Michael Collins and Ryan Mc-Donald for sharing drafts of their related, concurrentpapers.
This work was supported in part by NSFITR grant IIS-0313193.ReferencesY.
Altun, M. Johnson, and T. Hofmann.
2003.
Inves-tigating loss functions and optimization methods fordiscriminative learning of label sequences.
In Proc.
ofEMNLP.J.
K. Baker.
1979.
Trainable grammars for speech recog-nition.
In Proc.
of the Acoustical Society of America,pages 547?550.P.
Bartlett, M. Collins, B. Taskar, and D. McAllester.2004.
Exponentiated gradient algorithms for large-margin structured classification.
In Advances in NIPS17.L.
E. Baum and T. Petrie.
1966.
Statistical inference forprobabilistic functions of finite state Markov chains.Annals of Mathematical Statistics, 37:1554?1563.A.
Bo?hmova?, J.
Hajic?, E.
Hajic?ova?, and B. Hladka?.2003.
The PDT: a 3-level annotation scenario.In A. Abeille, editor, Building and ExploitingSyntactically-Annotated Corpora.
Kluwer.L.
Bottou.
2003.
Stochastic learning.
In Advanced Lec-tures in Machine Learning, pages 146?168.
Springer.S.
Buchholz and E. Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In Proc.
ofCoNLL.G.
Carroll and E. Charniak.
1992.
Two experiments onlearning probabilistic dependency grammars from cor-pora.
Technical report, Brown University.S.
Chaiken.
1982.
A combinatorial proof of the all mi-nors matrix tree theorem.
SIAM Journal on Algebraicand Discrete Methods, 3(3):319?329.139W.-K. Chen.
1965.
Topological analysis for activenetworks.
IEEE Transactions on Circuit Theory,12(1):85?91.D.
Chiang and D. Bikel.
2002.
Recovering latent infor-mation in treebanks.
In Proc.
of COLING.A.
Culotta and J. Sorensen.
2004.
Dependency tree ker-nels for relation extraction.
In Proc.
of ACL.A.
Dempster, N. Laird, and D. Rubin.
1977.
Maximumlikelihood estimation from incomplete data via the EMalgorithm.
Journal of the Royal Statistical Society B,39:1?38.M.
Dreyer and J. Eisner.
2006.
Better informed trainingof latent syntactic features.
In Proc.
of EMNLP.J.
Eisner.
1996.
Three new probabilistic models for de-pendency parsing: An exploration.
In Proc.
of COL-ING.V.
Goel and W. Byrne.
2000.
Minimum Bayes risk auto-matic speech recognition.
Computer Speech and Lan-guage, 14(2):115?135.J.
Goodman.
1996.
Parsing algorithms and metrics.
InProc.
of ACL.J.
Hajic?, O.
Smrz?, P. Zema?nek J.
S?naidauf, and E. Bes?ka.2004.
Prague Arabic Dependency Treebank: Devel-opment in data and tools.
In Proc.
of the NEMLAR In-tern.
Conf.
on Arabic Language Resources and Tools.J.
Hall, J. Nivre, and J. Nilsson.
2006.
Discriminativelearning for data-driven dependency parsing.
In Proc.of COLING-ACL.T.
Jaakkola, M. Meila, and T. Jebara.
1999.
Maximumentropy discrimination.
In Advances in NIPS 12.M.
Johnson.
2001.
Joint and conditional estimation oftagging and parsing models.
In Proc.
of ACL.D.
Klein and C. D. Manning.
2002.
A generativeconstituent-context model for improved grammar in-duction.
In Proc.
of ACL.T.
Koo and M. Collins.
2005.
Hidden-variable modelsfor discriminative reranking.
In Proc.
of EMNLP.T.
Koo, A. Globerson, X. Carreras, and M. Collins.
2007.Structured prediction models via the Matrix-Tree The-orem.
In Proc.
of EMNLP-CoNLL.M.
T. Kromann.
2003.
The Danish dependency treebankand the underlying linguistic theory.
In Proc.
of TLT.S.
Kumar and W. Byrne.
2004.
Minimum Bayes riskdecoding for statistical machine translation.
In Proc.of HLT-NAACL.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
of ICML.D.
C. Liu and J. Nocedal.
1989.
On the limited mem-ory BFGS method for large scale optimization.
Math.Programming, 45:503?528.T.
Matsuzaki, Y. Miyao, and J. Tsujii.
2005.
Probabilis-tic CFG with latent annotations.
In Proc.
of ACL.A.
McCallum, D. Freitag, and F. Pereira.
2000.
Maxi-mum entropy Markov models for information extrac-tion and segmentation.
In Proc.
of ICML.R.
McDonald and F. Pereira.
2006.
Online learning ofapproximate dependency parsing algorithms.
In Proc.of EACL.R.
McDonald and G. Satta.
2007.
On the complexityof non-projective data-driven dependency parsing.
InProc.
of IWPT.R.
McDonald, K. Crammer, and F. Pereira.
2005a.
On-line large-margin training of dependency parsers.
InProc.
of ACL.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005b.Non-projective dependency parsing using spanningtree algorithms.
In Proc.
of HLT-EMNLP.M.
Minoux.
1999.
A generalization of the all minors ma-trix tree theorem to semirings.
Discrete Mathematics,199:139?150.Y.
Miyao and J. Tsujii.
2002.
Maximum entropy estima-tion for feature forests.
In Proc.
of HLT.F.
C. N. Pereira, N. Tishby, and L. Lee.
1993.
Distribu-tional clustering of English words.
In Proc.
of the 31stACL.S.
Petrov and D. Klein.
2007.
Improved inference forunlexicalized parsing.
In Proc.
of HLT-NAACL.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable tree an-notation.
In Proc.
of COLING-ACL.D.
Prescher.
2005.
Head-driven PCFGs with latent-headstatistics.
In Proc.
of IWPT.C.
Quirk, A. Menezes, and C. Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal SMT.
In Proc.
of ACL.A.
Ratnaparkhi, S. Roukos, and R. T. Ward.
1994.
Amaximum entropy model for parsing.
In Proc.
of IC-SLP.S.
Riezler, D. Prescher, J. Kuhn, and M. Johnson.
2000.Lexicalized stochastic modeling of constraint-basedgrammars using log-linear measures and EM training.In Proc.
of ACL.N.
A. Smith and J. Eisner.
2005.
Contrastive estimation:Training log-linear models on unlabeled data.
In Proc.of ACL.B.
Taskar, D. Klein, M. Collins, D. Koller, and C. Man-ning.
2004.
Max-margin parsing.
In Proc.
of EMNLP.N.
Tishby, F. C. N. Pereira, and W. Bialek.
1999.
Theinformation bottleneck method.
In Proc.
of the 37thAllerton Conference on Communication, Control andComputing, pages 368?377.W.
T. Tutte.
1984.
Graph Theory.
Addison-Wesley.L.
van der Beek, G. Bouma, R. Malouf, and G. van No-ord.
2002.
The Alpino dependency treebank.
InCLIN.140
