Compi lat ion of HPSG to TAG*Robert KasperDept.
of LinguisticsOhio State University222 Oxley HallColumbus, OH 43210U.S.A.kasper~l ing.ohio-state.eduBernd  K ie fer  Klaus NetterDeutsches Forschungszentrumffir Ki instl iche Intell igenz, GmbHStuhlsatzenhausweg 366123 SaarbrfickenGermany(k iefer lnetter}Qdfki .uni -sb.deK.
Vijay-ShankerCIS Dept.University of DelawareNewark, DE 19716U.S.Avijay@cis.udel.eduAbstractWe present an implemented compilationalgorithm that translates HPSG into lex-icalized feature-based TAG, relating con-cepts of the two theories.
While HPSG hasa more elaborated principle-based theoryof possible phrase structures, TAG pro-vides the means to represent lexicalizedstructures more explicitly.
Our objectivesare met by giving clear definitions that de-termine the projection of structures fromthe lexicon, and identify "maximal" pro-jections, auxiliary trees and foot nodes.1 In t roduct ionHead Driven Phrase Structure Grammar (HPSG)and Tree Adjoining Grammar (TAG) are two frame-works which so far have been largely pursued in par-allel, taking little or no account of each other.
In thispaper we will describe an algorithm which will com-pile HPSG grammars, obeying certain constraints,into TAGs.
However, we are not only interested inmapping one formalism into another, but also in ex-ploring the relationship between concepts employedin the two frameworks.HPSG is a feature-based grammatical frameworkwhich is characterized by a modular specificationof linguistic generalizations through extensive use ofprinciples and lexicalization ofgrammatical informa-tion.
Traditional grammar rules are generalized toschemata providing an abstract definition of gram-matical relations, such as head-of, complement-of,subject-of, adjunct-of, etc.
Principles, such as the*We would like to thank A. Abeill6, D. Flickinger,A.
Joshi, T. Kroch, O. Rambow, I.
Sag and H. Uszko-reit for valuable comments and discussions.
The reseaxchunderlying the paper was supported by research grantsfrom the German Bundesministerium fiir Bildung, Wis-senschaft, Forschung und Technologie (BMBF) to theDFKI projects DIsco, FKZ ITW 9002 0, PARADICE,FKZ ITW 9403 and the VERBMOB1L project, FKZ 01IV 101 K/l, and by the Center for Cognitive Science atOhio State University.Head-Feature-, Valence-, Non-Local- or Semantics-Principle, determine the projection of informationfrom the lexicon and recursively define the flow ofinformation in a global structure.
Through thismodular design, grammatical descriptions are bro-ken down into minimal structural units referring tolocal trees of depth one, jointly constraining the setof well-formed sentences.In HPSG, based on the concept of "head-domains", local relations (such as complement-of,adjunct-of) are defined as those that are realizedwithin the domain defined by the syntactic head.This domain is usually the maximal projection of thehead, but it may be further extended in some cas-es, such as raising constructions.
In contrast, filler-gap relations are considered non-local.
This localvs.
non-local distinction in HPSG cuts across therelations that are localized in TAG via the domainsdefined by elementary trees.
Each elementary treetypically represents all of the arguments that aredependent on a lexical functor.
For example, thecomplement-of and filler-gap relations are localizedin TAG, whereas the adjunct-of relation is not.Thus, there is a fundamental distinction betweenthe different notions of localization that have beenassumed in the two frameworks.
If, at first sight,these frameworks seem to involve a radically differ-ent organization of grammatical relations, it is nat-ural to question whether it is possible to compileone into the other in a manner faithful to both, andmore importantly, why this compilation is being ex-plored at all.
We believe that by combining the twoapproaches both frameworks will profit.From the HPSG perspective, this compilation of-fers the potential to improve processing efficiency.HPSG is a "lexicalist" framework, in the sense thatthe lexicon contains the information that determineswhich specific categories can be combined.
Howev-er, most HPSG grammars are not lexicalized in thestronger sense defined by Schabes et.al.
(SAJ88),where lexicaiization means that each elementarystructure in the grammar is anchored by some lex-ical item.
For example, HPSG typically assumes arule schema which combines a subject phrase (e.g.92NP) with a head phrase (e.g.
VP), neither of whichis a lexical item.
Consider a sentence involving atransitive verb which is derived by applying two ruleschemata, reducing first the object and then the sub-ject.
In a standard HPSG derivation, once the headverb has been retrieved, it must be computed thatthese two rules (and no other rules) are applicable,and then information about the complement andsubject constituents is projected from the lexiconaccording to the constraints on each rule schema.On the other hand, in a lexicalized TAG derivation,a tree structure corresponding to the combined in-stantiation of these two rule schemata is directlyretrieved along with the lexical item for the verb.Therefore, a procedure that compiles HPSG to TAGcan be seen as performing significant portions of anHPSG derivation at compile-time, so that the struc-tures projected from lexical items do not need tobe derived at run-time.
The compilation to TAGprovides a way of producing a strongly lexicalizedgrammar which is equivalent o the original HPSG,and we expect this lexicalization to yield a compu-tational benefit in parsing (cf.
(S J90)).This compilation strategy also raises several is-sues of theoretical interest.
While TAG belongs to aclass of mildly context-sensitive grammar formalisms(JVW91), the generative capacity of the formal-ism underlying HPSG (viz., recursive constraintsover typed feature structures) is unconstrained, al-lowing any recursively enumerable language to bedescribed.
In HPSG the constraints necessary tocharacterize the class of natural languages are stat-ed within a very expressive formalism, rather thanbuilt into the definition of a more restrictive for-malism, such as TAG.
Given the greater expressivepower of the HPSG formalism, it will not be pos-sible to compile an aribitrary HPSG grammar intoa TAG grammar.
However, our compilation algo-rithm shows that particular HPSG grammars maycontain constraints which have the effect of limitingthe generative capacity to that of a mildly context-sensitive language.1 Additionally, our work providesa new perspective on the different types of con-stituent combination in HPSG, enabling a classifi-cation of schemata nd principles in terms of moreabstract functor-argument relations.From a TAG perspective, using concepts em-ployed in the HPSG framework, we provide an ex-plicit method of determining the content of the el-ementary trees (e.g., what to project from lexicalitems and when to stop the projection) from anHPSG source specification.
This also provides amethod for deriving the distinctions between initialand auxiliary trees, including the identification of1We are only considering a syntactic fragment ofHPSG here.
It is not clear whether the semantic om-ponents of HPSG can also be compiled into a more con-strained formalism.foot nodes in auxiliary trees.
Our answers, whileconsistent with basic tenets of traditional TAG anal-yses, are general enough to allow an alternate lin-guistic theory, such as HPSG, to be used as a basisfor deriving a TAG.
In this manner, our work alsoserves to investigate the utility of the TAG frame-work itself as a means of expressing different linguis-tic theories and intuitions.In the following we will first briefly describe thebasic constraints we assume for the HPSG inputgrammar and the resulting form of TAG.
Next wedescribe the essential algorithm that determines theprojection of trees from the lexicon, and give formaldefinitions of auxiliary tree and foot node.
We thenshow how the computation of "sub-maximal" projec-tions can be triggered and carried out in a two-phasecompilation.2 BackgroundAs the target of our translation we assume a Lexi-calized Tree-Adjoining Grammar (LTAG), in whichevery elementary tree is anchored by a lexicalitem (SAJ88).We do not assume atomic labelling of nodes, un-like traditional TAG, where the root and foot nodesof an auxiliary tree are assumed to be labelled iden-tically.
Such trees are said to factor out recursion.However, this identity itself isn't sufficient o identi-fy foot nodes, as more than one frontier node may belabelled the same as the root.
Without such atomiclabels in HPSG, we are forced to address this issue,and present a solution that is still consistent withthe notion of factoring recursion.Our translation process yields a lexicalizedfeature-based TAG (VSJ88) in which feature struc-tures are associated with nodes in the frontier oftrees and two feature structures (top and bottom)with nodes in the interior.
Following (VS92), therelationships between such top and bottom fea-ture structures represent underspecified dominationlinks.
Two nodes standing in this domination rela-tion could become the same, but they are necessarilydistinct if adjoining takes place.
Adjoining separatesthem by introducing the path from the root to thefoot node of an auxiliary tree as a further specifica-tion of the underspecified domination link.For illustration of our compilation, we consid-er an extended HPSG following the specificationsin (PS94)\[404ff\].
The rule schemata include rules forcomplementation (i cluding head-subject and head-complement relations), head-adjunct, and filler-headrelations.The following rule schemata cover the combina-tion of heads with subjects and other complementsrespectively as well as the adjunct constructions.
22We abstract from quite a number of properitesand use the following abbreviations for feature names:S-----SYI"/SEM, L~LOChL, C~ChT, N-L----NON-LOChL, D-----DTRS,93Head-Sub j-Schemas L lC iS~ ()L eo~ms I-;-\] ( >I I EAD-DTR SILIC/SUBJ > D LCOMPSLeo~-DTR\[-~ \[\]\]Head- Comps-SchemaL I c |SUBJLCOm, s~AD-D~ slT.le | s~J  \ [ \ ]D LCa~S union(\[\], E\]) c0.~-D=\[.~ \ \]\]Head-Adjunct-SchemaLeo~s~AD-DTRIS \ [ \ ]  I C |S~JD LCOm, SADJ-DTRIS \[LIm~ADa.OD \ [ \ ] \ ]We assume a slightly modified and constrainedtreatment of non-local dependencies (SLASH), inwhich empty nodes are eliminated and a lexical ruleis used instead.
While SLASH introduction is based onthe standard filler-head schema, SLASH percolation isessentially constrained to the HEAD spine.Head-Filler-SchemaLIC/s~J \[\]<Lco"Ps ~<N-L\[SLASH < >\]Lie SUBJ \ [ \ ]| L L.-L\[~L.S.
<~>\]JJ|L~,.~.~.H-D~R\[s \ \]\] JSLASH termination is accounted for by a lexicalrule, which removes an element from one of the va-lence lists (e0MPS or sts J)  and adds it to the SLASHlist.Lexical Slash- Termination-RuleIL l ( : /St~J~/ ke0.P.,L.-L\[sLAs.
\]/'-Ic/~B~ \[\]LEX-DTR S / Lcom's unionqEl,~)L.-L\[sL's" < >\]The percolation of SLASH across head domains islexically determined.
Most lexical items will be spec-ified as having an empty SLASH list.
Bridge verbs(e.g., equi verbs such as want) or other heads al-lowing extraction out of a complement share theirown SLASH value with the SLASH of the respectivecomplement.
3Equi and Bridge Verb"N-L \[SL,SH E\]\]-~ r ~ ,  <\[\]>111\vpk L,-,-\[s,-As,~-l\] J\]}Finally, we assume that rule schemata nd prin-ciples have been compiled together (automaticallyor manually) to yield more specific subtypes of theschemata.
This does not involve a loss of general-ization but simply means a further refinement of thetype hierarchy.
LP constraints could be compiledout beforehand or during the compilation of TAGstructures, since the algorithm is lexicon driven.3 A lgor i thm3.1 Basic IdeaWhile in TAG all arguments related to a particu-lar functor are represented in one elementary treestructure, the 'functional application' in HPSG isdistributed over the phrasal schemata, each of whichcan be viewed as a partial description of a local tree.Therefore we have to identify which constituents inaWe choose such a lexicalized approach, because itwill allow us to maintain a restriction that every TAGtree resulting from the compilation must be rooted ina non-emtpy lexical item.
The approach will accountfor extraction of complements out of complements, i.e.,along paths corresponding to chains of government rela-tions.As far as we can see, the only limitation arising fromthe percolation of SLASH only along head-projections ison extraction out of adjuncts, which may be desirablefor some languages like English.
On the other hand,these constructions would have to be treated by multi-component TAGs, which axe not covered by the intendedinterpretation of the compilation algorithm anyway.94a phrasal schema count as functors and arguments.In TAG different functor argument relations, suchas head-complement, head-modifier etc., are repre-sented in the same format as branches of a trunkprojected from a lexical anchor.
As mentioned, thisanchor is not always equivalent to the HPSG notionof a head; in a tree projected from a modifier, for ex-ample, a non-head (ADJUNCT-DTR) counts as a func-tor.
We therefore have to generalize over differenttypes of daughters in HPSG and define a general no-tion of a functor.
We compute the functor-argumentstructure on the basis of a general selection relation.Following (Kas92) 4, we adopt the notion of a se-lector daughter (SD), which contains a selector fea-ture (SF) whose value constrains the argument (ornon-selector) daughter (non-SD)) For example, in ahead-complement structure, the SD is the HEAD-DTR,as it contains the list-valued feature coMPs (the SF)each of whose elements elects a C0m~-DTR, i.e., an el-ement of the CoMPs list is identified with the SYNSE~4value of a COMP-DTR.We assume that a reduction takes place along withselection.
Informally, this means that if F is the se-lector feature for some schema, then the value (or theelement(s) in the list-value) of 1: that selects the non-SD(s) is not contained in the F value of the mothernode.
In case F is list-valued, we-assume that therest of the elements in the list (those that did notselect any daughter) are also contained in the F atthe mother node.
Thus we say that F has been re-duced by the schema in question.The compilation algorithm assumes that allHPSG schemata will satisfy the condition of si-multaneous election and reduction, and that eachschema reduces at least one SF.
For the head-complement- and head-subject-schema, these con-ditions follow from the Valence Principle, and theSFs are coMPs and SUBJ, respectively.
For the head-adjunct-schema, the ADJUNCT-DTR is the SD, becauseit selects the HEAD-DTR by its NOD feature.
The NODfeature is reduced, because it is a head feature,whose value is inherited only from the HEAD-DTR andnot from the ADJUNCT-DTR.
Finally, for the filler-head-schema, the HEAD-DTR is the SD, as  it selects theFILLER-DTR by its SLASH value, which is bound off,not inherited by the mother, and therefore reduced.We now give a general description of the compila-tion process.
Essentially, we begin with a lexical de-4The algorithm presented here extends and refines theapproach described by (Kas92) by stating more precisecriteria for the projection of features, for the termina-tion of the algorithm, and for the determination of thosestructures which should actually be used as elementarytrees.5Note that there might be mutual selection (asin the case of the specifier-head-relations proposedin (PS94)\[44ff\]).
If there is mutual selection, we haveto stipulate one of the daughters as the SD.
The choicemade would not effect he correctness ofthe compilation.scription and project phrases by using the schematato reduce the selection information specified by thelexical type.Basic Algor i thm Take a lexical type L and initial-ize by creating a node with this type.
Add anode n dominating this node.For any schema S in which specified SFs of nare reduced, try to instantiate S with n corre-sponding to the SD of S. Add another node mdominating the root node of the instantiatedschema.
(The domination links are introducedto allow for the possibility of adjoining.)
Re-peat this step (each time with n as the rootnode of the tree) until no further reduction ispossible.We will fill in the details below in the followingorder: what information to raise across dominationlinks (where adjoining may take place), how to de-termine auxiliary trees (and foot nodes), and whento terminate the projection.We note that the trees produced have a trunkleading from the lexical anchor (node for the givenlexical type) to the root.
The nodes that are sib-lings of nodes on the trunk, the selected aughters,are not elaborated further and serve either as footnodes or substitution odes.3.2 Raising Features Across DominationLinksQuite obviously, we must raise the SFs across dom-ination links, since they determine the applicabilityof a schema and licence the instantiation of an SD.If no SF were raised, we would lose all informationabout the saturation status of a functor, and thealgorithm would terminate after the first iteration.There is a danger in raising more than the SFs.For example, the head-subject-schema in Germanwould typically constrain a verbal head to be finite.Raising HEAD features would block its application tonon-finite verbs and we would not produce the treesrequired for raising-verb adjunction.
This is againbecause heads in HPSG are not equivalent o lexi-cal anchors in TAG, and that other local propertiesof the top and bottom of a domination link coulddiffer.
Therefore HEAD features and other LOCAL fea-tures cannot, in general, be raised across dominationlinks, and we assume for now that only the SFs areraised.Raising all SFs produces only fully saturated el-ementary trees and would require the root and footof any auxiliary tree to share all SFs, in order to becompatible with the SF values across any domina-tion links where adjoining can take place.
This is toostrong a condition and will not allow the resultingTAG to generate all the trees derivable with the giv-en HPSG (e.g., it would not allow unsaturated VPcomplements).
In ?
3.5 we address this concern by95using a multi-phase compilation.
In the first phase,we raise all the SFs.3.3 Detect ing  Aux i l ia ry  Trees and FootNodesTraditionally, in TAG, auxiliary trees are said to beminimal recursive structures that have a foot node(at the frontier) labelled identical to the root.
Assuch category labels (S, NP etc.)
determine wherean auxiliary tree can be adjoined, we can informallythink of these labels as providing selection informa-tion corresponding to the SFs of HPSG.
Factoring ofrecursion can then be viewed as saying that auxiliarytrees define a path (called the spine) from the rootto the foot where the nodes at extremities have thesame selection information.
However, a closer lookat TAG shows that this is an oversimplification.
Ifwe take into account he adjoining constraints (orthe top and bottom feature structures), then it ap-pears that the root and foot share only some selec-tion information.Although the encoding of selection information bySFs in HPSG is somewhat different han that tradi-tionally employed in TAG, we also adopt the notionthat the extremities of the spine in an auxiliary treeshare some part (but not necessarily all) of the se-lection information.
Thus, once we have produced atree, we examine the root and the nodes in its fron-tier.
A tree is an auxiliary tree if the root and somefrontier node (which becomes the foot node) havesome non-empty SF value in common.
Initial treesare those that have no such frontier nodes.\ [SUBS<>\]T1 COMPS < >SLASH \ [ \ ]\ [ \ ]  , D',JCOMPS < >SLASH \[ \ ]D', \ [ \ ]  coMPs <>SLASH \ [ \ ]  ICOMPS >SLASHwant(equi verb)In the trees shown, nodes detected as foot nodesare marked with *.
Because of the SUBJ and SLASHvalues, the HEAD-DTR is the foot of T2 below (an-chored by an adverb) and COMP-DTR is the foot ofT3 (anchored by a raising verb).
Note that in thetree T1 anchored by an equi-verb, the foot nodeis detected because the SLASH value is shared, al-though the SUBJ is not.
As mentioned, we assumethat bridge verbs, i.e., verbs which allow extractionout of their complements, hare their SLASH valuewith their clausal complement.3.4 Terminat ionReturning to the basic algorithm, we will now con-sider the issue of termination, i.e., how much do weneed to reduce as we project a tree from a lexicalitem.Normally, we expect a SF with a specified valueto be reduced fully to an empty list by a series of ap-plications of rule schemata.
However, note that theSLASH value is unspecified at the root of the treesT2 and T3.
Of course, such nodes would still uni-fy with the SD of the filler-head-schema (which re-duces SLASH), but applying this schema could leadto an infinite recursion.
Applying a reduction to anunspecified SF is also linguistically unmotivated asit would imply that a functor could be applied to anargument that it never explicitly selected.However, simply blocking the reduction of a SFwhenever its value is unspecified isn't sufficient.
Forexample, the root of T2 specifies the subs to be anon-empty list.
Intuitively, it would not be appro-priate to reduce it further, because the lexical anchor(adverb) doesn't semantically license the SUBJ argu-ment itself.
It merely constrains the modified headto have an unsaturated SUBS.\[ suBs \[\] \] T2 COMPS < >SLASH \[ \ ], \[suBJ \ [ \ ]<\ [1>I , D \[ \ ]  COMPS < > L ' SLASH \[ \ ]  JSUBJ < > \] ,COMPS < > J SLASH < > M0D \[\]VP-adverbRaising Verb (and Infinitive Marker to)-N-L \[SLASH \[~\]COMPS / s LCOMPS\[<> J ?
\vp \[H-L\[SLASH \ [ \ ] \ ]96ID:COMPSSLASHraising verb\[\] \]T3 COMPS < >SLASH \ [ \ ]\[ COMPSSLASHD\] <>\[\]To motivate our termination criterion, considerthe adverb tree and the asterisked node (whose SLASHvalue is shared with SLASH at the root).
Being anon-trunk node, it will either be a foot or a sub-stitution node.
In either case, it will eventually beunified with some node in another tree.
If that oth-er node has a reducible SLASH value, then we knowthat the reduction takes place in the other tree, be-cause the SLASH value must have been raised acrossthe domination link where adjoining takes place.
Asthe same SLASH (and likewise suB J) value should notbe reduced in both trees, we state our terminationcriteria as follows:Termination Cr i te r ion  The value of an SF F atthe root node of a tree is not reduced furtherif it is an empty list, or if it is shared withthe value of F at some non-trunk node in thefrontier.Note that because of this termination criterion,the adverb tree projection will stop at this point.
Asthe root shares some selector feature values (SLASHand SUB J) with a frontier node, this node becomesthe foot node.
As observed above, adjoining thistree will preserve these values across any dominationlinks where it might be adjoined; and if the valuesstated there are reducible then they will be reducedin the other tree.
While auxiliary trees allow argu-ments selected at the root to be realized elsewhere,it is never the case for initial trees that an argu-ment selected at the root can be realized elsewhere,because by our definition of initial trees the selec-tion of arguments i not passed on to a node in thefrontier.We also obtain from this criterion a notion of localcompleteness.
A tree is locally complete as soon asall arguments which it licenses and which are notlicensed elsewhere are realized.
Global completenessis guaranteed because the notion of "elsewhere" isonly and always defined for auxiliary trees, whichhave to adjoin into an initial tree.3.5 Add i t iona l  PhasesAbove, we noted that the preservation of some SFsalong a path (realized as a path from the root tothe foot of an auxiliary tree) does not imply that allSFs need to be preserved along that path.
Tree T1provides such an example, where a lexical item, anequi-verb, triggers the reduction of an SF by takinga complement that is unsaturated for SUBJ but nevershares this value with one of its own SF values.To allow for adjoining of auxiliary trees whoseroot and foot differ in their SFs, we could producea number of different rees representing partial pro-jections from each lexical anchor.
Each partial pro-jection could be produced by raising some subset ofSFs across each domination link, instead of raisingall SFs.
However, instead of systematically raisingall possible subsets of SFs across domination links,we can avoid producing a vast number of these par-tial projections by using auxiliary trees to provideguidance in determining when we need to raise onlya particular subset of the SFs.Consider T1 whose root and foot differ in theirSFs.
From this we can infer that a SUBJ SF shouldnot always be raised across domination links in thetrees compiled from this grammar.
However, it isonly useful to produce a tree in which the susJ valueis not raised when the bottom of a domination linkhas both a one element list as value for SUBJ andan empty COMPS list.
Having an empty SUBJ list atthe top of the domination link would then allow foradjunction by trees such as T1.This leads to the following multi-phase compila-tion algorithm.
In the first phase, all SFs are raised.It is determined which trees are auxiliary trees, andthen the relationships between the SFs associatedwith the root and foot in these auxiliary trees arerecorded.
The second phase begins with lexical typesand considers the application of sequences of ruleschemata s before.
However, immediately after ap-plying a rule schema, the features at the bottom ofa domination link are compared with the foot nodesof auxiliary trees that have differing SFs at foot androot.
Whenever the features are compatible withsuch a foot node, the SFs are raised according to therelationship between the root and foot of the auxil-iary tree in question.
This process may need to beiterated based on any new auxiliary trees producedin the last phase.3.6 Example DerivationIn the following we provide a sample derivation forthe sentence(I know) what Kim wants to give to Sandy.Most of the relevant HPSG rule schemata nd lex-ical entries necessary to derive this sentence werealready given above.
For the noun phrases what,Kim and Sandy, and the preposition to no specialassumptions are made.
We therefore only add theentry for the ditransitive verb give, which we taketo subcategorize for a subject and two object com-plements.97Ditransitive VerbL c?MPS imp\[ \]pp\[ 1)From this lexical entry, we can derive in thefirst phase a fully saturated initial tree by apply-ing first the lexical slash-termination rule, and thenthe head-complement-, head-subject and filler-head-rule.
Substitution at the nodes on the frontier wouldyield the string what Kim gives to Sandy.T4 COMPS < >SLASH < >\[\]NPwhatI v:ICOMPS < >SLASH < \[ \ ]  >\ [ \ ]  ,NP D', ' \[susJ '<\[\]>\]Kim COMPS < >SLASH < \[ \ ]  >, \ [ \ ]V', ppICOMPS < > to SandySLASH < >COMPS < , >SLASH < >givesThe derivations for the trees for the matrix verbwant and for the infinitival marker to (equivalent toa raising verb) were given above in the examples T1and T3.
Note that the suBJ feature is only reducedin the former, but not in the latter structure.In the second phase we derive from the entry forgive another initial tree (Ts) into which the auxiliarytree T1 for want can be adjoined at the topmostdomination link.
We also produce a second tree withsimilar properties for the infinitive marker to (T6).SUBJ <> \]T5 COMPS < >SLASH < >NP COMPS < >SLASH < \[\]  >whatD:ICOMPS < >SLASH < \[ \ ]  >, \ [ \ ]D', ppICOMPS < to SandySLASH <COMPS < , \ [ \ ]  >SLASH < >giveT6 COMPS < >SLASH < \ [ \ ]  >.
:SLASH \ [ \ ]  JD' , \ [ \ ]  COMPS < >SLASH \[ \ ]COMPS > *SLASHtoBy first adjoining the tree T6 at the topmost dom-ination link of T5 we obtain a structure T7 corre-sponding to the substring what ... to give to Sandy.Adjunction involves the identification of the footnode with the bottom of the domination link andidentification of the root with top of the domina-tion link.
Since the domination link at the root ofthe adjoined tree mirrors the properties of the ad-junction site in the initial tree, the properties of thedomination link are preserved.98SUBJ <> \]T7 COMPS <SLASH < >NP COMPS < >SLASH < \ [ \ ]  >what 'D:ICOMPS < >SLASH < \[ \ ]  >\[ \[ COMPS < > \[\] COMPS < >SLASH < > SLASH < \[\]  >, \ [ \ ]D: ppICOMPS < > to SandySLASH < >"?1COMPS < , >SLASH < >giveThe final derivation step then involves the adjunc-tion of the tree for the equi verb into this tree, againat the topmost domination link.
This has the effectof inserting the substring Kim wants into what ... togive to Sandy.4 Conc lus ionWe have described how HPSG specifications can becompiled into TAG, in a manner that is faithful toboth frameworks.
This algorithm has been imple-mented in Lisp and used to compile a significantfragment of a German HPSG.
Work is in progress oncompiling an English grammar developed at CSLI.This compilation strategy illustrates how linguis-tic theories other than those previously exploredwithin the TAG formalism can be instantiated inTAG, allowing the association of structures with anenlarged domain of locality with lexical items.
Wehave generalized the notion of factoring recursion inTAG, by defining auxiliary trees in a way that is notonly adequate for our purposes, but also provides auniform treatment of extraction from both clausaland non-clausal complements (e.g., VPs) that is notpossible in traditional TAG.It should be noted that the results of our compila-tion will not always conform to conventional linguis-tic assumptions often adopted in TAGs, as exempli-fied by the auxiliary trees produced for equi verbs.Also, as the algorithm does not currently include anydownward expansion from complement nodes on thefrontier, the resulting trees will sometimes be morefractioned than if they had been specified irectly ina TAG.We are currently exploring the possiblity of com-piling HPSG into an extension of the TAG formal-ism, such as D-tree grammars (RVW95) or the UVG-DL formalism (Ram94).
These somewhat more pow-erful formalisms appear to be adequate for somephenomena, such as extraction out of adjuncts (re-call ?2) and certain kinds of scrambling, which ourcurrent method does not handle.
More flexiblemethods of combining trees with dominance linksmay also lead to a reduction in the number of treesthat must be produced in the second phase of ourcompilation.There are also several techniques that we expectto lead to improved parsing efficiency of the resultingTAG.
For instance, it is possible to declare specificnon-SFs which can be raised, thereby reducing thenumber of useless trees produced uring the multi-phase compilation.
We have also developed a schemeto effectively organize the trees associated with lex-ical items.ReferencesRobert Kasper.
On Compiling Head Driven PhraseStructure Grammar into Lexicalized Tree AdjoiningGrammar.
In Proceedings of the 2 "a Workshop onTAGs, Philadelphia, 1992.A.
K. Joshi, K. Vijay-Shanker and D. Weir.
The con-vergence of mildly context-sensitive grammatical for-malisms.
In P. Sells, S. Shieber, and T. Wasow, eds.,Foundational Issues in Natural Language Processing.MIT Press, 1991.Carl Pollard and Ivan Sag.
Head Driven Phrase Struc-ture Grammar.
CSLI, Stanford &: University of Chica-go Press, 1994.O.
Rambow.
Formal and Computational Aspects ofNatural Language Syntax.
Ph.D. thesis.
Univ.
ofPhiladelphia.
Philadelphia, 1994.O.
Rambow, K. Vijay-Shanker and D. Weir.
D-TreeGrammars.
In: ACL-95.Y.
Schabes, A. Abeille, and A. K. Joshi.
Parsing Strate-gies with 'Lexicalized' Grammars: Application toTree Adjoining Grammars.
COLING-88, pp.
578-583.Y.
Schabes, and A. K. Joshi.
Parsing with lexicalizedtree adjoining grammar.
In M. Tomita, ed., Cur-rent Issues in Parsing Technologies.
Kluwer AcademicPublishers, 1990.K.
Vijay-Shanker.
Using Descriptions of Trees in a TAG.Computational Linguistics, 18(4):481-517, 1992.K.
Vijay-Shanker and A. K. Joshi.
Feature StructureBased Tree Adjoining Grammars.
In: COLING-88.
: 99
