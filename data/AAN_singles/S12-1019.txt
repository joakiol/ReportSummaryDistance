First Joint Conference on Lexical and Computational Semantics (*SEM), pages 114?123,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsSelecting Corpus-Semantic Models for Neurolinguistic DecodingBrian MurphyMachine Learning Dept.Carnegie Mellon UniversityPittsburgh, USAbrianmurphy@cmu.eduPartha TalukdarMachine Learning Dept.Carnegie Mellon UniversityPittsburgh, USAppt@cs.cmu.eduTom MitchellMachine Learning Dept.Carnegie Mellon UniversityPittsburgh, USAtom.mitchell@cs.cmu.eduAbstractNeurosemantics aims to learn the mappingbetween concepts and the neural activitywhich they elicit during neuroimaging ex-periments.
Different approaches have beenused to represent individual concepts, butcurrent state-of-the-art techniques requireextensive manual intervention to scale toarbitrary words and domains.
To over-come this challenge, we initiate a system-atic comparison of automatically-derivedcorpus representations, based on varioustypes of textual co-occurrence.
We findthat dependency parse-based features arethe most effective, achieving accuraciessimilar to the leading semi-manual ap-proaches and higher than any publishedfor a corpus-based model.
We also findthat simple word features enriched withdirectional information provide a close-to-optimal solution at much lower computa-tional cost.1 IntroductionThe cognitive plausibility of computationalmodels of word meaning has typically beentested using behavioural benchmarks, such asidentification of synonyms among close asso-ciates (the TOEFL task for language learners,see e.g.
Landauer and Dumais, 1997); emulatingelicited judgments of pairwise similarity (such asRubenstein and Goodenough, 1965); judgmentsof category membership (e.g.
Battig and Mon-tague, 1969); and word priming effects (Lundand Burgess, 1996).
Mitchell et al (2008) in-troduced a new task in neurosemantic decoding?
using models of semantics to learn the map-ping between concepts and the neural activitywhich they elicit during neuroimaging experi-ments.
This was achieved with a linear modelwhich used training data to find neural basis im-ages that correspond to the assumed semanticdimensions (for instance, one such basis imagemight be the activity of the brain for words rep-resenting animate concepts), and subsequentlyused these general patterns and known seman-tic dimensions to infer the fMRI activity thatshould be elicited by an unseen stimulus con-cept.
Follow-on work has experimented withother neuroimaging modalities (Murphy et al,2009), and with a range of semantic models in-cluding elicited property norms (Chang et al,2011), corpus derived models (Devereux andKelly, 2010; Pereira et al, 2011) and structuredontologies (Jelodar et al, 2010).The current state-of-the-art performance onthis task is achieved using models that are hand-tailored in some respect, whether using manualannotation tasks (Palatucci et al, 2009), use ofa domain-appropriate curated corpus (Pereiraet al, 2011), or selection of particular collocatesto suit the concepts to be described (Mitchellet al, 2008).
While these approaches are clearlyvery successful, it is questionable whether theyare a general solution to describe the vari-ous parts-of-speech and semantic domains thatmake up a speaker?s vocabulary.
The Mitchellet al (2008) 25-verb model would probably haveto be extended to describe the lexicon at large,and it is unclear whether such a compact modelcould be maintained.
While Wikipedia (Pereiraet al, 2011) has very broad and increasing cov-114erage, it is possible that it will remain inad-equate for specialist vocabularies, or for less-studied languages.
And while the method usedby Palatucci et al (2009) distributes the anno-tation task efficiently by crowd-sourcing, it stillrequires that appropriate questions are compiledby researchers, a task that is both difficult toperform in a systematic way, and which may notgeneralize to more abstract concepts.In this paper we examine a representative setof corpus-derived models of meaning, that re-quire no manual intervention, and are applicableto any syntactic and semantic domain.
We con-centrate on which types of basic corpus patternperform well on the neurosemantic decodingtask: LSA-style word-region co-occurrences,and various HAL-style word-collocate featuresincluding raw tokens, POS tags, and a full de-pendency parse.
Otherwise a common featureextraction and preprocessing pipeline is used: aco-occurrence frequency cutoff, application of afrequency normalization weighting, and dimen-sionality reduction with SVD.The following section describes how the brainactivity data was gathered and processed; theconstruction of several corpus-derived modelsof meaning; and the regression-based meth-ods used to predict one from the other, evalu-ated with a brain-image matching task (Mitchellet al, 2008).
In section 3 we report the re-sults, and in the Conclusion we discuss both thepractical implications, and what this works sug-gests for the cognitive plausibility of distribu-tional models of meaning.2 Methods2.1 Brain activity featuresThe dataset used here is that described in detailin (Mitchell et al, 2008) and released publicly1in conjunction with the NAACL 2010 Work-shop on Computational Neurolinguistics (Mur-phy et al, 2010).
Functional MRI (fMRI) datawas collected from 9 participants while they per-formed a property generation task.
The stimuliwere line-drawings, accompanied by their text1http://www.cs.cmu.edu/afs/cs/project/theo-73/www/science2008/data.htmllabel, of everyday concrete concepts, with 5 ex-emplars of each of 12 semantic classes (mam-mals, body parts, buildings, building parts,clothes, furniture, insects, kitchen utensils, mis-cellaneous functional artifacts, work tools, veg-etables, and vehicles).
Stimuli remained onscreen for three seconds, and each was each pre-sented six times, in random order, to give a totalof 360 image presentations in the session.The fMRI images were recorded with 3.0Tscanner at 1 second intervals, with a spatial reso-lution of 3x3x6mm.
The resulting data was pre-processed with the SPM package (Friston et al,2007); the blood-oxygen-level response was ap-proximated by taking a boxcar average over asequence of brain images in each trial; percentsignal change was calculated relative to rest pe-riods, and the data from each of the six repeti-tions of each stimulus were averaged to yield asingle brain image for each concept.
Finally, agrey-matter anatomical mask was used to selectonly those voxels (three-dimensional pixels) thatoverlap with cortex, yielding approximately 20thousand features per participant.2.2 Models of semanticsOur objective is to compare current semanticrepresentations that get state-of-the-art perfor-mance on the neuro-semantics task with repre-sentative distributional models of semantics thatcan be derived from arbitrary corpora, usingvarying degrees of linguistic preprocessing.
Aseries of candidate models were selected to rep-resent the variety of ways in which basic textualfeatures can be extracted and represented, in-cluding token co-occurrence in a small local win-dow, dependency parses of whole sentences, anddocument co-occurrence, among others.
Otherparameters were kept fixed in a way that theliterature suggests would be neutral to the var-ious models, and so allow a fair comparisonamong them (Sahlgren, 2006; Bullinaria andLevy, 2007; Turney and Pantel, 2010).All textual statistics were gathered from a setof 50m English-language web-page documentsconsisting of 16 billion words.
Where a fixedtext window was used, we chose an extent of?4 lower-case tokens either side of the target115word of interest, which is in the mid-range ofoptimal values found by various authors (Lundand Burgess, 1996; Rapp, 2003; Sahlgren, 2006).Positive pointwise-mutual-information (1,2) wasused as an association measure to normalizethe observed co-occurrence frequency p(w, f) forthe varying frequency of the target word p(w)and its features p(f).
PPMI up-weights co-occurrences between rare words, yielding posi-tive values for collocations that are more com-mon than would be expected by chance (i.e.
ifword distributions were independent), and dis-cards negative values that represent patterns ofco-occurrences that are rarer than one would ex-pect by chance.
It has been shown to performwell generally, with both word- and document-level statistics, in raw and dimensionality re-duced forms (Bullinaria and Levy, 2007; Turneyand Pantel, 2010).2PPMIwf ={PMIwf if PMIwf > 00 otherwise(1)PMIwf = log(p(w, f)p(w)p(f))(2)A frequency threshold is commonly appliedfor three reasons: low-frequency co-occurrencecounts are more noisy; PMI is positively bi-ased towards hapax co-occurrences; and dueto Zipfian distributions a cut-off dramaticallyreduces the amount of data to be processed.Many authors use a threshold of approximately50-100 occurrences for word-collocate models(Lund and Burgess, 1996; Lin, 1998; Rapp,2003).
Since Bullinaria and Levy (2007) findimproving performance with models using pro-gressively lower cutoffs we explored two cut-offsof 20 and 50 which equate to low co-occurrencesthresholds of 0.00125 or 0.003125 per million re-spectively; for the word-region model we chosea threshold of 2 occurrences of a target term ina document, to keep the input features to a rea-sonable dimensionality (Bradford, 2008).After applying these operations to the inputdata from each model, the resulting dimension-2Preliminary analyses confirmed that PPMI per-formed as well or better than alternatives including log-likelihood, TF-IDF, and log-entropy.ality ranged widely, from about 500 thousand,to tens of millions.
A singular value decompo-sition (SVD) was applied to identify the 1000dimensions within each model with the great-est explanatory power, which also has the ef-fect of combining similar dimensions (such assynonyms, inflectional variants, topically simi-lar documents) into common components, anddiscarding more noisy dimensions in the data.Again there is variation in the number of di-mension that authors use: here we experimentwith 300 and 1000.
For decomposition we useda sparse SVD method, the Implicitly RestartedArnoldi Method (Lehoucq et al, 1998; Joneset al, 2001), which was coherent with the PPMInormalization used, since a zero value repre-sented both negative target-feature associations,and those that were not observed or fell belowthe frequency cut-off.
We also streamlined thetask by reducing the input data C (of n targetwords by m co-occurrence features) to a squarematrix CCT of size n ?
n, taking advantage ofthe equality of their left singular vectors U. ForSVD to generalize well over the many input fea-tures, it is also important to have more trainingcases that the small set of 60 concrete nounsused in our evaluation task.
Consequently wegathered all statistics over a set of the 40,000most frequent word-forms found in the Ameri-can National Corpus (Nancy Ide and Keith Su-derman, 2006), which should approximate thescale and composition of the vocabulary of auniversity-educated speaker of English (Nationand Waring, 1997), and over 95% of tokens typ-ically encountered in English.2.2.1 Hand-tailored benchmarksThe state-of-the-art models on this brain ac-tivity prediction task are both hand-tailored.Mitchell et al (2008) used a model of seman-tics based on co-occurrence in the Google 1T 5-gram corpus of English (Brants and Franz, 2006)with a small set of 25 Verbs chosen to rep-resent everyday sensory-motor interaction withconcrete objects, such as see, move, listen.
Werecreated this using our current parameters (webdocument corpus, co-occurrence frequency cut-off, PPMI normalization).
The second hand-116tailored dataset we used was a set of ElicitedProperties inspired by the 20 Questions game,and gathered using Mechanical Turk (Palatucciet al, 2009; Palatucci, 2011).
Multiple infor-mants were asked to answer one or more of 218questions ?related to size, shape, surface prop-erties, and typical usage?
such as Do you seeit daily?, Is it wild?, Is it man-made?
with ascalar response ranging from 1 to 5.
The re-sulting responses were then averaged over infor-mants, and then the values of each question weregrouped into 5 bins, giving all dimensions simi-lar mean and variance.2.2.2 Word-Region ModelLatent Semantic Analysis (Deerwester et al,1990; Landauer and Dumais, 1997), and itsprobabilistic cousins (Blei et al, 2003; Grif-fiths et al, 2007), express the meaning of aword as a distribution of co-occurrence acrossa set of documents, or other text-regions suchas paragraphs.
This word-region matrix in-stantiates the assumption that words that sharea topical domain (such as medicine, entertain-ment, philosophy) would be expected to appearin similar sub-sets of text-regions.
In such amodel, the nearest neighbors of a target wordare syntagmatically related (i.e.
appear along-side each other), and for judge might includelawyer, court, crime, or prison.The Document model used here is looselybased on LSA, taking the frequency of occur-rence of each of our 40,000 vocabulary wordsin each of 50 million documents as its inputdata, and it follows Bullinaria and Levy (2007);Turney and Pantel (2010) in using PPMI as anormalization function.
We have not investi-gated variations on the decomposition algorithmin any detail, such as those using non-negativematrix factorization, probabilistic LSA or LDAtopic models, as the objective in this paper isto provide a direct comparison between the dif-ferent types of basic collocation information en-coded in corpora, rather than evaluate the bestalgorithmic means for discovering latent dimen-sions in those co-occurrences.
Nor have we eval-uated performance on a more structured corpusinput (Pereira et al, 2011).
However prelimi-nary tests with the Wikipedia corpus, and withLDA, using the Gensim package (Rehurek andSojka, 2010) yielded similar performances.2.2.3 Word-Collocate ModelsWord-collocate models make a complemen-tary assumption to that of the document model:that words with closely-related categorical ortaxonomic properties should appear in the sameposition of similar sentences.
In a basic word-collocate model, based on a word-word co-occurrence matrix, the nearest neighbors ofjudge might be athlete, singer, or fire-fighter,reflecting paradigmatic relatedness (i.e.
substi-tutability).
Word-collocate models are furtherdifferentiated by the amount of linguistic anno-tation attached to word features, ranging fromsimple word-form features in a fixed-width win-dow around the concept word, to more elaborateword sequence patterns and parses includingparts of speech and dependency relation tags.Among these alternatives, we might expect adependency model to be the most powerful.
In-tuitively, the information that John is sentientis similarly encoded in the text John likes cakeand John seems to really really like cake, and asuitably effective parser should be able to gen-eralize over this variation, to extract the samedependency relationship of John-subject-like.
Incontrast a narrow window-based model mightexclude informative features (such as like in thesecond example), while including presumablyuninformative ones (such as really).
Howeverparsers have the disadvantage of being computa-tionally expensive (meaning that they typicallyare applied to smaller corpora) and usually in-troduce some noise through their errors.
Conse-quently, simpler window-based models have of-ten been found to be as effective.The most basic model considered is theWord-Form model, in which all lower-case to-kens (word forms and punctuation) found withinfour positions left and right of the target wordare recorded, yielding simple features such as{john, likes}.
It may also be termed a ?flat?model in contrast to those which assign a vari-able weight to collocates, progressively lower asone moves further than the target position (e.g.117Lund et al, 1995).
We did not use a stop-list, asBullinaria and Levy (2007) found co-occurrencewith very high frequency words also to be infor-mative for semantic tasks.
We also expect thatthe subsequent steps of normalizing with PPMI,reduction with SVD, and use of regularised re-gression should be able to recognize when suchhigh-frequency words are not informative andthen discount these, without the need for suchassumptions upfront.The Stemmed model is a slight variation onthe Word-Form model, where the same statisticsare aggregated after applying Lancaster stem-ming (Paice, 1990; Loper and Bird, 2002).The Directional model, inspired by Schu?tzeand Pedersen (1993), is also derived from theword-form model, but differentiates between co-occurrence to the left or to the right of the targetword, with features such as {john L, cake R}.The Part-of-Speech model (Kanejiya et al,2003; Widdows, 2003) replaces each lower-case word-token with its part-of-speech disam-biguated form (e.g.
likes VBZ, cake NN ).
Theseannotations were extracted from the full depen-dency parse described below.The Sequence model draws on a range ofwork that uses word sequence patterns (Lin andPantel, 2001; Almuhareb and Poesio, 2004; Ba-roni et al, 2010), and may also be considered anapproximation of models that use shallow syn-tactic analysis (Grefenstette, 1994; Curran andMoens, 2002).
All distinct token sequences upto length 4 either side of the target word werecounted.Finally the Dependency model uses a fulldependency parse, which might be consideredthe most informed representation of the word-collocate relationships instantiated in corpussentences, and this approach has been used byseveral authors (Lin, 1998; Pado?
and Lapata,2007; Baroni and Lenci, 2010).
The featuresused are pairs of dependency relation and lex-eme corresponding to each edge linked to a tar-get word of interest (e.g.
likes subj ).
The parserused here was Malt, which achieves accuracies of85% when deriving labelled dependencies on En-glish text (Hall et al, 2007).
The features pro-duced by this module are much more limited,to those words that have a direct dependencyrelation with the word of interest.2.3 Linear Learning ModelA linear regression model will allow us to eval-uate how well a given model of word semanticscan be used to predict brain activity.
We fol-low the analysis in Mitchell et al (2008) andsubsequently adopted by several other researchgroups (see Murphy et al, 2010).
For each par-ticipant and selected fMRI feature (i.e.
eachvoxel, which records the time-course of neuralactivity at a fixed location in the brain), we traina model where the level of activation of the latter(the blood oxygenation level) in response to dif-ferent concepts is approximated by a regularisedlinear combination of their semantic features:f = C?
+ ?||?||2 (3)where f is the vector of activations of a spe-cific fMRI feature for different concepts, the ma-trix C contains the values of the semantic fea-tures for the same concepts, ?
is the vector ofweights we must learn for each of those (corpus-derived) features, and ?
tunes the degree of reg-ularisation.
We can illustrate this with a toyexample, containing several stimulus conceptsand their attributes on three semantic dimen-sions: cat (+animate, -big, +moving); phone(-animate, -big, -moving); elephant (+animate,+big, +moving); skate-board (-animate, -big,+moving).
After training over all the voxels inour fMRI data with this simple semantic model,we can derive whole brain images that are typ-ical of each of the semantic dimensions.
Thepower of the model is its ability to predict ac-tivity for concepts that were not in the trainingset ?
for instance the brain activation elicited bythe word car might be approximated by combin-ing the images see for -animate, +big, +moving,even though this combination of properties wasnot observed during training.The linear model was estimated with aleast squared errors method and L2 regularisa-tion, selecting the lambda parameter from therange 0.0001 to 5000 using Generalized Cross-Validation (see Hastie et al, 2011, p.244).
The118activation of each fMRI voxel in response to anew concept that was not in the training datawas predicted by a ?-weighted sum of the val-ues on each semantic dimension, building a pic-ture of expected the global neural activity re-sponse for an arbitrary concept.
Again follow-ing Mitchell et al (2008) we use a leave-2-outparadigm in which a linear model for each neu-ral feature is trained in turn on all concepts mi-nus 2, having selected the 500 most stable voxelsin the training set using the same correlationalmeasure across stimulus presentations.
For eachof the 2 left-out concepts, we predict the globalneural activation pattern, as just described.
Wethen try to correctly match the predicted andobserved activations, by measuring the cosinedistance between the model-generated estimateof fMRI activity and the that observed in the ex-periment.
If the sum of the matched cosine dis-tances is lower than the sum of the mismatcheddistances, we consider the prediction successful?
otherwise as failed.
At chance levels, expectedmatching accuracy is 50%, and significant per-formance above chance can be estimated usingthe binomial test, once variance had been veri-fied over independent trials (i.e.
where no singlestimulus concept is shared between pairs).3 ResultsTable 1 shows the main results of the leave-two-out brain-image matching task.
They showthe mean classification performance over 1770word pairs (60 select 2) by 9 participants.
All ofthese classification accuracies are highly signif-icant at p  0.001 over test trials (binomial,chance 50%, n=1770*9) and p < 0.001 overwords (binomial, chance 50%, n=60).
Therewere some significant differences between mod-els when making inferences over trials, but forthe small set of words used here it is not possibleto make firm conclusions about the superiorityof one model over the other, that could be confi-dently expected to generalize to other stimuli orexperiments.
However, we do achieve classifica-tion accuracies that are as high, or higher thanany previously published (Palatucci et al, 2009;Pereira et al, 2011), while models based on verySemantic Models Features Accuracy25 Verbs 25 78.5Elicited Properties 218 83.5Document (f2) 1000 76.2Word Form 1000 80.0Stemmed 1000 76.2Direction 1000 80.2Part-of-Speech 1000 80.0Sequence 1000 78.5Dependency 1000 83.1Table 1: Brain activity prediction accuracy on leave-2-out pair-matching task.
A frequency cutoff of 20was used for all 1000 dimensional models.Semantic Models 300 Feats.
1000 Feats.Document (f2) 79.9 76.2Word Form 78.1 80.0Stemmed 77.9 76.2Direction 80.0 80.2Part-of-Speech 77.9 80.0Sequence 72.9 78.5Dependency 81.6 83.1Table 2: Effect of SVD dimensionality in the leave-2-out pair-matching setting; frequency cutoff of 20.different basic features (directional word-forms;dependency relations; document co-occurrence)yield very similar performance.3.1 Effect of Number of DimensionsHere we evaluate what effect the number of SVDdimensions used has on the final performanceof various semantic models.
Experimental re-sults comparing 300 and 1000 dimensions arepresented in Table 2, all based on a frequencycutoff of 20.
We observe that performance im-proves in 5 out of 7 semantic models compared,with the highest performance achieved by theDependency model when 1000 SVD dimensionswere used.3.2 Effect of Frequency CutoffIn this section, we evaluate what effect frequencycutoff has on the brain prediction accuracy ofvarious semantic models.
From the results inTable 3, we observe only marginal changes asthe frequency cutoff varied from 20 to 50.
Thissuggests that the semantic models of this set of119Semantic Models Cutoff = 50 Cutoff = 20Document (f2) 79.9 79.9Word Form 78.5 78.1Stemmed 78.2 77.9Direction 80.8 80.0Part-of-Speech 77.5 77.9Sequence 74.4 72.9Dependency 81.3 81.6Table 3: Effect of frequency cutoff in the leave-2-outpair-matching setting; 300 SVD dimensions.words are not very sensitive to variations in thefrequency cutoff under current experimental set-tings, and do not benefit clearly from the de-crease in sparsity and increase in noise that alower threshold produces.3.3 Information Overlap AnalysisTo verify that the models are in fact substan-tially different, we performed a follow-on analy-sis that measured the informational overlap be-tween the corpus-derived models.
Given twomodels A and B, both with dimensionality 40thousand words by 300 SVD dimensions, we canevaluate the extent to which A (used as thepredictor semantic representation) contains theinformation encoded in B (the explained rep-resentation).
As shown in (4), for each SVDcomponent c, we take the left singular vectorbc as a dependent variable and fit it with a lin-ear model, using the matrix A (all left singu-lar vectors) as independent variables.
The ex-plained variance for this column is weighted byits squared singular value s2c in B, and the sum ofthese component-wise variances gives the totalvariance explained R2A?B.R2A?B =300?c=1s2c?s2cRA?bc (4)Figure 1 indicates that the first three models,which are all derived from token occurrences in a?4 window, are close to identical.
The sequenceand document models are relatively dissimilar,and the dependency model occupies a middleground, with some similarity to all the models.It is also interesting to note that the among thefirst cluster of word-form derived models, theFigure 1: Informational Overlap between Corpus-Derived Datasets, in R2directional one has the highest similarity to thedependency model.4 ConclusionThe main result of this study was that weachieved classification accuracies as high asany published, and within a fraction of a per-centage point of the human benchmark 20Questions data, using completely unsupervised,data-driven models of semantics based on a largerandom sample of web-text.
The most linguisti-cally informed among the models (and so, per-haps the most psychologically plausible), basedon dependency parses, is the most successful.Still the performance of sometimes radically dif-ferent models, from Document-based (syntag-matic) and Word-Form-based (paradigmatic), issurprisingly similar.
One reason for this may bethat we have reached a ceiling in performanceon the fMRI data, due to its inherent noise ?
inthis regard it is interesting to note that an at-tempt to classify individual concepts using thisdata directly, without an intervening model ofsemantics, also achieves about 80% (though on adifferent task, Shinkareva et al, 2008).
Anotherpossible explanation is that both methods revealequivalent sets of underlying semantic dimen-sions, but figure 1 suggests not.
Alternatively,it may be that the small set of 60 words exam-ined here may be as well-distinguished by means120of their taxonomic differences, as by their top-ical differences, a suggestion supported by theresults in Pereira et al (2011, see Figure 2A).From the perspective of computational effi-ciency however, some of the models have cleareradvantages.
The Dependency and Part-of-Speech models are processing-intensive, sincethe broad vocabulary considered requires thatthe very large quantities of text pass througha parsing or tagging pipeline (though thesetasks can be parallelized).
The Sequence andDocument models conversely require very largeamounts of memory to store all their featuresduring SVD.
In comparison, the Direction modelis impressive, as it achieves close to optimal per-formance, despite being very cheap to producein terms of processor time and memory foot-print.
Its relatively superior performance maybe due to the relatively fixed word-order of En-glish, making it a good approximation of a De-pendency model.
For instance, given the nar-row ?4 token windows used here, the Directionfeatures shaky Left and donate Right (relativeto a target noun) are probably nearly identicalto the Dependency features shaky Adj and do-nate Subj.
The Sequence model might also beseen as an approximate Dependency model, butone with the addition of more superficial colloca-tions such as ?fish and chips?
or ?Judge Judy?,which are less relevant to our semantic task.The evidence for the influence of the scal-ing parameters (number of SVD dimensions,frequency cutoff) is mixed: cut-off appears tohave little effect either way, and increasing thenumber of dimensions can help or hinder (com-pare the Sequence and Document models).
Wecan speculate that the Document model is al-ready ?saturated?
with 300 dimensions/topics,but that the other models based on propertieshave a higher inherent dimensionality.
It mayalso be a lower cut-off and higher dimensional-ity would show clearer benefits over a larger setof semantic/syntactic domains, including lower-frequency words (the lowest frequency work inthe set of 60 used here was igloo, which has anincidence of 0.3 per million words in the ANC).PPMI appears to be both effective, and par-simonious with assumptions one might makeabout conceptual representations, where itwould be cognitively onerous and unnecessaryto encode all negative features (such as the factsthat dogs do not have wheels, are not commu-nication events, and do not belong in the avi-ation domain).
But while SVD is certainly ef-fective in dealing with the pervasive synonymyand polysemy seen in corpus-feature sets, it isless clear that it reveals psychologically plausi-ble dimensions of meaning.
Alternatives such asnon-negative matrix factorization (Lee and Se-ung, 1999) or Latent Dirichlet Allocation (Bleiet al, 2003) might extract more readily inter-pretable dimensions; or alternative regularisa-tion methods such as Elastic Nets, Lasso (Hastieet al, 2011), or Network Regularisation (Sandleret al, 2009) might even be capable of identifyingmeaningful clusters of features when learning di-rectly on co-occurrence data.
Finally, we shouldconsider whether more derived datasets could beused as input data in place of the basic corpusfeatures used here, such as the full facts learnedby the NELL system (Carlson et al, 2010), orcrowd-sourced data which can be easily gatheredfor any word (e.g.
association norms, Kiss et al,1973), though different algorithmic means wouldbe needed to deal with their extreme degree ofsparsity.The results also suggest a series of follow-onanalyses.
A priority should be to test thesemodels against a wider range of neuroimagingdata modalities (e.g.
MEG, EEG) and stim-ulus sets, including abstract kinds (see Mur-phy et al 2012, for a preliminary study), andparts-of-speech beyond nouns.
It may be that aputative complementarity between word-regionand word-collocate models is only revealed whenwe look at a broader sample of the humanlexicon.
And beyond establishing what infor-mational content is required to make semanticdistinctions, other factorisation methods (e.g.sparse or non-negative decompositions) could beapplied to yield more interpretable dimensions.Other classification tasks might also be moresensitive for detecting differences between mod-els, such as the test of word identification amonga set by rank accuracy, as used in (Shinkarevaet al, 2008).121ReferencesAlmuhareb, A. and Poesio, M. (2004).
Attribute-based and value-based clustering: An evaluation.In Proceedings of EMNLP, pages 158?165.Baroni, M. and Lenci, A.
(2010).
DistributionalMemory : A General Framework for Corpus-BasedSemantics.
Computational Linguistics, 36(4):673?721.Baroni, M., Murphy, B., Barbu, E., and Poesio, M.(2010).
Strudel: A corpus-based semantic modelbased on properties and types.
Cognitive Science,34(2):222?254.Battig, W. F. and Montague, W. E. (1969).
Cate-gory Norms for Verbal Items in 56 Categories: AReplication and Extension of the Connecticut Cat-egory Norms.
Journal of Experimental PsychologyMonographs, 80(3):1?46.Blei, D. M., Ng, A. Y., and Jordan, M. I.
(2003).Latent Dirichlet Allocation.
Journal of MachineLearning Research, 3(4-5):993?1022.Bradford, R. B.
(2008).
An empirical study of re-quired dimensionality for large-scale latent seman-tic indexing applications.
Proceeding of the 17thACM conference on Information and knowledgemining CIKM 08, pages 153?162.Brants, T. and Franz, A.
(2006).
Web 1T 5-gramVersion 1.Bullinaria, J.
A. and Levy, J. P. (2007).
Extractingsemantic representations from word co-occurrencestatistics: A computational study.
Behavior Re-search Methods, 39(3):510?526.Carlson, A., Betteridge, J., Kisiel, B., Settles, B.,Jr., E. R. H., and Mitchell, T. M. (2010).
To-ward an Architecture for Never-Ending LanguageLearning.
Artificial Intelligence, 2(4):3?3.Chang, K.-m. K., Mitchell, T., and Just, M.
A.(2011).
Quantitative modeling of the neural repre-sentation of objects: how semantic feature normscan account for fMRI activation.
NeuroImage,56(2):716?727.Curran, J. R. and Moens, M. (2002).
Improvementsin automatic thesaurus extraction.
In SIGLEX,pages 59?66.Deerwester, S., Dumais, S., Landauer, T., Furnas,G., and Harshman, R. (1990).
Indexing by La-tent Semantic Analysis.
Journal of the AmericanSociety of Information Science, 41(6):391 ?
407.Devereux, B. and Kelly, C. (2010).
Using fMRI ac-tivation to conceptual stimuli to evaluate meth-ods for extracting conceptual representations fromcorpora.
In Murphy, B., Korhonen, A., andChang, K. K.-M., editors, 1st Workshop on Com-putational Neurolinguistics.Friston, K. J., Ashburner, J. T., Kiebel, S. J.,Nichols, T. E., and Penny, W. D. (2007).
Statis-tical Parametric Mapping: The Analysis of Func-tional Brain Images, volume 8.
Academic Press.Grefenstette, G. (1994).
Explorations in AutomaticThesaurus Discovery.
Kluwer, Dordrecht.Griffiths, T. L., Steyvers, M., and Tenenbaum, J.
B.(2007).
Topics in semantic representation.
Psy-chological Review, 114(2):211?244.Hall, J., Nilsson, J., Nivre, J., Eryigit, G., Megyesi,B., Nilsson, M., and Saers, M. (2007).
Single Maltor Blended?
A Study in Multilingual Parser Op-timization.
CoNLL Shared Task Session, pages933?939.Hastie, T., Tibshirani, R., and Friedman, J.
(2011).The Elements of Statistical Learning, volume 18 ofSpringer Series in Statistics.
Springer, 5th edition.Jelodar, A.
B., Alizadeh, M., and Khadivi, S. (2010).WordNet Based Features for Predicting Brain Ac-tivity associated with meanings of nouns.
In Mur-phy, B., Korhonen, A., and Chang, K. K.-M., ed-itors, 1st Workshop on Computational Neurolin-guistics, pages 18?26.Jones, E., Oliphant, T., Peterson, P., and Et Al.(2001).
SciPy: Open source scientific tools forPython.Kanejiya, D., Kumar, A., and Prasad, S. (2003).Automatic evaluation of students?
answers usingsyntactically enhanced LSA.
Building educationalapplications, NAACL, 2:53?60.Kiss, G. R., Armstrong, C., Milroy, R., and Piper, J.(1973).
An associative thesaurus of English and itscomputer analysis.
In Aitken, A. J., Bailey, R. W.,and Hamilton-Smith, N., editors, The Computerand Literary Studies.
Edinburgh University Press.Landauer, T. and Dumais, S. (1997).
A solution toPlato?s problem: the latent semantic analysis the-ory of acquisition, induction, and representation ofknowledge.
Psychological Review, 104(2):211?240.Lee, D. D. and Seung, H. S. (1999).
Learning theparts of objects by non-negative matrix factoriza-tion.
Nature, 401(6755):788?91.Lehoucq, R. B., Sorensen, D. C., and Yang, C.(1998).
Arpack users?
guide: Solution of largescale eigenvalue problems with implicitly restartedArnoldi methods.
SIAM.122Lin, D. (1998).
Automatic Retrieval and Clusteringof Similar Words.
In COLING-ACL, pages 768?774.Lin, D. and Pantel, P. (2001).
DIRT ?
discoveryof inference rules from text.
Proceedings of theseventh ACM SIGKDD international conferenceon Knowledge discovery and data mining KDD 01,datamining:323?328.Loper, E. and Bird, S. (2002).
{NLTK}: The natu-ral language toolkit.
In ACL Workshop, volume 1,pages 63?70.
Association for Computational Lin-guistics.Lund, K. and Burgess, C. (1996).
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instru-ments, and Computers, 28:203?208.Lund, K., Burgess, C., and Atchley, R. (1995).
Se-mantic and associative priming in high dimen-sional semantic space.
In Proceedings of the 17thCognitive Science Society Meeting, pages 660?665.Mitchell, T. M., Shinkareva, S. V., Carlson, A.,Chang, K.-M., Malave, V. L., Mason, R. A., andJust, M. A.
(2008).
Predicting Human Brain Ac-tivity Associated with the Meanings of Nouns.
Sci-ence, 320:1191?1195.Murphy, B., Baroni, M., and Poesio, M. (2009).
EEGresponds to conceptual stimuli and corpus seman-tics.
In Proceedings of EMNLP, pages 619?627.ACL.Murphy, B., Korhonen, A., and Chang, K. K.-M., editors (2010).
Proceedings of the 1st Work-shop on Computational Neurolinguistics, NAACL-HLT, Los Angeles.
ACL.Murphy, B., Talukdar, P., and Mitchell, T. (2012).Comparing Abstract and Concrete ConceptualRepresentations using Neurosemantic Decoding.In NAACL Workshop on Cognitive Modelling andComputational Linguistics.Nancy Ide and Keith Suderman (2006).
The Amer-ican National Corpus First Release.
Proceedingsof the Fifth Language Resources and EvaluationConference (LREC).Nation, P. and Waring, R. (1997).
Vocabulary size,text coverage and word lists.
In Schmitt, N. andMcCarthy, M., editors, Vocabulary Description ac-quisition and pedagogy, pages 6?19.
CambridgeUniversity Press.Pado?, S. and Lapata, M. (2007).
Dependency-basedconstruction of semantic space models.
Computa-tional Linguistics, 33(2):161?199.Paice, C. D. (1990).
Another stemmer.
SIGIR Fo-rum, 24(3):56?61.Palatucci, M., Hinton, G., Pomerleau, D., andMitchell, T. M. (2009).
Zero-Shot Learning withSemantic Output Codes.
Advances in Neural In-formation Processing Systems, 22:1?9.Palatucci, M. M. (2011).
Thought Recognition: Pre-dicting and Decoding Brain Activity Using theZero-Shot Learning Model.
PhD thesis, CarnegieMellon University.Pereira, F., Detre, G., and Botvinick, M. (2011).Generating Text from Functional Brain Images.Frontiers in Human Neuroscience, 5:1?11.Rapp, R. (2003).
Word Sense Discovery Based onSense Descriptor Dissimilarity.
Proceedings of theNinth Machine Translation Summit, pp:315?322.Rehurek, R. and Sojka, P. (2010).
Software Frame-work for Topic Modelling with Large Corpora.
InNew Challenges, LREC 2010, pages 45?50.
ELRA.Rubenstein, H. and Goodenough, J.
B.
(1965).
Con-textual correlates of synonymy.
Communicationsof the ACM, 8(10):627?633.Sahlgren, M. (2006).
The Word-Space Model: Usingdistributional analysis to represent syntagmaticand paradigmatic relations between words in high-dimensional vector spaces.
Dissertation, Stock-holm University.Sandler, T., Talukdar, P. P., Ungar, L. H., andBlitzer, J.
(2009).
Regularized Learning with Net-works of Features.
Advances in Neural Informa-tion Processing Systems 21, 4:1401?1408.Schu?tze, H. and Pedersen, J.
(1993).
A Vector Modelfor syntagmatic and paradigmatic relatedness.
InMaking Sense of Words Proceedings of the 9thAnnual Conference of the University of WaterlooCentre for the New OED and Text Research, pages104?113.Shinkareva, S. V., Mason, R. A., Malave, V. L.,Wang, W., Mitchell, T. M., and Just, M.
A.(2008).
Using fMRI Brain Activation to Iden-tify Cognitive States Associated with Perceptionof Tools and Dwellings.
PloS ONE, 3(1).Turney, P. D. and Pantel, P. (2010).
From Frequencyto Meaning: Vector Space Models of Semantics.Artificial Intelligence, 37(1):141?188.Widdows, D. (2003).
Unsupervised methods for de-veloping taxonomies by combining syntactic andstatistical information.
In NAACL, pages 197?204.
Association for Computational Linguistics.123
