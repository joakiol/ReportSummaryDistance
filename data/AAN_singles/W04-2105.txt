Word lookup on the basis of associations:from an idea to a roadmapMichael ZOCKLIMSI-CNRSB.P.
133, 91403 Orsay,Francezock@limsi.frSlaven BILACTokyo Institute of TechnologyOokayama 2-12-1, Meguro 152-8552,Japansbilac@cl.cs.titech.ac.jpAbstractWord access is an obligatory step in languageproduction.
In order to achieve his communica-tive goal, a speaker/writer needs not only tohave something to say, he must also find thecorresponding word(s).
Yet, knowing a word,i.e.
having it stored in a data-base or memory(human mind or electronic device) does not im-ply that one is able to access it in time.
Thisis a clearly a case where computers (electronicdictionaries) can be of great help.In this paper we present our ideas of howan enhanced electronic dictionary can help peo-ple to find the word they are looking for.
Theyet-to-be-built resource is based on the age-oldnotion of association: every idea, concept orword is connected.
In other words, we assumethat people have a highly connected conceptual-lexical network in their mind.
Finding a wordamounts thus to entering the network at anypoint by giving the word or concept coming totheir mind (source word) and then following thelinks (associations) leading to the word they arelooking for(target word).Obviously, in order to allow for this kindof access, the resource has to be built accord-ingly.
This requires at least two things: (a) in-dexing words by the associations they evoke,(b) identification and labeling of the most fre-quent/useful associations.
This is precisely ourgoal.
Actually, we propose to build an associa-tive network by enriching an existing electronicdictionary (essentially) with (syntagmatic) as-sociations coming from a corpus, representingthe average citizen?s shared, basic knowledge ofthe world (encyclopedia).
Such an enhancedelectronic database resembles in many respectsour mental dictionary.
Combining the power ofcomputers and the flexibility of the human mind(omnidirectional navigation and quick jumps),it emulates to some extent the latter in its ca-pacity to navigate quickly and efficiently in alarge data base.While the notions of association and spread-ing activation are fairly old, their use to supportword access via computer is new.
The resourcestill needs to be built, and this is not a trivialtask.
We discuss here some of the strategies andproblems involved in accomplishing it with thehelp of people and computers (automation).1 IntroductionWe all experience now and then the problemof being unable to find the word expressing theidea we have in our mind.
It we care and havetime we may reach for a dictionary.
Yet, thiskind of resource may be of little help, if it ex-pects from us precisely what we are looking for :a perfectly spelled word, expressing the idea wetry to convey.
While perfect input may be rea-sonable in the case of analysis (comprehension),it certainly is not in the case of synthesis (gener-ation) where the starting point is conceptual innature: a message, the (partial) definition of aword, a concept or a word related to the targetword.
The language producer needs a dictio-nary allowing for reverse access.
A thesaurusdoes that, but only in a very limited way: theentry points are basically topical.People use various methods to initiate searchin their mind : words, concepts, partial descrip-tions, etc.
If we want to mimic these functional-ities by a computer, we must build the resourceaccordingly.
Let us assume that the text pro-ducer is looking for a word that he cannot ac-cess.
Instead he comes up with another word(or concept)1 somehow related to the former.He may not know precisely how the two relate,but he knows that they are related.
He may alsoknow to some extent how close their relation-ship is, whether a given link is relevant or not,that is, whether it can lead directly (synonym,1We will comment below on the difference betweenconcepts and words.antonym, hyperonym) or indirectly to the tar-get word.
Since the relationship between thesource- and the target word is often indirect,several lookups may be necessary: each one ofthem having the potential to contain either thetarget word (direct lookup), or a word leadingtowards it (indirect lookup).2 How reasonable is it to expectperfect input?The expectation of perfect input is unrealisticeven in analysis,2 but clearly more so in gener-ation.
The user may well be unable to providethe required information: be it because he can-not access in time the word he is looking for,even though he knows it,3 or because he doesnot know the word yet expressing the idea hewants to convey.
This latter case typically oc-curs when using a foreign language or when try-ing to use a very technical term.
Yet, not beingable to find a word, does not imply that onedoes not know anything concerning the word.Actually, quite often the contrary is the case.Suppose, you were looking for a word ex-pressing the following ideas: domesticated ani-mal, producing milk suitable for making cheese.Suppose further that you knew that the targetword was neither cow nor sheep.
While noneof this information is sufficient to guarantee theaccess of the intended word goat, the informa-tion at hand (part of the definition) could cer-tainly be used.
For some concrete proposals go-ing in this direction, see (Bilac et al, 2004), orthe OneLook reverse dictionary.4 Besides thedefinition information, people often have otherkind of knowledge concerning the target word.In particular, they know how the latter relatesto other words.
For example, they know thatgoats and sheep are somehow connected, thatboth of them are animals, that sheep are appre-ciated for their wool and meet, that sheep tendto follow each other blindly, while goats man-age to survive, while hardly eating anything,etc.
In sum, people have in their mind lexi-cal networks: all words, concepts or ideas theyexpress are highly interconnected.
As a result,any one of the words or concepts has the po-tential to evoke each other.
The likelihood for2Obviously, looking for ?pseudonym?
under the letter?S?
in a dictionary won?t be of great help.3Temporary amnesia, known as the TOT, or tip-of-the-tongue problem (Brown and McNeill, 1996; Zock andFournier, 2001; Zock, 2002)4http://www.onelook.com/reverse-dictionary.shtmlthis to happen depends, among other things, onsuch factors as frequency (associative strength),saliency and distance (direct vs. indirect ac-cess).
As one can see, associations are a verygeneral and powerful mechanism.
No matterwhat we hear, read or say, any idea is likely toremind us of something else.5 This being so, weshould make use of it.63 Search based on the relationsbetween concepts and wordsIf one agrees with what we have just said, onecould view the mental dictionary as a huge se-mantic network composed of nodes (words andconcepts) and links (associations), with eitherbeing able to activate the other.7 Finding a5The idea according to which the mental dictionary(or encyclopedia) is basically an associative network,composed of nodes (words or concepts) and links (as-sociations) is not new, neither is the idea of spreadingactivation.
Actually the very notion of association goesback at least to Aristotle (350BC), but it is also inher-ent in work done by philosophers (Locke, Hume), phys-iologists (James & Stuart Mills), psychologists (Galton,1880; Freud, 1901; Jung and Riklin, 1906) and psycholin-guists (Deese, 1965; Jenkins, 1970; Schvaneveldt, 1989).For surveys in psycholinguistics see (Ho?rmann, 1972), ormore recent work (Spitzer, 1999).
The notion of associa-tion is also implicit in work on semantic networks (Quil-lian, 1968), hypertext (Bush, 1945), the web (Nelson,1967), connectionism (Dell et al, 1999) and, of course,in WordNet (Miller et al, 1993; Fellbaum, 1998).6In the preceding sections we used several times theterms words and concepts interchangeably, as if they werethe same.
Of course, they are very different.
Yet, notknowing what a concept looks like (a single node, orevery node, i.e.
headword of the word?s definition?
), wethink it is safer to assume that the user can communicatewith the computer (dictionary) only via words.
Hence,concepts are represented by words, yet, since the twoare connected, one can be accessed via the other, whichaddresses the interface problem with the computer.
An-other point worth mentionning is the fact that associa-tions may depend on the nature of the arguments (wordsvs.
concepts).
While in theory anything can be associ-ated with anything (words with words, words with con-cepts, concepts with concepts, etc.
), in practice wordstend to trigger a different set of associations than con-cepts.
Also, the connectivity between words and con-cepts explains to some extent the power and the flexi-bility of the human mind.
Words are shorthand labelsfor concepts, and given the fact that the two are linked,one can make big leaps in no time and easily move fromone plane (let?s say the conceptual level) to the other(the linguistic counterpart).
Words can be reached viaconcepts, but the latter can also serve as starting pointto find a word.
Compared to the links between conceptswhich are a superhighway, associations between wordsare more like countryroads.7Actually, one could question the very notion of men-tal dictionary which is convenient, but misleading in asit supposes a dedicated part for this task in our brain.
AFigure 1: Search based on propagation in a network (internal representation)word amounts thus to entering the network andfollowing the links leading from the source node(the first word that comes to your mind) to thetarget word (the one you are looking for).
Sup-pose you wanted to find the word ?nurse?
(targetword), yet the only token coming to your mindwere ?hospital?.
In this case the system wouldgenerate internally a graph with the source wordat the center and all the associated words atthe periphery.
Put differently, the system wouldbuild internally a semantic network with ?hos-pital?
in the center and all its associated wordsas satellites (figure 1).8Obviously, the greater the number of associ-ations, the more complex the graph.
Given thediversity of situations in which a given objectmay occur we are likely to build many associa-tions.
In other words, lexical graphs tend to be-multiply indexed mental encyclopedia, composed of poly-morph information (concepts, words, meta-linguistic in-formation) seems much more plausible to us.8AKO: a kind of; ISA: subtype; TIORA: typicallyinvolved object, relation or actor.come complex, too complex to be a good repre-sentation to support navigation.
Readability ishampered by at least two factors: high connec-tivity (the great number of links or associationsemanating from each word), and distribution:conceptually related nodes, that is, nodes acti-vated by the same kind of assocation are scat-tered around, that is, they do not necessarilyoccur next to each other, which is quite confus-ing for the user.
In order to solve this problemwe suggest to display by category (chunks) allthe words linked by the same kind of associationto the source word (see figure 2).
Hence, ratherthan displaying all the connected words as a flatlist, we suggest to present them in chunks to al-low for categorial search.
Having chosen a cat-egory, the user will be presented a list of wordsor categories from which he must choose.
If thetarget word is in the category chosen by the user(suppose he looked for a hyperonyme, hence hechecked the ISA-bag), search stops, otherwise itgoes on.
The user could choose either anothercategory (eg.
AKO or TIORA), or a word inFigure 2: Proposed candidates, grouped accord-ing to the nature of the linkthe current list, which would then become thenew starting point.4 A resource still to be builtThe fact that the links are labeled has some veryimportant consequences.
(a) While maintainingthe power of a highly connected graph (possiblecyclic navigation), it has at the interface levelthe simplicity of a tree: each node points onlyto data of the same type, i.e.
same kind of asso-ciation.
(b) Words being presented in clusters,navigation can be accomplished by clicking onthe appropriate category.
The assumption be-ing that the user generally knows to which cate-gory the target word belongs (or at least, he canrecognize within which of the listed categories itfalls), and that categorical search is in principlefaster than search in a huge list of unordered(or, alphabetically ordered) words.Word access, as described here, amounts tonavigating in a huge associative network.
Ofcourse, such a network has to be built.
Thequestion is how.
Our proposal is to buildit automatically by parsing an existing corpuscontaining sufficient amount of information onworld knowledge (for example, an encyclope-dia).
This would yield a set of associations(see below),9 which still need to be labeled.
Arich ontology should be helpful in determiningthe adequate label for many, if not most of thelinks.
Unlike private information,10 which by9The assumption being that every word co-occurringwith another word in the same sentence is a candidate ofan association.
The more frequently two words co-occurin a given corpus, the greater their associative strength.10For example, the word elephant may remind you of adefinition cannot and should not be put into apublic dictionary,11 encyclopedic knowledge canbe added in terms of associations, as this infor-mation expresses commonly shared knowledge,that is, the kind of associations most peoplehave when encountering a given word.
Take forexample the word elephant.
An electronic dic-tionary like Word Net associates the followinggloss with the headword: large, gray, four-leggedmammal, while Webster gives the following in-formation:A mammal of the order Proboscidia,of which two living species, ElephasIndicus and E. Africanus, and severalfossil species, are known.
They havea proboscis or trunk, and two largeivory tusks proceeding from the ex-tremity of the upper jaw, and curvingupwards.
The molar teeth are largeand have transverse folds.
Elephantsare the largest land animals now ex-isting.While this latter entry is already quite rich(trunk, ivory tusk, size), an encyclopedia con-tains even more information.12 If all this in-formation were added to an electronic resource,it would enable us to access the same word(e.g.
elephant) via many more associations thanever before.
By looking at the definition hereabove, one will notice that many associationsare quite straightforward (color, size, origin,etc.
), and since most of them appear frequentlyin a pattern-like manner it should be possibleto extract them automatically (see footnote 18below).
If one agrees with these views, the re-maining question is how to extract this encyclo-pedic information and to add it to an existingelectronic resource.
Below we will outline somemethods for extracting associated words anddiscuss the feasibility of using current method-ology to achieve this goal.5 Automatic extraction of wordassociationsAbove we outlined the need for obtaining asso-ciations between words and using them to im-prove dictionary accessibility.
While the associ-ations can be obtained through association ex-periments with human subjects, this strategy isspecific animal, trip or location (zoo, country in Africa).11This does not (and should not) preclude the possi-bility to add it to one?s personal dictionary.12You may consider taking a look at Wikipedia (http://en.wikipedia.org/wiki/) which is free.not very satisfying due to the high cost of run-ning the experiments (time and money), anddue to its static nature.
Indeed, given the costs,it is impossible to repeat these experiments totake into account the evolution of a society.Hence, the goal is to automatically extract asso-ciations from large corpora.
This problem wasaddressed by a large number of researchers, butin most cases it was reduced to extraction of col-locations which are a proper subset of the set ofassociated words.
While hard to define, colloca-tions appear often enough in corpora to be ex-tractable by statistical and information-theorybased methods.There are several basic methods for evalu-ating associations between words: based onfrequency counts (Choueka, 1988; Wettler andRapp, 1993), information theoretic (Churchand Hanks, 1990) and statistical significance(Smadja, 1993).
The statistical significanceoften evaluate whether two words are inde-pendant using hypothesis tests such as t-score(Church et al, 1991), the X2, the log-likelihood(Dunning, 1993) and Fisher?s exact test (Peder-sen, 1996).
Extracted sets for associated wordsare further pruned using numerical methods, orlinguistic knowledge to obtain a subset of collo-cations.The various extraction measures have beendiscussed in great detail in the literature (Man-ning and Schu?tze, 1999; McKeown and Radev,2000), their performance has been compared(Dunning, 1993; Pedersen, 1996; Evert andKrenn, 2001), and the methods have been com-bined to improve overall performance (Inkpenand Hirst, 2002).
Most of these methods wereoriginally applied in large text corpora, butmore recently the web has been used as a cor-pus (Pearce, 2001; Inkpen and Hirst, 2002).Collocation extraction methods have been usednot only for English, but for many other lan-guages: French (Ferret, 2002), German (Ev-ert and Krenn, 2001) and Japanese (Nagao andMori, 1994), to cite but those.The most obvious question in this contextis to clarify to what extent available colloca-tion extraction techniques fulfill our needs of ex-tracting and labeling word associations.
Sincecollocations are a subset of association, it is pos-sible to apply collocation extraction techniquesto obtain related words, ordered in terms of therelative strength of association.The result of this kind of numerical extractionwould be a large set of numerically weightedword pairs.
The problem with this approach isthat the links are only labeled in terms of theirrelative associative strength, but not categori-cally, which makes it impossible to group andpresent them in a meaningful way for the dic-tionary user.
Clusters based only on the notionof association strength are inadequate for thekind of navigation described here above.
Henceanother step is necessary: qualification of thelinks according to their types.
Only once thisis done, a human being could use it to navi-gate through a large conceptual-lexical network(the dictionary) as described above.
Unfortu-nately, research on automatic link identificationhas been rather sparse.
Most attempts havebeen devoted to the extraction of certain typesof links (usually syntactic type (Lin, 1998) oron extensions of WordNet with topical informa-tion contained in a thesaurus (Stevenson, 2002)or on the WWW (Agirre et al, 2000).
Addi-tional methods need to be considered in orderto reveal (automatically) the kind of associa-tions holding between words and/or concepts.Earlier in this paper we have suggested the useof an encyclopedia as a source of general worldknowledge.
It should be noted, though, thatthere are important differences between largecorpora and encyclopedias.
Large corpora usu-ally contain a lot of repetitive texts on a lim-ited number of topics (e.g.
newspaper articles)which makes them very suitable for statisticalmethods.
On the other hand, while being max-imally informative and comprehensive, encyclo-pedias are written in a highly controlled lan-guage, and their content is continually updatedand re-edited, with the goal to avoid unneces-sary repetition.
While most of the informationcontained in an entry is important, there is alack of redundancy.
Hence, measures capable ofhandling word pairs with low appearance counts(e.g.
log-likelihood or Fisher?s exact test) shouldbe favored.
Also, rather than looking at indi-vidual words, one might want to look at wordpatterns instead.6 Discussion and ConclusionWe have raised and partially answered the ques-tion of how a dictionary should be indexed inorder to support word access.
We were partic-ularly concerned with the language producer,as his needs (and knowledge at the onset) arequite different from the ones of the language re-ceiver (listener/reader).
It seems that, in orderto achieve our goal, we need to do two things:add to an existing electronic dictionary informa-tion that people tend to associate with a word,that is, build and enrich a semantic network,and provide a tool to navigate in it.
To thisend we have suggested to label the links, as thiswould reduce the graph complexity and allowfor type-based navigation.
Actually our basicproposal is to extend a resource like WordNetby adding certain links, in particular on the hor-izontal axis (syntagmatic relations).
These linksare associations, and their role consists in help-ing the encoder to find ideas (concepts/words)related to a given stimulus (brainstorming), orto find the word he is thinking of (word access).One problem that we are confronted with is toidentify possible associations.
Ideally we wouldneed a complete list, but unfortunately, thisdoes not exist.
Yet, there is a lot of highlyrelevant information out there.
For exam-ple, Mel?cuk?s lexical functions (Mel?cuk, 1992),Fillmore?s FRAMENET13, work on ontolo-gies (CYC), thesaurus (Roget), WordNets (theoriginal version from Princeton, divers Euro-WordNets, BalkaNet), HowNet14, the workdone by MICRA, the FACTOTUM project15or the Wordsmyth dictionary/thesaurus combi-nation16.
Of course, one would need to makechoices here and probably add links.
Anotherproblem is to identify useful associations.
Notevery possible association is necessarily plausi-ble.
Hence, the idea to take as corpus some-thing that expresses shared knowledge, for ex-ample, an encyclopedia.
The associations itcontains can be considered as being plausible.We could also collect data by watching peo-ple using a dictionary and identify search pat-terns.17 Next, we could run psycholinguistic ex-periments.18 While the typical paradigm hasbeen to ask people to produce a response (red)to some stimulus (rose), we could ask them toidentify or label the links between words (e.g.apple-fruit, lemon-yellow, etc.).
The ease of la-13http://www.icsi.berkeley.edu/~framenet/14http://www.keenage.com/html/e_index.html15http://humanities.uchicago.edu/homes/MICRA/16http://www.wordsmyth.com/17One such pattern could be: give me the word for abird with yellow feet and a long beak, that can swim.Actually, word access problems frequently come underthe form of questions like: What is the word for X thatY?, where X is usually a hypernym and Y a stereotypical,possibly partial functional/relational/case description ofthe target word.18Actually, this has been done for decades, but witha different goal in mind (Nelson, 1967), http://cyber.acomp.usf.edu/FreeAssociation/.beling will probably depend upon the origin ofthe words (the person asked to label the link orsomebody else).Another approach would be to extract col-locations from a corpus and label them auto-matically.
There are tools for extracting co-occurrences (see section 5.5), and ontologiescould be used to qualify some of the links be-tween collocational elements.
While this ap-proach might work fine for couples like coffee-strong, or wine-red (since an ontology would re-veal that red is a kind of color, which is preciselythe link type: i.e.
association), one may doubtthat it could reveal the nature of the link be-tween smoke and fire.
Yet, most humans wouldimmediately recognize this as a causal link.
Asone can see, there are still quite a few seriousproblems to be solved.
Nevertheless, we do be-lieve that these obstacles can be removed, andthat the approach presented here has the poten-tial to improve word access, making the wholeprocess more powerful, natural and intuitive,hence efficient.ReferencesE.
Agirre, E. Hovy O. Ansa, and D. Mar-tinez.
2000.
Enriching very large ontologiesusing the WWW.
In Proc.
of ECAI OntologyLearning Workshop.S.
Bilac, W. Watanabe, T. Hashimoto, T. Toku-naga, and H. Tanaka.
2004.
Dictionarysearch based on the target word description.In Proc.
of the Tenth Annual Meeting of TheAssociation for Natural Language Processing(NLP2004), pages 556?559.R.
Brown and D. McNeill.
1996.
The tip ofthe tonuge phenomenon.
Journal of VerbalLearning and Verbal Behaviour, 5:325?337.V.
Bush.
1945.
As we may think.
The AtlanticMonthly, 176:101?108.Y.
Choueka.
1988.
Looking for needles in ahaystack.
In Proc.
of the RIAO Conferenceon User-Oriented Context Based Text andImage Handling, pages 609?623.K.
Church and P. Hanks.
1990.
Word associa-tion norms, mutual information and lexicog-raphy.
Computational Linguistics, 16:22?29.K.
Church, W. Gale, P. Hanks, and D. Hin-dle.
1991.
Using statistics in lexical analysis.In U. Zernik, editor, Lexical Acquisition: Ex-ploiting On-Line Resources to Build a Lexi-con.
Lawrance Erlbaum Associates.J.
Deese.
1965.
The structure of associations inlanguage and thought.
Johns Hopkins Press.G.
S. Dell, F. Chang, and Z. M. Griffin.
1999.Connectionist models of language produc-tion: Lexical access and grammatical encod-ing.
Cognitive Science, 23:517?542.T.
Dunning.
1993.
Accurate methods for statis-tics of surprise and coincidence.
Computa-tional Linguistics, 19:61?74.S.
Evert and B. Krenn.
2001.
Methods for thequalitative evaluation of lexical associationmeasures.
In Proc.
of the 39th Annual meet-ing of Association of Computational Linguis-tics (ACL 2001), pages 188?195.C.
Fellbaum.
1998.
WordNet: An ElectronicLexical Database and some of its Applica-tions.
MIT Press.O.
Ferret.
2002.
Using collocations for topicsegmentation and link detection.
In Proc.
ofthe 19th International Conference on Compu-tational Linguistics, pages 261?266.S.
Freud.
1901.
Psychopathology of everydaylife.
Payot, 1997 edition.F.
Galton.
1880.
Psychometric experiments.Brain, 2:149?162.H.
Ho?rmann.
1972.
Introduction a` la psycholin-quistique.
Larousse.D.
Z. Inkpen and G. Hirst.
2002.
Acquiringcollocations for lexical choice between near-synonyms.
In Proc.
of Unsupervised LexicalAcquisition Workshop of the ACL SIGLEX,pages 67?76.J.
J. Jenkins.
1970.
The 1952 minnesotaword association norms.
In L. Postman andG.
Kepper, editors, Norms of Word Associa-tion, pages 1?38.
Academic Press.C.
G. Jung and F. Riklin.
1906.
Experimentelleunter-suchungen u?ber assoziationene gesun-der.
In C. G. Jung, editor, Diagnostische As-soziationsstudien, pages 7?145.
Barth.D.
Lin.
1998.
Extracting collocations from textcorpor.
In First Workshop on ComputationalTerminology.C.
D. Manning and H. Schu?tze.
1999.
Foun-dations of Statistical Natural Language Pro-cessing.
The MIT Press, Cambridge, Mas-sachusetts.K.
R. McKeown and Dragomir R. Radev.2000.
Collocations.
In H. Moisl R. Daleand H. Somers, editors, Handbook of NaturalLanguage Processing, pages 507?523.
MarcelDekker.I.
Mel?cuk.
1992.
Dictionnaire Explicatifet Combinatoire du franc?ais contemporain:recherche lexicose?mantique III.
Les presses del?universite?
de Montre?al.G.
A. Miller, R. Beckwith, C. Fellbaum,D.
Gross, and Katherine Miller, editors.1993.
Introduction to WordNet: An On-lineLexical Database.
Cognitive Science Labora-tory, Princeton University.M.
Nagao and S. Mori.
1994.
A new method ofn-gram statistics for large number of n andautomatic extraction of words and phrasesfrom large text data of Japanese.
In Proc.
ofthe 15th International Conference on Compu-tational Linguistics (COLING 1994), pages611?615.T.
Nelson.
1967.
Xanadu projet hypertextuel.D.
Pearce.
2001.
Synonymy in collocationextraction.
In Proc.
of NAACL?01 Work-shop on WordNet and Other Lexical Re-sources: Applications, Extensions and Cus-tomizations.Ted Pedersen.
1996.
Fishing for exactness.
InProc.
of the South-Central SAS Users GroupConference, pages 188?195.R.
Quillian.
1968.
Semantic memory.
In M.Minsky, editor, Semantic Information Pro-cessing, pages 216?270.The MIT Press.
Cam-bridge, MA.R.
Schvaneveldt, editor.
1989.
Pathfinder As-sociative Networks: studies in knowledge or-ganization.
Norwood.F.
Smadja.
1993.
Retrieving collocations fromtext: Xtract.
Computational Linguistics,19:143?177.M.
Spitzer.
1999.
The mind within the net:models of learning, thinking and acting.
MITPress.M.
Stevenson.
2002.
Augmenting noun tax-onomies by combining lexical similarity met-rics.
In Proc.
of the 19th International Con-ference on Computational Linguistics (COL-ING 2002), pages 953?959.M.
Wettler and R. Rapp.
1993.
Computation ofword associations based on the co-occurrencesof words in large corpora.
In Proc.
of the 1stWorkshop on Very Large Corpora: Academicand Industrial Perspectives.M.
Zock and J.-P. Fournier.
2001.
How cancomputers help the writer/speaker experienc-ing the tip-of-the-tongue problem ?
In Proc.of RANLP, pages 300?302.M.
Zock.
2002.
Sorry, what was your nameagain, or how to overcome the tip-of-thetongue problem with the help of a com-puter?
In Proc.
of the SemaNet workshopCOLING2002.
