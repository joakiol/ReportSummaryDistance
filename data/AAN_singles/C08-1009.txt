Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 65?72Manchester, August 2008Good Neighbors Make Good Senses:Exploiting Distributional Similarity for Unsupervised WSDSamuel BrodySchool of InformaticsUniversity of Edinburghs.brody@sms.ed.ac.ukMirella LapataSchool of InformaticsUniversity of Edinburghmlap@inf.ed.ac.ukAbstractWe present an automatic method for sense-labeling of text in an unsupervised manner.The method makes use of distributionallysimilar words to derive an automaticallylabeled training set, which is then used totrain a standard supervised classifier fordistinguishing word senses.
Experimentalresults on the Senseval-2 and Senseval-3datasets show that our approach yields sig-nificant improvements over state-of-the-artunsupervised methods, and is competitivewith supervised ones, while eliminatingthe annotation cost.1 IntroductionWord sense disambiguation (WSD), the task ofidentifying the intended meaning (sense) of wordsin context, is a long-standing problem in NaturalLanguage Processing.
Sense disambiguation is of-ten characterized as an intermediate task, which isnot an end in itself, but has the potential to improvemany applications.
Examples include summariza-tion (Barzilay and Elhadad, 1997), question an-swering (Ramakrishnan et al, 2003) and machinetranslation (Chan and Ng, 2007).WSD is commonly treated as a supervised clas-sification task.
Assuming we have access to datathat has been hand-labeled with correct wordsenses, we can train a classifier to assign sensesto unseen words in context.
While this approachoften achieves high accuracy, adequately largesense labeled data sets are unfortunately difficultto obtain.
For many words, domains, languages,and sense inventories they are unavailable, andc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.in most cases it is unreasonable to expect to ac-quire them.
Ng (1997) estimates that a high accu-racy domain-independent system for WSD wouldprobably need a corpus of about 3.2 million sensetagged words.
At a throughput of one word perminute (Edmonds, 2000), this would require about27 person-years of human annotation effort.SemCor (Fellbaum, 1998) is one of the few cor-pora that have been manually annotated for allwords ?
it contains sense labels for 23,346 lem-mas.
In spite of being widely used, SemCor con-tains too few tagged instances for the majority ofpolysemous words (typically fewer than 10 each).Supervised methods require much larger data setsthan this to perform adequately.The problem of obtaining sufficient labeleddata, often referred to as the data acquisition bot-tleneck, creates a significant barrier to the use ofsupervised WSD methods in real world applica-tions.
In this work we wish to take advantage of thehigh accuracy and strong capabilities of supervisedmethods, while eliminating the need for human an-notation of training data.
Our approach exploits asense inventory such asWordNet (Fellbaum, 1998)and corpus data to automatically create a collec-tion of sense labeled instances which can subse-quently serve to train any supervised classifier.
Thekey premise of our work is that a word?s sensescan be broadly described by semantically relatedwords.
So, rather than laboriously annotating allinstances of a polysemous word with its senses, wecollect instances of its related words and treat themas sense labels for the target word.
The methodis inexpensive, language-independent, and can beused to create large sense-labeled data withouthuman intervention.
Our results demonstrate sig-nificant improvements over state-of-the-art unsu-pervised methods that do not make use of hand-labeled annotations.In the following section we provide an overview65of existing work on unsupervised WSD.
Section 3introduces our method for automatically creat-ing sense annotations.
We present our evaluationframework in Section 4 and results in Section 5.2 Related WorkThe data requirements for supervisedWSD and thecurrent paucity of suitably annotated corpora formany languages and text genres, has sparked con-siderable interest in unsupervised methods.
Thesetypically come in two flavors: (1) developing al-gorithms that assign word senses without relyingon a sense-labeled corpus (Lesk, 1986; Galley andMcKeown, 2003) and (2) making use of pseudo-labels, i.e., labelled data that has not been specifi-cally annotated for sense disambiguation purposesbut contains some form of sense distinctions (Galeet al, 1992; Leacock et al, 1998).
We briefly dis-cuss representative examples of both approaches,with a bias to those closely related to our ownwork.Unsupervised Algorithms One of the first ap-proaches to unsupervised WSD, and the founda-tion of many algorithms to come, was originallyintroduced by Lesk (1986).
The method assigns asense to a target ambiguous word by comparing thedictionary definitions of each of its senses with thewords in the surrounding context.
The sense whosedefinition has the highest overlap (i.e., words incommon) with the context is assumed to be thecorrect one.
Despite its simplicity, the algorithmprovides a good baseline for comparison.
Cover-age can be increased by augmenting the dictionarydefinition (gloss) of each sense with the glosses ofrelated words and senses (Banerjee and Pedersen,2003).Although most algorithms disambiguate wordsenses in context, McCarthy et al (2004) proposea method that does not rely on contextual cues.Their algorithm capitalizes on the fact that the dis-tribution of word senses is highly skewed.
A largenumber of frequent words is often associated withone dominant sense.
Indeed, current supervisedmethods rarely outperform the simple heuristic ofchoosing the most common sense in the trainingdata (henceforth ?the first sense heuristic?
), despitetaking local context into account.
Rather than ob-taining the first sense via annotating word sensesmanually, McCarthy et al propose to acquire firstsenses automatically and use them for disambigua-tion.
Thus, by design, their algorithm assigns thesame sense to all instances of a polysemous word.Their approach is based on the observation thatdistributionally similar neighbors often providecues about a word?s senses.
Assuming that a setof neighbors is available, the algorithm quantifiesthe degree of similarity between the neighbors andthe sense descriptions of the polysemous word.The sense with the highest overall similarity is thefirst sense.
Specifically, the approach makes use oftwo similarity measures which complement eachother and provide a large amount of data regardingthe word senses.
Distributional similarity indicatesthe similarity between words in the distributionalfeature space, whereas WordNet similarity in the?semantic?
space, is used to discover which senseof the ambiguous word is used in the corpus, andcausing the distributional similarity.Pseudo-labels as Training Instances Gale et al(1992) pioneered the use of parallel corpora as asource of sense-tagged data.
Their key insight isthat different translations of an ambiguous wordcan serve to distinguish its senses.
Ng et al (2003)extend this approach further and demonstrate thatit is feasible for large scale WSD.
They gatherexamples from English-Chinese parallel corporaand use automatic word alignment as a meansof obtaining a translation dictionary.
Translationsare next assigned to senses of English ambiguouswords.
English instances corresponding to thesetranslations serve as training data.It has become common to use related wordsfrom a dictionary to learn contextual cues for WSD(Mihalcea, 2002).
Perhaps the first incarnation ofthis idea is found in Leacock et al (1998), whodescribe a system for acquiring topical contextsthat can be used to distinguish between senses.For each sense, related monosemous words are ex-tracted from WordNet using the various relation-ship connections between sense entries (e.g., hy-ponymy, hypernymy).
Their system then queriesthe Web with these related words.
The contextssurrounding the relatives of a specific sense arepresumed to be indicators of that sense, and usedfor disambiguation.
A similar idea, proposed byYarowsky (1992), is to use a thesaurus and acquireinformative contexts from words in the same cate-gory as the target.Our own work uses insights gained from unsu-pervised methods with the aim of creating largedatasets of sense-labeled instances without explicitmanual coding.
Unlike Ng et al (2003) our algo-rithm works on monolingual corpora, which are66much more abundant than parallel ones, and isfully automatic.
In their approach translations andtheir English senses must be associated manually.Similarly to McCarthy et al (2004), we assumethat words related to the target word are useful in-dicators of its senses.
Importantly, our method dis-ambiguates words in context and is able to assignadditional senses, besides the first one.3 MethodAs discussed earlier, our aim is to alleviate theneed for manual annotation by creating a largedataset labeled with word senses without humanintervention.
This dataset can be subsequentlyused by any supervised machine learning algo-rithm.
We assume here that we have access to acorpus and a sense inventory.
We first obtain a listof words that are semantically related to our tar-get word.
In the remainder of this paper we use theterm ?neighbors?
to refer to these words.
Next, weseparate the neighbors into sense-specific groups.Finally, we replace the occurrences of each neigh-bor in our corpus with an instance of the targetword, labeled with the matching sense for thatneighbor.
The procedure has two important steps:(1) acquiring neighbors and (2) associating themwith appropriate senses.
We describe our imple-mentation of each stage in more detail below.Neighbor Acquisition Considerable latitude isallowed in specifying appropriate neighbors for thetarget word.
Broadly speaking, the neighbors canbe extracted from a corpus or from a semantic re-source, for example the dictionary providing thesense inventory.
A wealth of algorithms have beenproposed in the literature for acquiring distribu-tional neighbors from a corpus (see Weeds (2003)for an overview).
They differ as to which featuresthey consider and how they use the distributionalstatistics to calculate similarity.Lin?s (1998) information-theoretic similaritymeasure is commonly used in lexicon acquisitiontasks and has demonstrated good performance inunsupervised WSD (McCarthy et al, 2004).
It op-erates over dependency relations.
A word w is de-scribed by a set T (w) of co-occurrence triplets< w,r,w?>, which can be viewed as a sparselyrepresented feature vector, where r representsthe type of relation (e.g., object-of , subject-of ,modified-by) between w and its dependent w?.
Thesimilarity between w1and w2is then defined as:?
(r,w)?T (w1)?T (w2)I(w1,r,w)+ I(w2,r,w)?
(r,w)?T (w1)I(w1,r,w)+?
(r,w)?T (w2)I(w2,r,w)where I(w,r,w?)
is the information value of w withregard to (r,w?
), defined as:I(w,r,w?)
= logcount(w,r,w?)
?
count(r)count(?,r,w?)
?
count(w,r,?
)The measure is used to estimate the pairwise simi-larity between the target word and all other wordsin the corpus (with the same part of speech); thek words most similar to the target are selected asits neighbors.A potential caveat with Lin?s (1998) distribu-tional similarity measure is its reliance on syn-tactic information for obtaining dependency rela-tions.
Parsing resources may not be available forall languages or domains.
An alternative is to use ameasure of distributional similarity which consid-ers word collocation statistics and therefore doesnot require a syntactic parser (see Weeds (2003)).As mentioned earlier, it is also possible to ob-tain neighbors simply by consulting a semanticdictionary.
In WordNet, for example, we can as-sume that WordNet relations, (e.g., hypernymy,hyponymy, synonymy) indicate words which aresemantic neighbors.
An advantage of using dis-tributional neighbors is that they reflect the char-acteristics of the corpus we wish to disambiguateand are potentially better suited for capturing sensedifferences across genres and domains, whereasdictionary-based neighbors are corpus-invariant.Associating Neighbors with Senses If theneighbors are extracted from WordNet, it is notnecessary to associate them with their senses asthey are already assigned a specific sense.
Distri-butional similarity methods, however, do not pro-vide a way to distinguish which neighbors per-tain to each sense of the target.
For that purpose,we adapt a method proposed by McCarthy et al(2004).
Specifically, for each acquired neighbor,we choose the sense of the target which givesthe highest semantic similarity score to any senseof the neighbor.
There are a large number of se-mantic similarity measures to choose from (seeBudanitsky and Hirst (2001) for an overview).We use Lesk?s measure as modified by Banerjeeand Pedersen (2003) for two reasons.
First, it has67been shown to perform well in the related taskof predominant sense detection (McCarthy et al,2004).
Second, it has the advantage of relying onlyupon the sense definitions, rather than the complexgraph structure which is unique to WordNet.
Thismakes the method more suitable for use with othersense inventories.Note that unlike McCarthy et al (2004), weare associating neighbors with senses, rather thanmerely trying to detect the predominant sense, andtherefore we require more precision in our selec-tion.
When it is unclear which sense of the targetword is most similar to a given neighbor (when thescores of two or more senses are close together),that neighbor is discarded.As an example, consider the word sense, whichhas four meanings1in WordNet: (1) a general con-scious awareness (e.g., a sense of security), (2) themeaning of a word (e.g., the dictionary gave sev-eral senses for the word), (3) sound practical judg-ment (e.g., I can?t see the sense in doing it now),and (4) a natural appreciation or ability (e.g., keenmusical sense).
On the British National Corpus(BNC), using Lin?s (1998) similarity method, weretrieve the following neighbors for the first andsecond sense, respectively:1. awareness, feeling, instinct, enthusiasm, sen-sation, vision, tradition, consciousness, anger,panic, loyalty2.
emotion, belief, meaning, manner, necessity,tension, motivationNo neighbors are associated with the last twosenses, indicating that they are not prevalentenough in the BNC to be detected by this method.Once sense-specific neighbors are acquired, thenext stage is to replace all instances of the neigh-bors in the corpus with the target ambiguous wordlabeled with the appropriate sense.
For example,when encountering the sentence ?...
attempt tostate the meaning of a word?, our method wouldautomatically transform this to ?...
attempt to statethe sense (s#2) of a word.?
These pseudo-labeledinstances comprise the training instances we pro-vide to our machine learning algorithms.4 Experimental SetupWe evaluated the performance of our approach onbenchmark datasets.
In this section we give details1We are using the coarse-grained representation accordingto Senseval 2 annotators.
The sense definitions are simplifiedfor the sake of brevity.regarding our training and test data, and describethe features and machine learners we employed.Finally, we discuss the methods to which we com-pare our approach.4.1 DataOur experiments use a subset of the data providedfor the English lexical sample task in the Sen-seval 2 (Preiss and Yarowsky, 2001) and Sense-val 3 (Mihalcea and Edmonds, 2004) evaluationexercises.
Since our method does not require handtagged training data, we merged the provided train-ing and test data into a single test set.As a proof of concept we focus on the disam-biguation of nouns, since they constitute the largestportion of content words (50% in the BNC).
In ad-dition, WordNet, which is our semantic resourceand point of comparison, has a wide coverageof nouns.
Also, for many tasks and applications(e.g., web queries) nouns are the most frequentlyencountered part-of-speech (Jansen et al, 2000).We made use of the coarse-grained sense group-ings provided for both Senseval datasets.
For manyapplications (e.g., information retrieval) coarselydefined senses are more useful (see Snow et al(2007) for discussion).Our training data was created from the BNC us-ing different ways of obtaining the neighbors of thetarget word.
As described in Section 3 we retrievedneighbors using Lin?s (1998) similarity measureon a RASP parsed (Briscoe and Carroll, 2002) ver-sion of the BNC.
We used subject and object de-pendencies, as well as adjective and noun modifierdependencies.
We also created training data setsusing collocational neighbors.
Specifically, usingthe InfoMap toolkit2, we constructed vector-basedrepresentations for individual words from the BNCusing a term-document matrix and the cosine sim-ilarity measure.
Vectors were initially constructedwith 1,000 dimensions, the most frequent con-tent words.
The space was reduced to 100 dimen-sions with singular value decomposition.
Finally,we also extracted neighbors from WordNet usingfirst-order and sibling relations (i.e., hyponyms ofthe same hypernym).
A problem often encounteredwhen using dictionary-based neighbors is that theyare themselves polysemous, and the related senseis often not the most prominent one in the corpus,which leads to noisy data.
We therefore experi-mented with using all neighbors for a given word2http://infomap.stanford.edu/68?The philosophical explanation of authority is not anattempt to state the sense of a word.
?Contextual features?10 words explanation, of, authority, be, ...?5 words an, attempt, to, state, of, a, ...Collocational features-2/+0 n-gram state the X-1/+1 n-gram the X of-0/+2 n-gram X of a-2/+0 POS n-gram Verb Det X-1/+1 POS n-gram Det X Prep-0/+2 POS n-gram X Prep DetSyntactic featuresObject of Verb obj of stateTable 1: Example sentence and extracted featuresfor the word sense; X denotes the target word.or only those which are monosemous and hope-fully less noisy.
In all cases we used 50 neighbors,the most similar nouns to the target.4.2 FeaturesWe used a rich feature space based on lemmas,part-of-speech (POS) tags and a variety of posi-tional and syntactic relationships of the target wordcapturing both immediate local context and widercontext.
These feature types have been widely usedin WSD algorithms (see Lee and Ng (2002) for anevaluation of their effectiveness).
Their use is illus-trated on a sample English sentence for the targetword sense in Table 1.4.3 Supervised ClassifiersOne of our evaluation goals was to examine theeffect of our training-data creation procedure ondifferent types of classifiers and determine whichones are most suited for use with our method.
Wetherefore chose three supervised classifiers (sup-port vector machines, maximum entropy, and labelpropagation) which are based on different learn-ing paradigms and have shown competitive per-formance in WSD (Niu et al, 2005; Preiss andYarowsky, 2001; Mihalcea and Edmonds, 2004).We summarize below their main characteristicsand differences.Support Vector Machines SVMs model clas-sification as the problem of finding a separatinghyperplane in a high dimensional vector space.They focus on differentiating between the mostproblematic cases ?
instances which are close toeach other in the high dimensional space, but havedifferent labels.
They are discriminative, ratherthan generative, and do not explicitly model theclasses.
SVMs have been applied successfully inmany NLP tasks.
We used the multi-class bound-constrained support vector classification (SVC)version of SVM described in Hsu and Lin (2001)and implemented in the BSVM package3.
All pa-rameters were set to their default values with theexception of the misclassification penalty, whichwas set to a high value (1,000) to penalize labelingall instances with the most frequent sense.Maximum Entropy Model Maximum entropy-based classifiers are a common alternative to otherprobabilistic classifiers, such as Naive Bayes, andhave received much interest in various NLP tasksranging from part-of-speech tagging to parsingand text classification.
They represent a probabilis-tic, global constrained approach.
They assume auniform, zero-knowledge model, under the con-straints of the training dataset.
The classifier findsthe (unique) maximal entropy model which con-forms to the expected feature distribution of thetraining data.
In our experiments, we usedMegam4a publicly available maximum entropy classifier(Daum?e III, 2004) with the default parameters.Label Propagation The basic Label Propaga-tion algorithm (Zhu and Ghahramani, 2002) repre-sents labeled and unlabeled instances as nodes inan undirected graph with weighted edges.
Initiallyonly the known data nodes are labeled.
The goalis to propagate labels from labeled to unlabeledpoints along the weighted edges.
The weights arebased on distance in a high-dimensional space.
Ateach iteration, only the original labels are fixed,whereas the propagated labels are ?soft?, and maychange in subsequent iterations.
This property al-lows the final labeling to be affected by more dis-tant labels, that have propagated further, and givesthe algorithm a global aspect.
We used SemiL5, apublicly available implementation of label propa-gation (all parameters were set to default values).4.4 Comparison with State-of-the-artAs an upper bound, we considered the accuracyof our classifiers when trained on the manually-labeled Senseval data (using the same experimen-tal settings and 5-fold crossvalidation).
This can beused to estimate the expected decrease in accuracycaused solely by the use of our automatic sense la-beling method.
We also compared our approach toother unsupervised ones.
These include McCarthy3http://www.csie.ntu.edu.tw/?cjlin/bsvm/4http://www.isi.edu/?hdaume/megam/index.html5http://www.engineers.auckland.ac.nz/?vkec00169et al?s (2004) method for inferring the predomi-nant sense and Lesk?s (1986) algorithm.
We modi-fied the latter slightly so as to increase its coverageand used McCarthy et al?s first sense heuristic todisambiguate unknown instances where no overlapwas found.
For McCarthy et al we used parame-ters they report as optimal.5 ResultsThe evaluation of our method was motivated bythree questions: (1) How do different choices inconstructing the pseudo-labeled training data af-fect WSD performance?
Here, we would like toassess whether the origin of the target word neigh-bors (e.g., from a corpus or dictionary) matters.
(2) What is the degree of noise and subsequentloss in accuracy incurred by our method?
(3) Howdoes the proposed approach compare against otherunsupervised methods?
In particular, we are in-terested to find out whether we outperform Mc-Carthy et al?s (2004) related method for predomi-nant sense detection.5.1 The Choice of NeighborsOur results are summarized in Table 2.
We re-port accuracy (rather than F-score) since all al-gorithms labeled all instances.
The three centercolumns present our results with the automaticallyconstructed training sets.The best accuracies are observed when the la-bels are created from distributionally similar wordsusing Lin?s (1998) dependency-based similaritymeasure (Depend).
We observe a small decrease inperformance (within the range of 2%?4%) whenusing collocational neighbors without any syntac-tic information.6Using the neighbors provided byWordNet leads to worse results than using dis-tributional neighbors.
The differences in perfor-mance are significant7(p < 0.01) on both Sense-val datasets for all classifiers and for bothWordNetconfigurations, i.e., using all neighbors (AllWN)vs. monosemous ones (MonoWN).This result may seem counterintuitive sinceneighbors provided by a semantic resource arebased on expert knowledge and are often more ac-curate than those obtained automatically.
However,semantic resources like WordNet are designed tobe as general as possible without a specific cor-pus or domain in mind.
They will therefore pro-vide related words for all senses, even rare ones,6We omit these results from the table for brevity.7Throughout, we report significance using a ?2test.which may not appear in our chosen corpus.
Distri-butional methods, on the other hand, are anchoredin the corpus.
The extracted neighbors are usu-ally relevant and representative of the corpus.
An-other drawback of resource-based neighbors is thatthey often do not share local behavior, i.e., theydo not appear in the same immediate local con-text and do not share the same syntax.
For this rea-son, the useful information that can be extractedfrom their contexts tends to be topical (e.g., wordsthat are indicative of the domain), rather than lo-cal (e.g., grammatical dependencies).
Topical in-formation is mostly useful when the difference be-tween senses can be attributed to a specific domain.However, for many words and senses, this is notthe case (Leacock et al, 1998).5.2 Comparison against Manual LabelsThe rightmost column of Table 2 shows the accu-racy of our classifiers when these are trained onthe manually annotated Senseval datasets.
In gen-eral, all algorithms exhibit a similar level of per-formance when trained on hand-coded data, withslightly lower scores for Senseval 3.
On Sense-val 2, the SVM is significantly better than the othertwo classifiers (p < 0.01).
On Senseval 3, labelpropagation is significantly worse than the others(p < 0.01).
The results shown here do not repre-sent the highest achievable performance in a su-pervised setting, but rather those obtained with-out extensive parameter tuning.
The best perform-ing systems on coarse-grained nouns in Sense-val 2 and 3 (Preiss and Yarowsky, 2001; Mihalceaand Edmonds, 2004) achieved approximately 76%and 80%, respectively.
Besides being more finelytuned, these systems employed more sophisticatedlearning paradigms (e.g., ensemble learning).When we compare the results from the manu-ally labeled data to those achieved with the dis-tributional neighbors, we can see that use of ourpseudo-labels results in accuracies that are ap-proximately 8?10% lower.
Since the results wereachieved using the same feature set and classi-fier settings, the comparison provides an estimateof the expected decrease in accuracy due only toour unsupervised tagging method.
With more de-tailed feature engineering and more sophisticatedmachine learning methods, we could probably im-prove our classifiers?
performance on the automat-ically labeled dataset.
Also note that improvementsin supervised methods can be expected to automat-ically translate to improvements in unsupervised70Senseval 2 AllWN MonoWN Depend ManualSVM 48.12 53.29 64.38 72.52MaxEnt 40.93 52.11 62.32 71.91LP 42.67 49.54 63.32 69.28McCarthy 59.98Lesk 48.12Senseval 3 AllWN MonoWN Depend ManualSVM 53.16 46.32 57.47 71.22MaxEnt 49.67 44.85 57.35 71.75LP 47.41 43.60 60.60 67.57McCarthy 57.14Lesk 48.66Table 2: Accuracy (%) on Senseval 2 and 3 lexicalsamples.
Support vector machines (SVM), maxi-mum entropy (MaxEnt) and label propagation (LP)are trained on automatically and manually labeleddata setsWSD using our method.Interestingly, label propagation performed rela-tively poorly on the manually labeled data.
How-ever, it ranks highly when using the automatic la-bels.
This may be due to the fact that LP is theonly algorithm that does not separate the train-ing and test set (it is principally a semi-supervisedmethod), allowing the properties of both to influ-ence the structure of the resulting graph.
Since theinstances in the training data are not actual occur-rences of the target word, it is important to learnwhich instances in the training set are closest to agiven instance in the test set.
The two other algo-rithms only attempt to distinguish between classesin the training set.5.3 Other Unsupervised MethodsAs shown in Table 2 our classifiers are signifi-cantly better than Lesk on both Senseval datasets(p < 0.01).
They also significantly outperform theautomatically acquired predominant sense (Mc-Carthy) on Senseval 2 (for the Maximum Entropyclassifier, the difference is significant at p < 0.05).On Senseval 3, all classifiers quantitatively outper-form the first sense heuristic, but the difference isstatistically significant only for label propagation(p < 0.01).
The differences in performance on thetwo datasets can be explained by analyzing theirsense distributions.
Senseval 3 has a higher levelof ambiguity (4.35 senses per word, on average,compared to 3.28 for Senseval 2), and is there-fore a more difficult dataset.
Although Senseval 3has a slightly lower percentage of first sense in-stances, the higher ambiguity means that the skewis, in fact, much higher than in Senseval 2.
A highSenseval 2 Depend ManualSVM 14.3 (60.1) 16.9 (60.4)MaxEnt 6.3 (66.9) 17.1 (56.7)LP 8.9 (63.3) 14.8 (49.4)Senseval 3 Depend ManualSVM 17.6 (45.0) 23.3 (60.0)MaxEnt 8.5 (55.0) 23.7 (60.9)LP 5.6 (60.9) 17.8 (53.5)Table 3: Percentage of non-first instances in auto-matically and manually labeled training data; num-bers in parentheses show the classifiers?
accuracyon these instances.skew towards the predominant sense means thereare less instances from which we can learn aboutthe rarer senses, and that we run a higher risk whenlabeling an instance as one of the rarer senses (in-stead of defaulting to the predominant one).Our method shares some of the principles ofMcCarthy et al?s (2004) unsupervised algorithm.However, instead of focusing on detecting a sin-gle predominant sense throughout the corpus, webuild a dataset that will allow us to learn about andidentify all existing (prevalent) senses.
Despite thefact that the first-sense heuristic is a strong base-line, and fall-back option in case of limited localinformation, it is not a true context-specific WSDalgorithm.
Any approach that ignores local con-text, and labels all instances with a single sensehas limited effectiveness when WSD is neededin an application.
Context-indifferent methods runthe risk of completely mistaking the predominantsense, thereby mis-labeling most of the instances,whereas approaches that consider local context areless prone to such large-scope errors.We further analyzed the performance of ourmethod by examining instances labeled withsenses other than the most frequent one.
Table 3shows the percentage of such instances depend-ing on the machine learner and type of trainingdata (automatic versus manual) being employed.It also presents the classifiers?
accuracy (figuresin parentheses) with regard to only the non-firstsenses.
When trained on the automatically labeleddata, our classifiers tend to be more conservativein assigning non-first senses.
Interestingly, we ob-tain similar accuracies with the classifiers trainedon the manually labeled data, even though the lat-ter assign more non-first senses.
It is worth notingthat the SVM labels two to three times as manyinstances with non-first-sense labels, yet achievessimilar levels of overall accuracy to the other clas-71sifiers (compare Tables 2 and 3) and only slightlylower accuracy on the non-first senses.
This wouldmake it a better choice when it is important to havemore data on rarer senses.6 Conclusions and Future WorkWe have presented an unsupervised approach toWSD which retains many of the advantages of su-pervised methods, while being free of the costlyrequirement for human annotation.
We demon-strate that classifiers trained using our method canout-perform state-of-the-art unsupervised meth-ods, and approach the accuracy of fully-supervisedmethods trained on manually-labeled data.In the future we plan to scale our system tothe all-words task.
There is nothing inherent inour method that restricts us to the lexical sample,which we chose primarily to assess the feasibil-ity of our ideas.
Another interesting direction con-cerns the use of our method in a semi-supervisedsetting.
For example, we could automatically ac-quire labeled instances for words whose senses arerare in a manually tagged dataset.
Finally, we couldpotentially improve accuracy, at the expense ofcoverage, by estimating confidence scores on theclassifiers?
predictions, and assigning labels onlyto instances with high confidence.AcknowledgmentsThe authors acknowledge the support of EPSRC(grant EP/C538447/1) and would like to thankDavid Talbot for his insightful suggestions.ReferencesS.
Banerjee, T. Pedersen.
2003.
Extended gloss overlaps asa measure of semantic relatedness.
In Proc.
of the 18thIJCAI, 805?810, Acapulco.R.
Barzilay, M. Elhadad.
1997.
Using lexical chains for textsummarization.
In Proc.
of the Intelligent Scalable TextSummarization Workshop, Madrid, Spain.T.
Briscoe, J. Carroll.
2002.
Robust accurate statistical an-notation of general text.
In Proc.
of the 3rd LREC, 1499?1504, Las Palmas, Gran Canaria.A.
Budanitsky, G. Hirst.
2001.
Semantic distance in Word-Net: An experimental, application-oriented evaluation offive measures.
In Proc.
of the ACL Worskhop on WordNetand Other Lexical Resources, Pittsburgh, PA.Y.
S. Chan, H. T. Ng.
2007.
Word sense disambiguationimproves statistical machine translation.
In Proc.
of the45th ACL, 33?40, Prague, Czech Republic.H.
Daum?e III.
2004.
Notes on CG and LM-BFGS optimiza-tion of logistic regression.P.
Edmonds.
2000.
Designing a task for SENSEVAL-2, 2000.Technical note.C.
Fellbaum, ed.
1998.
WordNet: An Electronic Database.MIT Press, Cambridge, MA.W.
Gale, K. Church, D. Yarowsky.
1992.
A method for dis-ambiguating word senses in a large corpus.
Computersand the Humanities, 26(2):415?439.M.
Galley, K. McKeown.
2003.
Improving word sense dis-ambiguation in lexical chaining.
In Proc.
of the 18th IJ-CAI, 1486?1488, Acapulco.C.
Hsu, C. Lin.
2001.
A comparison of methods for multi-class support vector machines, 2001.
Technical report, De-partment of Computer Science and Information Engineer-ing, National Taiwan University, Taipei, Taiwan.B.
J. Jansen, A. Spink, A. Pfaff.
2000.
Linguistic aspectsof web queries, 2000.
American Society of InformationScience, Chicago.C.
Leacock, M. Chodorow, G. A. Miller.
1998.
Using cor-pus statistics and wordnet relations for sense identification.Computational Linguistics, 24(1):147?165.Y.
K. Lee, H. T. Ng.
2002.
An empirical evaluation of knowl-edge sources and learning algorithms for word sense dis-ambiguation.
In Proc.
of the EMNLP, 41?48, NJ.M.
Lesk.
1986.
Automatic sense disambiguation using ma-chine readable dictionaries: How to tell a pine cone froman ice cream cone.
In Proc.
of the 5th SIGDOC, 24?26,New York, NY.D.
Lin.
1998.
Automatic retrieval and clustering of similarwords.
In Proc.
of the ACL/COLING, 768?774, Montreal.D.
McCarthy, R. Koeling, J. Weeds, J. Carroll.
2004.
Findingpredominant senses in untagged text.
In Proc.
of the 42thACL, 280?287, Barcelona, Spain.R.
F. Mihalcea, P. Edmonds, eds.
2004.
Proc.
of theSENSEVAL-3, Barcelona, 2004.R.
F. Mihalcea.
2002.
Word sense disambiguation withpattern learning and automatic feature selection.
NaturalLanguage Engineering, 8(4):343?358.H.
T. Ng, B. Wang, Y. S. Chan.
2003.
Exploiting paralleltexts for word sense disambiguation: an empirical study.In Proc.
of the 41st ACL, 455?462, Sapporo, Japan.H.
T. Ng.
1997.
Getting serious about word sense disam-biguation.
In Proc.
of the ACL SIGLEX Workshop on Tag-ging Text with Lexical Semantics: Why, What, and How?,1?7, Washington, DC.Z.
Y. Niu, D. H. Ji, C. L. Tan.
2005.
Word sense disam-biguation using label propagation based semi-supervisedlearning.
In Proc.
of the 43rd ACL, 395?402, Ann Arbor.J.
Preiss, D. Yarowsky, eds.
2001.
Proc.
of the 2nd Interna-tional Workshop on Evaluating Word Sense Disambigua-tion Systems, Toulouse, France, 2001.G.
Ramakrishnan, A. Jadhav, A. Joshi, S. Chakrabarti,P.
Bhattacharyya.
2003.
Question answering via Bayesianinference on lexical relations.
In Proc.
of the ACL 2003workshop on Multilingual summarization and QA, 1?10.R.
Snow, S. Prakash, D. Jurafsky, A. Y. Ng.
2007.
Learningto merge word senses.
In Proc.
of the EMNLP/CoNLL,1005?1014, Prague, Czech Republic.J.
Weeds.
2003.
Measures and Applications of Lexical Dis-tributional Similarity.
Ph.D. thesis, University of Sussex.D.
Yarowsky.
1992.
Word-sense disambiguation using statis-tical models of Roget?s categories trained on large corpora.In Proc.
of the 14th COLING, 454?460, Nantes, France.X.
Zhu, Z. Ghahramani.
2002.
Learning from labeled andunlabeled data with label propagation.
Technical report,CMU-CALD-02, 2002.72
