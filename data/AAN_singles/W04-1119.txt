A Semi-Supervised Approach to Build Annotated Corpus for Chinese NamedEntity RecognitionXiaoshan FANG#, Jianfeng GAO*, Huanye SHENG#*Microsoft Research Asia, jfgao@microsoft.com#Shanghai Jiaotong University, China, {fang-xs, hysheng}@sjtu.edu.cnAbstract1This paper presents a semi-supervised ap-proach to reduce human effort in building anannotated Chinese corpus.
One of the disad-vantages of many statistical Chinese namedentity recognition systems is that training datamay be in short supply, and manually buildingannotated corpus is expensive.
In the proposedapproach, we construct an 80M hand-annotated corpus in three steps: (1)Automatically annotate training corpus; (2)Manually refine small subsets of the automati-cally annotated corpus; (3) Combine smallsubsets and whole corpus in a bootstrappingprocess.
Our approach is tested on a state-of-the-art Chinese word segmentation system(Gao et al, 2003, 2004).
Experiments showthat only a small subset of hand-annotatedcorpus is sufficient to achieve a satisfying per-formance of the named entity component inthis system.1 IntroductionThe success of applying statistical methods tonatural language processing tasks depends to alarge degree upon the quality and amount of avail-able training data.This paper presents our method of creating train-ing data for the statistical Chinese word segmenterproposed in Gao et al (2003).
The segmenter isbased on improved source-channel models, whichare trained on a large amount of annotated trainingdata.
Whereas the hand-annotation is a very expen-sive task, creating the training data automaticallyremains an open research problem.
Our approachfalls somewhere between the two extremes of thespectrum.
We try to minimize the human effortwhile keeping the quality of the annotation rea-sonably good for model estimation.
The method tobe presented has been discussed briefly in Gao etal.
(2003).
This paper presents an extended de-scription with more details and experimental re-sults.1 This work was done while the author was visitingMicrosoft Research Asia.The training data refer to a set of Chinese sen-tences where word boundaries and types have beenannotated.
Our basic solution is the bootstrappingapproach described in Gao et al (2002).
It consistsof three steps: (1) Initially, we use a greedy wordsegmenter to annotate the corpus, and obtain initialmodels based on the initial annotated corpus; (2)We re-annotate the corpus using the obtained mod-els; (3) Re-train the models using the re-annotatedcorpus.
Steps 2 and 3 are iterated until the per-formance of the system converges.In this approach, the quality of the resultingmodels depends to a large degree upon the qualityof the initial annotated corpus.
Because there aremany named entities that are not stored in a dic-tionary, traditional dictionary-based forwardmaximum matching (FMM) algorithm is not suffi-cient to create a good initial corpus.
We thusmanually annotate named entities on a small subset(call seed set) of the training data.
Then, we obtaina model on the seed set (called seed model).
Wethus improve the initial model which is trained onthe initial annotated training corpus by interpolat-ing it with the seed model.
Our experiments showthat a relatively small seed set (e.g., 10 millioncharacters, which takes approximately three weeksfor 4 persons to annotate the NE tags) is enough toget a good improved model for initialization.The remainder of this paper is organized as fol-lows: Section 2 summarizes the related work.
Sec-tion 3 deals with our approach to improve modelestimation for Chinese word segmentation.
Theexperiments are presented at Section 4.
Finally weconclude in Section 5.2 Related workTraditional statistical approaches use a paramet-ric model with maximum likelihood estimation(MLE), usually with smoothing methods to dealwith data sparseness problems.
These approacheshave been introduced for the task of Chinese wordsegmentation.
According to the training data used(word-segmented or not), the Chinese word seg-mentation can be achieved in a supervised or unsu-pervised manner.As an example of unsupervised training, Ge et al(1999) presents a simple zero-th order Markovmodel of the words in Chinese text.
They devel-oped an efficient algorithm to train their model onan unsegmented corpus.
Their basic assumption isthat Chinese words are usually 1 to 4 characterslong.
They however did not take into account alarge amount of named entities (e.g.
Chinese or-ganization name, transliterate name and some per-son names) most of which are longer than 4 char-acters (e.g., ???????
Microsoft ResearchAsia, ?????
California, ?????
awoman?s name which puts her husband?s surnameahead).An and Wong used Hidden Markov Models(HMM) for segmentation.
Their system is solelytrained on a corpus which has been manually anno-tated with word boundaries and Part-of-Speechtags.
Wu (2003) also used the training data to tunethe segmentation parameters of their MSR-NLPChinese system.
He used the annotated trainingdata to deal with the morphologically derivedwords.In this paper we present a semi-supervised train-ing method where we use both an auto-segmentedtraining corpus and a small hand-annotated subsetof it.
Comparing to unsupervised approaches, ourapproach leads to a better segmenter that canidentify much more named entities which are notin the dictionary.
Comparing to supervised ap-proaches, our method requires much less humaneffort for data annotation.The Chinese word segmenter used in this studyis described in Gao et al (2003).
The segmenterprovides a unified approach to word segmentationand named entity (NE) recognition.
This unifiedapproach is based on the improved source-channelmodels of Chinese sentence generation, with twocomponents: a source model and a set of channelmodels.
For each word class (e.g.
a person name),there is a channel model (referred to as class modelafterwards) that estimates the generative probabil-ity of a character string given the word type.
Thesource model is used to estimate the generativeprobability of a word sequence, in which eachword belongs to one word class (e.g.
a word in alexicon or a named entity).
In another word, it in-dicates, given a context, how likely a word occurs.So the source model is also referred to as contextmodel afterwards.
This paper focuses the discus-sion on how to create annotated corpus for contextmodel estimation.3 A semi-supervised approach to improvecontext model estimationIn this study the context model is a trigrammodel which estimates the probability of a wordclass.Ideally, given an annotated corpus, where eachsentence is segmented into words which are taggedby their word types, the trigram word class prob-abilities can be calculated using MLE, togetherwith a backoff schema (Katz, 1987) to deal withthe sparse data problem.
Unfortunately, buildingsuch annotated training corpora is very expensive.Our basic solution is the bootstrapping approachdescribed in Gao et al (2002).
It consists of threesteps: (1) Initially, a greedy word segmenter (i.e.FMM) is used to annotate the corpus, and an initialcontext model is obtained based on the initial an-notated corpus; (2) Re-annotate the corpus usingthe obtained models; (3) Re-train the contextmodel using the re-annotated corpus.
Steps 2 and 3are iterated until the performance of the systemconverges.In the above approach, the quality of the contextmodel depends to a large degree upon the qualityof the initial annotated corpus, which is howevernot satisfied due to the fact that many named enti-ties cannot be identifying using the greedy wordsegmenter which is based on the dictionary.
As aconsequence, the above approach achieves a lowaccuracy in detecting Chinese named entities.A straightforward solution to the above problemis to obtain large amount of high-quality annotatedcorpus for context model estimation.
Unfortunately,manually creating such annotated corpus  is veryexpensive.
For example, Douglas (1999) pointedout that at least up to about 1.2 million words oftraining data are necessary to train an HMM namerecognizer.
To guarantee a high degree of accuracy(e.g.
90% F-measure), it requires about 800 hours,or 20 person*weeks of labor to annotate and checkthe amount of data.
This is almost certainly moretime than would be required by a skilled rule writerto write a rule-based name recognizer achievingthe same level of performance, assuming all thenecessary resources, such as lexicons and namelists, are already available.Our training data contains approximately 80million Chinese characters from various domainsof text.
We are facing three questions in annotatingthe training data.
(1) How to generate a high qual-ity hand-annotated corpus?
(2) How to best use thevaluable hand-annotated corpus so as to achieve asatisfying performance?
(3) What is the optimalsize of the hand-annotated corpus, considering thetradeoff between the cost of human labor and theperformance of the resulting segmenter?We leave the answers to the first and third ques-tions to Section 4.
In what follows, we describe ourmethod of using small set of human-annotated cor-pus to boost the quality of the annotation of theentire corpus.
It consists of 6 steps.Step 1: Manually annotate named entities on asmall subset (call seed set) of the training data.Step 2: Obtain a context model on the seed set(called seed model).Step 3: Re-annotate the training corpus using theseed model and then train an improved contextmodel using the re-annotated corpus.Step 4: Manually annotate another small subsetof the training data.
Repeat Steps (2) and (3) untilthe entire training data have been annotated.Step 5: Repeat steps 1 to 4 using different seedsets (we used three seed sets in our experiments, aswe shall describe in Section 4).Step 6: Combine all context models obtained instep 5 via linear interpolation:P(xyz) =?
?i?
Pi(xyz) (1)Here Pi(xyz) is the trigram probability of the i-thcontext model.
?s is the interpolation weightswhich vary from 0 to 1.4 ExperimentsIn this section, we first present our experimentson the generation and evaluation of hand-annotatedcorpus to answer the first two questions.
Then, theanswer to the third question is given in subsection4.2.4.1 The generation and evaluation of hand-annotated corpus4.1.1 The generation of hand-annotated corpusFour students, whose major is Chinese language,annotate the corpus according to a pre-definedMSRA?s guideline of Chinese named entities.
Wefind that we have to revise the guideline when theywere annotating the corpus.
For example, Chinesecharacter string ?
?
?
?
?
?
(ShanghaiExposition)?can be tagged as either ?
[L ?]?????
or ?
[L ??]????.
Here ???
is the abbre-viation of ???(Shanghai)?.
???
is the abbrevia-tion of ???(city)?.
L is the tag of location name.It is not clearly described in the guideline wherethe named entity?s right boundary is.We obtain in total three manually annotated sub-sets (i.e.
seed sets) by the following process:1.
Annotate the training data using a greedyword segmenter.
Highlight the NEs and theirtags.2.
Randomly select 10 million characters fromthe annotated training data and then ask thestudents to manually refine these 10 millioncharacters.
The refinement includes correctingthe wrong NE tags and adding missing NEtags.3.
Repeat the second step, and then combine theobtained new 10-million-character subset withthe first one.
Hence, a 20-million-charactersubset of the training data is obtained.4.
Repeat the second step, and then combine theobtained new 10-million-character subset withthe 20-million-character subset.
Hence, a 30-million-character subset of the training data isobtained.A manually annotated test set was developed aswell.
The text corpus contains approximately a halfmillion Chinese characters that have been proof-read and balanced in terms of domain, styles, andtimes.4.1.2 The evaluation of hand-annotated corpusTo evaluate the quality of our annotated corpus,we trained a context model using the method de-scribed in Section 3, with the first-obtained 10-million-character seed set.
We then compare theperformance of the resulting segmenter with thoseof other state-of-the-art segmenters and the FMMsegmenter.4.1.2.1 Evaluation metricsWe conduct evaluations in terms of precision (P)and recall (R).NEsidentifiedofnumberNEsidentifiedcorrectlyofnumberP ??????
?=  (2)NEsallofnumberNEsidentifiedcorrectlyofnumberR ??????
?=  (3)4.1.2.2 Segmenters in Comparison1.
The MSWS system is one of the bestavailable products.
It is released by Micro-soft?
(as a set of Windows APIs).
MSWS firstconducts the word-breaking using MM (aug-mented by heuristic rules for disambiguation),and then conducts factoid detection and NERusing rules.2.
The LCWS system is one of the best re-search systems in mainland China.
It is re-leased by Beijing Language University.
Thesystem works similarly to MSWS, but has alarger dictionary containing more PNs andLNs.3.
The PBWS system is a rule-based Chineseparser which can also output the word seg-mentation results.
It explores high-level lin-guistic knowledge, such as syntactic structurefor Chinese word segmentation and NER.4.1.2.3 ResultsThe performance of the resulting segmenter iscompared with those of three state-of-the-art seg-menters and FMM segmenter in Table 1.
Here PN,LN and ON stand for person name, location nameand organization name respectively.
The first col-umn lists the segmenters.As can be seen from Table 1, the resulting seg-menter (SSSC.10m) achieves comparable resultswith those of the other three state-of-the-art wordsegmenters.
From Table 1 we also find that oursemi-supervised approach makes a 2.4%-49% im-provement over FMM.Segmenter PN LN ONR % P % R % P % R % P %MSWS 74.4 90.7 44.2 93.5 46.9 64.2LCWS 78.1 94.5 72.0 85.4 13.1 71.3PBWS 78.7 78.0 73.6 76.7 21.6 81.7FMM 65.7 84.4 82.7 76.0 56.6 38.6SSSC.10m 73.6 86.6 80.7 89.5 84.3 56.8Impr.
(%) 12.0 2.6 2.4 17.7 49.0 47.1Table 1: Results on different Chinese word seg-mentersThe results show a moderate amount of hand-annotated corpus leads our segmenter to a state-of-the-art performance.4.2 The optimal size of the hand-annotatedcorpusRegarding the third question: what is the optimalsize of the hand-annotated subset, considering thetradeoff between the cost of human labor and theperformance of the resulting segmenter?
We obtaina series of results using 10-30-million-charactersubsets as seed sets, and then plot three graphsshowing the relationship between the performancesand their corresponding human efforts ofconstructing the seed set.4.2.1 BaselinesWe use two baselines.One is the FMM method, which does not useannotated training data.
It is used to evaluate theperformances using 10-30-million-character sub-sets (see Figure 1).The other is to use all the human effort of anno-tating the whole training data, which takes about1920 person*hours human effort.
It is used to cal-culate how much labor we would save by using thesemi-supervised approach described in section 3.4.2.2 ResultsThe relationship between the performances andtheir corresponding human efforts of constructingthe seed sets is shown in Figure 1.
The X-axes givethe human efforts on building 10, 20, and 30-million-character subsets.
They are 360, 720, and1080 person*hours.
The Y-axes show the recalland precision results on person name, locationname and organization name, separately.607080900 360 720 1080PNRecall (%) Precision (%)30456075900 360 720 1080ON7080901000 360 720 1080Human effort (person*hour)LNFigure 1: The relationship between the perform-ances and their corresponding human effortsWe observe that both the recall and precision re-sults first go upwards, and level off after the use of720 person*hours, which is the corresponding hu-man effort of constructing 20 million characters.This means that 20 million characters is a satura-tion point, because more human effort does notlead to any improvement in performance, and lesshuman effort leads to lower performance.From the fact that manually annotating thewhole training data costs 1920 person*hours, weindicate that by using our semi-supervised ap-proach we save 62.5% human labor in corpus an-notation.5 ConclusionThis paper presents a semi-supervised method tosave human effort in building annotated corpus.This method uses a small set of human-annotatedcorpus to boost the quality of the annotation of theentire corpus.
We test this method on Gao?s Chi-nese word segmentation system, which achieves astate-of-the-art performance on SIGHAN backoffdata sets (Gao et al 2004).Several conclusions can be drawn from our ex-periments:z The obtained corpus is of high quality.z 20-million-characters is the optimal size ofhand-annotated subset to boost the 80-million-character training data, considering the trade-off between the cost of human labor and theperformance of the resulting segmenter.z We save 62.5% human labor in corpus anno-tation.ReferencesAn, Q. and Wong, W. S. 1996.
Automatic segmen-tation and tagging of Hanzi text using a hybridalgorithm.
In Proceedings of the 9th Interna-tional Conference on Indus-trial & EngineeringApplications of AI & Expert Systems.Borthwick, Andrew.
1999.
A Maximum EntropyApproach to Named Entity Recognition.
Ph.D.thesis, New York University.Douglas Appelt.
1999.
Introduction to InformationExtraction Technology.
A Tutorial Prepared forIJCAI-99.Gao, Jianfeng, Joshua Goodman, Mingjing Li andKai-Fu Lee.
2002.
Toward a unified approach tostatistical language modeling for Chinese.
ACMTALIP, 1(1): 3-33.Gao, Jianfeng, Mu Li and Changning Huang.
2003.Improved source-channel models for Chineseword segmentation.
In ACL-2003.
Sapporo, Ja-pan.Gao, Jianfeng, Andi Wu, Mu Li, Chang-NingHuang, Hongqiao Li, Haowei Qin and XinsongXia.
2004.
Adaptive Chinese Word Segmentation.In proceedings of ACL 2004.
Barcelona, Spain.Ge, X., Pratt, W. and Smyth, P. 1999.
DiscoveringChinese Words from Unsegmented Text.
SIGIR-99, pages 271-272.Hockenmaier, J. and Brew, C. 1998.
Error-drivenlearning of Chinese word segmentation.
In J.Guo, K. T. Lua, and J. Xu, editors, 12th PacificConference on Language and Information, pp.218?229, Singapore.
Chinese and Oriental Lan-guages Processing Society.Katz, S. M. 1987.
Estimation of probabilities fromsparse data for the language model componentof a speech recognizer.
IEEE Trans.
Acoustics,Speech Signal Process.
ASSP-35, 3 (March),400-401.Palmer, David.
1997.
A Trainable Rule-Based Al-gorithm for Word Segmentation.
In Proceedingsof the 35th Annual Meeting of the Associationfor Computational Linguistics (ACL ?97), Ma-drid.Sun, Jian, Jianfeng Gao, Lei Zhang, Ming Zhou,and Changning Huang.
2002.
Chinese named en-tity identification using class-based languagemodel.
In: COLING 2002.
Taipei, Taiwan.Wu, Andi.
2003.
Chinese Word Segmentation inMSR-NLP.
In Proceedings of the SecondSIGHAN Workshop on Chinese Language Proc-essing, Sapporo, Japan.
