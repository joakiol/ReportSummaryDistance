Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,pages 45?52, Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005Towards a Prototyping Tool for Behavior Oriented Authoring ofConversational Agents for Educational ApplicationsGahgene Gweon, Jaime Arguello, Carol Pai, Regan Carey, ZacharyZaiss, Carolyn Ros?Human-Computer Interaction Institute/ Language Technologies InstituteCarnegie Mellon University5000 Forbes Avenue, Pittsburgh, PA 15213 USAGgweon,jarguell,cpai,rcarey,zzaiss,cp3a@andrew.cmu.eduAbstractOur goal is to develop tools for facili-tating the authoring of conversationalagents for educational applications, andin particular to enable non-computational linguists to accomplishthis task efficiently.
Such a tool wouldbenefit both learning researchers, al-lowing them to study dialogue in newways, and educational technology re-searchers, allowing them to quicklybuild dialogue based help systems fortutoring systems.
We argue in favor ofa user-centered design methodology.We present our work-in-progress de-sign for authoring, which is motivatedby our previous tool development ex-periences and preliminary contextualinterviews and then refined throughuser testing and iterative design.1 IntroductionThis paper reports work in progress towardsdeveloping TuTalk, an authoring environmentdeveloped with the long term goal of enablingthe authoring of effective tutorial dialogueagents.
It was designed for developers withoutexpertise in knowledge representation, artificialintelligence, or computational linguistics.
In ourprevious work we have reported progress to-wards the development of authoring tools spe-cifically focusing on robust languageunderstanding capabilities (Ros?
et al, 2003;Ros?
& Hall, 2004; Ros?, et al, 2005).
In thispaper, we explore issues related to authoringboth at the dialogue and sentence level, as wellas the interaction between these two levels ofauthoring.
Some preliminary work on the un-derlying architecture is reported in (Jordan, Ro-s?, & VanLehn, 2001; Aleven & Ros?, 2004;Ros?
& Torrey, 2004).
In this paper we focuson the problem of making this computationallinguistics technology accessible to our targetuser population.We are developing the TuTalk authoring en-vironment in connection with a number of exist-ing local research projects related to educationaltechnology in general and tutorial dialogue inparticular.
It is being developed primarily foruse within the Pittsburgh Sciences of LearningCenter (PSLC) data shop, which includes devel-opment efforts for a suite of authoring tools tobe used for building the infrastructure for 7 dif-ferent computer enhanced courses designated asLearnLab courses.
These LearnLab courses,which are conducted within local secondaryschools as well as universities, and which in-clude Chinese, French, English as a SecondLanguage, Physics, Algebra, Geometry, andChemistry, involve heavy use of technologyboth for the purpose of supporting learning aswell as for the purpose of conducting learningresearch in a classroom setting.
Other local pro-jects related to calculus and thermodynamics45tutoring also have plans to use TuTalk.
In thispaper we will discuss specifically how we haveused corpora related to ESL, physics, thermody-namics, and calculus in our development effort.To support this multi-domain effort, it is es-sential that the technology we develop be do-main independent and usable by a non-technicaluser population, or at least a user population notpossessing expertise in knowledge representa-tion, artificial intelligence, or computational lin-guistics.
Thus, we are employing a corpus basedmethodology that bootstraps domain specificauthoring using examples of desired conversa-tional behavior for the domain.2 A Historical PerspectiveWhile a focus on design based on standardsand practices from human-computer interactioncommunity have not received a great deal ofattention in previously published tool develop-ment efforts known to the computational linguis-tics community, our experience tells us thatinsufficient attention to these details leads to thedevelopment of tools that are unusable, particu-larly to the user population that we target withour work.Some desiderata related to the design of oursystem are obvious based on our target userpopulation.
Currently, many educational tech-nology oriented research groups do not havecomputational linguists on their staff with theexpertise required to author domain specificknowledge sources for use with sophisticatedstate-of-the-art understanding systems, such asCARMEL (Ros?, 2000) or TRIPS (Allen et al,2001).
However, previous studies have shownthat, while scaffolding and guidance is requiredto support the authoring process, non-computational linguists possess many of the ba-sic skills required to author conversational inter-faces (Ros?, Pai, & Arguello, 2005).
Because themain barrier of entry to such sophisticated toolsare expertise in understanding the underlyingdata structures and linguistically motivated rep-resentation, our tools should have an interfacethat masks the unnecessary details and providesintuitive widgets that manipulate the data inways that are consistent with the mental modelsthe users bring with them to the authoring proc-ess.
In order to be maximally accessible to de-velopers of educational technology, the systemshould involve minimal programming.The design of Carmel-Tools (Ros?
et al,2003; Ros?
& Hall, 2004), the first generation ofour authoring tools, was based on these obviousdesiderata and not on any in-depth analysis ofdata collected from our target user population.While an evaluation of the underlying computa-tional linguistics technology showed promise(Ros?
& Hall, 2004), the results from actual au-thoring use were tremendously disappointing.A formal study reported in (Ros?, et al, 2005)demonstrates that even individuals with exper-tise in computational linguistics have difficultypredicting the coverage of knowledge sourcesthat would be generated automatically from ex-ample texts annotated with desired representa-tions.
Informal user studies involving actual useof Carmel-Tools then showed that a conse-quence of this lack of ability is that authors wereleft without a clear strategy for moving throughtheir corpus.
As a result, time was lost from an-notating examples that did not yield the maxi-mum amount of new knowledge in the generatedknowledge sources.
Furthermore, since authorstended not to test the generated knowledgesources as they were annotating examples, errorswere difficult for them to track later, despite fa-cilities designed to help them with that task.Another finding from our user studies wasthat although the interface prevented authorsfrom violating the constraints they designed intotheir predicate language, it did not keep authorsfrom annotating similar texts with very differentrepresentations, thus introducing a great deal ofspurious ambiguity.
Thus, they did not naturallymaintain consistency in their application of theirown designed meaning representation languagesacross example texts.
An additional problemwas that authors sometimes decomposed exam-ples in ways that lead to overly general rules,which then lead to incorrect analyses when theserules matched inappropriate examples.These disappointing results convinced us ofthe importance of taking a user-centered designapproach to our authoring interface redesignprocess.463 Preliminary Design Intents fromContextual InterviewsThe core essence of the user-centered designapproach is designing from data rather than frompreconceived notions of what will be useful andwhat will work well.
Expert blind spots oftenlead to designs based on intuitions that overlookneeds or overly emphasize issues that are notcentrally important (Koedinger & Nathan, 2004;Nathan & Koedinger, 2000).
Contextual inquiryis used at an early stage in the user-centered de-sign process to collect the foundational data onwhich to build a design (Beyer and Holtzbatt,2000).
Contextual Inquiry is a popular methoddeveloped within the Human Computer Interac-tion community where the design team gathersdata from end users while watching what theusers do in context of their work.
Contextualinterviews are used to illuminate these observa-tions by engaging end-users in interviews inwhich they show specific instances within theirwork life that are relevant for the design process.These methods help define requirements as wellas plan and prioritize important aspects of func-tionality.
At the same time, the system design-ers get a chance to gain insights about the users?environment, tasks, cultural influences and diffi-culties in the current processes.Many aspects of the Tutalk tool were de-signed based on contextual inquiry (CI) data.The design team conducted five CIs with userswho have experience in using existing authoringtools such as Carmel-Tools (Ros?
& Hall, 2004).The design team leader also spent one week ob-serving novice tool users working with the cur-rent set of tools at an Intelligent TutoringSummer School.
Here we will discuss somefindings from those CIs and observations andhow they motivated some general design intents,which we flesh out later in the paper.A common pattern we observed in our CIswas that having different floating windows fordifferent tasks fills up the computer screen rela-tively quickly and confuses authors as to wherethey are in the process of authoring.
The TuTalkdesign addresses this observed problem by an-choring the main window and switching only thecomponents of the window as needed.
A stan-dard logic for layout and view switching helpsauthors know what to expect in different con-texts.
Placement of buttons in TuTalk is consis-tently near the textboxes that they control, and abounding box is drawn around related sets ofcontrols so that the user does not get lost tryingto figure out where the buttons are or what theyare for.We observed that authors needed to refer tocheat sheets and user documentation to use theircurrent tools effectively and that different usersdid not employ the same terminology to refer tosimilar functionality, which made communica-tion difficult.
Furthermore, their current suitesof tools were not designed as one integrated en-vironment.
Thus, a lot of shuffling of files fromone directory to another was required in order tocomplete the authoring process.
Users withoutUnix operating system experience found thisespecially confusing.
Our goal is to require onlyvery minimal documentation that can be ob-tained on-line in the context of use.TuTalk is a single, integrated environmentthat makes use of GUI widgets for actions ratherthen requiring any text-based commands or filesystem activity.
In this way we hope to avoidrequiring the users to use a manual or a ?cheat-sheet?
reference for the commands they forget.As is common practice, TuTalk also uses consis-tent labels throughout the interface to promoteunderstandability and communication with tooldevelopers as well as other dialogue system de-velopers.4 Exploring the User?s Mental Modelthrough User StudiesAs an additional way of gaining insights intowhat sort of interface would make the process ofauthoring conversational interfaces accessible,we conducted a small, exploratory user study inwhich we examined how members of our targetuser population think about the structure of lan-guage.Two groups of college-level participants withno deep linguistics training were asked to readthree transcribed conversations about orderingfrom a menu at a restaurant from our English asa Second Language corpus.
The three specificrestaurant dialogues were chosen because oftheir breadth of topic coverage and richness inlinguistic expression.
Participants were asked toperform tasks with these dialogues to mimic47three levels of conversational interface author-ing:Macro Organization Tasks (dialogue level)Level 1.
How authors understand, seg-ment, and organize dialogue topicsLevel 2.
How authors generalize acrossdialogues as part of constructing a?model?
scriptMicro Organization Task (sentence level)Level 3.
How authors categorize anddecompose sentences within these dia-loguesThe first group (Group A, five participants)was asked to perform Macro Organization Tasksbefore processing sentences for the Micro Or-ganization Tasks.
The second group (Group B,four participants) was asked to perform thesesets of tasks in the opposite order.Our findings for the Macro OrganizationTasks showed that participants effectively brokedown dialogues into segments that reflected in-tuitive breaks in the conversation.
These topicswere then organized into semantically relatedcategories.
Although participants were not ex-plicitly instructed on how to organize the topics,every participant used spatial proximity as a rep-resentation for semantic relatedness.
Anotherfinding was the presence of primacy effects inthe ?model?
restaurant scripts they were asked toconstruct.
These scripts were heavily influencedby the first dialogue read.
As a result, importanttopics that surfaced in the other two dialogueswere omitted from the model scripts.Furthermore, we found that participants inGroup B took much longer in completing theMicro Organization Task (35-40 minutes as op-posed to 25-30 minutes) without performing theMacro Organization Tasks first.
In general, wefound that participants clustered sentences basedon surface characteristics rather than creatingontologically similar classes that would be moreuseful from a system development perspective.In a follow-up study we are exploring ways ofguiding users to cluster sentences in ways thatare more useful from a system building perspec-tive.Our preliminary findings show that getting anoverall sense of the corpus facilitates micro-level organization.
This is hindered by two fac-tors:  First, primacy effects interfere with macro-level comprehension.
Second, system developersstruggle to strategically select portions of theircorpus on which to focus their initial efforts.5 Stage One: Corpus OrganizationWhile existing tools from our previous workrequired authors to organize their corpus dataprior to their interaction with the tools, both ourcontextual research and user studies indicatedthat support for organizing corpus data prior toauthoring is important.In light of this concern, the TuTalk authoringprocess consists of three main stages.
Corpuscollection, corpus data organization throughwhat we call the InfoMagnet interface, and au-thoring propper.
First, a corpus is collected byasking users to engage in conversation usingeither a typed or spoken chat interface.
In thecase of spoken input, the speech is then tran-scribed into textual form.
Second, the raw cor-pus data is automatically preprocessed fordisplay and interactive organization using theInfoMagnet interface.
As part of the preprocess-ing, dialogue protocols are segmented automati-cally at topic boundaries, which can be adjustedby hand later during authoring propper.
Thetopic oriented segments are then clustered semi-automatically into topic based classes.
The out-put from this stage is an XML file where dia-logue segments are reassembled into theiroriginal dialogue contexts, with each utterancelabeled by topic.
This XML file is finally passedonto the authoring environment propper, whichis then used for finer grained processing, such asshifting topic segment boundaries and labelingmore detailed utterance functionality.Our design is for knowledge sources that arerunable from our dialogue system engine to begenerated directly from the knowledge base cre-ated during the fine-grained authoring process asin Carmel-Tools (Ros?
& Hall, 2004), howevercurrently our focus is on iterative developmentof a prototype of the authoring interaction de-sign.
Thus, more work is required to create thefinal end-to-end implementation.
In this sectionwe focus on the design of the corpus collectionand organization part of the authoring process.485.1 Corpus CollectionAn important part of our mission is developingtechnology that can use collected and automati-cally pre-processed corpus data to guide andstreamline the authoring process.
Prior to thearduous process of organizing and extractingmeaningful data, a corpus must be collected.As part of the PSLC and other local tutorialdialogue efforts we have collected corpus datafrom multiple domains that we have made use ofin our development process.
In particular, wehave been working with data collected in con-nection with the PSLC Physics and English as aSecond Language LearnLab courses as well aslocal Calculus and Thermodynamics tutoringprojects.
Currently we have physics tutoringdata primarily from one physics tutor (interac-tions with 40 students), thermodynamics datafrom four different tutors (interactions with 27students), Calculus data from four different tu-tors (84 dialogues), and ESL dialogues collectedfrom 15 pairs of students (30 dialogues alto-gether).While we have drawn upon data from all ofthese domains for testing the underlying lan-guage processing technology for our develop-ment effort, for our user studies we have so farmainly drawn upon our ESL corpus, which in-cludes conversations between students aboutevery-day tasks such as ordering from a restau-rant or about their pets.
We chose the languageESL data for our initial user tests because weexpected it to be easy for a general population torelate to, but we plan to begin using calculusdata as well.5.2 InfoMagnets InterfaceAs mentioned previously, once the raw dia-logue corpus is collected, the next step is to siftthrough this data and assign utterances (orgroups of utterances) to classes conceptualizedby the author.
Clustering is a natural step in thiskind of exploratory data analysis, as it promoteslearning by grouping and generalizing fromwhat we know about some of the objects in acluster.
For this purpose we have designed theInfoMagnets interface, which introduces a non-technical metaphor to the task of iterative docu-ment clustering.
The InfoMagnets interface wasdesigned to address the problems identified inthe user study discussed above in Section 4.Specifically, we expected that those problemscould be addressed with an interface that:1.
Divides dialogues into topic basedsegments and automatically clustersthem into conceptually similar classes2.
Eliminates primacy effects of sequen-tial dialogue consumption by creating aninclusive compilation of all dialoguetopics3.
Makes the topic similarity of docu-ments easily accessible to the userThe InfoMagnets interface is displayed inFigure 1.
The larger circles (InfoMagnets) cor-respond to cluster centroids and the smaller ones(particles) correspond to actual spans of text.Lexical cohesion in the vector space translatesinto attraction in the InfoMagnet space.
The at-traction from each particle to each InfoMagnet isevident from the particle?s position with respectto all InfoMagnets and its reaction-time when anInfoMagnet is moved by the user, which causesthe documents that have some attraction with itto redistribute themselves in the InfoMagnetspace.Figure 1 InfoMagnets InterfaceBeing an unsupervised learning method, clus-tering often requires human-intervention forfine-tuning (e.g.
removing semantically-weakdiscriminators, culling meaningless clusters, ordeleting/splitting clusters too fine/coarse for theauthor?s purpose).
The InfoMagnets interfaceprovides all this functionality, while shieldingthe author from the computational details inher-ent in these tasks49Initially, the corpus is clustered using the Bi-secting K-means Algorithm described in (Kumaret al, 1998).
Although this is a hard clusteringalgorithm, the InfoMagnet interface shows theparticles association with all clusters, given bythe position of the particle.
Using a cross-hairlens, the author is able to view the contents ofeach cluster centroid and each particle.
The au-thor is able to select a group of particles andview the common features between these parti-cles and any InfoMagnet in the space.
The inter-face allows the editing of InfoMagnets byadding and removing features, splitting In-foMagnets, and removing InfoMagnets.
Whenthe user edits an InfoMagnets, the effect in theparticle distribution is shown immediately and inan animated way.5.3 XML formatThe data collected from the conversationsin .txt format are reformatted into XML formatbefore being displayed with InfoMagnet tool.The basic XML file contains a transcription ofthe conversational data and has the followingstructure: Under the top root tag, there is <dia-logue> tag which designates the conversionabout a topic.
It has an ?id?
attribute so that wecan keep track of each separate conversation.Then each sentence has a <sentence> tag withtwo attributes ?uid?
and ?agent?.
?uid?
is a uni-versal id and ?agent?
tells who was speaking.Additionally, sentences are grouped into seg-ments, marked off with a <subtopic> tag.The user?s interaction with the InfoMagnet in-terface adds a ?subtopic-name?
attribute to thesubtopic tag.
Then, the authoring interfaceproper, described below, allows for further ad-justments and additions to the xml tags.
Thefinal knowledge sources will be generated fromthis XML based representation.6 AuthoringThe authoring environment proper consists oftwo main views, namely the authoring view andtutoring view.
The authoring view is where theauthor designs the behavior of the conversa-tional agent.
The authoring view has two levels;the topic level and the subtopic level.
The tutor-ing view is what a student will be looking atwhen interacting with the conversational agent.Our focus here is on the Authoring view.Authoring View: Topic LevelThe Topic level of the authoring view allows formanipulating the relationship between subtopicsas well as the definition of the subtopic.
Figure 2shows the topic level authoring view, whichconsists of two panels.
In the left, the author in-puts the description of the task that the studentwill engage in with the agent.
The author canspecify whether the student will be typing ortalking, the title of the topic, the task description,an optional picture that aids with the task (suchas a menu or a map of a city), and a time limit.In the right panel of the topic level authoringview, the structure imposed on the data by inter-action with the InfoMagnets interface is dis-played in sequential form.
The top section of theinterface (figure 2, section A) has a textbox forspecifying an xml file to read.
The next section(figure 2, section B), ?Move / Rename Subtopic?displays the subtopics.
The order of the subtop-ics displayed in this section acts as a guidelinefor the agent to follow during the conversation.Double-clicking on a subtopic will display asubtopic view on the right panel.
This view actsas a reference for the agent?s conversationwithin the subtopic and is explained in the nextsection.
The author can also rearrange the orderof subtopics by selecting a subtopic and usingthe ?>?
and ?<?
buttons to move the subtopicright or left respectively.
?x?
is used to deletethe subtopic.
The author can also specifywhether the discussion of a subtopic is required(displayed in red) or optional (in green) usingthe checkbox that is labeled ?required?.
Clickingon the ?Hide Opt?
button will only display therequired subtopics.The last section of the right panel in topiclevel authoring view (figure 2, section C) is ti-tled ?move subtopic divider?.
A blue line de-notes the border of the subtopic.
The author canmove the line up or down to move the boundaryof the subtopics automatically inserted by theInfoMagnets interface.
The author can also clickon any part of conversation and press the ?split?button to split the subtopic in two sections.
Inaddition, she can change the label of the sub-topic segment using the drop down list.50Figure 2: Topic Level Authoring ViewAuthoring View: Subtopic LevelWhile the Topic View portion of the authoringinterface proper allows specification of whichsubtopics can occur as part of a dialogue, whichare required and which are optional, and whatthe default ordering is, the Subtopic Level is forspecification of the low level turn-by-turn detailsof what happens within a subtopic segment.This section reports early work on the design ofthis portion of the interface.The subtopic view displays a structure that theconversational agent refers to in deciding whatits next contribution should be.
The buildingblocks from which knowledge sources for thedialogue engine will be generated are templatesabstracted from example dialogue segments,similar to KCD specifications (Jordan, Ros?, &VanLehn, 2001; Ros?
& Torry, 2004).
As partof the process of abstracting templates, each ut-terance is tagged with its utterance type using amenu-based interface as in (Gweon et al, sub-mitted).
The utterance type determines whatwould be an appropriate form for a response.Identifying this is meant to allow the dialoguemanager to maintain coherence in the emergingdialogue.
Users may also trim out undesiredportions of text from the actual example frag-ments in abstracting out templates to be used forgenerating knowledge sources.Each utterance type has sets of template re-sponse types associated with them.
The full setof utterance types includes Open questions,Closed questions, Understanding check ques-tions, Assertions, Commands/Requests, Ac-knowledgements, Acceptances, and Rejections.The templates will not be used in their authoredform.
Instead, they will be used to generateknowledge sources in the form required by thebackend dialogue system as in (Ros?
& Hall,2004), although this is still work in progress.Each template is composed of one or more ex-changes during which the speaker who initiatedthe segment maintains conversational control.
Ifcontrol shifts to the other speakers, a new tem-plate is used to guide the conversation.
Aftereach of the controlling speaker?s turns within thesegment are listed a number of prototypical re-sponses.
One of these responses is a default re-sponse that signals that the dialogue shouldproceed to the next turn in the template.
Theother prototypical responses are associated withsubgoals that are in turn associated with othertemplates.
Thus, the dialogue takes on a hierar-chical structure.Mixed initiative interaction is meant toemerge from the underlying template-basedstructure by means of the multi-threaded dis-course management approach discussed in (Ros?& Torrey, 2004).
To this end, templates aremeant to be used in two ways.
The first way is51when the dialogue system has conversationalcontrol.
In this case, conversations can be man-aged as in (Ros?
et al, 2001).
The second way inwhich templates are used is for determining howto respond when user?s have conversational con-trol.
Provided that the user?s utterances matchwhat is expected of the conversational partici-pant who is in control based on the current tem-plate, then the system can simply pick one of theexpected responses.
Otherwise if at some pointthe user?s response does not match, the systemshould check whether the user is initiating yet adifferent segment.
If not, then the system shouldtake conversational control.7 Future PlansIn this paper we have discussed our user re-search and design process to date for the devel-opment of TuTalk, an authoring environment forconversational agents for educational purposes.We are continuing our user research and designiteration with the plan of end-to-end system test-ing in actual use starting this summer.AcknowledgementsThis work was supported in part by Office of NavalResearch, Cognitive and Neural Sciences DivisionGrant N00014-05-1-0043 and NSF GrantSBE0354420.ReferencesAleven , V. and Ros?, C. P. 2004.
Towards EasierCreation of Tutorial Dialogue Systems: Integrationof Authoring Environments for Tutoring and Dia-logue Systems, Proceedings of the ITS Workshopon Tutorial Dialogue SystemsAllen, J., Byron, D., Dzikovska, M., Ferguson, G.,Galescu, L., & Stent, A.
2000.
An Architecture fora Generic Dialogue Shell.
NLENG: Natural Lan-guage Engineering, Cambridge University Press, 6(3), 1-16.Beyer, H. & Holtzblatt, K. (1998).
Contextual De-sign, Morgan Kaufmann Publishers.Gweon, G., Ros?, C., Wittwer, J., Nueckles, M.(submitted).
Supporting Efficient and ReliableContent Analysis with Automatic Text ProcessingTechnology, Submitted to INTERACT ?05.Jordan, P., Ros?, C. P., & VanLehn, K. (2001).
Toolsfor Authoring Tutorial Dialogue Knowledge.
In J.D.
Moore, C. L. Redfield, & W. L. Johnson (Eds.
),Proceedings of AI-ED 2001 (pp.
222-233).
Am-sterdam, IOS Press.Koedinger, K. R. & Nathan, M. J.
(2004).
The realstory behind story problems: Effects of representa-tions on quantitative reasoning.
The Journal of theLearning Sciences, 13(2).Nathan, M. J.
& Koedinger, K. R. (2000).
Movingbeyond teachers?
intuitive beliefs about algebralearning.
Mathematics Teacher, 93, 218-223.Porter, M. 1980.
An Algorithm for Suffix Stripping,Program 14 {3}:130 ?
137.Robertson, S. and Walker, S., 1994.
Some simpleeffective approximations to the 2-poisson modelfor probabilistic weighted retrieval Proceedings ofSIGIR-94.Ros?, C. P., and Torrey, C. (2004).
,DRESDEN: To-wards a Trainable Tutorial Dialogue Manager toSupport Negotiation Dialogues for Learning andReflection, Proceedings of the Intelligent TutoringSystems Conference.Ros?, C. P. and Hall, B.
(2004).
A Little Goes a LongWay: Quick Authoring of Semantic KnowledgeSources for Interpretation, Proceedings of SCa-NaLu ?04.Ros?, C. P. 2000.
A framework for robust semanticinterpretation.
In Proceedings of the First Meetingof the North American Chapter of the Associationfor Computational Linguistics, pages 311?318.Ros?, C. P., Pai, C., Arguello, J.
2005.
Enabling Non-Linguists to Author Advanced Conversational In-terfaces Easily.
Proceedings of FLAIRS 2005.Steinbach, Kepis, and Kumar, A Comparison ofDocument Clustering Techniques, pg.
8.http://lucene.apache.org52
