Unsupervised Discovery of Phonological Categories throughSupervised Learning of Morphological RulesWalter  Dae lemans*CL & AI, T i lburg  Uniw'xs i tyP .
( ) .Box  90153, 5000 LE  T i lburgThe  Nether landswal ter ,  dae-I emans@kub, n lAbstractWe describe a case study in tit(', ap-plication of symbolic machinc learningtechniques for the discow;ry of linguis-tic rules and categories.
A supervisedrule induction algorithm is used to learnto predict the.
correct dimilmtive suffixgiven the phonological representation fDutch nouns.
The system produces ruleswhich are comparable, to rules proposedby linguists, l,Slrthermore, in the processof learning this morphological task, thephonemes used are grouped into phono-logically relevant categories.
We discussthe relevance of our method for linguis-tics attd language technology.1 I n t roduct ionThis paper shows how machine lem'ning tech-niques can be used to induce linguistically relevantrules and categories fl'om data.
Statistical, con-nectionist, and machine learning induction (data-oriented approaches) are currently nsed mainly inlanguage, engineering at)t)lications in order to alle-viate the.
linguistic knowledge acquisition bottle-neck (the fact that lexical an(t grammatical knowl-edge usually has to be reformulated t'i'()iii scratchwhenever a new application has to be built oran existing application ported to a new domain),and to solve problems with robustness and cover-age inherent in knowledge-based (the.ory-oriente.d,hand-crafting) approaches.
Linguistic relevance.or inspectability of the induced knowledge is usu-ally not an issue in this type of research.
\]n lin-guistics, on the other hand, it is usually agreedthat while computer modeling is a useful (or essen-tial) tool for enforcing internal consistency, com-pleteness, and empirical validity of the linguistictheory being modeled, its role in formulating orevaluating linguistic theories is minimal.In this paper, we argue that machine learningtechniques can also assist in linguistic theory for-*Visiting fl'.llow at NIAS (Netherlands Instituee forAdvanced Studies), Wassenaar, The.
Netherlands.Peter  Berck  and  Steven  Gi l l i sL inguist ics ,  Univers i ty  of AntwerpUnive.rs i te i tsple in 1, 2610 WiMjkl ~elgiumsteven, gillis@uia, ua.
ac .
bepeter, berck@uia, ua.
ac.
bemation by providing a new tool for the evalua-tion of linguistic hypotheses, for the extractionof rules front corpora, and for the discovery (ifuseflll linguistic categories.
As a case.
study, weapply Quinlan's C4.5 inductive machine learningme.thod (Quinlan, 1993) to a particular linguistictask (diminutive fi)rmation in Dutch) and showthat it; can be use(l (i) to test linguistic hypothe-ses about this process, (ii) to discover interestingmorphological rules, and (iii) discover interestingphonological categories.
Nothing hinges on ourchoic.e of (\]4.5 as a rule induction mechanism.
Wcchose it because it is an easily available and so-phisticated instance of the class of rule inductionalgorithms.A second focus of this paper is the interac-tion between supervised and unsulmrvised ma-chine learning me.thods in linguistic discovery, insupervised learning, the.
learner is presented a setof examples (the experience of the system).
Theseexamples consist of an inImt outtmt association(in our case, e.g., a representation of a llotln asinput, and the corresponding dimilmtive sul\[ix asoutput).
Unsupervised learning methods do not1)rovide the h',m'ner with inforlnatioil at)out theoutf)ut to be generated; only the inputs ar(; I)re-sented to the learner as experience, not the targetoutputs.Unsupervised learning is necessarily more lim-ited t, hm~ supervised learning; the only informa-tion it has to construct categories i the similaritybetween inputs.
Unsupervised learning has beensuccessflflly applied e.g.
for the discovery of syn-tactic categories from corpora on the basis of dis-tributional inforlnation about words (Finch andChalet 1992, tIughes 1994, Schiitze 1995).
Wewill show that it, is possible and useful to makeuse of unsupervised learning relative to a particu-lar task which is being learned in a supervised way.In our experinmnt, phonological categories are dis-covered in an unsupervised way, as a side-effect ofthe supervised learning of a morphological prob-lem.
We will also show that this raises interesl;ingquestions about, the.
task-dependence of linguisticcategory systems.952 Superv ised  Ru le  Induct ion  w i thC4.5For the experiments, we used C4.5 (Quinlan,1993).
Although several decision tree and ruleinduction variants have been proposed, we chosethis program because it is widely available andreasonably well tested.
C4,5 is a TDIDT (TopDown Induction of Decision Trees) decision treelearning algorithm which constructs a decisiontree on the basis of a set of examples (tit('.
trainingset).
This decision tree has tests (feature names)as nodes, and feature values as branches betweennodes.
The leaf nodes are labeled with a categoryname and constitute the output of the system.
Adecision tree constructed on the basis of examplesis used after training to assign a class to patterns.To test whether the tree has actually learned theproblem, and has not just memorized the itemsit was trained on, the 9eneralization accuracy ismeasured by testing the learned tree on a part ofthe dataset not used in training.The algorithm for the construction of a C4.5decision tree can be easily stated.
Given are atraining set T (a collection of examples), and afinite number of classes C1 ... C~.1.
If T contains one or more cases all belongingto the same class Cj, then the decision treefor 5/" is a leaf node with category Cj.2.
If T is empty, a category has to be found onthe basis of other information (e.g.
domainknowledge).
The heuristic used here is thatthe most frequent class in the initial trainingset is used.3.
If T contains different classes then(at Choose a test (feature) with a finite num-ber of outcomes (values), and partitionT into subsets of examples that have thesame outcome for tim test chosen.
Thedecision tree.
consists of a root node con-taining the test, and a branch for eachoutcome, each bt'anch leading to a sub-set of the original set.
(b) Apply the procedure recursively to sub-sets created this way.In this algorithm, it is not specitied which testto choose to split a node into sut)trees at somepoint.
Taking one at random will usually result inlarge decision trees with poor generalization per-formanee, as uninformative tests may be chosen.Considering all possible trees consistent; with thedata is computationally intractable, so a reliableheuristic test selection method has to be found.The method used in C4.5 is based on the con-cept of mutual information (or information gain).Whenever a test has to be selected, the feature ischosen with the highest information gain.
This isthe feature that reduces the information entropyof the training (sub)set on average most, when itsvalue would be known.
For the computation ofinformation gain, see Quinlan (1993).Decision trees can be easily and automaticallytransformed into sets of if-then rules (productionrules), which are in general easier to understandby domain experts (linguists in our case).
InC4.5 this tree-to-ruh; transformation i volves ad-ditional statistical evaluation resulting sometimesin a rule set more understandable att(.l accuratethan the corresponding decision tree.The C4.5 algorithm also contains a value group-ing method which, on the basis of statistical in-formation, collapses different values for a featureinto the same category.
That way, more concisedecision trees and rules can be produced (insteadof sew'~ral different branches or rule conditions foreach wflue, only one branch or condition has tobe detined, making reference to a (;lass of values).The algorithm works as a heuristic search of thesearch space of all possible partitionings of the wd-ues of a particular tbature into sets, with the for-Ination of homogeneous nodes (nodes representingexamples with predominantly the same category)as a heuristic guide.
See Quinlan (1993) for moreinformation.3 D iminut ive  Format ion  in DutchIn the remainder of this t)ape.r, we will describea case study of using C4.5 to test linguistic hy-1)otheses attd to discover regularities and cate-gories.
Tit(,.
case study concerns allomorphy inDutch diminutive formation, "one of the morevexed probleins of l)utch i,honology (...) \[and\]one of the most spectacular phenomena of mod-ern Dutch morphophonemics" (Trommelen 1983).Diminutive forlnation is a productive morpholog-ical rule in Dutch.
Diminutives are formed by at-taching a form of the Germanic sntfix -tje to t;hesingular base form of a noun.
The suffix showsallomorphic variation (Table 1).Nounhuts (house)man (man)raam (window)woning (house)baan (job)FormhuisjemannetjeraalnpjewoninkjebaantjeSuffix-jc-gtj(;-pj('~-tieTable 1: Allomorphic variation in Dutch diminu-tives.The fi'equency distribution of the different cat-egories is given in Table 2.
We distinguish be-tween database frequency (frequency of a suffix ina list, of 3900 diminutive forms of nouns we tookfrom the CELEX lexical database 1) and corpus~Developed by tile Center for Lexical Ilfforma-tion, Nijmegen.
l)istributed by tile Linguistic DataConsortium.96frequency (frequency of ~ sutfix in the text corpuson which the word list was based).1).,,al,as.
'X, LC(,,p-s \]~t.i7 --|- - -~897 - ->ig.7~, T 3b.,~%-1U / ar.a'x, / i;,?
/ / 7 0.9% tj 104 2.7'x, 1 4.o% 1i?io J 77 2 0% 3 8% /1,, _ : ?
'l'abh~ 2: Lexicoil an(l (:O,l)US \[requ(!n(:y of alh/-morphs.llistoricnlly, dilh;rcnt a.nalyses of diminutive for-real;ion }tav(~ taken a (lifferenl, view of tile rulesthai; goveru the (:hoi(:(', of 1;he diminutiv(~ sullix,and ot!
the, linguistic con(:el)l;s playing a role inthese rules (see, e.g.
T(; Winkel 11866, Kruizinga1(.t15, Cohen 11958, and l'ef('~t'ellces ill Tl'OItlillt',lell1983).
In t;ho, lal;1;er, il; ix argued l;hal; (limimll;iveformation ix a local 1)recess, in which collCel)l;ssuch as word stress and morphological st, rll(;l,llre(proposed ill l;he earlier analyses) (1() not play ar()le.
The rhyme of the last syllabic of tim nounis necessary and sutlicienl; t(/ predict I;}m col'l'(~cl;a/lomort)h. The, nal;uraJ (:ategorics (or feal,ures)wlfi(:h are hyllothesised in her rules in(:lu(h', obst, r'u-ents, .sonorwnl,.% alld the (:lass of bimoraic vowels(consisting of long vowels, diphtongs and schwa).Diminutive formation is a. Slna\[l linguisl;i(: (lo-main for which different COmlmting l,hcories have}men pr(/t)os('xl ~&ll(\[ fol' whi(:ll (liff(',r(~nt generaliza-l;ions (in terms of rules and linguistic categories)have been proposed.
What  we will show tw, x~; ishow machine learning techniques tllay t)(~ l lSed I;O(i) test competing hyi)otheso~s, (ii) discovc, r gene, r-alizations in the data whi(;h c}tIl I;ll(}II t)e comt/are(1to the generMizal;ions formulated })y linguists, aim(iii) discover phonologi(:al categories in ml unsu-pervised way by supervised learning of diminutivesuttix prediction.4: Exper imentsli'or ea(:h of l,he 3900 nouns we coll(!cted, th(!
fol-lowing information was kept.1.
The phoneme transcription describing thesyllable structure (in terms of onset, nucleus,and coda) of l;he last three syllables of theword.
Missing slots are imlicatexl with =.2.
D)r each of l;hese l;hree last syllables the, pres-elICe ()I' abse .
l l ce  o f  Sl;l'O, ss.3.
The (:orreslionding dimitmtive al lomorph, ab-breviated to E (-etjc), T (-tie,), ./ l-j(;), K (-Me), and I' (-pie).
This is the' '(:al,egory' ofthe word to be learned by the learner.Some examples are given below (l;he word itselfand its gloss are provided for convenience and werenot used in the exllerimenl,s ).- b i = - z @ = + m h nt  J b iezenmand (basket ).
.
.
.
.
.
.
.
+ b I x E b ig  (p ig ).
.
.
.
+ b K = - b a n T b i jbaan  (s ide  job).
.
.
.
+ b K = ~ b @ i T b i jbe l  (b ib le )4.1 Exper imenta l  MethodThe, ext)(wim(ml;al set-u t) use(t in all eXl)Crin/(ml:sconsisted of a ten-lbhl cross-wflid;ttion eXl)erimcnt(Weiss & Kulikowski 1991).
In this set-up, thedatabase is part it ioned l;en time~s, each with ;t diL\['orelll.
101~/ (If lll(~ dal;asel; as the tesl prot, mid theremaining 9/1% as training parL.
For each ?
)f l}tel,(',n simulations in our exp('~riinelll;s, I;h(~ l;esl; p;u't,was used to to, st go, ueralization perfornuu,:e. Thesuccess rate of an algoril;hm is obtained I)y cah:u-l a t ; ing  Ihc  av(ua , r (  ,, aCClll '/ lCy (ll l l ltl l)(!l ' O\[: l;(~SI, t)nt, -I,ern categories correctly predit:ted) over the l:entest sets in the ten-fold cross-validation eXlmri-lilO.n{;.4.2 Learna l  f i l ityThe, exp(~rim(mts show thai; the diminutive li~-marion 1)roblem is learnMfle in a data-(/ri(ml;(~(lway (i.e.
1)y extraction of regularities \['rein (!x-amlflCs , without, any a priori knowledge ahout~the domain").
The average accuracy on unseentx~st data of 98.4% should be compared to bast;-line l)crforlnan(:e measures baso, d on tnolmbilit~y -based guessing.
This baseline would t)e an accu-ra(:y of a.l)out 4()~ for this prol)h;m. This showst;hat the tn'()l)h'm is a.lm(/st t)(;rlh(:tly h',aruabl(!
I)yinduction, It, shouhl 1)e noted that CI';I,I,;X con-tains a numl)(~r ()\[ coding (',trots, so that some (fflhe ~wrong' all(mlOrl)hs \])r(',(li(:ted by the ma(:lfineh;arning system were actually (:(II'I'(~(;L, Wq did notcorrect for this.\]It the next; three secl;ions, we will describe Lheresull;s of l;he (~xImrim(;nts; tirst on the 1;ask of (:Olll-paring conlli(:ting l;he(/reti(:al hypotheses, then ondiscoverittg linguistic gen(;ralizaLions, and flintily(m unsul)(~rvis(~(l dis(:overy of l/h(/nologica.l cat(>gories.5 L ingu is t i c  Hypothes is  Tes t ing()n the basis of the analysis of I )utch diminutiveformation by TronuneJen (1983), discussed brietlyin SecLion 3, Lhe following hypotheses (among oth-ers) can be \[brnmlated.1.
Only informatioil about  the last, syllable isre, levant in predicting the, correct al lomorph.2.
\[nlormation about l;he onset of the last sylla-bi(, is irrelevant in predicting the, correct al-lomorph.3.
Stress is irrelevant in predicting l;he correctal lomorph.
:~lCxcepl; syllMde stru(:tm-e,97In other words, information about the rhyineof the last syllable of a noun is necessary andsufficient o predict the correct allomorph of thediminutive suffix.
To test these hypotheses, weperformed four experiments, training and testingthe C4.5 machine learning algorithm with fore' dif-ferent corpora.
These corpora contained the fol-lowing information.1.
All information (stress, onset, nucleus, coda)about the three last syllables (3-SYLL cor-pus).2.
All information about the last syllable(SONC corpus).3.
Information about the last syllable withoutstress (ONC corpus).4.
Information about the last syllable withoutstress and onset (NC corpus).5.1 ResultsTable 3 lists the learnability results.
The gener-alization error is given for each allomorph for thefour different; training corpora.s''mXalII -kjy\[ -p3eErrors  and Er ror  percental  e.s3 SYLI, SONC ONC NC61 1.6 79 2.0 80 2.0 77 2.013 0.716 1.126 7.34 5.22 1.913 0.715 1.049 13.70 02 1.914 0.716 1.148 13.50 02 1.914 0.714 1.044 12.30 05 4.8Table 3: Error of C4.5 on the different corpora.The overall best results are achieved with themost elaborate corpus (containing all informationabout; the three last syllables), suggesting that,eontra Trommelen, important information is lostby restricting attention to only the last syllable.As far as the different encodings of the last sylla-ble are concerned, however, the learnability exper-iment coroborates Trommelen's claim that stressand onset are not necessary to predict the correctdiminutive allomorph.
When we look at the errorrates for individual allomorphs, a more complexpicture emerges.
The error rate on -etje dramati-cally increases (from 7% to 14%) when restrictinginformation to the last syllable.
The -k~e allo-morph, on the other hand, is learned perfectly onthe basis of the last syllable alone.
What has hap-pened here is that the learning method has over-generalized a rule predicting -kje after the velarnasal, because the data do not contain enough in-formation to correctly handle the notoriously diffi-cult opposition between words like leerling (pupil,takes -etje) and koning (king, takes -kje).
Purther-more, the error rate on -pje is doubled when onsetinformation is left out from the corpus.We can conch;de from these experiments thatalthough the broad lines of the analysis by Trom-melen (1983) are correct, the learnability resultspoint at a number of problems with it (notablywith -kje versus -etje and with -pje).
We will movenow to the use of inductive learning algorithms asa generator of generalizations about the domain,and compare these generalizations to the analysisof Trommelen.6 Superv ised  Learn ing  o fL ingu is t i c  Genera l i za t ionsWhen looking only at the rhyme of the last sylla-ble (the NC corpus), the decision tree generatedby C4.5 looks as follows:Decision Tree:coda in {rk,nt,lt,rt,p,k,t,st,s,ts,rs,rp,f,x, ik,Nk,mp, xt,rst,ns ,nst,rx,kt, ft, if ,mr, Ip,ks, is,kst, ix} : Jcoda in {n,=,l, j ,r,m,N,rn,rm,w,lm}:nucleus in {I,A,},O,E}:coda in {n,l,r,m}: Ecoda in {=,j,rn}: Tcoda in {rm,lm}: Pcoda = N:1 nucleus = I: KI nucleus in {A,O,E}: Enucleus in {K,a,e,u,M,@,y,o,i,L,), I,<}:\[ coda in {n,=,l,j,r,rn,w}: T\[ coda = m: PNotice that the phoneme representation usedby CELEX (called DISC) is shown here insteadof the more standard IPA font, and that the valuegrouping mechanism of C4.5 has created a mnnberof phonological categories by collapsing differentphonemes into sets indicated by curly brackets.This decision tree should be read as follows:first check the coda (of the last syllable).
If itends in an obstruent, the allomorph is -jc.
If not,check tile nucleus.
If it is bimoraic, and the codai s /m/ ,  decide -pje, if the coda is not /m/ ,  decide-tje.
When the coda is not an obstruent, the nu-cleus is short and the coda is /ng/,  we have tolook at the nucleus again to decide between -kjeand -etje (this is where the overgeneralization to-kje for words in -ing occurs).
Finally, the coda(nasa-liquid or not) helps us distinguish between-etje and -pje for those cases where the nucleus isshort.
It should be clear that this tree can eas-ily be formulated as a set of rules without loss ofaccuracy .An interesting problem is that the -et je versus-kje problem for words ending in -ing couht hot besolved by referring only to the last syllable (C4.5and any other statistically based induction algo-rithm overgeneralize to -kjc).
The following is theknowledge derived by C4.5 t'rofll the flfll corpus,with all information about the three last syllables(the 3 SYLL corpus).
We provide the rule versionof the inferred knowledge this time.98Default class is -tje1.
IF coda last  is  / lm/ or /rm/THEN -pje2.
IF nucleus last is \[+bimoraic\]coda last is /m/THEN -pje3.
IF coda last is /N/THEN IF nucleus penultimate is empty(monosyllabic word) or schwaTHEN -etjeELSE -kje4.
IF nucleus last is \[+short\]coda last is \[+nas\] or \[+liq\]THEN -etje5.
IF coda last  is  \[+obstruent\]THEN - jeThe default class is -tjc, which is the allomorphchosen when none of the other rules apply.
Thisexplains why this rule set looks simi)h'.r than tit(;decision tree earlier.The first thing which is interesting in this ruleset, is that only tlu'ee of the twelve presented fea-tures (coda an(1 nllclelts of (;lie last syllable, nll-cleus of the i)emlltimate syllal)le) m'e used in therules.
Contrary to the hyi)oth(;sis of Trommelen,apart from the rhyme of the last sylbfl)le, the m>(:\[eus of the pemfltimate sylhd)le is taken to \])ere.levant ;~s well.The induced rules roughly correspond to theprevious decision tree, but; in ad(lition a solutionis provided to the -etje versus -kje problem forwords ending in -in9 (rule 3) making use of in-formation about the nucleus of the.
t)emfltiInatesyllabi(;.
Rule 3 states that words ending in /ng/get -etjeas (liminutive alloinorl)h when they aremonosyllables (nucleus of the penultimate syllableis empty) or when they have a schwa as t)(multi -mate rainless, and -kjc othe, rwise.
As fro as wenow, this generalization has not been prot)osed inthis form in the lmblished literature on diminutiveformation.We conclude from this part of the experimentthat the Inaehine learning inethod has suc(:ee(ledin extracting a sophistieate(l set of linguistic rulesfrom the examph'.s in a purely data-oriented way,an(l that these rules are formulated at a level thatmakes their use in the development of linguistictheories possible.7 D iscovery  o f  Phono log ica lCategor iesTo structure the phoneme inventory of a language,linguists define features.
'\['hese ekLIl be interpretedas sets of st)ee(:h sounds (categories): e.g.
thecategory (or feature) labial groups those speechsounds that involve the lips as an a(:tive art\[c-ulator.
Speech sounds behmg to different cate-gories, i.e., are defined by ditferent \['e~tures.
l,',.g.t is voiceless, a coronal, and a stop.
Categoriest)roposed in phonology are inspired by articula-tory, acoustic or tmreeptual phonetic ditferencesbetween speech sounds.
They are also proposedto allow an optimally concise or elegant formu-lation of rules for the description of phonologi-cal or mot'phological processes.
E.g., the so-calleAmajor (:lass features (obstruents, nasals, liquids,glides, vowels) efficiently explain syllable structureeomput;ation, lint are of little use in the definitionof rules describing assimilation.
For ass\[re\[In\[ion,placu of mti(:ulation f(~atllr(~s arc t)est ilse(l. Thissituation has led to the t)roposa.l of many dillhrenCphonoh)gieal category systems.Whih; constructing the decision tre.e (see prey\[-Oils section), several t)honologically relevant cat-(;gories are 'discovered' by the value groupingmechanism in C4.5, including the nasals, the liq-uids, the obstruents, the short vowels, mtd thebimoraic vowels.
This last category correspondscompletely with the (then new) category hypoth-esise, d by Trommelen and containing the long vow-els, tit(; diphtongs att(l the s('hwa, in oth(;r words,the learning a.lgorithm has discovered this set ofphonemes to 1)e a useful category in solving the(|iminut;ive formation problem t)y t)rovi(ling ml e?-I,e.nsional detinition of it (a lisl; of tim inst;ulees ()\[:I;he ea.tegory).This raises the question of the task-dependenceof linguistic categories.
Similar experiments inDutch t)lural formation, for examt)le, fail to pro-duce th(' (:atcgory of bimoraic vowels, and for sometasks, categori(:s show u t) which hi~vc no ontolog-ical status in linguistics.
In other words, mak-ing category formation del)endent oil the task tot)e learned, unde.rmitms the.
tratlitional inguisticideas about absolute, task-indel)endent (and even1;mguage-in(h',t)endeitt) categories.
We presenthe I 'e  & lI(!~,v methodology with which this flltl(lDo-mental issue in linguistics can t)(; investigated:category systems ext;racted for difl'erent asks indifferent languages can be studied to see whichcategories (if any) truely have a universal status.This is subject br fllrther resem'ch.
It wouhl alsol)e use.rid to stu(ly the indu(:ed categories whenintensional descriptions (feature represeutations)are used as input instead of extensional descrit)-lions (phoitetnes).We also experimented with a siml)h;r alternativeto the computationally complex heuristic ategory\[orma.tion algorithm used by (;4.5.
This methodis inspire(1 by machine learning work on wflue di fference metrics (Stanfill & Waltz, 1986; Cost &Salzberg, :1993).
Starting fl'om the training setof the sut)ervised learning exl)erinlent (the set ()finput ouq)ut mappings used by the system to ex-tract rules), we selc(:t a particular feature (e.g.
thecoda of the last syllable), and comt)ute a table as-99sociating with each t)ossit)le value of tile featurethe number of times the pattern in which it, oc-curs was assigned to each different category (inthis case, each of the the five allomorphs).
Thisproduces a table with for each value a distributionover categories.
This table is then used in stan-dard clustering approaches to derive categories ofvalues (in this case consonmlts).
The followingis one of these clustering results.
The exampleshows that this computationally simple approachalso succeeds in discovering categories in an unsu-pervised way on tile basis of data for supervisedlearning.. .
.
.
.
.
.
> lI .
.
.
.
.
.
.
> r-I .
.
.
.
.
.
.
.
.
.
.
.
> nI - - - I  .
.
.
.
.
I .
.
.
.
.
> tI I I .
.
.
.
.
I .
.
.
.
> kI .
.
.
.
.
.
.
.
.
.
.
.
I I .
.
.
.
> sI .
.
.
.
.
I - ->  pI .
.
.
.
I I - ->  fI .
.
.
.
.
I .
.
.
.
> mI .
.
.
.
I .
.
.
.
> NI .
.
.
.
I - ->  xI__1-> jI ->  wSeveral categories, relevant for diminutive for-mation, such as liquids, nasals, the velar nasal,semi-vowels, fi'icatives etc., are reflected in thishierarchical clustering.8 Conc lus ionWe have shown by example that machine learn-ing technique, s can profitably be used in linguisticsas a tool for the comparison of linguistic theoriesand hypotheses or for the discovery of new lin-guistic theories in the form of linguistic rules orcategories.The case study we presented concerns dimiml-live formation in Dutch, for which we showed that(i) machine learning techniques can be used to cor-roborate and falsify some of the existing theoriesabout the phenomenon, and (ii) machine learningtechniques can be used to (re)discover interestinglinguistic rules (e.g.
the rule solving the -etjc ver-sus -kje problem) and categories (e.g.
the categoryof bimoraic vowels).The extracted system can of course also be usedin language technology as a data-oriented systemfor solving particular linguistic tasks (in this casediminutive format!on).
In order to test the usabil-ity of the approach for this application, we com-pared the performance of the extracted rule sys-tem to tile performance of the hand-crafted rulesystem proposed by Trommelen.
Table 4 showsfor each allomorph the number of errors by theC4.5 rules (trained using corpus NC, i.e.
only therhyme of the last syllable) as opposed to an imple-mentation of the rules suggested by ~l?ommelen.One problem with the latter is thai; they often sug-gest more than one allomorph (the rules are notmutually exclusive).
In those cases where morethan one rule applies, a choice was made at ran-dom.Suffix Trommelen C4.5-tje 53 11-jc 12 12-eric 28 39-~iie 38 o-pje 21 4Total 152 66Table 4: Comparison of accuracy between hand-crafl;ed and induced rules.The comparison shows that C4.5 did a good jobof finding an elegant and accurate rule-based e-scription of the problem.
This rule set is usefulboth in linguistics (for evaluation, refinement, anddiscovery of theories) and in language technology.ReferencesCohen, A. Het Nederlandse diminutiefsuilix; eenmorfologische I)roeve.
De Nic~twe Taalgids, 51,40-45, 1958.Cost, S. and Salzberg, S. 'A weighte, d nearestneight)or algorithm tor learning with symbolicfeatures.'
Mach, ine Learning, l(/, 57 78, 1993.Finch, S. & N. Chater.
'Bootstrapping Syntac-tic Categories Using Statistical Methods', in:W.Daelemans & 1).Powers (eds.
), Backgrov, r~,dand E.,cperirnent.s in Machine Lear'nb~,g of Natv,-ral Language, Tilburg University, ITK, 1992.Itughes, J.
'Automatically Acquiring a Classifica-tion of Words', Phi) dissertation, University ofLeeds, School of Computer Studies, 1994Kruisinga, E. De vorm vail verkMnwoorden.
DeNieuwc ~lhalgids , 9, 96-97, 1915.Quinlan, J. It,.
C~.5 Programs for machine learn-ing 1993.Sch{itze, H., Ambiguity in Language Learning:Computational nd Cognitive Models, PhD dis-sertation, Stanford University, Department ofLinguistics, 1995.Stanfill, C. and Waltz, D.L.
'Toward Memory-based Reasoning'.
Communications of the A CM29, 1986, 1213 1228.\]?onunelen, M.T.G.
Thc syllable in Dutch, withspecial reference to diminutive formation.
Foris,Dordrecht, 1983.Weiss, S. and Kulikowski, C. (1991).
Computersystems that learn.
Morgan Kaufmann, SanMarco.Winkel, L.A. Te.
Over de verklcinwoorden.
DeTaalgids 4: 81-116.i00
