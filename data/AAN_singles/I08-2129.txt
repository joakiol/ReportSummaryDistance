How to Take Advantage of the Limitations with Markov Clustering?The Foundations of Branching Markov Clustering (BMCL)Hiroyuki AkamaTokyo Institute of TechnologyW9-10, 2-12-1,O-okayama, Meguroku,152-8552 Tokyo, Japanakama.h.aa@m.titech.ac.jpMaki MiyakeUniversity of OsakaMachikane-machi, Toyo-naka-shi,560-0043 Osaka, Japanmmiyake@lang.osaka-u.ac.jpJaeyoung JungTokyo Institute of TechnologyW9-10, 2-12-1,O-okayama, Meguroku,152-8552 Tokyo, Japanjung.j.aa@m.titech.ac.jpAbstractIn this paper, we propose a novel approachto optimally employing the MCL (MarkovCluster Algorithm) by ?neutralizing?
thetrivial disadvantages acknowledged by itsoriginal proposer.
Our BMCL (BranchingMarkov Clustering) algorithm makes itpossible to subdivide a large core clusterinto appropriately resized sub-graphs.
Util-izing three corpora, we examine the effectsof the BMCL which varies according to thecurvature (clustering coefficient) of a hubin a network.1 MCL limitations?1.1 MCL and modularity QThe Markov Cluster Algorithm (MCL) (Van Don-gen, 2000) is well-recognized as an effectivemethod of graph clustering.
It involves changingthe values of a transition matrix toward either 0 or1 at each step in a random walk until the stochasticcondition is satisfied.
When the hadamard powerfor each transition probability value is divided bythe sum of each column, the rescaling processyields a transition matrix for the next stage.
Afterrepeatedly alternating for about 20 times betweentwo steps?random walk (expansion) and probabil-ity modification (inflation)?the process will fi-nally reach a convergence stage in which the wholegraph is subdivided into a set of ?hard?
clusters thathave no overlap.
Although this method has beengenerally applied in various domains with notablesuccesses (such as Tribe-MCL clustering of pro-teins (Enright et al, 2002); Synonymy Network,created by the addition of noise data (Gfeller,2005); and Lexical Acquisition (Dorow et al,2005)), Van Dongen et al (2001) frankly acknowl-edge that there are limitations or weaknesses.
Forinstance, the readme file, which is included withthe free MCL software available via the Internetfrom Van Dongen?s group, remarks that ?MCL isprobably not suited for clustering tree graphs?.It should also be noted, however, that the grouphas provided no mathematical evidence for theirclaim of the MCL?s unsuitability for hierarchicalapplications.
What prompts this subtle caveat inthe first place?
Is this a limitation on the type ofgraph clustering that can employ random walks forspectral analysis?
Or, is it difficult for this tech-nique to (re-)form or adjust graph clusters thathave already been clustered into a kind of multi-layered organization?
Such questions are very im-portant when comparing the MCL with other graphclustering methods that employ (greedy) algo-rithms developed step by step in a tree form.A tree graph is essentially a kind of dendrogram,which means clustering results can be generatedsolely by making a cross cut at some height be-tween the root and the leaves.
In other words, asthere is no horizontal connection at the same level,it is not possible to create triangle circulation pathsin a single stroke.
However, the graph coefficientknown as ?curvature?
(Dorow, 2005) is appropri-ate for defining such structures.
The curvature, orthe cluster coefficient, of a vertex is defined as afraction of existing links among a node?s neighborsout of all possible links between neighbors.
Thus,a tree graph may be regarded as a chain of stargraphs where all the vertices have a curvature val-ue of 0.901It is certainly true that when a hub has a lowcurvature value, the corresponding cluster will beless cohesive and more sparse than usual.
Themodularity Q value is very low in such cases whenwe try to measure the accuracy of results fromMCL clustering.
Modularity Q indicates differ-ences in edge distributions between a graph withmeaningful partitions and a random graph for iden-tical vertices conditions.
According to Newmanand Girvan, ?
?=iiii aeQ )(2 , where i is the clus-ter number of cluster ic , iie is the proportion ofinternal links in the whole graph and ia  is the ex-pected proportion of ic ?s edges calculated as thetotal number of degrees in ic divided by the total ofall degrees (2*the number of all edges) in thewhole graph.
This value has been widely used asan index to evaluate the accuracy of clustering re-sults.1.2 Karate club simulationHowever, it would be an exaggeration to regardModularity Q is an almighty tool for accuratelydetermining the attribution value of each vertex ina graph cluster.
That is only true for modularity-based greedy algorithms that select vertices pair-ings be merged into a cluster at each step of thetree-form integration process based on modularityoptimization criterion.
However, such methodssuffer from the problem that once a merger is exe-cuted based on a discrimination error, there is nochance of subsequently splitting pairings that be-long to different subgroups.This fatal error can be illustrated as follows.Zachary?s famous ?Karate Club?
is often used assupervised data for graph clustering, because thecomplex relationships among the club members arepresented as a graph composed of edges represent-ing acquaintances and vertices coded indicatingfinal attachments to factions.
If the results ofgraph clustering were to match with the actualcomposition of sects within the club, one couldclaim that the tested method was capable of simu-lating the social relationships.However, the real difficulties lie at boundarypositions.
It is worth pointing out that the degreeof ambiguity is the same (0.5) for both vertices 3and 10 in Figure I, indicating that they occupy neu-tral positions while in reality they belong to differ-ent subgroups.
All modularity-based greedy algo-rithms would inevitably bind the two nodes at anearlier step in the dendrogram construction (at thefirst merging step in experiments conducted byNewman and Danon and at the second in Pujol?sexperiment).
In contrast, MCL is one of the rareclustering methods that avoids this type of mis-judgment (accurate results for the karate club net-work were also obtained with the Ward method),even though the modularity Q value for MCL is alittle lower (0.371) than values for greedy algo-rithms (for example, 0.3807 for Newman et al?sfast algorithm and 0.418 for Danon et al?s modi-fied algorithm).Figure I Karate clubThe karate club case suggests the possibility ofusing both graph clustering and modularity Q fromdifferent perspectives.
MCL allows us to regardboth clustering and discrimination on the sameplan if we do not treat modularity Q as an optimi-zation index but rather as an index of structuringdynamics balancing assembly and division.
To theextent that a graph clustering method is evaluatedin terms of its effectiveness in a variety of dis-crimination analyses with learning data extractedfrom real situations, it should be useful as a simu-lation tool.
For example, it is possible to test withthe karate club network the effects of supplement-ing the network by adding to the original graphanother hub with the highest degree value.
As thecurvature value of this new hub varies according tothe selection of vertices which become adjacent toit, we can re-execute MCL for the overall graph tosee how curvature is closely related with how itinfluences clustering results.
In general cases, thehub of a whole graph also tends to be the represen-tative node for the large-sized Markov clustercalled the ?core cluster?
(Jung, 2006).902Let us imagine that a highly influential new-comer joins the karate club and tries to contactwith half (17) of all the members, functioning as ahub within the network.
Even though this is apurely hypothetical situation, it is possible to pre-dict the impact on the network with MCL.Figures II, III Hub to high or low degree nodesFor example, one could classify the 34 verticesinto higher and lower degree subgroups, and set ahub that is adjacent to all vertices for one subgroupbut is far from the other subgroup.
MCL resultswould indicate that even when adding a hub withthe highest curvature value, it would be ineffectualin preventing a split (Figure II).
However, if thenewcomer were to be a friend with less sociablemembers, the club would be saved from being tornapart.
A hub connected with the lower degree sub-group, and thus having the lowest curvature value,would become part of the largest core cluster, be-cause the MCL would not subdivide the graph(Figure III).
In short, the results of MCL computa-tion hinge on the curvature value of the hub withthe highest degree value.2 The basic concept of BMCLThis connection-sensitive feature of MCL bringsus back to the limitations that Van Dogen et alinform their software users of.
Do these limita-tions really render the MCL unsuitable for treegraphs?
Should we not regard a low modularity Qvalue for a graph as a positive attribute if it is dueto the low curvature value for a hub?
In a very realsense, these questions are actually asking about thesame thing.
The point can be clearer if conceivedof in relation to a non-directed and cascading typeof three-layer graph, as depicted in Figure IV.Figure IV Three-layer tree-form networkThe root node at the top (the hub) is linked to allthe vertices in the intermediate layer but to none atthe bottom layer, even though there are moderatelevels of connectivity between the layers.
Connec-tions within a layer are extremely rare or absent.Clearly, the curvature of the hub would be influ-enced by the very low connectivity within layer 2.0.01 0.02 0.030.11corecluster &singletonclusters1core cluster &singleton clus-ters1cluster(not divided)0.151cluster or 2coreclusters1cluster(not divided)1cluster(not divided)0.2 2core clusters2coreclusters1cluster(not divided)Table I. MCL results for the structured Random Graph903We have executed computations at least 10times under the same condition in order to generatethis type of structured random graph with 500 ver-tices in the two layers respectively.
A randomgraph was produced by using a binominal distribu-tion.
Although between connection rates were var-ied from 0.1 to 0.2 and within connection rates forthe intermediate layer from 0.01 to 0.03, no edgeswere inserted into the lower layer.
MCL resultsobtained for this architecture are almost constant,as shown in Table I.In this experiment, all singleton clusters con-sisted of vertices belonging to layer 2.
In caseswhere the whole graph was split into 2 core clus-ters, one cluster would correspond to the hub pluslayer 3 while the other would correspond to layer 2.There was no exception when the between connec-tion rate was 0.2.
This means that, quite curiously,the hub formed a core cluster around itself withvertices that were not all adjacent to it, so that onesthat were connected with it in the raw data were allsegregated into the other cluster.
In this case, theModularity Q value for each core cluster was zeroor extremely low.Nevertheless, in spite of this inaccuracy, thistype of network can easily be by modified by theBMCL method that we discuss later.
It can beindirectly subdivided by graph clustering, if insidethe same cluster, a latent shortcut is set betweenone vertex and another.
Such a latent connectioncan be counted in place of a path of length 2 that istraced in the original adjacency as a detour via avertex of another cluster.
If all latent adjacencyrelationships are enumerated in this way, exceptfor those for the hub, the core cluster will be re-clustered by a second application of the MCL torealize a sort of hierarchical clustering (in this casefor a quasi-tree graph), which has been regarded asbeing a limitation with the MCL.This principle can be called Branching MarkovClustering (BMCL) in the sense that it makes itpossible to correct for unbalances in cluster-sizesby dividing large Markov clusters into appropriatebranches.
In other words, BMCL is a way of re-building adjacency relationships "inside" MCLclusters, by making reference to "outside" path in-formation.
It then becomes natural to realize thatthe lower the curvature value of the hub is?reflecting sparse connectivity inside the hub?s clus-ter?the more effective BMCL will be in subdivid-ing the core cluster, which will augment the modu-larity Q value for the clustering results.3 Applying BMCL corpora data3.1 The BMCL algorithmIn this section, we apply our BMCL method to asemantic network that is almost exhaustively ex-tracted from typical documents of a specific struc-ture.
It is supposed that if the MCL is applied toword association or co-occurrence data it will yieldconcept clusters where words are classified accord-ing to similar topics or similar meanings as para-digms.
However, because the word distribution ofa corpus approximately follows Zipf?s law andproduces a small-world scale-free network (Stey-vers et al, 2005), the MCL will result in a biaseddistribution of cluster sizes, with a few extraordi-narily large core clusters that lack any particularfeatures.In order to overcome such difficulties in build-ing appropriate lexical graphs for corpus data, wepropose an original way of appropriately subdivid-ing core clusters by taking into account graph coef-ficients, especially the curvature of a hub word.As mentioned above, BMCL is most effective forclusters that, containing a high-degree and low-curvature vertex, display a local part of a networkwith highly sparse connectivity when a hub iseliminated.
This feature increases the efficiency ofthe BMCL by making it possible to introducemoderate connection rates for latent adjacencies.In contrast to a ?real?
adjacency between the ver-tices ki, represented here by 1),( =kid , the ?latent?adjacency 1),( =jid v  will subsequently be definedto closely adapt to the connection state for thedataset, which we will utilize in testing the BMCL.The hub hM of each Markov cluster M is supposedto be the vertex with the largest degree for M .Here, we set a sufficiently large core cluster C , aset of hubs H and the hub of C as hC .Under suchconditions, we can formulize the set of externalhubs bypassing the intra-core connections jiK ,  as;}1),(),(,,|{ ,,, ==??
kjdkidKkHKK jijiji ,where Chh CjCijiji???
?,,, , HCh ?
.
We also propose anadditional function callednArgTopn , which identi-fies the set of n nodes that have the highest connec-904tion values.
This is to produce a moderate connec-tion rate which allows us to execute appropriateMCL operations by appropriately setting two prun-ing thresholds, ?p and ?q.
These are applied in therow direction by fixing i  in the intra-core connec-tion matrix to the number of the shortest paths be-tween ji, -- || , jiK -- to make the following prun-ing rule:1),(|)|&&|(| ,,=>???=jidKArgTopnjKifvjinpjiq?
?This rule extracts from the intra-core connec-tion matrix a latent adjacency matrix to which theMCL is applied once again in order to obtain ap-propriately resized sub-clusters from a huge corecluster.3.2 A range of corpus dataIn this section, three documents were selected tak-ing into consideration the curvature value of a hubwith the highest degree and the density of connec-tions with or without this hub among the verticesof a core cluster at the level of a raw data graph.I.
Associative Concept Dictionary of JapaneseWords (Ishizaki et al, 2001), hereafter abbreviatedas ACDJ, which consists of 33,018 words and240,093 word pairing collected in an associationtask involving 10 participants.
Of these, 9,373critical words were selected to create well-arrangedsemantic network by removing the rarest 1-degreedangling words and rarer words with a degree of 2but curvature values of 0.II.
Gakken?s Large Dictionary of Japanese (Kin-daichi & Ikeda, 1988), hereafter abbreviated asGLDJ, which is an authoritative Japanese diction-ary with some features of an encyclopedia in termsof its rich explanatory texts and copious examples.We selected 98,083 words after removing noisewords, functional words, and 1,321 isolated wordsto extract word pairs by combining every head-word with every other headword included withinan entry text.III.
WordNet.
We used only the "data.noun" filewhere the lexical information for each noun is de-fined by a set of index numbers corresponding notwith words themselves but with their senses.
Theco-occurrence relationships for 98,794 meaningswere extracted from every data block that containsa series of indexes, which also covers other parts-of-speech.The principle for building a semantic networkfor each of these documents was to select relevant?word pairs?
or ?index pairs?
indicating the lexicalrelationships of adjacency, association or co-occurrence, respectively.
Table II presents graphinformation for the three data sets and the resultsof applying both the MCL and the BMCL to them.Table II Data about the three corporaAlthough the first data (ACDJ) is much smaller,it is worthwhile executing because it represents aconcrete example of the network type discussedearlier, namely, a three-layer architecture around ahub (quasi-tree graph).
The connection rate in thecore cluster is very low (0.002 with and 0 withoutthe hub), as is the modularity Q value for the MCL(0.094).
However, subdivision of the core clusterin the BMCL results yielded a high modularity Qvalue (0.606) when latent adjacencies derived frombypassing connections with a threshold of q?
=3were used.The last two data (GLDJ and WordNet) are di-rectly comparable because they are quite similar insize and provide sharp contrast, particularly interms of curvature values (GLDJ: 8.51106E-05 <<WordNet: 0.0405), and modularity Q values for theMCL (GLDJ: 0.176 << WordNet: 0.841).
ForWordNet, the high connection rate in the core clus-ter (0.03) makes it difficult for it to be subdividedby any clustering method, even if the hub is elimi-nated.
In terms of the GLDJ, the core cluster wasrepeatedly divided by the BMCL and the modular-ity for the subdivision turned out to be 0.2214 witha threshold of p?
= 1.However, there is another way to split the corecluster into sub graphs, which does not require theuse of the latent adjacency information which iscrucial for the BMCL.
That other method, whichcan be called the ?Simply-Repeated MCL (SR-MCL)?, involves applying the MCL once again toACDJ GLDJ WordNetNum of Vertices 9373 98083 98794Degree Mean 19.963 13.8939 63.7155Hub Word House Archaic Words IndividualDegree of Hub 563 12959 2773Curvature of Hub 0.0398 8.51106E-05 0.0405Core Cluster Size 158 8962 2597Connection Rate of Core Cluster 0.0022 0.000328782 0.030539Ibid (Without Hub) 0 0.000153119 0.03005Q for the First MCL 0.0946409 0.176 0.841275Q for the BMCL 0.606284 0.221 -0.094905the part of the original adjacency matrix that corre-sponds to the vertices apart from the hub, andwhich become members of the core cluster as aresult of the first MCL.
In the case of ACDJ, it isimpossible to execute the SR-MCL, because thereis no edge that is not connected to the hub withinthe core cluster, and so all the vertices apart fromthe hub would be isolated if the hub were removed.A similar problem is also encountered with thecore cluster of the GLDJ, even though the SR-MCL increases the modularity Q value (0.769)much more than the BMCL.
Vertices that danglefrom the hub?37% of the core members?wouldbe dropped from the second MCL computation ifthe latent adjacency is not used, which, on theother hand, assures a high recall rate (0.88).
Thus,we have adopted an eclectic way to maintain bothrelatively high recall (the proportion of non-isolated nodes) and relatively high precision (themodularity Q of the intra-core clustering).
This iswhat we may call a ?Mixed BMCL?
which in-volves combining the latent adjacency matrix ex-clusively for the vertices dangling to the hub andthe raw adjacency part matrix for the remainingones that are connected among them.
As Figure Vhighlights, the F-measureRPPR??
+?
)1((R: recall; P:precision) underscores the effectiveness of theMixed BMCL for the GLDJ.Figure V Comparison of the methods (?
=0.4)4 ConclusionThis paper has examined MCL outputs obtainedfor some rather problematic conditions, such as theclustering of a tree graph and clustering for a net-work that contains a hub that has a very low curva-ture value.
In such cases, many of the vertices ad-jacent to the hub are removed from the cluster thatit represents.
However, compensating for that, thehub cluster will absorb many other vertices?someof which are not directly connected to the hub it-self?to form a large-sized core cluster.
That iswhen our proposed method of Branching MCL(BMCL) is most effective in adjusting cluster sizesby utilizing latent adjacency.
Subdivision of thecore cluster can facilitate the interpretation of theclassified concepts.When the curvature of the hub is a little higherthan in such extreme conditions, the combinationof the ordinary MCL and the BMCL (a MixedBMCL) can work well in increasing the F-Measurescore.
However, it is not possible to reapply theMCL to a dense core cluster that is organizedaround a hub with a very high curvature value.
Adirection for further research will be to automati-cally select from between the BMCL and theMixed-BMCL.
The SR-MCL or similar modifica-tions may yield the optimal approach to dividingmassive Markov clusters into appropriate subsets.ReferencesClauset, A, Newman M.E.J., and Moore, C. Find-ing Community Structure in Very Large Net-works, Phys.
Rev.
E 70, 066111 (2004)Danon, L., Diaz-Guilera, A., and Arenas, A. Effectof Size Heterogeneity on Community Identifica-tion in Complex Networks, J. Stat.
Mech.P11010 (2006)Dorow, B. et al Using Curvature and MarkovClustering in Graphs for Lexical Acquisition andWord Sence Discrimination, MEANING-2005,2nd Workshop organized by the MEAN-ING Project, February,3rd-4th.
(2005)Kindaichi, H., Ikeda, Y. Gakken?s Large Diction-ary of Japanese, GAKKEN CO, LTD. (1988)Newman M. E. J. and Girvan M., Finding andevaluating community structure in networks,Physical Review E 69.
026113, (2004)Okamoto, J., Ishizaki, S. Associative Concept Dic-tionary and its Comparison with Electronic Con-cept Dictionaries,http://afnlp.org/pacling2001/pdf/okamoto.pdf,(2001).Pujol, J.M., B?jar, J. and Delgado, J.
"ClusteringAlgorithm for Determining Community Struc-ture in Large Networks".Physical Review E 74(2007):016107Van Dongen, S. Graph Clustering by Flow Simula-tion.
PhD thesis, University of Utrecht.
(2000)00.20.40.60.811.2Recall Precision F-MeasureSR-MCLBMCLMixed BMCL906
