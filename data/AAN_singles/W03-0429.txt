Named Entity Recognition using Hundreds of Thousands of FeaturesJames Mayfield and Paul McNamee and Christine PiatkoThe Johns Hopkins University Applied Physics Laboratory11100 Johns Hopkins Road, Laurel, Maryland 20723-6099 USA{mayfield,mcnamee,piatko}@jhuapl.eduAbstractWe present an approach to named entity recog-nition that uses support vector machines to cap-ture transition probabilities in a lattice.
Thesupport vector machines are trained with hun-dreds of thousands of features drawn from theCoNLL-2003 Shared Task training data.
Mar-gin outputs are converted to estimated prob-abilities using a simple static function.
Per-formance is evaluated using the CoNLL-2003Shared Task test set; Test B results were F?=1= 84.67 for English, and F?=1 = 69.96 for Ger-man.1 IntroductionLanguage independence is difficult to achieve in namedentity recognition (NER) because different languages ap-pear to require different features.
Most NER systems (ortaggers) are severely limited in the number of featuresthey may consider, because the computational expense ofhandling large numbers of features is high, and becausethe risk of overtraining increases with the number of fea-tures.
Thus, the feature set must be finely tuned to beeffective.
Such constrained feature sets are naturally lan-guage dependent.Increasing the number of features that a tagger can han-dle would ameliorate this problem, because the designercould select many relatively simple features in lieu of afew highly tuned features.
Because support vector ma-chines (SVMs) (Vapnik, 1995) can handle large numbersof parameters efficiently while simultaneously limitingovertraining, they are good candidates for application tonamed entity recognition.
This paper proposes a novelway to use SVMs for named entity recognition calledSVM-Lattice, describes a large feature space that we usedon the CoNLL-2003 Shared Task (Tjong Kim Sang andDe Meulder, 2003), and presents results from that task.2 ModelWe are interested in a lattice-based approach to namedentity recognition.
In this approach, each sentence is pro-cessed individually.
A lattice is built with one columnper word of the sentence (plus a start state).
Each columncontains one vertex for each possible tag.
Each vertex inone column is connected by an edge to every vertex in thenext column that may legitimately follow it (some tran-sitions, such as from I-LOC to B-PER are disallowed).Given such a lattice, our task is first to assign probabili-ties to each of the arcs, then to find the highest likelihoodpath through the lattice based on those probabilities.
Thispath corresponds to the highest likelihood tagging of thesentence.Hidden Markov models break the probability calcula-tions into two pieces: transition probabilities (the proba-bility of moving from one vertex to another independentof the word at the destination node), and emission proba-bilities (the probability that a given word would be gener-ated from a certain state independent of the path taken toget to that state).
These probability distributions are cal-culated separately because the training data are typicallytoo sparse to support a reasonable maximum likelihoodestimate of the joint probability.
However, there is noreason that these two distributions could not be combinedgiven a suitable estimation technique.A support vector machine is a binary classifier that usessupervised training to predict whether a given vector is ina target class.
All SVM training and test data occupya single high-dimensional vector space.
In its simplestform, training an SVM amounts to finding the hyperplanethat separates the positive training samples from the neg-ative samples by the largest possible margin.
This hyper-plane is then used to classify the test vectors; those thatlie on one side of the hyperplane are classified as mem-bers of the positive class, while others are classified asmembers of the negative class.
In addition to the clas-sification decision, the SVM also produces a margin foreach vector?its distance from the hyperplane.SVMs have two useful properties for our purposes.First, they can handle very high dimensional spaces, aslong as individual vectors are sparse (i.e., each vector hasextent along only a small subset of the dimensions).
Sec-ondly, SVMs are resistant to overtraining, because onlythe training vectors that are closest to the hyperplane(called support vectors) dictate the parameters for the hy-perplane.
So SVMs would seem to be ideal candidatesfor estimating lattice probabilities.Unfortunately, SVMs do not produce probabilities, butrather margins.
In fact, one of the reasons that SVMswork so well is precisely because they do not attempt tomodel the entire distribution of training points.
To useSVMs in a lattice approach, then, a mechanism is neededto estimate probability of category membership given amargin.Platt (1999) suggests such a method.
If the range ofpossible margins is partitioned into bins, and positive andnegative training vectors are placed into these bins, eachbin will have a certain percentage of positive examples.These percentages can be approximated by a sigmoidfunction: P (y = 1 | f) = 1/(1 + exp(Ax + b)).
Plattgives a simple iterative method for estimating sigmoidparameters A and B, given a set of training vectors andtheir margins.This approach can work well if a sufficient number ofpositive training vectors are available.
Unfortunately, inthe CoNLL-2003 shared task, many of the possible labeltransitions have few exemplars.
Two methods are avail-able to handle insufficient training data: smoothing, andguessing.In the smoothing approach, linear interpolation is usedto combine the model for the source to target pair thatlacks sufficient data with the model made from a com-bination of all transitions going to the target label.
Forexample, we could smooth the probabilities derived forthe I-ORG to I-LOC transition with the probability thatany tag would transition to the I-LOC state at the samepoint in the sentence.The second approach is to guess at an appropriatemodel without examining the training data.
While in the-ory this could prove to be a terrible approach, in practicefor the Shared Task, selection of fixed sigmoid parame-ters works better than using Platt?s method to train theparameters.
Thus, we fix A = ?2 and b = 0.
We con-tinue to believe that Platt?s method or something like itwill ultimately lead to superior performance, but our cur-rent experiments use this untrained model.Our overall approach then is to use SVMs to estimatelattice transition probabilities.
First, due to the low fre-quency of B-XXX tags in the training data, we converteach B-XXX tags to the corresponding I-XXX tag; thus,our system never predicts B-XXX tags.
Then, we featur-ize the training data, forming sparse vectors suitable forinput to our SVM package, SVMLight 5.00 (Joachims,1999).
Our feature set is described in the following sec-tion.
Next, we train one SVM for each transition typeseen in the training data.
We used a cubic kernel for allof our experiments; this kernel gives a consistent boostover a linear kernel, while still training in a reasonableamount of time.
If we were to use Platt?s approach, the re-sulting classifiers would be applied to further (preferablyheld-out) training data to produce a set of margins, whichwould be used to estimate appropriate sigmoid parame-ters for each classifier.
Sigmoid estimates that sufferedfrom too few positive input vectors would be replacedby static estimates, and the sigmoids would optionally besmoothed.To evaluate a test set, the test input is featurized usingthe same features as were used with the training data, re-sulting in a separate vector for each word of the input.Each classifier built during the training phase is then ap-plied to each test vector to produce a margin.
The marginis mapped to a probability estimate using the static sig-moid described above.
When all of the probabilities havebeen estimated and applied to the lattice, a Viterbi-likealgorithm is used to find the most likely path through thelattice.
This path identifies the final tag for each word ofthe input sentence.3 FeaturesThe advantage of the ability to handle large numbers offeatures is that we do not need to consider how well afeature is likely to work in a particular language beforeproposing it.
We use the following features:1. the word itself, both unchanged and lower-cased;2. the character 3-grams and 4-grams that compose theword;3. the word?s capitalization pattern and digit pattern;4. the inverse of the word?s length;5. whether the word contains a dash;6. whether the word is inside double quote marks;7. the inverse of the word?s position in the sentence,and of the position of that sentence in the document;8. the POS, CHUNK and LEMMA features from thetraining data;9. whether the word is part of any entity, accordingto a previous application of the TnT-Subcat tagger(Brants, 2000) (see below) trained on the tag set {O,I-ENTITY} (Test A F?=1 performance was 94.70English and 74.33 German on this tag set); andRun Description Test LOC MISC ORG PER Overall1.
Tnt Test A 86.67 79.60 73.04 88.54 82.90Test B 81.28 68.98 65.71 82.84 75.542.
Tnt + subcat Test A 91.46 81.41 80.63 91.64 87.49Test B 85.71 68.41 73.82 87.95 80.683.
SVM-Lattice Test A 92.14 84.86 83.70 93.73 89.63Test B 87.09 72.81 78.84 90.40 83.924.
SVM-Lattice+ Test A 93.75 86.02 85.90 93.91 90.85Test B 88.77 74.19 79.00 90.67 84.67Table 1: English evaluation results.
F?=1 measures for subcategories, and overall.Run Description Test LOC MISC ORG PER Overall1.
Tnt Test A 59.51 49.58 48.71 53.77 53.29Test B 66.16 46.45 50.00 64.51 59.012.
Tnt + subcat Test A 67.62 54.97 56.18 65.04 61.46Test B 66.13 46.01 55.35 74.07 62.903.
SVM-Lattice Test A 67.04 54.18 65.77 64.01 63.48Test B 68.47 51.88 60.67 73.07 65.474.
SVM-Lattice+ Test A 72.58 58.13 65.76 74.92 68.72Test B 73.60 50.98 63.69 80.20 69.96Table 2: German evaluation results.
F?=1 measures for subcategories, and overall.10.
the maximum likelihood estimate, based on thetraining data, of the word?s prior probability of beingin each class.In some runs, we also use:11. the tag assigned by a previous application of theSVM-Lattice tagger, or by another tagger.Each of these features is applied not just to the wordbeing featurized, but also to a range of words on eitherside of it.
We typically use a range of three (or, phraseddifferently, a centered window of seven).
We also ap-plied some of these features to the environment of thefirst occurrence of the word in the document.
For ex-ample, if the first occurrence of ?Bush?
in the documentwere followed by ?League,?
then the second occurrenceof ?Bush?
would receive the feature ?first-occurrence-is-followed-by-league.
?Some values of the above features will be encounteredduring testing but not during training.
For example, aword that occurs in the test set but not the training set willlack a known value for the first feature in the list above.To handle these cases, we assign any feature that appearsonly once in the training data to a special ?never-before-seen?
class.
This gives us examples at training time ofunseen features, which we can then train on.Using the Shared Task English training data, this ap-proach to featurization leads to a feature space of wellover 600,000 features, while the German data results inover a million features.
Individual vectors typically haveextent along a few hundred of these features.There is a significant practical consideration in apply-ing the method.
The vectors produced by the featur-izer for input to the SVM package are voluminous, lead-ing to significant I/O costs, and slowing tag assignment.Two methods might ameliorate this problem.
First, sim-ple compression techniques would be quite effective inreducing file sizes, if the SVM package would supportthem.
Secondly, most vectors represent negative exam-ples; a portion of these could probably be eliminated en-tirely without significantly affecting system performance.We have done no tuning of our feature set, preferringto spend our time adding new features and relying on theSVMs to ignore useless features.
This is advantageouswhen applying the technique to a language that we donot understand (such as any of the world?s various non-English languages).4 ResultsWe evaluated our approach using the CoNLL-2003 En-glish and German training and test sets, and the conll-eval scoring software.
We ran two baseline tests usingThorsten Brants?
TnT tagger (2000), and two tests ofSVM-Lattice:1.
TnT: The TnT tagger applied as distributed.2.
TnT+subcat: The TnT tagger applied to a refinedtag set.
Each tag type was subcategorized into aboutforty subtag types; each instance of a tag in the textwas then replaced by the appropriate subtag.
For ex-ample, a number (e.g., 221) that was part of a loca-tion received an I-LOC-alldigits tag; a location withan initial capital letter (e.g., Baker) received an I-LOC-initcap tag; and one of the 30 most commonwords (e.g., of) that was part of a location received a(word-specific) I-LOC-of tag.
This run served bothto calibrate the SVM-Lattice performance scores,and to provide input for the SVM-Lattice+ run be-low.3.
SVM-Lattice: Features 1-10 (listed above in theFeatures section)4.
SVM-Lattice+: Features 1-11, using the output ofruns SVM-Lattice and TnT+subcat as input fea-tures.Scores for each English test are shown in Table 1; Ger-man tests are shown in Table 2.
Table 3 shows the re-sults of the SVM-Lattice+ run in more detail.
The resultsshow that the technique performs well, at least comparedwith the baseline technique provided with the CoNLL-2003 data (whose English Test B F?=1 measure is 59.61English and 30.30 German).5 ConclusionThe SVM-Lattice approach appears to give good resultswithout language-specific tuning; it handily outperformsthe CoNLL-2003 Shared Task baseline, and beats a basicHMM tagger as well.
Use of SVMs allows the introduc-tion of a large number of features.
These features canbe introduced with little concern for dependency amongfeatures, and without significant knowledge of the targetlanguage.
It is likely that our results reflect some degreeof overfitting, given the large number of parameters weuse; however, we suspect this effect is not large.
Thus,the SVM-Lattice technique is particularly well suited tolanguage-neutral entity recognition.
We expect it willalso perform well on other tasks that can be cast as tag-ging problems, such as part-of-speech tagging and syn-tactic chunking.AcknowledgmentsSignificant theoretical and implementation contributionswere made to this work by Claudia Pearce, for which weare grateful.We gratefully acknowledge the provision of theReuters Corpus Vol.
1: English language, 1996-08-20to 1997-08-19 by Reuters Limited.English devel.
Precision Recall F?=1LOC 94.42% 93.09% 93.75MISC 88.80% 83.41% 86.02ORG 85.24% 86.58% 85.90PER 92.79% 95.06% 93.91overall 90.97% 90.73% 90.85English test Precision Recall F?=1LOC 88.22% 89.33% 88.77MISC 74.89% 73.50% 74.19ORG 79.31% 78.69% 79.00PER 89.71% 91.65% 90.67overall 84.45% 84.90% 84.67German devel.
Precision Recall F?=1LOC 72.77% 72.40% 72.58MISC 71.00% 49.21% 58.13ORG 72.57% 60.11% 65.76PER 83.70% 67.81% 74.92overall 75.48% 63.07% 68.72German test Precision Recall F?=1LOC 75.08% 72.17% 73.60MISC 63.62% 42.54% 50.98ORG 69.20% 58.99% 63.69PER 86.53% 74.73% 80.20overall 75.97% 64.82% 69.96Table 3: Results for the development and test evaluationsfor the English and German tasks.ReferencesThorsten Brants.
2000.
TnT-A statistical part-of-speechtagger.
In Proceedings of ANLP-2000.
Seattle, Wash-ington.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In C. Burges B. Scho?lkopf andA.
Smola, editors, Support Vector Learning.
MITPress.John C. Platt.
1999.
Probabilistic Outputs for Sup-port Vector Machines and Comparisons to Regular-ized Likelihood Methods.
In B. Scholkopf A. Smola,P.
Bartlett and D. Schuurmans, editors, Advances inLarge Margin Classifiers.
MIT Press.Erik F. Tjong Kim Sang and Fien De Meulder.
2003.
In-troduction to the CoNLL-2003 Shared Task: LanguageIndependent Named Entity Recognition.
In Proceed-ings of CoNLL-2003.
Edmonton, Canada.Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer-Verlag.
