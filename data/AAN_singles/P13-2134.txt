Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 765?770,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsThe Effects of Lexical Resource Quality on Preference Violation DetectionJesse DunietzComputer Science DepartmentCarnegie Mellon UniversityPittsburgh, PA, 15213, USAjdunietz@cs.cmu.eduLori Levin and Jaime CarbonellLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA, 15213, USA{lsl,jgc}@cs.cmu.eduAbstractLexical resources such as WordNet andVerbNet are widely used in a multitudeof NLP tasks, as are annotated corporasuch as treebanks.
Often, the resourcesare used as-is, without question or exam-ination.
This practice risks missing sig-nificant performance gains and even entiretechniques.This paper addresses the importance ofresource quality through the lens of achallenging NLP task: detecting selec-tional preference violations.
We presentDAVID, a simple, lexical resource-basedpreference violation detector.
With as-is lexical resources, DAVID achieves anF1-measure of just 28.27%.
When theresource entries and parser outputs fora small sample are corrected, however,the F1-measure on that sample jumpsfrom 40% to 61.54%, and performanceon other examples rises, suggesting thatthe algorithm becomes practical given re-fined resources.
More broadly, this pa-per shows that resource quality matterstremendously, sometimes even more thanalgorithmic improvements.1 IntroductionA variety of NLP tasks have been addressedusing selectional preferences or restrictions, in-cluding word sense disambiguation (see Navigli(2009)), semantic parsing (e.g., Shi and Mihalcea(2005)), and metaphor processing (see Shutova(2010)).
These semantic problems are quite chal-lenging; metaphor analysis, for instance, has longbeen recognized as requiring considerable seman-tic knowledge (Wilks, 1978; Carbonell, 1980).The advent of extensive lexical resources, an-notated corpora, and a spectrum of NLP toolspresents an opportunity to revisit such challengesfrom the perspective of selectional preference vio-lations.
Detecting these violations, however, con-stitutes a severe stress-test for resources designedfor other tasks.
As such, it can highlight shortcom-ings and allow quantifying the potential benefits ofimproving resources such as WordNet (Fellbaum,1998) and VerbNet (Schuler, 2005).In this paper, we present DAVID (Detector ofArguments of Verbs with Incompatible Denota-tions), a resource-based system for detecting pref-erence violations.
DAVID is one component ofMETAL (Metaphor Extraction via Targeted Anal-ysis of Language), a new system for identifying,interpreting, and cataloguing metaphors.
One pur-pose of DAVID was to explore how far lexicalresource-based techniques can take us.
Thoughour initial results suggested that the answer is ?notvery,?
further analysis revealed that the problemlies less in the technique than in the state of exist-ing resources and tools.Often, it is assumed that the frontier of perfor-mance on NLP tasks is shaped entirely by algo-rithms.
Manning (2011) showed that this may nothold for POS tagging ?
that further improvementsmay require resource cleanup.
In the same spirit,we argue that for some semantic tasks, exemplifiedby preference violation detection, resource qual-ity may be at least as essential as algorithmic en-hancements.2 The Preference Violation DetectionTaskDAVID builds on the insight of Wilks (1978) thatthe strongest indicator of metaphoricity is the vi-olation of selectional preferences.
For example,only plants can literally be pruned.
If laws isthe object of pruned, the verb is likely metaphori-cal.
Flagging such semantic mismatches betweenverbs and arguments is the task of preference vio-lation detection.765We base our definition of preferences on thePragglejaz guidelines (Pragglejaz Group, 2007)for identifying the most basic sense of a word asthe most concrete, embodied, or precise one.
Sim-ilarly, we define selectional preferences as the se-mantic constraints imposed by a verb?s most basicsense.
Dictionaries may list figurative senses ofprune, but we take the basic sense to be cuttingplant growth.Several types of verbs were excluded from thetask because they have very lax preferences.
Theseinclude verbs of becoming or seeming (e.g., trans-form, appear), light verbs, auxiliaries, and aspec-tual verbs.
For the sake of simplifying implemen-tation, phrasal verbs were also ignored.3 Algorithm DesignTo identify violations, DAVID employs a simplealgorithm based on several existing tools and re-sources: SENNA (Collobert et al 2011), a seman-tic role labeling (SRL) system; VerbNet, a com-putational verb lexicon; SemLink (Loper et al2007), which includes mappings between Prop-Bank (Palmer et al 2005) and VerbNet; andWordNet.
As one metaphor detection componentof METAL?s several, DAVID is designed to favorprecision over recall.
The algorithm is as follows:1.
Run the Stanford CoreNLP POS tagger(Toutanova et al 2003) and the TurboParserdependency parser (Martins et al 2011).2.
Run SENNA to identify the semantic argu-ments of each verb in the sentence using thePropBank argument annotation scheme (Arg0,Arg1, etc.).
See Table 1 for example output.3.
For each verb V , find all VerbNet entries forV .
Using SemLink, map each PropBank argu-ment name to the corresponding VerbNet the-matic roles in these entries (Agent, Patient,etc.).
For example, the VerbNet class for pruneis carve-21.2-2.
SemLink maps Arg0 tothe Agent of carve-21.2-2 and Arg1 tothe Patient.4.
Retrieve from VerbNet the selectional restric-tions of each thematic role.
In our runningexample, VerbNet specifies +int controland +concrete for the Agent and Patient ofcarve-21.2-2, respectively.5.
If the head of any argument cannot be inter-preted to meet V ?s preferences, flag V as a vi-olation.
?The politician pruned laws regulating plasticbags, and created new fees for inspecting dairyfarms.
?Verb Arg0 Arg1pruned The politician laws .
.
.
bagsregulating laws plastic bagscreated The politician new feesinspecting - - dairy farmsTable 1: SENNA?s SRL output for the examplesentence above.
Though this example demon-strates only two arguments, SENNA is capable oflabeling up to six.Restriction WordNet Synsetsanimate animate being.n.01people.n.01person.n.01concrete physical object.n.01matter.n.01substance.n.01organization social group.n.01district.n.01Table 2: DAVID?s mappings between somecommon VerbNet restriction types and WordNetsynsets.Each VerbNet restriction is interpreted as man-dating or forbidding a set of WordNet hypernyms,defined by a custom mapping (see Table 2).For example, VerbNet requires both the Patientof a verb in carve-21.2-2 and the Themeof a verb in wipe manner-10.4.1-1 tobe concrete.
By empirical inspection, concretenouns are hyponyms of the WordNet synsetsphysical object.n.01, matter.n.03,or substance.n.04.
Laws (the Patient ofprune) is a hyponym of none of these, so prunewould be flagged as a violation.4 Corpus AnnotationTo evaluate our system, we assembled a corpusof 715 sentences from the METAL project?s cor-pus of sentences with and without metaphors.
Thecorpus was annotated by two annotators follow-ing an annotation manual.
Each verb was markedfor whether its arguments violated the selectionalpreferences of the most basic, literal meaning ofthe verb.
The annotators resolved conflicts by dis-766Error source FrequencyBad/missing VN entries 4.5 (14.1%)Bad/missing VN restrictions 6 (18.8%)Bad/missing SL mappings 2 (6.3%)Parsing/head-finding errors 3.5 (10.9%)SRL errors 8.5 (26.6%)VN restriction system too weak 4 (12.5%)Confounding WordNet senses 3.5 (10.9%)Endemic errors: 7.5 (23.4%)Resource errors: 12.5 (39.1%)Tool errors: 12 (37.5%)Total: 32 (100%)Table 3: Sources of error in 90 randomly selectedsentences.
For errors that were due to a combi-nation of sources, 1/2 point was awarded to eachsource.
(VN stands for VerbNet and SL for Sem-Link.
)cussing until consensus.5 Initial ResultsAs the first row of Table 4 shows, our initial eval-uation left little hope for the technique.
Withsuch low precision and F1, it seemed a lexicalresource-based preference violation detector wasout.
When we analyzed the errors in 90 randomlyselected sentences, however, we found that mostwere not due to systemic problems with the ap-proach; rather, they stemmed from SRL and pars-ing errors and missing or incorrect resource entries(see Table 3).
Armed with this information, we de-cided to explore how viable our algorithm wouldbe absent these problems.6 Refining The DataTo evaluate the effects of correcting DAVID?s in-puts, we manually corrected the tool outputs andresource entries that affected the aforementioned90 sentences.
SRL output was corrected for ev-ery sentence, while SemLink and VerbNet entrieswere corrected only for each verb that produced anerror.6.1 Corrections to Tool Output (Parser/SRL)Guided by the PropBank database and annotationguidelines, we corrected all errors in core roleassignments from SENNA.
These corrections in-cluded relabeling arguments, adding missed argu-ments, fixing argument spans, and deleting anno-tations for non-verbs.
The only parser-related er-ror we corrected was a mislabeled noun.6.2 Correcting Corrupted Data in VerbNetThe VerbNet download is missing several sub-classes that are referred to by SemLink or thathave been updated on the VerbNet website.
Someroles also have not been updated to the latest ver-sion, and some subclasses are listed with incor-rect IDs.
These problems, which caused SemLinkmappings to fail, were corrected before reviewingerrors from the corpus.Six subclasses needed to be fixed, all of whichwere easily detected by a simple script that did notdepend on the 90-sentence subcorpus.
We there-fore expect that few further changes of this typewould be needed for a more complete resource re-finement effort.6.3 Corpus-Based Updates to SemLinkOur modifications to SemLink?s mappings in-cluded adding missing verbs, adding missing rolesto mappings, and correcting mappings to more ap-propriate classes or roles.
We also added null map-pings in cases where a PropBank argument had nocorresponding role in VerbNet.
This makes thesystem?s strategy for ruling out mappings more re-liable.No corrections were made purely based on thesample.
Any time a verb?s mappings were edited,VerbNet was scoured for plausible mappings forevery verb sense in PropBank, and any nonsensi-cal mappings were deleted.
For example, whenthe phrase go dormant caused an error, we in-spected the mappings for go.
Arguments of all but2 of the 7 available mappings were edited, eitherto add missing arguments or to correct nonsensi-cal ones.
These changes actually had a net neg-ative impact on test set performance because thebad mappings had masked parsing and selectionalpreference problems.Based on the 90-sentence subcorpus, we mod-ified 20 of the existing verb entries in SemLink.These changes included correcting 8 role map-pings, adding 13 missing role mappings to existingsenses, deleting 2 incorrect senses, adding 11 verbsenses, correcting 2 senses, deleting 1 superfluousrole mapping, and adding 46 null role mappings.
(Note that although null mappings represented thelargest set of changes, they also had the least im-pact on system behavior.)
One entirely new verbwas added, as well.7676.4 Corpus-Based Updates to VerbNetNineteen VerbNet classes were modified, and oneclass had to be added.
The modifications gener-ally involved adding, correcting, or deleting se-lectional restrictions, often by introducing or re-arranging subclasses.
Other changes amounted tofixing clerical errors, such as incorrect role namesor restrictions that had been ANDed instead ofORed.An especially difficult problem was an inconsis-tency in the semantics of VerbNet?s subclass sys-tem.
In some cases, the restrictions specified ona verb in a subclass did not apply to subcatego-rization frames inherited from a superclass, but inother cases the restrictions clearly applied to allframes.
The conflict was resolved by duplicatingsubclassed verbs in the top-level class wheneverdifferent selectional restrictions were needed forthe two sets of frames.As with SemLink, samples determined onlywhich classes were modified, not what modifica-tions were made.
Any non-obvious changes toselectional restrictions were verified by examin-ing dozens of verb instances from SketchEngine?s(Kilgarriff et al 2004) corpus.
For example, theAgent of seek was restricted to +animate, butthe corpus confirmed that organizations are com-monly described non-metaphorically as seeking,so the restriction was updated to +animate |+organization.7 Results After Resource RefinementAfter making corrections for each set of 10 sen-tences, we incrementally recomputed F1 and pre-cision, both on the subcorpus corrected so far andon a test set of all 625 sentences that were nevercorrected.
(The manual nature of the correction ef-fort made testing k-fold subsets impractical.)
Theresults for 30-sentence increments are shown inTable 4.The most striking feature of these figures is howmuch performance improves on corrected sen-tences: for the full 90 sentences, F1 rose from30.43% to 61.54%, and precision rose even moredramatically from 31.82% to 80.00%.
Interest-ingly, resource corrections alone generally made alarger difference than tool corrections alone, sug-gesting that resources may be the dominant fac-tor in resource-intensive tasks such as this one.Even more compellingly, the improvement fromcorrecting both the tools and the resources wasnearly double the sum of the improvements fromeach alone: tool and resource improvements inter-act synergistically.The effects on the test corpus are harder tointerpret.
Due to a combination of SRL prob-lems and the small number of sentences cor-rected, the scores on the test set improved littlewith resource correction; in fact, they even dippedslightly between the 30- and 60-sentence incre-ments.
Nonetheless, we contend that our resultstestify to the generality of our corrections: aftereach iteration, every altered result was either anerror fixed or an error that should have appearedbefore but had been masked by another.
Note alsothat all results on the test set are without correctedtool output; presumably, these sentences wouldalso have improved synergistically with more ac-curate SRL.
How long corrections would continueto improve performance is a question that we didnot have the resources to answer, but our resultssuggest that there is plenty of room to go.Some errors, of course, are endemic to the ap-proach and cannot be fixed either by improved re-sources or by better tools.
For example, we con-sider every WordNet sense to be plausible, whichproduces false negatives.
Additionally, the selec-tional restrictions specified by VerbNet are fairlyloose; a more refined set of categories might cap-ture the range of verbs?
restrictions more accu-rately.8 Implications for Future RefinementEffortsAlthough improving resources is infamouslylabor-intensive, we believe that similarly refiningthe remainder of VerbNet and SemLink would bedoable.
In our study, it took about 25-35 person-hours to examine about 150 verbs and to mod-ify 20 VerbNet classes and 25 SemLink verb en-tries (excluding time for SENNA corrections, fix-ing corrupt VerbNet data, and analysis of DAVID?serrors).
Extrapolating from our experience, we es-timate that it would take roughly 6-8 person-weeksto systematically fix this particular set of issueswith VerbNet.Improving SemLink could be more complex,as its mappings are automatically generated fromVerbNet annotations on top of the PropBank cor-pus.
One possibility is to correct the generatedmappings directly, as we did in our study, whichwe estimate would take about two person-months.768With the addition of some metadata from the gen-eration process, it would then be possible to followthe corrected mappings back to annotations fromwhich they were generated and fix those annota-tions.
One downside of this approach is that if themappings were ever regenerated from the anno-tated corpus, any mappings not encountered in thecorpus would have to be added back afterwards.Null role mappings would be particularly thornyto implement.
To add a null mapping, we mustknow that a role definitely does not belong, andis not just incidentally missing from an exam-ple.
For instance, VerbNet?s defend-85 classtruly has no equivalent to Arg2 in PropBank?sdefend.01, but Arg0 or Arg1 may be missingfor other reasons (e.g., in a passive).
It may be bestto simply omit null mappings, as is currently done.Alternatively, full parses from the Penn Treebank,on which PropBank is based, might allow distin-guishing phenomena such as passives where argu-ments are predictably omitted.The maintainers of VerbNet and PropBank areaware of many of the issues we have raised, andwe have been in contact with them about possi-ble approaches to fixing them.
They are particu-larly aware of the inconsistent semantics of selec-tional restrictions on VerbNet subclasses, and theyhope to fix this issue within a larger attempt at re-tooling VerbNet?s selectional restrictions.
In themeantime, we are sharing our VerbNet modifica-tions with them for them to verify and incorporate.We are also sharing our SemLink changes so thatthey can, if they choose, continue manual correc-tion efforts or trace SemLink problems back to theannotated corpus.9 ConclusionOur results argue for investing effort in developingand fixing resources, in addition to developing bet-ter NLP tools.
Resource and tool improvementsinteract synergistically: better resources multiplythe effect of algorithm enhancements.
Gains fromfixing resources may sometimes even exceed whatthe best possible algorithmic improvements canprovide.
We hope the NLP community will takeup the challenge of investing in its resources to theextent that its tools demand.AcknowledgmentsThanks to Eric Nyberg for suggesting building asystem like DAVID, to Spencer Onuffer for his an-Sent.
Tools Rsrcs P F1715 0 0 27.14% 28.27%625 0 0 26.55% 27.98%625 0 corr.
26.37% 28.15%30 0 0 50.00% 40.00%30 30 0 66.67% 44.44%30 0 corr.+30 62.50% 50.00%30 30 corr.+30 87.50% 70.00%625 0 corr.+30 27.07% 28.82%60 0 0 35.71% 31.25%60 60 0 54.55% 31.38%60 0 corr.+60 53.85% 45.16%60 60 corr.+60 90.91% 68.97%625 0 corr.+60 26.92% 28.74%90 0 0 31.82% 30.43%90 90 0 44.44% 38.10%90 0 corr.+90 47.37% 41.86%90 90 corr.+90 80.00% 61.54%625 0 corr.+90 27.37% 28.99%Table 4: Performance on preference violation de-tection task.
Column 1 shows the sentence count.Columns 2 and 3 show how many sentences?SRL/parsing and resource errors, respectively, hadbeen fixed (?corr.?
indicates corrupted files).notation efforts, and to Davida Fromm for curatingMETAL?s corpus of Engish sentences.This work was supported by the IntelligenceAdvanced Research Projects Activity (IARPA)via Department of Defense US Army ResearchLaboratory contract number W911NF-12-C-0020.The U.S. Government is authorized to reproduceand distribute reprints for Governmental purposesnotwithstanding any copyright annotation thereon.Disclaimer: The views and conclusions containedherein are those of the authors and should not beinterpreted as necessarily representing the officialpolicies or endorsements, either expressed or im-plied, of IARPA, DoD/ARL, or the U.S. Govern-ment.ReferencesJaime G. Carbonell.
1980.
Metaphor: a key to ex-tensible semantic analysis.
In Proceedings of the18th annual meeting on Association for Computa-tional Linguistics, ACL ?80, pages 17?21, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Ronan Collobert, Jason Weston, Le?on Bottou, Michael769Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa.2011.
Natural language processing (almost) fromscratch.
J. Mach.
Learn.
Res., 12:2493?2537,November.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
Bradford Books.Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and DavidTugwell.
2004.
The Sketch Engine.
In Proceedingsof EURALEX.Edward Loper, Szu-ting Yi, and Martha Palmer.
2007.Combining lexical resources: Mapping betweenPropBank and VerbNet.
In Proceedings of the 7thInternational Workshop on Computational Linguis-tics, Tilburg, the Netherlands.Christopher D Manning.
2011.
Part-of-speech tag-ging from 97% to 100%: is it time for some linguis-tics?
In Computational Linguistics and IntelligentText Processing, pages 171?189.
Springer.Andre?
F. T. Martins, Noah A. Smith, Pedro M. Q.Aguiar, and Ma?rio A. T. Figueiredo.
2011.
Dual de-composition with many overlapping components.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?11,pages 238?249, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Roberto Navigli.
2009.
Word sense disambiguation: Asurvey.
ACM Computing Surveys (CSUR), 41(2):10.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?106.Pragglejaz Group.
2007.
MIP: A method for iden-tifying metaphorically used words in discourse.Metaphor and Symbol, 22(1):1?39.Karin K. Schuler.
2005.
VerbNet: A Broad-Coverage, Comprehensive Verb Lexicon.
Ph.D. the-sis, University of Pennsylvania, Philadelphia, PA.AAI3179808.Lei Shi and Rada Mihalcea.
2005.
Putting pieces to-gether: Combining FrameNet, VerbNet and Word-Net for robust semantic parsing.
In AlexanderGelbukh, editor, Computational Linguistics and In-telligent Text Processing, volume 3406 of Lec-ture Notes in Computer Science, pages 100?111.Springer Berlin Heidelberg.Ekaterina Shutova.
2010.
Models of metaphor in NLP.In Proceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics, ACL ?10,pages 688?697, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology- Volume 1, NAACL ?03, pages 173?180, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Yorick Wilks.
1978.
Making preferences more active.Artificial Intelligence, 11:197?223.770
