Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 687?696,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsKnowledge Graph Embedding via Dynamic Mapping MatrixGuoliang Ji, Shizhu He, Liheng Xu, Kang Liu and Jun ZhaoNational Laboratory of Pattern Recognition (NLPR)Institute of Automation Chinese Academy of Sciences, Beijing, 100190, China{guoliang.ji,shizhu.he,lhxu,kliu,jzhao}@nlpr.ia.ac.cnAbstractKnowledge graphs are useful resources fornumerous AI applications, but they are farfrom completeness.
Previous work such asTransE, TransH and TransR/CTransR re-gard a relation as translation from head en-tity to tail entity and the CTransR achievesstate-of-the-art performance.
In this pa-per, we propose a more fine-grained modelnamed TransD, which is an improvementof TransR/CTransR.
In TransD, we usetwo vectors to represent a named sym-bol object (entity and relation).
The firstone represents the meaning of a(n) entity(relation), the other one is used to con-struct mapping matrix dynamically.
Com-pared with TransR/CTransR, TransD notonly considers the diversity of relations,but also entities.
TransD has less param-eters and has no matrix-vector multipli-cation operations, which makes it can beapplied on large scale graphs.
In Experi-ments, we evaluate our model on two typ-ical tasks including triplets classificationand link prediction.
Evaluation resultsshow that our approach outperforms state-of-the-art methods.1 IntroductionKnowledge Graphs such as WordNet (Miller1995), Freebase (Bollacker et al 2008) and Yago(Suchanek et al 2007) have been playing a piv-otal role in many AI applications, such as relationextraction(RE), question answering(Q&A), etc.They usually contain huge amounts of structureddata as the form of triplets (head entity, relation,tail entity)(denoted as (h, r, t)), where relationmodels the relationship between the two entities.As most knowledge graphs have been built eithercollaboratively or (partly) automatically, they of-ten suffer from incompleteness.
Knowledge graphcompletion is to predict relations between entitiesbased on existing triplets in a knowledge graph.
Inthe past decade, much work based on symbol andlogic has been done for knowledge graph comple-tion, but they are neither tractable nor enough con-vergence for large scale knowledge graphs.
Re-cently, a powerful approach for this task is to en-code every element (entities and relations) of aknowledge graph into a low-dimensional embed-ding vector space.
These methods do reasoningover knowledge graphs through algebraic opera-tions (see section ?Related Work?
).Among these methods, TransE (Bordes et al2013) is simple and effective, and also achievesstate-of-the-art prediction performance.
It learnslow-dimensional embeddings for every entity andrelation in knowledge graphs.
These vector em-beddings are denoted by the same letter in bold-face.
The basic idea is that every relation is re-garded as translation in the embedding space.
Fora golden triplet (h, r, t), the embedding h is closeto the embedding t by adding the embedding r,that is h + r ?
t. TransE is suitable for 1-to-1relations, but has flaws when dealing with 1-to-N, N-to-1 and N-to-N relations.
TransH (Wanget al 2014) is proposed to solve these issues.TransH regards a relation as a translating oper-ation on a relation-specific hyperplane, which ischaracterized by a norm vector wrand a trans-lation vector dr.
The embeddings h and t arefirst projected to the hyperplane of relation r toobtain vectors h?= h ?
w>rhwrand t?=t ?
w>rtwr, and then h?+ dr?
t?.
Bothin TransE and TransH, the embeddings of enti-ties and relations are in the same space.
How-ever, entities and relations are different types ob-jects, it is insufficient to model them in the samespace.
TransR/CTransR (Lin et al 2015) set amapping matrix Mrand a vector r for every re-lation r. In TransR, h and t are projected to theaspects that relation r focuses on through the ma-687Entity Space Relation Spacer1h1t2h2t3h3tiim nrh p i pm nrt p i p?
?= += +M r h IM r t I??
( )1,2,3i = rrrr1?h2?h3?h1?t2?t3?tFigure 1: Simple illustration of TransD.
Eachshape represents an entity pair appearing in atriplet of relation r. Mrhand Mrtare mappingmatrices of h and t, respectively.
hip, tip(i =1, 2, 3), and rpare projection vectors.
hi?andti?
(i = 1, 2, 3) are projected vectors of entities.The projected vectors satisfy hi?+ r ?
ti?
(i =1, 2, 3).trix Mrand then Mrh + r ?
Mrt.
CTransR isan extension of TransR by clustering diverse head-tail entity pairs into groups and learning distinctrelation vectors for each group.
TransR/CTransRhas significant improvements compared with pre-vious state-of-the-art models.
However, it also hasseveral flaws: (1) For a typical relation r, all en-tities share the same mapping matrix Mr. How-ever, the entities linked by a relation always con-tains various types and attributes.
For example, intriplet (friedrich burklein, nationality, germany),friedrich burklein and germany are typical differ-ent types of entities.
These entities should be pro-jected in different ways; (2) The projection oper-ation is an interactive process between an entityand a relation, it is unreasonable that the map-ping matrices are determined only by relations;and (3) Matrix-vector multiplication makes it haslarge amount of calculation, and when relationnumber is large, it also has much more param-eters than TransE and TransH.
As the complex-ity, TransR/CTransR is difficult to apply on large-scale knowledge graphs.In this paper, we propose a novel method namedTransD to model knowledge graphs.
Figure 1shows the basic idea of TransD.
In TransD, we de-fine two vectors for each entity and relation.
Thefirst vector represents the meaning of an entity ora relation, the other one (called projection vector)represents the way that how to project a entity em-bedding into a relation vector space and it willbe used to construct mapping matrices.
There-fore, every entity-relation pair has an unique map-ping matrix.
In addition, TransD has no matrix-by-vector operations which can be replaced byvectors operations.
We evaluate TransD with thetask of triplets classification and link prediction.The experimental results show that our method hassignificant improvements compared with previousmodels.Our contributions in this paper are: (1)We pro-pose a novel model TransD, which constructs adynamic mapping matrix for each entity-relationpair by considering the diversity of entities and re-lations simultaneously.
It provides a flexible styleto project entity representations to relation vec-tor space; (2) Compared with TransR/CTransR,TransD has fewer parameters and has no matrix-vector multiplication.
It is easy to be appliedon large-scale knowledge graphs like TransE andTransH; and (3) In experiments, our approachoutperforms previous models including TransE,TransH and TransR/CTransR in link predictionand triplets classification tasks.2 Related WorkBefore proceeding, we define our mathematicalnotations.
We denote a triplet by (h, r, t) and theircolumn vectors by bold lower case letters h, r, t;matrices by bold upper case letters, such as M;tensors by bold upper case letters with a hat, suchas?M.
Score function is represented by fr(h, t).For a golden triplet (h, r, t) that corresponds to atrue fact in real world, it always get a relativelyhigher score, and lower for an negative triplet.Other notations will be described in the appropri-ate sections.2.1 TransE, TransH and TransR/CTransRAs mentioned in Introduction section, TransE(Bordes et al 2013) regards the relation r as trans-lation from h to t for a golden triplet (h, r, t).Hence, (h+r) is close to (t) and the score functionisfr(h, t) = ?
?h + r?
t?22.
(1)TransE is only suitable for 1-to-1 relations, thereremain flaws for 1-to-N, N-to-1 and N-to-N rela-tions.To solve these problems, TransH (Wang et al2014) proposes an improved model named trans-lation on a hyperplane.
On hyperplanes of differ-ent relations, a given entity has different represen-tations.
Similar to TransE, TransH has the scorefunction as follows:fr(h, t) = ?
?h?+ r?
t??22.
(2)688Model #Parameters # Operations (Time complexity)Unstructured (Bordes et al 2012; 2014) O(Nem) O(Nt)SE (Bordes et al 2011) O(Nem + 2Nrn2)(m = n) O(2m2Nt)SME(linear) (Bordes et al 2012; 2014) O(Nem + Nrn + 4mk + 4k)(m = n) O(4mkNt)SME (bilinear) (Bordes et al 2012; 2014) O(Nem + Nrn + 4mks + 4k)(m = n) O(4mksNt)LFM (Jenatton et al 2012; Sutskever et al 2009) O(Nem + Nrn2)(m = n) O((m2+ m)Nt)SLM (Socher et al 2013) O(Nem + Nr(2k + 2nk))(m = n) O((2mk + k)Nt)NTN (Socher et al 2013) O(Nem + Nr(n2s + 2ns + 2s))(m = n) O(((m2+ m)s + 2mk + k)Nt)TransE (Bordes et al 2013) O(Nem + Nrn)(m = n) O(Nt)TransH (Wang et al 2014) O(Nem + 2Nrn)(m = n) O(2mNt)TransR (Lin et al 2015) O(Nem + Nr(m + 1)n) O(2mnNt)CTransR (Lin et al 2015) O(Nem + Nr(m + d)n) O(2mnNt)TransD (this paper) O(2Nem + 2Nrn) O(2nNt)Table 1: Complexity (the number of parameters and the number of multiplication operations in an epoch)of several embedding models.
Neand Nrrepresent the number of entities and relations, respectively.Ntrepresents the number of triplets in a knowledge graph.
m is the dimension of entity embeddingspace and n is the dimension of relation embedding space.
d denotes the average number of clusters of arelation.
k is the number of hidden nodes of a neural network and s is the number of slice of a tensor.In order to ensure that h?and t?are on the hy-perplane of r, TransH restricts ?wr?
= 1.Both TransE and TransH assume that entitiesand relations are in the same vector space.
Butrelations and entities are different types of ob-jects, they should not be in the same vector space.TransR/CTransR (Lin et al 2015) is proposedbased on the idea.
TransR set a mapping matrixMrfor each relation r to map entity embeddinginto relation vector space.
Its score function is:fr(h, t) = ?
?Mrh + r?Mrt?22.
(3)where Mr?
Rm?n, h, t ?
Rnand r ?
Rm.CTransR is an extension of TransR.
As head-tailentity pairs present various patterns in different re-lations, CTransR clusters diverse head-tail entitypairs into groups and sets a relation vector for eachgroup.2.2 Other ModelsUnstructured.
Unstructured model (Bordes et al2012; 2014) ignores relations, only models entitiesas embeddings.
The score function isfr(h, t) = ??h?
t?22.
(4)It?s a simple case of TransE.
Obviously, Unstruc-tured model can not distinguish different relations.Structured Embedding (SE).
SE model (Bordeset al 2011) sets two separate matrices MrhandMrtto project head and tail entities for each rela-tion.
Its score function is defined as follows:fr(h, t) = ?
?Mrhh?Mrtt?1(5)Semantic Matching Energy (SME).
SME model(Bordes et al 2012; 2014) encodes each namedsymbolic object (entities and relations) as a vector.Its score function is a neural network that capturescorrelations between entities and relations via ma-trix operations.
Parameters of the neural networkare shared by all relations.
SME defines two se-mantic matching energy functions for optimiza-tion, a linear formg?= M?1e?+ M?2r + b?
(6)and a bilinear formg?= (M?1e?)?
(M?2r) + b?
(7)where ?
= {left, right}, eleft= h, eright= tand ?
is the Hadamard product.
The score func-tion isfr(h, t) = gleft>gright(8)In (Bordes et al2014), matrices of the bilinearform are replaced by tensors.Latent Factor Model (LFM).
LFM model (Je-natton et al 2012; Sutskever et al 2009) en-codes each entity into a vector and sets a ma-trix for every relation.
It defines a score functionfr(h, t) = h>Mrt, which incorporates the inter-action of the two entity vectors in a simple andeffecitve way.Single Layer Model (SLM).
SLM model is de-signed as a baseline of Neural Tensor Network(Socher et al 2013).
The model constructs a non-linear neural network to represent the score func-tion defined as follows.fr(h, t) = u>rf(Mr1h + Mr2t + br) (9)where Mr1, Mr2and brare parameters indexedby relation r, f() is tanh operation.689Neural Tensor Network (NTN).
NTN model(Socher et al 2013) extends SLM model by con-sidering the second-order correlations into nonlin-ear neural networks.
The score function isfr(h, t) = u>rf(h>?Wrt + Mr[ht]+ br) (10)where?Wrrepresents a 3-way tensor, Mrdenotesthe weight matrix, bris the bias and f() is tanhoperation.
NTN is the most expressive model sofar, but it has so many parameters that it is difficultto scale up to large knowledge graphs.Table 1 lists the complexity of all the abovemodels.
The complexity (especially for time) ofTransD is much less than TransR/CTransR and issimilar to TransE and TransH.
Therefore, TransDis effective and train faster than TransR/CTransR.Beyond these embedding models, there is other re-lated work of modeling multi-relational data, suchas matrix factorization, recommendations, etc.
Inexperiments, we refer to the results of RESCALpresented in (Lin et al 2015) and compare with it.3 Our MethodWe first define notations.
Triplets are representedas (hi, ri, ti)(i = 1, 2, .
.
.
, nt), where hidenotesa head entity, tidenotes a tail entity and ride-notes a relation.
Their embeddings are denoted byhi, ri, ti(i = 1, 2, .
.
.
, nt).
We use ?
to representgolden triplets set, and use ?
?to denote negativetriplets set.
Entities set and relations set are de-noted by E and R, respectively.
We use Im?ntodenote the identity matrix of size m?
n.3.1 Multiple Types of Entities and RelationsConsidering the diversity of relations, CTransRsegments triplets of a specific relation r intoseveral groups and learns a vector representa-tion for each group.
However, entities alsohave various types.
Figure 2 shows severalkinds of head and tail entities of relation loca-tion.location.partially containedby in FB15k.
Inboth TransH and TransR/CTransR, all types of en-tities share the same mapping vectors/matrices.However, different types of entities have differ-ent attributes and functions, it is insufficient to letthem share the same transform parameters of a re-lation.
And for a given relation, similar entitiesshould have similar mapping matrices and other-wise for dissimilar entities.
Furthermore, the map-ping process is a transaction between entities andrelations that both have various types.
Therefore,we propose a more fine-grained model TransD,which considers different types of both entitiesand relations, to encode knowledge graphs intoembedding vectors via dynamic mapping matricesproduced by projection vectors.Figure 2: Multiple types of entities of relation lo-cation.location.partially containedby.3.2 TransDModel In TransD, each named symbol object (en-tities and relations) is represented by two vectors.The first one captures the meaning of entity (rela-tion), the other one is used to construct mappingmatrices.
For example, given a triplet (h, r, t),its vectors are h,hp, r, rp, t, tp, where subscriptp marks the projection vectors, h,hp, t, tp?
Rnand r, rp?
Rm.
For each triplet (h, r, t), weset two mapping matrices Mrh,Mrt?
Rm?ntoproject entities from entity space to relation space.They are defined as follows:Mrh= rph>p+ Im?n(11)Mrt= rpt>p+ Im?n(12)Therefore, the mapping matrices are determinedby both entities and relations, and this kind ofoperation makes the two projection vectors inter-act sufficiently because each element of them canmeet every entry comes from another vector.
Aswe initialize each mapping matrix with an identitymatrix, we add the Im?nto Mrhand Mrh.
Withthe mapping matrices, we define the projected vec-tors as follows:h?= Mrhh, t?= Mrtt (13)690Then the score function isfr(h, t) = ?
?h?+ r?
t?
?22(14)In experiments, we enforce constrains as ?h?2?1, ?t?2?
1, ?r?2?
1, ?h??2?
1 and ?t?
?2?1.Training Objective We assume that there arenttriplets in training set and denote the ith tripletby (hi, ri, ti)(i = 1, 2, .
.
.
, nt).
Each triplet has alabel yito indicate the triplet is positive (yi= 1)or negative (yi= 0).
Then the golden and neg-ative triplets are denoted by ?
= {(hj, rj, tj) |yj= 1} and ?
?= {(hj, rj, tj) | yj= 0}, respec-tively.
Before training, one important trouble isthat knowledge graphs only encode positive train-ing triplets, they do not contain negative examples.Therefore, we obtain ?
from knowledge graphsand generate ?
?as follows: ?
?= {(hl, rk, tk) |hl6= hk?
yk= 1}?
{(hk, rk, tl) | tl6= tk?
yk=1}.
We also use two strategies ?unif?
and ?bern?described in (Wang et al 2014) to replace the heador tail entity.Let us use ?
and ?
?to denote a golden tripletand a corresponding negative triplet, respectively.Then we define the following margin-based rank-ing loss as the objective for training:L =??????????[?
+ fr(??)?
fr(?
)]+(15)where [x]+, max (0, x), and ?
is the margin sep-arating golden triplets and negative triplets.
Theprocess of minimizing the above objective is car-ried out with stochastic gradient descent (SGD).In order to speed up the convergence and avoidoverfitting, we initiate the entity and relation em-beddings with the results of TransE and initiate allthe transfer matrices with identity matrices.3.3 Connections with TransE, TransH andTransR/CTransRTransE is a special case of TransD when the di-mension of vectors satisfies m = n and all projec-tion vectors are set zero.TransH is related to TransD when we set m =n.
Under the setting, projected vectors of entitiescan be rewritten as follows:h?= Mrhh = h + h>phrp(16)t?= Mrtt = t + t>ptrp(17)Hence, when m = n, the difference betweenTransD and TransH is that projection vectors aredeterminded only by relations in TransH, butTransD?s projection vectors are determinded byboth entities and relations.As to TransR/CTransR, TransD is an improve-ment of it.
TransR/CTransR directly defines amapping matrix for each relation, TransD con-sturcts two mapping matrices dynamically foreach triplet by setting a projection vector for eachentity and relation.
In addition, TransD has nomatrix-vector multiplication operation which canbe replaced by vector operations.
Without loss ofgenerality, we assume m ?
n, the projected vec-tors can be computed as follows:h?= Mrhh = h>phrp+[h>,0>]>(18)t?= Mrtt = t>ptrp+[t>,0>]>(19)Therefore, TransD has less calculation thanTransR/CTransR, which makes it train faster andcan be applied on large-scale knowledge graphs.4 Experiments and Results AnalysisWe evaluate our apporach on two tasks: tripletsclassification and link prediction.
Then we showthe experiments results and some analysis of them.4.1 Data SetsTriplets classification and link prediction are im-plemented on two popular knowledge graphs:WordNet (Miller 1995) and Freebase (Bollackeret al 2008).
WordNet is a large lexical knowledgegraph.
Entities in WordNet are synonyms whichexpress distinct concepts.
Relations in WordNetare conceptual-semantic and lexical relations.
Inthis paper, we use two subsets of WordNet: WN11(Socher et al 2013) and WN18 (Bordes et al2014).
Freebase is a large collaborative knowl-edge base consists of a large number of the worldfacts, such as triplets (anthony asquith, location,london) and (nobuko otowa, profession, actor).We also use two subsets of Freebase: FB15k (Bor-des et al 2014) and FB13 (Socher et al 2013).Table 2 lists statistics of the 4 datasets.Dataset #Rel #Ent #Train #Valid #TestWN11 11 38,696 112,581 2,609 10,544WN18 18 40,943 141,442 5,000 5,000FB13 13 75,043 316,232 5908 23,733FB15k 1,345 14,951 483,142 50,000 59,071Table 2: Datesets used in the experiments.6914.2 Triplets ClassificationTriplets classification aims to judge whether agiven triplet (h, r, t) is correct or not, which is abinary classification task.
Previous work (Socheret al 2013; Wang et al 2014; Lin et al 2015)had explored this task.
In this paper ,we use threedatasets WN11, FB13 and FB15k to evaluate ourapproach.
The test sets of WN11 and FB13 pro-vided by (Socher et al 2013) contain golden andnegative triplets.
As to FB15k, its test set onlycontains correct triplets, which requires us to con-struct negative triplets.
In this parper, we constructnegative triplets following the same setting usedfor FB13 (Socher et al 2013).For triplets classification, we set a threshold ?rfor each relation r. ?ris obtained by maximizingthe classification accuracies on the valid set.
For agiven triplet (h, r, t), if its score is larger than ?r,it will be classified as positive, otherwise negative.We compare our model with several previousembedding models presented in Related Work sec-tion.
As we construct negative triplets for FB15kby ourselves, we use the codes of TransE, TransHand TransR/CTransR provied by (Lin et al 2015)to evaluate the datasets instead of reporting the re-sults of (Wang et al2014; Lin et al 2015) directly.In this experiment, we optimize the objectivewith ADADELTA SGD (Zeiler 2012).
We selectthe margin ?
among {1, 2, 5, 10}, the dimen-sion of entity vectors m and the dimension of re-lation vectors n among {20, 50, 80, 100}, andthe mini-batch size B among {100, 200, 1000,4800}.
The best configuration obtained by validset are:?
= 1,m, n = 100, B = 1000 and tak-ing L2as dissimilarity on WN11; ?
= 1,m, n =100, B = 200 and taking L2as dissimilarity onFB13; ?
= 2,m, n = 100, B = 4800 and tak-ing L1as dissimilarity on FB15k.
For all thethree datasets, We traverse to training for 1000rounds.
As described in Related Work section,TransD trains much faster than TransR (On ourPC, TransR needs 70 seconds and TransD merelyspends 24 seconds a round on FB15k).Table 3 shows the evaluation results of tripletsclassification.
On WN11, we found that there are570 entities appearing in valid and test sets butnot appearing in train set, we call them ?NULLEntity?.
In valid and test sets, there are 1680(6.4%) triplets containing ?NULL Entity?.
InNTN(+E), these entity embeddings can be ob-tained by word embedding.
In TransD, how-Data sets WN11 FB13 FB15KSE 53.0 75.2 -SME(bilinear) 70.0 63.7 -SLM 69.9 85.3 -LFM 73.8 84.3 -NTN 70.4 87.1 68.2NTN(+E) 86.2 90.0 -TransE(unif) 75.9 70.9 77.3TransE(bern) 75.9 81.5 79.8TransH(unif) 77.7 76.5 74.2TransH(bern) 78.8 83.3 79.9TransR(unif) 85.5 74.7 81.1TransR(bern) 85.9 82.5 82.1CTransR(bern) 85.7 - 84.3TransD(unif) 85.6 85.9 86.4TransD(bern) 86.4 89.1 88.0Table 3: Experimental results of Triplets Classifi-cation(%).
?+E?
means that the results are com-bined with word embedding.ever, they are only initialized randomly.
There-fore, it is not fair for TransD, but we also achievethe accuracy 86.4% which is higher than that ofNTN(+E) (86.2%).
From Table 3, we can con-clude that: (1) On WN11, TransD outperforms anyother previous models including TransE, TransHand TransR/CTransR, especially NTN(+E); (2)On FB13, the classification accuracy of TransDachieves 89.1%, which is significantly higher thanthat of TransE, TransH and TransR/CTransR andis near to the performance of NTN(+E) (90.0%);and (3) Under most circumstances, the ?bern?sampling method works better than ?unif?.Figure 3 shows the prediction accuracy of dif-ferent relations.
On the three datasets, differentrelations have different prediction accuracy: someare higher and the others are lower.
Here we fo-cus on the relations which have lower accuracy.On WN11, the relation similar to obtains accuracy51%, which is near to random prediction accuracy.In the view of intuition, similar to can be inferredfrom other information.
However, the number ofentity pairs linked by relation similar to is only1672, which accounts for 1.5% in all train data,and prediction of the relation needs much infor-mation about entities.
Therefore, the insufficientof train data is the main cause.
On FB13, theaccuracies of relations cuase of death and genderare lower than that of other relations because theyare difficult to infer from other imformation, espe-cially cuase of death.
Relation gender may be in-ferred from a person?s name (Socher et al 2013),but we learn a vector for each name, not for thewords included in the names, which makes the69250 60 70 80 90 100has_instancesimilar_tomember_meronymdomain_regionsubordinate_instance_ofdomain_topicmember_holonymsynset_domain_topichas_partpart_oftype_ofAccuracy(%)WN11unifbern50 60 70 80 90 100cause_of_deathgenderprofessionreligionnationalityinstitutionethnicityAccuracy(%)FB13unifbern50 60 70 80 90 1004550556065707580859095100Accuracy(%) of "bern"Accuracy(%)of "unif"FB15KFigure 3: Classification accuracies of different relations on the three datasets.
For FB15k, each trianglerepresent a relation, in which the red triangles represent the relations whose accuracies of ?bern?
or?unif?
are lower than 50% and the blacks are higher than 50%.
The red line represents the functiony = x.
We can see that the most relations are in the lower part of the red line.names information useless for gender.
On FB15k,accuracies of some relations are lower than 50%,for which some are lack of train data and some aredifficult to infer.
Hence, the ability of reasoningnew facts based on knowledge graphs is under acertain limitation, and a complementary approachis to extract facts from plain texts.4.3 Link PredictionLink prediction is to predict the missing h or t fora golden triplet (h, r, t).
In this task, we removethe head or tail entity and then replace it with allthe entities in dictionary in turn for each triplet intest set.
We first compute scores of those corruptedtriplets and then rank them by descending order;the rank of the correct entity is finally stored.
Thetask emphasizes the rank of the correct entity in-stead of only finding the best one entity.
Simi-lar to (Bordes et al 2013), we report two mea-sures as our evaluation metrics: the average rankof all correct entites (Mean Rank) and the propor-tion of correct entities ranked in top 10 (Hits@10).A lower Mean Rank and a higher Hits@10 shouldbe achieved by a good embedding model.
We callthe evaluation setting ?Raw?.
Noting the fact thata corrupted triplet may also exist in knowledgegraphs, the corrupted triplet should be regard asa correct triplet.
Hence, we should remove thecorrupted triplets included in train, valid and testsets before ranking.
We call this evaluation setting?Filter?.
In this paper, we will report evaluationresults of the two settings .In this task, we use two datasets: WN18 andFB15k.
As all the data sets are the same, werefer to their experimental results in this paper.On WN18, we also use ADADELTA SGD (Zeiler2012) for optimization.
We select the margin ?among {0.1, 0.5, 1, 2}, the dimension of entityvectors m and the dimension of relation vectors namong {20, 50, 80, 100}, and the mini-batch sizeB among {100, 200, 1000, 1400}.
The best con-figuration obtained by valid set are:?
= 1,m, n =50, B = 200 and taking L2as dissimilarity.
Forboth the two datasets, We traverse to training for1000 rounds.Experimental results on both WN18 and FB15kare shown in Table 4.
From Table 4, we canconclude that: (1) TransD outperforms otherbaseline embedding models (TransE, TransH andTransR/CTransR), especially on sparse dataset,i.e., FB15k; (2) Compared with CTransR, TransDis a more fine-grained model which considers themultiple types of entities and relations simultane-ously, and it achieves a better performance.
It in-dicates that TransD handles complicated internalcorrelations of entities and relations in knowledgegraphs better than CTransR; (3) The ?bern?
sam-pling trick can reduce false negative labels than?unif?.For the comparison of Hits@10 of differentkinds of relations, Table 5 shows the detailedresults by mapping properties of relations1onFB15k.
From Table 5, we can see that TransDoutperforms TransE, TransH and TransR/CTransRsignificantly in both ?unif?
and ?bern?
settings.TransD achieves better performance than CTransRin all types of relations (1-to-1, 1-to-N, N-to-1 andN-to-N).
For N-to-N relations in predicting bothhead and tail, our approach improves the Hits@10by almost 7.4% than CTransR.
In particular, for1Mapping properties of relations follows the same rules in(Bordes et al 2013)693Data sets WN18 FB15KMetricMean Rank Hits@10 Mean Rank Hits@10Raw Filt Raw Filt Raw Filt Raw FiltUnstructured (Bordes et al 2012) 315 304 35.3 38.2 1,074 979 4.5 6.3RESCAL (Nickle, Tresp, and Kriegel 2011) 1,180 1,163 37.2 52.8 828 683 28.4 44.1SE (Bordes et al 2011) 1,011 985 68.5 80.5 273 162 28.8 39.8SME (linear) (Bordes et al2012) 545 533 65.1 74.1 274 154 30.7 40.8SME (Bilinear) (Bordes et al 2012) 526 509 54.7 61.3 284 158 31.3 41.3LFM (Jenatton et al 2012) 469 456 71.4 81.6 283 164 26.0 33.1TransE (Bordes et al 2013) 263 251 75.4 89.2 243 125 34.9 47.1TransH (unif) (Wang et al 2014) 318 303 75.4 86.7 211 84 42.5 58.5TransH (bern) (Wang et al 2014) 401 388 73.0 82.3 212 87 45.7 64.4TransR (unif) (Lin et al 2015) 232 219 78.3 91.7 226 78 43.8 65.5TransR (bern) (Lin et al 2015) 238 225 79.8 92.0 198 77 48.2 68.7CTransR (unif) (Lin et al 2015) 243 230 78.9 92.3 233 82 44.0 66.3CTransR (bern) (Lin et al 2015) 231 218 79.4 92.3 199 75 48.4 70.2TransD (unif) 242 229 79.2 92.5 211 67 49.4 74.2TransD (bern) 224 212 79.6 92.2 194 91 53.4 77.3Table 4: Experimental results on link prediction.Tasks Prediction Head (Hits@10) Prediction Tail (Hits@10)Relation Category 1-to-1 1-to-N N-to-1 N-to-N 1-to-1 1-to-N N-to-1 N-to-NUnstructured (Bordes et al 2012) 34.5 2.5 6.1 6.6 34.3 4.2 1.9 6.6SE (Bordes et al 2011) 35.6 62.6 17.2 37.5 34.9 14.6 68.3 41.3SME (linear) (Bordes et al2012) 35.1 53.7 19.0 40.3 32.7 14.9 61.6 43.3SME (Bilinear) (Bordes et al 2012) 30.9 69.6 19.9 38.6 28.2 13.1 76.0 41.8TransE (Bordes et al 2013) 43.7 65.7 18.2 47.2 43.7 19.7 66.7 50.0TransH (unif) (Wang et al 2014) 66.7 81.7 30.2 57.4 63.7 30.1 83.2 60.8TransH (bern) (Wang et al 2014) 66.8 87.6 28.7 64.5 65.5 39.8 83.3 67.2TransR (unif) (Lin et al 2015) 76.9 77.9 38.1 66.9 76.2 38.4 76.2 69.1TransR (bern) (Lin et al 2015) 78.8 89.2 34.1 69.2 79.2 37.4 90.4 72.1CTransR (unif) (Lin et al 2015) 78.6 77.8 36.4 68.0 77.4 37.8 78.0 70.3CTransR (bern) (Lin et al 2015) 81.5 89.0 34.7 71.2 80.8 38.6 90.1 73.8TransD (unif) 80.7 85.8 47.1 75.6 80.0 54.5 80.7 77.9TransD (bern) 86.1 95.5 39.8 78.5 85.4 50.6 94.4 81.2Table 5: Experimental results on FB15K by mapping properities of relations (%).N-to-1 relations (predicting head) and 1-to-N rela-tions (predicting tail), TransD improves the accu-racy by 9.0% and 14.7% compared with previousstate-of-the-art results, respectively.
Therefore,the diversity of entities and relations in knowl-edge grahps is an important factor and the dynamicmapping matrix is suitable for modeling knowl-edge graphs.5 Properties of Projection VectorsAs mentioned in Section ?Introduction?, TransDis based on the motivation that each mapping ma-trix is determined by entity-relation pair dynam-ically.
These mapping matrices are constructedwith projection vectors of entities and relations.Here, we analysis the properties of projection vec-tors.
We seek the similar objects (entities and rela-tions) for a given object (entities and relations) byprojection vectors.
As WN18 has the most enti-ties (40,943 entities which contains various typesof words.
FB13 also has many entities, but themost are person?s names) and FB15k has the mostrelations (1,345 relations), we show the similarityof projection vectors on them.
Table 6 and 7 showthat the same category objects have similar projec-tion vectors.
The similarity of projection vectorsof different types of entities and relations indicatesthe rationality of our method.6 Conclusions and Future WorkWe introduced a model TransD that embed knowl-edge graphs into continues vector space for theircompletion.
TransD has less complexity and moreflexibility than TransR/CTransR.
When learningembeddings of named symbol objects (entities orrelations), TransD considers the diversity of themboth.
Extensive experiments show that TransDoutperforms TrasnE, TransH and TransR/CTransRon two tasks including triplets classification andlink prediction.As shown in Triplets Classification section, notall new facts can be deduced from the exist-694Datesets WN18Entities and Definitions upset VB 4 cause to overturn from an upright ornormal positionsrbija NN 1 a historical region in central andnorthern YugoslaviaSimilar Entities andDefinitionssway VB 4 cause to move back and forth montenegro NN 1 a former country bordering on theAdriatic Seashift VB 2 change place or direction constantina NN 1 a Romanian resort city on the BlackSeaflap VB 3 move with a thrashing motion lappland NN 1 a region in northmost Europe inhab-ited by Lappsfluctuate VB 1 cause to fluctuate or move in a wave-like patternplattensee NN 1 a large shallow lake in western Hun-garyleaner NN 1 (horseshoes) the throw of a horse-shoe so as to lean against (but not en-circle) the stakebrasov NN 1 a city in central Romania in thefoothills of the Transylvanian AlpsTable 6: Entity projection vectors similarity (in descending order) computed on WN18.
The similarityscores are computed with cosine function.Datesets FB15kRelation /location/statistical region/rent50 2./measurement unit/dated money value/currencySimilar relations/location/statistical region/rent50 3./measurement unit/dated money value/currency/location/statistical region/rent50 1./measurement unit/dated money value/currency/location/statistical region/rent50 4./measurement unit/dated money value/currency/location/statistical region/rent50 0./measurement unit/dated money value/currency/location/statistical region/gdp nominal./measurement unit/dated money value/currencyRelation /sports/sports team/roster./soccer/football roster position/playerSimilar relations/soccer/football team/current roster./sports/sports team roster/player/soccer/football team/current roster./soccer/football roster position/player/sports/sports team/roster./sports/sports team roster/player/basketball/basketball team/historical roster./sports/sports team roster/player/sports/sports team/roster./basketball/basketball historical roster position/playerTable 7: Relation projection vectors similarity computed on FB15k.
The similarity scores are computedwith cosine function.ing triplets in knowledge graphs, such as rela-tions gender, place of place, parents and chil-dren.
These relations are difficult to infer from allother information, but they are also useful resourcefor practical applications and incomplete, i.e.
theplace of birth attribute is missing for 71% of allpeople included in FreeBase (Nickel, et al 2015).One possible way to obtain these new triplets isto extract facts from plain texts.
We will seekmethods to complete knowledge graphs with newtriplets whose entities and relations come fromplain texts.AcknowledgmentsThis work was supported by the National BasicResearch Program of China (No.
2014CB340503)and the National Natural Science Foundation ofChina (No.
61272332 and No.
61202329).ReferencesGeorge A. Miller.
1995.
WordNet: A lexicaldatabase for english.
Communications of the ACM,38(11):39-41.Bollacker K., Evans C., Paritosh P., Sturge T., and Tay-lor J.
2008.
Freebase: A collaboratively createdgraph database for structuring human knowledge.In Proceedings of the 2008 ACM SIGMOD Inter-national Conference on Management of Data.
pages1247-1250.Fabian M. Suchanek, Kasneci G., Weikum G. 2007.YAGO: A core of semantic Knowledge UnifyingWordNet and Wikipedia.
In Proceedings of the 16thinternational conference on World Wide Web.Bordes A., Usunier N., Garcia-Dur?an A.
2013.
Trans-lating Embeddings for Modeling Multi-relationalData.
In Proceedings of NIPS.
pags:2787-2795.Wang Z., Zhang J., Feng J. and Chen Z.
2014.Knowledge graph embedding by translating on hy-perplanes.
In Proceedings of AAAI.
pags:1112-1119.Lin Y., Zhang J., Liu Z., Sun M., Liu Y., Zhu X.2015.
Learning Entity and Relation Embeddings forKnowledge Graph Completion.
In Proceedings ofAAAI.Bordes A., Glorot X., Weston J., and Bengio Y.
2012.Joint learning of words and meaning representationsfor open-text semantic parsing.
In Proceedings ofAISTATS.
pags:127-135.Bordes A., Glorot X., Weston J., and Bengio Y.2014.
A semantic matching energy function for lear-ing with multirelational data.
Machine Learning.94(2):pags:233-259.Bordes A., Weston J., Collobert R., and Bengio Y.2011.
Learning structured embeddings of knowl-edge bases.
In Proceedings of AAAI.
pags:301-306.695Jenatton R., Nicolas L. Roux, Bordes A., and Oboz-inaki G. 2012.
A latent factor model for highlymulti-relational data.
In Proceedings of NIPS.pags:3167-3175.Sutskever I., Salakhutdinov R. and Joshua B. Tenen-baum.
2009.
Modeling Relational Data usingBayesian Clustered Tensor Factorization.
In Pro-ceedings of NIPS.
pags:1821-1828.Socher R., Chen D., Christopher D. Manning and An-drew Y. Ng.
2013.
Reasoning With Neural TensorNetworks for Knowledge Base Completion.
In Pro-ceedings of NIPS.
pags:926-934.Weston J., Bordes A., Yakhnenko O. Manning and Un-unier N. 2013.
Connecting language and knowledgebases with embedding models for relation extrac-tion.
In Proceedings of EMNLP.
pags:1366-1371.Matthew D. Zeiler.
2012.
ADADELTA: AN ADAP-TIVE LEARNING RATE METHOD.
In Proceed-ings of CVPR.Socher R., Huval B., Christopher D Manning.
Manningand Andrew Y. Ng.
2012.
Semantic Compositional-ity through Recursive Matrix-vector Spaces.
In Pro-ceedings of EMNLP.Nickel M., Tresp V., Kriegel H-P. 2011.
A three-way model for collective learning on multi-relationaldata.
In Proceedings of ICML.
pages:809-816.Nickel M., Tresp V., Kriegel H-P. 2012.
FactorizingYAGO: Scalable Machine Learning for Linked Data.In Proceedings of WWW.Nickel M., Tresp V. 2013a.
An Analysis of Ten-sor Models for Learning from Structured Data.Machine Learning and Knowledge Discovery inDatabases, Springer.Nickel M., Tresp V. 2013b.
Tensor Factorization forMulti-Relational Learning.
Machine Learning andKnowledge Discovery in Databases, Springer.Nickel M., Murphy K., Tresp V., Gabrilovich E.2015.
A Review of Relational Machine Learningfor Knowledge Graphs.
In Proceedings of IEEE.696
