Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 773?781,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPQuadratic-Time Dependency Parsing for Machine TranslationMichel GalleyComputer Science DepartmentStanford UniversityStanford, CA 94305-9020mgalley@cs.stanford.eduChristopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305-9010manning@cs.stanford.eduAbstractEfficiency is a prime concern in syntactic MT de-coding, yet significant developments in statisti-cal parsing with respect to asymptotic efficiencyhaven?t yet been explored in MT.
Recently,McDonald et al (2005b) formalized dependencyparsing as a maximum spanning tree (MST) prob-lem, which can be solved in quadratic time relativeto the length of the sentence.
They show that MSTparsing is almost as accurate as cubic-time depen-dency parsing in the case of English, and that itis more accurate with free word order languages.This paper applies MST parsing to MT, and de-scribes how it can be integrated into a phrase-baseddecoder to compute dependency language modelscores.
Our results show that augmenting a state-of-the-art phrase-based system with this dependencylanguage model leads to significant improvementsin TER (0.92%) and BLEU (0.45%) scores on fiveNIST Chinese-English evaluation test sets.1 IntroductionHierarchical approaches to machine translationhave proven increasingly successful in recentyears (Chiang, 2005; Marcu et al, 2006; Shenet al, 2008), and often outperform phrase-basedsystems (Och and Ney, 2004; Koehn et al, 2003)on target-language fluency and adequacy.
How-ever, their benefits generally come with high com-putational costs, particularly when chart parsing,such as CKY, is integrated with language modelsof high orders (Wu, 1996).
Indeed, synchronousCFG parsing with m-grams runs in O(n3m) time,where n is the length of the sentence.1Furthermore, synchronous CFG approaches of-ten only marginally outperform the most com-1The algorithmic complexity of (Wu, 1996) isO(n3+4(m?1)), though Huang et al (2005) present amore efficient factorization inspired by (Eisner and Satta,1999) that yields an overall complexity of O(n3+3(m?1)),i.e., O(n3m).
In comparison, phrase-based decoding can runin linear time if a distortion limit is imposed.
Of course, thiscomparison holds only for approximate algorithms.
Sinceexact MT decoding is NP complete (Knight, 1999), there isno exact search algorithm for either phrase-based or syntacticMT that runs in polynomial time (unless P = NP).petitive phrase-based systems in large-scale ex-periments such as NIST evaluations.2 This lackof significant difference may not be completelysurprising.
Indeed, researchers have shown thatgigantic language models are key to state-of-the-art performance (Brants et al, 2007), andthe ability of phrase-based decoders to handlelarge-size, high-order language models with noconsequence on asymptotic running time duringdecoding presents a compelling advantage overCKY decoders, whose time complexity grows pro-hibitively large with higher-order language mod-els.While context-free decoding algorithms (CKY,Earley, etc.)
may sometimes appear too computa-tionally expensive for high-end statistical machinetranslation, there are many alternative parsing al-gorithms that have seldom been explored in themachine translation literature.
The parsing liter-ature presents faster alternatives for both phrase-structure and dependency trees, e.g., O(n) shift-reduce parsers and variants ((Ratnaparkhi, 1997;Nivre, 2003), inter alia).
While deterministicparsers are often deemed inadequate for dealingwith ambiguities of natural language, highly accu-rate O(n2) algorithms exist in the case of depen-dency parsing.
Building upon the theoretical workof (Chu and Liu, 1965; Edmonds, 1967), McDon-ald et al (2005b) present a quadratic-time depen-dency parsing algorithm that is just 0.7% less ac-curate than ?full-fledged?
chart parsing (which, inthe case of dependency parsing, runs in timeO(n3)(Eisner, 1996)).In this paper, we show how to exploit syn-tactic dependency structure for better machinetranslation, under the constraint that the depen-2Results of the 2008 NIST Open MT evaluation(http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/mt08_official_results_v0.html) reveal that, while many ofthe best systems in the Chinese-English and Arabic-Englishtasks incorporate synchronous CFG models, score differ-ences with the best phrase-based system were insignificantlysmall.773dency structure is built as a by-product of phrase-based decoding, without reliance on a dynamic-programming or chart parsing algorithm such asCKY or Earley.
Adapting the approach of Mc-Donald et al (2005b) for machine translation, weincrementally build dependency structure left-to-right in time O(n2) during decoding.
Most in-terestingly, the time complexity of non-projectivedependency parsing remains quadratic as the or-der of the language model increases.
This pro-vides a compelling advantage over previous de-pendency language models for MT (Shen et al,2008), which use a 5-gram LM only during rerank-ing.
In our experiments, we build a competi-tive baseline (Koehn et al, 2007) incorporating a5-gram LM trained on a large part of Gigawordand show that our dependency language modelprovides improvements on five different test sets,with an overall gain of 0.92 in TER and 0.45 inBLEU scores.
These results are found to be statis-tically very significant (p?
.01).2 Dependency parsing for machinetranslationIn this section, we review dependency parsing for-mulated as a maximum spanning tree problem(McDonald et al, 2005b), which can be solved inquadratic time, and then present its adaptation andnovel application to phrase-based decoding.Dependency models have recently gained con-siderable interest in many NLP applications, in-cluding machine translation (Ding and Palmer,2005; Quirk et al, 2005; Shen et al, 2008).
De-pendency structure provides several compellingadvantages compared to other syntactic represen-tations.
First, dependency links are close to the se-mantic relationships, which are more likely to beconsistent across languages.
Indeed, Fox (2002)found inter-lingual phrasal cohesion to be greaterthan for a CFG when using a dependency rep-resentation, for which she found only 12.6% ofhead crossings and 9.2% modifier crossings.
Sec-ond, dependency trees contain exactly one nodeper word, which contributes to cutting down thesearch space during parsing: indeed, the task ofthe parser is merely to connect existing nodesrather than hypothesizing new ones.
Finally, de-pendency models are more flexible and accountfor (non-projective) head-modifier relations thatCFG models fail to represent adequately, whichis problematic with certain types of grammaticalconstructions and with free word order languages,who do you think they hired ?
WP VB PRP VB PRP VBD .
1 2 3 4 5 6 7<root> <root> 0Figure 1: A dependency tree with directed edges going fromheads to modifiers.
The edge between who and hired causesthis tree to be non-projective.
Such a head-modifier relation-ship is difficult to represent with a CFG, since all words di-rectly or indirectly headed by hired (i.e., who, think, they, andhired) do not constitute a contiguous sequence of words.as we will see later in this section.The most standardly used algorithm for parsingwith dependency grammars is presented in (Eis-ner, 1996; Eisner and Satta, 1999).
It runs in timeO(n3), where n is the length of the sentence.
Theiralgorithm exploits the special properties of depen-dency trees to reduce the worst-case complexity ofbilexical parsing, which otherwise requires O(n4)for bilexical constituency-based parsing.
While itseems difficult to improve the asymptotic runningtime of the Eisner algorithm beyond what is pre-sented in (Eisner and Satta, 1999), McDonald etal.
(2005b) show O(n2)-time parsing is possible iftrees are not required to be projective.
This re-laxation entails that dependencies may cross eachother rather than being required to be nested, asshown in Fig.
1.
More formally, a non-projectivetree is any tree that does not satisfy the followingdefinition of a projective tree:Definition.
Let x = x1 ?
?
?xn be an input sentence,and let y be a rooted tree represented as a setin which each element (i, j) ?
y is an orderedpair of word indices of x that defines a depen-dency relation between a head xi and a modifierx j.
By definition, the tree y is said to be projec-tive if each dependency (i, j) satisfies the follow-ing property: each word in xi+1 ?
?
?x j?1 (if i < j)or in x j+1 ?
?
?xi?1 (if j < i) is a descendent of headword xi.This relaxation is key to computational effi-ciency, since the parser does not need to keeptrack of whether dependencies assemble into con-tiguous spans.
It is also linguistically desirablein the case of free word order languages such asCzech, Dutch, and German.
Non-projective de-pendency structures are sometimes even neededfor languages like English, e.g., in the case of thewh-movement shown in Fig.
1.
For languages774with relatively rigid word order such as English,there may be some concern that searching thespace of non-projective dependency trees, whichis considerably larger than the space of projectivedependency trees, would yield poor performance.That is not the case: dependency accuracy for non-projective parsing is 90.2% for English (McDon-ald et al, 2005b), only 0.7% lower than a projec-tive parser (McDonald et al, 2005a) that uses thesame set of features and learning algorithm.
In thecase of dependency parsing for Czech, (McDonaldet al, 2005b) even outperforms projective parsing,and was one of the top systems in the CoNLL-06shared task in multilingual dependency parsing.2.1 O(n2)-time dependency parsing for MTWe now formalize weighted non-projective de-pendency parsing similarly to (McDonald et al,2005b) and then describe a modified and more ef-ficient version that can be integrated into a phrase-based decoder.Given the single-head constraint, parsing an in-put sentence x = (x0,x1, ?
?
?
,xn) is reduced to la-beling each word x j with an index i identifying itshead word xi.
We include the dummy root symbolx0 = ?root?
so that each word can be a modifier.We score each dependency relation using a stan-dard linear models(i, j) = ?
?
f(i, j) (1)whose weight vector ?
is trained usingMIRA (Crammer and Singer, 2003) to opti-mize dependency parsing accuracy (McDonald etal., 2005a).
As is commonly the case in statisticalparsing, the score of the full tree is decomposedas the sum of the score of all edges:s(x,y) = ?
(i, j)?y?
?
f(i, j) (2)When there is no need to ensure projectivity, onecan independently select the highest scoring edge(i, j) for each modifier x j, yet we generally want toensure that the resulting structure is a tree, i.e., thatit does not contain any circular dependencies.
Thisoptimization problem is a known instance of themaximum spanning tree (MST) problem.
In ourcase, the graph is directed?indeed, the equalitys(i, j) = s( j, i) is generally not true and would belinguistically aberrant?so the problem constitutesan instance of the less-known MST problem fordirected graphs.
This problem is solved with theChu-Liu-Edmonds (CLE) algorithm (Chu and Liu,1965; Edmonds, 1967).Formally, we represent the graph G = (V,E)with a vertex set V = x = {x0, ?
?
?
,xn} and a setof directed edges E = [0,n]?
[1,n], in which eachedge (i, j), representing the dependency xi ?
x j,is assigned a score s(i, j).
Finding the spanningtree y ?
E rooted at x0 that maximizes s(x,y) asdefined in Equation 2 has a straightforward solu-tion in O(n2 log(n)) time for dense graphs such asG, though Tarjan (1977) shows that the problemcan be solved in O(n2).
Hence, non-projectivedependency parsing is solved in quadratic time.The main idea behind the CLE algorithm is tofirst greedily select for each word x j the incom-ing edge (i, j) with highest score, then to succes-sively repeat the following two steps: (a) identifya loop in the graph, and if there is none, halt; (b)contract the loop into a single vertex, and updatescores for edges coming in and out of the loop.Once all loops have been eliminated, the algorithmmaps back the maximum spanning tree of the con-tracted graph onto the original graph G, and it canbe shown that this yields a spanning tree that is op-timal with respect to G and s (Georgiadis, 2003).The greedy approach of selecting the highestscoring edge (i, j) for each modifier x j caneasily be applied left-to-right during phrase-baseddecoding, which proceeds in the same order.For each hypothesis expansion, our decodergenerates the following information for the newhypothesis h:?
a partial translation x;?
a coverage set of input words c;?
a translation score ?
.In the case of non-projective dependency parsing,we need to maintain additional information foreach word x j of the partial translation x:?
a predicted POS tag t j;?
a dependency score s j.Dependency scores s j are initialized to ?
?.Each time a new word is added to a partial hy-pothesis, the decoder executes the routine shownin Table 1.
To avoid cluttering the pseudo-code,we make here the simplifying assumption thateach hypothesis expansion adds exactly one word,though the real implementation supports the caseof phrases of any length.
Line 3 determineswhether the translation hypothesis is complete, inwhich case it explicitly builds the graph G and775Decoding: hypothesis expansion step.1.
Inferer generates new hypothesis h = (x,c,?)2.
j?
|x|?13.
t j?
tagger(x j?3, ?
?
?
,x j)4. if complete(c)5.
Chu-Liu-Edmonds(h)6. else7.
for i = 1 to j8.
s j = max(s j,s(i, j))9. si = max(si,s( j, i))Table 1: Hypothesis expansion with dependency scoring.finds the maximum spanning tree.
Note that it isimpractical to identify loops each time a new wordis added to a translation hypothesis, since this re-quires explicitly storing the dense graph G, whichwould require an O(n2) copy operation duringeach hypothesis expansion; this would of courseincrease time and space complexity (the max op-eration in lines 8 and 9 only keeps the current bestscoring edges).
If there is any loop, the depen-dency score is adjusted in the last hypothesis ex-pansion.
In practice, we delay the computation ofdependency scores involving word x j until tag t j+1is generated, since dependency parsing accuracy isparticularly low (?0.8%) when the next tag is un-known.We found that dependency scores with or with-out loop elimination are generally close and highlycorrelated, and that MT performance without fi-nal loop removal was about the same (generallyless than 0.2% BLEU).
While it seems that loopygraphs are undesirable when the goal is to obtain asyntactic analysis, that is not necessarily the casewhen one just needs a language modeling score.2.2 Features for dependency parsingIn our experiments, we use sets of features that aresimilar to the ones used in the McDonald parser,though we make a key modification that yields anasymptotic speedup that ensures a genuine O(n2)running time.The three feature sets that were used in our ex-periments are shown in Table 2.
We write h-word,h-pos, m-word, m-pos to refer to head and modi-fier words and POS tags, and append a numericalvalue to shift the word offset either to the left or tothe right (e.g., h-pos+1 is the POS to the right ofthe head word).
We use the symbol ?
to representfeature conjunctions.
Each feature in the table hasa distinct identifier, so that, e.g., the POS featuresUnigram features:h-word, h-pos, h-word ?
h-pos,m-word, m-pos, m-word ?
m-posBigram features:h-word ?
m-word, h-pos ?
m-pos,h-word ?
h-pos ?
m-word, h-word ?
h-pos ?
m-pos,m-word ?
m-pos ?
h-word, m-word ?
m-pos ?
h-pos,h-word ?
h-pos ?
m-word ?
m-posAdjacent POS features:h-pos ?
h-pos+1 ?
m-pos?1 ?
m-pos,h-pos ?
h-pos+1 ?
m-pos ?
m-pos+1,h-pos?1 ?
h-pos ?
m-pos?1 ?
m-pos,h-pos?1 ?
h-pos ?
m-pos ?
m-pos+1In-between POS features:if i < j:h-pos ?
h-pos+k ?
m-pos k ?
[ i,min(i+5, j) ]h-pos ?
m-pos?k ?
m-pos k ?
[max(i, j?5), j ]if i > j:m-pos ?
m-pos+k ?
h-pos k ?
[ j,min( j+5, i) ]m-pos ?
h-pos?k ?
h-pos k ?
[max( j, i?5), i ]Table 2: Features for dependency parsing.
It is quite similarto the McDonald (2005a) feature set, except that it does notinclude the set of all POS tags that appear between each can-didate head-modifier pair (i, j).
This modification is essentialin order to make our parser run in trueO(n2) time, as opposedto (McDonald et al, 2005b).SOURCE IDS GENRE SENTENCESEnglish CTB 050?325 newswire 3027English ATB all newswire 13628OntoNotes all broadcast news 14056WSJ 02?21 financial news 39832Total 70543Table 3: Characteristics of our training data.
The second col-umn identifies documents and sections selected for training.h-pos are all distinct from m-pos features.3The primary difference between our feature setsand the ones of McDonald et al is that their set of?in between POS features?
includes the set of alltags appearing between each pair of words.
Ex-tracting all these tags takes time O(n) for any arbi-trary pair (i, j).
Since i and j are both free vari-ables, feature computation in (McDonald et al,2005b) takes time O(n3), even though parsing it-self takes O(n2) time.
To make our parser gen-uinely O(n2), we modified the set of in-betweenPOS features in two ways.
First, we restrict ex-traction of in-between POS tags to those wordsthat appear within a window of five words rel-ative to either the head or the modifier.
Whilethis change alone ensures that feature extraction isnow O(1) for each word pair, this causes a fairlyhigh drop of performance (dependency accuracy3In addition to these basic features, we follow McDonaldin conjoining most features with two extra pieces of infor-mation: a boolean variable indicating whether the modifierattaches to the left or to the right, and the binned distancebetween the two words.776ALGORITHM TIME SETUP TRAINING TESTING ACCURACYProjective O(n3) Parsing WSJ(02-21) WSJ(23) 90.60Chu-Liu-Edmonds O(n3) Parsing WSJ(02-21) WSJ(23) 89.64Chu-Liu-Edmonds O(n2) Parsing WSJ(02-21) WSJ(23) 89.32Local classifier O(n2) Parsing WSJ(02-21) WSJ(23) 89.15Projective O(n3) MT CTB(050-325) CTB(001-049) 86.33Chu-Liu-Edmonds O(n3) MT CTB(050-325) CTB(001-049) 85.68Chu-Liu-Edmonds O(n2) MT CTB(050-325) CTB(001-049) 85.43Local classifier O(n2) MT CTB(050-325) CTB(001-049) 85.22Projective O(n3) MT CTB(050-325), WSJ(02-21), ATB, OntoNotes CTB(001-049) 87.40(**)Chu-Liu-Edmonds O(n3) MT CTB(050-325), WSJ(02-21), ATB, OntoNotes CTB(001-049) 86.79Chu-Liu-Edmonds O(n2) MT CTB(050-325), WSJ(02-21), ATB, OntoNotes CTB(001-049) 86.45(*)Local classifier O(n2) MT CTB(050-325), WSJ(02-21), ATB, OntoNotes CTB(001-049) 86.29Table 4: Dependency parsing experiments on test sentences of any length.
The projective parsing algorithm is the one imple-mented as in (McDonald et al, 2005a), which is known as one of the top performing dependency parsers for English.
The O(n3)non-projective parser of (McDonald et al, 2005b) is slightly more accurate than our version, though ours runs in O(n2) time.
?Local classifier?
refers to non-projective dependency parsing without removing loops as a post-processing step.
The resultmarked with (*) identifies the parser used for our MT experiments, which is only about 1% less accurate than a state-of-the-artdependency parser (**).on our test was down 0.9%).
To make our gen-uinely O(n2) parser almost as accurate as the non-projective parser of McDonald et al, we conjoineach in-between POS with its position relative to(i, j).
This relatively simple change reduces thedrop in accuracy to only 0.34%.43 Dependency parsing experimentsIn this section, we compare the performance ofour parsing model to the ones of McDonald et alSince our MT test sets include newswire, web, andaudio, we trained our parser on different genres.Our training data includes newswire from the En-glish translation treebank (LDC2007T02) and theEnglish-Arabic Treebank (LDC2006T10), whichare respectively translations of sections of the Chi-nese treebank (CTB) and Arabic treebank (ATB).We also trained the parser on the broadcast-news treebank available in the OntoNotes corpus(LDC2008T04), and added sections 02-21 of theWSJ Penn treebank.
Documents 001-040 of theEnglish CTB data were set aside to constitute atest set for newswire texts.
Our other test set isthe standard Section 23 of the Penn treebank.
Thesplits and amounts of data used for training are dis-played in Table 3.Parsing experiments are shown in Table 4.
We4We need to mention some practical considerations thatmake feature computation fast enough for MT.
Most featuresare precomputed before actual decoding.
All target-languagewords to appear during beam search can be determined in ad-vance, and all their unigram feature scores are precomputed.For features conditioned on both head and modifier, scoresare cached whenever possible.
The only features that are notcached are the ones that include contextual POS tags, sincetheir miss rate is relatively high.distinguish two experimental conditions: Parsingand MT.
For Parsing, sentences are cased and tok-enization abides to the PTB segmentation as usedin the Penn treebank version 3.
For the MT set-ting, texts are all lower case, and tokenizationwas changed to improve machine translation (e.g.,most hyphenated words were split).
For this set-ting, we also had to harmonize the four treebanks.The most crucial modification was to add NP in-ternal bracketing to the WSJ (Vadas and Curran,2007), since the three other treebanks contain thatinformation.
Treebanks were also transformed tobe consistent with MT tokenization.
We evaluateMT parsing models on CTB rather than on WSJ,since CTB contains newswire and is thus morerepresentative of MT evaluation conditions.To obtain part-of-speech tags, we use astate-of-the-art maximum-entropy (CMM) tagger(Toutanova et al, 2003).
In the Parsing setting, weuse its best configuration, which reaches a taggingaccuracy of 97.25% on standard WSJ test data.
Inthe MT setting, we need to use a less effective tag-ger, since we cannot afford to perform Viterbi in-ference as a by-product of phrase-based decoding.Hence, we use a simpler tagging model that as-signs tag ti to word xi by only using features ofwords xi?3 ?
?
?xi, and that does not condition anydecision based on any preceding or next tags (ti?1,etc.).
Its performance is 95.02% on the WSJ, and95.30% on the English CTB.
Additional experi-ments reveal two main contributing factors to thisdrop on WSJ: tagging uncased texts reduces tag-ging accuracy by about 1%, and using only word-based features further reduces it by 0.6%.Table 4 shows that the accuracy of our truly777O(n2) parser is only .25% to .34% worse thanthe O(n3) implementation of (McDonald et al,2005b).5 Compared to the state-of-the-art projec-tive parser as implemented in (McDonald et al,2005a), performance is 1.28% lower on WSJ, butonly 0.95% when training on all our available dataand using the MT setting.
Overall, we believe thatthe drop of performance is a reasonable price topay considering the computational constraints im-posed by integrating the dependency parser into anMT decoder.The table also shows a gain of more than 1% independency accuracy by adding ATB, OntoNotes,and WSJ to the English CTB training set.
Thefour sources were assigned non-uniform weights:we set the weight of the CTB data to be 10 timeslarger than the other corpora, which seems to workbest in our parsing experiments.
While this im-provement of 1% may seem relatively small con-sidering that the amount of training data is morethan 20 times larger in the latter case, it is quiteconsistent with previous findings in domain adap-tation, which is known to be a difficult task.
Forexample, (Daume III, 2007) shows that training alearning algorithm on the weighted union of dif-ferent data sets (which is basically what we did)performs almost as well as more involved domainadaptation approaches.4 Machine translation experimentsIn our experiments, we use a re-implementationof the Moses phrase-based decoder (Koehn etal., 2007).
We use the standard features imple-mented almost exactly as in Moses: four trans-lation features (phrase-based translation probabil-ities and lexically-weighted probabilities), wordpenalty, phrase penalty, linear distortion, and lan-guage model score.
We also incorporated the lex-icalized reordering features of Moses, in order toexperiment with a baseline that is stronger than thedefault Moses configuration.The language pair for our experiments isChinese-to-English.
The training data consists ofabout 28 million English words and 23.3 million5Note that our results on WSJ are not exactly the sameas those reported in (McDonald et al, 2005b), since we usedslightly different head finding rules.
To extract dependenciesfrom treebanks, we used the LTH Penn Converter (http://nlp.cs.lth.se/pennconverter/), which extractsdependencies that are almost identical to those used for theCoNLL-2008 Shared Task.
We constrain the converter not touse functional tags found in the treebanks, in order to make itpossible to use automatically parsed texts (i.e., perform self-training) in future work.Chinese words drawn from various news parallelcorpora distributed by the Linguistic Data Con-sortium (LDC).
In order to provide experimentscomparable to previous work, we used the samecorpora as (Wang et al, 2007): LDC2002E18,LDC2003E07, LDC2003E14, LDC2005E83,LDC2005T06, LDC2006E26, LDC2006E8, andLDC2006G05.
Chinese words were automaticallysegmented with a conditional random field (CRF)classifier (Chang et al, 2008) that conforms to theChinese Treebank (CTB) standard.In order to train a competitive baseline given ourcomputational resources, we built a large 5-gramlanguage model using the Xinhua and AFP sec-tions of the Gigaword corpus (LDC2007T40) inaddition to the target side of the parallel data.This data represents a total of about 700 mil-lion words.
We manually removed documents ofGigaword that were released during periods thatoverlap with those of our development and testsets.
The language model was smoothed with themodified Kneser-Ney algorithm as implementedin (Stolcke, 2002), and we only kept 4-grams and5-grams that occurred at least three times in thetraining data.6For tuning and testing, we use the official NISTMT evaluation data for Chinese from 2002 to 2008(MT02 to MT08), which all have four English ref-erences for each input sentence.
We used the 1082sentences of MT05 for tuning and all other sets fortesting.
Parameter tuning was done with minimumerror rate training (Och, 2003), which was usedto maximize BLEU (Papineni et al, 2001).
SinceMERT is prone to search errors, especially withlarge numbers of parameters, we ran each tuningexperiment three times with different initial condi-tions.
We used n-best lists of size 200 and a beamsize of 200.
In the final evaluations, we report re-sults using both TER (Snover et al, 2006) and theoriginal BLEU metric as described in (Papineni etal., 2001).
All our evaluations are performed onuncased texts.The results for our translation experiments areshown in Table 5.
We compared two systems: onewith the set of features described earlier in thissection.
The second system incorporates one ad-ditional feature, which is the dependency language6We found that sections of Gigaword other than Xinhuaand AFP provide almost no improvement in our experiments.By leaving aside the other sections, we were able to increasethe order of the language model to 5-gram and perform rela-tively little pruning.
This LM required 16GB of RAM duringtraining.778BLEU[%]DEP.
LM MT05 (tune) MT02 MT03 MT04 MT06 MT08no 33.42 33.38 33.13 36.21 32.16 24.83yes 34.19 (+.77**) 33.85 (+.47) 33.73 (+.6*) 36.67 (+.46*) 32.84 (+.68**) 24.91 (+.08)TER[%]DEP.
LM MT05 (tune) MT02 MT03 MT04 MT06 MT08no 57.41 58.07 57.32 56.09 57.24 61.96yes 56.27 (?1.14**) 57.15 (?.92**) 56.09 (?1.23**) 55.30 (?.79**) 56.05 (?1.19**) 61.41 (?.55*)MT05 (tune) MT02 MT03 MT04 MT06 MT08Sentences 1082 878 919 1788 1664 1357Table 5: MT experiments with and without a dependency language model.
We use randomization tests (Riezler and Maxwell,2005) to determine significance: differences marked with a (*) are significant at the p?
.05 level, and those marked as (**) aresignificant at the p?
.01 level.model score computed with the dependency pars-ing algorithm described in Section 2.
We usedthe dependency model trained on the English CTBand ATB treebank, WSJ, and OntoNotes.We see that the Moses decoder with integrateddependency language model systematically out-performs the Moses baseline.
For BLEU evalu-ations, differences are significant in four out ofsix cases, and in the case of TER, all differencesare significant.
Regarding the small difference inBLEU scores on MT08, we would like to pointout that tuning on MT05 and testing on MT08had a rather adverse effect with respect to trans-lation length: while the two systems are rela-tively close in terms of BLEU scores (24.83 and24.91, respectively), the dependency LM providesa much bigger gain when evaluated with BLEUprecision (27.73 vs. 28.79), i.e., by ignoring thebrevity penalty.
On the other hand, the differenceon MT08 is significant in terms of TER.Table 6 provides experimental results on theNIST test data (excluding the tuning set MT05) foreach of the three genres: newswire, web data, andspeech (broadcast news and conversation).
Thelast column displays results for all test sets com-bined.
Results do not suggest any noticeable dif-ference between genres, and the dependency lan-guage model provides significant gains on all gen-res, despite the fact that this model was primarilytrained on news data.We wish to emphasize that our positive re-sults are particularly noteworthy because they areachieved over a baseline incorporating a compet-itive 5-gram language model.
As is widely ac-knowledged in the speech community, it can bedifficult to outperform high-order n-gram modelsin large-scale experiments.
Finally, we quantifiedthe effective running time of our phrase-based de-coder with and without our dependency languageBLEU[%]DEP.
LM newswire web speech allno 32.86 21.75 36.88 32.29yes 33.19 22.64 37.51 32.74(+0.33) (+0.89) (+0.63) (+0.45)TER[%]DEP.
LM newswire web speech allno 57.73 62.64 55.16 58.02yes 56.73 61.97 54.26 57.10(?1) (?0.67) (?0.9) (?0.92)newswire web speech allSentences 4006 1149 1451 6606Table 6: Test set performances on MT02-MT04 and MT06-MT08, where the data was broken down by genre.
Giventhe large amount of test data involved in this table, all theseresults are statistically highly significant (p?
.01).10 20 30 40 50 60 70 80 90020406080100120140160sentence lengthsecondsdepLMbaselineFigure 2: Running time of our phrase-based decoder with andwithout quadratic-time dependency LM scoring.model using MT05 (Fig.
2).
In both settings, weselected the best tuned model, which yield the per-formance shown in the first column of Table 5.Our decoder was run on an AMD Opteron Proces-sor 2216 with 16GB of memory, and without re-sorting to any rescoring method such as cube prun-ing.
In the case of English translations of 40 wordsand shorter, the baseline system took 6.5 secondsper sentence, whereas the dependency LM systemspent 15.6 seconds per sentence, i.e., 2.4 times thebaseline running time.
In the case of translations779longer than 40 words, average speeds were respec-tively 17.5 and 59.5 seconds per sentence, i.e., thedependency was only 3.4 times slower.75 Related workPerhaps due to the high computational cost of syn-chronous CFG decoding, there have been variousattempts to exploit syntactic knowledge and hier-archical structure in other machine translation ex-periments that do not require chart parsing.
Usinga reranking framework, Och et al (2004) foundthat various types of syntactic features providedonly minor gains in performance, suggesting thatphrase-based systems (Och and Ney, 2004) shouldexploit such information during rather than afterdecoding.
Wang et al (2007) sidestep the need tooperate large-scale word order changes during de-coding (and thus lessening the need for syntacticdecoding) by rearranging input words in the train-ing data to match the syntactic structure of thetarget language.
Finally, Birch et al (2007) ex-ploit factored phrase-based translation models toassociate each word with a supertag, which con-tains most of the information needed to build a fullparse.
When combined with a supertag n-gramlanguage model, it helps enforce grammatical con-straints on the target side.There have been various attempts to reduce thecomputational expense of syntactic decoding, in-cluding multi-pass decoding approaches (Zhangand Gildea, 2008; Petrov et al, 2008) and rescor-ing approaches (Huang and Chiang, 2007).
In thelatter paper, Huang and Chiang introduce rescor-ing methods named ?cube pruning?
and ?cubegrowing?, which first use a baseline decoder (ei-ther synchronous CFG or a phrase-based sys-tem) and no LM to generate a hypergraph, andthen rescoring this hypergraph with a languagemodel.
Huang and Chiang show significant speedincreases with little impact on translation quality.We believe that their approach is orthogonal (andpossibly complementary) to our work, since ourpaper proposes a new model for fully-integrateddecoding that increases MT performance, anddoes not rely on rescoring.7We note that our Java-based decoder is research ratherthan industrial-strength code and that it could be substantiallyoptimized.
Hence, we think the reader should pay more at-tention to relative speed differences between the two systemsrather than absolute timings.6 Conclusion and future workIn this paper, we presented a non-projective de-pendency parser whose time-complexity of O(n2)improves upon the cubic time implementation of(McDonald et al, 2005b), and does so with lit-tle loss in dependency accuracy (.25% to .34%).Since this parser does not need to enforce projec-tivity constraints, it can easily be integrated intoa phrase-based decoder during search (rather thanduring rescoring).
We use dependency scores asan extra feature in our MT experiments, and foundthat our dependency model provides significantgains over a competitive baseline that incorporatesa large 5-gram language model (0.92% TER and0.45% BLEU absolute improvements).We plan to pursue other research directions us-ing dependency models discussed in this paper.While we use a dependency language model toexemplify the use of hierarchical structure withinphrase based decoders, we could extend this workto incorporate dependency features of both source-and target side.
Since parsing of the source is rel-atively inexpensive compared to the target side,it would be relatively easy to condition head-modifier dependencies not only on the two tar-get words, but also on their corresponding Chi-nese words and their relative positions in the Chi-nese tree.
This would enable the decoder to cap-ture syntactic reordering without requiring trees tobe isomorphic or even projective.
It would alsobe interesting to apply these models to target lan-guages that have free word order, which wouldpresumably benefit more from the flexibility ofnon-projective dependency models.AcknowledgementsThe authors wish to thank the anonymous review-ers for their helpful comments on an earlier draftof this paper, and Daniel Cer for his implementa-tion of Phrasal, a phrase-based decoder similar toMoses.
This paper is based on work funded bythe Defense Advanced Research Projects Agencythrough IBM.
The content does not necessarily re-flect the views of the U.S. Government, and no of-ficial endorsement should be inferred.ReferencesA.
Birch, M. Osborne, and P. Koehn.
2007.
CCG su-pertags in factored statistical machine translation.
InProc.
of the Workshop on Statistical Machine Trans-lation, pages 9?16.780T.
Brants, A. Popat, P. Xu, F. Och, and J.
Dean.
2007.Large language models in machine translation.
InProc.
of EMNLP-CoNLL, pages 858?867.P.
Chang, M. Galley, and C. Manning.
2008.
Optimiz-ing Chinese word segmentation for machine transla-tion performance.
In Proc.
of the ACL Workshop onStatistical Machine Translation, pages 224?232.D.
Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proc.
of ACL,pages 263?270.Y.
J. Chu and T. H. Liu.
1965.
On the shortest arbores-cence of a directed graph.
Science Sinica, 14:1396?1400.K.
Crammer and Y.
Singer.
2003.
Ultraconservativeonline algorithms for multiclass problems.
Journalof Machine Learning Research, 3:951?991.H.
Daume III.
2007.
Frustratingly easy domain adap-tation.
In Proc.
of ACL, pages 256?263.Y.
Ding and M. Palmer.
2005.
Machine translation us-ing probabilistic synchronous dependency insertiongrammars.
In Proc.
of ACL, pages 541?548.J.
Edmonds.
1967.
Optimum branchings.
Research ofthe National Bureau of Standards, 71B:233?240.J.
Eisner and G. Satta.
1999.
Efficient pars-ing for bilexical context-free grammars and head-automaton grammars.
In Proc.
of ACL, pages 457?464.J.
Eisner.
1996.
Three new probabilistic models for de-pendency parsing: An exploration.
In Proc.
of COL-ING, pages 340?345.H.
Fox.
2002.
Phrasal cohesion and statistical machinetranslation.
In Proc.
of EMNLP, pages 304?311.L.
Georgiadis.
2003.
Arborescence optimization prob-lems solvable by Edmonds?
algorithm.
TheoreticalComputer Science, 301(1-3):427?437.L.
Huang and D. Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InProc.
of ACL, pages 144?151.L.
Huang, H. Zhang, and D. Gildea.
2005.
Ma-chine translation as lexicalized parsing with hooks.In Proc.
of the International Workshop on ParsingTechnology, pages 65?73.K.
Knight.
1999.
Decoding complexity in word-replacement translation models.
ComputationalLinguistics, 25(4):607?615.P.
Koehn, F. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proc.
of NAACL.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkitfor statistical machine translation.
In Proc.
of ACL,Demonstration Session.D.
Marcu, W.Wang, A. Echihabi, and K. Knight.
2006.SPMT: Statistical machine translation with syntact-ified target language phrases.
In Proc.
of EMNLP,pages 44?52.R.
McDonald, K. Crammer, and F. Pereira.
2005a.
On-line large-margin training of dependency parsers.
InProc.
of ACL, pages 91?98.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic.2005b.
Non-projective dependency parsing usingspanning tree algorithms.
In Proc.
of HLT-EMNLP,pages 523?530.J.
Nivre.
2003.
An efficient algorithm for projec-tive dependency parsing.
In Proc.
of the Inter-national Workshop on Parsing Technologies (IWPT03), pages 149?160.F.
Och and H. Ney.
2004.
The alignment templateapproach to statistical machine translation.
Compu-tational Linguistics, 30(4):417?449.F.
Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-mada, A. Fraser, S. Kumar, L. Shen, D. Smith,K.
Eng, V. Jain, Z. Jin, and D. Radev.
2004.
A smor-gasbord of features for statistical machine transla-tion.
In Proceedings of HLT-NAACL.F.
Och.
2003.
Minimum error rate training for statisti-cal machine translation.
In Proc.
of ACL.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2001.BLEU: a method for automatic evaluation of ma-chine translation.
In Proc.
of ACL.S.
Petrov, A. Haghighi, and D. Klein.
2008.
Coarse-to-fine syntactic machine translation using languageprojections.
In Proc.
of EMNLP, pages 108?116.C.
Quirk, A. Menezes, and C. Cherry.
2005.
De-pendency treelet translation: syntactically informedphrasal SMT.
In Proc.
of ACL, pages 271?279.A.
Ratnaparkhi.
1997.
A linear observed time statis-tical parser based on maximum entropy models.
InProc.
of EMNLP.S.
Riezler and J. Maxwell.
2005.
On some pitfallsin automatic evaluation and significance testing forMT.
In Proc.
of the ACL Workshop on Intrinsic andExtrinsic Evaluation Measures for Machine Trans-lation and/or Summarization, pages 57?64.L.
Shen, J. Xu, and R. Weischedel.
2008.
A newstring-to-dependency machine translation algorithmwith a target dependency language model.
In Proc.of ACL, pages 577?585.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A study of translation edit ratewith targeted human annotation.
In Proc.
of AMTA,pages 223?231.A.
Stolcke.
2002.
SRILM ?
an extensible languagemodeling toolkit.
In Proc.
Intl.
Conf.
on SpokenLanguage Processing (ICSLP?2002).R.
Tarjan.
1977.
Finding optimum branchings.
Net-works, 7:25?35.K.
Toutanova, D. Klein, C. Manning, and Y. Singer.2003.
Feature-rich part-of-speech tagging with acyclic dependency network.
In Proc.
of NAACL,pages 173?180.D.
Vadas and J. Curran.
2007.
Adding noun phrasestructure to the Penn treebank.
In Proc.
of ACL,pages 240?247.C.
Wang, M. Collins, and P. Koehn.
2007.
Chinesesyntactic reordering for statistical machine transla-tion.
In Proc.
of EMNLP-CoNLL, pages 737?745.D.
Wu.
1996.
A polynomial-time algorithm for statis-tical machine translation.
In Proc.
of ACL.H.
Zhang and D. Gildea.
2008.
Efficient multi-passdecoding for synchronous context free grammars.
InProc.
of ACL, pages 209?217.781
