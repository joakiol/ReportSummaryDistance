Usability Evaluation inSpoken Language Dialogue SystemsLaila Dybkj?r and Niels Ole BernsenNatural Interactive Systems Laboratory, University of Southern DenmarkScience Park 10, 5230 Odense M, Denmarklaila@nis.sdu.dk, nob@nis.sdu.dkAbstractThe paper first addresses a series ofissues basic to evaluating the usabilityof spoken language dialogue systems,including types and purpose ofevaluation, when to evaluate andwhich methods to use, userinvolvement, how to evaluate andwhat to evaluate.
We then go on topresent and discuss a comprehensiveset of usability evaluation criteria forspoken language dialogue systems.1 IntroductionUsability is becoming an increasingly importantissue in the development and evaluation ofspoken language dialogue systems (SLDSs).Many companies would pay large amounts toknow exactly which features make SLDSsattractive to users and how to evaluate whethertheir system has these features.
In spite of itskey importance far less resources have beeninvested in the usability aspect of SLDSs overthe years than in SLDS component technologies.The usability aspect has often been neglected inSLDS development and evaluation and there hasbeen surprisingly little research in importantuser-related issues, such as user reactions toSLDSs in the field, users?
linguistic behaviour,or the main factors which determine overall usersatisfaction.
However, there now seems to begrowing recognition that usability is asimportant as, and partly independent of, thetechnical quality of any SLDS component andthat quality usability constitutes an importantcompetitive parameter.Most of today's SLDSs are walk-up-and-usesystems for shared-goal tasks.
Usability of walk-up-and-use systems is of utmost importance,since users of such systems cannot be expectedto undertake extensive training about the systemor to read the user manual.
Help must beavailable online and needed as infrequently aspossible.There is at present no systematicunderstanding of which factors must be takeninto account to optimise SLDS usability andthus also no consensus as to which usabilityevaluation criteria to use.
Ideally, such anunderstanding should be comprehensive, i.e.include all major usability perspectives onSLDSs, and exhaustive, i.e.
describe eachperspective as it pertains to the detaileddevelopment and evaluation of any possibleSLDS.
This paper addresses the aspect ofcomprehensiveness by proposing a set ofusability evaluation criteria.
The criteria arederived from a set of usability issues that haveresulted from a decomposition of the complexspace of SLDS usability best practice.The present paper focuses on walk-up-and-use SLDSs for shared-goal tasks and reviewssubstantial parts of the authors?
work aspresented in, e.g., (Dybkj?r and Bernsen 2000).For additional examples the reader is referred tothis paper.
Due to space limitations fewexamples are included in the present paper.In the following we first briefly address typesand purpose of evaluation (Section 2), when toevaluate and which methods to use (Section 3),user involvement (Section 4), and how toevaluate (Section 5).
Section 6 presents theproposed set of evaluation criteria and discussesthe usability issues behind these.
Section 7concludes the paper.2 Types and purpose of evaluationEvaluation can be quantitative or qualitative,subjective or objective.
Quantitative evaluationconsists in quantifying some parameter throughan independently meaningful number,percentage etc.
which in principle allowscomparison across systems.
Qualitativeevaluation consists in estimating or judgingsome parameter by reference to expert standardsand rules.
Subjective evaluation consists injudging some parameter by reference to users?opinions.
Objective evaluation produces subject-independent parameter assessment.
Ideally, wewould like to obtain quantitative and objectiveprogress evaluation scores for usability whichcan be objectively compared to scores obtainedfrom evaluation of other SLDSs.
This is whathas been attempted in the PARADISEframework based on the claim that task successand dialogue cost are potentially relevantcontributors to user satisfaction (Walker,Litman, Kamm and Abella 1997).
However,many important usability issues cannot besubjected to quantification and objective expertevaluation is sometimes highly uncertain or non-existent.The purpose of evaluation may be to detectand analyse design and implementation errors(diagnostic evaluation), measure SLDSperformance in terms of a set of quantitativeand/or qualitative parameters (performanceevaluation), or evaluate how well the system fitsits purpose and meets actual user needs andexpectations (adequacy evaluation), cf.
(Hirschmann and Thompson 1996, Gibbon,Moore.
and Winski 1997, Bernsen et al 1998).The latter purpose is the more important onefrom a usability point of view although theothers are relevant as well.
Which type ofevaluation to use and for which purpose,depends on the evaluation criterion which isbeing applied (see below).
Other generalreferences to natural language systemsevaluation are (EAGLES 1996, Gaizauskas1997, Sparck Jones and Galliers 1996).3 When to evaluate and methods touseUsability evaluation should start as early aspossible and continue throughout development.In general, the earlier design errors are beingidentified, the easier and cheaper it is to correctthem.
Different methods of evaluation may haveto be applied for evaluating a particularparameter depending on the phase in thelifecycle in which evaluation takes place.
Earlydesign evaluation can be based on mock-upexperiments with users and on design walk-throughs.
Wizard of Oz simulations withrepresentative task scenarios can providevaluable evaluation data.
When the system hasbeen implemented, controlled scenario-basedtests with representative users and field tests canbe used.
Recorded dialogues with the(simulated) system should be carefully analysedfor indications that the users have problems orexpectations which exceed the capabilities of thesystem.
Human-system interaction data shouldbe complemented by interviews andquestionnaires to enable assessment of usersatisfaction.
If users are interacting with theprototype on the basis of scenarios, there are atleast two issues to be aware of.
Firstly, scenariosshould be designed to avoid priming the userson how to interact with the system.
Secondly,sub-tasks covered by the scenarios will notnecessarily be representative of the sub-taskswhich real users (not using scenarios) wouldexpect the system to cover.The final test of the system is often called theacceptance test.
It involves real users and mustsatisfy the evaluation criteria defined as part ofthe requirements specification.4 User involvementIn general, representative users from the targetuser group(s) should be involved in evaluationfrom early on.
The developers themselves cancertainly discover many of the usabilityproblems with the early design andimplementation, especially when supported bystate-of-the-art usability standards, evaluationcriteria and design support tools.
The problem isthat they know too well how to interact with thesystem in order to avoid creating interactionproblems which the system cannot handle.
Forthe time being, there is no alternative toinvolving the target users in all or most systemevaluation phases and for most usabilityevaluation purposes.
This is costly and complexto do.
However, the data analysis which iscrucial to benefiting from trials with the system,is as necessary after trials with developers as itis after trials with representative users.
Even theearly involvement of representative users is noguarantee that the system will ultimatelyproduce sufficient user satisfaction.
For onething, the data distribution they generate maynot match the behaviour of the users of thesystem, once installed.
For another,experimental user trials are different from realsituations of use in which time, money and trustare really at stake.
For these reasons, andparticularly when introducing SLDSs which areinnovative in some respect, it is necessary toprepare and budget for field trials with theimplemented system as well as for thesubsequent data analysis and fine-tuning of thesystem.
Users who are ?only?
involved in a testcan be much more indifferent to, or morepositive towards, a system with poor usabilitycharacteristics than real users who havesomething to loose if the system lets them down(Bernsen et al 1998).5 How to evaluateEvaluation, including usability evaluation, isnon-trivial and cannot be explained simply bystating what to evaluate (cf.
Section 6) and whatthe developers?
options are.
One of the mostdifficult questions in evaluation probably is howto do it properly.
We have developed a templatewhich supports consistent and detaileddescription of each evaluation criterion (Bernsenand Dybkj?r 2000).
The template includes thefollowing issues: what is being evaluated (e.g.feedback adequacy), the system part evaluated(e.g.
the dialogue manager), type of evaluation(e.g.
qualitative), method(s) of evaluation (e.g.controlled user experiments), symptoms to lookfor (e.g.
user clarification questions), life cyclephase(s) (e.g.
simulation), importance ofevaluation (e.g.
crucial), difficulty of evaluation(e.g.
easy), cost of evaluation (e.g.
expensive),and support tools (e.g.
SMALTO), see(www.disc2.dk/tools).
The idea is that thecombined set of (i) design options for SLDSusability, (ii) usability evaluation criteria, and(iii) template-based characterisation of eachcriterion, will provide developers with sufficientinformation for proper evaluation of theirSLDSs.6 What to evaluateIn general terms, a usable SLDS must satisfyuser needs which are similar to those whichmust be satisfied by other interactive systems.The SLDS must be easy to understand andinteract with.
Interaction should be smooth andthe user should feel in control throughout thedialogue with the system.
It is the task of theSLDS developer to meet those user needsconsidered as overall usability design goals.However, SLDSs are very different from moretraditional interactive systems whose usabilityaspects have been investigated for decades, suchas systems controlled through graphical userinterfaces involving screen, keyboard andmouse.
Perhaps the most important difference isthat speech is perceptually transient rather thanstatic.
This means that the user must pick up theoutput information provided by the system themoment it is being provided or else miss italtogether.
Moreover, the user has no way ofinspecting the interface prior to interaction.
Ifthe interface is not self-evident all the waythrough the dialogue it must be learnt by trial-and-error through repeated interaction, which isunsatisfactory for the casual walk-up-and-useuser.
Secondly, the processing (recognition,language understanding, dialogue management)of spoken input remains difficult to design anderror-prone in execution, which is why SLDSsmust be crafted with extreme care to ensure thatusers do not produce spoken input which thesystem is incapable of handling.In the following we present a set of usabilityevaluation criteria which are based on resultsachieved in the European DISC project(www.disc2.dk) on best practice in thedevelopment and evaluation of SLDSs(Failenschmid et al 1999).
Our claim is thatquality usability of SLDSs may be pursued byfocusing on a comprehensive set of 15 usabilityissues which include all major usabilityperspectives on SLDSs.
These usability issues -or a subset thereof, depending on how advancedand complex the system is to be ?
should berepresented in the system specification andshould therefore also be reflected in a set ofevaluation criteria for the system which wouldappear mandatory for evaluating the usability ofthe SLDS.
More details on the usability issuescan be found in (Dybkj?r and Bernsen 2000).6.1 Modality appropriatenessThe majority of task-oriented SLDSs have so farused speech as the only input/output modality.However, an increasing number of systems nowcombine spoken input/output with othermodalities.It is well-known that speech-only interactionis not appropriate for all tasks and applications,and the same is true for any particular modalitycombination which includes speech input andspeech output.
Few users would e.g.
be happy tospeak aloud their pin code to the bank tellermachine in the street.
The developers shouldattempt to make sure that spoken input andoutput, possibly combined with otherinput/output modalities, is an appropriatemodality choice for the planned application.
Ifthe chosen modalities are inappropriate, chancesare that the users either will not accept theapplication or will refrain from using some ofthe modalities it offers.
Common sense,experimentation and/or the use of the toolSMALTO (www.disc2.dk/tools) may help thedevelopers in making the right modality choice.6.2 Input recognition adequacyFrom the user?s point of view, good speechrecognition means that the system rarely gets theuser?s spoken input wrong or fails to recognisewhat the user just said.
Recognition success, asperceived by the user, not only depends onrecogniser quality but also on how other parts ofthe SLDS handle the user?s input.
Goodrecogniser quality nevertheless remains the keyfactor in making users confident that the systemwill successfully get what they say.Walk-up-and-use systems may be used bymany different users in highly differentenvironments.
The speech recogniser, therefore,and depending on more specific information onits intended users and environments ofinteraction, must be trained to recognise avariety of dialects and accents, speakers ofdifferent gender, age and voice quality, speakingwith a low or a loud voice, in noisy or quietenvironments, and with varying channel quality.Adequate information on users andenvironments is essential input to the selectionand creation of training data.
To assess thequality of the system?s recognition capabilitiesprior to running the full system, speechrecognition accuracy may be tested on therecogniser with users from the target group(s).6.3 Naturalness of user speechSpeaking to an SLDS should feel as easy andnatural as possible.
It does not help the user thatthe system?s speech recognition is perfect inprinciple if the input vocabulary and grammarexpected from the user are not the ones whichthe user is likely to use and thus cannot beunderstood.
Depending on, e.g., the task andusers?
experience, what is ?natural?
inputlanguage may vary considerably.What is being experienced as natural inputspeech is also highly relative to the system?soutput phrasing.
For example, lengthy and/oroverly polite system utterances are likely toinvite similar linguistic user behaviour, therebyburdening input recognition and understandingunnecessarily.
The system?s output languagethus should be used to control users?
inputlanguage so that the latter becomes manageablefor the system whilst still feeling natural to theuser.
If the minimal constraints imposed by thetask are satisfied and the system?s outputlanguage adequately controls the user?s inputlanguage, users may well feel that the dialogueis natural even if they are not inclined to engagein lengthy conversation.Analysis of data from system simulations,questionnaires and interviews is a useful tool forobtaining information on users?
input languageand on what they perceive as being natural inputlanguage.6.4 Output voice qualityFrom the user?s point of view, good SLDSoutput voice quality means that the system?sspeech is clear and intelligible, does not demandan extra listening effort, is not particularly noisesensitive or distorted by clicks and otherextraneous sounds, has natural intonation andprosody, uses an appropriate speaking rate, andis pleasant to listen to (Karlsson 1999).
Takentogether, these requirements are difficult to meettoday.There are three main types of output speech:recordings of entire system utterances,concatenation of recorded words and phrases,and text-to-speech (TTS).
Concatenated speechis the most frequently used type of speech intoday?s SLDSs.
For walk-up-and-use systems inparticular, TTS may simply be too difficult tounderstand for infrequent users while fullrecordings are much too inflexible.
Moreover,too natural output speech, like full recordings,may suggest to users that the system is far morecapable and human-like than it actually is,encouraging them to address the system in away which is more conversational and talkativethan it can handle.The type of output voice chosen is likely toaffect users?
perception of the system as awhole.
In particular, and together with thequality of the speech output, the voice type has amajor influence on how pleasant users find the?system?s voice?.
Voice type includes featuressuch as male/female, deep/high voice, speakingrate, and emotions.In order to gather input on user preferenceswith respect to the system?s output voice,representative users of the system underdevelopment may be asked to listen to different?system voices?
and provide feedback on whichone they prefer and what they like and dislikeabout each of them.6.5 Output phrasing adequacyRegardless of the topic, the system shouldexpress itself co-operatively in order tomaximise the likelihood that the task is achievedas smoothly and efficiently as possible .
Tofacilitate successful interaction, the contents ofthe system?s output should be correct, relevantand sufficiently informative without being over-informative.
Users have good reason fordissatisfaction if the system provides falseinformation, e.g., if the database is not beingproperly updated.
Lack of relevance of systemoutput caused by, e.g., misrecognition, willtypically lead to meta-communication dialogue.System output should be sufficientlyinformative.
Otherwise, misunderstandings mayoccur which are only detected much later duringinteraction, if at all, or which, at best, lead toimmediate requests for clarification by the user.Conversely, the system should not provide toomuch or overly verbose information.
Users maythen e.g.
become inattentive, try to take thedialogue initiative, or become confused andinitiate clarification meta-communication.The form of system expressions should beclear and unambiguous, and language and, as faras possible, terminology should be consistentand familiar to the user (Bernsen et al 1998).Unclarity naturally leads to uncertainty and needfor clarification.
So does ambiguity if detectedby the user.
If undetected, as often happens, theeffects of ambiguity can be severe.
If the userunknowingly selects a non-intended meaning ofa word or phrase uttered by the system, all sortsof things can go wrong.
To help avoid ambiguityit is, moreover, advisable to use the sameexpressions for the same purposes throughoutthe dialogue.
The system preferably should notuse terms and expressions which are not familiarto most or all of its users.
If the system must dothat, unfamiliar terminology should be explainedeither proactively (before users ask) or throughadequate measures for clarification meta-communication.Developers may use CODIAL - a tool basedon Cooperativity Theory ?
as support for thedesign and evaluation of co-operative systemdialogue (www.disc2.dk/tools).It is important to realise that the system?soutput language tends to have a massive primingeffect on the user?s language.
It is, therefore,crucial that the words and grammar used insystem output can be recognised and understoodby the system itself.
Similarly, the systemshould have a speaking style which inducesusers to provide input that is to the point and canbe handled by the system.Interaction data analysis is needed to assessthe efficiency of the input control strategiesadopted.
User contacts through interviews andquestionnaires are good means for obtainingearly input on how users experience thesystem?s output.6.6 Feedback adequacyAdequate feedback is essential for users to feelin control during interaction.
The user must feelconfident that the system has understood theinformation input in the way it was intended,and the user must be told which actions thesystem has taken and what the system iscurrently doing.
A difficult thing is that tellingthe user is not always good enough ?
the usermust be told in such a way that the user noticeswhat the system says.
It may therefore be a goodthing for SLDSs to provide several differentkinds of feedback to their users.
We distinguishbetween process feedback and informationfeedback.When the system processes informationreceived from the user and hence may not bespeaking for a while, process feedback ?
whichmay be provided in many different ways - keepsthe user informed on what is going on.
A userwho is uncertain about what the system is doing,if anything, is liable to produce unwanted inputor to believe that the system has crashed anddecide to hang up.
Moreover, the uncertaintyitself is likely to affect negatively the user?ssatisfaction with the system.Feedback on the system?s understanding ofwhat the user just said and on the actions takenby the system helps ensure that, throughout thedialogue, the user is left in no doubt as to whatthe system has understood and is doing.Information feedback can be provided indifferent ways and more or less explicitly.
Theamount and nature of the information feedbackdepends e.g.
on the cost and risk involved in theuser-system transaction.
A user who is uncertainas to what the system has understood, or done, isliable to produce unwanted input and to reactnegatively to the way the system works.6.7 Adequacy of dialogue initiativeTo support natural interaction, an SLDS needs areasonable choice of dialogue initiative, anappropriate dialogue structure, sufficient taskand domain coverage, and sufficient reasoningcapabilities.Spoken human-human dialogue isprototypically mixed-initiative.
However, manytask-oriented dialogues tend to be directedprimarily by one of the interlocutors.
Users mayeven feel satisfied with less initiative wheninteracting with an SLDS than when talking to aperson as long as the dialogue initiative distri-bution fits the task(s) the system and the usermust solve together, and provided that the rest ofthe best practice issues proposed in this paperare properly attended to.
Thus, system directeddialogue can work well for tasks in which thesystem simply requires a series of specificpieces of information from the user, in particularif the user is new to the system.
To satisfyexperienced users, the system may have to beable to cope with the larger packages of inputinformation which are natural to these users.In principle, a (mainly) user directeddialogue is as much of an aberration from mixedinitiative dialogue as is the (mainly) systemdirected dialogue.
Currently, user directeddialogue would seem to be appropriate primarilyfor applications designed for experienced userswho know how to make themselves understoodby the system.
Unless supported by screengraphics or other additional modalities,inexperienced users are likely to address thesystem in ways it cannot cope with.Mixed initiative dialogue, i.e.
a mixture ofsystem and user initiative, is often both desirableand technically feasible.
At some points in thedialogue it may be appropriate that the systemtakes the initiative to guide the user, obtainmissing information, or handle an error.
At otherpoints, such as when the user needs informationfrom the system, is already familiar with thesystem or wants to correct an error, it isappropriate for the user to take the initiative.6.8 Naturalness of the dialoguestructureAs long as we cannot build fully conversationalsystems, dialogue designers may have to imposesome kind of structure onto the dialogue,determining which topics (or sub-tasks) could beaddressed when.
It is important that the structureimposed on the dialogue is natural to the user,reflecting the user?s intuitive expectations,especially in system directed dialogue in whichthe user is not supposed to interfere with thedialogue structure.
Unnatural dialogue structurewill often cause users to try to take the initiativein ways which the system cannot cope with.6.9 Sufficiency of task and domaincoverageSufficient task and domain coverage is alsocrucial to natural interaction.
Even if unfamiliarwith SLDSs, users normally have ratherdetailed expectations to the information orservice which they should be able to obtainfrom the system.
It is important that the systemmeet these expectations.
If, for some reason, thesystem is not able to perform a certain sub-taskwhich users would expect the system to handle,this has to be stated clearly.
Even then, usersatisfaction is likely to suffer.6.10 Sufficiency of the system?sreasoning capabilitiesContextually adequate reasoning is a classicalproblem in the design of natural interaction.Even when users have been appropriatelyprimed to expect a rather primitive interlocutor,they tend to assume that the system is able toperform the bits and pieces of reasoning whichhumans are able to do without thinking andwhich are inseparable parts of natural dialogueabout the task.
Typically, therefore, SLDSs mustincorporate both facts and inferences about thetask as well as general world knowledge in orderto act as adequate interlocutors.
Defining whichkinds of reasoning the system must be capableof is part and parcel of defining the system?stask and domain coverage and subject tosimilarly difficult decisions on task delimitation.It is possible to get rough ideas on initiativedistribution, users?
models of the task, and howto delimit the domain from studying recordedhuman-human dialogues on tasks similar tothose which the system is intended to cover.However, the recordings should only beconsidered possible starting points.
In particular,as task complexity grows, developers are likelyto find themselves forced to adopt morerestrictive task delimitations and impose a morerigid dialogue structure than those which theyfound in the human-human dialogues.
Havingdone that, the resulting interaction model needsearly testing and evaluation.
In particular, if thedeveloper is into relatively high task complexitycompared to the state of the art, early testing isstrongly recommended.6.11 Sufficiency of interaction guidanceSufficient interaction guidance is essential forusers to feel in control during interaction.Interaction guidance can be particularly hard toget right in speech-only, walk-up-and-useSLDSs.
Speech is inappropriate for providinglengthy and complex ?user manual?
instructionsup front for first-time users.
Moreover, at anygiven time some users will already be familiarwith the system whereas others will be novices.Issues to consider include cues for turn-takingvs.
barge-in; help facilities; and highlighting ofnon-obvious system behaviour.Barge-in allows the user to speed up theinteraction, e.g.
by interrupting already familiarinstruction prompts.
If the system does not allowbarge-in, it must provide clear cues for turn-taking, making it completely clear to the userwhen to speak and when to refrain fromspeaking because the system does not listen.Cues can be explicit or implicit.
If the user startsspeaking while the system is still listening butprocessing the previous user input, the user?snew input may cause problems for the dialoguemanager which has to generate an appropriateresponse to disjoint pieces of user input.
And ifthe system is not listening any more, importantinput could be lost in cases when users do notmerely repeat themselves.General and explicit instructions on what thesystem can and cannot do and how to interactwith it may be provided in a spoken introductionwhich can be repeated on request or be skippedby experienced users.
In fact, most speech-onlySLDSs strictly need some up-front introductionto guide interaction.
We already mentioned thewhen-(not)-to-speak issue above.
Just asimportantly, the system should be perfectly clearabout the task(s) which the user can accomplishthrough interaction.
The introduction should notbe too long because then users cannot rememberthe instructions.
Moreover, the instructions mustbe feasible for the user.
If the instructionsneeded by the walk-up-and-use user are toomany to be presented in the system?sintroduction, some of them may be relocated forpresentation at particular points duringinteraction and only when needed.Providing useful help mechanisms is adifficult interaction design task.
Help may be animplicit part of the dialogue, be available onrequest by saying ?help?
; or be automaticallyenabled if the user is having problemsrepeatedly, for instance in being recognised.
Inthis case the system may, e.g., propose how toexpress input or inform the user on what can besaid.Sufficiency of interaction guidance should becarefully evaluated by exposing the SLDS tointeraction with representative users.6.12 Error handling adequacyEven if the best practice issues discussed so farhave been taken into account carefully duringspecification, design and implementation, theSLDS and its users will still make errors duringdialogue.
In human-system interaction, errorprevention is far preferable to error correction,and what those best practice issues do is to helpprevent errors from occurring during interaction.Also as regards error handling current SLDSsare far inferior to their human interlocutors.
Thisis why adequate error handling remains adifficult issue in SLDS development.
Intuitively,this issue can be decomposed along twodimensions: (a) either the system initiates error-handling meta-communication or the userinitiates error-handling meta-communication.And (b) when error-handling meta-communication is initiated, it is either becauseone party has failed to hear or understand theother or because what was heard or understoodis false, or it is because what was heard orunderstood is somehow in need of clarification.We distinguish, therefore, between system oruser initiated repair meta-communication andsystem or user initiated clarification meta-communication.System-initiated repair meta-communicationis needed whenever the system either did notunderstand or was uncertain that it understoodcorrectly what was said.
In such cases, thesystem must ask for repetition, ask the user tospeak louder or modify the way the input isbeing expressed in other specified ways, or tellthe user what it did understand and ask forconfirmation or correction.
In case of a repeatedmisunderstanding the system may either chooseto fall back on a human operator, close thedialogue, or, better, start graceful degradation,i.e.
change the level of interaction into a simplerone.
If users simply fail to respond, then thesystem should tell that it is expecting their input.Users may also be understood by the system tohave said something which is false and henceneeds to be corrected.
User-initiated repairmeta-communication can be designed in severaldifferent ways.
Ideally, users should just initiaterepair the same way they would have done indialogue with a human, but since users mayexpress their corrections in many different waysthis is very difficult.
Some systems require theuser to use specifically designed keywords.
Theproblem is that using keywords for correction isunnatural and hence difficult for the user toremember.
A third approach is the ?eraser?principle where the user simply repeats his inputuntil the system has received the message.Whilst this solution may work well for low-complexity tasks, it may be difficult to keeptrack of in high-complexity tasks.
And it will notwork if the system cannot recognise input onany sub-task all the time.Very roughly speaking, clarification meta-communication is more difficult to design forthan repair meta-communication, and user-initiated clarification meta-communication ismore difficult to design for than system-initiatedclarification meta-communication.
System-initiated clarification is needed when the user?sinput is inconsistent, ambiguous orunderspecified.
In such cases, the system mustask for clarification, for instance by pointing outthat an expression is inconsistent.
User-initiatedclarification is needed whenever the systemproduces inconsistent or ambiguous utterances,or uses terms with which the user is not familiar.Unfortunately, handling user clarificationquestions is difficult for SLDSs and the systemdevelopers might not have discovered all thepotential problems in the first place.
If they had,they could have tried to prevent all or most ofthe problems from occurring through adequateoutput phrasing or other means.
Due to thenature of their domain, some tasks inherentlyrequire facilities for clarifying the terminologyused because it may not be a practical option fore.g.
a car sales system to explain all domainterms as it goes along.Most SLDSs need abilities for handlingsystem- and user-initiated repair, and manySLDSs need system-initiated clarificationabilities.
There is no simple decision procedurefor deciding which mechanisms to include in aparticular SLDS.
Sensible decisions very muchdepend on factors such as domain, taskcomplexity, user population and peculiarities ofuser behaviour which can only be discoveredthrough interaction data analysis.6.13 Sufficiency of adaptation to userdifferencesIt is useful to distinguish between four types ofuser: system expert/domain expert, systemexpert/domain novice, system novice/domainexpert and system novice/domain novice.
AnSLDS needs not support all four groups, ofcourse.
If the target user group is domain andsystem experts only, then, obviously, the systemis not a walk-up-and-use system and thus fallsoutside the group of SLDSs considered in thispaper.
If the primary target group is systemnovice users, on-line instructions and other helpinformation is likely to be needed.
This needtends to increase even further when the systemnovices are also domain novices who needexplanation of domain technicalities.Given the relative simplicity of currentSLDSs, walk-up-and-use users may quicklybecome (system) experts.
This means thatinteraction should be supported and facilitatedfor both system novices and system experts.Special shortcuts for expert interaction can be agood solution.
Such shortcuts include e.g.introductions which can be skipped easilythrough barge-in or explicit de-selection.6.14 Number of interaction problemsLack of co-operativity in the system?s outputmay be diagnosed from the occurrence ofcommunication problems in simulated or realuser-system interaction.
Data capture andanalysis is costly, however, especially becauselarge amounts of data may be needed fortriggering most of the communication problemswhich the system is likely to cause.
To reducecost, and to help identify those kinds of lack ofcooperativity which are less likely to causecommunication problems, CODIAL may beused both for walk-throughs through theinteraction design prior to data capture and forthe actual data analysis.6.15 User satisfactionObjectively measured quality, technical andotherwise, does have an impact on usersatisfaction, but this is far from being the wholestory.
User satisfaction is inherently subjective,building on personal preferences and contextualfactors.
Unfortunately, some of the mostdifficult usability issues exactly concerncontextual adequacy, i.e.
adequacy of the full setof contextual factors which contribute to makingan SLDS acceptable to its users.
These factorsremain insufficiently explored both as regardswhich they are and as regards their individualcontributions to user satisfaction.
It is possiblethat contextual factors, such as serviceimprovements or economical benefits, areamong the most important factors influencingusers?
satisfaction with SLDSs.Much still remains to be discovered abouthow the behaviour of SLDSs affect thesatisfaction of their users.
Therefore, subjectiveevaluation remains a cornerstone in SLDSevaluation.
User questionnaires and interviewsremain core tools for gathering information onuser satisfaction.7 Conclusion and future workWe have presented a brief guide to practicalevaluation of walk-up-and-use SLDSs forshared-goal tasks followed by a set of usabilityevaluation criteria.
Within this framework, manyissues remain unresolved or even unaddressed.Deployment usability issues are still poorlyunderstood as are the usability issues arisingfrom multimodal and natural interactiveapplications which integrate speech-only SLDSsinto larger systems.
Usability questionnairedesign remains poorly understood.
The sameapplies to cultural differences in the perceptionof SLDS usability.Much work remains to be done before wehave a solid all-round understanding of usabilityevaluation of SLDSs.
The authors work in tworecently started projects on walk-up-and-useSLDSs, one commercial and one researchsystem, in which the guidelines presented abovewill be tested and, very likely, extended.ReferencesBernsen, N. O., Dybkj?r, H. and Dybkj?r, L.1998.
Designing Interactive Speech Systems.From First Ideas to User Testing.
Berlin,Springer.Bernsen, N. O. and Dybkj?r, L. 2000.
AMethodology for Evaluating Spoken LanguageDialogue Systems and Their Components.Proceedings of LREC 2000, Athens, May 2000,183-188.Cole, R. A., Mariani, J., Uszkoreit, H.,Zaenen, A. and Zue, V. W. (Editorial Board),Varile, G. and Zampolli, A.
(Managing Editors).1996.
Survey of the State of the Art in HumanLanguage Technology.
Sponsors: NationalScience Foundation, Directorate XIII-E of theCommission of the European Communities,Center for Spoken Language Understanding,Oregon Graduate Institute.
URL:http://www.cse.ogi.edu/CSLU/HLTsurvey/.Dybkj?r, L. and Bernsen, N.O.
2000.Usability Issues in Spoken Language DialogueSystems.
In Natural Language Engineering,Special Issue on Best Practice in SpokenLanguage Dialogue System Engineering,Volume 6 Parts 3 & 4 September 2000, 243-272.EAGLES.
1996.
Evaluation of NaturalLanguage Processing Systems.
Final Report,EAGLES Document EAG-EWG-PR2.Copenhagen, Center for Sprogteknologi,http://issco-www.unige.ch/projects/eagles/ewg99/index.htmlFailenschmid, K., Williams, D., Dybkj?r, L.and Bernsen, N. O.
1999.
Draft proposal on bestpractice methods and procedures in humanfactors.
DISC Deliverable D3.6.http://www.disc2.dk.Gaizauskas, R.
(Ed.)
1997.
Proceedings ofthe SALT Club Workshop on Evaluation inSpeech and Language Technology, Sheffield.Gibbon, D., Moore, R. and Winski, R.
(Eds.).1997.
Handbook of standards and resources forspoken language systems.
Mouton de Gruyter,Berlin, New York.Hirschmann, L. and Thompson, H. S. 1996.Overview of evaluation in speech and naturallanguage processing.
In Cole et al 1996, Section13.1.Karlsson, I.
1999.
Draft proposal on bestpractice methods and procedures in speechgeneration.
DISC Deliverable D3.3.http://www.disc2.dkSparck Jones, K. and Galliers, J.
1996.Evaluating natural language processingsystems.
Lecture Notes in Artificial Intelligence1083.
Berlin, Springer.Walker, M. A., Litman, D. J., Kamm, C. A.and Abella, A.
1997.
Evaluating interactivedialogue systems: Extending componentevaluation to integrated system evaluation.Proceedings of the ACL/EACL Workshop onSpoken Dialog Systems , Madrid, 1-8.
