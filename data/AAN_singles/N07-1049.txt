Proceedings of NAACL HLT 2007, pages 388?395,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsTree Revision Learning for Dependency ParsingGiuseppe AttardiDipartimento di InformaticaUniversita` di PisaPisa, Italyattardi@di.unipi.itMassimiliano CiaramitaYahoo!
Research BarcelonaBarcelona, Spainmassi@yahoo-inc.comAbstractWe present a revision learning model forimproving the accuracy of a dependencyparser.
The revision stage corrects the out-put of the base parser by means of revi-sion rules learned from the mistakes ofthe base parser itself.
Revision learningis performed with a discriminative classi-fier.
The revision stage has linear com-plexity and preserves the efficiency of thebase parser.
We present empirical evalu-ations on the treebanks of two languages,which show effectiveness in relative errorreduction and state of the art accuracy.1 IntroductionA dependency parse tree encodes useful semantic in-formation for several language processing tasks.
De-pendency parsing is a simpler task than constituentparsing, since dependency trees do not have ex-tra non-terminal nodes and there is no need for agrammar to generate them.
Approaches to depen-dency parsing either generate such trees by consid-ering all possible spanning trees (McDonald et al,2005), or build a single tree on the fly by means ofshift-reduce parsing actions (Yamada & Matsumoto,2003).
In particular, Nivre and Scholz (2004) andAttardi (2006) have developed deterministic depen-dency parsers with linear complexity, suitable forprocessing large amounts of text, as required, for ex-ample, in information retrieval applications.We investigate a novel revision approach todependency parsing related to re-ranking andtransformation-based methods (Brill, 1993; Brill,1995; Collins, 2000; Charniak & Johnson, 2005;Collins & Koo, 2006).
Similarly to re-ranking, thesecond stage attempts to improve the output of abase parser.
Instead of re-ranking n-best candi-date parses, our method works by revising a sin-gle parse tree, either the first-best or the one con-structed by a deterministic shift-reduce parser, as intransformation-based learning.
Parse trees are re-vised by applying rules which replace incorrect withcorrect dependencies.
These rules are learned bycomparing correct parse trees with incorrect treesproduced by the base parser on a training corpus.We use the same training corpus on which the baseparser was trained, but this need not be the case.Hence, we define a new learning task whose outputspace is a set of revision rules and whose input isa set of features extracted at each node in the parsetrees produced by the parser on the training corpus.A statistical classifier is trained to solve this task.The approach is more suitable for dependencyparsing since trees do not have non-terminal nodes,therefore revisions do not require adding/removingnodes.
However, the method applies to any parsersince it only analyzes output trees.
An intuitive mo-tivation for this method is the observation that adependency parser correctly identifies most of thedependencies in a tree, and only local correctionsmight be necessary to produce a correct tree.
Per-forming several parses in order to generate multipletrees would often just repeat the same steps.
Thiscould be avoided by focusing on the points where at-tachments are incorrect.
In the experiments reportedbelow, on average, the revision stage performs 4.28388corrections per sentence, or one every 6.25 tokens.In our implementation we adopt a shift-reduceparser which minimizes computational costs.
Theresulting two-stage parser has complexity O(n), lin-ear in the length of the sentence.
We evaluated ourmodel on the treebanks of English and Swedish.
Theexperimental results show a relative error reductionof, respectively, 16% and 11% with respect to thebase parser, achieving state of accuracy on Swedish.2 Dependency parsingDetection of dependency relations can be usefulin tasks such as information extraction (Culotta &Sorensen, 2004), lexical acquisition (Snow et al,2005), ontology learning (Ciaramita et al, 2005),and machine translation (Ding & Palmer, 2005).A dependency parser is trained on a corpus an-notated with lexical dependencies, which are eas-ier to produce by annotators without deep linguis-tic knowledge and are becoming available in manylanguages (Buchholz & Marsi, 2006).
Recent de-velopments in dependency parsing show that deter-ministic parsers can achieve good accuracy (Nivre &Scholz, 2004), and high performance, in the range ofhundreds of sentences per second (Attardi, 2006).A dependency parser takes as input a sentences and returns a dependency graph G. Let D ={d1, d2, ..., dm} be the set of permissible depen-dency types.
A dependency graph for a sentences = ?s1, s2, ..., sn?
is a labeled directed graph G =(s,A), such that:(a) s is the set of nodes, corresponding to the to-kens in the input string;(b) A is a set of labeled arcs (wi, d, wj), wi,j ?
s,d ?
D; wj is called the head, wi the modifierand d the dependency label;(c) ?wi ?
s there is at most one arc a ?
A, suchthat a = (wi, d, wj);(d) there are no cycles;In statistical parsing a generator (e.g.
aPCFG) is used to produce a number of candidatetrees (Collins, 2000) with associated scores.
Thisapproach has been used also for dependency parsing,generating spanning trees as candidates and comput-ing the maximum spanning tree using discriminativelearning algorithms (McDonald et al, 2005).Shift ?S,n|I,T,A??n|S,I,T,A?
(1)Right ?s|S,n|I,T,A??S,n|I,T,A?{(s,r,n)}?
(2)Left ?s|S,n|I,T,A??S,s|I,T,A?{(n,r,s)}?
(3)Right2?s1|s2|S,n|I,T,A??s1|S,n|I,T,A?{(s2,r,n)}?(4)Left2?s1|s2|S,n|I,T,A??s2|S,s1|I,T,A?{(n,r,s2)}?(5)Right3?s1|s2|s3|S,n|I,T,A??s1|s2|S,n|I,T,A?{(s3,r,n)}?(6)Left3?s1|s2|s3|S,n|I,T,A??s2|s3|S,s1|I,T,A?{(n,r,s3)}?
(7)Extract ?s1|s2|S,n|I,T,A??n|s1|S,I,s2|T,A?
(8)Insert ?S,I,s1|T,A??s1|S,I,T,A?
(9)Table 1.
The set of parsing rules of the base parser.Yamada and Matsumoto (2003) have proposed analternative approach, based on deterministic bottom-up parsing.
Instead of learning directly which treeto assign to a sentence, the parser learns whichShift/Reduce actions to use for building the tree.Parsing is cast as a classification problem: at eachstep the parser applies a classifier to the features rep-resenting its current state to predict the next action toperform.
Nivre and Scholz (2004) proposed a vari-ant of the model of Yamada and Matsumoto that re-duces the complexity from the worst case quadraticto linear.
Attardi (2006) proposed a variant of therules that allows deterministic single-pass parsingand as well as handling non-projective relations.Several approaches to dependency parsing on multi-ple languages have been evaluated in the CoNLL-XShared Task (Buchholz & Marsi, 2006).3 A shift-reduce dependency parserAs a base parser we use DeSR, a shift-reduceparser described in (Attardi, 2006).
The parserconstructs dependency trees by scanning input sen-tences in a single left-to-right pass and performingShift/Reduce parsing actions.
The parsing algorithmis fully deterministic and has linear complexity.
Itsbehavior can be described as repeatedly selectingand applying some parsing rules to transform itsstate.The state of the parser is represented by a quadru-389ple ?S, I, T,A?
: S is the stack, I is the list of (re-maining) input tokens, T is a stack of saved to-kens and A is the arc relation for the dependencygraph, consisting of a set of labeled arcs (wi, r, wj),wi, wj ?
W (the set of tokens), and d ?
D (theset of dependencies).
Given an input sentence s,the parser is initialized to ?
?, s, ?, ?
?, and terminateswhen it reaches the configuration ?s, ?, ?, A?.Table 1 lists all parsing rules.
The Shift ruleadvances on the input, while the various Left,Right variants create links between the next in-put token and some previous token on the stack.Extract/Insert generalize the previous rules byrespectively moving one token to the stack T andreinserting the top of T into S. An essential differ-ence with respect to the rules of Yamada and Mat-sumoto (2003) is that the Right rules move back tothe input the top of the stack, allowing some furtherprocessing on it, which would otherwise require asecond pass.
The extra Left and Right rules (4-7, Table 1), and the ExtractInsert rules (8 and9, Table 1), are new rules added for handling non-projective trees.
The algorithm works as follows:Algorithm 1: DeSRinput: s = w1, w2, ..., wnbeginS ?
?
?I ?
?w1, w2, ..., wn?T ?
??A?
?
?while I 6= ??
dox?
getContext(S, I, T,A)y ?
estimateAction(w,x)performAction(y, S, I, T,A)endThe function getContext() extracts a vector xof contextual features around the current token, i.e.,from a subset of I and S. estimateAction() pre-dicts a parsing action y given a trained modelw andx.
In the experiments presented below, we used asfeatures the lemma, Part-of-Speech, and dependencytype of the following items:?
2 top items from S;?
4 items from I;Step Descriptionr Up to root nodeu Up one parent?n Left to the n-th token+n Right to the n-th token[ Head of previous constituent] Head of following constituent> First token of previous constituent< First token of following constituentd??
Down to the leftmost childd + + Down to the rightmost childd?
1 Down to the first left childd + 1 Down to the first right childdP Down to token with POS PTable 2.
Description of the atomic movements allowed onthe graph relatively to a token w.?
2 leftmost and 2 rightmost children from thetop of S and I .4 Revising parse treesThe base parser is fairly accurate and even whenthere are mistakes most sentence chunks are correct.The full correct parse tree can often be recovered byperforming just a small number of revisions on thebase parse.
We propose to learn these revisions andto apply them to the single best tree output by thebase parser.
Such an approach preserves the deter-ministic nature of the parser, since revising the treerequires a second sequential step over the whole sen-tence.
The second step may also improve accuracyby incorporating additional evidence, gathered fromthe analysis of the tree which is not available duringthe first stage of parsing.Our approach introduces a second learning taskin which a model is trained to revise parse trees.Several questions needs to be addressed: which treetransformations to use in revising the parse tree,how to determine which transformation to apply, inwhich order, and which features to use for learning.4.1 Basic graph movementsWe define a revision as a combination of atomicmoves on a graph; e.g., moving a link to the follow-ing or preceding token in the sentence, up or downthe graph following the directed edges.
Table 2 sum-marizes the set of atomic steps we used.390Figure 1.
An incorrect dependency tree: the dashed arrow from ?sale?
to ?by?
should be replaced with the one from?offered?
to ?by?.4.2 Revision rulesA revision rule is a sequence of atomic steps on thegraph which identifies the head of a modifier.
As anexample, Figure 1 depicts a tree in which the mod-ifier ?by?
is incorrectly attached to the head ?sale?
(dashed arrow), rather than to the correct head ?of-fered?
(continuous arrow)1.
There are several possi-ble revision rules for this case: ?uu?, move up twonodes; ?3, three tokens to the left, etc.
To boundthe complexity of feature extraction the maximumlength of a sequence is bound to 4.
A revision fora dependency relation is a link re-direction, whichmoves a single link in a tree to a different head.
Thisis an elementary transformation which preserves thenumber of nodes in the tree.A possible problem with these rules is that theyare not tree-preserving, i.e.
a tree may become acyclic graph.
For instance, rules that create a linkto a descendant introduce cycles, unless the appli-cation of another rule will link one of the nodes inthe path to the descendant to a node outside the cy-cle.
To address these issues we apply the followingheuristics in selecting the proper combination: rulesthat redirect to child nodes are chosen only whenno other rule is applicable (upwards rule are safe),and shorter rules are preferred over longer ones.
Inour experiments we never observed the productionof any cycles.On Wall Street Journal Penn Treebank section 22we found that the 20 most frequent rules are suffi-cient to correct 80% of the errors, see Table 3.
Thisconfirms that the atomic movements produce simpleand effective revision rules.1Arrows go from head to modifier as agreed among the par-ticipants to the CoNLL-X shared task.COUNTS RULE TARGET LOCATION983 uu Up twice685 -1 Token to the left469 +1 Token to the right265 [ Head of previous constituent215 uuu Up 3 times197 +1u Right, up194 r To root174 -1u Left, up116 >u Token after constituent, up103 ud??
Up down to leftmost child90 V To 1st child with POS verb83 d+1 Down to first right child82 uuuu Up 4 times74 < Token before constituent73 ud+1 Up down to 1st right child71 uV Up, down to 1st verb61 ud-1 Up, down to last left child56 ud+1d+1 Up, down to 1st right child twice55 d+1d+1 Down to 1st right child twice48 d??
Down to leftmost childTable 3.
20 most frequent revision rules in wsj22.4.3 Tree revision problemThe tree revision problem can be formalized as fol-lows.
Let G = (s,A) be a dependency tree forsentence s = ?w1, w2, ..., wn?.
A revision rule isa mapping r : A ?
A which, when applied to anarc a = (wi, d, wj), returns an arc a?
= (wi, d, ws).A revised parse tree is defined as r(G) = (s,A?
)such that A?
= {r(a) : a ?
A}.This definition corresponds to applying the revi-sions to the original tree in a batch, as in (Brill,1993).
Alternatively, one could choose to apply thetransformations incrementally, applying each one tothe tree resulting from previous applications.
Wechose the first alternative, since the intermediatetrees created during the transformation process maynot be well-formed dependency graphs, and analyz-ing them in order to determine features for classifi-391cation might incur problems.
For instance, the graphmight have abnormal properties that differ fromthose of any other graph produced by the parser.Moreover, there might not be enough cases of suchgraphs to form a sufficiently large training set.5 Learning a revision modelWe frame the problem of revising a tree as a super-vised classification task.
Given a training set S =(xi, yi)Ni=1, such that xi ?
IRd and yi ?
Y , our goalis to learn a classifier, i.e., a function F : X ?
Y .The output space represents the revision rules, inparticular we denote with y1 the identity revisionrule.
Features represents syntactic and morphologi-cal properties of the dependency being examined inits context on the graph.5.1 Multiclass perceptronThe classifier used in revision is based on the per-ceptron algorithm (Rosemblatt, 1958), implementedas a multiclass classifier (Crammer & Singer, 2003).One introduces a weight vector ?i ?
IRd for eachyi ?
Y , in which ?i,j represents the weight associ-ated with feature j in class i, and learn ?
with theperceptron from the training data using a winner-take-all discriminant function:F (x) = argmaxy?Y?x, ?y?
(10)The only adjustable parameter in this model is thenumber of instances T to use for training.
We choseT by means of validation on the development data,typically with a value around 10 times the size of thetraining data.
For regularization purposes we adoptan average perceptron (Collins, 2002) which returnsfor each y, ?y = 1T?Tt=1 ?ty, the average of allweight vectors ?ty posited during training.
The per-ceptron was chosen because outperformed other al-gorithms we experimented with (MaxEnt, MBL andSVM), particularly when including feature pairs, asdiscussed later.5.2 FeaturesWe used as features for the revision phase the sametype of features used for training the parser (de-scribed in Section 3).
This does not have to be thecase in general.
In fact, one might want to introducefeatures that are specific for this task.
For example,global features of the full tree which might be notpossible to represent or extract while parsing, as instatistical parse re-ranking (Collins & Koo, 2006).The features used are lemma, Part-of-Speech, anddependency type of the following items: the currentnode, its parent, grandparent, great-grandparent, ofthe children thereof and, in addition, the previousand next tokens of the node.
We also add as featuresall feature pairs that occurred more than 10 times,to reduce the size of the feature space.
In alternativeone could use a polynomial kernel.
We preferred thisoption because, given the large size of the trainingdata, a dual model is often impractical.5.3 Revision modelGiven a dependency graph G = (s,A), for a sen-tence s = ?w1, ..., wn?, the revised tree is R(G) =(s,A?
), where each dependency a?i is equal to F (ai).In other words, the head in ai has been changed, ornot, according to the rule predicted by the classifier.In particular, we assume that revisions are indepen-dent of each other and perform a revision of a treefrom left to right.
As Table 3 suggests, there aremany revision rules with low frequency.
Rather thanlearning a huge classifier, for rules with little train-ing data, we limit the number of classes to a valuek.
We experimented with values between 30 and50, accounting for 98-99% of all rules, and even-tually used 50, by experimenting with the develop-ment portion of the data.
All rules that fall outsidethe threshold are collected in a single class y0 of ?un-resolved?
cases.
If predicted, y0, similarly to y1, hasno effect on the dependency.Occasionally, in 59 sentences out of 2416 onsection 23 of the Wall Street Journal Penn Tree-bank (Marcus et al, 1993), the shift-reduce parserfails to attach a node to a head, producing a dis-connected graph.
The disconnected node will ap-pear as a root, having no head.
The problem occursmost often on punctuations (66/84 on WSJ section23), so it affects only marginally the accuracy scores(UAS, LAS) as computed in the CoNLL-X evalua-tion (Buchholz & Marsi, 2006).
A final step of therevision deals with multiple roots, using a heuristicrule it selects one of the disconnected sub-trees asroot, a verb, and attaches all sub-trees to it.392Figure 2.
Frequency of the 30 most frequent rules ob-tained with different parsers on wsj22 and wsj2-21.5.4 Algorithm complexityThe base dependency parser is deterministic and per-forms a single scan over the sentence.
For each wordit performs feature extraction and invokes the classi-fier to predict the parsing action.
If prediction time isbound by a constant, as in linear classifiers, parsinghas linear complexity.
The revision pass is deter-ministic and performs similar feature extraction andprediction on each token.
Hence, the complexity ofthe overall parser is O(n).
In comparison, the com-plexity of McDonald?s parser (2006) is cubic, whilethe parser of Yamada and Matsumoto (2003) has aworst case quadratic complexity.6 Experiments6.1 Data and setupWe evaluated our method on English using the stan-dard partitions of the Wall Street Journal Penn Tree-bank: sections 2-21 for training, section 22 fordevelopment, and section 23 for evaluation.
Theconstituent trees were transformed into dependencytrees by means of a script implementing rules pro-posed by Collins and Yamada2.
In a second eval-uation we used the Swedish Treebank (Nilsson etal., 2005) from CoNLL-X, approximately 11,000sentences; for development purposes we performedcross-validation on the training data.We trained two base parsers on the Penn Tree-bank: one with our own implementation of Maxi-2http://w3.msi.vxu.se/%7enivre/research/Penn2Malt.htmlParser UAS LASDeSR-ME 84.96 83.53DeSR-MBL 88.41 86.85Revision-MBL 89.11 86.39Revision-ME 90.27 86.44N&S 87.3 -Y&M 90.3 -MST-2 91.5 -Table 4.
Results on the Wall Street Journal Penn Tree-bank.mum Entropy, one with the TiMBL library for Mem-ory Based Learning (MBL, (Timbl, 2003)).
Weparsed sections 2 to 21 with each parser and pro-duced two datasets for training the revision model:?wsj2-21.mbl?
and ?wsj2-21.me?.
Each depen-dency is represented as a feature vector (cf.
Sec-tion 5.2), the prediction is a revision rule (cf.
Sec-tion 4.2).
For the smaller Swedish data we trainedone base parser with MaxEnt and one with the SVMimplementation in libSVM (Chang & Lin, 2001) us-ing a polynomial kernel with degree 2.6.2 ResultsOn the Penn Treebank, the base parser trained withMBL (DeSR-MBL) achieves higher accuracy, 88.41unlabeled accuracy score (UAS), than the sameparser trained with MaxEnt (DeSR-ME), 84.96UAS.
The revision model trained on ?wsj2-21.me?
(Revision-ME) increases the accuracy of DeSR-MEto 88.01 UAS (+3%).
The revision model trainedon ?wsj2-21.mbl?
(DeSR-MBL) improves the accu-racy of DeSR-MBL from 88.42 to 89.11 (+0.7%).The difference is mainly due to the fact that DeSR-MBL is quite accurate on the training data, almost99%, hence ?wsj2-21.mbl?
contains less errors onwhich to train the revision parser.
This is typi-cal of the memory-based learning algorithm usedin DeSR-MBL.
Conversely, DeSR-ME achieves ascore of of 85% on the training data, which iscloser to the actual accuracy of the parser on unseendata.
As an illustration, Figure 2 plots the distri-butions of revision rules in ?wsj2-21.mbl?
(DeSR-MBL), ?wsj2-21.me?
(DeSR-ME), and ?wsj22.mbl?
(DeSR-MBL) which represents the distribution ofcorrect revision rules on the output of DeSR-MBLon the development set.
The distributions of ?wsj2-393Parser UAS LASDeSR-SVM 88.41 83.31Revision-ME 89.76 83.13Corston-Oliver& Aue 89.54 82.33Nivre 89.50 84.58Table 5.
Results on the Swedish Treebank.21.me?
and ?wsj22.mbl?
are visibly similar, while?wsj2-21.mbl?
is significantly more skewed towardsnot revising.
Hence, the less accurate parser DeSR-ME might be more suitable for producing revisiontraining data.
Applying the revision model trainedon ?wsj2-21.me?
(Revision-ME) to the output ofDeSR-MBL the result is 90.27% UAS.
A relativeerror reduction of 16.05% from the previous 88.41UAS of DeSR-MBL.
This finding suggests that itmay be worth while experimenting with all possi-ble revision-model/base-parser pairs as well as ex-ploring alternative ways for generating data for therevision model; e.g., by cross-validation.Table 4 summarizes the results on the Penn Tree-bank.
Revision models are evaluated on the outputof DeSR-MBL.
The table also reports the scores ob-tained on the same data set by by the shift reduceparsers of Nivre and Scholz?s (2004) and YamadaandMatsumoto (2003), andMcDonald and Pereira?ssecond-order maximum spanning tree parser (Mc-Donald & Pereira, 2006).
However the scores arenot directly comparable, since in our experimentswe used the settings of the CoNLL-X Shared Task,which provide correct POS tags to the parser.On the Swedish Treebank collection we traineda revision model (Revision-ME) on the output ofthe MaxEnt base parser.
We parsed the evalua-tion data with the SVM base parser (DeSR-SVM)which achieves 88.41 UAS.
The revision modelachieves 89.76 UAS, with a relative error reduc-tion of 11.64%.
Here we can compare directly withthe best systems for this dataset in CoNLL-X.
Thebest system (Corston-Oliver & Aue, 2006), a vari-ant of the MST algorithm, obtained 89.54 UAS,while the second system (Nivre, 2006) obtained89.50; cf.
Table 5.
Parsing the Swedish evalua-tion set (about 6,000 words) DeSR-SVM processes1.7 words per second on a Xeon 2.8Ghz machine,DeSR-ME parses more than one thousand w/sec.
Inthe revision step Revision-ME processes 61 w/sec.7 Related workSeveral authors have proposed to improve parsingvia re-ranking (Collins, 2000; Charniak & Johnson,2005; Collins & Koo, 2006).
The base parser pro-duces a list of n-best parse trees for a sentence.
There-ranker is trained on the output trees, using addi-tional global features, with a discriminative model.These approaches achieve error reductions up to13% (Collins & Koo, 2006).
In transformation-based learning (Brill, 1993; Brill, 1995; Satta &Brill, 1995) the learning algorithm starts with abaseline assignment, e.g., the most frequent Part-of-Speech for a word, then repeatedly applies rewritingrules.
Similarly to re-ranking our method aims atimproving the accuracy of the base parser with anadditional learner.
However, as in transformation-based learning, it avoids generating multiple parsesand applies revisions to arcs in the tree which it con-siders incorrect.
This is consistent with the architec-ture of our base parser, which is deterministic andbuilds a single tree, rather than evaluating the bestoutcome of a generator.With respect to transformation-based methods,our method does not attempt to build a tree but onlyto revise it.
That is, it defines a different output spacefrom the base parser?s: the possible revisions on thegraph.
The revision model of Nakagawa et al (2002)applies a second classifier for deciding whether thepredictions of a base learner are accurate.
However,the model only makes a binary decision, which issuitable for the simpler problem of POS tagging.The work of Hall and Novak (Hall & Novak, 2005)is the closest to ours.
Hall and Novak develop a cor-rective model for constituency parsing in order torecover non-projective dependencies, which a stan-dard constituent parser does not handle.
The tech-nique is applied to parsing Czech.8 ConclusionWe presented a novel approach for improving theaccuracy of a dependency parser by applying re-vision transformations to its parse trees.
Experi-mental results prove that the approach is viable andpromising.
The proposed method achieves good ac-curacy and excellent performance using a determin-istic shift-reduce base parser.
As an issue for furtherinvestigation, we mention that in this framework, as394in re-ranking, it is possible to exploit global featuresin the revision phase; e.g., semantic features such asthose produced by named-entity detection systems.AcknowledgmentsWe would like to thank Jordi Atserias and BrianRoark for useful discussions and comments.ReferencesG.
Attardi.
2006.
Experiments with a MultilanguageNon-Projective Dependency Parser.
In Proceedings ofCoNNL-X 2006.S.
Buchholz and E. Marsi.
2006.
Introduction toCoNNL-X Shared Task on Multilingual DependencyParsing.
In Proceedings of CoNNL-X 2006.E.
Brill.
1993.
Automatic Grammar Induction and Pars-ing free Text: A Transformation-Based Approach.
InProceedings of ACL 1993.E.
Brill.
1995.
Transformation-Based Error-DrivenLearning and Natural Language Processing.
Compu-tational Linguistics 21(4): pp.543-565.E.
Charniak and M. Johnson.
2005.
Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking.In Proceedings of ACL 2005.C.
Chang and C. Lin.
2001.
LIBSVM: A Libraryfor Support Vector Machines.
Software available athttp://www.csie.ntu.edu.tw/ cjlin/libsvm.M.
Ciaramita, A. Gangemi, E. Ratsch, J. Saric?
and I. Ro-jas.
2005.
Unsupervised Learning of Semantic Rela-tions between Concepts of a Molecular Biology Ontol-ogy.
In Proceedings of IJCAI 2005.M.
Collins.
2000.
Discriminative Reranking for NaturalLanguage Parsing.
In Proceedings of ICML 2000.M.
Collins.
2002.
Discriminative Training Meth-ods for Hidden Markov Models: Theory and Experi-ments with Perceptron Algorithms.
In Proceedings ofEMNLP 2002.M.
Collins and T. Koo.
2006.
Discriminative Rerankingfor Natural Language Parsing.
Computational Lin-guistics 31(1): pp.25-69.K.
Crammer and Y.
Singer.
2003.
Ultraconservative On-line Algorithms for Multiclass Problems.
Journal ofMachine Learning Research 3: pp.951-991.S.
Corston-Oliver and A. Aue.
2006.
Dependency Pars-ing with Reference to Slovene, Spanish and Swedish.In Proceedings of CoNLL-X.A.
Culotta and J. Sorensen.
2004.
Dependency Tree Ker-nels for Relation Extraction.
In Proceedings of ACL2004.W.
Daelemans, J. Zavrel, K. van der Sloot, andA.
van den Bosch.
2003.
Timbl: Tilburg memorybased learner, version 5.0, reference guide.
TechnicalReport ILK 03-10, Tilburg University, ILK.Y.
Ding and M. Palmer.
2005.
Machine Translation us-ing Probabilistic Synchronous Dependency InsertionGrammars.
In Proceedings of ACL 2005.K.
Hall and V. Novak.
2005.
Corrective Modeling forNon-Projective Dependency Parsing.
In Proceedingsof the 9th International Workshop on Parsing Tech-nologies.M.
Marcus, B. Santorini and M. Marcinkiewicz.
1993.Building a Large Annotated Corpus of English: ThePenn Treebank.
Computational Linguistics, 19(2): pp.313-330.R.
McDonald, F. Pereira, K. Ribarov and J. Hajic?.
2005.Non-projective Dependency Parsing using SpanningTree Algorithms.
In Proceedings of HLT-EMNLP2005.R.
McDonald and F. Pereira.
2006.
Online Learningof Approximate Dependency Parsing Algorithms.
InProceedings of EACL 2006.T.
Nakagawa, T. Kudo and Y. Matsumoto.
2002.
Revi-sion Learning and its Applications to Part-of-SpeechTagging.
In Proceedings of ACL 2002.J.
Nilsson, J.
Hall and J. Nivre.
2005.
MAMBA MeetsTIGER: Reconstructing a Swedish Treebank from An-tiquity.
In Proceedings of the NODALIDA.J.
Nivre and M. Scholz.
2004.
Deterministic Depen-dency Parsing of English Text.
In Proceedings ofCOLING 2004.J.
Nivre.
2006.
Labeled Pseudo-Projective DependencyParsing with Support Vector Machines.
In Proceed-ings of CoNLL-X.F.
Rosemblatt.
1958.
The Perceptron: A ProbabilisticModel for Information Storage and Organization in theBrain.
Psych.
Rev., 68: pp.
386-407.G.
Satta and E. Brill.
1995, Efficient Transformation-Based Parsing.
In Proceedings of ACL 1996.R.
Snow, D. Jurafsky and Y. Ng 2005.
Learning Syn-tactic Patterns for Automatic Hypernym Discovery.
InProceedings of NIPS 17.H.
Yamada and Y. Matsumoto.
2003.
Statistical De-pendency Analysis with Support Vector Machines.In Proceedings of the 9th International Workshop onParsing Technologies.395
