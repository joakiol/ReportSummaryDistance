Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 969?978,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsText Segmentation by Language Using Minimum Description LengthHiroshi YamaguchiGraduate School ofInformation Science and Technology,University of Tokyoyamaguchi.hiroshi@ci.i.u-tokyo.ac.jpKumiko Tanaka-IshiiFaculty and Graduate School of InformationScience and Electrical Engineering,Kyushu Universitykumiko@ait.kyushu-u.ac.jpAbstractThe problem addressed in this paper is to seg-ment a given multilingual document into seg-ments for each language and then identify thelanguage of each segment.
The problem wasmotivated by an attempt to collect a largeamount of linguistic data for non-major lan-guages from the web.
The problem is formu-lated in terms of obtaining the minimum de-scription length of a text, and the proposed so-lution finds the segments and their languagesthrough dynamic programming.
Empirical re-sults demonstrating the potential of this ap-proach are presented for experiments usingtexts taken from the Universal Declaration ofHuman Rights and Wikipedia, covering morethan 200 languages.1 IntroductionFor the purposes of this paper, a multilingual textmeans one containing text segments, limited to thoselonger than a clause, written in different languages.We can often find such texts in linguistic resourcescollected from the World Wide Web for many non-major languages, which tend to also contain portionsof text in a major language.
In automatic process-ing of such multilingual texts, they must first be seg-mented by language, and the language of each seg-ment must be identified, since many state-of-the-artNLP applications are built by learning a gold stan-dard for one specific language.
Moreover, segmen-tation is useful for other objectives such as collectinglinguistic resources for non-major languages and au-tomatically removing portions written in major lan-guages, as noted above.
The study reported here wasmotivated by this objective.
The problem addressedin this article is thus to segment a multilingual textby language and identify the language of each seg-ment.
In addition, for our objective, the set of targetlanguages consists of not only major languages butalso many non-major languages: more than 200 lan-guages in total.Previous work that directly concerns the problemaddressed in this paper is rare.
The most similarprevious work that we know of comes from twosources and can be summarized as follows.
First,(Teahan, 2000) attempted to segment multilingualtexts by using text segmentation methods used fornon-segmented languages.
For this purpose, he useda gold standard of multilingual texts annotated byborders and languages.
This segmentation approachis similar to that of word segmentation for non-segmented texts, and he tested it on six differentEuropean languages.
Although the problem set-ting is similar to ours, the formulation and solutionare different, particularly in that our method usesonly a monolingual gold standard, not a multilin-gual one as in Teahan?s study.
Second, (Alex, 2005)(Alex et al, 2007) solved the problem of detectingwords and phrases in languages other than the prin-cipal language of a given text.
They used statisti-cal language modeling and heuristics to detect for-eign words and tested the case of English embed-ded in German texts.
They also reported that suchprocessing would raise the performance of Germanparsers.
Here again, the problem setting is similar toours but not exactly the same, since the embeddedtext portions were assumed to be words.
Moreover,the authors only tested for the specific language pairof English embedded in German texts.
In contrast,our work considers more than 200 languages, andthe portions of embedded text are larger: up to theparagraph level to accommodate the reality of mul-tilingual texts.
The extension of our work to addressthe foreign word detection problem would be an in-teresting future work.From a broader view, the problem addressed inthis paper is further related to two genres of previ-ous work.
The first genre is text segmentation.
Ourproblem can be situated as a sub-problem from theviewpoint of language change.
A more common set-ting in the NLP context is segmentation into seman-tically coherent text portions, of which a represen-tative method is text tiling as reported by (Hearst,1997).
There could be other possible bases for text969segmentation, and our study, in a way, could leadto generalizing the problem.
The second genre isclassification, and the specific problem of text clas-sification by language has drawn substantial atten-tion (Grefenstette, 1995) (Kruengkrai et al, 2005)(Kikui, 1996).
Current state-of-the-art solutions usemachine learning methods for languages with abun-dant supervision, and the performance is usuallyhigh enough for practical use.
This article con-cerns that problem together with segmentation buthas another particularity in aiming at classificationinto a substantial number of categories, i.e., morethan 200 languages.
This means that the amount oftraining data has to remain small, so the methodsto be adopted must take this point into considera-tion.
Among works on text classification into lan-guages, our proposal is based on previous studies us-ing cross-entropy such as (Teahan, 2000) and (Juola,1997).
We explain these works in further detail in?3.This article presents one way to formulate the seg-mentation and identification problem as a combina-torial optimization problem; specifically, to find theset of segments and their languages that minimizesthe description length of a given multilingual text.
Inthe following, we describe the problem formulationand a solution to the problem, and then discuss theperformance of our method.2 Problem FormulationIn our setting, we assume that a small amount (upto kilobytes) of monolingual plain text sample datais available for every language, e.g., the UniversalDeclaration of Human Rights, which serves to gen-erate the language model used for language identifi-cation.
This entails two sub-assumptions.First, we assume that for all multilingual text,every text portion is written in one of the givenlanguages; there is no input text of an unknownlanguage without learning data.
In other words,we use supervised learning.
In line with recenttrends in unsupervised segmentation, the problemof finding segments without supervision could besolved through approaches such as Bayesian meth-ods; however, we report our result for the supervisedsetting since we believe that every segment must belabeled by language to undergo further processing.Second, we cannot assume a large amount oflearning data, since our objective requires us to con-sider segmentation by both major and non-majorlanguages.
For most non-major languages, only alimited amount of corpus data is available.1This constraint suggests the difficulty of applyingcertain state-of the art machine learning methods re-quiring a large learning corpus.
Hence, our formu-lation is based on the minimum description length(MDL), which works with relatively small amountsof learning data.In this article, we use the following terms andnotations.
A multilingual text to be segmented isdenoted as X = x1, .
.
.
, x|X|, where xi denotesthe i-th character of X and |X| denotes the text?slength.
Text segmentation by language refers hereto the process of segmenting X by a set of bordersB = [B1, .
.
.
, B|B|], where |B| denotes the num-ber of borders, and each Bi indicates the locationof a language border as an offset number of charac-ters from the beginning.
Note that a pair of squarebrackets indicates a list.
Segmentation in this paperis character-based, i.e., a Bi may refer to a positioninside a word.
The list of segments obtained fromB is denoted as X = [X0, .
.
.
, X|B|], where the con-catenation of the segments equals X .
The languageof each segment Xi is denoted as Li, where Li ?
L,the set of languages.
Finally, L = [L0, .
.
.
, L|B|]denotes the sequence of languages corresponding toeach segmentXi.
The elements in each adjacent pairin L must be different.We formulate the problem of segmenting a multi-lingual text by language as follows.
Given a multi-lingual text X , the segments X for a list of bordersB are obtained with the corresponding languages L.Then, the total description length is obtained by cal-culating each description length of a segment Xi forthe language Li:(X?, L?)
= argminX,L|B|?i=0dlLi(Xi).
(1)The function dlLi(Xi) calculates the descriptionlength of a text segment Xi through the use of alanguage model for Li.
Note that the actual totaldescription length must also include an additionalterm, log2 |X|, giving information on the numberof segments (with the maximum to be segmented1In fact, our first motivation was to collect a certain amountof corpus data for non-major languages from Wikipedia.970by each character).
Since this term is a commonconstant for all possible segmentations and the min-imization of formula (1) is not affected by this term,we will ignore it.The model defined by (1) is additive for Xi, sothe following formula can be applied to search forlanguage Li given a segment Xi :L?i = argminLi?LdlLi(Xi), (2)under the constraint that Li 6= Li?1 for i ?
{1, .
.
.
|B|}.
The function dl can be further decom-posed as follows to give the description length in aninformation-theoretic manner:dlLi(Xi) =?
log2 PLi(Xi)+ log2 |X|+ log2 |L|+ ?.
(3)Here, the first term corresponds to the code lengthof the text chunk Xi given a language model forLi, which in fact corresponds to the cross-entropyof Xi for Li multiplied by |Xi|.
The remainingterms give the code lengths of the parameters usedto describe the length of the first term: the secondterm corresponds to the segment location; the thirdterm, to the identified language; and the fourth term,to the language model of language Li.
This fourthterm will differ according to the language modeltype; moreover, its value can be further minimizedthrough formula (2).
Nevertheless, since we use auniform amount of training data for every language,and since varying ?
would prevent us from improv-ing the efficiency of dynamic programming, as ex-plained in ?4, in this article we set ?
to a constantobtained empirically.Under this formulation, therefore, when detect-ing the language of a segment as in formula (2), theterms of formula (3) other than the first term will beconstant: what counts is only the first term, simi-larly to much of the previous work explained in thefollowing section.
We thus perform language de-tection itself by minimizing the cross-entropy ratherthan the MDL.
For segmentation, however, the con-stant terms function as overhead and also serve toprohibit excessive decomposition.Next, after briefly introducing methods to calcu-late the first term of formula (3), we explain the so-lution to optimize the combinatorial problem of for-mula (1).3 Calculation of Cross-EntropyThe first term of (3), ?
log2 PLi(Xi), is the cross-entropy of Xi for Li multiplied by |Xi|.
Vari-ous methods for computing cross-entropy have beenproposed, and these can be roughly classified intotwo types based on different methods of univer-sal coding and the language model.
For example,(Benedetto et al, 2002) and (Cilibrasi and Vita?nyi,2005) used the universal coding approach, whereas(Teahan and Harper, 2001) and (Sibun and Reynar,1996) were based on language modeling using PPMand Kullback-Leibler divergence, respectively.In this section, we briefly introduce two meth-ods previously studied by (Juola, 1997) and (Teahan,2000) as representative of the two types, and we fur-ther explain a modification that we integrate into thefinal optimization problem.
We tested several othercoding methods, but they did not perform as well asthese two methods.3.1 Mean of Matching Statistics(Farach et al, 1994) proposed a method to esti-mate the entropy, through a simplified version of theLZ algorithm (Ziv and Lempel, 1977), as follows.Given a text X = x1x2 .
.
.
xixi+1 .
.
., Leni is de-fined as the longest match length for two substringsx1x2 .
.
.
xi and xi+1xi+2 .
.
..
In this article, we de-fine the longest match for two strings A and B as theshortest prefix of string B that is not a substring ofA.
Letting the average of Leni be E [Len], Farachproved that |E [Len]?
log2 iH(X) | probabilistically con-verges to zero as i ?
?, where H(X) indicates theentropy of X .
Then, H(X) is estimated asH?
(X) = log2 iE [Len] .
(Juola, 1997) applied this method to estimate thecross-entropy of two given texts.
For two stringsY = y1y2 .
.
.
y|Y | and X = x1x2 .
.
.
x|X|, letLeni(Y ) be the match length starting from xi of Xfor Y 2.
Based on this formulation, the cross-entropyis approximately estimated asJ?Y (X) =log2 |Y |E [Leni(Y )].2This is called a matching statistics value, which explainsthe subsection title.971Since formula (1) of ?2 is based on adding thedescription length, it is important that the wholevalue be additive to enable efficient optimization (aswill be explained in ?4).
We thus modified Juola?smethod as follows to make the length additive:J?
?Y (X) = E[log2 |Y |Leni(Y )].Although there is no mathematical guarantee thatJ?Y (X) or J?
?Y (X) actually converges to the cross-entropy, our empirical tests showed a good estimatefor both cases3.
In this article, we use J?
?Y (X) asa function to obtain the cross-entropy and for multi-plication by |X| in formula (3).3.2 PPMAs a representative method for calculating thecross-entropy through statistical language model-ing, we adopt prediction by partial matching (PPM),a language-based encoding method devised by(Cleary and Witten, 1984).
It has the particular char-acteristic of using a variable n-gram length, unlikeordinary n-gram models4.
It models the probabilityof a text X with a learning corpus Y as follows:PY (X) = PY (x1 .
.
.
x|X|)=|X|?t=1PY (xt|xt?1 .
.
.
xmax(1,t?n)),where n is a parameter of PPM, denoting the max-imum length of the n-grams considered in themodel5.
The probability PY (X) is estimated by es-cape probabilities favoring the longer sequences ap-pearing in the learning corpus (Bell et al, 1990).The total code length of X is then estimated as?
logPY (X).
Since this value is additive and givesthe total code length of X for language Y , we adoptthis value in our approach.4 Segmentation by Dynamic ProgrammingBy applying the above methods, we propose a solu-tion to formula (1) through dynamic programming.3This modification means that the original J?Y (X) is ob-tained through the harmonic mean, with Len obtainedthrough the arithmetic mean, whereas J?
?Y (X) is obtainedthrough the arithmetic mean with Len as the harmonicmean.4In the context of NLP, this is known as Witten-Bell smooth-ing.5In the experiments reported here, n is set to 5 throughout.Considering the additive characteristic of the de-scription length formulated previously as formula(1), we denote the minimized description length fora given text X simply as DP(X), which can be de-composed recursively as follows6:DP(X) = mint?
{0,...,|X|},L?L{DP(x0 .
.
.
xt?1)+ dlL(xt .
.
.
x|X|)},(4)In other words, the computation of DP(X) is de-composed into obtaining the addition of two termsby searching through t ?
{0, .
.
.
, |X|} and L ?
L.The first term gives theMDL for the first t charactersof textX , while the second term, dlL(xt+1 .
.
.
x|X|),gives the description length of the remaining charac-ters under the language model for L.We can straightforwardly implement this recur-sive computation through dynamic programming, bymanaging a table of size |X| ?
|L|.
To fill a cell ofthis table, formula (4) suggests referring to t ?
|L|cells and calculating the description length of therest of the text forO(|X|?t) cells for each language.Since t ranges up to |X|, the brute-force computa-tional complexity is O(|X|3 ?
|L|2).The complexity can be greatly reduced, however,when the function dl is additive.
First, the de-scription length can be calculated from the previ-ous result, decreasing O(|X| ?
t) to O(1) (to ob-tain the code length of an additional character).
Sec-ond, the referred number of cells t ?
|L| is in factU ?
|L|, with U  |X|: for MMS, U can beproven to be O(log |Y |), where |Y | is the maximumlength among the learning corpora; and for PPM, Ucorresponds to the maximum length of an n-gram.Third, this factor U ?
|L| can be further decreasedto U ?
2, since it suffices to possess the results forthe two7 best languages in computing the first termof (4).
Consequently, the complexity decreases toO(U ?
|X| ?
|L|).6This formula can be used directly to generate a set L inwhich all adjacent elements differ.
The formula can also beused to generate segments for which some adjacent lan-guages coincide and then further to generate L throughpost-processing by concatenating segments of the samelanguage.7This number means the two best scores for different lan-guages, which is required to obtain L directly: in additionto the best score, if the language of the best coincides withL in formula (4), then the second best is also needed.
Ifsegments are subjected to post-processing, this value canbe one.972Table 1: Number of languages for each writing systemcharacter kinds UDHR WikiLatin 260 158Cyrillic 12 20Devanagari 0 8Arabic 1 6Other 4 305 Experimental Setting5.1 Monolingual Texts (Training / Test Data)In this work, monolingual texts were used both fortraining the cross-entropy computation and as testdata for cross-validation: the training data does notcontain any test data at all.
Monolingual texts werealso used to build multilingual texts, as explained inthe following subsection.Texts were collected from the World Wide Weband consisted of two sets.
The first data set con-sisted of texts from the Universal Declaration ofHuman Rights (UDHR)8.
We consider UDHR themost suitable text source for our purpose, since thecontent of every monolingual text in the declarationis unique.
Moreover, each text has the tendencyto maximally use its own language and avoid vo-cabulary from other languages.
Therefore, UDHR-derived results can be considered to provide an em-pirical upper bound on our formulation.
The set Lconsists of 277 languages , and the texts consist ofaround 10,000 characters on average.The second data set was Wikipedia data fromWikipedia Downloads9, denoted as ?Wiki?
in thefollowing discussion.
We automatically assembledthe data through the following steps.
First, tags inthe Wikipedia texts were removed.
Second, shortlines were removed since they typically are not sen-tences.
Third, the amount of data was set to 10,000characters for every language, in correspondencewith the size of the UDHR texts.
Note that thereis a limit to the complete cleansing of data.
Afterthese steps, the set L contained 222 languages withsufficient data for the experiments.Many languages adopt writing systems other thanthe Latin alphabet.
The numbers of languages forvarious representative writing systems are listed inTable 1 for both UDHR and Wiki, while the Ap-8http://www.ohchr.org/EN/UDHR/Pages/Introduction.aspx9http://download.wikimedia.org/pendix at the end of the article lists the actual lan-guages.
Note that in this article, a character meansa Unicode character throughout, which differs froma character rendered in block form for some writingsystems.To evaluate language identification for monolin-gual texts, as will be reported in ?6.1, we conductedfive-times cross-validation separately for both datasets.
We present the results in terms of the averageaccuracy AL, the ratio of the number of texts with acorrectly identified language to |L|.5.2 Multilingual Texts (Test Data)Multilingual texts were needed only to test the per-formance of the proposed method.
In other words,we trained the model only through monolingualdata, as mentioned above.
This differs from themost similar previous study (Teahan, 2000), whichrequired multilingual learning data.The multilingual texts were generated artificially,since multilingual texts taken directly from the webhave other issues besides segmentation.
First, propernouns in multilingual texts complicate the final judg-ment of language and segment borders.
In prac-tical application, therefore, texts for segmentationmust be preprocessed by named entity recognition,which is beyond the scope of this work.
Second, thesizes of text portions in multilingual web texts dif-fer greatly, which would make it difficult to evaluatethe overall performance of the proposed method in auniform manner.Consequently, we artificially generated two kindsof test sets from a monolingual corpus.
The first isa set of multilingual texts, denoted as Test1, suchthat each text is the conjunction of two portions indifferent languages.
Here, the experiment is focusedon segment border detection, which must segmentthe text into two parts, provided that there are twolanguages.
Test1 includes test data for all languagepairs, obtained by five-times cross-validation, giving25?|L|?
(|L|?1) multilingual texts.
Each portionof text for a single language consists of 100 char-acters taken from a random location within the testdata.The second kind of test set is a set of multilingualtexts, denoted as Test2, each consisting of k seg-ments in different languages.
For the experiment, kis not given to the procedure, and the task is to ob-tain k as well as B and L through recursion.
Test2973was generated through the following steps:1.
Choose k from among 1,. .
.
,5.2.
Choose k languages randomly from L, wheresome of the k languages can overlap.3.
Perform five-times cross-validation on the textsof all languages.
Choose a text length ran-domly from {40,80,120,160}, and randomlyselect this many characters from the test data.4.
Shuffle the k languages and concatenate thetext portions in the resultant order.For this Test2 data set, every plot in the graphsshown in ?6.2 was obtained by randomly averaging1,000 tests.By default, the possibility of segmentation is con-sidered at every character offset in a text, whichprovides a lower bound for the proposed method.Although language change within the middle of aword does occur in real multilingual documents,it might seem more realistic to consider languagechange at word borders.
Therefore, in addition tochoosing B from {1, .
.
.
, |X|}, we also tested ourapproach under the constraint of choosing bordersfrom bordering locations, which are the locations ofspaces.
In this case, B is chosen from this subset of{1, .
.
.
, |X|}, and, in step 3 above, text portions aregenerated so as to end at these bordering locations.Given a multilingual text, we evaluate the outputsB and L through the following scores:PB/RB: Precision/recall of the borders detected(i.e., the correct borders detected, divided bythe detected/correct border).PL/RL: Precision/recall of the languages detected(i.e., the correct languages detected, divided bythe detected/correct language).P s and Rs are obtained by changing the param-eter ?
given in formula (3), which ranges over1,2,4,.
.
.
,256 bits.
In addition, we verify the speed,i.e., the average time required for processing a text.Although there are web pages consisting of textsin more than 2 languages, we rarely see a web pagecontaining 5 languages at the same time.
There-fore, Test1 reflects the most important case of 2 lan-guages only, whereas Test2 reflects the case of mul-tiple languages to demonstrate the general potentialof the proposed approach.The experiment reported here might seem like acase of over-specification, since all languages areconsidered equally likely to appear.
Since our mo-tivation has been to eliminate a portion in a major0.70.750.80.850.90.9510  20  40  60  80  100  120  140  160  180  200accuracyinput length (characters)PPM (UDHR)MMS (UDHR)PPM (Wiki)MMS (Wiki)Figure 1: Accuracy of language identification for mono-lingual textslanguage from the text, there could be a formula-tion specific to the problem.
We consider it trivial,however, to specify such a narrow problem withinour formulation, and it will lead to higher perfor-mance than that of the reported results, in any case.Therefore, we believe that our general formulationand experiment show the broadest potential of ourapproach to solving this problem.6 Experimental Results6.1 Language Identification PerformanceWe first show the performance of language identifi-cation using formula (2), which is used as the com-ponent of the text segmentation by language.
Fig-ure 1 shows the results for language identificationof monolingual texts with the UDHR and Wiki testdata.
The horizontal axis indicates the size of the in-put text in characters, the vertical axis indicates theaccuracy AL, and the graph contains four plots10 forMMS and PPM for each set of data.Overall, all plots rise quickly despite the se-vere conditions of a large number of languages(over 200), a small amount of input data, and asmall amount of learning data.
The results showthat language identification through cross-entropy ispromising.Two further global tendencies can be seen.
First,the performance was higher for UDHR than forWiki.
This is natural, since the content of Wikipediais far broader than that of UDHR.
In the case ofUDHR, when the test data had a length of 40 char-acters, the accuracy was over 95% for both the PPMand the MMS methods.
Second, PPM achieved10The results for PPM and MMS for UDHR are almost thesame, so the graph appears to contain only three plots.97400.10.20.30.40.50.60.70.80.91-5 -4 -3 -2 -1  0  1  2  3  4  5cummulativeproportionrelative position (characters)PPM (UDHR)MMS (UDHR)PPM (Wiki)MMS (Wiki)Figure 2: Cumulative distribution of segment bordersslightly better performance than did MMS.
Whenthe test data amounted to 100 characters, PPMachieved language identification with accuracy ofabout 91.4%.
For MMS, the identification accu-racy was a little less significant and was about 90.9%even with 100 characters of test data.The amount of learning data seemed sufficient forboth cases, with around 8,000 characters.
In fact,we conducted tests with larger amounts of learningdata and found a faster rise with respect to the inputlength, but the maximum possible accuracy did notshow any significant increase.Errors resulted from either noise or mistakes dueto the language family.
The Wikipedia test data wasnoisy, as mentioned in ?5.1.
As for language fam-ily errors, the test data includes many similar lan-guages that are difficult even for humans to correctlyjudge.
For example, Indonesian and Malay, Picardand Walloon, and Norwegian Bokma?l and Nynorskare all pairs representative of such confusion.Overall, the language identification performanceseems sufficient to justify its application to our mainproblem of text segmentation by language.6.2 Text Segmentation by LanguageFirst, we report the results obtained using the Test1data set.
Figure 2 shows the cumulative distributionobtained for segment border detection.
The horizon-tal axis indicates the relative location by characterwith respect to the correct border at zero, and thevertical axis indicates the cumulative proportion oftexts whose border is detected at that relative point.The figure shows four plots for all combinations ofthe two data sets and the two methods.
Note thatsegment borders are judged by characters and notby bordering locations, as explained in ?5.2.0.80.850.90.9510.8  0.85  0.9  0.95  1precisionrecall0.980.970.880.87PPM (UDHR)MMS (UDHR)PPM (Wiki)MMS (Wiki)0.60.650.70.750.80.6  0.65  0.7  0.75  0.8precisionrecall0.770.760.700.68PPM (UDHR)MMS (UDHR)PPM (Wiki)MMS (Wiki)Figure 3: PL/RL (language, upper graph) and PB/RB(border, lower graph) results, where borders were takenfrom any character offsetSince the plots rise sharply at the middle of thehorizontal axis, the borders were detected at or verynear the correct place in many cases.Next, we examine the results for Test2.
Fig-ure 3 shows the two precision/recall graphs for lan-guage identification (upper graph) and segment bor-der detection (lower graph), where borders weretaken from any character offset.
In each graph,the horizontal axis indicates precision and the ver-tical axis indicates recall.
The numbers appearingin each figure are the maximum F-score values foreach method and data set combination.
As can beseen from these numbers, the language identifica-tion performance was high.
Since the text portionsize was chosen from among the values 40, 80, 120,or 160, the performance is comprehensible from theresults shown in ?6.1.
Note also that PPM performedslightly better than did MMS.For segment border performance (lower graph),however, the results were limited.
The main reasonfor this is that both MMS and PPM tend to detecta border one character earlier than the correct loca-tion, as was seen in Figure 2.
At the same time,much of the test data contains unrealistic borders9750.70.750.80.850.90.9510.7  0.75  0.8  0.85  0.9  0.95  1precisionrecall0.940.910.840.81PPM (UDHR)MMS (UDHR)PPM (Wiki)MMS (Wiki)Figure 4: PB/RB, where borders were limited to spaces00.20.40.60.810  200  400  600  800  1000time (s)input length (characters)PPM (UDHR)MMS (UDHR)PPM (Wiki)MMS (Wiki)Figure 5: Average processing speed for a textwithin a word, since the data was generated by con-catenating two text portions with random borders.Therefore, we repeated the experiment with Test2under the constraint that a segment border could oc-cur only at a bordering location, as explained in ?5.2.The results with this constraint were significantlybetter, as shown in Figure 4.
The best result was forUDHR with PPM at 0.9411.
We could also observehow PPM performed better at detecting borders inthis case.
In actual application, it would be possibleto improve performance by relaxing the proceduralconditions, such as by decreasing the number of lan-guage possibilities.In this experiment for Test2, k ranged from 1 to5, but the performance was not affected by the sizeof k. When the F-score was examined with respectto k, it remained almost equal to k in all cases.
Thisshows how each recursion of formula (4) works al-most independently, having segmentation and lan-guage identification functions that are both robust.Lastly, we examine the speed of our method.Since |L| is constant throughout the comparison,11The language identification accuracy slightly increased aswell, by 0.002.the time should increase linearly with respect to theinput length |X|, with increasing k having no ef-fect.
Figure 5 shows the speed for Test2 processing,with the horizontal axis indicating the input lengthand the vertical axis indicating the processing time.Here, all character offsets were taken into consid-eration, and the processing was done on a machinewith a Xeon5650 2.66-GHz CPU.
The results con-firm that the complexity increased linearly with re-spect to the input length.
When the text size becameas large as several thousand characters, the process-ing time became as long as a second.
This timecould be significantly decreased by introducing con-straints on the bordering locations and languages.7 ConclusionThis article has presented a method for segmentinga multilingual text into segments, each in a differ-ent language.
This task could serve for preprocess-ing of multilingual texts before applying language-specific analysis to each text.
Moreover, the pro-posed method could be used to generate corpora in avariety of languages, since many texts in minor lan-guages tend to contain chunks in a major language.The segmentation task was modeled as an opti-mization problem of finding the best segment andlanguage sequences to minimize the descriptionlength of a given text.
An actual procedure for ob-taining an optimal result through dynamic program-ming was proposed.
Furthermore, we showed a wayto decrease the computational complexity substan-tially, with each of our two methods having linearcomplexity in the input length.Various empirical results were shown for lan-guage identification and segmentation.
Overall,when segmenting a text with up to five random por-tions of different languages, where each portion con-sisted of 40 to 120 characters, the best F-scores forlanguage identification and segmentation were 0.98and 0.94, respectively.For our future work, details of the methods mustbe worked out.
In general, the proposed approachcould be further applied to the actual needs of pre-processing and to generating corpora of minor lan-guages.976ReferencesBeatrice Alex, Amit Dubey, and Frank Keller.
2007.Using foreign inclusion detection to improve parsingperformance.
In Proceedings of the Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 151?160.Beatrice Alex.
2005.
An unsupervised system for iden-tifying english inclusions in german text.
In Pro-ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics, Student ResearchWorkshop, pages 133?138.T.C.
Bell, J.G.
Cleary, and I. H. Witten.
1990.
Text Com-pression.
Prentice Hall.Dario Benedetto, Emanuele Caglioti, and Vittorio Loreto.2002.
Language trees and zipping.
Physical ReviewLetters, 88(4).Rudi Cilibrasi and Paul Vita?nyi.
2005.
Clustering bycompression.
IEEE Transactions on Information The-ory, 51(4):1523?1545.John G. Cleary and Ian H. Witten.
1984.
Data compres-sion using adaptive coding and partial string matching.IEEE Transactions on Communications, 32:396?402.Martin Farach, Michiel Noordewier, Serap Savari, LarryShepp, Abraham J. Wyner, and Jacob Ziv.
1994.
Onthe entropy of dna: Algorithms and measurementsbased on memory and rapid convergence.
In Proceed-ings of the Sixth Annual ACM-SIAM Symposium onDiscrete Algorithms, pages 48?57.Gregory Grefenstette.
1995.
Comparing two languageidentification schemes.
In Proceedings of 3rd Inter-national Conference on Statistical Analysis of TextualData, pages 263?268.Marti A. Hearst.
1997.
Texttiling: Segmenting text intomulti-paragraph subtopic passages.
ComputationalLinguistics, 23(1):33?64.Patrick Juola.
1997.
What can we do with small cor-pora?
document categorization via cross-entropy.
InProceedings of an Interdisciplinary Workshop on Sim-ilarity and Categorization.Gen-itiro Kikui.
1996.
Identifying the coding system andlanguage of on-line documents on the internet.
In Pro-ceedings of 16th International Conference on Compu-tational Linguistics, pages 652?657.Casanai Kruengkrai, Prapass Srichaivattana, VirachSornlertlamvanich, and Hitoshi Isahara.
2005.
Lan-guage identification based on string kernels.
InProceedings of the 5th International Symposium onCommunications and Information Technologies, pages926?929.Penelope Sibun and Jeffrey C. Reynar.
1996.
Languageidentification: Examining the issues.
In Proceedingsof 5th Symposium on Document Analysis and Infor-mation Retrieval, pages 125?135.William J. Teahan and David J. Harper.
2001.
Usingcompression-based language models for text catego-rization.
In Proceedings of the Workshop on LanguageModeling and Information Retrieval, pages 83?88.William John Teahan.
2000.
Text classification and seg-mentation using minimum cross-entropy.
In RIAO,pages 943?961.Jacob Ziv and Abraham Lempel.
1977.
A universal al-gorithm for sequential data compression.
IEEE Trans-actions on Information Theory, 23(3):337?343.AppendixThis Appendix lists all the languages contained in our data sets,as summarized in Table 1.For UDHRLatinAchinese, Achuar-Shiwiar, Adangme, Afrikaans, Aguaruna,Aja, Akuapem Akan, Akurio, Amahuaca, Amarakaeri, Ambo-Pasco Quechua, Arabela, Arequipa-La Unio?n Quechua, Arpi-tan, Asante Akan, Asha?ninka, Ashe?ninka Pajonal, Asturian,Auvergnat Occitan, Ayacucho Quechua, Aymara, Baatonum,Balinese, Bambara, Baoule?, Basque, Bemba, Beti, Bikol, Bini,Bislama, Bokma?l Norwegian, Bora, Bosnian, Breton, Buginese,Cajamarca Quechua, Caldero?n Highland Quichua, Candoshi-Shapra, Caquinte, Cashibo-Cacataibo, Cashinahua, Catalan,Cebuano, Central Kanuri, Central Mazahua, Central Nahuatl,Chamorro, Chamula Tzotzil, Chayahuita, Chickasaw, Chiga,Chokwe, Chuanqiandian Cluster Miao, Chuukese, Corsican,Cusco Quechua, Czech, Dagbani, Danish, Dendi, Ditammari,Dutch, Eastern Maninkakan, Emiliano-Romagnolo, English,Esperanto, Estonian, Ewe, Falam Chin, Fanti, Faroese, Fi-jian, Filipino, Finnish, Fon, French, Friulian, Ga, Gagauz,Galician, Ganda, Garifuna, Gen, German, Gheg Albanian,Gonja, Guarani, Gu?ila?
Zapotec, Haitian Creole, Haitian Cre-ole (popular), Haka Chin, Hani, Hausa, Hawaiian, Hiligaynon,Huamal?
?es-Dos de Mayo Hua?nuco Quechua, Huautla Maza-tec, Huaylas Ancash Quechua, Hungarian, Ibibio, Icelandic,Ido, Igbo, Iloko, Indonesian, Interlingua, Irish, Italian, Ja-vanese, Jola-Fonyi, K?iche?, Kabiye`, Kabuverdianu, Kalaal-lisut, Kaonde, Kaqchikel, Kasem, Kekch?
?, Kimbundu, Kin-yarwanda, Kituba, Konzo, Kpelle, Krio, Kurdish, Lamnso?,Languedocien Occitan, Latin, Latvian, Lingala, Lithuanian,Lozi, Luba-Lulua, Lunda, Luvale, Luxembourgish, Madurese,Makhuwa, Makonde, Malagasy, Maltese, Mam, Maori,Mapudungun, Margos-Yarowilca-Lauricocha Quechua, Mar-shallese, Mba, Mende, Metlato?noc Mixtec, Mezquital Otomi,Mi?kmaq, Miahuatla?n Zapotec, Minangkabau, Mossi, Mozara-bic, Murui Huitoto, M?
?skito, Ndonga, Nigerian Pidgin, Nomat-siguenga, North Jun?
?n Quechua, Northeastern Dinka, NorthernConchucos Ancash Quechua, Northern Qiandong Miao, North-ern Sami, Northern Kurdish, Nyamwezi, Nyanja, Nyemba,Nynorsk Norwegian, Nzima, Ojitla?an Chinantec, Oromo,Palauan, Pampanga, Papantla Totonac, Pedi, Picard, PichisAshe?ninka, Pijin, Pipil, Pohnpeian, Polish, Portuguese, Pu-laar, Purepecha, Pa?ez, Quechua, Rarotongan, Romanian, Ro-mansh, Romany, Rundi, Salinan, Samoan, San Lu?
?s Potos?
?Huastec, Sango, Sardinian, Scots, Scottish Gaelic, Serbian,977Serer, Seselwa Creole French, Sharanahua, Shipibo-Conibo,Shona, Slovak, Somali, Soninke, South Ndebele, SouthernDagaare, Southern Qiandong Miao, Southern Sotho, Spanish,Standard Malay, Sukuma, Sundanese, Susu, Swahili, Swati,Swedish, Sa?otomense, Tahitian, Tedim Chin, Tetum, TidikeltTamazight, Timne, Tiv, Toba, Tojolabal, Tok Pisin, Tonga(Tonga Islands), Tonga (Zambia), Tsonga, Tswana, Turkish,Tzeltal, Umbundu, Upper Sorbian, Urarina, Uzbek, VeracruzHuastec, Vili, Vlax Romani, Walloon, Waray, Wayuu, Welsh,Western Frisian, Wolof, Xhosa, Yagua, Yanesha?, Yao, Yapese,Yoruba, Yucateco, Zhuang, ZuluCyrillicAbkhazian, Belarusian, Bosnian, Bulgarian, Kazakh, Mace-donian, Ossetian, Russian, Serbian, Tuvinian, Ukrainian, YakutArabicStandard ArabicOtherJapanese, Korean, Mandarin Chinese, Modern GreekFor WikiLatinAfrikaans, Albanian, Aragonese, Aromanian, Arpitan, As-turian, Aymara, Azerbaijani, Bambara, Banyumasan, Basque,Bavarian, Bislama, Bosnian, Breton, Catala`, Cebuano, CentralBikol, Chavacano, Cornish, Corsican, Crimean Tatar, Croatian,Czech, Danish, Dimli, Dutch, Dutch Low Saxon, Emiliano-Romagnolo, English, Esperanto, Estonian, Ewe, Extremaduran,Faroese, Fiji Hindi, Finnish, French, Friulian, Galician, Ger-man, Gilaki, Gothic, Guarani, Hai//om, Haitian, Hakka Chi-nese, Hawaiian, Hungarian, Icelandic, Ido, Igbo, Iloko, Indone-sian, Interlingua, Interlingue, Irish, Italian, Javanese, Kabyle,Kalaallisut, Kara-Kalpak, Kashmiri, Kashubian, Kongo, Ko-rean, Kurdish, Ladino, Latin, Latvian, Ligurian, Limburgan,Lingala, Lithuanian, Lojban, Lombard, Low German, LowerSorbian, Luxembourgish, Malagasy, Malay, Maltese, Manx,Maori, Mazanderani, Min Dong Chinese, Min Nan Chinese,Nahuatl, Narom, Navajo, Neapolitan, Northern Sami, Norwe-gian, Norwegian Nynorsk, Novial, Occitan, Old English, Pam-panga, Pangasinan, Panjabi, Papiamento, Pennsylvania Ger-man, Piemontese, Pitcairn-Norfolk, Polish, Portuguese, Pushto,Quechua, Romanian, Romansh, Samoan, Samogitian Lithua-nian, Sardinian, Saterfriesisch, Scots, Scottish Gaelic, Serbo-Croatian, Sicilian, Silesian, Slovak, Slovenian, Somali, Span-ish, Sranan Tongo, Sundanese, Swahili, Swati, Swedish, Taga-log, Tahitian, Tarantino Sicilian, Tatar, Tetum, Tok Pisin, Tonga(Tonga Islands), Tosk Albanian, Tsonga, Tswana, Turkish,Turkmen, Uighur, Upper Sorbian, Uzbek, Venda, Venetian,Vietnamese, Vlaams, Vlax Romani, Volapu?k, Vo?ro, Walloon,Waray, Welsh, Western Frisian, Wolof, Yoruba, Zeeuws, ZuluCyrillicAbkhazian, Bashkir, Belarusian, Bulgarian, Chuvash, Erzya,Kazakh, Kirghiz, Macedonian, Moksha, Moldovan, Mongo-lian, Old Belarusian, Ossetian, Russian, Serbian, Tajik, Udmurt,Ukrainian, YakutArabicArabic, Egyptian Arabic, Gilaki, Mazanderani, Persian,Pushto, Uighur, UrduDevanagariBihari, Hindi, Marathi, Nepali, Newari, SanskritOtherAmharic, Armenian, Assamese, Bengali, Bishnupriya,Burmese, Central Khmer, Chinese, Classical Chinese, Dhivehi,Gan Chinese, Georgian, Gothic, Gujarati, Hebrew, Japanese,Kannada, Lao, Malayalam, Modern Greek, Official Aramaic,Panjabi, Sinhala, Tamil, Telugu, Thai, Tibetan, Wu Chinese,Yiddish, Yue Chinese978
