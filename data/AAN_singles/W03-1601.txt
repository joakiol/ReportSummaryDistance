Generation of single-sentence paraphrases frompredicate/argument structure using lexico-grammatical resourcesRaymond Kozlowski, Kathleen F. McCoy, and K. Vijay-ShankerDepartment of Computer and Information SciencesUniversity of DelawareNewark, DE 19716, USAkozlowsk,mccoy,vijay@cis.udel.eduAbstractParaphrases, which stem from the va-riety of lexical and grammatical meansof expressing meaning available in alanguage, pose challenges for a sen-tence generation system.
In thispaper, we discuss the generation ofparaphrases from predicate/argumentstructure using a simple, uniform gen-eration methodology.
Central to ourapproach are lexico-grammatical re-sources which pair elementary seman-tic structures with their syntactic re-alization and a simple but powerfulmechanism for combining resources.1 IntroductionIn natural language generation, producing somerealization of the input semantics is not the onlygoal.
The same meaning can often be expressedin various ways using dierent lexical and syn-tactic means.
These dierent realizations, calledparaphrases, vary considerably in appropriate-ness based on pragmatic factors and commu-nicative goals.
If a generator is to come up withthe most appropriate realization, it must be ca-pable of generating all paraphrases that realizethe input semantics.
Even if it makes choices onpragmatic grounds during generation and pro-duces a single realization, the ability to generatethem all must still exist.Variety of lexical and grammatical formsof expression pose challenges to a generator((Stede, 1999); (Elhadad et al, 1997); (Nicolovet al, 1995)).
In this paper, we discuss the gen-eration of single-sentence paraphrases realizingthe same semantics in a uniform fashion using asimple sentence generation architecture.In order to handle the various ways of realiz-ing meaning in a simple manner, we believe thatthe generation architecture should not be awareof the variety and not have any special mech-anisms to handle the dierent types of realiza-tions1.
Instead, we want all lexical and gram-matical variety to follow automatically from thevariety of the elementary building blocks of gen-eration, lexico-grammatical resources.We have developed a fully-operational proto-type of our generation system capable of gen-erating the examples presented in this paper,which illustrate a wide range of paraphrases.As we shall see, the paraphrases that are pro-duced by the system depend entirely on theactual lexicon used in the particular applica-tion.
Determining the range of alternate formsthat constitute paraphrases is not the focus ofthis work.
Instead, we describe a framework inwhich lexico-grammatical resources, if properlydened, can be used to generate paraphrases.2 Typical generation methodologySentence generation takes as input some seman-tic representation of the meaning to be conveyedin a sentence.
We make the assumption that1Ability to handle variety in a uniform manner is alsoimportant in multilingual generation as some forms avail-able in one language may not be available in another.ENJOYEXPERIENCER THEMEAMY INTERACTIONFigure 1: The semantics underlying (2a-2b)the input is a hierarchical predicate/argumentstructure such as that shown in Fig.
1.
Theoutput of this process should be a set of gram-matical sentences whose meaning matches theoriginal semantic input.One standard approach to sentence genera-tion from predicate/argument structure (like thesemantic-head-driven generation in (Shieber etal., 1990)) involves a simple algorithm.1.
decompose the input into the top predicate(to be realized by a (single) lexical item thatserves as the syntactic head) and identifythe arguments and modiers2.
recursively realize arguments, then modi-ers3.
combine the realizations in step 2 with thehead in step 1In realizing the input in Fig.
1, the input can bedecomposed into the top predicate which can berealized by a syntactic head (a transitive verb)and its two arguments, the experiencer and thetheme.
Suppose that the verb enjoy is chosento realize the top predicate.
The two argumentscan then be independently realized as Amy andthe interaction.
Finally, the realization of theexperiencer, Amy, can be placed in the subjectposition and that of the theme, the interaction,in the complement position, yielding (2a).Our architecture is very similar but we arguefor a more central role of lexico-grammatical re-sources driving the realization process.3 Challenges in generatingparaphrasesParaphrases come from various sources.
In thissection, we give examples of some types of para-phrases we handle and discuss the challengesthey pose to other generators.
We also identifytypes of paraphrases we do not consider.3.1 Paraphrases we handleSimple synonymy The simplest source ofparaphrases is simple synonymy.
We take sim-ple synonyms to be dierent words that havethe same meaning and are of the same syntacticcategory and set up the same syntactic context.
(1a) Booth killed Lincoln.
(1b) Booth assassinated Lincoln.A generation system must be able to allowthe same semantic input to be realized in dif-ferent ways.
Notice that the words kill and as-sassinate are not always interchangeable, e.g.,assassinate is only appropriate when the victimis a famous person.
Such constraints need to becaptured with selectional restrictions lest inap-propriate realizations be produced.Dierent placement of argument realiza-tions Sometimes dierent synonyms, like theverbs enjoy and please, place argument realiza-tions dierently with respect to the head, as il-lustrated in (2a-2b).
(2a) Amy enjoyed the interaction.
(2b) The interaction pleased Amy.To handle this variety, a uniform generationmethodology should not assume a xed map-ping between thematic and syntactic roles butlet each lexical item determine the placement ofargument realizations.
Generation systems thatuse such a xed mapping must override it forthe divergent cases (e.g., (Dorr, 1993)).Words with overlapping meaning Thereare often cases of dierent words that realize dif-ferent but overlapping semantic pieces.
The eas-iest way to see this is in what has been termedincorporation, where a word not only realizes apredicate but also one or more of its arguments.Dierent words may incorporate dierent argu-ments or none at all, which may lead to para-phrases, as illustrated in (3a-3c).
(3a) Charlesew across the ocean.
(3b) Charles crossed the ocean by plane.
(3c) Charles went across the ocean by plane.Notice that the verby realizes not only go-ing but also the mode of transportation being aplane, the verb cross with its complement real-ize going whose path is across the object realizedby the complement, and the verb go only real-izes going.
For all of these verbs, the remainingarguments are realized by modiers.Incorporation shows that a uniform genera-tor should use the word choices to determine 1)what portion of the semantics they realize, 2)what portions are to be realized as argumentsof the realized semantics, and 3) what portionsremain to be realized and attached as modiers.Generation systems that assume a one-to-onemapping between semantic and syntactic units(e.g., (Dorr, 1993)) must use special processingfor cases of overlapping semantics.Dierent syntactic categories Predicatescan often be realized by words of dierent syn-tactic categories, e.g., the verb found and thenoun founding, as in (4a-4b).
(4a) I know that Olds founded GM.
(4b) I know about the founding of GM by Olds.Words of dierent syntactic categories usu-ally have dierent syntactic consequences.
Onesuch consequence is the presence of additionalsyntactic material.
Notice that (4b) containsthe prepositions of and by while (4a) does not.These prepositions might be considered a syn-tactic consequence of the use of the noun found-ing in this conguration.
Another syntactic con-sequence is a dierent placement of argument re-alizations.
The realization of the founder is thesubject of the verb found in (4a) while in (4b)the use of founding leads to its placement in theobject position of the preposition by.Grammatical alternations Words can beput in a variety of grammatical alternations suchas the active and passive voice, as in (5a-5b), thetopicalized form, the it-cleft form, etc.
(5a) Oswald killed Kennedy.
(5b) Kennedy was killed by Oswald.The choice of dierent grammatical alterna-tions has dierent syntactic consequences whichmust be enforced in generation, such as the pres-ence or absence of the copula and the dierentplacement of argument realizations.
In somesystems such as ones based on Tree-AdjoiningGrammars (TAG), including ours, these con-sequences are encapsulated within elementarystructures of the grammar.
Thus, such systemsdo not have to specically reason about theseconsequences, as do some other systems.More complex alternations The same con-tent of excelling at an activity can be realized bythe verb excel, the adverb well, and the adjectivegood, as illustrated in (6a-6c).
(6a) Barbara excels at teaching.
(6b) Barbara teaches well.
(6c) Barbara is a good teacher.This variety of expression, often called headswitching, poses a considerable diculty formost existing sentence generators.
The di-culty stems from the fact that the realizationof a phrase (sentence) typically starts with thesyntactic head (verb) which sets up a syntacticcontext into which other constituents are t. Ifthe top predicate is the excelling, we have to beable to start generation not only with the verbexcel but also with the adverb well and the ad-jective good, typically not seen as setting up anappropriate syntactic context into which the re-maining arguments can be t. Existing genera-tion systems that handle this variety do so usingspecial assumptions or exceptional processing,all in order to start the generation of a phrasewith the syntactic head (e.g., (Stede, 1999), (El-hadad et al, 1997), (Nicolov et al, 1995), (Dorr,1993)).
Our system does not require that the se-mantic head map to the syntactic head.Dierent grammatical forms realizing se-mantic content Finally, we consider a case,which to our knowledge is not handled by othergeneration systems, where grammatical formsrealize content independently of the lexical itemon which they act, as in (7a-7b).
(7a) Who rules Jordan?
(7b) Identify the ruler of Jordan!The wh-question form, as used in (7a), real-izes a request for identication by the listener(in this case, the ruler of Jordan).
Likewise, theimperative structure (used in (7b)) realizes a re-quest or a command to the listener (in this case,to identify the ruler of Jordan).3.2 Paraphrases we do not considerSince our focus is on sentence generation and notsentence planning, we only consider the genera-tion of single-sentence paraphrases.
Hence, wedo not have the ability to generate (8a-8b) fromthe same input.
(8a) CS1 has a programming lab.
(8b) CS1 has a lab.
It involves programming.Since we do not reason about the semanticinput, including deriving entailment relations,we cannot generate (9a-9b) from the same input.
(9a) Oslo is the capital of Norway.
(9b) Oslo is located in Norway.4 Our generation methodologyGeneration in our system is driven by thesemantic input, realized by selecting lexico-grammatical resources matching pieces of it,starting with the top predicate.
The realizationof a piece containing the top predicate providesthe syntactic context into which the realizationsof the remaining pieces can be t (their place-ment being determined by the resource).The key to our ability to handle paraphrasesin a uniform manner is that our processing isdriven by our lexicon and thus we do not makeany a priori assumptions about 1) the amountof the input realized by a lexical unit, 2) the re-lationship between semantic and syntactic types(and thus the syntactic rank or category of therealization of the top piece), 3) the nature ofthe mapping between thematic roles and syn-tactic positions, and 4) the grammatical alter-nation (e.g., there are dierent resources for thesame verb in dierent alternations: the active,passive, topicalized, etc.).
Because this informa-tion is contained in each lexico-grammatical re-source, generation can proceed no matter whatchoices are specied about these in each indi-vidual resource.
Our approach is fundamen-tally dierent from systems that reason directlyabout syntax and build realizations by syntacticrank ((Bateman, 1997), (Elhadad et al, 1997);(Nicolov et al, 1995); (Stone and Doran, 1997)).4.1 Our algorithmOur generation algorithm is a simple, recursive,semantic-head-driven generation process, con-sistent with the approach described in section 2,but one driven by the semantic input and thelexico-grammatical resources.1.
given an unrealized input, nd a lexico-grammatical resource that matches a por-tion including the top predicate and satis-es any selectional restrictions2.
recursively realize arguments, then modi-ers3.
combine the realizations in step 2 with theresource in step 1, as determined by the re-source in step 1Notice the prominence of lexico-grammatical re-sources in steps 1 and 3 of this algorithm.
Thestandard approach in section 2 need not bedriven by resources.4.2 Lexico-grammatical resourcesThe key to the simplicity of our algorithm lies inthe lexico-grammatical resources, which encap-sulate information necessary to carry throughgeneration.
These consist of three parts: the semantic side: the portion of seman-tics realized by the resource (including thepredicate and any arguments; this part ismatched against the input semantics) the syntactic side: either word(s) in a syn-tactic conguration or a grammatical formwithout words, and syntactic consequences a mapping between semantic and syntacticconstituents indicating which constituenton the semantic side is realized by whichconstituent on the syntactic sideConsider the resources for the verbs enjoy andplease in Fig.
2.
The semantic sides indicatethat these resources realize the predicate ENJOYand the thematic roles EXPERIENCER and THEME.The arguments lling those roles (which must berealized separately, as indicated by dashed out-lines) appear as variables X and Y which will bematched against actual arguments.
The syntac-tic sides contain the verbs enjoy and please inthe active voice conguration.
The mappingsinclude links between ENJOY and its realizationas well as links between the unrealized agent (X)or theme (Y) and the subject or the complement.Our mapping between semantic and syntacticconstituents bears resemblance to the pairings inSynchronous TAG (Shieber and Schabes, 1990).Just like in Synchronous TAG, the mapping isVPNPVPNPVenjoyuS0 01ENJOYEXPERIENCER THEMEX Y1VPNPVPNPVpleaseuS0 01ENJOYEXPERIENCER THEMEX Y1Figure 2: Two dierent resources for ENJOYcritical for combining realizations (in step 3 ofour algorithm in section 4.1).
There are, how-ever, advantages that our approach has.
Forone, we are not constrained by the isomorphismrequirement in a Synchronous TAG derivation.Also, the DSG formalism that we use aordsgreaterexibility, signicant in our approach, asdiscussed later in this paper (and in more detailin (Kozlowski, 2002b)).4.3 The grammatical formalismBoth step 3 of our algorithm (putting re-alizations together) and the needs of lexico-grammatical resources (the encapsulation ofsyntactic consequences such as the positionof argument realizations) place signicant de-mands on the grammatical formalism to be usedin the implementation of the architecture.
Onegrammatical formalism that is well-suited forour purposes is the D-Tree Substitution Gram-mars (DSG, (Rambow et al, 2001)), a variantof Tree-Adjoining Grammars (TAG).
This for-malism features an extended domain of localityandexibility in encapsulation of syntactic con-sequences, crucial in our architecture.Consider the elementary DSG structures onthe right-hand-side of the resources for enjoyand please in Fig.
2.
Note that nodes markedwith # are substitution nodes corresponding tosyntactic positions into which the realizations ofSu0 011VPNPVPpleaseNPVuS0 011NPthe interactionAmyVPNPVPVenjoyFigure 3: Combining argument realizations withthe resources for enjoy and pleasearguments will be substituted.
The positions ofboth the subject and the complement are en-capsulated in these elementary structures.
Thisallows the mapping between semantic and syn-tactic constituents to be dened locally withinthe resources.
Dotted lines indicate dominationof length zero or more where syntactic material(e.g., modiers) may end up.4.4 Using resources in our algorithmStep 1 of our algorithm requires matching the se-mantic side of a resource against the top of theinput and testing selectional restrictions.
A se-mantic side matches if it can be overlaid againstthe input.
Details of this process are givenin (Kozlowski, 2002a).
Selectional restrictions(type restrictions on arguments) are associatedwith nodes on the semantic side of resources.In their evaluation, the appropriate knowledgebase instance is accessed and its type is tested.More details about using selectional restrictionsin generation and in our architecture are givenin (Kozlowski et al, 2002).Resources for enjoy and please which matchthe top of the input in Fig.
1 are shown inFig.
2.
In doing the matching, the argumentsAMY and INTERACTION are unied with X andY.
The dashed outlines around X and Y indicatethat the resource does not realize them.
Our al-gorithm calls for the independent recursive real-ization of these arguments and then putting to-gether those realizations with the syntactic sideof the resource, as indicated by the mapping.0uflyVPNPVPS0VGO1PLANEAGENT MODEXACROSSVPNPVPNPVcrossuS0 01GOAGENT PATHX1THEMEYFigure 4: Two dierent resources for GOPATH MODEACROSSOCEANTHEMEAGENTGOPLANECHARLESFigure 5: The semantics underlying (3a-3c) withportion realized by cross in boldThis is shown in Fig.
3.
The argument realiza-tions, Amy and the interaction, are placed in thesubject and complement positions of enjoy andplease, according to the mapping in the corre-sponding resources.4.5 Driving decomposition by resourcesThe semantic side of a resource determineswhich arguments, if any, are realized by the re-source, while the matching done in step 1 of ouralgorithm determines the portions that must berealized by modiers.
This is always done thesame way regardless of the resources selectedand how much of the input they realize, suchas the two resources realizing the predicate GOshown in Fig.
4, one fory which incorporatesMODE PLANE and another for cross which incor-porates PATH ACROSS.YXAGENT THEMENPFOUNDY1XTHEMEAGENT100SufoundVVPNP VPuN?theu 0NPD N?1FOUND2PP1byP NPfoundinguofP NPu PP2NN?Figure 6: Two dierent resources for FOUNDSuppose the semantic input underlying (3a-3c) is as given in Fig.
5.
The portion shownin bold is realized by the resource for cross inFig.
4.
The agent of GO and the theme of ACROSSare to be realized as arguments.
The remainingthematic role MODE with the argument PLANE ll-ing it, is to be realized by a modier.4.6 Encapsulation of syntacticconsequencesAll syntactic information should be encapsu-lated within resources and transparent to thealgorithm.
This includes the identication of ar-guments, including their placement with respectto the realization.
Another example of a syn-tactic consequence is the presence of additionalsyntactic material required by the lexical item inthe particular syntactic conguration.
The verbfound in the active conguration, as in (4a), doesnot require any additional syntactic material.On the other hand, the noun founding in theconguration with prepositional phrases headedby of and by, as in (4b), may be said to requirethe use of the prepositions.
The resources forfound and founding are shown in Fig.
6.
Encap-sulation of such consequences allows us to avoidspecial mechanisms to keep track of and enforceEXPERIENCEREXCELTHEME[0]:1uVVPuatPPP[0]:excelVP[0]:[0]:0uwellAdv0Adv?1Adv?1VPAdvPPAGENTNP1PRONPSPTHEMEEXPERIENCEREXCEL00SVPFigure 7: Two dierent resources for EXCELthem for individual resources.4.7 Syntactic rank and categoryNo assumptions are made about the realizationof a piece of input semantics, including its syn-tactic rank and category.
For instance, the pred-icate EXCEL can be realized by the verb excel,the adverb well, and the adjective good, as illus-trated in (6a-6c).
The processing is the same:a resource is selected and any argument realiza-tions are attached to the resource.Fig.
7 shows a resource for the predicateEXCEL realized by the verb excel.
What is in-teresting about this case is that the DSG for-malism we chose allows us to encapsulate thePRO in the subject position of the complementas a syntactic consequence of the verb excel inthis conguration.
The other resource for EXCELshown in Fig.
7 is unusual in that the predicateis realized by an adverb, well.
Note the link be-tween the uninstantiated theme on the semanticside and the position for its corresponding syn-tactic realization, the substitution node VP12.Suppose the semantic input underlying (6a-2Also notice that the experiencer of EXCEL is consid-ered realized by the well resource and coindexed with theagent of the theme of EXCEL, to be realized by a separateresource.
[1][1]: BARBARATEACH[1]AGENTEXCELTHEMEEXPERIENCERFigure 8: The semantics underlying (6a-6c)6c) is as given in Fig.
8 and the well resource inFig.
7 is selected to realize the top of the seman-tics.
The matching in step 1 of our algorithmdetermines that the subtree of the input rootedat TEACH must be recursively realized.
The re-alization of this subtree yields Barbara teaches.Because of the link between the theme of EXCELand the VP1node of well, the realization Bar-bara teaches is substituted to the VP1node ofwell.
This is a more complex substitution thanin regular TAG (where the substitution node isidentied with the root of the argument realiza-tion), and is equivalent to the adjunction of wellto Barbara teaches.
In DSG, we are able to treatstructures such as the well structure as initialand not auxiliary, as TAG would.
Thus, argu-ment realizations are combined with all struc-tures in a uniform fashion.4.8 Grammatical formsAs discussed before, grammatical forms them-selves can realize a piece of semantics.
For in-stance, the imperative syntactic form realizes arequest or a command to the listener, as shownin Fig.
9.
Likewise, the wh-question form real-izes a request to identify, also shown in Fig.
9.In our system, whether the realization has anylexical items is not relevant.4.9 The role of DSGWe believe that the choice of the DSG formal-ism plays a crucial role in maintaining our sim-ple methodology.
Like TAG, DSG allows cap-turing syntactic consequences in one elementarystructure.
DSG, however, allows even greaterexibility in what is included in an elementarystructure.
Note that in DSG we may have non-immediate domination links between nodes of[empty:+][subj?empty:+][0]:[0]:REQUESTACTIONACTIONREQUESTPYOUSNP(you)IDENTIFYTHEMES 1NP [inv:+]SNP?whouNYOU SET?OFTHEME SUCH?THATPAGENTFigure 9: Two dierent resources for REQUESTdierent syntactic categories (e.g., between the Sand NP in Fig.
9 and also in the excel at structurein Fig.
7).
DSG also allows uniform treatmentof complementation and modication using theoperations of substitution (regardless of the re-alization of the predicate, e.g., the structures inFig.
7) and adjunction, respectively.5 ConclusionsAlthough we only consider paraphrases with thesame semantics, there is still a wide variety ofexpression which poses challenges to any genera-tion system.
In overcoming those challenges andgenerating in a simple manner in our architec-ture, our lexico-grammatical resources play animportant role in each phase of generation.
En-capsulation of syntactic consequences within ele-mentary syntactic structures keeps our method-ology modular.
Whatever those consequences,often very dierent for dierent paraphrases,generation always proceeds in the same manner.Both the algorithm and the constraints onour lexico-grammatical resources place signif-icant demands on the grammatical formalismused for the architecture.
We nd that the DSGformalism meets those demands well.ReferencesJohn Bateman.
1997.
Enabling technology for mul-tilingual natural language generation: the KPMLdevelopment environment.
Natural Language En-gineering, 3(1):15{55.Bonnie J. Dorr.
1993.
Interlingual machine transla-tion: a parametrized approach.
Articial Intelli-gence, 63(1):429{492.Michael Elhadad, Kathleen McKeown, and JacquesRobin.
1997.
Floating constraints in lexicalchoice.
Computational Intelligence, 23:195{239.Raymond Kozlowski, Kathleen F. McCoy, andK.
Vijay-Shanker.
2002.
Selectional restrictionsin natural language sentence generation.
In Pro-ceedings of the 6th World Multiconference on Sys-temics, Cybernetics, and Informatics (SCI'02).Raymond Kozlowski.
2002a.
Driving multilingualsentence generation with lexico-grammatical re-sources.
In Proceedings of the Second Interna-tional Natural Language Generation Conference(INLG'02) - Student Session.Raymond Kozlowski.
2002b.
DSG/TAG - An appro-priate grammatical formalism forexible sentencegeneration.
In Proceedings of the Student ResearchWorkshop at the 40th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL'02).Nicolas Nicolov, Chris Mellish, and Graeme Ritchie.1995.
Sentence Generation from ConceptualGraphs.
In Proceedings of the 3rd InternationalConference on Conceptual Structures (ICCS'95).Owen Rambow, K. Vijay-Shanker, and David Weir.2001.
D-Tree Substitution Grammars.
Computa-tional Linguistics, 27(1):87{122.Stuart M. Shieber and Yves Schabes.
1990.
Syn-chronous Tree-Adjoining Grammars.
In Proceed-ings of the 13th International Conference on Com-putational Linguistics.Stuart M. Shieber, Gertjan van Noord, FernandoC.
N. Pereira, and Robert C. Moore.
1990.Semantic-Head-Driven Generation.
Computa-tional Linguistics, 16(1):30{42.Manfred Stede.
1999.
Lexical semantics and knowl-edge representation in multilingual text genera-tion.
Kluwer Academic Publishers, Boston.Matthew Stone and Christine Doran.
1997.
Sen-tence Planning as Description Using Tree Adjoin-ing Grammar.
In Proceedings of the 35th AnnualMeeting of the Association for Computational Lin-guistics (ACL'97).
