Explo i t ing a Probabi l ist ic  Hierarchical Mode l  for Generat ionSrinivas Bangalore and Owen RambowAT&T Labs Research180 Park AvenueF lorham Park, NJ 07932{sr in?,  rambow}@research,  a r t .
comAbst ractPrevious stochastic approaches to generationdo not include a tree-based representation ofsyntax.
While this may be adequate or evenadvantageous for some applications, other ap-plications profit from using as much syntacticknowledge as is available, leaving to a stochas-tic model only those issues that are not deter-mined by the grammar.
We present initial re-suits showing that a tree-based model derivedfrom a tree-annotated corpus improves on a treemodel derived from an unannotated corpus, andthat a tree-based stochastic model with a hand-crafted grammar outpertbrms both.1 I n t roduct ionFor many apt)lications in natural anguage gen~eration (NLG), the range of linguistic expres-sions that must be generated is quite restricted,and a grammar tbr generation can be fltlly spec-ified by hand.
Moreover, in ma W cases it; is veryimportant not to deviate from certain linguis-tic standards in generation, in which case hand-crafted grammars give excellent control.
How-ever, in other applications tbr NLG the varietyof the output is much bigger, and the demandson the quality of the output somewhat less strin-gent.
A typical example is NLG in the con-text of (interlingua- or transthr-based) machinetranslation.
Another reason for reb~xing thequality of the output may be that not enoughtime is available to develop a flfll grammar tbra new target language in NLG.
In all thesecases, stochastic ("empiricist") methods pro-vide an alternative to hand-crafted ("rational-ist") approaches to NLG.
To our knowledge, thefirst to use stochastic techniques in NLG wereLangkilde and Knight (1998a) and (1998b).
Inthis paper, we present FERGUS (Flexible Em-piricist/Rationalist Generation Using Syntax).FErtGUS follows Langkilde and Knight's seminalwork in using an n-gram language model, but; weaugment it with a tree-based stochastic modeland a traditional tree-based syntactic grammar.More recent work on aspects of stochastic gen-eration include (Langkilde and Knight, 2000),(Malouf, 1999) and (Ratnaparkhi, 2000).Betbre we describe in more detail how we usestochastic models in NLG, we recall the basictasks in NLG (Rainbow and Korelsky, 1992; Re-iter, 1994).
During text  p lanning,  content andstructure of the target text; are determined toachieve the overall communicative goal.
Dur-ing sentence planning, linguistic means - inparticular, lexical and syntactic means are de-termined to convey smaller pieces of meaning.l)uring real izat ion,  the specification chosen insentence planning is transtbrmed into a surfacestring, by line~rizing and intlecting words in thesentence (and typically, adding function words).As in the work by Langkilde and Knight, ourwork ignores the text planning stage, but it; doesaddress the sentence, planning and the realiza-tion stages.The structure of the paper is as tbllows.
InSection 2, we present he underlying rammat-ical tbrmalism, lexicalized tree-adjoining gram-mar (LTAG).
In Section 3, we describe the ar-chitecture of the system, and some of the mod-ules.
In Section 4 we discuss three experiments.In Section 5 we colnpare our work to that ofLangkilde and Knight (1998a).
We concludewith a summary of on-going work.2 Modeling SyntaxIn order to model syntax, we use an existingwide-coverage grammar of English, the XTAGgrammar developed at the University of Peru>sylvania (XTAG-Gronp, 1999).
XTAG is a tree~adjoining grammar (TAG) (Joshi, 1987a).
In42,,.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
, .
.T rees  used  in der ivat ion  " -~,- " "7 / / " ' ,P  A , / /  i P--,7N Aux 1) N / N l' NI' ~ / l) A NI I I I I I ".-JI I Ithere was n{} cost estimate i'of the second phase1 '{3 71 Y2 {z2 74  71 75  (z 1Other supertags for the loxemcs found in the training corpus:IIOI1C {Z4 {Z 1 {~ I (z 1 {z4{z 5 {z 2 7 2 {z 55 more 11 more  4 more I(} moreIlOIle {z 3 {z 2{z 1 3' 25 IIIOrC 2 Ill{WeFigure 1: An excerl}t from the Xq'AG gr~um\]lm" t(} derive Th,{"r('.
wa.s u,o to.st {:stim,,tc .fi)r the .secondphase.
; dotted lines show t)ossit}le a{ljun('ti{ms that were not madea TAG, the elementary structures are \])hrase-structure trees which are comt)osed using twoot}er~tions , sut}stitui,ion (w\]fich al}i}{;n{ts onetree ~1; the fl:ontier of another) mtd a(tjnlmtio\]t(which ins{;rts one tree into the mi{l{ll{', of im-o|;her).
In gral)hi{:al re i ) rese l l ta l ; i{} l l  , \ ] l o{tes  &I;which substitul;ion can take 1)lac{'~ are \]uarkedwith dow\]>arrows.
In linguisI;ic uses (}f TAG,we asso{'ial;e one lexical item (its anchor) witheach tree, and {}he or (typically) more trees witheach lexical ire\]n; its a result we obtain a lexi-calized TAG or LTAG.
Since ea{'h lexi{:al itemis associated with a whole tree (rather thanjust a phrase-stru{'ture ule, tbr exa\]nl)le), wecm\] st)e(:i\[y t}oth the t)re{licate-argument struc-ture of the lexeme (t}y includillg nodes at whichits arguments must sut}stitute) and morl)h{}-syntactic onstraints uch as sut}je('t-verb agree-men| within the sl;rucl;ure associated with thel exeme.
This property is retbrred to as TAG'scztcndcd domain of locality.
N{)l;e that in anLTAG, I;here is no distinction betw{:en lexiconnnd grammar.
A Smnl)le grammar is shown inF igure  1.-We depart fl:om XTAG in our treatment of|;rees tbr adjuncts (such as adverl}s), an{t in-stead tbllow McDonMd and Pusteiovsky (1985).While in XTAG the elementary tree for an ad-.iuncl; conl;ains 1)hrase sl;ru{:i;ure |;hat atta{:hesl,he adjmmt to ll{}(tes in another tree with thestag anchored by71 \])et72 N7:~ A ux7.t Pro, l}0 T1',~ A(ljadjoins to directionNPNS, VPNP, VPSNrightrightrighth:ftrightrightFigure 2: Adjmmtion table tbr graamnar frag-lUe l l tsl)ecitie(1 label (say, VP) from the specified di-rection (say, fronl the left), in our systenl thetrees for adjuncts imply express their active va-lency, trot 11o1\[; how they connect to the lexicalitem they modi\[y.
This ilfl'ormal;ion is kept inthe  adjunct|on table which is associated with the.grammar; an excerpt is shown in Figure 2.
Treest;hat can adjoin to other trees (and have entriesin the adjunct|on table) ;~re called gamma-trees,the other trees (which can only t)e substitutedinto other trees) are alpha-trees.Note that we can refer to a tree by a combi-nation of its name, called its supertag, and itsanchor.
N)r example, (q is the supertag of anall)ha-tree anchored 1)y a noun that projects upto NP, wMle 72 is |;lie superi;ag of it gamma treeanchored by a noun that only t)rojects 1;{) N (we43assume adjectives are adjoined at N), and, asthe adjunction table shows, can right-adjoin toan N. So that es t imate~ is a particular treein our LTAG grammar.
Another tree that a su-pertag can be associated with is ~t~, which rep-resents the predicative use of a noun.1 Not allnouns are associated with all nominal supertags:the expletive there is only an cq.When we derive a sentence using an LTAG,we combine elementary trees fl'om the grmnmarusing adjunction and substitution.
For extort-pie, to derive the sentence There was no costestimate for the second phase from the gram-mar in Figure 1, we substitute the tree tbr thereinto the tree tbr estimate.
We then adjoin inthe trees tbr the auxiliary was, the determinerno, and the modit)ing noun cost.
Note thatthese adjunctions occur at different nodes: atVP, NP~ and N, respectively.
We then adjoin inthe preposition, into which we substitute ph, ase,into which we adjoin the and second.
Note thatall adjunctions are by gamma trees, and all sub-stitution by alpha trees.If we want to represent this derivation graphi-cally, we can do so in a derivation tree, which weobtain as follows: whelmver we adjoin or sub-stitute a tree t~ into a tree t2, we add a newdaughter labeled t~ to the node labeled tg.
Asexplained above, the name of each tree used isthe lexeme along with the supertag.
(We omitthe address at which substitution or adjunctiontakes place.)
The derivation tree t br our deriva-tion is shown in Figure 3.
As can be seen, thisstructure is a dependency tree and resembles arepresentation of lexical argument structure.aoshi (1987b) claims that TAG's propertiesmake it particularly suited as a syntactic rep-resentation tbr generation.
Specifically, its ex-tended domain of locality is useflfl in genera-tion tbr localizing syntactic properties (includ-ing word order as well as agreement and othermorphological processes), and lexicalization isuseful tbr providing an interfime from seman-tics (the deriw~tion tree represent the sentence'spredicate-argument structure).
Indeed, LTAGhas been used extensively in generation, start-ing with (McDonald and Pustejovsky, 1985).1Sentences such as Peter is a doctor can be analyzedwith with be as the head, as is more usual, or with doctoras the head, as is done in XTAG 1)eeause the be reallybehaves like an auxiliary, not like a flfll verb.estimatethere was no cost for74c~ 1 7 3 71 72phase c~the second71 75Figure 3: Derivation tree tbr LTAG deriw~tionof There was no cost estimate for the secondphase3 System Overv iewFERGUS is composed of three modules: the 2?eeChooser, the Unraveler, and the Linear Prece-dence (LP) Chooser.
The input to the system isa dependency tree as shown in Figm'e 4.
Notethat the nodes are labeled only with lexemes,not with supertags.
2 The Tree Chooser thenuses a stochastic tree model to choose TAGtrees fbr the nodes in the input structure.
Thisstep can be seen as analogous to "supertag-ging" (Bangalore and Joshi, 1999), except thatnow supertags (i.e., names of trees) must befbund tbr words in a tree rather than tbr wordsin a linear sequence.
The Unraveler then usesthe XTAG grammar to produce a lattice of allpossible linearizations that arc compatible withthe supertagged tree and the XTAG.
The LPChooser then chooses the most likely traversalof this lattice, given a language model.
We dis-cuss the three components in more detail.The Tree Chooser draws on a tree model,which is a representation of XTAG derivationtbr 1,000,000 words of the Wall Street Journal.
aThe ~IYee Chooser makes the simplifying as-2In the system that we used in the experiments de-scribed in Section 4, all words (including flmction words)need to be present in tt, e inlmt representation, flflly in-flected.
This is of course unrealistic for applications.
Inthis paper, we only aim to show that the use of a %'eeModel improves performance of a stochastic generator.See Section 6 for further discussion.3This was constructed from the Penn ~lS"ee Bank us-ing some heuristics, since the Pemt ~IYee Bank does notcontain hill head-dependent infornlation; as a result ofthe use of heuristics, the Tree Model is not flflly correct.44estimatethere was no cost forphasethe secondFigure 4: Inlmt to FEII.GUSSmnl)tions that the (:hoice of n tree.
tbr ~t nodedei)ends only on its daughter nodes, thus allow-ing \]'or a tot)-(lown dynamic l)rogrmnlning algo-ril;hln.
St)ccifically, a node 'q in the intml; si;ru(:-ture is assigned ~t sui)e, rt;~g s so th;tt the 1)rol):t -|)ilil;y of fin(ling the treelet (;()m\])ose(t of ~1 withsuperta X ,~ ;rod :dl of its (l;mght(;rs (as foun(tin I;he ini)ut sl;rucl;ure) is m;rximiz(;d, and suchl;ha, t .
'~ is (:Oml)a, tit)le with 'q'~s mother ~tll(l hersut)e, rtag .sin.
Here, "('omt)atible" l:nemis |;hat;the tree ret)resclfl;ed by .
'~ can 1)e adjoined orsubstii;uted into the tree ret)resented by ,%~, :m-('or(ling to the XTAG gra, nmmr.
For our exmn-t)le senl;en(:(;, the, ini)ui; 1;o the sysl,e,m is the t;reeshown in Figure d, and the oul;1)ul; fi'om l;he ~.l~ee(~hooser is the, tree.
;ts shown in \],'igure.
3.
No(;cthat while a (le, riw~tion tree in TAG fully Sl)(:(:-iiies a derivation and thus :t smTth,(:e, s(mte.n(:e,the oul;lmt fl:om the ~l~-ee Chooser (loes not;.There are two reasons.
\]?irstly, as exi)laine.d at;the end of Section 2, fin: us trees (:orrespond-ing to adjuncts are underspe.
(-itied with rest)ectto the adjunct ion site aat(t/or I;h(; a(ljmwl;iondirection (from left; or fl'Oln right) in the treeof the mother node, or they nmy 1)e m~orde.re(lwith respc(:t o other ad.iun('ts (tbr ex~nni)l(; , thefmnous adjective ordering t)roblem).
Secondly,Sul)ert;ags nl~y h~ve been (:hose.n incorre(:l;ly ornot at ;ill.The Unr;~veler takes ;~s input the senti-specitied derivation tree, (Figure 3) ml(l 1)ro-duces a word lattice.
Each node, in the deriw>tion tree consisl;s of ~t lexi(:al item m~d a su-pertag.
The linear order ()f the dmlghte.rs withrest)cot to l;he he;td 1)osil;ion of ;t sut)ertng isst)ecilied in the Xrl'AG grmnmar.
This informa-tion is (:onsulted to order the (laughter nodesITAG I)eliwtlion Treewilht,ut ,SIIpeltags"l'lCC (?h?~)scl / J -= - Tree\\  hIOne siagle sealli specified \ \'I'AG l)cdvalion \] lees \I \[(h'alll,llill'\]W(nd l.alticeShillgFigure 5: Ar(:hii;e(:ture of FERGUSwith rcsl)e(:t to the head at each le.vel of the(terival;ion tree.
in cases where ~ daughter nodeC&ll \])(I ntta('hed at more thin1 ()lie t)lace in thehead SUl)ertag (as is the (:;~se in our exmnt)le for"was and for), n disjunction of M1 these, positionsare.
assigned to the dmlghter node.
A botton>up algorithm the.n constructs ~ lattice that ell-(;odes the strings rei)re.sented 1)y (;~(:1~ level ofth(!
derivation tr(x'..
The latti('e~ at the.
root ofthe (teriwttion tr(w. is the result o171;\]m Um';tveler.
'Fhe resulting l~ttti(:(; for the ('.Xaml)h'.
s(ml;e.nce isshown in Figure 6.The \]~t;ti('.e.
OUtlmt from the.
Unra.veh'a" en-codes all t)ossible word sequences l)erniitted1)y the derivation strueialre.
We rmlk these.word sequen(:es in the order of their likeli-hoo(l 1)y composing the lattice with a finite-state machine rel)rese.nting ~ trigrmn bmgu~Ge1no(tel.
This mo(M has 1)ee.n ('onstructed froln1,000,0000 words of W~dl Stre, et Journal (:orpus.We 1)i(:k the 1)est path through the lattice, re-sulting from the comt)osition using the Viterl)ialgorithm, ;m(t this to I) ranking word sequenceis the outt)ut of the LP Chooser.4 Experiments and ResultsIn order l:o show |;ll~tl; Lhe llSO, of ~t tl:ce lIlode\]trod a, grmmnar doe.s indeed hell) pe, rformmme,we pe.rforme, d three experiments:45QFigure 6: Word lattice tbr example sentenceafter Tree Chooser and Unraveler using thesupertag-based model?
For the baseline experiment, we impose arandom tree structure ibr each sentence ofthe cortms and build a Tree Model whoseparameters consist of whether a lexeme l~tprecedes or tbllows her mother lexeme lm.We call this the Baseline Left-Right (LR)Model.
This model generates There wasest imate for  phase the second no cost .
forour example input.?
In the second experiment, we derive theparmneters tbr the LR model fl'om an an-notated corpus, in particular, the XTAGderivation tree cortms.
This model gener-ates Th, crc no est imate J'or the second phasewas cost .
tbr our example input.?
In the third experiment, as describedin Section 3, we employ the supertag-basedtree model whose parameters consist ofwhether a lexeme l d with supertag Sd is ztdependent of Im with supertag sin.
Fm'-thermore we use the supertag in~brmationprovided by the XTAG grammar to or-der the dependents.
This model generatesThcrc was no cost est imate for  the secondphase .
tbr our example input, which is in-deed the sentence ibund in the WSJ.As in the case of machine translation, evalu-ation in generation is a complex issue.
We usetwo metrics suggested in the MT literature (A1-shawl et al, 1.998) based on string edit; distancet)etween the outtmt of the generation systemand the reference corpus string front the WSJ.These metrics, simple accuracy and generationaccuracy, allow us to evaluate without humanintervention, automatically and objectively.
4Simple accuracy is the mnnber of insertion(I), deletion (D) and substitutions (S) errorsbetween the target language strings in the testcorpus and the strings produced by the genera-tion model.
The metric is summarized in Equa-tion (1).
R is the number of tokens in the targetstring.
This metric is similar to the string dis-tance metric used for measuring speech recog-nition accuracy.I + D + .qS implcAccuracy  = (1 - - --) (1)R4\~7c do not address the issue of whether these metricscan be used for comparative valuation of other genera-tion systems.46TreeModelSimt)le Go, ner~rtionAc(:ura('y AccuracyAverage timeper scnten(:(;Baseline LR Model 41.2% 56.2% 186ms~l?
(;cbank derived LI/.
Model 52.9% 66.8% 129msSut)ertag-bascd Model 58.!
)% 72.4% 517msTabl(; 1: Performance results front the thre(', tree models.Unlike sl)eech recognition, the task of gener-ation involves reordering of tokens.
The simpleaccuracy metric, however, penalizes a mist)lacc.dtoken twice, as a deletion from its c.xpo, ct('.d posi-tion and insertion at at different l)osition.
Wc llSO~ second metric, Generation A(:(:ura('y, shown inEqm~tion (2), which treats (hilt|ion of ~ token~tt OIIC location in 1;11(; string ~md th(; insertionof the same tok(m ~t anoth('a" location in timstring as one single mov('an(mt (;trot (M).
Thisis in addition to the rem~fining insertions (1 t)and deletions (Dl).Ge'n(~'rationAcc',,racy = (1 -54 + I I + 1)' -t- ,q )(2)The siml)lc, a(:cura('y, g(merntion a('(:ur;my a,n(ltim av(n:ag(~ time, ti)r goamration of (;a,(:h l;cst; s(~,u -t(m('c for tim tin'o,(', (}Xl)crinmnts ;~r(~ tabul~m,xl in%d)le 1.
The test set consist(xl of 1 O0 r~m(tonfly(:hoscn WS.I s(mt(m(:(; with ml ~w(n:age l ngt;h of16 words.
As can be seen, tim sut)crtng-1)asedmo(M |rot)roves over the LR model derived frommmotated ata ~md both models improv(; overthe baseline LR mod(:l.Sul)ertngs incorl)or~te richer infbrmationst|oh as argunmnt mid a(tjunci: disl;in(:tion, andnmnbcr and types of argunmnts.
YVe cxt)(;(:t toiml)rove the performance of the supcrtag-bas(;dmodel by taking these features into a(:(:ount.In ongoing work, we h~vc developed tree-based metrics in addition to the string-l)asedpresented here, in order to ewfluate sto(:hasticgener~tion models.
We h~vc also attempted tocorrelate these quantitative metrics with human(tualitativ(~ judgcnl(mts.
Ado, tail(~d dis(:ussionof these experiments and results is t)r(',s(mto, din (Bangalore (',|; al., 2000).5 Compar i son  w i th  Langk i lde  8zKn ightLangkildc and Knight (1998a) use a hand-(:rafted grmmmu: that maps semantic represen-tations to sequences of words with lino, arizationconstraints.
A COml)lex semantic st, ructur( ~, istrnnsl~ted to ~L lattice,, mid a bigrmn langungemode,1 t;hell (:hoost~,s &lltOllg {;}lo, l)ossiblo, surface,strings (moo(led in the l~ttice.The system of Langkildc 8~ Knight, Nitrogen,is similar to FERGUS in that generation is di-vided into two phases, the first of which resultsin a lattice fl'om which a surNcc si;ring is chosenduring the, s(;cond t)has(; using a language model(in our case a trigram model, in Nitrogen's casea.
1)igr~ml 1no(M).
Ih)w(',ver, (;t1(; first t)hases nr(',quit(', ditf(;r(mt.
In FEI(.GUS, we sI;m:i; with a lex-i(:~d pr(',dit:at(;-argulnent st;ru(;l;ur(~ while in Ni-trogen, a more s0,mantic intmt is used.
FEII.GUS(:ould (',asily |)(; augm(;nt(;d with a t)r(;t)ro(:cssorl;h~d; maps a so, m;mti(: rc, t)ro, s(mtal;ion t;o ore: syn-ta(:ti(: inl)ut; this is not the focus of our r(~sc~u'ch.\[Iowev(',r, ther(~ are two more imt)orl,mfl; differ-(m('es.
First, |;t1(; h~m(t-crafl;ed grmmnar in Ni-trogen maps dir(;(:tly from semantics to a linearr(~l)r(;sentation , skipping tho, nr|)or(;s(:(mt rcI)rc-sentation usually f~vore(t br the, rod)r(',s(mtn|;ionof syntax.
There is no stochastic tree model,since, the, re, ~tr(', no trees.
In FEI{GUS, in|tied('hoices arc, ma(tc stochastically t)ascd on timtree rcl)rcscntation in the "I?ce Chooser.
Thisallows us to capture stochastically certain long-(tisl;ance cfli',(:ts which n-grmns camlot, such assct)~ration of p;n'ts of a collocations (such aspeT:form an ope~ution) through interl)osing ad-juncts (John peT:formed a long, .somewhat e-dious, and quite frustrating opcration on hi,sborder collie).
Second, tim hand-('rafl;cd gram-ln;tr llSCd in FEll.
(-IUS was crafted indel)endcntlyfl'om the n(;('xl for gent, rat;ion and is a imrcly(l(;(:larative rcl)rcs(mtation of English syntax.
As47such, we can use it to handle morphological ef-fects such as agreement, which cannot in gen-eral be clone by an n-gram model and which are,at; the same time, descriptively straightforwardand which are handled by all non-stochasticgeneration modules.6 Conclus ion and Out lookWe have presented empirical evidence that us-ing a tree model in addition to a language modelcan improve stochastic NLG.FERGUS aS presented in this paper is notready to be used as a module in applications.Specifically, we will add a morphological compo-nent, a component that handles flmction words(auxiliaries, determiners), and a componentthat handles imnctuation.
In all three cases, wewill provide both knowledge-based and stochas-tic components, with the aim of comparing theirbehaviors, and using one type as a back-up tbrthe other type.
Finally, we will explore FI;R-OUS when applied to a language tbr which amuch more limited XTAG grammar is available(for example, specit\[ying only the basic sentenceword order as, sw, SVO, and speci(ying subject-verb agreement).
In the long run, we intendFEI/OUS to become a flexible system which willuse hand-crafted knowledge as much as possibleand stochastic models as much as necessary.ReferencesHiyan Alshawi, Srinivas Bangalore, and ShonaDouglas.
1998.
Automatic acquisition of hi-erarchical transduction models tbr machine translation.
In Proceedings of the 36th AnnualMeeting Association for Computational Lin-guistics, Montreal, Canada.Srinivas Bangalore and Aravind Joshi.
1999.Supertagging: An approach to ahnost pars-ing.
Computational Linguistics, 25(2).Sriniw~s Bangalore, Owen Rainbow, and SteveWhittaker.
2000.
Ewfluation Metrics forGeneration.
In Proceedings of InternationalCor~:ferenee on Natural Language Generation,Mitzpe Ramon.
Isreal.Aravind K. Joshi.
1987a.
An introduction toTree Adjoining Grammars.
In A. Manaster-Ramer, editor, Mathematics of Language,pages 87-115.
John Benjamins, Amsterdam.Aravind K. Joshi.
1987b.
Tlm relevance oftree adjoining grammar to generation.
InGerard Kempeu, editor, Natural LanguageGeneration: New Results in Artificial In-teUigence, Psychology and Linguistics, pages233 252.
Kluwer Academic Publishers, Dor-drecht /Boston /Lancaster.Irene Langkilde and Kevin Knight.
1998a.
Gen-eration that exploits corpus-based statisticalknowledge.
In 36th Meeting of the Associa-tion .for Computational Linguistics and 17thInternational Cor~:\['crcnce on ComputationalLinguistics (COLING-A CL'98), pages 704-710, Montrdal, Canada.Irene Langkilde and Kevin Knight.
1998b.The practical value of n-grams in genera-tion.
In Proceedings of the Ninth Interna-tional Natural Language Generation Work-shop (INLG'98), Niagara-on-the-Lake, On-tario.Irene Langkilde and Kevin Knight.
2000.Forest-based statistical sentence generation.In Proceedings of First North American A CL,Seattle, USA, May.Robert Malouf.
1999.
Two methods tbr 1)re-dieting the order of prenonfinal t~djectives inenglish.
In Pwceedings of CLINg9.David D. McDonMd and James D. Pusteiovsky.1985.
%~gs as a grammatical formalism tbrgeneration.
In 23rd Meeting of the Associa-tion for Computational Linguistics (A CL '85),pages 94 103, Chicago, IL.Owen l:\[ambow and Tany~ Korelsky.
1992.
Ap-plied text generation.
In Third Conference onApplied Natural Language Processing, pages40 47, %ento, Italy.Adwait t/.atllaparkhi.
2000.
Trainable methodsfor surface natural language generation.
InProceedings of First North American ACL,Seattle, USA, May.Ehud Reiter.
1994.
Has a consensus NL gen-eration architecture appeared, and is it psy-cholinguistically plausible?
In Proceedings ofthe 7th International Workshop on NaturalLanguage Generation, pages 163-170, Maine.The XTAG-Group.
1999.
A lexicalized %'eeAdjoining Grammar for English.
TechnicalReport ht tp  ://w~rw.
c is .
upenn, edu/~xtag/tech- repor t / tech- repor t  .htral, The Insti-tute for Research in Cognitive Science, Uni-versity of Pennsylvania.48
