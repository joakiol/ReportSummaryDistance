Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 848?855,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsUnsupervised Coreference Resolution in a Nonparametric Bayesian ModelAria Haghighi and Dan KleinComputer Science DivisionUC Berkeley{aria42, klein}@cs.berkeley.eduAbstractWe present an unsupervised, nonparamet-ric Bayesian approach to coreference reso-lution which models both global entity iden-tity across a corpus as well as the sequen-tial anaphoric structure within each docu-ment.
While most existing coreference workis driven by pairwise decisions, our modelis fully generative, producing each mentionfrom a combination of global entity proper-ties and local attentional state.
Despite be-ing unsupervised, our system achieves a 70.3MUC F1 measure on the MUC-6 test set,broadly in the range of some recent super-vised results.1 IntroductionReferring to an entity in natural language canbroadly be decomposed into two processes.
First,speakers directly introduce new entities into dis-course, entities which may be shared across dis-courses.
This initial reference is typically accom-plished with proper or nominal expressions.
Second,speakers refer back to entities already introduced.This anaphoric reference is canonically, though ofcourse not always, accomplished with pronouns, andis governed by linguistic and cognitive constraints.In this paper, we present a nonparametric generativemodel of a document corpus which naturally con-nects these two processes.Most recent coreference resolution work has fo-cused on the task of deciding which mentions (nounphrases) in a document are coreferent.
The domi-nant approach is to decompose the task into a col-lection of pairwise coreference decisions.
One thenapplies discriminative learning methods to pairs ofmentions, using features which encode propertiessuch as distance, syntactic environment, and so on(Soon et al, 2001; Ng and Cardie, 2002).
Althoughsuch approaches have been successful, they haveseveral liabilities.
First, rich features require plen-tiful labeled data, which we do not have for corefer-ence tasks in most domains and languages.
Second,coreference is inherently a clustering or partitioningtask.
Naive pairwise methods can and do fail to pro-duce coherent partitions.
One classic solution is tomake greedy left-to-right linkage decisions.
Recentwork has addressed this issue in more global ways.McCallum and Wellner (2004) use graph partion-ing in order to reconcile pairwise scores into a finalcoherent clustering.
Nonetheless, all these systemscrucially rely on pairwise models because cluster-level models are much harder to work with, combi-natorially, in discriminative approaches.Another thread of coreference work has focusedon the problem of identifying matches betweendocuments (Milch et al, 2005; Bhattacharya andGetoor, 2006; Daume and Marcu, 2005).
Thesemethods ignore the sequential anaphoric structureinside documents, but construct models of how andwhen entities are shared between them.1 Thesemodels, as ours, are generative ones, since the fo-cus is on cluster discovery and the data is generallyunlabeled.In this paper, we present a novel, fully genera-tive, nonparametric Bayesian model of mentions in adocument corpus.
Our model captures both within-and cross-document coreference.
At the top, a hi-erarchical Dirichlet process (Teh et al, 2006) cap-1Milch et al (2005) works with citations rather than dis-courses and does model the linear structure of the citations.848tures cross-document entity (and parameter) shar-ing, while, at the bottom, a sequential model ofsalience captures within-document sequential struc-ture.
As a joint model of several kinds of discoursevariables, it can be used to make predictions abouteither kind of coreference, though we focus experi-mentally on within-document measures.
To the bestof our ability to compare, our model achieves thebest unsupervised coreference performance.2 Experimental SetupWe adopt the terminology of the Automatic ContextExtraction (ACE) task (NIST, 2004).
For this paper,we assume that each document in a corpus consistsof a set of mentions, typically noun phrases.
Eachmention is a reference to some entity in the domainof discourse.
The coreference resolution task is topartition the mentions according to referent.
Men-tions can be divided into three categories, propermentions (names), nominal mentions (descriptions),and pronominal mentions (pronouns).In section 3, we present a sequence of increas-ingly enriched models, motivating each from short-comings of the previous.
As we go, we will indicatethe performance of each model on data from ACE2004 (NIST, 2004).
In particular, we used as ourdevelopment corpus the English translations of theArabic and Chinese treebanks, comprising 95 docu-ments and about 3,905 mentions.
This data was usedheavily for model design and hyperparameter selec-tion.
In section 5, we present final results for newtest data from MUC-6 on which no tuning or devel-opment was performed.
This test data will form ourbasis for comparison to previous work.In all experiments, as is common, we will assumethat we have been given as part of our input the truemention boundaries, the head word of each mentionand the mention type (proper, nominal, or pronom-inal).
For the ACE data sets, the head and mentiontype are given as part of the mention annotation.
Forthe MUC data, the head was crudely chosen to bethe rightmost mention token, and the mention typewas automatically detected.
We will not assumeany other information to be present in the data be-yond the text itself.
In particular, unlike much re-lated work, we do not assume gold named entityrecognition (NER) labels; indeed we do not assumeobserved NER labels or POS tags at all.
Our pri-?
?K?KZiH iJI????
?ZiH iIJ(a) (b)Figure 1: Graphical model depiction of document level en-tity models described in sections 3.1 and 3.2 respectively.
Theshaded nodes indicate observed variables.mary performance metric will be the MUC F1 mea-sure (Vilain et al, 1995), commonly used to evalu-ate coreference systems on a within-document basis.Since our system relies on sampling, all results areaveraged over five random runs.3 Coreference Resolution ModelsIn this section, we present a sequence of gener-ative coreference resolution models for documentcorpora.
All are essentially mixture models, wherethe mixture components correspond to entities.
Asfar as notation, we assume a collection of I docu-ments, each with Ji mentions.
We use random vari-ables Z to refer to (indices of) entities.
We will use?z to denote the parameters for an entity z, and ?to refer to the concatenation of all such ?z .
X willrefer somewhat loosely to the collection of variablesassociated with a mention in our model (such as thehead or gender).
We will be explicit about X and ?zshortly.Our goal will be to find the setting of the entityindices which maximize the posterior probability:Z?
= argmaxZP (Z|X) = argmaxZP (Z,X)= argmaxZ?P (Z,X,?)
dP (?
)where Z,X, and ?
denote all the entity indices, ob-served values, and parameters of the model.
Notethat we take a Bayesian approach in which all pa-rameters are integrated out (or sampled).
The infer-ence task is thus primarily a search problem over theindex labels Z.849(a)(b)(c)The Weir Group1, whose2headquarters3is in the US4, is a large, specialized corporation5investing in the area of electricitygeneration.
This  power plant6, which7will be situated in Rudong8, Jiangsu9, has an annual generation capacity of 2.4 million kilowatts.The Weir Group1, whose1headquarters2is in the US3, is a large, specialized corporation4investing in the area of electricitygeneration.
This  power plant5, which1will be situated in Rudong6, Jiangsu7, has an annual generation capacity of 2.4 million kilowatts.The Weir Group1, whose1headquarters2is in the US3, is a large, specialized corporation4investing in the area of electricitygeneration.
This  power plant5, which5will be situated in Rudong6, Jiangsu7, has an annual generation capacity of 2.4 million kilowatts.Figure 2: Example output from various models.
The output from (a) is from the infinite mixture model of section 3.2.
It incorrectlylabels both boxed cases of anaphora.
The output from (b) uses the pronoun head model of section 3.3.
It correctly labels the firstcase of anaphora but incorrectly labels the second pronominal as being coreferent with the dominant document entity The WeirGroup.
This error is fixed by adding the salience feature component from section 3.4 as can be seen in (c).3.1 A Finite Mixture ModelOur first, overly simplistic, corpus model is the stan-dard finite mixture of multinomials shown in fig-ure 1(a).
In this model, each document is indepen-dent save for some global hyperparameters.
Insideeach document, there is a finite mixture model witha fixed numberK of components.
The distribution ?over components (entities) is a draw from a symmet-ric Dirichlet distribution with concentration ?.
Foreach mention in the document, we choose a compo-nent (an entity index) z from ?.
Entity z is then asso-ciated with a multinomial emission distribution overhead words with parameters ?hZ , which are drawnfrom a symmetric Dirichlet over possible mentionheads with concentration ?H .2 Note that here the Xfor a mention consists only of the mention head H .As we enrich our models, we simultaneously de-velop an accompanying Gibbs sampling procedureto obtain samples from P (Z|X).3 For now, all headsH are observed and all parameters (?
and ?)
can beintegrated out analytically: for details see Teh et al(2006).
The only sampling is for the values of Zi,j ,the entity index of mention j in document i. Therelevant conditional distribution is:4P (Zi,j |Z?i,j ,H) ?
P (Zi,j |Z?i,j)P (Hi,j |Z,H?i,j)where Hi,j is the head of mention j in document i.Expanding each term, we have the contribution ofthe prior:P (Zi,j = z|Z?i,j) ?
nz + ?2In general, we will use a subscripted ?
to indicate concen-tration for finite Dirichlet distributions.
Unless otherwise spec-ified, ?
concentration parameters will be set to e?4 and omittedfrom diagrams.3One could use the EM algorithm with this model, but EMwill not extend effectively to the subsequent models.4Here, Z?i,j denotes Z?
{Zi,j}where nz is the number of elements of Z?i,j withentity index z.
Similarly we have for the contribu-tion of the emissions:P (Hi,j = h|Z,H?i,j) ?
nh,z + ?Hwhere nh,z is the number of times we have seen headh associated with entity index z in (Z,H?i,j).3.2 An Infinite Mixture ModelA clear drawback of the finite mixture model is therequirement that we specify a priori a number of en-tities K for a document.
We would like our modelto select K in an effective, principled way.
A mech-anism for doing so is to replace the finite Dirichletprior on ?
with the non-parametric Dirichlet process(DP) prior (Ferguson, 1973).5 Doing so gives themodel in figure 1(b).
Note that we now list an in-finite number of mixture components in this modelsince there can be an unbounded number of entities.Rather than a finite ?
with a symmetric Dirichletdistribution, in which draws tend to have balancedclusters, we now have an infinite ?.
However, mostdraws will have weights which decay exponentiallyquickly in the prior (though not necessarily in theposterior).
Therefore, there is a natural penalty foreach cluster which is actually used.With Z observed during sampling, we can inte-grate out ?
and calculate P (Zi,j |Z?i,j) analytically,using the Chinese restaurant process representation:P (Zi,j = z|Z?i,j) ?
{?, if z = znewnz, otherwise(1)where znew is a new entity index not used in Z?i,jand nz is the number of mentions that have entity in-dex z.
Aside from this change, sampling is identical5We do not give a detailed presentation of the Dirichlet pro-cess here, but see Teh et al (2006) for a presentation.850PERS : 0.97,   LOC : 0.01,  ORG: 0.01,  MISC: 0.01Entity TypeSING: 0.99, PLURAL: 0.01NumberMALE: 0.98, FEM: 0.01, NEUTER: 0.01GenderBush : 0.90,   President : 0.06,  .....Head?t?h?n?gX =Z ZM T N GH?
??
(a) (b)Figure 3: (a) An entity and its parameters.
(b)The head modeldescribed in section 3.3.
The shaded nodes indicate observedvariables.
The mention type determines which set of parents areused.
The dependence of mention variable on entity parameters?
and pronoun head model ?
is omitted.to the finite mixture case, though with the numberof clusters actually occupied in each sample driftingupwards or downwards.This model yielded a 54.5 F1 on our develop-ment data.6 This model is, however, hopelesslycrude, capturing nothing of the structure of coref-erence.
Its largest empirical problem is that, un-surprisingly, pronoun mentions such as he are giventheir own clusters, not labeled as coreferent with anynon-pronominal mention (see figure 2(a)).3.3 Pronoun Head ModelWhile an entity-specific multinomial distributionover heads makes sense for proper, and some nom-inal, mention heads, it does not make sense to gen-erate pronominal mentions this same way.
I.e., allentities can be referred to by generic pronouns, thechoice of which depends on entity properties such asgender, not the specific entity.We therefore enrich an entity?s parameters ?
tocontain not only a distribution over lexical heads?h, but also distributions (?t, ?g, ?n) over proper-ties, where ?t parametrizes a distribution over en-tity types (PER, LOC, ORG, MISC), and ?g for gen-der (MALE, FEMALE, NEUTER), and ?n for number(SG, PL).7 We assume each of these property distri-butions is drawn from a symmetric Dirichlet distri-bution with small concentration parameter in orderto encourage a peaked posterior distribution.6See section 4 for inference details.7It might seem that entities should simply have, for exam-ple, a gender g rather than a distribution over genders ?g .
Thereare two reasons to adopt the softer approach.
First, one canrationalize it in principle, for entities like cars or ships whosegrammatical gender is not deterministic.
However, the real rea-son is that inference is simplified.
In any event, we found theseproperty distributions to be highly determinized in the posterior.????
?Z1 Z 3L 1S 1T1N1G1M1=NAMZ 2L 2S 2N2G2M2=NOMT2H2 ="president"H1 ="Bush"H3 ="he"N2 =SGG2 =MALEM3=PROT2L 3S 3?IFigure 4: Coreference model at the document level with entityproperties as well salience lists used for mention type distri-butions.
The diamond nodes indicate deterministic functions.Shaded nodes indicate observed variables.
Although it appearsthat each mention head node has many parents, for a given men-tion type, the mention head depends on only a small subset.
De-pendencies involving parameters ?
and ?
are omitted.Previously, when an entity z generated a mention,it drew a head word from ?hz .
It now undergoes amore complex and structured process.
It first drawsan entity type T , a gender G, a number N from thedistributions ?t, ?g, and ?n, respectively.
Once theproperties are fetched, a mention type M is chosen(proper, nominal, pronoun), according to a globalmultinomial (again with a symmetric Dirichlet priorand parameter ?M ).
This corresponds to the (tem-porary) assumption that the speaker makes a randomi.i.d.
choice for the type of each mention.Our head model will then generate a head, con-ditioning on the entity, its properties, and the men-tion type, as shown in figure 3(b).
If M is not apronoun, the head is drawn directly from the en-tity head multinomial with parameters ?hz .
Other-wise, it is drawn based on a global pronoun head dis-tribution, conditioning on the entity properties andparametrized by ?.
Formally, it is given by:P (H|Z, T,G,N,M,?,?)
={P (H|T,G,N,?
), if M =PROP (H|?hZ), otherwiseAlthough we can observe the number and gen-der draws for some mentions, like personal pro-nouns, there are some for which properties aren?tobserved (e.g., it).
Because the entity prop-erty draws are not (all) observed, we must nowsample the unobserved ones as well as the en-tity indices Z.
For instance, we could sample851Salience Feature Pronoun Proper NominalTOP 0.75 0.17 0.08HIGH 0.55 0.28 0.17MID 0.39 0.40 0.21LOW 0.20 0.45 0.35NONE 0.00 0.88 0.12Table 1: Posterior distribution of mention type given salienceby bucketing entity activation rank.
Pronouns are preferred forentities which have high salience and non-pronominal mentionsare preferred for inactive entities.Ti,j , the entity type of pronominal mention j indocument i, using, P (Ti,j |Z,N,G,H,T?i,j) ?P (Ti,j |Z)P (Hi,j |T,N,G,H), where the posteriordistributions on the right hand side are straight-forward because the parameter priors are all finiteDirichlet.
Sampling G and N are identical.Of course we have prior knowledge about the re-lationship between entity type and pronoun headchoice.
For example, we expect that he is used formentions with T = PERSON.
In general, we assumethat for each pronominal head we have a list of com-patible entity types, which we encode via the prioron ?.
We assume ?
is drawn from a Dirichlet distri-bution where each pronoun head is given a syntheticcount of (1 + ?P ) for each (t, g, n) where t is com-patible with the pronoun and given ?P otherwise.So, while it will be possible in the posterior to usehe to refer to a non-person, it will be biased towardsbeing used with persons.This model gives substantially improved predic-tions: 64.1 F1 on our development data.
As can beseen in figure 2(b), this model does correct the sys-tematic problem of pronouns being considered theirown entities.
However, it still does not have a pref-erence for associating pronominal references to en-tities which are in any way local.3.4 Adding SalienceWe would like our model to capture how mentiontypes are generated for a given entity in a robust andsomewhat language independent way.
The choice ofentities may reasonably be considered to be indepen-dent given the mixing weights ?, but how we realizean entity is strongly dependent on context (Ge et al,1998).In order to capture this in our model, we enrichit as shown in figure 4.
As we proceed through adocument, generating entities and their mentions,we maintain a list of the active entities and theirsaliences, or activity scores.
Every time an entity ismentioned, we increment its activity score by 1, andevery time we move to generate the next mention,all activity scores decay by a constant factor of 0.5.This gives rise to an ordered list of entity activations,L, where the rank of an entity decays exponentiallyas new mentions are generated.
We call this list asalience list.
Given a salience list, L, each possibleentity z has some rank on this list.
We discretizethese ranks into five buckets S: TOP (1), HIGH (2-3), MID (4-6), LOW (7+), and NONE.
Given the entitychoices Z, both the list L and buckets S are deter-ministic (see figure 4).
We assume that the mentiontype M is conditioned on S as shown in figure 4.We note that correctly sampling an entity now re-quires that we incorporate terms for how a changewill affect all future salience values.
This changesour sampling equation for existing entities:P (Zi,j = z|Z?i,j) ?
nz?j?
?jP (Mi,j?
|Si,j?
,Z) (2)where the product ranges over future mentions in thedocument and Si,j?
is the value of future saliencefeature given the setting of all entities, including set-ting the current entity Zi,j to z.
A similar equationholds for sampling a new entity.
Note that, as dis-cussed below, this full product can be truncated asan approximation.This model gives a 71.5 F1 on our developmentdata.
Table 1 shows the posterior distribution of themention type given the salience feature.
This modelfixes many anaphora errors and in particular fixes thesecond anaphora error in figure 2(c).3.5 Cross Document CoreferenceOne advantage of a fully generative approach is thatwe can allow entities to be shared between docu-ments in a principled way, giving us the capacity todo cross-document coreference.
Moreover, sharingacross documents pools information about the prop-erties of an entity across documents.We can easily link entities across a corpus by as-suming that the pool of entities is global, with globalmixing weights ?0 drawn from a DP prior withconcentration parameter ?.
Each document uses852????
?Z1 Z 3L 1S 1T1N1G1M1=NAMZ 2L 2S 2N2G2M2=NOMT2H2 ="president"H1 ="Bush"H3 ="he"N2 =SGG2 =MALEM3=PROT2L 3S 3?0??
?IFigure 5: Graphical depiction of the HDP coreference modeldescribed in section 3.5.
The dependencies between the globalentity parameters ?
and pronoun head parameters ?
on the men-tion observations are not depicted.the same global entities, but each has a document-specific distribution ?i drawn from a DP centered on?0 with concentration parameter ?.
Up to the pointwhere entities are chosen, this formulation followsthe basic hierarchical Dirichlet process prior of Tehet al (2006).
Once the entities are chosen, our modelfor the realization of the mentions is as before.
Thismodel is depicted graphically in figure 5.Although it is possible to integrate out ?0 as wedid the individual ?i, we instead choose for ef-ficiency and simplicity to sample the global mix-ture distribution ?0 from the posterior distributionP (?0|Z).8 The mention generation terms in themodel and sampler are unchanged.In the full hierarchical model, our equation (1) forsampling entities, ignoring the salience componentof section 3.4, becomes:P (Zi,j = z|Z?i,j , ?0)?{?
?u0 , if z = znewnz + ?
?z0 , otherwisewhere ?z0 is the probability of the entity z under thesampled global entity distribution and ?u0 is the un-known component mass of this distribution.The HDP layer of sharing improves the model?spredictions to 72.5 F1 on our development data.
Weshould emphasize that our evaluation is of courseper-document and does not reflect cross-documentcoreference decisions, only the gains through cross-document sharing (see section 6.2).8We do not give the details here; see Teh et al (2006) for de-tails on how to implement this component of the sampler (called?direct assignment?
in that reference).4 Inference DetailsUp until now, we?ve discussed Gibbs sampling, butwe are not interested in sampling from the poste-rior P (Z|X), but in finding its mode.
Instead ofsampling directly from the posterior distribution, weinstead sample entities proportionally to exponen-tiated entity posteriors.
The exponent is given byexp cik?1 , where i is the current round number (start-ing at i = 0), c = 1.5 and k = 20 is the total num-ber of sampling epochs.
This slowly raises the pos-terior exponent from 1.0 to ec.
In our experiments,we found this procedure to outperform simulated an-nealing.
We also found sampling the T , G, and Nvariables to be particularly inefficient, so instead wemaintain soft counts over each of these variables anduse these in place of a hard sampling scheme.
Wealso found that correctly accounting for the futureimpact of salience changes to be particularly ineffi-cient.
However, ignoring those terms entirely madenegligible difference in final accuracy.95 Final ExperimentsWe present our final experiments using the fullmodel developed in section 3.
As in section 3, weuse true mention boundaries and evaluate using theMUC F1 measure (Vilain et al, 1995).
All hyper-parameters were tuned on the development set only.The document concentration parameter ?
was set bytaking a constant proportion of the average numberof mentions in a document across the corpus.
Thisnumber was chosen to minimize the squared errorbetween the number of proposed entities and trueentities in a document.
It was not tuned to maximizethe F1 measure.
A coefficient of 0.4 was chosen.The global concentration coefficient ?
was chosento be a constant proportion of ?M , where M is thenumber of documents in the corpus.
We found 0.15to be a good value using the same least-square pro-cedure.
The values for these coefficients were notchanged for the experiments on the test sets.5.1 MUC-6Our main evaluation is on the standard MUC-6 for-mal test set.10 The standard experimental setup for9This corresponds to truncating equation (2) at j?
= j.10Since the MUC data is not annotated with mention types,we automatically detect this information in the same way as Luo853Dataset Num Docs.
Prec.
Recall F1MUC-6 60 80.8 52.8 63.9+DRYRUN-TRAIN 251 79.1 59.7 68.0+ENGLISH-NWIRE 381 80.4 62.4 70.3Dataset Prec.
Recall F1ENGLISH-NWIRE 66.7 62.3 64.2ENGLISH-BNEWS 63.2 61.3 62.3CHINESE-NWIRE 71.6 63.3 67.2CHINESE-BNEWS 71.2 61.8 66.2(a) (b)Table 2: Formal Results: Our system evaluated using the MUC model theoretic measure Vilain et al (1995).
The table in (a) isour performance on the thirty document MUC-6 formal test set with increasing amounts of training data.
In all cases for the table,we are evaluating on the same thirty document test set which is included in our training set, since our system in unsupervised.
Thetable in (b) is our performance on the ACE 2004 training sets.this data is a 30/30 document train/test split.
Train-ing our system on all 60 documents of the trainingand test set (as this is in an unsupervised system,the unlabeled test documents are present at train-ing time), but evaluating only on the test documents,gave 63.9 F1 and is labeled MUC-6 in table 2(a).One advantage of an unsupervised approach isthat we can easily utilize more data when learning amodel.
We demonstrate the effectiveness of this factby evaluating on the MUC-6 test documents with in-creasing amounts of unannotated training data.
Wefirst added the 191 documents from the MUC-6dryrun training set (which were not part of the train-ing data for official MUC-6 evaluation).
This modelgave 68.0 F1 and is labeled +DRYRUN-TRAIN in ta-ble 2(a).
We then added the ACE ENGLISH-NWIREtraining data, which is from a different corpora thanthe MUC-6 test set and from a different time period.This model gave 70.3 F1 and is labeled +ENGLISH-NWIRE in table 2(a).Our results on this test set are surprisingly com-parable to, though slightly lower than, some recentsupervised systems.
McCallum and Wellner (2004)report 73.4 F1 on the formal MUC-6 test set, whichis reasonably close to our best MUC-6 number of70.3 F1.
McCallum and Wellner (2004) also reporta much lower 91.6 F1 on only proper nouns men-tions.
Our system achieves a 89.8 F1 when evalu-ation is restricted to only proper mentions.11 Theet al (2004).
A mention is proper if it is annotated with NERinformation.
It is a pronoun if the head is on the list of En-glish pronouns.
Otherwise, it is a nominal mention.
Note we donot use the NER information for any purpose but determiningwhether the mention is proper.11The best results we know on the MUC-6 test set using thestandard setting are due to Luo et al (2004) who report a 81.3F1 (much higher than others).
However, it is not clear this is acomparable number, due to the apparent use of gold NER fea-tures, which provide a strong clue to coreference.
Regardless, itis unsurprising that their system, which has many rich features,would outperform ours.HEAD ENT TYPE GENDER NUMBERBush: 1.0 PERS MALE SGAP: 1.0 ORG NEUTER PLviacom: 0.64, company: 0.36 ORG NEUTER SGteamsters: 0.22, union: 0.78, MISC NEUTER PLTable 3: Frequent entities occurring across documents alongwith head distribution and mode of property distributions.closest comparable unsupervised system is Cardieand Wagstaff (1999) who use pairwise NP distancesto cluster document mentions.
They report a 53.6 F1on MUC6 when tuning distance metric weights tomaximize F1 on the development set.5.2 ACE 2004We also performed experiments on ACE 2004 data.Due to licensing restrictions, we did not have accessto the ACE 2004 formal development and test sets,and so the results presented are on the training sets.We report results on the newswire section (NWIREin table 2b) and the broadcast news section (BNEWSin table 2b).
These datasets include the prenomi-nal mention type, which is not present in the MUC-6 data.
We treated prenominals analogously to thetreatment of proper and nominal mentions.We also tested our system on the Chinesenewswire and broadcast news sections of the ACE2004 training sets.
Our relatively higher perfor-mance on Chinese compared to English is perhapsdue to the lack of prenominal mentions in the Chi-nese data, as well as the presence of fewer pronounscompared to English.Our ACE results are difficult to compare exactlyto previous work because we did not have accessto the restricted formal test set.
However, we canperform a rough comparison between our results onthe training data (without coreference annotation) tosupervised work which has used the same trainingdata (with coreference annotation) and evaluated onthe formal test set.
Denis and Baldridge (2007) re-854port 67.1 F1 and 69.2 F1 on the English NWIRE andBNEWS respectively using true mention boundaries.While our system underperforms the supervised sys-tems, its accuracy is nonetheless promising.6 Discussion6.1 Error AnalysisThe largest source of error in our system is betweencoreferent proper and nominal mentions.
The mostcommon examples of this kind of error are appos-itive usages e.g.
George W. Bush, president of theUS, visited Idaho.
Another error of this sort can beseen in figure 2, where the corporation mention isnot labeled coreferent with the The Weir Groupmen-tion.
Examples such as these illustrate the regular (atleast in newswire) phenomenon that nominal men-tions are used with informative intent, even when theentity is salient and a pronoun could have been usedunambiguously.
This aspect of nominal mentions isentirely unmodeled in our system.6.2 Global CoreferenceSince we do not have labeled cross-document coref-erence data, we cannot evaluate our system?s cross-document performance quantitatively.
However, inaddition to observing the within-document gainsfrom sharing shown in section 3, we can manuallyinspect the most frequently occurring entities in ourcorpora.
Table 3 shows some of the most frequentlyoccurring entities across the English ACE NWIREcorpus.
Note that Bush is the most frequent entity,though his (and others?)
nominal cluster presidentis mistakenly its own entity.
Merging of proper andnominal clusters does occur as can be seen in table 3.6.3 Unsupervised NERWe can use our model to for unsupervised NERtagging: for each proper mention, assign the modeof the generating entity?s distribution over entitytypes.
Note that in our model the only way an en-tity becomes associated with an entity type is bythe pronouns used to refer to it.12 If we evaluateour system as an unsupervised NER tagger for theproper mentions in the MUC-6 test set, it yields a12Ge et al (1998) exploit a similar idea to assign gender toproper mentions.per-label accuracy of 61.2% (on MUC labels).
Al-though nowhere near the performance of state-of-the-art systems, this result beats a simple baseline ofalways guessing PERSON (the most common entitytype), which yields 46.4%.
This result is interest-ing given that the model was not developed for thepurpose of inferring entity types whatsoever.7 ConclusionWe have presented a novel, unsupervised approachto coreference resolution: global entities are sharedacross documents, the number of entities is deter-mined by the model, and mentions are generated bya sequential salience model and a model of pronoun-entity association.
Although our system does notperform quite as well as state-of-the-art supervisedsystems, its performance is in the same generalrange, despite the system being unsupervised.ReferencesI.
Bhattacharya and L. Getoor.
2006.
A latent dirichlet modelfor unsupervised entity resolution.
SIAM conference on datamining.Claire Cardie and Kiri Wagstaff.
1999.
Noun phrase corefer-ence as clustering.
EMNLP.Hal Daume and Daniel Marcu.
2005.
A Bayesian model for su-pervised clustering with the Dirichlet process prior.
JMLR.Pascal Denis and Jason Baldridge.
2007.
Global, joint determi-nation of anaphoricity and coreference resolution using inte-ger programming.
HLT-NAACL.Thomas Ferguson.
1973.
A bayesian analysis of some non-parametric problems.
Annals of Statistics.Niyu Ge, John Hale, and Eugene Charniak.
1998.
A statisticalapproach to anaphora resolution.
Sixth Workshop on VeryLarge Corpora.Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda Kamb-hatla, and Salim Roukos.
2004.
A mention-synchronouscoreference resolution algorithm based on the bell tree.
ACL.Andrew McCallum and Ben Wellner.
2004.
Conditional mod-els of identity uncertainty with application to noun corefer-ence.
NIPS.Brian Milch, Bhaskara Marthi, Stuart Russell, David Sontag,Daniel L. Ong, and Andrey Kolobov.
2005.
Blog: Proba-bilistic models with unknown objects.
IJCAI.Vincent Ng and Claire Cardie.
2002.
Improving machine learn-ing approaches to coreference resolution.
ACL.NIST.
2004.
The ACE evaluation plan.W.
Soon, H. Ng, and D. Lim.
2001.
A machine learning ap-proach to coreference resolution of noun phrases.
Computa-tional Linguistics.Yee Whye Teh, Michael Jordan, Matthew Beal, and David Blei.2006.
Hierarchical dirichlet processes.
Journal of the Amer-ican Statistical Association, 101.Marc Vilain, John Burger, John Aberdeen, Dennis Connolly,and Lynette Hirschman.
1995.
A model-theoretic corefer-ence scoring scheme.
MUC-6.855
