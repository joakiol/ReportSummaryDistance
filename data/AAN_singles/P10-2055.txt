Proceedings of the ACL 2010 Conference Short Papers, pages 296?300,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsA Semi-Supervised Key Phrase Extraction Approach: Learning fromTitle Phrases through a Document Semantic NetworkDecong Li1, Sujian Li1, Wenjie Li2, Wei Wang1, Weiguang Qu31Key Laboratory of Computational Linguistics, Peking University2Department of Computing, The Hong Kong Polytechnic University3 School of Computer Science and Technology, Nanjing Normal University{lidecong,lisujian, wwei }@pku.edu.cn   cswjli@comp.polyu.edu.hk  wgqu@njnu.edu.cnAbstractIt is a fundamental and important task to ex-tract key phrases from documents.
Generally,phrases in a document are not independent indelivering the content of the document.
In or-der to capture and make better use of their re-lationships in key phrase extraction, we sug-gest exploring the Wikipedia knowledge tomodel a document as a semantic network,where both n-ary and binary relationshipsamong phrases are formulated.
Based on acommonly accepted assumption that the titleof a document is always elaborated to reflectthe content of a document and consequentlykey phrases tend to have close semantics to thetitle, we propose a novel semi-supervised keyphrase extraction approach in this paper bycomputing the phrase importance in the se-mantic network, through which the influenceof title phrases is propagated to the otherphrases iteratively.
Experimental results dem-onstrate the remarkable performance of thisapproach.1 IntroductionKey phrases are defined as the phrases that ex-press the main content of a document.
Guided bythe given key phrases, people can easily under-stand what a document describes, saving a greatamount of time reading the whole text.
Conse-quently, automatic key phrase extraction is inhigh demand.
Meanwhile, it is also fundamentalto many other natural language processing appli-cations, such as information retrieval, text clus-tering and so on.Key phrase extraction can be normally cast asa ranking problem solved by either supervised orunsupervised methods.
Supervised learning re-quires a large amount of expensive training data,whereas unsupervised learning totally ignoreshuman knowledge.
To overcome the deficienciesof these two kinds of methods, we propose anovel semi-supervised key phrase extraction ap-proach in this paper, which explores title phrasesas the source of knowledge.It is well agreed that the title has a similar roleto the key phrases.
They are both elaborated toreflect the content of a document.
Therefore,phrases in the titles are often appropriate to bekey phrases.
That is why position has been aquite effective feature in the feature-based keyphrase extraction methods (Witten, 1999), i.e., ifa phrase is located in the title, it is ranked higher.However, one can only include a couple ofmost important phrases in the title prudently dueto the limitation of the title length, even thoughmany other key phrases are all pivotal to the un-derstanding of the document.
For example, whenwe read the title ?China Tightens Grip on theWeb?, we can only have a glimpse of what thedocument says.
On the other hand, the keyphrases, such as ?China?, ?Censorship?, ?Web?,?Domain name?, ?Internet?, and ?CNNIC?, etc.can tell more details about the main topics of thedocument.
In this regard, title phrases are oftengood key phrases but they are far from enough.If we review the above example again, we willfind that the key phrase ?Internet?
can be in-ferred from the title phrase ?Web?.
As a matterof fact, key phrases often have close semantics totitle phrases.
Then a question comes to our minds:can we make use of these title phrases to inferthe other key phrases?To provide a foundation of inference, a seman-tic network that captures the relationships amongphrases is required.
In the previous works (Tur-dakov and Velikhov, 2008), semantic networksare constructed based on the binary relations, andthe semantic relatedness between a pair of phras-es is formulated by the weighted edges that con-nects them.
The deficiency of these approaches isthe incapability to capture the n-ary relationsamong multiple phrases.
For example, a group of296phrases may collectively describe an entity or anevent.In this study, we propose to model a semanticnetwork as a hyper-graph, where verticesrepresent phrases and weighted hyper-edgesmeasure the semantic relatedness of both binaryrelations and n-ary relations among phrases.
Weexplore a universal knowledge base ?
Wikipedia?
to compute the semantic relatedness.
Yet ourmajor contribution is to develop a novel semi-supervised key phrase extraction approach bycomputing the phrase importance in the semanticnetwork, through which the influence of titlephrases is propagated to the other phrases itera-tively.The goal of the semi-supervised learning is todesign a function that is sufficiently smooth withrespect to the intrinsic structure revealed by titlephrases and other phrases.
Based on the assump-tion that semantically related phrases are likelyto have similar scores, the function to be esti-mated is required to assign title phrases a higherscore and meanwhile locally smooth on the con-structed hyper-graph.
Zhou et al?s work (Zhou2005) lays down a foundation for our semi-supervised phrase ranking algorithm introducedin Section 3.
Experimental results presented inSection 4 demonstrate the effectiveness of thisapproach.2 Wikipedia-based Semantic NetworkConstructionWikipedia1 is a free online encyclopedia, whichhas unarguably become the world?s largest col-lection of encyclopedic knowledge.
Articles arethe basic entries in the Wikipedia, with each ar-ticle explaining one Wikipedia term.
Articlescontain links pointing from one article to another.Currently, there are over 3 million articles and 90million links in English Wikipedia.
In addition toproviding a large vocabulary, Wikipedia articlesalso contain a rich body of lexical semantic in-formation expressed via the extensive number oflinks.
During recent years, Wikipedia has beenused as a powerful tool to compute semantic re-latedness between terms in a good few of works(Turdakov 2008).We consider a document composed of thephrases that describe various aspects of entitiesor events with different semantic relationships.We then model a document as a semantic net-work formulated by a weighted hyper-graph1 www.wikipedia.orgG=(V, E, W), where each vertex vi?V (1?i?n)represents a phrase, each hyper-edge ej?E(1?j?m) is a subset of V, representing binary re-lations or n-ary relations among phrases, and theweight w(ej) measures the semantic relatednessof ej.By applying the WSD technique proposed by(Turdakov and Velikhov, 2008), each phrase isassigned with a single Wikipedia article that de-scribes its meaning.
Intuitively, if the fraction ofthe links that the two articles have in common tothe total number of the links in both articles ishigh, the two phrases corresponding to the twoarticles are more semantically related.
Also, anarticle contains different types of links, which arerelevant to the computation of semantic related-ness to different extent.
Hence we adopt theweighted Dice metric proposed by (Turdakov2008) to compute the semantic relatedness ofeach binary relation, resulting in the edge weightw(eij), where eij is an edge connecting the phrasesvi and vj.To define the n-ary relations in the semanticnetwork, a proper graph clustering technique isneeded.
We adopt the weighted Girvan-Newmanalgorithm (Newman 2004) to cluster phrases (in-cluding title phrases) by computing their bet-weenness centrality.
The advantage of this algo-rithm is that it need not specify a pre-definednumber of clusters.
Then the phrases, withineach cluster, are connected by a n-ary relation.
n-ary relations among the phrases in the same clus-ter are then measured based on binary relations.The weight of a hyper-edge e is defined as:( ) ( )| | ij ije ew e w ee???
?
(1)where |e| is the number of the vertices in e, eij isan edge with two vertices included in e and ?
?
0is a parameter balancing the relative importanceof n-ary hyper-edges compared with binary ones.3 Semi-supervised Learning from TitleGiven the document semantic networkrepresented as a phrase hyper-graph, one way tomake better use of the semantic information is torank phrases with a semi-supervised learningstrategy, where the title phrases are regarded aslabeled samples, while the other phrases as unla-beled ones.
That is, the information we have atthe beginning about how to rank phrases is thatthe title phrases are the most important phrases.Initially, the title phrases are assigned with a pos-itive score of 1 indicating its importance and oth-297er phrases are assigned zero.
Then the impor-tance scores of the phrases are learned iterativelyfrom the title phrases through the hyper-graph.The key idea behind hyper-graph based semi-supervised ranking is that the vertices whichusually belong to the same hyper-edges shouldbe assigned with similar scores.
Then, we havethe following two constraints:1.
The phrases which have many incident hy-per-edges in common should be assigned similarscores.2.
The given initial scores of the title phrasesshould be changed as little as possible.Given a weighted hyper-graph G, assume aranking function f over V, which assigns eachvertex v an importance score f(v).
f can bethought as a vector in Euclid space R|V|.
For theconvenience of computation, we use an inci-dence matrix H to represent the hypergraph, de-fined as:0, if ( , ) 1, ifv eh v e v e???
?
??
(2)Based on the incidence matrix, we define thedegrees of the vertex v and the hyper-edge e as(3)and(4)Then, to formulate the above-mentioned con-straints, let  denote the initial score vector, thenthe importance scores of the phrases are learnediteratively by solving the following optimizationproblem:| |2arg min { ( ) }Vf R f f y??
?
?
?
(5)2{ , }1 1 ( ) ( )( ) ( )2 ( ) ( ) ( )e E u v ef u f vf w ee d u d v??
??
??
?
??
??
??
?
(6)where ?> 0 is the parameter specifying thetradeoff between the two competitive items.
LetDv and De denote the diagonal matrices contain-ing the vertex and the hyper-edge degrees re-spectively, W denote the diagonal matrix con-taining the hyper-edge weights, f* denote the so-lution of (6).
Zhou has given the solution (Zhou,2005) as.
* * (1 )f f y?
??
?
?
?
(7)where1/2 1 1/2Tv e vD HWD H D?
?
??
?
and 1/ ( 1)?
??
?
.Using an approximation algorithm (e.g.
Algo-rithm 1), we can finally get a vector frepresenting the approximate phrase scores.Algorithm 1: PhraseRank(V, T, a, b)Input: Title phrase set = {v1,v2,?,vt},the set of otherphrases ={vt+1,vt+2,?,vn}, parameters ?
and ?, con-vergence threshold ?Output: The approximate phrase scores fConstruct a document semantic network for all thephrases {v1,v2,?,vn} using the method described  insection 2.Let1/2 1 1/2Tv e vD HWD H D?
?
?
??
;Initialize the score vector y as 1,1iy i t?
?
?
, and0,jy t j n?
?
?
;Let , k = 0;REPEAT1 (1 )k kf f y??
??
?
?
?
;, ;;UNTILENDFinally we rank phrases in descending order ofthe calculated importance scores and select thosehighest ranked phrases as key phrases.
Accord-ing to the number of all the candidate phrases,we choose an appropriate proportion, i.e.
10%, ofall the phrases as key phrases.4 Evaluation4.1 Experiment Set-upWe first collect all the Wikipedia terms to com-pose of a dictionary.
The word sequences thatoccur in the dictionary are identified as phrases.Here we use a finite-state automaton to accom-plish this task to avoid the imprecision of pre-processing by POS tagging or chunking.
Then,we adopt the WSD technique proposed by (Tur-dakov and Velikhov 2008) to find the corres-ponding Wikipedia article for each phrase.
Asmentioned in Section 2, a document semanticnetwork in the form of a hyper-graph is con-structed, on which Algorithm 1 is applied to rankthe phrases.To evaluate our proposed approach, we select200 pieces of news from well-known Englishmedia.
5 to 10 key phrases are manually labeledin each news document and the average numberof the key phrases is 7.2 per document.
Due tothe abbreviation and synonymy phenomena, weconstruct a thesaurus and convert all manual andautomatic phrases into their canonical formswhen evaluated.
The traditional Recall, Precisionand F1-measure metrics are adopted for evalua-tion.
This section conducts two sets of experi-ment: (1) to examine the influence of two para-meters: ?
and ?, on the key phrase extractionperformance; (2) to compare with other wellknown state-of-art key phrase extraction ap-proaches.2984.2 Parameter tuningThe approach involves two parameters: ?
(?
?0)is a relation factor balancing the influence of n-ary relations and binary relations; ?
(0??
?1) is alearning factor tuning the influence from the titlephrases.
It is hard to find a global optimized so-lution for the combination of these two factors.So we apply a gradient search strategy.
At first,the learning factor is set to ?=0.8.
Different val-ues of ?
ranging from 0 to 3 are examined.
Then,given that ?
is set to the value with the best per-formance, we conduct experiments to find anappropriate value for ?.4.2.1 ?
: Relation FactorFirst, we fix the learning factor ?
as 0.8 random-ly and evaluate the performance by varying ?value from 0 to 3.
When ?=0, it means that theweight of n-ary relations is zero and only binaryrelations are considered.
As we can see fromFigure 1, the performance is improved in mostcases in terms of F1-measure and reaches a peakat ?=1.8.
This justifies the rational to incorpo-rate n-ary relations with binary relations in thedocument semantic network.Figure 1.
F1-measures with ?
in [0 3]4.2.2 ?
: Learning factorNext, we set the relation factor ?=1.8, we in-spect the performance with the learning factor ?ranging from 0 to 1.
?=1 means that the rankingscores learn from the semantic network withoutany consideration of title phrases.
As shown inFigure 2, we find that the performance almostkeep a smooth fluctuation as ?
increases from 0to 0.9, and then a diving when ?=1.
This provesthat title phrases indeed provide valuable infor-mation for learning.Figure 2.
F1-measure with ?
in [0,1]4.3 Comparison with Other ApproachesOur approach aims at inferring important keyphrases from title phrases through a semanticnetwork.
Here we take a method of synonymexpansion as the baseline, called WordNet ex-pansion here.
The WordNet2 expansion approachselects all the synonyms of the title phrases in thedocument as key phrases.
Afterwards, our ap-proach is evaluated against two existing ap-proaches, which rely on the conventional seman-tic network and are able to capture binary rela-tions only.
One approach combines the title in-formation into the Grineva?s community-basedmethod (Grineva et al, 2009), called title-community approach.
The title-community ap-proach uses the Girvan-Newman algorithm tocluster phrases into communities and selectsthose phrases in the communities containing thetitle phrases as key phrases.
We do not limit thenumber of key phrases selected.
The other one isbased on topic-sensitive LexRank (Otterbacher etal., 2005), called title-sensitive PageRank here.The title-sensitive PageRank approach makes useof title phrases to re-weight the transitions be-tween vertices and picks up 10% top-rankedphrases as key phrases.Approach Precision Recall F1Title-sensitive Pa-geRank (d=0.15)34.8% 39.5% 37.0%Title-community 29.8% 56.9% 39.1%Our approach(?=1.8, ?=0.5)39.4% 44.6% 41.8%WordNet expansion(baseline)7.9%  32.9% 12.5%Table 1.
Comparison with other approachesTable 1 summarizes the performance on thetest data.
The results presented in the table showthat our approach exhibits the best performanceamong all the four approaches.
It follows that thekey phrases inferred from a document semanticnetwork are not limited to the synonyms of titlephrases.
As the title-sensitive PageRank ap-2 http://wordnet.princeton.edu299proach totally ignores the n-ary relations, its per-formance is the worst.
Based on binary relations,the title-community approach clusters phrasesinto communities and each community can beconsidered as an n-ary relation.
However, thisapproach lacks of an importance propagationprocess.
Consequently, it has the highest recallvalue but the lowest precision.
In contrast, ourapproach achieves the highest precision, due toits ability to infer many correct key phrases usingimportance propagation among n-ary relations.5 ConclusionThis work is based on the belief that key phrasestend to have close semantics to the title phrases.In order to make better use of phrase relations inkey phrase extraction, we explore the Wikipediaknowledge to model one document as a semanticnetwork in the form of hyper-graph, throughwhich the other phrases learned their importancescores from the title phrases iteratively.
Experi-mental results demonstrate the effectiveness androbustness of our approach.AcknowledgmentsThe work described in this paper was partiallysupported by NSFC programs (No: 60773173,60875042 and 90920011), and Hong Kong RGCProjects (No: PolyU5217/07E).
We thank theanonymous reviewers for their insightful com-ments.ReferencesDavid Milne, Ian H. Witten.
2008.
An Effective,Low-Cost Measure of Semantic RelatednessObtained from Wikipedia Links.
In Wikipediaand AI workshop at the AAAI-08 Conference,Chicago, US.Dengyong Zhou, Jiayuan Huang and BernhardSch?lkopf.
2005.
Beyond Pairwise Classifica-tion and Clustering Using Hypergraphs.
MPITechnical Report, T?bingen, Germany.Denis Turdakov and Pavel Velikhov.
2008.
Semanticrelatedness metric for wikipedia conceptsbased on link analysis and its application toword sense disambiguation.
In Colloquium onDatabases and Information Systems (SYRCoDIS).Ian H. Witten, Gordon W. Paynter, Eibe Frank , CarlGutwin , Craig G. Nevill-Manning.
1999.
KEA:practical automatic keyphrase extraction, InProceedings of the fourth ACM conference on Dig-ital libraries, pp.254-255, California, USA.Jahna Otterbacher, Gunes Erkan and Dragomir R.Radev.
2005.
Using Random Walks for Ques-tion-focused Sentence Retrieval.
In Proceedingsof HLT/EMNLP 2005, pp.
915-922, Vancouver,Canada.Maria Grineva, Maxim Grinev and Dmitry Lizorkin.2009.
Extracting key terms from noisy andmultitheme documents, In Proceedings of the18th international conference on World wide web,pp.
661-670, Madrid, Spain.Michael Strube and Simone Paolo Ponzetto.2006.WikiRelate!
Computing Semantic Rela-tedness using Wikipedia.
In Proceedings of the21st National Conference on Artificial Intelligence,pp.
1419?1424, Boston, MA.M.
E. J. Newman.
2004.
Analysis of Weighted Net-works.
Physical Review E 70, 056131.300
