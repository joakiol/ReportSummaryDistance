Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 511?519,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsDistributional Lexical Entailment by Topic CoherenceLaura RimellUniversity of CambridgeComputer Laboratorylaura.rimell@cl.cam.ac.ukAbstractAutomatic detection of lexical entailment,or hypernym detection, is an importantNLP task.
Recent hypernym detectionmeasures have been based on the Distri-butional Inclusion Hypothesis (DIH).
Thispaper assumes that the DIH sometimesfails, and investigates other ways of quan-tifying the relationship between the co-occurrence contexts of two terms.
We con-sider the top features in a context vectoras a topic, and introduce a new entailmentdetection measure based on Topic Coher-ence (TC).
Our measure successfully de-tects hypernyms, and a TC-based familyof measures contributes to multi-way rela-tion classification.1 IntroductionAutomatically detecting lexical entailment ?
forexample, that lion entails animal or guitar entailsinstrument, also known as hypernym detection ?is an important linguistic task in its own right, andis also a prerequisite for recognizing entailmentsbetween longer text segments such as phrases orsentences (Bos and Markert, 2005; Garrette et al.,2011; Baroni et al., 2012; Beltagy et al., 2013).Several recent techniques for hypernym de-tection have made use of distributional seman-tics (Weeds and Weir, 2003; Weeds et al., 2004;Clarke, 2009; Kotlerman et al., 2010; Lenci andBenotto, 2012).
These techniques are based on theDistributional Inclusion Hypothesis (Geffet andDagan, 2005), hereafter DIH, which proposes thatif term A entails term B (B is a hypernym of A),then the contexts in which A occurs are a subset ofthose in which B occurs.
For example, all the con-texts (co-occurrences) of lion ?
which might in-clude zoo, hunt, wild, food, etc.
?
are also contextsof animal.
Existing measures look at the amountof overlap between the co-occurrences of A and B,in order to judge whether B is a hypernym of A.The motivation for the present paper is the well-known fact that the DIH is not fully correct.
Thereare many reasons why a hyponym might occurin contexts where its hypernym does not.
Somecontexts are collocational, e.g.
lion king.
Othercontexts are highly specific, e.g.
mane appliesuniquely to lions, horses, and zebras; it would beunusual to see text about animals with manes.
Theneed to be informative is also relevant: lion cubwill occur much more frequently than animal cub,since animal is of the wrong level of generality topair with cub.Moreover, the more general a hypernym be-comes ?
up to the level of WordNet root elements,such as entity ?
its predominant sense ceases tocorrespond to the sense intended in hyponym-hypernym chains.
Thus we never hear about goingto visit an entity at the zoo.This paper starts from the assumption that theDIH sometimes fails, and investigates not theamount of containment of A?s features in B?s fea-tures, but rather the nature of the non-containedfeatures.
We consider the top features of a dis-tributional vector as a topic, and use recent mea-sures for automatically measuring Topic Coher-ence (Newman et al., 2010; Mimno et al., 2011)to evaluate how the topics change under variousconditions.
Using a notion of vector negation, weinvestigate whether the distributional topic of e.g.lion becomes more or less coherent when we sub-tract the contexts of animal.We introduce a new measure, Ratio of Changein Topic Coherence (RCTC), for detecting lexicalentailment.
The measure detects hypernyms withreasonable accuracy, and a family of Topic Coher-ence measures is used to perform amulti-way clas-sification of tuples by relation class.
Finally, weinvestigate how the level of generality of a hyper-nym affects entailment measures.5112 Related WorkHistorically, manually developed resources suchas WordNet (Miller, 1995) have been used to sup-ply lexical entailment information to NLP appli-cations (Bos and Markert, 2005).
More recently,a number of techniques for detecting lexical en-tailment have been developed using distributionalsemantics (Weeds and Weir, 2003; Weeds et al.,2004; Geffet and Dagan, 2005; Clarke, 2009;Kotlerman et al., 2010; Lenci and Benotto, 2012).These measures quantify to what extent the co-occurrence features of a term A are included inthose of another term B, by a direct comparisonof the distributional vectors~A and~B.
Kotlermanet al.
(2010) use the notion of Average Precisionfrom Information Retrieval to weight the relativeimportance of the overlapping features.
Lenci andBenotto (2012) also check the extent to which B?sfeatures are not a subset of A?s, as a proxy for themore general character of B.
The success of thesefeature inclusion measures has provided generalsupport for the DIH.
Following Szpektor and Da-gan (2008), inclusion measures are also sometimesbalanced with similarity measures such as LINsimilarity (Lin, 1998), to ensure that A and B aresemantically related, since unrelated pairs that dif-fer in frequency can mimic feature inclusion.Previous distributional approaches to hypernymdetection have generally involved a single mea-sure, designed to rank hypernyms above other re-lation classes.
Evaluation has largely involvedeither ranking or binary classification tasks, andthere has been little work on using a variety ofmeasures to distinguish multiple relation classes.Lenci and Benotto (2012) perform a ranking taskusing the multi-class BLESS dataset (Baroni andLenci, 2011), but not a classification.
We performa multi-way classification using a variety of TopicCoherence measures.
Recent Semantic RelationClassification shared tasks (SemEval-2010 Task 8,SemEval-2012 Task 2) are also relevant, thoughthe relation classes and approaches have differed.3 Topic Coherence for DistributionalLexical EntailmentThe intuition behind our approach is to investigatewhether term A, the candidate hyponym, has a co-herent topic reflected in its distributional features,which apply only to A and not to its hypernym B.Consider A=beer, B=beverage.
They may sharefeatures such as drink, cold, and party.
But if weminimize or exclude B?s features and examine theremaining features of A (we discuss how to do thisin Section 3.3), we might be left with more specificfeatures such as pint, lager, and brew.If A and B share almost all contexts, we wouldbe left with a set of uninformative features, merelycorpus noise.
If A and B share few contexts, therewould be little change to A?s topic when excludingB?s features.
Between the extremes, a range ofchange in A?s topic is possible; we seek to quantifythis change and relate it to entailment.To do this we need a way of treating a distri-butional context vector as a topic.
We treat theN highest-weighted context features in~A as thetopic of A (topicA).
If we represent the vector~A ?
{fci,A}i, where fci,Ais the weighted co-occurrence value of context feature ci, then topicAis a set {cj}, j ?
1...N , of the N highest-weightedcontext features cjin~A.3.1 HypothesesWe consider two opposing hypotheses.Hypothesis 1: Removing hypernym B?s fea-tures from topicA will decrease the coherence oftopicA.
If being a B is very important to being anA, then the collection of remaining features maybecome more random.
Hypothesis 1 is consistentwith the DIH, since it implies that the importantfeatures of A are also features of B.As a corollary, removing A?s features from Bmay not change the coherence of topicB verymuch.
Since A is just an instance of B, topicBretains coherence (i.e.
there?s a lot to being an an-imal besides what?s involved in being a lion).Hypothesis 2: Removing hypernym B?s fea-tures from topicA will increase the coherence oftopicA.
Perhaps A, by virtue of being more spe-cific, occurs in a highly coherent set of contextswhere B does not.
Hypothesis 2 is inconsistentwith the DIH, since it imples that a hyponym al-ways has specific features which the hypernymdoes not share.As a corollary, removing hyponym A?s featuresfrom hypernym B might decrease the coherence oftopicB, if removing specific features leaves onlymore general, less informative features behind.3.2 Topic Coherence MeasureWe use a Topic Coherence (TC) measure from re-cent work on automatic evalution of topics gener-ated from corpora by latent variable models (New-man et al., 2010; Mimno et al., 2011; Stevens et512al., 2012).
TC measures are applied to the top Nwords from a generated topic.
They assign pair-wise relatedness scores to the words, and returnthe mean or median from the word-pair scores.We adopt the best method from Newman et al.
(2010), equal to the median pairwise PointwiseMutual Information (PMI) of the top N words, us-ing Wikipedia as a background corpus for PMI.1The measure is given in Equation (1):TC({cj}) = median(PMI(ci, ck), i, k ?
1...N, i < k)(1)where {cj} is the topic, and PMI is defined as:PMI(ci, ck) = logp(ci, ck)p(ci)p(ck)(2)We use intra-sentence co-occurrence in Wikipediafor calculating PMI.Note that our definition of a topic, namely thetop N features from a distributional vector, doesnot correspond to a topic generated by a latentvariable model, because it does not have a prob-ability distribution over words.
However, the TCmeasures we adopt do not make use of such aprobability distribution except for choosing the topN words from a topic, which are then treated as anunordered set for the pairwise operations.
New-man et al.
(2010) uses N=10, and Mimno et al.
(2011) uses N=5...20; we investigate a range of N.3.3 Vector NegationFor removing one topic from another, we draw onthe concept of vector negation (Widdows and Pe-ters, 2003; Widdows, 2003).
Vector negation hasproved useful for modeling word senses in Infor-mation Retrieval.
For example, one might want toformulate a query for suitNOT lawsuit, which willretrieve terms such as shirt and jacket and excludeplaintiff and damages.We test two versions of vector negation.
Thefirst, Widdows (Widdows, 2003), represents ANOT B as the projection of~A onto~B?, the sub-space orthogonal to~B in the vector space V .Specifically,~B??
{v ?
V : v ?~B = 0}.
Theformula for Widdows A NOT B is:A NOT B ?~A?~A ?~B|~B|2~B (3)The second, Strict negation, simply zeros outany context features of A that are non-zero in B:fci,AnotB?
(0 if fci,B6= 0fi,Aif fci,B= 0(4)1In our case, Wikipedia is also the source corpus for ourcontext vectors.This measure is harsher than Widdows negation,which decreases the value of common features butdoes not remove them completely.3.4 Generality MeasureHerbelot and Ganesalingam (2013) experimentwith hypernym detection using a generality mea-sure.
They measure the Kullback-Leibler (KL) di-vergence (Eq.
5) between the probability distribu-tion over context words for a term A, and the back-ground probability distribution.
The idea is thatthe greater the KL divergence, the more informa-tive and therefore specific the term is, while hyper-nyms are likely to be more general.DKL(p(fi|A)||p(fi)) = ?iln(p(fi|A)p(fi))p(fi) (5)Herbelot and Ganesalingam (2013) found thatKL divergence on its own was not sufficient forsuccessful hypernym detection.
We experimentwith it in combination with TC measures.4 Methods4.1 Context VectorsWe produced context vectors from a 2010Wikipedia download, lemmatized using morpha(Minnen et al., 2001).
The 10Kmost frequent lem-mas in the corpus, minus common stop words andthe 25 most frequent lemmas, served as the contextfeatures.
Feature co-occurrences were counted ina 7-word window around the target lemma (threewords each side of the target lemma), and limitedto intra-sentence co-occurrences.Co-occurrence counts were weighted using T-test.
We chose T-test because it does not over-emphasize infrequent features; however, early ex-periments with Positive PMI weighting showedthe overall performance of our measures to be sim-ilar with both weighting schemes.We benchmarked our context vectors on theWS353 word similarity task (Finkelstein et al.,2002) and found them to be of comparable accu-racy with previous literature.Rel Class Target Related Word TotalHYPER alligator animal 638COORD alligator lizard 1,760MERO alligator mouth 1,402RAND-N alligator message 3,253Table 1: Examples from the BLESS subset; num-ber of tuples per relation in the development set.513Macroaverage MicroaverageRelation Class Relation ClassCoherence of HYPER MERO COORD RAND-N HYPER MERO COORD RAND-NTopicA 5.14 ?1.63 5.16 ?1.66 5.13 ?1.63 5.16 ?1.66 5.14 ?1.59 5.37 ?1.56 5.22 ?1.63 5.28 ?1.62TopicAnotB 3.82 ?1.27 3.86 ?1.02 3.49 ?0.94 5.07 ?1.50 3.88 ?1.73 4.07 ?1.42 3.58 ?1.51 5.17 ?1.64TopicA-TopicAnotB 1.32 ?1.54 1.30 ?1.28 1.64 ?1.58 0.09 ?0.43 1.26 ?1.86 1.30 ?1.49 1.64 ?1.92 0.11 ?0.83TopicB 4.97 ?0.58 4.51 ?0.52 5.02 ?0.73 4.49 ?0.24 5.01 ?1.15 4.53 ?1.44 5.07 ?1.63 4.50 ?1.30TopicBnotA 4.36 ?0.55 3.92 ?0.53 3.33 ?0.67 4.45 ?0.27 4.37 ?1.15 3.89 ?1.32 3.35 ?1.61 4.46 ?1.41TopicB-TopicBnotA 0.61 ?0.69 0.59 ?0.48 1.68 ?0.88 0.04 ?0.14 0.64 ?1.34 0.64 ?1.33 1.72 ?2.07 0.04 ?0.77Table 2: Average Topic Coherence measures on the development set, using N=10, Strict negation.4.2 Evaluation DatasetWe used a subset of the BLESS dataset (Baroniand Lenci, 2011) as defined by Lenci and Benotto(2012).
The entire dataset consists of 200 con-crete nouns in 17 broad noun classes (e.g.
cloth-ing, amphibian/reptile, vegetable, container), par-ticipating in a variety of relations.
The subset con-tains the relation classes hypernym (HYPER), co-ordinate (COORD, i.e.
co-hyponym), meronym(MERO, i.e.
part-of), and random-noun (RAND-N, an unrelated noun).
It consists of 14,547 tuplesin total.
Table 1 gives an example of each rela-tion class, along with the total number of tuplesper class in the development data.Since there was no pre-defined development-test split for the BLESS subset, we randomly se-lected half of the data for development.
For eachof the 17 broad noun classes, we randomly chosehalf of the target nouns, and included all their HY-PER, COORD, MERO, and RAND-N tuples.
Thisresulted in a development set consisting of 96 tar-get nouns and 7,053 tuples; and a test set consist-ing of 104 nouns and 7,494 tuples.5 Topic Coherence BehaviorWe first investigate how topic coherence behavesacross the four relation classes.
Table 2 showsthe average values and standard deviation of TC-related measures on the development data.
Theleft-hand side gives macro-averages, where valuesare first averaged per-class for each target word,then averaged across the 96 target words in the de-velopment set.
The right-hand side gives micro-averages across all tuples in the development set.The micro- and macro-averages are similar, andwe report macro-averages from now on.2Row 1 of Table 2 shows the original coherenceof topicA, and row 2 the coherence of topicAnotB.2Lenci and Benotto (2012) also report macro-averages,but our figures are not comparable to theirs, which are basedon a nearest-neighbor analysis.Row 3 is simply the difference between the two,showing the absolute change in coherence.
Rows4-6 are analogous.
In general, coherence valuesfor A and B ranged from the 3?s to the 6?s, withvery high coherence of 7 or 8 and very low coher-ence of 1 or 2.
We did not normalize TC values.Comparing rows 1 and 4, we see that the B top-ics are slightly less coherent than the A topics,probably due to the makeup of the dataset (B termsinclude hypernyms and random words, while Aterms are concrete nouns).Column 1 shows that removing hypernym Bfrom A results in a decrease in coherence, from5.14 to 3.82.
The difference in coherence, 1.32in this case, is shown in row 3.
Removing Afrom B also results in a coherence decrease, buta much smaller one: only a 0.61 average absolutedecrease.
Because the starting coherence values ofA and B may be different, we focus on the amountof change in coherence when we perform the nega-tion (rows 3 and 6), rather than the absolute coher-ence of the negated vectors (rows 2 and 5).Interestingly, column 2 shows that the be-haviour of meronyms is almost identical to hyper-nyms.
This is surprising for two reasons: first,meronyms are intuitively more specific than theirholonyms; and second, previous studies tended toconflate hypernyms with coordinates rather thanmeronyms (Lenci and Benotto, 2012).Column 3, rows 3 and 6, show that coor-dinates behave differently from hypernyms andmeronyms.
Vector negation in both directionsresults in a similar loss of coherence (1.64 and1.68), reflecting the fact that coordinates have asymmetrical relationship.
The average change isalso greater, although there is a wide variance.
Incolumn 4, the coherence differences for randomnouns are again symmetrical, but in this case verysmall, since a randomly selected noun will notshare many contexts with the target word.We can also define a TC-based similarity mea-514Relation ClassMeasure HYPER MERO COORD RAND-NTC Meet 5.36 5.12 5.98 3.62LIN 0.41 0.41 0.48 0.22GenKLA 4.89 4.89 4.89 4.89GenKLB 4.60 4.49 5.01 4.95DiffGenKL 0.29 0.40 -0.12 -0.05Table 3: Average similarity and generality mea-sures on the dev.
set, using N=10, Strict negation.sure.
We define~A MEET~B as the intersec-tion of two vectors, where each feature valuefci,A MEET B?
min(fci,A, fci,B).
Table 3 showsTC(A MEET B), with LIN similarity (Lin, 1998)between A and B for comparison.
We expect thatif A and B are similar, their common featureswill form a coherent topic.
Indeed hypernymsand meronyms have high values, with coordinatesslightly higher and random nouns much lower.Table 3 also shows the KL divergence-basedgenerality measure from Section 3.4.
Term B isslightly more general (lower score) than termA forhypernyms and meronyms.
This may suggest thatmeronyms are more general distributionally thantheir holonyms, e.g.
leg is a holonym of alligator,but also associated with many other animals.Table 4 shows the topics for owl and its hyper-nym creature.
Using Strict negation to create owlNOT creature causes a number of contexts to beremoved from owl: sized, owl, burrow, hawk, typ-ical, medium, eagle, large, nest.
Instead, moreidiosyncratic contexts rise to the top, includingnorthern, mexican, grouping, and bar (as in anowl?s markings).
These idiosyncratic contexts arenot mutually informative and cause a sizeable de-crease in TC.On the other hand, removing owl from crea-ture does not decrease the coherence nearly asmuch.
The contexts that are promoted ?
fantas-tic, bizarre, fairy ?
are mutually consistent withthe other creature contexts.So far our results support Hypothesis 1: remov-ing B from A decreases its coherence.
However,we hypothesize that this may not be the case forhypernyms at all levels of generality.
Consider-ing the pair owl-chordate, there is no change fromtopicA to topicAnotB.
But chordate loses a size-able amount of coherence when owl is removed;the topic changes from primitive, ancestral, ances-tor, evolution, lineage, basal, earliest, fossil, non-,neural (TC 6.62), to earliest, non-, neural, affinity,probable, genome, suspected, universally, group,approximation (TC 3.60).6 Hypernym Detection MeasuresSince we use the same dataset as Lenci andBenotto (2012), we report the invCL measure in-troduced in that paper, which outperformed theother measures reported there, including those ofWeeds and Weir (2003), Weeds et al.
(2004), andClarke (2009).
Let fAbe the weight of feature f in~A, and let FAbe the set of features with non-zeroweights in~A.
Then we have:CL(A,B) =?f?FA?Fbmin(fA, fB)?f?FAfA(6)invCL(A,B) =pCL(A,B) ?
(1?
CL(B,A)) (7)We also report the balAPinc measure of Kotler-man et al.
(2010), which is not included in theLenci and Benotto (2012) evaluation.
This mea-sure begins with APinc, in which the features of Aare ranked by weight, highest to lowest:APinc(A,B) =?r?1...|FA|P (r) ?
rel(fr)|FA|(8)where P (r) is the ?precision?
at rank r, that is,how many of B?s features are included at rank rin the features of A; and rel(fr) is a relevancefeature reflecting how important fris in B (seeKotlerman et al.
(2010) for details).
The balancedversion balAPinc is:balAPinc(A,B) =pLIN(A,B) ?APinc(A,B) (9)owl (5.19) owl not creature (3.25) creature (5.91) creature not owl (5.09) owl meet creature (4.14)barn barn mythical mythical smallsized grey -like supernatural largeowl northern strange alien burrowburrow mexican supernatural legendary nighthawk falcon magical fantastic elftypical creek alien bizarre littlemedium mountains evil aquatic gianteagle grouping legendary dangerous preylarge bar giant vicious huntnest california resemble fairy purpleTable 4: Topics from the development data with Topic Coherence values.515WiddowsN = 5 10 15 20HYPER 1.00 1.00 1.00 1.00MERO 0.99 1.00 1.00 1.00COORD 1.02 1.00 1.00 1.01RAND-N 1.00 1.00 1.00 1.00StrictN = 5 10 15 20HYPER 1.64 1.42 1.23 1.19MERO 1.91 1.23 1.24 1.20COORD 1.36 1.15 1.10 1.16RAND-N 1.08 1.03 1.03 1.02Table 5: RCTC with varying N and neg type.We introduce a new measure, Ratio of Changein Topic Coherence (RCTC).
Based on Section 5,we expect that for hypernyms the change in coher-ence from A to AnotB is greater than the changefrom B to BnotA.
However, we cannot simply usethe ratio (A-AnotB)/(B-BnotA), because the verysmall changes in the RAND-N class result in verysmall denominators and unstable values.
Instead,we consider two ratios: the magnitude of TC(A)compared to TC(AnotB), and the magnitude ofTC(B) compared to TC(BnotA).
We take the ra-tio of these figures:RCTC(A,B) =TC(topicA)TC(topicAnotB)TC(topicB)TC(topicBnotA)(10)If topicA is much more coherent than AnotB,the numerator will be relatively large.
If topicBis not much more coherent than topicBnotA, thedenominator will be relatively small.
Both of thesefactors encourage RCTC to be larger.3We also balanced RCTC with three differentfactors: LIN similarity, a generality ratio, andTC(MeetAB).
In each case we calculated the bal-anced value as?RCTC ?
factor.7 Experiments and DiscussionWe first look at the effect of N (topic size) andnegation type on RCTC on the development data(Table 5).
It is clear that RCTC distinguishes rela-tion types using Strict but not Widdows negation.We believe this is because, as the ?harsher?
ver-sion of negation, it allows less-related features torise to the top of the topic and reveal greater dif-ferences in topic coherence.
N=10 was the only3Although TC values are PMI values, which can be neg-ative, in practice the median pairwise PMI is almost nevernegative, because there tend to be more positive than nega-tive values among the pairwise comparisons.
Therefore, wehave not accounted for sign in the ratio.
We have handledas special cases the few instances where TC(topicAnotB) orTC(topicBnotA) takes the value of ?infinity due to zero co-occurrences between many of the features.invCL bal RCTC RCTC RCTC RCTCAPinc bal bal balLIN GEN MEETHYPER 0.41 0.23 1.37 0.72 1.09 2.62MERO 0.39 0.22 1.28 0.70 1.06 2.51COORD 0.38 0.22 1.44 0.71 1.05 2.50RAND-N 0.25 0.10 1.03 0.46 1.01 1.92Table 6: Hypernym identification on full dataset:average value by relation.value that ranked hypernyms the highest; we useN=10 for the remaining experiments.We then proceed to hypernym identification onthe full dataset (Table 6).
All measures we testedassigned the highest average value to hypernyms(in bold) compared to the other relations.7.1 Ranking TaskLenci and Benotto (2012) introduced a rankingtask for hypernym detection on the BLESS data,which we replicate here.
In this task a measure isused to rank all tuples from the data.
The accuracyof the ranking is assessed from the point of viewof each relation class.
The goal is for hypernymsto have the highest accuracy of all the classes.We report the Information Retrieval (IR) mea-sure Mean Average Precision (MAP) for eachclass, following Lenci and Benotto (2012).
Wealso report Mean R-Precision (RPrec), equal to theprecision at rank R where R is the number of ele-ments in the class.
None of the measures we eval-uated achieves the highest result for hypernyms4,though invCL consistently performs better for hy-pernyms than do the other measures (Table 7).Both MAP and RPrec give more weight to cor-rect rankings near the top of the list, as is suit-able for IR applications.
In the context of hyper-nym detection, they could test a system?s ability tofind one or two good-quality hypernyms quicklyfrom a set of candidates.
However, these measuresare less appropriate for testing whether a systemcan, in general, rank hypernyms over other rela-tions.
Therefore, we also report Mean Area Un-der the ROC Curve, or Wilcoxon-Mann-Whitneystatistic (AUC), which gives equal weight to cor-rect rankings at the top and bottom of the list, andalso compensates for unbalanced data.
Table 7shows that RCTCbalMEET performs identicallyto invCL on the AUC measure.
This comparisonsuggests that invCL is better at placing hypernyms4Lenci and Benotto (2012) report a different result, possi-bly due to the use of different context vectors.516invCL balAPinc RCTC RCTC RCTC RCTCbalLIN balGEN balMEETRPrecHyper 0.30 0.25 0.17 0.20 0.12 0.19Mero 0.32 0.29 0.30 0.31 0.21 0.32Coord 0.39 0.43 0.27 0.42 0.27 0.40Rand-N 0.18 0.19 0.38 0.16 0.42 0.18AUCHyper 0.18 0.17 0.16 0.17 0.14 0.18Mero 0.31 0.31 0.27 0.31 0.24 0.31Coord 0.38 0.39 0.25 0.39 0.28 0.37Rand-N 0.13 0.13 0.32 0.12 0.34 0.15MAPHyper 0.35 0.30 0.22 0.24 0.17 0.24Mero 0.37 0.35 0.35 0.36 0.27 0.37Coord 0.41 0.46 0.30 0.45 0.32 0.43Rand-N 0.32 0.32 0.43 0.31 0.46 0.33Table 7: Ranking results.
Bold indicates best result for hypernyms by evaluation measure.at the top of the ranking, but over the whole datasetthe two measures rank hypernyms above other tu-ples equally.7.2 Classification TaskWe performed a four-way classification of tuplesby relation class.
We used LIBSVM (Chang andLin, 2011).
As described in Section 4.2, theBLESS data is unbalanced, with hypernyms ?
ourtarget class ?
making up only about 9% of thedata.
To address this imbalance, we used LIB-SVM?s option to increase the cost associated withthe smaller classes during parameter tuning andtraining.
We based the weights on the develop-ment data only (HYPER: 9% of the data, weightfactor 10; MERO: 20% of the data, weight factor5; COORD: 25% of the data, weight factor 4).We used LIBSVM?s default Radial Basis Func-tion kernel.
On the development data we per-formed 10-fold cross-validation.
We used LIB-SVM?s grid.py utility for tuning the parameters Cand ?
separately for each fold.
We also tuned andtrained models on the development data and testedthem on the test data.We used four sets of features (Table 8): (1)invCL on its own; (2) TC features; (3) all features(invCL, TC, plus additional similarity and gener-ality measures); and (4) all except TC features.The results of classification on the developmentdata are shown in Table 9, and on the test data inTable 10.
Although we report overall accuracy,this is a poor measure of classificaton quality forunbalanced data.
The tables therefore provide thePrecision, Recall, and F-score by relation class.The overall accuracy is respectable, althoughit can be seen that the hypernym class was themost difficult to predict, despite weighting the costfunction.
Hypernyms may be particularly difficultFeature DescriptioninvCL Lenci?s invCL(A,B) (Eq.
7)topicA TC(A)topicAnotB TC(B)diffTopicA TC(A)?
TC(A NOT B)ratioTopicsA TC(A NOT B)/TC(A)topicB TC(B)topicBnotA TC(B NOT A)diffTopicB TC(B)?
TC(B NOT A)ratioTopicsB TC(B NOT A)/TC(B)topicMeetAB TC(A MEET B)ratioTopics1 TC(A NOT B)/TC(B NOT A)ratioTopics2 diffTopicA / diffTopicBDiffTopics1 diffTopicA - diffTopicBDiffTopics2 diffTopicA + diffTopicBRCTC RCTC(A,B) (Eq.
10)RCTCbalMEET RCTCbalMEET(A,B)APinc Kotlerman?s APinc(A,B) (Eq.
8)balAPinc Kotlerman?s balAPinc(A,B) (Eq.
9)LIN LIN similaritygenKLA DKL(p(fi|A)||p(fi)) (Eq.
5)genKLB DKL(p(fi|B)||p(fi)) (Eq.
5)diffGenKL genKLA - genKLBratioGenKL genKLA / genKLBRCTCbalLIN RCTCbalLIN(A,B)RCTCbalGEN RCTCbalGEN(A,B)RCTCbalInvCL RCTC(A,B) bal.
with invCL(A,B)Table 8: Features used in classification experi-ment.
InvCL; TC features; additional features.to isolate given their similarity to meronyms andintermediate status between coordinates and ran-dom nouns on some of the features.Importantly, while previous work has focusedon single measures such as invCL, the classifica-tion task highlights a key aspect of the TC ap-proach.
Because we can measure the TC of sev-eral different vectors for any given tuple (originalterms, negated topics, intersection, etc.)
we canperform multi-way classification much more accu-rately than with the invCL measure alone.
More-over, the TC features make an important contribu-tion to the multi-way classification over and aboveinvCL and other previous similarity and generality517Feature Set Acc Class P R FinvCL 39.2Hyper 29.2 19.6 22.5Mero 25.5 51.7 34.0Coord 19.3 26.4 21.3Rand-N 73.5 44.9 55.6TC Feats 56.7Hyper 20.3 41.4 27.1Mero 36.5 48.4 41.4Coord 66.5 54.5 59.5Rand-N 87.1 64.7 74.2All except TC 59.2Hyper 28.7 19.7 22.9Mero 35.1 56.2 43.2Coord 58.2 54.5 56.2Rand-N 85.5 71.0 77.5All 64.0Hyper 30.5 24.4 26.7Mero 44.9 44.6 44.6Coord 60.3 65.6 62.8Rand-N 80.0 79.6 79.7Table 9: Classification results on developmentdata using 10-fold cross-validation.Feature Set Acc Class P R FinvCL 42.2Hyper 31.1 19.3 23.8Mero 32.6 54.3 40.7Coord 23.1 29.3 25.8Rand-N 75.8 48.2 59.0TC Feats 56.2Hyper 20.0 45.1 27.7Mero 36.7 42.9 40.0Coord 64.2 56.5 60.1Rand-N 88.6 64.5 74.6All except TC 60.6Hyper 23.9 17.9 20.5Mero 38.1 56.4 45.5Coord 58.2 56.1 57.1Rand-N 86.5 73.8 79.6All 63.1Hyper 33.9 28.6 31.0Mero 44.1 36.9 40.2Coord 57.2 64.3 60.6Rand-N 78.2 81.5 79.8Table 10: Classification results on test data usingdevelopment data as training.measures, with the set of all features yielding thehighest overall accuracy.Another interesting result is that classificationwith the TC features alone results in much higherrecall (though lower precision) for hypernymsthan any of the other feature sets, and on the de-velopment data (Table 9) results in the highest F-score for hypernyms.8 Hypernym DepthWe performed a simple preliminary experiment totest the speculation that the interaction betweentopics depends on the level of generality of the hy-pernym.
Using the WordNet::Similarity package(Pedersen et al., 2004), we divided the develop-ment data into bins according to the depth of thehypernym from the WordNet root node.
Table 11shows average values by hypernym depth.D Qty diffA diffB RCTC invCL balAPinc1 1 0.66 0.27 1.08 0.15 0.013 35 0.33 0.16 1.12 0.44 0.235 108 0.32 -0.65 1.32 0.33 0.166 41 1.21 0.24 1.50 0.44 0.217 160 1.45 0.64 1.34 0.44 0.278 136 1.30 0.90 1.25 0.35 0.199 71 1.37 1.09 1.26 0.41 0.2310 51 1.90 2.10 2.08 0.41 0.2411 15 1.85 1.50 1.23 0.48 0.3112 13 2.08 1.45 1.24 0.28 0.1713 3 2.49 0.97 1.67 0.27 0.1214 4 2.02 1.97 1.05 0.27 0.09Table 11: Average value by depth D of hypernym.There is a striking result for diffA, i.e.TC(topicA) - TC(topicAnotB): the deeper the hy-pernym in the WordNet hierarchy, the greater thevalue.
This suggests that more abstract hypernymshave less interaction with their hyponyms?
topics.A similar, though less pronounced, effect is seenfor diffB.
However, the three measures RCTC,invCL, and balAPinc remain relatively stable asthe hypernym depth changes.
While this is some-what reassuring, these averages clearly have notyet captured the difficulty which the DIH encoun-ters in individual cases such as owl?chordate.9 ConclusionsWe have introduced a set of Topic Coherence mea-sures, particularly the Ratio of Change in TopicCoherence, to identify hypernyms.
These mea-sures perform comparably to previous hypernymdetection measures on many tasks, while provid-ing a different view of the relationship between thedistributional vectors of two terms, and contribut-ing to a more accurate multi-way relation classifi-cation, especially higher recall for hypernyms.The approach presented here provides a start-ing point for entailment measures that do not relysolely on the Distributional Inclusion Hypothesis.One issue with the current proposal is that it testsfor a single coherent distributional topic, whereasmultiple senses may be represented in a word?s topcontext features.
Future work will integrate WordSense Disambiguation methods into the Topic Co-herence based lexical entailment approach.AcknowledgmentsThis work is supported by EPSRC grantEP/I037512/1.
We gratefully acknowledgehelpful discussion from Stephen Clark, TamaraPolajnar, Julie Weeds, Jeremy Reffin, David Weir,and the anonymous reviewers.518ReferencesMarco Baroni and Alessandro Lenci.
2011.
Howwe BLESSed distributional semantic evaluation.
InProceedings of the EMNLP workshop on GEMS:GEometrical Models of natural language Semantics,pages 1?10, Edinburgh.Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,and Chung chieh Shan.
2012.
Entailment above theword level in distributional semantics.
In Proceed-ings of EACL, pages 23?32.Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-rette, Katrin Erk, and Raymond Mooney.
2013.Montague meets markov: Deep semantics withprobabilistic logical form.
In Proceedings of *SEM,pages 11?21, Atlanta, Georgia.Johan Bos and Katja Markert.
2005.
Recognising tex-tual entailment with logical inference.
In Proceed-ings of HLT-EMNLP, pages 628?635, Vancouver.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
ACMTransactions on Intelligent Systems and Technol-ogy, 2:27:1?27:27.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Daoud Clarke.
2009.
Context-theoretic semantics fornatural language: an overview.
In Proceedings ofthe EACL workshop on GEMS: GEometrical Mod-els of natural language Semantics, pages 112?119,Athens.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2002.
Placing search in context: Theconcept revisited.
ACM Transactions on Informa-tion Systems, 20:116?131.Dan Garrette, Katrin Erk, and Raymond Mooney.2011.
Integrating logical representations with prob-abilistic information using Markov Logic.
In Pro-ceedings of IWCS, Oxford, UK.M.
Geffet and I. Dagan.
2005.
The distributional in-clusion hypotheses and lexical entailment.
In Pro-ceedings of ACL, Michigan.Aur?elie Herbelot and Mohan Ganesalingam.
2013.Measuring semantic content in distributional vec-tors.
In Proceedings of ACL.Lili Kotlerman, Ido Dagan, Idan Szpektor, and MaayanZhitomirsky-Geffet.
2010.
Directional distribu-tional similarity for lexical inference.
Natural Lan-guage Engineering, 16:359?389.Alessandro Lenci and Giuli Benotto.
2012.
Identify-ing hypernyms in distributional semantic spaces.
InProceedings of *SEM, pages 75?79, Montreal.Dekang Lin.
1998.
An information-theoretic defini-tion of similarity.
In Proceedings of ICML, Madi-son, Wisconson.George A. Miller.
1995.
WordNet: A lexicaldatabase for English.
Communications of the ACM,38(11):39?41.David Mimno, Hanna M. Wallach, Edmund Talley,Miriam Leenders, and Andrew McCallum.
2011.Optimizing semantic coherence in topic models.
InProceedings of EMNLP, pages 262?272, Edinburgh.Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of English.
Nat-ural Language Engineering, 7(3):207?223.David Newman, Jey Han Lau, Karl Grieser, and Tim-othy Baldwin.
2010.
Automatic evaluation of topiccoherence.
In Proceedings of NAACL, pages 100?108, Los Angeles, California.Ted Pedersen, Siddarth Patwardhan, and Jason Miche-lizzi.
2004.
WordNet::Similarity - measuring therelatedness of concepts.
In Proceedings of NAACL(Demonstration System), pages 38?41, Boston, MA.Keith Stevens, Philip Kegelmeyer, David Andrzejew-ski, and David Butler.
2012.
Exploring topic coher-ence over many models and many topics.
In Pro-ceedings of EMNLP, pages 952?961, Jeju Island,Korea.I.
Szpektor and I. Dagan.
2008.
Learning entailmentrules for unary templates.
In Proceedings of COL-ING, Manchester, UK.Julie Weeds and David Weir.
2003.
A general frame-work for distributional similarity.
In Proceedings ofEMNLP, pages 81?88, Sapporo, Japan.Julie Weeds, David Weir, and Diana McCarthy.
2004.Characterising measures of lexical distributionalsimilarity.
In Proceedings of COLING, pages 1015?1021, Geneva.Dominic Widdows and Stanley Peters.
2003.
Wordvectors and quantum logic.
In Proceedings ofthe Eight Mathematics of Language Conference,Bloomington, Indiana.Dominic Widdows.
2003.
Orthogonal negation in vec-tor spaces for modelling word-meanings and docu-ment retrieval.
In Proceedings of ACL, pages 136?143, Sapporo, Japan.519
