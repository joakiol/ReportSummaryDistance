Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 25?32,Prague, 28 June 2007. c?2007 Association for Computational LinguisticsConcept Disambiguation for Improved Subject AccessUsing Multiple Knowledge SourcesTandeep Sidhu, Judith Klavans, and Jimmy LinCollege of Information StudiesUniversity of MarylandCollege Park, MD 20742tsidhu@umiacs.umd.edu, {jklavans, jimmylin}@umd.eduAbstractWe address the problem of mining text forrelevant image metadata.
Our work is situ-ated in the art and architecture domain,where highly specialized technical vocabu-lary presents challenges for NLP tech-niques.
To extract high quality metadata,the problem of word sense disambiguationmust be addressed in order to avoid leadingthe searcher to the wrong image as a resultof ambiguous ?
and thus faulty ?
meta-data.
In this paper, we present a disam-biguation algorithm that attempts to selectthe correct sense of nouns in textual de-scriptions of art objects, with respect to arich domain-specific thesaurus, the Art andArchitecture Thesaurus (AAT).
We per-formed a series of intrinsic evaluations us-ing a data set of 600 subject terms ex-tracted from an online National Gallery ofArt (NGA) collection of images and text.Our results showed that the use of externalknowledge sources shows an improvementover a baseline.1.
IntroductionWe describe an algorithm that takes noun phrasesand assigns a sense to the head noun or phrase,given a large domain-specific thesaurus, the Artand Architecture Thesaurus1 (published by theGetty Research Institute).
This research is part ofthe Computational Linguistics for Metadata1http://www.getty.edu/research/conducting_research/vocabularies/aat/Building (CLiMB) project (Klavans 2006, Kla-vans in preparation), which aims to improve im-age access by automatically extracting metadatafrom text associated with images.
We presenthere a component of an overall architecture thatautomatically mines scholarly text for metadataterms.
In order to filter and associate a term witha related concept, ambiguous terms must be clari-fied.
The disambiguation of terms is a basic chal-lenge in computational linguistics (Ide and Vero-nis 1990, Agirre and Edmonds 2006).As more non-specialists in digital librariessearch for images, the need for subject term ac-cess has increased.
Subject terms enrich catalogrecords with valuable broad-reaching metadataand help improve image access (Layne 1994).Image seekers will receive more relevant resultsif image records contain terms that reflect con-ceptual, semantic, and ontological relationships.Furthermore, subject terms associated with hier-archical and faceted thesaural senses promise tofurther improve precision in image access.
Suchterms map to standardized thesaurus records thatinclude the term?s preferred, variant, and relatednames, including both broader and specific con-cepts, and other related concepts.
This informa-tion can then be filtered, linked, and subsequentlytested for usefulness in performing richer imageaccess.
As with other research on disambigua-tion, our hypothesis is that accurate assignment ofsenses to metadata index terms will results inhigher precision for searchers.
This hypothesiswill be fully tested as we incorporate the disam-biguation module in our end-to-end CLiMBToolkit, and as we perform user studies.Finding subject terms and mapping them to athesaurus is a time-intensive task for catalogers25(Rasmussen 1997, Ferguson and Intner 1998).Doing so typically involves reading image-relatedtext or other sources to find subject terms.
Evenso, the lack of standard vocabulary in extensivesubject indexing means that the enriched numberof subject terms could be inadvertently offset bythe vocabulary naming problem (Baca 2002).This paper reports on our results using thesubject terms in the AAT; the CLiMB project isalso using the Thesaurus of Geographic Names(TGN) and the Union List of Artist Names(ULAN).
Since the focus of this paper is on dis-ambiguation of common nouns rather than propernouns, the AAT is our primary resource.2.
Resources2.1 Art and Architecture Thesaurus (AAT)The AAT is a widely-used multi-faceted thesau-rus of terms for the cataloging and indexing ofart, architecture, artifactual, and archival materi-als.
Since the AAT offers a controlled vocabularyfor recording and retrieval of data in object, bib-liographic, and visual databases, it is of interest toa wide community.In the AAT, each concept is describedthrough a record which has a unique ID, preferredname, record description, variant names, broader,narrower, and related terms.
In total, AAT has31,000 such records.
For the purpose of this arti-cle, a record can be viewed as synonymous withsense.
Within the AAT, there are 1,400 homo-nyms, i.e., records with same preferred name.For example, the term wings has five senses inthe AAT (see Figure 1 below).Wings (5 senses):?
Sense#1: Used for accessories that project outwardfrom the shoulder of a garment and are made of clothor metal.?
Sense#2: Lateral parts or appendages of a work ofart, such as those found on a triptych.?
Sense#3: The areas offstage and to the side of theacting area.?
Sense#4: The two forward extensions to the sides ofthe back on an easy chair.?
Sense#5: Subsidiary parts of buildings extending outfrom the main portion.Figure 1:  Selection of AAT records for term ?wings?Table 1 shows the breakdown of the AAT vo-cabulary by number of senses with a sample lexi-cal item for each frequency.# ofSenses# ofHomonymsExample2 1097 bells3 215 painting4 50 alabaster5 39 wings6 9 boards7 5 amber8 2 emerald9 1 plum10 1 emerald green11 1 magenta12 1 ocher13 1 carmine14 2 slateTable 1:  Scope of the disambiguation problem in AATNote that there are potentially three tasks thatcould be addressed with our algorithm: (i) map-ping a term to the correct sense in the AAT, (ii)selecting amongst closely related terms in theAAT, and (iii) mapping synonyms onto a singleAAT entry.
In this paper, our primary focus is ontask (i); we handle task (ii) with a simple rankingapproach; we do not address task (iii).Table 1 shows that multiple senses per termmakes mapping subject terms to AAT very chal-lenging.
Manual disambiguation would be slow,tedious, and unrealistic.
Thus we explore auto-matic methods since, in order to identify the cor-rect sense of a term in running text, each of thesesenses needs to be viewed in context.2.2 The Test CollectionThe data set of terms that we use for evaluationcomes from the National Gallery of Art (NGA)online archive2.
This collection covers paintings,sculpture, decorative arts, and works from theMiddle Ages to the present.
We randomly se-lected 20 images with corresponding text fromthis collection and extracted noun phrases to formthe data set.
The data set was divided into twocategories: the training set and the test set.
Thetraining set consisted of 326 terms and was used2 http://www.nga.gov/home.htm26to develop the algorithm.
The test set consistedof 275 terms and was used to evaluate.Following standard procedure in word sensedisambiguation tasks (Palmer et al 2006),groundtruth for the data set was created manuallyby two labelers (referred to as Labeler 1 and La-beler 2 in Section 4 below).
These labelers werepart of the larger CLiMB project but they werenot involved in the development of the disam-biguation algorithm.
The process of creating thegroundtruth involved picking the correct AATrecord for each of the terms in the data set.Terms not appearing in the AAT (as determinedby the labelers) were given an AAT record valueof zero.
Each labeler worked independently onthis task and had access to the online version ofthe AAT and the text where each term appeared.Interannotator agreement for the task was encour-agingly high, at 85% providing a notional upperbound for automatic system performance (Gale etal.
1992).Not all terms in this dataset required disam-biguation; 128 terms (out of 326) under the train-ing set and 96 terms (out of 275) under the testset required disambiguation, since they matchedmore than one AAT record.
The dataset we se-lected was adequate to test our different ap-proaches and to refine our techniques.
We intendto run over more data as we collect and annotatemore resources for evaluation.2.3 SenseRelate AllWords3 and WordNet4SenseRelate AllWords (Banerjee and Pederson2003, Patwardhan et al 2003) is a Perl programthat our algorithm employs to perform basic dis-ambiguation of words.
We have adapted Sen-seRelate for the purpose of disambiguating AATsenses.Given a sentence, SenseRelate AllWords dis-ambiguates all the words in that sentence.
It usesword sense definitions from WordNet (in thiscase WordNet 2.1), a large lexical database ofEnglish nouns, verbs, adjectives, and adverbs.
Asan example, consider the text below:3 http://sourceforge.net/projects/senserelate4 http://wordnet.princeton.edu/With more than fifty individual scenes, the al-tarpiece was about fourteen feet wide.The SenseRelate result is:With more#a#2 than fifty#n#1 individual#n#1scene#n#10 the altarpiece#n#1 be#v#1 about#r#1fourteen#n#1 foot#n#2 wide#a#1In the above example, more#a#2 means SenseRe-late labeled more as an adjective and mapped it tosecond meaning of more (found in WordNet).fifty#n#1 means SenseRelate labeled fifty as anoun and mapped it to first meaning of fifty(found in WordNet).
Note, that fifty#n#1 maps toa sense in WordNet, whereas in our algorithm itneeds to map to an AAT sense.
In Section 3, weshow how we translate a WordNet sense to anAAT sense for use in our algorithm.To perform disambiguation, SenseRelate re-quires that certain parameters be set:  (1) thenumber of words around the target word (alsoknown as the context window), and  (2) the simi-larity measure.
We used a value of 20 for thecontext window, which means that SenseRelatewill use 10 words to the left and 10 words to theright of the target word to determine the correctsense.
We used lesk as the similarity measure inour algorithm which is based on Lesk (1986).This decision was based on several experimentswe did with various context window sizes andvarious similarity measures on a data set of 60terms.273.
Methodology3.1 Disambiguation AlgorithmFigure 2:  Disambiguation AlgorithmFigure 2 above shows that first we identify thenoun phrases from the input document.
Then wedisambiguate each noun phrase independently byfirst looking it up in the AAT.
If a record isfound, we move on to the next step; otherwise welook up the head noun (as the noun phrase) in theAAT.Second, we filter out any AAT records wherethe noun phrase (or the head noun) is used as anadjective (for a term like painting this would bepainting techniques, painting knives, paintingequipment, etc).
Third, if zero records are foundin the AAT, we label the term as ?not found inAAT.?
If only one matching record is found, welabel the term with the ID of this record.
Fourth,if more than one record is found, we use the dis-ambiguation techniques outlined in the next sec-tion to find the correct record.3.2 Techniques for DisambiguationFor each of the terms, the following techniqueswere applied in the order they are given in thissection.
If a technique failed to disambiguate aterm, we applied the next technique.
If none ofthese techniques was able to disambiguate, weselected the first AAT record as the correct re-cord.
Findings for each technique are provided inthe Results section below.First, we used all modifiers that are in thenoun phrase to find the correct AAT record.
Wesearched for the modifiers in the record descrip-tion, variant names, and the parent hierarchynames of all the matching AAT senses.
If thistechnique narrowed down the option set to onerecord, then we found our correct record.
Forexample, consider the term ceiling coffers.
Forthis term we found two records: coffers (cofferedceiling components) and coffers (chests).
Thefirst record has the modifier ceiling in its recorddescription, so we were able to determine thatthis was the correct record.Second, we used SenseRelate AllWords andWordNet.
This gave us the WordNet sense of ournoun phrase (or its head noun).
Using that sensedefinition from WordNet, we next examinedwhich of the AAT senses best matches with theWordNet sense definition.
For this, we used theword overlapping technique where we awarded ascore of N to an AAT record where N wordsoverlap with the sense that SenseRelate picked.The AAT record with the highest score was se-lected as the correct record.
If none of the AATrecords received any positive score (above a cer-tain threshold), then it was decided that this tech-nique could not find the one correct match.As an example, consider finding the correctsense for the single word noun bells using Sen-seRelate:1.
Given the input sentence:??
city officials, and citizens were followed bywomen and children ringing bells for joy.?2.
Search for AAT records.
There are two recordsfor the bells in AAT:a. bells: ?Flared or bulbous terminals found onmany open-ended aerophone tubes?.b.
bells: ?Percussion vessels consisting of a hollowobject, usually of metal but in some cultures ofhard clay, wood, or glass, which when struck emitsa sound by the vibration of most of its mass;??3.
Submit the input sentence to SenseRelate, whichprovides a best guess for the correspondingWordNet senses for each word.4.
Get SenseRelate output, which indicates that theWordNet definition for bells is WordNet-Sense1,i.e., ?a hollow device made of metal that makes aringing sound when struck?28SenseRelate output:city#n#1 official#n#1 and citizen#n#1 be#v#1follow#v#20 by#r#1 woman#n#1 and child#n#1ringing#a#1 bell#n#1 for joy#n#15.
Find the correct AAT match using word overlap ofthe WordNet definition and the two AAT defini-tions for bells:WordNet:  ?a hollow device made of metal thatmakes a ringing sound when struck?compared with:AAT: ?Flared or bulbous terminals found on manyopen-ended aerophone tubes?and compared with:AAT:  ?Percussion vessels consisting of a hollowobject, usually of metal but in some cultures ofhard clay, wood, or glass, which when struckemits a sound by the vibration of most of itsmass;??6.
The second AAT sense is the correct sense accord-ing to the word overlap (see Table 2 below):Comparison Score Word OverlapAAT ?
Definition 1 andWordNet Sense10 NoneAAT ?
Definition 2 andWordNet Sense14 hollow, metal,sound, struckTable 2: Word Overlap to Select AAT DefinitionNotice that we only used the AAT record descrip-tion for performing the word overlap.
We ex-perimented by including other information pre-sent in the AAT record (like variant names, par-ent AAT record names) also, but simply using therecord description yielded the best results.Third, we used AAT record names (preferredand variant) to find the one correct match.
If oneof the record names matched better than the otherrecord names to the noun phrase name, that re-cord was deemed to be the correct record.
Forexample, the term altar more appropriatelymatches altars (religious building fixtures) thanaltarpieces (religious visual works).
Anotherexample is children, which better matches chil-dren (youth) than offspring (people by family re-lationship).Fourth, if none of the above techniquessucceeded in selecting one record, we used themost common sense definition for a term (takenfrom WordNet) in conjunction with the AAT re-sults and word overlapping mentioned above tofind the one correct record.4.
Results and Evaluation4.1 MethodologiesWe used three different evaluation methods toassess the performance of our algorithm.
Thefirst evaluation method computes whether ouralgorithm picked the correct AAT record (i.e., theAAT sense picked is in agreement with thegroundtruth).
The second method computeswhether the correct record is among the top threerecords picked by our algorithm.
In Table 3 be-low, this is referred to as Top3.
The third evalua-tion method computes whether the correct recordis in top five records picked by our algorithm,Top5.
The last two evaluations helped us deter-mine the usability of our algorithm in situationswhere it does not pick the correct record but itstill narrows down to top three or top five results.We ranked the AAT records according totheir preferred name for the baseline, given theabsence of any other disambiguation algorithm.Thus, AAT records that exactly matched the termin question appear on top, followed by recordsthat partially matched the term.
For example, forterm feet, the top three records were feet (terminalelements of objects), French feet (bracket feet),and Spanish feet (furniture components).
For thenoun wings, the top three records were wings(shoulder accessories), wings (visual works com-ponents), and wings (backstage spaces).4.2 Overall ResultsIn this section, we present evaluation results forall the terms.
In the next section, we present re-sults for only those terms that required disam-biguation.Overall results for the training set (326 terms)are shown in Table 3.
This table shows that over-all accuracy of our algorithm is 76% and 68% forLabeler 1 and Labeler 2, respectively.
The base-line accuracy is 69% for Labeler 1 and 62% forLabeler 2.
The other two evaluations show muchbetter results.
The Top 3 and Top5 evaluationshave accuracy of 84% and 88% for Labeler 1 andaccuracy of 78% and 79% for Labeler 2.
Thisargues for bringing in additional techniques to29enhance the SenseRelate approach in order toselect from Top3 or Top5.Evaluation Labeler 1 Labeler 2Algorithm Accuracy 76% 68%Baseline Accuracy 69% 62%Top3 84% 78%Top5 88% 79%Table 3: Results for Training Set (n=326 terms)In contrast to Table 3 for the training set, Table 4shows results for the test set.
Labeler 1 shows anaccuracy of 74% on the algorithm and 72% on thebaseline; Labeler 2 has an accuracy of 73% onthe algorithm and 69% on the baseline.Evaluation Labeler 1 Labeler 2Algorithm Accuracy 74% 73%Baseline Accuracy 72% 69%Top3 79% 79%Top5 81% 80%Table 4: Results for Test Set (n=275 terms)4.3 Results for Ambiguous TermsThis section shows the results for the terms fromthe training set and the test set that required dis-ambiguation.
Table 5 below shows that our algo-rithm?s accuracy for Labeler 1 is 55% comparedto the baseline accuracy of 35%.
For Labeler 2,the algorithm accuracy is 48% compared to base-line accuracy of 32%.
This is significantly lessthan the overall accuracy of our algorithm.
Top3and Top5 evaluations have accuracy of 71% and82% for Labeler 1 and 71% and 75% for Labeler2.Evaluation Labeler 1 Labeler 2Algorithm Accuracy 55% 48%Baseline Accuracy 35% 32%Top3 71% 71%Top5 82% 75%Table 5: Ambiguous Terms for Training (n=128 terms)Similar results can be seen for the test set (96terms) in Table 6 below.
Labeler 1 shows an ac-curacy of 50% on the algorithm and 42% on thebaseline; Labeler 2 has an accuracy of 53% onthe algorithm and 39% on the baseline.Evaluation Labeler 1 Labeler 2Algorithm Accuracy 50% 53%Baseline Accuracy 42% 39%Top3 63% 68%Top5 68% 71%Table 6: Results for Ambiguous Termsunder the Test Set (n=96 terms)4.4 AnalysisTable 7 shows that SenseRelate is used for mostof the AAT mappings, and provides a breakdownbased upon the disambiguation technique used.Row One in Table 7 shows how few terms weredisambiguated using the lookup modifier tech-nique, just 1 in the training set and 3 in the testset.Row Technique TrainingSet(n=128)Test  Set(n=96)One LookupModifier1 3Two SenseRelate 108 63Three Best RecordMatch14 12Four Most CommonSense5 18Table 7: Breakdown of AAT mappingsby Disambiguation TechniqueRows Two and Three show that most of the termswere disambiguated using the SenseRelate tech-nique followed by the Best Record Match tech-nique.
The Most Common Sense technique (RowFour) accounted for the rest of the labelings.Table 8 gives insight into the errors of our algo-rithm for the training set terms:Technique Reason for Error ErrorCountSenseRelate picked wrongWordNet sense16WordNet does not have thesense8Definitions did not overlap 11SenseRelateOther reasons 10Best RecordMatch10LookupModifier0Most Com-mon Sense3Table 8: Breakdown of the errors in our algorithmunder training set (58 total errors)Table 8 shows the following:(1) Out of the total of 58 errors, 16 errors werecaused because SenseRelate picked the wrongWordNet sense.
(2) 8 errors were caused because WordNet didnot  contain the sense of the word in which it was30being used.
For example, consider the term work-shop.
WordNet has two definitions of workshop:i.
?small workplace where handcrafts or manufac-turing are done?
andii.
?a brief intensive course for a small group; em-phasizes problem solving?but AAT has an additional definition that wasreferred by term workshop in the NGA text:?In the context of visual and decorative arts, refersto groups of artists or craftsmen collaborating toproduce works, usually under a master's name?
(3) 11 errors occurred because the AAT recorddefinition and the WordNet sense definition didnot overlap.
Consider the term figures in the sen-tence, ?As with The Holy Family, the style of thefigures offers no clear distinguishing characteris-tic.?
Then examine the AAT and WordNet sensedefinitions below for figures:AAT sense: ?Representations of humans or ani-mals?WordNet sense: ?a model of a bodily form (espe-cially of a person)?These definitions do not have any words in com-mon, but they discuss the same concept.
(4) 10 errors occurred in the Best Record Matchtechnique, 0 errors occurred under the LookupModifier Technique, and 3 errors occurred underthe Most Common Sense technique.5.
ConclusionWe have shown that it is possible to create anautomated program to perform word sense dis-ambiguation in a field with specialized vocabu-lary.
Such an application could have great poten-tial in rapid development of metadata for digitalcollections.
Still, much work must be done inorder to integrate our disambiguation programinto the CLiMB Toolkit, including the following:(1) Our algorithm?s disambiguation accuracy isbetween 48-55% (Table 5 and Table 6), and sothere is room for improvement in the algorithm.Currently we depend on an external program(SenseRelate) to perform much of the disam-biguation (Table 7).
Furthermore, SenseRelatemaps terms to WordNet and we then map theWordNet sense to an AAT sense.
This extra stepis overhead, and it causes errors in our algorithm.We can either explore the option of re-implementing concepts behind SenseRelate todirectly map terms to the AAT, or we may needto find additional approaches to employ hybridtechniques (including machine learning) for dis-ambiguation.
At the same time, we may benefitfrom the fact that WordNet, as a general resource,is domain independent and thus offers wider cov-erage.
We will need to explore the trade-off inprecision between different configurations usingthese different resources.
(2) We need more and better groundtruth.
Ourcurrent data set of noun phrases includes termlike favor, kind, and certain aspects.
These termsare unlikely to be used as meaningful subjectterms by a cataloger and will never be mapped toAAT.
Thus, we need to develop reliable heuris-tics to determine which noun phrases are poten-tially high value subject index terms.
A simplefrequency count does not achieve this purpose.Currently we are evaluating based on ground-truth that our project members created.
Instead,we would like to extend the study to a wider setof image catalogers as labelers, since they will bethe primary users of the CLiMB tool.
Imagecatalogers have experience in finding subjectterms and mapping subject terms to the AAT.They can also help determine which terms arehigh quality subject terms.In contrast to working with the highly experi-enced image cataloger, we also want to extend thestudy to include various groups with differentuser needs.
For example, journalists have ongo-ing needs for images, and they tend to search bysubject.
Using participants like these for markupand evaluation promises to provide comparativeresults, ones which will enable us to effectivelyreach a broad audience.We also would like to test our algorithm onmore collections.
This will help us ascertainwhat kind of improvements or additions wouldmake CLiMB a more general tool.6.
AcknowledgementsWe thank Rachel Wadsworth and Carolyn Shef-field.
We also acknowledge Philip Resnik forvaluable discussion.317.
ReferencesBaca, Murtha, ed.
2002.
Introduction to art imageaccess: issues, tools, standards, strategies.
GettyResearch Institute.Banerjee, S., and T. Pedersen.
2003.
Extendedgloss overlaps as a measure of semantic relat-edness.
In Proceedings of the Eighteenth Inter-national Joint Conference on ArtificialIntelli-gence, 805?810.Ferguson, Bobby and Sheila Intner.
1998.
SubjectAnalysis: Blitz Cataloging Workbook.
West-port, CT:Libraries Unlimited Inc.Gale, W. A., K. W. Church, and D. Yarowsky.1992.
Using bilingual materials to developword sense disambiguation methods.
In Pro-ceedings of the Fourth International Confer-ence on Theoretical and Methodological Issuesin Machine Translation, 101-112, Montreal,Canada.Ide, Nancy M. and Jean Veronis.
1990.
MappingDictionaries: A Spreading Activation Ap-proach.
In Proceedings of the 6th Annual Con-ference of the UW Centre for the New OED andText Research, 52-64 Waterloo, Ontario.Lesk, Michael.
1986.
Automatic Sense Disam-biguation Using Machine Readable Dictionar-ies: How to Tell a Pine Cone from an IceCream Cone.
In Proceedings of ACM SIGDOCConference, 24-26, Toronto, Canada.Klavans, Judith L. 2006.
Computational Linguis-tics for Metadata Building (CLiMB).
In Pro-cedings of the OntoImage Workshop, G. Gref-fenstette, ed.
Language Resources and Evalua-tion Conference (LREC), Genova, Italy.Klavans, Judith L. (in preparation).
Using Com-putational Linguistic Techniques and Thesaurifor Enhancing Metadata Records in ImageSearch:  The CLiMB Project.Layne, Sara Shatford.
1994.
Some issues in theindexing of images.
Journal of the AmericanSociety for Information Science, 583-588.Palmer, Martha, Hwee Tou Ng, & Hoa TrangDang.
2006.
Evaluation of WSD Systems.Word Sense Disambiguation: Algorithms andApplications.
Eneko Agirre and Philip Ed-monds, ed.
75-106.
Dordrecht, The Nether-lands:Springer.Patwardhan, S., S. Banerjee, S. and T. Pedersen.2003.
Using measures of semantic relatednessfor word sense disambiguation.
Proceedings ofthe Fourth International Conference on Intelli-gent Text Processing and Computational Lin-guistics, 241?257.Rasmussen, Edie.
M. 1997.
Indexing images.
An-nual Review of Information Science and Tech-nology (ARIST), 32, 169-196.32
