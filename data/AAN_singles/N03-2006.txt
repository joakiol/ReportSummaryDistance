Adaptation Using Out-of-Domain Corpus within EBMTTakao Doi,  Eiichiro Sumita, Hirofumi YamamotoATR Spoken Language Translation Research Laboratories2-2-2 Hikaridai, Kansai Science City, Kyoto, 619-0288 Japan{takao.doi, eiichiro.sumita, hirofumi.yamamoto}@atr.co.jpAbstractIn order to boost the translation quality ofEBMT based on a small-sized bilingual cor-pus, we use an out-of-domain bilingual corpusand, in addition, the language model of an in-domain monolingual corpus.
We conductedexperiments with an EBMT system.
The twoevaluation measures of the BLEU score andthe NIST score demonstrated the effect of us-ing an out-of-domain bilingual corpus and thepossibility of using the language model.1 IntroductionExample-Based Machine Translation (EBMT) is adapt-able to new domains.
If you simply prepare a bilingualcorpus of a new domain, you?ll get a translation systemfor the domain.
However, if only a small-sized corpus isavailable, low translation quality is obtained.
We ex-plored methods to boost translation quality based on asmall-sized bilingual corpus in the domain.
Amongthese methods, we use an out-of-domain bilingual cor-pus and, in addition, the language model (LM) of an in-domain monolingual corpus.
For accuracy of the LM, alarger training set is better.
The training set is a targetlanguage corpus, which can be more easily preparedthan a bilingual corpus.In prior works, statistical machine translation(Brown, 1993) used not only LM but also translationmodels.
However, making a translation model requires abilingual corpus.
On the other hand, in some studies onmultiple-translation selection, the LM of the target lan-guage is used to calculate translation scores (Kaki,1999; Callison-Burch, 2001).
For adaptation, we use theLM of an in-domain target language.In the following sections, we describe the methodsusing an out-of-domain bilingual corpus and an in-domain monolingual corpus.
Moreover, we report onour experiments.2 Adaptation MethodsEBMT (Nagao, 1984) retrieves the translation ex-amples that are most similar to an input expression andadjusts the examples to obtain the translation.
TheEBMT system in our approach retrieves not only in-domain examples, but also out-of-domain examples.When using out-of-domain examples, suitability to thetarget domain is considered.
We tried the followingthree types of adaptation methods.
(1) Merging equallyAn in-domain corpus and an out-of-domain corpus aresimply merged and used without distinction.
(2) Merging with preference for in-domain corpusAn in-domain corpus and an out-of-domain corpus aremerged.
However, when multiple examples with thesame similarity are retrieved, the in-domain examplesare used.
(3) Using LMBeforehand, we make an LM of an in-domain targetlanguage corpus and, according to the LM, assign aprobability to the target sentence of each out-of-domainexample.In the example retrieval phase of the EBMT system,two types of examples are handled differently.
(3-1) From in-domain examples, the most similar exam-ples are retrieved.
(3-2) From out-of-domain examples, not only the mostsimilar examples but also other examples that arenearly as similar are retrieved.
In the retrieved ex-amples, examples with the highest probabilities oftheir target sentences by the LM are selected.
(3-3) From the results of both (3-1) and (3-2), the mostsimilar examples are selected.
Examples of (3-1) areused when the similarities are equal to each other.3 Translation Experiments3.1 ConditionsIn order to evaluate the adaptability of an EBMT without-of-domain examples, we applied the methods de-scribed in Section 2 to the EBMT and evaluated thetranslation quality in Japanese-to-English translation.We used an EBMT, DP-match Driven transDucer (D3,Sumita, 2001)  as a test bed.We used two Japanese-and-English bilingual cor-pora.
In this experiment on adaptation, as an out-of-domain corpus, we used Basic Travel Expression Cor-pus (BTEC, described as BE-corpus in Takezawa,2002); as an in-domain corpus,  we used a telephoneconversation corpus (TEL).
The statistics of the corporaare shown in Table 1.
TEL is split into two parts: a  testset of 1,653 sentence pairs and a training set of 9,918.Perplexities reveal the large difference between the in-domain and out-of-domain corpora.The translation qualities were evaluated by theBLEU score (Papineni, 2001) and the NIST score(Doddington, 2002).
The evaluation methods comparethe system output translation with a set of referencetranslations of the same source text by  finding se-quences of words in the reference translations thatmatch those in the system output translation.
We usedthe English sentence corresponding to each input Japa-nese sentence in the test set as the reference translation.Therefore, achieving a better score by the evaluationmeans that the translation results can be regarded  asmore adequate translations for the domain.In order to simulate incremental expansion of an in-domain bilingual corpus and to observe the relationshipbetween corpus size and translation quality, translationswere performed with some subsets of the training cor-pus.
The numbers of the sentence pairs are 0, 1000, .. ,5000 and 9918, adding randomly selected examplesfrom the training set.The LM of the domain?s target language was theword trigram model of the English sentences of thetraining set of TEL.
We tried two patterns of training setquantities in making the LM: 1) all of the training set,and 2) the part of the set used for translation examplesaccording to the numbers mentioned above.3.2 ResultsTable 2 shows the BLEU scores from the translationexperiment, which show certain tendencies.
Generally,by using more in-domain examples, the translation re-sults steadily achieve better scores.
The score when us-ing 4,000 in-domain examples exceeded that whenusing 152,172 out-of-domain examples.
Equal mergingoutperformed using only out-of-domain examples.Merging with in-domain preference outperformed equalmerging, and using LM outperformed merging with in-domain preference.
Comparing the two cases using LM,using LM made from all of the training set got a slightlybetter scores than the other, which implies that betterLM is made from a larger corpus.
All of the adaptationmethods are more effective when a smaller-sized in-domain corpus is available.
When using no in-domainexamples, the effect of using LM made from the entiretraining set was relatively large.Table 3 shows the NIST scores for the same experi-ment.
We can observe the same tendencies as in thetable of BLEU scores, except that the advantage of us-ing LM made from all of the training set over that froma partial set was not observed.4 Conclusion and Future WorkA corpus-based approach is able to quickly build a ma-chine translation system for a new domain if a bilingualcorpus of that domain is available.
However, if only asmall-sized corpus is available, a low translation qualityis obtained.
In order to boost the performance, severalmethods using out-of-domain data were explored in thispaper.
The experimental results showed the effect ofusing an out-of-domain corpus by two evaluation meas-ures, i.e., the BLEU score and the NIST score.We also showed the possibility of increasing thetranslation quality by using the LM of the domain?starget language.
However, the gains from using the LMin the evaluation scores were not significant.
We mustcontinue experiments with other corpora and undervarious conditions.
In addition, though we?ve implicitlyassumed a high-quality in-domain corpus, next we?dlike to investigate using a low-quality corpus.Table 1.
Corpus StatisticsBTEC TELJapanese English Japanese English# of sentences 152,172 11,571# of words 1,045,694 909,270 103,860 92,749Vocabulary size 19,999 12,268 5,242 4,086Average sen-tence length 6.87 5.98 8.98 8.0224.19 28.85 37.22 40.04TEL language model BTEC language modelPerplexity (word trigram)190.77 142.04 57.27 81.26AcknowledgementsThe research reported here was supported in part by acontract with the Telecommunications AdvancementOrganization of Japan entitled, ?A study of speech dia-logue translation technology based on a large corpus?.ReferencesTakezawa, T. et al 2002.
Toward a Broad-coverageBilingual Corpus for Speech Translation of TravelConversations in the Real World, Proc.
of  LREC-2002Papineni, K. et al 2001.
Bleu: a Method for AutomaticEvaluation of Machine Translation, RC22176, Sep-tember 17, 2001, Computer ScienceDoddington, G. 2002.
Automatic Evaluation of MachineTranslation Quality Using N-gram Co-OccurrenceStatistics.
Proc.
of the HLT 2002 ConferenceNagao, M. 1984.
A Framework of a Mechanical Trans-lation between Japanese and English by AnalogyPrinciple, in Artificial and Human Intelligence,Elithorn, A. and Banerji, R.
(eds.).
North-HollandSumita, E. 2001 Example-based machine translationusing DP-matching between word sequences, Proc.of DDMT Workshop of 39th ACLBrown, P. F. et al 1993.
The mathematics of statisticalmachine translation: Parameter estimation, Computa-tional Linguistics, 19(2)Kaki, S. et al 1999.
Scoring multiple translations usingcharacter N-gram, Proc.
of  NLPRS-99Callison-Burch, C. et al 2001.
A Program for Auto-matically Selecting the Best Output from MultipleMachine Translation Engines, Proc.
of MT SummitVIIITable 2.
Experimental results of translation by BLEU scores# of in-domain examples 0 1,000 2,000 3,000 4,000 5,000 9,918Using in-domain examples --- 0.0190 0.0602 0.0942 0.1200 0.1436 0.2100Using out-of-domain examples 0.1099Merging equally 0.1271 0.1430 0.1590 0.1727 0.1868 0.2303Merging with preference for in-domain 0.1296 0.1469 0.1632 0.1776 0.1922 0.2333Using LM of partial training set0.10990.1361 0.1538 0.1686 0.1829 0.1976Using LM of all training set 0.1225 0.1393 0.1557 0.1716 0.1852 0.1987 0.2387Table 3.
Experimental results of translation by NIST scores# of in-domain examples 0 1,000 2,000 3,000 4,000 5,000 9,918Using in-domain examples --- 0.0037 0.1130 0.4168 0.7567 1.1619 2.7400Using out-of-domain examples 1.1126Merging equally 1.4283 1.7367 2.0690 2.3405 2.6142 3.5772Merging with preference for in-domain 1.4580 1.7975 2.1343 2.4045 2.7088 3.6255Using LM of partial training set1.11261.7454 2.0449 2.3639 2.5825 2.9304Using LM of all training set 1.4404 1.7007 2.0125 2.3484 2.5992 2.8973 3.7544
