Proceedings of the BioNLP Shared Task 2013 Workshop, pages 76?85,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsGeneralizing an Approximate Subgraph Matching-based System to ExtractEvents in Molecular Biology and Cancer GeneticsHaibin Liuhaibin.liu@nih.govNCBI, Bethesda, MD, USAKarin Verspoorkarin.verspoor@nicta.com.auNICTA, Melbourne, VIC, AustraliaDonald C. Comeaucomeau@ncbi.nlm.nih.govNCBI, Bethesda, MD, USAAndrew MacKinlayandrew.mackinlay@nicta.com.auNICTA, Melbourne, VIC, AustraliaW.
John Wilburwilbur@ncbi.nlm.nih.govNCBI, Bethesda, MD, USAAbstractWe participated in the BioNLP 2013 sharedtasks, addressing the GENIA (GE) and the Can-cer Genetics (CG) event extraction tasks.
Ourevent extraction is based on the system we re-cently proposed for mining relations and eventsinvolving genes or proteins in the biomedicalliterature using a novel, approximate subgraphmatching-based approach.
In addition to han-dling the GE task involving 13 event types uni-formly related to molecular biology, we gener-alized our system to address the CG task tar-geting a challenging set of 40 event types re-lated to cancer biology with various argumentsinvolving 18 kinds of biological entities.
More-over, we attempted to integrate a distributionalsimilarity model into our system to extend thegraph matching scheme for more events.
In ad-dition, we evaluated the impact of using paths ofall possible lengths among event participants askey contextual dependencies to extract potentialevents as compared to using only the shortestpaths within the framework of our system.We achieved a 46.38% F-score in the CG taskand a 48.93% F-score in the GE task, ranking3rd and 4th respectively.
The consistent perfor-mance confirms that our system generalizes wellto various event extraction tasks and scales tohandle a large number of event and entity types.1 IntroductionUnderstanding the sophisticated interactions betweenvarious components of biological systems and conse-quences of these biological processes on the functionand behavior of the systems provides profound im-pacts on translational biomedical research, leading tomore rapid development of new therapeutics and vac-cines for combating diseases.
For the past five years,the BioNLP shared task series has served as an in-strumental platform to promote the development oftext mining methodologies and resources for the au-tomatic extraction of semantic events involving genesor proteins such as gene expression, binding, or reg-ulatory events from the biomedical literature (Kim etal., 2009; Kim et al 2011).
An event typically cap-tures the association of multiple participants of vary-ing numbers and with diverse semantic roles (Anani-adou et al 2010).
Since events often serve as partic-ipants in other events, the extraction of such nestedevent structures provides an integrated, network viewof these biological processes.Previous shared tasks focused exclusively onevents at the molecular and sub-cellular level.
How-ever, biological processes at higher levels of organi-zation are equally important, such as cell prolifer-ation, organ growth and blood vessel development.While preserving the classic event extraction taskssuch as the GE task, the BioNLP-ST 2013 broad-ens the scope of application domains by introducingmany new issues in biology such as cancer geneticsand pathway curation.
On behalf of NCBI (NationalCenter for Biotechnology Information), our team par-ticipated in the GENIA (GE) task and the Cancer Ge-netics (CG) task.
Compared to the GE task that aimsfor 13 types of events concerning the protein NF-?B,the CG task targets a challenging set of 40 types ofbiological processes related to the development andprogression of cancer involving 18 entity types.
Thisadditionally requires that event extraction systems beable to associate entities and events at the molecularlevel with anatomy level effects and organism leveloutcomes of cancer biology.Our event extraction is based on the system we re-cently proposed for mining relations and events in-volving genes or proteins in the biomedical litera-ture using a novel, Approximate Subgraph Matching-based (ASM) approach (Liu et al 2013a).
Whenevaluated on the GE task of the BioNLP-ST 2011, itsperformance is comparable to the top systems in ex-tracting 9 types of biological events.
In the BioNLP-76ST 2013, we generalized our system to investigateboth CG and GE tasks.
Moreover, we attempted to in-tegrate a distributional similarity model into the sys-tem to extend the graph matching scheme for moreevents.
The graph representation that considers pathsof all possible lengths (all-paths) between any twonodes has been encoded in graph kernels used inconjunction with Support Vector Machines (SVM),and led to state-of-the-art performance in extractingprotein-protein (Airola et al 2008) and drug-drug in-teractions (Zhang et al 2012).
Borrowing from theidea of the all-paths representation, in addition, weevaluated the impact of using all-paths among eventparticipants as key contextual dependencies to extractpotential events as compared to using only the short-est paths within the framework of our system.The rest of the paper is organized as follows: InSection 2, we briefly introduce our ASM-based eventextraction system.
Section 3 describes our experi-ments aiming to extend our system.
Section 4 elab-orates some implementation details and Section 5presents our results and discussion.
Finally, Section6 summarizes the paper and introduces future work.2 ASM-based Event ExtractionThe underlying assumption of our event extractionapproach is that the contextual dependencies of eachstated biological event represent a typical context forsuch events in the biomedical literature.
Our ap-proach falls into the machine learning category ofinstance-based reasoning (Alpaydin, 2004).
Specif-ically, the key contextual structures are learned fromeach labeled positive instance in a set of train-ing data and maintained as event rules in the formof subgraphs.
Extraction of events is performedby searching for an approximate subgraph isomor-phism between key dependencies and input sen-tence graphs using an approximate subgraph match-ing (ASM) algorithm designed for literature-basedrelational knowledge extraction (Liu et al 2013a).By introducing error tolerance into the graph match-ing process, our approach is capable of retrievingevents encoded within complex dependency contextswhile maintaining the extraction precision at a highlevel.
The ASM algorithm has been released as opensource software1.
See (Liu et al 2013a) for more de-tails on the ASM algorithm, its complexity and thecomparison with existing graph distance metrics.Figure 1 illustrates the overall architecture of ourASM-based system with three core components high-1http://asmalgorithm.sourceforge.netlighted: rule induction, sentence matching and ruleset optimization.
Our approach focuses on extract-ing events expressed within the boundaries of a singlesentence.
It is also assumed that entities involved inthe target event have been annotated.
Next, we brieflydescribe the core components of the system.Rule InductionPreprocessingSentence MatchingPostprocessingTraining data Testing dataRule SetOptimizationFigure 1: ASM-based Event Extraction Framework2.1 Rule InductionEvent rules are learned automatically using the fol-lowing method.
Starting with the dependency graphof each training sentence, for each annotated event,the shortest dependency path connecting the eventtrigger to each event argument in the undirected ver-sion of the graph is selected.
While additional in-formation such as individual words in each sentence(bag-of-words), sequences of words (n-grams) andsemantic concepts is typically used in the state-of-the-art supervised learning-based systems to cover abroader context (Airola et al 2008; Buyko et al2009; Bjo?rne et al 2012), the shortest path be-tween two tokens in the dependency graph is par-ticularly likely to carry the most valuable informa-tion about their mutual relationship (Bunescu andMooney, 2005a; Thomas et al 2011b; Rinaldi etal., 2010).
In case there exists more than one short-est path, all of them are considered.
For multi-tokenevent triggers, the shortest path connecting every trig-ger token to each event argument is extracted, and theunion of the paths is then computed for each trigger.For regulatory events that take a sub-event as an ar-gument, the shortest path is extracted so as to connectthe trigger of the main event to that of the sub-event.For complex events that involve multiple argu-ments, we computed the dependency path union ofall shortest paths from trigger to each event argument,resulting in a graph in which all event participants arejointly depicted.
Individual dependency paths con-necting triggers to each argument are also consideredto determine event arguments independently.
If the77resulting arguments share the same event trigger, theyare grouped together to form a potential event.
In ourapproach, the individual paths aim to retrieve morepotential events while the path unions retain the pre-cision advantage of joint inference.While the dependencies of such paths are used asthe graph representation of the event, a detailed de-scription records the participants of the event, theirsemantic role labels and the associated nodes in thegraph.
All participating biological entities are re-placed with a tag denoting their entity type, e.g.
?Pro-tein?
or ?Organism?, to ensure generalization of thelearned rules.
As a result, each annotated event isgeneralized and transformed into a generic graph-based rule.
The resulting event rules are categorizedinto different target event types.2.2 Sentence MatchingEvent extraction is achieved by matching the inducedrules to each testing sentence and applying the de-scriptions of rule tokens (e.g.
role labels) to the cor-responding sentence tokens.
Since rules and sentenceparses all possess a graph representation, event recog-nition becomes a subgraph matching problem.
Weintroduced a novel approximate subgraph matching(ASM) algorithm (Liu et al 2013a) to identify a sub-graph isomorphic to a rule graph within the graph ofa testing sentence.
The ASM problem is defined asfollows.Definition 1.
An event rule graph Gr =(Vr, Er) is approximately isomorphic to a subgraphSs of a sentence graph Gs = (Vs, Es), denotedby Gr ?=t Ss ?
Gs, if there is an injectivemapping f : Vr ?
Vs such that, for a giventhreshold t, t ?
0, the subgraph distance be-tween Gr and Gs satisfies 0 ?
subgraphDistf (Gr,Gs) ?
t, where subgraphDistf (Gr, Gs) = ws ?structDistf (Gr, Gs) + wl ?
labelDistf (Gr, Gs) +wd ?
directionalityDistf (Gr, Gs).The subgraph distance is proposed to be theweighted summation of three penalty-based measuresfor a candidate match between the two graphs.
Themeasure structDist compares the distance betweeneach pair of matched nodes in one graph to thedistance between corresponding nodes in the othergraph, and accumulates the structural differences.The distance in rule graphs is defined as the lengthof the shortest path between two nodes.
The distancein sentence graphs is defined as the length of the pathbetween corresponding nodes that leads to minimumstructural difference with the distance in rule graphs.Because dependency graphs are edge-labeled, ori-ented graphs, the measures labelDist and direction-alityDist evaluate respectively the overall differencesin edge labels and directionalities on the comparedpath between each pair of matched nodes in the twographs.
The real numbers ws, wl and wd are non-negative weights associated with the measures.The weights ws, wl and wd are defaulted to beequal but can be tuned to change the emphasis of theoverall distance function.
The distance threshold tcontrols the isomorphism quality of the retrieved sub-graphs from sentences.
A smaller t allows only lim-ited variations and always looks for a sentence sub-graph as closely isomorphic to the rule graph as pos-sible.
A larger t enables the extraction of events de-scribed in complicated dependency contexts, thus in-creasing the chance of retrieving more events.
How-ever, it can incur a bigger search cost due to the eval-uation of more potential solutions.An iterative, bottom-up matching process is usedto ensure the extraction of complex and nested events.Starting with the extraction of simple events, simpleevent rules are first matched with a testing sentence.Next, as potential arguments of higher level events,obtained simple events continue to participate in thesubsequent matching process between complex eventrules and the sentence to initiate the iterative processfor detecting complex events with nested structures.The process terminates when no new candidate eventis generated for the testing sentence.During the matching phase we relax the eventrules that contain sub-event arguments such that anymatched event can substitute for the sub-event.
Webelieve that the contextual structures linking anno-tated sub-events of a certain type are generalizableto other event types.
This relaxation increases thechance of extracting complex events with nestedstructures but still takes advantage of the contextualconstraints encoded in the rule graphs.2.3 Rule Set OptimizationTypical of instance-based reasoners, the accuracy ofrules with which to compare an unseen sentence iscrucial to the success of our approach.
For instance, aTranscription rule encoding a noun compound mod-ification dependency between ?TNF?
and ?mRNA?derived from an event context ?expression of TNFmRNA?
should not produce a Transcription eventfor the general phrase ?level of TNF mRNA?
eventhough they share a matchable dependency.
Suchmatches result in false positive events.78Therefore, we measured the accuracy of each ruleri in terms of its prediction result via Eq.(1).
For rulesthat produce at least one prediction, we ranked thembyAcc(ri) and excluded the ones with aAcc(ri) ratiolower than an empirical threshold, e.g.
1:4.Acc(ri) =#correct predictions by ri#total predictions by ri(1)Because of nested event structures, the removalof some rules might incur a propagating effect onrules relying on them to produce arguments for theextraction of higher order events.
Therefore, an it-erative rule set optimization process, in which eachiteration performs sentence matching, rule rankingand rule removal sequentially, is conducted, lead-ing to a converged, optimized rule set.
While theASM algorithm aims to extract more potential events,this performance-based evaluation component en-sures the precision of our event extraction framework.3 Extensions to Event Extraction SystemIn the BioNLP-ST 2013, we attempted two differentways to extend the current event extraction system:(1) integrate a distributional similarity model into thesystem to extend the graph matching scheme for moreevents; (2) use paths of all possible lengths (all-paths)among event participants as key contextual depen-dencies to extract events.
We next elaborate thesesystem extensions in detail.3.1 Integrating Distributional Similarity ModelThe proposed subgraph distance measure of the ASMalgorithm focuses on capturing differences in theoverall graph structure, edge labels and directional-ities.
However, when determining the injective nodemapping between graphs, the matching remains at thesurface word level.In the current setting, various node features can beconsidered when comparing two graph nodes, result-ing in different matching criteria.
The features in-clude POS tags (P), event trigger (T), token lemmas(L) and tokens themselves (A).
For instance, a match-ing criterion, ?P*+L?, requires that the relaxed POStags (P*) and the lemmatized form (L) of tokens beidentical for each rule node to match with a sentencenode.
The relaxed POS allows the plural form ofnouns to match with the singular form, and the con-jugations of verbs to match with each other.
How-ever, the inability to go beyond surface level match-ing prevents node tokens that share similar meaningbut possess distinct orthography from matching witheach other.
For instance, a mismatch between ruletoken ?crucial?
and a sentence token ?critical?
couldlead to an undiscovered Positive regulation event.We attempted to use only POS information in thenode matching scheme and observed a nearly 14%increase in recall (Liu et al 2013b).
However, theprecision drops sharply, resulting in an undesirableF-score.
This indicates that the lexical informationis a critical supplement to the contextual dependencyconstraints in accurately capturing events within theframework of our system.
Moreover, we attempted toextend the node matching using the synsets of Word-Net (Fellbaum, 1998) to allow tokens to match withtheir synonyms (Liu et al 2011).
However, sinceWordNet is developed for the general English lan-guage, it relates biomedical terms e.g., ?expression?with general words such as ?aspect?
and ?face?, thusleading to incorrect events.In this work, we integrated a distributional simi-larity model (DSM) into our node matching schemeto further improve the generalization of event rules.A distributional similarity model is constructedbased on the distributional hypothesis (Harris, 1954):words that occur in the same contexts tend to sharesimilar meanings.
We expect that the incorporationof DSM will enable our system to capture matchingtokens in testing sentences that do not appear in thetraining data while maintaining the extraction pre-cision at a high level.
There have been many ap-proaches to compute the similarity between wordsbased on their distribution in a corpus (Landauer andDumais, 1997; Pantel and Lin, 2002).
The output is aranked list of similar words to each word.
We reim-plemented the model proposed by (Pantel and Lin,2002) in which each word is represented by a fea-ture vector and each feature corresponds to a contextwhere the word appears.
The value of the featureis the pointwise mutual information (Manning andSchu?tze, 1999) between the feature and the word.
Letc be a context and Fc(w) be the frequency count of aword w occurring in context c. The pointwise mutualinformation, miw,c between c and w is defined as:miw,c =Fc(w)N?iFi(w)N ?
?jFc(j)N(2)where N =?i?jFi(j) is the total frequency countof all words and their contexts.Since mutual information is known to be biasedtowards infrequent words/features, the above mutual79information value is multiplied by a discounting fac-tor as described in (Pantel and Lin, 2002).
The simi-larity between two words is then computed using thecosine coefficient (Salton and McGill, 1986) of theirmutual information vectors.We experimented with two different approaches tointegrate the DSM into our event extraction system.First, the model is directly embedded into the nodematching scheme.
Once a match cannot be deter-mined by surface tokens, the DSM is invoked to allowa match if the sentence token appears in the list of thetop M most similar words to the rule token.
Sec-ond, additional event rules are generated by replac-ing corresponding rule tokens with their top M mostsimilar words, rather than allow DSM to participatein the node matching.
While the first method mea-sures the consolidated extraction ability of an eventrule by combining its DSM-generalized performance,the second approach provides a chance to evaluate theimpact of each DSM-introduced similar word indi-vidually on event extraction.3.2 Adopting All-paths for Event RulesAirola et alproposed an all-paths graph (APG) ker-nel for extracting protein-protein interactions (PPI),in which the kernel function counts weighted shareddependency paths of all possible lengths (Airola etal., 2008).
Thomas et aladopted this kernel asone of the three models used in the ensemble learn-ing for extracting drug-drug interactions (Thomas etal., 2011a) and won the recent DDIExtraction 2011challenge (Segura-Bedmar et al 2011).
The JULIElab adapted the APG kernel to event extraction us-ing syntactically pruned and semantically enricheddependency graphs (Buyko et al 2009).The graph representation of the kernel consists oftwo sub-representations: the full dependency parseand the surface word sequence of the sentence wherea pair of interacting entities occurs.
At the expenseof computational complexity, this representation en-ables the kernel to explore broader contexts of aninteraction, thus taking advantage of the entire de-pendency graph of the sentence.
When comparingtwo interaction instances, instead of using only theshortest path that might not always provide suffi-cient syntactic information about relations, the ker-nel considers paths of all possible lengths betweenany two nodes.
More recently, a hash subgraph pair-wise (HSP) kernel-based approach was also proposedfor drug-drug interactions and adopts the same graphrepresentation as the APG kernel (Zhang et al 2012).In contrast, the graph representation that our ASMalgorithm searches in a sentence is inherently re-stricted to the shortest path among target entities inevent rules, as described in Section 2.2.
Borrowingfrom the idea of the all-path graph representation, inthis work we attempted to explore contexts beyondthe shortest paths to enrich our rule set.
We evalu-ated within the framework of our system the impactof using acyclic paths of all possible lengths amongevent participants as key contextual dependencies topopulate the event rule set as compared to using onlythe shortest paths in the current system setting.4 Implementation4.1 PreprocessingWe employed the preprocessed data in theBioC (Comeau et al 2013) compliant XML formatprovided by the shared task organizers as supportingresources.
The BioC project attempts to addressthe interoperability among existing natural languageprocessing tools by providing a unified BioC XMLformat.
The supporting analyses include tokeniza-tion, sentence segmentation, POS tagging andlemmatization.
Different syntactic parsers analyzetext based on different underlying methodologies, forinstances, the Stanford parser (Klein and Manning,2003) performs joint inference over the product of anunlexicalized Probabilistic Context-Free Grammar(PCFG) parser and a lexicalized dependency parserwhile the McClosky-Charniak-Johnson (Charniak)parser (McClosky and Charniak, 2008) is based onN -best parse reranking over a lexicalized PCFGmodel.
In order to take advantage of multiple aspectsof structural analysis of sentences, both Stanfordparser and Charniak parser, which are among the bestperforming parsers trained on the GENIA Treebankcorpus, are used to parse the training sentences andproduce dependency graphs for learning event rules.Only the Charniak parser is used on the testingsentences in the event extraction phase.4.2 ASM Parameter SettingThe GE task includes 13 different event types.
Sinceeach type possesses its own event contexts, an indi-vidual threshold te is assigned to each type.
Togetherwith the 3 distance function weights ws, wl and wd,the ASM requires 16 parameters for the GE event ex-traction task.
Similarly, the ASM requires 43 param-eters to cater to the 40 diverse event types of the CGtask.
As reported in (Liu et al 2013a), we used agenetic algorithm (GA) (Cormen et al 2001) to au-80tomatically determine values of the 12 ASM param-eters for the 2011 GE task using the training data.We inherited these previously determined parametersand adapted them into the 2013 tasks according tothe event type and its argument configuration.
For in-stance, ?Pathway?
events in the CG task is assignedthe same te as the ?Binding?
events in the GE task asthey possess similar argument configurations.Table 1 shows the parameter setting for the 2013GE task with the equal weights ws = wl = wd con-straint.
The graph node matching criterion ?P*+L?that requires the relaxed POS tags and the token lem-mas to be identical is used in the ASM.Parameter Value Parameter ValuetGene expression 8 tUbiquitination 3tTranscription 7 tBinding 7tProtein catabolism 10 tRegulation 3tPhosphorylation 8 tPositive regulation 3tLocalization 8 tNegative regulation 3tAcetylation 3 ws 10tDeacetylation 3 wl 10tProteinmodification 3 wd 10Table 1: ASM parameter setting for the 2013 GE task4.3 Distributional Similarity ModelIn our implementation, we made following improve-ments to the original Pantel model (Pantel and Lin,2002): (1) lemmas of words generated by the Bi-oLemmatizer (Liu et al 2012) are used to achievegeneralization.
The POS information is combinedwith each lemmatized word to disambiguate its cat-egory.
(2) instead of the linear context where aword occurs, we take advantage of dependency con-texts inferred from dependency graphs.
For instance,?toxicity?amod?
is extracted as a feature of the to-ken ?nonhematopoietic JJ?.
It captures the dependenttoken, the type and the directionality of the depen-dency.
(3) the resulting miw,c is scaled into the [0, 1]range by?
?miw,c1 + ?
?miw,cto avoid greater miw,c valuesdominating the similarity calculation between words.An empirical ?
= 0.01 is used.
(4) while only theimmediate dependency contexts of a word are usedin our model, our implementation is flexible so thatcontexts of various dependency depths could be takeninto consideration.In order to cover a wide range of words and capturethe diverse usages of them in biomedical texts, in-stead of resorting to an existing corpus, our distribu-tional similarity model is built based on a random se-lection of 5 million abstracts from the entire PubMed.When computing miw,c, we filtered out contexts ofeach word where the word occurs less than 5 times.Eventually, the model contains 2.8 million distinct to-kens and 0.4 million features.
When it is queried withan amino acid, e.g, ?lysine?, the top 15 tokens in theresulting ranked list are all correct amino acid names.5 Results and DiscussionThis section reports our results on the GE and the CGtasks respectively, including the attempted extensionsto our ASM-based event extraction system.5.1 GE task5.1.1 DatasetsThe 2013 GE task dataset is composed of full-textarticles from PubMed Central, which are divided intosmaller segments by the task organizers according tovarious sections of the articles.
Table 2 presents somestatistics of the GE dataset.Attributes Counted Training Development TestingFull article segments 222 249 305Proteins 3,571 4,138 4,359Annotated events 2,817 3,199 3,301Table 2: Statistics of BioNLP-ST 2013 GE datasetAs distributed, the development set is bigger thanthe training set.
For better system generalization, werandomly reshuffled the data and created a 353/118training/development division, a roughly 3:1 ratioconsistent with the settings in previous GE tasks.The results reported on the training/development datathereafter are based on our new data partition.5.1.2 GE Results on Development SetTable 3 shows the event extraction results on the 118development documents based on event rules derivedfrom different parsers.
Only the numbers of unique,optimized rules are reported and those that possessisomorphic graph representations determined by anExact Subgraph Matching (ESM) algorithm (Liu etal., 2013b) are removed.
The ensemble rule set com-bines rules derived from both parsers and achievesa better performance than that of using individualparsers.
It makes sense that the Charniak parser isfavored and leads to a performance close to the en-semble performance because sentences from whichevents are extracted are parsed by the Charniak parseras well.
However, we retained the additional rulesfrom the Stanford parser in the hope that they maycontribute to the testing data.When embedding the distributional similaritymodel (DSM) directly into the graph node matching81Parser Type Event Rule Recall Precision F-scoreCharniak 2,923 47.01% 66.01% 54.91%Stanford 3,305 43.66% 67.67% 53.08%Ensemble 4,617 47.45% 65.65% 55.09%Table 3: Performance of using different parsersscheme, we performed the DSM on all rule tokens ex-cept biological entities, meaning that for each rule to-ken, if a match will be granted if a rule token appearsin the top M most similar word list of a sentence to-ken, e.g., ?DSM 3?
denotes the top 3 similar wordsdetermined by the DSM.
We further performed DSMonly on trigger tokens for comparison, as presentedin Table 4.All Tokens Recall Precision F-scoreDSM 1 47.98% 52.56% 50.17%DSM 3 48.68% 35.07% 40.77%DSM 10 53.43% 19.38% 28.44%Trigger Tokens Recall Precision F-scoreDSM 1 48.06% 54.22% 50.95%DSM 3 48.59% 37.00% 42.01%DSM 10 53.35% 24.65% 33.72%Table 4: Performance of integrated DSMEven though the DSM helps to substantially in-crease the recall to 53.43%, we observed a significantprecision drop which leads to an inferior F-score tothe ensemble baseline in Table 3.
A close evaluationof the generated graph matches reveals that antonymsproduced by the DSM contributes to most of the falsepositive events.
For instance, the most similar wordsfor the verb ?increase?
and the adjective ?high?
re-turned by the model are ?decrease?
and ?low?
be-cause they tend to occur in the same contexts.
Fur-ther investigation is needed to automatically filter outthe antonyms.
When generating additional rules us-ing the top M most similar words from the DSM,since all the rules undergo the optimization process,the event extraction precision is ensured.
However,the recall increase from simple events is diluted bythe counter effect of the introduced false positives indetecting regulation-related complex events, result-ing in a comparable performance to the baseline.Table 5 gives the performance comparison of us-ing all-paths and the shortest paths in our event ex-traction system.
Using all-paths does not bring in asignificant improvement in F-score but takes 27 it-erations to optimize as compared to the 5-iterationoptimization on shortest paths.
Most of the rules in-duced from all-paths are eventually discarded by theoptimization process.
The all-paths graph represen-tation was motivated by the observation that short-est paths between candidate entities often excluderelation-signaling words when detecting binary re-lationships (Airola et al 2008).
Exploring broadercontexts ensures such words to be considered.
In theevent extraction task, however, since triggers havebeen annotated, they are naturally incorporated intothe shortest paths connecting trigger to each event ar-gument.
This in part explains why contexts beyondshortest paths did not bring in an appreciable benefit.All Tokens Recall Precision F-scoreAll-paths 48.77% 64.64% 55.59%Shortest paths 47.45% 65.65% 55.09%Table 5: Performance of using all-paths5.1.3 GE Results on Testing SetSince integrating the DSM and all-paths do not pro-vide significant performance improvements to oursystem, we decided to retain the original settings inthe ASM when extracting events from the testingdata.
While most of the 2011 shared task datasets arecomposed of PubMed abstracts compared to full-textarticles in the 2013 GE task, our system focuses onextracting events expressed within the boundaries ofa single sentence.
Therefore, in order to take advan-tage of existing annotated resources, we incorporatedthe annotated data of 2011 GE task and EPI (Epi-genetics and Post-translational Modifications) task toenrich the training instances of corresponding eventtypes of the 2013 GE task.
Eventually, we obtained atotal of 14,448 rules of different event types from ourtraining data.
In practice, it takes the ASM less than asecond to match the entire rule set with one documentand return results.Our submitted system achieves a 48.93% F-scoreon the 305 testing documents of the GE task, ranking4th among 12 participating teams.
Table 6 presentsthe performance of the top eight systems.System Recall Precision F-scoreEVEX 45.44% 58.03% 50.97%TEES 2.1 46.17% 56.32% 50.74%BioSEM 42.47% 62.83% 50.68%NCBI 40.53% 61.72% 48.93%DlutNLP 40.81% 57.00% 47.56%HDS4NLP 37.11% 51.19% 43.03%NICTANLM 36.99% 50.68% 42.77%USheff 31.69% 63.28% 42.23%Table 6: Performance of top 8 systems in GE taskOur performance is within a reasonable mar-gin from the best-performing system ?EVEX?, andshows an overall superior precision over most partic-ipating teams; only two of the top 5 systems obtained82a precision in the 60% range.
Particularly for theregulation-related complex events, we are the onlyteam that achieved a precision over 55% among all12 participating systems.
This indicates that eventrules automatically learned and optimized over train-ing data generalize well to the unseen text, and havethe ability to identify precisely corresponding events.We further evaluated the impact of the additonaltraining instances from 2011 tasks and the ensemblerule set derived from different parsers as presentedin Table 7.
With the help from the 2011 data, ourF-score is increased by 3% and we became the onlyteam that detected ?Ubiquitination?
events from test-ing data.
In addition, rules derived from the Stanfordparser do not provide additional benefits on the test-ing data compared to using the Charniak parser alone.System Attribute Recall Precision F-scoreEnsemble 2013 + 2011 data 40.53% 61.72% 48.93%Ensemble 2013 data 35.63% 63.91% 45.75%Charniak 2013 data 35.29% 65.71% 45.92%Table 7: Impact of 2011 data and ensemble rule set5.2 CG task5.2.1 DatasetsThe CG task dataset is prepared based on a previ-ously released corpus of angiogenesis domain ab-stracts (Wang et al 2011).
It targets a challengingset of 40 types of biological processes related to thedevelopment and progression of cancer involving 18entity types (Pyysalo et al 2012).
Table 8 presentssome statistics of the CG dataset.Attributes Counted Training Development TestingAbstracts 300 100 200Entities 10,935 3,634 6,955Annotated events 8,803 2,915 5,972Table 8: Statistics of BioNLP-ST 2013 CG dataset5.2.2 CG Results on Testing SetWe generalized our event extraction system to the CGtask and the corresponding annotated data of the 2011tasks is also incorporated in the training phase to ob-tain the optimized event rule set.
Due to time con-straints, the impact of integrating the DSM and all-paths is not evaluated on the CG task.
We achieveda 46.38% F-score on the 200 testing documents ofthe CG task, ranking 3rd among the 6 participatingteams.
Table 9 gives the primary evaluation results ofthe 6 participating teams; only ?TEES-2.1?
and weparticipated in both GE and CG tasks.
The detailedresults of each of the targeted 40 event types is avail-able from the official CG task website.Team Recall Precision F-scoreTEES-2.1 48.76% 64.17% 55.41%NaCTeM 48.83% 55.82% 52.09%NCBI 38.28% 58.84% 46.38%RelAgent 41.73% 49.58% 45.32%UET-NII 19.66% 62.73% 29.94%ISI 16.44% 47.83% 24.47%Table 9: Performance of all systems in 2013 CG taskInconsistent with other biological entities, the en-tity annotation for the optional ?Site?
argument in-volved in events such as ?Binding?, ?Mutation?
and?Phosphorylation?
are not provided by the task orga-nizers.
We consider that detecting ?Site?
entities isrelated to entity detection and we would like to focusour system on the event extraction itself.
Thus, wedecided to ignore the ?Site?
argument in our system.However, a problem will arise that even though theother arguments are correctly identified for an event,it might still be evaluated as false positive if a ?Site?argument is not detected.
This results in both falsepositive and false negative events.
In addition, sincewe did not perform the secondary task which requiresus to detect modifications of the predicted events, in-cluding negation and speculation, about 7.5% anno-tated instances in the testing data are thus missed,causing damage to our recall in the overall evalua-tion.
The organizers have agreed to issue an additonalevaluation that will focus on core event extraction tar-gets excluding optional arguments such as ?Site?
andthe secondary task.
We will conduct more detailedanalysis on the results once they are made available.6 Conclusion and Future WorkIn the BioNLP-ST 2013, we generalized our ASM-based system to address both GE and CG tasks.We attempted to integrate a distributional similaritymodel into our system to extend the graph match-ing scheme.
We also evaluated the impact of usingpaths of all possible lengths among event participantsas key contextual dependencies to extract potentialevents as compared to using only the shortest pathswithin the framework of our system.We achieved a 46.38% F-score in the CG task anda 48.93% F-score in the GE task, ranking 3rd and4th respectively.
While the distributional similaritymodel did not improve the overall performance of oursystem in the tasks, we would like to further investi-gate the antonym problem introduced by the model inour future work.83AcknowledgmentsThis research was supported by the Intramural Re-search Program of the NIH, NLM.ReferencesAntti Airola, Sampo Pyysalo, Jari Bjo?rne, TapioPahikkala, Filip Ginter, and Tapio Salakoski1.
2008.All-paths graph kernel for protein-protein interactionextraction with evaluation of cross-corpus learning.BMC Bioinformatics, 9 Suppl 11:s2.Ethem Alpaydin.
2004.
Introduction to Machine Learn-ing.
MIT Press.Sophia Ananiadou, Sampo Pyysalo, Jun?ichi Tsujii, andDouglas B. Kell.
2010.
Event extraction for systemsbiology by text mining the literature.
Trends in Biotech-nology, 28(7):381?390.Jari Bjo?rne, Filip Ginter, and Tapio Salakoski.
2012.
Uni-versity of turku in the BioNLP?11 shared task.
BMCBioinformatics, 13 Suppl 11:S4.Razvan C. Bunescu and Raymond J. Mooney.
2005a.A shortest path dependency kernel for relation extrac-tion.
In Proceedings of the conference on Human Lan-guage Technology and Empirical Methods in NaturalLanguage Processing, pages 724?731.Razvan C. Bunescu and Raymond J. Mooney.
2005b.Subsequence kernels for relation extraction.
In Pro-ceedings of the 19th Conference on Neural InformationProcessing Systems (NIPS).
Vancouver, BC, December.Ekaterina Buyko, Erik Faessler, Joachim Wermter, andUdo Hahn.
2009.
Event extraction from trimmed de-pendency graphs.
In BioNLP ?09: Proceedings of theWorkshop on BioNLP, pages 19?27, Morristown, NJ,USA.
Association for Computational Linguistics.Donald C. Comeau, Rezarta Islamaj Dog?an, Paolo Ci-ccarese, Kevin Bretonnel Cohen, Martin Krallinger,Florian Leitner, Zhiyong Lu, Yifan Peng, Fabio Ri-naldi, Manabu Torii, Alfonso Valencia, Karin Verspoor,Thomas C. Wiegers, Cathy H. Wu, and W. John Wilbur.2013.
BioC: A minimalist approach to interoperabilityfor biomedical text processing.
submitted.Thomas H. Cormen, Charles E. Leiserson, Ronald L.Rivest, and Clifford Stein.
2001.
Introduction to Al-gorithms.
The MIT Press.Christiane Fellbaum.
1998.
WordNet: An Electronic Lex-ical Database.
Bradford Books.Zellig Harris.
1954.
Distributional structure.
Word,10(23):146?162.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overview ofBioNLP?09 shared task on event extraction.
In Pro-ceedings of BioNLP Shared Task 2009 Workshop, pages1?9.
Association for Computational Linguistics.Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, RobertBossy, Ngan Nguyen, and Jun?ichi Tsujii.
2011.Overview of BioNLP shared task 2011.
In Proceedingsof BioNLP Shared Task 2011 Workshop, pages 1?6.
As-sociation for Computational Linguistics, June.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In ACL ?03: Proceedings of the41st Annual Meeting on Association for ComputationalLinguistics, pages 423?430.
Association for Computa-tional Linguistics.Thomas K. Landauer and Susan T. Dumais.
1997.
A so-lution to plato?s problem: The latent semantic analysistheory of acquisition, induction, and representation ofknowledge.
Psychological review, 104(2):211?240.Haibin Liu, Ravikumar Komandur, and Karin Verspoor.2011.
From graphs to events: A subgraph matching ap-proach for information extraction from biomedical text.In Proceedings of BioNLP Shared Task 2011 Work-shop, pages 164?172.
Association for ComputationalLinguistics, June.Haibin Liu, Tom Christiansen, William A Baumgartner,and Karin Verspoor.
2012.
Biolemmatizer: a lemmati-zation tool for morphological processing of biomedicaltext.
Journal of Biomedical Semantics, 3:3.Haibin Liu, Lawrence Hunter, Vlado Keselj, and KarinVerspoor.
2013a.
Approximate subgraph matching-based literature mining for biomedical events and re-lations.
PLOS ONE, 8:4 e60954.Haibin Liu, Vlado Keselj, and Christian Blouin.
2013b.Exploring a subgraph matching approach for extractingbiological events from literature.
Computational Intel-ligence.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of statistical natural language processing.MIT Press, Cambridge, MA, USA.David McClosky and Eugene Charniak.
2008.
Self-training for biomedical parsing.
In Proceedings of theAssociation for Computational Linguistics, pages 101?104, Columbus, Ohio.
The Association for ComputerLinguistics.Patrick Pantel and Dekang Lin.
2002.
Discovering wordsenses from text.
In Proceedings of the eighth ACMSIGKDD international conference on Knowledge dis-covery and data mining, KDD ?02, pages 613?619,New York, NY, USA.
ACM.Sampo Pyysalo, Tomoko Ohta, Makoto Miwa, Han-CheolCho, Jun?ichi Tsujii, and Sophia Ananiadou.
2012.Event extraction across multiple levels of biological or-ganization.
Bioinformatics, 28:i575?i581.Fabio Rinaldi, Gerold Schneider, Kaarel Kaljurand, SimonClematide, Thrse Vachon, and Martin Romacker.
2010.Ontogene in BioCreative II.5.
IEEE/ACM Trans.
Com-put.
Biology Bioinform., 7(3):472?480.84Gerard Salton and Michael J. McGill.
1986.
Introductionto Modern Information Retrieval.
McGraw-Hill, Inc.,New York, NY, USA.Isabel Segura-Bedmar, Paloma Martinez, and DanielSanchez-Cisneros.
2011.
The 1st DDIExtraction-2011Challenge Task: Extraction of Drug-Drug Interactionsfrom Biomedical Texts.
In Proceedings of the 1st Chal-lenge Task on Drug-Drug Interaction Extraction 2011,pages 1?9.Philippe Thomas, Mariana Neves, Illes Solt, DomonkosTikk, and Ulf Leser.
2011a.
Relation extraction fordrug-drug interactions using ensemble learning.
In Pro-ceedings of DDIExtraction-2011 challenge task, pages11?18.Philippe Thomas, Stefan Pietschmann, Ille?s Solt,Domonkos Tikk, and Ulf Leser.
2011b.
Not alllinks are equal: Exploiting dependency types for theextraction of protein-protein interactions from text.
InProceedings of BioNLP 2011 Workshop, pages 1?9.Association for Computational Linguistics, June.Domonkos Tikk, Philippe Thomas, Peter Palaga, Jo?rgHakenberg, and Ulf Leser.
2010.
A comprehensivebenchmark of kernel methods to extract protein?proteininteractions from literature.
PLoS Computational Biol-ogy, 6:e1000837, July.Xinglong Wang, Iain McKendrick, Ian Barrett, Ian Dix,Tim French, Jun?ichi Tsujii, and Sophia Ananiadou.2011.
Automatic extraction of angiogenesis bioprocessfrom text.
Bioinformatics, 27(19):2730?2737.Yijia Zhang, Hongfei Lin, Zhihao Yang, Jian Wang, andYanpeng Li.
2012.
A single kernel-based approach toextract drug-drug interactions from biomedical litera-ture.
PLOS ONE, 7(11): e48901.85
