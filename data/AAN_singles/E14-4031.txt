Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 159?163,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsImproving Dependency Parsers using Combinatory Categorial GrammarBharat Ram Ambati Tejaswini DeoskarInstitute for Language, Cognition and ComputationSchool of Informatics, University of Edinburghbharat.ambati@ed.ac.uk, {tdeoskar,steedman}@inf.ed.ac.ukMark SteedmanAbstractSubcategorization information is a usefulfeature in dependency parsing.
In thispaper, we explore a method of incorpo-rating this information via CombinatoryCategorial Grammar (CCG) categoriesfrom a supertagger.
We experiment withtwo popular dependency parsers (Maltand MST) for two languages: Englishand Hindi.
For both languages, CCGcategories improve the overall accuracyof both parsers by around 0.3-0.5% inall experiments.
For both parsers, wesee larger improvements specifically ondependencies at which they are known tobe weak: long distance dependencies forMalt, and verbal arguments for MST.
Theresult is particularly interesting in the caseof the fast greedy parser (Malt), since im-proving its accuracy without significantlycompromising speed is relevant for largescale applications such as parsing the web.1 IntroductionDependency parsers can recover much of thepredicate-argument structure of a sentence, whilebeing relatively efficient to train and extremelyfast at parsing.
Dependency parsers have beengaining in popularity in recent times due tothe availability of large dependency treebanksfor several languages and parsing shared tasks(Buchholz and Marsi, 2006; Nivre et al., 2007a;Bharati et al., 2012).Ambati et al.
(2013) showed that the perfor-mance of Malt (Nivre et al., 2007b) on the freeword order language, Hindi, is improved by usinglexical categories from Combinatory CategorialGrammar (CCG) (Steedman, 2000).
In this paper,we extend this work and show that CCG categoriesare useful even in the case of English, a typolog-ically different language, where parsing accuracyof dependency parsers is already extremely high.In addition, we also demonstrate the utility ofCCG categories to MST (McDonald et al., 2005)for both languages.
CCG lexical categoriescontain subcategorization information regardingthe dependencies of predicates, including long-distance dependencies.
We show that providingthis subcategorization information in the form ofCCG categories can help both Malt and MST onprecisely those dependencies for which they areknown to have weak rates of recovery.
The resultis particularly interesting for Malt, the fast greedyparser, as the improvement in Malt comes withoutsignificantly compromising its speed, so that itcan be practically applied in web scale parsing.Our results apply both to English, a fixed wordorder and morphologically simple language, andto Hindi, a free word order and morphologicallyrich language, indicating that CCG categoriesfrom a supertagger are an easy and robust wayof introducing lexicalized subcategorizationinformation into dependency parsers.2 Related WorkParsers using different grammar formalismshave different strengths and weaknesses, andprior work has shown that information from oneformalism can improve the performance of aparser in another formalism.
Sagae et al.
(2007)achieved a 1.4% improvement in accuracy over astate-of-the-art HPSG parser by using dependen-cies from a dependency parser for constrainingwide-coverage rules in the HPSG parser.
Coppolaand Steedman (2013) incorporated higher-orderdependency features into a cube decoding phrase-structure parser and obtained significant gainson dependency recovery for both in-domain andout-of-domain test sets.Kim et al.
(2012) improved a CCG parser usingdependency features.
They extracted n-best parsesfrom a CCG parser and provided dependency159Pierre Vinken will join the board as a nonexecutive director Nov. 29N/N N (S[dcl]\NP)/(S[b]\NP) ((S[b]\NP)/PP)/NP NP/N N PP/NP NP/N N/N N ((S\NP)\(S\NP))/N N> > > >N NP N (S\NP)\(S\NP)T >NP NP> >(S[b]\NP)/PP PP>S[b]\NP<S[b]\NP>S[dcl]\NP>S[dcl]Figure 1: A CCG derivation and the Stanford scheme dependencies for an example sentence.features from a dependency parser to a re-rankerwith an improvement of 0.35% in labelled F-scoreof the CCGbank test set.
Conversely, Ambatiet al.
(2013) showed that a Hindi dependencyparser (Malt) could be improved by using CCGcategories.
Using an algorithm similar to Cakici(2005) and Uematsu et al.
(2013), they first cre-ated a Hindi CCGbank from a Hindi dependencytreebank and built a supertagger.
They providedCCG categories from a supertagger as features toMalt and obtained overall improvements of 0.3%and 0.4% in unlabelled and labelled attachmentscores respectively.3 Data and ToolsFigure 1 shows a CCG derivation with CCGlexical categories for each word and Stanfordscheme dependencies (De Marneffe et al., 2006)for an example English sentence.
(Details of CCGand dependency parsing are given by Steedman(2000) and K?ubler et al.
(2009).
)3.1 TreebanksIn English dependency parsing literature, Stanfordand CoNLL dependency schemes are widelypopular.
We used the Stanford parser?s built-inconverter (with the basic projective option) togenerate Stanford dependencies and Penn2Malt1to generate CoNLL dependencies from PennTreebank (Marcus et al., 1993).
We used standardsplits, training (sections 02-21), development(section 22) and testing (section 23) for ourexperiments.
For Hindi, we worked with theHindi Dependency Treebank (HDT) releasedas part of Coling 2012 Shared Task (Bharati etal., 2012).
HDT contains 12,041 training, 1,233development and 1,828 testing sentences.We used the English (Hockenmaier and Steed-man, 2007) and Hindi CCGbanks (Ambati et al.,1http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html2013) for our experiments.
For Hindi we used twolexicons: a fine-grained one (with morphologicalinformation) and a coarse-grained one (withoutmorphological information).3.2 SupertaggersWe used Clark and Curran (2004)?s supertaggerfor English, and Ambati et al.
(2013)?s supertag-ger for Hindi.
Both are Maximum Entropy basedCCG supertaggers.
The Clark and Curran (2004)supertagger uses different features like word, part-of-speech, and contextual and complex bi-gramfeatures to obtain a 1-best accuracy of 91.5% onthe development set.
In addition to the abovementioned features, Ambati et al.
(2013) em-ployed morphological features useful for Hindi.The 1-best accuracy of Hindi supertagger for fine-grained and coarse-grained lexicon is 82.92% and84.40% respectively.3.3 Dependency ParsersThere has been a significant amount of work onparsing English and Hindi using the Malt andMST parsers in the recent past (Nivre et al.,2007a; Bharati et al., 2012).
We first run theseparsers with previous best settings (McDonald etal., 2005; Zhang and Nivre, 2012; Bharati etal., 2012) and treat them as our baseline.
Inthe case of English, Malt uses arc-standard andstack-projective parsing algorithms for CoNLLand Stanford schemes respectively and LIBLIN-EAR learner (Fan et al., 2008) for both theschemes.
MST uses 1st-order features, and a pro-jective parsing algorithm with 5-best MIRA train-ing for both the schemes.
For Hindi, Malt usesthe arc-standard parsing algorithm with a LIBLIN-EAR learner.
MST uses 2nd-order features, non-projective algorithm with 5-best MIRA training.For English, we assigned POS-tags using a per-ceptron tagger (Collins, 2002).
For Hindi, we alsodid all our experiments using automatic features160Language ExperimentMalt MSTUAS LAS UAS LASEnglishStanford Baseline 90.32 87.87 90.36 87.18Stanford + CCG 90.56** (2.5) 88.16** (2.5) 90.93** (5.9) 87.73** (4.3)CoNLL Baseline 89.99 88.73 90.94 89.69CoNLL + CCG 90.38** (4.0) 89.19** (4.1) 91.48** (5.9) 90.23** (5.3)HindiBaseline 88.67 83.04 90.52 80.67Fine CCG 88.93** (2.2) 83.23* (1.1) 90.97** (4.8) 80.94* (1.4)Coarse CCG 89.04** (3.3) 83.35* (1.9) 90.88** (3.8) 80.73* (0.4)Table 1: Impact of CCG categories from a supertagger on dependency parsing.
Numbers in bracketsare percentage of errors reduced.
McNemar?s test compared to baseline, * = p < 0.05 ; ** = p < 0.01(Hindi Malt results (grey background) are from Ambati et al.
(2013)).
(POS, chunk and morphological information)extracted using a Hindi shallow parser2.4 CCG Categories as FeaturesFollowing Ambati et al.
(2013), we used supertagswhich occurred at least K times in the trainingdata, and backed off to coarse POS-tags otherwise.For English K=1, i.e., when we use CCG cate-gories for all words, gave the best results.
K=15gave the best results for Hindi due to sparsity is-sues, as the data for Hindi is small.
We provideda supertag as an atomic symbol similar to a POStag and didn?t split it into a list of argument andresult categories.
We explored both Stanford andCoNLL schemes for English and fine and coarse-grained CCG categories for Hindi.
All feature andparser tuning was done on the development data.We assigned automatic POS-tags and supertags tothe training data.4.1 Experiments with Supertagger outputWe first used gold CCG categories extracted fromeach CCGbank as features to the Malt and MST,to get an upper bound on the utility of CCG cate-gories.
As expected, gold CCG categories boostedthe Unlabelled Attachment Score (UAS) and La-belled Attachment Score (LAS) by a large amount(4-7% in all the cases).We then experimented with using automaticCCG categories from the English and Hindi su-pertaggers as a feature to Malt and MST.
With au-tomatic categories from a supertagger, we got sta-tistically significant improvements (McNemar?stest, p < 0.05 for Hindi LAS and p < 0.01 for therest) over the baseline parsers, for all cases (Table1).
Since the CCGbanks used to train the supertag-gers are automatically generated from the con-stituency or dependency treebanks used to train2http://ltrc.iiit.ac.in/analyzer/hindi/the dependency parsers, the improvements areindeed due to reparameterization of the model toinclude CCG categories and not due to additionalhand annotations in the CCGbanks.
This showsthat the rich subcategorization information pro-vided by automatically assigned CCG categoriescan help Malt and MST in realistic applications.For English, in case of Malt, we achieved0.3% improvement in both UAS and LAS forStanford scheme.
For CoNLL scheme, theseimprovements were 0.4% and 0.5% in UAS andLAS respectively.
For MST, we got around 0.5%improvements in all cases.In case of Hindi, fine-grained supertags gavelarger improvements for MST.
We got finalimprovements of 0.5% and 0.3% in UAS and LASrespectively.
In contrast, for Malt, Ambati et al.
(2013) had shown that coarse-grained supertagsgave larger improvements of 0.3% and 0.4% inUAS and LAS respectively.
Due to better handlingof error propagation in MST, the richer informa-tion in fine-grained categories may have surpassedthe slightly lower supertagger performance,compared to coarse-grained categories.4.2 Analysis: EnglishWe analyze the impact of CCG categories ondifferent labels (label-wise) and distance ranges(distance-wise) for CoNLL scheme dependencies(We observed a similar impact for the Stanfordscheme dependencies as well).
Figure 2a showsthe F-score for three major dependency labels,namely, ROOT (sentence root), SUBJ (subject),OBJ (object).
For Malt, providing CCG categoriesgave an increment of 1.0%, 0.3% for ROOT andSUBJ labels respectively.
For MST, the improve-ments for ROOT and SUBJ were 0.5% and 0.8%respectively.
There was no significant improve-ment for OBJ label, especially in the case of Malt.16187.792.588.792.893.4 92.588.293.9 93.388.589909192939495 MaltMalt + CCGMSTMST + CCG86.5 86.5868788ROOT SUBJ DOBJ(a) Label-wise impact98.278.6 80.898.379.2 81.798.480.884.598.581.885.578838893981-5 6-10 >10Malt Malt + CCG MST MST + CCG(b) Distance-wise impactFigure 2: Label-wise and Distance-wise impact of supertag features on Malt and MST for EnglishFigure 2b shows the F-score of dependenciesbased on the distance ranges between words.
Thepercentage of dependencies in the 1?5, 6?10 and>10 distance ranges are 88.5%, 6.6% and 4.9% re-spectively out of the total of around 50,000 depen-dencies.
For both Malt and MST, there was veryslight improvement for short distance dependen-cies (1?5) but significant improvements for longerdistances (6?10 and >10).
For Malt, there wasan improvement of 0.6% and 0.9% for distances6?10, and >10 respectively.
For MST, theseimprovements were 1.0% and 1.0% respectively.4.3 Analysis: HindiIn the case of Hindi, for MST, providing CCGcategories gave an increment of 0.5%, 0.4% and0.3% for ROOT, SUBJ and OBJ labels respec-tively in F-score over the baseline.
Ambati et al.
(2013) showed that for Hindi, providing CCGcategories as features improved Malt in betterhandling of long distance dependencies.The percentage of dependencies in the 1?5,6?10 and >10 distance ranges are 82.2%,8.6% and 9.2% respectively out of the total ofaround 40,000 dependencies.
Similar to English,there was very slight improvement for shortdistance dependencies (1?5).
But for longerdistances, 6?10, and >10, there was significantimprovement of 1.3% and 1.3% respectivelyfor MST.
Ambati et al.
(2013) reported similarimprovements for Malt as well.4.4 DiscussionThough valency is a useful feature in dependencyparsing (Zhang and Nivre, 2011), Zhang and Nivre(2012) showed that providing valency informationdynamically, in the form of the number of depen-dencies established in a particular state duringparsing, did not help Malt.
However, as we haveshown above, providing this information as a staticlexical feature in the form of CCG categories doeshelp Malt.
In addition to specifying the number ofarguments, CCG categories also contain syntactictype and direction of those arguments.
However,providing CCG categories as features to zpar(Zhang and Nivre, 2011) didn?t have significantimpact as it is already using similar information.4.5 Impact on Web Scale ParsingGreedy parsers such as Malt are very fast and arepractically useful in large-scale applications suchas parsing the web.
Table 2, shows the speed ofMalt, MST and zpar on parsing English test datain CoNLL scheme (including POS-tagging andsupertagging time).
Malt parses 310 sentences persecond, compared to 35 and 11 of zpar and MSTrespectively.
Clearly, Malt is orders of magnitudefaster than MST and zpar.
After using CCGcategories from the supertagger, Malt parses 245sentences per second, still much higher than otherparsers.
Thus we have shown a way to improveMalt without significantly compromising speed,potentially enhancing its usefulness for web scaleparsing.Parser Ave. Sents / Sec Total TimeMST 11 3m 36szpar 35 1m 11sMalt 310 0m 7.7sMalt + CCG 245 0m 10.2sTable 2: Time taken to parse English test data.5 ConclusionWe have shown that informative CCG categories,which contain both local subcategorization infor-mation and capture long distance dependencieselegantly, improve the performance of two de-pendency parsers, Malt and MST, by helpingin recovering long distance relations for Maltand local verbal arguments for MST.
This istrue both in the case of English (a fixed wordorder language) and Hindi (free word order andmorphologically richer language), extending theresult of Ambati et al.
(2013).
The result isparticularly interesting in the case of Malt whichcannot directly use valency information, whichCCG categories provide indirectly.
It leads to animprovement in performance without significantlycompromising speed and hence promises to beapplicable to web scale processing.162ReferencesBharat Ram Ambati, Tejaswini Deoskar, and MarkSteedman.
2013.
Using CCG categories to improveHindi dependency parsing.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics (Volume 2: Short Papers), pages604?609, Sofia, Bulgaria.Akshar Bharati, Prashanth Mannem, and Dipti MisraSharma.
2012.
Hindi Parsing Shared Task.
In Pro-ceedings of Coling Workshop on Machine Transla-tion and Parsing in Indian Languages, Kharagpur,India.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of the Tenth Conference on Computa-tional Natural Language Learning, pages 149?164,New York City, New York.Ruken Cakici.
2005.
Automatic induction of a CCGgrammar for Turkish.
In Proceedings of the ACLStudent Research Workshop, pages 73?78, Ann Ar-bor, Michigan.Stephen Clark and James R. Curran.
2004.
The impor-tance of supertagging for wide-coverage CCG pars-ing.
In Proceedings of COLING-04, pages 282?288.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: theory and experi-ments with perceptron algorithms.
In Proceedingsof the conference on Empirical methods in naturallanguage processing, EMNLP ?02, pages 1?8.Greg Coppola and Mark Steedman.
2013.
The effectof higher-order dependency features in discrimina-tive phrase-structure parsing.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics (Volume 2: Short Papers), pages610?616, Sofia, Bulgaria.Marie Catherine De Marneffe, Bill Maccartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InIn LREC 2006.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: A li-brary for large linear classification.
J. Mach.
Learn.Res., 9:1871?1874, June.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A Corpus of CCG Derivations and Depen-dency Structures Extracted from the Penn Treebank.Computational Linguistics, 33(3):355?396.Sunghwan Mac Kim, Dominick Ng, Mark Johnson,and James Curran.
2012.
Improving combina-tory categorial grammar parse reranking with depen-dency grammar features.
In Proceedings of COL-ING 2012, pages 1441?1458, Mumbai, India.Sandra K?ubler, Ryan McDonald, and Joakim Nivre.2009.
Dependency Parsing.
Synthesis Lectures onHuman Language Technologies.
Morgan & Clay-pool Publishers.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of the 43rd An-nual Meeting on Association for Computational Lin-guistics, pages 91?98, Ann Arbor, Michigan.Joakim Nivre, Johan Hall, Sandra K?ubler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007a.
The CoNLL 2007 shared task ondependency parsing.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007, pages915?932, Prague, Czech Republic.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, G?ulsen Eryigit, Sandra K?ubler, SvetoslavMarinov, and Erwin Marsi.
2007b.
Maltparser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(2):95?135.Kenji Sagae, Yusuke Miyao, and Jun?ichi Tsujii.
2007.HPSG parsing with shallow dependency constraints.In Proceedings of the 45th Annual Meeting of the As-sociation of Computational Linguistics, pages 624?631, Prague, Czech Republic.Mark Steedman.
2000.
The Syntactic Process.
MITPress, Cambridge, MA, USA.Sumire Uematsu, Takuya Matsuzaki, Hiroki Hanaoka,Yusuke Miyao, and Hideki Mima.
2013.
Inte-grating multiple dependency corpora for inducingwide-coverage Japanese CCG resources.
In Pro-ceedings of the 51st Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 1042?1051, Sofia, Bulgaria.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 188?193, Portland, Ore-gon, USA.Yue Zhang and Joakim Nivre.
2012.
Analyzingthe effect of global learning and beam-search ontransition-based dependency parsing.
In Proceed-ings of COLING 2012: Posters, pages 1391?1400,Mumbai, India, December.163
