Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 65?73,Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational LinguisticsLearning to Extract Folktale KeywordsDolf Trieschnigg, Dong Nguyen and Marie?t TheuneUniversity of TwenteEnschede, The Netherlands{d.trieschnigg,d.nguyen,m.theune}@utwente.nlAbstractManually assigned keywords provide avaluable means for accessing large docu-ment collections.
They can serve as a shal-low document summary and enable moreefficient retrieval and aggregation of infor-mation.
In this paper we investigate key-words in the context of the Dutch Folk-tale Database, a large collection of storiesincluding fairy tales, jokes and urban leg-ends.
We carry out a quantitative and qual-itative analysis of the keywords in the col-lection.
Up to 80% of the assigned key-words (or a minor variation) appear in thetext itself.
Human annotators show moder-ate to substantial agreement in their judg-ment of keywords.
Finally, we evaluate alearning to rank approach to extract andrank keyword candidates.
We concludethat this is a promising approach to auto-mate this time intensive task.1 IntroductionKeywords are frequently used as a simple wayto provide descriptive metadata about collectionsof documents.
A set of keywords can conciselypresent the most important aspects of a documentand enable quick summaries of multiple docu-ments.
The word cloud in Figure 1, for instance,gives a quick impression of the most importanttopics in a collection of over 40,000 documents (acollection of Dutch folktales).Keyword assignment or generation is the taskof finding the most important, topical keywords orkeyphrases to describe a document (Turney, 2000;Frank et al 1999).
Based on keywords, smallgroups of documents (Hammouda et al 2005) orlarge collections of documents (Park et al 2002)can be summarized.
Keyword extraction is a re-stricted case of keyword assignment: the assignedkeywords are a selection of the words or phrasesappearing in the document itself (Turney, 2000;Frank et al 1999).In this paper we look into keyword extractionin the domain of cultural heritage, in particularfor extracting keywords from folktale narrativesfound in the Dutch Folktale Database (more onthis collection in section 3).
These narrativesmight require a different approach for extractionthan in other domains, such as news stories andscholarly articles (Jiang et al 2009).
Stories in theDutch Folktale Database are annotated with un-controlled, free-text, keywords.
Because suggest-ing keywords which do not appear in the text is aconsiderably harder task to automate and to eval-uate, we restrict ourselves to keywords extractedfrom the text itself.In the first part of this paper we study the cur-rent practice of keyword assignment for this col-lection.
We analyze the assigned keywords in thecollection as a whole and present a more fine-grained analysis of a sample of documents.
More-over, we investigate to what extent human anno-tators agree on suitable keywords extracted fromthe text.
Manually assigning keywords is an ex-pensive and time-consuming process.
Automaticassignment would bring down the cost and time toarchive material.
In the second part of this paperwe evaluate a number of automatic keyword ex-traction methods.
We show that a learning to rankapproach gives promising results.The overview of this paper is as follows.
Wefirst describe related work in automatic keywordassignment.
In section 3 we introduce the DutchFolktale Database.
In section 4 we present an anal-ysis of the keywords currently used in the folktaledatabase.
In section 5 we investigate the agree-ment of human annotators on keyword extraction.In section 6 we present and evaluate an automaticmethod for extracting and ranking keywords.
Weend with a discussion and conclusion in section 7.65Keyword (translation) Frequencydood (death) 5,861man (man) 4,547vrouw (woman) 4,154sterven (to die) 3,168huis (house) 2,894spokerij (haunting) 2,491duivel (devil) 2,487nacht (night) 2,449voorteken (omen) 2,380voorloop (forerunnings) 2,372geld (money) 2,370toverij (sorcery) 2,322zien (to see) 2,270heks (witch) 2,233boer (farmer) 2,189water (water) 2,177angst (fear) 2,091hekserij (witchcraft) 1,911kind (child) 1,853spoken (ghosts) 1,747spook (ghost) 1,742seks (sex) 1,659Figure 1: Frequent keywords in the Dutch Folktale Database2 Related WorkBecause of space limitations, we limit our dis-cussion of related work to keyword extraction inthe context of free-text indexing.
Automated con-trolled vocabulary indexing is a fundamentallydifferent task (see for instance Medelyan and Wit-ten (2006) and Plaunt and Norgard (1998)).Typically, keyword extraction consists of twosteps.
In the first step candidate keywords are de-termined and features, such as the frequency orposition in the document, are calculated to char-acterize these keywords.
In the second step thecandidates are filtered and ranked based on thesefeatures.
Both unsupervised and supervised algo-rithms have been used to do this.2.1 Candidate ExtractionCandidate keywords can be extracted in a numberof ways.
The simplest approach is to treat eachsingle word as a candidate keyword, optionallyfiltering out stop words or only selecting wordswith a particular Part-of-Speech (Liu et al 2009a;Jiang et al 2009).
More sophisticated approachesallow for multi-word keywords, by extracting con-secutive words from the text, optionally limited tokeywords adhering to specific lexical patterns (Os-inski and Weiss, 2005; Hulth, 2003; Rose et al2010; Frank et al 1999; Turney, 2000).2.2 Features to Characterize KeywordsMany features for characterizing candidate key-words have been investigated previously, withvarying computational complexities and resourcerequirements.
The simplest features are basedon document and collection statistics, for instancethe frequency of a potential keyword in the doc-ument and the inverse document frequency in thecollection (Turney, 2000; Hulth, 2003; Frank etal., 1999).
Examples of more complex featuresare: features based on characteristics of lexicalchains, requiring a lexical database with wordmeanings (Ercan and Cicekli, 2007); features re-lated to frequencies in external document collec-tions and query logs (Bendersky and Croft, 2008;Yih et al 2006; Liu et al 2009b; Xu et al 2010);and a feature to determine the cohesiveness of re-trieved documents with that keyword (Benderskyand Croft, 2008).2.3 Unsupervised Methods for KeywordExtractionUnsupervised methods for keyword extractiontypically rely on heuristics to filter and rank thekeywords in order of importance.
For instance,by ranking the candidates by their importance inthe collection ?
estimated by the inverse docu-ment frequency.
Another approach is to apply thePageRank algorithm to determine the most impor-tant keywords based on their co-occurrence link-structure (Mihalcea and Tarau, 2004).
Liu et al(2009b) employed clustering to extract keywordsthat cover all important topics from the originaltext.
From each topic cluster an exemplar is deter-mined and for each exemplar the best correspond-ing keyword is determined.2.4 Supervised Methods for KeywordExtractionEarly supervised methods used training data to setthe optimal parameters for (unsupervised) systems66based on heuristics (Turney, 2000).
Other methodsapproached keyword extraction as a binary classi-fication problem: given a candidate keyword it hasto be classified as either a keyword or not.
Meth-ods include decision trees (Bendersky and Croft,2008), Naive Bayes (Frank et al 1999) and Sup-port Vector Machines (Zhang et al 2006).
Zhanget al(2008) approached keyword extraction as alabeling problem for which they employed condi-tional random fields.
Recently, keyword extrac-tion has been cast as a ranking problem and learn-ing to rank techniques have been applied to solveit (Jiang et al 2009).
Jiang et al(2009) concludedthat learning to rank approaches performed betterthan binary classifiers in the context of extractingkeywords from scholarly texts and websites.
Dif-ferent variations of learning to rank exist, see (Li,2011) for an overview.3 The Dutch Folktale DatabaseThe Dutch Folktale Database is a repository ofover 40,000 folktales in Dutch, old Dutch, Frisianand a large number of Dutch dialects.
The mate-rial has been collected in the 19th, 20th and 21thcenturies, and consists of stories from various pe-riods, including the Middle Ages and the Renais-sance.
The collection has both an archival and aresearch function.
It preserves an important partof the oral cultural heritage of the Netherlands andcan be used for comparative folk narrative studies.Since 2004 the database is available online1.The real value of the database does not only liethe stories themselves, but also in their manuallyadded set of descriptive metadata fields.
Thesefields include, for example, a summary in Dutch,a list of proper names present in the folktales, anda list of keywords.
Adding these metadata is atime-consuming and demanding task.
In fact, theamount of work involved hampers the growth ofthe folktale database.
A large backlog of digitizedfolktales is awaiting metadata assignment beforethey can be archived in the collection.
Being ableto automatically assign keywords to these docu-ments would be a first step to speed up the archiv-ing process.4 Analysis of Assigned KeywordsIn this section we analyze the keywords that havebeen manually assigned to the stories in the DutchFolktale Database.
First we look at the keywords1http://www.verhalenbank.nl, in Dutch only0 10 20 30 40 50Number of assigned keywords0500100015002000Document frequencyFigure 2: Number of assigned keywords per doc-umentassigned to the collection as a whole.
After that wemake a more fine-grained analysis of the keywordsassigned to a selection of the documents.4.1 Quantitative AnalysisWe analyzed a snapshot from the Dutch FolktaleDatabase (from early 2012) that consists of 41,336folktales.
On average, 15 keywords have been as-signed to each of these documents (see Figure 2).The median number of assigned keywords is 10,however.
The keywords vocabulary has 43,195unique keywords, most of which consist of a sin-gle word (90%).
Figure 1 shows a word cloudof keywords used in the collection; more frequentkeyword types appear larger.
On the right, it liststhe most frequent keyword types (and their trans-lations).
The assignment of keywords to docu-ments has a Zipfian distribution: a few keywordtypes are assigned to many documents, whereasmany keyword types are assigned to few docu-ments.When we limit our collection to stories in Dutch(15,147 documents), we can determine how manyof the manually assigned keywords can be foundliterally in the story text2.
We define the keywordcoverage of a document as the fraction of its as-signed keywords which is found in the full textor its summary.
The average keyword coverageof the Dutch stories is 65%.
Figure 3 shows ahistogram of the coverage.
It shows that most ofthe documents have a keyword coverage of 0.5 ormore.2Stories in other languages or dialects have been assignedDutch keywords.670.0 0.2 0.4 0.6 0.8 1.0Keyword coverage by full-text and summary050010001500200025003000Document frequencyFigure 3: Keyword coverage of folktales in Dutch4.2 Qualitative AnalysisThe quantitative analysis does not provide insightinto what kind of keywords have been assigned.Therefore, we analyzed a selection of documentsmore thoroughly.
For each of the five largest gen-res in the collection (fairy tale, traditional legend,joke, urban legend and riddle) we sampled 10 talesand manually classified the keywords assigned tothese folktales.
A total of almost 1000 keywordswas analyzed.
Table 1 summarizes the statistics ofthis analysis.
Almost 80% of the keywords appearliterally or almost literally in the text.
The almostliteral appearances include keywords which differin quantity (plural versus singular form) and verbforms.
Verb forms vary in tense (present ratherthan past tense) and infinitive keywords of sepa-rable verbs.
An example of the latter is the as-signment of the keyword ?terugkeren?, to return,where ?keren?
(?
turn) and ?terug?
(?
back) areused in a sentence.
Of the analyzed keywords5% are synonyms of words appearing the text and2.3% are hypernyms of words appearing the text(e.g.
?wapen?, weapon, is used as a keyword with?mes?, knife, mentioned in the text).
The remain-ing 13% of the keywords represent abstract topic,event and activity descriptions.
For example, thekeyword ?wegsturen?, to send away, when one ofthe characters explicitly asks someone to leave.Other examples are the keywords ?baan?, job, and?arbeid?, labor, when the story is about an unem-ployed person.Based on these numbers we can conclude thatbased on extraction techniques alone we shouldbe able to reproduce a large portion of the manualkeyword assignment.
When thesauri are employedto find synonyms and hypernyms, up to 87% of themanually assigned keywords could be found.
Amuch harder task is to obtain the remaining 13%Classification Count Perc.Literal 669 67.6%Almost literal 120 12.1%Synonym 49 5.0%Hypernym 23 2.3%Typing error 2 0.2%Other 126 12.7%Total 989 100.0%Table 1: Keyword types in a set of 1000 folktalesof more abstract keywords, which we will study infuture research.5 Evaluating Agreement in KeywordAssignmentThe previous analyses raise the question whetherthe keywords have been consistently assigned: doannotators choose the same keywords when pre-sented with the same text?
Moreover, knowingthe difficulty of the task for human annotators willgive us an indication of the level of performancewe may expect from automatic keyword assign-ment.
To determine the agreement between an-notators we asked ten annotators to classify thevocabulary of five folktales from different genres.Frog3 (van den Bosch et al 2007) was used toextract the vocabulary of lemmas.
After carefullyreading a folktale, the annotator classified the al-phabetically sorted list of lemmas extracted fromthe text.
Each lemma was classified as either: 1)not a relevant keyword ?
should not be assignedto this document (non); 2) a relevant keyword ?should be assigned (rel); 3) a highly relevant key-word ?
should definitely be assigned (hrel).
Thethree levels of relevance were used to see whetherannotators have a preference for certain keywords.The pairwise agreement between annotators wasmeasured using Cohen?s kappa.
Each documentwas judged twice, totaling a set of 25 documents.Most of the annotators were familiar with the folk-tale database and its keywords; two were activecontributors to the database and thus had previousexperience in assigning keywords to folktales.On average, the annotators judged 79% of thevocabulary as non-relevant as keywords.
9% and12% of the vocabulary was judged as relevant andhighly relevant respectively, but there was a largevariation in these percentages: some annotatorsassigned more highly relevant keywords, othersassigned more relevant keywords.3http://ilk.uvt.nl/frog/68Cohen?s KappaClasses Average ?
Min Maxnon, rel, hrel 0.48 0.14 0.16 0.77non, rel + hrel 0.62 0.16 0.25 0.92non + rel, hrel 0.47 0.20 0.0 0.84Table 2: Classification agreement between annota-tors.
Non: non-relevant, rel: relevant, hrel: highlyrelevant.The two experienced annotators showed a con-sistently higher average agreement in comparisonto the other annotators (0.56 and 0.50 for non, rel,hrel; 0.7 and 0.64 for non, rel + hrel; 0.56 and 0.50for non + rel, hrel).
Moreover, they assigned more(relevant and highly relevant) keywords to the doc-uments on average.Table 2 summarizes the agreement measuredbetween annotators.
The first row indicates theagreement when considering agreement over allthree classes; the second row indicates the agree-ment when treating relevant and highly relevantkeywords as the same class; the last row showsthe agreement in indicating the same highly rel-evant keywords.
The numbers indicate moder-ate agreement between annotators over all threeclasses and when considering the choice of highlyrelevant keywords.
Annotators show substantialagreement on deciding between non-relevant andrelevant keywords.
Table 3 shows the agreementbetween annotators on keywords with differentparts of speech (CGN4 tagset).
Most disagree-ments are on nouns, adjectives and verbs.
Verbsand adjectives show few agreements on relevantand highly relevant keywords.
In contrast, on20% of the nouns annotators agree on their rele-vance.
It appears that the annotators do not agreewhether adjectives and verbs should be used askeywords at all.
We can give three other reasonswhy annotators did not agree.
First, for longerstories annotators were presented with long listsof candidate keywords.
Sometimes relevant key-words might have been simply overlooked.
Sec-ond, it turned out that some annotators selectedsome keywords in favor to other keywords (for in-stance a hyponym rather than a hypernym), whereothers simply annotated both as relevant.
Third,the disagreement can be explained by lack of de-tailed instructions.
The annotators were not toldhow many (highly) relevant keywords to select or4Corpus Gesproken Nederlands (Spoken Dutch Corpus),http://lands.let.kun.nl/cgn/ehome.htmwhat criteria should be met by the keywords.
Suchinstructions are not available to current annotatorsof the collection either.We conclude that annotators typically agree onthe keywords from a text, but have a varyingnotion of highly relevant keywords.
The aver-age keywords-based representation strongly con-denses the documents vocabulary: a document canbe represented by a fifth (21%) of its vocabulary5.This value can be used as a cut-off point for meth-ods ranking extracted keywords, discussed here-after.6 Automatically Extracting KeywordsIn the last part of this paper we look into automati-cally extracting keywords.
We compare a learningto rank classifier to baselines based on frequencyand reuse in their ability to reproduce keywordsfound in manually classified folktales.In all cases we use the same method for extract-ing keyword candidates.
Since most of the manualkeywords are single words (90% of the used key-word types in the collection), we simply extractsingle words as keyword candidates.
We use Frogfor tokenization and part of speech tagging.
Stopwords are not removed.6.1 Baseline SystemsWe use a basic unsupervised baseline for keywordextraction: the words are ranked according to de-scending TF-IDF.
We refer to this system as TF-IDF.
TF, term frequency, and IDF, inverse docu-ment frequency, are indicators of the term?s localand global importance and are frequently used ininformation retrieval to indicate the relative impor-tance of a word (Baeza-Yates and Ribeiro-Neto,2011).Note that a word appearing once in the collec-tion has the highest IDF score.
This would implythat the most uncommon words are also the mostimportant resulting in a bias towards spelling er-rors, proper names, and other uncommon words.Hence, our second baseline takes into accountwhether a keyword has been used before in a train-ing set.
Again, the candidates are ranked by de-scending TF-IDF, but now keywords appearing inthe training collection are ranked above the key-words not appearing in the collection.
We refer tothis baseline as TF-IDF-T.5Based on the figures that on average 9% of the vocabu-lary is judged as relevant and 12% as highly relevant69Part of speech Adjective Adverb Noun Special Numeral Prep.
VerbNumber of words 272 257 646 131 53 268 664Agreement non 70% 96% 40% 95% 81% 99% 73%rel 4% 0% 6% 0% 0% 0% 3%hrel 1% 0% 14% 2% 2% 0% 4%Disagreement non?
rel 15% 2% 17% 2% 11% 0% 12%non?
hrel 5% 1% 8% 2% 4% 1% 5%rel?
hrel 5% 0% 15% 0% 2% 0% 4%Table 3: Agreement and disagreement of annotators on keywords with different parts of speech.
Valuesare column-wise percentages.
Tags with full agreement are not shown.6.2 Learning to Rank KeywordsFollowing Jiang et al(2009) we apply a learn-ing to rank technique to rank the list of extractedkeywords.
We train an SVM to classify the rel-ative ordering of pairs of keywords.
Words cor-responding to manual keywords should be rankedhigher than other words appearing in the docu-ment.
We use SVM-rank to train a linear rankingSVM (Joachims, 2006).
We use the following fea-tures.6.2.1 Word ContextWe use the following word context features:starts uppercase: indicates whether the tokenstarts with an uppercase letter (1) or not (0).
Sinceproper names are not used as keywords in the folk-tale database, this feature is expected to be a neg-ative indicator of a word being a keyword.contains space: indicates whether the token con-tains a space (Frog extracts some Dutch multi-word phrases as a single token).
Tokens withspaces are not very common.is number: indicates whether the token consistsof only digits.
Numbers are expected not to be akeyword.contains letters: indicates whether the token con-tains at least a single letter.
Keywords are expectedto contain letters.all capital letters: indicates whether the tokenconsists of only capital letters.
Words with onlycapital letters are not expected to be keywords.single letter: indicates whether the token consistsof only one letter.
One letter keywords are veryuncommon.contains punctuation: indicates whether the to-ken contains punctuation such as apostrophes.Keywords are expected not to contain punctuation.part of speech: indicates the part of speech ofthe token (each tag is a binary feature).
Nounsare expected to be a positive indicator of key-words (Jiang et al 2009).6.2.2 Document ContextWe use the following document context features:tf: the term frequency indicates the number of ap-pearances of the word divided by the total numberof tokens in the document.first offset: indicates the offset of the word?sfirst appearance in the document, normalized bythe number of tokens in the document (follow-ing Zhang et al(2008)).
Important (key)words areexpected to be mentioned early.first sentence offset: indicates the offset of thefirst sentence in which the token appears, normal-ized by the number of sentences in the document.sentence importance: indicates the maxi-mum importance of a sentence in which theword appears, as measured by the SumBasicscore (Nenkova and Vanderwende, 2005).
Sum-Basic determines the relative importance of sen-tences solely on word probability distributions inthe text.dispersion: indicates the dispersion or scatteringof the word in the document.
Words which arehighly dispersed are expected to be more impor-tant.
The DPnorm is used as a dispersion measure,proposed in Gries (2008).6.2.3 Collection ContextWe use the following features from the collec-tion/training context:idf: the inverse document frequency indicates thecollection importance of the word based on fre-quency: frequent terms in the collection are lessimportant than rare terms in the collection.tf.idf: combines the tf and idf features by multi-plying them.
It indicates a trade-off between localand global word importance.is training keyword: indicates whether the wordis used in the training collection as a keyword.assignment ratio: indicates the percentage ofdocuments in which the term is present in the textand in which it is also assigned as a keyword.706.3 Evaluation MethodWe evaluate the ranking methods on their abilityto reproduce the manual assignment of keywords.Ideally the ranking methods rank these manualkeywords highest.
We measure the effectivenessof ranking in terms of (mean) average precision(MAP), precision at rank 5 (P@5) and precision atrank R (P@R), similar to Jiang et al(2009).
Notethat we use all the manually assigned keywordsas a ground truth, including words which do notoccur in the text itself.
This lowers the highestachievable performance, but it will give a betteridea of the performance for the real task.We perform a 10-fold stratified cross-validationwith a set of 10,900 documents from the DutchFolktale Database, all written in modern Dutch.6.4 ResultsTable 4 lists the performance of the three testedsystems.
The TF-IDF system performs worst,and is significantly outperformed by the TF-IDF-T system, which in turn is significantly outper-formed by the rank-SVM system.
On average,rank-SVM returns 3 relevant keywords in its top5.
The reported mean average precision valuesare affected by manual keywords which are notpresent in the text itself.
To put these numbersin perspective: if we would put the manual key-words which are in the text in an optimal ranking,i.e.
return these keywords first, we would achievean upper bound mean average precision of 0.5675.Taking into account the likelihood that some ofthe highly ranked false positives are relevant af-ter all (the annotator might have missed a relevantkeyword) and considering the difficulty of the task(given the variation in agreement between manualannotators), we argue that the rank-SVM performsquite well.Jiang et al(2009) reported MAPs of 0.288 and0.503 on the ranking of extracted keyphrases fromscholarly articles and tags from websites respec-tively.
Based on these numbers, we could arguethat assigning keywords to folktales is harder thanreproducing the tags of websites, and slightly eas-ier than reproducing keyphrases from scientific ar-ticles.
Because of differences in the experimentalsetup (e.g.
size of the training set, features andsystem used), it is difficult to make strong claimson the difficulty of the task.System MAP P@5 P@RTF-IDF 0.260 0.394 0.317TF-IDF-T 0.336 0.541 0.384rank-SVM 0.399 0.631 0.453Table 4: Keyword extraction effectiveness.
Thedifferences between systems are statistically sig-nificant (paired t-test, p< 0.001)Change inFeature MAP P@5 P@Rassignment ratio -0.036 -0.056 -0.038is training keyword 0.006 0.002 0.005tf.idf -0.004 -0.010 -0.002part of speech -0.003 -0.007 0.000dispersion -0.001 -0.001 0.000idf 0.001 0.002 0.000starts uppercase 0.000 0.000 -0.001first offset 0.000 0.000 0.000tf 0.000 0.000 0.000contains space 0.000 0.000 0.000is number 0.000 0.000 0.000all capital letters 0.000 0.000 0.000contains punctuation 0.000 0.000 0.000contains letters 0.000 0.000 0.000sentence importance 0.000 0.000 0.000first sentence offset 0.000 0.000 0.000single letter 0.000 0.000 0.000Table 5: Differences in performance when leavingout features.
The features are ordered by descend-ing difference in MAP.6.5 Feature AblationTo determine the added value of the individual fea-tures we carried out an ablation study.
Table 5lists the changes in performance when leaving outa particular feature (or group of features in caseof part of speech).
It turns out that many featurescan be left out without hurting the performance.All the features testing simple word characteristics(such as single letter) do not, or only marginallyinfluence the results.
Also taking into account theimportance of sentences (sentence importance), orthe first appearance of a word (first offset and firstsentence offset) does not contribute to the results.System MAP P@5 P@Rrank-SVM 0.399 0.631 0.453minimum set 0.405 0.631 0.459Table 6: Results using the full set of features andthe minimum set of features (assignment ratio,tf.idf, part of speech and dispersion).
Differencesbetween systems are statistically significant (t-test,p < 0.001).71Genre (# stories) MAP P@5 P@RTrad.
legend (3783) 0.439 0.662 0.494Joke (2793) 0.353 0.599 0.405Urban legend (1729) 0.398 0.653 0.459Riddle (1067) 0.391 0.573 0.415Fairy tale (558) 0.404 0.670 0.477Pers.
narrative (514) 0.376 0.593 0.437Legend (221) 0.409 0.622 0.478None (122) 0.366 0.602 0.421Other (113) 0.405 0.648 0.472All (10900) 0.399 0.631 0.453Table 7: SVM performance split according tostory genre.
Values in bold are significantly dif-ferent from the results on the other genres (inde-pendent t-test, p-value < 0.01)These observations suggest that almost identi-cal results can be obtained using only the featuresassignment ratio, tf.idf, part of speech and disper-sion.
The results reported in Table 6 confirm this(we do note that these results were obtained by op-timizing on the test set).6.6 Performance on Folktale GenresThe folktale database contains stories from differ-ent folktale genres, varying from legends to fairytales and jokes.
Table 7 lists the performance mea-sures per story genre.
Values in bold indicate sig-nificant differences with the stories from the othergenres combined.
The performance on traditionallegends turns out to be significantly better thanother genres: this could be explained by the factthat on average these stories are longer and there-fore contain more keywords.
Similarly, the de-crease can be explained for jokes, which are muchshorter on average.
Another explanation could bethat more abstract keywords are used to indicatethe type of joke.
Interestingly, the riddles, whichare even shorter than jokes, do not perform sig-nificantly worse than the other genres.
Personalnarratives also underperformed in comparison tothe other genres.
We cannot readily explain this,but we suspect it may have something to do withthe fact that personal narratives are more varied incontent and contain more proper names.7 Discussion and ConclusionIn this work we analyzed keywords in the contextof the Dutch Folktale Database.
In this database,on average 15 keywords have been assigned to astory, many of which are single keywords whichappear literally or almost literally in the text itself.Keyword annotators show moderate to substantialagreement in extracting the same keywords for astory.
We showed that a learning to rank methodusing features based on assignment ratio, tf.idf,part of speech and dispersion can be effectivelyused to extract and rank keyword candidates.
Webelieve that this system can be used to suggesthighly relevant keyword candidates to human an-notators to speed up the archiving process.In our evaluation we aimed to reproduce themanual annotations, but it is unclear whether bet-ter performing systems are actually more helpfulto the user.
In an ad hoc retrieval scenario, inwhich the user issues a single query and reviewsa list of retrieved documents, extracted keywordsmight be used to boost the early precision of theresults.
However, a user might not even noticea difference when a different keyword extractionsystem is used.
Moreover, the more abstract key-words which do not appear in the text might bemore important for the user experience.
In fu-ture work we want to get insight in how keywordscontribute to the end user experience.
Ideally, theevaluation should directly measure how useful thevarious keywords are for accessing the collection.In this work we considered only extracting key-words from the text we want to annotate.
Giventhe multilingual content of the database this is alimited approach: if the goal of assigning key-words is to obtain a normalized representation ofthe stories, this approach will require translationof either the source text (before extraction) or theextracted keywords.
Even in the monolingual sce-nario, the extraction of keywords is limited in deal-ing with differences in style and word use.
Writersmay use different words or use words in a differ-ent way; ideally the representation based on key-words is a normalized representation which closesthis semantic gap.
In future work we will look intoannotation with keywords from multi-lingual the-sauri combined with free-text keywords extractedfrom the text itself.
Finally, we want to look intoclassification of abstract themes and topics.AcknowledgmentsThis research was supported by the Folktales asClassifiable Texts (FACT) project, part of theCATCH programme funded by the NetherlandsOrganisation for Scientific Research (NWO).72ReferencesR Baeza-Yates and B. Ribeiro-Neto.
2011.
Modern In-formation Retrieval.
The Concepts and TechnologyBehind Search.
Addison-Wesley.M.
Bendersky and W.B.
Croft.
2008.
Discovering keyconcepts in verbose queries.
In Proceedings of SI-GIR 2008, pages 491?498.G.
Ercan and I. Cicekli.
2007.
Using lexical chainsfor keyword extraction.
Information Processing &Management, 43(6):1705?1714.E.
Frank, G.W.
Paynter, I.H.
Witten, C. Gutwin, andC.G.
Nevill-Manning.
1999.
Domain-specifickeyphrase extraction.
In Proceedings of IJCAI-99,pages 668?673.
Morgan Kaufmann Publishers Inc.,San Francisco, CA, USA.Stefan Th.
Gries.
2008.
Dispersions and adjusted fre-quencies in corpora.
International Journal of Cor-pus Linguistics, 13(4):403?437.K.
Hammouda, D. Matute, and M. Kamel.
2005.Corephrase: Keyphrase extraction for documentclustering.
Machine Learning and Data Mining inPattern Recognition, pages 265?274.A.
Hulth.
2003.
Improved automatic keyword extrac-tion given more linguistic knowledge.
In Proceed-ings of EMNLP, volume 10, pages 216?223, Morris-town, NJ, USA.
Association for Computational Lin-guistics.X.
Jiang, Y. Hu, and H. Li.
2009.
A ranking ap-proach to keyphrase extraction.
In Proceedings ofthe 32nd international ACM SIGIR conference onResearch and development in information retrieval,pages 756?757.
ACM.T.
Joachims.
2006.
Training Linear SVMs in Lin-ear Time.
In the 12th ACM SIGKDD internationalconference, pages 217?226, New York, NY, USA.ACM.H.
Li.
2011.
Learning to Rank for Information Re-trieval and Natural Language Processing.
SynthesisLectures on Human Language Technology.
Morgan& Claypool Publishers.F.
Liu, D. Pennell, F. Liu, and Y. Liu.
2009a.
Unsu-pervised approaches for automatic keyword extrac-tion using meeting transcripts.
In Proceedings ofNAACL 2009, pages 620?628.
Association for Com-putational Linguistics.Z.
Liu, P. Li, Y. Zheng, and M. Sun.
2009b.
Cluster-ing to find exemplar terms for keyphrase extraction.In Proceedings of EMNLP, pages 257?266.
Associ-ation for Computational Linguistics.O Medelyan and Ian H Witten.
2006.
Thesaurus basedautomatic keyphrase indexing.
In JCDL 2006, pages296?297.
ACM.R.
Mihalcea and P. Tarau.
2004.
Textrank: Bringingorder into texts.
In Proceedings of EMNLP, vol-ume 4, pages 404?411.
Barcelona, Spain.A.
Nenkova and L. Vanderwende.
2005.
The impact offrequency on summarization.
Microsoft Research,Redmond, Washington, Tech.
Rep. MSR-TR-2005-101.S.
Osinski and D. Weiss.
2005.
A concept-driven al-gorithm for clustering search results.
Intelligent Sys-tems, IEEE, 20(3):48?54.Y.
Park, R.J. Byrd, and B.K.
Boguraev.
2002.
Auto-matic glossary extraction: beyond terminology iden-tification.
In Proceedings of COLING 2002, pages1?7.
Association for Computational Linguistics.Christian Plaunt and Barbara A Norgard.
1998.
AnAssociation Based Method for Automatic Indexingwith a Controlled Vocabulary.
Journal of the Ameri-can Society for Information Science and Technology,49(10):888?902.S.
Rose, D. Engel, N. Cramer, and W. Cowley.
2010.Automatic keyword extraction from individual doc-uments.
In Michael W. Berry and Jacob Kogan, ed-itors, Text Mining: Applications and Theory, pages3?20.
John Wiley & Sons.P.D.
Turney.
2000.
Learning algorithms for keyphraseextraction.
Information Retrieval, 2(4):303?336.A.
van den Bosch, G.J.
Busser, W. Daelemans, andS Canisius.
2007.
An efficient memory-based mor-phosyntactic tagger and parser for Dutch.
In F. vanEynde, P. Dirix, I. Schuurman, and V. Vandeghinste,editors, Selected Papers of the 17th ComputationalLinguistics in the Netherlands Meeting, pages 99?114, Leuven, Belgium.S.
Xu, S. Yang, and F.C.M.
Lau.
2010.
Keyword ex-traction and headline generation using novel wordfeatures.
Proceedings of the Twenty-Fourth AAAIConference on Artificial Intelligence (AAAI-10).W.
Yih, J. Goodman, and V.R.
Carvalho.
2006.
Find-ing advertising keywords on web pages.
In Proceed-ings of the 15th international conference on WorldWide Web, pages 213?222.
ACM.K.
Zhang, H. Xu, J. Tang, and J. Li.
2006.
Keywordextraction using support vector machine.
Advancesin Web-Age Information Management, pages 85?96.C.
Zhang, H. Wang, Y. Liu, D. Wu, Y. Liao, andB.
Wang.
2008.
Automatic keyword extrac-tion from documents using conditional randomfields.
Journal of Computational Information Sys-tems, 4(3):1169?1180.73
