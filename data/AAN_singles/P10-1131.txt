Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1288?1297,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsPhylogenetic Grammar InductionTaylor Berg-Kirkpatrick and Dan KleinComputer Science DivisionUniversity of California, Berkeley{tberg, klein}@cs.berkeley.eduAbstractWe present an approach to multilin-gual grammar induction that exploits aphylogeny-structured model of parameterdrift.
Our method does not require anytranslated texts or token-level alignments.Instead, the phylogenetic prior coupleslanguages at a parameter level.
Joint in-duction in the multilingual model substan-tially outperforms independent learning,with larger gains both from more articu-lated phylogenies and as well as from in-creasing numbers of languages.
Acrosseight languages, the multilingual approachgives error reductions over the standardmonolingual DMV averaging 21.1% andreaching as high as 39%.1 IntroductionLearning multiple languages together should beeasier than learning them separately.
For exam-ple, in the domain of syntactic parsing, a rangeof recent work has exploited the mutual constraintbetween two languages?
parses of the same bi-text (Kuhn, 2004; Burkett and Klein, 2008; Kuz-man et al, 2009; Smith and Eisner, 2009; Sny-der et al, 2009a).
Moreover, Snyder et al (2009b)in the context of unsupervised part-of-speech in-duction (and Bouchard-Co?te?
et al (2007) in thecontext of phonology) show that extending be-yond two languages can provide increasing ben-efit.
However, multitexts are only available forlimited languages and domains.
In this work, weconsider unsupervised grammar induction withoutbitexts or multitexts.
Without translation exam-ples, multilingual constraints cannot be exploitedat the sentence token level.
Rather, we capturemultilingual constraints at a parameter level, us-ing a phylogeny-structured prior to tie together thevarious individual languages?
learning problems.Our joint, hierarchical prior couples model param-eters for different languages in a way that respectsknowledge about how the languages evolved.Aspects of this work are closely related to Co-hen and Smith (2009) and Bouchard-Co?te?
et al(2007).
Cohen and Smith (2009) present a modelfor jointly learning English and Chinese depen-dency grammars without bitexts.
In their work,structurally constrained covariance in a logisticnormal prior is used to couple parameters betweenthe two languages.
Our work, though also differ-ent in technical approach, differs most centrally inthe extension to multiple languages and the use ofa phylogeny.
Bouchard-Co?te?
et al (2007) consid-ers an entirely different problem, phonological re-construction, but shares with this work both theuse of a phylogenetic structure as well as the useof log-linear parameterization of local model com-ponents.
Our work differs from theirs primarilyin the task (syntax vs. phonology) and the vari-ables governed by the phylogeny: in our model itis the grammar parameters that drift (in the prior)rather than individual word forms (in the likeli-hood model).Specifically, we consider dependency inductionin the DMV model of Klein and Manning (2004).Our data is a collection of standard dependencydata sets in eight languages: English, Dutch, Dan-ish, Swedish, Spanish, Portuguese, Slovene, andChinese.
Our focus is not the DMV model itself,which is well-studied, but rather the prior whichcouples the various languages?
parameters.
Whilesome choices of prior structure can greatly com-plicate inference (Cohen and Smith, 2009), wechoose a hierarchical Gaussian form for the driftterm, which allows the gradient of the observeddata likelihood to be easily computed using stan-dard dynamic programming methods.In our experiments, joint multilingual learningsubstantially outperforms independent monolin-gual learning.
Using a limited phylogeny that1288only couples languages within linguistic familiesreduces error by 5.6% over the monolingual base-line.
Using a flat, global phylogeny gives a greaterreduction, almost 10%.
Finally, a more articu-lated phylogeny that captures both inter- and intra-family effects gives an even larger average relativeerror reduction of 21.1%.2 ModelWe define our model over two kinds of randomvariables: dependency trees and parameters.
Foreach language ?
in a set L, our model will generatea collection t?
of dependency trees ti?.
We assumethat these dependency trees are generated by theDMV model of Klein and Manning (2004), whichwe write as ti?
?
DMV(??).
Here, ??
is a vectorof the various model parameters for language ?.The prior is what couples the ??
parameter vectorsacross languages; it is the focus of this work.
Wefirst consider the likelihood model before movingon to the prior.2.1 Dependency Model with ValenceA dependency parse is a directed tree t over tokensin a sentence s. Each edge of the tree specifies adirected dependency from a head token to a de-pendent, or argument token.
The DMV is a gen-erative model for trees t, which has been widelyused for dependency parse induction.
The ob-served data likelihood, used for parameter estima-tion, is the marginal probability of generating theobserved sentences s, which are simply the leavesof the trees t. Generation in the DMV model in-volves two types of local conditional probabilities:CONTINUE distributions that capture valence andATTACH distributions that capture argument selec-tion.First, the Bernoulli CONTINUE probability dis-tributions P CONTINUE(c|h, dir, adj; ??)
model thefertility of a particular head type h. The outcomec ?
{stop, continue} is conditioned on the headtype h, direction dir, and adjacency adj.
If a headtype?s continue probability is low, tokens of thistype will tend to generate few arguments.Second, the ATTACH multinomial probabilitydistributions P ATTACH(a|h, dir; ??)
capture attach-ment preferences of heads, where a and h are bothtoken types.
We take the same approach as pre-vious work (Klein and Manning, 2004; Cohen andSmith, 2009) and use gold part-of-speech labels astokens.
Thus, the basic observed ?word?
types areEnglish Dutch SwedishDanish Spanish Portuguese Slovene ChineseGlobalIndo-EuropeanGermanicWestGermanicNorthGermanicIbero-RomanceItalic Balto-SlavicSlavicSino-TibetanSiniticFigure 1: An example of a linguistically-plausible phylo-genetic tree over the languages in our training data.
Leavescorrespond to (observed) modern languages, while internalnodes represent (unobserved) ancestral languages.actually word classes.2.1.1 Log-Linear ParameterizationThe DMV?s local conditional distributions wereoriginally given as simple multinomial distribu-tions with one parameter per outcome.
However,they can be re-parameterized to give the followinglog-linear form (Eisner, 2002; Bouchard-Co?te?
etal., 2007; Berg-Kirkpatrick et al, 2010):P CONTINUE(c|h, dir, adj; ??)
=exp??
?T f CONTINUE(c, h, dir, adj)?Pc?
exp??
?T f CONTINUE(c?, h, dir, adj)?P ATTACH(a|h, dir; ??)
=exp??
?T f ATTACH(a, h, dir)?Pa?
exp??
?T f ATTACH(a?, h, dir)?The parameters are weights ??
with one weightvector per language.
In the case where the vec-tor of feature functions f has an indicator for eachpossible conjunction of outcome and conditions,the original multinomial distributions are recov-ered.
We refer to these full indicator features asthe set of SPECIFIC features.2.2 Phylogenetic PriorThe focus of this work is coupling each of the pa-rameters ??
in a phylogeny-structured prior.
Con-sider a phylogeny like the one shown in Fig-ure 1, where each modern language ?
in L is aleaf.
We would like to say that the leaves?
pa-rameter vectors arise from a process which slowly1289drifts along each branch.
A convenient choice isto posit additional parameter variables ?
?+ at in-ternal nodes ?+ ?
L+, a set of ancestral lan-guages, and to assume that the conditional dis-tribution P (??|?par(?))
at each branch in the phy-logeny is a Gaussian centered on ?par(?
), wherepar(?)
is the parent of ?
in the phylogeny and?
ranges over L ?
L+.
The variance structureof the Gaussian would then determine how muchdrift (and in what directions) is expected.
Con-cretely, we assume that each drift distribution isan isotropic Gaussian with mean ?par(?)
and scalarvariance ?2.
The root is centered at zero.
We havethus defined a joint distribution P (?|?2) where?
= (??
: ?
?
L?L+).
?2 is a hyperparameter forthis prior which could itself be re-parameterized todepend on branch length or be learned; we simplyset it to a plausible constant value.Two primary challenges remain.
First, infer-ence under arbitrary priors can become complex.However, in the simple case of our diagonal co-variance Gaussians, the gradient of the observeddata likelihood can be computed directly using theDMV?s expected counts and maximum-likelihoodestimation can be accomplished by applying stan-dard gradient optimization methods.
Second,while the choice of diagonal covariance is effi-cient, it causes components of ?
that correspondto features occurring in only one language to bemarginally independent of the parameters of allother languages.
In other words, only featureswhich fire in more than one language are coupledby the prior.
In the next section, we therefore in-crease the overlap between languages?
features byusing coarse projections of parts-of-speech.2.3 Projected FeaturesWith diagonal covariance in the Gaussian driftterms, each parameter evolves independently ofthe others.
Therefore, our prior will be mostinformative when features activate in multiplelanguages.
In phonology, it is useful to mapphonemes to the International Phonetic Alphabet(IPA) in order to have a language-independentparameterization.
We introduce a similarly neu-tral representation here by projecting language-specific parts-of-speech to a coarse, shared inven-tory.Indeed, we assume that each language has a dis-tinct tagset, and so the basic configurational fea-tures will be language specific.
For example, whenSPECIFIC: Activate for only one conjunction of out-come and conditions:1(c = ?, h = ?, dir = ?, adj = ?
)SHARED: Activate for heads from multiple languagesusing cross-lingual POS projection pi(?
):1(c = ?, pi(h) = ?, dir = ?, adj = ?
)CONTINUE distribution feature templates.SPECIFIC: Activate for only one conjunction of out-come and conditions:1(a = ?, h = ?, dir = ?
)SHARED: Activate for heads and arguments frommultiple languages using cross-lingualPOS projection pi(?
):1(pi(a) = ?, pi(h) = ?, dir = ?
)1(pi(a) = ?, h = ?, dir = ?
)1(a = ?, pi(h) = ?, dir = ?
)ATTACH distribution feature templates.Table 1: Feature templates for CONTINUE and ATTACH con-ditional distributions.an English VBZ takes a left argument headed by aNNS, a feature will activate specific to VBZ-NNS-LEFT.
That feature will be used in the log-linearattachment probability for English.
However, be-cause that feature does not show up in any otherlanguage, it is not usefully controlled by the prior.Therefore, we also include coarser features whichactivate on more abstract, cross-linguistic config-urations.
In the same example, a feature will fireindicating a coarse, direction-free NOUN-VERB at-tachment.
This feature will now occur in multiplelanguages and will contribute to each of those lan-guages?
attachment models.
Although such cross-lingual features will have different weight param-eters in each language, those weights will covary,being correlated by the prior.The coarse features are defined via a projec-tion ?
from language-specific part-of-speech la-bels to coarser, cross-lingual word classes, andhence we refer to them as SHARED features.
Foreach corpus used in this paper, we use the taggingannotation guidelines to manually define a fixedmapping from the corpus tagset to the followingcoarse tagset: noun, verb, adjective, adverb, con-junction, preposition, determiner, interjection, nu-meral, and pronoun.
Parts-of-speech for whichthis coarse mapping is ambiguous or impossibleare not mapped, and do not have correspondingSHARED features.We summarize the feature templates for theCONTINUE and ATTACH conditional distributionsin Table 1.
Variants of all feature templates thatignore direction and/or adjacency are included.
Inpractice, we found it beneficial for all language-1290independent features to ignore direction.Again, only the coarse features occur in mul-tiple languages, so all phylogenetic influence isthrough those.
Nonetheless, the effect of the phy-logeny turns out to be quite strong.2.4 LearningWe now turn to learning with the phylogeneticprior.
Since the prior couples parameters acrosslanguages, this learning problem requires param-eters for all languages be estimated jointly.
Weseek to find ?
= (??
: ?
?
L ?
L+) whichoptimizes log P (?|s), where s aggregates the ob-served leaves of all the dependency trees in all thelanguages.
This can be written aslog P (?)
+ logP (s|?)
?
log P (s)The third term is a constant and can be ignored.The first term can be written aslogP (?)
=??
?L?L+12?2 ???
?
?par(?
)?22 + Cwhere C is a constant.
The form of logP (?)
im-mediately shows how parameters are penalized forbeing different across languages, more so for lan-guages that are near each other in the phylogeny.The second termlog P (s|?)
=??
?Llog P (s?|??
)is a sum of observed data likelihoods underthe standard DMV models for each language,computable by dynamic programming (Kleinand Manning, 2004).
Together, this yields thefollowing objective function:l(?)
=??
?L?L+12?2 ???
?
?par(?
)?22 +??
?L logP (s?|??
)which can be optimized using gradient methodsor (MAP) EM.
Here we used L-BFGS (Liu et al,1989).
This requires computation of the gradientof the observed data likelihood log P (s?|??
)which is given by:?
logP (s?|??)
= Et?|s?[?
log P (s?, t?|??)]=??????????????????
?c,h,dir,adj ec,h,dir,adj(s?
; ??)
?
[f CONTINUE(c, h, dir, adj) ??c?
P CONTINUE(c?|h, dir, adj; ??
)f CONTINUE(c?, h, dir, adj)]?a,h,dir ea,h,dir(s?
; ??)
?
[f ATTACH(a, h, dir) ??a?
P ATTACH(a?|h, dir; ??
)f ATTACH(a?, h, dir)]?????????????????
?The expected gradient of the log joint likelihoodof sentences and parses is equal to the gradient ofthe log marginal likelihood of just sentences, orthe observed data likelihood (Salakhutdinov et al,2003).
ea,h,dir(s?
; ??)
is the expected count of thenumber of times head h is attached to a in direc-tion dir given the observed sentences s?
and DMVparameters ??.
ec,h,dir,adj(s?
; ??)
is defined simi-larly.
Note that these are the same expected countsrequired to perform EM on the DMV, and are com-putable by dynamic programming.The computation time is dominated by the com-putation of each sentence?s posterior expectedcounts, which are independent given the parame-ters, so the time required per iteration is essentiallythe same whether training all languages jointly orindependently.
In practice, the total number of it-erations was also similar.3 Experimental Setup3.1 DataWe ran experiments with the following languages:English, Dutch, Danish, Swedish, Spanish, Por-tuguese, Slovene, and Chinese.
For all languagesbut English and Chinese, we used corpora from the2006 CoNLL-X Shared Task dependency parsingdata set (Buchholz and Marsi, 2006).
We used theshared task training set to both train and test ourmodels.
These corpora provide hand-labeled part-of-speech tags (except for Dutch, which is auto-matically tagged) and provide dependency parses,which are either themselves hand-labeled or havebeen converted from hand-labeled parses of otherkinds.
For English and Chinese we use sections2-21 of the Penn Treebank (PTB) (Marcus et al,1993) and sections 1-270 of the Chinese Tree-bank (CTB) (Xue et al, 2002) respectively.
Sim-ilarly, these sections were used for both trainingand testing.
The English and Chinese data setshave hand-labeled constituency parses and part-of-speech tags, but no dependency parses.
We usedthe Bikel Chinese head finder (Bikel and Chiang,2000) and the Collins English head finder (Collins,1999) to transform the gold constituency parsesinto gold dependency parses.
None of the corporaare bitexts.
For all languages, we ran experimentson all sentences of length 10 or less after punctua-tion has been removed.When constructing phylogenies over the lan-guages we made use of their linguistic classifica-tions.
English and Dutch are part of the West Ger-1291English Dutch SwedishDanish Spanish Portuguese Slovene ChineseWestGermanicNorthGermanicIbero-Romance Slavic SiniticGlobalEnglish Dutch SwedishDanish Spanish Portuguese Slovene ChineseGlobal(a)(b)(c)English Dutch SwedishDanish Spanish Portuguese Slovene ChineseWestGermanicNorthGermanicIbero-Romance Slavic SiniticFigure 2: (a) Phylogeny for FAMILIES model.
(b) Phylogenyfor GLOBAL model.
(c) Phylogeny for LINGUISTIC model.manic family of languages, whereas Danish andSwedish are part of the North Germanic family.Spanish and Portuguese are both part of the Ibero-Romance family.
Slovene is part of the Slavicfamily.
Finally, Chinese is in the Sinitic family,and is not an Indo-European language like the oth-ers.
We interchangeably speak of a language fam-ily and the ancestral node corresponding to thatfamily?s root language in a phylogeny.3.2 Models ComparedWe evaluated three phylogenetic priors, each witha different phylogenetic structure.
We comparewith two monolingual baselines, as well as an all-pairs multilingual model that does not have a phy-logenetic interpretation, but which provides verysimilar capacity for parameter coupling.3.2.1 Phylogenetic ModelsThe first phylogenetic model uses the shallow phy-logeny shown in Figure 2(a), in which only lan-guages within the same family have a shared par-ent node.
We refer to this structure as FAMILIES.Under this prior, the learning task decouples intoindependent subtasks for each family, but no reg-ularities across families can be captured.The family-level model misses the constraintsbetween distant languages.
Figure 2(b) shows an-other simple configuration, wherein all languagesshare a common parent node in the prior, meaningthat global regularities that are consistent acrossall languages can be captured.
We refer to thisstructure as GLOBAL.While the global model couples the parametersfor all eight languages, it does so without sensi-tivity to the articulated structure of their descent.Figure 2(c) shows a more nuanced prior struc-ture, LINGUISTIC, which groups languages firstby family and then under a global node.
Thisstructure allows global regularities as well as reg-ularities within families to be learned.3.2.2 Parameterization and ALLPAIRS ModelDaume?
III (2007) and Finkel and Manning (2009)consider a formally similar Gaussian hierarchy fordomain adaptation.
As pointed out in Finkel andManning (2009), there is a simple equivalence be-tween hierarchical regularization as described hereand the addition of new tied features in a ?flat?model with zero-meaned Gaussian regularizationon all parameters.
In particular, instead of param-eterizing the objective in Section 2.4 in terms ofmultiple sets of weights, one at each node in thephylogeny (the hierarchical parameterization, de-scribed in Section 2.4), it is equivalent to param-eterize this same objective in terms of a single setof weights on a larger of group features (the flatparameterization).
This larger group of featurescontains a duplicate set of the features discussed inSection 2.3 for each node in the phylogeny, eachof which is active only on the languages that are itsdescendants.
A linear transformation between pa-rameterizations gives equivalence.
See Finkel andManning (2009) for details.In the flat parameterization, it seems equallyreasonable to simply tie all pairs of languages byadding duplicate sets of features for each pair.This gives the ALLPAIRS setting, which we alsocompare to the tree-structured phylogenetic mod-els above.3.3 BaselinesTo evaluate the impact of multilingual constraint,we compared against two monolingual baselines.The first baseline is the standard DMV withonly SPECIFIC features, which yields the standardmultinomial DMV (weak baseline).
To facilitatecomparison to past work, we used no prior for thismonolingual model.
The second baseline is theDMV with added SHARED features.
This modelincludes a simple isotropic Gaussian prior on pa-1292Monolingual MultilingualPhylogeneticCorpus Size BaselineBaselinew/SHAREDALLPAIRSFAMILIESBESTPAIRGLOBALLINGUISTICWest Germanic English 6008 47.1 51.3 48.5 51.3 51.3 (Ch) 51.2 62.3Dutch 6678 36.3 36.0 44.0 36.1 36.2 (Sw) 44.0 45.1North Germanic Danish 1870 33.5 33.6 40.5 31.4 34.2 (Du) 39.6 41.6Swedish 3571 45.3 44.8 56.3 44.8 44.8 (Ch) 44.5 58.3Ibero-Romance Spanish 712 28.0 40.5 58.7 63.4 63.8 (Da) 59.4 58.4Portuguese 2515 38.5 38.5 63.1 37.4 38.4 (Sw) 37.4 63.0Slavic Slovene 627 38.5 39.7 49.0 ?
49.6 (En) 49.4 48.4Sinitic Chinese 959 36.3 43.3 50.7 ?
49.7 (Sw) 50.1 49.6Macro-Avg.
Relative Error Reduction 17.1 5.6 8.5 9.9 21.1Table 2: Directed dependency accuracy of monolingual and multilingual models, and relative error reduction over the monolin-gual baseline with SHARED features macro-averaged over languages.
Multilingual models outperformed monolingual modelsin general, with larger gains from increasing numbers of languages.
Additionally, more nuanced phylogenetic structures out-performed cruder ones.rameters.
This second baseline is the more directcomparison to the multilingual experiments here(strong baseline).3.4 EvaluationFor each setting, we evaluated the directed de-pendency accuracy of the minimum Bayes risk(MBR) dependency parses produced by our mod-els under maximum (posterior) likelihood parame-ter estimates.
We computed accuracies separatelyfor each language in each condition.
In addition,for multilingual models, we computed the relativeerror reduction over the strong monolingual base-line, macro-averaged over languages.3.5 TrainingOur implementation used the flat parameteriza-tion described in Section 3.2.2 for both the phy-logenetic and ALLPAIRS models.
We originallydid this in order to facilitate comparison with thenon-phylogenetic ALLPAIRS model, which has noequivalent hierarchical parameterization.
In prac-tice, optimizing with the hierarchical parameteri-zation also seemed to underperform.11We noticed that the weights of features shared across lan-guages had larger magnitude early in the optimization proce-dure when using the flat parameterization compared to us-ing the hierarchical parameterization, perhaps indicating thatcross-lingual influences had a larger effect on learning in itsinitial stages.All models were trained by directly optimizingthe observed data likelihood using L-BFGS (Liu etal., 1989).
Berg-Kirkpatrick et al (2010) suggestthat directly optimizing the observed data likeli-hood may offer improvements over the more stan-dard expectation-maximization (EM) optimizationprocedure for models such as the DMV, espe-cially when the model is parameterized using fea-tures.
We stopped training after 200 iterations inall cases.
This fixed stopping criterion seemed tobe adequate in all experiments, but presumablythere is a potential gain to be had in fine tuning.To initialize, we used the harmonic initializer pre-sented in Klein and Manning (2004).
This type ofinitialization is deterministic, and thus we did notperform random restarts.We found that for all models ?2 = 0.2 gave rea-sonable results, and we used this setting in all ex-periments.
For most models, we found that vary-ing ?2 in a reasonable range did not substantiallyaffect accuracy.
For some models, the directed ac-curacy was less flat with respect to ?2.
In theseless-stable cases, there seemed to be an interac-tion between the variance and the choice betweenhead conventions.
For example, for some settingsof ?2, but not others, the model would learn thatdeterminers head noun phrases.
In particular, weobserved that even when direct accuracy did fluc-tuate, undirected accuracy remained more stable.12934 ResultsTable 2 shows the overall results.
In all cases,methods which coupled the languages in someway outperformed the independent baselines thatconsidered each language independently.4.1 Bilingual ModelsThe weakest of the coupled models was FAMI-LIES, which had an average relative error reduc-tion of 5.6% over the strong baseline.
In this case,most of the average improvement came from a sin-gle family: Spanish and Portuguese.
The limitedimprovement of the family-level prior comparedto other phylogenies suggests that there are impor-tant multilingual interactions that do not happenwithin families.
Table 2 also reports the maximumaccuracy achieved for each language when it waspaired with another language (same family or oth-erwise) and trained together with a single commonparent.
These results appear in the column headedby BESTPAIR, and show the best accuracy for thelanguage on that row over all possible pairingswith other languages.
When pairs of languageswere trained together in isolation, the largest bene-fit was seen for languages with small training cor-pora, not necessarily languages with common an-cestry.
In our setup, Spanish, Slovene, and Chi-nese have substantially smaller training corporathan the rest of the languages considered.
Other-wise, the patterns are not particularly clear; com-bined with subsequent results, it seems that pair-wise constraint is fairly limited.4.2 Multilingual ModelsModels that coupled multiple languages per-formed better in general than models that onlyconsidered pairs of languages.
The GLOBALmodel, which couples all languages, if crudely,yielded an average relative error reduction of9.9%.
This improvement comes as the numberof languages able to exert mutual constraint in-creases.
For example, Dutch and Danish had largeimprovements, over and above any improvementsthese two languages gained when trained with asingle additional language.
Beyond the simplisticGLOBAL phylogeny, the more nuanced LINGUIS-TIC model gave large improvements for English,Swedish, and Portuguese.
Indeed, the LINGUIS-TIC model is the only model we evaluated thatgave improvements for all the languages we con-sidered.It is reasonable to worry that the improvementsfrom these multilingual models might be partiallydue to having more total training data in the mul-tilingual setting.
However, we found that halv-ing the amount of data used to train the English,Dutch, and Swedish (the languages with the mosttraining data) monolingual models did not sub-stantially affect their performance, suggesting thatfor languages with several thousand sentences ormore, the increase in statistical support due to ad-ditional monolingual data was not an important ef-fect (the DMV is a relatively low-capacity modelin any case).4.3 Comparison of PhylogeniesRecall the structures of the three phylogeniespresented in Figure 2.
These phylogenies dif-fer in the correlations they can represent.
TheGLOBAL phylogeny captures only ?universals,?while FAMILIES captures only correlations be-tween languages that are known to be similar.
TheLINGUISTIC model captures both of these effectssimultaneously by using a two layer hierarchy.Notably, the improvement due to the LINGUISTICmodel is more than the sum of the improvementsdue to the GLOBAL and FAMILIES models.4.4 Phylogenetic vs. ALLPAIRSThe phylogeny is capable of allowing appropri-ate influence to pass between languages at mul-tiple levels.
We compare these results to theALLPAIRS model in order to see whether limi-tation to a tree structure is helpful.
The ALL-PAIRS model achieved an average relative errorreduction of 17.1%, certainly outperforming boththe simple phylogenetic models.
However, therich phylogeny of the LINGUISTIC model, whichincorporates linguistic constraints, outperformedthe freer ALLPAIRS model.
A large portion ofthis improvement came from English, a languagefor which the LINGUISTIC model greatly outper-formed all other models evaluated.
We found thatthe improved English analyses produced by theLINGUISTIC model were more consistent with thismodel?s analyses of other languages.
This consis-tency was not present for the English analyses pro-duced by other models.
We explore consistency inmore detail in Section 5.4.5 Comparison to Related WorkThe likelihood models for both the strong mono-lingual baseline and the various multilingual mod-1294els are the same, both expanding upon the standardDMV by adding coarse SHARED features.
Thesecoarse features, even in a monolingual setting, im-proved performance slightly over the weak base-line, perhaps by encouraging consistent treatmentof the different finer-grained variants of parts-of-speech (Berg-Kirkpatrick et al, 2010).2 Theonly difference between the multilingual systemsand the strong baseline is whether or not cross-language influence is allowed through the prior.While this progression of model structure issimilar to that explored in Cohen and Smith(2009), Cohen and Smith saw their largest im-provements from tying together parameters for thevarieties of coarse parts-of-speech monolinugally,and then only moderate improvements from allow-ing cross-linguistic influence on top of monolin-gual sharing.
When Cohen and Smith comparedtheir best shared logistic-normal bilingual mod-els to monolingual counter-parts for the languagesthey investigate (Chinese and English), they re-ported a relative error reduction of 5.3%.
In com-parison, with the LINGUISTIC model, we saw amuch larger 16.9% relative error reduction overour strong baseline for these languages.
Evaluat-ing our LINGUISTIC model on the same test setsas (Cohen and Smith, 2009), sentences of length10 or less in section 23 of PTB and sections 271-300 of CTB, we achieved an accuracy of 56.6 forChinese and 60.3 for English.
The best modelsof Cohen and Smith (2009) achieved accuracies of52.0 and 62.0 respectively on these same test sets.Our results indicate that the majority of ourmodel?s power beyond that of the standard DMVis derived from multilingual, and in particular,more-than-bilingual, interaction.
These are, to thebest of our knowledge, the first results of this kindfor grammar induction without bitext.5 AnalysisBy examining the proposed parses we found thatthe LINGUISTIC and ALLPAIRS models producedanalyses that were more consistent across lan-guages than those of the other models.
Wealso observed that the most common errors canbe summarized succinctly by looking at attach-ment counts between coarse parts-of-speech.
Fig-ure 3 shows matrix representations of dependency2Coarse features that only tie nouns and verbs are ex-plored in Berg-Kirkpatrick et al (2010).
We found that thesewere very effective for English and Chinese, but gave worseperformance for other languages.counts.
The area of a square is proportional to thenumber of order-collapsed dependencies wherethe column label is the head and the row label isthe argument in the parses from each system.
Forease of comprehension, we use the cross-lingualprojections and only show counts for selected in-teresting classes.Comparing Figure 3(c), which shows depen-dency counts proposed by the LINGUISTIC model,to Figure 3(a), which shows the same for thestrong monolingual baseline, suggests that theanalyses proposed by the LINGUISTIC model aremore consistent across languages than are theanalyses proposed by the monolingual model.
Forexample, the monolingual learners are dividedas to whether determiners or nouns head nounphrases.
There is also confusion about which la-bels head whole sentences.
Dutch has the problemthat verbs modify pronouns more often than pro-nouns modify verbs, and pronouns are predictedto head sentences as often as verbs are.
Span-ish has some confusion about conjunctions, hy-pothesizing that verbs often attach to conjunctions,and conjunctions frequently head sentences.
Moresubtly, the monolingual analyses are inconsistentin the way they head prepositional phrases.
Inthe monolingual Portuguese hypotheses, preposi-tions modify nouns more often than nouns mod-ify prepositions.
In English, nouns modify prepo-sitions, and prepositions modify verbs.
Both theDutch and Spanish models are ambivalent aboutthe attachment of prepositions.As has often been observed in other contexts(Liang et al, 2008), promoting agreement canimprove accuracy in unsupervised learning.
Notonly are the analyses proposed by the LINGUISTICmodel more consistent, they are also more in ac-cordance with the gold analyses.
Under the LIN-GUISTIC model, Dutch now attaches pronouns toverbs, and thus looks more like English, its sisterin the phylogenetic tree.
The LINGUISTIC modelhas also chosen consistent analyses for preposi-tional phrases and noun phrases, calling preposi-tions and nouns the heads of each, respectively.The problem of conjunctions heading Spanish sen-tences has also been corrected.Figure 3(b) shows dependency counts for theGLOBAL multilingual model.
Unsurprisingly, theanalyses proposed under global constraint appearsomewhat more consistent than those proposedunder no multi-lingual constraint (now three lan-1295Figure 3: Dependency counts in proposed parses.
Row label modifies column label.
(a) Monolingual baseline with SHAREDfeatures.
(b) GLOBAL model.
(c) LINGUISTIC model.
(d) Dependency counts in hand-labeled parses.
Analyses proposed bymonolingual baseline show significant inconsistencies across languages.
Analyses proposed by LINGUISTIC model are moreconsistent across languages than those proposed by either the monolingual baseline or the GLOBAL model.guages agree that prepositional phrases are headedby prepositions), but not as consistent as those pro-posed by the LINGUISTIC model.Finally, Figure 3(d) shows dependency countsin the hand-labeled dependency parses.
It appearsthat even the very consistent LINGUISTIC parsesdo not capture the non-determinism of preposi-tional phrase attachment to both nouns and verbs.6 ConclusionEven without translated texts, multilingual con-straints expressed in the form of a phylogeneticprior on parameters can give substantial gainsin grammar induction accuracy over treating lan-guages in isolation.
Additionally, articulated phy-logenies that are sensitive to evolutionary structurecan outperform not only limited flatter priors butalso unconstrained all-pairs interactions.7 AcknowledgementsThis project is funded in part by the NSF un-der grant 0915265 and DARPA under grantN10AP20007.1296ReferencesT.
Berg-Kirkpatrick, A.
Bouchard-Co?te?, J. DeNero,and D. Klein.
2010.
Painless unsupervised learn-ing with features.
In North American Chapter of theAssociation for Computational Linguistics.D.
M. Bikel and D. Chiang.
2000.
Two statistical pars-ing models applied to the Chinese treebank.
In Sec-ond Chinese Language Processing Workshop.A.
Bouchard-Co?te?, P. Liang, D. Klein, and T. L. Grif-fiths.
2007.
A probabilistic approach to diachronicphonology.
In Empirical Methods in Natural Lan-guage Processing.S.
Buchholz and E. Marsi.
2006.
Computational Nat-ural Language Learning-X shared task on multilin-gual dependency parsing.
In Conference on Compu-tational Natural Language Learning.D.
Burkett and D. Klein.
2008.
Two languages arebetter than one (for syntactic parsing).
In EmpiricalMethods in Natural Language Processing.S.
B. Cohen and N. A. Smith.
2009.
Shared logisticnormal distributions for soft parameter tying in un-supervised grammar induction.
In North AmericanChapter of the Association for Computational Lin-guistics.M.
Collins.
1999.
Head-driven statistical models fornatural language parsing.
In Ph.D. thesis, Universityof Pennsylvania, Philadelphia.H.
Daume?
III.
2007.
Frustratingly easy domain adap-tation.
In Association for Computational Linguis-tics.J.
Eisner.
2002.
Parameter estimation for probabilisticfinite-state transducers.
In Association for Compu-tational Linguistics.J.
R. Finkel and C. D. Manning.
2009.
Hierarchi-cal bayesian domain adaptation.
In North AmericanChapter of the Association for Computational Lin-guistics.D.
Klein and C. D. Manning.
2004.
Corpus-basedinduction of syntactic structure: Models of depen-dency and constituency.
In Association for Compu-tational Linguistics.J.
Kuhn.
2004.
Experiments in parallel-text basedgrammar induction.
In Association for Computa-tional Linguistics.G.
Kuzman, J. Gillenwater, and B. Taskar.
2009.
De-pendency grammar induction via bitext projectionconstraints.
In Association for Computational Lin-guistics/International Joint Conference on NaturalLanguage Processing.P.
Liang, D. Klein, and M. I. Jordan.
2008.Agreement-based learning.
In Advances in NeuralInformation Processing Systems.D.
C. Liu, J. Nocedal, and C. Dong.
1989.
On thelimited memory BFGS method for large scale opti-mization.
Mathematical Programming.M.
P. Marcus, M. A. Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of English:the penn treebank.
Computational Linguistics.R.
Salakhutdinov, S. Roweis, and Z. Ghahramani.2003.
Optimization with EM and expectation-conjugate-gradient.
In International Conference onMachine Learning.D.
A. Smith and J. Eisner.
2009.
Parser adapta-tion and projection with quasi-synchronous gram-mar features.
In Empirical Methods in Natural Lan-guage Processing.B.
Snyder, T. Naseem, and R. Barzilay.
2009a.
Unsu-pervised multilingual grammar induction.
In Asso-ciation for Computational Linguistics/InternationalJoint Conference on Natural Language Processing.B.
Snyder, T. Naseem, J. Eisenstein, and R. Barzi-lay.
2009b.
Adding more languages improves un-supervised multilingual part-of-speech tagging: ABayesian non-parametric approach.
In North Amer-ican Chapter of the Association for ComputationalLinguistics.N.
Xue, F-D Chiou, and M. Palmer.
2002.
Buildinga large-scale annotated Chinese corpus.
In Interna-tional Conference on Computational Linguistics.1297
