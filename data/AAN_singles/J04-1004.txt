c?
2004 Association for Computational LinguisticsAccessor Variety Criteria for ChineseWord ExtractionHaodi Feng?
Kang Chen?Shandong University Tsinghua UniversityCity University of Hong KongXiaotie Deng?
Weimin Zheng?City University of Hong Kong Tsinghua UniversityWe are interested in the problem of word extraction from Chinese text collections.
We define aword to be a meaningful string composed of several Chinese characters.
For example, ,?percent?, and , ?more and more?, are not recognized as traditional Chinese words from theviewpoint of some people.
However, in our work, they are words because they are very widely usedand have specific meanings.
We start with the viewpoint that a word is a distinguished linguisticentity that can be used in many different language environments.
We consider the charactersthat are directly before a string (predecessors) and the characters that are directly after a string(successors) as important factors for determining the independence of the string.
We call suchcharacters accessors of the string, consider the number of distinct predecessors and successors of astring in a large corpus (TREC 5 and TREC 6 documents), and use them as the measurement of thecontext independency of a string from the rest of the sentences in the document.
Our experimentsconfirm our hypothesis and show that this simple rule gives quite good results for Chinese wordextraction and is comparable to, and for long words outperforms, other iterative methods.1.
IntroductionWords are the basic linguistic units of natural language processing.
The importanceof word extraction is stressed in many papers.
According to Huang, Chen, and Tsou(1996), the word is the basic unit in natural language processing (NLP), as it is at thelexical level where all modules interface.
Possible modules involved are the lexicon,speech recognition, syntactic parsing, speech synthesis, semantic interpretation, andso on.
Thus, the identification of lexical words and/or the delimitation of words inrunning texts is a prerequisite of NLP.
Teahan et al (2000) state that interpreting a textas a sequence of words is beneficial for some information retrieval and storage tasks:for example, full-text searches, word-based compression, and key-phrase extraction.According to Guo (1997), words and tokens are the primary building blocks in almostall linguistic theories and language-processing systems, including Japanese (Kobayasi,Tokumaga, and Tanaka 1994), Korean (Yun, Lee, and Rim 1995), German (Pachunkeet al 1992), and English (Garside, Leech, and Sampson 1987), in various media, such?
School of Computer Science and Technology, Jinan, PRC; Department of Computer Science, Tat CheeAvenue, Kowloon, Hong Kong.
E-mail: fenghd@cs.cityu.edu.hk or fenghaodi@hotmail.com.?
Department of Computer Science and Technology, Peking, PR China.
E-mail: {ck99,zwm-dcs}@mails.tsinghua.edu.cn.?
Department of Computer Science, Tat Chee Avenue, Kowloon, Hong Kong.
E-mail: csdeng@cityu.edu.hk.76Computational Linguistics Volume 30, Number 1as continuous speech and cursive handwriting, and in numerous applications, suchas translation, recognition, indexing, and proofreading.
The identification of words innatural language is nontrivial since, as observed by Chao (1968), linguistic words oftenrepresent a different set than do sociological words.Chinese texts are character based, not word based.
Each Chinese character standsfor one phonological syllable and in most cases represents a morpheme.
This presentsa problem, as only less than 10% of the word types (and less than 50% of the tokensin a text) in Chinese are composed of a single character (Chen et al 1993).
However,Chinese texts, and texts in some other Oriental languages such as Japanese, do nothave delimiters such as spaces to mark the boundaries of meaningful words.
Even forEnglish text, some phrases consist of several words.
However, the problem in Englishis not as dominant a factor as in Chinese.
How to extract words from Chinese texts isstill an interesting problem.
Note that word extraction is different from the very closelyrelated problem of sentence segmentation.
Word extraction aims to collect all of themeaningful strings in a text.
Sentence segmentation partitions a sentence into severalconsecutive meaningful segments.
Word extraction should be easier than sentencesegmentation, and the problems involved in it can be solved using simpler methods.Some Chinese information-retrieval systems operate at the character level insteadof the word level, for example, the Csmart system (Chien 1995).
However, to furtherimprove the efficiency of natural Chinese processing, it is commonly thought to beimportant to apply studies from linguistics (Kwok 1997).
Lexicon construction is con-sidered to be one of the most important tasks.
Single Chinese characters can quiteoften carry different meanings.
This ambiguity can be resolved when the charactersare combined with other characters to form a word.
Chinese words can be unigrams,bigrams, trigrams, or n-grams, where n > 3.
According to the Frequency Dictionaryof Modern Chinese (Beijing Language Institute 1986), among the 9,000 most frequentChinese words, 26.7% are unigrams, 69.8% are bigrams, 2.7% are trigrams, 0.007% arefour-grams, and 0.002% are five-grams.
There are lexicons for identifying some (andprobably most of the frequent) words.
However, sometimes less-frequent words aremore effective.
Weeber, Vos, and Baayen (2000) recently extracted side-effect-relatedterms in a medical-information extraction system and found that many of the termshad a frequency of less than five.
This indicates that low-frequency words may alsocarry very important information.
Our experiments show that we can extract low-frequency words using a simple method without overly degrading the precision.There are generally two directions in which words can be formed (Huang, Chen,and Tsou 1996).
One is the deductive strategy, whereby words are identified throughthe segmentation of running texts.
The other is the inductive strategy, which identifieswords through the compositional process of morpho-lexical rules.
This strategy repre-sents words with common characteristics (e.g., numeric compounds) by rules.
In Chi-nese text segmentation there are three basic approaches (Sproat et al 1996): pure heuris-tic, pure statistical, and a hybrid of the two.
The heuristic approach identifies wordsby applying prior knowledge or morpho-lexical rules governing the derivation of newwords.
The statistical approach identifies words based on the distribution of their com-ponents in a large corpus.
Sproat and Shih (1990) develop a purely statistical methodthat utilizes the mutual information between two characters: I(x, y) = log p(x,y)p(x)p(y) ; thelimitation of the method is that it can deal only with words of length two charac-ters.
Ge, Pratt, and Smyth (1999) introduce a simple probabilistic model based on theoccurrence probability of the words that constitute a set of predefined assumptions.Chien (1997) develops a PAT-tree-based method that extracts significant words by ob-serving mutual information of two overlapped patterns with the significance function77Feng, Chen, Deng, and Zheng Accessor Variety Criteria for Chinese Word ExtractionSEc =Pr(c)Pr(a)+Pr(b)?Pr(c) , where a and b are the two biggest substrings of string c. Zhang,Gao, and Zhou (2000) propose the application of a statistical method that is based oncontext dependence and mutual information.
Yamamoto and Church (2001) experi-ment with both mutual information and residual inverse document frequency (RIDF)1as criteria for deciding Japanese words, and their main contribution is in affordinga reduced method for computing term and document frequency.
In almost all of thework cited to this point, the dimension that is used to compute mutual information isterm frequency.
Chen and Bai (1998) propose a corpus-based learning approach thatlearns grammatical rules and automatically evaluates them.
Chang and Su (1997) usean unsupervised Viterbi training process to select potential unknown words and iter-atively truncate unlikely unknown words in the augmented dictionary.
Teahan et al(2000) propose a compression-based algorithm for Chinese text segmentation.
Paolaand Stevenson (2001) demonstrate an effective combination of deeper linguistic knowl-edge with the robustness and scalability of a statistical technique to derive knowledgeabout thematic relations for verb classification.
Mo et al (1996) deal with the iden-tification of the determinative-measure compounds in parsing Mandarin Chinese bydeveloping grammatical rules to combine determinators and measures.We introduce another concept, accessor variety (AV) (for a detailed definition, referto subsection 3.1), to describe the extent to which a string is likely to be a meaning-ful word.
Actually, Harris (1970) uses similar criteria to determine English morphemeboundaries, and our work is partially motivated by his success.
We first discard thosestrings with accessor varieties that are smaller than a certain number (called the thresh-old; see subsequent discussion).
The remaining strings are considered to be potentiallymeaningful words.
In addition, we apply rules to remove strings that consist of a wordand adhesive characters (clarified in subsection 3.2).
Our experiment shows that evenfor small thresholds, quite good results can be obtained.In Section 2, we introduce examples of unknown words, the identification of whichis the task of our work.
In Section 3, we discuss our method.
In Section 4, we presentour experimental results.
We conclude our work with a discussion and a comparison toprevious results in Section 5.
In Section 6, we list some future work that can be pursuedfollowing the concept of AV.
We note that although our method is quite simple, it ismarginally better than previous comparable results.
This method distinguishes itselffrom statistically based approaches and grammatical rules.
Because of its simplicity, itcan be used easily in computer-based applications.
Moreover, innovative variations ofour method and its combination with statistical methods and grammatical methodsare worthy of further exploration.2.
Unknown WordsAs defined by Chen and Bai (1998), unknown words are words that are not listedin an ordinary dictionary, and word extraction seeks to identify such words.
To givereaders an intuitive view of these words, we list the types of unknown words that mostfrequently appear (Chen and Bai [1998] list 14 different types).
What we should pointout here is that except for numeric-type compounds, which are extracted separately,we extract all the other types of words together.1.
Proper names.
These include acronyms, Chinese names, and those words thathave been borrowed from other languages: for example, , ?Bank of China?
; ,1 RIDF = observed IDF ?
predicted IDF = ?
log dfD + log(1 ?
e?
tfD ), where tf , df , and D are termfrequency, document frequency, and number of documents, respectively.78Computational Linguistics Volume 30, Number 1?Feng Haodi?
(Chinese girl?s name); , ?Prince Edward?
; , ?Microsoft?
;and , ?the United Kingdom of Britain and NorthernIreland?.
To recognize proper names is the first task for Chinese word extraction,because their meanings cannot be obtained through the combination of smaller words,as in the compound words that are described next.
Therefore, a reasonable way toapproach them is to deduce them from Chinese text collections.2.
Compound words.
These are strings with specified meanings that are composedof shorter meaningful words: for example, , ?Industry and CommerceBank of China?, is composed of , ?China?, , ?industry and commerce?, and, ?bank?
; and , ?foreign businessmen invested company?, is com-posed of , ?foreign businessmen?, , ?invest?, and , ?company?.
Compoundwords account for a large proportion of Chinese words because it is very easy to com-pose a new compound word out of smaller known words.
There are about 5,000commonly used Chinese characters, but the number of compound Chinese words isunpredictable.
We want to extract those compounds that are accepted as words by mostpeople.3.
Derived words.
These are words that have affix morphemes: for example,, ?modernization?, and , ?computerization?, both of which contain af-fix morpheme .4.
Numeric-type compounds.
Some examples of numeric-type compounds wouldbe 1999 , ?1999?
; , ?the first session?
; , ?year 2000?
; and , ?11streets?.
Although these words have specific meanings and are used frequently, mostdictionaries do not contain them.
It is not very difficult to identify them, since thereare morphological rules (Mo et al 1996) for generating these words.
Such numeric-type compounds contain numbers as the main components, and measure charactersor words are used nearby.3.
Proposed ApproachOne of the important parameters that is employed in statistical methods for automaticChinese word extraction is word or character frequency.
Equivalent frequencies, suchas document frequency and term frequency, are used analogously.
Algorithms thatare based on these frequencies are used to measure how likely it is that a particularstring of characters is a meaningful word, according to the belief that ?when a stringis repeated many times, it must carry a meaning.?
However, in this article, we usenot frequency, but accessor variety.
This can be explained as ?when a string appearsunder different linguistic environments, it may carry a meaning.?
We introduce theconcept accessor variety as a new criterion for identifying meaningful Chinese words.3.1 Accessor VarietyIn Chinese text, each substring of a whole sentence can potentially form a word, butonly some substrings carry clear meanings and thus form a correct word.
For example,the sentence has 21 substrings, but only four substrings, , ,, and , can be considered words (we do not consider single-characterwords here).
In some implementations, the segmentation method is used to extractthose words (recent reviews on Chinese word segmentation include Wang, Su, andMo [1990] and Wu and Tseng [1993]).
There are several commonly used segmentationmethods such as forward maximum matching and backward maximum matching(Teahan et al 2000; Dai, Loh, and Khoo 1999; Sproat et al 1996).
If the dictionary in-cludes the words , , and , then forward maximum matching will ex-tract two words, and , after segmenting the sentence.
If is deleted79Feng, Chen, Deng, and Zheng Accessor Variety Criteria for Chinese Word Extractionfrom the dictionary, then the sentence will be segmented into , , , and, and two words, and , are obtained.
Furthermore, if is removedfrom the dictionary, then another, different segmentation pattern will be achieved.Therefore, the dictionary is an important factor in these methods.
In fact, this sen-tence has ambiguities (?The door handle is broken?
or ?the door hurts the hand?
), andthe segmentation methods try to find a reasonable way to solve this problem.
We donot segment the sentence but extract those substrings that might possibly form words.The accessor variety criterion is used to decide whether a substring should be retainedor discarded.
Sentences (1)?
(4) can be used to illustrate the meaning of accessor variety:(1) , ?The door hurts the hand?
or ?The door handle is broken?.
(2) , ?Xiao Ming fixed the door handle?.
(3) , ?This door handle is very beautiful?.
(4) , ?This door handle is broken?.Consider how to extract the word from these four sentences.
In fact, the three-character string has three distinct prefixes, ?S?, , (?S?
denotes the start ofa sentence), and four distinct suffixes, , ?E?, , (?E?
denotes the termination ofa sentence).
This means that the string can be used in at least three different environ-ments and might carry meanings that are independent of those of the other charactersin these four sentences.
In this case three = min{three, four} is called the accessor varietyof string .We use the criterion accessor variety to evaluate how independently a string isused, and thus how likely it is that the string can be a word.
The accessor variety ofa string s of more than one character is defined asAV(s) = min{Lav(s), Rav(s)}.Here Lav(s) is called the left accessor variety and is defined as the number of distinctcharacters (predecessors) except ?S?
that precede s plus the number of distinct sen-tences of which s appears at the beginning.
Similarly, the right accessor variety Rav(s)is defined as the number of distinct characters (successors) except ?E?
that succeed splus the number of distinct sentences in which s appears at the end.
In other words,characters ?S?
and ?E?
are repeatedly counted.
The reason for doing this is that somewords usually appear at the beginning or the end of sentences.
For example, ,?suddenly?, is often used separately as a short sentence.
Therefore, ?S?
and ?E?
willbe counted multiple times, and we regard as a meaningful word, although thereare probably very rarely other characters preceding or succeeding it.The extracted words should ensure an AV value of no less than a predefinedthreshold, which means that such strings should appear in enough different environ-ments and therefore be considered meaningful.
Our experiments show that even witha small threshold, the result is quite precise.3.2 Adhesive CharactersThere are some characters, such as auxiliary characters (a mark following an adjec-tive) and (a mark following an adverb), that often adhere to other words as headsor tails to compose a string with a high AV value that is not an actual linguistic word;we call these characters adhesive characters.
For example, , ?of people?, has avery high AV value because many adjectives (and hence many predecessors) precede, ?people?
(e.g., , ?here people?, and , ?diligent people?
).Moreover, many words (and hence many successors) can succeed it to describe thebehavior of the people (e.g., , ?people here make a living80Computational Linguistics Volume 30, Number 1out of commerce?, and , ?diligent people are working?).
It seemsthat combines with very firmly, but cannot be accepted as a wordby most people.
There are also some nonauxiliary characters that very frequently ad-here to other, shorter words.
In our method, we ignore the difference between theauxiliary and nonauxiliary adhesive characters and extract them under the same cri-teria.
Recalling the discussion about the AV value, we divide the adhesive charactersinto two groups.
The head-adhesive characters often stick at the heads of other wordsand have high Rav values, and the tail-adhesive characters often stick at the tails ofother words and have high Lav values.
How adhesive characters are found will bediscussed in the article.The adhesive characters should be stripped from the string for constructing a well-formed word.
According to the places in which these characters appear, three casesare considered.
If the leftmost consecutive characters of a string are all head-adhesivecharacters, then we say that it is in the h+core style.
If the rightmost consecutivecharacters of a string are all tail-adhesive characters, then we say that it is in the core+tstyle.
A string that is in both h+core and core+t styles is said to be in the h+core+t style,where the core is the inner part of the string found by removing the left consecutivehead-adhesive characters and right consecutive tail-adhesive characters.
For example,, ?of I?, is in the h+core style and , ?I?, is the core, , ?my?, is in the core+tstyle and , ?I?, is the core; and , ?of procedure is?, is in the h+core+t styleand , ?procedure?, is the core.
In other words, none of the strings matching thesethree cases should be considered words, that is, they should be discarded.With the help of adhesive characters, we can introduce the ADHESIVE JUDGErules to discard all those strings that have a high AV score but are unlikely to be realwords:1.
A string that is composed of two characters in any of the h+core, core+t,and h+core+t (no core in the case of two characters) styles should bediscarded if it does not appear in a specified electronic dictionary.
Forexample, strings such as , ?of I?, and , ?one of?, will bediscarded, whereas strings such as , ?surely?, , ?comprehend?,and , ?Jane?
(a girl?s name), will remain.
Under this rule, mostmeaningful two-character strings that are unknown to the dictionary willbe recognized as meaningful words because they rarely contain adhesivecharacters.2.
A string that is made up of more than two characters in any of the threestyles (h+core, core+t, and h+core+t) should be discarded if the core is ameaningful multi-character word.3.
The most frequently used auxiliary words, such as , ?of?, , ?have?,(a mark indicating completion), and , ?at?, must be used to delimitthe original string.
If any token is found to be an identifiedmulticharacter word (a word in the specified dictionary or extracted bythis algorithm before processing the string under consideration), then theoriginal string is abandoned.All of the strings will be kept as meaningful words if they survive eliminationaccording to these rules.
According to these rules, strings such as , ?of pro-cedure is?, , ?one of?, and , ?of I?, should be abandoned, whereas ,?actually?, and , ?seek truth from facts?, should remain even though they allcontain auxiliary words.81Feng, Chen, Deng, and Zheng Accessor Variety Criteria for Chinese Word Extraction3.3 Numeric-Type CompoundsWe define numeric-type compounds to be strings of numbers or strings that con-tain substrings of numbers followed by measures.
For example, , ?thousandsupon thousands?, , ?the first Olympic games?, and , ?50 kilo-grams?, will be considered as potential numeric-type compounds, whereas ,?wholehearted?, will not be, because and are not measures.
Numbers includeArabic numbers of SBC case and DBC case and Chinese numbers in both simplifiedform and traditional form.
Special words such as , ?several?, , ?about?, and ,?or so?, are treated as numbers too.
Measures include both Chinese measures and for-eign measures (e.g., , ?mu?, , ?chi?, , ?ounce?, and , ?gallon?).
Because ofthe special nature of words of this type, some lexicons do not include them, which iswhy we extract them separately.
In our method, a numeric-type compound must befirst a maximal numeric-type string, which means that the string cannot be precededor succeeded by other numbers or measures in the sentence under consideration.
Forexample, when processing the sentence , ?October2nd, 1977, is his birthday?, strings , ?one?, , ?nineteen?, , ?1977?,, ?October?, and , ?second day?, are not extracted.
The only numeric-typecompound that is extracted from this sentence is , ?October 2nd,1977?.
The numeric-type compound candidates are then further examined according tothe ADHESIVE JUDGE rules, and the survivors of that examination are eventually ac-cepted as numeric-type compounds.
Notice that for strings of only numbers or stringsof numbers followed by measures, we set the threshold to one.As we process numeric-type compounds separately, we ignore strings that containnumeric-type compounds when we extract the ordinary words.4.
Experimental Results4.1 Setup of the Experimental EnvironmentThe corpus-based word extraction method described in the previous section was testedon a 153 MB corpus consisting of People?s Daily news and Xinhua news from TREC5 andTREC6 (Linguistic Data Consortium, n.d.).
We also conducted experiments on a smallcorpus that has approximately 1.7 MB of data and is a part of the former corpus.Neither corpus was annotated.
The system dictionary that we used in each experi-ment was downloaded from http://www.mandarintools.com/segmenter.html and contains119,538 terms from two to seven characters long.
In our method, a preprocessingstep was performed on the corpus in which we eliminated all of the non-Chinesesymbols.
Each uninterrupted Chinese character sequence was kept as one line in thetransformed data.
For each line in the data file, all possible substrings were extracted,along with their predecessors and successors.
Those predecessors and successors werefinally merged, and the AV, Lav, and Rav values were calculated.
Different thresholdswere used for discarding those strings with low AV values and checking how thethreshold affects the results.
Moreover, the ADHESIVE JUDGE rules were used for thefurther discarding of those strings that seemed unlikely to be words.A list of adhesive characters is needed when we apply the ADHESIVE JUDGErules.
We constructed the adhesive character list based on the accessor variety infor-mation of single characters.
Characters with high Lav values were considered to betail-adhesive characters.
Characters with high Rav values were considered to be head-adhesive characters.
Characters with very high AV values were considered to be thedelimiters that are used in rule (3) of the ADHESIVE JUDGE rules.
In the end, weplaced 68 tail-adhesive characters, 66 head-adhesive characters, and 16 delimiters onour list.82Computational Linguistics Volume 30, Number 1In our experiments, we performed only one step of each of the ADHESIVE JUDGErules (in either direction) for discarding meaningless multicharacter strings.
That is,in any of the three styles (h+core, core+t, or h+core+t), only the leftmost or rightmostcharacter was considered among all of the head- or tail-adhesive characters.
If the firstcharacter of a string was a head-adhesive character and the remaining substring (afterstripping the first character) was found in the system dictionary or the preextractedshorter word lists (and thus a core was found), such a string was considered to be inthe h+core form and thrown away.
The same judgment process was used in the core+tand h+core+t styles.
In other words, only the first or last character, or both, of a stringwere used in rule (2) of the ADHESIVE JUDGE rules.
Such simplification does nothurt the results too much.The AV value threshold is another important factor in this method.
We tested dif-ferent thresholds to evaluate how they influenced the performance.
One might imaginethat a higher threshold will result in higher precision while causing the loss of somerecall.
This phenomenon was certainly observed in our experiments.
Word length hasa relationship with the threshold: that is, longer words required a smaller thresholdto reach the same precision, or higher precision could be obtained on longer wordswith the same threshold, because longer words have more specific usage and appearin fewer environments.Our first experiment was carried out on the small corpus of Xinhua news.
Stringswith lengths varying from two to ten characters were examined.
In the following, wetested our method on the large corpus and all strings with lengths from two to sevencharacters.
In the end, we extracted the numeric-type compounds from each corpus.In addition, there is no commonly accepted standard for evaluating the perfor-mance of word extraction methods, and it is very hard to decide whether a word ismeaningful or not (Sproat et al 1996).
We define precision as the number of extractedwords that would be meaningful in a Chinese native speaker?s opinion, divided by thetotal number of extracted compounds.
As it is very hard to find all of the words in theoriginal corpus that would be found meaningful by a Chinese person, it is very hard tocount recall in the traditional way, that is, the number of meaningful words extracteddivided by the number of all meaningful words in the original data.
On the other hand,it is also impossible to approach traditional precision and traditional recall by compar-ing the hand-segmented sample sentences and the automatically segmented sentences,as people usually do, because our method does not touch upon segmentation.
Thereason that we do not consider segmentation is that we aim only to investigate theperformance of AV itself, whereas the involvement of a segmentation module wouldinevitablly influence our judgment on the performance of AV.
Therefore, we substitutepartial recall for traditional recall.
We define partial recall as the number of extractedmeaningful words (from the whole corpus) that appear in a sample corpus dividedby the total number of meaningful words in the sample corpus.
Evidently, the partialrecall value will be no smaller, and usually greater, than the recall value calculated inthe traditional way.
This point will be clearly reflected by the following experimentalresults.
What should be pointed out here is that some people use the F-measure as anevaluation metric (Ricardo and Berthier 1999; Chang and Su 1997).
However, this isdifficult to interpret according to Beeferman, Berger, and Lafferty (1999).
In our opin-ion, as the F-measure or precision-recall curves are based on two parameters, recalland precision, it is enough for us only to list the partial recall and precision.4.2 Experiments on the Small CorpusAs noted previously, the small corpus contained approximately 1.7 MB data of Xinhuanews.
We processed all of the strings in the corpus with lengths from one to ten83Feng, Chen, Deng, and Zheng Accessor Variety Criteria for Chinese Word ExtractionTable 1Some of the words extracted from the small corpus.Economy of GuangDong and Hong Kongnew region of PuDongSihanouk (name)Italian Teamnature protection regionAdministration Committee of PLOUNESCOAssociation of Relations Across the Taiwan Straits (ARATS)GuangDa International Trust Investment CompanyYiZheng Chemical Fibre United CompanyParent Ocean Petroleum Company of ChinaXiaoLangDi Irrigation Hinge ProjectFrench Open TennisHong Kong Special Administration RegionUnited Nations Security CouncilAsia Development BankInnovation of the Economy Systemmost-favoured-nation clauseChristopher (name)Preparing CommitteeMandela (name)UBA Championship Cupcharacters.
Table 1 shows some of the extracted correct words that were not containedin the system dictionary.We can see that almost all of these words are compound words, proper names,or derived words.
It would be almost impossible to list all of them in a general-purpose dictionary.
Furthermore, some of them occur only a few times.
For example,only occurs three times in this corpus.
The method we used has theability to extract low-frequency words.Table 2 shows the overall precision performance when the word length is notspecified.
We set the threshold from two to nine and observed that with a largerthreshold we could obtain more precise results.
As the number of words extractedwas very large (approximately 30,000 words), we randomly chose a portion (oftenapproximately 1,000 words) of the total set of extracted words as the test set to calculatethe precision; that is, we listed all of the extracted words, and then for each word,we generated a random number between zero and one.
If the number was smallerthan the number of test words divided by the number of all extracted words (here1, 000/30, 000), then the corresponding word was chosen.
Human judgment was thenused to check whether an extracted word was a correct or spurious word.In the evaluation phase we found that the method performed differently on stringsof different lengths.
Hence, we also checked the precision performance with specifiedword lengths.
We set the threshold to three and obtained the data in Table 3.
Againwe used the sampling method just described to test the overall precision.From Table 3 we can see that the method worked almost equally well on all wordlengths except length three.
After checking the results, we found that three-characterstrings are often constructed from a two-character legitimiate word together with asingle character.
It is difficult to judge with such a simple method whether such three-character strings are legitimate words.Beyond precision, another concern is partial recall.
In other words, how manywords will be missed using such a method?
The corpus contained 55,788 sentences.84Computational Linguistics Volume 30, Number 1Table 2Experiments on the threshold?precision relationship of the small corpus.Threshold Precision Number of words2 64.4% 37,0933 83.8% 14,4684 89.6% 8,6485 94.1% 6,1476 96.8% 4,7577 97.4% 3,8008 97.3% 3,1629 97.7% 2,734Table 3Experiments on the word length?precision relationship of the small corpus.Word length Precision Number of words2 90.2% 6,9623 56.6% 2,5324 91.4% 3,4175 85.1% 7126 90.4% 4937 89.4% 1808 90.1% 1119 80.3% 61Table 4Experiments on the threshold?partial recall relationship of the small corpus.Threshold Partial recall Number of words2 76.7% 37,0933 66.5% 14,4684 59.0% 8,6485 54.3% 6,1476 50.3% 4,7577 47.1% 3,8008 44.0% 3,1629 41.5% 2,734We checked only a small portion (a random sample of approximately 2,000 sentences)of the total corpus.
We used this sample to find meaningful words by hand.
The resultof automatic extraction from the whole corpus was then compared with that of handextraction of the sample sentences.
The partial recall was computed as the number ofwords in both sets divided by the number of words in the human extraction set.
Welist the experimental partial-recall values in Table 4.We analyzed the instance with the threshold of two.
Some of the words weremissed because they occurred only once, which was less than the threshold.
Some ofthe words were missed because they occurred only in very restricted environments.This means that although they appeared more than once in the corpus, their acces-sor variety value was only one.
In the latter case, we could extract the strings thatcontained such strings as substrings.
The details are discussed in the section on erroranalysis.85Feng, Chen, Deng, and Zheng Accessor Variety Criteria for Chinese Word ExtractionTable 5Some words extracted from the large corpus.desolate roadHuang Yanping (Chinese name)goose feather fanBi Tong Ling (name of a Chinese medicine)send love by swan goosebeloved hometownMaGu offers birthday presentright of independent managementthe Peking Museumthe Technology Institute of East Chinafake and bad merchandisethe Peking penmanship temple fairSunday photo newspapersocialistic modernization4.3 Experiments on the Large CorpusThe corpus that was used for these experiments was the TREC Chinese corpus (Lin-guistic Data Consortium, n.d.), which contains 160,000 articles, including articles thatwere published in the People?s Daily from 1991 to 1993 and a portion of news releasedby the Xinhua News Agency in 1994 and 1995.
In the experiment, we extracted wordswith lengths of two to seven characters.
The data contained approximately 7,000,000sentences.
We first eliminated the non-Chinese characters.
All of the experiments thatwere carried out on the small corpus were also conducted on the large corpus.
In Ta-ble 5 we first show some legitimate words that were extracted from the large corpus.Notice that these words cannot be found in the word list that was extracted from thesmall corpus or in the system dictionary.In Table 6, we show the overall precision performance.
The performance trendsthat were observed in Table 2 can be also observed here.
However, as this corpus ismuch larger than the previous one, many characters have the chance to occur togetherto form spurious words.
That is why the precision is much lower than that for thesmall corpus.
Nevertheless, as the corpus is much larger now, a correct word canoccur in many more environments than in the small corpus, which suggests that wecan improve the precision by using a large threshold for the accessor variety valuewithout overly degrading the partial recall.
For example, when the threshold is set tonine, the precision is as high as 73.4% and the partial recall remains as high as 80.4%.Table 6Experiments on the threshold?precision relationship of the large corpus.Threshold Precision Number of words2 51.2% 2,854,7003 58.3% 1,269,3784 69.0% 788,9645 70.3% 562,4076 70.4% 432,8307 73.8% 349,5118 74.2% 291,6889 73.4% 249,90486Computational Linguistics Volume 30, Number 1Table 7Experiments on the word length?precision relationship of the large corpus with thresholdthree.Word length Precision Partial recall Number of words2 37.8% 92.3% 266,0273 22.9% 83.5% 335,5574 68.9% 80.9% 360,4135 67.0% 83.3% 141,1536 76.0% 81.6% 123,3927 70.7% 64.3% 42,836Table 8Experiments on the word length?precision relationship of the large corpus with threshold nine.Word length Precision Partial recall Number of words2 71.7% 90.0% 77,2003 52.7% 73.0% 55,0154 74.6% 70.2% 78,8685 75.0% 63.9% 18,7756 86.9% 63.2% 15,6637 89.4% 42.9% 4,383The precision and partial-recall performance in respect to the word length wasalso tested on the large corpus.
The same sample method was used, and the resultsfor thresholds three and nine are shown in Tables 7 and 8, respectively.Note that there is a great jump in the precision for word lengths two and threeafter we change the threshold from three to nine, but the partial recall does not changemuch.
For longer words, the method even performs well with threshold three.The next experiment was intended to test the partial-recall performance for allof the words with lengths from two to seven.
The result is shown in Table 9, whichindicates that the partial-recall value is satisfactory even with a large threshold.
Thismeans that we can extract most of the words in the corpus.4.4 Experiments on Numeric-Type CompoundsIn this section, we consider numeric-type compounds.
Some of the compounds of thistype that were extracted from the large corpus are listed in Table 10.Table 9Experiments on the threshold?partial recall relationship of the large corpus.Threshold Partial recall Number of words2 89.2% 2,854,7003 87.2% 1,269,3784 85.6% 788,9645 84.2% 562,4076 83.0% 432,8307 82.0% 349,5118 81.2% 291,6889 80.4% 249,90487Feng, Chen, Deng, and Zheng Accessor Variety Criteria for Chinese Word ExtractionTable 10Numeric-type compounds extracted.3 2 March 2ndfirst timeMay the Fourth, 1992two sides of the Straitrelationship between two countriesthirty Kilograms or so100 one hundred Hong Kong dollars200 two hundred ouncesforty thousand mu4.5 Error AnalysisTwo kinds of errors occurred: the extraction of meaningless strings as meaningfulwords and the neglect of meaningful words.
Some errors of the two types are listedbelow.4.5.1 Meaningless Strings Extracted.
A number of meaningless strings were extracted:for example, , ?solve the Republic of Bosnia and Hercegovina?
; ,?meeting today?
; , ?employ international?
; , ?title match order?
;, ?today China?
; , ?related part?
; , ?game today?
; , ?interna-tional pass?
; , ?city people?
; , ?will next?
; , ?field people?
; , ?be-come country?
; , ?province order?
; , ?point to?
; , ?city first?
; and ,?people attention?.Most of these errors occurred because the strings are made up of one shortermeaningful word and one character that has a high accessor variety value but is absentfrom the adhesive-character list.
For example, is composed of (accessorvariety value 133 in the large corpus) and , but is not on the adhesive-characterlist.
Therefore, was extracted as a word.
However, if we list too many charactersas adhesive characters, the partial recall will be degraded.
To give another example,, ?bank of China deliver?, was extracted as a meaningful word even thoughits meaning is very unclear.
In the string , we considered andto be adhesive characters and regarded as being in the h+core+t style.However, the core is not in the system dictionary or the shorter word listthat we extracted previously.
Hence, it passed the ADHESIVE JUDGE rules and wasretained as a word.
It is hard to discard strings such as and ,even though their meanings are not at all clear.4.5.2 Meaningful Words Missed.
A number of meaningful words were missed: forexample, , ?clear?
; , ?gaseous state?
; , ?sight-seeing interest is notfulfilled?
; , ?barren?
; , ?carry forward?
; , ?Straits Exchange Foundation?
;, ?recently?
; and , ?African National Congress?.The main reason for these errors is that the strings occur only once in the corpus ortheir accessor varieties are smaller than the threshold.
One way to solve this problemis to use a larger corpus to improve the partial recall.
Another reason for these errors isthat the string is composed of a shorter word plus an adhesive character, in which caseit was discarded according to the ADHESIVE JUDGE rules.
For example, iscomposed of and , where is a word in the system dictionary and is anadhesive character.
To solve this problem, we can use fewer adhesive characters at thecost of some precision.
To give another example, , ?Chang Jiang triangle88Computational Linguistics Volume 30, Number 1region?, is a meaningful word that appeared in the corpus but was not extracted,because it contains a substring that can be interpreted as a numeric compound?three jiao?
(which means 0.3 Chinese RMB), and therefore we discarded it.
However,we can extract this string as a numeric-type compound.5.
ConclusionWe have described a hybrid method for extracting Chinese words from the Chinesetext corpus using accessor variety and adhesive characters.
We tested the method onthe performance of different thresholds and word lengths and different corpus sizes.We conclude that the method based on accessor variety and adhesive charactersperforms efficiently in fulfilling word extraction tasks.
The precision with the smallcorpus we used was much larger than that with the large corpus, but the situationwas opposite for partial recall.
For example, when the threshold was set to three, theprecision and partial recall with the small corpus were 83.8% and 66.5%, respectively,whereas with the large corpus they were 58.3% and 87.2%, respectively.
When thethreshold was set to nine, the corresponding numbers were 97.7% and 41.5% versus73.4% and 80.4%.
As even human judges differ when facing the task of segmenting atext into words and test corpora differ from system to system (Sproat et al 1996), it isvery difficult to compare two methods.To convincingly illustrate the efficiency of our method, we chose one of the mostdirect ways: We implemented Chang and Su?s (1997) method and our own method on acorpus, the size of which was similar to the one that was used in their paper.
We choseChang and Su?s paper as reference for two reasons: Their approach was unsupervised,just like ours, and it was a complicated iterative method that integrated several com-monly used word-filtering techniques (including Viterbi training, mutual information,entropy, and joint Gaussian mixture density function) to improve their result.
Theirsegmentation system contains two modules: One is the segmentation module, whichis used to segment words and calculate the frequencies of the words; the other is thefiltering module, which is used to rank the likelihood ratios of the words, and furtherto filter out those words with low likelihood ratios from the augmented dictionaryand add those words with high likelihood ratios into the augmented dictionary.
Thesystem iteratively repeats these two modules until a predefined condition is fulfilled.We will show that even compared to such a deliberate approach, our simple methodis marginally better.
For simplicity, we will use IT to refer to Chang and Su?s methodand AV to refer to our method, where the symbol IT implies iterative and AV impliesaccessor variety.We combined PD9208.SGML and PD9209.SGML (files of People?s Daily as publishedin August and September 1992, which is a proportion of the TREC Chinese corpus[Linguistic Data Consortium, n.d.]) to form a file of 376,053 sentences after the clearingstep (notice that in Chang and Su?s [1997] paper, 311,591 sentences were used).We conducted two comparison experiments, one for extracting words with lengthsof two to four characters and the other for extracting words with lengths of two toseven characters.
The reason for selecting these two sets of word lengths is that Changand Su considered only words with lengths of two to four characters, whereas in ourmethod we consider words with lengths of two to seven characters.
In both experi-ments, the number of iterations for IT was 21 (because Chang and Su also conducted21 iterations), and the AV value threshold (when the AV value of a string is greaterthan or equal to this threshold, it is considered to be a word) for our method is three.Because we do not segment the file in AV, it is impossible to count the precisionand recall by comparing the hand-segmented sample sentences with the automatically89Feng, Chen, Deng, and Zheng Accessor Variety Criteria for Chinese Word Extractionsegmented sample sentences.
(In this case, sample sentences are first obtained, thenthey are segmented both by hand and automatically by the method under examina-tion.
The precision is equal to the number of words that are extracted both by handand automatically, divided by the total number of words that are extracted automati-cally.
The recall is equal to the number of words that are extracted both by hand andautomatically, divided by the total number of words that are extracted by hand.)
Thisevaluation method was applied in Chang and Su?s (1997) original work.
Instead, weevaluated both IT and AV with the method that we described in the previous sections.We randomly chose 1,000 words of each word length (in the first experiment, wordlength varied from two to four, and in the second experiment, word length variedfrom two to seven) from the output dictionary that was generated by each method.The precision of each word length was then defined as the proportion of correct wordsamong the 1,000 sample words of the same word length.
Regarding partial recall (weused partial recall as a substitute for traditional recall, as discussed previously), wefirst randomly chose sentences from the unsegmented file, and then segmented themby hand.
Then we extracted words with different lengths from this set of sentences.The partial recall of each word length was then defined as the number of words ofthat length that were extracted both from the hand-segmented sample sentences andfrom the automatically generated output dictionary, divided by the total number ofwords of that length that were extracted from the hand-segmented sample sentences.The system dictionaries that we used in each experiment were derived fromthe large dictionary described before (i.e., a dictionary downloaded from http://www.mandarintools.com/segmenter.html that contains 119,538 terms from two to seven char-acters long).
In each experiment, the size of the system dictionary and the size of theapplied corpus were chosen to approach those of the system dictionary and the corpusthat were mentioned in Chang and Su?s (1997) original work.In each experiment, all of the values of precision and partial recall of both ITand AV were counted by the same person.
Therefore, the evaluation results should bereasonably credible.In the experiment of extracting words of lengths two to four, the system dictionarycontained 24,705 bigrams, 4,355 trigrams, and 4,252 four-grams, that is, a total of 33,312entities.
We randomly chose 979 sentences and segmented them by hand.
Suppose thatthe word set obtained was S. We then removed from S those segments that occurredin the system dictionary and those segments that appeared less than five times in theoriginal corpus (the 376,053 sentences).
The latter removal was undertaken becauseChang and Su (1997) did not consider segments with frequency of less than five.Hence, from S, we obtained 580 bigrams, 156 trigrams, and 135 four-grams.
Thesewords were considered to be new words extracted by hand from the sample sentencesand were used to test the partial recall for each method, IT and AV.
In Table 11, welist the precision and partial-recall value for each word length from two to four forboth IT and AV.We can see from the table that IT outperforms AV for word length two, but thesituation is just the opposite for word length four.
With word length three, the twomethods perform comparatively, and AV?s performance is slightly worse.
Consideringthat our method, AV, is much simpler than IT, we conclude that it is quite promising.Because we observed from this experiment that the performance of our methodimproves with increased word length, we conducted another experiment to furtherexamine this phenomenon.
In this experiment, we extracted words with lengths fromtwo to seven characters.
The system dictionary that we used contained 38,097 entries,with 27,986 bigrams, 4,906 trigrams, 4,834 four-grams, 238 five-grams, 89 six-grams and44 seven-grams.
We randomly chose 1,989 sentences and segmented them by hand.90Computational Linguistics Volume 30, Number 1Table 11Precision and partial recall of word lengths two to four of the first experiment on IT and AV.Bigram Trigram Four-gramPrecisionIT 57.69% 26.18% 56.93%AV 47.04% 25.75% 68.76%Partial recallIT 85.69% 84.62% 81.48%AV 75.34% 81.41% 87.41%Table 12Precision and partial recall of word lengths two to seven of the second experiment on IT andAV.Bigram Trigram Four-gram Five-gram Six-gram Seven-gramPrecisionIT 49.85% 25.38% 59.12% 32.71% 56.60% 32.62%AV 42.70% 28.28% 68.86% 54.66% 73.77% 70.23%Partial recallIT 84.84% 71.59% 78.05% 70.37% 80.65% 84.62%AV 80.83% 81.06% 88.35% 83.33% 90.32% 76.92%After filtering out the segments that appeared in the system dictionary and those withfrequencies less than five, the numbers of new words that were extracted by handfrom the sample sentences of word lengths two to seven were 699, 264, 369, 54, 31,and 13, respectively.
These words were used to test the partial recall.
In Table 12, welist the results of the second experiment.
The precision and partial-recall values werecomputed in the same way as were the values in Table 11.This table strongly indicates that AV outperforms IT for all word lengths exceptfor word length two.
Two characters have greater chances of occurring together indifferent environments than larger numbers of characters.
This degrades the precisionof our method in the case of bigrams, as the threshold that we used for AV valuewas three, i.e., when the AV value of a bigram was greater than or equal to three, weregarded it as a word.
The reason for the lower partial recall of AV with word lengthtwo is that we filtered out all of the bigrams that were both absent from the systemdictionary and had adhesive characters.
For larger word lengths, only those gramswith specific meanings had chances of occurring together in different environments;that is, they had higher AV values, which resulted in a higher precision value in ourmethod.
The reason for higher partial recall values of AV with longer grams is that evenwhen a longer gram with higher AV value both was absent from the system dictionaryand had adhesive characters, we did not dogmatically filter it out.
Alternatively, wefurthered examined whether it was in one of the three styles h+core, core+t, or h+core+t(as discussed in Section 3).
If it was in one of these styles, then we filtered it out.There are several ways to explain the performance of IT being better than that ofAV with bigrams but worse with longer grams.
First, IT does not consider adhesivecharacters, which helps improve the partial recall while degrading the precision, asmany grams contain adhesive characters and they are hard to inspect (note that in ourmethod, we filtered out some of the grams with adhesive characters).
Second, IT usesseveral techniques to filter out the bad candidates for real words, which is intended tohelp improve the precision.
But there are several deficiencies in this design.
In the IT91Feng, Chen, Deng, and Zheng Accessor Variety Criteria for Chinese Word Extractionsegmentation module, a longer segment is preferred.
(For each sentence, IT tries to findthe segmentation with the highest likelihood, where the likelihood is defined as themultiplication of the relative frequencies of all the segments, and the relative frequencyof one segment is defined as the frequency of that segment divided by the sum of thefrequency of all grams [Chang and Su 1997].
Therefore, if a segmentation has moresegments, then its likelihood value is smaller.)
This will inevitably degrade the partialrecall of shorter grams.
On the other hand, because system dictionaries usually containvery limited numbers of longer terms, IT?s filter module (i.e., a likelihood-rankingmodel) has inadequate information to correctly describe the feature functions of wordclass or nonword class for longer grams.
This will inevitably degrade both the precisionand the partial recall for longer grams, as real words might be considered nonwords,and nonwords might be considered real words.
Finally, although the combination ofseveral features seems more comprehensive, it also generates more noise than usingonly one feature.We think that all of the factors that we described above can roughly explain thephenomenon that is presented in Table 12.
Comparing Table 12 to Table 11, we find thatthe results are slightly different even for the same word lengths.
One reason for this isthat in different experiments, we used different system dictionaries.
Note that all of theresults were obtained only on new words.
Therefore, the size of the system dictionarywill affect the result of the experiment.
Usually, the larger the system dictionary is, thepoorer are the precision and recall that are obtained.
In the dictionary that we used inthe latter experiment, there were more bigrams than in the dictionary that we used inthe first experiment.
That is why the precision value and the partial-recall value forbigrams are smaller than those in the first experiment.
As there are similar numbersof trigrams and four-grams in both dictionaries, the results for these grams are veryclose in both experiments.
Another factor that may lead to these differences is theuse of different sample sentences and different methods to segment them by hand fortesting partial recall.
In the former experiment, we considered only terms with lengthsfrom two to four characters, and hence only segmented the sample sentences to termsof lengths from two to four.
In the latter experiment, we considered all terms withlengths from two to seven characters.6.
Discussion of Future WorkIn this work, we have proposed accessor variety as an alternative to the commonlyused frequency criterion.
Our approach may give rise to new research directions inChinese text processing.
Our promising results for word extraction make it a potentialuseful method for other problems as well.In addition, word extraction is the basic step for many text-processing tasks.
Itis related to but different from word segmentation.
Extracted words can be used asthe fundamental elements for related application problems, such as creating a textsummary for a bundle of articles and text clustering.Futhermore, words as sequences of letters occur not only in language processing,but also in other application areas.
Our method may be of some heuristic value to otherrelated problems, such as those involving substring processing (Deng, Li, and Wang2002; Thijs et al 2002; Narasimhan et al 2002), and biomedical concepts identificationMajoros, Subramanian, and Yandell 2003).Finally, in our simple method, we process the data only once, and no iterative re-finement is applied.
The result is comparable even to that of very comprehensive sys-tems and shows some improvement with longer grams.
The simplicity of our methodmakes it especially suitable for processing large corpora.92Computational Linguistics Volume 30, Number 1AcknowledgmentsWe would like to thank Jing-Shin Chang forhelp in our implementation of their methodIT.
Many thanks also go to Chun-yu Kit forhis suggestions.
And we also thank theanonymous reviewers for their constructiveadvice in revising the original manuscript.The work described in this article was fullysupported by a grant of NSFC/RGC jointresearch scheme (N CityU 102/01,NSFC60131160743).ReferencesBeeferman, Doug, Adam Berger, andJohn D. Lafferty.
1999.
Statistical modelsfor text segmentation.
Machine Learning,34(1?3):177?210.Beijing Language Institute.
1986.
Xian daihan yu pin lu ci dian [Word FrequencyDictionary of Modern Chinese].
BeijingLanguage Institute Press.Chang, Jing-Shin and Keh-Yih Su.
1997.
Anunsupervised iterative method forChinese new lexicon extraction.International Journal of ComputationalLinguistics and Chinese Language Processing,2(2):97?148.Chao, Yuen-Ren.
1968.
A Grammar of SpokenChinese.
University of California Press,Berkeley and Los Angeles.Chen, Ching-Yu, Shu-Fen Tseng, Chu-RenHuang, and Keh-Jiann Chen.
1993.
Somedistributional properties of MandarinChinese?A study based on the AcademiaSinica corpus.
Proceedings of Pacific AsiaConference on Formal and ComputationalLinguistics I, pages 81?95, Taipei.Chen, Keh-Jiann and Ming-Hong Bai.
1998.Unknown word detection for Chinese bya corpus-based learning method.International Journal of ComputationalLinguistics and Chinese Language Processing,3(1):27?44.Chien, Lee-Feng.
1995.
Csmart?Ahigh-performance Chinese documentretrieval system.
In Proceedings of the 1995International Conference of ComputerProcessing of Oriental Languages (ICCPOL),pages 176?183, Hawaii.Chien, Lee-Feng.
1997.
PAT-tree-basedkeyword extraction for Chineseinformation retrieval.
In Proceedings of the20th Annual International ACM SIGIRConference on Research and Development inInformation Retrieval, pages 50?58,Philadelphia.Dai, Yubin, Teck Ee Loh, and ChristopherKhoo.
1999.
A new statistical formula forChinese text segmentation incorporatingcontextual information.
In SIGIR ?99:Proceedings of the 22nd Annual InternationalACM SIGIR Conference on Research andDevelopment in Information Retrieval, pages82?89, Berkely, CA.Deng, Xiaotie, Guojun Li, and LushengWang.
2002.
Center and distinguisher forstrings with unbounded alphabet.
Journalof Combinatorial Optimization, 6(4):383?400.Garside, Roger, Geoffrey Leech, andGeoffrey Sampson.
1987.
TheComputational Analysis of English: ACorpus-Based Approach.
London, Longman.Ge, Xian-Ping, Wanda Pratt, and PadhraicSmyth.
1999.
Discovering Chinese wordsfrom unsegmented text.
In Proceedings ofthe 22nd Annual International ACM SIGIRConference on Research and Development inInformation Retrieval, pages 271?272,Berkeley, CA.Guo, Jin.
1997.
Critical tokenization and itsproperties.
Computational Linguistics,23(4):569?596.Harris, Zellig S. 1970.
Morphemeboundaries within words.
In Papers inStructural and Transformational Linguistics.D.
Reidel, Dordrecht, pages 68?77.Huang, Chu-Ren, Keh-Jiann Chen, andBen-Jamin K. Tsou.
1996.
Readings inChinese Natural Language Processing.
InJournal of Chinese Linguistics MonographSeries no.
9, edited by Chu-ren Huang etal., pages 1?22.Kobayasi, Yosiyuki, Takenobu Tokumaga,and Hozumi Tanaka.
1994.
Analysis ofJapanese compound nouns usingcollocational information.
In Proceedings ofthe 15th International Conference onComputational Linguistics (COLING?94),vol.
2, pages 865?869, Kyoto, Japan.Kwok, Kui-Lam.
1997.
Comparingrepresentations in Chinese informationretrieval.
In Proceedings of the 20th AnnualInternational ACM SIGIR Conference onResearch and Development in InformationRetrieval, pages 34?41, Philadelphia.Linguistic Data Consortium.
n.d. TRECMandarin-Text Retrieval ConferenceMandarin Newswire, LDC 2000T52.Majoros, William H., G. Mani Subramanian,and Mark Yandell.
2003.
Identification ofkey concepts in biomedical literatureusing a modified Markov heuristic.Bioinformatics, 19(3):402?407.Mo, Ruo-Ping J., Yao-Jung Yang, Keh-JiannChen, and Chu-Ren Huang.
1996.Determinative-measure compounds inMandarin Chinese: Formation rules andparser implementation.
In Readings in93Feng, Chen, Deng, and Zheng Accessor Variety Criteria for Chinese Word ExtractionChinese Natural Language Processing, Journalof Chinese Linguistics Monograph Seriesno.
9, edited by Chu-ren Huang et al,pages 123?146.Narasimhan, Giri, Changsong Bu, YuanGao, Xuning Wang, Ning Xu, and KalaiMathee.
2002.
Mining protein sequencesfor motifs.
Journal of Computational Biology,9(5):707?720.Pachunke, Thomas, Oliver Mertineit, KlausWothke, and Rudolf Schmidt.
1992.
Broadcoverage automatic morphologicalsegmentation of German words.
InProceedings of the 14th InternationalConference on Computational Linguistics(COLING?92), vol.
4, pages 1218?1222,Nantes, France.Paola, Merlo and Suzanne Stevenson.
2001.Automatic verb classification based onstatistical distribution of argumentstructure.
Computational Linguistics,27(3):373?408.Ricardo, Baeza-Yates and Ribeiro-NetoBerthier.
1999.
Modern Information Retrieval.ACM Press, Addison Wesley Longman.Sproat, Richard and Chilin Shih.
1990.
Astatistical method for finding wordboundaries in Chinese text.
ComputerProcessing of Chinese and Oriental Languages,4:336?351.Sproat, Richard, Chilin Shih, William Gale,and Nancy Chang.
1996.
A stochasticfinite-state word-segmentation algorithmfor Chinese.
Computational Linguistics,22(3):377?404.Teahan, William J., Yingying Wen, Rodger J.McNab, and Ian H. Witten.
2000.
Acompression-based algorithm for Chineseword segmentation.
ComputationalLinguistics, 26(3):375?393.Thijs, Gert, Kathleen Marchal, MagaliLescot, Stephane Rombauts, Bart DeMoor, Pierre Rouze, and Yves Moreau.2002.
A Gibbs sampling method to detectoverrepresented motifs in the upstreamregions of coexpressed genes.
Journal ofComputational Biology, 9(2):447?464.Wang, Yong-Heng, Hai-Ju Su, and Yan Mo.1990.
Automatic processing of Chinesewords.
Journal of Chinese InformationProcessing, 4(4):1?11.Weeber, Marc, Rein Vos, and R. HaraldBaayen.
2000.
Extracting the lowestfrequency words: Pitfalls and possibilities.Computational Linguistics, 26(3):301?317.Wu, Zimin and Gwyneth Tseng.
1993.Chinese text segmentation for textretrieval: Achievements and problems.Journal of the American Society forInformation Science, 44(9):532?542.Yamamoto, Mikio and Kenneth W. Church.2001.
Using suffix arrays to compute termfrequency and document frequency for allsubstrings in a corpus.
ComputationalLinguistics, 27(1):1?30.Yun, Bo-Hyun, Ho Lee, and Hae-ChangRim.
1995.
Analysis of Korean compoundnouns using statistical information.
InProceedings of the 1995 InternationalConference on Computer Processing ofOriental Languages (ICCPOL-95), Hawaii.Zhang, Jian, Jianfeng Gao, and Ming Zhou.2000.
Extraction of Chinese compoundwords?An experimental study on a verylarge corpus.
Proceedings of the SecondChinese Language Processing Workshop,pages 132?139, Hong Kong.
