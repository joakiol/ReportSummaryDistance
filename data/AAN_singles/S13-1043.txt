Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 306?316, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsAutomatically Identifying Implicit Arguments toImprove Argument Linking and Coherence ModelingMichael Roth and Anette FrankDepartment of Computational LinguisticsHeidelberg University, Germany{mroth,frank}@cl.uni-heidelberg.deAbstractImplicit arguments are a discourse-level phe-nomenon that has not been extensively stud-ied in semantic processing.
One reason forthis lies in the scarce amount of annotated datasets available.
We argue that more data ofthis kind would be helpful to improve exist-ing approaches to linking implicit argumentsin discourse and to enable more in-depth stud-ies of the phenomenon itself.
In this paper, wepresent a range of studies that empirically val-idate this claim.
Our contributions are three-fold: we present a heuristic approach to auto-matically identify implicit arguments and theirantecedents by exploiting comparable texts;we show how the induced data can be used astraining data for improving existing argumentlinking models; finally, we present a novel ap-proach to modeling local coherence that ex-tends previous approaches by taking into ac-count non-explicit entity references.1 IntroductionSemantic role labeling systems traditionally processtext in a sentence-by-sentence fashion, construct-ing local structures of semantic meaning (Palmer etal., 2010).
Information relevant to these structures,however, can be non-local in natural language texts(Palmer et al 1986; Fillmore, 1986, inter alia).
Inthis paper, we view instances of this phenomenon,also referred to as implicit arguments, as elementsof discourse.
In a coherent discourse, each utter-ance focuses on a salient set of entities, also called?foci?
(Sidner, 1979) or ?centers?
(Joshi and Kuhn,1979).
According to the theory of Centering (Groszet al 1995), the salience of an entity in a discourseis reflected by linguistic factors such as choice ofreferring expression and syntactic form.
Both ex-tremes of salience, i.e., contexts of referential conti-nuity (Brown, 1983) and irrelevance, can also be re-flected by the non-realization of an entity.
Altoughspecific instances of non-realization, so-called zeroanaphora, have been well-studied in discourse anal-ysis (Sag and Hankamer, 1984; Tanenhaus and Carl-son, 1990, inter alia), this phenomenon has widelybeen ignored in computational approaches to entity-based coherence modeling.
It could, however, pro-vide an explanation for local coherence in cases thatare not covered by current models of Centering (cf.Louis and Nenkova (2010)).
In this work, we pro-pose a new model to predict whether realizing anargument contributes to local coherence in a givenposition in discourse.
Example (1) shows a text frag-ment, in which argument realization is necessary inthe first sentence but redundant in the second.
(1) El Salvador is now the only Latin Ameri-can country which still has troops in [Iraq].Nicaragua, Honduras and the DominicanRepublic have withdrawn their troops [?
].From a semantic processing perspective, a humanreader can easily infer that ?Iraq?, the marked en-tity in the first sentence of Example (1), is also animplicit argument of the predicate ?withdraw?
in thesecond sentence.
This inference step is, however,difficult to model computationally as it involves aninterplay of two challenging sub-tasks: first, a se-mantic processor has to determine that an argumentis not realized (but inferrable); and second, a suit-306able antecedent has to be found within the discoursecontext.
For the remainder of this paper, we refer tothese steps as identifying and linking implicit argu-ments to discourse antecedents.As indicated by Example (1), implicit argumentsare an important aspect in semantic processing, yetthey are not captured in traditional semantic role la-beling systems.
The main reasons for this are thescarcity of annotated data, and the inherent difficultyof inferring discourse antecedents automatically.In this paper, we propose to induce implicit ar-guments and discourse antecedents by exploitingcomplementary (explicit) information obtained frommonolingual comparable texts (Section 3).
We ap-ply the empirically acquired data in argument link-ing (Section 4) and coherence modeling (Section 5).We conclude with a discussion on the advantages ofour data set and outline directions for future work(Section 6).2 Related workThe most prominent approach to entity-based coher-ence modeling nowadays is the entity grid model byBarzilay and Lapata (2005).
It has originally beenproposed for automatic sentence ordering but hasalso been applied in coherence evaluation and read-ability assessment (Barzilay and Lapata, 2008; Pitlerand Nenkova, 2008), and story generation (McIntyreand Lapata, 2009).
Based on the original model,a few extensions have been proposed: for exam-ple, Filippova and Strube (2007) and Elsner andCharniak (2011b) suggested additional features tocharacterize semantic relatedness between entitiesand features specific to single entities, respectively.Other entity-based approaches to coherence model-ing include the pronoun model by Charniak and El-sner (2009) and the discourse-new model by Elsnerand Charniak (2008).
All of these approaches are,however, based on explicitly realized entity men-tions only, ignoring references that are inferrable.The role of implicit arguments has been studiedearly on in the context of semantic processing (Fill-more, 1986; Palmer et al 1986).
Yet, the phe-nomenon has mostly been ignored in semantic rolelabeling.
First data sets, focusing on implicit argu-ments, have only recently become available: Rup-penhofer et al(2010) organized a SemEval sharedtask on ?linking events and participants in dis-course?, Gerber and Chai (2012) made available im-plicit argument annotations for the NomBank corpus(Meyers et al 2008) and Moor et al(2013) pro-vide annotations for parts of the OntoNotes corpus(Weischedel et al 2011).
However, these resourcesare very limited: The annotations by Moor et alandGerber and Chai are restricted to 5 and 10 predi-cate types, respectively.
The training set of the Se-mEval task contains only 245 resolved implicit argu-ments in total.
As pointed out by Silberer and Frank(2012), additional training data can be heuristicallycreated by treating anaphoric mentions as implicitarguments.
Their experimental results showed thatartificial training data can indeed improve results,but only when obtained from corpora with manualsemantic role annotations (on the sentence level) andgold coreference chains.3 Identifying and linking implicitargumentsThe aim of this work is to automatically constructa data set of implicit arguments and their discourseantecedents.
We propose an induction approach thatexploits complementary information obtained frompairs of comparable texts.
As a basis for this ap-proach, we rely on several preparatory steps pro-posed in the literature that first identify informa-tion two documents have in common (cf.
Figure 1).In particular, we align corresponding predicate-argument structures (PAS) using graph-based clus-tering (Roth and Frank, 2012b).
We then determineco-referring entities across the texts using corefer-ence resolution techniques on concatenated docu-ment pairs (Lee et al 2012).
These preprocessingsteps are described in more detail in Section 3.1.Given the preprocessed comparable texts andaligned PAS, we propose to heuristically iden-tify implicit arguments and link them to theirantecedents via the cross-document coreferencechains.
We describe the details of this approach inSection 3.2.3.1 Data preparationThe starting point for our approach is the data set ofautomatically aligned predicate pairs that has beenreleased by Roth and Frank (2012a).1 This data1cf.
http://www.cl.uni-heidelberg.de/%7Emroth/307Sentence that comprises a PAS with an (correctly predicted) implicit argument induced antecedentThe [?A0] [operatingA3] loss, as measured by .
.
.
widened to 189 million euros .
.
.
T-Online[?s]It was handed over to Mozambican control .
.
.
33 years after [?A0] independence.
Mozambique[?s].
.
.
[local officials A0] failed to immediately report [the accident A1] [?A2] .
.
.
[to] the governmentTable 1: Three positive examples of automatically induced implicit argument and antecedent pairs.Figure 1: Illustration of the induction approach: textsconsist of PAS (represented by overlapping circles);we exploit alignments between corresponding predicatesacross texts (marked by solid lines) and co-referring enti-ties (marked by dotted lines) to infer implicit arguments(marked by ?i?)
and link antecedents (curly dashed line)set, henceforth just R&F data, is a collection of283,588 predicate pairs that have been aligned ?withhigh precision?2 across comparable newswire arti-cles from the Gigaword corpus (Parker et al 2011).To use these documents for our argument induc-tion technique, we apply a couple of pre-processingtools on each single document and perform cross-document entity coreference on pairs of documents.Single document pre-processing.
We apply sev-eral preprocessing steps to all documents inthe R&F data: we use the Stanford CoreNLPpackage3 for tokenization and sentence split-ting.
We then apply MATE tools (Bohnet, 2010;Bjo?rkelund et al 2010), including the integratedPropBank/NomBank-style semantic parser, to re-construct local predicate-argument structures foraligned predicates.
Finally, we resolve pronouns thatoccur in a PAS using the coreference resolution sys-tem by Martschat et al(2012).2The used method achieved a precision of 86.2% at a recallof 29.1% on the Roth and Frank (2012a) test set.3http://nlp.stanford.edu/software/Cross-document coreference.
We apply cross-document coreference resolution to induce an-tecedents for implicit arguments.
In practice, weuse the Stanford Coreference System (Lee et al2013) and run it on pairs of texts by simply pro-viding a single document as input, comprising of aconcatenation of the two texts.
To perform this stepwith high precision, we only use the most preciseresolution sieves: ?String Match?, ?Relaxed StringMatch?, ?Precise Constructs?, ?Strict Head Match[A-C]?, and ?Proper Head Noun Match?.3.2 Identification and linking approachGiven a pair of aligned predicates from two compa-rable texts, we examine the parser output to identifythe arguments in each predicate-argument structure(PAS).
We compare the set of realized argument po-sitions in both structures to determine whether onePAS contains an argument position (explicit) thathas not been realized in the other PAS (implicit).For each implicit argument, we identify appropri-ate antecedents by considering the cross-documentcoreference chain of its explicit counterpart.
As ourgoal is to link arguments within discourse, we re-strict candidate antecedents to mentions that occurin the same document as the implicit argument.We apply a number of restrictions to the resultingpairs of implicit arguments and antecedents to mini-mize the impact of errors from preprocessing:- The aligned PAS should consist of a differentnumber of arguments (to minimize the impactof argument labeling errors)- The antecedent should not be a resolved pro-noun (to avoid errors resulting from incorrectpronoun resolution)- The antecedent should not be in the same sen-tence as the implicit argument (to circumventcases, in which an implicit argument is actu-ally explicit but has not been recognized by theparser)3083.3 Resulting data setWe apply the identification and linking approach tothe full R&F data set of aligned predicates.
As a re-sult, we induce a total of 701 implicit argument andantecedent pairs, each in a separate document, in-volving 535 different predicates.
Examples are dis-played in Table 1.
Note that 701 implicit argumentsfrom 283,588 pairs of predicate-argument structuresseem to represent a fairly low recall.
Most predicatepairs in the high precision data set of Roth and Frank(2012a) do, however, consist of identical argumentpositions (84.5%).
In the remaining cases, in whichan implicit argument can be identified (15.5%), anantecedent in discourse cannot always be found us-ing the high precision coreference sieves.
This doesnot mean that implicit arguments are a rare phe-nomenon in general.
In fact, 38.9% of all manuallyaligned predicate pairs in Roth and Frank (2012a)involved a different number of arguments.We manually evaluated a subset of 90 induced im-plicit arguments and found 80 discourse antecedentsto be correct (89%).
Some incorrectly linked in-stances still result from preprocessing errors.
In Ta-ble 2, we present a range of different error types thatoccurred when extracting implicit arguments with-out any restrictions.4 Experiment 1: Linking implicitargumentsOur first experiment assesses the utility of automat-ically induced implicit arguments and antecedentpairs for the task of implicit argument linking.
Forevaluation, we use the data sets from the SemEval2010 task on Linking Events and their Participantsin Discourse (Ruppenhofer et al 2010, henceforthjust SemEval).
For direct comparison with previousresults and heuristic acquisition techniques (cf.
Sec-tion 2), we apply the implicit argument identifica-tion and linking model by Silberer and Frank (2012,henceforth S&F) for training and testing.4.1 Task summaryBoth the training and test sets of the SemEval taskare text corpora extracted from Sherlock Holmesnovels, with manual frame semantic annotations in-cluding implicit arguments.
In the actual linkingtask (?NI-only?
), labels are provided for local argu-ments and participating systems have to perform thefollowing three sub-tasks: (1) identify implicit argu-ments (IA), (2) predict whether each IA is resolvableand, if so, (3) find an appropriate antecedent.The task organizers provide two versions of theirdata sets: one based on FrameNet annotations andone based on PropBank/NomBank annotations.
Wefound that the latter, however, only contains a sub-set of the implicit argument annotations from theFrameNet-based version.
As all previous results inthis task have been reported on the FrameNet dataset, we adopt the same setting.
Note that our addi-tional training data is automatically labeled with aPropBank/NomBank-style parser.
That is, we needto map our annotations to FrameNet.
The organizersof the SemEval shared task provide a manual map-ping dictionary for predicates in the annotated dataset.
We make use of this manual mapping and ad-ditionally use SemLink 1.14 for mapping predicatesand arguments not in the dictionary.4.2 Model detailsWe make use of the system by S&F to train a newmodel for the NI-only task.
As mentioned in the pre-vious sub-section, this task consists of three steps:In step (1), implicit arguments are identified as un-filled FrameNet core roles that are not competingwith roles that are already filled; in step (2), a SVMclassifier is used to predict whether implicit argu-ments are resolvable based on a small amount offeatures ?
semantic type of the affected Frame Ele-ment, the relative frequency of its realization type inthe SemEval training corpus, and a boolean featurethat indicates whether the affected sentence is in pas-sive voice and does not contain a (deep) subject.
Instep (3), we apply the same features and classifier asS&F, i.e., the BayesNet implementation from Weka(Witten and Frank, 2005), to find appropriate an-tecedents for (predicted) resolvable arguments.
S&Freport that their best results were obtained whenconsidering all entities as candidate antecedents thatare syntactic constituents from the present and thepast two sentences, or entities that occurred at leastfive times in the previous discourse (?Chains+Win?setting).
In their evaluation, the latter of these tworestrictions crucially depended on gold coreferencechains.
As the automatic coreference chains in our4http://verbs.colorado.edu/semlink/309Sentence that comprises a PAS with an (incorrectly predicted) implicit argument induced antecedent(1) ..
[Statistics?]
released [Tuesday TMP ] [?A0] showed the death toll dropped .
.
.
official statistics(2) A [French LOC?]
[?A0] draft resolution .
.
.
demands full .
.
.
compliance .
.
.
France(3) An earthquake .
.
.
is capable of causing .. [heavy EXT ] damage [?A2?]
majorTable 2: Examples of erroneous pairs of implicit arguments and antecedents.
In (1), the parser did not recognize?Statistics?
as an argument of showed; in (2), the parser mislabeled ?French?
as a locative modifier; both errors leadto incorrectly identified implicit arguments.
In (3), the implicit argument is correct but the wrong antecedent wasidentified because ?major?
had been mislabeled in the aligned predicate-argument structuredata are rather sparse (and noisy), we only considersyntactic constituents from the present and the pasttwo sentences as antecedents (?SentWin?
setting).Before training and testing a new model withour own data, we perform feature selection us-ing 10-fold cross validation.
We run the featureselection on a combination of the SemEval train-ing data and our additional data set in order tofind a set of features that generalizes best acrossthe two different corpora.
We found these to befeatures regarding ?prominence?, selectional pref-erences (?sp supersense?
), the POS tags of entitymentions, and semantic types of argument positions(?semType dni.entity?).
Note that the S&F systemdoes not make use of any lexicalized information.Instead, semantic features are computed based onthe highest abstraction level in WordNet (Fellbaum,1998).
For detailed description of all features, seeSilberer and Frank (2012).4.3 ResultsFor direct comparison in the full task, both withS&F?s model and other previously published results,we adopt the precision, recall and F1 measures asdefined in Ruppenhofer et al(2010).
We compareour results with those previously reported on the Se-mEval task (see Table 3 for a summary): Chen etal.
(2010) adapted SEMAFOR, the best performingsystem that participated in the actual task in 2010.Tonelli and Delmonte (2011) presented a revisedversion of their SemEval system (Tonelli and Del-monte, 2010), which outperformed SEMAFOR interms of recall (6%) and F1 score (8%).
The bestresults in terms of recall and F1 score up to datehave been reported by Laparra and Rigau (2012),with 25% and 19%, respectively.
Our model outper-forms their state-of-the-art system in terms of preci-sion (21%) but at a higher cost of recall (8%).
TwoP R FChen et al(2010)5 0.25 0.01 0.02Tonelli and Delmonte (2011) 0.13 0.06 0.08Laparra and Rigau (2012) 0.15 0.25 0.19Laparra and Rigau (2013) 0.14 0.18 0.16Gorinski et al(2013)6 0.14 0.12 0.13S&F (no additional data) 0.06 0.09 0.07S&F (best additional data) 0.09 0.11 0.10This paper 0.21 0.08 0.12Table 3: Results in terms of precision (P), recall (R) andF1 score (F) for identifying and linking implicit argu-ments in the SemEval test set.influencing factors for their high recall are probably(1) their improved method for identifying (resolv-able) implicit arguments, and (2) their addition oflexicalized and ontological features.Comparison to the original results reported byS&F, whose system we use, shows that our addi-tional data improves precision (from 6% to 21%)and F1 score (from 7% to 12%).
The loss in recallis marginal (-1%) given the size of the test set (259resolvable cases in total).
The result in precision isthe second highest score reported on this task.
Inter-estingly, the improvements are higher than those ofthe best training set used in the original study by Sil-berer and Frank (2012), even though their additionaldata set is three times bigger than ours and is basedon manual semantic annotations.
We conjecture thattheir low gain in precision could be a side effect trig-gered by two factors: on the one hand, their modelcrucially relies on coreference chains, which are au-tomatically generated for the test set and hence arerather noisy.
On the other hand, their heuristicallycreated training data might not represent implicit ar-gument instances adequately.3105 Experiment 2: Implicit arguments incoherence modelingIn our second experiment, we examine the effect ofimplicit arguments on local coherence, i.e., the ques-tion of how well a local argument (non-)realizationfits into a given context.
We approach this questionas follows: first, we assemble a data set of documentpairs that differ only with respect to a single realiza-tion decision (Section 5.1).
Given each pair in thisdata set, we ask human annotators to indicate theirpreference for the implicit or explicit argument re-alization in the pre-specified context (Section 5.2).Second, we attempt to emulate the decision pro-cess computationally using a discriminative modelbased on discourse and entity-specific features (Sec-tion 5.3).5.1 Data compilationWe use the induced data set (henceforth sourcedata), as described in Section 3, as a starting pointfor composing a set of document pairs that involveimplicit and explicit arguments.
To make sure thateach document pair in this data set only differs withrespect to a single realization decision, we first cre-ate two copies of each document from the sourcedata: one copy remains in its original form, and theother copy will be modified with respect to a sin-gle argument realization.
Example (2) illustrates anexample of an original and modified (marked by anasterik) sentence:(2) [The Dalai Lama?sA0] visit [to FranceA1] endson Tuesday.
* [The Dalai Lama?sA0] visit ends on Tuesday.Note that adding and removing arguments at ran-dom can lead to structures that are semanticallyimplausible.
Hence, we restrict this procedure topredicate-argument structures (PAS) that actuallyoccur and are aligned across two texts, and createmodifications by replacing a single argument posi-tion in one text with the corresponding argument po-sition in the comparable text.
Examples (2) and (3)5Results as reported in Tonelli and Delmonte (2011)6Results computed as an average over the scores given forboth test files; rounded towards the number given for the testfile that contained more instances.show two such comparable texts.
The original PASin Example (2) contains an explicit argument that isimplicit in the aligned PAS and hence removed inthe modified version.
Vice versa, the original textin (3) involves an implicit argument, which is madeexplicit in the modified version.
(3) [The Dalai Lama?sA0] visit coincides with theBeijing Olympics.
* [The Dalai Lama?sA0] visit [to FranceA1] co-incides with the Beijing Olympics.We ensure that the modified structure fits intothe given context grammatically by only consid-ering PAS with identical predicate form and con-stituent order.
We found that this restriction con-strains affected arguments to be modifiers, prepo-sitional phrases and direct objects.
We argue thatthis is actually a desirable property because morecomplicated alternations could affect coherence bythemselves; resulting interplays would make it diffi-cult to distinguish between the isolated effect of ar-gument realization itself and other effects, triggeredfor example by sentence order (Gordon et al 1993).5.2 AnnotationWe set up a web experiment using the NLTK pack-age (Belz and Kow, 2011) to collect (local) coher-ence ratings for implicit and explicit arguments.
Forthis experiment, we compiled a data set of 150 doc-ument pairs.
As described in Section 5.1, each textpair consists of mostly the same text, with the onlydifference being one argument realization.We presented all 150 pairs to two annotators7 andasked them to indicate their preference for one al-ternative over the other using a continuous sliderscale.
The annotators got to see the full texts, withthe alternatives presented next to each other.
Tomake texts easier to read and differences easier tospot, we collapsed all identical sentences into onecolumn and highlighted the aligned predicate (inboth texts) and the affected argument (in the explicitcase).
An example is shown in Figure 2.
To avoidany bias in the annotation process, we shuffled thesequence of text pairs and randomly assigned theside of display (left/right) of each realization type7Both annotators are undergraduate students in Computa-tional Linguistics.311Figure 2: Texts as displayed to the annotators.(explicit/implicit).
Note that instead of providing adefinition of local coherence ourselves, we simplyasked the annotators to rate how ?natural?
a realiza-tion sounds given the discourse context.We found that annotators made use of the full rat-ing scale, which spans from -50 to +50, with the ex-tremes indicating either a strong preference for thetext on the left hand side or the right hand side, re-spectively.
Most ratings are, however, concentratedmore towards the center of the scale (i.e., aroundzero).
This seems to imply that the use of im-plicit or explicit arguments did not make a consid-erable difference most of the time.
The first authorconfirmed this assumption and resolved disagree-ments between annotators in several group discus-sions.
The annotators also affirmed that some casesdo not read naturally when a specific argument is oris not realized at a given position in discourse.
Ex-amples (4) and (5) illustrate two cases, in which aredundant argument is realized (A4, or destination)or a coherence establishing argument has been omit-ted (A2, or co-signer).8(4) ?
The remaining contraband was picked up atLe Havre.
The containers had arrived [inLe Havre] from China.
(5) ?
Lt.-Gen. Mohamed Lamari (.
.
. )
deniedhis country wanted South African weaponsto fight Muslim rebels fighting the govern-ment.
?We are not going to fight a flea with8Note that both examples are only excerpts from the affectedtexts.
The annotators got to see the full context.a hammer,?
Lamari told reporters after sign-ing the agreement of intent [?
].Following discussions with the annotators, wediscarded all items from the final data set, for whichno clear preference could be established (72%) orthe annotators had different preferences (9%).
Wemapped all remaining items into two classes accord-ing to whether the affected argument had to be im-plicit (9 texts) or explicit (20 texts).
All 29 uniquelyclassified texts are used as a small gold standard testset for evaluation.5.3 Coherence modelWe model the decision process that underlies the(non-)realization of arguments using a SVM classi-fier and a range of discourse features.
The featurescan be classified into three groups: features specificto the affected predicate-argument structure (Parg),the (automatic) coreference chain of the affected ar-gument (Coref), and the discourse context (Disc).Parg includes the absolute and relative number ofrealized arguments; the number of modifiers in thePAS; and the total length (in words) of the PAS andthe complete sentence.Coref includes the number of previous/follow-upmentions in a fixed sentence window; the distance(in number of words/sentences) to the previous/nextmention; the distribution of occurrences over theprevious/succeeding two sentences;9 and the POS ofprevious/follow-up mentions.Disc includes the total number of coreferencechains in the text; the occurrence of pronounsin the current sentence; lexical repetitions in theprevious/follow-up sentence; the current position indiscourse (begin, middle, end); and a feature indi-cating whether the affected argument occured in thefirst sentence.Note that most of these features overlap withthose successfully applied in previous work.
Forexample, Pitler and Nenkova (2008) also use text9This type of feature is very similar to the transition pat-terns in the original entity grid.
The only difference is that ourfeatures are not typed with respect to the grammatical functionof explicit realizations.
The reason for skipping this informa-tion lies in the insignificant amount of relevant samples in our(noisy) training data.312length, sentence-to-sentence transitions, word over-lap and pronoun occurrences as features for predict-ing readability.
Our own contribution lies in the defi-nition of PAS-specific features and the adaptation ofall features to the task of predicting (non-)realizationof arguments in a predicate-argument structure.5.4 Training dataWe do not make use of any manually annotated datafor training.
Instead, our model relies solely on theautomatically induced source data, described in Sec-tion 3, for learning.
We prepare this data set as fol-lows: first, we remove all data points that also occurin the test set.
Second, we split all pairs of texts intotwo groups ?
texts that contain a predicate-argumentstructure in which an implicit argument has beenidentified (IA), and their comparable counterpartsthat contain the aligned PAS with an explicit argu-ment (EA).
All texts are labelled according to theirgroup.
For all texts in group EA, we remove the ex-plicit argument from the aligned PAS.
This way, thefeature extractor always gets to see the text and au-tomatic annotations as if the realization decision hadnot been performed and can thus extract unbiasedfeature values for the affected entity and argumentposition.5.5 Evaluation settingThe goal of this task is to correctly predict the re-alization type (implicit or explicit) of an argumentthat maximizes the coherence of the document.
Asa proxy for coherence, we use the naturalness rat-ings given by our annotators.
We evaluate classifica-tion performance on the part of our test set for whichclear preferences have been established.
We reportresults in terms of precision, recall and F1 score.
Wecompute precision as the fraction of correct classifierdecisions divided by the total number of classifica-tions; and recall as the fraction of correct classifierdecisions divided by the total number of test items.Note that precision and recall are identical when themodel provides a class label for every test item.
Wecompute F1 as the harmonic mean between precisionand recall.For comparison with previous work, we furtherapply a couple of previously proposed local co-herence models: the original entity grid model byBarzilay and Lapata (2005), a modified version thatuses topic models (Elsner and Charniak, 2011a) andan extended version that includes entity-specific fea-tures (Elsner and Charniak, 2011b).
We further ap-ply the discourse-new model by Elsner and Charniak(2008) and the pronoun-based model by Charniakand Elsner (2009).
For all of the aforementionedmodels, we use their respective implementation pro-vided with the Brown Coherence Toolkit10.
Notethat the toolkit only returns one coherence score foreach document.
To use the toolkit for argument clas-sification, we use two documents per data point ?one that contains the affected argument explicitlyand one that does not (implicit argument) ?
and treatthe higher scoring variant as classification output.
Ifboth documents achieve the same score, we neithercount the test item as correctly nor as incorrectlyclassified.
In contrast, we apply our own model onlyon the document that contains the implicit argument,and use the classifier to predict whether this realiza-tion type fits into the given context or not.
Note thatour model has an advantage here because it is specif-ically designed for this task.
Yet, all models com-pute local coherence ratings based on entity occur-rences and should thus be able to predict which re-alization type coheres best with the given discoursecontext.115.6 ResultsThe results are summarized in Table 4.
As all mod-els provided class labels for almost all test instances,we focus our discussion on F1 scores.
The majorityclass in our test set is the explicit realization type,making up 20 of the 29 test items (69%).The original entity grid model produced differingscores for the two realization types only in 26 cases.The model exhibits a strong preference for the im-plicit realization type: it predicts this class in 22cases, resulting in an F1 score of only 15%.
Tak-ing a closer look at the features of the model revealsthat this an expected outcome: in its original set-ting, the entity grid learns realization patterns in theform of sentence-to-sentence transitions.
Most enti-ties are, however, only mentioned a few times in a10cf.
http://www.ling.ohio-state.edu/%7Emelsner/11Recall that input document pairs are identical except for theaffected argument position.
Consequently, the resulting coher-ence scores only differ with respect to affected entity realiza-tions.313P R FEntity grid models ?
?
?Baseline entity grid 0.15** 0.14** 0.15**Extended entity grid 0.19** 0.17** 0.18**Topical entity grid 0.34** 0.34** 0.34**Other models ?
?
?Pronouns 0.43** 0.34** 0.38**Discourse-newness 0.48** 0.48** 0.48**This paper ?
?
?Our (full) model 0.90 0.90 0.90Simplified model 0.83 0.83 0.83Majority class 0.69* 0.69* 0.69*Table 4: Results in terms of precision (P), recall (R) andF1 score for correctly predicting argument realization; re-sults that significantly differ from our (full) model aremarked with asterisks (* p<0.1; ** p<0.01)text, which means that non-realizations make up the?most probable?
class ?
independently of whetherthey are relevant in a given context or not.
The mod-els by Charniak and Elsner (2009) and Elsner andCharniak (2011a), which are not based on an entitygrid, do not suffer from this effect and achieve bet-ter results, with F1 scores of 38% and 48%, respec-tively.
The topical and entity-specific refinements tothe entity grid model also alleviate the bias towardsnon-realizations, resulting in improved F1 scores of18% and 34%, respectively.To counter-balance this issue altogether, we traina simplified version of our own model that onlyuses features that involve occurrence patterns.
Themain difference between this simplified model andthe original entity grid model lies in the differentuse of training data: while entity grid models treatall non-realized items equally, our model gets to?see?
actual examples of entities that are implicit.In other words, our simplified model takes into ac-count implicit mentions of entities, not only explicitones.
The results show that this extra informationhas a significant (p<0.01, using a randomization test(Yeh, 2000)) impact on test set performance, basi-cally raising F1 from 15% to 83%.
Using all featuresof our model further increases F1 score to 90%, thehighest score achieved overall.The highest weighted features in our model in-clude all three feature groups: for example, thenumber of coreferent mentions within the preceed-ing/following two sentences (Coref), the numberof words already realized in the affected predicate-argument structure (Parg), and the total number ofcoreference chains in the document (Disc).6 ConclusionsIn this paper, we presented a novel approach to ac-curately induce implicit arguments and discourse an-tecedents from comparable texts (cf.
Section 3).
Wedemonstrated the benefit of this kind of data for link-ing implicit arguments and modeling local coher-ence.
Our experiments revealed three particularlyinteresting results.Firstly, a small data set of (automatically induced)implicit arguments can have a greater impact on ar-gument linking models than a bigger data set of ar-tificially created instances (cf.
Section 4).
Secondly,the use of implicit vs. explicit arguments, while be-ing a subtle difference in most contexts, can have aclear impact on text ratings.
Thirdly, our automat-ically created training data enables models to learnfeatures that considerably improve prediction of lo-cally coherent argument realizations (cf.
Section 5).For the task of implicit argument linking, moretraining data will be needed to further advancethe state-of-the-art.
Our method for inducingthis kind of data, by exploiting aligned predicate-argument structures from comparable texts, hasshown promising results.
Future work will haveto explore this direction more fully, for example,by identifying ways to induce data with higher re-call.
Integrating argument (non-)realization into afull model of local coherence also remains part offuture work.
In this paper, we presented a suitablebasis for such work: a training set that contains em-pirical data on implicit arguments in discourse; anda feature set that models argument realization withhigh accuracy.AcknowledgmentsWe are grateful to the Landesgraduiertenfo?rderungBaden-Wu?rttemberg for funding within the researchinitiative ?Coherence in language processing?
atHeidelberg University.
We thank our annotators andfour anonymous reviewers.314ReferencesRegina Barzilay and Mirella Lapata.
2005.
Modelinglocal coherence: An entity-based approach.
In Pro-ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics, Ann Arbor, Michi-gan, USA, 25?30 June 2005, pages 141?148.Regina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.
Computa-tional Linguistics, 34(1):1?34.Anja Belz and Eric Kow.
2011.
Discrete vs. continuousrating scales for language evaluation in nlp.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 230?235, Portland, Oregon, USA,June.Anders Bjo?rkelund, Bernd Bohnet, Love Hafdell, andPierre Nugues.
2010.
A high-performance syntac-tic and semantic dependency parser.
In Coling 2010:Demonstration Volume, pages 33?36, Beijing, China,August.
Coling 2010 Organizing Committee.Bernd Bohnet.
2010.
Top accuracy and fast dependencyparsing is not a contradiction.
In Proceedings of the23rd International Conference on Computational Lin-guistics (Coling 2010), pages 89?97, Beijing, China,August.Cheryl Brown.
1983.
Topic continuity in written englishnarrative.
In Talmy Givon, editor, Topic Continuityin Discourse: A Quantitative Cross-Language Study.John Benjamins, Amsterdam, The Netherlands.Eugene Charniak and Micha Elsner.
2009.
EM worksfor pronoun anaphora resolution.
In Proceedings ofthe 12th Conference of the European Chapter of theACL (EACL 2009), pages 148?156, Athens, Greece,March.Desai Chen, Nathan Schneider, Dipanjan Das, andNoah A. Smith.
2010.
SEMAFOR: Frame argumentresolution with log-linear models.
In Proceedings ofthe 5th International Workshop on Semantic Evalua-tion, pages 264?267, Uppsala, Sweden, July.Micha Elsner and Eugene Charniak.
2008.
Coreference-inspired coherence modeling.
In Proceedings of ACL-08: HLT, Short Papers, pages 41?44, Columbus, Ohio,June.Micha Elsner and Eugene Charniak.
2011a.
Disentan-gling chat with local coherence models.
In Proceed-ings of the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies, pages 1179?1189, Portland, Oregon, USA,June.Micha Elsner and Eugene Charniak.
2011b.
Extendingthe entity grid with entity-specific features.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 125?129, Portland, Oregon, USA,June.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge, Mas-sachusetts, USA.Katja Filippova and Michael Strube.
2007.
Extendingthe entity-grid coherence model to semantically re-lated entities.
In Proceedings of the 11th EuropeanWorkshop on Natural Language Generation, SchlossDagstuhl, Germany, 17?20 June 2007, pages 139?142.C.
J. Fillmore.
1986.
Pragmatically controlled zeroanaphora.
In Proceedings of the twelfth annual meet-ing of the Berkeley Linguistics Society, pages 95?107.Matthew Gerber and Joyce Chai.
2012.
Semantic RoleLabeling of Implicit Arguments for Nominal Predi-cates.
Computational Linguistics, 38(4):755?798.Peter C. Gordon, Barbara J. Grosz, and Laura A. Gilliom.1993.
Pronouns, names, and the centering of attentionin discourse.
Cognitive Science, 17:311?347.Philip Gorinski, Josef Ruppenhofer, and CarolineSporleder.
2013.
Towards weakly supervised resolu-tion of null instantiations.
In Proceedings of the 10thInternational Conference on Computational Semantics(IWCS 2013) ?
Long Papers, pages 119?130, Potsdam,Germany, March.Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.1995.
Centering: A framework for modeling the lo-cal coherence of discourse.
Computational Linguis-tics, 21(2):203?225.Aravind K. Joshi and Steve Kuhn.
1979.
Centered logic:The role of entity centered sentence representation innatural language inferencing.
In Proceedings of the6th International Joint Conference on Artificial Intel-ligence, Tokyo, Japan, August, pages 435?439.Egoitz Laparra and German Rigau.
2012.
Exploiting ex-plicit annotations and semantic types for implicit argu-ment resolution.
In Proceedings of the Sixth IEEE In-ternational Conference on Semantic Computing (ICSC2010), pages 75?78, Palermo, Italy, September.
IEEEComputer Society.Egoitz Laparra and German Rigau.
2013.
Sources of ev-idence for implicit argument resolution.
In Proceed-ings of the 10th International Conference on Compu-tational Semantics (IWCS 2013) ?
Long Papers, pages155?166, Potsdam, Germany, March.Heeyoung Lee, Marta Recasens, Angel Chang, MihaiSurdeanu, and Dan Jurafsky.
2012.
Joint entity andevent coreference resolution across documents.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, pages489?500, Jeju Island, Korea, July.Heeyoung Lee, Angel Chang, Yves Peirsman, NathanaelChambers, Mihai Surdeanu, and Dan Jurafsky.
2013.315Deterministic coreference resolution based on entity-centric, precision-ranked rules.
Computational Lin-guistics, 39(4).
Accepted for publication.Annie Louis and Ani Nenkova.
2010.
Creating localcoherence: An empirical assessment.
In Human Lan-guage Technologies: The 2010 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 313?316, Los An-geles, California, June.Sebastian Martschat, Jie Cai, Samuel Broscheit, E?vaMu?jdricza-Maydt, and Michael Strube.
2012.
Amultigraph model for coreference resolution.
In JointConference on EMNLP and CoNLL - Shared Task,pages 100?106, Jeju Island, Korea, July.Neil McIntyre and Mirella Lapata.
2009.
Learning totell tales: A data-driven approach to story generation.In Proceedings of the Joint Conference of the 47thAnnual Meeting of the Association for ComputationalLinguistics and the 4th International Joint Conferenceon Natural Language Processing, Singapore, 2?7 Au-gust 2009, pages 217?225.Adam Meyers, Ruth Reeves, and Catherine Macleod.2008.
NomBank v1.0.
Linguistic Data Consortium,Philadelphia.Tatjana Moor, Michael Roth, and Anette Frank.
2013.Predicate-specific annotations for implicit role bind-ing: Corpus annotation, data analysis and evaluationexperiments.
In Proceedings of the 10th InternationalConference on Computational Semantics (IWCS 2013)?
Short Papers, pages 369?375, Potsdam, Germany,March.Martha S. Palmer, Deborah A. Dahl, Rebecca J. Schiff-man, Lynette Hirschman, Marcia Linebarger, and JohnDowding.
1986.
Recovering implicit information.
InProceedings of the 24th Annual Meeting of the Associ-ation for Computational Linguistics, New York, N.Y.,10?13 June 1986, pages 10?19.Martha Palmer, Daniel Gildea, and Nianwen Xue.
2010.Synthesis Lectures on Human Language Technolo-gies.
Morgan & Claypool.Robert Parker, David Graff, Jumbo Kong, Ke Chen, andKazuaki Maeda.
2011.
English Gigaword Fifth Edi-tion.
Linguistic Data Consortium, Philadelphia.Emily Pitler and Ani Nenkova.
2008.
Revisiting read-ability: A unified framework for predicting text qual-ity.
In Proceedings of the 2008 Conference on Empir-ical Methods in Natural Language Processing, pages186?195, Honolulu, Hawaii, October.Michael Roth and Anette Frank.
2012a.
Aligning pred-icate argument structures in monolingual comparabletexts: A new corpus for a new task.
In Proceedingsof the First Joint Conference on Lexical and Computa-tional Semantics, pages 218?227, Montreal, Canada,June.Michael Roth and Anette Frank.
2012b.
Aligningpredicates across monolingual comparable texts us-ing graph-based clustering.
In Proceedings of the2012 Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning, pages 171?182, Jeju Island, Ko-rea, July.Josef Ruppenhofer, Caroline Sporleder, Roser Morante,Collin Baker, and Martha Palmer.
2010.
SemEval-2010 Task 10: Linking Events and Their Participantsin Discourse.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluations, pages 45?50, Up-psala, Sweden, July.Ivan A.
Sag and Jorge Hankamer.
1984.
Towards a The-ory of Anaphoric Processing.
Linguistics and Philos-ophy, 7:325?345.Candace L. Sidner.
1979.
Towards a computational the-ory of definite anaphora comprehension in English.Technical Report AI-Memo 537, Massachusetts Insti-tute of Technology, AI Lab, Cambridge, Mass.Carina Silberer and Anette Frank.
2012.
Casting implicitrole linking as an anaphora resolution task.
In Pro-ceedings of the First Joint Conference on Lexical andComputational Semantics (*SEM 2012), pages 1?10,Montre?al, Canada, 7-8 June.Michael K. Tanenhaus and Greg N. Carlson.
1990.
Com-prehension of Deep and Surface Verbphrase Anaphors.Language and Cognitive Processes, 5(4):257?280.Sara Tonelli and Rodolfo Delmonte.
2010.
VENSES++:Adapting a deep semantic processing system to theidentification of null instantiations.
In Proceedings ofthe 5th International Workshop on Semantic Evalua-tion, pages 296?299, Uppsala, Sweden, July.Sara Tonelli and Rodolfo Delmonte.
2011.
Desperatelyseeking implicit arguments in text.
In Proceedings ofthe ACL 2011 Workshop on Relational Models of Se-mantics, pages 54?62, Portland, Oregon, USA, June.Ralph Weischedel, Martha Palmer, Mitchell Marcus, Ed-uard Hovy, Sameer Pradhan, Lance Ramshaw, Nian-wen Xue, Ann Taylor, Jeff Kaufman, Michelle Fran-chini, Mohammed El-Bachouti, Robert Belvin, andAnn Houston.
2011.
OntoNotes Release 4.0.
Lin-guistic Data Consortium, Philadelphia.Ian H. Witten and Eibe Frank.
2005.
Data Mining: Prac-tical Machine Learning Tools and Techniques.
Mor-gan Kaufmann, San Francisco, California, USA, 2ndedition.Alexander Yeh.
2000.
More accurate tests for the sta-tistical significance of result differences.
In Proceed-ings of the 18th International Conference on Computa-tional Linguistics, pages 947?953, Saarbru?cken, Ger-many, August.316
