Automatic Estimation of Word Significance oriented forSpeech-based Information RetrievalTakashi ShichiriGraduate School of Science and Tech.Ryukoku UniversitySeta, Otsu 520-2194, Japanshichiri@nlp.i.ryukoku.ac.jpHiroaki NanjoFaculty of Science and Tech.Ryukoku UniversitySeta, Otsu 520-2194, Japannanjo@nlp.i.ryukoku.ac.jpTakehiko YoshimiFaculty of Science and Tech.Ryukoku UniversitySeta, Otsu 520-2194, Japanyoshimi@nlp.i.ryukoku.ac.jpAbstractAutomatic estimation of word significanceoriented for speech-based Information Re-trieval (IR) is addressed.
Since the sig-nificance of words differs in IR, automaticspeech recognition (ASR) performance hasbeen evaluated based on weighted word er-ror rate (WWER), which gives a weighton errors from the viewpoint of IR, insteadof word error rate (WER), which treats allwords uniformly.
A decoding strategy thatminimizes WWER based on a MinimumBayes-Risk framework has been shown, andthe reduction of errors on both ASR and IRhas been reported.
In this paper, we proposean automatic estimation method for wordsignificance (weights) based on its influenceon IR.
Specifically, weights are estimated sothat evaluation measures of ASR and IR areequivalent.
We apply the proposed methodto a speech-based information retrieval sys-tem, which is a typical IR system, and showthat the method works well.1 IntroductionBased on the progress of spoken language process-ing, the main target of speech processing has shiftedfrom speech recognition to speech understanding.Since speech-based information retrieval (IR) mustextract user intention from speech queries, it is thusa typical speech understanding task.
IR typicallysearches for appropriate documents such as news-paper articles or Web pages using statistical match-ing for a given query.
To define the similarity be-tween a query and documents, the word vector spacemodel or ?bag-of-words?
model is widely adopted,and such statistics as the TF-IDF measure are intro-duced to consider the significance of words in thematching.
Therefore, when using automatic speechrecognition (ASR) as a front-end of such IR systems,the significance of the words should be considered inASR; words that greatly affect IR performance mustbe detected with higher priority.Based on such a background, ASR evaluationshould be done from the viewpoint of the qualityof mis-recognized words instead of quantity.
Fromthis point of view, word error rate (WER), which isthe most widely used evaluation measure of ASRaccuracy, is not an appropriate evaluation measurewhen we want to use ASR systems for IR becauseall words are treated identically in WER.
Insteadof WER, weighted WER (WWER), which consid-ers the significance of words from a viewpoint ofIR, has been proposed as an evaluation measure forASR.
Nanjo et.al showed that the ASR based onthe Minimum Bayes-Risk framework could reduceWWER and the WWER reduction was effective forkey-sentence indexing and IR (H.Nanjo et al, 2005).To exploit ASR which minimizes WWER for IR,we should appropriately define weights of words.Ideal weights would give a WWER equivalent toIR performance degradation when a correspondingASR result is used as a query for the IR system.
Af-ter obtaining such weights, we can predict IR degra-dation by simply evaluating ASR accuracy, and thus,minimum WWER decoding (ASR) will be the mosteffective for IR.204For well-defined IRs such as relational databaseretrieval (E.Levin et al, 2000), significant words(=keywords) are obvious.
On the contrary, de-termining significant words for more general IRtask (T.Misu et al, 2004) (C.Hori et al, 2003) is noteasy.
Moreover, even if significant words are given,the weight of each word is not clear.
To properlyand easily integrate the ASR system into an IR sys-tem, the weights of words should be determined au-tomatically.
Conventionally, they are determined byan experienced system designer.
Actually, in con-ventional studies of minimum WWER decoding forkey-sentence indexing (H.Nanjo and T.Kawahara,2005) and IR (H.Nanjo et al, 2005), weights weredefined based on TF-IDF values used in back-endindexing or IR systems.
These values reflect wordsignificance for IR, but are used without having beenproven suitable for IR-oriented ASR.
In this paper,we propose an automatic estimation method of wordweights based on the influences on IR.2 Evaluation Measure of ASR for IR2.1 Weighted Word Error Rate (WWER)The conventional ASR evaluation measure, namely,word error rate (WER), is defined as Equation (1).WER =I + D + SN(1)Here, N is the number of words in the correct tran-script, I is the number of incorrectly inserted words,D is the number of deletion errors, and S is the num-ber of substitution errors.
For each utterance, DPmatching of the ASR result and the correct transcriptis performed to identify the correct words and calcu-late WER.Apparently in WER, all words are treated uni-formly or with the same weight.
However, theremust be a difference in the weight of errors, sinceseveral keywords have more impact on IR or theunderstanding of the speech than trivial functionalwords.
Based on the background, WER is gener-alize and weighted WER (WWER), in which eachword has a different weight that reflects its influenceASR result : a b c d e fCorrect transcript : a c d?
f gDP result : C I C S C DWWER = (VI + VD + VS)/VNVN = va + vc + vd?
+ vf + vg, VI = vbVD = vg, VS = max(vd + ve, vd?
)vi: weight of word iFigure 1: Example of WWER calculationon IR, is introduced.
WWER is defined as follows.WWER =VI + VD + VSVN(2)VN = ?wivwi(3)VI = ?w?i?I vw?i(4)VD = ?wi?D vwi(5)VS = ?segj?S vsegj(6)vsegj= max(?w?i?segjvw?i, ?wi?segjvwi)Here, vwiis the weight of word wi, which is the i-thword of the correct transcript, and vw?iis the weightof word w?i, which is the i-th word of the ASR re-sult.
segj represents the j-th substituted segment,and vsegjis the weight of segment segj .
For seg-ment segj , the total weight of the correct words andthe recognized words are calculated, and then thelarger one is used as vsegj.
In this work, we usealignment for WER to identify the correct words andcalculate WWER.
Thus, WWER equals WER if allword weights are set to 1.
In Fig.
1, an example of aWWER calculation is shown.WWER calculated based on ideal word weightsrepresents IR performance degradation when theASR result is used as a query for IR.
Thus, we mustperform ASR to minimize WWER for speech-basedIR.2.2 Minimum Bayes-Risk DecodingNext, a decoding strategy to minimizeWWER basedon the Minimum Bayes-Risk framework (V.Goel etal., 1998) is described.In Bayesian decision theory, ASR is describedwith a decision rule ?
(X): X ?
W?
.
Using a real-valued loss function l(W, ?
(X)) = l(W, W ?
), the205decision rule minimizing Bayes-risk is given as fol-lows.
It is equivalent to the orthodox ASR (maxi-mum likelihood ASR) when a 0/1 loss function isused.?
(X) =argminW?W ?l(W, W ?)
?
P (W ?|X) (7)The minimization of WWER is realized us-ing WWER as a loss function (H.Nanjo andT.Kawahara, 2005) (H.Nanjo et al, 2005).3 Estimation of Word WeightsA word weight should be defined based on its in-fluence on IR.
Specifically, weights are estimatedso that WWER will be equivalent to an IR perfor-mance degradation.
For an evaluation measure of IRperformance degradation, IR score degradation ratio(IRDR), which is described in detail in Section 4.2,is introduced in this work.
The estimation of weightsis performed as follows.1.
Query pairs of a spoken-query recognition re-sult and its correct transcript are set as trainingdata.
For each query pair m, do procedures 2to 5.2.
Perform IR with a correct transcript and calcu-late IR score Rm.3.
Perform IR with a spoken-query ASR resultand calculate IR score Hm.4.
Calculate IR score degradation ratio(IRDRm = 1 ?
HmRm).5.
Calculate WWERm.6.
Estimate word weights so that WWERm andIRDRm are equivalent for all queries.Practically, procedure 6 is defined to minimize themean square error between both evaluation mea-sures (WWER and IRDR) as follows.F (x) =?m(Em(x)Cm(x)?
IRDRm)2?
min (8)Here, x is a vector that consists of the weights ofwords.
Em(x) is a function that determines the sumof the weights of mis-recognized words.
Cm(x) isa function that determines the sum of the weightsof the correct transcript.
Em(x) and Cm(x) corre-spond to the numerator and denominator of Equation(2), respectively.In this work, we adopt the steepest decent methodto determine the weights that give minimal F (x).Initially, all weights are set to 1, and then each wordweight (xk) is iteratively updated based on Equation(9) until the mean square error between WWER andIRDR is converged.xk?
=??????????
?xk ?
?
if?F?xk> 0xk + ?
else if?F?xk< 0xk otherwise(9)where?F?xk=?m2(EmCm?IRDRm)?
(EmCm?IRDRm)?=?m2(EmCm?IRDRm)?E?m ?
Cm ?
Em ?
C?mC2m=?m2(EmCm?IRDRm)?1Cm(E?m?C?m?EmCm)=?m2Cm(WWERm?IRDRm)(E?m?C?m?WWERm)4 Weight Estimation on Orthodox IR4.1 WEB Page RetrievalIn this paper, weight estimation is evaluated withan orthodox IR system that searches for appropri-ate documents using statistical matching for a givenquery.
The similarity between a query and docu-ments is defined by the inner product of the featurevectors of the query and the specific document.
Inthis work, a feature vector that consists of TF-IDFvalues is used.
The TF-IDF value is calculated foreach word t and document (query) i as follows.TF-IDF(t, i) =tft,iDLiavglen + tft,i?
logNdft(10)Here, term frequency tft,i represents the occur-rence counts of word t in a specific document i, anddocument frequency dft represents the total number206of documents that contain word t. A word that oc-curs frequently in a specific document and rarely oc-curs in other documents has a large TF-IDF value.We normalize TF values using length of the docu-ment (DLi) and average document lengths over alldocuments (avglen) because longer document havemore words and TF values tend to be larger.For evaluation data, web retrieval task ?NTCIR-3WEB task?, which is distributed by NTCIR (NTC, ),is used.
The data include web pages to be searched,queries, and answer sets.
For speech-based informa-tion retrieval, 470 query utterances by 10 speakersare also included.4.2 Evaluation Measure of IRFor an evaluation measure of IR, discount cumula-tive gain (DCG) is used, and described below.DCG(i) =??
?g(1) if i = 1DCG(i ?
1) + g(i)log(i)otherwise(11)g(i) =??????????
?h if di ?
Ha else if di ?
Ab else if di ?
Bc otherwiseHere, di represents i-th retrieval result (docu-ment).
H, A, and B represent a degree of relevance;H is labeled to documents that are highly relevant tothe query.
A and B are labeled to documents that arerelevant and partially relevant to the query, respec-tively.
?h?, ?a?, ?b?, and ?c?
are the gains, and in thiswork, (h, a, b, c) = (3, 2, 1, 0) is adopted.
When re-trieved documents include many relevant documentsthat are ranked higher, the DCG score increases.In this work, word weights are estimated so thatWWER and IR performance degradation will beequivalent.
For an evaluation measure of IR perfor-mance degradation, we define IR score degradationratio (IRDR) as below.IRDR = 1 ?HR(12)R represents a DCG score calculated with IR resultsby text query, and H represents a DCG score givenby the ASR result of the spoken query.
IRDR repre-sents the ratio of DCG score degradation affected byASR errors.4.3 Automatic speech recognition systemIn this paper, ASR system is set up with follow-ing acoustic model, language model and a decoderJulius rev.3.4.2(A.Lee et al, 2001).
As for acous-tic model, gender independent monophone model(129 states, 16 mixtures) trained with JNAS corpusare used.
Speech analysis is performed every 10msec.
and a 25 dimensional parameter is computed(12 MFCC + 12?MFCC + ?Power).
For languagemodel, a word trigram model with the vocabulary of60K words trained with WEB text is used.Generally, trigram model is used as acousticmodel in order to improve the recognition accuracy.However, monophone model is used in this paper,since the proposed estimation method needs recog-nition error (and IRDR).4.4 Results4.4.1 Correlation between Conventional ASRand IR Evaluation MeasuresWe analyzed the correlations of conventionalASR evaluation measures with IRDR by selectingappropriate test data as follows.
First, ASR is per-formed for 470 spoken queries of an NTCIR-3 webtask.
Then, queries are eliminated whose ASR re-sults do not contain recognition errors and querieswith which no IR results are retrieved.
Finally, weselected 107 pairs of query transcripts and their ASRresults as test data.For all 107 pairs, we calculated WER and IRDRusing corresponding ASR result.
Figure 2 shows thecorrelations between WER and IRDR.
Correlationcoefficient between both is 0.119.
WER is not cor-related with IRDR.
Since our IR system only usesthe statistics of nouns, WER is not an appropriateevaluation measure for IR.
Conventionally, for suchtasks, keyword recognition has been performed, andkeyword error rate (KER) has been used as an evalu-ation measure.
KER is calculated by setting all key-word weights to 1 and all weights of the other wordsto 0 in WWER calculation.
Figure 3 shows the cor-relations between KER and IRDR.
Although IRDRis more correlated with KER than WER, KER is notsignificantly correlated with IRDR (correlation co-efficient: 0.224).
Thus, KER is not a suitable eval-uation measure of ASR for IR.
This fact shows thateach keyword has a different influence on IR and2070204060801000 20 40 60 80 100word error rate (%)ratioofIRscoredegradation(%)R=0.119Figure 2: Correlation between ratio of IR scoredegradation and WER0204060801000 20 40 60 80 100keyword error rate (%)ratioofIRscoredegradation(%)R=0.224Figure 3: Correlation between ratio of IR scoredegradation and KERshould be given a different weight based on its influ-ence on IR.4.4.2 Correlation between WWER and IREvaluation MeasureIn ASR for IR, since some words are significant,each word should have a different weight.
Thus, weassume that each keyword has a positive weight, andnon-keywords have zero weight.
WWER calculatedwith these assumptions is then defined as weightedkeyword error rate (WKER).Using the same test data (107 queries), keywordweights were estimated with the proposed estima-tion method.
The correlation between IRDR andWKER calculated with the estimated word weightsis shown in Figure 4.
A high correlation betweenIRDR and WKER is confirmed (correlation coeffi-cient: 0.969).
The result shows that the proposedmethod works well and proves that giving a differ-ent weight to each word is significant.The proposed method enables us to extend text-based IR systems to speech-based IR systems withtypical text queries for the IR system, ASR resultsof the queries, and answer sets for each query.
ASRresults are not necessary since they can be substi-tuted with simulated texts that can be automaticallygenerated by replacing some words with others.
Onthe contrary, text queries and answer sets are indis-pensable and must be prepared.
It costs too muchto make answer sets manually since we should con-sider whether each answer is relevant to the query.For these reasons, it is difficult to apply the methodto a large-scale speech-based IR system.
An esti-mation method without hand-labeled answer sets isstrongly required.An estimation method without hand-labeled an-swer sets, namely, the unsupervised estimation ofword weights, is also tested.
Unsupervised estima-tion is performed as described in Section 3.
In un-supervised estimation, the IR result (document set)with a correct transcript is regarded as an answer set,namely, a presumed answer set, and it is used forIRDR calculation instead of a hand-labeled answerset.The result (correlation between IRDR andWKER) is shown in Figure 5.
Without hand-labeled answer sets, we obtained high correlation(0.712 of correlation coefficient) between IRDR andWKER.
The result shows that the proposed estima-tion method is effective and widely applicable to IRsystems since it requires only typical text queries forIR.
With theWWER given by the estimated weights,IR performance degradation can be confidently pre-dicted.
It is confirmed that the ASR approach tominimize such WWER, which is realized with de-coding based on a Minimum Bayes-Risk frame-work (H.Nanjo and T.Kawahara, 2005)(H.Nanjo etal., 2005), is effective for IR.4.5 DiscussionIn this section, we discuss the problem of wordweight estimation.
Although we obtained high cor-relation between IRDR and KWER, the estimationmay encounter the over-fitting problem when we usesmall estimation data.
When we want to design aspeech-based IR system, a sufficient size of typi-cal queries is often prepared, and thus, our proposedmethod can estimate appropriate weights for typicalsignificant words.
Moreover, this problem will be2080204060801000 20 40 60 80 100weighted keyword error rate (%)ratioofIRscoredegradation(%)R=0.969Figure 4: Correlation between ratio of IR scoredegradation and WKER (supervised estimation)0204060801000 20 40 60 80 100weighted keyword error rate (%)ratioofIRscoredegradation(%)R=0.712Figure 5: Correlation between ratio of IR scoredegradation and WKER (unsupervised estimation)avoided using a large amount of dummy data (pair ofquery and IRDR) with unsupervised estimation.
Inthis work, although obtained correlation coefficientof 0.712 in unsupervised estimation, it is desirableto obtain much higher correlation.
There are muchroom to improve unsupervised estimation method.In addition, since typical queries for IR systemwill change according to the users, current topic,and so on, word weights should be updated accord-ingly.
It is reasonable approach to update wordweights with small training data which has been in-put to the system currently.
For such update sys-tem, our estimation method, which may encounterthe over-fitting problem to the small training data,may work as like as cache model (P.Clarkson andA.J.Robinson, 1997), which gives higher languagemodel probability to currently observed words.5 ConclusionWe described the automatic estimation of word sig-nificance for IR-oriented ASR.
The proposed esti-mation method only requires typical queries for theIR, and estimates weights of words so that WWER,which is an evaluation measure for ASR, will beequivalent to IRDR, which represents a degree of IRdegradation when an ASR result is used as a queryfor IR.
The proposed estimation method was evalu-ated on a web page retrieval task.
WWER based onestimated weights is highly correlated with IRDR.It is confirmed that the proposed method is effectiveand we can predict IR performance confidently withsuch WWER, which shows the effectiveness of ourproposed ASR approach minimizing such WWERfor IR.Acknowledgment: The work was partly supportedby KAKENHI WAKATE(B).ReferencesA.Lee, T.Kawahara, and K.Shikano.
2001.
Julius ?
anopen source real-time large vocabulary recognition en-gine.
In Proc.
EUROSPEECH, pages 1691?1694.C.Hori, T.Hori, H.Isozaki, E.Maeda, S.Katagiri, andS.Furui.
2003.
Deriving disambiguous queries in aspoken interactive ODQA system.
In Proc.
IEEE-ICASSP, pages 624?627.E.Levin, S.Narayanan, R.Pieraccini, K.Biatov,E.Bocchieri, G.D.Fabbrizio, W.Eckert, S.Lee,A.Pokrovsky, M.Rahim, P.Ruscitti, and M.Walker.2000.
The AT&T-DARPA communicator mixed-initiative spoken dialogue system.
In Proc.
ICSLP.H.Nanjo and T.Kawahara.
2005.
A new ASR evalua-tion measure and minimum Bayes-risk decoding foropen-domain speech understanding.
In Proc.
IEEE-ICASSP, pages 1053?1056.H.Nanjo, T.Misu, and T.Kawahara.
2005.
MinimumBayes-risk decoding considering word significancefor information retrieval system.
In Proc.
INTER-SPEECH, pages 561?564.NTCIR project web page.
http://research.nii.ac.jp/ntcir/.P.Clarkson and A.J.Robinson.
1997.
Language ModelAdaptation using Mixtures and an Exponentially De-caying cache.
In Proc.
IEEE-ICASSP, volume 2,pages 799?802.T.Misu, K.Komatani, and T.Kawahara.
2004.
Confirma-tion strategy for document retrieval systems with spo-ken dialog interface.
In Proc.
ICSLP, pages 45?48.V.Goel, W.Byrne, and S.Khudanpur.
1998.
LVCSRrescoring with modified loss functions: A decision the-oretic perspective.
In Proc.
IEEE-ICASSP, volume 1,pages 425?428.209
