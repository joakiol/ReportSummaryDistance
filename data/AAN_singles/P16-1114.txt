Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1203?1212,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsPrediction of Prospective User Engagement with Intelligent AssistantsShumpei Sano, Nobuhiro Kaji, and Manabu SassanoYahoo Japan Corporation9-7-1 Akasaka, Minato-ku, Tokyo 107-6211, Japan{shsano, nkaji, msassano}@yahoo-corp.jpAbstractIntelligent assistants on mobile devices,such as Siri, have recently gained con-siderable attention as novel applicationsof dialogue technologies.
A tremendousamount of real users of intelligent assis-tants provide us with an opportunity to ex-plore a novel task of predicting whetherusers will continually use their intelligentassistants in the future.
We developed pre-diction models of prospective user engage-ment by using large-scale user logs ob-tained from a commercial intelligent as-sistant.
Experiments demonstrated thatour models can predict prospective userengagement reasonably well, and outper-forms a strong baseline that makes predic-tion based past utterance frequency.1 IntroductionIntelligent assistants on mobile devices, such asSiri,1have recently gained considerable atten-tion as novel applications of dialogue technologies(Jiang et al, 2015).
They receive instructions fromusers via voice control to execute a wide rangeof tasks (e.g., searching the Web, setting alarms,making phone calls, and so on).
Some are able toeven chat or play games with users (Kobayashi etal., 2015).Intelligent assistants possess a unique character-istic as an object of dialogue study.
Popular intel-ligent assistants have thousands or even millionsof real users, thanks to the prevalence of mobiledevices.
Some of those users continually use in-telligent assistants for a long period of time, whileothers stop using them after a few trials.
Such userbehaviors are rarely observed in conventional ex-perimental environments, where dialogue systems1http://www.apple.com/ios/sirihave only a small number of experimental partici-pants who almost always continue to use the sys-tems for the whole duration of the experiment.This paper explores a novel task of predictingwhether a user will continue to use intelligent as-sistants in the future (This task is referred to asprospective user engagement prediction and itsdefinition is given in Section 3).
We attempt to de-velop such a prediction model, which would con-tribute to enhancing intelligent assistants in manyways.
For example, if users who are likely to stopusing systems can be identified, intelligent assis-tants can take actions to gain or maintain their in-terest (e.g., by sending push notifications).This task is related to, but is significantly differ-ent from, user engagement detection, which hasbeen extensively explored in prior dialogue stud-ies (Wang and Hirschberg, 2011; Forbes-Riley etal., 2012; Forbes-Riley and Litman, 2013; Oerteland Salvi, 2013).
The prior studies attempt to pre-dict how strongly users are currently engaged indialogues with systems.
On the other hand, thegoal of this study is to predict how strongly userswill be engaged with intelligent assistants in thefuture.
The largest difference lies in whether theprediction target is user engagement at present orin the future.
Also, our definition of engagementis slightly different from the prior ones.
In thisstudy, engagement is considered as a sentiment asto whether users like intelligent assistants and feellike they want to use them continually.To develop and evaluate models of prospectiveuser engagement prediction, we exploit large-scaleuser logs obtained from a commercial intelligentassistant.
Since monitoring users?
long-term be-haviors is considered crucial for precise predictionof their prospective engagement, we tailor variousfeatures by extracting usage patterns from a longhistory of user dialogues.
The resulting featuresare contrastive to those previously used for user1203engagement detection, in which features are basi-cally extracted from a single user utterance.Experimental results demonstrated that ourmodels are able to predict prospective user en-gagement reasonably well and are overwhelm-ingly better than a strong baseline that makes pre-dictions based on past utterance frequency.
Wealso discuss the trade-off between prediction ac-curacy and instancy.
Specifically, we investigatehow the prediction performance improves as wewait for more user dialogues to be collected.2 Yahoo!
Voice AssistThis section summarizes Yahoo!
Voice As-sist2(hereafter Voice Assist), a commercial intel-ligent assistant that is investigated in this study.Although our investigation exclusively focused onthis system, we will discuss how our findings canbe generalized to other intelligent assistants inSection 5.5Table 1 illustrates example dialogues of VoiceAssist users.3As illustrated, Voice Assist offersa variety of functions to mobile users.
They arelargely categorized into two types:device operation Voice Assist allows users to op-erate mobile devices through dialogue.
Thisincludes setting alarms, making phone calls,searching the Web, launching an app, and soon (e.g., V1, V3, V4, and V5).chat Voice Assist can give greetings to, have con-versations with, and play games with users(e.g., V2 and V6).
In contrast to device oper-ations for accomplishing certain tasks, thesefunctions are offered for fun or for facilitatingsmooth communication.Voice Assist currently supports 66 functions (in-cluding setting alarms, the word chain game, etc.
)and they can further be classified into fine-grainedtypes, although a detailed description of them isbeyond the scope of this paper.Voice Assist users can register personal pro-file such as their home address and birthday, withwhich the system makes personalized responses.For example, the home address are used to esti-mate users?
location when launching weather fore-2http://v-assist.yahoo.co.jp (in Japanese)3Because Voice Assist supports only Japanese, all utter-ances are made in Japanese.
In this paper, we present Englishtranslations rather than the original Japanese to facilitate non-Japanese readers?
understanding.U1 Wake me up at 8 o?clock tomorrow.V1 OK. Set the alarm to 8 am.
(show the timer configuration)U2 Good morning.V2 Good morning.
Thank you for talking to me.U3 Check today?s weather.V3 Tokyo?s weather will be fine today.
(launch weather forecast app)U4 From Tokyo to Kyoto.V4 A rapid express is available at 9:30.
(launch transit app to show timetable)U5 What time will it arrive?V5 It will arrive at Kyoto at 11:50.
(show the timetable again)U6 Let?s play the word chain game.V6 OK. Apple...Table 1: Example dialogues of Voice Assist users.U andV indicate the user and Voice Assist, respec-tively.
The notes in parentheses represent actionsthat Voice Assist takes after the responses.cast apps (i.e., response V3), while knowing birth-days allows Voice Assist to send greeting mes-sages to users on their birthdays.3 Prospective User EngagementPredictionThis section specifies the task of prospective userengagement prediction.
We first explain the userlog data used in our experiments.
We then describetwo kinds of task settings.3.1 User log dataWe conducted an empirical study in which we ex-amined Voice Assist user logs.
We randomly sam-pled 348,295 users who used the system at leastonce from March 2015 to June 2015 (16 weeks)and extracted all their dialogue histories duringthat period.
The log data included 7,472,915 ut-terances in total.Table 2 illustrates examples of user logs.
Weused the following seven attributes: user ID, nick-name, birthday, time stamp, user utterance, sys-tem response, and response type.
Because it is notmandatory to register the personal profiles (includ-ing nicknames, birthdays, etc.
), they are some-times missing, as indicated by N/A in the table.The response type represents the 66 functions sup-ported by Voice Assist.
The time stamps wereused to segment utterances into sessions, as rep-1204ID Nickname Birthday Time Stamp Use Utterance System Response TypeA John 2000-1-1 2015-3-1 23:50 Wake me up at 8 am tomorrow.
OK. Set the alarm to 8 am.
ALARM2015-3-2 08:10 Good morning.
Good morning.
CHAT2015-3-2 08:13 Check today?s weather.
Tokyo?s weather will be fine today.
WEATHERB N/A 2002-1-1 2015-3-1 08:00 From Tokyo to Kyoto.
A rapid express is available at 9:30.
TRANSIT2015-3-1 08:01 What time will it arrive?
It will arrive at Kyoto at 11:50.
TRANSIT2015-3-5 19:10 Let?s play the word chain game.
OK. Apple... WORD CHAINTable 2: User log examples.
The dashed line represents the session boundary.resented by dashed lines in the table.
We follow(Jiang et al, 2015) to define sessions as utterancesequences in which the interval of two adjacent ut-terances does not exceed 30 minutes.3.2 Task definitionWe propose two types of prospective user engage-ment prediction tasks.
In both tasks, we collectuser dialogues from the first eight weeks of theuser logs (referred to as observation period.
Wewill discuss on length of observation period inSection 5.4), and then use those past dialogues topredict whether users are engaged with the intelli-gent assistant in the last eight weeks of the log data(referred to as prediction period).4We specificallyexplored two prediction tasks as follows.Dropout prediction The first task is to predictwhether a given user will not at all use the systemin the prediction period.
This task is referred to asdropout prediction and is formulated as a binaryclassification problem.
The model of dropout pre-diction would allow intelligent assistants to takeproactive actions against users who are likely tostop using the system.
There are 71,330 dropoutusers, who does not at all use the system in theprediction period, among 275,630 in our data set.Engagement level prediction The second taskaims at predicting how frequently the system willbe used in the prediction period by a given user.Because there are outliers, or heavy users, who usethe system extremely frequently (one user used thesystem as many as 1,099 times in the eight weeks),we do not attempt to directly predict the number ofutterances or sessions.
Instead, we define engage-ment levels as detailed below, and aim at predict-ing those values.The engagement levels are defined as follows.First, users are sorted in the ascending order of4We removed users from the log data if the number of ses-sions was only once in the observation period, because suchdata lack a sufficient amount of dialogue histories for makinga reliable prediction.Level # of sessions # of users1 0 71,3302 1?3 66,6263 4?13 69,5514 14?
68,123Table 3: User distribution over the four engage-ment levels.
The second column represents inter-vals of the number of sessions corresponding tothe four levels.the number of sessions they made in the predictionperiod.
We then split users into four equally-sizedgroups.
The engagement levels of users in the fourgroups are defined as 1, 2, 3, and 4, respectively(Table 3).
Note that a larger value of the engage-ment level means that the users are more engagedwith the intelligent assistants.
This task is referredto as engagement level prediction and is formu-lated as a regression problem.The engagement level prediction has differentapplications from the dropout prediction.
For ex-ample, it would allow us to detect in advance thata user?s engagement level will change from four tothree in the near future.
It is beyond the scope ofdropout prediction task to foresee such a change.4 FeaturesThe dropout prediction is performed using lin-ear support vector machine (SVM) (Fan et al,2008), while the engagement level prediction isperformed using support vector regression (SVR)(Smola and Sch?olkopf, 2004) on the same featureset.
Here, we divide the features into four cate-gories by their function: utterance frequency fea-tures, response frequency features, time intervalfeatures, and user profile features.
Table 4 liststhese features.4.1 Utterance frequency featuresHere, we describe the features related to utterancefrequency.
These features attempt to capture our1205#Features Name Definition1 Utterance The number of utterances7 UtterancewWeeks The number of utterances in recent w weeks1 LongUtterance The number of lengthy utterances1 UrgedUtterance The number of utterances made in response to push notifications1 Restatement The number of restatement utterances100 UtteranceTopici The number of utterances including words in the i-th cluster1 Session The number of sessions7 SessionwWeeks The number of sessions in recent w weeks7 SessionByDay The number of sessions during each day of the week66 Response(t) The number of responses with response type t66 FirstResponse(t) Response(t) computed by using only the first responses in sessions1 LongResponse The number of lengthy responses1 ErrorMessage The number of error messages1 MaxInterval Max days between adjacent utterances1 MinInterval Min days between adjacent utterances1 AvgInterval Average days between adjacent utterances1 InactivePeriod Days from the last utterance date66 InactivePeriod(t) InactivePeriod computed for each type of the last response1 Nickname Whether or not a user has provided nickname information1 Birthday Whether or not a user has provided birthday information6 Age User?s age categoryTable 4: List of features.
The utterance frequency features, response frequency features, and time intervalfeatures are all scaled.intuition that users who frequently use intelligentassistants are likely to be engaged with them.Utterance The number of utterances in the obser-vation period.
For scaling purposes, the valueof this feature is set to log10(x+1), where x isthe number of utterances.
The same scalingis performed on all features but user profilefeatures.UtterancewWeeks The number of utterances inthe last w (1 ?
w < 8) weeks of the obser-vation period.LongUtterance The number of lengthy utter-ances (more than 20 characters long).
Jiang etal.
(2015) pointed out that long utterances areprone to cause ASR errors.
Since ASR errorsare a factor that decreases user engagement,users who are prone to make long utterancesare likely to be disengaged.UrgedUtterance The number of utterances madein response to push notifications sent from thesystem.
We expect that engaged users tend toreact to push notifications.Restatement The number of restatements madeby users.
Jiang et al (2015) found that userstend to repeat previous utterances in case ofASR errors.
An utterance is regarded as a re-statement of the previous one if their normal-ized edit distance (Li and Liu, 2007) is below0.5.UtteranceTopici The number of utterances in-cluding a keyword belonging to i-th wordcluster.
To induce the word clusters,100-dimensional word embeddings are firstlearned from the log data using WORD2VEC(Mikolov et al, 2013)5, and then K-meansclustering (K=100) is performed (Mac-Queen, 1967).
All options of WORD2VEC areset to the default values.
These features aimat capturing topics on utterances or speechacts.
Table 5 illustrates example words in theclusters.
For example, utterances includingwords in the cluster ID 36 and 63 are con-sidered to be greeting acts and sports-relatedconversations, respectively.5https://code.google.com/archive/p/word2vec1206Cluster ID Example words14 (Weather) pollen, typhoon, temperature23 (Curse) die, stupid, shit, shurrup, dorf36 (Greeting) thanks, good morning, hello48 (Sentiment) funny, cute, good, awesome63 (Sports) World cup, Nishikori, YankeesTable 5: Example words in the clusters.
Clus-ter names (presented in parentheses) are manuallyprovided by the authors to help readers understandthe word clusters.Session The number of sessions in the observa-tion period.SessionwWeeks The number of sessions in thelast w (1 ?
w < 8) weeks of the observa-tion period.SessionByDay The number of sessions in eachday of week.
There are seven different fea-tures of this type.4.2 Response frequency featuresHere, we describe the features of the response fre-quency.Response(t) The number of system responseswith response type t.FirstResponse(t) Response(t) features that arecomputed by using only the first responsesin sessions.
Our hypothesis is that first re-sponses in sessions crucially affect user en-gagement.LongResponse The number of lengthy responses(more than 50 characters long).
Becauselonger responses require a longer readingtime, they are prone to irritate users and con-sequently decrease user engagement.ErrorMessage The number of error messages.Voice Assist returns error messages (Sorry, Idon?t know.)
when it fails to find appropriateresponses to the user?s utterances.
We con-sider that these error messages decrease userengagement.4.3 Time interval featuresHere, we describe the features related to the ses-sion interval times.MaxInterval The maximum interval (in days) be-tween adjacent sessions in the observationperiod.MinInterval The minimum interval (in days) be-tween adjacent sessions in the observationperiod.AvgInterval The average interval (in days) be-tween adjacent sessions in the observationperiod.InactivePeriod The time span (in days) from thelast utterance to the end of the observation pe-riod.InactivePeriod(t) InactivePeriod computed sep-arately for each type t of the last response.4.4 User profile featuresHere, we describe the features of the user?s profileinformation.
Since it is not mandotory for usersto register their profiles, we expect that those whohave provided profile information are likely to beengaged with the system.Nickname A binary feature representing whetheror not the user has provided their nickname.Birthday A binary feature representing whetheror not the user has provided their birthday.Age Six binary features representing the user?sage.
They respectively indicate whether theuser is less than twenty years, in their 20?s,30?s, 40?s, or 50?s, or is more than 60 yearsold.
Note that these features are availableonly if the user has provided their birthday.5 ExperimentsIn this section, we describe our experimental re-sults and discuss them.5.1 Experimental settingsWe randomly divided the log data into training, de-velopment, and test sets with the ratio of 8:1:1.Note that we confirmed that the users in differ-ent data sets do not overlap with each other.
Wetrained the model with the training set and opti-mized hyperparameters with the development set.The test set was used for a final blind test to eval-uate the learnt model.We used the LIBLINEAR tool (Fan et al, 2008)to train the SVM for the dropout prediction and1207Accuracy F?measureBaseline 56.8 0.482Proposed 77.6 0.623Utterance frequency 70.2 0.578Response frequency 54.8 0.489Time interval 74.6 0.617User profile 39.9 0.406Table 6: Classification accuracies and F?measuresin the dropout prediction task.Precision RecallBaseline 0.350 0.774Proposed 0.553 0.714Utterance frequency 0.458 0.785Response frequency 0.346 0.831Time interval 0.507 0.789User profile 0.273 0.793Table 7: Precisions and Recalls in the dropout pre-diction task.the SVR for the engagement level prediction task.We optimized theC parameter on the developmentset.
In the dropout prediction task, we used the-w option to weigh the C parameter of each classwith the inverse ratio of the number of users in thatclass.
We also used the -B option to introduce thebias term.Next, we describe the evaluation metrics.
Weused accuracy and F1?measure in the dropout pre-diction task.
Mean squared error (MSE) andSpearman rank correlation coefficient were used inthe engagement level prediction task.
These eval-uation metrics are commonly used in classificationand regression tasks.We compare the proposed models with base-line method.
Because we have no previous workon both tasks, we defined baseline method of ourown.
The baseline method was trained in the sameframework as the proposed methods except thatthey used only Session feature.
We chose Ses-sion for baseline because frequency of use featuressuch as Session were shown predictive to similartasks (Kloft et al, 2014; Sadeque et al, 2015) toprospective user engagement.5.2 ResultsTable 6 illustrates the result of dropout predictiontask.
The first row compares the proposed methodwith the baseline.
We can see that the proposedFigure 1: Accuracies per the number of sessions inthe observation period of the proposed method andthe baseline.
The rightmost points represent theaccuracy of the users whose number of sessions inthe observation period are equal to or more than40.MSE SpearmanBaseline 0.784 0.595Proposed 0.578 0.727Utterance frequency 0.632 0.693Response frequency 0.798 0.584Time interval 0.645 0.692User profile 1.231 0.146Table 8: MSE and Spearman?s ?
in the engage-ment level prediction task.model outperforms the baseline.
This indicatesthe effectiveness of our feature set.
The secondrow illustrates the performances of the proposedmethod when only one feature type is used.
Thisresult suggests that the utterance frequency andtime interval features are especially useful, whilethe combination of all types of features performsthe best.
We conducted McNemar test (McNemar,1947) to investigate the significance of these im-provements, and confirmed that all improvementsare statistically significant (p < 0.01).Table 7 shows the precisions and the recalls ofdropout prediction task.
As shown in Table 7,the precision of the proposed method performs thebest while the recall is worst.
We consider that theperformance of the precision is more important forour model because taking proactive actions againstusers who are likely to stop using the system is oneof the assumed applications.
Taking proactive ac-tions (e.g., push notifications) against users contin-ually using the system might irritate them and de-1208Figure 2: Correlation between the oracle engagement levels and the ones predicted by the baselinemethod (left) and by the proposed method (right).crease their user engagement.
Therefore, the rateof the users who actually intend to stop using thesystem in the users predicted as dropout affects theeffectiveness of these proactive actions.
The resultthat the precision of the proposed method is 0.553and that of the baseline is 0.350 is, in other words,using the proposed model improves the effective-ness by 20% absolute in taking these actions.Figure 1 shows the accuracies per the number ofsessions in the observation period of the proposedmethod and the baseline.
The proposed methodconsistently outperforms the baseline throughoutthe number of sessions in the observation period.In particular, the proposed method predicts wellthe dropout of users whose number of sessions isaround five compared to the baseline.
These re-sults again indicate the effectiveness of the combi-nation of our feature set.Table 8 shows the result of engagement levelprediction task.
We again observe similar trends tothe dropout prediction task.
The proposed methodoutperforms the baseline.
The utterance frequencyand time interval features are the most effective,while the combination of all four feature typesachieves the best performance in both evaluationmetrics.Figure 2 visualizes the correlation between theoracle engagement levels and the ones predictedby the baseline (left) and by the proposed method(right).
We can intuitively reconfirm that the pro-posed method is able to predict the engagementlevels reasonably well.5.3 Investigation of feature weightsWe investigate weights of the features learned bythe SVR for figuring out what features contributeto the precise prediction of prospective user en-gagement.Table 9 exemplifies features that received largeweights for the four feature types.
We observethat most features with large positive or negativeweights are from the utterance frequency and timeinterval features.
Those include Session, Utter-ance, and InactivePeriod.
It is interesting to seethat UrgedUtterance, which is based on an utter-ance type specific to mobile users, also receives alarge positive weight.Further detailed analysis revealed that the pro-posed model captures some linguistic proper-ties that correlate with the prospective user en-gagement.
For example, UtteranceTopic36 andUtteranceTopic23 recieve positive and negativeweights, respectively.
This follows our intuitionsince those clusters correspond to greeting andcurse words (c.f.
Table 5).
We also observe Re-sponse(WORD CHAIN), Response(QUIZ) (wordassociation quiz), and Response(TRIVIA) (show-ing some trivia) receive positive weights.
Thismeans that playing games or showing some triviaattract users.
It is interesting to see that this re-sult is consistent with findings in (Kobayashi et al,2015).
It also follows our intuition that the weightof ErrorMessage feature is negative.5.4 Discussion on length of observationperiodNext, we investigate how the length of the obser-vation period affects the prediction performance.We varied the length of the observation periodsfrom one to eight weeks, and evaluated the results(Figure 3).Figure 3 demonstrates that the model perfor-1209Figure 3: Results of dropout prediction (left) and engagement level prediction (right) across differentobservation periods (in weeks).Weight Feature0.67 Session0.59 Utterance0.28 Session7Weeks0.26 UrgedUtterance0.02 UtteranceTopic36-0.05 UtteranceTopic230.08 Response(WORD CHAIN)0.08 Response(QUIZ)0.04 Response(TRIVIA)-0.03 ErrorMessage-0.23 InactivePeriod(ALARM)-0.46 InactivePeriod0.05 Birthday0.04 Age60sTable 9: Feature weights learned by the SVR.mance generally improves as the observation pe-riod becomes longer in both tasks.
When we in-crease the length of the observation period fromone week to eight weeks, the accuracy increasesby 7.9% in the dropout prediction and Spearman?s?
increases by 4.1 point in the engagement levelprediction.
The most significant improvements areachieved when we increase the length from oneweek to two weeks in the three metrics exceptthe F?measure.
This suggests that it is generallyeffective to collect user dialogues of two weekslong, rather than as long as eight weeks or more.This approach would allow to make predictionspromptly without waiting for user dialogues to becollected for a long time, while harming accuracy(or other evaluation metrics) as little as possible.5.5 Application to other intelligent assistantsHere, we discuss how well our approach appliesto intelligent assistants other than Voice Assist.The results of this study are considered to applyto other intelligent assistants so long as user logslike the ones in Table 2 are available.
The concernis that some attributes in Table 2 may not be avail-able in other systems.
In the following, we inves-tigate two attributes, response types and profiles,that are specific to Voice Assist.We consider that response types like ours areavailable in user logs of many other intelligentassistants as well.
Because our response typesmostly correspond to commands issued when op-erating mobile devices, response types analogousto ours can be obtained by simply logging thecommands.
Alternatively, it would be possible toemploy taggers like (Jiang et al, 2015) to auto-matically type system responses.As for profiles, it is likely that similar informa-tion is also available in many other intelligent as-sistants because profile registration is a commonfunction in many IT services including intelligentassistants.
For example, Cortana offers greetingsand other activities on special days registered byusers.6Even if user profiles were not at all avail-able, we consider that it would not seriously spoilthe significance of this study, because our exper-iments revealed that user profiles are among theleast predictive features.6http://m.windowscentral.com/articles(an article posted on Dec. 5, 2015)12106 Related WorkMany dialogue studies have explored the issueof detecting user engagement as well as relatedaffects such as interest and uncertainty (Wangand Hirschberg, 2011; Forbes-Riley et al, 2012;Forbes-Riley and Litman, 2013; Oertel and Salvi,2013).
As discussed in Section 1, these stud-ies typically use a single user utterance to predictwhether the user is currently engaged in dialogueswith systems.
We introduced a new perspectiveon this line of research by exploring models ofpredicting prospective user engagement in a large-scale empirical study.Kobayashi et al (2015) investigated how gamesplayed with intelligent assistants affect prospec-tive user engagement.
Although their researchinterest was prospective user engagement likeours, they exclusively studied the effect of playinggame, and left other factors unexplored.
In addi-tion, they did not develop any prediction models.Recently, user satisfaction for intelligent assis-tants gain attention(Jiang et al, 2015; Kiselevaet al, 2016a; Kiseleva et al, 2016b).
Jiang etal.
(2015) proposed an automatic method of as-sessing user satisfaction with intelligent assistants.Kiseleva et al extended the study of Jiang etal.
for prediction (2016a) and detailed understand-ing (2016b) of user satisfaction with intelligent as-sistants.
Although both satisfaction and engage-ment are affective states worth considering by in-telligent assistants, their research goals were quitedifferent from ours.
In their studies, user sat-isfaction was measured as to whether intelligentassistants can accomplish predefined tasks (e.g.,checking the exchange rate between US dollarsand Australian dollars).
This virtually assessestask-level response accuracy, which is a differentnotion from user engagement.Nevertheless, we consider that their studies areclosely related to ours and indeed helpful for im-proving the proposed model.
Since user satisfac-tion is considered to greatly affect prospective userengagement, it might be a good idea to use au-tomatically evaluated satisfaction levels as addi-tional features.
The proposed model currently usesErrorMessage feature as an alternative that can beimplemented with ease.Several studies have investigated the chancesof predicting continuous participation in SNSssuch as MOOC and health care forum (Ros?e andSiemens, 2014; Kloft et al, 2014; Ramesh et al,2014; Sadeque et al, 2015).
Unlike those studies,this study exclusively investigates a specific typeof dialogue system, namely intelligent assistants,and aims at uncovering usage and/or response pat-terns that strongly affect prospective user engage-ment.
Consequently, many of the proposed fea-tures are specially designed to analyze intelligentassistant users rather than SNS participants.Our work also relates to the evaluation of di-alogue systems.
Walker et al (1997) presentedthe offline evaluation framework for spoken dialogsystem (PARADISE).
They integrate various eval-uation metrics such as dialogue success and dia-logue costs into one performance measure func-tion.
Although our goal is to predict prospectiveuser engagement and different from theirs, somemeasures (e.g., the number of utterances) are use-ful to predict prospective user engagement with in-telligent assistants.7 ConclusionThis paper explored two tasks of predictingprospective user engagement with intelligent as-sistants: dropout prediction and engagementlevel prediction.
The experiments successfullydemonstrated that reasonable performance can bearchived in both tasks.
Also, we examined howthe length of the observation period affects pre-diction performance, and investigated the trade-offbetween prediction accuracy and instancy.
The fu-ture work includes using those prediction modelsin a real service to take targeted actions to userswho are likely to stop using intelligent assistants.ReferencesRong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
The Journal ofMachine Learning Research, 9:1871?1874.Kate Forbes-Riley and Diane Litman.
2013.
Whendoes disengagement correlate with performance inspoken dialog computer tutoring?
InternationalJournal of Artificial Intelligence in Education, 22(1-2):39?58.Kate Forbes-Riley, Diane Litman, Heather Friedberg,and Joanna Drummond.
2012.
Intrinsic and extrin-sic evaluation of an automatic user disengagementdetector for an uncertainty-adaptive spoken dialoguesystem.
In Proceedings of the 2012 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 91?102.
Association for Computa-tional Linguistics.1211Jiepu Jiang, Ahmed Hassan Awadallah, Rosie Jones,Umut Ozertem, Imed Zitouni, Ranjitha Gu-runath Kulkarni, and Omar Zia Khan.
2015.
Auto-matic online evaluation of intelligent assistants.
InProceedings of the 24th International Conferenceon World Wide Web, pages 506?516.
InternationalWorld Wide Web Conferences Steering Committee.Julia Kiseleva, Kyle Williams, Jiepu Jiang,Ahmed Hassan Awadallah, Imed Zitouni, Aidan CCrook, and Tasos Anastasakos.
2016a.
Predictinguser satisfaction with intelligent assistants.
InProceedings of the 39th International ACM SIGIRConference on Research and Development inInformation Retrieval, pages 495?505.
ACM.Julia Kiseleva, Kyle Williams, Jiepu Jiang, AhmedHassan Awadallah, Aidan C Crook, Imed Zitouni,and Tasos Anastasakos.
2016b.
Understanding usersatisfaction with intelligent assistants.
In Proceed-ings of the 2016 ACM SIGIR Conference on HumanInformation Interaction and Retrieval, pages 121?130.
ACM.Marius Kloft, Felix Stiehler, Zhilin Zheng, and NielsPinkwart.
2014.
Predicting MOOC dropout overweeks using machine learning methods.
In Pro-ceedings of the 2014 Empirical Methods in NaturalLanguage Processing, pages 60?65.
Association forComputational Linguistics.Hayato Kobayashi, Kaori Tanio, and Manabu Sassano.2015.
Effects of game on user engagement with spo-ken dialogue system.
In Proceedings of the 16th An-nual Meeting of the Special Interest Group on Dis-course and Dialogue, pages 422?426.
Associationfor Computational Linguistics.Yujian Li and Bo Liu.
2007.
A normalizedLevenshtein distance metric.
Pattern Analysisand Machine Intelligence, IEEE Transactions on,29(6):1091?1095.James MacQueen.
1967.
Some methods for classi-fication and analysis of multivariate observations.In Proceedings of the Fifth Berkeley Symposiumon Mathematics, Statistics and Probability, Vol.
1,pages 281?297.Quinn McNemar.
1947.
Note on the sampling errorof the difference between correlated proportions orpercentages.
Psychometrika, 12(2):153?157.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in Neural Information ProcessingSystems 26, pages 3111?3119.
Curran Associates,Inc.Catharine Oertel and Giampiero Salvi.
2013.
A gaze-based method for relating group involvement to in-dividual engagement in multimodal multiparty dia-logue.
In Proceedings of the 15th ACM on Interna-tional Conference on Multimodal Interaction, pages99?106.
ACM.Arti Ramesh, Dan Goldwasser, Bert Huang,Hal Daum?e III, and Lise Getoor.
2014.
Learninglatent engagement patterns of students in onlinecourses.
In Proceedings of the Twenty-EighthAAAI Conference Artificial Intelligence, pages1272?1278.
Association for the Advancement ofArtificial Intelligence.Carolyn Ros?e and George Siemens.
2014.
Shared taskon prediction of dropout over time in massively openonline courses.
In Proceedings of the 2014 Con-ference on Empirical Methods in Natural LanguageProcessing Workshop on Modeling Large Scale So-cial Interaction in Massively Open Online Courses,pages 39?41.
Association for Computational Lin-guistics.Farig Sadeque, Thamar Solorio, Ted Pedersen, PrashaShrestha, and Steven Bethard.
2015.
Predictingcontinued participation in online health forums.
InProceedings of the Sixth International Workshop onHealth Text Mining and Information Analysis, pages12?20.
Association for Computational Linguistics.Alex J. Smola and Bernhard Sch?olkopf.
2004.
A tu-torial on support vector regression.
Statistics andComputing, 14(3):199?222.Marilyn A Walker, Diane J Litman, Candace A Kamm,and Alicia Abella.
1997.
Paradise: A frameworkfor evaluating spoken dialogue agents.
In Proceed-ings of the eighth conference on European chap-ter of the Association for Computational Linguistics,pages 271?280.
Association for Computational Lin-guistics.William Yang Wang and Julia Hirschberg.
2011.Detecting levels of interest from spoken dialogwith multistream prediction feedback and similaritybased hierarchical fusion learning.
In Proceedingsof the 12th Annual Meeting of the Special InterestGroup on Discourse and Dialogue, pages 152?161.Association for Computational Linguistics.1212
