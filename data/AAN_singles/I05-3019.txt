Unigram Language Model for Chinese Word SegmentationAitao ChenYahoo!
Inc.701 First AvenueSunnyvale, CA 94089aitao@yahoo-inc.comYiping ZhouYahoo!
Inc.701 First AvenueSunnyvale, CA 94089zhouy@yahoo-inc.comAnne ZhangYahoo!
Inc.701 First AvenueSunnyvale, CA 94089annezhangya@yahoo.comGordon SunYahoo!
Inc.701 First AvenueSunnyvale, CA 94089gzsun@yahoo-inc.comAbstractThis paper describes a Chinese wordsegmentation system based on unigramlanguage model for resolving segmen-tation ambiguities.
The system is aug-mented with a set of pre-processors andpost-processors to extract new words inthe input texts.1 IntroductionThe Yahoo team participated in all four closedtasks and all four open tasks at the second inter-national Chinese word segmentation bakeoff.2 System DescriptionThe underlying algorithm in our word segmenta-tion system is the unigram language model inwhich words in a sentence are assumed to occurindependently.
For an input sentence, we exam-ine all possible ways to segment the new sen-tence with respect to the segmentationdictionary, and choose the segmentation of thehighest probability, which is estimated based onthe unigram model.Our system also has a few preprocessors andpostprocessors.
The main preprocessors includerecognizers for extracting names of people,places and organizations, and recognizer fornumeric expressions.
The proper name recog-nizers are built based on the maximum entropymodel, and the numeric expression recognizer isbuilt as a finite state automaton.
The conditionalmaximum entropy model in our implementationis based on the one described in Section 2.5 in(Ratnaparkhi, 1998), and features are the sameas those described in (Xue and Shen, 2003).One of the post-processing steps is to com-bine single characters in the initial segmentationif each character in a sequence of characters oc-curs in a word much more frequently than as aword on its own.
The other post-processing pro-cedure checks the segmentation of a text frag-ment in the input text against the segmentationin the training data.
If the segmentation pro-duced by our system is different from the one inthe training data, we will use the segmentationin the training data as the final segmentation.More details on the segmentation algorithm andthe preprocessors and postprocessors can befound in (Chen, 2003).Our system processes a sentence independ-ently.
For an input sentence, the preprocessorsare applied to the input sentence to extract nu-meric expressions and proper names.
The ex-tracted numeric expressions and proper namesare added to the segmentation dictionary, if theyare not already in the dictionary.
Then the inputsentence is segmented into words.
Finally thepost-processing procedures are applied to theinitial segmentation to produce the final seg-mentation.
Our system processes texts encodedin UTF-8; and it is used in all 8 tasks.3 ResultsTable 1 presents the results of the 10 officialruns we submitted in all 8 tasks.Run id R P F R-oovR-inas-closed 0.955 0.934 0.947 0.468 0.978as-open 0.958 0.938 0.948 0.506 0.978138cityu-closed 0.949 0.931 0.940 0.561 0.980cityu-open 0.952 0.937 0.945 0.608 0.980pku-closed 0.953 0.946 0.950 0.636 0.972pku-open-a 0.964 0.966 0.965 0.841 0.971msr-closed-a 0.969 0.952 0.960 0.379 0.985msr-closed-b 0.968 0.953 0.960 0.381 0.984msr-open-a 0.970 0.957 0.963 0.466 0.984msr-open-b 0.971 0.961 0.966 0.512 0.983Table 1: Summary of Yahoo official results.The first element in the run id is the corpusname, as referring to the Academia Sinica cor-pus, cityu the City University of Hong Kongcorpus, pku the Peking University Corpus, andmsr the Microsoft Research corpus.
The secondelement in the run id is the type of task, closedor open.
The second column shows the recall,the third column the precision, and the fourthcolumn F-score.
The last two columns presentthe recall of the out-of-vocabulary words and therecall of the words in the training data, respec-tively.3.1 Closed TasksFor the AS closed task run as-closed, wemanually identified about 15 thousands personnames and about 4 thousands place names fromthe AS training corpus.
We then built a personname recognizer and a place name recognizerfrom the AS training data.
All the name recog-nizers we built are based on the maximum en-tropy model.
We also built a rule-based numericexpression recognizer implemented as a finitestate automaton.The segmentation dictionary consists of thewords in the training data with occurrence fre-quency compiled from the training data.
Foreach character, the probability that a characteroccurs in a word is also computed from thetraining data only.Each line of texts in the testing data set isprocessed independently.
From an input line,first the person name recognizer and place namerecognizer are used to extract person and placenames; the numeric expression recognizer isused to extract numeric expressions.
The ex-tracted new proper names and new numeric ex-pressions are added to the segmentationdictionary with a constant occurrence frequencyof 0.5 before the input text is segmented.
Afterthe segmentation, a sequence of single charac-ters is combined into a single unit if each of thecharacters in the sequence occurs much morefrequently in a word than as a word on its own.The threshold of a character occurring in a wordis set to 0.80.
Also the quad-grams down to uni-grams in the segmentation are checked againstthe training data.
When a text fragment is seg-mented in a different way by our system than inthe training data, we use the segmentation of thetext fragment in the training data as the finaloutput.The runs cityu-closed and pku-closed areproduced in the same way.
We first manuallyidentified the person names and place names inthe training data, and then built name recogniz-ers from the training data.
The name recognizersand numeric expression recognizer are used firstto extract proper names and numeric expressionsbefore segmentation.
The post-processing is alsothe same.Two runs, named msr-closed-a and msr-closed-b, respectively, are submitted using theMicrosoft Research corpus for the closed task.Unlike in the other three corpora, the numericexpressions are much more versatile, and there-fore, more difficult to write regular expressionsto identify them.
We manually identified thenumeric expressions, person names, placenames, and organization names in the trainingdata, and then built maximum entropy model-based recognizers for extracting numeric expres-sions and names of people, place, and organiza-tions.
Also the organization names in this corpusare not segmented into words like in the otherthree corpora.
The organization name recognizeris word-based while the other three recognizersare character-based.
The only difference be-tween these two runs is that the run msr-closed-b includes an organization name recognizerwhile the other run msr-closed-a does not.3.2 Open TasksFor the AS open task, we used a user diction-ary and a person name recognizer and a placename recognizer, both trained on the combinedAS corpus and the CITYU corpus.
However, thebase dictionary and word frequency counts arecompiled from only the AS corpus.
For the openrun, we used the annotated AS corpus we ac-quired from Academia Sinica.
Also the phrasesegmentation table is built from the AS trainingdata only.
The AS open run as-open was pro-duced with the new person and place name rec-ognizers and with the user dictionary.
The139performance of the open run is almost the sameas that of the close run.The training data used in the CITYU opentask is the same as in the closed task.
We built aperson name recognizer and a place name rec-ognizer from the combined AS and CITYU cor-pora.
In training a recognizer, we only kept thesentences that contain at least one person orplace name.
The run cityu-open was producedwith new person name and place name recog-nizers trained on the combined corpora but with-out user dictionary.
The base dictionary andfrequency counts are from the CITYU trainingdata.
We prepared a user dictionary for theCITYU open run but forgot to turn on this fea-ture in the configuration file.
We repeated theCITYU open run cityu-open with user diction-ary.
The recall is 0.959; precision is 0.953; andF-score is 0.956.For the PKU open task run pku-open-a, wetrained our segmenter from the word-segmentedPeople?s Daily corpus covering the period ofJanuary 1 through June 30, 1998.
Our base dic-tionary with word frequency counts, charactercounts, and phrase segmentation table are builtfrom this larger training corpus of about 7 mil-lion words.
The words in this corpus are anno-tated with part-of-speech categories.
Both thenames of people and the names of places areuniquely tagged in this corpus.
We created atraining set for person name recognizer by com-bining the sentences in the People?s Daily cor-pus that contain at least one person name withthe sentences in the MSR training corpus thatcontain at least one person name.
The personnames in the MSR corpus were manually identi-fied.
From the combined training data for personnames, we built a person name recognizer basedon the maximum entropy model.
The placename recognizer was built in the same way.
ThePKU open run pku-open-a was produced usingthe segmenter trained on the 6-month People?sDaily corpus with the new person and placename recognizer trained on the People?s Dailycorpus and the MSR corpus.
A user dictionaryof about 100 thousand entries, most beingproper names, was used in the PKU open run.The training data used for the MSR openruns is the same MSR training corpus.
Our basedictionary, together with word frequency counts,and phrase segmentation table are built from theMSR training data only.
The numeric expressionrecognizer is the same as the one used in theclosed task.
The person name recognizer andplace name recognizer are the same as thoseused in the PKU open task.
We built an organi-zation name recognizer from the People?s Dailycorpus where organization names are marked.For example, the text fragment ?[?
?/ns ???/j]nt?
is marked by a pair of brackets and taggedwith ?nt?
in the annotated People?s Daily cor-pus.
We extracted all the sentences containing atleast one organization name and built a word-based recognizer.
The feature templates are thesame as in person name or place name recog-nizer.
We submitted two MSR open task runs,named msr-open-a and msr-open-b, respec-tively.
The only difference between these tworuns is that the first run msr-open-a did not in-clude an organization name recognizer, whilethe run msr-open-b used the organization namerecognizer built on the annotated People?s Dailycorpus.
Both runs were produced with a userdictionary, the new person name recognizer andnew place name recognizer.
The increase of F-score from 0.963 to 0.966 is due to the organiza-tion name recognizer.
While the organizationname recognizer correctly identified many or-ganization names, it also generated many falsepositives.
So the positive impact was offset bythe false positives.At about 12 hours before the due time, welearned that multiple submissions for the sametask are acceptable.
A colleague of ours submit-ted one PKU open run with the run id ?b?
andone MSR open run with the run id ?c?
in thebakeoff official results using a different wordsegmentation system without being tuning forthe bakeoff.
These two open runs are not dis-cussed in this paper.4 DiscussionsThe differences between our closed task runsand open task runs are rather small for both theAS corpus and the CITYU corpus.
Our CITYUopen run would be substantially better had weused our user dictionary.
The open task run us-ing the PKU corpus is much better than theclosed task run.
We performed a number of ad-ditional evaluations in both the PKU closed taskand the PKU open task.
Table 2 below presentsthe evaluation results with different features ac-tivated in our system.
The PKU training corpus140was used in all the experiments presented in Ta-ble 2.Run Features R P F1 base-dict 0.9386 0.9095 0.92382 1+num-expr 0.9411 0.9161 0.92853 2+person+place 0.9440 0.9249 0.93434 3+single-char 0.9404 0.9420 0.94125 4+consistency-checking0.9529 0.9464 0.9496Table 2: Results with different features appliedin PKU closed task.Table 3 presents the results with different fea-tures applied in the PKU open task.
The 6-month annotated People?s Daily corpus wasused in all the experiments shown in Table 3.Run Features R P F1 base-dict 0.9523 0.9503 0.95132 1+user-dict 0.9534 0.9565 0.95493 2+num-expr 0.9547 0.9605 0.95764 3+person+place 0.9562 0.9647 0.96045 4+single-char 0.9487 0.9650 0.95686 5+consistency-checking0.9637 0.9664 0.9650Table 3: Results with different features appliedin PKU open task.In the features column, base-dict refers to thebase dictionary built from the training data only;user-dict the additional user dictionary; num-expr the numeric expression recognizer imple-mented as a finite state automaton; person theperson name recognizer; place the place namerecognizer; single-char combining a sequence ofsingle characters when each one of them occursin words much more frequently than as a wordon its own; and lastly consistency-checkingchecking segmentations against the trainingtexts and choosing the segmentation in the train-ing texts if the segmentation of a text fragmentproduced by our system is different from the onein the training data.
The tables show the resultswith more and more features included.
Each runin the both tables includes one or two new fea-tures over the previous run.
The last run num-bered 5 in Table 2 is our official PKU closed runlabeled pku-closed in Table 1; and the last runnumbered 6 in Table 3 is our official PKU openrun labeled pku-open-a in Table 1.The F-score for our closed PKU task run is0.950 with all available features, while using thelarger People?s Daily corpus as training data andits dictionary alone, the F-score is 0.9513.
So alarger training data contributed significantly tothe increase in performance in our PKU opentask run.
The user dictionary, the numeric ex-pression recognizer, the person name recognizer,and the place name recognizer all contributed tothe better performance of our PKU closed runand open run.
Selectively combining sequenceof single characters appreciably improved theprecision while marginally decreased the recallin the PKU closed run.
However, in the opentask run, combining single characters did notresult in better performance, probably becausethe new words recovered by combining singlecharacters are already in our user dictionary forthe open run.
Finally consistency checking sub-stantially improved the performance for both theclosed run and the open run.5 ConclusionWe presented a word segmentation system thatuses unigram language model to select the mostprobable segmentation among all possible can-didates for an input text.
The system is aug-mented with proper name recognizers, numericexpression recognizers, and post-processingmodules to extract new words.
Overall the rec-ognizers and the post-processing modules sub-stantially improved the baseline performance.The larger training data set used in the PKUopen task also significantly increased the per-formance of our PKU open run.
The additionaluser dictionary is another major contributor toour better performance in the open tasks overthe closed tasks.ReferencesAitao Chen.
2003.
Chinese Word Segmentation Us-ing Minimal Linguistic Knowledge.
In: Proceed-ings of the Second SIGHAN Workshop onChinese Language Processing.Nianwen Xue and Libin Shen.
2003.
Chinese WordSegmentation as LMR Tagging.
In: Proceedings ofthe Second SIGHAN Workshop on Chinese Lan-guage Processing.Adwait Ratnaparkhi.
1998.
Maximum Entropy Mod-els for Natural Language Ambiguity Resolution.Dissertation in Computer and Information Sci-ence, University of Pennsylvania.141
