Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 110?114,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsExodus ?
Exploring SMT for EU InstitutionsMichael Jellinghaus1,2, Alexandros Poulis1, David Kolovratn?
?k31: European Parliament, Luxembourg2: Saarland University, Saarbru?cken, Germany3: Charles University in Prague, Czech Republicmicha@coli.uni-sb.de, apoulis@europarl.europa.eu, david@kolovratnik.netAbstractIn this paper, we describe Exodus, a jointpilot project of the European Commission?sDirectorate-General for Translation (DGT)and the European Parliament?s Directorate-General for Translation (DG TRAD) whichexplores the potential of deploying new ap-proaches to machine translation in Europeaninstitutions.
We have participated in theEnglish-to-French track of this year?s WMT10shared translation task using a system trainedon data previously extracted from large in-house translation memories.1 Project Background1.1 Translation at EU InstitutionsThe European Union?s policy on multilingualism1 re-quires enormous amounts of documents to be trans-lated into the 23 official languages (which yield 506translation directions).
To cope with this task, the EUhas the biggest translation service in the world, em-ploying almost 5000 internal staff as translators (out ofwhich 1750 at the European Commission (EC) and 760at the European Parliament (EP) alone), backed up bymore than 2000 support staff.
In 2009, the total outputof the Commission?s Directorate-General for Transla-tion (DGT) and the Parliament?s Directorate-Generalfor Translation (DG TRAD) together was more than 3million translated pages.
Thus, it is not surprising thatthe cost of all translation and interpreting services ofall the EU institutions amounts to 1% of the annual EUbudget (2008 figures).
According to our estimations,this is more than e 1 billion per year.1.2 Machine Translation and Other TranslationTechnologies at EU InstitutionsIn order to make the translators?
work more efficient sothat they can translate more pages in the same time,a number of tools like terminology databases, bilin-gual concordancers, and, most importantly, translationmemories are at their disposition, most of which areheavily used.1http://ec.europa.eu/education/languages/eu-language-policy/index en.htmIn real translation production scenarios, MachineTranslation is usually used to complement transla-tion memory tools (TM tool).
Translation memoriesare databases that contain text segments (usually sen-tences) that are stored together with their translations.Each such pair of source and target language segmentsis called a translation unit.
Translation units also con-tain useful meta-data (creation date, document type,client, etc.)
that allow us to filter the data both for trans-lation and machine translation purposes.A TM tool tries to match the segments within a doc-ument that needs to be translated with segments in thetranslation memory and propose translations.
If thememory contains an identical string then we have a so-called exact or 100% match which yields a very reliabletranslation.
Approximate or partial matches are calledfuzzy matches and usually, the minimum value of afuzzy match is set to 65%?70%.
Lower matches arenot considered as usable since they demand more edit-ing time than typing a translation from scratch.
Firstexperiments have shown that the quality of SMT out-put for certain language pairs is equal or similar to 70%fuzzy matches.Consequently, the cases where machine translationcan play a helpful role in this context is when, for a seg-ment to be translated, there is no exact match and theavailable fuzzy matches do not exceed a certain thresh-old.
This threshold in our case is expected to be 85% orlower.
To this end, there exists a system called ECMT(European Commission Machine Translation; also ac-cessible to other European institutions) which is a rule-based system.However, only certain translation directions are cov-ered by ECMT, and its maintenance is quite compli-cated and requires quite a lot of dedicated and special-ized human resources.
In the light of these facts andwith the addition of the languages of (prospective) newmember states, statistical approaches to machine trans-lation seem to offer a viable alternative.First of all, SMT is data-driven, i.e.
it exploits par-allel corpora of which there are plenty at the EU in-stitutions in the form of translation memories.
Trans-lation memories have two main advantages over otherparallel corpora.
First of all, they contain almost ex-clusively perfectly aligned segments, as each segmentis stored together with its translation, and secondly,110they contain cleaner data since their content is regu-larly maintained by linguists and database administra-tors.
SMT systems are quicker to develop and easierto maintain than rule-based systems.
The availabilityof free, open-source software like Moses2 (Koehn etal., 2007), GIZA++3 (Och and Ney, 2003) and the likeconstitutes a further argument in their favor.Early experiments with Moses were started by mem-bers of DGT?s Portuguese Language Department asearly as summer 2008 (Leal Fontes and Machado,2009), then turned into a wider interinstitutional projectwith the codename Exodus, currently combining re-sources from European Commission?s DGT and Euro-pean Parliament?s DGTRAD.
Exodus is the first jointproject of the interinstitutional Language TechnologyWatch group where a number of EU institutions joinforces in the field of language technology.2 Participation in WMT 2010 SharedTaskAfter the English-Portuguese experiments, the first lan-guage pair for which we developed a system witha sizeable amount of training data was English-to-French.
This system has been developed for testingat the European Parliament.
As English-to-French isalso one of the eight translation directions evaluated inthis year?s shared translation task, we decided to partic-ipate.
The reasons behind this decision are manifold:We would like to?
know where we stand in comparison to other sys-tems,?
learn about what system adaptations are the mostbeneficial,?
make our project known to potential collaborators,?
compare the WMT10 evaluation results to the out-come of our in-house evaluation.There is, however, one major difference between theevaluation as carried out in WMT10 and our in-houseevaluation: The test data of WMT10 consists exclu-sively of news articles and is thus out-of-domain forour system intended for use within the European Parlia-ment.
This means that the impact of training our systemon the in-domain data we obtain from our translationmemories cannot be assessed properly, i.e.
taking intoconsideration our specific translation production needs.Therefore, we would like to invite other interestedgroups to also translate our in-domain test data withthe goal of seeing how our translation scenario couldbenefit from their setups.
Due to legal issues, however,we unfortunately cannot provide our internal trainingdata at this moment.2http://www.statmt.org/moses/3http://www.fjoch.com/GIZA++.html3 Data UsedTo build our English-to-French MT system, we didnot use any of the data provided by the organizers ofthe WMT10 shared translation task.
Instead, we useddata that was extracted from the translation memoriesat the core of EURAMIS (European Advanced Multi-lingual Information System; (Theologitis, 1997; Blatt,1998)) which are the fruit of thousands of man-yearscontributed by translators at EU institutions who, eachday, upload the majority of the segments they translate.Initially (before pre-processing), our EN-FR cor-pus contained 10,446,450 segments and included doc-uments both from the Commission and the EP fromcommon legislative procedures.
These segments wereextracted in November 2009 from 7 translation memo-ries hosted in Euramis.
Currently, we do not have in-formation about the exact document types coming fromthe Commission?s databases.
The Parliament?s docu-ment types used include, among others:?
legislative documents such as draft reports, finalreports, amendments, opinions, etc.,?
documents for the plenary such as questions, res-olutions or session amendments,?
committee and delegation documents,?
documents concerning the ACP4 and the EMPA5,?
internal documents such as budget estimates, staffregulations, rules of procedure, etc.,?
calls for tender.Any sensitive or classified documents orCommission-internal documents that do not be-long to common legislative procedures have beenexcluded from the data.In terms of preprocessing, we performed severalsteps.
First, we obtained translation memory exchange(TMX) files from EURAMIS and converted them toUTF-8 text as the Euramis native character encodingis UCS-2.
Then we removed certain control charac-ters which otherwise would have halted processing, weextracted the two single-language corpora into a plain-text file, tokenized and lowercased the data.
Finally,we separated the corpus into training data (9,300,682segments), and data for tuning and testing ?
500 seg-ments each.
These segments did not exceed a max-imum length of 60 tokens and were excluded fromthe preparation of the translation and language models.The models were then trained on the remaining seg-ments.
The maximum length of 60 tokens was appliedhere as well.4African, Caribbean and Pacific Group of States5Euro-Mediterranean Parliamentary Assembly111Metric ScoreBLEU 18.8BLEU-cased 16.9TER 0.747Table 1: Automatic scores calculated for Exodus inWMT104 Building the Models and DecodingThe parallel data described above was used to train anEnglish-to-French translation model and a French tar-get language model.
This was done on a server runningSun Solaris with 64 GB of RAM and 8 double coreCPU?s @1800 Mhz (albeit shared with other processesrunning simultaneously).In general, we simply used a vanilla Moses instal-lation at this point, leaving the integration of more so-phisticated features to a later moment, i.e.
after a thor-ough analysis of the results of the present evaluationcampaign when we will know which adaptations yieldthe most significant improvements.For the word alignments, we chose MGIZA (Gaoand Vogel, 2008), using seven threads per MGIZA in-stance, with the parallel option, i.e.
one MGIZA in-stance per pair direction running in parallel.
The targetlanguage model is a 7-gram, binarized IRSTLM (Fed-erico et al, 2008).
The weights of the distortion, trans-lation and language models were optimized with re-spect to BLEU scores (Papineni et al, 2002) on a givenheld-out set of sentences with Minimum Error RateTraining (MERT; (Och, 2003)) in 15 iterations.After the actual translation with Moses, an additionalrecasing ?translation?
model was applied in the samemanner.
Finally, the translation output underwent min-imal automatic postprocessing based on regular expres-sion replacements.
This was mainly undertaken in or-der to fix the distribution of whitespace and some re-maining capitalization issues.5 Results5.1 WMT10 EvaluationIn one of the tasks of the WMT10 human evaluationcampaign, people were asked to rank competing trans-lations.
From each 1-through-5 ranking of a set of 5system outputs, 10 pairwise comparisons are extracted.Then, for each system, a score is computed that tellshow often it was ranked equally or better than the othersystem.
For our system, this score is 32.35%, meaningit ranked 17th out of 19 systems for English-to-French.A number of automatic scores were also calculated andappear in Table 1.5.2 Evaluation at the European ParliamentAs the goal behind building our system has been to pro-vide a tool to translators at EU institutions, we havealso had it evaluated by two of our colleagues, bothEvaluator A Evaluator B OverallReference 1.75 2.06 1.97ECMT 3.34 3.31 3.32Google 3.59 3.28 3.37Exodus 3.52 3.45 3.47Table 2: Average relative rank (on a scale from 1 to 5)OK Edited BadReference 29 30 2ECMT 8 57 2Google 7 33 5Exodus 13 62 12Table 3: Results of Editing Task (?OK?
means ?No cor-rections needed?
; ?Bad?
means ?Unable to correct?
)native speakers of French and working as professionaltranslators of the French Language Unit at the Parlia-ment?s DG TRAD.For this purpose, we had 1742 sentences of in-housedocuments translated by our system as well as bythe rule-based ECMT and the statistics-based GoogleTranslate.6,7 We developed an online evaluation toolbased on the one used by the WMT evaluation cam-paign in the last years (Callison-Burch et al, 2009)where we asked the evaluators to perform three differ-ent tasks.In the first one, they were shown the three automatictranslations plus a human reference in random orderand asked to rank the four versions relative to eachother on a scale from 1 to 5.
The average relative rankscan be seen in Table 2.The second task consisted of post-editing a giventranslation.
Again, the sentence might come from oneof three MT systems, or be a human translation.
Theabsolute number of items that did not need any correc-tions, had to be edited, or were impossible to correctare shown in Table 3.For the third and last task, only translations of ourown system were displayed.
Here, the evaluatorsshould simply assign them to one of four quality cat-egories as proposed by (Roturier, 2009), and addition-ally tick boxes standing for the presence of 13 differenttypes of errors in the sentence concerning word order,punctuation, or different types of syntactic/semanticproblems.
A total of 150 segments were judged.
Forthe categorization results, see Tables 4 and 5.5.3 Evaluation at the European CommissionOn a side note, the Portuguese Language Departmentalso performed a manual evaluation (Leal Fontes andMachado, 2009) which involved 14 of their managersand translators, comparing their Moses-based system to6http://translate.google.com7As about a third of the source documents are not public,we could not send those to Google Translate.112Items ProportionExcellent 28 18.6%Good 42 28%Medium 45 30%Poor 35 23.3%Table 4: Results of Categorization Task: Quality Cate-goriesError type OccurrencesWord orderSingle word 11Sequence of words 42Incorrect word(s)Wrong lexical choice 51Wrong terminology choice 6Incorrect form 77Extra word(s) 21Missing word(s) 14Style 44Idioms 1Untranslated word(s) 5Punctuation 24Letter case 7Other 5Table 5: Results of Categorization Task: Error TypesECMT and Google.
Table 6 shows how many peopleconsidered Moses better, similar, or worse compared toECMT and Google, respectively.Moses-based SMT did well in fields where ECMTis systematically used (e.g.
Justice and Home Affairsand Trade) and showed a big improvement over ECMTin terminology-intensive domains (e.g.
Fisheries).
Asof early 2009, more than half of their translators (58%)now already use ECMT systematically in production,i.e.
for all English and French originals.
85% use it forspecific language combinations or for certain domainsonly.
On a voluntary basis, they have been replacingECMT with Moses-based SMT for the translation ofday-to-day incoming documents.
Over a three-monthperiod, more than 2500 pages have been translated inthis manner, and the translators of the Portuguese de-partment declared themselves ready to switch over toan SMT system as soon as it should become available.Compared to Better Similar WorseECMT 7 5 2Google 5 5 3Table 6: Portuguese Language Department evaluationresults of Moses-based MT system6 Discussion of ResultsAs expected, our system did not rank among the topcompetitors in the WMT10 shared task.
This is mainlydue to the data we trained on, which is of a very spe-cific domain (common legislative procedures of Eu-ropean Institutions) and relatively small in size whencompared to what others used for this language combi-nation.
In addition, we more or less used Moses out-of-the-box with no sophisticated add-ons or optimization.In the internal evaluation, our system beat neitherGoogle Translate nor ECMT overall but it did show asimilar performance.
This is all the more encourag-ing as Exodus has been built within less than a month,while ECMT has been developed and maintained in ex-cess of 30 years, and while Google Translate is basedon manpower and computing resources that a publicadministration body usually cannot provide.Finally, the successful trials of SMT software at theEC?s Portuguese department seem to indicate that sucha system holds enormous potential, especially when aserious adaptation to specific language combinationsand domains is taken into consideration.7 OutlookFurther use and development of SMT at EU institutionsdepends on the outcome of internal evaluations, amongother factors.
We plan to extend our activities to otherlanguage pairs, an English-to-Greek machine transla-tion project already having started.
Given a continu-ation of the currently promising results, Exodus willeventually be integrated into the CAT (computer-aidedtranslation) tools used by EU translators.8 Further-more, we would like to release an extended EuroParlcorpus not only containing parliamentary proceedingsbut also other types of public documents.
We estimatethat such a step should foster research to the benefit ofboth EU institutions and machine translation in gen-eral.8 ConclusionsWe have presented Exodus, a joint pilot project ofthe European Commission?s Directorate-General forTranslation (DGT) and the European Parliament?sDirectorate-General for Translation (DG TRAD) withthe aim of exploring the potential of deploying newapproaches to machine translation in European insti-tutions.Our system is based on a fairly vanilla Moses instal-lation and trained on data extracted from large in-housetranslation memories covering a range of EU docu-ments.
The obtained models use 7-grams.We applied the Exodus system to this year?s WMT10shared English-to-French translation task.
As the test8However, speed issues will have to be addressed beforeas the current system is not able to provide translations in realtime.113data stems from a different domain than the one tar-geted by our system, we did not outperform the com-petitors.
However, results from in-house evaluation arepromising and indicate the big potential of SMT forEuropean Institutions.AcknowledgmentsWe would very much like to thank (in alphabetical or-der) Manuel Toma?s Carrasco Ben?
?tez, Dirk De Paepe,Alfons De Vuyst, Peter Hjorts?, Herman Jenne?, Hila?rioLeal Fontes, Maria Jose?
Machado, Spyridon Pilos, Joa?oRosas, Helmut Spindler, Filiep Spyckerelle, and Ange-lika Vaasa for their invaluable help and support.David Kolovratn?
?k was supported by the Czech Sci-ence Foundation under contract no.
201/09/H057 andby the Grant Agency of Charles University under con-tract no.
100008/2008.ReferencesA.
Blatt.
1998.
EURAMIS : Added value by integra-tion.
In T&T Terminologie et Traduction, 1.1998,pages 59?73.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
InProceedings of the Fourth Workshop on StatisticalMachine Translation, pages 1?28, Athens, Greece,March.
Association for Computational Linguistics.Marcello Federico, Nicola Bertoldi, and Mauro Cet-tolo.
2008.
IRSTLM: an Open Source Toolkit forHandling Large Scale Language Models.
In Pro-ceedings of Interspeech, Brisbane, Australia.Qin Gao and Stephan Vogel.
2008.
Parallel Implemen-tations of Word Alignment Tool.
In Software En-gineering, Testing, and Quality Assurance for Natu-ral Language Processing, pages 49?57, Columbus,Ohio, June.
Association for Computational Linguis-tics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In Proc.
of ACL Demo and Poster Sessions, pages177?180, Jun.Hila?rio Leal Fontes and Maria Jose?
Machado.
2009.Contribution of the Portuguese Langauge Depart-ment to the Evaluation of Moses Machine Transla-tion System.
Technical report, Portuguese LanguageDepartment, DGT, European Commission, Decem-ber.Franz Josef Och and Hermann Ney.
2003.
A Sys-tematic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?51,March.Franz Josef Och.
2003.
Minimum Error Rate Trainingfor Statistical Machine Translation.
In Proceedingsof ACL, Sapporo, Japan, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A Method for AutomaticEvaluation of Machine Translation.
In ACL ?02:Proceedings of the 40th Annual Meeting on Asso-ciation for Computational Linguistics, pages 311?318, Morristown, NJ, USA.
Association for Com-putational Linguistics.Johann Roturier.
2009.
Deploying novel MT tech-nology to raise the bar for quality: A review of keyadvantages and challenges.
In The twelfth MachineTranslation Summit, Ottawa, Canada, August.
Inter-national Association for Machine Translation.D.
Theologitis.
1997.
EURAMIS, the platform of theEC translator.
In EAMT Workshop, pages 17?32.114
