Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 193?197,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsNative Language Detection with Tree Substitution GrammarsBen SwansonBrown Universitychonger@cs.brown.eduEugene CharniakBrown Universityec@cs.brown.eduAbstractWe investigate the potential of Tree Substitu-tion Grammars as a source of features for na-tive language detection, the task of inferringan author?s native language from text in a dif-ferent language.
We compare two state of theart methods for Tree Substitution Grammarinduction and show that features from bothmethods outperform previous state of the artresults at native language detection.
Further-more, we contrast these two induction algo-rithms and show that the Bayesian approachproduces superior classification results with asmaller feature set.1 IntroductionThe correlation between a person?s native language(L1) and aspects of their writing in a second lan-guage (L2) can be exploited to predict L1 label givenL2 text.
The International Corpus of Learner En-glish (Granger et al 2002), or ICLE, is a large setof English student essays annotated with L1 labelsthat allows us to bring the power of supervised ma-chine learning techniques to bear on this task.
Inthis work we explore the possibility of automaticallyinduced Tree Substitution Grammar (TSG) rules asfeatures for a logistic regression model1 trained topredict these L1 labels.Automatic TSG induction is made difficult by theexponential number of possible TSG rules given acorpus.
This is an active area of research with twodistinct effective solutions.
The first uses a nonpara-metric Bayesian model to handle the large number1a.k.a.
Maximum Entropy Modelof rules (Cohn and Blunsom, 2010), while the sec-ond is inspired by tree kernel methods and extractscommon subtrees from pairs of parse trees (Sangatiand Zuidema, 2011).
While both are effective, weshow that the Bayesian method of TSG inductionproduces superior features and achieves a new bestresult at the task of native language detection.2 Related Work2.1 Native Language DetectionWork in automatic native language detection hasbeen mainly associated with the ICLE, published in2002.
Koppel et al(2005) first constructed sucha system with a feature set consisting of functionwords, POS bi-grams, and character n-grams.
Thesefeatures provide a strong baseline but cannot capturemany linguistic phenomena.More recently, Wong and Dras (2011a) consid-ered syntactic features for this task, using logis-tic regression with features extracted from parsetrees produced by a state of the art statistical parser.They investigated two classes of features: rerank-ing features from the Charniak parser and CFG fea-tures.
They showed that while reranking featurescapture long range dependencies in parse trees thatCFG rules cannot, they do not produce classificationperformance superior to simple CFG rules.
TheirCFG feature approach represents the best perform-ing model to date for the task of native language de-tection.
Wong and Dras (2011b) also investigatedthe use of LDA topic modeling to produce a latentfeature set of reduced dimensionality, but failed tooutperform baseline systems with this approach.1932.2 TSG inductionOne inherent difficulty in the use of TSGs is con-trolling the size of grammars automatically in-duced from data, which with any reasonable corpusquickly becomes too large for modern workstationsto handle.
When automatically induced TSGs werefirst proposed by Bod (1991), the problem of gram-mar induction was tackled with random selection offragments or weak constraints that led to massivegrammars.A more principled technique is to use a sparsenonparametric prior, as was recently presented byCohn et al(2009) and Post and Gildea (2009).
Theyprovide a local Gibbs sampling algorithm, and Cohnand Blunsom (2010) later developed a block sam-pling algorithm with better convergence behavior.While this Bayesian method has yet to producestate of the art parsing results, it has achieved stateof the art results for unsupervised grammar induc-tion (Blunsom and Cohn, 2010) and has been ex-tended to synchronous grammars for use in sentencecompression (Yamangil and Shieber, 2010).More recently, (Sangati and Zuidema, 2011) pre-sented an elegantly simple heuristic inspired by treekernels that they call DoubleDOP.
They showed thatmanageable grammar sizes can be obtained from acorpus the size of the Penn Treebank by recordingall fragments that occur at least twice, subject to apairwise constraint of maximality.
Using an addi-tional heuristic to provide a distribution over frag-ments, DoubleDOP achieved the current state of theart for TSG parsing, competing closely with the ab-solute best results set by refinement based parsers.2.3 Fragment Based ClassificationThe use of parse tree fragments for classificationbegan with Collins and Duffy (2001).
They usedthe number of common subtrees between two parsetrees as a convolution kernel in a voted perceptronand applied it as a parse reranker.
Since then, suchtree kernels have been used to perform a variety oftext classification tasks, such as semantic role la-beling (Moschitti et al 2008), authorship attribu-tion (Kim et al 2010), or the work of Suzuki andIsozaki (2006) that performs question classification,subjectivity detection, and polarity identification.Syntactic features have also been used in non-kernelized classifiers, such as in the work of Wongand Dras (2011a) mentioned in Section 2.1.
Ad-ditional examples include Raghavan et al(2010),which uses a CFG language model to perform au-thorship attribution, and Post (2011), which usesTSG features in a logistic regression model to per-form grammaticality detection.3 Tree Substitution GrammarsTree Substitution Grammars are similar to ContextFree Grammars, differing in that they allow rewriterules of arbitrary parse tree structure with any num-ber of nonterminal or terminal leaves.
We adopt thecommon term fragment2 to refer to these rules, asthey are easily visualised as fragments of a completeparse tree.SNP VPVBZhatesNPNPNNPGeorgeNPNNbroccoliNPNNSshoesFigure 1: Fragments from a Tree Substitution Grammarcapable of deriving the sentences ?George hates broccoli?and ?George hates shoes?.3.1 Bayesian InductionNonparametric Bayesian models can represent dis-tributions of unbounded size with a dynamic param-eter set that grows with the size of the training data.One method of TSG induction is to represent a prob-abilistic TSG with Dirichlet Process priors and sam-ple derivations of a corpus using MCMC.Under this model the posterior probability of afragment e is given asP (e|e?, ?, P0) =#e + ?P0#?
+ ?
(1)where e?
is the multiset of fragments in the currentderivations excluding e, #e is the count of the frag-ment e in e?, and #?
is the total number of frag-ments in e?
with the same root node as e. P0 is2As opposed to elementary tree, often used in related work194a PCFG distribution over fragments with a bias to-wards small fragments.
?
is the concentration pa-rameter of the DP, and can be used to roughly tunethe number of fragments that appear in the sampledderivations.With this posterior distribution the derivations ofa corpus can be sampled tree by tree using the blocksampling algorithm of Cohn and Blunsom (2010),converging eventually on a sample from the trueposterior of all derivations.3.2 DoubleDOP InductionDoubleDOP uses a heuristic inspired by tree kernels,which are commonly used to measure similarity be-tween two parse trees by counting the number offragments they share.
DoubleDOP uses the same un-derlying technique, but caches the shared fragmentsinstead of simply counting them.
This yields a setof fragments where each member is guaranteed toappear at least twice in the training set.In order to avoid unmanageably large grammarsonly maximal fragments are retained in each pair-wise extraction, which is to say that any shared frag-ment that occurs inside another shared fragment isdiscarded.
The main disadvantage of this methodis that the complexity scales quadratically with thetraining set size, as all pairs of sentences must beconsidered.
It is fully parallelizable, however, whichmediates this disadvantage to some extent.4 Experiments4.1 MethodologyOur data is drawn from the International Corpusof Learner English (Version 2), which consists ofraw unsegmented English text tagged with L1 la-bels.
Our experimental setup follows Wong andDras (2011a) in analyzing Chinese, Russian, Bul-garian, Japanese, French, Czech, and Spanish L1 es-says.
As in their work we randomly sample 70 train-ing and 25 test documents for each language.
All re-ported results are averaged over 5 subsamplings ofthe full data set.Our data preproccesing pipeline is as fol-lows: First we perform sentence segmentation withOpenNLP and then parse each sentence with a 6split grammar for the Berkeley Parser (Petrov et al2006).
We then replace all terminal symbols whichdo not occur in a list of 598 function words3 witha single UNK terminal.
This aggressive removal oflexical items is standard in this task and mitigates theeffect of other unwanted information sources suchas topic and geographic location that are correlatedwith native language in the data.We contrast three different TSG feature sets in ourexperiments.
First, to provide a baseline, we sim-ply read off the CFG rules from the data set (notethat a CFG can be taken as a TSG with all frag-ments having depth one).
Second, in the methodwe call BTSG, we use the Bayesian induction modelwith the Dirichlet process?
concentration parameterstuned to 100 and run for 1000 iterations of sampling.We take as our resulting finite grammar the frag-ments that appear in the sampled derivations.
Third,we run the parameterless DoubleDOP (2DOP) in-duction method.Using the full 2DOP feature set produces over400k features, which heavily taxes the resources ofa single modern workstation.
To balance the featureset sizes between 2DOP and BTSG we pass backover the training data and count the actual numberof times each fragment recovered by 2DOP appears.We then limit the list to the n most common frag-ments, where n is the average number of fragmentsrecovered by the BTSG method (around 7k).
We re-fer to results using this trimmed feature set with thelabel 2DOP, using 2DOP(F) to refer to DoubleDOPwith the full set of features.Given each TSG, we create a binary feature func-tion for each fragment e in the grammar such that thefeature fe(d) is active for a document d if there ex-ists a derivation of some tree t ?
d that uses e. Clas-sification is performed with the Mallet package forlogistic regression using the default initialized Max-EntTrainer.5 Results5.1 Predictive PowerThe resulting classification accuracies are shown inTable 1.
The BTSG feature set gives the highest per-formance, and both true TSG induction techniquesoutperform the CFG baseline.3We use the stop word list distributed with the ROUGE sum-marization evaluation package.195Model Accuracy (%)CFG 72.62DOP 73.52DOP(F) 76.8BTSG 78.4Table 1: Classification accuracyThe CFG result represents the work of Wong andDras (2011a), the previous best result for this task.While in their work they report 80% accuracy withthe CFG model, this is for a single sampling of thefull data set.
We observed a large variance in clas-sification accuracy over such samplings, which in-cludes some values in their reported range but witha much lower mean.
The numbers we report arefrom our own implementation of their CFG tech-nique, and all results are averaged over 5 randomsamplings from the full corpus.For 2DOP we limit the 2DOP(F) fragments bychoosing the 7k with maximum frequency, but theremay exist superior methods.
Indeed, Wong andDras (2011a) claims that Information Gain is a bettercriteria.
However, this metric requires a probabilis-tic formulation of the grammar, which 2DOP doesnot supply.
Instead of experimenting with differentlimiting metrics, we note that when all 400k rulesare used, the averaged accuracy is only 76.8 percent,which still lags behind BTSG.5.2 RobustnessWe also investigated different classification strate-gies, as binary indicators of fragment occurrenceover an entire document may lead to noisy results.Consider a single outlier sentence in a documentwith a single fragment that is indicative of the in-correct L1 label.
Note that it is just as important inthe eyes of the classifier as a fragment indicative ofthe correct label that appears many times.
To inves-tigate this phenomena we classified individual sen-tences, and used these results to vote for each docu-ment level label in the test set.We employed two voting schemes.
In the first,VoteOne, each sentence contributes one vote to itsmaximum probability label.
In the second, VoteAll,the probability of each L1 label is contributed as apartial vote.
Neither method increases performanceModel VoteOne (%) VoteAll (%)CFG 69.6 74.72DOP 69.1 73.5BTSG 72.5 76.5Table 2: Sentence based classification accuracyfor BTSG or 2DOP, but what is more interestingis that in both cases the CFG model outperforms2DOP (with less than half of the features).
Therobust behavior of the BTSG method shows that itfinds correctly discriminative features across severalsentences in each document to a greater extent thanother methods.5.3 ConcisionOne possible explanation for the superior perfor-mance of BTSG is that DDOP is prone to yieldingmultiple fragments that represent the same linguisticphenomena, leading to sets of highly correlated fea-tures.
While correlated features are not crippling toa logistic regression model, they add computationalcomplexity without contributing to higher classifica-tion accuracy.To address this hypothesis empirically, we con-sidered pairs of fragments eA and eB and calcu-lated the pointwise mutual information (PMI) be-tween events signifying their occurrence in a sen-tence.
For BTSG, the average pointwise mutual in-formation over all pairs (eA, eB) is ?.14, while for2DOP it is ?.01.
As increasingly negative valuesof PMI indicate exclusivity, this supports the claimthat DDOP?s comparative weakness is to some ex-tent due to feature redundancy.6 ConclusionIn this work we investigate automatically inducedTSG fragments as classification features for nativelanguage detection.
We compare Bayesian and Dou-bleDOP induced features and find that the formerrepresents the data with less redundancy, is more ro-bust to classification strategy, and gives higher clas-sification accuracy.
Additionally, the Bayesian TSGfeatures give a new best result for the task of nativelanguage detection.196ReferencesMohit Bansal and Dan Klein 2010.
Simple, accurateparsing with an all-fragments grammar.
Associationfor Computational Linguistics.Phil Blunsom and Trevor Cohn 2010.
UnsupervisedInduction of Tree Substitution Grammars for Depen-dency Parsing.
Empirical Methods in Natural Lan-guage Processing.Rens Bod 1991.
A Computational Model of LanguagePerformance: Data Oriented Parsing.
ComputationalLinguistics in the Netherlands.Trevor Cohn, Sharon Goldwater, and Phil Blunsom.2009.
Inducing Compact but Accurate Tree-Substitution Grammars.
In Proceedings NAACL.Trevor Cohn, and Phil Blunsom 2010.
Blocked inferencein Bayesian tree substitution grammars.
Associationfor Computational Linguistics.Michael Collins, Nigel Duffy 2001.
Convolution Ker-nels for Natural Language.
Advances in Neural Infor-mation Processing Systems.Joshua Goodman 2003.
Efficient parsing of DOP withPCFG-reductions.
In Bod et al chapter 8..S. Granger, E. Dagneaux and F. Meunier.
2002.
Interna-tional Corpus of Learner English, (ICLE).Sangkyum Kim, Hyungsul Kim, Tim Weninger, and Ji-awei Han 2010.
Authorship classification: a syn-tactic tree mining approach.
Proceedings of the ACMSIGKDD Workshop on Useful Patterns.Koppel, Moshe and Schler, Jonathan and Zigdon, Kfir.2005.
Determining an author?s native language bymining a text for errors.
Proceedings of the eleventhACM SIGKDD international conference on Knowl-edge discovery in data mining.Alessandro Moschitti, Daniele Pighin and Roberto Basili2008.
Tree Kernels for Semantic Role Labeling.
Com-putational Linguistics.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein 2006.
Learning Accurate, Compact, and In-terpretable Tree Annotation.
Association for Compu-tational Linguistics.Matt Post and Daniel Gildea.
2009.
Bayesian Learningof a Tree Substitution Grammar.
Association for Com-putational Linguistics.Matt Post.
2011.
Judging Grammaticality with Tree Sub-stitution Grammar Derivations.
Association for Com-putational Linguistics.Sindhu Raghavan, Adriana Kovashka and RaymondMooney 2010.
Authorship attribution using proba-bilistic context-free grammars.
Association for Com-putational Linguistics.Sangati, Federico and Zuidema, Willem 2011.
AccurateParsing with Compact Tree-Substitution Grammars:Double-DOP.
Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Processing.Jun Suzuki and Hideki Isozaki 2006.
Sequence and treekernels with statistical feature mining.
Advances inNeural Information Processing Systems.Sze-Meng Jojo Wong and Mark Dras 2011.
Exploit-ing Parse Structures for Native Language Identifica-tion.
Proceedings of the 2011 Conference on Empiri-cal Methods in Natural Language Processing.Sze-Meng Jojo Wong and Mark Dras 2011.
Topic Mod-eling for Native Language Identification.
Proceedingsof the Australasian Language Technology AssociationWorkshop.Elif Yamangil, Stuart M. Shieber 2010.
Bayesian Syn-chronous Tree-Substitution Grammar Induction andIts Application to Sentence Compression.. Associa-tion for Computational Linguistics.197
