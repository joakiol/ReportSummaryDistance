Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
22?32, Prague, June 2007. c?2007 Association for Computational LinguisticsWhat is the Jeopardy Model?
A Quasi-Synchronous Grammar for QAMengqiu Wang and Noah A. Smith and Teruko MitamuraLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213 USA{mengqiu,nasmith,teruko}@cs.cmu.eduAbstractThis paper presents a syntax-driven ap-proach to question answering, specificallythe answer-sentence selection problem forshort-answer questions.
Rather than us-ing syntactic features to augment exist-ing statistical classifiers (as in previouswork), we build on the idea that ques-tions and their (correct) answers relate toeach other via loose but predictable syntac-tic transformations.
We propose a prob-abilistic quasi-synchronous grammar, in-spired by one proposed for machine trans-lation (D. Smith and Eisner, 2006), and pa-rameterized by mixtures of a robust non-lexical syntax/alignment model with a(noptional) lexical-semantics-driven log-linearmodel.
Our model learns soft alignments asa hidden variable in discriminative training.Experimental results using the TREC datasetare shown to significantly outperform strongstate-of-the-art baselines.1 Introduction and MotivationOpen-domain question answering (QA) is a widely-studied and fast-growing research problem.
State-of-the-art QA systems are extremely complex.
Theyusually take the form of a pipeline architecture,chaining together modules that perform tasks suchas answer type analysis (identifying whether thecorrect answer will be a person, location, date,etc.
), document retrieval, answer candidate extrac-tion, and answer reranking.
This architecture is sopredominant that each task listed above has evolvedinto its own sub-field and is often studied and evalu-ated independently (Shima et al, 2006).At a high level, the QA task boils down to onlytwo essential steps (Echihabi andMarcu, 2003).
Thefirst step, retrieval, narrows down the search spacefrom a corpus of millions of documents to a fo-cused set of maybe a few hundred using an IR en-gine, where efficiency and recall are the main fo-cus.
The second step, selection, assesses each can-didate answer string proposed by the first step, andfinds the one that is most likely to be an answerto the given question.
The granularity of the tar-get answer string varies depending on the type ofthe question.
For example, answers to factoid ques-tions (e.g., Who, When, Where) are usually singlewords or short phrases, while definitional questionsand other more complex question types (e.g., How,Why) look for sentences or short passages.
In thiswork, we fix the granularity of an answer to a singlesentence.Earlier work on answer selection relies only onthe surface-level text information.
Two approachesare most common: surface pattern matching, andsimilarity measures on the question and answer, rep-resented as bags of words.
In the former, pat-terns for a certain answer type are either craftedmanually (Soubbotin and Soubbotin, 2001) or ac-quired from training examples automatically (Itty-cheriah et al, 2001; Ravichandran et al, 2003;Licuanan and Weischedel, 2003).
In the latter,measures like cosine-similarity are applied to (usu-ally) bag-of-words representations of the questionand answer.
Although many of these systems haveachieved very good results in TREC-style evalua-tions, shallow methods using the bag-of-word repre-sentation clearly have their limitations.
Examples of22cases where the bag-of-words approach fails aboundin QA literature; here we borrow an example used byEchihabi and Marcu (2003).
The question is ?Whois the leader of France?
?, and the sentence ?HenriHadjenberg, who is the leader of France ?s Jewishcommunity, endorsed ...?
(note tokenization), whichis not the correct answer, matches all keywords inthe question in exactly the same order.
(The cor-rect answer is found in ?Bush later met with FrenchPresident Jacques Chirac.?
)This example illustrates two types of variationthat need to be recognized in order to connect thisquestion-answer pair.
The first variation is thechange of the word ?leader?
to its semantically re-lated term ?president?.
The second variation is thesyntactic shift from ?leader of France?
to ?Frenchpresident.?
It is also important to recognize that?France?
in the first sentence is modifying ?com-munity?, and therefore ?Henri Hadjenberg?
is the?leader of ... community?
rather than the ?leader ofFrance.?
These syntactic and semantic variations oc-cur in almost every question-answer pair, and typi-cally they cannot be easily captured using shallowrepresentations.
It is also worth noting that suchsyntactic and semantic variations are not unique toQA; they can be found in many other closely relatedNLP tasks, motivating extensive community effortsin syntactic and semantic processing.Indeed, in this work, we imagine a generativestory for QA in which the question is generatedfrom the answer sentence through a series of syn-tactic and semantic transformations.
The same storyhas been told for machine translation (Yamada andKnight, 2001, inter alia), in which a target languagesentence (the desired output) has undergone seman-tic transformation (word to word translation) andsyntactic transformation (syntax divergence acrosslanguages) to generate the source language sen-tence (noisy-channel model).
Similar stories canalso be found in paraphrasing (Quirk et al, 2004;Wu, 2005) and textual entailment (Harabagiu andHickl, 2006; Wu, 2005).Our story makes use of a weighted formalismknown as quasi-synchronous grammar (hereafter,QG), originally developed by D. Smith and Eisner(2006) for machine translation.
Unlike most syn-chronous formalisms, QG does not posit a strict iso-morphism between the two trees, and it providesan elegant description for the set of local configura-tions.
In Section 2 we situate our contribution in thecontext of earlier work, and we give a brief discus-sion of quasi-synchronous grammars in Section 3.Our version of QG, called the Jeopardy model, andour parameter estimation method are described inSection 4.
Experimental results comparing our ap-proach to two state-of-the-art baselines are presentedin Section 5.
We discuss portability to cross-lingualQA and other applied semantic processing tasks inSection 6.2 Related WorkTo model the syntactic transformation process, re-searchers in these fields?especially in machinetranslation?have developed powerful grammaticalformalisms and statistical models for representingand learning these tree-to-tree relations (Wu andWong, 1998; Eisner, 2003; Gildea, 2003; Melamed,2004; Ding and Palmer, 2005; Quirk et al, 2005;Galley et al, 2006; Smith and Eisner, 2006, in-ter alia).
We can also observe a trend in recent workin textual entailment that more emphasis is put onexplicit learning of the syntactic graph mapping be-tween the entailed and entailed-by sentences (Mac-Cartney et al, 2006).However, relatively fewer attempts have beenmade in the QA community.
As pointed out byKatz and Lin (2003), most early experiments inQA that tried to bring in syntactic or semanticfeatures showed little or no improvement, and itwas often the case that performance actually de-graded (Litkowski, 1999; Attardi et al, 2001).
Morerecent attempts have tried to augment the bag-of-words representation?which, after all, is simply areal-valued feature vector?with syntactic features.The usual similarity measures can then be used onthe new feature representation.
For example, Pun-yakanok et al (2004) used approximate tree match-ing and tree-edit-distance to compute a similarityscore between the question and answer parse trees.Similarly, Shen et al (2005) experimented with de-pendency tree kernels to compute similarity betweenparse trees.
Cui et al (2005) measured sentencesimilarity based on similarity measures between de-pendency paths among aligned words.
They usedheuristic functions similar to mutual information to23assign scores to matched pairs of dependency links.Shen and Klakow (2006) extend the idea furtherthrough the use of log-linear models to learn a scor-ing function for relation pairs.Echihabi and Marcu (2003) presented a noisy-channel approach in which they adapted the IBMmodel 4 from statistical machine translation (Brownet al, 1990; Brown et al, 1993) and applied it to QA.Similarly, Murdock and Croft (2005) adopted a sim-ple translation model from IBM model 1 (Brown etal., 1990; Brown et al, 1993) and applied it to QA.Porting the translation model to QA is not straight-forward; it involves parse-tree pruning heuristics(the first two deterministic steps in Echihabi andMarcu, 2003) and also replacing the lexical trans-lation table with a monolingual ?dictionary?
whichsimply encodes the identity relation.
This brings usto the question that drives this work: is there a statis-tical translation-like model that is natural and accu-rate for question answering?
We propose Smith andEisner?s (2006) quasi-synchronous grammar (Sec-tion 3) as a general solution and the Jeopardy model(Section 4) as a specific instance.3 Quasi-Synchronous GrammarFor a formal description of QG, we recommendSmith and Eisner (2006).
We briefly review the cen-tral idea here.
QG arose out of the empirical obser-vation that translated sentences often have some iso-morphic syntactic structure, but not usually in en-tirety, and the strictness of the isomorphism mayvary across words or syntactic rules.
The idea is that,rather than a synchronous structure over the sourceand target sentences, a tree over the target sentenceis modeled by a source-sentence-specific grammarthat is inspired by the source sentence?s tree.1 Thisis implemented by a ?sense?
?really just a subsetof nodes in the source tree?attached to each gram-mar node in the target tree.
The senses define analignment between the trees.
Because it only looselylinks the two sentences?
syntactic structure, QG isparticularly well-suited for QA insofar as QA is like?free?
translation.A concrete example that is easy to understandis a binary quasi-synchronous context-free grammar1Smith and Eisner also show how QG formalisms generalizesynchronous grammar formalisms.
(denoted QCFG).
Let VS be the set of constituent to-kens in the source tree.
QCFG rules would take theaugmented form?X, S1?
?
?Y, S2?
?Z, S3?
?X, S1?
?
wwhere X,Y, and Z are ordinary CFG nonterminals,each Si ?
2VS (subsets of nodes in the source treeto which the nonterminals align), and w is a target-language word.
QG can be made more or less ?lib-eral?
by constraining the cardinality of the Si (weforce all |Si| = 1), and by constraining the relation-ships among the Si mentioned in a single rule.
Theseare called permissible ?configurations.?
An exampleof a strict configuration is that a target parent-childpair must align (respectively) to a source parent-child pair.
Configurations are shown in Table 1.Here, following Smith and Eisner (2006), we usea weighted, quasi-synchronous dependency gram-mar.
Apart from the obvious difference in appli-cation task, there are a few important differenceswith their model.
First, we are not interested in thealignments per se; we will sum them out as a hid-den variable when scoring a question-answer pair.Second, our probability model includes an optionalmixture component that permits arbitrary features?we experiment with a small set of WordNet lexical-semantics features (see Section 4.4).
Third, we ap-ply a more discriminative training method (condi-tional maximum likelihood estimation, Section 4.5).4 The Jeopardy ModelOur model, informally speaking, aims to follow theprocess a player of the television game show Jeop-ardy!
might follow.
The player knows the answer(or at least thinks he knows the answer) and mustquickly turn it into a question.2 The question-answerpairs used on Jeopardy!
are not precisely what wehave in mind for the real task (the questions are notspecific enough), but the syntactic transformation in-spires our model.
In this section we formally define2A round of Jeopardy!
involves a somewhat involved andspecific ?answer?
presented to the competitors, and the firstcompetitor to hit a buzzer proposes the ?question?
that leads tothe answer.
For example, an answer might be, This Eastern Eu-ropean capital is famous for defenestrations.
In Jeopardy!
theplayers must respond with a queston: What is Prague?24this probability model and present the necessary al-gorithms for parameter estimation.4.1 Probabilistic ModelThe Jeopardy model is a QG designed for QA.
Letq = ?q1, ..., qn?
be a question sentence (each qi is aword), and let a = ?a1, ..., am?
be a candidate an-swer sentence.
(We will use w to denote an abstractsequence that could be a question or an answer.)
Inpractice, these sequences may include other infor-mation, such as POS, but for clarity we assume justwords in the exposition.
Let A be the set of can-didate answers under consideration.
Our aim is tochoose:a?
= argmaxa?Ap(a | q) (1)At a high level, we make three adjustments.
Thefirst is to apply Bayes?
rule, p(a | q) ?
p(q |a) ?
p(a).
Because A is known and is assumed tobe generated by an external extraction system, wecould use that extraction system to assign scores(and hence, probabilities p(a)) to the candidate an-swers.
Other scores could also be used, such asreputability of the document the answer came from,grammaticality, etc.
Here, aiming for simplicity, wedo not aim to use such information.
Hence we treatp(a) as uniform over A.3The second adjustment adds a labeled, directeddependency tree to the question and the answer.The tree is produced by a state-of-the-art depen-dency parser (McDonald et al, 2005) trained onthe Wall Street Journal Penn Treebank (Marcus etal., 1993).
A dependency tree on a sequence w =?w1, ..., wk?
is a mapping of indices of words to in-dices of their syntactic parents and a label for thesyntactic relation, ?
: {1, ..., k} ?
{0, ..., k} ?
L.Each word wi has a single parent, denoted w?
(i).par .Cycles are not permitted.
w0 is taken to be the invis-ible ?wall?
symbol at the left edge of the sentence; ithas a single child (|{i : ?
(i) = 0}| = 1).
The labelfor wi is denoted ?
(i).lab.The third adjustment involves a hidden variableX , the alignment between question and answer3The main motivation for modeling p(q | a) is that it is eas-ier to model deletion of information (such as the part of the sen-tence that answers the question) than insertion.
Our QG doesnot model the real-world knowledge required to fill in an an-swer; its job is to know what answers are likely to look like,syntactically.words.
In our model, each question-word maps toexactly one answer-word.
Let x : {1, ..., n} ?
{1, ...,m} be a mapping from indices of words in qto indices of words in a.
(It is for computational rea-sons that we assume |x(i)| = 1; in general x couldrange over subsets of {1, ...,m}.)
Because we de-fine the correspondence in this direction, note that itis possible for multple question words to map to thesame answer word.Why do we treat the alignmentX as a hidden vari-able?
In prior work, the alignment is assumed to beknown given the sentences, but we aim to discoverit from data.
Our guide in this learning is the struc-ture inherent in the QG: the configurations betweenparent-child pairs in the question and their corre-sponding, aligned words in the answer.
The hiddenvariable treatment lets us avoid commitment to anyone x mapping, making the method more robust tonoisy parses (after all, the parser is not 100% ac-curate) and any wrong assumptions imposed by themodel (that |x(i)| = 1, for example, or that syntactictransformations can explain the connection betweenq and a at all).4Our model, then, definesp(q, ?q | a, ?a) =?xp(q, ?q, x | a, ?a) (2)where ?q and ?a are the question tree and answertree, respectively.
The stochastic process defined byour model factors cleanly into recursive steps thatderive the question from the top down.
The QG de-fines a grammar for this derivation; the grammar de-pends on the specific answer.Let ?
iw refer to the subtree of ?w rooted at wi.
Themodel is defined by:p(?
iq | qi, ?q(i), x(i), ?a) = (3)p#kids(|{j : ?q(j) = i, j < i}| | qi, left)?p#kids(|{j : ?q(j) = i, j > i}| | qi, right)?
?j:?q(j)=im?x(j)=0pkid (qj , ?q(j).lab | qi, ?q(i), x(i), x(j), ?a)?p(?
jq | qj , ?q(j), x(j), ?a)4If parsing performance is a concern, we might also treat thequestion and/or answer parse trees as hidden variables, thoughthat makes training and testing more computationally expen-sive.25Note the recursion in the last line.
While the abovemay be daunting, in practice it boils down only todefining the conditional distribution pkid , since thenumber of left and right children of each node neednot be modeled (the trees are assumed known)?p#kids is included above for completeness, but in themodel applied here we do not condition it on qi andtherefore do not need to estimate it (since the treesare fixed).pkid defines a distribution over syntactic childrenof qi and their labels, given (1) the word qi, (2) theparent of qi, (3) the dependency relation betweenqi and its parent, (4) the answer-word qi is alignedto, (5) the answer-word the child being predicted isaligned to, and (6) the remainder of the answer tree.4.2 Dynamic ProgrammingGiven q, the score for an answer is simply p(q, ?q |a, ?a).
Computing the score requires summing overalignments and can be done efficiently by bottom-updynamic programming.
Let S(j, `) refer to the scoreof ?
jq, assuming that the parent of qj , ?q(j).par , isaligned to a`.
The base case, for leaves of ?q, is:S(j, `) = (4)p#kids(0 | qj , left) ?
p#kids(0 | qj , right)?m?k=0pkid (qj , ?q(j).lab | q?q(j) , `, k, ?a)Note that k ranges over indices of answer-words tobe aligned to qj .
The recursive case isS(i, `) = (5)p#kids(|{j : ?q(j) = i, j < i}| | qj , left)?p#kids(|{j : ?q(j) = i, j > i}| | qj , right)?m?k=0pkid (qi, ?q(i).lab | q?q(i), `, k, ?a)?
?j:?q(j)=iS(j, k)Solving these equations bottom-up can be donein O(nm2) time and O(nm) space; in practice thisis very efficient.
In our experiments, computing thevalue of a question-answer pair took two seconds onaverage.5 We turn next to the details of pkid , the coreof the model.4.3 Base ModelOur base model factors pkid into three conditionalmultinomial distributions.pbasekid (qi, ?q(i).lab | q?q(i), `, k, ?a) =p(qi.pos | ak.pos) ?
p(qi.ne | ak.ne)?p(?q(i).lab | config(?q, ?a, i)) (6)where qi.pos is question-word i?s POS label andqi.ne is its named-entity label.
config mapsquestion-word i, its parent, and their alignees toa QG configuration as described in Table 1; notethat some configurations are extended with addi-tional tree information.
The base model does notdirectly predict the specific words in the question?only their parts-of-speech, named-entity labels, anddependency relation labels.
This model is very sim-ilar to Smith and Eisner (2006).Because we are interested in augmenting the QGwith additional lexical-semantic knowledge, we alsoestimate pkid by mixing the base model with amodel that exploits WordNet (Miller et al, 1990)lexical-semantic relations.
The mixture is given by:pkid (?
| ?)
= ?pbasekid (?
| ?)+(1??
)plskid (?
| ?)
(7)4.4 Lexical-Semantics Log-Linear ModelThe lexical-semantics model plskid is defined by pre-dicting a (nonempty) subset of the thirteen classesfor the question-side word given the identity ofits aligned answer-side word.
These classes in-clude WordNet relations: identical-word, synonym,antonym (also extended and indirect antonym), hy-pernym, hyponym, derived form, morphologicalvariation (e.g., plural form), verb group, entailment,entailed-by, see-also, and causal relation.
In ad-dition, to capture the special importance of Wh-words in questions, we add a special semantic re-lation called ?q-word?
between any word and anyWh-word.
This is done through a log-linear modelwith one feature per relation.
Multiple relations mayfire, motivating the log-linear model, which permits?overlapping?
features, and, therefore prediction of5Experiments were run on a 64-bit machine with 2?
2.2GHzdual-core CPUs and 4GB of memory.26any of the possible 213 ?
1 nonempty subsets.
Itis important to note that this model assigns zeroprobability to alignment of an answer-word withany question-word that is not directly related to itthrough any relation.
Such words may be linked inthe mixture model, however, via pbasekid .6(It is worth pointing out that log-linear modelsprovide great flexibility in defining new features.
Itis straightforward to extend the feature set to includemore domain-specific knowledge or other kinds ofmorphological, syntactic, or semantic information.Indeed, we explored some additional syntactic fea-tures, fleshing out the configurations in Table 1 inmore detail, but did not see any interesting improve-ments.
)parent-child Question parent-child pair align respec-tively to answer parent-child pair.
Aug-mented with the q.-side dependency la-bel.child-parent Question parent-child pair align respec-tively to answer child-parent pair.
Aug-mented with the q.-side dependency la-bel.grandparent-child Question parent-child pair align respec-tively to answer grandparent-child pair.Augmented with the q.-side dependencylabel.same node Question parent-child pair align to thesame answer-word.siblings Question parent-child pair align to sib-lings in the answer.
Augmented withthe tree-distance between the a.-side sib-lings.c-command The parent of one answer-side word isan ancestor of the other answer-sideword.other A catch-all for all other types of config-urations, which are permitted.Table 1: Syntactic alignment configurations are par-titioned into these sets for prediction under the Jeop-ardy model.4.5 Parameter EstimationThe parameters to be estimated for the Jeopardymodel boil down to the conditional multinomialdistributions in pbasekid , the log-linear weights in-side of plskid , and the mixture coefficient ?.7 Stan-6It is to preserve that robustness property that the models aremixed, and not combined some other way.7In our experiments, all log-linear weights are initialized tobe 1; all multinomial distributions are initialized as uniform dis-dard applications of log-linear models apply con-ditional maximum likelihood estimation, which forour case involves using an empirical distribution p?over question-answer pairs (and their trees) to opti-mize as follows:max??q,?q,a,?ap?
(q, ?q,a, ?a) log p?
(q, ?q | a, ?a)?
??
?Px p?
(q,?q,x|a,?a)(8)Note the hidden variable x being summed out; thatmakes the optimization problem non-convex.
Thissort of problem can be solved in principle by condi-tional variants of the Expectation-Maximization al-gorithm (Baum et al, 1970; Dempster et al, 1977;Meng and Rubin, 1993; Jebara and Pentland, 1999).We use a quasi-Newton method known as L-BFGS(Liu and Nocedal, 1989) that makes use of the gra-dient of the above function (straightforward to com-pute, but omitted for space).5 ExperimentsTo evaluate our model, we conducted experimentsusing Text REtrieval Conference (TREC) 8?13 QAdataset.85.1 Experimental SetupThe TREC dataset contains questions and answerpatterns, as well as a pool of documents returned byparticipating teams.
Our task is the same as Pun-yakanok et al (2004) and Cui et al (2005), wherewe search for single-sentence answers to factoidquestions.
We follow a similar setup to Shen andKlakow (2006) by automatically selecting answercandidate sentences and then comparing against ahuman-judged gold standard.We used the questions in TREC 8?12 for trainingand set aside TREC 13 questions for development(84 questions) and testing (100 questions).
To gen-erate the candidate answer set for development andtesting, we automatically selected sentences fromeach question?s document pool that contains one ormore non-stopwords from the question.
For gen-erating the training candidate set, in addtion to thesentences that contain non-stopwords from the ques-tion, we also added sentences that contain correcttributions; ?
is initialized to be 0.1.8We thank the organizers and NIST for making the datasetpublicly available.27answer pattern.
Manual judgement was producedfor the entire TREC 13 set, and also for the first 100questions from the training set TREC 8?12.9 On av-erage, each question in the development set has 3.1positive and 17.1 negative answers.
There are 3.6positive and 20.0 negative answers per question inthe test set.We tokenized sentences using the standard tree-bank tokenization script, and then we performedpart-of-speech tagging using MXPOST tagger (Rat-naparkhi, 1996).
The resulting POS-tagged sen-tences were then parsed using MSTParser (McDon-ald et al, 2005), trained on the entire Penn Treebankto produce labeled dependency parse trees (we useda coarse dependency label set that includes twelvelabel types).
We used BBN Identifinder (Bikel et al,1999) for named-entity tagging.As answers in our task are considered to be sin-gle sentences, our evaluation differs slightly fromTREC, where an answer string (a word or phraselike 1977 or George Bush) has to be accompaniedby a supporting document ID.
As discussed by Pun-yakanok et al (2004), the single-sentence assump-tion does not simplify the task, since the hardest partof answer finding is to locate the correct sentence.From an end-user?s point of view, presenting thesentence that contains the answer is often more in-formative and evidential.
Furthermore, although thejudgement data in our case are more labor-intensiveto obtain, we believe our evaluation method is a bet-ter indicator than the TREC evaluation for the qual-ity of an answer selection algorithm.To illustrate the point, consider the example ques-tion, ?When did James Dean die??
The correct an-9More human-judged data are desirable, though we will ad-dress training from noisy, automatically judged data in Sec-tion 5.4.
It is important to note that human judgement of an-swer sentence correctness was carried out prior to any experi-ments, and therefore is unbiased.
The total number of questionsin TREC 13 is 230.
We exclude from the TREC 13 set questionsthat either have no correct answer candidates (27 questions), orno incorrect answer candidates (19 questions).
Any algorithmwill get the same performance on these questions, and thereforeobscures the evaluation results.
6 such questions were also ex-cluded from the 100 manually-judged training questions, result-ing in 94 questions for training.
For computational reasons (thecost of parsing), we also eliminated answer candidate sentencesthat are longer than 40 words from the training and evaluationset.
After these data preparation steps, we have 348 positiveQ-A pairs for training, 1,415 Q-A pairs in the development set,and 1,703 Q-A pairs in the test set.swer as appeared in the sentence ?In 1955, actorJames Dean was killed in a two-car collision nearCholame, Calif.?
is 1955.
But from the same docu-ment, there is another sentence which also contains1955: ?In 1955, the studio asked him to become atechnical adviser on Elia Kazan?s ?East of Eden,?starring James Dean.?
If a system missed the firstsentence but happened to have extracted 1955 fromthe second one, the TREC evaluation grants it a ?cor-rect and well-supported?
point, since the documentID matches the correct document ID?even thoughthe latter answer does not entail the true answer.
Ourevaluation does not suffer from this problem.We report two standard evaluation measures com-monly used in IR and QA research: mean av-erage precision (MAP) and mean reciprocal rank(MRR).
All results are produced using the standardtrec eval program.5.2 Baseline SystemsWe implemented two state-of-the-art answer-findingalgorithms (Cui et al, 2005; Punyakanok et al,2004) as strong baselines for comparison.
Cui etal.
(2005) is the answer-finding algorithm behindone of the best performing systems in TREC eval-uations.
It uses a mutual information-inspired scorecomputed over dependency trees and a single align-ment between them.
We found the method to be brit-tle, often not finding a score for a testing instancebecause alignment was not possible.
We extendedthe original algorithm, allowing fuzzy word align-ments through WordNet expansion; both results arereported.The second baseline is the approximate tree-matching work by Punyakanok et al (2004).
Theiralgorithm measures the similarity between ?q and ?aby computing tree edit distance.
Our replication isclose to the algorithm they describe, with one subtledifference.
Punyakanok et al used answer-typing incomputing edit distance; this is not available in ourdataset (and our method does not explicitly carry outanswer-typing).
Their heuristics for reformulatingquestions into statements were not replicated.
Wedid, however, apply WordNet type-checking and ap-proximate, penalized lexical matching.
Both resultsare reported.28development set test settraining dataset model MAP MRR MAP MRR100 manually-judged TreeMatch 0.4074 0.4458 0.3814 0.4462+WN 0.4328 0.4961 0.4189 0.4939Cui et al 0.4715 0.6059 0.4350 0.5569+WN 0.5311 0.6162 0.4271 0.5259Jeopardy (base only) 0.5189 0.5788 0.4828 0.5571Jeopardy 0.6812 0.7636 0.6029 0.6852+2,293 noisy Cui et al 0.2165 0.3690 0.2833 0.4248+WN 0.4333 0.5363 0.3811 0.4964Jeopardy (base only) 0.5174 0.5570 0.4922 0.5732Jeopardy 0.6683 0.7443 0.5655 0.6687Table 2: Results on development and test sets.
TreeMatch is our implementation of Punyakanok et al(2004); +WN modifies their edit distance function using WordNet.
We also report our implementation ofCui et al (2005), along with our WordNet expansion (+WN).
The Jeopardy base model and mixture withthe lexical-semantics log-linear model perform best; both are trained using conditional maximum likelihoodestimation.
The top part of the table shows performance using 100 manually-annotated question examples(questions 1?100 in TREC 8?12), and the bottom part adds noisily, automatically annotated questions 101?2,393.
Boldface marks the best score in a column and any scores in that column not significantly worseunder a a two-tailed paired t-test (p < 0.03).5.3 ResultsEvaluation results on the development and test setsof our model in comparison with the baseline algo-rithms are shown in Table 2.
Both our model andthe model in Cui et al (2005) are trained on themanually-judged training set (questions 1-100 fromTREC 8?12).
The approximate tree matching algo-rithm in Punyakanok et al (2004) uses fixed edit dis-tance functions and therefore does not require train-ing.
From the table we can see that our model signif-icantly outperforms the two baseline algorithms?even when they are given the benefit of WordNet?on both development and test set, and on both MRRand MAP.5.4 Experiments with Noisy Training DataAlthough manual annotation of the remaining 2,293training sentences?
answers in TREC 8?12 was toolabor-intensive, we did experiment with a simple,noisy automatic labeling technique.
Any answerthat had at least three non-stop word types seen inthe question and contains the answer pattern definedin the dataset was labeled as ?correct?
and used intraining.
The bottom part of Table 2 shows the re-sults.
Adding the noisy data hurts all methods, butthe Jeopardy model maintains its lead and consis-tently suffers less damage than Cui et al (2005).
(The TreeMatch method of Punyakanok et al (2004)does not use training examples.
)5.5 Summing vs. MaximizingUnlike most previous work, our model does not tryto find a single correspondence between words in thequestion and words in the answer, during training orduring testing.
An alternative method might choosethe best (most probable) alignment, rather than thesum of all alignment scores.
This involves a slightchange to Equation 3, replacing the summation witha maximization.
The change could be made duringtraining, during testing, or both.
Table 3 shows thatsumming is preferable, especially during training.6 DiscussionThe key experimental result of this work is thatloose syntactic transformations are an effective wayto carry out statistical question answering.One unique advantage of our model is the mix-ture of a factored, multinomial-based base modeland a potentially very rich log-linear model.
Thebase model gives our model robustness, and the log-29test settraining decoding MAP MRR?
?
0.6029 0.6852?
max 0.5822 0.6489max ?
0.5559 0.6250max max 0.5571 0.6365Table 3: Experimental results on comparing sum-ming over alignments (?)
with maximizing (max)over alignments on the test set.
Boldface marks thebest score in a column and any scores in that columnnot significantly worse under a a two-tailed paired t-test (p < 0.03).linear model allows us to throw in task- or domain-specific features.
Using a mixture gives the advan-tage of smoothing (in the base model) without hav-ing to normalize the log-linear model by summingover large sets.
This powerful combination leadsus to believe that our model can be easily portedto other semantic processing tasks where modelingsyntactic and semantic transformations is the key,such as textual entailment, paraphrasing, and cross-lingual QA.The traditional approach to cross-lingual QA isthat translation is either a pre-processing or post-processing step done independently from the mainQA task.
Notice that the QG formalism that we haveemployed in this work was originally proposed formachine translation.
We might envision transfor-mations that are performed together to form ques-tions from answers (or vice versa) and to translate?a Jeopardy!
game in which bilingual players mustask a question in a different language than that inwhich the answer is posed.7 ConclusionWe described a statistical syntax-based model thatsoftly aligns a question sentence with a candidateanswer sentence and returns a score.
Discrimina-tive training and a relatively straightforward, barely-engineered feature set were used in the implementa-tion.
Our scoring model was found to greatly out-perform two state-of-the-art baselines on an answerselection task using the TREC dataset.AcknowledgmentsThe authors acknowledge helpful input from threeanonymous reviewers, Kevin Gimpel, and DavidSmith.
This work is supported in part byARDA/DTO Advanced Question Answering forIntelligence (AQUAINT) program award numberNBCHC040164.ReferencesGiuseppe Attardi, Antonio Cisternino, FrancescoFormica, Maria Simi, Alessandro Tommasi, Ellen M.Voorhees, and D. K. Harman.
2001.
Selectively usingrelations to improve precision in question answering.In Proceedings of the 10th Text REtrieval Conference(TREC-10), Gaithersburg, MD, USA.Leonard E. Baum, Ted Petrie, George Soules, and Nor-man Weiss.
1970.
A maximization technique occur-ring in the statistical analysis of probabilistic functionsof Markov chains.
The Annals of Mathematical Statis-tics, 41(1):164?171.Daniel M. Bikel, Richard Schwartz, and Ralph M.Weischedel.
1999.
An algorithm that learns whats?
ina name.
Machine Learning, 34(1-3):211?231.Peter F. Brown, John Cocke, Stephen A. Della Pietra,Vincent J. Della Pietra, Frederick Jelinek, John D. Laf-ferty, Robert L. Mercer, and Paul S. Roossin.
1990.
Astatistical approach to machine translation.
Computa-tional Linguistics, 16(2):79?85.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19(2):263?311.Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-Seng Chua.
2005.
Question answering passage re-trieval using dependency relations.
In Proceedings ofthe 28th ACM-SIGIR International Conference on Re-search and Development in Information Retrieval, Sal-vador, Brazil.Arthur Dempster, Nan Laird, and Donald Rubin.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
Journal of the Royal Statistical Society,39(1):1?38.Yuan Ding and Martha Palmer.
2005.
Machine trans-lation using probabilistic synchronous dependency in-sertion grammars.
In Proceedings of the 43st AnnualMeeting of the Association for Computational Linguis-tics (ACL), Ann Arbor, MI, USA.Abdessamad Echihabi and Daniel Marcu.
2003.
Anoisy-channel approach to question answering.
InProceedings of the 41st Annual Meeting of the Associ-ation for Computational Linguistics (ACL), Sapporo,Japan.Jason Eisner.
2003.
Learning non-isomorphic tree map-pings for machine translation.
In Proceedings of the41st Annual Meeting of the Association for Computa-tional Linguistics (ACL), Sapporo, Japan.30Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proceed-ings of the 21st International Conference on Computa-tional Linguistics and the 44st Annual Meeting of theAssociation for Computational Linguistics (COLING-ACL), Sydney, Australia.Daniel Gildea.
2003.
Loosely tree-based alignment formachine translation.
In Proceedings of the 41st An-nual Meeting on Association for Computational Lin-guistics (ACL), Sapporo, Japan.Sanda Harabagiu and Andrew Hickl.
2006.
Methodsfor using textual entailment in open-domain questionanswering.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the 44thAnnual Meeting of the Association for ComputationalLinguistics (COLING-ACL), Sydney, Australia.Abraham Ittycheriah, Martin Franz, and Salim Roukos.2001.
IBM?s statistical question answering system?TREC-10.
In Proceedings of the 10th Text REtrievalConference (TREC-10), Gaithersburg, MD, USA.Tony Jebara and Alex Pentland.
1999.
Maximum con-ditional likelihood via bound maximization and theCEM algorithm.
In Proceedings of the 1998 Confer-ence on Advances in Neural Information ProcessingSystems II (NIPS), pages 494?500, Denver, CO, USA.Boris Katz and Jimmy Lin.
2003.
Selectively usingrelations to improve precision in question answering.In Proceedings of the EACL-2003 Workshop on Nat-ural Language Processing for Question Answering,Gaithersburg, MD, USA.Jinxi Xu Ana Licuanan and Ralph Weischedel.
2003.Trec2003 qa at bbn: Answering definitional questions.In Proceedings of the 12th Text REtrieval Conference(TREC-12), Gaithersburg, MD, USA.Kenneth C. Litkowski.
1999.
Question-answering us-ing semantic relation triples.
In Proceedings of the8th Text REtrieval Conference (TREC-8), Gaithers-burg, MD, USA.Dong C. Liu and Jorge Nocedal.
1989.
On the limitedmemory BFGS method for large scale optimization.Math.
Programming, 45:503?528.Bill MacCartney, Trond Grenager, Marie-Catherinede Marneffe, Daniel Cer, and Christopher D. Manning.2006.
Learning to recognize features of valid textualentailments.
In Proceedings of the Human LanguageTechnology Conference of the North American Chap-ter of the Association for Computational Linguistics(HLT-NAACL), New York, NY, USA.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of english: the penn treebank.
Computational Lin-guistics, 19(2):313?330.Ryan McDonald, Koby Crammer, and Fernado Pereira.2005.
Online large-margin training of dependencyparsers.
In Proceedings of the 43st Annual Meeting ofthe Association for Computational Linguistics (ACL),Ann Arbor, MI, USA.I.
Dan Melamed.
2004.
Algorithms for syntax-awarestatistical machine translation.
In Proceedings of theConference on Theoretical and Methodological Issuesin Machine Translation (TMI), Baltimore, MD, USA.Xiao-Li Meng and Donald B. Rubin.
1993.
Maximumlikelihood estimation via the ECM algorithm: A gen-eral framework.
Biometrika, 80:267?278.George A. Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine J. Miller.
1990.WordNet: an on-line lexical database.
InternationalJournal of Lexicography, 3(4).Vanessa Murdock and W. Bruce Croft.
2005.
A trans-lation model for sentence retrieval.
In Proceedings ofthe conference on Human Language Technology andEmpirical Methods in Natural Language Processing(HLT-EMNLP), Vancouver, BC, USA.Vasin Punyakanok, Dan Roth, and Wen-Tau Yih.
2004.Mapping dependencies trees: An application to ques-tion answering.
In Proceedings of the 8th Interna-tional Symposium on Artificial Intelligence and Math-ematics, Fort Lauderdale, FL, USA.Chris Quirk, Chris Brockett, and William Dolan.
2004.Monolingual machine translation for paraphrase gen-eration.
In Proceedings of the 2004 Conference onEmpirical Methods in Natural Language Processing(EMNLP), Barcelona, Spain.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal SMT.
In Proceedings of the 43rd AnnualMeeting on Association for Computational Linguistics(ACL), Ann Arbor, MI, USA.Adwait Ratnaparkhi.
1996.
A maximum entropy part-of-speech tagger.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP), Philadelphia, PA, USA.Deepak Ravichandran, Abharam Ittycheriah, and SalimRoukos.
2003.
Automatic derivation of surface textpatterns for a maximum entropy based question an-swering system.
In Proceedings of the Human Lan-guage Technology Conference and North AmericanChapter of the Association for Computational Linguis-tics (HLT-NAACL), Edmonton, Canada.Dan Shen and Dietrich Klakow.
2006.
Exploring corre-lation of dependency relation paths for answer extrac-tion.
In Proceedings of the 21st International Confer-ence on Computational Linguistics and 44th AnnualMeeting of the Association for Computational Linguis-tics (COLING-ACL), Sydney, Australia.Dan Shen, Geert-Jan M. Kruijff, and Dietrich Klakow.2005.
Exploring syntactic relation patterns for ques-tion answering.
In Proceedings of the Second Interna-tional Joint Conference on Natural Language Process-ing (IJCNLP), Jeju Island, Republic of Korea.Hideki Shima, Mengqiu Wang, Frank Lin, and TerukoMitamura.
2006.
Modular approach to error analysisand evaluation for multilingual question answering.
InProceedings of the Fifth International Conference onLanguage Resources and Evaluation (LREC), Genoa,Italy.31David A. Smith and Jason Eisner.
2006.
Quasi-synchronous grammars: Alignment by soft projectionof syntactic dependencies.
In Proceedings of the HLT-NAACL Workshop on Statistical Machine Translation,New York, NY, USA.Martin M. Soubbotin and Sergei M. Soubbotin.
2001.Patterns for potential answer expressions as clues tothe right answers.
In Proceedings of the 10th TextREtrieval Conference (TREC-10), Gaithersburg, MD,USA.Dekai Wu and Hongsing Wong.
1998.
Machinetranslation with a stochastic grammatical channel.In Proceedings of the 17th International Conferenceon Computational Linguistics (COLING), Montreal,Canada.Dekai Wu.
2005.
Recognizing paraphrases and textualentailment using inversion transduction grammars.
InProceedings of the ACL Workshop on Empirical Mod-eling of Semantic Equivalence and Entailment, AnnArbor, MI, USA.Kenji Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
In Proceedings of the39th Annual Meeting of the Association for Compu-tational Linguistics (ACL), Toulouse, France.32
