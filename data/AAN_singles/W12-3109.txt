Proceedings of the 7th Workshop on Statistical Machine Translation, pages 91?95,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsBlack Box Features for the WMT 2012 Quality Estimation Shared TaskChristian BuckSchool of InformaticsUniversity of EdinburghEdinburgh, UK, EH8 9ABchristian.buck@ed.ac.ukAbstractIn this paper we introduce a number of newfeatures for quality estimation in machinetranslation that were developed for the WMT2012 quality estimation shared task.
We findthat very simple features such as indicators ofcertain characters are able to outperform com-plex features that aim to model the connectionbetween two languages.1 Introduction and TaskThis paper describes the features and setup used inour submission to the WMT 2012 quality estimation(QE) shared task.
Given a machine translation (MT)system and a corpus of its translations which havebeen rated by humans, the task is to build a predic-tor that can accurately estimate the quality of fur-ther translations.
The human ratings range from 1(incomprehensible) to 5 (perfect translation) and aregiven as the mean rating of three different judges.Formally we are presented with a source sentencefJ1 and a translation eI1 and we need to assign a scoreS(fJ1 , eI1) ?
[1, 5] or, in the ranking task, order thesource-translation pairs by expected quality.2 ResourcesThe organizers have made available a baseline QEsystem that consists of a number of well establishedfeatures (Blatz et al, 2004) and serves as a startingpoint for development.
Furthermore the MT systemthat generated the translations is available along withits training data.
Compared to the large training cor-pus of the MT engine, the QE system is based on amuch smaller training set as detailed in Table 1.# sentenceseuroparl-nc 1,714,385train 1,832test 422Table 1: Corpus statistics3 FeaturesIn the literature (Blatz et al, 2004) a large numberof features have been considered for confidence es-timation.
These can be grouped into four generalcategories:1.
Source features make a statement about thesource sentence, assessing the difficulty oftranslating a particular sentence with the sys-tem at hand.
Some sentences may be very easyto translate, e.g.
short and common phrases,while long and complex sentences are still be-yond the system?s capabilities.2.
Translation features model the connection be-tween source and target.
While this is veryclosely related to the general problem of ma-chine translation, the advantage in confidenceestimation is that we can exercise unconstruc-tive criticism, i.e.
point out errors without of-fering a better translation.
In addition, there isno need for an efficient search algorithm, thusallowing for more complex models.3.
Target features judge the translation of the sys-tem without regarding in which way it wasproduced.
They often resemble the language91model used in the noisy channel formulation(Brown et al, 1993) but can also pinpoint morespecific issues.
In practice, the same features asfor the source side can be used; the interpreta-tion however is different.4.
Engine features are often referred to as glassbox features (Specia et al, 2009).
They de-scribe the process which produced the transla-tion in question and usually rely on the innerworkings of the MT system.
Examples includemodel scores and word posterior probabilities(WPP) (Ueffing et al, 2003).In this work we focus on the first three categoriesand ignore the particular system that produced thetranslations.
Such features are commonly referredto as black box features.
While some glass box fea-tures, e.g.
word posterior probabilities, have led topromising results in the past, we chose to explorenew features potentially applicable to translationsfrom any source, e.g.
translations found on the web.3.1 Binary IndicatorsMTranslatability (Bernth and Gdaniec, 2001) gives anotion of the structural complexity of a sentence thatrelates to the quality of the produced translation.
Inthe literature, several characteristics that may hin-der proper translation have been identified, amongthem poor grammar and misplaced punctuation.
Asa very simple approximation we implement binaryindicators that detect clauses by looking for quota-tion marks, hyphens, commas, etc.
Another binaryfeature marks numbers and uppercase words.3.2 Named EntitiesAnother aspect that might pose a potential problemto MT is the occurrence of words that were only ob-served a few times or in very particular contexts, asit is often the case for Named Entities.
We used theStanford NER Tagger (Finkel et al, 2005) to detectwords that belong to one of four groups: Person, Lo-cation, Organization and Misc.
Each group is repre-sented by a binary feature.Counts are given in Table 2.
The test set has sig-nificantly less support for the Misc category, possi-bly hinting that this data was taken from a differentsource or document.
To avoid the danger of biasingtrain (src) test (src)abs rel abs relPerson 623 34% 141 33%Location 479 26% 99 23%Organization 505 28% 110 26%Misc 428 23% 53 13%Table 2: Distribution of Named Entities.
The counts arebased on a binary features, i.e.
multiple occurrences aretreated as a single one.the classifier we decided not to use the Misc indica-tor in our experiments.3.3 Backoff BehaviorIn related work (Raybaud et al, 2011) the backoffbehavior of a 3-gram LM was found to be the mostpowerful feature for word level QE.
We compute foreach word the longest seen n-gram (up to n = 4)and take the average length as a feature.
N-grams atthe beginning of a sentence are extended with <s>tokens to avoid penalizing short sentences.
This isdone on both the source and target side.3.4 Discriminative Word LexiconFollowing the approach of Mauser et al (2009) wetrain log-linear binary classifiers that directly modelp(e|fJ1 ) for each word e ?
eI1:p(e|fJ1 ) =exp(?f?fJ1?e,f)1 + exp(?f?fJ1?e,f) (1)where ?e,f are the trained model weights.
Pleasenote that this introduces a global dependence on thesource sentence so that every source word may influ-ence the choice of all words in eI1 as opposed to thelocal dependencies found in the underlying phrase-based MT system.Assuming independence among the words in thetranslated sentence we could compute the probabil-ity of the sentence pair as:p(eI1|fJ1 ) =?e?eI1p(e|fJ1 ) ??e/?eI1(1?
p(e|fJ1 )).
(2)In practice the second part of Equation (2) is toonoisy to be useful given the large number of words92source resumption of the sessiontarget reanudacio?n del per?
?odo de sesionesTable 3: Example entry of filtered training corpus.that do not appear in the sentence at hand.
We there-fore focus on the observed words and use the geo-metric mean of their individual probabilities:xDWL(fJ1 , eI1) =??
?e?eI1p(e|fJ1 )??1/I.
(3)We also compute the probability of the lowestscoring word as an additional feature:xDWLmin(fJ1 , eI1) = mine?eI1p(e|fJ1 ).
(4)3.5 Neural NetworksWe seek to directly predict the words in eI1 usinga neural network.
In order to do so, both sourceand target sentence are encoded as high dimensionalvectors in which positive entries mark the occur-rence of words.
This representation is commonlyreferred to as the vector space model and has beensuccessfully used for information retrieval.The dimension of the vector representation is de-termined by the respective sizes of the source andtarget vocabulary.
Without further pre-processingwe would need to learn a mapping from a 90k (|Vf |)to a 170k (|Ve|) dimensional space.
Even though ourimplementation is specifically tailored to exploit thesparsity of the data, such high dimensionality makestraining prohibitively expensive.Two approaches to reduce dimensionality are ex-plored in this work.
First, we simply remove allwords that never occur in the QE data of 2,254 sen-tences from the corpus leaving 8,365 input and 9,000output nodes.
This reduces the estimated trainingtime from 11 days to less than 6 hours per iteration1.Standard stochastic gradient decent on a three-layerfeed-forward network is used.As shown in Table 3 the filtering can lead to arti-facts in which case an erroneous mapping is learned.Moreover the filtering approach does not scale wellas the QE corpus and thereby the vocabulary grows.1using a 2.66 GHz Intel Xeon and 2 threadsOur second approach to reduce dimensionalityuses the hashing trick (Weinberger et al, 2009): ahash function is applied to each word and the sen-tence is represented by the hashed values whichare again transformed using vector space model asabove.
The dimensionality reduction is due to thefact that there are less possible hash values thanwords in the vocabulary.
To reduce the loss of infor-mation due to collisions, several different hash func-tions are used.
The resulting vector representationclosely resembles a Bloom Filter (Bloom, 1970).This approach scales well but introduces two newparameters: the number of hash functions to useand the dimensionality of the resulting space.
Inour experiments we have used SHA-1 hashes withthree different salts of which we used the first 12bits, thereby mapping the sentences into a 4096-dimensional space.The results presented in Section 4 based on net-works with 500 hidden nodes which were trained forat least 10 iterations.
The networks are not traineduntil convergence due to time constraints; additionaltraining iterations will likely result in better per-formance.
Experiments using 250 or 1000 hiddennodes showed very similar results.After the models are trained we compare the pre-dicted and the observed target vectors and derivetwo features: (i) the euclidean distance, denoted asNNdist and HNNdist for the filtered and hashed ver-sions respectively and (ii) the geometric mean ofthose dimensions where we expect a positive value,denoted as NNprop+ and HNNprob+ in Table 5.3.6 Edit DistanceUsing Levenshtein Distance we computed the dis-tance to the closest entry in the training corpus.
Theidea is that a sentence that was already seen almostidentically would be easier to translate.
Likewise,a translation that is very close to an element of thecorpus is likely to be a good translation.
This wasperformed for both source and target side and oncharacter as well as on word level giving a total offour (EDIT) scores.
The scores are normalized bythe length of the respective lines.93source corpus ?
?
"europarl-nc 37 227 25,637train 0 0 641test 78 76 100Table 4: Counts of different quotation mark characters.4 ExperimentsIn this work we focus on the prediction of humanassessment of translation quality, i.e.
the regressiontask of the WMT12 QE shared task.
Our submissionfor the ranking task is derived from the order impliedby the predicted scores without further re-ranking.In general our efforts were directed towards fea-ture engineering and not to the machine learning as-pects.
Therefore, we apply a standard pipeline anduse neural networks for regression.
All parametertuning is performed using 5-fold cross validation onthe baseline set of 17 features as provided by the or-ganizers.4.1 Preprocessing and AnalysisTo avoid including our own judgment, no more thanthe first ten lines of the test data were visually in-spected in order to ensure that the training and testdata was preprocessed in the same manner.
Further-more, the distribution of individual characters wasinvestigated.
As shown in Table 4, the test data dif-fers from the training corpus in treatment of quo-tation marks.
Hence, we replaced all typographi-cal quotation marks ( ?, ? )
with the standard doublequote symbol (").Prior to computation of the features described inSubsections 3.3, 3.4 and 3.5 all numbers are re-placed with a special $number token.Baseline features are used without further scal-ing; experiments where all features were scaled tothe [0, 1] range showed a drop in accuracy.While we implemented the training ourselves forthe features presented in Subsection 3.5, the opensource neural network library FANN2 is used forall experiments in this section.
As the performanceof individual classifiers shows a high variance, pre-sumably due to local minima, all experiments areconducted using ensembles on 500 networks trained2http://leenissen.dk/fann/wp/Feature (Section) MAE RMSE |PCC|BACKOFF (3.3) 0.0 0.0INDICATORS (3.1) +0.5 +0.7NER (3.2) +0.5 +0.4DWLmin (3.4) ?0.1 ?0.1 0.19DWL (3.4) 0.0 ?0.1 0.36EDIT (3.6) - tgt words 0.0 0.0 0.32EDIT (3.6) - tgt chars ?0.1 0.0 0.27EDIT (3.6) - src words 0.0 0.0 0.36EDIT (3.6) - src chars +0.2 +0.1 0.37NNdist (3.5) 0.0 0.0 0.35NNprob+ (3.5) +0.1 +0.2 0.35HNNdist (3.5) 0.0 0.0 0.37HNNprob+ (3.5) +0.1 +0.1 0.35Table 5: Analysis of individual features using 5-foldcross-validation.
Positive values indicate improvementover a baseline of MAE 57.7% and RMSE 72.7%; e.g.including the DWL feature actually worsens RMSE from72.7% to 72.8%.The last column gives the Pearson correlation coefficientbetween the feature and the score if the feature is a singlecolumn.
This information was not used in feature selec-tion as it is not based on cross validation.with random initialization.
Their consensus is com-puted as the average of the individual predictions.4.2 Feature EvaluationTo evaluate the contribution of individual features,each feature is tested in conjunction with all base-line features, using the parameters that were opti-mized on the baseline set.
This slightly favors thebaseline features but we still expect that expressiveadditional features lead to a noticeable performancegain.
The results are detailed in Table 5.
In addi-tion to the main evaluation metrics, mean averageerror (MAE) and root mean squared error (RMSE),we report the Pearson correlation coefficient (PCC)as a measure of predictive strength of a single fea-ture.
Because features are not used alone this doesnot directly translate into overall performance.
Still,it can be observed that our proposed features showgood correlation to the target variable.
For compari-son, among the baseline features only 2 of 17 reacha PCC of over 0.3.While the results generally remain inconclusive,some very simple features that indicate difficulties94for the translation engine show good performance.In particular binary markers of named entities andand the indicator features introduced in Subsection3.1 perform well.
Further experiments with the lattershow their contribution to the systems performancecan be attributed to a single feature: the indicator ofthe genitive case, i.e.
occurrences of ?s or s?.Testing more combinations of simple and com-plex features may lead to improvements at the riskof over-fitting on the cross validation setup.
As asimple remedy several feature sets were created atrandom, always combining all baseline features andseveral new features presented in this paper.
Averag-ing of the individual results of all sets that performedbetter than the baseline resulted in our submission.4.3 Results and DiscussionOf all the features detailed only a few lead to a con-siderable improvement.
This is also reflected by ourresults on the test data which are nearly indistin-guishable from the performance of the baseline sys-tem.
While this is disappointing, our more complexfeatures introduce a number of free parameters andfurther experimentation will be needed to conclu-sively assess their usefulness.
In particular, featuresbased on neural networks can be further optimizedand tested in other settings.Even though the machine learning aspects of thistask are not the focus of this work we are confidentthat the proposed setup is sound and can be reusedin further evaluations.5 ConclusionWe described a number of new features that can beused to predict human judgment of translation qual-ity.
Results suggest pointing out sentences that arehard to translate, e.g.
because they are too complex,is a promising approach.We presented a detailed evaluation of the utilityof individual features and a solid baseline setup forfurther experimentation.
The system, based on anensemble of neural networks, is insensitive to pa-rameter settings and yields competitive results.Our new features can potentially be applied for amultitude of applications and may deliver insightsinto the fundamental problems that cause translationerrors, thus aiding the progress in MT research.AcknowledgmentsThis work was supported by the MateCAT project,which is funded by the EC under the 7th FrameworkProgramme.ReferencesArendse Bernth and Claudia Gdaniec.
2001.
Mtranslata-bility.
Machine Translation, 16(3):175?218, Septem-ber.John Blatz, Erin Fitzgerald, George Foster, Simona Gan-drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,and Nicola Ueffing.
2004.
Confidence estimation formachine translation.
In Proceedings of Coling 2004,pages 315?321, Geneva, Switzerland, August.Burton H. Bloom.
1970.
Space/time trade-offs in hashcoding with allowable errors.
Communications of theACM, 13(7):422?426, July.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19(2):263?312.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informationinto information extraction systems by gibbs sampling.In Proceedings of the 43rd Annual Meeting on Associ-ation for Computational Linguistics, ACL 2005, pages363?370, Stroudsburg, PA, USA.Arne Mauser, Sas?a Hasan, and Hermann Ney.
2009.
Ex-tending statistical machine translation with discrimi-native and trigger-based lexicon models.
In Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 210?217, Singapore, August.Sylvain Raybaud, David Langlois, and Kamel Sma??li.2011.
?this sentence is wrong.?
detecting errors inmachine-translated sentences.
Machine Translation,25(1):1?34, March.Lucia Specia, Nicola Cancedda, Marc Dymetman, MarcoTurchi, and Nello Cristianini.
2009.
Estimatingthe sentence-level quality of machine translation sys-tems.
In Proceedings of the 13th Annual Conferenceof the European Association for Machine Translation,EAMT-2009, pages 28?35, Barcelona, Spain, May.Nicola Ueffing, Klaus Macherey, and Hermann Ney.2003.
Confidence measures for statistical machinetranslation.
In Machine Translation Summit, pages394?401, New Orleans, LA, September.Kilian Q. Weinberger, Anirban Dasgupta, John Langford,Alexander J. Smola, and Josh Attenberg.
2009.
Fea-ture hashing for large scale multitask learning.
InICML ?09: Proceedings of the 26th Annual Interna-tional Conference on Machine Learning, ACM Inter-national Conference Proceeding Series, pages 1113?1120, Montreal, Quebec, Canada, June.95
