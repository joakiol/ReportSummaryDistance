Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 73?76,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPGeneralizing over Lexical Features:Selectional Preferences for Semantic Role ClassificationBe?nat Zapirain, Eneko AgirreIxa TaldeaUniversity of the Basque CountryDonostia, Basque Country{benat.zapirain,e.agirre}@ehu.esLlu?
?s M`arquezTALP Research CenterTechnical University of CataloniaBarcelona, Catalonialluism@lsi.upc.eduAbstractThis paper explores methods to allevi-ate the effect of lexical sparseness in theclassification of verbal arguments.
Weshow how automatically generated selec-tional preferences are able to generalizeand perform better than lexical features ina large dataset for semantic role classifi-cation.
The best results are obtained witha novel second-order distributional simi-larity measure, and the positive effect isspecially relevant for out-of-domain data.Our findings suggest that selectional pref-erences have potential for improving a fullsystem for Semantic Role Labeling.1 IntroductionSemantic Role Labeling (SRL) systems usuallyapproach the problem as a sequence of two sub-tasks: argument identification and classification.While the former is mostly a syntactic task, thelatter requires semantic knowledge to be takeninto account.
Current systems capture semanticsthrough lexicalized features on the predicate andthe head word of the argument to be classified.Since lexical features tend to be sparse (especiallywhen the training corpus is small) SRL systemsare prone to overfit the training data and general-ize poorly to new corpora.This work explores the usefulness of selectionalpreferences to alleviate the lexical dependence ofSRL systems.
Selectional preferences introducesemantic generalizations on the type of argumentspreferred by the predicates.
Therefore, they areexpected to improve generalization on infrequentand unknown words, and increase the discrimina-tive power of the argument classifiers.For instance, consider these two sentences:JFK was assassinated (in Dallas)LocationJFK was assassinated (in November)TemporalBoth share syntactic and argument structure, sothe lexical features (i.e., the words ?Dallas?
and?November?)
represent the most important knowl-edge to discriminate between the two different ad-junct roles.
The problem is that, in new text,one may encounter similar expressions with newwords like Texas or Autumn.We propose a concrete classification problem asour main evaluation setting for the acquired selec-tional preferences: given a verb occurrence anda nominal head word of a constituent dependanton that verb, assign the most plausible role to thehead word according to the selectional preferencemodel.
This problem is directly connected to ar-gument classification in SRL, but we have iso-lated the evaluation from the complete SRL task.This first step allows us to analyze the potentialof selectional preferences as a source of seman-tic knowledge for discriminating among differentrole labels.
Ongoing work is devoted to the inte-gration of selectional preference?derived featuresin a complete SRL system.2 Related WorkAutomatic acquisition of selectional preferencesis a relatively old topic, and will mention themost relevant references.
Resnik (1993) proposedto model selectional preferences using semanticclasses from WordNet in order to tackle ambiguityissues in syntax (noun-compounds, coordination,PP-attachment).Brockman and Lapata (2003) compared sev-eral class-based models (including Resnik?s se-lectional preferences) on a syntactic plausibilityjudgement task for German.
The models re-turn weights for (verb, syntactic function, noun)triples, and the correlation with human plausibil-ity judgement is used for evaluation.
Resnik?sselectional preference scored best among class-based methods, but it performed equal to a simple,purely lexical, conditional probability model.73Distributional similarity has also been used totackle syntactic ambiguity.
Pantel and Lin (2000)obtained very good results using the distributionalsimilarity measure defined by Lin (1998).The application of selectional preferences to se-mantic roles (as opposed to syntactic functions)is more recent.
Gildea and Jurafsky (2002) isthe only one applying selectional preferences ina real SRL task.
They used distributional clus-tering and WordNet-based techniques on a SRLtask on FrameNet roles.
They report a very smallimprovement of the overall performance when us-ing distributional clustering techniques.
In this pa-per we present complementary experiments, witha different role set and annotated corpus (Prop-Bank), a wider range of selectional preferencemodels, and the analysis of out-of-domain results.Other papers applying semantic preferencesin the context of semantic roles, rely on theevaluation on pseudo tasks or human plausibil-ity judgments.
In (Erk, 2007) a distributionalsimilarity?based model for selectional preferencesis introduced, reminiscent of that of Pantel andLin (2000).
The results over 100 frame-specificroles showed that distributional similarities getsmaller error rates than Resnik and EM, with Lin?sformula having the smallest error rate.
Moreover,coverage of distributional similarities and Resnikare rather low.
Our distributional model for selec-tional preferences follows her formalization.Currently, there are several models of distri-butional similarity that could be used for selec-tional preferences.
More recently, Pad?o and Lap-ata (2007) presented a study of several parametersthat define a broad family of distributional similar-ity models, including publicly available software.Our paper tests similar techniques to those pre-sented above, but we evaluate selectional prefer-ence models in a setting directly related to SRclassification, i.e., given a selectional preferencemodel for a verb we find the role which fits bestfor a given head word.
The problem is indeedqualitatively different: we do not have to chooseamong the head words competing for a role (asin the papers above) but among selectional prefer-ences competing for a head word.3 Selectional Preference ModelsIn this section we present all the variants for ac-quiring selectional preferences used in our study,and how we apply them to the SR classification.WordNet-based SP models: we use Resnik?s se-lectional preference model.Distributional SP models: Given the availabil-ity of publicly available resources for distribu-tional similarity, we used 1) a ready-made the-saurus (Lin, 1998), and 2) software (Pad?o and La-pata, 2007) which we run on the British NationalCorpus (BNC).In the first case, Lin constructed his thesaurusbased on his own similarity formula run over alarge parsed corpus comprising journalism texts.The thesaurus lists, for each word, the most sim-ilar words, with their weight.
In order to get thesimilarity for two words, we could check the entryin the thesaurus for either word.
But given thatthe thesaurus is not symmetric, we take the av-erage of both similarities.
We will refer to thissimilarity measure as simthlin.
Another option isto use second-order similarity, where we computethe similarity of two words using the entries in thethesaurus, either using the cosine or Jaccard mea-sures.
We will refer to these similarity measuresas simth2jacand simth2coshereinafter.For the second case, we tried the optimal pa-rameters as described in (Pad?o and Lapata, 2007,p.
179): word-based space, medium context, log-likelihood association, and 2,000 basis elements.We tested Jaccard, cosine and Lin?s measure (Lin,1998) for similarity, yielding simjac, simcosandsimlin, respectively.3.1 Role Classification with SP ModelsGiven a target sentence where a predicate and sev-eral potential argument and adjunct head wordsoccur, the goal is to assign a role label to each ofthe head words.
The classification of candidatehead words is performed independently of eachother.Since we want to evaluate the ability of selec-tional preference models to discriminate amongdifferent roles, this is the only knowledge that willbe used to perform classification (avoiding the in-clusion of any other feature commonly used inSRL).
Thus, for each head word, we will simplyselect the role (r) of the predicate (p) which fitsbest the head word (w).
This selection rule is for-malized as:R(p, w) = argmaxr?Roles(p)S(p, r, w)being S(p, r, w) the prediction of the selectionalpreference model, which can be instantiated withall the variants mentioned above.74For the sake of comparison we also define a lex-ical baseline model, which will determine the con-tribution of lexical features in argument classifica-tion.
For a test pair (p, w) the model returns therole under which the head word occurred most of-ten in the training data given the predicate.4 Experimental SettingThe data used in this work is the benchmark cor-pus provided by the CoNLL-2005 shared task onSRL (Carreras and M`arquez, 2005).
The dataset,of over 1 million tokens, comprises PropBank sec-tions 02-21 for training, and sections 24 and 23 fordevelopment and test, respectively.
In these ex-periments, NEG, DIS and MOD arguments havebeen discarded because, apart from not being con-sidered ?pure?
adjunct roles, the selectional pref-erences implemented in this study are not able todeal with non-nominal argument heads.The predicate?rol?head (p, r, w) triples for gen-eralizing the selectional preferences are extractedfrom the arguments of the training set, yield-ing 71,240 triples, from which 5,587 differentpredicate-role selectional preferences (p, r) arederived by instantiating the different models inSection 3.Selectional preferences are then used, to predictthe corresponding roles of the (p, w) pairs fromthe test corpora.
The test set contains 4,134 pairs(covering 505 different predicates) to be classifiedinto the appropriate role label.
In order to studythe behavior on out-of-domain data, we also testedon the PropBanked part of the Brown corpus.
Thiscorpus contains 2,932 (p, w) pairs covering 491different predicates.The performance of each selectional preferencemodel is evaluated by calculating the standard pre-cision, recall and F1measures.
It is worth men-tioning that none of the models is able to predictthe role when facing an unknown head word.
Thishappens more often with WordNet based models,which have a lower word coverage compared todistributional similarity?based models.5 Results and DiscussionThe results are presented in Table 1.
The lexi-cal row corresponds to the baseline lexical matchmethod.
The following row corresponds to theWordNet-based selectional preference model.
Thedistributional models follow, including the resultsobtained by the three similarity formulas on theprec.
rec.
F1prec.
recall F1lexical .779 .349 .482 .663 .059 .108res .589 .495 .537 .505 .379 .433simJac.573 .564 .569 .481 .452 .466simcos.607 .598 .602 .507 .476 .491simLin.580 .560 .570 .500 .470 .485simthLin.635 .625 .630 .494 .464 .478simth2Jac.657 .646 .651 .531 .499 .515simth2cos.654 .644 .649 .531 .499 .515Table 1: Results for WSJ test (left), and Browntest (right)co-occurrences extracted from the BNC (simJac,simcossimLin), and the results obtained whenusing Lin?s thesaurus directly (simthLin) and as asecond-order vector (simth2Jacand simth2cos).As expected, the lexical baseline attains veryhigh precision in all datasets, which underscoresthe importance of the lexical head word featuresin argument classification.
The recall is quitelow, specially in Brown, confirming and extend-ing (Pradhan et al, 2008), which also reports sim-ilar performance drops when doing argument clas-sification on out-of-domain data.One of the main goals of our experiments is toovercome the data sparseness of lexical featuresboth on in-domain and out-of-domain data.
Allour selectional preference models improve overthe lexical matching baseline in recall, up to 30absolute percentage points in the WSJ test datasetand 44 absolute percentage points in the Browncorpus.
This comes at the cost of reduced preci-sion, but the overall F-score shows that all selec-tional preference models improve over the base-line, with up to 17 absolute percentage pointson the WSJ datasets and 41 absolute percentagepoints on the Brown dataset.
The results, thus,show that selectional preferences are indeed alle-viating the lexical sparseness problem.As an example, consider the following headwords of potential arguments of the verb wearfound in the test set: doctor, men, tie, shoe.
Noneof these nouns occurred as heads of arguments ofwear in the training data, and thus the lexical fea-ture would be unable to predict any role for them.Using selectional preferences, we successfully as-signed the Arg0 role to doctor and men, and theArg1 role to tie and shoe.Regarding the selectional preference variants,WordNet-based and first-order distributional sim-ilarity models attain similar levels of precision,but the former are clearly worse on recall and F1.75The performance loss on recall can be explainedby the worse lexical coverage of WordNet whencompared to automatically generated thesauri.
Ex-amples of words missing in WordNet include ab-breviations (e.g., Inc., Corp.) and brand names(e.g., Texaco, Sony).
The second-order distribu-tional similarity measures perform best overall,both in precision and recall.
As far as we know,it is the first time that these models are applied toselectional preference modeling, and they prove tobe a strong alternative to first-order models.
Therelative performance of the methods is consistentacross the two datasets, stressing the robustness ofall methods used.Regarding the use of similarity software (Pad?oand Lapata, 2007) on the BNC vs. the use ofLin?s ready-made thesaurus, both seem to performsimilarly, as exemplified by the similar results ofsimLinand simthLin.
The fact that the former per-formed better on the Brown data, and worse on theWSJ data could be related to the different corporaused to compute the co-occurrence, balanced cor-pus and journalism texts respectively.
This couldbe an indication of the potential of distributionalthesauri to adapt to the target domain.Regarding the similarity metrics, the cosineseems to perform consistently better for first-orderdistributional similarity, while Jaccard providedslightly better results for second-order similarity.The best overall performance was for second-order similarity, also using the cosine.
Giventhe computational complexity involved in build-ing a complete thesaurus based on the similaritysoftware, we used the ready-made thesaurus ofLin, but could not try the second-order version onBNC.6 Conclusions and Future WorkWe have empirically shown how automaticallygenerated selectional preferences, using WordNetand distributional similarity measures, are able toeffectively generalize lexical features and, thus,improve classification performance in a large-scale argument classification task on the CoNLL-2005 dataset.
The experiments show substantialgains on recall and F1compared to lexical match-ing, both on the in-domain WSJ test and, espe-cially, on the out-of-domain Brown test.Alternative selectional models were studied andcompared.
WordNet-based models attain goodlevels of precision but lower recall than distribu-tional similarity methods.
A new second-ordersimilarity method proposed in this paper attainsthe best results overall in all datasets.The evidence gathered in this paper suggeststhat using semantic knowledge in the form of se-lectional preferences has a high potential for im-proving the results of a full system for SRL, spe-cially when training data is scarce or when appliedto out-of-domain corpora.Current efforts are devoted to study the integra-tion of the selectional preference models presentedin this paper in a in-house SRL system.
We areparticularly interested in domain adaptation, andwhether distributional similarities can profit fromdomain corpora for better performance.AcknowledgmentsThis work has been partially funded by the EU Commis-sion (project KYOTO ICT-2007-211423) and Spanish Re-search Department (project KNOW TIN2006-15049-C03-01).
Be?nat enjoys a PhD grant from the University of theBasque Country.ReferencesCarsten Brockmann and Mirella Lapata.
2003.
Evaluatingand combining approaches to selectional preference ac-quisition.
In Proceedings of the 10th Conference of theEuropean Chapter of the ACL, pages 27?34.X.
Carreras and L. M`arquez.
2005.
Introduction to theCoNLL-2005 Shared Task: Semantic role labeling.
InProceedings of the Ninth Conference on ComputationalNatural Language Learning (CoNLL-2005), pages 152?164, Ann Arbor, MI, USA.Katrin Erk.
2007.
A simple, similarity-based model for se-lectional preferences.
In Proceedings of the 45th AnnualMeeting of the Association of Computational Linguistics,pages 216?223, Prague, Czech Republic.D.
Gildea and D. Jurafsky.
2002.
Automatic labeling of se-mantic roles.
Computational Linguistics, 28(3):245?288.Dekang Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In COLING-ACL, pages 768?774.Sebastian Pad?o and Mirella Lapata.
2007.
Dependency-based construction of semantic space models.
Computa-tional Linguistics, 33(2):161?199, June.Patrick Pantel and Dekang Lin.
2000.
An unsupervised ap-proach to prepositional phrase attachment using contex-tually similar words.
In Proceedings of the 38th AnnualConference of the ACL, pages 101?108.S.
Pradhan, W. Ward, and J. H. Martin.
2008.
Towards robustsemantic role labeling.
Computational Linguistics, 34(2).Philip Resnik.
1993.
Semantic classes and syntactic ambigu-ity.
In Proceedings of the workshop on Human LanguageTechnology, pages 278?283, Morristown, NJ, USA.76
