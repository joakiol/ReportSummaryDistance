Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631?1642,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsRecursive Deep Models for Semantic CompositionalityOver a Sentiment TreebankRichard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang,Christopher D. Manning, Andrew Y. Ng and Christopher PottsStanford University, Stanford, CA 94305, USArichard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu{jeaneis,manning,cgpotts}@stanford.eduAbstractSemantic word spaces have been very use-ful but cannot express the meaning of longerphrases in a principled way.
Further progresstowards understanding compositionality intasks such as sentiment detection requiresricher supervised training and evaluation re-sources and more powerful models of com-position.
To remedy this, we introduce aSentiment Treebank.
It includes fine grainedsentiment labels for 215,154 phrases in theparse trees of 11,855 sentences and presentsnew challenges for sentiment composition-ality.
To address them, we introduce theRecursive Neural Tensor Network.
Whentrained on the new treebank, this model out-performs all previous methods on several met-rics.
It pushes the state of the art in singlesentence positive/negative classification from80% up to 85.4%.
The accuracy of predictingfine-grained sentiment labels for all phrasesreaches 80.7%, an improvement of 9.7% overbag of features baselines.
Lastly, it is the onlymodel that can accurately capture the effectsof negation and its scope at various tree levelsfor both positive and negative phrases.1 IntroductionSemantic vector spaces for single words have beenwidely used as features (Turney and Pantel, 2010).Because they cannot capture the meaning of longerphrases properly, compositionality in semantic vec-tor spaces has recently received a lot of attention(Mitchell and Lapata, 2010; Socher et al 2010;Zanzotto et al 2010; Yessenalina and Cardie, 2011;Socher et al 2012; Grefenstette et al 2013).
How-ever, progress is held back by the current lack oflarge and labeled compositionality resources and?00This 0film??
?0does 0n?t0+care +0about +++++cleverness 0,0wit0or+00any 00other +kind+0of ++intelligent + +humor0.Figure 1: Example of the Recursive Neural Tensor Net-work accurately predicting 5 sentiment classes, very neg-ative to very positive (?
?, ?, 0, +, + +), at every node of aparse tree and capturing the negation and its scope in thissentence.models to accurately capture the underlying phe-nomena presented in such data.
To address this need,we introduce the Stanford Sentiment Treebank anda powerful Recursive Neural Tensor Network thatcan accurately predict the compositional semanticeffects present in this new corpus.The Stanford Sentiment Treebank is the first cor-pus with fully labeled parse trees that allows for acomplete analysis of the compositional effects ofsentiment in language.
The corpus is based onthe dataset introduced by Pang and Lee (2005) andconsists of 11,855 single sentences extracted frommovie reviews.
It was parsed with the Stanfordparser (Klein and Manning, 2003) and includes atotal of 215,154 unique phrases from those parsetrees, each annotated by 3 human judges.
This newdataset alws us to analyze the intricacies of senti-ment and to capture complex linguistic phenomena.Fig.
1 shows one of the many examples with clearcompositional structure.
The granularity and size of1631this dataset will enable the community to train com-positional models that are based on supervised andstructured machine learning techniques.
While thereare several datasets with document and chunk labelsavailable, there is a need to better capture sentimentfrom short comments, such as Twitter data, whichprovide less overall signal per document.In order to capture the compositional effects withhigher accuracy, we propose a new model called theRecursive Neural Tensor Network (RNTN).
Recur-sive Neural Tensor Networks take as input phrasesof any length.
They represent a phrase through wordvectors and a parse tree and then compute vectors forhigher nodes in the tree using the same tensor-basedcomposition function.
We compare to several super-vised, compositional models such as standard recur-sive neural networks (RNN) (Socher et al 2011b),matrix-vector RNNs (Socher et al 2012), and base-lines such as neural networks that ignore word order,Naive Bayes (NB), bi-gram NB and SVM.
All mod-els get a significant boost when trained with the newdataset but the RNTN obtains the highest perfor-mance with 80.7% accuracy when predicting fine-grained sentiment for all nodes.
Lastly, we use a testset of positive and negative sentences and their re-spective negations to show that, unlike bag of wordsmodels, the RNTN accurately captures the sentimentchange and scope of negation.
RNTNs also learnthat sentiment of phrases following the contrastiveconjunction ?but?
dominates.The complete training and testing code, a livedemo and the Stanford Sentiment Treebank datasetare available at http://nlp.stanford.edu/sentiment.2 Related WorkThis work is connected to five different areas of NLPresearch, each with their own large amount of relatedwork to which we cannot do full justice given spaceconstraints.Semantic Vector Spaces.
The dominant ap-proach in semantic vector spaces uses distributionalsimilarities of single words.
Often, co-occurrencestatistics of a word and its context are used to de-scribe each word (Turney and Pantel, 2010; Baroniand Lenci, 2010), such as tf-idf.
Variants of this ideause more complex frequencies such as how often aword appears in a certain syntactic context (Padoand Lapata, 2007; Erk and Pado?, 2008).
However,distributional vectors often do not properly capturethe differences in antonyms since those often havesimilar contexts.
One possibility to remedy this is touse neural word vectors (Bengio et al 2003).
Thesevectors can be trained in an unsupervised fashionto capture distributional similarities (Collobert andWeston, 2008; Huang et al 2012) but then also befine-tuned and trained to specific tasks such as sen-timent detection (Socher et al 2011b).
The modelsin this paper can use purely supervised word repre-sentations learned entirely on the new corpus.Compositionality in Vector Spaces.
Most ofthe compositionality algorithms and related datasetscapture two word compositions.
Mitchell and La-pata (2010) use e.g.
two-word phrases and analyzesimilarities computed by vector addition, multiplica-tion and others.
Some related models such as holo-graphic reduced representations (Plate, 1995), quan-tum logic (Widdows, 2008), discrete-continuousmodels (Clark and Pulman, 2007) and the recentcompositional matrix space model (Rudolph andGiesbrecht, 2010) have not been experimentally val-idated on larger corpora.
Yessenalina and Cardie(2011) compute matrix representations for longerphrases and define composition as matrix multipli-cation, and also evaluate on sentiment.
Grefen-stette and Sadrzadeh (2011) analyze subject-verb-object triplets and find a matrix-based categoricalmodel to correlate well with human judgments.
Wecompare to the recent line of work on supervisedcompositional models.
In particular we will de-scribe and experimentally compare our new RNTNmodel to recursive neural networks (RNN) (Socheret al 2011b) and matrix-vector RNNs (Socher etal., 2012) both of which have been applied to bag ofwords sentiment corpora.Logical Form.
A related field that tackles com-positionality from a very different angle is that oftrying to map sentences to logical form (Zettlemoyerand Collins, 2005).
While these models are highlyinteresting and work well in closed domains andon discrete sets, they could only capture sentimentdistributions using separate mechanisms beyond thecurrently used logical forms.Deep Learning.
Apart from the above mentioned1632work on RNNs, several compositionality ideas re-lated to neural networks have been discussed by Bot-tou (2011) and Hinton (1990) and first models suchas Recursive Auto-associative memories been exper-imented with by Pollack (1990).
The idea to relateinputs through three way interactions, parameterizedby a tensor have been proposed for relation classifi-cation (Sutskever et al 2009; Jenatton et al 2012),extending Restricted Boltzmann machines (Ranzatoand Hinton, 2010) and as a special layer for speechrecognition (Yu et al 2012).Sentiment Analysis.
Apart from the above-mentioned work, most approaches in sentiment anal-ysis use bag of words representations (Pang and Lee,2008).
Snyder and Barzilay (2007) analyzed largerreviews in more detail by analyzing the sentimentof multiple aspects of restaurants, such as food oratmosphere.
Several works have explored sentimentcompositionality through careful engineering of fea-tures or polarity shifting rules on syntactic structures(Polanyi and Zaenen, 2006; Moilanen and Pulman,2007; Rentoumi et al 2010; Nakagawa et al 2010).3 Stanford Sentiment TreebankBag of words classifiers can work well in longerdocuments by relying on a few words with strongsentiment like ?awesome?
or ?exhilarating.?
How-ever, sentiment accuracies even for binary posi-tive/negative classification for single sentences hasnot exceeded 80% for several years.
For the moredifficult multiclass case including a neutral class,accuracy is often below 60% for short messageson Twitter (Wang et al 2012).
From a linguisticor cognitive standpoint, ignoring word order in thetreatment of a semantic task is not plausible, and, aswe will show, it cannot accurately classify hard ex-amples of negation.
Correctly predicting these hardcases is necessary to further improve performance.In this section we will introduce and provide someanalyses for the new Sentiment Treebank which in-cludes labels for every syntactically plausible phrasein thousands of sentences, allowing us to train andevaluate compositional models.We consider the corpus of movie review excerptsfrom the rottentomatoes.com website orig-inally collected and published by Pang and Lee(2005).
The original dataset includes 10,662 sen-nerdy ?folks|Verynegative|Negative|Somewhatnegative|Neutral|Somewhatpositive|Positive|Verypositivephenomenal ?fantasy ?best ?sellers|Verynegative|Negative|Somewhatnegative|Neutral|Somewhatpositive|Positive|Verypositive?
?Figure 3: The labeling interface.
Random phrases wereshown and annotators had a slider for selecting the senti-ment and its degree.tences, half of which were considered positive andthe other half negative.
Each label is extracted froma longer movie review and reflects the writer?s over-all intention for this review.
The normalized, lower-cased text is first used to recover, from the origi-nal website, the text with capitalization.
RemainingHTML tags and sentences that are not in Englishare deleted.
The Stanford Parser (Klein and Man-ning, 2003) is used to parses all 10,662 sentences.In approximately 1,100 cases it splits the snippetinto multiple sentences.
We then used Amazon Me-chanical Turk to label the resulting 215,154 phrases.Fig.
3 shows the interface annotators saw.
The sliderhas 25 different values and is initially set to neutral.The phrases in each hit are randomly sampled fromthe set of all phrases in order to prevent labels beinginfluenced by what follows.
For more details on thedataset collection, see supplementary material.Fig.
2 shows the normalized label distributions ateach n-gram length.
Starting at length 20, the ma-jority are full sentences.
One of the findings fromlabeling sentences based on reader?s perception isthat many of them could be considered neutral.
Wealso notice that stronger sentiment often builds upin longer phrases and the majority of the shorterphrases are neutral.
Another observation is that mostannotators moved the slider to one of the five po-sitions: negative, somewhat negative, neutral, posi-tive or somewhat positive.
The extreme values wererarely used and the slider was not often left in be-tween the ticks.
Hence, even a 5-class classificationinto these categories captures the main variabilityof the labels.
We will name this fine-grained senti-ment classification and our main experiment will beto recover these five labels for phrases of all lengths.16335 10 15 20 25 30 35 40 45N-Gram Length0%20%40%60%80%100%%of Sentiment ValuesNeutralSomeZhat 3ositiYe3ositiYeVer\ 3ositiYeSomeZhat NegatiYeNegatiYeVer\ NegatiYe(a)(a)(b)(b)(c)(c)(d)(d)Distributions of sentiment values for (a) unigrams,(b) 10-grams, (c) 20-grams, and (d) full sentences.Figure 2: Normalized histogram of sentiment annotations at each n-gram length.
Many shorter n-grams are neutral;longer phrases are well distributed.
Few annotators used slider positions between ticks or the extreme values.
Hencethe two strongest labels and intermediate tick positions are merged into 5 classes.4 Recursive Neural ModelsThe models in this section compute compositionalvector representations for phrases of variable lengthand syntactic type.
These representations will thenbe used as features to classify each phrase.
Fig.
4displays this approach.
When an n-gram is given tothe compositional models, it is parsed into a binarytree and each leaf node, corresponding to a word,is represented as a vector.
Recursive neural mod-els will then compute parent vectors in a bottomup fashion using different types of compositional-ity functions g. The parent vectors are again givenas features to a classifier.
For ease of exposition,we will use the tri-gram in this figure to explain allmodels.We first describe the operations that the below re-cursive neural models have in common: word vectorrepresentations and classification.
This is followedby descriptions of two previous RNN models andour RNTN.Each word is represented as a d-dimensional vec-tor.
We initialize all word vectors by randomlysampling each value from a uniform distribution:U(?r, r), where r = 0.0001.
All the word vec-tors are stacked in the word embedding matrix L ?Rd?|V |, where |V | is the size of the vocabulary.
Ini-tially the word vectors will be random but the L ma-trix is seen as a parameter that is trained jointly withthe compositionality models.We can use the word vectors immediately asparameters to optimize and as feature inputs toa softmax classifier.
For classification into fiveclasses, we compute the posterior probability overnot      very       good ...a          b             cp1 =g(b,c)p2 = g(a,p1)0 0 ++ +-Figure 4: Approach of Recursive Neural Network mod-els for sentiment: Compute parent vectors in a bottom upfashion using a compositionality function g and use nodevectors as features for a classifier at that node.
This func-tion varies for the different models.labels given the word vector via:ya = softmax(Wsa), (1)where Ws ?
R5?d is the sentiment classificationmatrix.
For the given tri-gram, this is repeated forvectors b and c. The main task of and differencebetween the models will be to compute the hiddenvectors pi ?
Rd in a bottom up fashion.4.1 RNN: Recursive Neural NetworkThe simplest member of this family of neural net-work models is the standard recursive neural net-work (Goller and Ku?chler, 1996; Socher et al2011a).
First, it is determined which parent alreadyhas all its children computed.
In the above tree ex-ample, p1 has its two children?s vectors since bothare words.
RNNs use the following equations tocompute the parent vectors:1634p1 = f(W[bc]), p2 = f(W[ap1]),where f = tanh is a standard element-wise nonlin-earity, W ?
Rd?2d is the main parameter to learnand we omit the bias for simplicity.
The bias can beadded as an extra column to W if an additional 1 isadded to the concatenation of the input vectors.
Theparent vectors must be of the same dimensionality tobe recursively compatible and be used as input to thenext composition.
Each parent vector pi, is given tothe same softmax classifier of Eq.
1 to compute itslabel probabilities.This model uses the same compositionality func-tion as the recursive autoencoder (Socher et al2011b) and recursive auto-associate memories (Pol-lack, 1990).
The only difference to the former modelis that we fix the tree structures and ignore the re-construction loss.
In initial experiments, we foundthat with the additional amount of training data, thereconstruction loss at each node is not necessary toobtain high performance.4.2 MV-RNN: Matrix-Vector RNNThe MV-RNN is linguistically motivated in thatmost of the parameters are associated with wordsand each composition function that computes vec-tors for longer phrases depends on the actual wordsbeing combined.
The main idea of the MV-RNN(Socher et al 2012) is to represent every word andlonger phrase in a parse tree as both a vector anda matrix.
When two constituents are combined thematrix of one is multiplied with the vector of theother and vice versa.
Hence, the compositional func-tion is parameterized by the words that participate init.Each word?s matrix is initialized as a d?d identitymatrix, plus a small amount of Gaussian noise.
Sim-ilar to the random word vectors, the parameters ofthese matrices will be trained to minimize the clas-sification error at each node.
For this model, each n-gram is represented as a list of (vector,matrix) pairs,together with the parse tree.
For the tree with (vec-tor,matrix) nodes:(p2,P2)(a,A) (p1,P1)(b,B) (c,C)the MV-RNN computes the first parent vector and itsmatrix via two equations:p1 = f(W[CbBc]), P1 = f(WM[BC]),where WM ?
Rd?2d and the result is again a d ?
dmatrix.
Similarly, the second parent node is com-puted using the previously computed (vector,matrix)pair (p1, P1) as well as (a,A).
The vectors are usedfor classifying each phrase using the same softmaxclassifier as in Eq.
1.4.3 RNTN:Recursive Neural Tensor NetworkOne problem with the MV-RNN is that the numberof parameters becomes very large and depends onthe size of the vocabulary.
It would be cognitivelymore plausible if there was a single powerful com-position function with a fixed number of parameters.The standard RNN is a good candidate for such afunction.
However, in the standard RNN, the inputvectors only implicitly interact through the nonlin-earity (squashing) function.
A more direct, possiblymultiplicative, interaction would allow the model tohave greater interactions between the input vectors.Motivated by these ideas we ask the question: Cana single, more powerful composition function per-form better and compose aggregate meaning fromsmaller constituents more accurately than many in-put specific ones?
In order to answer this question,we propose a new model called the Recursive Neu-ral Tensor Network (RNTN).
The main idea is to usethe same, tensor-based composition function for allnodes.Fig.
5 shows a single tensor layer.
We define theoutput of a tensor product h ?
Rd via the follow-ing vectorized notation and the equivalent but moredetailed notation for each slice V [i] ?
Rd?d:h =[bc]TV [1:d][bc];hi =[bc]TV [i][bc].where V [1:d] ?
R2d?2d?d is the tensor that definesmultiple bilinear forms.1635Slices of       StandardTensor Layer          Layerp = f             V[1:2]        +   WNeural Tensor LayerbcbcbcTp = f                             +Figure 5: A single layer of the Recursive Neural Ten-sor Network.
Each dashed box represents one of d-manyslices and can capture a type of influence a child can haveon its parent.The RNTN uses this definition for computing p1:p1 = f([bc]TV [1:d][bc]+W[bc]),where W is as defined in the previous models.
Thenext parent vector p2 in the tri-gram will be com-puted with the same weights:p2 = f([ap1]TV [1:d][ap1]+W[ap1]).The main advantage over the previous RNNmodel, which is a special case of the RNTN whenV is set to 0, is that the tensor can directly relate in-put vectors.
Intuitively, we can interpret each sliceof the tensor as capturing a specific type of compo-sition.An alternative to RNTNs would be to make thecompositional function more powerful by adding asecond neural network layer.
However, initial exper-iments showed that it is hard to optimize this modeland vector interactions are still more implicit than inthe RNTN.4.4 Tensor Backprop through StructureWe describe in this section how to train the RNTNmodel.
As mentioned above, each node has asoftmax classifier trained on its vector representa-tion to predict a given ground truth or target vectort.
We assume the target distribution vector at eachnode has a 0-1 encoding.
If there are C classes, thenit has length C and a 1 at the correct label.
All otherentries are 0.We want to maximize the probability of the cor-rect prediction, or minimize the cross-entropy errorbetween the predicted distribution yi ?
RC?1 atnode i and the target distribution ti ?
RC?1 at thatnode.
This is equivalent (up to a constant) to mini-mizing the KL-divergence between the two distribu-tions.
The error as a function of the RNTN parame-ters ?
= (V,W,Ws, L) for a sentence is:E(?)
=?i?jtij log yij + ???
?2 (2)The derivative for the weights of the softmax clas-sifier are standard and simply sum up from eachnode?s error.
We define xi to be the vector at nodei (in the example trigram, the xi ?
Rd?1?s are(a, b, c, p1, p2)).
We skip the standard derivative forWs.
Each node backpropagates its error through tothe recursively used weights V,W .
Let ?i,s ?
Rd?1be the softmax error vector at node i:?i,s =(W Ts (yi ?
ti))?
f ?
(xi),where ?
is the Hadamard product between the twovectors and f ?
is the element-wise derivative of fwhich in the standard case of using f = tanh canbe computed using only f(xi).The remaining derivatives can only be computedin a top-down fashion from the top node through thetree and into the leaf nodes.
The full derivative forV and W is the sum of the derivatives at each ofthe nodes.
We define the complete incoming errormessages for a node i as ?i,com.
The top node, inour case p2, only received errors from the top node?ssoftmax.
Hence, ?p2,com = ?p2,s which we canuse to obtain the standard backprop derivative forW (Goller and Ku?chler, 1996; Socher et al 2010).For the derivative of each slice k = 1, .
.
.
, d, we get:?Ep2?V [k]= ?p2,comk[ap1] [ap1]T,where ?p2,comk is just the k?th element of this vector.Now, we can compute the error message for the two1636children of p2:?p2,down =(W T ?p2,com + S)?
f ?
([ap1]),where we defineS =d?k=1?p2,comk(V [k] +(V [k])T)[ap1]The children of p2, will then each take half of thisvector and add their own softmax error message forthe complete ?.
In particular, we have?p1,com = ?p1,s + ?p2,down[d+ 1 : 2d],where ?p2,down[d + 1 : 2d] indicates that p1 is theright child of p2 and hence takes the 2nd half of theerror, for the final word vector derivative for a, itwill be ?p2,down[1 : d].The full derivative for slice V [k] for this trigramtree then is the sum at each node:?E?V [k]=Ep2?V [k]+ ?p1,comk[bc] [bc]T,and similarly for W .
For this nonconvex optimiza-tion we use AdaGrad (Duchi et al 2011) which con-verges in less than 3 hours to a local optimum.5 ExperimentsWe include two types of analyses.
The first type in-cludes several large quantitative evaluations on thetest set.
The second type focuses on two linguisticphenomena that are important in sentiment.For all models, we use the dev set and cross-validate over regularization of the weights, wordvector size as well as learning rate and minibatchsize for AdaGrad.
Optimal performance for all mod-els was achieved at word vector sizes between 25and 35 dimensions and batch sizes between 20 and30.
Performance decreased at larger or smaller vec-tor and batch sizes.
This indicates that the RNTNdoes not outperform the standard RNN due to sim-ply having more parameters.
The MV-RNN has or-ders of magnitudes more parameters than any othermodel due to the word matrices.
The RNTN wouldusually achieve its best performance on the dev setafter training for 3 - 5 hours.
Initial experimentsModelFine-grained Positive/NegativeAll Root All RootNB 67.2 41.0 82.6 81.8SVM 64.3 40.7 84.6 79.4BiNB 71.0 41.9 82.7 83.1VecAvg 73.3 32.7 85.1 80.1RNN 79.0 43.2 86.1 82.4MV-RNN 78.7 44.4 86.8 82.9RNTN 80.7 45.7 87.6 85.4Table 1: Accuracy for fine grained (5-class) and binarypredictions at the sentence level (root) and for all nodes.showed that the recursive models worked signifi-cantly worse (over 5% drop in accuracy) when nononlinearity was used.
We use f = tanh in all ex-periments.We compare to commonly used methods that usebag of words features with Naive Bayes and SVMs,as well as Naive Bayes with bag of bigram features.We abbreviate these with NB, SVM and biNB.
Wealso compare to a model that averages neural wordvectors and ignores word order (VecAvg).The sentences in the treebank were split into atrain (8544), dev (1101) and test splits (2210) andthese splits are made available with the data release.We also analyze performance on only positive andnegative sentences, ignoring the neutral class.
Thisfilters about 20% of the data with the three sets hav-ing 6920/872/1821 sentences.5.1 Fine-grained Sentiment For All PhrasesThe main novel experiment and evaluation metricanalyze the accuracy of fine-grained sentiment clas-sification for all phrases.
Fig.
2 showed that a finegrained classification into 5 classes is a reasonableapproximation to capture most of the data variation.Fig.
6 shows the result on this new corpus.
TheRNTN gets the highest performance, followed bythe MV-RNN and RNN.
The recursive models workvery well on shorter phrases, where negation andcomposition are important, while bag of featuresbaselines perform well only with longer sentences.The RNTN accuracy upper bounds other models atmost n-gram lengths.Table 1 (left) shows the overall accuracy numbersfor fine grained prediction at all phrase lengths andfull sentences.1637    1*UDP/HQJWK$FFXUDF\    1*UDP/HQJWK&XPXODWLYH$FFXUDF\0RGHO517109511511EL1%1%Figure 6: Accuracy curves for fine grained sentiment classification at each n-gram lengths.
Left: Accuracy separatelyfor each set of n-grams.
Right: Cumulative accuracy of all ?
n-grams.5.2 Full Sentence Binary SentimentThis setup is comparable to previous work on theoriginal rotten tomatoes dataset which only usedfull sentence labels and binary classification of pos-itive/negative.
Hence, these experiments show theimprovement even baseline methods can achievewith the sentiment treebank.
Table 1 shows resultsof this binary classification for both all phrases andfor only full sentences.
The previous state of theart was below 80% (Socher et al 2012).
With thecoarse bag of words annotation for training, many ofthe more complex phenomena could not be captured,even by more powerful models.
The combination ofthe new sentiment treebank and the RNTN pushesthe state of the art on short phrases up to 85.4%.5.3 Model Analysis: Contrastive ConjunctionIn this section, we use a subset of the test set whichincludes only sentences with an ?X but Y ?
structure:A phraseX being followed by but which is followedby a phrase Y .
The conjunction is interpreted asan argument for the second conjunct, with the firstfunctioning concessively (Lakoff, 1971; Blakemore,1989; Merin, 1999).
Fig.
7 contains an example.
Weanalyze a strict setting, where X and Y are phrasesof different sentiment (including neutral).
The ex-ample is counted as correct, if the classifications forboth phrases X and Y are correct.
Furthermore,the lowest node that dominates both of the wordbut and the node that spans Y also have to have thesame correct sentiment.
For the resulting 131 cases,the RNTN obtains an accuracy of 41% compared toMV-RNN (37), RNN (36) and biNB (27).5.4 Model Analysis: High Level NegationWe investigate two types of negation.
For each type,we use a separate dataset for evaluation.++??
?0There ?0are ?
?0?slow 0and?repetitive0parts0, 0but+0it +00has 00just 0enough++spice +0to +0keep +0it +interesting0.Figure 7: Example of correct prediction for contrastiveconjunction X but Y .Set 1: Negating Positive Sentences.
The first setcontains positive sentences and their negation.
Inthis set, the negation changes the overall sentimentof a sentence from positive to negative.
Hence, wecompute accuracy in terms of correct sentiment re-versal from positive to negative.
Fig.
9 shows twoexamples of positive negation the RNTN correctlyclassified, even if negation is less obvious in the caseof ?least?.
Table 2 (left) gives the accuracies over 21positive sentences and their negation for all models.The RNTN has the highest reversal accuracy, show-ing its ability to structurally learn negation of posi-tive sentences.
But what if the model simply makesphrases very negative when negation is in the sen-tence?
The next experiments show that the modelcaptures more than such a simplistic negation rule.Set 2: Negating Negative Sentences.
The sec-ond set contains negative sentences and their nega-tion.
When negative sentences are negated, the sen-timent treebank shows that overall sentiment shouldbecome less negative, but not necessarily positive.For instance, ?The movie was terrible?
is negativebut the ?The movie was not terrible?
says only that itwas less bad than a terrible one, not that it was good(Horn, 1989; Israel, 2001).
Hence, we evaluate ac-1638+ +00Roger 0Dodger++0is +0one +0of ++0the ++0most +compelling0variations00on 00this 0theme0.
?00Roger 0Dodger?
?0is ?0one ?0of ?
?0the ??
?least +compelling0variations00on 00this 0theme0.+0I +++liked 000every 00single 0minute00of 00this 0film0.
?0I ?
?00did 0n?t00like 000a 00single 0minute00of 00this 0film0.
?0It ?
?00?s 0just ?+incredibly ?
?dull0.00It 00000?s +definitely?not?
?dull 0.Figure 9: RNTN prediction of positive and negative (bottom right) sentences and their negation.ModelAccuracyNegated Positive Negated NegativebiNB 19.0 27.3RNN 33.3 45.5MV-RNN 52.4 54.6RNTN 71.4 81.8Table 2: Accuracy of negation detection.
Negated posi-tive is measured as correct sentiment inversions.
Negatednegative is measured as increases in positive activations.curacy in terms of how often each model was ableto increase non-negative activation in the sentimentof the sentence.
Table 2 (right) shows the accuracy.In over 81% of cases, the RNTN correctly increasesthe positive activations.
Fig.
9 (bottom right) showsa typical case in which sentiment was made morepositive by switching the main class from negativeto neutral even though both not and dull were nega-tive.
Fig.
8 shows the changes in activation for bothsets.
Negative values indicate a decrease in aver-     EL1%551095115171 1HJDWHG3RVLWLYH6HQWHQFHV&KDQJHLQ$FWLYDWLRQ     EL1%551095115171 1HJDWHG1HJDWLYH6HQWHQFHV&KDQJHLQ$FWLYDWLRQFigure 8: Change in activations for negations.
Only theRNTN correctly captures both types.
It decreases positivesentiment more when it is negated and learns that negat-ing negative phrases (such as not terrible) should increaseneutral and positive activations.age positive activation (for set 1) and positive valuesmean an increase in average positive activation (set2).
The RNTN has the largest shifts in the correct di-rections.
Therefore we can conclude that the RNTNis best able to identify the effect of negations uponboth positive and negative sentiment sentences.1639n Most positive n-grams Most negative n-grams1 engaging; best; powerful; love; beautiful bad; dull; boring; fails; worst; stupid; painfully2 excellent performances; A masterpiece; masterfulfilm; wonderful movie; marvelous performancesworst movie; very bad; shapeless mess; worstthing; instantly forgettable; complete failure3 an amazing performance; wonderful all-ages tri-umph; a wonderful movie; most visually stunningfor worst movie; A lousy movie; a complete fail-ure; most painfully marginal; very bad sign5 nicely acted and beautifully shot; gorgeous im-agery, effective performances; the best of theyear; a terrific American sports movie; refresh-ingly honest and ultimately touchingsilliest and most incoherent movie; completelycrass and forgettable movie; just another badmovie.
A cumbersome and cliche-ridden movie;a humorless, disjointed mess8 one of the best films of the year; A love for filmsshines through each frame; created a masterfulpiece of artistry right here; A masterful film froma master filmmaker,A trashy, exploitative, thoroughly unpleasant ex-perience ; this sloppy drama is an empty ves-sel.
; quickly drags on becoming boring and pre-dictable.
; be the worst special-effects creation ofthe yearTable 3: Examples of n-grams for which the RNTN predicted the most positive and most negative responses.         1*UDP/HQJWK$YHUDJH*URXQG7UXWK6HQWLPHQW0RGHO517109511511Figure 10: Average ground truth sentiment of top 10 mostpositive n-grams at various n. The RNTN correctly picksthe more negative and positive examples.5.5 Model Analysis: Most Positive andNegative PhrasesWe queried the model for its predictions on whatthe most positive or negative n-grams are, measuredas the highest activation of the most negative andmost positive classes.
Table 3 shows some phrasesfrom the dev set which the RNTN selected for theirstrongest sentiment.Due to lack of space we cannot compare topphrases of the other models but Fig.
10 shows thatthe RNTN selects more strongly positive phrases atmost n-gram lengths compared to other models.For this and the previous experiment, please findadditional examples and descriptions in the supple-mentary material.6 ConclusionWe introduced Recursive Neural Tensor Networksand the Stanford Sentiment Treebank.
The combi-nation of new model and data results in a systemfor single sentence sentiment detection that pushesstate of the art by 5.4% for positive/negative sen-tence classification.
Apart from this standard set-ting, the dataset al poses important new challengesand allows for new evaluation metrics.
For instance,the RNTN obtains 80.7% accuracy on fine-grainedsentiment prediction across all phrases and capturesnegation of different sentiments and scope more ac-curately than previous models.AcknowledgmentsWe thank Rukmani Ravisundaram and TayyabTariq for the first version of the online demo.Richard is partly supported by a Microsoft Re-search PhD fellowship.
The authors gratefully ac-knowledge the support of the Defense Advanced Re-search Projects Agency (DARPA) Deep Explorationand Filtering of Text (DEFT) Program under AirForce Research Laboratory (AFRL) prime contractno.
FA8750-13-2-0040, the DARPA Deep Learningprogram under contract number FA8650-10-C-7020and NSF IIS-1159679.
Any opinions, findings, andconclusions or recommendations expressed in thismaterial are those of the authors and do not neces-sarily reflect the view of DARPA, AFRL, or the USgovernment.1640ReferencesM.
Baroni and A. Lenci.
2010.
Distributional mem-ory: A general framework for corpus-based semantics.Computational Linguistics, 36(4):673?721.Y.
Bengio, R. Ducharme, P. Vincent, and C. Janvin.2003.
A neural probabilistic language model.
J.Mach.
Learn.
Res., 3, March.D.
Blakemore.
1989.
Denial and contrast: A relevancetheoretic analysis of ?but?.
Linguistics and Philoso-phy, 12:15?37.L.
Bottou.
2011.
From machine learning to machinereasoning.
CoRR, abs/1102.1808.S.
Clark and S. Pulman.
2007.
Combining symbolic anddistributional models of meaning.
In Proceedings ofthe AAAI Spring Symposium on Quantum Interaction,pages 52?55.R.
Collobert and J. Weston.
2008.
A unified architecturefor natural language processing: deep neural networkswith multitask learning.
In ICML.J.
Duchi, E. Hazan, and Y.
Singer.
2011.
Adaptive sub-gradient methods for online learning and stochastic op-timization.
JMLR, 12, July.K.
Erk and S. Pado?.
2008.
A structured vector spacemodel for word meaning in context.
In EMNLP.C.
Goller and A. Ku?chler.
1996.
Learning task-dependent distributed representations by backpropaga-tion through structure.
In Proceedings of the Interna-tional Conference on Neural Networks (ICNN-96).E.
Grefenstette and M. Sadrzadeh.
2011.
Experimentalsupport for a categorical compositional distributionalmodel of meaning.
In EMNLP.E.
Grefenstette, G. Dinu, Y.-Z.
Zhang, M. Sadrzadeh, andM.
Baroni.
2013.
Multi-step regression learning forcompositional distributional semantics.
In IWCS.G.
E. Hinton.
1990.
Mapping part-whole hierarchies intoconnectionist networks.
Artificial Intelligence, 46(1-2).L.
R. Horn.
1989.
A natural history of negation, volume960.
University of Chicago Press Chicago.E.
H. Huang, R. Socher, C. D. Manning, and A. Y. Ng.2012.
Improving Word Representations via GlobalContext and Multiple Word Prototypes.
In ACL.M.
Israel.
2001.
Minimizers, maximizers, and therhetoric of scalar reasoning.
Journal of Semantics,18(4):297?331.R.
Jenatton, N. Le Roux, A. Bordes, and G. Obozinski.2012.
A latent factor model for highly multi-relationaldata.
In NIPS.D.
Klein and C. D. Manning.
2003.
Accurate unlexical-ized parsing.
In ACL.R.
Lakoff.
1971.
If?s, and?s, and but?s about conjunction.In Charles J. Fillmore and D. Terence Langendoen, ed-itors, Studies in Linguistic Semantics, pages 114?149.Holt, Rinehart, and Winston, New York.A.
Merin.
1999.
Information, relevance, and social deci-sionmaking: Some principles and results of decision-theoretic semantics.
In Lawrence S. Moss, JonathanGinzburg, and Maarten de Rijke, editors, Logic, Lan-guage, and Information, volume 2.
CSLI, Stanford,CA.J.
Mitchell and M. Lapata.
2010.
Composition in dis-tributional models of semantics.
Cognitive Science,34(8):1388?1429.K.
Moilanen and S. Pulman.
2007.
Sentiment composi-tion.
In In Proceedings of Recent Advances in NaturalLanguage Processing.T.
Nakagawa, K. Inui, and S. Kurohashi.
2010.
Depen-dency tree-based sentiment classification using CRFswith hidden variables.
In NAACL, HLT.S.
Pado and M. Lapata.
2007.
Dependency-based con-struction of semantic space models.
ComputationalLinguistics, 33(2):161?199.B.
Pang and L. Lee.
2005.
Seeing stars: Exploiting classrelationships for sentiment categorization with respectto rating scales.
In ACL, pages 115?124.B.
Pang and L. Lee.
2008.
Opinion mining and senti-ment analysis.
Foundations and Trends in InformationRetrieval, 2(1-2):1?135.T.
A.
Plate.
1995.
Holographic reduced representations.IEEE Transactions on Neural Networks, 6(3):623?641.L.
Polanyi and A. Zaenen.
2006.
Contextual valenceshifters.
In W. Bruce Croft, James Shanahan, Yan Qu,and Janyce Wiebe, editors, Computing Attitude and Af-fect in Text: Theory and Applications, volume 20 ofThe Information Retrieval Series, chapter 1.J.
B. Pollack.
1990.
Recursive distributed representa-tions.
Artificial Intelligence, 46, November.M.
Ranzato and A. Krizhevsky G. E. Hinton.
2010.Factored 3-Way Restricted Boltzmann Machines ForModeling Natural Images.
AISTATS.V.
Rentoumi, S. Petrakis, M. Klenner, G. A. Vouros, andV.
Karkaletsis.
2010.
United we stand: Improvingsentiment analysis by joining machine learning andrule based methods.
In Proceedings of the Seventhconference on International Language Resources andEvaluation (LREC?10), Valletta, Malta.S.
Rudolph and E. Giesbrecht.
2010.
Compositionalmatrix-space models of language.
In ACL.B.
Snyder and R. Barzilay.
2007.
Multiple aspect rank-ing using the Good Grief algorithm.
In HLT-NAACL.R.
Socher, C. D. Manning, and A. Y. Ng.
2010.
Learningcontinuous phrase representations and syntactic pars-ing with recursive neural networks.
In Proceedings ofthe NIPS-2010 Deep Learning and Unsupervised Fea-ture Learning Workshop.1641R.
Socher, C. Lin, A. Y. Ng, and C.D.
Manning.
2011a.Parsing Natural Scenes and Natural Language withRecursive Neural Networks.
In ICML.R.
Socher, J. Pennington, E. H. Huang, A. Y. Ng, andC.
D. Manning.
2011b.
Semi-Supervised RecursiveAutoencoders for Predicting Sentiment Distributions.In EMNLP.R.
Socher, B. Huval, C. D. Manning, and A. Y. Ng.
2012.Semantic compositionality through recursive matrix-vector spaces.
In EMNLP.I.
Sutskever, R. Salakhutdinov, and J.
B. Tenenbaum.2009.
Modelling relational data using Bayesian clus-tered tensor factorization.
In NIPS.P.
D. Turney and P. Pantel.
2010.
From frequency tomeaning: Vector space models of semantics.
Journalof Artificial Intelligence Research, 37:141?188.H.
Wang, D. Can, A. Kazemzadeh, F. Bar, andS.
Narayanan.
2012.
A system for real-time twit-ter sentiment analysis of 2012 u.s. presidential elec-tion cycle.
In Proceedings of the ACL 2012 SystemDemonstrations.D.
Widdows.
2008.
Semantic vector products: Some ini-tial investigations.
In Proceedings of the Second AAAISymposium on Quantum Interaction.A.
Yessenalina and C. Cardie.
2011.
Composi-tional matrix-space models for sentiment analysis.
InEMNLP.D.
Yu, L. Deng, and F. Seide.
2012.
Large vocabularyspeech recognition using deep tensor neural networks.In INTERSPEECH.F.M.
Zanzotto, I. Korkontzelos, F. Fallucchi, and S. Man-andhar.
2010.
Estimating linear models for composi-tional distributional semantics.
In COLING.L.
Zettlemoyer and M. Collins.
2005.
Learning tomap sentences to logical form: Structured classifica-tion with probabilistic categorial grammars.
In UAI.1642
