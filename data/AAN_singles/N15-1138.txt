Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1275?1280,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsUnsupervised Sparse Vector Densification for Short Text SimilarityYangqiu Song and Dan RothDepartment of Computer ScienceUniversity of Illinois at Urbana-ChampaignUrbana, IL 61801, USA{yqsong,danr}@illinois.eduAbstractSparse representations of text such as bag-of-words models or extended explicit semanticanalysis (ESA) representations are commonlyused in many NLP applications.
However, forshort texts, the similarity between two such s-parse vectors is not accurate due to the smallterm overlap.
While there have been multipleproposals for dense representations of words,measuring similarity between short texts (sen-tences, snippets, paragraphs) requires combin-ing these token level similarities.
In this paper,we propose to combine ESA representationsand word2vec representations as a way to gen-erate denser representations and, consequent-ly, a better similarity measure between shorttexts.
We study three densification mecha-nisms that involve aligning sparse representa-tion via many-to-many, many-to-one, and one-to-one mappings.
We then show the effective-ness of these mechanisms on measuring simi-larity between short texts.1 IntroductionBag-of-words model has been used for many ap-plications as the state-of-the-art method for taskssuch as document classifications and information re-trieval.
It represents each text as a bag-of-words,and computes the similarity, e.g., cosine value, be-tween two sparse vectors in the high-dimensionalspace.
When the contextual information is insuffi-cient, e.g., due to the short length of the document,explicit semantic analysis (ESA) has been used asa way to enrich the text representation (Gabrilovichand Markovitch, 2006; Gabrilovich and Markovitch,2007).
Instead of using only the words in a doc-ument, ESA uses a bag-of-concepts retrieved fromWikipedia to represent the text.
Then the similaritybetween two texts can be computed in this enrichedconcept space.Both bag-of-words and bag-of-concepts model-s suffer from the sparsity problem.
Because bothmodels use sparse vectors to represent text, whencomparing two pieces of texts, the similarity can bezero even when the text snippets are highly related,but make use of different vocabulary.
We can expectthat these two texts are related but the similarity val-ue does not reflect that.
ESA, despite augmentingthe lexical space with relevant Wikipedia concepts,still suffers from the sparsity problem.
We illustratethis problem with the following simple experiment,done by choosing a documents from the ?rec.autos?group in the 20-newsgroups data set1.
For both doc-uments and the label description ?cars?
(here we fol-low the description shown in (Chang et al, 2008;Song and Roth, 2014)), we computed 500 conceptsusing ESA.
Then we identified the concepts that ap-pear both in the document ESA representation andin the label ESA representation.
The average sizesof this intersection (number of overlapping conceptsin the document and label representation) are shownin Table 1.
In addition to the original documents, wealso split each document into 2, 4, 8, 16 equal lengthparts, computed the ESA representation of each, andthen the intersection with the ESA representation ofthe label.
Table 1 shows that the number of conceptsshared by the label and the document representationdecreases significantly, even if not as significantly1http://qwone.com/?jason/20Newsgroups/1275Table 1: Average sizes of the intersection between theESA concept representations of documents and label-s.
Both documents and label are represented with 500Wikipedia concepts.
Documents are split into differentlengths.# of split Avg.
# of words per doc.
Avg.
# of concepts1 209.6 23.12 104.8 18.14 52.4 13.88 26.2 10.616 13.1 8.4as the drop in the document size.
For example, thereare on average 8 concepts in the intersection of twovectors with 500 non-zero concepts when we spliteach document into 16 parts.When there are fewer overlapping terms betweentwo pieces of texts, it can cause mismatch or biasedmatch and result in less accurate comparison.
In thispaper, we propose to use unsupervised approachesto improve the representation, along with a corre-sponding similarity approach between these repre-sentations.
Our contribution is twofold.
First, weincorporate the popular word2vec (Mikolov et al,2013a; Mikolov et al, 2013b) representations intoESA representation, and show that incorporating se-mantic relatedness between Wikipedia titles can in-deed help the similarity measure between short texts.Second, we propose and evaluate three mechanism-s for comparing the resulting representations.
Weverify the superiority of the proposed methods usingthree different NLP tasks.2 Sparse Vector DensificationIn this section, we introduce a way to computethe similarity between two sparse vectors by aug-menting the original similarity measure, i.e., co-sine similarity.
Suppose we have two vectors x =(x1, .
.
.
, xV)Tand y = (y1, .
.
.
, yV)Twhere V isthe vocabulary size.
Traditional cosine similaritycomputes the dot product between these two vec-tors and normalizes it by their norms: cos(x,y) =xTy||x||?||y||.
This requires each dimension of x to bealigned with the same dimension of y.
Note thatfor sparse vectors x and y, most of the the elementscan be zero.
Aligning the indices can result in zerosimilarity even though the two pieces of texts are re-lated.
Thus, we propose to align different indices ofx and y together to increase the similarity value.We can rewrite the vectors x and y as x ={xa1, .
.
.
, xanx} and y = {yb1, .
.
.
, ybny}, where aiand bjare indices of the non-zero terms in x and y(1 ?
ai, bj?
V ).
xaiand ybiare the weights asso-ciated to the terms in the vocabulary.
Suppose thereare non-zero terms nxand nyin x and y respective-ly.
Then cosine similarity can be rewritten as:cos(x,y) =?nxi=1?nyj=1?(ai?
bj)xaiybj||x|| ?
||y||, (1)where ?(?)
is the Dirac function ?
(0) = 1 and?
(other) = 0.
Suppose we can compute the simi-larity between terms aiand bj, which is denoted as?
(ai, bj), then the problem is how to aggregate thesimilarities between all ai?s and bj?s to augment theoriginal cosine similarity.2.1 Similarity AugmentationThe most intuitive way to integrate the similaritiesbetween terms is averaging them:SA(x,y) =1nx||x|| ?
ny||y||nx?i=1ny?j=1xaiybj?
(ai, bj).
(2)This similarity averages all the pairwise similaritiesbetween terms ai?s and bj?s.
However, we can ex-pect a lot of the similarities ?
(ai, bj) to be close tozero.
In this case, instead of introducing the relat-edness between nonidentical terms, it will also in-troduce noise.
Therefore, we also consider an align-ment mechanism that we implement greedily via amaximum matching mechanism:SM(x,y) =1||x|| ?
||y||nx?i=1xaiybjmaxj?
(ai, bj).
(3)We choose j as argmaxj??
(ai, bj?)
and substitutethe similarity ?
(ai, bj) between terms aiand bjin-to the final similarity between x and y.
Note thatthis similarity is not symmetric.
Thus, if one needsa symmetric similarity, the similarity can be com-puted by averaging two similarities SM(x,y) andSM(y,x).The above two similarity measurements are sim-ple and intuitive.
We can think about SA(x,y)as leveraging term many-to-many mapping, while1276(a) rec.autos vs. sci.electronics (full doc.)
(b) rec.autos vs. sci.electronics (1/16 doc.)
(c) rec.autos vs. rec.motorcycles (full doc.)
(d) rec.autos vs. rec.motorcycles (1/16 doc.
)Figure 1: Accuracy of dataless classification using ESA and Dense-ESA with different numbers of concepts.SM(x,y) uses only one-to-many term mapping.SA(x,y) can introduce small and noisy similarityvalues between terms.
While SM(x,y) essentiallyaligns each term in x with it?s best match in y, werun the risk that multiple components of x will se-lect the same element in y.
To ensure that all thenon-zero terms in x and y are matched, we proposeto constrain this metric by disallowing many-to-onemapping.
We do that by using a similarity metricbased on the Hungarian method (Papadimitriou andSteiglitz, 1982).
The Hungarian method is a combi-natorial optimization algorithm that solves the bipar-tite graph matching problem by finding an optimalassignment matching the two sides of the graph on aone-to-one basis.
Assume that we run the Hungari-an method on the the pair {x,y}, and let h(ai) = bjdenote the outcome of the algorithm, that is aiisaligned with bj.
(We assume here, for simplicity,that nx= ny; we can always achieve that by addingsome zero weighted terms that are not aligned).
Thewe define the similarity as:SH(x,y) =1||x|| ?
||y||nx?i=1xaiyh(ai)?
(ai, h(ai)).
(4)2.2 Term Similarity MeasureTo evaluate the term similarity ?
(?, ?
), we use lo-cal contextual similarity based on distributed rep-resentations.
We adopt the word2vec (Mikolov etal., 2013a; Mikolov et al, 2013b) approach to ob-tain a dense representation of words.
The represen-tation of each word is predicted based on the contextword distribution in a window around it.
We trainedword2vec on the Wikipedia dump data using the de-fault parameters (CBOW model with window sizeas five).
For each word, we finally obtained a 200dimensional vector.
If the term is a phrase, we sim-ply average words?
vectors of each phrase to obtainthe representation following the original word2vecapproach (Mikolov et al, 2013a; Mikolov et al,2013b).
We use two vectors a and b to represent thevectors for the two terms.
To evaluate the similaritybetween two terms, for the average approach as E-q.
(2), we use the RBF kernel over the two vectorsexp{?||a?
b||2/(0.03 ?
||a|| ?
||b||)} as the similari-ty for all the experiments, since this will have a goodproperty to cut the terms with small similarities.
Forthe max and Hungarian approach as Eqs.
(3) and (4),we simply use the cosine similarity between the twoword2vec vectors.
In addition, we cut off all simi-larities below threshold ?
and map them to zero.3 ExperimentsWe experiment on three data sets.
We use datalessclassification (Chang et al, 2008; Song and Roth,2014) over 20-newsgroups data set to verify the cor-rectness of our argument of short text problems, anduse two short text data sets to evaluate documentsimilarity measurement and event classification forsentences.3.1 Dataless ClassificationDataless classification uses the similarity betweendocuments and labels in an enriched ?semantic?
s-pace to determine in which category the given doc-ument is.
In this experiment, we used the label de-scriptions provided by (Chang et al, 2008).
It hasbeen shown that ESA outperforms other representa-tions for dataless classification (Chang et al, 2008;Song and Roth, 2014).
Thus, we chose ESA as our1277Table 2: Accuracy of dataless classification using ESA and Dense-ESA with 500 dimensions.rec.autos vs. sci.electronics (easy) rec.autos vs. rec.motorcycles (difficult)Method Full document Short (1/16 doc.)
Full document Short (1/16 doc.
)ESA (Cosine) 87.75% 56.55% 80.95% 46.64%Dense-ESA (Average) 87.80% 64.67% 81.11% 59.38%Dense-ESA (Max) 87.10% 64.34% 84.30% 59.11%Dense-ESA (Hungarian) 88.85% 65.95% 82.15% 59.65%Figure 2: Boxplot of similarity scores for ?rec.autos vs. sci.electronics?
(easy, left) and ?rec.autos vs.rec.motorcycles?
(difficult, right).
For each method of ESA and Dense-ESA with max matching in Eq.
(3), we com-pute S(d, l1) and S(d, l2) between a document d and the labels l1and l2.
Then we compute S(d) = S(d, l1)?S(d, l2).For each ground truth label, we draw the distribution of S(?)
with outliers in the figures.
For example, ?ESA:autos?shows the S(?
)?s distribution of the data with label ?rec.autos.?
The t-test results show that the distributions of differentlabels are significantly different (99%).
We can see that Dense-ESA pulls apart the distributions of different labels andthat the separation is more significant for the more difficult problem (right).baseline method.
To demonstrate how the length ofdocuments affects the classification result, we usedboth full documents and the 16 split parts (the part-s are associated with the same label as the origi-nal document).
To demonstrate the impact of den-sification, we selected two problems as an illustra-tion: ?rec.autos vs. sci.electronics?
and ?rec.autosvs.
rec.motorcycles.?
While the former problem isrelatively easy since they belong to different super-classes, the latter problem is more difficult sincethey are under the same super-class.
The value ofthreshold ?
for max matching and Hungarian baseddensification is set to 0.85 empirically.Figure 1 shows the results of the dataless clas-sification using ESA and ESA with densification(Dense-ESA) with different numbers of Wikipediaconcepts as the representation dimensionality.
Wecan see that Dense-ESA significantly improves thedataless classification results.
As shown in Table 2,while the max matching and Hungarian matchingbased methods are typically the best metrics themost significant results, the improvements are moresignificant for shorter documents, and for more diffi-cult problems.
Figure 2 highlights this observation.Table 3: Spearman?s correlation of document similarityusing ESA and Dense-ESA with 500 concepts.Method Spearman?s correlationESA (Cosine) 0.5665Dense-ESA (Average) 0.5814Dense-ESA (Max) 0.5888Dense-ESA (Hungarian) 0.60033.2 Document SimilarityWe used the data set provided by Lee et al2(Lee etal., 2005) to evaluate pairwise short document simi-larity.
There are 50 documents and the average num-ber of words is 80.2.
We averaged all the humanannotations for the same document pair as the sim-ilarity score.
After computing the scores for pairsof documents, we used Spearman?s correlation to e-valuate the results.
Larger correlation score mean-s that the similarity is more consistent with humanannotation.
The best word level based similarity re-sult is close to 0.5 (Lee et al, 2005).
We tried thecosine similarity between ESA representations and2http://faculty.sites.uci.edu/mdlee/similarity-data/1278Table 4: F1of sentence event type classification usingESA and Dense-ESA with 500 concepts.Method F1(mean?std)ESA (Cosine) 0.469?0.011Dense-ESA (Average) 0.451?0.010Dense-ESA (Max) 0.481?0.008Dense-ESA (Hungarian) 0.475?0.016also Dense-ESA.
The value of ?
for max matchingbased densification is set to 0.95, and for Hungari-an based densification it is set to 0.89.
We can seethat from Table 3, ESA is better than the word basedmethod, and that all versions of Dense-ESA outper-form the original ESA.3.3 Event ClassificationIn this experiment, we chose the ACE20053data setto test how well we can classify sentences into even-t types without any training.
There are eight type-s of events: life, movement, conflict, contact, etc.We chose all the sentences that contain event infor-mation as the data set.
Following the dataless clas-sification protocol, we compare the similarity be-tween sentences and label descriptions to determinethe event types.
There are 3,644 unique sentenceswith events, including 2,712 sentences having on-ly one event type, 421 having two event types, and30 having three event types.
The average length ofthe sentences is 23.71.
Thus, this is a multi-labelclassification problem.
To test the approaches, weused five-fold cross validation to select the thresh-olds for each class to classify whether the sentencebelongs to an event type.
The value of threshold ?for both max matching and Hungarian based densifi-cation is also set to 0.85 empirically.
Then we reportthe mean and standard derivation over five runs.
Theresults are shown in Table 4.
We can see that Dense-ESA also outperforms ESA.4 Related WorkESA (Gabrilovich and Markovitch, 2006;Gabrilovich and Markovitch, 2007) and dis-tributed word representations (Ratinov and Roth,2009; Turian et al, 2010; Collobert et al, 2011;Mikolov et al, 2013a; Mikolov et al, 2013b; Pen-nington et al, 2014) are popular text representations3http://www.itl.nist.gov/iad/mig/tests/ace/2005/that encode world knowledge.
Recently, severalrepresentations were proposed to extend wordrepresentations for phrases or sentences (Lu and Li,2013; Hermann and Blunsom, 2014; Passos et al,2014; Kalchbrenner et al, 2014; Le and Mikolov,2014; Hu et al, 2014; Sutskever et al, 2014; Zhaoet al, 2015).
In this paper, we evaluate how tocombine two off-the-shelf representations to densifythe similarity between text data.Yih et al also used average matching and a dif-ferent maximum matching for QA problem (Yih etal., 2013).
However, their sparse representation isstill at the word level while ours is based on ESA.Interestingly, related ideas to our average matchingmechanism have been proposed also in the comput-er vision community, which is the set kernel (or setsimilarity) (Smola et al, 2007; Gretton et al, 2012;Xiong et al, 2013).5 ConclusionIn this paper, we study the mechanisms of com-bining two popular representations of text, i.e., E-SA and word2vec, to enhance computing short textsimilarity.
Furthermore, we proposed three differ-ent mechanisms to compute the similarity betweenthese representations, and demonstrated, using threedifferent data sets that the proposed method outper-forms the traditional ESA.AcknowledgmentsThis work is supported by the Multimodal Informa-tion Access & Synthesis Center at UIUC, part of C-CICADA, a DHS Science and Technology Centerof Excellence, by the Army Research Laboratory(ARL) under agreement W911NF-09-2-0053, andby DARPA under agreement number FA8750-13-2-0008.
The views and conclusions contained hereinare those of the authors and should not be interpret-ed as necessarily representing the official policies orendorsements, either expressed or implied by theseagencies or the U.S. Government.1279ReferencesM.
Chang, L. Ratinov, D. Roth, and V. Srikumar.
2008.Importance of semantic representation: Dataless clas-sification.
In AAAI, pages 830?835.R.
Collobert, J. Weston, L. Bottou, M. Karlen,K.
Kavukcuoglu, and P. P. Kuksa.
2011.
Naturallanguage processing (almost) from scratch.
J. Mach.Learn.
Res., 12:2493?2537.E.
Gabrilovich and S. Markovitch.
2006.
Overcomingthe brittleness bottleneck using Wikipedia: Enhancingtext categorization with encyclopedic knowledge.
InAAAI, pages 1301?1306.E.
Gabrilovich and S. Markovitch.
2007.
Computing se-mantic relatedness using Wikipedia-based explicit se-mantic analysis.
In IJCAI, pages 1606?1611.A.
Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch?olkopf,and A. Smola.
2012.
A kernel two-sample test.
J.Mach.
Learn.
Res., 13:723?773.K.
M. Hermann and P. Blunsom.
2014.
Multilingualmodels for compositional distributed semantics.
In A-CL, pages 58?68.B.
Hu, Z. Lu, H. Li, and Q. Chen.
2014.
Convolutionalneural network architectures for matching natural lan-guage sentences.
In NIPS, pages 2042?2050.N.
Kalchbrenner, E. Grefenstette, and P. Blunsom.
2014.A convolutional neural network for modelling sen-tences.
In ACL, pages 655?665.Q.
V. Le and T. Mikolov.
2014.
Distributed represen-tations of sentences and documents.
In ICML, pages1188?1196.M.
D. Lee, B. Pincombe, and M. Welsh.
2005.
An empir-ical evaluation of models of text document similarity.In CogSci, pages 1254?1259.Z.
Lu and H. Li.
2013.
A deep architecture for matchingshort texts.
In NIPS, pages 1367?1375.T.
Mikolov, I. Sutskever, K. Chen, G. S. Corrado, andJ.
Dean.
2013a.
Distributed representations of wordsand phrases and their compositionality.
In NIPS, pages3111?3119.T.
Mikolov, W.-t. Yih, and G. Zweig.
2013b.
Linguisticregularities in continuous space word representations.In HLT-NAACL, pages 746?751.C.
H. Papadimitriou and K. Steiglitz.
1982.
Combinato-rial Optimization: Algorithm und Complexity.
Engle-wood Cliffs, NJ: Prentice-Hall.A.
Passos, V. Kumar, and A. McCallum.
2014.
Lexiconinfused phrase embeddings for named entity resolu-tion.
In CoNLL, pages 78?86.J.
Pennington, R. Socher, and C. D. Manning.
2014.Glove: Global vectors for word representation.
InEMNLP, pages 1532?1543.L.
Ratinov and D. Roth.
2009.
Design challenges andmisconceptions in named entity recognition.
In CoN-LL, pages 147?155.A.
J. Smola, A. Gretton, L. Song, and B. Sch?olkopf.2007.
A hilbert space embedding for distributions.
InALT, pages 13?31.Y.
Song and D. Roth.
2014.
On dataless hierarchical textclassification.
In AAAI, pages 1579?1585.I.
Sutskever, O. Vinyals, and Q. V. Le.
2014.
Sequenceto sequence learning with neural networks.
In NIPS,pages 3104?3112.J.
Turian, L. Ratinov, and Y. Bengio.
2010.
Word rep-resentations: A simple and general method for semi-supervised learning.
In ACL, pages 384?394.L.
Xiong, B. P?oczos, and J. G. Schneider.
2013.
Efficientlearning on point sets.
In ICDM, pages 847?856.W.
Yih, M. Chang, C. Meek, and A. Pastusiak.
2013.Question answering using enhanced lexical semanticmodels.
In ACL, pages 1744?1753.Y.
Zhao, Z. Liu, and M. Sun.
2015.
Phrase type sensitivetensor indexing model for semantic composition.
InAAAI.1280
