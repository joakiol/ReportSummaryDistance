Automatic summarization of search engine hit listsDragomir R. RadevSchool of Information, University of Michigan550 E. University St.Ann Arbor,/vii 48109radev@umich, eduWeiguo FanUniversity of Michigan Business School701 Tappan St.Ann Arbor, M148109wfan@umich, eduAbstractWe present our work on open-domainmulti-document summarization in theframework of Web search.
Our system,SNS (pronounced "essence"), retrievesdocuments related to an unrestricted userquery and summarizes a subset of them asselected by the user.
We present a task-based extrinsic evaluation of the quality ofthe produced multi-document summaries.The evaluation results show thatsummarization quality is relatively highand does help improve the reading speedand judge the relevance of the retrievedURLs.1 IntroductionOnline information is increasingly available atan exponential rate.
According to a recentstudy by NetSizer (2000), the number of webhosts has increased from 30 million inJan.1998 to 44 million in Jan. 1999, and tomore than 70 million in Jan. 2000.
More than2 million new hosts were added to the Internetin Feb. 2000, according to this report.
SimilarInternet growth results were reported byIntemet Domain Service (IDS, 2000).
Thenumber of web pages on the Intemet was 320million pages in Dec. 1997 as reported byLawrence t al.
(1997), 800 million in Feb.1999 (Lawrence t al.
1999), and more than1,720 million in March, 2000 (Censorware,2000).
The number of pages available on theInternet alost doubles every ear.To help alleviate the information overloadproblem and help users find the informationthey need, many search engines emerge.
Theybuild a huge centralized atabase to index aportion of the Intemet: ranging from 10million to more than 300 million of webpages.
Search engines do help reduce theinformation overload problem by allowing auser to do a centralized search, but they alsobring up another problem for the user: toomany web pages are returned for a singlequery.
To find out which documents areuseful, the user often have to sift throughhundreds of pages to find out that only a fewof them are relevant.
Moreover, browsingthrough the long list of retrieval results is sotedious that few users would be willing to gothrough.
That's why research results haveshown that search engine users often give uptheir search in the first try, examining no morethan 10 documents (Jansen et al 2000).
Itwould be very helpful if an effective searchengine could be designed to help classify theretrieved web pages into clusters and providemore contextual nd summary information tohelp these users explore the retrieval set moreefficiently.Recent advances in information retrieval,natural language processing, computationallinguistics make it easier to build a helpfulsearch engine based on summaries of hit lists.We describe in this paper a prototype system,SNS, which blends the traditional informationretrieval technology with the advanceddocument clustering and multi-documentsummarization technology in an integratedframework.
The following steps are performedfor a given query:99Figure 1: Architecture diagramThe general architecture of our system isshown in Figure 1.
User interaction with SNScan be done in three different modes:?
Web search mode.
The user enters ageneral-domain query in the search engine(MySearch).
The result is a set of relateddocuments (the hit-list).
The user thenselects which of  the hits should besummarized.
MEAD, the summarizationcomponent produces a cross-documentsummary of the documents selected by theuser from the hit list.?
Intranet mode.
The user indicates whatcollection of documents needs to besummarized.
These documents are notnecessarily extracted from the Web.?
Clustering mode.
The user indicates thateither the hit list of the search engine or astand-alone document collection needs tobe clustered.
CIDR, the clusteringcomponent, creates clusters of documents.For each cluster, MEAD produces a cross-document summary.Our paper is organized as follows.
Sections 24 describe the system.
More specifically:Section 2 explains how the search engineoperates, Section 3 deals with the clusteringmodule while Section 4 presents the multi-document summarizer.
Section 5 describes theuser interface of  the system.
In Section 6, wepresent some experimental results.
After wecompare our work to related research inSection 7, we conclude the paper in Section 8.2 SearchThe search component of SNS is apersonalized search engine called MySearch.MySearch utilizes a centralized relationaldatabase to store all the URL indexes andother related URL information.
Spiders areused to fetch URLs from the Internet.
After aURL is downloaded, the following steps areapplied to index the URL:?
Parse the HTML file, remove all thosetags?
Apply Porter's stemming algorithms toeach keyword.?
Remove stop words?
Index each keyword into the databasealong with its frequency and positioninformation.The contents of URLs are indexed based onthe locations of the keywords: Anchor, Title,and Body.
This allows weighted retrievalbased on different word positions.
Forexample, a user can specify that he'd like togive a weight 5 for the keyword appearing inthe title, 4 for anchor, and 2 for body.
Thisinformation can be saved in his personalprofile and used for later weighted ranking.Besides the weighted search, MySearch alsosupports Boolean search and Vector Spacesearch (Salton, 1989).
For the vector spacemodel, the famous TF-IDF is used for rankingpurpose.
We used a modified version of TF-IDF: log(or+O.5)*log(N/df), where if means thenumber of times a term appeared in thecontent of an URL, N is the total number ofdocuments in the text collection, and dfstandsfor the number of  unique URLs in which aterm appears in the entire collection.A user can choose which search method hewants to use.
He/she can also combineBoolean search with Vector Space search.These options are provided to give users moreflexibility to control the retrieval results as100past research indicated that different rankingfunctions give different performances (Salton,1989).A sample search for "Clinton" using the TF-IDF Vector Space search is shown in Figure 3.The keyword "Clinton" is highlighted using adifferent color to help users get morecontextual information.
The retrieval statusvalue is shown in a bold black font after theURL title.3 ClusteringOur system uses two types of clustered input-either the set of hits that the user has selectedor the output of our own clustering engine -CIDR (Columbia Intelligent DocumentRelater).
CIDR is described in (Radev et al,1999).
It uses an iterative algorithm thatcreates as a side product so-called "documentcentroids".
The centroids contain the mosthighly relevant words to the entire cluster (notto the user query).
We use these words to findthe most salient "themes" in the cluster ofdocuments.3.1 Finding themes within clustersOne of the underlying assumptions behindSNS is that when a user selects a set of hitsafter reading the single-document summariesfrom the hit list retrieved by the system, he orshe performs a cognitive activity whereby heor she selects documents which appear to berelated to one or more common themes.
Themulti-document summarization algorithmattempts to identify these themes and toidentify the most salient passages from theselected ocuments using a pseudo-documentcalled the cluster centroid which is computedautomatically from the entire list of hitsselected by the user.3.2 Computing centroidsFigure 2 describes a sample of a clustercentroid.
The TF column indicates the averageterm frequency of a given term within thecluster.
E.g., a TF value of 13.33 for threedocuments indicates that the term "'deny"appears 40 times in the three documents.
TheIDF values are computed from a mixture of200 MB of news and web-based ocuments.Term TF IDF Scoreapp 20.67 8.90 'I 83.88lewinsky 34.67 5.25 182.03currie 15.33 7.60 116.50ms 32.00 3.06 '97.97january 25.33 3.30 83.60jordan 18.67 4.06 75.81referrai 9.00 7.43 66.88magaziner 6.67 10.00 66.64Deny 13.33 4.92 65.61Admit 13.00 4.92 63.97monica 14.67 4.29 62.85oic 5.67 I 0.00 56.64betty 8.00 6.01 48.06vernon 8.67 5.49 47.54'do .... 32.67 1.40 45.80Telephoned 6.67 6.86 45.74.you 36.33 1.19 43.30i 42.67 0.96 40.84clinton 16.33 2.23 36.39jones 11.33 3.17 35.88or 32.33 ~ 1.09 35.20gif 3.33 9.30 31.01white 12.00 2.50 30.01tripp 4.67 6.23 29.10ctv 3.00 ~ 9.30 27.91december 7.33 3.71 27.19Figure 2: A sample cluster centroifl4 Centroid-based summarizationThe main technique that we use forsummarization is sentence extraction.
Wescore individually each sentence within acluster and output hese that score the highest.A more detailed escription of the summarizercan be found in (Radev et al, 2000).The input to the summarization component isa cluster of documents.
These documents canbe either the result of a user query or theoutput of CIDR.The summarizer takes as input a cluster o lddocuments with a total of n sentences as wellas a compression ratio parameter r whichindicates how much of the original cluster topreserve.101The output consists of a sequence of In * r\]sentences from the original documents in thesame order as the input documents.
Thehighest-ranking sentences are includedaccording to the scoring formula below:S~ = wcC~ + wpPi + wfFiIn the formula, we, wp, wf are weights.
Ci is thecentroid score of the sentence, P~ is thepositional score of the sentence, and F~ is thescore of the sentence according to the overlapwith the first sentence of the document.4.1 Centroid valueThe centroid value C~ for sentence Si iscomputed as the sum of the centroid values Cwof all words in the sentence.
For example, thesentence "President Clinton met with VernonJordon in January" gets a score of 243.34which is the sum of the individual eentroidvalues of the words (clinton = 36.39; vernon =47.54; jordan = 75.81; january = 83.60).Ci = E cww4.2 Positional valueThe positional value is computed as follows:the first sentence in a document gets the samescore Cm,~, as the highest-ranking sentence inthe document according to the centroid value.The score for all sentences within a documentis computed according to the followingformula:Pi = (n - i + 1) .
mFx(Ci )n tFor example, if the sentence described aboveappears as the third sentence out of 30 in adocument and the largest centroid value of anysentence in the given document is 917.31, thepositional value P3  will be = 28/30 * 917.314.3 First-sentence overlapThe overlap value is computed as the innerproduct of the sentence vectors for the currentsentence i and the first sentence of thedocument.
The sentence vectors are the n-dimensional representations of the words ineach sentence whereby the value at position iof  a sentence vector indicates the number ofoccurrences of that word in the sentence.Fi = Sl Si4.4 Combining the three parametersAs indicated in (Radev & al., 2000) we haveexperimented with several weighting schemesfor the three parameters (centroid, position,and first-sentence overlap).
Until this moment,we have not come to the point in which thethree weights we, wp, and wf are eitherautomatically learned or derived from a userprofile.
Instead, we have experimented withvarious sets of empirically determined valuesfor the weights.
In this paper the results arebased on equal weights for the threeparameters wc = wp = wf= 1.5 User InterfaceWe describe in this section the user interfacefor web search mode as described earlier inSection 1.One component of  our system is the searchengine (MySearch).
The detailed esign of thesearch component is discussed in Section 2.The result of  a sample query "'Clinton" to oursearch engine is shown starting in Figure 4.102SSEARCHTcmponn'yWeb In~fa~?
~bgm._~J~Displaying t lom 1-10 oftolal ~1~ NSea'c.h: I .
court v online \[LgS\] rCourt 'IV Onrme Tcxt ofPrtsident Cllhaton's respomcs to JudkktyDragomir R. Radcv * httv:/fwww.?ourttv.?om/?
ase ffiles/dlntonerisis/1127' '~ amwerptcxt.htrd43 KB0 Cohmabla U.1990-2000?
~.
of Michipn2000 index JLJU.I~.
STAR.R SCANDAl., A site chronicling the ?n'ongdoings ofth,c ..,Shadow Oovcmmcat of Kcrmct5 W .
.
.
.
.
~:?
http.J/www, gt ocitles.
?
om/c apitoll'lill/$~at e/9634/ 27KB '..'iMaintain?d by radev(~fumich.eduFigure 3: Sample user queryA user has the option to choose a specificranking function as well as the number ofretrieval results to be shown in a single screen.The keyword contained in the query string willbe automatically highlighted in the searchresults to provide contextual information forthe user.The overall interface for SNS is shown inFigure 4.
On the top right of the frame is theMySearch search engine.
When a usersubmits a query, the screen in Figure 5appears.
As can be seen from Figure 5, there isa check box along with each retrieved record.This allows the user to tell the summarizationengine which documents he/she wants tosummarize.
After the user clicks thesummarization button, the summarizationoption screen is displayed as shown in bottomof Figure 6.
The summarization ption screenallows a user to specify the summarizationcompression ratio.
Figure 7 shows thesummarization result for four URLs with thecompression ratio set as 30%.103_------:tl .
.
.
.
.
.
.
, : - - - -  ,,,~,,, I I I =1~i ,~.
.
.
.
.
.
>~,<... .
.
.
.
_ .... ~,, ~7~L ,.~ ?
~.~"raltllw.=y?
Abou~Wlllli0 FmtStmmii~=~mco Columbh U.I~8.1000O U. of ~,clillm2OOO,==,~, , ,=  I I~  -~.~,!i l iq iPlease click on "submit" in the frame above to continue.Mtilidm=d by rad~n,(~a, s i eh,~a~F igure  4: SNS in ter face  (framed)SSEARCH s~ iop s~ ~_~a~ Io .
.
.
."
- -  5 ~,,.BT ~ ' y  .
;Web \ ]n~l??
i!
9. ooficv ncws events ~ ~?f in l  rc~r t  dot=Is chinese ?s~ie~rm~?
c f f~ \[1.76\] I~?
Ab~t  1~o~?y corn Niwi Evmts Dili~F Biiii~i i Cox Ripmt D, ctliRi Chi~st?
~ F..il#muqpl FJTolrtl TodI~fl NIwl  D.-.
::W "ti~t~ Fm Close Up Foundation U S PoFlcy Towlrd Cuba ~ Up Foundafiou Spctill :;:lSumm~mtfiom Tol~c Psq~ U S Poficy TowL..Drlt!,omh" R. Rt~l~ o htlod/www.dgleuo.om/euba.hi m 291-~ :::!.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
: ...................o C~lumbil U.199g-2000 ENext 1 > >1 : ' : ~ I0 'I3.
o f~ J~ i~ ........................................................................................................................................2000 Pmme~ by MySQL.
~,~pacl~e m =~m L~m SP.~RC mv~.
l~ySmscl ~s ~ sazm a~e sncegvv~at ~e~cmmmezce Ib  l i  ~ U~,~.~'~ of ~,~c~s Bm~e~Ally ~ ' , ,~et l  in~ llulllellion.l Io ttle syl l l=l  iloul~l be lt.ected Io lbe w ~b~ter .lease c !ck on su mi in he frame a ov e con :uue.
.... ~lld .a ,~o4 by rade~,t~ch.fdu .II~ .
.
, - : s ,= .=o.
.
; .
,  o : , .
:  .
, -  , .
, .
:~ , .
: .
.+  , "<-  .
.
.
.
.
- ?
.
~ - -  , - -  ~ ~  ~1~igure 5: Sea:ch output along with user selection of documents to be summarized104SSEARCHr~Web Int~'fa~e?
,About10.
dose foundatlon pol;c V cub:~ \[1.74;\] I~Close Up Foundation U S Policy Toward Cuba Close Up F o ~  SpecialTopic Page U S Policy Towa...o http~',~.ww.ClOSeUp.o:~cu~htm 29 K~S?areb: Stmm, m~2~ag 4welguo Fan:t0:i hnp.~/w~v ?
!o,;c,~.o~cubxhtmo U. of Mid iani ~ e d  by radev(~umfch.edu ltFigure 6: Selected documents for summarizationSSEARCHTempm'm'yWeb Im:c:ffacc?
.Abou+.Se~cl~Wci~uo FanSmmmeiantio~Dragomir R. P, adevC Columbia U.1998-20130O U. of Miehi~n2OOO10.
clos9 fotmda6on pol;c.y cuba !1.76\] I~Summary @ 30% of the URLs that youhtqrJ/www.
?Om'ttv, eem/e~gf~s/c~tonczLt~/l12798_answerst~ht~hl~'J/~,,w.scatllcl~c~ om/czlz~/l~owsc&tm~71~_O52297.hmdselected:ii ISnn'mq~,~ng 1360 .~cm, anccs ~ 30% = 408 scntc~nccsS '~, t ion  stmrt?diTightcnmg tbc Embm'g?F?r akn?st fmW Yeses the Unlt?d States has n?t imported any Cubm ~53.71~ .nor allowed ~ ~ food.m~dk~ supplies.re', c~ to.
?~ ~ .
.
.
.
.
.
i i !~o o~c~ ?ounUy has joined lhc Umlcd States in the lmdc cmbmrgo agminst Cuba in fact theM=finta~ncd by r~evC,wnich.e~Figure 7: Output of the summarizer105The following information is shown in thesummarization result screen in Figure 7:?
The number of sentences in the text of theset of  URLs that the user selected?
The number of sentences in the summaryThe sentences representing the themes of thoseselected URLs and their relative scores.
Thesentences are ordered the same way they appearin the original set of documents.6 Experimental resultsOur system was evaluated using the task-basedextrinsic measure as suggested in (Mani et al1999).
The experiment was set up as follows:Three sets of  documents on different topics wereselected prior to the experiment.
The topics andtheir corresponding document information areshown in Table 1.~opie No.
Lengtl~S1 200k$2 Introduction to Data Mining 100k$3 Intelligent Agents and their application inInformation retrieval 5 160kTable 1: Evaluation Topics and their corresponding document set information!iIThe term data mining is then this high-level application techniques / tools used to 5^9..present and analyze data for decision makers ~~!
!
~! '
==I!
i i1 \ ]  I i l I i=I i i~ "?
~,  = .
.
.
.
.
.
.
.
.
.
, ....... .
,= = ~.~ ,116111 =.
,  = ...... - .=  ...,... ......... ,, .......... = iI~.
',=~ .==.,~!\],!~:;= =!, I iFigure 8: A sample of the summarization result for $2 at 10% compression rateAs Table 1 shows, the articles in topic set $1 arelonger than both these in $2 and $3.
The articlesin $3 are the shortest, with each 32k in average.The number of documents in each topic set is106also different.
The variations of document lengthand different number of documents in each topicset will help test the robustness of oursummarization algorithms.We used SNS to generate both 10% and 20%summaries for each topic.
A sample of the 10%summary for topic $2 is shown in Figure 8.
Fourusers were selected for evaluation of thesesummarization results.
Each user was asked toread through the set of full articles for each topicf'wst, followed by its corresponding 10% and20% summaries.
After these 4 users finishedeach set, they were asked to assign a readabilityscore (1-10) for each summary.
The higher thereadability score is, the more readable andmeaningful for comprehension is the summary.The time of reading both full articles andsummaries was tracked and recorded.Table 2: Summarization evaluation: detailed results7.92Table 3: Summary of the evaluation resultsThe detailed evaluation results are shown in topics.
The summaries generated by SNS areTable 2.
Table 3 gives the summary of the Table also very readable.
For example, The average2.
It's shown in Table 2 that these four users readability score (which is obtained byhave different reading speeds.
However, their averaging the readability scores assigned by thereading speed is pretty consistent across the 3 four users) for 10% and 20% summaries for107topic S1, is 8, 8 respectively.
For topic $3, theaverage readability score for 10% and 20%summaries is 7.75, and 8.75, respectively.Similarly, for $2 the average readability scorefor 10% and 20% summaries is 8 and 8.5,respectively.
The differences in the averagereadability score also suggest that (a) oursummarizer favors longer documents overshorter documents; (tO 20% summaries aregenerally favorable over 10% summaries.
Thedifference in the readability score between 10%and 20% summaries i bigger in $3 (diff = 1.0)than in S1 (diff = 0).
These interesting findingsraise interesting questions for future research.As can be seen from Table 3, the 20% summaryachieves better eadability score in overall thanthe 10% summary.
The speedup of the 10%summary over full articles is 6.87.
That is, withreading material reduced by 900%, the speedupin reading is only 687%.
This suggests that theremay be a little bit difficulty in reading the 10%summary result.
This may be due to the simplesentence boundary detection algorithm we used.The feedback from users in the evaluation seemsto corffirm the above reason.
As more sentenceswere included in the 20% summaries, thespeedup in reading (4.22) almost approached theoptimal speedup ratio (5.0)L7 Related WorkNeto et al (2000) describes a text mining toolthat performs document clustering and textsummarization.
They used the Autoclassalgorithm to perform document clustering andused TF-ISF (an adaptation of TF-IDF) toperform sentence ranking and generate thesummarization output.
Our work is differentfrom theirs in that we perform personalizedsummarization based on the retrieval result froma generic personalized web-based search engine.A more complicated sentence ranking functionsis employed to boost the ranking performance.The compression ratio for the summary iscustomizable by a user.
Both single-documentfor a single URL and multiple-documenti Since the length of the summary is only 20% of theoriginal documents, he maximum speedup in terms ofreading time is 1/0.2=5.summarization for a cluster of URLs aresupported inour system.More related work can be found in Extractorweb site http'J/extractor.iit.nrc.ca/.
They useMetaCrawler to perform web-based search andautomatically generate summaries for eachURLs retrieved.
They only support singledocument summarization i  their engine and thecompression rate of the summarizer is also non-customizable.
We not only support both singleand multiple document summarization, but alsoallow the user to specify the summarizationcompression ratio as well as to get per-clustersummaries of automatically generated clusters,which, we believe, are more valuable to onlineusers and give them more flexibility and controlof the surnrnarization results.8 Conclusion and Future WorkWe described in this paper a prototype systemSNS, which integrates natural languageprocessing and information retrieval techniquesto perform automatic ustomized summarizationof search engine results.
The user interface anddetailed design of SNS's components are alsodiscussed.
Task-based extrinsic evaluationshowed that the system is of reasonably highquality.The following issues will be addressed in thefuture.8.1 Interaction between sentence inclusionin a summaryThere are two types of interaction (orreinforcement) between sentences in a summary:negative and positive.Negative interaction occurs when the inclusionof one sentence in the summary indicates thatanother sentence should not appear in thesummary.
This is particularly relevant o multi-document summarization as in this case:negative interaction models the non-inclusion ofredundant information.The case of positive interaction i volves positivereinforcement between sentences.
For example,if a sentence with a referring expression is to be108included in a stma~lary, typically the sentencecontaining the antecedent should also be added.We will investigate specific setups in whichpositive and/or negative reinforcement betweensentences i  practical and useful.8.2 PersonalizationWe will investigate additional techniques forproducing personalized summaries.
Some of theapproaches that we are considering are:Query words: favoring sentences thatinclude words from the user query in theWeb-based scenarioPersonal preferences and interaction history:we would favor sentences that match theuser profile (e.g., overlapping with his or herlong-term interests and/or recent querieslogged by the system).8.3 Technical limitationsThe current version of our system uses a fairlybasic sentence delimiting component.
We willinvestigate the user of robust sentence boundaryidentification modules in the future.We will also investigate the possibility of somelimited-form anaphora resolution component.8.4 AvailabilityA demonstration version of SNS is available atthe following UP.L:http://www.si.umich.edu/-radev/ssearch/Lawrence, S., and Giles, C. L. (1997).
Searching theWorld Wide Web, Science, 280(3), 98-100.Lawrence, S., and Giles, C. L. (1999).
Accessibility ofinformation on the web, Nature, 400, 107-109.Mani, I. and BIoedorn, E. (1999).
Summarizingsimilarities and di~rences among relateddocuments.
Information Retrieval 1(1): 35--67.Mani, I., House, D., Klein, G., Hirschman, L., Obrst,L., Firmin, T., Chrzanowski, M., and Sundheim, B.(1998).
The TIPSTER SUMMA C TextSummarization Evaluation.
The MITRECorporation Technical Report MTR 98W0000138,McLean, Virginia.McKeown, K. and D. R. Radev.
Generating Summariesof Multiple News Articles.
Proceedings, ACMConference on Research and Development inInformation Retrieval S1GIR'95 (Seattle, WA, July1995).NetSizer (2000).
http~//www.netsizer.com/.Neto, J. L., Santos, A. D., Kaestner, C. A.
A., andFreitas, A.
A.
(2000).
Document clustering and textsummarization.
In Proceedings, 4th Int.
Conferenceon Practical Applications of Knowledge Discoveryand Data Mining (PADD-2000), 41-55.
London:The Practical Application Company.Radev, D. R., Hatzivassiloglou, V., and McKeown,K.
A Description of the CIDR System as Used forTDT-2.
Proceedings, DARPA Broadcast NewsWorkshop, (Herndon, VA, February 1999).Radev, D. R, Jing, H., and Stys-Budzikowska, M.Summarization of multiple documents: clustering,sentence xtraction, and evaluation.
Proceedings,ANLP-NAACL Workshop on AutomaticSummarization, (Seattle, WA, April 2000)Salton, G. (1989).
Automatic Text Processing.Addison-Wesley Publishing Co., Reading, MA,1989.ReferencesCarbonell, J. and Goldstein, J.
(1998).
The use ofMMR, Diversity-Based Reranking for ReorderingDocuments and Producing Summaries.
PosterSession, SIGIR'98, Melbourne, Australia.Censorware (2000).http://www.censorware.org/web size/.Extractor (2000).
http://extractor.iit.nrc.ca/.IDS .
(2000).
lnternet Domain Survey.http://www:isc.org/dsl.Jansen, B. J., Spink, A., and Saracevic, T. (2000).Real life.
real users, and real needs: a study andanalysis of user queries on the web.
InformationProcessing and Management.
36(2), 207-227.109
