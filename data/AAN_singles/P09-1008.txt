Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 64?72,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPTopological Field Parsing of GermanJackie Chi Kit CheungDepartment of Computer ScienceUniversity of TorontoToronto, ON, M5S 3G4, Canadajcheung@cs.toronto.eduGerald PennDepartment of Computer ScienceUniversity of TorontoToronto, ON, M5S 3G4, Canadagpenn@cs.toronto.eduAbstractFreer-word-order languages such as Ger-man exhibit linguistic phenomena thatpresent unique challenges to traditionalCFG parsing.
Such phenomena producediscontinuous constituents, which are notnaturally modelled by projective phrasestructure trees.
In this paper, we exam-ine topological field parsing, a shallowform of parsing which identifies the ma-jor sections of a sentence in relation tothe clausal main verb and the subordinat-ing heads.
We report the results of topo-logical field parsing of German using theunlexicalized, latent variable-based Berke-ley parser (Petrov et al, 2006) Withoutany language- or model-dependent adapta-tion, we achieve state-of-the-art results onthe Tu?Ba-D/Z corpus, and a modified NE-GRA corpus that has been automaticallyannotated with topological fields (Beckerand Frank, 2002).
We also perform a qual-itative error analysis of the parser output,and discuss strategies to further improvethe parsing results.1 IntroductionFreer-word-order languages such as German ex-hibit linguistic phenomena that present uniquechallenges to traditional CFG parsing.
Topic focusordering and word order constraints that are sen-sitive to phenomena other than grammatical func-tion produce discontinuous constituents, which arenot naturally modelled by projective (i.e., with-out crossing branches) phrase structure trees.
Inthis paper, we examine topological field parsing, ashallow form of parsing which identifies the ma-jor sections of a sentence in relation to the clausalmain verb and subordinating heads, when present.We report the results of parsing German usingthe unlexicalized, latent variable-based Berkeleyparser (Petrov et al, 2006).
Without any language-or model-dependent adaptation, we achieve state-of-the-art results on the Tu?Ba-D/Z corpus (Telljo-hann et al, 2004), with a F1-measure of 95.15%using gold POS tags.
A further reranking ofthe parser output based on a constraint involv-ing paired punctuation produces a slight additionalperformance gain.
To facilitate comparison withprevious work, we also conducted experiments ona modified NEGRA corpus that has been automat-ically annotated with topological fields (Beckerand Frank, 2002), and found that the Berkeleyparser outperforms the method described in thatwork.
Finally, we perform a qualitative error anal-ysis of the parser output on the Tu?Ba-D/Z corpus,and discuss strategies to further improve the pars-ing results.German syntax and parsing have been studiedusing a variety of grammar formalisms.
Hocken-maier (2006) has translated the German TIGERcorpus (Brants et al, 2002) into a CCG-basedtreebank to model word order variations in Ger-man.
Foth et al (2004) consider a version of de-pendency grammars known as weighted constraintdependency grammars for parsing German sen-tences.
On the NEGRA corpus (Skut et al, 1998),they achieve an accuracy of 89.0% on parsing de-pendency edges.
In Callmeier (2000), a platformfor efficient HPSG parsing is developed.
Thisparser is later extended by Frank et al (2003)with a topological field parser for more efficientparsing of German.
The system by Rohrer andForst (2006) produces LFG parses using a manu-ally designed grammar and a stochastic parse dis-ambiguation process.
They test on the TIGER cor-pus and achieve an F1-measure of 84.20%.
InDubey and Keller (2003), PCFG parsing of NE-GRA is improved by using sister-head dependen-cies, which outperforms standard head lexicaliza-tion as well as an unlexicalized model.
The best64performing model with gold tags achieve an F1of 75.60%.
Sister-head dependencies are useful inthis case because of the flat structure of NEGRA?strees.In contrast to the deeper approaches to parsingdescribed above, topological field parsing identi-fies the major sections of a sentence in relationto the clausal main verb and subordinating heads,when present.
Like other forms of shallow pars-ing, topological field parsing is useful as the firststage to further processing and eventual seman-tic analysis.
As mentioned above, the output ofa topological field parser is used as a guide tothe search space of a HPSG parsing algorithm inFrank et al (2003).
In Neumann et al (2000),topological field parsing is part of a divide-and-conquer strategy for shallow analysis of Germantext with the goal of improving an information ex-traction system.Existing work in identifying topological fieldscan be divided into chunkers, which identify thelowest-level non-recursive topological fields, andparsers, which also identify sentence and clausalstructure.Veenstra et al (2002) compare three approachesto topological field chunking based on finite statetransducers, memory-based learning, and PCFGsrespectively.
It is found that the three techniquesperform about equally well, with F1 of 94.1% us-ing POS tags from the TnT tagger, and 98.4% withgold tags.
In Liepert (2003), a topological fieldchunker is implemented using a multi-class ex-tension to the canonically two-class support vec-tor machine (SVM) machine learning framework.Parameters to the machine learning algorithm arefine-tuned by a genetic search algorithm, with aresulting F1-measure of 92.25%.
Training the pa-rameters to SVM does not have a large effect onperformance, increasing the F1-measure in the testset by only 0.11%.The corpus-based, stochastic topological fieldparser of Becker and Frank (2002) is based ona standard treebank PCFG model, in which ruleprobabilities are estimated by frequency counts.This model includes several enhancements, whichare also found in the Berkeley parser.
First,they use parameterized categories, splitting non-terminals according to linguistically based intu-itions, such as splitting different clause types (theydo not distinguish different clause types as basiccategories, unlike Tu?Ba-D/Z).
Second, they takeinto account punctuation, which may help iden-tify clause boundaries.
They also binarize the veryflat topological tree structures, and prune rulesthat only occur once.
They test their parser on aversion of the NEGRA corpus, which has beenannotated with topological fields using a semi-automatic method.Ule (2003) proposes a process termed DirectedTreebank Refinement (DTR).
The goal of DTR isto refine a corpus to improve parsing performance.DTR is comparable to the idea of latent variablegrammars on which the Berkeley parser is based,in that both consider the observed treebank to beless than ideal and both attempt to refine it by split-ting and merging nonterminals.
In this work, split-ting and merging nonterminals are done by consid-ering the nonterminals?
contexts (i.e., their parentnodes) and the distribution of their productions.Unlike in the Berkeley parser, splitting and merg-ing are distinct stages, rather than parts of a sin-gle iteration.
Multiple splits are found first, thenmultiple rounds of merging are performed.
Nosmoothing is done.
As an evaluation, DTR is ap-plied to topological field parsing of the Tu?Ba-D/Zcorpus.
We discuss the performance of these topo-logical field parsers in more detail below.All of the topological parsing proposals pre-date the advent of the Berkeley parser.
The exper-iments of this paper demonstrate that the Berke-ley parser outperforms previous methods, many ofwhich are specialized for the task of topologicalfield chunking or parsing.2 Topological Field Model of GermanTopological fields are high-level linear fields inan enclosing syntactic region, such as a clause(Ho?hle, 1983).
These fields may have constraintson the number of words or phrases they contain,and do not necessarily form a semantically co-herent constituent.
Although it has been arguedthat a few languages have no word-order con-straints whatsoever, most ?free word-order?
lan-guages (even Warlpiri) have at the very least somesort of sentence- or clause-initial topic field fol-lowed by a second position that is occupied byclitics, a finite verb or certain complementizersand subordinating conjunctions.
In a few Ger-manic languages, including German, the topologyis far richer than that, serving to identify all ofthe components of the verbal head of a clause,except for some cases of long-distance dependen-65cies.
Topological fields are useful, because whileGermanic word order is relatively free with respectto grammatical functions, the order of the topolog-ical fields is strict and unvarying.Type FieldsVL (KOORD) (C) (MF) VC (NF)V1 (KOORD) (LV) LK (MF) (VC) (NF)V2 (KOORD) (LV) VF LK (MF) (VC) (NF)Table 1: Topological field model of German.Simplified from Tu?Ba-D/Z corpus?s annotationschema (Telljohann et al, 2006).In the German topological field model, clausesbelong to one of three types: verb-last (VL), verb-second (V2), and verb-first (V1), each with a spe-cific sequence of topological fields (Table 1).
VLclauses include finite and non-finite subordinateclauses, V2 sentences are typically declarativesentences and WH-questions in matrix clauses,and V1 sentences include yes-no questions, andcertain conditional subordinate clauses.
Below,we give brief descriptions of the most commontopological fields.?
VF (Vorfeld or ?pre-field?)
is the first con-stituent in sentences of the V2 type.
This isoften the topic of the sentence, though as ananonymous reviewer pointed out, this posi-tion does not correspond to a single functionwith respect to information structure.
(e.g.,the reviewer suggested this case, where VFcontains the focus: ?Wer kommt zur Party?
?Peter kommt zur Party.
?Who is coming tothe Party?
?Peter is coming to the party.)?
LK (Linke Klammer or ?left bracket?)
is theposition for finite verbs in V1 and V2 sen-tences.
It is replaced by a complementizerwith the field label C in VL sentences.?
MF (Mittelfeld or ?middle field?)
is an op-tional field bounded on the left by LK andon the right by the verbal complex VC orby NF.
Most verb arguments, adverbs, andprepositional phrases are found here, unlessthey have been fronted and put in the VF, orare prosodically heavy and postposed to theNF field.?
VC is the verbal complex field.
It includesinfinite verbs, as well as finite verbs in VLsentences.?
NF (Nachfeld or ?post-field?)
containsprosodically heavy elements such as post-posed prepositional phrases or relativeclauses.?
KOORD1 (Koordinationsfeld or ?coordina-tion field?)
is a field for clause-level conjunc-tions.?
LV (Linksversetzung or ?left dislocation?)
isused for resumptive constructions involvingleft dislocation.
For a detailed linguistictreatment, see (Frey, 2004).Exceptions to the topological field model as de-scribed above do exist.
For instance, parentheticalconstructions exist as a mostly syntactically inde-pendent clause inside another sentence.
In our cor-pus, they are attached directly underneath a clausalnode without any intervening topological field, asin the following example.
In this example, the par-enthetical construction is highlighted in bold print.Some clause and topological field labels under theNF field are omitted for clarity.
(1) (a) (SIMPX ?
(VF Man) (LK mu?)
(VC verstehen) ?, (SIMPX sagte er), ?
(NF da?
dieseMinderheiten seit langer Zeit massiv von denNazis bedroht werden)).
?
(b) Translation: ?One must understand,?
he said,?that these minorities have been massivelythreatened by the Nazis for a long time.
?3 A Latent Variable ParserFor our experiments, we used the latent variable-based Berkeley parser (Petrov et al, 2006).
La-tent variable parsing assumes that an observedtreebank represents a coarse approximation ofan underlying, optimally refined grammar whichmakes more fine-grained distinctions in the syn-tactic categories.
For example, the noun phrasecategory NP in a treebank could be viewed as acoarse approximation of two noun phrase cate-gories corresponding to subjects and object, NP?S,and NP?VP.The Berkeley parser automates the process offinding such distinctions.
It starts with a simple bi-narized X-bar grammar style backbone, and goesthrough iterations of splitting and merging non-terminals, in order to maximize the likelihood ofthe training set treebank.
In the splitting stage,1The Tu?Ba-D/Z corpus distinguishes coordinating andnon-coordinating particles, as well as clausal and field co-ordination.
These distinctions need not concern us for thisexplanation.66Figure 1: ?I could never have done that just for aesthetic reasons.?
Sample Tu?Ba-D/Z tree, with topolog-ical field annotations and edge labels.
Topological field layer in bold.an Expectation-Maximization algorithm is used tofind a good split for each nonterminal.
In themerging stage, categories that have been over-split are merged together to keep the grammar sizetractable and reduce sparsity.
Finally, a smoothingstage occurs, where the probabilities of rules foreach nonterminal are smoothed toward the prob-abilities of the other nonterminals split from thesame syntactic category.The Berkeley parser has been applied to theTu?BaD/Z corpus in the constituent parsing sharedtask of the ACL-2008 Workshop on Parsing Ger-man (Petrov and Klein, 2008), achieving an F1-measure of 85.10% and 83.18% with and withoutgold standard POS tags respectively2.
We chosethe Berkeley parser for topological field parsingbecause it is known to be robust across languages,and because it is an unlexicalized parser.
Lexi-calization has been shown to be useful in moregeneral parsing applications due to lexical depen-dencies in constituent parsing (e.g.
(Ku?bler et al,2006; Dubey and Keller, 2003) in the case of Ger-man).
However, topological fields explain a higherlevel of structure pertaining to clause-level wordorder, and we hypothesize that lexicalization is un-likely to be helpful.4 Experiments4.1 DataFor our experiments, we primarily used the Tu?Ba-D/Z (Tu?binger Baumbank des Deutschen / Schrift-sprache) corpus, consisting of 26116 sentences(20894 training, 2611 development, 2089 test,with a further 522 sentences held out for future ex-2This evaluation considered grammatical functions aswell as the syntactic category.periments)3 taken from the German newspaper dietageszeitung.
The corpus consists of four levelsof annotation: clausal, topological, phrasal (otherthan clausal), and lexical.
We define the task oftopological field parsing to be recovering the firsttwo levels of annotation, following Ule (2003).We also tested the parser on a version of the NE-GRA corpus derived by Becker and Frank (2002),in which syntax trees have been made projec-tive and topological fields have been automaticallyadded through a series of linguistically informedtree modifications.
All internal phrasal structurenodes have also been removed.
The corpus con-sists of 20596 sentences, which we split into sub-sets of the same size as described by Becker andFrank (2002)4.
The set of topological fields inthis corpus differs slightly from the one used inTu?Ba-D/Z, making no distinction between clausetypes, nor consistently marking field or clauseconjunctions.
Because of the automatic anno-tation of topological fields, this corpus containsnumerous annotation errors.
Becker and Frank(2002) manually corrected their test set and eval-uated the automatic annotation process, reportinglabelled precision and recall of 93.0% and 93.6%compared to their manual annotations.
There arealso punctuation-related errors, including miss-ing punctuation, sentences ending in commas, andsentences composed of single punctuation marks.We test on this data in order to provide a bet-ter comparison with previous work.
Although wecould have trained the model in Becker and Frank(2002) on the Tu?Ba-D/Z corpus, it would not have3These are the same splits into training, development, andtest sets as in the ACL-08 Parsing German workshop.
Thiscorpus does not include sentences of length greater than 40.416476 training sentences, 1000 development, 1058 test-ing, and 2062 as held-out data.
We were unable to obtainthe exact subsets used by Becker and Frank (2002).
We willdiscuss the ramifications of this on our evaluation procedure.67Gold tags Edge labels LP% LR% F1% CB CB0% CB ?
2% EXACT%- - 93.53 93.17 93.35 0.08 94.59 99.43 79.50+ - 95.26 95.04 95.15 0.07 95.35 99.52 83.86- + 92.38 92.67 92.52 0.11 92.82 99.19 77.79+ + 92.36 92.60 92.48 0.11 92.82 99.19 77.64Table 2: Parsing results for topological fields and clausal constituents on the Tu?Ba-D/Z corpus.been a fair comparison, as the parser depends quiteheavily on NEGRA?s annotation scheme.
For ex-ample, Tu?Ba-D/Z does not contain an equiva-lent of the modified NEGRA?s parameterized cat-egories; there exist edge labels in Tu?BaD/Z, butthey are used to mark head-dependency relation-ships, not subtypes of syntactic categories.4.2 ResultsWe first report the results of our experiments onthe Tu?Ba-D/Z corpus.
For the Tu?Ba-D/Z corpus,we trained the Berkeley parser using the defaultparameter settings.
The grammar trainer attemptssix iterations of splitting, merging, and smoothingbefore returning the final grammar.
Intermediategrammars after each step are also saved.
Therewere training and test sentences without clausalconstituents or topological fields, which were ig-nored by the parser and by the evaluation.
Aspart of our experiment design, we investigated theeffect of providing gold POS tags to the parser,and the effect of incorporating edge labels into thenonterminal labels for training and parsing.
In allcases, gold annotations which include gold POStags were used when training the parser.We report the standard PARSEVAL measuresof parser performance in Table 2, obtained by theevalb program by Satoshi Sekine and MichaelCollins.
This table shows the results after five it-erations of grammar modification, parameterizedover whether we provide gold POS tags for pars-ing, and edge labels for training and parsing.
Thenumber of iterations was determined by experi-ments on the development set.
In the evaluation,we do not consider edge labels in determiningcorrectness, but do consider punctuation, as Ule(2003) did.
If we ignore punctuation in our evalu-ation, we obtain an F1-measure of 95.42% on thebest model (+ Gold tags, - Edge labels).Whether supplying gold POS tags improvesperformance depends on whether edge labels areconsidered in the grammar.
Without edge labels,gold POS tags improve performance by almosttwo points, corresponding to a relative error reduc-tion of 33%.
In contrast, performance is negativelyaffected when edge labels are used and gold POStags are supplied (i.e., + Gold tags, + Edge la-bels), making the performance worse than not sup-plying gold tags.
Incorporating edge label infor-mation does not appear to improve performance,possibly because it oversplits the initial treebankand interferes with the parser?s ability to determineoptimal splits for refining the grammar.Parser LP% LR% F1%Tu?Ba-D/ZThis work 95.26 95.04 95.15Ule unknown unknown 91.98NEGRA - from Becker and Frank (2002)BF02 (len.
?
40) 92.1 91.6 91.8NEGRA - our experimentsThis work (len.
?
40) 90.74 90.87 90.81BF02 (len.
?
40) 89.54 88.14 88.83This work (all) 90.29 90.51 90.40BF02 (all) 89.07 87.80 88.43Table 3: BF02 = (Becker and Frank, 2002).
Pars-ing results for topological fields and clausal con-stituents.
Results from Ule (2003) and our resultswere obtained using different training and test sets.The first row of results of Becker and Frank (2002)are from that paper; the rest were obtained by ourown experiments using that parser.
All results con-sider punctuation in evaluation.To facilitate a more direct comparison with pre-vious work, we also performed experiments on themodified NEGRA corpus.
In this corpus, topo-logical fields are parameterized, meaning that theyare labelled with further syntactic and semantic in-formation.
For example, VF is split into VF-RELfor relative clauses, and VF-TOPIC for those con-taining topics in a verb-second sentence, amongothers.
All productions in the corpus have alsobeen binarized.
Tuning the parameter settings onthe development set, we found that parameterizedcategories, binarization, and including punctua-tion gave the best F1 performance.
First-orderhorizontal and zeroth order vertical markoviza-68tion after six iterations of splitting, merging, andsmoothing gave the best F1 result of 91.78%.
Weparsed the corpus with both the Berkeley parserand the best performing model of Becker andFrank (2002).The results of these experiments on the test setfor sentences of length 40 or less and for all sen-tences are shown in Table 3.
We also show otherresults from previous work for reference.
Wefind that we achieve results that are better thanthe model in Becker and Frank (2002) on the testset.
The difference is statistically significant (p =0.0029, Wilcoxon signed-rank).The results we obtain using the parser of Beckerand Frank (2002) are worse than the results de-scribed in that paper.
We suggest the followingreasons for this discrepancy.
While the test setused in the paper was manually corrected for eval-uation, we did not correct our test set, because itwould be difficult to ensure that we adhered to thesame correction guidelines.
No details of the cor-rection process were provided in the paper, and de-scriptive grammars of German provide insufficientguidance on many of the examples in NEGRA onissues such as ellipses, short infinitival clauses,and expanded participial constructions modifyingnouns.
Also, because we could not obtain the ex-act sets used for training, development, and test-ing, we had to recreate the sets by randomly split-ting the corpus.4.3 Category Specific ResultsWe now return to the Tu?Ba-D/Z corpus for amore detailed analysis, and examine the category-specific results for our best performing model (+Gold tags, - Edge labels).
Overall, Table 4 showsthat the best performing topological field cate-gories are those that have constraints on the typeof word that is allowed to fill it (finite verbs inLK, verbs in VC, complementizers and subordi-nating conjunctions in C).
VF, in which only oneconstituent may appear, also performs relativelywell.
Topological fields that can contain a vari-able number of heterogeneous constituents, on theother hand, have poorer F1-measure results.
MF,which is basically defined relative to the positionsof fields on either side of it, is parsed several pointsbelow LK, C, and VC in accuracy.
NF, whichcontains different kinds of extraposed elements, isparsed at a substantially worse level.Poorly parsed categories tend to occur infre-quently, including LV, which marks a rare re-sumptive construction; FKOORD, which markstopological field coordination; and the discoursemarker DM.
The other clause-level constituents(PSIMPX for clauses in paratactic constructions,RSIMPX for relative clauses, and SIMPX forother clauses) also perform below average.Topological FieldsCategory # LP% LR% F1%PARORD 20 100.00 100.00 100.00VCE 3 100.00 100.00 100.00LK 2186 99.68 99.82 99.75C 642 99.53 98.44 98.98VC 1777 98.98 98.14 98.56VF 2044 96.84 97.55 97.20KOORD 99 96.91 94.95 95.92MF 2931 94.80 95.19 94.99NF 643 83.52 81.96 82.73FKOORD 156 75.16 73.72 74.43LV 17 10.00 5.88 7.41Clausal ConstituentsCategory # LP% LR% F1%SIMPX 2839 92.46 91.97 92.21RSIMPX 225 91.23 92.44 91.83PSIMPX 6 100.00 66.67 80.00DM 28 59.26 57.14 58.18Table 4: Category-specific results using grammarwith no edge labels and passing in gold POS tags.4.4 Reranking for Paired PunctuationWhile experimenting with the development setof Tu?Ba-D/Z, we noticed that the parser some-times returns parses, in which paired punctuation(e.g.
quotation marks, parentheses, brackets) isnot placed in the same clause?a linguistically im-plausible situation.
In these cases, the high-levelinformation provided by the paired punctuation isoverridden by the overall likelihood of the parsetree.
To rectify this problem, we performed a sim-ple post-hoc reranking of the 50-best parses pro-duced by the best parameter settings (+ Gold tags,- Edge labels), selecting the first parse that placespaired punctuation in the same clause, or return-ing the best parse if none of the 50 parses satisfythe constraint.
This procedure improved the F1-measure to 95.24% (LP = 95.39%, LR = 95.09%).Overall, 38 sentences were parsed with pairedpunctuation in different clauses, of which 16 werereranked.
Of the 38 sentences, reranking improvedperformance in 12 sentences, did not affect perfor-mance in 23 sentences (of which 10 already had aperfect parse), and hurt performance in three sen-tences.
A two-tailed sign test suggests that rerank-69ing improves performance (p = 0.0352).
We dis-cuss below why sentences with paired punctuationin different clauses can have perfect parse results.To investigate the upper-bound in performancethat this form of reranking is able to achieve, wecalculated some statistics on our (+ Gold tags, -Edge labels) 50-best list.
We found that the aver-age rank of the best scoring parse by F1-measureis 2.61, and the perfect parse is present for 1649of the 2088 sentences at an average rank of 1.90.The oracle F1-measure is 98.12%, indicating thata more comprehensive reranking procedure mightallow further performance gains.4.5 Qualitative Error AnalysisAs a further analysis, we extracted the worst scor-ing fifty sentences by F1-measure from the parsedtest set (+ Gold tags, - Edge labels), and comparedthem against the gold standard trees, noting thecause of the error.
We analyze the parses beforereranking, to see how frequently the paired punc-tuation problem described above severely affects aparse.
The major mistakes made by the parser aresummarized in Table 5.Problem Freq.Misidentification of Parentheticals 19Coordination problems 13Too few SIMPX 10Paired punctuation problem 9Other clause boundary errors 7Other 6Too many SIMPX 3Clause type misidentification 2MF/NF boundary 2LV 2VF/MF boundary 2Table 5: Types and frequency of parser errors inthe fifty worst scoring parses by F1-measure, us-ing parameters (+ Gold tags, - Edge labels).Misidentification of Parentheticals Parentheti-cal constructions do not have any dependencies onthe rest of the sentence, and exist as a mostly syn-tactically independent clause inside another sen-tence.
They can occur at the beginning, end, orin the middle of sentences, and are often set offorthographically by punctuation.
The parser hasproblems identifying parenthetical constructions,often positing a parenthetical construction whenthat constituent is actually attached to a topolog-ical field in a neighbouring clause.
The follow-ing example shows one such misidentification inbracket notation.
Clause internal topological fieldsare omitted for clarity.
(2) (a) Tu?Ba-D/Z: (SIMPX Weder das Ausma?
derScho?nheit noch der fru?here oder spa?tereZeitpunkt der Geburt macht einen der Zwillingefu?r eine Mutter mehr oder weniger echt /authentisch / u?berlegen).
(b) Parser: (SIMPX Weder das Ausma?
derScho?nheit noch der fru?here oder spa?tereZeitpunkt der Geburt macht einen der Zwillingefu?r eine Mutter mehr oder weniger echt)(PARENTHETICAL / authentisch /u?berlegen.
)(c) Translation: ?Neither the degree of beauty northe earlier or later time of birth makes one of thetwins any more or less real/authentic/superior toa mother.
?We hypothesized earlier that lexicalization isunlikely to give us much improvement in perfor-mance, because topological fields work on a do-main that is higher than that of lexical dependen-cies such as subcategorization frames.
However,given the locally independent nature of legitimateparentheticals, a limited form of lexicalization orsome other form of stronger contextual informa-tion might be needed to improve identification per-formance.Coordination Problems The second most com-mon type of error involves field and clause coordi-nations.
This category includes missing or incor-rect FKOORD fields, and conjunctions of clausesthat are misidentified.
In the following example,the conjoined MFs and following NF in the cor-rect parse tree are identified as a single long MF.
(3) (a) Tu?Ba-D/Z: Auf dem europa?ischen Kontinentaber hat (FKOORD (MF kein Land und keineMacht ein derartiges Interesse an gutenBeziehungen zu Ru?land) und (MF auch keinLand solche Erfahrungen im Umgang mitRu?land)) (NF wie Deutschland).
(b) Parser: Auf dem europa?ischen Kontinent aberhat (MF kein Land und keine Macht einderartiges Interesse an guten Beziehungen zuRu?land und auch kein Land solcheErfahrungen im Umgang mit Ru?land wieDeutschland).
(c) Translation: ?On the European continent,however, no land and no power has such aninterest in good relations with Russia (asGermany), and also no land (has) suchexperience in dealing with Russia as Germany.
?Other Clause Errors Other clause-level errorsinclude the parser predicting too few or too manyclauses, or misidentifying the clause type.
Clausesare sometimes confused with NFs, and there is onecase of a relative clause being misidentified as a70main clause with an intransitive verb, as the finiteverb appears at the end of the clause in both cases.Some clause errors are tied to incorrect treatmentof elliptical constructions, in which an elementthat is inferable from context is missing.Paired Punctuation Problems with pairedpunctuation are the fourth most common type oferror.
Punctuation is often a marker of clauseor phrase boundaries.
Thus, predicting pairedpunctuation incorrectly can lead to incorrectparses, as in the following example.
(4) (a) ?
Auch (SIMPX wenn der Krieg heute einMobilisierungsfaktor ist) ?
, so Pau , ?
(SIMPXdie Leute sehen , da?
man fu?r die Arbeit wiederauf die Stra?e gehen mu?)
.
?
(b) Parser: (SIMPX ?
(LV Auch (SIMPX wenn derKrieg heute ein Mobilisierungsfaktor ist)) ?
, soPau , ?
(SIMPX die Leute sehen , da?
man fu?rdie Arbeit wieder auf die Stra?e gehen mu?))
.
?
(c) Translation: ?Even if the war is a factor formobilization,?
said Pau, ?the people see, thatone must go to the street for employment again.
?Here, the parser predicts a spurious SIMPXclause spanning the text of the entire sentence, butthis causes the second pair of quotation marks tobe parsed as belonging to two different clauses.The parser also predicts an incorrect LV field.
Us-ing the paired punctuation constraint, our rerank-ing procedure was able to correct these errors.Surprisingly, there are cases in which pairedpunctuation does not belong inside the sameclause in the gold parses.
These cases are ei-ther extended quotations, in which each of thequotation mark pair occurs in a different sen-tence altogether, or cases where the second of thequotation mark pair must be positioned outsideof other sentence-final punctuation due to ortho-graphic conventions.
Sentence-final punctuationis typically placed outside a clause in this versionof Tu?Ba-D/Z.Other Issues Other incorrect parses generatedby the parser include problems with the infre-quently occurring topological fields like LV andDM, inability to determine the boundary betweenMF and NF in clauses without a VC field sepa-rating the two, and misidentifying appositive con-structions.
Another issue is that although theparser output may disagree with the gold stan-dard tree in Tu?Ba-D/Z, the parser output may bea well-formed topological field parse for the samesentence with a different interpretation, for ex-ample because of attachment ambiguity.
Each ofthe authors independently checked the fifty worst-scoring parses, and determined whether each parseproduced by the Berkeley parser could be a well-formed topological parse.
Where there was dis-agreement, we discussed our judgments until wecame to a consensus.
Of the fifty parses, we de-termined that nine, or 18%, could be legitimateparses.
Another five, or 10%, differ from the goldstandard parse only in the placement of punctua-tion.
Thus, the F1-measures we presented abovemay be underestimating the parser?s performance.5 Conclusion and Future WorkIn this paper, we examined applying the latent-variable Berkeley parser to the task of topologicalfield parsing of German, which aims to identify thehigh-level surface structure of sentences.
Withoutany language or model-dependent adaptation, weobtained results which compare favourably to pre-vious work in topological field parsing.
We furtherexamined the results of doing a simple rerankingprocess, constraining the output parse to put pairedpunctuation in the same clause.
This rerankingwas found to result in a minor performance gain.Overall, the parser performs extremely well inidentifying the traditional left and right bracketsof the topological field model; that is, the fieldsC, LK, and VC.
The parser achieves basically per-fect results on these fields in the Tu?Ba-D/Z corpus,with F1-measure scores for each at over 98.5%.These scores are higher than previous work in thesimpler task of topological field chunking.
The fo-cus of future research should thus be on correctlyidentifying the infrequently occuring fields andconstructions, with parenthetical constructions be-ing a particular concern.
Possible avenues of fu-ture research include doing a more comprehensivediscriminative reranking of the parser output.
In-corporating more contextual information might behelpful to identify discourse-related constructionssuch as parentheses, and the DM and LV topolog-ical fields.AcknowledgementsWe are grateful to Markus Becker, Anette Frank,Sandra Kuebler, and Slav Petrov for their invalu-able help in gathering the resources necessary forour experiments.
This work is supported in partby the Natural Sciences and Engineering ResearchCouncil of Canada.71ReferencesM.
Becker and A. Frank.
2002.
A stochastic topo-logical parser for German.
In Proceedings of the19th International Conference on ComputationalLinguistics, pages 71?77.S.
Brants, S. Dipper, S. Hansen, W. Lezius, andG.
Smith.
2002.
The TIGER Treebank.
In Proceed-ings of the Workshop on Treebanks and LinguisticTheories, pages 24?41.U.
Callmeier.
2000.
PET?a platform for experimen-tation with efficient HPSG processing techniques.Natural Language Engineering, 6(01):99?107.A.
Dubey and F. Keller.
2003.
Probabilistic parsingfor German using sister-head dependencies.
In Pro-ceedings of the 41st Annual Meeting of the Associa-tion for Computational Linguistics, pages 96?103.K.A.
Foth, M. Daum, and W. Menzel.
2004.
Abroad-coverage parser for German based on defea-sible constraints.
Constraint Solving and LanguageProcessing.A.
Frank, M. Becker, B. Crysmann, B. Kiefer, andU.
Schaefer.
2003.
Integrated shallow and deepparsing: TopP meets HPSG.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics, pages 104?111.W.
Frey.
2004.
Notes on the syntax and the pragmaticsof German Left Dislocation.
In H. Lohnstein andS.
Trissler, editors, The Syntax and Semantics of theLeft Periphery, pages 203?233.
Mouton de Gruyter,Berlin.J.
Hockenmaier.
2006.
Creating a CCGbank and aWide-Coverage CCG Lexicon for German.
In Pro-ceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguis-tics, pages 505?512.T.N.
Ho?hle.
1983.
Topologische Felder.
Ph.D. thesis,Ko?ln.S.
Ku?bler, E.W.
Hinrichs, and W. Maier.
2006.
Is it re-ally that difficult to parse German?
In Proceedingsof EMNLP.M.
Liepert.
2003.
Topological Fields Chunking forGerman with SVM?s: Optimizing SVM-parameterswith GA?s.
In Proceedings of the International Con-ference on Recent Advances in Natural LanguageProcessing (RANLP), Bulgaria.G.
Neumann, C. Braun, and J. Piskorski.
2000.
ADivide-and-Conquer Strategy for Shallow Parsingof German Free Texts.
In Proceedings of the sixthconference on Applied natural language processing,pages 239?246.
Morgan Kaufmann Publishers Inc.San Francisco, CA, USA.S.
Petrov and D. Klein.
2008.
Parsing German withLatent Variable Grammars.
In Proceedings of theACL-08: HLT Workshop on Parsing German (PaGe-08), pages 33?39.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable treeannotation.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the Association for Compu-tational Linguistics, pages 433?440, Sydney, Aus-tralia, July.
Association for Computational Linguis-tics.C.
Rohrer and M. Forst.
2006.
Improving coverageand parsing quality of a large-scale LFG for Ger-man.
In Proceedings of the Language Resourcesand Evaluation Conference (LREC-2006), Genoa,Italy.W.
Skut, T. Brants, B. Krenn, and H. Uszkoreit.1998.
A Linguistically Interpreted Corpus of Ger-man Newspaper Text.
Proceedings of the ESSLLIWorkshop on Recent Advances in Corpus Annota-tion.H.
Telljohann, E. Hinrichs, and S. Kubler.
2004.The Tu?Ba-D/Z treebank: Annotating German with acontext-free backbone.
In Proceedings of the FourthInternational Conference on Language Resourcesand Evaluation (LREC 2004), pages 2229?2235.H.
Telljohann, E.W.
Hinrichs, S. Kubler, and H. Zins-meister.
2006.
Stylebook for the Tubingen Tree-bank of Written German (Tu?Ba-D/Z).
Seminar furSprachwissenschaft, Universitat Tubingen, Tubin-gen, Germany.T.
Ule.
2003.
Directed Treebank Refinement for PCFGParsing.
In Proceedings of Workshop on Treebanksand Linguistic Theories (TLT) 2003, pages 177?188.J.
Veenstra, F.H.
Mu?ller, and T. Ule.
2002.
Topolog-ical field chunking for German.
In Proceedings ofthe Sixth Conference on Natural Language Learn-ing, pages 56?62.72
