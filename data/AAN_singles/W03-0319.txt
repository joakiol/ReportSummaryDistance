An LSA Implementation AgainstParallel Texts in French and EnglishKatri A. ClodfelderIndiana Universitykclodfel@indiana.eduAbstractThis paper presents the results of applyingthe Latent Semantic Analysis (LSA)methodology to a small collection ofparallel texts in French and English.
Thegoal of the analysis was to determine  whatthe methodology might  reveal regardingthe difficulty level of either the machine-translation (MT) task or the text-alignment(TA) task.In a perfectly parallel corpus where thetexts are exactly aligned, it is expected thatthe word distributions between the twolanguages be perfectly symmetrical.Where they are symmetrical, the difficultylevel of the machine-translation or the text-alignment task should be low.
The resultsof this analysis show that even in aperfectly aligned corpus, the worddistributions between the two languagesdeviate and because they do, LSA maycontribute much to our understanding ofthe difficulty of the MT and TA tasks.1.
CreditsThis paper discusses an implementation of theLatent Semantic Analysis (LSA) methodology againsta small collection of perfectly parallel texts in Frenchand English1.
The texts were made available by theHLT-NAACL and are taken from daily House journalsof the Canadian Parliament.
They were edited byUlrich Germann.
The LSA procedures wereimplemented in R, a system for statistical computationand graphics, and were written by John C. Paolillo atIndiana University, Bloomington.2.
IntroductionLSA is an analytical methodology that usesmathematical procedures and vector space modeling1 At present, the collection of texts used consists of 30English and 30 French documents, all perfectly mated.Cognates were not distinguished between languages, e.g.,revolution is counted as both a French and an English term.techniques to generate an abstract, numericalrepresentation of the relationships among words anddocuments in a collection of texts (the corpus).
In thisanalysis, the methodology is used to identify thesymmetry that exists among the pattern ofrelationships and associations in the parallel texts.Where texts are perfectly aligned, it is expected thatfor every occurrence of a word in one language, anexact correspondent exists in the other language.However, the analysis shows that even in a perfectlyaligned corpus, the word distributions between the twolanguages deviate and a one-to-one association doesnot exist.An example of how word symmetry breaks downin parallel texts can be seen in two "sets" of paralleldocuments, F1-E1 and F2-E2.
In these paireddocuments, the cross-language term correspondencebetween the French term "je" and the English term "I"in the two sets shows that in the first pair, "je" occurs42 times and "I" occurs only 37 times.
In the secondpair, "je" occurs 59 times in the French document and62 times in the English document.
Such differences inword usage patterns between corresponding terms arevery common and create difficulties for the MT or TAtasks.Because of the way LSA represents word-usageassociations and patterns among documents and terms,it may have much to offer in understanding thedifficulty levels of these tasks.
This analysis showsthat, in spite of usage differences resulting in non-symmetrical cross-language word distributionsbetween the corresponding terms of any givenlanguage pair, the LSA methodology is capable ofidentifying the appropriate usage pattern for each ofthe terms, within its own language.
This  paperpresents a first look at the alignment patterns found ina parallel corpus and how LSA may offer someinsights into the MA and TA tasks.3.
LSAThe LSA methodology begins with the term-by-document matrix, an n x m  matrix where each value inthe matrix is the frequency of the nth word in the mthdocument.
A weighting procedure is applied thatweights each of the term frequencies (TF) by theinverse document frequency (IDF)2 (Salton, G. et al1968).
A very powerful mathematical procedure,known as singular value decomposition (SVD), is thenperformed against the transformed matrix.
SVDpermits the reduction of any n x m matrix to a set ofthree matrices, such that M = U?VT, whereU = ( m x m matrix of left singular vectors )?
= ( n x m diagonal matrix containing thesingular values of M)VT = ( n x n transposed row matrix of theright singular vectors ).While the SVD solution of any given matrix can re-create the original matrix, exactly, its primary valuelies in  its capacity to infer what the pattern ofrelationships and associations is for the words in thedocuments, if  all the linguistic data in the corpus isrepresented on a smaller number of dimensions thanthat of the original matrix (Landauer et al1998).4.
InterpretationAlthough it is useful to think of the "dimensions"as representing the document vectors, it should beemphasized that a "dimension" is a more abstractnotion related to the contribution that a givendocument vector makes in explaining the relationshipsand associations of the linguistic data contained in thecorpus.
In this section, an example of how the SVDprocedure depicts the word and documentrelationships on the different dimensions is discussed.In Figure 1, the location of the documents isshown on Dimensions 1 and 2.
Notice that onDimension 2 (the vertical axis), all of the Englishdocuments (En) lie above zero and all of the Frenchdocuments (Fn) lie below.
Moreover, thecorresponding French and English document pairs arealmost perfectly aligned across the horizontal axis.
Asan abstract notion, Dimension 2 clearly represents thetwo languages.On Dimension 1, the documents occur from left toright, in the order of smallest document to largestdocument, making this dimension representative ofdocument size.
No attention was paid to ordering thedocuments according to size when the data was inputinto the model.
The LSA model is able to identify thisrelationship, automatically, and represent it, as shownhere in Figure 1.2 The IDF of term, t, is the ratio of the total number ofdocuments to the total number of documents in which thegiven term, t, occurs, e.g., if term, t, occurred in 6 of the 60documents in the Corpus, its IDF ratio is 60/6 (or 10).Document Plots on Dimensions 1 and 2Figure 1Figure 2 shows the word relationships onDimensions 1 and 2.
The English words arerepresented by "x" and the French words by "+".Word Plots on Dimensions 1 and 2Figure 2Where the two symbols cross, the data point representsa cognate.3  In the same way that the documents weresplit with the English documents occuring above zero3 The term cognate is used to include words such as "chose",which means "thing" in French and is the past tense of "tochoose" in English; "pays", which is "country" in French andpresent tense of "to pay" in English; and so on.and the French documents below in Figure 1, so arethe English and French words split in Figure 2.
Asshown, the English words are located above zero onthe vertical axis and the French words are locatedbelow.
Whether a cognate is represented as an Englishterm and appears above zero or as a French termappearing below, is entirely dependent on which set oflanguage documents "drives" the association for thecognate.Figure 2 shows that, in spite of the very highdegree of symmetry between the document-pairs ofthe French and English texts (Figure 1), the cross-language patterns of association among the words inthe documents are not completely symmetrical.
Ifthey were, there would be a corresponding "x" signabove  the horizontal axis in exactly the same positionas every "+" sign below the axis.
From an MT or TAperspective, the greater the degree of cross-languagesymmetry among words in the documents, the easierthe task of selecting the appropriate target term.
Whencross-language symmetry is low, however, the task offinding the appropriate target term is more difficult.5.
Symmetry of Query ResultsFor the most part, single-term queries accuratelyidentified the most relevant, same-languagedocuments and they did so in spite of non-symmetrical, language-specific, usage-associations ofthe query terms.
For example, in Table 1, the queryusing the English term "aboriginals" returned E22 asits most relevant document; however, the query usingthe corresponding French term "autochtones" returnedF09 as its most relevant document.Query EABORIGINALSE22E09E17E11E27E19E12E07E24E29Query FAUTOCHTONESF09F17F07F06F27F19F12F11F04F23Aboriginal-Autochtones Query ResultsTable 1At first glance, these query results would seem to beundesirable.
However, they are perfectly consistentwith the language-specific usage patterns of these twoterms.In French, because of number agreement betweenadjectives and nouns, the plural form of "autochtone"is used quite frequently in comparison to the pluralform of its English counterpart "aboriginal", wherenumber agreement is not required.
For example, theFrench usage of "les  (peuples) autochtones" is oftenrealized as "the aboriginal people(s)" in thecorresponding English document.
The impact of thisnon-symmetrical usage pattern of correspondinglanguage terms is seen in the query results.
While themost relevant French document, F09, contains 50occurrences of the plural "autochtones", itscorresponding English document (returned as secondrelevant in Query E) contains two occurrences of theplural "aboriginals" and 49 occurrences of the singular"aboriginal".Continuing on with this example, Query Eidentified the English document, E22, as the mostrelevant to the query term.
E22 contains nineoccurrences of the plural "aboriginals" and nooccurrences of the singular "aboriginal".
Thus, theresults of Queries E and F shown in Table 1demonstrate that not only is the LSA methodologysensitive to the language-specific word distributions ofcross-language word pairs, it is also capable ofdistinguishing those distributional variations in orderto identify the most relevant documents for thelanguage of the query term,  accordingly.
In otherwords, given the dissimilarity in the distributions ofthe plural forms of each term in the cross-languageword pair, the LSA methodology behavedappropriately for each of the queries.The order of the documents returned as relevant tothe queries in Table 1 is important from an MT andTA perspective also because it shows that LSA hassome capability to "align" similar, but not exact,terms.
For example, in Query E, the first threedocuments all contain the exact query term,"aboriginals".
The next five documents contain onlythe singular form of the query term.
The remainingtwo documents do not contain either form of the queryterm.
In other words, after the documents thatcontained the exact query term, the LSA methodologychose as more relevant, documents containing thesingular form of the query term over documents thatcontained no form of the query term.
If the LSAmethodology were only identifying relevantdocuments on the basis of finding terms with an exactmatch to the query term, it would have no preferencefor choosing documents containing the singular formof the query term over documents that contained noform of the query term.6.
ConclusionThis paper presented a brief discussion on thepossibility that the LSA methodology may havesomething to contribute with respect to identifying thedifficulty level of the MT and TA tasks.
In particular,it was shown that LSA can represent the symmetricaland non-symmetrical relationships that exist amongthe terms in cross-language document pairs.
It wasalso shown that LSA has some capability to "align"similar terms in order to identify relevant documentsin response to a query.Such positive results using the large, non-homegenous documents that were used in this analysisare very promising and suggest that further research inthis area is needed.
Additional work is planned thatwill separate the documents into smaller, syntacticalunits that will be analyzed using a very similarapproach.
However, the words and "documents"represented in the semantic space that is generated bythese very small syntactic units will have verydifferent  associations and relationships than they doas part of the larger, non-homegenous texts.It is believed that by restricting the input texts tovery small syntactic units, the LSA methodology willbe able to make the proper "alignment" associationsbetween the cross-language word pairs and, as a result,provide some information regarding the types of wordpairs that are most difficult to align by an MT or TAsystem.Finally, the small number of documents used inthis analysis4 raises questions about the optimalnumber of input texts that are needed to obtain validresults using the LSA methodology.
It may be that amuch smaller number of input texts is needed thanformerly believed, in order to "train" an LSA-basedsystem to correlate the appropriate cross-languageword pairs.4 30 mated pairs (or 60 documents in total)ReferencesLaundauer, Foltz, Laham.
1998.
"An Introduction toLatent Semantic Analysis."
Discourse Processses, 25,259-284.
1998.Rehder, Bob, Michael L. Littman, Susan Dumais,Thomas K. Landauer.
1997.
"Automatic 3-LanguageCross-Language Information Retrieval with LatentSemantic Indexing."
TREC-6, 233-239.Salton, G., and Lesk, M. E. 1968.
"Computerevaluation of indexing and text processing."
Journalof the Association for Computing Machinery 15.1.
