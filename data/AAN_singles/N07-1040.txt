Proceedings of NAACL HLT 2007, pages 316?323,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsWhose idea was this, and why does it matter?Attributing scientific work to citationsAdvaith Siddharthan & Simone TeufelNatural Language and Information Processing GroupUniversity of Cambridge Computer Laboratory{as372,sht25}@cl.cam.ac.ukAbstractScientific papers revolve around cita-tions, and for many discourse leveltasks one needs to know whose workis being talked about at any point inthe discourse.
In this paper, we in-troduce the scientific attribution task,which links different linguistic expres-sions to citations.
We discuss thesuitability of different evaluation met-rics and evaluate our classification ap-proach to deciding attribution both in-trinsically and in an extrinsic evalua-tion where information about scientificattribution is shown to improve per-formance on Argumentative Zoning, arhetorical classification task.1 IntroductionIn the recent past, there has been a focus oninformation management from scientific litera-ture.
In the genetics domain, for instance, in-formation extraction of genes and gene?proteininteractions helps geneticists scan large amountsof information (e.g., as explored in the TRECGenomics track (Hersh et al, 2004)).
Elsewhere,citation indexes (Garfield, 1979) provide biblio-metric data about the frequency with which par-ticular papers are cited.
The success of citationindexers such as CiteSeer (Giles et al, 1998) andGoogle Scholar relies on the robust detectionof formal citations in arbitrary text.
In bibli-ographic information retrieval, anchor text, i.e.,the context of a citation can be used to charac-terise (index) the cited paper using terms out-side of that paper (Bradshaw, 2003); O?Connor(1982) presents an approach for identifying thearea around citations where the text focuses onthat citation.
And automatic citation classifi-cation (Nanba and Okumura, 1999; Teufel etal., 2006) determines the function that a cita-tion plays in the discourse.For such information access and retrieval pur-poses, the relevance of a citation within a paperis often crucial.
One can estimate how impor-tant a citation is by simply counting how oftenit occurs in the paper.
But as Kim and Webber(2006) argue, this ignores many expressions intext which refer to the cited author?s work butwhich are not as easy to recognise as citations.They address the resolution of instances of thethird person personal pronoun ?they?
in astron-omy papers: it can either refer to a citation or tosome entities that are part of research within thepaper (e.g., planets or galaxies).
Several appli-cations should profit in principle from detectingconnections between referring expressions andcitations.
For instance, in citation function clas-sification, the task is to find out if a citation isdescribed as flawed or as useful.
Consider:Most computational models of discourseare based primarily on an analysis ofthe intentions of the speakers [Cohen andPerrault, 1979][Allen and Perrault,1980][Grosz and Sidner, 1986]WEAK.The speaker will form intentions based onhis goals and then act on these intentions, pro-ducing utterances.
The hearer will then re-construct a model of the speaker?s intentionsupon hearing the utterance.
This approachhas many strong points, but does notprovide a very satisfactory account ofthe adherence to discourse conventions in di-alogue.The three citations above are described as flawed(detectable by ?does not provide a very satis-factory account?
), and thus receive the labelWeak.
However, in order to detect this, onemust first realise that ?this approach?
refers to316the three cited papers.
A contrasting hypoth-esis could be that the citations are used (thusdeserving the label Use; the cue phrase ?basedon?
might make us think so (as in the context?our work is based on?).
This, however, can beruled out if we know that ?the speaker?
is notreferring to some aspect of the current paper.2 The scientific attribution taskWe define an attribution task where possible ref-erents are members of the reference list (i.e.,each cited paper), the Current-Paper, anda back-off category No-Specific-Paper formarkables that are not attributable to any spe-cific paper(s).
Our markables are as follows:all definite descriptions (e.g., ?the hearer?, andincluding demonstrative noun phrases such as?these intentions?
), all ?work?
nouns1, and allpronouns (possessive, personal and demonstra-tive); c.f., underlined strings in the above exam-ple.
Our notion of attribution link encompassestwo relations:1.
Anaphoric: The referents are entire re-search papers, or the papers?
authors2.
Subpart: The referents are some compo-nent of an approach/argument/claim in theresearch paperThere are two tasks: attributing a linguisticexpression to the right paper (including the cur-rent paper) ?
a task we call scientific attribution?
and deciding whether or not the expression isanaphoric to the entirety of the paper, or just tosome subpart of it.Kim and Webber (2006) solve the problem ofdistinguishing between these relations for onecase.
They decide whether the pronoun ?they?anaphorically refers to the authors of a cited pa-per, or whether it refers to some entity that isdiscussed in (a subpart of) a paper (e.g., ?galax-ies?).
In this paper, we tackle the other problemof scientific attribution.We do not distinguish between the two typesof links stated above, but only identify which ci-tation(s) a linguistic expression is attributable1We use a list of around 40 research methodology re-lated nouns from Teufel and Moens (2002), such as e.g.,?study, account, investigation, result?
etc.
These arenouns we are particularly interested in.to.
For tasks of interest to us, it is not enoughto only consider anaphoric references to entirepapers; authors often make statements compar-ing/using/criticising aspects or subparts of citedwork.
We therefore consider a far wider rangeof markables than Kim and Webber?s single pro-noun ?they?.Our attribution task differs from the tradi-tional anaphora resolution task in that we havea fixed list of possible referents (the referencelist items, Current-Paper or No-Specific-Paper) that are known upfront.
Also, we donot form co-reference chains; we attribute a re-ferring expression directly to one or more ref-erents.
Ours is therefore a multi-label classi-fication task, where the citations, Current-Paper and No-Specific-Paper are the labels,and where one or more labels are assigned toeach markable.We evaluate intrinsically by comparing tohuman-annotated attribution, and extrinsicallyby showing that automatically acquired knowl-edge about scientific attribution improves per-formance on a discourse classification task?Argumentative Zoning (Teufel and Moens,2002), where sentences are labelled as oneof {Own, Other, Background, Textual,Aim, Basis, Contrast} according to their rolein the author?s argument.We describe our data in ?3 and methodologyin ?4, discuss evaluation metrics in ?5, and eval-uate intrinsically in ?6 and extrinsically in ?7.3 DataWe used data from the CmpLg (Computationand Language archive; 320 conference articlesin computational linguistics).
The articles arein XML format.We produced an annotated corpus (10 arti-cles, 4290 data points, i.e., markables) based onwritten guidelines.
The task was found to bequite intuitive by our annotators, and this wasreflected in high agreement - Krippendorff?s al-pha2 of more than 0.8 (2 annotators, 3 papers,1429 data points) on the attribution task.
Thedistribution of classes was, as expected, quiteskewed: 69% of markables are attributable to2see description in ?5.2317Current-Paper, 7% to no specific paper and24% to specific references (on average, 1.7 perreference).
Details about the annotation pro-cess and human agreement figures can be foundin Siddharthan and Teufel (2007).4 Machine Learning ApproachWe frame the attribution problem as a classi-fication task: Given a markable (the definitedescription/pronoun/work noun under consid-eration), a binary yes/no decision is made foreach cited paper, and a binary yes/no decisionis made for whether the markable is attributableto the current paper.
The list of labels for themarkable is compiled by including all the cita-tions for which the machine learner returns yes,and Current-Paper if the learner returns yes.If the list is empty (learner returns no for every-thing), the label is No-Specific-Paper.Since the model for whether a markable is at-tributable to the current work is likely to bedifferent from the model for whether it is at-tributable to a citation, we trained separatemodels for the two problems.4.1 Deciding attribution to a citationFor each data point to be classified (called NPbelow), we create a machine learning instancefor each reference list item by automaticallycomputing the following features from POS-tagged text:1.
Properties of data point (NP) and the closest Cita-tion instance (CIT) of the reference list item:(a) Type of NP (Definite Description/WorkNoun/Pronoun)(b) CIT is a self Citation or not(c) CIT is syntactic (in running text) or paren-thetical(d) Is CIT Hobbs?
prediction (searching left?rightstarting from current sentence and then con-sidering previous sentences, is CIT the firstcitation or reference to current work found)?2.
Distance measures:(a) Dist.
between NP and CIT measured in words(b) Dist.
between NP and CIT measured in sen-tences(c) Dist.
between NP and CIT measured in para-graphs(d) Is CIT after NP in the discourse (cataphor)?
(e) Distance between CIT and the closest firstperson pronoun or ?this paper?
in words3.
Contextual:(a) Rank of CIT (how many other reference listitems are closer)(b) Number of times CIT is cited in the paragraph(c) Number of times CIT is cited in the wholepaper(d) Current Section heading (this feature has 5values: Introduction, Methods, Results, Con-clusions, Unrecognised)4.
Agreement:(a) Agreement Number (He/She & single authornon-self citation)(b) Agreement Person (First & Current/Self Ci-tation, Third and Not-Current)We have a chicken and egg problem with cal-culating the distance of a reference to currentwork in 2(e).
Unlike citations, these are not un-ambiguously marked in the text.
We calculatedistance from the closest first person pronoun(even though these could possibly refer to a selfcitation, rather than current work) or the phrase?this paper?, which can again refer to other cita-tions but predominantly refers to current work.4.2 Deciding attribution to current workWe use the same features for the second clas-sifier that makes the decision on whether thedata point refers to Current-Paper, with thefollowing changes: Features 1(b,c) are removedas they are meaningless; 1(d) checks Hobbs?prediction for a first person pronoun/?this pa-per?, rather than CIT; in 2(a?d), the distance ismeasured between the closest first person pro-noun/?this paper?
and the markable, ratherthan a citation and the markable; similarly, in3(b,c) we count instances of first person pro-noun/?this paper?
; for 2(e), we now calculatethe distance of the closest citation instance.
Inshort, the same features are used, but currentwork and citations are swapped.5 Evaluation MetricsWe consider two evaluation metrics.
The firstis the scoring system used for the co-referencetask in the Message Understanding ConferencesMUC-6 and MUC-7.
The second is Krippen-dorff?s ?.
We briefly discuss both below.5.1 The MUC-6/MUC-7 MetricThe MUC-6/MUC-7 Co-reference evaluationmetric (Vilain et al, 1995) works by compar-ing co-reference classes across two annotated318files.
Calling one annotation the ?model?
andthe other the ?system?, for each co-referenceclass S in the model, c(S) is the minimal num-ber of co-reference links needed to generate theclass (this is one less than the cardinality of theclass; c(S) = |S| ?
1).
m(S) is the number of?missing?
links in the system annotation rela-tive to the co-reference class as marked up inthe model.
In other words, this is the minimumnumber of co-reference links that need to beadded to the system annotation to fully gener-ate the co-reference class S in the model.
Recallerror is then RE(S) = m(S)/c(S) and Recall isR(S) = 1 ?
RE = c(S)?m(S)c(S) .
Recall for the en-tire file (or set of files) is calculated by summingover all co-reference classes in the model:R =?i c(Si) ?
m(Si)?i c(Si)Precision (P ) is calculated by swapping themodel and system and the f-measure (F =2R ?
P/(R + P )) is symmetric with respect toboth annotations.5.2 Krippendorff?s AlphaWe follow Passonneau (2004) and Poesio andArtstein (2005) in using Krippendorff (1980)?s?
metric to compute agreement between anno-tations.
The advantage of ?
over the more com-monly used ?
metric is that ?
allows for par-tial agreement when annotators assign multiplelabels to the same markable; in this case calcu-lating agreement on a markable requires a moregraded agreement calculation than the ?1 if setsare identical and 0 otherwise?
provided for by?.
Krippendorff?s ?
measures disagreement, andallows for the use of distance metrics to calculatepartial disagreement.
Following Passonneau, wepresent results using four distance metrics:1.
(N)ominal: Two sets have distance N = 0if they are identical and N = 1 if they arenot.
?
calculated using the nominal dis-tance metric is equivalent to ?.2.
(J)accard: Two sets A and B have dis-tance J = 1 ?
|A ?
B|/|A ?
B|.
In otherwords, the distance between two sets islarger, the smaller their intersection and thelarger their union.3.
(D)ice: Two sets A and B have distanceD = 1 ?
2 ?
|A ?
B|/(|A| + |B|).
In prac-tice, the Dice distance metric behaves simi-larly to the Jaccard metric, but tends to besmaller, resulting in slightly higher ?.4.
(M)ASI: This is the Jaccard distance Jweighted by a monotonicity distance mwhere, m = 0 if two sets are identical;m = 0.33 if one is a subset of the other;m = 0.67 if the intersection and the twoset differences are all non-null; m = 1 if thetwo sets are disjoint.
Formally, the MASImetric is M = m ?
J .As an example, consider two sets {a, b, c} and{b, c, d}.
The distances between these sets areN = 1, J = 1?2/4 = 0.5, D = 1?2?2/(3+3) =0.33 and M = 0.67 ?
0.5 = 0.33.Krippendorff?s ?
is defined as ?
= 1?Do/De,where Do is the observed disagreement and Deis the disagreement that is expected by chance:Do =1c(c ?
1)?j?k?k?njknjk?dkk?De =1c(c ?
1)?k?k?nknk?dkk?In the above formulae, c is the number ofcoders, njk is the number of times item j isclassed as category k, nk is the number of timesany item is classed as category k and dkk?
is thedistance between categories k and k ?.Like ?, Krippendorff?s ?
is 1 when there isperfect agreement, 0 when the observed agree-ment is only what was expected by chance, neg-ative when observed agreement is less than ex-pected by chance and positive when observedagreement is greater than expected by chance.6 Intrinsic Evaluation ResultsWe ran a machine learning experiment us-ing 10-fold cross-validation and the memory-based learner IBk3 (with k=6), using the Wekatoolkit (Witten and Frank, 2000).
The perfor-mance is shown in Tables 1 and 2.
To positionthese results we compare them with three base-line lower bounds and the human performanceupper bound in Table 3.
We use three baselines:3Memory based learning gave better results on thistask than other learners (NB, HNB, IBk, J48, cf.
?
7.3.319Paper Items ?-N ?-J ?-D ?-M %A?0003055 446 .601 .606 .607 .610 85%0005006 446 .670 .704 .711 .715 81%0005015 462 .679 .696 .701 .706 81%0005025 277 .707 .707 .707 .707 86%0006011 393 .766 .771 .772 .775 88%0006038 578 .551 .568 .573 .578 79%0007035 393 .570 .590 .600 .609 90%0008026 449 .700 .700 .700 .700 87%0001001 420 .564 .565 .569 .571 88%0001020 429 .730 .778 .790 .801 88%AVG.
429 .654 .669 .673 .677 85%?% Agreement, the conservative estimate measuredusing the Nominal metricTable 1: Agreement with Human Gold Standard?
BASEM (Major Class): All data points arelabelled CURRENT-WORK?
BASEP (Previous): Data points are taggedwith the most recent label?
BASEH (Hobbs?
Prediction): Data pointsare tagged with the label found by Hobbs?
(1986) search (Search left to right in eachsentence, starting from current sentence,then considering previous sentences)As Table 3 shows, our machine learning ap-proach performs much better than the base-lines on all the agreement metrics, and is indeedcloser to human performance than to any of thebaselines.
The MUC evaluation appears to pro-duce highly inflated results on our task ?
whenthere is a small set of co-reference classes andone of these classes contains 70% of data points,it takes only a small number of missing links tocorrect annotations.
This results in unreason-ably high values, particularly for the majorityclass baseline of labelling every data point asCurrent-Paper.
We believe that the ?
met-rics provide a much more realistic estimate ofthe difficulty of the task and the relative perfor-mances of different approaches.Table 4 shows the performance of the ma-chine learner for each of the three types of lin-guistic expressions considered.
Pronouns arethe easiest to resolve, with on average 90% re-solved correctly (an agreement with the humangold standard of ?
= .71).
This drops to 85%(?
= .68) for definite descriptions and demon-stratives, and further to 78% (?
= .63) for re-Paper No.
Classes Recall Precision F0003055 14 .934 .886 .9100005006 17 .875 .870 .8720005015 19 .897 .876 .8860005025 16 .903 .874 .8880006011 14 .942 .909 .9250006038 25 .905 .893 .8990007035 18 .957 .926 .9410008026 9 .966 .962 .9640001001 14 .949 .908 .9280001020 18 .924 .926 .925TOTAL 164 .924 .903 .913Table 2: Evaluation using MUC-6/7 softwareAlgo ?-N ?-J ?-D ?-M %Agr?muc-fBaseM .002 .001 .001 .001 69% .934BaseP -.101 -.083 -.081 -.077 19% .894BaseH .387 .397 .399 .407 72% .910IBk .654 .669 .673 .677 85% .913Hum??
.806 .808 .808 .809 91% .965?% Agreement, the conservative estimate measuredusing the Nominal metric?
?Agreement between two human annotators over asubset of the corpus (3 files, 1429 data points)Table 3: Comparison with Baselines and HumanPerformance (Averaged results)maining work nouns (i.e., those not already in adefinite noun phrase).While all the features contributed to the re-ported results, the most important features (interms of information gain) for deciding attribu-tion to a citation were the paragraph level cita-tion count 3(b), the distance features 2(a,b,c,d),the rank 3(a) and the Hobbs?
prediction 1(d).The most important features for deciding attri-bution to the current paper were the distancefeatures 2(a,c,e), the rank 3(a) and the Hobbs?prediction 1(d).7 Extrinsic EvaluationTo demonstrate the use of automatic scientificattribution classification, we studied its util-ity for one well known discourse annotationtask: Argumentative Zoning (Teufel and Moens,2002).
Argumentative Zoning (AZ) is the task ofapplying one of seven discourse level tags (Fig-ure 1) to each sentence in a scientific paper.These categories model several aspects of sci-entific papers: from the distinction of segmentsby who an idea is attributed to (Own ?
Other ?Background), to the judgement of how the au-320Paper Pronouns Definites Work Nouns?M %N ?M %N ?M %N0003055 .746 94% .556 83% .735 87%0005006 .846 91% .703 85% .700 78%0005015 .662 83% .692 79% .787 86%0005025 .804 89% .717 87% .514 78%0006011 .824 91% .807 91% .615 76%0006038 .603 90% .609 81% .430 66%0007035 .577 94% .507 91% .770 87%0008026 .678 88% .726 87% .551 78%0011001 .562 97% .633 87% .377 81%0011020 .792 90% .798 92% .808 89%AVG.
.709 90% .675 85% .629 78%Table 4: Results for different markable typesCategory DescriptionBackground Generally accepted background knowl-edgeOther Specific other workOwn Own work: method, results, futureworkAim Specific research goalTextual Textual section structureContrast Contrast, comparison, weakness ofother solutionBasis Other work provides basis for own workFigure 1: AZ Annotation schemethors relate to other work (Contrast ?
Basis)to the rhetorical status of high-level discoursegoals (statement of Aim; overview of sectionstructure (Textual)).
Some of these categories(Background, Other and Own) occur in zonesthat span many sentences.
Other categories typ-ically occur in short zones, often just a singlesentence (Textual, Aim, Contrast, Basis).In all work to date, classification of sentencesinto one of the AZ categories has been performedon the basis of features extracted from withinthe sentence, and a few contextual features suchas section heading and location in document.Scientific attribution links previously unresolvednoun phrases or pronouns in the sentence to cita-tions.
As this provides the machine learner withmore information, AZ results should improve.7.1 AZ DataThe evaluation corpus used is the one fromTeufel and Moens (2002).
It consists of 80 con-ference papers in computational linguistics, con-taining around 12000 sentences.
Each of theseis manually tagged as one of {OWN, OTH, BKG,BAS, AIM, CTR, TXT}.
The reliability observedis reasonable (Kappa=0.71)).7.2 FeaturesFollowing Teufel and Moens (2002), we used su-pervised ML using features extracted by shallowprocessing (POS tagging and pattern matching):?
Lexical (cue phrase) features consistof three features: the first models occur-rence of about 1700 manually identified sci-entific cue phrases (such as ?in this paper?
).The cue phrases are classified into semanticgroups.
The second models the main verbof the sentence, by lookup in a verb lexiconorganised by 13 main clusters of verb types(e.g.
?change verbs?
), and the third modelsthe likely subject of the sentence, by clas-sifying them either as the authors, or otherresearchers, or none of the above, using anextensive lexicon of regular expressions.?
Content word features model occurrenceand density of content words in the sen-tences, where content words are either de-fined as non-stoplist words in the subsectionheading preceding the sentence, or as wordswith a high TF*IDF score.?
Linguistic features include (complex)tense, voice, and presence of an auxiliary.?
Citation features detect properties of for-mal citations in text, such as the occurrenceof authors?
names in text, the position of acitation in text, and whether the citationis a self citation (i.e., includes any of theauthors of the paper itself).?
Location features: Rhetorical roles areexpected at certain places in the document,for instance, background sentences are morelikely to occur at the beginning of the text,and goal statements often occur after abouta fifth of the paper.
We model this by split-ting the text into ten segments and assign-ing each sentence to the segment it is lo-cated in.
We also use the section headingas a contextual feature.Some categories tend to occur in blocks (e.g.,Own, Other, Background), and the contextin terms of the label of the previous sentencehas good predictive value.
We model this (the321Learner kappa Macro-FNo Attrib With Attrib No WithNB .45 .46 .53 .53HNB .42 .45 .51 .53IBk .34 .36 .39 .39J48 .38 .41 .41 .48Stacking .45 .48 .51 .53Table 5: Improvement on AZ from using auto-matic scientific attribution classification.so-called History feature) by running the clas-sifier twice, and including the prediction for theprevious sentence as a feature the second time.Due to practical considerations, we obtainedour linguistic features using the RASP part ofspeech tagger (Briscoe and Carroll, 1995), whenin previous work we used the LT TTT (Groveret al, 2000).
We would not expect this to in-fluence results much, however.
Another differ-ence is that we use around 1700 additional cuephrases acquired from previous work on anotherdiscourse task4 (Teufel et al, 2006).In addition to these features, we use fourfeatures obtained from the scientific attributiontask described in this paper:Scientific Attribution Features:?
Whether there is any reference to currentwork in the sentence?
Whether there is any reference to any spe-cific citation in the sentence?
Whether there is any reference in the sen-tence to work that is in neither the currentpaper nor any specific citation?
Which of these, if any, is in subject positionOur aim is to explore whether these featuresobtained from the scientific attribution task in-fluence machine learning performance on AZ.7.3 AZ resultsWe ran five different machine learners with andwithout the four scientific attribution features(c.f., ?7.2).
Note that our labelled data for theattribution task does not overlap with the 80 pa-pers in the AZ corpus, and all attribution pre-dictions used in features for this AZ experiment4These cues are acquired manually from files that arenot part of the AZ evaluation corpus.Without Attribution FeaturesAim Ctr Txt Own Bkg Bas OthP .44 .42 .52 .84 .46 .34 .47R .61 .30 .68 .88 .45 .37 .37F .52 .35 .59 .86 .46 .35 .42Correctly Classified Instances 73.0%Kappa statistic 0.45Macro-F 0.51With Attribution FeaturesAim Ctr Txt Own Bkg Bas OthP .57 .42 .57 .84 .44 .40 .55R .61 .27 .66 .90 .47 .43 .42F .59 .33 .61 .87 .46 .41 .47Correctly Classified Instances 74.7%Kappa statistic 0.48Macro-F 0.53Table 6: Best AZ results using Stacked classifier:with and without Attribution Features.are obtained entirely from unseen (and indeedunlabelled) data based on the model learnt on10 papers (c.f., ?6).
The learners we used (withdefault Weka settings) are:?
NB: Naive Bayes learner?
HNB: Hidden Naive Bayes learner?
IBk: Memory based learner?
J48: Decision tree based learner?
STACKING: combining NB and J48 classi-fiers with the stacking methodAs mentioned under History feature above, werun each learner twice, the second time includ-ing the machine learning prediction for the pre-vious sentence (as we found in Teufel and Moens(2002) for NB, we noticed a slight improvementin performance when using the history feature(between .005 and .01 on both ?
and Macro-F for all learners)).
We found an improvementfrom including the four reference features withall the learners, as shown in Table 5.For a more detailed view of where the im-provement comes from, refer to Table 6, whichshows precision, recall and f-measure per cate-gory for our best learner.
The biggest improve-ments from using attribution features are for thecategories Other, Aim and Bas.
The improve-ment in Other was to be be expected, as thiszone is directly related to the attribution classi-fication.
The large improvements in Aim and322Aim Ctr Txt Own Bkg Bas OthP .44 .34 .57 .84 .40 .37 .52R .65 .20 .66 .88 .50 .40 .39F .52 .26 .61 .86 .44 .38 .44Correctly Classified Instances 72.5%Kappa statistic 0.45Macro-F 0.50Table 7: Teufel and Moens (2002)?s best AZ re-sults (Naive Bayes Classifier).Bas is good news, as these are amongst ourmost informative rhetorical categories for down-stream tasks.
Our best results of Kappa=0.48and Macro-F=0.53 are better than the best pre-viously published results on task (Kappa=0.45and Macro-F=0.50 in Teufel and Moens (2002)).Our results improve on the results of Teufel andMoens (2002) (reproduced in Table 7) ?
bothoverall and for each individual category.8 ConclusionsWe have described a new reference task - decid-ing scientific attribution, and demonstrated highhuman agreement (?
> 0.8) on this task.
Ourmachine learning solution using shallow featuresachieves an agreement of ?M = 0.68 with thehuman gold standard, increasing to ?M = 0.71if only pronouns need to be resolved.
We havealso demonstrated that information about scien-tific attribution improves results for a discourseclassification task (Argumentative Zoning).We believe that similar improvements can beachieved on other discourse annotation tasks inthe scientific literature domain.
In particular,we plan to investigate the use of scientific at-tribution information for the citation functionclassification task.AcknowledgementsThis work was funded by the EPSRC projectSciBorg (EP/C010035/1, Extracting the Sciencefrom Scientific Publications).ReferencesS.
Bradshaw.
2003.
Reference directed indexing: Re-deeming relevance for subject search in citationindexes.
In Proc.
of ECDL.T.
Briscoe and J. Carroll.
1995.
Developing andevaluating a probabilistic LR parser of part-of-speech and punctuation labels.
In Proc.
of IWPT-95, Prague / Karlovy Vary, Czech Republic.E.
Garfield.
1979.
Citation Indexing: Its Theory andApplication in Science, Technology and Humani-ties.
J. Wiley, New York, NY.C.
L. Giles, K. Bollacker, and S. Lawrence.
1998.Citeseer: An automatic citation indexing system.In Proc.
of the Third ACM Conference on DigitalLibraries.C.
Grover, C. Matheson, A. Mikheev, and M. Moens.2000.
LT TTT - A flexible tokenisation tool.
InProc.
of LREC-00, Athens, Greece.W.
Hersh, R. Bhuptiraju, L. Ross, P. Johnson, A.Cohen, and D. Kraemer.
2004.
Trec 2004 ge-nomics track overview.
In Proc.
of TREC.J.
Hobbs.
1986.
Resolving Pronoun References.
InReadings in Natural Language, Grosz, B., Sparck-Jones, K. and Webber, B.
(eds.)
Morgan Kauf-man.Y.
Kim and B. Webber.
2006.
Automatic refer-ence resolution in astronomy articles.
In Proc.
of20th International CODATA Conference, Beijing,China.K.
Krippendorff.
1980.
Content Analysis: An in-troduction to its methodology.
Sage Publications,Beverly Hills.H.
Nanba and M. Okumura.
1999.
Towards multi-paper summarization using reference information.In Proc.
of IJCAI-99.J.
O?Connor.
1982.
Citing statements: Computerrecognition and use to improve retrieval.
Informa-tion Processing and Management, 18(3):125?131.R.
Passonneau.
2004.
Computing reliability forcoreference annotation.
In Proc.
of LREC-04, Lis-bon, Portugal.M.
Poesio and R. Artstein.
2005.
Annotating(anaphoric) ambiguity.
In Proc.
of the CorpusLinguistics Conference, Birmingham, UK.A.
Siddharthan and S. Teufel.
2007.
Whoseidea was this?
Deciding attribution in scien-tific literature.
In Proc.
of the 6th DiscourseAnaphora and Anaphor Resolution Colloquium(DAARC?07), Lagos, Portugal.S.
Teufel and M. Moens.
2002.
Summarising sci-entific articles ?
experiments with relevance and rhetorical status.
Computational Linguistics,28(4):409?446.S.
Teufel, A. Siddharthan, and D. Tidhar.
2006.
Au-tomatic classification of citation function.
In Proc.of EMNLP-06, Sydney, Australia.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1995.
A model-theoretic corefer-ence scoring scheme.
In Proc.
of the 6th MessageUnderstanding Conference, San Francisco.I.
Witten and E. Frank.
2000.
Data Mining: Prac-tical Machine Learning Tools and Techniques withJava Implementations.
Morgan Kaufmann.323
