Trainable Sentence Planning for Complex InformationPresentation in Spoken Dialog SystemsAmanda StentStony Brook UniversityStony Brook, NY 11794U.S.A.stent@cs.sunysb.eduRashmi PrasadUniversity of PennsylvaniaPhiladelphia, PA 19104U.S.A.rjprasad@linc.cis.upenn.eduMarilyn WalkerUniversity of SheffieldSheffield S1 4DPU.K.M.A.Walker@sheffield.ac.ukAbstractA challenging problem for spoken dialog sys-tems is the design of utterance generation mod-ules that are fast, flexible and general, yet pro-duce high quality output in particular domains.A promising approach is trainable generation,which uses general-purpose linguistic knowledgeautomatically adapted to the application do-main.
This paper presents a trainable sentenceplanner for the MATCH dialog system.
Weshow that trainable sentence planning can pro-duce output comparable to that of MATCH?stemplate-based generator even for quite com-plex information presentations.1 IntroductionOne very challenging problem for spoken dialogsystems is the design of the utterance genera-tion module.
This challenge arises partly fromthe need for the generator to adapt to manyfeatures of the dialog domain, user population,and dialog context.There are three possible approaches to gener-ating system utterances.
The first is template-based generation, used in most dialog systemstoday.
Template-based generation enables aprogrammer without linguistic training to pro-gram a generator that can efficiently producehigh quality output specific to different dialogsituations.
Its drawbacks include the need to(1) create templates anew by hand for each ap-plication; (2) design and maintain a set of tem-plates that work well together in many dialogcontexts; and (3) repeatedly encode linguisticconstraints such as subject-verb agreement.The second approach is natural language gen-eration (NLG), which divides generation into:(1) text (or content) planning, (2) sentenceplanning, and (3) surface realization.
NLGpromises portability across domains and dialogcontexts by using general rules for each genera-tion module.
However, the quality of the outputfor a particular domain, or a particular dialogcontext, may be inferior to that of a template-based system unless domain-specific rules aredeveloped or general rules are tuned for the par-ticular domain.
Furthermore, full NLG may betoo slow for use in dialog systems.A third, more recent, approach is trainablegeneration: techniques for automatically train-ing NLG modules, or hybrid techniques thatadapt NLG modules to particular domains oruser groups, e.g.
(Langkilde, 2000; Mellish,1998; Walker, Rambow and Rogati, 2002).Open questions about the trainable approachinclude (1) whether the output quality is highenough, and (2) whether the techniques workwell across domains.
For example, the trainingmethod used in SPoT (Sentence Planner Train-able), as described in (Walker, Rambow and Ro-gati, 2002), was only shown to work in the traveldomain, for the information gathering phase ofthe dialog, and with simple content plans in-volving no rhetorical relations.This paper describes trainable sentenceplanning for information presentation in theMATCH (Multimodal Access To City Help) di-alog system (Johnston et al, 2002).
We pro-vide evidence that the trainable approach isfeasible by showing (1) that the training tech-nique used for SPoT can be extended to anew domain (restaurant information); (2) thatthis technique, previously used for information-gathering utterances, can be used for infor-mation presentations, namely recommendationsand comparisons; and (3) that the qualityof the output is comparable to that of atemplate-based generator previously developedand experimentally evaluated with MATCHusers (Walker et al, 2002; Stent et al, 2002).Section 2 describes SPaRKy (Sentence Plan-ning with Rhetorical Knowledge), an extensionof SPoT that uses rhetorical relations.
SPaRKyconsists of a randomized sentence plan gen-erator (SPG) and a trainable sentence planranker (SPR); these are described in Sections 3strategy:recommenditems: Chanpen Thairelations:justify(nuc:1;sat:2); justify(nuc:1;sat:3); jus-tify(nuc:1;sat:4)content: 1. assert(best(Chanpen Thai))2. assert(has-att(Chanpen Thai, decor(decent)))3. assert(has-att(Chanpen Thai, service(good))4. assert(has-att(Chanpen Thai, cuisine(Thai)))Figure 1: A content plan for a recommendationfor a restaurant in midtown Manhattanstrategy:compare3items: Above, Carmine?srelations:elaboration(1;2); elaboration(1;3); elabora-tion(1,4); elaboration(1,5); elaboration(1,6);elaboration(1,7); contrast(2;3); contrast(4;5);contrast(6;7)content: 1. assert(exceptional(Above, Carmine?s))2. assert(has-att(Above, decor(good)))3. assert(has-att(Carmine?s, decor(decent)))4. assert(has-att(Above, service(good)))5. assert(has-att(Carmine?s, service(good)))6. assert(has-att(Above, cuisine(New Ameri-can)))7. assert(has-att(Carmine?s, cuisine(italian)))Figure 2: A content plan for a comparison be-tween restaurants in midtown Manhattanand 4.
Section 5 presents the results of twoexperiments.
The first experiment shows thatgiven a content plan such as that in Figure 1,SPaRKy can select sentence plans that commu-nicate the desired rhetorical relations, are sig-nificantly better than a randomly selected sen-tence plan, and are on average less than 10%worse than a sentence plan ranked highest byhuman judges.
The second experiment showsthat the quality of SPaRKy?s output is compa-rable to that of MATCH?s template-based gen-erator.
We sum up in Section 6.2 SPaRKy ArchitectureInformation presentation in the MATCH sys-tem focuses on user-tailored recommendationsand comparisons of restaurants (Walker et al,2002).
Following the bottom-up approach totext-planning described in (Marcu, 1997; Mel-lish, 1998), each presentation consists of a set ofassertions about a set of restaurants and a spec-ification of the rhetorical relations that hold be-tween them.
Example content plans are shownin Figures 1 and 2.
The job of the sentenceplanner is to choose linguistic resources to real-ize a content plan and then rank the resultingalternative realizations.
Figures 3 and 4 showalternative realizations for the content plans inFigures 1 and 2.Alt Realization H SPR2 Chanpen Thai, which is a Thai restau-rant, has decent decor.
It has goodservice.
It has the best overall qualityamong the selected restaurants.3 .285 Since Chanpen Thai is a Thai restau-rant, with good service, and it has de-cent decor, it has the best overall qual-ity among the selected restaurants.2.5 .146 Chanpen Thai, which is a Thai restau-rant, with decent decor and good ser-vice, has the best overall quality amongthe selected restaurants.4 .70Figure 3: Some alternative sentence plan real-izations for the recommendation in Figure 1.
H= Humans?
score.
SPR = SPR?s score.Alt Realization H SPR11 Above and Carmine?s offer exceptionalvalue among the selected restaurants.Above, which is a New Americanrestaurant, with good decor, has goodservice.
Carmine?s, which is an Italianrestaurant, with good service, has de-cent decor.2 .7312 Above and Carmine?s offer exceptionalvalue among the selected restaurants.Above has good decor, and Carmine?shas decent decor.
Above and Carmine?shave good service.
Above is a NewAmerican restaurant.
On the otherhand, Carmine?s is an Italian restau-rant.2.5 .5013 Above and Carmine?s offer exceptionalvalue among the selected restaurants.Above is a New American restaurant.It has good decor.
It has good service.Carmine?s, which is an Italian restau-rant, has decent decor and good service.3 .6720 Above and Carmine?s offer exceptionalvalue among the selected restaurants.Carmine?s has decent decor but Abovehas good decor, and Carmine?s andAbove have good service.
Carmine?s isan Italian restaurant.
Above, however,is a New American restaurant.2.5 .4925 Above and Carmine?s offer exceptionalvalue among the selected restaurants.Above has good decor.
Carmine?s isan Italian restaurant.
Above has goodservice.
Carmine?s has decent decor.Above is a New American restaurant.Carmine?s has good service.NR NRFigure 4: Some of the alternative sentence planrealizations for the comparison in Figure 2.
H= Humans?
score.
SPR = SPR?s score.
NR =Not generated or rankedThe architecture of the spoken language gen-eration module in MATCH is shown in Figure 5.The dialog manager sends a high-level commu-nicative goal to the SPUR text planner, whichselects the content to be communicated using auser model and brevity constraints (see (WalkerSynthesizerHow to Say ItRealizerSurfaceAssignerProsodySpeechUTTERANCESYSTEMSentenceSPURPlannerCommunicativeDIALOGUEMANAGERGoalsTextPlannerWhat to SayFigure 5: A dialog system with a spoken lan-guage generatoret al, 2002)).
The output is a content plan fora recommendation or comparison such as thosein Figures 1 and 2.SPaRKy, the sentence planner, gets the con-tent plan, and then a sentence plan generator(SPG) generates one or more sentence plans(Figure 7) and a sentence plan ranker (SPR)ranks the generated plans.
In order for theSPG to avoid generating sentence plans that areclearly bad, a content-structuring module firstfinds one or more ways to linearly order the in-put content plan using principles of entity-basedcoherence based on rhetorical relations (Knottet al, 2001).
It outputs a set of text plantrees (tp-trees), consisting of a set of speechacts to be communicated and the rhetorical re-lations that hold between them.
For example,the two tp-trees in Figure 6 are generated forthe content plan in Figure 2.
Sentence planssuch as alternative 25 in Figure 4 are avoided;it is clearly worse than alternatives 12, 13 and20 since it neither combines information basedon a restaurant entity (e.g Babbo) nor on anattribute (e.g.
decor).The top ranked sentence plan output by theSPR is input to the RealPro surface realizerwhich produces a surface linguistic utterance(Lavoie and Rambow, 1997).
A prosody as-signment module uses the prior levels of linguis-tic representation to determine the appropriateprosody for the utterance, and passes a marked-up string to the text-to-speech module.3 Sentence Plan GenerationAs in SPoT, the basis of the SPG is a set ofclause-combining operations that operate on tp-trees and incrementally transform the elemen-tary predicate-argument lexico-structural rep-resentations (called DSyntS (Melcuk, 1988))associated with the speech-acts on the leavesof the tree.
The operations are applied in abottom-up left-to-right fashion and the result-ing representation may contain one or more sen-tences.
The application of the operations yieldstwo parallel structures: (1) a sentence plantree (sp-tree), a binary tree with leaves labeledby the assertions from the input tp-tree, and in-terior nodes labeled with clause-combining op-erations; and (2) one or more DSyntS trees(d-trees) which reflect the parallel operationson the predicate-argument representations.We generate a random sample of possiblesentence plans for each tp-tree, up to a pre-specified number of sentence plans, by ran-domly selecting among the operations accord-ing to a probability distribution that favors pre-ferred operations1.
The choice of operation isfurther constrained by the rhetorical relationthat relates the assertions to be combined, asin other work e.g.
(Scott and de Souza, 1990).In the current work, three RST rhetorical rela-tions (Mann and Thompson, 1987) are used inthe content planning phase to express the rela-tions between assertions: the justify relationfor recommendations, and the contrast andelaboration relations for comparisons.
Weadded another relation to be used during thecontent-structuring phase, called infer, whichholds for combinations of speech acts for whichthere is no rhetorical relation expressed in thecontent plan, as in (Marcu, 1997).
By explicitlyrepresenting the discourse structure of the infor-mation presentation, we can generate informa-tion presentations with considerably more inter-nal complexity than those generated in (Walker,Rambow and Rogati, 2002) and eliminate thosethat violate certain coherence principles, as de-scribed in Section 2.The clause-combining operations are generaloperations similar to aggregation operationsused in other research (Rambow and Korelsky,1992; Danlos, 2000).
The operations and the1Although the probability distribution here is hand-crafted based on assumed preferences for operations suchas merge, relative-clause and with-reduction, itmight also be possible to learn this probability distribu-tion from the data by training in two phases.nucleus:<3>assert-com-decorcontrastnucleus:<2>assert-com-decor nucleus:<6>assert-com-cuisinenucleus:<7>assert-com-cuisinecontrastnucleus:<4>assert-com-servicenucleus:<5>assert-com-servicecontrastelaborationnucleus:<1>assert-com-list_exceptional infernucleus:<3>assert-com-decornucleus:<5>assert-com-servicenucleus:<7>assert-com-cuisineinferinfernucleus:<2>assert-com-decor nucleus:<6>assert-com-cuisinenucleus:<4>assert-com-serviceelaborationnucleus:<1>assert-com-list_exceptional contrastFigure 6: Two tp-trees for alternative 13 in Figure 4.constraints on their use are described below.merge applies to two clauses with identicalmatrix verbs and all but one identical argu-ments.
The clauses are combined and the non-identical arguments coordinated.
For example,merge(Above has good service;Carmine?s hasgood service) yields Above and Carmine?s havegood service.
merge applies only for the rela-tions infer and contrast.with-reduction is treated as a kind of?verbless?
participial clause formation in whichthe participial clause is interpreted with thesubject of the unreduced clause.
For exam-ple, with-reduction(Above is a New Amer-ican restaurant;Above has good decor) yieldsAbove is a New American restaurant, with gooddecor.
with-reduction uses two syntacticconstraints: (a) the subjects of the clauses mustbe identical, and (b) the clause that under-goes the participial formation must have a have-possession predicate.
In the example above, forinstance, the Above is a New American restau-rant clause cannot undergo participial forma-tion since the predicate is not one of have-possession.
with-reduction applies only forthe relations infer and justify.relative-clause combines two clauses withidentical subjects, using the second clause torelativize the first clause?s subject.
For ex-ample, relative-clause(Chanpen Thai is aThai restaurant, with decent decor and good ser-vice;Chanpen Thai has the best overall qualityamong the selected restaurants) yields ChanpenThai, which is a Thai restaurant, with decentdecor and good service, has the best overall qual-ity among the selected restaurants.
relative-clause also applies only for the relations inferand justify.cue-word inserts a discourse connective(one of since, however, while, and, but, and onthe other hand), between the two clauses to becombined.
cue-word conjunction combinestwo distinct clauses into a single sentence with acoordinating or subordinating conjunction (e.g.Above has decent decor BUT Carmine?s hasgood decor), while cue-word insertion insertsa cue word at the start of the second clause, pro-ducing two separate sentences (e.g.
Carmine?sis an Italian restaurant.
HOWEVER, Aboveis a New American restaurant).
The choice ofcue word is dependent on the rhetorical relationholding between the clauses.Finally, period applies to two clauses to betreated as two independent sentences.Note that a tp-tree can have very differentrealizations, depending on the operations of theSPG.
For example, the second tp-tree in Fig-ure 6 yields both Alt 11 and Alt 13 in Figure 4.However, Alt 13 is more highly rated than Alt11.
The sp-tree and d-tree produced by the SPGfor Alt 13 are shown in Figures 7 and 8.
Thecomposite labels on the interior nodes of the sp-PERIOD_elaborationPERIOD_contrastRELATIVE_CLAUSE_inferPERIOD_inferPERIOD_infer <4>assert-com-service <7>assert-com-cuisine MERGE_infer<3>assert-come-decor <5>assert-com-service<2>assert-com-decor<6>assert-com-cuisine<1>assert-com-list_exceptionalFigure 7: Sentence plan tree (sp-tree) for alternative 13 in Figure 4offerexceptionalamongrestaurantselectedAbove_and_Carmine?sCarmine?sBE3restaurantCarmine?sItaliandecordecent AND2servicegoodHAVE1PERIODNew_AmericanBE3Above Above decorgoodHAVE1restaurantAbovegoodHAVE1servicePERIODPERIODvaluePERIODFigure 8: Dependency tree (d-tree) for alternative 13 in Figure 4tree indicate the clause-combining relation se-lected to communicate the specified rhetoricalrelation.
The d-tree for Alt 13 in Figure 8 showsthat the SPG treats the period operation aspart of the lexico-structural representation forthe d-tree.
After sentence planning, the d-treeis split into multiple d-trees at period nodes;these are sent to the RealPro surface realizer.Separately, the SPG also handles referring ex-pression generation by converting proper namesto pronouns when they appear in the previousutterance.
The rules are applied locally, acrossadjacent sequences of utterances (Brennan etal., 1987).
Referring expressions are manipu-lated in the d-trees, either intrasententially dur-ing the creation of the sp-tree, or intersenten-tially, if the full sp-tree contains any period op-erations.
The third and fourth sentences for Alt13 in Figure 4 show the conversion of a namedrestaurant (Carmine?s) to a pronoun.4 Training the Sentence PlanRankerThe SPR takes as input a set of sp-trees gener-ated by the SPG and ranks them.
The SPR?srules for ranking sp-trees are learned from a la-beled set of sentence-plan training examples us-ing the RankBoost algorithm (Schapire, 1999).Examples and Feedback: To apply Rank-Boost, a set of human-rated sp-trees are en-coded in terms of a set of features.
We startedwith a set of 30 representative content plans foreach strategy.
The SPG produced as many as 20distinct sp-trees for each content plan.
The sen-tences, realized by RealPro from these sp-trees,were then rated by two expert judges on a scalefrom 1 to 5, and the ratings averaged.
Each sp-tree was an example input for RankBoost, witheach corresponding rating its feedback.Features used by RankBoost: RankBoostrequires each example to be encoded as a set ofreal-valued features (binary features have val-ues 0 and 1).
A strength of RankBoost is thatthe set of features can be very large.
We used7024 features for training the SPR.
These fea-tures count the number of occurrences of certainstructural configurations in the sp-trees and thed-trees, in order to capture declaratively de-cisions made by the randomized SPG, as in(Walker, Rambow and Rogati, 2002).
The fea-tures were automatically generated using fea-ture templates.
For this experiment, we usetwo classes of feature: (1) Rule-features: Thesefeatures are derived from the sp-trees and repre-sent the ways in which merge, infer and cue-word operations are applied to the tp-trees.These feature names start with ?rule?.
(2) Sent-features: These features are derived from theDSyntSs, and describe the deep-syntactic struc-ture of the utterance, including the chosen lex-emes.
As a result, some may be domain specific.These feature names are prefixed with ?sent?.We now describe the feature templates usedin the discovery process.
Three templates wereused for both sp-tree and d-tree features; twowere used only for sp-tree features.
Local featuretemplates record structural configurations localto a particular node (its ancestors, daughtersetc.).
Global feature templates, which are usedonly for sp-tree features, record properties of theentire sp-tree.
We discard features that occurfewer than 10 times to avoid those specific toparticular text plans.Strategy System Min Max Mean S.D.Recommend SPaRKy 2.0 5.0 3.6 .71HUMAN 2.5 5.0 3.9 .55RANDOM 1.5 5.0 2.9 .88Compare2 SPaRKy 2.5 5.0 3.9 .71HUMAN 2.5 5.0 4.4 .54RANDOM 1.0 5.0 2.9 1.3Compare3 SPaRKy 1.5 4.5 3.4 .63HUMAN 3.0 5.0 4.0 .49RANDOM 1.0 4.5 2.7 1.0Table 1: Summary of Recommend, Compare2and Compare3 results (N = 180)There are four types of local featuretemplate: traversal features, sister features,ancestor features and leaf features.
Localfeature templates are applied to all nodes in asp-tree or d-tree (except that the leaf feature isnot used for d-trees); the value of the resultingfeature is the number of occurrences of thedescribed configuration in the tree.
For eachnode in the tree, traversal features record thepreorder traversal of the subtree rooted atthat node, for all subtrees of all depths.
Anexample is the feature ?rule traversal assert-com-list exceptional?
(with value 1) of thetree in Figure 7.
Sister features record allconsecutive sister nodes.
An example is the fea-ture ?rule sisters PERIOD infer RELATIVECLAUSE infer?
(with value 1) of thetree in Figure 7.
For each node in thetree, ancestor features record all the ini-tial subpaths of the path from that nodeto the root.
An example is the feature?rule ancestor PERIOD contrast*PERIODinfer?
(with value 1) of the tree in Figure 7.Finally, leaf features record all initial substringsof the frontier of the sp-tree.
For example, thesp-tree of Figure 7 has value 1 for the feature?leaf #assert-com-list exceptional#assert-com-cuisine?.Global features apply only to the sp-tree.
They record, for each sp-tree and foreach clause-combining operation labeling a non-frontier node, (1) the minimal number of leavesdominated by a node labeled with that op-eration in that tree (MIN); (2) the maximalnumber of leaves dominated by a node la-beled with that operation (MAX); and (3)the average number of leaves dominated bya node labeled with that operation (AVG).For example, the sp-tree in Figure 7 hasvalue 3 for ?PERIOD infer max?, value 2 for?PERIOD infer min?
and value 2.5 for ?PE-RIOD infer avg?.5 Experimental ResultsWe report two sets of experiments.
The first ex-periment tests the ability of the SPR to select ahigh quality sentence plan from a population ofsentence plans randomly generated by the SPG.Because the discriminatory power of the SPR isbest tested by the largest possible population ofsentence plans, we use 2-fold cross validation forthis experiment.
The second experiment com-pares SPaRKy to template-based generation.Cross Validation Experiment: We re-peatedly tested SPaRKy on the half of the cor-pus of 1756 sp-trees held out as test data foreach fold.
The evaluation metric is the human-assigned score for the variant that was ratedhighest by SPaRKy for each text plan for eachtask/user combination.
We evaluated SPaRKyon the test sets by comparing three data pointsfor each text plan: HUMAN (the score of thetop-ranked sentence plan); SPARKY (the scoreof the SPR?s selected sentence); and RANDOM(the score of a sentence plan randomly selectedfrom the alternate sentence plans).We report results separately for comparisonsbetween two entities and among three or moreentities.
These two types of comparison are gen-erated using different strategies in the SPG, andcan produce text that is very different both interms of length and structure.Table 1 summarizes the difference betweenSPaRKy, HUMAN and RANDOM for recom-mendations, comparisons between two entitiesand comparisons between three or more enti-ties.
For all three presentation types, a pairedt-test comparing SPaRKy to HUMAN to RAN-DOM showed that SPaRKy was significantlybetter than RANDOM (df = 59, p < .001) andsignificantly worse than HUMAN (df = 59, p< .001).
This demonstrates that the use of atrainable sentence planner can lead to sentenceplans that are significantly better than baseline(RANDOM), with less human effort than pro-gramming templates.Comparison with template generation:For each content plan input to SPaRKy, thejudges also rated the output of a template-based generator for MATCH.
This template-based generator performs text planning and sen-tence planning (the focus of the current pa-per), including some discourse cue insertion,clause combining and referring expression gen-eration; the templates themselves are describedin (Walker et al, 2002).
Because the templatesare highly tailored to this domain, this genera-tor can be expected to perform well.
Exampletemplate-based and SPaRKy outputs for a com-parison between three or more items are shownin Figure 9.Strategy System Min Max Mean S.D.Recommend Template 2.5 5.0 4.22 0.74SPaRKy 2.5 4.5 3.57 0.59HUMAN 4.0 5.0 4.37 0.37Compare2 Template 2.0 5.0 3.62 0.75SPaRKy 2.5 4.75 3.87 0.52HUMAN 4.0 5.0 4.62 0.39Compare3 Template 1.0 5.0 4.08 1.23SPaRKy 2.5 4.25 3.375 0.38HUMAN 4.0 5.0 4.63 0.35Table 2: Summary of template-based genera-tion results.
N = 180Table 2 shows the mean HUMAN scores forthe template-based sentence planning.
A pairedt-test comparing HUMAN and template-basedscores showed that HUMAN was significantlybetter than template-based sentence planningonly for compare2 (df = 29, t = 6.2, p < .001).The judges evidently did not like the templatefor comparisons between two items.
A pairedt-test comparing SPaRKy and template-basedsentence planning showed that template-basedsentence planning was significantly better thanSPaRKy only for recommendations (df = 29, t= 3.55, p < .01).
These results demonstratethat trainable sentence planning shows promisefor producing output comparable to that of atemplate-based generator, with less program-ming effort and more flexibility.The standard deviation for all three template-based strategies was wider than for HUMANor SPaRKy, indicating that there may becontent-specific aspects to the sentence plan-ning done by SPaRKy that contribute to out-put variation.
The data show this to be cor-rect; SPaRKy learned content-specific prefer-ences about clause combining and discourse cueinsertion that a template-based generator can-System Realization HTemplate Among the selected restaurants, the fol-lowing offer exceptional overall value.Uguale?s price is 33 dollars.
It has gooddecor and very good service.
It?s aFrench, Italian restaurant.
Da Andrea?sprice is 28 dollars.
It has good decor andvery good service.
It?s an Italian restau-rant.
John?s Pizzeria?s price is 20 dollars.It has mediocre decor and decent service.It?s an Italian, Pizza restaurant.4.5SPaRKy Da Andrea, Uguale, and John?s Pizze-ria offer exceptional value among the se-lected restaurants.
Da Andrea is an Ital-ian restaurant, with very good service, ithas good decor, and its price is 28 dol-lars.
John?s Pizzeria is an Italian , Pizzarestaurant.
It has decent service.
It hasmediocre decor.
Its price is 20 dollars.Uguale is a French, Italian restaurant,with very good service.
It has good decor,and its price is 33 dollars.4Figure 9: Comparisons between 3 or moreitems, H = Humans?
scorenot easily model, but that a trainable sentenceplanner can.
For example, Table 3 shows thenine rules generated on the first test fold whichhave the largest negative impact on the finalRankBoost score (above the double line) andthe largest positive impact on the final Rank-Boost score (below the double line), for com-parisons between three or more entities.
Therule with the largest positive impact shows thatSPaRKy learned to prefer that justifications in-volving price be merged with other informationusing a conjunction.These rules are also specific to presentationtype.
Averaging over both folds of the exper-iment, the number of unique features appear-ing in rules is 708, of which 66 appear in therule sets for two presentation types and 9 ap-pear in the rule sets for all three presentationtypes.
There are on average 214 rule features,428 sentence features and 26 leaf features.
Themajority of the features are ancestor features(319) followed by traversal features (264) andsister features (60).
The remainder of the fea-tures (67) are for specific lexemes.To sum up, this experiment shows that theability to model the interactions between do-main content, task and presentation type is astrength of the trainable approach to sentenceplanning.6 ConclusionsThis paper shows that the training techniqueused in SPoT can be easily extended to a newN Condition ?s1 sent anc PROPERNOUN RESTAURANT*HAVE1 ?
16.5-0.8592 sent anc II Upper East Side*ATTR IN1*locate ?
4.5-0.8523 sent anc PERIOD infer*PERIOD infer*PERIOD elaboration ?
-?-0.5424 rule anc assert-com-service*MERGE infer?
1.5-0.3565 sent tvl depth 0 BE3 ?
4.5 -0.3466 rule anc PERIOD infer*PERIOD infer*PERIOD elaboration ?
-?-0.3457 rule anc assert-com-decor*PERIOD infer*PERIOD infer*PERIOD contrast *PE-RIOD elaboration?
-?-0.3428 rule anc assert-com-food quality*MERGEinfer ?
1.50.3989 rule anc assert-com-price*CWCONJUNCTION infer*PERIOD justify?
-?0.527Table 3: The nine rules generated on the firsttest fold which have the largest negative impacton the final RankBoost score (above the dou-ble line) and the largest positive impact on thefinal RankBoost score (below the double line),for Compare3.
?s represents the increment ordecrement associated with satisfying the condi-tion.domain and used for information presentationas well as information gathering.
Previous workon SPoT also compared trainable sentence plan-ning to a template-based generator that hadpreviously been developed for the same appli-cation (Rambow et al, 2001).
The evalua-tion results for SPaRKy (1) support the resultsfor SPoT, by showing that trainable sentencegeneration can produce output comparable totemplate-based generation, even for complex in-formation presentations such as extended com-parisons; (2) show that trainable sentence gen-eration is sensitive to variations in domain ap-plication, presentation type, and even humanpreferences about the arrangement of particu-lar types of information.7 AcknowledgmentsWe thank AT&T for supporting this research,and the anonymous reviewers for their helpfulcomments on this paper.ReferencesI.
Langkilde.
Forest-based statistical sentence gen-eration.
In Proc.
NAACL 2000, 2000.S.
E. Brennan, M. Walker Friedman, and C. J. Pol-lard.
A centering approach to pronouns.
In Proc.25th Annual Meeting of the ACL, Stanford, pages155?162, 1987.L.
Danlos.
2000.
G-TAG: A lexicalized formal-ism for text generation inspired by tree ad-joining grammar.
In Tree Adjoining Grammars:Formalisms, Linguistic Analysis, and Processing.CSLI Publications.M.
Johnston, S. Bangalore, G. Vasireddy, A. Stent,P.
Ehlen, M. Walker, S. Whittaker, and P. Mal-oor.
MATCH: An architecture for multimodal di-alogue systems.
In Annual Meeting of the ACL,2002.A.
Knott, J. Oberlander, M. O?Donnell and C. Mel-lish.
Beyond Elaboration: the interaction of rela-tions and focus in coherent text.
In Text Repre-sentation: linguistic and psycholinguistic aspects,pages 181-196, 2001.B.
Lavoie and O. Rambow.
A fast and portable re-alizer for text generation systems.
In Proc.
of the3rd Conference on Applied Natural Language Pro-cessing, ANLP97, pages 265?268, 1997.W.C.
Mann and S.A. Thompson.
Rhetorical struc-ture theory: A framework for the analysis of texts.Technical Report RS-87-190, USC/InformationSciences Institute, 1987.D.
Marcu.
From local to global coherence: abottom-up approach to text planning.
In Proceed-ings of the National Conference on Artificial In-telligence (AAAI?97), 1997.C.
Mellish, A. Knott, J. Oberlander, and M.O?Donnell.
Experiments using stochastic searchfor text planning.
In Proceedings of INLG-98.1998.I.
A. Melc?uk.
Dependency Syntax: Theory and Prac-tice.
SUNY, Albany, New York, 1988.O.
Rambow and T. Korelsky.
Applied text genera-tion.
In Proceedings of the Third Conference onApplied Natural Language Processing, ANLP92,pages 40?47, 1992.O.
Rambow, M. Rogati and M. A. Walker.
Evalu-ating a Trainable Sentence Planner for a SpokenDialogue Travel System In Meeting of the ACL,2001.R.
E. Schapire.
A brief introduction to boosting.
InProc.
of the 16th IJCAI, 1999.D.
R. Scott and C. Sieckenius de Souza.
Gettingthe message across in RST-based text generation.In Current Research in Natural Language Gener-ation, pages 47?73, 1990.A.
Stent, M. Walker, S. Whittaker, and P. Maloor.User-tailored generation for spoken dialogue: Anexperiment.
In Proceedings of ICSLP 2002., 2002.M.
A. Walker, S. J. Whittaker, A. Stent, P. Mal-oor, J. D. Moore, M. Johnston, and G. Vasireddy.Speech-Plans: Generating evaluative responsesin spoken dialogue.
In Proceedings of INLG-02.,2002.M.
Walker, O. Rambow, and M. Rogati.
Training asentence planner for spoken dialogue using boost-ing.
Computer Speech and Language: Special Is-sue on Spoken Language Generation, 2002.
