Proceedings of the Linguistic Annotation Workshop, pages 77?84,Prague, June 2007. c?2007 Association for Computational LinguisticsAdding semantic role annotation to a corpus of written DutchPaola Monachesi, Gerwert Stevens and Jantine TrapmanUtrecht University, Uil-OTS, Trans 10, 3512 JK Utrecht, The Netherlands{Paola.Monachesi, Gerwert.Stevens, Jantine.Trapman}@let.uu.nlAbstractWe present an approach to automatic se-mantic role labeling (SRL) carried out inthe context of the Dutch Language CorpusInitiative (D-Coi) project.
Adapting ear-lier research which has mainly focused onEnglish to the Dutch situation poses an in-teresting challenge especially because thereis no semantically annotated Dutch corpusavailable that can be used as training data.Our automatic SRL approach consists ofthree steps: bootstrapping from a syntacti-cally annotated corpus by means of a rule-based tagger developed for this purpose,manual correction on the basis of the Prop-Bank guidelines which have been adapted toDutch and training a machine learning sys-tem on the manually corrected data.1 IntroductionThe creation of semantically annotated corpora haslagged dramatically behind.
As a result, the need forsuch resources has now become urgent.
Several ini-tiatives have been launched at the international levelin the last years, however, they have focused almostentirely on English and not much attention has beendedicated to the creation of semantically annotatedDutch corpora.Within the Dutch Language Corpus Initiative (D-Coi)1, a recently completed Dutch project, guide-lines have been developed for the annotation of aDutch written corpus.
In particular, a pilot corpus1http://lands.let.ru.nl/projects/d-coi/has been compiled, parts of which have been en-riched with (verified) linguistic annotations.One of the innovative aspects of the D-Coi projectwith respect to previous initiatives, such as the Spo-ken Dutch Corpus (CGN - Corpus Gesproken Ned-erlands) (Oostdijk, 2002), was the development of aprotocol for a semantic annotation layer.
In particu-lar, two types of semantic annotation have been ad-dressed, that is semantic role assignment and tempo-ral and spatial semantics (Schuurman and Monach-esi, 2006).
The reason for this choice lies in the factthat semantic role assignment (i.e.
the semantic rela-tionships identified between items in the text such asthe agents or patients of particular actions), is one ofthe most attested and feasible types of semantic an-notation within corpora.
On the other hand, tempo-ral and spatial annotation was chosen because thereis a clear need for such a layer of annotation in ap-plications like information retrieval or question an-swering.The focus of this paper is on semantic role an-notation.
We analyze the choices we have madein selecting an appropriate annotation protocol bytaking into consideration existing initiatives suchas FrameNet (Johnson et al, 2002) and PropBank(Kingsbury et al, 2002) (cf.
also the Chinese andArabic PropBank).
We motivate our choice for thePropBank annotation scheme on the basis of thepromising results with respect to automatic seman-tic role labeling (SRL) which have been obtained forEnglish.
Furthermore, we discuss how the SRL re-search could be adapted to the Dutch situation giventhat no semantically annotated corpus was availablethat could be used as training data.772 Existing projectsDuring the last few years, corpora enriched with se-mantic role information have received much atten-tion, since they offer rich data both for empirical in-vestigations in lexical semantics and large-scale lex-ical acquisition for NLP and Semantic Web applica-tions.
Several initiatives are emerging at the inter-national level to develop annotation systems of ar-gument structure.
Within our project we have triedto exploit existing results as much as possible andto set the basis for a common standard.
We wantto profit from earlier experiences and contribute toexisting work by making it more complete with ourown (language specific) contribution given that mostresources have been developed for English.The PropBank and FrameNet projects have beenevaluated in order to assess whether the approachand the methodology they have developed for theannotation of semantic roles could be adopted forour purposes.
Given the results they have achieved,we have taken their insights and experiences as ourstarting point.FrameNet reaches a level of granularity in thespecification of the semantic roles which mightbe desirable for certain applications (i.e.
Ques-tion Answering).
Moreover, the predicates arelinked to an underlying frame ontology that clas-sifies the verbs within a semantic hierarchy.
Onthe other hand, despite the relevant work ofGildea and Jurafsky (2002), it is still an openissue whether FrameNet classes and frame ele-ments can be obtained and used automatically be-cause of the richness of the semantic structures em-ployed (Dzikovska et al, 2004).
Furthermore, theFrameNet approach might raise problems with re-spect to uniformity of role labeling even if humanannotators are involved.
Incompleteness, however,constitutes the biggest problem, i.e.
several framesand relations among frames are missing mainly be-cause FrameNet is still under development.
Adopt-ing the FrameNet lexicon for semantic annotationmeans contributing to its development with the ad-dition of (language specific) and missing frames.In our study, we have assumed that the FrameNetclassification even though it is based on Englishcould be applicable to Dutch as well.
This as-sumption is supported by the fact that the Germanproject Saarbru?cken Lexical Semantics Annotationand analysis (SALSA) (K. Erk and Pinkal, 2003)has adopted FrameNet with good results.
AlthoughDutch and English are quite similar, there are differ-ences on both sides.
For example, in the case of theSpanish FrameNet it turned out that frames may dif-fer in their number of elements across languages (cf.
(Subirats and Sato, 2004)).The other alternative was to employ the Prop-Bank approach which has the advantage of provid-ing clear role labels and thus a transparent anno-tation for both annotators and users.
Furthermore,there are promising results with respect to automaticsemantic role labeling for English thus the annota-tion process could be at least semi-automatic.
A dis-advantage of this approach is that we would have togive up the classification of frames in an ontology,as is the case in FrameNet, which could be very use-ful for certain applications, especially those relatedto the Semantic Web.
However, in (Monachesi andTrapman, 2006) suggestions are given on how thetwo approaches could be reconciled.The prospect of semi-automatic annotation wasthe decisive factor in the decision to adopt the Prop-Bank approach for the annotation of semantic roleswithin the D-Coi project.3 Automatic SRL: bootstrapping a corpuswith semantic rolesEver since the pioneering article of Gildea and Ju-rafsky (2002), there has been an increasing inter-est in automatic semantic role labeling (SRL).
How-ever, previous research has focused mainly on En-glish.
Adapting earlier research to the Dutch situ-ation poses an interesting challenge especially be-cause there is no semantically annotated Dutch cor-pus available that can be used as training data.
Fur-thermore, no PropBank frame files for Dutch exist.To solve the problem of the unavailability of train-ing data, we have developed a rule-based tagger tobootstrap a syntactically annotated corpus with se-mantic roles.
After manual correction, this corpuswas used as training data for a machine learningSRL system.
The input data for our SRL approachconsists of Dutch sentences, syntactically annotatedby the Dutch dependency parser Alpino (Bouma etal., 2000).78Syntactic annotation of our corpus is based on theSpoken Dutch Corpus (CGN) dependency graphs(Moortgat et al, 2000).
A CGN dependency graphis a tree-structured directed acyclic graph in whichnodes and edges are labeled with respectively c-labels (category-labels) and d-labels (dependencylabels).
C-labels of nodes denote phrasal categories,such as NP (noun phrase) and PP, c-labels of leafsdenote POS tags.
D-Labels describe the grammati-cal (dependency) relation between the node and itshead.
Examples of such relations are SU (subject),OBJ (direct object) and MOD (modifier).Intuitively, dependency structures are a great re-source for a rule-based semantic tagger, for they di-rectly encode the argument structure of lexical units,e.g.
the relation between constituents.
Our goal wasto make optimal use of this information in an au-tomatic SRL system.
In order to achieve this, wefirst defined a basic mapping between nodes in adependency graph and PropBank roles.
This map-ping forms the basis of our rule-based SRL system(Stevens, 2006).Mapping subject and object complements toPropBank arguments is straightforward: subjects aremapped to ARG0 (proto-typical agent), direct ob-jects to ARG1 (proto-typical patient) and indirect ob-jects to ARG2.
An exception is made for ergativesand passives, for which the subject is labeled withARG1.Devising a consistent mapping for higher num-bered arguments is more difficult, since their label-ing depends in general on the frame entry of thecorresponding predicate.
Since we could not useframe information, we used a heuristic method.
Thisheuristic strategy entails that after numbering sub-ject/object complements with the rules stated above,other complements are labeled in a left-to-right or-der, starting with the first available argument num-ber.
For example, if the subject is labeled withARG0 and there are no object complements, the firstavailable argument number is ARG1.Finally, a mapping for several types of modifierswas defined.
We refrained from the disambiguationtask, and concentrated on those modifiers that can bemapped consistently.
These modifiers are:?
ArgM-NEG - Negation markers.?
ArgM-REC - Reflexives and reciprocals.?
ArgM-PRD - Markers of secondary predi-cation: modifiers with the dependency labelPREDM?
ArgM-PNC - Purpose clauses: modifiers thatstart with om te .
These modifiers are markedby Alpino with the c-label OTI.?
ArgM-LOC - Locative modifiers: modifierswith the dependency label LD, the LD label isused by Alpino to mark modifiers that indicatea location of direction.4 XARA: a rule based SRL taggerWith the help of the mappings discussed above, wedeveloped a rule-based semantic role tagger, whichis able to bootstrap an unannotated corpus with se-mantic roles.
We used this rule-based tagger to re-duce the manual annotation effort.
After all, startingmanual annotation from scratch is time consumingand therefore expensive.
A possible solution is tostart from a (partially) automatically annotated cor-pus.The system we developed for this purpose iscalled XARA (XML-based Automatic Role-labelerfor Alpino-trees) (Stevens, 2006).
2 XARA iswritten in Java, the cornerstone of its rule-basedapproach is formed by XPath expressions; XPath(Clark and DeRose, 1999) is a powerful query lan-guage for the XML format.The corpus format we used in our experiments isthe Alpino XML format.
This format is designed tosupport a range of linguistic queries on the depen-dency trees in XPath directly (Bouma and Kloost-erman, 2002).
The structure of Alpino XML doc-uments directly corresponds to the structure of thedependency tree: dependency nodes are representedby NODE elements, attributes of the node elementsare the properties of the corresponding dependencynode, e.g.
c-label, d-label, pos-tag, lemma, etc.A rule in XARA consist of an XPath expressionthat addresses a node in the dependency tree, and atarget label for that node, i.e.
a rule is a (path,label)pair.
For example, a rule that selects direct objectnodes and labels them with ARG1 can be formulatedas:(//node[@rel=?obj1?
], 1)2The system is available at:http://www.let.uu.nl/?Paola.Monachesi/personal/dcoi/index.html79In this example, a numeric label is used to label anumbered argument.
For ARGMs, string value canbe used as target label.After their definition, rules can be applied to localdependency domains, i.e.
subtrees of a dependencytree.
The local dependency domain to which a ruleis applied, is called the rule?s context.
A context isdefined by an XPath expression that selects a groupof nodes.
Contexts for which we defined rules inXARA are verbal domains, that is, local dependencystructures with a verb as head.Table 1 shows the performance of XARA on ourtreebank.Table 1: Results of SRL with XARALabel Precision Recall F?=1Overall 65,11% 45,83% 53,80Arg0 98.97% 94.95% 96.92Arg1 70.08% 64.83% 67.35Arg2 47.41% 36.07% 40.97Arg3 13.89% 6.85% 9.17Arg4 1.56% 1.35% 1.45ArgM-LOC 83.49% 13.75% 23.61ArgM-NEG 72.79% 58.79% 65.05ArgM-PNC 91.94% 39.31% 55.07ArgM-PRD 63.64% 26.25% 37.17ArgM-REC 85.19% 69.70% 76.67Notice XARA?s performance on highered num-bered arguments, especially ARG4.
Manual inspec-tion of the manual labeling reveals that ARG4 argu-ments often occur in propositions without ARG2 andARG3 arguments.
Since our current heuristic label-ing method always chooses the first available argu-ment number, this method will have to be modifiedin order achieve a better score for ARG4 arguments.5 Manual correctionThe annotation by XARA of our tree bank, wasmanually corrected by one human annotator, how-ever, in order to deal with a Dutch corpus, the Prop-Bank annotation guidelines needed to be revised.Notice that both PropBank and D-Coi share theassumption that consistent argument labels shouldbe provided across different realizations of the sameverb and that modifiers of the verb should be as-signed functional tags.
However, they adopt a dif-ferent approach with respect to the treatment oftraces since PropBank creates co-reference chainsfor empty categories while within D-coi, empty cat-egories are almost non existent and in those fewcases in which they are attested, a coindexation hasbeen established already at the syntactic level.
Fur-thermore, D-coi assumes dependency structures forthe syntactic representation of its sentences whilePropBank employs phrase structure trees.
In addi-tion, Dutch behaves differently from English withrespect to certain constructions and these differencesshould be spelled out.In order to annotate our corpus, the PropBankguidelines needed a revision because they have beendeveloped for English and to add a semantic layerto the Penn TreeBank.
Besides the adaption (andextension) of the guidelines to Dutch (Trapman andMonachesi, 2006), we also have to consider a Dutchversion of the PropBank frameindex.
In PropBank,frame files provide a verb specific description of allpossible semantic roles and illustrate these roles byexamples.
The lack of example sentences makesconsistent annotation difficult.
Since defining a setof frame files from scratch is very time consuming,we decided to attempt an alternative approach, inwhich we annotated Dutch verbs with the same ar-gument structure as their English counterparts, thususe English frame files instead of creating Dutchones.
Although this causes some problems, for ex-ample, not all Dutch verbs can be translated to a100% equivalent English counterpart, such prob-lems proved to be relatively rare.
In most cases ap-plying the PropBank argument structure to Dutchverbs was straightforward.
If translation was notpossible, an ad hoc decision was made on how tolabel the verb.In order to verify the correctness of the annota-tion carried out automatically by XARA, we haveproceeded in the following way:1. localize the verb and translate it to English;only the argument structure of verbs is consid-ered in our annotation while that of NPs, PPsand other constituents has been neglected forthe moment.2.
check the verb?s frames file in Prop-Bank; the appropriate roles for each80verb could be identified in PropBank(http://verbs.colorado.edu/framesets/).3. localize the arguments of the verb; argumentsare usually NPs, PPs and sentential comple-ments.4.
localize the modifiers; in addition to the argu-ments of a verb, modifiers of place, time, man-ner etc.
are marked as well.An appropriate tool has been selected to carry outthe manual correction.
We have made an investiga-tion to evaluate three different tools for this purpose:CLaRK3, Salto4 and TrEd5.
On the basis of our mainrequirements, that is whether the tool is able to han-dle the xml-structure we have adopted and whetherit provides a user-friendly graphical interface and wehave come to the conclusion that the TrEd tool wasthe most appropriate for our needs.During the manual correction process, some prob-lems have emerged, as for example the fact thatwe have encountered some phenomena, such as theinterpretation of modifiers, for which linguistic re-search doesn?t provide a standard solution yet, wehave discarded these cases for the moment but itwould be desirable to address them in the future.Furthermore, the interaction among levels shouldbe taken more into consideration.
Even though theAlpino parser has an accuracy on our corpus of81%?90% (van Noord, 2006) and the syntactic cor-pus which has been employed for the annotation ofthe semantic roles had been manually corrected, wehave encountered examples in which the annotationprovided by the syntactic parser was not appropri-ate.
This is the case of a PP which was labeled asmodifier by the syntactic parser but which shouldbe labeled as argument according to the PropBankguidelines.
There should thus be an agreement inthese cases so that the syntactic structure can be cor-rected.
Furthermore, we have encountered problemswith respect to PP attachment, that is the syntacticrepresentation gives us correct and incorrect struc-tures and at the semantic level we are able to disam-biguate.
More research is necessary about how todeal with the incorrect representations.3http://www.bultreebank.org/clark/index.html4http://www.coli.uni-saarland.de/projects/salsa/5http://ufal.mff.cuni.cz/ pajas/tred/6 The TiMBL classification systemThe manually corrected sentences have been usedas training and test data for an SRL classificationsystem.
For this learning system we have em-ployed a Memory Based Learning (MBL) approach,implemented in the Tilburg Memory based learner(TiMBL) (Daelemans et al, 2004).TiMBL assigns class labels to training instanceson the basis of features and the feature set playsan important role in the performance of a classi-fier.
In choosing the feature set for our system, wemainly looked at previous research, especially sys-tems that participated in the 2004 and 2005 CoNLLshared tasks on semantic role labeling (Carreras andMa`rquez, 2005).It is worth noting that none of the systems in theCoNLL shared tasks used features extracted fromdependency structures.
However, we encounteredone system (Hacioglu, 2004) that did not participatein the CoNLL-shared task but did use the same dataand was based on dependency structures.
The maindifference with our system is that Hacioglu did notuse a dependency parser to create the dependencytrees, instead existing constituent trees were con-verted to dependency structures.
Furthermore, thesystem was trained and tested on English sentences.From features used in previous systems and someexperimentation with TiMBL, we derived the fol-lowing feature set.
The first group of features de-scribes the predicate (verb):(1) Predicate stem - The verb stem, provided byAlpino.
(2) Predicate voice - A binary feature indicatingthe voice of the predicate (passive/active).The second group of features describes the candi-date argument:(3) Argument c-label - The category label (phrasaltag) of the node, e.g.
NP or PP.
(4) Argument d-label - The dependency label ofthe node, e.g.
MOD or SU.
(5) Argument POS-tag - POS tag of the node if thenode is a leaf node, null otherwise.
(6) Argument head-word - The head word of therelation if the node is an internal node or thelexical item (word) if it is a leaf.81(7) Argument head-word - The head word of therelation if the node is an internal node or thelexical item (word) if it is a leaf.
(8) Head-word POS tag - The POS tag of the headword.
(9) c-label pattern of argument - The left to rightchain of c-labels of the argument and its sib-lings.
(10) d-label pattern - The left to right chain of d-labels of the argument and its siblings.
(11) c-label & d-label of argument combined -The c-label of the argument concatenated withits d-label.The training set consists of predicate/argumentpairs encoded in training instances.
Each instancecontains features of a predicate and its candidateargument.
Candidate arguments are nodes (con-stituents) in the dependency tree.
This pair-wiseapproach is analogous to earlier work by van denBosch et al (2004) and Tjong Kim Sang et al (2005)in which instances were build from verb/phrase pairsfrom which the phrase parent is an ancestor of theverb.
We adopted their approach to dependencytrees: only siblings of the verb (predicate) are con-sidered as candidate arguments.In comparison to experiments in earlier work, wehad relatively few training data available: our train-ing corpus consisted of 2,395 sentences which com-prise 3066 verbs, 5271 arguments and 3810 modi-fiers.6 To overcome our data sparsity problem, wetrained the classifier using the leave one out (LOO)method (-t leave_one_out option in TiMBL).With this option set, every data item in turn is se-lected once as a test item, and the classifier is trainedon all remaining items.
Except for the LOO op-tion, we only used the default TiMBL settings dur-ing training, to prevent overfitting because of datasparsity.7 Results & EvaluationTable 2 shows the performance of the TiMBL clas-sifier on our annotated dependency treebank.
Fromthese sentences, 12,113 instances were extracted.
To6We refer to (Oostdijk and Boves, 2006) for general infor-mation about the domain of the D-Coi corpus and its design.measure the performance of the systems, the auto-matically assigned labels were compared to the la-bels assigned by a human annotator.Table 2: Results of TiMBL classificationLabel Precision Recall F?=1Overall 70.27% 70.59% 70.43Arg0 90.44% 86.82% 88.59Arg1 87.80% 84.63% 86.18Arg2 63.34% 59.10% 61.15Arg3 21.21% 19.18% 20.14Arg4 54.05% 54.05% 54.05ArgM-ADV 54.98% 51.85% 53.37ArgM-CAU 47.24% 43.26% 45.16ArgM-DIR 36.36% 33.33% 34.78ArgM-DIS 74.27% 70.71% 72.45ArgM-EXT 29.89% 28.57% 29.21ArgM-LOC 57.95% 54.53% 56.19ArgM-MNR 52.07% 47.57% 49.72ArgM-NEG 68.00% 65.38% 66.67ArgM-PNC 68.61% 64.83% 66.67ArgM-PRD 45.45% 40.63% 42.90ArgM-REC 86.15% 84.85% 85.50ArgM-TMP 55.95% 53.29% 54.58It is difficult to compare our results with thoseobtained with other existing systems, since our sys-tem is the first one to be applied to Dutch sentences.Moreover, our data format, data size and evalua-tion methods (separate test/train/develop sets ver-sus LOO) are different from earlier research.
How-ever, to put our results somewhat in perspective, welooked mainly at systems that participated in theCoNLL shared tasks on semantic role labeling.The best performing system that participated inCoNLL 2005 reached an F1 of 80.
There were sevensystems with an F1 performance in the 75-78 range,seven more with performances in the 70-75 rangeand five with a performance between 65 and 70 (Car-reras and Ma`rquez, 2005).A system that did not participate in the CoNLLtask, but still provides interesting material for com-parison since it is also based on dependency struc-tures, is the system by Hacioglu (2004).
This systemscored 85,6% precision, 83,6% recall and 84,6 F1 onthe CoNLL data set, which is even higher than thebest results published so far on the PropBank data82sets (Pradhan et al, 2005): 84% precision, 75% re-call and 79 F1.
These results support our claim thatdependency structures can be very useful in the SRLtask.As one would expect, the overall precision andrecall scores of the classifier are higher than thoseof the XARA rule-based system.
Yet, we expecteda better performance of the classifier on the lowernumbered arguments (ARG0 and ARG1).
Our hy-pothesis is that performance on these arguments canbe improved by by adding semantic features to ourfeature set.Examples of such features are the subcategoriza-tion frame of the predicate and the semantic category(e.g.
WordNet synset) of the candidate argument.We expect that such semantic features will improvethe performance of the classifier for certain types ofverbs and arguments, especially the lower numberedarguments ARG0 and ARG1 and temporal and spa-tial modifiers.
For example, the Dutch prepositionover can either head a phrase indicating a locationor a time-span.
The semantic category of the neigh-boring noun phrase might be helpful in such cases tochoose the right PropBank label.
Thanks to new lex-ical resources, such as Cornetto (Vossen, 2006), andclustering techniques based on dependency struc-tures (van de Cruys, 2005), we might be able to addlexical semantic information about noun phrases infuture research.Performance of the classifier can also be im-proved by automatically optimizing the feature set.The optimal set of features for a classifier canbe found by employing bi-directional hill climbing(van den Bosch et al, 2004).
There is a wrapperscript (Paramsearch) available that can be used withTiMBL and several other learning systems that im-plements this approach7.
In addition, iterative deep-ening (ID) can be used as a heuristic way of findingthe optimal algorithm parameters for TiMBL.8 Conclusions & Future workWe have presented an approach to automatic seman-tic role labeling based on three steps: bootstrappingfrom a syntactically annotated Dutch corpus with arule-based tagger developed for this purpose, man-ual correction and training a machine learning sys-7URL: http://ilk.uvt.nl/software.html#paramsearchtem on the manually corrected data.The promising results in this area obtained for En-glish on the basis of PropBank role labels was a de-cisive factor for our choice to adopt the PropBankannotation scheme which has been adapted for theannotation of the Dutch corpus.
However, we wouldlike to adopt the conceptual structure of FrameNet,even though not necessarily the granularity of itsrole assignment approach, to this end we are link-ing manually the predicates annotated with the Prop-Bank semantic roles to the FrameNet ontology.Only a small part of the D-Coi corpus has been an-notated with semantic information, in order to yieldinformation with respect to its feasibility.
We be-lieve that a more substantial annotation task will becarried out in the framework of a follow-up projectaiming at the construction of a 500 million word cor-pus, in which one million words will be annotatedwith semantic information.
Hopefully, in the follow-up project, it will be possible to carry out experi-ments and measure inter-annotator agreement sincedue to financial constraints only one annotator hasannotated the current corpus.Finally, it would be interesting to see how theclassifier would perform on larger collections andnew genres of data.
The follow-up of the D-Coiproject will provide new semantically annotated datato facilitate research in this area.ReferencesG.
Bouma and G. Kloosterman.
2002.
Querying depen-dency treebanks in xml.
In Proceedings of the Thirdinternational conference on Language Resources andEvaluation (LREC).
Gran Canaria.G.
Bouma, G. van Noord, and R. Malouf.
2000.
Alpino:wide-coverage computational analysis of dutch.X.
Carreras and L. Ma`rquez.
2005.
Introduction tothe conll-2005 shared task: Semantic role labeling.In Proceedings of the Eighth Conference on Compu-tational Natural Language Learning (CoNLL-2005).Boston, MA, USA.J.
Clark and S. DeRose.
1999.
Xml path language(xpath).
W3C Recommendation 16 November 1999.URL: http://www.w3.org/TR/xpath.D.
Daelemans, D. Zavrel, K. van der Sloot, andA.
van den Bosch.
2004.
Timbl: Tilburg memorybased learner, version 5.1, reference guide.
ILK Tech-nical Report Series 04-02, Tilburg University.83M.
Dzikovska, M. Swift, and J. Allen.
2004.
Buildinga computational lexicon and ontology with framenet.In Proceedings of the workshop Building Lexi-cal Resources with Semantically Annotated Corpora(LREC) 2004.
Lisbon.D.
Gildea and D. Jurafsky.
2002.
Automatic labeling ofsemantic roles.
Comput.
Linguist., 28(3):245?288.K.
Hacioglu.
2004.
Semantic role labeling using de-pendency trees.
In COLING ?04: Proceedings of the20th international conference on Computational Lin-guistics, page 1273.
August 2004.C.
R. Johnson, C. J. Fillmore, M. R. L. Petruck, C. F.Baker, M. J. Ellsworth, J. Ruppenhofer, and E. J.Wood.
2002.
FrameNet:Theory and Practice.S.
Pado K. Erk, A. Kowalski and M. Pinkal.
2003.
To-wards a resource for lexical semantics: A large ger-man corpus with extensive semantic annotation.
InProceedings of ACL 2003.
Sapporo.P.
Kingsbury, M. Palmer, and M. Marcus.
2002.
Addingsemantic annotation to the penn treebank.
In Proceed-ings of the Human Language Technology Conference(HLT?02).P.
Monachesi and J. Trapman.
2006.
Merging framenetand propbank in a corpus of written dutch.
In Proceed-ings of (LREC) 2006.
Genoa.M.
Moortgat, I. Schuurman, and T. van der Wouden.2000.
CGN syntactische annotatie.
Internal reportCorpus Gesproken Nederlands.N.
Oostdijk and L. Boves.
2006.
User requirementsanalysis for the design of a reference corpus of writ-ten dutch.
In Proceedings of (LREC) 2006.
Genoa.N.
Oostdijk.
2002.
The design of the spoken dutch cor-pus.
In P. Peters, P. Collins, and A. Smith, editors, NewFrontiers of Corpus Research, pages 105?112.
Ams-terdam: Rodopi.S.
Pradhan, K., V. Krugler, W. Ward, J. H. Martin, andD.
Jurafsky.
2005.
Support vector learning for seman-tic argument classification.
Machine Learning Jour-nal, 1-3(60):11?39.E.
Tjong Kim Sang, S. Canisius, A. van den Bosch, andT.
Bogers.
2005.
Applying spelling error correctiontechniques for improving semantic role labeling.
InProceedings of the Ninth Conference on Natural Lan-guage Learning (CoNLL-2005).
Ann Arbor, MI, USA.I.
Schuurman and P. Monachesi.
2006.
The contours of asemantic annotation scheme for dutch.
In Proceedingsof Computational Linguistics in the Netherlands 2005.University of Amsterdam.
Amsterdam.G.
Stevens.
2006.
Automatic semantic role labeling in adutch corpus.
Master?s thesis, Universiteit Utrecht.C.
Subirats and H. Sato.
2004.
Spanish framenet andframesql.
In 4th International Conference on Lan-guage Resources and Evaluation.
Workshop on Build-ing Lexical Resources from Semantically AnnotatedCorpora.
Lisbon (Portugal), May 2004.J.
Trapman and P. Monachesi.
2006.
Manual for theannotation of semantic roles in d-coi.
Technical report,University of Utecht.Tim van de Cruys.
2005.
Semantic clustering in dutch.In Proceedings of CLIN 2005.A.
van den Bosch, S. Canisius, W. Daelemans, I. Hen-drickx, and E. Tjong Kim Sang.
2004.
Memory-based semantic role labeling: Optimizing features, al-gorithm, and output.
In H.T.
Ng and E. Riloff, edi-tors, Proceedings of the Eighth Conference on Compu-tational Natural Language Learning (CoNLL-2004).Boston, MA, USA.G.
van Noord.
2006.
At last parsing is now operational.In Proceedings of TALN 06.
Leuven.P.
Vossen.
2006.
Cornetto: Een lexicaal-semantischedatabase voor taaltechnologie.
Dixit Special Issue.Stevin.84
