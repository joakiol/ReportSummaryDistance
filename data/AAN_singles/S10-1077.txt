Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 345?350,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsJU_CSE_TEMP: A First Step towards Evaluating Events, Time Ex-pressions and Temporal RelationsAnup Kumar Kolya1, Asif Ekbal2 and Sivaji Bandyopadhyay31,3Department of Computer Science and Engineering, Jadavpur University,Kolkata-700032, India2Department of Computational Linguistics, Heidelberg University,Heidelberg-69120, GermanyEmail: anup.kolya@gmail.com1, asif.ekbal@gmail.com2and sivaji_cse_ju@yahoo.com3AbstractTemporal information extraction is apopular and interesting research field inthe area of Natural Language Processing(NLP).
In this paper, we report our workson TempEval-2 shared task.
This is ourfirst participation and we participated inall the tasks, i.e., A, B, C, D, E and F. Wedevelop rule-based systems for Tasks Aand B, whereas the remaining tasks arebased on a machine learning approach,namely Conditional Random Field(CRF).
All our systems are still in theirdevelopment stages, and we report thevery initial results.
Evaluation results onthe shared task English datasets yield theprecision, recall and F-measure values of55%, 17% and 26%, respectively forTask A and 48%, 56% and 52%, respec-tively for Task B (event recognition).The rest of tasks, namely C, D, E and Fwere evaluated with a relatively simplermetric: the number of correct answers di-vided by the number of answers.
Experi-ments on the English datasets yield theaccuracies of 63%, 80%, 56% and 56%for tasks C, D, E and F, respectively.1 IntroductionTemporal information extraction is, nowadays, apopular and interesting research area of NaturalLanguage Processing (NLP).
Generally, eventsare described in different newspaper texts, sto-ries and other important documents whereevents happen in time and the temporal locationand ordering of these events are specified.
Oneof the important tasks of text analysis clearly re-quires identifying events described in a text andlocating these in time.
This is also important in awide range of NLP applications that includetemporal question answering, machine transla-tion and document summarization.In the literature, temporal relation identifica-tion based on machine learning approaches canbe found in Boguraev et el.
(2005), Mani et al(2006), Chambers et al (2007) and some of theTempEval 2007 participants (Verhagen et al,2007).
Most of these works tried to improveclassification accuracies through feature engi-neering.
The performance of any machine learn-ing based system is often limited by the amountof available training data.
Mani et al (2006) in-troduced a temporal reasoning component thatgreatly expands the available training data.
Thetraining set was increased by a factor of 10 bycomputing the closure of the various temporalrelations that exist in the training data.
They re-ported significant improvement of the classifica-tion accuracies on event-event and event-timerelations.
Their experimental result showed theaccuracies of 62.5%-94.95% and 73.68%-90.16% for event-event and event-time relations,respectively.
However, this has two shortcom-ings, namely feature vector duplication causedby the data normalization process and the unreal-istic evaluation scheme.
The solutions to theseissues are briefly described in Mani et al (2007).In TempEval 2007 task, a common standard da-taset was introduced that involves three temporalrelations.
The participants reported F-measurescores for event-event relations ranging from42% to 55% and for event-time relations from73% to 80%.
Unlike (Mani et al, 2007; 2006),event-event temporal relations were not dis-course-wide (i.e., any pair of events can be tem-porally linked) in TempEval 2007.
Here, theevent-event relations were restricted to eventswithin two consecutive sentences.
Thus, thesetwo frameworks produced highly dissimilar re-345sults for solving the problem of temporal relationclassification.In order to apply various machine learning al-gorithms, most of the authors formulated tempo-ral relation as an event paired with a time or an-other event and translated these into a set of fea-ture values.
Some of the popularly used machinelearning techniques were Naive-Bayes, DecisionTree (C5.0), Maximum Entropy (ME) and Sup-port Vector Machine (SVM).
Machine learningtechniques alone cannot always yield good accu-racies.
To achieve reasonable accuracy, someresearchers (Mao et al, 2006) used hybrid ap-proach.
The basic principle of hybrid approach isto combine the rule-based component with ma-chine learning.
It has been shown in (Mao et al,2006) that classifiers make most mistakes nearthe decision plane in feature space.
The authorscarried out a series of experiments for each of thethree tasks on four models, namely naive-Bayes,decision tree (C5.0), maximum entropy and sup-port vector machine.
The system was designed insuch a way that they can take the advantage ofrule-based as well as machine learning duringfinal decision making.
But, they did not explainexactly in what situations machine learning orrule based system should be used given a particu-lar instance.
They had the option to call eithercomponent on the fly in different situations sothat they can take advantage of the two empiricalapproaches in an integrated way.The rest of the paper is structured as follows.We present very brief descriptions of the differ-ent tasks in Section 2.
Section 3 describes ourapproach in details with rule-based techniquesfor tasks A and B in Subsection 3.1, CRF basedtechniques in Subsection 3.2 for tasks C, D, Eand F, and features in Subsection 3.3.
Detailedevaluation results are reported in Section 4.
Fi-nally, Section 5 concludes the paper with a direc-tion to future works.2 Task DescriptionThe main research in this area involves identifi-cation of all temporal referring expressions,events and temporal relations within a text.
Themain challenges involved in this task were firstaddressed during TempEval-1 in 2007 (Verhagenet al, 2007).
This was an initial evaluation exer-cise based on three limited tasks that were con-sidered realistic both from the perspective of as-sembling resources for development and testingand from the perspective of developing systemscapable of addressing the tasks.
In TempEval2007, following types of event-time temporalrelations were considered: Task A (relation be-tween the events and times within the same sen-tence), Task B (relation between events anddocument creation time) and Task C (relationbetween verb events in adjacent sentences).
Thedata sets were based on TimeBank, a hand-builtgold standard of annotated texts using the Ti-meML markup scheme1.
The data sets includedsentence boundaries, timex3 tags (including thespecial document creation time tag), and eventtags.
For tasks A and B, a restricted set of eventswas used, namely those events that occur morethan 5 times in TimeBank.
For all three tasks, therelation labels used were before, after, overlap,before-or-overlap, overlap-or-after and vague.Six teams participated in the TempEval tasks.Three of the teams used statistics exclusively,one used a rule-based system and the other twoemployed a hybrid approach.
For task A, therange of F-measure scores were from 0.34 to0.62 for the strict scheme and from 0.41 to 0.63for the relaxed scheme.
For task B, the scoreswere from 0.66 to 0.80 (strict) and 0.71 to 0.81(relaxed).
Finally, task C scores range from 0.42to 0.55 (strict) and from 0.56 to 0.66 (relaxed).In TempEval-2, the following six tasks wereproposed:A:  The main task was to determine the extent ofthe time expressions in a text as defined by theTimeML timex3 tag.
In addition, values of thefeatures type and val had to be determined.
Thepossible values of type are time, date, duration,and set; the value of val is a normalized value asdefined by the timex2 and timex3 standards.B.
Task was to determine the extent of the eventsin a text as defined by the TimeML event tag.
Inaddition, the values of the features tense, aspect,polarity, and modality had to be determined.C.
Task was to determine the temporal relationbetween an event and a time expression in thesame sentence.D.
Temporal relation between an event and thedocument creation time had to be determined.E.
Temporal relation between two main events inconsecutive sentences had to be determined.F.
Temporal relation between two events, whereone event syntactically dominates the otherevent.In our present work, use handcrafted rules forTask A and Task B.
All the other tasks, i.e., C,D, E and F are developed based on the wellknown statistical algorithm, Conditional Random1www.timeml.org for details on TimeML346Field (CRF).
For CRF, we use only those fea-tures that are available in the training data.
Allthe systems are evaluated on the TempEval-2 shared task English datasets.
Evaluation resultsyield the precision, recall and F-measure valuesof 55%, 17% and 26%, respectively for Task Aand 48%, 56% and 52%, respectively for Task B.Experiments on the other tasks demonstrate theaccuracies of 63%, 80%, 56% and 56% for C, D,E and F, respectively.3 Our ApproachIn this section, we present our systematic ap-proach for evaluating events, time expressionsand temporal relations as part of our first partici-pation in the TempEval shared task.
We partici-pated in all the six tasks of TempEval-2.
Rule-based systems are developed using a preliminaryhandcrafted set of rules for tasks A and B. Weuse machine learning approach, namely CRF forsolving the remaining tasks, i.e., C, D, E and F.3.1 Rules for Task A and Task BWe manually identify a set of rules studying thevarious features available in the training data.There were some exceptions to these rules.
How-ever, a rule is used if it is found to be correctmost of the time throughout the training data.
Itis to be noted that these are the very preliminaryrules, and we are still working on finding outmore robust rules.
Below, we present the rulesfor tasks A and B.Task A.
The time expression is identified by de-fining appropriate regular expression.
The regu-lar expressions are based on several entities thatdenote month names, year, weekdays and thevarious digit expressions.
We also use a list ofkeywords (e.g., day, time, AM, PM etc.)
that de-note the various time expressions.
The values ofvarious attributes (e.g., type and value) of timeexpressions are computed by some simple tem-plate matching algorithms.Task B.
In case of Task B, the training data isinitially passed through the Stanford PoS tagger2.We consider the tokens as the events that aretagged with POS tags such as VB, VBG, VBN,VBP, VBZ and VBD, denoting the various verbexpressions.
Values of different attributes arecomputed as follows.2 http://nlp.stanford.edu/software/tagger.shtmla.
Tense: A manually augmented suffix list suchas: "ed","d","t" etc.
is used to capture the propertense of any event verb from surface level ortho-graphic variations.b.
Aspect: The Tense-Aspect-Modality (TAM)for English verbs is generally associated withauxiliaries.
A list is manually prepared.
Any oc-currence of main verb with continuous aspectleads to search for the adjacent previous auxil-iary and rules are formulated to extract TAMrelation using the manually generated checklist.A separate list of auxiliaries is prepared and suc-cessfully used for detection of progressive verbs.c.
Polarity: Verb-wise polarity is assigned by theoccurrence of previous negation words.
If anynegation word appears before any event verbthen the resultant polarity is negative; otherwise,the verb considered as positive by default.d.
Modality: We prepare a manual list that con-tains the words such as: may, could, would etc.The presence of these modal auxiliaries givesmodal tag to the targeted verb in a sentence oth-erwise it is considered a non-modal.e.
Class: We select ?occurrence?
to be class val-ue by default.3.2 Machine Learning Approach for TasksC, D, E and FFor tasks C-F, we use a supervised machinelearning approach that is based on CRF.
We con-sider the temporal relation identification task as apair-wise classification problem in which thetarget pairs?a TIMEX3 tag and an EVENT?aremodelled using CRF, which can include arbitraryset of features, and still can avoid overfitting in aprincipled manner.Introduction to CRF.
CRF (Lafferty et al,2001), is used to calculate the conditional prob-ability of values on designated output nodesgiven values on other designated input nodes.The conditional probability of a state sequence1, 2, ..., TS s s s=<1 2,O o>  given an observation se-quence , ....., )To o=<  is calculated as:1 ,1 11( | ) exp( ( , , ))T Kk k t to t kP s o f s s o tZ?
?= == ??
?
)where, 1 ,( , ,k t tf s s o t?kis a feature functionwhose weight ?
is to be learned via training.The values of the feature functions may rangebetween .....?
?
+ ?
, but typically they are347binary.
To make all conditional probabilities sumup to 1, we must calculate the normalizationfactor,01 1exp( ( , , ))T Ks k k tt k1 ,tZ f s s o t?
?= == ?
?
?
,which, as in HMMs, can be obtained efficientlyby dynamic programming.To train a CRF, the objective function to bemaximized is the penalized log-likelihood of thestate sequences given the observation sequence:2( ) ( )21log( ( | ))2Ni i kiL P s o1Kk???
?==?
=?
?>,where, { } is the labeled training da-ta.
The second sum corresponds to a zero-mean,( ) ( ),i io s<2?
-variance Gaussian prior over parameters,which facilitates optimization by making the li-kelihood surface strictly convex.CRFs generally can use real-valued functionsbut it is often required to incorporate the binaryvalued features.
A feature function1 ,( , ,k t t )f s s o t?
has a value of 0 for most casesand is only set to  1, when 1,t ts s?
are certainstates and the observation has certain properties.Here, we set parameters ?
to maximize the pe-nalized log-likelihood using Limited-memoryBFGS (Sha and Pereira, 2003) a quasi-Newtonmethod that is significantly more efficient, andwhich results in only minor changes in accuracydue to changes in ?
.We use the OpenNLP C++ based CRF++ pack-age 3 , a simple, customizable, and open sourceimplementation of CRF for segmenting /labelingsequential data.3.3 Features of Tasks C, D, E and FWe extract the gold-standard TimeBank featuresfor events and times in order to train/test theCRF.
In the present work, we mainly use thevarious combinations of the following features:(i).
Part of Speech (POS) of event terms: It de-notes the POS information of the event.
The fea-tures values may be either of ADJECTIVE,NOUN, VERB, and PREP.(ii).
Event Tense: This feature is useful to cap-ture the standard distinctions among the gram-matical categories of verbal phrases.
The tenseattribute can have values, PRESENT, PAST,3http://crfpp.sourceforge.netFUTURE, INFINITIVE, PRESPART, PAST-PART, or NONE.(iii).
Event Aspect: It denotes the aspect of theevents.
The aspect attribute may take values,PROGRESSIVE, PERFECTIVE and PERFEC-TIVE PROGRESSIVE or NONE.(iv).
Event Polarity: The polarity of an eventinstance is a required attribute represented by theboolean attribute, polarity.
If it is set to ?NEG?,the event instance is negated.
If it is set to ?POS?or not present in the annotation, the event in-stance is not negated.(v).
Event Modality: The modality attribute isonly present if there is a modal word that modi-fies the instance.(vi).
Event Class: This is denoted by the?EVENT?
tag and used to annotate those ele-ments in a text that mark the semantic eventsdescribed by it.
Typically, events are verbs butcan be nominal also.
It may belong to one of thefollowing classes:REPORTING: Describes the action of a personor an organization declaring something, narratingan event, informing about an event, etc.
For ex-ample, say, report, tell, explain, state etc.PERCEPTION: Includes events involving thephysical perception of another event.
Suchevents are typically expressed by verbs like: see,watch, glimpse, behold, view, hear, listen, over-hear etc.ASPECTUAL: Focuses on different facets ofevent history.
For example, initiation, reinitia-tion, termination, culmination, continuation etc.I_ACTION: An intentional action.
It introducesan event argument which must be in the text ex-plicitly describing an action or situation fromwhich we can infer something given its relationwith the I_ ACTION.I_STATE: Similar to the I_ACTION class.
Thisclass includes states that refer to alternative orpossible words, which can be introduced by sub-ordinated clauses, nominalizations, or untensedverb phrases (VPs).STATE: Describes circumstances in whichsomething obtains or holds true.Occurrence: Includes all of the many otherkinds of events that describe something that hap-pens or occurs in the world.(vii).
Type of temporal expression: It repre-sents the temporal relationship holding betweenevents, times, or between an event and a time ofthe event.(viii).
Event Stem:  It denotes the stem of thehead event.348(ix).
Document Creation Time: The documentcreation time of the event.4 Evaluation ResultsEach of the tasks is evaluated with the Tem-pEval-2 shared task datasets.4.1 Evaluation SchemeFor the extents of events and time expressions(tasks A and B), precision, recall and the F-measure are used as evaluation metrics, using thefollowing formulas:Precision (P) = tp/ (tp + fp)Recall (R) = tp/ (tp + fn)F-measure = 2 *(P * R)/ (P + R)Where, tp is the number of tokens that are partof an extent in both keys and response,fp is the number of tokens that are part of an ex-tent in the response but not in the key, andfn is the number of tokens that are part of an ex-tent in the key but not in the response.An even simpler evaluation metric similar tothe definition of ?accuracy?
is used to evaluatethe attributes of events and time expressions (thesecond part of tasks, A and B) and for relationtypes (tasks C through F).
The metric, henceforthreferred to as ?accuracy?, is defined as below:Number of correct answers/ Number of an-swers present in the test data4.2 ResultsFor tasks A and B, we identify a set of rules fromthe training set and apply them on the respectivetest sets.The tasks C, D, E and F are based on CRF.
Wedevelop a number of models based on CRF usingthe different features included into it.
A featurevector consisting of the subset of the availablefeatures as described in Section 2.3 is extractedfor each of <event, timex>, <event, DCT>,<event, event> and <event, event> pairs in tasksC, D, E and F, respectively.
Now, we have atraining data in the form ( , , where,  isthe ith pair along with its feature vector and  isit?s corresponding TempEval relation class.Models are built based on the training data andthe feature template.
The procedure of training issummarized below:)i iW T iWiT1.
Define the training corpus, C.2.
Extract the corresponding relation fromthe training corpus.3.
Create a file of candidate features, in-cluding lexical features derived from thetraining corpus.4.
Define a feature template.5.
Compute the CRF weights ?k for every fKusing the CRF toolkit with the trainingfile and feature template as input.During evaluation, we consider the followingfeature templates for the respective tasks:(i) Task C: Feature vector consisting of currenttoken, polarity, POS, tense, class and value;combination of token and type, combination oftense and value of the current token, combinationof aspect and type of current token, combinationof aspect, value and type of the current token.
(ii) Task D: Feature vector consisting of currenttoken and POS; combination of POS and tense ofthe current token, combination of polarity andPOS of the current token, combination of POSand aspect of current token, combination of po-larity and POS of current token, combination ofPOS, tense and aspect of the current token.(iii).
Task E: Current token, combination ofevent-class and event-id of the current token,combination of POS tags of the pair of events,combination of (tense, aspect) values of the eventpairs.(iv).
Task F: Current token, combination of POStags of the pair of events, combination of tensevalues of the event pairs, combination of the as-pect values of the event pairs, combination of theevent classes of the event pairs.Experimental results of tasks A and B are re-ported in Table 1 for English datasets.
The re-sults for task A, i.e., recognition and normaliza-tion of time expressions, yield the precision, re-call and F-measure values of 55%, 17% and26%, respectively.
For task B, i.e., event recogni-tion, the system yields precision, recall and F-measure values of 48%, 56% and 52%, respec-tively.
Event attribute identification shows theaccuracies of 98%, 98%, 30%, 95% and 53% forpolarity, mood, modality, tense, aspect and class,respectively.
These systems are the baselinemodels, and the performance can further be im-proved with a more carefully handcrafted set ofrobust rules.
In further experiments, we wouldalso like to apply machine learning methods tothese problems.349Task  precision(in %)recall(in %)F-measure(in %)A 55% 17% 26%B 48% 56% 52%Table 1.
Experimental results on tasks A and BEvaluation results on the English datasets fortasks C, D, E and F are presented in Table 2.
Ex-periments show the accuracies of 63%, 80%,56% and 56% for tasks C, D, E and F, respec-tively.
Results show that our system performsbest for task D, i.e., relationships between eventand document creation time.
The systemachieves an accuracy of 63% for task C that findsthe temporal relation between an event and a timeexpression in the same sentence.
The system per-forms quite similarly for tasks E and F. It is to benoted that there is still the room for performanceimprovement.
In the present work, we did notcarry out sufficient experiments to identify themost suitable feature templates for each of thetasks.
In future, we would experiment after se-lecting a development set for each task; and findout appropriate feature template depending uponthe performance on the development set.Task  Accuracy (in %)C 63%D 80%E 56%F 56%Table 2.
Experimental results on tasks C, D, Eand F5 Conclusion and Future WorksIn this paper, we report very preliminary resultsof our first participation in the TempEval sharedtask.
We participated in all the tasks of Tem-pEval-2, i.e., A, B, C, D, E and F for English.We develop the rule-based systems for tasks Aand B, whereas the remaining tasks are based ona machine learning approach, namely CRF.
Allour systems are still in their development stages.Evaluation results on the shared task Englishdatasets yield the precision, recall and F-measurevalues of 55%, 17% and 26%, respectively forTask A and 48%, 56% and 52%, respectively forTask B (event recognition).
Experiments on theEnglish datasets yield the accuracies of 63%,80%, 56% and 56% for tasks C, D, E and F, re-spectively.Future works include identification of moreprecise rules for tasks A and B.
We would alsolike to experiment with CRF for these two tasks.We would experiment with the various featuretemplates for tasks C, D, E and F. Future worksalso include experimentations with other ma-chine learning techniques like maximum entropyand support vector machine.ReferencesBoguraev, B. and R. K. Ando.
2005.
TimeMLCompliant Text Analysis for Temporal Rea-soning.
In Proceedings of Nineteenth Interna-tional Joint Conference on Artificial Intelli-gence (IJCAI-05), Edinburgh, Scotland, Au-gust, pages 997?1003.Chambers, N., S., Wang, and D., Jurafsky.
,2007.
Classifying Temporal Relations betweenEvents.
In Proceedings of the ACL 2007 Demoand Poster Sessions, Prague, Czech Republic,June, pages 173?176.Lafferty, J., McCallum, A., and Pereira, F.Conditional Random Fields: ProbabilisticModels for Segmenting and Labeling Se-quence Data.
In Proceedings of 18th Interna-tional Conference on Machine Learning,2001.Mani, I., B., Wellner, M., Verhagen, and J.Pustejovsky.
2007.
Three Approaches toLearning TLINKs in TimeML.
Technical Re-port CS-07-268, Computer Science Depart-ment, Brandeis University, Waltham, USA.Mani, I., Wellner, B., Verhagen, M., Lee C.M.,Pustejovsky, J.
2006.
Machine Learning ofTemporal Relation.
In Proceedings of theCOLING/ACL, Sydney, Australia, ACL.Mao, T., Li., T., Huang, D., Yang, Y.
2006.
Hy-brid Models for Chinese Named Entity Rec-ognition.
In Proceedings of the Fifth SIGHANWorkshop on Chinese Language Processing.Sha, F., Pereira, F. 2003.
Shallow  Parsing  withConditional Random Fields.
In Proceedings ofHLT-NAACL, 2003.Verhagen, M., Gaizauskas, R., Schilder, F., Hep-ple, M., Katz, G., Pustejovsky, and J.: SemE-val-2007 Task 15: TempEval Temporal Rela-tion Identification.
2007.
In Proceedings of theSemEval-2007, Prague, June 2007, pages 75-80.350
