The Influence of Argument Structure on Semantic Role AssignmentSebastian Pad?SALSADept.
of Computational LinguisticsSaarland UniversitySaarbr?ckenpado@coli.uni-sb.deGemma BoledaGLiComDept.
of Translation and InterpretingPompeu Fabra UniversityBarcelonagemma.boleda@upf.eduAbstractWe present a data and error analysis for semantic rolelabelling.
In a first experiment, we build a generic statis-tical model for semantic role assignment in the FrameNetparadigm and show that there is a high variance in perfor-mance across frames.
The main hypothesis of our paperis that this variance is to a large extent a result of differ-ences in the underlying argument structure of the pred-icates in different frames.
In a second experiment, weshow that frame uniformity, which measures argumentstructure variation, correlates well with the performancefigures, effectively explaining the variance.1 IntroductionRecent years have witnessed growing interest incorpora with semantic annotation, especially onthe semantic role (or argument structure) level.
Anumber of projects are working on producing suchcorpora through manual annotation, among whichare FrameNet (Baker et al, 1998), the PragueDependency Treebank (Hajic?ov?, 1998), Prop-Bank (Kingsbury et al, 2002), and SALSA (Erk etal., 2003).For semantic role annotation to be widely usefulfor NLP, however, robust and accurate methods forautomatic semantic role assignment are necessary.Starting with Gildea and Jurafsky (2000), a num-ber of studies have developed (almost exclusivelystatistical) models of this task, e.g.
Thompson etal.
(2003) and Fleischman et al (2003).
This year(2004), semantic role labelling served as the sharedtask at two conferences, CoNLL1 and SENSEVAL2.However, almost all studies have concentrated onthe technical aspects of the models ?
identifying in-formative feature sets and suitable statistical frame-works ?
with the goal of optimising the performanceof the models on the complete dataset.
The onlystudy we are aware of with a more detailed evalu-ation is Fleischman et al (2003), who neverthelesscome to the conclusion that either ?new features?,1http://www.lsi.upc.es/~conll04st/2http://www.clres.com/SensSemRoles.html?more data?, or ?more sophisticated models?
areneeded.The present study is a first step in pursuing thethird alternative, presenting a data and error anal-ysis for semantic role assignment in the FrameNetparadigm.
We first build two different, generic sta-tistical models for semantic role assignment, whichare fairly representative for the span of models in-vestigated in the literature.
A frame-wise evalua-tion shows that the models exhibit a large variancein performance across frames.Our hypothesis is that this variance is to a largeextent caused by differences in the underlying ar-gument structure of the predicates: Frames whichare less uniform, i.e.
whose predicates have a moreheterogeneous mapping between semantic roles andsyntactic functions, are more difficult to label auto-matically.
In order to put this hypothesis, which isintuitively very plausible, on a a firm empirical foot-ing, we investigate the relationship between frameuniformity and the variance in the data and showthat the two variables correlate.
Since argumentstructure has been investigated mostly for verbs, werestrict our study to verbal predicates.Structure of the paper.
In Section 2 we give abrief introduction to FrameNet.
Section 3 outlinesthe first experiment and discusses the variance inperformance across frames.
In Section 4, we definetwo measures of frame uniformity based on argu-ment structure, and show in our second experiment(Section 5) that they correlate with the performancefigures.
Finally, Section 6 discusses the implica-tions of our results for semantic role assignment.2 FrameNetFrameNet is a lexical resource based on Fillmore?sFrame Semantics (Fillmore, 1985).
It is designedas an ontology of frames, representations of pro-totypical situations.
Each frame provides a set ofpredicates (nouns, verbs or adjectives) which canintroduce the frame.
The semantic roles are frame-specific, since they are defined as categories of enti-ties or concepts pertaining to the particular situationa predicate evokes.The following sentences are examples for the se-mantic annotation provided in the FrameNet cor-pus for verbs in the IMPACT frame, which describesa situation in which typically ?an Impactor makessudden, forcible contact with the Impactee, or twoImpactors both ... [make] forcible contact?3 .
(1) a.
[Impactee His car] was struck [Impactor bya third vehicle].b.
[Impactor The door] slammed[Result shut].c.
[Impactors Their vehicles] collided[Place at Pond Hill].Note that the frame-specificity of semantic roles inFrameNet has important consequences for semanticrole assignment, since there is no direct way to gen-eralise across frames.
Therefore, the learning forautomatic assignment of semantic roles has to pro-ceed frame-wise.
Thus, the data sparseness prob-lem is especially acute, and automatic assignmentfor frames with no training data is very difficult (seeGildea and Jurafsky (2002)).3 Experiment 1: Frame-Wise Evaluationof Semantic Role AssignmentIn our first experiment, we perform a detailed(frame-wise) evaluation of semantic role assign-ment to discover general patterns in the data.
Ouraim is not to outperform existing models, but toreplicate the workings of existing models so that ourfindings are representative for the task as it is cur-rently addressed.
To this end, we (a) use a standarddataset, the FrameNet data, (b) model the task withtwo different statistical frameworks, and (c) keepour models as generic as possible.3.1 Data and experimental setupFor this experiment, we use 57758 manually anno-tated sentences from FrameNet (release 2), corre-sponding to all the sentences with verbal predicates(2228 lemmata from 196 frames).
Gildea and Ju-rafsky (2000) and Fleischman et al (2003) used aprevious release of the dataset with less annotatedinstances, but covered all predicates (verbs, nounsand adjectives).Data preparation.
After tagging the data withTnT (Brants, 2000), we parse them using the Collinsparsing model 3 (Collins, 1997).
We consider only3From the definition of the frame at http://www.icsi.berkeley.edu/~framenet/.
Examples adapted fromthe FrameNet data, release 2.the most probable parse for each sentence and sim-plify the resulting parse tree by removing all unarynodes.
We lemmatise the head of each constituentwith TreeTagger (Schmid, 1994).Gold standard.
We transform the FrameNetcharacter-offset annotations for semantic roles intoour constituent format by determining the maximalprojection for each semantic role, i.e.
the set of con-stituents that exactly covers the extent of the role.
Aconstituent is assigned a role iff it is in the maximumprojection of a role.Classification procedure.
The instances to beclassified are all parse tree constituents.
Since di-rect assignment of role labels to instances fails dueto the preponderance of unlabelled instances, whichmake up 86.7% of all instances, we follow Gildeaand Jurafsky (2000) in splitting the task into twosequential subtasks: first, argument recognition de-cides for each instance whether it bears a semanticrole or not; then, argument labelling assigns a labelto instances recognised as role-bearers.
For the sec-ond step, we train frame-specific classifiers, sincethe frame-specificity of roles does not allow to eas-ily combine training data from different frames.Statistical modelling.
We perform the classifica-tion twice, with two learners from different statisti-cal frameworks, in order to make our results morerepresentative for the different statistical modelsemployed so far for the task.
The first learner usesthe maximum entropy (Maxent) framework, whichhas been applied e.g.
by Fleischman et al (2003).The model is trained with the estimate software,which implements the LMVM algorithm (Malouf,2002)4.
The second learner is an instance of amemory-based learning (MBL) algorithm, the   -nearest neighbour algorithm.
We use the implemen-tation provided by TiMBL (Daelemans et al, 2003)with the recommended parameters, namely ,adopting modified value difference with gain ratiofeature weighting as similarity metric.3.2 FeaturesIn accordance with our goal of keeping our modelsgeneric, we use a set of vary (syntactic and lexical)features which more than one study in the literaturehas found helpful, without optimising the featuresfor the individual learners.Constituent features: The first type of featurerepresents properties of the constituent in question.We use the phrase type and head lemma of eachconstituent; its preposition (if available); its position4Software available for download at http://www-rohan.sdsu.edu/ malouf/pubs.htmlrelative to the predicate (left, right or overlapping);the phrase type of its mother constituent; whether itis an argument of the target, according to the parser;and the path between target and constituent as wellas its length.Sentence level features: The second type of fea-ture describes the context of the current instance.The predicate is represented by its lemma, its part ofspeech, its (heuristic) subcategorisation frame, andits governing verb.
We also compile a list of all theprepositions in the sentence.3.3 ResultsAll results in this section are averages over F scoresobtained using 10-fold cross validation.
For eachframe, we perform two evaluations, one in exactmatch and one in overlap mode.
In exact matchmode, an assignment only counts as a true positiveif it coincides exactly with the gold standard, whilein overlap mode it suffices that they are not disjoint.F scores are then computed in the usual manner.Table 1 shows the performance of the differentconfigurations over the complete dataset, and thestandard deviation of these results over all frames.To illustrate the results for individual frames, Ta-ble 2 lists frame-specific performances for five ran-domly selected frames and how they varied overcross validation runs.Maxent MBLExact Match 53.3   10.8 56.9   10.1Overlap 70.0   11.0 74.2   10.0Table 1: Overall F scores and standard deviationacross frames for Experiment 1.3.4 Analysis and DiscussionIn terms of overall results, the MBL model outper-forms the Maxent model by 3 to 4 points F-score.However, all our results lie broadly in the rangeof existing systems with a similar architecture (i.e.sequential argument identification and labelling):Gildea and Jurafsky (2002) report      , andFleischman et al (2003)   for exact matchevaluation.
We assume that our feature formulationis more suitable for the MBL model.
Also, we donot smooth the Maxent model, while we use the rec-ommended optimised parameters for TiMBL.Our most remarkable finding is the high amountof variance presented by the numbers in Table 1.Computed across frames, the standard deviationamounts to 10% to 11%, consistently across eval-uation measures and statistical frameworks.
Sincethese figures are results of a 10-fold cross valida-tion run, it is improbable that the effect is solelyExact match Maxent MBLAPPEARANCE 50.5   4.5 60.1   7.3AVOIDING 47.9   5.0 51.3   6.9JUDGM._COMM.
57.0   1.5 57.5   3.4ROBBERY 38.4   19.1 37.9   16.2WAKING_UP 60.5   11.4 64.4   11.8Overlap Maxent MBLAPPEARANCE 68.3   4.0 75.0   5.6AVOIDING 68.6   4.3 72.7   5.9JUDGM._COMM.
76.9   1.6 77.6   1.8ROBBERY 61.2   20.6 55.2   17.6WAKING_UP 75.1   9.1 77.6   7.8Total Exact Match 53.3   0.5 56.9   0.4Total Overlap 70.0   0.4 74.2   0.5Table 2: F scores and standard deviations over crossvalidation runs for five random frames (Exp.
1).due to chance splits into training and test data.
Thisassessment is supported by Table 2, which showsthat, while the performance on individual framescan vary largely (especially for small frames likeROBBERY), the average performance on all framesvaries less than 0.5% over the cross validation runs.The reasons which lead to the across-frames vari-ance warrant investigation, since they may lead tonew insights about the nature of the task in question,answering Fleischman et al?s (2003) call for bet-ter models.
Some of the plausible variables whichmight explain the variance are the number of seman-tic roles per frame, the amount of training data, andthe number of verbs per frame.However, we suggest that a fourth variable mighthave a more decisive influence.
Seen from a lin-guistic perspective, semantic role assignment is justan application of linking, i.e.
learning the regulari-ties of the relationship between semantic roles andtheir possible syntactic realisation and applying thisknowledge.
Therefore, our main hypothesis is: Themore varied the realisation possibilities of the verbsin a frame, the more difficult it is for the learner tolearn the correct linking patterns, and therefore themore error-prone semantic role assignment.
Eventhough this claim appears intuitively true, it hasnever been explicitly made nor empirically tested,and its consequences might be relevant for the de-sign of future models of semantic role assignment.As an example, compare the frame IMPACT,as exemplified by the instances in (1), with theframe INGESTION, which contains predicates suchas drink, consume or nibble.
While every sentencein (1) shows a different linking pattern, linking forINGESTION is rather straightforward: the subject isusually the Ingestor, and the direct object is an In-gestible.
This is reflected in the scores:   for IMPACT and    for INGESTION (exactmatch scores for the MBL model).The most straightforward strategy to test for thedifferent variables would be to perform multiplecorrelation analyses.
However, this approach has aserious drawback: The results are hard to interpretwhen more than one variable is significantly corre-lated with the data, and this is increasingly prob-able with higher amounts of data points.
Instead,we adopt a second strategy, namely to design a newdata set in which all variables but one are controlledfor and correlation can be tested unequivocally.
Thenew experiment is explained in Section 5.
Section 4describes the quantitative model of argument struc-ture required for the experiment.4 Argument Structure and FrameUniformityIn this section, we define the concepts we require totest our hypothesis quantitatively.
First, we defineargument structure for our data in a corpus-drivenway.
Then, we define the uniformity of a frame ac-cording to its variance in argument structure.4.1 An Empirical Model of ArgumentStructureWork in theoretical linguistics since at least Gru-ber (1965) and Jackendoff (1972) has attempted toaccount for the regularities in the syntactic reali-sation of semantic arguments.
Models for role as-signment also rely on these regularities, as can beseen from the kind of features used for this task (seeSection 3.2), which are either syntactic or lexical.Thus, current models for automatic role labellingrely on the regularities at the syntax-semantics in-terface.
Unlike theoretical work, however, they donot explicitly represent these regularities, but extractstatistical properties about them from data.The model of argument structure we develop inthis section retains the central idea of linking theory,namely to model argument structure symbolically,but deviates in two ways from traditional work inorder to bridge the gap to statistical approaches: (1),in order to emulate the situation of the learners, weuse only the data available from the FrameNet cor-pus; this excludes e.g.
the use of more detailed lexi-cal information about the predicates.
(2), to be ableto characterise not only the possibility, but also theprobability of linking patterns, we take frequencyinformation into account.Our definition proceeds in three steps.
First, wedefine the concept of a pattern, then we define theargument structure of a predicate, and finally the ar-gument structure of a frame.Patterns.
A pattern encodes the argument struc-ture information present in one annotated corpussentence.
It is an unordered set of pairs of seman-tic role and syntactic function, corresponding to allroles occurring in the sentences and their realisa-tions.
The syntactic functions used in the FrameNetcorpus are as follows5: COMP (complement), EXT(subject in a broad sense, which includes control-ling subjects), OBJ (object), MOD (modifier), GEN(genitive modifier, as ?John?
in John?s hat).
For ex-ample, Sentence (1-a) gives rise to the patternImpactee  EXT Impactor  COMP which states that the Impactee is realised as subjectand the Impactor as complement.Argument Structure for Predicates and Frames.For each verb, we collect the set of all patterns inthe annotated sentences.
The argument structure ofa verb is then a vector  , whose dimensionality is thenumber of patterns found for the frame.
Each cell is filled with the frequency with which pattern  oc-curs for the predicate, so that the vector mirrors thedistribution of the occurrences of the verb over thepossible patterns.
Finally, the set of all vectors forthe predicates in a frame is a model for the argumentstructure of the frame.The intuition behind this formalisation is that twoverbs which realise their arguments alike will showa similar distribution of patterns, and conversely, ifthey differ in their linking, these differences will bemirrored in different pattern distributions.Example.
If we only had the three sentences in (1)for the IMPACT corpus, the three occurring patternswould be {(Impactee, EXT), (Impactor, COMP)},{(Impactor, EXT), (Result, COMP)}, and {(Im-pactors, EXT), (Place, MOD)}.
The argument struc-ture of the frame would be      ffcontaining the information for the predicates strike,slam and collide, respectively.
The variation arisesfrom differences in syntactic construction (e.g.
pas-sive vs. active), but also, more significantly, fromlexical differences: collide accepts a reciprocal plu-ral subject, i.e.
an Impactors role, while strike doesnot.
This model is very simple, but achieves the5See Johnson et al (2002) for details.goal of highlighting the differences and similaritiesin the mapping between semantics and syntax fordifferent verbs in a frame.4.2 Uniformity of Argument StructureAt this point, we can define a measure to computethe uniformity of a frame from the frame?s argumentstructure, which is defined as a set of integer-valuedvectors.Similarity metrics developed for vector spacemodels are obvious candidates, but work in this areahas concentrated on metrics for comparing two vec-tors, whereas we may have an arbitrary number ofpredicates per frame.
Therefore, we borrow the con-cept of cost function from clustering, as exemplifiedby the well known sum-of-squares function usedin the k-means algorithm (see e.g.
Kaufman andRousseeuw (1990)), which estimates the ?cost?
ofa cluster as the sum of squared distances   betweeneach vectorand the cluster centroid  : 6   Under this view, a good cluster is one with a lowcost, and the goal of the clustering algorithm isto minimise the average distance to the centroid.However, for our purposes it is more convenientfor a good cluster to have a high rating.
Therefore,we turn the cost function into a ?quality?
function.By replacing the distance function with a similarityfunction, we say that a good cluster is one with ahigh average similarity to the centroid:  If we consider each frame to be a cluster and eachpredicate to be an object in the cluster, representedby the argument structure vector, the values ofcan be interpreted as a measure for frame unifor-mity: Verbs with a similar argument structure willhave similar vectors, resulting in high values offor the frame, and vice versa.What intuitively validates this formalisation isthat frames are clusters of predicates grouped to-gether on semantic grounds, i.e.
predicates in aframe share a common set of arguments.
Whatchecks is whether the mapping from semantics tosyntax is also similar.6The centroid of a cluster is ?a point in  -dimensional spacefound by averaging the measurement values along each dimen-sion?
(Kaufman and Rousseeuw, 1990, p. 112), so that it is thepoint situated at the ?center?
of the cluster.In order to obtain an actual measure for frameuniformity, we take two further steps.
First, we in-stantiatewith the cosine similarity , which hasbeen found to be appropriate for a wide range oflinguistic tasks (see e.g.
Lee (1999)) and ranges be-tween 0 (least similar) and 1 (identity): Second, we normalise the values of, whichgrow in fiff , the number of vectors, to fl ffi !, tomake them interpretable analogously to values ofthe cosine similarity.
Since this is possible in twodifferent ways, we obtain two different measures forframe uniformity.
The first one, which we call nor-malised quality-based uniformity ( "# ), simply di-vides the values byff:"#  ffflThe second measure, weighted quality-based uni-formity ( $%"# ), is a weighted average of the simi-larities.
The weights are given by the vector sizes ?in our case, the frequency of the predicates:$&"'#  ()()))flThe weighting lends more importance to well-attested predicates, limiting the amount of noise in-troduced by infrequent predicates.
Therefore, ourintuition is that $%"# should be a better measurethan "# for argument structure uniformity.5 Experiment 2: Explaining the VarianceWith Argument StructureWith two measures for the uniformity of argumentstructure at hand, we now proceed to test our mainhypothesis.5.1 Data and Experimental SetupAs argued in Section 3.4, our aim in this experimentis to control for the most plausible sources of per-formance variance and isolate the influence of argu-ment structure.To meet this condition, we perform both the ex-periments and the uniformity measure calculationon a controlled subset of the data, with the condi-tion that both the number of verbs and the numberof sentences are the same for each frame.Following the methodology in Keller and La-pata (2003), we divide the verbs into four fre-quency bands, frequency being absolute number ofannotated sentences: low (5), medium-low (12),medium-high (22), and high (38).
We set the bound-aries between the bands as the quartiles of all theverbs containing at least 5 annotated examples7 .
Foreach frame, 2 verbs in each frequency band arerandomly chosen.
This reduces our frame samplefrom 196 to 40.
We furthermore randomly select anumber of sentences for each verb which matchesthe boundaries between frequency bands, that is, allverbs in each frequency bands are artificially set tohave the same number of annotated sentences.
Thismethod assures that all frames in the experimenthave 8 verbs and 154 sentences, so that both the per-formance figures and the uniformity measures wereacquired under equal conditions.The models for semantic role assignment weretrained in the same way as for Experiment 1 (seeSection 3.1), using the same features.
We also per-formed 10-fold cross validation as before.
The uni-formity measures "# and $%"# were computed ac-cording to the definitions in Section 4.2.5.2 Results and DiscussionTable 3 shows the overall results and variance acrossframes for the new dataset.
Table 4 contains detailedperformance results (Columns 1 and 2) and unifor-mity figures (Columns 3 and 4) for five randomlydrawn frames.Maxent MBLExact Match 47.5   11.0 53.4   11.1Overlap 66.4   11.0 72.4   9.9Table 3: Overall F scores and standard deviationacross frames for Experiment 2.The overall results for the new, controlled datasetare 3 to 5 points F-score worse than in Experi-ment 1, which is a result of the artificial limitationof larger frames to fewer training examples.
Other-wise, the same tendencies hold: The memory-basedlearner again performs better than the maximum en-tropy learner, and overlap evaluation returns higherscores than exact match.
More relevantly, the datashow the same amount of variance across frames asbefore (between 10 and 11%), even though the mostplausible sources of variance are controlled for.
Thevariation over cross validation runs is somewhatlarger, but still small (2.0%/1.9% for Maxent and0.9%/0.8% for MBL, respectively).We can now test our main hypothesis through ananalysis of the correlation between performance and7We consider 5 to be the (very) minimum number of in-stances necessary to construct a representative argument struc-ture for a predicate.Exact match Maxent MBL "# $%"#BODY_MOVMT.
51.2 57.5 33.0 39.0COMMERCE 25.7 41.9 27.4 31.1MOTION 54.6 58.1 57.2 60.8PERC._ACTIVE 52.1 51.5 30.0 35.4REMOVING 59.3 60.1 58.7 64.2Overlap Maxent MBL "# $%"#BODY_MOVMT.
56.4 64.8 33.0 39.0COMMERCE 48.9 66.4 27.4 31.1MOTION 68.1 71.9 57.2 60.8PERC._ACTIVE 69.3 69.0 30.0 35.4REMOVING 76.1 77.2 58.7 64.2Table 4: F scores and frame uniformities for datafrom Exp.
2.
"'# = normalised uniformity, $%"# =weighted uniformity (in percentages).uniformity figures.
We log-transformed both vari-ables to guarantee normal distribution and used thestandard Pearson product-moment correlation coef-ficient, testing for positive correlation (higher uni-formity ?
higher performance).
The results in Ta-ble 5 show that all correlation tests are significant,and most are highly significant.
This constitutesvery good empirical support for our hypothesis.Exact match Maxent MBL"# 0.39 (  =0.007) 0.33 (  =0.04)$%"# 0.45 (  =0.002) 0.35 (  =0.01)Overlap Maxent MBL"# 0.54 (  <0.001) 0.50 (  <0.001)$%"# 0.58 (  <0.001) 0.55 (  <0.001)Table 5: Pearson coefficients and significancelevels for correlating frame performance and frameuniformity for the dataset from Experiment 2.We find that $&"# yields consistently higher cor-relation measures (and therefore more significantcorrelations) than "# , which supports our hypoth-esis from Section 4 that $%"# is a better measure forargument structure uniformity.
Recall that the in-tuition behind the weighting is to let well-attestedpredicates (those with higher frequency) have alarger influence upon the measure.
However, an in-dependent experiment for the adequacy of the mea-sures should be devised to verify this hypothesis.A comparison of the evaluation modes shows thatframe uniformity correlates more strongly with theoverlap evaluation measures than with exact match.We presume that this is due to the evaluation figuresin exact match mode being somewhat noisier.
Allother things being equal, random errors introducedduring the different processing stages (e.g.
parsingerrors) are more likely to influence the exact matchoutcome: A processing error which leads to a par-tially right argument assignment will influence theoutcome of the exact match evaluation, but not ofthe overlap evaluation.As for the two statistical frameworks, uniformityis better correlated with the Maxent model than withthe MBL model, even though MBL performs bet-ter on the evaluation.
However, this does not meanthat the correlation will become weaker for seman-tic role labelling systems performing at higher levelsof accuracy.
We compared our current models withan earlier version, which had an overall lower per-formance of about 5 points F-score.
Using the samedata, the correlation coefficients were on average0.09 points lower, and the p-values were not signif-icant for the Maxent model in exact match mode.This indicates that correlations tend to increase forbetter models.Therefore, we attribute the difference betweenthe Maxent and the MBL model to their individualproperties, or more specifically to differences in thedistribution of the performance figures for the in-dividual frames around the mean.
While they aremore evenly distributed in the MBL model, theypresent a higher peak with more outliers in the Max-ent model, which is also reflected in the slightlyhigher standard deviation of the Maxent model (cf.Tables 1 and 3).
In short, the Maxent model appearsto be more sensitive to differences in the data.Nevertheless, both models correlate strongly witheach other in both evaluation modes (      ,  <0.001 for exact match,    ,   <0.001 foroverlap).
Thus, they agree to a large extent on whichframes are easy or difficult to label.Our present results, thus, seem to indicate thatthe influence of argument structure cannot be solvedby simply improving existing systems or choosingother statistical frameworks.
Instead, there is a sys-tematic relationship between the uniformity of theargument structures of the predicates in the framesand the performance of automatic role assignment.6 Conclusion and OutlookIn this paper, we have performed an error analysisfor semantic role assignment, concentrating on therelationship between argument structure and seman-tic role assignment.
To obtain general results, wekept our models as general as possible and verifiedour results in two different statistical frameworks.In our first experiment, we showed that there isconsiderable variance across frames in the perfor-mance of semantic role assignment, and hypothe-sised that the effect was due to the varying ?diffi-culty?
of the underlying argument structure.
To testthe hypothesis, we defined a measure of frame uni-formity which modelled the variability of argumentstructure.
In a second experiment, in which we con-trolled for other plausible sources of variance, weshowed a reliable correlation between performanceand uniformity figures.The underlying reason for the difficulty of seman-tic role assignment is that FrameNet is essentiallyan ontological classification.
While the predicatesof one frame share the same semantic arguments,they can vary widely in their linking patterns.
With-out unlimited training data, automatic role assign-ment has to find and exploit regularities in linkingto achieve good results.
A priori, this can only bedone within frames, since roles are frame-specific,and there is no unique right mapping between roles.Consequently, as observed by Fleischman etal.
(2003), relatively rare constructions, such as pas-sives, are frequent error sources.
Because such con-structions have to be learnt individually for eachframe, data sparseness is a serious issue.
A similarproblem arises for lexical differences in the linkingproperties of predicates in a frame, as with the col-lide vs. strike case discussed above.
Here, the learn-ing has to take into account that the relevant linkingproperties differ between individual predicates.Our results suggest that the variance caused byargument structure will not disappear with betterclassifiers, but that the problem of inadequate gener-alisations should be addressed in a principled way.There are several possible approaches to do so.First, the classic statistical approach: Combin-ing evidence from different frame-specific roles toalleviate data sparseness.
To this end, Gildea andJurafsky (2002) developed a mapping from frame-specific to syntactic roles, but results did not im-prove much.
Baldewein et al (2004) experimentwith EM-driven generalisation, and obtain also onlymodest improvements.A second approach is to identify other levels,different from frames, at which regularities can belearnt better.
One possibility is to identify smallerunits within frames which have a more uniformstructure and which can be learnt more easily.
Sinceuniformity is defined in terms of a quality function,clustering would be the natural method to employfor this task.
However, this method is only viablefor frames with a large amount of annotation.A more general idea in this spirit is to construct anindependent classification of verbs motivated at theargument structure level (transitive, intransitive, un-accusative, etc.
), e.g.
using data sources like Levin?sverb classes (Levin, 1993).
This would allow mod-els to learn class-specific regularities and diathesisalternations more easily.
However, it is unclear ifthere is a unique level at which all relevant regulari-ties can be stated.
A more realistic variant might beto map FrameNet roles to an existing, more syntac-tically oriented role set, such as PropBank.
Theseroles can serve as an intermediate level to capturemapping regularities, and can be translated back tosemantically defined FrameNet roles when the map-ping has been accomplished.A third, different approach to semantic roleassignment is presented by Frank (2004), whopresents a syntax-semantics interface to extractsymbolic frame element projection rules from anLFG-annotated corpus and discusses strategies togeneralise over these rules.
Such an approach is,due to the finer control over the generalisation, notas susceptible to the problem described in this studyas purely statistical models.
However, it has yet tobe tested on large-scale semantic role assignment.AcknowledgementsWe are grateful to Katrin Erk, Alexander Koller andthree anonymous reviewers for helpful commentson previous versions of the paper.
We also thankthe audiences at the Prospects and Advances in theSyntax/Semantics Interface Workshop in Nancy andthe Computational Linguistics Seminar at UPF fortheir suggestions.
This work is supported by theDepartament d?Universitats, Recerca i Societat de laInformaci?
(grant 2001FI-00582, Gemma Boleda).ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Proceed-ings of COLING-ACL-98, Montreal, Canada.U.
Baldewein, K. Erk, S. Pado, and D. Prescher.
2004.Semantic role labelling with similarity-based general-isation using EM-based clustering.
In Proceedings ofSENSEVAL-3, Barcelona, Spain.Thorsten Brants.
2000.
TnT - a statistical part-of-speechtagger.
In Proceedings of ANLP-2000, Seattle, WA.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings of ACL-97, Madrid, Spain.Walter Daelemans, Jakub Zavrel, Ko van der Sloot, andAntal van den Bosch.
2003.
Timbl: Tilburg memorybased learner, version 5.0, reference guide.
Techni-cal Report ILK 03-10, Tilburg University.
Availablefrom http://ilk.uvt.nl/downloads/pub/papers/ilk0310.ps.gz.Katrin Erk, Andrea Kowalski, Sebastian Pado, and Man-fred Pinkal.
2003.
Towards a resource for lexical se-mantics: A large German corpus with extensive se-mantic annotation.
In Proceedings of ACL-03, Sap-poro, Japan.Charles J. Fillmore.
1985.
Frames and the semantics ofunderstanding.
Quaderni di Semantica, IV(2).Michael Fleischman, Namhee Kwon, and Ed Hovy.2003.
Maximum entropy models for FrameNet clas-sification.
In Proceedings of EMNLP-03, Sapporo,Japan.Anette Frank.
2004.
Generalisations over corpus-induced frame assignment rules.
In Proceedings of theLREC 2004 Workshop on Building Lexical Resourcesfrom Semantically Annotated Corpora, pages 31?38,Lissabon, Portugal.Dan Gildea and Daniel Jurafsky.
2000.
Automatic la-beling of semantic roles.
In Proceedings of ACL-00,pages pages 512?520, Hong Kong.Daniel Gildea and Dan Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28(3):245?288.Jeffrey Gruber.
1965.
Studies in lexical relations.
MITWorking Papers in Linguistics, Cambridge, MA.Eva Hajic?ov?.
1998.
Prague Dependency Treebank:From Analytic to Tectogrammatical Annotation.
InProceedings of TSD-98, Brno, Czech Republic.Ray Jackendoff.
1972.
Semantic Interpretation in Gen-erative Grammar.
MIT Press, Cambridge, MA.C.
R. Johnson, C. J. Fillmore, M. R. L. Petruck,C.
F. Baker, M. J. Ellsworth, J. Ruppenhofer,and E. J.
Wood.
2002.
FrameNet: Theory andPractice.
http://www.icsi.berkeley.edu/~framenet/book/book.html.L.
Kaufman and P. J. Rousseeuw.
1990.
Finding Groupsin Data: an Introduction to Cluster Analysis.
JohnWiley & Sons, New York City, NY.Frank Keller and Mirella Lapata.
2003.
Using the webto obtain frequencies for unseen bigrams.
Computa-tional Linguistics, 29:459?484.Paul Kingsbury, Martha Palmer, and Mitch Marcus.2002.
Adding semantic annotation to the Penn Tree-Bank.
In Proceedings of HLT-02, San Diego, CA.Lillian Lee.
1999.
Measures of distributional similar-ity.
In Proceedings of ACL-99, pages 25?32, CollegePark, MD.Beth Levin.
1993.
English Verb Classes and Alter-nations: A Preliminary Investigation.
University ofChicago Press, Chicago, IL.Rob Malouf.
2002.
A comparison of algorithms formaximum entropy parameter estimation.
In Proc.
ofCoNLL-02, Taipei, Taiwan.Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In Proceedings of NeMLaP,Manchester, UK.Cynthia A. Thompson, Roger Levy, and ChristopherManning.
2003.
A generative model for FrameNetsemantic role labeling.
In Proceedings of ECML-03,Cavtat, Croatia.
