Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 779?787,Beijing, August 2010Evaluating Dependency Representation for Event ExtractionMakoto Miwa1 Sampo Pyysalo1 Tadayoshi Hara1 Jun?ichi Tsujii1,2,31Department of Computer Science, the University of Tokyo2School of Computer Science, University of Manchester3National Center for Text Mining{mmiwa,smp,harasan,tsujii}@is.s.u-tokyo.ac.jpAbstractThe detailed analyses of sentence struc-ture provided by parsers have been appliedto address several information extractiontasks.
In a recent bio-molecular event ex-traction task, state-of-the-art performancewas achieved by systems building specif-ically on dependency representations ofparser output.
While intrinsic evalua-tions have shown significant advances inboth general and domain-specific pars-ing, the question of how these translateinto practical advantage is seldom con-sidered.
In this paper, we analyze howevent extraction performance is affectedby parser and dependency representation,further considering the relation betweenintrinsic evaluation and performance atthe extraction task.
We find that goodintrinsic evaluation results do not alwaysimply good extraction performance, andthat the types and structures of differ-ent dependency representations have spe-cific advantages and disadvantages for theevent extraction task.1 IntroductionAdvanced syntactic parsing methods have beenshown to effective for many information extrac-tion tasks.
The BioNLP 2009 Shared Task, a re-cent bio-molecular event extraction task, is onesuch task: analysis showed that the application ofa parser correlated with high rank in the task (Kimet al, 2009).
The automatic extraction of bio-molecular events from text is important for a num-ber of advanced domain applications such as path-way construction, and event extraction thus a keytask in Biomedical Natural Language Processing(BioNLP).Methods building feature representations andextraction rules around dependency representa-tions of sentence syntax have been successfullyapplied to a number of tasks in BioNLP.
Severalparsers and representations have been applied inhigh-performing methods both in domain studiesin general and in the BioNLP?09 shared task inparticular, but no direct comparison of parsers orrepresentations has been performed.
Likewise,a number of evaluation of parser outputs againstgold standard corpora have been performed in thedomain, but the broader implications of the resultsof such intrinsic evaluations are rarely considered.The BioNLP?09 shared task involved documentscontained also in the GENIA treebank (Tateisi etal., 2005), creating an opportunity for direct studyof intrinsic and task-oriented evaluation results.As the treebank can be converted into various de-pendency formats using existing format conver-sion methods, evaluation can further be extendedto cover the effects of different representations.In this this paper, we consider three types of de-pendency representation and six parsers, evaluat-ing their performance from two different aspects:dependency-based intrinsic evaluation, and effec-tiveness for bio-molecular event extraction with astate-of-the-art event extraction system.
Compar-ison of intrinsic and task-oriented evaluation re-779     !!"Figure 1: Event Extraction.sults shows that performance against gold stan-dard annotations is not always correlated withevent extraction performance.
We further findthat the dependency types and overall structuresemployed by the different dependency representa-tions have specific advantages and disadvantagesfor the event extraction task.2 Bio-molecular Event ExtractionIn this study, we adopt the event extraction taskdefined in the BioNLP 2009 Shared Task (Kim etal., 2009) as a model information extraction task.Figure 1 shows an example illustrating the taskof event extraction from a sentence.
The sharedtask provided common and consistent task defi-nitions, data sets for training and evaluation, andevaluation criteria.
The shared task defined fivesimple events (Gene expression, Transcription,Protein catabolism, Phosphorylation, and Local-ization) that take one core argument, a multi-participant binding event (Binding), and three reg-ulation events (Regulation, Positive regulation,and Negative regulation) used to capture both bi-ological regulation and general causation.
Theparticipants of simple and Binding events werespecified to be of the general Protein type, whileregulation-type events could also take other eventsas arguments, creating complex event structures.We consider two subtasks, Task 1 and Task 2,out of the three defined in the shared task.
Task 1focuses on core event extraction, and Task 2involves augmenting extracted events with sec-ondary arguments (Kim et al, 2009).
Events arerepresented with a textual trigger, type, and ar-guments, where the trigger is a span of text thatstates the event in text.
In Task 1 the event argu-ments that need to be extracted are restricted to thecore Theme and Cause roles, with secondary ar-guments corresponding to locations and sites con-sidered in Task 2.2.1 Event Extraction SystemFor evaluation, we apply the system of Miwa et al(2010b).
The system was originally developed forfinding core events (Task 1) using the native out-put of the Enju and GDep parsers.
The systemconsists of three supervised classification-basedmodules: a trigger detector, an event edge detec-tor, and a complex event detector.
The triggerdetector classifies each word into the appropriateevent types, the event edge detector classifies eachedge between an event and a candidate participantinto an argument type, and the complex event de-tector classifies event candidates constructed byall edge combinations, deciding between eventand non-event.
The system uses one-vs-all sup-port vector machines (SVMs) for classification.The system operates on one sentence at a time,building features for classification based on thesyntactic analyses for the sentence provided bythe two parsers as well as the sequence of thewords in the sentence, including the target candi-date.
The features include the constituents/wordsaround entities (triggers and proteins), the depen-dencies, and the shortest paths among the enti-ties.
The feature generation is format-independentregarding the shared properties of different for-mats, but makes use also of format-specific infor-mation when available for extracting features, in-cluding the dependency tags, word-related infor-mation (e.g.
a lexical entry in Enju format), andthe constituents and their head information.We apply here a variant of the base system in-corporating a number of modifications.
The ap-plied system performs feature selection removingtwo classes of features that were found not to bebeneficial for extraction performance, and appliesa refinement of the trigger expressions of events.The system is further extended to find also sec-ondary arguments (Task 2).
For a detailed descrip-tion of these improvements, we refer to Miwa etal.
(2010a).3 Parsers and RepresentationsSix publicly available parsers and three depen-dency formats are considered in this paper.
The780     Figure 2: Stanford basic dependency tree        	Figure 3: CoNLL-X dependency tree     Figure 4: Predicate Argument Structureparsers are GDep (Sagae and Tsujii, 2007), theBikel parser (Bikel) (Bikel, 2004), the Stanfordparser with two probabilistic context-free gram-mar (PCFG) models1 (Wall Street Journal (WSJ)model (Stanford WSJ) and ?augmented English?model (Stanford eng)) (Klein and Manning,2003), the Charniak-Johnson reranking parser,using David McClosky?s self-trained biomedi-cal parsing model (MC) (McClosky, 2009), theC&C CCG parser, adapted to biomedical text(C&C) (Rimell and Clark, 2009), and the Enjuparser with the GENIA model (Miyao et al,2009).
The formats are Stanford Dependen-cies (SD) (Figure 2), the CoNLL-X dependencyformat (CoNLL) (Figure 3) and the predicate-argument structure (PAS) format used by Enju(Figure 4).
With the exception of Stanford andEnju, the analyses of these parsers were providedby the BioNLP 2009 Shared Task organizers.The six parsers operate in a number of differentframeworks, reflected in their analyses.
GDep isa native dependency parser that produces CoNLLdependency trees, with dependency types similarto those of CoNLL 2007.
Bikel, Stanford, and MC1Experiments showed no benefit from using the lexical-ized models with the Stanford parser.Figure 5: Format conversion dependencies in sixparsers.
Formats adopted for the evaluation areshown in solid boxes.
SD: Stanford Dependencyformat, CCG: Combinatory Categorial Grammaroutput format, PTB: Penn Treebank format, andPAS: Predicate Argument Structure in Enju for-mat.are phrase-structure parsers trained on Penn Tree-bank format (PTB) style treebanks, and they pro-duce PTB trees.
C&C is a deep parser based onCombinatory Categorial Grammar (CCG), and itsnative output is in a CCG-specific format.
Theoutput of C&C can be converted into SD by arule-based conversion script (Rimell and Clark,2009).
Enju is deep parser based on Head-drivenPhrase Structure Grammar (HPSG) and producesa format containing predicate argument structuresalong with a phrase structure tree in Enju format,which can be converted into PTB format (Miyaoet al, 2009).For direct comparison and for the study of con-tribution of the formats in which the six parsersoutput their analyses to task performance, we ap-ply a number of conversions between the out-puts, shown in Figure 5.
The Enju PAS output isconverted into PTB using the method introducedby (Miyao et al, 2009).
SD is generated fromPTB by the Stanford tools (de Marneffe et al,2006), and CoNLL generated from PTB by us-ing Treebank Converter (Johansson and Nugues,2007).
With the exception of GDep, all CoNLLoutputs are generated by the conversion and thusshare dependency types.
We note that all of theseconversions can introduce some errors in the con-version process.7814 Evaluation Setting4.1 Event Extraction EvaluationEvent extraction performance is evaluated usingthe evaluation script provided by the BioNLP?09shared task organizers for the development dataset, and the online evaluation system of the taskfor the test data set2 .
Results are reported underthe official evaluation criterion of the task, i.e.
the?Approximate Span Matching/Approximate Re-cursive Matching?
criterion.The event extraction system described in Sec-tion 2.1 is used with the default settings given in(Miwa et al, 2010b).
The C-values of SVMs areset to 1.0, but the positive and negative examplesare balanced by placing more weight on the posi-tive examples.
The examples predicted with con-fidence greater than 0.5, as well as the exampleswith the most confident labels, are extracted.
Task1 and Task 2 are solved at once for the evaluation.Some of the parse results do not include wordbase forms or part-of-speech (POS) tags, whichare required by the event extraction system.
Toapply these parsers, the GENIA Tagger (Tsuruokaet al, 2005) output is adopted to add this informa-tion to the results.4.2 Dependency Representation EvaluationThe parsers are evaluated with precision, recall,and F-score for each dependency type.
We notethat the parsers may produce more fine-grainedword segmentations than that of the gold standard:for example, two words ?p70(S6)-kinase activa-tion?
in the gold standard tree (Figure 6 (a)) issegmented into five words by Enju (Figure 6 (b)).In the evaluation the word segmentations in thegold tree are used, and dependency transfer andword-based normalization are performed to matchparser outputs to these.
Dependencies related tothe segmentations are transferred to the enclosingword as follows.
If one word is segmented intoseveral segments by a parser, all the dependenciesbetween the segments are removed (Figure 6 (c))and the dependency between another word andthe segments is converted into the dependency be-tween the two words (Figure 6 (d)).2http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask/The parser outputs in SD and CoNLL can beassumed to be trees, so each node in the tree haveonly one parent node.
However, in the convertedtree nodes can have more than one parent.
Wecannot simply apply accuracy, or (un)labeled at-tachment score3.
Word-based normalization isperformed to avoid negative impact by the wordsegmentations by parsers.
When (a) and (d) inFigure 6 are compared, the counts of correct re-lations will be 1.0 (0.5 for upper NMOD and 0.5for lower NMOD in Figure 6 (d)) for the parser(precision), and the counts of correct relations willbe 1.0 (for NMOD in Figure 6 (a)) for the gold(recall).
This F-score is a good approximation ofaccuracy.4.3 GENIA treebank processingFor comparison and evaluation, the texts in theGENIA treebank (Tateisi et al, 2005) are con-verted to the various formats as follows.
To createPAS, the treebank is converted with Enju, and fortrees that fail conversion, parse results are used in-stead.
The GENIA treebank is also converted intoPTB4, and then converted into SD and CoNLL asdescribed in Section 3.
While based on manuallyannotated gold data, the converted treebanks arenot always correct due to conversion errors.5 EvaluationThis section presents evaluation results.
Intrinsicevaluation is first performed in Section 5.1.
Sec-tion 5.2 considers the effect of different SD vari-ants.
Section 5.3 presents the results of experi-ments with different parsers.
Section 5.4 showsthe performance of different parsers.
Finally, theperformance of the event extraction system is dis-cussed in context of other proposed methods forthe task in Section 5.5.5.1 Intrinsic EvaluationWe initially briefly consider the results of an in-trinsic evaluation comparing parser outputs to ref-erence data automatically derived from the goldstandard treebank.
Table 1 shows results for theparsers whose outputs could be converted into the3http://nextens.uvt.nl/?conll/4http://categorizer.tmit.bme.hu/?illes/genia ptb/782(a) Gold Word Segmen-tations (b) Parser Word Seg-mentations (c) Inner DependencyRemoval(d) Dependency Trans-ferFigure 6: Example of Word Segmentations of the words by gold and Enju and Dependency Transfer.Typed UntypedSD CoNLL SD CoNLLP R F P R F P R F P R FBikel 70.31 70.37 70.34 77.81 77.56 77.69 80.54 80.60 80.57 82.43 82.18 82.31SP WSJ 74.11 73.94 74.03 81.41 81.47 81.44 81.36 81.16 81.26 84.05 84.05 84.05SP eng 79.08 78.89 78.98 84.92 84.82 84.87 84.16 83.96 84.06 86.54 86.47 86.51C&C 80.31 78.04 79.16 - 84.91 82.28 83.57 -MC 79.56 79.63 79.60 88.13 87.87 88.00 87.43 87.50 87.47 89.81 89.42 89.62Enju 85.59 85.62 85.60 88.59 89.51 89.05 88.28 88.30 88.29 90.24 90.77 90.50Table 1: Comparison of precision, recall, and F-score results with five parsers (two models for Stanford)in two different formats on the development data set (SP abbreviates for Stanford Parser).
Resultsshown separately for evaluation including dependency types and one eliminating them.
Parser/modelcombinations above the line do not use in-domain data, others do.SD and CoNLL dependency representations us-ing the Stanford tools and Treebank Converter, re-spectively.
For Stanford, both the Penn TreebankWSJ section and ?augmented English?
(eng) mod-els were tested; the latter includes biomedical do-main data.
The Enju results for PAS are 91.48with types and 93.39 without in F-score.
GDepnot shown as its output is not compatible with thatof Treebank Converter.Despite numerical differences, the two repre-sentations and two criteria (typed/untyped) allproduce largely the same ranking of the parsers.5The evaluations also largely agree on the magni-tude of the reduction in error afforded through theuse of in-domain training data for the Stanfordparser, with all estimates falling in the 15-19%range.
Similarly, all show substantial differencesbetween the parsers, indicating e.g.
that the errorrate of Enju is 50% or less of that of Bikel.These results serve as a reference point for ex-trinsic evaluation results.
However, it should be5One larger divergence is between typed and untyped SDresults for MC.
Analysis suggest one cause is frequent errorsin tagging hyphenated noun-modifiers such as NF-kappaB asadjectives.BD CD CDP CTDTask 1 55.60 54.35 54.59 54.42Task 2 53.94 52.65 52.88 52.76Table 2: Comparison of the F-score results withdifferent SD variants on the development data setwith the MC parser.
The best score in each task isshown in bold.noted that as the parsers make use of annotateddomain training data to different extents, this eval-uation does not provide a sound basis for directcomparison of the parsers themselves.5.2 Stanford Dependency SettingSD have four different variants: basic depen-dencies (BD), collapsed dependencies (CD), col-lapsed dependencies with propagation of conjunctdependencies (CDP), and collapsed tree depen-dencies (CTD) (de Marneffe and Manning, 2008).Except for BD, these variants do not necessarilyconnect all the words in the sentence, and CD andCDP do not necessarily form a tree structure.
Ta-ble 2 shows the comparison results with the MCparser.
Dependencies are generalized by remov-ing expressions after ?
?
of the dependencies (e.g.783?
with?
in prep with) for better performance.
Wefind that basic dependencies give the best perfor-mance to event extraction, with little differencebetween the other variants.
This result is surpris-ing, as variants other than basic have features suchas the resolution of conjunctions that are specif-ically designed for practical applications.
How-ever, basic dependencies were found to consis-tently provide best performance also for the otherparsers6.
Thus, in the following evaluation, thebasic dependencies are adopted for all SD results.5.3 Parser Comparison on Event ExtractionResults with different parsers and different for-mats on the development data set are summarizedin Table 3.
Baseline results are produced by re-moving dependency information from the parseresults.
The baseline results differ between therepresentations as the word base forms and POStags produced by the GENIA tagger for use withSD and CoNLL are different from PAS, and be-cause head word information in the Enju format isused.
The evaluation finds best results for bothtasks with Enju, using its native output format.However, as discussed in Section 2.1, the treat-ment of PAS and the other two formats are slightlydifferent, this result does not necessarily indicatethat PAS is the best alternative for event extrac-tion.The Bikel and Stanford WSJ parsers, lackingmodels adapted to the biomedical domain, per-forms mostly worse than the other parsers.
Theother parsers, even though trained on the treebank,do not provide performance as high as that forusing the GENIA treebank, but, with the excep-tion of Stanford eng with CoNLL, results with theparsers are only slightly worse than results withthe treebank.
The results with the data derivedfrom the GENIA treebank can be considered asupper bounds for the parsers and formats at thetask, although conversion errors are expected tolower these bounds to some extent.
The resultssuggest that there is relative little remaining ben-efit to be gained from improving parser perfor-mance.6Collapsed tree dependencies are not evaluated on theC&C parser since the conversion is not provided.5.4 Effects of Dependency RepresentationIntrinsic evaluation results (Section 5.1) cannotbe used directly for comparing the parsers, sincesome of the parsers contain models trained on theGENIA treebank.
To investigate the effects of theevaluation results to the event extraction, we per-formed event extraction with eliminating the de-pendency types.
Table 4 summarizes the resultswith the dependency structures (without the de-pendency types) on the development data set.
In-terestingly, we find the performance increases inBikel and Stanford by eliminating the dependencytypes.
This implies that the inaccurate depen-dency types shown in Table 1 confused the eventextraction system.
SD and PAS drops more thanCoNLL, and Enju with CoNLL structures performbest in total when the dependency types are re-moved.
This result shows that the formats havetheir own strengths in finding events, and CoNLLstructure with SD or PAS types can be a good rep-resentation for the event extraction.By comparing Table 3, Table 1, and Table 4,we found that the better dependency performancedoes not always produce better event extractionperformance especially when the difference of thedependency performance is small.
MC and Enjuresults show that performance in dependency isimportant for event extraction.
SD can be betterthan CoNLL for the event extraction (shown withthe gold treebank data in Table 3), but the typesand relations of CoNLL were well predicted, andMC and Enju performed better for CoNLL thanfor SD in total.5.5 Performance of Event Extraction SystemSeveral systems are compared by the extractionperformance on the shared task test data in Ta-ble 5.
GDep and Enju with PAS are used for theevaluation, which is the same evaluation settingwith the original system by Miwa et al (2010b).The performance of the best systems in the orig-inal shared task is shown for reference ((Bjo?rneet al, 2009) in Task 1 and (Riedel et al, 2009)in Task 2).
The event extraction system performssignificantly better than the best systems in theshared task, further outperforming the originalsystem.
This shows that the comparison of theparsers is performed with a state-of-the-art sys-784Task 1 Task 2SD CoNLL PAS SD CoNLL PASBaseline 51.05 - 50.42 49.17 - 48.88Bikel 53.29 53.22 - 51.40 51.27 -Stanford WSJ 53.51 54.38 - 52.02 52.04 -Stanford eng 55.02 53.66 - 53.41 52.74 -GDep - 55.70 - - 54.37 -MC 55.60 56.01 - 53.94 54.51 -C&C 56.09 - - 54.27 - -Enju 55.48 55.74 56.57 54.06 54.37 55.31GENIA 56.34 56.09 57.94 55.04 54.57 56.40Table 3: Comparison of F-score results with six parsers in three different formats on the developmentdata set.
Results without dependency information are shown as baselines.
The results with the GENIAtreebank (converted into PTB and PAS) are shown for comparison.
The best score in each task is shownin bold, and the best score in each task and format is underlined.Task 1 Task 2SD CoNLL PAS SD CoNLL PASBikel 53.41 (+0.12) 53.92 (+0.70) - 51.59 (+0.19) 52.21 (+0.94) -Stanford WSJ 53.03 (-0.48) 54.52 (+0.14) - 51.43 (-0.59) 52.60 (-0.14) -Stanford eng 54.48 (-0.54) 54.02 (+0.36) - 52.88 (-0.53) 52.28 (+0.24) -GDep - 54.97 (-0.73) - - 53.71 (-0.66) -MC 54.22 (-1.38) 55.24 (-0.77) - 52.73 (-1.21) 53.42 (-1.09) -C&C 54.64 (-1.45) - - 52.98 (-1.29) - -Enju 53.74 (-1.74) 55.66 (-0.08) 55.23 (-1.34) 52.29 (-1.77) 53.97 (-0.40) 53.69 (-1.62)GENIA 55.79 (-0.55) 55.64 (-0.45) 56.42 (-1.52) 54.17 (-0.87) 53.83 (-0.74) 55.34 (-1.06)Table 4: Comparison of F-score results with six parsers in three different dependency structures (with-out the dependency types) on the development data set.
The changes from Table 3 are shown.Simple Binding Regulation AllTask 1Ours 66.84 / 78.22 / 72.08 48.70 / 52.65 / 50.60 38.48 / 55.06 / 45.30 50.13 / 64.16 / 56.28Miwa 65.31 / 76.44 / 70.44 52.16 / 53.08 / 52.62 35.93 / 46.66 / 40.60 48.62 / 58.96 / 53.29Bjo?rne 64.21 / 77.45 / 70.21 40.06 / 49.82 / 44.41 35.63 / 45.87 / 40.11 46.73 / 58.48 / 51.95Riedel N/A 23.05 / 48.19 / 31.19 26.32 / 41.81 / 32.30 36.90 / 55.59 / 44.35Baseline 62.94 / 68.38 / 65.55 48.41 / 34.50 / 40.29 29.40 / 40.00 / 33.89 43.93 / 50.11 / 46.82Task 2Ours 65.43 / 75.56 / 70.13 46.42 / 50.31 / 48.29 38.18 / 54.45 / 44.89 49.20 / 62.57 / 55.09Riedel N/A 22.35 / 46.99 / 30.29 25.75 / 40.75 / 31.56 35.86 / 54.08 / 43.12Baseline 60.88 / 63.78 / 62.30 44.99 / 31.78 / 37.25 29.07 / 39.52 / 33.50 42.62 / 47.84 / 45.08Table 5: Comparison of Recall / Precision / F-score results on the test data set.
Results on simple,binding, regulation, and all events are shown.
GDep and Enju with PAS are used.
Results by Miwa etal.
(2010b), Bjo?rne et al (2009), Riedel et al (2009), and Baseline for Task 1 and Task 2 are shown forcomparison.
Baseline results are produced by removing dependency information from the parse resultsof GDep and Enju.
The best score in each result is shown in bold.tem.6 Related WorkMany approaches for parser comparison havebeen proposed, and most comparisons have usedgold treebanks with intermediate formats (Cleggand Shepherd, 2007; Pyysalo et al, 2007).
Parsercomparison has also been proposed on specifictasks such as unbounded dependencies (Rimellet al, 2009) and textual entailment ( ?Onder Eker,2009)7.
Among them, application-oriented parsercomparison across several formats was first intro-duced by Miyao et al (2009), who compared eightparsers and five formats for the protein-protein in-teraction (PPI) extraction task.
PPI extraction, the7http://pete.yuret.com/785recognition of binary relations of between pro-teins, is one of the most basic information ex-traction tasks in the BioNLP field.
Our findingsdo not conflict with those of Miyao et al Eventextraction can be viewed as an additional extrin-sic evaluation task for syntactic parsers, providingmore reliable and evaluation and a broader per-spective into parser performance.
An additionaladvantage of application-oriented evaluation onBioNLP shared task data is the availability of amanually annotated gold standard treebank, theGENIA treebank, that covers the same set of ab-stracts as the task data.
This allows the gold tree-bank to be considered as an evaluation standard,in addition to comparison of performance in theprimary task.7 ConclusionWe compared six parsers and three formats on abio-molecular event extraction task with a state-of-the-art event extraction system from two dif-ferent aspects: dependency-based intrinsic eval-uation and task-based extrinsic evaluation.
Thespecific task considered was the BioNLP sharedtask, allowing the use of the GENIA treebank asa gold standard parse reference.
Five of the sixconsidered parsers were applied using biomedi-cal models trained on the GENIA treebank, andthey were found to produce similar performance.The comparison of the parsers from two aspectsshowed slightly different results, and and thedependency representations have advantages anddisadvantages for the event extraction task.The contributions of this paper are 1) the com-parison of intrinsic and extrinsic evaluation onseveral commonly used parsers with a state-of-the-art system, and 2) demonstration of the lim-itation and possibility of the parser and systemimprovement on the task.
One limitation of thisstudy is that the comparison between the parsersis not perfect, as the parsers are used with the pro-vided models, the format conversions miss someinformation from the original formats, and resultswith different formats depend on the ability ofthe event extraction system to take advantage oftheir strengths.
To maximize comparability, thesystem was designed to extract features identi-cally from similar parts of the dependency-basedformats, further adding information provided byother formats, such as the lexical entries of theEnju format, from external resources.
The resultsof this paper are expected to be useful as a guidenot only for parser selection for biomedical infor-mation extraction but also for the development ofevent extraction systems.The comparison in the present evaluation islimited to the dependency representation.
As fu-ture work, it would be informative to extend thecomparison to other syntactic representation, suchas the PTB format.
Finally, the evaluation showedthat the system fails to recover approximately40% of events even when provided with manuallyannotated treebank data, showing that other meth-ods and resources need to be adopted to furtherimprove bio-molecular event extraction systems.Such improvement is left as future work.AcknowledgmentsThis work was partially supported by Grant-in-Aid for Specially Promoted Research (MEXT,Japan), Genome Network Project (MEXT, Japan),and Scientific Research (C) (General) (MEXT,Japan).786ReferencesBikel, Daniel M. 2004.
A distributional analysis of alexicalized statistical parsing model.
In In EMNLP,pages 182?189.Bjo?rne, Jari, Juho Heimonen, Filip Ginter, AnttiAirola, Tapio Pahikkala, and Tapio Salakoski.
2009.Extracting complex biological events with richgraph-based feature sets.
In Proceedings of theBioNLP?09 Shared Task on Event Extraction, pages10?18.Clegg, Andrew B. and Adrian J. Shepherd.
2007.Benchmarking natural-language parsers for biolog-ical applications using dependency graphs.
BMCBioinformatics, 8.de Marneffe, Marie-Catherine and Christopher D.Manning.
2008.
Stanford typed dependencies man-ual.
Technical report, September.de Marneffe, Marie-Catherine, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of the IEEE / ACL 2006 Workshop onSpoken Language Technology.Johansson, Richard and Pierre Nugues.
2007.
Ex-tended constituent-to-dependency conversion forEnglish.
In Proceedings of NODALIDA 2007, Tartu,Estonia, May 25-26.Kim, Jin-Dong, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overviewof bionlp?09 shared task on event extraction.
InBioNLP ?09: Proceedings of the Workshop onBioNLP, pages 1?9.Klein, Dan and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In ACL ?03: Proceed-ings of the 41st Annual Meeting on Association forComputational Linguistics, pages 423?430, Morris-town, NJ, USA.
Association for Computational Lin-guistics.McClosky, David.
2009.
Any Domain Parsing: Au-tomatic Domain Adaptation for Natural LanguageParsing.
Ph.D. thesis, Department of Computer Sci-ence, Brown University.Miwa, Makoto, Sampo Pyysalo, Tadayoshi Hara, andJun?ichi Tsujii.
2010a.
A comparative study of syn-tactic parsers for event extraction.
In BioNLP2010:Proceedings of the Workshop on BioNLP, Uppsala,Sweden, July.Miwa, Makoto, Rune S?tre, Jin-Dong Kim, andJun?ichi Tsujii.
2010b.
Event extraction with com-plex event classification using rich features.
Jour-nal of Bioinformatics and Computational Biology(JBCB), 8(1):131?146, February.Miyao, Yusuke, Kenji Sagae, Rune S?tre, TakuyaMatsuzaki, and Jun ichi Tsujii.
2009.
Evalu-ating contributions of natural language parsers toprotein-protein interaction extraction.
Bioinformat-ics, 25(3):394?400.
?Onder Eker.
2009.
Parser evaluation using textualentailments.
Master?s thesis, Bog?azic?i ?Universitesi,August.Pyysalo, Sampo, Filip Ginter, Veronika Laippala, Ka-tri Haverinen, Juho Heimonen, and Tapio Salakoski.2007.
On the unification of syntactic annotationsunder the stanford dependency scheme: A casestudy on bioinfer and genia.
In Biological, transla-tional, and clinical language processing, pages 25?32, Prague, Czech Republic, June.
Association forComputational Linguistics.Riedel, Sebastian, Hong-Woo Chun, Toshihisa Takagi,and Jun?ichi Tsujii.
2009.
A markov logic approachto bio-molecular event extraction.
In BioNLP ?09:Proceedings of the Workshop on BioNLP, pages 41?49, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Rimell, Laura and Stephen Clark.
2009.
Porting alexicalized-grammar parser to the biomedical do-main.
J. of Biomedical Informatics, 42(5):852?865.Rimell, Laura, Stephen Clark, and Mark Steedman.2009.
Unbounded dependency recovery for parserevaluation.
In Proceedings of the 2009 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 813?821, Singapore, August.
Asso-ciation for Computational Linguistics.Sagae, Kenji and Jun?ichi Tsujii.
2007.
Dependencyparsing and domain adaptation with LR models andparser ensembles.
In EMNLP-CoNLL 2007.Tateisi, Yuka, Akane Yakushiji, Tomoko Ohta, andJunfichi Tsujii.
2005.
Syntax annotation for the ge-nia corpus.
In Proceedings of the IJCNLP 2005,Companion volume, pages 222?227, Jeju Island,Korea, October.Tsuruoka, Yoshimasa, Yuka Tateishi, Jin-Dong Kim,Tomoko Ohta, John McNaught, Sophia Ananiadou,and Jun?ichi Tsujii.
2005.
Developing a robustpart-of-speech tagger for biomedical text.
In Boza-nis, Panayiotis and Elias N. Houstis, editors, Pan-hellenic Conference on Informatics, volume 3746 ofLecture Notes in Computer Science, pages 382?392.Springer.787
