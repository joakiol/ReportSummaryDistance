Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 186?194,Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational LinguisticsMachine Translation and Monolingual Postediting:The AFRL WMT-14 SystemLane O.B.
SchwartzAir Force Research Laboratorylane.schwartz@us.af.milTimothy AndersonAir Force Research Laboratorytimothy.anderson.20@us.af.milJeremy GwinnupSRA International?jeremy.gwinnup.ctr@us.af.milKatherine M. YoungN-Space Analysis LLC?katherine.young.1.ctr@us.af.milAbstractThis paper describes the AFRL sta-tistical MT system and the improve-ments that were developed during theWMT14 evaluation campaign.
As partof these efforts we experimented witha number of extensions to the stan-dard phrase-based model that improveperformance on Russian to Englishand Hindi to English translation tasks.In addition, we describe our effortsto make use of monolingual Englishspeakers to correct the output of ma-chine translation, and present the re-sults of monolingual postediting of theentire 3003 sentences of the WMT14Russian-English test set.1 IntroductionAs part of the 2014 Workshop on MachineTranslation (WMT14) shared translation task,the human language technology team at theAir Force Research Laboratory participatedin two language pairs: Russian-English andHindi-English.
Our machine translation sys-tem represents enhancements to our systemfrom IWSLT 2013 (Kazi et al., 2013).
In thispaper, we focus on enhancements to our pro-cedures with regard to data processing and thehandling of unknown words.In addition, we describe our efforts to makeuse of monolingual English speakers to correctthe output of machine translation, and presentthe results of monolingual postediting of theentire 3003 sentences of the WMT14 Russian-English test set.
Using a binary adequacy clas-sification, we evaluate the entire postedited?This work is sponsored by the Air Force ResearchLaboratory under Air Force contract FA-8650-09-D-6939-029.test set for correctness against the referencetranslations.
Using bilingual judges, we fur-ther evaluate a substantial subset of the post-edited test set using a more fine-grained ade-quacy metric; using this metric, we show thatmonolingual posteditors can successfully pro-duce postedited translations that convey all ormost of the meaning of the original source sen-tence in up to 87.8% of sentences.2 System DescriptionWe submitted systems for the Russian-to-English and Hindi-to-English MT sharedtasks.
In all submitted systems, we use thephrase-based moses decoder (Koehn et al.,2007).
We used only the constrained data sup-plied by the evaluation for each language pairfor training our systems.2.1 Data PreparationBefore training our systems, a cleaning passwas performed on all data.
Unicode charac-ters in the unallocated and private use rangeswere all removed, along with C0 and C1 con-trol characters, zero-width and non-breakingspaces and joiners, directionality and para-graph markers.2.1.1 Hindi ProcessingThe HindEnCorp corpus (Bojar et al., 2014)is distributed in tokenized form; in order toensure a uniform tokenization standard acrossall of our data, we began by detokenized thisdata using the Moses detokenization scripts.In addition to normalizing various extendedLatin punctuation marks to their Basic Latinequivalents, following Bojar et al.
(2010) wenormalized Devanagari Danda (U+0964),Double Danda (U+0965), and Abbrevia-tion Sign (U+0970) punctuation marks toLatin Full Stop (U+002E), any Devana-186gari Digit to the equivalent ASCII Digit,and decomposed all Hindi data into UnicodeNormalization Form D (Davis and Whistler,2013) using charlint.1 In addition, we per-formed Hindi diacritic and vowel normaliza-tion, following Larkey et al.
(2003).Since no Hindi-English development testset was provided in WMT14, we randomlysampled 1500 sentence pairs from the Hindi-English parallel training data to serve this pur-pose.
Upon discovering duplicate sentences inthe corpus, 552 sentences that overlapped withthe training portion were removed from thesample, leaving a development test set of 948sentences.2.1.2 Russian ProcessingThe Russian sentences contained many exam-ples of mixed-character spelling, in which bothLatin and Cyrillic characters are used in a sin-gle word, relying on the visual similarity of thecharacters.
For example, although the firstletter and last letter in the word c?????
ap-pear visually indistinguishable, we find thatthe former is U+0063 Latin Small LetterC and the latter is U+0441 Cyrillic SmallLetter Es.
We created a spelling normal-ization program to convert these words to allCyrillic or all Latin characters, with a pref-erence for all-Cyrillic conversion if possible.Normalization also removes U+0301 Combin-ing Acute Accent ( ??)
and converts U+00F2Latin Small Letter O with Grave (?
)and U+00F3 Latin Small Letter O withAcute (?)
to the unaccented U+043E Cyril-lic Small Letter O (?
).The Russian-English Common Crawl par-allel corpus (Smith et al., 2013) is relativelynoisy.
A number of Russian source sentencesare incorrectly encoded using characters in theLatin-1 supplement block; we correct thesesentences by shifting these characters aheadby 350hex code points into the correct Cyrilliccharacter range.2We examine the Common Crawl parallelsentences and mark for removal any non-Russian source sentences and non-English tar-get sentences.
Target sentences were markedas non-English if more than half of the charac-1http://www.w3.org/International/charlint2For example: ????????
??
???????
??????
?
????.
?becomes ????????
??
???????
??????
?
????.
?ters in the sentence were non-Latin, or if morethan half of the words were unknown to theaspell English spelling correction program,not counting short words, which frequentlyoccur as (possibly false) cognates across lan-guages (English die vs. German die, Englishon vs. French on, for example).
Becauseaspell does not recognize some proper names,brand names, and borrowed words as knownEnglish words, this method incorrectly flagsfor removal some English sentences which havea high proportion of these types of words.Source sentences were marked as non-Russian if less than one-third of the charac-ters were within the Russian Cyrillic range, orif non-Russian characters equal or outnumberRussian characters and the sentence containsno contiguous sequence of at least three Rus-sian characters.
Some portions of the Cyrilliccharacter set are not used in typical Russiantext; source sentences were therefore markedfor removal if they contained Cyrillic exten-sion characters Ukrainian I (?
?
), Yi(?
?
),Ghe With Upturn (?
?)
or Ie (?
?)
in ei-ther upper- or lowercase, with exceptions forU+0406 Ukrainian I (?)
in Roman numeralsand for U+0491 Ghe With Upturn (?)
whenit occurred as an encoding error artifact.3Sentence pairs where the source was identi-fied as non-Russian or the target was identifiedas non-English were removed from the parallelcorpus.
Overall, 12% of the parallel sentenceswere excluded based on a non-Russian sourcesentence (94k instances) or a non-English tar-get sentence (11.8k instances).Our Russian-English parallel training dataincludes a parallel corpus extracted fromWikipedia headlines (Ammar et al., 2013),provided as part of the WMT14 shared trans-lation task.
Two files in this parallel cor-pus (wiki.ru-en and guessed-names.ru-en)contained some overlapping data.
We re-moved 6415 duplicate lines within wiki.ru-en(about 1.4%), and removed 94 lines ofguessed-names.ru-en that were alreadypresent in wiki.ru-en (about 0.17%).3Specifically, we allowed lines containing ?
where itappears as an encoding error in place of an apostro-phe within English words.
For example: ??????
TheKelly Family I?m So Happy ????????????
???
Lyrics-Keeper.
?1872.2 Machine TranslationOur baseline system is a variant of the MIT-LL/AFRL IWSLT 2013 system (Kazi et al.,2013) with some modifications to the trainingand decoding processes.2.2.1 Phrase Table TrainingFor our Russian-English system, we traineda phrase table using the Moses ExperimentManagement System (Koehn, 2010b), withmgiza (Gao and Vogel, 2008) as the wordaligner; this phrase table was trained using theRussian-English Common Crawl, News Com-mentary, Yandex (Bojar et al., 2013), andWikipedia headlines parallel corpora.The phrase table for our Hindi-English sys-tem was trained using a similar in-house train-ing pipeline, making use of the HindEnCorpand Wikipedia headlines parallel corpora.2.2.2 Language Model TrainingDuring the training process we built n-gramlanguage models (LMs) for use in decodingand rescoring using the KenLM language mod-elling toolkit (Heafield et al., 2013).
Class-based language models (Brown et al., 1992)were also trained, for later use in n-best listrescoring, using the SRILM language mod-elling toolkit (Stolcke, 2002).We trained a 6-gram language model from the LDC EnglishGigaword Fifth Edition, for use in both theHindi-English and Russian-English systems.All language models were binarized in orderto reduce model disk usage and loading time.For the Russian-to-English task, we concate-nated the English portion of the parallel train-ing data for the WMT 2014 shared transla-tion task (Common Crawl, News Commen-tary, Wiki Headlines and Yandex corpora) inaddition to the shared task English monolin-gual training data (Europarl, News Commen-tary and News Crawl corpora) into a trainingset for a large 6-gram language model usingKenLM.
We denote this model as ?BigLM?.
In-dividual 6-gram models were also constructedfrom each respective corpus.For the Hindi-to-English task, individual 6-gram models were constructed from the re-spective English portions of the HindEnCorpand Wikipedia headlines parallel corpora, andfrom the monolingual English sections of theEuroparl and News Crawl corpora.Decoding FeaturesP(f | e)P(e | f)Pw(f | e)Pw(e | f)Phrase PenaltyLexical BackoffWord PenaltyDistortion ModelUnknown Word PenaltyLexicalized Reordering ModelOperation Sequence ModelRescoring FeaturesPclass(E) ?
7-gram class-based LMPlex(F | E) ?
sentence-level averagedlexical translation scoreTable 1: Models used in log-linear combina-tion2.2.3 Decoding, n-best List Rescoring,and OptimizationWe decode using the phrase-based moses de-coder (Koehn et al., 2007), choosing the besttranslation for each source sentence accordingto a linear combination of decoding features:?E = arg maxE?
?r?rhr(E,F) (1)We make use of a standard set of decodingfeatures, listed in Table 1.
In contrast to ourIWSLT 2013 system, all experiments submit-ted to this year?s WMT evaluation made useof version 2.1 of moses, and incorporated ad-ditional decoding features, namely the Oper-ation Sequence Model (Durrani et al., 2011)and Lexicalized Reordering Model (Tillman,2004; Galley and Manning, 2008).Following Shen et al.
(2006), we usethe word-level lexical translation probabili-ties Pw(fj| ei) to obtain a sentence-level aver-aged lexical translation score (Eq.
2), which isadded as an additional feature to each n-bestlist entry.Plex(F | E) =?j?1...J1I + 1?i?1...IPw(fj| ei)(2)Shen et al.
(2006) use the term ?IBM model 1score?
to describe the value calculated in Eq.2.
While the lexical probability distribution188from IBM Model 1 (Brown et al., 1993) couldin fact be used as the Pw(fj| ei) in Eq.
2, inpractice we use a variant of Pw(fj| ei) definedby Koehn et al.
(2003).We also add a 7-gram class language modelscore Pclass(E) (Brown et al., 1992) as an ad-ditional feature of each n-best list entry.
Afteradding these features to each translation in ann-best list, Eq.
1 is applied, rescoring the en-tries to extract new 1-best translations.To optimize system performance we trainscaling factors, ?r, for both decoding andrescoring features so as to minimize an ob-jective error criterion.
In our systems we useDREM (Kazi et al., 2013) or PRO (Hopkinsand May, 2011) to perform this optimization.For development data during optimization,we used newstest2013 for the Russian-to-English task and newsdev2014 for the Hindi-to-English task supplied by WMT14.2.2.4 Unknown WordsFor the Hindi-to-English task, unknown wordswere marked during the decoding process andwere transliterated by the icu4j Devanagari-to-Latin transliterator.4For the Russian-to-English task, we selec-tively stemmed and inflected input words notfound in the phrase table.
Each input sentencewas examined to identify any source wordswhich did not occur as a phrase of length 1in the phrase table.
For each such unknownword, we used treetagger (Schmid, 1994;Schmid, 1995) to identify the part of speech,and then we removed inflectional endings toderive a stem.
We applied all possible Rus-sian inflectional endings for the given part ofspeech; if an inflected form of the unknownword could be found as a stand-alone phrasein the phrase table, that form was used to re-place the unknown word in the original Rus-sian file.
If multiple candidates were found,we used the one with the highest frequency ofoccurrence in the training data.
This processreplaces words that we know we cannot trans-late with semantically similar words that wecan translate, replacing unknown words like???????
?photon?
(instrumental case) witha known morphological variant ?????
?pho-ton?
(nominative case) that is found in the4http://site.icu-project.orgBLEU BLEU-casedSystem1 hi-en 13.1 12.12 ru-en 32.0 30.83 ru-en 32.2 31.04 ru-en 31.5 30.35 ru-en 33.0 31.1Table 2: Translation results, as measured byBLEU (Papineni et al., 2002).phrase table.
Selective stemming of just theunknown words allows us to retain informa-tion that would be lost if we applied stemmingto all the data.Any remaining unknown words weretransliterated as a post-process, using asimple letter-mapping from Cyrillic charactersto Latin characters representing their typicalsounds.2.3 MT ResultsOur best Hindi-English system fornewstest2014 is listed in Table 2 as System1.
This system uses a combination of 6-gramlanguage models built from HindEnCorp,News Commentary, Europarl, and NewsCrawl corpora.
Transliteration of unknownwords was performed after decoding butbefore n-best list rescoring.System 2 is Russian-English, and handlesunknown words following ?2.2.4.
We used asindependent decoder features separate 6-gramLMs trained respectively on Common Crawl,Europarl, News Crawl, Wiki headlines andYandex corpora.
This system was optimizedwith DREM.
No rescoring was performed.
Wealso tested a variant of System 2 which didperform rescoring.
That variant (not listed inTable 2) performed worse than System 2, withscores of 31.2 BLEU and 30.1 BLEU-cased.System 3, our best Russian-English systemfor newstest2014, used the BigLM and Giga-word language models (see ?2.2.2) as indepen-dent decoder features and was optimized withDREM.
Rescoring was performed after de-coding.
Instead of following ?2.2.4, unknownwords were dropped to maximize BLEU score.We note that the optimizer assigned weights of0.314 and 0.003 to the BigLM and Gigawordmodels, respectively, suggesting that the opti-mizer found the BigLM to be much more use-189Figure 1: Posteditor user interfaceDocuments Sentences WordsPosteditor1 44 950 200862 21 280 60313 25 476 101944 25 298 61645 20 301 58096 15 210 44337 10 140 26508 15 348 6743All 175 3003 62110Table 3: Number of documents within theRussian-English test set processed by eachmonolingual human posteditor.
Number ofmachine translated sentences processed byeach posteditor is also listed, along with thetotal number of words in the correspondingRussian source sentences.# ?
# ?
% ?Posteditor1 684 266 72.0%2 190 90 67.9%3 308 168 64.7%4 162 136 54.4%5 194 107 64.5%6 94 116 44.8%7 88 52 62.9%8 196 152 56.3%All 1916 1087 63.8%Table 4: For each monolingual posteditor, thenumber and percentage of sentences judged tobe correct (?)
versus incorrect (?)
accordingto a monolingual human judge.612 The postedited translation is superiorto the reference translation10 The meaning of the Russian sourcesentence is fully conveyed in the post-edited translation8 Most of the meaning is conveyed6 Misunderstands the sentence in a ma-jor way; or has many small mistakes4 Very little meaning is conveyed2 The translation makes no sense at allTable 5: Evaluation guidelines for bilingualhuman judges, adapted from Albrecht et al.
(2009).Evaluation Category2 4 6 8 10 120.2% 2.2% 9.8% 24.7% 60.2% 2.8%Table 6: Percentage of evaluated sentencesjudged to be in each category by a bilingualjudge.
Category labels are defined in Table 5.Evaluation Category2 4 6 8 10 12# ?
2 20 72 89 79 4# ?
0 1 21 146 493 23% ?
0% 5% 23% 62% 86% 85%Table 7: Number of sentences in each evalu-ation category (see Table 5) that were judgedas correct (?)
or incorrect (?)
according to amonolingual human judge.190ful than the Gigaword LM.
This intuition wasconfirmed by an experimental variation of Sys-tem 3 (not listed in Table 2) where we omittedthe BigLM; that variant performed substan-tially worse, with scores of 25.3 BLEU and24.2 BLEU-cased.
We also tested a variantof System 3 which did not perform rescoring;that variant (also not listed in Table 2) per-formed worse, with scores of 31.7 BLEU and30.6 BLEU-cased.The results of monolingual postediting (see?3) of System 4 (a variant of System 2 tunedusing PRO) uncased output is System 5.
Dueto time constraints, the monolingual post-editing experiments in ?3 were conducted (us-ing the machine translation results from Sys-tem 4) before the results of Systems 2 and 3were available.
The Moses recaser was appliedin all experiments except for System 5.3 Monolingual PosteditingPostediting is the process whereby a humanuser corrects the output of a machine trans-lation system.
The use of basic posteditingtools by bilingual human translators has beenshown to yield substantial increases in termsof productivity (Plitt and Masselot, 2010) aswell as improvements in translation quality(Green et al., 2013) when compared to bilin-gual human translators working without as-sistance from machine translation and post-editing tools.
More sophisticated interactiveinterfaces (Langlais et al., 2000; Barrachinaet al., 2009; Koehn, 2009b; Denkowski andLavie, 2012) may also provide benefit (Koehn,2009a).We hypothesize that for at least some lan-guage pairs, monolingual posteditors with noknowledge of the source language can success-fully translate a substantial fraction of testsentences.
We expect this to be the case espe-cially when the monolingual humans are do-main experts with regard to the documents tobe translated.
If this hypothesis is confirmed,this could allow for multi-stage translationworkflows, where less highly skilled monolin-gual posteditors triage the translation pro-cess, postediting many of the sentences, whileforwarding on the most difficult sentences tomore highly skilled bilingual translators.Small-scale studies have suggested thatmonolingual human posteditors, workingwithout knowledge of the source language, canalso improve the quality of machine trans-lation output (Callison-Burch, 2005; Koehn,2010a; Mitchell et al., 2013), especially if well-designed tools provide automated linguisticanalysis of source sentences (Albrecht et al.,2009).In this study, we designed a simple user in-terface for postediting that presents the userwith the source sentence, machine transla-tion, and word alignments for each sentencein a test document (Figure 1).
While it mayseem counter-intuitive to present monolingualposteditors with the source sentence, we foundthat the presence of alignment links betweensource words and target words can in fact aida monolingual posteditor, especially with re-gard to correcting word order.
For example, inour experiments posteditors encountered somesentences where a word or phrase was enclosedwithin bracketing punctuation marks (such asquotation marks, commas, or parentheses) inthe source sentence, and the machine transla-tion system incorrectly reordered the word orphrase outside the enclosing punctuation; byexamining the alignment links the posteditorswere able to correct such reordering mistakes.The Russian-English test set comprises 175documents in the news domain, totaling 3003sentences.
We assigned each test documentto one of 8 monolingual5 posteditors (Table3).
The postediting tool did not record tim-ing information.
However, several posteditorsinformally reported that they were able to pro-cess on average approximately four documentsper hour; if accurate, this would indicate aprocessing speed of around one sentence perminute.Following Koehn (2010a), we evaluatedpostedited translation quality according toa binary adequacy metric, as judged by amonolingual English speaker6 against the En-5All posteditors are native English speakers.
Poste-ditors 2 and 3 know Chinese and Arabic, respectively,but not Russian.
Posteditor 8 understands the Cyrilliccharacter set and has a minimal Russian vocabularyfrom two undergraduate semesters of Russian takenseveral years ago.6All monolingual adequacy judgements were per-formed by Posteditor 1.
Additional analysis of Post-editor 1?s 950 postedited translations were indepen-dently judged by bilingual judges against the referenceand the source sentence (Table 7).191glish references.
In this metric, incorrectspellings of transliterated proper names werenot grounds to judge as incorrect an otherwiseadequate postedited translation.
Binary ade-quacy results are shown in Table 4; we observethat correctness varied widely between poste-ditors (44.8?72.0%), and between documents.Interestingly, several posteditors self-reported that they could tell which documentswere originally written in English and weresubsequently translated into Russian, andwhich were originally written in Russian,based on observations that sentences fromthe latter were substantially more difficult topostedit.
Once per-document source languagedata is released by WMT14 organizers, weintend to examine translation quality on aper-document basis and test whether postedi-tors did indeed perform worse on documentswhich originated in Russian.Using bilingual judges, we further evaluate asubstantial subset of the postedited test set us-ing a more fine-grained adequacy metric (Ta-ble 5).
Because of time constraints, only thefirst 950 postedited sentences of the test set6were evaluated in this manner.
Each sentencewas evaluated by one of two bilingual humanjudges.
In addition to the 2-10 point scale ofAlbrecht et al.
(2009), judges were instructedto indicate (with a score of 12) any sentenceswhere the postedited machine translation wassuperior to the reference translation.
Usingthis metric, we show in Table 6 that monolin-gual posteditors can successfully produce post-edited translations that convey all or most ofthe meaning of the original source sentence inup to 87.8% of sentences; this includes 2.8%which were superior to the reference.Finally, as part of WMT14, the results ofour Systems 1 (hi-en), 3 (ru-en), and 5 (post-edited ru-en) were ranked by monolingual hu-man judges against the machine translationoutput of other WMT14 participants.
Thesejudgements are reported in WMT (2014).Due to time constraints, the machine trans-lations (from System 4) presented to postedi-tors were not evaluated by human judges, nei-ther using our 12-point evaluation scale noras part of the WMT human evaluation rank-ings.
However, to enable such evaluation byfuture researchers, and to enable replication ofour experimental evaluation, the System 4 ma-chine translations, the postedited translations,and the monolingual and bilingual evaluationresults are released as supplementary data toaccompany this paper.4 ConclusionIn this paper, we present data preparation andlanguage-specific processing techniques for ourHindi-English and Russian-English submis-sions to the 2014 Workshop on Machine Trans-lation (WMT14) shared translation task.
Oursubmissions examine the effectiveness of han-dling various monolingual target language cor-pora as individual component language mod-els (System 2) or alternatively, concatenatedtogether into a single big language model (Sys-tem 3).
We also examine the utility of n-best list rescoring using class language modeland lexicalized translation model rescoringfeatures.In addition, we present the results of mono-lingual postediting of the entire 3003 sentencesof the WMT14 Russian-English test set.
Post-editing was performed by monolingual Englishspeakers, who corrected the output of ma-chine translation without access to externalresources, such as bilingual dictionaries or on-line search engines.
This system scored high-est according to BLEU of all Russian-Englishsubmissions to WMT14.Using a binary adequacy classification, weevaluate the entire postedited test set for cor-rectness against the reference translations.
Us-ing bilingual judges, we further evaluate a sub-stantial subset of the postedited test set us-ing a more fine-grained adequacy metric; usingthis metric, we show that monolingual postedi-tors can successfully produce postedited trans-lations that convey all or most of the meaningof the original source sentence in up to 87.8%of sentences.AcknowledgementsWe would like to thank the members of theSCREAM group at Wright-Patterson AFB.Opinions, interpretations, conclusions and recom-mendations are those of the authors and are not nec-essarily endorsed by the United States Government.Cleared for public release on 1 Apr 2014.
Origina-tor reference number RH-14-112150.
Case number88ABW-2014-1328.192ReferencesJoshua S. Albrecht, Rebecca Hwa, and G. Elisa-beta Marai.
2009.
Correcting automatic trans-lations through collaborations between MT andmonolingual target-language users.
In Proceed-ings of the 12th Conference of the EuropeanChapter of the Association for ComputationalLinguistics (EACL ?12), pages 60?68, Athens,Greece, March?April.Waleed Ammar, Victor Chahuneau, MichaelDenkowski, Greg Hanneman, Wang Ling,Austin Matthews, Kenton Murray, NicolaSegall, Yulia Tsvetkov, Alon Lavie, and ChrisDyer.
2013.
The CMU machine translation sys-tems at WMT 2013: Syntax, synthetic trans-lation options, and pseudo-references.
In Pro-ceedings of the Eighth Workshop on StatisticalMachine Translation (WMT ?13), pages 70?77,Sofia, Bulgaria, August.Sergio Barrachina, Oliver Bender, FranciscoCasacuberta, Jorge Civera, Elsa Cubel,Shahram Khadivi, Antonio Lagarda, HermannNey, Jesu?s Toma?s, Enrique Vidal, and Juan-Miguel Vilar.
2009.
Statistical approaches tocomputer-assisted translation.
ComputationalLinguistics, 35(1):3?28, March.Ondr?ej Bojar, Pavel Stran?a?k, and Daniel Zeman.2010.
Data issues in English-to-Hindi machinetranslation.
In Proceedings of the Seventh In-ternational Conference on Language Resourcesand Evaluation (LREC ?10), pages 1771?1777,Valletta, Malta, May.Ond?rej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow,Philipp Koehn, Christof Monz, Matt Post, RaduSoricut, and Lucia Specia.
2013.
Findings of the2013 Workshop on Statistical Machine Trans-lation.
In Proceedings of the Eighth Workshopon Statistical Machine Translation (WMT ?13),pages 1?44, Sofia, Bulgaria, August.Ond?rej Bojar, Vojt?ech Diatka, Pavel Rychl?y, PavelStra?n?ak, Ale?s Tamchyna, and Dan Zeman.2014.
Hindi-English and Hindi-only corpus formachine translation.
In Proceedings of the NinthInternational Language Resources and Evalua-tion Conference (LREC ?14), Reykjavik, Ice-land, May.
ELRA, European Language Re-sources Association.Peter Brown, Vincent Della Pietra, Peter deSouza,Jenifer Lai, and Robert Mercer.
1992.
Class-based n-gram models of natural language.
Com-putational Linguistics, 18(4):467?479, Decem-ber.Peter Brown, Vincent Della Pietra, Stephen DellaPietra, and Robert Mercer.
1993.
The math-ematics of statistical machine translation: pa-rameter estimation.
Computational Linguistics,19(2):263?311, June.Chris Callison-Burch.
2005.
Linear B system de-scription for the 2005 NIST MT evaluation exer-cise.
In Proceedings of the NIST 2005 MachineTranslation Evaluation Workshop.Mark Davis and Ken Whistler.
2013.
Unicode nor-malization forms.
Technical Report UAX #15,The Unicode Consortium, September.
Rev.
39.Michael Denkowski and Alon Lavie.
2012.
Trans-Center: Web-based translation research suite.In Proceedings of the AMTA 2012 Workshopon Post-Editing Technology and Practice DemoSession, November.Nadir Durrani, Helmut Schmid, and AlexanderFraser.
2011.
A joint sequence translationmodel with integrated reordering.
In Proceed-ings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics (ACL ?11),pages 1045?1054, Portland, Oregon, June.Michel Galley and Christopher D. Manning.
2008.A simple and effective hierarchical phrase re-ordering model.
In Proceedings of the 2008 Con-ference on Empirical Methods in Natural Lan-guage Processing (EMNLP ?08), pages 848?856,Honolulu, Hawai?i, October.Qin Gao and Stephan Vogel.
2008.
Parallel im-plementations of word alignment tool.
In Soft-ware Engineering, Testing and Quality Assur-ance for Natural Language Processing, pages49?57, Columbus, Ohio, June.Spence Green, Jeffrey Heer, and Christopher D.Manning.
2013.
The efficacy of human post-editing for language translation.
In Proceedingsof the ACM SIGCHI Conference on Human Fac-tors in Computing Systems (CHI ?13), pages439?448, Paris, France, April?May.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.Clark, and Philipp Koehn.
2013.
Scalable mod-ified Kneser-Ney language model estimation.
InProceedings of the 51st Annual Meeting of theAssociation for Computational Linguistics (ACL?13), pages 690?696, Sofia, Bulgaria, August.Mark Hopkins and Jonathan May.
2011.
Tuningas ranking.
In Proceedings of the 2011 Confer-ence on Empirical Methods in Natural LanguageProcessing (EMNLP ?11), pages 1352?1362, Ed-inburgh, Scotland, U.K.Michaeel Kazi, Michael Coury, Elizabeth Salesky,Jessica Ray, Wade Shen, Terry Gleason, TimAnderson, Grant Erdmann, Lane Schwartz,Brian Ore, Raymond Slyh, Jeremy Gwinnup,Katherine Young, and Michael Hutt.
2013.The MIT-LL/AFRL IWSLT-2013 MT system.In The 10th International Workshop on Spo-ken Language Translation (IWSLT ?13), pages136?143, Heidelberg, Germany, December.193Philipp Koehn, Franz Joseph Och, and DanielMarcu.
2003.
Statistical phrase-based trans-lation.
In Proceedings of the 2003 HumanLanguage Technology Conference of the NorthAmerican Chapter of the Association for Com-putational Linguistics (HLT-NAACL ?13), pages48?54, Edmonton, Canada, May?June.Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Chris Dyer, Ondr?ej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical ma-chine translation.
In Proceedings of the 45thAnnual Meeting of the Association for Compu-tational Linguistics (ACL ?07) Demo and PosterSessions, pages 177?180, Prague, Czech Repub-lic, June.Philipp Koehn.
2009a.
A process study of com-puter aided translation.
Machine Translation,23(4):241?263, November.Philipp Koehn.
2009b.
A web-based interactivecomputer aided translation tool.
In Proceedingsof the ACL-IJCNLP 2009 Software Demonstra-tions, pages 17?20, Suntec, Singapore, August.Philipp Koehn.
2010a.
Enabling monolingualtranslators: Post-editing vs. options.
In Hu-man Language Technologies: The 2010 AnnualConference of the North American Chapter ofthe Association for Computational Linguistics(HLT-NAACL ?10), pages 537?545, Los Ange-les, California, June.Philipp Koehn.
2010b.
An experimental manage-ment system.
The Prague Bulletin of Mathemat-ical Linguistics, 94:87?96, December.Philippe Langlais, George Foster, and Guy La-palme.
2000.
TransType: A computer-aidedtranslation typing system.
In Proceedings ofthe ANLP/NAACL 2000 Workshop on Embed-ded Machine Translation Systems, pages 46?51,Seattle, Washington, May.Leah S. Larkey, Margaret E. Connell, and NasreenAbduljaleel.
2003.
Hindi CLIR in thirty days.ACM Transactions on Asian Language Informa-tion Processing (TALIP), 2(2):130?142, June.Linda Mitchell, Johann Roturier, and SharonO?Brien.
2013.
Community-based post-editingof machine translation content: monolingual vs.bilingual.
In Proceedings of the 2nd Work-shop on Post-editing Technology and Practice(WPTP-2), pages 35?43, Nice, France, Septem-ber.
EAMT.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: A method for au-tomatic evaluation of machine translation.
InProceedings of the 40th Annual Meeting of theAssociation for Computational Linguistics (ACL?02), pages 311?318, Philadelphia, Pennsylva-nia, July.Mirko Plitt and Franc?ois Masselot.
2010.
A pro-ductivity test of statistical machine translationpost-editing in a typical localisation context.The Prague Bulletin of Mathematical Linguis-tics, 93:7?16, January.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings ofthe International Conference on New Methodsin Language Processing, Manchester, England,September.Helmut Schmid.
1995.
Improvements in part-of-speech tagging with an application to German.In Proceedings of the EACL SIGDAT Workshop,Dublin, Ireland, March.Wade Shen, Brian Delaney, and Tim Anderson.2006.
The MIT-LL/AFRL IWSLT-2006 MTsystem.
In The 3rd International Workshop onSpoken Language Translation (IWSLT ?06), Ky-oto, Japan.Jason R. Smith, Herve Saint-Amand, MagdalenaPlamada, Philipp Koehn, Chris Callison-Burch,and Adam Lopez.
2013.
Dirt cheap web-scaleparallel text from the common crawl.
In Pro-ceedings of the 51st Annual Meeting of the As-sociation for Computational Linguistics (ACL?13), pages 1374?1383, Sofia, Bulgaria, August.Andreas Stolcke.
2002.
SRILM ?
an extensiblelanguage modeling toolkit.
In Proceedings of the7th International Conference on Spoken Lan-guage Processing (ICSLP ?02), pages 901?904,Denver, Colorado, September.Christoph Tillman.
2004.
A unigram orientationmodel for statistical machine translation.
InProceedings of the Human Language TechnologyConference of the North American Chapter ofthe Association for Computational Linguistics(HLT-NAACL ?04), Companion Volume, pages101?104, Boston, Massachusetts, May.WMT.
2014.
Findings of the 2014 Workshop onStatistical Machine Translation.
In Proceedingsof the Ninth Workshop on Statistical MachineTranslation (WMT ?14), Baltimore, Maryland,June.194
