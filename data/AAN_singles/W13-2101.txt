Proceedings of the 14th European Workshop on Natural Language Generation, pages 1?9,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsAligning Formal Meaning Representations with Surface Strings forWide-coverage Text GenerationValerio Basile Johan Bos{v.basile,johan.bos}@rug.nlCenter for Language and Cognition Groningen (CLCG)University of Groningen, The NetherlandsAbstractStatistical natural language generationfrom abstract meaning representationspresupposes large corpora consisting oftext?meaning pairs.
Even though suchcorpora exist nowadays, or could be con-structed using robust semantic parsing, thesimple alignment between text and mean-ing representation is too coarse for de-veloping robust (statistical) NLG systems.By reformatting semantic representationsas graphs, fine-grained alignment can beobtained.
Given a precise alignment at theword level, the complete surface form ofa meaning representations can be deducedusing a simple declarative rule.1 IntroductionSurface Realization is the task of producing flu-ent text from some kind of formal, abstract rep-resentation of meaning (Reiter and Dale, 2000).However, while it is obvious what the output ofa natural language generation component shouldbe, namely text, there is little to no agreementon what its input formalism should be (Evans etal., 2002).
Since open-domain semantic parsersare able to produce formal semantic representa-tions nowadays (Bos, 2008; Butler and Yoshi-moto, 2012), it would be natural to see generationas a reversed process, and consider such seman-tic representations as input of a surface realizationcomponent.The idea of using large text corpora annotatedwith formal semantic representations for robustgeneration has been presented recently (Basile andBos, 2011; Wanner et al 2012).
The need for for-mal semantic representations as a basis for NLGwas expressed already much earlier by Power(1999), who derives semantic networks enrichedwith scope information from knowledge represen-tations for content planning.
In this paper we takea further step towards the goal of generating textfrom deep semantic representations, and considerthe issue of aligning the representations with sur-face strings that capture their meaning.First we describe the basic idea of align-ing semantic representations (logical forms) withsurface strings in a formalism-independent way(Section 2).
Then we apply our method to awell-known and widely-used semantic formalism,namely Discourse Representation Theory (DRT),first demonstrating how to represent DiscourseRepresentation Structures (DRSs) as graphs (Sec-tion 3) and showing that the resulting DiscourseRepresentation Graphs (DRGs) are equivalent toDRSs but are more convenient to fulfill word-level alignment (Section 4).
Finally, in Section 5we present a method that generates partial surfacestrings for each discourse referent occurring in thesemantic representation of a text, and composesthem into a complete surface form.
All in all, wethink this would be a first and important step insurface realization from formal semantic represen-tations.2 Aligning Logic with TextSeveral different formal semantic representationshave been proposed in the literature, and althoughthey might differ in various aspects, they also havea lot in common.
Many semantic representations(or logical forms as they are sometimes referredto) are variants of first-order logic and share basicbuilding blocks such as entities, properties, and re-lations, complemented with quantifiers, negationand further scope operators.A simple snapshot of a formal meaning repre-sentation is the following (with symbols composedout of WordNet (Fellbaum, 1998) synset identi-fiers to abstract away from natural language):blue#a#1(x) ?
cup#n#1(x)How could this logical form be expressed in nat-ural language?
Or put differently, how could we1realize the variable x in text?
As simple as it is, xdescribes ?a blue cup?, or if your target languageis Italian, ?una tazza blu?, or variants hereof, e.g.
?every blue cup?
(if x happens to be bound by uni-versally quantified) or perhaps as ?una tazza az-zurra?, using a different adjective to express blue-ness.
This works for simple examples, but howdoes it scale up to larger and more complex se-mantic representations?In a way, NLG can be viewed as a machinetranslation (MT) task, but unlike translating fromone natural language into another, the task is hereto translate a formal (unambiguous) language intoa natural language like English or Italian.
Currentstatistical MT techniques are based on large paral-lel corpora of aligned source and target text.
In thispaper we introduce a method for precise alignmentof formal semantic representations and text, withthe purpose of creating a large corpus that couldbe used in NLG research, and one that opens theway for statistical approaches, perhaps similar tothose used in MT.Broadly speaking, alignments between seman-tic representations and surface strings can be madein three different ways.
The simplest strategy, butalso the least informative, is to align a semanticrepresentation with a sentence or complete textwithout further information on which part of therepresentation produces what part of the surfaceform.
This might be enough to develop statisti-cal NLG systems for small sentences, but proba-bly does not scale up to handle larger texts.
Alter-natively, one could devise more complex schemesthat allow for a more fine-grained alignment be-tween parts of the semantic representation and sur-face strings (words and phrases).
Here there aretwo routes to follow, which we call the minimaland maximal alignment.In maximal alignment, each single piece of thesemantic representation corresponds to the wordsthat express that part of the meaning.
Possibleproblems with this approach are that perhaps notevery bit of the semantic representation corre-sponds to a surface form, and a single word couldalso correspond to various pieces in the seman-tic representation.
This is an interesting optionto explore, but in this paper we present the al-ternative approach, minimal alignment, which isa method where every word in the surface stringpoints to exactly one part of the semantic repre-sentation.
We think this alignment method formsa better starting point for the development of astatistical NLG component.
With sufficient datain the form of aligned texts with semantic repre-sentations, these alignments can be automaticallylearned, thus creating a model to generate surfaceforms from abstract, logical representations.However, aligning semantic representationswith words is a difficult enterprise, primarily be-cause formal semantic representations are not flatlike a string of words and often form complexstructures.
To overcome this issue we representformal semantic representations as a set of tu-ples.
For instance, returning to our earlier exam-ple representation for ?blue cup?, we could repre-sent part of it by the tuples ?blue#a#1,arg,x?
and?cup#n#1,arg,x?.
For convenience we can displaythis as a graph (Figure 1).xb l u e # a # 1c u p # n # 1Figure 1: Logical form graph.Note that in this example several tuples are notshown for clarity (such as conjunction and thequantifier).
We show below that we can indeedrepresent every bit of semantic information in thisformat without sacrificing the capability of align-ment with the text.
The important thing now is toshow how alignments between tuples and wordscan be realized, which is done by adding an ele-ment to each tuple denoting the surface string, forinstance ?cup#n#1,arg,x,?tazza?
?, as in Figure 2.xb l u e # a # 1"blue""a"c u p # n # 1"cup" xb l u e # a # 1"blu""una"c u p # n # 1"tazza"Figure 2: Logical form graphs aligned with sur-face forms in two languages.We can further refine the alignment by sayingsomething about the local order of surface expres-sions.
Again, this is done by adding an elementto the tuple, in this case one that denotes the localorder of a logical term.
We will make this clear bycontinuing with our example, where we add wordorder encoded as numerical indices in the tuple,e.g.
?cup#n#1,arg,x,?tazza?,2?, as Figure 3 shows.From these graphs we can associate the term2xb l u e # a # 1"blue"2"a" 1c u p # n # 1"cup" 3 xb l u e # a # 1"blu"3"una" 1c u p # n # 1"tazza" 2Figure 3: Encoding local word order.x with the surface strings ?a blue cup?
and ?unatazza blu?.
But the way we express local or-der is not limited to words and can be employedfor partial phrases as well, if one adopts a neo-Davidsonian event semantics with explicit the-matic roles.
This can be achieved by using thesame kind of numerical indices already used forthe alignment of words.
The example in Figure 4shows how to represent an event ?hit?
with its the-matic roles, preserving their relative order.
We callsurface forms ?partial?
or ?incomplete?
when theycontain variables, and ?complete?
when they onlycontain tokens.
The corresponding partial surfaceform is then ?y hit z?, where y and z are place-holders for surface strings.xyzagen t1h i t # v # 1 "hit" 2t h eme3Figure 4: Graph for a neo-Davidsonian structure.This is the basic idea of aligning surface stringswith parts of a deep semantic representation.
Notethat precise alignment is only possible for wordswith a lexical semantics that include first-ordervariables.
For words that introduce scope oper-ators (negation particles, coordinating conjuncts)we can?t have the cake and eat it: specifying thelocal order with respect to an entity or event vari-able directly and at the same time associating itwith an operator isn?t always possible.
To solvethis we introduce surface tuples that complement asemantic representation to facilitate perfect align-ment.
We will explain this in more detail in thefollowing sections.3 Discourse Representation GraphsThe choice of semantic formalism should ideallybe independent from the application of naturallanguage generation itself, to avoid bias and spe-cific tailoring the semantic representation to one?s(technical) needs.
Further, the formalism shouldhave a model-theoretic backbone, to ensure thatthe semantic representations one works with actu-ally have an interpretation, and can consequentlybe used in inference tasks using, for instance, auto-mated deduction for first-order logic.
Given thesecriteria, a good candidate is Discourse Represen-tation Theory, DRT (Kamp and Reyle, 1993), thatcaptures the meaning of texts in the form of Dis-course Representation Structures (DRSs).DRSs are capable of effectively representingthe meaning of natural language, covering manylinguistic phenomena including pronouns, quanti-fier scope, negation, modals, and presuppositions.DRSs are recursive structures put together by logi-cal and non-logical symbols, as in predicate logic,and in fact can be translated into first-order logicformulas (Muskens, 1996).
The way DRSs arenested inside each other give DRT the ability toexplain the behaviour of pronouns and presuppo-sitions (Van der Sandt, 1992).Aligning DRSs with texts with fine granularityis hard because words and phrases introduce dif-ferent kinds of semantic objects in a DRS: dis-course referents, predicates, relations, but alsological operators such as negation, disjunction andimplication that introduce embedded DRSs.
Aprecise alignment of a DRS with its text on thelevel of words is therefore a non-trivial task.To overcome this issue, we apply the idea pre-sented in the previous section to DRSs, making allrecursion implicit by representing them as directedgraphs.
We call a graph representing a DRS aDiscourse Representation Graph (DRG, in short).DRGs encode the same information as DRSs, butare expressed as a set of tuples.
Essentially, thisis done by reification over DRSs ?
every DRSsgets a unique label, and the arity of DRS condi-tions increases by one for accommodating a DRSlabel.
This allows us to reformulate a DRS as a setof tuples.A DRS is an ordered pair of discourse refer-ents (variables over entities) and DRS-conditions.DRS-conditions are basic (representing propertiesor relations) or complex (to handle negation anddisjunction).
To reflect these different constructs,we distinguish three types of tuples in DRGs:?
?K,referent,X?
means that X is a discoursereferent in K (referent tuples);?
?K,condition,C?
means that C is a condition3?x1 e1customer(x1)pay(e1)agent(e1,x1)k1 unary ??
scope k2k2 referent e1k2 referent x1k2 event payk2 concept customerk2 role agentcustomer instance x1pay instance e1agent internal e1agent external x1k1 ?unary k2e 1referentx1referentpayeven tcus tomerconceptag en trolescopeinstanceinstanceinternalexternalFigure 5: DRS and corresponding DRG (in tuples and in graph format) for ?A customer did not pay.
?in K (condition tuples), with various sub-types: concept, event, relation, role, named,cardinality, attribute, unary, and binary;?
?C,argument,A?
means that C is a conditionwith argument A (argument tuples), withthe sub-types internal, external, instance,scope, antecedent, and consequence.With the help of a concrete example, it is easy tosee that DRGs have the same expressive power asDRSs.
Consider for instance a DRS with negation,before and after labelling it (Figure 6):x yr(x,y)?zp(x)s(z,y)K1:x yc1:r(x,y)c2:?K2:zc3:p(x)c4:s(z,y)Figure 6: From DRS to DRG: labelling.Now, from the labelled DRS we can derive thefollowing three referent tuples: ?K1,referent,x?,?K1,referent,y?, and ?K2,referent,z?
; the follow-ing four condition tuples: ?K1,relation,c1:r?,?K1,unary,c2:?
?, ?K2,concept,c3:p?, and?K2,relation,c4:s?
; and the following argu-ment tuples: ?c1:r,internal,x?, ?c1:r,external,y?,?c2:?,scope,K2?, ?c3:p,instance,x?,?c4:s,internal,z?, and ?c4:s,external,y?.
Fromthese tuples, it is straightforward to recreatea labelled DRS, and by dropping the labelssubsequently, the original DRS resurfaces again.For the sake of readability we sometimes leaveout labels in examples throughout this paper.
Inaddition, we also show DRGs in graph-like pic-tures, where the tuples that form a DRG are theedges, and word-alignment information attachedat the tuple level is shown as labels on the graphedges, as in Figure 9.
In such graphs, nodes repre-senting discourse referents are square shaped, andnodes representing conditions are oval shaped.Note that labelling conditions is crucial to dis-tinguish between syntactically equivalent condi-tions occurring in different (embedded) DRSs.Unlike Power?s scoped semantic network forDRSs, we don?t make the assumption that condi-tions appear in the DRS in which their discoursereferents are introduced (Power, 1999).
The ex-ample in Figure 6 illustrates that this assumptionis not sound: the condition p(x) is in a differentDRS than where its discourse referent x is intro-duced.
Further note that our reification proce-dure yields ?flatter?
representations than similarformalisms (Copestake et al 1995; Reyle, 1993),and this makes it more convenient to align surfacestrings with DRSs with a high granularity, as wewill show below.4 Word-Aligned DRGsIn this section we show how the alignment be-tween surface text and its logical representation isrealized by adding information of the tuples thatmake up a DRG.
This sounds more straightfor-ward than it is.
For some word classes this is in-deed easy to do.
For others we need additionalmachinery in the formalism.
Let?s start with thestraightforward cases.
Determiners are usually as-sociated with referent tuples.
Content words, suchas nouns, verbs, adverbs and adjectives, are typ-ically directly associated with one-place relationsymbols, and can be naturally aligned with argu-ment tuples.
Verbs are assigned to instance tu-ples linking its event condition; likewise, nounsare typically aligned to instance tuples which linkdiscourse referents to the concepts they express;adjectives are aligned to tuples of attribute con-ditions.
Finally, words expressing relations (suchas prepositions), are attached to the external ar-gument tuple linking the relation to the discoursereferent playing the role of external argument.Although the strategy presented for DRG?text4alignment is intuitive and straightforward to im-plement, there are surface strings that don?t corre-spond to something explicit in the DRS.
To thisclass belong punctuation symbols, and semanti-cally empty words such as (in English) the infiniti-val particle, pleonastic pronouns, auxiliaries, thereinsertion, and so on.
Furthermore, function wordssuch as ?not?, ?if?, and ?or?, introduce semanticmaterial, but for the sake of surface string gener-ation could be better aligned with the event thatthey take the scope of.
To deal with all these cases,we extend DRGs with surface tuples of the form?K,surface,X?, whose edges are decorated with therequired surface strings.
Figure 7 shows an exam-ple of a DRG extended with such surface tuples.k1 unary ??
scope k2k2 referent e1k2 referent x1 1 Ak2 event payk2 concept customerk2 role agentcustomer instance x1 2 customerpay instance e1 4 payagent internal e1 1agent external x1k2 surface e1 2 didk2 surface e1 3 notk2 surface e1 5 .Figure 7: Word-aligned DRG for ?A customer didnot pay.?
All alignment information (includingsurface tuples) is highlighted.Note that surface tuples don?t have any influ-ence on the meaning of the original DRS ?
theyjust serve for the purpose of alignment of the re-quired surface strings.
Also note in Figure 7 theindices that were added to some tuples.
They serveto express the local order of surface information.Following the idea sketched in Section 2, the to-tal order of words is transformed into a local rank-ing of edges relative to discourse referents.
This ispossible because the tuples that have word tokensaligned to them always have a discourse referentas third element (the head of the directed edge, interms of graphs).
We group tuples that share thesame discourse referent and then assign indices re-flecting the relative order of how these tuples arerealized in the original text.Illustrating this with our example in Figure 7,we got two discourse referents: x1 and e1.
Thediscourse referent x1 is associated with three tu-ples, of which two are indexed (with indices 1and 2).
Generating the surface string for x1 suc-ceeds by traversing the edges in the order speci-fied, resulting in [A,customer] for x1.
The refer-ent e1 associates with six tuples, of which fourare indexed (with indices 1?4).
The order ofthese tuples would yield the partial surface string[x1,did,not,pay,.]
for e1.Note that the manner in which DRSs are con-structed during analysis ensures that all discoursereferents are linked to each other by taking thetransitive closure of all binary relations appearingin a DRS, and therefore we can reconstruct the to-tal order from composing the local orders.
In thenext section we explain how this is done.5 Surface CompositionIn this section we show in detail how sur-face strings can be generated from word-alignedDRGs.
It consists of two subsequent steps.
First,a surface form is associated with each discoursereferent.
Secondly, surface forms are put togetherin a bottom-up fashion, to generate the completeoutput.
During the composition, all of the dis-course referents are associated with their own sur-face representation.
The surface form associatedwith the discourse unit that contains all other dis-course units is then the text aligned with the origi-nal DRG.Surface forms of discourse referents are lists oftokens and other discourse referents.
Recall thatthe order of the elements of a discourse referent?ssurface form is reflected by the local ordering oftuples, as explained in the previous section, andtuples with no index are simply ignored when re-constructing surface strings.The surface form is composed by taking eachtuple belonging to a specific discourse referent, inthe correct order, and adding the tokens alignedwith the tuple to a list representing the surfacestring for that discourse referent.
An importantpart of this process is that binary DRS relations,represented in the DRG by a pair of internal andexternal argument tuple, are followed unidirec-tionally: if the tuple is of the internal type, thenthe discourse referent on the other end of the re-lation (i.e.
following its external tuple edge) isadded to the list.
Surface forms for embeddedDRSs include the discourse referents of the events5k1 : e4x1 : Michelle e1 : x1 thinks p1e1 : Michelle thinks p1p1 : that e2x2 : Obama e2 : x2 smokes .e2 : Obama smokes .p1 : that Obama smokes .e1 : Michelle thinks that Obama smokes .k1 : Michelle thinks that Obama smokes .Figure 8: Surface composition of embedded structures.they contain.Typically, discourse units contain exactly oneevent (the main event of the clause).
Phenomenasuch as gerunds (e.g.
?the laughing girl?)
and rel-ative clauses (e.g.
?the man who smokes?)
mayintroduce more than one event in a discourse unit.To ensure correct order and grouping, we borrowa technique from description logic (Horrocks andSattler, 1999) and invert roles in DRGs.
Ratherthan representing ?the laughing girl?
as [girl(x) ?agent(e,x) ?
laugh(e)], we represent it as [girl(x)?
agent?1(x,e) ?
laugh(e)], making use of R(x,y)?
R?1(y,x) to preserve meaning.
This ?trick?
en-sures that we can describe the local order of nounphrases with relative clauses and alike.To wrap things up, the composition operation isused to derive complete surface forms for DRGs.Composition puts together two surface forms,where one of them is complete, and one of themis incomplete.
It is formally defined as follows:?1 : ?
?2 : ?1?1?2?2 : ?1?
?2(1)where ?1 and ?2 are discourse referents, ?
is a listof tokens, and ?1 and ?2 are lists of word tokensand discourse referents.
In the example from Fig-ure 7, the complete surface form for the discourseunit k1 is derived by means of composition as for-mulated in (1) as follows:k2 : e1x1 : A customer e1 : x1 did not paye1 : A customer did not pay .k2 : A customer did not pay .The procedure for generation described here isreminiscent of the work of (Shieber, 1988) whoalso employs a deductive approach.
In particularour composition operation can be seen as a simpli-fied completion.Going back to the example in Section 4, sub-stituting the value of x1 in the incomplete sur-face form of e1 produces the surface string[A,customer,did,not,pay,.]
for e1.6 Selected PhenomenaWe implemented a first prototype using our align-ment and realization method and tested it on ex-amples taken from the Groningen Meaning Bank,a large annotated corpus of texts paired with DRSs(Basile et al 2012).
Naturally, we came acrossphenomena that are notoriously hard to analyze.Most of these we can handle adequately, but somewe can?t currently account for and require furtherwork.6.1 Embedded ClausesIn the variant of DRT that we are using, propo-sitional arguments of verbs introduce embeddedDRSs associated with a discourse referent.
Thisis a good test for our surface realization formal-ism, because it would show that it is capable of re-cursively generating embedded clauses.
Figure 9shows the DRG for the sentence ?Michelle thinksthat Obama smokes.
?k1x1referente 1referentp1referent"that"1subordinates:propt h inkeven tmichellen amedAgentroleThemerolex2e 2referentreferentpunctuat ion"."
3Pat ien trolesmokeeven tobaman amedinstance"thinks" 2instance"Michelle" 1ex tint1int 3ex tex tint1ins tance"smokes"2instance"Obama"1Figure 9: Word-aligned DRG for the sentence?Michelle thinks that Obama smokes.
?Here the surface forms of two discourse units(main and embedded) are generated.
In order togenerate the complete surface form, first the em-bedded clause is generated, and then composed6with the incomplete surface form of the mainclause.
As noted earlier, during the compositionprocess, the complete surface form for each dis-course referent is generated (Figure 8), showinga clear alignment between the entities of the se-mantic representation and the surface forms theyrepresent.6.2 CoordinationCoordination is another good test case for a lin-guistic formalism.
Consider for instance ?Sub-sistence fishing and commercial trawling occurwithin refuge waters?, where two noun phrases arecoordinated, giving rise to either a distributive (in-troducing two events in the DRS) or a collectiveinterpretation (introducing a set formation of dis-course referents in the DRS).
We can account forboth interpretations (Figure 10).Note that, interestingly, using the distributiveinterpretation DRG as input to the surface realiza-tion component could result, depending on howwords are aligned, in a surface form ?fishing oc-curs and trawling occurs?, or as ?fishing and trawl-ing occur?.6.3 Long-Distance DependenciesCases of extraction, for instance with WH-movement, could be problematic to capture withour formalism.
This is in particular an issue whenextraction crosses more than one clause boundary,as in ?Which car does Bill believe John bought?.Even though these cases are rare in the real world,a complete formalism for surface realization mustbe able to deal with such cases.
The question iswhether this is a separate generation task in thedomain of syntax (White et al 2007), or whetherthe current formalism needs to be adapted to coversuch long-distance dependencies.
Another rangeof complications are caused by discontinuous con-stituents, as in the Dutch sentence ?Ik heb kaart-jes gekocht voor Berlijn?
(literally: ?I have tick-ets bought for Berlin?
), where the prepositionalphrase ?voor Berlijn?
is an argument of the nounphrase ?kaartjes?.
In our formalism the only align-ment possible would result in the sentence ?Ikheb kaartjes voor Berlijn gekocht?, which is ar-guably a more fluent realization of the sentence,but doesn?t correspond to the original text.
If oneuses the original text as gold standard, this couldcause problems in evaluation.
(One could alsobenefit from this deficiency, and use it to generatemore than one gold standard surface string.
Thisis something to explore in future work.
)6.4 Control VerbsIn constructions like ?John wants to swim?, thecontrol verb ?wants?
associates its own subjectwith the subject of the infinitival clause that it hasas argument.
Semantically, this is realized by vari-able binding.
Generating an appropriate surfaceform for semantic representation with controlledvariables is a challenge: a naive approach wouldgenerate ?John wants John to swim?.
One possi-ble solution is to add another derivation rule forsurface composition dedicated to deal with caseswhere a placeholder variable occurs in more thanone partial surface form, substituting a null stringfor a variable following some heuristic rules.
Asecond, perhaps more elegant solution is to inte-grate a language model into the surface composi-tion process.7 Related workOver the years, several systems have emerged thataim at generate surface forms from different kindof abstract input representations.
An overview ofthe state-of-the-art is showcased by the submis-sions to the Surface Realization Shared Task (Belzet al 2012).
Bohnet et al(2010), for instance,employ deep structures derived from the CoNLL2009 shared task, essentially sentences annotatedwith shallow semantics, lemmata and dependencytrees; as the authors state, these annotations are notmade with generation in mind, and they necessi-tate complex preprocessing steps in order to derivesyntactic trees, and ultimately surface forms.
Theformat presented in this work has been especiallydeveloped with statistical approaches in mind.Nonetheless, there is very little work on ro-bust, wide-scale generation from DRSs, surpris-ingly perhaps given the large body of theoreticalresearch carried out in the framework of DiscourseRepresentation Theory, and practical implemen-tations and annotated corpora of DRSs that arenowadays available (Basile et al 2012).
This isin contrast to the NLG work in the framework ofLexical Functional Grammar (Guo et al 2011).Flat representation of semantic representa-tions, like the DRGs that we present, have alsobeen put forward to facilitate machine translation(Schiehlen et al 2000) and for evaluation pur-poses (Allen et al 2008), and semantic parsing7k1e 1referente 2referentsurface"and" 1x1referentx2referentt h emerolet h emeroleoccureven toccureven tfishingconceptt rawlingconceptinternal 1externalinternal2externalins tance"occur"2instance"occur"3instance"fishing" 1instance"trawling" 1k1e 1referentx1referentx2referentx3surface"and" 2referentsupe r s e trelationsupe r s e trelationoccureven tfishingconceptt rawlingconceptt h emeroleexternalinternal 1externalinternal3ins tance"occur" 2instance"fishing" 1instance"trawling" 1internal 1externalFigure 10: Analysis of NP coordination, in a distributive (left) and a collective interpretation (right).
(Le and Zuidema, 2012) just because they?re eas-ier and more efficient to process.
Packed seman-tic representations (leaving scope underspecified)also resemble flat representations (Copestake etal., 1995; Reyle, 1993) and can be viewed asgraphs, however they show less elaborated reifica-tion than the DRGs presented in this paper, and aretherefore less suitable for precise alignment withsurface strings.8 ConclusionWe presented a formalism to align logical forms,in particular Discourse Representation Structures,with surface text strings.
The resulting graph rep-resentations (DRGs), make recursion implicit byreification over nested DRSs.
Because of their?flat?
structure, DRGs can be precisely alignedwith the text they represent at the word level.
Thisis key to open-domain statistical Surface Real-ization, where words are learned from abstract,syntactic or semantic, representations, but alsouseful for other applications such as learning se-mantic representations directly from text (Le andZuidema, 2012).
The actual alignment betweenthe tuples that form a DRG and the surface formsthey represent is not trivial, and requires to makeseveral choices.Given the alignment with text, we show that itis possible to directly generate surface forms fromautomatically generated word-aligned DRGs.
Todo so, a declarative procedure is presented, thatgenerates complete surface forms from alignedDRGs in a compositional fashion.
The methodworks in a bottom-up way, using discourse ref-erents as starting points, then generating a sur-face form for each of them, and finally compos-ing all of the surface form together into a com-plete text.
We are currently building a large corpusof word-aligned DRSs, and are investigating ma-chine learning methods that could automaticallylearn the alignments.Surprisingly, given that DRT is one of the beststudied formalisms in formal semantics, there isn?tmuch work on generation from DRSs so far.
Thecontribution of this paper presents a method toalign DRSs with surface strings, that paves theway for robust, statistical methods for surface gen-eration from deep semantic representations.8ReferencesJames F. Allen, Mary Swift, and Will de Beaumont.2008.
Deep Semantic Analysis of Text.
In JohanBos and Rodolfo Delmonte, editors, Semantics inText Processing.
STEP 2008 Conference Proceed-ings, volume 1 of Research in Computational Se-mantics, pages 343?354.
College Publications.Valerio Basile and Johan Bos.
2011.
Towards generat-ing text from discourse representation structures.
InProceedings of the 13th European Workshop on Nat-ural Language Generation (ENLG), pages 145?150,Nancy, France.Valerio Basile, Johan Bos, Kilian Evang, and NoortjeVenhuizen.
2012.
Developing a large semanticallyannotated corpus.
In Proceedings of the Eight In-ternational Conference on Language Resources andEvaluation (LREC 2012), pages 3196?3200, Istan-bul, Turkey.Anja Belz, Bernd Bohnet, Simon Mille, Leo Wanner,and Michael White.
2012.
The surface realisationtask: Recent developments and future plans.
In Bar-bara Di Eugenio, Susan McRoy, Albert Gatt, AnjaBelz, Alexander Koller, and Kristina Striegnitz, ed-itors, INLG, pages 136?140.
The Association forComputer Linguistics.Bernd Bohnet, Leo Wanner, Simon Mille, and Ali-cia Burga.
2010.
Broad coverage multilingualdeep sentence generation with a stochastic multi-level realizer.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics,pages 98?106.Johan Bos.
2008.
Wide-Coverage Semantic Analy-sis with Boxer.
In J. Bos and R. Delmonte, editors,Semantics in Text Processing.
STEP 2008 Confer-ence Proceedings, volume 1 of Research in Compu-tational Semantics, pages 277?286.
College Publi-cations.Alastair Butler and Kei Yoshimoto.
2012.
Bankingmeaning representations from treebanks.
Linguis-tic Issues in Language Technology - LiLT, 7(1):1?
?A?S?22.Ann Copestake, Dan Flickinger, Rob Malouf, SusanneRiehemann, and Ivan Sag.
1995.
Translation us-ing Minimal Recursion Semantics.
In Proceedingsof the Sixth International Conference on Theoreticaland Methodological Issues in Machine Translation,pages 15?32, University of Leuven, Belgium.Roger Evans, Paul Piwek, and Lynne Cahill.
2002.What is NLG?
In Proceedings of the Second Inter-national Conference on Natural Language Genera-tion, pages 144?151.Christiane Fellbaum, editor.
1998.
WordNet.
An Elec-tronic Lexical Database.
The MIT Press.Yuqing Guo, Haifeng Wang, and Josef van Genabith.2011.
Dependency-based n-gram models for gen-eral purpose sentence realisation.
Natural LanguageEngineering, 17:455?483.Ian Horrocks and Ulrike Sattler.
1999.
A descrip-tion logic with transitive and inverse roles and rolehierarchies.
Journal of logic and computation,9(3):385?410.Hans Kamp and Uwe Reyle.
1993.
From Discourseto Logic; An Introduction to Modeltheoretic Seman-tics of Natural Language, Formal Logic and DRT.Kluwer, Dordrecht.Phong Le and Willem Zuidema.
2012.
Learning com-positional semantics for open domain semantic pars-ing.
Forthcoming.Reinhard Muskens.
1996.
Combining Montague Se-mantics and Discourse Representation.
Linguisticsand Philosophy, 19:143?186.R.
Power.
1999.
Controlling logical scope in text gen-eration.
In Proceedings of the 7th European Work-shop on Natural Language Generation, Toulouse,France.E.
Reiter and R. Dale.
2000.
Building Natural Lan-guage Generation Systems.
Cambridge UniversityPress.Uwe Reyle.
1993.
Dealing with Ambiguities by Un-derspecification: Construction, Representation andDeduction.
Journal of Semantics, 10:123?179.Michael Schiehlen, Johan Bos, and Michael Dorna.2000.
Verbmobil interface terms (vits).
In WolfgangWahlster, editor, Verbmobil: Foundations of Speech-to-Speech Translation.
Springer.Stuart M. Shieber.
1988.
A uniform architecture forparsing and generation.
In Proceedings of the 12thconference on Computational linguistics - Volume2, COLING ?88, pages 614?619, Stroudsburg, PA,USA.
Association for Computational Linguistics.Rob A.
Van der Sandt.
1992.
Presupposition Projec-tion as Anaphora Resolution.
Journal of Semantics,9:333?377.Leo Wanner, Simon Mille, and Bernd Bohnet.
2012.Towards a surface realization-oriented corpus anno-tation.
In Proceedings of the Seventh InternationalNatural Language Generation Conference, INLG?12, pages 22?30, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Michael White, Rajakrishnan Rajkumar, and ScottMartin.
2007.
Towards broad coverage surface real-ization with CCG.
In Proceedings of the Workshopon Using Corpora for NLG: Language Generationand Machine Translation (UCNLG+MT).9
