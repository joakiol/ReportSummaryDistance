Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 843?851,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPSemi-Supervised Cause Identification from Aviation Safety ReportsIsaac Persing and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{persingq,vince}@hlt.utdallas.eduAbstractWe introduce cause identification, a newproblem involving classification of in-cident reports in the aviation domain.Specifically, given a set of pre-definedcauses, a cause identification system seeksto identify all and only those causes thatcan explain why the aviation incident de-scribed in a given report occurred.
The dif-ficulty of cause identification stems in partfrom the fact that it is a multi-class, multi-label categorization task, and in part fromthe skewness of the class distributions andthe scarcity of annotated reports.
To im-prove the performance of a cause identi-fication system for the minority classes,we present a bootstrapping algorithm thatautomatically augments a training set bylearning from a small amount of labeleddata and a large amount of unlabeled data.Experimental results show that our algo-rithm yields a relative error reduction of6.3% in F-measure for the minority classesin comparison to a baseline that learnssolely from the labeled data.1 IntroductionAutomatic text classification is one of the most im-portant applications in natural language process-ing (NLP).
The difficulty of a text classificationtask depends on various factors, but typically, thetask can be difficult if (1) the amount of labeleddata available for learning the task is small; (2)it involves multiple classes; (3) it involves multi-label categorization, where more than one labelcan be assigned to each document; (4) the classdistributions are skewed, with some categoriessignificantly outnumbering the others; and (5) thedocuments belong to the same domain (e.g., moviereview classification).
In particular, when the doc-uments to be classified are from the same domain,they tend to be more similar to each other withrespect to word usage, thus making the classesless easily separable.
This is one of the reasonswhy topic-based classification, even with multipleclasses as in the 20 Newsgroups dataset1, tends tobe easier than review classification, where reviewsfrom the same domain are to be classified accord-ing to the sentiment expressed2 .In this paper, we introduce a new text classifi-cation problem involving the Aviation Safety Re-porting System (ASRS) that can be viewed as adifficult task along each of the five dimensions dis-cussed above.
Established in 1967, ASRS collectsvoluntarily submitted reports about aviation safetyincidents written by flight crews, attendants, con-trollers, and other related parties.
These incidentreports are made publicly available to researchersfor automatic analysis, with the ultimate goal ofimproving the aviation safety situation.
One cen-tral task in the automatic analysis of these reportsis cause identification, or the identification of whyan incident happened.
Aviation safety experts atNASA have identified 14 causes (or shaping fac-tors in NASA terminology) that could explain whyan incident occurred.
Hence, cause identificationcan be naturally recast as a text classification task:given an incident report, determine which of a setof 14 shapers contributed to the occurrence of theincident described in the report.As mentioned above, cause identification isconsidered challenging along each of the fiveaforementioned dimensions.
First, there is ascarcity of incident reports labeled with theshapers.
This can be attributed to the fact thatthere has been very little work on this task.
Whilethe NASA researchers have applied a heuristicmethod for labeling a report with shapers (Posse1http://kdd.ics.uci.edu/databases/20newsgroups/2Of course, the fact that sentiment classification requiresa deeper understanding of a text also makes it more difficultthan topic-based text classification (Pang et al, 2002).843et al, 2005), the method was evaluated on only20 manually labeled reports, which are not madepublicly available.
Second, the fact that this isa 14-class classification problem makes it morechallenging than a binary classification problem.Third, a report can be labeled with more than onecategory, as several shapers can contribute to theoccurrence of an aviation incident.
Fourth, theclass distribution is very skewed: based on ananalysis of our 1,333 annotated reports, 10 of the14 categories can be considered minority classes,which account for only 26% of the total num-ber of labels associated with the reports.
Finally,our cause identification task is domain-specific,involving the classification of documents that allbelong to the aviation domain.This paper focuses on improving the accuracyof minority class prediction for cause identifica-tion.
Not surprisingly, when trained on a datasetwith a skewed class distribution, most supervisedmachine learning algorithms will exhibit good per-formance on the majority classes, but relativelypoor performance on the minority classes.
Unfor-tunately, achieving good accuracies on the minor-ity classes is very important in our task of identify-ing shapers from aviation safety reports, where 10out of the 14 shapers are minority classes, as men-tioned above.
Minority class prediction has beentackled extensively in the machine learning liter-ature, using methods that typically involve sam-pling and re-weighting of training instances, withthe goal of creating a less skewed class distribution(e.g., Pazzani et al (1994), Fawcett (1996), Ku-bat and Matwin (1997)).
Such methods, however,are unlikely to perform equally well for our causeidentification task given our small labeled set, asthe minority class prediction problem is compli-cated by the scarcity of labeled data.
More specif-ically, given the scarcity of labeled data, manywords that are potentially correlated with a shaper(especially a minority shaper) may not appear inthe training set, and the lack of such useful indi-cators could hamper the acquisition of an accurateclassifier via supervised learning techniques.We propose to address the problem of minorityclass prediction in the presence of a small trainingset by means of a bootstrapping approach, wherewe introduce an iterative algorithm to (1) use asmall set of labeled reports and a large set of unla-beled reports to automatically identify words thatare most relevant to the minority shaper under con-sideration, and (2) augment the labeled data by us-ing the resulting words to annotate those unlabeledreports that can be confidently labeled.
We evalu-ate our approach using cross-validation on 1,333manually annotated reports.
In comparison to asupervised baseline approach where a classifier isacquired solely based on the training set, our boot-strapping approach yields a relative error reduc-tion of 6.3% in F-measure for the minority classes.In sum, the contributions of our work are three-fold.
First, we introduce a new, challengingtext classification problem, cause identificationfrom aviation safety reports, to the NLP commu-nity.
Second, we created an annotated dataset forcause identification that is made publicly availablefor stimulating further research on this problem3.Third, we introduce a bootstrapping algorithm forimproving the prediction of minority classes in thepresence of a small training set.The rest of the paper is organized as follows.
InSection 2, we present the 14 shapers.
Section 3 ex-plains how we preprocess and annotate the reports.Sections 4 and 5 describe the baseline approachesand our bootstrapping algorithm, respectively.
Wepresent results in Section 6, discuss related workin Section 7, and conclude in Section 8.2 Shaping FactorsAs mentioned in the introduction, the task of causeidentification involves labeling an incident reportwith all the shaping factors that contributed to theoccurrence of the incident.
Table 1 lists the 14shaping factors, as well as a description of eachshaper taken verbatim from Posse et al (2005).As we can see, the 14 classes are not mutually ex-clusive.
For instance, a lack of familiarity withequipment often implies a deficit in proficiency inits use, so the two shapers frequently co-occur.
Inaddition, while some classes cover a specific andwell-defined set of issues (e.g., Illusion), some en-compass a relatively large range of situations.
Forinstance, resource deficiency can include prob-lems with equipment, charts, or even aviation per-sonnel.
Furthermore, ten shaping factors can beconsidered minority classes, as each of them ac-count for less than 10% of the labels.
Accuratelypredicting minority classes is important in this do-main because, for example, the physical factorsminority shaper is frequently associated with in-cidents involving near-misses between aircraft.3http://www.hlt.utdallas.edu/?persingq/ASRSdataset.html844Id Shaping Factor Description %1 Attitude Any indication of unprofessional or antagonistic attitude by a controller or flight crew mem-ber, e.g., complacency or get-homeitis (in a hurry to get home).2.42 CommunicationEnvironmentInterferences with communications in the cockpit such as noise, auditory interference, radiofrequency congestion, or language barrier.5.53 Duty Cycle A strong indication of an unusual working period, e.g., a long day, flying very late at night,exceeding duty time regulations, having short and inadequate rest periods.1.84 Familiarity A lack of factual knowledge, such as new to or unfamiliar with company, airport, or aircraft.
3.25 Illusion Bright lights that cause something to blend in, black hole, white out, sloping terrain, etc.
0.16 Other Anything else that could be a shaper, such as shift change, passenger discomfort, or disori-entation.13.37 PhysicalEnvironmentUnusual physical conditions that could impair flying or make things difficult.
16.08 PhysicalFactorsPilot ailment that could impair flying or make things more difficult, such as being tired,drugged, incapacitated, suffering from vertigo, illness, dizziness, hypoxia, nausea, loss ofsight or hearing.2.29 Preoccupation A preoccupation, distraction, or division of attention that creates a deficit in performance,such as being preoccupied, busy (doing something else), or distracted.6.710 Pressure Psychological pressure, such as feeling intimidated, pressured, or being low on fuel.
1.811 Proficiency A general deficit in capabilities, such as inexperience, lack of training, not qualified, or notcurrent.14.412 ResourceDeficiencyAbsence, insufficient number, or poor quality of a resource, such as overworked or unavail-able controller, insufficient or out-of-date chart, malfunctioning or inoperative or missingequipment.30.013 Taskload Indicators of a heavy workload or many tasks at once, such as short-handed crew.
1.914 Unexpected Something sudden and surprising that is not expected.
0.6Table 1: Descriptions of shaping factor classes.
The ?%?
column shows the percent of labels the shapers account for.3 DatasetWe downloaded our corpus from the ASRS web-site4.
The corpus consists of 140,599 incidentreports collected during the period from January1998 to December 2007.
Each report is a freetext narrative that describes not only why an in-cident happened, but also what happened, where ithappened, how the reporter felt about the incident,the reporter?s opinions of other people involved inthe incident, and any other comments the reportercared to include.
In other words, a lot of informa-tion in the report is irrelevant to (and thus compli-cates) the task of cause identification.3.1 PreprocessingUnlike newswire articles, at which many topic-based text classification tasks are targeted, theASRS reports are informally written using variousdomain-specific abbreviations and acronyms, tendto contain poor grammar, and have capitalizationinformation removed, as illustrated in the follow-ing sentence taken from one of the reports.HAD BEEN CLRED FOR APCH BYZOA AND HAD BEEN HANDED OFFTO SANTA ROSA TWR.4http://asrs.arc.nasa.gov/This sentence is grammatically incorrect (due tothe lack of a subject), and contains abbrevia-tions such as CLRED, APCH, and TWR.
Thismakes it difficult for a non-aviation expert to un-derstand.
To improve readability (and hence fa-cilitate the annotation process), we preprocesseach report as follows.
First, we expand the ab-breviations/acronyms with the help of an officiallist of acronyms/abbreviations and their expandedforms5.
Second, though not as crucial as the firststep, we heuristically restore the case of the wordsby relying on an English lexicon: if a word ap-pears in the lexicon, we assume that it is not aproper name, and therefore convert it into lower-case.
After preprocessing, the example sentenceappears ashad been cleared for approach by ZOAand had been handed off to santa rosatower.Finally, to facilitate automatic analysis, we stemeach word in the narratives.3.2 Human AnnotationNext, we randomly picked 1,333 preprocessed re-ports and had two graduate students not affiliated5See http://akama.arc.nasa.gov/ASRSDBOnline/pdf/ASRS Decode.pdf.
In the very infrequently-occurring casewhere the same abbreviation or acronym may have morethan expansion, we arbitrarily chose one of the possibilities.845Id Total (%) F1 F2 F3 F4 F51 52 (3.9) 11 7 7 17 102 119 (8.9) 29 29 22 16 233 38 (2.9) 10 5 6 9 84 70 (5.3) 11 12 9 14 245 3 (0.2) 0 0 0 1 26 289 (21.7) 76 44 60 42 677 348 (26.1) 73 63 82 59 718 48 (3.6) 11 14 8 11 49 145 (10.9) 29 25 38 28 2510 38 (2.9) 12 10 4 7 511 313 (23.5) 65 50 74 46 7812 652 (48.9) 149 144 125 123 11113 42 (3.2) 7 8 8 6 1314 14 (1.1) 3 3 3 3 2Table 2: Number of occurrences of each shapingfactor in the dataset.
The ?Total?
column shows the num-ber of narratives labeled with each shaper and the percentageof narratives tagged with each shaper in the 1,333 labelednarrative set.
The ?F?
columns show the number narrativesassociated with each shaper in folds F1 ?
F5.x (# Shapers) 1 2 3 4 5 6Percentage 53.6 33.2 10.3 2.7 0.2 0.1Table 3: Percentage of documents with x labels.with this research independently annotate themwith shaping factors, based solely on the defi-nitions presented in Table 1.
To measure inter-annotator agreement, we compute Cohen?s Kappa(Carletta, 1996) from the two sets of annotations,obtaining a Kappa value of only 0.43.
This notonly suggests the difficulty of the cause identifica-tion task, but also reveals the vagueness inherentin the definition of the 14 shapers.
As a result,we had the two annotators re-examine each reportfor which there was a disagreement and reach anagreement on its final set of labels.
Statistics of theannotated dataset can be found in Table 2, wherethe ?Total?
column shows the size of each of the14 classes, expressed both as the number of re-ports that are labeled with a particular shaper andas a percent (in parenthesis).
Since we will per-form 5-fold cross validation in our experiments,we also show the number of reports labeled witheach shaper under the ?F?
columns for each fold.To get a better idea of how many reports have mul-tiple labels, we categorize the reports according tothe number of labels they contain in Table 3.4 Baseline ApproachesIn this section, we describe two baseline ap-proaches to cause identification.
Since our ulti-mate goal is to evaluate the effectiveness of ourbootstrapping algorithm, the baseline approachesonly make use of small amounts of labeled data foracquiring classifiers.
More specifically, both base-lines recast the cause identification problem as aset of 14 binary classification problems, one forpredicting each shaper.
In the binary classificationproblem for predicting shaper si, we create onetraining instance from each document in the train-ing set, labeling the instance as positive if the doc-ument has si as one of its labels, and negative oth-erwise.
After creating training instances, we traina binary classifier, ci, for predicting si, employingas features the top 50 unigrams that are selectedaccording to information gain computed over thetraining data (see Yang and Pedersen (1997)).
TheSVM learning algorithm as implemented in theLIBSVM software package (Chang and Lin, 2001)is used for classifier training, owing to its robustperformance on many text classification tasks.In our first baseline, we set al the learning pa-rameters to their default values.
As noted before,we divide the 1,333 annotated reports into fivefolds of roughly equal size, training the classifierson four folds and applying them separately to theremaining fold.
Results are reported in terms ofprecision (P), recall (R), and F-measure (F), whichare computed by aggregating over the 14 shapersas follows.
Let tpi be the number of test reportscorrectly labeled as positive by ci; pi be the totalnumber of test reports labeled as positive by ci;and ni be the total number of test reports that be-long to si according to the gold standard.
Then,P =?i tpi?i pi,R =?i tpi?i ni, and F = 2PRP + R.Our second baseline is similar to the first, ex-cept that we tune the classification threshold (CT)to optimize F-measure.
More specifically, recallthat LIBSVM trains a classifier that by default em-ploys a CT of 0.5, thus classifying an instance aspositive if and only if the probability that it be-longs to the positive class is at least 0.5.
How-ever, this may not be the optimal threshold to useas far as performance is concerned, especially forthe minority classes, where the class distributionis skewed.
This is the motivation behind tuningthe CT of each classifier.
To ensure a fair compar-ison with the first baseline, we do not employ ad-ditional labeled data for parameter tuning; rather,we reserve 25% of the available training data fortuning, and use the remaining 75% for classifier846acquisition.
This amounts to using three foldsfor training and one fold for development in eachcross validation experiment.
Using the develop-ment data, we tune the 14 CTs jointly to optimizeoverall F-measure.
However, an exact solution tothis optimization problem is computationally ex-pensive.
Consequently, we find a local maximumby employing a local search algorithm, which al-ters one parameter at a time to optimize F-measureby holding the remaining parameters fixed.5 Our Bootstrapping AlgorithmOne of the potential weaknesses of the two base-lines described in the previous section is that theclassifiers are trained on only a small amount oflabeled data.
This could have an adverse effecton the accuracy of the resulting classifiers, espe-cially those for the minority classes.
The situationis somewhat aggravated by the fact that we areadopting a one-versus-all scheme for generatingtraining instances for a particular shaper, which,together with the small amount of labeled data, im-plies that only a couple of positive instances maybe available for training the classifier for a minor-ity class.
To alleviate the data scarcity problemand improve the accuracy of the classifiers, wepropose in this section a bootstrapping algorithmthat automatically augments a training set by ex-ploiting a large amount of unlabeled data.
The ba-sic idea behind the algorithm is to iteratively iden-tify words that are high-quality indicators of thepositive or negative examples, and then automati-cally label unlabeled documents that contain a suf-ficient number of such indicators.Our bootstrapping algorithm, shown in Figure1, aims to augment the set of positive and neg-ative training instances for a given shaper.
Themain function, Train, takes as input four argu-ments.
The first two arguments, P and N , are thepositive and negative instances, respectively, gen-erated by the one-versus-one scheme from the ini-tial training set, as described in the previous sec-tion.
The third argument, U , is the unlabeled setof documents, which consists of all but the doc-uments in the training set.
In particular, U con-tains the documents in the development and testsets.
Hence, we are essentially assuming accessto the test documents (but not their labels) dur-ing the training process, as in a transductive learn-ing setting.
The last argument, k, is the numberof bootstrapping iterations.
In addition, the algo-Train(P,N, U, k)Inputs:P : positively labeled training examples of shaper xN : negatively labeled training examples of shaper xU : set of unlabeled narratives in corpusk: number of bootstrapping iterationsPW ?
?NW ?
?for i = 0 to k ?
1 doif |P | > |N | then[P, PW ]?
ExpandTrainingSet(P,N, U, PW )else[N, NW ]?ExpandTrainingSet(N,P, U, NW )end ifend forExpandTrainingSet(A,B, U, W )Inputs:A, B, U : narrative setsW : unigram feature setfor j = 1 to 4 dot?
arg maxt/?W(log( C(t,A)C(t,B)+1 ))// C(t, X): number of narratives in X containing tW ?W ?
{t}end forreturn [A ?
S(W, U), W ]// S(W,U): narratives in U containing ?
3 words in WFigure 1: Our bootstrapping algorithm.rithm uses two variables, PW and NW , to storethe sets of high-quality indicators for the positiveinstances and the negative instances, respectively,that are found during the bootstrapping process.Next, we begin our k bootstrapping iterations.In each iteration, we expand either P or N , de-pending on their relative sizes.
In order to keepthe two sets as close in size as possible, we chooseto expand the smaller of the two sets.6 After that,we execute the function ExpandTrainingSet to ex-pand the selected set.
Without loss of general-ity, assume that P is chosen for expansion.
Todo this, ExpandTrainingSet selects four words thatseem much more likely to appear in P than inN from the set of candidate words7.
To selectthese words, we calculate the log likelihood ratiolog( C(t,P )C(t,N)+1 ) for each candidate word t, whereC(t, P ) is the number of narratives in P that con-tain t, and C(t,N) similarly is the number of nar-ratives in N that contain t. If this ratio is large,6It may seem from the way P and N are constructed thatN is almost always larger than P and therefore is unlikely tobe selected for expansion.
However, the ample size of the un-labeled set means that the algorithm still adds large numbersof narratives to the training data.
Hence, even for minorityclasses, P often grows larger than N by iteration 3.7A candidate word is a word that appears in the trainingset (P ?N ) at least four times.847we posit that t is a good indicator of P .
Note thatincrementing the count in the denominator by onehas a smoothing effect: it avoids selecting wordsthat appears infrequently in P and not at all in N .There is a reason for selecting multiple words(rather than just one word) in each bootstrap-ping iteration: we want to prevent the algorithmfrom selecting words that are too specific to onesubcategory of a shaping factor.
For example,shaping factor 7 (Physical Environment) is com-posed largely of incidents influenced by weatherphenomena.
In one experiment, we tried select-ing only one word per bootstrapping iteration.For shaper 7, the first word added to PW was?snow?.
Upon the next iteration, the algorithmadded ?plow?
to PW.
While ?plow?
may itself beindicative of shaper 7, we believe its selection wasdue to the recent addition to P of a large number ofnarratives containing ?snow?.
Hence, by selectingfour words per iteration, we are forcing the algo-rithm to ?branch out?
among these subcategories.After adding the selected words to PW , weaugment P with all the unlabeled documents con-taining at least three words from PW .
The rea-son we impose the ?at least three?
requirementis precision: we want to ensure, with a reason-able level of confidence, that the unlabeled doc-uments chosen to augment P should indeed belabeled with the shaper under consideration, asincorrectly labeled documents would contaminatethe labeled data, thus accelerating the deteriorationof the quality of the automatically labeled data insubsequent bootstrapping iterations and adverselyaffecting the accuracy of the classifier trained on it(Pierce and Cardie, 2001).The above procedure is repeated in each boot-strapping iteration.
As mentioned above, if Nis smaller in size than P , we will expand N in-stead, adding to NW the four words that are thestrongest indicators of a narrative being a negativeexample of the shaper under consideration, andaugmenting N with those unlabeled narratives thatcontain at least three words from NW .The number of bootstrapping iterations is con-trolled by the input parameter k. As we will seein the next section, we run the bootstrapping algo-rithm for up to five iterations only, as the qualityof the bootstrapped data deteriorates fairly rapidly.The exact value of k will be determined automati-cally using development data, as discussed below.After bootstrapping, the augmented trainingdata can be used in combination with any of thetwo baseline approaches to acquire a classifier foridentifying a particular shaper.
Whichever base-line is used, we need to reserve one of the fivefolds to tune the parameter k in our cross vali-dation experiments.
In particular, if the secondbaseline is used, we will tune CT and k jointlyon the development data using the local search al-gorithm described previously, where we adjust thevalues of both CT and k for one of the 14 classi-fiers in each step of the search process to optimizethe overall F-measure score.6 Evaluation6.1 Baseline SystemsSince our evaluation centers on the question ofhow effective our bootstrapping algorithm is in ex-ploiting unlabeled documents to improve classifierperformance, our two baselines only employ theavailable labeled documents to train the classifiers.Recall that our first baseline, which we callB0.5 (due to its being a baseline with a CT of0.5), employs default values for all of the learn-ing parameters.
Micro-averaged 5-fold cross val-idation results of this baseline for all 14 shapersand for just 10 minority classes (due to our focuson improving minority class prediction) are ex-pressed as percentages in terms of precision (P),recall (R), and F-measure (F) in the first row ofTable 4.
As we can see, the baseline achievesan F-measure of 45.4 (14 shapers) and 35.4 (10shapers).
Comparing these two results, the higherF-measure achieved using all 14 shapers can be at-tributed primarily to improvements in recall.
Thisshould not be surprising: as mentioned above, thenumber of positive instances of a minority classmay be small, thus causing the resulting classi-fier to be biased towards classifying a documentas negative.Instead of employing a CT value of 0.5, oursecond baseline, Bct, tunes CT using one of thetraining folds and simply trains a classifier on theremaining three folds.
For parameter tuning, wetested CTs of 0.0, 0.05, .
.
., 1.0.
Results of thisbaseline are shown in row 2 of Table 4.
In com-parison to the first baseline, we see that F-measureimproves considerably by 7.4% and 4.5% for 14shapers and 10 shapers respectively8 , which illus-8It is important to note that the parameters are optimizedseparately for each pair of 14-shaper and 10-shaper exper-iments in this paper, and that the 10-shaper results are not848All 14 Classes 10 Minority ClassesSystem P R F P R FB0.5 67.0 34.4 45.4 68.3 23.9 35.4Bct 47.4 59.2 52.7 47.8 34.3 39.9E0.5 60.9 40.4 48.6 53.2 35.3 42.4Ect 50.5 54.9 52.6 49.1 39.4 43.7Table 4: 5-fold cross validation results.trates the importance of employing the right CTfor the cause identification task.6.2 Our ApproachNext, we evaluate the effectiveness of our boot-strapping algorithm in improving classifier per-formance.
More specifically, we apply the twobaselines separately to the augmented training setproduced by our bootstrapping algorithm.
Whencombining our bootstrapping algorithm with thefirst baseline, we produce a system that we callE0.5 (due to its being trained on the expandedtraining set with a CT of 0.5).
E0.5 has only onetunable parameter, k (i.e., the number of boot-strapping iterations), whose allowable values are0, 1, .
.
., 5.
When our algorithm is used in com-bination with the second baseline, we produce an-other system, Ect, which has both k and the CTas its parameters.
The allowable values of theseparameters, which are to be tuned jointly, are thesame as those employed by Bct and E0.5.Results of E0.5 are shown in row 3 of Table4.
In comparison to B0.5, we see that F-measureincreases by 3.2% and 7.0% for 14 shapers and10 shapers, respectively.
Such increases can beattributed to less imbalanced recall and precisionvalues, as a result of a large gain in recall accom-panied by a roughly equal drop in precision.
Theseresults are consistent with our intuition: recall canbe improved with a larger training set, but preci-sion can be hampered when learning from nois-ily labeled data.
Overall, these results suggest thatlearning from the augmented training set is useful,especially for the minority classes.Results of Ect are shown in row 4 of Table 4.In comparison to Bct, we see mixed results: F-measure increases by 3.8% for 10 shapers (whichrepresents a relative error reduction of 6.3%, butdrops by 0.1% for 14 shapers.
Overall, these re-sults suggest that when the CT is tunable, train-ing set expansion helps the minority classes buthurts the remaining classes.
A closer look at theresults reveals that the 0.1% F-measure drop is duesimply extracted from the 14-shaper experiments.to a large drop in recall accompanied by a smallergain in precision.
In other words, for the fournon-minority classes, the benefits obtained fromusing the bootstrapped documents can also be ob-tained by simply adjusting the CT.
This could beattributed to the fact that a decent classifier can betrained using only the hand-labeled training exam-ples for these four shapers, and as a result, the au-tomatically labeled examples either provide verylittle new knowledge or are too noisy to be useful.On the other hand, for the 10 minority classes, the3.8% gain in F-measure can be attributed to a si-multaneous rise in recall and precision.
Note thatsuch gain cannot possibly be obtained by simplyadjusting the CT, since adjusting the CT alwaysresults in higher recall and lower precision or viceversa.
Overall, the simultaneous rise in recall andprecision implies that the bootstrapped documentshave provided useful knowledge, particularly inthe form of positive examples, for the classifiers.Even though the bootstrapped documents are nois-ily labeled, they can still be used to improve theclassifiers, as the set of initially labeled positiveexamples for the minority classes is too small.6.3 Additional AnalysesQuality of the bootstrapped data.
Since thebootstrapped documents are noisily labeled, a nat-ural question is: How noisy are they?
To get asense of the accuracy of the bootstrapped docu-ments without further manual labeling, recall thatour experimental setup resembles a transductivesetting where the test documents are part of theunlabeled data, and consequently, some of themmay have been automatically labeled by the boot-strapping algorithm.
In fact, 137 documents in thefive test folds were automatically labeled in the14-shaper Ect experiments, and 69 automaticallylabeled documents were similarity obtained fromthe 10-shaper Ect experiments.
For 14 shapers, theaccuracies of the positively and negatively labeleddocuments are 74.6% and 97.1%, respectively,and the corresponding numbers for 10 shapers are43.2% and 81.3%.
These numbers suggest thatnegative examples can be acquired with high ac-curacies, but the same is not true for positive ex-amples.
Nevertheless, learning the 10 shapersfrom the not-so-accurately-labeled positive exam-ples still allows us to outperform the correspond-ing baseline.849Shaping Factor Positive Expanders Negative ExpandersFamiliarity unfamiliar, layout, unfamilarity, relyPhysical Environment cloud, snow, ice, windPhysical Factors fatigue, tire, night, rest, hotel, awake, sleep, sick declare, emergency, advisory, separationPreoccupation distract, preoccupied, awareness, situational,task, interrupt, focus, eye, configure, sleepdeclare, ice snow, crash, fire, rescue, anti,smokePressure bad, decision, extend, fuel, calculate, reserve,diversion, alternateTable 5: Example positive and negative expansion words collected by Ect for selected shaping factors.Analysis of the expanders.
To get an idea ofwhether the words acquired during the bootstrap-ping process (henceforth expanders) make intu-itive sense, we show in Table 5 example positiveand negative expanders obtained for five shapingfactors from the Ect experiments.
As we can see,many of the positive expanders are intuitively ob-vious.
We might, however, wonder about the con-nection between, for example, the shaper Famil-iarity and the word ?rely?, or between the shaperPressure and the word ?extend?.
We suspect thatthe bootstrapping algorithm is likely to make poorword selections particularly in the cases of the mi-nority classes, where the positively labeled train-ing data used to select expansion words is moresparse.
As suggested earlier, poor word choiceearly in the algorithm is likely to cause even poorerword choice later on.On the other hand, while none of the negativeexpanders seem directly meaningful in relation tothe shaper for which they were selected, some ofthem do appear to be related to other phenomenathat may be negatively correlated with the shaper.For instance, the words ?snow?
and ?ice?
wereselected as negative expanders for Preoccupationand also as positive expanders for Physical Envi-ronment.
While these two shapers are only slightlynegatively correlated, it is possible that Preoccu-pation may be strongly negatively correlated withthe subset of Physical Environment incidents in-volving cold weather.7 Related WorkSince we recast cause identification as a text clas-sification task and proposed a bootstrapping ap-proach that targets at improving minority classprediction, the work most related to ours involvesone or both of these topics.Guzma?n-Cabrera et al (2007) address theproblem of class skewness in text classification.Specifically, they first under-sample the majorityclasses, and then bootstrap the classifier trainedon the under-sampled data using unlabeled doc-uments collected from the Web.Minority classes can be expanded without theavailability of unlabeled data as well.
For ex-ample, Chawla et al (2002) describe a methodby which synthetic training examples of minor-ity classes can be generated from other labeledtraining examples to address the problem of im-balanced data in a variety of domains.Nigam et al (2000) propose an iterative semi-supervised method that employs the EM algorithmin combination with the naive Bayes generativemodel to combine a small set of labeled docu-ments and a large set of unlabeled documents.
Mc-Callum and Nigam (1999) suggest that the ini-tial labeled examples can be obtained using a listof keywords rather than through annotated data,yielding an unsupervised algorithm.Similar bootstrapping methods are applicableoutside text classification as well.
One of themost notable examples is Yarowsky?s (1995) boot-strapping algorithm for word sense disambigua-tion.
Beginning with a list of unlabeled contextssurrounding a word to be disambiguated and a listof seed words for each possible sense, the algo-rithm iteratively uses the seeds to label a trainingset from the unlabeled contexts, and then uses thetraining set to identify more seed words.8 ConclusionsWe have introduced a new problem, cause identi-fication from aviation safety reports, to the NLPcommunity.
We recast it as a multi-class, multi-label text classification task, and presented a boot-strapping algorithm for improving the predictionof minority classes in the presence of a small train-ing set.
Experimental results show that our algo-rithm yields a relative error reduction of 6.3% inF-measure over a purely supervised baseline whenapplied to the minority classes.
By making ourannotated dataset publicly available, we hope tostimulate research in this challenging problem.850AcknowledgmentsWe thank the three anonymous reviewers for theirinvaluable comments on an earlier draft of thepaper.
We are indebted to Muhammad ArshadUl Abedin, who provided us with a preprocessedversion of the ASRS corpus and, together withMarzia Murshed, annotated the 1,333 documents.This work was supported in part by NASA GrantNNX08AC35A and NSF Grant IIS-0812261.ReferencesJean Carletta.
1996.
Assessing agreement on classi-fication tasks: The Kappa statistic.
ComputationalLinguistics, 22(2):249?254.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIB-SVM: A library for support vector machines.Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O.Hall, and W. Philip Kegelmeyer.
2002.
SMOTE:Synthetic minority over-sampling technique.
Jour-nal of Artificial Intelligence Research, 16:321?357.Tom Fawcett.
1996.
Learning with skewed class distri-butions ?
summary of responses.
Machine Learn-ing List: Vol.
8, No.
20.Rafael Guzma?n-Cabrera, Manuel Montes-y-Go?mez,Paolo Rosso, and Luis Villasen?or Pineda.
2007.Taking advantage of the Web for text classificationwith imbalanced classes.
In Proceedings of MICAI,pages 831?838.Miroslav Kubat and Stan Matwin.
1997.
Addressingthe curse of imbalanced training sets: One-sided se-lection.
In Proceedings of ICML, pages 179?186.Andrew McCallum and Kamal Nigam.
1999.
Textclassification by bootstrapping with keywords, EMand shrinkage.
In Proceedings of the ACL Work-shop for Unsupervised Learning in Natural Lan-guage Processing, pages 52?58.Kamal Nigam, Andrew McCallum, Sebastian Thrun,and Tom Mitchell.
2000.
Text classification fromlabeled and unlabeled documents using EM.
Ma-chine Learning, 39(2/3):103?134.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification us-ing machine learning techniques.
In Proceedings ofEMNLP, pages 79?86.Michael Pazzani, Christopher Merz, Patrick Murphy,Kamal Ali, Timothy Hume, and Clifford Brunk.1994.
Reducing misclassification costs.
In Proceed-ings of ICML, pages 217?225.David Pierce and Claire Cardie.
2001.
Limitations ofco-training for natural language learning from largedatasets.
In Proceedings of EMNLP, pages 1?9.Christian Posse, Brett Matzke, Catherine Anderson,Alan Brothers, Melissa Matzke, and Thomas Ferry-man.
2005.
Extracting information from narratives:An application to aviation safety reports.
In Pro-ceedings of the Aerospace Conference 2005, pages3678?3690.Yiming Yang and Jan O. Pedersen.
1997.
A compara-tive study on feature selection in text categorization.In Proceedings of ICML, pages 412?420.David Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In Pro-ceedings of the ACL, pages 189?196.851
