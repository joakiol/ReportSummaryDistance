25Grammatical Inference and First Language AcquisitionAlexander Clark (asc@aclark.demon.co.uk)ISSCO / TIM, University of GenevaUNI-MAIL, Boulevard du Pont-d?Arve,CH-1211 Gene`ve 4, SwitzerlandAbstractOne argument for parametric models of languagehas been learnability in the context of first languageacquisition.
The claim is made that ?logical?
ar-guments from learnability theory require non-trivialconstraints on the class of languages.
Initial formal-isations of the problem (Gold, 1967) are howeverinapplicable to this particular situation.
In this pa-per we construct an appropriate formalisation of theproblem using a modern vocabulary drawn from sta-tistical learning theory and grammatical inferenceand looking in detail at the relevant empirical facts.We claim that a variant of the Probably Approxi-mately Correct (PAC) learning framework (Valiant,1984) with positive samples only, modified so it isnot completely distribution free is the appropriatechoice.
Some negative results derived from crypto-graphic problems (Kearns et al, 1994) appear to ap-ply in this situation but the existence of algorithmswith provably good performance (Ron et al, 1995)and subsequent work, shows how these negative re-sults are not as strong as they initially appear, andthat recent algorithms for learning regular languagespartially satisfy our criteria.
We then discuss theapplicability of these results to parametric and non-parametric models.1 IntroductionFor some years, the relevance of formal resultsin grammatical inference to the empirical questionof first language acquisition by infant children hasbeen recognised (Wexler and Culicover, 1980).
Un-fortunately, for many researchers, with a few no-table exceptions (Abe, 1988), this begins and endswith Gold?s famous negative results in the identifi-cation in the limit paradigm.
This paradigm, thoughstill widely used in the grammatical inference com-munity, is clearly of limited relevance to the issueat hand, since it requires the model to be able toexactly identify the target language even when anadversary can pick arbitrarily misleading sequencesof examples to provide.
Moreover, the paradigm asstated has no bounds on the amount of data or com-putation required for the learner.
In spite of the inap-plicability of this particular paradigm, in a suitableanalysis there are quite strong arguments that beardirectly on this problem.Grammatical inference is the study of machinelearning of formal languages.
It has a vast formalvocabulary and has been applied to a wide selec-tion of different problems, where the ?languages?under study can be (representations of) parts of nat-ural languages, sequences of nucleotides, moves ofa robot, or some other sequence data.
For any con-clusions that we draw from formal discussions tohave any applicability to the real world, we mustbe sure to select, or construct, from the rich set offormal devices available an appropriate formalisa-tion.
Even then, we should be very cautious aboutmaking inferences about how the infant child mustor cannot learn language: subsequent developmentsin GI might allow a more nuanced description inwhich these conclusions are not valid.
The situationis complicated by the fact that the field of grammti-cal inference, much like the wider field of machinelearning in general, is in a state of rapid change.In this paper we hope to address this problem byjustifying the selection of the appropriate learningframework starting by looking at the actual situa-tion the child is in, rather than from an a priori deci-sion about the right framework.
We will not attempta survey of grammatical inference techniques; norshall we provide proofs of the theorems we use here.Arguments based on formal learnability have beenused to support the idea of parameter based theo-ries of language (Chomsky, 1986).
As we shall seebelow, under our analysis of the problem these ar-guments are weak.
Indeed, they are more pertinentto questions about the autonomy and modularity oflanguage learning: the question whether learning ofsome level of linguistic knowledge ?
morphologyor syntax, for example ?
can take place in isolationfrom other forms of learning, such as the acquisitionof word meaning, and without interaction, ground-ing and so on.26Positive results can help us to understand how hu-mans might learn languages by outlining the class ofalgorithms that might be used by humans, consid-ered as computational systems at a suitable abstractlevel.
Conversely, negative results might be help-ful if they could demonstrate that no algorithms of acertain class could perform the task ?
in this case wecould know that the human child learns his languagein some other way.We shall proceed as follows: after briefly de-scribing FLA, we describe the various elements ofa model of learning, or framework.
We then makea series of decisions based on the empirical factsabout FLA, to construct an appropriate model ormodels, avoiding unnecessary idealisation whereverpossible.
We proceed to some strong negative re-sults, well-known in the GI community that bear onthe questions at hand.
The most powerful of these(Kearns et al, 1994) appears to apply quite directlyto our chosen model.
We then discuss an interest-ing algorithm (Ron et al, 1995) which shows thatthis can be circumvented, at least for a subclass ofregular languages.
Finally, after discussing the pos-sibilities for extending this result to all regular lan-guages, and beyond, we conclude with a discussionof the implications of the results presented for thedistinction between parametric and non-parametricmodels.2 First Language AcquisitionLet us first examine the phenomenon we are con-cerned with: first language acquisition.
In the spaceof a few years, children almost invariably acquire,in the absence of explicit instruction, one or more ofthe languages that they are exposed to.
A multitudeof subsidiary debates have sprung up around thiscentral issue covering questions about critical peri-ods ?
the ages at which this can take place, the ex-act nature of the evidence available to the child, andthe various phases of linguistic use through whichthe infant child passes.
In the opinion of many re-searchers, explaining this ability is one of the mostimportant challenges facing linguists and cognitivescientists today.A difficulty for us in this paper is that many ofthe idealisations made in the study of this field arein fact demonstrably false.
Classical assumptions,such as the existence of uniform communities oflanguage users, are well-motivated in the study ofthe ?steady state?
of a system, but less so whenstudying acquisition and change.
There is a regret-table tendency to slip from viewing these idealisa-tions correctly ?
as counter-factual idealizations ?
toviewing them as empirical facts that need to be ex-plained.
Thus, when looking for an appropriate for-mulation of the problem, we should recall for exam-ple the fact that different children do not converge toexactly the same knowledge of language as is some-times claimed, nor do all of them acquire a languagecompetently at all, since there is a small proportionof children who though apparently neurologicallynormal fail to acquire language.
In the context ofour discussion later on, these observations lead usto accept slightly less stringent criteria where we al-low a small probability of failure and do not demandperfect equality of hypothesis and target.3 Grammatical InferenceThe general field of machine learning has a spe-cialised subfield that deals with the learning of for-mal languages.
This field, Grammatical Inference(GI), is characterised above all by an interest in for-mal results, both in terms of formal characterisa-tions of the target languages, and in terms of formalproofs either that particular algorithms can learn ac-cording to particular definitions, or that sets of lan-guage cannot be learnt.
In spite of its theoreticalbent, GI algorithms have also been applied withsome success.
Natural language, however is not theonly source of real-world applications for GI.
Otherdomains include biological sequence data, artificiallanguages, such as discovering XML schemas, orsequences of moves of a robot.
The field is alsodriven by technical motives and the intrinsic ele-gance and interest of the mathematical ideas em-ployed.
In summary it is not just about language,and accordingly it has developed a rich vocabularyto deal with the wide range of its subject matter.In particular, researchers are often concernedwith formal results ?
that is we want algorithmswhere we can prove that they will perform in a cer-tain way.
Often, we may be able to empirically es-tablish that a particular algorithm performs well, inthe sense of reliably producing an accurate model,while we may be unable to prove formally that thealgorithm will always perform in this way.
Thiscan be for a number of reasons: the mathematicsrequired in the derivation of the bounds on the er-rors may be difficult or obscure, or the algorithmmay behave strangely when dealing with sets of datawhich are ill-behaved in some way.The basic framework can be considered as agame played between two players.
One player, theteacher, provides information to another, the learner,and from that information the learner must identifythe underlying language.
We can break down thissituation further into a number of elements.
We as-sume that the languages to be learned are drawn27in some way from a possibly infinite class of lan-guages, L, which is a set of formal mathematicalobjects.
The teacher selects one of these languages,which we call the target, and then gives the learnera certain amount of information of various typesabout the target.
After a while, the learner then re-turns its guess, the hypothesis, which in general willbe a language drawn from the same class L. Ide-ally the learner has been able to deduce or induceor abduce something about the target from the in-formation we have given it, and in this case the hy-pothesis it returns will be identical to, or close insome technical sense, to the target.
If the learnercan conistently do this, under whatever constraintswe choose, then we say it can learn that class of lan-guages.
To turn this vague description into some-thing more concrete requires us to specify a numberof things.?
What sort of mathematical object should weuse to represent a language??
What is the target class of languages??
What information is the learner given??
What computational constraints does thelearner operate under??
How close must the target be to the hypothesis,and how do we measure it?This paper addresses the extent to which negativeresults in GI could be relevant to this real world sit-uation.
As always, when negative results from the-ory are being applied, a certain amount of cautionis appropriate in examining the underlying assump-tions of the theory and the extent to which these areapplicable.
As we shall see, in our opinion, noneof the current negative results, though powerful, areapplicable to the empirical situation.
We shall ac-cordingly, at various points, make strong pessimisticassumptions about the learning environment of thechild, and show that even under these unrealisticallystringent stipulations, the negative results are stillinapplicable.
This will make the conclusions wecome to a little sharper.
Conversely, if we wantedto show that the negative results did apply, to beconvincing we would have to make rather optimisticassumptions about the learning environment.4 Applying GI to FLAWe now have the delicate task of selecting, or ratherconstructing, a formal model by identifying the vari-ous components we have identified above.
We wantto choose the model that is the best representationof the learning task or tasks that the infant childmust perform.
We consider that some of the em-pirical questions do not yet have clear answers.
Inthose cases, we shall make the choice that makes thelearning task more difficult.
In other cases, we maynot have a clear idea of how to formalise some in-formation source.
We shall start by making a signif-icant idealisation: we consider language acquisitionas being a single task.
Natural languages as tradi-tionally describe have different levels.
At the veryleast we have morphology and syntax; one mightalso consider inter-sentential or discourse as an ad-ditional level.
We conflate all of these into a singletask: learning a formal language; in the discussionbelow, for the sake of concreteness and clarity, weshall talk in terms of learning syntax.4.1 The LanguageThe first question we must answer concerns the lan-guage itself.
A formal language is normally definedas follows.
Given a finite alphabet ?, we define theset of all strings (the free monoid) over ?
as ?
?.We want to learn a language L ?
??.
The alpha-bet ?
could be a set of phonemes, or characters, ora set of words, or a set of lexical categories (partof speech tags).
The language could be the set ofwell-formed sentences, or the set of words that obeythe phonotactics of the language, and so on.
We re-duce all of the different learning tasks in languageto a single abstract task ?
identifying a possibly in-finite set of strings.
This is overly simplistic sincetransductions, i.e.
mappings from one string to an-other, are probably also necessary.
We are usinghere a standard definition of a language where everystring is unambiguously either in or not in the lan-guage..
This may appear unrealistic ?
if the formallanguage is meant to represent the set of grammati-cal sentences, there are well-known methodologicalproblems with deciding where exactly to draw theline between grammatical and ungrammatical sen-tences.
An alternative might be to consider accept-ability rather than grammaticality as the definingcriterion for inclusion in the set.
Moreover, thereis a certain amount of noise in the input ?
Thereare other possibilities.
We could for example use afuzzy set ?
i.e.
a function from ??
?
[0, 1] whereeach string has a degree of membership between 0and 1.
This would seem to create more problemsthan it solves.
A more appealing option is to learndistributions, again functions f from ??
?
[0, 1]but where?s?L f(s) = 1.
This is of course theclassic problem of language modelling, and is com-pelling for two reasons.
First, it is empirically wellgrounded ?
the probability of a string is related to itsfrequency of occurrence, and secondly, we can de-28duce from the speech recognition capability of hu-mans that they must have some similar capability.Both possibilities ?
crisp languages, and distri-butions ?
are reasonable.
The choice depends onwhat one considers the key phenomena to be ex-plained are ?
grammaticality judgments by nativespeakers, or natural use and comprehension of thelanguage.
We favour the latter, and accordinglythink that learning distributions is a more accurateand more difficult choice.4.2 The class of languagesA common confusion in some discussions of thistopic is between languages and classes of lan-guages.
Learnability is a property of classes oflanguages.
If there is only one language in theclass of languages to be learned then the learnercan just guess that language and succeed.
A classwith two languages is again trivially learnable ifyou have an efficient algorithm for testing member-ship.
It is only when the set of languages is expo-nentially large or infinite, that the problem becomesnon-trivial, from a theoretical point of view.
Theclass of languages we need is a class of languagesthat includes all attested human languages and ad-ditionally all ?possible?
human languages.
Natu-ral languages are thought to fall into the class ofmildly context-sensitive languages, (Vijay-Shankerand Weir, 1994), so clearly this class is largeenough.
It is, however, not necessary that our classbe this large.
Indeed it is essential for learnabilitythat it is not.
As we shall see below, even the classof regular languages contains some subclasses thatare computationally hard to learn.
Indeed, we claimit is reasonable to define our class so it does not con-tain languages that are clearly not possible humanlanguages.4.3 Information sourcesNext we must specify the information that our learn-ing algorithm has access to.
Clearly the primarysource of data is the primary linguistic data (PLD),namely the utterances that occur in the child?s envi-ronment.
These will consist of both child-directedspeech and adult-to-adult speech.
These are gen-erally acceptable sentences that is to say sentencesthat are in the language to be learned.
These arecalled positive samples.
One of the most long-running debates in this field is over whether thechild has access to negative data ?
unacceptable sen-tences that are marked in some way as such.
Theconsensus (Marcus, 1993) appears to be that they donot.
In middle-class Western families, children areprovided with some sort of feedback about the well-formedness of their utterances, but this is unreliableand erratic, not a universal of global child-raising.Furthermore this appears to have no effect on thechild.
Children do also get indirect pragmatic feed-back if their utterances are incomprehensible.
In ouropinion, both of these would be better modelled bywhat is called a membership query: the algorithmmay generate a string and be informed whether thatstring is in the language or not.
However, we feelthat this is too erratic to be considered an essentialpart of the process.
Another question is whether theinput data is presented as a flat string or annotatedwith some sort of structural evidence, which mightbe derived from prosodic or semantic information.Unfortunately there is little agreement on what theconstituent structure should be ?
indeed many lin-guistic theories do not have a level of constituentstructure at all, but just dependency structure.Semantic information is also claimed as an im-portant source.
The hypothesis is that children canuse lexical semantics, coupled with rich sources ofreal-world knowlege to infer the meaning of utter-ances from the situational context.
That would bean extremely powerful piece of information, but it isclearly absurd to claim that the meaning of an utter-ance is uniquely specified by the situational context.If true, there would be no need for communicationor information transfer at all.
Of course the contextputs some constraints on the sentences that will beuttered, but it is not clear how to incorporate thisfact without being far too generous.
In summary itappears that only positive evidence can be unequiv-ocally relied upon though this may seem a harsh andunrealistic environment.4.4 PresentationWe have now decided that the only evidence avail-able to the learner will be unadorned positive sam-ples drawn from the target language.
There are var-ious possibilities for how the samples are selected.The choice that is most favourable for the learner iswhere they are slected by a helpful teacher to makethe learning process as easy as possible (Goldmanand Mathias, 1996).
While it is certainly true thatcarers speak to small children in sentences of sim-ple structure (Motherese), this is not true for all ofthe data that the child has access to, nor is it uni-versally valid.
Moreover, there are serious techni-cal problems with formalising this, namely what iscalled ?collusion?
where the teacher provides exam-ples that encode the grammar itself, thus trivialisingthe learning process.
Though attempts have beenmade to limit this problem, they are not yet com-pletely satisfactory.
The next alternative is that theexamples are selected randomly from some fixed29distribution.
This appears to us to be the appropri-ate choice, subject to some limitations on the dis-tributions that we discuss below.
The final option,the most difficult for the learner, is where the se-quence of samples can be selected by an intelli-gent adversary, in an attempt to make the learnerfail, subject only to the weak requirement that eachstring in the language appears at least once.
This isthe approach taken in the identification in the limitparadigm (Gold, 1967), and is clearly too stringent.The remaining question then regards the distribu-tion from which the samples are drawn: whether thelearner has to be able to learn for every possible dis-tribution, or only for distributions from a particularclass, or only for one particular distribution.4.5 ResourcesBeyond the requirement of computability we willwish to place additional limitations on the computa-tional resources that the learner can use.
Since chil-dren learn the language in a limited period of time,which limits both the amount of data they have ac-cess to and the amount of computation they can use,it seems appropriate to disallow algorithms that useunbounded or very large amounts of data or time.As normal, we shall formalise this by putting poly-nomial bounds on the sample complexity and com-putational complexity.
Since the individual samplesare of varying length, we need to allow the compu-tational complexity to depend on the total length ofthe sample.
A key question is what the parametersof the sample complexity polynomial should be.
Weshall discuss this further below.4.6 Convergence CriteriaNext we address the issue of reliability: the extentto which all children acquire language.
First, vari-ability in achievement of particular linguistic mile-stones is high.
There are numerous causes includingdeafness, mental retardation, cerebral palsy, specificlanguage impairment and autism.
Generally, autis-tic children appear neurologically and physicallynormal, but about half may never speak.
Autism,on some accounts, has an incidence of about 0.2%.Therefore we can require learning to happen witharbitrarily high probability, but requiring it to hap-pen with probability one is unreasonable.
A relatedquestion concerns convergence: the extent to whichchildren exposed to a linguistic environment endup with the same language as others.
Clearly theyare very close since otherwise communication couldnot happen, but there is ample evidence from stud-ies of variation (Labov, 1975), that there are non-trivial differences between adults, who have grownup with near-identical linguistic experiences, aboutthe interpretation and syntactic acceptability of sim-ple sentences, quite apart from the wide purely lex-ical variation that is easily detected.
A famous ex-ample in English is ?Each of the boys didn?t come?.Moreover, language change requires some chil-dren to end up with slightly different grammarsfrom the older generation.
At the very most, weshould require that the hypothesis should be closeto the target.
The function we use to measure the?distance?
between hypothesis and target depends onwhether we are learnng crisp languages or distribu-tions.
If we are learning distributions then the ob-vious choice is the Kullback-Leibler divergence ?
avery strict measure.
For crisp languages, the prob-ability of the symmetric difference with respect tosome distribution is natural.4.7 PAC-learningThese considerations lead us to some variant of theProbably Approximately Correct (PAC) model oflearning (Valiant, 1984).
We require the algorithmto produce with arbitrarily high probability a goodhypothesis.
We formalise this by saying that for any?
> 0 it must produce a good hypothesis with prob-ability more than 1 ?
?.
Next we require a goodhypothesis to be arbitrarily close to the target, so wehave a precision  and we say that for any  > 0, thehypothesis must be less than  away from the target.We allow the amount of data it can use to increase asthe confidence and precision get smaller.
We definePAC-learning in the following way: given a finitealphabet ?, and a class of languages L over ?, analgorithm PAC-learns the class L, if there is a poly-nomial q, such that for every confidence ?
> 0 andprecision  > 0, for every distribution D over ?
?,for every language L in L, whenever the number ofsamples exceeds q(1/, 1/?, |?|, |L|), the algorithmmust produce a hypothesis H such that with prob-ability greater than 1 ?
?, PrD(H?L > ).
Herewe use A?B to mean the symmetric difference be-tween two sets.
The polynomial q is called thesample complexity polynomial.
We also limit theamount of computation to some polynomial in thetotal length of the data it has seen.
Note first of allthat this is a worst case bound ?
we are not requiringmerely that on average it comes close.
Additionallythis model is what is called ?distribution-free?.
Thismeans that the algorithm must work for every com-bination of distribution and language.
This is a verystringent requirement, only mitigated by the factthat the error is calculated with respect to the samedistribution that the samples are drawn from.
Thus,if there is a subset of ??
with low aggregate proba-bility under D, the algorithm will not get many sam-30ples from this region but will not be penalised verymuch for errors in that region.
From our point ofview, there are two problems with this framework:first, we only want to draw positive samples, but thedistributions are over all strings in ?
?, and includesome that give a zero probability to all strings inthe language concerned.
Secondly, this is too pes-simistic because the distribution has no relation tothe language: intuitively it?s reasonable to expectthe distribution to be derived in some way from thelanguage, or the structure of a grammar generatingthe language.
Indeed there is a causal connectionin reality since the sample of the language the childis exposed to is generated by people who do in factknow the language.One alternative that has been suggested is thePAC learning with simple distributions model intro-duced by (Denis, 2001).
This is based on ideas fromcomplexity theory where the samples are drawn ac-cording to a universal distribution defined by theconditional Kolmogorov complexity.
While math-ematically correct this is inappropriate as a modelof FLA for a number of reasons.
First, learnabilityis proven only on a single very unusual distribution,and relies on particular properties of this distribu-tion, and secondly there are some very large con-stants in the sample complexity polynomial.The solution we favour is to define some natu-ral class of distributions based on a grammar or au-tomaton generating the language.
Given a class oflanguages defined by some generative device, thereis normally a natural stochastic variant of the de-vice which defines a distribution over that language.Thus regular languages can be defined by a finite-state automaton, and these can be naturally ex-tended to Probabilistic finite state automaton.
Sim-ilarly context free languages are normally definedby context-free grammmars which can be extendedagain to to Probabilistic or stochastic CFG.
Wetherefore propose a slight modification of the PAC-framework.
For every class of languages L, definedby some formal device define a class of distribu-tions defined by a stochastic variant of that device.D.
Then for each language L, we select the set ofdistributions whose support is equal to the languageand subject to a polynomial bound (q)on the com-plexity of the distribution in terms of the complex-ity of the target language: D+L = {D ?
D : L =supp(D)?|D| < q(|L|)}.
Samples are drawn fromone of these distributions.There are two technical problems here: first, thisdoesn?t penalise over-generalisation.
Since the dis-tribution is over positive examples, negative exam-ples have zero weight, so we need some penaltyfunction over negative examples or alternativelyrequire the hypothesis to be a subset of the tar-get.
Secondly, this definition is too vague.
Theexact way in which you extend the ?crisp?
lan-guage to a stochastic one can have serious con-sequences.
When dealing with regular languages,for example, though the class of languages definedby deterministic automata is the same as that de-fined by non-deterministic languages, the same isnot true for their stochastic variants.
Additionally,one can have exponential blow-ups in the numberof states when determinising automata.
Similarly,with CFGs, (Abney et al, 1999) showed that con-verting between two parametrisations of stochasticContext Free languages are equivalent but that thereare blow-ups in both directions.
We do not have acompletely satisfactory solution to this problem atthe moment; an alternative is to consider learningthe distributions rather than the languages.In the case of learning distributions, we have thesame framework, but the samples are drawn accord-ing to the distribution being learned T , and we re-quire that the hypothesis H has small divergencefrom the target: D(T ||H) < .
Since the divergenceis infinite if the hypothesis gives probability zero toa string in the target, this will have the consequencethat the target must assign a non-zero probability toevery string.5 Negative ResultsNow that we have a fairly clear idea of various waysof formalising the situation we can consider the ex-tent to which formal results apply.
We start by con-sidering negative results, which in Machine Learn-ing come in two types.
First, there are information-theoretic bounds on sample complexity, derivedfrom the Vapnik-Chervonenkis (VC) dimension ofthe space of languages, a measure of the complex-ity of the set of hypotheses.
If we add a parameterto the sample complexity polynomial that representsthe complexity of the concept to be learned then thiswill remove these problems.
This can be the size ofa representation of the target which will be a poly-nomial in the number of states, or simply the num-ber of non-terminals or states.
This is very standardin most fields of machine learning.The second problem relates not to the amountof information but to the computation involved.Results derived from cryptographic limitations oncomputational complexity, can be proved based onwidely held and well supported assumptions thatcertain hard cryptographic problems are insoluble.In what follows we assume that there are no effi-cient algorithms for common cryptographic prob-31lems such as factoring Blum integers, inverting RSAfunction, recognizing quadratic residues or learningnoisy parity functions.There may be algorithms that will learn with rea-sonable amounts of data but that require unfeasiblylarge amounts of computation to find.
There area number of powerful negative results on learningin the purely distribution-free situation we consid-ered and rejected above.
(Kearns and Valiant, 1989)showed that acyclic deterministic automata are notlearnable even with positive and negative exam-ples.
Similarly, (Abe and Warmuth, 1992) showeda slightly weaker representation dependent result onlearning with a large alphabet for non-deterministicautomata, by showing that there are strings such thatmaximising the likelihood of the string is NP-hard.Again this does not strictly apply to the partially dis-tribution free situation we have chosen.However there is one very strong result that ap-pears to apply.
A straightforward consequence of(Kearns et al, 1994) shows that Acyclic Determinis-tic Probabilistic FSA over a two letter alphabet can-not be learned under another cryptographic assump-tion (the noisy parity assumption).
Therefore anyclass of languages that includes this comparativelyweak family will not be learnable in out framework.But this rests upon the assumption that the classof possible human languages must include somecryptographically hard functions.
It appears thatour formal apparatus does not distinguish betweenthese cryptographic functions which hav been con-sciously designed to be hard to learn, and natu-ral languages which presumably have evolved to beeasy to learn since there is no evolutionary pressureto make them hard to decrypt ?
no intelligent preda-tors eavesdropping for example.
Clearly this is aflaw in our analysis: we need to find some morenuanced description for the class of possible humanlanguages that excludes these hard languages or dis-tributions.6 Positive resultsThere is a positive result that shows a way forward.A PDFA is ?-distinguishable the distributions gen-erated from any two states differ by at least ?
inthe L?-norm, i.e.
there is a string with a differ-ence in probability of at least ?.
(Ron et al, 1995)showed that ?-distinguishable acyclic PDFAs canbe PAC-learned using the KLD as error functionin time polynomial in n, 1/, 1/?, 1/?, |?|.
Theyuse a variant of a standard state-merging algorithm.Since these are acyclic the languages they defineare always finite.
This additional criterion of distin-guishability suffices to guarantee learnability.
Thiswork can be extended to cyclic automata (Clark andThollard, 2004a; Clark and Thollard, 2004b), andthus the class of all regular languages, with the ad-dition of a further parameter which bounds the ex-pected length of a string generated from any state.The use of distinguishability seems innocuous; insyntactic terms it is a consequence of the plausiblecondition that for any pair of distinct non-terminalsthere is some fairly likely string generated by oneand not the other.
Similarly strings of symbols innatural language tend to have limited length.
Analternate way of formalising this is to define a classof distinguishable automata, where the distinguisha-bility of the automata is lower bounded by an in-verse polynomial in the number of states.
This isformally equivalent, but avoids adding terms to thesample complexity polynomial.
In summary thiswould be a valid solution if all human languagesactually lay within the class of regular languages.Note also the general properties of this kind of al-gorithm: provably learning an infinite class of lan-guages with infinite support using only polynomialamounts of data and computation.It is worth pointing out that the algorithm doesnot need to ?know?
the values of the parameters.Define a new parameter t, and set, for example n =t, L = t, ?
= e?t,  = t?1 and ?
= t?1.
This givesa sample complexity polynomial in one parameterq(t).
Given a certain amount of data N we can justchoose the largest value of t such that q(t) < N ,and set the parameters accordingly.7 Parametric modelsWe can now examine the relevance of these re-sults to the distinction between parametric and non-parametric languages.
Parametric models are thosewhere the class of languages is parametrised by asmall set of finite-valued (binary) parameters, wherethe number of paameters is small compared to thelog2 of the complexity of the languages.
Withoutthis latter constraint the notion is mathematicallyvacuous, since, for example, any context free gram-mar in Chomsky normal form can be parametrisedwith N3 + NM + 1 binary parameters where Nis the number of non-terminals and M the num-ber of terminals.
This constraint is also necessaryfor parametric models to make testable empiricalpredictions both about language universals, devel-opmental evidence and relationships between thetwo (Hyams, 1986).
We neglect here the importantissue of lexical learning: we assume, implausibly,that lexical learning can take place completely be-fore syntax learning commences.
It has in the pastbeen stated that the finiteness of a language class32suffices to guarantee learnability even under a PAC-learning criterion (Bertolo, 2001).
This is, in gen-eral, false, and arises from neglecting constraints onthe sample complexity and the computational com-plexities both of learning and of parsing.
The neg-ative result of (Kearns et al, 1994) discussed aboveapplies also to parametric models.
The specific classof noisy parity functions that they prove are unlearn-able, are parametrised by a number of binary pa-rameters in a way very reminiscent of a parametricmodel of language.
The mere fact that there are afinite number of parameters does not suffice to guar-antee learnability, if the resulting class of languagesis exponentially large, or if there is no polynomialalgorithm for parsing.
This does not imply that allparametrised classes of languages will be unlearn-able, only that having a small number of parame-ters is neither necessary nor sufficient to guaranteeefficient learnability.
If the parameters are shallowand relate to easily detectable properties of the lan-guages and are independent then learning can oc-cur efficiently (Yang, 2002).
If they are ?deep?
andinter-related, learning may be impossible.
Learn-ability depends more on simple statistical propertiesof the distributions of the samples than on the struc-ture of the class of languages.Our conclusion then is ultimately that the theoryof learnability will not be able to resolve disputesabout the nature of first language acquisition: theseproblems will have to be answered by empirical re-search, rather than by mathematical analysis.AcknowledgementsThis work was supported in part by the ISTProgramme of the European Community, underthe PASCAL Network of Excellence, IST-2002-506778, funded in part by the Swiss Federal Officefor Education and Science (OFES).
This publicationonly reflects the authors?
views.ReferencesN.
Abe and M. K. Warmuth.
1992.
On the com-putational complexity of approximating distribu-tions by probabilistic automata.
Machine Learn-ing, 9:205?260.N.
Abe.
1988.
Feasible learnability of formal gram-mars and the theory of natural language acquisi-tion.
In Proceedings of COLING 1988, pages 1?6.S.
Abney, D. McAllester, and F. Pereira.
1999.
Re-lating probabilistic grammars and automata.
InProceedings of ACL ?99.Stefano Bertolo.
2001.
A brief overview of learn-ability.
In Stefano Bertolo, editor, Language Ac-quisition and Learnability.
Cambridge UniversityPress.Noam Chomsky.
1986.
Knowledge of Language :Its Nature, Origin, and Use.
Praeger.Alexander Clark and Franck Thollard.
2004a.PAC-learnability of probabilistic deterministic fi-nite state automata.
Journal of Machine LearningResearch, 5:473?497, May.Alexander Clark and Franck Thollard.
2004b.
Par-tially distribution-free learning of regular lan-guages from positive samples.
In Proceedings ofCOLING, Geneva, Switzerland.F.
Denis.
2001.
Learning regular languages fromsimple positive examples.
Machine Learning,44(1/2):37?66.E.
M. Gold.
1967.
Language indentification in thelimit.
Information and control, 10(5):447 ?
474.S.
A. Goldman and H. D. Mathias.
1996.
Teach-ing a smarter learner.
Journal of Computer andSystem Sciences, 52(2):255?267.N.
Hyams.
1986.
Language Acquisition and theTheory of Parameters.
D. Reidel.M.
Kearns and G. Valiant.
1989.
Cryptographiclimitations on learning boolean formulae and fi-nite automata.
In 21st annual ACM symposiumon Theory of computation, pages 433?444, NewYork.
ACM, ACM.M.J.
Kearns, Y. Mansour, D. Ron, R. Rubinfeld,R.E.
Schapire, and L. Sellie.
1994.
On the learn-ability of discrete distributions.
In Proc.
of the25th Annual ACM Symposium on Theory of Com-puting, pages 273?282.W.
Labov.
1975.
Empirical foundations of linguis-tic theory.
In R. Austerlitz, editor, The Scope ofAmerican Linguistics.
Peter de Ridder Press.G.
F. Marcus.
1993.
Negative evidence in languageacquisition.
Cognition, 46:53?85.D.
Ron, Y.
Singer, and N. Tishby.
1995.
On thelearnability and usage of acyclic probabilistic fi-nite automata.
In COLT 1995, pages 31?40,Santa Cruz CA USA.
ACM.L.
Valiant.
1984.
A theory of the learnable.
Com-munications of the ACM, 27(11):1134 ?
1142.K.
Vijay-Shanker and David J. Weir.
1994.The equivalence of four extensions of context-free grammars.
Mathematical Systems Theory,27(6):511?546.Kenneth Wexler and Peter W. Culicover.
1980.
For-mal Principles of Language Acquisition.
MITPress.C.
Yang.
2002.
Knowledge and Learning in Natu-ral Language.
Oxford.
