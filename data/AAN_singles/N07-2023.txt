Proceedings of NAACL HLT 2007, Companion Volume, pages 89?92,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsA Geometric Interpretation ofNon-Target-Normalized Maximum Cross-channel Correlationfor Vocal Activity Detection in MeetingsKornel LaskowskiinterACT, Universita?t KarlsruheKarlsruhe, Germanykornel@ira.uka.deTanja SchultzinterACT, Carnegie Mellon UniversityPittsburgh PA, USAtanja@cs.cmu.eduAbstractVocal activity detection is an impor-tant technology for both automatic speechrecognition and automatic speech under-standing.
In meetings, standard vocalactivity detection algorithms have beenshown to be ineffective, because partici-pants typically vocalize for only a frac-tion of the recorded time and because,while they are not vocalizing, their channelsare frequently dominated by crosstalk fromother participants.
In the present work,we review a particular type of normaliza-tion of maximum cross-channel correlation,a feature recently introduced to address thecrosstalk problem.
We derive a plausiblegeometric interpretation and show how theframe size affects performance.1 IntroductionVocal activity detection (VAD) is an important tech-nology for any application with an automatic speechrecognition (ASR) front end.
In meetings, partic-ipants typically vocalize for only a fraction of therecorded time.
Their temporally contiguous contri-butions should be identified prior to ASR in order toleverage speaker adaptation schemes and languagemodel constraints, and to associate recognized out-put with specific speakers (who said what).
Segmen-tation into such contributions is informed primarilyby VAD on a frame-by-frame basis.Individual head-mounted microphone (IHM)recordings of meetings present a particular challengefor VAD, due to crosstalk from other participants.Most state-of-the-art VAD systems for meetings relyon decoding in a binary speech/non-speech space,assuming independence among participants, but areincreasingly relying on features specifically designedto address the crosstalk issue (Wrigley et al, 2005).A feature which has attracted attention since itsuse in VAD post-processing in (Pfau et al, 2001)is the maximum cross-channel correlation (XC),max?
?jk (?
), between channels j and k, where ?
isthe lag.
When designing features descriptive of thekth channel, XC is frequently normalized by the en-ergy in the target1 channel k (Wrigley et al, 2003).Alternately, XC can be normalized by the energy inthe non-target channel j (Laskowski et al, 2004),a normalization which we refer to here as NT-Norm,extending the Norm and S-Norm naming conventionsin (Wrigley et al, 2005).
Table 1 shows several typesof normalizations which have been explored.Normalization of XC Mean Min Max(none) maxj 6=k ?jk(?)
[2][4] [2][4] [2][4]Norm maxj 6=k ?jk(?
)?kk(0) [2][4] [2][4] [2][4]S-Norm maxj 6=k ?jk(?)?
?jj(0)?kk(0)[2][4][5] [2][4] [1][2][4]NT-Norm maxj 6=k ?jk(?
)?jj(0) [3] [6] [6]Table 1: Normalizations and statistics of cross-channel correlation features to describe channel k.In [1], a median-smoothed version was used in post-processing.
In [3], the sum (JMXC) was used in-stead of the mean.
In [5], cross-correlation was com-puted over samples and features.
In [6], the mini-mum and the maximum were jointly referred to asNMXC.
References in bold depict features selectedby an automatic feature selection algorithm in [2] and[4].
(1:(Pfau et al, 2001), 2:(Wrigley et al, 2003),3:(Laskowski et al, 2004), 4:(Wrigley et al, 2005),5:(Huang, 2005), 6:(Boakye and Stolcke, 2006))1The target/non-target terms are due to (Boakye andStolcke, 2006).89The present work revisits NT-Norm normalization,which has been successfully used in a threshold de-tector (Laskowski et al, 2004), in automatic initiallabel assignment (Laskowski and Schultz, 2006), andas part of a two-state decoder feature vector (Boakyeand Stolcke, 2006).
Our main contribution is a geo-metric interpretation of NT-Norm XC, in Section 2.We also describe, in Section 3, several contrastiveexperiments, and discuss the results in Section 4.2 Geometric InterpretationWe propose an interpretable geometric approxima-tion to NT-Norm XC for channel k,?k,j =max?
?jk (?
)?jj, ?j 6=k (1)We assume the simplified response in the kth IHMmicrophone at a distance dk from a single pointsource s (t) to bemk (t) .= Ak( 1dks(t?
dkc)+ ?k (t)), (2)where c, Ak and ?k (t) are the speed of sound, thegain of microphone k, and source-uncorrelated noiseat microphone k, respectively.
Cross-channel corre-lation is approximated over a frame of size ?
by?jk (?)
=?
?AjAkdjdks (t) s (t?
?)
dt , (3)where ?
?
(dj ?
dk) /c.
Letting Ps ???
s2 (t) dt andP?k ???
?2k (t) dt,?jj (0) = A2j(1d2jPs + P?j), (4)max?
?jk (?)
=AjAkdjdkPs , (5)respectively, as the maximum of ?jk (?)
occurs at??
= (dk ?
dj) /c.
In consequence,max?
?jk (?
)?jj (0)?
djdk, (6)provided thatAkAj??1?
P?j1d2jPs + P?j??
?
1 , (7)i.e., under assumptions of similar microphone gains,a non-negligible farfield signal-to-noise ratio at eachmicrophone, and the simplifications embodied inEquation 2, NT-Norm XC approximates the relativedistances of 2 microphones to the single point sources (t).
We stress that this approximation requires noside knowledge about the true positions of the par-ticipants or of their microphones.Importantly, this interpretation is valid only if ?
?lies within the integration window ?
in Equation 3.In (Boakye and Stolcke, 2006), the authors showedthat when the analysis window is 25 ms, the NMXCfeature is not as robust as frame-level energy flooringfollowed by cross-channel normalization (NLED).3 Experimental Setup3.1 VAD and ASR SystemsOur multispeaker VAD system, shown in Figure 1,was introduced in (Laskowski and Schultz, 2006).Rather than detecting the 2-state speech (V) vs.non-speech (N ) activity of each partipant indepen-dently, the system implements a Viterbi search forthe best path through a 2K-state vocal interac-tion space, where K is the number of participants.Segmentation consists of three passes: initial la-bel assignment (ILA), described in the next subsec-tion, for acoustic model training; simultaneous multi-participant Viterbi decoding; and smoothing to pro-duce segments for ASR.
In the current work, duringdecoding, we limit the maximum number of simulta-neously vocalizing participants to 3.This system is an improved version of that fieldedin the NIST Rich Transcription 2006 Meeting Recog-nition evaluation (RT06s)2, to produce automaticsegmentation in the IHM condition on conferencemeetings.
The ASR system which we use in thispaper is as described in (Fu?gen et al, 2007).3.2 Unsupervised ILAFor unsupervised labeling of the test audio, prior toacoustic model training, we employ the criterionq?
[k] =??
?V if?j 6=klog(max?
?jk(?
)?jj(0))> 0N otherwise .
(8)Assuming equality in Equation 6, this correspondsto declaring a participant as vocalizing when the dis-tance between the location of the dominant soundsource and that participant?s microphone is smallerthan the geometric mean of the distances from thesource to the remaining microphones, ie.
whenK?1?
?j 6=kdj > dk (9)2http://www.nist.gov/speech/tests/rt/90AMTRAININGVITERBIDECODINGAMILAmultichannel audioREFRAMINGSMOOTHING ASRPass 1WER??`q?
?q?q?FFigure 1: VAD system architecture, with 4 error measurement points.
Symbols as in the text.We refer to this algorithm as ILAave.
For contrast wealso consider ILAmin, with the sum in Equation 8 re-placed by the minimum over j 6=k.
This correspondsto declaring a participant as vocalizing when the dis-tance between the location of the dominant soundsource and that participant?s microphone is smallerthan the distance from the source to any other mi-crophone.
We do not consider ILAmax, whose inter-pretation in light of Equation 6 is not useful.3.3 DataThe data used in the described experiments con-sist of two datasets from the NIST RT-05s andRT-06s evaluations.
The data which had beenused for VAD system improvement, rt05s eval*,is the complete rt05s eval set less one meeting,NIST 20050412-1303.
This meeting was excludedas it contains a participant without a microphone, acondition known a priori to be absent in rt06s eval;we use the latter in its entirety.3.4 Description of ExperimentsThe experiments we present aim to compare ILAaveand ILAmin, and to show how the size of the inte-gration window, ?, affects system performance.
Asour VAD decoder operates at a frame size of 100ms,we introduce a reframing step between the ILA com-ponent and both AM training and decoding; see Fig-ure 1.
V is assigned to each 100ms frame if 50% ormore of the frame duration is assigned V by ILA;otherwise, the 100ms frame is assigned an N label.We measure performance in four locations withinthe combined VAD+ASR system architecture, alsoshown in Figure 1.
We compute a VAD frame er-ror just after reframing (q?F ), just after decoding(q?
), and just after smoothing (?
(q?)).
This er-ror is the sum of the miss rate (MS), and the falsealarm rate excluding intervals of all-participant si-lence (FAX), computed against unsmoothed word-level forced alignment references.
We use this met-ric for comparative purposes only, across the vari-ous measurement points.
We also use first-pass ASRword error rates (WERs), after lattice rescoring, asa final measure of performance impact.We evaluate, over a range of ILA frame sizes, theperformance of ILAave(3), with a maximum numberof simultaneously vocalizing participants of 3, andfor the contrastive ILAmin.
We note that ILAminis capable of declaring at most one microphone at atime as being worn by a current speaker.
As a re-sult, construction of acoustic models for overlappedvocal activity states, described in (Laskowski andSchultz, 2006), results in states of at most 2 simul-taneously vocalizing participants.
We therefore referto ILAmin as ILAmin(2), and additionally considerILAave(2), in which states with 3 simultaneously vo-calizing participants are removed.4 Results and DiscussionWe show the results of our experiments in Ta-ble 2.
First-pass WERs, using reference segmenta-tion (.stm), vary by 1.3% absolute (abs) betweenrt05s eval and rt06s eval.
We also note that re-moving the one meeting with a participant withouta microphone reduces the rt05s eval manual seg-mentation WER by 1.7% abs.
WERs obtained withautomatic segmentation should be compared to themanual segmentation WERs for each set.As the q?F columns shows, ILAmin(2) entails sig-nificantly more VAD errors than ILAave.
Notably,although we do not show the breakdown, ILAmin(2)is characterized by fewer false alarms, but missesmuch more speech than ILAave(2).
This is due inpart to its inability to identify simultaneous talk-ers.
However, following acoustic model training anduse (q?
), the VAD error rates between the two algo-rithms are approximately equal.In studying the WERs for each ILA algorithm in-dependently, the variation across ILA frame sizes inthe range 25?100 ms can be significant: for example,it is 1.2% abs for ILAmin(2) on rt06s eval, com-pared to the difference with manual segmentation of3.1% abs.
Error curves, as a function of ILA framesize, are predominantly shallow parabolas, except at75 ms (notably for ILAmin(2) at q?F ); we believe that91VAD, rt05s WER, 1st passILA ?
q?F q?
?
(q?)
05 05* 06a 100 31.3 16.7 16.0 39.0 34.1 39.6v 75 33.6 16.6 15.9 38.9 34.1 39.9e 50 35.2 16.7 16.0 38.8 34.0 39.33 25 36.8 17.3 16.3 39.6 34.2 39.7a 100 31.3 15.8 15.2 37.8 34.4 39.7v 75 33.6 15.6 15.0 37.9 34.4 39.6e 50 35.2 15.8 15.2 37.6 34.3 39.32 25 36.8 16.4 15.6 38.1 34.3 39.5m 100 43.4 15.8 14.7 38.2 35.2 39.3i 75 51.9 15.6 14.6 38.1 35.2 39.3n 50 47.1 15.7 14.6 37.9 35.1 40.12 25 47.7 16.2 14.9 38.1 35.4 40.5refs 9.5 9.5 9.5 36.1 34.4 37.4Table 2: VAD errors, measured at three points in oursystem, and first-pass WERs for rt05s eval (05),as well as first-pass WERs for rt05s eval* (05*)and rt06s eval (06).
Results are shown for 3 con-trastive VAD systems (ILAave(3), ILAave(2) andILAmin(2)), and 4 ILA frame sizes (100ms, 75ms,50ms, and 25ms).this is because 75 ms does not divide evenly into thedecoder frame size of 100 ms, causing more deletionsacross the reframing step than for other ILA framesizes.
Error minima appear for an ILA frame sizesomewhere between 50 ms and 75 ms, for both ASRand post-decoding VAD errors.Although (Pfau et al, 2001) considered a maxi-mum lag of 250 samples (15.6ms, or 5m at the speedof sound), their computation of S-Norm XC useda rectangular window.
Here, as in (Laskowski andSchultz, 2006) and (Boakye and Stolcke, 2006), weuse a Hamming window.
Our results suggest that alarge, broadly tapered window is important for Equa-tion 6 to hold.The table also shows that for datasets with-out uninstrumented participants, rt05s eval*and rt06s eval, ILAmin(2) is outperformed byILAave(2) by as much as 1.1% abs in WER, espe-cially at small frame sizes.
The difference for the fullrt05s eval dataset is smaller.
The results also sug-gest that reducing the maximum degree of simulta-neous vocalization from 3 to 2 during decoding is aneffective means of reducing errors (ASR insertions,not shown) for uninstrumented participants.5 ConclusionsWe have derived a geometric approximation for aparticular type of normalization of maximum cross-channel correlation, NT-Norm XC, recently intro-duced for multispeaker vocal activity detection.
Ourderivation suggests that it is effectively comparingthe distance between each speaker?s mouth and eachmicrophone.
This is novel, as geometry is most ofteninferred using the lag of the crosscorrelation maxi-mum, rather than its amplitude.Our experiments suggest that frame sizes of 50?75ms lead to WERs which are lower than those for ei-ther 100 ms or 25 ms by as much as 1.2% abs; thatILAave outperforms ILAmin as an initial label as-signment criterion; and that reducing the degree ofsimultaneous vocalization during decoding may ad-dress problems due to uninstrumented participants.6 AcknowledgmentsThis work was partly supported by the EuropeanUnion under the integrated project CHIL (IST-506909), Computers in the Human Interaction Loop.ReferencesK.
Boakye and A. Stolcke.
2006.
Improved SpeechActivity Detection Using Cross-Channel Features forRecognition of Multiparty Meetings.
Proc.
of INTER-SPEECH, Pittsburgh PA, USA, pp1962?1965.C.
Fu?gen, S. Ikbal, F. Kraft, K. Kumatani, K. Laskowski,J.
McDonough, M. Ostendorf, S. Stu?ker, and M.Wo?lfel.
2007.
The ISL RT-06S Speech-to-Text Evalu-ation System.
Proc.
of MLMI, Springer Lecture Notesin Computer Science 4299, pp407?418.Z.
Huang and M. Harper.
2005.
Speech Activity Detec-tion on Multichannels of Meeting Recordings.
Proc.
ofMLMI, Springer Lecture Notes in Computer Science3869, pp415?427.K.
Laskowski, Q. Jin, and T. Schultz.
2004.Crosscorrelation-based Multispeaker Speech ActivityDetection.
Proc.
of INTERSPEECH, Jeju Island,South Korea, pp973?976.K.
Laskowski and T. Schultz.
2006.
Unsupervised Learn-ing of Overlapped Speech Model Parameters for Multi-channel Speech Activity Detection in Meetings.
Proc.of ICASSP, Toulouse, France, I:993?996.T.
Pfau and D. Ellis and A. Stolcke.
2001.
Multi-speaker Speech Activity Detection for the ICSI Meet-ing Recorder.
Proc.
of ASRU, Madonna di Campiglio,Italy, pp107?110.S.
Wrigley, G. Brown, V. Wan, and S. Renals.
2003.Feature Selection for the Classification of Crosstalkin Multi-Channel Audio.
Proc.
of EUROSPEECH,Geneva, Switzerland, pp469?472.S.
Wrigley, G. Brown, V. Wan, and S. Renals.
2005.Speech and Crosstalk Detection in Multichannel Au-dio.
IEEE Trans.
on Speech and Audio Processing,13:1, pp84?91.92
