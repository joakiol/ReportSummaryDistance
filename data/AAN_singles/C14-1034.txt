Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 345?354, Dublin, Ireland, August 23-29 2014.Influence of Target Reader Background and Text Features on TextReadability in Bangla: A Computational ApproachManjira SinhaDepartment of ComputerScience and EngineeringIndian Institute of TechnologyKharagpurWest Bengal, Indiamanjira@cse.iitkgp.ernet.inTirthankar DasguptaDepartment of ComputerScience and EngineeringIndian Institute ofTechnology KharagpurWest Bengal, Indiatirtha@cse.iitkgp.ernet.inAnupam BasuDepartment of ComputerScience and EngineeringIndian Institute ofTechnology KharagpurWest Bengal, Indiaanupam@cse.iitkgp.ernet.inAbstractIn this paper, we have studied the effect of two important factors influencing text readability inBangla: the target reader and text properties.
Accordingly, at first we have built a novel Banglareadability dataset of 135 documents annotated by 50 readers from two different backgrounds.We have identified 20 different features that can affect the readability of Bangla texts; thefeatures were divided in two groups, namely, ?classic?
and ?non-classic?.
Preliminarycorrelation analysis reveals that text features have varying influence on the text hardness statedby the two groups.
We have employed support vector machine (SVM) and support vectorregression (SVR) techniques to model the reading difficulties of Bangla texts.
In addition todeveloping different models targeted towards different type of readers, separate combinationsof features were tested to evaluate their comparative contributions.
Our study establishes thatthe perception of text difficulty varies largely with the background of the reader.
To the best ofour knowledge, no such work on text readability has been recorded earlier in Bangla.1 IntroductionReadability of a text generally refers to how well a reader is able to comprehend the content of a text,through reading (Dale and Chall, 1948).
Readability is a complex cognitive phenomenon where, thecognitive load of a text for a reader depends on both the characteristics of a text like, lexical choice,syntactic complexity, semantic complexity, discourse level complexity and on the background of theuser.
Several experiments have already established that readability of texts are quite languagedependent and existing readability measures in English cannot directly be used to compute readabilityof other languages like, Bangla and Hindi (Sinha et al., 2012).
Yet, compared to the numerousreadability measures in English and other European languages(Benjamin, 2012), few initiatives havebeen taken to compute text readability in a Eastern Indo-Aryan language like Bangla or any otherIndian languages which are structurally very different from many of their Indo-European cousins suchas English, which is of West-Germanic descent (Sinha et al., 2012).
One important factor that affectsthe readability of a text is the background of the respective reader.
According to Dale (Dale, 1949),?The interpretation of the expressed thought is related more to the reader?s informational backgroundand motivations than to the internal evidences of the expressional facility of the author?.
Reader?sbackground is a complex derivative of one?s educational and socio-economic state.
As per one of thepioneering works in readability by Dale and Chall (1949), the outcome of reading depends on manycharacteristics of the prospective readers including ?reading abilities, interests, age, sex, intellectualThis work is licensed under a Creative Commons Attribution 4.0 International License.345maturity, background of information etc.?
However, we do not know of any such investigations forBangla text readability that have investigate the way background of a reader affect the readability oftext.
Such language specific study is needed as Bangla as a language is very different from Englishand the inapplicability of English readability formulae for Bangla text has already been established.Considering the above issues as our motivation, in this paper we have developed models to predictreading difficulty of a Bangla document perceived according to different target reader groups.
Tocategorize among different reader groups, we have considered age, education and socio-economic dataas indicators of comprehension ability.
In addition, we have also explored the impact of different typesof text features on text comprehensibility in Bangla.
However, development and evaluation of suchmodel requires availability of well-annotated resources.
To the best of our knowledge, noautomatically accessible data annotated according to the reading difficulty level is available forBangla.
Therefore, we have developed a digital resource pool of Bangla text documents in Unicodeencoding that can be used for various NLP tasks such as feature extraction, document analysis etc.Such a dataset is essential to analyze readability of text documents based on the target reader.
Next,we have visualized the text readability problem from a machine learning perspective as a classificationproblem using support vector machines (SVM) and an estimation problem using support vectorregression (SVR).
Our study is based on a wide range of textual features, from the syntactic andlexical features of a text like, its average sentence length, average word length in terms of visual units,to discourse level features like, number of jukta-akshars (consonant conjuncts) , number of differentparts of speeches, named entity and lexical relations (refer to section 3).
Although regression analysishas been previously used to model the text readability in Bangla, reader group specific analysis andmachine learning techniques like support vectors have not been used so far.
We have considered twotarget reader groups namely Group-1(or Adult group) with average age of 23 Yrs and Group-2 (orminor?s group) with average age of 15 Yrs.The organization of the paper is as follows: section 2 presents a brief literature survey on existingreadability metrics for English and Bangla; section 3 defines the features of a text considered in thisstudy, and empirical data collection, section 4 discusses the experiment observations, the predictiontechniques and presents the results and validations for the two techniques.
Finally, section 5 offersconclusion and perspective.2 Related WorksThe quantitative analysis of text readability started with L.A. Sherman in 1880 (Sherman, 1893).
Tilldate, English and other languages have got over 200 readability metrics (DuBay, 2004; Rabin et al.,1988).The existing quantitative approaches towards predicting readability of a text can be broadlyclassified into three categories (Benjamin, 2012):Classical methods: they analyze the syntactic features of a text like sentence length, paragraphlength etc.
The examples are Flesch Reading Ease Score (Flesch, 1948), FOG index (Gunning, 1968),Fry graph (Fry, 1968), SMOG (McLaughlin, 1969) etc.
The formulae do not take into account thebackground of the reader and the semantic features of the text such as whether the actual contents aremaking sense or not.
Despite their shortcomings, these simple metrics are easy to calculate andprovide a rough estimation of reading difficulty of a text provided.Cognitively motivated methods: texts are analyzed based on the cognitive features like, cohesion,organization and users?
background.
Proposition and inference model (Kintsch and Van Dijk, 1978),prototype theory (Rosch, 1978), latent semantic analysis (Landauer et al., 1998), Coh-metrix (Graesseret al., 2004) are some prominent members of this group.
This group of models moves beyond thesurface features of a text and try to measure objectively the different cognitive indicators associatedwith text and the reader.
However, it has been observed that, many situations, some traditionalindicators perform as well as the newer and more difficult versions (Crossley et al., 2007).Statistical language modeling: This class of approaches incorporates the power machine learningmethods to the field of readability.
They are particularly useful in determining readability of web texts(Collins-Thompson and Callan, 2005; Collins-Thompson and Callan, 2004; Si and Callan, 2003) (Liuet al., 2004).
SVM has been used to identify grammatical patterns within a text and classificationbased on it (Schwarm and Ostendorf, 2005; Heilman et al., 2008; Petersen and Ostendorf, 2009).Although, these methods sound promising, the problem is that they cannot act as standalone measure:346they need an amount of training data for classifiers appropriate to a particular user group and oftenthese measures takes into account complex text features which for resource poor languages needmanual effort to annotate.In Bangla, only a couple of works have been executed on text readability.
Das and Roychoudhury(Das and Roychoudhury, 2006) studied a miniature model with respect to one parametric and twoparametric fits.
They have used seven paragraphs from seven literary texts.
They considered twostructural features of a text: average sentence length and number of syllables per 100 words.
Theyfound the two-parametric fit as better performer.
Sinha et al.
(Sinha et al., 2012) has developed tworeadability formulae for Bangla texts using regression analysis.
For their study sixteen texts of length,about 100 words were used.
They have considered six structural or syntactic features of a text for thework.
They have demonstrated that the English readability formulae such as Flesch Reading EaseIndex, SMOG Index do not perform appropriately while being applied to Bangla documents.
Theyhave found the textual features like average word length, number of polysyllabic words and number ofjukta-akshars in a text to be the most influential ones.
Both the works mentioned have taken intoaccount a small subset of potentially important text features; none them have considered feature suchas the extent of text cohesion.
Moreover, their study did not explore the influence of readers?background on text readability.
In our study, we have addressed the issue of readers?
background aswell as the effect of features at different textual level.3 Empirical Data CollectionAs mentioned, there is no annotated data present in Bangla, which can provide a direct classificationof text difficulty for Bangla readers.
Therefore, we have undertaken an effort to annotate theexperiment texts with the target readers of Bangla.3.1 ParticipantsOur objective in this study is to investigate how readability varies with the background of the reader.Therefore, two different target reader groups have been considered to study the relationship of effectof text parameters on comprehension and user background.
SEC1 or socio-economic classification hasbeen stated according to the standards of Market Research Society of India (MRSI).
MRSI has defined12 socio-economic strata: A1 to E3, in the decreasing order.
These strata have been designed based onthe education level of the chief wage earner of the family and the number of ?consumer durables?
(asper a predefined list including agricultural land) owned by the family.
It has been seen that this way ofgrading reflect the social and economic position of a household in terms of fields such as education,awareness etc.
As can be inferred from the chart, the participants range from classes C2 to E1 (C2, D1,D2, E1), which represents the medium to low social-economic classes.Type BackgroundMean age(Standarddeviation)Group 1 (adult): 25 native speakersof BanglaEducation: pursuing graduation 22.8 (1.74)SEC: C2-E1Group 2 (minors): 25 nativespeakers of BanglaEducation: pursuing secondary or highersecondary15 (1.24)SEC: C2-E2Table1: User Statistics3.2 Readability corpus preparationWe have stated in the introduction about the scarcity of annotated digital resource pool in Banglauseful for automatic processing.
Although there are a few works on text readability in Bangla, the datais not available in accessible formats.
To address the problem, we have developed a corpus of Bangladocuments.
The current size of the resource is about 250 documents of length about 2000 wordsspanning over broad categories such as News, literature, blogs, articles etc.
A number of different text1 http://imrbint.com/research/The-New-SEC-system-3rdMay2011.pdf347features were computed against each document.
The descriptions of the features and the justificationfor them have been stated below.3.3 Feature selection:Inferring from the cognitive load theory (Paas et al., 2003), we have assumed that the cognitive loadexerted by a text on a reader depends on syntactic and lexical properties of a text like, averagesentence length, average word length, number of polysyllabic words and as well as discourse featureslike the counts of the different parts of speeches and the number of co-references one has to resolve inorder to comprehend the text.
The logic behind such assumptions is as follows: while processing a texta user has to parse the sentences in it and extract semantically relevant meaning from those sentencesand the words.
In order to process a sentence, one has to take into account the length of the sentenceand types of words contained in it; in addition, to infer the meaning of a sentence, it is important toestablish the connections or the nature of dependencies among the different words in a sentence.
Therole of a word is determined by its parts of speech and its way of use in that context; apart from it, thewords can have varied complexity based on factors like their length, count of syllables.
Similarly, atthe discourse level, a reader not only has to comprehend each sentence or paragraph, but also has toinfer the necessary co-references among them to understand the message conveyed by the text.
Thecomplexity of this task depends on the number of entities (noun, proper nouns) in the text, how oneentity is connected with other, relationships like synonymy, polysemy, and hyponymy.
To capture theeffects of all these parameters in our readability models, we have considered text features over a broadrange.
The details of the features are presented in Table 2.
The word features like average word length,average syllable per word, sentence features like average sentence length and discourse features likenumber of polysyllabic words, number of jukta-akshars (consonant conjuncts) have been calculated asstated by Sinha et al.
(Sinha et al., 2012), as the features need customizations for Bangla.
Thecalculations based on lexical chains have been followed from Galley and McKeown (Galley andMcKeown, 2003).Feature Descriptionword featuresaverage word length Bangla orthographic word consists of a combination of four types of graphemes2,each of them is considered as a single visual unit.
Average word length is totalword length in terms of visual units divided by number of words.average syllable per word Total word length in terms of syllable divided by total number of words.sentence featuresaverage sentence length Total sentence length in terms of words divided by number of sentence.$(noun phrase) Average number of NP per sentence$(verb phrase) Average number of VP per sentence$(adjective) Average number of adjectives per sentence$(postposition) Average number of postpositions per sentence.
Bangla grammar has postpositions,instead of prepositions present in English.
Unlike English, postpositions in Banglado not belong to separate part of speech.
The postpositions require their objectnoun to take possessive, objective or locative case.
Suffixes act as the casemarkers.$(entity) average number of named entity per sentence$(unique entity) Average number of unique entity per sentence$(clauses) Average number of clauses per sentence2 http://en.wikipedia.org/wiki/Bengali_alphabet#Characteristics_of_the_orthographic_word348discourse featuresNumber of polysyllabicwords and normalizedmeasure for 30 sentencesPolysyllabic words are the words whose count of syllable exceeds 2.number of jukta-akshars(consonant conjuncts)Total number of jukta-akshars in a text of 2000 words.
It is an important featurefor Bangla because each of the clusters has separate orthographic andphonemic (in some cases) representation than the constituents consonants.#(noun phrase) Total number of NP in the document#(verb phrase) Total number of VP in the document#(adjective) Total number of adjective in the document.#(postposition) Total number of postpositions in the document.#(entity) Total number of named entity in the document#(unique entity) Total number of unique entity in the document#(lexical chain)* Total number of lexical chain in the documentaverage lexical chainlength*Computed over the documentTable2: Details of text features considered for the studyThe features marked with * in the above table have been manually annotated against each text.
The otherfeatures, though they are computed automatically, a round of manual checking was incorporated for the sake ofcorrectness.Expert annotations and user annotations:Since there is no formal ranking of Bangla texts according to their reading levels, therefore, thedocuments were then annotated by language experts to approximate the suitable reading level for eachdocument.
However, to develop any practical readability application, feedbacks from actual users arenecessary.
From the resource pool mentioned in Introduction, 135 texts were chosen for the presentstudy: two sets of distinct 45 texts were for each group: for the adult group those were the textsannotated by experts to have relatively high reading level and for the minor?s group, the texts wereannotated as having relatively low reading level; pairwise t-test were performed between the two typeof text features to assure that their difference is significant (p<0.05).The rest 45 texts are common to both the groups to account for the difference in comprehension forthe same document and the assumption that may in some cases group 2 participants have comparablereading skill as of group 1: consequently, the texts annotated by experts as demanding high readinglevel were selected for this purpose.
These were required to ensure that the experimental data spansover a broad range and is unbiased.
The text details are presented in table 2 below.Source of TextsNumber of textsGr.1 Gr.2 commonLiterary corpora_classical  5  5 5Literary corpora_contemporay 6  5 6News corpora_general news 6  6  5News corpora_interview 5 6 6Blog corpora_personal 6  5 5Blog corpora_official 5 5 5Article corpora_ scholar 6 7  7Article corpora_general 6  6 6Table3: Text detailsEach participant was asked 2 questions: ?How easy was it for you to understand/comprehend thetext??
and ?How interesting was the reading to you??.
Against each question, they were to answer on a5 point scale (1=easy, 5=very hard).
Inter-rater reliability was measured through Krippendorff?s alpha33 http://en.wikipedia.org/wiki/Krippendorff's_alpha349and ?
= 0.81 was found.
Therefore, we concluded that annotators agree more often than would haveoccurred by chance.
We have measured the correlation between the outcomes of two questionscorresponding to each of the fifty annotators; and found that in each case the correlation was greaterthan 0.8 (p < 0.05).
Therefore, the questions can be considered as equivalent, and subsequently wehave considered the rating for the first question as user input for our readability models.Corresponding to each text, the average of the user ratings was considered for further processing.4 Analysis and Model Development4.1 Correlation coefficientsWe have performed partial spearman correlation between each of the features and user rating.
Table 4presents some of the examples from each type of features due to the space limitation; resultscorresponding to other features are also described subsequently.
The following features have selectedas they have been used in the existing literature for Bangla (Sinha et al., 2012).
The correlations arepresented separately for the distinct texts and the common texts delivered to the two groups of users.This will allow us to investigate is there any significance difference of reading feedbacks between thedifferent target populations.Feature Correlation coefficient r (Significance(if p<0.05) p value)Different texts Common textsGr.
1 Gr.
2 Gr.1 Gr.
2Word featuresaverage sentence length 0.8 (0.0017) 0.33(0.2011) 0.75 (0.0013) 0.54 (0.08)average word length 0.60 (0.0142) 0.73(0.0041) 0.66 (0.0026) 0.8 (0.0032)Sentence featuresaverage syllable perword0.66 (0.06) 0.64(0.0047) 0.60(0.07) 0.75(0.0043)Discourse featuresnumber of polysyllabicwords0.73 (0.0013) 0.74 (0.0008) 0.67(0.0021) 0.65(0.0006)normalized measure for30 sentences0.76(0.0011) 0.66 (0.0041) 0.65 (0.0015) 0.66(0.0032)number of jukta-akshars  0.87 (0.0018) 0.39 (0.1228) 0.81 (0.0024) 0.85 (0.0043)Table 4: Correlation coefficients (user rating vs text features)Some interesting observations can be made from the above table:?
Average sentence length or mean number of words per sentence have been long found to be astrong predictor of text difficulty [1].
In our case, while this holds true for the adult data, thecorrelation is less for the minors and it is not significant.?
Average syllable per word does not hold significant correlation for the adult data in both casesbut it does for the minor?s group?
Jukta-akshars or consonant conjuncts have major impact on text readability in Bangla (Sinhaet al., 2012).
For adult data, it can be seen that this feature has a strong and significantcorrelation, which not true for the user data of group 2 for separate texts.
On the other hand, forthe common texts this feature was found to have high significant correlation with both thereader groups.
This is may be due to the nature of the common texts.?
Apart from the above two cases, the above table also presents evidence in support of the factthat the reader?s perception of text difficulty in relation to text features changes with the targetreader background.The impact of the remaining features has been discussed here with respect to the two different types oftext scenarios:350Distinct texts for two groups:?
In case of the readers from the first group, the user ratings have high correlation (?
> 0.65)with $(clauses), #(verb phrases), #( unique entity), #(lexical chain) and  average lexical chainlength.
The correlations are also significant.
However, the correlations with $(noun phrase),$(verb phrase) $(postpositions), #(postpositions), #(adjective) were found to be insignificant.The correlation of user annotation with features such as $(entity), $(unique entity) were foundto be low (?
< 0.45) but significant.?
The group 2 readers were found to show high (?
> 0.65) and significant correlation with $(verbphrases), $(unique entity), $(clauses), #(entity), #(lexical chain) and average lexical chain span.The correlations with $(postposition), #(postpositions) were not significant.
Features like$(noun phrase), $(adjective) and #(adjective) were found to have low (?
< 0.45) but significantcorrelations with user ratings.Common texts for both groups:?
It has been observed that the group 2 user ratings have higher correlation with the sentence levelfeatures than the discourse level features.
In particular, features such as number of $(nounphrase), $(adjective), $(unique entity) and $(clauses) have high correlation with the textdifficulty ratings provided by the minor?s group.
Among the discourse level features #(entity)and #(unique entity)have a high correlation, but #(verb phrase), #(adjective) were found to havenot significant influence.?
On the other hand, the adult data are more inclined towards discourse features such as #(nounphrase) and #(verb phrase),  #(unique entity) in a document.
This may be due to the ability ofthe older people to comprehend the text as a whole rather than inferring meaning fromindividual units at a time.
From sentence level feature $(clause) was found to be significant andimportant in terms of correlation, but $(noun phrase), $(adjective) do not bear significantcorrelation.?
Properties like lexical chain, which require a reader to establish connections among differentattributes of a concept have great significance for both group1 and group2 annotations.?
For both the user groups the influence of average $(postposition and #(postposition) were foundto be little and insignificant.From the above discussions, it is evident that the two different target reader groups show a largedifference in their reading pattern and perception of text difficulty.
The difference has been observedin both the cases: when they were presented with different type of texts and with same texts.Therefore, it has been established that the target reader background plays an important role inmodelling text difficulty.
Accordingly, in the following sections, we have developed different modelsof different reader groups, and in the process we have also shown that the models have differentparameter values and configurations.4.2 Computational modellingAnalyses of correlation coefficients give an estimation of trend in user ratings against text features.The next step is to develop suitable models for automatic readability prediction.
To achieve theobjective, we have used machine-learning methods such as support vector machine (SVM) andsupport vector regression (SVR) techniques.
In addition, we have also presented a comparative studyof performances of different text features in readability model building in this section.
The featureshave been used in three combinations.
First they were divided in  two categories i) comprising of onlythe six features mentioned in table 4 as they represent the ?classical?
features used extensively tomodel text readability, and ii) second category consists of the rest 14 features and the group is termed?non-classical?
, this yielded the first two combinations.
The third combination consists of all thefeatures.
Therefore, we have evaluated six different types of SVM and SVR models for each group.We have employed a binary SVM classifier here.
Given a training set instance-class pairs (??
,??
), i= 1?l, where ??
?
??
and ?
?
1,?1 l  , the general equation of a SVM is (Manning et al., 2008):35112???
+  ?
??l???
?????????,?
= ??????
??????,?
= ??????????????
????
?
(equation: 1)??
???
??
+ ?
?
1?
??
, ??
?????
????????
?
0               ?
(equation: 2)In this work, we have taken 90 texts against each group of users by combining the 45 reader groupspecific texts and 45 common texts (refer to section 3).
Then for each category of reader, the textswere shuffled randomly.
We have used 70 texts for training and 20 texts for evaluation of the modeland performed 2-fold cross validation.
The minimum, maximum and median of the rating distributionlie respectively at (2.33), (8.4) and (5.92) for adult (group1) and at (1.83), (8.2) and (5.5) for minor(group 2).
To train and test the SVM models, we needed to spit the data in two classes ( easy andhard), this has been done by assigning the ratings less than the median in to class easy (label ?-1?)
andthe rest to the class hard (label ?1?
), i.e., the user ratings were mapped to the label space ?.
In case ofSVR, the label space mapping was not required.
The text features were mapped to the feature space ??
.Although we have tested four types of kernel functions: linear, polynomial, radial basis and sigmoidon the data using LIBSVM (Chang and Lin, 2011) software, here only the results corresponding tolinear and polynomial kernels have been presented as the other two kernels performed poorly.
Toevaluate the quality of the classifications for SVM, multiple correlation (R) and percentage of textsaccurately classified (Acc) have been used.
R denotes the extent to which the predictions are close tothe actual classes and its square (R2) indicates the percentage of dependent variable variation that canbe explained by the model.
Therefore, while percentage accuracy is an indicator to how well the modelhas performed to classify, R indicates the extent of explanatory power it posses.
A better fit will havelarge R-value as well as Acc.
For SVR, root mean square error (RMSE) has been reported instead ofAcc; a good fit will have less RMSE.
Below tables present, the SVM and SVR results for adult andminor?s data for different kernels and different combination of features.
The kernels were evaluatedfor a number of SVM parameter combinations and only the result corresponding to the most efficientone is presented.Features Classic features Non-classic features All featuresSVM parameters C = 10; d = 2; r = 0; ?
= 1/6 = 0.1; ??
= 0.01 (total support vector = 28)Kernel R Acc.
R Acc.
R Acc.linear 0.75 76% 0.73 79% 0.80 87%Polynomial 0.73 75% 0.72 75% 0.75 79.5%Table 5: SVM for group1 readersFeatures Classic features Non-classic features All featuresSVM parameters C = 1; d = 2; r = 0; ?
= 1/6 = 0.1; ??
= 0.001 (total support vector = 22 )Kernel R Acc.
R Acc R Acc.Linear 0.75 75% 0.72 77% 0.83 86%Polynomial 0.71 70% 0.73 72% 0.78 76%Table 6: SVM for group2 readersFeatures Classic Non-classic features All featuresKernel R RMSE R RMSE R RMSElinear 0.56 1.6 0.53 1.7 0.68 1.1Polynomial 0.43 2.2 0.47 11.2 0.56 23.3Table 7: SVR for group1 readersFeatures Classic Non-classic features AllKernel R RMSE R RMSE R RMSElinear 0.50 1.5 0.54 1.4 0.65 1.2Polynomial 0.47 3.1 0.45 15.5 0.51 29.7352Table 8: SVR for group2 readersFrom table 5 and table 6, it can be seen that the SVM for the two target reader groups differsignificantly in term of parameter attributes and their accuracy.
It is also evident that incorporatingonly non-classic features versus classic features improves the accuracy of SVM very slightly and bothtypes of features have similar explanatory power; combining both the classic and non -classic featureimproves the accuracy and multiple correlations significantly.
The SVR from table 7 and table 8 showthe similar trend in terms of feature performances: classic and non-classis features have comparableRMSE and R, but there is significant gain when the two types are taken together.
The regressionequations for group1 and group2 readers differ in the coefficients of the feature variables; these implythat the two groups require different readability models.
Moreover, the linear kernel was found toperform better than the polynomial kernel in all the cases.5 ConclusionIn this paper, we have studied the effect of two important factors affecting text readability in Bangla:the target reader and text properties.
We have found that the perception of text difficulty varies largelywith the background of the reader.
Accordingly, we have developed computational models to computereadability of Bangla text documents based on the target reader group.
In order to achieve our goal wehave first developed a novel Bangla dataset annotated in terms of text readability by users withvarying age group.
A preliminary analysis of the reading pattern of each target group was performedby analysing the correlation of text features with user annotations.
Next, we have applied the SVMclassifier to classify text documents into two different classes namely, hard and easy; the SVM for thetwo reader groups have different properties, implying the difference between two correspondingmodels.
We have also compared the performance of the classifier based on the feature set they use.We observed that in contrast to applying only the classical features or the non-classic features,performance of the classifier improves if both types of features are used.
This is true for both the adultas well as the minor?s dataset.
Overall, we have achieved an accuracy of around 86% for the minor?sdataset and 87% for the adult dataset respectively.
In addition to classification, support vectorregression has been used to model text difficulty from an estimation perspective.
The result of theSVR also establishes our previous findings.
To the best of our knowledge, no such work on textreadability has been recorded earlier in Indian languages, especially in Bangla.
The next step of thisstudy is to analyse the performance of the readability formula from one group (say adult) when appliedto the other group (say minors) and vice versa.
We will also repeat our study with more spread apartuser groups spread over less diverse economic strata.
In future, we are planning to develop for multi-class text readability models.
The work will also be extended to model text comprehensibility forreading disabilities in Bangla.ReferenceBenjamin, R. (2012).
Reconstructing readability: Recent developments and recommendations in the analysis oftext difficulty.
Educational Psychology Review, 24:1?26.Chang, C.-C. and Lin, C.-J.
(2011).
Libsvm: a library for support vector machines.
ACM Transactions onIntelligent Systems and Technology (TIST), 2(3):27.Collins-Thompson, K. and Callan, J.
(2004).
A language modeling approach to predicting reading difficulty.
InProceedings of HLT/NAACL, volume 4.Collins-Thompson, K. and Callan, J.
(2005).
Predicting reading difficulty with statistical language models.Journal of the American Society for Information Science and Technology, 56(13):1448?1462.Dale, E. (1949).
Readability.Dale, E. and Chall, J.
(1948).
A formula for predicting readability.
Educational research bulletin, pages 11?28.Das, S. and Roychoudhury, R. (2006).
Readability modelling and comparison of one and two parametric fit: Acase study in bangla*.
Journal of Quantitative Linguistics, 13(01):17?34.DuBay, W. (2004).
The principles of readability.
Impact Information, pages 1?76.Flesch, R. (1948).
A new readability yardstick.
Journal of applied psychology, 32(3):221.353Fry, E. (1968).
A readability formula that saves time.
Journal of reading, 11(7):513?578.Galley, M. and McKeown, K. (2003).
Improving word sense disambiguation in lexical chaining.
In IJCAI,volume 3, pages 1486?1488.Graesser, A., McNamara, D., Louwerse, M., and Cai, Z.
(2004).
Coh-metrix: Analysis of text on cohesion andlanguage.
Behavior Research Methods, 36(2):193?202.Gunning, R. (1968).
The technique of clear writing.
McGraw-Hill NewYork, NY.Heilman, M., Collins-Thompson, K., and Eskenazi, M. (2008).
An analysis of statistical models and features forreading difficulty prediction.
In Proceedings of the Third Workshop on Innovative Use of NLP for BuildingEducational Applications, pages 71?79.
Association for Computational Linguistics.Kintsch, W. and Van Dijk, T. (1978).
Toward a model of text comprehension and production.
Psychologicalreview, 85(5):363.Landauer, T., Foltz, P., and Laham, D. (1998).
An introduction to latent semantic analysis.
Discourse processes,25(2-3):259?284.Liu, X., Croft, W., Oh, P., and Hart, D. (2004).
Automatic recognition of reading levels from user queries.
InProceedings of the 27th annual international ACM SIGIR conference on Research and development ininformation retrieval, pages 548?549.
ACM.Manning, C. D., Raghavan, P., and Sch?tze, H. (2008).
Introduction to information retrieval, volume 1.Cambridge University Press Cambridge.McLaughlin, G. (1969).
Smog grading: A new readability formula.
Journal of reading, 12(8):639?646.Paas, F., Renkl, A., and Sweller, J.
(2003).
Cognitive load theory and instructional design: Recent developments.Educational psychologist, 38(1):1?4.Petersen, S. E. and Ostendorf, M. (2009).
A machine learning approach to reading level assessment.
ComputerSpeech & Language, 23(1):89?106.Rabin, A., Zakaluk, B., and Samuels, S. (1988).
Determining difficulty levels of text written in languages otherthan english.
Readability: Its past, present & future.
Newark DE: International Reading Association, pages46?76.Rosch, E. (1978).
Principles of categorization.
Fuzzy grammar: a reader, pages 91?108.Schwarm, S. and Ostendorf, M. (2005).
Reading level assessment using support vector machines and statisticallanguage models.
In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,pages 523?530.
Association for Computational Linguistics.Sherman, L. (1893).
Analytics of literature: A manual for the objective study of english poetry and prose.Boston: Ginn.Si, L. and Callan, J.
(2003).
A semisupervised learning method to merge search engine results.
ACMTransactions on Information Systems (TOIS), 21(4):457?491.Sinha, M., Sharma, S., Dasgupta, T., and Basu, A.
(2012).
New readability measures for Bangla and Hindi texts.In Proceedings of COLING 2012: Posters, pages 1141?1150, Mumbai, India.
The COLING 2012 OrganizingCommittee.354
