Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 391?399,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPA Comparison of Model Free versus Model Intensive Approaches toSentence CompressionTadashi NomotoNational Institute of Japanese Literature10-3 Midori TachikawaTokyo 190-0014 Japannomoto@acm.orgAbstractThis work introduces a model free approach tosentence compression, which grew out of ideasfrom Nomoto (2008), and examines how it com-pares to a state-of-art model intensive approachknown as Tree-to-Tree Transducer, or T3 (Cohnand Lapata, 2008).
It is found that a model freeapproach significantly outperforms T3 on the par-ticular data we created from the Internet.
We alsodiscuss what might have caused T3?s poor perfor-mance.1 IntroductionWhile there are a few notable exceptions (Hori andFurui, 2004; Yamagata et al, 2006), it would besafe to say that much of prior research on sen-tence compression has been focusing on what wemight call ?model-intensive approaches,?
wherethe goal is to mimic human created compressionsas faithfully as possible, using probabilistic and/ormachine learning techniques (Knight and Marcu,2002; Riezler et al, 2003; Turner and Charniak,2005; McDonald, 2006; Clarke and Lapata, 2006;Cohn and Lapata, 2007; Cohn and Lapata, 2008;Cohn and Lapata, 2009).
Because of this, thequestion has never been raised as to whether amodel free approach ?
where the goal is not tomodel what humans would produce as compres-sion, but to provide compressions just as useful asthose created by human ?
will offer a viable alter-native to model intensive approaches.
This is thequestion we take on in this paper.11One caveat would be in order.
By model free approach,we mean a particular approach which does not furnish anyparameters or weights that one can train on human createdcompressions.
An approach is said to be model-intensive if itdoes.
So as far as the present paper is concerned, we mightdo equally well with a mention of ?model free?
(?model-intensive?)
replaced with ?unsupervised?
(?supervised?
), or?non-trainable?
(?trainable?
).An immediate benefit of the model-free ap-proach is that we could free ourselves from thedrudgery of collecting gold standard data from hu-mans, which is costly and time-consuming.
An-other benefit is intellectual; it opens up an alterna-tive avenue to addressing the problem of sentencecompression hitherto under-explored.Also breaking from the tradition of previous re-search on sentence compression, we explore theuse of naturally occurring data from the Internetas the gold standard.
The present work builds onand takes further an approach called ?Generic Sen-tence Trimmer?
(GST) (Nomoto, 2008), demon-strating along the way that it could be adapted forEnglish with relative ease.
(GST was originallyintended for Japanese.)
In addition, to get a per-spective on where we stand with this approach, wewill look at how it fares against a state-of-the-artmodel intensive approach known as ?Tree-to-TreeTransducer?
(T3) (Cohn, 2008), on the corpus wecreated.2 ApproachNomoto (2008) discusses a two-level modelfor sentence compression in Japanese termed?Generic Sentence Trimmer?
(GST), which con-sists of a component dedicated to producing gram-matical sentences, and another to reranking sen-tences in a way consistent with gold standard com-pressions.
For the convenience?s sake, we referto the generation component as ?GST/g?
and theranking part as ?GST/r.?
The approach is moti-vated largely by the desire to make compressedsentences linguistically fluent, and what it doesis to retain much of the syntax of the source sen-tence as it is, in compression, which stands in con-trast to Filippova and Strube (2007) and Filippovaand Strube (2008), who while working with de-pendency structure (as we do), took the issue to besomething that can be addressed by selecting andreordering constituents that are deemed relevant.391CDEBAFGHP 1P 2P 3Figure 1: Dependency Structure for ?ABCDEFGH?Getting back to GST, let us consider a sentence,(1) The bailout plan was likely to depend onprivate investors to purchase the toxic assetsthat wiped out the capital of many banks.Among possible compressions GST/g producesfor the sentence are:(2)a.
The bailout plan was likely to depend onprivate investors to purchase the toxic assets.b.
The bailout plan was likely to depend onprivate investors.c.
The bailout plan was likely to depend oninvestors.d.
The bailout plan was likely.Notice that they do not differ much from thesource sentence (1), except that they get some ofthe parts chopped off.
In the following, we talkabout how this could done systematically.3 Compression with TerminatingDependency PathsOne crucial feature of GST is the notion of Ter-minating Dependency Paths or TDPs, which en-ables us to factorize a dependency structure intoa set of independent fragments.
Consider strings = ABCDEFGH with a dependency structure asshown in Figure 1.
We begin by locating terminalnodes, i.e., those which have no incoming edges,depicted as filled circles in Figure 1.
Next we finda dependency (singly linked) path from each ter-minal node to the root (labeled E).
This would giveus three paths p1= A-C-D-E, p2= B-C-D-E, andp3= H-G-F-E (represented by dashed arrows inFigure 1).CDEB#CDEA#GFEH#@ %Figure 2: TDP Trellis and POTs.Given TDPs, we set out to find a set T of allsuffixes for each TDP, including an empty string,which would look like:T (p1) = {?A C D E?, ?C D E?, ?D E?, ?E?, ??
}T (p2) = {?B C D E?, ?C D E?, ?D E?, ?E?, ??
}T (p3) = {?G F E?, ?F E?, ?E?, ??
}Next we combine suffixes, one from each set T ,while removing duplicates if any.
Combining, forinstance, ?A C D E?
?
T (p1), ?C D E?
?
T (p2),and ?G F E?
?
T (p3), would produce {A C DE G F}, which we take to correspond to a stringACDEGF, a short version of s.As a way of doing this systematically, we putTDPs in a trellis format as in Figure 2, each filerepresenting a TDP, and look for a path across thetrellis, which we call ?POT.?
It is easy to see thattraveling across the trellis (while keeping recordof nodes visited), gives you a particular way inwhich to combine TDPs: thus in Figure 2, we havethree POTs, C-B-F, A-C-H, and A-B-F, giving riseto BCDEF, ACDEFGH, and ABCDEF, respectively(where ?&?
denotes a starting node, ?%?
an endingnode, and ?#?
an empty string).
Note that the POTin effect determines what compression we get.Take for instance a POT C-B-F. To get to a com-pression, we first expand C-B-F to get {?C D E?1,?B C D E?2, ?F E?3} (call it E(C-B-F)).
(Note thateach TDP is trimmed to start with a node at a cor-responding position of the POT.)
Next we take aunion of TDPs treating them as if they were sets:thus?E(C-B-F) = {B C D E F} = BCDEF.4 N-Best Search over TDP TrellisAn obvious problem of this approach, however,is that it spawns hundreds of thousands of possi-ble POTs.
We would have as many as 53= 125of them for the eight-character long string in Fig-ure 1.392p l a nwa st od ependl i kel yt heba i lo u tpu r c ha seoni n vest o rst ot het o xica s setswi pe dt hato u tba n k sm a n yca p ita lt heo f01234567d e p t hFigure 3: Dependency StructureWhat we propose to deal with this problem is tocall on a particular ranking scheme to discriminateamong candidates we get.
Our scheme takes theform of Equation 3 and 4.W (x) = idf(x) + exp(?depth(x)) (3)S(p) =?x0,...xn?E(p)W (xi) (4)depth(x) indicates the distance between x and theroot, measured by the number of edges one needto walk across to reach the root from x.
Figure 3shows how the depth is gauged for nodes in a de-pendency structure.
idf(x) represents the log ofthe inverse document frequency of x.
The equa-tions state that the score S of a POT p is given asthe sum of weights of nodes that comprise?E(p).Despite their being simple, equations 3 and 4nicely capture our intuition about the way the trim-ming or compression should work, i.e., that thedeeper we go down the tree, or the further awayyou are from the main clause, the less importantinformation becomes.
Putting aside idf(x) forthe moment, we find in Figure 3, W (assets) >W (capital) > W (banks) > W (many).
Also de-picted in the figure are four TDPs starting withmany, the (preceding toxic), investors, and the(preceding bailout).Finally, we perform a best-first search over thetrellis to pick N highest scoring POTs, using For-Table 1: Drop-me-not rules.
A ?|?
stands for or.?a:b?
refers to an element which has both a and bas attributes.
Relation names such as nsubj, aux,neg, etc., are from de Marneffe et al (2006).R1.
VB ?
nsubj | aux | neg | markR2.
VB ?
WDT | WRBR3.
JJ ?
copR4.
NN ?
det | copR5.
NN ?
poss:WP (=?whose?)R6.
?
?
conj & ccward DP/Backward A* (Nagata, 1994), with theevaluation function given by Equation 4.
Wefound that the beam search, especially when usedwith a small width value, does not work as wellas the best first search as it tends to produce veryshort sentences due to its tendency to focus oninner nodes, which generally carry more weightscompared to those on the edge.
In the experimentsdescribed later, we limited the number of candi-dates to explore at one time to 3,000, to make thesearch computationally feasible.5 ?Drop-me-not?
RulesSimply picking a path over the TDP trellis (POT),however, does not warrant the grammaticality ofthe tree that it generates.
Take for instance, a de-pendency rule, ?likely?plan, was, depend,?
whichforms part of the dependency structure for sen-tence (1).
It gives rise to three TDPs, ?plan, likely?,?was, likely?, and ?depend, likely?.
Since we mayarbitrarily choose either of the two tokens in eachTDP with a complete disregard for a syntagmaticcontext that each token requires, we may end upwith sequences such as ?plan likely,?
?plan waslikely,?
or ?plan likely depend?
(instances of a sametoken are collapsed into one).
This would obvi-ously suggest the need for some device to makethe way we pick a path syntagmatically coherent.The way we respond to the issue is by introduc-ing explicit prohibitions, or ?drop-me-not?
rulesfor POTs to comply with.
Some of the majorrules are shown in Table 1.
A ?drop-me-not?
rule(DMN) applies to a local dependency tree consist-ing of a parent node and its immediate child nodes.The intent of a DMN rule is to prohibit any one ofthe elements specified on the right hand side of thearrow from falling off in the presence of the head393node; they will be gone only if their head node is.R1 says that if you have a dependency treeheaded by VB with nsubj, aux, neg, ormark amongits children, they should stay with VB; R2 pro-hibits against eliminating a WDT or WRB-labeledword in a dependency structure headed by VB; R6disallows either cc or conj to drop without accom-panying the other, for whatever type the head nodeassumes.In Table 2, we find some examples that moti-vate the kinds of DMN rules we have in Table 1.Note that given the DMNs, the generation of ?waslikely depend,?
or ?plan likely depend?
is no longerpossible for the sentence in Figure 3.6 Reranking with CRFsPipelining GST/g with CRFs allows us to tap intoa host of features found in the sentence that couldusefully be exploited toward generating compres-sion, and requires no significant change in the wayit is first conceived in Nomoto (2008), in order tomake it work for English.
It simply involves trans-lating an output by GST/g into the form that al-lows the use of CRFs; this could be done simplyby labeling words included in compression as ?1?and those taken out as ?0,?
which would producea binary representation of an output generated byGST/g.
Given a source sentence x and a set G(S)of candidate compressions generated by GST/g ?represented in binary format ?
we seek to solvethe following,y?= argmaxy?G(S)p(y|x;?).
(5)where y?could be found using regular linear-chain CRFs (Lafferty et al, 2001).
?
stands formodel parameters.
In building CRFs, we made useof features representing lexical forms, syntacticcategories, dependency relations, TFIDF, whethera given word appears in the title of an article, andthe left and right lexical contexts of a word.7 T3Cohn and Lapata (2008; 2009) are a recent attemptto bring a machine learning framework known as?Structured SVM?
to bear on sentence compres-sion and could be considered to be among thecurrent state-of-art approaches.
Roughly speak-ing, their approach or what they call ?Tree-to-TreeTransducer?
(T3) takes sentence compression tobe the problem of classifying the source sentenceTable 3: RSS item and its sourceR Two bombings rocked Iraq today, killing atleast 80 in attacks at a shrine in Karbalaand a police recruiting station in Ramadi.S Baghdad, Jan. 5 ?
Two new suicide bomb-ings rocked Iraq today, killing at least 80in an attack at a shrine in the Shiite city ofKarbala and a police recruiting station inthe Sunni city of Ramadi.with its target sentence, where one seeks to findsome label y, which represents a compression, fora given source sentence x, that satisfies the follow-ing equation,f(x;w) = argmaxy?YF (y, x;w), (6)andF (y, x;w) = ?w,?
(y, x)?, (7)where w, a vector representing model parameters,is determined in such a way that for a target classy and a prediction y?, F (x, y;w) ?
F (x, y?
;w) >?
(y, y?)
?
?, ?y?
?= y; ?
(y, y?)
represents a lossfunction and ?
a slack variable (Tsochantaridis etal., 2005).
?
(y, x) represents a vector of featuresculled from y and x, and ?
?, ??
a dot product.For each of the rules used to derive a source sen-tence, T3 makes a decision on how or whether totransform the rule, with reference to ?
?, ?
?, whichtakes into account such features as the numberof terminals, root category, and lengths of fron-tiers, which eventually leads to a compression viaa chart style dynamic programming.8 CorpusParting ways with previous work on sentence com-pression, which heavily relied on humans to creategold standard references, this work has a particu-lar focus on using data gathered from RSS feeds,which if successful, could open a door to buildinggold standard data in large quantities rapidly andwith little human effort.
The primary objective ofthe present work is to come up with an approachcapable of exploiting naturally occurring data asreferences for compression.
So we are interested394Table 2: Examples.
a ?rb means that b stands in an r-relation to a.rel, nsubj In defying the President, Bill Frist was veering to the political center in a yearduring which he had artfully courted his party?s right wing.couted ?relduringveering?nsubjBill Fristneg Isaac B. Weisfuse says that the idea that a pandemic flu will somehow skip the 21stcentury does not make any sense.make?negnotmark Prime Minister Ariel Sharon of Israel lashed out at protesters as troops finishedclearing all but the last of the 21 Gaza settlements.finished ?markasWDT The announcement offered few details that would convince protestants that theyshould resume sharing power with the I.R.A.
?s political wing.convince ?wdtthatWRB Arbitron, a company best known for its radio ratings, is testing a portable, pager-size device that tracks exposure to media throughout the day, wherever its wearermay go.go ?wrbwherevercop Buildings in a semi-abandoned town just inside Mexico that is a haven for would-be immigrants and smugglers will be leveled.haven ?copisaux, poss:WP Harutoshi Fukui has penned a handful of best sellers whose common themesresonate in a country shedding its pacifism and rearming itself.resonate ?poss:WPwhosepenned ?auxhasTable 4: RSS Corpus from NYTimes.com.areas # of itemsINTERNATIONAL 2052NYREGION 1153NATIONAL 1351OBITUARIES 541OPINION 1031SCIENCE 465SPORTS 1451TECHNOLOGY 978WASHINGTON 1297in finding out how GST compares with T3 fromthis particular perspective.We gathered RSS feeds at NYTimes.com over aperiod of several months, across different sections,including INTERNATIONAL, NATIONAL, NYRE-GION, BUSINESS, and so forth, out of which werandomly chose 2,000 items for training data and116 for testing data.
For each RSS summary, welocated its potential source sentence in the linkedpage, using a similarity metric known as Soft-TFIDF (Cohen et al, 2003).2Table 4 gives a run-down on areas items came from and how many ofthem we collected for each of these areas.For the ease of reference, we refer to a corpus ofthe training and test data combined as ?NYT-RSS,?and let ?NYT-RSS(A)?
denote the training part of2SoftTFIDF is a hybrid of the TFIDF scheme and an edit-distance model known as Jaro-Winkler(Cohen et al, 2003).NYT-RSS, and ?NYT-RSS(B)?
the testing part.9 ExperimentsWe ran the Stanford Parser on NYT-RSS to extractdependency structures for sentences involved, tobe used with GST/g (de Marneffe et al, 2006;Klein and Manning, 2003).
We manually devel-oped 28 DMN rules out of NYT-RSS(A), some ofwhich are presented in Table 1.
An alignment be-tween the source sentence and its correspondinggold standard compression was made by SWA ora standard sequence alignment algorithm by Smithand Waterman (1981).
Importantly, we set upGST/g and T3 in such a way that they rely on thesame set of dependency analyses and alignmentswhen they are put into operation.
We trained T3on NYT-RSS(A) with default settings except for??epsilon?
and ??delete?
options which we turnedoff, as preliminary runs indicated that their use ledto a degraded performance (Cohn, 2008).
We alsoset the loss function as was given in the defaultsettings.
We trained both GST/r, and T3 on NYT-RSS(A).We ran GST/g and GST/g+r, i.e., GST/rpipelined with GST/g, varying the compressionrate from 0.4 to 0.7.
This involved letting GST/grank candidate compressions by S(p) and thenchoosing the first candidate to satisfy a given com-pression rate, whereas GST/g+r was made to out-put the highest ranking candidate as measured byp(y | x; ?
), which meets a particular compressionrate.
It should be emphasized, however, that in T3,varying compression rate is not something the userhas control over; so we accepted whatever output395Table 5: Results on NYT-RSS.
?
*?-marked figuresmean that performance of GST/g is different fromthat of GST/g+r (on the comparable CompR) at5% significance level according to t-test.
The fig-ures indicate average ratings.Model CompR Intelligibility Rep.GST/g+r 0.446 2.836 2.612GST/g 0.469 3.095 2.569GST/g+r 0.540 2.957 2.767GST/g 0.562 3.069 3.026?GST/g+r 0.632 2.931 2.957GST/g 0.651 3.060 3.259?GST/g+r 0.729 3.155 3.345GST/g 0.743 3.328 3.621?T3 0.353 1.750 1.586Gold Std.
0.657 4.776 3.931T3 generated for a given sentence.Table 5 shows how GST/g, GST/g+r, and T3performed on NYT-RSS, along with the gold stan-dard, on a scale of 1 to 5.
Ratings were solicitedfrom 4 native speakers of English.
?CompR?
in-dicates compression rate.
?Intelligibility?
meanshow well the compression reads; ?representative-ness?
how well the compression represents itssource sentence.
Table 6 presents a guideline forrating, describing what each rating should mean,which was also presented to human judges to fa-cilitate evaluation.The results in Table 5 indicate a clear supe-riority of GST/g and GST/g+r over T3, whiledifferences in intelligibility between GST/g andGST/g+r were found not statistically significant.What is intriguing, though, is that GST/g producedperformance statistically different in representa-tiveness from GST/g+r at 5% level as marked bythe asterisk.Shown in Table 8 are examples of compressioncreated by GST/g+r, GST/g and T3, together withgold standard compressions and relevant sourcesentences.
One thing worth noting about the ex-amples is that T3 keeps inserting out-of-the-sourceinformation into compression, which obviouslyhas done more harm than good to compression.Table 6: Guideline for RatingMEANING EXPLANATION SCOREvery bad For intelligbility, it means that thesentence in question is rubbish; nosense can be made out of it.
Asfor representativeness, it means thatthere is no way in which the com-pression could be viewed as repre-senting its source.1poor Either the sentence is broken or failsto make sense for the most part, or itis focusing on points of least signifi-cance in the source.2fair The sentence can be understood,though with some mental effort; itcovers some of the important pointsin the source sentence.3good The sentence allows easy compre-hension; it covers most of importantpoints talked about in the source sen-tence.4excellent The sentence reads as if it were writ-ten by human; it gives a very goodidea of what is being discussed in thesource sentence.5Table 7: Examples from corpora.
?C?
stands forreference compression; ?S?
source sentence.NYT-RSSC Jeanine F. Pirro said that she would abandon herplans to unseat senator Hillary Rodham Clinton andwould instead run for state attorney general .S Jeanine F. Pirro, whose campaign to unseat UnitedStates senator Hillary Rodham Clinton was in up-heaval almost from the start, said yesterday that shewould abandon the race and would instead run forattorney general of New York.CLwrittenC Montserrat, the Caribbean island, is bracing itselffor arrests following a fraud investigation by Scot-land Yard.S Montserrat, the tiny Caribbean island that onceboasted one bank for every 40 inhabitants, is brac-ing itself this Easter for a spate of arrests followinga three-year fraud and corruption investigation byScotland Yard.CLspokenC This gives you the science behind the news, with top-ics explained in detail, from Mad Cow disease tocomets.S This page gives you the science behind the news,with hundreds of topics explained in detail, fromMad Cow disease to comets.396Table 8: form GST/g+r, GST/g, T3, and Gold standard.
(?Source?
represents a source sentence.
)GST/g+r The Corporation plans to announce today at the Game Show that it will begin selling theXbox 360, its new video console , on Nov 22.GST/g The Microsoft Corporation plans to announce at the Tokyo Game Show that it will beginselling Xbox 360, new video console , on Nov.T3 The Microsoft Corporation in New York plans to announce today at the Tokyo GameShow it will begin selling the Xbox 360 , its new video game console, on Nov 22.Gold The Microsoft Corporation plans to announce Thursday at the Tokyo Game Show that itwill begin selling the Xbox 360 , its new video game console, on Nov. 22.Source The Microsoft Corporation plans to announce today at the Tokyo Game Show that it willbegin selling the Xbox 360, its new video game console, on Nov 22.GST/g+r Scientists may have solved the chemical riddle of why the SARS virus causes such pneu-monia and have developed a simple therapy.GST/g Scientists may have solved the chemical riddle of why the virus causes such a pneumoniaand have developed a simple therapy.T3 The scientists may solved the chemical riddle of the black river of why the SARS viruscauses such a deadly pneumonia.Gold Scientists may have solved the riddle of why the SARS virus causes such a deadly pneu-monia.Source Scientists may have solved the chemical riddle of why the SARS virus causes such adeadly pneumonia and have developed a simple therapy that promises to decrease theextraordinarily high death rate from the disease, according to a report in the issue of thejournal nature-medicine that came out this week.GST/g+r A flu shot from GlaxoSmithKline was approved by American regulators and the Corpo-ration vaccine plant, shut year because of, moved closer to being opened work to avoid.GST/g A flu shot was approved by regulators yesterday and the Chiron Corporation vaccineplant, shut , moved closer to being opened as officials work to avoid shortage.T3 A flu shot from gaza was the Chiron Corporation?s Liverpool vaccine plant, shut last yearof a contamination shortage,, but critics suggest he is making it worse.Gold The Chiron Corporation?s liverpool vaccine plant , shut last year because of contamina-tion, moved closer to being opened as officials work to avoid another shortage.Source A flu shot fromGlaxoSmithKline was approved by American regulators yesterday and theChiron Corporation?s Liverpool vaccine plant , shut last year because of contamination,moved closer to being opened as officials work to avoid another shortage.CLwrittenDensity?20 ?10 0 10 20 300.000.020.040.060.08CLspokenDensity?20 ?10 0 10 20 300.000.020.040.060.08NYTDensity?10 0 10 20 30 400.000.050.100.150.20Figure 4: Density distribution of alignment scores.
The x-dimension represents the degree of alignmentbetween gold standard compression and its source sentence.397Table 9: Alignment Scores by SWANYT-RSS CLwritten CLspoken-3.061 (2000) -1.882 (1629) 0.450 (4110)10 Why T3 failsIt is interesting and worthwhile to ask what causedT3, heavily clad in ideas from the recent ma-chine learning literature, to fail on NYT-RSS, asopposed to the ?CLwritten?
and ?CLspoken?
cor-pora on which T3 reportedly prevailed comparedto other approaches (Cohn and Lapata, 2009).The CLwritten corpus comes from written sourcesin the British National Corpus and the AmericanNews Text corpus; the CLspoken corpus comesfrom transcribed broadcast news stories (cf.
Ta-ble 7).We argue that there are some important dif-ferences between the NYT-RSS corpus and theCLwritten/CLspoken corpora that may have led toT3?s poor record with the former corpus.The CLwritten and CLspoken corpora were cre-ated with a specific purpose in mind: namely toassess the compression-by-deletion approach.
Sotheir authors had a very good reason to limit goldstandard compressions to those that can be arrivedat only through deletion; annotators were care-fully instructed to create compression by delet-ing words from the source sentence in a way thatpreserves the gist of the original sentence.
Bycontrast, NYT-RSS consists of naturally occurringcompressions sampled from live feeds on the In-ternet, where relations between compression andits source sentence are often not as straightfor-ward.
For instance, to arrive at a compression inNYT-RSS in Table 7 involves replacing race withher plans to unseat senator Hillary Rodam Clin-ton, which is obviously beyond what is possiblewith the deletion based approach.In CLwritten and CLspoken, on the other hand,compressions are constructed out of parts that ap-pear in verbatim in the original sentence, as Ta-ble 7 shows: thus one may get to the compres-sions by simply crossing off words from the origi-nal sentence.To see whether there is any significant differ-ence among NYT-RSS, CLwritten and CLspoken,we examined how well gold standard compres-sions are aligned with source sentences on eachof the corpora, using SWA.
Table 9 shows whatwe found.
Parenthetical numbers represent howmany pairs of compression and source are found ineach corpus.
A larger score means a tighter align-ment between gold standard compression and itssource sentence: we find in Table 9 that CLspokenhas a source sentence more closely aligned withits compression than CLwritten, whose alignmentsare more closely tied than NYT-RSS?s.Figure 4 (found in the previous page) showshow SWA alignment scores are distributed overeach of the corpora.
CLwritten and CLspokenhave peaks at around 0, with an almost entiremass of scores concentrating in an area close to orabove 0.
This means that for the most of the casesin either CLwritten or CLspoken, compression isvery similar in form to its source.
In contrast,NYT-RSS has a heavy concentration of scores ina stretch between -5 and -10, indicating that forthe most of time, the overlap between compres-sion and its source is rather modest compared toCLwritten and CLspoken.So why does T3 fails on NYT-RSS?
BecauseNYT-RSS contains lots of alignments that are onlyweakly related: in order for T3 to perform well,the training corpus should be made as free of spu-rious data as possible, so that most of the align-ments are rated over and around 0 by SWA.
Ourconcern is that such data may not happen naturally,as the density distribution of NYT-RSS shows,where the majority of alignments are found far be-low 0, which could raise some questions about therobustness of T3.11 ConclusionsThis paper introduced the model free approach,GST/g, which works by creating compressionsonly in reference to dependency structure, andlooked at how it compares with a model intensiveapproach T3 on the data gathered from the Inter-net.
It was found that the latter approach appearsto crucially rely on the way the corpus is con-structed in order for it to work, which may mean ahuge compromise.Interestingly enough, GST/g came out a winneron the particular corpus we used, even outperform-398ing its CRFs harnessed version, GST/g+r in repre-sentativeness.
This suggests that we might gainmore by improving fluency of GST/g than by fo-cusing on its representativeness, which in any casecame close to that of human at 70% compressionlevel.
The future work should also look at how thepresent approach fares on CLwritten and CLspo-ken, for which T3 was found to be effective.AcknowledgementsThe author likes to express gratitude to the review-ers of EMNLP for the time and trouble they tookto review the paper.
Their efforts are greatly ap-preciated.ReferencesJames Clarke and Mirella Lapata.
2006.
Modelsfor sentence compression: A comparison across do-mains, training requirements and evaluation mea-sures.
In Proceedings of the 21st COLING and 44thACL, pages 377?384, Sydney, Australia, July.William W. Cohen, Pradeep Ravikumar, andStephen E. Fienberg.
2003.
A comparison ofstring distance metrics for name-matching tasks.
InSubbarao Kambhampati and Craig A. Knoblock,editors, IIWeb, pages 73?78.Trevor Cohn and Mirella Lapata.
2007.
Large mar-gin synchronous generation and its application tosentence compression.
In Proceedings of the 2007EMNLP-CoNLL, pages 73?82, Prague, Czech Re-public, June.Trevor Cohn and Mirella Lapata.
2008.
Sentencecompression beyond word deletion.
In Proceedingsof the 22nd COLING, pages 137?144, Manchester,UK, August.Trevor Cohn and Mirella Lapata.
2009.
Sen-tence compression as tree transduction.
Draft athttp://homepages.inf.ed.ac.uk/tcohn/t3/.Trevor Cohn.
2008.
T3: Tree Transducer Toolkit.http://homepages.inf.ed.ac.uk/tcohn/t3/.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InLREC 2006.Katja Filippova and Michael Strube.
2007.
Generatingconstituent order in german clauses.
In Proceedingsof the 45th ACL, pages 320?327, Prague, Czech Re-public, June.Katja Filippova and Michael Strube.
2008.
Sentencefusion via dependency graph compression.
In Pro-ceedings of the 2008 EMNLP, pages 177?185, Hon-olulu, Hawaii, October.C.
Hori and Sadaoki Furui.
2004.
Speech summa-rization: an approach through word extraction anda method for evaluation.
IEICE Transactions on In-formation and Systems, E87-D(1):15?25.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st ACL, pages 423?430, Sapporo, Japan, July.Kevin Knight and Daniel Marcu.
2002.
Summariza-tion beyond sentence extraction: A probabilistic ap-proach to sentence compression.
Artificial Intelli-gence, 139:91?107.John Lafferty, Andrew MacCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of the 18th ICML-2001.Ryan McDonald.
2006.
Discriminative sentence com-pression with soft syntactic evidence.
In Proceed-ings of the 11th EACL, pages 297?304.Masaaki Nagata.
1994.
A stochastic japanese morpho-logical analyzer using a forward-dp backward-a* n-best search algorithm.
In Proceedings of COLING-94.Tadashi Nomoto.
2008.
A generic sentence trimmerwith CRFs.
In Proceedings of ACL-08: HLT, pages299?307, Columbus, Ohio, June.Stefan Riezler, Tracy H. King, Richard Crouch, andAnnie Zaenen.
2003.
Statistical sentence conden-sation using ambiguity packing and stochastic dis-ambiguation methods for lexical functional gram-mar.
In Proceedings of HLT-NAACL 2003, pages118?125, Edmonton.T.
F. Smith and M. S. Waterman.
1981.
Identifica-tion of common molecular subsequence.
Journal ofMolecular Biology, 147:195?197.Ioannis Tsochantaridis, Thomas Hofmann, ThorstenJoachims, and Yasemin Altun.
2005.
Support vec-tor machine learning for interdependent and struc-tured output spaces.
Journal of Machine LearningResearch, 6:1453?1484.Jenie Turner and Eugen Charniak.
2005.
Supervisedand unsupervised learning for sentence compres-sion.
In Proceedings of the 43rd ACL, pages 290?297, Ann Arbor, June.Kiwamu Yamagata, Satoshi Fukutomi, Kazuyuki Tak-agi, and Kzauhiko Ozeki.
2006.
Sentence compres-sion using statistical information about dependencypath length.
In Proceedings of TSD 2006 (LectureNotes in Computer Science, Vol.
4188/2006), pages127?134, Brno, Czech Republic.399
