Using word similarity lists for resolving indirect anaphoraCaroline Gasperin and Renata VieiraPIPCA - UnisinosS?o Leopoldo, Brazil{caroline,renata}@exatas.unisinos.brAbstractIn this work we test the use of word similarity listsfor anaphora resolution in Portuguese corpora.
Weapplied an automatic lexical acquisition techniqueover parsed texts to identify semantically similarwords.
After that, we made use of this lexicalknowledge to resolve coreferent definite descrip-tions where the head-noun of the anaphor is differ-ent from the head-noun of its antecedent, which wecall indirect anaphora.1 IntroductionIn this work we investigate the use of word similar-ity list for treating coreference, especially the caseswhere the coreferent expressions have semanticallyrelated head nouns (instead of same head nouns),which we call indirect anaphora.We applied a lexical acquisition technique(Gasperin, 2001) over Portuguese parsed corpora toautomatically identify semantically similar words.After that, we made use of this lexical knowledgeto resolve the coreferent definite descriptions wherethe head-noun of the anaphor is different from thehead-noun of its antecedent.Previous work on anaphoric resolution of Englishtexts has used acquired lexical knowledge in differ-ent ways, examples are (Poesio et al, 2002; Schulteim Walde, 1997; Bunescu, 2003).This paper is organised as follows.
The next sec-tion explain our notion of indirect anaphora.
Section3 details the tools and techniques used to the con-struction of our lexical resource.
Section 4 presentsour heuristic for solving the indirect anaphors on thebasis of such resource.
Section 5 details the corpuswe are using for evaluating the proposed heuristics.Section 6 reports the implementation of the heuris-tic and in Section 7 we present our experimentsover Portuguese annotated corpora.
In Section 8 wediscuss our results and compare them to previousworks.
Finally, Section 9 presents our concludingcomments.2 Indirect anaphoraCoreference has been defined by (van Deemter andKibble, 2000) as the relation holding between lin-guistic expressions that refer to the same extralin-guistic entity.
A slightly different discourse rela-tion is anaphora.
In an anaphoric relation the in-terpretation of an expression is dependent on previ-ous expressions within the same discourse (in vari-ous ways).
Therefore, an anaphoric relation may becoreferent or not.
An expression may be anaphoricin the strict sense that its interpretation is only possi-ble on the basis of the antecedent, as it is in generalthe case of pronouns in written discourse.
On theother hand, it might be just coreferent, in the sensethat the entity has been mentioned before in the text.In this work, we focus on the expressions that areanaphoric and coreferent, and restricting even more,just the indirect cases, when the antecedent head-noun and the anaphor head-noun are not same butsemantically related.To clarify what we mean by indirect anaphora,we detail the classification we adopted in our pre-vious work (Vieira et al, 2002; Vieira et al, 2003).Our classes of analyses were based on the analy-ses of English texts presented in (Poesio and Vieira,1998), with the difference that we divided the Bridg-ing class of their analyses into two different classes,separating coreferent (Indirect Anaphora) and non-coreferent (Other Anaphora) cases.
Each definitedescription (d) is classified into one of the follow-ing four classes:1.
Direct anaphora: d corefers with a previousexpression a; d and a have the same nominalhead:a.
A Comiss?o tem conhecimento do livro... (theCommission knows the book)d. a Comiss?o constata ainda que o livro n?o sedebru?a sobre a actividade das v?rias... (the Com-mission remarks that the book ignores the activityof various)2.
Indirect anaphora: d corefers with a previousexpression a; d and a have different nominalheads:a. a circula?
?o dos cidad?es que dirigem-se... (theflow of the citizens heading to...)d. do controle das pessoas nas fronteiras (the con-trol of the people in the borders)3.
Other Anaphora: d does not corefer with a pre-vious expression a, but depends for its interpre-tation on a:a. o recrutamento de pessoal cient?fico e t?cnico...(the recruitment of scientific and technical em-ployees)d. as condi?
?es de acesso ?
carreira cient?fica (theconditions of employment for scientific jobs)4.
Discourse New: the interpretation of d does notdepend on any previous expression:d. o livro n?o se debru?a sobre a actividade dasv?rias organiza?
?es internacionais... (the book ig-nores the activity of various international organ-isation...)In (Schulte im Walde, 1997) acquired lexicalknowledge is used for solving bridging descrip-tions, a broader class of anaphoric relations thatincludes our class, indirect anaphora.
(Poesio etal., 2002) presents alternative techniques, basedon syntactic patterns, focusing on meronymy rela-tions.
Finally, (Bunescu, 2003) deals with anotherclass of anaphoric descriptions, which is also in-cluded in the bridging class, called as associativeanaphora, following (Hawkins, 1978), where asso-ciative anaphora is an anaphoric relation betweennon-coreferent entities.3 Lexical resourceOur lexical resource consists on lists of semanticallyrelated words.
These lists are constructed automati-cally by a syntax-based knowledge-poor technique.The technique used is described in (Gasperin et al,2001; Gasperin, 2001), and it is an extension of thetechnique presented in (Grefenstette, 1994).Briefly, this technique consists on extracting spe-cific syntactic contexts for every noun in the parsedwhole corpus and then applying a similarity mea-sure (the weighted Jaccard measure) to compare thenouns by the contexts they have in common (morecontexts they share, more similar they are).
As syn-tactic context, we understand any word that estab-lishes a syntactic relation with a given noun in thecorpus.
An example of one kind of syntactic contextconsidered is subject/verb, meaning that two nounsthat occur as subject of the same verb share thiscontext.
Other examples of syntactic contexts areverb/object, modifier/noun, etc.
To each context itis assigned a global and a local weight: the first re-lated to the context frequency in the corpus, and thesecond related to its frequency as a context of thenoun in focus.
As output, we have a list of the mostsimilar nouns to each noun in the corpus, orderedby the similarity value.
We present the similaritylist for the noun acusa?
?o (accusation) in Table 1as an example.Table 1: Similarity list for the noun acusa?
?oden?ncia (denunciation)esc?ndalo (scandal)crime (crime)pedido (demand)declara?
?o (declaration)proposta (proposal)not?cia (news)acusa?
?o carta (letter)(accusation) lista (list)cargo (post)ataque (attack)arma (gun)caso (case)impress?o (impression)reclama?
?o (complain)The similarity lists can contain any kind of se-mantic relation (e.g.
synonymy, hyponymy, etc.
)between the words, but they are not classified.
Ingeneral, the similarity lists for the less frequentwords in the corpus contain some non-semanticallyrelated words (noise), since the relations were basedon few syntactic contexts they shared along the cor-pus.The main advantage of this technique is the pos-sibility of having a corpus-tunned lexical resourcebuilt completely automatically.
This resource re-flects closely the semantic relations present in thecorpus used to create the lists.
So, we believe thesimilarity lists are more suitable for being used aslexical knowledge for resolving the anaphoras thana generic lexical base (e.g.
Wordnet), since it focuson the semantic relations between the terms that ap-pear in the corpus, without considering extra mean-ings that some words could have.
New lists could begenerated from each corpus that one aims to resolvethe anaphoras.To generate the similarity lists for Portuguese weutilised a 1,400,000-words corpus from the Brazil-ian newspaper ?Folha de S?o Paulo?, containingnews about different subjects (sports, economics,computers, culture, etc.).
This corpus includes theset of texts that was hand-annotated with corefer-ence information in previous work (Vieira et al,2002; Salmon-Alt and Vieira, 2002).
The corpuswas parsed by the Portuguese parser PALAVRAS(Bick, 2000), provided by VISL project1.We created two different sets of similarity lists:one considering just nouns and the other consider-ing nouns and proper names.
So, the first set of listsincludes one list for each noun in the corpus andeach list is composed by other common nouns.
Thesecond set of lists has one list for each noun andproper name in the corpus, and each list is com-posed by other nouns and proper names.
The firstset contains 8019 lists and the second 12275, corre-sponding to the different nouns (and proper names)appearing in the corpus.
Each similarity list con-tains the 15 words that are more similar to the wordin focus, according to the calculated similarity val-ues.Having lexical information about the propernames in the corpus is important, since we havemany coreference cases whose anaphor or an-tecedent is a proper name.
But when generating thesimilarity lists, proper names bring noise (in gen-eral they are less frequent then common nouns) andthe lists became more heterogeneous (includes morenon semantically related words).4 Using similar words lists to solveindirect anaphoraFrom the manual annotation and classification of680 definite descriptions we selected those casesclassified as indirect anaphora (95).
For each ofthem there is a list of candidate antecedents.
Thislist is formed by all NPs that occur in the text.
Weconsider as candidates all the NPs that occur in thetext before the anaphor being mentioned.Our heuristic for solving indirect anaphoras usinglists of similar words is the following.
Consider:?
Hana is the head-noun of the anaphor?
Hcani is the head-noun of the antecedent can-didate i?
Lana is the anaphor?s list of similar nouns?
Lcani is the list of similar nouns for the candi-date i?
So,Hcani is considered the antecedent ofHanaif(1)Hcani ?
Lanaor(2)Hana ?
Lcani1See http://visl.hum.sdu.dk/visl/pt/or(3)Lana 3 Hj ?
LcaniWe call (1) ?right direction?, (2) ?opposite direc-tion?, and (3) ?indirect way?.We consider (1) > (2) > (3) when regarding thereliability of the semantic relatedness betweenHanaand Hcani .If the application of the heuristic resulted in morethan one possible antecedent, we adopted a weight-ing scheme to choose only one among them.
Thecandidate with the lowest weight wins.
For rankingthe possible antecedents, we considered two param-eters:?
reliability: how the possible antecedent was se-lect, according to (1), (2) or (3).
A penalis-ing value is added to its weight: 0, 40, 200,respectively.
The higher penalty for the ?indi-rect way?
is because we expected it could causemany false positives;?
recency: we consider the distance in words be-tween the anaphor and the possible antecedent.The penalty values for the reliability parameterwere chosen in such a way they could be in the samemagnitude as the recency parameter values, that aremeasured in words.
For example, if candidate A is250 words far from the anaphor and was selectedby (1) (getting weight=250) and a candidate B is 10words far from the anaphor and was selected by (3)(getting weight=210), candidate B will be selectedas the correct antecedent.5 Our evaluation corpusAs result of previous work (Vieira et al, 2002;Vieira et al, 2003), we have a Portuguese corpusmanually annotated with coreference information.This corpus is considered our gold-standard to eval-uate the performance of the heuristic presented inthe previous section.
The study aimed to verify ifwe could get a similar distribution of types of defi-nite descriptions for Portuguese and English, whichwould serve as an indication that the same heuristicstested for English (Vieira et al, 2000) could applyfor Portuguese.
The main annotation task in thisexperiment was identifying antecedents and classi-fying each definite description according to the fourclasses presented in section 2.For the annotation task, we adopted the MMAXannotation tool (M?ller and Strube, 2001), that re-quires all data to be encoded in XML format.
Thecorpus is encoded by <word> elements with sequen-tial identifiers, and the output - the anaphors andits antecedents - are enconded as <markable> ele-ments, with the anaphor markable pointing to theantecedent markable by a ?pointer?
attribute.The annotation process was split in 4 steps: se-lecting coreferent terms; identifying the antecedentof coreferent terms; classifying coreferent terms (di-rect or indirect); classifying non-coreferent terms(discourse new or other anaphora).
About half ofthe anaphoras were classified as discourse new de-scriptions, which account for about 70% of non-coreferent cases.
Among the coreferent cases thenumber of direct coreference is twice the number ofindirect coreference.
This confirms previous workdone for English.For the present work, we took then the 95 casesclassified as indirect coreference to serve as ourevaluation set.
In 14 of this cases, the relation be-tween anaphor and antecedent is synonymy, in 43of the cases the relation is hyponymy, and in 38, theantecedent or the anaphor are a proper name.6 Implementing heuristics for indirectanaphora in ARTOur heuristics were implemented as an XSLstylesheet on the basis of the Anaphora ResolutionTool (ART) (Vieira et al, 2003).The tool integrates a set of heuristics correspond-ing to one or more stylesheets to resolve differentsorts of anaphora.
The heuristics may be applied ina sequence defined by the user.
As resolving directanaphoric descriptions (the ones where anaphor andantecedent have the same head noun) is a much sim-pler problem with high performance rates as shownin previous results (Vieira et al, 2000; Bean andRiloff, 1999), these heuristics should be appliedfirst in a system that resolves definite descriptions.In this work, however, we decided to consider forthe experiments just the anaphoras that were pre-viously annotated as indirect and check if the pro-posed heuristic is able to find the correct antecedent.ART allows the user to define the set of anaphorsto be resolved, in our case they are selected frompreviously classified definite descriptions.
Thestylesheet for indirect anaphora takes as input thislist of indirect anaphors, a list of the candidates andthe similarity lists.
We consider all NPs in the textas candidates, and for each anaphor we consider justthe candidates that appear before it in the text (weare ignoring cataphora at moment).All the input and output data is in XML for-mat, based on the data format used by MMAX.Our stylesheet for solving indirect anaphora takesthe <markable> elements with empty ?pointer?
at-tribute (coming unsolved from passing by the previ-Table 2: Results considering just nounsDescription NumbersTotal indirect anaphors 57CorrectlyresolvedanaphorsRight direction 8Opposite direction 5Indirect way 6TOTAL 19 (33.3%)Unsolved anaphors 21ously applied stylesheets/heuristics) and create andintermediate file with <anaphor> elements to be re-solved.
The resolved <anaphor>s are again encodedas <markable>s, with the ?pointer?
filled.
A de-tailed description of our data encoding is presentedin (Gasperin et al, 2003).7 ExperimentsWe run two experiments: one using the similaritylists with proper names and another with the listscontaining just common nouns.With these experiments we verify the values forprecision, recall and false positives on the taskof choosing an semantically similar antecedent foreach indirect anaphor.
Our annotated corpus has 95indirect anaphors with nominal antecedents, where57 of them do not include proper names (as anaphoror as antecedent).
We use a non annotated version ofthis corpus for the experiments.
It contains around6000 words, from 24 news texts of 6 different news-paper sections.Firstly, we reduced both sets of similarity lists tocontain just the list for the words present in this por-tion of the corpus (660 lists without proper namesand 742 including proper names).7.1 Experiment 1Considering the 57 indirect anaphoras to be solved(the ones that do not include any proper name), wecould solve 19 of them.
It leads to a precision of52.7% and a a recall of 33.3%.
Table 2 shows theresult of our study considering the set of commonnoun lists.Most of the cases could be resolved by ?right di-rection?, that represents the more intuitive way.
21of the cases didn?t get any antecedent.
We got 17false positives, with different causes:1. the right antecedent was not in the lists, there-fore it could not be found but other wrong an-tecedents were retrieved.
For example, in meuamigo Ives Gandra da Silva Martins escreveupara esse jornal ... o conselheiro Ives (myfriend Ives_Gandra_da_Silva_Martins wroteto this newspaper ... the councillor Ives), twomore candidates head-nouns are similar wordsto ?conselheiro?
(councillor): ?arquiteto?
(ar-chitect) and ?consultor?
(consultant), but not?amigo?
(friend);2. the right antecedent was in the lists but anotherwrong antecedent was given higher weights,because of proximity to the anaphora, as in theexample a rodovia Comandante Jo?o Ribeirode Barros ... pr?ximo a ponte ... ao ten-tar atravessar a estrada (the highway Coman-dante Joao Ribeiro de Barros ... near to thebridge ... while trying to cross the road).
Here,the correct antecedent to ?a estrada?
(the road)is ?rodovia?
(the highway) and it is present in?estrada?
?s similarity list (right direction), butalso is ?ponte?
(the bridge) and it is closer tothe anaphor in the text.As expected, most of the false positives (11 cases)were ?resolved?
by ?indirect way?.Considering all similar words found among thecandidates, not just the one with highest weight, wecould find the correct antecedent in 24 cases (42%).The average number of similar words among thecandidates was 2.8, taking into account again thepositive and false positive cases.
These numbersreport how much the similarity lists encode the se-mantic relations present in the corpus.
64% of thesynonymy cases and 28% of the hyponymy casescould be resolved.
35% of the hyponymy cases re-sulted in false positives, the same happened withjust 14% of the synonymy cases.7.2 Experiment 2We replicated the previous experiment now usingthe similarity lists that include proper names.
Table3 shows the results considering the set of lists fornouns and proper names.
Considering the 95 indi-rect anaphoras to be solved, we could solve 21 ofthem.
It leads to a precision of 36.8% and a a recallof 22.1%.
There was no antecedent found for 38anaphors, and 36 anaphors got wrong antecedents(half of them by ?inderect way?).
We observed thesame causes for false positives as the two presentedfor experiment 1.Considering all cases resolved (correct and falseones), we could find the correct antecedent amongthe similar words of the anaphor in 31 cases(32.6%).
The average number of similar wordsamong the candidates was 2.75.
The numbers forsynonymy and hyponymy cases were the same asin experiment 1 - 64% and 28% respectively.
Thenumbers for proper names were 50% of false posi-tives and 50% of unresolved cases.
It means noneTable 3: Results considering nouns and propernamesDescription NumbersTotal indirect anaphors 95CorrectlyresolvedanaphorsRight direction 13Opposite direction 3Indirect way 5TOTAL 21 (22.1%)Unsolved anaphors 38of the cases that include proper names could beresolved, but do not means they hadn?t any influ-ence in other nouns similarity lists.
In 26% of thefalse positive cases, the correct antecedent (a propername) was in the anaphor similarity list (but was notselected due to the weighting strategy).The experiment with the similarity lists that in-clude proper names was able to solve more cases,but experiment 1 got better precision and recall val-ues.8 Related workAn evaluation of the use of WordNet for treatingbridging descriptions is presented in (Poesio et al,1997).
This evaluation considers 204 bridging de-scriptions, distributed as follows, where NPj is theanaphora and NPi is antecedent.?
synonymy relation between NPj and NPi: 12cases;?
hypernymy relation between NPj and NPi: 14cases;?
meronymy between NPj and NPi: 12;?
NPj related with NPi being a proper name: 49;?
NPj sharing a same noun in NPi other thanhead (compound nouns): 25;?
NPj with antecedent being an event 40;?
NPj with antecedents being an implicit dis-course topic: 15;?
other types of inferences holding between NPjand antecedent: 37.Due to the nature of the relations, only some ofthem were expected to be found in WordNet.
ForSynonymy, hypernymy and meronymy, 39% of the38 cases could be solved on the basis of WordNet.From this related work we can see the large varietyof cases one can found in a class such as bridging.
Inour work we concentrated on coreference relations,these can be related to synonymy, hypernymy, andproper name sub-classes evaluated in (Poesio et al,1997).The technique presented in (Schulte im Walde,1997) based on lexical acquisition from the BritishNational Corpus was evaluated against the samecases in (Poesio et al, 1997).
For synonymy, hy-pernymy and meronymy, it was reported that 22%of the 38 cases were resolved.
In (Poesio et al,2002) the inclusion of syntactic patterns improvedthe resolution of meronymy in particular, result-ing in 66% of the meronymy cases being resolved.Bunescu (Bunescu, 2003) reports for his method onresolving associative anaphora (anaphoric relationbetween non-coreferent entities) a precision of 53%when his recall is 22.7%.9 Concluding remarksWe tested the use of word similarity lists on re-solving indirect anaphoras on Portuguese newspa-per texts.
We presented our heuristic for searchingword similarity lists to be able to find the relationbetween an anaphor and its antecedent.
We con-sidered similarity lists containing proper names andlists containing just common nouns.
Our heuris-tic was able to resolve 33.3% of the cases, withprecision of 52.7% when considering just commonnouns, and we got 22.1%recall with precision of36.8% when including proper names.
Even thoughconsidering proper names give us the possibility oftreating more anaphora cases, we got lower preci-sion than using the lists with only nouns, since suchlists are more homogeneous.
These results are com-parable to previous work dealing with such complexanaphora.As future work, we intend to integrate our heuris-tic for indirect anaphora with other heuristics foranaphora resolution into ART and investigate thebest combination of application of these.
Concern-ing refining the proposed heuristic, we intend torun more experiments aiming to tune the penalis-ing weights when choosing an antecedent amongthe candidates already selected by the search on thesimilarity lists.AcknowledgementsWe would like to thank CNPq (Brazil) / INRIA(France) for their financial support, and SusanneSalmon-Alt, for her collaboration in this work.ReferencesD.
Bean and E. Riloff.
1999.
Corpus-based identi-fication of non-anaphoric noun phrases.
In Pro-ceedings of the 37th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL-99).Eckhard Bick.
2000.
The Parsing SystemPALAVRAS: Automatic Grammatical Analysis ofPortuguese in a Constraint Grammar Frame-work.
Ph.D. thesis, ?rhus University, ?rhus.Razvan Bunescu.
2003.
Associative anaphora res-olution: A web-based approach.
In Proceedingsof EACL 2003 - orkshop on The ComputationalTreatment of Anaphora, Budapest.Caroline Gasperin, Pablo Gamallo, AlexandreAgustini, Gabriel Lopes, and Vera Lima.
2001.Using syntactic contexts for measuring word sim-ilarity.
In Proceedings of the Workshop on Se-mantic Knowledge Acquisition and Categorisa-tion, Helsink, Finland.Caroline Gasperin, Renata Vieira, Rodrigo Goulart,and Paulo Quaresma.
2003.
Extracting xmlsyntactic chunks from portuguese corpora.
InTraitement automatique des langues minoritaires- TALN 2003, Btaz-sur-mer, France.Caroline Varaschin Gasperin.
2001.
Extra?
?o au-tom?tica de rela?
?es sem?nticas a partir de re-la?
?es sint?ticas.
Master?s thesis, PUCRS, PortoAlegre.Gregory Grefenstette.
1994.
Explorations in Auto-matic Thesaurus Discovery.
Kluwer AcademicPublishers, USA.John A. Hawkins.
1978.
Definiteness and Indef-initeness.
Humanities Press, Atlantic Highland,NJ.Christoph M?ller and Michael Strube.
2001.MMAX: A tool for the annotation of multi-modalcorpora.
In Proceedings of the IJCAI 2001, pages45?50, Seattle.Massimo Poesio and Renata Vieira.
1998.
Acorpus-based investigation of definite descriptionuse.
Computational Linguistics, 24(2):183?216.Massimo Poesio, Renata Vieira, and Simone Teufel.1997.
Resolving bridging descriptions in unre-stricted texts.
In Proceedings of the Practical,Robust, Anaphora Resolution for UnrestrictedTexts, Workshop on Operational Factors, Madrid.Massimo Poesio, Tomonori Ishikawa, SabineSchulte Im Walde, and Renata Vieira.
2002.
Ac-quiring lexical knowledge for anaphora resolu-tion.
In Proceedings of LREC 2002, Las PalmasDe Gran Canaria.Susanne Salmon-Alt and Renata Vieira.
2002.Nominal expressions in multilingual corpora:Definites and demonstratives.
In Proceedings ofthe LREC 2002, Las Palmas de Gran Canaria.Sabine Schulte im Walde.
1997.
ResolvingBridging Descriptions in High-DimensionalSpace.
Master?s thesis, Institut f?r MaschinelleSprachverarbeitung, University of Stuttgart, andCenter for Cognitive Science, University ofEdinburgh.K.
van Deemter and R. Kibble.
2000.
On corefer-ring: Coreference in muc and related annotationschemes.
Computational Linguistics, 26(4).Renata Vieira, Susanne Salmon-Alt, and EmmanuelSchang.
2002.
Multilingual corpora annotationfor processing definite descriptions.
In Proceed-ings of the PorTAL 2002, Faro.Renata Vieira, Caroline Gasperin, and RodrigoGoulart.
2003.
From manual to automatic anno-tation of coreference.
In Proceedings of the In-ternational Symposium on Reference Resolutionand Its Applications to Question Answering andSummarization, Venice.Vieira et al 2000.
Extra?
?o de sintagmas nominaispara o processamento de co-refer?ncia.
In Anaisdo V Encontro para o processamento computa-cional da L?ngua Portuguesa escrita e falada -PROPOR, Atibaia.
