The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 63?72,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsMeasuring the Use of Factual Information in Test-Taker EssaysBeata Beigman KlebanovEducational Testing Service660 Rosedale RoadPrinceton, NJ 08541, USAbbeigmanklebanov@ets.orgDerrick HigginsEducational Testing Service660 Rosedale RoadPrinceton, NJ 08541, USAdhiggins@ets.orgAbstractWe describe a study aimed at measuring theuse of factual information in test-taker essaysand assessing its effectiveness for predictingessay scores.
We found medium correlationswith the proposed measures, that remainedsignificant after the effect of essay length wasfactored out.
The correlations did not dif-fer substantionally between a simple, rela-tively robust measure vs a more sophisticatedmeasure with better construct validity.
Impli-cations for development of automated essayscoring systems are discussed.1 IntroductionAutomated scoring of essays deals with various as-pects of writing, such as grammar, usage, mecha-nics, as well as organization and content (Attaliand Burstein, 2006).
For assessment of content,the focus is traditionally on topical appropriatenessof the vocabulary (Attali and Burstein, 2006; Lan-dauer et al, 2003; Louis and Higgins, 2010; Chenet al, 2010; De and Kopparapu, 2011; Higgins etal., 2006; Ishioka and Kameda, 2006; Kakkonen etal., 2005; Kakkonen and Sutinen, 2004; Lemaireand Dessus, 2001; Rose?
et al, 2003; Larkey, 1998),although recently other aspects, such as detectionof sentiment or figurative language, have started toattract attention (Beigman Klebanov et al, 2012;Chang et al, 2006).The nature of factual information used in an es-say has not so far been addressed, to our knowledge;yet a misleading premise, insufficient factual basis,or an example that flies in the face of the reader?sknowledge clearly detract from an essay?s quality.This paper presents a study on assessing the useof factual knowledge in argumentative essays on ge-neral topics written for a graduate school entranceexam.
We propose a definition of fact, and an opera-tionalization thereof.
We find that the proposed mea-sure has positive medium-strength correlation withessay grade, which remains significant after the im-pact of essay length is factored out.
In order toquantify which aspects of the measure drive the ob-served correlations, we gradually relax the measure-ment procedure, down to a simple and robust proxymeasure.
Surprisingly, we find that the correlationsdo not change throughout the relaxation process.
Wediscuss the findings in the context of validity vs re-liability of measurement, and point out implicationsfor automated essay scoring.2 What is a Fact?To help articulate the notion of fact, we use the fol-lowing definition from a seminal text in argumenta-tion theory: ?...
in the context of argumentation, thenotion of fact is uniquely characterized by the ideathat is held of agreements of a certain type relatingto certain data, those which refer to an objective rea-lity, and, in Poincare?s words, designate essentially?what is common to several thinking beings, andcould be common to all?
(Perelman and Olbrechts-Tyteca, 1969, 67).
Factuality is thus a matter of se-lecting certain kinds of data and securing a certaintype of agreement over those data.Of the different statements that refer to objec-tive reality, the term facts is used to ?designate ob-63jects of precise, limited agreement?
(Perelman andOlbrechts-Tyteca, 1969, 69).
These are contrastedwith presumptions ?
statements connected to whatis normal and likely (ibid.).
We suggest that the dis-tinctions in the scope of the required agreement canbe related to the referential device used in a state-ment: If the reference is more rigid (Kripke, 1980),that is, less prone to change in time and to inde-terminacy of the boundaries, the scope of the ne-cessary agreement is likely to be more precise andlimited.
With proper names prototypically being themost rigid designators, we will focus our efforts onstatements about named entities.1Perhaps the simplest model of the universal au-dience is an encyclopedia ?
a body of knowledgethat is verified by experts, and is, therefore, ?com-mon to several thinking beings, and could be com-mon to all?
by virtue of the authority of the expertsand the wide availability of the resource.
However,many facts known to various groups of people thatcould be known to all are absent from any encyclo-pedia.
The knowledge contained in the WWW atlarge, reaching not only statements explicitly con-tributed to an encyclopedia but also those made bypeople on their blogs ?
is perhaps as close as it getsto a working model of the universal audience.Recent developments in Open Information Ex-traction make it possible to tap into this vast know-ledge resource.
Indeed, fact-checking is one of theapplications the developers of OpenIE have in mindfor their emergent technology (Etzioni et al, 2008).3 Open Information ExtractionTraditionally, the goal of an information extrac-tion system is automated population of structureddatabases of events or concepts of interest and theirproperties by analyzing large corpora of text (Chin-chor et al, 1993; Onyshkevych, 1993; Grishman andSundheim, 1995; Ravichandran and Hovy, 2002;Agichtein and Gravano, 2000; Davidov and Rap-poport, 2009).1For example, Barack Obama picks out precisely one per-son, and the same one in 2010 as it did in 1990.
In contrast, thecurrent US president picks out different people every 4-8 years.For indeteminacy of boundaries, consider a statement like USofficials are wealthy.
To determine its truth, one must first se-cure agreement on acceptable referents of US officials.In contrast, the recently proposed Open Informa-tion Extraction paradigm aims to detect related pairsof entities without knowing in advance what kinds ofrelations exist between entities in the source data andwithout any seeding (Banko and Etzioni, 2008).
Thepossibility of such extraction in English is attributedby the authors to a small number of syntactic pat-terns that realize binary relations between entities.In particular, they found that almost 40% of such re-lations are realized by the argument-verb-argumentpattern (henceforth, AVA) (see Table 1 in Banko andEtzioni (2008)).The TextRunner system (Banko and Etzioni,2008) is trained using a CRF classifier on S-V-Otuples from a parsed corpus as positive examples,and tuples that violate phrasal structure as negativeones.
The examples are described using featuresthat do not require parsing or semantic role labe-ling.
Features include part-of-speech tags, regularexpressions (detecting capitalization, punctuation,etc.
), context words belonging to closed classes, andconjunctions of features occurring in adjacent posi-tions within six words of the current word.TextRunner achieves P=0.94, R=0.65, and F-Score=0.77 on the AVA pattern (Banko and Etzioni,2008).
We note that all relations in the test sen-tences involve a predicate connecting two named en-tities, or a named entity and a date.2 The authorskindly made available to us for research purposes adatabase of about 2 bln AVA extractions producedby TextRunner; this database was used in the expe-riments reported below.4 DataWe randomly sampled essays written on 10 diffe-rent prompts, 200 essays per prompt.
Essays aregraded on the scale of 1-6; the distribution of gradesis shown in table 1.Grade 1 2 3 4 5 6% 0.6 4.9 23.5 42.6 23.8 4.7Table 1: The distribution of grades for 2,000 essays.2http://www.cs.washington.edu/research/knowitall/hlt-naacl08-data.txt645 Building Queries from EssaysWe define a query as a 3-tuple <NE,?,NP>,3 whereNE is a named entity and NP is a noun phrase fromthe same or neighboring sentence in a test-taker es-say (the selection process is described in section5.2).
We use the pattern of predicate matches againstthe TextRunner database to assess the degree and theequivocality of the connection between NE and NP.5.1 Named Entities in Test-Taker EssayWe use the Stanford Named Entity Recognizer(Finkel et al, 2005) that tags named entities as peo-ple, locations, organizations, and miscellaneous.
Weannotated a sample of 90 essays for named entities;the sample yielded 442 tokens, which we classifiedas shown in Table 2.
The Enamex classes (people,locations, organizations) account for 58% of all theentities in the sample.
The recognizer?s recall ofpeople and locations is excellent (though they arenot always classified correctly ?
see caption of Ta-ble 2), although test-taker essays feature additionalentity types that are not detected as well.Category Recall ExamplesLocation 0.98 Iraq, USAPerson 0.96 George W. Bush, FreudOrg.
0.87 Guggenheim FoundationGov.
0.79 No Child Left BehindAwards 0.79 Nobel PrizeEvents 0.68 Civil War, World War ISci & Tech 0.59 GPS, Windows 3.11Art 0.44 Beowulf, Little WomenTable 2: Recall of the Stanford NER by category.
Notethat an entity is counted as recalled as long as it is iden-tified as belonging to any NE category, even if it is mis-classified.
For example, Freud is tagged as location, butwe count it towards the recall of people.In terms of precision, we observed that the taggermade few clear mistakes, such as tagging sentence-initial adverbs and their mis-spelled versions asnamed entities (Eventhough, Afterall).
The bulk of3We do not attempt matching the predicate, as (1) in manycases there is no clearly lexicalized predicate (see the discussionof single step patterns in section 5.2) and (2) adding a predicatefield would make matches against the database sparser (see sec-tion 6.1).the 96 items over-generated by the tagger are in the?grey area?
?
while we haven?t marked them, theyare not clearly mistakes.
A common case are namesof national and religious groups, such as Muslimor Turkish, or capitalizations of otherwise commonnouns for emphasis and elevation, such as Arts orMasters.
Given our objective to ground the queriesin items with specific referents, these are less sui-table.
If all such cases are counted as mistakes, thetagger?s precision is 82%.5.2 Selection of NPsWe employ a grammar-based approach for selectingNPs.
We use the Stanford dependency parser (deMarneffe et al, 2006; Klein and Manning, 2003) todetermine dependency relations.In order to find out which dependency paths con-nect between named entities and clearly related NPsin essays, we manually marked concepts related to95 NEs in 10 randomly sampled essays.
We marked210 query-able concepts in total.
The resulting 210dependency paths were classified according to thedirection of the movement.Out of the 210 paths, 51 (24%) contain a singleupward or downard step, that is, are cases wherethe NE is the head of the constituent in which theNP is embedded, or the other way around.
Someexamples are shown in Figure 1.
Note that the pre-dicate connecting NE and NP is not lexicalized, butthe existence of connection is signaled by the close-knit grammatical pattern.The most prolific family of paths starts with anupward step, followed by a sequences of 1-4 down-wards steps; 71 (34%) of all paths are of this type.Most typically, the first upward move connects theNE to the predicate of which it is an argument, and,down from there, to either the head of another argu-ment (??)
or to an argument?s head?s modifier (???
).These are explicit relations, where the relation istypically lexicalized by the predicate.We expand the context of extraction beyond a sin-gle sentence only for NEs classified as PERSON.
Weapply a gazetteer of private names by gender fromUS Census 2010 to expand a NE of a given gen-der with the appropriate personal pronouns; a wordthat is a part of the original name (only surname, for4NE=Kroemer; NP=Heterojunction Bipolar Transitor65?
a Nobel Prize in a science field?
Chaucer, in the 14 century, ...?
the prestige of the Nobel Prize?
Kidman?s talent??
Kroemer received the Nobel Prize???
Kroemer received the Nobel Prize for his workon the Heterojunction Bipolar Transitor4Figure 1: Examples of dependency paths used for queryconstruction.example), is also considered an anaphor and a can-didate for expansion.
We expand the context of thePERSON entity as long as the subsequent sentenceuses any of the anaphors for the name.
This way, wehope to capture an extended discussion of a namedentity and construct queries around its anaphoricmentions just as we do around the regular, NE men-tion.
A name that is not predominantly male or fe-male is not expanded with personal pronouns.
Ta-ble 3 shows the distribution of queries automaticallygenerated from the sample of 2,000 essays.?
2,817 15.9%?
798 4.5%??
813 4.6%??
372 2.1%??
4,940 27.8%???
2,691 15.1%????
1,568 8.8%???
3,772 21.2%total 17,771 100%Table 3: Distribution of queries by path type.6 Matching and Filtering Queries6.1 Relaxation for improved matchingTo estimate the coverage of the fact repository withrespect to the queries extracted from essays, we sub-mit each query to the TextRunner repository in the<NE,?,NP> format and record the number of timesthe repository returned any matches at all.
The per-centage of matched queries is 21%.
To increase thechances of finding a match, we process the NP to re-move determiners and pre-modifiers of the head thatare very frequent words, such as removing a veryfrom a very beautiful photograph.Additionally, we produce three variants of the NP.The first, NP1, contains only the sequence of nounsending with the head noun; in the example, NP1would be photograph.
The second variant, NP2,contains only the word that is rarest in the wholeof NP.
All capitalized words are given the lowestfrequency of 1.
Thus, if any of the NP words arecapitalized, the NP2 would either contain an out ofvocabulary word to the left of the first capitalizedword, or the leftmost capitalized word.
This meansthat names would typically be split such that only thefirst name is taken.
For example, the NP the authorOrhan Phamuk would generate NP2 Orhan.
Whenno capitalized words exist, we take the rarest one,thus a NP category 3 hurricane would yield NP2hurricane.
The third variant only applies to NPswith capitalized parts, and takes the rightmost capi-talized word in the query.
Thus, the NP the actressNicole Kidman would yield NP3 Kidman.Applying these procedures to every NP inflatesthe number of actual queries posed to the TextRun-ner repository by almost two-fold (31,211 instead of17,771), while yielding a 50% increase in the num-ber of cases where at least one variant of the originalquery had at least one match against the repository(from 21% to 35%).6.2 Match-specific filtersIn order to zero in on matches that correpond to fac-tual statements and indeed pertain to the queried ar-guments, we implement a number of filters.Predicate filtersWe filter out modal and hedged predicates, usinglists of relevant markers.
We remove predicates likemight turn out to be or possibly attended, as well asfuture tense predicates (marked with will).Argument filtersFor matches that passed the predicate filters, wecheck the arguments.
Let mARG be the actualstring that matched ARG (ARG ?{NE,NP}).
LetEC (Essay Context) refer to source sentence(s) in66the essay.5 We filter out the following matches:?
Capitalized words follow ARG in mARG thatare not in EC;?
>1 capitalized or rare words precede ARG inmARG that are not in EC and not honorifics;?
mARG is longer than 8 words;?
More than 3 words follow ARG in mARG.The filters target cases where mARG is more spe-cific than ARG, and so the connection to ARG mightbe tenuous, such as ARG=Harriet Beecher Stowe,mARG = Harriet Beecher Stowe Center.6.3 Filters based on overall pattern of matches6.3.1 Negation filterFor all matches for a given query that passed thefilters in section 6.2, we tally positive vs negativepredicates.6 If the ratio of negative to positive isabove a threshold (we use 0.1), we consider thequery an unsuitable candidate for being ?potentiallycommon to all,?
and therefore do not credit the au-thor with having mentioned a fact.This criterion of potential acceptance by a uni-versal audience fails a query such as <BarackObama,?,US citizen>, based on the following pat-tern of matches:Count Predicate10 is not4 is2 was always1 is really1 isn?t1 was notIn a similar fashion, an essay writer?s statementthat ?The beating of Rodney King in Los Angeles... made for tense race relations?
is not quite in ac-cord with the 16 hits garnered by the statement ?TheLos Angeles riots were not caused by the RodneyKing verdict,?
against other hits with predicates likeerupted after, occurred after, resulted from, weresparked by, followed.5A single sentence, unless anaphor-based expansion wascarried out; see section 5.2.6We use a list of negation markers to detect those.Somewhat more subtly, the connection betweenAlbert Einstein and atomic bomb, articulated as ?Forexample, Albert Einstein?s accidental developmentof the atomic bomb has created a belligerent tech-nological front?
by a test-taker, is opposed by 6 hitswith the predicate did not build against matches withpredicates such as paved the way to, led indirectlyto, helped in, created the theory of.
The conflictingaccounts seem to reflect a lack of consensus on thedegree of Einstein?s responsibility.The cases above clearly demonstrate the implica-tions of the argumentative notion of facts used inour project.
Facts are statements that the audience isprepared to accept without further justification, dif-ferently from arguments, and even from presump-tions (statements about what is normal and likely),for which, as Perelman and Olbrechts-Tyteca (1969)observe, ?additional justification is beneficial forstrengthening the audience?s adherence.?
Certainlyin the Obama case and possibly in others, a differentnotion of factuality, for example, a notion that em-phasizes availability of legally acceptable suppor-ting evidence, would have led to a different result.Yet, in an ongoing instance of argumentation, themere need to resort to such a proof is already a signthat the audience is not prepared to accept a state-ment as a fact.6.4 Additional filtersWe also implemented a number of filters aimed atdetecting excessive diversity in the matches, whichcould suggest that there is no clear and systema-tic relation between the NE and the NP.
The filtersare conjunctions of thresholds operating over mea-sures such as purity of matches (percentage of exactmatches in NE or NP), degree of overlap of non-purematches with the context of the query in the essay,clustering of the predicates (recurrence of the samepredicates across matches), general frequencies ofNE and NP.7 Evaluation7.1 Manual check of queriesA manual check of a small subset of queries was ini-tially intended as an interim evaluation of the queryconstruction process, to see how often the producedqueries are deficient candidates for later verification.67However, we also decided to include a human fact-check of the queries that were found to be verifiable,to see the kinds of factual mistakes made in essays.A research assistant was asked to classify 500queries into Wrong (the NE and NP are notrelated in the essay), Trivial (almost any NEcould be substituted, as in <WWI,?, Historians>),Subjective (<T.S.Eliot,?,the most frightening poetof all time>), VC ?
verifiable and correct, VI ?
veri-fiable and incorrect.
Table 4 shows the distribution.W T S VC VI18% 13% 13% 54% 2%Table 4: The distribution of query types for 500 queries.Queries classified as Wrong (18%) mostly cor-respond to parser mistakes.
Trivial and Subjectivequeries, while not attributing to the author connec-tions that she has not made, are of questionable valueas far as fact-checking goes.
Perhaps the most sur-prising figure is the meager amount of verifiable andincorrect queries.
Examples of relevant statementsfrom essays include (NE and NP are boldfaced):?
For example, Paul Gaugin who was a sucess-ful business man, with a respectable wife andfamily, suddenly gave in to the calling of thearts and left his life.
(He was a failing busi-nessman immediately before leaving family.)?
For example, in Jane Austin?s Little Women,she portrays the image of a lovely family andthe wonders of womenhood.
(The book is byLouisa May Alcott.)?
This occurrence can be seen with the Rod-ney King problem in California during the late1980?s.
(The Rodney King incident occurredon March 3, 1991).?
We see the philosophers Aristotle, Plato,Socrates and their practical writings of thepolitical problems and issues of the day.
(Socrates is not known to have left writings.
)First, we observe that factual mistakes are rare.Furthermore, they seem to pertain to one in a seriesof related facts, most of which are correct and testifyto the author?s substantial knowledge about the mat-ter ?
consider Paul Gaugin?s biography or the con-tents of ?Little Women?
in the examples above.
Itis therefore unclear how detrimental the occasionalfactual ?glitches?
are to the quality of the essay.8 Application to Essay ScoringWe show Pearson correlations between humanscores given to essays and a number of characte-ristics derived from the work described here, as wellas the partial correlations when the effect of essaylength is factored out.
We calculated both the cor-relations using raw numbers and on a logarithmicscale, with the latter generally producing higher cor-realtions.
Therefore, we are reporting the correla-tions between grade and the logarithm of the rele-vant characteristic.
The characteristics are:#NE The number of NE tokens in an essay.#Queries The number of queries generated by thesystem from the given essay (as described insection 5.2).#Matched Queries The number of queries forwhich a match was found in the TextRunnerdatabase.
If the original query or any of its ex-pansion variants (see section 6.1) had matches,the query contributes a count of 1.#Filtered Matches The number of queries thatpassed the filters introduced in section 6.
If theoriginal query or any of its expansion variantspassed the filters, the query contributes a countof 1.Table 5 shows the results.
First, we find that allcorrelations are significant at p=0.05, as well as thepartial correlations exluding the effect of length for 7out of 10 prompts.
All correlations are positive, thatis, the more factual information a writer employs inan essay, the higher the grade ?
beyond the oft re-ported correlations between the grade and the lengthof an essay (Powers, 2005).Second, we notice that all characteristics ?
fromthe number of named entities to the number of fil-tered matches ?
produce similar correlation figures.Third, there are large differences between averagenumbers of named entities per essay across prompts.68Prompt NE Pearson Corr.
with Grade Partial Corr.
Removing Length#NE #Q #Mat.
# Filt.
#NE #Q #Mat.
# Filt.P1 280 0.144 0.154 0.182 0.185 0.006 0.019 0.058 0.076P2 406 0.265 0.259 0.274 0.225 0.039 0.053 0.072 0.069P3 452 0.245 0.225 0.188 0.203 0.049 0.033 0.009 0.051P4 658 0.327 0.302 0.335 0.327 0.165 0.159 0.177 0.160P5 704 0.470 0.477 0.473 0.471 0.287 0.294 0.304 0.305P6 750 0.429 0.415 0.388 0.373 0.271 0.242 0.244 0.257P7 785 0.470 0.463 0.479 0.469 0.302 0.302 0.341 0.326P8 838 0.423 0.390 0.406 0.363 0.264 0.228 0.266 0.225P9 919 0.398 0.445 0.426 0.393 0.158 0.209 0.233 0.219P10 986 0.455 0.438 0.375 0.336 0.261 0.257 0.170 0.175AV.
678 0.363 0.357 0.353 0.335 0.180 0.180 0.187 0.186Table 5: Pearson correlation and partial correlation removing the effect of length between a number of characteristics(all on a log scale) and the grade.
The second column shows the total number of identified named entities in the200-essay sample from the given prompt.
The prompts are sorted by the second column.Generally, the higher the number, the better the num-ber of named entities in the essay predicts its grade(the more NEs the higher the grade).
This suggeststhat the use of named entities might be relativelyirrelevant for some prompts, and much more rele-vant for others.
For example, prompt P10 reads?The arts (painting, music, literature, etc.)
revealthe otherwise hidden ideas and impulses of a soci-ety,?
thus practically inviting exemplification usingspecific works of art or art movements, while suc-cess with prompt P1 ?
?The human mind will al-ways be superior to machines because machines areonly tools of human minds?
?
is apparently not asdependent on named entity based exemplification.Excluding prompts with smaller than average totalnumber of named entities (<678), the correlationsaverage 0.40-0.44 across the various characteristics,with partial correlations averaging 0.25-0.26.9 Discussion and Conclusion9.1 Summary of the main resultIn this article, we proposed a way to measure theuse of factual information in text-taker essays.
Wedemonstrated that the use of factual information isindicative of essay quality, observing positive corre-lations between the count of instances of fact-use inessays and the grade of the essay, beyond what canbe attributed to a correlation between the total num-ber of words in an essay and the grade.9.2 What is driving the correlations?We also investigated which of the components ofthe fact-use measure were responsible for the ob-served correlations.
Specifically, we considered (a)the number instances of fact-use that were verifiedagainst a database of human-produced assertions,filtered for controversy and excessive diversity; (b)the number of instances of fact-use that were verifiedagainst the database, without subsequent filtering;(c) the number of instances of fact-use identified inan essay (without checking against the database); (d)the number of named entities used in an essay (with-out constructing queries around the entity).
Thesesteps correspond to a gradual relaxation of the fullfact-checking procedure all the way to a proxy mea-sure that counts the number of named entities.We observed similar correlations throughout therelaxation procedure.
We therefore conclude that thenumber of named entities is the driving force behindthe correlations, with no observed effect of the queryconstruction and verification procedures.7 This re-sult could be explained by two factors.First, a manual check of 500 queries showed thatfactual mistakes are rare ?
only 2% of the queriescorresponded to factually incorrect statements.
Fur-thermore, mistakes were often accompanied by the7While the trend is in the direction of an increase in Pearsoncorrelations from (a) to (d), the differences are not statisticallysignificant.69test-taker?s use of additional facts about the same en-tity which were correct; this might alleviate the im-pact of a mistake in the eyes of a grader.Second, the query verification procedure appliedto only about 35% of the queries ?
those for whichat least one match was found in the database, thatis, 65% of the queries could not be assessed usingthe database of 2 bln extractions.
The verificationprocedure is thus much less robust than the proce-dure for detecting named entities, which performs atabove >80% recall and precision.9.3 Implications for automated scoringOur results suggest that essays on a general topicwritten by adults for a high-stakes exam containfew incorrect facts, so the potential for a full fact-checking system to improve correlations with gradesbeyond merely detecting the potential for a factualstatement using a named entity recognizer is notlarge.
While a measure based on the number of?verified?
facts found in an essay demonstrated asignificant correlation with human scores beyondthe contribution of essay length, a simpler measurebased only on the number of named entities in theessay demonstrated a similar relationship with hu-man scores.Given the similarity in the two features?
empiri-cal usefulness, it would seem that the feature thatcounts the number of named entities in an essay is abetter candidate, due to its simplicity and robustness.However, there is another perspective from which afeature based only on the number of named entitiesin an essay may be less suitable for use in scoring:the perspective of construct validity, the degree towhich a test (or, in this case, a scoring system) ac-tually measures what it purports to.
As mentionedabove, the number of named entities in an essay is,at best, a proxy measure,8 roughly indicative of thereferencing of factual statements in support of an ar-gument within an essay.
Because the measure itselfis not directly sensitive to how named entities areused in the essay, though, even entities with no con-nection to the essay topic would tend to contributeto the score, and the measure is therefore vulnerableto manipulation by test-takers.8For a discussion of proxes vs trins in essay grading, see(Page and Petersen, 1995).An obvious strategy to exploit this scoring mecha-nism would be to simply include more named enti-ties in an essay, either interspersing them randomlythroughout the text, or including them in long lists ofexamples to illustrate a single point.
Such a blatantapproach could potentially be detected by the use ofa filter or advisory (Higgins et al, 2006; Landaueret al, 2003) designed to identify anomalous writingstrategies.
However, there could be more subtle ap-proaches to exploiting such a feature.
For example,it is possible that test-takers might be inclined to in-crease their use of named entities by adducing morefacts in support of an argument, and would go be-yond the comfort zone of their actual factual know-ledge, thus making more factual mistakes.
Test gam-ing strategies have been recognized as a threat to au-tomated scoring systems for some time (Powers etal., 2001), and there is evidence based on test tak-ers?
own self-reported behavior that this threat is real(Powers, 2011).
This is one major reason why large-scale operational testing programs (such as GRE orTOEFL) use automated essay scoring only in com-bination with human ratings.
In sum, the degree towhich a linguistic feature is predictive of human es-say scores is not the only criterion for evaluation; thewashback effects of using the feature (on writing be-havior and on instruction) must also be considered.The second finding of this study is that the ef-fectiveness of fact-checking for essay assessment iscompromised by the limited coverage of the wealthof factual statements made by essay writers, withonly 35% of queries garnering any hits at all in alarge general-purpose database of assertions.
It ispossible, however, that OpenIE technology can beused to collect more focused repositories on specifictopics, such as the history of the American CivilWar, which could be used to assess responses totasks related to that particular subject matter.
Thisis one of the directions of our future research.ReferencesEugene Agichtein and Luis Gravano.
2000.
Snowball:Extracting relations from large plain-text collections.In Proceedings of the 5th ACM conference on DigitalLibraries, pages 85?94.
ACM.Yigal Attali and Jill Burstein.
2006.
Automated Es-70say Scoring With e-rater R?V.2.
Journal of Technology,Learning, and Assessment, 4(3).Michele Banko and Oren Etzioni.
2008.
The TradeoffsBetween Open and Traditional Relation Extraction.
InProceedings of the 46th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 28?36,Columbus, OH, June.
Association for ComputationalLinguistics.Beata Beigman Klebanov, Jill Burstein, Nitin Madnani,Adam Faulkner, and Joel Tetreault.
2012.
BuildingSubjectivity Lexicon(s) From Scratch For Essay Data.In Proceedings of CICLING, New Delhi, India.Tao-Hsing Chang, Chia-Hoang Lee, and Yu-MingChang.
2006.
Enhancing Automatic Chinese Es-say Scoring System from Figures-of-Speech.
In Pro-ceedings of the 20th Pacific Asia Conference on Lan-guage, Information and Computation, pages 28?34.Yen-Yu Chen, Chien-Liang Liu, Chia-Hoang Lee, andTao-Hsing Chang.
2010.
An Unsupervised Auto-mated Essay Scoring System.
IEEE Transactions onIntelligent Systems, 25(5):61?67.Nancy Chinchor, Lynette Hirschman, and David Lewis.1993.
Evaluating Message Understanding Systems:An Analysis of the Third Message UnderstandingConference (MUC-3).
Computational Linguistics,19(3):409?449.Dmitry Davidov and Ari Rappoport.
2009.
Geo-mining:Discovery of Road and Transport Networks Using Di-rectional Patterns.
In Proceedings of EMNLP, pages267?275.Arijit De and Sunil Kopparapu.
2011.
An unsupervisedapproach to automated selection of good essays.
InRecent Advances in Intelligent Computational Systems(RAICS), 2011 IEEE, pages 662 ?666.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher Manning.
2006.
Generating Typed De-pendency Parses from Phrase Structure Parses.
In Pro-ceedings of LREC, pages 449?454, Genoa, Italy, May.Oren Etzioni, Michele Banko, Stephen Soderland, andDaniel Weld.
2008.
Open information extraction fromthe web.
Commun.
ACM, 51(12):68?74.Jenny Finkel, Trond Grenager, and Christopher Manning.2005.
Incorporating Non-local Information into In-formation Extraction Systems by Gibbs Sampling.
InProceedings of the 43rd Annual Meeting of the Asso-ciation for Computational Linguistics, pages 363?370,Ann Arbor, MI, June.
Association for ComputationalLinguistics.Ralph Grishman and Beth Sundheim.
1995.
Design ofthe MUC-6 evaluation.
In Proceedings of MUC, pages1?11.Derrick Higgins, Jill Burstein, and Yigal Attali.
2006.Identifying off-topic student essays without topic-specific training data.
Natural Language Engineering,12(2):145?159.Tsunenori Ishioka and Masayuki Kameda.
2006.
Auto-mated Japanese Essay Scoring System based on Arti-cles Written by Experts.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand 44th Annual Meeting of the Association for Com-putational Linguistics, pages 233?240, Sydney, Aus-tralia, July.
Association for Computational Linguistics.Tuomo Kakkonen and Erkki Sutinen.
2004.
Automaticassessment of the content of essays based on coursematerials.
In Proceedings of the International Confer-ence on Information Technology: Research and Edu-cation, pages 126?130, London, UK.Tuomo Kakkonen, Niko Myller, Jari Timonen, and ErkkiSutinen.
2005.
Automatic Essay Grading with Prob-abilistic Latent Semantic Analysis.
In Proceedings ofthe Second Workshop on Building Educational Appli-cations Using NLP, pages 29?36, Ann Arbor, Michi-gan, June.
Association for Computational Linguistics.Dan Klein and Christopher Manning.
2003.
AccurateUnlexicalized Parsing.
In Proceedings of the 41st An-nual Meeting of the Association for ComputationalLinguistics, pages 423?430, Sapporo, Japan, July.
As-sociation for Computational Linguistics.Saul Kripke.
1980.
Naming and Necessity.
Harvard Uni-versity Press.Thomas Landauer, Darrell Laham, and Peter Foltz.
2003.Automated scoring and annotation of essays with theIntelligent Essay Assessor.
In Mark Shermis and JillBurstein, editors, Automated essay scoring: A cross-disciplinary perspective, pages 87?112.
Lawrence Erl-baum Associates, Mahwah, New Jersey.Leah Larkey.
1998.
Automatic essay grading using textcategorization techniques.
In Proceedings of SIGIR,pages 90?95, Melbourne, AU.Beno?
?t Lemaire and Philippe Dessus.
2001.
A System toAssess the Semantic Content of Student Essays.
Jour-nal of Educational Computing Research, 24:305?320.Annie Louis and Derrick Higgins.
2010.
Off-topic essaydetection using short prompt texts.
In Proceedings ofthe NAACL HLT 2010 Fifth Workshop on InnovativeUse of NLP for Building Educational Applications,pages 92?95, Los Angeles, California, June.
Associ-ation for Computational Linguistics.Boyan Onyshkevych.
1993.
Template design for infor-mation extraction.
In Proceedings of MUC, pages 19?23.Ellis Page and Nancy Petersen.
1995.
The computermoves into essay grading: Updating the ancient test.Phi Delta Kappan, 76:561?565.Cha?
?m Perelman and Lucie Olbrechts-Tyteca.
1969.
TheNew Rhetoric: A Treatise on Argumentation.
Notre71Dame, Indiana: University of Notre Dame Press.Translated by John Wilkinson and Purcell Weaverfrom French original published in 1958.Donald Powers, Jill Burstein, Martin Chodorow, MaryFowles, and Karen Kukich.
2001.
StumpingE-Rater: Challenging the Validity of AutomatedEssay Scoring.
ETS research report RR-01-03,http://www.ets.org/research/policy research reports/rr-01-03.Donald Powers.
2005.
?Wordiness?
: A selective reviewof its influence, and suggestions for investigatingits relevance in tests requiring extended writtenresponses.
ETS research memorandum RM-04-08,http://www.ets.org/research/policy research reports/rm-04-08.Donald Powers.
2011.
Scoring the TOEFLIndependent Essay Automatically: Re-actions of Test Takers and Test ScoreUsers.
ETS research manuscript RM-11-34,http://www.ets.org/research/policy research reports/rm-11-34.Deepak Ravichandran and Eduard Hovy.
2002.
Learningsurface text patterns for a Question Answering System.In Proceedings of ACL, pages 41?47.Carolyn Rose?, Antonio Roqueand, Dumisizwe Bhembe,and Kurt VanLehn.
2003.
A hybrid text classifica-tion approach for analysis of student essays.
In Pro-ceedings of the Second Workshop on Building Educa-tional Applications Using NLP, pages 29?36.72
