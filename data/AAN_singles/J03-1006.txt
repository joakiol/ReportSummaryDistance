c?
2003 Association for Computational LinguisticsSquibs and DiscussionsWeighted Deductive Parsing and Knuth?sAlgorithmMark-Jan Nederhof?University of GroningenWe discuss weighted deductive parsing and consider the problem of finding the derivation withthe lowest weight.
We show that Knuth?s generalization of Dijkstra?s algorithm for the shortest-path problem offers a general method to solve this problem.
Our approach is modular in the sensethat Knuth?s algorithm is formulated independently from the weighted deduction system.1.
IntroductionAs for algorithms in general, there are significant advantages to specifying parsingalgorithms in a modular way (i.e., as the combination of subalgorithms).
First, modularspecifications often allow simpler implementations.
Secondly, if otherwise seeminglydistinct types of parser are described in a modular way, the common parts can oftenbe more readily identified, which helps to classify and analyze parsing algorithms.In this article we discuss a modular design for weighted deductive parsing bydistinguishing between a weighted deduction system, on the one hand, which per-tains to the choice of grammatical formalism and parsing strategy, and the algorithmthat finds the derivation with the lowest weight, on the other.
The latter is Dijkstra?salgorithm for the shortest-path problem (Dijkstra 1959) as generalized by Knuth (1977)for a problem on grammars.
It has been argued by, for example, Backhouse (2001),that this algorithm can be used to solve a wide range of problems on context-freegrammars.
A brief presentation of a very similar algorithm for weighted deductiveparsing has been given before by Eisner (2000, Figure 3.5e).Our presentation contrasts with that of Klein and Manning (2001), who offer an in-divisible specification for a small collection of parsing strategies for weighted context-free grammars only, referring to a generalization of Dijkstra?s algorithm to hypergraphsby Gallo et al (1993).
This article also addresses the efficiency of Knuth?s algorithmfor weighted deductive parsing, relative to the more commonly used algorithm byViterbi.2.
Weighted Deductive ParsingThe use of deduction systems for specifying parsers has been proposed by Shieber,Schabes, and Pereira (1995) and Sikkel (1997).
As already remarked by Goodman(1999), deduction systems can also be extended to manipulate weights.1 Here we de-?
Faculty of Arts, Humanities Computing, University of Groningen, P.O.
Box 716, NL-9700 ASGroningen, The Netherlands.
E-mail: markjan@let.rug.nl.
Secondary affiliation is the German ResearchCenter for Artificial Intelligence (DFKI).1 Weighted deduction is closely related to probabilistic logic, although the problem considered in thisarticle (viz., finding derivations with lowest weights) is different from typical problems in probabilisticlogic.
For example, Frisch and Haddawy (1994) propose inference rules that manipulate logicalformulas attached to intervals of probabilities, and the objective of deduction is to determine intervalsthat are as narrow as possible.136Computational Linguistics Volume 29, Number 1Initializer:y : [B ?
?
?, j, j]{(y : B ?
?)
?
P0 ?
j ?
nScanner:x1 : [A ?
?
?
a?, i, j]x1 : [A ?
?a ?
?, i, j + 1]???
(y1 : A ?
?a?)
?
P0 ?
i ?
j < naj+1 = aCompleter:x1 : [A ?
?
?
B?, i, j]x2 : [B ?
?
?, j, k]x1 + x2 : [A ?
?B ?
?, i, k]???
(y1 : A ?
?B?)
?
P(y2 : B ?
?)
?
P0 ?
i ?
j ?
k ?
nGoal items: [S ?
?
?, 0, n] for any (y : S ?
?)
?
P, where S is the start symbolFigure 1Weighted deduction system for bottom-up parsing.fine such a weighted deduction system for parsing as consisting of a finite set ofinference rules of the form:x1 : I1x2 : I2...xm : Imf (x1, x2, .
.
.
, xm) : I0????
?c1...cpwhere m ?
0 and p ?
0, and I0, I1, .
.
.
, Im are items, of which I0 is the consequentand I1, .
.
.
, Im are the antecedents, and c1, .
.
.
, cp is a list of side conditions linkingthe inference rule to the grammar and the input string.2 We assign unique variablesx1, .
.
.
, xm to each of the antecedents, and a weight function f , with x1, .
.
.
, xm as argu-ments, to the consequent.
This allows us to assign a weight to each occurrence of an(instantiated) item that we derive by an inference rule, by means of a function on theweights of the (instantiated) antecedents of that rule.A weighted deduction system furthermore contains a set of goal items; like theinference rules, this set is parameterized by the grammar and the input.
The objectiveof weighted deductive parsing is to find the derivation of a goal item with the lowestweight.
In this article we assume that, for a given grammar and input string, eachinference rule can be instantiated in a finite number of ways, which ensures that thisproblem can be solved under the constraints on the weight functions to be discussedin Sections 4 and 5.Our examples will be restricted to context-free parsing and include the deductionsystem for weighted bottom-up parsing in Figure 1 and that for weighted top-downparsing in Figure 2.
The latter is very close to an extension of Earley?s algorithmdescribed by Lyon (1974).
The side conditions refer to an input string w = a1 ?
?
?
an andto a weighted context-free grammar with a set of productions P, each of which has theform (y: A ?
?
), where y is a non-negative real-valued weight, A is a nonterminal,2 Note that we have no need for (explicit) axioms, since we allow inference rules to have zeroantecedents.137Nederhof Weighted Deductive ParsingStarter:y : [S ?
?
?, 0, 0]{(y : S ?
?)
?
P, where S is the start symbolPredictor:x1 : [A ?
?
?
B?, i, j]y2 : [B ?
?
?, j, j]???
(y1 : A ?
?B?)
?
P(y2 : B ?
?)
?
P0 ?
i ?
j ?
nScanner, completer and set of goal items are as in Figure 1.Figure 2Weighted deduction system for top-down parsing.Starter:(y, y) : [S ?
?
?, 0, 0]{(y : S ?
?)
?
P, where S is the start symbolScanner:(z1, x1) : [A ?
?
?
a?, i, j](z1, x1) : [A ?
?a ?
?, i, j + 1]???
(y1 : A ?
?a?)
?
P0 ?
i ?
j < naj+1 = aPredictor:(z1, x1) : [A ?
?
?
B?, i, j](z1 + y2, y2) : [B ?
?
?, j, j]???
(y1 : A ?
?B?)
?
P(y2 : B ?
?)
?
P0 ?
i ?
j ?
nCompleter:(z1, x1) : [A ?
?
?
B?, i, j](z2, x2) : [B ?
?
?, j, k](z1 + x2, x1 + x2) : [A ?
?B ?
?, i, k]???
(y1 : A ?
?B?)
?
P(y2 : B ?
?)
?
P0 ?
i ?
j ?
k ?
nSet of goal items is as in Figure 1.Figure 3Alternative weighted deduction system for top-down parsing.and ?
is a list of zero or more terminals or nonterminals.
We assume the weightof a grammar derivation is given by the sum of the weights of the occurrences ofproductions therein.Weights may be atomic entities, as in the deduction systems discussed above,where they are real-valued, but they may also be composed entities.
For example,Figure 3 presents an alternative form of weighted top-down parsing using pairs ofvalues, following Stolcke (1995).
The first value is the forward weight, that is, the sumof weights of all productions that were encountered in the lowest-weighted derivationin the deduction system of an item [A ?
?
?
?, i, j].
The second is the inner weight;that is, it considers the weight only of the current production A ?
??
plus the weightsof productions in lowest-weighted grammar derivations for nonterminals in ?.
Theseinner weights are the same values as the weights in Figures 1 and 2.
In fact, if we omitthe forward weights, we obtain the deduction system in Figure 2.Since forward weights pertain to larger parts of grammar derivations than theinner weights, they may be better suited to direct the search for the lowest-weightedcomplete grammar derivation.
We assume a pair (z1, x1) is smaller than (z2, x2) if and138Computational Linguistics Volume 29, Number 1only if z1 < z2 or z1 = z2 ?
x1 < x2.
(Tendeau [1997] has shown the general idea canalso be applied to left-corner parsing.
)In order to link (weighted) deduction systems to literature to be discussed inSection 3, we point out that a deduction system having a grammar G in a certainformalism F and input string w in the side conditions can be seen as a construction c ofa context-free grammar c(G, w) out of grammar G and input w. The set of productionsof c(G, w) is obtained by instantiating the inference rules in all possible ways usingproductions from G and input positions pertaining to w. The consequent of suchan instantiated inference rule then acts as the left-hand side of a production, andthe (possibly empty) list of antecedents acts as its right-hand side.
In the case ofa weighted deduction system, the productions are associated with weight functionscomputing the weight of the left-hand side from the weights of the right-hand sidenonterminals.For example, if the input is w = a1a2a3, and if there are two productions in theweighted context-free grammar G of the form (y1: A ?
C B D), (y2: B ?
E) ?
P,then from the completer in Figure 1 we may obtain, among others, a production[A ?
C B ?
D, 0, 2] ?
[A ?
C ?
B D, 0, 1] [B ?
E ?, 1, 2], with associated weight functionf (x1, x2) = x1 + x2, which states that if the production is used in a derivation, then theweights of the two subderivations should be added.
The number of productions inc(G, w) is determined by the number of ways we can instantiate inference rules, whichin the case of Figure 1 is O(|G|2 ?
n3), where |G| is the size of G in terms of the totalnumber of occurrences of terminals and nonterminals in productions.If we assume, without loss of generality, that there is only one goal item, then thisgoal item becomes the start symbol of c(G, w).3 Since there are no terminals in c(G, w),either the grammar generates the language {}, containing only the empty string , orit generates the empty language; in the latter case, this indicates that w is not in thelanguage generated by G.Note that for all three examples above, the derivation with the lowest weightallowed by c(G, w) encodes the derivation with the lowest weight allowed by G forw.
Together with the dynamic programming algorithm to be discussed in the nextsection that finds the derivation with the lowest weight on the basis of c(G, w), weobtain a modular approach to describing weighted parsers: One part of the descriptionspecifies how to construct grammar c(G, w) out of grammar G and input w, and thesecond part specifies the dynamic programming algorithm to investigate c(G, w).Such a modular way of describing parsers in the unweighted case has alreadybeen fully developed in work by Lang (1974) and Billot and Lang (1989).
Insteadof a deduction system, they use a pushdown transducer to express a parsing strat-egy such as top-down parsing, left-corner parsing or LR parsing.
Such a pushdowntransducer can in the context of their work be regarded as specifying a context-freegrammar c(G, w), given a context-free grammar G and an input string w. The secondpart of the description of the parser is a dynamic programming algorithm for actuallyconstructing c(G, w) in polynomial time in the length of w.This modular approach to describing parsing algorithms is also applicable to for-malisms F other than context-free grammars.
For example, it was shown by Vijay-Shanker and Weir (1993) that tree-adjoining parsing can be realized by constructing acontext-free grammar c(G, w) out of a tree-adjoining grammar G and an input stringw.
This can straightforwardly be generalized to weighted (in particular, stochastic)tree-adjoining grammars (Schabes 1992).3 If there is more than one goal item, then a new symbol needs to be introduced as the start symbol.139Nederhof Weighted Deductive ParsingIt was shown by Boullier (2000) that F may furthermore be the formalism of rangeconcatenation grammars.
Since the class of range concatenation grammars generatesexactly PTIME, this demonstrates the generality of the approach.4Instead of string input, one may also consider input consisting of a finite au-tomaton, along the lines of Bar-Hillel, Perles, and Shamir (1964); this can be triviallyextended to the weighted case.
That we restrict ourselves to string input in this articleis motivated by presentational considerations.3.
Knuth?s AlgorithmThe algorithm by Dijkstra (1959) effectively finds the shortest path from a distin-guished source node in a weighted, directed graph to a distinguished target node.The underlying idea of the algorithm is that it suffices to investigate only the shortestpaths from the source node to other nodes, since longer paths can never be extendedto become shorter paths (weights of edges are assumed to be non-negative).Knuth (1977) generalizes this algorithm to the problem of finding lowest-weightedderivations allowed by a context-free grammar with weight functions, similar to thosewe have seen in the previous section.
(The restrictions Knuth imposes on the weightfunctions will be discussed in the next section.)
Again, the underlying idea of thealgorithm is that it suffices to investigate only the lowest-weighted derivations ofnonterminals.The algorithm by Knuth is presented in Figure 4.
We have taken the liberty ofmaking some small changes to Knuth?s formulation.
The largest difference betweenKnuth?s formulation and ours is that we have assumed that the context-free grammarwith weight functions on which the algorithm is applied has the form c(G, w), obtainedby instantiating the inference rules of a weighted deduction system for given grammarG and input w. Note, however, that c(G, w) is not fully constructed before applyingKnuth?s algorithm, and the algorithm accesses only as much of it as is needed in itssearch for the lowest-weighted goal item.In the algorithm, the set D contains items I for which the lowest overall weighthas been found; this weight is given by ?(I).
The set E contains items I0 that can bederived in one step from items in D, but for which the lowest weight ?
(I0) found thusfar may still exceed the lowest overall weight for I0.
In each iteration, it is establishedthat the lowest weight ?
(I) for an item I in E is the lowest overall weight for I, whichjustifies transferring I to D. The algorithm can be extended to output the derivationcorresponding to the goal item with the lowest weight; this is fairly trivial and willnot be discussed here.A few remarks about the implementation of Knuth?s algorithm are in order.
First,instead of constructing E and ?
anew at step 2 for each iteration, it may be moreefficient to construct them only once and revise them every time a new item I is addedto D. This revision consists in removing I from E and combining it with existing itemsin D, as antecedents of inference rules, in order to find new items to be added toE and/or to update ?
to assign lower values to items in E. Typically, E would beorganized as a priority queue.Second, practical implementations would maintain appropriate tables for indexingthe items in such a way that when a new item I is added to D, the lists of existing itemsin D together with which it matches the lists of antecedents of inference rules can be4 One may even consider formalisms F that generate languages beyond PTIME, but such applications ofthe approach would not necessarily be of practical value.140Computational Linguistics Volume 29, Number 11.
Let D be the empty set ?.2.
Determine the set E and the function ?
as follows:?
E is the set of items I0 /?
D such that there is at least oneinference rule from the deduction system that can beinstantiated to a production of the form I0 ?
I1 ?
?
?
Im, for somem ?
0, with weight function f , where I1, .
.
.
, Im ?
D.?
For each such I0 ?
E, let ?
(I0) be the minimal weightf (?
(I1), .
.
.
,?
(Im)) for all such instantiated inference rules.3.
If E is empty, then report failure and halt.4.
Choose an item I ?
E such that ?
(I) is minimal.5.
Add I to D, and let ?
(I) = ?(I).6.
If I is a goal item, then output ?
(I) and halt.7.
Repeat from step 2.Figure 4Knuth?s generalization of Dijkstra?s algorithm.
Implicit are a weighted deduction system, agrammar G and an input w. For conditions on correctness, see Section 4.efficiently found.
Since techniques for such kinds of indexing are well-established inthe computer science literature, no further discussion is warranted here.4.
Conditions on the Weight FunctionsA sufficient condition for Knuth?s algorithm to correctly compute the derivations withthe lowest weights is that the weight functions f are all superior, which means that theyare monotone nondecreasing in each variable and that f (x1, .
.
.
, xm) ?
max(x1, .
.
.
, xm)for all possible values of x1, .
.
.
, xm.
For this case, Knuth (1977) provides a short andelegant proof of correctness.
Note that the weight functions in Figure 1 are all superior,so that correctness is guaranteed.In the case of the top-down strategy from Figure 2, however, the weight functionsare not all superior, since we have constant weight functions for the predictor, whichmay yield weights that are less than their arguments.
It is not difficult, however, toshow that Knuth?s algorithm still correctly computes the derivations with the lowestweights, given that we have already established the correctness for the bottom-upcase.First, note that items of the form [B ?
?
?, j, j], which are introduced by theinitializer in the bottom-up case, can be introduced by the starter or the predictor inthe top-down case; in the top-down case, these items are generally introduced laterthan in the bottom-up case.
Second, note that such items can contribute to finding agoal item only if from [B ?
?
?, j, j] we succeed in deriving an item [B ?
?
?, j, k] that iseither such that B = S, j = 0, and k = n, or such that there is an item [A ?
?
?
B?, i, j].In either case, the item [B ?
?
?, j, j] can be introduced by the starter or predictorso that [B ?
?
?, j, k] will be available to the algorithm if and when it is needed todetermine the derivation with the lowest weight for [S ?
?
?, 0, n] or [A ?
?B ?
?, i, k],respectively, which will then have a weight greater than or equal to that of [B ?
?
?, j, j].141Nederhof Weighted Deductive ParsingFor the alternative top-down strategy from Figure 3, the proof of correctness issimilar, but now the proof depends for a large part on the additional forward weights,the first values in the pairs (z, x); note that the second values are the inner weights(i.e., the weights we already considered in Figures 1 and 2).
An important observationis that if there are two derivations for the same item with weights (z1, x1) and (z2, x2),respectively, such that z1 < z2 and x1 > x2, then there must be a third derivation of thatitem with weight (z1, x2).
This shows that no relevant inner weights are overlookedbecause of the ordering we imposed on pairs (z, x).Since Figures 1 through 3 are merely examples to illustrate the possibilities ofdeduction systems and Knuth?s algorithm, we do not provide full proofs of correctness.5.
Viterbi?s AlgorithmThis section places Knuth?s algorithm in the context of a more commonly used alter-native.
This algorithm is applicable on a weighted deduction system if a simple partialorder on items exists that is such that the antecedents of an inference rule are alwaysstrictly smaller than the consequent.
When this is the case, we may treat items fromsmall to large to compute their lowest weights.
There are no constraints on the weightfunctions other than that they should be monotone nondecreasing.The algorithm by Viterbi (1967) may be the earliest that operates according to thisprinciple.
The partial order is based on the linear order given by a string of inputsymbols.
In this article we will let the term ?Viterbi?s algorithm?
refer to the generaltype of algorithm to search for the derivation with the lowest weight given a deductionsystem, a grammar, an input string, and a partial order on items consistent with theinference rules in the sense given above.5Another example of an algorithm that can be seen as an instance of Viterbi?s algo-rithm was presented by Jelinek, Lafferty, and Mercer (1992).
This algorithm is essen-tially CYK parsing (Aho and Ullman 1972) extended to handle weights (in particular,probabilities).
The partial order on items is based on the sizes of their spans (i.e., thenumber of input symbols that the items cover).
Weights of items with smaller spansare computed before the weights of those with larger spans.
In cases in which a simplea priori order on items is not available but derivations are guaranteed to be acyclic,one may first determine a topological sorting of the complete set of derivable itemsand then compute the weights based on that order, following Martelli and Montanari(1978).A special situation arises when a deduction system is such that inference rulesallow cyclic dependencies within certain subsets of items, but dependencies betweenthese subsets represent a partial order.
One may then combine the two algorithms:Knuth?s (or Dijkstra?s) algorithm is used within each subset and Viterbi?s algorithmis used to relate items in distinct subsets.
This is exemplified by Bouloutas, Hart, andSchwartz (1991).In cases in which both Knuth?s algorithm and Viterbi?s algorithm are applicable,the main difference between the two is that Knuth?s algorithm may halt as soon asthe lowest weight for a goal item is found, and no items with larger weights than thatgoal item need to be treated, whereas Viterbi?s algorithm treats all derivable items.
Thissuggests that Knuth?s algorithm may be more efficient than Viterbi?s.
The worst-casetime complexity of Knuth?s algorithm, however, involves an additional factor because5 Note that some authors let the term ?Viterbi algorithm?
refer to any algorithm that computes the?Viterbi parse,?
that is, the parse with the lowest weight or highest probability.142Computational Linguistics Volume 29, Number 1of the maintenance of the priority queue.
Following Cormen, Leiserson, and Rivest(1990), this factor is O(log(?c(G, w)?
)), where ?c(G, w)?
is the number of nonterminalsin c(G, w), which is an upper bound on the number of elements on the priority queueat any given time.
Furthermore, there are observations by, for example, Chitrao andGrishman (1990), Tjong Kim Sang (1998, Sections 3.1 and 3.4), and van Noord et al(1999, Section 3.9), that suggest that the apparent advantage of Knuth?s algorithm doesnot necessarily lead to significantly lower time costs in practice.In particular, consider deduction systems with items associated with spans like, forexample, that in Figure 1, in which the span of the consequent of an inference rule isthe concatenation of the spans of the antecedents.
If weights of individual productionsin G differ only slightly, as is often the case in practice, then different derivations for anitem have only slightly different weights, and the lowest such weight for a certain itemis roughly proportional to the size of its span.
This suggests that Knuth?s algorithmtreats most items with smaller spans before any item with a larger span is treated, andsince goal items typically have the maximal span, covering the complete input, thereare few derivable items at all that are not treated before any goal item is found.6.
ConclusionsWe have shown how a general weighted parser can be specified in two parts, thefirst being a weighted deduction system, and the second being Knuth?s algorithm (orpossibly Viterbi?s algorithm, where applicable).
Such modular specifications have cleartheoretical and practical advantages over indivisible specifications.
For example, wemay identify common aspects of otherwise seemingly distinct types of parser.
Further-more, modular specifications allow simpler implementations.
We have also identifiedclose connections between our approach to specifying weighted parsers and well-established theory of grammars and parsing.
How the efficiency of Knuth?s algorithmrelates to that of Viterbi?s algorithm in a practical setting is still to be investigated.AcknowledgmentsThe author is supported by the RoyalNetherlands Academy of Arts and Sciences.I am grateful to Gertjan van Noord, GiorgioSatta, Khalil Sima?an, Frederic Tendeau, andthe anonymous referees for valuablecomments.ReferencesAho, Alfred V. and Jeffrey D. Ullman.
1972.Parsing, volume 1 of The Theory of Parsing,Translation and Compiling.
Prentice-Hall,Englewood Cliffs, New Jersey.Backhouse, Roland.
2001.
Fusion onlanguages.
In 10th European Symposium onProgramming, volume 2028 of Lecture Notesin Computer Science, pages 107?121.Springer-Verlag, Berlin, April.Bar-Hillel, Y., M. Perles, and E. Shamir.1964.
On formal properties of simplephrase structure grammars.
InY.
Bar-Hillel, editor, Language andInformation: Selected Essays on Their Theoryand Application.
Addison-Wesley, Reading,Massachusetts, pages 116?150.Billot, Sylvie and Bernard Lang.
1989.
Thestructure of shared forests in ambiguousparsing.
In 27th Annual Meeting of theAssociation for Computational Linguistics,Proceedings of the Conference,pages 143?151, Vancouver, BritishColumbia, Canada, June.Boullier, Pierre.
2000.
Range concatenationgrammars.
In Proceedings of the SixthInternational Workshop on ParsingTechnologies, pages 53?64, Trento, Italy,February.Bouloutas, A., G. W. Hart, and M. Schwartz.1991.
Two extensions of the Viterbialgorithm.
IEEE Transactions on InformationTheory, 37(2):430?436.Chitrao, Mahesh V. and Ralph Grishman.1990.
Statistical parsing of messages.
InSpeech and Natural Language Proceedings,pages 263?266, Hidden Valley,Pennsylvania, June.Cormen, Thomas H., Charles E. Leiserson,and Ronald L. Rivest.
1990.
Introduction toAlgorithms.
MIT Press, Cambridge.Dijkstra, E. W. 1959.
A note on two143Nederhof Weighted Deductive Parsingproblems in connexion with graphs.Numerische Mathematik, 1:269?271.Eisner, Jason.
2000.
Bilexical grammars andtheir cubic-time parsing algorithms.
InH.
Bunt and A. Nijholt, editors, Advancesin Probabilistic and Other ParsingTechnologies.
Kluwer Academic Publishers,Dordrecht, The Netherlands, pages 29?61.Frisch, Alan M. and Peter Haddawy.
1994.Anytime deduction for probabilistic logic.Artificial Intelligence, 69:93?122.Gallo, Giorgio, Giustino Longo, StefanoPallottino, and Sang Nguyen.
1993.Directed hypergraphs and applications.Discrete Applied Mathematics, 42:177?201.Goodman, Joshua.
1999.
Semiring parsing.Computational Linguistics, 25(4):573?605.Jelinek, F., J. D. Lafferty, and R. L. Mercer.1992.
Basic methods of probabilisticcontext free grammars.
In P. Laface andR.
De Mori, editors, Speech Recognition andUnderstanding?Recent Advances, Trends andApplications.
Springer-Verlag, Berlin,pages 345?360.Klein, Dan and Christopher D. Manning.2001.
Parsing and hypergraphs.
InProceedings of the Seventh InternationalWorkshop on Parsing Technologies,pages 123?134, Beijing, October.Knuth, Donald E. 1977.
A generalization ofDijkstra?s algorithm.
Information ProcessingLetters, 6(1):1?5.Lang, Bernard.
1974.
Deterministictechniques for efficient non-deterministicparsers.
In Automata, Languages andProgramming, 2nd Colloquium, volume 14of Lecture Notes in Computer Science,pages 255?269, Saarbru?cken.Springer-Verlag, Berlin.Lyon, Gordon.
1974.
Syntax-directedleast-errors analysis for context-freelanguages: A practical approach.Communications of the ACM, 17(1):3?14.Martelli, Alberto and Ugo Montanari.
1978.Optimizing decision trees throughheuristically guided search.Communications of the ACM,21(12):1025?1039.Schabes, Yves.
1992.
Stochastic lexicalizedtree-adjoining grammars.
In Proceedings ofthe 15th International Conference onComputational Linguistics, volume 2,pages 426?432, Nantes, August.Shieber, Stuart M., Yves Schabes, andFernando C. N. Pereira.
1995.
Principlesand implementation of deductive parsing.Journal of Logic Programming, 24:3?36.Sikkel, Klaas.
1997.
Parsing Schemata.Springer-Verlag, Berlin.Stolcke, Andreas.
1995.
An efficientprobabilistic context-free parsingalgorithm that computes prefixprobabilities.
Computational Linguistics,21(2):167?201.Tendeau, Fre?de?ric.
1997.
Analyse syntaxique etse?mantique avec e?valuation d?attributs dans undemi-anneau.
Ph.D. thesis, University ofOrle?ans.Tjong Kim Sang, Erik F. 1998.
MachineLearning of Phonotactics.
Ph.D. thesis,University of Groningen.van Noord, Gertjan, Gosse Bouma, RobKoeling, and Mark-Jan Nederhof.
1999.Robust grammatical analysis for spokendialogue systems.
Natural LanguageEngineering, 5(1):45?93.Vijay-Shanker, K. and David J. Weir.
1993.The use of shared forests in tree adjoininggrammar parsing.
In Sixth Conference of theEuropean Chapter of the Association forComputational Linguistics, Proceedings of theConference, pages 384?393, Utrecht, TheNetherlands, April.Viterbi, Andrew J.
1967.
Error bounds forconvolutional codes and anasymptotically optimum decodingalgorithm.
IEEE Transactions on InformationTheory, IT-13(2):260?269.
