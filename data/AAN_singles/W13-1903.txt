Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 18?26,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsDiscovering Narrative Containers in Clinical TextTimothy A. Miller1, Steven Bethard2, Dmitriy Dligach1,Sameer Pradhan1, Chen Lin1, and Guergana K. Savova11 Children?s Hospital Informatics Program, Boston Children?s Hospital and Harvard Medical Schoolfirstname.lastname@childrens.harvard.edu2 Center for Computational Language and Education Research, University of Colorado Bouldersteven.bethard@colorado.eduAbstractThe clinical narrative contains a great dealof valuable information that is only under-standable in a temporal context.
Events,time expressions, and temporal relationsconvey information about the time courseof a patient?s clinical record that must beunderstood for many applications of inter-est.
In this paper, we focus on extractinginformation about how time expressionsand events are related by narrative con-tainers.
We use support vector machineswith composite kernels, which allows forintegrating standard feature kernels withtree kernels for representing structuredfeatures such as constituency trees.
Ourexperiments show that using tree kernelsin addition to standard feature kernels im-proves F1 classification for this task.1 IntroductionClinical narratives are a rich source of unstruc-tured information that hold great potential for im-pacting clinical research and clinical care.
Thesenarratives consist of unstructured natural languagedescriptions of various stages of clinical care,which makes them information dense but chal-lenging to use computationally.
Information ex-tracted from these narratives is already being usedfor clinical research tasks such as automatic phe-notype classification for collecting disease cohortsretrospectively (Ananthakrishnan et al 2013),which can in turn be used for a variety of studies,including pharmacogenomics (Lin et al 2012;Wilke et al 2011).
Future applications may useinformation extracted from the clinical narrative atthe point of care to assist physicians in decision-making in a real time fashion.One of the most interesting and challenging as-pects of clinical text is the pervasiveness of tempo-rally grounded information.
This includes a num-ber of clinical concepts which are events with fi-nite time spans (e.g., surgery or x-ray), time ex-pressions (December, postoperatively), and linksthat relate events to times or other events.
For ex-ample, surgery last May relates the time last Maywith the event surgery via the CONTAINS relation,while Vicodin after surgery relates the medicationevent Vicodin with the procedure event surgery viathe AFTER relation.
There are many potential ap-plications of clinical information extraction thatare only possible with an understanding of the or-dering and duration of the events in a clinical en-counter.In this work we focus on extracting a particu-lar temporal relation, CONTAINS, that holds be-tween a time expression and an event expression.This level of representation is based on the compu-tational discourse model of narrative containers(Pustejovsky and Stubbs, 2011), which are timeexpressions or events which are central to a sec-tion of a text, usually manifested by being rela-tive hubs of temporal relation links.
We argue thatcontainment relations are useful as an intermediatelevel of granularity between full temporal relationextraction and ?coarse?
temporal bins (Raghavanet al 2012) like before admission, on admission,and after admission.
Correctly extracting CON-TAINS relations will, for example, allow for moreaccurate placement of events on a timeline, tothe resolution possible by the number of time ex-pressions in the document.
We suspect that thisfiner grained information will also be more usefulfor downstream applications like coreference, forwhich coarse information was found to be useful.The approach we develop is a supervised machine18learning approach in which pairs of time expres-sions and events are classified as CONTAINS ornot.
The specific approach is a support vector ma-chine using both standard feature kernels and treekernels, a novel approach to this problem in thisdomain that has shown promise on other relationextraction tasks.This work makes use of a new corpus we devel-oped as part of the THYME1 project (TemporalHistory of Your Medical Events) focusing on tem-poral events and relations in clinical text.
This cor-pus consists of clinical and pathology notes on col-orectal cancer from Mayo Clinic.
Gold standardannotations include Penn Treebank-style phrasestructure in addition to clinically relevant temporalannotations like clinical events, temporal expres-sions, and various temporal relations.2 Background and Related Work2.1 Annotation MethodologyThe THYME annotation guidelines2 detail the ex-tension of TimeML (Pustejovsky et al 2003b)to the annotations of events, temporal expres-sions and temporal relations in the clinical do-main.
In summary, an EVENT is anything that isrelevant to the clinical timeline.
Temporal expres-sions (TIMEX3s) in the clinical domain are simi-lar to those in the general domain with two excep-tions.
First, TimeML sets and frequencies occurmuch more often in the clinical domain, especiallywith regard to medications and treatments (Clar-itin 30mg twice daily).
The second deviation is anew type of TIMEX3 ?
PREPOSTEXP which coverstemporally complex terms like preoperative, post-operative, and intraoperative.EVENTs and TIMEX3s are ordered on a timelinethrough temporal TLINKs which range from fairlycoarse (the relation to document time creation) tofairly granular (the explicit pairwise TLINKs be-tween EVENTs and/or TIMEX3s).
Of note for thiswork, the CONTAINS relation between a TIMEX3and an EVENT means that the span of the EVENTis completely within the span of the TIMEX3.
Theinterannotator agreement F1-score for CONTAINSfor the set of documents used here was 0.60.2.2 Narrative ContainersOne relatively new concept for marking temporalrelations is that of narrative containers, as in Puste-1http://clear.colorado.edu/TemporalWiki2Annotation guidelines are posted on the THYME wiki.jovsky and Stubbs (2011).
Narrative containersare time spans which are central to the discourseand often subsume multiple events and time ex-pressions.
They are often anchored by a time ex-pression, though more abstract events may also actas anchors.
Using the narrative container frame-work significantly reduces the number of explicitTLINK annotations yet retains a relevant degree ofgranularity enabling inferencing.Consider the following clinical text examplewith DocTime of February 8.The patient recovered well after her ini-tial first surgery on December 16th toremove the adenocarcinoma, althoughon the evening of January 3rd she wasadmitted with a fever and treated withantibiotics.There are three narrative containers in this snip-pet ?
(1) the broad period leading up to the docu-ment creation time which includes the events of re-covered and adenocarcinoma, (2) December 16th,which includes the events of surgery and remove,and (3) January 3rd, which includes the events ofadmitted, fever, and treated.Using only the relation to the document creationtime would provide too coarse of a timeline result-ing in collapsing the three narrative containers (thecoarse time bins of Raghavan et al(2012) wouldcollapse all events into the before admission cat-egory).
On the other hand, marking explicit linksbetween every pair of events and temporal expres-sions would be tedious and redundant.
In this ex-ample, there is no need to explicitly mark that, forinstance, fever was AFTER surgery, because weknow that the fever happened on January 3rd andthat the surgery happened on December 16th, andthat January 3rd is AFTER December 16th.
Withthe grouping of EVENTs in this way, we can inferthe links between them and reduce annotator ef-fort.
Narrative containers strike the right balancebetween parsimony and expressiveness.2.3 Related WorkOf course, the possibility of annotating temporalcontainment relations was allowed by even the ear-liest versions of the TimeML specification usingTLINKs with the relation type INCLUDES.
How-ever, TimeML is a specification not a guideline,and as such, the way in which temporal relationshave been annotated has varied widely and no19corpus has previously been annotated with narra-tive containers in mind.
In the TimeBank corpus(Pustejovsky et al 2003a), annotators annotatedonly a sparse, mostly disconnected graph of thetemporal relations that seemed salient to them.
InTempEval 2007 and 2010 (Verhagen et al 2007;Verhagen et al 2010), annotators annotated onlyrelations in specific constructions ?
e.g.
all pairsof events and times in a sentence ?
and used a re-stricted set of relation types that excluded the IN-CLUDES relation.
TempEval 2013 (UzZaman etal., 2013) allowed INCLUDES relations, but againonly in particular constructions or when the rela-tion seemed salient to the annotators.
The 2012i2b2 Challenge3, which provided TimeML anno-tations on clinical data, annotated the INCLUDESrelation, but merged it with other relations for theevaluation due to low inter-annotator agreement.Since no narrative container-annotated corporaexist, there are also no existing models for extract-ing narrative container relations.
However, wecan draw on the various methods applied to re-lated temporal relation tasks.
Most relevant is thework on linking events to timestamps.
This wasone of the subtasks in TempEval 2007 and 2010,and systems used a variety of features includingwords, part-of-speech tags, and the syntactic pathbetween the event and the time (Bethard and Mar-tin, 2007; Llorens et al 2010).
Syntactic pathfeatures were also used in the 2012 i2b2 Chal-lenge, where they provided gains especially forintra-sentential temporal links (Xu et al 2013).Recent research has also looked to syntac-tic tree kernels for temporal relation extraction.Mirroshandel et al(2009) used a path-enclosedtree (i.e., selecting only the sub-tree containingthe event and time), and used various weightingscheme variants of this approach on the Time-Bank (Pustejovsky et al 2003a) and Opinion4corpora.
Hovy et al(2012) used a flat tree struc-ture for each event-time pair, including only token-based information (words, part of speech tags) be-tween the event and time, and found that addingsuch tree kernels on top of a baseline set of fea-tures improved event-time linking performance onthe TempEval 2007 and Machine Reading cor-pora (Strassel et al 2010).
While Mirroshandel etal.
saw improvements using a representation withsyntactic structure, Hovy et alused the flat tree3http://i2b2.org/NLP/TemporalRelations4Also known as the AQUAINT TimeML corpus ?http://www.timeml.orgstructure because they found that ?using a full-parse syntactic tree as input representation did nothelp performance.?
Thus, it remains an open ques-tion exactly where and when syntactic tree kernelswill help temporal relation extraction.3 MethodsInspired by this prior work, we treat the narrativecontainer extraction task as a within-sentence rela-tion extraction task between time and event men-tions.
For each sentence, this approach iteratesover every gold standard annotated EVENT, pair-ing it with each TIMEX3 in the sentence, and usesa supervised machine learning algorithm to clas-sify each pair as related by the CONTAINS relationor not.
Training examples are generated in thesame way, with pairs corresponding to annotatedlinks marked as positive examples and all othersmarked as negative.
We investigate a variety offeatures for the classifier as well as a variety oftree kernel combinations.This straightforward approach does not addressall relation pairs, setting aside event-event rela-tions and inter-sentential relations, which are bothlikely to require different approaches.3.1 SVM with Tree KernelsThe machine learning approach we use is supportvector machine (SVM) with standard feature ker-nels, tree kernels, and composite kernels that com-bine the two.
SVMs are used extensively for clas-sification tasks in natural language processing, dueto robust performance and widely available soft-ware packages.
We take advantage of the abilityin SVMs to represent structured features such astrees using convolution kernels (Collins and Duffy,2001), also known as tree kernels.
This kernelcomputes similarity between two tree structuresby computing the number of common sub-trees,with a weight parameter to discount the influenceof larger structural similarities.
The specific for-malism we use is sometimes called a subset treekernel (Moschitti, 2006), which checks for simi-larity on subtrees of all sizes, as long as each sub-tree has its production rule completely expanded.A useful property of kernels is that a linear com-bination of two kernels is guaranteed to be a ker-nel (Cristianini and Shawe-Taylor, 2000).
In ad-dition, the product of two kernels is also a ker-nel.
This means that it is simple to combine tradi-tional feature-based kernels used in SVMs (linear,20polynomial, radial basis function) with tree ker-nels representing structural information.
This ap-proach of using composite kernels has been widelyused in the task of relation extraction where syn-tactic information is presumed to be useful, but ishard to represent as traditional numeric features.We investigate a few different composite ker-nels here, including a linear combination:KC(o1, o2) = ?
?KT (t1, t2) +KF (f1, f2) (1)where a composite kernel KC operates on objectsoj composed of features fj and tree tj , by addinga tree kernel KT weighted by ?
to a feature kernelKF .
We also use a composite kernel that takes theproduct of kernels:KC(o1, o2) = KT (t1, t2) ?KF (f1, f2) (2)Sometimes it is beneficial to make use of multi-ple syntactic ?views?
of the same instance.
Belowwe will describe many different tree representa-tions, and the tree kernel framework allows themto all be used simultaneously, by simply summingthe similarities of the different representations andtaking the combined sum as the tree kernel value:KT ({t11, t21 .
.
.
, tN1 }, {t12, t22, .
.
.
, tN2 }) =N?i=1KT (ti1, ti2) (3)where i indexes the N different tree views.
In allkernel combinations we compute the normalizedversion of both the feature and tree kernels so thatthey can be combined on an even footing.The actual implementations we use for train-ing are the SVM-LIGHT-TK package (Mos-chitti, 2006), which is a tree kernel extension toSVMlight (Joachims, 1999).
At test time, weuse the SVM-LIGHT-TK bindings of the ClearTKtoolkit (Ogren et al 2009) in a module built ontop of Apache cTAKES (Savova et al 2010), totake advantage of the pre-processing stages.3.2 Flat FeaturesThe flat features developed for the standard fea-ture kernel include the text of each argument as awhole, the tokens of each argument represented asa bag of words, the first and last word of each ar-gument, and the preceding and following words ofeach argument as bags of words.
The token con-text between arguments is also represented usingthe text span as a whole, the first and last words,the set of words represented as a bag of words, andthe distance between the arguments.
In addition,part of speech (POS) tag features are extracted foreach mention, with separate bag of POS tag fea-tures for each argument.
The POS features aregenerated by the cTAKES POS tagger.We also include semantic features of each argu-ment.
For event mentions, we include a featuremarking the contextual modality, which can takeon the possible values Actual, Hedged, Hypothet-ical, or Generic, which is part of the gold stan-dard annotations.
This feature was included as itwas presumed that actual events are more likelyto have definite time spans, and thus be relatedto times, than hypothetical or generic mentions ofevents.
For time mentions we include a feature forthe time class, with possible values of Date, Time,Duration, Quantifier, Set, or Prepostexp.
The timeclass feature was used as it was hypothesized thatdates and times are more likely to contain eventsthan sets (e.g., once a month).3.3 Tree Kernel RepresentationsWe leverage existing tree kernel representationsfor this work, using some directly and others asstarting point to a domain-specific representation.First, we take advantage of the (relatively) flatstructured tree kernel representations of Hovy etal.
(2012).
This representation uses lexical itemssuch as POS tags rather than constituent struc-ture, but places them into an ordered tree struc-ture, which allows tree kernels to use them as abag of items while also taking advantage of order-ing structure when it is useful.
Figure 1 shows anexample tree for an event-time pair for which a re-lation exists, where the lexical information used isPOS tag information for each term (the represen-tation that Hovy et alfound most useful).
We alsoused a version of this representation where the sur-face form is used instead of the POS tag.While Hovy et alshowed positive results usingthis representation over just standard features, it isstill somewhat constrained in its ability to repre-sent long distance relations.
This is because thesubset tree kernel compares only complete ruleproductions, and with long distance relations a flattree structure will have a production that is too bigto learn.
Alternatively, tree kernel representationscan be based on constituent structure, as is com-mon in the relation extraction literature.
This will21BOPEvent-ActualTOKVBNTOKTOTOKVBTOKNNTimex-DateTOKJJTOKNNBOWEvent-ActualTOKscheduledTOKtoTOKundergoTOKsurgeryTimex-DateTOKnextTOKweekFigure 1: Two trees indicating the flat tree kernelrepresentation.
Above is the bag of POS tags ver-sion; below is the bag of words version.hopefully allow for the representation of longerdistance relations by taking advantage of syntacticsub-structure with smaller productions.
The rep-resentations used here are known as Feature Trees(FT), Path Trees (PT) and Path-Enclosed Trees(PET).The Feature Tree representation takes the en-tire syntax tree for the sentence containing botharguments and inserts semantic information aboutthose arguments.
That information includes the ar-gument type (EVENT or TIMEX) as an additionaltree node above the constituent enclosing the argu-ment.
We also append semantic class informationto the argument (contextual modality for events,time class for times), as in the flat features.The Feature Tree representation is not com-monly used, as it includes an entire sentencearound the arguments of interest, and that may in-clude a great deal of unrelated structure that addsnoise to the classifier.
Here we include it in an at-tempt to get to the root of an apparent discrepancyin the tree kernel literature, as explained in Sec-tion 2, in which Hovy et al(2012) report a nega-tive result and Mirroshandel et al(2009) report apositive result for using constituency structure intree kernels for temporal relation extraction.The Path Tree representation uses a sub-tree ofthe whole constituent tree, but removes all nodesthat are not along the path between the two argu-ments.
Path information has been used in standardfeature kernels (Pradhan et al 2008), with eachindividual path being a possible boolean feature.VPArg1-Event-Actualarg1SVPVPArg2-Timex-Datearg2Figure 2: Path Tree (PT) representationAnother representation making use of the path treetakes contiguous subsections of the path tree, or?path n-grams,?
in an attempt to combat the spar-sity of using the whole path (Zheng et al 2012).By using the path representation with a tree ker-nel, the model should get the benefit of all differentsizes of path n-grams, up to the size of the wholepath.
This representation is augmented by addingin argument nodes with event and time features, asin the Feature Tree.
Unlike the Feature Tree andthe PET below, the Path Tree representation doesnot include word nodes, because the important as-pect of this representation is the labels of the nodeson the path between arguments.
Figure 2 shows anexample of what this representation looks like.The Path-Enclosed Tree representation is basedon the smallest sub-tree that encloses the two pro-posed arguments.
This is a representation that hasshown value in other work using tree kernels forrelation extraction (Zhang et al 2006; Mirroshan-del et al 2009).
The information contained inthe PET representation is a superset of that con-tained in the Path Tree representation, since it in-cludes the full path between arguments as wellas the structure between arguments and the ar-gument text.
This means that it can take intoaccount path information while also consideringconstituent structure between arguments that mayplay a role in determining whether the two ar-guments are related.
For example, temporal cuewords like after or during may occur between ar-guments and will not be captured by Path Trees.Like the PT representation, the PET representa-tion is augmented with the semantic informationspecified above in the Feature Tree representation.Figure 3 shows an example of this representation.22VPArg1-Event-ActualVBNscheduledVPTOtoVPVBundergoNPNNsurgeryArg2-Timex-DateNPJJnextNNweekFigure 3: Path-Enclosed Tree representation4 EvaluationThe corpus we used for evaluations was describedin Section 2.
There are 78 total notes in the corpus,with three notes for each of 26 patients.
The datais split into training (50%), development (25%),and test (25%) sections based on patient number,so that each patient?s notes are all in the samesection.
The combined training and developmentset used for final training consists of 4378 sen-tences with 49,050 tokens, and 7372 events, 849time expressions, and 2287 CONTAINS relations.There were 774 positive instances of CONTAINSin the training data, with 1513 negative instances.For constituent structure and features we use thegold standard treebank and event and time featuresfrom our corpus.
Preliminary work suggests thatautomatic parses from cTAKES do not harm per-formance very much, but the focus of this work ison the relation extraction so we use gold standardparses.
All preliminary experiments were done us-ing the development set for testing.We designed a set of experiments to exam-ine several hypotheses regarding extraction of theCONTAINS relation and the efficacy of differenttree kernel representations.
The first two config-urations test simple rule-based baseline systems,CLOSEST-P and CLOSEST-R, for distance-relateddecision rule systems meant to optimize precisionand recall, respectively.
CLOSEST-P hypothesizesa CONTAINS link between every TIMEX3 and theclosest annotated EVENT, which will make fewlinks overall.
CLOSEST-R hypothesizes a CON-TAINS link between every EVENT and the closestTIMEX3, which will make many more links.The next configuration, Flat Features, uses thetoken and part of speech features along with ar-gument semantics features, as described in Sec-tion 3.
While this feature set may not seem ex-haustive, in preliminary work many traditional re-lation extraction features were tried and found tonot have much effect.
This particular configura-tion was tested because it is most comparable tothe bag of word and bag of POS kernels fromHovy et al(2012), and should help show whetherthe tree kernel is providing anything over an equiv-alent set of basic features.We then examine several composite kernels, allusing the same feature kernel, but using differenttree kernel-based representations.
First, we use acomposite kernel which uses the bag of word andbag of POS tree views, as in Hovy et al(2012).Next, we add in two additional tree views to thetree kernel, Path-Enclosed Tree and Path Tree,which are intended to examine the effect of usingtraditional syntax, and the long distance featuresthat they enable.
The final experimental config-uration replaces the PET and PT representationsfrom the last configuration with the Feature Treerepresentation.
This tests the hypothesis that thedifference between positive results for tree kernelsin this task (as in, say, Mirroshandel et al(2009))and negative results reported by Hovy et al(2012)is the difference between using a full-parse treeand using standard sub-tree representations.For the rule-based systems, there are no param-eters to tune.
Our machine-learning systems arebased on support vector machines (SVM), whichrequire tuning of several parameters, includingkernel type (linear, polynomial, and radial basisfunction), the parameters for each kernel, and c,the cost of misclassification.
Tree kernels intro-duce an additional parameter ?
for weighting largestructures, and the use of a composite kernel in-troduces parameters for which kernel combinationoperator to use, and how to weight the differentkernels for the sum operator.For each machine learning configuration, weperformed a large grid search over the combinedparameter space, where we trained on the train-ing set and tested on the development set.
Forthe final experiments, the parameters were chosenthat optimized the F1 score on the developmentset.
Qualitatively, the parameter tuning stronglyfavored configurations which combined the ker-nels using the sum operator, and recall and pre-cision were strongly correlated with the SVM pa-rameter c. Using these parameters, we then trained23on the combined training and development setsand tested on the official test set.4.1 Evaluation MetricsThe state of evaluating temporal relations has beenevolving over the past decade.
This is partiallydue to the inferential properties of temporal rela-tions, because it is possible to define the same setof relations using different set of axioms.
To takea very simple example, given a gold set of rela-tions A<B and B<C, and given the system outputA<B, A<C and B<C, if one were to compute aplain precision/recall metric, then the axiom A<Cwould be counted against the system, when onecan easily infer from the gold set of relations thatit is indeed correct.
With more relations the infer-ence process becomes more complex.Recently there has been some work tryingto address the shortcomings of the plain F1score (Muller and Tannier, 2004; Setzer et al2006; UzZaman and Allen, 2011; Tannier andMuller, 2008; Tannier and Muller, 2011).
How-ever, the community has not yet come to a consen-sus on the best evaluation approach.
Two recentevaluations, TempEval-3 (UzZaman et al 2013)and the 2012 i2b2 Challenge (Sun et al 2013),used an implementation of the proposal by (Uz-Zaman and Allen, 2011).
However, as describedin Cherry et al(2013), this algorithm, which usesa greedy graph minimization approach, is sensi-tive to the order in which the temporal relationsare presented to the scorer.
In addition, the scoreris not able to give credit for non-redundant, non-minimum links (Cherry et al 2013) as with thethe case of the relation A<C mentioned earlier.Considering that the measures for evaluatingtemporal relations are still evolving, we decided touse plain F-score, with recall and precision scoresalso reported.
This score is computed across allintra-sentential EVENT-TIMEX3 pairs in the goldstandard, where precision = # correct predictions# predictions ,recall = # correct predictions# gold standard relations , and F1 score =2?precision?recallprecision+recall .4.2 Experimental ResultsResults are shown in Table 1.
Rule-based base-lines perform reasonably well, but are heavily bi-ased in terms of precision or recall.
The ma-chine learning baseline cannot even obtain thesame performance as the CLOSEST-R rule-basedsystem, though it is more balanced in terms of pre-System Precision Recall F1CLOSEST-P 0.754 0.537 0.627CLOSEST-R 0.502 0.947 0.656Flat Features (FF) 0.705 0.593 0.645FF+Bag Trees (BT) 0.649 0.728 0.686FF+BT+PET+PT 0.770 0.707 0.737FF+BT+FT 0.691 0.691 0.691Table 1: Table of results of main experiments.cision and recall.
Using a composite kernel whichadds in the flat token-based tree kernels improvesperformance over the standard feature kernel by4.1 points.
Adding in the Path Tree and Path-Enclosed Tree constituency-based trees along withthe flat trees improves F1 score to our best resultof 73.7.
Finally, replacing PT and PET representa-tions with the Feature Tree representation does notoffer any performance improvement over the FlatFeatures + Bag Trees configuration.4.3 Error AnalysisWe performed error analysis on the outputs of thebest-performing system (FF+BT+PET+PT in Ta-ble 1).
First, we note that the parameter searchwas optimized for F1.
This resulted in the highest-scoring configuration using a composite kernelwith the sum operator, polynomial kernel for thesecondary kernel, ?
= 0.5, tree kernel weight (T )of 0.1, and c = 10.0.
This high value of c and lowvalue of T results in higher precision and lowerrecall, but there were configurations with lower cand higher T which made the opposite tradeoff,with only marginally worse F1-score.
For the pur-poses of error analysis, however, this configurationleads to a focus on false negatives.First, the false positives contained many rela-tions that were legitimately ambiguous or possibleannotator errors.
An example ambiguous case isShe is currently being treated on the Surgical Ser-vice for..., in which the system generates the re-lation CONTAINS(currently, treated), but the goldstandard labels as OVERLAP.
This example is am-biguous because it is not clear from just the lin-guistic context whether the treatment is whollycontained in the small time window denoted bycurrently, or whether it started a while ago or willcontinue into the future.
There are many similarcases where the event is a disease/disorder type,and the specific nature of the disease is impor-tant to understanding whether this is a CONTAINS24or OVERLAP relation, specifically understandingwhether the disease is chronic or more acute.Another source of false positives were wherethe event and time were clearly related, but notwith CONTAINS.
In the example reports that shehas been having intermittent bleeding since Mayof 1998, the term since clearly indicates that thisis a BEGINS-ON relation between bleeding andMay of 1998.
This is a case where having othertemporal relation classifiers may be useful, as theycan compete and the relation can be assigned towhichever classifier is more confident.False negatives frequently occurred in contextswhere the event and time were far apart.
Syn-tactic tree kernels were introduced to help im-prove recall on longer-distance relations, and weresuccessful up to a limit.
However, certain ex-amples are so far apart that the algorithm mayhave had difficulty sorting noise from importantstructure.
For example, the system did not findthe CONTAINS(October 27, 2010, oophorectomy)relation in the sentence:October 27, 2010, Dr. XXX performedexploratory laparotomy with an trans-verse colectomy and Dr. YYY performeda total abdominal hysterectomy with abilateral salpingo-oophorectomy.Here, while the date may be part of the same sen-tence as the event, the syntactic relation betweenthe pair is not what makes the relation; the date isacting as a kind of discourse marker that indicatesthat the following events are contained.
This sug-gests that discourse-level features may be usefuleven for the intra-sentential classification task.Other false negatives occurred where there wassyntactic complexity, even on shorter examples.The subset tree kernel used here matches com-plete rule productions, and across complex struc-ture with large productions, the chances of findingsimilarity decreases substantially.
Thus, eventswithin coordination or separated from the time byclause breaks are more difficult to relate to thetime due to the multiple different ways of relatingthese different syntactic elements.Finally, there are some examples where the an-chor of a narrative container is an event with mul-tiple sub-events.
In these cases, the system per-forms well at relating a time expression to the an-chor event, but may miss the sub-events that arefarther away.
This is a case where having an event-event TLINK classifier, then applying determinis-tic closure rules, would allow a combined systemto link the sub-events to the time expression.5 Discussion and ConclusionIn this paper we have developed a system for auto-matically identifying CONTAINS relations in clini-cal text.
The experiments show first that a machinelearning approach that intelligently integrates con-stituency information can greatly improve perfor-mance over rule-based baselines.
We also showthat the tree kernel approach, which can model se-quence better than a bag of tokens-style approach,is beneficial even when it uses the same features.Finally, the experiments show that choosing thecorrect representation is important for tree kernelapproaches, and specifically that using a full parsetree may give inferior performance compared tosub-trees focused on the structure of interest.In general, there is much work to be done in thearea of representing temporal information in clin-ical records.
Many of the inputs to the algorithmdescribed in this paper need to be extracted auto-matically, including time expressions and events.Work on relations will focus on adding featuresto represent discourse information and richer rep-resentation of event semantics.
Discourse infor-mation may help with the longer-distance errors,where the time expression acts almost as a topicfor an extended description of events.
Better un-derstanding of event semantics, such as whethera disease is chronic or acute, or typical durationfor a treatment, may help constrain relations.
Inaddition, we will explore the effectiveness of us-ing dependency tree structure, which has been use-ful in the domain of extracting relations from thebiomedical literature (Tikk et al 2013).AcknowledgementsThe work described was supported by Tempo-ral History of Your Medical Events (THYME)NLM R01LM010090 and Integrating Informat-ics and Biology to the Bedside (i2b2) NCBOU54LM008748.
Thanks to the anonymous re-viewers for thorough and insightful comments.ReferencesNaushad UzZaman, Hector Llorens, et al2013.
Semeval-2013 task 1: Tempeval-3: Evaluating time expressions,events, and temporal relations.
In Second Joint Confer-ence on Lexical and Computational Semantics (*SEM),25Volume 2: Proceedings of the Seventh International Work-shop on Semantic Evaluation (SemEval 2013), pages 1?9,Atlanta, Georgia, USA, June.
Association for Computa-tional Linguistics.Ashwin N Ananthakrishnan, Tianxi Cai, et al2013.
Improv-ing case definition of Crohn?s disease and ulcerative col-itis in electronic medical records using natural languageprocessing: a novel informatics approach.
Inflammatorybowel diseases.Steven Bethard and James H. Martin.
2007.
CU-TMP: Tem-poral relation classification using syntactic and semanticfeatures.
In Proceedings of the 4th International Work-shop on Semantic Evaluations (SemEval-2007), pages129?132.Colin Cherry, Xiaodan Zhu, et al2013.
A la recherche dutemps perdu: extracting temporal relations from medicaltext in the 2012 i2b2 NLP challenge.
Journal of the Amer-ican Medical Informatics Association, March.Michael Collins and Nigel Duffy.
2001.
Convolution kernelsfor natural language.
In Neural Information ProcessingSystems.Nello Cristianini and John Shawe-Taylor.
2000.
An Introduc-tion to Support Vector Machines and Other Kernel-basedLearning Methods.
Cambridge University Press.Dirk Hovy, James Fan, et al2012.
When did that happen?
:linking events and relations to timestamps.
In Proceedingsof the 13th Conference of the European Chapter of the As-sociation for Computational Linguistics, pages 185?193.Association for Computational Linguistics.Thorsten Joachims.
1999.
Making large scale svm learningpractical.
In B. Schlkopf, C. Burges, and A. Smola, edi-tors, Advances in Kernel Methods - Support Vector Learn-ing.
Universita?t Dortmund.Chen Lin, Helena Canhao, et al2012.
Feature engineeringand selection for rheumatoid arthritis disease activity clas-sification using electronic medical records.
In Proceed-ings of ICML Workshop on Machine Learning for ClinicalData.Hector Llorens, Estela Saquete, and Borja Navarro.
2010.TIPSem (english and spanish): Evaluating crfs and seman-tic roles in tempeval-2.
In Proceedings of the 5th Interna-tional Workshop on Semantic Evaluation, pages 284?291.Association for Computational Linguistics.Seyed Abolghasem Mirroshandel, M Khayyamian, andGR Ghassem-Sani.
2009.
Using tree kernels for clas-sifying temporal relations between events.
Proc.
of thePACLIC23, pages 355?364.Alessandro Moschitti.
2006.
Efficient convolution kernelsfor dependency and constituent syntactic trees.
In Ma-chine Learning: ECML 2006, pages 318?329.
Springer.Philippe Muller and Xavier Tannier.
2004.
Annotating andmeasuring temporal relations in texts.
In Proceedings ofthe 20th international conference on Computational Lin-guistics, COLING ?04, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Philip V. Ogren, Philipp G. Wetzler, and Steven J. Bethard.2009.
ClearTK: a framework for statistical natural lan-guage processing.
In Unstructured Information Manage-ment Architecture Workshop at the Conference of the Ger-man Society for Computational Linguistics and LanguageTechnology, 9.Sameer S Pradhan, Wayne Ward, and James H Martin.
2008.Towards robust semantic role labeling.
ComputationalLinguistics, 34(2):289?310.James Pustejovsky and Amber Stubbs.
2011.
Increasing in-formativeness in temporal annotation.
In Proceedings ofthe 5th Linguistic Annotation Workshop, pages 152?160.James Pustejovsky, Patrick Hanks, et al2003a.
The time-bank corpus.
In Corpus linguistics, volume 2003, page 40.James Pustejovsky, Jose?
Casta no, et al2003b.
Timeml:Robust specification of event and temporal expressions intext.
In Fifth International Workshop on ComputationalSemantics (IWCS-5).Preethi Raghavan, Eric Fosler-Lussier, and Albert M Lai.2012.
Temporal classification of medical events.
In Pro-ceedings of the 2012 Workshop on Biomedical NaturalLanguage Processing, pages 29?37.
Association for Com-putational Linguistics.Guergana K. Savova, James J. Masanz, et al2010.
Mayoclinical text analysis and knowledge extraction system(cTAKES): architecture, component evaluation and appli-cations.
J Am Med Inform Assoc, 17(5):507?513.Andrea Setzer, Robert Gaizauskas, and Mark Hepple.
2006.The role of inference in the temporal annotation and anal-ysis of text.
Language Resources and Evaluation, 39(2-3):243?265, February.Stephanie Strassel, Dan Adams, et al2010.
The DARPAmachine reading program - encouraging linguistic and rea-soning research with a series of reading tasks.
In Proceed-ings of the Seventh International Conference on LanguageResources and Evaluation (LREC?10), Valletta, Malta.Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner.
2013.Evaluating temporal relations in clinical text: 2012 i2b2challenge.
Journal of the American Medical InformaticsAssociation, April.Xavier Tannier and Philippe Muller.
2008.
Evaluation met-rics for automatic temporal annotation of texts.
Proceed-ings of the Proceedings of the Sixth International Lan-guage Resources and Evaluation (LREC?08).Xavier Tannier and Philippe Muller.
2011.
Evaluating tem-poral graphs built from texts via transitive reduction.
J.Artif.
Int.
Res., 40(1):375413, January.Domonkos Tikk, Ille?s Solt, et al2013.
A detailed error anal-ysis of 13 kernel methods for protein-protein interactionextraction.
BMC bioinformatics, 14(1):12.Naushad UzZaman and James Allen.
2011.
Temporal eval-uation.
In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: Human Lan-guage Technologies, page 351?356.Marc Verhagen, Robert Gaizauskas, et al2007.
Semeval-2007 task 15: Tempeval temporal relation identification.In Proceedings of the Fourth International Workshop onSemantic Evaluations (SemEval-2007), pages 75?80.Marc Verhagen, Roser Sauri, et al2010.
Semeval-2010 task13: Tempeval-2.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluation, pages 57?62, Uppsala,Sweden, July.
Association for Computational Linguistics.RA Wilke, H Xu, et al2011.
The emerging role of electronicmedical records in pharmacogenomics.
Clinical Pharma-cology & Therapeutics, 89(3):379?386.Yan Xu, Yining Wang, et al2013.
An end-to-end system toidentify temporal relation in discharge summaries: 2012i2b2 challenge.
Journal of the American Medical Infor-matics Association : JAMIA.Min Zhang, Jie Zhang, and Jian Su.
2006.
Exploring syn-tactic features for relation extraction using a convolutiontree kernel.
In Proceedings of the Human Language Tech-nology Conference of the North American Chapter of theACL, pages 288?295.Jiaping Zheng, Wendy W Chapman, et al2012.
A sys-tem for coreference resolution for the clinical narrative.Journal of the American Medical Informatics Association,19:660?667.26
