Trajectory Based Word Sense DisambiguationXiaojie Wang ??
Yuji Matsumoto ?
?Graduate School of Information Science, Nara Institute of Science and Technology8916-5 Takayama, Ikoma, Nara, 630-0192, Japan?School of Information Engineering, Beijing University of Posts and TechnologyBeijing, 100876, China{xiaoji-w, matsu}@is.naist.jpAbstractClassifier combination is a promising wayto improve performance of word sensedisambiguation.
We propose a newcombinational method in this paper.
We firstconstruct a series of Na?ve Bayesianclassifiers along a sequence of orderlyvarying sized windows of context, andperform sense selection for both trainingsamples and test samples using theseclassifiers.
We thus get a sense selectiontrajectory along the sequence of contextwindows for each sample.
Then we make useof these trajectories to make finalk-nearest-neighbors-based sense selection fortest samples.
This method aims to lower theuncertainty brought by classifiers usingdifferent context windows and make morerobust utilization of context while performwell.
Experiments show that our approachoutperforms some other algorithms on bothrobustness and performance.1  IntroductionWord sense disambiguation (WSD) has longbeen a central issue in Natural LanguageProcessing (NLP).
In many NLP tasks, such asMachine Translation, Information Retrieval etc.,WSD plays a very important role in improvingthe quality of systems.
Many different algorithmshave been used for this task, including somemachine learning (ML) algorithms, such asNa?ve Bayesian model, decision trees, andexample based learners.
Since differentalgorithms have different strengths and performwell on different feature space, classifiercombination is a reasonable candidate to achievebetter performance by taking advantages ofdifferent approaches.
In the field of ML,ensembles of classifiers have been shown to besuccessful in last decade (Dietterich 1997).
For thespecific task of WSD, classifier combination hasbeen received more and more attention in recentyears.Kilgarriff and Rosenzweig (2000) presentedthe first empirical study.
They combined theoutput of the participating SENSEVAL1 systemsvia simple voting.
Pedersen (2000) built anensemble of Na?ve Bayesian classifiers, each ofwhich is based on lexical features that representco-occurring words in varying sized windows ofcontext.
The sense that receives majority of thevotes was assigned as the final selection.Stevenson and Wilks (2001) presented aclassifier combination framework where threedifferent disambiguation modules werecombined using a memory-based approach.Hoste et al (2002) used word experts consistedof four memory-based learners trained ondifferent context.
Output of the word experts isbased on majority voting or weighted voting.Florian et al(2002) and Florian and Yarowsky(2002) used six different classifiers ascomponents of their combination.
Theycompared several different strategies ofcombination, which include combining theposterior distribution, combination based onorder statistics and several different voting.Klein et al (2002) combined a number ofdifferent first-order classifiers using majorityvoting, weighted voting and maximum entropy.In Park (2003), a committee of classifiers wasused to learn from the unlabeled examples.
Thelabel of an unlabeled example is predicted byweighted majority voting.
Frank at al.
(2003)presented a locally weighted Na?ve Bayesianmodel.
For a given test instance, they first chosek-nearest-neighbors from training samples for it,then constructed a Na?ve Bayesian classifier byusing these k-nearest-neighbors in stead of alltraining samples.This paper presents a new combinationalapproach.
We firstly construct a series of Na?veBayesian classifiers along a sequence of orderlyvarying sized windows of context, and makesense selection for both training samples and testsamples using these classifiers.
We thus get atrajectory of sense selection for each sample, andthen use the sense trajectory basedk-nearest-neighbors to make final decision fortest samples.This method is motivated by an observationthat there is an unavoidable uncertainty when aclassifier is used to make sense selection.
Ourapproach aims to alleviate this uncertainty andthus make more robust utilization of contextwhile perform well.
Experiments show ourapproach outperform some other algorithms onboth robustness and performance.The remainder of this paper is organized asfollows: Section 2 gives the motivation of ourapproach, describes the uncertainty in senseselection brought by classifiers themselves.
Insection 3, we present the decision trajectorybased approach.
We then implement someexperiments in section 4, and give someevaluations and discussions in section 5.
Finally,we draw some conclusions.2  The Trajectory of Sense SelectionOur method is originally motivated by anobservation on relation between sense selectionby a classifier and the context it uses to make thisselection.As well known, context is the only means toidentify the sense of a polysemous word.
Ide(1998) identified three types of context:micro-context, topical context and domain.
Inpractice, a context window ( l , r ), whichincludes l  words to the left and r  words tothe right of the target word, is predetermined byhuman or chosen automatically by a performancecriterion.
Only information in the contextwindow is then used for classifiers anddisambiguating.
What is the best window size forWSD has been long for a problem.
Weaver (1955)hoped we could find a minimum value of thewindow size which can lead to the correct choiceof sense for the target ambiguous word.Yarowsky (1994) argued the optimal value issensitive to the type of ambiguity.
Semantic ortopic-based ambiguities warrant a larger window(from 20 to 50), while more local syntacticambiguities warrant a smaller window (3 or 4).Leacock at el (1998) showed the local context issuperior to topical context as an indicator ofword sense.
Yarowsky (2002) suggested thatdifferent algorithms prefer different windowsizes.Followed by these works, it is clear thatdifferent window sizes might cause differentsense selection for an occurrence of the targetword even when a same algorithm is used.Yarowsky (2002) gave a investigation on howthe performance changes with different windowsizes for several different algorithms and severaldifferent types of word.
In fact, even for human,different window sizes might cause differentsense selections for a same occurrence of anambiguous word.
For example, considering word???
(It has two different senses: ?read?
and?think?)
in senesce S1.S1: ?/  ?/ ?/?/  ?/   ?
?/   ?/  ?/.
(I) (think) (this) (book) (worthy)  (a) (read)When we use a context window (1,1), it is notclear which sense should be more possible in thissentence.
When we use (3,3), because thecollocation with ?
give a very strongindication for ?
?s sense, it is natural that weselect the sense of ?read?
for ?.
When we usewindow (6,6), we select the sense of ?think?
forit.Here, the occurrence of the ambiguous word isthe same; it is the difference of context windowsthat make the sense selection different.
Since thecontext window is a built-in parameter of aclassifier, as long as we use a classifier todistinguish an ambiguous word, we had tochoose a window size.
Supposing a classifier isan observer, choosing a window size is necessaryfor the observer to implement an observation.Different choices of the window size might causedifferent observational results for the sameoccurrence.
That means there is an uncertaintybrought by observer itself.
It reminds us that therelation between the window size and the senseselection is to some extent similar with therelation between a particle?s position and itsmomentum in Heisenberg Uncertainty Principle.By the Uncertainty Principle, when wemeasure the position and the momentum of aparticle, we cannot measure them withzero-variance simultaneously.
In QuantumTheory, the wave-function is used to describe thestate of a particle.
The method to deal with thisproblem in Quantum Theory suggests us an ideato deal with the similar problem in WSD.Firstly, since the existence of the uncertaintyof sense selection at different window sizes,sense selection for the target word at only onecontext window cannot give a completedescription of its sense.
To grasp a completedescription of its sense, it is necessary to getsense selections along a series of observation, i.e.using a sequence of context window to get atrajectory of sense selection.Secondly, unlike that in Uncertainty Principle,the intuition is that, in most of time, when wehave enough observations, we can be doubtlesslysure the sense of the target word.
So, we makefinal unique sense selection based on thetrajectory of sense selection.
Since the finalselection is based on a sense trajectory alongdifferent window sizes, we thus think it mayhelpful to alleviate the uncertainty brought bydifference of context windows.In this way, our approach aims to improverobustness of WSD.
Here the robustness meansthat sense selection is not sensitive to thewindow size.
This kind of robustness isespecially important to WSD system in noise ororal corpus, where there are many occasionalinserted words near the target word.
Besidesrobustness, to achieve better performance is alsonecessary, if robustness is at a low level ofperformance, it is useless.3  Decision Trajectory Based WSDIn our approach, we firstly use Na?ve Bayesian(NB) algorithm to construct a sense selectiontrajectory along a sequence of orderly varyingsized windows of context for each sample,including both training samples and test samples.Then we use k-nearest-neighbors(KNN)algorithm to make final decision for each testsample based on these trajectories.Let w  be an ambiguous word, it has ndifferent senses, 1s ?
is ?
ns .
Supposing wehave q  training samples 1S ?
jS ?
qS , whereiq  samples are tagged with sense is ,qqi =?
.
We present our approach in twostages: training stage and test stage.
Figure 1gives a skeleton of the algorithm.In the training stage, we first choose asequence of context windows.mT : ( 1p ,?
kp ... mp )Where kp =( kl , kr ) is a context windowwhich includes kl  words to the left of word wand kr  words to the right.
We call mT  atrajectory of context windows, kp  is a windowpoint in this trajectory.
For example, a trajectory((1,1), (3,3), (5,5), (7,7), (9,9)) includes 5 points.For each window point kp  in mT , weconstruct a classier by using NB algorithm basedon context word in kp .
Let )( kpC  denote theclassifier, it can be thought as an operator that makesense selection upon samples.
With the change ofthe window point, we can get a operate vector:))(),...,(),...,(( 1 mk pCpCpCC =Training stage:1.
To construct a operator vector C  along asequence of context windows Tm :))(),...,(),...,(( 1 mk pCpCpCC = .
)( kpC  is a NB classifier learned by all thetagged data using kp  as the contextwindow.2.
For each training sample jS , to operate Cupon it to construct a sense trajectory, j?
( qj ,...,1= ).Test stage:1.
For a new sample S , to construct itsdecision trajectory ?
by operating Cupon it.2.
For qj ,...,1= , to calculate ),( jd ??3.
to make KNN-based sense choice for S .Figure 1.
The algorithm of trajectory-based WSDFor a sample jS  for sense is , we use)( kpC ( jS ) to denote using )( kpC  to classifyjS , we can get a sense selection denoted by)( kj p?
, i.e., )( kpC ( jS )= )( kj p?
.
We call)( kj p?
a point decision.
If )( kj p?
= is , weborrow a term to call jS  an eigen-sample of theoperator )( kpC , is  is its eigenvaluve.With the change of the window point, we get asequence of point decisions for sample jSalong the window trajectory mT , we denoted itbyj?
=( )( 1pj?
,?, )( mj p?
)We call it decision trajectory of sample jSalong the context windows trajectory mT .
If allelements of j?
is is , i.e.
j?
=( is ,?, is ), wecall jS  an eigen-sample of operator C , j?
isa eigen-trajectory of C .In this way, we transfer training samples intotraining decision trajectories, which will be usedas instance for final KNN-based sense selection.An eigen-trajectory is a good indication for asample, but when all the training samples areeigen-samples, it is not a good thing fordisambiguating new samples.
We will discussthis case in section 5.2.After finishing training stage of our approach,we have a context windows trajectory mT , asequence of classifiers )( kpC  along mT ?
anda decision trajectory for each training sample.
Allthese compose of our classifier for a new samplein test.
When a new sample is given, we firstcalculate a decision trajectory ?
for it by usingC  operating upon it.
Let?
=( )( 1p?
,?, )( mp?
)We then calculate the similarity between ?
andj?
, kj ,...2,1=  by using (3.1).mppSimmiijij?== 1))(),((),(?????
(3.1)Where 1),( =yx?
at yx = , and 0),( =yx?at yx ?
.
We then choose h  training decisiontrajectory samples as ?
?s h  nearest neighbors,supposing that ih  samples are tagged withsense is  among these nearest neighbors,hhi =?
, ii qh ?
, then by solving (3.2), Wechoose ?is  as the final sense selection for thenew sample.??????
=ihjjniSimi11),(maxarg ??
(3.2)If all training samples are eign-samples, thesimilarity between ?
and j?
for the samesense are the same, (3.2) is changed to:},...,1|,})({{|maxarg1mkipi kni===???
?
(3.3)Then the final KNN-based decision is simplifiedto majority voting along the decision trajectoryof the new sample.4  Experiments4.1 Experimental DataAll experimental data were extracted fromChinese People Daily from January 1995 toDecember 1996.
Eight Chinese ambiguous wordswere used in the experiments as listed in the firstcolumn of Table1.
In the Second column, wegive some information about samples used inexperiments.
The number before each bracket isthe number of senses.
Numbers in each bracketare amounts of samples used for each sense.They were annotated by a Chinese native speaker,and checked by another native speaker.
Somesamples without inter-agreement between twonative speakers had been excluded.Only word co-occurrences in given windowsare used as features through all experiments inthis paper.4.2  Experimental  MethodIn order to do a comparative study, we haveimplemented not only our algorithm, but alsofour other related algorithms in our experiments.They fall into two classes.
NB (Manning andSchutze 1999) and KNN (Ng and Lee 1996) aretwo components of our approach.
Locallyweighted NB(LWNB, Frank et al 2003) andEnsemble NB(ENB Pedersen 2000) are twocombinational approaches.
Since our aim is tocompare not only the performance but also therobustness of these algorithms, we implementedeach algorithm in following way.We note our approach TB_KNN when (3.2) isused for final decision, and TB_VOTE when (3.3)is used for final decision.We firstly constructed a sequence of contextwindows kp =( kl , kr ) 40,...,1=k  infollowing way:1.
Initiate: 1,0 11 == rl2.
Generate next window:??
?<=+==+==++++kkkkkkkkkkkkrlifrrllrlifrrll1111,11,39,...,1=kWe then constructed a sequence of windowtrajectories.
),...,( 1 ii ppT =    40,...,1=iWe implemented TB_KNN and TB_VOTE oneach trajectory from 1T  to 40T .Obviously, our iT -based sense selection andip -based selection in fact make use of samecontext surrounding the target word.
ip  is thebiggest window along iT .
We implemented NBclassifiers (noted by P) from 1p  to 40p .KNN was implemented along the samesequence of context window, from 1p  to 40p .For the implementation of algorithm LWNB,we used the measure in (Ng and Lee 1996) to find knearest neighbors for each sample, and thenconstructed a NB classifier according to(Frank2003).
This algorithm was also implementedfor each context window along the sequence from1p  to 40p .ENB was implemented according to (Petersen2000).
Different left and right window sizes weused is (1,2,3,4,5,6,10,15,20).
Since oneimplementation of this algorithm make use of allthese different window sizes.
It cannot beimplemented along above windows sequence, sothere is only one implementation for thisalgorithm.For each ambiguous word, we implementedabove experiments respectively, each experimentwas a 10-fold cross-validation, at each time 90%of the data were used as training data, 5% wereused as development data, and other 5% wereused as test data.4.3 Experimental ResultsWe give the results curves for word ???
inFigure 2 and for word ???
in Figure 3.
In bothfigures, x-axis is the context window, from (0,1)to (20,20), y-axis is F-measure, and differentmarker style is for different algorithms.
Resultscurves for other six target words have similarshapes.We list a summary of results for all 8 words inTable 1.
In TB_KNN column, there are three values:mean, maximum and standard variance ofF-measure of 40 different trajectories from 1T  to40T .
Results are summarized in the same way incolumn TB_VOTE.
For column P, KNN andLWNB, three values are mean, maximum andstandard variance of F-measure of 40 differentpoints from 1p  to 40p .
In column ENB, there isonly one F-measure.5  Evaluation5.1 Comparison with other algorithmsAs we have mentioned, we compare results ofeach algorithm on both performance androbustness.
Performance can be compareddirectly from F-measure point-wise along asequence of context windows(or trajectories).
Wealso use mean and maximum (max) along thesequence to give an overall comparison.Robustness of an algorithm means that sensedecision varies gracefully with the change ofcontext windows (or trajectories) it uses.Intuitionally, it can be reflected by acontext-performance curve, a flat curve is morerobust than a sharp one.
We also use standardvariance (S.V.)
along a sequence of senseselection to give an overall comparison.
Asequence with small standard variance is morerobust than that with a big one.From Figures 2 and 3, we can get anintuitional impression that TB_KNN not onlyachieve the best performance at most of points,but also has the flattest curve shape.
This meansTB_KNN outperforms other algorithm on bothperformance and robustness.
This can be detailedin Table 1 by comparing mean/max/S.V.
ofTB_KNN with their correspondences in otheralgorithms.Comparing values in the TB_KNN columnwith their correspondences in column P, we canfind all values of TB_KNN are consistentlybetter than those in P. For ?mean?
and ?max?, abigger one is better, while for S.V., a little one isbetter.
Comparing values in the TB_KNNcolumn with their correspondences in columnKNN, we can find nearly all values of TB_KNNare better than those in KNN.
(Except thatKNN?s max and S.V.
for word ???
are betterthen those in TB_KNN).
All differences aresignificant.
This means our decision trajectorybased classifier is better than a NB classifier or aKNN classifier.
The combination takesadvantages of both NB and KNN methods.
Itseems that KNN directly based on wordco-occurrence features suffers deeply from datasparseness.
While KNN based on decisiontrajectory can alleviate the influence of datasparseness.
In our final KNN decision, senseselection is also not sensitive to the number ofnearest neighbors.Comparing values in TB_KNN column withtheir correspondences in column LWNB, we canfind most of values in TB_KNN are better thantheir correspondences in LWNB.
But thedifferences are not so bigger than those describedpoints in the trajectoryF-Measure1.0.9.8.7?TB_KTB_VPKNNLWNBFigure 2. context-performance curves for ??
?points in the trajectoryF-Measure1.0.9.8.7?TB_KTB_VPKNNLWNBFigure 3. context-performance curves for ??
?Word Num-Sen TB_KNN TB_VOTE  P KNN LWNB ENB?
3(68,73,35) 0.88/0.91/0.03 0.86/0.89/0.03 0.83/0.88/0.06 0.86/0.91/0.04 0.87/0.92/0.04 0.89?
3(31,62,64) 0.95/0.98/0.04 0.94/0.96/0.05 0.90/0.95/0.05 0.83/0.91/0.05 0.90/0.96/0.04 0.96?
3(25,28,18) 0.89/0.94/0.03 0.88/0.94/0.04 0.80/0.90/0.10 0.69/0.82/0.07 0.76/0.87/0.07 0.82?
4(42,36,31,28) 0.80/0.84/0.04 0.79/0.83/0.04 0.74/0.83/0.06 0.75/0.81/0.05 0.74/0.80/ 0.04 0.79?
2(24,33) 0.93/ 0.97/0.03 0.92/0.97/0.04 0.88/0.95/0.05 0.85/0.92/0.05 0.91/ 0.97/0.04 0.89?
2(40,36) 0.91/0.96/0.06 0.89/0.94/0.07 0.85/0.96/0.18 0.89/0.97/0.06 0.87/0.97/0.06 0.84?
2(43,52) 0.86/0.89/0.03 0.84/0.87/0.04 0.83/0.88/0.04 0.73/0.80/0.03 0.82/0.88/0.04 0.89?
2(15,15) 0.83/ 0.92/0.05 0.82/ 0.89/0.05 0.77/0.87/0.07 0.49/0.82/0.13 0.79/0.89/0.06 0.77Table 1 result summaryin above paragraph, especially when the numberof training samples is relatively big.
In Frank etal.
(2003), the number of training samples islarge.
(Most of them are more than severalhundreds.)
They used 50 local training samplesto construct a NB classifier.
It is alwaysimpossible in our experiments and in most WSDtasks.Although not all of the values of mean inTB_KNN column are bigger than theircorrespondences in ENB, all maximums arebigger (or equal) than those in ENB.
Comparingwith ENB, We think the trajectory based approachmay make use of NB decisions in a moresystematical way than selecting some classifiersfor voting in ENB, and also, our approach receivesbenefits from the final KNN decision, which canmake some exceptions under consideration.Let us give a discussion on how ourtrajectory-based approach makes use ofinformation in context.Firstly, although each NB classifier usebag-of-words as its features, because windowsize for NB classifiers is extended sequentially,the decision trajectory thus reflects influencesbrought by context words in different positions.That is to say, changing the position of aco-occurrence word in a sentence might causedifferent final decision in trajectory-basedapproach.
While in point-based approach, aslong as the co-occurrence word is in the contextwindow, a classifier based on bag-of-wordsfeatures always makes the same selection nomatter how to change the position of that word.From this view, the trajectory-based approach infact makes use of position information of wordsin context.Secondly, because of its implicit utilization ofposition information of context words, it maymake use of information from some decisionslocally correct but globally wrong.
For example,we consider sentence S1 in section 2 again.S1:?/ ?/ ?/?/ ?/    ?
?/  ?/  ?/.
(I) (think) (this) (book) (worthy) (a)  (read)On the one hand, as we have said, when weuse context window (3,3), we select the sense of?read?
for ?.
Although it is a wrong senseselection for this word in this sentence (whencontext window is (6,6)), it is a correct selectionfor the local collocation (when ?
collocateswith ?, its sense is ?read?).
By saving thisinformation, we cannot only make use ofinformation of sense selection for the sentence,but also information for this collocation.
In otherwords, the sentence S1 gives us two samples fordifferent senses of the target word.On the other hand, that a polysemous wordchanges their probability for different sense withthe change of context window is one type ofpattern for sense ambiguity, the trajectory basedapproach seems an efficient way to grasp thispattern of ambiguity.5.2 TrajectoryIn TB_KNN, we need to calculate a sensedecision trajectory for each training sample, notall of these trajectories are eigen-trajecories.
InTB_VOTE, we don?t calculate sense decisiontrajectories for training samples, all trainingdecision trajectories are regarded as eigen-trajectory, final decision for a new samplereduces to majority voting along the trajectory.Comparing TB_KNN and TB_VOTE, we canfind that both performance and robustness ofTB_VOTE fall.
This means existence ofnon-eigen-trajectory is in fact helpful, which canmake some exceptions under consideration byusing KNN.In above experiments, we generated atrajectory by adding one context word each time.We further explored if a looser trajectory can getthe same performance.
We first excluded evenpoints in original trajectories in aboveexperiments to get some new trajectories.
Forexample, by excluding even points of thetrajectory },...,{ 40140 ppT = , we got:20,...,1},,..,,...,{ 3912120' == ?
kpppT kNote this 20'T  is different from 20T  in aboveexperiments, where 20T  is:20,...,1},,..,,...,{ 20120 == kpppT kIn this way, we got 20 different trajectoriesTG2: 20'1' ,...,TT , jT '  includes half numberof points comparing with its correspondencejT2  in above experiments.
The longesttrajectory includes 20 points.
We repeated aboveTB_KNN experiment along these newtrajectories.
Results are listed in columnTB_KNN TG2 in Table2.
We excluded evenpoints to generate TG3 and TG4 which includeat most 10 and 5 points respectively in theirtrajectories.
We also repeated same TB_KNNexperiment on TG3 and TG4.TB_KNN TG2 TB_KNN TG3 TB_KNN TG4?
0.87/0.90/0.03 0.87/0.91/0.03 0.86/0.89/0.03?
0.95/0.97/0.01 0.95/0.96/0.01 0.94/0.95/0.02?
0.90/0.93/0.02 0.90/0.91/0.02 0.90/0.93/0.03?
0.80/0.84/0.02 0.78/0.82/0.03 0.77/0.81/0.02?
0.93/0.97/0.03 0.93/0.95/0.02 0.92/0.96/0.03?
0.91/0.94/0.06 0.91/0.93/0.06 0.90/0.94/0.09?
0.86/0.90/0.03 0.86/0.90/0.04 0.82/0.85/0.03?
0.83/0.92/0.05 0.85/0.92/0.05 0.82/0.92/0.07Table 2: shorter length in the trajectoryFrom Table 2, we can find that performanceof classifiers using trajectories with smallnumber of points do not decrease significantly.That is to say, a shorter trajectory can alsoachieve good performance.6  conclusionsThis paper presents a new type of classifiercombination method.
We firstly construct asequence of NB classifiers along orderly varyingsized windows of context, and get a trajectory ofsense selection for each sample, then use thesense trajectory based KNN to make finaldecision for test samples.
Experiments show thatour approach outperforms some other algorithmson both robustness and performance.We will do further investigations on thetrajectory to see if there exists some skeletalpoints like quantum numbers in thewavefunction in Quantum Theory.ReferencesThomas G. Dietterich.
1997.
Machine LearningResearch: Four Current Directions.
AI Magazine.Vol.
18, No.
4 pp.97-136.Radu Florian, Silviu Cucerzan, C Schafer and D.Yarowsky.
2002.
Combining Classifiers forWord Sense Disambiguation.
Journal of NaturalLanguage Engineering.
Vol.
8 No.4.Radu Florian and D. Yarowsky.
2002.
ModelingConsensus: Classifier Combination for WordSense Disambiguation.
In Proceedings ofEMNLP'02, pp25-32.Eibe Frank, M. Hall and Bernhard Pfahringer.2003.
Locally Weighted Na?ve Bayes.Proceedings of the Conference on Uncertaintyin Artificial Intelligence.V?ronique Hoste, I. Hendrickx, W. Daelemans,and A. van den Bosch.2002.
Parameteroptimization for machine-learning of wordsense disambiguation.
Natural LanguageEngineering,8(3).Nancy Ide, J Veronis.1998.
Introduction to theSpecial Issue on Word Sense Disambiguation:The State of the Art.
Computational Linguistics,24(1):1-40.Dan Klein, K. Toutanova, H. Tolga Ilhan, S. D.Kamvar, and C. D. Manning.
2002.
CombiningHeterogeneous Classifiers for Word-SenseDisambiguation.
In Workshop on Word SenseDisambiguation at ACL 40, pages 74-80.Adam Kilgarriff and J. Rosenzweig (2000).Framework and results for English Senseval.Computers and the Humanities.
34(1):15-48.Chris D. Manning and H. Schutze.
1999.Foundations of Statistical Natural LanguageProcessing.
MIT Press.Rada Mihalcea.
2002.
Word Sense DisambiguationUsing Pattern Learning and Automatic FeatureSelection, Journal of Natural Language andEngineering, 8(4):343-358.Hwee Tou Ng, Hian Beng Lee.
1996.
IntegratingMultiple Knowledge Sources to DisambiguateWord Sense: An Exemplar-Based Approach.
InProceedings of the Thirty-Fourth ACL.Ted Pedersen 2000.
A Simple Approach toBuilding Ensembles of Naive BayesianClassifiers for Word Sense Disambiguation.
Inthe Proceedings of the NAACL-00.David Yarowsky 1994.
Decision Lists forLexical Ambiguity Resolution: Application toAccent Restoration in Spanish and French.''
InProceedings of the 32nd ACL.
pp.
88-95.David Yarowsky and R. Florian.2002.Evaluating Sense Disambiguation PerformanceAcross Diverse Parameter Spaces.
Journal ofNatural Language Engineering, Vol.8, No 4.
