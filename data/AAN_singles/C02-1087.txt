Selforganizing classification on the Reuters news corpusStefan WermterThe Informatics CentreSchool of CETUniversity of SunderlandSt.
Peter?s Way, Sunderland SR6 0DDUnited KingdomStefan.wermter@sunderland.ac.ukChihli Hung1The Informatics CentreSchool of CETUniversity of SunderlandSt.
Peter?s Way, Sunderland SR6 0DDUnited KingdomChihli.hung@sunderland.ac.uk1 Hung is a lecturer of De Lin Institute of Technology as well.AbstractIn this paper we propose an integration of aselforganizing map and semantic networksfrom WordNet for a text classification taskusing the new Reuters news corpus.
Thisneural model is based on significance vectorsand benefits from the presentation ofdocument clusters.
The Hypernym relationin WordNet supplements the neural model inclassification.
We also analyse therelationships of news headlines and theircontents of the new Reuters corpus by aseries of experiments.
This hybrid approachof neural selforganization and symbolichypernym relationships is successful toachieve good classification rates on 100,000full-text news articles.
These resultsdemonstrate that this approach can scale upto a large real-world task and show a lot ofpotential for text classification.IntroductionText classification is the categorization ofdocuments with respect to a set of predefinedcategories.
Traditional neural techniques forclassification problems cannot present theirresults easily without adding extra modules butselforganizing memory networks (SOM) arecapable of combining topological presentationwith neural learning.
We extract suitablerelations from WordNet to present a semanticmap of news articles and show that theserelations can complement neural techniques intext categorization.
This integration of SOM andWordNet is proposed to deal with the textclassification of news articles.The remainder of this paper is organised asfollows.
In Section 1, we give a brief review ofSOM.
Section 2 is dedicated to a description ofmethods of dimensionality reduction.
In section3 of our hybrid neural approach, the new versionof the Reuters corpus and the results of ourexperiments are presented.1 Selforganising Memory for LearningClassificationAccording to the theory and the organisation ofbiological systems, neurons with similarfunctions are placed together.
Based on  this idea,Kohonen proposed SOM (Kohonen 1982).
SOM,based on an unsupervised learning principle, canmap a multi-dimensional dataset into alow-dimensional space, usually 2-dimensional.SOM learns to place similar data on topologicallyclose areas on the map.
Therefore, people canchoose the relevant clusters of documents on themap to get relevant documents.
However, it isimpossible for one map to encompass thecontinuously growing data source.In such cases, the categories are often arranged ina hierarchy or an adaptive structure, e.g.Incremental Grid Growing model (Blackmoreand Miikkulainen 1993), Growing CellStructures (Fritzke 1993), Hierarchical SOM(Wan and Fraser 1994), and AdaptiveCoordinates (Rauber 1996; Merkl and Rauber1997).
Presentation and explanation are apossibly weakness for most ANN models to textclassification.
The robustness of the SOMalgorithm and its appealing visualization effectsmake it a prime candidate in text classification(Lin et al 1991; Ritter and Kohonen 1989;Honkela 1997).2 Dimensionality ReductionVSM (Vector Space Model) is a basic techniqueto transform text documents to numeric vectors.Often neural networks including the SOM modelfor text classification apply VSM on theirpre-processing stage.
SOM does not reduce thelength of vectors but only presents the highdimensionality of input vectors by prearrangedunits on a low dimensional space.
Dealing with ahuge text collections means dealing with hugedimensionality that needs to be reduced forneural approaches such as SOM (Berry et al1999).In the field of linear algebra, PCA (PrincipalComponent Analysis), SVD (Singular ValueDecomposition) and Random projection areeffective for dimensionality reduction but sufferfrom two main side effects.
The first one is thatthe results are difficult to interpret and the secondone is a reduction of the accuracy.Rather than introducing hierarchies from SOMwe want to exploit existing semantic knowledge,especially here from WordNet.
WordNet (Miller,1985) is a network of semantic relationshipsbetween English words.
Semantic relationsamong words construct a network.
The sets ofsynonyms compose synsets, which are the verybasic relations in WordNet.
Words in the samesynset have the same or similar concept and viceversa.
In addition to synonymy, there are severaldifferent types of semantic relations such asantonymy, hyponymy, meronymy, troponomy,and entailment in each different syntacticcategory, i.e.
nouns, verbs, adjectives andadverbs.
This semantic dictionary is useful inextracting the real concept of a word, a query or adocument in the field of text mining (Richardson1994; Richardson and Smeaton 1995; Voorhees1993; Voorhees 1998; Scott and Matwin 1998;Gonzalo et al 1998; Moldovan and Mihalcea1998; Moldovan and Mihalcea 2000).
Usingthese semantic relations in WordNet, one indexword may present its many synonyms, siblings orother relevant words.
Therefore, by mappingwords to more general concepts, WordNet can beused to reduce the dimensionality.Instead of using these approaches to reducemulti-dimensional vectors, we apply significancevectors to present the importance of words ineach semantic category and use pre-assignedtopics as axes of multi-dimensional space.
Thusa news article can be represented by an-dimension vector, where n is the number ofpre-assigned topics.
This method offers a way todivert from the huge dimensionality curse.
Amore detail description is shown in section 3.2.3 Selforganizing classification on thenew Reuters corpus using WordNet3.1 The New Version of Reuters CorpusWe work with the new version of Reuters corpus(Reuters 2000).
This corpus is made up of 984Mbytes of newspaper articles in compressedformat from issues of Reuters between the 20thAug., 1996 and 19th Aug., 1997.
The number oftotal news articles is 806,791, which contain9,822,391 paragraphs, 11,522,874 sentences andabout 2 hundred million word occurrences.Each document is saved in a standard XMLformat and is pre-classified by 3 different codesof categories, which are industry code, regioncode and topic code.
We are currently interestedin the topic code only.
126 topics are defined inthis new corpus but 23 of them contain no articles.All articles except 10,186 of them are classifiedin at least one topic.In our first experiments we concentrate on 8major topics (Table 1).
In order to get acomparison of the performance with and withoutthe use of WordNet and the relation of headlinesand full-text news articles, a series ofexperiments have been performed.
First, we usethe first 100,000 news headlines for training andanother 100,000 news headlines for test.
Thesecond experiment is exactly the same as the firstone but we use full-text instead of headlines.
Inthe third experiment, we use 100,000 full-textnews articles for training and use their headlinesfor test.
The fourth experiment is opposite to thethird one.
An integration of SOM and WordNetwill be presented in last two experiments.Table 1.
The description of chosen topics andtheir distribution over whole corpusno Topic Description Distribution1 C15 performance 149,3582 C151 accounts/earnings 81,2003 CCAT corporate/industrial 372,0974 E21 government finance 42,5735 ECAT economics 116,2056 GCAT government/social 232,0317 GCRIM crime,law enforcement32,0368 GDIP internationalrelations37,6303.2 Presenting Text Documents bySignificance VectorsWe use pre-assigned topics as axes of amulti-dimensional space and apply significancevectors to present the importance of words ineach semantic category based on (Wermter 2000).Significance vectors are defined by the frequencyof a word in different topics.
A significancevector is presented with topic elements (t1t2?tj),where tj presents the frequency of a word in jsemantic category.
Thus a document x ispresented with:?
?===nimjjijijttopicinwwordforFrequencyttopicinwwordforFrequencytwx11),(where n is the number of words and m is thenumber of topics.
This Method1 vector is thesummation of significance vectors.
(1)Method 1 can be susceptible to the number ofnews documents observed in each topic.
Analternative method 2 of vector presentation canalleviate skewed distributions.
Thus a documentx is modified as:)ln(),(11111jinimjjininimjjijijttopicinwwordforFrequencyttopicinwwordforFrequencyttopicinwwordforFrequencyttopicinwwordforFrequencytwx????
?=====?=(2)Because only nouns and verbs have thehypernym relation in WordNet and becausenouns and verbs convey enough information ofdocument concepts, we remove all words exceptnouns and verbs found in WordNet in ourexperiments.
We also benefit by a function ofWordNet, morphword, as a simple stemming tool.After above pre-processing, our 100,000 newsarticle training set represents the total number of8,920,287 (381,871) word occurrences and thetotal number of 22,848 (10,185) distinct words infull-text and headline experiments respectively.An example of these vector representationmethods is shown in (Table 2).
Note that therepresentation of ?to?
is the 0-vector since is notshown in nouns and verbs collections ofWordNet.Table 2.
Examples of rounded significancevectors on news headline experiment.
Topiccodes are presented on number 1 to 8 (Table 1).Word 1 2 3 4 5 6 7 8Recovery .13 .05 .33 .02 .29 .13 .04 .01Excitement .00 .00 .00 .00 1.0 .00 .00 .00Brings .01 .00 .19 .03 .14 .49 .05 .08Mexican .03 .01 .19 .02 .16 .42 .14 .01Markets .11 .04 .55 .04 .16 .09 .01 .00To .00 .00 .00 .00 .00 .00 .00 .00Life .16 .09 .39 .01 .04 .23 .07 .02Method1 .44 .20 1.66 .12 1.79 1.35 .31 .13Method2 1.01 .57 1.79 .38 3.76 1.85 1.01 .393.3 Classification and Presentation usingSOMOur work is based on the SOM algorithm(Vesanto et al 1999).
We give each news articlea topic label.
This label is determined by themost significant weights of topics in an inputvector based on one of the above methods.
Theninput vectors are normalised.
After the trainingprocess, a label of a map unit is assignedaccording to the highest number of assignedlabels.
For example, if 3 news articles of ECATand 10 news articles of CCAT are mapped to unit1, then the label of unit 1 will be associated withCCAT.
Therefore, all units present theirfavourite news article labels.
We adopt asemi-supervised SOM concept to add an extrasemantic vector, xs, with a small number 0.2 as itshighest value to represent the desired class.
In ourcase xs has 8 elements, as has x.
That is, thedocument vector d is represented as d=[xs x], e.g.
[0 0 0 0 0.2 0 0 0 0.44 0.20 1.7 0.12 1.79 1.350.31 0.13].
This approach can make the border ofSOM units more prominent and also can be usedto verify the performance of text classification.
ASOM map with 225 output units is shown in (Fig1) based on classifying these 16 elementdocument vectors.
Other architectures (e.g.
25x25) have been tested and show similar clearresults.SOM 01-Feb-2002significance333333333333333333333333333333333333333333333333333333333333333333333333333333333333335555563555555661566661666666466666666266666666666666666666066666666668667666666666Fig.
1.
SOM with 15*15 units.
Reuters topiccodes are presented on numbers (Table 1)3.4 Composing Semantic Clusters fromWordNetWordNet physically builds the databaseaccording to syntactic categories and semanticrelations among synsets.
In our work, we use thehypernym-hyponymy relation.
A hypernym of aterm is a more general term where hyponymy ismore specific.
For example, an apple is a kind ofedible fruit, so edible fruit is a hypernym of appleand an apple is a hyponymy of edible fruit.
Weuse the hypernym relation because the concept ofthis relation is similar to the definition of newsclassification.The concept of a category of news is moregeneral than each distinct news article.
Newsarticles with a similar concept will be grouped ina same class, and each group member, i.e.
eachdistinct news article, still has its own specificmeaning.
We use a 2-level hypernym to replaceeach word in a news article with its hypernymterm in order to get a more general concept of itsoriginal word.
Only nouns and verbs in WordNetconsist of this hypernym relation.
Polysemousand synonymous terms can be represented inseveral synsets and each synset may lie in adifferent hypernym hierarchy.
It is difficult todecide the concept of a document that containsseveral ambiguous terms.
Salton and Lesk givean example that offers a useful approach (Saltonand Lesk 1971).
The set of nouns base, bat,glove, and hit have each their own differentsenses, but putting them together means the gameof baseball clearly.
We use this idea and takeadvantage of synsets?
glosses, which are anexplanation of the meaning of each concept.Then the correct concept of a term is decided bycomparing the similarity of each gloss with thesemantic term-topic database of Reuters.
Forexample, the first news article is pre-assigned totopic ECAT.
The first term of the headline of thisarticle is recovery that consists of 3 senses asNoun and 0 senses as Verb.
Thus, there are 3glosses for this word.
We count the number ofthe co-occurrence of terms shown in each glossand the pre-assigned term-topic database.
Thenwe average the significance of terms by dividingby the total number of terms in each gloss.
Thus,the most significance of the gloss means the mostpossibility of the sense.
Finally every term isreplaced by its 2-level hypernym.
This approachis successful to reduce the total number ofdistinct words in the training set by 83.15% and72.84% in full-text and headline experimentsrespectively (Table 3).
Furthermore, thisapproach can also offer an easy way to extract areasonable right word sense for an ambiguousword.
We will represent our results in theexperiment section.Table 3.
The total number of distinct words intraining set with and without the help of WordNetNews source without With reductionHeadline 10,185 2,766 72.84%Full-text 22,848 3,851 83.15%3.5 Evaluation MethodThe label shown on a trained SOM is a preferenceand it is possible that several different labels areassigned to the same SOM unit.
We consider thatevery input vector which is mapped to this unitwill be reassigned the unit label to replace itsoriginal label.
In our above example, those 3news articles lose their label of ECAT and get theunit label of CCAT.
Kohonen et al (2000) definethe classification error as "all documents thatrepresented a minority newsgroup at any gridpoint were counted as classification errors."
Ourclassification accuracy is very similar toKohonen?s, but we use the corpus itself to verifythe performance.
If the replaced input vectorlabel matches ONE of the original labels assignedby Reuters, it is a correct mapping.
The accuracyis calculated from the proportion of the numberof relevant mappings to the number of input newsarticles.
Some news articles have the label 0because after pre-processing these articles arezero vectors.3.6 Results of Experiments3.6.1 Selforganization classification basedon News Headline and Full-textThe first 100,000 news articles are used fortraining and the following 100,000 news articlesare used for testing the generality.
SOMrepresents the original distribution of source dataso it is important to describe the distribution ofdata sets (Table 4).
Because a news article can beclassified in several topics, the distribution overchosen topics is inevitably not even.Table 4.
The distribution of articles from newReuters corpus over the semantic categoriesTraining Set Test Set no Number Distribution Number Distribution1 20,448 12.39% 25,810 14.84%2 10,427 6.32% 13,876 7.98%3 57,641 34.94% 61,120 35.15%4 7,034 4.26% 7,061 4.06%5 18,871 11.44% 19,312 11.11%6 38,792 23.51% 35,983 20.70%7 5,317 3.22% 4,588 2.64%8 6,447 3.91% 6,120 3.52%We have four experiments in this subsection.
Inthe first experiment, the first 100,000 news titlesare used for training and 100,000 successivenews titles are used for test.
The secondexperiment is same as the first one but full-textnews articles are used instead of headlines only.We then try to use the trained SOM based onfull-text news to test the coherence of news titlesentences.
The fourth experiment is inversely tothe third one.
The results are shown in Table 5-8respectively.
We find that our significancevector representation methods can achieve highaccuracy.
Second, even though full-text newsarticles contain more information than headlinesthere is no big difference in accuracy for a textclassification task.
Third, a trained SOM basedon news headlines or based on full-text news canbe highly generalised.
However, the former ismore general than the latter.
Although the newversion of Reuters news corpus is used in thiswork, this result is similar to the conclusion ofRodr?guez et al (1997) who use the old versionof Reuters and confirms that the topic headings inReuters corpus tend to consist of frequent wordsin the news document itself and this helps the taskof news classification.Table 5.
Accuracy on 100,000 news titles fortraining and test setMethod Training set Test set1 88.85% 87.55%2 91.07% 89.03%Table 6.
Accuracy on 100,000 full-text newsarticles for training and test setMethod Training set Test set1 85.70% 85.96%2 92.77% 92.01%Table 7.
Accuracy on 100,000 full-text news fortraining and their headlines for testMethod Full-text fortrainingHeadline fortest1 85.70% 80.81%2 92.77% 80.18%Table 8.
Accuracy on 100,000 news headlinesfor training and their full-text news for testMethod Headline fortrainingFull-text fortest1 88.85% 84.11%2 91.07% 89.95%3.6.2 Selforganization classification withand without the help of WordNetOur results using 2-level hypernym relation aresignificant for several reasons.
First, wesuccessfully reduce the total number of distinctwords from 10,185 to 2,766 (22,848 to 3,851) inour training tests based on news headline andfull-text news respectively (Table 3).
Second,with the use of WordNet, this hybrid neuraltechnique successfully improves the accuracy ofnews classification without any loss ofcategorisation ability (Table 9-10).Table 9.
Accuracy without and with the help ofWordNet 2-level hypernym on 100,000 full-textfor training setMethod  SOM SOM withWordNet1 85.70% 94.21%2 92.77% 98.95%Table 10.
Accuracy without and with the help ofWordNet 2-level hypernym on 100,000 newstitles for training setMethod SOM SOM withWordNet1 88.85% 89.94%2 91.07% 90.65%Discussion and ConclusionIn the past there had been no consistentconclusions about the value of WordNet forinformation retrieval tasks (Mihalcea andMoldovan 2000).
Experiments performed usingdifferent methodologies led to various, sometimecontradicting results (Voorhees 1998).
This isprobably because extracting the concept of aword is seriously dependent on otherunambiguous words.
Text classification ismapping documents with similar concepts to acluster with a more general concept.If a vector label matches ONE of the originallabels assigned by Reuters, it is considered acorrect mapping.
Another test could be toconsider a multi-topic a NEW topic.
This addsmany more classes and topics.
In this case, wefound 54.29% and  80.51% on 100,000 full-textnews articles without and with the help ofWordNet respectively, demonstrating the   meritof using WordNet even more.We have demonstrated that it is suitable to use thehypernym relation from WordNet for textclassification.
We successfully used this relationand improved the text classification performancesubstantially.
By merging statistical neuralmethods and semantic symbolic relations, ourhybrid neural learning technique is robust toclassify real-word text documents and allows usto learn to classify above 98% of 100,000documents to a correct topic.ReferencesBerry, M.W., Drmac, Z. and Jessup, E.R.
(1999).Matrices, Vector Spaces, and InformationRetrieval.
SIAM Review, Vol.
41, No.
2, pp.334-362.Blackmore, J. and Miikkulainen, R. (1993).Incremental Grid Growing: EncodingHigh-Dimensional Structure into aTwo-Dimensional Feature Map.
InProceedings of the IEEE InternationalConference on Neural Networks (ICNN?93),San Francisco, CA, USA.Fritzke, B.
(1993).
Kohonen Feature Maps andGrowing Cell Structures ?
a PerformanceComparison.
Advances in Neural InformationProcessing Systems 5, C.L.
Gibs, S.J.
Hanson,J.D.
Cowan (eds.
), Morgan Kaufmann, SanMateo, CA, USA.Gonzalo, J., Verdejo, F., Chugur, I. and Cigarran,J.
(1998).
Indexing with WordNet Synsets CanImprove Text Retrieval.
In Proceedings of theCOLING/ACL Workshop on Usage of WordNetin Natural Language Processing Systems,MontrealHonkela, T. (1997).
Self-Organizing Maps inNatural Language Processing.
PhD thesis.Helsinki University of Technology.Kohonen, T. (1982).
Self-Organized Formationof Topologically Correct Feature Maps.Biological Cybernetics, 43, pp.59-69.Kohonen, T., Kaski, S., Lagus, K., Saloj?rvi, J.,Honkela, J., Paatero, V., and Saarela A.
(2000).Self organization of a massive documentcollection.
In IEEE Transactions on NeuralNetworks, Vol.
11, No.
3, pp.
574-585.Lin, X., Soergel, D. and Marchionini, G. (1991).A Self-Organizing Semantic Map forInformation Retrieval.
In Proceedings of theFourteenth Annual International ACM/SIGIRConference on Research and Development inInformation Retrieval, pp.
262-269, Chicago.Merkl, D. and Rauber, A.
(1997).
AlternativeWays for Cluster Visualization inSelf-Organizing Maps.
In Proc.
Of theWorkshop on Self-Organizing Maps(WSOM97), Helsinki, Finland.Mihalcea, R. and Moldovan, D. (2000), SemanticIndexing Using WordNet Senses, InProceedings of ACL Workshop on IR & NLP,Hong Kong, 2000.Miller, G. A.
(1985).
WordNet: A DictionaryBrowser.
In Proceedings of the FirstInternational Conference on Information inData, University of Waterloo, Waterloo.Moldovan, D. and Mihalcea, R. (1998).
AWordNet-Based Interface to Internet SearchEngines.
In Proceedings of FLAIRS-98, May1998, Sanibel Island, FL.Moldovan, D. and Mihalcea, R. (2000).Improving the Search on the Internet by UsingWordNet and Lexical Operators.
In IEEEInternet Computing, vol.
4 no.
1, pp.34-43.Rauber, A.
(1996).
Cluster Visualization inUnsupervised Neural Networks.
Diplomarbeit,Technische Universitat Wien, Austria.Richardson, R. (1994).
A Semantic-basedApproach to Information Processing.
PhDDissertation, Dublin City University.Richardson, R. and Smeaton, A.F.
(1995).
UsingWordNet in a Knowledge-Based Approach toInformation Retrieval.
Working PaperCA-0395, School of Computer Applications,Dublin City University, Dublin.Ritter, H. and Kohonen, T. (1989).Self-Organizing Semantic Maps.
BiologicalCybernetics, 61. pp.
241-254.Rodr?guez, Manuel de Buenaga, Jos?
Mar?aG?mez-Hidalgo and Bel?n D?az-Agudo (1997).Using WordNet to Complement TrainingInformation in Text Categorization.
In Proc.RANLP-97, Standford, March 25-27.Reuters Corpus (2000).
Volume 1, Englishlanguage, 1996-08-20 to 1997-08-19, releasedate 2000-11-03, Format version 1.http://about.reuters.com/researchandstandards/corpus/Salton, G. and Lesk, M. E. (1971).
InformationAnalysis and Dictionary Construction.
InSalton, G. Eds.
(1971).
The SMART RetrievalSystem: Experiments in Automatic DocumentProcessing, chapter 6, pp.
115-142.Prentice-Hall, Inc. Englewood Cliffs, NewJersey.Scott, S. and Matwin, S. (1998).
TextClassification Using WordNet Hypernyms.
InProceedings of the COLING/ACL Workshopon Usage of WordNet in Natural LanguageProcessing Systems, Montreal.Vesanto, J., Himberg, J., Alhoniemi, E. andParhankangas, J.
(1999).
Self-Organizing Mapin matlab: the Som Toolbox.
In Proceedings ofthe Matlab DSP Conference 1999, pp.
35-40,Espoo, Finland.Voorhees, E. M. (1993).
Using WordNet toDisambiguate Word Senses for Text Retrieval.In Proceedings of the sixteenth annualinternational ACM SIGIR conference onResearch and Development in InformationRetrieval, pp.
171 ?
180.Voorhees, E. M. (1998).
Using WordNet for TextRetrieval.
In Fellbaum C. Eds.
(1998).WordNet : an electronic lexical database.
MITPress, Cambridge, Mass.
pp.
285-303.Wan, W and Fraser, D. (1994).
MultipleKohonen Self-Organizing Maps: Supervisedand Unsupervised Formation with Applicationto Remotely Sensed Image Analysis.
InProceedings of the 7th Australian RemoteSensing Conference, Melbourne, Australia.Wermter, S. (2000).
Neural Network Agents forLearning Semantic Text Classification.Information Retrieval, 3(2), pp.87-103.
