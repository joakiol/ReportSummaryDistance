Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 373?379,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsMT Quality Estimation: The CMU System for WMT?13Almut Silja HildebrandCarnegie Mellon UniversityPittsburgh, USAsilja@cs.cmu.eduStephan VogelQatar Computing Research InstituteDoha, Qatarsvogel@qf.org.qaAbstractIn this paper we present our entry to theWMT?13 shared task: Quality Estima-tion (QE) for machine translation (MT).We participated in the 1.1, 1.2 and 1.3sub-tasks with our QE system trained onfeatures from diverse information sourceslike MT decoder features, n-best lists,mono- and bi-lingual corpora and gizatraining models.
Our system shows com-petitive results in the workshop sharedtask.1 IntroductionAs MT becomes more and more reliable, morepeople are inclined to use automatically translatedtexts.
If coming across a passage that is obviouslya mistranslation, any reader would probably startto doubt the reliability of the information in thewhole article, even though the rest might be ad-equately translated.
If the MT system had a QEcomponent to mark translations as reliable or pos-sibly erroneous, the reader would know to use in-formation from passages marked as bad transla-tions with caution, while still being able to trustother passages.
In post editing a human translatorcould use translation quality annotation as an indi-cation to whether editing the MT output or trans-lating from scratch might be faster.
Or he coulduse this information to decide where to start in or-der to improve the worst passages first or skip ac-ceptable passages altogether in order to save time.Confidence scores can also be useful for applica-tions such as cross lingual information retrieval orquestion answering.
Translation quality could bea valuable ranking feature there.Most previous work in the field estimates con-fidence on the sentence level (e.g.
Quirk etal.
(2004)), some operate on the word level (e.g.Ueffing and Ney (2007), Sanchis et al(2007),and Bach et al(2011)), whereas Soricut and Echi-habi (2010) use the document level.Various classifiers and regression models havebeen used in QE in the past.
Gandrabur and Foster(2003) compare single layer to Multi Layer Per-ceptron (MLP), Quirk et al(2004) report that Lin-ear Regression (LR) produced the best results ina comparison of LR, MLP and SVM, Gamon etal.
(2005) use SVM, Soricut and Echihabi (2010)find the M5P tree works best among a number ofregression models, while Bach et al(2011) definethe problem as a word sequence labeling task anduse MIRA.The QE shared task was added to the WMTevaluation campaign in 2012 (Callison-Burch etal., 2012), providing standard training and testdata for system development.2 WMT?13 Shared TaskIn this WMT Shared Task for Quality Estima-tion1 there were tasks for sentence and word levelQE.
We participated in all sub-tasks for Task 1:Sentence-level QE.Task 1.1: Scoring and ranking for post-editingeffort focuses on predicting HTER per segmentfor the translations of one specific MT system.Task 1.2: System selection/ranking required topredict a ranking for up to five translations ofthe same source sentence by different MT sys-tems.
The training data provided manual labels forranking including ties.
Task 1.3: Predicting post-editing time participants are asked to predict thetime in seconds a professional translator will taketo post edit each segment.1http://www.statmt.org/wmt13/quality-estimation-task.html373Besides the training data with labels, for eachof these tasks additional resources were provided.These include bilingual training corpora, languagemodels, 1000-best lists, models from giza andmoses training and various other statistics andmodels depending on task and language pair.3 Features3.1 Language ModelsTo calculate language model (LM) features, wetrain traditional n-gram language models with n-gram lengths of four and five using the SRILMtoolkit (Stolcke, 2002).
We calculate our featuresusing the KenLM toolkit (Heafield, 2011).
Wenormalize all our features with the target sentencelength to get an average word feature score, whichis comparable for translation hypotheses of differ-ent length.
In addition to the LM probability werecord the average n-gram length found in the lan-guage model for the sentence, the total number ofLM OOVs and OOVs per word, as well as themaximum and the minimum word probability ofthe sentence, six features total.We use language models trained on source lan-guage data and target language data to measuresource sentence difficulty as well as translationfluency.3.2 Distortion ModelThe moses decoder uses one feature from a dis-tance based reordering model and six featuresfrom a lexicalized reordering model: Given aphrase pair, this model considers three eventsMonotone, Swap, and Discontinuous in two direc-tions Left and Right.
This results in six events:LM (left-monotone), LS (left-swap), LD (left-discontinuous) and RM (right-monotone), RS,RD.These distortion features are calculated for eachphrase.
For a total sentence score we normalize bythe phrase count for each of the seven features.3.3 Phrase TableFrom the phrase table we use the features fromthe moses decoder output: inverse phrase trans-lation probability, inverse lexical weighting, di-rect phrase translation probability and direct lex-ical weighting.
For a total sentence score we nor-malize by the phrase count.
We use the numberof phrases used to generate the hypothesis and theaverage phrase length as additional features, sixfeatures total.3.4 Statistical Word LexicaFrom giza training we use IBM-4 statistical wordlexica in both directions.
We use six probabil-ity based features as described in Hildebrand andVogel (2008): Normalized probability, maximumword probability and word deletion count fromeach language direction.To judge the translation difficulty of each wordin the source sentence we collect the number oflexicon entries for each word similar to Gandraburand Foster (2003).
The intuition is, that a wordwith many translation alternatives in the word-to-word lexicon is difficult to translate while a wordwith only a few translation choices is easy to trans-late.In fact it is not quite this straight forward.
Thereare words in the lexicon, which have many lex-icon entries, but the probability for them is notvery equally distributed.
One entry has a veryhigh probability while all others have a very lowone - not much ambiguity there.
Other wordson the other hand have several senses in one lan-guage and therefore are translated frequently intotwo or three different words in the target language.There the top entries in the lexicon might eachhave about 30% probability.
To capture this be-havior we do not only count the total number ofentries but also the number of entries with a prob-ability over a threshold of 0.01.For example one word with a rather high num-ber of different translations in the English-Spanishstatistical lexicon is the period (.)
with 1570 en-tries.
It has only one translation with a probabilityover the threshold which is the period (.)
in Span-ish at a probability of 0.9768.
This shows a clearchoice and rather little ambiguity despite the highnumber of different translations in the lexicon.For each word we collect the number of lexi-con entries, the number of lexicon entries over thethreshold, the highest probability from the lexiconand whether or not the word is OOV.
If a word hasno lexicon entry with a probability over the thresh-old we exclude the word from the lexicon for thispurpose and count it as an OOV.
As sentence levelfeatures we use the sum of the word level featuresnormalized by the sentence length as well as thetotal OOV count for the sentence, which results infive features.3743.5 Sentence Length FeaturesThe translation difficulty of a source sentence isoften closely related to the sentence length, aslonger sentences tend to have a more complexstructure.
Also a skewed ratio between the lengthof the source sentence and its translation can be anindicator for a bad translation.We use plain sentence length features, namelythe source sentence length, the translation hypoth-esis length and their ratio as introduced in Quirk(2004).Similar to Blatz et al(2004) we use the n-bestlist as an information source.
We calculate the av-erage hypothesis length in the n-best list for onesource sentence.
Then we compare the current hy-pothesis to that and calculate both the diversionfrom that average as well as their ratio.
We alsocalculate the source-target ratio to this average hy-pothesis length.To get a representative information on thelength relationship of translations from the sourceand target languages in question, we use the par-allel training corpus.
We calculate the overall lan-guage pair source to target sentence length ratioand record the diversion of the current hypothesis?source-target ratio from that.The way sentences are translated from one lan-guage to another might differ depending on howcomplex the information is, that needs to be con-veyed, which in turn might be related to the sen-tence length and the ratio between source andtranslation.
As a simple way of capturing thisphenomenon we divide the parallel training cor-pus into three classes (short, medium, long) bythe length of the source language sentence.
Theboundaries of these classes are the mean 26.84plus and minus the standard deviation 14.54 of thesource sentence lengths seen in the parallel cor-pus.
We calculate the source/target length ratio foreach of the three classes separately.
The resultingstatistics for the parallel training corpora can befound in Table 1.
For English - Spanish the ratiofor all classes is close to one, for other languagepairs these differ more clearly.As features for each hypothesis we use a binaryindicator for its membership to each class and itsdeviation from the length ratio of its class.
Thisresults in 12 sentence length related features in to-tal.En trainnumber of sentences 1,714,385average length 26.84standard deviation 14.54class short 0 - 12.29class medium 12.29 - 41.38class long 41.38 - 100s/t ratio overall 0.9624s/t ratio for short 0.9315s/t ratio for medium 0.9559s/t ratio for long 0.9817Table 1: Sentence Length Statistics for theEnglish-Spanish Parallel Corpus3.6 Source Language Word and Bi-gramFrequency FeaturesThe length of words is often related to whetherthey are content words and how frequently theyare used in the language.
Therefore we use themaximum and average word length as features.Similar to Blatz et al(2004) we sort the vo-cabulary of the source side of the training corpusby occurrence frequency and then divide it intofour parts, each of which covers 25% of all to-kens.
As features we use the percentage of wordsin the source sentence that fall in each quartile.Additionally we use the number and percentage ofsource words in the source sentence that are OOVor very low frequency, using count 2 as threshold.We also collect all bigram statistics for the cor-pus and calculate the corresponding features forthe source sentence based on bigrams.
This addsup to fourteen features from source word and cor-pus statistics.3.7 N-Best List Agreement & DiversityWe use the three types of n-best list based featuresdescribed in Hildebrand and Vogel (2008): Posi-tion Dependent N-best List Word Agreement, Po-sition independent N-best List N-gram Agreementand N-best List N-gram Probability.To measure n-best list diversity, we comparethe top hypothesis to the 5th, 10th, 100th, 200th,300th, 400th and 500th entry in the n-best list(where they exist) to see how much the transla-tion changes throughout the n-best list.
We calcu-late the Levenshtein distance (Levenshtein, 1966)between the top hypothesis and the three lowerranked ones and normalize by the sentence length375of the first hypothesis.
We also record the n-bestlist size and the size of the vocabulary in the n-best list for each source sentence normalized bythe source sentence length.Fifteen agreement based and nine diversitybased features add up to 24 n-best list based fea-tures.3.8 Source Parse FeaturesThe intuition is that a sentence is harder to trans-late, if its structure is more complicated.
A sim-ple indicator for a more complex sentence struc-ture is the presence of subclauses and also thelength of any clauses and subclauses.
To obtain theclause structure, we parse the source language sen-tence using the Stanford Parser2 (Klein and Man-ning, 2003).
Features are: The number of clausesand subclauses, the average clause length, and thenumber of sentence fragments found.
If the parsedoes not contain a clause tag, it is treated as oneclause which is a fragment.3.9 Source-Target Word Alignment FeaturesA forced alignment algorithm utilizes the trainedalignment models from the MT systems GIZA(Och and Ney, 2003) training to align each sourcesentence to each translation hypothesis.We use the score given by the word alignmentmodels, the number of unaligned words and thenumber of NULL aligned words, all normalizedby the sentence length, as three separate features.We calculate those for both language directions.Hildebrand and Vogel (2010) successfully appliedthese features in n-best list re-ranking.3.10 Cohesion PenaltyFollowing the cohesion constraints described inBach et al(2009) we calculate a cohesion penaltyfor the translation based on the dependency parsestructure of the source sentence and the wordalignment to the translation hypothesis.
To obtainthese we use the Stanford dependency parser (deMarneffe et al 2006) and the forced alignmentfrom Section 3.9.For each head word we collect all dependentwords and also their dependents to form each com-plete sub-tree.
Then we project each sub-tree ontothe translation hypothesis using the alignment.
Wetest for each sub-tree, whether all projected wordsin the translation are next to each other (cohesive)2http://nlp.stanford.edu/software/lex-parser.shtmlor if there are gaps.
From the collected gaps wesubtract any unaligned words.
Then we count thenumber of gaps as cohesion violations as well ashow many words are in each gap.
We go recur-sively up the tree, always including all sub-treesfor each head word.
If there was a violation inone of the sub-trees it might be resolved by addingin its siblings, but if the violation persists, it iscounted again.4 ClassifiersFor all experiments we used the Weka3 data min-ing toolkit described in Hall et.
al.
(2009) to com-pare four different classifiers: Linear Regression(LR), M5P tree (M5Ptree), Multi Layer Percep-tron (MLP) and Support Vector Machine for Re-gression (SVM).
Each of these has been identi-fied as effective in previous publications.
All butone of the Weka default settings proved reliable,changing the learning rate for the MLP from de-fault: 0.3 to 0.01 improved the performance con-siderably.
We report Mean Absolute Error (MAE)and Root Mean Squared Error (RMSE) for all re-sults.5 Experiment ResultsFor Tasks 1.1 and 1.3 we used the 1000-best out-put provided.
As first step we removed duplicateentries in these n-best list.
This brought the sizedown to an average of 152.9 hypotheses per sourcesentence for the Task 1.1 training data, 172.7 onthe WMT12 tests set and 204.3 hypotheses persource sentence on the WMT13 blind test data.The training data for task 1.3 has on average 129.0hypothesis per source sentence, the WMT13 blindtest data 129.8.In addition to our own features described abovewe extracted the 17 features used in the WMT12baseline for all sub-tasks via the software providedfor the WMT12-QE shared task.5.1 Task 1.1Task 1.1 is to give a quality score between 0 and1 for each segment in the test set, predicting theHTER score for the segment and also to give arank for each segment, sorting the entire test setfrom best quality of translation to worst.For Task 1.1 our main focus was the scoringtask.
We did submit a ranking for the blind test3http://www.cs.waikato.ac.nz/ml/weka/376wmt12-test: WMT12 manual quality labelsWMT12 best system: Language Weaver 0.61 - 0.75WMT12 baseline system 0.69 - 0.82feat.
set #feat LR M5Pt MLP SVMfull 117 0.617 - 0.755 0.618 - 0.756 0.619 - 0.773 0.609 - 0.750no WMT12-base 100 0.618 - 0.766 0.618 - 0.767 0.603 - 0.757 0.611 - 0.761slim 69 0.621 - 0.767 0.621 - 0.766 0.614 - 0.768 0.627 - 0.773wmt12-test: HTERfull 117 0.125 - 0.162 0.126 - 0.163 0.122 - 0.156 0.121 - 0.156no WMT12-base 100 0.124 - 0.160 0.123 - 0.159 0.125 - 0.159 0.121 - 0.155slim 69 0.125 - 0.161 0.126 - 0.161 0.124 - 0.159 0.123 - 0.158wmt13-test: HTERWMT12 baseline system 0.148 - 0.182full 117 0.146 - 0.183 0.147 - 0.185 0.156 - 0.199 0.142 - 0.180no WMT12-base 100 0.144 - 0.180 0.144 - 0.180 0.156 - 0.203 0.139 - 0.176slim 69 0.147 - 0.182 0.147 - 0.181 0.153 - 0.194 0.142 - 0.177Table 2: Task 1.1: Results in MAE and RMSE on the WMT12 test set for WMT12 manual labels as wellas WMT13 HTER as target class and the WMT13 test set for HTERset as well, which resulted from simply sorting thetest set by the estimated HTER per segment.In Table 2 we show the results for some ex-periments comparing the performance of differ-ent feature sets and classifiers.
For developmentwe used the WMT12-QE test set and both theWMT12 manual labels as well as HTER as targetclass.
We compared the impact of removing the17 WMT12-baseline features ?no WMT12-base?and training a ?slim?
system by removing nearlyhalf the features, which showed to have a smallerimpact on the overall performance in preliminaryexperiments.
Among the removed features aren-best list based features, redundant features be-tween ours, the moses based and the base17 fea-tures and some less reliable features like e.g.
thelexicon deletion features, who?s thresholds need tobe calibrated carefully for each new language pair.We submitted the full+MLP and the no-WMT12-base+SVM output to the shared task, shown inbold in the table.The official result for our system for task 1.1on the WMT13 blind data is MAE 13.84, RMSE17.46 for the no-WMT12-base+SVM system andMAE 15.25 RMSE 18.97 for the full+MLP sys-tem.
Surprising here is the fact that our full systemclearly outperforms the 17-feature baseline on theWMT12 test set, but is behind it on the WMT13blind test set.
(Baseline bb17 SVM: MAE 14.81,RMSE 18.22) Looking at the WMT13 test set re-sults, we should have chosen the slim+SVM sys-tem variant.5.2 Task 1.2Task 1.2 asks to rank different MT systems bytranslation quality on a segment by segment basis.Since the manually annotated ranks in task 1.2allowed ties, we treated them as quality scores andran the same QE system on this data as we didfor task 1.1.
We submitted the full-MLP outputwith the only difference that for this data set thedecoder based features were not available.
Werounded the predicted ranks to integer.
Since thetraining data contains many ties we did not employa strategy to resolve ties.As a contrastive approach we ran the hypothe-sis selection system described in Hildebrand andVogel (2010) using the BLEU MT metric as rank-ing criteria.
For this system it would have beenvery beneficial to have access to the n-best listsfor the different system?s translations.
The BLEUscore for the translation listed as the first systemfor each source sentence would be 30.34 on theentire training data.
We ran n-best list re-rankingusing MERT (Och, 2003) for two feature sets: Thefull feature set, 100 features in total and a slim fea-ture set with 59 features.
For the slim feature setwe removed all features that are solely based on377the source sentence, since those have no impact onre-ranking an n-best list.
The BLEU score for thetraining set improved to 45.25 for the full featureset and to 45.76 for the slim system.
Thereforewe submitted the output of the slim system to theshared task.
This system does not predict ranksdirectly, but estimates ranking according to BLEUgain on the test set.
Therefore the new ranking isalways ranks 1-5 without ties.The official result uses Kendalls tau with andwithout ties penalized.
Our two submissionsscore: ?0.11 /?0.11 for the BLEU optimized sys-tem and?0.63 / 0.23 for the classifier system.
Theclassifier system is the best submission in the ?tiesignored?
category.5.3 Task 1.3Task 1.3 is to estimate post editing time on a persegment basis.In absence of a development test set we used10-fold cross-validation on the training data to de-termine the best feature set and classifier for thetwo submissions.
Table 3 shows the results on ourpreliminary tests for four classifiers and three fea-ture sets.
The ?no pr.?
differs from the full fea-ture set only by removing the provided features, inthis case the 17 WMT12-baseline features and the?translator ID?
and ?nth in doc?
features.
For the?slim?
system run the feature set size was cut inhalf in order to prevent overfitting to the trainingdata since the training data set is relatively small.We used the same criteria as in Task 1.1.
Forthe shared task we submitted the full+SVM andslim+LR variants, shown in bold in the table.The official result for our entries on the WMT13blind set in MAE and RMSE are: 53.59 - 92.21 forthe full system and 51.59 - 84.75 for the slim sys-tem.
The slim system ranks 3rd for both metricsand outperforms the baseline at 51.93 - 93.36.6 ConclusionsIn this WMT?13 QE shared task we submitted tothe 1.1, 1.2 and 1.3 sub-tasks.
In development wefocused on the scoring type tasks.In general there don?t seem to be significant dif-ferences between the different classifiers.Surprising is the fact that our full system fortask 1.1 clearly outperforms the 17-feature base-line on the WMT12 test set, but is behind it onthe WMT13 blind test set.
This calls into ques-tion whether the performance on the WMT12 testset was the right criterium for selecting a systemvariant for submission.The relative success of the ?slim?
system vari-ant over the full feature set shows that our systemwould most likely benefit from a sophisticated fea-ture selection method.
We plan to explore this infuture work.ReferencesNguyen Bach, Stephan Vogel, and Colin Cherry.
2009.Cohesive constraints in a beam search phrase-baseddecoder.
In HLT-NAACL (Short Papers), pages 1?4.Nguyen Bach, Fei Huang, and Yaser Al-Onaizan.2011.
Goodness: A method for measuring machinetranslation confidence.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies,pages 211?219, Portland, Oregon, USA, June.
As-sociation for Computational Linguistics.John Blatz, Erin Fitzgerald, George Foster, SimonaGandrabur, Cyril Goutte, Alex Kulesza, AlbertoSanchis, and Nicola Ueffing.
2004.
Confidence es-timation for machine translation.
Technical report,Final report JHU / CLSP 2003 Summer Workshop,Baltimore.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical ma-chine translation.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, pages10?51, Montre?al, Canada, June.
Association forComputational Linguistics.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InLREC-06.Michael Gamon, Anthony Aue, and Martine Smets.2005.
Sentence-level mt evaluation without refer-ence translations: Beyond language modeling.
InIn European Association for Machine Translation(EAMT.Simona Gandrabur and George Foster.
2003.
Con-fidence estimation for translation prediction.
In InProceedings of CoNLL-2003, page 102.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: an update.SIGKDD Explorations, 11(1):10?18.Kenneth Heafield.
2011.
KenLM: Faster and SmallerLanguage Model Queries.
In Proceedings of theSixth Workshop on Statistical Machine Translation,pages 187?197, Edinburgh, Scotland, July.
Associa-tion for Computational Linguistics.378feat.
set #feat class.
10-fold cross train WMT13 testfull 119 LR 45.73 - 73.52 39.74 - 63.92 54.45 - 88.68full 119 M5Pt 44.49 - 74.05 35.81 - 57.36 50.05 - 85.22full 119 MLP 48.05 - 75.68 41.03 - 68.70 54.38 - 88.93full 119 SVM 40.88 - 73.61 34.70 - 69.69 53.74 - 92.26no pr 100 LR 46.06 - 74.94 40.39 - 66.00 52.13 - 86.68no pr 100 M5Pt 43.80 - 74.30 36.80 - 59.47 50.86 - 87.42no pr 100 MLP 47.70 - 75.41 39.85 - 68.30 52.39 - 87.93no pr 100 SVM 41.35 - 74.68 35.59 - 70.99 52.87 - 92.22slim 59 LR 44.72 - 73.86 41.14 - 67.44 51.71 - 84.83slim 59 M5Pt 43.77 - 74.43 35.26 - 56.84 57.75 - 102.68slim 59 MLP 46.98 - 74.38 40.35 - 69.79 51.06 - 85.48slim 59 SVM 40.42 - 74.47 36.88 - 71.59 51.09 - 90.18Table 3: Task 1.3: Results in MAE and RMSE for 10-fold cross validation and the whole training set aswell as the WMT13 blind test setAlmut Silja Hildebrand and Stephan Vogel.
2008.Combination of machine translation systems via hy-pothesis selection from combined n-best lists.
InMT at work: Proceedings of the Eighth Confer-ence of the Association for Machine Translation inthe Americas, pages 254?261, Waikiki, Hawaii, Oc-tober.
Association for Machine Translation in theAmericas.Almut Silja Hildebrand and Stephan Vogel.
2010.CMU system combination via hypothesis selec-tion for WMT?10.
In Proceedings of the JointFifth Workshop on Statistical Machine Translationand MetricsMATR, pages 307?310.
Association forComputational Linguistics.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Erhard Hinrichs andDan Roth, editors, Proceedings of the 41st AnnualMeeting of the Association for Computational Lin-guistics, pages 423?430.Vladimir Iosifovich Levenshtein.
1966.
Binary codescapable of correcting deletions, insertions, and re-versals.
Soviet Physics Doklady, 10(8):707?710.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 160?167, Sap-poro, Japan, July.
Association for ComputationalLinguistics.Christopher B. Quirk.
2004.
Training a sentence-level machine translation confidence measure.
InProceedings of the Fourth International Conferenceon Language Resources and Evaluation, pages 825?828, Lisbon, Portugal, May.
LREC.Alberto Sanchis, Alfons Juan, Enrique Vidal, and De-partament De Sistemes Informtics.
2007.
Estima-tion of confidence measures for machine translation.In In Procedings of Machine Translation Summit XI.Radu Soricut and Abdessamad Echihabi.
2010.Trustrank: Inducing trust in automatic translationsvia ranking.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, pages 612?621, Uppsala, Sweden, July.
As-sociation for Computational Linguistics.Andreas Stolcke.
2002.
Srilm - an extensible lan-guage modeling toolkit.
In Proceedings Interna-tional Conference for Spoken Language Processing,Denver, Colorado, September.Nicola Ueffing and Hermann Ney.
2007.
Word-level confidence estimation for machine translation.Computational Linguistics, 33(1):9?40.379
