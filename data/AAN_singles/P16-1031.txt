Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 322?332,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsBi-Transferring Deep Neural Networks for Domain AdaptationGuangyou Zhou1, Zhiwen Xie1, Jimmy Xiangji Huang2, and Tingting He11School of Computer, Central China Normal University, Wuhan 430079, China2School of Information Technology, York University, Toronto, Canada{gyzhou,xiezhiwen,tthe}@mail.ccnu.edu.cn jhuang@yorku.caAbstractSentiment classification aims to automati-cally predict sentiment polarity (e.g., pos-itive or negative) of user generated sen-timent data (e.g., reviews, blogs).
Dueto the mismatch among different domains,a sentiment classifier trained in one do-main may not work well when directlyapplied to other domains.
Thus, domainadaptation for sentiment classification al-gorithms are highly desirable to reduce thedomain discrepancy and manual labelingcosts.
To address the above challenge,we propose a novel domain adaptationmethod, called Bi-Transferring Deep Neu-ral Networks (BTDNNs).
The proposedBTDNNs attempts to transfer the sourcedomain examples to the target domain, andalso transfer the target domain examplesto the source domain.
The linear transfor-mation of BTDNNs ensures the feasibilityof transferring between domains, and thedistribution consistency between the trans-ferred domain and the desirable domain isconstrained with a linear data reconstruc-tion manner.
As a result, the transferredsource domain is supervised and followssimilar distribution as the target domain.Therefore, any supervised method can beused on the transferred source domain totrain a classifier for sentiment classifica-tion in a target domain.
We conduct ex-periments on a benchmark composed ofreviews of 4 types of Amazon products.Experimental results show that our pro-posed approach significantly outperformsthe several baseline methods, and achievesan accuracy which is competitive with thestate-of-the-art method for domain adapta-tion.1 IntroductionWith the rise of social media (e.g., blogs and so-cial networks etc.
), more and more user generatedsentiment data have been shared on the Web (Panget al, 2002; Pang and Lee, 2008; Liu, 2012; Zhouet al, 2011).
They exist in the form of user re-views on shopping or opinion sites, in posts ofblogs/questions or customer feedbacks.
This hascreated a surge of research in sentiment classifi-cation (or sentiment analysis), which aims to au-tomatically determine the sentiment polarity (e.g.,positive or negative) of user generated sentimentdata (e.g., reviews, blogs, questions).Machine learning algorithms have been provedpromising and widely used for sentiment classifi-cation (Pang et al, 2002; Pang and Lee, 2008; Liu,2012).
However, the performance of these modelsrelies on manually labeled training data.
In manypractical cases, we may have plentiful labeled datain the source domain, but very few or no labeleddata in the target domain with a different data dis-tribution.
For example, we may have many labeledbooks reviews, but we are interested in detect-ing the polarity of electronics reviews.
Reviewsfor different products might have different vocab-ularies, thus classifiers trained on one domain of-ten fail to produce satisfactory results when trans-ferring to another domain.
This has motivatedmuch research on cross-domain (domain adapta-tion) sentiment classification which transfers theknowledge from the source domain to the targetdomain (Thomas et al, 2006; Snyder and Barzi-lay, 2007; Blitzer et al, 2006; Blitzer et al, 2007;Daume III, 2007; Li and Zong, 2008; Li et al,2009; Pan et al, 2010; Kumar et al, 2010; Glo-rot et al, 2011; Chen et al, 2011a; Chen et al,2012; Li et al, 2012; Xia et al, 2013a; Li et al,2013; Zhou et al, 2015a; Zhuang et al, 2015).Depending on whether the labeled data areavailable for the target domain, cross-domain sen-322timent classification can be divided into two cat-egories: supervised domain adaptation and unsu-pervised domain adaptation.
In scenario of super-vised domain adaptation, labeled data is availablein the target domain but the number is usually toosmall to train a good sentiment classifier, whilein unsupervised domain adaptation only unlabeleddata is available in the target domain, which ismore challenging.
This work focuses on the un-supervised domain adaptation problem of whichthe essence is how to employ the unlabeled dataof target domain to guide the model learning fromthe labeled source domain.The fundamental challenge of cross-domainsentiment classification lies in that the source do-main and the target domain have different data dis-tribution.
Recent work has investigated severaltechniques for alleviating the domain discrepancy:instance-weight adaptation (Huang et al, 2007;Jiang and Zhai, 2007; Li and Zong, 2008; Man-sour et al, 2009; Dredze et al, 2010; Chen et al,2011b; Chen et al, 2011a; Chen et al, 2012; Liet al, 2013; Xia et al, 2013a) and feature repre-sentation adaptation (Thomas et al, 2006; Snyderand Barzilay, 2007; Blitzer et al, 2006; Blitzer etal., 2007; Li et al, 2009; Pan et al, 2010; Zhouet al, 2015a; Zhuang et al, 2015).
The first kindof methods assume that some training data in thesource domain are very useful for the target do-main and these data can be used to train modelsfor the target domain after re-weighting.
In con-trast, feature representation approaches attempt todevelop an adaptive feature representation that iseffective in reducing the difference between do-mains.Recently, some efforts have been initiated onlearning robust feature representations with deepneural networks (DNNs) in the context of cross-domain sentiment classification (Glorot et al,2011; Chen et al, 2012).
Glorot et al (2011) pro-posed to learn robust feature representations withstacked denoising auto-encoders (SDAs) (Vincentet al, 2008).
Denoising auto-encoders are one-layer neural networks that are optimized to recon-struct input data from partial and random corrup-tion.
These denoisers can be stacked into deeplearning architectures.
The outputs of their in-termediate layers are then used as input featuresfor SVMs (Fan et al, 2008).
Chen et al (2012)proposed a marginalized SDA (mSDA) that ad-dressed the two crucial limitations of SDAs: high?
???
?
?Labeled Source Domain?Unlabeled Target Domain?Transferred  Source DomainFigure 1: The framework of Bi-transferring DeepNeural Networks (BTDNNs).
Through BTDNNs,a source domain example can be transferred to thetarget domain where it can be reconstructed by thetarget domain examples, and vice versa.computational cost and lack of scalability to high-dimensional features.
However, these methodslearn the unified domain-invariable feature repre-sentations by combining the source domain dataand that of the target domain data together, whichcannot well characterize the domain-specific fea-tures as well as the commonality of domains.To this end, we propose a Bi-Transferring DeepNeural Networks (BTDNNs) which can transferthe source domain examples to the target domainand also transfer the target domain examples tothe source domain, as shown in Figure 1.
InBTDNNs, the linear transformation makes the fea-sibility of transferring between domains, and thelinear data reconstruction manner ensures the dis-tribution consistency between the transferred do-main and the desirable domain.
Specifically, ourBTDNNs has one common encoder fc, two de-coders gsand gtwhich can map an example to thesource domain and the target domain respectively.As a result, the source domain can be transferredto the target domain along with its sentiment la-bel, and any supervised method can be used onthe transferred source domain to train a classifierfor sentiment classification in the target domain, asthe transferred source domain data share the sim-ilar distribution as the target domain.
Experimen-tal results show that the proposed approach signifi-cantly outperforms several baselines, and achievesan accuracy which is competitive with the state-of-323the-art method for cross-domain sentiment classi-fication.The remainder of this paper is organized as fol-lows.
Section 2 introduces the related work.
Sec-tion 3 describes our proposed bi-transferring deepneural networks (BTDNNs).
Section 4 presentsthe experimental results.
In Section 5, we con-clude with ideas for future research.2 Related WorkDomain adaptation aims to generalize a classifierthat is trained on a source domain, for which typi-cally plenty of training data is available, to a targetdomain, for which data is scarce.
Cross-domaingeneralization is important in many real applica-tions, the key challenge is that data in the sourceand the target domain are often distributed differ-ently.Recent work has investigated several techniquesfor alleviating the difference in the context ofcross-domain sentiment classification task.
Blitzeret al (2007) proposed a structural correspon-dence learning (SCL) algorithm to train a cross-domain sentiment classifier.
SCL is motivated bya multi-task learning algorithm, alternating struc-tural optimization (ASO), proposed by Ando andZhang (2005).
Given labeled data from a sourcedomain and unlabeled data from both source andtarget domains, SCL attempts to model the rela-tionship between ?pivot features?
and ?non-pivotfeatures?.
Pan et al (2010) proposed a spectralfeature alignment (SFA) algorithm to align thedomain-specific words from the source and targetdomains into meaningful clusters, with the helpof domain-independent words as a bridge.
In theway, the cluster can be used to reduce the gapbetween domain-specific words of two domains.Dredze et al (2010) combined classifier weightsusing confidence-weighted learning, which repre-sented the covariance of the weight vectors.
Xiaet al (2013a) proposed an instance selection andinstance weighting method for cross-domain sen-timent classification.
After that, Xia et al (2013b)proposed a feature ensemble plus sample selectionmethod to further improve the sentiment classifi-cation adaptation.
Zhou et al (Zhou et al, 2015b)proposed to bridge the domain gap with the help oftopical correspondence.
Li et al (2009) proposedto transfer common lexical knowledge across do-mains via matrix factorization techniques.
Zhouet al (2015a) further improved the matrix fac-torization techniques via a regularization term onthe pivots and domain-specific words, ensuringthat the pivots capture only correspondence as-pects and the domain-specific words capture onlyindividual aspects.
Li and Zong (2008) pro-posed the multi-label consensus training approachwhich combined several base classifiers trainedwith SCL.
Chen et al (2012) proposed a domainadaptation algorithm based on sample and featureselection.
Li et al (2013) proposed an active learn-ing algorithm for cross-domain sentiment classifi-cation.
Xiao and Guo (2013) investigated the on-line active domain adaptation problem in a novelbut practical setting where the labels can be ac-quired with a lower cost in the source domain thanin the target domain.There has also been research in exploring care-ful structuring of features or prior knowledge fordomain adaptation.
Daum?e III (2007) proposed akernel-mapping function which maps both sourceand target domains data to a high-dimensional fea-ture space so that data points from the same do-main are twice as similar as those form differentdomains.
Dai et al (2008) proposed translatedlearning which used a language model to link theclass labels to the features in the source domain,which in turn is translated to the features in thetarget domain.
Xia et al (2010) proposed a POS-based ensemble model for cross-domain sentimentclassification.
Xiao et al (2013) proposed a super-vised representation learning method to tackle do-main adaptation by inducing predictive latent fea-tures based on supervised word clustering.
He etal.
(2011) employed a joint sentiment-topic modelfor cross-domain sentiment classification; Bolle-gala et al (2011) used a sentiment sensitive the-saurus to perform cross-domain sentiment classi-fication.
Xiao and Guo (2015) proposed to learndistributed state representations for cross-domainsequence predictions.Recently, some efforts have been initiated onlearning robust feature representations with deepneural networks (DNNs) for cross-domain nat-ural language processing.
Glorot et al (2011)and Chen et al (2012) proposed to use deeplearning for cross-domain sentiment classification.Most recently, Yang and Eisenstein (2014) pro-posed an unsupervised domain adaptation methodwith marginalized structured dropout.
Further-more, Yang and Eisenstein (2015) proposed touse feature embeddings with metadata domain at-tributes for multi-domain adaptation.
In this paper,324our proposed approach BTDNNs tackles the do-main discrepancy with a linear data constructionmanner, which can effectively model the domain-specific features as well as the commonality ofdomains.
Deep learning techniques have alsobeen proposed to heterogeneous transfer learn-ing (Socher et al, 2013; Zhou et al, 2014; Kanet al, 2015; Long et al, 2015), where knowledgeis transferred from one modality to another basedon the correspondences at hand.
Our proposedframework can be considered as a more generalcase, where the bias of the correspondences be-tween the source and target domains is constrainedwith a linear data reconstruction manner.Besides, other researchers also explore theDNNs for sentiment analysis (Socher et al, 2011;Tang et al, 2014; Tang et al, 2015; Zhai andZhang, 2016; Chandar et al, 2014).
However,all these methods focus on the sentiment analysiswithout considering the domain discrepancy.
Inthis paper, we focus on domain adaptation for sen-timent classification with a different model formu-lation and task definition.3 Bi-Transferring Deep Neural Networks3.1 Problem DefinitionGiven two domains Xsand Xt, where XsandXtare referred to a source domain and a targetdomain, respectively.
Suppose we have a set oflabeled sentiment examples as well as some un-labeled examples in the source domain Xswithsize ns, containing terms from a vocabulary Vwith size m. The examples in the source domainXscan be represented as a term-document matrixXs= {xs1, ?
?
?
,xsns} ?
Rm?ns, with their senti-ment labels ys= {ys1, ?
?
?
,ysns}, where xsi?
Rmis the feature representation of the i-th source do-main example with a tf-idf weight of the corre-sponding term and ysi?
{+1,?1} is its sentimentlabel.1Similarly, suppose we have a set of unlabeledexamples in the target domain Xtwith size nt,containing terms from a vocabulary V with sizem.
The examples in target domain Xtcan alsobe represented as a term-document matrix Xt={x(t)1, ?
?
?
,x(t)nt} ?
Rm?nt, where each exampledenotes a tf-idf weight of the corresponding term.The task of cross-domain sentiment classificationis to learn a robust classifier to predict the polarity1We use upper case and lower case characters representthe matrices and vectors respectively throughout the paper.of unseen examples from Xt.
Note that we onlyconsider one source domain and one target domainin this paper.
However, our proposed algorithm isa general framework and can be easily adapted tomulti-domain problems.3.2 Basic Auto-EncoderAn auto-encoder is an unsupervised neural net-work which is trained to reconstruct a given in-put vector from its latent representation (Bengioet al, 2007).
It can be seen as a special neuralnetwork with three layers: the input layer, the la-tent layer, and the reconstruction layer.
An auto-encoder contains two parts: encoder and decoder.The encoder, denoted as f , attempts to map an in-put vector x ?
Rm?1to the latent representationz ?
Rk?1, in which k is the number of neurons inthe latent layer.
Usually, f is a nonlinear functionas follows:z = f(x) = se(Wx + b) (1)where seis the activation function of the en-coder, whose input is called the activation func-tion, which is usually non-linear, such as sigmoidfunction or tanh function is a linear transform pa-rameter, and b ?
Rk?1is the basis.The decoder, denoted as g, tries to map the la-tent representation z back to a reconstruction:g(z) = sd(W?z + b?)
(2)Similarly, sdis the activation function of the de-coder with parameters {W?,b?
}.The training objective is the determination ofparameters {W,b} and {W?,b?}
that minimizethe average reconstruction errors:L = minW,b,W?,b?N?i=1??xi?
g(f(xi))?
?22(3)where xirepresents the i-th one of N training ex-amples.
Parameters {W,b} and {W?,b?}
can beoptimized by stochastic or mini-batch gradient de-scent.
By minimizing the reconstruction error, werequire the latent features should be able to recon-struct the original input as much as possible.3.3 Bi-Transferring Deep Neural NetworksThe traditional auto-encoder in subsection 3.2 at-tempts to reconstruct the input itself, which isusually used for feature representation learning.Nevertheless, our proposed bi-transferring deepneural networks (BTDNNs) attempts to transfer325examples between domains to deal with the do-main discrepancy, with the inspiration of DNNsin computer vision (Kan et al, 2015).
Moti-vated by the successful application in computervision (Kan et al, 2015), we construct the archi-tecture of BTDNNs with one encoder fe, and twodecoders, gsand gtshown in Figure 1, which cantransform an input example to the source domainand the target domain respectively.2Specifically, the encoder fctries to map an inputexample x into the latent feature representation z,which is common to both the source and target do-mains as follows:z = fc(x) = se(Wcx + bc) (4)The decoder gsattempts to map the latent rep-resentation to the source domain, and the decodergtattempts to map the latent representation to thetarget domain as follows:gs(x) = sd(Wsz + bs) (5)gt(x) = sd(Wtz + bt) (6)where se(?)
and sd(?)
are the element-wise nonlin-ear activation function, e.g., sigmoid or tanh func-tion, Wcand bcare the parameters for encoder fc,Wsand bsare the parameters for decoder gs, Wtand btare the parameters for decoder gt.Following the literature (Kan et al, 2015), weattempt to map the source domain examples Xstothe source domain (e.g., Xsitself) with an encoderfcand a decoder gs.
Similarly, given an encoderfcand a decoder gt, we aim to map the sourcedomain examples Xsto the target domain.
Al-though it is unknown what the mapped exampleslook like, they are expected to follow the similardistribution as the target domain.
This kind of dis-tribution consistency between two domains can becharacterized from the perspective of a linear datareconstruction manner.The two domains Xsand Xtcan be gener-ally reconstructed from each other, and their dis-tances can be used to measure the domain discrep-ancy.
Following the literature (He et al, 2012),BTDNNs attempt to represent a transferred sourcedomain gt(fc(xsi)) with a linear reconstructionfunction from the target domain:?gt(fc(xsi))?Xt?ti)?22(7)2In the implementation, we use the stacked denoisingauto-encoders (SDA) (Vincent et al, 2008) to model thesource and the target domain data.where ?tiis the coefficients for the reconstructionof transferred source domain examples.
Equa-tion (7) enforces that each example of transferreddomain is consistent with that of target domain,which ensures that the transferred source domainfollows the similar distribution as the target do-main.
The overall objective for the examples ofsource domain Xscan be formulated as below:minfc,gs,gt,?si?Xs?
gs(fc(Xs))?22+ ?gt(fc(Xs))?XtBt)?22s.t.
?
?ti?22< ?, Bt= [?t1, ?t2, ?
?
?
, ?tns]T?
Rns?ntwhere gs(fc(Xs) = [gs(fc(xs1)), ?
?
?
,gs(fc(xsnt))]and gt(fc(Xs) = [gt(fc(xt1)),gt(fc(xtns))].
Thesame simplifications are used hereinafter ifwithout misunderstanding.Similarly, for the examples of target domain Xt,with encoder fcand decoder gtthey should bemapped on the target domain.
Also, with encoderfcand decoder gsthey should be mapped to thesource domain, where they can be reconstructedby the source domain examples from the point ofview of a linear data reconstruction manner (He etal., 2012), so as to ensure a similar distribution be-tween the source domain and the transferred targetdomain.
The overall objective for the examples oftarget domain Xtcan be written as:minfc,gs,gt,?ti?Xt?
gt(fc(Xt))?22+ ?gs(fc(Xt))?XsBs)?22s.t.
?
?sj?22< ?, Bs= [?s1, ?s2, ?
?
?
, ?snt]T?
Rnt?nsCombining the above equations, the overall ob-jective of BTDNNs can be formulated as follows:minfc,gs,gt,Bs,Bt?Xs?
gs(fc(Xs))?22+ ?gt(fc(Xs))?XtBt)?22+ ?Xt?
gt(fc(Xt))?22+ ?gs(fc(Xt))?XsBs)?22(8)+ ?(ns?i=1??ti?22+nt?j=1?
?sj?22)where ?
is a regularization parameter controllingthe amount of shrinkage.
With the optimizationof equation (8), our proposed approach BTDNNscan map any input examples to the source and tar-get domains respectively.
Especially, the sourcedomain examples Xscan transferred to the tar-get domain along with their sentiment labels.
Thetransferred source domain data gt(fs(Xs)) sharethe similar distribution as the target domain, soany supervised method can be used to learn a clas-sifier for sentiment classification in the target do-main.
In this paper, a linear support vector ma-chine (SVM) (Fan et al, 2008) is employed forbuilding sentiment classification models.3263.4 Learning AlgorithmNote that the optimization problem in equation (8)is not convex in variables {fc,gs,gt,Bs,Bt} to-gether.
However, when considering one variableat a time, the cost function turns out to be con-vex.
For example, given {gs,gt,Bs,Bt}, the costfunction is a convex function w.r.t.
fc.
Therefore,although we cannot expect to get a global min-imum of the above problem, we shall develop asimple and efficient optimization algorithm via al-ternative iterations.3.4.1 Optimize {fc, gs, gt} given {Bs, Bt}When Bsand Btare fixed, the objective functionin equation (8) can be formulated as:minfc,gs,gt?Xs?
gs(fc(Xs))?22+ ?gt(fc(Xs))?
?Xt)?22+ ?Xt?
gt(fc(Xt))?22+ ?gs(fc(Xt))?
?Xs)?22(9)where?Xs= XsBsand?Xt= XtBt.
Equation(9) can easily optimized by gradient descent as thebasic auto-encoder (Bengio et al, 2007).3.4.2 Optimize {Bs, Bt} given {fc, gs, gt}When {fc, gs, gt} are fixed, the objective functionin equation (8) can be written as:minBs,Bt?Gt?XtBt)?22+ ?Gs?XsBs)?22+ ?(ns?i=1??ti?22+nt?j=1?
?sj?22)where gs(fc(Xt)) = Gs= [gs1, ?
?
?
,gsnt] andgt(fc(Xs)) = Gt= [gt1, ?
?
?
,gtns].
Since GsandGtare independent with each other, so they can beoptimized independently.
The optimization of Gswith other variables fixed is a least squares prob-lem with `2-regularization.
It can also be decom-posed into ntoptimization problems, with eachcorresponding to one ?sjand can be solved in par-allel:min?sj?gsj?Xs?sj?22+ ??
?sj?22(10)for j = 1, 2, ?
?
?
, nt.
It is a standard `2-regularizedleast squares problem and the solution is:?sj=(XTsXs+ ?I)?1XTsgsj(11)where I is an identity matrix with all entries equalto 1.Similarly, The optimization of Gtcan also bedecomposed into ns`2-regularized least squaresproblems and the solution of each one is:?ti=(XTtXt+ ?I)?1XTtgti(12)for i = 1, 2, ?
?
?
, ns.
We repeat the above equa-tions until fc, gs, gt, Bsand Btconverge or amaximum number of iterations is exceeded.3.5 Algorithm ComplexityIn this section, we analyze the computationalcomplexity of the learning algorithm described inequations (9), (11) and (12).
Besides express-ing the complexity of the algorithm using big Onotation, we also count the number of arithmeticoperations to provide more details about the runtime.
Computational complexity of learning ma-trix Gsis O(m ?
ns?
k) per iteration.
Simi-larly, for each iteration, learning matrices GttakesO(m ?
nt?
k).
Learning matrices Bsand Bttakes O(m2?ns) and O(m2?nt) operations periteration.
In real applications, we have k  m.Therefore, the overall complexity of the algorithm,dominated by computation of matrices Bsand Bt,is O(m2?
n) where n = max(ns, nt).4 Experiments4.1 Data SetDomain adaptation for sentiment classification hasbeen widely studied in the NLP community.
Alarge majority experiments are performed on thebenchmark made of reviews of Amazon productsgathered by Blitzer et al (2006).
This data setcontains 4 different domains: Book (B), DVDs(D), Electronics (E) and Kitchen (K).
For sim-plicity and comparability, we follow the conven-tion of (Blitzer et al, 2006; Pan et al, 2010; Glorotet al, 2011; Xiao and Guo, 2013) and only con-sider the binary classification problem whether areview is positive (higher than 3 stars) or negative(3 stars or lower).
There are 1000 positive and1000 negative reviews for each domain, as wellas approximately 4,000 unlabeled reviews (vary-ing slightly between domains).
The positive andnegative reviews are also exactly balanced.Following the literature (Pan et al, 2010), wecan construct 12 cross-domain sentiment classifi-cation tasks: D?
B, E?
B, K?
B, K?
E, D?E, B?
E, B?
D, K?
D, E?
D, B?
K, D?K, E?
K, where the word before an arrow corre-sponds with the source domain and the word afteran arrow corresponds with the target domain.
Tobe fair to other algorithms that we compare to, weuse the raw bag-of-words unigram/bigram featuresas their input and pre-process with tf-idf (Blitzeret al, 2006).
Table 1 presents the statistics of thedata set.327B->D E->D K->D717273747576777879808182Accuracy (%)baselineSCLMCTSFAPJNMFSDAmSDATLDABTDNNsD->B E->B K->B7172737475767778798081Accuracy (%)baselineSCLMCTSFAPJNMFSDAmSDATLDABTDNNsB->E D->E K->E7274767880828486Accuracy (%)baselineSCLMCTSFAPJNMFSDAmSDATLDABTDNNsB->K D->K E->K7476788082848688Accuracy (%)baselineSCLMCTSFAPJNMFSDAmSDATLDABTDNNsFigure 2: Average results for cross-domain sentiment classification on the Amazon product benchmarkof 4 domains.Domain #Train #Test #Unlab.
% Neg.Books 1600 400 4465 50%DVDs 1600 400 5945 50%Electronics 1600 400 5681 50%Kitchen 1600 400 3586 50%Table 1: Amazon review statistics.
This table de-picts the number of training, testing and unlabeledreviews for each domain, as well as the portion ofnegative training reviews of the data set.4.2 Compared MethodsAs a baseline method, we train a linear SVM (Fanet al, 2008) on the raw bag-of-words representa-tion of the labeled source domain and test it on thetarget domain.
In the original paper regarding thebenchmark data set, Blitzer et al (2006) adaptedStructural Correspondence Learning (SCL) forsentiment analysis.
Li and Zong (2008) proposedthe Multi-label Consensus Training (MCT) ap-proach which combined several base classifierstrained with SCL.
Pan et al (2010) first used aSpectral Feature Alignment (SFA) algorithm toalign words from the source and target domainsto help bridge the gap between them.
Zhou etal.
(2015a) proposed a method called PJNMF,which linked heterogeneous input features withpivots via joint non-negative matrix factorization.Recently, some efforts have been initiated onlearning robust feature representations with DNNsfor cross-domain sentiment classification.
Glo-rot et al (2011) first employed stacked Denois-ing Auto-encoders (SDA) to extract meaningfulrepresentation for domain adaptation.
Chen etal.
(2012) proposed marginalized SDA (mSDA)that addressed the high computational cost andlack of scalability to high-dimensional features.Zhuang et al (2015) proposed a state-of-the-artmethod called transfer learning with deep auto-encoders (TLDA).For SCL, PJNMF, SDA, mSDA and TLDA, weuse the source codes provided by the authors.
ForSFA and MCT, we re-implement them based onthe original papers.
The above methods serve ascomparisons in our empirical evaluation.
For faircomparison, all hyper-parameters are set by 5-foldcross validation on the training set from the sourcedomain.3For our proposed BTDNNs, the number3We keep the default value of some of the parameters inSCL and SFA, e.g., the number of stop-words removed andstemming parameters ?
as they were already tuned for this328of hidden neurons is set as 1000, the regularizationparameter ?
is tuned via 5-fold cross-validation.For SDA, mSDA, TLDA and BTDNNs, we canconstruct the classifiers for the target domain intwo ways.
The first way is directly to use thestacking SVM on top of the output of the hiddenlayer.
The second way is to apply the standardSVM to train a classifier for source domain in theembedding space.
Then the classifiers is appliedto predict sentiment labels for target domain data.For fair comparison with the shallow models, wechoose the second way in this paper.Figure 2 shows the accuracy of classification re-sults for all methods and for all source-target do-main pairs.
We can check that all compared meth-ods achieve the similar performance with the re-sults reported in the original papers.
From Fig-ure 2, we can see that our proposed approachBTDNNs outperforms all other eight comparisonmethods in general.
The baseline performspoorly on all the 12 tasks, while the other sevendomain adaptation methods, SCL, MCT, SFA,PJNMF, SDA, mSDA and TLDA, consistentlyoutperform the baseline method across all the12 tasks, which demonstrates that the transferredknowledge from the source domain to the tar-get domain is useful for sentiment classification.Nevertheless, the improvements achieved by theseseven methods over the baseline are muchsmaller than the proposed approach BTDNNs.Surprisingly, we note that the deep learningbased methods (SDA, mSDA and TLDA) performworse than our approach, the reason may be thatSDA, mSDA and TLDA learn the unified domain-invariable feature representations by combiningthe source domain data and that of the target do-main data together, which cannot well characterizethe domain-specific features as well as the com-monality of domains.
On the contrary, our pro-posed BTDNNs ensures the feasibility of transfer-ring between domains, and the distribution con-sistency between the transferred domain and thedesirable domain is constrained with a linear datareconstruction manner.We also conduct significance tests for our pro-posed approach BTDNNs and the state-of-the-artmethod (TLDA) using a McNemar paired test forlabeling disagreements (Gillick and Cox, 1989).In general, the average result on the 12 source-target domain pairs indicates that the differencebenchmark set by the authors.1.6 1.65 1.7 1.75 1.8 1.85 1.9 1.95 21.61.651.71.751.81.851.91.952Proxy A-distance on raw inputProxy A-distance onBTDNNsEKBDDE DKBKBEFigure 3: Proxy A-distance between domains ofthe Amazon benchmark for the 6 different pairs.between BTDNNs and TLDA is mildly significantwith p < 0.08.
Furthermore, we also conduct theexperiments on a much larger industrial-strengthdata set of 22 domains (Glorot et al, 2011).
Thepreliminary results show that BTDNNs signifi-cantly outperforms TLDA (p < 0.05).
Therefore,we will report our detailed results and discussionsin our future work.4.3 Domain DivergenceIn this subsection, we look into how similar twodomains are to each other.
Ben-David et al (2006)showed that the A-distance as a measure of howdifferent between the two domains.
They hypoth-esized that it should be difficult to discriminate be-tween the source and target domains in order tohave a good transfer between them.
In practice,computing the exact A-distance is impossible andone has to compute a proxy.
Similar to (Glorotet al, 2011), the proxy for the A-distance is thendefined as 2(1 ?
2), where  is the generaliza-tion error of a linear SVM classifier trained on thebinary classification problem to distinguish inputsbetween the two domains.Figure 3 presents the results for each pair ofdomains.
Surprisingly, the distance is increasedwith the help of new feature representations, e.g.,distinguishing between domains becomes easierwith the BTDNNs features.
We explain this effectthrough the fact that BTDNNs can ensure the fea-sibility of transferring between domains, and thedistribution consistency between the transferreddomain and the desirable domain is constrainedwith a linear data reconstruction manner, whichcan learn a generally better representations for theinput data.
This helps both tasks, distinguish-ing between domains and sentiment classification329(e.g., in the book domain BTDNNs might inter-polate the feature ?exciting?
from ?boring?, bothare not particularly relevant for sentiment classifi-cation but might help distinguish the review fromthe Electronic domain.
).5 Conclusions and Future WorkIn this paper, we propose a novel Bi-TransferringDeep Neural Networks (BTDNNs) for cross-domain sentiment classification.
The proposedBTDNNs attempts to transfer the source domainexamples to the target domain, and also trans-fer the target domain examples to the source do-main.
The linear transformation of BTDNNs en-sures the feasibility of transferring between do-mains, and the distribution consistency betweenthe transferred domain and the desirable domain isconstrained with a linear data reconstruction man-ner.
Experimental results show that BTDNNs sig-nificantly outperforms the several baselines, andachieves an accuracy which is competitive with thestate-of-the-art method for sentiment classificationadaptation.There are some ways in which this researchcould be continued.
First, since deep learningmay obtain better generalization on large-scaledata sets (Bengio, 2009), a straightforward pathof the future research is to apply the proposedBTDNNs for domain adaptation on a much largerindustrial-strength data set of 22 domains (Glorotet al, 2011).
Second, we will try to investigatethe use of the proposed approach for other kindsof data set, such as 20 newsgroups and Reuters-21578 (Li et al, 2012; Zhuang et al, 2013).AcknowledgmentsThis work was supported by the National NaturalScience Foundation of China (No.
61303180 andNo.
61573163), the Fundamental Research Fundsfor the Central Universities (No.
CCNU15ZD003and No.
CCNU16A02024), and also supportedby a Discovery grant from the Natural Sciencesand Engineering Research Council (NSERC) ofCanada and an NSERC CREATE award.
Wethank the anonymous reviewers for their insight-ful comments.ReferencesRie Kubota Ando and Tong Zhang.
2005.
A frame-work for learning predictive structures from multi-ple tasks and unlabeled data.
J. Mach.
Learn.
Res.,6:1817?1853.Shai Ben-David, John Blitzer, Koby Crammer, and Fer-nando Pereira.
2006.
Analysis of representationsfor domain adaptation.
In NIPS, pages 137?144.Yoshua Bengio, Pascal Lamblin, Dan Popovici, HugoLarochelle, Universite De Montreal, and MontrealQuebec.
2007.
Greedy layer-wise training of deepnetworks.
In NIPS, pages 153?160.Yoshua Bengio.
2009.
Learning deep architectures forAI.
Foundations and Trends in Machine Learning,2(1):1?127.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In EMNLP, pages 120?128.John Blitzer, M. Dredze, and Fernando Pereira.
2007.Biographies, bollywood, boom-boxes and blenders:domain adaptation for sentiment classification.
InEMNLP, pages 120?128.Danushka Bollegala, David Weir, and John Carroll.2011.
Using multiple sources to construct a senti-ment sensitive thesaurus for cross-domain sentimentclassification.
In ACL, pages 132?141.Sarath Chandar, Stanislas Lauly, Hugo Larochelle,Mitesh M Khapra, Balaraman Ravindran, VikasRaykar, and Amrita Saha.
2014.
An autoencoderapproach to learning bilingual word representations.In NIPS, pages 1?9.Minmin Chen, John Blitzer, and Kilian Weinberger.2011a.
Co-training for domain adaptation.
In NIPS,pages 1?9.Minmin Chen, Kilian Weinberger, and Yixin Chen.2011b.
Automatic feature decomposition for singleview co-training.
In ICML, pages 953?960.Minmin Chen, Zhixiang Xu, Kilian Weinberger, andFei Sha.
2012.
Marginalized denoising autoen-coders for domain adaptation.
In ICML, pages 767?774.W.
Dai, Y. Chen, G. Xue, Q. Yang, and Y. Yu.
2008.Translated learning: transfer learning across differ-ent feature spaces.
In NIPS, pages 353?360.Hal Daume III.
2007.
Frustratingly easy domain adap-tation.
In ACL, pages 256?263.Mark Dredze, Alex Kulesza, and Koby Crammer.2010.
Multi-domain learning by confidence-weighted parameter combination.
Machine Learn-ing, 79(12):123?149.R.
Fan, Chang K., Hsieh C., Wang X., and Lin C. 2008.Liblinear: A library for large linear classification.
J.Mach.
Learn.
Res., 9:1871?1874.330L.
Gillick and S. Cox.
1989.
Some statistical issuesin the comparison of speech recoginition algorithms.In ICASSP, pages 532?535.Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011.
Domain adaptation for large-scale sentimentclassification: A deep learning approach.
In ICML,pages 513?520.Yulan He, Chenghua Lin, and Harith Alani.
2011.Automatically extracting polarity-bearing topics forcross-domain sentiment classification.
In ACL,pages 123?131.Zhanying He, Chun Chen, Bu, Jiajun, Can Wang, Li-jun Zhang, Deng Cai, and Xiaofei He.
2012.
Doc-ument summarization based on data reconstruction.In AAAI, pages 620?626.J.
Huang, A. Smola, A. Gretton, K. Bordwardt, andB.
Scholkopf.
2007.
Correcting samples selectionbias by unlabeled data.
In NIPS, pages 601?608.J.
Jiang and C. Zhai.
2007.
Instance weighting fordomain adaptation in nlp.
In ACL, pages 264?271.Meina Kan, Shiguang Shan, and Xilin Chen.
2015.Bi-shifting auto-encoder for unsupervised domainadaptation.
In ICCV, pages 3846?3854.Abhishek Kumar, Avishek Saha, and Hal Daum?e III.2010.
A co-regularization based semi-superviseddomain adaptation.
In NIPS, pages 478?486.Shoushan Li and Chengqing Zong.
2008.
Multi-domain adaption for sentiment classification: Us-ing multiple classifier combining classification.
InNLPKE.Tao Li, Vikas Sindhwani, Chris H. Q. Ding, andYi Zhang 0005.
2009.
Knowledge transformationfor cross-domain sentiment classification.
In SIGIR,pages 716?717.Lianghao Li, Xiaoming Jin, and Mingsheng Long.2012.
Topic correlation analysis for cross-domaintext classification.
In AAAI, pages 998?1004.Shoushan Li, Yunxia Xue, Zhongqing Wang, andGuodong Zhou.
2013.
Active learning for cross-domain sentiment classification.
In IJCAI, pages2127?2133.B.
Liu.
2012.
Sentiment analysis and opinion mining.Morgan & Claypool Publishers.Mingsheng Long, Yue Cao, Jianmin Wang, andMichael I. Jordan.
2015.
Learning transferablefeatures with deep adaptation networks.
In ICML,pages 97?105.T.
Mansour, M. Mohri, and A. Rostamizadeh.
2009.Domain adatation with multiple sources.
In NIPS,pages 264?271.Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, QiangYang, and Zheng Chen.
2010.
Cross-domain sen-timent classification via spectral feature alignment.In WWW, pages 751?760.Bo Pang and Lillian Lee.
2008.
Opinion miningand sentiment analysis.
Found.
Trends Inf.
Retr.,2(12):1?135.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
sentiment classification using machine learningtechniques.
In EMNLP, pages 79?86.Benjamin Snyder and Regina Barzilay.
2007.
Multipleaspect ranking using the good grief algorithm.
InNAACL, pages 300?307.Richard Socher, Jeffrey Pennington, Eric H. Huang,Andrew Y. Ng, and Christopher D. Manning.
2011.Semi-supervised recursive autoencoders for predict-ing sentiment distributions.
In EMNLP, pages 151?161.Richard Socher, Milind Ganjoo, Christopher D. Man-ning, and Andrew Y. Ng.
2013.
Zero-shot learningthrough cross-modal transfer.
In NIPS, pages 935?943.Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, TingLiu, and Bing Qin.
2014.
Learning sentiment-specific word embedding for twitter sentiment clas-sification.
In ACL, pages 1555?1565.Duyu Tang, Bing Qin, and Ting Liu.
2015.
Documentmodeling with gated recurrent neural network forsentiment classification.
In EMNLP, pages 1422?1432.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Get outthe vote: Determining support or opposition fromcongressional floor-debate transcripts.
In EMNLP,pages 327?335.Pascal Vincent, Hugo Larochelle, Yoshua Bengio, andPierre-Antoine Manzagol.
2008.
Extracting andcomposing robust features with denoising autoen-coders.
In ICML, pages 1096?1103.Rui Xia, , and Chengqing Zong.
2010.
A pos-basedensemble model for cross-domain sentiment classi-fication.
In IJCNLP, pages 614?622.Rui Xia, Xuelei Hu, Jianfeng Lu, and Chengqing Zong.2013a.
Instance selection and instance weighting forcross-domain sentiment classification via pu learn-ing.
In IJCAI, pages 2276?2182.Rui Xia, Chengqing Zong, Xuelei Hu, and CambriaErik.
2013b.
Feature ensemble plus sample selec-tion: Domain adaptation for sentiment classification.IEEE Intelligent Systems, 28(3):10?18.Min Xiao and Yuhong Guo.
2013.
Online active learn-ing for cost-sensitive domain adaptation.
In CoNLL,pages 1?9.331Min Xiao and Yuhong Guo.
2015.
Learning hiddenmarkov models with distributed state representationsfor domain adaptation.
In ACL, pages 524?529.Min Xiao, Feipeng Zhao, and Yuhong Guo.
2013.Learning latent word representations for domainadaptation using supervised word clustering.
InEMNLP, pages 152?162.Yi Yang and Jacob Eisenstein.
2014.
Fast easy unsu-pervised domain adaptation with marginalized struc-tured dropout.
In ACL, pages 538?544.Yi Yang and Jacob Eisenstein.
2015.
Unsupervisedmulti-domain adaptation with feature embeddings.In NAACL, pages 672?682.Shuangfei Zhai and Zhongfei (Mark) Zhang.
2016.Semi-supervised autoencoder for sentiment analy-sis.
In AAAI, pages 1394?1400.Guangyou Zhou, Li Cai, Jun Zhao, and Kang Liu.2011.
Phrase-based translation model for questionretrieval in community question answer archives.
InACL, pages 653?662.Joey Tianyi Zhou, Sinno Jialin Pan, IvorW.
Tsang,and Yan Yan.
2014.
Hybrid heterogeneous trans-fer learning through deep learning.
In AAAI, pages2213?2219.Guangyou Zhou, Tingting He, Wensheng Wu, and Xi-aohua Hu.
2015a.
Linking heterogeneous input fea-tures with pivots for domain adaptation.
In IJCAI,pages 1419?1425.Guangyou Zhou, Yin Zhou, Xiyue Guo, Xinhui Tu, andTingting He.
2015b.
Cross-domain sentiment clas-sification via topical correspondence transfer.
Neu-rocomputing, 159:298?305.Fuzhen Zhuang, Ping Luo, Peifeng Yin, Qing He, andZhongzhi Shi.
2013.
Concept learning for cross-domain text classification: A general probabilisticframework.
In IJCAI, pages 1960?1966.Fuzhen Zhuang, Xiaohu Cheng, Ping Luo, Sinno JialinPan, and Qing He.
2015.
Supervised representationlearning: Transfer learning with deep autoencoders.In IJCAI, pages 4119?4125.332
