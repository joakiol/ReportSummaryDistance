Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
112?121, Prague, June 2007. c?2007 Association for Computational LinguisticsA Comparative Evaluation of Deep and Shallow Approaches to theAutomatic Detection of Common Grammatical ErrorsJoachim Wagner, Jennifer Foster, and Josef van Genabith?National Centre for Language TechnologySchool of Computing, Dublin City University, Dublin 9, Ireland{jwagner, jfoster, josef}@computing.dcu.ieAbstractThis paper compares a deep and a shallowprocessing approach to the problem of clas-sifying a sentence as grammatically well-formed or ill-formed.
The deep processingapproach uses the XLE LFG parser and En-glish grammar: two versions are presented,one which uses the XLE directly to performthe classification, and another one whichuses a decision tree trained on features con-sisting of the XLE?s output statistics.
Theshallow processing approach predicts gram-maticality based on n-gram frequency statis-tics: we present two versions, one whichuses frequency thresholds and one whichuses a decision tree trained on the frequen-cies of the rarest n-grams in the input sen-tence.
We find that the use of a decision treeimproves on the basic approach only for thedeep parser-based approach.
We also showthat combining both the shallow and deepdecision tree features is effective.
Our eval-uation is carried out using a large test set ofgrammatical and ungrammatical sentences.The ungrammatical test set is generated au-tomatically by inserting grammatical errorsinto well-formed BNC sentences.1 IntroductionThis paper is concerned with the task of predict-ing whether a sentence contains a grammatical er-ror.
An accurate method for carrying out automatic?Also affiliated to IBM CAS, Dublin.grammaticality judgements has uses in the areas ofcomputer-assisted language learning and grammarchecking.
Comparative evaluation of existing errordetection approaches has been hampered by a lackof large and commonly used evaluation error cor-pora.
We attempt to overcome this by automaticallycreating a large error corpus, containing four dif-ferent types of frequently occurring grammatical er-rors.
We use this corpus to evaluate the performanceof two approaches to the task of automatic error de-tection.
One approach uses low-level detection tech-niques based on POS n-grams.
The other approachis a novel parser-based method which employs deeplinguistic processing to discriminate grammatical in-put from ungrammatical.
For both approaches, weimplement a basic solution, and then attempt to im-prove upon this solution using a decision tree clas-sifier.
We show that combining both methods im-proves upon the individual methods.N-gram-based approaches to the problem of errordetection have been proposed and implemented invarious forms by Atwell(1987), Bigert and Knutsson(2002), and Chodorow and Leacock (2000) amongstothers.
Existing approaches are hard to comparesince they are evaluated on different test sets whichvary in size and error density.
Furthermore, most ofthese approaches concentrate on one type of gram-matical error only, namely, context-sensitive or real-word spelling errors.
We implement a vanilla n-gram-based approach which is tested on a very largetest set containing four different types of error.The idea behind the parser-based approach to er-ror detection is to use a broad-coverage hand-craftedprecision grammar to detect ungrammatical sen-112tences.
This approach exploits the fact that a pre-cision grammar is designed, in the traditional gen-erative grammar sense (Chomsky, 1957), to dis-tinguish grammatical sentences from ungrammati-cal sentences.
This is in contrast to treebank-basedgrammars which tend to massively overgenerate anddo not generally aim to discriminate between thetwo.
In order for our approach to work, the coverageof the precision grammars must be broad enough toparse a large corpus of grammatical sentences, andfor this reason, we choose the XLE (Maxwell andKaplan, 1996), an efficient and robust parsing sys-tem for Lexical Functional Grammar (LFG) (Kaplanand Bresnan, 1982) and the ParGram English gram-mar (Butt et al, 2002) for our experiments.
This sys-tem employs robustness techniques, some borrowedfrom Optimality Theory (OT) (Prince and Smolen-sky, 1993), to parse extra-grammatical input (Franket al, 1998), but crucially still distinguishes betweenoptimal and suboptimal solutions.The evaluation corpus is a subset of an un-grammatical version of the British National Cor-pus (BNC), a 100 million word balanced corpus ofBritish English (Burnard, 2000).
This corpus is ob-tained by automatically inserting grammatical errorsinto the original BNC sentences based on an analysisof a manually compiled ?real?
error corpus.This paper makes the following contributions tothe task of automatic error detection:1.
A novel deep processing XLE-based approach2.
An effective and novel application of decisiontree machine learning to both shallow and deepapproaches3.
A novel combination of deep and shallow pro-cessing4.
An evaluation of an n-gram-based approach ona wider variety of errors than has previouslybeen carried out5.
A large evaluation error corpusThe paper is organised as follows: in Section 2,we describe previous approaches to the problem oferror detection; in Section 3, a description of theerror corpus used in our evaluation experiments ispresented, and in Section 4, the two approaches toerror detection are presented, evaluated, combinedand compared.
Section 5 provides a summary andsuggestions for future work.2 Background2.1 Precision GrammarsA precision grammar is a formal grammar designedto distinguish ungrammatical from grammatical sen-tences.
This is in contrast to large treebank-inducedgrammars which often accept ungrammatical input(Charniak, 1996).
While high coverage is required,it is difficult to increase coverage without also in-creasing the amount of ungrammatical sentencesthat are accepted as grammatical by the grammar.Most publications in grammar-based automatic errordetection focus on locating and categorising errorsand giving feedback.
Existing grammars are re-used(Vandeventer Faltin, 2003), or grammars of limitedsize are developed from scratch (Reuer, 2003).The ParGram English LFG is a hand-craftedbroad-coverage grammar developed over severalyears with the XLE platform (Butt et al, 2002).
TheXLE parser uses OT to resolve ambiguities (Princeand Smolensky, 1993).
Grammar constraints re-sulting in rare constructions can be marked as ?dis-preferred?
and constraints resulting in common un-grammatical constructions can be marked as ?un-grammatical?.
The use of constraint ordering andmarking increases the robustness of the grammar,while maintaining the grammatical / ungrammati-cal distinction (Frank et al, 1998).
The EnglishResource Grammar (ERG) is a precision Head-Driven Phrase Structure Grammar (HPSG) of En-glish (Copestake and Flickinger, 2000; Pollard andSag, 1994).
Its coverage is not as broad as the XLEEnglish grammar.
Baldwin et al (2004) propose amethod to identify gaps in the grammar.
Blunsomand Baldwin (2006) report ongoing development.There has been previous work using the ERG andthe XLE grammars in the area of computer-assistedlanguage learning.
Bender et al (2004) use a ver-sion of the ERG containing mal-rules to parse ill-formed sentences from the SST corpus of Japaneselearner English (Emi et al, 2004).
They then usethe semantic representations of the ill-formed inputto generate well-formed corrections.
Khader et al(2004) study whether the ParGram English LFG canbe used for computer-assisted language learning by113adding additional OT marks for ungrammatical con-structions observed in a learner corpus.
However,the evaluation is preliminary, on only 50 test items.2.2 N-gram MethodsMost shallow approaches to grammar error detectionoriginate from the area of real-word spelling errorcorrection.
A real-word spelling error is a spellingor typing error which results in a token which is an-other valid word of the language in question.The (to our knowledge) oldest work in this areais that of Atwell (1987) who uses a POS tagger toflag POS bigrams that are unlikely according to areference corpus.
While he speculates that the bi-gram frequency should be compared to how oftenthe same POS bigram is involved in errors in an errorcorpus, the proposed system uses the raw frequencywith an empirically established threshold to decidewhether a bigram indicates an error.
In the samepaper, a completely different approach is presentedthat uses the same POS tagger to consider spellingvariants that have a different POS.
In the examplesentence I am very hit the POS of the spelling vari-ant hot/JJ is added to the list NN-VB-VBD-VBN ofpossible POS tags of hit.
If the POS tagger chooseshit/JJ, the word is flagged and the correction hot isproposed to the user.
Unlike most n-gram-based ap-proaches, Atwell?s work aims to detect grammar er-rors in general and not just real-word spelling errors.However, a complete evaluation is missing.The idea of disambiguating between the elementsof confusion sets is related to word sense disam-biguation.
Golding (1995) builds a classifier basedon a rich set of context features.
Mays et al (1991)apply the noisy channel model to the disambiguationproblem.
For each candidate correction S?
of theinput S the probability P (S?
)P (S|S?)
is calculatedand the most likely correction selected.
This methodis re-evaluated by Wilcox-O?Hearn et al (2006) onWSJ data with artificial real-word spelling errors.Bigert and Knutsson (2002) extend upon a basicn-gram approach by attempting to match n-grams oflow frequency with similar n-grams in order to re-duce overflagging.
Furthermore, n-grams crossingclause boundaries are not flagged and the similaritymeasure is adapted in the case of phrase boundariesthat usually result in low frequency n-grams.Chodorow and Leacock (2000) use a mutual in-formation measure in addition to raw frequency of n-grams.
Apart from this, their ALEK system employsother extensions to the basic approach, for exam-ple frequency counts from both generic and word-specific corpora are used in the measures.
It is notreported how much each of these contribute to theoverall performance.Rather than trying to implement all of the pre-vious n-gram approaches, we implement the basicapproach which uses rare n-grams to predict gram-maticality.
This property is shared by all previousshallow approaches.
We also test our approach on awider class of grammatical errors.3 Ungrammatical DataIn this section, we discuss the notion of an artifi-cial error corpus (Section 3.1), define the type ofungrammatical language we are dealing with (Sec-tion 3.2), and describe our procedure for creating alarge artificial error corpus derived from the BNC(Section 3.3).3.1 An Artificial Error CorpusIn order to meaningfully evaluate a shallow ver-sus deep approach to automatic error detection, alarge test set of ungrammatical sentences is needed.A corpus of ungrammatical sentences can take theform of a learner corpus (Granger, 1993; Emi et al,2004), i. e. a corpus of sentences produced by lan-guage learners, or it can take the form of a more gen-eral error corpus comprising sentences which are notnecessarily produced in a language-learning contextand which contain competence and performance er-rors produced by native and non-native speakers ofthe language (Becker et al, 1999; Foster and Vogel,2004; Foster, 2005).
For both types of error corpus,it is not enough to collect a large set of sentenceswhich are likely to contain an error - it is also neces-sary to examine each sentence in order to determinewhether an error has actually occurred, and, if it has,to note the nature of the error.
Thus, like the cre-ation of a treebank, the creation of a corpus of un-grammatical sentences requires time and linguisticknowledge, and is by no means a trivial task.A corpus of ungrammatical sentences which islarge enough to be useful can be created auto-matically by inserting, deleting or replacing words114in grammatical sentences.
These transformationsshould be linguistically realistic and should, there-fore, be based on an analysis of naturally producedgrammatical errors.
Automatically generated errorcorpora have been used before in natural languageprocessing.
Bigert (2004) and Wilcox-O?Hearn etal.
(2006), for example, automatically introducespelling errors into texts.
Here, we generate a largeerror corpus by automatically inserting four differentkinds of grammatical errors into BNC sentences.3.2 Commonly Produced Grammatical ErrorsFollowing Foster (2005), we define a sentence to beungrammatical if all the words in the sentence arewell-formed words of the language in question, butthe sentence contains one or more error.
This er-ror can take the form of a performance slip whichcan occur due to carelessness or tiredness, or a com-petence error which occurs due to a lack of knowl-edge of a particular construction.
This definition in-cludes real-word spelling errors and excludes non-word spelling errors.
It also excludes the abbrevi-ated informal language used in electronic communi-cation.
Using the above definition as a guideline, a20,000 word corpus of ungrammatical English sen-tences was collected from a variety of written textsincluding newspapers, academic papers, emails andwebsite forums (Foster and Vogel, 2004; Foster,2005).
The errors in the corpus were carefully anal-ysed and classified in terms of how they might becorrected using the three word-level correction op-erators: insert, delete and substitute.
The followingfrequency ordering of the three word-level correc-tion operators was found:substitute (48%) > insert (24%) > delete (17%) >combination (11%)Stemberger (1982) reports the same ordering of thesubstitution, deletion and insertion correction oper-ators in a study of native speaker spoken languageslips.
Among the grammatical errors which can becorrected by substituting one word for another, themost common errors are real-word spelling errorsand agreement errors.
In fact, 72% of all errors fallinto one of the following four classes:1. missing word errors:What are the subjects?
> What the subjects?2.
extra word errors:Was that in the summer?
> Was that in the sum-mer in?3.
real-word spelling errors:She could not comprehend.
> She could nocomprehend.4.
agreement errors:She steered Melissa round a corner.
> Shesteered Melissa round a corners.A similar classification was adopted by Nicholls(1999), having analysed the errors in a learner cor-pus.
Our research is currently limited to the four er-ror types given above, i. e. missing word errors, ex-tra word errors, real-word spelling errors and agree-ments errors.
However, it is possible for it to be ex-tended to handle a wider class of errors.3.3 Automatic Error CreationThe error creation procedure takes as input a part-of-speech-tagged corpus of sentences which are as-sumed to be well-formed, and outputs a corpus ofungrammatical sentences.
The automatically intro-duced errors take the form of the four most com-mon error types found in the manually created cor-pus, i. e. missing word errors, extra word errors, real-word spelling errors and agreement errors.
For eachsentence in the original tagged corpus, an attempt ismade to automatically produce four ungrammaticalsentences, one for each of the four error types.
Thus,the output of the error creation procedure is, in fact,four error corpora.3.3.1 Missing Word ErrorsIn the manually created error corpus of Foster(2005), missing word errors are classified based onthe part-of-speech (POS) of the missing word.
98%of the missing parts-of-speech come from the fol-lowing list (the frequency distribution in the errorcorpus is given in brackets):det (28%) > verb (23%) > prep (21%) > pro (10%)> noun (7%) > ?to?
(7%) > conj (2%)We use this information when introducing missingword errors into the BNC sentences.
For each sen-tence, all words with the above POS tags are noted.One of these is selected and deleted.
The abovefrequency ordering is respected so that, for exam-ple, missing determiner errors are produced more of-ten than missing pronoun errors.
No ungrammatical115sentence is produced if the original sentence con-tains just one word or if the sentence contains nowords with parts-of-speech in the above list.3.3.2 Extra Word ErrorsWe introduce extra word errors in the followingthree ways:1.
Random duplication of any token within a sen-tence: That?s the way we we learn here.2.
Random duplication of any POS within a sen-tence: There it he was.3.
Random insertion of an arbitrary token into thesentence: Joanna drew as a long breadth.Apart from the case of duplicate tokens, the extrawords are selected from a list of tagged words com-piled from a random subset of the BNC.
Again, ourprocedure for inserting extra words is based on theanalysis of extra word errors in the 20,000 word er-ror corpus of Foster (2005).3.3.3 Real-Word Spelling ErrorsWe classify an error as a real-word spelling er-ror if it can be corrected by replacing the erroneousword with another word with a Levenshtein distanceof one from the erroneous word, e.g.
the and they.Based on the analysis of the manually created er-ror corpus (Foster, 2005), we compile a list of com-mon English real-word spelling error word pairs.For each BNC sentence, the error creation proce-dure records all tokens in the sentence which appearas one half of one of these word pairs.
One tokenis selected at random and replaced by the other halfof the pair.
The list of common real-word spellingerror pairs contains such frequently occurring wordsas is and a, and the procedure therefore produces anill-formed sentence for most input sentences.3.3.4 Agreement ErrorsWe introduce subject-verb and determiner-nounnumber agreement errors into the BNC sentences.We consider both types of agreement error equallylikely and introduce the error by replacing a singulardeterminer, noun or verb with its plural counterpart,or vice versa.
For English, subject-verb agreementerrors can only be introduced for present tense verbs,and determiner-noun agreement errors can only beintroduced for determiners which are marked fornumber, e.g.
demonstratives and the indefinite ar-ticle.
The procedure would be more productive ifapplied to a morphologically richer language.3.3.5 Covert ErrorsJames (1998) uses the term covert error to de-scribe a genuine language error which results in asentence which is syntactically well-formed undersome interpretation different from the intended one.The prominence of covert errors in our automati-cally created error corpus is estimated by manuallyinspecting 100 sentences of each error type.
The per-centage of grammatical structures that are inadver-tently produced for each error type and an exampleof each one are shown below:?
Agreement Errors, 7%Mary?s staff include Jones,Smith and Murphy> Mary?s staff includes Jones,Smith and Mur-phy?
Real-Word Spelling Errors, 10%And then?
> And them??
Extra Word Errors, 5%in defiance of the free rider prediction > in de-fiance of the free rider near prediction?
Missing Word Errors, 13%She steered Melissa round a corner > Shesteered round a cornerThe occurrence of these covert errors can be re-duced by fine-tuning the error creation procedure butthey can never be completely eliminated.
Indeed,they should not be eliminated from the test data,because, ideally, an optimal error detection systemshould be sophisticated enough to flag syntacticallywell-formed sentences containing covert errors aspotentially ill-formed.14 Error Detection EvaluationIn this section we present the error detection eval-uation experiments.
The experimental setup is ex-plained in Section 4.1, the results are presented inSection 4.2 and they are analysed in Section 4.3.1An example of this is given in the XLE User Documen-tation (http://www2.parc.com/isl/groups/nltt/xle/doc/).
The authors remark that an ungrammatical read-ing of the sentence Lets go to the store in which Lets is missingan apostrophe, is preferable to the grammatical yet implausibleanalysis in which Lets is a plural noun.1164.1 Experimental Setup4.1.1 Test Data and Evaluation ProcedureThe following steps are carried out to producetraining and test data for this experiment:1.
Speech material, poems, captions and list itemsare removed from the BNC.
4.2 million sen-tences remain.
The order of sentences is ran-domised.2.
For the purpose of cross-validation, the corpusis split into 10 parts.3.
Each part is passed to the 4 automatic error in-sertion modules described in Section 3.3, re-sulting in 40 additional sets of varying size.4.
The first 60,000 sentences of each of the 50sets, i. e. 3 million sentences, are parsed withXLE.25.
N-gram frequency information is extracted forthe first 60,000 sentences of each set.
An addi-tional 20,000 is extracted as held-out data.6.
10 sets with mixed error types are produced byjoining a quarter of each respective error set.7.
For each error type (including mixed errors)and cross-validation set, the 60,000 grammat-ical and 60,000 ungrammatical sentences arejoined.8.
Each cross-validation run uses one set out ofthe 10 as test data (120,000 sentences) and theremaining 9 sets for training (1,080,000 sen-tences).The experiment is a standard binary classificationtask.
The methods classify the sentences of the testsets as grammatical or ungrammatical.
We use thestandard measures of precision, recall, f-score andaccuracy (Figure 1).
True positives are understoodto be ungrammatical sentences that are identified assuch.
The baseline precision and accuracy is 50%as half of the test data is ungrammatical.
If 100%of the test data is classified as ungrammatical, re-call will be 100% and f-score 2/3.
Recall showsthe accuracy we would get if the grammatical halfof the test data was removed.
Parametrised methods2We use the XLE command parse-testfile with parse-literally set to 1, max xle scratch storage set to 1,000 MB, time-out to 60 seconds, and the XLE English LFG.
Skimming is notswitched on and fragments are.Measure Formulaprecision tp/(tp + fp)recall tp/(tp + fn)f-score 2pr ?
re/(pr + re)accuracy (tp + tn)/(tp + tn + fp + fn)Figure 1: Evaluation measures: tp = true positives,fp = false positives, tn = true negatives, fn = falsenegatives, pr = precision, re = recallare first optimised for accuracy and then the othermeasures are taken.
Therefore, f-scores below theartificial 2/3 baseline are meaningful.4.1.2 Method 1: Precision GrammarAccording to the XLE documentation, a sentenceis marked with a star (*) if its optimal solution usesa constraint marked as ungrammatical.
We use thisstar feature, parser exceptions and zero number ofparses to classify a sentence as ungrammatical.4.1.3 Method 2: POS N-gramsIn each cross-validation run, the full data of theremaining 9 sets of step 2 of the data generation(see Section 4.1.1) is used as a reference corpus of0.9?4, 200, 000 = 3, 800, 000 assumedly grammat-ical sentences.
The reference corpora and data setsare POS tagged with the IMS TreeTagger (Schmidt,1994).
Frequencies of POS n-grams (n = 2, .
.
.
, 7)are counted in the reference corpora.
A test sentenceis flagged as ungrammatical if it contains an n-grambelow a fixed frequency threshold.
Method 2 hastwo parameters: n and the frequency threshold.4.1.4 Method 3: Decision Trees on XLE OutputThe XLE parser outputs additional statistics foreach sentence that we encode in six features:?
An integer indicating starredness (0 or 1) andvarious parser exceptions (-1 for time out, -2for exceeded memory, etc.)?
The number of optimal parses3?
The number of unoptimal parses?
The duration of parsing?
The number of subtrees?
The number of words3The use of preferred versus dispreferred constraints areused to distinguish optimal parses from unoptimal ones.117Training data for the decision tree learner is com-posed of 9?60, 000 = 540, 000 feature vectors fromgrammatical sentences and 9 ?
15, 000 = 135, 000feature vectors from ungrammatical sentences ofeach error type, resulting in equal amounts of gram-matical and ungrammatical training data.We choose the weka implementation of machinelearning algorithms for the experiments (Witten andFrank, 2000).
We use a J48 decision tree learnerwith the default model.4.1.5 Method 4: Decision Trees on N-gramsMethod 4 follows the setup of Method 3.
How-ever, the features are the frequencies of the rarestn-grams (n = 2, .
.
.
, 7) in the sentence.
Therefore,the feature vector of one sentence contains 6 num-bers.4.1.6 Method 5: Decision Trees on CombinedFeature SetsThis method combines the features of Methods 3and 4 for training a decision tree.4.2 ResultsTable 1 shows the results for Method 1, which usesXLE starredness, parser exceptions4 and zero parsesto classify grammaticality.
Table 2 shows the re-sults for Method 2, the basic n-gram approach.
Ta-ble 3 shows the results for Method 3, which classi-fies based on a decision tree of XLE features.
Theresults for Method 4, the n-gram-based decision treeapproach, are shown in Table 4.
Finally, Table 5shows the results for Method 5 which combines n-gram and XLE features in decision trees.In the case of Method 2, we first have to find opti-mal parameters.
As only very limited integer valuesfor n and the threshold are reasonable, an exhaustivesearch is feasible.
We considered n = 2, .
.
.
, 7 andfrequency thresholds below 20,000.
Separate held-out data (400,000 sentences) is used in order to avoidoverfitting.
Best accuracy is achieved with 5-gramsand a threshold of 4.
Table 2 reports results withthese parameters.4XLE parsing (see footnote 2 for configuration) runs outof time for 0.7 % and out of memory for 2.5 % of sentences,measured on training data of the first cross-validation run, i. e.540,000 grammatical sentence and 135,000 of each error type.14 sentences of 3 million caused the parser to terminate abnor-mally.Error type Pr.
Re.
F-Sc.
Acc.Agreement 66.2 64.6 65.4 65.8Real-word 63.5 57.3 60.3 62.2Extra word 64.4 59.7 62.0 63.4Missing word 59.2 47.8 52.9 57.4Mixed errors 63.5 57.3 60.3 62.2Table 1: Classification results with XLE starredness,parser exceptions and zero parses (Method 1)Error type Pr.
Re.
F-Sc.
Acc.Agreement 58.6 51.7 55.0 57.6Real-word 64.0 64.9 64.5 64.2Extra word 64.8 67.3 66.0 65.4Missing word 57.2 48.8 52.7 56.1Mixed errors 61.5 58.2 59.8 60.8Table 2: Classification results with 5-gram and fre-quency threshold 4 (Method 2)The standard deviation of results across cross-validation runs is below 0.006 on all measures, ex-cept for Method 4.
Therefore we only report averagepercentages.
The highest observed standard devia-tion is 0.0257 for recall of Method 4 on agreementerrors.For Methods 3, 4 and 5, the decision tree learneroptimises accuracy and, in doing so, chooses a trade-off between precision and recall.4.3 AnalysisBoth Method 1 (Table 1) and Method 2 (Table 2)achieve above baseline accuracy for all error types.However, Method 1, which uses the XLE starredfeature, parser exceptions and zero parses to de-termine whether or not a sentence is grammatical,slightly outperforms Method 2, which uses the fre-Error type Pr.
Re.
F-Sc.
Acc.Agreement 67.0 79.3 72.6 70.1Real-word 63.4 67.6 65.4 64.3Extra word 63.0 66.4 64.7 63.7Missing word 59.7 57.8 58.7 59.4Mixed errors 63.4 67.8 65.6 64.4Table 3: Classification results with decision tree onXLE output (Method 3)118Error type Pr.
Re.
F-Sc.
Acc.Agreement 61.2 53.8 57.3 59.9Real-word 65.3 64.3 64.8 65.1Extra word 66.4 67.4 66.9 66.7Missing word 59.1 49.2 53.7 57.5Mixed errors 63.3 58.7 60.9 62.3Table 4: Classification results with decision tree onvectors of frequency of rarest n-grams (Method 4)Error type Pr.
Re.
F-Sc.
Acc.Agreement 67.1 75.2 70.9 69.2Real-word 65.8 70.7 68.1 67.0Extra word 65.9 71.2 68.5 67.2Missing word 61.2 58.0 59.5 60.6Mixed errors 65.2 68.8 66.9 66.0Table 5: Classification results with decision tree onjoined feature set (Method 5)quency of POS 5-grams to detect an error.
TheXLE deep-processing approach is better than the n-gram-based approach for agreement errors (f-score+10.4).
Examining the various types of agree-ment errors, we can see that this is especially thecase for singular subjects followed by plural cop-ula verbs (recall +37.7) and determiner-noun num-ber mismatches (recall +23.6 for singular nouns and+18.0 for plural nouns), but not for plural subjectsfollowed by singular verbs (recall -24.0).
The rela-tively poor performance of Method 2 on agreementerrors involving determiners could be due to the lackof agreement marking on the Penn Treebank deter-miner tag used by TreeTagger.Method 1 is outperformed by Method 2 for real-word spelling and extra word errors (f-score -4.2,-4.0).
Unsurprisingly, Method 2 has an advantageon those real-word spelling errors that change thePOS (recall -8.8 for Method 1).
Both methods per-form poorly on missing word errors.
For both meth-ods there are only very small differences in perfor-mance between the various missing word error sub-types (identified by the POS of the deleted word).Method 3, which uses machine learning to exploitall the information returned by the XLE parser, im-proves performance from Method 1, the basic XLEmethod, for all error types.5 The general improve-ment comes from an improvement in recall, mean-ing that more ungrammatical sentences are actu-ally flagged as such without compromising preci-sion.
The improvement is highest for agreementerrors (f-score +7.2).
Singular subject with pluralcopula errors (e. g. The man are) peak at a recall of91.0.
The Method 3 results indicate that informationon the number of solutions (optimal and unoptimal),the number of subtrees, the time taken to parse thesentence and the number of words can be used topredict grammaticality.
It would be interesting toinvestigate this approach with other parsers.Method 4, which uses a decision tree with n-gram-based features, confirms the results of Method2.
The decision trees?
root nodes are similar or evenidentical (depending on cross-validation run) to thedecision rule of Method 2 (5-gram frequency below4).
However, the 10 decision trees have between1,111 and 1,905 nodes and draw from all features,even bigrams and 7-grams that perform poorly ontheir own.
The improvements are very small thoughand they are not significant according the criterion ofnon-overlapping cross-validation results.
The mainreason for the evaluation of Method 4 is to provideanother reference point for comparison of the finalmethod.The overall best results are those for Method 5,the combined XLE, n-gram and machine-learning-based method, which outperforms the next bestmethod, Method 3, on all error types apart fromagreement errors (f-score -1.7, +2.7, +3.8, +0.8).For agreement errors, it seems that the relativelypoor results for n-grams have a negative effect on therelatively good results for the XLE.
Figure 2 showsthat the performance is almost constant on ungram-matical data in the important sentence length rangefrom 5 to 40.
However, there is a negative correla-tion of accuracy and sentence length for grammati-cal sentences.
Very long sentences of any kind tendto be classified as ungrammatical, except for missingword errors which remain close to the 50% baselineof coin-flipping.For all methods, missing word errors are theworst-performing, particularly in recall (i. e. the ac-5The +0.3 increase in average accuracy for extra word errorsis not clearly significant as the results of cross-validation runsoverlap.119Figure 2: Accuracy by sentence length for Method 5measured on separate grammatical and ungrammat-ical data: Gr = Grammatical, AG = Agreement, RW= Real-Word, EW = Extra Word, MW = MissingWordcuracy on ungrammatical data alone).
This meansthat the omission of a word is less likely to result inthe sentence being flagged as erroneous.
In contrast,extra word errors perform consistently and relativelywell for all methods.5 Conclusion and Future WorkWe evaluated a deep processing approach and a POSn-gram-based approach to the automatic detection ofcommon grammatical errors in a BNC-derived arti-ficial error corpus.
The results are broken down byerror type.
Together with the deep approach, a deci-sion tree machine learning algorithm can be used ef-fectively.
However, extending the shallow approachwith the same learning algorithm gives only smallimprovements.
Combining the deep and shallow ap-proaches gives an additional improvement on all butone error type.Our plan is to investigate why all methods per-form poorly on missing word errors, to extend theerror creation procedure so that it includes a widerrange of errors, to try the deep approach with otherparsers, to integrate additional features from state-of-the-art shallow techniques and to repeat the ex-periments for languages other than English.AcknowledgementsThis work is supported by the IRCSET Embark Ini-tiative (basic research grant SC/02/298 and postdoc-toral fellowship P/04/232).
The training and testdata used in this reseach is based on the British Na-tional Corpus (BNC), distributed by Oxford Univer-sity Computing Services on behalf of the BNC Con-sortium.
We thank Djame?
Seddah for helping us torun the XLE parsing on the SFI/HEA Irish Centrefor High-End Computing (ICHEC) and the authorswish to acknowledge ICHEC for the provision ofcomputational facilities and support.ReferencesEric Atwell.
1987.
How to detect grammatical errors ina text without parsing it.
In Proceedings of the 3rdEACL, pages 38?45, Morristown, NJ.Timothy Baldwin, John Beavers, Emily M. Bender, DanFlickinger, Ara Kim, and Stephan Oepen.
2004.Beauty and the beast: What running a broad-coverageprecision grammar over the BNC taught us about thegrammar - and the corpus.
In Pre-Proceedings of theInternational Conference on Linguistic Evidence: Em-pirical, Theoretical and Computational Perspectives,pages 21?26.Markus Becker, Andrew Bredenkamp, Berthold Crys-mann, and Judith Klein.
1999.
Annotation of errortypes for German news corpus.
In Proceedings of theATALA Workshop on Treebanks, Paris, France.Emily M. Bender, Dan Flickinger, Stephan Oepen, andTimothy Baldwin.
2004.
Arboretum: Using a preci-sion grammar for grammar checking in CALL.
In Pro-ceedings of the InSTIL/ICALL Symposium: NLP andSpeech Technologies in Advanced Language LearningSystems, Venice, Italy.Johnny Bigert and Ola Knutsson.
2002.
Robust errordetection: a hybrid approach combining unsupervisederror detection and linguistic knowledge.
In Proceed-ings RO-MAND-02, Frascati, Italy.Johnny Bigert.
2004.
Probabilistic detection of context-sensitive spelling errors.
In Proceedings of LREC-04,volume Five, pages 1633?1636, Lisbon, Portugal.Phil Blunsom and Timothy Baldwin.
2006.
Multilingualdeep lexical acquisition for HPSGs via supertagging.In Proceedings of EMNLP-06, pages 164?171, Syd-ney.Lou Burnard.
2000.
User reference guide for the Britishnational corpus.
Technical report, Oxford UniversityComputing Services.120Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-roshi Masuichi, and Christian Rohrer.
2002.
The par-allel grammar project.
In Proceedings of COLING-2002 Workshop on Grammar Engineering and Evalu-ation, pages 1?7, Morristown, NJ, USA.Eugene Charniak.
1996.
Tree-bank grammars.
Tech-nical Report CS-96-02, Department of Computer Sci-ence, Brown University.Martin Chodorow and Claudia Leacock.
2000.
An unsu-pervised method for detecting grammatical errors.
InProceedings of NAACL-00, pages 140?147, San Fran-cisco, CA.Noam Chomsky.
1957.
Syntactic Structures.
Mouton.Ann Copestake and Dan Flickinger.
2000.
An open-source grammar development environment and broad-coverage English grammar using HPSG.
In Proceed-ings of LREC-02, Athens, Greece.Izumi Emi, Kiyotaka Uchimoto, and Hitoshi Isahara.2004.
The overview of the SST speech corpus ofJapanese learner English and evaluation through theexperiment on automatic detection of learners?
er-rors.
In Proceedings of LREC-04, volume Four, pages1435?1439, Lisbon, Portugal.Jennifer Foster and Carl Vogel.
2004.
Good reasonsfor noting bad grammar: Constructing a corpus of un-grammatical language.
In Stephan Kepser and MargaReis, editors, Pre-Proceedings of the InternationalConference on Linguistic Evidence: Empirical, The-oretical and Computational Perspectives, pages 151?152, Tu?bingen, Germany.Jennifer Foster.
2005.
Good Reasons for Noting BadGrammar: Empirical Investigations into the Parsingof Ungrammatical Written English.
Ph.D. thesis, Uni-versity of Dublin, Trinity College, Dublin, Ireland.Anette Frank, Tracy Holloway King, Jonas Kuhn, andJohn Maxwell.
1998.
Optimality theory style con-straint ranking in large-scale LFG grammars.
In Pro-ceedings of LFG-98, Brisbane, Australia.Andrew R. Golding.
1995.
A Bayesian hybrid methodfor context-sensitive spelling correction.
In Proceed-ings of the Third Workshop on Very Large Corpora,pages 39?53, Boston, MA.Sylviane Granger.
1993. International corpus of learnerEnglish.
In J. Aarts, P. de Haan, and N.Oostdijk, ed-itors, English Language Corpora: Design, Analysisand Exploitation, pages 57?71.
Rodopi, Amsterdam.Carl James.
1998.
Errors in Language Learning andUse: Exploring Error Analysis.
Addison WesleyLongman.Ron Kaplan and Joan Bresnan.
1982.
Lexical FunctionalGrammar: a formal system for grammatical represen-tation.
In Joan Bresnan, editor, The Mental Represen-tation of Grammatical Relations, pages 173?281.
MITPress.Rafiq Abdul Khader, Tracy Holloway King, and MiriamButt.
2004.
Deep CALL grammars: The LFG-OT experiment.
http://ling.uni-konstanz.de/pages/home/butt/dgfs04call.pdf.John Maxwell and Ron Kaplan.
1996.
An EfficientParser for LFG.
In Proceedings of LFG-96, Grenoble.Eric Mays, Fred J. Damerau, and Robert L. Mercer.1991.
Context based spelling correction.
InformationProcessing and Management, 23(5):517?522.D.
Nicholls.
1999.
The Cambridge learner corpus ?
errorcoding and analysis.
In Summer Workshop on LearnerCorpora, Tokyo, Japan.Carl Pollard and Ivan A.
Sag.
1994.
Head-Driven PhraseStructure Grammar.
University of Chicago Press andCSLI Publications.Alan Prince and Paul Smolensky.
1993.
Optimality The-ory.
MIT Press, Cambridge, Massachusetts.Veit Reuer.
2003.
PromisD - Ein Analyseverfahrenzur antizipationsfreien Erkennung und Erkla?rung vongrammatischen Fehlern in Sprachlehrsystemen.
Ph.D.thesis, Humboldt-Universita?t zu Berlin, Berlin, Ger-many.Helmut Schmidt.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In Proceedings of the In-ternational Conference on New Methods in LanguageProcessing, pages 44?49, Manchester, England.J.P.
Stemberger.
1982.
Syntactic errors in speech.
Jour-nal of Psycholinguistic Research, 11(4):313?45.Anne Vandeventer Faltin.
2003.
Syntactic Error Diag-nosis in the context of Computer Assisted LanguageLearning.
Ph.D. thesis, Universite?
de Gene`ve.L.
Amber Wilcox-O?Hearn, Graeme Hirst, and Alexan-der Budanitsky.
2006.
Real-word spelling correc-tion with trigrams: A reconsideration of the Mays,Damerau, and Mercer model.
http://ftp.cs.toronto.edu/pub/gh/WilcoxOHearn-etal-2006.pdf.Ian H. Witten and Eibe Frank.
2000.
Data Mining: Prac-tical Machine Learning Tools and Techniques withJava Implementations.
Morgan Kaufmann Publishers.121
