Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 25?32,Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language ProcessingAggregating Machine Learning and Rule Based Heuristics for NamedEntity RecognitionKarthik Gali, Harshit Surana, Ashwini Vaidya, Praneeth Shishtla andDipti Misra SharmaLanguage Technologies Research Centre,International Institute of Information Technology,Hyderabad, India.karthikg@students.iiit.ac.in, surana.h@gmail.com,ashwini_vaidya@research.iiit.ac.in, praneethms@students.iiit.ac.in,dipti@iiit.ac.inAbstractThis paper, submitted as an entry for theNERSSEAL-2008 shared task, describes asystem build for Named Entity Recognitionfor South and South East Asian Languages.Our paper combines machine learningtechniques with language specific heuris-tics to model the problem of NER for In-dian languages.
The system has been testedon five languages: Telugu, Hindi, Bengali,Urdu and Oriya.
It uses CRF (ConditionalRandom Fields) based machine learning,followed by post processing which in-volves using some heuristics or rules.
Thesystem is specifically tuned for Hindi andTelugu, we also report the results for theother four languages.1 IntroductionNamed Entity Recognition (NER) is a task thatseeks to locate and classify entities (?atomic ele-ments?)
in a text into predefined categories such asthe names of persons, organizations, locations, ex-pressions of times, quantities, etc.
It can be viewedas a two stage process:1.
Identification of entity boundaries2.
Classification into the correct categoryFor example, if ?Mahatma Gandhi?
is a namedentity in the corpus, it is necessary to identify thebeginning and the end of this entity in the sentence.Following this step, the entity must be classifiedinto the predefined category, which is NEP(Named Entity Person) in this case.This task is the precursor for many natural lan-guage processing applications.
It has been used inQuestion Answering (Toral et al 2005) as well asMachine Translation (Babych et al 2004).The NERSSEAL contest has used 12 categoriesof named entities to define a tagset.
The data hasbeen manually tagged for training and testing pur-poses for the contestants.The task of building a named entity recognizerfor South and South East Asian languages presentsseveral problems related to their linguistic charac-teristics.
We will first discuss some of these lin-guistic issues, followed by a description of themethod used.
Further, we show some of the heuris-tics used for post-processing and finally an analy-sis of the results obtained.2 Previous WorkThe linguistic methods generally use rulesmanually written by linguists.
There are severalrule based NER systems, containing mainly lexi-calized grammar, gazetteer lists, and list of triggerwords, which are capable of providing upto 92% f-measure accuracy for English (McDonald, 1996;Wakao et al, 1996).Linguistic approach uses hand-crafted ruleswhich need skilled linguistics.
The chief disadvan-tage of these rule-based techniques is that they re-quire huge experience and grammatical knowledgeof the particular language or domain and these sys-tems are not transferable to other languages or do-mains.
However, given the closer nature of manyIndian languages, the cost of adaptation of a re-25source from one language to another could be quiteless (Singh and Surana, 2007).Various machine learning techniques have alsobeen successfully used for the NER task.
Generallyhidden markov model (Bikel et al,1997), maxi-mum entropy (Borthwick, 1999), conditional ran-dom field (Li and Mccallum, 2004) are more popu-lar machine learning techniques used for the pur-pose of NER.Hybrid systems have been generally more effec-tive at the task of NER.
Given lesser data and morecomplex NE classes which were present inNERSSEAL shared task, hybrid systems makemore sense.
Srihari et al (2000) combines MaxEnt,hidden markov model (HMM) and handcraftedrules to build an NER system.Though not much work has been done for otherSouth Asian languages, some previous work fo-cuses on NER for Hindi.
It has been previouslyattempted by Cucerzan and Yarowsky in their lan-guage independent NER work which used morpho-logical and contextual evidences (Cucerzan andYarowsky, 1999).
They ran their experiment with5 different languages.
Among these the accuracyfor Hindi was the worst.
For Hindi the systemachieved 42% f-value with a recall of 28% andabout 85% precision.
A result which highlightslack of good training data, and other various issuesinvolved with linguistic handling of Indian lan-guages.Later approaches have resulted in better resultsfor Hindi.
Hindi NER system developed by Wei Liand Andrew Mccallum (2004) using conditionalrandom fields (CRFs) with feature induction haveachieved f-value of 71%.
(Kumar and Bhat-tacharyya, 2006) used maximum entropy markovmodel to achieve f-value of upto 80%.3 Some Linguistic Issues3.1 Agglutinative NatureSome of the SSEA languages have agglutinativeproperties.
For example, a Dravidian language likeTelugu has a number of postpositions attached to astem to form a single word.
An example is:guruvAraMwo = guruvAraM + woup to Wednesday = Wednesday + up toMost of the NERs are suffixed with several dif-ferent postpositions, which increase the number ofdistinct words in the corpus.
This in turn affectsthe machine learning process.3.2 No CapitalizationAll the five languages have scripts without graphi-cal cues like capitalization, which could act as animportant indicator for NER.
For a language likeEnglish, the NER system can exploit this feature toits advantage.3.3 AmbiguityOne of the properties of the named entities in theselanguages is the high overlap between commonnames and proper names.
For instance Kamal (inHindi) can mean ?lotus?, which is not a named en-tity, but it can also be a person?s name, in whichcase, it is a named entity.Among the named entities themselves, there isambiguity between a location name Bangalore ekbadzA shaher heI (Bangalore is a big city) or a per-son?s surname ?M.
Bangalore shikshak heI?
(M.Bangalore is a teacher).3.4 Low POS Tagging Accuracy for NounsFor English, the available tools like POS (Part ofSpeech) tagger can be used to provide features formachine learning.
This is not very helpful forSSEA languages because the accuracy for nounand proper noun tags is quite low (PVS and G.,2006) Hence, features based on POS tags cannotbe used for NER for these languages.To illustrate this difficulty, we conducted thefollowing experiment.
A POS tagger (described inPVS & G.,2006) was run on the Hindi test data.The data had 544 tokens with NEL, NEP, NEOtags.
The POS tagger should have given the NNP(proper noun) tag for all those named entities.However the tagger was able to tag only 80 tokensaccurately.
This meant that only 14.7% of thenamed entities were correctly recognized.3.5 Spelling VariationOne other important language related issue is thevariation in the spellings of proper names.
For in-stance the same name Shri Ram Dixit can be writ-ten as Sri.
Ram Dixit, Shree Ram Dixit, Sh.
R. Dixitand so on.
This increases the number of tokens tobe learnt by the machine and would perhaps alsorequire a higher level task like co-reference resolu-tion.262.6 Pattern of suffixes We have converted this format into the BIOformat as described in Ramshaw et.
al.
For exam-ple, the above format will now be shown as:Named entities of Location (NEL) or Person(NEP) will share certain common suffixes, whichcan be exploited by the learning algorthm.
For in-stance, in Hindi, -pur (Rampur, Manipur) or -giri(Devgiri) are suffixes that will appear in the namedentities for Location.
Similarly, there are suffixeslike -swamy (Ramaswamy, Krishnaswamy) or -deva (Vasudeva, Mahadeva) which can be com-monly found in named entities for person.
Thesesuffixes are cues for some of the named entities inthe SSEA languages.Rabindranath  B-NEPTagore   I-NEPne   Okahaa   OThe training data set contains (approximately)400,000 Hindi, 50,000 Telugu, 35,000 Urdu,93,000 Oriya and 120,000 Bengali words respec-tively.A NER system can be rule-based, statistical orhybrid.
A rule-based NER system uses hand-written rules to tag a corpus with named entities.
Astatistical NER system learns the probabilities ofnamed entities using training data, whereas hybridsystems use both.5 Conditional Random FieldsConditional Random Fields (CRFs) are undirectedgraphical models used to calculate the conditionalprobability of values on designated output nodesgiven values assigned to other designated inputnodes.
Developing rule-based taggers for NER can be cumbersome as it is a language specific process.Statistical taggers require large amount of anno-tated data (the more the merrier) to train.
Our sys-tem is a hybrid NER tagger which first uses Condi-tional Random Fields (CRF) as a machine learningtechnique followed by some rule based post-processing.In the special case in which the output nodes ofthe graphical model are linked by edges in a linearchain, CRFs make a first-order Markov independ-ence assumption, and thus can be understood asconditionally-trained finite state machines (FSMs).Let o = (o,,oWe treat the named entity recognition problemas a sequential token-based tagging problem.According to Lafferty et.
al.
CRF outperformsother Machine Learning algorithms viz., HiddenMarkov Models (HMM), Maximum EntropyMarkov Model (MEMM) for  sequence labelingtasks.4 Training dataThe training data given by the organizers was inSSF format1.
For example in SSF format, thenamed entity ?Rabindranath Tagore?
will be shownin the following way:0 (( SSF1  ((  NP  <ne=NEP>1.1  Rabindranath1.2 Tagore))2 ne3 kahaa))1 http://shiva.iiit.ac.in/SPSAL2007/ssf-analysis-representation.pdf2,o3 ,o4 ,... oT  ) be some observed in-put data sequence, such as a sequence of words intext in a document,(the values on n input nodes ofthe graphical model).
Let S be a set of FSM states,each of which is associated with a label, l ?
?.Let s = (s ,s ,s  ,s  ,... s1 2 3 4 T ) be some sequence ofstates, (the values on T output nodes).
By theHammersley-Clifford theorem, CRFs define theconditional probability of a state sequence given aninput sequence to be:where Zo is a normalization factor over all statesequences is an arbitrary feature function over itsarguments, and ?k is a learned weight for each fea-ture function.
A feature function may, for example,be defined to have value 0 or 1.
Higher ?
weightsmake their corresponding FSM transitions morelikely.
CRFs define the conditional probability of alabel sequence based on the total probability overthe state sequences,27where l(s) is the sequence of labels correspond-ing to the labels of the states in sequence s.Note that the normalization factor, Zo, (alsoknown in statistical physics as the partition func-tion) is the sum of the scores of all possible states.And that the number of state sequences is expo-nential in the input sequence length, T. In arbitrar-ily-structured CRFs, calculating the partition func-tion in closed form is intractable, and approxima-tion methods such as Gibbs sampling or loopy be-lief propagation must be used.
In linear-chainstructured CRFs (in use here for sequence model-ing), the partition function can be calculated effi-ciently by dynamic programming.6 CRF Based Machine LearningWe used the CRF model to perform the initial tag-ging followed by post-processing.6.1 Statistical TaggingIn the first phase, we have used language inde-pendent features to build the model using CRF.Orthographic features (like capitalization, decimals),affixes (suffixes and prefixes), context (previouswords and following words), gazetteer features, POSand morphological features etc.
are generally used forNER.
In English and some other languages, capitali-zation features play an important role as NEs aregenerally capitalized for these languages.
Unfortu-nately as explained above this feature is not applica-ble for the Indian languages.Precision Recall F-MeasurePm Pn Pl Rm Rn Rl Fm Fn FlBengali 53.34 49.28 58.27 26.77 25.88 31.19 35.65 33.94 40.63Hindi 59.53 63.84 64.84 41.21 41.74 40.77 48.71 50.47 50.06Oriya 39.16 40.38 63.70 23.39 19.24 28.15 29.29 26.06 39.04Telugu 10.31 71.96 65.45 68.00 30.85 29.78 08.19 43.19 40.94Urdu 43.63 44.76 48.96 36.69 34.56 39.07 39.86 39.01 43.46Table 1: Evaluation of the NER System for Five LanguagesThe exact set of features used are described be-low.6.2 Window of the WordsWords preceding or following the target word maybe useful for determining its category.
Following afew trials we found that a suitable window size isfive.6.3 SuffixesStatistical suffixes of length 1 to 4 have been con-sidered.
These can capture information for namedentities having the NEL tag like Hyderabad,Secunderabad, Ahmedabad etc., all of which endin -bad.
We have collected lists of such suffixes forNEP (Named Entity Person) and NEL (Named En-tity Location) for Hindi.
In the machine learningmodel, this resource can be used as a binary fea-ture.
A sample of these lists is as follows:Type of NE Example suffixes(Hindi)NE- Location -desa, -vana, -nagara,-garh, -rashtra, -giriNE ?
Person -raja, -natha, -lal, -bhai,-pathi, -krishnanTable 2: Suffixes for Hindi NER287 Heuristics Based Post Processing 6.4 PrefixesStatistical prefixes of length 1 to 4 have been con-sidered.
These can take care of the problems asso-ciated with a large number of distinct tokens.
Asmentioned earlier, agglutinative languages canhave a number of postpositions.
The use of pre-fixes will increase the probability of   Hyderabadand Hyderabadlo (Telugu for ?in Hyderabad?)
be-ing treated as the same token.Complex named entities like fifty five kilogramscontain a Named Entity Number within a NamedEntity Measure.
We observed that these were notidentified accurately enough in the machine learn-ing based system.
Hence, instead of applying ma-chine learning to handle nested entities we makeuse of rule-based post processing.7.1 Second Best TagTable 3: F-Measure (Lexical) for NE TagsBengali Hindi Oriya Telugu UrduIt was observed that the recall of the CRF model islow.
In order to improve recall, we have used thefollowing rule:  if the best tag given by the CRFmodel is O (not a named entity) and the confidenceof the second best tag is greater than 0.15, then thesecond best tag is considered as the correct tag.NEP 35.22 54.05 52.22 01.93 31.22NED NA 42.47 01.97 NA 21.27NEO 11.59 45.63 14.50 NA 19.13NEA NA 61.53 NA NA NAWe observed an increase of 7% in recall and 3%decrease in precision.
This resulted in a 4% in-crease in the F-measure, which is a significant in-crease in performance.
The decrease in precision isexpected as we are taking the second tag.NEB NA NA NA NA NANETP 42.30 NA NA NA NANETO 33.33 13.77 NA 01.66 NANEL 45.27 62.66 48.72 01.49 57.857.2 Nested Entities NETI 55.85 79.09 40.91 71.35 63.47NEN 62.67 80.69 24.94 83.17 13.75 One of the important tasks in the contest was toidentify nested named entities.
For example if weconsider eka kilo (Hindi: one kilo) as NEM(Named Entity Measure), it contains a NEN(Named Entity Number) within it.NEM 60.51 43.75 19.00 26.66 84.10NETE 19.17 31.52 NA 08.91 NAThe CRF model tags eka kilo as NEM and in or-der to tag eka as NEN we have made use of otherresources like a gazetteer for the list of numbers.We used such lists for four languages.6.5 Start of a sentenceThere is a possibility of confusing the NEN(Named Entity Number) in a sentence with thenumber that appears in a numbered list.
The num-bered list will always have numbers at the begin-ning of a sentence and hence a feature that checksfor this property will resolve the ambiguity with anactual NEN.7.3 GazetteersFor Hindi, we made use of three different kinds ofgazetteers.
These consisted of lists for measures(entities like kilogram, millimetre, lakh), numeralsand quantifiers (one, first, second) and time ex-pressions (January, minutes, hours) etc.
Similarlists were used for all the other languages exceptUrdu.
These gazetteers were effective in identify-ing this relatively closed class of named entitiesand showed good results for these languages.6.6 Presence of digitsUsually, the presence of digits indicates that thetoken is a named entity.
For example, the tokens92, 10.1 will be identified as Named Entity Num-ber based on the binary feature ?contains digits?.6.7 Presence of  four digits 8 EvaluationIf the token is a four digit number, it is likelier tobe a NETI (Named Entity Time).
For example,1857, 2007 etc.
are most probably years.The evaluation measures used for all the five lan-guages are precision, recall and F-measure.
Thesemeasures are calculated in three different ways:291.
Maximal Matches: The largest possiblenamed entities are matched with the refer-ence data.The amount of annotated corpus available forHindi was substantially more.
This should haveideally resulted in better results for Hindi with themachine learning approach.
But, the results wereonly marginally better than other languages.
A ma-jor reason for this was that a very high percentage(44%) of tags in Hindi were NETE.
The tagsetgives examples like ?Horticulture?, ?ConditionalRandom Fields?
for the tag NETE.
It has also beenmentioned that even manual annotation is harderfor NETE as it is domain specific.
This affected theoverall results for Hindi because the performancefor NETE was low (Table 3).2.
Nested Matches: The largest possible aswell as nested named entities are matched.3.
Lexical Item Matches: The lexical itemsinside largest possible named entities arematched.9 ResultsThe results of evaluation as explained in the previ-ous section are shown in the Table-1.
The F-measures for nested lexical match are also shownindividually for each named entity tag separately inTable-3Num ofNE tokensNum ofknown NE% of un-known NEBengali 1185 277 23.3710 Unknown Words Hindi 1120 417 37.23Table 4 shows the number of unknown words pre-sent in the test data when compared with the train-ing data.Oriya 1310 563 42.97Telugu 1150 145 12.60First column shows the number of uniqueNamed entity tags present in the test data for eachlanguage.
Second column shows the number ofunique known named entities present in the testdata.
Third column shows the percentage of uniqueunknown words present in the test data of differentlanguages when compared to training data.Urdu 631 179 28.36Table 4: Unknown WordAlso, the F-measures of NEN, NETI, and NEMcould have been higher because they are relativelyclosed classes.
However, certain NEN can be am-biguous (Example: eka is a NEN for ?one?
inHindi, but in a different context it can be a non-number.
For instance eka-doosra is Hindi for ?eachother?
).11 Error AnalysisWe can observe from the results that the maximalF-measure for Telugu is very low when comparedto lexical F-measure and nested F-measure.
Thereason is that the test data of Telugu contains alarge number of long named entities (around 6words), which in turn contain around 4 - 5 nestednamed entities.
Our system was able to tag nestednamed entities correctly unlike maximal namedentity.In a language like Telugu, NENs will appear asinflected words.
For example 2001lo, guru-vaaramto.10     Conclusion and Further WorkIn this paper we have presented the results of usinga two stage hybrid approach for the task of namedentity recognition for South and South East AsianLanguages.
We have achieved decent Lexical F-measures of 40.63, 50.06, 39.04, 40.94, and 43.46for Bengali, Hindi, Oriya, Telugu and Urdu respec-tively without using many language specific re-sources.We can also observe that the maximal F-measure for Telugu is very low when compared toother languages.
This is because Telugu test datahas very few known words.Urdu results are comparatively low chiefly be-cause gazetteers for numbers and measures wereunavailable.We plan to extend our work by applying ourmethod to other South Asian languages, and byusing more language specific constraints and re-sources.
We also plan to incorporate semi-supervised extraction of rules for NEs (Saha et.
al,302008) and use transliteration techniques to produceIndian language gazetteers (Surana and Singh,2008).
Use of character models for increasing thelower recalls (Shishtla et.
al, 2008) is also under-way.
We also plan to enrich the Indian dependencytree bank (Begum et.
al, 2008) by use of our NERsystem.11 AcknowledgmentsWe would like to thank the organizer Mr. AnilKumar Singh deeply for his continuous supportduring the shared task.ReferencesB.
Babych, and A. Hartley, Improving Machine transla-tion Quality with Automatic Named Entity Recognition.www.mt-archive.info/EAMT-2003- Babych.pdfRafiya Begum, Samar Husain, Arun Dhwaj, Dipti MisraSharma, Lakshmi Bai, and Rajeev Sangal.
2008.
De-pendency annotation scheme for Indian languages.
InProceedings of IJCNLP-2008, Hyderabad, India.M.
Bikel Daniel, Miller Scott, Schwartz Richard andWeischedel Ralph.
1997.
Nymble: A High Performance Learning Name-finder.
In Proceedings of theFifth Conference on Applied Natural LanguageProcessing.S.
Cucerzan, and D. Yarowsky, 1999.
Language inde-pendent named entity recognition combining mor-phological and contextual evidence.
Proceedings ofthe Joint SIGDAT Conference on EMNLP and VLC.N.
Kumar and Pushpak Bhattacharyya.
2006.
NamedEntity Recognition in Hindi using MEMM.
In Tech-nical Report, IIT Bombay, India.John Lafferty, Andrew McCallum and FernandoPereira.
2001.
Conditional Random Fields: Probabil-istic Models for Segmenting and Labeling SequenceData.
Proc.
18th International Conf.
on MachineLearning.D.
McDonald 1996.
Internal and external evidence inthe identification and semantic categorization ofproper names.
In B. Boguraev and J. Pustejovsky,editors, Corpus Processing for Lexical Acquisition.Avinesh PVS and Karthik G. Part-Of-Speech Taggingand Chunking Using Conditional Random Fields andTransformation Based Learning.
Proceedings of theSPSAL workshop during IJCAI?07.Lance Ramshaw and Mitch Marcus.
Text ChunkingUsing Transformation-Based Learning.
Proceedingsof the Third Workshop on Very Large Corpora.S.K.
Saha , S. Chatterji , S. Dandapat , S. Sarkar  and P.Mitra 2008.
A Hybrid Approach for Named EntityRecognition in Indian Languages.
In Proceedings ofIJCNLP Workshop on NER for South and South EastAsian Languages.Fei Sha and Fernando Pereira.
2003.
Shallow Parsingwith Conditional Random Fields.
In the Proceedingsof HLT-NAACL.P.
Shishtla, P. Pingali , V. Varma  2008.
A Character n-gram Based Approach for Improved Recall in IndianLanguage NER.
In Proceedings of IJCNLP Work-shop on NER for South and South East Asian Lan-guages.Cucerzan Silviu and Yarowsky David.
1999.
LanguageIndependent Named Entity Recognition CombiningMorphological and Contextual Evidence.
In Proceed-ings of the Joint SIGDAT Conference on EMNLP andVLC.A.
K. Singh and H. Surana  Can Corpus Based Meas-ures be Used for Comparative Study of Languages?In Proceedings of Ninth Meeting of the ACL SpecialInterest Group in Computational Morphology andPhonology.
ACL.
2007.R.
Srihari, C. Niu and W. Li  2000.
A Hybrid Approachfor Named Entity and Sub-Type Tagging.
In Pro-ceedings of the sixth conference on Applied naturallanguage processing.H.
Surana and A. K. Singh 2008.
A More Discerningand Adaptable Multilingual Transliteration Mecha-nism for Indian Languages.
In Proceedings of theThird International Joint Conference on NaturalLanguage Processing.Charles Sutton, An Introduction to Conditional RandomFields for Relational Learning.T.
Wakao , R. Gaizauskas  and Y. Wilks 1996.
Evalua-tion of an algorithm for the recognition and classifi-cation of proper names.
In Proceedings of COLING.Li Wei and McCallum Andrew.
2004.
Rapid Develop-ment of Hindi Named Entity Recognition using Con-ditional Random Fields and Feature Induction.
InACM Transactions on Computational Logic.CRF++:.Yet another Toolkit.http://crfpp.sourceforge.net/3132
