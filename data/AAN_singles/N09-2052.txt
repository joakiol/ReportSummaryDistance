Proceedings of NAACL HLT 2009: Short Papers, pages 205?208,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsUsing N-gram based Features for Machine TranslationSystem CombinationYong Zhao1 Xiaodong HeGeorgia Institute of Technology Microsoft ResearchAtlanta, GA 30332, USA Redmond, WA 98052, USAyongzhao@gatech.edu xiaohe@microsoft.comAbstractConventional confusion network basedsystem combination for machine translation(MT) heavily relies on features that arebased on the measure of agreement ofwords in different translation hypotheses.This paper presents two new features thatconsider agreement of n-grams in differenthypotheses to improve the performance ofsystem combination.
The first one is basedon a sentence specific online n-gramlanguage model, and the second one isbased on n-gram voting.
Experiments on alarge scale Chinese-to-English MT taskshow that both features yield significantimprovements on the translationperformance, and a combination of themproduces even better translation results.1 Introduction1In past years, the confusion network based systemcombination approach has been shown withsubstantial improvements in various machinetranslation (MT) tasks (Bangalore, et.
al., 2001,Matusov, et.
al., 2006, Rosti, et.
al., 2007, He,et.
al., 2008).
Given hypotheses of multiplesystems, a confusion network is built by aligningall these hypotheses.
The resulting networkcomprises a sequence of correspondence sets, eachof which contains the alternative words that arealigned with each other.
To derive a consensushypothesis from the confusion network, decodingis performed by selecting a path with the maximumoverall confidence score among all paths that passthe confusion network (Goel, et.
al., 2004).1 The work was performed when Yong Zhao was an intern atMicrosoft ResearchThe confidence score of a hypothesis could beassigned in various ways.
Fiscus (1997) usedvoting by frequency of word occurrences.
Manguet.
al., (2000) computed a word posteriorprobability based on voting of that word indifferent hypotheses.
Moreover, the overallconfidence score is usually formulated as a log-linear model including extra features includinglanguage model (LM) score, word count, etc.Features based on word agreement measure areextensively studied in past work (Matusov, et.
al.,2006, Rosti, et.
al., 2007, He, et.
al., 2008).However, utilization of n-gram agreementinformation among the hypotheses has not beenfully explored yet.
Moreover, it was argued thatthe confusion network decoding may introduceundesirable spur words that break coherentphrases (Sim, et.
al., 2007).
Therefore, we wouldprefer the consensus translation that has better n-gram agreement among outputs of single systems.In the literature, Zens and Ney (2004)proposed an n-gram posterior probability basedLM for MT.
For each source sentence, a LM istrained on the n-best list produced by a single MTsystem and is used to re-rank that n-best list itself.On the other hand, Matusov et al (2008) proposedan ?adapted?
LM for system combination, wherethis ?adapted?
LM is trained on translationhypotheses of the whole test corpus from all singleMT systems involved in system combination.Inspired by these ideas, we propose two newfeatures based on n-gram agreement measure toimprove the performance of system combination.The first one is a sentence specific LM built ontranslation hypotheses of multiple systems; thesecond one is n-gram-voting-based confidence.Experimental results are presented in the context ofa large-scale Chinese-English translation task.2052 System Combination for MTOne of the most successful approaches forsystem combination for MT is based onconfusion network decoding as described in(Rosti, et.
al., 2007).
Given translationhypotheses from multiple MT systems, one ofthe hypotheses is selected as the backbone forthe use of hypothesis alignment.
This is usuallydone by a sentence-level minimum Bayes risk(MBR) re-ranking method.
The confusionnetwork is constructed by aligning all thesehypotheses against the backbone.
Words thatalign to each other are grouped into acorrespondence set, constituting competitionlinks of the confusion network.
Each path in thenetwork passes exactly one link from eachcorrespondence set.
The final consensus outputrelies on a decoding procedure that chooses apath with the maximum confidence score amongall paths that pass the confusion network.The confidence score of a hypothesis isusually formalized as a log-linear sum of severalfeature functions.
Given a source languagesentence ?
, the total confidence of a targetlanguage hypothesis ?
= (?1 ,?
, ??)
in theconfusion network can be represented as:????
?
?
= ???
?
??
?,??
?=1+ ?1??????
?+ ?2??????
(?
)(1)where the feature functions include wordposterior probability  ?(??
|?,?
), LM probability???(?
), and the number of real words ??????
in?
.
Usually, the model parameter ?i could betrained by optimizing an evaluation metric, e.g.,BLEU score, on a held-out development set.3 N-gram Online Language ModelGiven a source sentence ?, the fractional count?
?1?
?
of an n-gram ?1?
is defined as:?
?1?
?
=   ?
??
???=???????(?
?
??
?+1?, ?1?)
(2)where ??
denotes the hypothesis set, ?
?,?denotes the Kronecker function, and ?(??
|?)
isthe posterior probability of translationhypothesis ??
, which is expressed as theweighted sum of the system specific posteriorprobabilities through the systems that containshypothesis ??
,?
?
?
=  ???(???=1??
,?
1(?
?
??? )
(3)where ??
is the weight for the posteriorprobability of the kth system ??
, and 1 ?
is theindicator function.Follows Rosti, et.
al.
(2007), system specificposteriors are derived based on a rank-basedscoring scheme.
I.e., if translation hypothesis ?
?is the rth best output in the n-best list of system??
, posterior ?
??
??
,?
is approximated as:?
??
??
,?
=1/(1 + ?
)?1/(1 + ??)?||???
||?
?=1(4)where ?
is a rank smoothing parameter.Similar to (Zens and Ney, 2004), astraightforward approach of using n-gramfractional counts is to formulate it as a sentencespecific online LM.
Then the online LM scoreof a path in the confusion network will be addedas an additional feature in the log-linear modelfor decoding.
The online n-gram LM score iscomputed by:?(??
|????+1?
?1 ,?)
=?(????+1?
|?)?(????+1?
?1 |?
)(5)The LM score of hypothesis ?
is obtained by:???
?
?
= ?
??
|????+1?
?1 ,???=?
(6)Since new n-grams unseen in originaltranslation hypotheses may be proposed by theCN decoder, LM smoothing is critical.
In ourapproach, the score of the online LM issmoothed by taking a linear interpolation tocombine scores of different orders.206??????
?
??
|????+1?
?1 ,?=  ???(??
|????+1?
?1 ,?)?
?=1(7)In our implementation, the interpolation weights{ ?? }
can be learned along with othercombination parameters in the same Max-BLEUtraining scheme via Powell's search.4 N-gram-Voting-Based ConfidenceMotivated by features based on voting of singleword, we proposed new features based on N-gram voting.
The voting score ?
?1?
?
of an n-gram ?1?
is computed as:?
?1?
?
=  ?
??
?
1(?1?
?
??)?????
(8)It receives a vote from each hypothesis thatcontains that n-gram, and weighted by theposterior probability of that hypothesis, wherethe posterior probability ?
??
?
is computed by(3).
Unlike the fractional count, each hypothesiscan vote no more than once on an n-gram.?
?1?
?
takes a value between 0 and 1.
Itcan be viewed as the confidence of the n-gram?1?
.
Then the n-gram-voting-based confidencescore of a hypothesis ?
is computed as theproduct of confidence scores of n-grams in E:???
,?
?
?
= ???
,?
?1?
?,?
=?(???+??1|?)??
?+1?=1(9)where n can take the value of 2, 3, ?, N. Inorder to prevent zero confidence, a small back-off confidence score is assigned to all n-gramsunseen in original hypotheses.Augmented with the proposed n-gram basedfeatures, the final log-linear model becomes:????
?
?= ????
??
?,??
?=1+ ?1??????
?+ ?2??????
?
+ ?3??????
?
?+ ??+2??????
,?
?
??
?=2(10)5 EvaluationWe evaluate the proposed n-gram based featureson the Chinese-to-English (C2E) test in the pastNIST Open MT Evaluations.
The experimentalresults are reported in case sensitive BLEUscore (Papineni, et.
al., 2002).The dev set, which is used for systemcombination parameter training, is the newswireand newsgroup parts of NIST MT06, whichcontains a total of 1099 sentences.
The test set isthe "current" test set of NIST MT08, whichcontains 1357 sentences of newswire and web-blog data.
Both dev and test sets have fourreference translations per sentence.Outputs from a total of eight single MTsystems were combined for consensustranslations.
These selected systems are basedon various translation paradigms, such asphrasal, hierarchical, and syntax-based systems.Each system produces 10-best hypotheses pertranslation.
The BLEU score range for the eightindividual systems are from 26.11% to 31.09%on the dev set and from 20.42% to 26.24% onthe test set.
In our experiments, a state-of-the-artsystem combination method proposed by He, et.al.
(2008) is implemented as the baseline.
Thetrue-casing model proposed by Toutanova et al(2008) is used.Table 1 shows results of adding the onlineLM feature.
Different LM orders up to four aretested.
Results show that using a 2-gram onlineLM yields a half BLEU point gain over thebaseline.
However, the gain is saturated after aLM order of three, and fluctuates after that.Table 2 shows the performance of using n-gram-voting-based confidence features.
The bestresult of 31.01% is achieved when up to 4-gramconfidence features are used.
The BLEU scorekeeps improving when longer n-gramconfidence features are added.
This indicatesthat the n-gram voting based confidence featureis robust to high order n-grams.We further experimented with incorporatingboth features in the log-linear model andreported the results in Table 3.
Given theobservation that the n-gram voting basedconfidence feature is more robust to high ordern-grams, we further tested using different n-gram orders for them.
As shown in Table 3,using 3-gram online LM plus 2~4-gram voting207based confidence scores yields the best BLEUscores on both dev and test sets, which are37.98% and 31.35%, respectively.
This is a 0.84BLEU point gain over the baseline on the MT08test set.Table 1: Results of adding the n-gram online LM.BLEU % Dev  TestBaseline 37.34 30.511-gram online LM 37.34 30.512-gram online LM 37.86 31.023-gram online LM 37.87 31.084-gram online LM 37.86 31.01Table 2: Results of adding n-gram voting basedconfidence features.BLEU % Dev  TestBaseline 37.34 30.51+ 2-gram voting 37.58 30.88+ 2~3-gram voting 37.66 30.96+ 2~4-gram voting 37.77 31.01Table 3: Results of using both n-gram online LMand n-gram voting based confidence featuresBLEU % Dev  TestBaseline 37.34 30.512-gram LM + 2-gram voting 37.78 30.983-gram LM + 2~3-gram voting 37.89 31.214-gram LM + 2~4-gram voting 37.93 31.083-gram LM + 2~4-gram voting 37.98 31.356 ConclusionThis work explored utilization of n-gramagreement information among translationoutputs of multiple MT systems to improve theperformance of system combination.
This is anextension of an earlier idea presented at theNIPS 2008 Workshop on Speech and Language(Yong and He 2008).
Two kinds of n-gram basedfeatures were proposed.
The first is based on anonline LM using n-gram fractional counts, andthe second is a confidence feature based on n-gram voting scores.
Our experiments on theNIST MT08 Chinese-English task showed thatboth methods yield nice improvements on thetranslation results, and incorporating both kindsof features produced the best translation resultwith a BLEU score of 31.35%, which is a 0.84%improvement.ReferencesJ.G.
Fiscus, 1997.
A post-processing system to yieldreduced word error rates: Recognizer Output VotingError Reduction (ROVER), in Proc.
ASRU.S.
Bangalore, G. Bordel, and G. Riccardi, 2001.Computing consensus translation from multiplemachine translation systems, in Proc.
ASRU.E.
Matusov, N. Ueffing, and H. Ney, 2006.Computing consensus translation from multiplemachine translation systems using enhancedhypotheses alignment, in Proc.
EACL.A.-V.I.
Rosti, S. Matsoukas, and R. Schwartz, 2007.Improved Word-Level System Combination forMachine Translation.
In Proc.
ACL.X.
He, M. Yang, J. Gao, P. Nguyen, and R. Moore,2008.
Indirect-HMM-based hypothesis alignment forcombining outputs from machine translationsystems, in Proc.
EMNLP.L.
Mangu, E. Brill, and A. Stolcke, 2000.
FindingConsensus in Speech Recognition: Word ErrorMinimization and Other Applications of ConfusionNetworks, Computer Speech and Language,14(4):373-400.R.
Zens and H. Ney, 2004.
N-Gram posteriorprobabilities for statistical machine translation, inProc.
HLT-NAACL.K.C.
Sim, W.J.
Byrne, M.J.F.
Gales, H. Sahbi andP.C.
Woodland, 2007.
Consensus network decodingfor statistical machine translation systemcombination.
in Proc.
ICASSP.V.
Goel, S. Kumar, and W. Byrne, 2004.
Segmentalminimum Bayes-risk decoding for automatic speechrecognition.
IEEE transactions on Speech and AudioProcessing, vol.
12, no.
3.K.
Papineni, S. Roukos, T. Ward, and W. Zhu, 2002.BLEU: a method for automatic evaluation ofmachine translation.
in Proc.
ACL.K.
Toutanova, H. Suzuki and A. Ruopp.
2008.
ApplyingMorphology Generation Models to MachineTranslation.
In Proc.
of ACL.Yong Zhao and Xiaodong He.
2008.
SystemCombination for Machine Translation Using N-GramPosterior Probabilities.
NIPS 2008 WORKSHOP onSpeech and Language: Learning-based Methods andSystems.
Dec. 2008E.
Matusov, G. Leusch, R. E. Banchs, N. Bertoldi, D.Dechelotte, M. Federico, M. Kolss, Y. Lee, J. B.Marino, M. Paulik, S. Roukos, H. Schwenk, and H.Ney.
2008.
System Combination for MachineTranslation of Spoken and Written Language.
IEEETransactions on Audio, Speech and LanguageProcessing, Sept. 2008.208
