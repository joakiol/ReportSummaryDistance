Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1064?1074,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsEnd-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRFXuezhe Ma and Eduard HovyLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USAxuezhem@cs.cmu.edu, hovy@cmu.eduAbstractState-of-the-art sequence labeling systemstraditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing.In this paper, we introduce a novel neu-tral network architecture that benefits fromboth word- and character-level representa-tions automatically, by using combinationof bidirectional LSTM, CNN and CRF.Our system is truly end-to-end, requir-ing no feature engineering or data pre-processing, thus making it applicable toa wide range of sequence labeling tasks.We evaluate our system on two data setsfor two sequence labeling tasks ?
PennTreebank WSJ corpus for part-of-speech(POS) tagging and CoNLL 2003 cor-pus for named entity recognition (NER).We obtain state-of-the-art performance onboth datasets ?
97.55% accuracy for POStagging and 91.21% F1 for NER.1 IntroductionLinguistic sequence labeling, such as part-of-speech (POS) tagging and named entity recogni-tion (NER), is one of the first stages in deep lan-guage understanding and its importance has beenwell recognized in the natural language processingcommunity.
Natural language processing (NLP)systems, like syntactic parsing (Nivre and Scholz,2004; McDonald et al, 2005; Koo and Collins,2010; Ma and Zhao, 2012a; Ma and Zhao, 2012b;Chen and Manning, 2014; Ma and Hovy, 2015)and entity coreference resolution (Ng, 2010; Maet al, 2016), are becoming more sophisticated,in part because of utilizing output information ofPOS tagging or NER systems.Most traditional high performance sequence la-beling models are linear statistical models, includ-ing Hidden Markov Models (HMM) and Condi-tional Random Fields (CRF) (Ratinov and Roth,2009; Passos et al, 2014; Luo et al, 2015), whichrely heavily on hand-crafted features and task-specific resources.
For example, English POS tag-gers benefit from carefully designed word spellingfeatures; orthographic features and external re-sources such as gazetteers are widely used in NER.However, such task-specific knowledge is costlyto develop (Ma and Xia, 2014), making sequencelabeling models difficult to adapt to new tasks ornew domains.In the past few years, non-linear neural net-works with as input distributed word representa-tions, also known as word embeddings, have beenbroadly applied to NLP problems with great suc-cess.
Collobert et al (2011) proposed a simple buteffective feed-forward neutral network that inde-pendently classifies labels for each word by us-ing contexts within a window with fixed size.
Re-cently, recurrent neural networks (RNN) (Gollerand Kuchler, 1996), together with its variants suchas long-short term memory (LSTM) (Hochreiterand Schmidhuber, 1997; Gers et al, 2000) andgated recurrent unit (GRU) (Cho et al, 2014),have shown great success in modeling sequentialdata.
Several RNN-based neural network mod-els have been proposed to solve sequence labelingtasks like speech recognition (Graves et al, 2013),POS tagging (Huang et al, 2015) and NER (Chiuand Nichols, 2015; Hu et al, 2016), achievingcompetitive performance against traditional mod-els.
However, even systems that have utilized dis-tributed representations as inputs have used theseto augment, rather than replace, hand-crafted fea-tures (e.g.
word spelling and capitalization pat-terns).
Their performance drops rapidly when themodels solely depend on neural embeddings.1064In this paper, we propose a neural network ar-chitecture for sequence labeling.
It is a truly end-to-end model requiring no task-specific resources,feature engineering, or data pre-processing be-yond pre-trained word embeddings on unlabeledcorpora.
Thus, our model can be easily appliedto a wide range of sequence labeling tasks on dif-ferent languages and domains.
We first use con-volutional neural networks (CNNs) (LeCun et al,1989) to encode character-level information of aword into its character-level representation.
Thenwe combine character- and word-level represen-tations and feed them into bi-directional LSTM(BLSTM) to model context information of eachword.
On top of BLSTM, we use a sequentialCRF to jointly decode labels for the whole sen-tence.
We evaluate our model on two linguisticsequence labeling tasks ?
POS tagging on PennTreebank WSJ (Marcus et al, 1993), and NERon English data from the CoNLL 2003 sharedtask (Tjong Kim Sang and De Meulder, 2003).Our end-to-end model outperforms previous state-of-the-art systems, obtaining 97.55% accuracy forPOS tagging and 91.21% F1 for NER.
The con-tributions of this work are (i) proposing a novelneural network architecture for linguistic sequencelabeling.
(ii) giving empirical evaluations of thismodel on benchmark data sets for two classic NLPtasks.
(iii) achieving state-of-the-art performancewith this truly end-to-end system.2 Neural Network ArchitectureIn this section, we describe the components (lay-ers) of our neural network architecture.
We intro-duce the neural layers in our neural network one-by-one from bottom to top.2.1 CNN for Character-level RepresentationPrevious studies (Santos and Zadrozny, 2014;Chiu and Nichols, 2015) have shown that CNNis an effective approach to extract morphologicalinformation (like the prefix or suffix of a word)from characters of words and encode it into neuralrepresentations.
Figure 1 shows the CNN we useto extract character-level representation of a givenword.
The CNN is similar to the one in Chiu andNichols (2015), except that we use only characterembeddings as the inputs to CNN, without char-acter type features.
A dropout layer (Srivastava etal., 2014) is applied before character embeddingsare input to CNN.P l a y i n g PaddingPaddingCharEmbeddingConvolutionMax PoolingCharRepresentationFigure 1: The convolution neural network for ex-tracting character-level representations of words.Dashed arrows indicate a dropout layer applied be-fore character embeddings are input to CNN.2.2 Bi-directional LSTM2.2.1 LSTM UnitRecurrent neural networks (RNNs) are a powerfulfamily of connectionist models that capture timedynamics via cycles in the graph.
Though, in the-ory, RNNs are capable to capturing long-distancedependencies, in practice, they fail due to the gra-dient vanishing/exploding problems (Bengio et al,1994; Pascanu et al, 2012).LSTMs (Hochreiter and Schmidhuber, 1997)are variants of RNNs designed to cope with thesegradient vanishing problems.
Basically, a LSTMunit is composed of three multiplicative gateswhich control the proportions of information toforget and to pass on to the next time step.
Fig-ure 2 gives the basic structure of an LSTM unit.Figure 2: Schematic of LSTM unit.1065Formally, the formulas to update an LSTM unitat time t are:it= ?
(Wiht?1+Uixt+ bi)ft= ?
(Wfht?1+Ufxt+ bf)c?t= tanh(Wcht?1+Ucxt+ bc)ct= ftct?1+ itc?tot= ?
(Woht?1+Uoxt+ bo)ht= ottanh(ct)where ?
is the element-wise sigmoid functionand  is the element-wise product.
xtis theinput vector (e.g.
word embedding) at timet, and htis the hidden state (also called out-put) vector storing all the useful information at(and before) time t. Ui,Uf,Uc,Uodenote theweight matrices of different gates for input xt,and Wi,Wf,Wc,Woare the weight matricesfor hidden state ht.
bi, bf, bc, bodenote the biasvectors.
It should be noted that we do not includepeephole connections (Gers et al, 2003) in the ourLSTM formulation.2.2.2 BLSTMFor many sequence labeling tasks it is benefi-cial to have access to both past (left) and future(right) contexts.
However, the LSTM?s hiddenstate httakes information only from past, know-ing nothing about the future.
An elegant solutionwhose effectiveness has been proven by previouswork (Dyer et al, 2015) is bi-directional LSTM(BLSTM).
The basic idea is to present each se-quence forwards and backwards to two separatehidden states to capture past and future informa-tion, respectively.
Then the two hidden states areconcatenated to form the final output.2.3 CRFFor sequence labeling (or general structured pre-diction) tasks, it is beneficial to consider the cor-relations between labels in neighborhoods andjointly decode the best chain of labels for a giveninput sentence.
For example, in POS tagging anadjective is more likely to be followed by a nounthan a verb, and in NER with standard BIO2 an-notation (Tjong Kim Sang and Veenstra, 1999)I-ORG cannot follow I-PER.
Therefore, we modellabel sequence jointly using a conditional randomfield (CRF) (Lafferty et al, 2001), instead of de-coding each label independently.Formally, we use z = {z1, ?
?
?
, zn} to repre-sent a generic input sequence where ziis the inputvector of the ith word.
y = {y1, ?
?
?
, yn} rep-resents a generic sequence of labels for z. Y(z)denotes the set of possible label sequences for z.The probabilistic model for sequence CRF definesa family of conditional probability p(y|z;W,b)over all possible label sequences y given z withthe following form:p(y|z;W,b) =n?i=1?i(yi?1, yi, z)?y?
?Y(z)n?i=1?i(y?i?1, y?i, z)where ?i(y?, y, z) = exp(WTy?,yzi+ by?,y) arepotential functions, and WTy?,yand by?,yare theweight vector and bias corresponding to label pair(y?, y), respectively.For CRF training, we use the maximum con-ditional likelihood estimation.
For a training set{(zi,yi)}, the logarithm of the likelihood (a.k.a.the log-likelihood) is given by:L(W,b) =?ilog p(y|z;W,b)Maximum likelihood training chooses parameterssuch that the log-likelihood L(W,b) is maxi-mized.Decoding is to search for the label sequence y?with the highest conditional probability:y?= argmaxy?Y(z)p(y|z;W,b)For a sequence CRF model (only interactions be-tween two successive labels are considered), train-ing and decoding can be solved efficiently byadopting the Viterbi algorithm.2.4 BLSTM-CNNs-CRFFinally, we construct our neural network model byfeeding the output vectors of BLSTM into a CRFlayer.
Figure 3 illustrates the architecture of ournetwork in detail.For each word, the character-level represen-tation is computed by the CNN in Figure 1with character embeddings as inputs.
Then thecharacter-level representation vector is concate-nated with the word embedding vector to feed intothe BLSTM network.
Finally, the output vectorsof BLSTM are fed to the CRF layer to jointly de-code the best label sequence.
As shown in Fig-ure 3, dropout layers are applied on both the in-put and output vectors of BLSTM.
Experimen-tal results show that using dropout significantly1066W eW ordEmbeddingare playing soc c erCharRepresentationF orw ardL S T MB ac k w ardL S T ML S T M L S T M L S T M L S T ML S T M L S T M L S T M L S T MPRP V B P N NV B GCRFL ayerFigure 3: The main architecture of our neuralnetwork.
The character representation for eachword is computed by the CNN in Figure 1.
Thenthe character representation vector is concatenatedwith the word embedding before feeding into theBLSTM network.
Dashed arrows indicate dropoutlayers applied on both the input and output vectorsof BLSTM.improve the performance of our model (see Sec-tion 4.5 for details).3 Network TrainingIn this section, we provide details about trainingthe neural network.
We implement the neural net-work using the Theano library (Bergstra et al,2010).
The computations for a single model arerun on a GeForce GTX TITAN X GPU.
Using thesettings discussed in this section, the model train-ing requires about 12 hours for POS tagging and 8hours for NER.3.1 Parameter InitializationWord Embeddings.
We use Stanford?s pub-licly available GloVe 100-dimensional embed-dings1trained on 6 billion words from Wikipediaand web text (Pennington et al, 2014)1http://nlp.stanford.edu/projects/glove/We also run experiments on two other setsof published embeddings, namely Senna 50-dimensional embeddings2trained on Wikipediaand Reuters RCV-1 corpus (Collobert et al, 2011),and Google?s Word2Vec 300-dimensional embed-dings3trained on 100 billion words from GoogleNews (Mikolov et al, 2013).
To test the effec-tiveness of pretrained word embeddings, we ex-perimented with randomly initialized embeddingswith 100 dimensions, where embeddings are uni-formly sampled from range [?
?3dim,+?3dim]where dim is the dimension of embeddings (Heet al, 2015).
The performance of different wordembeddings is discussed in Section 4.4.Character Embeddings.
Character embed-dings are initialized with uniform samples from[?
?3dim,+?3dim], where we set dim = 30.Weight Matrices and Bias Vectors.
Matrix pa-rameters are randomly initialized with uniformsamples from [?
?6r+c,+?6r+c], where r and care the number of of rows and columns in thestructure (Glorot and Bengio, 2010).
Bias vec-tors are initialized to zero, except the bias bfforthe forget gate in LSTM , which is initialized to1.0 (Jozefowicz et al, 2015).3.2 Optimization AlgorithmParameter optimization is performed with mini-batch stochastic gradient descent (SGD) withbatch size 10 and momentum 0.9.
We choose aninitial learning rate of ?0(?0= 0.01 for POS tag-ging, and 0.015 for NER, see Section 3.3.
), and thelearning rate is updated on each epoch of trainingas ?t= ?0/(1+ ?t), with decay rate ?
= 0.05 andt is the number of epoch completed.
To reduce theeffects of ?gradient exploding?, we use a gradientclipping of 5.0 (Pascanu et al, 2012).
We exploredother more sophisticated optimization algorithmssuch as AdaDelta (Zeiler, 2012), Adam (Kingmaand Ba, 2014) or RMSProp (Dauphin et al, 2015),but none of them meaningfully improve upon SGDwith momentum and gradient clipping in our pre-liminary experiments.Early Stopping.
We use early stopping (Giles,2001; Graves et al, 2013) based on performanceon validation sets.
The ?best?
parameters appear ataround 50 epochs, according to our experiments.2http://ronan.collobert.com/senna/3https://code.google.com/archive/p/word2vec/1067Layer Hyper-parameter POS NERCNNwindow size 3 3number of filters 30 30LSTMstate size 200 200initial state 0.0 0.0peepholes no noDropout dropout rate 0.5 0.5batch size 10 10initial learning rate 0.01 0.015decay rate 0.05 0.05gradient clipping 5.0 5.0Table 1: Hyper-parameters for all experiments.Fine Tuning.
For each of the embeddings, wefine-tune initial embeddings, modifying them dur-ing gradient updates of the neural network modelby back-propagating gradients.
The effectivenessof this method has been previously explored in se-quential and structured prediction problems (Col-lobert et al, 2011; Peng and Dredze, 2015).Dropout Training.
To mitigate overfitting, we ap-ply the dropout method (Srivastava et al, 2014) toregularize our model.
As shown in Figure 1 and 3,we apply dropout on character embeddings beforeinputting to CNN, and on both the input and out-put vectors of BLSTM.
We fix dropout rate at 0.5for all dropout layers through all the experiments.We obtain significant improvements on model per-formance after using dropout (see Section 4.5).3.3 Tuning Hyper-ParametersTable 1 summarizes the chosen hyper-parametersfor all experiments.
We tune the hyper-parameterson the development sets by random search.
Dueto time constrains it is infeasible to do a ran-dom search across the full hyper-parameter space.Thus, for the tasks of POS tagging and NER wetry to share as many hyper-parameters as possible.Note that the final hyper-parameters for these twotasks are almost the same, except the initial learn-ing rate.
We set the state size of LSTM to 200.Tuning this parameter did not significantly impactthe performance of our model.
For CNN, we use30 filters with window length 3.4 Experiments4.1 Data SetsAs mentioned before, we evaluate our neural net-work model on two sequence labeling tasks: POStagging and NER.Dataset WSJ CoNLL2003TrainSENT 38,219 14,987TOKEN 912,344 204,567DevSENT 5,527 3,466TOKEN 131,768 51,578TestSENT 5,462 3,684TOKEN 129,654 46,666Table 2: Corpora statistics.
SENT and TOKENrefer to the number of sentences and tokens in eachdata set.POS Tagging.
For English POS tagging, we usethe Wall Street Journal (WSJ) portion of PennTreebank (PTB) (Marcus et al, 1993), which con-tains 45 different POS tags.
In order to com-pare with previous work, we adopt the standardsplits ?
section 0?18 as training data, section 19?21 as development data and section 22?24 as testdata (Manning, 2011; S?gaard, 2011).NER.
For NER, We perform experiments onthe English data from CoNLL 2003 sharedtask (Tjong Kim Sang and De Meulder, 2003).This data set contains four different types ofnamed entities: PERSON, LOCATION, ORGA-NIZATION, and MISC.
We use the BIOES tag-ging scheme instead of standard BIO2, as pre-vious studies have reported meaningful improve-ment with this scheme (Ratinov and Roth, 2009;Dai et al, 2015; Lample et al, 2016).The corpora statistics are shown in Table 2.
Wedid not perform any pre-processing for data sets,leaving our system truly end-to-end.4.2 Main ResultsWe first run experiments to dissect the effective-ness of each component (layer) of our neural net-work architecture by ablation studies.
We com-pare the performance with three baseline systems?
BRNN, the bi-direction RNN; BLSTM, the bi-direction LSTM, and BLSTM-CNNs, the combi-nation of BLSTM with CNN to model character-level information.
All these models are run usingStanford?s GloVe 100 dimensional word embed-dings and the same hyper-parameters as shown inTable 1.
According to the results shown in Ta-ble 3, BLSTM obtains better performance thanBRNN on all evaluation metrics of both the twotasks.
BLSTM-CNN models significantly outper-form the BLSTM model, showing that character-level representations are important for linguisticsequence labeling tasks.
This is consistent with1068POS NERDev Test Dev TestModel Acc.
Acc.
Prec.
Recall F1 Prec.
Recall F1BRNN 96.56 96.76 92.04 89.13 90.56 87.05 83.88 85.44BLSTM 96.88 96.93 92.31 90.85 91.57 87.77 86.23 87.00BLSTM-CNN 97.34 97.33 92.52 93.64 93.07 88.53 90.21 89.36BRNN-CNN-CRF 97.46 97.55 94.85 94.63 94.74 91.35 91.06 91.21Table 3: Performance of our model on both the development and test sets of the two tasks, together withthree baseline systems.Model Acc.Gim?enez and M`arquez (2004) 97.16Toutanova et al (2003) 97.27Manning (2011) 97.28Collobert et al (2011)?97.29Santos and Zadrozny (2014)?97.32Shen et al (2007) 97.33Sun (2014) 97.36S?gaard (2011) 97.50This paper 97.55Table 4: POS tagging accuracy of our model ontest data from WSJ proportion of PTB, togetherwith top-performance systems.
The neural net-work based models are marked with ?.results reported by previous work (Santos andZadrozny, 2014; Chiu and Nichols, 2015).
Fi-nally, by adding CRF layer for joint decoding weachieve significant improvements over BLSTM-CNN models for both POS tagging and NER onall metrics.
This demonstrates that jointly decod-ing label sequences can significantly benefit the fi-nal performance of neural network models.4.3 Comparison with Previous Work4.3.1 POS TaggingTable 4 illustrates the results of our model forPOS tagging, together with seven previous top-performance systems for comparison.
Our modelsignificantly outperform Senna (Collobert et al,2011), which is a feed-forward neural networkmodel using capitalization and discrete suffix fea-tures, and data pre-processing.
Moreover, ourmodel achieves 0.23% improvements on accu-racy over the ?CharWNN?
(Santos and Zadrozny,2014), which is a neural network model based onSenna and also uses CNNs to model character-level representations.
This demonstrates the effec-tiveness of BLSTM for modeling sequential dataModel F1Chieu and Ng (2002) 88.31Florian et al (2003) 88.76Ando and Zhang (2005) 89.31Collobert et al (2011)?89.59Huang et al (2015)?90.10Chiu and Nichols (2015)?90.77Ratinov and Roth (2009) 90.80Lin and Wu (2009) 90.90Passos et al (2014) 90.90Lample et al (2016)?90.94Luo et al (2015) 91.20This paper 91.21Table 5: NER F1 score of our model on test dataset from CoNLL-2003.
For the purpose of com-parison, we also list F1 scores of previous top-performance systems.
?
marks the neural models.and the importance of joint decoding with struc-tured prediction model.Comparing with traditional statistical models,our system achieves state-of-the-art accuracy, ob-taining 0.05% improvement over the previouslybest reported results by S?gaard (2011).
It shouldbe noted that Huang et al (2015) also evaluatedtheir BLSTM-CRF model for POS tagging onWSJ corpus.
But they used a different splitting ofthe training/dev/test data sets.
Thus, their resultsare not directly comparable with ours.4.3.2 NERTable 5 shows the F1 scores of previous modelsfor NER on the test data set from CoNLL-2003shared task.
For the purpose of comparison, welist their results together with ours.
Similar to theobservations of POS tagging, our model achievessignificant improvements over Senna and the otherthree neural models, namely the LSTM-CRF pro-posed by Huang et al (2015), LSTM-CNNs pro-1069Embedding Dimension POS NERRandom 100 97.13 80.76Senna 50 97.44 90.28Word2Vec 300 97.40 84.91GloVe 100 97.55 91.21Table 6: Results with different choices of wordembeddings on the two tasks (accuracy for POStagging and F1 for NER).posed by Chiu and Nichols (2015), and the LSTM-CRF by Lample et al (2016).
Huang et al (2015)utilized discrete spelling, POS and context fea-tures, Chiu and Nichols (2015) used character-type, capitalization, and lexicon features, and allthe three model used some task-specific data pre-processing, while our model does not require anycarefully designed features or data pre-processing.We have to point out that the result (90.77%) re-ported by Chiu and Nichols (2015) is incompa-rable with ours, because their final model wastrained on the combination of the training and de-velopment data sets4.To our knowledge, the previous best F1 score(91.20)5reported on CoNLL 2003 data set is bythe joint NER and entity linking model (Luo etal., 2015).
This model used many hand-craftedfeatures including stemming and spelling features,POS and chunks tags, WordNet clusters, BrownClusters, as well as external knowledge bases suchas Freebase and Wikipedia.
Our end-to-end modelslightly improves this model by 0.01%, yielding astate-of-the-art performance.4.4 Word EmbeddingsAs mentioned in Section 3.1, in order to test theimportance of pretrained word embeddings, weperformed experiments with different sets of pub-licly published word embeddings, as well as a ran-dom sampling method, to initialize our model.
Ta-ble 6 gives the performance of three different wordembeddings, as well as the randomly sampled one.According to the results in Table 6, models usingpretrained word embeddings obtain a significantimprovement as opposed to the ones using randomembeddings.
Comparing the two tasks, NER relies4We run experiments using the same setting and get91.37% F1 score.5Numbers are taken from the Table 3 of the original pa-per (Luo et al, 2015).
While there is clearly inconsistencyamong the precision (91.5%), recall (91.4%) and F1 scores(91.2%), it is unclear in which way they are incorrect.POS NERTrain Dev Test Train Dev TestNo 98.46 97.06 97.11 99.97 93.51 89.25Yes 97.86 97.46 97.55 99.63 94.74 91.21Table 7: Results with and without dropout on twotasks (accuracy for POS tagging and F1 for NER).POS NERDev Test Dev TestIV 127,247 125,826 4,616 3,773OOTV 2,960 2,412 1,087 1,597OOEV 659 588 44 8OOBV 902 828 195 270Table 8: Statistics of the partition on each corpus.It lists the number of tokens of each subset for POStagging and the number of entities for NER.more heavily on pretrained embeddings than POStagging.
This is consistent with results reportedby previous work (Collobert et al, 2011; Huang etal., 2015; Chiu and Nichols, 2015).For different pretrained embeddings, Stanford?sGloVe 100 dimensional embeddings achieve bestresults on both tasks, about 0.1% better on POSaccuracy and 0.9% better on NER F1 score thanthe Senna 50 dimensional one.
This is dif-ferent from the results reported by Chiu andNichols (2015), where Senna achieved slightlybetter performance on NER than other embed-dings.
Google?s Word2Vec 300 dimensional em-beddings obtain similar performance with Sennaon POS tagging, still slightly behind GloVe.
Butfor NER, the performance on Word2Vec is far be-hind GloVe and Senna.
One possible reason thatWord2Vec is not as good as the other two embed-dings on NER is because of vocabulary mismatch?
Word2Vec embeddings were trained in case-sensitive manner, excluding many common sym-bols such as punctuations and digits.
Since we donot use any data pre-processing to deal with suchcommon symbols or rare words, it might be an is-sue for using Word2Vec.4.5 Effect of DropoutTable 7 compares the results with and withoutdropout layers for each data set.
All other hyper-parameters remain the same as in Table 1.
Weobserve a essential improvement for both the twotasks.
It demonstrates the effectiveness of dropoutin reducing overfitting.1070POSDev TestIV OOTV OOEV OOBV IV OOTV OOEV OOBVLSTM-CNN 97.57 93.75 90.29 80.27 97.55 93.45 90.14 80.07LSTM-CNN-CRF 97.68 93.65 91.05 82.71 97.77 93.16 90.65 82.49NERDev TestIV OOTV OOEV OOBV IV OOTV OOEV OOBVLSTM-CNN 94.83 87.28 96.55 82.90 90.07 89.45 100.00 78.44LSTM-CNN-CRF 96.49 88.63 97.67 86.91 92.14 90.73 100.00 80.60Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).4.6 OOV Error AnalysisTo better understand the behavior of our model,we perform error analysis on Out-of-Vocabularywords (OOV).
Specifically, we partition eachdata set into four subsets ?
in-vocabulary words(IV), out-of-training-vocabulary words (OOTV),out-of-embedding-vocabulary words (OOEV) andout-of-both-vocabulary words (OOBV).
A word isconsidered IV if it appears in both the trainingand embedding vocabulary, while OOBV if nei-ther.
OOTV words are the ones do not appear intraining set but in embedding vocabulary, whileOOEV are the ones do not appear in embeddingvocabulary but in training set.
For NER, an en-tity is considered as OOBV if there exists at leaseone word not in training set and at least one wordnot in embedding vocabulary, and the other threesubsets can be done in similar manner.
Table 8 in-forms the statistics of the partition on each corpus.The embedding we used is Stanford?s GloVe withdimension 100, the same as Section 4.2.Table 9 illustrates the performance of our modelon different subsets of words, together with thebaseline LSTM-CNN model for comparison.
Thelargest improvements appear on the OOBV sub-sets of both the two corpora.
This demonstratesthat by adding CRF for joint decoding, our modelis more powerful on words that are out of both thetraining and embedding sets.5 Related WorkIn recent years, several different neural networkarchitectures have been proposed and successfullyapplied to linguistic sequence labeling such asPOS tagging, chunking and NER.
Among theseneural architectures, the three approaches mostsimilar to our model are the BLSTM-CRF modelproposed by Huang et al (2015), the LSTM-CNNs model by Chiu and Nichols (2015) and theBLSTM-CRF by Lample et al (2016).Huang et al (2015) used BLSTM for word-levelrepresentations and CRF for jointly label decod-ing, which is similar to our model.
But thereare two main differences between their modeland ours.
First, they did not employ CNNs tomodel character-level information.
Second, theycombined their neural network model with hand-crafted features to improve their performance,making their model not an end-to-end system.Chiu and Nichols (2015) proposed a hybrid ofBLSTM and CNNs to model both character- andword-level representations, which is similar to thefirst two layers in our model.
They evaluated theirmodel on NER and achieved competitive perfor-mance.
Our model mainly differ from this modelby using CRF for joint decoding.
Moreover, theirmodel is not truly end-to-end, either, as it utilizesexternal knowledge such as character-type, capi-talization and lexicon features, and some data pre-processing specifically for NER (e.g.
replacing allsequences of digits 0-9 with a single ?0?).
Re-cently, Lample et al (2016) proposed a BLSTM-CRF model for NER, which utilized BLSTM tomodel both the character- and word-level infor-mation, and use data pre-processing the same asChiu and Nichols (2015).
Instead, we use CNN tomodel character-level information, achieving bet-ter NER performance without using any data pre-processing.There are several other neural networks previ-ously proposed for sequence labeling.
Labeau etal.
(2015) proposed a RNN-CNNs model for Ger-man POS tagging.
This model is similar to theLSTM-CNNs model in Chiu and Nichols (2015),with the difference of using vanila RNN insteadof LSTM.
Another neural architecture employing1071CNN to model character-level information is the?CharWNN?
architecture (Santos and Zadrozny,2014) which is inspired by the feed-forward net-work (Collobert et al, 2011).
CharWNN obtainednear state-of-the-art accuracy on English POS tag-ging (see Section 4.3 for details).
A similar modelhas also been applied to Spanish and PortugueseNER (dos Santos et al, 2015) Ling et al (2015)and Yang et al (2016) also used BSLTM to com-pose character embeddings to word?s representa-tion, which is similar to Lample et al (2016).
Pengand Dredze (2016) Improved NER for Chinese So-cial Media with Word Segmentation.6 ConclusionIn this paper, we proposed a neural network archi-tecture for sequence labeling.
It is a truly end-to-end model relying on no task-specific resources,feature engineering or data pre-processing.
Weachieved state-of-the-art performance on two lin-guistic sequence labeling tasks, comparing withpreviously state-of-the-art systems.There are several potential directions for futurework.
First, our model can be further improvedby exploring multi-task learning approaches tocombine more useful and correlated information.For example, we can jointly train a neural net-work model with both the POS and NER tags toimprove the intermediate representations learnedin our network.
Another interesting direction isto apply our model to data from other domainssuch as social media (Twitter and Weibo).
Sinceour model does not require any domain- or task-specific knowledge, it might be effortless to applyit to these domains.AcknowledgementsThis research was supported in part by DARPAgrant FA8750-12-2-0342 funded under the DEFTprogram.
Any opinions, findings, and conclusionsor recommendations expressed in this material arethose of the authors and do not necessarily reflectthe views of DARPA.ReferencesRie Kubota Ando and Tong Zhang.
2005.
A frame-work for learning predictive structures from multipletasks and unlabeled data.
The Journal of MachineLearning Research, 6:1817?1853.Yoshua Bengio, Patrice Simard, and Paolo Frasconi.1994.
Learning long-term dependencies with gra-dient descent is difficult.
Neural Networks, IEEETransactions on, 5(2):157?166.James Bergstra, Olivier Breuleux, Fr?ed?eric Bastien,Pascal Lamblin, Razvan Pascanu, Guillaume Des-jardins, Joseph Turian, David Warde-Farley, andYoshua Bengio.
2010.
Theano: a cpu and gpumath expression compiler.
In Proceedings of thePython for scientific computing conference (SciPy),volume 4, page 3.
Austin, TX.Danqi Chen and Christopher Manning.
2014.
A fastand accurate dependency parser using neural net-works.
In Proceedings of EMNLP-2014, pages 740?750, Doha, Qatar, October.Hai Leong Chieu and Hwee Tou Ng.
2002.
Named en-tity recognition: a maximum entropy approach usingglobal information.
In Proceedings of CoNLL-2003,pages 1?7.Jason PC Chiu and Eric Nichols.
2015.
Named en-tity recognition with bidirectional lstm-cnns.
arXivpreprint arXiv:1511.08308.Kyunghyun Cho, Bart van Merri?enboer, Dzmitry Bah-danau, and Yoshua Bengio.
2014.
On the propertiesof neural machine translation: Encoder?decoder ap-proaches.
Syntax, Semantics and Structure in Statis-tical Translation, page 103.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.Hong-Jie Dai, Po-Ting Lai, Yung-Chun Chang, andRichard Tzong-Han Tsai.
2015.
Enhancing ofchemical compound and drug name recognition us-ing representative tag scheme and fine-grained tok-enization.
Journal of cheminformatics, 7(S1):1?10.Yann N Dauphin, Harm de Vries, Junyoung Chung,and Yoshua Bengio.
2015.
Rmsprop and equili-brated adaptive learning rates for non-convex opti-mization.
arXiv preprint arXiv:1502.04390.C?cero dos Santos, Victor Guimaraes, RJ Niter?oi, andRio de Janeiro.
2015.
Boosting named entity recog-nition with neural character embeddings.
In Pro-ceedings of NEWS 2015 The Fifth Named EntitiesWorkshop, page 25.Chris Dyer, Miguel Ballesteros, Wang Ling, AustinMatthews, and Noah A. Smith.
2015.
Transition-based dependency parsing with stack long short-term memory.
In Proceedings of ACL-2015 (Volume1: Long Papers), pages 334?343, Beijing, China,July.Radu Florian, Abe Ittycheriah, Hongyan Jing, andTong Zhang.
2003.
Named entity recognitionthrough classifier combination.
In Proceedings ofHLT-NAACL-2003, pages 168?171.1072Felix A Gers, J?urgen Schmidhuber, and Fred Cummins.2000.
Learning to forget: Continual prediction withlstm.
Neural computation, 12(10):2451?2471.Felix A Gers, Nicol N Schraudolph, and J?urgenSchmidhuber.
2003.
Learning precise timing withlstm recurrent networks.
The Journal of MachineLearning Research, 3:115?143.Rich Caruana Steve Lawrence Lee Giles.
2001.
Over-fitting in neural nets: Backpropagation, conjugategradient, and early stopping.
In Advances in Neu-ral Information Processing Systems 13: Proceed-ings of the 2000 Conference, volume 13, page 402.MIT Press.Jes?us Gim?enez and Llu?
?s M`arquez.
2004.
Svmtool: Ageneral pos tagger generator based on support vectormachines.
In In Proceedings of LREC-2004.Xavier Glorot and Yoshua Bengio.
2010.
Understand-ing the difficulty of training deep feedforward neuralnetworks.
In International conference on artificialintelligence and statistics, pages 249?256.Christoph Goller and Andreas Kuchler.
1996.
Learn-ing task-dependent distributed representations bybackpropagation through structure.
In Neural Net-works, 1996., IEEE International Conference on,volume 1, pages 347?352.
IEEE.Alan Graves, Abdel-rahman Mohamed, and GeoffreyHinton.
2013.
Speech recognition with deep recur-rent neural networks.
In Proceedings of ICASSP-2013, pages 6645?6649.
IEEE.Kaiming He, Xiangyu Zhang, Shaoqing Ren, and JianSun.
2015.
Delving deep into rectifiers: Surpass-ing human-level performance on imagenet classifi-cation.
In Proceedings of the IEEE InternationalConference on Computer Vision, pages 1026?1034.Sepp Hochreiter and J?urgen Schmidhuber.
1997.Long short-term memory.
Neural computation,9(8):1735?1780.Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard H.Hovy, and Eric P. Xing.
2016.
Harnessing deepneural networks with logic rules.
In Proceedings ofACL-2016, Berlin, Germany, August.Zhiheng Huang, Wei Xu, and Kai Yu.
2015.
Bidirec-tional lstm-crf models for sequence tagging.
arXivpreprint arXiv:1508.01991.Rafal Jozefowicz, Wojciech Zaremba, and IlyaSutskever.
2015.
An empirical exploration of re-current network architectures.
In Proceedings of the32nd International Conference on Machine Learn-ing (ICML-15), pages 2342?2350.Diederik Kingma and Jimmy Ba.
2014.
Adam: Amethod for stochastic optimization.
arXiv preprintarXiv:1412.6980.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proceedings of ACL-2010, pages 1?11, Uppsala, Sweden, July.Matthieu Labeau, Kevin L?oser, Alexandre Allauzen,and Rue John von Neumann.
2015.
Non-lexicalneural architecture for fine-grained pos tagging.
InProceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing, pages232?237.John Lafferty, Andrew McCallum, and Fernando CNPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of ICML-2001, volume951, pages 282?289.Guillaume Lample, Miguel Ballesteros, Sandeep Sub-ramanian, Kazuya Kawakami, and Chris Dyer.2016.
Neural architectures for named entity recog-nition.
In Proceedings of NAACL-2016, San Diego,California, USA, June.Yann LeCun, Bernhard Boser, John S Denker, Don-nie Henderson, Richard E Howard, Wayne Hubbard,and Lawrence D Jackel.
1989.
Backpropagation ap-plied to handwritten zip code recognition.
Neuralcomputation, 1(4):541?551.Dekang Lin and Xiaoyun Wu.
2009.
Phrase clusteringfor discriminative learning.
In Proceedings of ACL-2009, pages 1030?1038.Wang Ling, Chris Dyer, Alan W Black, Isabel Tran-coso, Ramon Fermandez, Silvio Amir, Luis Marujo,and Tiago Luis.
2015.
Finding function inform: Compositional character models for open vo-cabulary word representation.
In Proceedings ofEMNLP-2015, pages 1520?1530, Lisbon, Portugal,September.Gang Luo, Xiaojiang Huang, Chin-Yew Lin, and Za-iqing Nie.
2015.
Joint entity recognition and disam-biguation.
In Proceedings of EMNLP-2015, pages879?888, Lisbon, Portugal, September.Xuezhe Ma and Eduard Hovy.
2015.
Efficient inner-to-outer greedy algorithm for higher-order labeleddependency parsing.
In Proceedings of the EMNLP-2015, pages 1322?1328, Lisbon, Portugal, Septem-ber.Xuezhe Ma and Fei Xia.
2014.
Unsupervised de-pendency parsing with transferring distribution viaparallel guidance and entropy regularization.
InProceedings of ACL-2014, pages 1337?1348, Bal-timore, Maryland, June.Xuezhe Ma and Hai Zhao.
2012a.
Fourth-order depen-dency parsing.
In Proceedings of COLING 2012:Posters, pages 785?796, Mumbai, India, December.Xuezhe Ma and Hai Zhao.
2012b.
Probabilistic mod-els for high-order projective dependency parsing.Technical Report, arXiv:1502.04174.1073Xuezhe Ma, Zhengzhong Liu, and Eduard Hovy.
2016.Unsupervised ranking model for entity coreferenceresolution.
In Proceedings of NAACL-2016, SanDiego, California, USA, June.Christopher D Manning.
2011.
Part-of-speech tag-ging from 97% to 100%: is it time for some linguis-tics?
In Computational Linguistics and IntelligentText Processing, pages 171?189.
Springer.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: the Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of ACL-2005,pages 91?98, Ann Arbor, Michigan, USA, June 25-30.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in neural information processingsystems, pages 3111?3119.Vincent Ng.
2010.
Supervised noun phrase corefer-ence research: The first fifteen years.
In Proceed-ings of ACL-2010, pages 1396?1411, Uppsala, Swe-den, July.
Association for Computational Linguis-tics.Joakim Nivre and Mario Scholz.
2004.
Deterministicdependency parsing of English text.
In Proceedingsof COLING-2004, pages 64?70, Geneva, Switzer-land, August 23-27.Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.2012.
On the difficulty of training recurrent neuralnetworks.
arXiv preprint arXiv:1211.5063.Alexandre Passos, Vineet Kumar, and Andrew McCal-lum.
2014.
Lexicon infused phrase embeddings fornamed entity resolution.
In Proceedings of CoNLL-2014, pages 78?86, Ann Arbor, Michigan, June.Nanyun Peng and Mark Dredze.
2015.
Named en-tity recognition for chinese social media with jointlytrained embeddings.
In Proceedings of EMNLP-2015, pages 548?554, Lisbon, Portugal, September.Nanyun Peng and Mark Dredze.
2016.
Improvingnamed entity recognition for chinese social mediawith word segmentation representation learning.
InProceedings of ACL-2016, Berlin, Germany, Au-gust.Jeffrey Pennington, Richard Socher, and ChristopherManning.
2014.
Glove: Global vectors for wordrepresentation.
In Proceedings of EMNLP-2014,pages 1532?1543, Doha, Qatar, October.Lev Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InProceedings of CoNLL-2009, pages 147?155.Cicero D Santos and Bianca Zadrozny.
2014.
Learningcharacter-level representations for part-of-speechtagging.
In Proceedings of ICML-2014, pages1818?1826.Libin Shen, Giorgio Satta, and Aravind Joshi.
2007.Guided learning for bidirectional sequence classi-fication.
In Proceedings of ACL-2007, volume 7,pages 760?767.Anders S?gaard.
2011.
Semi-supervised condensednearest neighbor for part-of-speech tagging.
In Pro-ceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 48?52, Portland, Oregon,USA, June.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2014.Dropout: A simple way to prevent neural networksfrom overfitting.
The Journal of Machine LearningResearch, 15(1):1929?1958.Xu Sun.
2014.
Structure regularization for structuredprediction.
In Advances in Neural Information Pro-cessing Systems, pages 2402?2410.Erik F. Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the conll-2003 shared task:Language-independent named entity recognition.
InProceedings of CoNLL-2003 - Volume 4, pages 142?147, Stroudsburg, PA, USA.Erik F. Tjong Kim Sang and Jorn Veenstra.
1999.
Rep-resenting text chunks.
In Proceedings of EACL?99,pages 173?179.
Bergen, Norway.Kristina Toutanova, Dan Klein, Christopher D Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency net-work.
In Proceedings of NAACL-HLT-2003, Volume1, pages 173?180.Zhilin Yang, Ruslan Salakhutdinov, and WilliamCohen.
2016.
Multi-task cross-lingual se-quence tagging from scratch.
arXiv preprintarXiv:1603.06270.Matthew D Zeiler.
2012.
Adadelta: an adaptive learn-ing rate method.
arXiv preprint arXiv:1212.5701.1074
