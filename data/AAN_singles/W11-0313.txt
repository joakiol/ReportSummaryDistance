Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 106?114,Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational LinguisticsAssessing Benefit from Feature Feedback in Active Learningfor Text ClassificationShilpa AroraLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon Universityshilpaa@cs.cmu.eduEric NybergLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon Universityehn@cs.cmu.eduAbstractFeature feedback is an alternative to instancelabeling when seeking supervision from hu-man experts.
Combination of instance andfeature feedback has been shown to reduce thetotal annotation cost for supervised learning.However, learning problems may not benefitequally from feature feedback.
It is well un-derstood that the benefit from feature feed-back reduces as the amount of training dataincreases.
We show that other characteristicssuch as domain, instance granularity, featurespace, instance selection strategy and propor-tion of relevant text, have a significant effecton benefit from feature feedback.
We estimatethe maximum benefit feature feedback mayprovide; our estimate does not depend on howthe feedback is solicited and incorporated intothe model.
We extend the complexity mea-sures proposed in the literature and proposesome new ones to categorize learning prob-lems, and find that they are strong indicatorsof the benefit from feature feedback.1 IntroductionLinear classifiers model the response as a weightedlinear combination of the features in input instances.A supervised approach to learning a linear classifierinvolves learning the weights for the features fromlabeled data.
A large number of labeled instancesmay be needed to determine the class association ofthe features and learn accurate weights for them.
Al-ternatively, the user may directly label the features.For example, for a sentiment classification task, theuser may label features, such as words or phrases,as expressing positive or negative sentiment.
Priorwork (Raghavan et al, 2006; Zaidan et al, 2007)has demonstrated that users are able to reliably pro-vide useful feedback on features.Direct feedback on a list of features (Raghavan etal., 2006; Druck et al, 2008) is limited to simple fea-tures like unigrams.
However, unigrams are limitedin the linguistic phenomena they can capture.
Struc-tured features such as dependency relations, paths insyntactic parse trees, etc., are often needed for learn-ing the target concept (Pradhan et al, 2004; Joshiand Rose?, 2009).
It is not clear how direct featurefeedback can be extended straightforwardly to struc-tured features, as they are difficult to present visu-ally for feedback and may require special expertiseto comprehend.
An alternative approach is to seekindirect feedback on structured features (Arora andNyberg, 2009) by asking the user to highlight spansof text, called rationales, that support the instancelabel (Zaidan et al, 2007).
For example, when clas-sifying the sentiment of a movie review, rationalesare spans of text in the review that support the senti-ment label for the review.Assuming a fixed cost per unit of work, it mightbe cheaper to ask the user to label a few features, i.e.identify relevant features and their class association,than to label several instances.
Prior work (Ragha-van et al, 2006; Druck et al, 2008; Druck et al,2009; Zaidan et al, 2007) has shown that a combi-nation of instance and feature labeling can be usedto reduce the total annotation cost required to learnthe target concept.
However, the benefit from featurefeedback may vary across learning problems.
If wecan estimate the benefit from feature feedback for a106given problem, we can minimize the total annotationcost for achieving the desired performance by select-ing the optimal annotation strategy (feature feedbackor not) at every stage in learning.
In this paper, wepresent the ground work for this research problem byanalyzing how benefit from feature feedback variesacross different learning problems and what charac-teristics of a learning problem have a significant ef-fect on benefit from feature feedback.We define a learning problem (P = {D, G, F , L,I , S}) as a tuple of the domain (D), instance gran-ularity (G), feature representation (F ), labeled dataunits (L), amount of irrelevant text (I) and instanceselection strategy (S).With enough labeled data, we may not benefitfrom feature feedback.
Benefit from feature feed-back also depends on the features used to representthe instances.
If the feature space is large, we mayneed several labeled instances to identify the rele-vant features, while relatively fewer labeled featuresmay help us quickly find these relevant features.Apart from the feature space size, it also matterswhat types of features are used.
When hand craftedfeatures from a domain expert are used (Pradhan etal., 2004) we expect to gain less from feature feed-back as most of the features will be relevant.
Onthe other hand, when features are extracted automat-ically as patterns in annotation graphs (Arora et al,2010) feature feedback can help to identify relevantfeatures from the large feature space.In active learning, instances to be labeled are se-lectively sampled in each iteration.
Benefit from fea-ture feedback will depend on the instances that wereused to train the model in each iteration.
In the caseof indirect feature feedback through rationales or di-rect feature feedback in context, instances selectedwill also determine what features receive feedback.Hence, instance selection strategy should affect thebenefit from feature feedback.In text classification, an instance may contain alarge amount of text, and even a simple unigramrepresentation will generate a lot of features.
Oftenonly a part of the text is relevant for the classifica-tion task.
For example, in movie reviews, often thereviewers talk about the plot and characters in addi-tion to providing their opinion about the movie.
Of-ten this extra information is not relevant to the clas-sification task and bloats the feature space withoutadding many useful features.
With feature feedback,we hope to filter out some of this noise and improvethe model.
Thus, the amount of irrelevant informa-tion in the instance should play an important role indetermining the benefit from feature feedback.
Weexpect to see less of such noise when the text in-stance is more concise.
For example, a movie reviewsnippet (about a sentence length) tends to have lessirrelevant text than a full movie review (several sen-tences).
In addition to analyzing document instanceswith varying amount of noise, we also compare thebenefit from feature feedback for problems with dif-ferent granularity.
Granularity for a learning prob-lem is defined based on the average amount of textin its instances.Benefit from feature feedback will also depend onhow feedback is solicited from the user and how itis incorporated back into the model.
Independentlyfrom these factors, we estimate the maximum pos-sible benefit and analyze how it varies across prob-lems.
Next we describe measures proposed in theliterature and propose some new ones for categoriz-ing learning problems.
We then discuss our experi-mental setup and analysis.2 Related WorkThere has been little work on categorizing learn-ing problems and how benefit from feature feedbackvaries with them.
To the best of our knowledgethere is only one work in this area by Raghavan etal.
(2007).
They categorize problems in terms oftheir feature complexity.
Feature complexity is de-fined in terms of the minimum number of featuresrequired to learn a good classifier (close to maxi-mum performance).
If the concept can be describedby a weighted combination of a few well-selectedfeatures, it is considered to be of low complexity.In this estimate of complexity, an assumption ismade that the best performance is achieved whenthe learner has access to all available features andnot for any subset of the features.
This is a reason-able assumption for text classification problems withrobust learners like SVMs together with appropriateregularization and sufficient training data.Instead of evaluating all possible combinations offeatures to determine the minimum number of fea-tures required to achieve close to the best perfor-107mance, feature complexity is estimated using an in-telligent ranking of the features.
This ranking isbased on their discriminative ability determined us-ing a large amount of labeled data (referred to asoracle) and a feature selection criterion such as In-formation Gain (Rijsbergen, 1979).
It is intuitivethat the rate of learning, i.e., the rate at which per-formance improves as we add more features to themodel, is also associated with problem complexity.Raghavan et al (2007) define the feature learningconvergence profile (pfl) as the area under the fea-ture learning curve (performance vs. number of fea-tures used in training), given by:pfl =?log2Nt=1 F1(M, 2t)log2N ?
F1(M,N)(1)where F1(M, 2t) is the F1 score on the test datawhen using all M instances for training with topranked 2t features.
The features are added at an ex-ponentially increasing interval to emphasize the rel-ative increase in feature space size.
The three featurecomplexity measures proposed by Raghavan et al(2007) are the following: 1) Feature size complex-ity (Nf ): Logarithm (base 2) of the number of fea-tures needed to achieve 95% of the best performance(when all instances are available), 2) Feature profilecomplexity (Fpc), given by Fpc = 1 ?
pfl, and 3)Combined feature complexity (Cf ) , Cf = Fpc ?
nf ,incorporates both the learning profile and the num-ber of features required.In order to evaluate the benefit from feature feed-back, Raghavan et al (2007) use their tandem learn-ing approach of interleaving instance and featurefeedback (Raghavan et al, 2006), referred to asinteractive feature selection (ifs).
The featuresare labeled as ?relevant?
(feature discriminates wellamong the classes), or ?non-relevant/don?t know?.The labeled features are incorporated into learningby scaling the value of the relevant features by a con-stant factor in all instances.Raghavan et al (2007) measure the benefit fromfeature feedback as the gain in the learning speedwith feature feedback.
The learning speed measuresthe rate of performance improvement with increas-ing amount of supervision.
It is defined in terms ofthe convergence profile similar to feature learningconvergence profile in Equation 1, except in termsof the number of labeled units instead of the num-ber of features.
A labeled unit is either a labeledinstance or an equivalent set of labeled features withthe same annotation time.
The benefit from featurefeedback is then measured as the difference in theconvergence profile with interactive feature selec-tion (pifs) and with labeled instances only (pal).Raghavan et al (2007) analysed 9 corpora and358 binary classification tasks.
Most of these cor-pora, such as Reuters (Lewis, 1995), 20-newsgroup(Lang, 1995), etc., have topic-based category la-bels.
For all classification tasks, they used simpleand fixed feature space containing only unigram fea-tures (n-gram features were added where it seemedto improve performance).
They observed a negativecorrelation (r = ?0.65) between the benefit fromfeature feedback and combined feature complexity(Cf ), i.e., feature feedback accelerates active learn-ing by an amount that is inversely proportional tothe feature complexity of the problem.
If a conceptcan be expressed using a few well-selected featuresfrom a large feature space, we stand to benefit fromfeature feedback as few labeled features can providethis information.
On the other hand, if learning aconcept requires all or most of the features in thefeature space, there is little knowledge that featurefeedback can provide.3 Estimating Maximum Benefit &Additional MeasuresIn this section, we highlight some limitations of theprior work that we address in this work.Raghavan et al (2007) only varied the domainamong different problems they analyzed, i.e, onlythe variable D in our problem definition (P ={D,G,F, L, I, S}).
However, as motivated in theintroduction, other characteristics are also importantwhen categorizing learning problems and it is notclear if we will observe similar results on problemsthat differ in these additional characteristics.
In thiswork, we apply their measures to problems that dif-fer in these characteristics in addition to the domain.Analysis in Raghavan et al (2007) is specific totheir approach for incorporating feature feedbackinto the model, which may not work well for all do-mains and datasets as also mentioned in their work(Section 6.1).
It is not clear how their results can be108extended to alternate approaches for seeking and in-corporating feature feedback.
Thus, in this work weanalyze the maximum benefit a given problem canget from feature feedback independent of the feed-back solicitation and incorporation approach.Raghavan et al (2007) analyze benefit from fea-ture feedback at a fixed training data size of 42 la-beled units.
However, the difference between learn-ing problems may vary with the amount of labeleddata.
Some problems may benefit significantly fromfeature feedback even at relatively larger amount oflabeled data.
On the other hand, with very largetraining set, the benefit from feature feedback canbe expected to be small and not significant for allproblems and all problems will look similar.
Thus,we evaluate the benefit from feature feedback at dif-ferent amount of labeled data.Raghavan et al (2007) evaluate benefit from fea-ture feedback in terms of the gain in learning speed.However, the learning rate does not tell us how muchimprovement we get in performance at a given stagein learning.
In fact, even if at every point in thelearning curve performance with feature feedbackwas lower than performance without feature feed-back, the rate of convergence to the correspondingmaximum performance may still be higher when us-ing feature feedback.
Thus, in this work, in addi-tion to evaluating the improvement in the learningspeed, we also evaluate the improvement in the ab-solute performance at a given stage in learning.3.1 Determining the Maximum BenefitAnnotating instances with or without feature feed-back may require different annotation time.
It isonly fair to compare different annotation strategiesat same annotation cost.
Raghavan et al (2006)found that on average labeling an instance takes thesame amount of time as direct feedback on 5 fea-tures.
Zaidan et al (2007) found that on averageit takes twice as much time to annotate an instancewith rationales than to annotate one without ratio-nales.
In our analysis, we focus on feedback on fea-tures in context of the instance they occur in, i.e., in-direct feature feedback through rationales or directfeedback on features that occur in the instance be-ing labeled.
Thus, based on the findings in Zaidan etal.
(2007), we assume that on average annotating aninstance with feature feedback takes twice as muchtime as annotating an instance without feature feed-back.
We define a currency for annotation cost asAnnotation cost Units (AUs).
For an annotation bud-get of a AUs, we compare two annotation strategiesof annotating a instances without feature feedbackor a2 instances with feature feedback.In this work, we only focus on using feature feed-back as an alternative to labeled data, i.e., to pro-vide evidence about features in terms of their rele-vance and class association.
Thus, the best featurefeedback can do is provide as much evidence aboutfeatures as evidence from a large amount of labeleddata (oracle).
Let F1(k,Nm) be the F1 score of amodel trained with features that occur in m train-ing instances (Nm) and evidence for these featuresfrom k instances (k ?
m).
For an annotation budgetof a AUs, we define the maximum improvement inperformance with feature feedback (IPa) as the dif-ference in performance with feature feedback fromoracle on a2 training instances and performance witha training instances without feature feedback.IPa = F1(o,Na2)?
F1(a,Na) (2)where o is the number of instances in the oracledataset (o >> a).
We also compare annotationstrategies in terms of the learning rate similar toRaghavan et al (2007), except that we estimate andcompare the maximum improvement in the learningrate.
For an annotation budget of a AUs, we definethe maximum improvement in learning rate from 0to a AUs (ILR0?a) as follows.ILR0?a = pcpwFF ?
pcpwoFF (3)where pcpwFF and pcpwoFF are the convergenceprofiles with and without feature feedback at sameannotation cost, calculated as follows.pcpwFF =?log2 a2t=1 F1(o,N2t)log2 a2 ?
F1(o,Na2 )(4)pcpwoFF =?log2at=2 F1(2t, N2t)(log2a?
1)?
F1(a,Na)(5)where 2t denotes the training data size in iterationt.
Like Raghavan et al (2007), we use exponen-tially increasing intervals to emphasize the relativeincrease in the training data size, since adding a few109labeled instances earlier in learning will give us sig-nificantly more improvement in performance thanadding the same number of instances later on.3.2 Additional MetricsThe feature complexity measures require an ?ora-cle?, simulated using a large amount of labeled data,which is often not available.
Thus, we need mea-sures that do not require an oracle.Benefit from feature feedback will depend on theuncertainty of the model on its predictions, since itsuggests uncertainty on the features and hence scopefor benefit from feature feedback.
We use the proba-bility of the predicted label from the model as an es-timate of the model?s uncertainty.
We evaluate howbenefit from feature feedback varies with summarystatistics such as mean, median and maximum prob-ability from the model on labels for instances in aheld out dataset.4 Experiments, Results and ObservationsIn this section, we describe the details of our exper-imental setup followed by the results.4.1 DataWe analyzed three datasets: 1) Movie reviewswith rationale annotations by Zaidan et al (2007),where the task is to classify the sentiment (posi-tive/negative) of a review, 2) Movie review snippetsfrom Rotten Tomatoes (Pang and Lee., 2005), and 3)WebKB dataset with the task of classifying whetheror not a webpage is a faculty member?s homepage.Raghavan et al (2007) found that the webpage clas-sification task has low feature complexity and ben-efited the most from feature feedback.
We compareour results on this task and the sentiment classifica-tion task on the movie review datasets.4.2 Experimental SetupTable 1 describes the different variables and theirpossible values in our experiments.
We make a log-ical distinction for granularity based on whether aninstance in the problem is a document (several sen-tences) or a sentence.
Labeled data is composed ofinstances and their class labels with or without fea-ture feedback.
As discussed in Section 3.1, instanceswith feature feedback take on average twice as muchtime to annotate as instances without feature feed-back.
Thus, we measure the labeled data in terms ofthe number of annotation cost units which may meandifferent number of labeled instances based on theannotation strategy.
We used two feature configura-tions of ?unigram only?
and ?unigram+dependencytriples?.
The unigram and dependency annotationsare derived from the Stanford Dependency Parser(Klein and Manning, 2003).Rationales by definition are spans of text in a re-view that convey the sentiment of the reviewer andhence are the part of the document most relevant forthe classification task.
In order to vary the amountof irrelevant text, we vary the amount of text (mea-sured in terms of the number of characters) aroundthe rationales that is included in the instance repre-sentation.
We call this the slack around rationales.When using the rationales with or without the slack,only features that overlap with the rationales (andthe slack, if used) are used to represent the instance.Since we only have rationales for the movie reviewdocuments, we only studied the effect of varying theamount of irrelevant text on this dataset.Variable Possible ValuesDomain (D) {Movie Review classifica-tion (MR), Webpage classi-fication (WebKB)}Instance Granu-larity (G){document (doc), sentence(sent)}Feature Space (F ) {unigram only (u), uni-gram+dependency (u+d)}Labeled Data(#AUs) (L){64, 128, 256, 512, 1024}Irrelevant Text (I) {0, 200, 400, 600,?
}Instance SelectionStrategy (S)){deterministic (deter), un-certainty (uncert)}Table 1: Experiment space for analysis of learning prob-lems (P = {D,G,F, L, I, S})For all our experiments, we used Support Vec-tor Machines (SVMs) with linear kernel for learn-ing (libSVM (Chang and Lin, 2001) in Minorthird(Cohen, 2004)).
For identifying the discrimina-tive features we used the information gain score.For all datasets we used 1800 total examples withequal number of positive and negative examples.
We110held out 10% of the data for estimating model?s un-certainty as explained in Section 3.2.
The resultswe present are averaged over 10 cross validationfolds on the remaining 90% of the data (1620 in-stances).
In a cross validation fold, 10% data is usedfor testing (162 instances) and all of the remaining1458 instances are used as the ?oracle?
for calculat-ing the feature complexity measures and estimatingthe maximum benefit from feature feedback as dis-cussed in Sections 2 and 3.1 respectively.
The train-ing data size is varied from 64 to 1024 instances(from the total of 1458 instances for training in afold), based on the annotation cost budget.
Instanceswith their label are added to the training set either inthe original order they existed in the dataset, i.e.
noselective sampling (deterministic), or in the decreas-ing order of current model?s uncertainty on them.Uncertainty sampling in SVMs (Tong and Koller,2000) selects the instances closest to the decisionboundary since the model is expected to be most un-certain about these instances.
In each slice of thedata, we ensured that there is equal distribution ofthe positive and negative class.
SVMs do not yieldprobabilistic output but a decision boundary, a com-mon practice is to fit the decision values from SVMsto a sigmoid curve to estimate the probability of thepredicted class (Platt, 1999).4.3 Results and AnalysisTo determine the effect of various factors on benefitfrom feature feedback, we did an ANOVA analysiswith Generalized Linear Model using a 95% confi-dence interval.
The top part of Table 2 shows theaverage F1 score for the two annotation strategiesat same annotation cost.
As can be seen, with fea-ture feedback, we get a significant improvement inperformance.Next we analyze the significance of the effect ofvarious problem characteristics discussed above onbenefit from feature feedback in terms of improve-ment in performance (IP ) at given annotation costand improvement in learning rate (ILR).
Improve-ment in learning rate is calculated by comparingthe learning profile for the two annotation strategieswith increasing amount of labeled data, up to themaximum annotation cost of 1024 AUs.As can be seen from the second part of Table 2,most of the factors have a significant effect on bene-fit from feature feedback.
The benefit is significantlyhigher for the webpage classification task than thesentiment classification task in the movie review do-main.
We found that average feature complexity forthe webpage classification task (Nf = 3.07) to belower than average feature complexity for the senti-ment classification task (Nf = 5.18) for 1024 train-ing examples.
Lower feature complexity suggeststhat the webpage classification concept can be ex-pressed with few keywords such as professor, fac-ulty, etc., and with feature feedback we can quicklyidentify these features.
Sentiment on the other handcan be expressed in a variety of ways which explainsthe high feature complexity.The benefit is more for document granularity thansentence granularity, which is intuitive as featurespace is substantially larger for documents and weexpect to gain more from the user?s feedback onwhich features are important.
This difference is sig-nificant for improvement in the learning rate andmarginally significant for improvement in perfor-mance.
Note that here we are comparing docu-ments (with or without rationale slack) and sen-tences.
However, documents with low rationaleslack should have similar amount of noise as a sen-tence.
Also, a significant difference between do-mains suggests that documents in WebKB domainmight be quite different from those in Movie Reviewdomain.
This may explain the marginal significantdifference between benefit for documents and sen-tences.
To understand the effect of granularity alone,we compared the benefit from feature feedback fordocuments (without removing any noise) and sen-tences in movie review domain only and we foundthat this difference in also not significant.
Thus, con-trary to our intuition, sentences and documents seemto benefit equally from feature feedback.The benefit is more when the feature space islarger and more diverse, i.e., when dependency fea-tures are used in addition to unigram features.
Wefound that on average adding dependency featuresto unigram features increases the feature space bya factor of 10.
With larger feature space, featurefeedback can help to identify a few relevant features.As can also be seen, feature feedback is more help-ful when there is more irrelevant text, i.e., there isnoise that feature feedback can help to filter out.Unlike improvement in performance, the improve-111ment in learning rate does not decrease monoton-ically as the amount of rationale slack decreases.This supports our belief that improvement in perfor-mance does not necessarily imply improvement inthe learning rate.
We saw similar result when com-paring benefit from feature feedback at different in-stance granularity.
Improvement in learning rate forproblems with different granularity was statisticallysignificant but improvement in performance was notsignificant.
Thus, both metrics should be used whenevaluating the benefit from feature feedback.We also observe that when training examples areselectively sampled as the most uncertain instances,we gain more from feature feedback than withoutselective sampling.
This is intuitive as instancesthe model is uncertain about are likely to containfeatures it is uncertain about and hence the modelshould benefit from feedback on features in these in-stances.
Next we evaluate how well the complexitymeasures proposed in Raghavan et al (2007) corre-late with improvement in performance and improve-ment in learning rate.V ar.
V alues AvgF1 GroupStrat.wFF 78.2 AwoFF 68.2 BV ar.
V alues AvgIP GrpIP AvgILR GrpILRDWebKB 11.9 A 0.32 AMR 8.0 B 0.20 BGDoc 10.9 A 0.30 ASent 9.0 A 0.22 BFu+d 12.1 A 0.30 Au 7.8 B 0.22 BI?
12.8 A 0.34 A600 11.2 A B 0.23 B400 11.1 A B 0.26 A B200 9.8 B 0.26 A B0 4.8 C 0.21 BSUncer.
12.7 A 0.32 ADeter.
7.1 B 0.20 BTable 2: Effect of variables defined in Table 1 on benefitfrom feature feedback.
AvgIP is the average increase inperformance (F1) and AvgILR is the average increase inthe learning rate.
Different letters in GrpIP and GrpILRindicate significantly different results.For a given problem with an annotation cost bud-get of a AUs, we calculate the benefit from featurefeedback by comparing the performance with fea-ture feedback on a2 instances and the performancewithout feature feedback on a instances as describedin Section 3.1.
The feature complexity measures arecalculated using a2 instances, since it should be thecharacteristics of these a2 training instances that de-termine whether we would benefit from feature feed-back on these a2 instances or from labeling newa2instances.
As can be seen from Table 3, the correla-tion of feature complexity measures with both mea-sures of benefit from feature feedback is strong, neg-ative and significant.
This suggests that problemswith low feature complexity, i.e.
concepts that canbe expressed with few well-selected features, benefitmore from feature feedback.It is intuitive that the benefit from feature feed-back decreases as amount of labeled data increases.We found a significant negative correlation (?0.574)between annotation budget (number of AUs) andimprovement in performance with feature feedback.However, note that this correlation is not verystrong, which supports our belief that factors otherthan the amount of labeled data affect benefit fromfeature feedback.Measure R(IP ) R(ILR)Nf -0.625 -0.615Fpc -0.575 -0.735Cf -0.603 -0.629Table 3: Correlation coefficient (R) for feature size com-plexity (Nf ), feature profile complexity (Fpc) and com-bined feature complexity (Cf ) with improvement in per-formance (IP ) and improvement in learning rate (ILR).All results are statistically significant (p < 0.05)Feature complexity measures require an ?oracle?simulated using a large amount of labeled datawhich is not available for real annotation tasks.In Section 3.2, we proposed measures based onmodel?s uncertainty that do not require an oracle.We calculate the mean, maximum and median ofthe probability scores from the learned model on in-stances in the held out dateset.
We found a signifi-cant but low negative correlation of these measureswith improvement in performance with feature feed-back (maxProb = ?0.384, meanProb = ?0.256,medianProb = ?0.242).
This may seem counter-intuitive.
However, note that when the training datais very small, the model might be quite certain about112its prediction even when it is wrong and feature feed-back may help by correcting the model?s beliefs.
Weobserved that these probability measures have onlymedium and significant positive correlation (around0.5) with training datasize.
Also, the held out datasetwe used may not be representative of the whole setand using a larger dataset may give us more accurateestimate of the model?s uncertainty.
There are alsoother ways to measure the model?s uncertainty, forexample, in SVMs the distance of an instance fromthe decision boundary gives us an estimate of themodel?s uncertainty about that instance.
We plan toexplore additional measures for model?s uncertaintyin the future.5 Conclusion and Future WorkIn this work, we analyze how the benefit from fea-ture feedback varies with different problem charac-teristics and how measures for categorizing learningproblems correlate with benefit from feature feed-back.
We define a problem instance as a tuple ofdomain, instance granularity, feature representation,labeled data, amount of irrelevant text and selectivesampling strategy.We compare the two annotation strategies, withand without feature feedback, in terms of both im-provement in performance at a given stage in learn-ing and improvement in learning rate.
Instead ofevaluating the benefit from feature feedback us-ing a specific feedback incorporation approach, weestimate and compare how the maximum benefitfrom feature feedback varies across different learn-ing problems.
This tells us what is the best featurefeedback can do for a given learning problem.We find a strong and significant correlation be-tween feature complexity measures and the twomeasures of maximum benefit from feature feed-back.
However, these measures require an ?ora-cle?, simulated using a large amount of labeled datawhich is not available in real world annotation tasks.We present measures based on the uncertainty of themodel on its prediction that do not require an oracle.The proposed measures have a low but significantcorrelation with benefit from feature feedback.
Inour current work, we are exploring other measuresof uncertainty of the model.
It is intuitive that a met-ric that measures the uncertainty of the model onparameter estimates should correlate strongly withbenefit from feature feedback.
Variance in param-eter estimates is one measure of uncertainty.
TheBootstrap or Jacknife method (Efron and Tibshirani,1994) of resampling from the training data is oneway of estimating variance in parameter estimatesthat we are exploring.So far only a linear relationship of various mea-sures with benefit from feature feedback has beenconsidered.
However, some of these relationshipsmay not be linear or a combination of several mea-sures together may be stronger indicators of the ben-efit from feature feedback.
We plan to do furtheranalysis in this direction in the future.We only considered one selective sampling strat-egy based on model?s uncertainty which we foundto provide more benefit from feature feedback.
Inthe future, we plan to explore other selective sam-pling strategies.
For example, density-based sam-pling (Donmez and Carbonell, 2008) selects the in-stances that are representative of clusters of simi-lar instances, and may facilitate more effective feed-back on a diverse set of features.In this work, feature feedback was simulated us-ing an oracle.
Feedback from the users, however,might be less accurate.
Our next step will be to ana-lyze how the benefit from feature feedback varies asthe quality of feature feedback varies.Our eventual goal is to estimate the benefit fromfeature feedback for a given problem so that the rightannotation strategy can be selected for a given learn-ing problem at a given stage in learning and the totalannotation cost for learning the target concept canbe minimized.
Note that in addition to the charac-teristics of the labeled data analyzed so far, expectedbenefit from feature feedback will also depend onthe properties of the data to be labeled next for thetwo annotation strategies - with or without featurefeedback.AcknowledgmentsWe thank Carolyn P.
Rose?, Omid Madani, HemaRaghavan, Jaime Carbonell, Pinar Donmez andChih-Jen Lin for helpful discussions, and the re-viewers for their feedback.
This work is supportedby DARPA?s Machine Reading program under con-tract FA8750-09-C-0172.113ReferencesShilpa Arora and Eric Nyberg.
2009.
Interactive annota-tion learning with indirect feature voting.
In Proceed-ings of NAACL-HLT 2009 (Student Research Work-shop).Shilpa Arora, Elijah Mayfield, Carolyn Penstein Rose?,and Eric Nyberg.
2010.
Sentiment classificationusing automatically extracted subgraph features.
InProceedings of the Workshop on Emotion in Text atNAACL.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIB-SVM: a library for support vector machines.
Soft-ware available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.William W. Cohen.
2004.
Minorthird: Methods for iden-tifying names and ontological relations in text usingheuristics for inducing regularities from data.Pinar Donmez and Jaime G. Carbonell.
2008.
PairedSampling in Density-Sensitive Active Learning.
InProceedings of the International Symposium on Arti-ficial Intelligence and Mathematics.Gregory Druck, Gideon Mann, and Andrew McCallum.2008.
Learning from labeled features using general-ized expectation criteria.
In SIGIR ?08: Proceedingsof the 31st annual international ACM SIGIR confer-ence on Research and development in information re-trieval, pages 595?602, New York, NY, USA.
ACM.Gregory Druck, Burr Settles, and Andrew McCallum.2009.
Active learning by labeling features.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing (EMNLP).
Associationfor Computational Linguistics.B.
Efron and R.J. Tibshirani.
1994.
An introduction tothe bootstrap.
Monographs on Statistics and AppliedProbability.
Chapman and Hall/CRC, New York.Mahesh Joshi and Carolyn Penstein Rose?.
2009.
Gen-eralizing dependency features for opinion mining.
InACL-IJCNLP ?09: Proceedings of the ACL-IJCNLP2009 Conference Short Papers, pages 313?316, Mor-ristown, NJ, USA.
Association for Computational Lin-guistics.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In ACL ?03: Proceedingsof the 41st Annual Meeting on Association for Compu-tational Linguistics, pages 423?430, Morristown, NJ,USA.
Association for Computational Linguistics.K.
Lang.
1995.
NewsWeeder: Learning to filter net-news.
In 12th International Conference on MachineLearning (ICML95), pages 331?339.D.
Lewis.
1995.
The reuters-21578 text categorizationtest collection.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploitingclass relationships for sentiment categorization withrespect to rating scales.
In Proceedings of ACL.John C. Platt.
1999.
Probabilistic outputs for supportvector machines and comparisons to regularized like-lihood methods.
In ADVANCES IN LARGE MARGINCLASSIFIERS, pages 61?74.
MIT Press.Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H.Martin, and Dan Jurafsky.
2004.
Shallow seman-tic parsing using support vector machines.
In Pro-ceedings of the Human Language Technology Con-ference/North American chapter of the Association ofComputational Linguistics (HLT/NAACL).Hema Raghavan, Omid Madani, and Rosie Jones.
2006.Active learning with feedback on features and in-stances.
Journal of Machine Learning Research,7:1655?1686.Hema Raghavan, Omid Madani, and Rosie Jones.
2007.When will feature feedback help?
quantifying thecomplexity of classification problems.
In IJCAI Work-shop on Human in the Loop Computing.C.
J.
Van Rijsbergen.
1979.
Information Retrieval.
But-terworths, London, 2 edition.Simon Tong and Daphne Koller.
2000.
Support vectormachine active learning with applications to text clas-sification.
In JOURNAL OF MACHINE LEARNINGRESEARCH, pages 999?1006.Omar Zaidan, Jason Eisner, and Christine Piatko.
2007.Using ?annotator rationales?
to improve machinelearning for text categorization.
In Human LanguageTechnologies: Proceedings of the Annual Conferenceof the North American Chapter of the Association forComputational Linguistics (NAACL-HLT), pages 260?267, Rochester, NY, April.114
