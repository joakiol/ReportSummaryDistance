Toward a Unified Evaluation Method for Multiple Reading SupportSystems: A Reading Speed-based ProcedureKatsunori KOTANINational Institute of Infor-mation and CommunicationsTechnology3-5 Hikaridai, Seika-cho,Soraku-gun, Kyoto, Japan,619-0289kat@khn.nict.go.jpTakehiko YOSHIMIRyukoku University1-5, Yokotani, Setaoe-cho,Otsu-shi, Shiga, Japan,520-2195Takeshi KUTSUMISharp Corporation492 Minosho-cho, Yamato-koriyama-shi, Nara, Japan,639-1185Ichiko SATASharp Corporation492 Minosho-cho, Yamatokoriyama-shi,Nara, Japan, 639-1185Hitoshi ISAHARANational Institute of Information andCommunications Technology3-5 Hikaridai, Seika-cho, Soraku-gun,Kyoto, Japan, 619-0289AbstractThis paper proposes a unified evalua-tion method for multiple reading sup-port systems such as a sentencetranslation system and a word transla-tion system.
In reading a non-nativelanguage text, these systems aim tolighten the reading burden.
When weevaluate the performance of these sys-tems, we cannot rely solely on thesetests, as the output forms are different.Therefore, we must assess the perform-ance of these systems based on the us-ers?
reading comprehension andreading speed.
We will further supportour findings with experimental results.They show that the reading-speed pro-cedure is able to evaluate the supportsystems, as well as, the comprehension-based procedure proposed by Ohguro(1993) and Fuji et al (2001).1 IntroductionThis paper presents an evaluation method fordifferent reading support systems such as a sen-tence-machine translation system (henceforth,an MT-system) and a word/phrase translationsystem (henceforth, a w/p-MT-system).
Al-though, there are various manual/automaticevaluation methods for these systems, e.g.,BLEU (Papineni et al 2002), these methods arebasically incapable of dealing with an MT-system and a w/p-MT-system at the same time,as they have different output forms.
On the con-trary, there are further methods which examinethe efficacy of these systems (Ohguro 1993; Fujiet al 2001).
These studies demonstrate the ef-fectiveness of the reading support systems bycomparing reading comprehension test scoresbetween an English-only text and the one withoutputs of either an MT-system (Fuji et al 2001)or a w/p-MT-system (Ohguro 1993).In our evaluation method, we examined thesystem based not only con comprehension butalso on speed, i.e., reading efficacy (Alderson2000).
If the system supports a user in an appro-priate way, then the reading efficacy would in-crease from the bottom line, i.e., text without anysupport.
The previous studies focused mainly onreading comprehension.
We will now broadenour examination to include reading speed.We are able to evaluate a system based onsingle sentences, as we measure sentence-reading speed.
In contrast, we are unable tocarry out such a local domain evaluation solelybased on the comprehension performance.This paper is organized as follows: Section 2reviews the previous studies, which evaluatedreading support systems based on the compre-hension performance, i.e., Ohguro (1993) andFuji et al (2001); Section 3 describes ourevaluation method, which evaluates both an244MT-system and a w/p-MT-system based onspeed performance; Section 4 reports the ex-perimental results.
Through the experiments,we confirmed that the speed performance-basedevaluation basically parallels the comprehensionperformance-based evaluation; and finally Sec-tion 5 presents our conclusions and future work.2 The Comprehension-based Methods2.1 Ohguro (1993)Ohguro (1993) carried out an experiment inwhich the efficacy of an English-Japanese w/p-MT-system was examined and reported that aw/p-MT-system would be of more aid to thosewith a lower reading ability.
Fifty-four non-native English speakers took part in the experi-ment.
Ohguro (1993) prepared 28 texts with 80comprehension questions extracted from variousTest of English for International Communication(TOEIC) texts.The experiment held two phases.
First, allthe participants read 14 English-only texts andanswered 40 comprehension questions.
On thebasis of the test score, the participants were di-vided into two groups so as to balance the read-ing ability between them.
Then, Ohguro (1993)gave English-only texts to one group, the controlgroup, and provided texts supported with the aw/p-MT-system to the other group.Ohguro (1993) hypothesized that the controlgroup would get similar test scores on both tests,as opposed to varying test scores from the othergroup.
In addition, it was predicted that thescores of the non-control group would depend onthe reading ability of the group members withrespect to TOEIC scores (Hypothesis I).
That is,a higher test score would be expected for thosewith a lower TOEIC score group.
Thus, Hy-pothesis I was incorrect given the results.
Oh-guro (1993) reanalysed the increase in the testscores by dividing that group into two.
Underthis revised analysis, he hypothesized that agreater increase in score would be shown in thesecond test by those with lower initial scores (therevised Hypothesis I).
This revised hypothesiswas correct given the result.
Ohguro (1993) con-cluded that the supporting effect of a w/p-MT-system was greater for those who had a lowerreading ability than those highly skilled readers.2.2 Fuji et al (2001)Fuji et al (2001) examined how the efficacy ofan English-Japanese MT system varied depend-ing on English reading ability.
Approximately200 non-native English speakers participated inthe experiment.
The participants were dividedinto 12 groups based on their TOEIC scores.The score range was between (i) less than 395and (ii) more than 900.
Fuji et al (2001) pre-pared three types of texts.
One was an English-only text as a control text, another containedonly translated sentences by an MT-system,and the other involved both English texts andthe MT-system outputs.
Each participant read14 texts, and answered 40 comprehension ques-tions.Through this experiment, Fuji et al (2001)observed that translation-only texts would de-grade the test scores for the higher TOEIC scoregroup, while the lower score group exhibited nodegrading effect.
In addition, they found thatEnglish texts with MT-outputs might increasethe test scores for the lower score group moregreatly than the higher score group.With respect to the test completion time, Fujiet al (2001) observed that an MT-system highlyshortened the time for the lower score grouprelative to the higher score group.2.3 SummaryThrough the surveys of these studies, we wereable to confirm that both a w/p-MT-system andan MT-system exhibited greater supporting ef-fects on the lower TOEIC score group than thehigher TOEIC score group.3 Evaluation with Reading Speed3.1 The purposeThe purpose of our evaluation is to pursue theefficacy of reading support systems with respectnot only to the users?
reading ability but also tothe readability of a complete text or a single sen-tence.
That is, we would like to explicatethrough the evaluation whether the supportingeffect might change due to the text propertiessuch as complexity of a syntactic structure, fa-miliarity of words, and so on.In order to depict such a local effect, we as-sume that the comprehension-based evaluation245would be inappropriate, as it is inefficient toassign a comprehension question to each sen-tence.
Suppose that we could evaluate readingsupport systems regarding such a local domain.Then, we could choose which system is proper,depending on his/her reading ability and thereadability of a text.
Such usage of reading sup-port systems would be useful.3.2 Reading Speed as an Evaluation CriterionIn our evaluation method, we adopt readingspeed performance as an evaluation criterion inaddition to the comprehension performance.There are three reasons for this adoption of read-ing speed.First, in contrast to reading comprehension,we can measure sentence-reading speed, andthus we can examine system efficacy on a sen-tence-level.Secondly, reading speed can be measuredwith any texts which is readable by the readingsupport systems.
For instance, we can evaluatesystem efficacy for texts such as newspapers,magazine articles, web pages, emails, and so on.By contrast, the comprehension-based evalua-tion requires comprehension questions.Thirdly, as shown below, we have statisti-cally found that the reading speed reflects thereadability of a sentence.
We confirmed thepositive correlation (r=0.7, p<0.01) betweenreading speed and readability of a text calculatedwith the so-called readability formula (Flesch1948).
Given this positive correlation, we as-sumed that reading speed indicates readability.Thus, a direct relationship exists between read-ability and reading speed.3.3 Reading Speed-based EvaluationMethodAssuming that reading speed reflects text read-ability, we can further assume that the readingsupport systems would affect text readability.That is, the positive supporting effect of a sys-tem would increase the text readability.
Giventhis, we can evaluate the efficacy of a system onthe basis of reading speed.Our evaluation method accepts the positiveeffect of a system if the reading speed is in-creased.
When the reading speed remains in-variant, or decreases, the method regards asystem as inefficient.
Thus, if we compare thereading speed between a supported and a non-supported text, the increase of speed should begreater for those who have a lower reading abil-ity than the highly skilled people on the basis ofprevious studies.4 Evaluation Experiment4.1 The Experimental PurposeWe conducted an experiment in order to exam-ine the validity of our method.
Given the read-ing speed evaluation method, it is predictedthat reading speed would reflect readability ofa text (Hypothesis 1) and reader?s ability (Hy-pothesis 2).As for readability of a text, we assume thatsupporting systems would increase readabilityof a text.
Therefore, we set the following hy-pothesis:Hypothesis 1:A non-supported English text would be themost difficult to read, whereas a manuallytranslated Japanese text would be the easiest.Supported text would fall mid-range.The efficacy of the supporting systems is in-versely related to the reader?s ability, as the pre-vious studies have shown.
Therefore, wepropose the following hypothesis:Hypothesis2:The inverse relation is detectable between thereading ability and the reading speed increase.4.2 The Experimental DesignOne hundred and two non-native Englishspeakers participated in the experiment.
Wedivided the participants into three groupsbased on their TOEIC scores: (i) those with alower score (400-595 pts.
), (ii) those with anintermediate score (600-795 pts.
); and (iii)those with a higher score (800-995 pts.).
Thegroup sizes were: (i) = 36, (ii) = 36, and (iii) =30.
We statistically compared average testscores and reading speed among these groups.We prepared eighty-four texts out of oursourced TOEIC texts.
Each text consists of apassage and some comprehension questions.We added outputs of supporting systems toeach text.246In this experiment, we examined the effi-cacy of the following supporting systems: asentence translation system, a word/phrasetranslation system, and a chunker.
Thus, wecreated four types of test texts: (i) Englishtexts glossed with sentence translations (here-after, E&MT); (ii) machine-translated texts(MT); (iii) English texts glossed with wordtranslation (RUB); and (iv) English texts withword/phrase boundary markers (CHU).In addition, we prepared two types of con-trol texts.
One is a raw English text, and theother is a human-translated Japanese text.
Werandomly selected sixteen texts from each textgroup and distributed eighty-four to each par-ticipant.
Thus, the participants are exposed toa variety of texts.In the experiment we used a reading processmonitoring tool and recorded the reading timeper sentence (see Yoshimi et al 2005 for furtherdescription).
We calculated the sentence read-ing speed based on words per minute (WPM)read.
As the cursor moves over each numberbar, the text is displayed sentence-by-sentence.See Figure 1.
There is no limit to how manytimes a sentence can be viewed.Figure 1.
Screenshot of the monitoring toolWe omitted the machine-translated wordsand focused solely on the number of Englishwords to calculate the reading speed.
Therefore,we were able to directly compare the readingspeed of a supported text to that of a non-supported English text.The goal of this study is to depict the effi-cacy of the support systems.
Hence, the actualreading speed of an English and Japanesemixed text was out of the scope.
If readingspeed was calculated based on both Englishand Japanese words, the reading speed of asupported text would be faster than an Englishtext, even though the reading time was thesame.
This is due to a greater number of wordsin the supported text.
Therefore, we calculatedreading speed based solely on English words toaccount for this implausible effect.
We alsoapplied this procedure to a manually translatedJapanese text.4.3 Experimental Results4.3.1 Tested DataBefore presenting the experimental results,one clarification is in order here.
We chose toanalyse a manageable 13 reading texts of thewhole data, i.e., 84 reading texts.
The textswe used varied in topic, style, and length.
Forinstance, they were article-based texts, reports,and advertisements.
Among these texts, weexamined article type texts.There were two reasons for this limitation.One concern was with the performance of thereading support systems.
We assumed that thesystem performance was dependant on textstyles, and that the system would most effec-tively support reading of article type texts be-cause they contained less stylistic variationscompared with other types of texts, particu-larly, advertisements.The other concern was with text length.Article type texts tended to be longer than theothers, and hence were more conducive to thesupporting effect of the systems as shown inTable 1.Text Words SentencesNon-article texts* 89.6 5.9Article texts 142.9 9.6Table 1.
Article texts and non-article texts*reports, advertisements, and announcements averagedtogether4.3.2 Testing Hypothesis 1: Reading SpeedWe are able to conclude in Hypothesis 1 thatthe reading speed of a supported text is slowerthan that of a non-supported English text.
SeeTable 2.
Therefore, the hypothesis is incorrectwith respect to the slowest speeds.
However, inregards to the fastest reading speed, Hypothesis1 was supported.247Text* Mean SD 95% CI of MeanENG 75.1 31.9 70.1 to 80.3CHU 74.1 36.5 68.3 to 80.1RUB 65.5 28.0 61.1 to 70.1MT 102.6 57.0 93.2 to 111.9E&MT 70.3 31.7 65.3 to 75.2JPN 163.1 80.7 149.7 to 176.6Table 2.
Mean reading speed*ENG, English texts; CHU, English texts marked withword/phrase boundary; RUB, English texts glossed withmachine-translated words; MT, machine-translated texts;E&MT, English texts glossed with machine-translatedsentences; JPN, manually-translated texts4.3.3 Testing Hypothesis 1: ComprehensionHypothesis 1 was not supported for the lowestcomprehension scores, paralleling readingspeed results.
Thus, the lowest score wasfound in the MT texts as shown in Table 3.The results supported the hypothesis in respectto the JPN texts scoring highest.Text Mean SD 95% CI of MeanENG 0.84 0.22 0.80 to 0.87CHU 0.84 0.25 0.80 to 0.88RUB 0.83 0.23 0.79 to 87MT 0.81 0.22 0.77 to 0.85E&MT 0.90 0.16 0.88 to 0.93JPN 0.93 0.15 0.90 to 0.95Table 3.
Mean percentatge of questions answered correctly.In order to analyse the reading data in moredetail, we compared the correct answer ratesamong the TOEIC test score groups.
We di-vided the participants into three groups based onTOEIC scores: 400-595 (BEGinner), 600-795(INTermediate), and 800-995 (ADVanced).The correct answer rate of each group isshown in Table 4.
In the BEG class, the lowestrate was found in English texts, and the highestwas seen in Japanese texts.
Although the high-est rate can be seen in Japanese texts, the lowestwas found in MT texts in the INT class andADV class.On the basis of comprehension test results,we confirmed that all the supporting systemsincreased comprehension test scores for theBEG class, E&MT for the INT class, but not forthe ADV class.BEG INT ADVENG 0.68 0.89 0.93CHU 0.74 0.85 0.92RUB 0.74 0.83 0.92MT 0.77 0.82 0.84E&MT 0.87 0.93 0.91JPN 0.87 0.96 0.94Table 4.
The correct answer rate by TOEIC score groupOn the basis of this result, we conclude thatthe reading support systems help the lowestTOEIC score group participants, while the sup-porting effect would be minor for the higherscore group.We analysed the mean rate with one-wayANOVA by contrasting the ENG texts or the JPNtexts.
The result is shown in Table 5.
The asteriskrefers to a non-significant difference, while thecheck mark shows a significant difference.In the BEG class, the rate of correct answersin the ENG texts was significantly lower than inthe E&MT texts.
There was no text that signifi-cantly differed from the JPN texts.In the INT class, there was no significant dif-ference compared with the ENG texts, while therate of the JPN texts significantly differed fromthe CHU, RUB, and MT texts.In the ADV class, there was no significantdifference comparing with the ENG texts.
Therate of the JPN texts showed a significant differ-ence from the MT texts.BEG INT ADVENG JPN ENG JPN ENG JPNCHU * * * ?
* *RUB * * * ?
* *MT * * * ?
* ?E&MT ?
* * * * *Table 5.
ANOVA results for the correct rate by TOEICscore group4.3.4 Testing Hypothesis 2We found variances in the Hypothesis 1.Thus, the most readable text was the JPN texts,whereas the least readable text was not the ENGtexts but the RUB texts(Table 3).
In addition,the other supported texts, the CHU, RUB, andE&MT texts were less readable than the non-supported ENG texts.
However, the MT textswere more readable than the ENG texts.
There-248fore, we were able to conclude that Hypothesis 1was supported among the ENG, MT, and JPNtexts.Given this, we focused on these texts andfound that Hypotheses 2 was correct.
As Table6 shows, the reading speed of the MT texts wasfaster than the ENG texts in all the groups.
Theincrease of the speed was inversely related to thereaders?
ability.
Thus, the increase was 47.3 inthe BEG class; 25.4 in the INT class; and 10.9 inthe ADV class.BEG INT ADVENG 62.4 73.2 89.2MT 109.7 98.6 100.1JPN 172.2 152.1 170.9Table 6.
The reading speed (WPM) by TOEIC score rangeWe analysed the mean reading speed (Table7) with one-way ANOVA by contrasting theENG texts or the JPN texts.
The speed of the MTtexts was significantly faster than that of the ENGtexts in the BEG and INT classes.
However, inthe ADV class, there was no text that signifi-cantly deferred from the ENG texts.
The readingspeed of the JPN texts was significantly fasterthan the other texts in all the classes.
See Table 8.Text BEG INT ADVENG 62.4 73.2 89.2CHU 63.2 63.4 98.1RUB 58.4 60.0 80.3MT 109.6 98.6 100.1E&MT 71.4 60.8 80.7JPN 172.2 152.2 1701.0Table 7.
The reading speed (WPM) by TOEIC score rangeBEG INT ADVENG JPN ENG JPN ENG JPNCHU * ?
* ?
* ?RUB * ?
* ?
* ?MT ?
?
?
?
* ?E&MT * ?
* ?
* ?Table 8.
ANOVA results for the reading speed by TOEICscore group5.
ConclusionIn this paper, we presented the reading speed-based evaluation method for reading supportsystems.
On the basis of the experiment, wefound that the method articulated the perform-ance of the systems, such as a chunker, a word-translation system, and a sentence-translationsystem.
We found that only a sentence-translation showed the supporting effect.
How-ever, this supporting effect was not available forthe advanced English learners.We have not yet discussed crossing effects ofcomprehension result and speed result, but wewill expect the further study would reveal it.ReferencesAlderson, J. C. 2000.
Assessing Reading.
CambridgeUniversity Press: Cambridge.Flesch, R. 1948.
A New Readability Yardstick.
Jour-nal of Applied Psychology 32: 221-233.Fuji, M., N. Hatanaka, E. Ito, S. Kamei, H. Kumai, T.Sukehiro, T. Yoshimi, & H. Isahara.
2001.Evaluation Method for Determining Groups ofUsers Who Find MT ?Useful.?
Proceedings of theMT Summit VIII.Ohguro, Y.
1993.
Evaluating the Validity of PrintingJapanese Words alongside English Text.
TechnicalReport on Information Processing Society of Ja-pan.
93-NL-79: 127-134.Papineni, K., S. Roukos, T. Ward, & W.-J.
Zhu.
2002.BLEU: A Method for Automatic Evaluation ofMachine Translation.
Proceedings of the 40th An-nual Meeting of the Association for the Computa-tional Linguistics: 311-318.Yoshimi, T., K. Kotani, T. Kutsumi, I. Sata, & H.Isahara.
2005.
A Method of Measuring ReadingTime for Assessing EFL-Learners?
Reading Abil-ity.
JSiSE 22: 24-29.249
