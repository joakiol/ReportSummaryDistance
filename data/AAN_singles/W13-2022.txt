Proceedings of the BioNLP Shared Task 2013 Workshop, pages 144?152,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsBuilding A Contrasting Taxa Extractor for Relation Identification fromAssertions: BIOlogical Taxonomy & Ontology Phrase Extraction SystemCyril GrouinLIMSI?CNRS, Orsay, Francecyril.grouin@limsi.frAbstractIn this paper, we present the methodswe used to extract bacteria and biotopesnames and then to identify the relationbetween those entities while participatingto the BioNLP?13 Bacteria and BiotopesShared Task.
We used machine-learningbased approaches for this task, namelya CRF to extract bacteria and biotopesnames and a simple matching algorithm topredict the relations.
We achieved poor re-sults: an SER of 0.66 in sub-task 1, and a0.06 F-measure in both sub-tasks 2 and 3.1 IntroductionThe BioNLP?13 Bacteria and Biotopes shared taskaims at extracting bacteria names (bacterial taxa)and biotopes names (bacteria habitats; geographi-cal and organization entities).
The task comprisesthree sub-tasks (Bossy et al 2012b).?
Sub-task 1 aims at extracting habitat namesand linking those names to the relevant con-cept from the OntoBiotope ontology.?
Sub-task 2 aims at identifying relations be-tween bacteria and habitats among two kindsof relations (localization, part-of) based ona ground truth corpus of bacteria and habitatnames.
The ?localization?
relation is the linkbetween a bacterium and the place where itlives while the ?part-of?
relation is the rela-tion between hosts and host parts (bacteria)(Bossy et al 2012a).?
Sub-task 3 aims at extracting all bacteria andbiotopes names (including both habitat andgeographical names), and then identifying re-lations between these concepts.In this paper, we present the methods we de-signed as first time participant to the BioNLP Bac-teria Biotopes Shared Task.2 BackgroundScientific documents provide useful informationin many domains.
Because processing those docu-ments is time-consuming for a human, NLP tech-niques have been designed to process a hugeamount of documents quickly.
The microorgan-isms ecology domain involves a lot of microorgan-isms (bacteria, living and dead cells, etc.)
andhabitats (food, medical, soil, water, hosts, etc.
)that have been described in details in the literature.NLP techniques would facilitate the access to in-formation from scientific texts and make it avail-able for further studies.Bacteria and biotopes identification has beenaddressed for the first time during the BioNLP2011 Bacteria Biotopes shared task (Bossy etal., 2012a; Kim et al 2011).
This task con-sisted in extracting bacteria location events fromtexts among eight categories (Host, HostPart, Ge-ographical, Environment, Food, Medical, Waterand Soil).Three teams participated in this task.
All sys-tems followed the same process: in a first stage,they detected bacteria names, detected and typedlocations; then, they used co-reference to link theextracted entities; the last stage focused on theevent extraction.Bjo?rne et al(2012) adapted an SVM-basedNamed Entity Recognition system and used thelist of Prokaryotic Names with Standing inNomenclature.
Nguyen and Tsuruoka (2011) useda CRF-based system and used the NCBI web pageabout the genomic BLAST.
Ratkovic et al(2012)designed an ad hoc rule-based system based on theNCBI Taxonomy.
The participants obtained poorresults (Table 1) which underlines the complexityof this task.144Team R P FRatkovic et al(2012) 0.45 0.45 0.45Nguyen and Tsuruoka (2011) 0.27 0.42 0.33Bjo?rne et al(2012) 0.17 0.52 0.26Table 1: Recall, Precision and F-measure atBioNLP 2011 Bacteria and Biotopes Shared Task3 Corpus3.1 PresentationThe corpus comprises web pages about bacte-rial species written for non-experts.
Each textconsists of a description of individual bacteriumand groups of bacteria, in terms of first observa-tion, characteristics, evolution and biotopes.
Twocorpora have been released including both rawtextual documents and external reference annota-tions.
The training corpus contains 52 textual doc-uments while the development corpus contains 26documents.
No tokenization has been performedover the documents.
In Table 2, we provide somestatistics on the annotations performed over bothcorpora for each type of entity to be annotated(bacteria, habitat, and geographical).Corpus Training Development# Documents 52 26# Words 16,294 9,534Avg # words/doc 313.3 366.7# Bacteria 832 515# Habitat 934 611# Geographical 91 77Table 2: Annotation statistics on both corpora3.2 Corpus analysisThe bacteria names appear in the texts, either intheir longer form (Xanthomonas axonopodis pv.citri), in a partial form (Xanthomonas) or in theirabbreviated form (Xac).
The abbreviations arecase-sensitives since they follow the original form:MmmSC is derived from M. mycoides ssp my-coides SC.1 A few bacteria names can appear inthe text followed by a trigger word: Spirillum bac-teria, but it will be abbreviated in the remainder ofthe text, sometimes with a higher degree of speci-ficity: S. volutans standing for Spirillum volutans.1Mycoplasma mycoides subspecies mycoides SmallColony in its longer form.4 MethodsThis year, the BioNLP organizers encouraged theparticipants to use supporting resources in orderto reduce the time-investment in the challenge.Those resources encompass sentence splitting, to-kenization, syntactic parsing, and biological anno-tations.
Moreover, a specific ontology has beenreleased for the Bacteria Biotopes task.We used some of the resources provided andcombined them with additional resources, in amachine-learning framework we specifically de-signed for this task.4.1 Linguistic resources4.1.1 The OntoBiotope OntologyOntoBiotope2 is an ontology tailored for thebiotopes domain.
The BioNLP-ST 2013 versionhas been released in the OBO format.
This ontol-ogy integrates 1,756 concepts.
Each concept hasbeen given a unique ID and is associated with ex-act terms and related synonyms.
The concept isalso defined in a ?is a?
relation.
The normaliza-tion of the habitat names in the first sub-task mustbe based on this ontology.For example, the concept microorganism(unique id MBTO:00001516) is a living organ-ism which unique id is MBTO:00000297.
For thisconcept, microbe is an exact synonym while mi-crobial is a related synonym (see Figure 1).
[Term]id: MBTO:00001516name: microorganismexact synonym: ?microbe?
[TyDI:23602]related synonym: ?microbial?
[TyDI:23603]is a: MBTO:00000297 !
living organismFigure 1: The concept microorganism in the On-toBiotope ontology4.1.2 The NCBI taxonomyIn order to help our system to identify the bacte-ria names, we built a list of 357,387 bacteria taxabased on the NCBI taxonomy database3 (Feder-hen, 2012).
This taxonomy describes a small part(about 10%) of the living species on earth, basedon public sequence databases.2http://bibliome.jouy.inra.fr/MEM-OntoBiotope/OntoBiotope_BioNLP-ST13.obo3http://www.ncbi.nlm.nih.gov/taxonomy/145It includes twelve categories of informationfrom the biological domain (bacteria, inverte-brates, mammals, phages, plants, primates, ro-dents, synthetics, unassigned, viruses, vertebratesand environmental samples).We extracted from this taxonomy all names be-longing to the Bacteria category, which represent24.3% of the content.
This output includes a fewvariants of bacteria names (see Table 3).tax id name txt name class346 Xanthomonas citri (exHasse 1915) Gabriel et al1989authority346 Xanthomonas citriscientificname346 Xanthomonas axonopodispv.
citrisynonym346 Xanthomonas campestris(pv.
citri)synonym346 Xanthomonas campestrispv.
Citri (A group)synonymTable 3: Bacteria names from the NCBI taxonomy4.1.3 The Cocoa annotationsCocoa is a WebAPI annotator tool for biologicaltext.4 We used the Cocoa annotations provided bythe organizers as part of the supporting resources.These annotations emphasize 37 pre-defined cate-gories.
We noticed a few categories are often tiedwith one of the three kinds of entities we have toprocess:?
Bacteria: Cell, Chemical, Mutant Organism,Organism, Protein, Unknown;?
Habitat: Body part, Cell, Cellu-lar component, Chemical, Disease, Food,Geometrical part, Habitat, Location,Multi-tissue structure, Organism, Organ-ism subdivision, Pathological formation,Tissue;?
Geographical: Company, Habitat, Technique,Unknown.We believe these categories should be useful toidentify bacteria and biotopes entities in the texts,and we used them as features in the CRF model(see column #10 in Table 4).4Compact cover annotator for biological noun phrases,http://npjoint.com/annotate.php4.2 System4.2.1 FormalismsDepending on the sub-task to process, we used twodistinct formalisms implemented in the Wapiti tool(Lavergne et al 2010) to build our models:?
Conditional Random Fields (CRF) (Laffertyet al 2001; Sutton and McCallum, 2006)to identify bacteria and biotopes names (sub-tasks 1 and 3).?
Maximum Entropy (MaxEnt) (Guiasu andShenitzer, 1985; Berger et al 1996) to pro-cess the relationships between entities (sub-tasks 2 and 3).4.2.2 Bacteria biotopes features setWe used several sets of features, including ?classi-cal?
internal features (columns #4 to #7 in Table 4:typographic, digit, punctuation, length) and a fewsemantic features.
In table 4, we present a sam-ple tabular file produced in order to train the CRFmodel.?
Presence of the token in the NCBI taxonomy(column #9);?
Presence of the token in the OntoBiotope on-tology (column #8);?
Category of the token based on the Cocoa an-notations (column #10);?
Unsupervised clusters (column #11) createdusing Brown?s algorithm (Brown et al 1992)with Liang?s code5 (Liang, 2005).Taxonomy feature.
We noticed that 1,169 to-kens out of 1,229 (95.1%) tokens we identified inthe NCBI taxonomy in both corpora correspond toa Bacteria name in the reference (Table 5).
Thischaracteristic should be useful to identify the bac-teria names.OntoBiotope feature.
Regarding the presenceof the token in the OntoBiotope ontology, we no-ticed that 1,487 tokens out of 1,906 (78.0%) fromboth corpora correspond to a habitat name in thereference (Table 6).
The identification of habitatnames will benefit from this characteristic.5http://www.cs.berkeley.edu/?pliang/software/1461 2 3 4 5 6 7 8 9 10 11 1233 8 Borrelia Mm O O 7 O NCBI Organism 11101010 B-Bacteria42 7 afzelii mm O O 7 O NCBI Organism O I-Bacteria49 1 .
O Punct O 1 O O O 0010 O51 4 This Mm O O 4 O O O 1001000 O56 7 species mm O O 7 O O Organism1 100101100 O64 3 was mm O O 3 O O O 0101000 O68 8 isolated mm O O 7 O O O 1100100 O77 4 from mm O O 4 O O O 011110110 O82 1 a mm O O 1 O O O 1011000 O84 4 skin mm O O 4 MBTO O Pathological 110111011 B-Habitatformation89 6 lesion mm O O 6 MBTO O Pathological 111101100 I-Habitatformation96 4 from mm O O 4 O O O 011110110 I-Habitat101 1 a mm O O 1 O O O 1011000 I-Habitat103 4 Lyme Mm O O 4 O O Disease 100010 I-Habitat108 7 disease mm O O 7 O O Disease 110111101 I-Habitat116 7 patient mm O O 7 MBTO O Organism2 1100110 I-Habitat124 2 in mm O O 2 O O O 0111100 O127 6 Europe Mm O O 6 MBTO O Habitat 111101101 B-Geographical134 2 in mm O O 2 O O O 0111100 O137 4 1993 O O Digit 4 O O O 111101101 O141 1 .
O Punct O 1 O O O 0010 OTable 4: Tabular used for training the CRF model.
Column 1: character offset; 2: length in characters;3: token; 4: typographic features; 5: presence of punctuation; 6: presence of digit; 7: length in characters(with a generic ?7?
category for length higher than seven characters); 8: presence of the token in theOntoBiotope ontology; 9: presence of the token in the NCBI taxonomy; 10: category of the token fromthe Cocoa annotations; 11: cluster identifier; 12: expected answerReference annotationToken in the NCBIPresent AbsentBacteria 1,169 1,543Geographical 0 276Habitat 2 2,466O (out of annotation) 58 25,060Table 5: Correspondence between the referenceannotation and the token based on the presence ofthe token in the NCBI taxonomy4.2.3 Normalization with OntoBiotopeHabitat names normalization consisted in linkingthe habitat names to the relevant concept in theOntoBiotope ontology using an exact match of thephrase to be normalized.
This exact match is basedon both singular and plural forms of the phraseto normalize, using a home-made function that in-cludes regular and irregular plural forms.
Never-theless, we did not manage discontinuous entities.Reference annotationToken in OntoBiotopePresent AbsentBacteria 1 2,711Geographical 156 120Habitat 1,487 981O (out of annotation) 262 24,856Table 6: Correspondence between the referenceannotation and the token based on the presence ofthe token in the OntoBiotope ontology4.2.4 Relationships approachesRelationships features set.
Our MaxEnt modelonly relies on the kind of entities that can be linkedtogether:?
Bacterium and Localization (Habitat) for a?localization?
relation,?
Host and Part for a ?PartOf?
relation (be-tween two entities being of the same type).147For example, Bifidobacterium is a bacterianame, human and human gastrointestinal tract aretwo habitats (localizations).
A ?localization?
re-lation can occur between Bifidobacterium and hu-man while a ?PartOf?
relation occurs between hu-man and human gastrointestinal tract.Basic approach.
For the official submission, wedid not use this model because of the followingremaining problems: (i) a few relations we pro-duced were not limited to the habitat category butalso involved the geographical category, (ii) wedid not manage the relations we produced in du-plicate, and (iii) the weight our CRF system gaveto each relation was not relevant enough to be used(for a relation involving A with B, C, and D, thesame weight was given in each relation).All of those problems led us to process the re-lations between entities using a too much simpleapproach: we only considered if the relation be-tween two entities from the test exists in the train-ing corpus.
This approach is not robust as it doesnot consider unknown relations.5 Results and Discussion5.1 Identification of bacteria and biotopesIn this subsection, we present the results weachieved on the development corpus (Table 2) toidentify bacteria and biotopes names without link-ing those names to the concept in the OntoBiotopeontology.
We built the model on the training cor-pus and applied it on the development corpus.
Theevaluation has been done using the conlleval.plscript6 (Tjong Kim Sang and Buchholz, 2000)that has been created to evaluate the results in theCoNLL-2000 Shared Task.
We chose this scriptbecause it takes as input a tabular file which iscommonly used in the machine-learning process.Nevertheless, the script does not take into accountthe offsets to evaluate the annotations, which isthe official way to evaluate the results.
We givein Table 7 the results we achieved.
Those re-sults show our system succeed to correctly iden-tify the bacteria and biotopes names.
Neverthe-less, the biotopes names are more difficult to pro-cess than the bacteria names.
Similarly, Kolluruet al(2011) achieved better results on the bacteriacategory rather than on the habitat, confirming thislast category is more difficult to process.6http://www.clips.ua.ac.be/conll2000/chunking/Category R P FBacteria 0.8794 0.9397 0.9085Geographical 0.6533 0.7903 0.7153Habitat 0.6951 0.8102 0.7482Overall 0.7771 0.8715 0.8216Table 7: Results on the bacteria biotopes identifi-cation (development corpus)There is still room for improvement, especiallyin order to improve the recall in each category.
Weplan to define some post-treatments so as to iden-tify new entities and thus, increase the recall inthose three categories.5.2 Official resultsSERSub-task 1 0.66 4th/4R P FSub-task 2 0.04 0.19 0.06 4th/4Sub-task 3 0.04 0.12 0.06 2nd/2Table 8: Official results and rank for LIMSI5.2.1 Habitat entities normalizationGeneral results.
The first sub-task is evaluatedusing the Slot Error Rate (Makhoul et al 1999),based on the exact boundaries of the entity to bedetected and the semantic similarity of the conceptfrom the ontology between reference and hypothe-sis (Bossy et al 2012b).
This semantic similarityis based on the ?is a?
relation between two con-cepts.We achieved a 0.66 SER which places us 4thout of four participants.
Other participants ob-tained SERs ranging from 0.46 to 0.49.
Our sys-tem achieved high precision (0.62) but low recall(0.35).
It produced two false positives and 144false negatives.
Out of 283 predicted habitats,175.34 are correct.
There was also a high numberof substitutions (187.66).Correct entity, incorrect categorization.
Onthe entity boundaries evaluation, our system SER(0.45) was similar to that of the other participants(from 0.46 to 0.42).
We achieved a 1.00 preci-sion, a 0.56 recall and a 0.71 F-measure (the bestfrom all participants).
Those results are consistentwith those we achieved on the development cor-pus (Table 7) and confirm the benefit of using aCRF-based system for entity detection.148While we correctly identified the habitat enti-ties, the ontology categorization proved difficult:we achieved an SER of 0.62 while other partic-ipants obtained SERs ranging from 0.38 to 0.35.For this task, we relied on exact match for map-ping the concept to be categorized and the con-cepts from the ontology, including both singularand plural forms match.
When no match wasfound, because the categorization was mandatory,we provided a default identifier?the first identi-fier from the ontology?which is rarely correct.75.2.2 Relationships between entitiesGeneral results.
The relation sub-task is evalu-ated in terms of recall and precision for the pre-dicted relations.
On both second and third sub-tasks, due to our too basic approach, we onlyachieved a 0.06 F-measure.
Obviously, becauseconsidering only existing relations is not a robustapproach, the recall is very low (R=0.04).
Theprecision is not as high as we expected (P=0.19),which indicates that if a relation exists in the train-ing corpus for two entities, this relation does notnecessarily occur within the test for the two sameentities (two entities can occur in the same textwithout any relation to be find between them).
Onthe second sub-task, other participants obtainedF-measures ranging from 0.42 to 0.27, while onthe third sub-task, the other participants obtaineda 0.14 F-measure, which underlines the difficultyof the relation task.Out of the two types of relation to be found,this simple approach yielded better results for theLocalization relation (F=0.07) than for the PartOfrelation (F=0.02).
While our results are probablytoo bad to yield a definite conclusion, the resultsof other participant also reflect a difference in per-formance for relation Localization and PartOf.Improvements.
After fixing the technical prob-lems we encountered, we plan to test other algo-rithms such as SVM, which may be more adaptedfor this kind of task.6 Additional experimentsAfter the official submission, we carried out addi-tional experiments.7We gave the MBTO:00000001 identifier which is the idfor the concept ?gaz seep?.6.1 Habitat entities normalization6.1.1 Beyond exact matchThe improvements we made on the habitat enti-ties normalization are only based on the mappingbetween the predicted concept and the ontology.In our official submission, we only used an exactmatch.
We tried to produce a more flexible map-ping in several ways.First, we tried to normalize the mention gather-ing all words from the mention into a single word.Indeed, the concept ?rain forest?
is not found inthe ontology while the concept ?rainforest?
in oneword exists.Second, we split the mention into single wordsand tried matching based on the features listed be-low, in order to manage the subsumption of con-cepts.?
all words except the first one: ?savannah?instead of ?brazilian savannah?,?
all words except the last one: ?glossina?
in-stead of ?glossina brevipalpis?,?
the last three words (we did not find examplein the corpus),?
the first three words: ?sugar cane fields?
in-stead of ?sugar cane fields freshly plantedwith healthy plants?,?
the last two words: ?tsetse fly?
instead of?blood-sucking tsetse fly?,?
and the first two words: ?tuberculoid gran-ulomas?
instead of ?tuberculoid granulomaswith caseous lesions?.If two parts of a mention can be mapped to twoconcepts in the ontology, we added both conceptsin the output.We also extended the coverage of the ontologyusing the reference normalization from both train-ing and development corpora, adding 316 entriesin the ontology.
Those new concepts can be con-sidered either as synonyms or as hyponyms:?
synonyms: ?root zone?
is a synonym of ?rhi-zosphere?.
While only the second one occursin the ontology, we added the first conceptwith the identifier from the second concept;?
hyponyms: ?bacteriologist?
and ?entomol-ogist?
are both hyponyms of ?researcher?.We gave the hypernym identifier to the hy-ponym concepts.149At last, if no concept was found in the ontology,instead of using the identifier of the first conceptin the ontology, we gave as a default identifier theone of the more frequent concept in the corpora.8This strategy improves system performance.6.1.2 ResultsThe improvements we made allowed us toachieved better results on the test corpus (table 9).While on the official submission we achieved a0.66 Slot Error Rate, we obtained a 0.53 SERthanks to the improvements we made.
This newresult does not lead us to obtain a better rank, but itis closer to the ones the other participants achieved(from 0.49 to 0.46).CategoryOfficial AdditionalEvaluation ExperimentsSubstitution 187.66 121.99Insertion 2 2Deletion 144 144Matches 175.34 241.01Predicted 283 283SER 0.66 0.53Recall 0.35 0.48Precision 0.62 0.85F-measure 0.44 0.61Table 9: Results on sub-task 1 on both the officialsubmission and the additional experimentsThese improvements led us to obtain better re-call, precision and F-measure.
While our re-call is still the lowest of all participants (0.48 vs.[0.60;0.72]), our precision is the highest (0.85 vs.[0.48;0.61]) and our F-measure is equal to thehighest one (0.61 vs. [0.57;0.61]).6.2 Relationships between entities6.2.1 ProcessingOn the relationships, as a first step, we fixedthe problems that prevented us to use the Max-Ent model during the submission stage: (i) weproduced correct files for the algorithm, remov-ing the geographical entities from our processingaccordingly with the guidelines, (ii) when deal-ing with all possible combinations of entities thatcan be linked together, we managed the relationsso as not to produce those relations in duplicate,8The concept ?human?
with identifier MBTO:00001402is the more frequent concept in all corpora while the concept?gaz seep?
with identifier MBTO:00000001 was never used.and (iii) we better managed the confidence scoregiven by the CRF on each relation.6.2.2 ResultsWe produced new models on the training corpusbased on the following features: entities to belinked, category of each entity, and whether a re-lation between those entities exists in the trainingcorpus.
We performed two evaluations of thosemodels: (i) on the development corpus, using theofficial evaluation script, and (ii) on the test cor-pus via the evaluation server.9 As presented inTable 10, we achieved worse results (F=0.02 andF=0.03) than our official submission (F=0.06) onthe test corpus.#Sub-task 2 Sub-task 3Dev Test Test1R 0.18 0.11 0.06P 0.49 0.01 0.01F 0.26 0.02 0.012R 0.58 0.02 0.02P 0.77 0.16 0.33F 0.66 0.03 0.04Table 10: Results on sub-tasks 2 and 3 based onthe additional experiments (#1 and #2)We also noticed that we achieved very poor re-sults on the test corpus while the evaluation on thedevelopment corpus provided promising results,with a F-measure decreasing from 0.26 to 0.02 onthe first experiment, and from 0.66 to 0.04 on thesecond one.
The difference between the resultsfrom both development and test corpora is hard tounderstand.
We have to perform additional anal-yses on the outputs we produced to identify theproblem that occurred.Moreover, we plan to use more contextual fea-tures (specific words that indicate the relation, dis-tance between two entities, presence of relativepronouns, etc.)
to improve the model.
Indeed, inrelations between concepts, not only the conceptsmust be studied but also the context in which theyoccur as well as the linguistic features used in theneighborhood of those concepts.9The reference annotations from the test corpus will notbe released to the participants.
Instead of those relations, anevaluation server has been opened after the official evaluationtook place.1507 ConclusionIn this paper, we presented the methods we usedas first time participant to the BioNLP BacteriaBiotopes Shared Task.To detect bacteria and biotopes names, we useda machine-learning approach based on CRFs.
Weused several resources to build the model, amongthem the NCBI taxonomy, the OntoBiotope on-tology, the Cocoa annotations, and unsupervisedclusters created through Brown?s algorithm.
Thenormalization of the habitat names with the con-cepts in the OntoBiotope ontology was performedwith a Perl script based on exact match of the en-tity to be found, taking into account its plural form.On this sub-task, we achieved a 0.66 Slot ErrorRate.In order to process the relationships between en-tities, our MaxEnt model was not ready for the of-ficial submission.
The simple approach we usedrelies on the identification of the relation betweenentities only if the relation exists in the trainingcorpus.
This simple approach is not robust enoughto correctly process new data.
On the relation sub-tasks, due to the approach we used, we achieved a0.06 F-measure.On the first sub-task, we enhanced our habitatentities normalization process, which led us to im-prove our Slot Error Rate from 0.66 (official sub-mission) to 0.53 (additional experiments).On the relation detection, first, we plan to makenew tests with more features, including contextualfeatures.
Second, we plan to test new algorithms,such as SVM which seems to be relevant to pro-cess relationships between entities.AcknowledgmentsThis work has been done as part of the Quaero pro-gram, funded by Oseo, French State Agency forInnovation.
I would like to thank the organizersfor their work and Aure?lie Ne?ve?ol for the proof-read of this paper.ReferencesAdam L Berger, Stephen Della Pietra, and Vincent JDella Pietra.
1996.
A maximum entropy approachto natural language processing.
Computational Lin-guistics, 22(1):39?71.Jari Bjo?rne, Filip Ginter, and Tapio Salakoski.
2012.University of Turku in the BioNLP?11 shared task.BMC Bioinformatics, 13(Suppl 11):S4.Robert Bossy, Julien Jourde, Alain-Pierre Manine,Philippe Veber, Erick Alphonse, Marteen van deGuchte, Philippe Bessie`res, and Claire Ne?dellec.2012a.
BioNLP shared task ?
the bacteria track.BMC Bioinformatics, 13(Suppl 11):S3.Robert Bossy, Claire Ne?dellec, and Julien Jourde,2012b.
Bacteria Biotope (BB) task at BioNLPShared Task 2013.
Task proposal.
INRA, Jouy-en-Josas, France.Peter F Brown, Vincent J Della Pietra, Peter V deSouza, Jenifer C Lai, and Robert L Mercer.
1992.Class-based n-gram models of natural language.Computational Linguistics, 18(4):467?79.Scott Federhen.
2012.
The NCBI taxonomy database.Nucleic Acids Res, 40(Database issue):D136?43.Silviu Guiasu and Abe Shenitzer.
1985.
The princi-ple of maximum entropy.
The Mathematical Intelli-gence, 7(1).Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, RobertBossy, Ngan Nguyen, and Juni?chi Tsujii.
2011.Overview of BioNLP shared task 2011.
In BioNLPShared Task 2011 Workshop Proc, pages 1?6, Port-land, OR.
ACL.BalaKrishna Kolluru, Sirintra Nakjang, Robert P Hirt,Anil Wipat, and Sophia Ananiadou.
2011.
Auto-matic extraction of microorganisms and their habi-tats from free text using text mining workflows.
JIntegr Bioinform, 8(2):184.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional Random Fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proc of ICML.Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.2010.
Practical very large scale CRFs.
Proc of ACL,pages 504?13, July.Percy Liang.
2005.
Semi-supervised learning for nat-ural language.
Master?s thesis, MIT.John Makhoul, Francis Kubala, Richard Schwartz, andRalph Weischedel.
1999.
Performance measures forinformation extraction.
In Proc.
of DARPA Broad-cast News Workshop, pages 249?52.Nhung T. H. Nguyen and Yoshimasa Tsuruoka.
2011.Extracting bacteria biotopes with semi-supervisednamed entity recognition and coreference resolution.In BioNLP Shared Task 2011 Workshop Proc, pages94?101, Portland, OR.
ACL.Zorana Ratkovic, Wiktoria Golik, and Pierre Warnier.2012.
Event extraction of bacteria biotopes: aknowledge-intensive NLP-based approach.
BMCBioinformatics, 13(Suppl 11):S8.Charles Sutton and Andrew McCallum.
2006.
An in-troduction to conditional random fields for relationallearning.
In Lise Getoor and Ben Taskar, editors,Introduction to Statistical Relational Learning.
MITPress.151Erik F. Tjong Kim Sang and Sabine Buchholz.2000.
Introduction to the CoNLL-2000 shared-task:Chunking.
In Proc of CoNLL-2000 and LLL-2000,pages 127?32, Lisbon, Portugal.152
