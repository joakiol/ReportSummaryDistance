Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 233?237,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsCoupling Label Propagation and Constraints for Temporal Fact ExtractionYafang Wang, Maximilian Dylla, Marc Spaniol and Gerhard WeikumMax Planck Institute for Informatics, Saarbru?cken, Germany{ywang|mdylla|mspaniol|weikum}@mpi-inf.mpg.deAbstractThe Web and digitized text sources containa wealth of information about named entitiessuch as politicians, actors, companies, or cul-tural landmarks.
Extracting this informationhas enabled the automated construction of largeknowledge bases, containing hundred millionsof binary relationships or attribute values aboutthese named entities.
However, in reality mostknowledge is transient, i.e.
changes over time,requiring a temporal dimension in fact extrac-tion.
In this paper we develop a methodologythat combines label propagation with constraintreasoning for temporal fact extraction.
Labelpropagation aggressively gathers fact candi-dates, and an Integer Linear Program is usedto clean out false hypotheses that violate tem-poral constraints.
Our method is able to im-prove on recall while keeping up with preci-sion, which we demonstrate by experimentswith biography-style Wikipedia pages and alarge corpus of news articles.1 IntroductionIn recent years, automated fact extraction from Webcontents has seen significant progress with the emer-gence of freely available knowledge bases, such asDBpedia (Auer et al, 2007), YAGO (Suchanek etal., 2007), TextRunner (Etzioni et al, 2008), orReadTheWeb (Carlson et al, 2010a).
These knowl-edge bases are constantly growing and contain cur-rently (by example of DBpedia) several million enti-ties and half a billion facts about them.
This wealthof data allows to satisfy the information needs ofadvanced Internet users by raising queries from key-words to entities.
This enables queries like ?Who ismarried to Prince Charles??
or ?Who are the team-mates of Lionel Messi at FC Barcelona?
?.However, factual knowledge is highly ephemeral:Royals get married and divorced, politicians holdpositions only for a limited time and soccer playerstransfer from one club to another.
Consequently,knowledge bases should be able to support moresophisticated temporal queries at entity-level, suchas ?Who have been the spouses of Prince Charlesbefore 2000??
or ?Who are the teammates of LionelMessi at FC Barcelona in the season 2011/2012?
?.In order to achieve this goal, the next big step is todistill temporal knowledge from the Web.Extracting temporal facts is a complex and time-consuming endeavor.
There are ?conservative?
strate-gies that aim at high precision, but they tend to sufferfrom low recall.
On the contrary, there are ?aggres-sive?
approaches that target at high recall, but fre-quently suffer from low precision.
To this end, weintroduce a method that allows us to gain maximumbenefit from both ?worlds?
by ?aggressively?
gath-ering fact candidates and subsequently ?cleaning-up?the incorrect ones.
The salient properties of our ap-proach and the novel contributions of this paper arethe following:?
A temporal fact extraction strategy that is ableto efficiently gather thousands of fact candidatesbased on a handful of seed facts.?
An ILP solver incorporating constraints on tem-poral relations among events (e.g., marriage ofa person must be non-overlapping in time).?
Experiments on real world news and Wikipediaarticles showing that we gain recall while keep-ing up with precision.2 Related WorkRecently, there have been several approaches thataim at the extraction of temporal facts for the auto-mated construction of large knowledge bases, but233time-aware fact extraction is still in its infancy.
Anapproach toward fact extraction based on coupledsemi-supervised learning for information extraction(IE) is NELL (Carlson et al, 2010b).
However, itdoes neither incorporate constraints nor temporal-ity.
TIE (Ling and Weld, 2010) binds time-pointsof events described in sentences, but does not dis-ambiguate entities or combine observations to facts.A pattern-based approach for temporal fact extrac-tion is PRAVDA (Wang et al, 2011), which utilizeslabel propagation as a semi-supervised learning strat-egy, but does not incorporate constraints.
Similarly,TOB is an approach of extracting temporal business-related facts from free text, which requires deep pars-ing and does not apply constraints as well (Zhang etal., 2008).
In contrast, CoTS (Talukdar et al, 2012)introduces a constraint-based approach of coupledsemi-supervised learning for IE, however not focus-ing on the extraction part.
Building on TimeML(Pustejovsky et al, 2003) several works (Verhagen etal., 2005; Mani et al, 2006; Chambers and Jurafsky,2008; Verhagen et al, 2009; Yoshikawa et al, 2009)identify temporal relationships in free text, but don?tfocus on fact extraction.3 FrameworkFacts and Observations.
We aim to extract factualknowledge transient over time from free text.
Morespecifically, we assume time T = [0, Tmax ] tobe a finite sequence of time-points with yearlygranularity.
Furthermore, a fact consists of arelation with two typed arguments and a time-interval defining its validity.
For instance, we writeworksForClub(Beckham,RMadrid)@[2003, 2008)to express that Beckham played for Real Madridfrom 2003 to 2007.
Since sentences containing afact and its full time-interval are sparse, we considerthree kinds of textual observations for each relation,namely begin, during, and end.
?Beckham signedfor Real Madrid from Manchester United in 2003.?includes both the begin observation of Beckham be-ing with Real Madrid as well as the end observationof working for Manchester.
A positive seed fact is avalid fact of a relation, while a negative seed fact isincorrect (e.g., for relation worksForClub, a positiveseed fact is worksForClub(Beckham,RMadrid),while worksForClub(Beckham,BMunich) is anegative seed fact).Framework.
As depicted in Figure 1, our frameworkis composed of four stages, where the first collectscandidate sentences, the second mines patterns fromthe candidates sentences, the third extracts temporalfacts from the sentences utilizing the patterns and thelast removes noisy facts by enforcing constraints.Preprocessing.
We retrieve all sentences from thecorpus comprising at least two entities and a temporalexpression, where we use YAGO for entity recogni-tion and disambiguation (cf.
(Hoffart et al, 2011)).Figure 1: System OverviewPattern Analysis.
A pattern is a n-gram based fea-ture vector.
It is generated by replacing entitiesby their types, keeping only stemmed nouns, verbsconverted to present tense and the last preposition.For example, considering ?Beckham signed for RealMadrid from Manchester United in 2003.?
the cor-responding pattern for the end occurrence is ?signfor CLUB from?.
We quantify the strength of eachpattern by investigating how frequent the pattern oc-curs with seed facts of a particular relation and howinfrequent it appears with negative seed facts.Fact Candidate Gathering.
Entity pairs that co-occur with patterns whose strength is above a mini-mum threshold become fact candidates and are fedinto the next stage of label propagation.4 T-Fact ExtractionBuilding on (Wang et al, 2011) we utilize LabelPropagation (Talukdar and Crammer, 2009) to deter-mine the relation and observation type expressed byeach pattern.Graph.
We create a graph G = (VF ?
?VP , E) havingone vertex v ?
VF for each fact candidate observedin the text and one vertex v ?
VP for each pattern.Edges between VF and VP are introduced whenever afact candidate appeared with a pattern.
Their weightis derived from the co-occurrence frequency.
Edges234among VP nodes have weights derived from the n-gram overlap of the patterns.Labels.
Moreover, we use one label for each observa-tion type (begin, during, and end) of each relation anda dummy label representing the unknown relation.Objective Function.
Let Y ?
R|V|?|Labels|+ de-note the graph?s initial label assignment, and Y?
?R|V|?|Labels|+ stand for the estimated labels of all ver-tices, Sl encode the seed?s weights on its diagonal,and R?l contain zeroes except for the dummy label?scolumn.
Then, the objective function is:L(Y?)
=?`[(Y?` ?
Y?
?`)TS`(Y?` ?
Y??`)+?1Y?T?`LY?
?` + ?2?Y?
?` ?R?`?2](1)Here, the first term (Y?` ?
Y?
?`)TS`(Y?` ?
Y?
?`)ensures that the estimated labels approximate theinitial labels.
The labeling of neighboring verticesis smoothed by ?1Y?T?`LY?
?`, where L refers to theLaplacian matrix.
The last term is a L2 regularizer.5 Cleaning of Fact CandidatesTo prune noisy t-facts, we compute a consistent sub-set of t-facts with respect to temporal constraints (e.g.joining a sports club takes place before leaving asports club) by an Integer Linear Program (ILP).Variables.
We introduce a variable xr ?
{0, 1} foreach t-fact candidate r ?
R, where 1 means the can-didate is valid.
Two variables xf,b, xf,e ?
[0, Tmax ]denote begin (b) and end (e) of time-interval of a factf ?
F .
Note, that many t-fact candidates refer to thesame fact f , since they share their entity pairs.Objective Function.
The objective function intendsto maximize the number of valid raw t-facts, wherewr is a weight obtained from the previous stage:max?r?Rwr ?
xrIntra-Fact Constraints.
xf,b and xf,e encode aproper time-interval by adding the constraint:?f ?
F xf,b < xf,eConsidering only a single relation, we assume thesetsRb,Rd, andRe to comprise its t-fact candidateswith respect to the begin, during, and end observa-tions.
Then, we introduce the constraints?l ?
{b, e}, r ?
Rl tl ?
xr ?
xf,l (2)?l ?
{b, e}, r ?
Rl xf,l ?
tl ?
xr + (1?
xr)Tmax (3)?r ?
Rd xf,b ?
tb ?
xr + (1?
xr)Tmax (4)?r ?
Rd te ?
xr ?
xf,e (5)where f has the same entity pair as r and tb, te arebegin and end of r?s time-interval.
Whenever xr isset to 1 for begin or end t-fact candidates, Eq.
(2)and Eq.
(3) set the value of xf,b or xf,e to tb or te,respectively.
For each during t-fact candidate withxr = 1, Eq.
(4) and Eq.
(5) enforce xf,b ?
tb andte ?
xf,e.Inter-Fact Constraints.
Since we can refer to a factf ?s time interval by xf,b and xf,e and the connectivesof Boolean Logic can be encoded in ILPs (Karp,1972), we can use all temporal constraints expressibleby Allen?s Interval Algebra (Allen, 1983) to specifyinter-fact constraints.
For example, we leverage thisby prohibiting marriages of a single person fromoverlapping in time.Previous Work.
In comparison to (Talukdar et al,2012), our ILP encoding is time-scale invariant.
Thatis, for the same data, if the granularity of T ischanged from months to seconds, for example, thesize of the ILP is not affected.
Furthermore, becausewe allow all relations of Allen?s Interval Algebra, wesupport a richer class of temporal constraints.6 ExperimentsCorpus.
Experiments are conducted in the soccerand the celebrity domain by considering the works-ForClub and isMarriedTo relation, respectively.
Foreach person in the ?FIFA 100 list?
and ?Forbes 100list?
we retrieve their Wikipedia article.
In addition,we obtained about 80,000 documents for the soccerdomain and 370,000 documents for the celebrity do-main from BBC, The Telegraph, Times Online andESPN by querying Google?s News Archive Search1in the time window from 1990-2011.
All hyperpa-rameters are tuned on a separate data-set.Seeds.
For each relation we manually select the 10positive and negative fact candidates with highestoccurrence frequencies in the corpus as seeds.Evaluation.
We evaluate precision by randomly sam-pling 50 (isMarriedTo) and 100 (worksForClub) factsfor each observation type and manually evaluatingthem against the text documents.
All experimentaldata is available for download from our website2.6.1 Pipeline vs. Joint ModelSetting.
In this experiment we compare the perfor-mance of the pipeline being stages 3 and 4 in Figure1news.google.com/archivesearch2www.mpi-inf.mpg.de/yago-naga/pravda/2351 and a joint model in form of an ILP solving thet-fact extraction and noise cleaning at the same time.Hence, the joint model resembles (Roth and Yih,2004) extended by Section 5?s temporal constraints.Relation ObservationLabel Propagation ILP for T-Fact ExtractionPrecision # Obs.
Precision # Obs.worksForClub begin 80% 2537 81% 2426 WithoutNoiseCleaningduring 78% 2826 86% 1153end 65% 440 50% 550isMarriedTo begin 52% 195 28% 232during 76% 92 6% 466end 62% 50 2% 551worksForClub begin 85% 2469 87% 2076WithNoiseCleaningduring 85% 2761 79% 1434end 74% 403 72% 275isMarriedTo begin 64% 177 74% 67during 79% 89 88% 61end 70% 47 71% 28Table 1: Pipeline vs. Joint ModelResults.
Table 1 shows the results on the pipelinemodel (lower-left), joint model (lower-right), label-propagation w/o noise cleaning (upper-left), and ILPfor t-fact extraction w/o noise cleaning (upper-right).Analysis.
Regarding the upper part of Table 1 thepattern-based extraction works very well for works-ForClub, however it fails on isMarriedTo.
The reasonis, that the types of worksForClub distinguish thepatterns well from other relations.
In contrast, isMar-riedTo?s patterns interfere with other person-personrelations making constraints a decisive asset.
Whencomparing the joint model and the pipeline model,the former sacrifices recall in order to keep up withthe latter?s precision level.
That is because the jointmodel?s ILP decides with binary variables on whichpatterns to accept.
In contrast, label propagation ad-dresses the inherent uncertainty by providing labelassignments with confidence numbers.6.2 Increasing RecallSetting.
In a second experiment, we move the t-factextraction stage away from high precision towardshigher recall, where the successive noise cleaningstage attempts to restore the precision level.Results.
The columns of Table 2 show results fordifferent values of ?1 of Eq.
(1).
From left to right,we used ?1 = e?1, 0.6, 0.8 for worksForClub and?1 = e?2, e?1, 0.6 for isMarriedTo.
The table?s up-per part reports on the output of stage 3, whereas thelower part covers the facts returned by noise cleaning.Analysis.
For the conservative setting label propa-gation produces high precision facts with only fewinconsistencies, so the noise cleaning stage has noeffect, i.e.
no pruning takes place.
This is the set-ting usual pattern-based approaches without cleaningstage are working in.
In contrast, for the standard set-ting (coinciding with Table 1?s left column) stage 3yields less precision, but higher recall.
Since there aremore inconsistencies in this setup, the noise cleaningstage accomplishes precision gains compensating forthe losses in the previous stage.
In the relaxed settingprecision drops too low, so the noise cleaning stage isunable to figure out the truly correct facts.
In general,the effects on worksForClub are weaker, since in thisrelation the constraints are less influential.Conservative Standard RelaxedPrec.
# Obs.
Prec.
# Obs.
Prec.
# Obs.worksForClub begin 83% 2443 80% 2537 80% 2608 WithoutNoiseCleaningduring 81% 2523 78% 2826 76% 2928end 77% 377 65% 440 62% 501isMarriedTo begin 72% 112 52% 195 44% 269during 90% 63 76% 92 52% 187end 67% 37 62% 50 36% 116worksForClub begin 83% 2389 85% 2469 84% 2536WithNoiseCleaningduring 88% 2474 85% 2761 75% 2861end 79% 349 72% 403 70% 463isMarriedTo begin 72% 111 64% 177 46% 239during 90% 62 79% 89 54% 177end 69% 36 68% 47 38% 110Table 2: Increasing Recall.7 ConclusionIn this paper we have developed a method that com-bines label propagation with constraint reasoningfor temporal fact extraction.
Our experiments haveshown that best results can be achieved by applying?aggressive?
label propagation with a subsequent ILPfor ?clean-up?.
By coupling both approaches weachieve both high(er) precision and high(er) recall.Thus, our method efficiently extracts high qualitytemporal facts at large scale.236AcknowledgementsThis work is supported by the 7th Framework ISTprogramme of the European Union through the fo-cused research project (STREP) on Longitudinal An-alytics of Web Archive data (LAWA) under contractno.
258105.ReferencesJames F. Allen.
1983.
Maintaining knowledge abouttemporal intervals.
Commun.
ACM, 26(11):832?843,November.So?ren Auer, Christian Bizer, Georgi Kobilarov, JensLehmann, and Zachary Ives.
2007.
Dbpedia: A nu-cleus for a web of open data.
In In 6th Intl SemanticWeb Conference, Busan, Korea, pages 11?15.
Springer.Andrew Carlson, Justin Betteridge, Bryan Kisiel, BurrSettles, Estevam R. Hruschka Jr., and Tom M. Mitchell.2010a.
Toward an architecture for never-ending lan-guage learning.
In AAAI, pages 1306?1313.Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-tevam R. Hruschka Jr., and Tom M. Mitchell.
2010b.Coupled semi-supervised learning for information ex-traction.
In Proceedings of the Third ACM Interna-tional Conference on Web Search and Data Mining(WSDM 2010).Nathanael Chambers and Daniel Jurafsky.
2008.
Jointlycombining implicit constraints improves temporal or-dering.
In EMNLP, pages 698?706.Oren Etzioni, Michele Banko, Stephen Soderland, andDaniel S. Weld.
2008.
Open information extractionfrom the web.
Commun.
ACM, 51(12):68?74, Decem-ber.Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino,Hagen Fu?rstenau, Manfred Pinkal, Marc Spaniol, Ste-fan Thater, and Gerhard Weikum.
2011.
Robust disam-biguation of named entities in text.
In Proc.
of EMNLP2011: Conference on Empirical Methods in NaturalLanguage Processing, Edinburgh, Scotland, UK, July2731, pages 782?792.Richard M. Karp.
1972.
Reducibility among combinato-rial problems.
In Complexity of Computer Computa-tions, pages 85?103.Xiao Ling and Daniel S. Weld.
2010.
Temporal infor-mation extraction.
In Proceedings of the AAAI 2010Conference, pages 1385 ?
1390, Atlanta, Georgia, USA,July 11-15.
Association for the Advancement of Artifi-cial Intelligence.Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong MinLee, and James Pustejovsky.
2006.
Machine learningof temporal relations.
In In ACL-06, pages 17?18.James Pustejovsky, Jose?
M. Castan?o, Robert Ingria, RoserSauri, Robert J. Gaizauskas, Andrea Setzer, GrahamKatz, and Dragomir R. Radev.
2003.
TimeML: Robustspecification of event and temporal expressions in text.In New Directions in Question Answering, pages 28?34.Dan Roth and Wen-Tau Yih, 2004.
A Linear ProgrammingFormulation for Global Inference in Natural LanguageTasks, pages 1?8.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: a core of semantic knowledge.In WWW ?07: Proceedings of the 16th InternationalConference on World Wide Web, pages 697?706, NewYork, NY, USA.
ACM.Partha Pratim Talukdar and Koby Crammer.
2009.
Newregularized algorithms for transductive learning.
InProceedings of the European Conference on MachineLearning and Knowledge Discovery in Databases: PartII, ECML PKDD ?09, pages 442?457, Berlin, Heidel-berg.
Springer-Verlag.Partha Pratim Talukdar, Derry Wijaya, and Tom Mitchell.2012.
Coupled temporal scoping of relational facts.In Proceedings of the Fifth ACM International Confer-ence on Web Search and Data Mining (WSDM), Seattle,Washington, USA, February.
Association for Comput-ing Machinery.Marc Verhagen, Inderjeet Mani, Roser Sauri, Robert Knip-pen, Seok Bae Jang, Jessica Littman, Anna Rumshisky,John Phillips, and James Pustejovsky.
2005.
Automat-ing temporal annotation with TARSQI.
In ACL ?05:Proceedings of the ACL 2005 on Interactive poster anddemonstration sessions, pages 81?84, Morristown, NJ,USA.
Association for Computational Linguistics.Marc Verhagen, Robert Gaizauskas, Frank Schilder, MarkHepple, Jessica Moszkowicz, and James Pustejovsky.2009.
The tempeval challenge: identifying temporalrelations in text.
Language Resources and Evaluation,43:161?179.Yafang Wang, Bin Yang, Lizhen Qu, Marc Spaniol, andGerhard Weikum.
2011.
Harvesting facts from textualweb sources by constrained label propagation.
In Pro-ceedings of the 20th ACM international conference onInformation and knowledge management, CIKM ?11,pages 837?846, New York, NY, USA.
ACM.Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-hara, and Yuji Matsumoto.
2009.
Jointly identifyingtemporal relations with markov logic.
In Proceedingsof the Joint Conference of the 47th Annual Meeting ofthe ACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Volume 1- Volume 1, ACL ?09, pages 405?413, Stroudsburg, PA,USA.
Association for Computational Linguistics.Qi Zhang, Fabian Suchanek, and Gerhard Weikum.
2008.TOB: Timely ontologies for business relations.
In 11thInternational Workshop on Web and Databases 2008(WebDB 2008), Vancouver, Canada.
ACM.237
