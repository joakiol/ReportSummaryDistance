Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 1?9,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsRecognition and Understanding of MeetingsSteve RenalsCentre for Speech Technology Research, University of EdinburghInformatics Forum, 10 Crichton Street, Edinburgh EH8 9AB, UKs.renals@ed.ac.uk homepages.inf.ed.ac.uk/srenals/AbstractThis paper is about interpreting human com-munication in meetings using audio, video andother signals.
Automatic meeting recognitionand understanding is extremely challenging,since communication in a meeting is sponta-neous and conversational, and involves mul-tiple speakers and multiple modalities.
Thisleads to a number of significant research prob-lems in signal processing, in speech recog-nition, and in discourse interpretation, tak-ing account of both individual and group be-haviours.
Addressing these problems requiresan interdisciplinary effort.
In this paper, Idiscuss the capture and annotation of multi-modal meeting recordings?resulting in theAMI meeting corpus?and how we have builton this to develop techniques and applicationsfor the recognition and interpretation of meet-ings.1 IntroductionOn the face of it, meetings do not seem to form acompelling research area.
Although many peoplespend a substantial fraction of their time in meetings(e.g.
the 1998 3M online survey at http://www.3m.com/meetingnetwork/), for most peoplethey are not the most enjoyable aspect of their work.However, for all the time that is spent in meet-ings, technological support for the meeting processis scant.
Meeting records usually take the form ofbrief minutes, personal notes, and more recent useof collaborative web 2.0 software.
Such records arelabour intensive to produce?because they are man-ually created?and usually fail to capture much ofthe content of a meeting, for example the factorsthat led to a particular decision and the different sub-jective attitudes displayed by the meeting attendees.For all the time invested in meetings, very little ofthe wealth of information that is exchanged is ex-plicitly preserved.To preserve the information recorded in meet-ings, it is necessary to capture it.
Obviously thisinvolves recording the speech of the meeting partic-ipants.
However, human communication is a mul-timodal activity with information being exchangedvia gestures, handwritten diagrams, and numeroussocial signals.
The creation of a rich meeting recordinvolves the capture of data across several modal-ities.
It is a key engineering challenge to capturesuch multimodal signals in a reliable, unobtrusiveand flexible way, but the greater challenges arisefrom unlocking the multimodal recordings.
If suchrecordings are not transcribed and indexed (at theleast), then access merely corresponds to replay.And it is rare that people will have the time, or theinclination, to replay a meeting.There is a long and interesting thread of researchwhich is concerned to better understand the dynam-ics of meetings and the way that groups function(Bales, 1951; McGrath, 1991; Stasser and Taylor,1991).
The types of analyses and studies carriedout by these authors is still someway beyond whatwe can do automatically.
The first significant workon automatic processing of meetings, coupled withan exploration of how people might interact withan archive of recorded meetings, was performed inthe mid 1990s (Kazman et al, 1996).
This workwas limited by the fact that it was not possible at1that time to transcribe meeting speech automatically.Other early work in the area concentrated on themultimodal capture and broadcast of meetings (Royand Luz, 1999; Cutler et al, 2002; Yong et al,2001).Three groups further developed approaches toautomatically index the content of meetings.
Ateam at Fuji Xerox PARC used video retrieval tech-niques such as keyframing to automatically gener-ate manga-style summaries of meetings (Uchihashiet al, 1999), Waibel and colleagues at CMU usedspeech recognition and video tracking for meet-ings (Waibel et al, 2001), and Morgan and col-leagues at ICSI focused on audio-only capture andspeech recognition (Morgan et al, 2003).
Since2003 research in the recognition and understand-ing of meetings has developed substantially, stim-ulated by evaluation campaigns such as the NISTRich Transcription (RT)1 and CLEAR2 evaluations,as well as some large multidisciplinary projects suchas AMI/AMIDA3, CHIL4 and CALO5.This paper is about the work we have carried outin meeting capture, recognition and interpretationwithin the AMI and AMIDA projects since 2004.One of the principal outputs of these projects wasa multimodal corpus of meeting recordings, anno-tated at a number of different levels.
In section 2 wediscuss collection of meeting data, and the construc-tion of the AMI corpus.
The remainder of the pa-per discusses the automatic recognition (section 3)and interpretation (section 4) of multimodal meetingrecordings, application prototypes (section 5) and is-sues relating to evaluation (section 6).2 The AMI corpusIdeally it would not be necessary to undertake a largescale data collection and annotation exercise, everytime we address a new domain.
However unsuper-vised adaptation techniques are still rather imma-ture, and prior to the collection of the AMI corpus,there had not been a controlled collection and multi-level annotation of multiparty interactions, recordedacross multiple modalities.1www.itl.nist.gov/iad/mig/tests/rt/2clear-evaluation.org/3www.amiproject.org/4chil.server.de/5caloproject.sri.com/Figure 1: AMI instrumented meeting room: four co-located participants, one joined by video conference.
Inthis case two microphone arrays and seven cameras wereused.One of our key motivations is the developmentof automatic approaches to recognise and interpretgroup interactions, using information spread acrossmultiple modalities, but collected as unobtrusivelyas possible.
This led to the design and constructionof the AMI Instrumented Meeting Rooms (figure 1)at the University of Edinburgh, Idiap Research In-stitute, and TNO Human Factors.
These rooms con-tained a set of standardised recording equipment in-cluding six or seven cameras (four of which wouldbe used for close-up views in meeting of up to fourpeople), an 8-element microphone array, a close-talking microphone for each participant (used toguarantee a clean audio signal for each speaker),as well capture of digital pens, whiteboards, sharedlaptop spaces, data projector and videoconferencingif used.
A considerable amount of hardware wasnecessary for ensuring frame-level synchronisation.More recently we have used a lighter weight setup,that uses a high resolution spherical digital videocamera system, and a single microphone array (7?20 elements, depending on meeting size) synchro-nised using software.
We have also constructed aprototype system using a low-cost, flexible array ofdigital MEMS microphones (Zwyssig et al, 2010).We used these instrumented meeting rooms torecord the AMI Meeting Corpus (Carletta, 2007).This corpus contains over 100 hours of meetingrecordings, with the different recording streams syn-chronised to a common timeline.
The corpus con-tains a number of manually created and automaticannotations, synchronised to the same timeline.
This2includes a high-quality manual word-level transcrip-tion of the complete corpus, as well as reference au-tomatic speech recognition output, using the speechrecognition system discussed in section 3 (using 5-fold cross-validation).
In addition to word-leveltranscriptions, the corpus includes manual annota-tions that describe the behaviour of meeting partici-pants at a number of levels.
These include dialogueacts, topic segmentation, extractive and abstractivesummaries, named entities, limited forms of headand hand gestures, gaze direction, movement aroundthe room, and head pose information.
Some of theseannotations, in particular video annotation, are ex-pensive to perform: about 10 hours of meetings havebeen completely annotated at all these levels; over70% of the corpus has been fully annotated withthe linguistic annotations.
NXT?the NITE XMLToolkit6?an XML-based open source software in-frastructure for multimodal annotation was used tocarry out and manage the annotations.About 70% of the AMI corpus consists of meet-ings based on a design scenario, in which four par-ticipants play roles in a design team.
The scenarioinvolves four team meetings, between which the par-ticipants had tasks to accomplish.
The participantroles were stimulated in real-time by email and webcontent.
Although the use of a scenario reduces theoverall realism of the meetings, we adopted this ap-proach for several reasons, most importantly: (1)there were some preferred design outcomes, mak-ing it possible to define some objective group out-come measures; (2) the knowledge and motivationof the participants was controlled, thus removing theserious confounding factors that would arise fromthe long history and context found in real organ-isations; and (3) allowing the meeting scenario tobe replicated, thus enabling system-level evaluations(as discussed in section 6).
We recorded and anno-tated thirty replicates of the scenario: this providesan unparalleled resource for system evaluation, butalso reduces the variability of the corpus (for ex-ample in terms of the language used).
The remain-ing 30% of the corpus contains meetings that wouldhave occurred anyway; these are meetings with alot less control than the scenario meetings, but withgreater linguistic variability.6sourceforge.net/projects/nite/All the meetings in the AMI corpus are spokenin English, but over half the participants are non-native speakers.
This adds realism in a Europeancontext, as well as providing an additional speechrecognition challenge.
The corpus is publicly avail-able7, and is released under a licence that is based onthe Creative Commons Attribution NonCommercialShareAlike 2.5 Licence.
This includes all the signalsand manual annotations, plus a number of automaticannotations (e.g.
speech recognition) made availableto lower the startup cost of performing research onthe corpus.3 Multimodal recognitionThe predominant motivation behind the collectionand annotation of the AMI corpus was to enable thedevelopment of multimodal recognisers to addressissues such as speech recognition, speaker diarisar-tion (Wooters and Huijbregts, 2007), gesture recog-nition (Al-Hames et al, 2007) and focus of attention(Ba and Odobez, 2008).
Although speech recog-nition is based on the (multichannel) audio signal,the other problems can be successfully addressed bycombining modalities.
(There is certainly informa-tion in other modalities that has the potential to makespeech recognition more accurate, but so far we havenot been able to use it consistently and robustly.
)Speech recognition: The automatic transcriptionof what is spoken in a meeting is an essential pre-requisite to interpreting a meeting.
Morgan et al(2003) described the speech recognition of meetingsas an ?ASR-complete?
problem.
Developing an ac-curate system for meeting recognition involves theautomatic segmentation of the recording into utter-ances from a single talker, robustness to reverbera-tion and competing acoustic sources, handling over-lapping talkers, exploitation of multiple microphonerecordings, as well as the core acoustic and languagemodelling problems that arise when attempting torecognise spontaneous, conversational speech.Our initial systems for meeting recognition usedaudio recorded with close-talking microphones, inorder to develop the core acoustic modelling tech-niques.
More recently our focus has been on recog-nising speech obtained using tabletop microphone7corpus.amiproject.org/3arrays, which are less intrusive but have a lowersignal-to-noise ratio.
Multiple microphone sys-tems are based on microphone array beamformingin which the individual microphone signals are fil-tered and summed to enhance signals coming froma particular direction, while suppressing signalsfrom competing locations (Wo?lfel and McDonough,2009).The core acoustic and language modelling com-ponents for meeting speech recognition correspondquite closely to the state-of-the-art systems used inother domains.
Acoustic modelling techniques in-clude vocal tract length normalisation, speaker adap-tation based on maximum likelihood linear trans-forms, and further training using a discriminativeminimum Bayes risk criterion such as minimumphone error rate (Gales and Young, 2007; Renalsand Hain, 2010).
In addition we have employed anumber of novel acoustic parameterisations includ-ing approaches based on local posterior probabilityestimation (Grezl et al, 2007) and pitch adaptivefeatures (Garau and Renals, 2008), the automaticconstruction of domain-specific language modelsusing documents obtained from the web by search-ing with n-grams obtained from meeting transcripts(Wan and Hain, 2006; Bulyko et al, 2007), and au-tomatic approaches to acoustic segmentation opti-mised for meetings (Wrigley et al, 2005; Dines etal., 2006).A feature of the systems developed for meetingrecognition is the use of multiple recognition passes,cross-adaptation and model combination (Hain etal., 2007).
In particular successive passes make useof more detailed?and more diverse?acoustic andlanguage models.
Different acoustic models trainedon different feature representations (e.g.
standardPLP features and posterior probability-based fea-tures) are cross-adapted, and different feature repre-sentations are also combined using linear transformssuch as heteroscedastic linear discriminant analysis(Kumar and Andreou, 1998).These systems have been evaluated in successiveNIST RT evaluations: the core microphone arraybased system has a word error rate of about 40%;after adaptation and feature combination steps, thiserror rate can be reduced to about 30%.
The equiv-alent close-talking microphone system has baselineword error rate of about 35%, reduced to less than25% after further recognition passes (Hain et al,2007).
The core system runs about five times slowerthan real-time, and the full system is about fourteentimes slower than real-time, on current commodityhardware.
We have developed a low-latency real-time system (with an error rate of about 41% for mi-crophone array input) (Garner et al, 2009), based onan open source runtime system8.4 Meeting interpretationOne of the interdisciplinary joys of working onmeetings is that researchers with different ap-proaches are able to build collaborations throughworking on common problems and common data.The automatic interpretation of meetings is a verygood example: meetings form an exciting challengefor work in things such as topic identification, sum-marisation, dialogue act recognition and the recog-nition of subjective content.
Although text-basedapproaches (using the output of a speech recogni-tion system) form strong baselines, it is often thecase that systems can be improved through the in-corporation of information characteristic of spokencommunication, such as prosody and speaker turnpatterns, as well video information such as head orhand movements.Segmentation: We have explored multistreamstatistical models to automatically segment meetingrecordings.
Meetings can be usefully segmented atmany different levels, for example into speech andnon-speech (an essential pre-processing for speechrecognition), into utterances spoken by a singletalker, into dialogue acts, into topics, and into ?meet-ing phases?.
The latter was the subject of our first in-vestigations in using multimodal multistream mod-els to segment meetings.Meetings are group events, characterised by bothindividual actions and group actions.
To obtainstructure at the group level, we and colleagues inthe M4 and AMI projects investigated segmentinga meeting into a sequence of group actions such asmonologue, discussion and presentation (McCowanet al, 2005).
We used a number of feature streamsfor this segmentation and labelling task includingspeaker turn dynamics, prosody, lexical information,8juicer.amiproject.org/4and participant head and hand movements (Diel-mann and Renals, 2007).
Our initial experimentsused an HMM to model the feature streams with asingle hidden state space, and resulted in an ?actionerror rate?
of over 40% (action error rate is analo-gous to word error rate, but defined over meetingactions, presumed not to overlap).
The HMM wasthen substituted by a richer DBN multistream modelin which each feature stream was processed inde-pendently at a lower level of the model.
These par-tial results were then combined at a higher level,thus providing hierarchical integration of the multi-modal feature streams.
This multistream approachenabled a later integration of feature streams andincreased flexibility in modelling the interdepen-dences between the different streams, enabling someaccommodation for asynchrony and multiple timescales.
Thus use of the richer DBN multistreammodel resulted in a significant lowering of the ac-tion error rate to around 13%.We extended this approach to look at a much finergrained segmentation: dialogue acts.
A dialogue actcan be viewed as a segment of speech labelled so asto roughly categorise the speaker?s intention.
In theAMI corpus each dialogue act in a meeting is givenone of 15 labels, which may be categorised as infor-mation exchange, making or eliciting suggestions oroffers, commenting on the discussion, social acts,backchannels, or ?other?.
The segmentation prob-lem is non-trivial, since a single stretch of speech(with no pauses) from a speaker may comprise sev-eral dialogue acts?and conversely a single dialogueact may contain pauses.
To address the tasks of auto-matically segmenting the speech into dialogue acts,and assigning a label to each segment, we employeda switching dynamic Bayesian network architecture,which modelled a set of features related to lexicalcontent and prosody and incorporates a weighted in-terpolated factored language model (Dielmann andRenals, 2008).
The switching DBN coordinated therecognition process by integrating all the availableresources.
This approach was able to leverage addi-tional corpora of conversational data by using themas training data for a factored language model whichwas used in conjunction with additional task spe-cific language models.
We followed this joint gener-ative model, with a discriminative approach, basedon conditional random fields, which performed a re-classification of the segmented dialogue acts.Our experiments on dialogue act recognition usedboth automatic and manual transcriptions of theAMI corpus.
The degradation when moving frommanual transcriptions to the output of a speechrecogniser was less than 10% absolute for both di-alogue act classification and segmentation.
Our ex-periments indicated that it is possible to perform au-tomatic segmentation into dialogue acts with a rel-atively low error rate.
However the operations oftagging and recognition into fifteen imbalanced DAcategories have a relatively high error rate, even afterdiscriminative reclassification, indicating that thisremains a challenging task.Summarisation: The automatic generation ofsummaries provides a natural way to succinctly de-scribe the content of a meeting, and can be an effi-cient way for users to obtain information.
We havefocussed on extractive techniques to construct sum-maries, in which the most relevant parts of a meetingare located, and concatenated together to provide a?cut-and-paste?
summary, which may be textual ormultimodal.Our approach to extractive summarisation isbased on automatically extracting relevant dialogueacts (or alternatively ?spurts?, segments spoken bya single speaker and delimited by silence) from ameeting (Murray et al, 2006).
This requires (as aminimum) the automatic speech transcription and,if spurts are not used, dialogue act segmentation.Lexical information is clearly extremely importantfor summarisation, but we have also found speakerfeatures (relating to activity, dominance and over-lap), structural features (the length and position ofdialogue acts), prosody, and discourse cues (phraseswhich signal likely relevance) to be important forthe development of accurate methods for extractivesummarisation of meetings.
Furthermore we haveexplored reduced dimension representations of text,based on latent semantic analysis, which we foundadded precision to the summarisation.
Using anevaluation measure referred to as weighted preci-sion, we discovered that it is possible to reliablyextract the most relevant dialogue acts, even in thepresence of speech recognition errors.55 Application prototypesWe have incorporated these meeting recognition andinterpretation components in a number of applica-tions.
Our basic approach to navigating meetingarchives centres on the notion of meeting browsers,in which media files, transcripts and segmentationsare synchronised to a common time line.
Figure 2(a) gives an example of such a browser, which alsoenables a user to pan and zoom within the capturedspherical video stream.We have explored (and, as discussed below, eval-uated) a number of ways of including automaticallygenerated summaries in meeting browsers.
Thebrowser illustrated in figure 2 (b) enables navigationby the summarised transcript or via the topic seg-mentation.
In this case the degree of summarisationis controlled by a slider, which removes those speechsegments that do no contribute to the summary.
Wehave also explored real-time (with a few utteranceslatency) approaches to summarisation, to facilitatemeeting ?catchup?
scenarios, including the genera-tion of audio only summaries, with about 60% ofthe speech removed (Tucker et al, 2010).
Visual-isations of summaries include a comic book layout(Castronovo et al, 2008), illustrated in figure 3.
Thisis related to ?VideoManga?
(Uchihashi et al, 1999),but driven by transcribed speech rather than visuallyidentified keyframes.The availability of real-time meeting speechrecognition, with phrase-level latency (Garner et al,2009), enables a new class of applications.
WithinAMIDA we developed a software architecture re-ferred to as ?The Hub?
to support real-time ap-plications.
The Hub is essentially a real-time an-notation server, mediating between annotation pro-ducers, such as speech recognition, and annotationconsumers, such as a real-time catchup browser.Of course many applications will be both produc-ers and consumers: for instance topic segmenta-tion consumes transcripts and speaker turn informa-tion and produces time aligned topic segments.
Agood example of an application made possible byreal-time recognition components and the Hub is theAMIDA Content Linking Device (Popescu-Belis etal., 2008).
Content linking is essentially a continualreal-time search in which a repository is searchedusing a query constructed from the current conver-(a) Basic web-based browser(b) Summary browserFigure 2: Two examples of meeting browsers, both in-clude time synchronisation with a searchable ASR tran-script and speaker activities.
(a) is a basic web-basedbrowser; (b) also employs extractive summarisation andtopic segmentation components.sational context.
In this case the context is obtainedfrom a speech recognition transcript of the past 30seconds of the conversation, and a query is con-structed using tf ?idf or a similar measure, combinedwith predefined keywords or topic weightings.
Therepository to be searched may be the web, or a por-tion of the web, or it may be an organisational doc-ument repository, including transcribed, structuredand indexed recordings of previous meetings.
Figure4 shows a basic interface to content linking.
We haveconstructed live content-linking systems, driven bymicrophone array based real-time speech recogni-tion, with the aim of presenting?without explicitquery?potentially relevant documents to meetingparticipants.6yeah they likespongymateriallike yeah asponge-ballokay likethisyeahyeahokay our secondaryaudience peopleabove a forty yearsin age they like thedark traditionalcoloursyeah materialslike wood thatwell i figure if wego for l_ l_c_d_we should have theadvancedyeahyeahokay that'smyyeah do yourthing timokayyeah whichbuttons do youwant to in itbecause you can buildin a back-forwardbutton and somesomebody would justwant to watch twochannelsyou we couldchoose whatwhat's betterplastic orrubberyeah i meanplastic isso you could gofor plastic but ifiguredyeahyeahit isn't i thinkyeahwell i don'tknow noMaterialsLCD screenButtonsmaterialsFigure 3: Comic book display of autom tically generatedmeeting summary.6 EvaluationThe multiple streams of data and multiple layers ofannotations that make up the AMI corpus enable it tobe used for evaluations of specific recognition com-ponents.
The corpus has been used to evaluate manydifferent things including voice activity detection,speaker diarisation and speech recognition (in theNIST RT evaluations), and head pose recognition(in the CLEAR evaluation).
In the spoken languageprocessing domain, the AMI corpus has been usedto evaluate meeting summarisation, topic segmen-tation, dialogue act recognition and cross-languageretrieval.In addition to intrinsic component-level evalu-ations, it is valuable to evaluate complete sys-tems, and components in a system context.
In theAMI/AMIDA projects, we investigated a number ofextrinsic evaluation frameworks for browsing andaccessing meeting archives.
The Browser Evalua-tion Test (BET) (Wellner et al, 2005) provides aframework for the comparison of arbitrary meet-ing browser setups, which may differ in terms ofwhich content extraction or abstraction componentsare employed.
In the BET test subjects have to an-swer true/false questions about a number of ?obser-vations of interest?
relating to a recorded meeting,using the browser under test with a specified timelimit (typically half the meeting length).We devel ped of a variant of the BET to specifi-Figure 4: Demonstration screenshot of the AMI auto-matic content linking device.
The subpanels show (clock-wise from top left) the ASR transcript, relevant docu-ments from the meeting document base, relevant web hitsand a a tag cloud.cally evaluate different summarisation approaches.In the Decision Audit evaluation (Murray et al,2009) the user?s task is to ascertain the factors acrossa number of meetings that lead to a particular deci-sion being made.
A set of browsers were constructeddiffering in the summarisation approach employed(manual vs. ASR transcripts; extractive vs. abstrac-tive vs. human vs. keyword-based summarisation),and the test subjects used them to perform the deci-sion audit.
Like the BET this evaluation is labour-intensive, but the results can be analysed using abattery of objective and subjective measures.
Con-clusi ns from carrying out this evaluation indicatedthat the task itself was quite challenging for users(even with human transcripts and summaries, mostusers could not find many factors involved in the de-cision), that automatic extractive summaries outper-formed reasonably competitive baseline approaches,and that although subjects reported ASR transcriptsto be unsatisfactory (due to the error rate) browsingusing the ASR transcript still resulted in users?
be-ing generally able to find the relevant parts of themeeting archive.77 ConclusionsIn this paper I have given an overview of our inves-tigations into automatic meeting recognition and in-terpretation.
Multiparty communication is a chal-lenging problem at many levels, from signal pro-cessing to discourse modelling.
A major part ofour attempt to address this problem, in an interdisci-plinary way, was the collection, annotation, and dis-tribution of the AMI meeting corpus.
The AMI cor-pus has been at the basis of nearly all the work thatwe have carried out in the area, from speech recog-nition to summarisation.
Multiparty speech recog-nition remains a difficult task, with a typical errorrate of over 20%, however the accuracy is enough toenable various components to build on top of it.
Amajor achievement has been the development of pro-totype applications that can use phrase-level latencyreal-time speech recognition.Many of the automatic approaches to meetingrecognition and characterisation are characterised byextensive combination at the feature stream, modeland system level.
In our experience, such ap-proaches offer consistent improvements in accuracyfor these complex, multimodal tasks.Meetings serve a social function, and much ofthis has been ignored in our work, so far.
We havefocussed principally on understanding meetings interms of their lexical content, augmented by vari-ous multimodal streams.
However in many inter-actions, the social signals are at least as importantas the propositional content of the words (Pentland,2008); it is a major challenge to develop meeting in-terpretation components that can infer and take ad-vantage of such social cues.
We have made initialattempts to do this, by attempting to include aspectssuch as social role (Huang and Renals, 2008).The AMI corpus involved a substantial effort frommany individuals, and provides an invaluable re-source.
However, we do not wish to do this again,even if we are dealing with a domain that is sig-nificantly different, such as larger groups, or family?meetings?.
However, our recognisers rely stronglyon annotated in-domain data.
It is a major chal-lenge to develop algorithms that are unsupervisedand adaptive to free us from the need to collect andannotate large amount of data each time we are in-terested in a new domain.AcknowledgmentsThis paper has arisen from a collaboration involvingseveral laboratories.
I have benefitted, in particular,from long-term collaborations with Herve?
Bourlard,Jean Carletta, Thomas Hain, and Mike Lincoln, andfrom a number of fantastic PhD students.
This workwas supported by the European IST/ICT ProgrammeProjects IST-2001-34485 (M4), FP6-506811 (AMI),FP6-033812 (AMIDA), and FP7-231287 (SSPNet).This paper only reflects the author?s views and fund-ing agencies are not liable for any use that may bemade of the information contained herein.ReferencesM.
Al-Hames, C. Lenz, S. Reiter, J. Schenk, F. Wallhoff,and G. Rigoll.
2007.
Robust multi-modal group actionrecognition in meetings from disturbed videos with theasynchronous hidden Markov model.
In Proc IEEEICIP.S.
O. Ba and J. M. Odobez.
2008.
Multi-party focus ofattention recognition in meetings from head pose andmultimodal contextual cues.
In Proc.
IEEE ICASSP.R.
F. Bales.
1951.
Interaction Process Analysis.
Addi-son Wesley, Cambridge MA, USA.I.
Bulyko, M. Ostendorf, M. Siu, T. Ng, A. Stolcke, andO.
Cetin.
2007.
Web resources for language modelingin conversational speech recognition.
ACM Transac-tions on Speech and Language Processing, 5(1):1?25.J.
Carletta.
2007.
Unleashing the killer corpus: expe-riences in creating the multi-everything AMI Meet-ing Corpus.
Language Resources and Evaluation,41:181?190.S.
Castronovo, J. Frey, and P. Poller.
2008.
A genericlayout-tool for summaries of meetings in a constraint-based approach.
In Machine Learning for MultimodalInteraction (Proc.
MLMI ?08).
Springer.R.
Cutler, Y. Rui, A. Gupta, J. Cadiz, I. Tashev, L. He,A.
Colburn, Z. Zhang, Z. Liu, and S. Silverberg.
2002.Distributed meetings: a meeting capture and broad-casting system.
In Proc.
ACM Multimedia, pages 503?512.A.
Dielmann and S. Renals.
2007.
Automatic meet-ing segmentation using dynamic Bayesian networks.IEEE Transactions on Multimedia, 9(1):25?36.A.
Dielmann and S. Renals.
2008.
Recognition of di-alogue acts in multiparty meetings using a switchingDBN.
IEEE Transactions on Audio, Speech and Lan-guage Processing, 16(7):1303?1314.J.
Dines, J. Vepa, and T. Hain.
2006.
The segmenta-tion of multi-channel meeting recordings for automaticspeech recognition.
In Proc.
Interspeech.8M.
J. F. Gales and S. J.
Young.
2007.
The application ofhidden Markov models in speech recognition.
Foun-dations and Trends in Signal Processing, 1(3):195?304.G.
Garau and S. Renals.
2008.
Combining spectral rep-resentations for large vocabulary continuous speechrecognition.
IEEE Transactions on Audio, Speech andLanguage Processing, 16(3):508?518.P.
Garner, J. Dines, T. Hain, A. El Hannani, M. Karafiat,D.
Korchagin, M. Lincoln, V. Wan, and L. Zhang.2009.
Real-time ASR from meetings.
In Proc.
In-terspeech.F.
Grezl, M. Karafiat, S. Kontar, and J. Cernocky.
2007.Probabilistic and bottle-neck features for lvcsr ofmeetings.
In Acoustics, Speech and Signal Process-ing, 2007.
ICASSP 2007.
IEEE International Confer-ence on, volume 4, pages IV?757?IV?760.T.
Hain, L. Burget, J. Dines, G. Garau, M. Karafiat,M.
Lincoln, J. Vepa, and V. Wan.
2007.
The amisystem for the transcription of speech in meetings.
InProc.
IEEE ICASSP?07.S.
Huang and S. Renals.
2008.
Unsupervised languagemodel adaptation based on topic and role informationin multiparty meetings.
In Proc.
Interspeech ?08.R.
Kazman, R. Al-Halimi, W. Hunt, and M. Mantei.1996.
Four paradigms for indexing video conferences.Multimedia, IEEE, 3(1):63?73.N.
Kumar and A. G. Andreou.
1998.
Heteroscedasticdiscriminant analysis and reduced rank HMMs for im-proved recognition.
Speech Communication, 26:283?297.I.
McCowan, D. Gatica-Perez, S. Bengio, G. Lathoud,M.
Barnard, and D. Zhang.
2005.
Automatic analysisof multimodal group actions in meetings.
IEEE Trans-actions on Pattern Analysis and Machine Intelligence,27(3):305?317.J.
E. McGrath.
1991.
Time, interaction, and performance(TIP): A theory of groups.
Small Group Research,22(2):147.N.
Morgan, D. Baron, S. Bhagat, H. Carvey, R. Dhillon,J.
Edwards, D. Gelbart, A. Janin, A. Krupski, B. Pe-skin, T. Pfau, E. Shriberg, A. Stolcke, and C. Woot-ers.
2003.
Meetings about meetings: research at ICSIon speech in multiparty conversations.
In Proc.
IEEEICASSP.G.
Murray, S. Renals, J. Moore, and J. Carletta.
2006.
In-corporating speaker and discourse features into speechsummarization.
In Proceedings of the Human Lan-guage Technology Conference of the NAACL, pages367?374.G.
Murray, T. Kleinbauer, P. Poller, T. Becker, S. Renals,and J. Kilgour.
2009.
Extrinsic summarization eval-uation: A decision audit task.
ACM Transactions onSpeech and Language Processing, 6(2):1?29.A.S.
Pentland.
2008.
Honest signals: how they shapeour world.
The MIT Press.A.
Popescu-Belis, E. Boertjes, J. Kilgour, P. Poller,S.
Castronovo, T. Wilson, A. Jaimes, and J. Car-letta.
2008.
The amida automatic content linking de-vice: Just-in-time document retrieval in meetings.
InMachine Learning for Multimodal Interaction (Proc.MLMI ?08).S.
Renals and T. Hain.
2010.
Speech recognition.
InA.
Clark, C. Fox, and S. Lappin, editors, Handbookof Computational Linguistics and Natural LanguageProcessing.
Wiley Blackwell.D.
M. Roy and S. Luz.
1999.
Audio meeting historytool: Interactive graphical user-support for virtual au-dio meetings.
In Proc.
ESCA Workshop on AccessingInformation in Spoken Audio, pages 107?110.G.
Stasser and LA Taylor.
1991.
Speaking turns in face-to-face discussions.
Journal of Personality and SocialPsychology, 60(5):675?684.S.
Tucker, O. Bergman, A. Ramamoorthy, and S. Whit-taker.
2010.
Catchup: a useful application of time-travel in meetings.
In Proc.
ACM CSCW, pages 99?102.S.
Uchihashi, J. Foote, A. Girgensohn, and J. Boreczky.1999.
Video manga: generating semantically mean-ingful video summaries.
In Proc.
ACM Multimedia,pages 383?392.A.
Waibel, M. Bett, F. Metze, K. Ries, T. Schaaf,T.
Schultz, H. Soltau, H. Yu, and K. Zechner.
2001.Advances in automatic meeting record creation and ac-cess.
In Proc IEEE ICASSP.V.
Wan and T. Hain.
2006.
Strategies for language modelweb-data collection.
In Proc IEEE ICASSP.P.
Wellner, M. Flynn, S. Tucker, and S. Whittaker.
2005.A meeting browser evaluation test.
In Proc.
ACMCHI,pages 2021?2024.M.
Wo?lfel and J. McDonough.
2009.
Distant SpeechRecognition.
Wiley.C.
Wooters and M. Huijbregts.
2007.
The ICSI RT07sspeaker diarization system.
In Multimodal Technolo-gies for Perception of Humans.
International Evalu-ation Workshops CLEAR 2007 and RT 2007, volume4625 of LNCS, pages 509?519.
Springer.S.
Wrigley, G. Brown, V. Wan, and S. Renals.
2005.Speech and crosstalk detection in multichannel audio.IEEE Transactions on Speech and Audio Processing,13(1):84?91.R.
Yong, A. Gupta, and J. Cadiz.
2001.
Viewing meet-ings captured by an omni-directional camera.
ACMTransactions on Computing Human Interaction.E.
Zwyssig, M. Lincoln, and S. Renals.
2010.
A digitalmicrophone array for distant speech recognition.
InProc.
IEEE ICASSP?10.9
