In Question Answering, Two Heads Are Better Than OneJennifer Chu-Carroll Krzysztof Czuba John Prager Abraham IttycheriahIBM T.J. Watson Research CenterP.O.
Box 704Yorktown Heights, NY 10598, U.S.A.jencc,kczuba,jprager,abei@us.ibm.comAbstractMotivated by the success of ensemble methodsin machine learning and other areas of natu-ral language processing, we developed a multi-strategy and multi-source approach to questionanswering which is based on combining the re-sults from different answering agents searchingfor answers in multiple corpora.
The answer-ing agents adopt fundamentally different strate-gies, one utilizing primarily knowledge-basedmechanisms and the other adopting statisticaltechniques.
We present our multi-level answerresolution algorithm that combines results fromthe answering agents at the question, passage,and/or answer levels.
Experiments evaluatingthe effectiveness of our answer resolution algo-rithm show a 35.0% relative improvement overour baseline system in the number of questionscorrectly answered, and a 32.8% improvementaccording to the average precision metric.1 IntroductionTraditional question answering (QA) systems typicallyemploy a pipeline approach, consisting roughly of ques-tion analysis, document/passage retrieval, and answer se-lection (see e.g., (Prager et al, 2000; Moldovan et al,2000; Hovy et al, 2001; Clarke et al, 2001)).
Although atypical QA system classifies questions based on expectedanswer types, it adopts the same strategy for locating po-tential answers from the same corpus regardless of thequestion classification.
In our own earlier work, we de-veloped a specialized mechanism called Virtual Annota-tion for handling definition questions (e.g., ?Who wasGalileo??
and ?What are antibiotics??)
that consults,in addition to the standard reference corpus, a structuredknowledge source (WordNet) for answering such ques-tions (Prager et al, 2001).
We have shown that betterperformance is achieved by applying Virtual Annotationand our general purpose QA strategy in parallel.
In thispaper, we investigate the impact of adopting such a multi-strategy and multi-source approach to QA in a more gen-eral fashion.Our approach to question answering is additionallymotivated by the success of ensemble methods in ma-chine learning, where multiple classifiers are employedand their results are combined to produce the final outputof the ensemble (for an overview, see (Dietterich, 1997)).Such ensemble methods have recently been adopted inquestion answering (Chu-Carroll et al, 2003b; Burgeret al, 2003).
In our question answering system, PI-QUANT, we utilize in parallel multiple answering agentsthat adopt different processing strategies and consult dif-ferent knowledge sources in identifying answers to givenquestions, and we employ resolution mechanisms to com-bine the results produced by the individual answeringagents.We call our approach multi-strategy since we com-bine the results from a number of independent agents im-plementing different answer finding strategies.
We alsocall it multi-source since the different agents can searchfor answers in multiple knowledge sources.
In this pa-per, we focus on two answering agents that adopt fun-damentally different strategies: one agent uses predomi-nantly knowledge-based mechanisms, whereas the otheragent is based on statistical methods.
Our multi-levelresolution algorithm enables combination of results fromeach answering agent at the question, passage, and/or an-swer levels.
Our experiments show that in most casesour multi-level resolution algorithm outperforms its com-ponents, supporting a tightly-coupled design for multi-agent QA systems.
Experimental results show signifi-cant performance improvement over our single-strategy,single-source baselines, with the best performing multi-level resolution algorithm achieving a 35.0% relative im-provement in the number of correct answers and a 32.8%improvement in average precision, on a previously un-seen test set.Edmonton, May-June 2003Main Papers , pp.
24-31Proceedings of HLT-NAACL 2003Answering AgentsKSPSemanticSearchKeywordSearchQuestionWordNetAnswerCycQFrameQuestionAnalysis QGoalsKnowledge-BasedAnswering AgentStatisticalAnswering AgentAquaintcorpusTRECcorpusEBAnswerResolutionDefinition QAnswering AgentKSP-BasedAnswering AgentKnowledge SourcesFigure 1: PIQUANT?s Architecture2 A Multi-Agent QA ArchitectureIn order to enable a multi-source and multi-strategy ap-proach to question answering, we developed a modu-lar and extensible QA architecture as shown in Figure 1(Chu-Carroll et al, 2003a; Chu-Carroll et al, 2003b).With a consistent interface defined for each component,this architecture allows for easy plug-and-play of individ-ual components for experimental purposes.In our architecture, a question is first processed by thequestion analysis component.
The analysis results arerepresented as a QFrame, which minimally includes a setof question features that help activate one or more an-swering agents.
Each answering agent takes the QFrameand generates its own set of requests to a variety ofknowledge sources.
This may include performing searchagainst a text corpus and extracting answers from the re-sulting passages, or performing a query against a struc-tured knowledge source, such as WordNet (Miller, 1995)or Cyc (Lenat, 1995).
The (intermediate) results fromthe individual answering agents are then passed on to theanswer resolution component, which combines and re-solves the set of results, and either produces the system?sfinal answers or feeds the intermediate results back to theanswering agents for further processing.We have developed multiple answering agents, somegeneral purpose and others tailored for specific ques-tion types.
Figure 1 shows the answering agents cur-rently available in PIQUANT.
The knowledge-based andstatistical answering agents are general-purpose agentsthat adopt different processing strategies and consult anumber of different text resources.
The definition-Qagent targets definition questions (e.g., ?What is peni-cillin??
and ?Who is Picasso??)
with a technique calledVirtual Annotation using the external knowledge sourceWordNet (Prager et al, 2001).
The KSP-based answer-ing agent focuses on a subset of factoid questions withspecific logical forms, such as capital(?COUNTRY) andstate tree(?STATE).
The answering agent sends requeststo the KSP (Knowledge Sources Portal), which returns, ifpossible, an answer from a structured knowledge source(Chu-Carroll et al, 2003a).In the rest of this paper, we briefly describe our twogeneral-purpose answering agents.
We then focus on amulti-level answer resolution algorithm, applicable at dif-ferent points in the QA process of these two answeringagents.
Finally, we discuss experiments conducted to dis-cover effective methods for combining results from mul-tiple answering agents.3 Component Answering AgentsWe focus on two end-to-end answering agents designedto answer short, fact-seeking questions from a collectionof text documents, as motivated by the requirements ofthe TREC QA track (Voorhees, 2003).
Both answer-ing agents adopt the classic pipeline architecture, con-sisting roughly of question analysis, passage retrieval,and answer selection components.
Although the answer-ing agents adopt fundamentally different strategies intheir individual components, they have performed quitecomparably in past TREC QA tracks (Voorhees, 2001;Voorhees, 2002).3.1 Knowledge-Based Answering AgentOur first answering agent utilizes a primarily knowledge-driven approach, based on Predictive Annotation (Prageret al, 2000).
A key characteristic of this approach is thatpotential answers, such as person names, locations, anddates, in the corpus are predictively annotated.
In otherwords, the corpus is indexed not only with keywords, asis typical for most search engines, but also with the se-mantic classes of these pre-identified potential answers.During the question analysis phase, a rule-based mech-anism is employed to select one or more expected an-swer types, from a set of about 80 classes used in thepredictive annotation process, along with a set of ques-tion keywords.
A weighted search engine query is thenconstructed from the keywords, their morphological vari-ations, synonyms, and the answer type(s).
The search en-gine returns a hit list of typically 10 passages, each con-sisting of 1-3 sentences.
The candidate answers in thesepassages are identified and ranked based on three criteria:1) match in semantic type between candidate answer andexpected answer, 2) match in weighted grammatical rela-tionships between question and answer passages, and 3)frequency of answer in candidate passages (redundancy).The answering agent returns the top n ranked candidateanswers along with a confidence score for each answer.3.2 Statistical Answering AgentThe second answering agent takes a statistical approachto question answering (Ittycheriah, 2001; Ittycheriah etal., 2001).
It models the distribution p(c|q, a), whichmeasures the ?correctness?
(c) of an answer (a) to a ques-tion (q), by introducing a hidden variable representing theanswer type (e) as follows:p(c|q, a) =?e p(c, e|q, a)=?e p(c|e, q, a)p(e|q, a)p(e|q, a) is the answer type model which predicts, fromthe question and a proposed answer, the answer type theyboth satisfy.
p(c|e, q, a) is the answer selection model.Given a question, an answer, and the predicted answertype, it seeks to model the correctness of this configura-tion.
These distributions are modeled using a maximumentropy formulation (Berger et al, 1996), using trainingdata which consists of human judgments of question an-swer pairs.
For the answer type model, 13K questionswere annotated with 31 categories.
For the answer selec-tion model, 892 questions from the TREC 8 and TREC 9QA tracks were used, along with 4K trivia questions.During runtime, the question is first analyzed by theanswer type model, which selects one out of a set of 31types for use by the answer selection model.
Simultane-ously, the question is expanded using local context anal-ysis (Xu and Croft, 1996) with an encyclopedia, and thetop 1000 documents are retrieved by the search engine.From these documents, the top 100 passages are chosenthat 1) maximize the question word match, 2) have thedesired answer type, 3) minimize the dispersion of ques-tion words, and 4) have similar syntactic structures as thequestion.
From these passages, candidate answers are ex-tracted and ranked using the answer selection model.
Thetop n candidate answers are then returned, each with anassociated confidence score.4 Answer ResolutionGiven two answering agents with the same pipeline archi-tecture, there are multiple points in the process at which(intermediate) results can be combined, as illustrated inFigure 2.
More specifically, it is possible for one answer-ing agent to provide input to the other after the questionanalysis, passage retrieval, and answer selection phases.In PIQUANT, the knowledge based agent may accept in-put from the statistical agent after each of these threephases.1 The contributions from the statistical agent aretaken into consideration by the knowledge based answer-ing agent in a phase-dependent fashion.
The rest of thissection details our combination strategies for each phase.4.1 Question-Level CombinationOne of the key tasks of the question analysis componentis to determine the expected answer type, such as PERSONfor ?Who discovered America??
and DATE for ?Whendid World War II end??
This information is taken into ac-count by most existing QA systems when ranking candi-date answers, and can also be used in the passage retrievalprocess to increase the precision of candidate passages.We seek to improve the knowledge-based agent?sperformance in passage retrieval and answer selectionthrough better answer type identification by consultingthe statistical agent?s expected answer type.
This task,however, is complicated by the fact that QA systems em-ploy different sets of answer types, often with differentgranularities and/or with overlapping types.
For instance,while one system may generate ROYALTY for the ques-tion ?Who was the King of France in 1702?
?, anothersystem may produce PERSON as the most specific an-swer type in its repertoire.
This is quite a serious problemfor us as the knowledge based agent uses over 80 answertypes while the statistical agent adopts only 31 categories.In order to distinguish actual answer type discrepan-cies from those due to granularity differences, we firstmanually created a mapping between the two sets of an-swer types.
This mapping specifies, for each answer typeused by the statistical agent, a set of possible correspond-ing types used by the knowledge-based agent.
For exam-ple, the GEOLOGICALOBJ class is mapped to a set of finergrained classes: RIVER, MOUNTAIN, LAKE, and OCEAN.At processing time, the statistical agent?s answer typeis mapped to the knowledge-based agent?s classes (SA-1Although it is possible for the statistical agent to receiveinput from the knowledge based agent as well, we have not pur-sued that option because of implementation issues.QuestionAnalysis 1PassageRetrieval 1AnswerSelection 1passages answersQuestionAnalysis 2PassageRetrieval 2AnswerSelection 2Agent 1 (Knowledge-Based)Agent 2 (Statistical)questiontypeQFrameFigure 2: Answer Resolution Strategiestypes), which are then merged with the answer type(s) se-lected by the knowledge-based agent itself (KBA-types)as follows:1.
If the intersection of KBA-types and SA-types isnon-null, i.e., the two agents produced consistent an-swer types, then the merged set is KBA-types.2.
Otherwise, the two sets of answer types are trulyin disagreement, and the merged set is the union ofKBA-types and SA-types.The merged answer types are then used by theknowledge-based agent in further processing.4.2 Passage-Level CombinationThe passage retrieval component selects, from a large textcorpus, a small number of short passages from which an-swers are identified.
Oftentimes, multiple passages thatanswer a question are retrieved.
Some of these passagesmay be better suited than others for the answer selectionalgorithm employed downstream.
For example, consider?When was Benjamin Disraeli prime minister?
?, whoseanswer can be found in both passages below:1.
Benjamin Disraeli, who had become prime ministerin 1868, was born into Judaism but was baptized aChristian at the age of 12.2.
France had a Jewish prime minister in 1936, Eng-land in 1868, and Spain, of all countries, in 1835,but none of them, Leon Blum, Benjamin Disraeli orJuan Alvarez Mendizabel, were devoutly observant,as Lieberman is.Although the correct answer, 1868, is present in bothpassages, it is substantially easier to identify the answerfrom the first passage, where it is directly stated, thanfrom the second passage, where recognition of parallelconstructs is needed to identify the correct answer.Because of strategic differences in question analysisand passage retrieval, our two answering agents often re-trieve different passages for the same question.
Thus, weperform passage-level combination to make a wider va-riety of passages available to the answer selection com-ponent, as shown in Figure 2.
The potential advantagesare threefold.
First, passages from agent 2 may containanswers absent in passages retrieved by agent 1.
Sec-ond, agent 2 may have retrieved passages better suited forthe downstream answer selection algorithm than those re-trieved by agent 1.
Third, passages from agent 2 may con-tain additional occurrences of the correct answer, whichboosts the system?s confidence in the answer through theredundancy measure.2Our passage-level combination algorithm adds to thepassages extracted by the knowledge-based agent the top-ranked passages from the statistical agent that containcandidate answers of the right type.
More specifically,the statistical agent?s passages are semantically annotatedand the top 10 passages containing at least one candidateof the expected answer type(s) are selected.34.3 Answer-Level CombinationThe answer selection component identifies, from a setof passages, the top n answers for the given question,with their associated confidence scores.
An answer-levelcombination algorithm takes the top answer(s) from theindividual answering agents and determines the overallbest answer(s).
Of our three combination algorithms, thismost closely resembles traditional ensemble methods, asvoting takes place among the end results of individual an-2On the other hand, such redundancy may result in errorcompounding, as discussed in Section 5.3.3We selected the top 10 passages so that the same numberof passages are considered from both answering agents.swering agents to determine the final output of the ensem-ble.We developed two answer-level combination algo-rithms, both utilizing a simple confidence-based votingmechanism, based on the premise that answers selectedby both agents with high confidence are more likely tobe correct than those identified by only one agent.4 Inboth algorithms, named entity normalization is first per-formed on all candidate answers considered.
In the firstalgorithm, only the top answer from each agent is takeninto account.
If the two top answers are equivalent, theanswer is selected with the combined confidence fromboth agents; otherwise, the more confident answer is se-lected.5 In the second algorithm, the top 5 answers fromeach agent are allowed to participate in the voting pro-cess.
Each instance of an answer votes with a weightequal to its confidence value and the weights of equiv-alent answers are again summed.
The answer with thehighest weight, or confidence value, is selected as thesystem?s final answer.
Since in our evaluation, the secondalgorithm uniformly outperforms the first, it is adopted asour answer-level combination algorithm in the rest of thepaper.5 Performance Evaluation5.1 Experimental SetupTo assess the effectiveness of our multi-level answer res-olution algorithm, we devised experiments to evaluate theimpact of the question, passage, and answer-level combi-nation algorithms described in the previous section.The baseline systems are the knowledge-based and sta-tistical agents performing individually against a singlereference corpus.
In addition, our earlier experimentsshowed that when employing a single answer findingstrategy, consulting multiple text corpora yielded betterperformance than using a single corpus.
We thus con-figured a version of our knowledge-based agent to makeuse of three available text corpora,6 the AQUAINT cor-pus (news articles from 1998-2000), the TREC corpus(news articles from 1988-1994),7 and a subset of the En-cyclopedia Britannica.
This multi-source version of theknowledge-based agent will be used in all answer resolu-tion experiments in conjunction with the statistical agent.We configured multiple versions of PIQUANT to eval-uate our question, passage, and answer-level combination4In future work we will be investigating weighted votingschemes based on question features.5The confidence values from both answering agents are nor-malized to be between 0 and 1.6The statistical agent is currently unable to consult multiplecorpora.7Both the AQUAINT and TREC corpora are available fromthe Linguistics Data Consortium, http://www.ldc.org.algorithms individually and cumulatively.
For cumula-tive effects, we 1) combined the algorithms pair-wise,and 2) employed all three algorithms together.
The twotest sets were selected from the TREC 10 and 11 QAtrack questions (Voorhees, 2002; Voorhees, 2003).
Forboth test sets, we eliminated those questions that did nothave known answers in the reference corpus.
Further-more, from the TREC 10 test set, we discarded all defini-tion questions,8 since the knowledge-based agent adoptsa specialized strategy for handling definition questionswhich greatly reduces potential contributions from otheranswering agents.
This results in a TREC 10 test set of313 questions and a TREC 11 test set of 453 questions.5.2 Experimental ResultsWe ran each of the baseline and combined systems on thetwo test sets.
For each run, the system outputs its topanswer and its confidence score for each question.
Allanswers for a run are then sorted in descending order ofthe confidence scores.
Two established TREC QA eval-uation metrics are adopted to assess the results for eachrun as follows:1.
% Correct: Percentage of correct answers.2.
Average Precision: A confidence-weighted scorethat rewards systems with high confidence in cor-rect answers as follows, where N is the number ofquestions:1NN?i=1# correct up to question i/iTable 1 shows our experimental results.
The top sec-tion shows the comparable baseline results from the sta-tistical agent (SA-SS) and the single-source knowledge-based agent (KBA-SS).
It also includes results for themulti-source knowledge-based agent (KBA-MS), whichimprove upon those for its single-source counterpart(KBA-SS).The middle section of the table shows the answerresolution results, including applying the question, pas-sage, and answer-level combination algorithms individu-ally (Q, P, and A, respectively), applying them pair-wise(Q+P, P+A, and Q+A), and employing all three algo-rithms (Q+P+A).
Finally, the last row of the table showsthe relative improvement by comparing the best perform-ing system configuration (highlighted in boldface) withthe better performing single-source, single-strategy base-line system (SA-SS or KBA-SS, in italics).Overall, PIQUANT?s multi-strategy and multi-sourceapproach achieved a 35.0% relative improvement in the8Definition questions were intentionally excluded by thetrack coordinator in the TREC 11 test set.TREC 10 (313) TREC 11 (453)% Corr Avg Prec % Corr Avg PrecSA-SS 36.7% 0.569 32.9% 0.534KBA-SS 39.6% 0.595 32.5% 0.531KBA-MS 43.8% 0.641 38.2% 0.622Q 44.7% 0.647 38.9% 0.632P 49.5% 0.661 40.0% 0.627A 49.5% 0.712 43.5% 0.704Q+P 48.9% 0.656 41.1% 0.640P+A 51.1% 0.711 44.2% 0.686Q+A 49.8% 0.716 43.9% 0.709Q+P+A 50.8% 0.706 44.4% 0.690rel.
improv.
29.0% 20.3% 35.0% 32.8%Table 1: Experimental Resultsnumber of correct answers and a 32.8% improvement inaverage precision on the TREC 11 data set.
Of the com-bined improvement, approximately half was achieved bythe multi-source aspect of PIQUANT, while the other halfwas obtained by PIQUANT?s multi-strategy feature.
Al-though the absolute average precision values are com-parable on both test sets and the absolute percentage ofcorrect answers is lower on the TREC 11 data, the im-provement is greater on TREC 11 in both cases.
Thisis because the TREC 10 questions were taken into ac-count for manual rule refinement in the knowledge-basedagent, resulting in higher baselines on the TREC 10 testset.
We believe that the larger improvement on the previ-ously unseen TREC 11 data is a more reliable estimate ofPIQUANT?s performance on future test sets.We applied an earlier version of our combination algo-rithms, which performed between our current P and P+Aalgorithms, in our submission to the TREC 11 QA track.Using the average precision metric, that version of PI-QUANT was among the top 5 best performing systemsout of 67 runs submitted by 34 groups.5.3 Discussion and AnalysisA cursory examination of the results in Table 1 allowsus to draw two general conclusions about PIQUANT?sperformance.
First, all three combination algorithms ap-plied individually improved upon the baseline using bothevaluation metrics on both test sets.
In addition, overallperformance is generally better the later in the processthe combination occurs, i.e., the answer-level combina-tion algorithm outperformed the passage-level combina-tion algorithm, which in turn outperformed the question-level combination algorithm.
Second, the cumulative im-provement from multiple combination algorithms is ingeneral greater than that from the components.
For in-stance, the Q+A algorithm uniformly outperformed the Qand A algorithms alone.
Note, however, that the Q+P+Aalgorithm achieved the highest performance only on theTREC 11 test set using the % correct metric.
We believeKBATREC 10 (313) TREC 11 (453)+ - + -SA + 185 43 254 58- 24 61 41 100Table 2: Passage Retrieval Analysisthat this is because of compounding errors that occurredduring the multiple combination process.In ensemble methods, the individual components mustmake different mistakes in order for the combined sys-tem to potentially perform better than the component sys-tems (Dietterich, 1997).
We examined the differencesin results between the two answering agents from theirquestion analysis, passage retrieval, and answer selectioncomponents.
We focused our analysis on the potentialgain/loss from incorporating contributions from the sta-tistical agent, and how the potential was realized as actualperformance gain/loss in our end-to-end system.At the question level, we examined those questionsfor which the two agents proposed incompatible answertypes.
On the TREC 10 test set, the statistical agent in-troduced correct answer types in 6 cases and incorrectanswer types in 9 cases.
As a result, in some cases thequestion-level combination algorithm improved systemperformance (comparing A and Q+A) and in others itdegraded performance (comparing P and Q+P).
On theother hand, on the TREC 11 test set, the statistical agentintroduced correct and incorrect answer types in 15 and6 cases, respectively.
As a result, in most cases perfor-mance improved when the question-level combination al-gorithm was invoked.
The difference in question analysisperformance again reflects the fact that TREC 10 ques-tions were used in question analysis rule refinement inthe knowledge-based agent.At the passage level, we examined, for each ques-tion, whether the candidate passages contained the cor-rect answer.
Table 2 shows the distribution of ques-tions for which correct answers were (+) and were not(-) present in the passages for both agents.
The bold-faced cells represent questions for which the statisticalagent retrieved passages with correct answers while theknowledge-based agent did not.
There were 43 and 58such questions in the TREC 10 and TREC 11 test sets, re-spectively, and employing the passage-level combinationalgorithm resulted only in an additional 18 and 8 correctanswers on each test set.
This is because the statisticalagent?s proposes in its 10 passages, on average, 29 candi-date answers, most of which are incorrect, of the propersemantic type per question.
As the downstream answerselection component takes redundancy into account in an-swer ranking, incorrect answers may reinforce one an-other and become top ranked answers.
This suggests thatKBATREC 10 (313) TREC 11 (453)1st 2-5th none 1st 2-5th noneSA 1st 66 22 26 93 21 352-5th 26 9 13 29 19 22none 45 14 92 51 21 162Table 3: Answer Voting Analysisthe relative contributions of our answer selection featuresmay not be optimally tuned for our multi-agent approachto QA.
We plan to investigate this issue in future work.At the answer level, we analyzed each agent?s top 5answers, used in the combination algorithm?s voting pro-cess.
Table 3 shows the distribution of questions forwhich an answer was found in 1st place, in 2nd-5th place,and not found in top 5.
Since we employ a linear vot-ing strategy based on confidence scores, we classify thecells in Table 3 as follows based on the perceived likeli-hood that the correct answers for questions in each cellwins in the voting process.
The boldfaced and underlinedcells contain highly likely candidates, since a correct an-swer was found in 1st place by both agents.9 The bold-faced cells consist of likely candidates, since a 1st placecorrect answer was supported by a 2nd-5th place answer.The italicized and underlined cells contain possible can-didates, while the rest of the cells cannot produce correct1st place answers using our current voting algorithm.
OnTREC 10 data, 194 questions fall into the highly likely,likely, and possible categories, out of which the voting al-gorithm successfully selected 155 correct answers in 1stplace.
On TREC 11 data, 197 correct answers were se-lected out of 248 questions that fall into these categories.These results represent success rates of 79.9% and 79.4%for our answer-level combination algorithm on the twotest sets.6 Related WorkThere has been much work in employing ensemble meth-ods to increase system performance in machine learning.In NLP, such methods have been applied to tasks suchas POS tagging (Brill and Wu, 1998), word sense dis-ambiguation (Pedersen, 2000), parsing (Henderson andBrill, 1999), and machine translation (Frederking andNirenburg, 1994).In question answering, a number of researchers haveinvestigated federated systems for identifying answers toquestions.
For example, (Clarke et al, 2003) and (Lin etal., 2003) employ techniques for utilizing both unstruc-9These cells are not marked as definite because in a smallnumber of cases, the two answers are not equivalent.
For exam-ple, for the TREC 9 question, ?Who is the emperor of Japan?
?,Hirohito, Akihito, and Taisho are all considered correct answersbased on the reference corpus.tured text and structured databases for question answer-ing.
However, the approaches taken by both these sys-tems differ from ours in that they enforce an order be-tween the two strategies by attempting to locate answersin structured databases first for select question types andfalling back to unstructured text when the former fails,while we explore both options in parallel and combinethe results from multiple answering agents.The multi-agent approach to question answering mostsimilar to ours is that by Burger et al (2003).
Theyapplied ensemble methods to combine the 67 runs sub-mitted to the TREC 11 QA track, using an unweightedcentroid method for selecting among the 67 proposed an-swers for each question.
However, their combined sys-tem did not outperform the top scoring system(s).
Fur-thermore, their approach differs from ours in that they fo-cused on combining the end results of a large number ofsystems, while we investigated a tightly-coupled designfor combining two answering agents.7 ConclusionsIn this paper, we introduced a multi-strategy and multi-source approach to question answering that enables com-bination of answering agents adopting different strategiesand consulting multiple knowledge sources.
In partic-ular, we focused on two answering agents, one adopt-ing a knowledge-based approach and one using statisticalmethods.
We discussed our answer resolution componentwhich employs a multi-level combination algorithm thatallows for resolution at the question, passage, and answerlevels.
Best performance using the % correct metric wasachieved by the three-level algorithm that combines af-ter each stage, while highest average precision was ob-tained by a two-level algorithm merging at the questionand answer levels, supporting a tightly-coupled designfor multi-agent question answering.
Our experimentsshowed that our best performing algorithms achieved a35.0% relative improvement in the number of correct an-swers and a 32.8% improvement in average precision ona previously unseen test set.AcknowledgmentsWe would like to thank Dave Ferrucci, Chris Welty, andSalim Roukos for helpful discussions, Diane Litman andthe anonymous reviewers for their comments on an ear-lier draft of this paper.
This work was supported inpart by the Advanced Research and Development Ac-tivity (ARDA)?s Advanced Question Answering for In-telligence (AQUAINT) Program under contract numberMDA904-01-C-0988.ReferencesAdam L. Berger, Vincent Della Pietra, and Stephen DellaPietra.
1996.
A maximum entropy approach to nat-ural language processing.
Computational Linguistics,22(1):39?71.Eric Brill and Jun Wu.
1998.
Classifier combination forimproved lexical disambiguation.
In Proceedings ofthe 36th Annual Meeting of the Association for Com-putational Linguistics, pages 191?195.John D. Burger, Lisa Ferro, Warren Greiff, John Hender-son, Marc Light, and Scott Mardis.
2003.
MITRE?sQanda at TREC-11.
In Proceedings of the EleventhText Retrieval Conference.
To appear.Jennifer Chu-Carroll, David Ferrucci, John Prager, andChristopher Welty.
2003a.
Hybridization in ques-tion answering systems.
In Working Notes of the AAAISpring Symposium on New Directions in Question An-swering, pages 116?121.Jennifer Chu-Carroll, John Prager, Christopher Welty,Krzysztof Czuba, and David Ferrucci.
2003b.
Amulti-strategy and multi-source approach to questionanswering.
In Proceedings of the Eleventh Text Re-trieval Conference.
To appear.Charles Clarke, Gordon Cormack, and Thomas Lynam.2001.
Exploiting redundancy in question answering.In Proceedings of the 24th SIGIR Conference, pages358?365.C.L.A.
Clarke, G.V.
Cormack, G. Kemkes, M. Laszlo,T.R.
Lynam, E.L. Terra, and P.L.
Tilker.
2003.
Statis-tical selection of exact answers.
In Proceedings of theEleventh Text Retrieval Conference.
To appear.Thomas G. Dietterich.
1997.
Machine learning research:Four current directions.
AI Magazine, 18(4):97?136.Robert Frederking and Sergei Nirenburg.
1994.
Threeheads are better than one.
In Proceedings of the FourthConference on Applied Natural Language Processing.John C. Henderson and Eric Brill.
1999.
Exploitingdiversity in natural language processing: Combiningparsers.
In Proceedings of the 4th Conference on Em-pirical Methods in Natural Language Processing.Eduard Hovy, Laurie Gerber, Ulf Hermjakob, MichaelJunk, and Chin-Yew Lin.
2001.
Question answeringin Webclopedia.
In Proceedings of the Ninth Text RE-trieval Conference, pages 655?664.Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, andAdwait Ratnaparkhi.
2001.
Question answering usingmaximum entropy components.
In Proceedings of the2nd Conference of the North American Chapter of theAssociation for Computational Linguistics, pages 33?39.Abraham Ittycheriah.
2001.
Trainable Question Answer-ing Systems.
Ph.D. thesis, Rutgers - The State Univer-sity of New Jersey.Douglas B. Lenat.
1995.
Cyc: A large-scale investmentin knowledge infrastructure.
Communications of theACM, 38(11).Jimmy Lin, Aaron Fernandes, Boris Katz, Gregory Mar-ton, and Stefanie Tellex.
2003.
Extracting an-swers from the web using knowledge annotation andknowledge mining techniques.
In Proceedings of theEleventh Text Retrieval Conference.
To appear.George Miller.
1995.
Wordnet: A lexical database forEnglish.
Communications of the ACM, 38(11).Dan Moldovan, Sanda Harabagiu, Marius Pasca, RadaMihalcea, Roxana Girju, Richard Goodrum, and VasileRus.
2000.
The structure and performance of an open-domain question answering system.
In Proceedings ofthe 39th Annual Meeting of the Association for Com-putational Linguistics, pages 563?570.Ted Pedersen.
2000.
A simple approach to building en-sembles of naive Bayesian classifiers for word sensedisambiguation.
In Proceedings of the 1st Conferenceof the North American Chapter of the Association forComputational Linguistics, pages 63?69.John Prager, Eric Brown, Anni Coden, and DragomirRadev.
2000.
Question-answering by predictive anno-tation.
In Proceedings of the 23rd SIGIR Conference,pages 184?191.John Prager, Dragomir Radev, and Krzysztof Czuba.2001.
Answering what-is questions by virtual anno-tation.
In Proceedings of Human Language Technolo-gies Conference, pages 26?30.Ellen M. Voorhees.
2001.
Overview of the TREC-9question answering track.
In Proceedings of the 9thText Retrieval Conference, pages 71?80.Ellen M. Voorhees.
2002.
Overview of the TREC 2001question answering track.
In Proceedings of the 10thText Retrieval Conference, pages 42?51.Ellen M. Voorhees.
2003.
Overview of the TREC2002 question answering track.
In Proceedings of theEleventh Text Retrieval Conference.
To appear.Jinxi Xu and W. Bruce Croft.
1996.
Query expansionusing local and global document analysis.
In Proceed-ings of the 19th SIGIR Conference, pages 4?11.
