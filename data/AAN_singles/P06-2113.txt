Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 882?889,Sydney, July 2006. c?2006 Association for Computational LinguisticsCombining Statistical and Knowledge-based Spoken LanguageUnderstanding in Conditional ModelsYe-Yi Wang, Alex Acero, Milind MahajanMicrosoft ResearchOne Microsoft WayRedmond, WA 98052, USA{yeyiwang,alexac,milindm}@microsoft.comJohn LeeSpoken Language SystemsMIT CSAILCambridge, MA 02139, USAjsylee@csail.mit.eduAbstractSpoken Language Understanding (SLU)addresses the problem of extracting semanticmeaning conveyed in an utterance.
Thetraditional knowledge-based approach to thisproblem is very expensive -- it requires jointexpertise in natural language processing andspeech recognition, and best practices inlanguage engineering for every new domain.On the other hand, a statistical learningapproach needs a large amount of annotateddata for model training, which is seldomavailable in practical applications outside oflarge research labs.
A generative HMM/CFGcomposite model, which integrates easy-to-obtain domain knowledge into a data-drivenstatistical learning framework, has previouslybeen introduced to reduce data requirement.The major contribution of this paper is theinvestigation of integrating prior knowledgeand statistical learning in a conditional modelframework.
We also study and compareconditional random fields (CRFs) withperceptron learning for SLU.
Experimentalresults show that the conditional modelsachieve more than 20% relative reduction inslot error rate over the HMM/CFG model,which had already achieved an SLU accuracyat the same level as the best results reportedon the ATIS data.1 IntroductionSpoken Language Understanding (SLU)addresses the problem of extracting meaningconveyed in an utterance.
Traditionally, theproblem is solved with a knowledge-basedapproach, which requires joint expertise innatural language processing and speechrecognition, and best practices in languageengineering for every new domain.
In the pastdecade many statistical learning approaches havebeen proposed, most of which exploit generativemodels, as surveyed in (Wang, Deng et al,2005).
While the data-driven approach addressesthe difficulties in knowledge engineering, itrequires a large amount of labeled data for modeltraining, which is seldom available in practicalapplications outside of large research labs.
Toalleviate the problem, a generative HMM/CFGcomposite model has previously been introduced(Wang, Deng et al, 2005).
It integrates aknowledge-based approach into a statisticallearning framework, utilizing prior knowledge tocompensate for the dearth of training data.
In theATIS evaluation (Price, 1990), this modelachieves the same level of understandingaccuracy (5.3% error rate on standard ATISevaluation) as the best system (5.5% error rate),which is a semantic parsing system based on amanually developed grammar.Discriminative training has been widely usedfor acoustic modeling in speech recognition(Bahl, Brown et al, 1986; Juang, Chou et al,1997; Povey and Woodland, 2002).
Most of themethods use the same generative modelframework, exploit the same features, and applydiscriminative training for parameteroptimization.
Along the same lines, we haverecently exploited conditional models by directlyporting the HMM/CFG model to HiddenConditional Random Fields (HCRFs)(Gunawardana, Mahajan et al, 2005), but failedto obtain any improvement.
This is mainly due tothe vast parameter space, with the parameterssettling at local optima.
We then simplified theoriginal model structure by removing the hiddenvariables, and introduced a number of importantoverlapping and non-homogeneous features.
Theresulting Conditional Random Fields (CRFs)(Lafferty, McCallum et al, 2001) yielded a 21%relative improvement in SLU accuracy.
We alsoapplied a much simpler perceptron learningalgorithm on the conditional model and observedimproved SLU accuracy as well.In this paper, we will first introduce thegenerative HMM/CFG composite model, thendiscuss the problem of directly porting the modelto HCRFs, and finally introduce the CRFs and882the features that obtain the best SLU result onATIS test data.
We compare the CRF andperceptron training performances on the task.2 Generative ModelsThe HMM/CFG composite model (Wang, Denget al, 2005) adopts a pattern recognitionapproach to SLU.
Given a word sequence W , anSLU component needs to find the semanticrepresentation of the meaning M  that has themaximum a posteriori probability ( )Pr |M W :( )( ) ( )?
arg max Pr |arg max Pr | PrMMM M WW M M== ?The composite model integrates domainknowledge by setting the topology of the priormodel, ( )Pr ,M according to the domainsemantics; and by using PCFG rules as part ofthe lexicalization model ( )Pr |W M .The domain semantics define an application?ssemantic structure with semantic frames.Figure 1 shows a simplified example of threesemantic frames in the ATIS domain.
The twoframes with the ?toplevel?
attribute are alsoknown as commands.
The ?filler?
attribute of aslot specifies the semantic object that can fill it.Each slot may be associated with a CFG rule,and the filler semantic object must beinstantiated by a word string that is covered bythat rule.
For example, the string ?Seattle?
iscovered by the ?City?
rule in a CFG.
It cantherefore fill the ACity (ArrivalCity) or theDCity (DepartureCity) slot, and instantiate aFlight frame.
This frame can then fill the Flightslot of a ShowFlight frame.
Figure 2 shows asemantic representation according to theseframes.< frame name=?ShowFlight?
toplevel=?1?><slot name=?Flight?
filler=?Flight?/>< /frame>< frame name=?GroundTrans?
toplevel=?1?>< slot name=?City?
filler=?City?/>< /frame>< frame name=?Flight?><slot name=?DCity?
filler=?City?/>< slot name=?ACity?
filler=?City?/>< /frame>Figure 1.
Simplified domain semantics for the ATISdomain.The semantic prior model comprises theHMM topology and state transition probabilities.The topology is determined by the domainsemantics, and the transition probabilities can beestimated from training data.
Figure 3 shows thetopology of the underlying states in the statisticalmodel for the semantic frames in Figure 1.
Ontop is the transition network for the two top-levelcommands.
At the bottom is a zoomed-in viewfor the ?Flight?
sub-network.
State 1 and state 4are called precommands.
State 3 and state 6 arecalled postcommands.
States 2, 5, 8 and 9represent slots.
A slot is actually a three-statesequence ?
the slot state is preceded by apreamble state and followed by a postamblestate, both represented by black circles.
Theyprovide contextual clues for the slot?s identity.<ShowFlight>< Flight>< DCity filler=?City?>Seattle< /DCity><ACity filler=?City?>Boston< /ACity>< /Flight>< /ShowFlight>Figure 2.
The semantic representation for ?Show methe flights departing from Seattle arriving at Boston?is an instantiation of the semantic frames in Figure 1.Figure 3.
The HMM/CFG model?s state topology, asdetermined by the semantic frames in Figure 1.The lexicalization model, ( )Pr |W M , depictsthe process of sentence generation from thetopology by estimating the distribution of wordsemitted by a state.
It uses state-dependent n-grams to model the precommands,postcommands, preambles and postambles, anduses knowledge-based CFG rules to model theslot fillers.
These rules help compensate for thedearth of domain-specific data.
In the remainderof this paper we will say a string is ?covered by aCFG non-terminal (NT)?, or equivalently, is?CFG-covered for s?
if the string can be parsedby the CFG rule corresponding to the slot s.Given the semantic representation in Figure 2,the state sequence through the model topology in883Figure 3 is deterministic, as shown in Figure 4.However, the words are not aligned to the statesin the shaded boxes.
The parameters in theircorresponding n-gram models can be estimatedwith an EM algorithm that treats the alignmentsas hidden variables.Figure 4.
Word/state alignments.
The segmentationof the word sequences in the shaded region is hidden.The HMM/CFG composite model wasevaluated in the ATIS domain (Price, 1990).
Themodel was trained with ATIS3 category Atraining data (~1700 annotated sentences) andtested with the 1993 ATIS3 category A testsentences (470 sentences with 1702 referenceslots).
The slot insertion-deletion-substitutionerror rate (SER) of the test set is 5.0%, leading toa 5.3% semantic error rate in the standard end-to-end ATIS evaluation, which is slightly betterthan the best manually developed system (5.5%).Moreover, a steep drop in the error rate isobserved after training with only the first twohundred sentences.
This demonstrates that theinclusion of prior knowledge in the statisticalmodel helps alleviate the data sparsenessproblem.3 Conditional ModelsWe investigated the application of conditionalmodels to SLU.
The problem is formulated asassigning a label l  to each element in anobservation .o  Here, o  consists of a wordsequence 1o?
and a list of CFG non-terminals(NT) that cover its subsequences, as illustrated inFigure 5.
The task is to label ?two?
as the ?Num-of-tickets?
slot of the ?ShowFlight?
command,and ?Washington D.C.?
as the ArrivalCity slotfor the same command.
To do so, the model mustbe able to resolve several kinds of ambiguities:1.
Filler/non-filler ambiguity, e.g., ?two?
caneither fill a Num-of-tickets slot, or itshomonym ?to?
can form part of the preambleof an ArrivalCity slot.2.
CFG ambiguity, e.g., ?Washington?
can beCFG-covered as either City or State.3.
Segmentation ambiguity, e.g., [Washington][D.C.] vs. [Washington D.C.].4.
Semantic label ambiguity, e.g., ?WashingtonD.C.?
can fill either an ArrivalCity orDepartureCity slot.Figure 5.
The observation includes a word sequenceand the subsequences covered by CFG non-terminals.3.1 CRFs and HCRFsConditional Random Fields (CRFs) (Lafferty,McCallum et al, 2001) are undirectedconditional graphical models that assign theconditional probability of a state (label) sequence1s?
with respect to a vector of features 1 1( )f os?
?, .They are of the following form:( )1 11( ) exp ( )( )o f oop s sz?
??
?
?| ; = ?
, .
;  (1)Here ( )11( ) exp ( )sz s???
?
; = ?
,?o f o  normalizesthe distribution over all possible state sequences.The parameter vector ?
is trained conditionally(discriminatively).
If we assume that 1s?
is aMarkov chain given o  and the feature functionsonly depend on two adjacent states, then1( 1) ( )1( )1   = exp ( )( )t tk kk tp sf s s tz?????
?=| ;?
?, , ,?
?
; ?
??
?ooo(2)In some cases, it may be natural to exploitfeatures on variables that are not directlyobserved.
For example, a feature for the Flightpreamble may be defined in terms of an observedword and an unobserved state in the shadedregion in Figure 4:( 1) ( )FlightInit,flights( )( )1 if =FlightInit  = flights;=0 otherwiseoot tt tf s s ts?
, , ,?
???
(3)In this case, the state sequence 1s?
is onlypartially observed in the meaning representation5 8: ( ) "DCity" ( ) "ACity"M M s M s= ?
= for thewords ?Seattle?
and ?Boston?.
The states for theremaining words are hidden.
Let ( )M?
representthe set of all state sequences that satisfy theconstraints imposed by .M  To obtain theconditional probability of ,M we need to sumover all possible labels for the hidden states:8841( 1) ( )1( )( )1   exp ( )( )t tk kk ts Mp Mf s s tz ??????=?
?| ; =?
?, , ,?
?
; ?
??
?
?oooCRFs with features dependent on hidden statevariables are called Hidden Conditional RandomFields (HCRFs).
They have been applied to taskssuch as phonetic classification (Gunawardana,Mahajan et al, 2005) and object recognition(Quattoni, Collins et al, 2004).3.2 Conditional Model TrainingWe train CRFs and HCRFs with gradient-basedoptimization algorithms that maximize the logposterior.
The gradient of the objective functionis( ) ( )( ) ( )1111( ) ( )( )P P sP P sL ss??????
?
?, | ,|?
??
= , ;?
??
??
, ;?
?l o l oo oE f oE f owhich is the difference between the conditionalexpectation of the feature vector given theobservation sequence and label sequence, and theconditional expectation given the observationsequence alone.
With the Markov assumption inEq.
(2), these expectations can be computedusing a forward-backward-like dynamicprogramming algorithm.
For CRFs, whosefeatures do not depend on hidden statesequences, the first expectation is simply thefeature counts given the observation and labelsequences.
In this work, we applied stochasticgradient descent (SGD) (Kushner and Yin, 1997)for parameter optimization.
In our experimentson several different tasks, it is faster than L-BFGS (Nocedal and Wright, 1999), a quasi-Newton optimization algorithm.3.3 CRFs and Perceptron LearningPerceptron training for conditional models(Collins, 2002) is an approximation to the SGDalgorithm, using feature counts from the Viterbilabel sequence in lieu of expected feature counts.It eliminates the need of a forward-backwardalgorithm to collect the expected counts, hencegreatly speeds up model training.
This algorithmcan be viewed as using the minimum margin of atraining example (i.e., the difference in the logconditional probability of the reference labelsequence and the Viterbi label sequence) as theobjective function instead of the conditionalprobability:( ) ( ) ( )ll o l o'' log | ; max log ' | ;L P P?
?
?= ?Here again, o  is the observation and l  is itsreference label sequence.
In perceptron training,the parameter updating stops when the Viterbilabel sequence is the same as the reference labelsequence.
In contrast, the optimization based onthe log posterior probability objective functionkeeps pulling probability mass from all incorrectlabel sequences to the reference label sequenceuntil convergence.In both perceptron and CRF training, weaverage the parameters over training iterations(Collins, 2002).4 Porting HMM/CFG Model to HCRFsIn our first experiment, we would like to exploitthe discriminative training capability of aconditional model without changing  theHMM/CFG model?s topology and feature set.Since the state sequence is only partially labeled,an HCRF is used to model the conditionaldistribution of the labels.4.1 FeaturesWe used the same state topology and features asthose in the HMM/CFG composite model.
Thefollowing indicator features are included:Command prior features capture the a priorilikelihood of different top-level commands:( 1) ( )( )( )1 if =0 C( )= , CommandSet0 otherwiseoPR t ttcf s s tt s cc?
, , ,?
?
= ?
??
?Here C(s) stands for the name of the commandthat corresponds to the transition networkcontaining state s.State Transition features capture the likelihoodof transition from one state to another:( 1) ( )( 1) ( ) 1 21 2,1 21 if( ) ,0 otherwisewhere  is a legal transition according to thestate topology.ot tTR t ts ss s s sf s s ts s??
?
= , =, , , = ??
?Unigram and Bigram features capture thelikelihoods of words emitted by a state:885( )( 1) ( )1( 1) ( )1( 1) ( ) 11 2,, ,1 21 if( ) ,0 otherwise( )1 if= ,0 otherwiseoooo ot tUG t tBG t tt t t ts ws w ws s wf s s tf s s ts s s s w w?????
??
= ?
=, , , = ?
?, , ,?
= ?
= ?
= ?
=??
( ) 1 2    | isFiller ; , TrainingDatas s w w w?
?
?
?The condition 1isFiller( )s  restricts 1s  to be a slotstate and not a pre- or postamble state.4.2 ExperimentsThe model is trained with SGD with theparameters initialized in two ways.
The flat startinitialization sets all parameters to 0.
Thegenerative model initialization uses theparameters trained by the HMM/CFG model.Figure 6 shows the test set slot error rates(SER) at different training iterations.
With theflat start initialization (top curve), the error ratenever comes close to the 5% baseline error rateof the HMM/CFG model.
With the generativemodel initialization, the error rate is reduced to4.8% at the second iteration, but the modelquickly gets over-trained afterwards.051015202530350 20 40 60 80 100 120Figure 6.
Test set slot error rates (in %) at differenttraining iterations.
The top curve is for the flat startinitialization, the bottom for the generative modelinitialization.The failure of the direct porting of thegenerative model to the conditional model can beattributed to the following reasons:?
The conditional log-likelihood function isno longer a convex function due to thesummation over hidden variables.
Thismakes the model highly likely to settle ona local optimum.
The fact that the flat startinitialization failed to achieve the accuracyof the generative model initialization is aclear indication of the problem.?
In order to account for words in the testdata, the n-grams in the generative modelare properly smoothed with back-offs tothe uniform distribution over thevocabulary.
This results in a huge numberof parameters, many of which cannot beestimated reliably in the conditionalmodel, given that model regularization isnot as well studied as in n-grams.?
The hidden variables make parameterestimation less reliable, given only a smallamount of training data.5 CRFs for SLUAn important lesson we have learned from theprevious experiment is that we should not thinkgeneratively when applying conditional models.While it is important to find cues that helpidentify the slots, there is no need to exhaustivelymodel the generation of every word in asentence.
Hence, the distinctions between pre-and postcommands, and pre- and postambles areno longer necessary.
Every word that appearsbetween two slots is labeled as the preamble stateof the second slot, as illustrated in Figure 7.
Thislabeling scheme effectively removes the hiddenvariables and simplifies the model to a CRF.
Itnot only expedites model training, but alsoprevents parameters from settling at a localoptimum, because the log conditional probabilityis now a convex function.Figure 7.
Once the slots are marked in thesimplified model topology, the state sequence is fullymarked, leaving no hidden variables and resulting in aCRF.
Here, PAC stands for ?preamble for arrivalcity,?
and PDC for ?preamble for departure city.
?The command prior and state transitionfeatures (with fewer states) are the same as in theHCRF model.
For unigrams and bigrams, onlythose that occur in front of a CFG-covered stringare considered.
If the string is CFG-covered forslot s, then the unigram and bigram features forthe preamble state of s are included.
Suppose thewords ?that departs?
occur at positions1 and t t?
in front of the word ?Seattle,?
whichis CFG-covered by the non-terminal City.
SinceCity can fill a DepartureCity or ArrivalCity slot,the four following features are introduced:886( 1) ( ) ( 1) ( )1 1PDC,that PAC,that( ) ( ) 1o oUG t t UG t tf s s t f s s t?
??
?, , , = , , , =And( 1) ( )1( 1) ( )1PDC,that,departsPAC,that,departs( )( ) 1ooBG t tBG t tf s s tf s s t???
?, , , =, , , =Formally,( )( 1) ( )1( 1) ( )1( 1) ( ) 11 2,, ,1 21 if( ) ,0 otherwise( )1 if= ,0 otherwiseoooo ot tUG t tBG t tt t t ts ws w ws s wf s s tf s s ts s s w w?????
??
= ?
=, , , = ?
?, , ,?
= = ?
= ?
=??
( )1 2 1 2| isFiller ;, | in  the training data,   andappears in front of sequence that is CFG-coveredfor .s sw w w w w ws?
?
?5.1 Additional FeaturesOne advantage of CRFs over generative modelsis the ease with which overlapping features canbe incorporated.
In this section, we describethree additional feature sets.The first set addresses a side effect of notmodeling the generation of every word in asentence.
Suppose a preamble state has neveroccurred in a position that is confusable with aslot state s, and a word that is CFG-covered for shas never occurred as part of the preamble statein the training data.
Then, the unigram feature ofthe word for that preamble state has weight 0,and there is thus no penalty for mislabeling theword as the preamble.
This is one of the mostcommon errors observed in the development set.The chunk coverage for preamble words featureintroduced to model the likelihood of a CFG-covered word being labeled as a preamble:( 1) ( )( ) ( ),( )1 if  C( ) covers( , )  isPre( )0 otherwiset tCCt ttc NTf s s ts c NT s?????
?, , ,= ?
?=oowhere isPre( )s  indicates that s is a preamblestate.Often, the identity of a slot depends on thepreambles of the previous slot.
For example, ?attwo PM?
is a DepartureTime in ?flight fromSeattle to Boston at two PM?, but it is anArrivalTime in ?flight departing from Seattlearriving in Boston at two PM.?
In both cases, theprevious slot is ArrivalCity, so the statetransition features are not helpful fordisambiguation.
The identity of the time slotdepends not on the ArrivalCity slot, but on itspreamble.
Our second feature set, previous-slotcontext, introduces this dependency to the model:( 1) ( )( 1) ( )1 2 11 1 2, ,1 2( )1 if ( , , 1)=  isFiller( )  Slot( ) Slot( )0 otherwisePC t tt tws sf s s ts s s s w s ts s s?
?, , ,?
= ?
= ?
??
??
?
?
???
?ooHere Slot( )s  stands for the slot associated withthe state ,s  which can be a filler state or apreamble state, as shown in Figure 7.1( , , 1)os t?
?
is the set of k words (where k is anadjustable window size) in front of the longestsequence that ends at position 1t ?
and that isCFG-covered by 1Slot( )s .The third feature set is intended to penalizeerroneous segmentation, such as segmenting?Washington D.C.?
into two separate City slots.The chunk coverage for slot boundary feature isactivated when a slot boundary is covered by aCFG non-terminal NT, i.e., when words in twoconsecutive slots (?Washington?
and ?D.C.?)
canalso be covered by one single slot:( 1) ( )( )1( 1) ( )( 1) ( ),( )if  C( ) covers( , )1isFiller( )  isFiller( )0 otherwiset tSBt ttt tt tc NTf s s ts c NTs ss s????????????
?, , ,= ??
?=?
?ooThis feature set shares its weights with thechunk coverage features for preamble words,and does not introduce any new parameters.Features # of Param.
SERCommand Prior 6+State Transition +1377 18.68%+Unigrams +14433 7.29%+Bigrams +58191 7.23%+Chunk Cov Preamble Word +156 6.87%+Previous-Slot Context +290 5.46%+Chunk Cov Slot Boundaries +0 3.94%Table 1.
Number of additional parameters and theslot error rate after each new feature set is introduced.5.2 ExperimentsSince the objective function is convex, theoptimization algorithm does not make anysignificant difference on SLU accuracy.
We887trained the model with SGD.
Other optimizationalgorithm like Stochastic Meta-Decent(Vishwanathan, Schraudolph et al, 2006) can beused to speed up the convergence.
The trainingstopping criterion is cross-validated with thedevelopment set.Table 1 shows the number of new parametersand the slot error rate (SER) on the test data,after each new feature set is introduced.
The newfeatures improve the prediction of slot identitiesand reduce the SER by 21%, relative to thegenerative HMM/CFG composite model.The figures below show in detail the impact ofthe n-gram, previous-slot context and chunkcoverage features.
The chunk coverage featurehas three settings: 0 stands for no chunkcoverage features; 1 for chunk coverage featuresfor preamble words only; and 2 for both wordsand slot boundaries.Figure 8 shows the impact of the order of n-gram features.
Zero-order means no lexicalfeatures for preamble states are included.
As thefigure illustrates, the inclusion of CFG rules forslot filler states and domain-specific knowledgeabout command priors and slot transitions havealready produced a reasonable SER under 15%.Unigram features for preamble states cut theerror by more than 50%, while the impact ofbigram features is not consistent -- it yields asmall positive or negative difference dependingon other experimental parameter settings.0%2%4%6%8%10%12%14%16%0 1 2Ngram OrderSlot Error RateChunkCoverage=0ChunkCoverage=1ChunkCoverage=2Figure 8.
Effects of the order of n-grams on SER.The window size for the previous-slot context featuresis 2.Figure 9 shows the impact of the CFG chunkcoverage feature.
Coverage for both preamblewords and slot boundaries help improve the SLUaccuracy.Figure 10 shows the impact of the windowsize for the previous-slot context feature.
Here, 0means that the previous-slot context feature isnot used.
When the window size is k, the k wordsin front of the longest previous CFG-coveredword sequence are included as the previous-slotunigram context features.
As the figureillustrates, this feature significantly reduces SER,while the window size does not make anysignificant difference.0%2%4%6%8%10%12%14%16%0 1 2Chunk CoverageSlot Error Raten=0n=1n=2Figure 9.
Effects of the chunk coverage feature.
Thewindow size for the previous-slot context feature is 2.The three lines correspond to different n-gram orders,where 0-gram indicates that no preamble lexicalfeatures are used.It is important to note that overlappingfeatures like ,  and CC SB PCf f f  could not be easilyincorporated into a generative model.0%2%4%6%8%10%12%0 1 2Window SizeSlot Error Rate n=0n=1n=2Figure 10.
Effects of the window size of theprevious-slot context feature.
The three lines representdifferent orders of n-grams (0, 1, and 2).
Chunkcoverage features for both preamble words and slotboundaries are used.5.3 CRFs vs. PerceptronsTable 2 compares the perceptron and CRFtraining algorithms, using chunk coveragefeatures for both preamble words and slotboundaries, with which the best accuracy results888are achieved.
Both improve upon the 5%baseline SER from the generative HMM/CFGmodel.
CRF training outperforms the perceptronin most settings, except for the one with unigramfeatures for preamble states and with windowsize 1 -- the model with the fewest parameters.One possible explanation is as follows.
Theobjective function in CRFs is a convex function,and so SGD can find the single global optimumfor it.
In contrast, the objective function for theperceptron, which is the difference between twoconvex functions, is not convex.
The gradientascent approach in perceptron training is hencemore likely to settle on a local optimum as themodel becomes more complicated.PSWSize=1 PSWSize=2Perceptron CRFs Perceptron CRFsn=1 3.76% 4.11% 4.23% 3.94%n=2 4.76% 4.41% 4.58% 3.94%Table 2.
Perceptron vs. CRF training.
Chunkcoverage features are used for both preamble wordsand slot boundaries.
PSWSize stands for the windowsize of the previous-slot context feature.
N is the orderof the n-gram features.The biggest advantage of perceptron learningis its speed.
It directly counts the occurrence offeatures given an observation and its referencelabel sequence and Viterbi label sequence, withno need to collect expected feature counts with aforward-backward-like algorithm.
Not only iseach iteration faster, but fewer iterations arerequired, when using SLU accuracy on a cross-validation set as the stopping criterion.
Overall,perceptron training is 5 to 8 times faster thanCRF training.6 ConclusionsThis paper has introduced a conditional modelframework that integrates statistical learningwith a knowledge-based approach to SLU.
Wehave shown that a conditional model reducesSLU slot error rate by more than 20% over thegenerative HMM/CFG composite model.
Theimprovement was mostly due to the introductionof new overlapping features into the model.
Wehave also discussed our experience in directlyporting a generative model to a conditionalmodel, and demonstrated that it may not bebeneficial at all if we still think generatively inconditional modeling; more specifically,replicating the feature set of a generative modelin a conditional model may not help much.
Thekey benefit of conditional models is the ease withwhich they can incorporate overlapping and non-homogeneous features.
This is consistent withthe finding in the application of conditionalmodels for POS tagging (Lafferty, McCallum etal., 2001).
The paper also compares differenttraining algorithms for conditional models.
Inmost cases, CRF training is more accurate,however, perceptron training is much faster.ReferencesBahl, L., P. Brown, et al 1986.
Maximum mutualinformation estimation of hidden Markov modelparameters for speech recognition.
IEEEInternational Conference on Acoustics, Speech,and Signal Processing.Collins, M. 2002.
Discriminative Training Methodsfor Hidden Markov Models: Theory andExperiments with Perceptron Algorithms.
EMNLP,Philadelphia, PA.Gunawardana, A., M. Mahajan, et al 2005.
Hiddenconditional random fields for phone classification.Eurospeech, Lisbon, Portugal.Juang, B.-H., W. Chou, et al 1997.
"Minimumclassification error rate methods for speechrecognition."
IEEE Transactions on Speech andAudio Processing 5(3): 257-265.Kushner, H. J. and G. G. Yin.
1997.
Stochasticapproximation algorithms and applications,Springer-Verlag.Lafferty, J., A. McCallum, et al 2001.
Conditionalrandom fields: probabilistic models for segmentingand labeling sequence data.
ICML.Nocedal, J. and S. J. Wright.
1999.
Numericaloptimization, Springer-Verlag.Povey, D. and P. C. Woodland.
2002.
Minimumphone error and I-smoothing for improveddiscriminative training.
IEEE InternationalConference on Acoustics, Speech, and SignalProcessing.Price, P. 1990.
Evaluation of spoken language system:the ATIS domain.
DARPA Speech and NaturalLanguage Workshop, Hidden Valley, PA.Quattoni, A., M. Collins and T. Darrell.
2004.Conditional Random Fields for ObjectRecognition.
NIPS.Vishwanathan, S. V. N., N. N. Schraudolph, et al2006.
Accelerated Training of conditional randomfields with stochastic meta-descent.
The LearningWorkshop, Snowbird, Utah.Wang, Y.-Y., L. Deng, et al 2005.
"Spoken languageunderstanding --- an introduction to the statisticalframework."
IEEE Signal Processing Magazine22(5): 16-31.889
