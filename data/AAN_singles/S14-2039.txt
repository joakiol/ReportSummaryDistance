Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 241?246,Dublin, Ireland, August 23-24, 2014.DLS@CU: Sentence Similarity from Word AlignmentMd Arafat Sultan?, Steven Bethard?, Tamara Sumner?
?Institute of Cognitive Science and Department of Computer ScienceUniversity of Colorado Boulder?Department of Computer and Information SciencesUniversity of Alabama at Birminghamarafat.sultan@colorado.edu, bethard@cis.uab.edu, sumner@colorado.eduAbstractWe present an algorithm for computingthe semantic similarity between two sen-tences.
It adopts the hypothesis that se-mantic similarity is a monotonically in-creasing function of the degree to which(1) the two sentences contain similar se-mantic units, and (2) such units occur insimilar semantic contexts.
With a simplis-tic operationalization of the notion of se-mantic units with individual words, we ex-perimentally show that this hypothesis canlead to state-of-the-art results for sentence-level semantic similarity.
At the Sem-Eval 2014 STS task (task 10), our systemdemonstrated the best performance (mea-sured by correlation with human annota-tions) among 38 system runs.1 IntroductionSemantic textual similarity (STS), in the contextof short text fragments, has drawn considerableattention in recent times.
Its application spans amultitude of areas, including natural language pro-cessing, information retrieval and digital learning.Examples of tasks that benefit from STS includetext summarization, machine translation, questionanswering, short answer scoring, and so on.The annual series of SemEval STS tasks (Agirreet al., 2012; Agirre et al., 2013; Agirre et al., 2014)is an important platform where STS systems areevaluated on common data and evaluation criteria.In this article, we describe an STS system whichparticipated and outperformed all other systems atSemEval 2014.The algorithm is a straightforward applicationof the monolingual word aligner presented in (Sul-This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/tan et al., 2014).
This aligner aligns related wordsin two sentences based on the following propertiesof the words:1.
They are semantically similar.2.
They occur in similar semantic contexts inthe respective sentences.The output of the word aligner for a sentencepair can be used to predict the pair?s semantic sim-ilarity by taking the proportion of their alignedcontent words.
Intuitively, the more semanticcomponents in the sentences we can meaningfullyalign, the higher their semantic similarity shouldbe.
In experiments on STS 2013 data reportedby Sultan et al.
(2014), this approach was foundhighly effective.
We also adopt this hypothesis ofsemantic compositionality for STS 2014.We implement an STS algorithm that is onlyslightly different from the algorithm in (Sultan etal., 2014).
The approach remains equally success-ful on STS 2014 data.2 BackgroundWe focus on two relevant topics in this section:the state of the art of STS research, and the wordaligner presented in (Sultan et al., 2014).2.1 Semantic Textual SimilaritySince the inception of textual similarity researchfor short text, perhaps with the studies reportedby Mihalcea et al.
(2006) and Li et al.
(2006),the topic has spawned significant research inter-est.
The majority of systems have been reportedas part of the SemEval 2012 and *SEM 2013 STStasks (Agirre et al., 2012; Agirre et al., 2013).Here we confine our discussion to systems thatparticipated in these tasks.With designated training data for several testsets, supervised systems were the most successfulin STS 2012 (B?ar et al., 2012;?Sari?c et al., 2012;241Jimenez et al., 2012).
Such systems typically ap-ply a regression algorithm on a large number ofSTS features (e.g., string similarity, syntactic sim-ilarity and word or phrase-level semantic similar-ity) to generate a final similarity score.
This ap-proach continued to do well in 2013 (Han et al.,2013; Wu et al., 2013; Shareghi and Bergler, 2013)even without domain-specific training data, but thebest results were demonstrated by an unsupervisedsystem (Han et al., 2013).
This has important im-plications for STS since extraction of each featureadds to the latency of a supervised system.
STSsystems are typically important in the context ofa larger system rather than on their own, so highlatency is an obvious drawback for such systems.We present an STS system that has simplicity,high accuracy and speed as its design goals, canbe deployed without any supervision, operates ina linguistically principled manner with purely se-mantic sentence properties, and was the top sys-tem at SemEval STS 2014.2.2 The Sultan et al.
(2014) AlignerThe word aligner presented in (Sultan et al., 2014)has been used unchanged in this work and plays acentral role in our STS algorithm.
We give only anoverview here; for the full details, see the originalarticle.We will denote the sentences being aligned (andare subsequently input to the STS algorithm) asS(1)and S(2).
We describe only content wordalignment here; stop words are not used in our STScomputation.The aligner first identifies word pairs w(1)i?S(1)and w(2)j?
S(2)such that:1. w(1)iand w(2)jhave non-zero semantic simi-larity, simWij.
The calculation of simWijisdescribed in Section 2.2.1.2.
The semantic contexts of w(1)iand w(2)jhavesome similarity, simCij.
We define the se-mantic context of a word w in a sentenceS as a set of words in S, and the seman-tic context of the word pair (w(1)i, w(2)j), de-noted by contextij, as the Cartesian productof the context of w(1)iin S(1)and the con-text of w(2)jin S(2).
We define several typesof context (i.e., several selections of words)and describe the corresponding calculationsof simCijin Section 2.2.2.3.
There are no competing pairs scoring higherAlignidenticalwordsequencesAlignnamedentitiesAligncontentwordsusingdepen-denciesAligncontentwordsusing sur-roundingwordsFigure 1: The alignment pipeline.than (w(1)i, w(2)j) under f(simW, simC) =0.9 ?
simW+ 0.1 ?
simC.
That is,there are no pairs (w(1)k, w(2)j) such thatf(simWkj, simCkj) > f(simWij, simCij),and there are no pairs (w(1)i, w(2)l) such thatf(simWil, simCil) > f(simWij, simCij).The weights 0.9 and 0.1 were derived empiri-cally via a grid search in the range [0, 1] (witha step size of 0.1) to maximize alignment per-formance on the training set of the (Brockett,2007) alignment corpus.
This set contains800 human-aligned sentence pairs collectedfrom a textual entailment corpus (Bar-Haimet al., 2006).The aligner then performs one-to-one word align-ments in decreasing order of the f value.This alignment process is applied in four stepsas shown in Figure 1; each step applies the aboveprocess to a particular type of context: identi-cal words, dependencies and surrounding contentwords.
Additionally, named entities are aligned ina separate step (details in Section 2.2.2).Words that are aligned by an earlier module ofthe pipeline are not allowed to be re-aligned bydownstream modules.2.2.1 Word SimilarityWord similarity (simW) is computed as follows:1.
If the two words or their lemmas are identi-cal, then simW= 1.2.
If the two words are present as a pairin the lexical XXXL corpus of the Para-phrase Database1(PPDB) (Ganitkevitch etal., 2013), then simW= 0.9.2For thisstep, PPDB was augmented with lemmatizedforms of the already existing word pairs.31PPDB is a large database of lexical, phrasal and syntacticparaphrases.2Again, the value 0.9 was derived empirically via a gridsearch in [0, 1] (step size = 0.1) to maximize alignment per-formance on the (Brockett, 2007) training data.3The Python NLTK WordNetLemmatizer was used tolemmatize the original PPDB words.2423.
For any other word pair, simW= 0.2.2.2 Contextual SimilarityContextual similarity (simC) for a word pair(w(1)i, w(2)j) is computed as the sum of the wordsimilarities for each pair of words in the context of(w(1)i, w(2)j).
That is:simCij=?
(w(1)k,w(2)l) ?
contextijsimWklEach of the stages in Figure 1 employs a specifictype of context.Identical Word Sequences.
Contextual sim-ilarity for identical word sequences (a word se-quence W which is present in both S(1)and S(2)and contains at least one content word) defines thecontext by pairing up each word in the instance ofW in S(1)with its occurrence in the instance ofW in S(2).
All such sequences with length ?
2are aligned; longer sequences are aligned beforeshorter ones.
This simple step was found to be ofvery high precision in (Sultan et al., 2014) and re-duces the overall computational cost of alignment.Named Entities.
Named entities are a specialcase in the alignment pipeline.
Even though thecontext for a named entity is defined in the sameway as it is defined for any other content word(as described below), named entities are alignedin a separate step before other content words be-cause they have special properties such as corefer-ring mentions of different lengths (e.g.
Smith andJohn Smith, BBC and British Broadcasting Cor-poration).
The head word of the named entity isused in dependency calculations.Dependencies.
Dependency-based contex-tual similarity defines the context for the pair(w(1)i, w(2)j) using the syntactic dependencies ofw(1)iand w(2)j.
The context is the set of all wordpairs (w(1)k, w(2)l) such that:?
w(1)kis a dependency of w(1)i,?
w(2)lis a dependency of w(2)j,?
w(1)iand w(2)jhave the same lexical category,?
w(1)kand w(2)lhave the same lexical category,and,?
The two dependencies are either identical orsemantically ?equivalent?
according to theequivalence table provided by Sultan et al.S(1): He wrote a book .nsubjdobjdetS(2): I read the book he wrote .nsubjdobjdetrcmodnsubjFigure 2: Example of dependency equivalence.(2014).
We explain semantic equivalence ofdependencies using an example below.Equivalence of Dependency Structures.
Con-sider S(1)and S(2)in Figure 2.
Note that w(1)2=w(2)6= ?wrote?
and w(1)4= w(2)4= ?book?
inthis pair.
Now, each of the two following typeddependencies: dobj(w(1)2, w(1)4) in S(1)and rc-mod(w(2)4, w(2)6) in S(2), represents the relation?thing that was written?
between the verb ?wrote?and its argument ?book?.
Thus, to summarize,an instance of contextual evidence for a possiblealignment between the pair (w(1)2, w(2)6) (?wrote?
)lies in the pair (w(1)4, w(2)4) (?book?)
and the equiv-alence of the two dependency types dobj and rc-mod.The equivalence table of Sultan et al.
(2014) isa list of all such possible equivalences among dif-ferent dependency types (given that w(1)ihas thesame lexical category as w(2)jand w(1)khas thesame lexical category as w(2)l).If there are no word pairs with identical orequivalent dependencies as defined above, i.e.
ifsimCij= 0, then w(1)iand w(2)jwill not bealigned by this module.Surrounding Content Words.
Surrounding-word-based contextual similarity defines the con-text of a word in a sentence as a fixed window of3 words to its left and 3 words to its right.
Onlycontent words in the window are considered.
(Asexplained in the beginning of this section, the con-text of the pair (w(1)i, w(2)j) is then the Cartesianproduct of the context of w(1)iin S(1)and w(2)jinS(2).)
Note that w(1)iand w(2)jcan be of differentlexical categories here.A content word can often be surrounded bystop words which provide almost no informationabout its semantic context.
The chosen windowsize is assumed, on average, to effectively make243Data Set Source of Text # of Pairsdeft-forum discussion forums 450deft-news news articles 300headlines news headlines 750images image descriptions 750OnWN word sense definitions 750tweet-news news articles and tweets 750Table 1: Test sets for SemEval STS 2014.sufficient contextual information available whileavoiding the inclusion of contextually unrelatedwords.
But further experiments are necessary todetermine the best span in the context of align-ment.Unlike dependency-based alignment, even ifthere are no similar words in the context, i.e.
ifsimCij= 0, w(1)imay still be aligned to w(2)jifsimWij> 0 and no alignments for w(1)ior w(2)jhave been found by earlier stages of the pipeline.2.2.3 The Alignment SequenceThe rationale behind the specific sequence ofalignment steps (Figure 1) was explained in (Sul-tan et al., 2014): (1) Identical word sequencealignment was found to be the step with thehighest precision in experiments on the (Brock-ett, 2007) training data, (2) It is convenient toalign named entities before other content wordsto enable alignment of entity mentions of differ-ent lengths, (3) Dependency-based evidence wasobserved to be more reliable (i.e.
of higher preci-sion) than textual evidence on the (Brockett, 2007)training data.3 MethodOur STS score is a function of the proportions ofaligned content words in the two input sentences.The proportion of content words in S(1)that arealigned to some word in S(2)is:prop(1)Al=|{i : [?j : (i, j) ?
Al] and w(1)i?
C}||{i : w(1)i?
C}|where C is the set of all content words in En-glish and Al are the predicted word alignments.
Aword alignment is a pair of indices (i, j) indicatingthat word w(1)iis aligned to w(2)j.
The proportionof aligned content words in S(2), prop(2)Al, can becomputed in a similar way.We posit that a simple yet sensible way to obtainan STS estimate for S(1)and S(2)is to take a meanData Set Run 1 Run 2deft-forum 0.4828 0.4828deft-news 0.7657 0.7657headlines 0.7646 0.7646images 0.8214 0.8214OnWN 0.7227 0.8589tweet-news 0.7639 0.7639Weighted Mean 0.7337 0.7610Table 2: Results of evaluation on SemEval STS2014 data.
Each value on columns 2 and 3 is thecorrelation between system output and human an-notations for the corresponding data set.
The lastrow shows the value of the final evaluation metric.of prop(1)Aland prop(2)Al.
Our two submitted runsuse the harmonic mean:sim(S(1), S(2)) =2?
prop(1)Al?
prop(2)Alprop(1)Al+ prop(2)AlIt is a more conservative estimate than the arith-metic mean, and penalizes sentence pairs with alarge disparity between the values of prop(1)Alandprop(2)Al.
Experiments on STS 2012 and 2013 datarevealed the harmonic mean of the two propor-tions to be a better STS estimate than the arith-metic mean.4 DataSTS systems at SemEval 2014 were evaluated onsix data sets.
Each test set consists of a numberof sentence pairs; each pair has a human-assignedsimilarity score in the range [0, 5] which increaseswith similarity.
Every score is the mean of fivescores crowdsourced using the Amazon Mechan-ical Turk.
The sentences were collected from avariety of sources.
In Table 1, we provide a briefdescription of each test set.5 EvaluationWe submitted the results of two system runs atSemEval 2014 based on the idea presented in Sec-tion 3.
The two runs were identical, except for thefact that for the OnWN test set, we specified thefollowing words as additional stop words duringrun 2 (but not during run 1): something, someone,somebody, act, activity, some, state.4For both4OnWN has many sentence pairs where each sentenceis of the form ?the act/activity/state of verb+ing some-thing/somebody?.
The selected words act merely as fillersin such pairs and consequently do not typically contribute tothe similarity scores.244Data Set Run 1 Run 2FNWN 0.4686 0.4686headlines 0.7797 0.7797OnWN 0.6083 0.8197SMT 0.3837 0.3837Weighted Mean 0.5788 0.6315Table 3: Results of evaluation on *SEM STS 2013data.runs, the tweet-news sentences were preprocessedby separating the hashtag from the word for eachhashtagged word.Table 2 shows the performance of each run.Rows 1 through 6 show the Pearson correlationcoefficients between the system scores and humanannotations for all test sets.
The last row showsthe value of the final evaluation metric, which is aweighted sum of all correlations in rows 1?6.
Theweight assigned to a data set is proportional to itsnumber of pairs.
Our run 1 ranked 7th and run 2ranked 1st among 38 submitted system runs.An important implication of these results is thefact that knowledge of domain-specific stop wordscan be beneficial for an STS system.
Even thoughwe imparted this knowledge to our system duringrun 2 via a manually constructed set of additionalstop words, simple measures like TF-IDF can beused to automate the process.5.1 Performance on STS 2012 and 2013 DataWe applied our algorithm on the 2012 and 2013STS test sets to examine its general utility.
Notethat the STS 2013 setup was similar to STS 2014with no domain-dependent training data, whereasseveral of the 2012 test sets had designated train-ing data.Over all the 2013 test sets, our two runs demon-strated weighted correlations of 0.5788 (rank: 4)and 0.6315 (rank: 1), respectively.
Table 3 showsperformances on individual test sets.
(Descrip-tions of the test sets can be found in (Agirre etal., 2013).)
Again, run 2 outperformed run 1 onOnWN by a large margin.On the 2012 test sets, however, the performancewas worse (relative to other systems), with respec-tive weighted correlations of 0.6476 (rank: 8) and0.6423 (rank: 9).
Table 4 shows performances onindividual test sets.
(Descriptions of the test setscan be found in (Agirre et al., 2012).
)This performance drop seems to be an obviousconsequence of the fact that our algorithm wasnot trained on domain-specific data: on STS 2013Data Set Run 1 Run 2MSRpar 0.6413 0.6413MSRvid 0.8200 0.8200OnWN 0.7227 0.7004SMTeuroparl 0.4267 0.4267SMTnews 0.4486 0.4486Weighted Mean 0.6476 0.6423Table 4: Results of evaluation on SemEval STS2012 data.data, the top two STS 2012 systems, with respec-tive weighted correlations of 0.5652 and 0.5221(Agirre et al., 2013), were outperformed by oursystem by a large margin.In contrast to the other two years, our run 1outperformed run 2 on the 2012 OnWN test setby a very small margin.
A closer inspectionrevealed that the previously mentioned sentencestructure ?the act/activity/state of verb+ing some-thing/somebody?
is much less common in this set,and as a result, our additional stop words tend toplay more salient semantic roles in this set than inthe other two OnWN sets (i.e.
they act relativelymore as content words than stop words).
The dropin correlation with human annotations is a con-sequence of this role reversal.
This result againshows the importance of a proper selection of stopwords for STS and also points to the challengesassociated with making such a selection.6 Conclusions and Future WorkWe show that alignment of related words in twosentences, if carried out in a principled and accu-rate manner, can yield state-of-the-art results forsentence-level semantic similarity.
Our system hasthe desired quality of being both accurate and fast.Evaluation on test data from different STS yearsdemonstrates its general applicability as well.The idea of STS from alignment is worth inves-tigating with larger semantic units (i.e.
phrases)in the two sentences.
Another possible researchdirection is to investigate whether the alignmentproportions observed for the two sentences can beused as features to improve performance in a su-pervised setup (even though this scenario is ar-guably less common in practice because of un-availability of domain or situation-specific train-ing data).AcknowledgmentsThis material is based in part upon work supportedby the National Science Foundation under Grant245Numbers EHR/0835393 and EHR/0835381.
Anyopinions, findings, and conclusions or recommen-dations expressed in this material are those of theauthor(s) and do not necessarily reflect the viewsof the National Science Foundation.ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
SemEval-2012 task 6: Apilot on semantic textual similarity.
In Proceed-ings of the First Joint Conference on Lexical andComputational Semantics, Volume 2: Proceedingsof the Sixth International Workshop on SemanticEvaluation, SemEval ?12, pages 385-393, Montreal,Canada.Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*SEM 2013 sharedtask: Semantic Textual Similarity.
In Proceedings ofthe Second Joint Conference on Lexical and Compu-tational Semantics, *SEM ?13, pages 32-43, Atlanta,Georgia, USA.Eneko Agirre, Carmen Banea, Claire Cardie, DanielCer, Mona Diab, Aitor Gonzalez-Agirre, WeiweiGuo, Rada Mihalcea, German Rigau, and JanyceWiebe.
2014.
SemEval-2014 Task 10: Multilin-gual semantic textual similarity.
In Proceedings ofthe 8th International Workshop on Semantic Evalu-ation, SemEval ?14, Dublin, Ireland.Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,Danilo Giampiccolo, Bernardo Magnini, and IdanSzpektor.
2006.
The Second PASCAL RecognisingTextual Entailment Challenge.
In Proceedings of theSecond PASCAL Challenges Workshop on Recognis-ing Textual Entailment, Venice, Italy.Daniel B?ar, Chris Biemann, Iryna Gurevych, andTorsten Zesch.
2012.
UKP: computing seman-tic textual similarity by combining multiple contentsimilarity measures.
In Proceedings of the 6th Inter-national Workshop on Semantic Evaluation, held inconjunction with the 1st Joint Conference on Lexicaland Computational Semantics, SemEval ?12, pages435-440, Montreal, Canada.Chris Brockett.
2007.
Aligning the RTE 2006 Corpus.Technical Report MSR-TR-2007-77, Microsoft Re-search.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The ParaphraseDatabase.
In Proceedings of the 2013 Conferenceof the North American Chapter of the Associationfor Computational Linguistics, NAACL-HLT ?13,pages 758-764, Atlanta, Georgia, USA.Lushan Han, Abhay Kashyap, Tim Finin, JamesMayfield, and Jonathan Weese.
2013.
UMBCEBIQUITY-CORE: Semantic Textual SimilaritySystems.
In Proceedings of the Second Joint Con-ference on Lexical and Computational Semantics,*SEM ?13, pages 44-52, Atlanta, Georgia, USA.Sergio Jimenez, Claudia Becerra, and Alexander Gel-bukh.
2012.
Soft Cardinality: a parameterized sim-ilarity function for text comparison.
In Proceedingsof the 6th International Workshop on Semantic Eval-uation, held in conjunction with the 1st Joint Con-ference on Lexical and Computational Semantics,SemEval ?12, pages 449-453, Montreal, Canada.Yuhua Li, David Mclean, Zuhair A. Bandar, James D.O?Shea, and Keeley Crockett.
2006.
Sentence sim-ilarity based on semantic nets and corpus statistics.IEEE Transactions on Knowledge and Data Engi-neering, vol.18, no.8.
1138-1150.Rada Mihalcea, Courtney Corley, and Carlo Strappa-rava.
2006.
Corpus-based and knowledge-basedmeasures of text semantic similarity.
In Proceed-ings of the 21st national conference on Artificial in-telligence, AAAI ?06, pages 775-780, Boston, Mas-sachusetts, USA.Frane?Sari?c, Goran Glava?s, Mladen Karan, Jan?Snajder,and Bojana Dalbelo Ba?si?c.
2012.
TakeLab: sys-tems for measuring semantic text similarity.
In Pro-ceedings of the 6th International Workshop on Se-mantic Evaluation, held in conjunction with the 1stJoint Conference on Lexical and Computational Se-mantics, SemEval ?12, pages 441-448, Montreal,Canada.Ehsan Shareghi and Sabine Bergler.
2013.
CLaC-CORE: Exhaustive Feature Combination for Mea-suring Textual Similarity.
In Proceedings of theSecond Joint Conference on Lexical and Computa-tional Semantics, *SEM ?13, pages 202-206, At-lanta, Georgia, USA.Md Arafat Sultan, Steven Bethard, and Tamara Sum-ner.
2014.
Back to Basics for Monolingual Align-ment: Exploiting Word Similarity and ContextualEvidence.
Transactions of the Association for Com-putational Linguistics, 2 (May), pages 219-230.Stephen Wu, Dongqing Zhu, Ben Carterette, and Hong-fang Liu.
2013.
MayoClinicNLP-CORE: Semanticrepresentations for textual similarity.
In Proceed-ings of the Second Joint Conference on Lexical andComputational Semantics, *SEM ?13, pages 148-154, Atlanta, Georgia, USA.246
