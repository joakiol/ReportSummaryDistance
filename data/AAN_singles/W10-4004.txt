Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 11?15,Beijing, August 2010How to Get the Same News from Different Language NewsPapersT.
Pattabhi R. K RaoAU-KBC Research CentreAnna University ChennaiSobha Lalitha DeviAU-KBC Research CentreAnna University Chennaisobha@au-kbc.orgAbstractThis paper presents an ongoing work onidentifying similarity between documentsacross News papers in differentlanguages.
Our aim is to identify similardocuments for a given News or event asa query, across languages and make crosslingual search more accurate and easy.For example given  an event or News inEnglish, all the English news documentsrelated to the query are retrieved as wellas in other languages such as Hindi,Bengali, Tamil, Telugu, Malayalam,Spanish.
We use Vector Space Model, aknown method for similarity calculation,but the novelty is in identification ofterms for VSM calculation.
Here a robusttranslation system is not used fortranslating the documents.
The system isworking with good recall and precision.1 IntroductionIn this paper we present a novel method foridentifying similar News documents fromvarious language families such as Indo-European, Indo- Aryan and Dravidian.
Thelanguages considered from the above languagefamilies are English, Hindi, Bengali, Tamil,Telugu, Malayalam and Spanish.
The Newsdocuments in various languages are obtainedusing a crawler.
The documents are representedas vector of terms.Given a query in any of the language mentionedabove, the documents relevant to the query areretrieved.
The first two document retrieved in thelanguage of the query is taken as base for theidentification of similar documents.
Thedocuments are converted into terms and theterms are translated to other languages usingbilingual dictionaries.
The terms thus obtained isused for similarity calculation.
The paper isfurther organized as follows.
In the followingsection 2, related work is described.
In section 3,the algorithm is discussed.
Section 4 describesexperiments and results.
The paper concludeswith section 5.2 Related WorkIn the past decade there has been significantamount of work done on finding similarity ofdocuments and organizing the documentsaccording to their content.
Similarity ofdocuments are identified using different methodssuch as Self-Organizing Maps (SOMs)(Kohonen et al 2000; Rauber, 1999), based onOntologies and taxanomy (Gruber, 1993; Resnik,1995), Vector Space Model (VSM) withsimilarity measures like Dice similarity,Jaccard?s similarity, cosine similarity (Salton,1989).Many similarity measures were developed,such as information content (Resnik, 1995)mutual information (Hindle, 1990), Dicecoefficient (Frakes and Baeza-Yates, 1992),cosine coefficient (Frakes and Baeza-Yates,1992), distance-based measurements (Lee et al,1989; Rada et al, 1989), and feature contrastmodel (Tversky, 1977).
McGill etc.
surveyedand compared 67 similarity measures used ininformation retrieval (McGill et al, 1979).113 MethodologySimilarity is a fundamental concept.
Twodocuments can be said to be similar if both thedocuments have same content, describing a topicor an event or an entity.
Similarity is a measureof degree of resemblance, or commonalitybetween the documents.In this work we have used Vector SpaceModel (VSM) for document representation.
InVSM the documents are represented as vectorsof unique terms.
Here we have performedexperiments by creating three types of documentvector space models.
In the first case we havetaken all unique words in the documentcollection for vector of terms.
In the second casewe take the terms after removing all stop words.In the third case we have taken a sequence ofwords as terms.
After the document model isbuilt we use cosine similarity measure to identifythe degree of similarity between documents.In this work we have taken documents fromthe languages mentioned in the previous section.For the purpose of identifying similar documentsacross the languages we use map of term vectorsof documents from English to other languages.Using the term vector map we can identifysimilar documents for various languages.3.1 Similarity analyserThe main modules are i) Document vectorcreator ii) Translator and iii) Similarityidentifier.a) Document Vector Creator: Each documentis represented as vector of terms.
Here we takethree types of term vectors.
In the first type asingle word is taken as a term which is thestandard implementation of VSM.
In the secondtype single words are taken but the stop wordsare removed.In the third type each term is a sequence ofwords, where we define the number of words inthe sequence as 4.
This moving window of 4 isobtained by performing many experiments usingdifferent combinations of words.
So our term ofvector is defined as a set of four consecutivewords, where the last three words in thepreceding sequence is considered as the firstthree words in the following sequence.
Forexample if a sentence has 10 words (w), thevector of terms for this sentence is w1w2w3w4,w2w3w4w5, w3w4w5w6, w4w5w6w7,w5w6w7w8, w6w7w8w9, w7w8w9w10.
Theweights of the terms in the vector are the termfrequency and inverse document frequency (tf-idf).
While creating document vectors, for Indianlanguages which are highly agglutinative andmorphologically rich we use morphologicalanalyzer to reduce the word into its root and it isused for document vector creation.The first two experiments are the standardVSM implementation.
The third experimentdiffers in the way the terms are taken forbuilding the VSM.
For building the VSM modelwhich is common for all language documenttexts, it is essential that there should betranslation/transliteration tool.
First the terms arecollected from individual language documentsand a unique list is formed.
The unique list ofwords is then translated using the translatormodule.b) Word by Word Translator: In this module,the terms from English documents are taken andare translated to different languages.
Thetranslation is done word by word with the use ofbilingual and multilingual synset dictionaries.This translation creates a map of terms fromEnglish to different languages.
We have usedbilingual dictionaries from English to Spanish,Hindi, Tamil, Telugu, and Malayalamdictionaries.
Also we have used multilingualsynset dictionaries for English, Tamil, Telugu,Hindi, and Malayalam.
For each pair of bilingualdictionaries there are more than 100K rootwords.
Since in this work we do not requiresyntactically and semantically correct translationof the sentences we adopted word to wordtranslation.
Hence we did not use any othersystem such as SMT for English to Indianlanguages.
Named entities require transliteration.Here we have used a transliteration tool.
Thistool uses rule based approach, based on thephoneme match.
The transliteration toolproduces all possible transliteration outputs.Here we take into consideration the top five bestpossible outputs.
For example the name ?LalKrishna Advani?
would get transliterations inIndian languages as ?laala krishna athvaani?,?laala krishna advaani?.c) Similarity Identifier: The similarityidentifier module takes the query in the formdocument as input and identifies all relevant12documents.
The similarity identifier uses cosinesimilarity measure over documents vectorcreator.
The cosine similarity measure is the dotproduct of two vectors and is between 0 and 1value.
The more it is closer to 1, the similarity ismore.
The formula of cosine similarity is asfollows:Sim(S1,S2)tj = ?
(W1j x W2j ) -- (1)Where,tj is a term present in both vectors S1and S2.W1j is the weight of term tj in S1 andW2j is the weight of term tj in S2.The weight of term tj in the vector S1 iscalculated by the formula given by equation (2),below.Wij=(tf*log(N/df))/[sqrt(Si12+Si22+?+Sin2)]--(2)Where,tf = term frequency of term tjN=total number of documents in the collectiondf = number of documents in the collection thatthe term tj occurs in.sqrt represents square rootThe denominator[sqrt(Si12+Si22+?
?+Sin2)] is the cosinenormalization factor.
This cosine normalizationfactor is the Euclidean length of the vector Si,where ?i?
is the document number in thecollection and Sin2 is the square of the productof (tf*log(N/df)) for term  in the vector Si.4 Experiments and ResultsWe have performed three experiments with twodifferent data sets.
The first data set wascollected by crawling the web for a single day?snews articles and obtained 1000 documents fromvarious online news magazines in variouslanguages.
The test set was taken from Times ofIndia, The Hindu for English, BBC, Dinamani,Dinamalar for Tamil, Yahoo for Telugu,Matrubhumi for Malayalam, BBC and DainikJagran for Hindi and BBC for Spanish.
Thedistribution of documents in the first set forvarious languages is as follows: 300 English,200 Tamil, 150 Telugu, 125 Hindi, 125Malayalam, 50 Spanish.
The figure 1 givenbelow shows the language distribution in thisfirst set.The number of similar documents were 600 inthis set.EnglishTamilTeluguHindiMalyalamSpanishFigure 1.
Data Distribution of Set 1In the second data set we have taken newsdocuments of one week time duration.
Thisconsisted of 9750 documents.
The languagedistribution for this data set is shown in figure 2.This second data set consisted of 5350 similardocuments.EnglishTamilTeluguHindiMalayalamSpanishFigure 2.
Data Distribution of Set 2In the first experiment we took all the uniquewords (separated by white space) as terms forbuilding the document vector.
In the secondexperiment the terms taken were same as thefirst experiment, except that all the stop wordswere removed.
In the third experiment, the termstaken for document vector creation were fourconsecutive words.
The results obtained forthree experiments for data set 1 is shown inTable 1.
And results for data set 2 are shown inTable 2.
Table 3 shows the similarityidentification for various languages.Here we take a news story document as aquery and perform similarity analysis across alldocuments in the document collection to identifysimilarly occurring news stories.
In the first dataset in the gold standard there are 600 similarpairs of documents.
And in the second data setthere are 5350 similar pairs of documents in thegold standard.It is observed that even though there weremore similar documents which could have beenidentified, but the system could not identifythose documents.
The cosine measure for those13unidentified documents was found to be lowerthan 0.8.
We have taken 0.8 as the threshold fordocuments to be considered similar.
In thedocuments which were not identified by thesystem, the content described consisted of lessnumber of words.
These were mostly twoparagraph documents; hence the similarity scoreobtained was less than the threshold.
Inexperiment three, we find that the number offalse positives is decreased and also the numberof documents identified similar is increased.
Thisis because, in this case the system sees for termsof four words and hence single word matches arereduced.
This reduces false positives.
The otheradvantage of this is the words get the context, ina sense that the words in each sequence are notindependent.
The words get an order and aresensitive to that order.
This solves sensedisambiguation.
Hence we find that it is solvingthe polysemy problem to some extent.
Thesystem can be further improved by creatingrobust map files between terms in differentlanguages.
The bilingual dictionaries also needto be improved.In our work, since we are using a sequence ofwords as terms for document vectors, we do notrequire proper, sophisticated translation systems.A word by word translation would suffice to getthe desired results.Table 1.
Similarity Results on Data Set 1Table 2.
Similarity Results on Data Set 2Table 3.Similarity Results Data Set with Ex:35 ConclusionHere we have shown how we can identifysimilar News document in various languages.The results obtained are encouraging; we obtainan average precision of 97.8% and recall of94.3%.
This work differs from previous works intwo aspects: 1) no language preprocessing of thedocuments is required and 2) terms taken forVSM are a sequence of four words.ReferencesFrakes, W. B. and Baeza-Yates, R., editors 1992.Information Retrieval, Data Structure andAlgorithms.
Prentice Hall.T.
R. Gruber.
1993.
A translation approach toportable ontologies, Knowledge Acquisition,5(2):199?220.Hindle, D. 1990.
Noun classification from predicate-argument structures.
In Proceedings of  ACL-90,pages 268?275, Pittsburg, Pennsylvania.Kohonen, Teuvo Kaski, Samuel Lagus, KristaSalojarvi, Jarkko Honkela, Jukka Paatero,VesaSaarela, Anti.
2000.
Self organisation of a massivedocument collection, IEEE Transactions on NeuralNetworks, 11(3): 574-585.Lee, J. H., Kim, M. H., and Lee, Y. J.
1989.Information retrieval based on conceptual distancein is-a hierarchies.
Journal of Documentation,49(2):188?207.McGill et al, M. 1979.
An evaluation of factorsaffecting document ranking by informationretrieval systems.
Project report, SyracuseUniversity School of Information Studies.Rauber, Andreas Merkl, Dieter.
1999.
The SOMLibdigital library system,  In the Proceedings of the3rd European Conference on Research andAdvanced Technology for Digital Libraries(ECDL'99), Paris, France.
Berlin: 323-341.Lang GoldStdsimilardocsSystemIdentifiedcorrectSystemIdentifiedwrongPrec%Rec%Eng 1461 1377 30 97.86 94.25Span 732 690 15 97.87 94.26Hin 588 554 11 98.05 94.22Mal 892 839 19 97.78 94.05Tam 932 880 22 97.56 94.42Tel 745 703 17 97.63 94.36AVG 97.79 94.26ExpNoGold stdSimilaritySystemIdentifiedCorrectSystemIdentifiedWrongPrec%Rec%1 600 534 50 91.4 89.02 600 547 44 92.5 91.23 600 565 10 98.3 94.2ExpNoGoldStandardSimilaritySystemIdentifiedCorrectSystemIdentifiedWrongPrec%Rec%1 5350 4820 476 91.0 90.02 5350 4903 410 92.3 91.63 5350 5043 114 97.8 94.314Rada, R., Mili, H., Bicknell, E., and Blettner, M.1989.
Development and application of a metric onsemantic nets.
IEEE Transaction on Systems, Man,and Cybernetics, 19(1):17?30.P.
Resnik.
1995.
Using information content toevaluate semantic similarity in taxonomy,Proceedings of IJCAI: 448?453.Salton, Gerald.
1989.
Automatic Text Processing: TheTransformation, Analysis and Retrieval ofInformation by Computer, Reading, MA: AddisonWesleyTversky, A.
1977.
Features of similarity.Pychological Review, 84:327?352.15
