Experiments on Sentence Boundary DetectionMark  Stevenson  and  Rober t  Ga izauskasDepar tment  of  Computer  Science,Un ivers i ty  of  Sheff ieldRegent  Cour t ,  211 Por tobe l lo  St reet ,Sheff ieldS1 4DP Un i ted  K ingdom{marks, robertg}@dcs, shef.
ac.ukAbst ractThis paper explores the problem of identifying sen-tence boundaries in the transcriptions produced byautomatic speech recognition systems.
An experi-ment which determines the level of human perform-ance for this task is described as well as a memory-based computational pproach to the problem.1 The  Prob lemThis paper addresses the problem of identifying sen-tence boundaries in the transcriptions produced byautomatic speech recognition (ASR) systems.
Thisis unusual in the field of text processing which hasgenerally dealt with well-punctuated text: some ofthe most commonly used texts in NLP are machinereadable versions of highly edited documents uchas newspaper articles or novels.
However, there aremany types of text which are not so-edited and theexample which we concentrate on in this paper isthe output from ASR systems.
These differ fromthe sort of texts normally used in NLP in a numberof ways; the text is generally in single case (usuallyupper), unpunctuated and may contain transcrip-tion errors.
1 Figure 1 compares a short text in theformat which would be produced by an ASR systemwith a fully punctuated version which includes caseinformation.
For the remainder of this paper error-free texts such as newspaper articles or novels shallbe referred to as "standard text" and the outputfrom a speech recognition system as "ASR text".There are many possible situations in which anNLP system may be required to process ASR text.The most obvious examples are NLP systems whichtake speech input (eg.
Moore et al (1997)).
Also,dictation software programs do not punctuate orcapitalise their output but, if this information couldbe added to ASR text, the results would be far moreusable.
One of the most important pieces of inform-1 Speech recognition systems are often evaluated in termsof word error ate (WER), the percentage oftokens which arewrongly transcribed.
For large vocabulary tasks and speaker-independent systems, WER varies between 7% and 50%, de-pending upon the quality of the recording being recognised.See, e.g., Cole (1996).G00D EVENING GIANNI VERSACE ONE OF THEWORLDS LEADING FASHION DESIGNERS HASBEEN MURDERED IN MIAMI POLICE SAY IT WASA PLANNED KILLING CARRIED OUT LIKE ANEXECUTION SCHOOLS INSPECTIONS ARE GOINGTO BE TOUGHER TO FORCE BAD TEACHERS OUTAND THE FOUR THOUSAND COUPLES WH0 SHAREDTHE QUEENS GOLDEN DAYGood evening.
Gi~nni Versace, one ofthe world's leading fashion designers,has been murdered in Miami.
Police sayit was a planned killing carried outlike an execution.
Schools inspectionsare going to be tougher to force badteachers out.
And the four thousandcouples who shared the Queen's goldenday.Figure 1: Example text shown in standard and ASRformatation which is not available in ASR output is sen-tence boundary information.
However, knowledge ofsentence boundaries i required by many NLP tech-nologies.
Part of speech taggers typically requireinput in the format of a single sentence per line (forexample Brill's tagger (Brill, 1992)) and parsers gen-erally aim to produce a tree spanning each sentence.Only the most trivial linguistic analysis can be car-ried out on text which is not split into sentences.It is worth mentioning that not all transcribedspeech can be sensibly divided into sentences.
It hasbeen argued by Gotoh and Renals (2000) that themain unit in spoken language is the phrase ratherthan the sentence.
However, there are situationsin which it is appropriate to consider spoken lan-guage to be made up from sentences.
One exampleis broadcast news: radio and television news pro-grams.
The DARPA HUB4 broadcast news evalu-ation (Chinchor et al, 1998) focussed on informa-tion extraction from ASR text from news programs.Although news programs are scripted there are of-ten deviations from the script and they cannot berelied upon as accurate transcriptions of the news84program.
The spoken portion of the British NationalCorpus (Burnard, 1995) contains 10 million wordsand was manually marked with sentence boundar-ies.
A technology which identifies entence boundar-ies could be used to speed up the process of creatingany future corpus of this type.It is important o distinguish the problem justmentioned and another problem sometimes called"sentence splitting".
This problem aims to identifysentence boundaries in standard text but since thisincludes punctuation the problem is effectively re-duced to deciding which of the symbols which poten-tially denote sentence boundaries ( .
,  !,  ?)
actuallydo.
This problem is not trivial since these punc-tuation symbols do not always occur at the end ofsentences.
For example in the sentence "Dr. Jonesl ec tures  at  U.C.L.A."
only the final full stop de-notes the end of a sentence.
For the sake of claritywe shall refer to the process of discovering sentenceboundaries in standard punctuated text as "punc-tuation disambiguation" and that of finding themin unpunctuated ASR text as "sentence boundarydetection".2 Related WorkDespite the potential application of technologywhich can carry out the sentence boundary detec-tion task, there has been little research into thearea.
However, there has been work in the re-lated field of punctuation disambiguation.
Palmerand Hearst (1994) applied a neural network to theproblem.
They used the Brown Corpus for trainingand evaluation, noting that 90% of the full stops inthis text indicate sentence boundaries.
They usedthe part of speech information for the words sur-rounding a punctuation symbol as the input to afeed-forward neural network.
But, as we mentioned,most part of speech taggers require sentence bound-aries to be pre-determined and this potential cir-cularity is avoided by using the prior probabilitiesfor each token, determined from the Brown corpusmarkup.
The network was trained on 573 potentialsentence nding marks from the Wall Street Journaland tested on 27,294 items from the same corpus.98.5% of punctuation marks were correctly disam-biguated.Reynar and Ratnaparkhi (1997) applied a max-imum entropy approach to the problem.
Theirsystem considered only the first word to the leftand right of any potential sentence boundary andclaimed that examining wider context did not help.For both these words the prefix, suffix, presence ofparticular characters in the prefix or suffix, whetherthe candidate is honorific (Mr., Dr.
etc.)
andwhether the candidate is a corporate designator (eg.Corp.)
are features that are considered.
This sys-tem was tested on the same corpus as Palmer andHearst's system and correctly identified 98.8% ofsentence boundaries.
Mikheev (1998) optimised thisapproach and evaluated it on the same test corpus.An accuracy of 99.2477% was reported, to our know-ledge this is the highest quoted result for this testset.These three systems achieve very high resultsfor the punctuation disambiguation task.
It wouldseem, then, that this problem has largely beensolved.
However, it is not clear that these techniqueswill be as successful for ASR text.
We now go on todescribe a system which attempts a task similar tosentence boundary detection of ASR text.Beeferman et al (1998) produced a system, "CY-BERPUNC", which added intra-sentence punctu-ation (i.e.
commas) to the output of an ASR system.They mention that the comma is the most frequentlyused punctuation symbol and its correct insertioncan make a text far more legible.
CYBERPUNCoperated by augmenting a standard trigram speechrecognition model with information about commas;it accesses only lexical information.
CYBERPUNCwas tested by separating the trigram model fromthe ASR system and applying it to 2,317 sentencesfrom the Wall Street Journal.
The system achieveda precision of 75.6% and recall of 65.6% comparedagainst he original punctuation i  the text.
2 A fur-ther qualitative valuation was carried out using 100randomly-drawn output sentences from the systemand 100 from the Wall Street Journal.
Six humanjudges blindly marked each sentence as either ac-ceptable or unacceptable.
It was found that thePenn TreeBank sentences were 86% correct and thesystem output 66% correct.
It is interesting that thehuman judges do not agree completely on the ac-ceptability of many sentences from the Wall StreetJournal.In the next section we go on to describe exper-iments which quantify the level of agreement thatcan be expected when humans carry out sentenceboundary detection.
Section 4 goes on to describe acomputational pproach to the problem.3 Determining Human Abi l i tyBeeferman et.
al.
's experiments demonstrated thathumans do not always agree on the acceptability ofcomma insertion and therefore it may be useful todetermine how often they agree on the placing ofsentence boundaries.
To do this we carried out ex-periments using transcriptions ofnews programmes,specifically the transcriptions of two editions of the~Precision and recall are complementary  evaluation met-rics commonly  used in Information Retrieval (van Rijsbergen,1979).
In this case precision is the percentage of commas pro-posed by the system which are correct while recall is the per-centage of the commas occurring in the test corpus which thesystem identified.R~BBC television program "The Nine O'Clock News" .3The transcriptions consisted of punctuated mixedcase text with sentences boundaries marked using areserved character ("; ").
These texts were producedby trained transcribers listening to the original pro-gram broadcast.Six experimental subjects were recruited.
All sub-jects were educated to at least Bachelor's degreelevel and are either native English speakers or flu-ent second language speakers.
Each subject waspresented with the same text from which the sen-tence boundaries had been removed.
The texts weretranscriptions of two editions of the news programfrom 1997, containing 534 sentences and representedaround 50 minutes of broadcast news.
The subjectswere randomly split into two groups.
The subjectsin the first group (subjects 1-3) were presented withthe text stripped of punctuation and converted toupper case.
This text simulated ASR text with noerrors in the transcription.
The remaining three sub-jects (4-6) were presented with the same text withpunctuation removed but case information retained(i.e.
mixed case text).
This simulated unpunctuatedstandard text.
All subjects were asked to add sen-tence boundaries to the text whenever they thoughtthey occurred.The process of determining human ability at somelinguistic task is generally made difficult by the lackof an appropriate reference.
Often all we have tocompare one person's judgement with is that of an-other.
For example, there have been attempts todetermine the level of performance which can be ex-pected when humans perform word sense disambig-uation (Fellbaum et al, 1998) but these have simplycompared some human judgements against otherswith one being chosen as the "expert".
We havealready seen, in Section 2, that there is a signific-ant degree of human disagreement over the accept-ability of intra-sentential punctuation.
The humantranscribers ofthe "Nine O'Clock News" have accessto the original news story which contains more in-formation than just the transcription.
Under theseconditions it is reasonable to consider their opinionas expert.Table 1 shows the performance of the human sub-jects compared to the reference transcripts.
4An algorithm was implemented to provide abaseline tagging of the text.
The average length ofsentences in our text is 19 words and the baseline al-gorithm randomly assigns a sentence break at eachword boundary with a probability of ~ .
The twoannotators labelled "random" show the results whenthis algorithm is applied.
This method produced a3This is a 25 minute long television ews program broad-cast in the United Kingdom on Monday to Friday evenings.4F-measure (F) is a weighted harmonic ombining preci-sion (P) and recall (R) via the formula 2PRPTR "very low result in comparison to the expert annota-tion.1 Upper 84 68 762 Upper 93 78 853 Upper 90 76 824 Mixed 97 90 945 Mixed 96 89 926 Mixed 97 67 79Random Upper 5 5 5Random Mixed 5 5 5Table 1: Results from Human Annotation Experi-mentThe performance of the human annotators on theupper case text is quite significantly lower thanthe reported performance of the algorithms whichperformed punctuation disambiguation on standardtext as described in Section 2.
This suggests thatthe performance which may be obtained for this taskmay be lower than has been achieved for standardtext.~Sarther insight into the task can be gained fromdetermining the degree to which the subjects agreed.Carletta (1996) argues that the kappa statistic (a)should be adopted to judge annotator consistencyfor classification tasks in the area of discourse anddialogue analysis.
It is worth noting that the prob-lem of sentence boundary detection presented so farin this paper has been formulated as a classificationtask in which each token boundary has to be clas-sifted as either being a sentence boundary or not.Carletta argues that several incompatible measuresof annotator agreement have been used in discourseanalysis, making comparison impossible.
Her solu-tion is to look to the field of content analysis, whichhas already experienced these problems, and adopttheir solution of using the kappa statistic.
This de-termines the difference between the observed agree-ment for a linguistic task and that which would beexpected by chance.
It is calculated according to for-mula 1, where Pr(A) is the proportion of times theannotators agree and Pr(E) the proportion whichwould be expected by chance.
Detailed instructionson calculating these probabilities are described bySiegel and Castellan (1988).Pr(A) - Pr(E)= (1)1 - Pr(E)The value of the kappa statistic ranges between1 (perfect agreement) and 0 (the level which wouldbe expected by chance).
It has been claimed thatcontent analysis researchers usually regard a > .8 todemonstrate good reliability and .67 < ~ < .8 al-f16lows tentative conclusions to be drawn (see Carletta(1996)).We began to analyse the data by computing thekappa statistic for both sets of annotators.
Amongthe two annotators who marked the mixed case (sub-jects 4 and 5) there was an observed kappa value of0.98, while there was a measure of 0.91 for the threesubjects who annotated the single case text.
Thesevalues are high and suggest a strong level of agree-ment between the annotators.
However, manualanalysis of the annotated texts suggested that thesubjects did not agree on many cases.
We then ad-ded the texts annotated by the "random" annotationalgorithm and calculated the new ~ values.
It wasfound that the mixed case test produced a kappavalue of 0.92 and the upper case text 0.91.
Thesevalues would still suggest a high level of agreementalthough the sentences produced by our random al-gorithm were nonsensical.The problem seems to be that most word bound-aries in a text are not sentence boundaries.
There-fore we could compare the subjects' annotationswho had not agreed on any sentence boundaries butfind that they agreed most word boundaries werenot sentence boundaries.
The same problem willeffect other standard measures of inter-annotatoragreement such as the Cramer, Phi and Kendallcoefficients (see Siegel and Castellan (1988)).
Car-letta mentions this problem, asking what the dif-ference would be if the kappa statistic were com-puted across "clause boundaries, transcribed wordboundaries, and transcribed phoneme boundaries"(Carletta, 1996, p. 252) rather than the sentenceboundaries he suggested.
It seems likely that moremeaningful ~ values would be obtained if we restric-ted to the boundaries between clauses rather thanall token boundaries.
However, it is difficult to ima-gine how clauses could be identified without parsingand most parsers require part of speech tagged inputtext.
But, as we already mentioned, part of speechtaggers often require input text split into sentences.Consequently, there is a lack of available systems forsplitting ASR text into grammatical clauses.4 A Computat iona l  Approach  toSentence  Boundary  Detect ionThe remainder of this paper describes an implemen-ted program which attempts entence boundary de-tection.
The approach is based around the Timblmemory-based learning algorithm (Daelemans et al,1999) which we previously found to be very success-ful when applied to the word sense disambiguationproblem (Stevenson and Wilks, 1999).Memory-based learning, also known as case-basedand lazy learning, operates by memorising a set oftraining examples and categorising new cases by as-signing them the class of the most similar learnedexample.
We apply this methodology to the sen-tence boundary detection task by presenting Timblwith examples of word boundaries from a train-ing text, each of which is categorised as eithersentence_boundary or no_boundary.
Unseen ex-amples are then compared and categorised with theclass of the most similar example.
We shall notdiscuss the method by which Timbl determines themost similar training example which is described byDaelemans et al (1999).Following the work done on punctuation disambig-uation and that of Beeferman et.
al.
on comma in-sertion (Section 2), we used the Wall Street Journaltext for this experiment.
These texts are reliablypart of speech tagged 5 and sentence boundaries canbe easily derived from the corpus.
This text wasinitially altered so as to remove all punctuation andmap all characters into upper case.
90% of the cor-pus, containing 965 sentence breaks, was used as atraining corpus with the remainder, which contained107 sentence breaks, being held-back as unseen testdata.
The first stage was to extract some statisticsfrom the training corpus.
We examined the trainingcorpus and computed, for each word in the text, theprobability that it started a sentence and the prob-ability that it ended a sentence.
In addition, for eachpart of speech tag we also computed the probabilitythat it is assigned to the first word in a sentence andthe probability that it is assigned to the last word.
6Each word boundary in the corpus was translated toa feature-vector representation consisting of 13 ele-ments, shown in Table 2.
Vectors in the test corpusare in a similar format, the difference being that theclassification (feature 13) is not included.The results obtained are shown in the top row ofTable 3.
Both precision and recall are quite prom-ising under these conditions.
However, this text isdifferent from ASR text in one important way: thetext is mixed case.
The experimented was repeatedwith capitalisation information removed; that is,features 6 and 12 were removed from the feature-vectors.
The results form this experiment are shownin the bottom row of Table 3.
It can be seen thatthe recorded performance is far lower when capital-isation information is not used, indicating that thisis an important feature for the task.These experiments have shown that it is mucheasier to add sentence boundary information tomixed case test, which is essentially standard textwith punctuation removed, than ASR text, even as-5Applying a priori tag probability distributions could havebeen used rather than the tagging in the corpus as such re-liable annotations may not be available for the output of anASR system.
Thus, the current experiments should be viewedas making an optimistic assumption.eWe attempted to smooth these probabilities using Good-Turing frequency estimation (Gale and Sampson, 1996) butfound that it had no effect on the final results.87Position Feature12345678910111213Preceding wordProbability preceding word ends a sentencePart of speech tag assigned to preceding wordProbability that part of speech tag (feature 3) is assigned to last word in a sentenceFlag indicating whether preceding word is a stop wordFlag indicating whether preceding word is capitalisedFollowing wordProbability following word begins a sentencePart of speech tag assigned to following wordProbability that part of speech (feature 9) is assigned to first word in a sentenceFlag indicating whether following word is a stop wordFlag indicating whether following word is capitalised wordsentence_boundary or no_boundaryTable 2: Features used in Timbl representationCase information \[I P I R I FApplied I 78 \[ 75 \[ 76Not applied 36 35 35Table 3: Results of the sentence boundary detectionprogramsuming a zero word error rate.
This result is inagreement with the results from the human annota-tion experiments described in Section 3.
However,there is a far greater difference between the auto-matic system's performance on standard and ASRtext than the human annotators.Reynar and Ratnaparkhi (1997) (Section 2) ar-gued that a context of one word either side is suf-ficient for the punctuation disambiguation problem.However, the results of our system suggest that thismay be insufficient for the sentence boundary detec-tion problem even assuming reliable part of speechtags (cf note 5).These experiments do not make use of prosodic in-formation which may be included as part of the ASRoutput.
Such information includes pause length,pre-pausal lengthening and pitch declination.
If thisinformation was made available in the form of extrafeatures to a machine learning algorithm then it ispossible that the results will improve.5 Conc lus ionThis paper has introduced the problem of sentenceboundary detection on the text produced by an ASRsystem as an area of application for NLP technology.An attempt was made to determine the level ofhuman performance which could be expected for thetask.
It was found that there was a noticeable dif-ference between the observed performance for mixedand upper case text.
It was found that the kappastatistic, a commonly used method for calculatinginter-annotator agreement, could not be applied dir-ectly in this situation.A memory-based system for identifying sentenceboundaries in ASR text was implemented.
Therewas a noticeable difference when the same systemwas applied to text which included case informationdemonstrating that this is an important feature forthe problem.This paper does not propose to offer a solution tothe sentence boundary detection problem for ASRtranscripts.
However, our aim has been to high-light the problem as one worthy of further explor-ation within the field of NLP and to establish somebaselines (human and algorithmic) against whichfurther work may be compared.AcknowledgementsThe authors would like to thank Steve Renals andYoshihiko Gotoh for providing the data for humanannotation experiments and for several useful con-versations.
They are also grateful to the followingpeople who took part in the annotation experiment:Paul Clough, George Demetriou, Lisa Ferry, MichaelOakes and Andrea Setzer.ReferencesD.
Beeferman, A. Berger, and J. Lafferty.
1998.
CY-BERPUNC: A lightweight punctuation annota-tion system for speech.
In Proceedings of the IEEEInternational Conference on Acoustics, Speechand Signal Processing, pages 689-692, Seattle,WA.E.
Brill.
1992.
A simple rule-based part of speechtagger.
In Proceeding of the Third Conference onApplied Natural Language Processing (ANLP-92),pages 152-155, Trento, Italy.R~ 88L.
Burnard, 1995.
Users Reference Guide for theBritish National Corpus.
Oxford University Com-puting Services.J.
Carletta.
1996.
Assessing agreement on classific-ation tasks: the kappa statistic.
ComputationalLinguistics, 22(2):249-254.N.
Chinchor, P. Robinson, and E. Brown.1998.
HUB-4 Named Entity Task Defini-tion (version 4.8).
Technical report, SAIC.http ://www.
nist.
gov/speech/hub4-98.R.
Cole, editor.
1996.
Survey of the State of theArt in Human Language Technology.
Available at:http://cslu.cse.ogi.edu/HLTsurvey/HLTsurvey.html.Site visited 17/11/99.W.
Daelemans, J. Zavrel, K. van der Sloot,and A. van den Bosch.
1999.
TiMBL: Tilburgmemory based learner version 2.0, reference guide.Technical report, ILK Technical Report 98-03.ILK Reference Report 99-01, Available fromhttp ://ilk.
kub.
nl/" ilk/papers/ilk9901, ps.
gz.C.
Fellbaum, J. Grabowski, S. Landes, and A. Ban-mann.
1998.
Matching words to senses in Word-Net: Naive vs. expert differentiation of senses.
InC. Fellbaum, editor, WordNet: An electronic lex-ieal database and some applications.
MIT Press,Cambridge, MA.W.
Gale and G. Sampson.
1996.
Good-Turingfrequency estimation without tears.
Journal ofQuantitave Linguistics, 2(3):217-37.Y.
Gotoh and S. Renals.
2000.
Information extrac-tion from broadcast news.
Philosophical Trans-actions of the Royal Society of London, series A:Mathematical, Physical and Engineering Sciences.
(to appear).A.
Mikheev.
1998.
Feature lattices for maximum en-tropy modelling.
In Proceedings of the 36th Meet-ing of the Association for Computational Linguist-ics (COLING-ACL-98), pages 848-854, Montreal,Canada.R.
Moore, J. Dowding, H. Bratt, J. Gawron,Y.
Gorfu, and A. Cheyer.
1997.
CommandTalk:A Spokcaa-Language Interface to Battlefield Simu-lations.
In Proceedings of the Fifth Conference onApplied Natural Language Processing, pages 1-7,Washington, DC.D.
Palmer and M. Hearst.
1994.
Adaptive sen-tence boundary disambiguation.
In Proceedings ofthe 1994 Conference on Applied Natural LanguageProcessing, pages 78-83, Stutgart, Germany.J.
Reynar and A. Ratnaparkhi.
1997.
A max-imum entropy approach to identifying sentenceboundries.
In Proceedings of the Fifth Conferenceon Applied Natural Language Processing, pages16-19, Washington, D.C.S.
Siegel and N. Castellan.
1988.
NonparametrieStatistics for the Behavioural Sciences.
McGraw-Hill, second edition.M.
Stevenson and Y. Wilks.
1999.
Combining weakknowledge sources for sense disambiguation.
InProceedings of the Sixteenth International JointConference on Artificial Intelligence, pages 884-889.
Stockholm, Sweden.C.
van Rijsbergen.
1979.
Information Retrieval.Butterworths, London.89
