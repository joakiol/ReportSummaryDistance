Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 430?439,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsMining Name Translations from Entity Graph Mapping?Gae-won You?
Seung-won Hwang?
Young-In Song?
Long Jiang?
Zaiqing Nie?
?Pohang University of Science and Technology, Pohang, Republic of Korea{gwyou,swhwang}@postech.ac.kr?Microsoft Research Asia, Beijing, China{yosong,longj,znie}@microsoft.comAbstractThis paper studies the problem of mining en-tity translation, specifically, mining Englishand Chinese name pairs.
Existing effortscan be categorized into (a) a transliteration-based approach leveraging phonetic similar-ity and (b) a corpus-based approach exploitingbilingual co-occurrences, each of which suf-fers from inaccuracy and scarcity respectively.In clear contrast, we use unleveraged re-sources of monolingual entity co-occurrences,crawled from entity search engines, repre-sented as two entity-relationship graphs ex-tracted from two language corpora respec-tively.
Our problem is then abstracted as find-ing correct mappings across two graphs.
Toachieve this goal, we propose a holistic ap-proach, of exploiting both transliteration sim-ilarity and monolingual co-occurrences.
Thisapproach, building upon monolingual corpora,complements existing corpus-based work, re-quiring scarce resources of parallel or compa-rable corpus, while significantly boosting theaccuracy of transliteration-based work.
Wevalidate our proposed system using real-lifedatasets.1 IntroductionEntity translation aims at mapping the entity names(e.g., people, locations, and organizations) in sourcelanguage into their corresponding names in targetlanguage.
While high quality entity translation is es-sential in cross-lingual information access and trans-?This work was done when the first two authors visited Mi-crosoft Research Asia.lation, it is non-trivial to achieve, due to the chal-lenge that entity translation, though typically bear-ing pronunciation similarity, can also be arbitrary,e.g., Jackie Chan and ? (pronounced ChengLong).
Existing efforts to address these challengescan be categorized into transliteration- and corpus-based approaches.
Transliteration-based approaches(Wan and Verspoor, 1998; Knight and Graehl, 1998)identify translations based on pronunciation similar-ity, while corpus-based approaches mine bilingualco-occurrences of translation pairs obtained fromparallel (Kupiec, 1993; Feng et al, 2004) or compa-rable (Fung and Yee, 1998) corpora, or alternativelymined from bilingual sentences (Lin et al, 2008;Jiang et al, 2009).
These two approaches have com-plementary strength?
transliteration-based similar-ity can be computed for any name pair but cannotmine translations of little (or none) phonetic simi-larity.
Corpus-based similarity can support arbitrarytranslations, but require highly scarce resources ofbilingual co-occurrences, obtained from parallel orcomparable bilingual corpora.In this paper, we propose a holistic approach,leveraging both transliteration- and corpus-basedsimilarity.
Our key contribution is to replace theuse of scarce resources of bilingual co-occurrenceswith the use of untapped and significantly largerresources of monolingual co-occurrences for trans-lation.
In particular, we extract monolingual co-occurrences of entities from English and ChineseWeb corpora, which are readily available from en-tity search engines such as PeopleEntityCube1, de-ployed by Microsoft Research Asia.
Such engine1http://people.entitycube.com430automatically extracts people names from text andtheir co-occurrences to retrieve related entities basedon co-occurrences.
To illustrate, Figure 1(a) demon-strates the query result for ?Bill Gates,?
retrievingand visualizing the ?entity-relationship graph?
of re-lated people names that frequently co-occur withBill in English corpus.
Similarly, entity-relationshipgraphs can be built over other language corpora, asFigure 1(b) demonstrates the corresponding resultsfor the same query, from Renlifang2 on ChineseWebcorpus.
From this point on, for the sake of simplic-ity, we refer to English and Chinese graphs, simplyasGe andGc respectively.
Though we illustrate withEnglish-Chinese pairs in the paper, our method canbe easily adapted to other language pairs.In particular, we propose a novel approach of ab-stracting entity translation as a graph matching prob-lem of two graphsGe andGc in Figures 1(a) and (b).Specifically, the similarity between two nodes veand vc in Ge and Gc is initialized as their transliter-ation similarity, which is iteratively refined based onrelational similarity obtained from monolingual co-occurrences.
To illustrate this, an English news ar-ticle mentioning ?Bill Gates?
and ?Melinda Gates?evidences a relationship between the two entities,which can be quantified from their co-occurrencesin the entire English Web corpus.
Similarly, wecan mine Chinese news articles to obtain the re-lationships between ???
?
and ???H???.
Once these two bilingual graphs of people andtheir relationships are harvested, entity translationcan leverage these parallel relationships to furtherevidence the mapping between translation pairs, asFigure 1(c) illustrates.To highlight the advantage of our proposed ap-proach, we compare our results with commercialmachine translators (1) Engkoo3 developed in Mi-crosoft Research Asia and (2) Google Translator4.In particular, Figure 2 reports the precision for twogroups?
?heads?
that belong to top-100 popular peo-ple (determined by the number of hits), among ran-domly sampled 304 people names5 from six graphpairs of size 1,000 each, and the remaining ?tails?.Commercial translators such as Google, leveraging2http://renlifang.msra.cn3http://www.engkoo.com4http://translate.google.com5See Section 4 for the sampling process.Ours Google Engkoo00.10.20.30.40.50.60.70.8PrecisionTailHeadFigure 2: Comparison for Head and Tail datasetsbilingual co-occurrences that are scarce for tails,show significantly lower precision for tails.
Mean-while, our work, depending solely on monolin-gual co-occurrences, shows high precision, for bothheads and tails.Our focus is to boost translation accuracy forlong tails with non-trivial Web occurrences in eachmonolingual corpus, but not with much bilingual co-occurrences, e.g., researchers publishing actively intwo languages but not famous enough to be featuredin multi-lingual Wikipedia entries or news articles.As existing translators are already highly accuratefor popular heads, this focus well addresses the re-maining challenges for entity translation.To summarize, we believe that this paper has thefollowing contributions:?
We abstract entity translation problem asa graph mapping between entity-relationshipgraphs in two languages.?
We develop an effective matching algo-rithm leveraging both pronunciation and co-occurrence similarity.
This holistic approachcomplements existing approaches and en-hances the translation coverage and accuracy.?
We validate the effectiveness of our approachusing various real-life datasets.The rest of this paper is organized as follows.
Sec-tion 2 reviews existing work.
Section 3 then devel-ops our framework.
Section 4 reports experimentalresults and Section 5 concludes our work.431(a) English PeopleEntityCube Ge (b) Chinese Renlifang Gc(c) Abstracting translation as graph mappingFigure 1: Illustration of entity-relationship graphs2 Related WorkIn this section, we first survey related efforts, cate-gorized into transliteration-based and corpus-basedapproaches.
Our approach leveraging both is com-plementary to these efforts.2.1 Transliteration-based ApproachesMany name translations are loosely based onphonetic similarity, which naturally inspirestransliteration-based translation of finding thetranslation with the closest pronunciation similarity,using either rule-based (Wan and Verspoor, 1998) orstatistical (Knight and Graehl, 1998; Li et al, 2004)approaches.
However, people are free to designatearbitrary bilingual names of little (or none) pho-netic similarity, for which the transliteration-basedapproach is not effective.2.2 Corpus-based ApproachesCorpus-based approach can mine arbitrary transla-tion pairs, by mining bilingual co-occurrences fromparallel and comparable bilingual corpora.
Usingparallel corpora (Kupiec, 1993; Feng et al, 2004),e.g., bilingual Wikipedia entries on the same per-son, renders high accuracy but suffers from highscarcity.
To alleviate such scarcity, (Fung and Yee,4321998; Shao and Ng, 2004) explore a more vast re-source of comparable corpora, which share no par-allel document- or sentence-alignments as in paral-lel corpora but describe similar contents in two lan-guages, e.g., news articles on the same event.
Al-ternatively, (Lin et al, 2008) extracts bilingual co-occurrences from bilingual sentences, such as an-notating terms with their corresponding translationsin English inside parentheses.
Similarly, (Jiang etal., 2009) identifies potential translation pairs frombilingual sentences using lexical pattern analysis.2.3 Holistic ApproachesThe complementary strength of the above two ap-proaches naturally calls for a holistic approach,such as recent work combining transliteration-and corpus-based similarity mining bilingual co-occurrences using general search engines.
Specifi-cally, (Al-Onaizan and Knight, 2002) uses translit-eration to generate candidates and then web corporato identify translations.
Later, (Jiang et al, 2007)enhances to use transliteration to guide web mining.Our work is also a holistic approach, but leverag-ing significantly larger corpora, specifically by ex-ploiting monolingual co-occurrences.
Such expan-sion enables to translate ?long-tail?
people entitieswith non-trivial Web occurrences in each monolin-gual corpus, but not much bilingual co-occurrences.Specifically, we initialize name pair similarity usingtransliteration-based approach, and iteratively rein-forces base similarity using relational similarity.3 Our FrameworkGiven two graphsGe = (Ve, Ee) andGc = (Vc, Ec)harvested from English and Chinese corpora respec-tively, our goal is to find translation pairs, or a set Sof matching node pairs such that S ?
Ve ?
Vc.
LetR be a |Ve|-by-|Vc| matrix where each Rij denotesthe similarity between two nodes i ?
Ve and j ?
Vc.Overall, with the matrix R, our approach consistsof the following three steps, as we will discuss in thefollowing three sections respectively:1.
Initialization: computing base translation sim-ilarities Rij between two entity nodes usingtransliteration similarity2.
Reinforcement model: reinforcing the trans-lation similarities Rij by exploiting the mono-lingual co-occurrences3.
Matching extraction: extracting the matchingpairs from the final translation similarities Rij3.1 Initialization with TransliterationWe initialize the translation similarity Rij as thetransliteration similarity.
This section explains howto get the transliteration similarity between Englishand Chinese names using an unsupervised approach.Formally, let an English name Ne =(e1, e2, ?
?
?
, en) and a Chinese name Nc =(c1, c2, ?
?
?
, cm) be given, where ei is an Englishword and Ne is a sequence of the words, and ciis a Chinese character and Nc is a sequence ofthe characters.
Our goal is to compute a scoreindicating the similarity between the pronunciationsof the two names.We first convert Nc into its Pinyin representationPYc = (s1, s2, ?
?
?
, sm), where si is the Pinyin rep-resentation of ci.
Pinyin is the romanization rep-resentation of pronunciation of Chinese character.For example, the Pinyin representation of Ne =(?Barack?, ?Obama?)
is PYc =(?ba?, ?la?, ?ke?,?ao?, ?ba?, ?ma?).
The Pinyin representations ofChinese characters can be easily obtained from Chi-nese character pronunciation dictionary.
In our ex-periments, we use an in-house dictionary, whichcontains pronunciations of 20, 774 Chinese charac-ters.
For the Chinese characters having multiple pro-nunciations, we only use the most popular one.Calculation of transliteration similarity betweenNe and Nc is now transformed to calculation of pro-nunciation similarity between Ne and PYc.
Becauseletters in Chinese Pinyins and English strings arepronounced similarly, we can further approximatepronunciation similarity between Ne and PYc us-ing their spelling similarity.
In this paper, we useEdit Distance (ED) to measure the spelling similar-ity.
Moreover, since words in Ne are transliteratedinto characters in PYc independently, it is more ac-curate to compute the ED between Ne and PYc, i.e.,EDname(Ne, PYc), as the sum of the EDs of allcomponent transliteration pairs, i.e., every ei in Neand its corresponding transliteration (si) in PYc.
Inother words, we need to first align all sj?s in PYcwith corresponding ei in Ne based on whether they433are translations of each other.
Then based on thealignment, we can calculate EDname(Ne, PYc) us-ing the following formula.EDname(Ne, PYc) =?iED(ei, esi) (1)where esi is a string generated by concatenating allsi?s that are aligned to ei and ED(ei, esi) is theEdit Distance between ei and esi, i.e., the mini-mum number of edit operations (including inser-tion, deletion and substitution) needed to transformei into esi.
Because an English word usually con-sists of multiple syllables but every Chinese charac-ter consists of only one syllable, when aligning ei?swith sj?s, we add the constraint that each ei is al-lowed to be aligned with 0 to 4 si?s but each si canonly be aligned with 0 to 1 ei.
To get the align-ment between PYc and Ne which has the minimalEDname(Ne, PYc), we use a Dynamic Program-ming based algorithm as defined in the followingformula:EDname(N1,ie , PY 1,jc ) = min(EDname(N1,i?1e , PY 1,jc ) + Len(ei),EDname(N1,ie , PY 1,j?1c ) + Len(sj),EDname(N1,i?1e , PY 1,j?1c ) + ED(ei, sj),EDname(N1,i?1e , PY 1,j?2c ) + ED(ei, PY j?1,jc ),EDname(N1,i?1e , PY 1,j?3c ) + ED(ei, PY j?2,jc ),EDname(N1,i?1e , PY 1,j?4c ) + ED(ei, PY j?3,jc ))where, given a sequence X = (x1, x2, ?
?
?
)such that xi is a word, X i,j is the subsequence(xi, xi+1, ?
?
?
, xj) of X and Len(X) is the numberof letters except spaces in the sequence X .
For in-stance, the minimal Edit Distance between the En-glish name ?Barack Obama?
and the Chinese Pinyinrepresentation ?ba la ke ao ba ma?
is 4, as thebest alignment is: ?Barack?
?
?ba la ke?
(ED: 3),?Obama??
?ao ba ma?
(ED: 1).
Finally the translit-eration similarity between Nc and Ne is calculatedusing the following formula.Simtl(Nc, Ne) = 1?EDname(Ne, PYc)Len(PYc) + Len(Ne)(2)For example, Simtl(?Barack Obama?, ??n.???j?)
is 1?
411+12 = 0.826.3.2 Reinforcement ModelFrom the initial similarity, we model our problem asan iterative approach that iteratively reinforces thesimilarityRij of the nodes i and j from the matchingsimilarities of their neighbor nodes u and v.The basic intuition is built on exploiting the sim-ilarity between monolingual co-occurrences of twodifferent languages.
In particular, we assume twoentities with strong relationship co-occur frequentlyin both corpora.
In order to express this intuition, weformally define an iterative reinforcement model asfollows.
Let Rtij denote the similarity of nodes i andj at t-th iteration:Rt+1ij = ??(u,v)k?Bt(i,j,?
)Rtuv2k+ (1?
?
)R0ij (3)The model is expressed as a linear combinationof (a) the relational similarity?Rtuv/2k and (b)transliteration similarity R0ij .
(?
is the coefficientfor interpolating two similarities.
)In the relational similarity, Bt(i, j, ?)
is an or-dered set of the best matching pairs between neigh-bor nodes of i and ones of j such that ?
(u, v)k ?Bt(i, j, ?
), Rtuv ?
?, where (u, v)k is the match-ing pair with k-th highest similarity score.
We con-sider (u, v) with similarity over some threshold ?,or Rtuv ?
?, as a matching pair.
In this neighbormatching process, if many-to-many matches exist,we select only one with the greatest matching score.Figure 3 describes such matching process more for-mally.
N(i) andN(j) are the sets of neighbor nodesof i and j, respectively, and H is a priority queuesorting pairs in the decreasing order of similarityscores.Meanwhile, note that, in order to express thatthe confidence for matching (i, j) progressively con-verges as the number of matched neighbors in-creases, we empirically use decaying coefficient1/2k for Rtuv, because?
?k=1 1/2k = 1.3.3 Matching ExtractionAfter the convergence of the above model, we getthe |Ve|-by-|Vc| similarity matrix R?.
From thismatrix, we extract one-to-one matches maximizingthe overall similarity.More formally, this problem can be stated asthe maximum weighted bipartite matching (West,4341.2.3.4.5.6.7.8.9.10.11.12.Bt(i, j, ?)?
{}?u ?
N(i),?v ?
N(j) : H.push(u, v;Rtuv)while H is not empty do(u, v; s)?
H.pop()if s < ?
thenbreakend ifif neither u nor v are matched yet thenBt(i, j, ?)?
Bt(i, j, ?)
?
{(u, v)}end ifend whilereturn Bt(i, j, ?
)Figure 3: How to get the ordered set Bt(i, j, ?)2000)?
Given two groups of entities Ve and Vc fromthe two graphs Ge and Gc, we can build a weightedbipartite graph is G = (V,E), where V = Ve ?
Vcand E is a set of edges (u, v) with weight R?uv.
Tofilter out null alignment, we construct only the edgeswith weight R?uv ?
?.
From this bipartite graph,the maximum weighted bipartite matching problemfinds a set of pairwise non-adjacent edges S ?
Esuch that?
(u,v)?S R?uv is the maximum.
Well-known algorithms include Hungarian algorithm withtime complexity of O(|V |2 log |V |+ |V ||E|) (West,2000).In this paper, to speed up processing, we considera greedy alternative with the following steps?
(1)choose the pair with the highest similarity score, (2)remove the corresponding row and column from thematrix, and (3) repeat (1) and (2) until their match-ing scores are over a specific threshold ?.4 ExperimentsThis section reports our experimental results to eval-uate our proposed approach.
First, we report our ex-perimental setting in Section 4.1.
Second, we vali-date the effectiveness and the scalability of our ap-proach over a real-life dataset in Section 4.2.4.1 Experimental SettingsThis section describes (1) how we collect the En-glish and Chinese EntityCube datasets, (2) how tobuild ground-truth test datasets for evaluating ourframework, and (3) how to set up three parameters?, ?, and ?.First, we crawled Ge = (Ve, Ee) and Gc =(Vc, Ec) from English and Chinese EntityCubes.Specifically, we built a graph pairs (Ge, Gc) expand-ing from a ?seed pair?
of nodes se ?
Ve and sc ?
Vcuntil the number of nodes for each graph becomes1,0006.
More specifically, when we build a graphGe by expanding from se, we use a queue Q. Wefirst initialize Q by pushing the seed node se.
Wethen iteratively pop a node ve from Q, save ve intoVe, and push its neighbor nodes in decreasing orderof co-occurrence scores with ve.
Similarly, we canget Gc from a counterpart seed node vc.
By usingthis procedure, we built six graph pairs from six dif-ferent seed pairs.
In particular, the six seed nodesare English names and its corresponding Chinesenames representing a wide range of occupation do-mains (e.g., ?Barack Obama,?
?Bill Gates,?
?BritneySpears,?
?Bruno Senna,?
?Chris Paul,?
and ?Eminem?
)as Table 1 depicts.
Meanwhile, though we demon-strate the effectiveness of the proposed method formining name translations in Chinese and Englishlanguages, the method can be easily adapted to otherlanguage pairs.Table 1: Summary for graphs and test datasets obtainedfrom each seed pairi |Ve|, |Vc| |Ti| English Name Chinese Name1 1,000 51 Barack Obama ?n.??
?j2 1,000 52 Bill Gates ?
?3 1,000 40 Britney Spears Y}??
??4 1,000 53 Bruno Senna Y0L?
?5 1,000 51 Chris Paul .????
[6 1,000 57 Eminem ?
??Second, we manually searched for about 50?ground-truth?
matched translations for each graphpair to build test datasets Ti, by randomly selectingnodes within two hops7 from the seed pair (se, sc),since nodes outside two hops may include nodeswhose neighbors are not fully crawled.
More specif-ically, due to our crawling process expanding to addneighbors from the seed, the nodes close to the seedhave all the neighbors they would have in the fullgraph, while those far from the node may not.
In or-der to pick the nodes that well represent the actual6Note, this is just a default setting, which we later increasefor scalability evaluation in Figure 6.7Note that the numbers of nodes within two hops in Ge andGc are 327 and 399 on average respectively.435neighbors, we built test datasets among those withintwo hops.
However, this crawling is used for theevaluation sake only, and thus does not suggest thebias in our proposed framework.
Table 1 describesthe size of such test dataset for each graph pair.Lastly, we set up the three parameters ?, ?, and?
using 6-fold cross validation with 6 test datasetsTi?s of the graphs.
More specifically, for eachdataset Ti, we decide ?i and ?i such that averageMRR for the other 5 test datasets is maximized.
(About MRR, see more details of Equation (4) inSection 4.2.)
We then decide ?i such that averageF1-score is maximized.
Figure 4 shows the averageMRR for ?i and ?i with default values ?
= 0.66and ?
= 0.2.
Based on these results, we set ?i withvalues {0.2, 0.15, 0.2, 0.15, 0.2, 0.15} that optimizeMRR in datasets T1, .
.
.
T6, and similarly ?i with{0.67, 0.65, 0.67, 0.67, 0.65, 0.67}.
We also set ?iwith values {0.63, 0.63, 0.61, 0.61, 0.61, 0.61} opti-mizing F1-score with the same default values ?
=0.2 and ?
= 0.66.
We can observe the variancesof optimal parameter setting values are low, whichsuggests the robustness of our framework.4.2 Experimental ResultsThis section reports our experimental results usingthe evaluation datasets explained in previous sec-tion.
For each graph pair, we evaluated the ef-fectiveness of (1) reinforcement model using MRRmeasure in Section 4.2.1 and (2) overall frameworkusing precision, recall, and F1 measures in Sec-tion 4.2.2.
We also validated (3) scalability of ourframework over larger scale of graphs (with up tofive thousand nodes) in Section 4.2.3.
(In all experi-mental results, Bold numbers indicate the best per-formance for each metric.
)4.2.1 Effectiveness of reinforcement modelWe evaluated the reinforcement model overMRR (Voorhees, 2001), the average of the recipro-cal ranks of the query results as the following for-mula:MRR = 1|Q|?q?Q1rankq(4)Each q is a ground-truth matched pair (u, v) suchthat u ?
Ve and v ?
Vc, and rankq is the rank of thesimilarity score of Ruv among all Ruk?s such thatk ?
Vc.
Q is a set of such queries.
By comparingMRRs for two matricesR0 andR?, we can validatethe effectiveness of the reinforcement model.?
Baseline matrix (R0): using only the translit-eration similarity score, i.e., without reinforce-ment?
Reinforced matrix (R?
): using the reinforcedsimilarity score obtained from Equation (3)Table 2: MRR of baseline and reinforced matricesSet MRRBaseline R0 Reinforced R?T1 0.6964 0.8377T2 0.6213 0.7581T3 0.7095 0.7989T4 0.8159 0.8378T5 0.6984 0.8158T6 0.5982 0.8011Average 0.6900 0.8082We empirically observed that the iterative modelconverges within 5 iterations.
In all experiments, weused 5 iterations for the reinforcement.Table 2 summarizes our experimental results.
Asthese figures show, MRR scores significantly in-crease after applying our reinforcement model ex-cept for the set T4 (on average from 69% to 81%),which indirectly shows the effectiveness of our rein-forcement model.4.2.2 Effectiveness of overall frameworkBased on the reinforced matrix, we evaluatedthe effectiveness of our overall matching frameworkusing the following three measures?
(1) precision:how accurately the method returns matching pairs,(2) recall: how many the method returns correctmatching pairs, and (3) F1-score: the harmonicmean of precision and recall.
We compared our ap-proach with a baseline, mapping two graphs withonly transliteration similarity.?
Baseline: in matching extraction, using R0 asthe similarity matrix by bypassing the rein-forcement step?
Ours: using R?, the similarity matrix con-verged by Equation (3)4360.1 0.15 0.2 0.25 0.30.770.780.790.80.810.820.830.840.85?
(?=0.66)AVG(MRR)?1?2?3?4?5?60.61 0.63 0.65 0.67 0.690.740.760.780.80.820.84?
(?=0.2)AVG(MRR)?1?2?3?4?5?60.57 0.59 0.61 0.63 0.650.680.690.70.710.720.730.74?
(?=0.2, ?=0.66)AVG(F1?score)?1?2?3?4?5?6Figure 4: Parameter setup for ?, ?, and ?In addition, we compared ours with the machinetranslators of Engkoo and Google.
Table 3 summa-rizes our experimental results.As this table shows, our approach results in thehighest precision (about 80% on average) withoutcompromising the best recall of Google, i.e., 61%of Google vs. 63% of ours.
Overall, our approachoutperforms others in all three measures.Meanwhile, in order to validate the translation ac-curacy over popular head and long-tail, as discussedin Section 1, we separated the test data into twogroups and analyzed the effectiveness separately.Figure 5 plots the number of hits returned for thenames from Google search engine.
According to thedistribution, we separate the test data into top-100popular people with the highest hits and the remain-ing, denoted head and tail, respectively.0 50 100 150 200 250 300 350104105106107108Number of namesNumber of hits in GoogleFigure 5: Distribution over number of hitsTable 4 shows the effectiveness with bothdatasets, respectively.
As difference of the effective-ness between tail and head (denoted diff ) with re-spect to three measures shows, our approach showsstably high precision, for both heads and tails.4.2.3 ScalabilityTo validate the scalability of our approach, weevaluated the effectiveness of our approach over thenumber of nodes in two graphs.
We built larger sixgraph pairs (Ge, Gc) by expanding them from theseed pairs further until the number of nodes reaches5,000.
Figure 6 shows the number of matched trans-lations according to such increase.
Overall, the num-ber of matched pairs linearly increases as the num-ber of nodes increases, which suggests scalability.The ratio of node overlap in two graphs is about be-tween 7% and 9% of total node size.1000 2000 3000 4000 500050100150200250300350|Ve| and |Vc|# matchedtranslationsFigure 6: Matched translations over |Ve| and |Vc|5 ConclusionThis paper abstracted name translation problem as amatching problem of two entity-relationship graphs.This novel approach complements existing nametranslation work, by not requiring rare resourcesof parallel or comparable corpus yet outperformingthe state-of-the-art.
More specifically, we combinebilingual phonetic similarity and monolingual Webco-occurrence similarity, to compute a holistic no-tion of entity similarity.
To achieve this goal, we de-437Table 3: Precision, Recall, and F1-score of Baseline, Engkoo, Google, and Ours over test sets TiSet Precision Recall F1-scoreEngkoo Google Baseline Ours Engkoo Google Baseline Ours Engkoo Google Baseline OursT1 0.5263 0.4510 0.5263 0.8974 0.3922 0.4510 0.1961 0.6863 0.4494 0.4510 0.2857 0.7778T2 0.7551 0.75 0.7143 0.8056 0.7115 0.75 0.2885 0.5577 0.7327 0.75 0.4110 0.6591T3 0.5833 0.7925 0.5556 0.7949 0.5283 0.7925 0.1887 0.5849 0.5545 0.7925 0.2817 0.6739T4 0.5 0.45 0.7368 0.7353 0.425 0.45 0.35 0.625 0.4595 0.45 0.4746 0.6757T5 0.6111 0.3137 0.5 0.7234 0.4314 0.3137 0.1765 0.6667 0.5057 0.3137 0.2609 0.6939T6 0.5636 0.8947 0.6 0.8605 0.5438 0.8947 0.1053 0.6491 0.5536 0.8947 0.1791 0.74AVG 0.5899 0.6086 0.6055 0.8028 0.5054 0.6086 0.2175 0.6283 0.5426 0.6086 0.3155 0.7034Table 4: Precision, Recall, and F1-score of Engkoo, Google, and Ours with head and tail datasetsMethod Precision Recall F1-scorehead tail diff head tail diff head tail diffEngkoo 0.6082 0.5854 0.0229 0.59 0.4706 0.1194 0.5990 0.5217 0.0772Google 0.75 0.5588 0.1912 0.75 0.5588 0.1912 0.75 0.5588 0.1912Ours 0.8462 0.7812 0.0649 0.66 0.6127 0.0473 0.7416 0.6868 0.0548veloped a graph alignment algorithm that iterativelyreinforces the matching similarity exploiting rela-tional similarity and then extracts correct matches.Our evaluation results empirically validated the ac-curacy of our algorithm over real-life datasets, andshowed the effectiveness on our proposed perspec-tive.AcknowledgmentsThis work was supported by Microsoft ResearchAsia NLP theme funding and MKE (Ministry ofKnowledge Economy), Korea, under the ITRC (In-formation Technology Research Center) supportprogram supervised by the IITA (Institute for In-formation Technology Advancement) (IITA-2009-C1090-0902-0045).ReferencesYaser Al-Onaizan and Kevin Knight.
2002.
Trans-lating Named Entities Using Monolingual and Bilin-gual Resources.
In Proceedings of the 40th AnnualMeeting on Association for Computational Linguistics(ACL?02), pages 400?408.
Association for Computa-tional Linguistics.Donghui Feng, Yajuan Lu?, and Ming Zhou.
2004.A New Approach for English-Chinese Named En-tity Alignment.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP?04), pages 372?379.
Association for Com-putational Linguistics.Pascale Fung and Lo Yuen Yee.
1998.
An IR Ap-proach for Translating New Words from Nonparal-lel,Comparable Texts.
In Proceedings of the 17th In-ternational Conference on Computational Linguistics(COLING?98), pages 414?420.
Association for Com-putational Linguistics.Long Jiang, Ming Zhou, Lee feng Chien, and Cheng Niu.2007.
Named Entity Translation withWebMining andTransliteration.
In Proceedings of the 20th Interna-tional Joint Conference on Artificial Intelligence (IJ-CAI?07), pages 1629?1634.
Morgan Kaufmann Pub-lishers Inc.Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu, andQingsheng Zhu.
2009.
Mining Bilingual Data fromthe Web with Adaptively Learnt Patterns.
In Proceed-ings of the 47th Annual Meeting of the Association forComputational Linguistics (ACL?09), pages 870?878.Association for Computational Linguistics.Kevin Knight and Jonathan Graehl.
1998.
Ma-chine Transliteration.
Computational Linguistics,24(4):599?612.Julian Kupiec.
1993.
An Algorithm for finding NounPhrase Correspondences in Bilingual Corpora.
In Pro-ceedings of the 31th Annual Meeting of the Associationfor Computational Linguistics (ACL?93), pages 17?22.Association for Computational Linguistics.Haizhou Li, Zhang Min, and Su Jian.
2004.
A JointSource-Channel Model for Machine Transliteration.In Proceedings of the 42nd Annual Meeting on Associ-ation for Computational Linguistics (ACL?04), pages159?166.
Association for Computational Linguistics.Dekang Lin, Shaojun Zhao, Benjamin Van Durme, andMarius Pasca.
2008.
Mining Parenthetical Transla-438tions from the Web by Word Alignment.
In Proceed-ings of the 46th Annual Meeting of the Associationfor Computational Linguistics (ACL?08), pages 994?1002.
Association for Computational Linguistics.Li Shao and Hwee Tou Ng.
2004.
Mining New WordTranslations from Comparable Corpora.
In Proceed-ings of the 20th International Conference on Computa-tional Linguistics (COLING?04), pages 618?624.
As-sociation for Computational Linguistics.Ellen M. Voorhees.
2001.
The trec question answeringtrack.
Natural Language Engineering, 7(4):361?378.Stephen Wan and Cornelia Maria Verspoor.
1998.
Auto-matic English-Chinese Name Transliteration for De-velopment of Multilingual Resources.
In Proceed-ings of the 17th International Conference on Compu-tational Linguistics (COLING?98), pages 1352?1356.Association for Computational Linguistics.Douglas Brent West.
2000.
Introduction to Graph The-ory.
Prentice Hall, second edition.439
