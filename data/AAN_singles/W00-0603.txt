A Rule-based Question Answering System for ReadingComprehension TestsE l len  R i lo f f  and Michae l  The lenDepartment  of Computer  ScienceUniversity of UtahSalt Lake City, Utah 84112{riloff, thelenm}@cs.utah.eduAbst rac tWe have developed a rule-based system, Quarc,that can reada short story and find the sentencein the story that best answers a given question.Quarc uses heuristic rules that look for lexicaland semantic lues in the question and the story.We have tested Quarc on reading comprehen-sion tests typically given to children in grades3-6.
Overall, Quarc found the correct sentence40% of the time, which is encouraging given thesimplicity of its rules.1 In t roduct ionIn the United States, we evaluate the readingability of children by giving them reading com-prehension tests.
These test typically consist ofa short story followed by questions.
Presum-ably, the tests are designed so that the readermust understand important aspects of the storyto answer the questions correctly.
For thisreason, we believe that reading comprehensiontests can be a valuable tool to assess the stateof the art in natural anguage understanding.These tests are especially challenging becausethey can discuss virtually any topic.
Conse-quently, broad-coverage natural language pro-cessing (NLP) techniques must be used.
But thereading comprehension tests also require seman-tic understanding, which is difficult to achievewith broad-coverage t chniques.We have developed a system called Quarcthat "takes" reading comprehension tests.Given a story and a question, Quarc finds thesentence in the story that best answers thequestion.
Quarc does not use deep languageunderstanding or sophisticated techniques, yetit achieved 40% accuracy in our experiments.Quarc uses hand-crafted heuristic rules thatlook for lexical and semantic lues in the ques-tion and the story.
In the next section, we de-scribe the reading comprehension tests.
In thefollowing sections, we describe the rules used byQuarc and present experimental results.2 Read ing  Comprehens ion  TestsFigure 1 shows an example of a reading compre-hension test from Remedia Publications.
Eachtest is followed by five "WH" questions: WHO,WHAT, WHEN, WHERE, and WHY.
1 The answersto the questions typically refer to a string inthe text, such as a name or description, whichcan range in length from a single noun phraseto an entire clause or sentence.
The answers toWHEN and WHERE questions are also sometimesinferred from the dateline of the story.
For ex-ample, (EGYPT, 1951) contains the answer tothe WHEN question in Figure 1.Ideally, a natural anguage processing systemwould produce the exact answer to a question.Identifying the precise boundaries of the answercan be tricky, however.
We will focus on thesomewhat easier task of identifying the sentencethat contains the answer to a question.3 A Ru le -based  System for  Quest ionAnswer ingQuarc (QUestion Answering for Reading Com-prehension) is a rule-based system that uses lex-ical and semantic heuristics to look for evidencethat a sentence contains the answer to a ques-tion.
Each type of WH question looks for differ-ent types of answers, so Quarc uses a separateset of rules for each question type (WHO, WHAT,WHEN, WHERE, WHY).Given a question and a story, Quarc parsesthe question and all of the sentences in the storyusing our partial parser Sundance.
Much of1There is also a lone HOW question in the data set,but we ignored it.13Tomb Keeps  I ts  Secrets(EGYPT, 1951) - A tomb was tound this year.
It was a tomb built for a king.
The king lived morethan 4,000 years ago.
His home was in Egypt.For years, no one saw the tomb.
It was carved deep in rock.
The wind blew sand over the top andhid it.
Then a team of diggers came along.
Their job was to search for hidden treasures.What they found thrilled them.
Jewels and gold were found in the tomb.
The king's treasureswere buried inside 132 rooms.The men opened a 10-foot-thick door.
It was 130 feet below the earth.
Using torches, they saw acase.
"It must contain the king's mummy!"
they said.
A mummy is a body wrapped in sheets.With great care, the case was removed.
It was taken to a safe place to be opened.
For two hours,workers tried to lift the lid.
At last, they got it off.Inside they saw ... nothing!
The case was empty.
No one knows where the body is hidden.
A newmystery has begun.1.
Who was supposed to be buried in the tomb?2.
Wha_t_is a mummy?3.
When did this .story happen?4.
Where was the 10-foot-thick door found?5.
Why was the body gone?Figure 1: Sample Reading Comprehension Testthe syntactic analysis is not used, but Quarcdoes use the morphological analysis, part-of-speech tagging, semantic lass tagging, and en-tity recognition.
The rules are applied to eachsentence in the story, as well as the title of thestory, with the exception that the title is notconsidered for WHY questions.
The dateline isalso a possible answer for WHEN and WHEREquestions and is evaluated using a special set ofdateline rules.Each rule awards a certain number of pointsto a sentence.
After all of the rules have beenapplied, the sentence (or dateline) that obtainsthe highest score is returned as the answer.All of the question types share a commonWordMatch function, which counts the numberof words that appear in both the question andthe sentence being considered.
The WordMatchfunction first strips a sentence of stopwords 2,and then matches the remaining words againstthe words in the question.
Two words match ifthey share the same morphological root.
Verbsseem to be especially important for recogniz-ing that a question and sentence are related, soverb matches are weighted more heavily thannon-verb matches.
Matching verbs are awarded6 points each and other matching words areawarded 3 points each.The other rules used by Quarc look for a vari-~We used a stopword list containing 41 words, mostlyprepositions, pronouns, and auxiliary verbs.ety of clues.
Lexical clues look for specific wordsor phrases.
Unless a rule indicates otherwise,words are compared using their morphologicalroots.
Some rules can be satisfied by any of sev-eral lexical items; these rules are written usingset notation (e.g., { yesterday, today, tomorrow}).Some rules also look for semantic lasses, whichwe will write in upper case (e.g., HUMAN).
Ourparser uses a dictionary and a semantic hierar-chy, so that words can be defined with semanticclasses.
The semantic lasses used by Quarc areshown below, along with a description of thewords assigned to each class.HUMAN:  2608 words, 3 including commonfirst names, last names, titles such as"Dr." and "Mrs.", and about 600 occupa-tion words acquired from WordNet (Miller,1990).LOCATION:  344 words, including 204 coun-try names and the 50 United States.MONTH:  the 12 months of the year.TIME:  667 words, 600 of which are enumer-ated years from 1400-1999.
The others aregeneral time expressions, including the 12months of the year.3About 2000 words came from the Social Security Ad-ministration's li t of the top 1000 names for each genderin 1998: www.ssa.gov/OACT/NOTES/note139/1998/top1000in98.html.14Our parser also recognizes two types of se-mantic entities: proper nouns and names.
APROPER__NOUN is defined as a noun phrasein which all words are capitalized.
A NAME isdefined as a PROPER_NOUN that contains atleast one HUMAN word.Each rule awards a specific number of pointsto a sentence, depending on how strongly therule believes that it found the answer.
A rulecan assign four possible point values: c lue(+3), good_clue (+4), conf ident  (+6), ands lam_dunk  (+20).
These point values werebased on our intuitions and worked well empir-ically, but they are not well justified.
The mainpurpose of these values is to assess the relativeimportance of each clue.Figure 2 shows the WHO rules, which use threefairly general heuristics as well as the Word-Match function (rule #1).
If the question (Q)does not contain any names, then rules #2 and#3 assume that the question is looking for aname.
Rule #2 rewards sentences that containa recognized NAME, and rule #3 rewards sen-tences that contain the word "name".
Rule #4awards points to all sentences that contain ei-ther a name or a reference to a human (often anoccupation, such as "writer").
Note that morethan one rule can apply to a sentence, in whichcase the sentence is awarded points by all of therules that applied.1.
Score(S) += WordMatch(Q,S)2.
If-~ contains(Q,NAME) andcontains(S,NAME)Then Score(S) += confident3.
If ~ contains(Q,NAME) andcontains(S,name)Then Score(S) += good_clue4.
If contains(S,{NAME,HUMAN})Then Score(S) += good_clueFigure 2: WHO RulesThe WHAT questions were the most difficultto handle because they sought an amazing va-riety of answers.
But Figure 3 shows a few spe-cific rules that worked reasonably well.
Rule #1is the generic word matching function sharedby all question types.
Rule #2 rewards sen-tences that contain a date expression if the ques-tion contains a month of the year.
This rulehandles questions that ask what occurred ona specific date.
We also noticed several "whatkind?"
questions, which looked for a descriptionof an object.
Rule #3 addresses these questionsby rewarding sentences that contain the word"call" or "from" (e.g., "It is called..." or "It ismade from ...").
Rule #4 looks for words asso-ciated with names in both the question and sen-tence.
Rule #5 is very specific and recognizesquestions that contain phrases such as "nameof <x>" or "name for <x>".
Any sentencethat contains a proper noun whose head nounmatches <x> will be highly rewarded.
For ex-ample, the question "What is the name of thecreek?"
is answered by a sentence that containsthe noun phrase "Pigeon Creek".1.
Score(S) += WordMatch(Q,S)2.
If contains(Q,MONTH) andcontains(S,{ today, yesterday,tomorrow, last night})Then Score(S) += clue3.
If contains(Q,kind) andcontains (S, { call,from})Then Score(S) += good_due4.
If contains(Q,narne) andeontains( S, { name, call, known} )Then Score += slam_dunk5.
If contains(Q,name+PP) andcontains(S,PROPER_NOUN) andcontains(PROPER_NOUN,head(PP))Then Score(S) += slam_dunkFigure 3: WHAT RulesThe rule set for WHEN questions, shown inFigure 4, is the only rule set that does not ap-ply the WordMatch function to every sentencein the story.
WHEN questions almost always re-quire a TIME expression, so sentences that donot contain a TIME expression are only con-sidered in special cases.
Rule #1 rewards allsentences that contain a TIME expression withgood_c lue points as well as WordMatch points.The remaining rules look-for specific words thatsuggest a duration of time.
Rule #3 is inter-esting because it recognizes that certain verbs("begin", "start") can be indicative of time evenwhen no specific time is mentioned.The WHERE questions almost always look forspecific locations, so the WHERE rules are veryfocused.
Rule #1 applies the general wordmatching function and Rule #2 looks for sen-15:'."1.
If contains(S,TIME)Then Score(S) += good_clueScore(S) += WordMatch(Q,S)2.
If contains(Q,the last) andcontains(S,{first, last, since, ago})Then Score(S) += slam_dunk?
3.
If contains( Q, { start, begin}) andcontains(S,{ start, begin,since,year})Then Score(S) += slam_dunkFigure 4: WHEN Rules1.
Score(S) += WordMatch(Q,S)2.
If contains(S,LocationPrep)Then Score(S) += good_clue3.
If contains(S, LOCATION)Then Score(S) += confidentFigure 5: WHERE Rulestences with a location preposition.
Quarc rec-ognizes 21 prepositions as being associated withlocations, such as "in", "at", "near", and "in-side".
Rule #3 looks for sentences that containa word belonging to the LOCATION semanticclass.WHY questions are handled differently thanother questions.
The WHY rules are based onthe observation that the answer to a WHY ques-tion often appears immediately before or im-mediately after the sentence that most closelymatches the question.
We believe that this isdue to the causal nature of WHY questions.First, all sentences are assigned a score usingthe WordMatch function.
Then the sentenceswith the top score are isolated.
We will refer tothese sentences as BEST.
Every sentence scoreis then reinitialized to zero and the WHY rules,shown in Figure 6, are applied to every sentencein the story.Rule #1 rewards all sentences that producedthe best WordMatch score because they areplausible candidates.
Rule #2 rewards sen-tences that immediately precede a best Word-Match sentence, and Rule #3 rewards sentencesthat immediately follow a best WordMatch sen-tence.
Rule #3 gives a higher score than Rules#1 and #2 because we observed that WHY an-swers are somewhat more likely to follow thebest WordMatch sentence.
Finally, Rule #4 re-wards sentences that contain the word "want"1.
I fSeBESTThen Score(S) +-- clue2.
If S immed, precedes member of BESTThen Score(S) += clue3.
If S immed, follows member of BESTThen Score(S) += good_clue4.
If contains(S,want)Then Score(S) += good_clue5.
If contains(S, { so, because} )Then Score(S) += good_clueFigure 6: WHY Rulesand Rule #5 rewards sentences that contain theword "so" or "because".
These words are in-dicative of intentions, explanations, and justifi-cations.The answers to WHEN and WHERE questionsare frequently found in the dateline rather thanthe story itself, so Quarc also considers the date-line as a possible answer.
Figure 7 shows thedateline rules, which are used for both WHENand WHERE questions.
The words "happen"and "take place" suggest hat the dateline maybe the best answer (rules #1 and #2).
We alsofound that that the words "this" and "story"were strong indicators that the dateline is thebest answer (rules #3 and #4) .
We found sev-eral sentences of the form "When did this hap-pen?"
or "When did this take place?".
Theverbs alone are not sufficient to be slam dunksbecause they often have a specific subject (e.g.,"When did the surprise happen?")
that refersback to a sentence in the story.
But when thewords "story" or "this" appear, the questionseems to be referring to the story in its entiretyand the dateline is the best answer.1.
If contains(Q,happen)Then Score(DATELINE) -b= good_clue2.
If contains(Q,take) andcontains(Q,place)Then Score(DATELINE) +---- good_clue3.
If contains(Q,this)Then Score(DATELINE) --b: slam_dunk4.
If contains(Q,story)Then Score(DATELINE) +---- slam_dunkFigure 7: Dateline RulesAfter all the rules have been applied to every16sentence in the story, the sentence (or dateline)with the highest score is returned as the bestanswer.
In the event of a tie, a WHY questionchooses the sentence that appears latest in thestory, and all other question types choose thesentence that appears earliest in the story.
Ifno sentence r ceives apositive score, then WHENand WHERE questions return the dateline as adefault, WHY questions return the last sentencein the story, and all other questions return thefirst sentence in the story.4 Exper imenta l  Resu l tsWe evaluates Quarc on the same data set thatwas used to evaluate the DeepRead readingcomprehension system (Hirschman et al, 1999).This data set contains 115 reading comprehen-sion tests, 55 of which were used for develop-ment and 60 of which were reserved for testingpurposes.
We also used the answer keys createdby the DeepRead evelopers (Hirschman et al,1999).
The HumSent  answers are sentencesthat a human judged to be the best answer foreach question.
The AutSent  answers are gen-erated automatically by determining which sen-tence contains the highest percentage of wordsin the published answer key, excluding stop-words.
We focused on obtaining the best pos-sible HumSent  score because we believed thathumans were more reliable than the automaticword-counting routine.Table 1 shows Quarc's results for each typeof question as well as its overall results.
Quarcachieved 40% HumSent  accuracy overall, butthe accuracy varied substantially across ques-tion types.
Quarc performed the best on WHENquestions, achieving 55% accuracy, and per-formed the worst on WHAT and WHY questions,reaching only 28% accuracy.Quarc's rules use a variety of knowledgesources, so we ran a set of experiments oevalu-ate the contribution of each type of knowledge.Figure 8 shows the results of these experiments,based on the HumSent  answer keys.
First,we evaluated the performance of Quarc's Word-Match function all by itself, giving equal weightto verbs and non-verbs.
The WordMatch func-tion alone, shown as Word on the graph, pro-duced 27% accuracy.
When we gave verbs twiceas much weight as non-verbs (?
Verb), overallaccuracy improved to 28%.
Interestingly, giv-WHOHumSent: 0.41 (24/59)AutSent: 0.49 (29/59)WHATHumSent: 0.28 (17/61)AutSent: 0.31 (19/61)WHENHumSent: 0.55 (33/60)AutSent: 0.28 (17/60)WHEREHumSent: 0.47 (28/60)AutSent: 0.48 (29/60)WHYHumSent: 0.28 (17/60)AutSent: 0.27 (16/60)OVERALLHumSent: 0.40 (119/300)AutSent: 0.37 (110/300)Table 1: Overall Resultsing extra weight to verbs improved the WHOand WHAT questions, but hurt the WHEN andWHERE questions.
These results suggest hatverbs should be weighted more heavily only forcertain question types, even though weNext, we wanted to see how much effect thesemantic classes had on performance, so weadded the rules that use semantic lasses.
Onlythe WHO, WHEN, and WHAT question types hadsuch rules, and performance improved on thosequestion types (+Sem).
We then added thedateline rules for the WHEN and WHERE ques-tions, and added the WHY rules that reward thesentences immediately preceding and followingthe best WordMatch sentence (rules #1-3 inFigure 6).
Figure 8 shows that these additions(+Why/Dateline) also improved results for allthree question types.Finally, we added the remaining rules thatlook for specific words and phrases.
The finalversion of Quarc achieved 40% HumSent  ac-curacy, which compares favorably with Deep-Read's results (36% HumSent  accuracy).
Fur-thermore, DeepRead's best results used hand-tagged named entity recognition and hand-tagged coreference r solution.
Quarc did notrely on any hand-tagging and did not performany coreference r slution.We also ran an experiment to evaluate thequality of Quarc's tie-breaking procedure, whichwas described at the end of Section 3.
When17HumSent  Score0 .55  - -0 .50  ---0 .45  ~ ~ ~ ~ ~ ~ .
.
~rps  ~ .
.0.40  - -  - " .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
2035 .
.
.
.
.
:>-0.30  ,-- "~I I Overa l ls ~  ~ - -  Who~ ~ - What" Wherest" ~-"0:25  - -  / ~ .0 .20  :- ////0.15  : /I I tWord + Verb  + Sem + Why/Date l ine  + Qtype  Ru lesFigure 8: Experimental Resultsmore than one sentence is tied with the bestscore, Quarc selects the sentence that appearsearliest in the story, except for WHY questionswhen Quarc chooses the sentence appearing lat-est in the story.
Table 2 shows the results ofremoving this tie-breaking procedure, so thatQuarc is allowed to output all sentences thatreceived the top score.
These results representan upper bound on performance if Quarc had aperfect ie-breaking mechanism.Table 2 shows that Quarc's performance onWHAT, WHEN, and WHY questions improved byseveral percentage points, but performance onWHO and WHERE questions was basically thesame.
Overall, Quarc was able to identify 46%of the correct sentences by generating 1.75 hy-potheses per question on average.
These re-sults suggest hat a better tie-breaking proce-dure could substantially improve Quarc's per-formance by choosing between the top two orthree candidates more intelligently.5 Lessons  LearnedQuarc's rules were devised by hand after ex-perimenting with the 55 reading comprehensiontests in the development set.
These simple rulesare probably not adequate to handle other typesof question-answering tasks, but this exercisegave us some insights into the problem.First, semantic lasses were extremely usefulfo r  WHO,  WHEN,  and W H E R E  questions becausethey look for descriptions of people, dates, andlocations.
Second, WHY questions are concernedwith causal information, and we discovered sev-eral keywords that were useful for identifyingintentions, explanations, and justifications.
Abetter understanding of causal relationships anddiscourse structure would undoubtedly be veryhelpful.
Finally, WHAT questions were the mostdifficult because they sought a staggering vari-ety of answers.
The only general pattern thatwe discovered was that WHAT questions oftenlook for a description of an event or an object.Reading comprehension tests are a wonderfultestbed for research in natural anguage process-ing because they require broad-coverage t ch-niques and semantic knowledge.
In the future,we plan to incorporate coreference resolution,which seems to be very important for this task.We also plan to experiment with techniques thatacquire semantic knowledge automatically (e.g.,(Riloff and Shepherd, 1997; Roark and Char-niak, 1998)) to generate bigger and better se-mantic lexicons.18WHOHumSent: 0.42 (25/59)AutSent: 0.53 (31/59)Avg # answers: 1.27WHATHumSent: 0.44 (27/61)AutSent: 0.49 (30/61)Avg # answers: 2.84WHENHumSent: 0.62 (37/60)AutSent: 0.32 (19/60)Avg # answers: 1.45WHEREHumSent: 0.48 (29/60)AutSent: 0.48 (29/60)Avg #-answers: 1.33WHYHumSent: 0.33 (20/60)AutSent: 0.30 (18/60)Avg # answers: 1.82OVERALLHumSent: 0.46 (138/300)AutSent: 0.42 (127/300)Avg # answers: 1.75Table 2: Generating multiple answers6 AcknowledgmentsThis research is supported in part by the Na-tional Science Foundation under grant IRI-9704240.Re ferencesL.
Hirschman, M. Light, E. Breck, andJ.
Burger.
1999.
Deep Read: A ReadingComprehension System.
In Proceedings of the37th Annual Meeting of the Association forComputational Linguistics.G.
Miller.
1990.
Wordnet: An On-line LexicalDatabase.
International Journal of Lexicog-raphy, 3(4).E.
Riloff and J. Shepherd.
1997.
A Corpus-Based Approach for Building Semantic Lex-icons.
In Proceedings of the Second Confer-ence on Empirical Methods in Natural Lan-guage Processing, pages 117-124.B.
Roark and E. Charniak.
1998.
Noun-phraseCo-occurrence Statistics for Semi-automaticSemantic Lexicon Construction.
In Proceed-ings off the 36th Annual Meeting of the Asso-ciation for Computational Linguistics, pages1110-1116.19
