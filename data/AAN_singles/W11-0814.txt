Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 92?100,Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational LinguisticsExtracting Transfer Rules for Multiword Expressions from Parallel CorporaPetter Haugereid and Francis BondDivision of Linguistics and Multilingual Studies,Nanyang Technological University, Singaporepetterha@ntu.edu.sg,bond@ieee.orgAbstractThis paper presents a procedure for extract-ing transfer rules for multiword expressionsfrom parallel corpora for use in a rule basedJapanese-English MT system.
We show thatadding the multi-word rules improves transla-tion quality and sketch ideas for learning moresuch rules.1 IntroductionBecause of the great ambiguity of natural language,it is hard to translate from one language to another.To deal with this ambiguity it is common to try toadd more context to a word, either in the form ofmulti-word translation patterns (Ikehara et al, 1991)or by adding more context to the translations in sta-tistical MT systems (Callison-Burch et al, 2005).In this paper, we present a way to learn largenumbers of multi-word translation rules from eitherdictionaries or parallel text, and show their effec-tiveness in a semantic?transfer-based Japanese-to-English machine translation system.
This researchis similar to work such as Nichols et al (2007).
Thenovelty lies in (i) the fact that we are learning rulesfrom parallel text and (ii) that we are learning muchmore complex rules.In Section 2, we outline the semantic transfer ma-chinery and we introduce the DELPH-IN machinetranslation initiative that provided the resources usedin its construction.
We describe in more detail howwe learn new rules in Section 3, and show their ef-fect in Section 4.
We briefly discuss the results andoutline future work in Section 5 and, finally, we con-clude this paper in Section 6.2 Semantic transferAll experiments are carried out using Jaen, a se-mantic transfer based machine translation system(Bond et al, 2011).
The system uses Minimal Re-cursion Semantics (MRS) as its semantic representa-tion (Copestake et al, 2005).
The transfer processtakes place in three steps.
First, a Japanese string isparsed with the Japanese HPSG grammar, JACY.
Thegrammar produces an MRS with Japanese predicates.Second, the Japanese MRS is transferred into an En-glish MRS. And finally, the English HPSG grammarERG generates an English string from the EnglishMRS.At each step of the translation process, stochasticmodels are used to rank the output.
There is a cutoffat 5, so the maximal amount of generated sentencesis 125 (5x5x5).
The final results are reranked usinga combined model (Oepen et al, 2007).While JACY and the ERG have been developedover many years, less effort has been put into thetransfer grammar, and this component is currentlythe bottleneck of the system.
In general, transferrules are the bottleneck for any system, and thereis a long history of trying to expand the number oftransfer rules types (Matsuo et al, 1997) and tokens(Yamada et al, 2002).In order to increase the coverage of the system(the number of words that we can translate) we buildrules automatically.
We look at strings that havea high probability of being a translation (identifiedfrom parallel corpora), and see if they fit a patterndefined in the transfer grammar.
A very simple pat-tern would be that of a noun predicate being trans-ferred as another noun predicate.
The transfer ruletype for this pattern is given in (1).
The type makes92sure that the LBL and the ARG0 values are kept whenthe relation is transferred, while the PRED value isleft underspecified.1(1)???????noun-mtrIN|RELS?
[LBL h1 , ARG0 x1]?OUT|RELS?
[LBL h1 , ARG0 x1]???????
?The rule for?
(hon)?
book, which is a subtypeof noun-mtr, is given in (2).(2)???????hon_bookIN|RELS?
[PRED _hon_n_rel]?OUT|RELS?
[PRED _book_n_of_rel]???????
?A linguistically more interesting transfer rule isthat for PP ?
Adjective transfer (see (3)), whichtakes as input 3 relations (the first for the noun, thesecond for the postposition, and the third for thequantifier of the noun, all properly linked), and out-puts one relation (for the adjective), for example ofan angle ?
angular, to give an English-to-Englishexample.
The output adjective relation is given thesame handle, index and external argument as the in-put postposition, so that the semantic linking withthe rest of the MRS is preserved.
In this way, modi-fiers of the PP will modify the Adjective, and so on.The use of this transfer rule is demonstrated in Sec-tion 3.1.21the LBL (label) of the relation is a tag, which can be used torefer to the relation (conventionally written with an h for han-dle).
The ARG0 is the index of the relation.
Nouns and deter-miners have referential indices (conventionally written with anx), while adjectives and verbs have event indices (written withan e).2The HCONS feature has as value a list of qeq constraints(equality modulo quantifiers), which function is to express thatthe label of a relation is equal to a handle in an argument posi-tion (without unifying them).(3)??????????????????????pp-adj_mtrIN?????????????RELS?
[LBL h1 , ARG0 x1][LBL h0 , ARG0 e0 ,ARG1 ext , ARG2 x1][ARG0 x1 , RSTR hr]?HCONS?
[HARG hr , LARG h1]??????????????OUT|RELS?
[LBL h0 , ARG0 e0 ,ARG1 ext]??????????????????????
?3 ProcedureWe are using GIZA++ (Och and Ney, 2003) andAnymalign (Lardilleux and Lepage, 2009) to gener-ate phrase tables from a collection of four JapaneseEnglish parallel corpora and one bilingual dictio-nary.
The corpora are the Tanaka Corpus (2,930,132words: Tanaka (2001)), the Japanese Wordnet Cor-pus (3,355,984 words: Bond et al (2010)), theJapanese Wikipedia corpus (7,949,605),3 and theKyoto University Text Corpus with NICT transla-tions (1,976,071 words: Uchimoto et al (2004)).The dictionary is Edict, a Japanese English dictio-nary (3,822,642 words: Breen (2004)).
The wordtotals include both English and Japanese words.We divided the corpora into development, test,and training data, and extracted the transfer rulesfrom the training data.
The training data of the fourcorpora together with the Edict dictionary form aparallel corpus of 20 million words (9.6 million En-glish words and 10.4 million Japanese words).
TheJapanese text is tokenized and lemmatized with theMeCab morphological analyzer (Kudo et al, 2004),and the English text is tokenized and lemmatizedwith the Freeling analyzer (Padr?
et al, 2010), withMWE, quantities, dates and sentence segmentationturned off.When applying GIZA++ and Anymalign to thelemmatized parallel corpus they produced phrase ta-bles with 10,812,423 and 5,765,262 entries, respec-tively, running GIZA++ with the default MOSESsettings and Anymalign for approximately 16 hours.3The Japanese-English Bilingual Corpus of Wikipedia?s Ky-oto Articles: http://alaginrc.nict.go.jp/WikiCorpus/index_E.html93We filtered out the entries with an absolute fre-quency of 1,4 and which had more than 4 words onthe Japanese side or more than 3 words on the En-glish side.
This left us with 6,040,771 Moses entriesand 3,435,176 Anymalign entries.
We then checkedagainst the Jacy lexicon on the Japanese side and theERG lexicon on the English side to ensure that thesource and the target could be parsed/generated bythe MT system.
Finally, we filtered out entries with atranslation probability, P(English|Japanese), of lessthan 0.1.
This gave us 1,376,456 Moses entries and234,123 Anymalign entries.
These were all phrasetable entries with a relatively high probability, con-taining lexical items known both to the parser andthe generator.For each of these phrase table entries, we lookedup the lexemes on either side in the Jacy/ERG lexi-cons, and represented them with the semantic predi-cate (and their syntactic category).5 Ambiguous lex-emes were represented with a list of predicates.
Werepresented each possible surface rule with a list ofall possible semantic predicate rules.
So a possiblesurface rule with two (two times) ambiguous lexi-cal items would give four possible semantic rules, apossible surface rule with three (two times) ambigu-ous lexical items would give eight possible seman-tic rules, and so on.
A total of 53,960,547 possiblesemantic rules were created.
After filtering out se-mantic transfer rules containing English predicatesof probability less than 0.2 compared to the mostfrequent predicate associated with the same surfaceform, this number was reduced to 26,875,672.6 Eachof these rules consists of two ordered lists of seman-tic predicates (one for Japanese and one for English).From these possible semantic transfer rules, weextracted transfer rules that fitted nine different pat-4The absolute frequency number can, according to AdrienLardilleux (p.c.
), be thought of as a confidence score.
Thelarger, the more accurate and reliable the translation probabili-ties.
1 is the lowest score.5As shown in (2), predicates reflect the syntactic category ofthe lexical item by means of an infix, e.g.
?_n_?
for noun.6We used a profile of the English training data from theTanaka Corpus and the Japanese Wordnet Corpus, parsed withthe ERG grammar, to find the probability of each English pred-icate, given its surface form.
For example the word sleep isassigned the predicate "_sleep_n_1_rel" 103 times, the predi-cate "_sleep_v_1_rel" 89 times, and "_sleep_v_in_rel" 2 times.Hence, semantic transfer rules containing the first two are ac-cepted, while rules conataining the last are filtered out.terns.
We extracted 81,690 rules from the Moses en-tries, and 52,344 rules from the Anymalign entries.The total number of rules extracted was 97,478.
(36,556 rules overlapped.)
Once the rule templateshave been selected and the thresholds set, the entireprocess is automatic.The distribution of the extracted rules over thenine patterns is shown in Table 1.In the first three patterns, we would simply see ifthe predicates had the appropriate ?_n_?
and ?_a_?infixes in them (for nouns and adjectives respec-tively).
82,651 rules fitted these patterns and wereaccepted as transfer rules.
The last six patterns wereslightly more complex, and are described below.3.1 PP?
adjectiveJapanese PPs headed by the postposition?
no ?of?often correspond to an adjective in English as illus-trated in (4).
(4) a.
??small.size?ofsmallb.
?
?music?ofmusicalIn order to extract transfer rules that fit this pat-tern, we checked for possible semantic rules hav-ing two predicates on the Japanese side and one onthe English side.
The first Japanese predicate wouldhave the infix ?_n_?
(be a noun), and the secondwould be ?_no_p_rel?
(the predicate of the postpo-sition ?).
The sole English predicate would havethe infix ?_a_?
(be an adjective).3.2 PP?
PPJapanese PPs headed by the postposition ?
de?with/by/in/on/at?
are, given certain NP comple-ments, translated into English PPs headed by thepreposition ?by?
(meaning ?by means of?)
where theprepositional object does not have a determiner, asillustrated in (5).
(5) ???
?taxi?DEby taxiBy checking for possible semantic transfer rulesfitting the pattern noun + de_p_rel on the Japanese94Input Output Moses Anymalign Merged rulesnoun + noun ?
noun + noun 34,691 23,333 38,529noun + noun ?
adj + noun 21,129 13,198 23,720noun + noun ?
noun 11,824 12,864 20,402PP ?
adj 753 372 1,022PP ?
PP 131 24 146verb + NP ?
verb + NP 9,985 1,926 10,256noun + adj ?
adj 544 243 566postp + noun + verb ?
verb 1,821 173 1,921PP + verb ?
verb 812 211 916Total 81,690 52,344 97,478Table 1: Transfer rule patterns.side, and the pattern by_p_rel and noun on the En-glish side, we created PP to PP transfer rules where,in addition to the predicates stemming from the lex-ical items, the English determiner was set to theempty determiner (udef_q_rel).
The resulting trans-fer rule for (5) is illustrated in (6).(6)????????????????????pp_pp_mtrIN?
[PRED _de_p_rel][PRED udef_q_rel][PRED _takushii_n_rel]?OUT?
[PRED _by_p_means_rel][PRED udef_q_rel][PRED _taxi_n_1_rel]????????????????????
?With this particular pattern we get transfer ruleswhich prevent us from generating all possible trans-lations of ?
(?with?, ?by?, ?on?, ?in?, or ?at?
), andkeeps the quantifier unexpressed.There are many other possible PP?PP patterns,such as???
start in/on/at/to ?in the beginning?.We started with one well known idiomatic Englishtype, but should learn many more.3.3 Verb + NP?
Verb + NPJapanese MWEs fitting the pattern noun + objectmarker (?)
+ verb usually are translated into En-glish MWEs fitting one out of three verb + NP pat-terns, illustrated in (7).
In (7a), the NP has an unex-pressed quantifier.
The English pattern in these caseswill be verb + noun.
In (7b), the NP has an indef-inite article.
The English pattern will then be verb+ _a_q_rel + noun.
And in (7c), the NP has defi-nite article.
The English pattern will then be verb +_the_q_rel + noun.
(7) a.
???tenisutennis?woACC?shido?
?masuPOLITEplay tennisb.
??seikeiliving?woACC??
?taterustand upmake a livingc.
??semeblame?woACC?
?oubeartake the blameBy adding these rules to the transfer grammar, weavoid generating sentences such as I play the ten-nis and He took a blame.
In addition, we are ableto constrain the translations of the individual words,greatly reducing the transfer search space3.4 Noun + Adjective?
AdjectiveJapanese has a multiword expression pattern that isnot found in English.
In this pattern, noun +?
(ga)+ adjective usually correspond to English adjectives,as shown in (8).
The pattern is an example of a dou-ble subject construction.
The Japanese adjective hasits subject provided by a noun, but still takes an ex-ternal subject.
Our transfer rule takes this external95subject and links it to the subject of the English ad-jective.
(8) XXXgagaga?seNOM?gaheight?
?takaiNOM highX is tallWith the new rules, the transfer grammar now cor-rectly translates (9) as She is very intelligent.
andnot Her head is very good., which is the translationproduced by the system without the new multiwordrules.
Notice the fact that the adverb modifying theadjective in Japanese is also modifying the adjectivein English.
(9) ??kanojoShe?waTOPIC??taihenvery?atamahead?gaNOM?
?yoigood?..She is very intelligent.Because of the flexibility of the rule based sys-tem, we can also parse, translate and generate manyvariants of this, including those where the adverbcomes in the middle of the MWE, or where a dif-ferent topic marker is used as in (10).
We learn thetranslation equivalences from text n-grams, but thenmatch them to complex patterns, thus taking advan-tage of the ease of processing of simple text, butstill apply them flexibly, with the power of the deepgrammar.
(10) ??kanojoShe?moFOCUS?atamahead?gaNOM??taihenvery?
?yoigood?..She is also very intelligent.She is very intelligent also.3.5 Postp + Noun + Verb?
Verb / PP + Verb?
VerbJapanese has two MWE patterns consisting of apostposition, a noun, and a verb, corresponding toa verb in English.
The first is associated with thepostposition?
no ?of?
(see (11)), and the second isassociated with the postposition ?
ni ?in/on/at/to?
(see (12)).
(11) ??rekishihistory?noof??benkyoustudy?woACC?
?surumakestudy history(12) ??kingyogoldfish?niin/on/at/to??esafeed?woACC?
?yarugivefeed the goldfishIn (11), the postposition?
no ?of?, the noun??
benkyou ?study?, and the verb??
suru ?make?are translated as study, while in (12), the postposi-tion ?
ni ?in/on/at/to?, the noun ??
esa ?feed?,and the verb??
yaru ?give?
are translated as feed.In both MWE patterns, the noun is marked with theobject marker?
wo.
The two patterns have differ-ent analysis: In (11), which has the no-pattern, thepostposition attaches to the noun, and the object ofthe postposition??
rekishi ?history?
functions asa second subject of the verb.
In (12), which has theni-pattern, the postposition attaches to the verb, andthe object of the postposition??
kingyo ?goldfish?is a part of a PP.
Given the different semantic rep-resentations assigned to the two MWE patterns, wehave created two transfer rule types.
We will have abrief look at the transfer rule type for the no transla-tion pattern, illustrated in (13).7(13)????????????????????????????p+n+arg12_arg12_mtrIN???????????????????RELS?
[LBL h2 , ARG0 event,ARG1 x3 , ARG2 x2][LBL h2 , ARG0 x3][ARG0 x3 , RSTR h3][LBL h1 , ARG0 e1 ,ARG1 x1 , ARG2 x3]?HCONS?
[HARG h3 , LARG h2]????????????????????OUT|RELS?
[LBL h1 , ARG0 e1 ,ARG1 x1 , ARG2 x2]????????????????????????????
?The input of the p+n+arg12_arg12_mtr transferrule type consists of (i) a postposition relation, (ii)a noun relation, (iii) a quantifier (of the the noun),7The transfer rule type for the ni translation pattern(pp+arg12_arg12_mtr) is identical to the transfer rule type forthe no translation pattern except from the linking of the postpo-sition in the input.96and (iv) a verb relation (listed as they appear on theRELS list).
The output relation is a verb relation.
No-tice that the ARG1 of the input verb relation is reen-tered as ARG1 of the output relation ( x1 ), and theARG2 of the input postposition relation is reenteredas ARG2 of the output relation ( x2 ).
The output re-lation is also given the same LBL and ARG0 valueas the input verb relation.
In this way, the JapaneseMWE is collapsed into one English relation whilesemantic links to the rest of the semantic representa-tion are maintained.3.6 SummaryOut of the 26,875,672 possible semantic predicaterules, we extracted 97,478 rules that fitted one of thenine patterns.
These rules were then included in thetransfer grammar of the MT system.4 ResultsThe impact of the MWE transfer rules on the MTsystem is illustrated in Table 2.We compare two versions of the system, one withautomatically extracted MWE rules and one with-out.
They both have hand-written MWE and singleword rules as well as automatically extracted sin-gle word rules extracted from Edict by Nichols et al(2007).The additional rules in + MWE are those pro-duced in Section 3.
The system was tested on heldout sections of the Tanaka Corpus (sections 003 to005).
As can be seen from the results, the overallsystem is still very much a research prototype, thecoverage being only just over 20%.Adding the new rules gave small but consistentincreases in both end-to-end coverage (19.3% to20.1%) and translation quality (17.80% to 18.18%)measured with NEVA (Forsbom, 2003).8When we look only at the 105 sentences whosetranslations were changed by the new rules theNEVA increased from 17.1% to 21.36%.
Investigat-ing the effects on development data, we confirmedthat when the new MWE rules hit, they almost al-ways improved the translation.
However, there isstill a problem of data-sparseness, we are missing8NEVA is an alternative to BLEU that is designed to providea more meaningful sentence-level score for short references.
Itis calculated identically to BLEU, but leaving out the log andexponent calculations.
We find it correlates highly with BLEU.instances of rule-types as well as missing many po-tential rule types.As an example of the former, we have a patternfor verb+NP ?
verb+NP, but were unable to learn?????
jihi wo negau ?beg for mercy: lit.
askfor compassion?.
We had one example in the train-ing data, and this was not enough to get over ourthreshold.
As an example of the latter, we do notcurrently learn any rules for Adverb+Verb?Verb al-though this is a common pattern.5 Discussion and Further WorkThe transfer rules learned here are based on co-occurrence data from corpora and a Japanese-to-English dictionary.
Many of the translations learnedare in fact compositional, especially for the com-pound noun and verb-object patterns.
For exam-ple, ?
?
??
ana-wo horu ?dig hole?
?
diga whole would have been translated using existingrules.
In this case the advantage of the MWE rule isthat it reduces the search space, so the system doesnot have to consider less likely translations such ascarve the shortages.
More interestingly, many of therules find non-compositional translations, or thosewhere the structure cannot be translated word forword.
Some of these are also idiomatic in the sourceand target language.
One of our long term goals isto move these expressions into the source and tar-get grammars.
Currently, both Jacy and the ERGhave idiom processing (based on Copestake et al,2002), but there are few idiomatic entries in theirlexicons.
Bilingual data can be a good source foridentifying these monolingual idioms, as it makesthe non-compositionality explicit.
An example ofa rule that uses the current idiom machinery is the(hand-built) rule N-ga chie-wo shiboru ?N squeezesknowledge?
?N racks N?s brains, where the subjectis co-indexed with a possessive pronoun modifyingthe object: I/You rack my/your brains.
Adding suchexpressions to the monolingual grammars simplifiesthe transfer rules and makes the grammars more use-ful for other tasks.In this paper we only presented results for ninemajor multi-word transfer rule types.
These werethose that appeared often in the training and devel-opment data.
We can straightforwardly extend thisin two ways: by extending the number of rule types97Version Parse Transfer Generation Total NEVA (%) F1coverage coverage coverage coverage?MWE 3614/4500 1647/3614 870/1647 870/4500 17.80 0.185(0 rules) (80.3%) (45.6%) (52.8%) (19.3%)+ adj/n 3614/4500 1704/3614 900/1704 900/4500 17.99 0.189(83,217 rules) (80.3%) (47.1%) (52.8%) (20.0%)+ PP 3614/4500 1659/3614 877/1659 877/4500 17.88 0.187(1,168 rules) (80.3%) (45.9%) (52.9%) (19.5%)+ verb 3614/4500 1688/3614 885/1688 885/4500 17.89 0.186(13,093 rules) (80.3%) (46.7%) (52.4%) (19.7%)+ MWE 3614/4500 1729/3614 906/1729 906/4500 18.18 0.190(97,478 rules) (80.3%) (47.8%) (52.4%) (20.1%)Table 2: Coverage of the MT system before and after adding the MWE transfer rules.and by extending the number of rule instances.Shirai et al (2001) looked at examples in a65,500-entry English-Japanese lexicon and esti-mated that there were at least 80 multi-wordJapanese patterns that translated to a single wordin English.
As we are also going from multi-wordto multi-word we expect that there will be evenmore than this.
Currently, adding another pattern isroughly an hour?s work (half to make the rule-type inthe transfer engine, half to make the rule matcher inthe rule builder).
To add another 100 patterns is thus6 weeks work.
Almost certainly this can be speededup by sharing information between the templates.We therefore estimate that we can greatly reduce thesparseness of rule-types with four weeks work.To improve the coverage of rule instances, weneed to look at more data, such as that aligned byUtiyama and Takahashi (2003).Neither absolute frequency nor estimated transla-tion probability give reliable thresholds for deter-mining whether rules are good or not.
Currentlywe are investigating two solutions.
One is feedbackcleaning, where we investigate the impact of eachnew rule and discard those that degrade translationquality, following the general idea of Imamura et al(2003).
The second is the more traditional human-in-the loop: presenting each rule and a series of rele-vant translation pairs to a human and asking them tojudge if it is good or not.
Ultimately, we would liketo extend this approach to crowd source the deci-sions.
There are currently two very successful onlinecollaborative Japanese-English projects (Edict andTatoeba, producing lexical entries and multilingualexamples respectively) which indicates that there isa large pool of interested knowledgeable people.Finally, we are working in parallel to qualitativelyimprove the MWE rules in two ways.
The first is toextend rules using semantic classes, not just words.This would mean we would need fewer rules, buteach rule would be more powerful.
Of course, manyrules are very idiomatic and should trigger on actuallexemes, but there are many, such as ????
?himei wo negau ?beg for mercy?
which allow somevariation ?
in this case there are at least three differ-ent verbs that are commonly used.
At a lower levelwe need to improve our handling of orthographicvariants so that a rule can match on different formsof the same word, rather than requiring several rules.We are working together with the Japanese WordNetto achieve these goals.The second approach is to learn complex rulesdirectly from the parallel text, in a similar way to(Jellinghaus, 2007) or (Way, 1999).
This will benecessary to catch rules that our templates do notinclude, but it is very easy to over-fit the rules to thetranslation data.
For this reason, we are still con-straining rules with templates.98Resource AvailabilityThe MWE expression rules made here and the ma-chine translation system that uses them are avail-able through an open source code repository.
In-stallation details can be found at http://wiki.delph-in.net/moin/LogonInstallation.
Thecode to make the rules is undergoing constant re-vision, when it settles down we intend to also add itto the repository.6 ConclusionThis paper presented a procedure for extractingtransfer rules for multiword expressions from paral-lel corpora for use in a rule based Japanese-EnglishMT system.
We showed that adding the multi-word rules improves translation coverage (19.3%to 20.1%) and translation quality (17.8% to 18.2%NEVA).
We show how we can further improve bylearning even more rules.AcknowledgmentsWe would like to thank the members of the LO-GON, and DELPH-IN collaborations for their supportand encouragement.
In addition we would like tothank the developers and maintainers of the otherresources we used in our project, especially JMDict,Tatoeba, Anymalign and Moses.
This project wassupported in part by Nanyang Technological Univer-sity (through a start-up grant on ?Automatically de-termining meaning by comparing a text to its trans-lation?
).ReferencesFrancis Bond, Hitoshi Isahara, Kiyotaka Uchimoto,Takayuki Kuribayashi, and Kyoko Kanzaki.
2010.Japanese WordNet 1.0.
In 16th Annual Meeting ofThe Association for Natural Language Process-ing, pages A5?3.
Tokyo.Francis Bond, Stephan Oepen, Eric Nichols, DanFlickinger, Erik Velldal, and Petter Haugereid.2011.
Deep open source machine translation.
Ma-chine Translation.
(Special Issue on Open sourceMachine Translation, to appear).James W. Breen.
2004.
JMDict: a Japanese-multilingual dictionary.
In Coling 2004 Workshopon Multilingual Linguistic Resources, pages 71?78.
Geneva.Chris Callison-Burch, Colin Bannard, and JoshSchroeder.
2005.
Scaling phrase-based statisticalmachine translation to larger corpora and longerphrases.
In 43nd Annual Meeting of the Associa-tion for Computational Linguistics: ACL-2005.Ann Copestake, Dan Flickinger, Carl J. Pol-lard, and Ivan A.
Sag.
2005.
Minimal Re-cursion Semantics: an introduction.
Re-search on Language and Computation, 3(4):281?332.
URL http://lingo.stanford.edu/sag/papers/copestake.pdf.Ann Copestake, Fabre Lambeau, Aline Villavicen-cio, Francis Bond, Timothy Baldwin, Ivan Sag,and Dan Flickinger.
2002.
Multiword expres-sions: Linguistic precision and reusability.
InProceedings of the Third International Confer-ence on Language Resources and Evaluation(LREC 2002), pages 1941?7.
Las Palmas, CanaryIslands.Eva Forsbom.
2003.
Training a super model look-alike: Featuring edit distance, n-gram occurrence,and one reference translation.
In In Proceedingsof the Workshop on Machine Translation Evalua-tion.
Towards Systemizing MT Evaluation.Satoru Ikehara, Satoshi Shirai, Akio Yokoo, andHiromi Nakaiwa.
1991.
Toward an MT systemwithout pre-editing ?
effects of new methodsin ALT-J/E ?.
In Third Machine TranslationSummit: MT Summit III, pages 101?106.
Wash-ington DC.
URL http://xxx.lanl.gov/abs/cmp-lg/9510008.Kenji Imamura, Eiichiro Sumita, and Yuji Mat-sumoto.
2003.
Feedback cleaning of machinetranslation rules using automatic evaluation.
InProceedings of the 41st Annual Meeting of theAssociation for Computational Linguistics, pages447?454.
Association for Computational Lin-guistics, Sapporo, Japan.
URL http://www.aclweb.org/anthology/P03-1057.Michael Jellinghaus.
2007.
Automatic Acquisition ofSemantic Transfer Rules for Machine Translation.Master?s thesis, Universit?t des Saarlandes.Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.2004.
Applying Conditional Random Fields toJapanese Morphological Analysis.
In Dekang Linand Dekai Wu, editors, Proceedings of EMNLP992004, pages 230?237.
Association for Computa-tional Linguistics, Barcelona, Spain.Adrien Lardilleux and Yves Lepage.
2009.Sampling-based multilingual alignment.
InProceedings of Recent Advances in NaturalLanguage Processing (RANLP 2009), pages214?218.
Borovets, Bulgaria.Yoshihiro Matsuo, Satoshi Shirai, Akio Yokoo, andSatoru Ikehara.
1997.
Direct parse tree translationin cooperation with the transfer method.
In DanielJoneas and Harold Somers, editors, New Methodsin Language Processing, pages 229?238.
UCLPress, London.Eric Nichols, Francis Bond, Darren Scott Appling,and Yuji Matsumoto.
2007.
Combining resourcesfor open source machine translation.
In The11th International Conference on Theoretical andMethodological Issues in Machine Translation(TMI-07), pages 134?142.
Sk?vde.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Stephan Oepen, Erik Velldal, Jan Tore L?nning,Paul Meurer, and Victoria Rosen.
2007.
Towardshybrid quality-oriented machine translation.
onlinguistics and probabilities in MT.
In 11th Inter-national Conference on Theoretical and Method-ological Issues in Machine Translation: TMI-2007, pages 144?153.Llu?s Padr?, Miquel Collado, Samuel Reese, MarinaLloberes, and Irene Castell?n.
2010.
Freeling 2.1:Five years of open-source language processingtools.
In Proceedings of 7th Language Resourcesand Evaluation Conference (LREC 2010).
La Val-letta.
(http://nlp.lsi.upc.edu/freeling.Satoshi Shirai, Kazuhide Yamamoto, and KazutakaTakao.
2001.
Construction of a dictionary totranslate japanese phrases into one english word.In Proceedings of ICCPOL?2001 (19th Interna-tional Conference on Computer Processing ofOriental Languages, pages 3?8.
Seoul.Yasuhito Tanaka.
2001.
Compilation of a mul-tilingual parallel corpus.
In Proceedings ofPACLING 2001, pages 265?268.
Kyushu.
(http://www.colips.org/afnlp/archives/pacling2001/pdf/tanaka.pdf).Kiyotaka Uchimoto, Yujie Zhang, Kiyoshi Sudo,Masaki Murata, Satoshi Sekine, and HitoshiIsahara.
2004.
Multilingual aligned paralleltreebank corpus reflecting contextual informa-tion and its applications.
In Gilles S?ras-set, editor, COLING 2004 Multilingual Linguis-tic Resources, pages 57?64.
COLING, Geneva,Switzerland.
URL http://acl.ldc.upenn.edu/W/W04/W04-2208.bib.Masao Utiyama and Mayumi Takahashi.
2003.English-Japanese translation alignment data.http://www2.nict.go.jp/x/x161/members/mutiyama/align/index.html.Andy Way.
1999.
A hybrid architecture for robustMT using LFG-DOP.
Journal of Experimentaland Theoretical Artificial Intelligence, 11.
SpecialIssue on Memory-Based Language Processing.Setsuo Yamada, Kenji Imamura, and Kazuhide Ya-mamoto.
2002.
Corpus-assisted expansion ofmanual mt knowledge.
In Ninth InternationalConference on Theoretical and MethodologicalIssues in Machine Translation: TMI-2002, pages199?208.
Keihanna, Japan.100
