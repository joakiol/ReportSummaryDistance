Project for production of closed-caption TV programsfor the hearing impairedTakahiro WakaoTelecommunications AdvancementOrganization of JapanUehara Shibuya-ku, Tokyo 151-0064, Japanwakao@shibuya.tao.or.jpEiji SawamuraTAOTerumasa EharaNHK Science and TechnicalResearch Lab / TAOIchiro MaruyamaTAOKatsuhiko ShiraiWaseda University, Department ofInformation and Computer Science / TAOAbstractWe describe an on-going project whoseprimary aim is to establish the technology ofproducing closed captions for TV newsprograms efficiently using natural anguageprocessing and speech recognition techniquesfor the benefit of the hearing impaired inJapan.
The project is supported by theTelecommunications AdvancementOrganisation of Japan with the help of theministry of Posts and Telecommunications.We propose natural anguage and speechprocessing techniques hould be used forefficient closed caption production of TVprograms.
They enable us to summarise TVnews texts into captions automatically, andsynchronise TV news texts with speech andvideo automatically.
Then the captions aresuperimposed onthe screen.We propose a combination of shallowmethods for the summarisation.
For all thesentences in the original text, an importancemeasure is computed based on key words inthe text to determine which sentences areimportant.
If some parts of the sentencesare judged unimportant, they are shortened ordeleted.
We also propose keyword pairmodel for the synchronisation between textand speech.IntroductionThe closed captions for TV programs are notprovided widely in Japan.
Only 10 percent ofthe TV programs are shown with captions, incontrast to 70 % in the United States and morethan 30 % in Britain.
Reasons why theavailability is low are firstly the characters usedin the Japanese language are complex and many.Secondly, at the moment, he closed captions areproduced manually and it is a time-consumingand costly task.
Thus we think the naturallanguage and speech processing technology willbe useful for the efficient production of TVprograms with closed captions.The Telecommunications AdvancementOrganisation of Japan with the support of theministry of Posts and Telecommunications hainitiated a project in which an electronicallyavailable text of TV news programs issummarised and syncrhorinised with the speechand video automatically, then superimposed onthe original programs.It is a five-year project which started in 1996,and its annual budget is about 200 million yen.In the following chapters we describe mainresearch issues in detail and the project schedule,and the results of our preliminary research onthe main research topics are presented.1340neWS SC:TV program withFigure 1 System Outline1 Research IssuesMain research issues in the project are asfollows:?
automatic text summarisation?
automatic synchronisation of text andspeech?
building an efficient closed captionproduction systemThe outline of the system is shown in Figure 1.Although all types of TV programs are to behandled in the project system, the first priority isgiven to TV news programs ince most of thehearing impaired people say they want to watchclosed-captioned TV news programs.
Theresearch issues are explained briefly next.1.1 Text SummarisationFor most of the TV news programs today, thescripts (written text) are available lectronicallybefore they are read out by newscasters.Japanese news texts are read at the speed ofbetween 350 and 400 characters per minute, andif all the characters in the texts are shown on theTV screen, there are too many of them to beunderstood well (Komine et al 1996).Therefore we need to summarise the newstexts to some extent, and then show them on thescreen.
The aim of the research on automatictext summarisation is to summarise the text fullyor partially automatically to a proper size toobtain closed captions.
The current aim is 70%summarisation in the number of characters.1.2 Synchronisation of Text and SpeechWe need to synchronise the text with the sound,or speech of the program.
This is done by handat present and we would like to employ speechrecognition technology to assist thesynchronisation.First, synchronising points between theoriginal text and the speech are determinedautomatically (recognition phase in Figurel).Then the captions are synchronised with thespeech and video (synchronisation phase inFigurel).1.3 Efficient Closed Caption ProductionSystemWe will build a system by integrating thesummarisation and synchronisation techniqueswith techniques for superimposing characters onto the screen.
We have also conductedresearch on how to present he captions on thescreen for the handicapped people.2 Project ScheduleThe project has two stages: the first 3 years andthe rest 2 years.
We research on the aboveissues and build a prototype system in the firststage.
The prototype system will be used toproduce closed captions, and the capability andfunctions of the system will be evaluated.
Wewill focus on improvement and evaluation of thesystem in the second stage.13413 Preliminary Research ResultsWe describe results of our research on automaticsummarisation a d automatic synchronisation ftext and speech.
Then, a study on how topresent captions on TV screen to the hearingimpaired people is briefly mentioned.3.1 Automatic Text SummarisationWe have a combination of shallow processingmethods for automatic text summarisation.The first is to compute key words in a text andimportance measures for each sentence, and thenselect importanct sentences for the text.
Thesecond is to shoten or delete unimportant partsin a sentence using Japanese language-specificrules.3.1.1 Sentence ExtractionEhara found that compared with newspaper text,TV news texts have longer sentences and eachtext has a smaller number of sentences (Ehara etal 1997).
If we summarise TV news text byselecting sentences from the orignal text, itwould be 'rough' summarisation.
On the otherhand, if we devide long sentences into smallerunits, thus increase the number of sentences inthe text, we may have finer and bettersummarisation (Kim & Ehara 1994).Therefore what is done in the system is that if asentence in a given text is too long, it will bepartitioned into smaller units with minimunchanges made to the original sentence.To compute importance measures for eachsentence, we need to find first key words of thetext.
We tested high-frequency key wordmethod (Luhn 1957, Edumundson 1969) and aTF-IDF-based (Text frequency, InverseDocument Frequency) method.
We evaluatedthe two methods using ten thousand TV newstexts, and found that high-frequency key wordmethod showed slightly better results than themethod based on TF-IDF scores (Wakao et al1997).3.1.2 Rules fo r  shortening textAnother way of reducing the number ofcharacters in a Japanese text, thus summarisingthe text, is to shorten or delete parts of thesentences.
For example, if a sentence ndswith a sahen verb followed by its inflection, orhelping verbs or particles to express properpoliteness, it does not change the meaningmuch even if we keep only the verb stem (orsahen noun) and delete the rest of it.
This isone of the ways found in the captions to shortenor delete unimportant parts of the sentences.We analysed texts and captions in a TVnews program which is broadcast fullycaptioned for the hearing impaired in Japan.
Wecomplied 16 rules.
The rules are devided into 5groups.
We describe them one by one below.1) Shotening and deletion of sentence ndsWe find some of phrases which come at theend of the sentence can be shortened ordeleted.
If a sahen verb is used as the mainverb, we can change it to its sahen noun.For example:?
... ke ikakush i te imasu(~mb'C l ,~T)--, ... keikaku (~)(note: keikakusuru = plan, sahen verb)If the sentence nds in a reporting style, wemay delete the verb part.?
... bekida to nobemashita(~  t:: ~ bt:)--~ bekida (~< ~ t?_)(bekida = should, nobemashita = have said)2) Keeping parts of sentenceImportant noun phrases are kept in captions,and the rest of the sentence is deleted.?
taihosaretano ha Matumoto shachou(~ ~ ~ 1":.
g) ~$.~'~,~  )--, taiho Matumoto shachou(~ $'~:~k~.
)(taiho = arrest, shachou = a companypresident, Matumoto = name of a person )3) Replacing with shorter phraseSome nouns are replaced with a simpler andshoter phrase.?
souridaijin (~) - - *  shushou (Yi~d)(souridaijin, shushou both mean a primeminister)Conneticting phrases omitted 4)Connecting phrases at the beginningsentence may be omitted.. shikashi ( b ~, b = however),ippou (--:8 = on the other hand)of the13425) Time expressions deletedComparative time expressions uch as today(kyou ~- \[\] ), yesterday (kinou, I?
\[\] ) can bedeleted.
However, the absolute time expressionssuch as May, 1998 (1 9 9 8~5 B)  stayunchanged in summarisation.When we apply these rules to selectedimportant sentences, we can reduce the size oftext further 10 to 20 percent.3.2 Automatic Synchronisation of Textand SpeechWe next synchronise the text and speech.
First,the written TV news text is changed into astream of phonetic transcriptions.
Second,we try to detect he time points of the text andtheir corresponding speech sections.
We havedeveloped 'keyword pair model' for thesynchronisation which is shown in Figure 2.Nu~l arcTA .. .
.
.
TB lcFigure 2 Keyword Pair ModelThe model consists of two sets of words(keywordsl and keywords2) before and after thesynchronisation point (point B).
Each setcontains one or two key words which arerepresented by a sequence of phonetic HMMs(Hidden Markov Models).
Each HMM is athree-loop, eight-mixture-distribution HM .We use 39 phonetic HMMs to represent allJapanese phonemes.When the speech is put in the model, non-synchronising input data travel through thegarbage arc while synchronising data go throughthe two keyword sets, which makes thelikelihood at point B increase.
Therefore if weobserve the likelihood at point B and it becomesbigger than a certain threshold, we decide it isthe synchronisation point for the input data.Thirty-four (34) keywords pairs were takenfrom the data which was not used in the trainingand selected for the evaluation of the model.We used the speech of four people for theevaluation.The evaluation results are shown in Table 1.They are the accuracy (detection rate) and falsealarm rate for the case that each keyword set hastwo key words.
The threshold is computed aslogarithm of the likelihood which is betweenzero and one, thus it becomes less than zero.Threshold-I0-20-30-40-50-60-70-80-90-I00-150-200-250-300Detection rate(%)34.5644.1254.4160.2964.7169.1269.8571.3278.6882.3591.1894.8595.5999.26False Alarm Rate(FA/KW/Hour)00000.060.060.060.120.180.180.541.211.812.41Table 1 Synchronisation DetectionAs the threshold decreases, the detection rateincreases, however, the false alarm rateincreases little (Maruyama 1998).3.3 Speech DatabaseWe have been gathering TV and radio newsspeech.
In 1996 we collected speech data bysimulating news programs, i.e.
TV news textswere read and recorded sentence by sentence ina studio.
It has seven and a half houses ofrecordings of twenty people (both male andfemale).
In 1997 we continued to record TVnews speech by simulation, and recorded speechdata from actual radio and TV programs.
It hasnow ten hours of actual radio recording and tenhours of actual TV programs.
We willcontinue to record speech data and increase thesize of the database in 1998.3.4 Caption PresentationWe have conducted a study, though on smallscale, on how to present captions on TV screen1343to the hearing impaired people.
Wesuperimposed captions by hand on several kindsof TV programs.
They were evaluated by thehadicapped people (hard of hearing persons) interms of the following points :?
characters : ize, font, colour?
number of lines?
timing?
location?
methods of scrolling?
inside or outside of the picture (see twoexamples below).Figure 3 Captions in the pictureFigure 4 Captions outside of the pictureMost of the subjects preferred 2-line, outsideof the picture captions without scrolling(Tanahashi, 1998).
This was still a preliminarystudy, and we plan to conduct similar evaluationby the hearing impaired people on large scale.ConclusionWe have described a national project, itsresearch issues and schedule, as well aspreliminary research results.
The project aim isto establish language and speech processingtechnology so that TV news program text issummarised and changed into captions, andsynchronised with the speech, and superimposedto the original program for the benefits of thehearing impaired.
We will continue to conductresearch and build a prototype TV captionproduction system, and try to put it to a practicaluse in the near future.AcknowledgementsWe would like to thank Nippon TelevisionNetwork Corporation for letting us use thepictures (Figure 3, 4) of their news program forthe purpose of our research.ReferencesEdmundson, H.P.
(1969) New Methods in AutomaticExtracting Journal of the ACM, 16(2), pp 264-285.Ehara, T., Wakao, T., Sawamura, E., Maruyama I.,Abe Y., Shirai K. (1997) Application of naturallanguage processing and speech processingtechnology to production of closed-caption TVprograms for the hearing impaired NLPRS 1997Kim Y.B., Ehara, T. (1994) A method ofpartitioning of long Japanese sentences withsubject resolution in J/E machine translation, Proc.of 1994 International Conference on ComputerProcessing of Oriental Languages, pp.467-473.Komine, K., Hoshino, H., Isono, H., Uchida, T.,Iwahana, Y.
(1996) Cognitive Experiments ofNews Captioning for Hearing Impaired PersonsTechnical Report of IECE (The Institution ofElectronics, Information and CommunicationEngineers), HCS96-23, in Japanese, pp 7-12.Lulm, H.P.
(1957) A statistical approach to themechanized encoding a d searching of literaryinformation IBM Journal of Research andDevelopment, 1(4), pp 309-317.Maruyama, I., Abe, Y., Ehara, T., Shirai, K. (1998) AStudy on Keyword spotting using Keyword pairmodels for Synchronization f Text and Speech,Acoustical Society of Japan, Spring meeting, 2-Q-13, in Japanese.Tanahashi D. (1998) Study on Caption Presentationfor TV news programs for the hearing impairedWaseda University, Department of Information andComputer Science (master's thesis) in Japanese.Wakao, T., Ehara, E., Sawamura, E., Abe, Y., Shirai,K.
(1997) Application of NLP technology toproduction of closed-caption TY programs inJapanese for the hearing impaired.
ACL 97workshop, Natural Language Processing forCommunication Aids, pp 55-58.1344
