Proceedings of the 7th Workshop on Statistical Machine Translation, pages 298?303,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsQCRI at WMT12: Experiments in Spanish-English and German-EnglishMachine Translation of News TextFrancisco Guzma?n, Preslav Nakov, Ahmed Thabet, Stephan VogelQatar Computing Research InstituteQatar FoundationTornado Tower, floor 10, PO box 5825Doha, Qatar{fguzman,pnakov,ahawad,svogel}@qf.org.qaAbstractWe describe the systems developed by theteam of the Qatar Computing Research Insti-tute for the WMT12 Shared Translation Task.We used a phrase-based statistical machinetranslation model with several non-standardsettings, most notably tuning data selectionand phrase table combination.
The evaluationresults show that we rank second in BLEU andTER for Spanish-English, and in the top tierfor German-English.1 IntroductionThe team of the Qatar Computing Research Insti-tute (QCRI) participated in the Shared TranslationTask of WMT12 for two language pairs:1 Spanish-English and German-English.
We used the state-of-the-art phrase-based model (Koehn et al, 2003) forstatistical machine translation (SMT) with severalnon-standard settings, e.g., data selection and phrasetable combination.
The evaluation results show thatwe rank second in BLEU (Papineni et al, 2002) andTER (Snover et al, 2006) for Spanish-English, andin the top tier for German-English.In Section 2, we describe the parameters of ourbaseline system and the non-standard settings weexperimented with.
In Section 3, we discuss ourprimary and secondary submissions for the two lan-guage pairs.
Finally, in Section 4, we provide a shortsummary.1The WMT12 organizers invited systems translating be-tween English and four other European languages, in both di-rections: French, Spanish, German, and Czech.
However, weonly participated in Spanish?English and German?English.2 System DescriptionBelow, in Section 2.1, we first describe our initialconfiguration; then, we discuss our incremental im-provements.
We explored several non-standard set-tings and extensions and we evaluated their impactwith respect to different baselines.
These baselinesare denoted in the tables below by a #number thatcorresponds to systems in Figures 1 for Spanish-English and in Figure 2 for German-English.We report case insensitive BLEU calculated onthe news2011 testing data using the NIST scoringtool v.11b.2.1 Initial ConfigurationOur baseline system can be summarized as follows:?
Training: News Commentary + Europarl train-ing bi-texts;?
Tuning: news2010;?
Testing: news2011;?
Tokenization: splitting words containing adash, e.g., first-order becomes first @-@ order;?
Maximum sentence length: 100 tokens;?
Truecasing: convert sentence-initial words totheir most frequent case in the training dataset;?
Word alignments: directed IBM model 4(Brown et al, 1993) alignments in both direc-tions, then grow-diag-final-and heuristics;?
Maximum phrase length: 7 tokens;?
Phrase table scores: forward & reverse phrasetranslation probabilities, forward & reverse lex-ical translation probabilities, phrase penalty;298?
Language model: 5-gram, trained on the targetside of the two training bi-texts;?
Reordering: lexicalized, msd-bidirectional-fe;?
Detokenization: reconnecting words that weresplit around dashes;?
Model parameter optimization: minimum errorrate training (MERT), optimizing BLEU.2.2 Phrase TablesWe experimented with two non-standard settings:Smoothing.
The four standard scores associatedwith each phrase pair in the phrase table (forward& reverse phrase translation probabilities, forward& reverse lexical translation probabilities) are nor-mally used unsmoothed.
We also experimented withGood-Turing and Kneser-Ney smoothing (Chen andGoodman, 1999).
As Table 1 shows, the latter worksa bit better for both Spanish-English and German-English.es-en de-enBaseline (es:#3,de:#4) 29.98 22.03Good Turing 29.98 22.07Kneser-Ney 30.16 22.30Table 1: Phrase table smoothing.Phrase table combination.
We built two phrasetables, one for News Commentary + Europarl and anadditional one for the UN bi-text.
We then mergedthem,2 adding additional features to each entry inthe merged phrase table: F1, F2, and F3.
Thevalue of F1/F2 is 1 if the phrase pair came from thefirst/second phrase table, and 0.5 otherwise, whileF3 is 1 if the phrase pair was in both tables, and 0.5otherwise.
We optimized the weights for all features,including the additional ones, using MERT.3 Table 2shows that this improves by +0.42 BLEU points.2In theory, we should also re-normalize the conditionalprobabilities (forward/reverse phrase translation probability,and forward/reverse lexicalized phrase translation probability)since they may not sum to one anymore.
In practice, this isnot that important since the log-linear phrase-based SMT modeldoes not require that the phrase table features be probabilities(e.g., F1, F2, F3, and the phrase penalty are not probabilities);moreover, we have extra features whose impact is bigger.3This is similar but different from (Nakov, 2008): when aphrase pair appeared in both tables, they only kept the entryfrom the first table, while we keep the entries from both tables.es-enBaseline (es:#7) 30.94Merging (1) News+EP with (2) UN 31.36Table 2: Phrase table merging.2.3 Language ModelsWe built the language models (LM) for our systemsusing a probabilistic 5-gram model with Kneser-Ney (KN) smoothing.
We experimented with LMstrained on different training datasets.
We used theSRILM toolkit (Stolcke, 2002) for training the lan-guage models, and the KenLM toolkit (Heafieldand Lavie, 2010) for binarizing the resulting ARPAmodels for faster loading with the Moses decoder(Koehn et al, 2007).2.3.1 Using WMT12 Corpora OnlyWe trained 5-gram LMs on datasets provided bythe task organizers.
The results are presented inTable 3.
The first line reports the baseline BLEUscores using a language model trained on the targetside of the News Commentary + Europarl trainingbi-texts.
The second line shows the results when us-ing an interpolation (minimizing the perplexity onthe news2010 tuning dataset) of different languagemodels, trained on the following corpora:?
the monolingual News Commentary corpusplus the English sides of all training NewsCommentary v.7 bi-texts (for French-English,Spanish-English, German-English, and Czech-English), with duplicate sentences removed(5.5M word tokens; one LM);?
the News Crawl 2007-2011 corpora, (1213Mword tokens; separate LM for each of these fiveyears);?
the Europarl v.7 monolingual corpus (60Mword tokens; one LM);?
the English side of the Spanish-English UN bi-text (360M word tokens; one LM).The last line in Table 3 shows the results whenusing an additional 5-gram LM in the interpolation,one trained on the English side of the 109 French-English bi-text (662M word tokens).299We can see that using these interpolations yieldsvery sizable improvements of 1.7-2.5 BLEU pointsover the baseline.
However, while the impact ofadding the 109 bi-text to the interpolation is clearlyvisible for Spanish-English (+0.47 BLEU), it is al-most negligible for German-English (+0.06 BLEU).Corpora es-en de-enBaseline (es:#1, de:#2) 27.34 20.01News + EP + UN (interp.)
29.36 21.66News + EP + UN + 109 (interp.)
29.83 21.72Table 3: LMs using the provided corpora only.2.3.2 Using GigawordIn addition to the WMT12 data, we used the LDCGigaword v.5 corpus.
We divided the corpus intoreasonably-sized chunks of text of about 2GB perchunk, and we built a separate intermediate languagemodel for each chunk.
Then, we interpolated theselanguage models, minimizing the perplexity on thenews2010 development set as with the previousLMs.
We experimented with two different strate-gies for creating the chunks by segmenting the cor-pus according to (a) data source, e.g., AFP, Xinhua,etc., and (b) year of release.
We thus compared theadvantages of interpolating epoch-consistent LMsvs.
source-coherent LMs.
We trained individualLMs for each of the segments and we added themto a pool.
Finally, we selected the ten most relevantones from this pool based on their perplexity on thenews2010 devset, and we interpolated them.The results are shown in Table 4.
The first lineshows the baseline, which uses an interpolation ofthe nine LMs from the previous subsection.
Thefollowing two lines show the results when using anLM trained on Gigaword only.
We can see that forSpanish-English, interpolation by year performs bet-ter, while for German-English, it is better to use theby-source chunks.
However, the following two linesshow that when we translate with two LMs, one builtfrom the WMT12 data only and one built using Gi-gaword data only, interpolation by year is preferablefor Gigaword for both language pairs.
For our sub-mitted systems, we used the LMs shown in bold inTable 4: we used a single LM for Spanish-Englishand two LMs for German-English.Language Models es-en de-enBaseline (es:#5, de:#6) 30.31 22.48GW by year 30.68 22.32GW by source 30.52 22.56News-etc + GW by year 30.60 22.71News-etc + GW by source 30.55 22.54Table 4: LMs using Gigaword.2.4 Parameter Tuning and Data SelectionParameter tuning is a very important step in SMT.The standard procedure consists of performing a se-ries of iterations of MERT to choose the model pa-rameters that maximize the translation quality on adevelopment set, e.g., as measured by BLEU.
Whilethe procedure is widely adopted, it is also recognizedthat the selection of an appropriate development setis important since it biases the parameters towardsspecific types of translations.
This is illustrated inTable 5, which shows BLEU on the news2011 testsetwhen using different development sets for MERT.Devset es-ennews2008 29.47news2009 29.14news2010 29.61Table 5: Using different tuning sets for MERT.To address this problem, we performed a selectionof development data using an n-gram-based similar-ity ranking.
The selection was performed over a poolof candidate sentences drawn from the news2008,news2009, and news2010 tuning datasets.
The sim-ilarity metric was defined as follows:sim(f, g) = 2match(f, g) ?
lenpen(f, g) (1)where 2match represents the number of bi-grammatches between sentences f and g, and lenpen isa length penalty to discourage unbalanced matches.We penalized the length difference using aninverted-squared sigmoid function:lenpen(f, g) = 3?
4 ?
sig([|f | ?
|g|?
]2)(2)300where |.| denotes the length of a sentence in num-ber of words, ?
controls the maximal tolerance todifferences, and sig is the sigmoid function.To generate a suitable development set, we av-eraged the similarity scores of candidate sentencesw.r.t.
to the target testset.
For instance:sf =1|G|?g?Gsim(f, g) (3)where G is the set of the test sentences.Finally, we selected a pool of candidates f fromnews2008, news2009 and news2011 to generate a2000-best tuning set.
The results when using each ofthe above penalty functions are presented on Table 6.devset es-enbaseline (es:#6) 30.68selection (?
= 5) 30.94selection (?
= 10) 30.90Table 6: Selecting sentences for MERT.The average length of the source-side sentencesin our selected sentence pairs was smaller than inour baseline, the news2011 development dataset.This means that our selected source-side sentencestended to be shorter than in the baseline.
Moreover,the standard deviation of the sentence lengths wassmaller for our samples as well, which means thatthere were fewer long sentences; this is good sincelong sentences can take very long to translate.
Asa result, we observed sizable speedup in parametertuning when running MERT on our selected tuningdatasets.2.5 Decoding and Hypothesis RerankingWe experimented with two decoding settings:(1) monotone at punctuation reordering (Tillmannand Ney, 2003), and (2) minimum Bayes risk decod-ing (Kumar and Byrne, 2004).
The results are shownin Table 7.
We can see that both yield improvementsin BLEU, even if small.2.6 System CombinationAs the final step in our translation system, we per-formed hypothesis re-combination of the output ofseveral of our systems using the Multi-Engine MTsystem (MEMT) (Heafield and Lavie, 2010).es-en de-enBaseline (es:#2,de:#3) 29.83 21.72+MP 29.98 22.03Baseline (es:#4,de:#5) 30.16 22.30+MBR 30.31 22.48Table 7: Decoding parameters.
Experiments withmonotone at punctuation (MP) reordering, and minimumBayes risk (MBR) decoding.The results for the actual news2012 testset areshown in Table 8: the system combination resultsare our primary submission.
We can see that systemcombination yielded 0.4 BLEU points of improve-ment for Spanish-English and 0.2-0.3 BLEU pointsfor German-English.3 Our SubmissionsHere we briefly describe the cumulative improve-ments when applying the above modifications to ourbaseline system, leading to our official submissionsfor the WMT12 Shared Translation Task.3.1 Spanish-EnglishThe development of our final Spanish-English sys-tem involved several incremental improvements,which have been described above and which aresummarized in Figure 1.
We started with a base-line system (see Section 2.1), which scored 27.34BLEU points.
From there, using a large inter-polated language model trained on the provideddata (see Section 2.3.1) yielded +2.49 BLEU pointsof improvement.
Monotone-at-punctuation de-coding contributed an additional improvement of+0.15, smoothing the phrase table using Kneser-Neyboosted the score by +0.18, and using minimumBayes risk decoding added another +0.15 BLEUpoints.
Changing the language model to one trainedon Gigaword v.5 and interpolated by year yielded+0.37 additional points of improvement.
Another+0.26 points came from tuning data selection.
Fi-nally, using the UN data in a merged phrase ta-ble (see Section 2.2) yielded another +0.42 BLEUpoints.
Overall, we achieve a total improvementover our initial baseline of about 4 BLEU points.30127.3429.83 29.98 30.16 30.3130.6830.9431.3625262728293031321:BASELINE2:+WMT-LM3:+MP4:+KN5:+MBR6:*GIGAV5-LM7:+TUNE-SEL8:+PT-MERGE!BLEUv12score(news-2011)Figure 1: Incremental improvements for the Spanish-English system.3.2 German-EnglishFigure 2 shows a similar sequence of improvementsfor our German-English system.
We started with abaseline (see Section 2.1) that scored 19.79 BLEUpoints.
Next, we performed compound splitting forthe German side of the training, the developmentand the testing bi-texts, which yielded +0.22 BLEUpoints of improvement.
Using a large interpolatedlanguage model trained on the provided corpora (seeSection 2.3.1) added another +1.71.
Monotone-at-punctuation decoding contributed +0.31, smoothingthe phrase table using Kneser-Ney boosted the scoreby +0.27, and using minimum Bayes risk decodingadded another +0.18 BLEU points.
Finally, adding asecond language model trained on the Gigaword v.5corpus interpolated by year yielded +0.23 additionalBLEU points.
Overall, we achieved about 3 BLEUpoints of total improvement over our initial baseline.3.3 Final SubmissionsFor both language pairs, our primary submissionwas a combination of the output of several of ourbest systems shown in Figures 1 and 2, which usedifferent experimental settings; our secondary sub-mission was our best individual system, i.e., theright-most one in Figures 1 and 2.The official BLEU scores, both cased and lower-cased, for our primary and secondary submissions,as evaluated on the news2012 dataset, are shownin Table 8.
For Spanish-English, we achieved thesecond highest BLEU and TER scores, while forGerman-English we were ranked in the top tier.news2012lower casedSpanish-EnglishPrimary 34.0 32.9Secondary 33.6 32.5German-EnglishPrimary 23.9 22.6Secondary 23.6 22.4Table 8: The official BLEU scores for our submissionsto the WMT12 Shared Translation Task.4 ConclusionWe have described the primary and the secondarysystems developed by the team of the Qatar Com-puting Research Institute for Spanish-English andGerman-English machine translation of news textfor the WMT12 Shared Translation Task.We experimented with phrase-based SMT, explor-ing a number of non-standard settings, most notablytuning data selection and phrase table combination,which we described and evaluated in a cumulativefashion.
The automatic evaluation metrics,4 haveranked our system second for Spanish-English andin the top tier for German-English.We plan to continue our work on data selectionfor phrase table and the language model training, inaddition to data selection for tuning.4The evaluation scores for WMT12 are available online:http://matrix.statmt.org/30219.79 20.0121.7222.03 22.3022.4822.711818.51919.52020.52121.52222.5231:BASELINE2:+SPLIT3:WMT-LM4:+MP5:+KN6:+MBR7:+GIGA+WMTLMBLEUv12score(news-2011)Figure 2: Incremental improvements for the German-English system.AcknowledgmentsWe would like to thank the anonymous reviewersfor their useful comments, which have helped us im-prove the text of this paper.ReferencesPeter Brown, Vincent Della Pietra, Stephen Della Pietra,and Robert Mercer.
1993.
The mathematics of statis-tical machine translation: parameter estimation.
Com-putational Linguistics, 19(2):263?311.Stanley Chen and Joshua Goodman.
1999.
An empiricalstudy of smoothing techniques for language modeling.Computer Speech & Language, 13(4):359?393.Kenneth Heafield and Alon Lavie.
2010.
Combin-ing machine translation output with open source:The Carnegie Mellon multi-engine machine transla-tion scheme.
The Prague Bulletin of MathematicalLinguistics, 93(1):27?36.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics on Human Language Technology - Volume 1,NAACL ?03, pages 48?54, Edmonton, Canada.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of ACL.
Demonstration session, ACL ?07, pages177?180, Prague, Czech Republic.Shankar Kumar and William Byrne.
2004.
MinimumBayes-risk decoding for statistical machine transla-tion.
In Susan Dumais, Daniel Marcu, and SalimRoukos, editors, Proceedings of the Annual Meetingof the North American chapter of the Association forComputational Linguistics, HLT-NAACL ?04, pages169?176, Boston, MA.Preslav Nakov.
2008.
Improving English-Spanish sta-tistical machine translation: Experiments in domainadaptation, sentence paraphrasing, tokenization, andrecasing.
In Proceedings of the Third Workshopon Statistical Machine Translation, WMT ?07, pages147?150, Prague, Czech Republic.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of theAnnual Meeting of the Association for ComputationalLinguistics, ACL ?02, pages 311?318, Philadelphia,PA.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of the Annual Meetig of the Associa-tion for Machine Translation in the Americas, AMTA?06, pages 223?231.Andreas Stolcke.
2002.
SRILM ?
an extensible lan-guage modeling toolkit.
In Proceedings of Intl.
Conf.on Spoken Language Processing, volume 2 of ICSLP?02, pages 901?904, Denver, CO.Christoph Tillmann and Hermann Ney.
2003.
Word re-ordering and a dynamic programming beam search al-gorithm for statistical machine translation.
Computa-tional Linguistics, 29(1):97?133.303
