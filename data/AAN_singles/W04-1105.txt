An Enhanced Modelfor Chinese Word Segmentation and Part-of-speech TaggingJiang FengDepartment ofComputer Science and EngineeringShanghai Jiao Tong UniversityShanghai, China, 200030f_jiang@sjtu.edu.cnLiu HuiDepartment ofComputer Science and EngineeringShanghai Jiao Tong UniversityShanghai, China, 200030lh_Charles@sjtu.edu.cnChen YuquanDepartment ofComputer Science and EngineeringShanghai Jiao Tong UniversityShanghai, China, 200030yqchen@mail.sjtu.edu.cnLu RuzhanDepartment ofComputer Science and EngineeringShanghai Jiao Tong UniversityShanghai, China, 200030rzlu@mail.sjtu.edu.cnAbstractThis paper will present an enhancedprobabilistic model for Chinese wordsegmentation and part-of-speech (POS)tagging.
The model introduces the informationof Chinese word length as one of its featuresto reach a more accurate result.
And inaddition, the model also achieves theintegration of segmentation and POS tagging.After presenting the model, this paper willgive a brief discussion on how to solve theproblems in statistics and how to furtherintegrate Chinese Named Entity Recognitioninto the model.
Finally, some figures ofexperiments and comparisons will be reported,which shows that the accuracy of wordsegmentation is 97.09%, and the accuracy ofPOS tagging is 98.77%.1 IntroductionGenerally, Chinese Lexical Analysis consists oftwo phases; one is word segmentation and theother is part-of-speech(POS) tagging.
Rule -basedapproach and statistic -based approach are twodominant ways in natural language processing, aswell as Chinese Lexical Analysis.
This paper willonly focus on the later one.
Hence, our model iscalled a probabilistic model.Scanning through the researches in this fieldbefore, we have just found two points at which theperformance of a Chinese word segmentation andPOS tagging system could get better.
One is the onthe system architecture, and the other is from theMachine Learning theory.First, the traditional way of Chinese LexicalAnalysis simply regards the word segmentationand POS tagging as two separated phases.
Eachone of them has its own algorithms and models.Dividing the whole process into two independentparts can lower the complexity of the design ofsystem, but decrease the performance as well,because the two are fully integrated when a humanprocessing a sentence.
Fortunately, manyresearchers have already noticed it, and recentprojects pay more attention on the integration ofword segmentation and POS tagging, such as [GaoShan, Zhang Yan.
2001]?s pseudo trigramintegrated model, [Fu Guohong et al 2001]?sanalyzer which incorporates backward DynamicProgramming and A* algorithm, [Sun Maosong, etal.
2003]?s ?Divide and Conquer integration?,[Zhang Huaping, et al 2003]?s hierarchical hiddenMarkov model and so on.
The experiments givenby these papers also showed a great potential of theintegrated models.Besides the system architecture, another pointshould be noticed.
A probabilistic model of wordsegmentation and POS tagging can be regarded asan instance of Machine Learning.
In MachineLearning, the feature extraction is the mostimportant aspect, and far more important than alearning algorithm.
In the models nowadays, itseems that the features for Chinese LexicalAnalysis are a little too simple.
Most of them taketag sequences, or word frequencies as thedistinguishing features and ignore the other usefulinformation that are provided by Chinese itself.In this paper, we will present an enhanced, nottoo complex, model for word segmentation andPOS tagging, which will not only inherit the meritof an integrated model, but also take a new feature(word length) into account.The second part of this paper will describe themodel, including the input, output, and someassumptions.
The third part will give some briefdiscussion about the model on some issues likedata sparseness and Named Entity Recognition.
Inthe final part, the results of our experiments will bereported.2 The ModelThe first step to establish the model is to make aformal description for its input and output.
Here, aChinese word segmentation and POS taggingsystem is viewed as with input,nCCC ,...,, 21where Ci is the i'th Chinese character of the inputsentence, and with output pairs, ( nm ?
)???????????????????????
?mmTLTLTL,...,2211where Li is the word length of the i?th word inthe segmented word sequence, Ti is the word tag,and each (Li, Ti) pair is corresponding to asegmented and tagged word, and ?==mini1.It is easily seen that the distinction between thismodel and other models is that this one introducesword length.
In fact, word length really works, andaffects the performance of the system in a greatdeal, of which our later experiments will approve.The motivation to introduce word length intoour model is initially from the classical Chinesepoems.
When we read these poems, we mayspontaneously obey some laws in where to have apause.
For example, in most cases, a 7-character-lined Jueju(A kind of poem format) is read as**/**/***.
And the pauses in a sentence are muchrelated to the length of words or chunks.
Even inmodern Chinese, word length also plays a part.Sometimes we prefer to use disyllabic words ratherthan single one, though both are correct ingrammar.
For example, in our daily lives, wealways say ?
/n /v /n?
or ?
/n/v /n?, but seldom hear ?
/n /v /n?,where ?
?, ?
?
and ?
?
have the samemeaning.
So, it is reasonable to assume that theoccurrence of the word length will obey someunwritten laws when human writes or speaks.Introducing the word length into the wordsegmentation and POS tagging model may be inaccord with the needs for processing Chinese.Another main characteristic of the model is thatit is an integrated model, because there is only onehop through the input sentence to the output word-tag sequence.The following text will introduce how the modelworks.
We will also inherit n-gram assumption inour model.Our destination is to find a sequence of (Li, Ti)pairs that maximizes the probability,)|),(( CTLPi.e.
)|),((maxarg),(),(CTLPTLTLR =  ?
?
?
?
.
2.1(For conveniece, we will use Arto represent asquence of A1, A2, A3...)And,)(),(*)),(|()|),((CPTLPTLCPCTLP = .......2.2For )(CP  is a constant given a C , we just needto consider )),(|( TLCP and ),( TLP .First consider )),(|( TLCP .
Suppose W  is thevertex of words that ),( TL represents(i.e.
thesegmented word sequence), and the dependencyassumption is like the following Bayers Network:Figure 2.1: Dependency assumption amonglength-tag pair, word and characterSo, we have,)),(|(*)|()),(|( TLWPWCPTLCP = ?2.3Because W  is the segmentation of C ,)|( WCP  is always 1, and by another assumptionthat the occurrence of every word is independent toeach other, then?==miiii TLWPTLWP1)),(|()),(|( ???2.
4where )),(|( iii TLWP  means the conditionalprobability of Wi under Li and Ti.
For example,P(?
?|2, v) is the conditional probability of?
?
under a 2-charactered verb which may becomputed as (the number of ?
?
appearing as averb) / (the number of all 2-charactered verbs).With 2.3 and 2.4, )),(|( TLCP  is ready.Then consider ),( TLP , which is easy toretrieve when we apply n-gram assumption.Suppose n is 2, which means that (Li, Ti) onlydepends on (Li-1, Ti-1).
?=--=miiiii TLTLPTLP111 )),(|),((),( ?
?
..2.5Here )),(|),(( 11 -- iiii TLTLP means theprobability of a Tag Ti with Length Li appearingnext to Tag Ti-1 with Length Li-1, which may becomputed as (the number of (Li-1, Ti-1)(Li, Ti)appearing in corpus) / (the number of (Li-1,  Ti-1)appearing in corpus).
So, ),( TLP  is also ready.Combining formula 2.1, 2.2, 2.3, 2.4 and 2.5, wehave,?= --???????????????????????
?=mi iiiiiiiTLR TLTLPTLWPTL1 11),()|(*)|(maxarg),(............................................2.6Now, the enhanced model is complete with 2.6.When establishing the model, we have madeseveral assumptions.1.
the dependency assumption between tag-lengthpairs, words and characters like the Bayersnetwork of figure 2.12.
Word and word are independent.3.
n-gram assumption on (T,L) pairs.The validation of these assumptions is stillsomewhat in doubt, but the computationalcomplexity of the model is decreased.All the resources required to achieve this modelare also listed, i.e., a word list withprobability )|( ???????
?iii TLWP , and an n-gram transitionnetwork with probability ),...,|(1111???????????????????????
?--+-+-iininiiiTLTLTLP .The algorithm to implement this model is alsorather simple, and using Dynamic Programming,we could finish the algorithm in O(cn), where n isthe length of input sentence, and c is a constantrelated to the maximum ambiguity in a position.3 DiscussionThough the model itself is not difficult toimplement as we have presented in last section,there are still some problems that we will beprobably encountered with in practice.
The firstone is the data sparseness when we do the statistics.Another is how to further integrate Chinese NamedEntity Recognition into the new, word-length-introduced model.3.1 Data SparsenessThe Data Sparseness happens when we arecalculating ),...,|(1111???????????????????????
?--+-+-iininiiiTLTLTLP .
After theword length is introduced, the need for largercorpus is greatly increased.
Suppose we are using atri-gram assumption on length-tag pairs, thenumber of tags is 28 as that of our system, and themax word length is 6, then the number of patternswe should count is,28 * 6 * 28 * 6 * 28 * 6 = 4,741,632.To retrieve a reasonable statistical result, thescale of the corpus should at least be several timeslarger than that value.
It is common that we don?thave such a large corpus, and meet the problem socalled Data Sparseness.One way to deal with the problem is to find agood smoothing, and another is to make furtherindependent assumption between word length andword tag.
The word length sequence and word tagsequence can be considered independent.
Thatmeans,),...,|(*),...,|(),...,|(11111111-+--+---+-+-=???????????????????????
?iniiiniiiininiiiTTTPLLLPTLTLTLP.........................................3.1Now, the patterns to count are just as many asthose of a traditional n-gram assumption that onlyassumes the dependency among tags.3.2 Named Entity Recognition IntegrationNamed Entity Recognition is one of the mostimportant parts of word segmentation and POStagging systems, for the words in word list arelimited while the language seems infinite.
Thereare always new words appearing in humanlanguage, among which human names, placenames and organization names are most commonand most valuble  to recognize.
The performance ofNamed Entity Recognition will have a deep impacton the performance of a whole word segmentationand POS tagging system.
The research on NamedEntity Recognition has appeared for many years.No matter whether the current performance ofNamed Entity Recognition is ideal or not, we willnot discuss it here, and instead, we will just showhow to integrate the existing Name EntityRecognition methods into the new model.During the integration, more attention should bepaid to the structural and probabilistic consistency.For structural consistency, the original systemstructure does not need modifying when a newmethod of Named Entity Recognition is applied.For probabilistic  consistency, the probabilit iesoutputted by the Named Entity Recognition shouldbe compatible with the probabilit ies of the wordsin the original word list.Here, we will take the Human NameRecognition as an example to show how to do theintegration.
[Zheng Jiahen, et al 2000] has presented aprobabilistic  method for Chinese Human NameRecognition, which is easy to understand andsuitable to be borrowed as a demonstration.That paper defined the probability for a ChineseHuman Name as:)(*)()|( kEiFiknsP = ............................3.2)(*)(*)()|( kEjMiFijknpP = .............3.3Where each one of ?i?, ?j?, ?k?
represents asingle Chinese characters, ?ik?, ?ijk?
are the stringswhich may be a human name, ?ns?
means a singlename when ?j?
is empty, ?np?
means plural namewhen ?j?
is not empty, F(i) is the probability of ?i?being a family name, M(j) means the probability of?j?
being the middle character of a human name,E(k) means the probability of ?k?
being the tailingcharacter of a human name, P(ns | ik ) is theprobability of ?ik?
being a single name, and P(np |ijk ) is the probability of ?ijk?
being a plural name.F(i), M(j), and E(k) are easily retrieved fromcorpus, so P(ns | ik ) and P(np | ijk) can be known.However, P(ns | ik ) and P(np | ijk) do not satisfythe requirements of the word length introducedmodel.
The model needs probabilit ies like)),(|( tlwP , where w is a word, t is a word tag, andl is the word length.
Therefore, P(ns | ik) needs tobe modified into P(ik | nh, 2), for ik is always a 2-charactered word, and likewise, P(np | ijk) needs tobe modified into P(ijk | nh, 3), where ?nh?
is theword tag for human name in our system.P(ns | ik ) is equivalent to P(nh, 2 | ik ) and P(np |ijk ) is equivalent to P(nh, 3 | ijk).
P(ns | ik) can beconverted into P(ik | nh, 2) through following way,)2,()()()|2,()2,()()|2,()2,|(nhPkPiPiknhPnhPikPiknhPnhikP==.........................................3.4where ?i?, ?k?
have the same meaning with thosein 3.2 and 3.3. and nh is the tag for human name.In this formula, ?i?
and ?k?
are assumed to beindependent.
P(nh, 2), P(i), P(k) are easy toretrieve, which represent the probability of a 2-charactered human name, the probability ofcharacter ?i?
and the probability of character ?k?.P(nh, 2 | ik) is computed from 3.2.
Thus, theconversion of P(nk  | nh, 2) to P(nh, 2 | ik ) is done.In the same way, P(np | ijk) can be convertedinto P(ijk | nh, 3) by:)3,()()()()|3,()3,()()|3,()3,|(nhPkPjPiPiknhPnhPijkPijknhPnhijkP==...........................................3.5Finally, the Human Name Recognition Moduleis integrated into the whole system.
The inputstring C1, C2, ?, Cn first goes through the HumanName Recognition module, and the moduleoutputs a temporary word list, which consists of acolumn of words that are probably human namesand a column of probabilities corresponding to thewords, which can be computed by 3.4 and 3.5.
Thewhole system then merges the temporary word listand the original word list into a new word list, andapplies the new word list in segmenting andtagging C1, C2, ?, Cn.4 Conclusion & ExperimentsThis paper has presented an enhancedprobabilistic model of Chinese Lexical Analysis,which introduces word length as one of thefeatures and achieves the integration of wordsegmentation, Named Entity Recognition and POStagging.At last, we will briefly give the results of ourexperiments.
In the previous experiments, we havecompared many simple probabilistic models forChinese word segmentation and POS tagging, andfound that the system using maximum wordfrequency as segmentation strategy and forwardtri-gram Markov model as POS tagging model(MWF + FTMM) reaches the best performance.Our comparisons will be done between theMWF+FTMM and the enhance model with tri-gram assumption.
The training corpus is 40MBannotated Chinese text from People?s Daily.
Thetesting data is about 1MB in size and is fromPeople?s Daily, too.MWF+FTMM New ModelWSA 95.24% 97.09%PTA 97.12% 98.77%Total 92.50% 95.90%Table 4.1: The accuracy by word,with named entity not consideredMWF+FTMM New ModelWSA 93.86% 95.68%PTA 93.89% 95.72%Total 88.13% 91.59%Table 4.2: The accuracy by word,with named entity consideredMWF+FTMM New ModelWSA 69.46% 82.63%PTA 72.58% 80.33%Total 50.42% 66.38%Table 4.3: The accuracy by sentence,with named entity not consideredMWF+FTMM New ModelWSA 63.86% 74.78%PTA 61.40% 67.41%Total 39.21% 50.42%Table 4.4: The accuracy by sentence,with named entity consideredNOTES:MWF: Maximum Word Frequency, a very simplestrategy in word segmentation disambiguation,which chooses the word sequence with maxprobability as its result.FTMM: Forward Tri-gram Markov Model, apopular model in POS tagging.MWF+FTMM: A strategy, which chooses theoutput that makes a balance between the MWFand FTMM as its result.WSA (by word): Word Segmentation Accuracy,measured by recall, i.e.
the number of correctsegments divided by the number of segmentsin corpus.
(In a problem like word segmentation, theresult of precision measurement is commonlyaround that of recall measurement.
)PTA (by word): POS Tagging Accuracy based oncorrect segmentation, the number of words thatare correctly segmented and tagged divided bythe number of words that are correctlysegmented.Total (by word): total accuracy of the system,measured by recall, i.e.
the number of wordsthat are correctly segmented and taggeddivided by the number of words in corpus, orsimply WSA * PTA.WSA (by sentence): the number of correctlysegmented sentences divided by the number ofsentences in corpus.
A correctly segmentedsentence is a sentence whose words are allcorrectly segmented.PTA (by sentence): the number of correctly taggedsentences divided by the number of correctlysegmented sentences in corpus.
A correctlytagged sentence is a sentence whose words areall correctly segmented and tagged.Total (by sentence): WSA * PTA.Named entity considered or not: When namedentity is not considered, all the unknown wordsin corpus are deleted before evaluation.Otherwise, nothing is done on the corpus.According to the results above (Table 4.1, Table4.2, Table 4.3, Table 4.4), the new enhanced modeldoes better than the MWF + FTMM in every field.Introducing the word length into a Chinese wordsegmentation and POS tagging system seemseffective.This paper just focuses on the pure probabilisticmodel for word segmetation and POS tagging.
Itcan be predicted that, with more disambiguationstrategies, such as some rule based approaches,being implemented into the new model to achievea multi-engine system, the performance will befurther improved.5 AcknowledgementsThank Fang Hua and Kong Xianglong for theirprevious work, who have just graduated.ReferencesSun Maosong, Xu Dongliang, Benjamin K Tsou.2003.
Integrated Chinese word segmentation andpart-of-speech tagging based on the divide-and-conquer strategy.
International Conference onNatural Language Processing and KnowledgeEngineering Proceedings, Beijing.Zhang Huaping, Liu Qun, et al 2003.
Chineselexical analysis using hierarchical hiddenMarkov model.
2nd SIGHAN workshopaffiliated with 41th ACL, Sapporo JapanFu Guohong, Wang Ping, Wang Xiaolong.
2001.Research on the approach of integrating chinesewordd segmentation with part-of-speech tagging.Application Research of Computer.
(In Chinese)Gao Shan, Zhang Yan.
2001.
The Research onIntegrated Chinese Word Segmentation andLabeling based on trigram statistical model.Natural Language Understanding & MachineTranslation (JSCL-2001), Taiyuan.
(In Chinese)Zheng Jiahen, Li Xin, et al 2000.
The Research ofChinese names recognition method based oncorpus.
Journal of Chinese InformationProcessing.
(In Chinese)
