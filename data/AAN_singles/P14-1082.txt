Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 871?880,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsTranslation Assistance by Translation of L1 Fragments in an L2 ContextMaarten van Gompel & Antal van den BoschCentre for Language StudiesRadboud University Nijmegenproycon@anaproy.nlAbstractIn this paper we present new research intranslation assistance.
We describe a sys-tem capable of translating native language(L1) fragments to foreign language (L2)fragments in an L2 context.
Practical ap-plications of this research can be framed inthe context of second language learning.The type of translation assistance systemunder investigation here encourages lan-guage learners to write in their target lan-guage while allowing them to fall back totheir native language in case the correctword or expression is not known.
Thesecode switches are subsequently translatedto L2 given the L2 context.
We studythe feasibility of exploiting cross-lingualcontext to obtain high-quality translationsuggestions that improve over statisticallanguage modelling and word-sense dis-ambiguation baselines.
A classification-based approach is presented that is in-deed found to improve significantly overthese baselines by making use of a contex-tual window spanning a small number ofneighbouring words.1 IntroductionWhereas machine translation generally concernsthe translation of whole sentences or texts fromone language to the other, this study focusses onthe translation of native language (henceforth L1)words and phrases, i.e.
smaller fragments, in aforeign language (L2) context.
Despite the ma-jor efforts and improvements, automatic transla-tion does not yet rival human-level quality.
Vex-ing issues are morphology, word-order change andlong-distance dependencies.
Although there is amorpho-syntactic component in this research, ourscope is more constrained; its focus is on the faith-ful preservation of meaning from L1 to L2, akin tothe role of the translation model in Statistical Ma-chine Translation (SMT).The cross-lingual context in our research ques-tion may at first seem artificial, but its design ex-plicitly aims at applications related to computer-aided language learning (Laghos and Panayiotis,2005; Levy, 1997) and computer-aided transla-tion (Barrachina et al, 2009).
Currently, lan-guage learners need to refer to a bilingual dictio-nary when in doubt about a translation of a word orphrase.
Yet, this problem arises in a context, notin isolation; the learner may have already trans-lated successfully a part of the text into L2 leadingup to the problematic word or phrase.
Dictionar-ies are not the best source to look up context; theymay contain example usages, but remain biased to-wards single words or short expressions.The proposed application allows code switch-ing and produces context-sensitive suggestions aswriting progresses.
In this research we test thefeasibility of the foundation of this idea.The fol-lowing examples serve to illustrate the idea anddemonstrate what output the proposed translationassistance system would ideally produce.
Theparts in bold correspond to respectively the in-serted fragment and the system translation.?
Input (L1=English,L2=Spanish): ?Hoy va-mos a the swimming pool.
?Desired output: ?Hoy vamos a la piscina.??
Input (L1-English, L2=German): ?Das wet-ter ist wirklich abominable.
?Desired output: ?Das wetter ist wirklichekelhaft.??
Input (L1=French,L2=English): ?I rentre `ala maison because I am tired.
?Desired output: ?I return home because I amtired.??
Input (L1=Dutch, L2=English): ?Workersare facing a massive aanval op their employ-871ment and social rights.
?Desired output: ?Workers are facing a mas-sive attack on their employment and socialrights.
?The main research question in this research ishow to disambiguate an L1 word or phrase toits L2 translation based on an L2 context, andwhether such cross-lingual contextual approachesprovide added value compared to baseline modelsthat are not context informed or compared to stan-dard language models.2 Data preparationPreparing the data to build training and test datafor our intended translation assistance system isnot trivial, as the type of interactive translation as-sistant we aim to develop does not exist yet.
Weneed to generate training and test data that real-istically emulates the task.
We start with a par-allel corpus that is tokenised for both L1 and L2.No further linguistic processing such as part-of-speech tagging or lemmatisation takes place in ourexperiments; adding this remains open for futureresearch.The parallel corpus is randomly sampled intotwo large and equally-sized parts.
One is the basisfor the training set, and the other is the basis forthe test set.
The reason for such a large test splitshall become apparent soon.From each of the splits (S), a phrase-translationtable is constructed automatically in an unsuper-vised fashion.
This is done using the scriptsprovided by the Statistical Machine Translationsystem Moses (Koehn et al, 2007).
It invokesGIZA++ (Och and Ney, 2000) to establish sta-tistical word alignments based on the IBM Mod-els and subsequently extracts phrases using thegrow-diag-final algorithm (Och and Ney,2003).
The result, independent for each set, willbe a phrase-translation table (T ) that maps phrasesin L1 to L2.
For each phrase-pair (fs, ft) thisphrase-translation table holds the computed trans-lation probabilities P (fs|ft) and P (ft|fs).Given these phrase-translation tables, we cannow extract both training data and test data usingthe algorithm in Figure 1.
In our discourse, thesource language (s) corresponds to L1, the fall-back language used for by the end-user for insert-ing fragments, whilst the target language (t) is L2.Step 4 is effectively a filter: two thresholdscan be configured to discard weak alignments,1.
using phrase-translation table T and par-allel corpus split S2.
for each aligned sentence pair(sentences?
Ss, sentencet?
St)in the parallel corpus split (Ss,St):3. for each fragment (fs?sentences, ft?
sentencet) where(fs, ft) ?
T :4. if P (fs|ft) ?
P (ft|fs) ?
?1and P (fs|ft) ?
P (ft|fs) ?
?2?P (fs|fstrongest t) ?
P (fstrongest t|fs):5.
Output a pair(sentence?t, sentencet) wheresentence?tis a copy of t but withfragment ftsubstituted by fs, i.e.
theintroduction of an L1 word or phrase inan L2 sentence.Figure 1: Algorithm for extracting training andtest data on the basis of a phrase-translation ta-ble (T ) and subset/split from a parallel corpus (S).The indentation indicates the nesting.i.e.
those with low probabilities, from the phrase-translation table so that only strong couplingsmake it into the generated set.
The parameter?1adds a constraint based on the product of thetwo conditional probabilities (P (ft|fs)?P (fs|ft)),and sets a threshold that has to be surpassed.A second parameter ?2further limits the con-sidered phrase pairs (fs, ft) to have the prod-uct of their conditional probabilities not not devi-ate more than a fraction ?2from the joint prob-ability for the strongest possible pairing for fs,the source fragment.
fstrongest tin Figure 1corresponds to the best scoring translation for agiven source fragment fs.
This metric thus effec-tively prunes weaker alternative translations in thephrase-translation table from being considered ifthere is a much stronger candidate.
Nevertheless,it has to be noted that even with ?1and ?2, the testset will include a certain amount of errors.
This isdue to the nature of the unsupervised method withwhich the phrase-translation table is constructed.For our purposes however, the test set suffices totest our hypothesis.872In our experiments, we choose fixed values forthese parameters, by manual inspection and judge-ment of the output.
The ?1parameter was set to0.01 and ?2to 0.8.
Whilst other thresholds maypossibly produce cleaner sets, this is hard to eval-uate as finding optimal values causes a prohibitiveincrease in complexity of the search space, andagain this is not necessary to test our hypothesis.The output of the algorithm in Fig-ure 1 is a modified set of sentence pairs(sentence?t, sentencet), in which the samesentence pair may be used multiple times withdifferent L1 substitutions for different fragments.The final test set is created by randomly samplingthe desired number of test instances.Note that the training set and test set are con-structed on their own respective and indepen-dently generated phrase-translation tables.
Thisensures complete independence of training andtest data.
Generating test data using the samephrase-translation table as the training data wouldintroduce a bias.
The fact that a phrase-translationtable needs to be constructed for the test data isalso the reason that the parallel corpus split fromwhich the test data is derived has to be largeenough, ensuring better quality.We concede that our current way of testing isa mere approximation of the real-world scenario.An ideal test corpus would consist of L2 sentenceswith L1 fallback as crafted by L2 language learn-ers with an L1 background.
However, such cor-pora do not exist as yet.
Nevertheless, we hope toshow that our automated way of test set genera-tion is sufficient to test the feasibility of our corehypothesis that L1 fragments can be translated toL2 using L2 context information.3 SystemWe develop a classifier-based system composed ofso-called ?classifier experts?.
Numerous classi-fiers are trained and each is an expert in translatinga single word or phrase.
In other words, for eachword type or phrase type that occurs as a fragmentin the training set, and which does not map to just asingle translation, a classifier is trained.
The clas-sifier maps the L1 word or phrase in its L2 contextto its L2 translation.
Words or phrases that alwaysmap to a single translation are stored in a sim-ple mapping table, as a classifier would have noadded value in such cases.
The classifiers use theIB1 algorithm (Aha et al, 1991) as implementedin TiMBL (Daelemans et al, 2009).1IB1 im-plements k-nearest neighbour classification.
Thechoice for this algorithm is motivated by the factthat it handles multiple classes with ease, but firstand foremost because it has been successfully em-ployed for word sense disambiguation in otherstudies (Hoste et al, 2002; Decadt et al, 2004),in particular in cross-lingual word sense disam-biguation, a task closely resembling our currenttask (van Gompel and van den Bosch, 2013).
Ithas also been used in machine translation stud-ies in which local source context is used to clas-sify source phrases into target phrases, rather thanlooking them up in a phrase table (Stroppa et al,2007; Haque et al, 2011).
The idea of local phraseselection with a discriminative machine learningclassifier using additional local (source-language)context was introduced in parallel to Stroppa et al(2007) by Carpuat and Wu (2007) and Gim?enezand M?arquez (2007); cf.
Haque et al (2011) foran overview of more recent methods.The feature vector for the classifiers representsa local context of neighbouring words, and op-tionally also global context keywords in a binary-valued bag-of-words configuration.
The local con-text consists of an X number of L2 words to theleft of the L1 fragment, and Y words to the right.When presented with test data, in which theL1 fragment is explicitly marked, we first checkwhether there is ambiguity for this L1 fragmentand if a direct translation is available in our sim-ple mapping table.
If so, we are done quickly andneed not rely on context information.
If not, wecheck for the presence of a classifier expert for theoffered L1 fragment; only then we can proceed byextracting the desired number of L2 local contextwords to the immediate left and right of this frag-ment and adding those to the feature vector.
Theclassifier will return a probability distribution ofthe most likely translations given the context andwe can replace the L1 fragment with the highestscoring L2 translation and present it back to theuser.In addition to local context features, we also ex-perimented with global context features.
Theseare a set of L2 contextual keywords for each L1word/phrase and its L2 translation occurring in thesame sentence, not necessarily in the immediateneighbourhood of the L1 word/phrase.
The key-words are selected to be indicative for a specific1http://ilk.uvt.nl/timbl873translation.
We used the method of extraction byNg and Lee (1996) and encoded all keywords ina binary bag of words model.
The experimentshowever showed that inclusion of such keywordsdid not make any noticeable impact on any of theresults, so we restrict ourselves to mentioning thisnegative result.Our full system, including the scripts fordata preparation, training, and evaluation, isimplemented in Python and freely availableas open-source from http://github.com/proycon/colibrita/ .
Version tag v0.2.1is representative for the version used in this re-search.3.1 Language ModelWe also implement a statistical language model asan optional component of our classifier-based sys-tem and also as a baseline to compare our systemto.
The language model is a trigram-based back-off language model with Kneser-Ney smooth-ing, computed using SRILM (Stolcke, 2002) andtrained on the same training data as the translationmodel.
No additional external data was broughtin, to keep the comparison fair.For any given hypothesisH , results from the L1to L2 classifier are combined with results from theL2 language model.
We do so by normalising theclass probability from the classifier (scoreT(H)),which is our translation model, and the languagemodel (scorelm(H)), in such a way that the high-est classifier score for the alternatives under con-sideration is always 1.0, and the highest languagemodel score of the sentence is always 1.0.
TakescoreT(H) and scorelm(H) to be log probabili-ties, the search for the best (most probable) trans-lation hypothesis?H can then be expressed as:?H = argmaxH(scoreT(H) + scorelm(H)) (1)If desired, the search can be parametrised withvariables ?3and ?4, representing the weights wewant to attach to the classifier-based translationmodel and the language model, respectively.
Inthe current study we simply left both weights set toone, thereby assigning equal importance to trans-lation model and language model.4 EvaluationSeveral automated metrics exist for the evaluationof L2 system output against the L2 reference out-put in the test set.
We first measure absolute accu-racy by simply counting all output fragments thatexactly match the reference fragments, as a frac-tion of the total amount of fragments.
This mea-sure may be too strict, so we add a more flexibleword accuracy measure which takes into accountpartial matches at the word level.
If output o isa subset of reference r then a score of|o||r|is as-signed for that sentence pair.
If instead, r is a sub-set of o, then a score of|r||o|will be assigned.
Aperfect match will result in a score of 1 whereasa complete lack of overlap will be scored 0.
Theword accuracy for the entire set is then computedby taking the sum of the word accuracies per sen-tence pair, divided by the total number of sentencepairs.We also compute a recall metric that measuresthe number of fragments that the system provideda translation for as a fraction of the total numberof fragments in the input, regardless of whetherthe fragment is translated correctly or not.
Thesystem may skip fragments for which it can findno solution at all.In addition to these, the system?s output can becompared against the L2 reference translation(s)using established Machine Translation evaluationmetrics.
We report on BLEU, NIST, METEOR,and word error rate metrics WER and PER.
Thesescores should generally be much better than thetypical MT system performances as only localchanges are made to otherwise ?perfect?
L2 sen-tences.5 BaselinesA context-insensitive yet informed baseline wasconstructed to assess the impact of L2 context in-formation in translating L1 fragments.
The base-line selects the most probable L1 fragment per L2fragment according to the phrase-translation ta-ble.
This baseline, henceforth referred to as the?most likely fragment?
baseline (MLF) is analo-gous to the ?most frequent sense?-baseline com-mon in evaluating WSD systems.A second baseline was constructed by weigh-ing the probabilities from the translation table di-rectly with the L2 language model described ear-lier.
It adds a LM component to the MLF base-line.
This LM baseline allows the comparison ofclassification through L1 fragments in an L2 con-text, with a more traditional L2 context modelling(i.e.
target language modelling) which is also cus-874tomary in MT decoders.
Computing this base-line is done in the same fashion as previously il-lustrated in Equation 1, where scoreTthen repre-sents the normalised p(t|s) score from the phrase-translation table rather than the class probabilityfrom the classifier.6 Experiments & ResultsThe data for our experiments were drawn fromthe Europarl parallel corpus (Koehn, 2005) fromwhich we extracted two sets of 200, 000 sentencepairs each for several language pairs.
These wereused to form the training and test sets.
The finaltest sets are a randomly sampled 5, 000 sentencepairs from the 200, 000-sentence test split for eachlanguage pair.All input data for the experiments in this sectionare publicly available2.Let us first zoom in to convey a sense of scaleon a specific language pair.
The actual Europarltraining set we generate for English (L1) to Span-ish (L2), i.e.
English fallback in a Spanish con-text, consists of 5, 608, 015 sentence pairs.
Thisnumber is much larger than the 200, 000 we men-tioned before because single sentence pairs may bereused multiple times with different marked frag-ments.
From this training set of sentence pairsover 100, 000 classifier experts are derived.
Theeleven largest classifiers are shown in Table 1,along with the number of training instances perclassifier.
The full table would reveal a Zipfiandistribution.Fragment Training instances Translationsthe 256,772 la, el, los, lasof 139,273 de, deland 128,074 y, de, eto 66,565 a, para, que, dea 54,306 un, unais 40,511 es, est?a, sefor 34,054 para, de, porthis 29,691 este, esta, estoEuropean 26,543Europea, EuropeoEuropeas, Europeoson 23,147 sobre, enof the 22,361 de la, de losTable 1: The top eleven classifier experts for En-glish to Spanish.
The eleventh entry is included asan example of a common phrasal fragmentAmong the classifier experts are only words andphrases that are ambiguous and may thus map to2Download and unpack http://lst.science.ru.nl/?proycon/colibrita-acl2014-data.zipFigure 2: Accuracy for different local contextsizes, Europarl English to Spanishmultiple translations.
This implies that such wordsand phrases must have occurred at least twice inthe corpus, though this threshold is made config-urable and could have been set higher to limit thenumber of classifiers.
The remaining 246, 380 un-ambiguous mappings are stored in a separate map-ping table.For the classifier-based system, we tested var-ious different feature vector configurations.
Thefirst experiment, of which the results are shown inFigure 2, sets a fixed and symmetric local contextsize across all classifiers, and tests three contextwidths.
Here we observe that a context width ofone yields the best results.
The BLEU scores, notincluded in the figure but shown in Table 2, showa similar trend.
This trend holds for all the MTmetrics.Table 2 shows the results for English to Span-ish in more detail and adds a comparison with thetwo baseline systems.
The various lXrY config-urations use the same feature vector setup for allclassifier experts.
HereX indicates the left contextsize and Y the right context size.
The auto con-figuration does not uniformly apply the same fea-ture vector setup to all classifier experts but insteadseeks to find the optimal setup per classifier expert.This shall be further discussed in Section 6.1.As expected, the LM baseline substantially out-performs the context-insensitive MLF baseline.Second, our classifier approach attains a sub-stantially higher accuracy than the LM baseline.Third, we observe that adding the language modelto our classifier leads to another significant gain875Configuration Accuracy Word Accuracy BLEU METEOR NIST WER PERMLF baseline 0.6164 0.6662 0.972 0.9705 17.0784 1.4465 1.4209LM baseline 0.7158 0.7434 0.9785 0.9739 17.1573 1.1735 1.1574l1r1 0.7588 0.7824 0.9801 0.9747 17.1550 1.1625 1.1444l2r2 0.7574 0.7801 0.9800 0.9746 17.1550 1.1750 1.1569l3r3 0.7514 0.7742 0.9796 0.9744 17.1445 1.1946 1.1780l1r1+LM 0.7810 0.7973 0.9816 0.9754 17.1685 1.0946 1.077auto 0.7626 0.7850 0.9803 0.9748 17.1544 1.1594 1.1424auto+LM 0.7796 0.7966 0.9815 0.9754 17.1664 1.1021 1.0845l1r0 0.6924 0.7223 0.9757 0.9723 17.1087 1.3415 1.3249l2r0 0.6960 0.7245 0.9759 0.9724 17.1091 1.3364 1.3193l2r1 0.7624 0.7849 0.9803 0.9748 17.1558 1.1554 1.1378Table 2: Europarl results for English to Spanish (i.e English fallback in Spanish context).
Recall =0.9422(configuration l1r1+LM in the results in Ta-ble 2).
It appears that the classifier approach andthe L2 language model are able to complementeach other.Statistical significance on the BLEU scores wastested using pairwise bootstrap sampling (Koehn,2004).
All significance tests were performedwith 5, 000 iterations.
We compared the out-comes of several key configurations.
We firsttested l1r1 against both baselines; both differ-ences are significant at p < 0.01 for both.
Thesame significance level was found when compar-ing l1r1+LM against l1r1, auto+LM againstauto, as well as the LM baseline against the MLFbaseline.
Automatic feature selection auto wasfound to perform statistically better than l1r1,but only at p < 0.05.
Conclusions with regard tocontext width may have to be tempered somewhat,as the performance of the l1r1 configuration wasfound to not be significantly better than that of thel2r2 configuration.
However, l1r1 performssignificantly better than l3r3 at p < 0.01, andl2r2 performs significantly better than l3r3 atp < 0.01.In Table 3 we present some illustrative exam-ples from the English?Spanish Europarl data.We show the difference between the most-likely-fragment baseline and our system.Likewise, Table 4 exemplifies small fragmentsfrom the l1r1 configuration compared to thesame configuration enriched with a languagemodel.
We observe in this data that the languagemodel often has the added power to choose a cor-rect translation that is not the first prediction ofthe classifier, but one of the weaker alternativesthat nevertheless fits better.
Though the classifiergenerally works best in the l1r1 configuration,i.e.
with context size one, the trigram-based lan-guage model allows further left-context informa-tion to be incorporated that influences the weightsof the classifier output, successfully forcing thesystem to select alternatives.
This combinationof a classifier with context size one and trigram-based language model proves to be most effectiveand reaches the best results so far.
We have notconducted experiments with language models ofother orders.6.1 Context optimisationIt has been argued that classifier experts in a wordsense disambiguation ensemble should be individ-ually optimised (Decadt et al, 2004; van Gompeland van den Bosch, 2013).
The latter study oncross-lingual WSD finds a positive impact whenconducting feature selection per classifier.
This in-tuitively makes sense; a context of one may seemto be better than any other when uniformly appliedto all classifier experts, but it may well be that cer-tain classifiers benefit from different feature selec-tions.
We therefore proceed with this line of inves-tigation as well.Automatic configuration selection was done byperforming leave-one-out testing (for small num-ber of instances) or 10-fold-cross validation (forlarger number of instances, n ?
20) on the train-ing data per classifier expert.
Various configura-tions were tested.
Per classifier expert, the bestscoring configuration was selected, referred to asthe auto configuration in Table 2.
The autoconfiguration improves results over the uniformly876Input: Mientras no haya prueba en contrario , la financiaci?on de partidos pol?
?ticos European s?olo se justifica , inclusodespu?es del tratado de Niza , desde el momento en que concurra a la expresi?on del sufragio universal , que es la ?unicadefinici?on aceptable de un partido pol?
?tico .MLF baseline: Mientras no haya prueba en contrario , la financiaci?on de partidos pol?
?ticos Europea s?olo se justifica ,incluso despu?es del tratado de Niza , desde el momento en que concurra a la expresi?on del sufragio universal , que es la?unica definici?on aceptable de un partido pol?
?tico .l1r1: Mientras no haya prueba en contrario , la financiaci?on de partidos pol?
?ticos europeos s?olo se justifica , inclusodespu?es del tratado de Niza , desde el momento en que concurra a la expresi?on del sufragio universal , que es la ?unicadefinici?on aceptable de un partido pol?
?tico .Input: Esta Directiva es nuestra oportunidad to marcar una verdadera diferencia , reduciendo la tr?agica p?erdida de vidasen nuestras carreteras .MLF baseline: Esta Directiva es nuestra oportunidad a marcar una verdadera diferencia , reduciendo la tr?agica p?erdidade vidas en nuestras carreteras .l1r1: Esta Directiva es nuestra oportunidad para marcar una verdadera diferencia , reduciendo la tr?agica p?erdida de vidasen nuestras carreteras .Input: Es la last vez que me dirijo a esta C?amara .MLF baseline: Es la pasado vez que me dirijo a esta C?amara .l1r1: Es la ?ultima vez que me dirijo a esta C?amara .Input: Pero el enfoque actual de la Comisi?on no puede conducir a una buena pol?
?tica ya que es tributario del fun-cionamiento del mercado y de las normas establecidas por la OMC , el FMI y el Banco Mundial , normas que siguensiendo desfavorables para los developing countries .MLF baseline: Pero el enfoque actual de la Comisi?on no puede conducir a una buena pol?
?tica ya que es tributario delfuncionamiento del mercado y de las normas establecidas por la OMC , el FMI y el Banco Mundial , normas que siguensiendo desfavorables para los los pa?
?ses en desarrollo .l1r1: Pero el enfoque actual de la Comisi?on no puede conducir a una buena pol?
?tica ya que es tributario del funcionamientodel mercado y de las normas establecidas por la OMC , el FMI y el Banco Mundial , normas que siguen siendo desfavor-ables para los pa?
?ses en desarrollo .Table 3: Some illustrative examples of MLF-baseline output versus system output, in which systemoutput matches the correct human reference output.
The actual fragments concerned are highlighted inbold.
The first example shows our system correcting for number agreement, the second a correctionin selecting the right preposition, and the third shows that the English word last can be translated indifferent ways, only one of which is correct in this context.
The last example shows a phrasal translation,in which the determiner was duplicated in the baselineapplied feature selection.
However, if we enablethe language model as we do in the auto+LMconfiguration we do not notice an improvementover l1r1+LM, surprisingly.
We suspect the lackof impact here can be explained by the trigram-based Language Model having less added valuewhen the (left) context size of the classifier is twoor three; they are now less complementary.Table 5 lists what context sizes have been cho-sen in the automatic feature selection.
A contextsize of one prevails in the vast majority of cases,which is not surprising considering the good re-sults we have already seen with this configuration.In this study we did not yet conduct optimisa-tion of the classifier parameters.
We used the IB1algorithm with k = 1 and the default values ofthe TiMBL implementation.
In earlier work vanGompel and van den Bosch (2013), we reporteda decrease in performance due to overfitting when66.5% l1r119.9% l2r27.7% l3r33.5% l4r42.4% l5r5Table 5: Frequency of automatically selected con-figurations on English to Spanish Europarl datasetthis is done, so we do not expect it to make a pos-itive impact.
The second reason for omitting thisis more practical in nature; to do this in combina-tion with feature selection would add substantialsearch complexity, making experiments far moretime consuming, even prohibitively so.The bottom lines in Table 2 represent resultswhen all right-context is omitted, emulating a real-time prediction when no right context is availableyet.
This has a substantial negative impact on re-877Input: Sin ese tipo de protecci?on la gente no aprovechar?a la oportunidad to vivir , viajar y trabajar donde les parezca enla Uni?on Europea .l1r1: Sin ese tipo de protecci?on la gente no aprovechar?a la oportunidad para vivir , viajar y trabajar donde les parezca enla Uni?on Europea .l1r1+LM: Sin ese tipo de protecci?on la gente no aprovechar?a la oportunidad de vivir , viajar y trabajar donde les parezcaen la Uni?on Europea .Input: La Comisi?on tambi?en est?a acometiendo medidas en el ?ambito social y educational con vistas a mejorar lasituaci?on de los ni?nos .l1r1: La Comisi?on tambi?en est?a acometiendo medidas en el ?ambito social y educativas con vistas a mejorar la situaci?onde los ni?nos .l1r1+LM: La Comisi?on tambi?en est?a acometiendo medidas en el ?ambito social y educativo con vistas a mejorar lasituaci?on de los ni?nos .Table 4: Some examples of l1r1 versus the same configuration enriched with a language model.sults.
We experimented with several asymmetricconfigurations and found that taking two words tothe left and one to the right yields even better re-sults than symmetric configurations for this dataset.
This result is in line with the positive effect ofadding the LM to the l1r1.In order to draw accurate conclusions, experi-ments on a single data set and language pair are notsufficient.
We therefore conducted a number of ex-periments with other language pairs, and presentthe abridged results in Table 6.There are some noticeable discrepancies forsome experiments in Table 6 when compared toour earlier results in Table 2.
We see that the lan-guage model baseline for English?French showsthe same substantial improvement over the base-line as our English?Spanish results.
The sameholds for the Chinese?English experiment.
How-ever, for English?Dutch and English?Chinesewe find that the LM baseline actually performsslightly worse than baseline.
Nevertheless, in allthese cases, the positive effect of including a Lan-guage Model to our classifier-based system againshows.
Also, we note that in all cases our systemperforms better than the two baselines.Another discrepancy is found in the BLEUscores of the English?Chinese experiments,where we measure an unexpected drop in BLEUscore under baseline.
However, all other scores doshow the expected improvement.
The error ratemetrics show improvement as well.
We thereforeattach low importance to this deviation in BLEUhere.In all of the aforementioned experiments, thesystem produced a single solution for each of thefragments, the one it deemed best, or no solutionat all if it could not find any.
Alternative evaluationmetrics could allow the system to output multiplealternatives.
Omission of a solution by definitioncauses a decrease in recall.
In all of our experi-ments recall is high (well above 90%), mostly be-cause train and test data lie in the same domain andhave been generated in the same fashion, lower re-call is expected with more real-world data.7 Discussion and conclusionIn this study we have shown the feasibility ofa classifier-based translation assistance system inwhich L1 fragments are translated in an L2 con-text, in which the classifier experts are built indi-vidually per word or phrase.
We have shown thatsuch a translation assistance system scores bothabove a context-insensitive baseline, as well as anL2 language model baseline.Furthermore, we found that combining thiscross-language context-sensitive technique withan L2 language model boosts results further.The presence of a one-word right-hand sidecontext proves crucial for good results, which hasimplications for practical translation assistance ap-plication that translate as soon as the user finishesan L1 fragment.
Revisiting the translation whenright context becomes available would be advis-able.We tested various configurations and concludethat small context sizes work better than largerones.
Automated configuration selection had pos-itive results, yet the system with context size oneand an L2 language model component often pro-duces the best results.
In static configurations, thefailure of a wider context window to be more suc-878Dataset L1 L2 Configuration Accuracy Word Accuracy BLEUeuroparl200k en nl baseline 0.7026 0.7283 0.9771europarl200k en nl LM baseline 0.6958 0.7195 0.9773europarl200k en nl l1r1 0.7790 0.7941 0.9814europarl200k en nl l1r1+LM 0.7838 0.7973 0.9818europarl200k en nl auto 0.7796 0.7947 0.9815europarl200k en nl auto+LM 0.7812 0.7954 0.9816europarl200k en fr baseline 0.5874 0.6403 0.9709europarl200k en fr LM baseline 0.7054 0.7319 0.9787europarl200k en fr l1r1 0.7416 0.7698 0.9797europarl200k en fr l1r1+LM 0.7680 0.7885 0.9815europarl200k en fr auto 0.7484 0.7737 0.9801europarl200k en fr auto+LM 0.7654 0.7860 0.9813iwslt12ted en zh baseline 0.6622 0.7122 0.6421iwslt12ted en zh LM baseline 0.6550 0.6982 0.6416iwslt12ted en zh l1r1 0.7150 0.7531 0.5736iwslt12ted en zh l1r1+LM 0.7296 0.7619 0.5826iwslt12ted en zh auto 0.7150 0.7519 0.5746iwslt12ted en zh auto+LM 0.7280 0.7605 0.5833iwslt12ted zh en baseline 0.5784 0.6167 0.9634iwslt12ted zh en LM baseline 0.6148 0.6463 0.9656iwslt12ted zh en l1r1 0.7104 0.7338 0.9709iwslt12ted zh en l1r1+LM 0.7270 0.7460 0.9721iwslt12ted zh en auto 0.7078 0.7319 0.9709iwslt12ted zh en auto+LM 0.7230 0.7428 0.9719Table 6: Results on different datasets and language pairs.
The iwslt12ted set is the dataset used in theIWSLT 2012 Evaluation Campaign (Federico et al, 2012), and is formed by a collection of transcriptionsof TED talks.
Here we used of just over 70, 000 sentences for training.
Recall for each of the four datasetsis 0.9498 (en-nl), 0.9494 (en-fr), 0.9386 (en-zh), and 0.9366 (zh-en)cesful may be attributed to the increased sparsitythat comes from such an expansion.The idea of a comprehensive translation assis-tance system may extend beyond the translation ofL1 fragments in an L2 context.
There are moreNLP components that might play a role if such asystem were to find practical application.
Wordcompletion or predictive editing (in combinationwith error correction) would for instance seem anindispensable part of such a system, and can beimplemented alongside the technique proposed inthis study.
A point of more practically-orientedfuture research is to see how feasible such combi-nations are and what techniques can be used.An application of our idea outside the area oftranslation assistance is post-correction of the out-put of some MT systems that, as a last-resortheuristic, copy source words or phrases into theiroutput, producing precisely the kind of input oursystem is trained on.
Our classification-based ap-proach may be able to resolve some of these casesoperating as an add-on to a regular MT system ?or as a independent post-correction system.Our system allows L1 fragments to be of arbi-trary length.
If a fragment was not seen duringtraining stage, and is therefore not covered by aclassifier expert, then the system will be unableto translate it.
Nevertheless, if a longer L1 frag-ment can be decomposed into subfragments thatare known, then some recombination of the trans-lations of said sub-fragments may be a good trans-lation for the whole.
We are currently exploringthis line of investigation, in which the gap withMT narrows further.Finally, an important line of future researchis the creation of a more representative test set.Lacking an interactive system that actually doeswhat we emulate, we hypothesise that good ap-proximations would be to use gap exercises, orcloze tests, that test specific aspects difficultiesin language learning.
Similarly, we may useL2 learner corpora with annotations of code-switching points or errors.
Here we then assumethat places where L2 errors occur may be indica-tive of places where L2 learners are in some trou-ble, and might want to fall back to generating L1.By then manually translating gaps or such prob-lematic fragments into L1 we hope to establish amore realistic test set.ReferencesD.
W. Aha, D. Kibler, and M. K. Albert.
1991.Instance-based learning algorithms.
Machine879Learning, 06(1):37?66, January.S.
Barrachina, O. Bender, F. Casacuberta, J. Civera,E.
Cubel, S. Khadivi, A. L. Lagarda, H. Ney,J.
Tom?as, E. Vidal, and J.M.
Vilar.
2009.
Statisticalapproaches to computer-assisted translation.
Com-putational Linguistics, 35(1):3?28.M.
Carpuat and D. Wu.
2007.
Improving statis-tical machine translation using word sense disam-biguation.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL), pages 61?72.W.
Daelemans, J. Zavrel, K. van der Sloot, and A. vanden Bosch.
2009.
TiMBL: Tilburg memory basedlearner, version 6.2, reference guide.
Technical Re-port ILK 09-01, ILK Research Group, Tilburg Uni-versity.B.
Decadt, V. Hoste, W. Daelemans, and A. van denBosch.
2004.
GAMBL, genetic algorithm optimiza-tion of memory-based WSD.
In R. Mihalcea andP.
Edmonds, editors, Proceedings of the Third In-ternational Workshop on the Evaluation of Systemsfor the Semantic Analysis of Text (Senseval-3), pages108?112, New Brunswick, NJ.
ACL.M.
Federico, M. Cettolo, L. Bentivogli, M. Paul, andS.
St?uker.
2012.
Overview of the IWSLT 2012 eval-uation campaign.
In Proceedings of the seventh In-ternational Workshop on Spoken Language Transla-tion (IWSLT), pages 12?33.J.
Gim?enez and L. M`arquez.
2007.
Context-aware dis-criminative phrase selection for statistical machinetranslation.
In Proceedings of the Second Workshopon Statistical Machine Translation, pages 159?166,Prague, Czech Republic, June.
Association for Com-putational Linguistics.R.
Haque, S. Kumar Naskar, A. van den Bosch, andA.
Way.
2011.
Integrating source-language con-text into phrase-based statistical machine transla-tion.
Machine Translation, 25(3):239?285, Septem-ber.V.
Hoste, I. Hendrickx, W. Daelemans, and A. vanden Bosch.
2002.
Parameter optimization for ma-chine learning of word sense disambiguation.
Natu-ral Language Engineering, 8(4):311?325.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkitfor statistical machine translation.
In Proceedings ofthe 45th Annual Meeting of the Association for Com-putational Linguistics Companion Volume Proceed-ings of the Demo and Poster Sessions, pages 177?180, Prague, Czech Republic, June.
Association forComputational Linguistics.P.
Koehn.
2004.
Statistical significance tests for ma-chine translation evaluation.
In Dekang Lin andDekai Wu, editors, Proceedings of EMNLP 2004,pages 388?395, Barcelona, Spain, July.
Associationfor Computational Linguistics.P.
Koehn.
2005.
Europarl: A parallel corpus for statis-tical machine translation.
In In Proceedings of theMachine Translation Summit X ([MT]?05)., pages79?86.A.
Laghos and Z. Panayiotis.
2005.
Computer assist-ed/aided language learning.
pages 331?336.M.
Levy.
1997.
Computer-assisted language learning:Context and conceptualization.
Oxford: ClarendonPress.H.
Tou Ng and H. Beng Lee.
1996.
Integrating multi-ple knowledge sources to disambiguate word sense:An exemplar-based approach.
In ACL, pages 40?47.F.J.
Och and H. Ney.
2000.
Giza++: Training of sta-tistical translation models.
Technical report, RWTHAachen, University of Technology.F.
J. Och and H. Ney.
2003.
A systematic compari-son of various statistical alignment models.
Comput.Linguist., 29(1):19?51, March.A.
Stolcke.
2002.
Srilm - an extensible language mod-eling toolkit.
In John H. L. Hansen and Bryan L.Pellom, editors, 7th International Conference onSpoken Language Processing, ICSLP2002 - INTER-SPEECH 2002, Denver, Colorado, USA, September16-20, 2002.
ISCA.N.
Stroppa, A. van den Bosch, and A.
Way.
2007.Exploiting source similarity for SMT using context-informed features.
In A.
Way and B. Gawronska,editors, Proceedings of the 11th International Con-ference on Theoretical Issues in Machine Transla-tion (TMI 2007), pages 231?240, Sk?ovde, Sweden.M.
van Gompel and A. van den Bosch.
2013.
WSD2:Parameter optimisation for memory-based cross-lingual word-sense disambiguation.
In Proceedingsof the 7th International Workshop on Semantic Eval-uation (SemEval 2013), in conjunction with the Sec-ond Joint Conference on Lexical and ComputationalSemantics.880
