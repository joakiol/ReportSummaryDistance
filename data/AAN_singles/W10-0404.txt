Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing, pages 24?32,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsThe Design of a Proofreading Software ServiceRaphael MudgeAutomatticWashington, DC 20036raffi@automattic.comAbstractWeb applications have the opportunity tocheck spelling, style, and grammar using asoftware service architecture.
A software ser-vice authoring aid can offer contextual spellchecking, detect real word errors, and avoidpoor grammar checker suggestions throughthe use of large language models.
Here wepresent After the Deadline, an open source au-thoring aid, used in production on Word-Press.com, a blogging platform with over tenmillion writers.
We discuss the benefits of thesoftware service environment and how it af-fected our choice of algorithms.
We summa-rize our design principles as speed overaccuracy, simplicity over complexity, and dowhat works.1 IntroductionOn the web, tools to check writing lag behind thoseoffered on the desktop.
No online word processingsuite has a grammar checker yet.
Few major webapplications offer contextual spell checking.
Thisis a shame because web applications have an op-portunity to offer authoring aids that are a genera-tion beyond the non-contextual spell-check mostapplications offer.Here we present After the Deadline, a productionsoftware service that checks spelling, style, andgrammar on WordPress.com1, one of the mostpopular blogging platforms.
Our system uses a1 An After the Deadline add-on for the Firefox web browser isavailable.
We also provide client libraries for embedding intoother applications.
See http://www.afterthedeadline.com.software service architecture.
In this paper we dis-cuss how this system works, the trade-offs of thesoftware service environment, and the benefits.We conclude with a discussion of our design prin-ciples: speed over accuracy, simplicity over com-plexity, and do what works.1.1 What is a Software Service?A software service (Turner et al, 2003) is an ap-plication that runs on a server.
Client applicationspost the expected inputs to the server and receivethe output as XML.Our software service checks spelling, style, andgrammar.
A client connects to our server, poststhe text, and receives the errors and suggestions asXML.
Figure 1 shows this process.
It is the cli-ent?s responsibility to display the errors and pre-sent the suggestions to the user.Figure 1.
After the Deadline Client/Server Interaction.241.2 ApplicationsOne could argue that web browsers should providespell and grammar check features for their users.Internet Explorer, the most used browser (Stat-Counter, 2010), offers no checking.
Firefox offersspell checking only.
Apple?s Safari web browserhas non-contextual spell and grammar checking.Application developers should not wait for thebrowsers to catch up.
Using a software servicearchitecture, applications can provide the samequality checking to their users regardless of theclient they connect with.
This is especially rele-vant as more users begin to use web applicationsfrom mobile and tablet devices.1.3 BenefitsA software service application has the advantagethat it can use the complete CPU and memory re-sources of the server.
Clients hoping to offer thesame level of proofreading, without a software ser-vice, will use more resources on the local system tostore and process the language models.Our system uses large memory-resident lan-guage models to offer contextually relevant spell-ing suggestions, detect real word errors, andautomatically find exceptions to our grammarrules.On disk our language model for English is165MB uncompressed, 32MB compressed.
Weuse hash tables to allow constant time access to thelanguage model data.
In memory our English lan-guage model expands to 1GB of RAM.
The mem-ory footprint of our language model is too large fora web browser or a mobile client.A software service also has maintenance advan-tages.
The grammar rules and spell checker dic-tionary are maintained in one place.
Updates tothese immediately benefit all clients that use theservice.In this environment, users lose the ability to up-date their spell checker dictionary directly.
Tocompensate, clients can offer users a way to al-ways ignore errors.
Our WordPress plugin allowsusers to ignore any error.
Ignored errors are nothighlighted in future checks.1.4 Operating RequirementsA software service authoring aid must be able torespond to multiple clients using the service at thesame time.
Our service regularly processes over100,000 requests a day on a single server.Our goal is to process one thousand words persecond under this load.Since our system works in the web environment,it must process both text and HTML.
We use aregular expression to remove HTML from text sentto the service.It?s important that our service report errors in away that the client can locate them.
The errorphrase alone is not enough because suggestionsmay differ based on the context of the error.We take a shortcut and provide clients with thetext used to match the error and the word that pre-cedes the error phrase.
For example, for indefinitearticle errors, the text used to match the error is themisused article and the word following it.
Theclient searches for this marker word followed bythe error text to find the error and present the cor-rect suggestions.
This scheme is not perfect, but itsimplifies our client and server implementations.2 Language ModelOur system derives its smarts from observed lan-guage use.
We construct our language model bycounting the number of times we see each se-quence of two words in a corpus of text.
Thesesequences are known as bigrams.
Our languagemodel is case sensitive.We trained our bigram language model usingtext from the Simple English edition of Wikipedia(Wikimedia, 2010), Project Gutenberg (Hart,2008), and several blogs.
We bootstrapped thisprocess by using Wikipedia and Project Gutenbergdata.
We then evaluated the contents of severalblogs looking for low occurrences of commonlymisspelled words and real word errors.
Blogs thathad a low occurrence of errors were then added toour corpus.
Our corpus has about 75 millionwords.We also store counts for sequences of threewords that end or begin with a potentially confusedword.
A potentially confused word is a word asso-ciated with a confusion set (see section 4.1).
Thereal word error detector feature relies on these con-fusion sets.
These counts are known as trigrams.We limit the number of trigrams stored to reducethe memory requirements.252.1 FunctionsThroughout this paper we will use the followingfunctions to refer to our language model.P(word): This function is the probability of a word.We divide the number of times the word occurs bythe total number of words observed in our corpusto calculate the probability of a word.P(wordn , wordn+1): This function is the probabilityof the sequence wordn wordn+1.
We divide thenumber of times the sequence occurs by the totalnumber of words observed in our corpus to calcu-late the probability of the sequence.Pn(wordn|wordn-1): This function is the probabilityof a word given the previous word.
We calculatethis with the count of the wordn-1 wordn sequencedivided by the count of the occurrences of wordn.Pp(wordn|wordn+1): This function is the probabilityof a word given the next word.
We use Bayes?Theorem to flip the conditional probability.
Wecalculate this result as: Pp(wordn|wordn+1) =Pn(wordn+1|wordn) * P(wordn) / P(wordn+1).Pn(wordn|wordn-1, wordn-2): This function is theprobability of a word given the previous twowords.
The function is calculated as the count ofthe wordn-2 wordn-1 wordn sequence divided by thecount of the wordn-2 wordn-1 sequence.Pn(wordn+1, wordn+2|wordn): is the probability of asequence of two words given the word that pre-cedes them.
This is calculated as the count ofwordn wordn+1 wordn+2 sequence divided by thecount of the occurrences of wordn.Pp(wordn|wordn+1, wordn+2): This function is theprobability of a word given the next two words.We calculate this result with Pn(wordn+1,wordn+2|wordn) * P(wordn) / P(wordn+1, wordn+2).3 Spell CheckingSpell checkers scan a document word by word andfollow a three-step process.
The first step is tocheck if the word is in the spell checker?s diction-ary.
If it is, then the word is spelled correctly.
Thesecond step is to generate a set of possible sugges-tions for the word.
The final step is to sort thesesuggestions with the goal of placing the intendedword in the first position.3.1 The Spell Checker DictionaryThe dictionary size is a matter of balance.
Toomany words and misspelled words will go unno-ticed.
Too few words and the user will see morefalse positive suggestions.We used public domain word-lists (Atkinson,2008) to create a master word list to generate ourspell checker dictionary.
We added to this list byanalyzing popular blogs for frequently occurringwords that were missing from our dictionary.
Thisanalysis lets us include new words in our masterword list of 760,211 words.Our spell checker dictionary is the intersection ofthis master word list and words found in our cor-pus.
We do this to prevent some misspelled wordsfrom making it into our spell checker dictionary.We only allow words that pass a minimal countthreshold into our dictionary.
We adjust thisthreshold to keep our dictionary size around125,000 words.Threshold Words Present Words Accuracy1 161,879 233 87.9%2 116,876 149 87.8%3 95,910 104 88.0%4 82,782 72 88.3%5 73,628 59 88.6%Table 1.
Dictionary Inclusion Threshold.Table 1 shows the effect of this threshold on thedictionary size, the number of present words fromWikipedia?s List of Common Misspellings(Wikipedia, 2009), and the accuracy of a non-contextual version of our spell checker.
We willrefer to the Wikipedia Common Misspellings listas WPCM through the rest of this paper.3.2 Generating SuggestionsTo generate suggestions our system first considersall words within an edit distance of two.
An edit isdefined as inserting a letter, deleting a letter, sub-stituting a letter, or transposing two letters (Dam-erau, 1964).26Consider the word post.
Here are several wordsthat are within one edit:cost substitute p, c pose substitute t, ehost substitute p, h posit insert imost substitute p, m posts insert spast substitute o, a pot delete epest substitute o, e pots transpose s, tpoet substitute s, e pout substitute s, uThe na?ve approach to finding words within oneedit involves making all possible edits to the mis-spelled word using our edit operations.
You mayremove any words that are not in the dictionary toarrive at the final result.
Apply the same algorithmto all word and non-word results within one edit ofthe misspelled word to find all words within twoedits.We store our dictionary as a Trie and generateedits by walking the Trie looking for words thatare reachable in a specified number of edits.
Whilethis is faster than the na?ve approach, generatingsuggestions is the slowest part of our spell checker.We cache these results in a global least-recently-used cache to mitigate this performance hit.We find that an edit distance of two is sufficientas 97.3% of the typos in the WPCM list are twoedits from the intended word.
When no sugges-tions are available within two edits, we considersuggestions three edits from the typo.
99% of thetypos from the WPCM list are within three edits.By doing this we avoid affecting the accuracy ofthe sorting step in a negative way and make it pos-sible for the system to suggest the correct word forsevere typos.3.3 Sorting SuggestionsThe sorting step relies on a score function that ac-cepts a typo and suggestion as parameters.
Theperfect score function calculates the probability ofa suggestion given the misspelled word (Brill andMoore, 2000).We approximate our scoring function using aneural network.
Our neural network is a multi-layer perceptron network, implemented as de-scribed in Chapter 4 of Programming CollectiveIntelligence (Segaran, 2007).
We created a train-ing data set for our spelling corrector by combiningmisspelled words from the WPCM list with ran-dom sentences from Wikipedia.Our neural network sees each typo (wordn) andsuggestion pair as several features with valuesranging from 0.0 to 1.0.
During training, the neu-ral network is presented with examples of sugges-tions and typos with the expected score.
Fromthese examples the neural network converges on anapproximation of our score function.We use the following features to train a neuralnetwork to calculate our suggestion scoring func-tion:editDistance(suggestion, wordn)firstLetterMatch(suggestion, wordn)Pn(suggestion|wordn-1)Pp(suggestion|wordn+1)P(suggestion)We calculate the edit distance using the Dam-erau?Levenshtein algorithm (Wagner and Fischer,1974).
This algorithm recognizes insertions, sub-stitutions, deletions, and transpositions as a singleedit.
We normalize this value for the neural net-work by assigning 1.0 to an edit distance of 1 and0.0 to any other edit distance.
We do this to pre-vent the occasional introduction of a correct wordwith an edit distance of three from skewing theneural network.The firstLetterMatch function returns 1.0 whenthe first letters of the suggestion and the typomatch.
This is based on the observation that mostwriters get the first letter correct when attemptingto a spell a word.
In the WPCM list, this is true for96.0% of the mistakes.
We later realized this cor-rector performed poorly for errors that swapped thefirst and second letter (e.g., oyu ?
you).
We thenupdated this feature to return 1.0 if the first andsecond letters were swapped.We also use the contextual fit of the suggestionfrom the language model.
Both the previous andnext word are used.
Consider the following exam-ple:The written wrd.Here wrd is a typo for word.
Now consider twosuggestions word and ward.
Both are an edit dis-tance of one from wrd.
Both words also have afirst letter match.
Pp(ward|written) is 0.00% whilePp(word|written) is 0.17%.
Context makes thedifference in this example.273.4 EvaluationTo evaluate our spelling corrector we created twotesting data sets.
We used the typo and word pairsfrom the WPCM list merged with random sen-tences from our Project Gutenberg corpus.
Wealso used the typo and word pairs from the ASpelldata set (Atkinson, 2002) merged with sentencesfrom the Project Gutenberg corpus.We measure our accuracy with the method de-scribed in Deorowicz and Ciura (2005).
For com-parison we present their numbers for ASpell andseveral versions of Microsoft Word along withours in Tables 2 and 3.
We also show the numberof misspelled words present in each system?s spellchecker dictionary.Present Words AccuracyASpell (normal) 14 56.9%MS Word 97 18 59.0%MS Word 2000 20 62.6%MS Word 2003 20 62.8%After the Deadline 53 66.1%Table 2.
Corrector Accuracy: ASpell Data.Present Words AccuracyASpell (normal) 44 84.7%MS Word 97 31 89.0%MS Word 2000 42 92.5%MS Word 2003 41 92.6%After the Deadline 143 92.7%Table 3.
Corrector Accuracy: WPCM Data.The accuracy number measures both the sugges-tion generation and sorting steps.
As with the ref-erenced experiment, we excluded misspelledentries that existed in the spell checker dictionary.Note that the present words number from Table 1differs from Table 3 as these experiments werecarried out at different times in the development ofour technology.4 Real Word ErrorsSpell checkers are unable to detect an error when atypo results in a word contained in the dictionary.These are called real word errors.
A good over-view of real word error detection and correction isPedler (2007).4.1 Confusion SetsOur real word error detector checks 1,603 words,grouped into 741 confusion sets.
A confusion setis two or more words that are often confused foreach other (e.g., right and write).
Our confusionsets were built by hand using a list of Englishhomophones as a starting point.4.2 Real Word Error CorrectionThe real word error detector scans the documentfinding words associated with a confusion set.
Foreach of these words the real word error detectoruses a score function to sort the confusion set.
Thescore function approximates the likelihood of aword given the context.
Any words that scorehigher than the current word are presented to theuser as suggestions.When determining an error, we bias heavily forprecision at the expense of recall.
We want usersto trust the errors when they?re presented.We implement the score function as a neuralnetwork.
We inserted errors into sentences fromour Wikipedia corpus to create a training corpus.The neural network calculates the score functionusing:Pn(suggestion|wordn-1)Pp(suggestion|wordn+1)Pn(suggestion|wordn-1, wordn-2)Pp(suggestion|wordn+1, wordn+2)P(suggestion)With the neural network our software is able toconsolidate the information from these statisticalfeatures.
The neural network also gives us a back-off method, as the neural network will deal withsituations that have trigrams and those that don?t.While using our system, we?ve found somewords experience a higher false positive rate thanothers (e.g., to/too).
Our approach is to removethese difficult-to-correct words from our confusionsets and use hand-made grammar rules to detectwhen they are misused.4.3 EvaluationWe use the dyslexic spelling error corpus fromPedler?s PhD thesis (2007) to evaluate the realword error correction ability of our system.
97.8%28of the 835 errors in this corpus are real-word er-rors.Our method is to provide all sentences to eachevaluated system, accept the first suggestion, andcompare the corrected text to the expected an-swers.
For comparison we present numbers forMicrosoft Word 2007 Windows, Microsoft Word2008 on MacOS X, and the MacOS X 10.6 built-ingrammar and spell checker.
Table 4 shows theresults.Microsoft Word 2008 and the MacOS X built-inproofreading tools do not have the benefit of a sta-tistical technique for real-word error detection.Microsoft Word 2007 has a contextual spell-checking feature.Precision RecallMS Word 07 - Win 90.0% 40.8%After the Deadline 89.4% 27.1%MS Word 08 - Mac 79.7% 17.7%MacOS X built-in  88.5% 9.3%Table 4.
Real Word Error Correction Performance.Most grammar checkers (including After theDeadline) use grammar rules to detect commonreal-word errors (e.g., a/an).
Table 4 shows thesystems with statistical real-word error correctorsare advantageous to users.
These systems correctfar more errors than those that only rely on a rule-based grammar checker.5 Grammar and Style CheckingThe grammar and style checker works withphrases.
Our rule-based grammar checker findsverb and determiner agreement errors, locatessome missing prepositions, and flags plural phrasesthat should indicate possession.
The grammarchecker also adds to the real-word error detection,using a rule-based approach to detect misusedwords.
The style checker points out complex ex-pressions, redundant phrases, clich?s, double nega-tives, and it flags passive voice and hidden verbs.Our system prepares text for grammar checkingby segmenting the raw text into sentences andwords.
Each word is tagged with its relevant part-of-speech (adjective, noun, verb, etc.).
The systemthen applies several grammar and style rules to thismarked up text looking for matches.
Grammarrules consist of regular expressions that match onparts-of-speech, word patterns, and sentence beginand end markers.Our grammar checker does not do a deep parseof the sentence.
This prevents us from writingrules that reference the sentence subject, verb, andobject directly.
In practice this means we?re un-able to rewrite passive voice for users and creategeneral rules to catch many subject-verb agreementerrors.Functionally, our grammar and style checker issimilar to Language Tool (Naber, 2003) with theexception that it uses the language model to filtersuggestions that don?t fit the context of the textthey replace, similar to work from Microsoft Re-search (Gamon, et al2008).5.1 Text SegmentationOur text segmentation function uses a rule-basedapproach similar to Yona (2002) to split raw textinto paragraphs, sentences, and words.
The seg-mentation is good enough for most purposes.Because our sentence segmentation is wrong attimes, we do not notify a user when they fail tocapitalize the first word in a sentence.5.2 Part-of-Speech TaggerA tagger labels each word with its relevant part-of-speech.
These labels are called tags.
A tag is ahint about the grammatical category of the word.Such tagging allows grammar and style rules toreference all nouns or all verbs rather than havingto account for individual words.
Our system usesthe Penn Tagset (Marcus et al 1993).The/DT little/JJ dog/NNlaughed/VBDHere we have tagged the sentence The little doglaughed.
The is labeled as a determiner, little is anadjective, dog is a noun, and laughed is a pasttense verb.We can reference little, large, and mean laugh-ing dogs with the pattern The .
*/JJ dog laughed.Our grammar checker separates phrases and tagswith a forward slash character.
This is a commonconvention.The part-of-speech tagger uses a mixed statisti-cal and rule-based approach.
If a word is knownand has tags associated with it, the tagger tries to29find the tag that maximizes the following probabil-ity:P(tagn|wordn) * P(tagn|tagn-1, tagn-2)For words that are not known, an alternatemodel containing tag probabilities based on wordendings is consulted.
This alternate model uses thelast three letters of the word.
Again the goal is tomaximize this probability.We apply rules from Brill?s tagger (Brill, 1995)to fix some cases of known incorrect tagging.
Ta-ble 5 compares our tagger accuracy for known andunknown words to a probabilistic tagger thatmaximizes P(tagn|wordn) only.Tagger Known  UnknownProbability Tagger 91.9% 72.9%Trigram Tagger 94.0% 76.7%Table 5.
POS Tagger Accuracy.To train the tagger we created training and test-ing data sets by running the Stanford POS tagger(Toutanova and Manning, 2000) against theWikipedia and Project Gutenberg corpus data.5.3 Rule EngineIt helps to think of a grammar checker as a lan-guage for describing phrases.
Phrases that match agrammar rule return suggestions that are trans-forms of the matched phrase.Some rules are simple string substitutions (e.g.,utilized ?
used).
Others are more complex.
Con-sider the following phrase:I wonder if this is your com-panies way of providing sup-port?This phrase contains an error.
The word compa-nies should be possessive not plural.
To create arule to find this error, we first look at how our sys-tem sees it:I/PRP wonder/VBP if/INthis/DT is/VBZ your/PRP$ com-panies/NNS way/NN of/IN pro-viding/VBG support/NNA rule to capture this error is:your .
*/NNS .
*/NNThis rule looks for a phrase that begins with theword your, followed by a plural noun, followed byanother noun.
When this rule matches a phrase,suggestions are generated using a template speci-fied with the rule.
The suggestion for this rule is:your \1:possessive \2Suggestions may reference matched words with\n, where n is the nth word starting from zero.This suggestion references the second and thirdwords.
It also specifies that the second wordshould be transformed to possessive form.
Oursystem converts the plural word to a possessiveform using the \1:possessive transform.Phrase Scoreyour companies way 0.000004%your company?s way 0.000030%Table 6.
Grammar Checker Statistical Filtering.Before presenting suggestions to the user, oursystem queries the language model to decide whichsuggestions fit in the context of the original text.Rules may specify which context fit functionthey want to use.
The default context fit functionis: Pn(wordn|wordn-1) + Pp(wordn|wordn+1) >(0.5 x [Pn(wordn|wordn-1) + Pp(wordn|wordn+1)]) +0.00001.This simple context fit function gets rid of manysuggestions.
Table 6 shows the scores from ourexample.
Here we see that the suggestion scoresnearly ten times higher than the original text.This statistical filtering is helpful as it relievesthe rule developer from the burden of finding ex-ceptions to the rule.
Consider the rules to identifythe wrong indefinite article:a [aeiouyhAEIOUYH18]\w+an [^aeiAEIMNRSX8]\w+One uses a when the next word has a consonantsound and an when it has a vowel sound.
Writingrules to capture this is wrought with exceptions.
Arule can?t capture a sound without hard codingeach exception.
For this situation we use a context30fit function that calculates the statistical fit of theindefinite article with the following word.
Thissaves us from having to manually find exceptions.Figure 2.
Rule Tree Example.Each rule describes a phrase one word and tagpattern at a time.
For performance reasons, thefirst token must be a word or part-of-speech tag.No pattern matching is allowed in the first token.We group rules with a common first word or taginto an n-ary rule tree.
Rules with common patternelements are grouped together until the word/tagpatterns described by the rule diverges from exist-ing patterns.
Figure 2 illustrates this.When evaluating text, our system checks if thereis a rule tree associated with the current word ortag.
If there is, our system walks the tree lookingfor the deepest match.
Each shaded node in Figure2 represents a potential match.
Associated witheach node are suggestions and hints for the statisti-cal checker.We measure the number of rules in our systemby counting the number of nodes that result in agrammar rule match.
Figure 2 represents six dif-ferent grammar rules.
Our system has 33,732 rulesto check for grammar and style errors.The capabilities of the grammar checker are lim-ited by our imagination and ability to create newrules.
We do not present the precision and recallof the grammar checker, as the coverage of ourhand-made rules is not the subject of this paper.6 ConclusionsOur approach to developing a software serviceproofreader is summarized with the followingprinciples:?
Speed over accuracy?
Simplicity over complexity?
Do what worksIn natural language processing there are manyopportunities to choose speed over accuracy.
Forexample, when tagging a sentence one can use aHidden Markov Model tagger or a simple trigramtagger.
In these instances we made the choice totrade accuracy for speed.When implementing the smarts of our system,we?ve opted to use simpler algorithms and focuson acquiring more data and increasing the qualityof data our system learns from.
As others havepointed out (Banko and Brill, 2001), with enoughdata the complex algorithms with their tricks ceaseto have an advantage over the simpler methods.Our real-word error detector is an example ofsimplicity over complexity.
With our simple tri-gram language model, we were able to correctnearly a quarter of the errors in the dyslexic writercorpus.
We could improve the performance of ourreal-word error corrector simply by adding moreconfusion sets.We define ?do what works?
as favoring mixedstrategies for finding and correcting errors.
Weuse both statistical and rule-based methods to de-tect real word errors and correct grammar mis-takes.Here we?ve shown a production software servicesystem used for proofreading documents.
Whiledesigning this system for production we?ve notedseveral areas of improvement.
We?ve explainedhow we implemented a comprehensive proofread-ing solution using a simple language model and afew neural networks.
We?ve also shown that thereare advantages to a software service from the useof large language models.After the Deadline is available under the GNUGeneral Public License.
The code and models areavailable at http://open.afterthedeadline.com.AcknowledgementsThe author would like to acknowledge the reviewcommittee for their questions and suggestions.The author would also like to acknowledge Niko-lay Bachiyski, Michael Yoshitaka Erlewine, andDr.
Charles Wallace who offered comments ondrafts of this paper.31ReferencesKevin  Atkinson.
2008, Kevin?s Wordlist Page.http://wordlist.sourceforge.net/, last accessed: 4 April2010.Kevin Atkinson, Spellchecker Test Kernel Results.2002.
http://aspell.net/test/orig/, last accessed: 28February 2010.Michele Banko and Eric Brill.
2001.
Scaling to veryvery large corpora for natural language disambigua-tion.
Proceedings of the 39th Annual Meeting of theAssociation for Computational Linguistics and the10th Conference of the European Chapter of the As-sociation for Computational Linguistics, Toulouse.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: a casestudy in part of speech tagging.
Computational Lin-guistics, 21:543?565.Eric Brill and Robert C. Moore.
2000.
An improvederror model for noisy channel spelling correction.Proceedings of the 38th Annual Meeting of the Asso-ciation for Computational Linguistics, Hong Kong,pp.
286?293.Fred J. Damerau.
1964.
A technique for computer detec-tion and correction of spelling errors.
Communica-tions of the ACM, 7(3): 659-664.Sebastian Deorowicz and Marcin G. Ciura.
2005.
Cor-recting spelling errors by modelling their causes.
In-ternational Journal of Applied Mathematics andComputer Science, 15(2):275?285.Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-der Klementiev, William Dolan, Dmitriy Belenko,and Lucy Vanderwende.
2008.
Using ContextualSpeller Techniques and Language Modeling for ESLError Correction.
Proceedings of IJCNLP, Hydera-bad, India, Asia Federation of Natural LanguageProcessing.Michael Hart.
2008.
Project Gutenberg.http://www.gutenberg.org/, last accessed: 28 Febru-ary 2010.Abby Levenberg.
2007.
Bloom filter and lossy diction-ary based language models.
Master of Science Dis-sertation, School of Informatics, University ofEdinburgh.Mitchell Marcus, Beatrice Santorini, and Maryann Mar-cinkiewicz.
1993.
Building a large annotated corpusof English: the Penn Treebank.
Computational Lin-guistics, 19(2).Daniel Naber.
2003.
A Rule-Based Style and GrammarChecker.
Diplomarbeit Technis Fakult?t, Universit?tBielefeld, Germany.Jennifer Pedler.
2007.
Computer Correction of Real-word Spelling Errors in Dyslexic Text.
PhD thesis,Birkbeck, London University.Segaran, T. 2007 Programming Collective Intelligence.First.
O'Reilly.
pp.
74-85StatCounter, 2010.
Top 5 Browsers from Feb 09 to Mar10.
http://gs.statcounter.com/, last accessed: 28 Feb-ruary 2010.Kristina Toutanova and Christopher D. Manning.
2000.Enriching the Knowledge Sources Used in a Maxi-mum Entropy Part-of-Speech Tagger.
Proceedings ofthe Joint SIGDAT Conference on Empirical Methodsin Natural Language Processing and Very LargeCorpora (EMNLP/VLC-2000), pp.
63-70.Mark Turner, David Budgen, and Pearl Brereton.
2003.Turning software into a service.
Computer,36(10):38?44.Robert A. Wagner and Michael J. Fischer.
1974.
Thestring-to-string correction problem.
Journal of ACM,21(1):168?173.Wikipedia, 2009.
List of Common Misspellings.http://en.wikipedia.org/wiki/Wikipedia:Lists_of_common_misspellings, last accessed: 28 February 2010.Wikimedia Inc. 2010.
Wikimedia Downloads.http://download.wikipedia.org/, last accessed: 28February 2010.Shloma Yona, 2002.
Lingua::EN::Sentence Module,CPAN.
http://search.cpan.org/~shlomoy/Lingua-EN-Sentence-0.25/lib/Lingua/EN/Sentence.pm, last ac-cessed: 28 February 2010.32
