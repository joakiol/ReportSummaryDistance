EXPERIMENTS IN SYNTACTIC AND SEMANTIC CLASSIFICATIONAND DISAMBIGUATION USING BOOTSTRAPPING*Robert P. Futrelle and Susan GauchBiological Knowledge LaboratoryCollege of Computer ScienceNortheastern UniversityBoston, MA 02115{futrelle, sgauch}@ccs.neu.eduABSTRACTBootstrap methods (unsupervisedclassification) that generate word classeswithout requiring pretagging have hadnotable success in the last few years.
Themethods described here strengthen theseapproaches and produce excellent word classesfrom a 200,000 word corpus.
The method usesmutual information measures plus positionalinformation from the words in the immediatecontext of a target word to computesimilarities.
Using the similarities, classesare built using hierarchical agglomerativeclustering.
At the leaves of the classificationtree, words are grouped by syntactic andsemantic similarity.
Further up the tree, theclasses are primarily syntactic.
Once theinitial classes are found, they can be used toclassify ambiguous words, i.e., part-of-speechtagging.
This is done by expanding eachcontext word of a target instance into a tightlydefined class of similar words, a simsct.
Theuse of simsets is shown to increase the taggingaccuracy from 83% to 92% for the forms"cloned" and "deduced".INTRODUCTIONThe identification of the syntactic lassand the discovery of semantic information forwords not contained in any on-line dictionaryor thesaurus i an important and challenging* This material is based upon work supported bythe National Science Foundation under Grant No.DIR-8814522.problem.
Excellent methods have beendeveloped for part-of-speech (POS) taggingusing stochastic models trained on partiallytagged corpora (Church, 1988; Cutting,Kupiec, Pedersen & Sibun, 1992).
Semanticissues have been addressed, particularly forsense disambiguation, by using large contexts,e.g., 50 nearby words (Gale, Church &Yarowsky, 1992) or by reference to on-linedictionaries (Krovetz, 1991; Lesk, 1986; Liddy& Paik, 1992; Zernik, 1991).
More recently,methods to work with entirely untaggedcorpora have been developed which show greatpromise (Brill & Marcus, 1992; Finch &Chater, 1992; Myaeng & Li, 1992; Schutze,1992).
They are particularly useful for textwith specialized vocabularies and word use.These methods of unsupervised classificationtypically have clustering algorithms at theirheart (Jain & Dubes, 1988).
They usesimilarity of contexts (the distributionprinciple) as a measure of distance in thespace of words and then cluster similar wordsinto classes.
This paper demonstrates aparticular approach to these classificationtechniques.In our approach, we take into account boththe relative positions of the nearby contextwords as well as the mutual information(Church & Hanks, 1990) associated with theoccurrence of a particular context word.
Thesimilarities computed from these measures ofthe context contain information about bothsyntactic and semantic relations.
Forexample, high similarity values are obtainedfor the two semantically similar nouns,117"diameter" and "length", as well as for the twoadjectives "nonmotile" and "nonchemotactic".We demonstrate the technique on threeproblems, all using a 200,000 word corpuscomposed of 1700 abstracts from a specializedfield of biology: #1: Generating the fullclassification tree for the 1,000 most frequentwords (covering 80% of all word occurrences).#2: The classification of 138 occurrences ofthe -ed forms, "cloned" and "deduced" into foursyntactic categories, including improvementsby using expanded context informationderived in #1.
#3: The classification of 100words that only occur once in the entire corpus(hapax legomena), again using expandedcontexts.The results described below were obtainedusing no pretagging or on-line dictionary, butthe results compare favorably with methodsthat do.
The results are discussed in terms ofthe semantic fields they delineate, theaccuracy of the classifications and the natureof the errors that occur.
The results make itclear that this new technology is verypromising and should be pursued vigorously.The power of the approach appears to resultfrom using a focused corpus, using detailedpositional information, using mutualinformation measures and using a clusteringmethod that updates the detailed contextinformation when each new cluster is formed.Our approach was inspired by the fascinatingresults achieved by Finch and Chater atEdinburgh and the methods they used (Finch& Chater, 1992).THE CORPUS - -  TECHNICAL ,FOCUSED AND SMALLIn the Biological Knowledge Laboratorywe are pursuing a number of projects toanalyze, store and retrieve biological researchpapers, including working with full text andgraphics (Futrelle, Kakadiaris, Alexander,Carriero, Nikolakis & Futrelle, 1992; Gauch &Futrelle, 1993).
The work is focused on thebiological field of bacterial chemotaxis.
Abiologist has selected approximately 1,700documents representing all the work done inthis field since its inception in 1965.
Ourstudy uses the titles for all these documentsplus all the abstracts available for them.
Theresulting corpus contains 227,408 words with13,309 distinct word forms, including 5,833words of frequency 1.
There are 1,686 titlesplus 8,530 sentences in the corpus.
Thesentence identification algorithm requires twofactors -- contiguous punctuation C.", "!
", or"?")
and capitalization of the following token.To eliminate abbreviations, the token prior tothe punctuation must not be a single capitalletter and the capitalized token after thepunctuation may not itself be followed by acontiguous ".
".is,An example of a sentence from the corpus"$pre2$ $prel$ one of the openreading frames was translated into aprotein with $pct$ amino acid identityto S. typhimurium Flil and $pct$identity to the beta subunit of E. coliATP  synthase $posl$ $pos2$"The positional items $pre... and $pos...have been added to furnish explicit context forsentence initial and sentence finalconstituents.
Numbers have been convertedto three forms corresponding to integers, realsand percentages C$pct$ in the exampleabove).
The machine-readable version of thecorpus uses double quoted items to easeprocessing by Lisp, our language of choice.The terminology we will use for describingwords is as follows:?
Target  word: A word to be classified.Context  words: Appearing withinsome distance of a target word,"The big ~ cat 9.a the mat...".?
Word class: Any defined set of wordforms or labeled instances.Simset:  A word class in which eachitem, an expansion word, has asimilarity greater than some chosencutoffto a single base word.?
Labeled instances: Forms such as"cloned48" or "cloned73VBN", that118would replace an occurrence of"cloned".DESCRIB ING ANDQUANTIFY ING WORDCONTI~TSIn these experiments, the context of atarget word is described by the preceding twocontext words and the following two contextwords, Figure 1.
Each position is representedby a 150 element vector corresponding to theoccurrence of the 150 highest frequency wordsin the corpus, giving a 600-dimensional vectordescribing the four-word context.
Initially, thecounts from all instances of a target word formw are summed so that the entry in thecorresponding context word position in thevector is the sum of the occurrences of thatcontext word in that  position for thecorresponding target word form; it is the jointfrequency of the context word.
For example, ifthe word the immediately precedes 10occurrences of the word gene in the corpusthen the element corresponding to the in the-1C context vector of gene is set to 10.Subsequently, a 600-dimensional vector ofmutual information values, MI, is computedfrom the frequencies as follows,log 2 NZ~ + Ml(cw)= \ [ .
\ ] .
j1 \ ]This expresses the mutual informationvalue for the context word c appearing withthe target word w. The mutual information islarge whenever a context word appears at amuch higher frequency, fcw, in theneighborhood of a target word than would bepredicted from the overall frequencies in thecorpus, fc and fw.
The formula adds 1 to thefrequency ratio, so that a 0 (zero) occurrencecorresponds to 0 mutual information.
Apossibly better strategy (Church, Gale, Hanks& Hindle, 1991) is capable of generatingnegative mutual information for the non-occurrence or low-frequency occurrence of avery high-frequency word and has the form,?
fN(~ +I)'~ Ml(cw) = log 2 ~ .
/\[ I#.
JIn any case, some smoothing is necessaryto prevent the mutual  information fromdiverging when fcw= O.S IMILARITY ,  CLUSTERING ANDCLASS IF ICAT ION IN  WORDSPACEWhen the mutual information vectors arecomputed for a number of words, they can becompared to see which words have similarcontexts.
The comparison we chose is theinner product, or cosine measure, which canvary between -1.0 and +1.0 (Myaeng & Li,1992).
Once this similarity is computed for allword pairs in a set, various techniques can beused to identify classes of similar words.
Themethod we chose is hierarchical agglomerativeclustering (Jain & Dubes, 1988).
The twowords with the highest similarity are firstjo ined  into a two-word  c lus ter .Word to be classified with context: -2C -1C W +1C +2CFigure 1.
The 600-dimensional context vector around a target word W. Each subvecterdescribes the frequency and mutual information of the occurrences of the 150 highestfrequency words, HFC, in the corpus.119A mutual information vector for thecluster is computed and the cluster andremaining words are again compared,choosing the most similar to join, and so on.
(To compute the new mutual informationvector, the context frequencies in the vectorsfor the two words or clusters joined at eachstep are summed, element-wise.)
In this way,a binary tree is constructed with words at theleaves leading to a single root covering allwords.
Each cluster, anode in the binary tree,is described by an integer denoting its positionin the sequence of cluster formation, the totalnumber of words, the similarity of the twochildren that make it up, and its memberwords.
Here, for example, are the first 15clusters from the analysis described inExperiment #I in the next section,(0 2 0.73926157 is was)(1 2 0.6988309 were are)(Z 4 0.708@31 ( is  was) (were are))(3 2 0.65726656 found shown)(4 Z 0.6216794 the a)(5 Z 0.5913143 s mM)(6 Z 0.59088105 col i  typhimurium)(7 2 0.586728 galactose ribose)(8 2 0.58630705 method procedure)(9 2 0.58404166 K-12 K12)(10 2 0.5833811 required necessary)(11 3 0.5793458 rain (s raM))(12 2 0.5750035 isolated constructed)(13 3 0.56909233 (found shown) used)(14 2 0.$6750214 cel ls strains)(I5 3 0.5652546 mutants (ce l ls  strains))In this sample it is clear that clusters aresometimes formed by the pairing of twoindividual words, sometimes by pairing oneword and a previous cluster, and sometimesby combining two already formed clusters.In normal tagging, a word is viewed as amember of one of a small number of classes.In the classification approach we are using,there can be thousands of classes, from pairsof words up to the root node which contains allwords in a single class.
Thus, every classgenerated is viewed extensionally, it is astructured collection of occurrences in thecorpus, with their attendant frequencies andcontexts.
The classes so formed will reflectthe particular word use in the corpus they arederived from.EXPERIMENT #1:CLASSIFICATION OF THE 1,000HIGHEST FREQUENCY WORDSThe first experiment classified the 1,000highest frequency words in the corpus,producing 999 clusters (0-998) during theprocess.
$pre... and Spas... words wereincluded in the context set, but not in thetarget set.
Near the leaves, words clusteredby syntax (part of speech) and by semantics.Later, larger clusters tended to contain wordsof the same syntactic class, but with lesssemantic homogeneity.
In each examplebelow, the words listed are the entire contentsof the cluster mentioned.
The most strikingproperty of the clusters produced was theclassification of words into coherent semanticfields.
Grefenstette has pointed out(Grefenstette, 1992) that the Deese antonyms,such as "large" and "small" or "hot" and "cold"show up commonly in these analyses.
Ourmethods discovered entire graded fields,rather than just pairs of opposites.
Thefollowing example shows a cluster ofseventeen adjectives describing comparativequantity terms, cluster 756, similarity 0.28,decreased, effective, few, greater, high,higher, increased, large, less, low,lower, more, much, no, normal,reduced, shortNote that pairs such as "high" and"higher" and "low" and "lower" appear.
"No",meaning "none" in this collection, is located atone extreme.
The somewhat marginal item,"effective", entered the cluster late, at cluster704.
It appears in collocations, uch as "aseffective as" and "effective than", in which theother terms also appear.
Comparing thecluster to Roger's (Berrey, 1962) we find thatall the items are in the Roget categoryComparative Quantity except for "effective"and "no".
The cluster item, "large" is not inthis Roget category but the category doesinclude "big", "huge" and "vast", so theomission is clearly an error in Roget's.
With120this correction, 88% (15/17) of the items are inthe single Roget category.The classification of technical terms fromgenetics and biochemistry is of particularinterest, because many of these terms do notappear in available dictionaries or thesauri.Cluster 374, similarity 0.37, contains these 18items,che, cheA, cheB, cheR, cheY, cheZ, double,fla, flaA, taB,  flaE, H2, hag, mot,motB, tar, trg, tsrAll of these are abbreviations for specificbacterial mutations, except for "double".
Itsappearance drives home the point that theclassification depends entirely on usage.
20 ofthe 30 occurrences of "double" precede thewords "mutant" or "mutants", as do most ofthe othermutation terms in this cluster.Cluster 240, similarity 0.4 contains thesetermS,microscopy, electrophoresis,chromatographyEach of these is a noun describing acommon technique used in experiments in thisdomain.The standard Linnean nomenclature ofGenus followed by species, such as Escherichiacoli, is reflected by cluster 414, which contains22 species names, and cluster 510, whichcontains 9 genus names.In scientific research, the determination ofcausal factors and the discovery of essentialelements is a major goal.
Here are sixconcepts in this semantic field comprisingcluster 183, similarity 0.43,required, necessary, involved, responsible,essential, importantThese te rms are used a lmostinterchangeably in our corpus, but they don'tfare as well in Roget's because ofanthropocentric attachments o concepts uchas fame, duty and legal liability.Discussion of Experiment #1Given the limited context and modestsized corpus, the classification algorithm isbound to make mistakes, though a study ofthe text concordance will always tell us whythe algorithm failed in any specific case.
Forexample, as the similarity drops to 0.24 atcluster 824 we see the adverb triple "greatly","rapidly" and "almost".
This is stillacceptable, but by cluster 836 (similarity 0.24)we see the triple, "them", "ring", "rings".
Atthe end there is only a single cluster, 998,which must include all words.
It comestogether stubbornly with a negative similarityof-0.51.
One problem encountered in thiswork was that the later, larger clusters haveless coherence than we would hope for,identifying an important research issue.Experiment #1 took 20 hours to run on aSymbolics XL1200.A fundamental problem is to devisedecision procedures that will tell us whichclasses are semantically or syntacticallyhomogeneous; procedures that tell us where tocut the tree.
The examples shown earlierbroke down soon after, when words or clusterswhich in our judgment were weakly relatedbegan to be added.
We are exploring thenumerous methods to refine clusters onceformed as well as methods to validate clustersfor homogeneity (Jain & Dubes, 1988).
Thereare also resampling methods to validateclusters formed by top-down partit ioningmethods (Jain & Moreau, 1987).
All of thesemethods are computationally demanding butthey can result in criteria for when to stopclustering.
On the other hand, we mustn'tassume that word relations are so simple thatwe can legitimately insist on finding neatlyseparated clusters.
Word relations maysimply be too complex and graded for this everto occur.The semantic fields we discovered werenot confined to synonyms.
To understand whythis is the case, consider the sentences, "Thetemperature is higher today."
and, "Thetemperature is lower today."
There is no wayto tell from the syntax which word to expect.The choice is dependent on the situation in theworld; it represents data from the world.
The121utterances are informative for just thatreason.
Taking this reasoning a step further,information theory would suggest that for twocontrasting words to be maximallyinformative, they should appear about equallyoften in discourse.
This is born out in ourcorpus (fhigher=58, i~ower=46) and for theBrown corpus (fhigher=147, fiower=110).
Thesame relations are found for many othercontrasting pairs, with some bias towards"positive" terms.
The most extreme "positive"bias in our corpus is fposs ib le=88,fimpossible=0; "never say never" seems to bethe catchphrase here - -  highly appropriate forthe field of biology.Some of the chemical term clusters thatwere generated are interesting because theycontain class terms such as "sugar" and "ion"along with specific members of the classes(hyponyms), such as "maltose" and "Na +''.Comparing these in our KWIC  concordancesuggests that there may be methodicaltechniques for identifying some of thesegeneralization hierarchies using machinelearning (supervised classification) (Futrelle &Gauch, 1993).
For another discussion ofattempts to generate generalizationhierarchies, see (Myaeng & Li, 1992).As a corpus grows and new words appear,one way to classify them is to find theirsimilarity to the N words for which contextvectors have already been computed.
Thisrequires N comparisons.
A more efficientmethod which would probably give the sameresult would be to successively compare theword to clusters in the tree, starting at theroot.
At each node, the child which is mostsimilar to the unclassified word is followed.This is a logarithmic search technique forfinding the best matching class which takesonly O(log2N) steps.
In such an approach,the hierarchical cluster is being used as adecision tree, which have been much studiedin the machine learning literature (Quinlan,1993).
This is an alternate view of theclassification approach as the unsupervisedlearning of a decision tree.EXPERIMENT #2:DISAMBIGUATION OF -EDFORMSThe following experiment is interestingbecause it shows a specific use for thesimilarity computations.
They are used hereto increase the accuracy of termdisambiguation which means selecting thebest tag or class for a potentially ambiguousword.
Again, this is a bootstrap method; noprior tagging is needed to construct heclasses.
But if we do identify the tags for afew items by hand or by using a hand-taggedreference corpus, the tags for all the otheritems in a cluster can be assumed equal to theknown items.The passive voice is used almostexclusively in the corpus, with some use of theeditorial "We".
This results in a profusion ofparticiples such as "detected", "sequenced" and"identified".
But such -ed forms can also besimple past tense forms or adjectives.
Inaddition, we identified their use in apostmodifying participle clause such as,"... the value ~ from thismeasurement."
Each one of the 88 instancesof "cloned" and the 50 instances of "deduced"was hand tagged and given a unique ID.
Thenclustering was applied to the resultingcollection, giving the result shown inFigure 2A.
Experiments #2 and #3 took about15 minutes each to run.The resultant clusters are somewhatcomplex.
There are four tags and we haveshown the top four clusters, but two of theclusters contain adjectives exclusively.
Thepast participle and postmodifier occur togetherin the same cluster.
(We studied the childrenof cluster 4, hoping to find better separation,but they are no better. )
The scoring metricwe chose was to associate each cluster withthe items that were in the majority in thenode and score all other items as errors.
Thisis a good approximation to a situation inwhich a "gold standard" is available to classifythe clusters by independent means, such ascomparing the clusters to items from apretagged re ference corpus.122II , I  2 ,,VBO\] ?
JI i \]JJ = AdjectiveVBD = Verb, past tenseVBN : Verb, past participleVBNP = Participle in postmodifying clause46 VBN13 VBNP1 VBD1 JJFigure 2A.
Clustering of 88 occurrence of"cloned" and 50 occurrences of"deduced" into foursyntactic categories.
The abbreviations, such as "JJ", are based on (Francis & Kucera, 1982).There is a strong admixture of adjectives in cluster 2 and all the postmodifiers areconfounded with the past participles in cluster 4.
The total number of errors (minorityclasses in a cluster) is 23 for a success rate of(138-23)/138 = 83%.All minority members  of a cluster arecounted as errors.
This leads to the 83% errorrate quoted in the figure caption.The results shown in Figure 2A can beimproved as follows.
Because we are dealingwith single occurrences, only one element, orpossibly zero, in each of the four context wordvectors is filled, with frequency 1.
The other149 elements have frequency (and mutualinformation) 0.0.
These sparse vectors willtherefore have little or no overlap with vectorsfrom other occurrences.
In order to try toimprove the classification, we expanded thecontext values in an effort to produce moreoverlap, using the following strategy: Weproceed as if the corpus is far larger so that inaddition to the actual context words alreadyseen, there are many occurrences of highlysimilar words in the same positions.
For eachnon-zero context in each set of 150, we expandit to an ordered class of similar words in the150, picking words above a fixed similaritythreshold (0.3 for the experiments reportedhere).
Such a class is called a simset, made upof a base word and a sequence  of expansionwords.As an example of the expansion of contextwords via simsets, suppose that theoccurrence of the frequency 1 word"cheA-cheB" is immediately preceded by "few"and the occurrence of the frequency 1 word"CheA/CheB" is immediately preceded by"less".
The -I C context vectors for each willhave l's in different positions so there will beno overlap between them.
If we expanded"few" into a large enough simset, the set wouldeventually contain, "less", and vice-versa.Barring that, each simset might contain adistinct common word such as "decreased".
Ineither case, there would now be some overlapin the context vectors so that the similar useof "cheA-cheB" and "CheA/CheB" could bedetected.The apparent frequency of each expansionword is based on its corpus frequency relativeto the corpus frequency of the word beingexpanded.
To expand a single context wordinstance ci appearing with frequency fik in thecontext of 1 or more occurrences of centerword wk, choose all cj such that cj e {set ofhigh-frequency context words} and the123similarity S(ci,cj) _> St, a threshold value.
Setthe apparent frequency of each expansionword cj to fjk = S(ci,cj)xfik x fj / fi , where fiand fj are the corpus frequencies of ci and cj.Normalize the total frequency of the contextword plus the apparent frequencies of theexpansion words to fik.
For the example beingdiscussed here, fik = 1, St=0.3 and the averagenumber of expansion words was 6.Recomputing the classification of the -edforms with the expanded context words resultsin the improved classification shown in Figure2B.
The number of classification errors ishalved, yielding a success rate of 92%.
This iscomparable in performance to many stochastictagging algorithms.Discussion of Experiment #2This analysis is very similar to part-of-speech tagging.
The simsets of only 6 itemsare far smaller than the part-of-speechcategories conventionally used.
But since weuse high frequency words, they represent asubstantial portion of the instances.
Also,they have higher specificity than, say, Verb.Many taggers work sequentially and dependon the left context.
But some words are bestclassified by their right context.
We supplyboth.
Clearly this small experiment did notreach the accuracy of the very best taggers,but it performed well.This experiment has major ramificationsfor the future.
The initial classifications foundmerged all identical word forms together, bothas targets and contexts.
But disambiguationtechniques uch as those in Experiment #2can be used to differential ly tag wordoccurrences with some degree of accuracy.These newly classified items can in turn beused as new target and context items (if theirfrequencies are adequate) and the analysiscan be repeated.
Iterating the method in thisway should be able to refine the classes until afixed point is reached at which no furtherimprovement in classification occurs.
Themajor challenge in using this approach will beto keep it computationally tractable.
Thisapproach is similar in spirit to the iterativecomputational approaches of the HiddenMarkov Models (Kupiec, 1989; Kupiec, 1992;Rabiner, 1989), though our zeroth ordersolution begins quite close to the desiredresult, so it should converge very close to aglobal optimum.111 VBNP21I l l  VBNP 4 JJ 31 'I 9 VBD 46 VBN 3 JJ 1 JJ1 VBD1 VBNPFigure 2B.
Clustering of"cloned" and "deduced" after expansion of the context words.
Thepostmodifying form, not isolated before, is fairly well isolated in its own subclass.
The totalnumber of errors is reduced from 23 to 11, for a success rate of 92%.124EXPERIMENT #3:CLASSIFICATION OF SINGLEWORD OCCURRENCESWhen classifying multiple instances of asingle word form as we did in Experiment #2,there are numerous collocations that aid theclassification.
For example, 16 of the 50occurrences of the word "deduced" occur in thephrase, "of the ~ amino acid sequence".But with words of frequency 1, we cannot relyon such similarities.
Nevertheless, weexperimented with classifying 100 words ofcorpus frequency 1 with and withoutexpanding the context words.
Though handscoring the results is difficult, we estimatethat there were 8 reasonable pairs foundinitially and 26 pairs when expansion wasused.Examples of words that paired wellwithout expansion are "overlaps" and "flank"(due to a preceding "which") and "malB" and"cheA-cheB" (due to the context "...the\[malB, cheA-cheB\] region...").
Afterexpansion, pairs such as "setting", "resetting"appeared (due in part to the expansion of thepreceding "as" and "to" context words intosimsets which both included "with", "in" and"by").Discussion of Exper iment  #3.The amount of information available aboutfrequency 1 words can vary from a lot tonothing at all, and most frequently tends tothe latter, viz., "John and Mary looked at theblork."
Nevertheless,  such words areprominent, 44% of our corpus' vocabulary.About half Of them are non-technical and cantherefore be analyzed from other corpora oron-line dictionaries.
Word morphology andLatinate morphology in particular, can behelpful.
Online chemical databases,supplemented with rules for chemicalnomenclature will clarify additional items,e .g .
,  "2-epoxypropylphosphonic"  or"phosphoglucomutase-deflcient".Furthermore, there are naming conventionsfor genetic strains and mutants which aidrecognition.
The combination of all thesemethods hould lead to a reasonable accuracyin the classification of frequency 1words.FURTHER DISCUSSION ANDFUTURE DIRECTIONSOur corpus of 220,000 words is muchsmaller than ones of 40 million words (Finch& Chater, 1992) and certainly of 360 million(Brown, Della Pietra, deSousa, Lai & Mercer,1992).
But judging by the results we havepresented, especially for the full 1,000 wordclustering, our corpus appears to make up inspecificity for what it lacks in size.
Extendingthis work beyond abstracts to full papers willbe challenging because our corpus requiresSGML markup to deal with Greek characters,superscripts and subscripts, etc.
(Futrelle,Dunn, Ellis & Pescitelli, 1991).
We have over500,000 words from the bacterial chemotaxisresearch papers carefully marked up by handin this way.The characterization of context canobviously be extended to more contextpositions or words, and extensions of ourword-rooted expansion techniques arepotentially very powerful, combining broadcoverage with specificity in a "tunable" way.Morphology can be added to the contextvectors by using the ingenious suggestion ofBrill to collect high-frequency tri-letter wordendings (Brill & Marcus, 1992).One of the more subtle problems of thecontext specification is that it uses summedfrequencies, o it may fail to retain importantcorrelations.
Thus if only AB or CD sequencesoccurred, or only AD or CB sequences, theywould lead to the same (summed) contextvector.
The only correlations faithfullyretained are those with the target word.Characterizing context n-grams could helpwork around this problem, but is a non-trivialtask.ACKNOWLEDGMENTSThanks to Durstin Selfridge for a carefulreading of the drafts and to an anonymousreviewer for pointing out the work of (Phillips,1985) who builds networks to describe word125classes rather than trees as we have.
Thanksto the ERC for providing an excellent workingenvironment.BIBLIOGRAPHYBerrey, L. V.
(Ed.).
(1962).
Roget'sInternational Thesaurus.
New York, NY:Thomas Y. Crowell.Brill, E., & Marcus, M. (1992).
Tagging anUnfamiliar Text with Minimal HumanSupervision.
In AAA/  Fall SymposiumSeries: Probabilistic Approaches toNatural Language (Working Notes), (pp.10-16).
Cambridge, MA.Brown, P. F., Della Pietra, V. J., deSousa, P.V., Lai, J. C., & Mercer, R. L. (1992).Class-based n-gram models of naturallanguage.
Computational Linguistics,18(4), 467-479.Church, K., Gale, W., Hanks, P., & Hindle, D.(1991).
Using statistics in lexical analysis.In U. Zernik (Eds.
), Lexical Acquisition:Exploiting On-Line Resources to Build aLexicon (pp.
115-164).
Hillsdale, NJ:Lawrence Erlbaum.Church, K. W. (1988).
A Stochastic PartsProgram and Noun Parser forUnrestricted Text.
In Proc.
2nd Conf.
onApplied Nat.
Lang.
Processing, (pp.
136-143).
Austin, TX.Church, K. W., & Hanks, P. (1990).
WordAssociation Norms, Mutual Informationand Lexicography.
Computat iona lLinguistics, 16(1), 22-29.Cutting, D., Kupiec, J., Pedersen, J., & Sibun,P.
(1992).
A Practical Part-of-SpeechTagger.
In Proc.
3rd Conf.
on AppliedNatural Language Processing, (pp.
133-140).Finch, S., & Chater, N. (1992).
BootstrappingSyntactic Categories Using StatisticalMethods.
In W. Daelemans & D.
Powers(Ed.
), Proc.
Ist SHOE Workshop, (pp.
229-235).
Tilburg U., The Netherlands.Francis, W. N., & Kucera, H. (1982).Frequency Analysis of English Usage.Boston, MA: Houghton Mifflin.Futrelle, R. P., Dunn, C. C., Ellis, D. S., &Pescitelli, M. J., Jr. (1991).
Preprocessingand lexicon design for parsing technicaltext.
In Proc.
2nd Intern'l Workshop onParsing Technologies, (pp.
31-40): ACL.Futrelle, R. P., & Gauch, S. E. (1993).
Usingunsupervised and supervised classificationto discover word equivalences and otherrelations.
In 9th Ann.
Waterloo OED Conf.Oxford, England (submitted).Futrelle, R. P., Kakadiaris, I.
A., Alexander,J., Carriero, C. M., Nikolakis, N., &Futrelle, J. M. (1992).
UnderstandingDiagrams in Technical Documents.
IEEEComputer, 25(7), 75-78.Gale, W. A., Church, K. W., & Yarowsky, D.(1992).
Work on Statistical Methods forWord Sense Disambiguation.
I  AAAIFallSymposium Series: Probabil isticApproaches to Natural Language (WorkingNotes), (pp.
54-60).
Cambridge, MA.Gauch, S., & Futrelle, R. P. (1993).
Broad andDeep Classification Methods for MiningRaw Text.
In CIKM 93.
Washington, DC(submitted).Grefenstette, G. (1992).
Finding SemanticSimilarity in Raw Text: the DeeseAntonyms.
In AAAI Fall SymposiumSeries: Probabilistic Approaches toNatural Language (Working Notes), (pp.61-66).
Cambridge, MA.Jain, A. K., & Dubes, R. C. (1988).
Algorithmsfor Clustering Data.
Englewood Cliffs, NJ:Prentice Hall.Jain, A. K., & Moreau, J. V. (1987).
BootstrapTechnique in Cluster Analysis.
PatternRecognition, 20(5), 547-568.Krovetz, R. (1991).
Lexical Acquisition andInformation Retrieval.
In U. Zernik (Eds.
),Lexical Acquisition: Exploiting On-LineResources to Build a Lexicon (pp.
45-64).126Hillsdale, New Jersey: Lawrence ErlbaumAssociates, Publishers.Kupiec, J .
(1989).
Augmenting a hiddenMarkov model for phrase dependent wordtagging.
In DARPA Speech and NaturalLanguage Workshop, a, (pp.
92-98).
CapeCod MA.Kupiec, J.
(1992).
Robust part-of-speechtagging using a hidden Markov model.Computer Speech and Language, 6, 225-242.Lesk, M. (1986).
Automat ic  SenseDisambiguation Using Machine ReadableDictionaries: How to Tell a Pine Conefrom and Ice Cream Cone.
In ACMSIGDOC Conf., (pp.
24-26).
Toronto, Ont.
:ACM Press.Liddy, E. D., & Paik, W. (1992).
Statistically-Guided Word Sense Disambiguation.
InAAAI Fall Symposium Series:Probabilistic Approaches to NaturalLanguage (Working Notes), (pp.
98-107).Cambridge, MA.Myaeng, S. H., & Li, M. (1992).
Building TermClusters by Acquiring Lexical Semanticsfrom a Corpus.
In Y. Yesha (Ed.
), CIKM-92, (pp.
130-137).
Baltimore, MD: ISMM.Phillips, M. (1985).
Aspects of text structure:an investigation of the lexical organisationof text.
New York, NY: Elsevier.Quinlan, J. R. (1993).
C4.5: Programs forMachine Learning.
San Mateo, CA:Morgan Kaufmann.Rabiner, L. R. (1989).
A Tutorial on HiddenMarkov Models an Selected Applicationsin SpeeCh Recognition.
Proceedings of theIEEE, 77(2), 257-286.Schutze, H. (1992).
Context Space.
In AAAIFall Symposium Series: ProbabilisticApproaches to Natural Language (WorkingNotes), (pp.
113-120).
Cambridge, MA.Zernik, U.
(1991).
Tralnl vs. Train2: TaggingWord Senses in Corpus.
In U.
Zernik(Eds.
), Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon (pp.
97-112).
Hillsdale, New Jersey: LawrenceErlbaum Associates, Publishers.127
