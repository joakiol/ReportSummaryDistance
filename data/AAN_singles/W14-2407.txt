Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 33?38,Baltimore, Maryland USA, June 26 2014.c?2014 Association for Computational LinguisticsCooking with SemanticsJon MalmaudBrain and Cognitive SciencesMIT43 Vassar St.Cambridge, MAmalmaud@mit.eduEarl J. Wagner, Nancy Chang, Kevin MurphyGoogle1600 Amphitheatre PkwyMountain View, CAwag@google.com, ncchang@google.com,kpmurphy@google.comAbstractWe are interested in the automatic inter-pretation of how-to instructions, such ascooking recipes, into semantic representa-tions that can facilitate sophisticated ques-tion answering.
Recent work has shownimpressive results on semantic parsing ofinstructions with minimal supervision, butsuch techniques cannot handle much of thesituated and ambiguous language used ininstructions found on the web.
In this pa-per, we suggest how to extend such meth-ods using a model of pragmatics, based ona rich representation of world state.1 IntroductionUnderstanding instructional text found on the webpresents unique challenges and opportunities thatrepresent a frontier for semantic parsing.
Cru-cially, instructional language is situated: it as-sumes a situational context within which the agent(i.e., the reader) is to carry out a sequence of ac-tions, as applied to objects that are (or become)available in the immediate environment.
These ac-tions and objects may not be explicitly specified;indeed, much instructional language is ambigu-ous, underspecified and often even ungrammaticalrelative to conventional usage.In this ?vision paper?, we focus on interpretingcooking recipes.
While there are several servicesthat already support searching for recipes (suchas Google Recipe Search1, Yummly, Foodily, andMyTaste), the faceted search capabilities they pro-vide are limited to recipe meta-data such as ingre-dients, genres, cooking time, portions, and nutri-tion values.
Some of this information is explicitlymarked up in machine-readable form2.
However,1http://www.google.com/insidesearch/features/recipes/2See e.g.
http://microformats.org/wiki/recipe-formatsFigure 1: Example recipes.
Left: for a mixeddrink.
Right: for guacamole dip.the actual steps of the recipe are treated as an un-structured blob of text.
(The same problem ap-plies to other instructional sites, such as ehow.com, wikihow.com, answers.yahoo.com,www.instructables.com, etc.)
Interpretingthe steps of recipes (and instructions more gener-ally) is the goal of this paper.2 ChallengesThis section surveys some of the linguistic chal-lenges typical of the cooking domain, as illustratedby the two recipes in Figure 1.
These difficultiescan be classified broadly as problems arising fromthe interpretation of arguments, actions and con-trol structure.Arguments: One particularly salient character-istic of recipes is that they often feature argumentsthat are omitted, underspecified or otherwise de-pendent on the context.
Arguments may be elidedin syntactic contexts where they are usually re-quired (the so-called ?zero anaphora?
problem),especially when they are easily filled by an objectin the immediate context.
For example, the itemto set aside in (1a) is the just-treated cocktail glass,and the item to fill in (1b) and shake and then strainin (1c) is the recently mentioned shaker.
Note thatthe context may include the ingredient list itself, asillustrated by the elided argument(s) to be addedin the one-line recipe ?Add to a cocktail glass inthe order listed.?
Arguments may be implicitlyavailable, based on either domain-specific expec-tations of the initial context or the results of pre-33ceding steps.
The ice in (1b) isn?t listed in thecorresponding recipes ingredient list, since manycommon ingredients (water, ice, salt, pepper) areassumed to be available in most kitchens.
Some-times, the argument may never have been directlyverbalized, but rather is the result of a previous ac-tion.
Thus in the recipe ?Pour ingredients over iceand shake vigorously,?
the object to shake is thecontainer (only implicitly available) along with itscontents ?
which, once the ?pour?
instruction isexecuted, include both ice and the (listed) ingre-dients.
Note also that interpreting ?the remain-ing ingredients?
in (1b) requires an understand-ing of which ingredients have yet to be used atthat point in the recipe.
Arguments may be in-directly available, by association with an explic-itly available argument.
Recipe 2 mentions avo-cados in several explicit and implicit referring ex-pressions; of these only the ?them?
in (2a) maybe considered straightforward anaphoric reference(to the just-cut avocados).
Step (2b) involves ametonymic reference to the ?skin and pits?
wherethe part-whole relation between these items andthe avocado is what makes the instruction inter-pretable.
Step (2c) once again mentions ?avoca-dos?, but note that this now refers to the flesh ofthe avocados, i.e., the implicit scooped-out objectfrom (2a).
Arguments may be incompletely speci-fied, especially with respect to amount.
The exactamount of sugar needed in (1a) is not mentioned,for example.
Similarly, the amount of ice neededin (1b) depends on the size of the shaker and is notprecisely specified.Actions: Like arguments, action interpretationalso depends on the situational context.
For exam-ple, actions may have ambiguous senses, mainlydue to the elided arguments noted above.
The verb?shake?
in (1c), for example, yields a spurious in-transitive reading.
Actions may have argument-dependent senses: certain verbs may resolve todifferent motor actions depending on the affor-dances of their arguments.
For example, the ac-tion intended by the verb ?garnish?
in (1d) mightinvolve careful perching of the peel on the rimof the glass; in other recipes, the same verb ap-plied to nutmeg or cut fruit may be better inter-preted as an add action.
Actions may be omittedor implied, in particular by the way certain argu-ments are expressed.
Most recipes involving eggs,for example, do not explicitly mention the need tocrack them and extract their contents; this is a de-fault preparatory step.
Other ingredients vary inhow strongly they are associated with (implicit)preparatory steps.
For example, recipes callingfor ?1/4 avocado?
may require that something likesteps (2a-b) be undertaken (and their results quar-tered); the ?orange peel?
of (1d) may likewise de-pend on a separate procedure for extracting peelfrom an orange.Control structure: Instructions sometimesprovide more complex information about se-quence, coordination and control conditions.
Con-ditions: An action may be specified as being per-formed until some finish condition holds.
In (2c),the ?until smooth?
condition?itself featuring anelided avocado argument?controls how long theblending action should continue.
Other conditionsmentioned in recipes include ?Add crushed ice un-til the glass is almost full?, ?Stir until the glass be-gins to frost?, and ?Add salt to taste?.
Sequence:Though implicitly sequential, recipes occasion-ally include explicit sequencing language.
In therecipe ?Add to a cocktail glass in the order listed?,the order reflects that of the ingredient list.
Otherrecipes specify that certain steps can or should bedone ?ahead of time?, or else while other steps arein progress.
Alternatives: Recipes sometimes al-low for some variability, by specifying alternativeoptions for specific ingredients (?Garnish with atwist of lemon or lime?
), appliances or utensils(?Using a large fork (or a blender)...?
), and evenactions (?Chop or mash the avocados?
).As should be clear from these examples, the in-terpretation of a given step in a set of instructionsmay hinge on many aspects of situated and proce-dural knowledge, including at least: the physicalcontext (including the particular props and toolsassumed available); the incremental state result-ing from successful execution of previous steps;and general commonsense knowledge about theaffordances of specific objects or expected argu-ments of specific actions (or more conveniently,corpus-based verb-argument expectations that ap-proximate such knowledge, see e.g., (Nyga andBeetz, 2012)).
All of these sources of knowl-edge go significantly beyond those employed insemantic parsing models for single utterances andin non-procedural contexts.3 Proposed approachWe propose to maintain a rich latent context thatpersists while parsing an entire recipe, in contrast34BowlFlourMilkEmptyIn(packet)In(jug)Has(milk)In(packet)In(bowl)Add:from: jugto: bowlwhat: milkmanner: pouring"Pour milkinto a bowl"Has(milk, flour)In(bowl)In(bowl)Add:from: packetto: bowlwhat: flour"Add flour"Has(milk, flour)In(bowl)In(bowl)Mix:what: bowlmanner: wellS0 S1A1WA1 WA2"The pasteshould besmooth.
"WS3S2 S3A2 A3VA1VS3VideoImageWA3"Mix well"Figure 2: Our proposed probabilistic model, showing a possible trace of observed and latent variablesafter parsing each step of a pancake recipe.
See text for description of notation.to approaches that interpret each sentence inde-pendently.
This context represents the state of thekitchen, and statements in the recipes are inter-preted pragmatically with respect to the evolvingcontext.
More precisely, our model has the over-all structure of a discrete-time, partially observed,object-oriented Markov Decision Process, as il-lustrated in Figure 2.
The states and actions areboth hidden.
What we observe is text and/or im-ages/video; our goal is to infer the posterior overthe sequence of actions (i.e., to recover the ?true?recipe), given the noisy evidence.States and actions.
The world state Stis repre-sented as a set of objects, such as ingredients andcontainers, along with various predicates, encod-ing the quantity, location, and condition (e.g., rawor cooked, empty or full) of each object.
Note thatprevious work on situated semantic parsing oftenuses grid world environments where the only flu-ent is the agent?s location; in contrast, we allowany object to undergo state transformations.
Inparticular, objects can be created and destroyed.Each action Atis represented by a semanticframe, corresponding to a verb with various ar-guments or roles.
This specifies how to trans-form the state.
We also allow for sequencingand loop frames c.f., the ?robot control language?in (Matuszek et al., 2013).
We assume accessto a simple cooking simulator that can take in astream of low-level instructions to produce a newstate; this implements the world dynamics modelp(St|St?1,At).Text data.
We assume that the text of thet?th sentence, represented by WAt, describes thet?th primitive action, At.
We represent the con-ditional distribution p(At|WAt,St?1) as a log-linear model, as in prior work on frame-semanticparsing/ semantic role labeling (SRL) (Das et al.,2014).3However, we extend this prior work by al-lowing roles to be filled not just from spans fromthe text, but also by objects in the latent state vec-tor.
We will use various pragmatically-inspiredfeatures to represent the compatibility betweencandidate objects in the state vector and roles inthe action frame, including: whether the objecthas been recently mentioned or touched, whetherthe object has the right affordances for the cor-responding role (e.g., if the frame is ?mix?, andthe role is ?what?, the object should be mixable),3Although CCGs have been used in previous work on(situated) semantic parsing, such as (Artzi and Zettlemoyer,2013), we chose to use the simpler approach based on framesbecause the nature of the language that occurs in recipesis sufficiently simple (there are very few complex nestedclauses).35etc.
More sophisticated models, based on model-ing the belief state of the listener (e.g., (Goodmanand Stuhlm?uller, 2013; Vogel et al., 2013)) are alsopossible and within the scope of future work.In addition to imperative sentences, we some-times encounter descriptive sentences that de-scribe what the state should look like at a givenstep (c.f., (Lau et al., 2009)).
We let WStdenote asentence (possibly empty) describing the t?th state,St.
The distribution p(St|WSt) is a discriminativeprobabilistic classifier of some form.Visual data.
Much instructional information isavailable in the form of how-to videos.
In addi-tion, some textual instructions are accompanied bystatic images.
We would like to extend the modelto exploit such data, when available.Let a video clip associated with an action attime t be denoted by VAt.
We propose to learnp(At|VAt) using supervised machine learning.For features, we could use the output of standardobject detectors and their temporal trajectories, asin (Yu and Siskind, 2013), bags of visual wordsderived from temporal HOG descriptors as in (Daset al., 2013), or features derived from RGB-D sen-sors such as Kinect, as in (Song et al., 2013; Lei etal., 2012).There are many possible ways to fuse the in-formation from vision and text, i.e., to com-pute p(At|VAt,WAt,St?1).
The simplest ap-proach is to separately train the two conditionals,p(At|WAt,St?1) and p(At|VAt), and then trainanother model to combine them, using a separatevalidation set; this will learn the relative reliabilityof the two sources of signal.Learning and inference.
We assume that wehave manually labeled the actions At, and that theinitial state S0is fully observed (e.g., a list of in-gredients, with all containers empty).
If we ad-ditional assume that the world dynamics model isknown4and deterministic, then we can uniquelyinfer the sequence of states S1:T. This lets us usestandard supervised learning to fit the log-linearmodel p(At|WAt,St?1).In the future, we plan to relax the assumptionof fully labeled training data, and to allow forlearning from a distant supervision signal, simi-lar to (Artzi and Zettlemoyer, 2013; Branavan etal., 2009).
For example, we can prefer a parse thatresults in a final state in which all the ingredients4There has been prior work on learning world modelsfrom text, see e.g., (Sil and Yates, 2011; Branavan et al.,2012).have been consumed, and the meal is prepared.4 Preliminary resultsWe conducted a preliminary analysis to gauge thefeasibility and expected performance benefits ofour approach.
We used the raw recipes providedin the CMU Recipe Database (Tasse and Smith,2008), which consists of 260 English recipesdownloaded from allrecipes.com.
We thenapplied a state-of-the art SRL system (Das et al.,2014) to the corpus, using Propbank (Palmer et al.,2005) as our frame repository.
Figure 3 summa-rizes our findings.To judge the variance of predicates used in thecooking domain, we computed the frequency ofeach word tagged as a present-tense verb by a sta-tistical part-of-speech tagger, filtering out a smallnumber of common auxiliary verbs.
Our find-ings suggest a relatively small number of verbsaccount for a large percentage of observed instruc-tions (e.g, ?add?, ?bake?, and ?stir?).
The majorityof these verbs have corresponding framesets thatare usually correctly recognized, with some no-table exceptions.
Further, the most common ob-served framesets have a straightforward mappingto our set of kitchen state transformations, suchas object creation via combination (?add?, ?mix?,?combine?, ?stir in?
), location transfers (?place?,?set?
), and discrete state changes over a smallspace of features (?cook?, ?cut?, ?cool?, ?bake?
).To gain a preliminary understand of the limi-tations of the current SRL system and the possi-ble performance benefits of our proposed system,we hand-annotated five of our recipes as follows:Each verb in the recipe corresponding to an actionwas annotated with its best corresponding roleset(if any).
Each role in that roleset was marked aseither being explicitly present in the text, implic-itly present in our latent kitchen model but not inthe text (and so in principle, fillable by our model),or neither present in the text nor in our model.
Forexample, in?cover for forty minutes?, the frameset?cover?
has an explicit temporal role-filling (?forforty minutes?)
and an implicit role-filling (?thepot?
as the patient of ?cover?
).For each verb in the annotation, we checked ifthe SRL system mapped that verb to the correctroleset and if so, whether it filled the same seman-tic roles as the annotator indicated were explicitlypresent in the text.
Overall, we found 54% recallof the annotations by the SRL system.
We quali-36add stir mix bake combine remove place pour cook coolVerb020406080100120140Frequency add.02 combine.01cook.01 mix.01 place.01 stir.02 remain.01 bring.01 serve.01 melt.01Roleset020406080100120140FrequencyHeat oil in a large pot >until hot@  brown chicken >in the pot@ .5emoYe  chicken >from the pot@  and set >the chicken@  aside.6aute  onions until >the onions are@  soft, about  minutes.Add broth, beans, half the pepper, and all the chicken >to the pot@  coYer  and simmer >the pot contents@  for  minutes.Add parsley, cilantro, salt, and remaining pepper >to the pot@ , and simmer >the mixture@   more minutes.Figure 3: Results.
Top: Distribution of the ten most common verbs and framesets in 260 recipes fromallrecipes.com.
Bottom: An example recipe annotation.
Blue indicates propbank predicates.
Bracketedred indicates implicit propbank arguments not in the text, but in principle recognizable by our model.Green indicates quantifier adjectives which our model could resolve to an exact quantity, given initialingredient amounts.tatively notes several failure modes.
Many errorsarise from not recognizing predicates representedin the text as an imperative verb, likely becausePropBank contains few examples of such languagefor the labeler to learn from.
Other errors resultfrom ungrammatical constructs (e.g.
in ?cook fiveminutes?, the eliding of ?for?
causes ?five min-utes?
to incorrectly parse as a direct argument).Certain cooking-related verbs lack framesets en-tirely, such as ?prebake?.
Occasionally, the wrongroleset is chosen.
For example, in?Stir the mix-ture?
, ?Stir?
is labeled as ?stir.02: cause (emo-tional) reaction?
rather than ?stir.01: mix with acircular motion?.We also analyzed the quantity and qualitativetrends in the human annotations that refer to rolesfillable from the latent kitchen model but not lit-erally present in the text.
Overall, 52% of verbannotations referenced at least one such role.
Themost common situation (occurring for 36% of allannotated verbs) is the ?patient/direct object?
roleis elided in the text but inferable from the worldstate, as in ?simmer [the mixture] for 40 min-utes?.
The second most common is the ?location?modifier role is elided in the text, as in ?Removechicken [from the pot]?.
Overall, we believe ourproposed approach will improve the quality of theSRL system, and thus the overall interpretabilityof the recipes.5 Possible applicationsWe believe that semantic parsing of recipes andother instructional text could support a rich arrayof applications, such as the following:Deriving a ?canonical?
recipe.
It would beuseful to align different versions of the samerecipe to derive a ?canonical form?
cf., (Druck andPang, 2012; Tenorth et al., 2013b).Explaining individual steps.
It would be help-ful if a user could click on a confusing step in arecipe and get a more detailed explanation and/oran illustrative video clip.Automatically interpreting software instruc-tions.
Going beyond the recipe domain, it wouldbe useful to develop a system which can interpretinstructions such as how to install software, andthen automatically execute them (i.e., install thesoftware for you).
In practice, this may be toohard, so we could allow the system to ask for hu-man help if it gets stuck, cf.
(Deits et al., 2013).Robotics.
(Tenorth et al., 2013a) suggest min-ing natural language ?action recipes?
as a way tospecify tasks for service robots.
In the domainof food recipes, there have already been severaldemonstrations (e.g., (Beetz et al., 2011; Bollini etal., 2013)) of robots automatically cooking mealsbased on recipes.Task assistance using augmented reality.Imagine tracking the user as they follow some in-structions using a device such as Google glass, andoffering help when needed.
Such systems havebeen developed before for specialized domainslike maintenance and repair of military hardware5,but automatic parsing of natural language text po-tentially opens this up to the consumer market.
(Note that there is already a recipe app for GoogleGlass6, although it just displays a static list of in-structions.
)5For example, see http://graphics.cs.columbia.edu/projects/armar/index.htm.6See http://www.glassappsource.com/listing/all-the-cooks-recipes.37ReferencesY.
Artzi and L. Zettlemoyer.
2013.
Weakly supervisedlearning of semantic parsers for mapping instruc-tions to actions.
Trans.
Assoc.
for ComputationalLinguistics, 1:49?62.M.
Beetz, U. Klank, I. Kreese, A. Maldonado,L.
Mosenlechner, D. Pangercic, T. Ruhr, andM.
Tenorth.
2011.
Robotic roommates making pan-cakes.
In Intl.
Conf.
on Humanoid Robots.Mario Bollini, Stefanie Tellex, Tyler Thompson,Nicholas Roy, and Daniela Rus.
2013.
Interpretingand executing recipes with a cooking robot.
Experi-mental Robotics.SRK Branavan, H. Chen, L. Zettlemoyer, and R. Barzi-lay.
2009.
Reinforcement learning for mapping in-structions to actions.
In Association for Computa-tional Linguistics.S.R.K.
Branavan, N. Kushman, T. Lei, and R. Barzilay.2012.
Learning High-Level Planning from Text.
InACL.P.
Das, C. Xu, R. F. Doell, and J. J. Corso.
2013.
Athousand frames in just a few words: Lingual de-scription of videos through latent topics and sparseobject stitching.
In CVPR.D.
Das, D. Chen, A. Martins, N. Schneider, andN.
Smith.
2014.
Frame-semantic parsing.
Com-putational Linguistics.R.
Deits, S. Tellex, P. Thaker, D. Simeonov, T. Kol-lar, and N. Roy.
2013.
Clarifying Commandswith Information-Theoretic Human-Robot Dialog.J.
Human-Robot Interaction.G.
Druck and B. Pang.
2012.
Spice it Up?
MiningRenements to Online Instructions from User Gener-ated Content.
In ACL.Noah D Goodman and Andreas Stuhlm?uller.
2013.Knowledge and implicature: Modeling language un-derstanding as social cognition.
Topics in cognitivescience, 5(1):173?184.TA Lau, Clemens Drews, and Jeffrey Nichols.
2009.Interpreting Written How-To Instructions.
IJCAI.J.
Lei, X. Ren, and D. Fox.
2012.
Fine-grained kitchenactivity recognition using RGB-D.
In Ubicomp.C.
Matuszek, E. Herbst, L. Zettlemoyer, and D. Fox.2013.
Learning to parse natural language commandsto a robot control system.
Experimental Robotics,pages 1?14.D.
Nyga and M. Beetz.
2012.
Everything robots al-ways wanted to know about housework (but wereafraid to ask).
In Intl.
Conf.
on Intelligent Robotsand Systems.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
Theproposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?106.A.
Sil and A. Yates.
2011.
Extracting STRIPS Rep-resentations of Actions and Events.
In Recent Ad-vances in NLP.Young Chol Song, Henry Kautz, James Allen, MarySwift, Yuncheng Li, Jiebo Luo, and Ce Zhang.2013.
A markov logic framework for recognizingcomplex events from multimodal data.
In Proc.
15thACM Intl.
Conf.
Multimodal Interaction, pages 141?148.
ACM.D.
Tasse and N. Smith.
2008.
SOUR CREAM: To-ward Semantic Processing of Recipes.
TechnicalReport CMU-LTI-08-005, Carnegie Mellon Univer-sity, Pittsburgh, PA.M.
Tenorth, A. Perzylo, R. Lafrenz, and M. Beetz.2013a.
Representation and exchange of knowl-edge about actions, objects, and environments in theroboearth framework.
IEEE Trans.
on AutomationScience and Engineering, 10(3):643?651.M.
Tenorth, J. Ziegltrum, and M. Beetz.
2013b.
Auto-mated alignment of specifications of everyday ma-nipulation tasks.
In IEEE Intl.
Conf.
on IntelligentRobots and Systems.A.
Vogel, M. Bodoia, C. Potts, and D. Jurafsky.
2013.Emergence of Gricean Maxims from Multi-AgentDecision Theory.
In NAACL.Haonan Yu and JM Siskind.
2013.
Grounded languagelearning from video described with sentences.
In As-sociation for Computational Linguistics.38
