Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
1034?1043, Prague, June 2007. c?2007 Association for Computational LinguisticsValidation and Evaluation of Automatically Acquired MultiwordExpressions for Grammar EngineeringAline Villavicencio?
?, Valia Kordoni?, Yi Zhang?,Marco Idiart?
and Carlos Ramisch?
?Institute of Informatics, Federal University of Rio Grande do Sul (Brazil)?Department of Computer Sciences, Bath University (UK)?Department of Computational Linguistics, Saarland University, and DFKI GmbH (Germany)?Institute of Physics, Federal University of Rio Grande do Sul (Brazil)avillavicencio@inf.ufrgs.br, {yzhang,kordoni}@coli.uni-sb.deidiart@if.ufrgs.br, ceramisch@inf.ufrgs.brAbstractThis paper focuses on the evaluation of meth-ods for the automatic acquisition of MultiwordExpressions (MWEs) for robust grammar engi-neering.
First we investigate the hypothesis thatMWEs can be detected by the distinct statisticalproperties of their component words, regardlessof their type, comparing 3 statistical measures:mutual information (MI), ?2 and permutationentropy (PE).
Our overall conclusion is that atleast two measures, MI and PE, seem to differen-tiate MWEs from non-MWEs.
We then investi-gate the influence of the size and quality of differ-ent corpora, using the BNC and the Web searchengines Google and Yahoo.
We conclude that, interms of language usage, web generated corporaare fairly similar to more carefully built corpora,like the BNC, indicating that the lack of con-trol and balance of these corpora are probablycompensated by their size.
Finally, we show aqualitative evaluation of the results of automat-ically adding extracted MWEs to existing lin-guistic resources.
We argue that such a processimproves qualitatively, if a more compositionalapproach to grammar/lexicon automated exten-sion is adopted.1 IntroductionThe task of automatically identifying MultiwordExpressions (MWEs) like phrasal verbs (breakdown) and compound nouns (coffee machine)using statistical measures has been the focusof considerable investigative effort, (e.g.
Pearce(2002), Evert and Krenn (2005) and Zhang etal.
(2006)).
Given the heterogeneousness ofthe different phenomena that are considered tobe MWEs, there is no consensus about whichmethod is best suited for which type of MWE,and if there is a single method that can be suc-cessfully used for any kind of MWE.Another difficulty for work on MWE identifi-cation is that of the evaluation of the results ob-tained (Pearce, 2002; Evert and Krenn, 2005),starting from the lack of consensus about a pre-cise definition for MWEs (Villavicencio et al,2005).In this paper we investigate some of the is-sues involved in the evaluation of automaticallyextracted MWEs, from their extraction to theirsubsequent use in an NLP task.
In order to dothat, we present a discussion of different statisti-cal measures, and the influence that the size andquality of different data sources have.
We thenperform a comparison of these measures and dis-cuss whether there is a single measure that hasgood overall performance for MWEs in general,regardless of their type.
Finally, we perform aqualitative evaluation of the results of addingautomatically extracted MWEs to a linguisticresource, taking as basis for the evaluation theapproach proposed by Zhang et al (2006).
Weargue that such results can improve in qualityif a more compositional approach to MWE en-coding is adopted for the grammar extension.Having more accurate means of deciding for anappropriate method for identifying and incor-porating MWEs is critical for maintaining thequality of linguistic resources for precise NLP.This paper starts with a discussion of MWEs(?
2), of their coverage in linguistic resources(?
3), and of some methods proposed for auto-matically identifying them (?
4).
This is fol-lowed by a detailed investigation and compar-ison of measures for MWE identification (?
5).1034After that we present an approach for predictingappropriate lexico-syntactic categories for theirinclusion in a linguistic resource, and an evalu-ation of the results in a parsing task(?
7).
Wefinish with some conclusions and discussion offuture work.2 Multiword ExpressionsThe term Multiword Expressions has been usedto describe expressions for which the syntactic orsemantic properties of the whole expression can-not be derived from its parts (Sag et al, 2002),including a large number of related but distinctphenomena, such as phrasal verbs (e.g.
comealong), nominal compounds (e.g.
frying pan),institutionalised phrases (e.g.
bread and butter),and many others.
Jackendoff (1997) estimatesthe number of MWEs in a speaker?s lexicon tobe comparable to the number of single words.However, due to their heterogeneous character-istics,MWEs present a tough challenge for bothlinguistic and computational work (Sag et al,2002).
For instance, some MWEs are fixed, anddo not present internal variation, such as ad hoc,while others allow different degrees of internalvariability and modification, such as spill beans(spill several/musical/mountains of beans).Sag et al (2002) discuss two main ap-proaches commonly employed in NLP for treat-ing MWEs: the words-with-spaces approachmodels an MWE as a single lexical entry and itcan adequately capture fixed MWEs like by andlarge.
A compositional approach treats MWEsby general and compositional methods of lin-guistic analysis, being able to capture more syn-tactically flexible MWEs, like rock boat, whichcannot be satisfactorily captured by a words-with-spaces approach, since it would require lex-ical entries to be added for all the possiblevariations of an MWE (e.g.
rock/rocks/rockingthis/that/his... boat).
Therefore, to provide aunified account for the detection and encodingof these distinct but related phenomena is a realchallenge for NLP systems.3 Grammar and Lexicon Coverage inDeep ProcessingMany NLP tasks and applications, like Parsingand Machine Translation, depend on large-scalelinguistic resources, such as electronic dictionar-ies and grammars for precise results.
Severalsubstantial resources exist: e.g., hand-craftedlarge-scale grammars like the English ResourceGrammar (ERG - Flickinger (2000)) and theDutch Alpino Grammar (Bouma et al, 2001).Unfortunately, the construction of these re-sources is the manual result of human efforts andtherefore likely to contain errors of omission andcommission (Briscoe and Carroll, 1997).
Fur-thermore, due to the open-ended and dynamicnature of languages, such linguistic resources arelikely to be incomplete, and manual encoding ofnew entries and constructions is labour-intensiveand costly.Take, for instance, the coverage test resultsfor the ERG (a broad-coverage precision HPSGgrammar for English) on the British NationalCorpus (BNC).
Baldwin et al (2004), amongmany others, have investigated the main causesof parse failure, parsing a random sample of20,000 strings from the written component ofthe BNC using the ERG.
They have found thatthe large majority of failures is caused by miss-ing lexical entries, with 40% of the cases, andmissing constructions, with 39%, where missingMWEs accounted for 8% of total errors.
That is,even by a margin, the lexical coverage is lowerthan the grammar construction coverage.This indicates the acute need for robust (semi-)automated ways of acquiring lexical informa-tion for MWEs, and this is the one of the goalsof this work.
In the next section we discusssome approaches that have been developed in re-cent years to (semi-)automatically detect and/orrepair lexical and grammar errors in linguisticgrammars and/or extend their coverage.4 Acquiring MWEsThe automatic acquisition of specific types ofMWE has attracted much interest (Pearce,2002; Baldwin and Villavicencio, 2002; Evertand Krenn, 2005; Villavicencio, 2005; van der1035Beek, 2005; Nicholson and Baldwin, 2006).
Forinstance, Baldwin and Villavicencio (2002) pro-posed a combination of methods to extract Verb-Particle Constructions (VPCs) from unanno-tated corpora, that in an evaluation on theWall Street Journal achieved 85.9% precisionand 87.1% recall.
Nicholson and Baldwin (2006)investigated the prediction of the inherent se-mantic relation of a given compound nominaliza-tion using as statistical measure the confidenceinterval.On the other hand, Zhang et al (2006) lookedat MWEs in general investigating the semi-automated detection of MWE candidates intexts using error mining techniques and vali-dating them using a combination of the WorldWide Web as a corpus and some statistical mea-sures.
6248 sentences were then extracted fromthe BNC; these contained at least one of the 311MWE candidates verified with World Wide Webin the way described in Zhang et al (2006).
Foreach occurrence of the MWE candidates in thisset of sentences, the lexical type predictor pro-posed in Zhang and Kordoni (2006) predicted alexical entry candidate.
This resulted in 373 ad-ditional MWE lexical entries for the ERG gram-mar using a words-with-spaces approach.
As re-ported in Zhang et al (2006), this addition tothe grammar resulted in a significant increase ingrammar coverage of 14.4%.
However, no fur-ther evaluation was done of the results of themeasures used on the identification of MWEs orof the resulting grammar, as not all MWEs canbe correctly handled by the simple words-with-spaces approach (Sag et al, 2002).
And theseare the starting points of the work we are re-porting on here.5 Evaluation of the Identification ofMWEsOne way of viewing the MWE identification taskis, given a list of sequences of words, to distin-guish those that are genuine MWEs (e.g.
in thered), from those that are just sequences of wordsthat do not form any kind of meaningful unit(e.g.
of alcohol and).
In order to do that, onecommonly used approach is to employ statisti-cal measures (e.g.
Pearce (2002) for collocationsand Zhang et al (2006) for MWEs in general).When dealing with statistical analysis there aretwo important statistical questions that shouldbe addressed: How reliable is the corpus used?and How precise is the chosen statistical measureto distinguish the phenomena studied?.In this section we look at these issues, for theparticular case of trigrams, by testing differentcorpora and different statistical measures.
Forthat we use 1039 trigrams that are the outputof Zhang et al (2006) error mining system, andfrequencies collected from the BNC and fromthe World Wide Web.
The former were col-lected from two different portions of the BNC,namely the fragment of the BNC (BNCf ) usedin the error-mining experiments, and the com-plete BNC (from the site http://pie.usna.edu/),to test whether a larger sample of a more ho-mogeneous and well balanced corpus improvesresults significantly.
For the latter we used twodifferent search engines: Google and Yahoo, andthe frequencies collected reflect the number ofpages that had exact matches of the n-gramssearched, using the API tools for each engine.5.1 Comparing CorporaA corpus for NLP related work should be a re-liable sample of the linguistic output of a givenlanguage.
For this work in particular, we expectthat the relative ordering in frequency for differ-ent n-grams is preserved across corpora, in thesame domain (e.g.
a corpus of chemistry arti-cles).
For, if this is not the case, different con-clusions are certain to be drawn from differentcorpora.The first test we performed was a direct com-parison of the rank plots of the relative fre-quency of trigrams for the four corpora.
Weranked 1039 MWE-candidate trigrams accord-ing to their occurrence in each corpus and wenormalised this value by the total number oftimes any one of the 1039 trigrams appearedfor each corpus.
These normalisation valueswere: 66,101 times in BNCf , 322,325 in BNC,224,479,065 in Google and 6,081,786,313 in Ya-hoo.
It is possible to have an estimate of the sizeof each corpus from these numbers: the trigrams1036account for something like 0.3% of the BNC cor-pora, while for Google and Yahoo nothing canbe said since their sizes are not reliable numbers.Figure 1 displays the results.
The overall rank-ing distribution is very similar for these corporashowing the expected Zipf like behaviour in spiteof their different sizes.10-510-410-310-210-11 10 100 1000relativefrequencyrankBNCfBNCGoogleYahooFigure 1: Relative frequency rank for the 1039trigrams analysed.Of course, the information coming from Fig-ure 1 is not sufficient for our purposes.
The or-der of the trigrams could be very different insideeach corpus.
Therefore a second test is neededto compare the rankings of the n-grams in eachcorpus.
In order to do that we measure theKendall?s ?
scores between corpora.
Kendall?s ?is a non-parametric method for estimating cor-relation between datasets (Press et al, 1992).For the number of trigrams studied here theKendall?s scores obtained imply a significant cor-relation between the corpora with p<0.000001.The significance indicates that the data are cor-related and the null hypothesis of statisticalindependence is certainly disproved.
Unfortu-nately disproving the null hypothesis does notgive much information about the degree of cor-relation; it only asserts that it exists.
Thus, itcould be a very insignificant correlation.
In ta-ble 1, we display a more intuitive measure toestimate the correlation, the probability Q thatany 2 trigrams chosen from two corpora havethe same relative ordering in frequency.
Thisprobability is related to Kendall?s ?
through theexpression Q = (1 + ?
)/2 .BNC Google YahooBNCf 0.81 0.73 0.78BNC 0.73 0.77Google 0.86Table 1: The probability Q of 2 trigrams hav-ing the same frequency rank order for differentcorpora.The results show that the four corpora arecertainly correlated, and can probably be usedinterchangeably to access most of the statisti-cal properties of the trigrams.
Interestingly, ahigher correlation was observed between Yahooand Google than between BNCf and BNC, eventhough BNCf is a fragment of BNC, and there-fore would be expected to have a very high cor-relation.
This suggests that as corpora sizesincrease, so do the correlations between them,meaning that they are more likely to agree onthe ranking of a given MWE.5.2 Comparing statistical measures -are they equivalent?Here we concentrate on a single corpus, BNCf ,and compare the three statistical measures forMWE identification: Mutual Information (MI),?2 and Permutation Entropy (PE)(Zhang et al,2006), to investigate if they order the trigramsin the same fashion.MI and ?2 are typical measures of associa-tion that compare the joint probability of occur-rence of a certain group of events p(abc) witha prediction derived from the null hypothesisof statistical independence between these eventsp?
(abc) = p(a)p(b)p(c) (Press et al, 1992).
Inour case the events are the occurrences of wordsin a given position in an n-gram.
For a trigramwith words w1w2w3, ?2 is calculated as:?2 =?a,b,c[ n(abc)?
n?
(abc) ]2n?
(abc)where a corresponds either to the word w1 or to?w1 (all but the word w1) and so on.
n(abc)is the number of trigrams abc in the corpus,n?
(abc) = n(a)n(b)n(c)/N2 is the predictednumber from the null hypothesis, n(a) is the1037number of unigrams a, and N the number ofwords in the corpus.
Mutual Information, interms of these numbers, is:MI =?a,b,cn(abc)N log2[ n(abc)n?
(abc)]The third measure, permutation entropy, is ameasure of order association.
Given the wordsw1, w2, and w3, PE is calculated in this work as:PE = ??
(i,j,k)p(wiwjwk) ln [ p(wiwjwk) ]where the sum runs over all the permutationsof the indexes and, therefore, over all possiblepositions of the selected words in the trigram.The probabilities are estimated from the numberof occurrences of each permutation of a trigram(e.g.
by and large, large by and, and large by,and by large, large and by, and by large and) as:p(w1w2w3) =n(w1w2w3)?
(i,j,k)n(wiwjwk)PE was proposed by Zhang et al (2006) as apossible measure to detect MWEs, under thehypothesis that MWEs are more rigid to per-mutations and therefore present smaller PEs.Even though it is quite different from MI and?2, PE can also be thought as an indirect mea-sure of statistical independence, since the moreindependent the words are the closer PE is fromits maximal value (ln 6, for trigrams).
One pos-sible advantage of this measure over the othersis that it does not rely on single word counts,which are less accurate in Web based corpora.Given the rankings produced for each one ofthese three measures we again use Kendall?s ?test to assess correlation and its significance.Table 2 displays the Q probability of findingthe same ordering in these three measures.
Thegeneral conclusion from the table is that eventhough there is statistical significance in the cor-relations found (the p values are not displayed,but they are very low as before) the differ-ent measures order the trigrams very differently.There is a 70% chance of getting the same orderfrom MI and ?2, but it is safe to say that thesemeasures are very different from the PE, sincetheir Q values are very close to pure chance.MI?
?2 MI?PE ?2?PEQ 0.71 0.55 0.45Table 2: The probability Q of having 2 trigramswith the same rank order for different statisticalmeasures.5.3 Comparing Statistical Measures -are they useful?The use of statistical measures is widespread inNLP but there is no consensus about how goodthese measures are for describing natural lan-guage phenomena.
It is not clear what exactlythey capture when analysing the data.In order to evaluate if they would make goodpredictors for MWEs, we compare the measuresdistributions for MWEs and non-MWEs.
Forthat we selected as gold standard a set of around400 MWE candidates annotated by a nativespeaker1 as MWEs or not.
We then calculatedthe histograms for the values of MI, ?2 andPE for the two groups.
MI and ?2 were cal-culated only for BNCf .
Table 3 displays the re-sults of the Kolmogorov-Smirnof test (Press etal., 1992) for these histograms, where the firstvalue is Kolmogorov-Smirnov D value (D?
[0,1]and large D values indicate large differences be-tween distributions) and the second is the signif-icance probability (p) associated to D given thesizes of the data sets, in this case 90 for MWEsand 292 for non-MWEs.MIBNCf ?2BNCf PEY ahoo PEGoogleD 0.27 0.13 0.27 0.24p< 0.0001 0.154 0.0001 0.0005Table 3: Comparison of MI, ?2 and PEThe surprising result is that there is no statis-tical significance, at least using the Kolmogorov-Smirnov test, that indicates that being or notan MWE has some effect in the value of the tri-gram?s ?2.
The same does not happen for MIor PE.
They do seem to differentiate betweenMWEs and non-MWEs.
As discussed before thestatistical significance implies the existence of an1The native speaker is a linguist expert in MWEs.1038effect but has very little to say about the inten-sity of the effect.
As in the case of this work ourinterest is to use the effect to predict MWEs,the intensity is very important.
In the figuresthat follow we show the normalised histogramsfor MI, ?2(for the BNCf ) and PE (for the caseof Yahoo) for MWEs and non-MWEs.
The idealscenario would be to have non overlapping dis-tributions for the two cases, so a simple thresh-old operation would be enough to distinguishMWEs.
This is not the case in any of the plots.Starting from Figure 3 it clearly illustrates thenegative result for ?2 in table 3.
The other twodistributions show a visible effect in the form ofa slight displacement of the distributions to theleft for MWEs.
In particular for the distributionof PE, the large peak on the right, representingthe n-grams whose word order is irrelevant withrespect to its occurrence, has an important re-duction for MWEs.The statistical measures discussed here areall different forms of measuring correlations be-tween the component words of MWEs.
There-fore, as some types of MWEs may have strongerconstraints on word order, we believe that morevisible effects can be seen in these measures if welook at their application for individual types ofMWEs, which is planned for future work.
Thiswill bring an improvement to the power of MWEprediction of these measures.00.020.040.060.080.10.120.140.160.180.2-5.5 -5 -4.5 -4 -3.5 -3 -2.5 -2Probabilitylog(MI)MWEsnon-MWEsFigure 2: Normalised histograms of MI valuesfor MWEs and non-MWEs in BNCf .00.020.040.060.080.10.120.140.160.182  3  4  5  6  7  8Probabilitylog(?2)MWEsnon-MWEsFigure 3: Normalised histograms of ?2 valuesfor MWEs and non-MWEs in BNCf .00.050.10.150.20.25-3.5 -3 -2.5 -2 -1.5 -1 -0.5  0  0.5Probabilitylog(PE(Yahoo))MWEsnon-MWEsFigure 4: Normalised histograms of PE valuesfor MWEs and non-MWEs in Yahoo.6 Evaluation of the Extensions tothe GrammarOur ultimate goal is to maximally automatethe process of discovering and handling MWEs.With good statistical measures, we are ableto distinguish genuine MWE from non-MWEsamong the n-gram candidates.
However, fromthe perspective of grammar engineering, evenwith a good candidate list of MWEs, great ef-fort is still required in order to incorporate suchword units into a given grammar automaticallyand in a precise way.Zhang et al (2006) tried a simple ?word withspaces?
approach.
By acquiring new lexical en-tries for the MWEs candidates validated by thestatistical measures, the grammar coverage wasshown to improve significantly.
However, no fur-ther investigation on the parser accuracy was re-ported there.Taking a closer look at the MWE candidates1039proposed, we find that only a small proportion ofthem can be handled appropriately by the?wordwith spaces?
approach of Zhang et al (2006).Simply adding new lexical entries for all MWEscan be a workaround for enhancing the parsercoverage, but the quality of the parser output isclearly linguistically less interesting.On the other hand, we also find that a largeproportion of MWEs that cannot be correctlyhandled by the grammar can be covered prop-erly in a constructional way by adding one lex-ical entry for the head (governing) word of theMWE.
For example, the expression foot the billwill be correctly handled with a standard head-complement rule, if there is a transitive verbreading for the word foot in the lexicon.
Someother examples are: to put forward, the good of,in combination with, .
.
.
, where lexical exten-sion to the words in bold will allow the gram-mar to cover these MWEs.
In this paper, weemploy a constructional approach for the acqui-sition of new lexical entries for the head wordsof the MWEs.2It is arguable that such an approach may leadto some potential grammar overgeneration, asthere is no selectional restriction expressed inthe new lexical entry.
However, as far as theparsing task is concerned, such overgenerationis not likely to reduce the accuracy of the gram-mar significantly as we show later in this paperthrough a thorough evaluation.6.1 Experimental SetupWith the complete list of 1039 MWE candidatesdiscussed in section 5, we rank each n-gramaccording to each of the three statistical mea-sures.
The average of all the rankings is usedas the combined measure of the MWE candi-dates.
Since we are only interested in acquiringnew lexical entries for MWEs which are not cov-ered by the grammar, we used the error miningresults (Zhang et al, 2006; van Noord, 2004)to only keep those candidates with parsability?
0.1.
The top 30 MWE candidates are used in2The combination of the ?word with space?
approachof Zhang et al (2006) with the constructional approachwe propose here is an interesting topic that we want toinvestigate in future research.this experiment.We used simple heuristics in order to extractthe head words from these MWEs:?
the n-grams are POS-tagged with an auto-matic tagger;?
finite verbs in the n-grams are extracted ashead words;?
nouns are also extracted if there is no verbin the n-gram.Occasionally, the tagger errors might introducewrong head words.
However, the lexical typepredictor of Zhang and Kordoni (2006) that weused in our experiments did not generate inter-esting new entries for them in the subsequentsteps, and they were thus discarded, as discussedbelow.With the 30 MWE candidates, we extracteda sub-corpus from the BNC with 674 sentenceswhich included at least one of these MWEs.
Thelexical acquisition technique described in Zhangand Kordoni (2006) was used with this sub-corpus in order to acquire new lexical entries forthe head words.
The lexical acquisition modelwas trained with the Redwoods treebank (Oepenet al, 2002), following Zhang et al (2006).The lexical prediction model predicted foreach occurrence of the head words a most plau-sible lexical type in that context.
Only thosepredictions that occurred 5 times or more weretaken into consideration for the generation of thenew lexical entries.
As a result, we obtained 21new lexical entries.These new lexical entries were later mergedinto the ERG lexicon.
To evaluate the grammarperformance with and without these new lexicalentries, we1.
parsed the sub-corpus with/without newlexical entries and compared the grammarcoverage;2. inspected the parser output manually andevaluated the grammar accuracy.In parsing the sub-corpus, we used the PETparser (Callmeier, 2001).
For the manual eval-1040uation of the parser output, we used the tree-banking tools of the [incr tsdb()] system (Oepen,2001).6.2 Grammar PerformanceTable 4 shows that the grammar coverage im-proved significantly (from 7.1% to 22.7%) withthe acquired lexical entries for the head wordsof the MWEs.
This improvement in coverageis largely comparable to the result reported in(Zhang et al, 2006), where the coverage was re-ported to raise from 5% to 18% with the ?wordwith spaces?
approach (see also section 4).It is also worth mentioning that Zhang et al(2006) added 373 new lexical entries for a to-tal of 311 MWE candidates, with an averageof 1.2 entries per MWE.
In our experiment, weachieved a similar coverage improvement withonly 21 new entries for 30 different MWE candi-dates, with an average of 0.7 entries per MWE.This suggests that the lexical entries acquiredin our experiment are of much higher linguisticgenerality.To evaluate the grammar accuracy, we man-ually checked the parser outputs for the sen-tences in the sub-corpus which received at leastone analysis from the grammar before and af-ter the lexical extension.
Before the lexical ex-tension, 48 sentences are parsed, among which32 (66.7%) sentences contain at least one cor-rect reading (table 4).
After adding the 21 newlexical entries, 153 sentences are parsed, out ofwhich 124 (81.0%) sentences contain at least onecorrect reading.Baldwin et al (2004) reported in an earlierstudy that for BNC data, about 83% of the sen-tences covered by the ERG have a correct parse.In our experiment, we observed a much loweraccuracy on the sub-corpus of BNC which con-tains a lot of MWEs.
However, after the lexicalextension, the accuracy of the grammar recoversto the normal level.It is also worth noticing that we did not re-ceive a larger average number of analyses persentence (table 4), as it was largely balancedby the significant increase of sentences coveredby the new lexical entries.
We also foundthat the disambiguation model as described byToutanova et al (2002) performed reasonablywell, and the best analysis is ranked among top-5 for 66% of the cases, and top-10 for 75%.All of these indicate that our approach of lexi-cal acquisition for head words of MWEs achievesa significant improvement in grammar coveragewithout damaging the grammar accuracy.
Op-tionally, the grammar developers can check thevalidity of the lexical entries before they areadded into the lexicon.
Nonetheless, even asemi-automatic procedure like this can largelyreduce the manual work of grammar writers.7 ConclusionsIn this paper we looked at some of the issuesinvolved in the evaluation of the identificationof MWEs.
In particular we evaluated the useof three statistical measures for automaticallyidentifying MWEs.
The results suggest that atleast two of them (MI and PE) can distinguishMWEs.
In terms of the corpora used, a sur-prisingly higher level of agreement was foundbetween different corpora (Google and Yahoo)than between two fragments of the same one.This tells us two lessons.
First that even thoughGoogle and Yahoo were not carefully built to belanguage corpora their sizes compensate for thatmaking them fairly good samples of languageusage.
Second, a fraction of a smaller well bal-anced corpus may not necessarily be as balancedas the whole.Furthermore, we argued that for precise gram-mar engineering it is important to perform acareful evaluation of the effects of including au-tomatically acquired MWEs to a grammar.
Welooked at the evaluation of the effects in cover-age, size of the grammar and accuracy of theparses after adding the MWE-candidates.
Weadopted a compositional approach to the en-coding of MWEs, using some heuristics to de-tect the head of an MWE, and this resulted ina smaller grammar than that by Zhang et al(2006), still achieving a similar increase in cov-erage and maintaining a high level of accuracy ofparses, comparable to that reported by Baldwinet al (2004).The statistical measures are currently only1041item # parsed # avg.
analysis # coverage %ERG 674 48 335.08 7.1%ERG + MWE 674 153 285.01 22.7%Table 4: ERG coverage with/without lexical acquisition for the head words of MWEsused in a preprocessing step to filter the non-MWEs for the lexical type predictor.
Alterna-tively, the statistical outcomes can be incorpo-rated more tightly, i.e.
to combine with the lex-ical type predictor and give confidence scores onthe resulting lexical entries.
These possibilitieswill be explored in future work.ReferencesTimothy Baldwin and Aline Villavicencio.
2002.
Ex-tracting the unextractable: A case study on verb-particles.
In Proc.
of the 6th Conference on Nat-ural Language Learning (CoNLL-2002), Taipei,Taiwan.Timothy Baldwin, Emily M. Bender, Dan Flickinger,Ara Kim, and Stephan Oepen.
2004.
Road-testingthe English Resource Grammar over the BritishNational Corpus.
In Proceedings of the FourthInternational Conference on Language Resourcesand Evaluation (LREC 2004), Lisbon, Portugal.Gosse Bouma, Gertjan van Noord, and Robert Mal-ouf.
2001.
Alpino: Wide-coverage computationalanalysis of dutch.
In Computational Linguistics inThe Netherlands 2000.Ted Briscoe and John Carroll.
1997.
Automaticextraction of subcategorization from corpora.
InFifth Conference on Applied Natural LanguageProcessing, Washington, USA.Ulrich Callmeier.
2001.
Efficient parsing with large-scale unification grammars.
Master?s thesis, Uni-versita?t des Saarlandes, Saarbru?cken, Germany.Stefan Evert and Brigitte Krenn.
2005.
Using smallrandom samples for the manual evaluation of sta-tistical association measures.
Computer Speechand Language, 19(4):450?466.Dan Flickinger.
2000.
On building a more efficientgrammar by exploiting types.
Natural LanguageEngineering, 6(1):15?28.Ray Jackendoff.
1997.
Twistin?
the night away.
Lan-guage, 73:534?59.Jeremy Nicholson and Timothy Baldwin.
2006.
In-terpretation of compound nominalisations usingcorpus and web statistics.
In Proceedings of theWorkshop on Multiword Expressions: Identifyingand Exploiting Underlying Properties, pages 54?61, Sydney, Australia.
Association for Computa-tional Linguistics.Stephan Oepen, Kristina Toutanova, Stuart Shieber,Christopher Manning, Dan Flickinger, andThorsten Brants.
2002.
The LinGO Redwoodstreebank: Motivation and preliminary applica-tions.
In Proceedings of COLING 2002: The 17thInternational Conference on Computational Lin-guistics: Project Notes, Taipei.Stephan Oepen.
2001.
[incr tsdb()] ?
competenceand performance laboratory.
User manual.
Tech-nical report, Computational Linguistics, SaarlandUniversity, Saarbru?cken, Germany.Darren Pearce.
2002.
A comparative evaluation ofcollocation extraction techniques.
In Third Inter-national Conference on Language Resources andEvaluation, Las Palmas, Canary Islands, Spain.William H. Press, Saul A. Teukolsky, William T. Vet-terling, and Brian P. Flannery.
1992.
NumericalRecipes in C: The Art of Scientific Computing.Second edition.
Cambridge University Press.Ivan Sag, Timothy Baldwin, Francis Bond, AnnCopestake, and Dan Flickinger.
2002.
Multiwordexpressions: A pain in the neck for NLP.
In Pro-ceedings of the 3rd International Conference on In-telligent Text Processing and Computational Lin-guistics (CICLing-2002), pages 1?15, Mexico City,Mexico.Kristina Toutanova, Christoper D. Manning, Stu-art M. Shieber, Dan Flickinger, and StephanOepen.
2002.
Parse ranking for a rich HPSGgrammar.
In Proceedings of the First Workshopon Treebanks and Linguistic Theories (TLT2002),pages 253?263, Sozopol, Bulgaria.Leonoor van der Beek.
2005.
The extraction ofdeterminerless pps.
In Proceedings of the ACL-SIGSEM Workshop on The Linguistic Dimensionsof Prepositions and their Use in ComputationalLinguistics Formalisms and Applications, Colch-ester, UK.Gertjan van Noord.
2004.
Error mining for wide-coverage grammar engineering.
In Proceedings of1042the 42nd Meeting of the Association for Computa-tional Linguistics (ACL?04), Main Volume, pages446?453, Barcelona, Spain, July.Aline Villavicencio, Francis Bond, Anna Korhonen,and Diana McCarthy.
2005.
Introduction to thespecial issue on multiword expressions: having acrack at a hard nut.
Journal of Computer Speechand Language Processing, 19(4):365?377.Aline Villavicencio.
2005.
The availability of verb-particle constructions in lexical resources: Howmuch is enough?
Journal of Computer Speechand Language Processing, 19.Yi Zhang and Valia Kordoni.
2006.
Automated deeplexical acquisition for robust open texts process-ing.
In Proceedings of the Fifth InternationalConference on Language Resources and Evalua-tion (LREC 2006), Genoa, Italy.Yi Zhang, Valia Kordoni, Aline Villavicencio, andMarco Idiart.
2006.
Automated multiword ex-pression prediction for grammar engineering.
InProceedings of the Workshop on Multiword Ex-pressions: Identifying and Exploiting UnderlyingProperties, pages 36?44, Sydney, Australia.
Asso-ciation for Computational Linguistics.1043
