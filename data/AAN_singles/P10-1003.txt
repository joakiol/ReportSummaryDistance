Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 21?29,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsBitext Dependency Parsing with Bilingual Subtree ConstraintsWenliang Chen, Jun?ichi Kazama and Kentaro TorisawaLanguage Infrastructure Group, MASTAR ProjectNational Institute of Information and Communications Technology3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289{chenwl, kazama, torisawa}@nict.go.jpAbstractThis paper proposes a dependency parsingmethod that uses bilingual constraints toimprove the accuracy of parsing bilingualtexts (bitexts).
In our method, a target-side tree fragment that corresponds to asource-side tree fragment is identified viaword alignment and mapping rules thatare automatically learned.
Then it is ver-ified by checking the subtree list that iscollected from large scale automaticallyparsed data on the target side.
Our method,thus, requires gold standard trees only onthe source side of a bilingual corpus inthe training phase, unlike the joint parsingmodel, which requires gold standard treeson the both sides.
Compared to the re-ordering constraint model, which requiresthe same training data as ours, our methodachieved higher accuracy because of richerbilingual constraints.
Experiments on thetranslated portion of the Chinese Treebankshow that our system outperforms mono-lingual parsers by 2.93 points for Chineseand 1.64 points for English.1 IntroductionParsing bilingual texts (bitexts) is crucial for train-ing machine translation systems that rely on syn-tactic structures on either the source side or thetarget side, or the both (Ding and Palmer, 2005;Nakazawa et al, 2006).
Bitexts could providemore information, which is useful in parsing, thana usual monolingual texts that can be called ?bilin-gual constraints?, and we expect to obtain moreaccurate parsing results that can be effectivelyused in the training of MT systems.
With this mo-tivation, there are several studies aiming at highlyaccurate bitext parsing (Smith and Smith, 2004;Burkett and Klein, 2008; Huang et al, 2009).This paper proposes a dependency parsingmethod, which uses the bilingual constraints thatwe call bilingual subtree constraints and statisticsconcerning the constraints estimated from largeunlabeled monolingual corpora.
Basically, a (can-didate) dependency subtree in a source-languagesentence is mapped to a subtree in the correspond-ing target-language sentence by using word align-ment and mapping rules that are automaticallylearned.
The target subtree is verified by check-ing the subtree list that is collected from unla-beled sentences in the target language parsed bya usual monolingual parser.
The result is used asadditional features for the source side dependencyparser.
In this paper, our task is to improve thesource side parser with the help of the translationson the target side.Many researchers have investigated the useof bilingual constraints for parsing (Burkett andKlein, 2008; Zhao et al, 2009; Huang et al,2009).
For example, Burkett and Klein (2008)show that parsing with joint models on bitexts im-proves performance on either or both sides.
How-ever, their methods require that the training datahave tree structures on both sides, which are hardto obtain.
Our method only requires dependencyannotation on the source side and is much sim-pler and faster.
Huang et al (2009) proposes amethod, bilingual-constrained monolingual pars-ing, in which a source-language parser is extendedto use the re-ordering of words between two sides?sentences as additional information.
The input oftheir method is the source trees with their trans-lation on the target side as ours, which is mucheasier to obtain than trees on both sides.
However,their method does not use any tree structures on21the target side that might be useful for ambiguityresolution.
Our method achieves much greater im-provement because it uses the richer subtree con-straints.Our approach takes the same input as Huanget al (2009) and exploits the subtree structure onthe target side to provide the bilingual constraints.The subtrees are extracted from large-scale auto-parsed monolingual data on the target side.
Themain problem to be addressed is mapping wordson the source side to the target subtree becausethere are many to many mappings and reorderingproblems that often occur in translation (Koehn etal., 2003).
We use an automatic way for generat-ing mapping rules to solve the problems.
Basedon the mapping rules, we design a set of featuresfor parsing models.
The basic idea is as follows: ifthe words form a subtree on one side, their corre-sponding words on the another side will also prob-ably form a subtree.Experiments on the translated portion of theChinese Treebank (Xue et al, 2002; Bies et al,2007) show that our system outperforms state-of-the-art monolingual parsers by 2.93 points for Chi-nese and 1.64 points for English.
The results alsoshow that our system provides higher accuraciesthan the parser of Huang et al (2009).The rest of the paper is organized as follows:Section 2 introduces the motivation of our idea.Section 3 introduces the background of depen-dency parsing.
Section 4 proposes an approachof constructing bilingual subtree constraints.
Sec-tion 5 explains the experimental results.
Finally, inSection 6 we draw conclusions and discuss futurework.2 MotivationIn this section, we use an example to show theidea of using the bilingual subtree constraints toimprove parsing performance.Suppose that we have an input sentence pair asshown in Figure 1, where the source sentence is inEnglish, the target is in Chinese, the dashed undi-rected links are word alignment links, and the di-rected links between words indicate that they havea (candidate) dependency relation.In the English side, it is difficult for a parser todetermine the head of word ?with?
because thereis a PP-attachment problem.
However, in Chineseit is unambiguous.
Therefore, we can use the in-formation on the Chinese side to help disambigua-He  ate    the    meat with     a    fork    .?
(He) ?
(use) ??
(fork) ?
(eat) ?
(meat) ?(.
)Figure 1: Example for disambiguationtion.There are two candidates ?ate?
and ?meat?
to bethe head of ?with?
as the dashed directed links inFigure 1 show.
By adding ?fork?, we have twopossible dependency relations, ?meat-with-fork?and ?ate-with-fork?, to be verified.First, we check the possible relation of ?meat?,?with?, and ?fork?.
We obtain their correspondingwords ??
(meat)?, ??
(use)?, and ???(fork)?
inChinese via the word alignment links.
We ver-ify that the corresponding words form a subtreeby looking up a subtree list in Chinese (describedin Section 4.1).
But we can not find a subtree forthem.Next, we check the possible relation of ?ate?,?with?, and ?fork?.
We obtain their correspond-ing words ??
(ate)?, ??
(use)?, and ???
(fork)?.Then we verify that the words form a subtree bylooking up the subtree list.
This time we can findthe subtree as shown in Figure 2.?
(use) ??
(fork) ?
(eat)Figure 2: Example for a searched subtreeFinally, the parser may assign ?ate?
to be thehead of ?with?
based on the verification results.This simple example shows how to use the subtreeinformation on the target side.3 Dependency parsingFor dependency parsing, there are two main typesof parsing models (Nivre and McDonald, 2008;Nivre and Kubler, 2006): transition-based (Nivre,2003; Yamada and Matsumoto, 2003) and graph-based (McDonald et al, 2005; Carreras, 2007).Our approach can be applied to both parsing mod-els.In this paper, we employ the graph-based MSTparsing model proposed by McDonald and Pereira22(2006), which is an extension of the projec-tive parsing algorithm of Eisner (1996).
To usericher second-order information, we also imple-ment parent-child-grandchild features (Carreras,2007) in the MST parsing algorithm.3.1 Parsing with monolingual featuresFigure 3 shows an example of dependency pars-ing.
In the graph-based parsing model, features arerepresented for all the possible relations on singleedges (two words) or adjacent edges (three words).The parsing algorithm chooses the tree with thehighest score in a bottom-up fashion.ROOT     He  ate    the    meat   with     a    fork    .Figure 3: Example of dependency treeIn our systems, the monolingual features in-clude the first- and second- order features pre-sented in (McDonald et al, 2005; McDonaldand Pereira, 2006) and the parent-child-grandchildfeatures used in (Carreras, 2007).
We call theparser with the monolingual features monolingualparser.3.2 Parsing with bilingual featuresIn this paper, we parse source sentences with thehelp of their translations.
A set of bilingual fea-tures are designed for the parsing model.3.2.1 Bilingual subtree featuresWe design bilingual subtree features, as describedin Section 4, based on the constraints between thesource subtrees and the target subtrees that are ver-ified by the subtree list on the target side.
Thesource subtrees are from the possible dependencyrelations.3.2.2 Bilingual reordering featureHuang et al (2009) propose features based onreordering between languages for a shift-reduceparser.
They define the features based on word-alignment information to verify that the corre-sponding words form a contiguous span for resolv-ing shift-reduce conflicts.
We also implement sim-ilar features in our system.4 Bilingual subtree constraintsIn this section, we propose an approach that usesthe bilingual subtree constraints to help parsesource sentences that have translations on the tar-get side.We use large-scale auto-parsed data to obtainsubtrees on the target side.
Then we generate themapping rules to map the source subtrees onto theextracted target subtrees.
Finally, we design thebilingual subtree features based on the mappingrules for the parsing model.
These features in-dicate the information of the constraints betweenbilingual subtrees, that are called bilingual subtreeconstraints.4.1 Subtree extractionChen et al (2009) propose a simple method to ex-tract subtrees from large-scale monolingual dataand use them as features to improve monolingualparsing.
Following their method, we parse largeunannotated data with a monolingual parser andobtain a set of subtrees (STt) in the target lan-guage.We encode the subtrees into string format that isexpressed as st = w : hid(?w : hid)+1, where wrefers to a word in the subtree and hid refers to theword ID of the word?s head (hid=0 means that thisword is the root of a subtree).
Here, word ID refersto the ID (starting from 1) of a word in the subtree(words are ordered based on the positions of theoriginal sentence).
For example, ?He?
and ?ate?have a left dependency arc in the sentence shownin Figure 3.
The subtree is encoded as ?He:2-ate:0?.
There is also a parent-child-grandchild re-lation among ?ate?, ?with?, and ?fork?.
So thesubtree is encoded as ?ate:0-with:1-fork:2?.
If asubtree contains two nodes, we call it a bigram-subtree.
If a subtree contains three nodes, we callit a trigram-subtree.From the dependency tree of Figure 3, we ob-tain the subtrees, as shown in Figure 4 and Figure5.
Figure 4 shows the extracted bigram-subtreesand Figure 5 shows the extracted trigram-subtrees.After extraction, we obtain a set of subtrees.
Weremove the subtrees occurring only once in thedata.
Following Chen et al (2009), we also groupthe subtrees into different sets based on their fre-quencies.1+ refers to matching the preceding element one or moretimes and is the same as a regular expression in Perl.23ateHe He:1:2-ate:2:0atemeat ate:1:0-meat:2:1atewith ate:1:0-with:2:1meatthe the:1:2-meat:2:0withfork with:1:0-fork:2:1forka a:1:2-fork:2:0Figure 4: Examples of bigram-subtreesatemeat  with ate:1:0-meat:2:1-with:3:1 atewith   .
ate:1:0-with:2:1-.:3:1(a)He:1:3-NULL:2:3-ate:3:0ateHe  NULL ateNULL  meat ate:1:0-NULL:2:1-meat:3:1the:1:3-NULL:2:3-meat:3:0a:1:3-NULL:2:3-fork:3:0 with:1:0-NULL:2:1-fork:3:1ate:1:0-the:2:3-meat:3:1 ate:1:0-with:2:1-fork:3:2with:1:0-a:2:3-fork:3:1 NULL:1:2-He:2:3-ate:3:0He:1:3-NULL:2:1-ate:3:0 ate:1:0-meat:2:1-NULL:3:2ate:1:0-NULL:2:3-with:3:1 with:1:0-fork:2:1-NULL:3:2NULL:1:2-a:2:3-fork:3:0 a:1:3-NULL:2:1-fork:3:0ate:1:0-NULL:2:3-.:3:1 ate:1:0-.:2:1-NULL:3:2(b)NULL:1:2-the:2:3-meat:3:0 the:1:3-NULL:2:1-meat:3:0Figure 5: Examples of trigram-subtrees4.2 Mapping rulesTo provide bilingual subtree constraints, we needto find the characteristics of subtree mapping forthe two given languages.
However, subtree map-ping is not easy.
There are two main problems:MtoN (words) mapping and reordering, which of-ten occur in translation.
MtoN (words) map-ping means that a source subtree with M wordsis mapped onto a target subtree with N words.
Forexample, 2to3 means that a source bigram-subtreeis mapped onto a target trigram-subtree.Due to the limitations of the parsing algo-rithm (McDonald and Pereira, 2006; Carreras,2007), we only use bigram- and trigram-subtreesin our approach.
We generate the mapping rulesfor the 2to2, 2to3, 3to3, and 3to2 cases.
Fortrigram-subtrees, we only consider the parent-child-grandchild type.
As for the use of othertypes of trigram-subtrees, we leave it for futurework.We first show the MtoN and reordering prob-lems by using an example in Chinese-Englishtranslation.
Then we propose a method to auto-matically generate mapping rules.4.2.1 Reordering and MtoN mapping intranslationBoth Chinese and English are classified as SVOlanguages because verbs precede objects in simplesentences.
However, Chinese has many character-istics of such SOV languages as Japanese.
Thetypical cases are listed below:1) Prepositional phrases modifying a verb pre-cede the verb.
Figure 6 shows an example.
In En-glish the prepositional phrase ?at the ceremony?follows the verb ?said?, while its correspondingprepositional phrase ??(NULL)??(ceremony)?(at)?
precedes the verb ??(say)?
in Chinese.?
??
?
?Said at the ceremonyFigure 6: Example for prepositional phrases mod-ifying a verb2) Relative clauses precede head noun.
Fig-ure 7 shows an example.
In Chinese the relativeclause ???
(today) ??(signed)?
precedes thehead noun ???
(project)?, while its correspond-ing clause ?signed today?
follows the head noun?projects?
in English.??
??
?
?
?
?
?The 3 projects signed todayFigure 7: Example for relative clauses precedingthe head noun3) Genitive constructions precede head noun.For example, ???
(car) ??(wheel)?
can betranslated as ?the wheel of the car?.4) Postposition in many constructions ratherthan prepositions.
For example, ???(table)?(on)?
can be translated as ?on the table?.24We can find the MtoN mapping problem occur-ring in the above cases.
For example, in Figure 6,trigram-subtree ??(NULL):3-?(at):1-?
(say):0?is mapped onto bigram-subtree ?said:0-at:1?.Since asking linguists to define the mappingrules is very expensive, we propose a simplemethod to easily obtain the mapping rules.4.2.2 Bilingual subtree mappingTo solve the mapping problems, we use a bilingualcorpus, which includes sentence pairs, to automat-ically generate the mapping rules.
First, the sen-tence pairs are parsed by monolingual parsers onboth sides.
Then we perform word alignment us-ing a word-level aligner (Liang et al, 2006; DeN-ero and Klein, 2007).
Figure 8 shows an exampleof a processed sentence pair that has tree structureson both sides and word alignment links.ROOT    ??
??
??
??
?ROOT    They   are   on   the   fringes   of   society   .Figure 8: Example of auto-parsed bilingual sen-tence pairFrom these sentence pairs, we obtain subtreepairs.
First, we extract a subtree (sts) from asource sentence.
Then through word alignmentlinks, we obtain the corresponding words of thewords of sts.
Because of the MtoN problem, somewords lack of corresponding words in the targetsentence.
Here, our approach requires that at leasttwo words of sts have corresponding words andnouns and verbs need corresponding words.
If not,it fails to find a subtree pair for sts.
If the corre-sponding words form a subtree (stt) in the targetsentence, sts and stt are a subtree pair.
We alsokeep the word alignment information in the tar-get subtree.
For example, we extract subtree ???(society):2-??(fringe):0?
on the Chinese sideand get its corresponding subtree ?fringes(W 2):0-of:1-society(W 1):2?
on the English side, whereW 1 means that the target word is aligned to thefirst word of the source subtree, and W 2 meansthat the target word is aligned to the second wordof the source subtree.
That is, we have a sub-tree pair: ???(society):2-??(fringe):0?
and?fringe(W 2):0-of:1-society(W 1):2?.The extracted subtree pairs indicate the trans-lation characteristics between Chinese and En-glish.
For example, the pair ???(society):2-?
?(fringe):0?
and ?fringes:0-of:1-society:2?is a case where ?Genitive constructions pre-cede/follow the head noun?.4.2.3 Generalized mapping rulesTo increase the mapping coverage, we general-ize the mapping rules from the extracted sub-tree pairs by using the following procedure.
Therules are divided by ?=>?
into two parts: source(left) and target (right).
The source part isfrom the source subtree and the target part isfrom the target subtree.
For the source part,we replace nouns and verbs using their POStags (coarse grained tags).
For the target part,we use the word alignment information to rep-resent the target words that have correspond-ing source words.
For example, we have thesubtree pair: ???(society):2-??
(fringe):0?and ?fringes(W 2):0-of:1-society(W 1):2?, where?of?
does not have a corresponding word, the POStag of ???(society)?
is N, and the POS tag of???(fringe)?
is N. The source part of the rulebecomes ?N:2-N:0?
and the target part becomes?W 2:0-of:1-W 1:2?.Table 1 shows the top five mapping rules ofall four types ordered by their frequencies, whereW 1 means that the target word is aligned to thefirst word of the source subtree, W 2 means thatthe target word is aligned to the second word, andW 3 means that the target word is aligned to thethird word.
We remove the rules that occur lessthan three times.
Finally, we obtain 9,134 rulesfor 2to2, 5,335 for 2to3, 7,450 for 3to3, and 1,244for 3to2 from our data.
After experiments with dif-ferent threshold settings on the development datasets, we use the top 20 rules for each type in ourexperiments.The generalized mapping rules might generateincorrect target subtrees.
However, as described inSection 4.3.1, the generated subtrees are verifiedby looking up list STt before they are used in theparsing models.4.3 Bilingual subtree featuresInformally, if the words form a subtree on thesource side, then the corresponding words on thetarget side will also probably form a subtree.
For25# rules freq2to2 mapping1 N:2 N:0 =>W 1:2 W 2:0 927762 V:0 N:1 =>W 1:0 W 2:1 624373 V:0 V:1 =>W 1:0 W 2:1 496334 N:2 V:0 =>W 1:2 W 2:0 439995 ?
:2 N:0 =>W 2:0 W 1:2 253012to3 mapping1 N:2-N:0 =>W 2:0-of:1-W 1:2 103612 V:0-N:1 =>W 1:0-of:1-W 2:2 45213 V:0-N:1 =>W 1:0-to:1-W 2:2 29174 N:2-V:0 =>W 2:0-of:1-W 1:2 25785 N:2-N:0 =>W 1:2-?
:3-W 2:0 23163to2 mapping1 V:2-?/DEC:3-N:0 =>W 1:0-W 3:1 8732 V:2-?/DEC:3-N:0 =>W 3:2-W 1:0 6343 N:2-?/DEG:3-N:0 =>W 1:0-W 3:1 3194 N:2-?/DEG:3-N:0 =>W 3:2-W 1:0 3015 V:0-?/DEG:3-N:1 =>W 3:0-W 1:1 2473to3 mapping1 V:0-V:1-N:2 =>W 1:0-W 2:1-W 3:2 95802 N:2-?/DEG:3-N:0 =>W 3:0-W 2:1-W 1:2 70103 V:0-N:3-N:1 =>W 1:0-W 2:3-W 3:1 56424 V:0-V:1-V:2 =>W 1:0-W 2:1-W 3:2 45635 N:2-N:3-N:0 =>W 1:2-W 2:3-W 3:0 3570Table 1: Top five mapping rules of 2to3 and 3to2example, in Figure 8, words ???(they)?
and???
(be on)?
form a subtree , which is mappedonto the words ?they?
and ?are?
on the target side.These two target words form a subtree.
We nowdevelop this idea as bilingual subtree features.In the parsing process, we build relations fortwo or three words on the source side.
The con-ditions of generating bilingual subtree features arethat at least two of these source words must havecorresponding words on the target side and nounsand verbs must have corresponding words.At first, we have a possible dependency relation(represented as a source subtree) of words to beverified.
Then we obtain the corresponding targetsubtree based on the mapping rules.
Finally, weverify that the target subtree is included in STt.
Ifyes, we activate a positive feature to encourage thedependency relation.?
?
??
??
?
?
?
?
?Those are the 3 projects signed todayFigure 9: Example of features for parsingWe consider four types of features based on2to2, 3to3, 3to2, and 2to3 mappings.
In the 2to2,3to3, and 3to2 cases, the target subtrees do not addnew words.
We represent features in a direct way.For the 2to3 case, we represent features using adifferent strategy.4.3.1 Features for 2to2, 3to3, and 3to2We design the features based on the mappingrules of 2to2, 3to3, and 3to2.
For example, wedesign features for a 3to2 case from Figure 9.The possible relation to be verified forms sourcesubtree ???(signed)/VV:2-?(NULL)/DEC:3-??(project)/NN:0?
in which ???
(project)?is aligned to ?projects?
and ???(signed)?
isaligned to ?signed?
as shown in Figure 9.
Theprocedure of generating the features is shown inFigure 10.
We explain Steps (1), (2), (3), and (4)as follows:??/VV:2-?/DEC:3-?
?/NN:0projects(W_3) signed(W_1)(1)V:2-?/DEC:3-N:0W_3:0-W_1:1W 3:2 W 1:0(2)_ - _(3)projects:0-signed:1projects:2-signed:0 STt(4)3to2:YESFigure 10: Example of feature generation for 3to2case(1) Generate source part from the sourcesubtree.
We obtain ?V:2-?/DEC:3-N:0?
from??
?(signed)/VV:2-?(NULL)/DEC:3-??(project)/NN:0?.
(2) Obtain target parts based on the matchedmapping rules, whose source parts equal?V:2-?/DEC:3-N:0?.
The matched rules are?V:2-?/DEC:3-N:0 =>W 3:0-W 1:1?
and?V:2-?/DEC:3-N:0 => W 3:2-W 1:0?.
Thus,we have two target parts ?W 3:0-W 1:1?
and?W 3:2-W 1:0?.
(3) Generate possible subtrees by consider-26ing the dependency relation indicated in thetarget parts.
We generate a possible subtree?projects:0-signed:1?
from the target part ?W 3:0-W 1:1?, where ?projects?
is aligned to ???
(project)(W 3)?
and ?signed?
is aligned to ???
(signed)(W 1)?.
We also generate another pos-sible subtree ?projects:2-signed:0?
from ?W 3:2-W 1:0?.
(4) Verify that at least one of the generatedpossible subtrees is a target subtree, which is in-cluded in STt.
If yes, we activate this feature.
Inthe figure, ?projects:0-signed:1?
is a target subtreein STt.
So we activate the feature ?3to2:YES?to encourage dependency relations among ???
(signed)?, ??
(NULL)?, and ???
(project)?.4.3.2 Features for 2to3In the 2to3 case, a new word is added on the targetside.
The first two steps are identical as those inthe previous section.
For example, a source part?N:2-N:0?
is generated from ???(car)/NN:2-??(wheel)/NN:0?.
Then we obtain target partssuch as ?W 2:0-of/IN:1-W 1:2?, ?W 2:0-in/IN:1-W 1:2?, and so on, according to the matched map-ping rules.The third step is different.
In the target parts,there is an added word.
We first check if the addedword is in the span of the corresponding words,which can be obtained through word alignmentlinks.
We can find that ?of?
is in the span ?wheelof the car?, which is the span of the correspondingwords of ???(car)/NN:2-??
(wheel)/NN:0?.Then we choose the target part ?W 2:0-of/IN:1-W 1:2?
to generate a possible subtree.
Finally,we verify that the subtree is a target subtree in-cluded in STt.
If yes, we say feature ?2to3:YES?to encourage a dependency relation between ???(car)?
and ???
(wheel)?.4.4 Source subtree featuresChen et al (2009) shows that the source sub-tree features (Fsrc?st) significantly improve per-formance.
The subtrees are obtained from theauto-parsed data on the source side.
Then they areused to verify the possible dependency relationsamong source words.In our approach, we also use the same sourcesubtree features described in Chen et al (2009).So the possible dependency relations are verifiedby the source and target subtrees.
Combining twotypes of features together provides strong discrim-ination power.
If both types of features are ac-tive, building relations is very likely among sourcewords.
If both are inactive, this is a strong negativesignal for their relations.5 ExperimentsAll the bilingual data were taken from the trans-lated portion of the Chinese Treebank (CTB)(Xue et al, 2002; Bies et al, 2007), articles1-325 of CTB, which have English translationswith gold-standard parse trees.
We used the tool?Penn2Malt?2 to convert the data into dependencystructures.
Following the study of Huang et al(2009), we used the same split of this data: 1-270for training, 301-325 for development, and 271-300 for test.
Note that some sentence pairs wereremoved because they are not one-to-one alignedat the sentence level (Burkett and Klein, 2008;Huang et al, 2009).
Word alignments were gen-erated from the Berkeley Aligner (Liang et al,2006; DeNero and Klein, 2007) trained on a bilin-gual corpus having approximately 0.8M sentencepairs.
We removed notoriously bad links in {a,an, the}?{?(DE),?
(LE)} following the work ofHuang et al (2009).For Chinese unannotated data, we used theXIN CMN portion of Chinese Gigaword Version2.0 (LDC2009T14) (Huang, 2009), which has ap-proximately 311 million words whose segmenta-tion and POS tags are given.
To avoid unfair com-parison, we excluded the sentences of the CTBdata from the Gigaword data.
We discarded the an-notations because there are differences in annota-tion policy between CTB and this corpus.
We usedthe MMA system (Kruengkrai et al, 2009) trainedon the training data to perform word segmentationand POS tagging and used the Baseline Parser toparse all the sentences in the data.
For Englishunannotated data, we used the BLLIP corpus thatcontains about 43 million words of WSJ text.
ThePOS tags were assigned by the MXPOST taggertrained on training data.
Then we used the Base-line Parser to parse all the sentences in the data.We reported the parser quality by the unlabeledattachment score (UAS), i.e., the percentage of to-kens (excluding all punctuation tokens) with cor-rect HEADs.5.1 Main resultsThe results on the Chinese-source side are shownin Table 2, where ?Baseline?
refers to the systems2http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html27with monolingual features, ?Baseline2?
refers toadding the reordering features to the Baseline,?FBI?
refers to adding all the bilingual subtreefeatures to ?Baseline2?, ?Fsrc?st?
refers to themonolingual parsing systems with source subtreefeatures, ?Order-1?
refers to the first-order mod-els, and ?Order-2?
refers to the second-order mod-els.
The results showed that the reordering fea-tures yielded an improvement of 0.53 and 0.58points (UAS) for the first- and second-order mod-els respectively.
Then we added four types ofbilingual constraint features one by one to ?Base-line2?.
Note that the features based on 3to2 and3to3 can not be applied to the first-order models,because they only consider single dependencies(bigram).
That is, in the first model, FBI only in-cludes the features based on 2to2 and 2to3.
Theresults showed that the systems performed betterand better.
In total, we obtained an absolute im-provement of 0.88 points (UAS) for the first-ordermodel and 1.36 points for the second-order modelby adding all the bilingual subtree features.
Fi-nally, the system with all the features (OURS) out-performed the Baseline by an absolute improve-ment of 3.12 points for the first-order model and2.93 points for the second-order model.
The im-provements of the final systems (OURS) were sig-nificant in McNemar?s Test (p < 10?4).Order-1 Order-2Baseline 84.35 87.20Baseline2 84.88 87.78+2to2 85.08 88.07+2to3 85.23 88.14+3to3 ?
88.29+3to2 ?
88.56FBI 85.23(+0.88) 88.56(+1.36)Fsrc?st 86.54(+2.19) 89.49(+2.29)OURS 87.47(+3.12) 90.13(+2.93)Table 2: Dependency parsing results of Chinese-source caseWe also conducted experiments on the English-source side.
Table 3 shows the results, where ab-breviations are the same as in Table 2.
As in theChinese experiments, the parsers with bilingualsubtree features outperformed the Baselines.
Fi-nally, the systems (OURS) with all the featuresoutperformed the Baselines by 1.30 points for thefirst-order model and 1.64 for the second-ordermodel.
The improvements of the final systems(OURS) were significant in McNemar?s Test (p <10?3).Order-1 Order-2Baseline 86.41 87.37Baseline2 86.86 87.66+2to2 87.23 87.87+2to3 87.35 87.96+3to3 ?
88.25+3to2 ?
88.37FBI 87.35(+0.94) 88.37(+1.00)Fsrc?st 87.25(+0.84) 88.57(+1.20)OURS 87.71(+1.30) 89.01(+1.64)Table 3: Dependency parsing results of English-source case5.2 Comparative resultsTable 4 shows the performance of the system wecompared, where Huang2009 refers to the result ofHuang et al (2009).
The results showed that oursystem performed better than Huang2009.
Com-pared with the approach of Huang et al (2009),our approach used additional large-scale auto-parsed data.
We did not compare our system withthe joint model of Burkett and Klein (2008) be-cause they reported the results on phrase struc-tures.Chinese EnglishHuang2009 86.3 87.5Baseline 87.20 87.37OURS 90.13 89.01Table 4: Comparative results6 ConclusionWe presented an approach using large automati-cally parsed monolingual data to provide bilingualsubtree constraints to improve bitexts parsing.
Ourapproach remains the efficiency of monolingualparsing and exploits the subtree structure on thetarget side.
The experimental results show that theproposed approach is simple yet still provides sig-nificant improvements over the baselines in pars-ing accuracy.
The results also show that our sys-tems outperform the system of previous work onthe same data.There are many ways in which this researchcould be continued.
First, we may attempt to ap-ply the bilingual subtree constraints to transition-28based parsing models (Nivre, 2003; Yamada andMatsumoto, 2003).
Here, we may design new fea-tures for the models.
Second, we may apply theproposed method for other language pairs such asJapanese-English and Chinese-Japanese.
Third,larger unannotated data can be used to improve theperformance further.ReferencesAnn Bies, Martha Palmer, Justin Mott, and ColinWarner.
2007.
English Chinese translation treebankv 1.0.
In LDC2007T02.David Burkett and Dan Klein.
2008.
Two languagesare better than one (for syntactic parsing).
In Pro-ceedings of the 2008 Conference on Empirical Meth-ods in Natural Language Processing, pages 877?886, Honolulu, Hawaii, October.
Association forComputational Linguistics.X.
Carreras.
2007.
Experiments with a higher-orderprojective dependency parser.
In Proceedings ofthe CoNLL Shared Task Session of EMNLP-CoNLL2007, pages 957?961.WL.
Chen, J. Kazama, K. Uchimoto, and K. Torisawa.2009.
Improving dependency parsing with subtreesfrom auto-parsed data.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, pages 570?579, Singapore, Au-gust.
Association for Computational Linguistics.John DeNero and Dan Klein.
2007.
Tailoring wordalignments to syntactic machine translation.
In Pro-ceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 17?24,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Yuan Ding and Martha Palmer.
2005.
Machine trans-lation using probabilistic synchronous dependencyinsertion grammars.
In ACL ?05: Proceedings of the43rd Annual Meeting on Association for Computa-tional Linguistics, pages 541?548, Morristown, NJ,USA.
Association for Computational Linguistics.J.
Eisner.
1996.
Three new probabilistic models fordependency parsing: An exploration.
In Proc.
ofthe 16th Intern.
Conf.
on Computational Linguistics(COLING), pages 340?345.Liang Huang, Wenbin Jiang, and Qun Liu.
2009.Bilingually-constrained (monolingual) shift-reduceparsing.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Process-ing, pages 1222?1231, Singapore, August.
Associ-ation for Computational Linguistics.Chu-Ren Huang.
2009.
Tagged Chinese GigawordVersion 2.0, LDC2009T14.
Linguistic Data Con-sortium.P.
Koehn, F.J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proceedings of NAACL,page 54.
Association for Computational Linguistics.Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichiKazama, Yiou Wang, Kentaro Torisawa, and HitoshiIsahara.
2009.
An error-driven word-character hy-brid model for joint Chinese word segmentation andPOS tagging.
In Proceedings of ACL-IJCNLP2009,pages 513?521, Suntec, Singapore, August.
Associ-ation for Computational Linguistics.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of the HumanLanguage Technology Conference of the NAACL,Main Conference, pages 104?111, New York City,USA, June.
Association for Computational Linguis-tics.R.
McDonald and F. Pereira.
2006.
Online learningof approximate dependency parsing algorithms.
InProc.
of EACL2006.R.
McDonald, K. Crammer, and F. Pereira.
2005.
On-line large-margin training of dependency parsers.
InProc.
of ACL 2005.T.
Nakazawa, K. Yu, D. Kawahara, and S. Kurohashi.2006.
Example-based machine translation based ondeeper nlp.
In Proceedings of IWSLT 2006, pages64?70, Kyoto, Japan.J.
Nivre and S. Kubler.
2006.
Dependency parsing:Tutorial at Coling-ACL 2006.
In CoLING-ACL.J.
Nivre and R. McDonald.
2008.
Integrating graph-based and transition-based dependency parsers.
InProceedings of ACL-08: HLT, Columbus, Ohio,June.J.
Nivre.
2003.
An efficient algorithm for projectivedependency parsing.
In Proceedings of IWPT2003,pages 149?160.David A. Smith and Noah A. Smith.
2004.
Bilingualparsing with factored estimation: Using English toparse Korean.
In Proceedings of EMNLP.Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.2002.
Building a large-scale annotated Chinese cor-pus.
In Coling.H.
Yamada and Y. Matsumoto.
2003.
Statistical de-pendency analysis with support vector machines.
InProceedings of IWPT2003, pages 195?206.Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.2009.
Cross language dependency parsing us-ing a bilingual lexicon.
In Proceedings of ACL-IJCNLP2009, pages 55?63, Suntec, Singapore, Au-gust.
Association for Computational Linguistics.29
