Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 399?406,Sydney, July 2006. c?2006 Association for Computational LinguisticsEfficient sentence retrieval based on syntactic structureIchikawa Hiroshi, Hakoda Keita, Hashimoto Taiichi and Tokunaga TakenobuDepartment of Computer Science, Tokyo Institute of Technology{ichikawa,hokoda,taiichi,take}@cl.cs.titech.ac.jpAbstractThis paper proposes an efficient methodof sentence retrieval based on syntacticstructure.
Collins proposed Tree Kernelto calculate structural similarity.
However,structual retrieval based on Tree Kernelis not practicable because the size of theindex table by Tree Kernel becomes im-practical.
We propose more efficient al-gorithms approximating Tree Kernel: TreeOverlapping and Subpath Set.
These algo-rithms are more efficient than Tree Kernelbecause indexing is possible with practicalcomputation resources.
The results of theexperiments comparing these three algo-rithms showed that structural retrieval withTree Overlapping and Subpath Set werefaster than that with Tree Kernel by 100times and 1,000 times respectively.1 IntroductionRetrieving similar sentences has attracted muchattention in recent years, and several methodshave been already proposed.
They are useful formany applications such as information retrievaland machine translation.
Most of the methodsare based on frequencies of surface informationsuch as words and parts of speech.
These methodsmight work well concerning similarity of topics orcontents of sentences.
Although the surface infor-mation of two sentences is similar, their syntacticstructures can be completely different (Figure 1).If a translation system regards these sentences assimilar, the translation would fail.
This is becauseconventional retrieval techniques exploit only sim-ilarity of surface information such as words andparts-of-speech, but not more abstract informationsuch as syntactic structures.He beats a dog with aV DET NN P DETNPPPNPSstickNVPVPNPHe knows the girl with aV DET NN P DETNPPPNPSribbonNNPVPNPFigure 1: Sentences similar in appearance but dif-fer in syntactic structureCollins et al (Collins, 2001a; Collins, 2001b)proposed Tree Kernel, a method to calculate a sim-ilarity between syntactic structures.
Tree Kerneldefines the similarity between two syntactic struc-tures as the number of shared subtrees.
Retrievingsimilar sentences in a huge corpus requires cal-culating the similarity between a given query andeach of sentences in the corpus.
Building an indextable in advance could improve retrieval efficiency,but indexing with Tree Kernel is impractical due tothe size of its index table.In this paper, we propose two efficient algo-399rithms to calculate similarity of syntactic struc-tures: Tree Overlapping and Subpath Set.
Thesealgorithms are more efficient than Tree Kernel be-cause it is possible to make an index table in rea-sonable size.
The experiments comparing thesethree algorithms showed that Tree Overlapping is100 times faster and Subpath Set is 1,000 timesfaster than Tree Kernel when being used for struc-tural retrieval.After briefly reviewing Tree Kernel in section 2,in what follows, we describe two algorithms insection 3 and 4.
Section 5 describes experimentsto compare these three algorithms and discussionon the results.
Finally, we conclude the paper andlook at the future direction of our research in sec-tion 6.2 Tree Kernel2.1 Definition of similarityTree Kernel is proposed by Collins et al (Collins,2001a; Collins, 2001b) as a method to calculatesimilarity between tree structures.
Tree Kernel de-fines similarity between two trees as the numberof shared subtrees.
Subtree S of tree T is definedas any tree subsumed by T , and consisting of morethan one node, and all child nodes are included ifany.Tree Kernel is not always suitable because thedesired properties of similarity are different de-pending on applications.
Takahashi et al pro-posed three types of similarity based on Tree Ker-nel (Takahashi, 2002).
We use one of the similar-ity measures (equation (1)) proposed by Takahashiet alKC(T1, T2) = maxn1?N1, n2?N2C(n1, n2) (1)where C(n1, n2) is the number of shared subtreesby two trees rooted at nodes n1 and n2.2.2 Algorithm to calculate similarityCollins et al (Collins, 2001a; Collins, 2001b)proposed an efficient method to calculate TreeKernel by using C(n1, n2) as follows.?
If the productions at n1 and n2 are differentC(n1, n2) = 0?
If the productions at n1 and n2 are thesame, and n1 and n2 are pre-terminals, thenC(n1, n2) = 1?
Else if the productions at n1 and n2 are thesame and n1 and n2 are not pre-terminals,C(n1, n2) =nc(n1)?i=1(1 + C(ch(n1, i), ch(n2, i)))(2)where nc(n) is the number of children of node nand ch(n, i) is the i?th child node of n. Equa-tion (2) recursively calculates C on its child node,and calculating Cs in postorder avoids recalcula-tion.
Thus, the time complexity of KC(T1, T2) isO(mn), where m and n are the numbers of nodesin T1 and T2 respectively.2.3 Algorithm to retrieve sentencesNeither Collins nor Takahashi discussed retrievalalgorithms using Tree Kernel.
We use the follow-ing simple algorithm.
First we calculate the simi-larity KC(T1, T2) between a query tree and everytree in the corpus and rank them in descending or-der of KC .Tree Kernel exploits all subtrees shared by trees.Therefore, it requires considerable amount of timein retrieval because similarity calculation must beperformed for every pair of trees.
To improve re-trieval time, an index table can be used in general.However, indexing by all subtrees is difficult be-cause a tree often includes millions of subtrees.For example, one sentence in Titech Corpus (Noroet al, 2005) with 22 words and 87 nodes includes8,213,574,246 subtrees.
The number of subtreesin a tree with N nodes is bounded above by 2N .3 Tree Overlapping3.1 Definition of similarityWhen putting an arbitrary node n1 of tree T1 onnode n2 of tree T2, there might be the same pro-duction rule overlapping in T1 and T2.
We defineCTO(n1, n2) as the number of such overlappingproduction rules when n1 overlaps n2 (Figure 2).We will define CTO(n1, n2) more precisely.First we define L(n1, n2) of node n1 of T1 andnode n2 of T2.
L(n1, n2) represents a set of pairsof nodes which overlap each other when puttingn1 on n2.
For example in Figure 2, L(b11, b21) ={(b11, b21), (d11, d21), (e11, e21), (g11, g21), (i11, j21)}.L(n1, n2) is defined as follows.
Here ni and miare nodes of tree Ti, ch(n, i) is the i?th child ofnode n.1.
(n1, n2) ?
L(n1, n2)400(1) aT2bd egjgiab cd(2)egibd egjab cd egiab cd egi(3)giCTO(b11,b21) = 2agibd egjT1aCTO(g11,g21) = 1111111 111111112121212121 212221111111 111111112121212121 212221111111 111111112121212121 212221Figure 2: Example of similarity calculation2.
If (m1,m2) ?
L(n1, n2),(ch(m1, i), ch(m2, i)) ?
L(n1, n2)3.
If (ch(m1, i), ch(m2, i)) ?
L(n1, n2),(m1,m2) ?
L(n1, n2)4.
L(n1, n2) includes only pairs generated byapplying 2. and 3. recursively.CTO(n1, n2) is defined by using L(n1, n2) asfollows.CTO(n1, n2)=??????????????????(m1,m2)????????
?m1 ?
NT (T1)?
m2 ?
NT (T2)?
(m1,m2) ?
L(n1, n2)?
PR(m1) = PR(m2)?????????????????
?,(3)where NT (T ) is a set of nonterminal nodes in treeT , PR(n) is a production rule rooted at node n.Tree Overlapping similarity STO(T1, T2) is de-fined as follows by using CTO(n1, n2).STO(T1, T2) = maxn1?NT (T1) n2?NT (T2)CTO(n1, n2)(4)This formula corresponds to equation (1) of TreeKernel.As an example, we calculate STO(T1, T2) inFigure 2 (1).
Putting b11 on b21 gives Figure 2 (2)in which two production rules b ?
d e and e ?
goverlap respectively.
Thus, CTO(b11, b21) becomes2.
While overlapping g11 and g21 gives Figure 2 (3)in which only one production rule g ?
i overlaps.Thus, CTO(g11, g21) becomes 1.
Since there are noother node pairs which gives larger CTO than 2,STO(T1, T2) becomes 2.Table 1: Example of the index tablep I[p]a ?
b c {a11}b ?
d e {b11, b21}e ?
g {e11, e21}g ?
i {g11, g21}a ?
g b {a21}g ?
j {g21}3.2 AlgorithmLet us take an example in Figure 3 to explain thealgorithm.
Suppose that T0 is a query tree and thecorpus has only two trees, T1 and T2.The method to find the most similar tree to agiven query tree is basically the same as Tree Ker-nel?s (section 2.2).
However, unlike Tree Kernel,Tree Overlapping-based retrieval can be acceler-ated by indexing the corpus in advance.
Thus,given a tree corpus, we build an index table I[p]which maps a production rule p to its occurrences.Occurrences of production rules are representedby their left-hand side symbols, and are distin-guished with respect to trees including the rule and401(1) T0 a(2)bd egjgi(3)ab cd eScore: 2 pt.
Score: 1 pt.ab cd egiab cd eab cd eaT2bd egjgiab cd egiT10101010101111111 11111111212121212122210101010101111111 111111110101010101212121212121222121Figure 3: Example of Tree Overlapping-based retrievalthe position in the tree.
I[p] is defined as follows.I[p] =?????m??????
?T ?
F?
m ?
NT (T )?
p = PR(m)?????
(5)where F is the corpus (here {T1, T2}) and themeaning of other symbols is the same as the defi-nition of CTO (equation (3)).Table 1 shows an example of the index tablegenerated from T1 and T2 in Figure 3 (1).
In Ta-ble 1, a superscript of a nonterminal symbol iden-tifies a tree, and a subscript identifies a position inthe tree.By using the index table, we calculate C[n,m]with the following algorithm.for all (n,m) do C[n,m] := 0 endforeach n in NT (T0) doforeach m in I[PR(n)] do(n?,m?)
:= top(n,m)C[n?,m?]
:= C[n?,m?]
+ 1endendwhere top(n,m) returns the upper-most pair ofoverlapped nodes when node n and m overlap.The value of top uniquely identifies a situation ofoverlapping two trees.
Function top(n,m) is cal-culated by the following algorithm.function top(n,m);begin(n?,m?)
:= (n,m)while order(n?)
= order(m?)
don?
:= parent(n?)m?
:= parent(m?
)endreturn (n?,m?
)endwhere parent(n) is the parent node of n, andorder(n) is the order of node n among its siblings.Table 2 shows example values of top(n,m) gen-erated by overlapping T0 and T1 in Figure 3.
Notethat top maps every pair of corresponding nodesin a certain overlapping situation to a pair of theupper-most nodes of that situation.
This enablesus to use the value of top as an identifier of a situ-ation of overlap.Table 2: Examples of top(n,m)(n,m) top(n,m)(a01, a11) (a01, a11)(b01, b11) (a01, a11)(c01, c11) (a01, a11)Now C[top(n,m)] = CTO(n,m), therefore thetree similarity between a query tree T0 and eachtree T in the corpus STO(T0, T )can be calculatedby:STO(T0, T ) = maxn?NT (T0), m?NT (T )C[top(n,m)](6)3.3 Comparison with Tree KernelThe value of STO(T1, T2) roughly corresponds tothe number of production rules included in thelargest sub-tree shared by T1 and T2.
Therefore,this value represents the size of the subtree shared402by both trees, like Tree Kernel?s KC , though thedefinition of the subtree size is different.One difference is that Tree Overlapping consid-ers shared subtrees even though they are split by anonshared node as shown in Figure 4.
In Figure 4,T1 and T2 share two subtrees rooted at b and c, buttheir parent nodes are not identical.
While TreeKernel does not consider the superposition puttingnode a on h, Tree Overlapping considers putting aon h and assigns count 2 to this superposition.ab cf g(3)d ehb cf gd eab cf gd ehb cf gd eSTO(T1,T2) = 2(1) T1 (2) T2Figure 4: Example of counting two separatedshared subtrees as oneAnother, more important, difference is that TreeOverlapping retrieval can be accelerated by index-ing the corpus in advance.
The number of indexesis bounded above by the number of productionrules, which is within a practical index size.4 Subpath Set4.1 Definition of similaritySubpath Set similarity between two trees is de-fined as the number of subpaths shared by thetrees.
Given a tree, its subpaths is defined as aset of every path from the root node to leaves andtheir partial paths.Figure 5 (2) shows all subpaths in T1 and T2 inFigure 5(1).
Here we denotes a path as a sequenceof node names such as (a, b, d).
Therefore, Sub-path Set similarity of T1 and T2 becomes 15.4.2 AlgorithmSuppose T0 is a query tree, TS is a set of trees inthe corpus and P (T ) is a set of subpaths of T .
Wecan build an index table I[p] for each productionrule p as follows.I[p] = {T |T ?
TS ?
p ?
P (T )} (7)Using the index table, we can calculate the num-ber of shared subpaths by T0 and T , S[T ], by thefollowing algorithm:for all T S[T ] := 0;foreach p in P (T0) doforeach T in I[p] doS[T ] := S[T ] + 1endend4.3 Comparison with Tree KernelAs well as Tree Overlapping, Subpath Set retrievalcan be accelerated by indexing the corpus.
Thenumber of indexes is bounded above by L ?
D2where L is the maximum number of leaves of trees(the number of words in a sentence) and D is themaximum depth of syntactic trees.
Moreover, con-sidering a subpath as an index term, we can useexisting retrieval tools.Subpath Set uses less structural informationthan Tree Kernel and Tree Overlapping.
It doesnot distinguish the order and number of childnodes.
Therefore, the retrieval result tends to benoisy.
However, Subpath Set is faster than TreeOverlapping, because the algorithm is simpler.5 ExperimentsThis section describes the experiments which wereconducted to compare the performance of struc-ture retrieval based on Tree Kernel, Tree Overlap-ping and Subpath Set.5.1 DataWe conducted two experiments using different an-notated corpora.
Titech corpus (Noro et al, 2005)consists of about 20,000 sentences of Japanesenewspaper articles (Mainiti Shimbun).
Each sen-tence has been syntactically annotated by hand.Due to the limitation of computational resources,we used randomly selected 2,483 sentences as adata collection.Iwanami dictionary (Nishio et al, 1994) is aJapanese dictionary.
We extracted 57,982 sen-tences from glosses in the dictionary.
Each sen-tences was analyzed with a morphological an-alyzer, ChaSen (Asahara et al, 1996) and theMSLR parser (Shirai et al, 2000) to obtain syntac-tic structure candidates.
The most probable struc-ture with respect to PGLR model (Inui et al, 1996)was selected from the output of the parser.
Sincethey were not investigated manually, some sen-tences might have been assigned incorrect struc-tures.5.2 MethodWe conducted two experiments Experiment I andExperiment II with different corpora.
The queries403(1) aT2bd egjgiab cd egiT1(c),(a,c),(e,g,i),(b,e,g,i),(a,b,e,g,i)(2) Subpaths of T1Subpaths of T2SSS(T1,T2) = 15(a), (b), (d), (e), (g), (i),(a,b), (b,d), (b,e), (e,g), (g,i),(a,b,d), (a, b, e), (b,e,g),(a,b,e,g)(j),(a,g), (g,j),(a,g,i), (e,g,j),(b,e,g,j),(a,b,e,g,j)Figure 5: Example of subpathswere extracted from these corpora.
The algorithmsdescribed in the preceding sections were imple-mented with Ruby 1.8.2.
Table 3 outlines the ex-periments.Table 3: Summary of experimentsExperiment I IITarget corpus Titech Corpus Iwanami dict.Corpus size 2,483 sent.
57,982 sent.No.
of queries 100 1,000CPU Intel Xeon PowerPC G5(2.4GHz) (2.3GHz)Memory 2GB 2GB5.3 Results and discussionSince we select a query from the target corpus,the query is always ranked in the first place in theretrieval result.
In what follows, we exclude thequery tree as an answer from the result.We evaluated the algorithms based on the fol-lowing two factors: average retrieval time (CPUtime) (Table 4) and the rank of the tree which wastop-ranked in other algorithm (Table 5).
For ex-ample, in Experiment I of Table 5, the column??5th?
of the row ?TO/TK?
means that there were73 % of the cases in which the top-ranked tree byTree Kernel (TK) was ranked 5th or above by TreeOverlapping (TO).We consider Tree Kernel (TK) as the baselinemethod because it is a well-known existing simi-larity measure and exploits more information thanothers.
Table 4 shows that in both corpora, theretrieval speed of Tree Overlapping (TO) is aboutTable 4: Average retrieval time per query [sec]Algorithm Experiment I Experiment IITK 529.42 3796.1TO 6.29 38.3SS 0.47 5.1100 times faster than that of Tree Kernel, and theretrieval speed of Subpath Set (SS) is about 1,000times faster than that of Tree Kernel.
This re-sults show we have successfully accelerated theretrieval speed.The retrieval time of Tree Overlapping, 6.29and 38.3 sec./per query, seems be a bit long.
How-ever, we can shorten this time if we tune the im-plementation by using a compiler-type language.Note that the current implementation uses Ruby,an interpreter-type language.Comparing Tree Overlapping and Subpath Setwith respect to Tree Kernel (see rows ?TK/TO?and ?TK/SS?
), the top-ranked trees by Tree Kernelare ranked in higher places by Tree Overlappingthan by Subpath Set.
This means Tree Overlap-ping is better than Subpath Set in approximatingTree Kernel.Although the corpus of Experiment II is 20times larger than that of Experiment I, the figuresof Experiment II is better than that of Experiment Iin Table 5.
This could be explained as follows.In Experiment II, we used sentences from glossesin the dictionary, which tend to be formulaic andshort.
Therefore we could find similar sentenceseasier than in Experiment I.To summarize the results, when being used in404Table 5: The rank of the top-ranked tree by otheralgorithm [%]Experiment IA/B ?
1st?
?
5th ?
10thTO/TK 34.0 73.0 82.0SS/TK 16.0 35.0 45.0TK/TO 29.0 41.0 51.0SS/TO 27.0 49.0 58.0TK/SS 17.0 29.0 37.0TO/SS 29.0 58.0 69.0Experiment IIA/B ?
1st?
?
5th ?
10thTO/TK 74.6 88.0 92.0SS/TK 65.3 78.8 84.1TK/TO 71.1 81.0 84.6SS/TO 73.4 86.0 89.8TK/SS 65.5 75.9 79.7TO/SS 76.1 87.7 92.0similarity calculation of tree structure retrieval,Tree Overlapping approximates Tree Kernel bet-ter than Subpath Set, while Subpath Set is fasterthan Tree Overlapping.6 ConclusionWe proposed two fast algorithms to retrieve sen-tences which have a similar syntactic structure:Tree Overlapping (TO) and Subpath Set (SS).
Andwe compared them with Tree Kernel (TK) to ob-tain the following results.?
Tree Overlapping-based retrieval outputssimilar results to Tree Kernel-based retrievaland is 100 times faster than Tree Kernel-based retrieval.?
Subpath Set-based retrieval is not so goodat approximating Tree Kernel-based retrieval,but is 1,000 times faster than Tree Kernel-based retrieval.Structural retrieval is useful for annotationg cor-pora with syntactic information (Yoshida et al,2004).
We are developing a corpus annotation toolnamed ?eBonsai?
which supports human to anno-tate corpora with syntactic information and to re-trieve syntactic structures.
Integrating annotationand retrieval enables annotators to annotate a newinstance with looking back at the already anno-tated instances which share the similar syntacticstructure with the current one.
For such purpose,Tree Overlapping and Subpath Set alorithms con-tribute to speed up the retrieval process, thus makethe annotation process more efficient.However, ?similarity?
of sentences is affectedby semantic aspects as well as structural aspects.The output of the algorithms do not always con-form with human?s intuition.
For example, thetwo sentences in Figure 6 have very similar struc-tures including particles, but they are hardly con-sidered similar from human?s viewpoint.
With thisrespect, it is hardly to say which algorithm is su-perior to others.As a future work, we need to develop a methodto integrate both content-based and structure-based similarity measures.
To this end, we haveto evaluate the algorithms in real application envi-ronments (e.g.
information retrieval and machinetranslation) because desired properties of similar-ity are different depending on applications.ReferencesAsahara, M. and Matsumoto, Y., Extended Models andTools for High-performance Part-of-Speech Tagger.Proceedings of COLING 2000, 2000.Collins, M. and Duffy, N. Parsing with a Single Neu-ron: Convolution Kernels for Natural LanguageProblems.
Technical report UCSC-CRL-01-01, Uni-versity of California at Santa Cruz, 2001.Collins, M. and Duffy, N. Convolution Kernels for Nat-ural Language.
In Proceedings of NIPS 2001, 2001.Inui, K., Shirai, K., Tokunaga T. and Tanaka H., The In-tegration of Statistics-based Techniques in the Anal-ysis of Japanese Sentences.
Special Interest Groupof Natural Language Processing, Information Pro-cessing Society of Japan, Vol.
96, No.
114, 1996.Nagao, M. A framework of a mechanical translationbetween Japanese and English by analogy principle.In Alick Elithorn and Ranan Banerji, editors, Artif-ical and Human Intelligence, pages 173-180.
Ams-terdam, 1984.Noro, T., Koike, C., Hashimoto, T., Tokunaga, T. andTanaka, H. Evaluation of a Japanese CFG Derivedfrom a Syntactically Annotated Corpus with respectto Dependency Measures, The 5th Workshop onAsian Language Resources, pp.9-16, 2005.Nishio, M., Iwabuchi, E. and Mizutani, S.
(ed.
)Iwanami Kokugo Jiten, Iwanamishoten, 5th Edition,1994.Shirai, K., Ueki, M. Hashimoto, T., Tokunaga, T. andTanaka, H., MSLR Parser Tool Kit - Tools for Natu-ral Language Analysis.
Journal of Natural Language405P ADJ NN P NPPPPSPVPNPVNP(to) (young) (a teaching material company) (of) (man) (SBJ ) (came)(classroom)P ADJ NN P NPPPPSPVPNPVNP(to) (exploded) (bombshell) (of) (piece) (SBJ ) (hit)(head)"A young man of a teaching material company came to the classroom""A piece of the exploded bombshell hit his head"QueryTop- rankedFigure 6: Example of a retrieved similar sentenceProcessing, Vol.
7, No.
5, pp.
93-112, 2000.
(inJapanese)Somers, H., McLean, I., Jones, D. Experiments in mul-tilingual example-based generation.
CSNLP 1994:3rd conference on the Cognitive Science of NaturalLanguage Processing, Dublin, 1994.Takahashi, T., Inui K., and Matsumoto, Y.. Methodsof Estimating Syntactic Similarity.
Special InterestGroup of Natural Language Processing, InformationProcessing Society of Japan, NL-150-7, 2002.
(inJapanese)Yoshida, K., Hashimoto, T., Tokunaga, T. and Tanaka,H.. Retrieving annotated corpora for corpus annota-tion.
Proceedings of 4th International Conference onLanguage Resources and Evaluation: LREC 2004.pp.1775 ?
1778.
2004.406
