Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 40?49,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsA Nonparametric Bayesian Approach to Acoustic Model DiscoveryChia-ying Lee and James GlassComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of TechnologyCambridge, MA 02139, USA{chiaying,jrg}@csail.mit.eduAbstractWe investigate the problem of acoustic mod-eling in which prior language-specific knowl-edge and transcribed data are unavailable.
Wepresent an unsupervised model that simultane-ously segments the speech, discovers a properset of sub-word units (e.g., phones) and learnsa Hidden Markov Model (HMM) for each in-duced acoustic unit.
Our approach is formu-lated as a Dirichlet process mixture model inwhich each mixture is an HMM that repre-sents a sub-word unit.
We apply our modelto the TIMIT corpus, and the results demon-strate that our model discovers sub-word unitsthat are highly correlated with English phonesand also produces better segmentation than thestate-of-the-art unsupervised baseline.
We testthe quality of the learned acoustic models on aspoken term detection task.
Compared to thebaselines, our model improves the relative pre-cision of top hits by at least 22.1% and outper-forms a language-mismatched acoustic model.1 IntroductionAcoustic models are an indispensable componentof speech recognizers.
However, the standard pro-cess of training acoustic models is expensive, andrequires not only language-specific knowledge, e.g.,the phone set of the language, a pronunciation dic-tionary, but also a large amount of transcribed data.Unfortunately, these necessary data are only avail-able for a very small number of languages in theworld.
Therefore, a procedure for training acous-tic models without annotated data would not onlybe a breakthrough from the traditional approach, butwould also allow us to build speech recognizers forany language efficiently.In this paper, we investigate the problem of unsu-pervised acoustic modeling with only spoken utter-ances as training data.
As suggested in Garcia andGish (2006), unsupervised acoustic modeling canbe broken down to three sub-tasks: segmentation,clustering segments, and modeling the sound patternof each cluster.
In previous work, the three sub-problems were often approached sequentially andindependently in which initial steps are not related tolater ones (Lee et al, 1988; Garcia and Gish, 2006;Chan and Lee, 2011).
For example, the speech datawas usually segmented regardless of the clusteringresults and the learned acoustic models.In contrast to the previous methods, we approachthe problem by modeling the three sub-problems aswell as the unknown set of sub-word units as la-tent variables in one nonparametric Bayesian model.More specifically, we formulate a Dirichlet pro-cess mixture model where each mixture is a Hid-den Markov Model (HMM) used to model a sub-word unit and to generate observed segments of thatunit.
Our model seeks the set of sub-word units,segmentation, clustering and HMMs that best repre-sent the observed data through an iterative inferenceprocess.
We implement the inference process usingGibbs sampling.We test the effectiveness of our model on theTIMIT database (Garofolo et al, 1993).
Our modelshows its ability to discover sub-word units that arehighly correlated with standard English phones andto capture acoustic context information.
For the seg-mentation task, our model outperforms the state-of-40the-art unsupervised method and improves the rel-ative F-score by 18.8 points (Dusan and Rabiner,2006).
Finally, we test the quality of the learnedacoustic models through a keyword spotting task.Compared to the state-of-the-art unsupervised meth-ods (Zhang and Glass, 2009; Zhang et al, 2012),our model yields a relative improvement in precisionof top hits by at least 22.1% with only some degra-dation in equal error rate (EER), and outperformsa language-mismatched acoustic model trained withsupervised data.2 Related WorkUnsupervised Sub-word Modeling We followthe general guideline used in (Lee et al, 1988; Gar-cia and Gish, 2006; Chan and Lee, 2011) and ap-proach the problem of unsupervised acoustic mod-eling by solving three sub-problems of the task:segmentation, clustering and modeling each cluster.The key difference, however, is that our model doesnot assume independence among the three aspects ofthe problem, which allows our model to refine its so-lution to one sub-problem by exploiting what it haslearned about other parts of the problem.
Second,unlike (Lee et al, 1988; Garcia and Gish, 2006) inwhich the number of sub-word units to be learned isassumed to be known, our model learns the propersize from the training data directly.Instead of segmenting utterances, the authorsof (Varadarajan et al, 2008) trained a single stateHMM using all data at first, and then iterativelysplit the HMM states based on objective functions.This method achieved high performance in a phonerecognition task using a label-to-phone transducertrained from some transcriptions.
However, the per-formance seemed to rely on the quality of the trans-ducer.
For our work, we assume no transcriptionsare available and measure the quality of the learnedacoustic units via a spoken query detection task asin Jansen and Church (2011).Jansen and Church (2011) approached the task ofunsupervised acoustic modeling by first discoveringrepetitive patterns in the data, and then learned awhole-word HMM for each found pattern, where thestate number of each HMM depends on the averagelength of the pattern.
The states of the whole-wordHMMs were then collapsed and used to representacoustic units.
Instead of discovering repetitive pat-terns first, our model is able to learn from any givendata.Unsupervised Speech Segmentation One goalof our model is to segment speech data intosmall sub-word (e.g., phone) segments.
Most un-supervised speech segmentation methods rely onacoustic change for hypothesizing phone bound-aries (Scharenborg et al, 2010; Qiao et al, 2008;Dusan and Rabiner, 2006; Estevan et al, 2007).Even though the overall approaches differ, these al-gorithms are all one-stage and bottom-up segmenta-tion methods (Scharenborg et al, 2010).
Our modeldoes not make a single one-stage decision; instead, itinfers the segmentation through an iterative processand exploits the learned sub-word models to guideits hypotheses on phone boundaries.Bayesian Model for Segmentation Our model isinspired by previous applications of nonparametricBayesian models to segmentation problems in NLPand speaker diarization (Goldwater, 2009; Fox et al,2011); particularly, we adapt the inference methodused in (Goldwater, 2009) to our segmentation task.Our problem is, in principle, similar to the word seg-mentation problem discussed in (Goldwater, 2009).The main difference, however, is that our modelis under the continuous real value domain, and theproblem of (Goldwater, 2009) is under the discretesymbolic domain.
For the domain our problem is ap-plied to, our model has to include more latent vari-ables and is more complex.3 Problem FormulationThe goal of our model, given a set of spoken utter-ances, is to jointly learn the following:?
Segmentation: To find the phonetic boundarieswithin each utterance.?
Nonparametric clustering: To find a proper setof clusters and group acoustically similar seg-ments into the same cluster.?
Sub-word modeling: To learn a HMM to modeleach sub-word acoustic unit.We model the three sub-tasks as latent variablesin our approach.
In this section, we describe the ob-served data, latent variables, and auxiliary variables41?x2i?x3i?x4i?x5i?x6i?x7i?x8i?x9i?x10i?x11i?x1ib a n a n a?(xti)?
(t) 1 2 3 4 5 6 7 8 9 10 11?(bti)?(gqi)?g0i?g1i?g2i?g3i?g4i?g5i?g6i?
(pj ,ki)?p1,1i?p2,4i?p5,6i?p7,8i?p9,9i?p10,11i?
(cj ,ki)?c1,1i?c2,4i?c5,6i?c7,8i?c9,9i?c10,11i?(?c)??1??2??3??4??3??2?
(sti) 1 1 2 3 1 3 1 3 1 1 3Frame index Speech feature Boundary variable Boundary index SegmentCluster labelHMMHidden state[b] [ax] [n] [ae] [n] [ax] Pronunciation1 0 0 1 0 1 0 1 1  0 1Duration?
(dj,ki) 1 3 2 2 1 21 1 6 8 3 7 5 2 8 2 8 Mixture IDFigure 1: An example of the observed data and hiddenvariables of the problem for the word banana.
See Sec-tion 3 for a detailed explanation.of the problem and show an example in Fig.
1.
Inthe next section, we show the generative process ourmodel uses to generate the observed data.Speech Feature (xit) The only observed data forour problem are a set of spoken utterances, which areconverted to a series of 25 ms 13-dimensional Mel-Frequency Cepstral Coefficients (MFCCs) (Davisand Mermelstein, 1980) and their first- and second-order time derivatives at a 10 ms analysis rate.
Weuse xit ?
R39 to denote the tth feature frame of theith utterance.
Fig.
1 illustrates how the speech signalof a single word utterance banana is converted to asequence of feature vectors xi1 to xi11.Boundary (bit) We use a binary variable bit to in-dicate whether a phone boundary exists between xitand xit+1.
If our model hypothesizes xit to be the lastframe of a sub-word unit, which is called a boundaryframe in this paper, bit is assigned with value 1; or 0otherwise.
Fig.
1 shows an example of the boundaryvariables where the values correspond to the true an-swers.
We use an auxiliary variable giq to denote theindex of the qth boundary frame in utterance i. Tomake the derivation of posterior distributions easierin Section 5, we define gi0 to be the beginning ofan utterance, and Li to be the number of boundaryframes in an utterance.
For the example shown inFig.
1, Li is equal to 6.Segment (pij,k) We define a segment to be com-posed of feature vectors between two boundaryframes.
We use pij,k to denote a segment that con-sists of xij , xij+1 ?
?
?xik and dij,k to denote the lengthof pij,k.
See Fig.
1 for more examples.Cluster Label (cij,k) We use cij,k to specify thecluster label of pij,k.
We assume segment pij,k is gen-erated by the sub-word HMM with label cij,k.HMM (?c) In our model, each HMM has threeemission states, which correspond to the beginning,middle and end of a sub-word unit (Jelinek, 1976).A traversal of each HMM must start from the firststate, and only left-to-right transitions are allowedeven though we allow skipping of the middle andthe last state for segments shorter than three frames.The emission probability of each state is modeled bya diagonal Gaussian Mixture Model (GMM) with 8mixtures.
We use ?c to represent the set of param-eters that define the cth HMM, which includes statetransition probability aj,kc , and the GMM parametersof each state emission probability.
We use wmc,s ?
R,?mc,s ?
R39 and ?mc,s ?
R39 to denote the weight,mean vector and the diagonal of the inverse covari-ance matrix of the mth mixture in the GMM for thesth state in the cth HMM.Hidden State (sit) Since we assume the observeddata are generated by HMMs, each feature vector,xit, has an associated hidden state index.
We denotethe hidden state of xit as sit.Mixture ID (mit) Similarly, each feature vector isassumed to be emitted by the state GMM it belongsto.
We use mit to identify the Gaussian mixture thatgenerates xit.4 ModelWe aim to discover and model a set of sub-wordunits that represent the spoken data.
If we think ofutterances as sequences of repeated sub-word units,then in order to find the sub-words, we need a modelthat concentrates probability on highly frequent pat-terns while still preserving probability for previouslyunseen ones.
Dirichlet processes are particularysuitable for our goal.
Therefore, we construct ourmodel as a Dirichlet Process (DP) mixture model,of which the components are HMMs that are used42parameter of Bernoulli distribution??b???
?0concentration parameter of DP base distribution of DP??
prior distribution for cluster labels?btboundary variable?dj ,k duration of a segment?cj,kcluster label?
?cHMM parameters?sthidden state?mtGaussian mixture id?xtobserved feature vector deterministic relation???T??
?dj ,k????b?
?0?cj,k?st?j,k = gq+1,gq+1?xt?dj ,k?mt?bt?
?c?0 ?
q < L?Ttotal number of  observed features frames?Ltotal number of  segments determined by?bt?gqthe index of the       boundary variable with value 1?qthFigure 2: The graphical model for our approach.
The shaded circle denotes the observed feature vectors, and thesquares denote the hyperparameters of the priors used in our model.
The dotted arrows indicate deterministic relations.Note that the Markov chain structure over the st variables is not shown here due to limited space.to model sub-word units.
We assume each spokensegment is generated by one of the clusters in thisDP mixture model.
Here, we describe the genera-tive process our model uses to generate the observedutterances and present the corresponding graphicalmodel.
For clarity, we assume that the values ofthe boundary variables bit are given in the genera-tive process.
In the next section, we explain how toinfer their values.Let pigiq+1,giq+1for 0 ?
q ?
Li ?
1 be the seg-ments of the ith utterance.
Our model assumes eachsegment is generated as follows:1.
Choose a cluster label cigiq+1,giq+1for pigiq+1,giq+1.This cluster label can be either an existing la-bel or a new one.
Note that the cluster labeldetermines which HMM is used to generate thesegment.2.
Given the cluster label, choose a hidden statefor each feature vector xit in the segment.3.
For each xit, based on its hidden state, choose amixture from the GMM of the chosen state.4.
Use the chosen Gaussian mixture to generatethe observed feature vector xit.The generative process indicates that our modelignores utterance boundaries and views the entiredata as concatenated spoken segments.
Given thisviewpoint, we discard the utterance index, i, of allvariables in the rest of the paper.The graphical model representing this generativeprocess is shown in Fig.
2, where the shaded circledenotes the observed feature vectors, and the squaresdenote the hyperparameters of the priors used in ourmodel.
Specifically, we use a Bernoulli distributionas the prior of the boundary variables and imposea Dirichlet process prior on the cluster labels andthe HMM parameters.
The dotted arrows representdeterministic relations.
For example, the boundaryvariables deterministically construct the duration ofeach segment, d, which in turn sets the number offeature vectors that should be generated for a seg-ment.
In the next section, we show how to infer thevalue of each of the latent variables in Fig.
21.5 InferenceWe employ Gibbs sampling (Gelman et al, 2004)to approximate the posterior distribution of the hid-den variables in our model.
To apply Gibbs sam-pling to our problem, we need to derive the condi-tional posterior distributions of each hidden variableof the model.
In the following sections, we first de-rive the sampling equations for each hidden variableand then describe how we incorporate acoustic cuesto reduce the sampling load at the end.1Note that the value of pi is irrelevant to our problem; there-fore, it is integrated out in the inference process435.1 Sampling EquationsHere we present the sampling equations for eachhidden variable defined in Section 3.
We useP (?| ?
?
? )
to denote a conditional posterior probabil-ity given observed data, all the other variables, andhyperparameters for the model.Cluster Label (cj,k) Let C be the set of distinctivelabel values in c?j,k, which represents all the clusterlabels except cj,k.
The conditional posterior proba-bility of cj,k for c ?
C is:P (cj,k = c| ?
?
? )
?
P (cj,k = c|c?j,k; ?
)P (pj,k|?c)=n(c)N ?
1 + ?P (pj,k|?c) (1)where ?
is a parameter of the DP prior.
The first lineof Eq.
1 follows Bayes?
rule.
The first term is theconditional prior, which is a result of the DP priorimposed on the cluster labels 2.
The second term isthe conditional likelihood, which reflects how likelythe segment pj,k is generated by HMMc.
We use n(c)to represent the number of cluster labels in c?j,k tak-ing the value c and N to represent the total numberof segments in current segmentation.In addition to existing cluster labels, cj,k can alsotake a new cluster label, which corresponds to a newsub-word unit.
The corresponding conditional pos-terior probability is:P (cj,k 6= c, c ?
C| ?
?
? )
?
?N ?
1 + ??
?P (pj,k|?)
d?
(2)To deal with the integral in Eq.
2, we follow thesuggestions in (Rasmussen, 2000; Neal, 2000).
Wesample an HMM from the prior and compute thelikelihood of the segment given the new HMM toapproximate the integral.Finally, by normalizing Eq.
1 and Eq.
2, the Gibbssampler can draw a new value for cj,k by samplingfrom the normalized distribution.Hidden State (st) To enforce the assumption thata traversal of an HMM must start from the first stateand end at the last state3, we do not sample hiddenstate indices for the first and the last frame of a seg-ment.
For each of the remaining feature vectors in2See (Neal, 2000) for an overview on Dirichlet process mix-ture models and the inference methods.3If a segment has only 1 frame, we assign the first state to it.a segment pj,k, we sample a hidden state index ac-cording to the conditional posterior probability:P (st = s| ?
?
? )
?P (st = s|st?1)P (xt|?cj,k , st = s)P (st+1|st = s)= ast?1,scj,k P (xt|?cj,k , st = s)as,st+1cj,k (3)where the first term and the third term are the condi-tional prior ?
the transition probability of the HMMthat pj,k belongs to.
The second term is the like-lihood of xt being emitted by state s of HMMcj,k .Note for initialization, st is sampled from the firstprior term in Eq.
3.Mixture ID (mt) For each feature vector in a seg-ment, given the cluster label cj,k and the hidden stateindex st, the derivation of the conditional posteriorprobability of its mixture ID is straightforward:P (mt = m| ?
?
?
)?
P (mt = m|?cj,k , st)P (xt|?cj,k , st,mt = m)= wmcj,k,stP (xt|?mcj,k,st , ?mcj,k,st) (4)where 1 ?
m ?
8.
The conditional posterior con-sists of two terms: 1) the mixing weight of the mthGaussian in the state GMM indexed by cj,k and stand 2) the likelihood of xt given the Gaussian mix-ture.
The sampler draws a value for mt from thenormalized distribution of Eq.
4.HMM Parameters (?c) Each ?c consists of twosets of variables that define an HMM: the state emis-sion probabilities wmc,s, ?mc,s, ?mc,s and the state transi-tion probabilities aj,kc .
In the following, we derivethe conditional posteriors of these variables.Mixture Weight wmc,s: We use wc,s = {wmc,s|1 ?m ?
8} to denote the mixing weights of the Gaus-sian mixtures of state s of HMM c. We choose asymmetric Dirichlet distribution with a positive hy-perparameter ?
as its prior.
The conditional poste-rior probability of wc,s is:P (wc,s| ?
?
? )
?
P (wc,s;?
)P (mc,s|wc,s)?
Dir(wc,s;?)Mul(mc,s;wc,s)?
Dir(wc,s;??)
(5)where mc,s is the set of mixture IDs of feature vec-tors that belong to state s of HMM c. The mth entryof ??
is ?
+?mt?mc,s ?
(mt,m), where we use ?(?
)44P (pl,t, pt+1,r|c?,?)
= P (pl,t|c?,?
)P (pt+1,r|c?, cl,t,?)=[?c?Cn(c)N?
+ ?P (pl,t|?c) +?N?
+ ??
?P (pl,t|?)
d?]?
[?c?Cn(c) + ?
(cl,t, c)N?
+ 1 + ?P (pt+1,r|?c) +?N?
+ 1 + ??
?P (pt+1,r|?)
d?
]P (pl,r|c?,?)
=?c?Cn(c)N?
+ ?P (pl,r|?c) +?N?
+ ??
?P (pl,r|?)
d?Figure 3: The full derivation of the relative conditional posterior probabilities of a boundary variable.to denote the discrete Kronecker delta.
The last lineof Eq.
5 comes from the fact that Dirichlet distribu-tions are a conjugate prior for multinomial distribu-tions.
This property allows us to derive the updaterule analytically.Gaussian Mixture ?mc,s, ?mc,s: We assume the di-mensions in the feature space are independent.
Thisassumption allows us to derive the conditional pos-terior probability for a single-dimensional Gaussianand generalize the results to other dimensions.Let the dth entry of ?mc,s and ?mc,s be ?m,dc,s and?m,dc,s .
The conjugate prior we use for the two vari-ables is a normal-Gamma distribution with hyperpa-rameters ?0, ?0, ?0 and ?0 (Murphy, 2007).P (?m,dc,s , ?m,dc,s |?0, ?0, ?0, ?0)= N(?m,dc,s |?0, (?0?m,dc,s )?1)Ga(?m,dc,s |?0, ?0)By tracking the dth dimension of feature vectorsx ?
{xt|mt = m, st = s, cj,k = c, xt ?
pj,k}, wecan derive the conditional posterior distribution of?m,dc,s and ?m,dc,s analytically following the proceduresshown in (Murphy, 2007).
Due to limited space,we encourage interested readers to find more detailsin (Murphy, 2007).Transition Probabilities aj,kc : We represent thetransition probabilities at state j in HMM c using ajc.If we view ajc as mixing weights for states reachablefrom state j, we can simply apply the update rulederived for the mixing weights of Gaussian mixturesshown in Eq.
5 to ajc.
Assume we use a symmetricDirichlet distribution with a positive hyperparameter?
as the prior, the conditional posterior for ajc is:P (ajc| ?
?
? )
?
Dir(ajc; ??
)where the kth entry of ??
is ?
+ nj,kc , the numberof occurrences of the state transition pair (j, k) insegments that belong to HMM c.Boundary Variable (bt) To derive the conditionalposterior probability for bt, we introduce two vari-ables:l = (argmaxgqgq < t) + 1r = argmingqt < gqwhere l is the index of the closest turned-on bound-ary variable that precedes bt plus 1, while r is the in-dex of the closest turned-on boundary variable thatfollows bt.
Note that because g0 and gL are defined,l and r always exist for any bt.Note that the value of bt only affects segmentationbetween xl and xr.
If bt is turned on, the sampler hy-pothesizes two segments pl,t and pt+1,r between xland xr.
Otherwise, only one segment pl,r is hypoth-esized.
Since the segmentation on the rest of the dataremains the same no matter what value bt takes, theconditional posterior probability of bt is:P (bt = 1| ?
?
? )
?
P (pl,t, pt+1,r|c?,?)
(6)P (bt = 0| ?
?
? )
?
P (pl,r|c?,?)
(7)where we assume that the prior probabilities forbt = 1 and bt = 0 are equal; c?
is the set of clusterlabels of all segments except those between xl andxr ; and ?
indicates the set of HMMs that have as-sociated segments.
Our Gibbs sampler hypothesizesbt?s value by sampling from the normalized distribu-tion of Eq.
6 and Eq.
7.
The full derivations of Eq.
6and Eq.
7 are shown in Fig.
3.Note that in Fig.
3, N?
is the total number of seg-ments in the data except those between xl and xr.45For bt = 1, to account the fact that when the modelgenerates pt+1,r, pl,t is already generated and ownsa cluster label, we sample a cluster label for pl,t thatis reflected in the Kronecker delta function.
To han-dle the integral in Fig.
3, we sample one HMM fromthe prior and compute the likelihood using the newHMM to approximate the integral as suggested in(Rasmussen, 2000; Neal, 2000).5.2 Heuristic Boundary EliminationTo reduce the inference load on the boundary vari-ables bt, we exploit acoustic cues in the feature spaceto eliminate bt?s that are unlikely to be phoneticboundaries.
We follow the pre-segmentation methoddescribed in Glass (2003) to achieve the goal.
Forthe rest of the boundary variables that are proposedby the heuristic algorithm, we randomly initializetheir values and proceed with the sampling processdescribed above.6 Experimental SetupTo the best of our knowledge, there are no stan-dard corpora for evaluating unsupervised methodsfor acoustic modeling.
However, numerous relatedstudies have reported performance on the TIMITcorpus (Dusan and Rabiner, 2006; Estevan et al,2007; Qiao et al, 2008; Zhang and Glass, 2009;Zhang et al, 2012), which creates a set of strongbaselines for us to compare against.
Therefore, theTIMIT corpus is chosen as the evaluation set forour model.
In this section, we describe the methodsused to measure the performance of our model onthe following three tasks: sub-word acoustic model-ing, segmentation and nonparametric clustering.Unsupervised Segmentation We compare thephonetic boundaries proposed by our model to themanual labels provided in the TIMIT dataset.
Wefollow the suggestion of (Scharenborg et al, 2010)and use a 20-ms tolerance window to compute re-call, precision rates and F-score of the segmentationour model proposed for TIMIT?s training set.
Wecompare our model against the state-of-the-art un-supervised and semi-supervised segmentation meth-ods that were also evaluated on the TIMIT trainingset (Dusan and Rabiner, 2006; Qiao et al, 2008).Nonparametric Clustering Our model automat-ically groups speech segments into different clus-ters.
One question we are interested in answeringis whether these learned clusters correlate to En-glish phones.
To answer the question, we developa method to map cluster labels to the phone set ina dataset.
We align each cluster label in an utter-ance to the phone(s) it overlaps with in time byusing the boundaries proposed by our model andthe manually-labeled ones.
When a cluster labeloverlaps with more than one phone, we align itto the phone with the largest overlap.4 We com-pile the alignment results for 3696 training utter-ances5 and present a confusion matrix between thelearned cluster labels and the 48 phonetic units usedin TIMIT (Lee and Hon, 1989).Sub-word Acoustic Modeling Finally, and mostimportantly, we need to gauge the quality of thelearned sub-word acoustic models.
In previouswork, Varadarajan et al (2008) and Garcia andGish (2006) tested their models on a phone recog-nition task and a term detection task respectively.These two tasks are fair measuring methods, but per-formance on these tasks depends not only on thelearned acoustic models, but also other componentssuch as the label-to-phone transducer in (Varadara-jan et al, 2008) and the graphone model in (Garciaand Gish, 2006).
To reduce performance dependen-cies on components other than the acoustic model,we turn to the task of spoken term detection, whichis also the measuring method used in (Jansen andChurch, 2011).We compare our unsupervised acoustic modelwith three supervised ones: 1) an English triphonemodel, 2) an English monophone model and 3) aThai monophone model.
The first two were trainedon TIMIT, while the Thai monophone model wastrained with 32 hour clean read Thai speech fromthe LOTUS corpus (Kasuriya et al, 2003).
Allof the three models, as well as ours, used three-state HMMs to model phonetic units.
To conductspoken term detection experiments on the TIMITdataset, we computed a posteriorgram representa-tion for both training and test feature frames over the4Except when a cluster label is mapped to /vcl/ /b/, /vcl/ /g/and /vcl/ /d/, where the duration of the release /b/, /g/, /d/ isalmost always shorter than the closure /vcl/.
In this case, wealign the cluster label to both the closure and the release.5The TIMIT training set excluding the sa-type subset.46?
?b ?
?
?0 ?0 ?0 ?01 0.5 3 3 ?d 5 3 3/?dTable 1: The values of the hyperparameters of our model,where ?d and ?d are the dth entry of the mean and thediagonal of the inverse covariance matrix of training data.HMM states for each of the four models.
Ten key-words were randomly selected for the task.
For ev-ery keyword, spoken examples were extracted fromthe training set and were searched for in the test setusing segmental dynamic time warping (Zhang andGlass, 2009).In addition to the supervised acoustic models,we also compare our model against the state-of-the-art unsupervised methods for this task (Zhangand Glass, 2009; Zhang et al, 2012).
Zhang andGlass (2009) trained a GMM with 50 componentsto decode posteriorgrams for the feature frames, andZhang et al (2012) used a deep Boltzmann machine(DBM) trained with pseudo phone labels generatedfrom an unsupervised GMM to produce a posteri-orgram representation.
The evaluation metrics theyused were: 1) P@N, the average precision of the topN hits, where N is the number of occurrences of eachkeyword in the test set; 2) EER: the average equal er-ror rate at which the false acceptance rate is equal tothe false rejection rate.
We also report experimentalresults using the P@N and EER metrics.Hyperparameters and Training Iterations Thevalues of the hyperparameters of our model areshown in Table 1, where ?d and ?d are the dth en-try of the mean and the diagonal of the inverse co-variance matrix computed from training data.
Wepick these values to impose weak priors on ourmodel.6 We run our sampler for 20,000 iterations,after which the evaluation metrics for our model allconverged.
In Section 7, we report the performanceof our model using the sample from the last iteration.7 ResultsFig.
4 shows a confusion matrix of the 48 phonesused in TIMIT and the sub-word units learned from3696 TIMIT utterances.
Each circle represents amapping pair for a cluster label and an Englishphone.
The confusion matrix demonstrates a strong6In the future, we plan to extend the model and infer thevalues of these hyperparameters from data directly.05101520253035404550556065707580859095100105110115120iy ix ih ey eh y ae ay aw aa ao ah ax uh uw ow oy w l el er r m n en ng z s zh sh ch jh hh v f dh th d b dx g vcl t p k cl epi silFigure 4: A confusion matrix of the learned cluster labelsfrom the TIMIT training set excluding the sa type utter-ances and the 48 phones used in TIMIT.
Note that forclarity, we show only pairs that occurred more than 200times in the alignment results.
The average co-occurrencefrequency of the mapping pairs in this figure is 431.correlation between the cluster labels and individ-ual English phones.
For example, clusters 19, 20and 21 are mapped exclusively to the vowel /ae/.
Amore careful examination on the alignment resultsshows that the three clusters are mapped to the samevowel in a different acoustic context.
For example,cluster 19 is mapped to /ae/ followed by stop conso-nants, while cluster 20 corresponds to /ae/ followedby nasal consonants.
This context-dependent rela-tionship is also observed in other English phonesand their corresponding sets of clusters.
Fig.
4 alsoshows that a cluster may be mapped to multiple En-glish phones.
For instance, clusters 85 and 89 aremapped to more than one phone; nevertheless, acloser look reveals that these clusters are mapped to/n/, /d/ and /b/, which are sounds with a similar placeof articulation (i.e.
labial and dental).
These corre-lations indicate that our model is able to discover thephonetic composition of a set of speech data withoutany language-specific knowledge.The performance of the four acoustic models onthe spoken term detection task is presented in Ta-ble 2.
The English triphone model achieves the bestP@N and EER results and performs slightly bet-ter than the English monophone model, which indi-cates a correlation between the quality of an acous-tic model and its performance on the spoken termdetection task.
Although our unsupervised modeldoes not perform as well as the supervised English47unit(%) P@N EEREnglish triphone 75.9 11.7English monophone 74.0 11.8Thai monophone 56.6 14.9Our model 63.0 16.9Table 2: The performance of our model and three super-vised acoustic models on the spoken term detection task.acoustic models, it generates a comparable EER anda more accurate detection performance for top hitsthan the Thai monophone model.
This indicates thateven without supervision, our model captures andlearns the acoustic characteristics of a language au-tomatically and is able to produce an acoustic modelthat outperforms a language-mismatched acousticmodel trained with high supervision.Table 3 shows that our model improves P@N bya large margin and generates only a slightly worseEER than the GMM baseline on the spoken termdetection task.
At the end of the training process,our model induced 169 HMMs, which were used tocompute posteriorgrams.
This seems unfair at firstglance because Zhang and Glass (2009) only used50 Gaussians for decoding, and the better result ofour model could be a natural outcome of the highercomplexity of our model.
However, Zhang andGlass (2009) pointed out that using more Gaussianmixtures for their model did not improve their modelperformance.
This indicates that the key reason forthe improvement is our joint modeling method in-stead of simply the higher complexity of our model.Compared to the DBM baseline, our model pro-duces a higher EER; however, it improves the rel-ative detection precision of top hits by 24.3%.
Asindicated in (Zhang et al, 2012), the hierarchicalstructure of DBM allows the model to provide adescent posterior representation of phonetic units.Even though our model only contains simple HMMsand Gaussians, it still achieves a comparable, if notbetter, performance as the DBM baseline.
Thisdemonstrates that even with just a simple modelstructure, the proposed learning algorithm is ableto acquire rich phonetic knowledge from data andgenerate a fine posterior representation for phoneticunits.Table 4 summarizes the segmentation perfor-mance of the baselines, our model and the heuristicunit(%) P@N EERGMM (Zhang and Glass, 2009) 52.5 16.4DBM (Zhang et al, 2012) 51.1 14.7Our model 63.0 16.9Table 3: The performance of our model and the GMMand DBM baselines on the spoken term detection task.unit(%) Recall Precision F-scoreDusan (2006) 75.2 66.8 70.8Qiao et al (2008)* 77.5 76.3 76.9Our model 76.2 76.4 76.3Pre-seg 87.0 50.6 64.0Table 4: The segmentation performance of the baselines,our model and the heuristic pre-segmentation on TIMITtraining set.
*The number of phone boundaries in eachutterance was assumed to be known in this model.pre-segmentation (pre-seg) method.
The language-independent pre-seg method is suitable for seedingour model.
It eliminates most unlikely boundarieswhile retaining about 87% true boundaries.
Eventhough this indicates that at best our model onlyrecalls 87% of the true boundaries, the pre-seg re-duces the search space significantly.
In addition,it also allows the model to capture proper phonedurations, which compensates the fact that we donot include any explicit duration modeling mecha-nisms in our approach.
In the best semi-supervisedbaseline model (Qiao et al, 2008), the number ofphone boundaries in an utterance was assumed tobe known.
Although our model does not incorpo-rate this information, it still achieves a very closeF-score.
When compared to the baseline in whichthe number of phone boundaries in each utterancewas also unknown (Dusan and Rabiner, 2006), ourmodel outperforms in both recall and precision, im-proving the relative F-score by 18.8%.
The key dif-ference between the two baselines and our methodis that our model does not treat segmentation as astand-alone problem; instead, it jointly learns seg-mentation, clustering and acoustic units from data.The improvement on the segmentation task shownby our model further supports the strength of thejoint learning scheme proposed in this paper.8 ConclusionWe present a Bayesian unsupervised approach to theproblem of acoustic modeling.
Without any prior48knowledge, this method is able to discover phoneticunits that are closely related to English phones, im-prove upon state-of-the-art unsupervised segmenta-tion method and generate more precise spoken termdetection performance on the TIMIT dataset.
In thefuture, we plan to explore phonological context anduse more flexible topological structures to modelacoustic units within our framework.AcknowledgementsThe authors would like to thank Hung-an Chang andEkapol Chuangsuwanich for training the Englishand Thai acoustic models.
Thanks to Matthew John-son, Ramesh Sridharan, Finale Doshi, S.R.K.
Brana-van, the MIT Spoken Language Systems group andthe anonymous reviewers for helpful comments.ReferencesChun-An Chan and Lin-Shan Lee.
2011.
Unsupervisedhidden Markov modeling of spoken queries for spo-ken term detection without speech recognition.
In Pro-ceedings of INTERSPEECH, pages 2141 ?
2144.Steven B. Davis and Paul Mermelstein.
1980.
Com-parison of parametric representations for monosyllabicword recognition in continuously spoken sentences.IEEE Trans.
on Acoustics, Speech, and Signal Pro-cessing, 28(4):357?366.Sorin Dusan and Lawrence Rabiner.
2006.
On the re-lation between maximum spectral transition positionsand phone boundaries.
In Proceedings of INTER-SPEECH, pages 1317 ?
1320.Yago Pereiro Estevan, Vincent Wan, and Odette Scharen-borg.
2007.
Finding maximum margin segments inspeech.
In Proceedings of ICASSP, pages 937 ?
940.Emily Fox, Erik B. Sudderth, Michael I. Jordan, andAlan S. Willsky.
2011.
A sticky HDP-HMM withapplication to speaker diarization.
Annals of AppliedStatistics.Alvin Garcia and Herbert Gish.
2006.
Keyword spottingof arbitrary words using minimal speech resources.
InProceedings of ICASSP, pages 949?952.John S. Garofolo, Lori F. Lamel, William M. Fisher,Jonathan G. Fiscus, David S. Pallet, Nancy L.Dahlgren, and Victor Zue.
1993.
Timit acoustic-phonetic continuous speech corpus.Andrew Gelman, John B. Carlin, Hal S. Stern, and Don-ald B. Rubin.
2004.
Bayesian Data Analysis.
Textsin Statistical Science.
Chapman & Hall/CRC, secondedition.James Glass.
2003.
A probabilistic framework forsegment-based speech recognition.
Computer Speechand Language, 17:137 ?
152.Sharon Goldwater.
2009.
A Bayesian framework forword segmentation: exploring the effects of context.Cognition, 112:21?54.Aren Jansen and Kenneth Church.
2011.
Towards un-supervised training of speaker independent acousticmodels.
In Proceedings of INTERSPEECH, pages1693 ?
1696.Frederick Jelinek.
1976.
Continuous speech recogni-tion by statistical methods.
Proceedings of the IEEE,64:532 ?
556.Sawit Kasuriya, Virach Sornlertlamvanich, PatcharikaCotsomrong, Supphanat Kanokphara, and NattanunThatphithakkul.
2003.
Thai speech corpus for Thaispeech recognition.
In Proceedings of Oriental CO-COSDA, pages 54?61.Kai-Fu Lee and Hsiao-Wuen Hon.
1989.
Speaker-independent phone recognition using hidden Markovmodels.
IEEE Trans.
on Acoustics, Speech, and Sig-nal Processing, 37:1641 ?
1648.Chin-Hui Lee, Frank Soong, and Biing-Hwang Juang.1988.
A segment model based approach to speechrecognition.
In Proceedings of ICASSP, pages 501?504.Kevin P. Murphy.
2007.
Conjugate Bayesian analysis ofthe Gaussian distribution.
Technical report, Universityof British Columbia.Radford M. Neal.
2000.
Markov chain sampling meth-ods for Dirichlet process mixture models.
Journalof Computational and Graphical Statistics, 9(2):249?265.Yu Qiao, Naoya Shimomura, and Nobuaki Minematsu.2008.
Unsupervised optimal phoeme segmentation:Objectives, algorithms and comparisons.
In Proceed-ings of ICASSP, pages 3989 ?
3992.Carl Edward Rasmussen.
2000.
The infinite Gaussianmixture model.
In Advances in Neural InformationProcessing Systems, 12:554?560.Odette Scharenborg, Vincent Wan, and Mirjam Ernestus.2010.
Unsupervised speech segmentation: An analy-sis of the hypothesized phone boundaries.
Journal ofthe Acoustical Society of America, 127:1084?1095.Balakrishnan Varadarajan, Sanjeev Khudanpur, and Em-manuel Dupoux.
2008.
Unsupervised learning ofacoustic sub-word units.
In Proceedings of ACL-08:HLT, Short Papers, pages 165?168.Yaodong Zhang and James Glass.
2009.
Unsuper-vised spoken keyword spotting via segmental DTWon Gaussian posteriorgrams.
In Proceedings of ASRU,pages 398 ?
403.Yaodong Zhang, Ruslan Salakhutdinov, Hung-An Chang,and James Glass.
2012.
Resource configurable spokenquery detection using deep Boltzmann machines.
InProceedings of ICASSP, pages 5161?5164.49
