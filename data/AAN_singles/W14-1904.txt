Proceedings of the 5th Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 20?28,Baltimore, Maryland USA, August 26 2014. c?2014 Association for Computational LinguisticsSpeech recognition in Alzheimer?s disease with personal assistive robotsFrank Rudzicz1,2,?
and Rosalie Wang1 and Momotaz Begum3 and Alex Mihailidis2,11 Toronto Rehabilitation Institute, Toronto ON; 2 University of Toronto, Toronto ON;3 University of Massachussetts Lowell?frank@cs.toronto.eduAbstractTo help individuals with Alzheimer?s dis-ease live at home for longer, we are de-veloping a mobile robotic platform, calledED, intended to be used as a personal care-giver to help with the performance of ac-tivities of daily living.
In a series of ex-periments, we study speech-based inter-actions between each of 10 older adultswith Alzheimers disease and ED as theformer makes tea in a simulated home en-vironment.
Analysis reveals that speechrecognition remains a challenge for thisrecording environment, with word-levelaccuracies between 5.8% and 19.2% dur-ing household tasks with individuals withAlzheimer?s disease.
This work provides abaseline assessment for the types of tech-nical and communicative challenges thatwill need to be overcome in human-robotinteraction for this population.1 IntroductionAlzheimer?s disease (AD) is a progressive neu-rodegenerative disorder primarily impairing mem-ory, followed by declines in language, ability tocarry out motor tasks, object recognition, and ex-ecutive functioning (American Psychiatric Asso-ciation, 2000; Gauthier et al., 1997).
An accu-rate measure of functional decline comes fromperformance in activities of daily living (ADLs),such as shopping, finances, housework, and self-care tasks.
The deterioration in language com-prehension and/or production resulting from spe-cific brain damage, also known as aphasia, is acommon feature of AD and other related con-ditions.
Language changes observed clinicallyin older adults with dementia include increasingword-finding difficulties, loss of ability to verballyexpress information in detail, increasing use ofgeneric references (e.g., ?it?
), and progressing dif-ficulties understanding information presented ver-bally (American Psychiatric Association, 2000).Many nations are facing healthcare crises in thelack of capacity to support rapidly aging popula-tions nor the chronic conditions associated withaging, including dementia.
The current healthcaremodel of removing older adults from their homesand placing them into long-term care facilitiesis neither financially sustainable in this scenario(Bharucha et al., 2009), nor is it desirable.
Ourteam has been developing ?smart home?
systemsat the Toronto Rehabilitation Institute (TRI, partof the University Health Network) to help olderadults ?age-in-place?
by providing different typesof support, such as step-by-step prompts for dailytasks (Mihailidis et al., 2008), responses to emer-gency situations (Lee and Mihaildis, 2005), andmeans to communicate with family and friends.These systems are being evaluated within a com-pletely functional re-creation of a one-bedroomapartment located within The TRI hospital, calledHomeLab.
These smart home technologies useadvanced sensing techniques and machine learn-ing to autonomously react to their users, but theyare fixed and embedded into the environment, e.g.,as cameras in the ceiling.
Fixing the location ofthese technologies carries a tradeoff between util-ity and feasibility ?
installing multiple hardwareunits at all locations where assistance could be re-quired (e.g., bathroom, kitchen, and bedroom) canbe expensive and cumbersome, but installing toofew units will present gaps where a user?s activ-ity will not be detected.
Alternatively, integrat-ing personal mobile robots with smart homes canovercome some of these tradeoffs.
Moreover, as-sistance provided via a physically embodied robotis often more acceptable than that provided by anembedded system (Klemmer et al., 2006).With these potential advantages in mind, weconducted a ?Wizard-of-Oz?
study to explore the20feasibility and usability of a mobile assistive robotthat uses the step-by-step prompting approachesfor daily activities originally applied to our smarthome research (Mihailidis et al., 2008).
We con-ducted the study with older adults with mild ormoderate AD and the tasks of hand washing andtea making.
Our preliminary data analysis showedthat the participants reacted well to the robot itselfand the prompts that it provided, suggesting thefeasibility of using personal robots for this appli-cation (Begum et al., 2013).
One important iden-tified issue is the need for an automatic speechrecognition system to detect and understand ut-terances specifically from older adults with AD.The development of such a system will enablethe assistive robot to better understand the be-haviours and needs of these users for effective in-teractions and will further enhance environmental-based smart home systems.This paper presents an analysis of the speechdata collected from our participants with AD wheninteracting with the robot.
In a series of exper-iments, we measure the performance of modernspeech recognition with this population and withtheir younger caregivers with and without signalpreprocessing.
This work will serve as the basisfor further studies by identifying some of the de-velopment needs of a speech-based interface forrobotic caregivers for older adults with AD.2 Related WorkResearch in smart home systems, assistive robots,and integrated robot/smart home systems for olderadults with cognitive impairments has often fo-cused on assistance with activities of daily living(i.e., reminders to do specific activities accordingto a schedule or prompts to perform activity steps),cognitive and social stimulation and emergencyresponse systems.
Archipel (Serna et al., 2007)recognizes the user?s intended plan and providesprompts, e.g.
with cooking tasks.
Autominder,(Pollack, 2006), provides context-appropriate re-minders for activity schedules, and the COACH(Cognitive Orthosis for Assisting with aCtivitiesin the Home) system prompts for the task of hand-washing (Mihailidis et al., 2008) and tea-making(Olivier et al., 2009).
Mynatt et al.
(2004) havebeen developing technologies to support aging-in-place such as the Cooks Collage, which uses a se-ries of photos to remind the user what the last stepcompleted was if the user is interrupted during acooking task.
These interventions tend to be em-bedded in existing environments (e.g., around thesink area).More recent innovations have examined in-tegrated robot-smart home systems where sys-tems are embedded into existing environments thatcommunicate with mobile assistive robots (e.g.,CompanionAble, (Mouad et al., 2010); MobiservKompai, (Lucet, 2012); and ROBADOM (Tapusand Chetouani, 2010)).
Many of these projectsare targeted towards older adults with cognitiveimpairment, and not specifically those with sig-nificant cognitive impairment.
One of these sys-tems, CompanionAble, with a fully autonomousassistive robot, has recently been tested in a simu-lated home environment for two days each withfour older adults with dementia (AD or Pick?sdisease/frontal lobe dementia) and two with mildcognitive impairment.
The system provides assis-tance with various activities, including appoint-ment reminders for activities input by users orcaregivers, video calls, and cognitive exercises.Participants reported an overall acceptance of thesystem and several upgrades were reported, in-cluding a speech recognition system that had to bedeactivated by the second day due to poor perfor-mance.One critical component for the successful use ofthese technological interventions is the usability ofthe communication interface for the targeted users,in this case older adults with Alzheimer?s disease.As in communication between two people, com-munication between the older adult and the robotmay include natural, freeform speech (as opposedto simple spoken keyword interaction) and non-verbal cues (e.g., hand gestures, head pose, eyegaze, facial feature cues), although speech tends tobe far more effective (Green et al., 2008; Goodrichand Schultz, 2007).
Previous research indicatesthat automated communication systems are moreeffective if they take into account the affectiveand mental states of the user (Saini et al., 2005).Indeed, speech appears to be the most powerfulmode of communication for an assistive robot tocommunicate with its users (Tapus and Chetouani,2010; Lucet, 2012).2.1 Language use in dementia andAlzheimer?s diseaseIn order to design a speech interface for individ-uals with dementia, and AD in particular, it is21important to understand how their speech differsfrom that of the general population.
This then canbe integrated into future automatic speech recog-nition systems.
Guinn and Habash (2012) showed,through an analysis of conversational dialogs, thatrepetition, incomplete words, and paraphrasingwere significant indicators of Alzheimer?s dis-ease relative but several expected measures suchas filler phrases, syllables per minute, and pro-noun rate were not.
Indeed, pauses, fillers, for-mulaic speech, restarts, and speech disfluenciesare all hallmarks of speech in individuals withAlzheimer?s (Davis and Maclagan, 2009; Snoveret al., 2004).
Effects of Alzheimer?s disease onsyntax remains controversial, with some evidencethat deficits in syntax or of agrammatism could bedue to memory deficits in the disease (Reilly et al.,2011).Other studies has applied similar analyses torelated clinical groups.
Pakhomov et al.
(2010)identified several different features from the au-dio and corresponding transcripts of 38 patientswith frontotemporal lobar degeneration (FTLD).They found that pause-to-word ratio and pronoun-to-noun ratios were especially discriminative ofFTLD variants and that length, hesitancy, andagramatism correspond to the phenomenology ofFTLD.
Roark et al.
(2011) tested the ability of anautomated classifier to distinguish patients withmild cognitive impairment from healthy controlsthat include acoustic features such as pause fre-quency and duration.2.2 Human-robot interactionReceiving assistance from an entity with a physi-cal body (such as a robot) is often psychologicallymore acceptable than receiving assistance from anentity without a physical body (such as an em-bedded system) (Klemmer et al., 2006).
Physicalembodiment also opens up the possibility of hav-ing more meaningful interaction between the olderadult and the robot, as discussed in Section 5.Social collaboration between humans androbots often depends on communication in whicheach participant?s intention and goals are clear(Freedy et al., 2007; Bauer et al., 2008; Greenet al., 2008).
It is important that the humanparticipant is able to construct a useable ?men-tal model?
of the robot through bidirectional com-munication (Burke and Murphy, 1999) which caninclude both natural speech and non-verbal cues(e.g., hand gestures, gaze, facial cues), althoughspeech tends to be far more effective (Green et al.,2008; Goodrich and Schultz, 2007).Automated communicative systems that aremore sensitive to the emotive and the mental statesof their users are often more successful than moreneutral conversational agents (Saini et al., 2005).In order to be useful in practice, these commu-nicative systems need to mimic some of the tech-niques employed by caregivers of individuals withAD.
Often, these caregivers are employed by lo-cal clinics or medical institutions and are trainedby those institutions in ideal verbal communica-tion strategies for use with those having demen-tia (Hopper, 2001; Goldfarb and Pietro, 2004).These include (Wilson et al., 2012) but are notlimited to relatively slow rate of speech, verba-tim repetition of misunderstood prompts, closed-ended (e.g., ?yes/no?)
questions, and reduced syn-tactic complexity.
However, Tomoeda et al.
(1990)showed that rates of speech that are too slowmay interfere with comprehension if they intro-duce problems of short-term retention of workingmemory.
Small et al.
(1997) showed that para-phrased repetition is just as effective as verbatimrepetition (indeed, syntactic variation of commonsemantics may assist comprehension).
Further-more, Rochon et al.
(2000) suggested that the syn-tactic complexity of utterances is not necessarilythe only predictor of comprehension in individualswith AD; rather, correct comprehension of the se-mantics of sentences is inversely related to the in-creasing number of propositions used ?
it is prefer-able to have as few clauses or core ideas as possi-ble, i.e., one-at-a-time.3 Data collectionThe data in this paper come from a study toexamine the feasibility and usability of a per-sonal assistive robot to assist older adults withAD in the completion of daily activities (Begumet al., 2013).
Ten older adults diagnosed withAD, aged ?
55, and their caregivers were re-cruited from a local memory clinic in Toronto,Canada.
Ethics approval was received from theToronto Rehabilitation Institute and the Univer-sity of Toronto.
Inclusion criteria included fluencyin English, normal hearing, and difficulty com-pleting common sequences of steps, according totheir caregivers.
Caregivers had to be a familyor privately-hired caregiver who provides regular22care (e.g., 7 hours/week) to the older adult partici-pant.
Following informed consent, the older adultparticipants were screened using the Mini MentalState Exam (MMSE) (Folstein et al., 2001) to as-certain their general level of cognitive impairment.Table 1 summarizes relevant demographics.Sex Age (years) MMSE (/30)OA1 F 76 9OA2 M 86 24OA3 M 88 25OA4 F 77 25OA5 F 59 18OA6 M 63 23OA7 F 77 25OA8 F 83 19OA9 F 84 25OA10 M 85 15Table 1: Demographics of older adults (OA).
(a)(b)Figure 1: ED and two participants with AD duringthe tea-making task in the kitchen of HomeLab atTRI.3.1 ED, the personal caregiver robotThe robot was built on an iRobot base (operat-ing speed: 28 cm/second) and both its internalconstruction and external enclosure were designedand built at TRI.
It is 102 cm in height and hasseparate body and head components; the latter isprimarily a LCD monitor that shows audiovisualprompts or displays a simple ?smiley face?
other-wise, as shown in Figure 2.
The robot has twospeakers embedded in its ?chest?, two video cam-eras (one in the head and one near the floor, fornavigation), and a microphone.
For this study,the built-in microphones were not used in favor ofenvironmental Kinect microphones, discussed be-low.
This was done to account for situations whenthe robot and human participant were not in thesame room simultaneously.The robot was tele-operated throughout thetask.
The tele-operator continuously monitoredthe task progress and the overall affective stateof the participants in a video stream sent by therobot and triggered social conversation, askedtask-related questions, and delivered prompts toguide the participants towards successful comple-tion of the tea-making task (Fig.
1).Figure 2: The prototype robotic caregiver, ED.The robot used the Cepstral commercial text-to-speech (TTS) system using the U.S. English voice?David?
and its default parameters.
This systemis based on the Festival text-to-speech platform inmany respects, including its use of linguistic pre-processing (e.g., part-of-speech tagging) and cer-tain heuristics (e.g., letter-to-sound rules).
Spo-ken prompts consisted of simple sentences, some-times accompanied by short video demonstrationsdesigned to be easy to follow by people with a cog-nitive impairment.For efficient prompting, the tea-making taskwas broken down into different steps or sub-task.Audio or audio-video prompts corresponding to23each of these sub-tasks were recorded prior todata collection.
The human-robot interaction pro-ceeded according to the following script when col-laborating with the participants:1.
Allow the participant to initiate steps in eachsub-task, if they wish.2.
If a participant asks for directions, deliver theappropriate prompt.3.
If a participant requests to perform the sub-task in their own manner, agree if this doesnot involve skipping an essential step.4.
If a participant asks about the location of anitem specific to the task, provide a full-bodygesture by physically orienting the robot to-wards the sought item.5.
During water boiling, ask the participant toput sugar or milk or tea bag in the cup.
Timepermitting, engage in a social conversation,e.g., about the weather.6.
When no prerecorded prompt sufficiently an-swers a participant question, respond with thecorrect answer (or ?I don?t know?)
throughthe TTS engine.3.2 Study set-up and proceduresConsent included recording video, audio, anddepth images with the Microsoft Kinect sensor inHomeLab for all interviews and interactions withED.
Following informed consent, older adults andtheir caregivers were interviewed to acquire back-ground information regarding their daily activi-ties, the set-up of their home environment, and thetypes of assistance that the caregiver typically pro-vided for the older adult.Participants were asked to observe ED mov-ing in HomeLab and older adult participants wereasked to have a brief conversation with ED tobecome oriented with the robot?s movement andspeech characteristics.
The older adults werethen asked to complete the hand-washing and tea-making tasks in the bathroom and kitchen, respec-tively, with ED guiding them to the locations andproviding specific step-by-step prompts, as neces-sary.
The tele-operator observed the progress ofthe task, and delivered the pre-recorded promptscorresponding to the task step to guide the olderadult to complete each task.
The TTS systemwas used to respond to task-related questions andto engage in social conversation.
The caregiverswere asked to observe the two tasks and to in-tervene only if necessary (e.g., if the older adultshowed signs of distress or discomfort).
Theolder adult and caregiver participants were theninterviewed separately to gain their feedback onthe feasibility of using such a robot for assis-tance with daily activities and usability of the sys-tem.
Each study session lasted approximately 2.5hours including consent, introduction to the robot,tea-making interaction with the robot, and post-interaction interviews.
The average duration forthe tea-making task alone was 12 minutes.4 Experiments and analysisAutomatic speech recognition given these data iscomplicated by several factors, including a pre-ponderance of utterances in which human care-givers speak concurrently with the participants, aswell as inordinately challenging levels of noise.The estimated signal-to-noise ratio (SNR) acrossutterances range from?3.42 dB to 8.14 dB, whichis extremely low compared to typical SNR of 40dB in clean speech.
One cause of this low SNRis that microphones are placed in the environment,rather than on the robot (so the distance to the mi-crophone is variable, but relatively large) and thatthe participant often has their back turned to themicrophone, as shown in figure 1.As in previous work (Rudzicz et al., 2012),we enhance speech signals with the log-spectralamplitude estimator (LSAE) which minimizes themean squared error of the log spectra given amodel for the source speech Xk = Ake(j?k),where Ak is the spectral amplitude.
The LSAEmethod is a modification of the short-time spectralamplitude estimator that finds an estimate of thespectral amplitude, A?k, that minimizes the distor-tionE[(logAk ?
log A?k)2], (1)such that the log-spectral amplitude estimate isA?k = exp (E [lnAk |Yk])= ?k1 + ?kexp(12?
?vke?tt dt)Rk,(2)where ?k is the a priori SNR,Rk is the noisy spec-tral amplitude, vk = ?k1+?k ?k, and ?k is the a pos-teriori SNR (Erkelens et al., 2007).
Often this isbased on a Gaussian model of noise, as it is here(Ephraim and Malah, 1985).24As mentioned, there are many utterances inwhich human caregivers speak concurrently withthe participants.
This is partially confounded bythe fact that utterances by individuals with ADtend to be shorter, so more of their utterance is lost,proportionally.
Examples of this type where thecaregiver?s voice is louder than the participant?svoice are discarded, amounting to about 10% ofall utterances.
In the following analyses, func-tion words (i.e., prepositions, subordinating con-junctions, and determiners) are removed from con-sideration, although interjections are kept.
Propernames are also omitted.We use the HTK (Young et al., 2006) toolchain,which provides an implementation of a semi-continuous hidden Markov model (HMM) that al-lows state-tying and represents output densities bymixtures of Gaussians.
Features consisted of thefirst 13 Mel-frequency cepstral coefficients, theirfirst (?)
and second (??)
derivatives, and the logenergy component, for 42 dimensions.
Our owndata were z-scaled regardless of whether LSAEnoise reduction was applied.Two language models (LMs) are used, both tri-gram models derived from the English Gigawordcorpus, which contains 1200 word tokens (Graffand Cieri, 2003).
The first LM uses the first 5000most frequent words and the second uses the first64,000 most frequent words of that corpus.
Fiveacoustic models (AMs) are used with 1, 2, 4, 8,and 16 Gaussians per output density respectively.These are trained with approximately 211 hoursof spoken transcripts of the Wall Street Journal(WSJ) from over one hundred non-pathologicalspeakers (Vertanen, 2006).Table 2 shows, for the small- and large-vocabulary LMs, the word-level accuracies of thebaseline HTK ASR system, as determined bythe inverse of the Levenshtein edit distance, fortwo scenarios (sit-down interviews vs. duringthe task), with and without LSAE noise reduc-tion, for speech from individuals with AD andfor their caregivers.
These values are computedover all complexities of acoustic model and areconsistent with other tasks of this type (i.e., withthe challenges associated with the population andrecording set up), with this type of relatively un-constrained ASR (Rudzicz et al., 2012).
Apply-ing LSAE results in a significant increase in ac-curacy for both the small-vocabulary (right-tailedhomoscedastic t(58) = 3.9, p < 0.005, CI =[6.19,?])
and large-vocabulary (right-tailed ho-moscedastic t(58) = 2.4, p < 0.01, CI =[2.58,?])
tasks.
For the participants with AD,ASR accuracy is significantly higher in inter-views (paired t(39) = 8.7, p < 0.0001, CI =[13.8,?
]), which is expected due in large partto the closer proximity of the microphone.
Sur-prisingly, ASR accuracy on participants with ASRwas not significantly different than on caregivers(two-tailed heteroscedastic t(78) = ?0.32, p =0.75, CI = [?5.54, 4.0]).Figure 3 shows the mean ASR accuracy, withstandard error (?/?n), for each of the small-vocabulary and large-vocabulary ASR systems.The exponential function b0 + b1 exp(b2x) is fitto these data for each set, where bi are coef-ficients that are iteratively adjustable via meansquared error.
For the small-vocabulary data,R2 = 0.277 and F8 = 3.06, p = 0.12 ver-sus the constant model.
For the large-vocabularydata, R2 = 0.445 and F8 = 2.81, p = 0.13versus the constant model.
Clearly, there is anincreasing trend in ASR accuracy with MMSEscores, however an n-way ANOVA on ASR ac-curacy scores reveals that this increase is not sig-nificant (F1 = 47.07, p = 0.164).
Furthermore,neither the age (F1 = 1.39, p = 0.247) nor the sex(F1 = 0.98, p = 0.33) of the participant had a sig-nificant effect on ASR accuracy.
An additional n-way ANOVA reveals no strong interaction effectsbetween age, sex, and MMSE.8 10 12 14 16 18 20 22 24 26101520253035MMSE scoreASR accuracy(%)Small vocabLarge vocabFigure 3: MMSE score versus mean ASR accu-racy (with std.
error bars) and fits of exponentialregression for each of the small-vocabulary andlarge-vocabulary ASR systems.25Scenario Noise reduction AD caregiverSmall vocabularyInterview None 25.1 (?
= 9.9) 28.8 (?
= 6.0)LSAE 40.9 (?
= 5.6) 40.2 (?
= 5.3)In task None 13.7 (?
= 3.7) -LSAE 19.2 (?
= 9.8) -Large vocabularyInterview None 23.7 (?
= 12.9) 27.0 (?
= 10.0)LSAE 38.2 (?
= 6.3) 35.1 (?
= 11.2)In task None 5.8 (?
= 3.7) -LSAE 14.3 (?
= 12.8) -Table 2: ASR accuracy (means, and std.
dev.)
across speakers, scenario (interviews vs. during the task),and presence of noise reduction for the small and large language models.5 DiscussionThis study examined low-level aspects of speechrecognition among older adults with Alzheimer?sdisease interacting with a robot in a simulatedhome environment.
The best word-level accura-cies of 40.9% (?
= 5.6) and 39.2% (?
= 6.3)achievable with noise reduction and in a quiet in-terview setting are comparable with the state-of-the-art in unrestricted large-vocabulary text entry.These results form the basis for ongoing work inASR and interaction design for this domain.
Thetrigram language model used in this work encap-sulates the statistics of a large amount of speechfrom the general population ?
it is a speaker-independent model derived from a combinationof English news agencies that is not necessarilyrepresentative of the type of language used in thehome, or by our target population.
The acousticmodels were also derived from newswire data readby younger adults in quiet environments.
We arecurrently training and adapting language modelstuned specifically to older adults with Alzheimer?sdisease using data from the Carolina Conversa-tions database (Pope and Davis, 2011) and the De-mentiaBank database (Boller and Becker, 1983).Additionally, to function realistically, a lot ofambient and background noise will need to beovercome.
We are currently looking into deploy-ing a sensor network in the HomeLab that will in-clude microphone arrays.
Another method of im-proving rates of correct word recognition is to aug-ment the process from redundant information froma concurrent sensory stream, i.e., in multimodalinteraction (Rudzicz, 2006).
Combining gestureand eye gaze with speech, for example, can beused to disambiguate speech-only signals.Although a focus of this paper, verbal infor-mation is not the only modality in which human-robot interaction can take place.
Indeed, Wil-son et al.
(2012) showed that experienced humancaregivers employed various non-verbal and semi-verbal strategies to assist older adults with demen-tia about 1/3 as often as verbal strategies (see sec-tion 2.2).
These non-verbal and semi-verbal strate-gies included eye contact, sitting face-to-face, us-ing hand gestures, a calm tone of voice, instru-mental touch, exaggerated facial expressions, andmoving slowly.
Multi-modal communication canbe extremely important for individuals with de-mentia, who may require redundant channels fordisambiguating communication problems, espe-cially if they have a language impairment or a sig-nificant hearing impairment.It is vital that our current technological ap-proaches to caring for the elderly in their homesprogresses quickly, given the demographic shiftin many nations worldwide.
This paper providesa baseline assessment for the types of technicaland communicative challenges that will need to beovercome in the near future to provide caregivingassistance to a growing number of older adults.6 AcknowledgementsThe authors would like to thank Rajibul Huq andColin Harry, who designed and built the robot,Jennifer Boger and Goldie Nejat for their assis-tance in designing the study, and Sharon Cohenfor her consultations during the study.ReferencesAmerican Psychiatric Association.
2000.
Delirium,dementia, and amnestic and other cognitive disor-ders.
In Diagnostic and Statistical Manual of Men-tal Disorders, Text Revision (DSM-IV-TR), chap-ter 2.
American Psychiatric Association, Arlington,VA, fourth edition.26A.
Bauer, D. Wollherr, and M. Buss.
2008.
Human-robot collaboration: A survey.
International Journalof Humanoid Robotics, 5:47?66.Momotaz Begum, Rosalie Wang, Rajibul Huq, andAlex Mihailidis.
2013.
Performance of daily ac-tivities by older adults with dementia: The role ofan assistive robot.
In Proceedings of the IEEE In-ternational Conference on Rehabilitation Robotics,Washington USA, June.Ashok J. Bharucha, Vivek Anand, Jodi Forlizzi,Mary Amanda Dew, Charles F. Reynolds III, ScottStevens, and Howard Wactlar.
2009.
Intelligentassistive technology applications to dementia care:Current capabilities, limitations, and future chal-lenges.
American Journal of Geriatric Psychiatry,17(2):88?104, February.Franc?ois Boller and James Becker.
1983.
Dementia-Bank database.J.L.
Burke and R.R.
Murphy.
1999.
Situationawareness, team communication, and task perfor-mance in robot-assisted technical search: Bujoldgoes to bridgeport.
CMPSCI Tech.
Rep. CRASAR-TR2004-23, University of South Florida.B.
Davis and M. Maclagan.
2009.
Examiningpauses in Alzheimer?s discourse.
American jour-nal of Alzheimer?s Disease and other dementias,24(2):141?154.Y.
Ephraim and D. Malah.
1985.
Speech enhancementusing a minimum mean-square error log-spectralamplitude estimator.
Acoustics, Speech and SignalProcessing, IEEE Transactions on, 33(2):443 ?
445,apr.Jan Erkelens, Jesper Jensen, and Richard Heusdens.2007.
A data-driven approach to optimizing spec-tral speech enhancement methods for various errorcriteria.
Speech Communication, 49:530?541.M.
F. Folstein, S. E. Folstein, T. White, and M. A.Messer.
2001.
Mini-Mental State Examinationuser?s guide.
Odessa (FL): Psychological Assess-ment Resources.A.
Freedy, E. de Visser, G. Weltman, and N. Coeyman.2007.
Measurement of trust in human-robot collab-oration.
In Proceedings of International Conferenceon Collaborative Technologies and Systems, pages17 ?24.Serge Gauthier, Michel Panisset, Josephine Nalban-toglu, and Judes Poirier.
1997.
Alzheimer?s dis-ease: current knowledge, management and research.Canadian Medical Association Journal, 157:1047?1052.R.
Goldfarb and M.J.S.
Pietro.
2004.
Support systems:Older adults with neurogenic communication dis-orders.
Journal of Ambulatory Care Management,27(4):356?365.M.
A. Goodrich and A. C. Schultz.
2007.
Human-robot interaction: A survey.
Foundations and Trendsin Human-Computer Interaction, 1:203?275.David Graff and Christopher Cieri.
2003.
English gi-gaword.
Linguistic Data Consortium.S.
A.
Green, M. Billinghurst, X. Chen, and J. G. Chase.2008.
Human-robot collaboration: A literature re-view and augmented reality approach in design.
In-ternational Journal Advanced Robotic Systems, 5:1?18.Curry Guinn and Anthony Habash.
2012.
TechnicalReport FS-12-01, Association for the Advancementof Artificial Intelligence.T Hopper.
2001.
Indirect interventions to facilitatecommunication in Alzheimers disease.
Seminars inSpeech and Language, 22(4):305?315.S.
Klemmer, B. Hartmann, and L. Takayama.
2006.How bodies matter: five themes for interaction de-sign.
In Proceedings of the conference on DesigningInteractive systems, pages 140?149.Tracy Lee and Alex Mihaildis.
2005.
An intelligentemergency response system: Preliminary develop-ment and testing of automated fall detection.
Jour-nal of Telemedicine and Telecare, 11:194?198.Eric Lucet.
2012.
Social Mobiserv Kompai Robot toAssist People.
In euRobotics workshop on Robots inHealthcare and Welfare.Alex Mihailidis, Jennifer N Boger, Tammy Craig, andJesse Hoey.
2008.
The COACH prompting systemto assist older adults with dementia through hand-washing: An efficacy study.
BMC Geriatrics, 8(28).Mehdi Mouad, Lounis Adouane, Pierre Schmitt,Djamel Khadraoui, Benjamin Ga?teau, and PhilippeMartinet.
2010.
Multi-agents based system to coor-dinate mobile teamworking robots.
In Proceedingsof the 4th Companion Robotics Institute, Brussels.Elizabeth D. Mynatt, Anne-Sophie Melenhorst,Arthur D. Fisk, and Wendy A. Rogers.
2004.
Awaretechnologies for aging in place: Understanding userneeds and attitudes.
IEEE Pervasive Computing,3:36?41.Patrick Olivier, Andrew Monk, Guangyou Xu, andJesse Hoey.
2009.
Ambient kitchen: Designingsituation services using a high fidelity prototypingenvironment.
In Proceedings of the ACM 2nd Inter-national Conference on Pervasive Technologies Re-lated to Assistive Environments, Corfu Greece.S.
V. Pakhomov, G. E. Smith, D. Chacon, Y. Feliciano,N.
Graff-Radford, R. Caselli, and D. S. Knopman.2010.
Computerized analysis of speech and lan-guage to identify psycholinguistic correlates of fron-totemporal lobar degeneration.
Cognitive and Be-havioral Neurology, 23:165?177.27M.
E. Pollack.
2006.
Autominder: A case study of as-sistive technology for elders with cognitive impair-ment.
Generations, 30:67?69.Charlene Pope and Boyd H. Davis.
2011.
Findinga balance: The Carolinas Conversation Collection.Corpus Linguistics and Linguistic Theory, 7(1).J.
Reilly, J. Troche, and M. Grossman.
2011.
Lan-guage processing in dementia.
In A. E. Budson andN.
W. Kowall, editors, The Handbook of Alzheimer?sDisease and Other Dementias.
Wiley-Blackwell.Brian Roark, Margaret Mitchell, John-Paul Hosom,Kristy Hollingshead, and Jeffery Kaye.
2011.
Spo-ken language derived measures for detecting mildcognitive impairment.
IEEE Transactions on Au-dio, Speech, and Language Processing, 19(7):2081?2090.Elizabeth Rochon, Gloria S. Waters, and David Ca-plan.
2000.
The Relationship Between Measuresof Working Memory and Sentence Comprehensionin Patients With Alzheimer?s Disease.
Journal ofSpeech, Language, and Hearing Research, 43:395?413.Frank Rudzicz, Rozanne Wilson, Alex Mihailidis, Eliz-abeth Rochon, and Carol Leonard.
2012.
Commu-nication strategies for a computerized caregiver forindividuals with alzheimer?s disease.
In Proceed-ings of the Third Workshop on Speech and LanguageProcessing for Assistive Technologies (SLPAT2012)at the 13th Annual Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics (NAACL 2012), Montreal Canada, June.Frank Rudzicz.
2006.
Clavius: Bi-directional parsingfor generic multimodal interaction.
In Proceedingsof the joint meeting of the International Conferenceon Computational Linguistics and the Associationfor Computational Linguistics, Sydney Australia.Privender Saini, Boris de Ruyter, Panos Markopoulos,and Albert van Breemen.
2005.
Benefits of socialintelligence in home dialogue systems.
In Proceed-ings of INTERACT 2005, pages 510?521.A.
Serna, H. Pigot, and V. Rialle.
2007.
Modeling theprogression of alzheimer?s disease for cognitive as-sistance in smart homes.
User Modelling and User-Adapted Interaction, 17:415?438.Jeff A.
Small, Elaine S. Andersen, and Daniel Kem-pler.
1997.
Effects of working memory capacityon understanding rate-altered speech.
Aging, Neu-ropsychology, and Cognition, 4(2):126?139.M.
Snover, B. Dorr, and R. Schwartz.
2004.
Alexically-driven algorithm for disfluency detection.In ?Proceedings of HLT-NAACL 2004: Short Papers,pages 157?160.Adriana Tapus and Mohamed Chetouani.
2010.ROBADOM: the impact of a domestic robot on thepsychological and cognitive state of the elderly withmild cognitive impairment.
In Proceedings of theInternational Symposium on Quality of Life Technol-ogy Intelligent Systems for Better Living, Las VegasUSA, June.Cheryl K. Tomoeda, Kathryn A. Bayles, Daniel R.Boone, Alfred W. Kaszniak, and Thomas J. Slau-son.
1990.
Speech rate and syntactic complexityeffects on the auditory comprehension of alzheimerpatients.
Journal of Communication Disorders,23(2):151 ?
161.Keith Vertanen.
2006.
Baseline WSJ acoustic modelsfor HTK and Sphinx: Training recipes and recogni-tion experiments.
Technical report, Cavendish Lab-oratory, University of Cambridge.Rozanne Wilson, Elizabeth Rochon, Alex Mihailidis,and Carol Leonard.
2012.
Examining success ofcommunication strategies used by formal caregiversassisting individuals with alzheimer?s disease duringan activity of daily living.
Journal of Speech, Lan-guage, and Hearing Research, 55:328?341, April.Steve Young, Gunnar Evermann, Mark Gales, ThomasHain, Dan Kershaw, Xunying (Andrew) Liu, GarethMoore, Julian Odell, Dave Ollason and Dan Povey,Valtcho Valtchev, and Phil Woodland.
2006.
TheHTK Book (version 3.4).28
