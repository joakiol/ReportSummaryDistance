Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 175?183,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsFast Syntactic Analysis for Statistical Language Modelingvia Substructure Sharing and UptrainingAriya Rastrow, Mark Dredze, Sanjeev KhudanpurHuman Language Technology Center of ExcellenceCenter for Language and Speech Processing, Johns Hopkins UniversityBaltimore, MD USA{ariya,mdredze,khudanpur}@jhu.eduAbstractLong-span features, such as syntax, can im-prove language models for tasks such asspeech recognition and machine translation.However, these language models can be dif-ficult to use in practice because of the timerequired to generate features for rescoring alarge hypothesis set.
In this work, we pro-pose substructure sharing, which saves dupli-cate work in processing hypothesis sets withredundant hypothesis structures.
We applysubstructure sharing to a dependency parserand part of speech tagger to obtain significantspeedups, and further improve the accuracyof these tools through up-training.
When us-ing these improved tools in a language modelfor speech recognition, we obtain significantspeed improvements with bothN -best and hillclimbing rescoring, and show that up-trainingleads to WER reduction.1 IntroductionLanguage models (LM) are crucial components intasks that require the generation of coherent natu-ral language text, such as automatic speech recog-nition (ASR) and machine translation (MT).
Whiletraditional LMs use word n-grams, where the n ?
1previous words predict the next word, newer mod-els integrate long-span information in making deci-sions.
For example, incorporating long-distance de-pendencies and syntactic structure can help the LMbetter predict words by complementing the predic-tive power of n-grams (Chelba and Jelinek, 2000;Collins et al, 2005; Filimonov and Harper, 2009;Kuo et al, 2009).The long-distance dependencies can be modeledin either a generative or a discriminative framework.Discriminative models, which directly distinguishcorrect from incorrect hypothesis, are particularlyattractive because they allow the inclusion of arbi-trary features (Kuo et al, 2002; Roark et al, 2007;Collins et al, 2005); these models with syntactic in-formation have obtained state of the art results.However, both generative and discriminative LMswith long-span dependencies can be slow, for theyoften cannot work directly with lattices and requirerescoring large N -best lists (Khudanpur and Wu,2000; Collins et al, 2005; Kuo et al, 2009).
For dis-criminative models, this limitation applies to train-ing as well.
Moreover, the non-local features used inrescoring are usually extracted via auxiliary tools ?which in the case of syntactic features include part ofspeech taggers and parsers ?
from a set of ASR sys-tem hypotheses.
Separately applying auxiliary toolsto each N -best list hypothesis leads to major ineffi-ciencies as many hypotheses differ only slightly.Recent work on hill climbing algorithms for ASRlattice rescoring iteratively searches for a higher-scoring hypothesis in a local neighborhood of thecurrent-best hypothesis, leading to a much more ef-ficient algorithm in terms of the number, N , of hy-potheses evaluated (Rastrow et al, 2011b); the ideaalso leads to a discriminative hill climbing train-ing algorithm (Rastrow et al, 2011a).
Even so, thereliance on auxiliary tools slow LM application tothe point of being impractical for real time systems.While faster auxiliary tools are an option, they areusually less accurate.In this paper, we propose a general modifica-175tion to the decoders used in auxiliary tools to uti-lize the commonalities among the set of generatedhypotheses.
The key idea is to share substructurestates in transition based structured prediction al-gorithms, i.e.
algorithms where final structures arecomposed of a sequence of multiple individual deci-sions.
We demonstrate our approach on a local Per-ceptron based part of speech tagger (Tsuruoka et al,2011) and a shift reduce dependency parser (Sagaeand Tsujii, 2007), yielding significantly faster tag-ging and parsing of ASR hypotheses.
While thesesimpler structured prediction models are faster, wecompensate for the model?s simplicity through up-training (Petrov et al, 2010), yielding auxiliary toolsthat are both fast and accurate.
The result is signif-icant speed improvements and a reduction in worderror rate (WER) for both N -best list and the al-ready fast hill climbing rescoring.
The net resultis arguably the first syntactic LM fast enough to beused in a real time ASR system.2 Syntactic Language ModelsThere have been several approaches to include syn-tactic information in both generative and discrimi-native language models.For generative LMs, the syntactic informationmust be part of the generative process.
Structuredlanguage modeling incorporates syntactic parsetrees to identify the head words in a hypothesis formodeling dependencies beyond n-grams.
Chelbaand Jelinek (2000) extract the two previous exposedhead words at each position in a hypothesis, alongwith their non-terminal tags, and use them as con-text for computing the probability of the current po-sition.
Khudanpur and Wu (2000) exploit such syn-tactic head word dependencies as features in a maxi-mum entropy framework.
Kuo et al (2009) integratesyntactic features into a neural network LM for Ara-bic speech recognition.Discriminative models are more flexible sincethey can include arbitrary features, allowing fora wider range of long-span syntactic dependen-cies.
Additionally, discriminative models are di-rectly trained to resolve the acoustic confusion in thedecoded hypotheses of an ASR system.
This flexi-bility and training regime translate into better perfor-mance.
Collins et al (2005) uses the Perceptron al-gorithm to train a global linear discriminative modelwhich incorporates long-span features, such as head-to-head dependencies and part of speech tags.Our Language Model.
We work with a discrimi-native LM with long-span dependencies.
We use aglobal linear model with Perceptron training.
Werescore the hypotheses (lattices) generated by theASR decoder?in a framework most similar to thatof Rastrow et al (2011a).The LM score S(w,a) for each hypothesis w ofa speech utterance with acoustic sequence a is basedon the baseline ASR system score b(w,a) (initial n-gram LM score and the acoustic score) and ?0, theweight assigned to the baseline score.1 The score isdefined as:S(w,a) = ?0 ?
b(w,a) + F (w, s1, .
.
.
, sm)= ?0 ?
b(w,a) +d?i=1?i ?
?i(w, s1, .
.
.
, sm)where F is the discriminative LM?s score for thehypothesis w, and s1, .
.
.
, sm are candidate syntac-tic structures associated with w, as discussed be-low.
Since we use a linear model, the score is aweighted linear combination of the count of acti-vated features of the word sequence w and its as-sociated structures: ?i(w, s1, .
.
.
, sm).
Perceptrontraining learns the parameters ?.
The baseline scoreb(w,a) can be a feature, yielding the dot productnotation: S(w,a) = ??,?
(a,w, s1, .
.
.
, sm)?
OurLM uses features from the dependency tree and partof speech (POS) tag sequence.
We use the methoddescribed in Kuo et al (2009) to identify the twoprevious exposed head words, h?2, h?1, at each po-sition i in the input hypothesis and include the fol-lowing syntactic based features into our LM:1.
(h?2.w ?
h?1.w ?
wi) , (h?1.w ?
wi) , (wi)2.
(h?2.t ?
h?1.t ?
ti) , (h?1.t ?
ti) , (ti) , (tiwi)where h.w and h.t denote the word identity and thePOS tag of the corresponding exposed head word.2.1 Hill Climbing RescoringWe adopt the so called hill climbing framework ofRastrow et al (2011b) to improve both training andrescoring time as much as possible by reducing the1We tune ?0 on development data (Collins et al, 2005).176number N of explored hypotheses.
We summarizeit below for completeness.Given a speech utterance?s lattice L from a firstpass ASR decoder, the neighborhood N (w, i) of ahypothesis w = w1w2 .
.
.
wn at position i is de-fined as the set of all paths in the lattice that maybe obtained by editing wi: deleting it, substitutingit, or inserting a word to its left.
In other words,it is the ?distance-1-at-position i?
neighborhood ofw.
Given a position i in a word sequence w, allhypotheses in N (w, i) are rescored using the long-span model and the hypothesis w??
(i) with the high-est score becomes the new w. The process is re-peated with a new position ?
scanned left to right?
until w = w??
(1) = .
.
.
= w??
(n), i.e.
when witself is the highest scoring hypothesis in all its 1-neighborhoods, and can not be furthered improvedusing the model.
Incorporating this into trainingyields a discriminative hill climbing algorithm (Ras-trow et al, 2011a).3 Incorporating Syntactic StructuresLong-span models ?
generative or discriminative,N -best or hill climbing ?
rely on auxiliary tools,such as a POS tagger or a parser, for extractingfeatures for each hypothesis during rescoring, andduring training for discriminative models.
The top-m candidate structures associated with the ith hy-pothesis, which we denote as s1i , .
.
.
, smi , are gener-ated by these tools and used to score the hypothesis:F (wi, s1i , .
.
.
, smi ).
For example, sji can be a part ofspeech tag or a syntactic dependency.
We formallydefine this sequential processing as:w1tool(s)?????
s11, .
.
.
, sm1LM???
F (w1, s11, .
.
.
, sm1 )w2tool(s)?????
s12, .
.
.
, sm2LM???
F (w2, s12, .
.
.
, sm2 )...wktool(s)?????
s1k, .
.
.
, smkLM???
F (wk, s1k, .
.
.
, smk )Here, {w1, .
.
.
,wk} represents a set of ASR outputhypotheses that need to be rescored.
For each hy-pothesis, we apply an external tool (e.g.
parser) togenerate associated structures s1i , .
.
.
, smi (e.g.
de-pendencies.)
These are then passed to the languagemodel along with the word sequence for scoring.3.1 Substructure SharingWhile long-span LMs have been empirically shownto improve WER over n-gram LMs, the computa-tional burden prohibits long-span LMs in practice,particularly in real-time systems.
A major complex-ity factor is due to processing 100s or 1000s of hy-potheses for each speech utterance, even during hillclimbing, each of which must be POS tagged andparsed.
However, the candidate hypotheses of anutterance share equivalent substructures, especiallyin hill climbing methods due to the locality presentin the neighborhood generation.
Figure 1 demon-strates such repetition in an N -best list (N=10) anda hill climbing neighborhood hypothesis set for aspeech utterance from broadcast news.
For exam-ple, the word ?ENDORSE?
occurs within the samelocal context in all hypotheses and should receivethe same part of speech tag in each case.
Processingeach hypothesis separately wastes time.We propose a general algorithmic approach to re-duce the complexity of processing a hypothesis setby sharing common substructures among the hy-potheses.
Critically, unlike many lattice parsing al-gorithms, our approach is general and produces ex-act output.
We first present our approach and thendemonstrate its generality by applying it to a depen-dency parser and part of speech tagger.We work with structured prediction models thatproduce output from a series of local decisions: atransition model.
We begin in initial state pi0 andterminate in a possible final state pif .
All statesalong the way are chosen from the possible states?.
A transition (or action) ?
?
?
advances thedecoder from state to state, where the transition ?ichanges the state from pii to pii+1.
The sequenceof states {pi0 .
.
.
pii, pii+1 .
.
.
pif} can be mapped toan output (the model?s prediction.)
The choice ofaction ?
is given by a learning algorithm, such asa maximum-entropy classifier, support vector ma-chine or Perceptron, trained on labeled data.
Giventhe previous k actions up to pii, the classifier g :?
?
?k ?
R|?| assigns a score to each possi-ble action, which we can interpret as a probability:pg(?i|pii, ?i?1?i?2 .
.
.
?i?k).
These actions are ap-plied to transition to new states pii+1.
We note thatstate definitions can encode the k previous actions,which simplifies the probability to pg(?i|pii).
The177N -best list Hill climbing neighborhood(1) AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE(2) TO AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE(3) AL GORE HAS PROMISE THAT HE WOULD ENDORSE A CANDIDATE(4) SO AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE (1) YEAH FIFTY CENT GALLON NOMINATION WHICH WAS GREAT(5) IT?S AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE (2) YEAH FIFTY CENT A GALLON NOMINATION WHICH WAS GREAT(6) AL GORE HAS PROMISED HE WOULD ENDORSE A CANDIDATE (3) YEAH FIFTY CENT GOT A NOMINATION WHICH WAS GREAT(7) AL GORE HAS PROMISED THAT HE WOULD ENDORSE THE CANDIDATE(8) SAID AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE(9) AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE FOR(10) AL GORE HIS PROMISE THAT HE WOULD ENDORSE A CANDIDATEFigure 1: Example of repeated substructures in candidate hypotheses.score of the new state is thenp(pii+1) = pg(?i|pii) ?
p(pii) (1)Classification decisions require a feature represen-tation of pii, which is provided by feature functionsf : ??
Y , that map states to features.
Features areconjoined with actions for multi-class classification,so pg(?i|pii) = pg(f(pi) ?
?i), where ?
is a conjunc-tion operation.
In this way, states can be summarizedby features.Equivalent states are defined as two states pi andpi?
with an identical feature representation:pi ?
pi?
iff f(pi) = f(pi?
)If two states are equivalent, then g imposes the samedistribution over actions.
We can benefit from thissubstructure redundancy, both within and betweenhypotheses, by saving these distributions in mem-ory, sharing a distribution computed just once acrossequivalent states.
A similar idea of equivalent statesis used by Huang and Sagae (2010), except they useequivalence to facilitate dynamic programming forshift-reduce parsing, whereas we generalize it forimproving the processing time of similar hypothesesin general models.
Following Huang and Sagae, wedefine kernel features as the smallest set of atomicfeatures f?
(pi) such that,f?
(pi) = f?(pi?)
?
pi ?
pi?.
(2)Equivalent distributions are stored in a hash tableH : ??
?
?R; the hash keys are the states and thevalues are distributions2 over actions: {?, pg(?|pi)}.2For pure greedy search (deterministic search) we need onlyretain the best action, since the distribution is only used in prob-abilistic search, such as beam search or best-first algorithms.H caches equivalent states in a hypothesis set and re-sets for each new utterance.
For each state, we firstcheck H for equivalent states before computing theaction distribution; each cache hit reduces decod-ing time.
Distributing hypotheses wi across differ-ent CPU threads is another way to obtain speedups,and we can still benefit from substructure sharing bystoring H in shared memory.We use h(pi) = ?|f?
(pi)|i=1 int(f?i(pi)) as the hashfunction, where int(f?i(pi)) is an integer mapping ofthe ith kernel feature.
For integer typed featuresthe mapping is trivial, for string typed features (e.g.a POS tag identity) we use a mapping of the cor-responding vocabulary to integers.
We empiricallyfound that this hash function is very effective andyielded very few collisions.To apply substructure sharing to a transition basedmodel, we need only define the set of states ?
(in-cluding pi0 and pif ), actions ?
and kernel featurefunctions f?
.
The resulting speedup depends on theamount of substructure duplication among the hy-potheses, which we will show is significant for ASRlattice rescoring.
Note that our algorithm is not anapproximation; we obtain the same output {sji} aswe would without any sharing.
We now apply thisalgorithm to dependency parsing and POS tagging.3.2 Dependency ParsingWe use the best-first probabilistic shift-reduce de-pendency parser of Sagae and Tsujii (2007), atransition-based parser (Ku?bler et al, 2009) with aMaxEnt classifier.
Dependency trees are built byprocessing the words left-to-right and the classifierassigns a distribution over the actions at each step.States are defined as pi = {S,Q}: S is a stack of178Kernel features f?
(pi) for state pi = {S,Q}S = s0, s1, .
.
.
& Q = q0, q1, .
.
.
(1) s0.w s0.t s0.r (5) ts0?1s0.lch.t s0.lch.r ts1+1s0.rch.t s0.rch.r(2) s1.w s1.t s1.r (6) dist(s0, s1)s1.lch.t s1.lch.r dist(q0, s0)s1.rch.t s1.rch.r(3) s2.w s2.t s2.r(4) q0.w q0.t (7) s0.nchq1.w q1.t s1.nchq2.wTable 1: Kernel features for defining parser states.
si.wdenotes the head-word in a subtree and t its POS tag.si.lch and si.rch are the leftmost and rightmost childrenof a subtree.
si.r is the dependency label that relates asubtree head-word to its dependent.
si.nch is the numberof children of a subtree.
qi.w and qi.t are the word andits POS tag in the queue.
dist(s0,s1) is the linear distancebetween the head-words of s0 and s1.subtrees s0, s1, .
.
.
(s0 is the top tree) and Q arewords in the input word sequence.
The initial state ispi0 = {?, {w0, w1, .
.
.
}}, and final states occur whenQ is empty and S contains a single tree (the output).?
is determined by the set of dependency labelsr ?
R and one of three transition types:?
Shift: remove the head of Q (wj) and place it onthe top of S as a singleton tree (only wj .)?
Reduce-Leftr: replace the top two trees in S (s0and s1) with a tree formed by making the root ofs1 a dependent of the root of s0 with label r.?
Reduce-Rightr: same as Reduce-Leftr except re-verses s0 and s1.Table 1 shows the kernel features used in our de-pendency parser.
See Sagae and Tsujii (2007) for acomplete list of features.Goldberg and Elhadad (2010) observed that pars-ing time is dominated by feature extraction andscore calculation.
Substructure sharing reducesthese steps for equivalent states, which are persis-tent throughout a candidate set.
Note that there arefar fewer kernel features than total features, hencethe hash function calculation is very fast.We summarize substructure sharing for depen-dency parsing in Algorithm 1.
We extend the def-inition of states to be {S,Q, p} where p denotes thescore of the state: the probability of the action se-quence that resulted in the current state.
Also, fol-Algorithm 1 Best-first shift-reduce dependency parsingw ?
input hypothesisS0 = ?, Q0 = w, p0 = 1pi0 ?
{S0, Q0, p0} [initial state]H ?Hash table (??
??
R)Heap?
Heap for prioritizing states and performing best-first searchHeap.push(pi0) [initialize the heap]while Heap 6= ?
dopicurrent ?Heap.pop() [the best state so far]if picurrent = pif [if final state]return picurrent [terminate if final state]else ifH.find(picurrent)ActList?
H[picurrent] [retrieve action list from the hash table]else [need to construct action list]for all ?
?
?
[for all actions]p?
?
pg(?|picurrent) [action score]ActList.insert({?, p?
})H.insert(picurrent,ActList) [Store the action list into hash table]end iffor all {?, p?}
?
ActList [compute new states]pinew ?
picurrent ?
?Heap.push(pinew) [push to the heap]end whilelowing Sagae and Tsujii (2007) a heap is used tomaintain states prioritized by their scores, for apply-ing the best-first strategy.
For each step, a state fromthe top of the heap is considered and all actions (andscores) are either retrieved from H or computed us-ing g.3 We use pinew ?
picurrent ?
?
to denote theoperation of extending a state by an action ?
?
?4.3.3 Part of Speech TaggingWe use the part of speech (POS) tagger of Tsuruokaet al (2011), a transition based model with a Per-ceptron and a lookahead heuristic process.
The tag-ger processes w left to right.
States are defined aspii = {ci,w}: a sequence of assigned tags up to wi(ci = t1t2 .
.
.
ti?1) and the word sequence w. ?
isdefined simply as the set of possible POS tags (T )that can be applied.
The final state is reached onceall the positions are tagged.
For f we use the featuresof Tsuruoka et al (2011).
The kernel features aref?
(pii) = {ti?2, ti?1, wi?2, wi?1, wi, wi+1, wi+2}.While the tagger extracts prefix and suffix features,it suffices to look at wi for determining state equiv-alence.
The tagger is deterministic (greedy) in thatit only considers the best tag at each step, so we donot store scores.
However, this tagger uses a depth-3 Sagae and Tsujii (2007) use a beam strategy to increasespeed.
Search space pruning is achieved by filtering heap statesfor probability greater than 1b the probability of the most likelystate in the heap with the same number of actions.
We use b =100 for our experiments.4We note that while we have demonstrated substructuresharing for dependency parsing, the same improvements canbe made to a shift-reduce constituent parser (Sagae and Lavie,2006).179t2t1 ti 2 ti 1t1it2it|T |i t|T |i+1t1i+1t2i+1w1 w2 wi 1wi 2 wi wi+1 wi+2 wi+3?
?
??
?
?lookahead searchFigure 2: POS tagger with lookahead search of d=1.
Atwi the search considers the current state and next state.first search lookahead procedure to select the bestaction at each step, which considers future decisionsup to depth d5.
An example for d = 1 is shownin Figure 2.
Using d = 1 for the lookahead searchstrategy, we modify the kernel features since the de-cision forwi is affected by the state pii+1.
The kernelfeatures in position i should be f?
(pii) ?
f?(pii+1):f?
(pii) ={ti?2, ti?1, wi?2, wi?1, wi, wi+1, wi+2, wi+3}4 Up-TrainingWhile we have fast decoding algorithms for the pars-ing and tagging, the simpler underlying models canlead to worse performance.
Using more complexmodels with higher accuracy is impractical becausethey are slow.
Instead, we seek to improve the accu-racy of our fast tools.To achieve this goal we use up-training, in whicha more complex model is used to improve the accu-racy of a simpler model.
We are given two mod-els, M1 and M2, as well as a large collection ofunlabeled text.
Model M1 is slow but very accu-rate while M2 is fast but obtains lower accuracy.Up-training applies M1 to tag the unlabeled data,which is then used as training data for M2.
Likeself-training, a model is retrained on automatic out-put, but here the output comes form a more accuratemodel.
Petrov et al (2010) used up-training as adomain adaptation technique: a constituent parser ?which is more robust to domain changes ?
was usedto label a new domain, and a fast dependency parser5 Tsuruoka et al (2011) shows that the lookahead searchimproves the performance of the local ?history-based?
modelsfor different NLP taskswas trained on the automatically labeled data.
Weuse a similar idea where our goal is to recover theaccuracy lost from using simpler models.
Note thatwhile up-training uses two models, it differs fromco-training since we care about improving only onemodel (M2).
Additionally, the models can vary indifferent ways.
For example, they could be the samealgorithm with different pruning methods, whichcan lead to faster but less accurate models.We apply up-training to improve the accuracy ofboth our fast POS tagger and dependency parser.
Weparse a large corpus of text with a very accurate butvery slow constituent parser and use the resultingdata to up-train our tools.
We will demonstrate em-pirically that up-training improves these fast modelsto yield better WER results.5 Related WorkThe idea of efficiently processing a hypothesis set issimilar to ?lattice-parsing?, in which a parser con-sider an entire lattice at once (Hall, 2005; Chep-palier et al, 1999).
These methods typically con-strain the parsing space using heuristics, which areoften model specific.
In other words, they search inthe joint space of word sequences present in the lat-tice and their syntactic analyses; they are not guaran-teed to produce a syntactic analysis for all hypothe-ses.
In contrast, substructure sharing is a generalpurpose method that we have applied to two differ-ent algorithms.
The output is identical to processingeach hypothesis separately and output is generatedfor each hypothesis.
Hall (Hall, 2005) uses a latticeparsing strategy which aims to compute the marginalprobabilities of all word sequences in the lattice bysumming over syntactic analyses of each word se-quence.
The parser sums over multiple parses of aword sequence implicitly.
The lattice parser there-fore, is itself a language model.
In contrast, ourtools are completely separated from the ASR sys-tem, which allows the system to create whatever fea-tures are needed.
This independence means our toolsare useful for other tasks, such as machine transla-tion.
These differences make substructure sharing amore attractive option for efficient algorithms.While Huang and Sagae (2010) use the notion of?equivalent states?, they do so for dynamic program-ming in a shift-reduce parser to broaden the searchspace.
In contrast, we use the idea to identify sub-180structures across inputs, where our goal is efficientparsing in general.
Additionally, we extend the defi-nition of equivalent states to general transition basedstructured prediction models, and demonstrate ap-plications beyond parsing as well as the novel settingof hypothesis set parsing.6 ExperimentsOur ASR system is based on the 2007 IBMSpeech transcription system for the GALE Distilla-tion Go/No-go Evaluation (Chen et al, 2006) withstate of the art discriminative acoustic models.
SeeTable 2 for a data summary.
We use a modi-fied Kneser-Ney (KN) backoff 4-gram baseline LM.Word-lattices for discriminative training and rescor-ing come from this baseline ASR system.6 The long-span discriminative LM?s baseline feature weight(?0) is tuned on dev data and hill climbing (Rastrowet al, 2011a) is used for training and rescoring.
Thedependency parser and POS tagger are trained on su-pervised data and up-trained on data labeled by theCKY-style bottom-up constituent parser of Huang etal.
(2010), a state of the art broadcast news (BN)parser, with phrase structures converted to labeleddependencies by the Stanford converter.While accurate, the parser has a huge grammar(32GB) from using products of latent variable gram-mars and requires O(l3) time to parse a sentence oflength l. Therefore, we could not use the constituentparser for ASR rescoring since utterances can bevery long, although the shorter up-training text datawas not a problem.7 We evaluate both unlabeled(UAS) and labeled dependency accuracy (LAS).6.1 ResultsBefore we demonstrate the speed of our models, weshow that up-training can produce accurate and fastmodels.
Figure 3 shows improvements to parser ac-curacy through up-training for different amount of(randomly selected) data, where the last column in-dicates constituent parser score (91.4% UAS).
Weuse the POS tagger to generate tags for depen-dency training to match the test setting.
Whilethere is a large difference between the constituentand dependency parser without up-training (91.4%6For training a 3-gram LM is used to increase confusions.7Speech utterances are longer as they are not as effectivelysentence segmented as text.84.0?85.0?86.0?87.0?88.0?89.0?90.0?91.0?92.0?0M?
2.5M?
5M?
10M?
20M?
40M?
Cons?tuent?Parser?Accuracy?
(%)?Amount?of?Added?Uptraining?Data?Unlabeled?A?achment?Score?Labeled?A?achment?Score?Figure 3: Up-training results for dependency parsing forvarying amounts of data (number of words.)
The firstcolumn is the dependency parser with supervised trainingonly and the last column is the constituent parser (afterconverting to dependency trees.)vs.
86.2% UAS), up-training can cut the differ-ence by 44% to 88.5%, and improvements saturatearound 40m words (about 2m sentences.
)8 The de-pendency parser remains much smaller and faster;the up-trained dependency model is 700MB with6m features compared with 32GB for constituencymodel.
Up-training improves the POS tagger?s accu-racy from 95.9% to 97%, when trained on the POStags produced by the constituent parser, which has atagging accuracy of 97.2% on BN.We train the syntactic discriminative LM, withhead-word and POS tag features, using the fasterparser and tagger and then rescore the ASR hypothe-ses.
Table 3 shows the decoding speedups as well asthe WER reductions compared to the baseline LM.Note that up-training improvements lead to WER re-ductions.
Detailed speedups on substructure sharingare shown in Table 4; the POS tagger achieves a 5.3times speedup, and the parser a 5.7 speedup with-out changing the output.
We also observed speedupsduring training (not shown due to space.
)The above results are for the already fast hillclimbing decoding, but substructure sharing can alsobe used for N -best list rescoring.
Figure 4 (logarith-mic scale) illustrates the time for the parser and tag-ger to processN -best lists of varying size, with moresubstantial speedups for larger lists.
For example,for N=100 (a typical setting) the parsing time re-8Better performance is due to the exact CKY-style ?
com-pared with best-first and beam?
search and that the constituentparser uses the product of huge self-trained grammars.181Usage Data SizeAcoustic model training Hub4 acoustic train 153k uttr, 400 hrsBaseline LM training: modified KN 4-gram TDT4 closed captions+EARS BN03 closed caption 193m wordsDisc.
LM training: long-span w/hill climbing Hub4 (length <50) 115k uttr, 2.6m wordsBaseline feature (?0) tuning dev04f BN data 2.5 hrsSupervised training: dep.
parser, POS tagger Ontonotes BN treebank+ WSJ Penn treebank 1.3m words, 59k sent.Supervised training: constituent parser Ontonotes BN treebank + WSJ Penn treebank 1.3m words, 59k sent.Up-training: dependency parser, POS tagger TDT4 closed captions+EARS BN03 closed caption 193m words availableEvaluation: up-training BN treebank test (following Huang et al (2010)) 20k words, 1.1k sent.Evaluation: ASR transcription rt04 BN evaluation 4 hrs, 45k wordsTable 2: A summary of the data for training and evaluation.
The Ontonotes corpus is from Weischedel et al (2008).10?100?1000?10000?100000?1000000?1?
10?
100?
1000?Elapsed?Time?(sec)?N-??best?Size?(N)?No?Sharing?Substructure?Sharing?(a)1?10?100?1000?10000?1?
10?
100?
1000?Elapsed?Time?(sec)?N-??best?Size?(N)?No?Sharing?Substructure?Sharing?
(b)Figure 4: Elapsed time for (a) parsing and (b) POS tagging the N -best lists with and without substructure sharing.Substr.
Share (sec)LM WER No YesBaseline 4-gram 15.1 - -Syntactic LM 14.88,658 1,648+ up-train 14.6Table 3: Speedups and WER for hill climbing rescor-ing.
Substructure sharing yields a 5.3 times speedup.
Thetimes for with and without up-training are nearly identi-cal, so we include only one set for clarity.
Time spentis dominated by the parser, so the faster parser accountsfor much of the overall speedup.
Timing information in-cludes neighborhood generation and LM rescoring, so itis more than the sum of the times in Table 4.duces from about 20,000 seconds to 2,700 seconds,about 7.4 times as fast.7 ConclusionThe computational complexity of accurate syntac-tic processing can make structured language modelsimpractical for applications such as ASR that requirescoring hundreds of hypotheses per input.
We haveSubstr.
Share SpeedupNo YesParser 8,237.2 1,439.5 5.7POS tagger 213.3 40.1 5.3Table 4: Time in seconds for the parser and POS taggerto process hypotheses during hill climbing rescoring.presented substructure sharing, a general frameworkthat greatly improves the speed of syntactic toolsthat process candidate hypotheses.
Furthermore, weachieve improved performance through up-training.The result is a large speedup in rescoring time, evenon top of the already fast hill climbing framework,and reductions in WER from up-training.
Our re-sults make long-span syntactic LMs practical forreal-time ASR, and can potentially impact machinetranslation decoding as well.AcknowledgmentsThanks to Kenji Sagae for sharing his shift-reducedependency parser and the anonymous reviewers forhelpful comments.182ReferencesC.
Chelba and F. Jelinek.
2000.
Structured lan-guage modeling.
Computer Speech and Language,14(4):283?332.S.
Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon,H.
Soltau, and G. Zweig.
2006.
Advances in speechtranscription at IBM under the DARPA EARS pro-gram.
IEEE Transactions on Audio, Speech and Lan-guage Processing, pages 1596?1608.J.
Cheppalier, M. Rajman, R. Aragues, and A. Rozen-knop.
1999.
Lattice parsing for speech recognition.In Sixth Conference sur le Traitement Automatique duLangage Naturel (TANL?99).M Collins, B Roark, and M Saraclar.
2005.
Discrimina-tive syntactic language modeling for speech recogni-tion.
In ACL.Denis Filimonov and Mary Harper.
2009.
A jointlanguage model with fine-grain syntactic tags.
InEMNLP.Yoav Goldberg and Michael Elhadad.
2010.
An Ef-ficient Algorithm for Easy-First Non-Directional De-pendency Parsing.
In Proc.
HLT-NAACL, numberJune, pages 742?750.Keith B Hall.
2005.
Best-first word-lattice parsing:techniques for integrated syntactic language modeling.Ph.D.
thesis, Brown University.L.
Huang and K. Sagae.
2010.
Dynamic Programmingfor Linear-Time Incremental Parsing.
In Proceedingsof ACL.Zhongqiang Huang, Mary Harper, and Slav Petrov.
2010.Self-training with Products of Latent Variable Gram-mars.
In Proc.
EMNLP, number October, pages 12?22.S.
Khudanpur and J. Wu.
2000.
Maximum entropy tech-niques for exploiting syntactic, semantic and colloca-tional dependencies in language modeling.
ComputerSpeech and Language, pages 355?372.S.
Ku?bler, R. McDonald, and J. Nivre.
2009.
Depen-dency parsing.
Synthesis Lectures on Human Lan-guage Technologies, 2(1):1?127.Hong-Kwang Jeff Kuo, Eric Fosler-Lussier, Hui Jiang,and Chin-Hui Lee.
2002.
Discriminative training oflanguage models for speech recognition.
In ICASSP.H.
K. J. Kuo, L. Mangu, A. Emami, I. Zitouni, andL.
Young-Suk.
2009.
Syntactic features for Arabicspeech recognition.
In Proc.
ASRU.Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, andHiyan Alshawi.
2010.
Uptraining for accurate deter-ministic question parsing.
In Proceedings of the 2010Conference on Empirical Methods in Natural Lan-guage Processing, pages 705?713, Cambridge, MA,October.
Association for Computational Linguistics.Ariya Rastrow, Mark Dredze, and Sanjeev Khudanpur.2011a.
Efficient discrimnative training of long-spanlanguage models.
In IEEE Workshop on AutomaticSpeech Recognition and Understanding (ASRU).Ariya Rastrow, Markus Dreyer, Abhinav Sethy, San-jeev Khudanpur, Bhuvana Ramabhadran, and MarkDredze.
2011b.
Hill climbing on speech lattices : Anew rescoring framework.
In ICASSP.Brian Roark, Murat Saraclar, and Michael Collins.
2007.Discriminative n-gram language modeling.
ComputerSpeech & Language, 21(2).K.
Sagae and A. Lavie.
2006.
A best-first probabilis-tic shift-reduce parser.
In Proc.
ACL, pages 691?698.Association for Computational Linguistics.K.
Sagae and J. Tsujii.
2007.
Dependency parsingand domain adaptation with LR models and parser en-sembles.
In Proc.
EMNLP-CoNLL, volume 7, pages1044?1050.Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichiKazama.
2011.
Learning with Lookahead :Can History-Based Models Rival Globally OptimizedModels ?
In Proc.
CoNLL, number June, pages 238?246.Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,Martha Palmer, Nianwen Xue, Mitchell Marcus, AnnTaylor, Craig Greenberg, Eduard Hovy, Robert Belvin,and Ann Houston, 2008.
OntoNotes Release 2.0.
Lin-guistic Data Consortium, Philadelphia.183
