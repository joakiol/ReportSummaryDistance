Bootstrapping POS taggers using Unlabelled DataStephen Clark, James R. Curran and Miles OsborneSchool of InformaticsUniversity of Edinburgh2 Buccleuch Place, Edinburgh.
EH8 9LWfstephenc,jamesc,osborneg@cogsci.ed.ac.ukAbstractThis paper investigates booststrapping part-of-speech taggers using co-training, in which twotaggers are iteratively re-trained on each other?soutput.
Since the output of the taggers is noisy,there is a question of which newly labelled ex-amples to add to the training set.
We investi-gate selecting examples by directly maximisingtagger agreement on unlabelled data, a methodwhich has been theoretically and empiricallymotivated in the co-training literature.
Ourresults show that agreement-based co-trainingcan significantly improve tagging performancefor small seed datasets.
Further results showthat this form of co-training considerably out-performs self-training.
However, we find thatsimply re-training on all the newly labelled datacan, in some cases, yield comparable results toagreement-based co-training, with only a frac-tion of the computational cost.1 IntroductionCo-training (Blum and Mitchell, 1998), and several vari-ants of co-training, have been applied to a number ofNLP problems, including word sense disambiguation(Yarowsky, 1995), named entity recognition (Collinsand Singer, 1999), noun phrase bracketing (Pierce andCardie, 2001) and statistical parsing (Sarkar, 2001;Steedman et al, 2003).
In each case, co-training wasused successfully to bootstrap a model from only a smallamount of labelled data and a much larger pool of un-labelled data.
Previous co-training approaches have typ-ically used the score assigned by the model as an indi-cator of the reliability of a newly labelled example.
Inthis paper we take a different approach, based on theoret-ical work by Dasgupta et al (2002) and Abney (2002), inwhich newly labelled training examples are selected us-ing a greedy algorithm which explicitly maximises thePOS taggers?
agreement on unlabelled data.We investigate whether co-training based upon di-rectly maximising agreement can be successfully ap-plied to a pair of part-of-speech (POS) taggers: theMarkov model TNT tagger (Brants, 2000) and the max-imum entropy C&C tagger (Curran and Clark, 2003).There has been some previous work on boostrap-ping POS taggers (e.g., Zavrel and Daelemans (2000) andCucerzan and Yarowsky (2002)), but to our knowledgeno previous work on co-training POS taggers.The idea behind co-training the POS taggers is verysimple: use output from the TNT tagger as additionallabelled data for the maximum entropy tagger, and viceversa, in the hope that one tagger can learn useful infor-mation from the output of the other.
Since the output ofboth taggers is noisy, there is a question of which newlylabelled examples to add to the training set.
The addi-tional data should be accurate, but also useful, providingthe tagger with new information.
Our work differs fromthe Blum and Mitchell (1998) formulation of co-trainingby using two different learning algorithms rather than twoindependent feature sets (Goldman and Zhou, 2000).Our results show that, when using very small amountsof manually labelled seed data and a much larger amountof unlabelled material, agreement-based co-training cansignificantly improve POS tagger accuracy.
We also showthat simply re-training on all of the newly labelled datais surprisingly effective, with performance depending onthe amount of newly labelled data added at each itera-tion.
For certain sizes of newly labelled data, this sim-ple approach is just as effective as the agreement-basedmethod.
We also show that co-training can still benefitboth taggers when the performance of one tagger is ini-tially much better than the other.We have also investigated whether co-training can im-prove the taggers already trained on large amounts ofEdmonton, May-June 2003held at HLT-NAACL 2003 , pp.
49-55Proceeings of the Seventh CoNLL conferencemanually annotated data.
Using standard sections of theWSJ Penn Treebank as seed data, we have been unableto improve the performance of the taggers using self-training or co-training.Manually tagged data for English exists in large quan-tities, which means that there is no need to create taggersfrom small amounts of labelled material.
However, ourexperiments are relevant for languages for which thereis little or no annotated data.
We only perform the ex-periments in English for convenience.
Our experimentscan also be seen as a vehicle for exploring aspects of co-training.2 Co-trainingGiven two (or more) ?views?
(as described inBlum and Mitchell (1998)) of a classification task,co-training can be informally described as follows: Learn separate classifiers for each view using asmall amount of labelled seed data. Use each classifier to label some previously unla-belled data. For each classifier, add some subset of the newly la-belled data to the training data. Retrain the classifiers and repeat.The intuition behind the algorithm is that each classi-fier is providing extra, informative labelled data for theother classifier(s).
Blum and Mitchell (1998) derive PAC-like guarantees on learning by assuming that the twoviews are individually sufficient for classification and thetwo views are conditionally independent given the class.Collins and Singer (1999) present a variant of theBlum and Mitchell algorithm, which directly maximisesan objective function that is based on the level ofagreement between the classifiers on unlabelled data.Dasgupta et al (2002) provide a theoretical basis for thisapproach by providing a PAC-like analysis, using thesame independence assumption adopted by Blum andMitchell.
They prove that the two classifiers have lowgeneralisation error if they agree on unlabelled data.Abney (2002) argues that the Blum and Mitchell in-dependence assumption is very restrictive and typicallyviolated in the data, and so proposes a weaker indepen-dence assumption, for which the Dasgupta et al (2002)results still hold.
Abney also presents a greedy algorithmthat maximises agreement on unlabelled data, which pro-duces comparable results to Collins and Singer (1999) ontheir named entity classification task.Goldman and Zhou (2000) show that, if the newly la-belled examples used for re-training are selected care-fully, co-training can still be successful even when theviews used by the classifiers do not satisfy the indepen-dence assumption.In remainder of the paper we present a practicalmethod for co-training POS taggers, and investigate theextent to which example selection based on the work ofDasgupta et al and Abney can be effective.3 The POS taggersThe two POS taggers used in the experiments are TNT, apublicly available Markov model tagger (Brants, 2000),and a reimplementation of the maximum entropy (ME)tagger MXPOST (Ratnaparkhi, 1996).
The ME tagger,which we refer to as C&C, uses the same features as MX-POST, but is much faster for training and tagging (Cur-ran and Clark, 2003).
Fast training and tagging timesare important for the experiments performed here, sincethe bootstrapping process can require many tagging andtraining iterations.The model used by TNT is a standard tagging Markovmodel, consisting of emission probabilities, and transi-tion probabilities based on trigrams of tags.
It also dealswith unknown words using a suffix analysis of the targetword (the word to be tagged).
TNT is very fast for bothtraining and tagging.The C&C tagger differs in a number of ways fromTNT.
First, it uses a conditional model of a tag sequencegiven a string, rather than a joint model.
Second, MEmodels are used to define the conditional probabilities ofa tag given some context.
The advantage of ME mod-els over the Markov model used by TNT is that arbitraryfeatures can easily be included in the context; so as wellas considering the target word and the previous two tags(which is the information TNT uses), the ME models alsoconsider the words either side of the target word and, forunknown and infrequent words, various properties of thestring of the target word.A disadvantage is that the training times for ME mod-els are usually relatively slow, especially with iterativescaling methods (see Malouf (2002) for alternative meth-ods).
Here we use Generalised Iterative Scaling (Dar-roch and Ratcliff, 1972), but our implementation is muchfaster than Ratnaparkhi?s publicly available tagger.
TheC&C tagger trains in less than 7 minutes on the 1 millionwords of the Penn Treebank, and tags slightly faster thanTNT.Since the taggers share many common features, onemight think they are not different enough for effectiveco-training to be possible.
In fact, both taggers are suffi-ciently different for co-training to be effective.
Section 4shows that both taggers can benefit significantly from theinformation contained in the other?s output.The performance of the taggers on section 00 of theWSJ Penn Treebank is given in Table 1, for different seedset sizes (number of sentences).
The seed data is takenTagger 50 seed 500 seed ?
40,000 seedTNT 81.3 91.0 96.5C&C 73.2 88.3 96.8Table 1: Tagger performance for different seed setsfrom sections 2?21 of the Treebank.
The table shows thatthe performance of TNT is significantly better than theperformance of C&C when the size of the seed data isvery small.4 ExperimentsThe co-training framework uses labelled examples fromone tagger as additional training data for the other.
Forthe purposes of this paper, a labelled example is a taggedsentence.
We chose complete sentences, rather thansmaller units, because this simplifies the experiments andthe publicly available version of TNT requires completetagged sentences for training.
It is possible that co-training with sub-sentential units might be more effective,but we leave this as future work.The co-training process is given in Figure 1.
Ateach stage in the process there is a cache of unla-belled sentences (selected from the total pool of un-labelled sentences) which is labelled by each tagger.The cache size could be increased at each iteration,which is a common practice in the co-training litera-ture.
A subset of those sentences labelled by TNT isthen added to the training data for C&C, and vice versa.Blum and Mitchell (1998) use the combined set of newlylabelled examples for training each view, but we fol-low Goldman and Zhou (2000) in using separate labelledsets.
In the remainder of this section we consider two pos-sible methods for selecting a subset.
The cache is clearedafter each iteration.There are various ways to select the labelled examplesfor each tagger.
A typical approach is to select those ex-amples assigned a high score by the relevant classifier,under the assumption that these examples will be the mostreliable.
A score-based selection method is difficult toapply in our experiments, however, since TNT does notprovide scores for tagged sentences.We therefore tried two alternative selection methods.The first is to simply add all of the cache labelled by onetagger to the training data of the other.
We refer to thismethod as naive co-training.
The second, more sophisti-cated, method is to select that subset of the labelled cachewhich maximises the agreement of the two taggers on un-labelled data.
We call this method agreement-based co-training.
For a large cache the number of possible subsetsmakes exhaustive search intractable, and so we randomlysample the subsets.S is a seed set of labelled sentencesLT is labelled training data for TNTLC is labelled training data for C&CU is a large set of unlabelled sentencesC is a cache holding a small subset of Uinitialise:LT ?
LC ?
STrain TNT and C&C on Sloop:Partition U into the disjoint sets C and U?.Label C with TNT and C&CSelect sentences labelled by TNT and add to LCTrain C&C on LCSelect sentences labelled by C&C and add to LTTrain TNT on LTU = U?.Until U is emptyFigure 1: The general co-training processC is a cache of sentences labelled by the other taggerU is a set of sentences, used for measuring agreementinitialise:cmax ?
?
; Amax ?
0Repeat n times:Randomly sample c ?
CRetrain current tagger using c as additional dataif new agreement rate, A, on U > AmaxAmax ?
A; cmax ?
creturn cmaxFigure 2: Agreement-based example selectionThe pseudo-code for the agreement-based selectionmethod is given in Figure 2.
The current tagger is theone being retrained, while the other tagger is kept static.The co-training process uses the selection method for se-lecting sentences from the cache (which has been labelledby one of the taggers).
Note that during the selection pro-cess, we repeatedly sample from all possible subsets ofthe cache; this is done by first randomly choosing thesize of the subset and then randomly choosing sentencesbased on the size.
The number of subsets we consider isdetermined by the number of times the loop is traversedin Figure 2.If TNT is being trained on the output of C&C, then themost recent version of C&C is used to measure agreement(and vice versa); so we first attempt to improve one tag-ger, then the other, rather than both at the same time.
Theagreement rate of the taggers on unlabelled sentences isthe per-token agreement rate; that is, the number of timeseach word in the unlabelled set of sentences is assignedthe same tag by both taggers.For the small seed set experiments, the seed data wasan arbitrarily chosen subset of sections 10?19 of theWSJ Penn Treebank; the unlabelled training data wastaken from 50, 000 sentences of the 1994 WSJ sectionof the North American News Corpus (NANC); and theunlabelled data used to measure agreement was around10, 000 sentences from sections 1?5 of the Treebank.Section 00 of the Treebank was used to measure the ac-curacy of the taggers.
The cache size was 500 sentences.4.1 Self-Training and Agreement-based Co-trainingResultsFigure 3 shows the results for self-training, in which eachtagger is simply retrained on its own labelled cache ateach round.
(By round we mean the re-training of a sin-gle tagger, so there are two rounds per co-training itera-tion.)
TNT does improve using self-training, from 81.4%to 82.2%, but C&C is unaffected.
Re-running these ex-periments using a range of unlabelled training sets, froma variety of sources, showed similar behaviour.0.730.740.750.760.770.780.790.80.810.820.830 5 10 15 20 25 30 35 40 45 50AccuracyNumber of roundsTnTC&CFigure 3: Self-training TNT and C&C (50 seed sen-tences).
The upper curve is for TNT; the lower curve isfor C&C.Figure 4 gives the results for the greedy agreement co-training, using a cache size of 500 and searching through100 subsets of the labelled cache to find the one that max-imises agreement.
Co-training improves the performanceof both taggers: TNT improves from 81.4% to 84.9%,and C&C improves from 73.2% to 84.3% (an error re-duction of over 40%).Figures 5 and 6 show the self-training results andagreement-based results when a larger seed set, of 500sentences, is used for each tagger.
In this case, self-training harms TNT and C&C is again unaffected.
Co-training continues to be beneficial.Figure 7 shows how the size of the labelled data set (thenumber of sentences) grows for each tagger per round.0.720.740.760.780.80.820.840.860 5 10 15 20 25 30 35 40 45 50AccuracyNumber of roundsTnTC&CFigure 4: Agreement-based co-training betweenTNT and C&C (50 seed sentences).
The curve thatstarts at a higher value is for TNT.0.880.8850.890.8950.90.9050.910.9150 5 10 15 20 25 30 35 40 45 50AccuracyNumber of roundsTnTC&CFigure 5: Self-training TNT and C&C (500 seed sen-tences).
The upper curve is for TNT; the lower curve isfor C&C.Towards the end of the co-training run, more material isbeing selected for C&C than TNT.
The experiments us-ing a seed set size of 50 showed a similar trend, but thedifference between the two taggers was less marked.
Byexamining the subsets chosen from the labelled cache ateach round, we also observed that a large proportion ofthe cache was being selected for both taggers.4.2 Naive Co-training ResultsAgreement-based co-training for POS taggers is effectivebut computationally demanding.
The previous two agree-ment maximisation experiments involved retraining eachtagger 2, 500 times.
Given this, and the observation thatmaximisation generally has a preference for selecting alarge proportion of the labelled cache, we looked at naiveco-training: simply retraining upon all available mate-0.880.8850.890.8950.90.9050.910.9150.920 5 10 15 20 25 30 35 40 45 50AccuracyNumber of roundsTnTC&CFigure 6: Agreement-based co-training betweenTNT and C&C (500 seed sentences).
The curve thatstarts at a higher value is for TNT.0200040006000800010000120000 5 10 15 20 25 30 35 40 45 50C&CtntTnTFigure 7: Growth in training-set sizes for co-trainingTNT and C&C (500 seed sentences).
The upper curveis for C&C.rial (i.e.
the whole cache) at each round.
Table 2 showsthe naive co-training results after 50 rounds of co-trainingwhen varying the size of the cache.
50 manually labelledsentences were used as the seed material.
Table 3 showsresults for the same experiment, but this time with a seedset of 500 manually labelled sentences.We see that naive co-training improves as the cachesize increases.
For a large cache, the performance lev-els for naive co-training are very similar to those pro-duced by our agreement-based co-training method.
Af-ter 50 rounds of co-training using 50 seed sentences,the agreement rates for naive and agreement-based co-training were very similar: from an initial value of 73%to 97% agreement.Naive co-training is more efficient than agreement-based co-training.
For the parameter settings used inAmount added TNT C&C0 81.3 73.250 82.9 82.7100 83.5 83.3150 84.4 84.3300 85.0 84.9500 85.3 85.1Table 2: Naive co-training accuracy results when varyingthe amount added after each round (50 seed sentences)Amount added TNT C&C0 91.0 88.3100 92.0 91.9300 92.0 91.9500 92.1 92.01000 92.0 91.9Table 3: Naive co-training accuracy results when varyingthe amount added after each round (500 seed sentences)the previous experiments, agreement-based co-trainingrequired the taggers to be re-trained 10 to 100 timesmore often then naive co-training.
There are advan-tages to agreement-based co-training, however.
First,the agreement-based method dynamically selects the bestsample at each stage, which may not be the whole cache.In particular, when the agreement rate cannot be im-proved upon, the selected sample can be rejected.
Fornaive co-training, new samples will always be added,and so there is a possibility that the noise accumulatedat later stages will start to degrade performance (seePierce and Cardie (2001)).
Second, for naive co-training,the optimal amount of data to be added at each round (i.e.the cache size) is a parameter that needs to be determinedon held out data, whereas the agreement-based methoddetermines this automatically.4.3 Larger-Scale ExperimentsWe also performed a number of experiments using muchmore unlabelled training material than before.
Insteadof using 50, 000 sentences from the 1994 WSJ section ofthe North American News Corpus, we used 417, 000 sen-tences (from the same section) and ran the experimentsuntil the unlabelled data had been exhausted.One experiment used naive co-training, with 50 seedsentences and a cache of size 500.
This led to an agree-ment rate of 99%, with performance levels of 85.4% and85.4% for TNT and C&C respectively.
230, 000 sen-tences (?
5 million words) had been processed and wereused as training material by the taggers.
The other ex-periment used our agreement-based co-training approach(50 seed sentences, cache size of 1, 000 sentences, explor-ing at most 10 subsets in the maximisation process perround).
The agreement rate was 98%, with performancelevels of 86.0% and 85.9% for both taggers.
124, 000sentences had been processed, of which 30, 000 labelledsentences were selected for training TNT and 44, 000 la-belled sentences were selected for training C&C.Co-training using this much larger amount of unla-belled material did improve our previously mentioned re-sults, but not by a large margin.4.4 Co-training using Imbalanced ViewsIt is interesting to consider what happens when one viewis initially much more accurate than the other view.
Wetrained one of the taggers on much more labelled seeddata than the other, to see how this affects the co-trainingprocess.
Both taggers were initialised with either 500 or50 seed sentences, and agreement-based co-training wasapplied, using a cache size of 500 sentences.
The resultsare shown in Table 4.Seed material Initial Perf Final PerfTNT C&C TNT C&C TNT C&C50 500 81.3 88.3 90.0 89.4500 50 91.0 73.2 91.3 91.3Table 4: Co-training Results for Imbalanced ViewsCo-training continues to be effective, even when thetwo taggers are imbalanced.
Also, the final performanceof the taggers is around the same value, irrespective ofthe direction of the imbalance.4.5 Large Seed ExperimentsAlthough bootstrapping from unlabelled data is particu-larly valuable when only small amounts of training ma-terial are available, it is also interesting to see if self-training or co-training can improve state of the art POStaggers.For these experiments, both C&C and TNT were ini-tially trained on sections 00?18 of the WSJ Penn Tree-bank, and sections 19?21 and 22?24 were used as thedevelopment and test sets.
The 1994?1996 WSJ textfrom the NANC was used as unlabelled material to fill thecache.The cache size started out at 8000 sentences and in-creased by 10% in each round to match the increasinglabelled training data.
In each round of self-training ornaive co-training 10% of the cache was randomly se-lected and added to the labelled training data.
The ex-periments ran for 40 rounds.The performance of the different training regimes islisted in Table 5.
These results show no significant im-provement using either self-training or co-training withvery large seed datasets.
Self-training shows only a slightMethod WSJ19?21 WSJ22?24C&C TNT C&C TNTInitial 96.71 96.50 96.78 96.46Self-train 96.77 96.45 96.87 96.42Naive co-train 96.74 96.48 96.76 96.46Table 5: Performance with large seed setsimprovement for C&C1 while naive co-training perfor-mance is always worse.5 ConclusionWe have shown that co-training is an effective techniquefor bootstrapping POS taggers trained on small amountsof labelled data.
Using unlabelled data, we are able toimprove TNT from 81.3% to 86.0%, whilst C&C showsa much more dramatic improvement of 73.2% to 85.9%.Our agreement-based co-training results supportthe theoretical arguments of Abney (2002) andDasgupta et al (2002), that directly maximising theagreement rates between the two taggers reduces gen-eralisation error.
Examination of the selected subsetsshowed a preference for a large proportion of the cache.This led us to propose a naive co-training approach,which significantly reduced the computational costwithout a significant performance penalty.We also showed that naive co-training was unable toimprove the performance of the taggers when they hadalready been trained on large amounts of manually anno-tated data.
It is possible that agreement-based co-training,using more careful selection, would result in an improve-ment.
We leave these experiments to future work, butnote that there is a large computational cost associatedwith such experiments.The performance of the bootstrapped taggers is stilla long way behind a tagger trained on a large amountof manually annotated data.
This finding is in accordwith earlier work on bootstrapping taggers using EM (El-worthy, 1994; Merialdo, 1994).
An interesting questionwould be to determine the minimum number of manuallylabelled examples that need to be used to seed the sys-tem before we can achieve comparable results as usingall available manually labelled sentences.For our experiments, co-training never led to a de-crease in performance, regardless of the number of itera-tions.
The opposite behaviour has been observed in otherapplications of co-training (Pierce and Cardie, 2001).Whether this robustness is a property of the tagging prob-lem or our approach is left for future work.1This is probably by chance selection of better subsets.AcknowledgementsThis work has grown out of many fruitful discus-sions with the 2002 JHU Summer Workshop team thatworked on weakly supervised bootstrapping of statisticalparsers.
The first author was supported by EPSRC grantGR/M96889, and the second author by a Commonwealthscholarship and a Sydney University Travelling scholar-ship.
We would like to thank the anonymous reviewersfor their helpful comments, and also Iain Rae for com-puter support.ReferencesSteven Abney.
2002.
Bootstrapping.
In Proceedings ofthe 40th Annual Meeting of the Association for Compu-tational Linguistics, pages 360?367, Philadelphia, PA.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Proceed-ings of the 11th Annual Conference on ComputationalLearning Theory, pages 92?100, Madisson, WI.Thorsten Brants.
2000.
TnT - a statistical part-of-speechtagger.
In Proceedings of the 6th Conference on Ap-plied Natural Language Processing, pages 224?231.Michael Collins and Yoram Singer.
1999.
Unsupervisedmodels for named entity classification.
In Proceedingsof the Empirical Methods in NLP Conference, pages100?110, University of Maryland, MD.Silviu Cucerzan and David Yarowsky.
2002.
Boot-strapping a multilingual part-of-speech tagger in oneperson-day.
In Proceedings of the 6th Workshop onComputational Language Learning, Taipei, Taiwan.James R. Curran and Stephen Clark.
2003.
InvestigatingGIS and Smoothing for Maximum Entropy Taggers.In Proceedings of the 11th Annual Meeting of the Eu-ropean Chapter of the Association for ComputationalLinguistics, Budapest, Hungary.
(to appear).J.
N. Darroch and D. Ratcliff.
1972.
Generalized itera-tive scaling for log-linear models.
The Annals of Math-ematical Statistics, 43(5):1470?1480.Sanjoy Dasgupta, Michael Littman, and DavidMcAllester.
2002.
PAC generalization boundsfor co-training.
In T. G. Dietterich, S. Becker,and Z. Ghahramani, editors, Advances in NeuralInformation Processing Systems 14, pages 375?382,Cambridge, MA.
MIT Press.D.
Elworthy.
1994.
Does Baum-Welch re-estimationhelp taggers?
In Proceedings of the 4th Conferenceon Applied Natural Language Processing, pages 53?58, Stuttgart, Germany.Sally Goldman and Yan Zhou.
2000.
Enhancing super-vised learning with unlabeled data.
In Proceedings ofthe 17th International Conference on Machine Learn-ing, Stanford, CA.Robert Malouf.
2002.
A comparison of algorithmsfor maximum entropy parameter estimation.
In Pro-ceedings of the Sixth Workshop on Natural LanguageLearning, pages 49?55, Taipei, Taiwan.Bernard Merialdo.
1994.
Tagging English text witha probabilistic model.
Computational Linguistics,20(2):155?171.David Pierce and Claire Cardie.
2001.
Limitations ofco-training for natural language learning from largedatasets.
In Proceedings of the Empirical Methods inNLP Conference, Pittsburgh, PA.Adwait Ratnaparkhi.
1996.
A maximum entropy part-of-speech tagger.
In Proceedings of the EMNLP Con-ference, pages 133?142, Philadelphia, PA.Anoop Sarkar.
2001.
Applying co-training methods tostatistical parsing.
In Proceedings of the 2nd AnnualMeeting of the NAACL, pages 95?102, Pittsburgh, PA.Mark Steedman, Miles Osborne, Anoop Sarkar, StephenClark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,Steven Baker, and Jeremiah Crim.
2003.
Bootstrap-ping statistical parsers from small datasets.
In Pro-ceedings of the 11th Annual Meeting of the EuropeanChapter of the Association for Computational Linguis-tics, Budapest, Hungary.
(to appear).David Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In Proceed-ings of the 33rd Annual Meeting of the Associationfor Computational Linguistics, pages 189?196, Cam-bridge, MA.Jakub Zavrel and Walter Daelemans.
2000.
Bootstrap-ping a tagged corpus through combination of exist-ing heterogeneous taggers.
In Proceedings of the 2ndInternational Conference on Language Resources andEvaluation, pages 17?20, Athens, Greece.
