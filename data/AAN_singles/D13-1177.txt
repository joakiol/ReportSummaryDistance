Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1710?1720,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsLearning Biological Processes with Global ConstraintsAju Thalappillil Scaria?, Jonathan Berant?, Mengqiu Wang and Christopher D. ManningStanford University, StanfordJustin Lewis and Brittany HardingUniversity of Washington, SeattlePeter ClarkAllen Institute for Artificial Intelligence, SeattleAbstractBiological processes are complex phenom-ena involving a series of events that are re-lated to one another through various relation-ships.
Systems that can understand and rea-son over biological processes would dramat-ically improve the performance of semanticapplications involving inference such as ques-tion answering (QA) ?
specifically ?How?
?and ?Why??
questions.
In this paper, wepresent the task of process extraction, inwhich events within a process and the rela-tions between the events are automatically ex-tracted from text.
We represent processes bygraphs whose edges describe a set of temporal,causal and co-reference event-event relations,and characterize the structural properties ofthese graphs (e.g., the graphs are connected).Then, we present a method for extracting rela-tions between the events, which exploits thesestructural properties by performing joint in-ference over the set of extracted relations.On a novel dataset containing 148 descrip-tions of biological processes (released withthis paper), we show significant improvementcomparing to baselines that disregard processstructure.1 IntroductionA process is defined as a series of inter-relatedevents that involve multiple entities and lead to anend result.
Product manufacturing, economical de-velopments, and various phenomena in life and so-cial sciences can all be viewed as types of processes.Processes are complicated objects; consider for ex-ample the biological process of ATP synthesis de-scribed in Figure 1.
This process involves 12 en-tities and 8 events.
Additionally, it describes rela-tions between events and entities, and the relation-ship between events (e.g., the second occurrence ofthe event ?enter?, causes the event ?changing?).
?Both authors equally contributed to the paperAutomatically extracting the structure of pro-cesses from text is crucial for applications that re-quire reasoning, such as non-factoid QA.
For in-stance, answering a question on ATP synthesis, suchas ?How do H+ ions contribute to the productionof ATP??
requires a structure that links H+ ions(Figure 1, sentence 1) to ATP (Figure 1, sentence4) through a sequence of intermediate events.
Such?How??
questions are common on FAQ websites(Surdeanu et al 2011), which further supports theimportance of process extraction.Process extraction is related to two recent linesof work in Information Extraction ?
event extrac-tion and timeline construction.
Traditional event ex-traction focuses on identifying a closed set of eventswithin a single sentence.
For example, the BioNLP2009 and 2011 shared tasks (Kim et al 2009; Kimet al 2011) consider nine events types related toproteins.
In practice, events are currently almost al-ways extracted from a single sentence.
Process ex-traction, on the other hand, is centered around dis-covering relations between events that span multiplesentences.
The set of possible event types in processextraction is also much larger.Timeline construction involves identifying tem-poral relations between events (Do et al 2012; Mc-Closky and Manning, 2012; D?Souza and Ng, 2013),and is thus related to process extraction as both fo-cus on event-event relations spanning multiple sen-tences.
However, events in processes are tightly cou-pled in ways that go beyond simple temporal order-ing, and these dependencies are central for the pro-cess extraction task.
Hence, capturing process struc-ture requires modeling a larger set of relations thatincludes temporal, causal and co-reference relations.In this paper, we formally define the task ofprocess extraction and present automatic extractionmethods.
Our approach handles an open set of eventtypes and works over multiple sentences, extract-ing a rich set of event-event relations.
Furthermore,17107/5/13 7:01 PMbratPage 1 of 1http://127.0.0.1:8001/index.xhtml#/examples/emnlp2013/p66H+ ions flowing down their gradient enter a half channel in a stator, which is anchored in the membrane.H+ ions enter binding sites within a rotor, changing the shape of each subunit so that the rotor spins within the membrane.Spinning of the rotor causes an internal rod to spin as well.Turning of the rod activates catalytic sites in the knob that can produce ATP from ADP and P_i.Entity Event Entity Event Entity EntitycotempprevEntity Event Entity Entity Event Entity Entity Event Entitysamecauses causesprevEvent Entity Entity Eventsamesame causesEvent Entity Event Entity Event Entity Entityresultsameraw-materialcauses causes1234Figure 1: Partial annotation of the ATP synthesis process.
Most of the semantic roles have been removed for simplicity.we characterize a set of global properties of processstructure that can be utilized during process extrac-tion.
For example, all events in a process are some-how connected to one another.
Also, processes usu-ally exhibit a ?chain-like?
structure reflecting pro-cess progression over time.
We show that incor-porating such global properties into our model andperforming joint inference over the extracted rela-tions significantly improves the quality of processstructures predicted.
We conduct experiments on anovel dataset of process descriptions from the text-book ?Biology?
(Campbell and Reece, 2005) thatwere annotated by trained biologists.
Our methoddoes not require any domain-specific knowledge andcan be easily adapted to non-biology domains.The main contributions of this paper are:1.
We define process extraction and characterizeprocesses?
structural properties.2.
We model global structural properties in pro-cesses and demonstrate significant improve-ment in extraction accuracy.3.
We publicly release a novel data set of 148fully annotated biological process descrip-tions along with the source code for our sys-tem.
The dataset and code can be down-loaded from http://nlp.stanford.edu/software/bioprocess/.2 Process Definition and DatasetWe define a process description as a paragraph orsequence of tokens x = {x1, .
.
.
x|x|} that describesa series of events related by temporal and/or causalrelations.
For example, in ATP synthesis (Figure 1),the event of rotor spinning causes the event wherean internal rod spins.We model the events within a process and theirrelations by a directed graph P = (V,E), wherethe nodes V = {1, .
.
.
, |V |} represent event men-tions and labeled edges E correspond to event-eventrelations.
An event mention v ?
V is defined by atrigger tv, which is a span of words xi, xi+1, .
.
.
, xj ;and by a set of argument mentions Av, where eachargument mention av ?
Av is also a span of wordslabeled by a semantic role l taken from a set L. Forexample, in the last event mention of ATP synthesis,tv = produce, and one of the argument mentions isav = (ATP, RESULT).
A labeled edge (u, v, r) in thegraph describes a relation r ?
R between the eventmentions u and v. The task of process extraction isto extract the graph P from the text x.1A natural way to break down process extractioninto sub-parts is to first perform semantic role label-ing (SRL), that is, identify triggers and predict ar-gument mentions with their semantic role, and thenextract event-event relations between pairs of eventmentions.
In this paper, we focus on the secondstep, where given a set of event triggers T , we findall event-event relations, where a trigger representsthe entire event.
For completeness, we now describethe semantic roles L used in our dataset, and then1Argument mentions are also related by coreference rela-tions, but we neglect that since it is not central in this paper.1711present the set of event-event relationsR.The setL contains standard semantic roles such asAGENT, THEME, ORIGIN, DESTINATION and LO-CATION.
Two additional semantic roles were em-ployed that are relevant for biological text: RESULTcorresponds to an entity that is the result of an event,and RAW-MATERIAL describes an entity that is usedor consumed during an event.
For example, the lastevent ?produce?
in Figure 1, has ?ATP?
as the RE-SULT, and ?ADP?
as the RAW-MATERIAL.The event-event relation set R contains the fol-lowing (assuming a labeled edge (u, v, r)):1.
PREV denotes that u is an event immediatelybefore v. Thus, the edges (u, v, PREV) and(v, w, PREV), preclude the edge (u,w, PREV).For example, in ?When a photon strikes.
.
.
energy is passed .
.
.
until it reaches .
.
.
?,there is no edge (strikes, reaches, PREV) dueto the intervening event ?passed?.2.
COTEMP denotes that events u and v overlap intime (e.g., the first two event mentions flowingand enter in Figure 1).3.
SUPER denotes that event u includes eventv.
For instance, in ?During DNA replica-tion, DNA polymerases proofread each nu-cleotide.
.
.
?
there is an edge (DNA replication,proofread, SUPER).4.
CAUSES denotes that event u causes event v(e.g., the relation between changing and spinsin sentence 2 of Figure 1).5.
ENABLES denotes that event u creates precon-ditions that allow event v to take place.
Forexample, the description ?.
.
.
cause cancer cellsto lose attachments to neighboring cells.
.
.
, al-lowing them to spread into nearby tissues?
hasthe edge (lose, spread, ENABLES).
An in-tuitive way to think about the difference be-tween Causes and Enables is the following: ifu causes v this means that if u happens, thenv happens.
If u enables v, then if u does nothappen, then v does not happen.6.
SAME denotes that u and v both refer to thesame event (spins and Spinning in Figure 1).Early work on temporal logic (Allen, 1983) con-tained more temporal relations than are used in ourAvg Min Max# of sentences 3.80 1 15# of tokens 89.98 19 319# of events 6.20 2 15# of non-NONE relations 5.64 1 24Table 1: Process statistics over 148 process descriptions.NONE is used to indicate no relation.relation set R. We chose a relation set R that cap-tures the essential aspects of temporal relations be-tween events in a process, while keeping the annota-tion as simple as possible.
For instance, we includethe SUPER relation that appears in temporal anno-tations such as the Timebank corpus (Pustejovskyet al 2003) and Allen?s work, but in practice wasnot considered by many temporal ordering systems(Chambers and Jurafsky, 2008; Yoshikawa et al2009; Do et al 2012).
Importantly, our relation setalso includes the relations CAUSES and ENABLES,which are fundamental to modeling processes andgo beyond simple temporal ordering.We also added event coreference (SAME) to R.Do et al(2012) used event coreference informationin a temporal ordering task to modify probabilitiesprovided by pairwise classifiers prior to joint infer-ence.
In this paper, we simply treat SAME as an-other event-event relation, which allows us to easilyperform joint inference and employ structural con-straints that combine both coreference and temporalrelations simultaneously.
For example, if u and v arethe same event, then there can exist no w, such thatu is before w, but v is after w (see Section 3.3)We annotated 148 process descriptions based onthe aforementioned definitions.
Further details onannotation and data set statistics are provided in Sec-tion 4 and Table 1.Structural properties of processes Coherent pro-cesses exhibit many structural properties.
For ex-ample, two argument mentions related to the sameevent cannot overlap ?
a constraint that has beenused in the past in SRL (Toutanova et al 2008).
Inthis paper we focus on three main structural prop-erties of the graph P .
First, in a coherent pro-cess, all events mentioned are related to one another,and hence the graph P must be connected.
Sec-ond, processes tend to have a ?chain-like?
structurewhere one event follows another, and thus we expect1712Deg.
Gold Local Global0 0 29 01 219 274 2242 369 337 4083 46 14 17?
4 22 2 7Table 2: Node degree distribution for event mentions onthe training set.
Predictions for the Local and Globalmodels were obtained using 10-fold cross validation.nodes?
degree to generally be ?
2.
Indeed, 90% ofevent mentions have degree ?
2, as demonstratedby the Gold column of Table 2.
Last, if we considerrelations between all possible triples of events in aprocess, clearly some configurations are impossible,while others are common (illustrated in Figure 2).In Section 3.3, we show that modeling these proper-ties using a joint inference framework improves thequality of process extraction significantly.3 Joint Model for Process ExtractionGiven a paragraph x and a trigger set T , we wishto extract all event-event relations E. Similar to Doet al(2012), our model consists of a local pairwiseclassifier and global constraints.
We first introducea classifier that is based on features from previouswork.
Next, we describe novel features specific forprocess extraction.
Last, we incorporate global con-straints into our model using an ILP formulation.3.1 Local pairwise classifierThe local pairwise classifier predicts relations be-tween all event mention pairs.
In order to modelthe direction of relations, we expand the set R toinclude the reverse of four directed relations: PREV-NEXT, SUPER- SUB, CAUSES-CAUSED, ENABLES-ENABLED.
After adding NONE to indicate no rela-tion, and including the undirected relations COTEMPand SAME,R contains 11 relations.
The classifier ishence a function f : T ?
T ?
R. As an example,f(ti, tj) = PREV iff f(tj , ti) = NEXT.
Let n be thenumber of triggers in a process, and ti be the i-thtrigger in its description.
Since f(ti, tj) completelydetermines f(tj , ti), it suffices to consider only pairswith i < j.
Note that the process graph P is undi-rected under the new definition ofR.Table 3 describes features from previousFeature DescriptionPOS Pair of POS tagsLemma Pair of lemmasPrep?
Preposition lexeme, if in a prepositional phraseSent.
count Quantized number of sentences between triggersWord count Quantized number of words between triggersLCA Least common ancestor on constituency tree, if existsDominates?
Whether one trigger dominates otherShare Whether triggers share a child on dependency treeAdjacency Whether two triggers are adjacentWords btw.
For adjacent triggers, content words between triggersTemp.
btw.
For adjacent triggers, temporal connectives (from asmall list) between triggersTable 3: Features extracted for a trigger pair (ti, tj).
As-teriks (*) indicate features that are duplicated, once foreach trigger.work (Chambers and Jurafsky, 2008; Do et al2012) extracted for a trigger pair (ti, tj).
Somefeatures were omitted since they did not yieldimprovement in performance on a development set(e.g., lemmas and part-of-speech tags of contextwords surrounding ti and tj), or they require goldannotations provided in TimeBank, which we donot have (e.g., tense and aspect of triggers).
Toreduce sparseness, we convert nominalizations intotheir verbal forms when computing word lemmas,using WordNet?s (Fellbaum, 1998) derivation links.3.2 Classifier extensionsA central source of information to extract event-event relations from text are connectives such as af-ter, during, etc.
However, there is variability in theoccurrence of these connectives as demonstrated bythe following two sentences (connectives in bold-face, triggers in italics):1.
Because alleles are exchanged during gene flow, ge-netic differences are reduced.2.
During gene flow, alleles are exchanged, and geneticdifferences are hence reduced.Even though both sentences express the same re-lation (exchanged, reduced,CAUSES), the connec-tives used and their linear position with respect to thetriggers are different.
Also, in sentence 1, gene flowintervenes between exchanged and reduced.
Sinceour dataset is small, we wish to identify the trig-gers related to each connective, and share featuresbetween such sentences.
We do this using the syn-tactic structure and by clustering the connectives.1713tjti tk(a) SAME transitivitySAMESAMESAMEtjti tk(b) CAUSE-COTEMPCAUSESCAUSES COTEMPtjti tk(c) COTEMP transitivityCOTEMPCOTEMPCOTEMP / SAMEtjti tk(d) SAME contradictionPREVPREVSAMEtjti tk(e) PREV contradictionPREVPREVPREVFigure 2: Relation triangles (a)-(c) are common in the gold standard while (d)-(e) are impossible.Sentence 1 presents a typical case where by walk-ing up the dependency tree from the marker because,we can find the triggers related by this marker:becausemark????
exchangedadvcl????
reduced.
When-ever a trigger is the head of an adverbial clause andmarked by a mark dependency label, we walk on thedependency tree and look for a trigger in the mainclause that is closest to the root (or the root itselfin this example).
By utilizing the syntactic struc-ture, we can correctly spot that the trigger gene flowis not related to the trigger exchanged through theconnective because, even though they are linearlycloser.
In order to reduce sparseness of connectives,we created a hand-made clustering of 30 connectivesthat maps words into clusters2 (e.g., because, sinceand hence to a ?causality?
cluster).
After locatingthe relevant pair of triggers, we use these clustersto fire the same feature for connectives belonging tothe same cluster.
We perform a similar procedurewhenever a trigger is part of a prepositional phrase(imagine sentence 1 starting with ?due to allele ex-change during gene flow .
.
.
?)
by walking up theconstituency tree, but details are omitted for brevity.In sentence 2, the connective hence is an adverbialmodifier of the trigger reduced.
We look up the clus-ter for the connective hence and fire the same featurefor the adjacent triggers exchanged and reduced.We further extend our features to handle the richrelation set necessary for process extraction.
Thefirst event of a process is often expressed as a nom-inalization and includes subsequent events (SUPERrelation), e.g., ?The Calvin cycle begins by incor-porating...?.
To capture this, we add a feature thatfires when the first event of the process descriptionis a noun.
We also add two features targeted at the2The full set of connectives and their clustering are providedas part of our publicly released package.SAME relation: one indicating if the lemmas of tiand tj are the same, and another specifying the de-terminer of tj , if it exists.
Certain determiners in-dicate that an event trigger has already been men-tioned, e.g., the determiner this hints a SAME rela-tion in ?The next steps decompose citrate back tooxaloacetate.
This regeneration makes .
.
.
?.
Last,we add as a feature the dependency path between tiand tj , if it exists, e.g., in ?meiosis produces cellsthat divide .
.
.
?, the featuredobj???rcmod????
is fired forthe trigger pair produces and divide.
In Section 4.1we empirically show that our extensions to the localclassifier substantially improve performance.For our pairwise classifier, we train a maximumentropy classifier that computes a probability pijrfor every trigger pair (ti, tj) and relation r. Hence,f(ti, tj) = arg maxr pijr.3.3 Global ConstraintsNaturally, pairwise classifiers are local models thatcan violate global properties in the process structure.Figure 3 (left) presents an example for predictionsmade by the pairwise classifier, which result in twotriggers (deleted and dupcliated) that are isolatedfrom the rest of the triggers.
In this section, we dis-cuss how we incorporate constraints into our modelto generate coherent global process structures.Let ?ijr be the score for a relation r between thetrigger pair (ti, tj) (e.g., ?ijr = log pijr), and yijr bethe corresponding indicator variable.
Our goal is tofind an assignment for the indicators y = {yijr | 1 ?i < j ?
n, r ?
R}.
With no global constraints thiscan be formulated as the following ILP:1714arg maxy?ijr?ijryijr (1)s.t.
?i,j?ryijr = 1where the constraint ensures exactly one relation be-tween each event pair.
We now describe constraintsthat result in a coherent global process structure.Connectivity Our ILP formulation for enforcingconnectivity is a minor variation of the one sug-gested by Martins et al(2009) for dependency pars-ing.
In our setup, we want P to be a connected undi-rected graph, and not a directed tree.
However, anundirected graph P is connected iff there exists adirected tree that is a subgraph of P when edge di-rections are ignored.
Thus the resulting formulationis almost identical and is based on flow constraintswhich ensure that there is a path from a designatedroot in the graph to all other nodes.Let R?
be the set R \ NONE.
An edge (ti, tj) isin E iff there is some non-NONE relation betweenti and tj , i.e.
iff yij :=?r?R?
yijr is equal to 1.For each variable yij we define two auxiliary binaryvariables zij and zji that correspond to edges of thedirected tree that is a subgraph of P .
We ensure thatthe edges in the tree exist also in P by tying eachauxiliary variable to its corresponding ILP variable:?i<j zij ?
yij , zji ?
yij (2)Next, we add constraints that ensure that the graphstructure induced by the auxiliary variables is a treerooted in an arbitrary node 1 (The choice of rootdoes not affect connectivity).
We add for every i 6= ja flow variable ?ij which specifies the amount offlow on the directed edge zij .
?izi1 = 0, ?j 6=1?izij = 1 (3)?i?1i = n?
1 (4)?j 6=1?i?ij ?
?k?jk = 1 (5)?i 6=j ?ij ?
n ?
zij (6)Equation 3 says that all nodes in the graph haveexactly one parent, except for the root that has noparents.
Equation 4 ensures that the outgoing flowfrom the root is n?1, and Equation 5 states that eachof the other n ?
1 nodes consume exactly one unitof flow.
Last, Equation 6 ties the auxiliary variablesto the flow variables, making sure that flow occursonly on edges.
The combination of these constraintsguarantees that the graph induced by the variableszij is a directed tree and consequently the graph in-duced by the objective variables y is connected.Chain structure A chain is a connected graphwhere the degree of all nodes is ?
2.
Table 2presents nodes?
degree and demonstrates that indeedprocess graphs are close to being chains.
The fol-lowing constraint bounds nodes?
degree by 2:?j(?i<jyij +?j<kyjk ?
2) (7)Since graph structures are not always chains, weadd this as a soft constraint, that is, we penalize theobjective for each node with degree > 2.
The chainstructure is one of the several soft constraints weenforce.
Thus, our modified objective function is?ijr ?ijryijr +?k?K ?kCk, where K is the set ofsoft constraints, ?k is the penalty (or reward for de-sirable structures), and Ck indicates whether a con-straint is violated (or satisfied).
Note that under thisformulation our model is simply a constrained con-ditional model (wei Chang et al 2012).
The param-eters ?k are tuned on a development set (see Sec-tion 4).Relation triads A relation triad (or a re-lation triangle) for any three triggers ti, tjand tk in a process is a 3-tuple of relations(f(ti, tj), f(tj , tk), f(ti, tk)).
Clearly, some triadsare impossible while others are quite common.
Tofind triads that could improve process extraction, thefrequency of all possible triads in both the trainingset and the output of the pairwise classifier werefound, and we focused on those for which the clas-sifier and the gold standard disagree.
We are inter-ested in triads that never occur in training data butare predicted by the classifier, and vice versa.
Fig-ure 2 illustrates some of the triads found and Equa-1715tions 8-12 provide the corresponding ILP formula-tions.
Equations 8-10 were formulated as soft con-straints (expanding the setK) and were incorporatedby defining a reward ?k for each triad type.3 Onthe other hand, Equations 11-12 were formulated ashard constraints to prevent certain structures.1.
SAME transitivity (Figure 2a, Eqn.
8): Co-reference transitivity has been used in pastwork (Finkel and Manning, 2008) and we in-corporate it by a constraint that encourages tri-ads that respect transitivity.2.
CAUSE-COTEMP (Figure 2b, Eqn.
9): If ticauses both tj and tk, then often tj and tk areco-temporal.
E.g, in ?genetic drift has led toa loss of genetic variation and an increase inthe frequency of .
.
.
?, a single event causes twosubsequent events that occur simultaneously.3.
COTEMP transitivity (Figure 2c, Eqn.
10): Ifti is co-temporal with tj and tj is co-temporalwith tk, then usually ti and tk are either co-temporal or denote the same event.4.
SAME contradiction (Figure 2d, Eqn.
11): Ifti is the same event as tk, then their tempo-ral ordering with respect to a third trigger tjmay result in a contradiction, e.g., if tj is af-ter ti, but before tk.
We define 5 temporalcategories that generate(52)possible contradic-tions, but for brevity present just one represen-tative hard constraint.
This constraint dependson prediction of temporal and co-reference re-lations jointly.5.
PREV contradiction (Figure 2e, Eqn.
12): Asmentioned (Section 3.3), if ti is immediatelybefore tj , and tj is immediately before tk, thenti cannot be immediately before tk.yijSAME + yjkSAME + yikSAME ?
3 (8)yijCAUSES + yikCAUSES + yjkCOTEMP ?
3 (9)yijCOTEMP + yjkCOTEMP + yikCOTEMP+yikSAME ?
3 (10)yijPREV + yjkPREV + yikSAME ?
2 (11)yijPREV + yjkPREV ?
yikNONE ?
1 (12)3We experimented with a reward for certain triads or apenalty for others and empirically found that using rewards re-sults in better performance on the development set.We used the Gurobi optimization package4 tofind an exact solution for our ILP, which containsO(n2|R|) variables and O(n3) constraints.
We alsodeveloped an equivalent formulation amenable todual decomposition (Sontag et al 2011), which is afaster approximation method.
But practically, solv-ing the ILP exactly with Gurobi was quite fast (av-erage/median time per process: 0.294 sec/0.152 secon a standard laptop).4 Experimental EvaluationWe extracted 148 process descriptions by goingthrough chapters from the textbook ?Biology?
andmarking any contiguous sequence of sentences thatdescribes a process, i.e., a series of events that leadtowards some objective.
Then, each process descrip-tion was annotated by a biologist.
The annotator wasfirst presented with annotation guidelines and anno-tated 20 descriptions.
The annotations were thendiscussed with the authors, after which all processdescriptions were annotated.
After training a sec-ond biologist, we measured inter-annotator agree-ment ?
= 0.69, on 30 random process descriptions.Process descriptions were parsed with Stanfordconstituency and dependency parsers (Klein andManning, 2003; de Marneffe et al 2006), and 35process descriptions were set aside as a test set(number of training set trigger pairs: 1932, numberof test set trigger pairs: 906).
We performed 10-fold cross validation over the training set for featureselection and tuning of constraint parameters.
Foreach constraint type (connectivity, chain-structure,and five triad constraints) we introduced a param-eter and tuned the seven parameters by coordinate-wise ascent, where for hard constraints a binary pa-rameter controls whether the constraint is used, andfor soft constraints we attempted 10 different re-ward/penalty values.
For our global model we de-fined ?ijr = log pijr, where pijr is the probability atedge (ti, tj) for label r, given by the pairwise clas-sifier.We test the following systems: (a) All-Prev: Sincethe most common process structure was chain-like,we simply predict PREV for every two adjacent trig-gers in text.
(b) Localbase: A pairwise classifier withfeatures from previous work (Section 3.1) (c) Local:4www.gurobi.com1716Temporal FullP R F1 P R F1All-Prev 58.4 54.8 56.6 34.1 32.0 33.0Localbase 61.5 51.8 56.2 52.1 43.9 47.6Local 63.2 55.7?
59.2 54.7 48.3?
51.3Chain 64.5 60.5??
62.4?
56.1 52.6??
54.3?Global 63.9 61.4??
62.6??
56.2 54.0??
55.0?
?Table 4: Test set results on all experiments.
Best numberin each column is bolded.
?
and ?
denote statistical signif-icance (p < 0.01) against Localbase and Local baselines,respectively.A pairwise classifier with all features (Section 3.2)(d) Chain: For every two adjacent triggers, choosethe non-NONE relation with highest probability ac-cording to Local.
This baseline heuristically com-bines our structural assumptions with the pairwiseclassifier.
We deterministically choose a connectedchain structure, and then use the classifier to labelthe edges.
(e) Global: Our full model that uses ILPinference.To evaluate system performance we compare theset of predictions on all trigger pairs to the gold stan-dard annotations and compute micro-averaged pre-cision, recall and F1.
We perform two types of eval-uations: (a) Full: evaluation on our full set of 11relations (b) Temporal: Evaluation on temporal re-lations only, by collapsing PREV, CAUSES, and EN-ABLES to a single category and similarly for NEXT,CAUSED, and ENABLED (inter-annotator agreement?
= 0.75).
We computed statistical significanceof our results with the paired bootstrap resamplingmethod of 2000 iterations (Efron and Tibshirani,1993), where the units resampled are trigger-trigger-relation triples.4.1 ResultsTable 4 presents performance of all systems.
We seethat using global constraints improves performancealmost invariably on all measures in both full andtemporal evaluations.
Particularly, in the full eval-uation Global improves recall by 12% and overallF1 improves significantly by 3.7 points against Lo-cal (p < 0.01).
Recall improvement suggests thatmodeling connectivity allowed Global to add cor-rect relations in cases where some events were notconnected to one another.The Local classifier substantially outperformsLocalbase.
This indicates that our novel features(Section 3.2) are important for discriminating be-tween process relations.
Specifically, in the full eval-uation Local improves precision more than in thetemporal evaluation, suggesting that designing syn-tactic and semantic features for connectives is usefulfor distinguishing PREV, CAUSES, and ENABLESwhen the amount of training data is small.The Chain baseline performs only slightly worsethan our global model.
This demonstrates the strongtendency of processes to proceed linearly from oneevent to the other, which is a known property of dis-course structure (Schegloff and Sacks, 1973).
How-ever, since the structure is deterministically fixed,Chain is highly inflexible and does not allow anyextensions or incorporation of other structural con-straints or domain knowledge.
Thus, it can be usedas a simple and efficient approximation but is not agood candidate for a real system.
Further supportfor the linear nature of process structure is providedby the All-Prev baseline, which performs poorly inthe full evaluation, but in temporal evaluation worksreasonably well.Table 2 presents the degree distribution of Localand Global on the development set comparing to thegold standard.
The degree distribution of Global ismore similar to the gold standard than Local.
In par-ticular, the connectivity constraint ensures that thereare no isolated nodes and shifts mass from nodeswith degree 0 and 1 to nodes with degree 2.Table 5 presents the order in which constraintswere introduced into the global model using coor-dinate ascent on the development set.
Connectivityis the first constraint to be introduced, and improvesperformance considerably.
The chain constraint, onthe other hand, is included third and the improve-ment in F1 score is relatively smaller.
This can beexplained by the distribution of degrees in Table 2which shows that the predictions of Local does nothave many nodes with degree > 2.
As for triad con-straints, we see that four constraints are importantand are included in the model, but one is discarded.Last, we examined the results of Global whenmacro-averaging over processes, i.e., assigning eachprocess the same weight by computing recall, pre-cision and F1 for each process and averaging thosescores.
We found that results are quite similar(with a slight improvement): in the full evalua-1717Order Parameter name Value (?)
F1 score?
Local model ?
49.91 Connectivity constraint ?
51.22 SAME transitivity 0.5 52.93 Chain constraint -0.5 53.34 CAUSE-COTEMP 1.0 53.76 PREV contradiction ?
53.87 SAME contradiction ?
53.9Table 5: Order by which constraint parameters were setusing coordinate ascent on the development set.
For eachparameter, the value chosen and F1 score after includingthe constraint are provided.
Negative values correspondto penalties, positive values to rewards, and a value of?indicates a hard constraint.tion Global obtains R/P/F1 of 56.4/55.0/55.7, andin the temporal evaluation Global obtains R/P/F1 of63.8/62.3/63.1.4.2 Qualitative AnalysisFigure 3 shows two examples where global con-straints corrected the predictions of Local.
In Fig-ure 3, left, Local failed to predict the causal rela-tions skipped-deleted and used-duplicated, possiblybecause they are not in the same sentence and are notadjacent to one another.
By enforcing the connectiv-ity constraint, Global correctly adds the correct re-lations and connects deleted and duplicated to theother triggers in the process.In Figure 3, right, Local predicts a structure thatresults in a ?SAME contradiction?
structure.
Thetriggers bind and binds cannot denote the same eventif a third trigger secrete is temporally between them.However, Local predicts they are the same event, asthey share a lemma.
Global prohibits this structureand correctly predicts the relation as NONE.To better understand the performance of Local,we analyzed the confusion matrix generated basedon its predictions.
Although this is a challenging11-class classification task, most of the mass is con-centrated on the matrix diagonal, as desired.
Erroranalysis reveals that 17.5% of all errors are con-fusions between NONE and PREV, 11.1% betweenPREV and CAUSES, and 8.6% between PREV andCOTEMP.
This demonstrates that distinguishing theclasses PREV, CAUSES and COTEMP is challengingfor Local.
Our current global constraints do not ad-dress this type of error, and thus an important direc-tion for future work is to improve the local model.The global model depends on the predictions ofthe local classifier, and so enforcing global con-straints does not guarantee improvement in perfor-mance.
For instance, if Local produces a graph thatis disconnected (e.g., deleted in Figure 3, left), thenGlobal will add an edge.
However, the label of theedge is determined by scores computed based onthe local classifier, and if this prediction is wrong,we will now be penalized for both the false nega-tive of the correct class (just as before), and also forthe false positive of the predicted class.
Despite thatwe see that Global improves overall performance by3.7 F1 points on the test set.5 Related WorkA related line of work is biomedical event extrac-tion in recent BioNLP shared tasks (Kim et al2009; Kim et al 2011).
Earlier work employed apipeline architecture where first events are found,and then their arguments are identified (Miwa et al2010; Bjo?rne et al 2011).
Subsequent methods pre-dicted events and arguments jointly using Markovlogic (Poon and Vanderwende, 2010) and depen-dency parsing algorithms (McClosky et al 2011).Riedel and McCallum (2011) further improved per-formance by capturing correlations between eventsand enforcing consistency across arguments.Temporal event-event relations have been ex-tensively studied (Chambers and Jurafsky, 2008;Yoshikawa et al 2009; Denis and Muller, 2011;Do et al 2012; McClosky and Manning, 2012;D?Souza and Ng, 2013), and we leverage suchtechniques in our work (Section 3.1).
However,we extend beyond temporal relations alone, andstrongly rely on dependencies between processevents.
Chambers and Jurafsky (2011) learned eventtemplates (or frames), where events that are relatedto one another and their semantic roles are extracted.Recently, Cheung et al(2013) proposed an unsuper-vised generative model for inducing such templates.A major difference in our work is that we do notlearn typical event relations from a large and redun-dant corpus, but are given a paragraph and have a?one-shot?
chance to extract the process structure.We showed in this paper that global structuralproperties lead to significant improvements in ex-traction accuracy, and ILP is an effective framework1718shiftsskippedCAUSESCAUSESusedCAUSESCAUSESdeletedCAUSESCAUSESduplicatedCAUSESCAUSES bindsecreteCOTEMPPREV bindsSAMENONEPREVENABLESFigure 3: Process graph fragments.
Black edges (dotted) are predictions of Local, green (solid) are predictions ofGlobal, and gold (dashed) are gold standard edges.
To reduce clutter, we present the predictions of Global only whenit disagrees with Local.
In all other cases, the predictions of Global and Local are identical.
Original text, Left: ?...
thetemplate shifts .
.
.
, and a part of the template strand is either skipped by the replication machinery or used twice as atemplate.
As a result, a segment of DNA is deleted or duplicated.?
Right: ?Cells of mating type A secrete a signalingmolecule, which can bind to specific receptor proteins on nearby cells.
At the same time, cells secrete factor, whichbinds to receptors on A cells.
?for modeling global constraints.
Similar observa-tions and techniques have been proposed in otherinformation extraction tasks.
Reichart and Barzi-lay (2012) tied information from multiple sequencemodels that describe the same event by using globalhigher-order potentials.
Berant et al(2011) pro-posed a global inference algorithm to identify entail-ment relations.
There is an abundance of examplesof enforcing global constraints in other NLP tasks,such as in coreference resolution (Finkel and Man-ning, 2008), parsing (Rush et al 2012) and namedentity recognition (Wang et al 2013).6 ConclusionDeveloping systems that understand process de-scriptions is an important step towards building ap-plications that require deeper reasoning, such as bi-ological process models from text, intelligent tutor-ing systems, and non-factoid QA systems.
In thispaper we have presented the task of process extrac-tion, and developed methods for extracting relationsbetween process events.
Processes contain eventsthat are tightly coupled through strong dependen-cies.
We have shown that exploiting these structuraldependencies and performing joint inference over allevent mentions can significantly improve accuracyover several baselines.
We have also released a newdataset containing 148 fully annotated descriptionsof biological processes.
Though the models we builtwere trained on biological processes, they do not en-code domain specific information, and hence shouldbe extensible to other domains.In this paper we assumed that event triggers aregiven as input.
In future work, we want to performtrigger identification jointly with extraction of event-event relations.
As explained in Section 4.2, theperformance of our system is confined by the per-formance of the local classifier, which is trained onrelatively small amounts of data.
Since data annota-tion is expensive, it is important to improve the lo-cal classifier without increasing the annotation bur-den.
For example, one can use unsupervised meth-ods that learn narrative chains (Chambers and Ju-rafsky, 2011) to provide some prior on the typicalorder of events.
Alternatively, we can search on theweb for redundant descriptions of the same processand use this redundancy to improve classification.Last, we would like to integrate our method into QAsystems and allow non-factoid questions that requiredeeper reasoning to be answered by matching thequestions against the learned process structures.AcknowledgmentsThe authors would like to thank Roi Reichart forfruitful discussion and the anonymous reviewers fortheir constructive feedback.
This work was partiallyfunded by Vulcan Inc.
The second author was spon-sored by a Rothschild fellowship.ReferencesJames F. Allen.
1983.
Maintaining knowledge abouttemporal intervals.
Commun.
ACM, 26(11):832?843.Jonathan Berant, Ido Dagan, and Jacob Goldberger.2011.
Learning entailment relations by global graphstructure optimization.
Journal of Computational Lin-guistics, 38(1).1719Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,Tapio Pahikkala, and Tapio Salakoski.
2011.
Extract-ing contextualized complex biological events with richgraph-based feature sets.
Computational Intelligence,27(4):541?557.Neil Campbell and Jane Reece.
2005.
Biology.
Ben-jamin Cummings.Nathanael Chambers and Daniel Jurafsky.
2008.
Jointlycombining implicit constraints improves temporal or-dering.
In Proceedings of EMNLP.Nathanael Chambers and Dan Jurafsky.
2011.
Template-based information extraction without the templates.
InACL, pages 976?986.Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-derwende.
2013.
Probabilistic frame induction.
InProceedings of NAACL-HLT.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
In Pro-ceedings of LREC.Pascal Denis and Philippe Muller.
2011.
Predictingglobally-coherent temporal structures from texts viaendpoint inference and graph decomposition.
In Pro-ceedings of IJCAI.Quang Do, Wei Lu, and Dan Roth.
2012.
Joint infer-ence for event timeline construction.
In Proceedingsof EMNLP-CoNLL.Jennifer D?Souza and Vincent Ng.
2013.
Classifyingtemporal relations with rich linguistic knowledge.
InProceedings of NAACL-HLT.Bradley Efron and Robert Tibshirani.
1993.
An introduc-tion to the bootstrap, volume 57.
CRC press.Christiane Fellbaum, editor.
1998.
WordNet: An elec-tronic lexical database.
MIT Press.Jenny Rose Finkel and Christopher D. Manning.
2008.Enforcing transitivity in coreference resolution.
InProceedings of ACL.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Junichi Tsujii.
2009.
Overview ofBioNLP 09 shared task on event extraction.
In Pro-ceedings of BioNLP.Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, RobertBossy, and Junichi Tsujii.
2011.
Overview of BioNLPshared task 2011.
In Proceedings of BioNLP.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of ACL.Andre?
L. Martins, Noah A. Smith, and Eric P. Xing.2009.
Concise integer linear programming formu-lations for dependency parsing.
In Proceedings ofACL/IJCNLP.David McClosky and Christopher D. Manning.
2012.Learning constraints for consistent timeline extraction.In Proceedings of EMNLP-CoNLL, pages 873?882.David McClosky, Mihai Surdeanu, and Christopher D.Manning.
2011.
Event extraction as dependency pars-ing.
In Proceedings of ACL, pages 1626?1635.Makoto Miwa, Rune S?tre, Jin-Dong Kim, and Jun?ichiTsujii.
2010.
Event extraction with complex eventclassification using rich features.
J. Bioinformaticsand Computational Biology, 8(1).Hoifung Poon and Lucy Vanderwende.
2010.
Joint in-ference for knowledge extraction from biomedical lit-erature.
In Proceedings of HLT-NAACL.James Pustejovsky, Jose?
M. Castan?o, Robert Ingria,Roser Sauri, Robert J. Gaizauskas, Andrea Setzer,Graham Katz, and Dragomir R. Radev.
2003.TimeML: Robust specification of event and temporalexpressions in text.
In New Directions in Question An-swering.Roi Reichart and Regina Barzilay.
2012.
Multi-event ex-traction guided by global constraints.
In Proceedingsof HLT-NAACL.Sebastian Riedel and Andrew McCallum.
2011.
Fast androbust joint models for biomedical event extraction.
InProceedings of EMNLP.Alexander M. Rush, Roi Reichert, Michael Collins, andAmir Globerson.
2012.
Improved parsing and POStagging using inter-sentence consistency constraints.In Proceedings of EMNLP.Emanuel A Schegloff and Harvey Sacks.
1973.
Openingup closings.
Semiotica, 8(4):289?327.David Sontag, Amir Globerson, and Tommi Jaakkola.2011.
Introduction to dual decomposition for in-ference.
In Suvrit Sra, Sebastian Nowozin, andStephen J. Wright, editors, Optimization for MachineLearning.
MIT Press.Mihai Surdeanu, Massimiliano Ciaramita, and HugoZaragoza.
2011.
Learning to rank answers to non-factoid questions from web collections.
Computa-tional Linguistics, 37(2).Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2008.
A global joint model for semanticrole labeling.
Computational Linguistics, 34(2):161?191.Mengqiu Wang, Wanxiang Che, and Christopher D. Man-ning.
2013.
Effective bilingual constraints for semi-supervised learning of named entity recognizers.
InProceedings of AAAI.Ming wei Chang, Lev Ratinov, and Dan Roth.
2012.Structured learning with constrained conditional mod-els.
Machine Learning, 88(3):399?431, 6.Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-hara, and Yuji Matsumoto.
2009.
Jointly identifyingtemporal relations with Markov logic.
In Proceedingsof ACL/IJCNLP.1720
