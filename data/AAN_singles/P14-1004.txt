Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 36?46,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsDiscovering Latent Structure in Task-Oriented DialoguesKe Zhai?Computer Science, University of MarylandCollege Park, MD 20740zhaike@cs.umd.eduJason D. WilliamsMicrosoft ResearchRedmond, WA 98052jason.williams@microsoft.comAbstractA key challenge for computational conver-sation models is to discover latent struc-ture in task-oriented dialogue, since it pro-vides a basis for analysing, evaluating, andbuilding conversational systems.
We pro-pose three new unsupervised models todiscover latent structures in task-orienteddialogues.
Our methods synthesize hiddenMarkov models (for underlying state) andtopic models (to connect words to states).We apply them to two real, non-trivialdatasets: human-computer spoken dia-logues in bus query service, and human-human text-based chats from a live tech-nical support service.
We show that ourmodels extract meaningful state represen-tations and dialogue structures consistentwith human annotations.
Quantitatively,we show our models achieve superior per-formance on held-out log likelihood eval-uation and an ordering task.1 IntroductionModeling human conversation is a fundamentalscientific pursuit.
In addition to yielding ba-sic insights into human communication, compu-tational models of conversation underpin a hostof real-world applications, including interactivedialogue systems (Young, 2006), dialogue sum-marization (Murray et al, 2005; Daum?e III andMarcu, 2006; Liu et al, 2010), and even medi-cal applications such as diagnosis of psychologicalconditions (DeVault et al, 2013).Computational models of conversation can bebroadly divided into two genres: modeling andcontrol.
Control is concerned with choosing ac-tions in interactive settings?for example to maxi-mize task completion?using reinforcement learn-?Work done at Microsoft Research.ing (Levin et al, 2000), supervised learning (Hur-tado et al, 2010), hand-crafted rules (Larsson andTraum, 2000), or mixtures of these (Hendersonand Lemon, 2008).
By contrast, modeling?thegenre of this paper?is concerned with inferringa phenomena in an existing corpus, such as di-alogue acts in two-party conversations (Stolckeet al, 2000) or topic shifts in multi-party dia-logues (Galley et al, 2003; Purver et al, 2006;Hsueh et al, 2006; Banerjee and Rudnicky, 2006).Many past works rely on supervised learning orhuman annotations, which usually requires man-ual labels and annotation guidelines (Jurafsky etal., 1997).
It constrains scaling the size of trainingexamples, and application domains.
By contrast,unsupervised methods operate only on the observ-able signal (e.g.
words) and are estimated with-out labels or their attendant limitations (Crook etal., 2009).
They are particularly relevant becauseconversation is a temporal process where modelsare trained to infer a latent state which evolves asthe dialogue progresses (Bangalore et al, 2006;Traum and Larsson, 2003).Our basic approach is to assume that each ut-terance in the conversation is in a latent state,which has a causal effect on the words the conver-sants produce.
Inferring this model yields basicinsights into the structure of conversation and alsohas broad practical benefits, for example, speechrecognition (Williams and Balakrishnan, 2009),natural language generation (Rieser and Lemon,2010), and new features for dialogue policy opti-mization (Singh et al, 2002; Young, 2006).There has been limited past work on unsuper-vised methods for conversation modeling.
Choti-mongkol (2008) studies task-oriented conversa-tion and proposed a model based on a hiddenMarkov model (HMM).
Ritter et al (2010) ex-tends it by introducing additional word sources,and applies to non-task-oriented conversations?social interactions on Twitter, where the subjects36discussed are very diffuse.
The additional wordsources capture the subjects, leaving the state-specific models to express common dialogue flowssuch as question/answer pairs.In this paper, we retain the underlying HMM,but assume words are emitted using topic models(TM), exemplified by latent Dirichlet alocation(Blei et al, 2003, LDA).
LDA assumes each wordin an utterance is drawn from one of a set of latenttopics, where each topic is a multinomial distri-bution over the vocabulary.
The key idea is thatthe set of topics is shared across all states, andeach state corresponds to a mixture of topics.
Wepropose three model variants that link topics andstates in different ways.Sharing topics across states is an attractiveproperty in task-oriented dialogue, where a sin-gle concept can be discussed at many points in adialogue, yet different topics often appear in pre-dictable sequences.
Compared to past works, thedecoupling of states and topics gives our mod-els more expressive power and the potential to bemore data efficient.
Empirically, we find that ourmodels outperform past approaches on two real-world corpora of task-oriented dialogues.This paper is organized as follows: Section 2 in-troduces two task-oriented domains and corpora;Section 3 details three new unsupervised genera-tive models which combine HMMs and LDA andefficient inference schemes; Section 4 evaluatesour models qualitatively and quantitatively, and fi-nally conclude in Section 5.2 DataTo test the generality of our models, we study twovery different datasets: a set of human-computerspoken dialogues in quering bus timetable (Bus-Time), and a set of human-human text-based dia-logues in the technical support domain (TechSup-port).
In BusTime, the conversational structure isknown because the computer followed a determin-istic program (Williams, 2012), making it possibleto directly compare an inferred model to groundtruth on this corpus.1In TechSupport, there is noknown flowchart,2making this a realistic applica-tion of unsupervised methods.1Available for download at http://research.microsoft.com/en-us/events/dstc/2Technical support human agents use many types ofdocumentation?mainly checklists and guidelines, but ingeneral, there are no flowcharts.BusTime This corpus consists of logs of tele-phone calls between a spoken dialogue system andreal bus users in Pittsburgh, USA (Black et al,2010).
For the user side, the words logged are thewords recognized by the automatic speech recog-nizer.
The vocabulary of the recognizer was con-strained to the bus timetable task, so only wordsknown to the recognizer in advance are output.Even so, the word error rate is approximately 30-40%, due to the challenging audio conditions ofusage?with traffic noise and extraneous speech.The system asked users sequentially for a busroute, origin and destination, and optionally dateand time.
The system confirmed low-confidencespeech recognition results.
Due to the speechrecognition channel, system and user turns alwaysalternate.
An example dialogue is given below:System: Say a route like ?bus-route?, or say I?m not sure.User: ?bus-route?.System: I thought you said ?bus-route?, is that right?User: Yes.System: Say where?re you leaving from, like ?location?.User: ?location?.System: Okay, ?location?, where are you going to?...We discard dialogues with fewer than 20 ut-terances.
We also map all named entities (e.g.,?downtown?
and ?28X?)
to their semantic types(resp.
?location?
and ?bus-route?)
to reduce vo-cabulary size.
The corpus we use consists of ap-proximately 850 dialogue sessions or 30, 000 ut-terances.
It contains 370, 000 tokens (words or se-mantic types) with vocabulary size 250.TechSupport This corpus consists of logs ofreal web-based human-human text ?chat?
con-versations between clients and technical supportagents at a large corporation.
Usually, clients andagents first exchange names and contact informa-tion; after that, dialogues are quite free-form, asagents ask questions and suggest fixes.
Most dia-logues ultimately end when the client?s issue hasbeen resolved; some clients are provided with areference number for future follow-up.
An exam-ple dialogue is given below:Agent: Welcome to the answer desk!
My name is ?agent-name?.
How can I help you today?Agent: May I have your name, email and phone no.
?Client: Hi, ?agent-name?.
I recently installed new soft-ware but I kept getting error, can you help me?Agent: Sorry to hear that.
Let me help you with that.Agent: May I have your name, email and phone no.
?Client: The error code is ?error-code?.Client: It appears every time when I launch it.Client: Sure.
My name is ?client-name?.Client: My email and phone are ?email?, ?phone?.Agent: Thanks, ?client-name?, please give me a minute.37s0w0,iN0Ms0Ms0w0,iN0s1Ms0w0,iN0s1w1,iN1Ms0w0,iN0s1w1,iN1...snwn,iNnMM(a) LM-HMMs0w0,iN0s1...snMw1,iN1wn,iNnr1,ir0,irn,i?m?
E?ms0w0,iN0s1...snMw1,iN1wn,iNnr1,ir0,irn,itmgEums0w0,iN0s1Mw1,iN1r1,ir0,itmgEums0w0,iN0s1Mr0,itmgEums0w0,iN0s1MN1r1,ir0,itmgEums0N0Mr0,itmgEums0w0,iN0Mr0,itmgEumMtmgEums0N0MtmgEum(b) LM-HMMSFigure 1: Plate diagrams of baseline models, fromexisting work (Chotimongkol, 2008; Ritter et al,2010).
Variable definitions are given in the text....This data is less structured than BusTime;clients?
issues span software, hardware, network-ing, and other topics.
In addition, clients use com-mon internet short-hand (e.g., ?thx?, ?gtg?, ?ppl?,?hv?, etc), with mis-spellings (e.g., ?ofice?, ?off-fice?, ?erorr?, etc).
In addition, chats from the webinterface are segmented into turns when a user hits?Enter?
on a keyboard.
Therefore, clients?
inputand agents?
responses do not necessarily alternateconsecutively, e.g., an a ent?s response may takemultiple turns as in the above example.
Also, itis unreasonable to group consecutive chats fromthe same party to form a ?alternating?
structurelike BusTime dataset due to the asynchronism ofdifferent states.
For instance, the second blockof client inputs clearly comes from two differentstates which should not be merged together.We discard dialogues with fewer than 30 utter-ances.
We map named entities to their semantictypes, apply stemming, and remove stop words.3The corpus we use contains approximately 2, 000dialogue sessions or 80, 000 conversation utter-ances.
It consists of 770, 000 tokens, with a a vo-cabulary size of 6, 600.3 Latent Structure in DialoguesIn this work, our goal is to infer latent structurepresented in task-oriented conversation.
We as-sume that the structure can be encoded in a prob-abilistic state transition diagram, where the dia-logue is in one state at each utterance, and stateshave a causal effect on the words observed.
We as-sume the boundaries between utterances are given,which is trivial in many corpora.The simplest formulation we consider is anHMM where each state contains a unigram lan-guage model (LM), proposed by Chotimongkol(2008) for task-oriented dialogue and originally3We used regular expression to map named entities, andPorter stemmer in NLTK to stem all tokens.developed for discourse analysis by Barzilay andLee (2004).
We call it LM-HMM as in Figure 1(a).For a corpus of M dialogues, the m-th dialoguecontains n utterances, each of which contains Nnwords (we omit index m from terms because itwill be clear from context).
At n-th utterance,we assume the dialogue is in some latent state sn.Words in n-th utterance wn,1, .
.
.
, wn,Nnare gen-erated (independently) according to the LM.
Whenan utte ance is complete, the next state is drawnaccording to HMM, i.e., P (s?|s).While LM-HMM captures the basic intuition ofconversation structure, it assumes words are con-ditioned only on state.
Ritter et al (2010) extendsLM-HMM to allow words to be emitted from twoadditional sources: the topic of current dialogue?, or a background LM ?
shared across all dia-logues.
A multinomial pi indicates the expectedfraction of words from these three sources.
Forevery word in an utterance, first draw a source in-dicator r from pi, and then generate the word fromthe corresponding source.
We call it LM-HMMS(Figure 1(b)).
Ritter et al (2010) finds these al-ternate sources are important in non-task-orienteddomains, where events are diffuse and fleeting.For example, Twitter exchanges often focus on aparticular event (labeled X), and follow patternslike ?saw X last night?
?, ?X was amazing?.
HereX appears throughout the dialogue but does nothelp to distinguish conversational states in socialmedia.
We also explore similar variants.In this paper, these two models form our base-lines.
For all models, we use Markov chain MonteCarlo (MCMC) inference (Neal, 2000) to find la-tent variables that best fit observed data.
We alsoassume symmetric Dirichlet priors on all multino-mial distributions and apply collapsed Gibbs sam-pling.
In the rest of this section, we present ourmodels and their inference algorithms in turn.3.1 TM-HMMOur approach is to modify the emission probabil-ities of states to be distributions over topics ratherthan distributions over words.
In other words, in-stead of generating words via a LM, we generatewords from a topic model (TM), where each statemaps to a mixture of topics.
The key benefit of thisadditional layer of abstraction is to enable statesto express higher-level concepts through poolingof topics across states.
For example, topics mightbe inferred for content like ?bus-route?
or ?lo-38s0w0,iN0s1...snMz0,iw1,iN1z1,iwn,iNnzn,iTK?t?
ks0w0,iN0s1...snMz0,iw1,iN1z1,iwn,iNnzn,iTKhtgks0w0,iN0s1Mz0,iw1,iN1z1,iTKhtgks0w0,iN0s1Mz0,iN1z1,iTKhtgks0w0,iN0s1Mz0,iTKhtgks0w0,iN0Mz0,iTKhtgks0N0Mz0,iTKhtgks0MTKhtgkMTKhtgk(a) TM-HMMs0w0,iN0s1...snMz0,iw1,iN1z1,iwn,iNnzn,iKr1,ir0,irn,i?m?m?
ks0w0,iN0s1...snMz0,iw1,iN1z1,iwn,iNnzn,iKr1,ir0,irn,ihmumgks0w0,iN0s1Mz0,iN1z1,iKr1,ir0,ihmumgks0w0,iN0s1Mz0,iw1,iN1z1,iKr1,ir0,ihmumgks0w0,iN0s1Mz0,iN1z1,iKr0,ihmumgks0w0,iN0s1Mz0,iKr0,ihmumgks0w0,iN0Mz0,iKr0,ihmumgks0N0Mz0,iKr0,ihmumgks0N0Mz0,iKhmumgks0MKhmumgkMKhmumgkMKgk(b) TM-HMMSs0w0,iN0s1...snMz0,iw1,iN1z1,iwn,iNnzn,iKr1,ir0,irn,iT?
m?
k?ts0w0,iN0s1...snMz0,iw1,iN1z1,iwn,iNnzn,iKr1,ir0,irn,iThmgkuts0w0,iN0s1Mz0,iw1,iN1z1,iKr1,ir0,iThmgkuts0w0,iN0s1Mz0,iN1z1,iKr1,ir0,iThmgkuts0w0,iN0s1Mz0,iN1z1,iKr0,iThmgkuts0w0,iN0s1Mz0,iKr0,iThmgkuts0w0,iN0Mz0,iKr0,iThmgkuts0N0Mz0,iKr0,iThmgkuts0N0Mz0,iKThmgkuts0MKThmgkutMKThmgkutMKTgkut(c) TM-HMMSSFigure 2: Plate diagrams of proposed models.
TM-HMM is an HMM with state-wise topic distributions.TM-HMMS adds session-wise topic distribution and a source generator.
TM-HMMSS adds a state-wisesource generator.
Variable definitions are given in the text.cations?
; and other topics for dialogue acts, liketo ?ask?
or ?confirm?
information.
States couldthen be combinations of these, e.g., a state mightexpress ?ask bus route?
or ?confirm location?.This approach also decouples the number of top-ics from the number of states.
Throughout this pa-per, we denote the number of topics as K and thenumber of states as T .
We index words, turns anddialogues in the same ways as baseline models.We develop three generative models.
In the firstvariant (TM-HMM, Figure 2(a)), we assume everystate s in HMM is as ociated with a distributionover topics ?, and topics generate wordsw at eachutterance.
The other two models allow words tobe generated from different sources (in addition tostates), akin to the LM-HMMS model.TM-HMM generates a dialogue as following:1: For each utterance n in that dialogue, samplea state snbased on the previous state sn?1.2: For each word in utterance n, first draw atopic z from the state-specified distributionover topics ?snconditioned on sn, then gener-ate word w from the topic-specified distribu-tion over vocabulary ?zbased on z.We assume ?
?s and ?
?s are drawn from corre-sponding Dirichlet priors, as in LDA.The posterior distributions of state assignmentsnand topic assignment zn,iarep(sn|s?n, z,?,?)
?
p(sn|s?n,?)?
p(zn|s, z?n,?
), (1)p(zn,i|s,w, z?(n,i),?,?)
?
p(zn,i|s, z?(n,i),?)?
p(wn,i|sn,w?
(n,i), z,?
),where ?, ?, ?
are symmetric Dirichlet priors onstate-wise topic distribution ?t?s, topic-wise worddistribution ?t?s and state transition multinomials,respectively.
All probabilities can be computedusing collapsed Gibbs sampler for LDA (Griffithsand Steyvers, 2004) and HMM (Goldwater andGriffiths, 2007).
We iteratively sample all param-eters until convergence.3.2 TM-HMMSTM-HMMS (Figure 2(b)) extends TM-HMM to al-low words to be generated either from state LM(as in LM-HMM), or a set of dialogue topics(akin to LM-HMMS).
Because task-oriented dia-logues usually focus on a specific domain, a setof words appears repeatedly throughout a givendialogue.
Therefore, the topic distribution is of-ten stable throughout the entire dialogue, anddoes not vary from turn to turn.
For example,in the troubleshooting domain, dialogues aboutnetwork connections, desktop productivity, andanti-virus software could each map to differentsession-wide topics.
To express this, words inthe TM-HMMS model are generated either froma dialogue-specific topic distribution, or from astate-specific language model.4A distributionover sources is sampled once at the beginning ofeach dialogue and selects the expected fraction ofwords generated from different sources.The generative story for a dialogue session is:1: At the beginning of each session, draw a dis-tribution over topics ?
and a distribution overword sources ?
.2: For each utterance n in the conversation, drawa state snbased on previous state sn?1.3: For each word in utterance n, first choose aword source r according to ?
, and then de-pending on r, generate a word w either fromthe session-wide topic distribution ?
or thelanguage model specified by the state sn.4Note that a TM-HMMS model with state-specific topicmodels (instead of state-specific language models) would besubsumed by TM-HMM, since one topic could be used as thebackground topic in TM-HMMS.39Again, we impose Dirichlet priors on distributionsover topics ?
?s and distributions over words ?
?sas in LDA.
We also assume the distributions oversources ?
?s are governed by a Beta distribution.The session-wide topics is slightly differentfrom that used in LM-HMMS: LM-HMMS was de-veloped for social chats on Twitter where topicsare very diffuse and unlikely to repeat; hence of-ten unique to each dialogue.
By contrast, our mod-els are designed for task-oriented dialogues whichpertain to a given domain where topics are moretightly clustered; thus, in TM-HMMS session-widetopics are shared across the corpus.The posterior distributions of state assignmentsn, word source rn,iand topic assignment zn,iarep(sn|r, s?n,w,?,pi) ?
p(sn|s?n,?)?
p(wn|r, s,pi),p(rn,i|r?
(n,i), s,w,pi) ?
p(rn,i|r?(n,i),pi)?
p(wn,i|r, s,w?
(n,i), z,?
), (2)p(zn,i|r,w, z?(n,i),?,?)
?
p(zn,i|r, z?(n,i),?)?
p(wn,i|r,w?
(n,i), z,?
),where pi is a symmetric Dirichlet prior on session-wise word source distribution ?m?s, and othersymbols are defined above.
All these probabilitiesare Dirichlet-multinomial distributions and there-fore can be computed efficiently.3.3 TM-HMMSSThe TM-HMMSS (Figure 2(c)) model modifiesTM-HMMS to re-sample the distribution overword sources ?
at every utterance, instead of onceat the beginning of each session.
This modifica-tion allows the fraction of words drawn from thesession-wide topics to vary over the course of thedialogue.
This is attractive in task-oriented di-alogue, where some sections of the dialogue al-ways follow a similar script, regardless of sessiontopic?for example, the opening, closing, or ask-ing the user if they will take a survey.
To supportthese patterns, TM-HMMSS conditions the sourcegenerator distribution on the current state.The generative story of TM-HMMSS is verysimilar to TM-HMMS, except the distribution overword sources ?
?s are sampled at every state.
Adialogue is generated as following:1: For each session, draw a topic distribution ?.2: For each utterance n in the conversation, drawa state snbased on previous state sn?1, andsubsequently retrieve the state-specific distri-bution over word sources ?sn.3: For each word in utterance n, first sample aword source r according to ?sn, and then de-pending on r, generate a word w either fromthe session-wide topic distribution ?
or thelanguage model specified by the state sn.As in TM-HMMS, we assume multinomial distri-butions ?
?s and ?
?s are drawn from Dirichlet pri-ors; and ?
?s are governed by Beta distributions.The inference for TM-HMMSS is exactly sameas the inference for TM-HMMS, except the poste-rior distributions over word source rn,iis nowp(rn,i|r?
(n,i), s,w,pi) ?
p(rn,i|r?
(n,i), sn,pi)?
p(wn,i|r, s,w?
(n,i), z,?
), (3)where the first term is integrated over all sessionsand conditioned on the state assignment.3.4 Supporting Multiple PartiesSince our primary focus is task-oriented dia-logues between two parties, we assume everyword source is associated with two sets of LMs?one for system/agent and another for user/client.This configuration is similar to PolyLDA (Mimnoet al, 2009) or LinkLDA (Yano et al, 2009), suchthat utterances from different parties are treatedas different languages or blog-post and commentspairs.
In this work, we implement all models un-der this setting, but omit details in plate diagramsfor the sake of simplicity.In settings where the agent and client always al-ternate, each state emits both text before transi-tioning to the next state.
This is the case in theBusTime dataset, where the spoken dialogue sys-tem enforces strict turn-taking.
In settings whereagents or client may produce more than one utter-ance in a row, each state emits either agent text orclient text, then transitions to the next state.
Thisis the case in the TechSupport corpus, where eitherconversant may send a message at any time.3.5 Likelihood EstimationTo evaluate performance across different models,we compute the likelihood on held-out test set.For TM-HMM model, there are no local depen-dencies, and we therefore compute the marginallikelihood using the forward algorithm.
However,for TM-HMMS and TM-HMMSS models, the la-tent topic distribution ?
creates local dependen-cies, rendering computation of marginal likeli-40hoods intractable.
Hence, we use a Chib-styleestimator (Wallach et al, 2009).
Although it iscomputationally more expensive, it gives less bi-ased approximation of marginal likelihood, evenfor finite samples.
This ensures likelihood mea-surements are comparable across models.4 ExperimentsIn this section, we examine the effectiveness of ourmodels.
We first evaluate our models qualitativelyby exploring the inferred state diagram.
We thenperform quantitative analysis with log likelihoodmeasurements and an ordering task on a held-outtest set.
We train all models with 80% of the en-tire dataset and use the rest for testing.
We runthe Gibbs samplers for 1000 iterations and updateall hyper-parameters using slice sampling (Neal,2003; Wallach, 2008) every 10 iterations.
Thetraining likelihood suggest all models convergewithin 500?800 iterations.
For all Chib-style esti-mators, we collect 100 samples along the Markovchain to approximate the marginal likelihood.4.1 Qualitative EvaluationFigure 3 shows the state diagram for BusTime cor-pus inferred by TM-HMM without any supervi-sion.5Every dialogue is opened by asking the userto say a bus route, or to say ?I?m not sure.?
It thentransits to a state about location, e.g., origin anddestination.
Both these two states may continueto a confirmation step immediately after.
Afterverifying all the necessary information, the systemasks if the user wants ?the next few buses?.6Oth-erwise, the system follows up with the user on theparticular date and time information.
After systemreads out bus times, the user has options to ?re-peat?
or ask for subsequent schedules.In addition, we also include the human-annotated dialogue flow in Figure 4 for refer-ence (Williams, 2012).
It only illustrates the mostcommon design of system actions, without show-ing edge cases.
Comparing these two figures, thedialogue flow inferred by our model along themost probable path (highlighted in bold red in Fig-ure 3) is consistent with underlying design.
Fur-thermore, our models are able to capture edgecases?omitted for space?through a more gen-eral and probabilistic fashion.
In summary, our5Recall in BusTime, state transitions occur after each pairof system/user utterances, so we display them synchronously.6The system was designed this way because most userssay ?yes?
to this question, obviating the date and time.models yield a very similar flowchart to the under-lying design in a completely unsupervised way.7Figure 5 shows part of the flowchart forthe TechSupport corpus, generated by the TM-HMMSS model.8A conversation usually startswith a welcome message from a customer supportagent.
Next, clients sometimes report a problem;otherwise, the agent gathers the client?s identity.After these preliminaries, the agent usually checksthe system version or platform settings.
Then, in-formation about the problem is exchanged, and acycle ensues where agents propose solutions, andclients attempt them, reporting results.
Usually,a conversation loops among these states until ei-ther the problem is resolved (as the case shownin the figure) or the client is left with a referencenumber for future follow-up (not shown due tospace limit).
Although technical support is task-oriented, the scope of possible issues is vast andnot prescribed.
The table in Figure 5 lists the topranked words of selected topics?the categoriesclients often report problems in.
It illustrates that,qualitatively, TM-HMMSS discovers both problemcategories and conversation structures on our data.As one of the baseline model, we also include apart of flowchart generated by LM-HMM modelwith similar settings of T = 20 states.
Illus-trated by the highlighted states in 6, LM-HMMmodel conflates interactions that commonly occurat the beginning and end of a dialogue?i.e., ?ac-knowledge agent?
and ?resolve problem?, sincetheir underlying language models are likely to pro-duce similar probability distributions over words.By incorporating topic information, our proposedmodels (e.g., TM-HMMSS in Figure 5) are able toenforce the state transitions towards more frequentflow patterns, which further helps to overcome theweakness of language model.4.2 Quantitative EvaluationIn this section, we evaluate our models using loglikelihood and an ordering task on a held-out testset.
Both evaluation metrics measure the predic-tive power of a conversation model.7We considered various ways of making a quantitativeevaluation of the inferred state diagram, and proved difficult.Rather than attempt to justify a particular sub-division of each?design states?, we instead give several straightforward quan-titative evaluations in the next section.8Recall in this corpus, state transitions occur after emit-ting each agent or client utterances, which does not necessar-ily alternate in a dialogue, so we display client request andagent response separately.41state: ask for bus route(route:0.14), (say:0.13), (<bus-route>:0.12), (not:0.10),(sure:0.10), (im:0.09), (a:0.08), (bus:0.07), (like:0.06), ...e.g.
: say a bus route like <bus-route> or say i am not sure(<bus-route>:0.7), (the:0.07), (im:0.06), (not:0.05), (sure:0.04), (route:0.02), (any:0.01), ...e.g.
: <bus-route>/im not sure0.53state: confirm low-confidence speech recognition results(right:0.19), (is:0.19), (that:0.19), (<location>:0.12), (<bus-route>:0.05), (i:0.04), (you:0.03), (said:0.03), (thought:0.03), (over:0.03), ...e.g.
: i thought you said (<bus-route>/<location>) is that right(yes:0.45), (no:0.3), (yeah:0.12), (wrong:0.04),(correct:0.03), (back:0.02), (go:0.02), (nope:0.01), ...e.g.
: yes/no/yeah/wrong/correct/go back/nope0.120.320.530.15state: ask for locations(you:0.1), (are:0.09), (where:0.08), (to:0.07), (say:0.06), (from:0.05),(leaving:0.05), (going:0.05), (<location>:0.05), (okay:0.04), ...e.g.
: (okay <location>) say where are you (going to/leaving from)(<location>:0.84), (back:0.05), (go:0.05), ...e.g.
: <location>0.210.230.850.440.28state: ask if user is traveling now(say:0.8), (the:0.07), (you:0.07), (no:0.06), (yes:0.06), (do:0.06), (want:0.06), (buses:0.05), (few:0.05), (next:0.04), ...e.g.
: do you want the next few buses say yes or no(yes:0.5), (no:0.17), (yeah:0.16), (<bus-route>:0.07), (back:0.04), (go:0.04), (nope:0.01), ...e.g.
: yes/no/yeah0.31state: read out bus timetables(<location>:0.08), (at:0.05), (<time>:0.05), (next:0.05), (say:0.05), (from:0.04), (there:0.04), (<bus-route>:0.04), (to:0.04), ...e.g.
: there is a <bus-route> from <location> to <location> at <time> say next or repeat(next:0.4), (repeat:0.16), (over:0.11), (start:0.11),(previous:0.07), (go:0.06), (back:0.06), (goodbye:0.05), ...e.g.
: next/repeat/start over/previous0.120.42state: ask for date and time (optional)(<time>:0.14), (<date>:0.1), (the:0.06), (or:0.05), (like:0.05),(say:0.05), (you:0.05), (want:0.05), (at:0.04), (depart:0.04), ...e.g.
: say the time you want to depart like <time>(<time>:0.26), (<date>:0.14), (m:0.11),(depart:0.07), (a:0.07), (at:0.07), (by:0.03), ...e.g.
: depart (at/by) <time> a m <date>0.55Start?I?heard?61C,?is?that?right??Downtown,?is?that?correct??Did?you?just?say?Norwood??Say?just?the?day?you?want.?Say?just?the??me?you?want.?I'm?sorry,?I?can't?find?any?bus?at?all?that?run?from?Milton?to?Norwell.?I?checked?route?61C?and?I?also?checked?all?the?other?bus?routes?I?know?too.?Repeat,?next,?previous?At?11:45?PM?today,?there?is?a?61?C?from?5th?Ave?and?Main?St?Canton,?arriving?2nd?St?and?Grant?Ave?in?Norwood?at?12:34?AM.?Say?a?bus?route,?or?say?I?m?not?sure.?Where?are?you?leaving?from??(query?database)?Where?are?you?going?to??(query?database)?Do?you?want??mes?for?the?next?few?buses??
(query?database)?Figure 3: (Upper) Part of the flowchart inferred on Bus-Time, by TM-HMM model with K = 10 topics andT = 10 states.
The most probable path is highlighted,which is consistent with the underlying design (Figure 4).Cyan blocks are system actions and yellow blocks areuser responses.
In every block, the upper cell shows thetop ranked words marginalized over all topics and thelower cell shows some examples of that state.
Transitionprobability cut-off is 0.1.
States are labelled manually.Figure 4: (Left) Hand-crafted reference flowchart forBusTime (Williams, 2012).
Only the most common di-alogue flows are displayed.
System prompts shown areexample paraphrases.
Edge cases are not included.Log Likelihood The likelihood metric measuresthe probability of generating the test set under aspecified model.
As shown in Figure 7, our modelsyield as good or better likelihood than LM-HMMand LM-HMMS models on both datasets under allsettings.
For our proposed models, TM-HMMSand TM-HMMSS perform better than TM-HMMon TechSupport, but not necessarily on BusTime.In addition, we notice that the marginal benefit ofTM-HMMSS over TM-HMM is greater on Tech-Support dataset, where each dialogue focuses onone of many possible tasks.
This coincides withour belief that topics are more conversation de-pendent and shared across the entire corpus in cus-tomer support data?i.e., different clients in differ-ent sessions might ask about similar issues.Ordering Test Ritter et al (2010) proposes anevaluation based on rank correlation coefficient,which measures the degree of similarity betweenany two orderings over sequential data.
They useKendall?s ?
as evaluation metric, which is basedon the agreement between pairwise orderings oftwo sequences (Kendall, 1938).
It ranges from ?1to +1, where +1 indicates an identical orderingand ?1 indicates a reverse ordering.
The idea isto generate all permutations of the utterances ina dialogue (including true ordering), and computethe log likelihood for each under the model.
Then,Kendall?s ?
is computed between the most proba-ble permutation and true ordering.
The result is theaverage of ?
values for all dialogues in test corpus.Ritter et al (2010) limits their dataset by choos-ing Twitter dialogues containing 3 to 6 posts (ut-terances), making it tractable to enumerate all per-mutations.
However, our datasets are much larger,and enumerating all possible permutations of dia-logues with more than 20 or 30 utterances is infea-sible.
Instead, we incrementally build up the per-mutation set by adding one random permutation ata time, and taking the most probable permutationafter each addition.
If this process were continued(intractably!)
until all permutations are enumer-ated, the true value of Kendall?s ?
test would bereached.
In practice, the value appears to plateauafter a few dozen measurements.We present our results in Figure 8.
Our mod-els consistently perform as good or better than42Agent: conversation opening + identity checkhelp, answer, desk, microsoft, may,<agent>, welcom, name, number, phone, ...e.g.
: welcome to microsoft answer desk, i'm<agent>, how can i help you, may i haveyour name?Client: report problemtri, get, comput, cant, window, message,error, problem, instal, say, ...e.g.
: get problem in windows, cant installon computer, it says error messageAgent: conversation closurethank, answer, microsoft, desk, <client>,contact, help, chat, day, welcom, ...e.g.
: thank you for contacting microsoft answerdesk, you are welcome, have a nice dayAgent: acknowledge identitythank, minut, pleas, let, <client>, check,give, moment, ok, wait, ...e.g.
: thank you, <client>, please give mea moment, wait for a minute, let me checkAgent: system checkwindow, comput, instal, 7, use, 8,system, version, may, oper, ...e.g.
: may i know what version isoperating system you used?
windows 7?Client: system verificationok, ye(s), sure, pleas, thank, k,<prodkey>, one, problem, fine, ...e.g.
: ok, thanks, sure, <prodkey>,one problemAgent: acknowledge problemerror, messag, see, issu, sorri, help, get,thank, <client>, oh, ...e.g.
: sorry to hear that, thanks for errormessage, i see, let me help you on issueAgent: troubleshoot attemptclick, <href>, pleas, link, code, let, go,download, run, ok, ...e.g.
: please click <href> and go downloadthe code, let it run and see it is okClient: troubleshoot acknowledgementok, link, click, ye(s), code, dont, tri,download, get, say, ...e.g.
: ok, i am trying to download the codeClient: identity verification<email>, <phoneno>, <client>, ye(s),number, phone, email, name, sure, call, ...e.g.
: yes, my name is <client>sure, <client>, <phoneno>, <email>Agent: troubleshoot attemptinstal, comput, program, tri, issu,system, file, work, run, see, ...e.g.
: try to install file or run programand see the issue goes awayClient: resolved problemthank, ok, help, great, good, much,<agent>, ye(s), day, bye, ...e.g.
: yes, thank you <agent> so muchfor your great help, good day, bye0.2405810.209160.2072560.1456820.1315580.1225120.1172760.1085470.12918650.07982350.07649090.08935190.08014760.0854540.0776940.0881720.07284980.0839120.09173230.0888547Agent: conversation opening + identity checkhelp, answer, desk, microsoft, may, #agent#, welcom,name, number, phone, ...e.g.
: welcome to microsoft answer desk, i'm #agent#,how can i help you, may i have your name?Client: report problemtri, get, comput, cant, window, message, error,problem, instal, say, ...e.g.
: i get this problem in windows, cant installon my computer, it says this error messageAgent: conversation closurethank, answer, microsoft, desk, #client#,contact, help, chat, day, welcom, ...e.g.
: thank you for contacting microsoft answerdesk, you are welcome, have a nice dayAgent: acknowledge identitythank, minut, pleas, let, #client#, check,give, moment, ok, wait, ...e.g.
: thank you, #client#, please give me amoment, wait for a minute, let me checkAgent: system checkwindow, comput, instal, 7, use, 8,system, version, may, oper, ...e.g.
: may i know what version isoperating system you used?
windows 7?Client: system verificationok, ye(s), sure, pleas, thank, k, #prodkey#,one, problem, fine, ...e.g.
: ok, thanks, sure, #prodkey#, one problemAgent: acknowledge problemerror, messag, see, issu, sorri, help, get,thank, #client#, oh, ...e.g.
: sorry to hear that, thanks for errormessage, i see, let me help you on issueAgent: troubleshoot attemptclick, #href#, pleas, link, code, let, go,download, run, ok, ...e.g.
: please click #href# and go downloadthe code, let it run and see it is okClient: troubleshoot acknowledgementok, link, click, ye(s), code,dont, tri, download, get, say, ...e.g.
: ok, i am trying todownload the code, yesClient: identity verification#email#, #phoneno#, #client#, ye(s),number, phone, email, name, sure, call, ...e.g.
: yes, my name is #client#sure, #client#, #phoneno#, #email#Agent: troubleshoot attemptinstal, comput, program, tri, issu,system, file, work, run, see, ...e.g.
: try to install file or run programand see the issue goes awayClient: resolved problemthank, ok, help, great, good, much,#agent#, ye(s), day, bye, ...e.g.
: yes, thank you #agent# so muchfor your great help, good day, bye0.02405810.0209160.02072560.01456820.01315580.01225120.01172760.01085470.008918650.007982350.007649090.005935190.008014760.004554540.003776940.005681720.003284980.00539120.006173230.00588547Agent: conversation opening + identity checkhelp, answer, desk, may, <agent-name>,welcom, name, number, phone, ...e.g.
: welcome to answer desk, i'm <agent-name>, how can i help you, may i haveyour name?Client: report problemtri, get, comput, cant, window, message,error, problem, instal, say, ...e.g.
: get problem in windows, cant installon computer, it says error messageAgent: conversation closurethank, answer, desk, <client-name>, contact,help, chat, day, welcom, ...e.g.
: thank you for contacting answer desk,you are welcome, have a nice dayAgent: acknowledge identitythank, minut, pleas, let, <client-name>,check, give, moment, ok, wait, ...e.g.
: thank you, <client-name>, pleasegive me a moment, let me checkAgent: system checkwindow, comput, instal, 7, use, 8,system, version, may, oper, ...e.g.
: may i know what version isoperating system you used?
windows 7?Client: system verificationok, ye(s), sure, pleas, thank, k,<prodkey>, one, problem, fine, ...e.g.
: ok, thanks, sure, <prodkey>,one problemAgent: acknowledge problemerror, messag, see, issu, sorri, help, get,thank, <client-name>, oh, ...e.g.
: sorry to hear that, thanks for errormessage, i see, let me help you on issueAgent: troubleshoot attemptclick, <href>, pleas, link, code, let, go,download, run, ok, ...e.g.
: please click <href> and go downloadthe code, let it run and see it is okClient: troubleshoot acknowledgementok, link, click, ye(s), code, dont, tri,download, get, say, ...e.g.
: ok, i am trying to download the codeClient: identity verification<email>, <phone>, <client-name>, ye(s),number, phone, email, name, sure, call, ...e.g.
: yes, my name is <client-name>sure, <client-name>, <phone>, <email>Agent: troubleshoot attemptinstal, comput, program, tri, issu,system, file, work, run, see, ...e.g.
: try to install file or run programand see the issue goes awayClient: resolved problemthank, ok, help, great, good, much,<agent-name>, ye(s), day, bye, ...e.g.
: great, thanks <agent-name> somuch for your help, good day, bye0.240.210.210.150.130.120.120.110.130.080.080.090.080.090.080.090.070.080.090.09Agent: conversation opening + identity checkhelp, answer, desk, microsoft, may, <agent-name>, welcom, name, number, phone, ...e.g.
: welcome to microsoft answer desk, i'm<agent-name>, how can i help you, may ihave your name?Client: report problemtri, get, comput, cant, window, message,error, problem, instal, say, ...e.g.
: get problem in windows, cant installon computer, it says error messageAgent: conversation closurethank, answer, microsoft, desk, <client-name>,contact, help, chat, day, welcom, ...e.g.
: thank you for contacting microsoft answerdesk, you are welcome, have a nice dayAgent: acknowledge identitythank, minut, pleas, let, <client-name>,check, give, moment, ok, wait, ...e.g.
: thank you, <client-name>, pleasegive me a moment, let me checkAgent: system checkwindow, comput, instal, 7, use, 8,system, version, may, oper, ...e.g.
: may i know what version isoperating system you used?
windows 7?Client: system verificationok, ye(s), sure, pleas, thank, k,<prodkey>, one, problem, fine, ...e.g.
: ok, thanks, sure, <prodkey>,one problemAgent: acknowledge problemerror, messag, see, issu, sorri, help, get,thank, <client-name>, oh, ...e.g.
: sorry to hear that, thanks for errormessage, i see, let me help you on issueAgent: troubleshoot attemptclick, <href>, pleas, link, code, let, go,download, run, ok, ...e.g.
: please click <href> and go downloadthe code, let it run and see it is okClient: troubleshoot acknowledgementok, link, click, ye(s), code, dont, tri,download, get, say, ...e.g.
: ok, i am trying to download the codeClient: identity verification<email>, <phone>, <client-name>, ye(s),number, phone, email, name, sure, call, ...e.g.
: yes, my name is <client-name>sure, <client-name>, <phone>, <email>Agent: troubleshoot attemptinstal, comput, program, tri, issu,system, file, work, run, see, ...e.g.
: try to install file or run programand see the issue goes awayClient: resolved problemthank, ok, help, great, good, much,<agent-name>, ye(s), day, bye, ...e.g.
: great, thanks <agent-name> somuch for your help, good day, bye0.240.210.210.150.130.120.120.110.130.080.080.090.080.090.080.090.070.080.090.09Topic Top Ranked Wordspurchasemicrosoft, store, purchas, able, get,sir, order, site, mr, contact, mac, .
.
.browserinternet, explor, browser, ie, open,websit, googl, download, click,chrome, .
.
.backupfile, restor, system, comput, back,folder, creat, option, dont, delet, .
.
.bootcomput, boot, mode, option, disc,safe, recoveri, repair, back, clean,cd, disk, .
.
.updateupdat, window, servic, instal, pack,run, comput, download, check,restart, inform, system, error, fix, .
.
.networkconnect, internet, printer, comput,network, pc, print, access, wireless,hp, cable, adapt, router, speed, .
.
.anti-virus viru, scan, comput, remov, secur,run, system, anti, essenti, infect, de-fend, softwar, program, protect, an-tiviru, malwar, .
.
.hardwaredriver, devic, drive, dvd, cd, hard-war, issu, model, laptop, plug, soft-ware, usb, .
.
.windows window, upgrad, 8, download, 7, in-stal, bit, vista, pro, system, .
.
.officeoffic, 2010, word, microsoft, home,excel, version, 2007, student, docu-ment, trial, 2013, .
.
.outlookoutlook, account, email, mail, mi-crosoft, com, live, password, profil,contact, creat, server, access, .
.
.licensekey, product, activ, purchas, licens,valid, verifi, id, disc, pro, grenuin,.
.
.facilitywindow, 8, comput, instal, manufac-tur, system, oem, 7, pc, hp, .
.
.Figure 5: Part of flowchart (left) and topic table (right) on TechSupport dataset, generated by TM-HMMSSmodel under settings ofK = 20 topics and T = 20 states.
The topic table lists top ranked words in issuesdiscussed in the chats.
Cyan blocks are system actions and yellow blocks are user responses.
In everyblock, the upper cell shows top ranked words, and the lower cell shows example string patterns of thatstate.
Transition probability cut-off is 0.05.
States and topics are labelled manually.Agent: conversation opening + identity checkanswer, desk, help, <agent-name>,welcom, today, may, name, number, ...e.g.
: welcome to answer desk, i'm <agent-name>, how can i help you, may i haveyour name, case/phone number, account?Client: acknowledge agent / resolved problemthank, ok, help, much, good, great,<agent-name>, day, appreci, bye, ...e.g.
: ok, thanks, great, <agent-name>appreciate your help, good day, byeAgent: conversation closureanswer, desk, thank, contact, day, chat,great, session, com, help, ...e.g.
: thank you for contacting answer desk,you are welcome, have a nice dayAgent: acknowledge problemissu, sorri, call, help, number, suport,concern, <client-name>, <phone>, best, ...e.g.
: sorry to hear that, let me help withyour concern, <client-name>Client: confirm identitycall, number, phone, case, <time>, would,<agent-name>, pleas, <phone>, time, ...e.g.
: <agent-time>, my phone number is<phone>.
would you pleas call number...Agent: conversation closureanyth, els, welcom, help, <client-name>,today, assist, question, would, answer, ...e.g.
: you are welcome, anything else todayi would help/assist you, <client-name>?Agent: acknowledge identitygive, minut, pleas, check, let,thank, moment, 3, one, 5, ...e.g.
: thanks, one moment please,give me 3 minutes, let me checkClient: report problemupdat, window, install, <agent-name>, hello,error, get, problem, download, message, ...e.g.
: hello, <agent-name>, i get problem/errorwhen install/update/download in windows0.080.10.080.070.140.050.070.060.050.050.080.06state 0 0(0, answer) 0.14307(0, desk) 0.140908(0, help) 0.139542(0, #agentname#) 0.122925(0, microsoft) 0.121104(0, welcom) 0.119283(0, today) 0.0638546(0, may) 0.0453027(0, name) 0.0382462(0, hello) 0.023564state 0 1(0, offic) 0.129144(0, instal) 0.0691015(0, 2013) 0.0479101(0, purchas) 0.0395219(0, 365) 0.0366522(0, home) 0.0362107(0, product) 0.0322373(0, download) 0.0280432(0, key) 0.0260565(0, 2010) 0.024732state 1 0(1, offic) 0.211557(1, instal) 0.0920019(1, 2013) 0.057979(1, 365) 0.0517066(1, version) 0.0410626(1, purchas) 0.028898(1, home) 0.026237(1, use) 0.0245264(1, 2010) 0.0222455(1, product) 0.0188242state 1 1(1, updat) 0.117013(1, window) 0.0831741(1, instal) 0.0484751(1, #agentname#) 0.0456075(1, hello) 0.0335632(1, error) 0.0304088(1, get) 0.0246734(1, problem) 0.0212322(1, download) 0.0209454(1, messag) 0.0195116state 3 0(3, give) 0.0975515(3, minut) 0.0942818(3, pleas) 0.0906489(3, check) 0.0884691(3, let) 0.0703043(3, thank) 0.0675796(3, moment) 0.0523213(3, 3) 0.0485067(3, one) 0.0308869(3, 5) 0.0305236state 3 1(3, window) 0.129853(3, 7) 0.0926994(3, home) 0.074722(3, bit) 0.0595411(3, premium) 0.0479556(3, 64) 0.0463576(3, vista) 0.0419631(3, instal) 0.0351717(3, servic) 0.0323752(3, pack) 0.0303777state 4 0(4, support) 0.0786022(4, premium) 0.0519191(4, servic) 0.0504038(4, #dollaramt#) 0.0501403(4, issu) 0.0425636(4, softwar) 0.0376882(4, warranti) 0.035514(4, day) 0.0312315(4, 30) 0.0210854(4, fix) 0.0206901state 4 1(4, pay) 0.0413008(4, #dollaramt#) 0.0393345(4, dont) 0.0325617(4, fix) 0.0310324(4, support) 0.0275368(4, much) 0.0231672(4, ok) 0.0216379(4, cost) 0.0214194(4, issu) 0.0192346(4, money) 0.0192346state 5 0(5, thank) 0.313335(5, #clientname#) 0.149405(5, inform) 0.0777987(5, wait) 0.0738004(5, much) 0.0363618(5, correct) 0.0243669(5, patienc) 0.021459(5, patient) 0.0200051(5, #email#) 0.0156433(5, card) 0.0138259state 6 0(6, comput) 0.0520198(6, instal) 0.0475635(6, system) 0.038489(6, file) 0.0370306(6, viru) 0.035248(6, updat) 0.0345188(6, program) 0.0290093(6, run) 0.0215552(6, tri) 0.0201778(6, caus) 0.0179091state 6 1(6, window) 0.146467(6, 8) 0.0713756(6, instal) 0.0647403(6, 7) 0.0538974(6, comput) 0.0349627(6, upgrad) 0.0216923(6, laptop) 0.0199121(6, new) 0.0189411(6, ye) 0.0179701(6, use) 0.0178082state 7 0(7, answer) 0.0739787(7, microsoft) 0.0701565(7, desk) 0.0603621(7, thank) 0.0573362(7, contact) 0.042764(7, day) 0.0422066(7, chat) 0.0359159(7, great) 0.0321733(7, session) 0.0280325(7, com) 0.0226177state 7 1(7, call) 0.0779531(7, number) 0.0643551(7, phone) 0.0394806(7, case) 0.0305258(7, #time#) 0.0232293(7, would) 0.0222343(7, #agentname#) 0.0219027(7, pleas) 0.021571(7, #phoneno#) 0.0199127(7, time) 0.0169278state 8 0(8, click) 0.105887(8, right) 0.0329512(8, pleas) 0.0285344(8, start) 0.0279375(8, type) 0.0272213(8, open) 0.0236401(8, press) 0.0205365(8, window) 0.020059(8, see) 0.0197009(8, option) 0.0193427state 8 1(8, ok) 0.0348863(8, click) 0.0326788(8, internet) 0.0309129(8, screen) 0.0262772(8, open) 0.0238491(8, see) 0.0236283(8, dont) 0.0203172(8, right) 0.0200964(8, say) 0.0200964(8, window) 0.0194342state 9 1(9, #email#) 0.244926(9, #phoneno#) 0.240782(9, #clientname#) 0.144641(9, phone) 0.0414562(9, number) 0.0298531(9, ye) 0.0277811(9, email) 0.0273667(9, name) 0.0219795(9, 4) 0.0136915(9, cell) 0.0124484state 10 0(10, window) 0.155062(10, instal) 0.0641206(10, comput) 0.0618775(10, 8) 0.0571107(10, 7) 0.0501943(10, system) 0.0422498(10, use) 0.0337444(10, oper) 0.0298189(10, version) 0.0250522(10, manufactur) 0.0186031state 10 1(10, updat) 0.0326156(10, instal) 0.0297734(10, comput) 0.0230421(10, problem) 0.0224438(10, ago) 0.0221446(10, program) 0.0218454(10, system) 0.0210975(10, dont) 0.0209479(10, restor) 0.0190033(10, fix) 0.0185545state 11 0(11, number) 0.13071(11, phone) 0.0902297(11, email) 0.0720301(11, may) 0.0627649(11, case) 0.0584632(11, address) 0.0581323(11, pleas) 0.0576911(11, name) 0.05361(11, chat) 0.030447(11, disconnect) 0.0212921state 12 0(12, issu) 0.0431294(12, sorri) 0.0419008(12, call) 0.0310888(12, help) 0.0286316(12, number) 0.0264201(12, support) 0.0243314(12, concern) 0.0229799(12, #clientname#) 0.0218741(12, #phoneno#) 0.0202769(12, best) 0.0194169state 12 1(12, thank) 0.282447(12, ok) 0.0864091(12, help) 0.069528(12, much) 0.0459309(12, good) 0.0419375(12, great) 0.0410299(12, #agentname#) 0.0401223(12, day) 0.0217892(12, appreci) 0.0192479(12, bye) 0.0179773state 14 0(14, comput) 0.0877508(14, access) 0.061482(14, remot) 0.0601575(14, ok) 0.0432704(14, let) 0.0359858(14, connect) 0.0334472(14, tri) 0.0309086(14, issu) 0.0302464(14, restart) 0.0281493(14, check) 0.0224099state 14 1(14, ok) 0.436864(14, ye) 0.106903(14, thank) 0.101863(14, pleas) 0.0313091(14, sure) 0.0262695(14, great) 0.0199037(14, wait) 0.0191079(14, oh) 0.0164555(14, k) 0.015925(14, let) 0.0148641state 15 0(15, #href#) 0.129779(15, click) 0.118174(15, link) 0.0862138(15, pleas) 0.0784769(15, code) 0.055727(15, download) 0.0259771(15, run) 0.0230297(15, accept) 0.019806(15, open) 0.0179639(15, remot) 0.0166745state 15 1(15, ok) 0.100091(15, link) 0.0744436(15, click) 0.0557658(15, download) 0.0376455(15, code) 0.0292823(15, dont) 0.0281672(15, copi) 0.0245431(15, #href#) 0.0242643(15, page) 0.0237068(15, past) 0.0237068state 16 0(16, anyth) 0.1478(16, els) 0.146359(16, welcom) 0.0844204(16, help) 0.0792348(16, #clientname#) 0.0754896(16, today) 0.066847(16, assist) 0.0518665(16, question) 0.0233458(16, would) 0.0193126(16, answer) 0.0161437state 19 0(19, issu) 0.0736645(19, troubleshoot) 0.0553393(19, support) 0.0440807(19, step) 0.0358164(19, link) 0.0340198(19, fix) 0.0339001(19, help) 0.0325826(19, resolv) 0.0285103(19, advanc) 0.0240787(19, option) 0.02300080.0185990.01753580.01621860.0154410.01499670.01477450.01477450.01439360.01388580.01274320.01229890.01139430.01074360.01066430.009109090.008331480.007617350.00728410.006744530.00639540.006316060.006252580.006093880.005903450.005792360.005760620.005697150.005681280.005189320.005094110.00504650.00488780.004538670.004395850.004126070.004078460.004030850.004030850.003824550.003776940.003665850.00353890.00353890.00353890.003491290.003427810.003269110.003269110.003253250.003189770.00317390.003078680.002999330.002967590.002840640.002777160.002761290.002666070.002666070.00265020.002618470.002554990.002554990.00250738Figure 6: Part of flowchart on Tech-Support dataset, generated by LM-HMMmodel with T = 20 states.
Cyan blocksare system actions and yellow blocks areuser responses.
In very block, the uppercell shows the top ranked words, and thelower cell shows example word sequencesor string patterns of that state.
Transitionprobability cut-off is 0.05.
States are la-belled manually.
A poorly-inferred stateis highlighted, which seems to confla ethe ?acknowledge agent?
and ?resolveproblem?
states, a d TM-HMMSS modelhas properly disentangled (Figure 5).the baseline models.
For BusTime data, allmodels perform relatively well except LM-HMMwhich only indicates weak correlations.
TM-HMM out-performs all other models under all set-tings.
This is also true for TechSupport dataset.LM-HMMS, TM-HMMS and TM-HMMSS mod-els perform considerably well on BusTime, butnot on TechSupport data.
These three models al-low words to be generated from additional sourcesother than states.
Although this improves log like-lihood, it is possible these models encode less in-formation about the state sequences, at least inthe more diffuse TechSupport data.
In summary,under both quantitative evaluation measures, ourmodels advance state-of-the-art, however which ofour models is best depends on the application.43K10 K20 K30150000200000250000150000200000250000150000200000250000T10T20T30LM?HMMLM?HMMSTM?HMMTM?HMMSTM?HMMSSLM?HMMLM?HMMSTM?HMMTM?HMMSTM?HMMSSLM?HMMLM?HMMSTM?HMMTM?HMMSTM?HMMSSmodelnegativeloglikelihoodK10 K20 K306e+057e+058e+056e+057e+058e+056e+057e+058e+05T10T20T30LM?HMMLM?HMMSTM?HMMTM?HMMSTM?HMMSSLM?HMMLM?HMMSTM?HMMTM?HMMSTM?HMMSSLM?HMMLM?HMMSTM?HMMTM?HMMSTM?HMMSSmodelnegativeloglikelihoodFigure 7: Negative log likelihood on BusTime (upper) and TechSupport (lower) datasets (smaller is better)under different settings of topics K and states T .K10 K20 K300.00.51.00.00.51.00.00.51.0T10T20T300 25 50 75 100 0 25 50 75 100 0 25 50 75 100# of random permutationsaveragekendall's taumodel LM?HMM LM?HMMS TM?HMM TM?HMMS TM?HMMSSK10 K20 K300.00.51.00.00.51.00.00.51.0T10T20T300 25 50 75 100 0 25 50 75 100 0 25 50 75 100# of random permutationsaveragekendall's taumodel LM?HMM LM?HMMS TM?HMM TM?HMMS TM?HMMSSFigure 8: Average Kendall?s ?
measure on BusTime (upper) and TechSupport (lower) datasets (larger isbetter) against number of random permutations, under various settings of topics K and states T .5 Conclusion and Future WorkWe have presented three new unsupervised mod-els to discover latent structures in task-orienteddialogues.
We evaluated on two very differentcorpora?logs from spoken, human-computer dia-logues about bus time, and logs of textual, human-human dialogues about technical support.
Wehave shown our models yield superior perfor-mance both qualitatively and quantitatively.One possible avenue for future work is scala-bility.
Parallelization (Asuncion et al, 2012) oronline learning (Doucet et al, 2001) could signif-icantly speed up inference.
In addition to MCMC,another class of inference method is variationalBayesian analysis (Blei et al, 2003; Beal, 2003),which is inherently easier to distribute (Zhai et al,2012) and online update (Hoffman et al, 2010).AcknowledgmentsWe would like to thank anonymous reviewers andJordan Boyd-Graber for their valuable comments.We are also grateful to Alan Ritter and Bill Dolanfor their helpful discussions; and Kai (Anthony)Lui for providing TechSupport dataset.44ReferencesArthur Asuncion, Padhraic Smyth, Max Welling,David Newman, Ian Porteous, and Scott Triglia,2012.
Distributed Gibbs sampling for latent vari-able models.Satanjeev Banerjee and Alexander I Rudnicky.
2006.A texttiling based approach to topic boundary detec-tion in meetings.
In INTERSPEECH.Srinivas Bangalore, Giuseppe Di Fabbrizio, andAmanda Stent.
2006.
Learning the structure of task-driven human-human dialogs.
In ACL, Stroudsburg,PA, USA.Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In NAACL, pages113?120.Matthew J. Beal.
2003.
Variational Algorithms for Ap-proximate Bayesian Inference.
Ph.D. thesis.Alan W Black, Susanne Burger, Alistair Conkie,Helen Hastie, Simon Keizer, Nicolas Merigaud,Gabriel Parent, Gabriel Schubiner, Blaise Thomson,D.
Williams, Kai Yu, Steve Young, and Maxine Es-kenazi.
2010.
Spoken dialog challenge 2010: Com-parison of live and control test results.
In SIGDIAL.David M. Blei, Andrew Ng, and Michael Jordan.
2003.Latent Dirichlet alocation.
JMLR.Ananlada Chotimongkol.
2008.
Learning the Struc-ture of Task-oriented Conversations from the Corpusof In-domain Dialogs.
Ph.D. thesis.Nigel Crook, Ramn Granell, and Stephen G. Pulman.2009.
Unsupervised classification of dialogue actsusing a dirichlet process mixture model.
In SIG-DIAL.Hal Daum?e III and Daniel Marcu.
2006.
Bayesianquery-focused summarization.
In ACL-44: Pro-ceedings of the 21st International Conference onComputational Linguistics and the 44th annualmeeting of the Association for Computational Lin-guistics, pages 305?312, Morristown, NJ, USA.
As-sociation for Computational Linguistics.David DeVault, Kallirroi Georgila, Ron Artstein, Fab-rizio Morbini, David Traum, Stefan Scherer, AlbertRizzo, and Louis-Philippe Morency.
2013.
Verbalindicators of psychological distress in interactive di-alogue with a virtual human.
In SIGDIAL.Arnaud Doucet, Nando De Freitas, and Neil Gordon,editors.
2001.
Sequential Monte Carlo methods inpractice.
Springer Texts in Statistics.Michel Galley, Kathleen McKeown, Eric Fosler-Lussier, and Hongyan Jing.
2003.
Discourse seg-mentation of multi-party conversation.
In ACL.Sharon Goldwater and Thomas L. Griffiths.
2007.A fully Bayesian approach to unsupervised part-of-speech tagging.
In ACL.Thomas L. Griffiths and Mark Steyvers.
2004.
Findingscientific topics.
PNAS, 101(Suppl 1):5228?5235.James Henderson and Oliver Lemon.
2008.
Mixturemodel POMDPs for efficient handling of uncertaintyin dialogue management.
In ACL.Matthew Hoffman, David M. Blei, and Francis Bach.2010.
Online learning for latent Dirichlet alocation.In NIPS.Pei-yun Hsueh, Johanna D. Moore, and Steve Renals.2006.
Automatic segmentation of multiparty dia-logue.
In EACL.Llu?
?s F. Hurtado, Joaquin Planells, Encarna Segarra,Emilio Sanchis, and David Griol.
2010.
A stochas-tic finite-state transducer approach to spoken dialogmanagement.
In INTERSPEECH.Dan Jurafsky, Elizabeth Shriberg, and Debra Bi-asca.
1997.
Switchboard SWBD-DAMSL shallow-discourse-function annotation coders manual.
Insti-tute of Cognitive Science Technical Report, pages97?02.Maurice G. Kendall.
1938.
A new measure of rankcorrelation.
Biometrika Trust.Staffan Larsson and David R. Traum.
2000.
Informa-tion state and dialogue management in the TRINDIdialogue move engine toolkit.
Natural LanguageEngineering, 5(3/4):323?340.Esther Levin, Roberto Pieraccini, and Wieland Eckert.2000.
A stochastic model of human-machine inter-action for learning dialogue strategies.
IEEE Transon Speech and Audio Processing, 8(1):11?23.Jingjing Liu, Stephanie Seneff, and Victor Zue.
2010.Dialogue-oriented review summary generation forspoken dialogue recommendation systems.
InNAACL.David Mimno, Hanna Wallach, Jason Naradowsky,David Smith, and Andrew McCallum.
2009.Polylingual topic models.
In EMNLP.Gabriel Murray, Steve Renals, and Jean Carletta.
2005.Extractive summarization of meeting recordings.
InEuropean Conference on Speech Communicationand Technology.Radford M. Neal.
2000.
Markov chain sampling meth-ods for Dirichlet process mixture models.
Journal ofComputational and Graphical Statistics, 9(2):249?265.Radford M. Neal.
2003.
Slice sampling.
Annals ofStatistics, 31:705?767.45Matthew Purver, Konrad K?ording, Thomas L. Griffiths,and Joshua Tenenbaum.
2006.
Unsupervised topicmodelling for multi-party spoken discourse.
In ACL.Verena Rieser and Oliver Lemon.
2010.
Natural lan-guage generation as planning under uncertainty forspoken dialogue systems.
In EMNLP.Alan Ritter, Colin Cherry, and Bill Dolan.
2010.
Un-supervised modeling of twitter conversations.
InNAACL.Satinder Singh, Diane Litman, Michael Kearns, andMarilyn Walker.
2002.
Optimizing dialogue man-agement with reinforcement learning: Experimentswith the NJFun system.
Journal of Artificial Intelli-gence Research.Andreas Stolcke, Noah Coccaro, Rebecca Bates, PaulTaylor, Carol Van Ess-Dykema, Klaus Ries, Eliza-beth Shriberg, Daniel Jurafsky, Rachel Martin, andMarie Meteer.
2000.
Dialogue act modeling forautomatic tagging and recognition of conversationalspeech.
Computational Linguistics, September.David R Traum and Staffan Larsson.
2003.
The in-formation state approach to dialogue management.In Current and new directions in discourse and dia-logue, pages 325?353.Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov,and David Mimno.
2009.
Evaluation methods fortopic models.
In ICML.Hanna M. Wallach.
2008.
Structured Topic Models forLanguage.
Ph.D. thesis, University of Cambridge.Jason D. Williams and Suhrid Balakrishnan.
2009.
Es-timating probability of correctness for ASR N-bestlists.
In SIGDIAL.Jason D. Williams.
2012.
Challenges and opportuni-ties for state tracking in statistical spoken dialog sys-tems: Results from two public deployments.
Jour-nal of Selected Topics in Signal Processing.Tae Yano, William W. Cohen, and Noah A. Smith.2009.
Predicting response to political blog postswith topic models.
In NAACL, pages 477?485,Stroudsburg, PA, USA.
ACL.Steve Young.
2006.
Using POMDPs for dialog man-agement.
In Proceedings of the 1st IEEE/ACL Work-shop on Spoken Language Technologies (SLT06).Ke Zhai, Jordan Boyd-Graber, Nima Asadi, and Mo-hamad Alkhouja.
2012.
Mr. LDA: A flexible largescale topic modeling package using variational in-ference in mapreduce.
In WWW.46
