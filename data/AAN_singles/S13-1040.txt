Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 276?285, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsMetaphor Identification as InterpretationEkaterina ShutovaInternational Computer Science Institute andInstitute for Cognitive and Brain SciencesUniversity of California, Berkeleykatia@berkeley.eduAbstractAutomatic metaphor identification and inter-pretation in text have been traditionally con-sidered as two separate tasks in natural lan-guage processing (NLP) and addressed in-dividually within computational frameworks.However, cognitive evidence suggests that hu-mans are likely to perform these two tasks si-multaneously, as part of a holistic metaphorcomprehension process.
We present a novelmethod that performs metaphor identificationthrough its interpretation, being the first onein NLP to combine the two tasks in onestep.
It outperforms the previous approachesto metaphor identification both in terms of ac-curacy and coverage, as well as providing aninterpretation for each identified expression.1 IntroductionMetaphor undoubtedly gives our expression morevividness, distinction and artistry, however, it is alsoan important linguistic tool that has long becomepart of our every-day language.
Metaphors arisewhen one concept or domain is viewed in termsof the properties of another (Lakoff and Johnson,1980).
Consider the examples in (1) and (2).
(1) My car drinks gasoline.
(Wilks, 1978)(2) This policy is strangling business.The car in (1) and business in (2) are viewed asliving beings and thus they can drink or be stran-gled respectively.
The mapping between the car(the target concept) and living being (the sourceconcept) is systematic and results in a number ofmetaphorical expressions (e.g.
?This oil gives yourcar a second life?, ?this car has is very temperamen-tal?
etc.)
Lakoff and Johnson call such generalisa-tions a source?target domain mapping, or concep-tual metaphor.The ubiquity of metaphor in language has beenestablished in a number of corpus studies (Cameron,2003; Martin, 2006; Steen et al 2010; Shutovaand Teufel, 2010) and the role it plays in humanreasoning has been confirmed in psychological ex-periments (Thibodeau and Boroditsky, 2011).
Thismakes its automatic processing an important prob-lem for NLP and its numerous applications (suchas machine translation, information extraction, opin-ion mining and many others).
For example, theuse of the metaphorical verb strangle in (2) reflectsthe speaker?s negative opinion regarding the gov-ernment?s tight business regulations, which wouldbe an important fact for an opinion mining systemto discover (Narayanan, 1999).
Other experiments(Agerri, 2008) have investigated and confirmed therole of metaphor interpretation for textual entailmentresolution (RTE).The problem of metaphor modeling is rapidlygaining interest within NLP, with a growing numberof approaches exploiting statistical techniques (Ma-son, 2004; Gedigian et al 2006; Shutova, 2010;Shutova et al 2010; Turney et al 2011; Shutovaet al 2012a).
Compared to more traditional ap-proaches based on hand-coded knowledge (Fass,1991; Martin, 1990; Narayanan, 1997; Narayanan,1999; Feldman and Narayanan, 2004; Barnden andLee, 2002; Agerri et al 2007), these more recentmethods tend to have a wider coverage, as well as bemore efficient, accurate and robust.
However, eventhe statistical metaphor processing approaches so faroften focused on a limited domain or a subset of276phenomena (Gedigian et al 2006; Krishnakumaranand Zhu, 2007), and required training data (Shutovaet al 2010; Turney et al 2011), often resulting ina limited coverage.
The metaphor processing taskitself has been most commonly addressed in NLPas two individual subtasks: metaphor identificationand metaphor interpretation, with the systems focus-ing only on one of them at a time, or at best comb-ing the two in a pipeline (Shutova et al 2012a).Metaphor identification systems annotate metaphor-ical language in text, and metaphor interpretationsystems discover literal meanings of the previouslyannotated expressions.
However, cognitive evidencesuggests that humans are likely to perform identifi-cation and interpretation simultaneously, as part ofa holistic metaphor comprehension process (Coul-son, 2008; Utsumi, 2011; Gibbs and Colston, 2012).In this paper, we also take this stance and present thefirst computational method that identifies metaphori-cal expressions in unrestricted text by means of theirinterpretation.
Following Shutova (2010), we definemetaphor interpretation as a task of finding a literalparaphrase for a metaphorically used word and in-troduce the concept of symmetric reverse paraphras-ing as a criterion for metaphor identification.
Themain assumption behind our method is that the lit-eral paraphrases of literally-used words should yieldthe original phrase when paraphrased in reverse.
Forexample, when the expression ?clean the house?
isparaphrased as ?tidy the house?, the reverse para-phrasing of tidy would generate clean.
Our expec-tation is that such a symmetry in paraphrasing isindicative of literal use.
The metaphorically-usedwords are unlikely to exhibit this symmetry prop-erty when paraphrased in reverse.
For example, theliteral paraphrasing of the verb stir in ?stir excite-ment?
would yield ?provoke excitement?, but thereverse paraphrasing of provoke would not retrievestir, indicating the non-literal use of stir.We experimentally verify this hypothesis in a set-ting involving single-word metaphors expressed bya verb in verb-subject and verb-direct object rela-tions.
We apply the selectional preference-basedmetaphor paraphrasing method of Shutova (2010) toretrieve literal paraphrases of all input verbs and ex-tend the method to perform metaphor identification.In summary, our system (1) determines the likeli-hood of a verb being metaphorical based on its selec-tional preference strength (Resnik, 1993); (2) identi-fies a set of literal paraphrases for verbs that may beused metaphorically using the algorithm of Shutova(2010); (3) performs reverse paraphrasing of eachof the identified paraphrases, aiming to retrieve theoriginal expression; and (4) if the original expres-sion is retrieved then the verb is tagged as literal,otherwise it is tagged as metaphorical.We evaluated the performance of the system usingthe manually annotated metaphor corpus of Shutovaand Teufel (2010) in precision- and recall-orientedsettings.
In addition, we compared its performanceto that of a baseline using selectional preference vi-olation as an indicator of metaphor, as well as totwo previous metaphor identification approaches ofShutova et al(2010) and Turney et al(2011).2 Related WorkOne of the first attempts to identify and interpretmetaphorical expressions in text is the met* sys-tem of Fass (1991), that utilizes hand-coded knowl-edge and detects non-literalness via selectional pref-erence violation.
In case of a violation, the re-spective phrase is first tested for being metonymicusing hand-coded patterns (e.g.
CONTAINER-FOR-CONTENT).
If this fails, the system searches theknowledge base for a relevant analogy in order todiscriminate metaphorical relations from anomalousones.
The system of Krishnakumaran and Zhu(2007) uses WordNet (the hyponymy relation) andword bigram counts to predict verbal, nominal andadjectival metaphors at the sentence level.
The au-thors discriminate between conventional metaphors(included in WordNet) and novel metaphors.
Birkeand Sarkar (2006) present a sentence clustering ap-proach that employs a set of seed sentences an-notated for literalness and computes similarity be-tween the new input sentence and all of the seed sen-tences.
The system then tags the sentence as literalor metaphorical according to the annotation in themost similar seeds, attaining an f-score of 53.8%.The first system to discover source?target domainmappings automatically is CorMet (Mason, 2004).It does this by searching for systematic variationsin domain-specific verb selectional preferences.
Forexample, pour is a characteristic verb in both LABand FINANCE domains.
In the LAB domain it has277a strong preference for liquids and in the FINANCEdomain for money.
From this the system infers thedomain mapping FINANCE ?
LAB and the conceptmapping money ?
liquid.
Gedigian et al(2006)trained a maximum entropy classifier to discrimi-nate between literal and metaphorical use.
Theyannotated the sentences from PropBank (Kingsburyand Palmer, 2002) containing the verbs of MOTIONand CURE for metaphoricity.
They used PropBankannotation (arguments and their semantic types) asfeatures for classification and report an accuracyof 95.12% (however, against a majority baseline of92.90%).
The metaphor identification system ofShutova et al(2010) starts from a small seed setof metaphorical expressions, learns the analogies in-volved in their production and extends the set ofanalogies by means of verb and noun clustering.
Asa result, the system can recognize new metaphoricalexpressions in unrestricted text (e.g.
from the seed?stir excitement?
it infers that ?swallow anger?
isalso a metaphor), achieving a precision of 79%.Turney et al(2011) classify verbs and adjectivesas literal or metaphorical based on their level of con-creteness or abstractness in relation to a noun theyappear with.
They learn concreteness rankings forwords automatically (starting from a set of exam-ples) and then search for expressions where a con-crete adjective or verb is used with an abstract noun(e.g.
?dark humour?
is tagged as a metaphor and?dark hair?
is not).
They report an accuracy of 73%.3 Method3.1 Selectional Preference Strength FilteringOne of the early influential ideas in the field of com-putational metaphor processing is that metaphor rep-resents a violation of selectional preferences (SP)of a word in a given context (Wilks, 1975; Wilks,1978).
However, applied directly as an identifica-tion criterion, violation of SPs is also indicative ofmany other linguistic phenomena (e.g.
metonymy),and not only metaphor, which is problematic.
Wemodify this view and apply it to measure the poten-tial of a word to be used metaphorically based on itsselectional preference strength (SPS).
The main in-tuition behind SPS filtering is that not all verbs havean equal potential of being a metaphor.
For example,verbs such as choose, remember, describe or like donot have a strong preference for their direct objectsand are equally likely to appear with many argumentclasses.
If metaphor represents a violation of SPs,then the verbs with weak SPS are unlikely to be usedmetaphorically in any context.
For every verb in theinput text, the filter determines their likelihood ofbeing a metaphor based on their SPS and discardsthe weak ones.
The SPS filter is context-free, andthe reverse paraphrasing method is then applied inthe next steps to determine if the remaining verbsare indeed used metaphorically in the given context.We automatically acquired selectional preferencedistributions for verb-subject and verb-direct objectrelations from the British National Corpus (BNC)(Burnard, 2007) that was parsed using the RASPparser (Briscoe et al 2006; Andersen et al 2008).We applied the noun clustering method of Sun andKorhonen (2009) to 2000 most frequent nouns inthe BNC to obtain 200 common selectional prefer-ence classes.
To quantify selectional preferences, weadopted the SPS measure of Resnik (1993).
Resnikdefines SPS of a verb as the difference between theposterior distribution of noun classes in a particularrelation with the verb and their prior distribution inthat syntactic position (regardless of the verb).
Hequantifies this difference using the Kullback-Leiblerdivergence:SR(v) = D(P (c|v)||P (c)) =?cP (c|v) logP (c|v)P (c),(1)where P (c) is the prior probability of the noun class,P (c|v) is the posterior probability of the noun classgiven the verb and R is the grammatical relation.We calculated SPS for verb-subject and verb-direct object grammatical relations.
The optimal se-lectional preference strength thresholds were set ex-perimentally on a small heldout dataset at 0.30 forverb-subject and 0.70 for verb-direct object relations(via qualitative analysis of the data).
The system ex-cludes expressions containing the verbs with prefer-ence strength below these thresholds from the set ofcandidate metaphors.
Examples of verbs with weakdirect object SPs include e.g.
imagine, avoid, con-tain, dislike, make, admire, separate, remember andthe strong SPs are exhibited by e.g.
sip, hobble, roar,hoover, slam, skim, drink etc.2783.2 Literal ParaphrasingThe verbs that can be used metaphorically ac-cording to the SPS filter are then paraphrased us-ing the context-based literal paraphrasing methodof Shutova (2010).
While Shutova only usedthe method to paraphrase manually annotatedmetaphors, we extend and apply the method to para-phrasing of literally used terms and metaphor identi-fication, eliminating the need for manual annotationof metaphorical expressions.The system takes verbs and their context in theform of subject and direct-object relations as input.It generates a list of possible paraphrases of the verbthat can occur in the same context and ranks themaccording to their likelihood, as derived from thecorpus.
It then identifies shared features of the para-phrases and the verb using the WordNet (Fellbaum,1998) hierarchy and removes unrelated concepts.
Itthen identifies literal paraphrases among the remain-ing candidates based on the verb?s automatically in-duced selectional preferences and the properties ofthe context.3.2.1 Context-based Paraphrase RankingFollowing Shutova (2010), we compute the like-lihood L of a particular paraphrase of the verbv as a joint probability of the paraphrase i co-occurring with the other lexical items from its con-text w1, ..., wN in syntactic relations r1, ..., rN .Li = P (i, (w1, r1), (w2, r2), ..., (wN , rN )).
(2)Assuming statistical independence between the rela-tions of the terms in a phrase, we obtain:P (i, (w1, r1), (w2, r2), ..., (wN , rN )) =P (i) ?
P ((w1, r1)|i) ?
... ?
P ((wN , rN )|i).
(3)The probabilities can be calculated using maxi-mum likelihood estimation as P (i) = f(i)?k f(ik)and P (wn, rn|i) =f(wn,rn,i)f(i) , where f(i) is thefrequency of the interpretation irrespective of itsarguments,?k f(ik) is the number of times itspart of speech class is attested in the corpus andf(wn, rn, i) is the number of times the interpreta-tion co-occurs with context word wn in relation rn.By performing appropriate substitutions into (3), weobtain:P (i, (w1, r1), (w2, r2), ..., (wN , rN )) =f(i)?k f(ik)?f(w1, r1, i)f(i)?
... ?f(wN , rN , i)f(i)=?Nn=1 f(wn, rn, i)(f(i))N?1 ?
?k f(ik).
(4)This model is then used to rank the candidate sub-stitutes of the verb v in the fixed context accordingto the data.
The parameters of the model were esti-mated from the RASP-parsed BNC using the gram-matical relations output created by Andersen et al(2008).
The goal of this model is to emphasize theparaphrases that match the context of the verb in thesentence best.3.2.2 WordNet FilterAfter obtaining the initial list of possible substi-tutes for the verb v, the system filters out the termswhose meanings do not share any common proper-ties with that of the verb.
This overlap of propertiesis identified using the hyponymy relation in Word-Net.
Within the initial list of paraphrases, the sys-tem selects the terms that are hypernyms of the verbv, or share a common hypernym with it.
Follow-ing Shutova, we restrict the hypernym search to adepth of three levels in the taxonomy.
Table 1 showsthe filtered lists of paraphrases for the expressions?stir excitement?
and ?campaign surged?.
The goalof the filter is to discard unrelated paraphrases andthus ensure the meaning retention during paraphras-ing.
Note, however, that we define meaning reten-tion broadly, as sharing a set of similar basic prop-erties.
Such a broad definition distinguishes our sys-tem from other WordNet-based approaches to lexi-cal substitution (McCarthy and Navigli, 2007) andallows for a transition from metaphorical to literallanguage, while preserving the original meaning.3.2.3 SP-based Re-rankingThe lists of paraphrases which were generated asdescribed above contain some irrelevant paraphrases(e.g.
?campaign lifted?
for ?campaign surged?)
andsome metaphorically-used paraphrases (e.g.
?cam-paign soared?).
However, our aim is to identify lit-eral paraphrases among the candidates.
Shutova?smethod uses selectional preferences of the candi-279Log-likelihood ParaphraseVerb-DirectObjectstir excitement:-14.28 create-14.84 provoke-15.53 make-15.53 elicit-15.53 arouse-16.23 stimulate-16.23 raise-16.23 excite-16.23 conjureSubject-Verbcampaign surge:-13.01 run-15.53 improve-16.23 soar-16.23 liftTable 1: The list of paraphrases with the initial rankingdates for this purpose.
Candidates used metaphor-ically are likely to demonstrate semantic preferencefor the source domain, e.g.
soar would select forbirds or flying devices as its subject rather than cam-paigns (the target domain), whereas the ones usedliterally would have a higher preference for the tar-get domain.
This is yet another modification ofWilks?
SP violation view of metaphor.
Shutova(2010) has previously shown that selecting the para-phrases whose preferences the noun in the contextmatches best allows to filter out non-literalness, aswell as unrelated terms.As in case of the SPS filter, we automaticallyacquired selectional preference distributions of theverbs in the paraphrase lists (for verb-subject andverb-direct object relations) from the RASP-parsedBNC.
In order to quantify how well a particular ar-gument class fits the verb, we adopted the selectionalassociation measure proposed by Resnik (1993).
Se-lectional association is defined as follows:AR(v, c) =1SR(v)P (c|v) logP (c|v)P (c), (5)where P (c) is the prior probability of the noun class,P (c|v) is the posterior probability of the noun classgiven the verb and SR is the overall selectional pref-erence strength of the verb in the grammatical rela-tion R.We use selectional association as a measure ofsemantic fitness of the paraphrases into the con-Association ParaphraseVerb-DirectObjectstir excitement:0.0696 provoke0.0245 elicit0.0194 arouse0.0061 conjure0.0028 create0.0001 stimulate?
0 raise?
0 make?
0 exciteSubject-Verbcampaign surge:0.0086 improve0.0009 run?
0 soar?
0 liftTable 2: The list of paraphrases re-ranked using SPstext, which stands for their literalness.
The para-phrases are re-ranked based on their selectional as-sociation with the noun in the context.
The incor-rect or metaphorical paraphrases are de-emphasizedwithin this ranking.
The new ranking is shown inTable 2.
While the model in 3.2.1 selected the can-didate paraphrases that match the context better thanall other candidates, the SP model emphasizes theparaphrases that match this particular context betterthan any other context they may appear in.
Shutova?sexperiments have shown that the paraphrase in rank1 (i.e.
the verb with which the noun in the contexthas the highest selectional association) represents aliteral interpretation in 81% of all cases.
Such a levelof accuracy makes Shutova?s method state-of-the-artin metaphor paraphrasing.
We now apply it to thetask of metaphor identification.3.3 Reverse ParaphrasingAt the heart of our approach to metaphor iden-tification is the concept of reverse paraphrasing.The main intuition behind it is that when literally-used words are paraphrased with their literal substi-tutes, the reverse literal paraphrasing of that substi-tute should yield the original expression as one ofthe candidates.
This is, however, not the case formetaphor, since its literal paraphrase would yieldanother literal expression via literal paraphrasing.We ran the above paraphrasing method on everyverb in the input text and then again on the top280Original expression Lit.
paraphrase Reverse paraphraseVerb-DirectObjectstir excitement provoke: elicit, arouse,cause, create,stimulate, raise,makeelicit: provoke, arouse,see, derive, create,raise, makebuy a dressget: change, find, buy,purchase, take, hit,alter, ...purchase: get, buySubject-Verbcampaign surge improve: change, turnrun: succeed, direct,continue, lead, last,win, extend, ...prisoner escape flee: escape, runget: drive, go, turn,transfer, arrive,bring, come, ...Table 3: The list of top two literal paraphrases and theirreverse paraphrases, as identified by the systemtwo paraphrases it produces.
If this process resultedin retrieving the original expression then the latterwas tagged as literal, otherwise it was tagged asmetaphorical.
Some examples of reverse paraphras-ing results are given in Table 3.
One can see fromthe table that when the metaphorical verb stir in ?stirexcitement?
is paraphrased as the literal ?provoke?,the subsequent paraphrasing of ?provoke?
does notproduce ?stir?.
In contrast, when the literal expres-sion ?buy a dress?
is paraphrased as ?purchase?, thereverse paraphrasing generates ?buy?
as one of thecandidates, indicating the literalness of the originalexpression.
The same is true for the metaphoricalsurge in ?campaign surged?
and the literal escape in?the prisoner escaped?.4 Evaluation and Discussion4.1 BaselineThe baseline system is the implementation of the se-lectional preference violation view of Wilks (1978)using automatically induced SPs.
Such a choice of abaseline allows us to compare our own modificationsof the SP violation view to the original approach ofWilks in a computational setting, as well as evaluatethe latter on real-world data.
Another motivation be-hind this choice is that the symmetry of reverse para-phrasing can be seen as a kind of ?normality?
test, ina similar way as the satisfied selectional preferencesare in Wilk?s approach.
However, we believe thatthe SP-based reverse paraphrasing method capturessignificantly more information than SP violations doand thus compare the performance of the two meth-ods in an experimental setting.The baseline SP classes were created as describedabove and the preferences were quantified using se-lectional association as a measure.
The baseline sys-tem then classified the instances where selectionalassociation of the verb and the noun in the phrasewere below a certain threshold, as metaphorical.We determined the optimal threshold by qualitativeanalysis of the selectional preference distributions of50 verbs of different frequency and SPS (through theanalysis of literally and metaphorically-used argu-ments).
The threshold was averaged over individualverbs?
thresholds and equals 0.07 for direct objectrelations, and 0.09 for subject relations.4.2 Evaluation CorpusWe evaluated the system and the baseline against thecorpus of Shutova and Teufel (2010), that was man-ually annotated for metaphorical expressions.
Thecorpus is a 14,000-word subset of the BNC, withthe texts selected to retain the original balance ofgenre in the BNC itself.
The corpus contains ex-tracts from fiction, newspaper text, radio broadcast(transcribed speech), essays and journal articles onpolitics, social science and literature.
Shutova andTeufel (2010) identified 241 metaphorical expres-sions in the corpus, out of which 164 were verbalmetaphors.We parsed the corpus using the RASP parser andextracted subject and direct object relations from itsoutput.
Among the direct object relations there were310 literal phrases and 79 metaphorical ones; andamong the subject relations 206 were literal and 67metaphorical.
This constitutes a dataset of 662 rela-tions for the systems to classify.4.3 Results and DiscussionThe system and baseline performance was evaluatedagainst the corpus in terms of precision and recall.Precision, P , measures the proportion of metaphor-ical expressions that were tagged correctly among281Relation Bsln P System P Bsln R System RVerb-DObj 0.20 0.69 0.52 0.63Verb-Subj 0.13 0.66 0.59 0.70Average 0.17 0.68 0.55 0.66Table 4: Baseline and system performance by relationthe ones that were tagged by the system.
Recall,R, measures the proportion of metaphorical expres-sions that were identified out of all metaphorical ex-pressions in the gold standard corpus.
The systemP = 0.68 and R = 0.66, whereas the baseline onlyattains P = 0.17 and R = 0.55.
System perfor-mance by relation is shown in Table 4.
The hu-man ceiling for this task, according to the annotationexperiments of Shutova and Teufel (2010) approxi-mates to P = 0.80.
Figure 1 shows example sen-tences with metaphors identified and paraphrased bythe system.
Table 5 provides a breakdown of the an-notated instances into true / false positives and true/ false negatives.
As one can see from the table, thesystems can accurately annotate both metaphoricaland literal expressions, providing a balance betweenprecision and recall.The system outperforms the baseline for bothverb-subject and verb-direct object constructions.Its performance is also close to the previousmetaphor identification systems of Turney et al(2011) (accuracy of 0.73) and Shutova et al(2010)(precision of 0.79), however, the results are not di-rectly comparable due to different experimental set-tings.
Our method has a strong advantage over thesystem of Shutova et al(2010) in terms of cover-age: the latter system heavily relied on manually an-notated seed metaphors which limited its applicabil-ity in unrestricted text to the set of topics covered bythe seeds.
As opposed to this, our method is domain-independent and can be applied to any data.
Shutovaet al(2010) have not measured the recall of theirsystem, however indicated its possible coverage lim-itations.In addition, our system produces paraphrases forthe identified metaphorical expressions.
Since theidentification is directly dependent on the qualityof literal paraphrasing, the majority of the inter-pretations the system provided for the identifiedmetaphors appear to be correct.
However, we founda few instances where, despite the correct initialparaphrasing, the system was not able to identifyFYT Gorbachev inherited a Soviet state which was, ina celebrated Stalinist formulation, national in form butsocialist in content.Paraphrase: Gorbachev received a Soviet state whichwas, in a celebrated Stalinist formulation, national inform but socialist in content.CEK The Clinton campaign surged again and he easilywon the Democratic nomination.Paraphrase: The Clinton campaign improved again andhe easily won the Democratic nomination.CEK Their views reflect a lack of enthusiasm amongthe British people at large for John Major ?s idea of Eu-ropean unity.Paraphrase: Their views show a lack of enthusiasmamong the British people at large for John Major ?s ideaof European unity.J85 [..] the reasons for this superiority are never spelledout.Paraphrase [..] the reasons for this superiority are neverspecified.J85 Anyone who has introduced speech act theory tostudents will know that these technical terms are not atall easy to grasp.Paraphrase: Anyone who has introduced speech act the-ory to students will know that these technical terms arenot at all easy to understand.G0N The man?s voice cut in .Paraphrase: The man?s voice interrupted.Figure 1: Metaphors tagged by the system (in bold) andtheir paraphrasesthe metaphor, usually in case of highly convention-alized metaphorical expressions.
Overall, the mostfrequent system errors fall into the following cate-gories:Errors due to incorrect parsing: The system failedto discover some of the metaphorical expressions inthe corpus since their grammatical relations weremissed by the parser.
In addition, some of the in-stances were misclassified, e.g.
?pounds paid to[...]?
or ?change was greatly accelerated?
were la-beled as subject relations.
Overall, the parser missed9 metaphorical expressions.Errors due to incorrect paraphrasing: The mostcommon type of error that leads to false positives isthe incorrect paraphrasing (resulting in a change ofmeaning).
This makes it nearly impossible for thesystem to retrieve the original term.
There were also282Positives Negatives TotalTrue 99 464 563False 47 52 99Total 146 516Table 5: System tagging statisticscases where the system could not generate any para-phrase (usually for literal expressions, e.g.
?play ananthem?
).Errors due to metaphorical paraphrasing: Someof the system errors are due to metaphorical para-phrasing.
For example, the metaphorical expression?mend marriage?
was paraphrased as ?repair mar-riage?, which is also used metaphorically.
And re-pair in return generated mend, when paraphrased inreverse.
Errors of this type have been mainly trig-gered by the WordNet filter, and the fact that somemetaphorical senses are included in WordNet.Errors due to metaphor conventionality: a num-ber of conventional metaphors were missed by thesystem, since the original verb was retrieved due toits conventionality.
Such examples include ?imposea decision?, ?put the issue forward?, ?lead a life?.Such cases suggest that the system is better suited toidentify more creative, novel metaphors.Cases of metonymy: a few cases of gen-eral metonymy were annotated by the system asmetaphorical, e.g.
?shout support?, which stands for?shout the words of support?, and ?humiliate a mo-ment?, that is likely to mean ?humiliate the event ofthe moment?.
However, there were only 4 errors ofthis type in the data.Baseline Errors: The output of the baseline exhib-ited two main types of error.
The first stemmed fromthe conventionality of many metaphorical expres-sions, which resulted in their literal annotation.
Con-ventionality leads to high selectional association forverbs with their metaphorical arguments, e.g.
em-brace has {view, ideology, conception etc.}
class asits top ranked direct object argument with the selec-tional association of 0.18.
The second type of errorwas the system selecting many language anomaliesthat violate selectional preferences and tagging theseas metaphors.
This resulted in a high number of falsepositives.5 Conclusions and Future DirectionsPrevious research on metaphor addressed a num-ber of its aspects using both symbolic and statisti-cal techniques.
While some of this work met withsuccess with respect to precision in metaphor an-notation, the methods often focused on a limiteddomain and needed manually-labeled training data.Their dependence on manually annotated trainingdata made the systems hard to scale.
As a result,many of these systems are not directly applicable toaid real-world NLP due to their limited coverage.
Incontrast, our method does not require any manually-labeled data, which makes it more robust and appli-cable to a wide range of genres.
It is also the firstone to perform accurate metaphor identification andinterpretation in one step, as opposed to the previ-ous systems focusing on one part of the task only.It identifies metaphor with a precision of 68% anda recall of 66%, which is a very encouraging result.We believe that this work has important implicationsfor computational modeling of metaphor, and is rel-evant to a range of other semantic tasks within NLP.Although we have so far tested our system onverb-subject and verb-object metaphors only, we be-lieve that the described identification and paraphras-ing techniques can be similarly applied to a widerrange of syntactic constructions.
Extending the sys-tem to deal with more parts of speech and types ofphrases (e.g.
nominal and adjectival metaphors) ispart of our future work.Another promising future research avenue is inte-grating the techniques with unsupervised paraphras-ing and lexical substitution methods, using e.g.
dis-tributional similarity measures (Pucci et al 2009;McCarthy et al 2010) or vector space models ofword meaning (Erk and Pado?, 2008; Erk and Pado?,2009; De Cao and Basili, 2009; Shutova et al2012b).
These methods could fully or partly replacethe WordNet filter in the detection of similar basicfeatures of the concepts, or add useful informationto it.
Fully replacing the WordNet filter by an un-supervised method would make the system more ro-bust and more easily portable across domains andgenres.
This may also eliminate some of the systemerrors that arise from the inconsistent sense annota-tion and the inclusion of some metaphorical sensesin WordNet.283AcknowledgmentsThis work was supported by the ICSI MetaNetproject (grant number W911NF-12-C-0022).
Manythanks to Srini Narayanan, Eve Sweetser and JerryFeldman for their advice and feedback.ReferencesRodrigo Agerri, John Barnden, Mark Lee, and AlanWallington.
2007.
Metaphor, inference and domain-independent mappings.
In Proceedings of RANLP-2007, pages 17?23, Borovets, Bulgaria.Rodrigo Agerri.
2008.
Metaphor in textual entailment.In Proceedings of COLING 2008, pages 3?6, Manch-ester, UK.Oistein Andersen, Julien Nioche, Ted Briscoe, and JohnCarroll.
2008.
The BNC parsed with RASP4UIMA.In Proceedings of LREC 2008, pages 865?869, Mar-rakech, Morocco.John Barnden and Mark Lee.
2002.
An artificial intel-ligence approach to metaphor understanding.
Theoriaet Historia Scientiarum, 6(1):399?412.Julia Birke and Anoop Sarkar.
2006.
A clustering ap-proach for the nearly unsupervised recognition of non-literal language.
In In Proceedings of EACL-06, pages329?336.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the rasp system.
In Proceed-ings of the COLING/ACL on Interactive presentationsessions, pages 77?80.Lou Burnard.
2007.
Reference Guide for the British Na-tional Corpus (XML Edition).Lynne Cameron.
2003.
Metaphor in Educational Dis-course.
Continuum, London.Seana Coulson.
2008.
Metaphor comprehension and thebrain.
In R.W.
Gibbs, editor, Metaphor and Thought,Cambridge.
Cambridge University Press.Diego De Cao and Roberto Basili.
2009.
Combining dis-tributional and paradigmatic information in a lexicalsubstitution task.
In Proceedings of EVALITA work-shop, 11th Congress of Italian Association for Artifi-cial Intelligence.Katrin Erk and Sebastian Pado?.
2008.
A structuredvector space model for word meaning in context.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 897?906,Waikiki, Hawaii, USA.Katrin Erk and Sebastian Pado?.
2009.
Paraphrase as-sessment in structured vector space: exploring param-eters and datasets.
In Proceedings of the Workshop onGeometrical Models of Natural Language Semantics,pages 57?65.
Association for Computational Linguis-tics.Dan Fass.
1991. met*: A method for discriminatingmetonymy and metaphor by computer.
ComputationalLinguistics, 17(1):49?90.Jerome Feldman and Srini Narayanan.
2004.
Embodiedmeaning in a neural theory of language.
Brain andLanguage, 89(2):385?392.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database (ISBN: 0-262-06197-X).
MITPress, first edition.Matt Gedigian, John Bryant, Srini Narayanan, and Bran-imir Ciric.
2006.
Catching metaphors.
In In Proceed-ings of the 3rd Workshop on Scalable Natural Lan-guage Understanding, pages 41?48, New York.Raymond W. Gibbs and Herbert L. Colston.
2012.
In-terpreting Figurative Meaning.
Cambridge UniversityPress.Paul Kingsbury and Martha Palmer.
2002.
FromTreeBank to PropBank.
In Proceedings of LREC-2002, pages 1989?1993, Gran Canaria, Canary Is-lands, Spain.Saisuresh Krishnakumaran and Xiaojin Zhu.
2007.Hunting elusive metaphors using lexical resources.In Proceedings of the Workshop on ComputationalApproaches to Figurative Language, pages 13?20,Rochester, NY.George Lakoff and Mark Johnson.
1980.
Metaphors WeLive By.
University of Chicago Press, Chicago.James Martin.
1990.
A Computational Model ofMetaphor Interpretation.
Academic Press Profes-sional, Inc., San Diego, CA, USA.James Martin.
2006.
A corpus-based analysis of con-text effects on metaphor comprehension.
In A. Ste-fanowitsch and S. T. Gries, editors, Corpus-Based Ap-proaches to Metaphor and Metonymy, Berlin.
Moutonde Gruyter.Zachary Mason.
2004.
Cormet: a computational,corpus-based conventional metaphor extraction sys-tem.
Computational Linguistics, 30(1):23?44.Diana McCarthy and Roberto Navigli.
2007.
Semeval-2007 task 10: English lexical substitution task.
In Pro-ceedings of the 4th workshop on Semantic Evaluations(SemEval-2007), pages 48?53.Diana McCarthy, Bill Keller, and Roberto Navigli.
2010.Getting synonym candidates from raw data in the en-glish lexical substitution task.
In Proceedings of the14th EURALEX International Congress, Leeuwarden,The Netherlands.Srini Narayanan.
1997.
Knowledge-based Action Repre-sentations for Metaphor and Aspect (KARMA).
Tech-nical report, PhD thesis, University of California atBerkeley.284Srini Narayanan.
1999.
Moving right along: A compu-tational model of metaphoric reasoning about events.In Proceedings of AAAI 99), pages 121?128, Orlando,Florida.Dario Pucci, Marco Baroni, Franco Cutugno, andAlessandro Lenci.
2009.
Unsupervised lexical sub-stitution with a word space model.
In Proceedings ofEVALITA workshop, 11th Congress of Italian Associ-ation for Artificial Intelligence.Philip Resnik.
1993.
Selection and Information: AClass-based Approach to Lexical Relationships.
Ph.D.thesis, Philadelphia, PA, USA.Ekaterina Shutova and Simone Teufel.
2010.
Metaphorcorpus annotated for source - target domain map-pings.
In Proceedings of LREC 2010, pages 3255?3261, Malta.Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010.Metaphor identification using verb and noun cluster-ing.
In Proceedings of Coling 2010, pages 1002?1010,Beijing, China.Ekaterina Shutova, Simone Teufel, and Anna Korhonen.2012a.
Statistical Metaphor Processing.
Computa-tional Linguistics, 39(2).Ekaterina Shutova, Tim Van de Cruys, and Anna Korho-nen.
2012b.
Unsupervised metaphor paraphrasing us-ing a vector space model.
In Proceedings of COLING2012, Mumbai, India.Ekaterina Shutova.
2010.
Automatic metaphor inter-pretation as a paraphrasing task.
In Proceedings ofNAACL 2010, pages 1029?1037, Los Angeles, USA.Gerard J. Steen, Aletta G. Dorst, J. Berenike Herrmann,Anna A. Kaal, Tina Krennmayr, and Trijntje Pasma.2010.
A method for linguistic metaphor identifica-tion: From MIP to MIPVU.
John Benjamins, Ams-terdam/Philadelphia.Lin Sun and Anna Korhonen.
2009.
Improvingverb clustering with automatically acquired selectionalpreferences.
In Proceedings of EMNLP 2009, pages638?647, Singapore, August.Paul H. Thibodeau and Lera Boroditsky.
2011.Metaphors we think with: The role of metaphor in rea-soning.
PLoS ONE, 6(2):e16782, 02.Peter D. Turney, Yair Neuman, Dan Assaf, and YohaiCohen.
2011.
Literal and metaphorical sense iden-tification through concrete and abstract context.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?11,pages 680?690, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Akira Utsumi.
2011.
Computational exploration ofmetaphor comprehension processes using a semanticspace model.
Cognitive Science, 35(2):251?296.Yorick Wilks.
1975.
A preferential pattern-seeking se-mantics for natural language inference.
Artificial In-telligence, 6:53?74.Yorick Wilks.
1978.
Making preferences more active.Artificial Intelligence, 11(3):197?223.285
