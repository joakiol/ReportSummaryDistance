Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 455?465, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsMulti-instance Multi-label Learning for Relation ExtractionMihai Surdeanu?, Julie Tibshirani?, Ramesh Nallapati?, Christopher D.
Manning??
Stanford University, Stanford, CA 94305{mihais,jtibs,manning}@stanford.edu?
Artificial Intelligence Center, SRI Internationalnallapat@ai.sri.comAbstractDistant supervision for relation extraction(RE) ?
gathering training data by aligning adatabase of facts with text ?
is an efficient ap-proach to scale RE to thousands of differentrelations.
However, this introduces a challeng-ing learning scenario where the relation ex-pressed by a pair of entities found in a sen-tence is unknown.
For example, a sentencecontaining Balzac and France may expressBornIn or Died, an unknown relation, or no re-lation at all.
Because of this, traditional super-vised learning, which assumes that each ex-ample is explicitly mapped to a label, is notappropriate.
We propose a novel approachto multi-instance multi-label learning for RE,which jointly models all the instances of a pairof entities in text and all their labels usinga graphical model with latent variables.
Ourmodel performs competitively on two difficultdomains.1 IntroductionInformation extraction (IE), defined as the task ofextracting structured information (e.g., events, bi-nary relations, etc.)
from free text, has received re-newed interest in the ?big data?
era, when petabytesof natural-language text containing thousands of dif-ferent structure types are readily available.
How-ever, traditional supervised methods are unlikely toscale in this context, as training data is either lim-ited or nonexistent for most of these structures.
Oneof the most promising approaches to IE that ad-dresses this limitation is distant supervision, whichgenerates training data automatically by aligning aDB =(BornIn(Barack Obama,United States)EmployedBy(Barack Obama,United States))Sentence Latent LabelBarack Obama is the 44th and current Presidentof the United States.EmployedByObama was born in the United States just as hehas always said.BornInUnited States President Barack Obama meetswith Chinese Vice President Xi Jinping today.EmployedByObama ran for the United States Senate in 2004.
?Figure 1: Training sentences generated through distantsupervision for a database containing two facts.database of facts with text (Craven and Kumlien,1999; Bunescu and Mooney, 2007).In this paper we focus on distant supervision forrelation extraction (RE), a subproblem of IE that ad-dresses the extraction of labeled relations betweentwo named entities.
Figure 1 shows a simple exam-ple for a RE domain with two labels.
Distant super-vision introduces two modeling challenges, whichwe highlight in the table.
The first challenge isthat some training examples obtained through thisheuristic are not valid, e.g., the last sentence in Fig-ure 1 is not a correct example for any of the knownlabels for the tuple.
The percentage of such falsepositives can be quite high.
For example, Riedelet al2010) report up to 31% of false positives ina corpus that matches Freebase relations with NewYork Times articles.
The second challenge is thatthe same pair of entities may have multiple labelsand it is unclear which label is instantiated by anytextual mention of the given tuple.
For example, inFigure 1, the tuple (Barack Obama, United States)has two valid labels: BornIn and EmployedBy, each(latently) instantiated in different sentences.
In the455instance...instancelabelinstancelabel...objectFigure 2: Overview of multi-instance multi-label learn-ing.
To contrast, in traditional supervised learning thereis one instance and one label per object.
For relation ex-traction the object is a tuple of two named entities.
Eachmention of this tuple in text generates a different instance.Riedel corpus, 7.5% of the entity tuples in the train-ing partition have more than one label.We summarize this multi-instance multi-label(MIML) learning problem in Figure 2.
In this pa-per we propose a novel graphical model, which wecalled MIML-RE, that targets MIML learning for re-lation extraction.
Our work makes the followingcontributions:(a) To our knowledge, MIML-RE is the first RE ap-proach that jointly models both multiple instances(by modeling the latent labels assigned to instances)and multiple labels (by providing a simple method tocapture dependencies between labels).
For example,our model learns that certain labels tend to be gener-ated jointly while others cannot be jointly assignedto the same tuple.
(b) We show that MIML-RE performs competitivelyon two difficult domains.2 Related WorkDistant supervision for IE was introduced by Cravenand Kumlien (1999), who focused on the ex-traction of binary relations between proteins andcells/tissues/diseases/drugs using the Yeast ProteinDatabase as a source of distant supervision.
Sincethen, the approach grew in popularity (Bunescu andMooney, 2007; Bellare and McCallum, 2007; Wuand Weld, 2007; Mintz et al2009; Riedel et al2010; Hoffmann et al2011; Nguyen and Moschitti,2011; Sun et al2011; Surdeanu et al2011a).However, most of these approaches make one ormore approximations in learning.
For example,most proposals heuristically transform distant super-vision to traditional supervised learning (i.e., single-instance single-label) (Bellare and McCallum, 2007;Wu and Weld, 2007; Mintz et al2009; Nguyenand Moschitti, 2011; Sun et al2011; Surdeanuet al2011a).
Bunescu and Mooney (2007) andRiedel et al2010) model distant supervision forrelation extraction as a multi-instance single-labelproblem, which allows multiple mentions for thesame tuple but disallows more than one label per ob-ject.
Our work is closest to Hoffmann et al2011).They address the same problem we do (binary rela-tion extraction) with a MIML model, but they maketwo approximations.
First, they use a deterministicmodel that aggregates latent instance labels into aset of labels for the corresponding tuple by OR-ingthe classification results.
We use instead an object-level classifier that is trained jointly with the clas-sifier that assigns latent labels to instances and cancapture dependencies between labels.
Second, theyuse a Perceptron-style additive parameter update ap-proach, whereas we train in a Bayesian framework.We show in Section 5 that these approximations gen-erally have a negative impact on performance.MIML learning has been used in fields other thannatural language processing.
For example, Zhouand Zhang (2007) use MIML for scene classifica-tion.
In this problem, each image may be assignedmultiple labels corresponding to the different scenescaptured.
Furthermore, each image contains a set ofpatches, which forms the bag of instances assignedto the given object (image).
Zhou and Zhang pro-pose two algorithms that reduce the MIML problemto a more traditional supervised learning task.
Inone algorithm, for example, they convert the task toa multi-instance single-label problem by creating aseparate bag for each label.
Due to this, the pro-posed approach cannot model inter-label dependen-cies.
Moreover, the authors make a series of approx-imations, e.g., they assume that each instance in abag shares the bag?s overall label.
We instead modelall these issues explicitly in our approach.In general, our approach belongs to the categoryof models that learn in the presence of incomplete orincorrect labels.
There has been interest among ma-chine learning researchers in the general problem ofnoisy data, especially in the area of instance-basedlearning.
Brodley and Friedl (1999) summarizepast approaches and present a simple, all-purposemethod to filter out incorrect data before training.While potentially applicable to our problem, this ap-proach is completely general and cannot incorporateour domain-specific knowledge about how the noisy456data is generated.3 Distant Supervision for Relation ExtractionHere we focus on distant supervision for the ex-traction of relations between two entities.
We de-fine a relation as the construct r(e1, e2), where r isthe relation name, e.g., BornIn in Figure 1, and e1and e2 are two entity names, e.g., Barack Obamaand United States.
Note that there are entity tu-ples (e1, e2) that participate in multiple relations,r1, .
.
.
, ri.
In other words, the tuple (e1, e2) is theobject illustrated in Figure 2 and the different rela-tion names are the labels.
We define an entity men-tion as a sequence of text tokens that matches thecorresponding entity name in some text, and relationmention (for a given relation r(e1, e2)) as a pair ofentity mentions of e1 and e2 in the same sentence.Relation mentions thus correspond to the instancesin Figure 2.1 As the latter definition indicates, wefocus on the extraction of relations expressed in asingle sentence.
Furthermore, we assume that entitymentions are extracted by a different process, suchas a named entity recognizer.We define the task of relation extraction as a func-tion that takes as input a document collection (C), aset of entity mentions extracted from C (E), a set ofknown relation labels (L) and an extraction model,and outputs a set of relations (R) such that any of therelations extracted is supported by at least one sen-tence in C. To train the extraction model, we use adatabase of relations (D) that are instantiated at leastonce in C. Using distant supervision, D is alignedwith sentences in C, producing relation mentions forall relations in D.4 ModelOur model assumes that each relation mention in-volving an entity pair has exactly one label, but al-lows the pair to exhibit multiple labels across differ-ent mentions.
Since we do not know the actual re-lation label of a mention in the distantly supervisedsetting, we model it using a latent variable z thatcan take one of the k pre-specified relation labelsas well as an additional NIL label, if no relation isexpressed by the corresponding mention.
We modelthe multiple relation labels an entity pair can assume1For this reason, we use relation mention and relation in-stance interchangeably in this paper.. .
.. .
.
.
.
.. .
.Figure 3: MIML model plate diagram.
We unrolled they plate to emphasize that it is a collection of binary clas-sifiers (one per relation label), whereas the z classifier ismulti-class.
Each z and yj classifier has an additionalprior parameter, which is omitted here for clarity.using a multi-label classifier that takes as input thelatent relation types of the all the mentions involvingthat pair.
The two-layer hierarchical model is showngraphically in Figure 3, and is described more for-mally below.
The model includes one multi-classclassifier (for z) and a set of binary classifiers (foreach yj).
The z classifier assigns latent labels fromL to individual relation mentions or NIL if no rela-tion is expressed by the mention.
Each yj classifierdecides if relation j holds for the given entity tu-ple, using the mention-level classifications as input.Specifically, in the figure:?
n is the number of distinct entity tuples in D;?
Mi is the set of mentions for the ith entity pair;?
x is a sentence and z is the latent relation clas-sification for that sentence;?
wz is the weight vector for the multi-classmention-level classifier;?
k is the number of known relation labels in L;?
yj is the top-level classification decision for theentity pair as to whether the jth relation holds;?
wj is the weight vector for the binary top-levelclassifier for the jth relation.Additionally, we define Pi (Ni) as the set of allknown positive (negative) relation labels for the ithentity tuple.
In this paper, we construct Ni as L\Pi,but, in general, other scenarios are possible.
Forexample, both Sun et al2011) and Surdeanu et457al.
(2011a) proposed models where Ni for the ith tu-ple (e1, e2) is defined as: {rj | rj(e1, ek) ?
D, ek 6=e2, rj /?
Pi}, which is a subset of L\Pi.
That is, en-tity e2 is considered a negative example for relationrj (in the context of entity e1) only if rj exists in thetraining data with a different value.The addition of the object-level layer (for y) is animportant contribution of this work.
This layer cancapture information that cannot be modeled by themention-level classifier.
For example, it can learnthat two relation labels (e.g., BornIn and SpouseOf)cannot be generated jointly for the same entity tu-ple.
So, if the z classifier outputs both these la-bels for different mentions of the same tuple, the ylayer can cancel one of them.
Furthermore, the yclassifiers can learn when two labels tend to appearjointly, e.g., CapitalOf and Contained between twolocations, and use this occurrence as positive rein-forcement for these labels.
We discuss the featuresthat implement these ideas in Section 5.4.1 TrainingWe train the proposed model using hard discrimina-tive Expectation Maximization (EM).
In the Expec-tation (E) step we assign latent mention labels us-ing the current model (i.e., the mention and relationlevel classifiers).
In the Maximization (M) step weretrain the model to maximize the log likelihood ofthe data using the current latent assignments.In the equations that follow, we refer tow1, .
.
.
,wk collectively as wy for compactness.The vector zi contains the latent mention-level clas-sifications for the ith entity pair, while yi representsthe corresponding set of gold-standard labels (thatis, y(r)i = 1 if r ?
Pi, and y(r)i = 0 for r ?
Ni.
)Using these notations, the log-likelihood of the datais given by:LL(wy,wz) =n?i=1log p(yi|xi,wy,wz)=n?i=1log?zip(yi, zi|xi,wy,wz)The joint probability in the inner summation can bebroken up into simpler parts:p(yi, zi|xi,wy,wz)= p(zi|xi,wz)p(yi|zi,wy)=?m?Mip(z(m)i |x(m)i ,wz)?r?Pi?Nip(y(r)i |zi,w(r)y )where the last step follows from conditional inde-pendence.
Thus the log-likelihood for this problemis not convex (it includes a sum of products).
How-ever, we can still use EM, but the optimization fo-cuses on maximizing the lower bound of the log-likelihood, i.e., we maximize the above joint proba-bility for each entity pair in the database.
Rewritingthis probability in log space, we obtain:log p(yi, zi|xi,wy,wz) (1)=?m?Milog p(z(m)i |x(m)i ,wz)+?r?Pi?Nilog p(y(r)i |zi,w(r)y )The algorithm proceeds as follows.E-step: In this step we infer the mention-levelclassifications zi for each entity tuple, given all itsmentions, the gold labels yi, and current model, i.e.,wz and wy weights.
Formally, we seek to find:zi?
= argmaxzp(z|yi,xi,wy,wz)However it is computationally intractable to con-sider all vectors z as there is an exponential num-ber of possible assignments, so we approximate andconsider each mention separately.
Concretely,p(z(m)i |yi,xi,wy,wz)?
p(yi, z(m)i |xi,wy,wz)?
p(z(m)i |x(m)i ,wz)p(yi|z?i,wy)= p(z(m)i |x(m)i ,wz)?r?Pi?Nip(y(r)i |z?i,w(r)y )where z?i contains the previously inferred mentionlabels for group i, with the exception of compo-nent m whose label is replaced by z(m)i .
So fori = 1, .
.
.
, n, and for each m ?Mi we calculate:z(m)?i =argmaxzp(z|x(m)i ,wz)?
(2)?r?Pi?Nip(y(r)i |z?i,w(r)y )458Intuitively, the above equation indicates that men-tion labels are chosen to maximize: (a) the prob-abilities assigned by the mention-level model; (b)the probability that the correct relation labels are as-signed to the corresponding tuple; and (c) the prob-ability that the labels known to be incorrect are notassigned to the tuple.
For example, if a particularmention label receives a high mention-level proba-bility but it is known to be a negative label for thattuple, it will receive a low overall score.M-step: In this step we find wy,wz that maxi-mize the lower bound of the log-likelihood, i.e., theprobability in equation (1), given the current assign-ments for zi.
From equation (1) it is clear that thiscan be maximized separately with respect to wy andwz.
Intuitively, this step amounts to learning theweights for the mention-level classifier (wz) and theweights for each of the k top-level classifiers (wy).The updates are given by:w?z = argmaxwn?i=1?m?Milog p(z(m)?i |x(m)i ,w) (3)w(r)?y = argmaxw?1?i?n s.t.
r?Pi?Nilog p(y(r)i |z?i ,w) (4)Note that these are standard updates for logistic re-gression.
We obtained these weights using k + 1logistic classifiers: one multi-class classifier for wzand k binary classifiers for each relation label r ?
L.We implemented all using the L2-regularized logis-tic regression from the publicly-downloadable Stan-ford CoreNLP package.2 The main difference be-tween the classifiers is how features are generated:the mention-level classifier computes its featuresbased on xi, whereas the relation-level classifiersgenerate features based on the current assignmentsfor zi and the corresponding relation label r. Wediscuss the actual features used in our experimentsin Section 5.4.2 InferenceGiven an entity tuple, we obtain its relation labels asfollows.
We first classify its mentions:z(m)?i = argmaxzp(z|x(m)i ,wz) (5)2nlp.stanford.edu/software/corenlp.shtmlthen decide on the final relation labels using the top-level classifiers:y(r)?i = arg maxy?
{0,1}p(y|z?i ,w(r)y ) (6)4.3 Implementation DetailsWe discuss next several details that are crucial forthe correct implementation of the above model.Initialization: Since EM is not guaranteed to con-verge at the global maximum of the observed datalikelihood, it is important to provide it with goodstarting values.
In our context, the initial values arelabels assigned to zi, which are required to computeequation (2) in the first iteration (z?i).
We generatethese values using a local logistic regression classi-fier that uses the same features as the mention-levelclassifier in the joint model but treats each relationmention independently.
We train this classifier using?traditional?
distant supervision: for each relation inthe databaseD we assume that all the correspondingmentions are positive examples for the correspond-ing label (Mintz et al2009).
Note that this heuris-tic repeats relation mentions with different labels forthe tuples that participate in multiple relations.
Forexample, all the relation mentions in Figure 1 willyield datums with both the EmployedBy and BornInlabels.
Despite this limitation, we found that this isa better initialization heuristic than random assign-ment.For the second part of equation (2), we initial-ize the relation-level classifier with a model thatreplicates the at least one heuristic of Hoffmann etal.
(2011).
Each w(r)y model has a single feature witha high positive weight that is triggered when label ris assigned to any of the mentions in z?i .Avoiding overfitting: A na?
?ve implementation ofour approach leads to an unrealistic training scenariowhere the z classifier generates predictions (in equa-tion (2)) for the same datums it has seen in trainingin the previous iteration.
To avoid this overfittingproblem we used cross validation: we divided thetraining tuples in K distinct folds and trained K dif-ferent mention-level classifiers.
Each classifier out-puts p(z|x(m)i ,wz) for tuples in a given fold duringthe E-step (equation (2)) and is trained (equation (3))using tuples from all other folds.459At testing time, we compute p(z|x(m)i ,wz) inequation (5) as the average of the probabilities ofthe above set of mention classifiers:p(z|x(m)i ,wz) =?Kj=1 p(z|x(m)i ,wjz)Kwhere wjz are the weights of the mention classifierresponsible for fold j.
We found that this simplebagging model performs slightly better in practice(a couple of tenths of a percent) than training a sin-gle mention classifier on the latent mention labelsgenerated in the last training iteration.Inference during training: During the inferenceprocess in the E-step, the algorithm incrementally?flips?
mention labels based on equation (2), foreach group of mentions Mi.
Thus, z?i changes as thealgorithm progresses, which may impact the labelassigned to the remaining mentions in that group.
Toavoid any potential bias introduced by the arbitraryorder of mentions as seen in the data, we randomizeeach group Mi before we inspect its mentions.5 Experimental Results5.1 DataWe evaluate our algorithm on two corpora.
The firstwas developed by Riedel et al2010) by aligningFreebase3 relations with the New York Times (NYT)corpus.
They used the Stanford named entity recog-nizer (Finkel et al2005) to find entity mentions intext and constructed relation mentions only betweenentity mentions in the same sentence.Riedel et al2010) observes that evaluating onthis corpus underestimates true extraction accuracybecause Freebase is incomplete.
Thus, some re-lations extracted during testing will be incorrectlymarked as wrong, simply because Freebase has noinformation on them.
To mitigate this issue, Riedelet al2010) and Hoffman et al2011) perform asecond evaluation where they compute the accuracyof labels assigned to a set of relation mentions thatthey manually annotated.
To avoid any potential an-notation biases, we instead evaluate on a second cor-pus that has comprehensive annotations generatedby experts for all test relations.We constructed this second dataset using mainlyresources distributed for the 2010 and 2011 KBP3freebase.comshared tasks (Ji et al2010; Ji et al2011).
We gen-erated training relations from the knowledge baseprovided by the task organizers, which is a subsetof the English Wikipedia infoboxes from a 2008snapshot.
Similarly to the corpus of Riedel et althese infoboxes contain open-domain relations be-tween named entities, but with a different focus.For example, more than half of the relations inthe evaluation data are alternate names of organi-zations or persons (e.g., org:alternate names) or re-lations associated with employment and member-ship (e.g., per:employee of) (Ji et al2011).
Wealigned these relations against a document collec-tion that merges two distinct sources: (a) the col-lection provided by the shared task, which containsapproximately 1.5 million documents from a vari-ety of sources, including newswire, blogs and tele-phone conversation transcripts; and (b) a completesnapshot of the English Wikipedia from June 2010.During training, for each entity tuple (e1, e2), weretrieved up to 50 sentences that contain both en-tity mentions.4 We used Stanford?s CoreNLP pack-age to find entity mentions in text and, similarly toRiedel et al2010), we construct relation mentioncandidates only between entity mentions in the samesentence.
We analyzed a set of over 2,000 relationmentions and we found that 39% of the mentionswhere e1 is an organization name and 36% of men-tions where e1 is a person name do not express thecorresponding relation.At evaluation time, the KBP shared task requiresthe extraction of all relations r(e1, e2) given a querythat contains only the first entity e1.
To accommo-date this setup, we adjusted our sentence extractioncomponent to use just e1 as the retrieval query andwe kept up to 50 sentences that contain a mentionof the input entity for each evaluation query.
Fortuning and testing we used the 200 queries from the2010 and 2011 evaluations.
We randomly selected40 queries for development and used the remaining160 for the formal evaluation.To address the large number of negative examplesin training, Riedel et alubsampled them randomlywith a retention probability of 10%.
For the KBPcorpus, we followed the same strategy, but we used4Sentences were ranked using the similarity between theirparent document and the query that concatenates the two entitynames.
We used the default Lucene similarity measure.460# of gold # of gold % of gold entity tuples % of gold entity tuples % of mentions thatrelations relations with more than one label with multiple mentions in text do not express # of relation labelsin training in testing in training in training their relationRiedel 4,700 1,950 7.5% 46.4% up to 31% 51KBP 183,062 3,334 2.8% 65.1% up to 39% 41Table 1: Statistics about the two corpora used in this paper.
Some of the numbers for the Riedel dataset is from (Riedelet al2010; Hoffmann et al2011).a subsampling probability of 5% because this led tothe best results in development for all models.Table 1 provides additional statistics about thetwo corpora.
The table indicates that having mul-tiple mentions for an entity tuple is a very commonphenomenon in both corpora, and that having mul-tiple labels per tuple is more common in the Riedeldataset than KBP (7.5% vs. 2.8%).5.2 FeaturesOur model requires two sets of features: one for themention classifier (z) and one for the relation clas-sifier (y).
In the Riedel dataset, we used the samefeatures as Riedel et al2010) and Hoffmann etal.
(2011) for the mention classifier.
In the KBPdataset, we used a feature set that was developed inour previous work (Surdeanu et al2011b).
Thesefeatures can be grouped in three classes: (a) featuresthat model the two entities, such as their head words;(b) features that model the syntactic context of therelation mention, such as the dependency path be-tween the two entity mentions; and (c) features thatmodel the surface context, such as the sequence ofpart of speech tags between the two entity mentions.We used these features for all the models evaluatedon the KBP dataset.5For the relation-level classifier, we developed twofeature groups.
The first models Hoffmann et alat least one heuristic using a single feature, whichis set to true if at least one mention in zi has the la-bel r, which is modeled by the current relation clas-sifier.
The second group models the dependenciesbetween relation labels.
This is implemented by aset of |L| ?
1 features, where feature j is instan-tiated whenever the label modeled (r) is predictedjointly with another label rj (rj ?
L, rj 6= r) in zi.These features learn both positive and negative re-inforcements between labels.
For example, if labels5To avoid an excessive number of features in the KBP exper-iments, we removed features seen less than five times in train-ing.r1 and r2 tend to be generated jointly, the feature forthe corresponding dependency will receive a posi-tive weight in the models for r1 and r2.
Similarly, ifr1 and r2 cannot be generated jointly, the model willassign a negative weight to feature 2 in r1?s classi-fier and to feature 1 in r2?s classifier.
Note that thisfeature is asymmetric, i.e., feature 1 in r2?s classi-fier may have a different value than feature 2 in r1?sclassifier, depending on the accuracy of the individ-ual predictions for r1 and r2.5.3 BaselinesWe compare our approach against three models:Mintz++ ?
This is the model used to initialize themention-level classifier in our model.
As discussedin Section 4.3, this model follows the ?traditional?distant supervision heuristic, similarly to (Mintz etal., 2009).
However, our implementation has severaladvantages over the original model: (a) we modeleach relation mention independently, whereas Mintzet alollapsed all the mentions of the same entitytuple into a single datum; (b) we allow multi-labeloutputs for a given entity tuple at prediction timeby OR-ing the predictions for the individual rela-tion mentions corresponding to the tuple (similarlyto (Hoffmann et al2011))6; and (c) we use thesimple bagging strategy described in Section 4.3 tocombine multiple models.
Empirically, we observedthat these changes yield a significant improvementover the original proposal.
For this reason, we con-sider this model a strong baseline on its own.Riedel ?
This is the ?at-least-once?
model reportedin (Riedel et al2010), which had the best perfor-mance in that work.
This approach models the taskas a multi-instance single-label problem.
Note thatthis is the only model shown here that does not allowmulti-label outputs for an entity tuple.6We also allow multiple labels per tuple at training time,in which case we replicate the corresponding datum for eachlabel.
However, this did not improve performance significantlycompared to selecting a single label per datum during training.461Hoffmann ?
This is the ?MultiR?
model, which per-formed the best in (Hoffmann et al2011).
Thismodels RE as a MIML problem, but learns usinga Perceptron algorithm and uses a deterministic ?atleast one?
decision instead of a relation classifier.We used Hoffman?s publicly released code7 for theexperiments on the Riedel dataset and our own im-plementation for the KBP experiments.85.4 ResultsWe tuned all models using three-fold cross valida-tion for the Riedel dataset and using the develop-ment queries for the KBP dataset.
MIML-RE hastwo parameters that require tuning: the number ofEM epochs (T ) and the number of folds for the men-tion classifiers (K).9 The values obtained after tun-ing are T = 15,K = 5 for the Riedel dataset andT = 8,K = 3 for KBP.
Similarly, we tuned thenumber of epochs for the Hoffmann model on theKBP dataset, obtaining an optimal value of 20.On the Riedel dataset we evaluate all models us-ing standard precision and recall measures.
For theKBP evaluation we used the official KBP scorer,10with two changes: (a) we score with the parame-ter anydoc set to true, which configures the scorerto accept relation mentions as correct regardless oftheir supporting document; and (b) we score onlyon the subset of gold relations that have at least onemention in our sentences.
The first decision is neces-sary because the gold KBP answers contain support-ing documents only from the corpus provided by theorganizers but we retrieve candidate answers frommultiple collections.
The second is required becausethe focus of this work is not on sentence retrieval buton RE, which should be evaluated in isolation.11Similarly to previous work, we report preci-sion/recall curves in Figure 4.
We evaluate twovariants of MIML-RE: one that includes all thefeatures for the y model, and another (MIML-RE7cs.washington.edu/homes/raphaelh/mr/8The decision to reimplement the Hoffmann model was apractical one, driven by incompatibilities between their imple-mentation and our KBP framework.9We could also tune the prior parameters for both our modeland Mintz++, but we found in early experiments that the defaultvalue of 1 yields the best scores for all priors.10nlp.cs.qc.cuny.edu/kbp/2011/scoring.html11Due to these changes, the scores reported in this paper arenot directly comparable with the shared task scores.At-Least-One) which has only the at least onefeature.
For all the Bayesian models implementedhere, we sorted the predicted relations by the noisy-or score of the top predictions for their mentions.Formally, we rank a relation r predicted for group i,i.e., r ?
y?i , using:noisyOri(r) = 1??m?Mi(1?
s(m)i (r))where s(m)i (r) = p(r|x(m)i ,wz) if r = z(m)?i or 0 oth-erwise.
The noisy-or formula performs well forranking because it integrates model confidence (thehigher the probabilities, the higher the score) and re-dundancy (the more mentions are predicted with alabel, the higher that label?s score).
Note that theabove ranking score does not include the probabilityof the relation classifier (equation (6)) for MIML-RE.While we use equation (6) to generate y?i , we foundthat the corresponding probabilities are too coarseto provide a good ranking score.
This is caused bythe fact that our relation-level classifier works witha small number of (noisy) features.
Lastly, for ourimplementation of the Hoffmann et alodel, weused their ranking heuristic (sorting predictions bythe maximum extraction score for that relation).6 DiscussionFigure 4 indicates that MIML-RE generally outper-forms the current state of the art.
In the Riedeldataset, MIML-RE has higher overall recall than theRiedel et alodel, and, for the same recall point,MIML-RE?s precision is between 2 and 15 pointshigher.
For most of the curve, our model obtainsbetter precision for the same recall point than theHoffmann model, which currently has the best re-ported results on this dataset.
The difference is ashigh as 5 precision points around the middle of thecurve.
The Hoffmann model performs better close tothe extremities of the curve (low/high recall).
Nev-ertheless, we argue that our model is more stablethan Hoffmann?s: MIML-RE yields a smoother pre-cision/recall curve, without most of the depressionsseen in the Hoffmann results.
In the KBP dataset,MIML-RE performs consistently better than our im-plementation of Hoffmann?s model, with higher pre-cision values for the same recall point, and muchhigher overall recall.
We believe that these dif-ferences are caused by our Bayesian framework,4620.20.30.40.50.60.70.80.90  0.05  0.1  0.15  0.2  0.25  0.3PrecisionRecallHoffmannRiedelMintz++MIML-REMIML-RE At-Least-One0.20.30.40.50.60.70  0.05  0.1  0.15  0.2  0.25  0.3PrecisionRecallHoffmann (our implementation)Mintz++MIML-REMIML-RE At-Least-OneFigure 4: Results in the Riedel dataset (top) and the KBP dataset (bottom).
The Hoffmann scores in the KBP datasetwere generated using our implementation.
The other Hoffmann and Riedel results were taken from their papers.which provides a more formal implementation of theMIML problem.Figure 4 also indicates that MIML-RE yields a con-sistent improvement over Mintz++ (with the excep-tion of a few points in the low-recall portion of theKBP curves).
The difference in precision for thesame recall point is as high as 25 precision points inthe Riedel dataset and up to 5 points in KBP.
Over-all, the best F1 score of MIML-RE is slightly over 1point higher than the best F1 score of Mintz++ inthe Riedel dataset and 3 points higher in KBP.
Con-sidering that Mintz++ is a strong baseline and weevaluate on two challenging domains, we considerthese results proof that the correct modeling of theMIML scenario is beneficial.Lastly, Figure 4 shows that MIML-RE outper-forms its variant without label-dependency fea-tures (MIML-RE At-Least-One) in the higher-recall part of the curve in the Riedel dataset.
The im-provement is approximately 1 F1 point throughoutthe last segment of the curve.
The overall increasein F1 was found to be significant (p = 0.0296) in aone-sided, paired t-test over randomly sampled testdata.
We see a smaller improvement in KBP (con-centrated around the middle of the curve), likely be-cause the number of entity tuples with multiple la-bels in training is small (see Table 1).
Neverthe-less, this exercise shows that, when dependenciesbetween labels exist in a dataset, modeling them,which can be trivially done in MIML-RE, is useful.463P R F1Hoffmann (our implementation) 48.6 29.8 37.0Mintz++ 43.8 36.8 40.0MIML-RE 64.8 31.6 42.6MIML-RE At-Least-One 56.1 32.5 41.1Table 2: Results at the highest F1 point in the preci-sion/recall curve on the dataset that contains groups withat least 10 mentions.In a similar vein, we tested the models previ-ously described on a subset of the Riedel evalua-tion dataset that only includes groups with at least10 mentions.
This corpus contains approximately2% of the groups from the original testing partition,out of which 90 tuples have at least one known labeland 1410 groups serve as negative examples.For conciseness, we do not include the entireprecision/recall curves for this experiment, but sum-marize them in Table 2, which lists the performancepeak (highest F1 score) for each of the modelsinvestigated.
The table shows that MIML-RE obtainsthe highest F1 score overall, 1.5 points higher thanMIML-RE At-Least-One and 2.6 points higherthan Mintz++.
More importantly, for approximatelythe same recall point, MIML-RE obtains a precisionthat is over 8 percentage points higher than that ofMIML-RE At-Least-One.
A post-hoc inspectionof the results indicates that, indeed, MIML-RE suc-cessfully eliminates undesired labels when two(or more) incompatible labels are jointly assignedto the same tuple.
Take for example the tuple(Mexico City, Mexico), for which the correct re-lation is /location/administrative division/country.MIML-RE At-Least-One incorrectly predictsthe additional /location/location/contains relation,while MIML-RE does not make this predictionbecause it recognizes that these two labels are in-compatible in general: one location cannot both bewithin another location and contain it.
Indeed, ex-amining the weights assigned to label-dependencyfeatures in MIML-RE, we see that the model hasassigned a large negative weight to the depen-dency feature between /location/location/containsand /location/administrative division/countryfor the /location/location/contains class.
Wealso observe positive dependencies between la-bels.
For example, MIML-RE learns that therelations /people/person/place lived and /peo-ple/person/place of birth tend to co-occur andassigns a positive weight to this dependency featurefor the corresponding classes.These results strongly suggest that when all as-pects of the MIML scenario are present, our modelcan successfully capture them and make use of theadditional structure to improve performance.7 ConclusionIn this paper we showed that distant supervisionfor RE, which generates training data by aligning adatabase of facts with text, poses a distinct multi-instance multi-label learning scenario.
In this set-ting, each entity pair to be modeled typically hasmultiple instances in the text and may have multiplelabels in the database.
This is considerably differ-ent from traditional supervised learning, where eachinstance has a single, explicit label.We argued that this MIML scenario should beformally addressed.
We proposed, to our knowl-edge, the first approach that models all aspects of theMIML setting, i.e., the latent assignment of labels toinstances and dependencies between labels assignedto the same entity pair.We evaluated our model on two challenging do-mains and obtained state-of-the-art results on both.Our model performs well even when not all aspectsof the MIML scenario are common, and as seen inthe discussion, shows significant improvement whenevaluated on entity pairs with many labels or men-tions.
When all aspects of the MIML scenario arepresent, our model is well-equipped to handle them.The code and data used in the experiments re-ported in this paper are available at: http://nlp.stanford.edu/software/mimlre.shtml.AcknowledgmentsWe gratefully acknowledge the support of Defense Ad-vanced Research Projects Agency (DARPA) MachineReading Program under Air Force Research Laboratory(AFRL) prime contract no.
FA8750-09-C-0181.
Anyopinions, findings, and conclusion or recommendationsexpressed in this material are those of the author(s) anddo not necessarily reflect the view of the DARPA, AFRL,or the US government.
We gratefully thank RaphaelHoffmann and Sebastian Riedel for sharing their codeand data and for the many useful discussions.464ReferencesKedar Bellare and Andrew McCallum.
2007.
Learn-ing extractors from unlabeled text using relevantdatabases.
In Proceedings of the Sixth InternationalWorkshop on Information Extraction on the Web.Carla Brodley and Mark Friedl.
1999.
Identifying mis-labeled training data.
Journal of Artificial IntelligenceResearch (JAIR).Razvan Bunescu and Raymond Mooney.
2007.
Learningto extract relations from the web using minimal super-vision.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics.Mark Craven and Johan Kumlien.
1999.
Constructingbiological knowledge bases by extracting informationfrom text sources.
In Proceedings of the Seventh Inter-national Conference on Intelligent Systems for Molec-ular Biology.Jenny Rose Finkel, Trond Grenager, and Christopher D.Manning.
2005.
Incorporating non-local informationinto information extraction systems by gibbs sampling.In Proceedings of the 43nd Annual Meeting of the As-sociation for Computational Linguistics.Raphael Hoffmann, Congle Zhang, Xiao Ling, LukeZettlemoyer, and Daniel S. Weld.
2011.
Knowledge-based weak supervision for information extraction ofoverlapping relations.
In Proceedings of the AnnualMeeting of the Association for Computational Linguis-tics (ACL).Heng Ji, Ralph Grishman, Hoa T. Dang, Kira Griffitt, andJoe Ellis.
2010.
Overview of the TAC 2010 knowl-edge base population track.
In Proceedings of the TextAnalytics Conference.Heng Ji, Ralph Grishman, and Hoa T. Dang.
2011.Overview of the TAC 2011 knowledge base popula-tion track.
In Proceedings of the Text Analytics Con-ference.Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-sky.
2009.
Distant supervision for relation extrac-tion without labeled data.
In Proceedings of the 47thAnnual Meeting of the Association for ComputationalLinguistics.Truc Vien T. Nguyen and Alessandro Moschitti.
2011.End-to-end relation extraction using distant supervi-sion from external semantic repositories.
In Proceed-ings of the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies.Sebastian Riedel, Limin Yao, and Andrew McCallum.2010.
Modeling relations and their mentions withoutlabeled text.
In Proceedings of the European Confer-ence on Machine Learning and Knowledge Discoveryin Databases (ECML PKDD ?10).Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.2011.
New York University 2011 system for KBP slotfilling.
In Proceedings of the Text Analytics Confer-ence.Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-Closky, Angel X. Chang, Valentin I. Spitkovsky, andChristopher D. Manning.
2011a.
Stanford?s distantly-supervised slot-filling system.
In Proceedings of theText Analytics Conference.Mihai Surdeanu, David McClosky, Mason R. Smith, An-drey Gusev, and Christopher D. Manning.
2011b.Customizing an information extraction system to anew domain.
In Proceedings of the Workshop on Re-lational Models of Semantics, Portland, Oregon, June.Fei Wu and Dan Weld.
2007.
Autonomously semanti-fying Wikipedia.
In Proceedings of the InternationalConference on Information and Knowledge Manage-ment (CIKM).Z.H.
Zhou and M.L.
Zhang.
2007.
Multi-instance multi-label learning with application to scene classification.In Advances in Neural Information Processing Sys-tems (NIPS).465
