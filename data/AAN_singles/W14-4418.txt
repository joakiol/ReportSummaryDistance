Proceedings of the 8th International Natural Language Generation Conference, pages 118?122,Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational LinguisticsFlowGraph2Text: Automatic Sentence Skeleton Compilationfor Procedural Text Generation?1Shinsuke Mori ?2Hirokuni Maeta 1Tetsuro Sasada 2Koichiro Yoshino3Atsushi Hashimoto 1Takuya Funatomi 2Yoko Yamakata1Academic Center for Computing and Media Studies, Kyoto University2Graduate School of Informatics, Kyoto University3Graduate School of Law, Kyoto UniversityYoshida Honmachi, Sakyo-ku, Kyoto, Japan?forest@i.kyoto-u.ac.jpAbstractIn this paper we describe a method forgenerating a procedural text given itsflow graph representation.
Our mainidea is to automatically collect sen-tence skeletons from real texts by re-placing the important word sequenceswith their type labels to form a skeletonpool.
The experimental results showedthat our method is feasible and has apotential to generate natural sentences.1 IntroductionAlong with computers penetrating in our dailylife, the needs for the natural language gener-ation (NLG) technology are increasing moreand more.
If computers understand both themeaning of a procedural text and the progres-sion status, they can suggest us what to donext.
In such situation they can show sen-tences describing the next instruction on a dis-play or speak it.On this background we propose a methodfor generating instruction texts from a flowgraph representation for a series of procedures.Among various genres of procedural texts, wechoose cooking recipes, because they are one ofthe most familiar procedural texts for the pub-lic.
In addition, a computerized help systemproposed by Hashimoto et al.
(2008) calledsmart kitchen is becoming more and more re-alistic.
Thus we try to generate cooking pro-cedural texts from a formal representation fora series of preparation instructions of a dish.As the formal representation, we adopt theflow graph representation (Hamada et al.,2000; Mori et al., 2014), in which the verticesand the arcs correspond to important objects?His current affiliation is Cybozu Inc., Koraku 1-4-14, Bunkyo, Tokyo, Japan.or actions in cooking and relationships amongthem, respectively.
We use the flow graphs asthe input and the text parts as the referencesfor evaluation.Our generation method first automaticallycompiles a set of templates, which we call theskeleton pool, from a huge number of real pro-cedural sentences.
Then it decomposes the in-put flow graph into a sequence of subtrees thatare suitable for a sentence.
Finally it convertssubtrees into natural language sentences.2 Recipe Flow Graph CorpusThe input of our LNG system is the mean-ing representation (Mori et al., 2014) for cook-ing instructions in a recipe.
A recipe con-sists of three parts: a title, an ingredient list,and sentences describing cooking instructions(see Figure 1).
The meaning of the instruc-tion sentences is represented by a directedacyclic graph (DAG) with a root (the finaldish) as shown in Figure 2.
Its vertices havea pair of an important word sequence in therecipe and its type called a recipe named en-tity (NE)1.
And its arcs denote relationshipsbetween them.
The arcs are also classified intosome types.
In this paper, however, we donot use arc types for text generation, becausewe want our system to be capable of generat-ing sentences from flow graphs output by anautomatic video recognition system2 or thosedrawn by internet users.Each vertex of a flow graph has an NE com-posed of a word sequence in the text and itstype such as food, tool, action, etc.
Table 31Although the label set contains verb phrases, theyare called named entities.2By computer vision techniques such as (Regneri etal., 2013) we may be able to figure out what actiona person takes on what objects.
But it is difficult todistinguish the direct object and the indirect object,for example.1181.
??????????
(In a Dutch oven, heat oil.)?????????????????
(Add celery, green onions, and garlic.)????????
(Cook for about 1 minute.)2.
????????????????????????????????
(Add broth, water, macaroni, and pepper,and simmer until the pasta is tender.)3.
???????????
(Sprinkle the snipped sage.
)Figure 1: A recipe example.
The sentences areone of the ideal outputs of our problem.
Theyare also used as the reference in evaluation.lists all of the type labels along with the aver-age numbers of occurrences in a recipe textand examples.
The word sequences of ver-bal NEs do not include their inflectional end-ings.
From the definition we can say that thecontent words are included in the flow graphrepresentation.
Thus an NLG system has todecide their order and generate the functionwords (including inflectional endings for verbs)to connect them to form a sentence.3 Recipe Text GenerationThe problem in this paper is generating a pro-cedural text for cooking (ex.
Figure 1) from arecipe flow graph (ex.
Figure 2).Our method is decomposed into two mod-ules.
In this section, we explain them in detail.3.1 Skeleton Pool CompilationBefore the run time, we first prepare a skele-ton pool.
A skeleton pool is a collection ofskeleton sentences, or skeletons for short, anda skeleton is a sentence in which NEs havebeen replaced with NE tags.
The skeletonsare similar to the so-called templates and themain difference is that the skeletons are auto-matically converted from real sentences.
Thefollowing is the process to prepare a skeletonpool.1.
Crawl cooking procedural sentences fromrecipe sites.2.
Segment sentences into words by a wordsegmenter KyTea (Neubig et al., 2011).Then recognize recipe NEs by an NE rec-ognizer PWNER (Mori et al., 2012).3.
Replace the NE instances in the sentenceswith NE tags.Figure 2: The flow graph of the examplerecipe.Table 3: Named entity tags with average fre-quence per recipe.NE tag Meaning Freq.F Food 11.87T Tool 3.83D Duration 0.67Q Quantity 0.79Ac Action by the chef 13.83Af Action by foods 2.04Sf State of foods 3.02St State of tools 0.30We store skeletons with a key which is the se-quence of the NE tags in the order of theiroccurrence.3.2 Sentence PlanningOur sentence planner produces a sequence ofsubtrees each of which corresponds to a sen-tence.
There are two conditions.Cond.
1 Each subtree has an Ac as its root.Cond.
2 Every vertex is included in at leastone subtree.As a strategy for enumerating subtrees given aflow graph, we choose the following algorithm.1.
search for an Ac vertex by the depth firstsearch (DFS),2. each time it finds an Ac, return the largestsubtree which has an Ac as its root andcontains only unvisited vertices.3.
set the visited-mark to the vertices con-tained in the returned subtree,4.
go back to 1 unless all the vertices aremarked as visited.In DFS, we choose a child vertex randomlybecause a recipe flow graph is unordered.119Table 1: Corpus specifications.Usage #Recipes #Sent.
#NEs #Words #Char.Test 40 245 1,352 4,005 7,509NER training 360 2,813 12,101 51,847 97,911Skeleton pool 100,000 713,524 ?3,919,964 ?11,988,344 22,826,496The numbers with asterisc are estimated values on the NLP result.Table 2: Statistical results of various skeleton pool sizes.No.
of sentences used for 2,769 11,077 44,308 177,235 708,940skeleton pool compilation (1/256) (1/64) (1/16) (1/4) (1/1)No.
of uncovered subtrees 52 27 17 9 4Average no.
of skeletons 37.4 124.3 450.2 1598.1 5483.3BLEU 11.19 11.25 12.86 13.12 13.763.3 Sentence GenerationGiven a subtree sequence, our text realizergenerates a sentence by the following steps.1.
Collect skeletons from the pool whose NEkey matches the NE tag sequence speci-fied by the subtree3.2.
Select the skeleton that maximize a scor-ing function among collected ones.
As thefirst trial we use the frequency of skeletonsin the pool as the scoring function.3.
Replace each NE in the skeleton with theword sequence of the corresponding NE inthe subtree.4 EvaluationWe conducted experiments generating textsfrom flow graphs.
In this section, we reportthe coverage and the sentence quality.4.1 Experimental SettingsThe recipe flow graph corpus (Mori et al.,2014) contains 200 recipes.
We randomly se-lected 40 flow graphs as the test data fromwhich we generate texts.
The other 160 recipeswere used to train the NE recognizer PWNER(Mori et al., 2012) with 200 more recipes thatwe annotated with NE tags.
To compile theskeleton pool we crawled 100,000 recipes con-taining 713,524 sentences (see Table 1).4.2 Skeleton Pool CoverageFirst we counted the numbers of the skeletonsthat matches with a subtree (Step 1 in Subsec-tion 3.3) for all the subtrees in the test set by3This part is language dependent.
Since Japanese isSOV language, the instance of Ac is placed at the lastof the sentence to be generated.
Languages of othertypes like English may need some rules to change theNE tag order specified by the subtree into the propersentence element order.changing the number of the recipe sentencesused for the skeleton pool compilation.Table 2 shows the numbers of subtrees thatdo not have any matching skeleton in the pool(uncovered subtrees) and the average numberof skeletons in the pool for a subtree.
Fromthe results shown in the table we can say thatwhen we use 100,000 recipes for the skeletoncompilation, our method can generate a sen-tence for 98.4% subtrees.
And the table saysthat we can halve the number of uncoveredsubtrees by using about four times more sen-tences.
The average number of the skeletonssays that we have enough skeletons in averageto try more sophisticated scoring functions.4.3 Text QualityTo measure the quality of generated texts, wefirst calculated the BLEU (N = 4) (Papineniet al., 2002) with taking the original recipetexts as the references.
The unit in our caseis a sequence of sentences for a dish.
Table 2shows the average BLEU for all the test set.The result says that the more sentences we usefor the skeleton pool compilation, the betterthe generated sentences become.The absolute BLEU score, however, doesnot tell much about the quality of generatedtexts.
As it is well known, we can sometimeschange the instruction order in dish prepa-ration.
Therefore we conducted a subjectiveevaluation in addition.
We asked four evalu-ators to read 10 texts generated from 10 flowgraphs and answer the following questions.Q1.
How many ungrammatical two-word se-quences does the text contain?Q2.
How many ambiguous wordings do youfind in the text?Then we show the evaluators the originalrecipe text and asked the following question.120Table 4: Result of text quality survey on 10 recipe texts.Evaluator 1 Evaluator 2 Evaluator 3 Evaluator 4BLEU Q1 Q2 Q3 Q1 Q2 Q3 Q1 Q2 Q3 Q1 Q2 Q36.50 13 2 4 11 0 3 12 0 2 7 1 27.99 7 2 2 5 2 2 7 1 1 4 2 210.09 18 2 4 15 2 1 17 4 1 11 4 211.60 24 1 4 13 2 4 18 2 4 13 1 213.35 6 1 4 6 0 4 7 1 5 4 1 214.70 16 1 4 12 2 4 12 0 3 6 2 216.76 9 2 3 6 1 3 7 1 3 5 3 219.65 8 2 5 6 1 1 4 1 4 4 2 422.85 18 1 4 15 2 5 12 2 2 7 3 231.35 5 1 5 5 0 4 5 1 3 5 1 4Ave.
12.4 1.5 3.9 9.4 1.2 3.1 10.1 1.3 2.8 6.6 2.0 2.4PCC ?0.30 ?0.46 +0.57 ?0.24 ?0.24 +0.36 ?0.46 ?0.04 +0.26 ?0.29 ?0.04 +0.70PPC stands for Pearson correlation coefficient.Q3.
Will the dish be the same as the origi-nal recipe when you cook according to thegenerated text?
Choose the one among 5:completely, 4: almost, 3: partly, 2: differ-ent, or 1: unexecutable.Table 4 shows the result.
The generated textscontain 14.5 sentences in average.
The an-swers to Q1 tell that there are many grammat-ical errors.
We need some mechanism that se-lects more appropriate skeletons.
The numberof ambiguous wordings, however, is very low.The reason is that the important words aregiven along with the subtrees.
The average ofthe answer to Q3 is 3.05.
This result says thatthe dish will be partly the same as the originalrecipe.
There is a room for improvement.Finally, let us take a look at the correlationof the result of three Qs with BLEU.
The num-bers of grammatical errors, i.e.
the answersto Q1, has a stronger correlation with BLEUthan those of Q2 asking the semantic quality.These are consistent with the intuition.
Theanswer to Q3, asking overall text quality, hasthe strongest correlation with BLEU on aver-age among all the questions.
Therefore we cansay that for the time being the objective eval-uation by BLEU is sufficient to measure theperformance of various improvements.5 Related WorkOur method can be seen a member oftemplate-based text generation systems (Re-iter, 1995).
Contrary to the ordinarytemplate-based approach, our method first au-tomatically compiles a set of templates, whichwe call skeleton pool, by running an NE taggeron the real texts.
This allows us to cope withthe coverage problem with keeping the advan-tage of the template-based approach, abilityto prevent from generating incomprehensiblesentence structures.
The main contribution ofthis paper is to use an accurate NE tagger toconvert sentences into skeletons, to show thecoverages of the skeleton pool, and to evaluatethe method in a realistic situation.Among many applications of our method, aconcrete one is the smart kitchen (Hashimotoet al., 2008), a computerized cooking help sys-tem which watches over the chef by the com-puter vision (CV) technologies etc.
and sug-gests the chef the next action to be taken ora good way of doing it in a casual manner.
Inthis application, the text generation modulemake a sentence from a subtree specified bythe process supervision module.There are some other interesting applica-tions: a help system for internet users to writegood sentences, machine translation of a recipein a different language represented as a flowgraph, or automatic recipe generation froma cooking video based on CV and NLP re-searches such as (Regneri et al., 2013; Ya-makata et al., 2013; Yu and Siskind, 2013).6 ConclusionIn this paper, we explained and evaluated ourmethod for generating a procedural text froma flow graph representation.
The experimentalresults showed that our method is feasible es-pecially when we have huge number of real sen-tences and that some more sophistications arepossible to generate more natural sentences.121AcknowledgmentsThis work was supported by JSPS Grants-in-Aid for Scientific Research Grant Numbers26280084, 24240030, and 26280039.ReferencesReiko Hamada, Ichiro Ide, Shuichi Sakai, and Hide-hiko Tanaka.
2000.
Structural analysis of cook-ing preparation steps in japanese.
In Proceedingsof the fifth international workshop on Informa-tion retrieval with Asian languages, number 8 inIRAL ?00, pages 157?164.Atsushi Hashimoto, Naoyuki Mori, Takuya Fu-natomi, Yoko Yamakata, Koh Kakusho, andMichihiko Minoh.
2008.
Smart kitchen: A usercentric cooking support system.
In Proceedingsof the 12th Information Processing and Manage-ment of Uncertainty in Knowledge-Based Sys-tems, pages 848?854.Shinsuke Mori, Tetsuro Sasada, Yoko Yamakata,and Koichiro Yoshino.
2012.
A machine learn-ing approach to recipe text processing.
In Pro-ceedings of Cooking with Computer workshop.Shinsuke Mori, Hirokuni Maeta, Yoko Yamakata,and Tetsuro Sasada.
2014.
Flow graph cor-pus from recipe texts.
In Proceedings of theNineth International Conference on LanguageResources and Evaluation.Graham Neubig, Yosuke Nakata, and ShinsukeMori.
2011.
Pointwise prediction for robust,adaptable japanese morphological analysis.
InProceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
Bleu: a method for auto-matic evaluation of machine translation.
In Pro-ceedings of the 40th Annual Meeting of the As-sociation for Computational Linguistics, pages311?318.Michaela Regneri, Marcus Rohrbach, DominikusWetzel, Stefan Thater, Bernt Schiele, and Man-fred Pinkal.
2013.
Grounding action descrip-tions in videos.
Transactions of the Associationfor Computational Linguistics, 1(Mar):25?36.Ehud Reiter.
1995.
Nlg vs. templates.
In Pro-ceedings of the the Fifth European Workshop onNatural Language Generation, pages 147?151.Yoko Yamakata, Shinji Imahori, Yuichi Sugiyama,Shinsuke Mori, and Katsumi Tanaka.
2013.Feature extraction and summarization of recipesusing flow graph.
In Proceedings of the 5th In-ternational Conference on Social Informatics,LNCS 8238, pages 241?254.Haonan Yu and Jeffrey Mark Siskind.
2013.Grounded language learning from video de-scribed with sentences.
In Proceedings of the51st Annual Meeting of the Association forComputational Linguistics.122
