Ext ract ing  the Names  of Genes  and Gene Products  w i th  aH idden Markov  Mode lNige l  Co l l i e r ,  Ch ikash i  Nobata  and J un - i ch i  Tsu j i il )el)artm(mt of Information Science(h'aduate School of ScienceUniversity of Tokyo, Hongo-7-3-1Bunkyo-ku,  Tokyo 113, .JapanE-maih {n ige l ,  nova, t su j  ??}@?s.
s. u - tokyo ,  ac.
jpAbst ract\~e report the results of a study into the useof a linear interpolating hidden Marker model(HMM) for the task of extra.
('ting lxw\]mi(:al |;er-minology fl:om MEDLINE al)stra('ts and texl;sin the molecular-bioh)gy domain.
Tiffs is thefirst stage isl a. system that will exl;ra('l; evenl;information for automatically ut)da.ting 1)ioh)gydatabases.
We trained the HMM entirely with1)igrams based (m lexical and character fea-tures in a relatively small corpus of 100 MED-LINE abstract;s that were ma.rked-ul) l)y (lo-main experts wil;h term (:lasses u(:h as t)rol;einsand DNA.
I.Jsing cross-validation methods wea(:\]fieved a,n \].e-score of 0.73 and we (',xmnine the('ontrilmtion made by each 1)art of the interl)o-lation model to overconfing (la.ta Sl)arsen('.ss.1 In t roduct ionIll the last few ye~trs there has t)een a great in-vestment in molecula.r-l)iology resear(:h. Thishas yielded many results l;\]la.1;, 1;ogel;her wil;ha migration of m:c\]fival mal;erial to the inter-net, has resulted in an exl)losion in l;tm nuns-\])el7 of research tmbli('ations aa~ailat)le in onlinedatabases.
The results in these 1)al)ers how-ever arc not available ill a structured fornmt andhave to 1)e extracted and synthesized mammlly.Updating databases such as SwissProt (Bairochmid Apweiler, 1.997) this way is time (:onsmningand nmans l;h~tt he resull;s are not accessible soconveniently to he11) researchers in their work.Our research is aimed at autonmti(:ally ex-tra(:ting facts Kern scientific abstracts and flfllpapers ill the molecular-biology domain and us-ing these to update databases.
As the tirst stagein achieving this goal we have exl)lored th(; useof a generalisable, supervised training methodbased on hidden Markov models (ItMMs) (Ra-biner and .\]uang, 1986) fbr tim identification midclassitieation of technical expressions ill thesetexts.
This task can 1)e considered to be similarto the named c.ntity task in the MUC evaluationexercises (MUC, 1995).In our current work we are using abstractsavailable fl:om PubMed's MEDLINE (MED-\],INE, 1999).
The MEDLINE (lnta.l)ase is anonline collection of al)straets for pul)lished jour-nal articles in biology mid medicine and con-tains more than nine million articles.With the rapid growth in the mlmbcr of tmb-\]ished l)al)ers in the field of moh;('ular-biolog 3,there has been growing interest in the at)pli-cation of informa.tion extra(:tion, (Sekimizu etal., 1998) (Collier et al, 1999)(Thomas et al,1999) (Craven and Kmnlien, 1999), to help solvesouse (sf the t)robhmss that are associated withinformation overload.In the remainder of this i)aper we will firstof all (ratline the t)ackground to the task andthen d(~s('ril)e t;hc basics of ItMMs and the fi)r-real model wc are using.
The following sectionsgive an outline of a. lse\v tagged ('orlms (Ohta etal., 1999) thnt our team has deveh)i)ed using al)-stra('ts taken from a sub-domain of MEDLINF,and the results of our experinmnts on this cor-lmS.2 BackgroundIleeent studies into the use of SUl)ervisedlearning-t)ased models for the n~mled entity taskin the miero-lsioh)gy domain have.
shown thatlnodels based on HMMs and decision trees suchas (Nol)al;~t et al, 1999) ~,r(; much more gener-alisable and adaptable to slew classes of wordsthan systems based on traditional hand-lmilt1)attexns a.nd domain specific heuristic rulessuch as (Fukuda et al, 1998), overcoming the1)rol)lems associated with data sparseness withthe help of sophisticated smoothing algorithms201(Chen and Goodman, 1996).HMMs can be considered to be stochastic fi-nite state machines and have enjoyed successin a number of felds including speech recogni-tion and part-of-speech tagging (Kupiec, 1992).It has been natural therefore that these mod-els have been adapted tbr use in other word-class prediction tasks such as the atoned-entitytask in IE.
Such models are often based on n-grams.
Although the assumption that a word'spart-of speech or name class can be predictedby the previous n-1 words and their classes iscounter-intuitive to our understanding of lin-guistic structures and long distance dependen-cies, this simple method does seem to be highlyeffective ill I)ractice.
Nymble (Bikel et al,1997), a system which uses HMMs is one of themost successflfl such systems and trains on acorpus of marked-up text, using only characterfeatures in addition to word bigrams.Although it is still early days for the use ofHMMs for IE, we can see a number of trendsin the research.
Systems can be divided intothose which use one state per class such asNymble (at the top level of their backoff model)and those which automatically earn about themodel's tructure such as (Seymore t al., 1999).Additionally, there is a distinction to be madein the source of the knowledge for estimatingtransition t)robabilities between models whichare built by hand such as (Freitag and McCal-lure, 1999) and those which learn fl'om taggedcorpora in the same domain such as the modelpresented in this paper, word lists and corporain different domains - so-called distantly-labeleddata (Seymore t al., 1999).2.1 Challenges of name finding inmolecu lar -b io logy  textsThe names that we are trying to extract fall intoa number of categories that are often wider thanthe definitions used for the traditional named-entity task used in MUC and may be consideredto share many characteristics of term recogni-tion.The particular difficulties with identit)dngand elassit~qng terms in the molecular-biologydomain are all open vocabulary and irrgeularnaming conventions as well as extensive cross-over in vocabulary between classes.
The irreg-ular naming arises in part because of the num-ber of researchers from difli;rent fields who areTI - Activation of <PROTEIN> JAK kinases</PROTEIN> and <PROTEIN>STAT pTvteins</PR, OTEIN> by <PROTEIN> interlcukin - 2</PROTEIN> and <PROTEIN> intc~fc~vn alph, a</PROTEIN> , but not the <PROTEIN> T cellantigen receptor <~PROTEIN> , in <SOURCE.ct>h, uman T lymphoeytes </SOURCE.et> .AB The activation of <PROTEIN> Janusprotein t,.flvsine kinascs </PROTEIN> (<PROTEIN> JAI(s </PROTEIN> ) and<PROTEIN> signal transducer and ac-tivator of transcription </PROTEIN> (<PROTEIN> STAT </PROTEIN> ) pro-reins by <PROTEIN> intcrIcukin ( IL ) 2</PROTEIN> , thc  <PROTEIN> T cell antigenreceptor </PROTEIN> ( <PROTEIN> TCR</PROTEIN> ) and <PROTEIN> intc~fcrvn( IFN)  alpha </PROTEIN> was czplorcd in<SOURCE.ct> human periph, cral blood- derivedT cclls </SOURCE.et> and the <SOURCE.el>leukemic T cell line Kit225 </SOURCE.el> .Figure 1: Example MEDLINE sentence markedup in XML for lfiochemical named-entities.working on the same knowledge discovery areaas well as the large number of substances thatneed to be named.
Despite the best, etforts ofmajor journals to standardise the terminology,there is also a significant problem with syn-onymy so that often an entity has more tlm.none name that is widely used.
The class cross-over of terms arises because nla l ly  prot(:ins arenamed after DNA or RNA with which they re-act.All of the names which we mark up must be-long to only one of the name classes listed inTable 1.
We determined that all of these nameclasses were of interest o domain experts andwere essential to our domain model for eventextraction.
Example sentences from a nmrkedut) abstract are given in Figure 1.We decided not to use separate states ibrpre- and post-class words as had been used insome other systems, e.g.
(Freitag and McCal-lure, 1999).
Contrary to our expectations, weobserved that our training data provided verypoor maximum-likelihood probabilities for thesewords as class predictors.We found that protein predictor words hadthe only significant evidence and even this wasquite weak, except in tlm case of post-classwords which included a mmfi)er of head nounssuch as "molecules" or "heterodimers".
In our202Class ~/: Examl)le l)escriptionP1K)TEIN 21.25 .MK ki'n,a.se\])NA 358 IL-2 \]rlvmotcr\]{NA 30 771I?,S()UI{CF,.cl 93 le'ukemic T cell line Kit225S()UI\],CE.
(:t 417 h,'wm, an T lymphocytesSOURCE.too 21 ,%hizosacch, aromyces pombcS()URCE.mu 64 miceSOURCE.vi 90 ItJV-1S()UI{CE.sl 77 membraneS()UI{CE.ti 37 central 'ner,vo'us systemUNK t,y~vsine ph, osphovylal, iont)ro{xfiils~ protein groups,families~ cOral)loxes and Slll)Sl;I'llCI;lll'eS.I)NAs I)NA groups, regions and genesRNAs I~NA groups, regions and genescell line(:ell typelllOll()-organismmultiorganismvirusessublocat;iontissuelmckground wordsTable l: Named (mtilsy (:lasses.
~/: indi(:at(ts tsfic ~mmt)cr of XMI, tagged terms in our (:orpus of 100abstracts.early experiments using I IMMs that in(:orpo-rated pro- and 1)ost-c\]ass tates we \[imnd tha.tpcrforlnance was signiticantly worse than wil;h-Ollt; sll(;h si;at;cs an(l st) w('.
formulated the ~uodclas g,~ivcll i l S(;(;\[;iOll :/.~,.f(Qi,..~,l < _,Ffi,..~.,, >) +(1)and for all other words and their name classesas tbllows:3 Mx.
'tzho dThe lmrl)osc of our mod(;1 is Io lind t;hc n,osl:likely so(tilth, liCe of name classes (C) lbr a givense(tucncc of wor(ls (W).
The set of name ('lassesinchutcs the 'Unk' name (:lass whi('h we use li)r1)ackgromM words not 1)elonging to ally ()\[ theinteresting name classes given in Tal)lc 1 andt;hc given st;qu(m(:e of words which w(~ ,>('.
spansa single s(,Jd;cn('c.
The task is thcrcfor(~ 1(} max-intize Pr((TIH:).
\?c iml)lem(mt a I \ ]MM to es-t imate this using th('.
Markov assuml)tion thatP r (C I I?  )
can be t'(mnd from t)igrams of ha.meclasses.In th('.
following model we (:onsid(u" words to1)c ordered pairs consisting of a. surface word,W, and a. word tbature, 1", given as < W, F >.The word features thcms('Jvcs arc discussed inSection 3.1.As is common practice, we need to (:alculatcthe 1)rol)abilities for a word sequence for thefirst; word's name class and every other worddiflbrently since we have no initial nalnt>classto make a transit ion frolll.
Accordingly we usel;he R)llowing equation to (:alculatc the ilfitialname (:lass probability,~,,J'(Cz,..~,,I < wi~,..~, 19~,.,~,, >) +I',,.
( G)~o.1' ( GA ,./' (G;v~.f (G5:I./'(G), ~./' (G),,~.I(G)< Wt,l,} >,< l lS,_,, l , i  ~ >,G J) :-< 1'15., I ~,, >, < I,V~_~, l )_j >, G.-~) +< _, l'i >, < 115_ l, Ft,- ~ >, Ct-., ) +< 115, Fi >, < _, P~,_~ >, G ~) +< _, l,) >, < ._, 1% ~ >, C~__~) +(2)whc,:c f(I) is ('alculatcd with nmxinluln-likelihood estimates from counts on trainingdata, so that tbr example,.f(G,I < 1,~5,1,i >,< I,t,~_,, F~_~ >,G-~)  -T(< I lS, 1,~ >, G., < 1'15_,, 1~}_~ >, G.-@,T(< l'lZj,,l~J >,< \ [ 'Vt- l ,Ft- I  >,Ct- l )  ~3)Where T() has been found from counting theevents in thc training cortms.
In our currentsysl;oln \vc SC\[; t;tlc C()llSt;&lltS ~i }lJld o- i \])y halldall(l let ~ ai = 1.0, ~ Ai = 1.0, a0 > al k O-2,A0 > A I .
.
.
_> As.
Tile current name-class Ctis conditioned oil the current word and fea-t;llrc~ thc I)rcviolls name-class, ~*t--l: and t)rc-vious word an(t tbaturc.Equations 1 and 2 implement a linear-interpolating HMM that  incorporates a mmfl)cr203of sub-models (rethrred to fl'om now by theirA coefficients) designed to reduce the effects ofdata sparseness.
While we hope to have enoughtraining data to provide estimates tbr all modelparameters, in reality we expect to encounterhighly fl'agmented probability distributions.
Inthe worst case, when even a name class pairhas not been observed beibre in training, themodel defaults at A5 to an estimate of nameclass unigrams.
We note here that the bigramlanguage model has a non-zero probability asso-ciated with each bigram over the entire vocal)-ulary.Our model differs to a backoff ormulation be-cause we tbund that this model tended to sufferfl'om the data sparseness problem on our smalltraining set.
Bikel et alfor example consid-ers each backoff model to be separate models,starting at the top level (corresl)onding approx-imately to our Ao model) and then falling backto a lower level model when there not enoughevidence.
In contrast, we have combined thesewithin a single 1)robability calculation tbr state(class) transitions.
Moreover, we consider thatwhere direct bigram counts of 6 or more occurin the training set, we can use these directly toestimate the state transition probability and wense just the ,~0 model in this case.
For countsof less than 6 we smooth using Equation 2; thiscan be thought of as a simt)le form of q)nck-eting'.
The HMM models one state per name(:lass as well as two special states tbr the startand end o fa  sentence.Once the state transition l)rol)abilities havebeen calcnlated according to Equations 1 and 2,the Viterbi algorithm (Viterbi, 1967) is used tosearch the state space of 1)ossible name class as-signments.
This is done in linear time, O(MN 2)for 54 the nunfl)er of words to be classified andN the number of states, to find the highest prob-ability path, i.e.
to maxinfise Pr(W,  C).
In ourexl)eriments 5/i is the length of a test sentence.The final stage of our algorithm that is usedafter name-class tagging is complete is to use~ clean-up module called Unity.
This creates afrequency list of words and name-classes tbr adocmnent and then re-tags the document usingthe most frequently nsed name class assigned bythe HMM.
We have generally tbund that thisimproves F-score performance by al)out 2.3%,both tbr re-tagging spuriously tagged words andWord Feature Exmnl)leDigitNmnber 15SingleCap MGreekLetter alphaCapsAndDigits I2TwoCaps RalGDSLettersAndDigits p52hfitCap InterleukinLowCaps ka,t)paBLowercase kinasesIIyphonBackslash /OpenSquare \[CloseSquare \]ColonSemiColonPercent %Oi) enParen (CloseParen )CommaFullStopDeternliner theConjmmtion andOther * +Table 2: Word tbatures with examplestbr finding untagged words in mlknown contextsthat had been correctly tagged elsewhere in thetext.3.1 Word  featuresTable 2 shows the character t'eatnres that weused which are based on those given for Nymbleand extended to give high pertbrmance in bothmolecular-biology and newswire domains.
Theintnition is that such features provide evidencethat helps to distinguish nmne classes of words.Moreover we hyt)othesize that such featnreswill help the model to find sinfilarities betweenknown words that were tbnnd in the trainingset and unknown words (of zero frequency inthe training set) and so overcome the unknownword t)rol)lem.
To give a simple example: if weknow that LMP - 1 is a member of PROTEINand we encounter AP - 1 for the first time intesting, we can make a fairly good guess aboutthe category of the unknown word 'LMP' basedon its sharing the same feature TwoCaps  withthe known word 'AP' and 'AP's known relation-ship with '- 1'.Such unknown word evidence is captured insubmodels A1 through ),3 in Equation 2.
\?e204consider that character information 1)rovidesmore mealfingflll distinctions between name(;\]asses than for examI)le part-of-speech (POS),since POS will 1)redominmltly 1)e noun fi)r allname-class words.
The t'catures were chosento be as domain independent as possit)le, withthe exception of I lyphon and Greel,:Letter whichhave t)articular signitieance for the terminologyin this dolnain.4 Exper iments4.1 Tra in ing  and  tes t ing  setThe training set we used in our experiments('onsisted of 100 MEI)II, INI~ al)stra(:ts, markedUl) ill XS/\[L l)y a (lonmin ext)ert for the name('lasses given in Tal)le 1.
The mmfl)er of NEsthat were marked u 1) by class are also given inTfl)le 1 and the total lmmber of words in thecorlms is 299/\]:0.
The al)stracts were chosen froma sul)(lomain of moleeular-1)iology that we for-mulated by s(',ar(;hing under the terms h/uman,blood cell, trav,.scription ,/'actor in the 1)utiMeddatal)asc, This yiel(l('.
(t al)t)roximately 33(10 al/-stracts.4.2 Resu l tsThe results are given as F-scores, a (;Ollllll()llmeasurement for a(:(:ura(:y in tlw, MUC con-ferences that eonfl)ines r(;(:all and 1)re(:ision.These are eah:ulated using a standard MUC tool(Chinchor, 1995).
F-score is d('.iin(~d as'2 x lS"(eci.sion x l~cc, llF - .~cor.
= (4)l)'rccisio~, + \]?,cc(dlThe tirst set ot7 experiments we did shows theeffectiveness of the mode.1 for all name (:lassesand is smnmarized in Table 3.
We see that datasparseness does have an etfe('t~ with 1)roteins -the most mlmerous (;lass in training - gettingthe best result and I/,NA - the snmllc, st training(:lass - getting the worst result.
The tal)le alsoshows the ett'eetiveness of the character featureset, whi('h in general adds 10.6% to the F-score.This is mainly due to a t)ositive effect on wordsin the 1)R,OTEIN and DNA elases, but we alsosee that memt)ers of all SOURCE sul)-('lassessufl'er from featurization.We have atteml)ted to incorl)orate generali-sation through character t'eatm:es and linear in-teri)olation, which has generally \])een quite su(:-cessful.
Nevertheless we were (:urious to see justClass Base llase-l'eaturesPROTEIN 0.759 0.670 (-11.7%)DNA 0.472 0.376 (-20.3%)\]~NA 0.025 0.OOO (-leo.o%)SOURCE(all) 0.685 0.697 (+1.8%)S()UI{CE.cl 0.478 0.503 (+5.2%)SOURCE.el 0.708 0.752 (+6.2%)SOURCE.me 0.200 0.311 (+55.5%)SOURCE.mu 0.396 0.402 (+1.5%)SOURCE.vi 0.676 0.713 (+5.5%)S()URCI,Lsl 0.540 0.549 (+1.7%)SOURCE.ti 0.206 0.216 (+4.9%)All classes 0.728 0.651 (-10.6%)q)d)le 3: Named entity acquisition results us-ing 5-fi)ld cross validation on 100 XML taggedMEI)I~INE al/stra(:ts, 80 for training and 20 fin.testing, l\]ase-J'(',at'urc.s u es no character featureinibrmation.
)~ Mode\[ No.# Texts 0 1 2 3 4 58040201050.06 0.22 0.10 0.67 0.93 1.00.06 0.19 0.10 0.63 0.94 1.0().
()~l 0.15 0.09 0.59 0.89 1.00.03 0.12 0.08 0.52 0.83 1.00.02 0.09 0.06 0.41 0.68 1.0Tal)le 4: M(',an lmml)er of successflll calls to sul)-m(i(t(;ls during testing as a fl'aetion of total mnn-1)er (If stale transitions in the Viterl)i latti(:e, g/:T(!xis indicates the mmfl)er of al)stra(:ts used illtraining.whi(:h t)arts of the model were contributing tothe bigram s(:ores.
Table 4 shows the l)ercent-age of bigranls which could be mat('hed againsttraining t)igrams.
The result indicate tha~ ahigh 1)ereentage of dire(:t bigrams in the testeorl)uS never al)t)(;ar in the training (:oft)us andshows tha, t our HMM model is highly depel>(l(mt on smoothing through models ~kl and )~:~.\?e can take another view of the training data1)y 'salalni-slieing' the model so that only evi-(tenee from 1)art of the model is used.
Resultsare shown in Tat)le 5 and support the eonchl-sion that models Al, A2 and Aa are.
crucial atthis sir,(; of training data, although we wouldexpect their relative ilnportance to fifil as wehave more (tircct observations of bigrams withlarger training data sets.Tal)le 6 shows the rolmstness of the model205I Backoff models\[ F-score (all classes) 0.728 0.722 0.644 0.572 0.576 \]Table 5: F-scores using different nfixtures of models tested on 100 abstracts, 80 training and 20testing.I # Texts 80 40 20 10 5 \]I F-score 0.728 0.705 0.647 0.594 0.534\]Table 6:trainingstracts).F-score for all classes agMnst size ofcorpus (in number of MEDLINE ab-for data sparseness, so that even with only 10training texts the model can still make sensibledecisions about term identification and classi-fication.
As we would expect;, the table ;flsoclearly shows that more training data is better,and we have not yet reached a peak in pertbr-i nance .5 Conc lus ionHMMs are proving their worth for varioustasks in inibrmation extraction and the resultshere show that this good performance can beachieved across domains, i.e.
in molecular-biology as well as rising news paper reports.
Thetask itself', while being similar to named entityin MUC, is we believe more challenging due tothe large nunfl)er of terms which are not propernouns, such as those in the source  sub-classes aswell as the large lexieal overlap between classessuch as PROTEIN  and DNA.
A usefifl line ofwork in the future would be to find empiricalmethods for comparing difficulties of domains.Unlike traditional dictionary-based lnethods,the method we have shown has the advantage ofbeing portable and no hand-made patterns wereused.
Additiolmlly, since the character tbaturesare quite powerful, yet very general, there is lit-tle need for intervention to create domain spe-cific features, although other types of featurescould be added within the interpolation frame-work.
Indeed the only thing that is required isa quite small corpus of text containing entitiestagged by a domain expert.Currently we have optinfized the ,k constantsby hand but clearly a better way would be to dothis antomatically.
An obvious strategy to usewould be to use some iterative learning methodsuch as Expectation Maximization (Dempsteret al, 1977).The model still has limitations, most obvi-ously when it needs to identity, term boundariesfor phrases containing potentially ambiguous lo-cal structures uch as coordination and pa.ren-theses.
For such cases we will need to add post-processing rules.There are of course many NF, models thatare not based on HMMs that have had suc-cess in the NE task at the MUC conferences.Our main requirement in implementing a modelfor the domain of molecular-biology has beenease of development, accuracy and portabilityto other sub-domains since molecular-biology it-self is a wide field.
HMMs seemed to be themost favourable option at this time.
Alterna-tives that have also had considerable successare decision trees, e.g.
(Nobata et al, 1.999)and maximum-entropy.
The maximum entropymodel shown in (Borthwick et al, 1998) in par-ticular seems a promising approach because ofits ability to handle overlapping and large fea-ture sets within n well founded nmthenmticalti'amework.
However this implementation of themethod seems to incorporate a number of hand-coded domain specitic lexical Datures and dic-tionary lists that reduce portability.Undoubtedly we could incorporate richer tba-tures into our model and based on the evidenceof others we would like to add head nouns asone type of feature in the future.AcknowledgementsWe would like to express our gratitude to YukaTateishi and Tomoko Ohta of the Tsujii labora-tory for their efforts to produce the tagged cor-tins used in these experiments and to Sang-ZooLee also of the Tsujii laboratory tbr his com-ments regarding HMMs.
We would also like tothank the anonymous retirees tbr their helpflflcomments.206\]~{,eferencesA.
Bairoch and R. Apweiler.
1997.
The SWISS-PF\[OT 1)r{)t{~in sequence data bank and itsnew SUl)l)lement 15:EMBL.
Nucleic Acids Re-search, 25:31-36.D.
Bikel, S. Miller, I:L Schwartz, andR.
Wesichedel.
1997.
Nymble: a high-t)ertbrmanee l arning \]mlne-tin(ler.
In Pro-ceedings of the Fifth Co~@rcrcncc on AppliedNatural Langua9 e \])~vcessi'n,g, pages 194 201.A.
Borthwick, J.
Sterling, E. Agichtein, andll,.
Grishman.
1998.
Ext}l{}iting div(:rseknowledge sour(:es via lllaXillllllll (mtrol}y innamed entity recogniti{}n. In P'mcccdingsof the Worlcshop on Very Lar.qc Corpora(WVLC'98).S.
Chert and J. Goodman.
1996.
An empiricalstudy of smoothing te{:hmfiques tbr languagemotleling.
3/tst Annual Meeting of tlt,(: Associ-ation of Computational Linguistics, Calffofnia, USA, 24-27 .hme.N.
Chin{:h{}r. 1995.
MUC-5 ewduati{m etrics.In In Pwcecdings of th, c i"ffl, h, Mc.ss(u.le Un-dcrstandin 9 Cou:fe'rencc (MUC-5), Baltimore,,Maryland, USA., 1)ages 69 78.N.
Collier, It.S.
Park, N. Ogata, Y. Tateishi,C.
Nol}ata, 'F.
Ohta, T. Sekimizu, H. \]mai,and J. Tsujii.
1999.
The GENIA 1}r{)je(:t:corlms-1)ascd kn(}wlcdge acquisitio\], and in-forlnal, ion extra('tion f\]'Olll genome r{',sear(:ht)al)ers, in Proccediu, fl.s of the A n',,'aal M(',etingof the European ch, aptcr of the Association forComputational Lingu'istic,s (EA (/\]3 '99), 3 uuc.M.
Craven and 3, Kumlien.
1999.
Construct-ing bioh}gical knowh;{tg{; t}ases t)y extractinginformation from text sour(:es.
In \]}~vc(:(,Aingsof the 7th, hl, tcrnational CoTff(:rence on Intelli-gent Systcmps for Molecular Biology (ISMB-99), Heidellmrg, Germmly, August 6 10.A.P.
Dempster, N.M. Laird, and D.B.
Rubins.1977.
Maximmn likelihood from incoml)letedata via the EM algorithm.
,\]ou'rnal of theRoyal Statistical Society (B), 39:1-38.l).
Freitag and A. McCMlum.
1999.
Intbrma-tion extraction with HMMs and shrinkage.In Proceedings of the AAAl'99 Worl~.~h, op ou,Machine Learning for IT~:formation Extrac-tion, Orlando, Florida, July 19th.K.
Fuku(la, T. Tsunoda, A.
2)mmra, andT.
Takagi.
1998.
~12)ward intbrmation extrac-tion: identifying l)rotein names from biologi-eal papers.
Ill PTvcccdings of thc Pac'lific Sym-posium on Biocomp'uting'98 (PSB'98), .Jan-1uAYy..1.
Kupiec.
1992. l/obust Imrt-ofspeech tag-ging using a hidden markov model.
ComputerSpeech and Lang'aagc, 6:225-242.MEI)LINE.
1999.
The PubMeddatal)ase can be t'(mnd at:.httt)://www.ncbi.nhn.nih.gov/Pul}Med/.DAIIPA.
1995. l}roceeding.s o.fl th, c Sixth,Message Understanding Cou:fcrcnce(MUC-6),Cohmdfia, MI), USA, Nove, nfl}er.
MorganNail\['\] l lal l l l .C.
Nobata, N. Collier, and J. Tsu.iii.
1999.
Au-tomatic term identification and classificationin 1}iology texts.
In Proceeding.s" of the Nat-u'ral Lang,lmgc Pacific Rim Symposium (NL-PRS'2000), November.Y.
Ohta, Y. Tateishi, N. Collie'r, C. No-1)ata, K. II}ushi, and J. Tsujii.
1999.
Asenmntieally annotated cort)us from MED-L\]\[NE al)sl;ra{:l;s. In l}'rocccd,bu.l s of th.c ~:nth.Workshop on Go'home I~fformatics.
UniversalA{:ademy Press, Inc., 14 15 Deccntl)er.l~.
llabiner and B..\]uang.
1!)86.
An intro{tu{:-ti(m to hidden Markov too(Ms. H'2EE ASSPMagazi',,(',, 1}ages d 16, Jammry.T.
Sekilnizu, H. Park, and J.
'l'sujii.
1998.I{lenti\[ying l;he interaction 1)etween genes an{1gOlle i}ro(lucts \]}ase(l on f\]'e(lue\]My seen verbsin n\]e{tline al)si;rael;s. Ill ~(:'li,()?ll,('~ \]~ffor'm, al, ics'.Univcrsa,1 Academy Press, Inc.K.
Seymore, A. MeCallum, and l{.
I{oscnfeld.1999.
Learning hidden Markove strucl:urefor informati{m (,xtraction.
In \])wcccdings ofthe AAAl'99 Workshop on Macfli'n,(: Lcarni'n 9for l',fo'rmation E:draction, Orland{}, Flori{ta.,July 19th..J. Thomas, D. Milward, C. Ouzounis, S. Pul-man, and M. Carroll.
1999.
Automatic ex-traction of 1)rotein interactions fl'om s{'ien-tific abstracts.
In Proceedings of the I}ac'll/icSymposium on Biocomputing'99 (PSB'99),Hawaii, USA, Jmmary 4-9.A.
3.
Vit(;rbi.
1967.
Error l){mnds for {:onvolu-tions e{}{les and an asyml)totically optimumdeco(ling algorithm.
IEEE Tran,s'actiou,.s' onI~formation Theory, IT-13(2):260 269.207
