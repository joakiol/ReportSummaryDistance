Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 606?615,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsMinimum Error Rate Training by Sampling the Translation LatticeSamidh Chatterjee?Department of Computer ScienceFlorida State University, USAchatterj@cs.fsu.eduNicola CanceddaXerox Research Centre Europe6 Chemin de Maupertuis, 38240 Meylan, Francenicola.cancedda@xrce.xerox.comAbstractMinimum Error Rate Training is the algo-rithm for log-linear model parameter train-ing most used in state-of-the-art StatisticalMachine Translation systems.
In its originalformulation, the algorithm uses N-best listsoutput by the decoder to grow the Transla-tion Pool that shapes the surface on whichthe actual optimization is performed.
Recentwork has been done to extend the algorithmto use the entire translation lattice built bythe decoder, instead of N-best lists.
We pro-pose here a third, intermediate way, consist-ing in growing the translation pool using sam-ples randomly drawn from the translation lat-tice.
We empirically measure a systematic im-provement in the BLEU scores compared totraining using N-best lists, without sufferingthe increase in computational complexity as-sociated with operating with the whole lattice.1 IntroductionMost state-of-the-art Statistical Machine Translation(SMT) systems are based on a log-linear model ofthe conditional probability of generating a certaintranslation given a specific source sentence.
Morespecifically, the conditional probability of a transla-tion e and a word alignment a given a source sen-tence f is modeled as:?The work behind this paper was done during an intern-ship at the Xerox Research Centre Europe.
The author was par-tially supported by NSF through Grant CCF-0643593 and theAFOSR Young Investigator Research Program.P (e, a|f) ?
exp(K?k=1?khk (e,a,f))(1)where the hk(e,a,f) are feature functions provid-ing complementary sources of information on thequality of the produced translation (and alignment).Once such a model is known: the decoder (i.e.
theactual translation program), which builds a transla-tion by searching in the space of all possible transla-tions the one that maximizes the conditional proba-bility:(e?, a?)
= arg maxe,aK?k=1?khK(e,a,f) (2)where we have taken into account that the exponen-tial is monotonic.The parameters ?k determine the relative impor-tance of the different feature functions in the globalscore.
Best results are typically obtained by search-ing in the space of all possible parameter vectors ?
?for the one that minimizes the error on a held-outdevelopment dataset for which one or more refer-ence human translations are available, as measuredby some automatic measure.
This procedure is re-ferred to as Minimum Error Rate Training (MERT).1.1 Minimum Error Rate Training on N-bestListsThe most widespread MERT algorithm is the onedescribed in (Och, 2003).
This algorithm startsby initializing the parameter vector ??.
For eachsource sentence in the development set, the decoder606is used to initialize a translation pool with a list of N-best scoring candidate translations according to themodel.
Using this pool and the corresponding refer-ence translations then, an optimization procedure isrun to update the parameter vector to a ???
with re-duced error.
The decoder is then invoked again, thenew output N-best list is merged into the translationpool, and the procedure is iterated.
The algorithmstops either after a predefined number of iterationsor upon convergence, which is reached when no newelement is added to the translation pool of any sen-tence, or when the size of the update in the parametervector is below a threshold.The error measure minimized at each iteration isusually BLEU (Papineni et al, 2002).
BLEU essen-tially measures the precision with which the trans-lation produced by a system recovers n-grams ofdifferent orders from the available reference trans-lation(s), used as a gold standard.The optimization procedure that is run withineach iteration on the growing translation pools isbased on the key observation that BLEU only de-pends on the single translation receiving the highestscore by the translation model (which would be theone shown to the receipient) in the translation pool.This in turn means that, for any given sentence, itscontribution to BLEU changes only when the valueof the parameters change in such a way that the sen-tence ranking first according to the model switchesfrom one to another.
This situation does not changewhen one considers all the sentences in a develop-ment set instead of just one: while varying the ?
?vector, the BLEU score changes only when there isa change at the top of the ranking of the alternativesfor at least one sentence in the set.
In other words,BLEU is piece-wise constant in ??.
MERT then pro-ceeds by performing an iterative line search by fix-ing each time the value of all components of ??
butone1: for such a free parameter a global optimumcan be identified by enumerating all the points thatcause a change in BLEU.
The value of the compo-nent is then fixed at the middle of an interval withmaximum BLEU, and the procedure is iterated un-til convergence.
Since the error function is highlyirregular, and the iterative line search is not guaran-1More generally, one can select each time a combination ofcoordinates identifying a line in the parameter space, and is notrestricted to a coordinate direction.teed to converge to a global optimum, the procedureis repeated many times with different initializations,and the best convergence point is retained.The MERT algorithm suffers from the followingproblem: it assumes at each iteration that the setof candidates with a chance to make it to the top(for some value of the parameter vector) is wellrepresented in the translation pool.
If the transla-tion pool is formed in the standard way by merg-ing N-best lists, this assumption is easily violated inpractice.
Indeed, the N-best list often contains onlycandidates displaying minor differences, and repre-sents only a very small sample of alternative possi-ble translations, strongly biased by the current pa-rameter setting.Recognizing this shortcoming, Macherey et al(2008) extended the MERT algorithm so as to usethe whole set of candidate translations compactlyrepresented in the search lattice produced by the de-coder, instead of only a N-best list of candidatesextracted from it.
This is achieved via an elegantbut relatively heavy dynamic programming algo-rithm that propagates sufficient statistics (called en-velopes) throughout the whole search graph.
The re-ported theoretical worst-case complexity of this al-gorithm is O(|V ||E| log |E|), where V and E are thevertex set and the edge set of the lattice respectively.We propose here an alternative method consist-ing in sampling a list of candidate translations fromthe probability distribution induced by the transla-tion lattice.
This simple method produces a list ofcandidates more representative of the complete dis-tribution than an N-best list, side-stepping the in-tricacies of propagating envelopes throughout thelattice.
Computational complexity increases onlymarginally over the N-best list approach, while stillyielding significant improvements in final transla-tion quality.1.2 The translation latticeFinding the optimal translation according to Equa-tion 1 is NP-complete (Knight, 1999).
Most phrase-based SMT systems resort then to beam-searchheuristic algorithms for solving the problem approx-imately.
In their most widespread version, PBSMTdecoders proceed by progressively extending trans-lation prefixes by adding one new phrase at a time,and correspondingly ?consuming?
portions of the607source sentence.
Each prefix is associated with anode in a graph, and receives a score according tothe model.
Whenever two prefixes having exactlythe same possible extensions are detected, the lower-scoring one is merged into the other, thus creating are-entrancy in the directed graph, which has then thecharacteristics of a lattice (Figure 1).
Edges in thelattice are labelled with the phrase-pair that was usedto perform the corresponding extension, the sourceword positions that were covered in doing the ex-tension, and the corresponding increment in modelscore.0FI have aJ?ai une(1,2,3)?12.24bleue(4)...bluevoiture(5)...carJ?ai une bleueI have a blue(1,2,3,4)...2 31a blue carune voiture bleue(3,4,5)...I haveJ?ai(1,2)...Figure 1: A lattice showing some possible translations ofthe English sentence: I have a blue car.
The state withID 0 is the start state and the one with F is the final state.2 Related WorkSince its introduction, (Och, 2003) there has beenvarious suggestions for optimizing the MERT cri-terion.
Zens et al (2007) use the MERT criterionto optimize the N-best lists using the DownhillSimplex Algorithm (Press, 2007).
But the Down-hill Simplex Algorithm loses its robustness as thedimension goes up by more than 10 (Machereyet al, 2008).
Deterministic Annealing was sug-gested by Smith and Eisner (2006) where the au-thors propose to minimize the expected loss orrisk.
They define the expectation using a proba-bility distribution over hypotheses that they gradu-ally anneal to focus on the 1-best hypothesis.
Dif-ferent search strategies were investigated by Ceret al (2008).
Work has been done to investigate aperceptron-like online margin training for statisit-ical machine translation (Watanabe et al, 2007).Building on this paper, the most recent work toour knowledge has been done by Chiang et al(2008).
They explore the use of the Margin InfusedRelaxed Algorithm (MIRA) (Crammer and Singer,2003; Crammer et al, 2006) algorithm instead ofMERT.
Macherey et al (2008) propose a new varia-tion of MERT where the algorithm is tuned to workon the whole phrase lattice instead of N-best listonly.
The new algorithm constructs the error surfaceof all translations that are encoded in the phrase lat-tice.
They report significant convergence improve-ments and BLEU score gains over N-best MERTwhen trained on NIST 2008 translation tasks.
Morerecently, this algorithm was extended to work withhypergraphs encoding a huge number of translationsproduced by MT systems based on SynchronousContext Free Grammars (Kumar et al, 2009).
Allthe methods cited here work on either N-best lists orfrom whole translation lattices built by the decoder.To our knowledge, none of them proposes samplingtranslations from the lattice.3 Sampling candidate translations fromthe latticeIn this section we first start by providing an intu-ition of why we believe it is a good idea to samplefrom the translation lattice, and then describe in de-tail how we do it.3.1 An intuitive explanationThe limited scope of n-best lists rules out many al-ternative translations that would receive the highestscore for some values of the parameter vector.
Thecomplete set of translations that can be produced us-ing a fixed phrase table (also called reachable trans-lations) for a given source sentence can be repre-sented as a set of vectors in the space spanned bythe feature functions (Fig.
2).
Not all such transla-tions stand a chance to receive the highest score forany value of the parameter vector, though.
Indeed, iftranslations h, h?and h?
are such that hk ?
h?k ?
h?
?kfor all feature k, then there is no value of ??
thatwill give to h?
a score higher than both h and h?.The candidates that would rank first for some valueof the ??
parameter vector are those on the convexenvelope of the overall candidate set.
We know ofno effective way to generate this convex envelope in608polynomial time.
The set of candidates representedby the decoder lattice is a subset (enclosed in thelarger dashed polygon in the figure) of this set.
Thissubset is biased to contain translations ranking highaccording to the values of the parameter vector (thedirection labelled with ?)
used to produce it, becauseof the pruning strategies that guide the constructionof the translation lattice.
Both the N-best list andour proposed random sample are further subsets ofthe set of translations encoded in the lattice.
The N-best list is very biased towards translations that scorehigh with the current choice of parameters: its con-vex envelope (the smaller dashed polygon) is verydifferent from the one of the complete set of trans-lations, and also from that of the translations in thelattice.
The convex envelope of a random samplefrom the translation lattice (the dotted polygon in thefigure), will generally be somewhat closer to the en-velope of the whole lattice itself.The curves in the figure indicate regions of con-stant loss (e.g.
iso-BLEU score, much more irregu-larly shaped in reality than in the drawing).
For thissentence, then, the optimal choice of the parameterswould be around ??.
Performing an optimizationstep based on the random sample envelope wouldresult in a more marked update (?
?sample) in the di-rection of the best parameter vector than if an N-bestlist is used (?
?N-best).Notice that Figure 2 portraits a situation with onlytwo features, for obvious reasons.
In practice thenumber of features will be substantially larger, withvalues between five and twenty being common prac-tice.
In real cases, then, a substantially larger frac-tion of reachable translations will tend to lie on theconvex envelope of the set, and not inside the convexhull.3.2 The sampling procedureWe propose to modify the standard MERT algorithmand sample N candidates from the translation latticeaccording to the probability distribution over pathsinduced by the model, given the current setting ofthe ??
parameters, instead of using an N-best list.The sampling procedes from the root node of thelattice, corresponding to an empty translation can-didate covering no words of the source, by chosingstep by step the next edge to follow.
The probabilityreference??????h1h2??
?best in latticebest in random samplebest in N?best listbest reachableN?bestsamplelatticeFigure 2: Envelope of the set of reachable translationswhere the model has two feature functions h1 and h2.The envelope of the lattice is the outer dashed polygon,while the envelope of the N-best list is the inner one.
Us-ing the whole lattice as translation pool will result in amore marked update towards the optimal parameters.
Therandom sample from the lattice is enclosed by the dottedline.
If we use it, we can intuitively expect updates to-wards the optimum of intermediate effectiveness betweenthose of the N-best list method and those of the latticemethod.distribution for each possible follow-up is the poste-rior probability of following the edge given the pathprefix derived from the lattice: it is obtained via apreliminary backward sweep.Since feature functions are incremental over theedges by design, the non-normalized probability ofa path is given by:P (e1, .
.
.
, em) = ePmi=1 ?
(ei) (3)where?
(ei) =K?k=1?khk(ei) (4)is the score of edge ei.
With a small abuse of no-tation we will also denote it as ?
(nj,k), where it isintended that ei goes from node nj to node nk.
Let?sdenote with ?
(ni) the score of node ni, i.e.
the loga-rithm of the cumulative unnormalized probability ofall the paths in the lattice that go from node ni to afinal node.
The unnormalized probability of select-ing node nj starting from ni can then be expressedrecursively as follows:609S(nj |ni) ?
e(?(nj)+?
(ni,j)) (5)The scores required to compute this samplingprobabilities can be obtained by a simple backwardpass in the lattice.
Let Pi be the set of successorsof ni.
So the total unnormalized log-probability ofreaching a final state (i.e.
with a complete transla-tion) from ni is given by the equation below.?
(ni) = log(?nj?Pie(?(nj)+?
(ni,j))) (6)where we set ?
(ni) = 0 if Pi = ?, that is if niis a final node.
At the end of the backward sweep,?
(n0) contains the unnormalized cumulative prob-ability of all paths, i.e.
the partition function.
No-tice that this normalising constant cancels out whencomputing local sampling probabilities for traversednodes in the lattice.Once we know the transition probability (Eq.
5)for each node, we sample by starting in the root nodeof the lattice and at each step randomly selectingamong its successors, until we end in the final node.The whole sampling procedure is repeated as manytimes as the number of samples sought.
After col-lecting samples for each sentence, the whole list isused to grow the translation pool.Notice that when using this sampling method it isno longer possible to use the stability of the trans-lation pool as a stopping criterion.
The MERT al-gorithm must thus be run either for a fixed numberof iterations, or until the norm of the update to theparameter vector goes below a threshold.3.3 Time Complexity AnalysisFor each line search in the inner loop of the MERTalgorithm, all methods considered here need to com-pute the projection of the convex envelope that canbe scanned by leaving all components unchangedbut one2.
If we use either N-best lists or randomsamples to form the translation pool, and M is thesize of the translation pool, then computing the en-velope can be done in time O(M log M) using theSweepLine algorithm reproduced as Algorithm 1 in(Macherey et al, 2008).
As shown in the same ar-ticle, the lattice method for computing the envelope2In general, moving along a 1-dimensional subspace of theparameter space.is O(|V ||E| log |E|), where V is the vertex set of thelattice, and E is its edge set.
In standard decodersthere is a maximum limit D to the allowed distor-tion, and lattice vertices are organized in J priorityqueues 3 of size at most a, where J is the length ofthe source sentence and a is a parameter of the de-coder set by the user.
Also, there is a limit K tothe maximum number of source words spanned bya phrase, and only up to c alternative translationsfor a same source phrase are kept in the phrase ta-ble.
Under these standard conditions, the numberof outgoing edges E?
from each lattice vertex canbe bounded by a constant.
A way to see this is byconsidering that if an hypothesis is extended with aphrase, then the extended hypothesis must end up ina stack at most K stacks to the right of the originalone.
There are only aK places in these stacks, so itmust be |E?| ?
aK.Since the number of edges leaving each node isbounded by a constant, it is |E| = ?
(|V |), and thelattice method is O(|V |2 log(|V |)).
The maximumnumber of vertices in the lattice is limited by thecapacity of the stacks: |V | ?
aJ .
This eventuallyleads to a complexity of O(J2 log J) for the innerloop of the lattice method.It is interesting to observe that the complexity isdriven by the length of the source sentence in thecase of the lattice method, and by the size of thetranslation pool in the case of both the N-best listmethod and the random sampling method.
The lat-ter two methods are asymptotically more effective aslong as the size of the sample/N-best list grows sub-quadratically in the length of the sentence.
In mostof our experiments we keep the size of the sampleconstant, independent of the length of the sentence,but other choices can be considered.
Since the num-ber of reachable translations grows with the lengthof the source sentence, length-independent samplesexplore a smaller fraction of the reachable space.Generating samples (or n-best lists) of size increas-ing with the length of the source sentence could thuslead to more homogeneous sampling, and possibly abetter use of CPU time.We have so far compared methods in term of thecomplexity of the innermost loop: the search for aglobal optimum along a line in the parameter space.3Traditionally referred to as stacks.610This is indeed the most important analysis, sincethe line search is repeated many times.
In order tocomplete the analysis, we also compare the differ-ent methods in terms of the operations that need beperformed as part of the outer iteration, that is uponredecoding the development set with a new parame-ter vector.The N-best list method requires simply construct-ing an N-best list from the lattice.
This can be donein time linear in the size J of the sentence and in Nwith a backward sweep in the lattice.The sampling method requires sampling N timesthe lattice according to the probability distributioninduced by the weights on its edges.
We use adynamic programming approach for computing theposterior probabilities of traversing edges.
In thisphase we visit each edge of the lattice exactly once,hence this phase is linear in the number of edgesin the lattice, hence under the standard assumptionsabove in the length J of the sentence.
Once posteriorprobabilities are computed for the lattice, we needto sample N paths from it, each of which is com-posed of at most J edges4.
Under standard assump-tions, randomly selecting the next edge to follow ateach lattice node can be done in constant time, sothe whole sampling is also O(NJ), like extractingthe N-best list.No operation at all is required by the latticemethod in the outer loop, since the whole lattice ispassed over for envelope propagation to the innerloop.4 Experimental ResultsExperiments were conducted on the Europarl corpuswith the split used for the WMT-08 shared task (Eu-roparl training and test condition) for the languagepairs English-French (En-Fr), English-Spanish (En-Es) and English-German (En-De), each in both di-rections.
Training corpora contain between 1.2 and1.3 million sentence pairs each, development andtest datasets are of size 2,000.
Detailed token andtype statistics can be found in Callison-Burch et al(2008).
The Moses decoder (Koehn et al, 2007)was used for generating lattices and n-best lists.
Themaximum number of decoding iterations was set totwelve.
Since Moses was run with its lexicalised dis-4We assume all phrase pairs cover at least one source word.0.180.20.220.240.260.280.30.320 1 2 3 4 5 6 7 8 9 10 11 12nb.200.en-frs.200.en-frm.200.en-frs.100.en-frnb.100.en-fr0.30.320.20.220.240.260.280 1 2 3 4 5 6 7 8 9 10 11nb.200.fr-ens.200.fr-enm.200.fr-ens.100.fr-ennb.100.fr-enFigure 3: Learning curves (BLEU on the developmentset) for different tested conditions for English to French(top) and French to English (bottom).tortion model, there were 14 features.
Moses L1-normalises the parameter vector: parameter scalingonly marginally affects n-best list construction (viathreshold pruning during decoding), while it sub-stantially impacts sampling.For each of the six configurations, we comparedthe BLEU score on the test data when optimizingfeature weights with MERT using n-best and ran-dom samples of size 100 and 200.
In all cases weused 20 random restarts for MERT.
Results are pre-sented in Table 1.
We also ran non systematic ex-periments on some of the configurations with largersamples and n-best lists, with results changing verylittle from the respective 200 cases: we do not reportthem here.Learning curves (BLEU on the development set)are shown in Figure 3.
Learning curves for the othertested language pairs follow a similar pattern.6115 Analysis of resultsAll differences of the test scores between optimiz-ing the parameters using nbest-200 lists and fromrandomly sampled lists of size 200 were found tobe statisitically significant at 0.05 level at least.
Weused Approximate Randomization Test (Riezler andMaxwell, 2005) for the purpose, random samplingbeing done 1000 times.S-T NB-100 RS-100 NB-200 RS-200En-Fr 32.47 31.36 32.32 32.76Fr-En 32.43 31.77 32.46 32.91En-Es 29.21 28.98 29.65 30.19Es-En 30.97 30.41 31.22 31.66En-De 20.36 19.92 20.55 20.93De-En 27.48 26.98 27.30 27.62Table 1: Test set BLEU Scores for six different ?Source-Target?
PairsSomewhat surprisingly, while random samplingwith sample size of 200 yields overall the best re-sults, random sampling with size 100 give system-atically worse results than n-best lists of the samesize.
We conjectured that n-best lists and randomsamples could have complementary advantages.
In-deed, it seems intuitive that a good translation poolshould be sufficiently varied, as argued in Section3.1.
However it should also stand high chances tocontain the best reachable translation, or translationsclose to the best.
It might thus be that 100-best listsare unable to provide diversity, and random samplesof size 100 to guarantee sufficient quality.In order to test this conjecture we repeated ourexperiments, but at each iteration we used the unionof a 100 random sample and a 100 n-best list.
Re-sults for this experiments are in Table 2.
The cor-responding results with random samples of size 200are also repeated to ease comparison.
Depending onthe language pair, improvements over random sam-pling range from 0.17 (En-Es) to 0.44 (Fr-En) BLEUpoints.
Improvements over 200-best lists range from0.68 (De-En) to 0.89 (Fr-En) BLEU points.
Theseresults indicate quite clearly that N-best lists andrandom samples contribute complementary infor-mation to the translation pool: indeed, in most casesthere is very little or no overlap between the two.Convergence curves show that RS-200, NB-100Source-Target Mixed 100 + 100 RS-200En-Fr 33.17 32.76Fr-En 33.35 32.91En-Es 30.37 30.19Es-En 32.04 31.66En-De 21.31 20.93De-En 27.98 27.62Table 2: Test set BLEU Scores for the same ??Source-Target?
pairs using a mixed strategy combining a 100 N-best list and a random sample of size 100 after each roundof decoding.and M-200 (i.e.
the hybrid combination) systemati-cally converge to higher BLEU scores, on the devel-opment set and on their respective translation pools,than RS-100 and NB-200.
Notice however that it ismisleading to compare scores across different trans-lation pools, especially if these have substantiallydifferent sizes.
On the one hand adding more candi-dates increases the chances of adding one with highcontribution to the corpus BLEU, and can thus in-crease the achievable value of the objective function.On the other hand, adding more candidates reducesthe freedom MERT has to find parameter values se-lecting high-BLEU candidates for all sentences.
Tosee this, consider the extreme case when the transla-tion pools are all of size one and are provided by anoracle that gives the highest-BLEU reachable trans-lation for each sentence: the objective surface is un-informatively flat, all values of the parameters areequally good, and the BLEU score on the devset isthe highest achievable one.
If now we add to eachtranslation pool the second-best BLEU-scoring can-didate, BLEU will be maximized in a half-space foreach sentence in the development set: MERT will tryto select ?
in the intersection of all the half-spaces, ifthis is not empty, but will have to settle for a lower-scoring compromise otherwise.
The larger the trans-lation pools, the more difficult it becomes for MERTto ?make all sentences happy?.
A special case of thisis when adding more candidates extends the convexenvelopes in such a way that the best candidates fallin the interior of the convex hull.
It is difficult totell which of the two opposing effects (the one thattends to increase the value of the objective functionor the one that tends to depress it) is stronger in any612given case, but from the convergence curves it wouldseem that the first prevails in the case of randomsamples, whereas the second wins in the case of n-best lists.
In the case of random samples going fromsize 100 to 200 systematically leads to higher BLEUscore on the devsets, as more high-BLEU candidatesare drawn.
In the case of n-best lists, conversely,this leads to lower BLEU scores, as lower-BLEU (inaverage) candidates are added to translation poolsproviding a sharper representation of the BLEU sur-face and growing MERT out of the ?delusion?
that agiven high BLEU score is actually achieveable.In the light of this discussion, it is interestingto observe that the value achieved by the objectivefunction on the development set is only a weak pre-dictor of performance on the test set, e.g.
M-200never converges to values above those of NB-100,but is systematically superior on the test data.In Macherey et al (2008) the authors observe adip in the value of the objective function at the firstiteration when training using n-best lists.
We didnot observe this behaviour in our experiments.
Apossible explanation for this resides in the larger sizeof the n-best lists we use (100 or 200, compared to50 in the cited work) and in the smaller number ofdimensions (14 instead of 20-30).We hinted in Section 3.3 that it would seem rea-sonable to use samples/nbest-list of size increasingwith the length of the source sentence, so as to sam-ple reachable translations with a more uniform den-sity across development sentences.
We tested thisidea on the French to English condition, makingsamples size depend linearly on the length of thesentence, and in such a way that the average sam-ple size is either 100 or 200.
For average samplesize 100 we obtained a BLEU of 31.55 (comparedto 31.77 with the constant-size 100 random sample)and for average size 200 31.84 (32.46 in the cor-responding constant-size condition).
While partial,these results are not particularly encouraging w.r.t.using variable size samples.Finally, in order to assess the stability of the pro-posed training procedure across variations in devel-opment datasets, we experimented with extractingfive distinct devsets of size 2,000 each for the Frenchto English RS-200 condition, keeping the test setfixed: the maximum difference we observed was of0.33 BLEU points.6 ConclusionsWe introduced a novel variant to the well-knownMERT method for performing parameter estimationin Statistical Machine Translation systems based onlog-linear models.
This method, of straightforwardimplementation, is based on sampling candidatesfrom the posterior distribution as approximated byan existing translation lattice in order to progres-sively expand the translation pool that shapes theoptimization surface.
This method compares favor-ably against existing methods on different accounts.Compared to the standard method by which N-bestlists are used to grow the translation pool, it yieldsempirically better results as shown in our experi-ments, without significant penalties in terms of com-putational complexity.
These results are in agree-ment with the intuition that the sampling methodintroduces more variety in the translation pool, andthus allows to perform more effective parameter up-dates towards the optimum.
A hybrid strategy, con-sisting in combining N-best lists and random sam-ples, brings about further significant improvements,indicating that both quality and variety are desire-able in the translation pool that defines the optimiza-tion surface.
A possible direction to investigate inthe future consists in generalizing this hybrid strat-egy and combining random samples where the prob-ability distribution induced on the lattice by the cur-rent parameters is scaled by a further temperatureparameter ?
:P ?
(e, a|f) ?
P (e, a|f)?
(7)where for ?
= 1 the random samples used in this pa-per are obtained, for ?
tending to infinite the distri-bution becomes peaked around the single best path,thus producing samples similar to N-best lists, andsamples from other real values of the temperaturecan be combined.Compared to the method using the whole lat-tice, the proposed approaches have a substantiallylower computational complexity under very broadand common assumptions, and yet yield transla-tion quality improvements of comparable magnitudeover the baseline N-best list method.While the method presented in this paper oper-ates on the translation lattices generated by Phrase-Based SMT decoders, the extension to translation613forests generated by hierarchical decoders (Chiang,2007) seems straightforward.
In that case, the back-ward sweep for propagating unnormalized posteriorprobabilities is replaced by a bottom-up sweep, andthe sampling now concerns (binary) trees instead ofpaths, but the rest of the procedure is substantiallyunchanged.
We conjecture however that the exten-sion to translation forests would be less competitivecompared to working with the whole packed forest(as in (Kumar et al, 2009)) than lattice sampling iscompared to working with the whole lattice.
Thereason we believe this is that hierarchical modelslead to much more spurious ambiguity than phrase-based models, so that both the N-best method andthe sampling method explore a smaller portion of thecandidate space compared to the compact represen-tation of all the candidate translations in a beam.AcknowledgementsWe would like to thank Vassilina Nikoulina, GregHanneman, Marc Dymetman for useful discussionsand the anonymous reviewers for their suggestionsand constructive criticism.ReferencesChris Callison-Burch, Cameron Fordyce,Philipp Koehn, Christof Monz, and JoshSchroeder.
Further meta-evaluation of ma-chine translation.
In Proceedings of theACL 2008 Workshop on Statistical Ma-chine Translation, Columbus, Ohio, 2008.http://www.statmt.org/wmt08/pdf/WMT09.pdf.Daniel Cer, Daniel Jurafsky, and Christopher D.Manning.
Regularization and search for mini-mum error rate training.
In Proceedings of theThird Workshop on Statistical Machine Transla-tion, pages 26?34, Columbus, Ohio, 2008.
ISBN978-1-932432-09-1.David Chiang.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228,2007.David Chiang, Yuval Marton, and Philip Resnik.Online large-margin training of syntactic andstructural translation features.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing (EMNLP ?08), pages 224?233, Honolulu, Hawaii, 2008.Koby Crammer and Yoram Singer.
Ultracon-servative online algorithms for multiclass prob-lems.
Journal of Machine Learning Research(JMLR), 3:951?991, 2003.
ISSN 1532-4435. doi:http://dx.doi.org/10.1162/jmlr.2003.3.4-5.951.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
Onlinepassive-aggressive algorithms.
Journal of Ma-chine Learning Research (JMLR), 7:551?585,2006.
ISSN 1532-4435.Kevin Knight.
Decoding complexity in word-replacement translation modals.
ComputationalLinguistics, Squibs and Discussion, 25(4),, 1999.Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Chris Dyer, Ondrej Bojar,Alexandra Constantin, and Evan Herbst.
Moses:Open source toolkit for statistical machine trans-lation.
In Proceedings of the Annual Meetingof the Association for Computationl Linguistics(ACL ?07), pages 177?180, prague, Czech repub-lic, 2007.Shankar Kumar, Wolfgang Macherey, Chris Dyer,and Franz Och.
Efficient minimum error ratetraining and minimum bayes-risk decoding fortranslation hypergraphs and lattices.
In Proceed-ings of the Joint 47th Annual Meeting of theACL and the 4th International Joint Conferenceon Natural Language Processing of the AFNLP,pages 163?171, Suntec, Singapore, August 2009.Wolfgang Macherey, Franz Josef Och, IgnacioThayer, and Jakob Uszkoreit.
Lattice-based min-imum error rate training for statistical machinetranslation.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP ?08), pages 725?734, Honolulu,Hawaii, 2008.Franz Josef Och.
Minimum error rate trainingin statistical machine translation.
In Proceed-ings of the 41st Annual Meeting on Associa-tion for Computational Linguistics (ACL ?03),pages 160?167, Sapporo, Japan, 2003. doi:http://dx.doi.org/10.3115/1075096.1075117.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
BLEU: a method for automatic614evaluation of machine translation.
In Proceed-ings of the 40th Annual Meeting on Associationfor Computational Linguistics (ACL ?02), pages311?318, Philadelphia, Pennsylvania, 2002. doi:http://dx.doi.org/10.3115/1073083.1073135.William H. Press.
Numerical recipes : the artof scientific computing.
Cambridge UniversityPress, third edition, September 2007.
ISBN0521880688.Stefan Riezler and John T. Maxwell.
On some pit-falls in automatic evaluation and significance test-ing for MT.
In Proceedings of the ACL Workshopon Intrinsic and Extrinsic Evaluation Measuresfor Machine Translation and/or Summarization,pages 57?64, Ann Arbor, Michigan, June 2005.David A. Smith and Jason Eisner.
Minimum riskannealing for training log-linear models.
In Pro-ceedings of the Joint International Conferenceon Computational Linguistics and Annual meet-ing of the Association for Computational Linguis-tics (COLING/ACL ?06), pages 787?794, Sydney,Australia, 2006.Taro Watanabe, Jun Suzuki, Hajime Tsukada, andHideki Isozaki.
Online large-margin training forstatistical machine translation.
In Proceedings ofthe 2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 764?773, Prague, Czech Repub-lic, June 2007.Richard Zens, Sasa Hasan, and Hermann Ney.
Asystematic comparison of training criteria for sta-tistical machine translation.
In Proceedings ofthe 2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 524?532, Prague, Czech Repub-lic, June 2007.615
