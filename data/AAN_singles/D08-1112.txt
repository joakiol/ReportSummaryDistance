Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1070?1079,Honolulu, October 2008. c?2008 Association for Computational LinguisticsAn Analysis of Active Learning Strategies for Sequence Labeling TasksBurr Settles???Dept.
of Computer SciencesUniversity of WisconsinMadison, WI 53706, USAbsettles@cs.wisc.eduMark Craven???Dept.
of Biostatistics & Medical InformaticsUniversity of WisconsinMadison, WI 53706, USAcraven@biostat.wisc.eduAbstractActive learning is well-suited to many prob-lems in natural language processing, whereunlabeled data may be abundant but annota-tion is slow and expensive.
This paper aimsto shed light on the best active learning ap-proaches for sequence labeling tasks such asinformation extraction and document segmen-tation.
We survey previously used query selec-tion strategies for sequence models, and pro-pose several novel algorithms to address theirshortcomings.
We also conduct a large-scaleempirical comparison using multiple corpora,which demonstrates that our proposed meth-ods advance the state of the art.1 IntroductionTraditional supervised learning algorithms usewhatever labeled data is provided to induce a model.By contrast, active learning gives the learner a de-gree of control by allowing it to select which in-stances are labeled and added to the training set.
Atypical active learner begins with a small labeled setL, selects one or more informative query instancesfrom a large unlabeled pool U , learns from these la-beled queries (which are then added to L), and re-peats.
In this way, the learner aims to achieve highaccuracy with as little labeling effort as possible.Thus, active learning can be valuable in domainswhere unlabeled data are readily available, but ob-taining training labels is expensive.Such is the case with many sequence labelingtasks in natural language domains.
For example,part-of-speech tagging (Seung et al, 1992; Laffertyet al, 2001), information extraction (Scheffer et al,2001; Sang and DeMeulder, 2003; Kim et al, 2004),and document segmentation (Carvalho and Cohen,2004) are all typically treated as sequence labelingproblems.
The source data for these tasks (i.e., textdocuments in electronic form) are often easily ob-tained.
However, due to the nature of sequence la-beling tasks, annotating these texts can be rather te-dious and time-consuming, making active learningan attractive technique.While there has been much work on active learn-ing for classification (Cohn et al, 1994; McCallumand Nigam, 1998; Zhang and Oles, 2000; Zhu etal., 2003), active learning for sequence labeling hasreceived considerably less attention.
A few meth-ods have been proposed, based mostly on the con-ventions of uncertainty sampling, where the learnerqueries the instance about which it has the least cer-tainty (Scheffer et al, 2001; Culotta and McCallum,2005; Kim et al, 2006), or query-by-committee,where a ?committee?
of models selects the instanceabout which its members most disagree (Dagan andEngelson, 1995).
We provide more detail on theseand the new strategies we propose in Section 3.The comparative effectiveness of these ap-proaches, however, has not been studied.
Further-more, it has been suggested that uncertainty sam-pling and query-by-committee fail on occasion (Royand McCallum, 2001; Zhu et al, 2003) by query-ing outliers, e.g., instances considered informativein isolation by the learner, but containing little infor-mation about the rest of the distribution of instances.Proposed methods for dealing with these shortcom-ings have so far only considered classification tasks.1070This paper presents two major contributions foractive learning and sequence labeling tasks.
First,we motivate and introduce several new query strate-gies for probabilistic sequence models.
Second, weconduct a thorough empirical analysis of previouslyproposed methods with our algorithms on a varietyof benchmark corpora.
The remainder of this pa-per is organized as follows.
Section 2 provides abrief introduction to sequence labeling and condi-tional random fields (the sequence model used inour experiments).
Section 3 describes in detail allthe query selection strategies we consider.
Section 4presents the results of our empirical study.
Section 5concludes with a summary of our findings.2 Sequence Labeling and CRFsIn this paper, we are concerned with active learn-ing for sequence labeling.
Figure 1 illustrateshow, for example, an information extraction prob-lem can be viewed as a sequence labeling task.Let x = ?x1, .
.
.
, xT ?
be an observation sequenceof length T with a corresponding label sequencey = ?y1, .
.
.
, yT ?.
Words in a sentence corre-spond to tokens in the input sequence x, which aremapped to labels in y.
These labels indicate whetherthe word belongs to a particular entity class of inter-est (in this case, org and loc) or not (null).
Theselabels can be assigned by a sequence model basedon a finite state machine, such as the one shown tothe right in Figure 1.We focus our discussion of active learning forsequence labeling on conditional random fields, orCRFs (Lafferty et al, 2001).
The rest of this sec-tion serves as a brief introduction.
CRFs are sta-tistical graphical models which have demonstratedstate-of-the-art accuracy on virtually all of the se-quence labeling tasks mentioned in Section 1.
Weuse linear-chain CRFs, which correspond to condi-tionally trained probabilistic finite state machines.A linear-chain CRF model with parameters ?
de-fines the posterior probability of y given x to be1:P (y|x; ?)
=1Z(x)exp(T?t=1K?k=1?kfk(yt?1, yt,xt)).
(1)1Our discussion assumes, without loss of generality, thateach label is uniquely represented by one state, thus each labelsequence y corresponds to exactly one path through the model.locorgnullx:y:...thenullACMEorgInc.orgofficesnullinnullChicagolocFigure 1: An information extraction example treated asa sequence labeling task.
Also shown is a correspondingsequence model represented as a finite state machine.Here Z(x) is a normalization factor over all pos-sible labelings of x, and ?k is one of K modelparameter weights corresponding to some featurefk(yt?1, yt,xt).
Each feature fk describes the se-quence x at position t with label yt, observed alonga transition from label states yt?1 to yt in the finitestate machine.
Consider the example text from Fig-ure 1.
Here, fk might be the feature WORD=ACMEand have the value fk = 1 along a transition fromthe null state to the org state (and 0 elsewhere).Other features set to 1 here might be ALLCAPS andNEXTWORD=Inc.
The weights in ?
are set to max-imize the conditional log likelihood ` of training se-quences in the labeled data set L:`(L; ?)
=L?l=1logP (y(l)|x(l); ?)
?K?k=1?2k2?2,where L is the size of the labeled set L, and the sec-ond term is a Gaussian regularization penalty on ??
?to prevent over-fitting.
After training, labels can bepredicted for new sequences using the Viterbi algo-rithm.
For more details on CRFs and their trainingprocedures, see Sutton and McCallum (2006).Note that, while we describe the active learningalgorithms in the next section in terms of linear-chain CRFs, they have analogs for other kinds ofsequence models, such as hidden Markov models,or HMMs (Rabiner, 1989), probabilistic context-free grammars (Lari and Young, 1990), and generalCRFs (Sutton and McCallum, 2006).3 Active Learning with Sequence ModelsIn order to select queries, an active learner must havea way of assessing how informative each instance is.Let x?
be the most informative instance according tosome query strategy ?
(x), which is a function usedto evaluate each instance x in the unlabeled pool U .1071Given: Labeled set L, unlabeled pool U , querystrategy ?(?
), query batch size Brepeat// learn a model using the current L?
= train(L) ;for b = 1 to B do// query the most informative instancex?b = argmaxx?U ?
(x) ;// move the labeled query from U to LL = L ?
?x?b , label(x?b)?
;U = U ?
x?b ;enduntil some stopping criterion ;Algorithm 1: Pool-based active learning.Algorithm 1 provides a sketch of the generic pool-based active learning scenario.In the remainder of this section, we describe var-ious query strategy formulations of ?(?)
that havebeen used for active learning with sequence mod-els.
We also point out where we think these ap-proaches may be flawed, and propose several novelquery strategies to address these issues.3.1 Uncertainty SamplingOne of the most common general frameworks formeasuring informativeness is uncertainty sampling(Lewis and Catlett, 1994), where a learner queriesthe instance that it is most uncertain how to la-bel.
Culotta and McCallum (2005) employ a sim-ple uncertainty-based strategy for sequence modelscalled least confidence (LC):?LC(x) = 1 ?
P (y?|x; ?
).Here, y?
is the most likely label sequence, i.e., theViterbi parse.
This approach queries the instancefor which the current model has the least confidencein its most likely labeling.
For CRFs, this confi-dence can be calculated using the posterior proba-bility given by Equation (1).Scheffer et al (2001) propose another uncertaintystrategy, which queries the instance with the smallestmargin between the posteriors for its two most likelylabelings.
We call this approach margin (M):?M (x) = ?
(P (y?1|x; ?)
?
P (y?2|x; ?
)).Here, y?1 and y?2 are the first and second best la-bel sequences, respectively.
These can be efficientlycomputed using the N -best algorithm (Schwartzand Chow, 1990), a beam-search generalization ofViterbi, with N = 2.
The minus sign in front is sim-ply to ensure that ?M acts as a maximizer for usewith Algorithm 1.Another uncertainty-based measure of informa-tiveness is entropy (Shannon, 1948).
For a dis-crete random variable Y , the entropy is given byH(Y ) = ?
?i P (yi) logP (yi), and represents theinformation needed to ?encode?
the distribution ofoutcomes for Y .
As such, is it often thought of asa measure of uncertainty in machine learning.
Inactive learning, we wish to use the entropy of ourmodel?s posteriors over its labelings.
One way thishas been done with probabilistic sequence models isby computing what we call token entropy (TE):?TE(x) = ?1TT?t=1M?m=1P?
(yt = m) logP?
(yt = m),(2)where T is the length of x, m ranges over all pos-sible token labels, and P?
(yt = m) is shorthandfor the marginal probability that m is the label atposition t in the sequence, according to the model.For CRFs and HMMs, these marginals can be effi-ciently computed using the forward and backwardalgorithms (Rabiner, 1989).
The summed token en-tropies have typically been normalized by sequencelength T , to avoid simply querying longer sequences(Baldridge and Osborne, 2004; Hwa, 2004).
How-ever, we argue that querying long sequences shouldnot be explicitly discouraged, if in fact they containmore information.
Thus, we also propose the totaltoken entropy (TTE) measure:?TTE(x) = T ?
?TE(x).For most sequence labeling tasks, however, it ismore appropriate to consider the entropy of the la-bel sequence y as a whole, rather than some aggre-gate of individual token entropies.
Thus an alternatequery strategy is sequence entropy (SE):?SE(x) = ?
?y?P (y?|x; ?)
logP (y?|x; ?
), (3)where y?
ranges over all possible label sequences forinput sequence x.
Note, however, that the number1072of possible labelings grows exponentially with thelength of x.
To make this feasible, previous work(Kim et al, 2006) has employed an approximationwe call N-best sequence entropy (NSE):?NSE(x) = ??y?
?NP (y?|x; ?)
logP (y?|x; ?
),where N = {y?1, .
.
.
,y?N}, the set of the N mostlikely parses, and the posteriors are re-normalized(i.e., Z(x) in Equation (1) only ranges over N ).
ForN = 2, this approximation is equivalent to ?M , thusN -best sequence entropy can be thought of as a gen-eralization of the margin approach.Recently, an efficient entropy calculation via dy-namic programming was proposed for CRFs in thecontext of semi-supervised learning (Mann and Mc-Callum, 2007).
We use this algorithm to computethe true sequence entropy (3) for active learning ina constant-time factor of Viterbi?s complexity.
Hwa(2004) employed a similar approach for active learn-ing with probabilistic context-free grammars.3.2 Query-By-CommitteeAnother general active learning framework is thequery-by-committee (QBC) approach (Seung et al,1992).
In this setting, we use a committee of modelsC = {?
(1), .
.
.
, ?
(C)} to represent C different hy-potheses that are consistent with the labeled set L.The most informative query, then, is the instanceover which the committee is in most disagreementabout how to label.In particular, we use the query-by-bagging ap-proach (Abe and Mamitsuka, 1998) to learn a com-mittee of CRFs.
In each round of active learning,L is sampled (with replacement) L times to createa unique, modified labeled set L(c).
Each model?
(c) ?
C is then trained using its own correspondinglabeled set L(c).
To measure disagreement amongcommittee members, we consider two alternatives.Dagan and Engelson (1995) introduced QBC withHMMs for part-of-speech tagging using a measurecalled vote entropy (VE):?V E(x) = ?1TT?t=1M?m=1V (yt,m)ClogV (yt,m)C,where V (yt,m) is the number of ?votes?
labelm re-ceives from all the committee member?s Viterbi la-belings at sequence position t.McCallum and Nigam (1998) propose a QBCstrategy for classification based on Kullback-Leibler(KL) divergence, an information-theoretic measureof the difference between two probability distribu-tions.
The most informative query is considered tobe the one with the largest average KL divergencebetween a committee member?s posterior label dis-tribution and the consensus.
We modify this ap-proach for sequence models by summing the averageKL scores using the marginals at each token positionand, as with vote entropy, normalizing for length.We call this approach Kullback-Leibler (KL):?KL(x) =1TT?t=11CC?c=1D(?
(c)?C),where (using shorthand again):D(?
(c)?C) =M?m=1P?
(c)(yt = m) logP?
(c)(yt = m)PC(yt = m).Here PC(yt = m) = 1C?Cc=1 P?
(c)(yt = m), or the?consensus?
marginal probability that m is the labelat position t in the sequence.Both of these disagreement measures are normal-ized for sequence length T .
As with token en-tropy (2), this may bias the learner toward query-ing shorter sequences.
To study the effects of nor-malization, we also conduct experiments with non-normalized variants ?TV E and ?TKL.Additionally, we argue that these token-level dis-agreement measures may be less appropriate formost tasks than measuring the committee?s disagree-ment about the label sequence y as a whole.
There-fore, we propose sequence vote entropy (SVE):?SV E(x) = ??y?
?NCP (y?|x; C) logP (y?|x; C),where N C is the union of the N -best parses fromall models in the committee C, and P (y?|x; C) =1C?Cc=1 P (y?|x; ?
(c)), or the ?consensus?
posteriorprobability for some label sequence y?.
This can bethought of as a QBC generalization of N -best en-tropy, where each committee member casts a votefor the posterior label distribution.
We also explorea sequence Kullback-Leibler (SKL) variant:?SKL(x) =1CC?c=1?y?
?NCP (y?|x; ?
(c)) logP (y?|x; ?
(c))P (y?|x; C).10733.3 Expected Gradient LengthA third general active learning framework we con-sider is to query the instance that would impart thegreatest change to the current model if we knew itslabel.
Since we train discriminative models likeCRFs using gradient-based optimization, this in-volves querying the instance which, if labeled andadded to the training set, would create the greatestchange in the gradient of the objective function (i.e.,the largest gradient vector used to re-estimate pa-rameter values).Let ?`(L; ?)
be the gradient of the log-likelihood ` with respect to the model parameters ?,as given by Sutton and McCallum (2006).
Now let?`(L+?x,y?
; ?)
be the new gradient that would beobtained by adding the training tuple ?x,y?
to L.Since the query algorithm does not know the true la-bel sequence y in advance, we instead calculate theexpected gradient length (EGL):?EGL(x) =?y?
?NP (y?|x; ?)????`(L+?x,y??
; ?)???
,approximated as an expectation over the N -best la-belings, where ?
?
?
is the Euclidean norm of eachresulting gradient vector.
We first introduced this ap-proach in previous work on multiple-instance activelearning (Settles et al, 2008), and adapt it to queryselection with sequences here.
Note that, at querytime, ?`(L; ?)
should be nearly zero since ` con-verged at the previous round of training.
Thus, wecan approximate ?`(L+?x,y??
; ?)
?
?`(?x, y??
; ?
)for computational efficiency, because the training in-stances are assumed to be independent.3.4 Information DensityIt has been suggested that uncertainty sampling andQBC are prone to querying outliers (Roy and Mc-Callum, 2001; Zhu et al, 2003).
Figure 2 illus-trates this problem for a binary linear classifier us-ing uncertainty sampling.
The least certain instancelies on the classification boundary, but is not ?rep-resentative?
of other instances in the distribution, soknowing its label is unlikely to improve accuracy onthe data as a whole.
QBC and EGL exhibit similarbehavior, by spending time querying possible out-liers simply because they are controversial, or areexpected to impart significant change in the model.ABFigure 2: An illustration of when uncertainty samplingcan be a poor strategy for classification.
Shaded poly-gons represent labeled instances (L), and circles repre-sent unlabeled instances (U).
Since A is on the decisionboundary, it will be queried as the most uncertain.
How-ever, querying B is likely to result in more informationabout the data as a whole.We argue that this phenomenon can occur with se-quence labeling tasks as well as with classification.To address this, we propose a new active learningapproach called information density (ID):?ID(x) = ?SE(x) ?
(1UU?u=1sim(x,x(u)))?.That is, the informativeness of x is weighted by itsaverage similarity to all other sequences in U , sub-ject to a parameter ?
that controls the relative im-portance of the density term.
In the formulation pre-sented above, sequence entropy ?SE measures the?base?
informativeness, but we could just as easilyuse any of the instance-level strategies presented inthe previous sections.This density measure requires us to compute thesimilarity of two sequences.
To do this, we firsttransform each x, which is a sequence of featurevectors (tokens), into a single kernel vector ~x:~x =[T?t=1f1(xt), .
.
.
,T?t=1fJ(xt)],where fj(xt) is the value of feature fj for token xt,and J is the number of features in the input represen-tation2.
In other words, sequence x is compressedinto a fixed-length feature vector ~x, for which eachelement is the sum of the corresponding feature?svalues across all tokens.
We can then use cosine2Note that J 6= K, and fj(xt) here differs slightly from thefeature definition given in Section 2.
Since the labels yt?1 andyt are unknown before querying, the K features used for modeltraining are reduced down to the J input features here, whichfactor out any label dependencies.1074similarity on this simplified representation:simcos(x,x(u)) =~x ?
~x(u)?~x?
?
?~x(u)?.We have also investigated similarity functionsbased on exponentiated Euclidean distance and KL-divergence, the latter of which was also employed byMcCallum and Nigam (1998) for density-weightingQBC in text classification.
However, these measuresshow no improvement over cosine similarity, and re-quire setting additional hyper-parameters.One potential drawback of information density isthat the number of required similarity calculationsgrows quadratically with the number of instancesin U .
For pool-based active learning, we often as-sume that the size of U is very large.
However,these densities only need to be computed once, andare independent of the base information measure.Thus, when employing information density in a real-world interactive learning setting, the density scorescan simply be pre-computed and cached for efficientlookup during the actual active learning process.3.5 Fisher InformationWe also introduce a query selection strategy for se-quence models based on Fisher information, build-ing on the theoretical framework of Zhang and Oles(2000).
Fisher information I(?)
represents the over-all uncertainty about the estimated model parame-ters ?, as given by:I(?)
= ?
?xP (x)?yP (y|x; ?)?2?
?2logP (y|x; ?
).For a model with K parameters, the Fisher infor-mation takes the form of a K ?
K covariance ma-trix.
Our goal in active learning is to select the querythat most efficiently minimizes the model variancereflected in I(?).
This can be accomplished by op-timizing the Fisher information ratio (FIR):?FIR(x) = ?tr(Ix(?
)?1IU (?
)), (4)where Ix(?)
and IU (?)
are Fisher information ma-trices for sequence x and the unlabeled pool U , re-spectively.
The leading minus sign again ensuresthat ?FIR is a maximizer for use with Algorithm 1.Previously, Fisher information for active learninghas only been investigated in the context of simplebinary classification.
When employing FIR with se-quence models like CRFs, there are two additionalcomputational challenges.
First, we must integrateover all possible labelings y, which can, as we haveseen, be approximated as an expectation over theN -best labelings.
Second, the inner product in the ratiocalculation (4) requires inverting a K ?
K matrixfor each x.
In most interesting natural language ap-plications, K is very large, making this algorithmintractable.
However, it is common in similar situ-ations to approximate the Fisher information matrixwith its diagonal (Nyffenegger et al, 2006).
Thuswe estimate Ix(?)
using:Ix(?)
=?y?
?NP (y?|x; ?)[(?
logP (y?|x; ?)?
?1)2+ ?, .
.
.
,(?
logP (y?|x; ?)?
?K)2+ ?
],and IU (?)
using:IU (?)
=1UU?u=1Ix(u)(?
).For CRFs, the partial derivative at the root of eachelement in the diagonal vector is given by:?
logP (y?|x; ?)?
?k=T?t=1fk(y?t?1, y?t,xt)?T?t=1?y,y?P (y, y?|x)fk(y, y?,xt),which is similar to the equation used to compute thetraining gradient, but without a regularization term.A smoothing parameter ?
1 is added to preventdivision by zero when computing the ratio.Notice that this method implicitly selects repre-sentative instances by favoring queries with Fisherinformation Ix(?)
that is not only high, but similarto that of the overall data distribution IU (?).
Thisis in contrast to information density, which tries toquery representative instances by explicitly model-ing the distribution with a density weight.1075Corpus Entities Features InstancesCoNLL-03 4 78,644 19,959NLPBA 5 128,401 18,854BioCreative 1 175,331 10,000FlySlip 1 31,353 1,220CORA:Headers 15 22,077 935CORA:References 13 4,208 500Sig+Reply 2 25 617SigIE 12 10,600 250Table 1: Properties of the different evaluation corpora.4 Empirical EvaluationIn this section we present a large-scale empiricalanalysis of the query strategies described in Sec-tion 3 on eight benchmark information extractionand document segmentation corpora.
The data setsare summarized in Table 1.4.1 Data and MethodologyCoNLL-03 (Sang and DeMeulder, 2003) is a col-lection of newswire articles annotated with four en-tities: person, organization, location, and misc.NLPBA (Kim et al, 2004) is a large collectionof biomedical abstracts annotated with five entitiesof interest, such as protein, RNA, and cell-type.BioCreative (Yeh et al, 2005) and FlySlip (Vla-chos, 2007) also comprise texts in the biomedicaldomain, annotated for gene entity mentions in arti-cles from the human and fruit fly literature, respec-tively.
CORA (Peng and McCallum, 2004) consistsof two collections: a set of research paper headersannotated for entities such as title, author, and insti-tution; and a collection of references annotated withBibTeX fields such as journal, year, and publisher.The Sig+Reply corpus (Carvalho and Cohen, 2004)is a set of email messages annotated for signatureand quoted reply line segments.
SigIE is a subset ofthe signature blocks from Sig+Reply which we haveenhanced with several address book fields such asname, email, and phone.
All corpora are format-ted in the ?IOB?
sequence representation (Ramshawand Marcus, 1995).We implement all fifteen query selection strate-gies described in Section 3 for use with CRFs, andevaluate them on all eight data sets.
We also com-pare against two baseline strategies: random in-stance selection (i.e., passive learning), and na?
?velyquerying the longest sequence in terms of tokens.We use a typical feature set for each corpus based onthe cited literature (including words, orthographicpatterns, part-of-speech, lexicons, etc.).
Where theN -best approximation is used N = 15, and for allQBC methods C = 3; these figures exhibited a goodbalance of accuracy and training speed in prelimi-nary work.
For information density, we arbitrarilyset ?
= 1 (i.e., the information and density termshave equal weight).
In each experiment, L is ini-tialized with five random labeled instances, and upto 150 queries are subsequently selected from U inbatches of size B = 5.
All results are averagedacross five folds using cross-validation.We evaluate each query strategy by constructinglearning curves that plot the overall F1 measure (forall entities or segments) as a function of the num-ber of instances queried.
Due to lack of space, wecannot show learning curves for every experiment.Instead, Table 2 summarizes our results by reportingthe area under the learning curve for all strategieson all data.
Figure 3 presents a few representativelearning curves for six of the corpora.4.2 Discussion of Learning CurvesThe first conclusion we can draw from these resultsis that there is no single clear winner.
However, in-formation density (ID), which we introduce in thispaper, stands out.
It usually improves upon the basesequence entropy measure, never performs poorly,and has the highest average area under the learningcurve across all tasks.
It seems particularly effectiveon large corpora, which is a typical assumption forthe active learning setting.
Sequence vote entropy(SVE), a QBCmethod we propose here, is also note-worthy in that it is fairly consistently among the topthree strategies, although never the best.Second, the top uncertainty sampling strategiesare least confidence (LC) and sequence entropy(SE), the latter being the dominant entropy-basedmethod.
Among the QBC strategies, sequence voteentropy (SVE) is the clear winner.
We conclude thatthese three methods are the best base informationmeasures for use with information density.Third, query strategies that evaluate the en-tire sequence (SE, SVE, SKL) are generally su-perior to those which aggregate token-level infor-mation.
Furthermore, the total token-level strate-gies (TTE, TVE, TKL) outperform their length-1076Baselines Uncertainty Sampling Query-By-Committee OtherCorpus Rand Long LC M TE TTE SE NSE VE KL TVE TKL SVE SKL EGL ID FIRCoNLL-03 78.8 79.4 89.4 84.5 38.9 89.7 90.1 89.1 45.9 62.0 86.7 81.7 89.0 87.9 87.3 89.6 81.7NLPBA 59.9 67.6 71.0 62.9 53.4 70.9 71.5 68.9 52.4 53.1 66.9 63.5 71.8 68.5 69.3 73.1 73.6BioCreative 34.6 26.9 54.8 46.8 37.8 53.0 56.0 50.5 35.2 37.4 49.2 45.1 56.6 50.8 51.5 59.1 58.8FlySlip 112.1 121.0 125.1 119.5 110.3 124.9 125.4 124.1 113.3 109.4 124.1 119.5 122.7 120.7 125.9 126.8 118.2Headers 76.0 78.2 81.4 78.6 78.5 78.5 80.8 80.4 72.8 78.5 79.7 78.5 80.7 78.4 79.6 80.2 79.1References 90.0 86.0 89.8 91.5 84.4 88.6 88.4 89.4 85.1 89.1 88.7 88.2 89.9 86.9 88.2 88.7 87.1Sig+Reply 129.1 129.6 132.1 132.3 131.7 131.6 131.4 133.1 131.4 130.7 132.1 130.6 132.8 132.3 130.5 131.5 133.2SigIE 84.3 82.7 88.8 87.3 89.3 88.3 87.6 89.1 89.8 85.5 89.7 85.1 89.5 89.7 87.7 88.5 88.5Average 83.1 83.9 91.6 87.9 78.0 90.7 91.4 90.6 78.2 80.7 89.6 86.5 91.6 89.4 90.0 92.2 90.0Table 2: Detailed results for all query strategies on all evaluation corpora.
Reported is the area under the F1 learningcurve for each strategy after 150 queries (maximum possible score is 150).
For each row, the best method is shownboxed in bold, the second best is shown underlined in bold, and the third best is shown in bold.
The last row summa-rizes the results across all eight tasks by reporting the average area for each strategy.
Query strategy formulations forsequence models introduced in this paper are indicated with italics along the top.normalized counterparts (TE, VE, KL) in nearly allcases.
In fact, the normalized variants are often in-ferior even to the baselines.
While an argument canbe made that these shorter sequences might be eas-ier to label from a human annotator?s perspective,our ongoing work indicates that the relationship be-tween instance length and actual labeling costs (e.g.,elapsed annotation time) is not a simple one.
Anal-ysis of our experiment logs also shows that length-normalized methods are occasionally biased towardshort sequences with little intuitive value (e.g., sen-tences with few or no entities to label).
In addition,vote entropy appears to be a better disagreementmeasure for QBC strategies than KL divergence.Finally, Fisher information (FIR), while theoreti-cally sound, exhibits behavior that is difficult to in-terpret.
It is sometimes the winning strategy, but oc-casionally only on par with the baselines.
When itdoes show significant gains over the other strategies,these gains appear to be only for the first severalqueries (e.g., NLPBA and BioCreative in Figure 3).This inconsistent performance may be a result of theapproximations made for computational efficiency.Expected gradient length (EGL) also appears to ex-hibit mediocre performance, and is likely not worthits additional computational expense.4.3 Discussion of Run TimesHere we discuss the execution times for each querystrategy using current hardware.
The uncertaintysampling methods are roughly comparable in runtime (token-based methods run slightly faster), eachroutinely evaluating tens of thousands of sequencesin under a minute.
The QBC methods, on the otherhand, must re-train multiple models with each query,resulting in a lag of three to four minutes per querybatch (and up to 20 minutes for corpora with moreentity labels).The expected gradient length and Fisher informa-tion methods are the most computationally expen-sive, because they must first perform inference overthe possible labelings and then calculate gradientsfor each candidate label sequence.
As a result, theytake eight to ten minutes (upwards of a half hour onthe larger corpora) for each query.
Unlike the otherstrategies, their time complexities also scale linearlywith the number of model parameters K which, inturn, increases as new sequences are added to L.As noted in Section 3.4, information density in-curs a large computational cost to estimate the den-sity weights, but these can be pre-computed andcached for efficient lookup.
In our experiments, thispre-processing step takes less than a minute for thesmaller corpora, about a half hour for CoNLL-03and BioCreative, and under two hours for NLPBA.The density lookup causes no significant change inthe run time of the base information measure.
Giventhese results, we advocate information density withan uncertainty sampling base measure in practice,particularly for active learning with large corpora.5 ConclusionIn this paper, we have presented a detailed analy-sis of active learning for sequence labeling tasks.In particular, we have described and criticized thequery selection strategies used with probabilistic se-1077F1 measureF1 measure00.10.20.30.40.50.60.70  20  40  60  80  100  120  140information density (ID)Fisher information (FIR)query-by-committee (SVE)randomCoNLL-0300.10.20.30.40.50.60.70  20  40  60  80  100  120  140information density (ID)Fisher information (FIR)query-by-committee (SVE)randomNLPBA00.10.20.30.40.50.60.70  20  40  60  80  100  120  140information density (ID)Fisher information (FIR)query-by-committee (SVE)randomBioCreativenumber of instances queried00.20.40.60.810  20  40  60  80  100  120  140information density (ID)Fisher information (FIR)query-by-committee (SVE)randomFlySlipnumber of instances queried00.20.40.60.810  20  40  60  80  100  120  140information density (ID)Fisher information (FIR)query-by-committee (SVE)randomSig+Replynumber of instances queried00.10.20.30.40.50.60.70  20  40  60  80  100  120  140information density (ID)Fisher information (FIR)query-by-committee (SVE)randomSigIEFigure 3: Learning curves for selected query strategies on six of the evaluation corpora.quence models to date, and proposed several novelstrategies to address some of their shortcomings.Our large-scale empirical evaluation demonstratesthat some of these newly proposed methods advancethe state of the art in active learning with sequencemodels.
These methods include information density(which we recommend in practice), sequence voteentropy, and sometimes Fisher information.AcknowledgmentsWe would like to thank the anonymous reviewers fortheir helpful feedback.
This work was supported byNIH grants T15-LM07359 and R01-LM07050.ReferencesN.
Abe and H. Mamitsuka.
1998.
Query learning strate-gies using boosting and bagging.
In Proceedings ofthe International Conference on Machine Learning(ICML), pages 1?9.
Morgan Kaufmann.J.
Baldridge and M. Osborne.
2004.
Active learning andthe total cost of annotation.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 9?16.
ACL Press.V.R.
Carvalho and W. Cohen.
2004.
Learning to extractsignature and reply lines from email.
In Proceedingsof the Conference on Email and Anti-Spam (CEAS).D.
Cohn, L. Atlas, and R. Ladner.
1994.
Improving gen-eralization with active learning.
Machine Learning,15(2):201?221.A.
Culotta and A. McCallum.
2005.
Reducing labelingeffort for stuctured prediction tasks.
In Proceedingsof the National Conference on Artificial Intelligence(AAAI), pages 746?751.
AAAI Press.I.
Dagan and S. Engelson.
1995.
Committee-basedsampling for training probabilistic classifiers.
In Pro-ceedings of the International Conference on MachineLearning (ICML), pages 150?157.
Morgan Kaufmann.R.
Hwa.
2004.
Sample selection for statistical parsing.Computational Linguistics, 30(3):73?77.J.
Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-lier.
2004.
Introduction to the bio-entity recognitiontask at JNLPBA.
In Proceedings of the InternationalJoint Workshop on Natural Language Processing inBiomedicine and its Applications (NLPBA), pages 70?75.S.
Kim, Y.
Song, K. Kim, J.W.
Cha, and G.G.
Lee.2006.
MMR-based active machine learning for bionamed entity recognition.
In Proceedings of HumanLanguage Technology and the North American Asso-ciation for Computational Linguistics (HLT-NAACL),pages 69?72.
ACL Press.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedingsof the International Conference on Machine Learning(ICML), pages 282?289.
Morgan Kaufmann.K.
Lari and S. J.
Young.
1990.
The estimation of stochas-tic context-free grammars using the inside-outside al-gorithm.
Computer Speech and Language, 4:35?56.D.
Lewis and J. Catlett.
1994.
Heterogeneous un-certainty sampling for supervised learning.
In Pro-1078ceedings of the International Conference on MachineLearning (ICML), pages 148?156.
Morgan Kaufmann.G.
Mann and A. McCallum.
2007.
Efficient computationof entropy gradient for semi-supervised conditionalrandom fields.
In Proceedings of the North AmericanAssociation for Computational Linguistics (NAACL),pages 109?112.
ACL Press.A.
McCallum and K. Nigam.
1998.
Employing EMin pool-based active learning for text classification.In Proceedings of the International Conference onMachine Learning (ICML), pages 359?367.
MorganKaufmann.M.
Nyffenegger, J.C. Chappelier, and E. Gaussier.
2006.Revisiting Fisher kernels for document similarities.
InProceedings of the European Conference on MachineLearning (ECML), pages 727?734.
Springer.F.
Peng and A. McCallum.
2004.
Accurate informationextraction from research papers using conditional ran-dom fields.
In Proceedings of Human Language Tech-nology and the North American Association for Com-putational Linguistics (HLT-NAACL).
ACL Press.L.
R. Rabiner.
1989.
A tutorial on hidden Markov mod-els and selected applications in speech recognition.Proceedings of the IEEE, 77(2):257?286.L.A.
Ramshaw and M.P.
Marcus.
1995.
Text chunkingusing transformation-based learning.
In Proceedingsof the ACL Workshop on Very Large Corpora.N.
Roy and A. McCallum.
2001.
Toward optimal activelearning through sampling estimation of error reduc-tion.
In Proceedings of the International Conferenceon Machine Learning (ICML), pages 441?448.
Mor-gan Kaufmann.E.F.T.K.
Sang and F. DeMeulder.
2003.
Introduction tothe CoNLL-2003 shared task: Language-independentnamed entity recognition.
In Proceedings of theConference on Natural Language Learning (CoNLL),pages 142?147.T.
Scheffer, C. Decomain, and S. Wrobel.
2001.
Ac-tive hidden Markov models for information extraction.In Proceedings of the International Conference on Ad-vances in Intelligent Data Analysis (CAIDA), pages309?318.
Springer-Verlag.R.
Schwartz and Y.-L. Chow.
1990.
The N -best algo-rithm: an efficient and exact procedure for finding theN most likely sentence hypotheses.
In Proceedingsof the International Conference on Acoustics, Speech,and Signal Processing (ICASSP), pages 81?83.
IEEEPress.B.
Settles, M. Craven, and S. Ray.
2008.
Multiple-instance active learning.
In Advances in Neural Infor-mation Processing Systems (NIPS), volume 20, pages1289?1296.
MIT Press.H.S.
Seung, M. Opper, and H. Sompolinsky.
1992.Query by committee.
In Proceedings of the ACMWorkshop on Computational Learning Theory, pages287?294.C.
E. Shannon.
1948.
A mathematical theory of com-munication.
Bell System Technical Journal, 27:379?423,623?656.C.
Sutton and A. McCallum.
2006.
An introduction toconditional random fields for relational learning.
InL.
Getoor and B. Taskar, editors, Introduction to Sta-tistical Relational Learning.
MIT Press.A.
Vlachos.
2007.
Evaluating and combining biomedicalnamed entity recognition systems.
In BioNLP 2007:Biological, translational, and clinical language pro-cessing, pages 199?206.A.
Yeh, A. Morgan, M. Colosimo, and L. Hirschman.2005.
Biocreative task 1a: gene mention finding eval-uation.
BMC Bioinformatics, 6(Suppl 1):S2.T.
Zhang and F.J. Oles.
2000.
A probability analysison the value of unlabeled data for classification prob-lems.
In Proceedings of the International ConferenceonMachine Learning (ICML), pages 1191?1198.
Mor-gan Kaufmann.X.
Zhu, J. Lafferty, and Z. Ghahramani.
2003.
Combin-ing active learning and semi-supervised learning usinggaussian fields and harmonic functions.
In Proceed-ings of the ICML Workshop on the Continuum fromLabeled to Unlabeled Data, pages 58?65.1079
