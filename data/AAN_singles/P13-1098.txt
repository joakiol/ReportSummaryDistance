Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 993?1003,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA user-centric model of voting intention from Social MediaVasileios Lampos, Daniel Preot?iuc-Pietro and Trevor CohnComputer Science DepartmentUniversity of Sheffield, UK{v.lampos,d.preotiuc,t.cohn}@dcs.shef.ac.ukAbstractSocial Media contain a multitude of useropinions which can be used to predict real-world phenomena in many domains in-cluding politics, finance and health.
Mostexisting methods treat these problems aslinear regression, learning to relate wordfrequencies and other simple features toa known response variable (e.g., votingintention polls or financial indicators).These techniques require very careful fil-tering of the input texts, as most SocialMedia posts are irrelevant to the task.
Inthis paper, we present a novel approachwhich performs high quality filtering au-tomatically, through modelling not justwords but also users, framed as a bilin-ear model with a sparse regulariser.
Wealso consider the problem of modellinggroups of related output variables, us-ing a structured multi-task regularisationmethod.
Our experiments on voting inten-tion prediction demonstrate strong perfor-mance over large-scale input from Twitteron two distinct case studies, outperform-ing competitive baselines.1 IntroductionWeb Social Media platforms have ushered a newera in human interaction and communication.
Themain by-product of this activity is vast amounts ofuser-generated content, a type of information thathas already attracted the interest of both marke-teers and scientists because it offers ?
for the firsttime at a large-scale ?
unmediated access to peo-ples?
observations and opinions.One exciting avenue of research concentrateson mining interesting signals automatically fromthis stream of text input.
For example, by exploit-ing Twitter posts, it is possible to infer time seriesthat correlate with financial indicators (Bollen etal., 2011), track infectious diseases (Lampos andCristianini, 2010; Lampos et al, 2010; Paul andDredze, 2011) and, in general, nowcast the magni-tude of events emerging in real-life (Sakaki et al,2010; Lampos and Cristianini, 2012).
Other stud-ies suggest ways for modelling opinions encap-sulated in this content in order to forge brandingstrategies (Jansen et al, 2009) or understand vari-ous socio-political trends (Tumasjan et al, 2010;O?Connor et al, 2010; Lansdall-Welfare et al,2012).
The main theme of the aforementionedworks is linear regression between word frequen-cies and a real-world quantity.
They also tend toincorporate hand-crafted lists of search terms tofilter irrelevant content and use sentiment analy-sis lexicons for extracting opinion bias.
Conse-quently, they are quite often restricted to a specificapplication and therefore, generalise poorly to newdata sets (Gayo-Avello et al, 2011).In this paper, we propose a generic method thataims to be independent of the characteristics de-scribed above (use of search terms or sentimentanalysis tools).
Our approach is able to explorenot only word frequencies, but also the space ofusers by introducing a bilinear formulation forthis learning task.
Regularised regression on bothspaces allows for an automatic selection of themost important terms and users, performing at thesame time an improved noise filtering.
In addi-tion, more advanced regularisation functions en-able multi-task learning schemes that can exploitshared structure in the feature space.
The latterproperty becomes very useful in multi-output re-gression scenarios, where selected features are ex-pected to have correlated as well as anti-correlatedimpact on each output (e.g., when inferring votingintentions for competing political parties).We evaluate our methods on the domain ofpolitics using data from the microblogging ser-vice of Twitter to infer voting trends.
Our pro-993posed framework is able to successfully predictvoting intentions for the top-3 and top-4 partiesin the United Kingdom (UK) and Austria respec-tively.
In both case studies ?
bound by differ-ent characteristics (including language, time-spanand number of users) ?
the average prediction er-ror is smaller than 1.5% for our best model usingmulti-task learning.
Finally, our qualitative analy-sis shows that the models uncover interesting andsemantically interpretable insights from the data.2 DataFor the evaluation of the proposed methodologieswe have created two data sets of Social Media con-tent with different characteristics based in the UKand Austria respectively.
They are used for per-forming regression aiming to infer voting intentionpolls in those countries.
Data processing is per-formed using the TrendMiner architecture for So-cial Media analysis (Preot?iuc-Pietro et al, 2012).2.1 Tweets from users in the UKThe first data set (we refer to it as Cuk) used inour experimental process consists of approx.
60million tweets produced by approx.
42K UK Twit-ter users from 30/04/2010 to 13/02/2012.
We as-sumed each user to be from the UK, if the locationfield in their profile matched with a list of com-mon UK locations and their time zone was set toG.M.T.
In this way, we were able to extract hun-dreds of thousands of UK users, from which wesub-sampled 42K users to be distributed across theUK geographical regions proportionally to theirpopulation figures.12.2 Tweets for AustriaThe second data set (Cau) is shorter in terms ofthe number of users involved (1.1K), its time span(25/01 to 01/12/2012) and, consequently, of thetotal number of tweets considered (800K).
How-ever, this time the selection of users has been madeby Austrian political experts who decided whichaccounts to monitor by subjectively assessing thevalue of information they may provide towardspolitical-oriented topics.
Still, we assume that thedifferent users will produce information of varyingquality, and some should be eliminated entirely.However, we emphasise that there may be smaller1Data collection was performed using Twitter API,http://dev.twitter.com/, to extract all posts for ourtarget users.5 30 55 80 105 130 155 180 205 230051015202530354045VotingIntention %TimeCONLABLIB(a) 240 voting intention polls for the 3 major partiesin the UK (April 2010 to February 2012)5 20 35 50 65 80 95051015202530VotingIntention %TimeSP??VPFP?GR?
(b) 98 voting intention polls for the 4 major parties inAustria (January to December 2012)Figure 1: Voting intention polls for the UK andAustria.potential gains from user modelling compared tothe UK case study.
Another important distinctionis language, which for this data set is primarilyGerman with some English.2.3 Ground TruthThe ground truth for training and evaluating ourregression models is formed by voting intentionpolls from YouGov (UK) and a collection of Aus-trian pollsters2 ?
as none performed high fre-quency polling ?
for the Austrian case study.We focused on the three major parties in theUK, namely Conservatives (CON), Labour (LAB)and Liberal Democrats (LBD) and the four ma-jor parties in Austria, namely the Social Demo-cratic Party (SPO?
), People?s Party (O?VP), Free-dom Party (FPO?)
and the Green Alternative Party(GRU?).
Matching with the time spans of the datasets described in the previous sections, we haveacquired 240 unique polls for the UK and 65 pollsfor Austria.
The latter have been expanded to98 polls by replicating the poll of day i for day2Wikipedia, http://de.wikipedia.org/wiki/Nationalratswahl_in_\%D6sterreich_2013.994i ?
1 where possible.3 There exists some inter-esting variability towards the end for the UK polls(Fig.
1a), whereas for the Austrian case, the mainchanging point is between the second and the thirdparty (Fig.
1b).3 MethodsThe textual content posted on Social Media plat-forms unarguably contains valuable information,but quite often it is hidden under vast amounts ofunstructured user generated input.
In this section,we propose a set of methods that build on one an-other, which aim to filter the non desirable noiseand extract the most informative features not onlybased on word frequencies, but also by incorporat-ing users in this process.3.1 The bilinear modelThere exist a number of different possibilities forincorporating user information into a regressionmodel.
A simple approach is to expand the fea-ture set, such that each user?s effect on the re-sponse variable can be modelled separately.
Al-though flexible, this approach would be doomedto failure due to the sheer size of the resulting fea-ture set, and the propensity to overfit all but thelargest of training sets.
One solution is to groupusers into different types, such as journalist, politi-cian, activist, etc., but this presupposes a methodfor classification or clustering of users which is anon-trivial undertaking.
Besides, these na?
?ve ap-proaches fail to account for the fact that most usersuse similar words to express their opinions, byseparately parameterising the model for differentusers or user groups.We propose to account for individual userswhile restricting all users to share the same vocab-ulary.
This is formulated as a bilinear predictivemodel,f(X) = uTXw + ?
, (1)where X is an m ?
p matrix of user-word fre-quencies and u and w are the model parameters.Let Q ?
Rn?m?p be a tensor which captures ourtraining inputs, where n, m and p denote the con-sidered number of samples (each sample usuallyrefers to a day), terms and users respectively; Qcan simply be interpreted as n versions of X (de-noted by Qi in the remainder of the script), a dif-ferent one for each day, put together.
Each element3This has been carried out to ensure an adequate numberof training points in the experimental process.Qijk holds the frequency of term j for user k dur-ing the day i in our sample.
If a user k has postedci?k tweets during day i, and cijk ?
ci?k of themcontain a term j, then the frequency of j for thisday and user is defined as Qijk = cijkci?k .Aiming to learn sparse sets of users and termsthat are representative of the voting intention sig-nal, we formulate our optimisation task as follows:{w?,u?, ??}
= argminw,u,?n?i=1(uTQiw + ?
?
yi)2+ ?
(w, ?1) + ?
(u, ?2) ,(2)where y ?
Rn is the response variable (voting in-tention), w ?
Rm and u ?
Rp denote the termand user weights respectively, uTQiw expressesthe bilinear term, ?
?
R is a bias term and ?(?
)is a regularisation function with parameters ?1 or?2.
The first term in Eq.
2 is the standard regulari-sation loss function, namely the sum squared errorover the training instances.4In the main formulation of our bilinear model,as the regularisation function ?(?)
we use the elas-tic net (Zou and Hastie, 2005), an extension ofthe well-studied `1-norm regulariser, known as theLASSO (Tibshirani, 1996).
The `1-norm regu-larisation has found many applications in severalscientific fields as it encourages sparse solutionswhich reduce the possibility of overfitting and en-hance the interpretability of the inferred model(Hastie et al, 2009).
The elastic net applies anextra penalty on the `2-norm of the weight vector,and can resolve instability issues of LASSO whicharise when correlated predictors exist in the inputdata (Zhao and Yu, 2006).
Its regularisation func-tion ?el(?)
is defined by:?el (w, ?, ?)
= ?(1?
?2 ?w?22 + ?
?w?1), (3)where ?
> 0 and ?
?
[0, 1); setting parameter?
to its extremes transforms elastic net to ridgeregression (?
= 0) or vanilla LASSO (?
= 1).Eq.
2 can be treated as a biconvex learning task(Al-Khayyal and Falk, 1983), by observing thatfor a fixed w, learning u is a convex problem andvice versa.
Biconvex functions and possible ap-plications have been well studied in the optimi-sation literature (Quesada and Grossmann, 1995;4Note that other loss functions could be used here, suchas logistic loss for classification, or more generally bilinearvariations of Generalised Linear Models (Nelder and Wed-derburn, 1972).995Pirsiavash et al, 2009).
Their main advantage isthe ability to solve efficiently non-convex prob-lems by a repeated application of two convex pro-cesses, i.e., a form of coordinate ascent.
In ourcase, the bilinear technique makes it possible toexplore both word and user spaces, while main-taining a modest training complexity.Therefore, in our bilinear approach we dividelearning in two phases, where we learn word anduser weights respectively.
For the first phase weproduce the term-scores matrix V ?
Rn?m withelements given by:Vij =p?z=1uzQijz.
(4)V contains weighted sums of term frequenciesover all users for the considered set of days.
Theweights are held in u and are representative ofeach user.
The initial optimisation task is formu-lated as:{w?, ??}
= argminw,?
?Vw + ?
?
y?22+ ?el (w, ?1, ?1) ,(5)where we aim to learn a sparse but consistent setof weights w?
for the terms of our vocabulary.In the second phase, we are using w?
to formthe user-scores matrix D ?
Rn?p:Dik =m?z=1w?zQizk , (6)which now contains weighted sums over all termsfor the same set of days.
The optimisation taskbecomes:{u?, ??}
= argminu,?
?Du + ?
?
y?22+ ?el (u, ?2, ?2) .
(7)This process continues iteratively by insertingthe weights of the second phase back to phase one,and so on until convergence.
We cannot claim thata global optimum will be reached, but biconvexityguarantees that our global objective (Eq.
2) willdecrease in each step of this iterative process.
Inthe remainder of this paper, we refer to the methoddescribed above as Bilinear Elastic Net (BEN).3.2 Exploiting term-target or user-targetrelationshipsThe previous model assumes that the responsevariable y holds information about a single infer-ence target.
However, the task that we are ad-dressing in this paper usually implies the exis-tence of several targets, i.e., different political par-ties or politicians.
An important property, there-fore, is the ability to perform multiple output re-gression.
A simple way of adapting the model tothe multiple output scenario is by framing a sep-arate learning problem for each output, but tyingtogether some of the parameters.
Here we con-sider tying together the user weights u, to enforcethat the same set of users are relevant to all tasks,while learning different term weights.
Note thatthe converse situation, where w?s are tied and u?sare independent, can be formulated in an equiva-lent manner.Suppose that our target variable y ?
R?n refersnow to ?
political entities, y = [yT1yT2 ...yT?
]T; inthis formation the top n elements of y match tothe first political entity, the next n elements to thesecond and so on.
In the first phase of the bilin-ear model, we would have to solve the followingoptimisation task:{w?, ??}
= argminw,??
?i=1?Vwi + ?i ?
yi?22+?
?i=1?el (wi, ?1, ?1) ,(8)where V is given by Eq.
4 and w?
?
R?m de-notes the vector of weights which can be slicedinto ?
sub-vectors {w?1, ...,w??}
each one repre-senting a political entity.
In the second phase,sub-vectorsw?i are used to form the input matricesDi, i ?
{1, ..., ?}
with elements given by Eq.
6.The input matrix D?
is formed by the verticalconcatenation of all Di user score matrices, i.e.,D?
=[DT1 ...
DT?
]T, and the optimisation target isequivalent to the one expressed in Eq.
7.
SinceD?
?
R?n?p, the user weight vector u?
?
Rp andthus, we are learning a single weight per user andnot one per political party as in the previous step.The method described above allows learningdifferent term weights per response variable andthen binds them under a shared set of user weights.As mentioned before, one could also try the oppo-site (i.e., start by expanding the user space); boththose models can also be optimised in an itera-tive process.
However, our experiments revealedthat those approaches did not improve on theperformance of BEN.
Still, this behaviour couldbe problem-specific, i.e., learning different words996from a shared set of users (and the opposite) maynot be a good modelling practice for the domain ofpolitics.
Nevertheless, this observation served asa motivation for the method described in the nextsection, where we extract a consistent set of wordsand users that are weighted differently among theconsidered political entities.3.3 Multi-task learning with the `1/`2regulariserAll previous models ?
even when combining allinference targets ?
were not able to explore rela-tionships across the different task domains; in ourcase, a task domain is defined by a specific politi-cal label or party.
Ideally, we would like to make asparse selection of words and users but with a reg-ulariser that promotes inter-task sharing of struc-ture, so that many features may have a positiveinfluence towards one or more parties, but nega-tive towards the remaining one(s).
It is possible toachieve this multi-task learning property by intro-ducing a different set of regularisation constraintsin the optimisation function.We perform multi-task learning using an exten-sion of group LASSO (Yuan and Lin, 2006), amethod known as `1 /` 2 regularisation (Argyriou etal., 2008; Liu et al, 2009).
Group LASSO exploitsa predefined group structure on the feature spaceand tries to achieve sparsity in the group-level, i.e.,it does not perform feature selection (unlike theelastic net), but group selection.
The `1/`2 regu-lariser extends this notion for a ?
-dimensional re-sponse variable.
The global optimisation target isnow formulated as:{W ?, U?,??}
=argminW,U,??
?t=1n?i=1(uTtQiwt + ?t ?
yti)2+ ?1m?j=1?Wj?2 + ?2p?k=1?Uk?2,(9)where the input matrix Qi is defined in the sameway as earlier, W = [w1 ... w? ]
is the term weightmatrix (each wt refers to the t-th political entityor task), equivalently U = [u1 ... u?
], Wj and Ujdenote the j-th rows of weight matrices W andU respectively, and vector ?
?
R?
holds the biasterms per task.
In this optimisation process, weaim to enforce sparsity in the feature space but ina structured manner.
Notice that we are now regu-larising the `2,1 mixed norm ofW and U , which isdefined as the sum of the row `2-norms for thosematrices.
As a result, we expect to encourage theactivation of a sparse set of features (correspond-ing to the rows of W and U ), but with nonzeroweights across the ?
tasks (Argyriou et al, 2008).Consequently, we are performing filtering (manyusers and words will have zero weights) and, at thesame time, assign weights of different magnitudeand sign on the selected features, something thatsuits a political opinion mining application, wherepro-A often means anti-B.Eq.
9 can be broken into two convex tasks (fol-lowing the same notion as in Eqs.
5 and 7), wherewe individually learn {W,?}
and then {U,?
};each step of the process is a standard linear regres-sion problem with an `1/`2 regulariser.
Again, weare able iterate this bilinear process and in eachstep convexity is guaranteed.
We refer to thismethod as Bilinear Group `1/`2 (BGL).4 ExperimentsThe proposed models are evaluated on Cuk andCau which have been introduced in Section 2.
Wemeasure predictive performance, compare it to theperformance of several competitive baselines, andprovide a qualitative analysis of the parameterslearned by the models.4.1 Data preprocessingBasic preprocessing has been applied on the vo-cabulary index of Cuk and Cau aiming to filter outsome of the word features and partially reducethe dimensionality of the problem.
Stop wordsand web links were removed in both sets, togetherwith character sequences of length <4 and <3for Cuk and Cau respectively.5 As the vocabularysize of Cuk was significantly larger, for this dataset we have additionally merged Twitter hashtags(i.e., words starting with ?#?)
with their exact nontopic word match, where possible (by dropping the?#?
when the word existed in the index).
Afterperforming the preprocessing routines describedabove, the vocabulary sizes for Cuk and Cau wereset to 80,976 and 22,917 respectively.4.2 Predictive accuracyTo evaluate the predictive accuracy of our meth-ods, we have chosen to emulate a real-life scenario5Most of the times those character sequences were notvalid words.
This pattern was different in each language andthus, a different filtering threshold was applied in each dataset.9972 4 6 8 10 12 14 16 18 20 22 24 26 28 3000.40.81.21.622.4StepGlobal ObjectiveRMSEFigure 2: Global objective function and RMSE ona validation set for BEN in 15 iterations (30 steps)of the model.of voting intention prediction.
The evaluation pro-cess starts by using a fixed set of polls matchingto consecutive time points in the past for trainingand validating the parameters of each model.
Test-ing is performed on the following ?
(unseen) pollsof the data set.
In the next step of the evaluationprocess, the training/validation set is increased bymerging it with the previously used test set (?polls), and testing is now performed on the next?
unseen polls.
In our experiments, the number ofsteps in this evaluation process is set to 10 and ineach step the size of the test set is set to ?
= 5polls.
Hence, each model is tested on 50 unseenand consecutive in time samples.
The loss func-tion in our evaluation is the standard Mean SquareError (MSE), but to allow a better interpretationof the results, we display its root (RMSE) in ta-bles and figures.6The parameters of each model (?i for BEN and?i for BEN and BGL, i ?
{1, 2}) are optimisedusing a held-out validation set by performing gridsearch.
Note that it may be tempting to adapt theregularisation parameters in each phase of the it-erative training loop, however this would changethe global objective (see Eqs.
2 and 9) and thusconvergence will not be guaranteed.
A key ques-tion is how many iterations of training are requiredto reach convergence.
Figure 2 illustrates how theBEN global objective function (Eq.
2) convergesduring this iterative process and the model?s per-formance on an unseen validation set.
Notice thatthere is a large performance improvement after thefirst step (which alone is a linear solver), but over-fitting occurs after step 11.
Based on this result,for subsequent experiments we run the trainingprocess for two iterations (4 steps), and take the6RMSE has the same metric units as the response variable.CON LAB LBD ?B?
2.272 1.663 1.136 1.69Blast 2 2.074 1.095 1.723LEN 3.845 2.912 2.445 3.067BEN 1.939 1.644 1.136 1.573BGL 1.785 1.595 1.054 1.478Table 1: UK case study ?
Average RMSEs rep-resenting the error of the inferred voting intentionpercentage for the 10-step validation process; ?denotes the mean RMSE across the three politicalparties for each baseline or inference method.SPO?
O?VP FPO?
GRU?
?B?
1.535 1.373 3.3 1.197 1.851Blast 1.148 1.556 1.639 1.536 1.47LEN 1.291 1.286 2.039 1.152 1.442BEN 1.392 1.31 2.89 1.205 1.699BGL 1.619 1.005 1.757 1.374 1.439Table 2: Austrian case study ?
Average RMSEsfor the 10-step validation process.best performing model on the held-out validationset.We compare the performance of our methodswith three baselines.
The first makes a constantprediction of the mean value of the response vari-able y in the training set (B?
); the second predictsthe last value of y (Blast); and the third baseline(LEN) is a linear regression over the terms usingelastic net regularisation.
Recalling that each testset is made of 5 polls, Blast should be consideredas a hard baseline to beat7 given that voting inten-tions tend to have a smooth behaviour.
Moreover,improving on LEN partly justifies the usefulnessof a bilinear approach compared to a linear one.Performance results comparing inferred votingintention percentages and polls for Cuk and Cau arepresented in Tables 1 and 2 respectively.
For theUK case study, both BEN and BGL are able to beatall baselines in average performance across all par-ties.
However in the Austrian case study, LENperforms better that BEN, something that could bejustified by the fact that the users in Cau were se-lected by domain experts, and consequently therewas not much gain to be had by filtering them fur-ther.
Nevertheless, the difference in performancewas rather small (approx.
0.26% error) and the in-7The last response value could be easily included as a fea-ture in the model, and would likely improve predictive perfor-mance.9985 10 15 20 25 30 35 40 450510152025303540VotingIntention %TimeCONLABLIB(a) Ground Truth (polls)5 10 15 20 25 30 35 40 450510152025303540VotingIntention %TimeCONLABLIB(b) BEN5 10 15 20 25 30 35 40 450510152025303540VotingIntention %TimeCONLABLIB(c) BGLFigure 3: UK case study ?
Voting intention infer-ence results (50 polls, 3 parties).
Sub-figure 3a isa plot of ground truth as presented in voting inten-tion polls (Fig.
1a).ferences of LEN and BEN followed a very similarpattern (??
= .94 with p < 10?10).8 Multi-tasklearning (BGL) delivered the best inference per-formance in both case studies, which was on aver-age smaller than 1.48% (RMSE).Inferences for both BEN and BGL have beenplotted on Figures 3 and 4.
They are presented ascontinuous lines of 50 inferred points (per party)which are created by concatenating the inferences8Pearson?s linear correlation averaged across the fourAustrian parties.5 10 15 20 25 30 35 40 45051015202530VotingIntention %TimeSP??VPFP?GR?
(a) Ground Truth (polls)5 10 15 20 25 30 35 40 45051015202530VotingIntention %TimeSP??VPFP?GR?
(b) BEN5 10 15 20 25 30 35 40 45051015202530VotingIntention %TimeSP??VPFP?GR?
(c) BGLFigure 4: Austrian case study ?
Voting intentioninference results (50 polls, 4 parties).
Sub-figure4a is a plot of ground truth as presented in votingintention polls (Fig.
1b).on all test sets.9 For the UK case study, one mayobserve that BEN (Fig.
3b) cannot register anychange ?
with the exception of one test point ?
inthe leading party fight (CON versus LAB); BGL(Fig.
3c) performs much better in that aspect.
Inthe Austrian case study this characteristic becomesmore obvious.
BEN (Fig.
4b) consistently predictsthe wrong ranking of O?VP and FPO?, whereas BGL(Fig.
4c) does much better.
Most importantly, a9Voting intention polls were plotted separately to allow abetter presentation.999Party Tweet Score AuthorCON PM in friendly chat with top EU mate, Sweden?s Fredrik Reinfeldt, before family photo 1.334 JournalistHave Liberal Democrats broken electoral rules?
Blog on Labour complaint to cabinetsecretary?0.991 JournalistLAB Blog Post Liverpool: City of Radicals Website now Live <link> #liverpool #art 1.954 Art FanzineI am so pleased to hear Paul Savage who worked for the Labour group has been Ap-pointed the Marketing manager for the baths hall GREAT NEWS?0.552 Politician(Labour)LBD RT @user: Must be awful for TV bosses to keep getting knocked back by all thewomen they ask to host election night (via @user)0.874 LibDem MPBlog Post Liverpool: City of Radicals 2011 ?
More Details Announced #liverpool#art?0.521 Art FanzineSPO?
Inflationsrate in O?.
im Juli leicht gesunken: von 2,2 auf 2,1%.
Teurer wurde Wohnen,Wasser, Energie.Translation: Inflation rate in Austria slightly down in July from 2,2 to 2,1%.
Accom-modation, Water, Energy more expensive.0.745 JournalistHans Rauscher zu Felix #Baumgartner ?A klaner Hitler?
<link>Translation: Hans Rauscher on Felix #Baumgartner ?A little Hitler?
<link>?1.711 JournalistO?VP #IchPirat setze mich dafu?r ein, dass eine gro?e Koalition mathematisch verhindertwird!
1.Geige: #Gruene + #FPOe + #OeVPTranslation: #IPirate am committed to prevent a grand coalition mathematically!Calling the tune: #Greens + #FPO + #OVP4.953 Userkann das buch ?res publica?
von johannes #voggenhuber wirklich empfehlen!
so zumnachdenken und so... #europa #demokratieTranslation: can really recommend the book ?res publica?
by johannes#voggenhuber!
Food for thought and so on #europe #democracy?2.323 UserFPO?
Neue Kampagne der #Krone zur #Wehrpflicht: ?GIB BELLO EINE STIMME!
?Translation: New campaign by the #Krone on #Conscription: ?GIVE WOOFY AVOICE!
?7.44 Political satireKampagne der Wiener SPO?
?zum Zusammenleben?
spielt Rechtspopulisten in dieHa?nde <link>Translation: Campaign of the Viennese SPO?
on ?Living together?
plays right into thehands of right-wing populists <link>?3.44 Human RightsGRU?
Protestsong gegen die Abschaffung des Bachelor-Studiums Internationale Entwick-lung: <link> #IEbleibt #unibrennt #uniwutTranslation: Protest songs against the closing-down of the bachelor course of Inter-national Development: <link> #IDremains #uniburns #unirage1.45 Student UnionPilz ?ich will in dieser Republik weder kriminelle Asylwerber, noch kriminelle orangePolitiker?
- BZO?-Abschiebung ok, aber wohin?
#amPunktTranslation: Pilz ?i want neither criminal asylum-seekers, nor criminal orange politi-cians in this republic?
- BZO?-Deportation OK, but where?
#amPunkt?2.172 UserTable 3: Examples of tweets amongst the ones with top positive and negative scores per party for bothCuk and Cau data sets (tweets in Austrian have been translated in English as well).
Notice that weightmagnitude may differ per case study and party as they are based on the range of the response variableand the total number of selected features.general observation is that BEN?s predictions aresmooth and do not vary significantly with time.This might be a result of overfitting the modelto a single response variable which usually hasa smooth behaviour.
On the contrary, the multi-task learning property of BGL reduces this type ofoverfitting providing more statistical evidence forthe terms and users and thus, yielding not only abetter inference performance, but also a more ac-curate model.4.3 Qualitative AnalysisIn this section, we refer to features that have beenselected and weighted as significant by our bi-linear learning functions.
Based on the weightsfor the word and the user spaces that we re-trieve after the application of BGL in the last stepof the evaluation process (see the previous sec-tion), we compute a score (weighted sum) for eachtweet in our training data sets for both Cuk andCau.
Table 3 shows examples of interesting tweetsamongst the top weighted ones (positively as wellas negatively) per party.
Together with their text(anonymised for privacy reasons) and scores, wealso provide an attribute for the author (if present).In the displayed tweets for the UK study, the onlypossible outlier is the ?Art Fanzine?
; still, it seemsto register a consistent behaviour (positive towards1000LAB, negative towards LBD) and, of course, hid-den, indirect relationships may exist between po-litical opinion and art.
The Austrian case studyrevealed even more interesting tweets since train-ing was conducted on data from a very active pre-election period (we made an effort to translatethose tweets in English language as well).
Fora better interpretation of the presented tweets, itmay be useful to know that ?Johannes Voggen-huber?
(who receives a positive comment for hisbook) and ?Peter Pilz?
(whose comment is ques-tioned) are members of GRU?, ?Krone?
(or Kro-nen Zeitung) is the major newspaper in Austria10and that FPO?
is labelled as a far right party, some-thing that may cause various reactions from ?Hu-man Rights?
organisations.5 Related WorkThe topic of political opinion mining from So-cial Media has been the focus of various recentresearch works.
Several papers have presentedmethods that aim to predict the result of an elec-tion (Tumasjan et al, 2010; Bermingham andSmeaton, 2011) or to model voting intention andother kinds of socio-political polls (O?Connor etal., 2010; Lampos, 2012).
Their common fea-ture is a methodology based on a meta-analysisof word frequencies using off-the-shelf sentimenttools such as LIWC (Pennebaker et al, 2007)or Senti-WordNet (Esuli and Sebastiani, 2006).Moreover, the proposed techniques tend to incor-porate posting volume figures as well as hand-crafted lists of words relevant to the task (e.g.,names of politicians or parties) in order to filterthe content successfully.Such papers have been criticised as their meth-ods do not generalise when applied on differentdata sets.
According to the work in (Gayo-Avelloet al, 2011), the methods presented in (Tumasjanet al, 2010) and (O?Connor et al, 2010) failed topredict the result of US congressional elections in2009.
We disagree with the arguments support-ing the statement ?you cannot predict electionswith Twitter?
(Gayo-Avello, 2012), as many timesin the past actual voting intention polls have alsofailed to predict election outcomes, but we agreethat most methods that have been proposed so farwere not entirely generic.
It is a fact that the10?Accused of abusing its near monopoly to manipulatepublic opinion in Austria?, Wikipedia, 19/02/2013, http://en.wikipedia.org/wiki/Kronen_Zeitung.majority of sentiment analysis tools are English-specific (or even American English) and, mostimportantly, political word lists (or ontologies)change in time, per country and per party; hence,generalisable methods should make an effort tolimit reliance from such tools.Furthermore, our work ?
indirectly ?
meets theguidelines proposed in (Metaxas et al, 2011) aswe have developed a framework of ?well-defined?algorithms that are ?Social Web aware?
(since thebilinear approach aims to improve noise filtering)and that have been tested on two evaluation sce-narios with distinct characteristics.6 Conclusions and Future WorkWe have presented a novel method for text regres-sion that exploits both word and user spaces bysolving a bilinear optimisation task, and an ex-tension that applies multi-task learning for multi-output inference.
Our approach performs featureselection ?
hence, noise filtering ?
on large-scaleuser-generated inputs automatically, generalisesacross two languages without manual adaptationsand delivers some significant improvements overstrong performance baselines (< 1.5% error whenpredicting polls).
The application domain in thispaper was politics, though the presented methodsare generic and could be easily applied on variousother domains, such as health or finance.Future work may investigate further modellingimprovements achieved by applying different reg-ularisation functions as well as the adaptation ofthe presented models to classification problems.Finally, in the application level, we aim at an in-depth analysis of patterns and characteristics in theextracted sets of features by collaborating with do-main experts (e.g., political analysts).AcknowledgmentsThis work was funded by the TrendMiner project(EU-FP7-ICT n.287863).
All authors would liketo thank the political analysts (and especially PaulRingler) from SORA11 for their useful insights onpolitics in Austria.11SORA ?
Institute for Social Research and Consulting,http://www.sora.at.1001ReferencesFaiz A Al-Khayyal and James E Falk.
1983.
JointlyConstrained Biconvex Programming.
Mathematicsof Operations Research, 8(2):273?286.Andreas Argyriou, Theodoros Evgeniou, and Massi-miliano Pontil.
2008.
Convex multi-task featurelearning.
Machine Learning, 73(3):243?272, Jan-uary.Adam Bermingham and Alan F Smeaton.
2011.
Onusing Twitter to monitor political sentiment and pre-dict election results.
In Proceedings of the Workshopon Sentiment Analysis where AI meets Psychology(SAAIP 2011), pages 2?10, November.Johan Bollen, Huina Mao, and Xiaojun Zeng.
2011.Twitter mood predicts the stock market.
Journal ofComputational Science, 2(1):1?8, March.Andrea Esuli and Fabrizio Sebastiani.
2006.
Sen-tiWordNet: A publicly available lexical resourcefor opinion mining.
In Proceeding of the 5thConference on Language Resources and Evaluation(LREC), pages 417?422.Daniel Gayo-Avello, Panagiotis T Metaxas, and EniMustafaraj.
2011.
Limits of Electoral Predictionsusing Twitter.
In Proceedings of the Fifth Interna-tional AAAI Conference on Weblogs and Social Me-dia (ICWSM), pages 490?493.Daniel Gayo-Avello.
2012.
No, You Cannot PredictElections with Twitter.
IEEE Internet Computing,16(6):91?94, November.Trevor Hastie, Robert Tibshirani, and Jerome Fried-man.
2009.
The Elements of Statistical Learning.Springer Series in Statistics.
Springer.Bernard J Jansen, Mimi Zhang, Kate Sobel, and Ab-dur Chowdury.
2009.
Twitter power: Tweets aselectronic word of mouth.
Journal of the Ameri-can Society for Information Science and Technology,60(11):2169?2188.Vasileios Lampos and Nello Cristianini.
2010.
Track-ing the flu pandemic by monitoring the Social Web.In 2nd IAPR Workshop on Cognitive InformationProcessing, pages 411?416.
IEEE Press.Vasileios Lampos and Nello Cristianini.
2012.
Now-casting Events from the Social Web with StatisticalLearning.
ACM Transactions on Intelligent Systemsand Technology, 3(4):1?22, September.Vasileios Lampos, Tijl De Bie, and Nello Cristianini.2010.
Flu Detector - Tracking Epidemics on Twitter.In Proceedings of European Conference on MachineLearning and Principles and Practice of KnowledgeDiscovery in Databases (ECML PKDD), pages 599?602.
Springer.Vasileios Lampos.
2012.
On voting intentions infer-ence from Twitter content: a case study on UK 2010General Election.
CoRR, April.Thomas Lansdall-Welfare, Vasileios Lampos, andNello Cristianini.
2012.
Effects of the recessionon public mood in the UK.
In Proceedings of the21st international conference companion on WorldWide Web, WWW ?12 Companion, pages 1221?1226.
ACM.Jun Liu, Shuiwang Ji, and Jieping Ye.
2009.
Multi-task feature learning via efficient l2,1-norm mini-mization.
pages 339?348, June.Panagiotis T Metaxas, Eni Mustafaraj, and DanielGayo-Avello.
2011.
How (Not) To Predict Elec-tions.
In IEEE 3rd International Conference on So-cial Computing (SocialCom), pages 165 ?
171.
IEEEPress.John A Nelder and Robert W M Wedderburn.
1972.Generalized Linear Models.
Journal of the RoyalStatistical Society - Series A (General), 135(3):370.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R Routledge, and Noah A Smith.
2010.From Tweets to Polls: Linking Text Sentiment toPublic Opinion Time Series.
In Proceedings of theInternational AAAI Conference on Weblogs andSocial Media, pages 122?129.
AAAI Press.Michael J Paul and Mark Dredze.
2011.
You Are WhatYou Tweet: Analyzing Twitter for Public Health.Proceedings of the 5th International AAAI Confer-ence on Weblogs and Social Media, pages 265?272.James W Pennebaker, Cindy K Chung, Molly Ire-land, Amy Gonzales, and Roger J Booth.
2007.The Development and Psychometric Properties ofLIWC2007.
Technical report, Universities of Texasat Austin & University of Auckland, New Zealand.Hamed Pirsiavash, Deva Ramanan, and CharlessFowlkes.
2009.
Bilinear classifiers for visual recog-nition.
In Advances in Neural Information Process-ing Systems, volume 22, pages 1482?1490.Daniel Preot?iuc-Pietro, Sina Samangooei, TrevorCohn, Nicholas Gibbins, and Mahesan Niranjan.2012.
Trendminer: An Architecture for Real TimeAnalysis of Social Media Text.
In Sixth Interna-tional AAAI Conference on Weblogs and Social Me-dia, pages 38?42.
AAAI Press, July.Ignacio Quesada and Ignacio E Grossmann.
1995.
Aglobal optimization algorithm for linear fractionaland bilinear programs.
Journal of Global Optimiza-tion, 6(1):39?76, January.Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.2010.
Earthquake shakes Twitter users: real-timeevent detection by social sensors.
In Proceedingsof the 19th international conference on World WideWeb (WWW), pages 851?860.
ACM.Robert Tibshirani.
1996.
Regression shrinkage and se-lection via the lasso.
Journal of the Royal StatisticalSociety - Series B (Methodological), 58(1):267?288.1002Andranik Tumasjan, Timm O Sprenger, Philipp GSandner, and Isabell M Welpe.
2010.
Predictingelections with Twitter: What 140 characters revealabout political sentiment.
In Proceedings of the 4thInternational AAAI Conference on Weblogs and So-cial Media, pages 178?185.
AAAI.Ming Yuan and Yi Lin.
2006.
Model selection and es-timation in regression with grouped variables.
Jour-nal of the Royal Statistical Society - Series B: Statis-tical Methodology, 68(1):49?67.Peng Zhao and Bin Yu.
2006.
On model selectionconsistency of Lasso.
Journal of Machine LearningResearch, 7(11):2541?2563.Hui Zou and Trevor Hastie.
2005.
Regularizationand variable selection via the elastic net.
Journalof the Royal Statistical Society: Series B (StatisticalMethodology), 67(2):301?320, April.1003
