Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 91?99,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsVisual Information in Semantic RepresentationYansong Feng and Mirella LapataSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh, EH8 9AB, UKY.Feng-4@sms.ed.ac.uk, mlap@inf.ed.ac.ukAbstractThe question of how meaning might be ac-quired by young children and represented byadult speakers of a language is one of the mostdebated topics in cognitive science.
Existingsemantic representation models are primarilyamodal based on information provided by thelinguistic input despite ample evidence indi-cating that the cognitive system is also sensi-tive to perceptual information.
In this work weexploit the vast resource of images and associ-ated documents available on the web and de-velop a model of multimodal meaning repre-sentation which is based on the linguistic andvisual context.
Experimental results show thata closer correspondence to human data can beobtained by taking the visual modality into ac-count.1 IntroductionThe representation and modeling of word mean-ing has been a central problem in cognitive scienceand natural language processing.
Both disciplinesare concerned with how semantic knowledge is ac-quired, organized, and ultimately used in languageprocessing and understanding.
A popular traditionof studying semantic representation has been drivenby the assumption that word meaning can be learnedfrom the linguistic environment.
Words that are sim-ilar in meaning tend to behave similarly in termsof their distributions across different contexts.
Se-mantic spacemodels, among which Latent SemanticAnalysis (LSA, Landauer and Dumais 1997) is per-haps known best, operationalize this idea by captur-ing word meaning quantitatively in terms of simpleco-occurrence statistics.
Each word w is representedby a k element vector reflecting the local distribu-tional context of w relative to k context words.
Morerecently, topic models have been gaining ground asa more structured representation of word meaning.In contrast to more standard semantic space mod-els where word senses are conflated into a singlerepresentation, topic models assume that words ob-served in a corpus manifest some latent structure ?word meaning is a probability distribution over a setof topics (corresponding to coarse-grained senses).Each topic is a probability distribution over words,and the content of the topic is reflected in the wordsto which it assigns high probability.Semantic space (and topic) models are extractedfrom real language corpora, and thus provide a directmeans of investigating the influence of the statisticsof language on semantic representation.
They havebeen successful in explaining a wide range of be-havioral data ?
examples include lexical priming,deep dyslexia, text comprehension, synonym selec-tion, and human similarity judgments (see Landauerand Dumais 1997 and the references therein).
Theyalso underlie a large number of natural languageprocessing (NLP) tasks including lexicon acquisi-tion, word sense discrimination, text segmentationand notably information retrieval.
Despite their pop-ularity, these models offer a somewhat impoverishedrepresentation of word meaning based solely on in-formation provided by the linguistic input.Many experimental studies in language acquisi-tion suggest that word meaning arises not only fromexposure to the linguistic environment but also fromour interaction with the physical world.
For ex-ample, infants are from an early age able to formperceptually-based category representations (Quinnet al, 1993).
Perhaps unsurprisingly, words that re-fer to concrete entities and actions are among thefirst words being learned as these are directly ob-servable in the environment (Bornstein et al, 2004).Experimental evidence also shows that children re-spond to categories on the basis of visual features,e.g., they generalize object names to new objects of-ten on the basis of similarity in shape (Landau et al,1998) and texture (Jones et al, 1991).In this paper we aim to develop a unified mod-91eling framework of word meaning that captures themutual dependence between the linguistic and visualcontext.
This is a challenging task for at least tworeasons.
First, in order to emulate the environmentwithin which word meanings are acquired, we musthave recourse to a corpus of verbal descriptions andtheir associated images.
Such corpora are in shortsupply compared to the large volumes of solely tex-tual data.
Secondly, our model should integrate lin-guistic and visual information in a single representa-tion.
It is unlikely that we have separate representa-tions for different aspects of word meaning (Rogerset al, 2004).We meet the first challenge by exploiting mul-timodal corpora, namely collections of documentsthat contain pictures.
Although large scale corporawith a one-to-one correspondence between wordsand images are difficult to come by, datasets thatcontain images and text are ubiquitous.
For exam-ple, online news documents are often accompaniedby pictures.
Using this data, we develop a modelthat combines textual and visual information to learnsemantic representations.
We assume that imagesand their surrounding text have been generated bya shared set of latent variables or topics.
Our modelfollows the general rationale of topic models ?
it isbased upon the idea that documents are mixtures oftopics.
Importantly, our topics are inferred from thejoint distribution of textual and visual words.
Ourexperimental results show that a closer correspon-dence to human word similarity and association canbe obtained by taking the visual modality into ac-count.2 Related WorkThe bulk of previous work has focused on models ofsemantic representation that are based solely on tex-tual data.
Many of these models represent words asvectors in a high-dimensional space (e.g., Landauerand Dumais 1997), whereas probabilistic alterna-tives view documents as mixtures of topics, wherewords are represented according to their likelihoodin each topic (e.g., Steyvers and Griffiths 2007).Both approaches allow for the estimation of similar-ity between words.
Spatial models compare wordsusing distance metrics (e.g., cosine), while proba-bilistic models measure similarity between terms ac-cording to the degree to which they share the sametopic distributions.Within cognitive science, the problem of howwords are grounded in perceptual representationshas attracted some attention.
Previous modeling ef-forts have been relatively small-scale, using eitherartificial images, or data gathered from a few sub-jects in the lab.
Furthermore, the proposed modelswork well for the tasks at hand (e.g., either wordlearning or object categorization) but are not de-signed as a general-purpose meaning representation.For example, Yu (2005) integrates visual informa-tion in a computational model of lexical acquisi-tion and object categorization.
The model learns amapping between words and visual features fromdata provided by (four) subjects reading a children?sstory.
In a similar vein, Roy (2002) considers theproblem of learning which words or word sequencesrefer to objects in a synthetic image consisting of tenrectangles.
Andrews et al (2009) present a proba-bilistic model that incorporates perceptual informa-tion (indirectly) by combining distributional infor-mation gathered from corpus data with speaker gen-erated feature norms1 (which are also word-based).Much work in computer vision attempts to learnthe underlying connections between visual featuresand words from examples of images annotated withdescription keywords.
The aim here is to enhanceimage-based applications (e.g., search or retrieval)by developing models that can label images withkeywords automatically.
Most methods discoverthe correlations between visual features and wordsby introducing latent variables.
Standard latent se-mantic analysis (LSA) and its probabilistic variant(PLSA) have been applied to this task (Pan et al,2004; Hofmann, 2001; Monay and Gatica-Perez,2007).
More sophisticated approaches estimate thejoint distribution of words and regional image fea-tures, whilst treating annotation as a problem of sta-tistical inference in a graphical model (Blei and Jor-dan, 2003; Barnard et al, 2002).Our own work aims to develop a model of se-mantic representation that takes visual context intoaccount.
We do not model explicitly the correspon-dence of words and visual features, or learn a map-ping between words and visual features.
Rather,we develop a multimodal representation of meaningwhich is based on visual information and distribu-tional statistics.
We hypothesize that visual featuresare crucial in acquiring and representing meaning1Participants are given a series of object names and for eachobject they are asked to name all the properties they can thinkof that are characteristic of the object.92Michelle Obama fever hits the UKIn the UK on her firstvisit as first lady, MichelleObama seems to be mak-ing just as big an im-pact.
She has attracted asmuch interest and columninches as her husband onthis London trip; creatinga buzz with her dazzling outfits, her own scheduleof events and her own fanbase.
Outside Bucking-ham Palace, as crowds gathered in anticipation ofthe Obamas?
arrival, Mrs Obama?s star appeal wasapparent.Table 1: Each article in the document collection containsa document (the title is shown in boldface), and imagewith related content.and conversely, that linguistic information can beuseful in isolating salient visual features.
Our modelextracts a semantic representation from large docu-ment collections and their associated images withoutany human involvement.
Contrary to Andrews et al(2009) we use visual features directly without rely-ing on speaker generated norms.
Furthermore, un-like most work in image annotation, we do not em-ploy any goldstandard data where images have beenmanually labeled with their description keywords.3 Semantic Representation ModelMuch like LSA and the related topic models ourmodel creates semantic representations from largedocument collections.
Importantly, we assume thatthe documents are paired with images which in turndescribe some of the document?s content.
Our ex-periments make use of news articles which are of-ten accompanied with images illustrating events, ob-jects or people mentioned in the text.
Other datasetswith similar properties include Wikipedia entriesand their accompanying pictures, illustrated stories,and consumer photo collections.
An example newsarticle and its associated image is shown in Table 1(we provide more detail on the database we used inour experiments in Section 4).Our model exploits the redundancy inherent inthis multimodal collection.
Specifically, we assumethat the images and their surrounding text have beengenerated by a shared set of topics.
A potentialstumbling block here is the fact that images anddocuments represent distinct modalities: images arecommonly described by a continuous feature space(e.g., color, shape, texture; Barnard et al 2002; Bleiand Jordan 2003), whereas words are discrete.
For-tunately, we can convert the visual features from acontinuous onto a discrete space, thereby renderingimage features more like word units.
In the follow-ing we describe how we do this and then move on topresent an extension of Latent Dirichlet Allocation(LDA, Blei and Jordan 2003), a topic model that canbe used to represent meaning as a probability distri-bution over a set of multimodal topics.
Finally, wediscuss how word similarity can be measured underthis model.3.1 Image ProcessingA large number of image processing techniques havebeen developed in computer vision for extractingmeaningful features which are subsequently usedin a modeling task.
For example, a common firststep to all automatic image annotation methods ispartitioning the image into regions, using either animage segmentation algorithm (such as normalizedcuts; Shi and Malik 2000) or a fixed-grid layout(Feng et al, 2004).
In the first case the image isrepresented by irregular regions (see Figure 1(a)),whereas in the second case the image is partitionedinto smaller scale regions which are uniformly ex-tracted from a fixed grid (see Figure 1(b)).
The ob-tained regions are further represented by a standardset of features including color, shape, and texture.These can be treated as continuous vectors (Blei andJordan, 2003) or in quantized form (Barnard et al,2002).Despite much progress in image segmentation,there is currently no automatic algorithm that canreliably divide an image into meaningful parts.
Ex-tracting features from small local regions is thuspreferable, especially for image collections that arediverse and have low resolution (this is often the casefor news images).
In our work we identify local re-gions using a difference-of-Gaussians point detector(see Figure 1(c)).
This representation is based on de-scriptors computed over automatically detected im-age regions.
It provides a much richer (and hopefullymore informative) feature space compared to thealternative image representations discussed above.For example, an image segmentation algorithm,would extract at most 20 regions from the imagein Figure 1; uniform grid segmentation yields 14393(a) (b) (c)Figure 1: Image partitioned into regions of varying granularity using (a) the normalized cut image segmentation algo-rithm, (b) uniform grid segmentation, and (c) the SIFT point detector.
(11 ?
13) regions, whereas an average of 240 points(depending on the image content) are detected.
Anon-sparse feature representation is critical in ourcase, since we usually do not have more than oneimage per document.We compute local image descriptors using thethe Scale Invariant Feature Transform (SIFT) algo-rithm (Lowe, 1999).
Importantly, SIFT descriptorsare designed to be invariant to small shifts in posi-tion, changes in illumination, noise, and viewpointand can be used to perform reliable matching be-tween different views of an object or scene (Mikola-jczyk and Schmid, 2003; Lowe, 1999).
We furtherquantize the SIFT descriptors using the K-meansclustering algorithm to obtain a discrete set of vi-sual terms (visiterms) which form our visual vo-cabulary VocV .
Each entry in this vocabulary standsfor a group of image regions which are similarin content or appearance and assumed to origi-nate from similar objects.
More formally, each im-age I is expressed in a bag-of-words format vector,[v1,v2, ...,vL], where vi = n only if I has n regionslabeled with vi.
Since both images and documentsin our corpus are now represented as bags-of-words,and since we assume that the visual and textualmodalities express the same content, we can go astep further and represent the document and its as-sociated image as a mixture of verbal and visualwords dMix.
We will then learn a topic model on thisconcatenated representation of visual and textual in-formation.3.2 Topic ModelLatent Dirichlet Allocation (Blei et al, 2003; Grif-fiths et al, 2007) is a probabilistic model of text gen-eration.
LDA models each document using a mix-ture over K topics, which are in turn characterizedas distributions over words.
The words in the docu-ment are generated by repeatedly sampling a topicaccording to the topic distribution, and selecting aword given the chosen topic.
Under this framework,the problem of meaning representation is expressedas one of statistical inference: given some data ?textual and visual words ?
infer the latent structurefrom which it was generated.
Word meaning is thusmodeled as a probability distribution over a set oflatent multimodal topics.LDA can be represented as a three level hierarchi-cal Bayesian model.
Given a corpus consisting of Mdocuments, the generative process for a document dis as follows.
We first draw the mixing proportionover topics ?d from a Dirichlet prior with parame-ters ?.
Next, for each of the Nd words wdn in doc-ument d, a topic zdn is first drawn from a multino-mial distribution with parameters ?dn.
The probabil-ity of a word token w taking on value i given thattopic z = j is parametrized using a matrix ?
withbi j = p(w = i|z = j).
Integrating out ?d?s and zdn?s,gives P(D|?,?
), the probability of a corpus (or doc-ument collection):M?d=1ZP(?d |?)(Nd?n=1?zdnP(zdn|?d)P(wdn|zdn,?
))d?dThe central computational problem in topicmodeling is to compute the posterior distribu-tion P(?,z|w,?,?)
of the hidden variables givena document w = (w1,w2, .
.
.
,wN).
Although thisdistribution is intractable in general, a variety of ap-94proximate inference algorithms have been proposedin the literature including variational inferencewhich our model adopts.
Blei et al (2003) introducea set of variational parameters, ?
and ?, and showthat a tight lower bound on the log likelihood ofthe probability can be found using the followingoptimization procedure:(??,??)
= argmin?,?D(q(?,z|?,?)||p(?,z|w,?,?
))Here, D denotes the Kullback-Leibler (KL) diver-gence between the true posterior and the variationaldistribution q(?,z|?,?)
defined as: q(?,z|?,?)
=q(?|?
)?Nn=1 q(zn|?n), where the Dirichlet parame-ter ?
and the multinomial parameters (?1, .
.
.
,?N) arethe free variational parameters.
Notice that the opti-mization of parameters (??(w),??
(w)) is document-specific (whereas ?
is corpus specific).Previous applications of LDA (e.g., to docu-ment classification or information retrieval) typi-cally make use of the posterior Dirichlet parame-ters ??
(w) associated with a given document.
We arenot so much interested in ?
as we wish to obtain asemantic representation for a given word across doc-uments.
We therefore train the LDA model sketchedabove on a corpus of multimodal documents {dMix}consisting of both textual and visual words.
We se-lect the number of topics, K, and apply the LDA al-gorithm to obtain the ?
parameters, where ?
repre-sents the probability of a word wi given a topic z j,p(wi|z j) = ?i j.
The meaning of wi is thus extractedfrom ?
and is a K-element vector, whose compo-nents correspond to the probability of wi given eachlatent topic assumed to have generated the documentcollection.3.3 Similarity MeasuresThe ability to accurately measure the similarity orassociation between two words is often used as a di-agnostic for the psychological validity of semanticrepresentation models.
In the topic model describedabove, the similarity between two words w1 and w2can be intuitively measured by the extent to whichthey share the same topics (Griffiths et al, 2007).For example, we may use the KL divergence to mea-sure the difference between the distributions p and q:D(p,q) =K?j=1p j log2p jq jwhere p and q are shorthand for P(w1|z j)and P(w2|z j), respectively.The KL divergence is asymmetric and in many ap-plications, it is preferable to apply a symmetric mea-sure such as the Jensen Shannon (JS) divergence.The latter measures the ?distance?
between p and qthrough (p+q)2 , the average of p and q:JS(p,q) =12[D(p,(p+q)2)+D(q,(p+q)2)]An alternative approach to expressing the similar-ity between two words is proposed in Griffiths et al(2007).
The underlying idea is that word associationcan be expressed as a conditional distribution.
If wehave seen word w1, then we can determine the prob-ability that w2 will be also generated by comput-ing P(w2|w1).
Although the LDA generative modelallows documents to contain multiple topics, here itis assumed that both w1 and w2 came from a singletopic:P(w2|w1) =K?z=1P(w2|z)P(z|w1)P(z|w1) ?
P(w1|z)P(z)where p(z) is uniform, a single topic is sampledfrom the distribution P(z|w1), and an overall esti-mate is obtained by averaging over all topics K.Griffiths et al (2007) report results on mod-eling human association norms using exclu-sively P(w2|w1).
We are not aware of any previouswork that empirically assesses which measure is bestat capturing semantic similarity.
We undertake suchan empirical comparison as it is not a priory obvioushow similarity is best modeled under a multimodalrepresentation.4 Experimental SetupIn this section we discuss our experimental designfor assessing the performance of the model pre-sented above.
We give details on our training proce-dure and parameter estimation and present the base-line method used for comparison with our model.Data We trained the multimodal topic model onthe corpus created in Feng and Lapata (2008).
Itcontains 3,361 documents that have been down-loaded from the BBC News website.2 Each doc-ument comes with an image that depicts some ofits content.
The images are usually 203 pixels wide2http://news.bbc.co.uk/95and 152 pixels high.
The average document lengthis 133.85 words.
The corpus has 542,414 words intotal.
Our experiments used a vocabulary of 6,253textual words.
These were words that occurred atleast five times in the whole corpus, excludingstopwords.
The accompanying images were prepro-cessed as follows.
We first extracted SIFT featuresfrom each image (150 on average) which we subse-quently quantized into a discrete set of visual termsusing K-means.
As we explain below, we deter-mined an optimal value for K experimentally.Evaluation Our evaluation experiments comparedthe multimodal topic model against a standard text-based topic model trained on the same corpus whilstignoring the images.
Both models were assessed ontwo related tasks, that have been previously usedto evaluate semantic representation models, namelyword association and word similarity.In order to simulate word association, we usedthe human norms collected by Nelson et al (1999).3These were established by presenting a large num-ber of participants with a cue word (e.g., rice) andasking them to name an associate word in response(e.g.,Chinese, wedding, food, white).
For each word,the norms provide a set of associates and the fre-quencies with which they were named.
We can thuscompute the probability distribution over associatesfor each cue.
Analogously, we can estimate the de-gree of similarity between a cue and its associatesusing our model (and any of the measures in Sec-tion 3.3).
And consequently examine (using corre-lation analysis) the degree of linear relationship be-tween the human cue-associate probabilities and theautomatically derived similarity values.
We also re-port howmany times the word with the highest prob-ability under the model was the first associate in thenorms.
The norms contain 10,127 unique words intotal.
Of these, we created semantic representationsfor the 3,895 words that appeared in our corpus.Our word similarity experiment used the Word-Sim353 test collection (Finkelstein et al, 2002)which consists of relatedness judgments for wordpairs.
For each pair, a similarity judgment (ona scale of 0 to 10) was elicited from humansubjects (e.g., tiger-cat are very similar, whereasdelay?racism are not).
The average rating for eachpair represents an estimate of the perceived sim-ilarity of the two words.
The task varies slightlyfrom word association.
Here, participants are asked3http://www.usf.edu/Freeassociation.Figure 2: Performance of multimodal topic model on pre-dicting word association under varying topics and visualterms (development set).to rate perceived similarity rather than generate thefirst word that came into their head in response to acue word.
The collection contains similarity ratingsfor 353 word pairs.
Of these, we constructed seman-tic representations for the 254 that appeared in ourcorpus.
We also evaluated how well model producedsimilarities correlate with human ratings.
Through-out this paper we report correlation coefficients us-ing Pearson?s r.5 Experimental ResultsModel Selection The multimodal topic model hasseveral parameters that must be instantiated.
Theseinclude the quantization of the image features, thenumber of topics, the choice of similarity function,and the values for ?
and ?.
We explored the pa-rameter space on held-out data.
Specifically, we fitthe parameters for the word association and similar-ity models separately using a third of the associa-tion norms and WordSim353 similarity judgments,respectively.
As mentioned in Section 3.1 we usedK-means to quantize the image features into a dis-crete set of visual terms.
We varied K from 250to 2000.We also varied the number of topics from 25to 750 for both the multimodal and text-based topicmodels.
The parameter ?
was set to 0.1 and ?
wasinitialized randomly.
The model was trained usingvariational Bayes until convergence of its bound onthe likelihood objective.
This took 1,000 iterations.Figure 2 shows how word association perfor-mance varies on the development set with differentnumbers of topics (t) and visual terms (r) according96Figure 3: Performance of multimodal topic model on pre-dicting word similarity under varying topics and visualterms (development set).to three similarity measures: KL divergence, JS di-vergence, and P(w2|w1), the probability of word w2given w1 (see Section 3.3).
Figure 3 shows results onthe development set for the word similarity task.
Asfar as word association is concerned, we obtain bestresults with P(w2|w1), 750 visual terms and 750 top-ics (r = 0.188).
On word similarity, JS performs bestwith 500 visual terms and 25 topics (r = 0.374).
It isnot surprising that P(w2|w1) works best for word as-sociation.
The measure expresses the associative re-lations between words as a conditional distributionover potential response words w2 for cue word w1.A symmetric function is more appropriate for wordsimilarity as the task involves measuring the degreeto which to words share some meaning (expressedas topics in our model) rather than whether a word islikely to be generated as a response to another word.These differences also lead to different parametriza-tions of the semantic space.
A rich visual term vo-cabulary (750 terms) is needed for modeling associ-ation as broader aspects of word meaning are takeninto account, whereas a sparser more focused repre-sentation (with 500 visual terms and 25 overall top-ics) is better at isolating the common semantic con-tent between two words.
We explored the parame-ter space for the text-based topic model in a sim-ilar fashion.
On the word association task the bestcorrelation coefficient was achieved with 750 top-ics and P(w2|w1) (r = 0.139).
On word similarity,the best results were obtained with 75 topics and theJS divergence (r = 0.309).Model Word Association Word SimilarityUpperBnd 0.400 0.545MixLDA 0.123 0.318TxtLDA 0.077 0.247Table 2: Model performance on word association andsimilarity (test set).Model Comparison Table 2 summarizes our re-sults on the test set using the optimal set of pa-rameters as established on the development set.
Thefirst row shows how well humans agree with eachother on the two tasks (UpperBnd).
We estimatedthe intersubject correlation using leave-one-out re-sampling4 (Weiss and Kulikowski, 1991).
As canbe seen, in all cases the topic model based on tex-tual and visual modalities (MixLDA) outperformsthe model relying solely on textual information(TxtLDA).
The differences in performance are sta-tistically significant (p < 0.05) using a t-test (Cohenand Cohen, 1983).Steyvers and Griffiths (2007) also predict wordassociation using Nelson?s norms and a state-of-the-art LDA model.
Although they do not report correla-tions, they compute how many times the word withthe highest probability P(w2|w1) under the modelwas the first associate in the human norms.
Usinga considerably larger corpus (37,651 documents),they reach an accuracy of 16.15%.
Our corpus con-tains 3,361 documents, the MixLDA model per-forms at 14.15% and the LDA model at 13.16%.
Us-ing a vector-based model trained on the BNC corpus(100Mwords), Washtell andMarkert (2009) report acorrelation of 0.167 on the same association data set,whereas our model achieves a correlation of 0.123.With respect to word similarity, Marton et al (2009)report correlations within the range of 0.31?0.54 us-ing different instantiations of a vector-based modeltrained on the BNC with a vocabulary of 33,000words.
Our MixLDA model obtains a correlationof 0.318 with a vocabulary five times smaller (6,253words).
Although these results are not strictly com-parable due to the different nature and size of thetraining data, they give some indication of the qual-ity of our model in the context of other approachesthat exploit only the textual modality.
Besides, ourintent is not to report the best performance possible,4We correlated the data obtained from each participant withthe ratings obtained from all other participants and report theaverage.97GAME, CONSOLE, XBOX, SECOND, SONY, WORLD,TIME, JAPAN, JAPANESE, SCHUMACHER, LAP, MI-CROSOFT, ALONSO, RACE, TITLE, WIN, GAMERS,LAUNCH, RENAULT, MARKETPARTY, MINISTER, BLAIR, LABOUR, PRIME, LEADER,GOVERNMENT, TELL, BROW, MP, TONY, SIR, SECRE-TARY, ELECTION, CONFERENCE, POLICY, NEW, WANT,PUBLIC, SPEECHSCHOOL, CHILD, EDUCATION, STUDENT, WORK,PUPIL, PARENT, TEACHER, GOVERNMENT, YOUNG,SKILL, AGE, NEED, UNIVERSITY, REPORT, LEVEL,GOOD, HELL, NEW, SURVEYTable 3: Most frequent words in three topics learnt froma corpus of image-document pairs.but to show that a model of meaning representationis more accurate when taking visual information intoaccount.Table 3 shows some examples of the topicsfound by our model, which largely form coher-ent blocks of semantically related words.
In gen-eral, we observe that the model using image fea-tures tends to prefer words that visualize easily(e.g., CONSOLE, XBOX).
Furthermore, the visualmodality helps obtain crisper meaning distinctions.Here, SCHUMACHER is a very probable world forthe ?game?
cluster.
This is because the Formula Onedriver appears as a character in several video gamesdiscussed and depicted in our corpus.
For com-parison the ?game?
cluster for the text-based LDAmodel contains the words: GAME, USE, INTERNET,SITE, USE, SET, ONLINE, WEB, NETWORK, MUR-RAY, PLAY, MATCH, GOOD, WAY, BREAK, TECH-NOLOGY, WORK, NEW, TIME, SECOND.We believe the model presented here works bet-ter than a vanilla text-based topic model for at leastthree reasons: (1) the visual information helps cre-ate better clusters (i.e., conceptual representations)which in turn are used to measure similarity or as-sociation; these clusters themselves are amodal butexpress commonalities across the visual and textualmodalities; (2) the model is also able to capture per-ceptual correlations between words.
For example,RED is the most frequent associate for APPLE in Nel-son?s norms.
This association is captured in our vi-sual features (pictures with apples cluster with pic-tures showing red objects) even though RED does notco-occur with APPLE in our data; (3) finally, even incases where two words are visually very different interms of shape or color (e.g., BANANA and APPLE),they tend to appear in images with similar structure(e.g., on tables, in bowls, as being held or eaten bysomeone) and thus often share some common ele-ment of meaning.6 ConclusionIn this paper we developed a computational modelthat unifies visual and linguistic representations ofword meaning.
The model learns from natural lan-guage corpora paired with images under the assump-tion that visual terms and words are generated bymixtures of latent topics.
We have shown that acloser correspondence to human data can be ob-tained by explicitly taking the visual modality intoaccount in comparison to a model that estimates thetopic structure solely from the textual modality.
Be-yond word similarity and association, the approachis promising for modeling word learning and cate-gorization as well as a wide range of priming stud-ies.
Outwith cognitive science, we hope that someof the work described here might be of relevanceto more applied tasks such as thesaurus acquisition,word sense disambiguation, multimodal search, im-age retrieval, and summarization.Future improvements include developing a non-parametric version that jointly learns how many vi-sual terms and topics are optimal.
Currently, the sizeof the visual vocabulary and the number of topicsare parameters in the model, that must be tuned sep-arately for different tasks and corpora.
Another ex-tension concerns the creation of visual terms.
Ourmodel assumes that an image is a bag of words.
Theassumption is convenient for modeling purposes, butclearly false in the context of visual processing.
Im-age descriptors found closely to each other are likelyto represent the same object and should form oneterm rather than several distinct ones (Wang andGrimson, 2007).
Taking the spatial structure amongvisual words into account would yield better topicsand overall better semantic representations.
Analo-gously, we could represent documents by their syn-tactic structure (Boyd-Graber and Blei, 2009).ReferencesAndrews, M., G. Vigliocco, and D. Vinson.
2009.
In-tegrating experiential and distributional data to learnsemantic representations.
Psychological Review116(3):463?498.Barnard, K., P. Duygulu, D. Forsyth, N. de Freitas,D.
Blei, andM.
Jordan.
2002.
Matching words and pic-98tures.
Journal of Machine Learning Research 3:1107?1135.Blei, D. and M. Jordan.
2003.
Modeling annotated data.In Proceedings of the 26th Annual International ACMSIGIR Conference.
Toronto, ON, pages 127?134.Blei, D. M., A. Y. Ng, and M. I. Jordan.
2003.
LatentDirichlet alocation.
Journal of Machine Learning Re-search 3:993?1022.Bornstein, M. H., L. R. Cote, S. Maital, K. Painter, S.-Y.
Park, and L. Pascual.
2004.
Cross-linguistic analy-sis of vocabulary in young children: Spanish, Dutch,French, Hebrew, Italian, Korean, and American En-glish.
Child Development 75(4):1115?1139.Boyd-Graber, J. and D. Blei.
2009.
Syntactic topicmodels.
In Proceedings of the 22nd Conference onAdvances in Neural Information Processing Systems.MIT, Press, Cambridge, MA, pages 185?192.Cohen, J. and P. Cohen.
1983.
Applied Multiple Regres-sion/Correlation Analysis for the Behavioral Sciences.Hillsdale, NJ: Erlbaum.Feng, S., V. Lavrenko, and R. Manmatha.
2004.
Mul-tiple Bernoulli relevance models for image and videoannotation.
In Proceedings of the International Con-ference on Computer Vision and Pattern Recognition.Washington, DC, pages 1002?1009.Feng, Y. and M. Lapata.
2008.
Automatic image annota-tion using auxiliary text information.
In Proceedingsof the ACL-08: HLT .
Columbus, pages 272?280.Finkelstein, L., E. Gabrilovich, Y. Matias, E. Rivlin,Z.
Solan, G. Wolfman, and E. Ruppin.
2002.
Placingsearch in context: The concept revisited.
ACM Trans-actions on Information Systems 20(1):116?131.Griffiths, T. L., M. Steyvers, and J.
B. Tenenbaum.
2007.Topics in semantic representation.
Psychological Re-view 114(2):211?244.Hofmann, T. 2001.
Unsupervised learning by proba-bilistic latent semantic analysis.
Machine Learning41(2):177?196.Jones, S. S., L. B. Smith, and B. Landau.
1991.
Ob-ject properties and knowledge in early lexical learning.Child Development (62):499?516.Landau, B., L. Smith, and S. Jones.
1998.
Object percep-tion and object naming in early development.
Trendsin Cognitive Science 27:19?24.Landauer, T. and S. T. Dumais.
1997.
A solution toPlato?s problem: the latent semantic analysis theoryof acquisition, induction, and representation of knowl-edge.
Psychological Review 104(2):211?240.Lowe, D. 1999.
Object recognition from local scale-invariant features.
In Proceedings of InternationalConference on Computer Vision.
IEEE Computer So-ciety, pages 1150?1157.Marton, Y., S. Mohammad, and P. Resnik.
2009.
Estimat-ing semantic distance using soft semantic constraintsin knowledge-source ?
corpus hybrid models.
In Pro-ceedings of the 2009 Conference on Empirical Meth-ods in Natural Language Processing.
Singapore, pages775?783.Mikolajczyk, K. and C. Schmid.
2003.
A performanceevaluation of local descriptors.
In Proceedings of the9th International Conference on Computer Vision andPattern Recognition.
Nice, France, volume 2, pages257?263.Monay, F. and D. Gatica-Perez.
2007.
Modeling semanticaspects for cross-media image indexing.
IEEE Trans-actions on Pattern Analysis and Machine Intelligence29(10):1802?1817.Nelson, D. L., C. L. McEvoy, and T.A.
Schreiber.
1999.The university of South Florida word associationnorms.Pan, J., H. Yang, P. Duygulu, and C. Faloutsos.
2004.
Au-tomatic image captioning.
In Proceedings of the 2004International Conference on Multimedia and Expo.Taipei, pages 1987?1990.Quinn, P., P. Eimas, and S. Rosenkrantz.
1993.
Evidencefor representations of perceptually similar natural cate-gories by 3-month and 4-month old infants.
Perception22:463?375.Rogers, T. T., M. A. Lambon Ralph, P. Garrard,S.
Bozeat, J. L. McClelland, J. R. Hodges, and K. Pat-terson.
2004.
Structure and deterioration of semanticmemory: A neuropsychological and computational in-vestigation.
Psychological Review 111(1):205?235.Roy, D. 2002.
Learning words and syntax for a visual de-scription task.
Computer Speech and Language 16(3).Shi, J. and J. Malik.
2000.
Normalized cuts and imagesegmentation.
IEEE Transactions on Pattern Analysisand Machine Intelligence 22(8):888?905.Steyvers, M. and T. Griffiths.
2007.
Probabilistic topicmodels.
In T. Landauer, D. McNamara, S Dennis, andW Kintsch, editors, A Handbook of Latent SemanticAnalysis, Psychology Press.Wang, X. and E. Grimson.
2007.
Spatial latent Dirichletallocation.
In Proceedings of the 20th Conference onAdvances in Neural Information Processing Systems.MI Press, Cambridge, MA, pages 1577?1584.Washtell, J. and K. Markert.
2009.
A comparison of win-dowless and window-based computational associationmeasures as predictors of syntagmatic human associa-tions.
In Proceedings of the 2009 Conference on Em-pirical Methods in Natural Language Processing.
Sin-gapore, pages 628?637.Weiss, S. M. and C. A. Kulikowski.
1991.
Computer Sys-tems that Learn: Classification and Prediction Meth-ods from Statistics, Neural Nets, Machine Learning,and Expert Systems.
Morgan Kaufmann, San Mateo,CA.Yu, C. 2005.
The emergence of links between lexicalacquisition and object categorization: A computationalstudy.
Connection Science 17(3):381?397.99
