Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 505?513,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPModel Adaptation via Model Interpolation and Boostingfor Web Search RankingJianfeng Gao*, Qiang Wu*, Chris Burges*, Krysta Svore*,Yi Su#, Nazan Khan$, Shalin Shah$, Hongyan Zhou$*Microsoft Research, Redmond, USA{jfgao; qiangwu; cburges; ksvore}@microsoft.com#Johns Hopkins University, USAsuy@jhu.edu$Microsoft Bing Search, Redmond, USA{nazanka; a-shas; honzhou}@microsoft.comAbstractThis paper explores two classes of model adapta-tion methods for Web search ranking: Model In-terpolation and error-driven learning approachesbased on a boosting algorithm.
The results showthat model interpolation, though simple, achievesthe best results on all the open test sets where thetest data is very different from the training data.The tree-based boosting algorithm achieves thebest performance on most of the closed test setswhere the test data and the training data are sim-ilar, but its performance drops significantly onthe open test sets due to the instability of trees.Several methods are explored to improve therobustness of the algorithm, with limited success.1 IntroductionWe consider the task of ranking Web searchresults, i.e., a set of retrieved Web documents(URLs) are ordered by relevance to a query is-sued by a user.
In this paper we assume that thetask is performed using a ranking model (alsocalled ranker for short) that is learned on labeledtraining data (e.g., human-judgedquery-document pairs).
The ranking model actsas a function that maps the feature vector of aquery-document pair to a real-valued score ofrelevance.Recent research shows that such a learnedranker is superior to classical retrieval models intwo aspects (Burges et al, 2005; 2006; Gao et al,2005).
First, the ranking model can use arbitraryfeatures.
Both traditional criteria such as TF-IDFand BM25, and non-traditional features such ashyperlinks can be incorporated as features in theranker.
Second, if large amounts of high-qualityhuman-judged query-document pairs wereavailable for model training, the ranker couldachieve significantly better retrieval results thanthe traditional retrieval models that cannot ben-efit from training data effectively.
However,such training data is not always available formany search domains, such as non-Englishsearch markets or person name search.One of the most widely used strategies to re-medy this problem is model adaptation, whichattempts to adjust the parameters and/or struc-ture of a model trained on one domain (called thebackground domain), for which large amounts oftraining data are available, to a different domain(the adaptation domain), for which only smallamounts of training data are available.
In Websearch applications, domains can be defined byquery types (e.g., person name queries), or lan-guages, etc.In this paper we investigate two classes ofmodel adaptation methods for Web searchranking: Model Interpolation approaches anderror-driven learning approaches.
In modelinterpolation approaches, the adaptation data isused to derive a domain-specific model (alsocalled in-domain model), which is then com-bined with the background model trained on thebackground data.
This appealingly simple con-cept provides fertile ground for experimentation,depending on the level at which the combinationis implemented (Bellegarda, 2004).
In er-ror-driven learning approaches, the backgroundmodel is adjusted so as to minimize the rankingerrors the model makes on the adaptation data(Bacchiani et al, 2004; Gao et al 2006).
This isarguably more powerful than model interpola-tion for two reasons.
First, by defining a propererror function, the method can optimize moredirectly the measure used to assess the finalquality of the Web search system, e.g., NormalizedDiscounted Cumulative Gain (Javelin & Kekalainen,2000) in this study.
Second, in this framework,the model can be adjusted to be as fine-grained asnecessary.
In this study we developed a set oferror-driven learning methods based on aboosting algorithm where, in an incrementalmanner, not only each feature weight could be505changed separately, but new features could beconstructed.We focus our experiments on the robustnessof the adaptation methods.
A model is robust if itperforms reasonably well on unseen test datathat could be significantly different from trainingdata.
Robustness is important in Web searchapplications.
Labeling training data takes time.As a result of the dynamic nature of Web, by thetime the ranker is trained and deployed, thetraining data may be more or less out of date.Our results show that the model interpolation ismuch more robust than the boosting-based me-thods.
We then explore several methods to im-prove the robustness of the methods, includingregularization, randomization, and using shal-low trees, with limited success.2 Ranking Model and QualityMeasure in Web SearchThis section reviews briefly a particular exampleof rankers, called LambdaRank (Burges et al,2006), which serves as the baseline ranker in ourstudy.Assume that training data is a set of input/output pairs (x, y).
x is a feature vector extractedfrom a query-document pair.
We use approx-imately 400 features, including dynamic rankingfeatures such as term frequency and BM25, andstatistic ranking features such as PageRank.
y isa human-judged relevance score, 0 to 4, with 4 asthe most relevant.LambdaRank is a neural net ranker that mapsa feature vector x to a real value y that indicatesthe relevance of the document given the query(relevance score).
For example, a linear Lamb-daRank simply maps x to y with a learned weightvector w such that ?
= ?
?
?.
(We used nonli-near LambdaRank in our experiments).
Lamb-daRank is particularly interesting to us due to theway w is learned.
Typically, w is optimized w.r.t.a cost function using numerical methods if thecost function is smooth and its gradient w.r.t.
wcan be computed easily.
In order for the rankerto achieve the best performance in documentretrieval, the cost function used in trainingshould be the same as, or as close as possible to,the measure used to assess the quality of thesystem.
In Web search, Normalized DiscountedCumulative Gain (NDCG) (Jarvelin and Kekalai-nen, 2000) is widely used as quality measure.
Fora query,  NDCG is computed  as??
= ??2?
?
?
1log 1 + ??
?=1, (1)where ?(?)
is the relevance level of the j-th doc-ument, and the normalization constant Ni ischosen so that a perfect ordering would result in??
= 1.
Here L is the ranking truncation level atwhich NDCG is computed.
The ??
are then av-eraged over a query set.
However, NDCG, if itwere to be used as a cost function, is either flat ordiscontinuous everywhere, and thus presentschallenges to most optimization approaches thatrequire the computation of the gradient of thecost function.LambdaRank solves the problem by using animplicit cost function whose gradients are speci-fied by rules.
These rules are called ?-functions.Burges et al (2006) studied several ?-functionsthat were designed with the NDCG cost functionin mind.
They showed that LambdaRank withthe best ?-function outperforms significantly asimilar neural net ranker, RankNet (Burges et al,2005), whose parameters are optimized using thecost function based on cross-entropy.The superiority of LambdaRank illustrates thekey idea based on which we develop the modeladaptation methods.
We should always adaptthe ranking models in such a way that the NDCGcan be optimized as directly as possible.3 Model InterpolationOne of the simplest model interpolation methodsis to combine an in-domain model with a back-ground model at the model level via linear in-terpolation.
In practice we could combine morethan two in-domain/background models.
Let-ting Score(q, d) be a ranking model that maps aquery-document pair to a relevance score, thegeneral form of the interpolation model is?????(?,?)
= ????????
?,?
,?
?=1(2)where the ?
?s are interpolation weights, opti-mized on validation data with respect to a pre-defined objective, which is NDCG in our case.As mentioned in Section 2, NDCG is not easy tooptimize, for which we resort to two solutions,both of which achieve similar results in our ex-periments.The first solution is to view the interpolationmodel of Equation (2) as a linear neural netranker where each component  model Scorei(.)
isdefined as a feature function.
Then, we can usethe LambdaRank algorithm described in Section2 to find the optimal weights.An alternative solution is to view interpola-tion weight estimation as a multi-dimensionaloptimization problem, with each model as a506dimension.
Since NCDG is not differentiable, wetried in our experiments the numerical algo-rithms that do not require the computation ofgradient.
Among the best performers is thePowell Search algorithm (Press et al, 1992).
Itfirst constructs a set of N virtual directions thatare conjugate (i.e., independent with each other),then it uses line search N times, each on one vir-tual direction, to find the optimum.
Line searchis a one-dimensional optimization algorithm.Our implementation follows the one described inGao et al (2005), which is used to optimize theaveraged precision.The performance of model interpolation de-pends to a large degree upon the quality and thesize of adaptation data.
First of all, the adaptationdata has to be ?rich?
enough to suitably charac-terize the new domain.
This can only beachieved by collecting more in-domain data.Second, once the domain has been characterized,the adaptation data has to be ?large?
enough tohave a model reliably trained.
For this, we de-veloped a method, which attempts to augmentadaptation data by gathering similar data frombackground data sets.The method is based on the k-nearest-neighbor(kNN) algorithm, and is inspired by Bishop(1995).
We use the small in-domain data set D1as a seed, and expand it using the large back-ground data set D2.
When the relevance labelsare assigned by humans, it is reasonable to as-sume that queries with the lowest informationentropy of labels are the least noisy.
That is, forsuch a query most of the URLs are labeled ashighly relevant/not relevant documents ratherthan as moderately relevance/not relevantdocuments.Due to computational limitations ofkNN-based algorithms, a small subset of queriesfrom D1 which are least noisy are selected.
Thisdata set is called S1.
For each sample in D2, its3-nearest neighbors in S1 are found using a co-sine-similarity metric.
If the three neighbors arewithin a very small distance from the sample inD2, and one of the labels of the nearest neighborsmatches exactly, the training sample is selectedand is added to the expanded set E2, in its ownquery.
This way, S1 is used to choose trainingdata from D2, which are found to be close insome space.This process effectively creates several datapoints in close neighborhood of the points in theoriginal small data set D1, thus expanding theset, by jittering each training sample a little.
Thisis equivalent to training with noise (Bishop,1995), except that the training samples used areactual queries judged by a human.
This is foundto increase the NDCG in our experiments.4 Error-Driven LearningOur error-drive learning approaches to rankingmodeling adaptation are based on the StochasticGradient Boosting algorithm (or the boostingalgorithm for short) described in Friedman(1999).
Below, we follow the notations in Fried-man (2001).Let adaptation data (also called training data inthis section) be a set of input/output pairs {xi, yi},i = 1?N.
In error-driven learning approaches,model adaptation is performed by adjusting thebackground model into a new in-domain model?
: ?
?
?
that minimizes a loss function L(y, F(x))over all samples in training data??
= argmin??(??
,?(??))??=1.
(3)We further assume that F(x) takes the form ofadditive expansion as?
?
=  ???
?
; ???
?=0, (4)where h(x; a) is called basis function, and isusually a simple parameterized function of theinput x, characterized by parameters a.
In whatfollows, we drop a, and use h(x) for short.
Inpractice, the form of h has to be restricted to aspecific function family to allow for a practicallyefficient procedure of model adaptation.
?
is areal-valued coefficient.Figure 1 is the generic algorithm.
It startswith a base model F0, which is a backgroundmodel.
Then for m = 1, 2, ?, M, the algorithmtakes three steps to adapt the base model so as tobest fit the adaptation data: (1) compute the re-sidual of the current base model (line 3), (2) selectthe optimal basis function (line 4) that best fitsthe residual, and (3) update the base model byadding the optimal basis function (line 5).
Thetwo model adaptation algorithms that will bedescribed below follow the same 3-step adapta-tion procedure.
They only differ in the choice ofh.
In the LambdaBoost algorithm (Section 4.1) h1 Set F0(x) be the background ranking model2 for m = 1 to M do3 ???
= ???
?
?
,?
????
???
?
=??
?1 ?, for i = 1?
N4 (??
,?? )
= argmin?
,????
?
??(??)2?
?=15 ??
?
= ??
?1 ?
+ ???(?
)Figure 1.
The generic boosting algorithm for modeladaptation507is defined as a single feature, and in LambdaS-MART (Section 4.2), h is a regression tree.Now, we describe the way residual is com-puted, the step that is identical in both algo-rithms.
Intuitively, the residual, denoted by y?
(line 3 in Figure 1), measures the amount of er-rors (or loss) the base model makes on the train-ing samples.
If the loss function in Equation (3) isdifferentiable, the residual can be computedeasily as the negativegradient of the loss function.
As discussed inSection 2, we want to directly optimize theNDCD, whose gradient is approximated via the?-function.
Following Burges et al (2006), thegradient of a training sample (xi, yi), where xi is afeature vector representing the query-documentpair (qi, di), w.r.t.
the current base model is com-puted by marginalizing the ?-functions of alldocument pairs, (di, dj), of the query, qi, as???
= ?NDCG ?????????,???
(5)where ?NDCG is the NDCG gained by swappingthose two documents (after sorting all docu-ments by their current scores);  ???
?
??
?
??
is thedifference in ranking scores of di and dj given qi;and Cij is the cross entropy cost defined as???
?
?
???
= ??
?
?
?+ log(1 + exp(??
?
??
)).
(6)Thus, we have???????
?=?11 + exp ???.
(7)This ?-function essentially uses the cross en-tropy cost to smooth the change in NDCG ob-tained by swapping the two documents.
A keyintuition behind the ?-function is the observationthat NDCG does not treat all pairs equally; forexample, it costs more to incorrectly order a pair,where the irrelevant document is ranked higherthan a highly relevant document, than it does toswap a moderately relevant/not relevant pair.4.1 The LambdaBoost AlgorithmIn LambdaBoost, the basis function h is definedas a single feature (i.e., an element feature in thefeature vector x).
The algorithm is summarizedin Figure 2.
It iteratively adapts a backgroundmodel to training data using the 3-step proce-dure, as in Figure 1.
Step 1 (line 3 in Figure 2) hasbeen described.Step 2 (line 4 in Figure 2) finds the optimalbasis function h, as well as its optimal coefficient?, that best fits the residual according to theleast-squares (LS) criterion.
Formally, let h and ?denote the candidate basis function and its op-timal coefficient.
The LS error on training datais ??
?;?
=   ???
?
??
?
?=02, where ???
is com-puted as Equation (5).
The optimal coefficient ofh is estimated by solving the equation ?
???
???=1??2/??=0.
Then, ?
is computed as?
=????(??)??=1?(??)??=1.
(8)Finally, given its optimal coefficient ?, the op-timal LS loss of h is??
?;?
= ???
?
?????=1?????
????=12?2(??)??=1.
(9)Step 3 (line 5 in Figure 2) updates the basemodel by adding the chosen optimal basis func-tion with its optimal coefficient.
As shown inStep 2, the optimal coefficient of each candidatebasis function is computed when the basis func-tion is evaluated.
However, adding the basisfunction using its optimal efficient is prone tooverfitting.
We thus add a shrinkage coefficient 0< ?
< 1 ?
the fraction of the optimal line steptaken.
The update equation is thus rewritten inline 5 in Figure 2.Notice that if the background model containsall the input features in x, then LambdaBoostdoes not add any new features but adjust theweights of existing features.
If the backgroundmodel does not contain all of the input features,then LambdaBoost can be viewed as a featureselection method, similar to Collins (2000), whereat each iteration the feature that has the largestimpact on reducing training loss is selected andadded to the background model.
In either case,LambdaBoost adapts the background model byadding a model whose form is a (weighted) li-near combination of input features.
The propertyof linearity makes LambdaBoost robust and lesslikely to overfit in Web search applications.
Butthis also limits the adaptation capacity.
A simplemethod that allows us to go beyond linearadaptation is to define h as nonlinear terms of theinput features, such as regression trees inLambdaSMART.4.2 The LambdaSMART AlgorithmLambdaSMART was originally proposed in Wuet al (2008).
It is built on MART (Friedman, 2001)but uses the ?-function (Burges et a., 2006) to1 Set F0(x) to be the background ranking model2 for m = 1 to M do3 compute residuals according to Equation (5)4 select best hm (with its best ?m), according to LS,computed by Equations (8) and (9)5 ??
?
= ??
?1 ?
+ ????(?
)Figure 2.
The LambdaBoost algorithm for model adaptation.508compute gradients.
The algorithm is summa-rized in Figure 3.
Similar to LambdaBoost, ittakes M rounds, and at each boosting iteration, itadapts the background model to training datausing the 3-step procedure.
Step 1 (line 3 in Fig-ure 3) has been described.Step 2 (lines 4 to 6) searches for the optimalbasis function h to best fit the residual.
UnlikeLambdaBoost where there are a finite number ofcandidate basis functions, the function space ofregression trees is infinite.
We define h as a re-gression tree with L terminal nodes.
In line 4, aregression tree is built using Mean Square Errorto determine the best split at any node in the tree.The value associated with a leaf (i.e., terminalnode) of the trained tree is computed first as theresidual (computed via ?-function) for the train-ing samples that land at that leaf.
Then, sinceeach leaf corresponds to a different mean, aone-dimensional Newton-Raphson line step iscomputed for each leaf (lines 5 and 6).
These linesteps may be simply computed as the derivativesof the LambdaRank gradients w.r.t.
the modelscores si.
Formally, the value of the l-th leaf, ?ml,is computed as???
=??????????????
?, (10)where ???
is the residual of training sample i,computed in Equation (5), and  ??
is the deriva-tive of ???
, i.e., ??
= ????/??(??
).In Step 3 (line 7), the regression tree is addedto the current base model, weighted by theshrinkage coefficient 0 < ?
< 1.Notice that since a regression tree can beviewed as a complex feature that combines mul-tiple input features, LambdaSMART can be usedas a feature generation method.
LambdaSMARTis arguably more powerful than LambdaBoost inthat it introduces new complex features and thusadjusts not only the parameters but also thestructure of the background model1.
However,1  Note that in a sense our proposed LambdaBoostalgorithm is the same as LambdaSMART, but using asingle feature at each iteration, rather than a tree.
Inparticular, they share the trick of using the Lambdaone problem of trees is their high variance.Often a small change in the data can result in avery different series of splits.
As a result,tree-based ranking models are much less robustto noise, as we will show in our experiments.
Inaddition to the use of shrinkage coefficient 0 < ?< 1, which is a form of model regularizationaccording to Hastie, et al, (2001), we will ex-plore in Section 5.3 other methods of improvingthe model robustness, including randomizationand using shallow trees.5 Experiments5.1 The DataWe evaluated the ranking model adaptationmethods on two Web search domains, namely (1)a name query domain, which consists of onlyperson name queries, and (2) a Korean querydomain, which consists of queries that userssubmitted to the Korean market.For each domain, we used two in-domaindata sets that contain queries sampled respec-tively from the query log of a commercial Websearch engine that were collected in twonon-overlapping periods of time.
We used themore recent one as open test set, and split theother into three non-overlapping data sets,namely training, validation and closed test sets,respectively.
This setting provides a good si-mulation to the realistic Web search scenario,where the rankers in use are usually trained onearly collected data, and thus helps us investigatethe robustness of these model adaptation me-thods.The statistics of the data sets used in our per-son name domain adaptation experiments areshown in Table 1.
The names query set serves asthe adaptation domains, and Web-1 as the back-ground domain.
Since Web-1 is used to train abackground ranker, we did not split it totrain/valid/test sets.
We used 416 input featuresin these experiments.For cross-domain adaptation experimentsfrom non-Korean to Korean markets, Koreandata serves as the adaptation domain, and Eng-lish, Chinese, and Japanese data sets as thebackground domain.
Again, we did not split thedata sets in the background domain totrain/valid/test sets.
The statistics of these datasets are shown in Table 2.
We used 425 inputfeatures in these experiments.gradients to learn NDCG.1 Set F0(x) to be the background ranking model2 for m = 1 to M do3 compute residuals according to Equation (5)4 create a  L-terminal node tree, ??
?
???
?=1?
?5 for l = 1 to L do6 compute the optimal ?lm according to Equation(10), based on approximate Newton step.7 ??
?
= ??
?1 ?
+ ?
???1(?
?
???
)?=1?
?Figure 3.
The LambdaSMART algorithm for model adaptation.509In each domain, the in-domain training data isused to train in-domain rankers, and the back-ground data for background rankers.
Validationdata is used to learn the best training parametersof the boosting algorithms, i.e., M, the totalnumber of boosting iterations, ?, the shrinkagecoefficient, and L, the number of leaf nodes foreach regression tree (L=1 in LambdaBoost).Model performance is evaluated on theclosed/open test sets.All data sets contain samples labeled on a5-level relevance scale, 0 to 4, with 4 as mostrelevant and 0 as irrelevant.
The performance ofrankers is measured through NDCG evaluatedagainst closed/open test sets.
We report NDCGscores at positions 1, 3 and 10, and the averagedNDCG score (Ave-NDCG), the arithmetic meanof the NDCG scores at 1 to 10.
Significance test(i.e., t-test) was also employed.5.2 Model Adaptation ResultsThis section reports the results on two adapta-tion experiments.
The first uses a large set ofWeb data, Web-1, as background domain anduses the name query data set as adaptation data.The results are summarized in Tables 3 and 4.We compared the three model adaptation me-thods against two baselines: (1) the backgroundranker (Row 1 in Tables 3 and 4), a 2-layerLambdaRank model with 15 hidden nodes and alearning rate of 10-5 trained on Web-1; and (2) theIn-domain Ranker (Row 2), a 2-layer Lambda-Rank model with 10 hidden nodes and a learningrate of 10-5 trained on Names-1-Train.
We builttwo interpolated rankers.
The 2-way interpo-lated ranker (Row 3) is a linear combination ofthe two baseline rankers, where the interpolationweights were optimized on Names-1-Valid.
Tobuild the 3-way interpolated ranker (Row 4), welinearly interpolated three rankers.
In additionto the two baseline rankers, the third ranker istrained on an augmented training data, whichwas created using the kNN method described inSection 3.In LambdaBoost (Row 5) and LambdaSMART(Row 6), we adapted the background ranker toname queries by boosting the background rankerwith Names-1-Train.
We trained LambdaBoostwith the setting M = 500, ?
= 0.5, optimized onNames-1-Valid.
Since the background rankeruses all of the 416 input features, in each boostingiteration, LambdaBoost in fact selects one exist-ing feature in the background ranker and adjustsits weight.
We trained LambdaSMART with M =500, L = 20, ?
= 0.5, optimized on Names-1-Valid.We see that the results on the closed test set(Table 3) are quite different from the results onthe open test set (Table 4).
The in-domain rankeroutperforms the background ranker on theclosed test set, but underperforms significantlythe background ranker on the open test set.
Theinterpretation is that the training set and theclosed test set are sampled from the same dataset and are very similar, but the open test set is avery different data set, as described in Section 5.1.Similarly, on the closed test set, LambdaSMARToutperforms LambdaBoost with a big margindue to its superior adaptation capacity; but onthe open test set their performance difference ismuch smaller due to the instability of the trees inLambdaSMART, as we will investigate in detaillater.
Interestingly, model interpolation, thoughsimple, leads to the two best rankers on the opentest set.
In particular, the 3-way interpolatedranker outperforms the two baseline rankersColl.
Description  #qry.
# url/qryWeb-1 Background training data 31555 134Names-1-Train In-domain training data(adaptation data)5752 85Names-1-Valid In-domain validation data 158 154Names-1-Test Closed test data 318 153Names-2-Test Open test data 4370 84Table 1.
Data sets in the names query domain experiments,where # qry is number of queries, and # url/qry is numberof documents per query.Coll.
Description  # qry.
# url/qryWeb-En Background En training data 6167 198Web-Ja Background Ja training data 45012 58Web-Cn Background Ch training data 32827 72Kokr-1-Train In-domain Ko training data(adaptation data)3724 64Kokr-1-Valid In-domain validation data 334 130Kokr-1-Test Korean closed test data 372 126Kokr-2-Test Korean open test data 871 171Table 2.
Data sets in the Korean domain experiments.# Models NDCG@1 NDCG@3 NDCG@10 AveNDCG1 Back.
0.4575 0.4952 0.5446 0.50922 In-domain 0.4921 0.5296 0.5774 0.54333 2W-Interp.
0.4745 0.5254 0.5747 0.53914 3W-Interp.
0.4829 0.5333 0.5814 0.54545 ?-Boost 0.4706 0.5011 0.5569 0.51926 ?-SMART 0.5042 0.5449 0.5951 0.5623Table 3.
Close test results on Names-1-Test.# Models NDCG@1 NDCG@3 NDCG@10 AveNDCG1 Back.
0.5472 0.5347 0.5731 0.55102 In-domain 0.5216 0.5266 0.5789 0.54723 2W-Interp.
0.5452 0.5414 0.5891 0.56044 3W-Interp.
0.5474 0.5470 0.5951 0.56615 ?-Boost 0.5269 0.5233 0.5716 0.54286 ?-SMART 0.5200 0.5331 0.5875 0.5538Table 4.
Open test results on Names-2-Test.510significantly (i.e., p-value < 0.05 according tot-test) on both the open and closed test sets.The second adaptation experiment involvesdata sets from several languages (Table 2).2-layer LambdaRank baseline rankers were firstbuilt from Korean, English, Japanese, and Chi-nese training data and tested on Korean test sets(Tables 5 and 6).
These baseline rankers thenserve as in-domain ranker and backgroundrankers for model adaptation.
For model inter-polation (Tables 7 and 8), Rows 1 to 4 are three2-way interpolated rankers built by linearly in-terpolatingeach of the three background rankers with thein-domain ranker, respectively.
Row 4 is a 4-wayinterpolated ranker built by interpolating thein-domain ranker with the three backgroundrankers.
For LambdaBoost (Tables 9 and 10) andLambdaSMART (Tables 11 and 12), we used thesame parameter settings as those in the namequery experiments, and adapted the three back-ground rankers, to the Korean training data,Kokr-1-Train.The results in Tables 7 to 12 confirm what welearned in the name query experiments.
Thereare three main conclusions.
(1) Model interpola-tion is an effective method of ranking modeladaptation.
E.g., the 4-way interpolated rankeroutperforms other ranker significantly.
(2)LambdaSMART is the best performer on theclosed test set, but its performance drops signif-icantly on the open test set due to the instabilityof trees.
(3) LambdaBoost does not use trees.
Soits modeling capacity is weaker than Lamb-daSMART (e.g., it always underperformsLambdaSMART significantly on the closed testsets), but it is more robust due to its linearity (e.g.,it performs similarly to LambdaSMART on theopen test set).5.3 Robustness of Boosting AlgorithmsThis section investigates the robustness issueof the boosting algorithms in more detail.
Wecompared LambdaSMART with different valuesof L (i.e., the number of leaf nodes), and with andwithout randomization.
Our assumptions are (1)allowing more leaf nodes would lead to deepertrees, and as a result, would make the resultingranking models less robust; and (2) injectingrandomness into the basis function (i.e.
regres-sion tree) estimation procedure would improvethe robustness of the trained models (Breiman,2001; Friedman, 1999).
In LambdaSMART, therandomness can be injected at different levels oftree construction.
We found that the most effec-tive method is to introduce the randomness atthe node level (in Step 4 in Figure 3).
Before eachnode split, a subsample of the training data and asubsample of the features are drawn randomly.
(The sample rate is 0.7).
Then, the two randomlyselected subsamples, instead of the full samples,are used to determine the best split.# Ranker NDCG@1 NDCG@3 NDCG@10 AveNDCG1 Back.
(En) 0.5371 0.5413 0.5873 0.56162 Back.
(Ja) 0.5640 0.5684 0.6027 0.58083 Back.
(Cn) 0.4966 0.5105 0.5761 0.53934 In-domain  0.5927 0.5824 0.6291 0.6055Table 5.
Close test results of baseline rankers, on Kokr-1-Test# Ranker NDCG@1 NDCG@3 NDCG@10 AveNDCG1 Back.
(En) 0.4991 0.5242 0.5397 0.52782 Back.
(Ja) 0.5052 0.5092 0.5377 0.51943 Back.
(Cn) 0.4779 0.4855 0.5114 0.49424 In-domain  0.5164 0.5295 0.5675 0.5430Table 6.
Open test results of baseline rankers, on Kokr-2-Test# Ranker NDCG@1 NDCG@3 NDCG@10 AveNDCG1 Interp.
(En) 0.5954 0.5893 0.6335 0.60882 Interp.
(Ja) 0.6047 0.5898 0.6339 0.61163 Interp.
(Cn) 0.5812 0.5807 0.6268 0.60244 4W-Interp.
0.5878 0.5870 0.6289 0.6054Table 7.
Close test results of interpolated rankers, onKokr-1-Test.# Ranker NDCG@1 NDCG@3 NDCG@10 AveNDCG1 Interp.
(En) 0.5178 0.5369 0.5768 0.55002 Interp.
(Ja) 0.5274 0.5416 0.5788 0.55313 Interp.
(Cn) 0.5224 0.5339 0.5766 0.54874 4W-Interp.
0.5278 0.5414 0.5823 0.5549Table 8.
Open test results of interpolated rankers, onKokr-2-Test.# Ranker NDCG@1 NDCG@3 NDCG@10 AveNDCG1 ?-Boost (En) 0.5757 0.5716 0.6197 0.59352 ?-Boost (Ja) 0.5801 0.5807 0.6225 0.59823 ?-Boost (Cn)  0.5731 0.5793 0.6226 0.5972Table 9.
Close test results of ?-Boost rankers, on Kokr-1-Test.# Ranker NDCG@1 NDCG@3 NDCG@10 AveNDCG1 ?-Boost (En) 0.4960 0.5203 0.5486 0.52812 ?-Boost (Ja) 0.5090 0.5167 0.5374 0.52333 ?-Boost (Cn)  0.5177 0.5324 0.5673 0.5439Table 10.
Open test results of ?-Boost rankers, on Kokr-2-Test.# Ranker NDCG@1 NDCG@3 NDCG@10 AveNDCG1 ?-SMART(En)0.6096 0.6057 0.6454 0.62382 ?- SMART(Ja)0.6014 0.5966 0.6385 0.61723 ?- SMART(Cn)0.5955 0.6095 0.6415 0.6209Table 11.
Close test results of ?-SMART rankers, onKokr-1-Test.# Ranker NDCG@1 NDCG@3 NDCG@10 AveNDCG1 ?- SMART(En)0.5177 0.5297 0.5563 0.53912 ?- SMART(Ja)0.5205 0.5317 0.5522 0.53683 ?- SMART(Cn)0.5198 0.5305 0.5644 0.5410Table 12.
Open test results of ?-SMART rankers, onKokr-2-Test.511We first performed the experiments on namequeries.
The results on the closed and open test setsare shown in Figures 4 (a) and 4 (b), respectively.The results are consistent with our assumptions.There are three main observations.
First, the graybars in Figures 4 (a) and 4 (b) (boosting withoutrandomization) show that on the closed test set, asexpected, NDCG increases with the value of L, butthe correlation does not hold on the open test set.Second, the black bars in these figures (boostingwith randomization) show that in both closed andopen test sets, NDCG increases with the value of L.Finally, comparing the gray bars with their cor-responding black bars, we see that randomizationconsistently improves NDCG on the open test set,with a larger margin of gain for the boosting algo-rithms with deeper trees (L > 5).These results are very encouraging.
Randomi-zation seems to work like a charm.
Unfortunately,it does not work well enough to help the boostingalgorithm beat model interpolation on the open testsets.
Notice that all the LambdaSMART resultsreported in Section 5.2 use randomization with thesame sampling rate  of 0.7.
We repeated the com-parison in the cross-domain adaptation experi-ments.
As shown in Figure 4, results in 4 (c) and 4(d) are consistent with those on names queries in 4(b).
Results in 4 (f) show a visible performance dropfrom LambdaBoost to LambdaSMART with L = 2,indicating again the instability of trees.6 Conclusions and Future WorkIn this paper, we extend two classes of modeladaptation methods (i.e., model interpolation anderror-driven learning), which have been well stu-died in statistical language modeling for speechand natural language applications (e.g., Bacchianiet al, 2004; Bellegarda, 2004; Gao et al, 2006), toranking models for Web search applications.We have evaluated our methods on two adap-tation experiments over a wide variety of datasetswhere the in-domain datasets bear different levelsof similarities to their background datasets.
Wereach different conclusions from the results of theopen and close tests, respectively.
Our open testresults show that in the cases where the in-domaindata is dramatically different from the backgrounddata, model interpolation is very robust and out-performs the baseline and the error-driven learningmethods significantly; whereas our close test re-sults show that in the cases where the in-domaindata is similar to the background data, the tree-based boosting algorithm (i.e.
LambdaSMART) isthe best performer, and achieves a significant im-provement over the baselines.
We also show thatthese different conclusions are largely due to theinstability of the use of trees in the boosting algo-rithm.
We thus explore several methods of im-proving the robustness of the algorithm, such asrandomization, regularization, using shallow trees,with limited success.
Of course, our experiments,(a)  (b)(c)  (d)  (e)Figure 4.
AveNDCG results (y-axis) of LambdaSMART with different values of L (x-axis), where L=1 is LambdaBoost; (a) and (b) arethe results on closed and open tests using Names-1-Train as adaptation data, respectively;  (d),  (e) and (f) are the results on theKorean open test set, using background models trained on Web-En, Web-Ja, and Web-Cn data sets, respectively.0.490.500.510.520.530.540.550.560.571 2 4 10 200.530.540.540.550.551 2 4 10 200.500.510.520.530.540.551 2 4 10 200.490.500.510.520.530.541 2 4 10 200.510.520.530.540.551 2 4 10 20512described in Section 5.3, only scratch the surface ofwhat is possible.
Robustness deserves more inves-tigation and forms one area of our future work.Another family of model adaptation methodsthat we have not studied in this paper is transferlearning, which has been well-studied in the ma-chine learning community (e.g., Caruana, 1997;Marx et al, 2008).
We leave it to future work.To solve the issue of inadequate training data, inaddition to model adaptation, researchers havealso been exploring the use of implicit user feed-back data (extracted from log files) for rankingmodel training (e.g., Joachims et al, 2005; Radlinskiet al, 2008).
Although such data is very noisy, it isof a much larger amount and is cheaper to obtainthan human-labeled data.
It will be interesting toapply the model adaptation methods described inthis paper to adapt a ranker which is trained on alarge amount of automatically extracted data to arelatively small amount of human-labeled data.AcknowledgmentsThis work was done while Yi Su was visiting Mi-crosoft Research, Redmond.
We thank Steven Yao'sgroup at Microsoft Bing Search for their help withthe experiments.ReferencesBacchiani, M., Roark, B. and Saraclar, M. 2004.Language model adaptation with MAP estima-tion and the Perceptron algorithm.
InHLT-NAACL, 21-24.Bellegarda, J. R. 2004.
Statistical language modeladaptation: review and perspectives.
SpeechCommunication, 42: 93-108.Breiman, L. 2001.
Random forests.
Machine Learning,45, 5-23.Bishop, C.M.
1995.
Training with noise is equiva-lent to Tikhonov regularization.
Neural Computa-tion, 7, 108-116.Burges, C. J., Ragno, R., & Le, Q. V. 2006.
Learningto rank with nonsmooth cost functions.
In ICML.Burges, C., Shaked, T., Renshaw, E., Lazier, A.,Deeds, M., Hamilton, and Hullender, G. 2005.Learning to rank using gradient descent.
InICML.Caruana, R. 1997.
Multitask learning.
MachineLearning, 28(1): 41-70.Collins, M. 2000.
Discriminative reranking for nat-ural language parsing.
In ICML.Donmea, P., Svore, K. and Burges.
2008.
On thelocal optimality for NDCG.
Microsoft TechnicalReport, MSR-TR-2008-179.Friedman, J.
1999.
Stochastic gradient boosting.Technical report, Dept.
Statistics, Stanford.Friedman, J.
2001.
Greedy function approximation:a gradient boosting machine.
Annals of Statistics,29(5).Gao, J., Qin, H., Xia, X. and Nie, J-Y.
2005.
Lineardiscriminative models for information retrieval.In SIGIR.Gao, J., Suzuki, H. and Yuan, W. 2006.
An empiricalstudy on language model adaptation.
ACM Transon Asian Language Processing, 5(3):207-227.Hastie, T., Tibshirani, R. and Friedman, J.
2001.
Theelements of statistical learning.
Springer-Verlag,New York.Jarvelin, K. and Kekalainen, J.
2000.
IR evaluationmethods for retrieving highly relevant docu-ments.
In SIGIR.Joachims, T., Granka, L., Pan, B., Hembrooke, H.and Gay, G. 2005.
Accurately interpreting click-through data as implicit feedback.
In SIGIR.Marx, Z., Rosenstein, M.T., Dietterich, T.G.
andKaelbling, L.P. 2008.
Two algorithms for transferlearning.
To appear in Inductive Transfer: 10 yearslater.Press, W. H., S. A. Teukolsky, W. T. Vetterling andB.
P. Flannery.
1992.
Numerical Recipes In C.Cambridge Univ.
Press.Radlinski, F., Kurup, M. and Joachims, T. 2008.How does clickthrough data reflect retrievalquality?
In CIKM.Thrun, S. 1996.
Is learning the n-th thing any easierthan learning the first.
In NIPS.Wu, Q., Burges, C.J.C., Svore, K.M.
and Gao, J.2008.
Ranking, boosting, and model adaptation.Technical Report MSR-TR-2008-109, MicrosoftResearch.513
