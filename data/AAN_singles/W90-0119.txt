Reso lv ing  P lan  Ambigu i ty  for  Response  Generat ionPeter van Beek and Robin CohenDepartment of Computer ScienceUniversity of WaterlooWaterloo, OntarioCANADA N2L 3G1AbstractRecognizing the plan underlying a query aids in thegeneration of an appropriate response.
In this paper,we address the problem of how to generate coopera-tive responses when the user's plan is ambiguous.
Weshow that it is not always necessary to resolve theambiguity, and provide a procedure that estimateswhether the ambiguity matters to the task of formu-lating a response.
If the ambiguity does matter, wepropose to resolve the ambiguity by entering into aclarification dialogue with the user and provide a pro-cedure that performs this task.
Together, these pro-cedures allow a question-answering system to takeadvantage of the interactive and collaborative natureof dialogue in recognizing plans and resolving ambi-guity.IntroductionSomewhat obviously, plan recognition is the processof inferring an agent's plan from observation of theagent's actions.
The agent's actions can be physicalactions or speech actions.
Four principal methodsfor plan recognition have been proposed in theliterature.
The methods are plausible inference(Allen \[1\], Carberry \[2\], Litman \[15\], Sidner \[25\]),parsing (Huff and Lesser \[9\]), circumscribing ahierarchical representation f plans and using deduc-tion (Kautz \[12, 13\]), and abduction (Charniak andMcDermott \[6\], Konolige and Pollack \[14\], Poole\[24\]).Our particular interest is in the use of plan recog-nition in question-answering systems, where recog-nizing the plan underlying a user's queries aids in thegeneration of an appropriate response.
Here, theplans and goals of the user, once recognized, havebeen used to: supply more information than is expli-citly requested (Allen \[1\], Luria \[16\]), handle prag-matically ill-formed queries and resolve some inter-sentential ellipses (Carberry \[2, 3, 4\]), provide anexplanation from the appropriate perspective(McKeown et hi.
\[17\]), respond to queries that resultfrom an invalid plan (Pollack \[20, 21, 22\]), and avoidmisleading responses and produce user-specificcooperative responses (Joshi et a\].
\[10, 11\], van Beckand Cohen \[26, 27\], Cohen et al \[7\]).Example  1 (Joshi et al \[11\]).
As an example ofa cooperative response consider the followingexchange between student and student-advisor sys-tem.
The plan of the student is to avoid failing thecourse by dropping it.User: Can I drop numerical analysis?System: Yes, however you will still fail the coursesince your mark will be recorded as with-drawal while failing.If the system just gives the direct answer, "Yes" thestudent will remain unaware that the plan is faulty.The more cooperative answer warns the student.An important weakness of this work in responsegeneration, however, is the reliance on a plan recog-nition component being able to uniquely determinethe plan of the user.
This is clearly too strong anassumption as the user's actions often will be con-sistent with more than one plan, especially after onlyone or a few utterances when there is insufficientcontext to help decide the plan of the user.
InExample 1 there are many reasons why a studentmay want to drop a course, such as resolving ascheduling conflict, avoiding failing the course, orfinding the material uninteresting.
There may be noreason to prefer one alternative over the other, yetwe may still want to generate a response that doesmore than just give a direct answer to the user'squery.In this paper, we address the problem of what thesystem should do when the user's actions are ambi-guous as they are consistent with more than oneplan.
To the extent that this problem has been con-sidered by researchers in plan recognition, it is gen-erally assumed that the plan recognition systemovercomes the ambiguity problem by inferring themost likely interpretation, given its assessment of thecontext and dialogue so far and knowledge of typicalplans of action.
Thus there is a dependence on sali-ence heuristics to solve the ambiguity problem \[e.g.1, 2, 17, and see the final section\].
Existing propo-sals for resolving ambiguity beyond heuristics areunderspecified and what usually underlies these pro-posals is the assumption that we always want todetermine one unique plan \[2, 15, 19, 25\].We show how to relax the assumption that theplan recognition component returns a single plan.That is, given that the result of the plan recognitionphase will usually be a disjunction of possible plans,we show how to design a response component ogenerate cooperative responses given the disjunction.We show that it is not always necessary to resolveambiguity, and provide a procedure that allows the144response component o estimate whether the ambi-guity matters to the task of formulating a response.If the ambiguity does not matter, the response com-ponent can continue to answer the user's queries andignore the ambiguity in the underlying goals andplan until further queries help clarify which plan theuser is pursuing.
If the ambiguity does matter, thesystem should take advantage of the interactive andcollaborative nature of dialogue in recognizing plansand resolving ambiguity.
A key contribution of thiswork therefore is providing a clear criterion forwhen to respond to a question with a question thatwill differentiate between some of the possibilities.We also propose a specific solution to what questionsshould then be asked of the user.
Moreover, thesequestions are asked only to resolve the ambiguity tothe point where it no longer matters (this is notnecessarily to a unique plan).Example  2.
Here are two different examples togive a flavor of what we are proposing.
There aretwo agents: a cook and an expert who is cooperative,helpful, and adept at recognizing plans.
The expertobserves the cook making marinara sauce and recog-nizes the cook could be pursuing three possibleplans, all with the same top level goal of preparing ameal: make fettucini marinara or spaghetti marinara(both a pasta dish) or chicken marinara (a meatdish).a.
Suppose the cook then asks the expert: "Is a redwine a good choice?"
The expert has the criteria forwine selection that red wine should be served if themeal is chicken, fettucini marinara, or spaghettimarinara and white if fettucini alfredo.
There isenough information for the expert to decide that redwine should be bought and the ambiguity does notneed to be resolved to cooperatively answer thequestion.b.
Now suppose the expert also knows that theguest of honor is allergic to gluten and so would notbe able to eat if a pasta dish was served.
Here theambiguity is important as the expert has recognizedthat the cook's prepare-a-meal goal may conflictwith the cook's entertain-important-guest goal.
Theexpert will want to resolve the ambiguity enough tohe assured that the proposed meal does not include apasta dish and so clarifies this with the cook.Est imat ing Whether  theAmbigu i ty  MattersExample 2, above, showed that sometimes it isnecessary to resolve ambiguity and sometimes it isnot.
Here we give criteria for judging which is thecase.
The result is a procedure that allows theresponse component o estimate whether the ambi-guity matters to the task of formulating a response.Assuming we can answer the user's query, decidingwhen we want to give more than just a directanswer to the user's query depends on the plan ofthe user.
Previous work has identified several kindsof faults in a plan that a cooperative response shouldwarn a user about.
We generalize this work in iden-tifying faults and call it plan critiquing.
Our propo-sal therefore is to first apply a plan critiquing pro-cedure to determine possible faults.Plan CritiquingA plan may be labeled as faulty if it will fail toachieve its high level goal (e.g.
Allen \[1\]) or if thereare simply better alternatives for reaching that goal(e.g.
Pollack \[20\]).
Joshi et al \[10, 11\] formalize theabove and identify additional cases (such as warningthe user that a certain plan is the only way toachieve a goal).In \[26, 27\], we make some extensions to Joshi et al\[10, 11\] and give a procedure to determine faults inplans and to address these faults through coopera-tive responses.
Faults include both plans which failand plans which can be replaced by better alterna-tives.
In \[26, 27\], we also include the critical exten-sion of domain goals.
To  explain, the system needsto not only respond to goals inferred from thecurrent discourse but also to the domain goals a useris likely to have or known to have even though theyare not stated in, or derivable from, the discourse.Example 3.User: I'm not interested in the material and sowould like to drop the course.
Can I?The ideal response should say more than just "Yes",but also warn the student that the domain goal ofachieving a degree conflicts with the immediate goalof dropping the course.
This example shows howcompeting oals may exist and need to be addressedin a response.To determine whether the ambiguity matters, wepropose to apply an extension of the procedure of\[26, 27\], which will be sensitive to complementarygoals as well.
The standard assumption in coopera-tive response work is that the user is pursuing only asingle chain of goals; we allow actions to be used toachieve more than one complementary goal.
Forexample, in the course-advising domain we canassume that all users have the goal to avoid failing acourse.
If a user then asks "I'm not interested in thecourse and so would like to drop it.
Can I?"
theresponse should address not just the goal of avoidinguninteresting courses but also the complementarygoal of avoiding failing courses.
For example, theresponse should warn the user if he will fail thecourse because of withdrawal while failing (as inExample I).The algorithm to estimate whether the ambiguitymatters is below.
The deciding criterion is whetherthe critiques for all plans are the same.
By same cri-tique or same fault (Case 1. b. in the algorithm) wemean, for example, same better way or same conflictwith competing domain goal.145Input: A set of possible plans (the output from aplan recognition algorithm).Output: The set of possible plans with critiquesattached to each plan.Algorithm Critique:beginfor each plan in the set of possible plans docritique the plan and, if there is a fault, anno-tate the plan with the faultInput: A set of critiqued possible plans.Output: "Yes", the ambiguity matters, or "No"the ambiguity does not matter.Algorithm Ambiguity_matters:We are in one of the following two cases:Case 1.
"No", the ambiguity does not matter.The critiques are the same for all the plans.
Thatis, eithera.
every plan is faultless, orb.
every plan is annotated with the same fault.Case 2.
"Yes", the ambiguity does matter.The critiques are different for some or all of theplans.
That  is, eithera.
some, but not all, of the plans are faultless (thefaults may or may not be the same), orb.
every plan is annotated with a fault and thefaults are not all the same.endAn  Example  to  I l lustrate the P roposa lSuppose the user asks "Can I drop numericalanalysis?".
First, a plan recognition algorithm iscalled to determine the possible plans of the user.They are found to be:end ~ resolve schedule conflict --~ drop courseend ~ avoid uninteresting prof --~ drop courseSecond, algorithm Critique is called to critique theplans.
As a result, both plans are labeled with thesame fault that there is a better plan for achievingthe goal.
Third, Mgorithm Ambiguity_matter8 iscalled to determine whether the ambiguity regardingthe plan of the user matters to the task of formulat-ing a response.
It is found that the ambiguity doesnot matter  as both plans are annotated with thesame fault (Case 1.b of the algorithm).
Finally, thecritiqued plans are passed to a response generationprocedure.
The answer then given in this exampleis, "Yes, but a better way is to switch to anothersection.
"In general, in (Case 1.a) a response generation pro-cedure can just give a direct answer to the user'squery, and in (Case 1.b) can give a direct answerplus any warranted additional information, such astelling the user about the fault.In the above example it was found that the ambi-guity did not matter as there was enough informa-tion to generate a cooperative response.
If instead itwas found that the ambiguity did matter (Case 2 ofthe algorithm) we propose that we we enter into aclarification dialogue with the user to resolve theambiguity to the point where it no longer doesmatter.
That  is, until we are in Case 1.
A responsegeneration procedure would then be called.Clar i f i ca t ion  D ia logues :Quest ions  in Response  to  Quest ionsWhat should we ask the user when a clarification isnecessary?
Clearly, we do not want to simply listthe set of possible plans and ask which is being pur-sued.
Below is an algorithm that determines what tosay.
Our algorithm for estimating whether theambiguity matters is not dependent on the methodof plan recognition used.
Now our proposal for cla-rification dialogues is tied to a hierarchical planlibrary in the style of Kautz \[12\].
The input to thealgorithm is a set of possible plans where the user'saction is related to top-level or end goals throughchMns of goals.
Each plan in the set is annotatedwith a a critique.
The key idea is to ask about thehighest level possible, check whether the ambiguitystill needs to be further resolved, and if so, ask atthe next level down, iteratively, through the hierar-chy of goals.Input: A set of critiqued possible plans (the out-put from Mgorithm Critique).Output: The pruned set of possible plans such thatthe ambiguity no longer matters.Algorithm Clarify:begininitiMize the current level to be the first branchpoint from the top in the set of possible planswhile Ambiguity_matter8 = "Yes" doseparate out the distinct goals in the set ofremaining possible plans that are one levelbelow the current level and are annotated witha faultlist the goals (perhaps with their accompanyingannotations as justification for why we are ask-ing) and ask the user whether one of them ispart  of the plan being pursuedaccording to the answer, remove the plans thatare not being pursued from the set of possibleplans and update the current level in the hierar-chy that is being looked at to be the nextbranch pointend whi leend146next query.
.
.
.
.
~ .
.
.
.
.
.
.
.
- I  f" .
.
.
.
1I I I 'l' H res?lve H4" generate ' i 1. generate 2. resolve ii candidate ambiguity w. ' I I ambiguity response I"i---'i with user i plans heuristics iI II Plan Recognition Component i i Response Generation Component iL J L. JFig.
1.
Major modules of our proposed query-answering systemExample  2b (Revisited).
In our story in theintroduction about cooking for our allergic guest theexpert has recognized the following three plans:1. end ~ prepare meal ~ make meat dish ~ makechicken marinara ~ make marinara2.
end ~ prepare meal ~ make pasta dish ~ makefettucini marinara ~ make marinara3.
end ~ prepare meal ~ make pasta dish ~ makespaghetti marinara ~ make marinaraUsing the algorithm of the previous section, thethree plans are critiqued and it is found that theambiguity matters.
The plan involving a meat dishis found to be faultless but the two plans involving apasta dish are found to conflict with another goal ofthe cook: to entertain the guest.
Using the algo-rithm of this section, the question asked to resolvethe ambiguity would be "Are you making a pastadish (perhaps with justification of why we are ask-ing)?"
After either answer of yes or no we knowenough that the ambiguity no longer matters.
Notethat if we just ask the more general "What are youmaking?"
this allows such uninformative responses as"dinner" or just "you'll see".When asking a question we propose to ask aboutas high a level of goal as possible that still helps toresolve the ambiguity and to work top down.
A topdown approach is better as it provides a useful con-text for any later queries and makes users give us ananswer at a higher level than they may otherwise do.Moreover, the user may be mistaken about decompo-sitions or have some other wrong view of the domainand by stepping downward through the paths of pos-sible plans these misconceptions may be detected.Here is an example.
Suppose the user asks "Can Itake numerical analysis?".
The system recognizestwo plans.end ~ get_degree ~ B.A.
~ electives ~ courseend ~ get_degree ~ B.Sc.
~ required ~ coursea.
System: Are you pursuing a B.Sc.?b.
System: Are you trying to fulfill your elected orrequired courses?Question (a) is what our algorithm would ask.
Ques-tion (b) is what a procedure which uses a bottom upapproach would ask.
But (b) allows potential confu-sion to go undetected.
The user could answer"required", believing that the course is required fora B.A., for example.
Thus, we advocate Kautz styleplan recognition \[12\], as other plan recognitionmethods \[2, 15, 25\] would stop after branch pointspoints and thus could only propose electives andrequired as the two possible plans.
Question (b) isthe only question this previous work could ask theuser.Starting with the top most goals and workingdown may sometimes give as many questions as bot-tom up approaches.
However, others have notedthat bottom up dialogues are difficult to assimilatewithout misinterpretation \[2, p. 54\].
Therefore, wemaintain that the top down approach is more desir-able.
Moreover, some higher level questions, such asquestion (a), above, can be eliminated using goalsknown from the previous discourse or backgroundknowledge about the user.Current extensions we are examining includeallowing the user to have multiple goals so that morethan one path from top level goals to action may becorrect.
This requires resolving ambiguity in multi-ple paths through the set of possible plans.
Thiscould be done in a depth-first manner, using clue orredirection words to guide the user when we returnto resolve the ambiguity in the other branches.
Weare also investigating selecting the minimal subset ofthe possible plans from those with faults and thosewithout faults (at the moment he algorithm alwaystakes the subset with faults).Discuss ion  and Conc lus ionIn this section we summarize our proposals anddefend our position that this straightforward way ofdoing things is a good way.
With reference to Fig.1, we discuss the design of boxes 2, 3, and 4 and thetradeoffs involved between boxes 2 and 3.Box 2: Resolve the ambiguity with heuris-tics.
As mentioned earlier, many researchers haveproposed heuristics to prefer one plan over another\[1, 2, 17, 8, 18, 12, 23, 14\].
Some of these heuristicscan be incompatible with cooperative response147generation.
For example, Allen's \[1\] preferenceheuristics are generally incompatible with recogniz-ing and responding to faulty plans (such as theresponse in Example 1).
Because we are using planrecognition for response generation, this shouldaffect the design of Box 2 and therefore what getspassed to Box 3.Box 3: Reso lve  the  ambigu i ty  w i th  the  user.Previous work in response generation makes theassumption that what gets passed to the RG com-ponent is a single plan the PR 'component proposesthe user is pursuing.
We argue that, unless we arewilling to sometimes arbitrarily commit to one planinstead of another, there will be times when one plancannot be chosen over another and therefore therewill be ambiguity about which plan the user is pur-suing.
Result: we need a method to resolve theambiguity.
In plan recognition in a discourse setting(as opposed to key-hole recognition), the goals andplan the user holds are knowable simply by askingthe user.
But we do not want to always just ask if itis not necessary so we need to know when to start aclarification dialogue and what to say.
And when wedo ask, we want to ask the minimal number of ques-tions necessary to resolve the ambiguity until it nolonger matters.
To this end, box 3 contains a pro-cedure that estimates by plan critiquing whether theambiguity matters to the task of formulating aresponse.
If the ambiguity does not matter theresult is passed to box 4.
If the ambiguity doesmatter, a procedure is called that starts a clarifica-tion dialogue, responding to the user's question withquestions that iteratively differentiate between thepossibilities.Box 2 vs.
Box  3: The  t radeof fs .
Much previ-ous work in plan recognition makes the assumptionthat we want the PR component to commit to andreturn a single plan.
Carberry and McKeown, forexample, use a strong heuristic to commit to a singleplan \[2, 17\].
However, this means the system will attimes commit to the wrong plan.
Doing it this wayrequires the ability to handle natural languagedebugging dialogues.
Why we do not want to com-mit to a single plan and then, if we are wrong, repairusing a debugging dialogue?
Carberry \[5, p.4\] arguesthat a system will appear "unintelligent, obtuse, anduncooperative" if it engages in lengthy clarificationdialogues.
However, a procedure to perform adebugging dialogue is not specified and is, we specu-late, a difficult problem.
We argue for not commit-ting early.
Our hypothesis is that a clarificationdialogue is better than a debugging dialogue.
Thequestions in the clarification dialogues are simple toanswer, whereas determining that the system hasmisunderstood your goals and plan requires users toengage in plan recognition.
That is, users mustrecognize the plan the RG component is using fromits responses and note that it differs from theirplans.
Moreover, the user may not recognize we arewrong and be mislead.
Finally, we argue that, if thequestions are carefully chosen, the clarification dialo-gues need not be lengthy or too frequent.
Note thatpreference heuristics can still be used in ourapproach.
These would best be applied when toomany top level goals give an unwieldy clarificationquestion.Box 4: Generate  the  response .
Once Box 3 hasestimated that any remaining ambiguity does notmatter to generating a cooperative response, the dis-junction of possible plans is passed to Box 4.
Thereare two cases; both can now be handled as in previ-ous work except that there is now the additionalcomplication that we allow one action to contributeto more than one chain of goals.
The response com-ponent must then generate a conjunctive responsethat addresses each of the goals.1.Every plan is faultless, so we just give a directanswer to the user's query but ignore the underly-ing goals and plan until further queries help clarifywhich plan the user is pursuing, and2.
Every plan has the same fault, so we give a directanswer plus some additional information thatwarns the user about the deficiency and perhapssuggests ome alternatives ( ee \[10\], \[26\]).Soap Box:  This paper offers an important con-tribution to natural language generation.
Itdiscusses a clear criterion for when to initiate a cla-rification dialogue and proposes a specific solution towhat questions should be asked of the user toachieve clarification.
We believe that naturallanguage response generation systems should bedesigned to involve the user more directly and this issometimes achieved quite successfully with our pro-posals.There may be tradeoffs between overcommittingin the plan recognition process and engaging inlengthy clarification dialogue, particularly with alarge set of complex candidate plans.
This may sug-gest applying pruning heuristics more actively in theplan recognition process (Box 2) to reduce thenumber of questions asked in the clarification dialo-gue (Box 3).
For future work, these tradeoffs will beexamined more closely as we test the algorithmsmore extensively.Acknowledgements .
We thank Fei Song andBruce Spencer for comments on an earlier version ofthis paper and for many discussions about planrecognition.
Financial assistance was received fromthe Natural Sciences and Engineering ResearchCouncil of Canada (NSERC).References\[1\] Allen, J .
F .
1983.
Recognizing Intentions fromNatural Language Utterances.
In Computa-tional Models of Discourse, M. Brady and R.C.
Berwick (eds.).
MIT Press.\[2\] Carberry, S. 1985.
Pragmatic Modeling inInformation System Interfaces.
Ph.D. thesisavailable as University of Delaware Technical148Report 86-07, Newark, Del.\[3\] Carberry, S. 1988.
Modeling the User's Plansand Goals.
Computational Linguistics 14,23-27.\[4\] Carberry, S. 1989.
A Pragmatics-BasedApproach to Ellipsis Resolution.
ComputationalLinguistics 15, 75-96.\[5\] Carberry, S. 1989.
A New Look at PlanRecognition in Natural Language Dialogue.University of Delaware Technical Report 90-08,Newark, Del.\[6\] Charniak, E., and D. V. McDermott.
1985.Introduction to Artificial Intelligence.
AddisonWesley.\[71 Cohen, R., M. Jones, h. Sanmugasunderam, B.Spencer, and L. Dent.
1989.
ProvidingResponses Specific to a User's Goals and Back-ground.
International Journal of Expert Sys-tems: Research and Applications 2, 135-162.\[8\] Goldman, R., and E. Charniak.
1988.
A Proba-bilistic Assumption-Based Truth MaintenanceSystem for Plan Recognition.
Pros.
of theAAAI-88 Workshop on Plan Recognition, St.Paul, Minn.\[9\] Huff, K., and V. R. Lesser.
1982.
Knowledge-Based Command Understanding: An Examplefor the Software Development Environment.Computer and Information Sciences TechnicalReport 82-6, University of Massachusetts atAmherst, Amherst, Mass.
Cited in \[13\].\[10\] Joshi, A., B. Webber, and R. Weischedel.
1984.Living up to Expectations: Computing ExpertResponses.
Pros.
of the Third NationalConference on Artificial Intelligence, Austin,Tex., 169-175.\[11\] Joshi, A., B. Webber, and R. Weischedel.
1984.Preventing False Inferences.
Proc.
of lOthInternational Conference on ComputationalLinguistics (COLING), Stanford, Calif., 134-138.\[12\] Kantz, H. A.
1987.
A Formal Theory of PlanRecognition.
Ph.D. thesis available as Univer-sity of Rochester Technical Report 215, Roches-ter, N.Y.\[13\] Kautz, H. A., and J. F. Allen.
1986.
General-ized Plan Recognition.
Proc.
of the FifthNational Conference on Artificial Intelligence,Phil., Penn., 32-37.\[14\] Konolige, K., and M. E. Pollack.
1989.
Ascrib-ing Plans to Agents.
Pros.
of the EleventhInternational Joint Conference on ArtificialIntelligence, Detroit, Mich., 924-930.\[15\] Litman, D. J., and J. F. Allen.
1987.
A PlanRecognition Model for Subdialogue in Conversa-tions.
Cognitive Science 11, 163-200.\[16\] Luria, M. 1987.
Expressing Concern.
Proc.
ofthe 25th Conference of the Association forComputational Linguistics, Stanford, Calif.,221-227.\[17\] McKeown, K. R., M. Wish, and K. Matthews.1985.
Tailoring Explanations for the User.Pros.
of the Ninth International Joint Confer-ence on Artificial Intelligence, Los Angeles,Calif., 794-798.\[18\] Neufeld, E. 1989.
Defaults and Probabilities;Extensions and Coherence.
Pros.
of the FirstInternational Conference on Principles ofKnowledge Representation and Reasoning,Toronto, Ont., 312-323.\[19\] Perrault, C. R., J. F. Allen, and P. R. Cohen.1978.
Speech Acts as a Basis for UnderstandingDialogue Coherence.
Proc.
of the Pad Confer-ence on Theoretical lssues in NaturalLanguage Processing (TINLAP), Urbana-Champaign, Ill., 125-132.\[20\] Pollack, M. E. 1984.
Good Answers to BadQuestions: Goal Inference in Expert Advice-Giving.
Proc.
of the Fifth Canadian Confer-ence on Artificial Intelligence(CSCS//SCEIO}, London, Ont.\[21\] Pollack, M. E. 1986.
Inferring Domain Plans inQuestion-Answering.
Ph.D. thesis (Univ.
ofPenn.)
available as SRI International TechnicalNote 403, Menlo Park, Calif.\[22\] Pollack, M. E. 1986.
A Model of Plan Infer-ence that Distinguishes Between the Beliefs ofActors and Observers.
Pros.
of the PithConference of the Association for Computa-tional Linguistics, New York, N.Y., 207-214.\[23\] Pools, D. L. 1985.
On the Comparison ofTheories: Preferring the Most Specific Explana-tion.
Proc.
of the Ninth International JointConference on Artificial Intelligence, LosAngeles, Calif., 144-147.\[24\] Poole, D. L. 1989.
Explanation and Prediction:An Architecture for Default and Abductive Rea-soning.
Computational Intelligence 5, 97-110.\[25\] Sidner, C. L. 1985.
Plan Parsing for IntendedResponse Recognition in Discourse.
Computa-tional Intelligence 1, 1-10.\[26\] van Beek, P. 1987.
A Model For GeneratingBetter Explanations.
Proc.
of the P5th Confer-ence of the Association for ComputationalLinguistics, Stanford, Calif., 215-220.\[27\] van Beck, P., and R. Cohen.
1986.
TowardsUser-Specific Explanations from Expert Sys-tems.
Pros.
of the Sixth Canadian Conferenceon Artificial Intelligence (CSCSI/SCEIO),Montreal, Que., 194-198.149
