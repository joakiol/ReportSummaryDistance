An Estimate of an Upper Bound for theEntropy of EnglishPeter E Brown*Vincent J. Della Pietra*Robert L. Mercer*IBM T.J. Watson Research CenterStephen A. Della Pietra*Jennifer C. Lai*We present an estimate of an upper bound of 1.75 bits for the entropy of characters in printedEnglish, obtained by constructing a word trigram model and then computing the cross-entropybetween this model and a balanced sample of English text.
We suggest the well-known and widelyavailable Brown Corpus of printed English as a standard against which to measure progress inlanguage modeling and offer our bound as the first of what we hope will be a series of steadilydecreasing bounds.1.
IntroductionWe present an estimate of an upper bound for the entropy of characters in printedEnglish.
The estimate is the cross-entropy of the 5.96 million character Brown Corpus(Kucera and Francis 1967) as measured by a word trigram language model that weconstructed from 583 million words of training text.
We obtain an upper bound of 1.75bits per character.Since Shannon's 1951 paper, there have been a number of estimates of the entropyof English.
Cover and King (1978) list an extensive bibliography.
Our approach differsfrom previous work in that1.
We use a much larger sample of English text; previous estimates werebased on samples of at most a few hundred letters.2.
We use a language model to approximate he probabilities of characterstrings; previous estimates employed human subjects from whomprobabilities were elicited through various clever experiments.3.
We predict all printable ASCII characters.2.
MethodOur estimate for the entropy bound is based upon the well-known fact that the cross-entropy of a stochastic process as measured by a model is an upper bound on theentropy of the process.
In this section, we briefly review the relevant notions.2.1 Entropy, Cross-Entropy, and Text CompressionSuppose X = {... X-2, X- l ,  Xo, X1, X2...} is a stationary stochastic process over a finitealphabet.
Let P denote the probability distribution of X and let Ep denote xpectations* P.O.
Box 704, Yorktown Heights, NY 10598(~) 1992 Association for Computational LinguisticsComputational Linguistics Volume 18, Number 1with respect o P. The entropy of X is defined byH(X) =_ H(P) =_ -EplogP(Xo lX_I,X_2,...).
(1)If the base of the logarithm is 2, then the entropy is measured in bits.
It can be shownthat H(P) can also be expressed asH(P) = l im-Ep logP(Xo\ [X -1 ,X -2 , .
.
.
,X_n)  = l im-1EplogP(X1X2.
.
.Xn) .
(2)If the process is ergodic, then the Shannon-McMillan-Breiman theorem (Algoet andCover 1988) states that almost surelyH(P) = lim - 1 log P(X1X2... Xn).
(3)n---* cx~ y/Thus, for an ergodic process, an estimate of H(P) can be obtained from a knowledgeof P on a sufficiently long sample drawn randomly according to P.When P is not known, an upper bound to H(P) can still be obtained from anapproximation to P. Suppose that the stationary stochastic process M is a model forP.
The cross-entropy of P as measured by M is defined byH(P,M) =-- Ep logM(Xo lX_I,X_2,...).
(4)Under suitable regularity conditions, it can be shown thatH(P,M) = l im-Ep logM(Xo lX_ I ,X_E , .
.
.
,X_n)  = l im-1-EplogM(X1X2.. .Xn).n'--+ CX~ n"-*  DO n(s)If P is ergodic, then it can be shown that almost surely for PH(P, M) = lim - 1 logM(XIX2... Xn).
(6)n ---+ oo  nThe cross-entropy H(P, M) is relevant o us since it is an upper bound on theentropy H(P).
That is, for any model M,H(P) < H(P,M).
(7)The difference between H(P, M) and H(P) is a measure of the inaccuracy of the modelM.
More accurate models yield better upper bounds on the entropy.
Combining Equa-tions (6) and (7) we see that almost surely for P,H(P) < lim - 1 log M(X1X2... Xn).
(8)n---*OO nEntropy and cross-entropy can be understood from the perspective of text com-pression.
It is well known that for any uniquely decodable coding scheme (Cover andThomas 1991),Ep I(XIX2... Xn) ~ -Ep log e(XlX2... Xn) , (9)where I(X1X2...Xn) is the number of bits in the encoding of the string X1X2...Xn.Combining Equations (2) and (9), we see that H(P) is a lower bound on the averagenumber of bits per symbol required to encode a long string of text drawn from P:H(P) <__ lira 1Ep I(X1X2...Xn).
(10)n ---* oo  n32Brown et al An Estimate of an Upper Bound for the Entropy of EnglishOn the other hand, an arithmetic coding scheme (Bell, Cleary, and Witten 1990) usingmodel M will encode the sequence xlx2... Xn inIM(XlX2... Xn) = r -- logM(XlX2... Xn) + 11 (11)bits, where \[r\] denotes the smallest integer not less than r. Combining Equations (7)and (11) we see that H(P,M) is the number of bits per symbol achieved by usingmodel M to encode a long string of text drawn from P:H(P,M) = lim llM(X1X2...Xn).
(12)n---*oo n2.2 The Entropy BoundWe view printed English as a stochastic process over the alphabet of 95 printable ASCIIcharacters.
This alphabet includes, for example, all uppercase and lowercase l tters, alldigits, the blank, all punctuation characters, etc.
Using Equation (8) we can estimatean upper bound on the entropy of characters in English as follows:1.
Construct a language model M over finite strings of characters.2.
Collect a reasonably ong test sample of English text.3.
ThenH(English) <_ __1 log M(test sample),n(13)where n is the number of characters in the sample.We emphasize that for this paradigm to be reasonable, the language model Mmust be constructed without knowledge of the test sample.
Without his proscription, onemight, for example, construct a model that assigns probability one to the test sampleand zero to any other character string of the same length.
Even quite subtle use ofknowledge of the test sample can have a profound effect on the cross-entropy.
Forexample, the cross-entropy would be noticeably lower had we restricted ourselves tocharacters that appear in the test sample rather than to all printable ASCII characters,and would be lower still had we used the actual vocabulary of the test sample.
Butthese values could not be trumpeted as upper bounds to the entropy of English sinceEquation (13) would no longer be valid.3.
The Language ModelIn this section, we describe our language model.
The model is very simple: it capturesthe structure of English only through token trigram frequencies.
Roughly speaking,the model estimates the probability of a character sequence by dissecting the sequenceinto tokens and spaces and computing the probability of the corresponding tokensequence.
The situation is slightly more complicated than this since, for a fixed tokenvocabulary, some character sequences will not have any such dissection while otherswill have several.
For example, the sequence abc xyz might not have any dissectionwhile the sequence bedrock might be dissected as one token or as two tokens withoutan intervening space.We address the difficulty of sequences that cannot be dissected by introducing anunknown token that can account for any spelling.
We address the problem of multiple33Computational Linguistics Volume 18, Number 1dissections by considering the token sequences to be hidden.
The model generates asequence of characters in four steps:1.
It generates a hidden string of tokens using a token trigram model.2.
It generates a spelling for each token.3.
It generates a case for each spelling.4.
It generates a spacing string to separate cased spellings from one another.The final character string consists of the cased spellings separated by the spacingstrings.The probability of the character string is a sum over all of its dissections of thejoint probability of the string and the dissection:M(character~tring) = ~ M(character~string, dissection).
(14)dissectionsThe joint probability of the string and a dissection is a product of four factors:M ( character_string, dissection) =Mtoken (tokens) Mspetl (spellings I tokens) Mease (cased_spellings \[ pellings, tokens)Mspace( character_string I cased_spellings, spellings, tokens).
(15)3.1 The Token Trigram ModelThe token trigram model is a second-order Markov model that generates a token stringtit2.., tn by generating each token ti, in turn, given the two previous tokens ti-1 andti-2.
Thus the probability of a string isnMtoken(tlt2... tn)= Mtoken(tlt2) I-I Mtoken(ti \] ti-2ti-1)i=3(16)The conditional probabilities Mtoken(t 3 \[ tit2) are modeled as a weighted average offour estimators fiMtoken(t3 \] tlt2) = &3(t lt2)f3(t31tlt2)q-.~2(t lt2)f2(t3 I t2)q-~l( t l t2) f l ( t3)q- /~O(t l t2) fo,  (17)where the weights ,~i satisfy ~ ),i = 1 and /~i ~ 0.The estimators fi and the weights ;~i are determined from the training data using aprocedure that is explained in detail by Jelinek and Mercer (1980).
Basically, the trainingdata are divided into a large, primary segment and a smaller, held-out segment.
Theestimators fi are chosen to be the conditional frequencies in the primary segment,while the smoothing weights )~i are chosen to fit the combined model to the held-out segment.
In order to decrease the freedom in smoothing, the ,~i are constrainedto depend on (tit2) only through the counts c(tlt2) and c(t2) in the primary trainingsegment.
When c(tlt2) is large, we expect )~3(ht2) to be close to 1, since in this casethe trigram frequency in the primary segment should be a reliable estimate of the34Brown et al An Estimate of an Upper Bound for the Entropy of Englishfrequency in the held-out segment.
Similarly, when c(tlt2) is small, but c(t2) is large,we expect/k3(tlt2) tobe close to 0 and &2(tit2) to be close to 1.The token vocabulary consists of1.
293,181 spellings, including a separate ntry for each punctuationcharacter;2. a special unknown_token that accounts for all other spellings;3. a special sentenced;oundary_token that separates sentences.3.2 The Spelling ModelThe spelling model generates a spelling $1s2... Sk given a token.
For any token otherthan the unknown_token and sentence_boundary_token, the model generates the spelling ofthe token.
For the sentence_boundary_token, the model generates the null string.
Finally,for the unknown_token, the model generates a character string by first choosing a lengthk according to a Poisson distribution, and then choosing k characters independentlyand uniformly from the printable ASCII characters.
ThusMspell(SlS2... Sk \[ unknown_token) )~k = e -.~-k ~.
e ,  (18)where & is the average number of characters per token in the training text, 4.1, and1/p is the number of printable ASCII characters, 95.3.3 The Case ModelThe case model generates a cased spelling given a token, the spelling of the token,and the previous token.
For the unknown_token and sentence_boundary_token, thiscasedspelling is the same as the spelling.
For all other tokens, the cased spelling is obtainedby modifying the uncased spelling to conform with one of the eight possible patternsL + U + UL + ULUL + ULLUL + UUL + UUUL + LUL +Here U denotes an uppercase letter, L a lowercase letter, U + a sequence of one ormore uppercase l tters, and L + a sequence of one or more lowercase letters.
The casepattern only affects the 52 uppercase and lowercase letters.The case pattern C for a token t is generated by a model of the form:Mcase(C \[ t,b) =/~2(t) f (CIt ,  b) +/~l(t)f(C I b) + ,~0(t).
(19)Here b is a bit that is 1 if the previous token is the sentence_boundary_token and is 0otherwise.
We use b to model capitalization atthe beginning of sentences.3.4 The Spacing ModelThe spacing model generates the spacing string between tokens, which is either null,a dash, an apostrophe, or one or more blanks.
It is generated by an interpolated modelsimilar to that in Equation (19).
The actual spacing that appears between two tokensshould depend on the identity of each token, but in our model we only consider thedependence on the second token.
This simplifies the model, but still allows it to do35Computational Linguistics Volume 18, Number 1a good job of predicting the null spacing that precedes many punctuation marks.
Forstrings of blanks, the number of blanks is determined by a Poisson distribution.3.5 The Entropy BoundAccording to the paradigm of Section 2.2 (see Equation (13)), we can estimate anupper bound on the entropy of characters in English by calculating the languagemodel probability M(character_string) of a long string of English text.
For a very longstring it is impractical to calculate this probability exactly, since it involves a sumover the different hidden dissections of the string.
However, for any particular dis-section M(character-string) > M(character-string, dissection).
Moreover, for our model, astraightforward partition of a character string into tokens usually yields a dissectionfor which this inequality is approximately an equality.
Thus we settle for the slightlyless sharp boundH(English) <_ __1 log M(character_string, dissection)n(20)where dissection is provided by a simple finite state tokenizer.
By Equation (15), the jointprobability M(characterstring, dissection) is the product of four factors.
Consequently,the upper bound estimate (20) is the sum of four entropies,H(English) < Htoken(character-string) + Hspell(character-string)+ Hcase (character_string) + Hspacing (character-string).
(21)4.
The Data4.1 The Test SampleWe used as a test sample the Brown Corpus of English text (Kucera and Francis 1967).This well-known corpus was designed to represent a wide range of styles and varietiesof prose.
It consists of samples from 500 documents, each of which first appeared inprint in 1961.
Each sample is about 2,000 tokens long, yielding a total of 1,014,312tokens (according to the tokenization scheme used in reference \[Kucera nd Francis1967\]).We used the Form C version of the Brown Corpus.
Although in this version onlyproper names are capitalized, we modified the text by capitalizing the first letter ofevery sentence.
We also discarded paragraph and segment delimiters.4.2 The Training DataWe estimated the parameters of our language model from a training text of 583 milliontokens drawn from 18 different sources.
We emphasize that this training text does notinclude the test sample.
The sources of training text are listed in Table 1 and include textfrom:1..several newspaper and news magazine sources: the Associated Press; theUnited Press International (UPI); the Washington Post; and a collection ofmagazines published by Time Incorporated;two encyclopedias: Grolier's Encyclopedia nd the McGraw-HillEncyclopedia of Science and Technology;36Brown et al An Estimate of an Upper Bound for the Entropy of EnglishTable 1Training corpora.Source Mil l ions of wordsUnited Press International 203.768IBM Depositions 93.210Canadian Parliament 85.016Amoco PROFS (OC) 54.853Washington Post 40.870APHB 30.194Associated Press 24.069IBM Poughkeepsie (OC) 22.140Time Inc. 10.525Grolier's Encyclopedia 8.020McGraw-Hill Encyclopedia 2.173IBM Sterling Forest (OC) 1.745IBM Research (OC) 1.612Bartlett's Familiar Quotations 0.489Congressional Record 0.344Sherlock Holmes 0.340Chicago Manual of Style 0.214World Almanac and Book of Facts 0.173Total 582.7553. two literary sources: a collection of novels and magazine articles fromthe American Printing House for the Blind (APHB) and a collection ofSherlock Holmes novels and short stories;4. several egal and legislative sources: the 1973-1986 proceedings of theCanadian parliament; a sample issue of the Congressional Record; andthe depositions of a court case involving IBM;5. office correspondence (OC) from IBM and from Amoco;6. other miscellaneous sources: Bartlett's Familiar Quotations, the ChicagoManual of Style, and The World Almanac and Book of Facts.4.3 The Token VocabularyWe constructed the token vocabulary by taking the union of a number of lists includ-ing:1. two dictionaries;2. two lists of first and last names: a list derived from the IBM on-linephone directory, and a list of names we purchased from a marketingcompany;3. a list of place names derived from the 1980 U.S. census;4. vocabulary lists used in IBM speech recognition and machine translationexperiments.37Computational Linguistics Volume 18, Number 1Table 2Tokens in the test sample but not in the 293,181-token vocabulary.Token Occurrences*J 1776*F 1004Khrushchev 68Kohnstamm 35skywave 31Prokofieff 28Helva 22patient's 21dikkat 21Podger 21Katanga 21ekstrohm 20Skyros 20PIP 17Lalaurie 17roleplaying 16Pont's 15Fromm's 15Hardy's 15Helion 14The resulting vocabulary contains 89.02% of the 44,177 distinct okens in the BrownCorpus, and covers 99.09% of 1,014,312-token text.
The twenty most frequently occur-ring tokens in the Brown Corpus not contained in our vocabulary appear in Table 2.The first two, *J and *F, are codes used in the Brown Corpus to denote formulas andspecial symbols.5.
Results and ConclusionThe cross-entropy of the Brown Corpus and our model is 1.75 bits per character.
Table 3shows the contributions to this entropy from the token, spelling, case, and spacingcomponents ( ee Equation (21)).
The main contribution is, of course, from the tokenmodel: The contribution from the spelling model comes entirely from predicting thespelling of the unknown_token.
The model here is especially simple-minded, predictingeach of the 95 printable ASCII characters with equal probability.
While we can easilydo better, even if we were able to predict he characters in unknown tokens as wellas we predict those in known tokens, the contribution of the spelling model to theentropy would decrease by only 0.04 bits.
Likewise, we can entertain improvementsto the case and spacing models but any effect on the overall entropy would be small.Our bound is higher than previous entropy estimates, but it is statistically morereliable since it is based on a much larger test sample.
Previous estimates were nec-essarily based on very small samples ince they relied on human subjects to predictcharacters.
Quite apart from any issue of statistical significance, however, it is probablethat people predict English text better than the simple model that we have employedhere.The cross-entropy of a language model and a test sample provides a natural quan-titative measure of the predictive power of the model.
A commonly used measure ofthe difficulty of a speech recognition task is the word perplexity of the task (Bahl et38Brown et al An Estimate of an Upper Bound for the Entropy of EnglishTable 3Component contributions tothe cross-entropy.Component Cross-Entropy (bits)Token 1.61Spelling 0.08Case 0.04Spacing 0.02Total 1.75al.
1977).
The cross-entropy we report here is just the base two logarithm of the char-acter perplexity of a sample of text with respect o a language model.
For a numberof natural language processing tasks, such as speech recognition, machine translation,handwriting recognition, stenotype transcription, and spelling correction, languagemodels for which the cross-entropy is lower lead directly to better performance.We can also think of our cross-entropy asa measure of the compressibility of thedata in the Brown Corpus.
The ASCII cod4 for the characters in the Brown Corpus has 8bits per character.
Because only 95 of the characters are printable, it is a straightforwardmatter to reduce this to 7 bits per character.
With a simple Huffman code, which allotsbits so that common characters get short bit strings at the expense of rare characters,we can reach 4.46 bits per character.
More exotic compression schemes can reach fewerbits per character.
For example, the standard UNIX command compress, which employsa Lempel-Ziv scheme, compresses the Brown Corpus to 4.43 bits per character.
Millerand Wegman (1984) have developed an adaptive Lempel-Ziv scheme that achievesa compression to 4.20 bits per character on the Brown Corpus.
Our language modelallows us to reach a compression to 1.75 bits per character.We do not doubt that one can reduce the cross-entropy below 1.75 bits per charac-ter.
A simple way to do this is to find more reliable estimates of the parameters of themodel by using a larger collection of English text for training.
We might also considerstructural changes to the model itself.
Our model is static.
One can imagine adaptivemodels that profit from the text in the early part of the corpus to better predict helater part.
This idea is applicable to the token model and also to the spelling model.From a loftier perspective, we cannot help but notice that linguistically the trigramconcept, which is the workhorse of our language model, seems almost moronic.
Itcaptures local tactic constraints by sheer force of numbers, but the more well-protectedbastions of semantic, pragmatic, and discourse constraint and even morphological ndglobal syntactic onstraint remain unscathed, in fact unnoticed.
Surely the extensivework on these topics in recent years can be harnessed to predict English better thanwe have yet predicted it.We see this paper as a gauntlet thrown down before the computational linguisticscommunity.
The Brown Corpus is a widely available, standard corpus and the subjectof much linguistic research.
By predicting the corpus character by character, we obviatethe need for a common agreement on a vocabulary.
Given a model, the computationsrequired to determine the cross-entropy are within reach for even a modest researchbudget.
We hope by proposing this standard task to unleash a fury of competitiveenergy that will gradually corral the wild and unruly thing that we know the Englishlanguage to be.39Computational Linguistics Volume 18, Number 1ReferencesAlgoet, P. and Cover, T. (1988).
"A sandwichproof of the Shannon-McMillan-Breimantheorem."
Annals of Probability16(2):899-909.Bahl, L., Baker, J., Jelinek, E, and Mercer, R.(1977).
"Perplexity--a measure of thedifficulty of speech recognition tasks."
InProgram, 94th Meeting of the AcousticalSociety of America 62:$63, Suppl.
no.
1.Bell, T. C., Cleary, J. G., and Witten, I. H.(1990).
Text Compression.
Englewood Cliffs,N.J.
: Prentice Hall.Cover, T., and King, R. (1978).
"Aconvergent gambling estimate of theentropy of English."
IEEE Transactions onInformation Theory 24(4):413-421.Cover, T. M., and Thomas, J.
A.
(1991).Elements of Information Theory.
New York:John Wiley.Jelinek, F., and Mercer, R. L.
(1980).
"Interpolated estimation of Markovsource parameters from sparse data."
InProceedings, Workshop on Pattern Recognitionin Practice, Amsterdam, The Netherlands.Kucera, H., and Francis, W. (1967).Computational Analysis of Present-DayAmerican English.
Providence, R.I.: BrownUniversity Press.Miller, V. S., and Wegman, M. N.
(1984).
"Variations on a theme by Ziv andLempel."
Technical Report RC 10630, IBMResearch Division.Shannon, C. (1951).
"Prediction and entropyof printed English."
Bell Systems TechnicalJournal 30:50-64.40
