Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2271?2277,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsInsertion Position Selection Model for Flexible Non-Terminals inDependency Tree-to-Tree Machine TranslationToshiaki NakazawaJapan Science and Technology Agency5-3, Yonbancho, Chiyoda-ku, Tokyo, 102-8666, Japannakazawa@pa.jst.jpJohn Richardson and Sadao KurohashiKyoto UniversityYoshida-honmachi, Sakyo-ku, Kyoto, 606-8501, Japanjohn@nlp.ist.i.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jpAbstractDependency tree-to-tree translation modelsare powerful because they can naturally han-dle long range reorderings which is importantfor distant language pairs.
The translation pro-cess is easy if it can be accomplished onlyby replacing non-terminals in translation ruleswith other rules.
However it is sometimesnecessary to adjoin translation rules.
Flex-ible non-terminals have been proposed as apromising solution for this problem.
A flex-ible non-terminal provides several insertionposition candidates for the rules to be ad-joined, but it increases the computational costof decoding.
In this paper we propose a neu-ral network based insertion position selectionmodel to reduce the computational cost byselecting the appropriate insertion positions.The experimental results show the proposedmodel can select the appropriate insertion po-sition with a high accuracy.
It reduces the de-coding time and improves the translation qual-ity owing to reduced search space.1 IntroductionTree-to-tree machine translation models currentlyreceive limited attention.
However, we believe thatusing target-side syntax is important to achieve high-quality translations between distant language pairswhich require long range reorderings.
Especially,using dependency trees on both source and tar-get sides is promising for this purpose (Menezesand Quirk, 2007; Nakazawa and Kurohashi, 2010;Richardson et al, 2014).
Tree-based translationmodels naturally realize word reorderings using thenon-terminals or anchors for the attachment in thetranslation rules: therefore they do not need a re-ordering model which string-based models require.For example, suppose we have a translation rule withthe word alignment shown in Figure 1, it is easy totranslate a new input sentence which has ????(library)?
instead of ???
(park)?
because we canaccomplish it by simply substituting ?library?
for thetarget word ?park?
without considering the reorder-ing.
In this case, the source word ????
and targetword ?park?
work as the non-terminals.On the other hand, it is problematic when we needto adjoin a subtree which is not presented in train-ing sentences, which we call floating subtree in thispaper.
The floating subtrees are not necessarily ad-juncts, but any words or phrases.
For example, sup-pose the Japanese input sentence in Figure 1 has ???
(suddenly)?, but the training corpus providesonly a translation rule without the word.
In this casewe cannot directly use the rule for the translation be-cause it does not know where to insert the translationof the floating word in the output.
As another exam-ple, there is no context information available for thechildren of the OOV word in the input sentence, sowe need some special process to translate them.Previous work deals with this problem by us-ing glue rules (Chiang, 2005) or limiting the de-pendency structures to be well-formed (Shen et al,2008).
Richardson et al (2016) introduces the con-cept of flexible non-terminals.
It provides multiplepossible insertion positions for the floating subtreerather than fixed insertion positions.
A possible in-sertion position must satisfy the following condi-tions:?
it must be a child of the aligned word of theparent of the floating subtree2271?
it must not violate the projectivity of the depen-dency treeFor example, possible insertion positions for thefloating word ????
are shown in gray arrows inFigure 1.
Since ????
is a child of ?????
?, andthe translation of ??????
is ?called?, insertionpositions must be a child of ?called?.
Also, insertionpositions do not violate the projectivity of the tar-get tree.
Flexible non-terminals are analogous to theauxiliary tree of the tree adjoining grammars (TAG)(Joshi, 1985), which is successfully adopted in ma-chine translation (DeNeefe and Knight, 2009).
Thedifference is that TAG is defined on the constituencytrees rather than the dependency trees.Flexible non-terminals are powerful to handlefloating subtrees and it achieve better translationquality.
However the computational cost of decod-ing becomes high even though they are compactlyrepresented in the lattice form (Cromieres and Kuro-hashi, 2014).
In our experiments, using flexible non-terminals causes the decoding to be 3 to 6 timesslower than when they are not used.
Flexible non-terminals increase the number of translation rulesbecause the insertion positions are selected duringthe decoding.
However, we think it is possible to re-strict possible insertion positions or even select onlyone insertion position by looking at the tree struc-tures on both sides.In this paper, we propose a method to select theappropriate insertion position before decoding.
Thiscan not only reduce the decoding time but alsoimprove the translation quality because of reducedsearch space.2 Insertion Position SelectionWe assume that correct insertion positions can bedetermined before decoding, using the word to beinserted (I) with the context on the source side andthe context of the insertion positions on the targetside.
On the source side, we use the parent of I (Ps)and the distance of I from Ps (Ds).
On the targetside, we use the previous (Sp) and next (Sn) siblingof the insertion position, the parent of the insertionposition (Pt) and the distance of the insertion posi-tion from Pt (Dt).
The distances are calculated onthe siblings rather than the words in the sentence,and it is a positive or negative value if the insertion?
?
??
??
?
??
?
???
?he	 called	 her	 in	 the	 park	 yesterday(to)	(nominative)?
?
??
??
?
??
??
?
????
(suddenly)inputtranslation?ruleFigure 1: Example of an input sentence and a translation rule.We want to insert ???
(suddenly)?
which is not in the transla-tion rule.
The possible insertion positions in the target sentenceare shown in gray arrows.position is to the left or to the right of the parent re-spectively.Taking the insertion position between ?park?
and?yesterday?
in Figure 1 as an example, I = ???
?,Ps = ?????
?, Ds = +2, Sp = ?park?, Sn = ?yes-terday?, Pt = ?called?
and Dt = -3.
In cases whereSp or Sn is empty, we use special words ?
[[LEFT-START]]?, ?
[[LEFT-END]]?, ?
[[RIGHT-START]]?and ?[[RIGHT-END]]?.
In the case of ?yesterday?in Figure 1, Sp = ?in?
and Sn = ?
[[RIGHT-END]]?.These clues are fed into the neural network model tosolve the insertion position selection problem.2.1 Neural Network ModelFigure 2 shows the neural network model for the in-sertion position selection.
Given an insertion posi-tion candidate with an index k, the words (I , Ps,Skp , Skn, Pt) are first converted into vector represen-tations through the same three embedding layers:surface form embedding (200 dimensions.
), part-of-speech embedding (10 dimensions) and depen-dency type (or phrase category) embedding (10 di-mensions), and they are concatenated to create the220-dimension vectors.
The word embedding is arandomly initialized transformation from an one-hotvector to a 200 or 10-dimensional vector, and it islearned during the whole network training.Using these words and the distances, we createsource and target context vectors cks and ckt whichrepresent the information of source and target sides,respectively.
The distances (integer values) are di-rectly inputted to the network.
Then the context vec-2272220IPsPSp1Sn1DsDtk100100220220220220100source	context	vectortarget	context	vectorcontext	vector	of	the	given	inser4on	posi4on	111inser4on	posi4on	1inser4on	posi4on	2inser4on	posi4on	Nscore	vector 0.1	0.6							0.10	1						0so8max goldloss	=	so8max	cross-entropyword	to	be	insertedparent	of	Iprevious	siblingnext	siblingparent	of	the	inser4on	posi4ondistance	from	the	parentdistance	from	the	parentcs1ct1 ci1 s1s2sNFigure 2: The neural network for the insertion position selec-tion.
The numbers inside the boxes show the dimensions of thevectors.tor of the given insertion position cki is created usingcks and ckt .
Finally we get the score of the given in-sertion position sk from cki .
These vectors are calcu-lated as follows:cks = tanh(Wcs [I;Ps;Ds])ckt = tanh(Wct [Sp;Sn;Pt;Dkt ])cki = tanh(Wci [cks ; ckt ])sk = Wsckiwhere ?;?
means concatenation of the vectors.
Thesize of cks , ckt and cki is 100 in our experiments.The same network is applied to all the other in-sertion positions and get their scores.
Finally thescores are normalized by the softmax function, andthe loss is calculated by the softmax cross-entropyas the loss function.
All the links between layersare fully-connected.
We use dropout (50%) to avoidoverfitting.2.2 Training Data GenerationThe data for training the neural network model canbe automatically generated from the word-alignedparallel corpus with dependency parses in both sidesby Algorithm 1.
The ALIGNMENT function returnsthe aligned word in the target tree for the givensource word1, and the ISPARENTCHILD function re-turns TRUE if Pt is the parent of Ct.1In case of the multiple word alignment, we only use theroot word of them in both source and target sides.Algorithm 1 Training Data Generation for NNfor all Ps ?
words in source tree doPt = ALIGNMENT(Ps)for all Cs ?
CHILDREN(Ps) doCt = ALIGNMENT(Cs)if ISPARENTCHILD(Pt, Ct) thenGENERATEDATA(Ps, Cs, Pt, Ct)end ifend forend forJa?
En Ja?
ZhTraining 2,020,106 667,520Development 1,789 2,115Test 1,812 2,174Table 1: The number of sentences in ASPEC used in our exper-iments.The GENERATEDATA function generates one in-stance of training data to predict the position of Ctfrom Ps, Cs and Pt with their contexts by removingCt in the target tree.
The position where Ct exists isregarded as the correct insertion position, and othersas incorrect insertion positions.
Note that Cs corre-sponds to I in Figure 2.2.3 Insertion Position Selection in TranslationOnce the neural network model is trained, it can beapplied to select the most appropriate insertion po-sitions in the translation rules for the given float-ing subtree by looking at the score of each inser-tion position.
Translation rules only contain part ofthe original parallel sentence in most of the cases.This means that the context used for selecting the in-sertion position is different from that in the trainingdata for the neural network.
For example, if the in-put sentence does not have ????
(in the park)?
inFigure 1, the number of possible insertion positionsis 6 and we do not use ?in?
as the context.
How-ever, this is not so problematic because similar orsame context may appear in the different part of thecorpus.3 ExperimentsWe conducted two kinds of experiments: the in-sertion position selection and translation.
We usedASPEC (Nakazawa et al, 2016) as the dataset andthe numbers of the sentences of the corpus areshown in Table 1.
The Japanese morphological ana-lyzer (Kurohashi et al, 1994) and dependency parser(Kurohashi and Nagao, 1994) are used for Japanese2273Ja?
En En?
Ja Ja?
Zh Zh?
JaTraining 15.7M 5.7MDevelopment 160K 58KTest 160K 58Kave.
# IP 3.39 3.15 3.72 3.41best epoch 89 71 61 79mean loss 0.089 0.058 0.105 0.056Accuracy (%) 97.08 97.72 96.51 97.99Logit Accuracy (%) 55.00 89.03 68.04 83.16Table 2: The statistics of the data and results of the insertionposition selection experiments.sentences.
English sentences are first parsed by nl-parser (Charniak and Johnson, 2005) and then con-verted into word dependency trees using Collins?head percolation table (Collins, 1999).
We used Chi-nese word segmenter KKN (Shen et al, 2014) anddependency parser SKP (Shen et al, 2012) for Chi-nese sentences.
The supervised word alignment Nile(Riesa et al, 2011) was used.We used a state-of-the-art dependency tree-to-treedecoder (Richardson et al, 2014) with the defaultsettings.
The neural network is constructed andtrained using the Chainer (Tokui et al, 2015).3.1 Insertion Position SelectionThe training, development and test data for the neu-ral network is automatically generated by the proce-dure explained in Section 2.2.
The size of the gen-erated data from the ASPEC and the average num-ber of insertion positions for each floating subtreeare shown in Table 2.
We trained the model for 100epochs and used the best model on the developmentdata for testing.
The vocabulary size for the surfaceform was 50,000.For comparison, we also tried the logistic regres-sion to predict the correct insertion positions.
Be-cause our training data is huge, we used Multi-coreLIBLINEAR2 with L2-regularized logistic regres-sion (primal) solver.
The format of training in-stances are: one-hot (binary) vectors for surfaceform, POS and dependency type, and distancesscaled to [0, 1].
We first find the best value for the Cparameter, then train the model.
The best insertionposition is selected using the estimated probabilitiesfor each insertion position.The experimental results are also shown in Table2https://www.csie.ntu.edu.tw/?cjlin/libsvmtools/multicore-liblinear/2.
We evaluated the results by the mean loss of themodel and the accuracy on the test data.
The re-sult shows that our model can select the correct in-sertion position with very high accuracy for everylanguage pair while the classical logistic regressionmodel cannot.
This supports our claim stated in thebeginning of Section 2.X ?
Ja is easier and achieved slightly better ac-curacy than the reverse direction because Japaneseis a head-final language and all children are gener-ally put on the left of their parents.
There are somecases judged as incorrect but acceptable insertionpositions, and hence the true accuracies are higherthan the ones reported above.
We also investigatedthe top-2 accuracy and found that it is above 99.0%for Ja?
X and 99.5% for X?
Ja.Table 3 shows the detailed result of Jarightarrow En experiment.
The number ofinsertion-position is at least 2 (left/right of theparent) and it is easy to solve (more than 99%accuracy).
3 is a situation where the parent hasone child, and it is still not so difficult (97-98%accuracy).
About 70% of the test data have only2 or 3 insertion-positions.
Difficult cases are thesentences which have many adjuncts as in Figure 1,but we used the scientific paper corpora, where notso many adjuncts appear.3.2 TranslationWe conducted translation experiment using the AS-PEC in 3 settings:?
No Flexible: not using the flexible non-terminals and using simple glue rules as in thebaseline model of (Richardson et al, 2016) 3?
Baseline: using the flexible non-terminalswithout the insertion position selection?
Proposed: using only the most appropriate in-sertion position for the flexible non-terminalsWe also report the translation quality of conven-tional models for comparison: phrase-based SMT(PBSMT) and hierarchical phrase-based SMT (Hi-ero).
We used the default settings of Moses except-distortion-limit=20 for PBSMT.The translation quality is evaluated by the auto-matic evaluation measures BLEU (Papineni et al,352.5% of all the translation rules require glue rule, but it isapplied to 22.6% of the rules actually used in the translation.2274# of ins.
pos.
Top N accuracy (%)and ratio 1 2 3 4 5 6 7 8 92 (49.39%) 99.223 (18.15%) 96.37 99.604 (11.77%) 95.76 99.06 99.825 (6.30%) 94.24 98.39 99.38 99.856 (4.99%) 93.50 97.82 99.11 99.63 99.987 (3.96%) 93.13 97.40 98.87 99.56 99.89 99.988 (2.42%) 92.77 97.07 98.43 99.23 99.61 99.77 99.979 (1.50%) 92.14 96.19 97.85 99.01 99.50 99.67 99.92 100.0010 (0.77%) 92.04 96.62 97.75 98.47 99.03 99.52 99.92 99.92 100.00Table 3: Detailed Ja?
En insertion position selection experimental result.Ja?
En En?
Ja Ja?
Zh Zh?
JaBLEU RIBES Time BLEU RIBES Time BLEU RIBES Time BLEU RIBES TimePBSMT 18.45 64.51 - 27.48 68.37 - 27.96 78.90 - 34.65 77.25 -Hiero 18.72 65.11 - 30.19 73.47 - 27.71 80.91 - 35.43 81.04 -No Flexible 20.28 65.08 1.00 28.77 75.21 1.00 24.85 66.60 1.00 30.51 73.08 1.00Baseline 21.61 69.82 6.28 30.57 76.13 3.30 28.79 78.11 5.16 34.32 77.82 5.28Proposed 22.07?
70.49?
2.25 30.50 76.69?
1.27 29.83?
79.73?
2.21 34.71?
79.25?
1.89Table 4: The results of the translation experiments.
?
means the Proposed method achieved significantly better score than theBaseline (p < 0.01).2002) and RIBES (Isozaki et al, 2010) with the sig-nificance testing by bootstrap resampling (Koehn,2004).
RIBES is more sensitive to word order thanBLEU, so we expect an improvement in RIBES.
Wealso investigated relative decoding time compared tothe No Flexible setting.
Note that we used the word?decoding?
for only exploring the search space, andit does not include constructing the search space (asthe table lookup in Phrase-based SMT).
Our wholetranslation process is:1. translation rule extraction2.
insertion-position selection3.
decodingAt the time of the second step, we have all the trans-lation rules applicable to the input sentence.
Thecomputation time for each step is 3 ?
1 ?
2 so weonly focus on the time for step 3 in the experiments(the computation time for step 2 is negligibly small).The results are shown in Table 4.
The Proposedmethod achieved significantly better automatic eval-uation scores than the Baseline for all the languagepairs except the BLEU score of En ?
Ja direction.Also, the decoding time is reduced by about 60%relative to that of the Baseline.Our tree-based model is better than the conven-tional models except C ?
J, where the accuracy ofChinese parsing for the input sentences has a bad ef-fect.4 ConclusionIn this paper we have proposed a neural networkbased insertion position selection model to reducethe computational cost of the decoding for de-pendency tree-to-tree translation with flexible non-terminals.
The model successfully finds the appro-priate insertion position from the candidates and itleads to faster translation speed and better transla-tion quality due to the reduced search space.Currently, we use only words as the context butit seems promising to use subtrees as well.
For ex-ample, using the information of the subtree ?in thepark?
is more informative than using only ?in?
inFigure 1.
This is especially important for Japaneseas the target language because children of verbs areoften case markers and they do not provide enoughinformation when selecting the appropriate insertionposition.
It is possible to adopt existing models ofcreating vector representation of dependency sub-trees such as the model using recursive neural net-works (Liu et al, 2015) and convolutional neuralnetworks (Mou et al, 2015).2275ReferencesEugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics (ACL?05),pages 173?180.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics (ACL?05), pages 263?270.
As-sociation for Computational Linguistics.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Fabien Cromieres and Sadao Kurohashi.
2014.
Transla-tion rules with right-hand side lattices.
In Proceedingsof the 2014 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), pages 577?588.Association for Computational Linguistics.Steve DeNeefe and Kevin Knight.
2009.
Synchronoustree adjoining machine translation.
In Proceedings ofthe 2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 727?736.
Association forComputational Linguistics.Hideki Isozaki, Tsutomu Hirao, Kevin Duh, KatsuhitoSudoh, and Hajime Tsukada.
2010.
Automatic evalu-ation of translation quality for distant language pairs.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?10, pages 944?952, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.A.
K. Joshi.
1985.
Tree adjoining grammars: Howmuch context-sensitivity is required to provide reason-able structural descriptions?
In D. R. Dowty, L. Kart-tunen, and A. M. Zwicky, editors, Natural LanguageParsing: Psychological, Computational, and Theoret-ical Perspectives, pages 206?250.
Cambridge Univer-sity Press, Cambridge.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Dekang Lin andDekai Wu, editors, Proceedings of EMNLP 2004,pages 388?395, Barcelona, Spain, July.
Associationfor Computational Linguistics.Sadao Kurohashi and Makoto Nagao.
1994.
A syntacticanalysis method of long Japanese sentences based onthe detection of conjunctive structures.
ComputationalLinguistics, 20(4):507?534.Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto,and Makoto Nagao.
1994.
Improvements of Japanesemorphological analyzer JUMAN.
In Proceedings ofThe International Workshop on Sharable Natural Lan-guage, pages 22?28.Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou, andHoufeng WANG.
2015.
A dependency-based neu-ral network for relation classification.
In Proceed-ings of the 53rd Annual Meeting of the Associationfor Computational Linguistics and the 7th Interna-tional Joint Conference on Natural Language Process-ing (Volume 2: Short Papers), pages 285?290.
Associ-ation for Computational Linguistics.Arul Menezes and Chris Quirk, 2007.
Proceedings ofthe Second Workshop on Statistical Machine Transla-tion, chapter Using Dependency Order Templates toImprove Generality in Translation, pages 1?8.
Asso-ciation for Computational Linguistics.Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, andZhi Jin.
2015.
Discriminative neural sentence mod-eling by tree-based convolution.
In Proceedings ofthe 2015 Conference on Empirical Methods in NaturalLanguage Processing, pages 2315?2325, Lisbon, Por-tugal, September.
Association for Computational Lin-guistics.Toshiaki Nakazawa and Sadao Kurohashi.
2010.
Fullysyntactic ebmt system of kyoto team in ntcir-8.
InIn Proceedings of the 8th NTCIR Workshop Meet-ing on Evaluation of Information Access Technologies(NTCIR-8), pages 403?410.Toshiaki Nakazawa, Manabu Yaguchi, Kiyotaka Uchi-moto, Masao Utiyama, Eiichiro Sumita, Sadao Kuro-hashi, and Hitoshi Isahara.
2016.
ASPEC: Asianscientific paper excerpt corpus.
In Proceedings ofthe Ninth International Conference on Language Re-sources and Evaluation (LREC 2016), pages 2204?2208, Portoroz?, Slovenia, May.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalua-tion of machine translation.
In ACL, pages 311?318.John Richardson, Fabien Cromie`res, Toshiaki Nakazawa,and Sadao Kurohashi.
2014.
Kyotoebmt: Anexample-based dependency-to-dependency translationframework.
In Proceedings of 52nd Annual Meeting ofthe Association for Computational Linguistics: SystemDemonstrations, pages 79?84.
Association for Com-putational Linguistics.John Richardson, Fabien Cromiere`s, Toshiaki Nakazawa,and Sadao Kurohashi.
2016.
Flexible non-terminalsfor dependency tree-to-tree reordering.
In Proceed-ings of the 2016 Conference of the North AmericanChapter of the Association for Computational Linguis-tics: Human Language Technologies, pages 11?19.Association for Computational Linguistics.Jason Riesa, Ann Irvine, and Daniel Marcu.
2011.Feature-rich language-independent syntax-basedalignment for statistical machine translation.
InProceedings of the 2011 Conference on Empirical2276Methods in Natural Language Processing, pages497?507.
Association for Computational Linguistics.Libin Shen, Jinxi Xu, and Ralph M Weischedel.
2008.A new string-to-dependency machine translation algo-rithm with a target dependency language model.
InAssociation for Computational Linguistics.Mo Shen, Daisuke Kawahara, and Sadao Kurohashi.2012.
A reranking approach for dependency parsingwith variable-sized subtree features.
In Proceedingsof 26th Pacific Asia Conference on Language Infor-mation and Computing, pages 308?317.Mo Shen, Hongxiao Liu, Daisuke Kawahara, and SadaoKurohashi.
2014.
Chinese morphological analysiswith character-level pos tagging (short paper).
In Pro-ceedings of the 52nd Annual Meeting of the Associa-tion for Computational Linguistics (ACL2014), Balti-more, USA.Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clay-ton.
2015.
Chainer: a next-generation open sourceframework for deep learning.
In Proceedings of Work-shop on Machine Learning Systems (LearningSys) inThe Twenty-ninth Annual Conference on Neural Infor-mation Processing Systems (NIPS).2277
