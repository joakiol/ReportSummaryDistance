The SynDiKATe Text Knowledge Base GeneratorUdo HahnText Knowledge Engineering LabAlbert-Ludwigs-Universita?t FreiburgD-79085 Freiburg, Germanyhahn@coling.uni-freiburg.deMartin RomackerText Knowledge Engineering LabAlbert-Ludwigs-Universita?t FreiburgD-79085 Freiburg, Germanyromacker@coling.uni-freiburg.deABSTRACTSynDiKATe comprises a family of text understanding sys-tems for automatically acquiring knowledge from real-worldtexts, viz.
information technology test reports and medicalnding reports.
Their content is transformed to formal rep-resentation structures which constitute corresponding textknowledge bases.
SynDiKATe's architecture integrates re-quirements from the analysis of single sentences, as well asthose of referentially linked sentences forming cohesive texts.Besides centering-based discourse analysis mechanisms forpronominal, nominal and bridging anaphora, SynDiKATeis supplied with a learning module for automatically boot-strapping its domain knowledge as text analysis proceeds.1.
INTRODUCTIONThe SynDiKATe system belongs to the broad family ofinformation extraction (IE) systems [1].
Signicant progresshas been made already, as current IE systems provide robustshallow text processing such that frame-style templates arelled with factual information about particular entities (lo-cations, persons, event types, etc.)
from the analyzed doc-uments.
Nevertheless, typical MUC-style systems are alsolimited in several ways.
They provide no inferencing ca-pabilities which allow substantial reasoning about the tem-plate llers (hence, their understanding depth is low), andtheir potential to deal with textual phenomena is highlyconstrained, if it is available at all.
Also novel and unex-pected though potentially relevant information which doesnot match given template structures is hard to account for,since system designers commit to a xed collection of do-main knowledge templates (i.e., they have no concept learn-ing facilities).With SynDiKATe, we are addressing these shortcomingsand aim at a more sophisticated level of knowledge acqui-sition from real-world texts.
The documents we deal withare technical narratives in German language taken from twodomains, viz.
test reports from the information technology(IT) domain as processed by the itSynDiKATe system [8],.and nding reports from a medical subdomain (MED), theframework of the medSynDiKATe system [10, 9].
Our rstgoal is to extract conceptually and inferentially richer formsof knowledge than those captured by standard IE systemssuch as evaluative assertions and comparisons [25, 24], tem-poral [26] and spatial information [22].
Second, we alsowant to dynamically enhance the set of knowledge templatesthrough incremental taxonomy learning devices [12] so thatthe information extraction capability of the system is in-creased in a bootstrapping manner.
Third, SynDiKATe isparticularly sensitive to the treatment of textual referencerelations [27, 6, 14].
The capability to properly deal withvarious forms of anaphora is a prerequisite for the sound-ness and validity of the knowledge bases we create as a re-sult of the text understanding process and likewise for thefeasibility of sophisticated retrieval and question answeringapplications based on the acquired text knowledge.2.
SYSTEM ARCHITECTUREThe overall architecture of SynDiKATe, an acronymwhichstands for \Synthesis of Distributed Knowledge Acquiredfrom Texts", is summarized in Figure 1.
Incoming texts, Ti,are mapped into corresponding text knowledge bases, TKBi,which contain a representation of Ti's content.
This knowl-edge base platform may feed various information services,such as inferentially supported question answering (fact re-trieval), text passage retrieval or text summarization [7].2.1 Sentence-Level UnderstandingGrammatical knowledge for syntactic analysis is basedon a fully lexicalized dependency grammar [11], we refer toas Lexicon in Figure 1.
Basic word forms (lexemes) con-stitute the leaf nodes of the lexicon tree, which are furtherabstracted in terms of a hierarchy of lexeme class speci-cations at dierent levels of generality.
The Generic Lexi-con in Figure 1 contains lexical material which is domain-independent (lexemes such as move, with, or month), whiledomain-specic extensions are kept in specialized lexiconsserving the needs of particular subdomains, e.g., IT (harddisk, color printer, etc.)
or MED (gastritis, surface mucus,etc.).
Dependency grammars capture binary valency con-straints between a syntactic head (e.g., a noun) and possi-ble modiers (e.g., a determiner or an adjective).
To estab-lish a dependency relation between a head and a modier,all the lexicalized constraints on word order, compatibilityof morphosyntactic features, and semantic criteria must befullled.
This leads to a strictly local computation schemewhich inherently lends itself to robust partial parsing [5].Figure 1: Architecture of a SynDiKATe SystemConceptual knowledge about the dierent domains isexpressed in a Kl-One-like description logic language [28].Corresponding to the division at the lexical level, the on-tologies we provide are split up between one that is used byall applications, the Upper Ontology, while several dedicatedontologies account for the conceptual requirements of par-ticular domains, e.g., IT (HardDisk, ColorPrinter, etc.
)or MED (Gastritis, SurfaceMucus, etc.
).Semantic knowledge accounts for emerging conceptualrelations between conceptual items according to those de-pendency relations that are established between their cor-responding lexical items.
Semantic interpretation schematamediate between both levels in a way as abstract and generalas possible [20].
These schemata are applied to semanticallyinterpretable subgraphs which are, from a semantic point ofview, \minimal" subgraphs of the incrementally built depen-dency graph.
Their bounding nodes contain content words(i.e., nouns, verbs, and adjectives, all of which have a concep-tual correlate in the domain ontologies), while all possiblyintervening nodes (zero up to four) contain only noncon-tent words (such as prepositions, articles, auxiliaries, etc.,all of which have no conceptual correlates).
Semantic in-terpretation schemata are fully embedded in the knowledgerepresentation model and system (cf.
Figure 1).The ParseTalk system, which comprises the lexicalizedgrammar and associated dependency parser, is embedded inan object-oriented computation model.
So, the dependencyrelations are computed by lexical objects, so-called word ac-tors, through strictly local message passing, only involvingthe lexical items they represent.
To illustrate how a de-pendency relation is established computationally, we give asketch of the basic protocol for incremental parsing [5]: After a word has been read from textual input bythe WordScanner (step A1in Figure 1), its associatedlexeme (specied in the Lexicon) is identied (stepA2) and a corresponding word actor gets initialized(step B1).
As all content words are directly linked tothe conceptual system, each lexical item w that has aconceptual correlate C in the domain knowledge base(step A3) gets instantiated in the text knowledge base(step B2).
The lexical item Festplatte (hard disk)with the conceptual correlate Hard-Disk is instanti-ated, e.g., by Hard-Disk.3, the particular item beingtalked about in a given text.1 For integration in the parse tree, the newly createdword actor searches its head (alternatively, its modi-er) by sending parallel requests for dependential gov-ernment to its left context (step C).
The search spaceis restricted, since these requests are propagated up-wards only along the `right shoulder' of the depen-dency graph constructed so far.
All word actors ad-dressed this way check, in parallel, whether their va-lency restrictions, i.e., grammatical and conceptual con-straints, are met by the requesting word actor.
StepD simulates a conceptual check in the text knowledgebase, step E illustrates a test in the discourse memory. If all required constraints are fullled by one of thetargeted word actors, an immediate semantic interpre-tation is performed.
This usually alters the conceptualrepresentation structures by way of slot lling (step F ).Semantic interpretation consists of nding a relationallink between the conceptual correlates of the two contentwords bounding the associated semantically interpretablesubgraph.
The linkage may either be constrained by depen-dency relations (e.g., the subject: relation of a transitiveverb such as \sell" may only be interpreted conceptuallyin terms of agent or patient roles), by intervening lexicalmaterial (e.g., some prepositions impose special role con-straints, such as mit (with) does in terms of has-part orinstrument roles), or it may be constrained by concep-tual criteria only (as with the genitive: dependency rela-tion, which unlike subject: imposes no additional selectiveconceptual constraints for interpretation).
The correspond-ing knowledge about these language-specic constraints isdensely encoded in the Lexicon class hierarchy, an approachwhich heavily relies on the property inheritance mechanismsinherent to the object-oriented paradigm.2.2 Text-Level Understanding2.2.1 Referential Text PhenomenaThe textual phenomena we deal with in SynDiKATe es-tablish referential links between consecutive utterances in acoherent text such as illustrated by three possible continua-tions of sentence (1), with three dierent forms of extrasen-tential anaphora:(1) Compaq verkauft ein Notebook mit einer Festplatte, dievon Seagate hergestellt wird.
(Compaq sells a notebook with a hard disk that is man-ufactured by Seagate.
)(2) Pronominal Anaphora:Es ist mit einer Pentium-III-CPU ausgestattet.
(It comes with a Pentium-III CPU.
)(3) Nominal Anaphora:Der Rechner ist mit einer Pentium-III-CPU ausgestattet.
(The machine comes with a Pentium-III CPU.
)(4) Functional Anaphora:Der Arbeitsspeicher kann auf 96 MB erweitert werden.
(The main memory can be expanded up to 96MB.
)1Due to the recognition of referential relations at the textlevel of analysis this instantiation might be readjusted bysubsequent coreference declarations (cf.
Section 2.2).Compaq sells a Notebook with a hard disk that is manufactured by Seagate.3ein4251Compaqsubject:propo:verkauftdie,delimiter:subject:relative:specifier:NotebookhergestelltvonSeagateppadjunct:pobject:wirdppatt:pobject:object:mitspecifier:Festplatte.einerverbpart:Figure 2: Dependency Parse for Sentence (1)Figure 3: Conceptual Interpretation for Sentence (1)The results of sentence-level analysis for sentence (1) aregiven in Figure 2, which contains a syntactic dependencygraph (together with ve congurations of semantically in-terpretable subgraphs), and Figure 3, which displays its con-ceptual representation.
For text-level analysis, pronominalanaphora still heavily depend on grammatical conditions {the agreement of the antecedent (\Notebook") and the pro-noun (\Es" (it)) in gender and number; also conceptual cri-teria apply insofar as a potential antecedent must t theconceptual role (or case frame) restrictions when it is in-tegrated in governing structures, say, the head verb of theclause.
In general, however, the inuence of grammaticalcriteria gradually diminishes for other types of text phe-nomena, while the inuence of conceptual criteria increases.For nominal anaphora, number constraints are still valid,while a generalization relation between the anaphoric noun(\Rechner" (machine)) and its proper antecedent (\Note-book") must hold, in addition.
In the case of functionalanaphora, no grammar constraints at all apply, while quitesophisticated conceptual role path conditions come into play,e.g., \Arbeitsspeicher" (main memory) being a constituentphysical part of \Notebook".The problems text phenomena cause are of vital impor-tance for the adequacy of the representation structures re-sulting from text processing, and are centered around the no-tions of incomplete, invalid and incoherent knowledge bases.Incomplete knowledge bases emerge when references toalready established discourse entities are simply not recog-nized, as in the case of pronominal anaphora.
Consider thereference relationship between the pronoun \Es" (it) in sen-tence (2) which refers to the noun phrase \ein Notebook"(a notebook) in sentence (1).
The occurrence of the pro-noun is not reected at the conceptual level, since pronouns(as noncontent words) do not have conceptual correlates.Hence, an incomplete concept graph emerges as shown inFigure 4 | the referent for the pronoun \Es" (it), Note-book.2, is not linked to Pentium-III-CPU.6.
An adequatetreatment with a properly resolved anaphor is shown in Fig-ure 6, where the representation of the relevant portions ofsentence (1) is linked to the one of sentence (2), in particu-Figure 4: Unresolved Pronominal Anaphor, Sentence (2)Figure 5: Unresolved Nominal Anaphor, Sentence (3)Figure 6: Resolved Anaphors, Sentences (1) and (2)/(3)lar by determining the equip-patient role between Equip.7and the proper referent, Notebook.2.Invalid knowledge bases emerge when each entity whichhas a dierent denotation at the text surface is treated asa formally distinct conceptual item at the symbol level ofknowledge representation, although all dierent denotationsrefer literally to the same conceptual entity.
This is thecase for nominal anaphora, an example of which is given bythe reference relation between the noun phrase \Der Rech-ner" (the machine) in sentence (3) and the noun phrase \einNotebook" (a notebook) in sentence (1).
An invalid referen-tial description appears in Figure 5, where Computer.5 isintroduced as a new entity in the discourse, whereas Figure6 shows the valid conceptual representation capturing theintended meaning at the representation level, viz.
maintain-ing Notebook.2 as the proper referent (note that pronom-inal as well as nominal anaphora are two equivalent ways tocorefer to the discourse entity denoted by Notebook.2).Finally, incoherent knowledge bases emerge when entitieswhich are linked by nontaxonomic conceptual relations atthe knowledge level occur in a text such that an implicitreference to these relations can be made in the text source.Unlike the previously discussed cases of coference, theserelations have to be made explicit at the symbol level ofthe targeted text knowledge base by a search for connect-ing paths between the concepts involved [6].
This is thebasic scenario for functional (or bridging) anaphora.
Con-sider, e.g., the relationship holding between the noun phrase\Der Arbeitsspeicher" (the main memory) in sentence (4),which refers to the noun phrase \ein Notebook" (a note-book) in sentence (1).
In Figure 8 the relational link miss-ing in Figure 7 between Main-Memory.8 and Notebook.2is established (via a has-part-type relation, viz.
has-main-memory), and, hence, representational coherence at the sym-bol level of knowledge representation is preserved.Figure 7: Unresolved Functional Anaphor, Sentence (4)Figure 8: Resolved Functional Anaphor, Sentences (1)and (4)Disregarding textual phenomena will cause dysfunctionalsystem behavior.
A query Q such asQ : (retrieve ?x (Computer ?x))A-: (|I| Notebook.2, |I| Computer.5)A+: (|I| Notebook.2)triggers a search for all instances of Computer in the textknowledge base.
Given an invalid knowledge base (cf.
Fig-ures 3 and 5), the incorrect answer (A-) contains two entities,viz.
Notebook.2 and Computer.5 | both are in the ex-tension of the concept Computer.
If, however, a valid textknowledge base such as the one in Figure 6 or 8 is given,only the correct answer, Notebook.2, is inferred (A+).Rendering also quantitative substance to our claims, weanalyzed a randomly chosen sample of 100 reports on his-tological ndings with approximately 14,000 text tokens [9].In IT texts, (pro)nominal anaphora and functional anaphoraoccur at an almost balanced rate [27].
In the medical texts,however, functional anaphora turn out to be the major gluefor establishing local coherence, while anaphora, pronomi-nal anaphora in particular, play a far less important rolethan in other text genres.
The high proportion of func-tional anaphora (45%) [42%-48%]2and the remarkable rateof nominal (34%) [31%-37%] compared to extrasententialpronominal anaphora (2%) [1%-3%] is clearly an indicationof the primary orientation in medical texts to convey factsin a very compact manner.
Two consequences can be drawnfrom this observation.
First, resolution procedures for func-tional anaphora { supplementing well-researched proceduresfor (pro)nominal anaphora { have to be provided urgently(cf.
[6] for a fully worked out approach).
Second, functionalanaphora presuppose a considerable amount of deep back-ground knowledge, with emphasis on partonomic reasoning[13], supplementing well-known principles of taxonomic rea-soning for text understanding.2For all percentage numbers 95% condence intervals aresupplied in square brackets.2.2.2 Centering Model for Anaphora ResolutionIn order to avoid the emergence of incomplete, invalid andincoherent text knowledge bases we consider discourse enti-ties for establishing reference relations with upcoming itemsfrom the textual input at a local [27] and at a global level[14] of cohesion.
To preserve adequate text representationstructures a centering mechanism is used.
The discourse en-tities which occur in an utterance Uiconstitute its set offorward-looking centers, Cf(Ui).
The elements in Cf(Ui)are ordered to reect relative prominence in Uiin the sensethat the most highly ranked element of Cf(Ui) is the mostlikely antecedent of an anaphoric expression in Ui+1, whilethe remaining elements are ordered according to decreasingpreference for establishing referential links.While it is usually assumed (for the English language,in particular) that grammatical roles are the major deter-minant for the ranking on the Cf[4], we claim that forGerman { a language with relatively free word order { itis the functional information structure of the sentence [27].Accordingly, the constraints on the ordering of entries inCf(Ui) prefer hearer-old (either evoked or unused) elementsin an utterance (i.e., those that can be related to previ-ously introduced discourse elements or generally accessi-ble world knowledge) over mediated (inferrable) ones, whilethese are preferred over hearer-new (brand-new) elementsfor anaphora resolution.
If two elements belong to the samecategory, then preference is dened in terms of linear prece-dence of the discourse units in the source text.When we apply these criteria to sentence (1), Table 1 de-picts the resulting order of forward-looking centers in Cf(S1).Since we have no discourse-bound elements in the rst sen-tence, textual precedence applies exclusively to the orderingof the center list items.
Only nouns and their conceptualcorrelates are taken into consideration.
The tuple notationtakes the conceptual correlate of the lexical item in the textknowledge base in the rst place, while the lexical surfaceform appears in the second place.
(1) Cf: [Compaq: Compaq, Notebook.2: Notebook,Hard-Disk.3: Festplatte, Seagate: Seagate]Table 1: Centering Data for Sentence (1)Processing of the centering list Cf(S1) for sentence (3)until the generalization constraint is fullled, nally, resultsin a query whether Notebook is subsumed by Computer,the conceptual correlate of the lexical item \Rechner".
Asthis relationship obviously holds, in the conceptual represen-tation structure of sentence (3) (cf.
Figure 5) Computer.5,the literal instance identier, is declared coreferent toNote-book.2, the referentially valid identier.
Instead of havingtwo unlinked sentence graphs, Figures 3 and 5, the referenceresolution for (pro)nominal anaphora leads to joining themin a common valid text graph (Figure 6).
In particular,Notebook.2 links to the relation equip-patient, formerlyoccupied by Computer.5.
The corresponding centering listat the end of the analysis of sentence (3) is provided in Table2 (Cf(S1) has been updated to reect the consumption ofthe antecedent, Notebook.2, in the processing of Cf(S3)).
(1) Cf: [Compaq: Compaq, Notebook.2: Notebook,Hard-Disk.3: Festplatte, Seagate: Seagate](3) Cf: [Notebook.2: Rechner,Pentium-III-CPU.6: Pentium-III-CPU]Table 2: Centering Data for Sentences (1) and (3)2.3 Textual LearningThe approach to learning new concepts as a result of textunderstanding builds on two dierent sources of evidence |the prior knowledge of the domain the texts are about, andgrammatical constructions in which unknown lexical itemsoccur in the texts.
The architecture of SynDiKATe's con-cept learning component is depicted in Figure 9.linguisticqualitylabelsconceptuallabelsHypothesisspace-pHypothesisQualifierqualityspace-qQuality Machine12space-1space-ispace-nHypothesisHypothesisHypothesisLanguage Processordependency parse graphtext knowledge baseFigure 9: SynDiKATe's Learning ComponentThe ParseTalk system generates dependency parse graphs.The kinds of syntactic constructions (e.g., genitive, apposi-tive, comparative), in which unknown lexical items appear,are recorded and later assessed relative to the credit theylend to a particular concept hypothesis, e.g., high for ap-positives (\the notebook X"), lower for genitives (\Compaq'sX").
The conceptual interpretation of parse trees involvingunknown lexical items in the text knowledge base leads to thededuction of concept hypotheses.
These are further enrichedby conceptual annotations which reect structural patternsof consistency, mutual justication, analogy, etc.
relative toalready available concept descriptions in the text knowledgebase or other hypothesis spaces.
Both kinds of evidence, inparticular their predictive `goodness' for the learning task,are represented by corresponding sets of linguistic and con-ceptual quality labels.Alternative concept hypotheses for each unknown lexi-cal item are organized in terms of corresponding hypothesisspaces, each of which holds a dierent conceptual reading.An inference engine embedded in the terminological system,the so-called quality machine, determines the overall credi-bility of single concept hypotheses by taking the available setof quality labels for each hypothesis into account.
The qual-ier, a terminological classier extended by an evaluationmetric for quality classes, computes a preference ranking ofthose hypotheses which remain valid after the text has beenprocessed completely (cf.
[12] for details).3.
COVERAGE AND EVALUATIONSynDiKATe's coverage varies considerably depending onthe target domain.
The generic lexicon currently includes3,000 entries, the IT lexicon adds 5,000, while the MEDlexicon contributes 70,000 entries each.
The Upper Ontologycontains 1,200 concepts and roles, to which the IT ontologyadds 3,000 and the MED ontology contributes 240,000 items.The IT domain was chosen as a testbed that can be ex-tended on demand.
The MED domain, however, is subjectto ontology engineering eorts on a larger scale.
In orderto cope with the enormous knowledge engineering require-ments, we semi-automatically transformed large portions ofa semantically weak, yet high-volume medical terminology(UMLS) to a very large terminological knowledge base [21].Admittedly, SynDiKATe has not yet undergone a thor-ough empirical evaluation in one of the envisaged applica-tion dimensions.
We have, however, carefully evaluated itssubcomponents.
The results can be summarized as follows:Sentence Parsing.
We compared a standard active chartparser with full backtracking capabilities with the parserof SynDiKATe, which is characterized by limited memo-ization and restricted backtracking capabilities, using thesame grammar specications.
On average, SynDiKATe'sparser exhibits a linear time complexity the factor of whichis dependent on ambiguity rates of input sentences.
Theactive chart parser runs into exponential time complexitywhenever it encounters extragrammatical or ungrammaticalinput, since then it conducts an exhaustive search of the en-tire parse space.
The loss of structural descriptions due tothe parser's incompleteness amounts to 10% compared withthe complete, though intractable parser [5].Text Parsing.
While with respect to resolution capac-ity (eectiveness) no signicant dierences could be deter-mined, the functional centering model we propose outper-forms the best-known centering algorithms by a rate of 50%with respect to a measure of computation costs which con-siders \cheap" and \expensive" transitional moves betweenutterances to assess a text's coherency.
Hence, the proce-dure we propose is more e?cient [27].Semantic Interpretation.
Our group has been pioneer-ing work on the empirical evaluation of meaning representa-tions.
We assessed the quality and coverage of semantic in-terpretation for randomly sampled texts in the two domainswe consider.
While recall was rather low (57% for MED, 31%for IT), precision peaked at 97% and 94%, respectively [19].\Heavy" Semantics.
We can deal with intricate seman-tic phenomena for which we have provided the rst empiricalevaluation data available at all.
This relates to the resolu-tion of metonymies, where we have determined a gain ineectiveness that amounts to 16% compared with the bestprocedures known so far [16], as well as it relates to compar-atives and evaluative assertions, where gains in eectivenesswere almost tripled [25].Concept Learning.
The performance of the conceptlearning component has been compared to standard learningmechanisms based on the terminological classier availablein any sort of description logics systems.
Our data indicatean increase of performance of 8% (87% accuracy, while thatof standard classiers is on the order of 79%) [12].Evaluating a text knowledge acquisition rather than an IEsystem poses hard methodological problems [2].
The mainreason being that a gold standard for comparison | whatconstitutes a canonical, commonly agreed upon interpreta-tion of the content of a text?
| is hard to establish, evenfor technical texts.
A follow-up problem is constituted bythe lack of a signicant amount of annotated text knowl-edge bases on which comparative analyses might be assessed.MUC-style evaluation metrics, e.g., have already been qual-ied not to adequately reect the functionality of less con-strained text understanders [29].4.
CONCLUSIONSA major hypothesis underlying the design of SynDiKATeis that ignoring the referential relations between adjacentutterances will lead to referentially incomplete, invalid, orincoherent text knowledge bases.
We determine plausiblediscourse units for reference resolution using the centeringmodel.
This allows us to deal with various forms of pronom-inal, nominal and functional anaphora in a uniform way.In order to establish local coherence at the text represen-tation level, single discourse entities related by anaphoricexpressions have to be conceptually linked.
We claim thatonly sophisticated knowledge representation languages withpowerful terminological reasoning capabilities, such as thosefrom the KL-ONE family, are able to deal with the full rangeof challenges of referentially adequate text understanding, inparticular considering nominal and functional anaphora.These two types of anaphora pose an enormous burdenon the availability of rich domain knowledge.
We respondto this challenge in two ways.
In a large-scale knowledgeengineering eort, we semi-automatically transform a se-mantically weak though huge thesaurus-style medical knowl-edge source into a terminological knowledge base.
If such ahuman-made resource is missing, we turn to a purely auto-matic approach of bootstrapping a given domain knowledgebase as part of on-going text understanding processes.The depth of understanding we provide comes closest tosystems such as Scisor [18], Tacitus [15] or Pundit/Kernel[17], but SynDiKATe's knowledge acquisition strategies orlearning capabilities have no counterpart there.
Text under-standers which incorporate learning components are evenrarer but systems such as Snowy [3] or Wrap-Up [23] ei-ther have a very narrow domain theory and lack robustnessfor dealing with unseen input eectively, or fail to accountfor a wide range of referential text phenomena, respectively.5.
ACKNOWLEDGMENTSThe development of the SynDiKATe system has been sup-ported by various grants from Deutsche Forschungsgemeinschaftunder Ha 2097/*.
SynDiKATe would not have come to existencewithout the exciting contributions and enthusiasm of current andformer members of the group, in particular, Steen Staab,Katja Markert, Michael Strube, Martin Romacker, Stefan Schulz,Klemens Schnattinger, Norbert Broker, Peter Neuhaus, SusanneSchacht, Manfred Klenner, and Holger Schauer.6.
REFERENCES[1] Jim Cowie and Wendy Lehnert.
Information extraction.Communications of the ACM, 39(1):80{91, 1996.
[2] Carol Friedman and George Hripcsak.
Evaluating naturallanguage processors in the clinical domain.
Methods ofInformation in Medicine, 37(4/5):334{344, 1998.
[3] Fernando Gomez and Carlos Segami.
The recognition andclassication of concepts in understanding scientic texts.Journal of Experimental and Theoretical ArticialIntelligence, 1(1):51{77, 1989.
[4] Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.Centering: A framework for modeling the local coherence ofdiscourse.
Computational Linguistics, 21(2):203{225, 1995.
[5] Udo Hahn, Norbert Broker, and Peter Neuhaus.
Let'sParseTalk: Message-passing protocols for object-orientedparsing.
In H. Bunt and A. Nijholt, editors, Advances inProbabilistic and other Parsing Technologies, pages177{201.
Kluwer, 2000.
[6] Udo Hahn, Katja Markert, and Michael Strube.
Aconceptual reasoning approach to textual ellipsis.
InProceedings of the ECAI'96, pages 572{576, 1996.
[7] Udo Hahn and Ulrich Reimer.
Knowledge-based textsummarization: Salience and generalization operators forknowledge base abstraction.
In I. Mani and M. Maybury,editors, Advances in Automatic Text Summarization, pages215{232.
MIT Press, 1999.
[8] Udo Hahn and Martin Romacker.
Content management inthe SynDiKATe system: How technical documents areautomatically transformed to text knowledge bases.
Data &Knowledge Engineering, 35(2):137{159, 2000.
[9] Udo Hahn, Martin Romacker, and Stefan Schulz.
Discoursestructures in medical reports { watch out!
The generationof referentially coherent and valid text knowledge bases inthe medSynDiKATe system.
International Journal ofMedical Informatics, 53(1):1{28, 1999.
[10] Udo Hahn, Martin Romacker, and Stefan Schulz.
Howknowledge drives understanding: Matching medicalontologies with the needs of medical language processing.Articial Intelligence in Medicine, 15(1):25{51, 1999.
[11] Udo Hahn, Susanne Schacht, and Norbert Broker.Concurrent, object-oriented natural language parsing: TheParseTalk model.
International Journal ofHuman-Computer Studies, 41(1/2):179{222, 1994.
[12] Udo Hahn and Klemens Schnattinger.
Towards textknowledge engineering.
In Proceedings of the AAAI'98,pages 524{531, 1998.
[13] Udo Hahn, Stefan Schulz, and Martin Romacker.Partonomic reasoning as taxonomic reasoning in medicine.In Proceedings of the AAAI'99, pages 271{276, 1999.
[14] Udo Hahn and Michael Strube.
Centering in-the-large:Computing referential discourse segments.
In Proceedings ofthe ACL'97/EACL'97, pages 104{111, 1997.
[15] Jerry R. Hobbs, Mark E. Stickel, Douglas E. Appelt, andPaul Martin.
Interpretation as abduction.
ArticialIntelligence, 63(1/2):69{142, 1993.
[16] Katja Markert and Udo Hahn.
On the interaction ofmetonymies and anaphora.
In Proceedings of the IJCAI'97,pages 1010{1015, 1997.
[17] Martha S. Palmer, Rebecca J. Passonneau, Carl Weir, andTim Finin.
The Kernel text understanding system.Articial Intelligence, 63(1/2):17{68, 1993.
[18] Lisa F. Rau, Paul S. Jacobs, and Uri Zernik.
Informationextraction and text summarization using linguisticknowledge acquisition.
Information Processing &Management, 25(4):419{428, 1989.
[19] Martin Romacker and Udo Hahn.
An empirical assessmentof semantic interpretation.
In Proceedings of the NAACL2000, pages 327{334, 2000.
[20] Martin Romacker, Katja Markert, and Udo Hahn.
Leansemantic interpretation.
In Proceedings of the IJCAI'99,pages 868{875, 1999.
[21] Stefan Schulz and Udo Hahn.
Knowledge engineering bylarge-scale knowledge reuse: Experience from the medicaldomain.
In Proceedings of KR 2000, pages 601{610, 2000.
[22] Stefan Schulz, Udo Hahn, and Martin Romacker.
Modelinganatomical spatial relations with description logics.
InProceedings of the AMIA 2000, pages 779{783, 2000.
[23] Stephen Soderland and Wendy Lehnert.
Wrap-up: Atrainable discourse module for information extraction.Journal of Articial Intelligence Research, 2:131{158, 1994.
[24] Steen Staab and Udo Hahn.
Comparatives in context.
InProceedings of the AAAI'97, pages 616{621, 1997.
[25] Steen Staab and Udo Hahn.
\Tall", \good", \high" {compared to what?
In Proceedings of the IJCAI'97, pages996{1001, 1997.
[26] Steen Staab and Udo Hahn.
Scalable temporal reasoning.In Proceedings of the IJCAI'99, pages 1247{1252, 1999.
[27] Michael Strube and Udo Hahn.
Functional centering:Grounding referential coherence in information structure.Computational Linguistics, 25(3):309{344, 1999.
[28] William A.
Woods and James G. Schmolze.
The Kl-Onefamily.
Computers & Mathematics with Applications,23(2/5):133{177, 1992.
[29] P. Zweigenbaum, J. Bouaud, B. Bachimont, J. Charlet, andJ.-F. Boisvieux.
Evaluating a normalized conceptual repre-sentation produced from natural language patient dischargesummaries.
In Proceedings of the AMIA'97, pages 590{594.
