A TR IPART ITE  PLAN-BASED MODEL OF  D IALOGUELynn LambertSandra CarberryDepar tment  of Computer  and Information SciencesUniversity of DelawareNewark,  Delaware 19716, USAAbst ract  1This paper presents a tripartite model of dialogue inwhich three different kinds of actions are modeled:domain actions, problem-solving actions, and dis-course or communicative actions.
We contend thatour process model provides a more finely differenti-ated representation f user intentions than previousmodels; enables the incremental recognition of com-municative actions that cannot be recognized froma single utterance alone; and accounts for implicitacceptance of a communicated proposition.1 Int roduct ionThis paper presents a tripartite model of di-alogue in which intentions are modeled on threelevels: the domain level (with domain goals such astraveling by train), the problem-solving level (withplan-construction goals such as instantiating a pa-rameter in a plan), and the discourse level (withcommunicative goals such as ezpressing surprise).Our process model has three major advantages overprevious approaches: 1) it provides a better repre-sentation of user intentions than previous modelsand allows the nuances of different kinds of goalsand processing to be captured at each level; 27 itenables the incremental recognition of commumca-tire goals that cannot be recognized from a singleutterance alone; and 3) it differentiates between il-locutionary effects and desired perlocutionary ef-fects, and thus can account for the failure of aninform act to change a heater's beliefs\[Per90\] ~.2 L imitat ions of CurrentMode ls  of DiscourseA number of researchers have contendedthat a coherent discourse consists of segmentsthat are related to one another through sometype of structuring relation\[Gri75, MT83\] or haveused rhetorical relations to generate coherenttext\[Hov88, MP90\].
In addition, some researchers1 This material is based upon work supported by the Na-tional Science Foundation under Grant No.
IRI-8909332.The Government has certain rights in this material.2We would ilke to thank Kathy McCoy for her commentson various drafts of this paper.have modeled iscourse based on the semantic rela-tionship of individual clauses\[Po186a\] or groups ofclauses\[Rei78\].
But all of the above fail to capturethe goal-oriented nature of discourse.
Grosz andSidner\[GS86\] argue that recognizing the structuralrelationships among the intentions underlying a dis-course is necessary to identify discourse structure,but they do not provide the details of a compu-tational mechanism for recognizing these relation-ships.To account for the goal-oriented nature ofdiscourse, many researchers have adopted theplanning/plan-recognition paradigm\[APS0, PA80\]in which utterances are viewed as part of a planfor accomplishing a goal and understanding con-sists of recognizing this plan.
The most well-developed plan-based model of discourse is that ofLitman and AIIen\[LA87\].
However, their discourseplans conflate problem-solving actions and commu-nicative actions.
For example, their Correct-Planhas the flavor of a problem-solving plan that onewould pursue in attempting to construct anotherplan, whereas their Identify-Parameter takes onsome of the characteristics of a communicative planthat one would pursue when conveying information.More significantly, their model cannot capture therelationship among several utterances that are allpart of the same higher-level discourse plan if thatplan cannot be recognized and added to their planstack based on analysis of the first utterance alone.Thus, if more than one utterance is necessary torecognize a discourse goal (as is often the case, forexample, with warnings), Litman and Allen's modelwill not be able to identify the discourse goal pur-sued by the two utterances together or what rolethe first utterance plays with respect o the sec-ond.
Consider, for example, the following pair ofutterances:(1) The city of zz~ is considering filing forbankruptcy.
(2) One of your mutual funds owns zzz bonds.Although neither of the two utterances alone con-stitutes a warning, a natural anguage system mustbe able to recognize the warning from the set of twoutterances together.Our tripartite model of dialogue overcomesthese limitations.
It differentiates among domain,problem-solving, and communicative actions yetmodels the relationships among them, and enables47the recognition of communicative actions that takemore than one utterance to achieve but which can-not be recognized from the first utterance alone.In the remainder of this paper, we willpresent our tripartite model, motivating why ourmodel recognizes three different kinds of goals, de-scribing our dialogue model and how it is built in-crementally as a discourse proceeds, and illustrat-ing this plan inference process with a sample dia-logue.
Finally, we will outline our current researchon modeling negotiation dialogues and recognizingdiscourse acts such as expressing surprise.3 A Tr ipar t i te  Mode l3.1 Kinds of Goals and PlansOur plan recognition framework recognizesthree different kinds of goals: domain, problem-solving, and discourse.
In an information-seekingor expert-consultation dialogue, one participant isseeking information and advice about how to con-struct a plan for achieving some domain goal.
Aproblem-solving goal is a metagoal that is pursuedin order to construct a domain plan\[Wil81, LA87,Ram89\].
For example, if an agent has a goal ofearning an undergraduate d gree, the agent mighthave the problem-solving goal of selecting the in-stantiation of the degree parameter as BA or BSand then the problem-solving goal of building a sub-plan for satisfying the requirements for that degree.A number of researchers have demonstrated the im-portance of modeling domain and problem-solvinggoals\[PA80, WilS1, LA87, vBC86, Car87, Ram89\].Intuitively, a discourse goal is the com-municative goal that a speaker has in making anutterance\[.Car89\], such as obtaining informationor expressing surprise.
Recognition of discoursegoals provides expectations for subsequent utter-ances and suggests how these utterances should beinterpreted.
For example, the first two utterancesin the following exchange stablish the expectationthat S1 will either accept S2's response, or that S1will pursue utterances directed toward understand-ing and accepting it\[Car89\].
Consequently, Sl's sec-ond utterance should be recognized as expressingsurprise at S2's statement.SI: When does CS400 meet?$2:GS400 meets on Monday from 7.9p.m.SI: GS400 meets at night?A robust natural anguage system must recognizediscourse goals and the beliefs underlying them inorder to respond appropriately.The plan library for our process model con-tains the system's knowledge of goals, actions, andplans.
Although domain plans are not mutuallyknown by the participants\[Po186b\], how to commu-nicate and how to solve problems are common skillsthat people use in a wide variety of contexts, sothe system can assume that knowledge about dis-course and problem-solving plans is shared knowl-edge.
Our representation of a plan includes aheader giving the name of the plan and the actionit accomplishes, preconditions, applicability condi-tions, constraints, a body, effects, and goals.
Appli-cability conditions represent conditions that mustbe satisfied for the plan to be reasonable to pur-sue in the given situation whereas constraints limitthe allowable instantiation of variables in each ofthe components ofa plan\[LAB7, Car87\].
Especiallyin the case of discourse plans, the goals and effectsare likely to be different.
This allows us to dif-ferentiate between illocutionary and perlocutionaryeffects and capture the notion that one can, for ex-ample, perform an inform act without the heareradopting the communicated proposition.
3 Figure 1presents three discourse plans and one problem-solving and domain plan.3.2 Structure of the ModelAgents use utterances to perform commu-nicative acts, such as informing or asking a ques-tion.
These discourse actions can in turn be partof performing other discourse actions; for example,providing background ata can be part of askinga question.
Discourse actions can take more thanone utterance to complete; asking for informationrequires that a speaker equest the information andbelieve that the request is acceptable (i.e., that thespeaker say enough to ensure that the speaker be-lieves that the request is understandable, justified,and the necessary background information isknownby the respondent).
Thus, actions at the discourselevel form a tree structure in which each node rep-resents a communicative action that a participantis performing and the children of a node representcommunicative actions pursued in order to performthe parent action.Information eeded for problem-solving ac-tions is obtained through discourse actions, so dis-course actions can be executed in order to performproblem-solving actions as well as being part ofother discourse actions.
Similarly, domain plansare constructed through problem-solving actions, soproblem-solving actions can be executed in order toeventually perform domain actions as well as beingpart of plans for other problem-solving actions.Therefore, our Dialogue Model (DM) con-tains three levels of tree structures, 4 one for eachkind of action (discourse, problem-solving, and do-main) with links among the actions on different lev-els.
At the lowest level the discourse actions arerepresented; these actions may contribute to theproblem-solving actions at the middle level which,ZConsider, for example, someone saying "I informed yonof X 6at you wouldn't 6elieve me.
"4The DM is really a mental model of intentions\[Pol80b\].The structures shown in our figures implicitly capture a num-ber of intentions that are attributed to the participants, suchas the intention that the hearer recognize that the speakerbelieves the applicability conditions for the just initiated dis-course actions are satisfied and the intention that the par-ticipants follow through with the subactions that are part ofplans for actions in theDM.48Domain Plan-D1: {_agent earns a minor in _subj}Action: Get-Minor(.agent, ..sub j)Prec: have-plan(_agent, Plan-D1, Get-minor(.agent, .sub j))Body: 1.
Complete-Form(.agent, change-of-major-form, add-minor)2.
Take-Required-Courses(.agent, .sub j)Effects: have-minor(_agent, -sub j)Goal: have-minor(_agent, _sub j)Action:AppCond:Constr:Problem-solvin~ Plan-P1:{_agent1 and _agent~ build a plan \]or -agent1 to do _action}Build-Plan(_agentl, .agent2, .
action)want(.agentl, .action)plan-for(.plan, .action)action-in-plan-for(Aaction, .action)Prec: selected(_agentl, .action, .plan)know(.agent2, want(.agentl, .action))knowref(..agentl, .prop, prec-of(.prop, -plan))knowref(.agent2, .prop, prec-of(-prop, .plan))knowref(.agentl, .\]action, eed-do(.agentl, .laction, .action))knowref(_agent2, .laction, need-do(-agentl, Aaction, .action))1. for all actions .laction in .plan, Instantiate-Vars(-agentl, .agent2, _laction)2. for all actions .laction in -plan, Build-Plan(.agentl, .agent2, .laction)have-plan(_agentl, .plan, .action)have-plan(-agentl, .p an, .action)Body:Effects:Goal:Discourse Plan-C1: {_agentl asks -agent~ /or the values of.term \]or which -prop is true}Action: Ask-Ref(.agentl, .agent2, .term, .prop)AppCond: want(-agentl, knowref(-agentl, _term, believe(.agent2, -prop)))--knowref(.agentl, .term, believe(.agent2, .prop))Constr: term-in(_term, .prop)Body: Request(_agentl, .agent2, Informref(_agent2, .agentl, _term, ..prop))Make-Question-Acceptable(_agentl, _agent2, _prop)Effects: believe(-agent2, want(.agentl, Informref(.agent2, .agent1, _term, .prop)))(goal: want(.agent2, Answer-Ref(.agent2, .agent1, -term, _prop))Discourse Plan-C2:{-agent1 in\]orms _agent2 o\] _prop}Action: Inform(.agentl, .agent2, .prop)AppCond: believe(.agentl, know(-agentl, .prop))-,believe(.agentl, believe(.agent2, .prop))Body: Tell(.,xgentl, .agent2, .prop)Make-Prop-Believable(.agentl, .agent2, .prop)Effects: believe(.agent2, want(.agentl, believe(.agent2, -prop)))Goal: know(.,xgent2, .prop)Discourse Plan-C3:{_agent1 ells _prop to .agent~}Action: Tell(.agentl, .agent2, .prop)AppCond: believe(.agentl, .prop)-~believe(_agentl, be ieve(.agent2, believe(.agentl, .prop)))Body: Surface-Inform(.agentl, .agent2, .prop)Make-Statement-Understood(_agentl, .agent2, -prop)Effects: told-about(_agent2, .prop)Goal: believe(.agent2, believe(-agentl, .prop))Figure 1: Sample Plans from the Plan Library49in turn, may contribute to the domain actions atthe highest level (see Figure 3).
The planning agentis the agent of all actions at the domain level, sincethe plan being constructed is for his subsequent ex-ecution.
Since we are assuming a cooperative di-alogue in which the two participants are workingtogether to construct a domain plan, both partic-ipants are joint agents of actions at the problem-solving level.
Both participants make utterancesand thus either participant may be the agent of anaction at the discourse level.For example, a DM derived from two ut-terances is shown in Figure 3; its construction isdescribed in Section 3.3.
The DM in Figure 3 in-dicates that the inform and the request were bothpart of a plan for asking for information; the informprovided background data enabling the informationrequest to be accepted by the hearer.
Furthermore,the actions at the discourse l vel were pursued in or-der to perform a Build-Plan action at the problem-solving level, and this problem-solving action is be-ing performed in order to eventually perform thedomain action of getting a math minor.
The cur-rent focus of attention on each level is marked withan asterisk.3.3 Building the Dialogue ModelOur process model uses plan inferencerules\[APS0, Car87\], constraint satisfaction\[LAB7\],focusing heuristics\[Car87\], andfeatures of the newutterance to identify the relationship between theutterance and the existing dialogue model.
Theplan inference rules take as input a hypothesizedaction Ai and suggest other actions (either at thesame level in the DM or at the immediately higherlevel) that might be the agent's motivation for Ai.The focusing heuristics order according tocoherence the ways in which the DM might be ex-panded on each of the three levels to incorporatethe actions motivating a new utterance.
Our focus-ing heuristics at the discourse level are:1.
Expand the plan for an ancestor of the cur-rently focused action in the existing DM sothat it includes the new utterance, preferringto expand ancestors closest o the currently fo-cused action.
This accounts for new utterancesthat continue discourse acts already in the DM.2.
Enter a new discourse action whose plan canbe expanded to include both the existing dis-course level of the DM and the new utterance.This accounts for situations in which actionsat the discourse level of the previous DM arepart of a plan for another discourse act thathad not yet been conveyed.3.
Begin a new tree structure at the discourselevel.
This accounts for initiation of new dis-course plans unrelated to those already in theDM.The focusing heuristics, however, are notidentical for all three levels.
Although it is not pos-sible to expand the plan for the focused action onthe discourse level since it will always be a surfacespeech act, continuing the plan for the currentlyfocused action or expanding it to include a newaction are the most coherent expectations on theproblem-solving and domain levels.
This is becausethe agents are most expected to continue with theproblem-solving and domain plans on which theirattention is currently centered.
In addition, sinceactions at the discourse and problem-solving lev-els are currently being executed, they cannot bereturned to (although a similar action can be initi-ated anew and entered into the model).
However,since actions at the domain level are part of a planthat is being constructed for future execution, adomain subplan already completely developed maybe returned to for revision.
Although such a shiftin attention back to a previously considered sub-plan is not one of the strongest expectations, it isstill possible at the domain level.
Furthermore, newand unrelated iscourse plans will often be pursuedduring the course of a conversation whereas it is un-likely that several different domain plans (each rep-resenting a topic shift) will be investigated.
Thus,on the domain level, a return to a previously con-sidered domain subplan is preferred over a shift toa new domain plan that is unrelated to any alreadyin the DM.In addition to different focusing heuristicsand different agents at each level, our tripartitemodel enables us to capture different rules regard-ing plan retention.
A continually growing dialoguestructure does not seem to reflect the informationretained by humans.
We contend that the domainplan that is incrementally fleshed out and built atthe highest level should be maintained through-out the dialogue, since it provides knowledge aboutthe agent's intended omain actions that will beuseful in providing cooperative advice.
However,problem-solving and discourse actions need not beretained indefinitely.
If a problem-solving or dis-course action has not yet completed execution, thenits immediate children should be retained in theDM, since they indicate what has been done as partof performing that as yet uncompleted action; itsother descendants can be discarded since the apar-ent actions that motivated them are finished.
(Forillustration purposes, all actions have been retainedin Figure 3.
)We have expanded on Litman and Allen'snotion of constraint satisfaction\[LA87\] and Allenand Perrault's use of beliefs\[AP80\].
Our applica-bility conditions contain beliefs by the agent of theplan, and our recognition algorithm requires thatthe system be able to plausibly ascribe these beliefsin recognizing the plan.
The algorithm is given thesemantic representation f an utterance.
Then planinference rules are used to infer actions that mightmotivate the utterance; the belief ascription processduring constraint satisfaction determines whetherit is reasonable to ascribe the requisite beliefs tothe agent of the action and, if not, the inferenceis rejected.
The focusing heuristics allow expecta-50tions derived from the existing dialogue context oguide the recognition process by preferring thoseinferences that can eventually lead to the most ex-pected expansions of the existing dialogue model.In \[Car89\] we claimed that a cooperativeparticipant must explicitly or implicitly accept aresponse or pursue discourse goals directed towardbeing able to accept he response.
Thus our modeltreats failure to initiate a negotiation dialogue asimplicit acceptance of the proposition conveyed bythe response.
Consider, for example, the followingdialogue:SI: Who is teaching CS360 next semester?$2: Dr. Baker.SI: What time does it meet?Since Sl's second utterance cannot be interpretedas initiating a negotiation dialogue, S1 has implic-itly accepted the proposition that Dr. Baker isteaching CS360 next semester as true.
This no-tion of implicit acceptance is similar to a restrictedform of Perrault's default reasoning about the ef-fects of an inform act\[Per90\] and is explained fur-ther in \[Lam91\].3 .4  An  ExampleAs an example of how our process modelassimilates utterances and can incrementally rec-ognize a discourse action that cannot be recognizedfrom a single utterance, consider the following:SI: (a) I want a math minor.
(b) What should I do?A few of the plans needed to handle this exampleare shown in Figure 1; these plans assume a co-operative dialogue.
From the surface inform, planinference rules suggest hat S1 is executing a Tellaction and that this Tell is part of an Inform ac-tion (the applicability conditions for both actionscan be plausibly ascribed to S1) and these are en-tered into the discourse level of the DM.
No fur-ther inferences on this level are possible since theInform can be part of several discourse plans andthere is no existing dialogue context hat suggestswhich of these S1 might be pursuing.
The systeminfers that S1 wants the goal of the Inform action,namely know(S2, want(S1, Get-Minor(S1, Math))).Since this proposition is a precondition for buildinga plan for getting a math minor, the system infersthat S1 wants Build-Plan(S1, $2, Get-Minor(S1,math)) and this Build.Plan action is entered intothe problem-solving level of the DM.
From this, thesystem infers that S1 wants the goal of that action;since this result is the precondition for getting amath minor, the system infers that S1 wants to geta math minor and this domain action is entered intothe domain level of the DM.
The resulting discoursemodel, with links between the actions at differentlevels and the current focus of attention on eachlevel marked with an asterisk, is shown in Figure 2.The semantic representation f (b) isjR|| * \[Build-Plan~Sl, S2, Get -Minor ,S1 ,DomLin Leve l  f ' ' ' ' "  " ' ' ' ' ' ' J! "
\[Oct-Minor{S,, M&th~ \] I gT en~ble -&rc  P rob lem-so lv lng  Leve l~.,h??
J ,Discourse  Leve l  m .5 j dmm~en~o em em am amamm| !g 8ub&ctioa-sre ~ J| !
!
, \[ T,n{s~, s~, ...~{s~, Q ,-Mi.o,(S~, M.,b), I tJ sub6ct\[on-&r?
~ J !
!Figure 2: Dialogue Model from the first utteranceSurface-Request(S1, $2, Informref(S2, $1,_action1, need-do(S1, _action1, .action2)))From this utterance we can infer that $1 is per-forming a Request and thus may be performing anAsk.Re?
action (since Request is part of the bodyof the plan for Ask-Re~) and that S1 may thus beperforming an Obtain-Info-Ref action (since Ask-Re?
is part of the body of the plan for Obtain-In?o-Re\]) and that S1 wants the goal of the Obtain-In?o-Re?
action (namely, that $1 know the subactionsthat he needs to do in order to perform _a~tion2),which is in turn a precondition for building a plan.This produces the inference that $1 wants Build-Plan(S1, $2, .action2) which is an action at theproblem-solving level.The focusing heuristics suggest that themost coherent expectation at the discourse level isthat Sl's discourse level actions are part of a planfor performing the Tell action that is the parentof the action that was previously marked as thecurrent focus of attention in the discourse model.However, no line of inference from the second ut-terance represents an expansion of this plan.
(Thismeans that the proposition was understood withoutany clarification.
5) Similarly, no expansion of theplan for the Inform action (the other ancestor of thefocus of attention in the existing DM) succeeds inlinking the new utterance to the DM.
(This meansthat the communicated proposition was acceptedwithout any squaring away of beliefs\[Jos82\].
)Since the first focusing heuristic was unsuc-cessful in finding a relationship between the newutterance and the existing dialogue model, the sec-5We are assuming  that  the  hearer  has  an  oppor tun i ty  tointervene after an ut terance.
Th is  is a simpli f icat ion andmust  eventual ly  be removed to capture  a heater ' s  av ing hisrequests  for clarif ication and  negot iat ion  of beliefs unt i l  theend of the speaker 's  complete turn.51en~ble-&rcDom.in Levelf, .q| ~* \[ Oct-Minor{St, }~\[sth) J | ien~ble-~r?Problem-solvlng Level| J8 ~ IBuild-Plzn(51 53 Get-Mluor(Sl M~th)) : 4 : '.-}ensb le -L rc  | m m mI rmm~mmmmmi O bt.~-l~fo-Ref(St. $2.
-&ctlonl.
need-dotS1..Lctlonl.
Oct-Minor(St. M~.th)))sub,ction-&rc ?\[ A.k-I~ef~$1, $2, .Actionl .
.
.
.
d-do~Sl, Get.~Iinor~Sl, l~.th)))sub.ction-src ?
.~ctlon I,\[ M.ke-qu.s*lon-*?~-p~.bie~S~, s  .
.
.
.
d-do~s~, Oe~-Mi~o.~S~, ~.tb))) Isnbsction-&r?
?
.~ctionl,' t Glve-Bzckg .
.
.
.
d($1, 52 .
.
.
.
t(SI, Oet-lviinor(Sl, Msth)), IJ \[ Iteed-do(Sl, .~ctlonl, Oct-Minor(S1, M~th))) J JJ sub&ction-.rc ?| , \[ lnform~Sl, $2 .
.
.
.
.
~Sl, CJet-MinorISl, l~4&th))) Ji sub,Lction-t,r?| need-do{S1!| sub.ctlon-&rcI \] Surf .
.
.
.
,ztform{Sl, $2 .
.
.
.
t($1, Oet-Minor(Sl, Ms|h))) \] .
JI I need-do{St, IDiscourse Levelsub.ct lon- , rclllllIltlllJlliI '  lluformrefq S~, Sl, .~ctionl,  .sct lont t }ei-Minor(Sl.
MAth'}))subtction-src I !Surf&ce-J~.equest(Sl, S2s lnfotmref(S2, SI, .sctionl, | J.~?tionl?
Oet.lCfiuor{Sl!
MAth}}} \] J. .
.
.
.
.
JFigure 3: Dialogue Model derived from two utterancesond focusing heuristic is tried.
It suggests that thenew utterance and the actions at the discourse l velin the existing DM might both be part of an ex-panded plan for some other discourse action.
Theinferences described above lead from (b) to the dis-course action Ask-Ref whose plan can be expandedas shown in Figure 3 to include, as background forthe Ask-Ref, the Inform and the Tell actions thatwere entered into the DM from (a).
s The focusingheuristics uggest that the most coherent continu-ation at the problem-solving level is that the newutterance is continuing the Build-Plan that was pre-viously marked as the current focus of attention atthat level.
This is possible by instantiating .action2with Get-minor(S1, math).
Thus the DM is ex-panded as shown in Figure 3 with the new focusof attention on each level marked with an asterisk.Note that Sl's overall goal of obtaining informationwas not conveyed by (a) alone; consequently, onlyafter both utterances were coherently related couldit be determined that (a) was paxt of an overalldiscourse plan to obtain information and that (a)was intended to provide background ata for therequest being made in (b).
76Note that the actions in the body of Ask.Re\] ~re notordered; an agent can provide d~'ification and backgroundinformation before or after asking a question.7An inform action could also be used for other pur-poses, including justifying a question and merely conveyinginformation.Further queries would lead to more elaboratetree structures on the problem-solving and domainlevels.
For example, suppose that S1 is told thatMath 210 is a required course for a math minor.Then a subsequent query such as Who is teach-ing Math 210 next semester .
would be performinga discourse act of obtaining information in orderto perform a problem-solving action of instantiat-ing a parameter in a Learn-Material domain action.Since learning the materiM from one of the teach-ers of a course is part of a domain plan for taking acourse and since instantiating the parameters in ac-tions in the body of domain plans is part of buildingthe domain plan, further inferences would indicatethat this Instanfiafe- Wars problem-solving action isbeing executed in order to perform the problem-solving action of building a plan for the domainaction of taking Math 210 in order to build a planto get a math minor.
Consequently, the domainand problem-solving levels would be expanded sothat each contained several plans, with appropriatelinks between the levels.4 Cur rent  and  Future  WorkWe are currently examining the applicationsthat this model has in modeling negotiation dia-logues and discourse acts such as convince, warn,and express urprise.
To extend our notion of im-plicit acceptance ofa proposition to negotiation di-52alogues, we are exploring treating a discourse planas having successfully achieved its goal if it is plau-sible that all of its subacts have achieved their goalsand all of its applicability conditions (except hosenegated by the goal) are still true after the subactshave been executed.Especially in negotiation dialogues, a systemmust account for the fact that a user may changehis mind during a conversation.
But often peopleonly slightly modify their beliefs.
For example, thesystem might inform the user of some propositionabout which the user previously held no beliefs.
Inthat case, if the user has no reason to disbelieve theproposition, the user may adopt hat proposition asone of his own beliefs.
However, if the user disbe-lieved the proposition before the system performedthe inform, then the user might change from disbe-lief to neither belief nor disbelief; a robust model ofunderstanding must be able to handle a responsethat expresses doubt or even disbelief at a previousutterance, especially in modeling arguments andnegotiation dialogues.
Thus, a system should beable to (1) represent levels of belief, (2) recognizehow a speaker's utterance conveys these differentlevels of belief, (3) use these levels of belief in recog-nizing discourse plans, and (4) use previous contextand a user's responses to model changing beliefs.We are investigating the use of a multi-levelbelief model to represent the strength of an agent'sbeliefs and are studying how the form of an utter-ance and certain clue words contribute to conveyingthese beliefs.
Consider, for example, the followingtwo utterances:(1) Is Dr. Smith teaching CSMO?
(2) Isn't Dr. Smith teaching CSMO?A simple yes-no question as in utterance (1) sug-gests only that the speaker doesn't know whetherDr.
Smith teaches CS310 whereas the form of thequestion in utterance (2) suggests that the speakerhas a relatively strongbelief that Dr. Smith teachesCS310 but is uncertain of this.
These beliefs con-veyed by the surface speech act must be taken intoaccount during the plan recognition process.
Thusour plan recognition algorithm will first use the ef-fects of the surface speech act to suggest augmen-tations to the belief model.
These augmentationswill then be taken into account in deciding whetherrequisite beliefs for potential discourse acts can beplausibly ascribed to the speaker and will enable usto identify such discourse actions as expressing sur-prise.
\[Lam91\] further discusses the use of a multi-level belief model and its contribution i  modelingdialogue.cution level (corresponding to queries after commit-ment has been made to achieve agoal in a particularway).
In our tripartite model, discourse, problem-solving, and domain plans form a hierarchy withlinks between adjacent levels.
Whereas Ramshaw'sexploration level captures the consideration of al-ternative plans, our intermediate level captures thenotion of problem-solving and plan-construction,whether or not there has been a commitment toa particular way of achieving a domain goal.
Thusa query such as To whom do I make out the check?would be recognized as a query against he domainexecution level in Ramshaw's model (since it is aquery made after commitment to a plan such asopening a passbook savings account\[Ram91\]), butour model would treat it as a discourse plan thatis executed to further the problem-solving plan ofinstantiating a parameter in an action in a domainplan - -  i.e., our model would view the agent as ask-ing a question in order to further the constructionof his partially constructed domain plan.Our tripartite model offers several advan-tages.
Ramshaw's model assumes that the top-leveldomain plan is given at the outset of the dialogueand then his model expands that plan to accom-modate user queries.
Our model, on the otherhand, builds the DM incrementally at each levelas the dialogue progresses; it therefore can han-dle bottom-up dialogues\[Car87\] in which the user'soverall top-level goal is not explicitly known at theoutset and can recognize discourse actions that can-not be identified from a single utterance.
In addi-tion, our domain, problem-solving, and discourseplans are all recognized incrementally using basi-cally the same plan recognition algorithm on eachlevel\[Wil81\].
Consequently, we foresee being ableto extend our model to include additional pairs ofproblem-solving and discourse levels whose domainlevel contains an existing problem-solving or dis-course plan; this will enable us to handle utter-ances uch as What should we work on next?
(querytrying to further construction of a problem-solvingplan) and Do you have information about .
.
.
?
(query trying to further construction of a discourseplan to obtain information).Ramshaw's plan exploration strategies, hisdifferentiation between exploration and commit-ment, and his heuristics for recognizing adoption ofa plan are very important.
While our work has notyet addressed these issues, we believe that they areconsistent with our model and are best addressed atour problem-solving level by adding new problem-solvin~ metaplans.
Such an incorporation will haveseverat advantages, including the ability to handleutterances such as5 Re la ted  WorkRamshaw\[Ram91\] has developed a model ofdiscourse that contains a domain execution level,an exploration level, and a discourse level.
In hismodel, discourse plans can refer either to the explo-ration level (corresponding to queries about possi-ble ways of achieving a goal) or to the domain exe-I f  I decide to get a BA degree, then I'll takeFrench to meet the foreign language requirement.In the above case, the speaker is still exploring aplan for getting a BA degree, but has committedto taking French to satisfy the foreign language re-quirement should the plan for the BA degree beadopted.
It does not appear that Ramshaw's model53can handle such contingent commitment.
This en-richment of our problem-solving level may necessi-tate changes to our focusing heuristics.6 Conc lus ionsWe have presented a tripartite model of dia-logue that distinguishes between domain, problem-solving, and discourse or communicative actions.By modeling each of these three kinds of actionsas separate tree structures, with links between theactions on adjacent levels, our process model en-ables the incremental recognition of discourse ac-tions that cannot be identified from a single ut-terance alone.
However, it is still able to cap-ture the relationship between discourse, problem-solving, and domain actions.
In addition, it pro-vides a more finely differentiated representation fuser intentions than previous models, allows the nu-ances of different kinds of processing (such as dif-ferent focusing expectations and information reten-tion) to be captured at each level, and accounts forimplicit acceptance ofa communicated proposition.Our current work involves using this model to han-dle negotiation dialogues in which a hearer does notautomatically accept as valid the proposition com-municated by an inform action.ReferenceslAP80\]\[CarS7\]\[Car89\]\[Gri75\]\[GS86\]\[HovS8\]\[Jos82\]\[LA87\]James F. Allen and C. Raymond Perrault.Analyzing intention in utterances.
Artifi-cial Intelligence, 15:143-178, 1980.Sandra Carberry.
Pragmatic modeling:Toward a robust natural anguage inter-face.
Computational Intelligence, 3:117-136, 1987.Sandra Carberry.
A pragmatic.s-based ap-proach to ellipsis resolution.
Computa-tional Linguistics, 15(2):75-96, 1989.Joseph E. Grimes.
The Thread of Dis-course.
Mouton, 1975.Barbara Grosz and Candace Sidner.
At-tention, intention, and the structure ofdiscourse.
Computational Linguistics,12(3):175-204, 1986.Eduard H. Hovy.
Planning coherentmultisentential text.
Proceedings of the~6th Annual Meeting of the Associationfor Computational Linguistics, pages 163-169, 1988.Aravind K. Joshi.
Mutual beliefs inquestion-answer systems.
In N. Smith, ed-itor, Mutual Beliefs, pages 181-197, NewYork, 1982.
Academic Press.Diane Litman and James Allen.
A planrecognition model for subdialogues in con-versation.
Cognitive Science, 11:163-200,1987.\[Lam91\]\[MP90\]\[MT83\]\[PASO\]\[Per90\]\[Po186a\]\[Po186b\]\[Ram89\]\[Ram91\]\[Rei78\]\[vBC86\]\[Wil81\]Lynn Lambert.
Modifying beliefs in aplan-based iscourse model.
In Proceed-ings of the 29th Annual Meeting of theACL, Berkeley, CA, June 1991.Johanna Moore and Cecile Paris.
Plan-ning text for advisory dialogues.
In Pro-ceedings of the 27th Annual Meeting of theAssociation for Computational Linguis-tics, pages 203-211, Vancouver, Canada,1990.William C. Mann and Sandra A. Thomp-son.
Relational propositions in dis-course.
Technical Report ISI/RR-83-115,ISI/USC, November 1983.Raymond Perrault and James Allen.
Aplan-based analysis of indirect speechacts.
American Journal of ComputationalLinguistics, 6(3-4):167-182, 1980.Raymond Perrault.
An application of de-fault logic to speech act theory.
In PhilipCohen, Jerry Morgan, and Martha Pol-lack, editors, Intentions in Communica-tion, pages 161-185.
MIT Press, Cam-bridge, Massachusetts, 1990.Livia Polanyi.
The linguistics discoursemodel: Towards a formal theory of dis-course structure.
Technical Report 6409,Bolt Beranek and Newman LaboratoriesInc., Cambridge, Massachusetts, 1986.Martha Pollack.
Inferring Domain Plansin Question-Answering.
PhD thesis,University of Pennsylvania, Philadelphia,Pennsylvania, 1986.Lance A. Ramshaw.
Pragmatic Knowl-edge for Resolving lll-Formedness.
PhDthesis, University of Delaware, Newark,Delaware, June 1989.Lance A. Rarnshaw.
A three-level modelfor plan exploration.
In Proceedings of the29th Annual Meeting of the Associationfor Computational Linguistics, Berkeley,California, 1991.Rachel Reichman.
Conversational co-herency.
Cognitive Science, 2:283-327,1978.Peter van Beck and Robin Cohen.
To-wards user specific explanations from ex-pert systems.
In Proceedings of the SixthCanadian Conference on Artificial Intelli-gence, pages 194-198, Montreal, Canada,1986.Robert Wilensky.
Meta-planning: Repre-senting and using knowledge about plan-ning in problem solving and natural an-g uage understanding.
Cognitive Science, :197-233, 1981.54
