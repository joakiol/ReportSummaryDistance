Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics?System Demonstrations, pages 73?78,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsMUSEEC: A Multilingual Text Summarization ToolMarina Litvak1, Natalia Vanetik1, Mark Last2, and Elena Churkin11Department of Software EngineeringShamoon College of Engineering, Beer Sheva, Israel{marinal,natalyav}@sce.ac.il, elenach@ac.sce.ac.il2Department of Information Systems EngineeringBen Gurion University of the Negev, Beer Sheva, Israelmlast@bgu.ac.ilAbstractThe MUSEEC (MUltilingual SEntenceExtraction and Compression) summariza-tion tool implements several extractivesummarization techniques ?
at the level ofcomplete and compressed sentences ?
thatcan be applied, with some minor adapta-tions, to documents in multiple languages.The current version of MUSEEC pro-vides the following summarization meth-ods: (1) MUSE ?
a supervised summa-rizer, based on a genetic algorithm (GA),that ranks document sentences and ex-tracts top?ranking sentences into a sum-mary, (2) POLY ?
an unsupervised sum-marizer, based on linear programming(LP), that selects the best extract of docu-ment sentences, and (3) WECOM ?
an un-supervised extension of POLY that com-piles a document summary from com-pressed sentences.
In this paper, we pro-vide an overview of MUSEEC methodsand its architecture in general.1 IntroductionHigh quality summaries can significantly reducethe information overload of many professionals ina variety of fields.
Moreover, the publication ofinformation on the Internet in an ever?increasingvariety of languages dictates the importance of de-veloping multi?lingual summarization tools thatcan be readily applied to documents in multiplelanguages.There is a distinction between extractive sum-marization that is aimed at the selection of a sub-set of the most relevant fragments ?
mostly com-plete sentences ?
from a source text, and abstrac-tive summarization that generates a summary as areformulated synopsis expressing the main idea ofthe input documents.Unlike the abstractive summarization methods,which require natural language processing oper-ations, language-independent summarizers workin an extractive manner, usually via ranking frag-ments of a summarized text by a relevance scoreand selecting the top-ranked fragments (e.g., sen-tences) into a summary.
Because sentence scor-ing methods, like MUSE (MUltilingual SentenceExtractor) (Last and Litvak, 2012), use a greedyapproach, they cannot necessarily find the best ex-tract out of all possible combinations of sentences.Another approach, based on the maximum cov-erage principle (McDonald, 2007; Gillick andFavre, 2009), tries to find the best subset of ex-tracted sentences.
This problem is known as NP-hard (Khuller et al, 1999), but an approximate so-lution can be found by the POLY algorithm (Lit-vak and Vanetik, 2013) in polynomial time.Given the tight length constraints, extractivesystems that select entire sentences are quite lim-ited in the quality of summaries they can produce.Compressive summarization seeks to overcomethis limitation by compiling summaries from com-pressed sentences that are composed of strictly rel-evant information(Knight and Marcu, 2002).
WE-COM (Weighted COMpression) summarizationapproach (Vanetik et al, 2016) combines methodsfor term weighting and sentence compression intoa weighted compression model.
WECOM extendsPOLY by utilizing the choice of POLY?s objectivefunctions for the term-weighting model.In this paper, we present MUSEEC, a multi-lingual text summarization platform, which cur-rently implements three single-document sum-marization algorithms: MUSE (Last and Lit-vak, 2012), POLY algorithm (Litvak and Vanetik,2013), and WECOM (Vanetik et al, 2016).73Figure 1: MUSEEC pipeline2 MUSEEC: OverviewMUSEEC can be applied to documents in multi-ple languages.
The current version was tested onnine languages: English, Hebrew, Arabic, Persian,Russian, Chinese, German, French, and Spanish,and its summarization quality was evaluated onthree languages: English, Hebrew and Arabic.1The sections below provide brief descriptions ofthe system architecture and its main components.2.1 MUSEEC ArchitectureAs shown in Figure 1, MUSEEC runs a pipelinethat is composed of the following components:1.
Preprocessing.
MUSEEC can work with docu-ments written in any language by treating the textas a sequence of UTF-8 characters.
It performs thefollowing pre-processing operations: (1) sentencesegmentation, (2) word segmentation, (3) stem-ming, and (4) stop-word removal.
The last twooperations are skipped if they are unavailable fora given language.
Some optional, linguistic fea-tures require Part-of Speech (POS) tagging as apre-processing step as well.2.
Training.
This stage is optional and it is rel-evant only for the supervised MUSE algorithm.Given a set of training parameters, MUSE findsthe best vector of weights for a linear combinationof chosen sentence features.
The resulting vector(trained model) can be saved and used for futuresummarization of documents in the same or anyother language.3.
Ranking.
At this stage, entire sentences or theirparts (in case of compressive summarization) areranked.4.
Sentence Compression.
This stage is also op-tional and it is relevant only for compressive sum-marization performed by WECOM.
Given rankedsentence parts, new, shorter sentences are com-piled and ranked.1MUSEEC also participated in MultiLing 2011, 2013,and 2015 contests on English, Hebrew and Arabic, anddemonstrated excellent results.5.
Extraction.
Complete sentences are selectedin the case of MUSE and POLY, and compressedsentences in the case of WECOM.6.
Postprocessing.
The generated summaries canbe post-processed by anaphora resolution (AR)and named entity (NE) tagging operations, ifthe corresponding tools are provided for a givenlanguage.
MUSEEC utilizes Stanford CoreNLPpackage for English.7.
Results Presentation.
Summaries are pre-sented in two formats: sentences highlighted inthe original document, selected by the user froma list of input documents, and a list of extractedsentences shown in their original order.
The usercan also sort sentences by their rank and see theirscores.MUSEEC allows the user to setup various sum-marization parameters, general and specific for achosen algorithm, which are listed in Table 1.
Thetable does not contain explicit WECOM settingsbecause running WECOM equivalent to runningPOLY with ?compressive?
choice for the summarytype.2.2 MUltilingual Sentence Extractor (MUSE)MUSE implements a supervised learning ap-proach to extractive summarization, where thebest set of weights for a linear combination of sen-tence scoring metrics is found by a GA trainedon a collection of documents and their gold stan-dard summaries.
MUSE training can be performedfrom the MUSEEC tool.
The obtained weightingvector is used for sentence scoring in future sum-marizations.
Since most sentence scoring methodshave a linear computational complexity, only thetraining phase of MUSE, which may be appliedin advance, is time-consuming.
In MUSEEC, onecan use ROUGE-1 and ROUGE-2, Recall (Lin andHovy, 2003)2as fitness functions for measuringsummarization quality?similarity with gold stan-2We utilized the language-independent implementation ofROUGE that operates Unicode characters (Krapivin, 2014)74Parameter Description Possible Defaultname values valueGeneralInput path Documents folder Path nameOutput path Summaries folder Path nameSummary type Summarization Compressive, Extractiveapproach ExtractiveMethod Summarization MUSE, POLY MUSE (extr.
),method WECOM (comp.
)Limit by Summary length unit Words, Sentences,Ratio, Characters WordsLimit Summary length limit Numeric value Dependson unitAR Anaphora resolution Check box uncheckedNER Named Entity tagging Check box uncheckedMUSEMode Train a new model, Train, Summarize, Summarizesummarize documents Evaluateevaluate summarizerModel Model to save Path name(training mode), ormodel to use(summarize mode)Sent.
features Sentence scoring 31 basic metrics, 31 basic metricsfeatures 75 linguistic featuresGA trainingRatio split Ratio of training data [0..1] 1Population GA settings 500Size GA settings 100Elite count GA settings 5Rouge Rouge type as 1, 2 Rouge-1a fitness func.POLYObjective Optimization 8 functions, Function 2function function described in in Section 2.3Section 2.3Table 1: MUSEEC general and method-specificparameters.dard summaries, which should be maximized dur-ing the training.
The reader is referred to (Litvaket al, 2010) for a detailed description of the opti-mization procedure implemented by MUSE.The user can choose a subset of sentence met-rics that will be included by MUSE in the lin-ear combination.
By default, MUSEEC will usethe 31 language-independent metrics presentedin (Last and Litvak, 2012).
MUSEEC also allowsthe user to employ additional, linguistic features,which are currently available only for the Englishlanguage.
These features are based on lemmatiza-tion, multi-word expressions (MWE), NE recogni-tion (NER), and POS tagging, all performed withStanford CoreNLP package.
The list of linguisticfeatures is available in (Dlikman, 2015).The training time of the GA is proportional tothe number of GA iterations3multiplied by thenumber of individuals in a population, times thefitness (ROUGE) evaluation time.
The summa-rization time (given a model) is linear in numberof terms for all basic features.3On average, in our experiments the GA performed 5?
6iterations of selection and reproduction before reaching con-vergence.2.3 POLYnomial summarization withPOLYtopes (POLY)Following the maximum coverage principle, thegoal of POLY, which is an unsupervised summa-rizer, is to find the best subset of sentences that,under length constraints, can be presented as asummary.
POLY uses an efficient text represen-tation model with the purpose of representing allpossible extracts4without computing them explic-itly, that saves a great portion of computation time.Each sentence is represented by a hyperplane, andall sentences derived from a document form hyper-plane intersections (polytope).
Then, all possibleextracts can be represented by subplanes of hyper-plane intersections that are not located far from theboundary of the polytope.
POLY is aimed at find-ing the extract that optimizes the chosen objectivefunction.MUSEEC provides the following categories ofobjective functions, described in detail in (Litvakand Vanetik, 2013).1.
Maximal weighted term sum, that maximizesthe information coverage as a weighted term sumwith following weight options supported:1.
Term sum: all terms get weight 1;2.
POS F: terms appearing earlier in the text gethigher weight;3.
POS L: terms appearing close to the end ofthe text get higher weight;4.
POS B: terms appearing closer to text bound-aries (beginning or end) get higher weight;5.
TF: weight of a term is set to its frequency inthe document;6.
TF IDF: weight of a term is set to its tf*idfvalue;2.
McDonald ?
maximal sentence coverage andminimal sentence overlap, that maximizes thesummary similarity to the text and minimizes thesimilarity between sentences in a summary, basedon the Jaccard similarity measure (based on (Mc-Donald, 2007));3.
Gillick ?
maximal bigram sum and minimalsentence overlap, that maximizes the informationcoverage as a bigram sum while minimizing thesimilarity between sentences (based on (Gillick4exponential in the number of sentences75and Favre, 2009)).All functions produce term weights in [0, 1] thatare then used for calculating the importance scoresof each sentence.Like in MUSE, the sentences with the highestscore are added to the summary in a greedy man-ner.
The overall complexity of POLY is polyno-mial in number of sentences.
Further details aboutthe POLY algorithm can be found in (Litvak andVanetik, 2013).2.4 WEighted Compression (WECOM)In WECOM (Vanetik et al, 2016), we shortensentences by iteratively removing ElementaryDiscourse Units (EDUs), which were defined asgrammatically independent parts of a sentencein (Marcu, 1997).
We preserve the importantcontent by optimizing the weighting function thatmeasures cumulative importance and preserve avalid syntax by following the syntactic structureof a sentence.
The implemented approach consistsof the following steps:Term weight assignment.
We apply a weightingmodel (using one of the options available forPOLY) that assigns a non-negative weight to eachoccurrence of every term in all sentences of thedocument.EDU selection and ranking.
At this stage,we prepare a list of candidate EDUs for re-moval.
First, we generate the list of EDUs fromconstituency-based syntax trees (Manning andSch?utze, 1999) of sentences.
Then, we omitfrom the list those EDUs that may create agrammatically incorrect sentence if they wereto be removed.
Finally, we compute weightsfor all remaining EDU candidates from termweights obtained in the first stage and sort themby increasing weight.Budgeted sentence compression and selection.We define a summary cost as its length measuredin words or characters5.
We are given a budgetfor the summary cost, for example, the maximalnumber of words in a summary.
The compressivepart of WECOM is responsible for selectingEDUs in all sentences such that(1) the weight to cost ratio of the summary ismaximal; and(2) the summary length does not exceed a givenbudget.5depends on the user?s choice of a summary maximallengthThe compressed sentences are expected to bemore succinct than the originals, to contain theimportant content from the originals, and to begrammatically correct.
The compressed sentencesare selected to a summary by the greedy manner.The overall complexity of WECOM is bound byNlog(N), where N is a number of terms in allsentences.3 Experimental ResultsTables 2, 3, and 4 contain the summarized re-sults of automated evaluations for the MultiL-ing 2015, single-document summarization (MSS)task.
The quality of the summaries is measuredby ROUGE-1 (Recall, Precision, and F-measure),(C.-Y, 2004).
We also demonstrate the absoluteranks of each submission?P-Rank, R-Rank, andF-Rank?with their scores sorted by Precision, Re-call, and F-measure, respectively.
Only the bestsubmissions (in terms of F-measure) for each par-ticipating system are presented and sorted in de-scending order of their F-measure scores.
Twosystems?Oracles and Lead?were used as top-lineand baseline summarizers, respectively.
Oraclescompute summaries for each article using thecombinatorial covering algorithm in (Davis et al,2012)?sentences were selected from a text to max-imally cover the tokens in the human summary.Since the Oracles system can actually ?see?
thehuman summaries, it is considered as the optimalalgorithm and its scores are the best scores that ex-tractive approaches can achieve.
The Lead systemsimply extracts the leading substring of the bodytext of the articles having the same length as thehuman summary of the article.system P score R score F score P-Rank R-Rank F-RankOracles 0.601 0.619 0.610 1 1 1MUSE 0.488 0.500 0.494 2 3 2CCS 0.477 0.495 0.485 4 6 3POLY 0.475 0.494 0.484 5 8 5EXB 0.467 0.495 0.480 9 13 4NTNU 0.470 0.456 0.462 13 12 17LCS-IESI 0.461 0.456 0.458 15 15 18UA-DLSI 0.457 0.456 0.456 17 18 16Lead 0.425 0.434 0.429 20 24 20Table 2: MSS task.
English.As can be seen, MUSE outperformed all otherparticipating systems except for CCS in Hebrew.CCS (the CCS-5 submission, to be precise) usesthe document tree structure of sections, subsec-tions, paragraphs, and sentences, and compiles asummary from the leading sentences of recursive76system P score R score F score P-Rank R-Rank F-RankCCS 0.202 0.213 0.207 1 1 1MUSE 0.196 0.210 0.203 2 2 2POLY 0.189 0.203 0.196 4 4 6EXB 0.186 0.205 0.195 5 5 4Oracles 0.182 0.204 0.192 6 6 5Lead 0.168 0.178 0.173 12 13 12LCS-IESI 0.181 0.170 0.172 13 7 14Table 3: MSS task.
Hebrew.system P score R score F score P-Rank R-Rank F-RankOracles 0.630 0.658 0.644 1 1 1MUSE 0.562 0.569 0.565 2 4 2CCS 0.554 0.571 0.562 4 3 3EXB 0.546 0.571 0.558 8 2 7POLY 0.545 0.560 0.552 10 9 9LCS-IESI 0.540 0.527 0.531 11 13 12Lead 0.524 0.535 0.529 13 12 13Table 4: MSS task.
Arabic.bottom-up interweaving of the node leading sen-tences, starting from leaves (usually, paragraphsin a section).
POLY got very close scores, thoughit is an unsupervised approach and its comparisonto a supervised summarizer is not fair.MUSEEC also participated in the multi-document summarization (MMS) task, on En-glish, Hebrew and Arabic.
MUSE got first placeon Hebrew, and 2ndplaces on English and Ara-bic languages, out of 9 participants.
POLY gotthird place on Hebrew, 4thplace on English, and5thplace on Arabic, out of 9 participants.
Weexplain the differences between scores in Hebrewand other languages by the lack of NLP tools forthis language.
For example, none of the competingsystems performed stemming for Hebrew.
Also,it is possible that the quality of the gold standardsummaries or the level of agreement between an-notators in Hebrew was lower than in other lan-guages.WECOM was evaluated in (Vanetik et al, 2016)on three different datasets (DUC 2002, DUC 2004,and DUC 2007) using automated and human ex-periments.
Both automated and human scoreshave shown that compression significantly im-proves the quality of generated summaries.
Ta-ble 5 contains results for POLY and WECOMsummarizers on the DUC 2002 dataset.
Statis-tical testing (using a paired T-test) showed thatthere is a significant improvement in ROUGE-1 recall between ILP concept-based extractionmethod of Gillick and Favre (2009) and WECOMwith weights generated by Gillick and Favre?smethod.
Another significant improvement is be-tween ILP extraction method of McDonald (2007)and WECOM with weights generated by McDon-ald?s method.System R-1 R R-1 P R-1 F R-2 R R-2 P R-2 FPOLY + Gillick 0.401 0.407 0.401 0.160 0.162 0.160WECOM + Gillick 0.410* 0.413 0.409 0.166 0.166 0.165POLY + McDonald 0.393 0.407 0.396 0.156 0.159 0.156WECOM + McDonald 0.401* 0.403 0.399 0.158 0.158 0.157POLY + POS F 0.448 0.453 0.447 0.213 0.214 0.212WECOM + POS F 0.450 0.450 0.447 0.211 0.210 0.210Table 5: ROUGE-1 and -2 scores.
DUC 2002.Practical running times for MUSE (summariza-tion) and POLY are tens of milliseconds per a textdocument of a few thousand words.
WECOMrunning time is strictly dependent on the runningtime of dependency parsing performed by Stan-ford CoreNLP package, which takes 2?3 secondsper sentence.
Given pre-saved pre-processing re-sults, WECOM takes tens of milliseconds per doc-ument as well.4 Possible ExtensionsMUSEEC functionality can be easily extended us-ing its API.
New algorithms can be added by im-plementing new ranking and/or compression mod-ules of the pipeline.
The pipeline is dynamicallybuilt before running a summarization algorithm,and it can be configured by a programmer6.
Thecurrently implemented algorithms can also be ex-tended.
For example, a new sentence feature forMUSE can be implemented by preparing one con-crete class implementing a predefined interface.Using Java reflection, it does not require changesin any other code.
New objective functions canbe provided for POLY by implementation of oneconcrete class implementing the predefined inter-face and adding a few rows in the objective func-tions factory for creation instances of a new class(using factory method design pattern).
Using de-pendency injections design pattern, MUSEEC canswitch from Stanford CoreNLP package to anyother tool for text preprocessing.
MUSEEC istotally language-independent and works for anylanguage with input texts provided in UTF-8 en-coding.
If no text processing tools for a givenlanguage are provided, MUSEEC skips the rele-vant stages in its pipeline (for example, it doesnot perform stemming for Chinese).
Providingnew NLP tools can improve MUSEEC summa-rization quality on additional languages.
The sub-sequent stages in the MUSEEC pipeline (sentence6Because building pipeline requires programming skills,this option cannot be applied from GUI.77ranking and compression) are totally language-independent and work with structured data gener-ated during pre-processing.
The optional capabil-ities of NE tagging and AR in the post-processingstage may be also extended with additional NLPtools for specific languages.The programmer and user guidelines for ex-tending and using MUSEEC can be provided uponrequest.5 Final RemarksIn this paper, we present MUSEEC - a plat-form for summarizing documents in multiple lan-guages.
MUSEEC implements several variationsof three single-document summarization methods:MUSE, POLY, and WECOM.
The big advantageof MUSEEC is its multilinguality.
The system hasbeen successfully evaluated on benchmark docu-ment collections in three languages (English, Ara-bic, and Hebrew) and tested on six more lan-guages.
Also, MUSEEC has a flexible architec-ture and API, and it can be extended to other algo-rithms and languages.However, MUSEEC has the following limita-tions: all its methods, especially compressive, aredependent on the pre-processing tools, in terms ofsummarization quality and performance.
In orderto improve coherency of the generated summaries,the MUSEEC user can apply AR as well as NEtagging to the generated summaries.
More sophis-ticated post-processing operations performed onthe extracted text in MUSEEC can further improvethe user experience.The MUSEEC tool, along with itscode, is available under a BSD license onhttps://bitbucket.org/elenach/onr_gui/wiki/Home.
In the future, we intendto prepare a Web application allowing users toapply MUSEEC online.AcknowledgmentsThis work was partially funded by the U.S. De-partment of the Navy, Office of Naval Research.ReferencesLin C.-Y.
2004.
ROUGE: A Package for Auto-matic Evaluation of summaries.
In Proceedings ofthe Workshop on Text Summarization Branches Out(WAS 2004), pages 25?26.S.T.
Davis, J.M.
Conroy, and J.D.
Schlesinger.
2012.OCCAMS ?
An Optimal Combinatorial CoveringAlgorithm for Multi-document Summarization.
InProceedings of the IEEE 12th International Confer-ence on Data Mining Workshops, pages 454?463.A.
Dlikman.
2015.
Linguistic features and ma-chine learning methods in single-documentextractive summarization.
Master?s thesis, Ben-Gurion University of the Negev, Beer-Sheva, Israel.http://www.ise.bgu.ac.il/faculty/mlast/papers/Thesis-ver7.pdf.D.
Gillick and B. Favre.
2009.
A scalable global modelfor summarization.
In Proceedings of the NAACLHLT Workshop on Integer Linear Programming forNatural Language Processing.S.
Khuller, A. Moss, and J. Naor.
1999.
The budgetedmaximum coverage problem.
Information Precess-ing Letters, 70(1):39?45.K.
Knight and D. Marcu.
2002.
Summarization be-yond sentence extraction: A probabilistic approachto sentence compression.
Artificial Intelligence,139:91?107.E.
Krapivin.
2014.
JRouge?Java ROUGE Implementation.https://bitbucket.org/nocgod/jrouge/wiki/Home.M.
Last and M. Litvak.
2012.
Cross-lingual trainingof summarization systems using annotated corporain a foreign language.
Information Retrieval, pages1?28, September.C.-Y.
Lin and E. Hovy.
2003.
Automatic evaluationof summaries using N-gram co-occurrence statistics.In NAACL ?03: Proceedings of the 2003 Conferenceof the North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology, pages 71?78.M.
Litvak and N. Vanetik.
2013.
Mining the gaps:Towards polynomial summarization.
In Proceed-ings of the International Joint Conference on Nat-ural Language Processing, pages 655?660.M.
Litvak, M. Last, and M. Friedman.
2010.
A newapproach to improving multilingual summarizationusing a Genetic Algorithm.
In ACL ?10: Proceed-ings of the 48th Annual Meeting of the Associationfor Computational Linguistics, pages 927?936.C.
D. Manning and H. Sch?utze.
1999.
Foundations ofstatistical natural language processing, volume 999.MIT Press.D.
Marcu.
1997.
From discourse structures to textsummaries.
In Proceedings of the ACL, volume 97,pages 82?88.R.
McDonald.
2007.
A study of global inference al-gorithms in multi-document summarization.
In Ad-vances in Information Retrieval, pages 557?564.N.
Vanetik, M. Litvak, M. Last, and E. Churkin.
2016.An unsupervised constrained optimization approachto compressive summarization.
Manuscript submit-ted for publication.78
