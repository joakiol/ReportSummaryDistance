Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 547?554, Vancouver, October 2005. c?2005 Association for Computational LinguisticsWord Sense Disambiguation Using Sense Examples Automatically Acquiredfrom a Second LanguageXinglong WangSchool of InformaticsUniversity of Edinburgh2 Buccleuch Place, EdinburghEH8 9LW, UKxwang@inf.ed.ac.ukJohn CarrollDepartment of InformaticsUniversity of SussexFalmer, BrightonBN1 9QH, UKjohnca@sussex.ac.ukAbstractWe present a novel almost-unsupervisedapproach to the task of Word Sense Dis-ambiguation (WSD).
We build sense ex-amples automatically, using large quanti-ties of Chinese text, and English-Chineseand Chinese-English bilingual dictionar-ies, taking advantage of the observationthat mappings between words and mean-ings are often different in typologicallydistant languages.
We train a classifier onthe sense examples and test it on a goldstandard English WSD dataset.
The eval-uation gives results that exceed previousstate-of-the-art results for comparable sys-tems.
We also demonstrate that a littlemanual effort can improve the quality ofsense examples, as measured by WSD ac-curacy.
The performance of the classifieron WSD also improves as the number oftraining sense examples increases.1 IntroductionThe results of the recent Senseval-3 competition(Mihalcea et al, 2004) have shown that supervisedWSD methods can yield up to 72.9% accuracy1on words for which manually sense-tagged data areavailable.
However, supervised methods suffer fromthe so-called knowledge acquisition bottleneck: theyneed large quantities of high quality annotated data1This figure refers to the highest accuracy achieved in theSenseval-3 English Lexical Sample task with fine-grained scor-ing.to produce reliable results.
Unfortunately, veryfew sense-tagged corpora are available and manualsense-tagging is extremely costly and labour inten-sive.
One way to tackle this problem is trying toautomate the sense-tagging process.
For example,Agirre et al (2001) proposed a method for buildingtopic signatures automatically, where a topic signa-ture is a set of words, each associated with someweight, that tend to co-occur with a certain concept.Their system queries an Internet search engine withmonosemous synonyms of words that have multiplesenses in WordNet (Miller et al, 1990), and then ex-tracts topic signatures by processing text snippets re-turned by the search engine.
They trained a classifieron the topic signatures and evaluated it on a WSDtask, but the results were disappointing.In recent years, WSD approaches that exploitdifferences between languages have shown greatpromise.
Several trends are taking place simulta-neously under this multilingual paradigm.
A clas-sic one is to acquire sense examples using bilin-gual parallel texts (Gale et al, 1992; Resnik andYarowsky, 1997; Diab and Resnik, 2002; Ng et al,2003): given a word-aligned parallel corpus, the dif-ferent translations in a target language serve as the?sense tags?
of an ambiguous word in the sourcelanguage.
For example, Ng et al (2003) acquiredsense examples using English-Chinese parallel cor-pora, which were manually or automatically alignedat sentence level and then word-aligned using soft-ware.
A manual selection of target translations wasthen performed, grouping together senses that sharethe same translation in Chinese.
Finally, the occur-rences of the word on the English side of the parallel547texts were considered to have been disambiguatedand ?sense tagged?
by the appropriate Chinese trans-lations.
A classifier was trained on the extractedsense examples and then evaluated on the nouns inSenseval-2 English Lexical Sample dataset.
The re-sults appear good numerically, but since the sensegroups are not in the gold standard, comparison withother Senseval-2 results is difficult.
As discussed byNg et al, there are several problems with relying onbilingual parallel corpora for data collection.
First,parallel corpora, especially accurately aligned par-allel corpora are rare, although attempts have beenmade to mine them from the Web (Resnik, 1999).Second, it is often not possible to distinguish allsenses of a word in the source language, by merelyrelying on parallel corpora, especially when the cor-pora are relatively small.
This is a common problemfor bilingual approaches: useful data for some wordscannot be collected because different senses of poly-semous words in one language often translate to thesame word in the other.
Using parallel corpora canaggravate this problem, because even if a word sensein the source language has a unique translation in thetarget language, the translation may not occur in theparallel corpora at all, due to the limited size of thisresource.To alleviate these problems, researchers seekother bilingual resources such as bilingual dictio-naries, together with monolingual resources that canbe obtained easily.
Dagan and Itai (1994) proposedan approach to WSD using monolingual corpora, abilingual lexicon and a parser for the source lan-guage.
One of the problems of this method is that formany languages, accurate parsers do not exist.
Witha small amount of classified data and a large amountof unclassified data in both the source and the tar-get languages, Li and Li (2004) proposed bilingualbootstrapping.
This repeatedly constructs classifiersin the two languages in parallel and boosts the per-formance of the classifiers by classifying data ineach of the languages and by exchanging informa-tion regarding the classified data between two lan-guages.
With a certain amount of manual work, theyreported promising results, but evaluated on rela-tively small datasets.In previous work, we proposed to use Chinesemonolingual corpora and Chinese-English bilin-gual dictionaries to acquire sense examples (Wang,2004)2.
We evaluated the sense examples using avector space WSD model on a small dataset con-taining words with binary senses, with promisingresults.
This approach does not rely on scarce re-sources such as aligned parallel corpora or accurateparsers.This paper describes further progress based on ourproposal: we automatically build larger-scale senseexamples and then train a Na?
?ve Bayes classifier onthem.
We have evaluated our system on the EnglishLexical Sample Dataset from Senseval-2 and the re-sults show conclusively that such sense examplescan be used successfully in a full-scale fine-grainedWSD task.
We tried to analyse whether more senseexamples acquired this way would improve WSDaccuracy and also whether a little human effort onsense mapping could further improve WSD perfor-mance.The reminder of the paper is organised as fol-lows.
Section 2 outlines the acquisition algorithmfor sense examples.
Section 3 describes details ofbuilding this resource and demonstrates our appli-cation of sense examples to WSD.
We also presentresults and analysis in this section.
Finally, we con-clude in Section 4 and talk about future work.2 Acquisition of Sense ExamplesFollowing our previous proposal (Wang, 2004), weautomatically acquire English sense examples usinglarge quantities of Chinese text and English-Chineseand Chinese-English dictionaries.
The Chinese lan-guage was chosen because it is a distant languagefrom English and the more distant two languagesare, the more likely that senses are lexicalised differ-ently (Resnik and Yarowsky, 1999).
The underlyingassumption of this approach is that in general eachsense of an ambiguous English word corresponds toa distinct translation in Chinese.
As shown in Fig-ure 1, firstly, the system translates senses of an En-glish word into Chinese words, using an English-Chinese dictionary, and then retrieves text snippetsfrom a large amount of Chinese text, with the Chi-nese translations as queries.
Then, the Chinese textsnippets are segmented and then translated back toEnglish word by word, using a Chinese-English dic-2Sense examples were referred to as ?topic signatures?
inthat paper.548English ambiguous wordwSense 1 ofwSense 2 ofwChinese translation ofsense 2Chinese translation ofsense 1English-ChineseLexiconChinese text snippet 1Chinese text snippet 2... ...ChineseSearchEngineChinese-EnglishLexiconChinese text snippet 1Chinese text snippet 2... ...{English sense example 1for sense 1 of w}{English sense example 2for sense 1 of w}... ...{English sense example 1for sense 2 of w}{English sense example 2for sense 2 of w}... ...ChineseSegementationFigure 1.
Process of automatic acquisition of sense examples.For simplicity, assume w has two senses.tionary.
In this way, for each sense, a set of senseexamples is produced.
As an example, suppose onewants to retrieve sense examples for the financialsense of interest.
One first looks up the Chinesetranslations of this sense in an English-Chinese dic-tionary, and finds that |E is the right Chinesetranslation corresponding to this particular sense.Then, the next stage is to automatically build a col-lection of Chinese text snippets by either searchingin a large Chinese corpus or on the Web, using |E as query.
Since Chinese is a language writtenwithout spaces between words, one needs to use asegmentor to mark word boundaries before translat-ing the snippets word by word back to English.
Theresult is a collection of sense examples for the finan-cial sense of interest, each containing a bag of wordsthat tend to co-occur with that particular sense.
Forexample, {interest rate, bank, annual, economy, ...}might be one of the sense examples extracted for thefinancial sense of interest.
Note that words in a senseexample are unordered.Since this method acquires training data for WSDsystems from raw monolingual Chinese text, itavoids the problem of the shortage of English sense-tagged corpora, and also of the shortage of alignedbilingual corpora.
Also, if existing corpora arenot big enough, one can always harvest more textfrom the Web.
However, like all methods basedon the cross-language translation assumption men-tioned above, there are potential problems.
For ex-ample, it is possible that a Chinese translation of anEnglish sense is also ambiguous, and thus the con-tents of text snippets retrieved may be regarding aconcept other than the one we want.
In general,when the assumption does not hold, one could usethe glosses defined in a dictionary as queries to re-trieve text snippets, as comprehensive bilingual dic-tionaries tend to include translations to all senses ofa word, where multiword translations are used whenone-to-one translation is not possible.
Alternatively,a human annotator could map the senses and trans-lations by hand.
As we will describe later in thispaper, we chose the latter way in our experiments.3 Experiments and ResultsWe firstly describe in detail how we prepared thesense examples and then describe a large scale WSDevaluation on the English Senseval-2 Lexical Sam-ple dataset (Kilgarriff, 2001).
The results show thatour system trained with the sense examples achievedsignificantly better accuracy than comparable sys-tems.
We also show that when a little manual effortwas invested in mapping the English word sensesto Chinese monosemous translations, WSD perfor-mance improves accordingly.
Based on further ex-periments on a standard binary WSD dataset, wealso show that the technique scales up satisfacto-rily so that more sense examples help achieve betterWSD accuracy.3.1 Building Sense ExamplesFollowing the approach described in Section 2,we built sense examples for the 44 words in theSenseval-2 dataset3.
These 44 words have 223senses in total to disambiguate.
The first step wastranslating English senses to Chinese.
We used theYahoo!
Student English-Chinese On-line Dictio-nary4, as well as a more comprehensive electronicdictionary.
This is because the Yahoo!
dictionary isdesigned for English learners, and its sense granu-larity is rather coarse-grained.
It is good enough forwords with fewer or coarse-grained senses.
How-3These 44 words cover all nouns and adjectives in theSenseval-2 dataset, but exclude verbs.
We discuss this pointin section 3.2.4See: http://cn.yahoo.com/dictionary.549ever, the Senseval-2 Lexical Sample task5 usesWordNet 1.7 as gold standard, which has very finesense distinctions and translation granularity in theYahoo!
dictionary does not conform to this standard.PowerWord 20026 was chosen as a supplementarydictionary because it integrates several comprehen-sive English-Chinese dictionaries in a single appli-cation.
For each sense of an English word entry, bothYahoo!
and PowerWord 2002 dictionaries list notonly Chinese translations but also English glosses,which provides a bridge between WordNet synsetsand Chinese translations in the dictionaries.
In de-tail, to automatically find a Chinese translation forsense s of an English word w, our system looks upw in both dictionaries and determines whether w hasthe same or greater number of senses as in Word-Net.
If it does, in one of the bilingual dictionaries,we locate the English gloss g which has the max-imum number of overlapping words with the glossfor s in the WordNet synset.
The Chinese transla-tion associated with g is then selected.
Althoughthis simple method successfully identified Chinesetranslations for 23 out of the 44 words (52%), trans-lations for the remaining word senses remain un-known because the sense distinctions are differentbetween our bilingual dictionaries and WordNet.
Infact, unless an English-Chinese bilingual WordNetbecomes available, this problem is inevitable.
Forour experiments, we solved the problem by manu-ally looking up dictionaries and identifying transla-tions.
For each one of the 44 words, PowerWord2002 provides more Chinese translations than thenumber of its synsets in WordNet 1.7.
Thus the an-notator simply selects the Chinese translations thathe considers a best match to the corresponding En-glish senses.
This task took an hour for an annotatorwho speaks both languages fluently.It is possible that the Chinese translations are alsoambiguous, which can make the topic of a collectionof text snippets deviate from what is expected.
Forexample, the oral sense of mouth can be translated as?
or?n in Chinese.
However, the first translation5The task has two variations: one to disambiguate fine-grained senses and the other to coarse-grained ones.
We evalu-ated our sense examples on the former variation, which is obvi-ously more difficult.6A commercial electronic dictionary application.
We usedthe free on-line version at: http://cb.kingsoft.com.(?)
is a single-character word and is highly ambigu-ous: by combining with other characters, its mean-ing varies.
For example,?
?means ?an exit?
or ?toexport?.
On the other hand, the second translation(?n) is monosemous and should be used.
To as-sess the influence of such ?ambiguous translations?,we carried out experiments involving more humanlabour to verify the translations.
The same annotatormanually eliminated those highly ambiguous Chi-nese translations and then replaced them with lessambiguous or ideally monosemous Chinese trans-lations.
This process changed roughly half of thetranslations and took about five hours.
We comparedthe basic system with this manually improved one.The results are presented in section 3.2.Using translations as queries, the sense exampleswere automatically extracted from the Chinese Gi-gaword Corpus (CGC), distributed by the LDC7,which contains 2.7GB newswire text, of which900MB are sourced from Xinhua News Agency ofBeijing, and 1.8GB are drawn from Central Newsfrom Taiwan.
A small percentage of words havedifferent meanings in these two Chinese dialects,and since the Chinese-English dictionary (LDCMandarin-English Translation Lexicon Version 3.0)we use later is compiled with Mandarin usages inmind, we mainly retrieve data from Xinhua News.We set a threshold of 100, and only when the amountof snippets retrieved from Xinhua News is smallerthan 100, do we turn to Central News to collect moredata.
Specifically, for 48 out of the 223 (22%) Chi-nese queries, the system retrieved less than 100 in-stances from Xinhua News so it extracted more datafrom Central News.
In theory, if the training data isstill not enough, one could always turn to other textresources, such as the Web.To decide the optimal length of text snippets toretrieve, we carried out pilot experiments with twolength settings: 250 (?
110 English words) and400 (?
175 English words) Chinese characters, andfound that more context words helped improve WSDperformance (results not shown).
Therefore, we re-trieve text snippets with a length of 400 characters.We then segmented all text snippets, using an ap-plication ICTCLAS8.
After the segmentor marked7Available at: http://www.ldc.upenn.edu/Catalog/8See: http://mtgroup.ict.ac.cn/?zhp/ICTCLAS550all word boundaries, the system automatically trans-lated the text snippets word by word using the elec-tronic LDC Mandarin-English Translation Lexicon3.0.
As expected, the lexicon does not cover allChinese words.
We simply discarded those Chi-nese words that do not have an entry in this lexi-con.
We also discarded those Chinese words withmultiword English translations.
Since the discardedwords can be informative, one direction of our re-search in the future is to find an up-to-date wide cov-erage dictionary, and to see how much difference itwill make.
Finally, we filtered the sense exampleswith a stop-word list, to ensure only content wordswere included.We ended up with 223 sets of sense examplesfor all senses of the 44 nouns and adjectives in thetest dataset.
Each sense example contains a set ofwords that were translated from a Chinese text snip-pet, whose content should closely relate to the En-glish word sense in question.
Words in a sense ex-ample are unordered, because in this work we onlyused bag-of-words information.
Except for the verysmall amount of manual work described above tomap WordNet glosses to those in English-Chinesedictionaries, the whole process is automatic.3.2 WSD Experiments on Senseval-2 LexicalSample datasetThe Senseval-2 English Lexical Sample Datasetconsists of manually sense-tagged training and testinstances for nouns, adjectives and verbs.
We onlytested our system on nouns and adjectives becauseverbs often have finer sense distinctions, whichwould mean more manual work would need to bedone when mapping WordNet synsets to English-Chinese dictionary glosses.
This would involve us ina rather different kind of enterprise since we wouldhave moved from an almost-unsupervised to a moresupervised setup.We did not use the training data supplied with thedataset.
Instead, we train a classifier on our auto-matically built sense examples and test it on the testdata provided.
In theory, any machine learning clas-sifier can be applied.
We chose the Na?
?ve Bayes al-gorithm with kernel estimation9 (John and Langley,1995) which outperformed a few other classifiers in9We used the implementation in the Weka machine learningpackage, available at: http://www.cs.waikato.ac.nz/?ml/weka.art-n(5)authority-n(7)bar-n(13)blind-a(3)bum-n(4)chair-n(4)channel-n(7)child-n(4)church-n(3)circuit-n(6)colourless-a(2)cool-a(6)day-n(9)detention-n(2)dyke-n(2)facility-n(5)faithful-a(3)fatigue-n(4)feeling-n(6)fine-a(9)fit-a(3)free-a(8)graceful-a(2)green-a(7)grip-n(7)hearth-n(3)holiday-n(2)lady-n(3)local-a(3)material-n(5)mouth-n(8)nation-n(3)natural-a(10)nature-n(5)oblique-a(2)post-n(8)restraint-n(6)sense-n(5)simple-a(7)solemn-a(2)spade-n(3)stress-n(5)vital-a(4)yew-n(2)Basic39.821.544.774.564.480.032.456.359.448.866.750.932.460.682.827.166.777.350.034.344.837.370.053.235.348.564.571.738.547.838.339.514.627.772.434.717.425.949.373.167.645.041.082.8Sys BMW59.623.752.075.062.282.936.556.359.469.869.450.933.184.886.228.866.777.350.032.944.848.273.358.537.351.575.077.843.649.341.739.534.031.973.345.619.646.350.776.970.645.046.289.7Lesk(U)16.330.42.032.753.356.521.956.245.35.954.39.6043.857.146.626.144.22.05.73.47.372.410.617.681.229.050.931.644.931.718.96.841.372.46.328.924.512.124.060.62.6017.9Word46.152.024.6Avg.Basic29.920.741.174.560.081.231.556.353.148.245.726.932.262.585.720.769.676.711.78.644.829.358.653.235.346.964.569.236.839.138.335.114.623.972.434.76.720.745.564.066.737.542.121.4Sys AMW51.022.848.374.560.082.635.656.356.368.245.726.932.984.485.722.469.674.411.711.444.837.858.658.537.350.074.273.642.144.941.735.132.026.172.445.66.743.445.576.063.637.542.125.040.746.0RB16.310.03.340.015.623.212.318.829.710.642.913.57.643.828.613.821.725.69.87.131.015.962.121.319.631.238.728.326.310.111.721.66.815.244.810.111.124.513.632.018.212.821.157.1Baselines & A Senseval-2 EntryMFB41.839.138.478.268.976.813.754.756.227.165.746.260.062.553.648.378.376.756.942.958.635.479.375.535.371.977.464.255.320.336.778.427.245.769.031.628.924.551.596.063.648.792.178.618.150.5UNED50.034.827.874.511.181.217.843.862.555.331.446.220.078.135.725.978.386.060.844.348.335.479.378.721.665.654.858.534.253.648.370.344.723.927.641.817.830.251.596.054.520.594.771.446.4Table 1.
WSD accuracy on words in the English Senseval-2Lexical Sample dataset.
The left most column shows words,their POS tags and how many senses they have.
?Sys A?
and?Sys B?
are our systems, and ?MW?
denotes a multi-word de-tection module was used in conjunction with the ?Basic?
sys-tem.
For comparison, it also shows two baselines: ?RB?
is therandom baseline and ?MFB?
is the most-frequent-sense base-line.
?UNED?
is one of the best unsupervised participantsin the Senseval-2 competition and ?Lesk(U)?
is the highestunsupervised-baseline set in the workshop.
All accuracies areexpressed as percentages.our pilot experiments on other datasets (results notshown).
The average length of a sense example is35 words, which is much shorter than the length ofthe text snippets, which was set to 400 Chinese char-acters (?
175 English words).
This is because func-tion words and words that are not listed in the LDCMandarin-English lexicon were eliminated.
We didnot apply any weighting to the features because per-formance went down in our pilot experiments whenwe applied a TF.IDF weighting scheme (results notshown).
We also limited the maximum number of551training sense examples to 6000, for efficiency pur-poses.
We attempted to tag every test data instance,so our coverage (on nouns and adjectives) is 100%.To assess the influence of ambiguous Chinesetranslations, we prepared two sets of training data.As described in section 3.1: sense examples in thefirst set were prepared without taking ambiguity inChinese text into consideration, while those in thesecond set were prepared with a little more humaneffort involved trying to reduce ambiguity by us-ing less ambiguous translations.
We call the systemtrained on the first set ?Sys A?
and the one trainedon the second ?Sys B?.In this lexical sample task, multiwords are ex-pected to be picked out by participating WSD sys-tems.
For example, the answer art collection shouldbe supplied when this multiword occurs in a testinstance.
It would be judged wrong if one taggedthe art in art collection as the artworks sense, eventhough one could argue that this was also a cor-rect answer.
To deal with multiwords, we imple-mented a very simple detection module, which triesto match multiword entries in WordNet to the am-biguous word and its left and right neighbours.
Forexample, if the module finds art collection is an en-try in WordNet, it tags all occurrences of this multi-word in the test data, regardless of the prediction bythe classifier.The results are shown in Table 1.
Our ?Sys B?system, with and without the multiword detectionmodule, outperformed ?Sys A?, which shows thatsense examples acquired with less ambiguous Chi-nese translations contain less noise and thereforeboost WSD performance.
For comparison, the ta-ble also shows various baseline performance figuresand a system that participated in Senseval-210.
Con-sidering that the manual work involved in our ap-proach is negligible compared with manual sense-tagging, we classify our systems as unsupervisedand we should aim to beat the random baseline.This all four of our systems do easily.
We also eas-ily beat another unsupervised baseline ?
the Lesk(1986) baseline, which disambiguates words usingWordNet definitions.
The MFB baseline is actu-ally a ?supervised?
baseline, since an unsupervised10Accuracies for each word and averages were calculatedby us, based on the information on Senseval-2 Website.
See:http://www.sle.sharp.co.uk/senseval2/.system does not have such prior knowledge before-hand.
McCarthy et al (2004) argue that this is avery tough baseline for an unsupervised WSD sys-tem to beat.
Our ?Sys B?
with multiword detectionexceeds it.
?Sys B?
also exceeds the performanceof UNED (Ferna?ndez-Amoro?s et al, 2001), whichwas the second-best ranked11 unsupervised systemsin the Senseval-2 competition.There are a number of factors that can influenceWSD performance.
The distribution of training datafor senses is one.
In our experiments, we used allsense examples that we built for a sense (with anupper bound of 6000).
However, the distribution ofsenses in English text often does not match the dis-tribution of their corresponding Chinese translationsin Chinese text.
For example, suppose an Englishword w has two senses: s1 and s2, where s1 rarelyoccurs in English text, whereas sense s2 is used fre-quently.
Also suppose s1?s Chinese translation ismuch more frequently used than s2?s translation inChinese text.
Thus, the distribution of the two sensesin English is different from that of the translations inChinese.
As a result, the numbers of sense exam-ples we would acquire for the two senses would bedistributed as if they were in Chinese text.
A clas-sifier trained on this data would then tend to predictunseen test instances in favour of the wrong distribu-tion.
The word nation, for example, has three senses,of which the country sense is used more frequentlyin English.
However, in Chinese, the country senseand the people sense are almost equally distributed,which might be the reason for its WSD accuracy be-ing lower with our systems than most of the otherwords.
A possible way to alleviate this problem is toselect training sense examples according to an esti-mated distribution in natural English text, which canbe done by analysing available sense-tagged corporawith help of smoothing techniques, or with the un-supervised approach of (McCarthy et al, 2004).Cultural differences can cause difficulty in retriev-ing sufficient training data.
For example, transla-tions of senses of church and hearth appear only in-frequently in Chinese text.
Thus, it is hard to buildsense examples for these words.
Another problem,11One system performed better but their answers were noton the official Senseval-2 website so that we could not do thecomparison.
Also, that system did not attempt to disambiguateas many words as UNED and us.552as mentioned above, is that translations of Englishsenses can be ambiguous in Chinese.
For exam-ple, Chinese translations of the words vital, natu-ral, local etc.
are also ambiguous to some extent,and this might be a reason for their low perfor-mance.
One way to solve this, as we described, isto manually check the translations.
Another auto-matic way is that, before retrieving text snippets, wecould segment or even parse the Chinese corpora,which should reduce the level of ambiguity and leadto better sense examples.3.3 Further WSD ExperimentsOne of the strengths of our approach is that trainingdata come cheaply and relatively easily.
However,the sense examples are acquired automatically andthey inevitably contain a certain amount of noise,which may cause problems for the classifier.
To as-sess the relationship between accuracy and the sizeof training data, we carried out a series of experi-ments, feeding the classifier with different numbersof sense examples as training data.For these experiments, we used another standardWSD dataset, the TWA dataset.
This is a manu-ally sense-tagged corpus (Mihalcea, 2003), whichcontains 2-way sense-tagged text instances, drawnfrom the British National Corpus, for 6 nouns.
Wefirst built sense examples for all the 12 senses usingthe approach described above, then trained the sameNa?
?ve Bayes algorithm (NB) on different numbersof sense examples.In detail, for all of the 6 words, we did the fol-lowing: given a word wi, we randomly selected nsense examples for each of its senses si, from thetotal amount of sense examples built for si.
Thenthe NB algorithm was trained on the 2 ?
n exam-ples and tested on wi?s test instances in TWA.
Werecorded the accuracy and repeated this process 200times and calculated the mean and variance of the200 accuracies.
Then we assigned another value ton and iterated the above process until n took all thepredefined values.
In our experiments, n was takenfrom {50, 100, 150, 200, 400, 600, 800, 1000, 1200}for words motion, plant and tank and from {50, 100,150, 200, 250, 300, 350} for bass, crane and palm,because there were less sense example data availablefor the latter three words.
Finally, we used the t-test(p = 0.05) on pairwise sets of means and variancesto see if improvements were statistically significant.0.930.910.890.870.850  50  100  150  200  250  300  350  400bass0.780.760.740.720.70.680  50  100  150  200  250  300  350  400crane0.820.790.760.730.70.670.640  200  400  600  800  1000  1200  1400motion0.770.760.750.740.730.720  50  100  150  200  250  300  350  400palm0.760.740.720.70.680.660.640  200  400  600  800  1000  1200  1400plant0.740.720.70.680.660.640  200  400  600  800  1000  1200  1400tankFigure 2.
Accuracy scores with increasing number of trainingsense examples.
Each bar is a standard deviation.The results are shown in Figure 212.
34 out of 42t-scores are greater than the t-test critical values, sowe are fairly confident that the more training senseexamples used, the more accurate the NB classifierbecomes on this disambiguation task.4 Conclusions and Future WorkWe have presented WSD systems that use sense ex-amples as training data.
Sense examples are ac-quired automatically from large quantities of Chi-nese text, with the help of Chinese-English andEnglish-Chinese dictionaries.
We have tested ourWSD systems on the English Senseval-2 LexicalSample dataset, and our best system outperformedcomparable state-of-the-art unsupervised systems.Also, we found that increasing the number of thesense examples significantly improved WSD perfor-mance.
Since sense examples can be obtained verycheaply from any large Chinese text collection, in-12These experiments showed that our systems outperformedthe most-frequent-sense baseline and Mihalcea?s unsupervisedsystem (2003).553cluding the Web, our approach is a way to tackle theknowledge acquisition bottleneck.There are a number of future directions that wecould investigate.
Firstly, instead of using a bilin-gual dictionary to translate Chinese text snippetsback to English, we could use machine translationsoftware.
Secondly, we could try this approach onother language pairs, Japanese-English, for exam-ple.
This is also a possible solution to the problemthat ambiguity may be preserved between Chineseand English.
In other words, when a Chinese transla-tion of an English sense is still ambiguous, we couldtry to collect sense examples using translation in athird language, Japanese, for instance.
Thirdly, itwould be interesting to try to tackle the problem ofChinese WSD using sense examples built using En-glish, the reverse process to the one described in thispaper.AcknowledgementsThis research was funded by EU IST-2001-34460project MEANING: Developing Multilingual Web-Scale Language Technologies.ReferencesEneko Agirre, Olatz Ansa, David Martinez, and EduardHovy.
2001.
Enriching WordNet concepts with topicsignatures.
In Proceedings of the NAACL workshop onWordNet and Other Lexical Resources: Applications,Extensions and Customizations.
Pittsburgh, USA.Ido Dagan and Alon Itai.
1994.
Word sense disam-biguation using a second language monolingual cor-pus.
Computational Linguistics, 20(4):563?596.Mona Diab and Philip Resnik.
2002.
An unsupervisedmethod for word sense tagging using parallel corpora.In Proceedings of the 40th Anniversary Meeting of theAssociation for Computational Linguistics (ACL-02).Philadelphia, USA.David Ferna?ndez-Amoro?s, Julio Gonzalo, and FelisaVerdejo.
2001.
The UNED systems at Senseval-2.
In Poceedings of Second International Wordshopon Evaluating Word Sense Disambiguation Systems(Senseval-2).
Toulouse, France.William A. Gale, Kenneth W. Church, and DavidYarowsky.
1992.
Using bilingual materials to de-velop word sense disambiguation methods.
In Pro-ceedings of the International Conference on Theoret-ical and Methodological Issues in Machine Transla-tion, pages 101?112.George H. John and Pat Langley.
1995.
Estimating con-tinuous distributions in Bayesian classifiers.
In Pro-ceedings of the Eleventh Conference on Uncertainty inArtificial Intelligence, pages 338?345.Adam Kilgarriff.
2001.
English lexical sample task de-scription.
In Proceedings of the Second InternationalWorkshop on Evaluating Word Sense DisambiguationSystems (SENSEVAL-2).
Toulouse, France.Michael E. Lesk.
1986.
Automated sense disambigua-tion using machine-readable dictionaries: how to tell apinecone from an ice cream cone.
In Proceedings ofthe SIGDOC Conference.Hang Li and Cong Li.
2004.
Word translation dis-ambiguation using bilingual bootstrapping.
Compu-tational Linguistics, 20(4):563?596.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Finding predominant word senses inuntagged text.
In Proceedings of the 42nd AnnualMeeting of the Association for Computational Linguis-tics.
Barcelona, Spain.Rada Mihalcea, Timothy Chklovski, and Adam Killgar-iff.
2004.
The Senseval-3 English lexical sample task.In Proceedings of the Third International Workshop onthe Evaluation of Systems for the Semantic Analysis ofText (Senseval-3).Rada Mihalcea.
2003.
The role of non-ambiguous wordsin natural language disambiguation.
In Proceedings ofthe Conference on Recent Advances in Natural Lan-guage Processing, RANLP 2003.
Borovetz, Bulgaria.George A. Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine J. Miller.
1990.Introduction to WordNet: An on-line lexical database.Journal of Lexicography, 3(4):235?244.Hwee Tou Ng, Bin Wang, and Yee Seng Chan.
2003.
Ex-ploiting parallel texts for word sense disambiguation:an empirical study.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Linguis-tics.Philip Resnik and David Yarowsky.
1997.
A perspectiveon word sense disambiguation methods and their eval-uation.
In Proceedings of the ACL SIGLEX Workshopon Tagging Text with Lexical Semantics: Why, Whatand How?, pages 79?86.Philip Resnik and David Yarowsky.
1999.
Distinguish-ing systems and distinguishing senses: New evaluationmethods for word sense disambiguation.
Natural Lan-guage Engineering, 5(2):113?133.Philip Resnik.
1999.
Mining the Web for bilingual text.In Proceedings of the 37th Annual Meeting of the As-sociation for Computational Linguistics.Xinglong Wang.
2004.
Automatic acquisition of En-glish topic signatures based on a second language.
InProceedings of the Student Research Workshop at ACL2004.
Barcelona, Spain.554
