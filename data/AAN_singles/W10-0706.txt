Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 41?44,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsShared Task: Crowdsourced AccessibilityElicitation of Wikipedia ArticlesScott Novotney and Chris Callison-BurchCenter for Language and Speech ProcessingJohns Hopkins University3400 North Charles StreetBaltimore, MD, USAsnovotne@bbn.com ccb@jhu.eduAbstractMechanical Turk is useful for generatingcomplex speech resources like conversationalspeech transcription.
In this work, we ex-plore the next step of eliciting narrations ofWikipedia articles to improve accessibility forlow-literacy users.
This task proves a use-ful test-bed to implement qualitative vettingof workers based on difficult to define metricslike narrative quality.
Working with the Me-chanical Turk API, we collected sample nar-rations, had other Turkers rate these samplesand then granted access to full narration HITsdepending on aggregate quality.
While narrat-ing full articles proved too onerous a task tobe viable, using other Turkers to perform vet-ting was very successful.
Elicitation is possi-ble on Mechanical Turk, but it should conformto suggested best practices of simple tasks thatcan be completed in a streamlined workflow.1 IntroductionThe rise of Mechanical Turk publications in the NLPcommunity leaves no doubt that non-experts canprovide useful annotations for low cost.
Emergingbest practices suggest designing short, simple tasksthat require little amount of upfront effort to most ef-fectively use Mechanical Turk?s labor pool.
Suitabletasks are best limited to those easily accomplishedin ?short bites?
requiring little context switching.
Forinstance, most annotation tasks in prior work (Snowet al, 2008) required selection from an enumeratedlist, allowing for easy automated quality control anddata collection.More recent work to collect speech transcrip-tion (Novotney and Callison-Burch, 2010) or paral-lel text translations (Callison-Burch, 2009) demon-strated that Turkers can provide useful free-form an-notation.In this paper, we extend open ended collec-tion even further by eliciting narrations of EnglishWikipedia articles.
To vet prospective narrators,we use qualitative qualifications by aggregating theopinions of other Turkers on narrative style, thusavoiding quantification of qualitative tasks.The Spoken Wikipedia Project1 aims to increasethe accessibility of Wikipedia by recording articlesfor use by blind or illiterate users.
Since 2008, over1600 English articles covering topics from art totechnology have been narrated by volunteers.
Thecharitable nature of this work should provide addi-tional incentive for Turkers to complete this task.We use Wikipedia narrations as an initial proof-of-concept for other more challenging elicitation taskssuch as spontaneous or conversational speech.While previous work used other Turkers insecond-pass filtering for quality control, we flip thisprocess and instead require that narrators be judgedfavorably before working on full narration tasks.
Re-lying on human opinion sidesteps the difficult taskof automatically judging narrative quality.
This re-quires a multi-pass workflow to manage potentialnarrators and grant them access to the full narrationHITs through Mechanical Turk?s Qualifications.In this paper, we make the following points:?
Vetting based on qualitative criteria like nar-ration quality can be effectively implementedthrough Turker-provided ratings.1http://en.wikipedia.org/wiki/Wikipedia:WikiProject_Spoken_Wikipedia41?
Narrating full articles is too complex and time-consuming for timely task throughput - bestpractices are worth following.?
HITs should be streamlined as much as possi-ble.
Requiring Turkers to perform work outsideof the web interface seemingly hurt task com-pletion rate.2 Prior WorkThe research community has demonstrated thatcomplex annotations (like speech transcription andelicitation) can be provided through MechanicalTurk.Callison-Burch (2009) showed that Turkers couldaccomplish complex tasks like translating Urdu orcreating reading comprehension tests.McGraw et al (2009) used Mechanical Turk toimprove an English isolated word speech recognizerby having Turkers listen to a word and select froma list of probable words at a cost of $20 per hour oftranscription.Marge et al (2010) collected transcriptions ofclean speech and demonstrated that duplicate tran-scription of non-experts can match expert transcrip-tion.Novotney and Callison-Burch (2010) collectedtranscriptions of conversational speech for as littleas $5 / hour of transcription and demonstrated thatresources are better spent annotating more data thanimproving data quality.McGraw et al (2010) elicited short snippets ofEnglish street addresses through a web interface.103 hours were elicted in just over three days.3 Narration TaskUsing a python library for parsing Wikipedia2, weextracted all text under the <p> tag as a heuristicfor readable content.
We ignored all other contentlike lists, info boxes or headings.
Since we wantedto preserve narrative flow, each article was postedas one HIT, paying $0.05 per paragraph.
Articlesaveraged 40 paragraphs, so each HIT averaged $2 inpayment - some as little as $0.25.We provided instructions for using recordingsoftware and asked Turkers to record one para-graph at a time.
Using Mechanical Turk?s API,2http://github.com/j2labs/wikipydiawe generated an XML template for each para-graph and let the Turker upload a file through theFileUploadAnswer form.
The API supportsconstraints on file extensions, so we were able to re-quire that all files be in mp3 format before the Turkercould submit the work.Mechanical Turk?s API supports file requeststhrough the GetFileUploadURL call.
A URL isdynamically generated on Amazon?s servers whichstays active for one minute.
We then fetched eachaudio file and stored them locally on our own serversfor later processing.Since these narrations are meant for public con-sumption and are difficult to quality control, we re-quired prospective Turkers first qualify.4 Granting Qualitative QualificationsQualifications are prerequisites that limit whichTurkers can work on a HIT.
A common qualifica-tion provided by Mechanical Turk is a minimum ap-proval rating for a Turker, indicating what percent-age of submitted work was approved.
We created aqualification for our narration tasks since we wantedto ensure only those turkers with a good speakingvoice would complete our tasks.However, the definition of a ?good speakingvoice?
is not easy to quantify.
Luckily, this task iswell suited to Mechanical Turk?s concept of artifi-cial artificial intelligence.
Humans can easily decidea narrator?s quality while automatic methods wouldbe impractical.
Additionally, we never define whata ?good?
narration voice is, relying instead on publicopinion.4.1 WorkflowWe implemented the qualification ratings using theAPI with three different steps.
Turkers who wishto complete the full narration HITs are first directedto a ?qualification?
HIT with one sample paragraphpaying $0.05.
We then use other Turkers to rate thequality of the narrator, asking them to judge basedon speaking style, audio clarity and pronunciation.Post Qualification The narration qualification andfull narration HITs are posted.Sample HIT A prospective narrator uploads arecording of a sample paragraph earning $0.05.42The audio is downloaded and hosted on ourweb host.Rating HIT A HIT is created to be completed tentimes.
Turkers make a binary decision as towhether they would listen to a full article bythe narrator and optionally suggest feedback.Grant Qualification The ten ratings are collectedand if five or more are positive we grant thequalification.
The narrator is then automati-cally contacted with the decision and providedwith any feedback from the rating Turkers.Although not straightforward, the API made itpossible to dynamically create HITs, approve as-signments, sync audio files and ratings,notify work-ers and grant qualifications.
It does not, however,manage state across HITs, requiring us to implementour own control logic for associating workers withnarration and rating HITs.
Once implemented, man-aging the process was as simple as invoking threeperl scripts a few times a day.
These could easilybe rolled into one background process automaticallycontrolling the entire workflow.4.2 Effectiveness of Turker RatingsThirteen Turkers submitted sample audio files overthe course of a week.
Collecting the ten ratings tooka few hours per Turker.
The average rating for thenarrators was 7.5, with three of the thirteen beingrejected for having a score less than 5.
The authorsagreed with the sentiment of the raters and feel thatthe qualification process correctly filtered out thepoor narrators.Below is a sample of the comments for an ap-proved narrator and a rejected narrator.This Turker was approved with 9/10 votes.?
The narration was very easy to understand.
Thespeaker?s tone was even, well-paced, and clear.Great narration.?
Very good voice, good pace and modulation.?
Very nice voice and pleasant to listen to.
I wouldhave guessed that this was a professional voice ac-tor.This Turker was rejected with 3/10 votes.?
Monotone voice, uninterested and barely literate.
Iwould never listen to this voice for any length oftime.?
muddy audio quality; narrator has a tired and a verylow tone quality.?
Very solemn voice - didn?t like listening to it.5 Data AnalysisOf the thirteen qualified Turkers, only two went onto complete full narrations.
This happened only af-ter we shortened the articles to the initial five para-graphs and raised payment to $0.25 per paragraph.While the audio was clear, both authors exhibitedmispronunciations of domain-specific terms.
For in-stance, one author narrating Isaac Newton mispro-nounced Principia with a soft c (/prInsIpi9/) insteadof a hard c (/prInkIpi9/) and indices as /Ind>aIsEz/.Since the text is known ahead of time, one could in-clude a pronunciation guide for rare words to assistthe narrator.The more disapointing result, however, is the veryslow return of the narration task.
Contrasting withthe successful elicitation of (McGraw et al, 2010),two reasons clearly stand out.First, these tasks were much too long in length.This was due to constraints we placed on collectionto improve data quality.
We assumed that multiplenarrators for a single article would ruin the narrativeflow.
Since few workers were willing to completefive recordings, future work could chop each articleinto smaller chunks to be completed by multiple nar-rators.
In contrast, eliciting spoken addresses has noneed for continuity across samples, thus the individ-ual HITs in (McGraw et al, 2010) could be muchsmaller.Second, and more importantly, our HITs requiredmuch more effort on the part of the Turker.
We choseto fully use Mechanical Turk?s API to manage dataand did not implement audio recording or data trans-mission through the browser.
Turkers were requiredto record audio in a separate program and then up-load the files.
We thought the added ability to re-record and review audio would be a plus comparedto in-browser recording.
In contrast, (McGraw et al,2010) used a javascript package to record narrationsdirectly in the browser window.
While it was sim-ple to use the API, it raised too much of a barrier forTurkers to complete the task.435.1 Feasability for Full NarrationRegardless of the task effectiveness, it is not clearthat Mechanical Turk is cost effective for large scalenarration.
A reasonable first task would be to nar-rate the 2500 featured articles on Wikipedia?s homepage.
They average 44 paragraphs in length witharound 4311 words per article.
Narrating this corpuswould cost $5500 at the rate of $0.05 per paragraph -if workers would be willing to complete at that rate.6 ConclusionOur experiments with Mechanical Turk attemptedto find the limits of data collection and nebuloustask definitions.
Long-form narration was unsuc-cessful due to the length of the tasks and the lackof a streamlined workflow for the Turkers.
How-ever, assigning qualifications based upon aggregat-ing qualitative opinions was very successful.
Thistask exploited the strenghts of Mechanical Turk byquickly gathering judgements that are easy for hu-mans to make but near impossible to reliably auto-mate.The contrast between the failure of this narrationtask and the success of previous elicitation is dueto the nature of the underlying task.
Our desire tohave one narrator per article prevented elicitation inshort bites of a few seconds long.
Additionally, ourefforts to solely use Mechanical Turk?s API limitedthe simplicity of the workflow.
While our backendwork was greatly simplified since we relied on ex-isting data management code, the lack of in-browserrecording placed too much burden on the Turkers.We would make the following changes if we wereto reimplement this task:1.
Integrate the workflow into the browser.2.
Perform post-process quality control to blockbad narrators from completing more HITs.3.
Drop the requirement of one narrator per ar-ticle.
A successful compromise might be onesection, averaging around five paragraphs.4.
Only narrate the lead in to an article (first par-gagraph) first.
If a user requests a full narration,then seek out the rest of the article.5.
Place qualification as a much larger set of as-signments.
Turkers often sort HITs by avail-able assignments, so the qualification HIT wasrarely seen.ReferencesChris Callison-Burch.
2009.
Fast, Cheap, and Creative:Evaluating Translation Quality Using Amazons Me-chanical Turk.
EMNLP.Matthew Marge, Satanjeev Banerjee, and AlexanderRudnicky.
2010.
Using the amazon mechanical turkfor transcription of spoken language.
ICASSP, March.Ian McGraw, Alexander Gruenstein, and Andrew Suther-land.
2009.
A self-labeling speech corpus: Collectingspoken words with an online educational game.
In IN-TERSPEECH.Ian McGraw, Chia ying Lee, Lee Hetherington, and JimGlass.
2010.
Collecting Voices from the Crowd.LREC, May.Scott Novotney and Chris Callison-Burch.
2010.
Cheap,Fast and Good Enough: Automatic Speech Recogni-tion with Non-Expert Transcription .
NAACL, June.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Y. Ng.
2008.
Cheap and fast?but is it good?
:evaluating non-expert annotations for natural languagetasks.
In EMNLP.44
