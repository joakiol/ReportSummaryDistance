Eva luat ion  of  NLP  sys temsCoordinator: Bente MaegaardCenter for SprogteknologiDK-2300 Copenhagen SE-maihbente@cst.ku.dkComputational linguistics as a science has hadits evaluation methods since its early days: Aconcordance program can be evaluated accordingto its ability to find all occurrences, to list themproperly, to have a flexible user interface tc., fre-quency programs *nay be evaluated according totheir statistics, the possibility of lemmatisation,parsers are evaluated according to their efficiencyetc.
When we contemplate one component at atime and want a technical evaluation, we normallyhave no problem defining the evaluation criteria.But once we get into more complicated systems:machine translation, dialogue systems, messageunderstanding systems etc., the issue of evalua-tion becomes more complex.
The most importantreason for this is that a technical evaluation ofthe functionality of a component is no longer suf-ficient.
First of all there are a number of compo-nents that have to hmction together, so the techni-cal functionality of the whole system becomes anissue.
Secondly, and more importantly, the per-formance of the whole system has to be assessedin itself.
This performance cannot be calculatedby the evaluation of the components.In the United States, this insight has been usedin a series of competitions (MUC, TREC) be-tween research teams, where the system's perfor-mance on a specific task was the only element thatcounted.
There are various other ways to evalu-ate research results, - the beauty of the algorithm,the methodology, the simplicity, the efficiency, theability to explain linguistic facts, the ability to im-itate human behaviour etc.
The evaluation of re-search results, be they single components or wholesystems, is one of the strands of the panel discus-sion.When we turn to the application of research re-suits, as in the type of language technology sys-tems mentioned above: machine translation, dia-logue systems for practical use etc., the real as-sessment is made by users in their application ofthe system.
Is the system useful for the user?Is it fast?
Reliable?
How much of the job.
e.g.translation, can it do in average?
This raises ques-tions about how this type of information can bemeasured.
The assessment of language technol-ogy tools and its similarity with and/or differencefrom the evaluation of research results is the otherstrand of the panel discussion.It is very important for the field to agree uponstandard methods for evaluation and much workhas been done in this field in recent years.
Thepanellists will present heir experience and theirviews and we hope to be able to provoke a discus-sion with the audience.ReferencesGalliers, J.R and K. Sparck Jones, 1993.
Evaluat-ing Natural Language Processing Systems, Uni-versity of Cambridge, Cambridge (to appear ina printed version).Grishman, R. and B. Sundheim, 1996.
MessageUnderstanding Conference - 6: A Brief History.COLING-96 Proceedings, Copenhagen.Hendry, D.G.
and T.R.G.
Green, 1993.
SpellingMistakes: How well do correctors perform?
Ad-junct Proceedings of InterCHI'93.Information Technology - Software product eval-uation - Quality characteristics and guidelinesfor their use, 1991.
ISO/IEC 9126, Geneva.King, M., 1987.
Machine rIk-anslation Today, Ed-inburgh Information Technology Series 2, Edin-burgh University Press, Edinburgh.TEMAA, A Testbed Study of Evaluation Method-ologies: Authoring Aids, 1996.
Final report,Center for Sprogteknologi, Copenhagen.i000i001
