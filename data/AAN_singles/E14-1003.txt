Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 20?29,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsMinimum Translation Modeling with Recurrent Neural NetworksYuening HuDepartment of Computer ScienceUniversity of Maryland, College Parkynhu@cs.umd.eduMichael Auli, Qin Gao, Jianfeng GaoMicrosoft ResearchRedmond, WA, USA{michael.auli,qigao,jfgao}@microsoft.comAbstractWe introduce recurrent neural network-based Minimum Translation Unit (MTU)models which make predictions based onan unbounded history of previous bilin-gual contexts.
Traditional back-off n-grammodels suffer under the sparse nature ofMTUs which makes estimation of high-order sequence models challenging.
Wetackle the sparsity problem by modelingMTUs both as bags-of-words and as asequence of individual source and targetwords.
Our best results improve the out-put of a phrase-based statistical machinetranslation system trained on WMT 2012French-English data by up to 1.5 BLEU,and we outperform the traditional n-grambased MTU approach by up to 0.8 BLEU.1 IntroductionClassical phrase-based translation models relyheavily on the language model and the re-ordering model to capture dependencies betweenphrases.
Sequence models over Minimum Trans-lation Units (MTUs) have been shown to com-plement both syntax-based (Quirk and Menezes,2006) as well as phrase-based (Zhang et al., 2013)models by explicitly modeling relationships be-tween phrases.
MTU models have been tradi-tionally estimated using standard back-off n-gramtechniques (Quirk and Menezes, 2006; Crego andYvon, 2010; Zhang et al., 2013), similar to word-based language models (?2).However, the estimation of higher-order n-grammodels becomes increasingly difficult due to datasparsity issues associated with large n-grams, evenwhen training on over one hundred billion words(Heafield et al., 2013); bilingual units are muchsparser than words and are therefore even harderto estimate.
Another drawback of n-gram mod-els is that future predictions are based on a limitedamount of previous context that is often not suf-ficient to capture important aspects of human lan-guage (Rastrow et al., 2012).Recently, several feed-forward neural network-based models have achieved impressive improve-ments over traditional back-off n-gram models inlanguage modeling (Bengio et al., 2003; Schwenket al., 2007; Schwenk et al., 2012; Vaswani et al.,2013), as well as translation modeling (Allauzen etal., 2011; Le et al., 2012; Gao et al., 2013).
Thesemodels tackle the data sparsity problem by rep-resenting words in continuous space rather thanas discrete units.
Similar words are grouped inthe same sub-space rather than being treated asseparate entities.
Neural network models can beseen as functions over continuous representationsexploiting the similarity between words, therebymaking the estimation of probabilities over higher-order n-grams easier.However, feed-forward networks do not directlyaddress the limited context issue either, since pre-dictions are based on a fixed-size context, similarto back-off n-gram models.
We therefore focusin this paper on recurrent neural network architec-tures, which address the limited context issue bybasing predictions on an unbounded history of pre-vious events which allows to capture long-span de-pendencies.
Recurrent architectures have recentlyadvanced the state of the art in language model-ing (Mikolov et al., 2010; Mikolov et al., 2011a;Mikolov, 2012) outperforming multi-layer feed-forward based networks in perplexity and word er-ror rate for speech recognition (Arisoy et al., 2012;Sundermeyer et al., 2013).
Recent work has alsoshown successful applications to machine transla-tion (Mikolov, 2012; Auli et al., 2013; Kalchbren-ner and Blunsom, 2013).
We extend this work bymodeling Minimum Translation Units with recur-rent neural networks.Specifically, we introduce two recurrent neu-ral network-based MTU models to address the is-20M1 M2 M3 M4 M5Yu        ZuoTian JuXing Le HuiTanheld=> null=> Yesterday=> held=> the=> meeting?
??
??
?
?
?YuZuoTianJuXing_LenullHuiTannullthe meeting null yesterdayM1:M2:M3:M4:M5:Figure 1: Example Minimum Translation Unitpartitioning based on Zhang et al.
(2013).sues regarding data sparsity and limited contextsizes by leveraging continuous representations andthe unbounded history of the recurrent architec-ture.
Our first approach frames the problem as asequence modeling task over minimal units (?3).The second model improves over the first by mod-eling an MTU as a bag-of-words, thereby allow-ing us to learn representations over sub-structuresof minimal units that are shared across MTUs(?4).
Our models significantly outperform the tra-ditional back-off n-gram based approach and weshow that they act complementary to a very strongrecurrent neural network-based language modelbased solely on target words (?5).2 Minimum Translation UnitsBanchs et al.
(2005) introduced the idea of framingtranslation as a sequence modeling problem wherea sentence pair is generated in left-to-right order asa sequence of bilingual n-grams.
Minimum Trans-lation Units (Quirk and Menezes, 2006; Zhanget al., 2013) are an extension which additionallypermit tuples with empty source or target sides,thereby allowing insertion or deletion phrase pairs.The two basic requirements for MTUs are thatthere are no overlapping word alignment links be-tween phrase pairs and it should not be possible toextract smaller phrase pairs without violating theword alignment constraints.
Informally, we canthink of MTUs as small phrase pairs that cannotbe broken down any further without violating thetwo requirements.Minimum Translation Units partition a sentencepair into a set of minimal bilingual units or tu-Words MTUsTokens 34,769,416 14,853,062Types 143,524 1,315,512Singleton types 34.9% 80.1%Table 1: Token and type counts for both sourceand target words as well as MTUs based on theWMT 2006 German to English data set (cf.
?5).ples obtained by an algorithm similar to phrase-extraction (Koehn et al., 2003).
Figure 1 illus-trates such a partitioning.
Modeling minimal unitshas two advantages over considering larger phrasepairs that are effectively composed of MTUs:First, minimal units result in a unique partition-ing of a sentence pair.
This has the advantage thatwe avoid modeling spurious derivations, that is,multiple derivations generating the same sentencepair.
Second, minimal units result in smaller mod-els with a smoother distribution than models basedon composed units (Zhang et al., 2013).Sentence pairs can be generated in multiple or-ders, such as left-to-right or right-to-left, either insource or target order.
For example, the sourceleft-to-right order of the sentence pair in Figure 1is simply M1, M2, M3, M4, M5, while the tar-get left-to-right order is M3, M4, M5, M1, M2.We deal with inserted or deleted words similar toZhang et al.
(2013): The source side null token ofan inserted target phrase is placed next to the lastsource word aligned to the closest preceding non-null aligned target phrase; a similar rule is appliedto null tokens on the target side.
For example, inFigure 1 we place M4 straight after M3 because?the?, the aligned target phrase, is after ?held?, theprevious non-null aligned target phrase.We can straightforwardly estimate an n-grammodel over MTUs to estimate the probabilityof a sentence pair using standard back-off tech-niques commonly employed in language mod-eling.
For example, a trigram model in tar-get left-to-right order factors the sentence pair inFigure 1 as p(M3) p(M4|M3) p(M5|M3,M4)p(M1|M4,M5)p(M2|M5,M1).If we would like to model larger contexts, thenwe quickly run into data sparsity issues.
To illus-trate this point, consider the parameter growth ofan n-gram model which is driven by the vocabu-lary size |V | and the n-gram order n: O(|V |n).Clearly, the exact estimation of higher-order n-21gram probabilities becomes more difficult withlarge n, leading to the estimation of events withincreasingly sparse statistics, or having to relyon statistics from lower-order events with back-off models, which is less desirable.
Even word-based language models rarely ventured so farmuch beyond 5-gram statistics as demonstratedby Heafield et al.
(2013) who trained a, by to-day?s standards, very large 5-gram model on 130Bwords.
Data sparsity is therefore an even more sig-nificant issue for MTU models relying on muchlarger vocabularies.
In our setting, the MTU vo-cabulary is an order of magnitude larger than aword vocabulary obtained from the same data (Ta-ble 1).
Furthermore, most MTUs are observedonly once making the reliable estimation of prob-abilities very challenging.Neural network-based sequence models tacklethe data sparsity problem by learning continuousword representations, that group similar words to-gether in continuous space.
For example, thedistributional representations induced by recurrentneural networks have been found to have interest-ing syntactic and semantic regularities (Mikolovet al., 2013).
Furthermore, these representationscan be exploited to estimate more reliable statis-tics over higher-order n-grams than with discreteword units.
Recurrent neural networks go beyondfixed-size contexts and allow the model to keeptrack of long-span dependencies that are importantfor future predictions.
In the next sections we willpresent Minimum Translation Unit models basedon recurrent architectures.3 Atomic MTU RNN ModelThe first model we introduce is based on the recur-rent neural network language model of Mikolovet al.
(2010).
We frame the problem as a tradi-tional sequence modeling task which treats MTUsas atomic units, similar to the approach taken bythe traditional back-off n-gram models.The model is factored into an input layer, a hid-den layer with recurrent connections, and an out-put layer (Figure 2).
The input layer encodes theMTU at time t as a 1-of-N vector mtwith all val-ues being zero except for the entry representingthe MTU.
The output layer ytrepresents a proba-bility distribution over possible next MTUs; boththe input and output layers are of size |V |, the sizeof the MTU vocabulary.
The hidden layer state htencodes the history of all MTUs observed in themtht-1htytVWU00100000Figure 2: Structure of the atomic recurrent neu-ral network MTU model following the word-basedRNN model of Mikolov (2012).sequence up to time step t.The state of the hidden layer is determined bythe input layer and the hidden layer configurationof the previous time step ht?1.
The weights of theconnections between the layers are summarized ina number of matrices: U represents weights fromthe input layer to the hidden layer, and W repre-sents connections from the previous hidden layerto the current hidden layer.
Matrix V containsweights between the current hidden layer and theoutput layer.The hidden and output layers are computedvia a series of matrix-vector products and non-linearities:ht= s(Umt+Wht?1)yt= g(Vht)wheres(z) =11 + exp {?z}, g(zm) =exp {zm}?kexp {zk}are sigmoid and softmax functions, respectively.Additionally, the network is interpolated with amaximum entropy model of sparse n-gram fea-tures over input MTUs (Mikolov et al., 2011a).The maximum entropy weights D are added tothe output activations before applying the softmaxfunction and are estimated jointly with all otherparameters (Figure 3).11While these features depend on multiple input MTUs, we22mtht-1htytVWU00100000TDc tFigure 3: Structure of atomic recurrent neural net-work MTU model with classing layer ctand directconnections D between the input and output lay-ers (cf.
Figure 2).The model is optimized via a maximum likeli-hood objective function using stochastic gradientdescent.
Training is based on the truncated backpropagation through time algorithm, which unrollsthe network and then computes error gradientsover multiple time steps (Rumelhart et al., 1986);we use a cross entropy criterion to obtain the errorvector with respect to the output activations andthe desired prediction.
After training, the outputlayer represents posteriors p(mt+1|mtt?n+1,ht),the probability of the next MTU given the previ-ous n input MTUs mtt?n+1= mt, .
.
.
,mt?n+1and the current hidden layer configuration ht.Na?
?ve computation of the probability distribu-tion over the next MTU is very expensive for largevocabularies, such as commonly encountered forMTU models (Table 1).
A well established ef-ficiency trick assigns each possible output to aunique class and then uses a two-step process tofind the probability of an MTU, instead of comput-ing the probability of all possible outputs (Good-man, 2001; Emami and Jelinek, 2005; Mikolov etal., 2011b).
Under this scheme we compute theprobability of an MTU by multiplying the prob-ability of its class citwith the probability of thedepicted them for simplicity as a connection between thecurrent input vectormtand the output layer.minimal unit conditioned on the class:p(mt+1|mtt?n+1,ht) =p(cit|mtt?n+1,ht) p(mt+1|cit,mtt?n+1,ht)This factorization reduces the complexity of com-puting the output probabilities from O(|V |) toO(|C| + maxi|ci|) where |C| is the number ofclasses and |ci| is the number of minimal unitsin class ci.
The best case complexity O(?|V |)requires the number of classes and MTUs to beevenly balanced, i.e., each class contains exactlyas many minimal units as there are classes.Figure 3 illustrates how classing changes thestructure of the network by adding an additionaloutput layer for the class probabilities.4 Bag-of-words MTU RNN ModelThe previous model treats MTUs as atomic sym-bols which leads to large vocabularies requir-ing large parameter sets and expensive inference.However, similar MTUs may share the samewords, or words which are related in continuousspace.
The atomic MTU model does not exploitthis since it cannot access the internal structure ofa minimal unit.The approach we pursue next is to break MTUsinto individual source and target words (Le et al.,2012) in order to exploit structural similarities be-tween infrequently observed minimal units.
Sin-gletons represent the vast majority of our MTUvocabulary (Table 1).
This resembles the word-hashing trick of Huang et al.
(2013) who repre-sented individual words as a bag-of-character n-grams to reduce the vocabulary size of a neuralnetwork-based model in an information retrievalsetting.2We first describe a theoretically appealing butcomputationally expensive model and then discussa more practical variation.
The input layer of thismodel accepts the current minimal unit as a K-of-N vector representing K source and target wordsas opposed to the 1-of-N encoding of entire MTUsin the previous model (Figure 4).
Larger MTUsmay contain the same word more than once and wesimply adjust their count to one.3Different to the2Applying the same technique would likely result in too manycollisions since we are dealing with multi-word units insteadof single words.3We found no effect on accuracy when using the unmodifiedcount in initial experiments.23x tht-1htw tVWU10110100D ytC.
.
.. .
.. .
.sr ctgtMT UFigure 4: Structure of MTU bag-of-words recur-rent neural network model.
The input layer rep-resents a minimal unit as a bag-of-words and theoutput layer ytis a probability distribution overpossible next MTUs depending on the activationsof the word layer wtrepresenting source and tar-get words of minimal units.previous model, the input vector has now multipleactive entries whose signals are absorbed into thenew hidden layer configuration.This bag-of-words encoding of minimal unitsdramatically reduces the vocabulary size but it in-evitably maps different MTUs to the same encod-ing.
On our data set, we observe less than 0.2% ofminimal units that are involved in collisions, a ratethat is similar to Huang et al.
(2013).
In practicecollisions are unlikely to affect accuracy in our set-ting because MTUs that are mapped to the sameencoding usually do not differ much in semanticmeaning as illustrated by the following examples:erfolg haben ?
succeed collides with haben er-folg?
succeed, or damit ,?
to and , damit?
to;in both examples either the auxiliary verb haben orthe comma changes position, neither of which sig-nificantly changes the meaning for this particularpair of MTUs.The structure of the bag-of-words MTU RNNmodels is shown in Figure 4.
Similar to the atomicMTU RNN model (?3), the hidden layer combinesthe signal from the input layer and the previoushidden layer configuration.
The hidden layer acti-vations feed into a word layer wtrepresenting thesource and target words that part of all possibleMTUs; it is of the same size as the input layer.
Theword layer is connected to a convolutional out-put layer ytby weights summarized in the sparsematrix C. The output layer represents all possi-ble next minimal units, where each MTU entry isonly connected to neurons in the word layer repre-senting its source and target words.
The word andMTU layers are then computed as follows:wt= s(Vht)yt= g(Cwt)However, there are a number of computationalissues with this model: First, we cannot efficientlyfactor the word layer wtinto classes such as forthe atomic MTU RNN model because we requireall its activations to compute the MTU outputlayer yt.
This reduces the best case complex-ity of computing the word layer from O(?|V |)back to linear in the number of source and tar-get words |V |.
In practice this results in between200-1000 more activations that need to be com-puted, depending on the word vocabulary size.Second, turning the MTU output layer into a con-volutional layer is not enough to sufficiently re-duce the computational effort to compute the out-put activations since the number of connectionsbetween the word and MTU layers is very imbal-anced.
This is because frequent words, such asfunction words, are part of many MTUs and there-fore have a very high out-degree, e.g., the neuronrepresenting ?the?
has over 82K outgoing edges.On the other hand, infrequent words, have a verylow out-degree.
This imbalance makes it hardto efficiently compute activations and error gradi-ents, even on a GPU, since some neurons requiresubstantially more work than others.4For these reasons we decided to design a sim-pler, more tractable version of this model (Fig-ure 5).
The simplified model still represents aninput MTU as a bag-of-words but minimal unitsare generated word-by-word, first emitting sourcewords and then target words.
This is in contrastto the original model which predicted an MTU asa single unit.
Decomposing the next MTU intoindividual words dramatically reduces the size ofthe output layer, thereby resulting in faster com-putation of the outputs and making normalization4In initial experiments we found this model to be over twentytimes slower than the atomic MTU RNN model with esti-mated training times of over 6 weeks.
This was despite us-ing a vastly smaller vocabulary and by computing the wordlayer on a, by current standards, high-end GPU (NVIDIATesla K20c) using sparse matrix optimizations (cuSPARSE)for the convolutional layer.24mtht-1htytVWU10110100TDc tmt+ 1sr ctgtMT UFigure 5: Simplified MTU bag-of-words recurrentneural network model (cf.
Figure 4).
An MTU isinput as bag-of-words and the next MTU is pre-dicted as a sequence of both source and targetwords.into probabilities easier.
Furthermore, the outputlayer can be factorized into classes requiring onlya fraction of the neurons to be computed, a muchmore efficient solution compared to the originalmodel which required calculation of the entire out-put layer.The simplified model computes the probabilityof the next MTU mt+1as a product of individualword probabilities:p(mt+1|mtt?n+1,ht) = (1)?a1,...,au?mt+1p(ck|mtt?n+1,ht)p(ak|ck,mtt?n+1,ht)where we predict a sequence of source and targetwords a1, .
.
.
, au?
mt+1with a class-structuredoutput layer, similar to the atomic model (?3).Training still uses a cross entropy criterion andback propagation through time, however, errorvectors are computed on a per-word basis, insteadof a per-MTU basis.
Direct connections betweenthe input and output layers are based on source andtarget words which is less sparse than basing directfeatures on entire MTUs such as for the originalbag-of-words model.Overall, the simplified model retains the bag-of-words input representation of the original model,while permitting the efficient factorization of theword-output layer into classes.5 ExperimentsWe evaluate the effectiveness of both the atomicMTU RNN model (?3) and the simplified bag-of-words MTU RNN model (?4) in an n-best rescor-ing setting, comparing against a trigram back-offMTU model as well as the phrasal decoder 1-bestoutput which we denote as the baseline.5.1 Experimental SetupBaselines.
We experiment with an in-housephrase-based system similar to Moses (Koehn etal., 2007), scoring translations by a set of commonfeatures including maximum likelihood estimatesof source given target mappings pMLE(e|f) andvice versa pMLE(f |e), as well as lexical weight-ing estimates pLW(e|f) and pLW(f |e), word andphrase-penalties, a linear distortion feature anda lexicalized reordering feature.
The baselineincludes a standard modified Kneser-Ney word-based language model trained on the target-side ofthe parallel corpora described below.
Log-linearweights are estimated with minimum error ratetraining (MERT; Och, 2003).The 1-best output by the phrase-based decoderis the baseline accuracy.
As a second baseline weexperiment with a trigram back-off MTU modeltrained on all extracted MTUs, denoted as n-gramMTU.
The trigram MTU model is estimated withthe same modified Kneser-Ney framework as thetarget side language model.
All MTU models aretrained in target left-to-right MTU order whichperformed well in initial experiments.Evaluation.
We test our approach on two differ-ent data sets.
First, we train a German to Englishsystem based on the data of the WMT 2006 sharedtask (Koehn and Monz, 2006).
The parallel corpusincludes about 35M words of parliamentary pro-ceedings for training, a development set and twotest sets with 2000 sentences each.Second, we experiment with a French to En-glish system based on 102M words of training datafrom the WMT 2012 campaign.
The majority ofthe training data set is parliamentary proceedingsexcept for about 5m words which are newswire; allMTU models are trained on the newswire subsetsince we found similar accuracy to using all data ininitial experiments.
We evaluate on four newswiredomain test sets from 2008, 2010 and 2011 as wellas the 2010 system combination test set contain-ing between 2034 to 3003 sentences.
Log-linearweights are estimated on the 2009 data set com-25prising 2525 sentences.
We evaluate all systemsin a single reference BLEU setting.Rescoring Setup.
We rescore the 1000-best out-put of the baseline phrase-based decoder by ei-ther the trigram back-off MTU model or theRNN models.
The baseline accuracy is obtainedby choosing the 1-best decoder output.
We re-estimate the log-linear weights for rescoring byrunning a further iteration of MERT with the ad-ditional feature values; we initialize the rescoringfeature weight to zero and try 20 random restarts.At test time we use the new set of log-linearweights to rescore the test set n-best list.Neural Network Setup.
We trained the recur-rent neural network models on between 88% and93% of each data set and used the remainder asvalidation data.
The vocabulary of the atomicMTU RNN model is comprised of all MTU typeswhich were observed more than once in the train-ing data.5Similarly, we modeled all non-singletonwords for the bag-of-words MTU RNN model.We obtain classes for words or MTUs using aversion of Brown-Clustering with an additionalregularization term to optimize the runtime ofthe language model (Brown et al., 1992; Zweigand Makarychev, 2013).
Direct connections usefeatures over unigrams, bigrams and trigrams ofwords or MTUs, depending on the model.
Fea-tures are hashed to a table with at most 500 millionvalues following Mikolov et al.
(2011a).
We usethe standard settings for the model with the defaultlearning rate ?
= 0.1 that decays exponentially ifthe validation set entropy does not decrease.
Backpropagation through time computes error gradi-ents over the past twenty time steps.
Trainingis stopped after 20 epochs or when the valida-tion entropy does not decrease over two epochs.Throughout, we use a hidden layer size of 100which provided a good trade-off between time andaccuracy in initial experiments.5.2 ResultsWe first report the decoder 1-best output as thefirst baseline and then rescore our two data sets(Table 2 and Table 3) with the n-gram back-offMTU model to establish a second baseline (n-gram MTU).
The n-gram model improves by 0.4BLEU over the decoder 1-best on all test sets forGerman to English.
On French-English accuracy5We tried modeling all MTUs which did not contain a single-ton word but observed no significant effect on accuracy.dev test1 test2Baseline 25.8 26.0 26.0n-gram MTU 26.3 26.6 26.4atomic MTU RNN 26.5 26.8 26.5BoW MTU RNN 26.5 27.0 26.9word RNNLM 26.5 27.1 26.8Combined 26.8 27.3 27.1Table 2: German to English BLEU results forthe decoder 1-best output (Baseline) compared torescoring with a target left-to-right trigram MTUmodel (n-gram MTU), our two recurrent neuralnetwork-based MTU models, a word-based RNN-based language model (word RNNLM), as wellas a combination of the three RNN-based models(Combined).improves on three out of five sets by up to 0.7BLEU.Next, we evaluate the accuracy of the MTURNN models.
The atomic MTU RNN model im-proves over the n-gram MTU model on all test setsfor German to English, however, for French to En-glish the back-off model performs better on twoout of four test sets.The next question we answer is if breakingMTUs into individual units to leverage similaritiesin the internal structure can help accuracy.
The re-sults (Table 2 and Table 3) for the bag-of-wordsmodel (BoW MTU RNN) clearly show that this isthe case for both language pairs.
We significantlyimprove over the n-gram MTU model as well asthe atomic RNN model on all test sets.
We observegains of up to 0.5 BLEU over the n-gram MTUmodel for German to English as well as French toEnglish; improvements over the decoder baselineare up to 1.2 BLEU for French to English.How do our models compare to other neural net-work approaches that rely only on target side in-formation?
To answer this question we compareto the strong language model of Mikolov (2012;RNNLM) which has recently improved the state-of-the-art in language modeling perplexity.
Theresults (Table 2 and Table 3) show that RNNLMperforms competitively.
However, our approachesmodel translation since we use both source and tar-get information as opposed to scoring only the flu-ency of the target side, such as done by RNNLM.Can our models act complementary to a strongRNN language model?
Our final experiment com-bines the atomic MTU RNN model, the BoW26dev news2008 news2010 news2011 newssyscomb2010Baseline 24.3 20.5 24.4 25.1 24.3n-gram MTU 24.6 20.8 24.4 25.8 24.3atomic MTU RNN 24.6 20.7 24.4 25.5 24.3BoW MTU RNN 25.2 21.2 24.8 26.3 24.6word RNNLM 25.1 21.4 25.1 26.4 24.9Combined 25.4 21.4 25.1 26.6 24.9Table 3: French to English BLEU results for the decoder 1-best output (Baseline) compared to variousMTU models (cf.
Table 2).MTU RNN model, and the RNNLM (Combined).The results (Table 2 and Table 3) confirm that thisis the case.
For German to English translationaccuracy improves by 0.2 to 0.3 BLEU over theRNNLM alone, with gains of up to 1.3 BLEU overthe baseline and up to 0.7 BLEU over the n-gramMTU model.
Improvements for French to Englishare lower but we can see some gains on news2011and on the dev set.
Overall, we improve accuracyon the French to English task by up to 1.5 BLEUover the decoder 1-best, and by up to 0.8 BLEUover the n-gram MTU model.6 Related WorkOur approach of modeling Minimum TranslationUnits is very much in line with recent work on n-gram-based translation models (Crego and Yvon,2010), and more recently, continuous space-basedtranslation models (Le et al., 2012).
The mod-els presented in this paper differ in a number ofkey aspects: We use a recurrent architecture repre-senting an unbounded history of MTUs rather thana feed-forward style network.
Feed-forward net-works as well as back-off n-gram models rely on afinite history which results in predictions indepen-dent of anything but a short context of words.
Arecent side-by-side comparison between recurrentand feed-forward style neural networks (Sunder-meyer et al., 2013) has shown that recurrent ar-chitectures outperform feed-forward networks ina language modeling task, a similar problem tomodeling sequences over Minimum TranslationUnits.Furthermore, the input of our best model is abag-of-words representation of an MTU, unlikethe ordered source and target word n-grams usedby Crego and Yvon (2010) as well as Le et al.(2012).
Finally, we model both source and targetwords in a single recurrent neural network.
Theapproach of Le et al.
(2012) factorizes the jointprobability over an MTU sequence in a way thatsuggests the use of separate neural network mod-els for the source and the target sides, where eachmodel generates words on the respective side only.Other work on applying recurrent neural net-works to machine translation (Mikolov, 2012; Auliet al., 2013; Kalchbrenner and Blunsom, 2013)concentrated on word-based language and transla-tion models, whereas we model Minimum Trans-lation Units.7 Conclusion and Future WorkMinimum Translation Unit models based on recur-rent neural networks lead to substantial gains overtheir classical n-gram back-off models.
We intro-duced two models of which the best improves ac-curacy by up to 1.5 BLEU over the 1-best decoderoutput, and by 0.8 BLEU over a trigram MTUmodel in an n-best rescoring setting.Our experiments have shown that representingMTUs as bags-of-words leads to better accuracysince this exploits similarities in the internal struc-ture of Minimum Translation Units, which is notpossible when modeling them as atomic symbols.We have also shown that our models are comple-mentary to a very strong RNN language model(Mikolov, 2012).In future work, we would like to make the initialversion of the bag-of-words model computation-ally more tractable using a better GPU implemen-tation.
This model combines the efficient bag-of-words input representation with the ability to pre-dict MTUs as single units while explicitly model-ing the constituent words in an intermediate layer.8 AcknowledgementsWe would like to thank Kristina Toutanova forproviding a dataset and for helpful discussions re-lated to this work.
We also thank the four anony-mous reviewers for their comments.27ReferencesAlexandre Allauzen, H?el`ene Bonneau-Maynard, Hai-Son Le, Aur?elien Max, Guillaume Wisniewski,Franc?ois Yvon, Gilles Adda, Josep Maria Crego,Adrien Lardilleux, Thomas Lavergne, and ArtemSokolov.
2011.
LIMSI @ WMT11.
In Proc.
ofWMT, pages 309?315, Edinburgh, Scotland, July.Association for Computational Linguistics.Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, andBhuvana Ramabhadran.
2012.
Deep Neural Net-work Language Models.
In NAACL-HLT Work-shop on the Future of Language Modeling for HLT,pages 20?28, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Michael Auli, Michel Galley, Chris Quirk, and Geof-frey Zweig.
2013.
Joint Language and TranslationModeling with Recurrent Neural Networks.
In Proc.of EMNLP, October.Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert,Patrik Lambert, and Jos?e B. Mari?no.
2005.
Statis-tical Machine Translation of Euparl Data by Usingbilingual n-grams.
In Proc.
of ACL Workshop onBuilding and Using Parallel Texts, pages 133?136,Jun.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A Neural Probabilistic Lan-guage Model.
Journal of Machine Learning Re-search, 3:1137?1155.Peter F. Brown, Peter V. deSouza, Robert L. Mer-cer, Vincent J. Della Pietra, and Jenifer C. Lai.1992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics, 18(4):467?479,Dec.Josep Crego and Franc?ois Yvon.
2010.
Factored bilin-gual n-gram language models for statistical machinetranslation.
Machine Translation, 24(2):159?175.Ahmad Emami and Frederick Jelinek.
2005.
A Neu-ral Syntactic Language Model.
Machine Learning,60(1-3):195?227, September.Jianfeng Gao, Xiaodong He, Wen-tau Yih, andLi Deng.
2013.
Learning Semantic Representationsfor the Phrase Translation Model.
Technical ReportMSR-TR-2013-88, Microsoft Research, September.Joshua Goodman.
2001.
Classes for Fast MaximumEntropy Training.
In Proc.
of ICASSP.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.Clark, and Philipp Koehn.
2013.
Scalable ModifiedKneser-Ney Language Model Estimation.
In Proc.of ACL, August.Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,Alex Acero, and Larry Heck.
2013.
Learning DeepStructured Semantic Models for Web Search usingClickthrough Data.
In Proc.
of CIKM, October.Nal Kalchbrenner and Phil Blunsom.
2013.
Re-current Continuous Translation Models.
In Proc.of EMNLP, pages 1700?1709, Seattle, Washington,USA, October.
Association for Computational Lin-guistics.Philipp Koehn and Christof Monz.
2006.
Manual andautomatic evaluation of machine translation betweeneuropean languages.
In Proc.
of NAACL Workshopon Statistical Machine Translation, pages 102?121.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
InProc.
of HLT-NAACL, pages 127?133, Edmonton,Canada, May.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In Proc.
of ACL Demo and Poster Sessions, pages177?180, Prague, Czech Republic, Jun.Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.2012.
Continuous Space Translation Models withNeural Networks.
In Proc.
of HLT-NAACL, pages39?48, Montr?eal, Canada.
Association for Compu-tational Linguistics.Tom?a?s Mikolov, Karafi?at Martin, Luk?a?s Burget, JanCernock?y, and Sanjeev Khudanpur.
2010.
Recur-rent Neural Network based Language Model.
InProc.
of INTERSPEECH, pages 1045?1048.Tom?a?s Mikolov, Anoop Deoras, Daniel Povey, Luk?a?sBurget, and Jan?Cernock?y.
2011a.
Strategiesfor Training Large Scale Neural Network LanguageModels.
In Proc.
of ASRU, pages 196?201.Tom?a?s Mikolov, Stefan Kombrink, Luk?a?s Burget, JanCernock?y, and Sanjeev Khudanpur.
2011b.
Ex-tensions of Recurrent Neural Network LanguageModel.
In Proc.
of ICASSP, pages 5528?5531.Tom?a?s Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013.
Linguistic Regularities in Continuous Space-Word Representations.
In Proc.
of NAACL, pages746?751, Stroudsburg, PA, USA, June.
Associationfor Computational Linguistics.Tom?a?s Mikolov.
2012.
Statistical Language Modelsbased on Neural Networks.
Ph.D. thesis, Brno Uni-versity of Technology.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proc.
of ACL,pages 160?167, Sapporo, Japan, July.Chris Quirk and Arul Menezes.
2006.
Do we needphrases?
Challenging the conventional wisdom inStatistical Machine Translation.
In Proc.
of NAACL,pages 8?16, New York, Jun.28Ariya Rastrow, Sanjeev Khudanpur, and Mark Dredze.2012.
Revisiting the Case for Explicit SyntacticInformation in Language Models.
In NAACL-HLTWorkshop on the Future of Language Modeling forHLT, pages 50?58.
Association for ComputationalLinguistics.David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.Williams.
1986.
Learning Internal Representationsby Error Propagation.
In Symposium on Parallel andDistributed Processing.Holger Schwenk, Marta R. Costa-juss`a, and Jos?e A. R.Fonollosa.
2007.
Smooth Bilingual N -Gram Trans-lation.
In Proc.
of EMNLP, pages 430?438, Prague,Czech Republic, June.
Association for Computa-tional Linguistics.Holger Schwenk, Anthony Rousseau, and MohammedAttik.
2012.
Large, Pruned or Continuous SpaceLanguage Models on a GPU for Statistical MachineTranslation.
In NAACL-HLT Workshop on the Fu-ture of Language Modeling for HLT, pages 11?19.Association for Computational Linguistics.Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,Ben Freiberg, Ralf Schl?uter, and Hermann Ney.2013.
Comparison of Feedforward and RecurrentNeural Network Language Models.
In IEEE In-ternational Conference on Acoustics, Speech, andSignal Processing, pages 8430?8434, Vancouver,Canada, May.Ashish Vaswani, Yinggong Zhao, Victoria Fossum, andDavid Chiang.
2013.
Decoding with Large-scaleNeural Language Models improves Translation.
InProc.
of EMNLP.
Association for ComputationalLinguistics, October.Hui Zhang, Kristina Toutanova, Chris Quirk, and Jian-feng Gao.
2013.
Beyond left-to-right: Multiple de-composition structures for smt.
In Proc.
of NAACL,pages 12?21, Atlanta, Georgia, June.
Associationfor Computational Linguistics.Geoff Zweig and Konstantin Makarychev.
2013.Speed Regularization and Optimality in Word Class-ing.
In Proc.
of ICASSP.29
