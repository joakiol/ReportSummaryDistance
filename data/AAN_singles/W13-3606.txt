Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 43?51,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsA Tree Transducer Model for Grammatical Error CorrectionJan Buys and Brink van der MerweMIH Media Lab and Computer Science DivisionStellenbosch University, South Africajanbuys@ml.sun.ac.za, abvdm@cs.sun.ac.zaAbstractWe present an approach to grammatical er-ror correction for the CoNLL 2013 sharedtask based on a weighted tree-to-stringtransducer.
Rules for the transducer areextracted from the NUCLE training data.An n-gram language model is used torerank k-best sentence lists generated bythe transducer.
Our system obtains a pre-cision, recall and F1 score of 0.27, 0.1333and 0.1785, respectively, on the officialtest set.
On the revised annotations, theF1 score increases to 0.2505.
Our systemranked 6th out of the participating teamson both the original and revised test set an-notations.1 IntroductionThere has recently been an increase in research onautomated grammatical error detection and correc-tion for writing by English language learners (Lea-cock et al 2010).
In the most prominent line ofresearch, statistical classifiers are trained to de-tect or correct specific error types.
Features forthese classifiers are based on word context and lo-cal syntactic information.
The classifiers are com-bined, and a language model is often used to filtercorrections.
Research on this approach focussesespecially on preposition and determiner errors.Most of the systems in the HOO 2011 and 2012shared tasks (Dale and Kilgarriff, 2011; Dale etal., 2012) fall under this broad approach.In a second class of models, a model for gen-erating corrected sentences is formulated in thenoisy-channel framework, relying strongly on alanguage model to distinguish between grammati-cal and ungrammatical candidate corrections (Leeand Seneff, 2006; Turner and Charniak, 2007;Park and Levy, 2011).
Such models are often in-spired by techniques developed for statistical ma-chine translation (Brockett et al 2006).
Finally,rule-based methods are often used in commerciallanguage processing systems such as word pro-cessors.
Here large hand-crafted linguistically ex-pressive, error-tolerant grammars are used to anal-yse sentences and identify where constraints havebeen broken.In this paper we present our system for theCoNLL 2013 shared task in grammatical error cor-rection (Ng et al 2013).
Our grammar correctionmodel is based on a tree-to-string transducer thatis specified by a set of rules that each rewrite atree fragment to a string of words and variables.These rules are extracted automatically from a setof training examples.
Each training example con-sists of an incorrect sentence, a corresponding cor-rect sentence with its parse tree, and a word align-ment between the incorrect and correct sentences.During decoding the model searches for parsedwell-formed sentences that could be transformedinto a given incorrect sentence with high probabil-ity.
Sentences are split into linguistically plausibleclauses to decrease the average sentence length, inorder to improve decoding runtime.
In order todiscriminate more accurately between candidatesentence corrections an n-gram language modeltrained on a large corpus of well-formed text isused to rerank the k-best hypotheses that the trans-ducer model generates.
The tree transducer andlanguage model scores are weighted to maximizethe model F1 score on a validation set.
After de-coding the clauses are recombined into the originalsentence structure.The next section describes preprocessing andthe resources used by our system.
Section 3defines weighted tree-to-string transducers.
Wepresent the formulation of our error correctionmodel in section 4, and discuss decoding with itin section 5.
Section 6 describes language modelreranking.
System results are given in section 7.Finally, section 8 draws some conclusions and dis-cuss future work.432 Data Pre-processing2.1 NUCLEWe use the pre-processed version of the NUCLEcorpus (Dahlmeier et al 2013) released as train-ing data for this shared task.
The data con-sist of essays, subdivided into paragraphs.
Us-ing NLTK (Bird et al 2009), paragraphs weresplit into sentences with NLTK punkt and sen-tences were tokenized with NLTK word tokenize.Though this sentence-splitting and tokenization isnot error-free (for example, quotation marks arehandled incorrectly in some contexts), we use it tomaintain consistency in our model.
An error anno-tation in the data consists of the start and end tokenoffsets in a sentence, as well as the correction thatshould replace the text between the offsets.We divide the corpus into 80% training data,10% validation data and 10% development data.Splitting is performed by random selection at es-say level.
For each sentence with corrections, werefer to the original as the incorrect sentence, andto the version with the corrections applied to itas the correct sentence.
For the purpose of train-ing our models, all words are lowercased.
As de-scribed below, we also construct an alignment be-tween the words of each of these sentence pairs.The 2013 shared task focusses on five errortypes: Article or determiner, preposition, nounnumber, verb form, and subject-verb agreementerrors.
In the training data we only apply correc-tions of these types to obtain the correct versionof the sentences, though other error types are alsoincluded in the error annotations.
An alternativewould be to apply the corrections of other errortypes to the correct and incorrect versions of thesentences.
However, we decided against that in or-der to keep the training data realistically close tothe test data, which will also contain these other er-rors.
We do, however, correct some of the mechan-ical errors, especially spelling errors, in the incor-rect and correct versions of the training data, toreduce noise that these errors may introduce intothe model.In order to train a syntax-based model for gram-mar correction, the correct version of the sen-tences are parsed with the Berkeley parser (Petrovand Klein, 2007).
The Berkeley parser is a state-of-the-art unlexicalized parser.
Given that the cor-rect side of our training data will still contain er-rors, it is unlikely that lexicalized parsing will bemore accurate.
Parser options are set to obtain left-binarized parse trees under Viterbi decoding.2.2 Wikipedia language modelWe train the n-gram language model used in oursystem on a large corpus of text extracted fromthe English Wikipedia.
The April 2013 WikipediaXML dump1 is used.
This is parsed with thegwtwiki2 Wikipedia parser, and all sentences con-sisting of 6 or more words are extracted.
Thesesentences are tokenized with NLTK and lower-cased.
The corpus has about 1 500 millions words.As vocabulary we use the 64 000 words with thehighest frequency occurrence in the corpus.
A3-gram language model is trained from the cor-pus on this restricted vocabulary, to keep the sizeof the language model reasonable.
The languagemodel is trained and applied with the SRILMtoolkit (Stolcke, 2002).
Kneser-Ney smoothing isused to estimate the model weights.2.3 VocabularyWe set the vocabulary of the transducer model asthe union of the vocabulary of our language modeland the vocabulary of the words of the correct sen-tences in the NUCLE training data.
In the trans-ducer construction we ensure that all words in thisvocabulary can be accepted.
A large number ofURLs occur in the training data, as citations areincluded in some of the essays.
We replace thesewith a <url> symbol to reduce noise in the vo-cabulary.In the validation, development and test data,words that do not appear in the vocabulary are re-placed with an <unk> symbol.
We record the re-placed words, so that after decoding they can bereplaced back to their original positions.
We donot perform automatic spelling correction on thetest data as a preprocessing step: The occurrenceof out of vocabulary words is small enough thatperforming spelling correction will not have a sig-nificant impact on the performance of our system.We use the NLTK interface to WordNet (Miller,1995) to find pairs of singular and plural nounsand groups of verbs that have the same base form.All verbs and non-proper nouns that occur in thelanguage model vocabulary are grouped like this.The groups are used to construct additional rulesfor noun number and verb form errors.1http://download.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz22http://code.google.com/p/gwtwiki/443 Weighted Tree-to-string TransducersTree transducers are a class of automata-theoreticmodels that perform transformations on tree struc-tures.
There is a rich theory concerning these mod-els, and tree transducers with different restrictionscan compute different classes of transformations.Algorithms for weighted variants of these trans-ducers have recently been developed (Graehl et al2008; May, 2010) and applied to syntax-based sta-tistical machine translation.Tree-to-string transducers are a class of treetransducers that generalizes synchronous context-free grammars.
These transducers can be used totransform strings into trees: For a given outputstring, the decoding problem is to find the inputtree that could be transformed into the given stringwith the highest probability.
This decoding pro-cess is referred to as backward application.
Theformulation is due to the noisy-channel model of-ten followed in statistical machine translation.3.1 DefinitionsWe need a few preliminary definitions (the no-tation of May (2010) is generally followed): Aranked alphabet ?
is a finite set of symbols, eachwhich can take a finite set of ranks.
A tree t ?T?
is denoted by ?
(t1, .
.
.
, tk), where k ?
N,t1, .
.
.
, tk ?
T?
and ?
is a node of rank k. ?kdenotes the subset of ?
of all symbols with rankk.
T?
(S) is the set of all trees in T?
?S wheresymbols from S occur only at the leaves.
We do-nate by X = {x1, x2, .
.
.}
a set of variables, andXk = {x1, .
.
.
, xk}.
With respect to Xk, a treeu ?
T?
(Xk) or a sequence u ?
(??
(Q?X))?
islinear if each element of Xk occurs at most oncein u, and nondeleting if each element ofXk occursat least once in u.Formally, a weighted extended top-downtree-to-string transducer M is a 5-tuple(Q,?,?, R,Qd) (May, 2010, chap.
2).
Q isan alphabet of states that all have rank one.
?and ?
are the ranked input and output alphabets,respectively.
Qd is the set of initial states.
Ris a finite set of rules with an associated weightfunction pi : R ?
W .
Each rule r ?
R is ofthe form q.t ?
g for q ?
Q, t ?
T?
(X) andg ?
(?
?
(Q ?X))?.
The tree t should be linearin X , and each variable in g should also be in t.We refer to q.t as the left hand side of a rule, andto g as the right hand side.
M is linear if the righthand side of each rule is linear, and nondeleting ifthe right hand side of each rule is nondeleting withrespect to Xk, the set of variables on the left handside.For a rule r : q.t ?
g and e, f ?
(?
?
(Q ?T?
))?, a derivation step e ?r f is obtained byreplacing the left-most element of e of the formq(s), where s matches t, by a transformation ofg, where each instance of a variable is replacedwith the corresponding subtree of s. The sequenced = (r1, .
.
.
, rm) is a derivation of the pair (t, s)if t ?r1 t1 ?r2 .
.
.
?rm s, where ti ?
(?
?
(Q ?
T?))?
for 1 ?
i < m. The weight of d iswt(d) = pi(r1) ?
.
.
.
?
pi(rm).The tree transducer that we use is a weightedlinear, nondeleting top-down tree-to-string trans-ducer.
The expressive power of this transducerclass is sufficient for the transformations that weneed our model to perform.
Relaxing these re-strictions will increase the decoding complexityof the transducer model significantly.
Since wework with binarized trees, no node will have a rankgreater than 2.
Our transducer only has one state,q.
Look-ahead restrictions are added to restrict thevariables on the left hand side to match specificconstituents.3.2 Probability modelA tree-to-string transducer can represent a condi-tional probability model for the output sentencesgiven the input trees, or a joint probability modelover the input trees and output strings.
We willuse a joint probability distribution for our model.Note that there is spurious ambiguity in the modelat two levels: Firstly, it is possible that there canbe different derivations for the same tree-stringpair.
However, during the application of the modelthis ambiguity occurs infrequently.
Secondly, themodel can generate different trees with the sameyield.Suppose c is a correct sentence in the set C ofall possible correct sentences, and i is the given(possibly) incorrect sentence.
Let ?
(c) representthe set of all possible parse trees of c. Then wewant to find sentencec?
= arg maxc?CP (c, i) (1)= arg maxc?C?pi??
(c)P (pi, i) (2)The rule probabilities for the joint model areconditioned on the root node of the rule left handside (and not on the entire left hand side, as would45be the case for a conditional model).
The model istrained from a set of derivations constructed fromthe training data, as will be described below.
Letf(r) be the number of times that rule r occurs inall the training derivations.
Then the probabilityestimate of a rule isp(r|root(r)) =f(r)?r?:root(r?)=root(r)f(r?
)(3)In the construction of the transducer, some rulesthat do not occur in the training derivations areadded.
In order to give non-zero weights tothese added rules, we apply Good-Turing smooth-ing (Katz, 1987).
This method has the advantageof decreasing the counts of low-frequency ruleswhose rule counts may provide unreliable prob-ability estimates.
For the rules of each of the rootnonterminals, the counts of rules with frequenciesbetween 0 and 5 are re-estimated.4 Transducer Model Formulation4.1 Word alignmentIn order to extract rules from the training datato perform transformations between incorrect andcorrect sentences, we need to construct an align-ment between words in each pair of correct andincorrect sentences.
This approach is similar toaligning words in source and target language sen-tences for statistical machine translation.
We con-struct the alignments from given sequences of editoperations in the training data.
Figure 1(a) givesan example of a parse tree for a correct phrase,aligned with a corresponding incorrect phrase.Firstly, all words in a sentence that do not oc-cur in any edits are aligned one-to-one betweenthe correct and incorrect sentence.
In the example,?that?, ?up?
and ?across?
are aligned in this way.Then, for each edit annotation, we consider the in-correct and correct phrases of that edit.
Note thatin the NUCLE annotations the incorrect phrasewill always be non-empty, but the correct phrasemay be empty.Words that occur in both the correct and incor-rect edit phrases are aligned one-to-one.
We re-strict such alignments to prevent overlapping.
Inthe example in Figure 1(a), there is an edit to re-place ?the America?
with ?America?, so the word?America?
is aligned.
Adding these alignmentsmay split a phrase into unaligned subphrases.
Ifsuch a subphrase is empty on either side, then theword(s) on the other side have to be left unaligned.
(a)SBARSVPPPNPNNPAmericaINacross@VPPRTRPupVBDsprangWHNPWDTthatthat sprung up across the America(b)(1) q.WDT (that)?
that(2) q.VP (x0:@VP x1:PP)?
q.x0 q.x1(3) q.VBD (sprang)?
sprung(4) q.SBAR (x0:WHNP x1:S)?
q.x0 q.x1(5) q.PP (x0:IN x1:NP)?
q.x0 the q.x1(6) q.NNP (America)?
AmericaFigure 1: (a) Example alignment between a cor-rect parse tree and an incorrect clause.
(b) Somerules extracted from the example.In the example, ?the?
will be unaligned.
But if thesubphrases are non-empty on both sides, then allthe words on the incorrect side are aligned to allthe words on the correct side of the subphrase.
Inmany instances, this will occur for single word re-placements.
In the example, ?sprung?
is alignedwith ?sprang?
in this manner.4.2 Rule extractionWe follow the GHKM transducer rule extractionalgorithm described in (Galley et al 2004) and(Galley et al 2006).
Given a training example(pi, i, a), where pi is the correct tree, i the incorrectsentence and a the alignment, rules are extractedfor a tree-to-string derivation of (pi, i) that is min-imally consistent with the alignment a. Countsof how many times each rule is extracted over allthe training examples are used to estimate the ruleprobabilities.
A training example is represented asa directed graph as in Figure 1(a), with the edges46going downward.For each of the nodes in pi, we compute a spanand a complement span with respect to the nodesin i.
The span of a node n is defined by the indexesof the first and last words in f that are reachablefrom n. The spans of the leaves in the tree (thewords of the correct sentence) are defined by a,and the spans of the other nodes can be computedbottom-up for each node from the spans of its childnodes.
The complement span of n is the union ofthe spans of all nodes that are neither ancestors nordescendants of n. The complement spans can becomputed top-down for each node by taking theunion of the complement span of its parent andthe spans of its siblings.
Nodes whose spans andcomplement spans do not overlap, are called fron-tier nodes.
From each frontier node, a rule can beextracted: The left hand side of the rule is a subtreerooted at n. The subtree is extracted by traversingpi top-down from n, replacing all frontier nodesreached with variables (as more rules will be ex-tracted from there).
The right hand side is formedby the words of the span of n of the incorrect side,with the span of each left hand side frontier nodereplaced by the corresponding variable.Figure 1(b) gives sample rules extracted fromthe training example in Figure 1(a).
In the exampletree, all the constituent nodes are frontier nodes,as there are no complex rewrites.
For constituentsunder which no changes are made, CFG-like rulessuch as (1) and (2) are extracted.
In the casewhere a single word is substituted (in the example,?sprang?
with ?sprung?
), a rule for this substitu-tion will be extracted (3).
If there were no align-ment between these words, the algorithm wouldhave attached the word ?sprung?
to the rule headedby SBAR (4), which would clearly not have beenlinguistically sensible.
In the case of the deletionof a word in the incorrect sentence, that word willleft be unaligned (?the?
in the example).
The rulefor this word (5) will have as head the lowest nodethat spans the words in i to the left and right ofthe unaligned word ?
in the example, the PP node.Rewrite rules for aligned words in phrase edits arealso extracted (6).4.3 Additional rulesWe add rules to the transducer that involve wordsin the vocabulary.
Whichever of these rules havenot already been extracted from the training datawill be assigned a rule count of 0, otherwise theirNon-lexicalizedq.S (x0:NP x1:VP)?
q.x0 q.x1 ?0.596q.S (x0:VP x1:VP)?
q.x0 q.x1 ?5.781q.VP (x0:VP x1:SBAR)?
q.x0 q.x1 ?3.723Word identityq.NN (work)?
work ?2.614q.VBP (work)?
work ?2.475q.DT (the)?
the ?0.183Single word substitutionq.NN (work)?
works ?4.343q.VBP (work)?
working ?4.541q.VBZ (works)?
work ?4.802q.DT (the)?
a ?3.100q.IN (of )?
from ?3.901Phrase substitutionq.NP (DT (the) NN (right))?
rights ?5.109q.VP (VBG (being) VP (VBN (researched)))?
under researching ?6.272Context-sensitive phrase substitutionq.VP (TO (to) VP (VB (work) x0:PP))?
working q.x0 ?6.272q.PP (IN (in) S (VP (VBG (generating) x0:NP)))?
to generate q.x0 ?5.480Context-sensitive word insertion and deletionq.NP (DT (the x0:NN)?
q.x0 ?2.634q.VP (VBZ (has) x0:VP)?
q.x0 ?5.202q.VP (x0:VB x1:NP)?
q.x0 into q.x1 ?5.203Table 1: Example transducer rules by type, withlog probability weights.rule counts will be left unchanged.We need to ensure that there are lexical rewriterules for all the words in our vocabulary.
For eachof the words in the vocabulary we find one or twopossible part-of-speech tags, using the NLTK POStagger.
We add word identity rules in the form ofthe examples in Table 1.
An identity rule is alsoadded for the <unk> symbol.Additional rules are added for noun number andverb form errors, using the word groups extractedfrom the vocabulary and WordNet.
These rulesperform substitutions between singular and pluralnouns (in both directions) and between verbs withthe same base forms.
Subject-verb agreement er-rors are also concerned with the verb form in thesentence, so added verb form rules will also be ap-plicable to such errors.
Examples of these singleword substitutions are given in Table 1.
Since de-terminer and preposition errors are restricted to arelatively small number of possible substitutions,we assume that all relevant rules involving theseerrors have already been extracted from the train-ing data.See Table 1 for rule examples categorized bythe type of rewrite the rule performs.
Examplesof rules for all the error types under considerationare included.
Log probability rule weights are also47given.
Phrase substitution rules can be fully lexi-cal (without variables) or context-sensitive (whenthey have variables).
Word insertions and dele-tions will always be context-sensitive.5 Transducer Model Decoding5.1 Sentence to clause splittingA challenge to our transducer model on the NU-CLE dataset is the length of sentences.
On thetraining data, 46% of sentences have length greaterthan 20 and 13% have length greater that 30.The decoding time of our model increases sharplywhen the length of sentences becomes greater than20.
For lengths greater that 30 decoding is notpractically feasible on our available computationalresources.
In order to address this problem, weperform linguistically motivated sentence splits todecrease the length of sentences passed to the de-coder.
Clauses that are still longer than 30 wordsare not decoded.
Decoding was performed on adesktop computer with 8GB RAM.
To keep theoverall decoding time reasonable, we restrict de-coding to take no more than 1 minute per sentenceon average.Sentence splitting is based on constituencyparses (obtained with the Berkeley parser) of theincorrect sentences under consideration.
Sen-tences are split at clause level, using the heuristicsdescribed below.
The goal is to extracted clausesthat have a form similar to that of full sentences.We distinguish between S-clauses, that are indi-cated by S, SINV and SQ parse tree constituents,and SBAR-clauses, indicated by SBAR or SBARQparse tree constituents.
An SBAR-clause usuallyconsists of an introductory subordinating conjunc-tion or wh-word, followed by an S-clause.We perform splits on S-clauses.
A clausal splitis performed between the phrase before the startposition of the S-clause and the phrase after thatposition.
If the parse tree node of the S-clauseis the child of an SBAR-clause node, the split isperformed between the phrase before the startingposition of the SBAR-clause, and the phrase afterthe start of the S-clause.
The introductory words inthe SBAR-clause are excluded from the extractedclauses.Splits are also performed between some phrasesseparated by a coordinating conjunction, which isindicated by a CC tag in the parse tree.
Such a splitis performed only if the CC node is a child of an S-clause node.
The phrase before the conjunction issplit from the phrase after the conjunction, whilethe conjunction itself is excluded.After decoding and reranking has been per-formed, the clauses are recombined to reconstructthe original sentences.
For each clause the highest-scoring correct clause is chosen.
Finally, the origi-nal case of all the words in the sentence is restored,as all words were lowercased in the model.5.2 k-Best decodingWhen performing decoding with the transducermodel, we need to find the highest-scoring candi-date correct sentences, so that we can in turn findthe best sentence according to the overall model.We found that a good trade-off between speed andaccuracy is to find a list of trees of the 1000-best derivations for a given (incorrect) sentence.The weights of different derivations for which theparse trees have the same yields, are summed tofind weights for each of the hypothesis sentences.Note that this is an approximation of the summa-tion in equation (2), which is taken over all parsetrees with the same yield.In our implementation the weighted tree trans-ducer package Tiburon (May and Knight, 2006)is used.
Tiburon implements generic operationson regular tree grammars, tree-to-tree and tree-to-string transducers.
We use Tiburon to perform de-coding in our model, using its implementation ofbackwards application and k-best decoding.The decoding algorithm implemented byTiburon is based on a weighted version of theEarley parsing algorithm (May, 2010, chap.
4).Empirically, large rules have a detrimental impacton the decoding speed of the algorithm.
Toaddress this problem, we extract rules frombinarized parse trees, which results in smallerrules than using non-binarized parse trees.
InFigure 1(a), the node @VP indicates that a bi-narization has been performed on the subtreeVP (VBD PRT PP).
All remaining rules thathave more that four variables are removed.As the search space of the model is large, weneed to apply some heuristic pruning.
Followingpractices used in parsing models such as Huangand Chiang (2005), beam search is performed.The cell limit ?, the maximum number of hypothe-ses that can be kept at a state in the search process,is set to 30.
The beam width ?
is set to 10?4.
Thismeans that if a hypothesis score is worse than ?times the score of the best partial hypothesis found48up to a specific point in the model, the hypothesisis discarded.
?
and ?
were set to make decodingfeasible on available computational resources.The heuristic pruning may undermine some ofthe advantages our model might have in takingwhole sentence analyses into account to generateerror corrections.
However, we find that despitethis, the model is still able to generate hypothesiscorrections that take non-local dependencies intoconsideration.6 Language Model RerankingAlthough the transducer model defines a jointprobability distribution and is therefore sufficientto find corrections for given sentences, incorpo-rating an n-gram language model in our systemsignificantly increases its performance.
The mainreason for this is that the generative transducermodel alone does not have enough discriminativepower to distinguish between well-formed and un-grammatical sentences.6.1 EvaluationThe standard evaluation metric used for grammat-ical error correction is precision, recall and F1score.
Changes made to a given incorrect sentenceare represented by edits.
For a sample sentence,the sufficient statistics for this evaluation metric isthe 3-tuple (#correct system edits, #system edits,#gold standard edits).
This can be summed overall the examples being evaluated, and the preci-sion, recall and F1 scores can be computed fromthat.The shared task uses the M2 scorer, as de-scribed by Dahlmeier and Ng (2012).
Given theoriginal and system sentences, possible systemedit sequences are represented with a lattice.
Theedit sequence that is the best match with the goldstandard edit sequence is chosen to compute theedit scores.6.2 RerankingDuring decoding we compute the language modelscore for each of the hypothesis sentences gener-ated by our transducer model for a given incor-rect sentence.
The log probability scores of thetransducer and language models are normalizedby the length of the incorrect sentence.
In orderto weigh these two scores, the transducer score iskept fixed, and the language model score is multi-plied by a weight ?.
For a given incorrect sentenceData set Precision Recall F1 scoreValidation 0.065 0.153 0.092Development 0.079 0.149 0.103Test (original) 0.2700 0.1333 0.1785Test (revised) 0.3712 0.1891 0.2505Table 2: Model resultsi and a generated set of hypothesis correct sen-tences H(i), we want to findc?
= arg maxc?H(i)[TT (c, i) + ?
?
LM(c)] (4)where TT (c, i) gives the tree transducer scoreand LM(c) gives the language model score.
Theparameter ?
is set to maximize the F1 score of themodel on a validation set.
Let I be the set of in-correct sentences in this set.
Then we want to find??
= arg max?F1[?i?Iedits(c?, i, g(i))] (5)where c?
is given by (4) and edits is the sufficientstatistics for the F1 score of c?
for the incorrect sen-tence i and gold standard edits g(i).7 ResultsWe now present results of the model on our val-idation and development sets, as well as on theofficial test set.
A useful measure to analyze theperformance of our model is to perform oraclereranking on the hypothesis sets generated by thetransducer model.
For each sentence, the oraclepicks the hypothesis that will contribute to the bestpossible F1 score.
We are especially interested inhow frequently the correct sentence is among thehypothesis sentences ?
this is called the hypothesiscoverage.7.1 Development setsOn the development set, only 21% of clauses areannotated with corrections.
For clauses that haveno annotations, the hypothesis coverage is 99%,while for clauses that have annotations the hypoth-esis coverage is 49%.
The oracle obtains a 0.64 F1score.We tune the value of ?
on the validation set tomaximize the F1 score.
The best F1 score is ob-tained with ?
= 1.6949.
The system results on thevalidation set and the development set with this ?are given in Table 2.
It was found that a strong49Error type Development Testrecall recallNoun number 0.2231 0.1818Verb form 0.1839 0.1475Article or determiner 0.1564 0.1261Preposition 0.1655 0.0932Subject-verb agreement 0.0957 0.1048Table 3: Recall for each error type, on the devel-opment set and original test set.weight on the language model (a relatively large?)
increases the recall of the model.A breakdown of the recall for each error type isgiven in Table 3.
On the development set, the bestrecall is obtained for noun number errors, and theworst for subject-verb agreement errors.
A reasonfor the relatively low performance on agreementerrors may be due to the constituency parse treerepresentation used.
In a clause, the subject nounphrase and the predicate verb phrase, whose headverb must agree with the subject, are in differentsubtrees.
This increases the difficulty in modellingthe dependency between the subject and the verb.7.2 Test setThe test set released for this shared task consists of1381 sentences, which we split into 2247 clausesusing the heuristic described above.
The distribu-tion of sentence lengths is very similar to that ofthe training data.
The number of out of vocabu-lary words is quite small at 0.03%.
The set doesnot include any URLs, and the general impressionwas that it is less noisy than the training data.The system result on the test set is given in Ta-ble 2.
Scores for both the original and revised testdata annotations are given.
We submitted plau-sible corrections suggested by our system for thegold standard revision.
This contributed to a sig-nificant increase in our model score on the revisedannotations.
The model recall on the test set issimilar to that of the development on most errortypes, though the preposition error recall is signifi-cantly lower and the subject-verb agreement recallis slightly higher.
This may indicate that prepo-sition error correction rules in the model does notgeneralize well enough.The precision of our model is significantly bet-ter on the test set than on the development set.
Thiscan be explained by differences in the character-istics of the test set.
The relative occurrence ofannotated errors is much higher in the test set thanin the development set: 46% of clauses have cor-rections.
It has been found previously that a lowfrequency of errors increase the difficulty of thecorrection task (Dahlmeier and Ng, 2011).
This iscaused especially by an increase in the number ofsystem edits suggested for sentences that shouldnot be changed.
Our oracle found that for sen-tences that should not be changed, 100% of thecorrect unchanged hypotheses were generated bythe tree transducer, while for sentences that shouldbe changed, 50% of hypothesis sets contained thecorrect result.
The oracle obtains a 0.75 F1 score.The precision of the oracle model increases sig-nificantly, from 0.65 to 0.95.
Varying the choiceof ?
controls the trade-off between precision andrecall better on the test set than on the validationset.
These results indicate that our model is moresuited for data with the characteristics of the testset than for data similar to the development sets.8 ConclusionWe presented a novel approach to grammatical er-ror correction based on tree transducers, obtainingpromising results.
One of the weaknesses of ourmodel is handling insertions and deletions.
Themodel performs too many unnecessary deletions,especially removing content words or non-articledeterminers.
It also has difficulty in finding editswhere insertions such as article insertions shouldbe performed.For future work, ways of constructing betterrule sets for the transducer should be investigatedto take more dependencies into consideration andto improve probability estimates.
Techniques toimprove the runtime of the decoding algorithmwhile minimizing the loss in accuracy caused byheuristic pruning should be considered.
Alterna-tive approaches to reranking could also be investi-gated.
Including additional features may increasethe ability of the model to discriminate betweengrammatical and ungrammatical sentences.AcknowledgementThe financial support of MIH is acknowledged.50ReferencesSteven Bird, Ewan Klein, and Edward Loper.2009.
Natural Language Processing with Python.O?Reilly Media.Chris Brockett, William B. Dolan, and Michael Ga-mon.
2006.
Correcting ESL errors using phrasalSMT techniques.
In Proceedings of ACL, pages249?256.Daniel Dahlmeier and Hwee Tou Ng.
2011.
Grammat-ical error correction with alternating structure opti-mization.
In Proceedings of ACL, pages 915?923.Daniel Dahlmeier and Hwee Tou Ng.
2012.
Betterevaluation for grammatical error correction.
In Pro-ceedings of HTL-NAACL, pages 568?572.Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.2013.
Building a large annotated corpus of learnerEnglish: The NUS corpus of learner English.
InProceedings of the 8th Workshop on Innovative Useof NLP for Building Educational Applications, At-lanta, Georgia, USA.Robert Dale and Adam Kilgarriff.
2011.
Helping ourown: The HOO 2011 pilot shared task.
In Proceed-ings of the 13th EuropeanWorkshop on Natural Lan-guage Generation, pages 242?249, Nancy, France.Robert Dale, Ilya Anisimoff, and George Narroway.2012.
HOO 2012: A report on the preposition anddeterminer error correction shared task.
In Proceed-ings of the 7th Workshop on the Innovative Use ofNLP for Building Educational Applications, pages54?62, Montreal, Canada.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proceedings of HLT-NAACL, pages 273?280.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Pro-ceedings of ACL, pages 961?968.Jonathan Graehl, Kevin Knight, and Jonathan May.2008.
Training tree transducers.
ComputationalLinguistics, 34(3):391?427.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of the Ninth InternationalWorkshop on Parsing Technologies, pages 53?64.Slava M. Katz.
1987.
Estimation of probabilities fromsparse data for the language model of a speech rec-ognizer.
IEEE Transactions on Acoustics, Speech,and Signal Processing, 35(3):400?401.Claudia Leacock, Martin Chodorow, Michael Gamon,and Joel R. Tetreault.
2010.
Automated Grammat-ical Error Detection for Language Learners.
Syn-thesis Lectures on Human Language Technologies.Morgan & Claypool Publishers.John Lee and Stephanie Seneff.
2006.
Automaticgrammar correction for second-language learners.In Proceedings of Interspeech, pages 1978?1981.Jonathan May and Kevin Knight.
2006.
Tiburon: Aweighted tree automata toolkit.
In CIAA, volume4094 of Lecture Notes in Computer Science, pages102?113.
Springer.Jonathan May.
2010.
Weighted Tree Automata andTransducers for Syntactic Natural Language Pro-cessing.
Ph.D. thesis, University of Southern Cal-ifornia.George A. Miller.
1995.
WordNet: A lexicaldatabase for English.
Communications of the ACM,38(11):39?41.Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, ChristianHadiwinoto, and Joel Tetreault.
2013.
The CoNLL-2013 shared task on grammatical error correction.In Proceedings of CoNLL.Y.
Albert Park and Roger Levy.
2011.
Automatedwhole sentence grammar correction using a noisychannel model.
In Proceedings of ACL, pages 934?944.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of HLT-NAACL, pages 404?411.Andreas Stolcke.
2002.
SRILM: An extensible lan-guage modeling toolkit.
In Proceedings of ICSLP,pages 901?904.Jenine Turner and Eugene Charniak.
2007.
Languagemodeling for determiner selection.
In Proceedingsof HTL-NAACL, pages 177?180.51
