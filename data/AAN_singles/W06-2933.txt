Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),pages 221?225, New York City, June 2006. c?2006 Association for Computational LinguisticsLabeled Pseudo-Projective Dependency Parsingwith Support Vector MachinesJoakim NivreJohan HallJens NilssonSchool of Mathematicsand Systems EngineeringVa?xjo?
University35195 Va?xjo?, Sweden{nivre,jha,jni}@msi.vxu.seGu?ls?en Eryig?itDepartment ofComputer EngineeringIstanbul Technical University34469 Istanbul, Turkeygulsen@cs.itu.edu.trSvetoslav MarinovSchool of Humanitiesand InformaticsUniversity of Sko?vdeBox 40854128 Sko?vde, Swedensvetoslav.marinov@his.seAbstractWe use SVM classifiers to predict the nextaction of a deterministic parser that buildslabeled projective dependency graphs inan incremental fashion.
Non-projectivedependencies are captured indirectly byprojectivizing the training data for theclassifiers and applying an inverse trans-formation to the output of the parser.
Wepresent evaluation results and an erroranalysis focusing on Swedish and Turkish.1 IntroductionThe CoNLL-X shared task consists in parsing textsin multiple languages using a single dependencyparser that has the capacity to learn from treebankdata.
Our methodology for performing this task isbased on four essential components:?
A deterministic algorithm for building labeledprojective dependency graphs (Nivre, 2006).?
History-based feature models for predicting thenext parser action (Black et al, 1992).?
Support vector machines for mapping historiesto parser actions (Kudo and Matsumoto, 2002).?
Graph transformations for recovering non-projective structures (Nivre and Nilsson, 2005).All experiments have been performed using Malt-Parser (Nivre et al, 2006), version 0.4, which ismade available together with the suite of programsused for pre- and post-processing.11www.msi.vxu.se/users/nivre/research/MaltParser.html2 Parsing Methodology2.1 Parsing AlgorithmThe parsing algorithm used for all languages is thedeterministic algorithm first proposed for unlabeleddependency parsing by Nivre (2003) and extendedto labeled dependency parsing by Nivre et al (2004).The algorithm builds a labeled dependency graph inone left-to-right pass over the input, using a stackto store partially processed tokens and adding arcsusing four elementary actions (where top is the tokenon top of the stack and next is the next token):?
SHIFT: Push next onto the stack.?
REDUCE: Pop the stack.?
RIGHT-ARC(r): Add an arc labeled r from topto next; push next onto the stack.?
LEFT-ARC(r): Add an arc labeled r from nextto top; pop the stack.Although the parser only derives projective graphs,the fact that graphs are labeled allows non-projectivedependencies to be captured using the pseudo-projective approach of Nivre and Nilsson (2005) .Another limitation of the parsing algorithm is thatit does not assign dependency labels to roots, i.e., totokens having HEAD=0.
To overcome this problem,we have implemented a variant of the algorithm thatstarts by pushing an artificial root token with ID=0onto the stack.
Tokens having HEAD=0 can nowbe attached to the artificial root in a RIGHT-ARC(r)action, which means that they can be assigned anylabel.
Since this variant of the algorithm increasesthe overall nondeterminism, it has only been usedfor the data sets that include informative root labels(Arabic, Czech, Portuguese, Slovene).221FO L C P FE DS: top + + + + + +S: top?1 +I: next + + + + +I: next+1 + +I: next+2 +I: next+3 +G: head of top +G: leftmost dep of top +G: rightmost dep of top +G: leftmost dep of next +Table 1: Base model; S: stack, I: input, G: graph;FO: FORM, L: LEMMA , C: CPOS, P: POS,FE: FEATS, D: DEPREL2.2 History-Based Feature ModelsHistory-based parsing models rely on features of thederivation history to predict the next parser action.The features used in our system are all symbolicand extracted from the following fields of the datarepresentation: FORM, LEMMA, CPOSTAG, POSTAG,FEATS, and DEPREL.
Features of the type DEPRELhave a special status in that they are extracted duringparsing from the partially built dependency graphand may therefore contain errors, whereas all theother features have gold standard values during bothtraining and parsing.2Based on previous research, we defined a basemodel to be used as a starting point for language-specific feature selection.
The features of this modelare shown in Table 1, where rows denote tokens ina parser configuration (defined relative to the stack,the remaining input, and the partially built depen-dency graph), and where columns correspond to datafields.
The base model contains twenty features, butnote that the fields LEMMA, CPOS and FEATS are notavailable for all languages.2.3 Support Vector MachinesWe use support vector machines3 to predict the nextparser action from a feature vector representing thehistory.
More specifically, we use LIBSVM (Changand Lin, 2001) with a quadratic kernel K(xi, xj) =(?xTi xj +r)2 and the built-in one-versus-all strategyfor multi-class classification.
Symbolic features are2The fields PHEAD and PDEPREL have not been used at all,since we rely on pseudo-projective parsing for the treatment ofnon-projective structures.3We also ran preliminary experiments with memory-basedlearning but found that this gave consistently lower accuracy.converted to numerical features using the standardtechnique of binarization, and we split values of theFEATS field into its atomic components.4For some languages, we divide the training datainto smaller sets, based on some feature s (normallythe CPOS or POS of the next input token), which mayreduce training times without a significant loss inaccuracy (Yamada and Matsumoto, 2003).
To avoidtoo small training sets, we pool together categoriesthat have a frequency below a certain threshold t.2.4 Pseudo-Projective ParsingPseudo-projective parsing was proposed by Nivreand Nilsson (2005) as a way of dealing withnon-projective structures in a projective data-drivenparser.
We projectivize training data by a minimaltransformation, lifting non-projective arcs one stepat a time, and extending the arc label of lifted arcsusing the encoding scheme called HEAD by Nivreand Nilsson (2005), which means that a lifted arc isassigned the label r?h, where r is the original labeland h is the label of the original head in the non-projective dependency graph.Non-projective dependencies can be recovered byapplying an inverse transformation to the output ofthe parser, using a left-to-right, top-down, breadth-first search, guided by the extended arc labels r?hassigned by the parser.
This technique has been usedwithout exception for all languages.3 ExperimentsSince the projective parsing algorithm and graphtransformation techniques are the same for all datasets, our optimization efforts have been focused onfeature selection, using a combination of backwardand forward selection starting from the base modeldescribed in section 2.2, and parameter optimizationfor the SVM learner, using grid search for an optimalcombination of the kernel parameters ?
and r, thepenalty parameter C and the termination criterion ?,as well as the splitting feature s and the frequencythreshold t. Feature selection and parameter opti-mization have to some extent been interleaved, butthe amount of work done varies between languages.4Preliminary experiments showed a slight improvement formost languages when splitting the FEATS values, as opposed totaking every combination of atomic values as a distinct value.222Ara Bul Chi Cze Dan Dut Ger Jap Por Slo Spa Swe Tur TotalLAS 66.71 87.41 86.92 78.42 84.77 78.59 85.82 91.65 87.60 70.30 81.29 84.58 65.68 80.19UAS 77.52 91.72 90.54 84.80 89.80 81.35 88.76 93.10 91.22 78.72 84.67 89.50 75.82 85.48LAcc 80.34 90.44 89.01 85.40 89.16 83.69 91.03 94.34 91.54 80.54 90.06 87.39 78.49 86.75Table 2: Evaluation on final test set; LAS = labeled attachment score, UAS = unlabeled attachment score,LAcc = label accuracy score; total score excluding BulgarianThe main optimization criterion has been labeledattachment score on held-out data, using ten-foldcross-validation for all data sets with 100k tokensor less, and an 80-20 split into training and devtestsets for larger datasets.
The number of features inthe optimized models varies from 16 (Turkish) to 30(Spanish), but the models use all fields available fora given language, except that FORM is not used forTurkish (only LEMMA).
The SVM parameters fallinto the following ranges: ?
: 0.12?0.20; r: 0.0?0.6;C: 0.1?0.7; ?
: 0.01?1.0.
Data has been split on thePOS of the next input token for Czech (t = 200),German (t = 1000), and Spanish (t = 1000), andon the CPOS of the next input token for Bulgarian(t = 1000), Slovene (t = 600), and Turkish (t = 100).
(For the remaining languages, the training data hasnot been split at all.
)5 A dry run at the end of thedevelopment phase gave a labeled attachment scoreof 80.46 over the twelve required languages.Table 2 shows final test results for each languageand for the twelve required languages together.
Thetotal score is only 0.27 percentage points below thescore from the dry run, which seems to indicate thatmodels have not been overfitted to the training data.The labeled attachment score varies from 91.65 to65.68 but is above average for all languages.
Wehave the best reported score for Japanese, Swedishand Turkish, and the score for Arabic, Danish,Dutch, Portuguese, Spanish, and overall does notdiffer significantly from the best one.
The unlabeledscore is less competitive, with only Turkish havingthe highest reported score, which indirectly indicatesthat the integration of labels into the parsing processprimarily benefits labeled accuracy.4 Error AnalysisAn overall error analysis is beyond the scope of thispaper, but we will offer a few general observations5Detailed specifications of the feature models and learningalgorithm parameters can be found on the MaltParser web page.before we turn to Swedish and Turkish, focusing onrecall and precision of root nodes, as a reflection ofglobal syntactic structure, and on attachment scoreas a function of arc length.
If we start by consideringlanguages with a labeled attachment score of 85% orhigher, they are characterized by high precision andrecall for root nodes, typically 95/90, and by a grace-ful degradation of attachment score as arcs growlonger, typically 95?90?85, for arcs of length 1, 2and 3?6.
Typical examples are Bulgarian (Simovet al, 2005; Simov and Osenova, 2003), Chinese(Chen et al, 2003), Danish (Kromann, 2003), andSwedish (Nilsson et al, 2005).
Japanese (Kawataand Bartels, 2000), despite a very high accuracy, isdifferent in that attachment score drops from 98%to 85%, as we go from length 1 to 2, which mayhave something to do with the data consisting oftranscribed speech with very short utterances.A second observation is that a high proportion ofnon-projective structures leads to fragmentation inthe parser output, reflected in lower precision forroots.
This is noticeable for German (Brants et al,2002) and Portuguese (Afonso et al, 2002), whichstill have high overall accuracy thanks to very highattachment scores, but much more conspicuous forCzech (Bo?hmova?
et al, 2003), Dutch (van der Beeket al, 2002) and Slovene (Dz?eroski et al, 2006),where root precision drops more drastically to about69%, 71% and 41%, respectively, and root recall isalso affected negatively.
On the other hand, all threelanguages behave like high-accuracy languages withrespect to attachment score.
A very similar patternis found for Spanish (Civit Torruella and Mart??
An-ton?
?n, 2002), although this cannot be explained bya high proportion of non-projective structures.
Onepossible explanation in this case may be the fact thatdependency graphs in the Spanish data are sparselylabeled, which may cause problem for a parser thatrelies on dependency labels as features.The results for Arabic (Hajic?
et al, 2004; Smrz?et al, 2002) are characterized by low root accuracy223as well as a rapid degradation of attachment scorewith arc length (from about 93% for length 1 to 67%for length 2).
By contrast, Turkish (Oflazer et al,2003; Atalay et al, 2003) exhibits high root accu-racy but consistently low attachment scores (about88% for length 1 and 68% for length 2).
It is note-worthy that Arabic and Turkish, being ?typologicaloutliers?, show patterns that are different both fromeach other and from most of the other languages.4.1 SwedishA more fine-grained analysis of the Swedish resultsreveals a high accuracy for function words, whichis compatible with previous studies (Nivre, 2006).Thus, the labeled F-score is 100% for infinitivemarkers (IM) and subordinating conjunctions (UK),and above 95% for determiners (DT).
In addition,subjects (SS) have a score above 90%.
In all thesecases, the dependent has a configurationally defined(but not fixed) position with respect to its head.Arguments of the verb, such as objects (DO, IO)and predicative complements (SP), have a slightlylower accuracy (about 85% labeled F-score), whichis due to the fact that they ?compete?
in the samestructural positions, whereas adverbials (labels thatend in A) have even lower scores (often below 70%).The latter result must be related both to the relativelyfine-grained inventory of dependency labels for ad-verbials and to attachment ambiguities that involveprepositional phrases.
The importance of this kindof ambiguity is reflected also in the drastic differ-ence in accuracy between noun pre-modifiers (AT)(F > 97%) and noun post-modifiers (ET) (F ?
75%).Finally, it is worth noting that coordination, whichis often problematic in parsing, has high accuracy.The Swedish treebank annotation treats the secondconjunct as a dependent of the first conjunct and asthe head of the coordinator, which seems to facil-itate parsing.6 The attachment of the second con-junct to the first (CC) has a labeled F-score above80%, while the attachment of the coordinator to thesecond conjunct (++) has a score well above 90%.4.2 TurkishIn Turkish, very essential syntactic information iscontained in the rich morphological structure, where6The analysis is reminiscent of the treatment of coordinationin the Collins parser (Collins, 1999).concatenated suffixes carry information that in otherlanguages may be expressed by separate words.
TheTurkish treebank therefore divides word forms intosmaller units, called inflectional groups (IGs), andthe task of the parser is to construct dependenciesbetween IGs, not (primarily) between word forms(Eryig?it and Oflazer, 2006).
It is then importantto remember that an unlabeled attachment scoreof 75.8% corresponds to a word-to-word score of82.7%, which puts Turkish on a par with languageslike Czech, Dutch and Spanish.
Moreover, whenwe break down the results according to whether thehead of a dependency is part of a multiple-IG wordor a complete (single-IG) word, we observe a highlysignificant difference in accuracy, with only 53.2%unlabeled attachment score for multiple-IG headsversus 83.7% for single-IG heads.
It is hard to sayat this stage whether this means that our methodsare ill-suited for IG-based parsing, or whether it ismainly a case of sparse data for multiple-IG words.When we break down the results by dependencytype, we can distinguish three main groups.
The firstconsists of determiners and particles, which havean unlabeled attachment score over 80% and whichare found within a distance of 1?1.4 IGs from theirhead.7 The second group mainly contains subjects,objects and different kinds of adjuncts, with a scorein the range 60?80% and a distance of 1.8?5.2 IGs totheir head.
In this group, information about case andpossessive features of nominals is important, whichis found in the FEATS field in the data representation.We believe that one important explanation for ourrelatively good results for Turkish is that we breakdown the FEATS information into its atomic com-ponents, independently of POS and CPOS tags, andlet the classifier decide which one to use in a givensituation.
The third group contains distant depen-dencies, such as sentence modifiers, vocatives andappositions, which have a much lower accuracy.5 ConclusionThe evaluation shows that labeled pseudo-projectivedependency parsing, using a deterministic parsingalgorithm and SVM classifiers, gives competitiveparsing accuracy for all languages involved in the7Given that the average IG count of a word is 1.26 in thetreebank, this means that they are normally adjacent to the headword.224shared task, although the level of accuracy variesconsiderably between languages.
To analyze indepth the factors determining this variation, and toimprove our parsing methods accordingly to meetthe challenges posed by the linguistic diversity, willbe an important research goal for years to come.AcknowledgmentsWe are grateful for the support from T ?UB?ITAK(The Scientific and Technical Research Council ofTurkey) and the Swedish Research Council.
We alsowant to thank Atanas Chanev for assistance withSlovene, the organizers of the shared task for alltheir hard work, and the creators of the treebanksfor making the data available.ReferencesA.
Abeille?, editor.
2003.
Treebanks: Building and UsingParsed Corpora, volume 20 of Text, Speech and LanguageTechnology.
Kluwer Academic Publishers, Dordrecht.S.
Afonso, E. Bick, R. Haber, and D. Santos.
2002.
?Florestasinta?(c)tica?
: a treebank for Portuguese.
In Proc.
of LREC-2002, pages 1698?1703.N.
B. Atalay, K. Oflazer, and B.
Say.
2003.
The annotationprocess in the Turkish treebank.
In Proc.
of LINC-2003.E.
Black, F. Jelinek, J. D. Lafferty, D. M. Magerman, R. L. Mer-cer, and S. Roukos.
1992.
Towards history-based grammars:Using richer models for probabilistic parsing.
In Proc.
of the5th DARPA Speech and Natural Language Workshop, pages31?37.A.
Bo?hmova?, J.
Hajic?, E.
Hajic?ova?, and B. Hladka?.
2003.
ThePDT: a 3-level annotation scenario.
In Abeille?
(Abeille?,2003), chapter 7.S.
Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002.The TIGER treebank.
In Proc.
of TLT-2002.C.-C. Chang and C.-J.
Lin, 2001.
LIBSVM: A Libraryfor Support Vector Machines.
Software available athttp://www.csie.ntu.edu.tw/ cjlin/libsvm.K.
Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang, andZ.
Gao.
2003.
Sinica treebank: Design criteria, representa-tional issues and implementation.
In Abeille?
(Abeille?, 2003),chapter 13, pages 231?248.M.
Civit Torruella and Ma A.
Mart??
Anton??n.
2002.
Designprinciples for a Spanish treebank.
In Proc.
of TLT-2002.M.
Collins.
1999.
Head-Driven Statistical Models for NaturalLanguage Parsing.
Ph.D. thesis, University of Pennsylvania.S.
Dz?eroski, T. Erjavec, N. Ledinek, P. Pajas, Z.
?Zabokrtsky, andA.
?Zele.
2006.
Towards a Slovene dependency treebank.
InProc.
of LREC-2006.G.
Eryig?it and K. Oflazer.
2006.
Statistical dependency parsingof Turkish.
In Proc.
of EACL-2006.J.
Hajic?, O.
Smrz?, P. Zema?nek, J.
?Snaidauf, and E. Bes?ka.
2004.Prague Arabic dependency treebank: Development in dataand tools.
In Proc.
of NEMLAR-2004, pages 110?117.Y.
Kawata and J. Bartels.
2000.
Stylebook for the Japanesetreebank in VERBMOBIL.
Verbmobil-Report 240, Seminarfu?r Sprachwissenschaft, Universita?t Tu?bingen.M.
T. Kromann.
2003.
The Danish dependency treebank andthe underlying linguistic theory.
In Proc.
of TLT-2003.T.
Kudo and Y. Matsumoto.
2002.
Japanese dependency anal-ysis using cascaded chunking.
In Proc.
of CoNLL-2002,pages 63?69.J.
Nilsson, J.
Hall, and J. Nivre.
2005.
MAMBA meets TIGER:Reconstructing a Swedish treebank from antiquity.
In Proc.of the NODALIDA Special Session on Treebanks.J.
Nivre and J. Nilsson.
2005.
Pseudo-projective dependencyparsing.
In Proc.
of ACL-2005, pages 99?106.J.
Nivre, J.
Hall, and J. Nilsson.
2004.
Memory-based depen-dency parsing.
In Proc.
CoNLL-2004, pages 49?56.J.
Nivre, J.
Hall, and J. Nilsson.
2006.
MaltParser: A data-driven parser-generator for dependency parsing.
In Proc.
ofLREC-2006.J.
Nivre.
2003.
An efficient algorithm for projective depen-dency parsing.
In Proc.
of IWPT-2003, pages 149?160.J.
Nivre.
2006.
Inductive Dependency Parsing.
Springer.K.
Oflazer, B.
Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003.Building a Turkish treebank.
In Abeille?
(Abeille?, 2003),chapter 15.K.
Simov and P. Osenova.
2003.
Practical annotation schemefor an HPSG treebank of Bulgarian.
In Proc.
of LINC-2003,pages 17?24.K.
Simov, P. Osenova, A. Simov, and M. Kouylekov.
2005.Design and implementation of the Bulgarian HPSG-basedtreebank.
In Journal of Research on Language and Com-putation ?
Special Issue, pages 495?522.
Kluwer AcademicPublishers.O.
Smrz?, J.
?Snaidauf, and P. Zema?nek.
2002.
Prague depen-dency treebank for Arabic: Multi-level annotation of Arabiccorpus.
In Proc.
of the Intern.
Symposium on Processing ofArabic, pages 147?155.L.
van der Beek, G. Bouma, R. Malouf, and G. van Noord.2002.
The Alpino dependency treebank.
In ComputationalLinguistics in the Netherlands (CLIN).H.
Yamada and Y. Matsumoto.
2003.
Statistical dependencyanalysis with support vector machines.
In Proc.
of IWPT-2003, pages 195?206.225
