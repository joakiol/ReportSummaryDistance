Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 136?143,Sydney, July 2006. c?2006 Association for Computational LinguisticsUsing Machine-Learning to Assign Function Labels to ParserOutput for SpanishGrzegorz Chrupa?a1 and Josef van Genabith1,21National Center for Language TechnologyDublin City UniversityGlasnevin, Dublin 9, Ireland2IBM Dublin Center for Advanced Studiesgrzegorz.chrupala@computing.dcu.iejosef@computing.dcu.ieAbstractData-driven grammatical function tag as-signment has been studied for English us-ing the Penn-II Treebank data.
In this pa-per we address the question of whethersuch methods can be applied success-fully to other languages and treebank re-sources.
In addition to tag assignment ac-curacy and f-scores we also present re-sults of a task-based evaluation.
We usethree machine-learning methods to assignCast3LB function tags to sentences parsedwith Bikel?s parser trained on the Cast3LBtreebank.
The best performing method,SVM, achieves an f-score of 86.87% ongold-standard trees and 66.67% on parseroutput - a statistically significant improve-ment of 6.74% over the baseline.
In atask-based evaluation we generate LFGfunctional-structures from the function-tag-enriched trees.
On this task we achivean f-score of 75.67%, a statistically signif-icant 3.4% improvement over the baseline.1 IntroductionThe research presented in this paper formspart of an ongoing effort to develop methodsto induce wide-coverage multilingual Lexical-Functional Grammar (LFG) (Bresnan, 2001) re-sources from treebanks by means of automaticallyassociating LFG f-structure information with con-stituency trees produced by probabilistic parsers(Cahill et al, 2004).
Inducing deep syntactic anal-yses from treebank data avoids the cost and timeinvolved in manually creating wide-coverage re-sources.Lexical Functional Grammar f-structures pro-vide a level of syntactic representation based onthe notion of grammatical functions (e.g.
Sub-ject, Object, Oblique, Adjunct etc.).
This levelis more abstract and cross-linguistically more uni-form than constituency trees.
F-structures also in-clude explicit encodings of phenomena such ascontrol and raising, pro-drop or long distance de-pendencies.
Those characteristics make this levela suitable representation for many NLP applica-tions such as transfer-based Machine Translationor Question Answering.The f-structure annotation algorithm used forinducing LFG resources from the Penn-II treebankfor English (Cahill et al, 2004) uses configura-tional, categorial, function tag and trace informa-tion.
In contrast to English, in many other lan-guages configurational information is not a goodpredictor for LFG grammatical function assign-ment.
For such languages the function tags in-cluded in many treebanks are a much more impor-tant source of information for the LFG annotationalgorithm than Penn-II tags are for English.Cast3LB (Civit and Mart?
?, 2004), the Spanishtreebank used in the current research, containscomprehensive grammatical function annotation.In the present paper we use a machine-learning ap-proach in order to add Cast3LB function tags tonodes of basic constituent trees output by a prob-abilistic parser trained on Cast3LB.
To our knowl-edge, this paper is the first to describe applyinga data-driven approach to function-tag assignmentto a language other than English.Our method statistically significantly outper-forms the previously used approach which reliedexclusively on the parser to produce trees withCast3LB tags (O?Donovan et al, 2005).
Addi-tionally, we perform a task-driven evaluation ofour Cast3LB tag assignment method by using thetag-enriched trees as input to the Spanish LFG f-structure annotation algorithm and evaluating thequality of the resulting f-structures.Section 2 describes the Spanish Cast3LB tree-bank.
In Section 3 we describe previous researchin LFG induction for English and Spanish as well136as research on data-driven function tag assign-ment to parsed text in English.
Section 4 providesthe details of our approach to the Cast3LB func-tion tag assignment task.
In Sections 5 and 6 wepresent evaluation results for our method.
In Sec-tion 7 we present the error analysis of the results.Finally, in Section 8 we conclude and discuss ideasfor further research.2 The Spanish TreebankAs input to our LFG annotation algorithm we usethe output of Bikel?s parser (Bikel, 2002) trainedon the Cast3LB treebank (Civit and Mart?
?, 2004).Cast3LB contains around 3,500 constituency trees(100,000 words) taken from different genres ofEuropean and Latin American Spanish.
The POStags used in Cast3LB encode morphological infor-mation in addition to Part-of-Speech information.Due to the relatively flexible order of main sen-tence constituents in Spanish, Cast3LB uses a flat,multiply-branching structure for the S node.
Thereis no VP node, but rather all complements and ad-juncts depending on a verb are sisters to the gv(Verb Group) node containing this verb.
An exam-ple sentence (with the corresponding f-structure)is shown in Figure 1.Tree nodes are additionally labelled with gram-matical function tags.
Table 1 provides a list offunction tags with short explanations.
Civit (2004)provides Cast3LB function tag guidelines.Functional tags carry some of the informationthat would be encoded in terms of tree configura-tions in languages with stricter constituent orderconstraints than Spanish.3 Previous Work3.1 LFG AnnotationA methodology for automatically obtaining LFGf-structures from trees output by probabilisticparsers trained on the Penn-II treebank has beendescribed by Cahill et al (2004).
It has beenshown that the methods can be ported to other lan-guages and treebanks (Burke et al, 2004; Cahill etal., 2003), including Cast3LB (O?Donovan et al,2005).Some properties of Spanish and the encodingof syntactic information in the Cast3LB treebankmake it non-trivial to apply the method of auto-matically mapping c-structures to f-structures usedby Cahill et al (2004), which assigns grammaticalTag MeaningATR Attribute of copular verbCAG Agent of passive verbCC Compl.
of circumstanceCD Direct objectCD.Q Direct object of quantityCI Indirect objectCPRED Predicative complementCPRED.CD Predicative of Direct ObjectCPRED.SUJ Predicative of SubjectCREG Prepositional objectET Textual elementIMPERS Impersonal markerMOD Verbal modifierNEG NegationPASS Passive markerSUJ SubjectVOC VocativeTable 1: List of function tags in Cast3LB.functions to tree nodes based on their phrasal cat-egory, the category of the mother node and theirposition relative to the local head.In Spanish, the order of sentence constituentsis flexible and their position relative to the headis an imperfect predictor of grammatical function.Also, much of the information that the Penn-IITreebank encodes in terms of tree configurationsis encoded in Cast3LB in the form of functiontags.
As Cast3LB trees lack a VP node, the con-figurational information normally used in Englishto distinguish Subjects (NP which is left sister toVP) from Direct Objects (NP which is right sisterto V) is not available in Cast3LB-style trees.
Thismeans that assigning correct LFG functional an-notations to nodes in Cast3LB trees is rather dif-ficult without use of Cast3LB function tags, andthose tags are typically absent in output generatedby probabilistic parsers.In order to solve this difficulty, O?Donovan etal.
(2005) train Bikel?s parser to output complexcategory-function labels.
A complex label such assn-SUJ (an NP node tagged with the Subject gram-matical function) is treated as an atomic categoryin the training data, and is output in the trees pro-duced by the parser.
This baseline process is rep-resented in Figure 2.This approach can be problematic for two mainreasons.
Firstly, by treating complex labels asatomic categories the number of unique labels in-creases and parse quality can deteriorate due tosparse data problems.
Secondly, this approach, byrelying on the parser to assign function tags, offers137Sneg-NEGnonotgvespereexpectsn-SUJel lectorthe readersn-CDuna definicio?na definition?????????????
?PRED ?esperar?SUBJ,OBJ?
?NEG +TENSE PRESMOOD SUBJUNCTIVESUBJ[SPEC[SPEC-FORM EL]PRED ?lector?
]OBJ[SPEC[SPEC-FORM UNO]PRED ?definicio?n?]?????????????
?Figure 1: On the left flat structure of S. Cast3LB function tags are shown in bold.
On the right thecorresponding (simplified) LFG f-structure.
Translation: Let the reader not expect a definition.Figure 2: Processing architecture for the baseline.limited control over, or room for improvement in,this task.3.2 Adding Function Tags to Parser OutputThe solution we adopt instead is to add Cast3LBfunctional tags to simple constituent trees outputby the parser, as a postprocessing step.
For En-glish, such approaches have been shown to givegood results for the output of parsers trained onthe Penn-II Treebank.Blaheta and Charniak (2000) use a probabilis-tic model with feature dependencies encoded bymeans of feature trees to add Penn-II Treebankfunction tags to Charniak?s parser output.
They re-port an f-score 88.472% on original treebank treesand 87.277% on the correctly parsed subset of treenodes.Jijkoun and de Rijke (2004) describe a methodof enriching output of a parser with informationthat is included in the original Penn-II trees, suchas function tags, empty nodes and coindexations.They first transform Penn trees to a dependencyformat and then use memory-based learning toperform various graph transformations.
One of thetransformations is node relabelling, which addsfunction tags to parser output.
They report an f-score of 88.5% for the task of function tagging oncorrectly parsed constituents.4 Assigning Cast3LB Function Tags toParsed Spanish TextThe complete processing architecture of our ap-proach is depicted in Figure 3.
We describe it indetail in this and the following sections.We divided the Spanish treebank into a trainingset of 80%, a development set of 10%, and a testset of 10% of all trees.
We randomly assigned tree-bank files to these sets to ensure that different tex-tual genres are about equally represented amongthe training, development and test trees.4.1 Constituency ParsingFor constituency parsing we use Bikel?s (2002)parser for which we developed a Spanish languagepackage adapted to the Cast3LB data.
Prior toparsing, we perform one of the tree transforma-tions described by Cowan and Collins (2005), i.e.we add a CP and SBAR nodes to subordinate andrelative clauses.
This is undone in parser output.The category labels in the Spanish treebank arerather fine grained and often contain redundant in-formation.1 We preprocess the treebank and re-1For example there are several labels for Nominal Group,138Figure 3: Processing architecture for the machine-learning-based method.duce the number of category labels, only retainingdistinctions that we deem useful for our purposes.2For constituency parsing we also reduce thenumber of POS tags by including only selectedmorphological features.
Table 2 provides thelist of features included for the different parts ofspeech.
In our experiments we use gold standardPOS tagged development and test-set sentences asinput rather than tagging text automatically.The results of the evaluation of parsing perfor-mance on the test set are shown in Table 3.
La-belled bracketing f-score for all sentences is justbelow 84% for all sentences, and 84.58% for sen-tences of length ?
70.
In comparison, Cowanand Collins (2005) report an f-score of 85.1%(?
70) using a version of Collins?
parser adaptedfor Cast3LB, and using reranking to boost perfor-such as grup.nom.ms (masculine singular), grup.nom.fs (fem-inine singular), grup.nom.mp (masculine plural) etc.
Thisnumber and gender information is already encoded in thePOS tags of nouns heading these constituents.2The labels we retain are the following: INC, S, S.NF,S.NF.R, S.NF, S.R, conj.subord, coord, data, espec, gerundi,grup.nom, gv, infinitiu, interjeccio, morf, neg, numero, prep,relatiu, s.a, sa, sadv, sn, sp, and versions of those suffixedwith .co to indicate coordination).Part of Speech Features includedDeterminer type, numberNoun type, numberAdjective type, numberPronoun type, number, personVerb type, number, moodAdverb typeConjunction typeTable 2: Features included in POS tags.
Typerefers to subcategories of parts of speech such ase.g.
common and proper for nouns, or main, aux-iliary and semiauxiliary for verbs.
For details see(Civit, 2000).LB Precision LB Recall F-scoreAll 84.18 83.74 83.96?
70 84.82 84.35 84.58Table 3: Parser performance.mance.
They use a different, more reduced cat-egory label set as well as a different training-testsplit.
Both Cowan and Collins and the present pa-per report scores which ignore punctuation.4.2 Cast3LB Function TaggingFor the task of Cast3LB function tag assign-ment we experimented with three generic machinelearning algorithms: a memory-based learner(Daelemans and van den Bosch, 2005), a maxi-mum entropy classifier (Berger et al, 1996) and aSupport Vector Machine classifier (Vapnik, 1998).For each algorithm we use the same set of featuresto represent nodes that are to be assigned one ofthe Cast3LB function tags.
We use a special nulltag for nodes where no Cast3LB tag is present.In Cast3LB only nodes in certain contexts areeligible for function tags.
For this reason we onlyconsider a subset of all nodes as candidates forfunction tag assignment, namely those which aresisters of nodes with the category labels gv (VerbGroup), infinitiu (Infinitive) and gerundi (Gerund).For these candidates we extract the following threetypes of features encoding configurational, mor-phological and lexical information for the targetnode and neighboring context nodes:?
Node features: position relative to head, headlemma, alternative head lemma (i.e.
the headof NP in PP), head POS, category, definite-ness, agreement with head verb, yield, hu-man/nonhuman139?
Local features: head verb, verb person, verbnumber, parent category?
Context features: node features (except posi-tion) of the two previous and two followingsister nodes (if present).We used cross-validation for refining the setof features and for tuning the parameters of themachine-learning algorithms.
We did not use anyadditional automated feature-selection procedure.We made use of the following implementations:TiMBL (Daelemans et al, 2004) for Memory-Based Learning, the MaxEnt Toolkit (Le, 2004)for Maximum Entropy and LIBSVM (Chang andLin, 2001) for Support Vector Machines.
ForTiMBL we used k nearest neighbors = 7 and thegain ratio metric for feature weighting.
For Max-Ent, we used the L-BFGS parameter estimationand 110 iterations, and we regularize the modelusing a Gaussian prior with ?2 = 1.
For SVM weused the RBF kernel with ?
= 2?7 and the costparameter C = 32.5 Cast3LB Tag Assignment EvaluationWe present evaluation results on the original gold-standard trees of the test set as well as on thetest-set sentences parsed by Bikel?s parser.
Forthe evaluation of Cast3LB function tagging per-formance on gold trees the most straightforwardmetric is the accuracy, or the proportion of all can-didate nodes that were assigned the correct label.However we cannot use this metric for evalu-ating results on the parser output.
The trees out-put by the parser are not identical to gold standardtrees due to parsing errors, and the set of candi-date nodes extracted from parsed trees will not bethe same as for gold trees.
For this reason we usean alternative metric which is independent of treeconfiguration and uses only the Cast3LB functionlabels and positional indices of tokens in a sen-tence.
For each function-tagged tree we first re-move the punctuation tokens.
Then we extract aset of tuples of the form ?GF, i, j?, where GF isthe Cast3LB function tag and i ?
j is the rangeof tokens spanned by the node annotated with thisfunction.
We use the standard measures of preci-sion, recall and f-score to evaluate the results.Results for the three algorithms are shown inTable 4.
MBL and MaxEnt show a very sim-ilar performance, while SVM outperforms both,ttttt7.0 7.5 8.0 8.5 9.0 9.50.760.800.840.88log(n)AccuracysssssmmmmmFigure 4: Learning curves for TiMBL (t), MaxEnt(m) and SVM (s).Acc.
Prec.
Recall F-scoreMBL 87.55 87.00 82.98 84.94MaxEnt 88.06 87.66 86.87 85.52SVM 89.34 88.93 84.90 86.87Table 4: Cast3LB function tagging performancefor gold-standard treesscoring 89.34% on accuracy and 86.87% on f-score.
The learning curves for the three algo-rithms, shown in Figure 4, are also informative,with SVM outperforming the other two methodsfor all training set sizes.
In particular, the last sec-tion of the plot shows SVM performing almost aswell as MBL with half as much learning material.Neither of the three curves shows signs of hav-ing reached a maximum, which indicates that in-Precision Recall F-scoreall corr.
all corr.
all corr.Baseline 59.26 72.63 60.61 75.35 59.93 73.96MBL 64.74 78.09 64.18 78.75 64.46 78.42MaxEnt 65.48 78.90 64.55 79.44 65.01 79.17SVM 66.96 80.58 66.38 81.27 66.67 80.92Table 5: Cast3LB function tagging performancefor parser output, for all constituents, and for cor-rectly parsed constituents only140Methods p-valueBaseline vs SVM 1.169?
10?9Baseline vs MBL 2.117?
10?6MBL vs MaxEnt 0.0799MaxEnt vs SVM 0.0005Table 6: Statistical significance testing results onfor the Cast3LB tag assignment on parser output.Precision Recall F-scoreBaseline 73.95 70.67 72.27SVM 76.90 74.48 75.67Table 7: LFG F-structure evaluation results forparser outputcreasing the size of the training data should resultin further improvements in performance.Table 5 shows the performance of the threemethods on parser output.
The baseline con-tains the results achieved by treating compoundcategory-function labels as atomic during parsertraining so that they are included in parser output.For this task we present two sets of results: (i) forall constituents, and (ii) for correctly parsed con-stituents only.
Again the best algorithm turns outto be SVM.
It outperforms the baseline by a largemargin (6.74% for all constituents).The difference in performance for gold stan-dard trees, and the correctly parsed constituentsin parser output is rather larger than what Blahetaand Charniak report.
Further analysis is neededto identify the source of this difference but wesuspect that one contributing factor is the use ofgreater number of context features combined witha higher parse error rate in comparison to their ex-periments on the Penn II Treebank.
Since any mis-analysis of constituency structure in the vicinity oftarget node can have negative impact, greater re-liance on context means greater susceptibility toparse errors.
Another factor to consider is the factthat we trained and adjusted parameters on gold-standard trees, and the model learned may rely onfeatures of those trees that the parser is unable toreproduce.For the experiments on parser output (all con-stituents) we performed a series of sign tests inorder to determine to what extent the differencesin performance between the different methods arestatistically significant.
For each pair of methodswe calculate the f-score for each sentence in thetest set.
For those sentences on which the scoresdiffer (i.e.
the number of trials) we calculate inhow many cases the second method is better thanthe first (i.e.
the number of successes).
We thenperform the test with the null hypothesis that theprobability of success is chance (= 0.5) and thealternative hypothesis that the probability of suc-cess is greater than chance (> 0.5).
The resultsare summarized in Table 6.
Given that we perform4 pairwise comparisons, we apply the Bonferronicorrection and adjust our target ??
= ?4 .
For theconfidence level 95% (??
= 0.0125) all pairs givestatistically significant results, except for MBL vsMaxEnt.6 Task-Based LFG AnnotationEvaluationFinally, we also evaluated the actual f-structuresobtained by running the LFG-annotation algo-rithm on trees produced by the parser and enrichedwith Cast3LB function tags assigned using SVM.For this task-based evaluation we produced a goldstandard consisting of f-structures correspondingto all sentences in the test set.
The LFG-annotationalgorithm was run on the test set trees (which con-tained original Cast3LB treebank function tags),and the resulting f-structures were manually cor-rected.Following Crouch et al (2002), we convertthe f-structures to triples of the form ?GF,Pi, Pj?,where Pi is the value of the PRED attribute of thef-structure, GF is an LFG grammatical functionattribute, and Pj is the value of the PRED attributeof the f-structure which is the value of the GFattribute.
This is done recursively for each levelof embedding in the f-structure.
Attributes withatomic values are ignored for the purposes of thisevaluation.
The results obtained are shown in Ta-ble 7.
We also performed a statistical significancetest for these results, using the same method as forthe Cast3LB tag assigment task.
The p-value givenby the sign test was 2.118?10?5, comfortably be-low ?
= 1%.The higher scores achieved in the LFG f-structure evaluation in comparison with the pre-ceding Cast3LB tag assignment evaluation (Table5) can be attributed to two main factors.
Firstly,the mapping from Cast3LB tags to LFG grammat-ical functions is not one-to-one.
For example threeCast3LB tags (CC, MOD and ET) are all mappedto LFG ADJUNCT.
Thus mistagging a MOD as141ATR CC CD CI CREG MOD SUJATR 136 2 0 0 0 0 5CC 6 552 12 4 25 18 6CD 1 19 418 5 3 0 26CI 0 6 1 50 1 0 0CREG 0 6 0 2 43 0 0MOD 0 0 0 0 0 19 0SUJ 0 8 24 2 0 0 465Table 8: Simplified confusion matrix for SVMon test-set gold-standard trees.
The gold-standardCast3LB function tags are shown in the first row,the predicted tags in the first column.
So e.g.
SUJwas mistagged as CD in 26 cases.
Low frequencyfunction tags as well as those rarely mispredictedhave been omitted for clarity.CC does not affect the f-structure score.
On theother hand the Cast3LB CD tag can be mappedto OBJ, COMP, or XCOMP, and it can be easilydecided which one is appropriate depending onthe category label of the target node.
Addition-ally many nodes which receive no function tag inCast3LB, such as noun modifiers, are straightfor-wardly mapped to LFG ADJUNCT.
Similarly, ob-jects of prepositions receive the LFG OBJ function.Secondly, the f-structure evaluation metric isless sensitive to small constituency misconfigura-tions: it is not necessary to correctly identify thetoken range spanned by a target node as long as thehead (which provides the PRED attribute) is cor-rect.7 Error AnalysisIn order to understand sources of error and de-termine how much room for further improvementthere is, we examined the most common cases ofCast3LB function mistagging.
A simplified confu-sion matrix with the most common Cast3LB tagsis shown in Table 8.
The most common mistakesoccur between SUJ and CD, in both directions, andmany also CREGs are erroneously tagged as CC.7.1 Subject vs Direct ObjectWe noticed that in over 50% of cases when aDirect Object (CD) was misidentified as Subject(SUJ), the target node?s mother was a relativeclause.
It turns out that in Spanish relative clausesgenuine syntactic ambiguity is not uncommon.Consider the following Spanish phrase:(1) SistemasSystemsquewhichusanuseelDET95%95%deoflosDETordenadores.computersIts translation into English is either Systems thatuse 95% of computers or alternatively Systems that95% of computers use.
In Spanish, unlike in En-glish, preverbal / postverbal position of a con-stituent is not a good guide to its grammaticalfunction in this and similar contexts.
Human an-notators can use their world knowledge to decideon the correct semantic role of a target constituentand use it in assigning a correct grammatical func-tion, but such information is obviously not usedin our machine learning methods.
Thus such mis-takes seem likely to remain unresolvable in ourcurrent approach.7.2 Prepositional Object vs AdjunctThe frequent misidentification of PrepositionalObjects (CREG) as Adjuncts (CC) seen in Table 8can be accounted for by several factors.
Firstly,Prepositional Objects are strongly dependent onspecific verbs and the comparatively small size ofour training data means that there is limited oppor-tunity for a machine-learning algorithm to learnlow-frequency lexical dependencies.
Here the ob-vious solution is to use a more adequate amount oftraining material when it becomes available.A further problem with the Prepositional Object- Adjunct distinction is its inherent fuzziness.
Be-cause of this, treebank designers may fail to pro-vide easy-to-follow, clearcut guidelines and hu-man annotators necessarily exercise a certain de-gree of arbitrariness in assigning one or the otherfunction.8 Conclusions and Future ResearchOur research has shown that machine-learning-based Cast3LB tag assignment as a post-processing step to raw tree parser output statisti-cally significantly outperforms a baseline wherethe parser itself is trained to learn category/ Cast3LB-function pairs.
In contrast to theparser-based method, the machine-learning-basedmethod avoids some sparse data problems and al-lows for more control over Cast3LB tag assign-ment.
We have found that the SVM algorithm out-performs the other two machine learning methodsused.142In addition, we evaluated Cast3LB tag assign-ment in a task-based setting in the context of au-tomatically acquiring LFG resources for Spanishfrom Cast3LB.
Machine-learning-based Cast3LBtag assignment yields statistically-significantlyimproved LFG f-structures compared to parser-based assignment.One limitation of our method is the fact that ittreats the classification task separately for each tar-get node.
It thus fails to observe constraints on thepossible sequences of grammatical function tagsin the same local context.
Some functions areunique, such as the Subject, whereas others (Di-rect and Indirect Object) can only be realized by afull NP once, although they can be doubled by aclitic pronoun.
Capturing such global constraintswill need further work.AcknowledgementsWe gratefully acknowledge support from ScienceFoundation Ireland grant 04/IN/I527 for the re-search reported in this paper.ReferencesA.
L. Berger, V. J. Della Pietra, and S. A. Della Pietra.1996.
A maximum entropy approach to naturallanguage processing.
Computational Linguistics,22(1):39?71, March.D.
Bikel.
2002.
Design of a multi-lingual,parallel-processing statistical parsing engine.
InHuman Language Technology Conference (HLT),San Diego, CA, USA.
Software availableat http://www.cis.upenn.edu/?dbikel/software.html#stat-parser.D.
Blaheta and E. Charniak.
2000.
Assigning functiontags to parsed text.
In Proceedings of the 1st Con-ference of the North American Chapter of the ACL,pages 234?240, Rochester, NY, USA.J.
Bresnan.
2001.
Lexical-Functional Syntax.
Black-well Publishers, Oxford.M.
Burke, O. Lam, A. Cahill, R. Chan, R. O?Donovan,A.
Bodomo, J. van Genabith, and A.
Way.
2004.Treebank-based acquisition of a Chinese Lexical-Functional Grammar.
In Proceedings of the 18thPacific Asia Conference on Language, Informationand Computation (PACLIC-18).A.
Cahill, M. Forst, M. McCarthy, R. O?Donovan,and C. Roher.
2003.
Treebank-based multilingualunification-grammar development.
In Proceedingsof the 15th Workshop on Ideas and Strategies forMultilingual Grammar Development, ESSLLI 15,Vienna, Austria.A.
Cahill, M. Burke, R. O?Donovan, J. van Genabith,and A.
Way.
2004.
Long-distance dependencyresolution in automatically acquired wide-coveragePCFG-based LFG approximations.
In Proceed-ings of the 42nd Annual Meeting of the Associa-tion for Computational Linguistics, pages 319?326,Barcelona, Spain.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIB-SVM: a library for support vector machines.
Soft-ware available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.M.
Civit and M. A.
Mart??.
2004.
Building Cast3LB: ASpanish treebank.
Research on Language and Com-putation, 2(4):549?574, December.M.
Civit.
2000.
Gu?
?a para la anotacio?n mor-fosinta?ctica del corpus CLiC-TALP, X-TRACTWorking Paper.
Technical report.
Avail-able at http://clic.fil.ub.es/personal/civit/PUBLICA/guia morfol.ps.M.
Civit.
2004.
Gu?
?a para la anotacio?n de las funcionessinta?cticas de Cast3LB.
Technical report.
Avail-able at http://clic.fil.ub.es/personal/civit/PUBLICA/funcions.pdf.B.
Cowan and M. Collins.
2005.
Morphology andreranking for the statistical parsing of Spanish.
InConference on Empirical Methods in Natural Lan-guage Processing, Vancouver, B.C., Canada.R.
Crouch, R. M. Kaplan, T. H. King, and S. Riezler.2002.
A comparison of evaluation metrics for abroad-coverage stochastic parser.
In Conference onLanguage Resources and Evaluation (LREC 02).W.
Daelemans and A. van den Bosch.
2005.
Memory-Based Language Processing.
Cambridge UniversityPress, September.W.
Daelemans, J. Zavrel, K. van der Sloot, andA.
van den Bosch.
2004.
TiMBL: Tilburg MemoryBased Learner, version 5.1, Reference Guide.
Tech-nical report.
Available from http://ilk.uvt.nl/downloads/pub/papers/ilk0402.pdf.V.
Jijkoun and M. de Rijke.
2004.
Enriching the outputof a parser using memory-based learning.
In Pro-ceedings of the 42nd Annual Meeting of the Associa-tion for Computational Linguistics, pages 311?318,Barcelona, Spain.Zh.
Le, 2004.
Maximum Entropy ModelingToolkit for Python and C++.
Availableat http://homepages.inf.ed.ac.uk/s0450736/software/maxent/manual.pdf.R.
O?Donovan, A. Cahill, J. van Genabith, and A. Way.2005.
Automatic acquisition of Spanish LFG re-sources from the CAST3LB treebank.
In Proceed-ings of the Tenth International Conference on LFG,Bergen, Norway.V.
N. Vapnik.
1998.
Statistical Learning Theory.Wiley-Interscience, September.143
