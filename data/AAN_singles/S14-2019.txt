Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 135?139,Dublin, Ireland, August 23-24, 2014.BioinformaticsUA: Concept Recognition in Clinical Narratives Using aModular and Highly Efficient Text Processing FrameworkS?ergio MatosDETI/IEETAUniversity of Aveiro3810-193 Aveiro, Portugalaleixomatos@ua.ptTiago NunesDETI/IEETAUniversity of Aveiro3810-193 Aveiro, Portugaltiago.nunes@ua.ptJos?e Lu?
?s OliveiraDETI/IEETAUniversity of Aveiro3810-193 Aveiro, Portugaljlo@ua.ptAbstractClinical texts, such as discharge sum-maries or test reports, contain a valuableamount of information that, if efficientlyand effectively mined, could be used toinfer new knowledge, possibly leading tobetter diagnosis and therapeutics.
Withthis in mind, the SemEval-2014 Analysisof Clinical Text task aimed at assessingand improving current methods for identi-fication and normalization of concepts oc-curring in clinical narrative.
This paperdescribes our approach in this task, whichwas based on a fully modular architec-ture for text mining.
We followed a puredictionary-based approach, after perform-ing error analysis to refine our dictionaries.We obtained an F-measure of 69.4% inthe entity recognition task, achieving thesecond best precision over all submittedruns (81.3%), with above average recall(60.5%).
In the normalization task, weachieved a strict accuracy of 53.1% and arelaxed accuracy of 87.0%.1 IntroductionNamed entity recognition (NER) is an informationextraction task where the aim is to identify men-tions of specific types of entities in text.
This taskhas been one of the main focus in the biomedi-cal text mining research field, specially when ap-plied to the scientific literature.
Such efforts haveled to the development of various tools for therecognition of diverse entities, including speciesnames, genes and proteins, chemicals and drugs,anatomical concepts and diseases.
These tools useThis work is licensed under a Creative Commons Attribution4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/methods based on dictionaries, rules, and machinelearning, or a combination of those depending onthe specificities and requirements of each concepttype (Campos et al., 2013b).
After identifying en-tities occurring in texts, it is also relevant to dis-ambiguate those entities and associate each occur-rence to a specific concept, using an univocal iden-tifier from a reference database such as Uniprot1for proteins, or OMIM2for genetic disorders.
Thisis usually performed by matching the identifiedentities against a knowledge-base, possibly eval-uating the textual context in which the entity oc-curred to identify the best matching concept.The SemEval-2014 Analysis of Clinical Texttask aimed at the identification and normalizationof concepts in clinical narrative.
Two subtaskswere defined, where Task A was focused on therecognition of entities belonging to the ?disorders?semantic group of the Unified Medical LanguageSystem (UMLS), and Task B was focused on nor-malization of these entities to a specific UMLSConcept Unique Identifier (CUI).
Specifically, thetask definition required that concepts should onlybe normalized to CUIs that could be mapped to theSNOMED CT3terminology.In this paper, we present a dictionary-based ap-proach for the recognition of these concepts, sup-ported by a modular text analysis and annotationpipeline.2 Methods2.1 DataThe task made use of the ShARe corpus (Pradhanet al., 2013), which contains manually annotatedclinical notes from the MIMIC II database4(Saeedet al., 2011).
The corpus contains 298 documents,1http://www.uniprot.org/2http://www.omim.org/3http://www.ihtsdo.org/snomed-ct/4http://mimic.physionet.org/database.html135Processing pipelineDictionariesReaderSentenceTaggerNLPDocumentsAnnotatedDocumentsModelsDictionaryTaggerML TaggerPost-processingCustom ModuleWriterRelation extractorIndexerAbbreviation resolutionDisambiguatorFigure 1: Neji?s processing pipeline used for annotating the documents.
Boxes with dotted lines indicateoptional processing modules.
Machine-learning models were not used.with a total of 11156 annotations of disorder men-tions.
These annotations include a UMLS conceptidentifier when such normalization was possibleaccording to the annotation guidelines.Besides this manually annotated corpus, a largerunannotated data set was also made available totask participants, in order to allow the applicationof unsupervised methods.2.2 Processing PipelineWe used Neji, an open source framework forbiomedical concept recognition based on an au-tomated processing pipeline that supports thecombined application of machine learning anddictionary-based approaches (Campos et al.,2013a).
Apart from offering a flexible frame-work for developing different text mining sys-tems, Neji includes various built-in methods, fromtext loading and pre-processing, to natural lan-guage parsing and entity tagging, all optimizedfor processing biomedical text.
Namely, it in-cludes a sentence splitting module adapted fromthe Lingpipe library5and a customized versionof GDep (Sagae and Tsujii, 2007) for tokeniza-tion, part-of-speech tagging, and other natural lan-guage processing tasks.
Figure 1 shows the com-plete Neji text processing pipeline, illustrating itsmodule based architecture built on top of a com-mon data structure.
The dictionary module per-forms exact, case-insensitive matching using De-terministic Finite Automatons (DFAs), allowing5http://alias-i.com/lingpipe/index.htmlvery efficient processing of documents and match-ing against dozens of dictionaries containing mil-lions of terms.Neji has been validated against differentbiomedical literature corpora, using specificallycreated machine learning models and dictionar-ies.
Regarding the recognition of disorder con-cepts, Neji achieved an F-measure of 68% on ex-act mathing and 83% on approximate matchingagainst the NCBI disease corpus, using a puredictionary-based approach (Do?gan and Lu, 2012).2.3 DictionariesFollowing the task description and the corpus an-notation guidelines, we compiled dictionaries forthe following UMLS semantic types, using the2012AB version of the UMLS Metathesaurus:?
Congenital Abnormality?
Acquired Abnormality?
Injury or Poisoning?
Pathologic Function?
Disease or Syndrome?
Mental or Behavioral Dysfunction?
Cell or Molecular Dysfunction?
Anatomical Abnormality?
Neoplastic Process?
Signs and SymptomsAdditionally, although the semantic type ?Find-ings?
was not considered as part of the ?Disorders?group, we created a customized dictionary includ-ing only those concepts of this semantic type thatoccurred as an annotation in the training data.
If136a synonym of a given concept was present in thetraining data annotations, we added all the syn-onyms of that concept to this dictionary.
Thisallowed including some concepts that occur veryfrequently (e.g.
?fever?
), while filtering out manyconcepts of this semantic type that are not relevantfor this task.
In total, these dictionaries containalmost 1.5 million terms, of which 525 thousand(36%) were distinct terms, for nearly 293 thousanddistinct concept identifiers.Refining the dictionariesIn order to expand the dictionaries, we pre-processed the UMLS terms to find certain patternsindicating acronyms.
For example, if a term suchas ?Miocardial infarction (MI)?
or ?Miocardial in-farction - MI?
appeared as a synonym for a givenUMLS concept, we checked if the acronym (in thisexample, ?MI?)
was also a synonym for that con-cept, and added it to a separate dictionary if thiswas not the case.
This resulted in the addition of10430 terms, for which only 1459 (14%) were dis-tinct, for 2086 concepts.
These numbers reflect theexpected ambiguity in the acronyms, which repre-sents one of the main challenges in the annotationof clinical texts.Furthermore, in order to improve the baselineresults obtained with the initial dictionaries, weperformed error analysis to identify frequent er-rors in the automatic annotations.
Using the man-ual annotations as reference, we counted the num-ber of times a term was correctly annotated in thedocuments (true positives) and compared it to thenumber of times that same term caused an annota-tion to be incorrectly added (a false positive).
Wethen defined an exclusion list containing 817 termsfor which the ratio of these two counts was 0.25 orless.Following the same approach, we created a sec-ond exclusion list by comparing the number ofFNs to the number of FPs, and selecting thoseterms for which this ratio was lower than 0.5.
Thisresulted in an exclusion list containing 623 terms.We also processed the unannotated data set, inorder to identify frequently occurring terms thatcould be removed from the dictionaries to avoidlarge numbers of false positives.
This dataset in-cludes over 92 thousand documents, which wereprocessed in around 23 minutes (an average of67 documents per second) and produced almost4 million annotations.
Examples of terms fromour dictionaries that occur very frequently in thisdata set are: ?sinus rhythm?, which occurred al-most 35 thousand times across all documents, and?past medical history?, ?allergies?
and ?abnormal-ities?, all occurring more than 15 thousand times.In fact, most of the highly frequent terms belongedto the ?Findings?
semantic type.
Although thisanalysis gave some insights regarding the contentof the data, its results were not directly used torefine the dictionaries, since the filtering steps de-scribed above led to better overall results.2.4 Concept NormalizationAccording to the task description, only thoseUMLS concepts that could be mapped to aSNOMED CT identifier should be considered inthe normalization step, while all other entitiesshould be added to the results without a conceptidentifier.
We followed a straightforward normal-ization strategy, by assigning the correspondingUMLS CUIs to each identified entity, during thedictionary-matching phase.
We then filtered outany CUIs that did not have a SNOMED CT map-ping in the UMLS data.
In the cases when multi-ple idenfiers were still left, we naively selected thefirst one, according the dictionary ordering definedabove, followed in the end by the filtered ?Find-ings?
dictionary and the additional acronyms dic-tionary.3 Results and Discussion3.1 Evaluation MetricsThe common evaluation metrics were used toevaluate the entity recognition task, namelyPrecision = TP/(TP + FP ) and Recall =TP/(TP+FN), where TP, FP and FN are respec-tively the number of true positive, false positive,and false negative annotations, and Fmeasure =2?
Precision?Recall/(Precision+Recall),the harmonic mean of precision and recall.
Addi-tionally, the performance was evaluated consider-ing both strict and relaxed, or overlap, matching ofthe gold standard annotations.For the normalization task, the metric used toevaluate performance was accuracy.
Again, twomatching methods were considered: strict accu-racy was defined as the ratio between the numberof correct identifiers assigned to the predicted en-tities, and the total number of entities manuallyannotated in the corpus; while relaxed accuracymeasured the ratio between the number of correct137Task A Task BStrict Relaxed Strict RelaxedRun P R F P R F Acc AccBest 0,843 0,786 0,813 0,936 0,866 0,900 0,741 0,873Average 0,648 0,574 0,599 0,842 0,731 0,770 0,461 0,7530 0,813 0,605 0,694 0,929 0,693 0,794 0,527 0,8701 0,600 0,621 0,610 0,698 0,723 0,710 0,531 0,8552 0,753 0,538 0,628 0,865 0,621 0,723 0,463 0,861Table 1: Official results on the test dataset.
The best results for each task and matching strategy areidentified in bold.
The best run from all participating teams as well as the overall average are shown forcomparison.identifiers and the number of entities correctly pre-dicted by the system.3.2 Test ResultsWe submitted three runs of annotations for thedocuments in the test set, as described below:?
Run 0: Resulting annotations were filteredusing the first exclusion list (817 terms,TP/FP ratio 0.25 or lower).
The ex-tra acronyms dictionary was not used, andmatches up to 3 characters long were filteredout, except if they were 3 characters long andappeared as uppercase in the original text.?
Run 1: The extra acronyms dictionary wasincluded.
The same exclusion list as in Run0 was used, but short annotations were notremoved.?
Run 2: The extra acronyms dictionary wasincluded.
The second exclusion list was used,and short annotations were not removed.Table 1 shows the official results obtained onthe test set for each submitted run.Overall, the best results were obtained with themore stringent dictionaries and filtering, leadingto a precision of 81.3% and and F-measure of69.4%.
This results was achieved without the useof the additional acronyms list, and also by re-moving short annotations.
This filtering does notdiscard annotations with three characters if theyappeared in uppercase in the original text, as thismore clearly indicates the use of an acronym.
Pre-liminary evaluation on the training data showedthat this choice had a small, but positive contri-bution to the overall results.We achieved the second-best precision resultswith this first run, considering both strict and re-laxed matching.
Although this level of precisionwas not associated to a total loss in recall, wewere only able to identify 70% of the disorderentities, even when considering relaxed match-ing.
To overcome this limitation, we will evalu-ate the combined use of dictionaries and machine-learning models, taking advantage of the Nejiframework.
Another possible limitation has todo with the recognition and disambiguation ofacronyms, which we will also evaluate further.Regarding the normalization results (Task B),we achieved the 12th and 10th best overall results,considering strict and relaxed accuracies respec-tively, corresponding to the 7th and 6th best team.For relaxed matching, our results are 5,8% lowerthan the best team, which is a positive result giventhe na?
?ve approach taken.
These performancesmay be improved as a result of enhancements inthe entity recognition step, and by applying a bet-ter normalization strategy.4 ConclusionsWe present results for the recognition and normal-ization of disorder mentions in clinical texts, us-ing a dictionary-based approach .
The dictionarieswere iteratively filtered following error-analysis,in order to better tailor the dictionaries accordingto the task annotation guidelines.
In the end, aprecision of 81.3% was achieved, for a recall of60.5% and a F-measure of 69.4%.
The use ofa machine-learning based approach and a betteracronym resolution method are being studied withthe aim of improving the recall rate.In the normalization task, using the refined dic-tionaries directly, we achieved a strict accuracy of53.1% and a relaxed accuracy of 87.0%.
Strict138normalization results, as given by the metric de-fined for this task, are dependent on the entityrecognition recall rate, and are expected to followimprovements that may be achieved in that step.AcknowledgementsThis work was supported by National Fundsthrough FCT - Foundation for Science and Tech-nology, in the context of the project PEst-OE/EEI/UI0127/2014.
S. Matos is funded by FCTunder the FCT Investigator programme.ReferencesDavid Campos, S?ergio Matos, and Jos?e Lu?
?s Oliveira.2013a.
A modular framework for biomedical con-cept recognition.
BMC Bioinformatics, 14:281.David Campos, S?ergio Matos, and Jos?e Lu?
?s Oliveira,2013b.
Current Methodologies for BiomedicalNamed Entity Recognition, pages 839?868.
JohnWiley & Sons, Inc., Hoboken, New Jersey.Rezarta Islamaj Do?gan and Zhiyong Lu.
2012.
Animproved corpus of disease mentions in PubMed ci-tations.
In Proceedings of BioNLP?12, pages 91?99,Stroudsburg, PA, USA, June.Sameer Pradhan, Noemie Elhadad, Brett South, DavidMartinez, Lee Christensen, Amy Vogel, HannaSuominen, Wendy Chapman, and Guergana Savova.2013.
Task 1: ShARe/CLEF eHealth EvaluationLab 2013.
Online Working Notes of the CLEF 2013Evaluation Labs and Workshop.Mohammed Saeed, Mauricio Villarroel, Andrew Reis-ner, Gari Clifford, Li-Wei Lehman, George Moody,Thomas Heldt, Tin Kyaw, Benjamin Moody, andRoger Mark.
2011.
Multiparameter IntelligentMonitoring in Intensive Care II (MIMIC-II): apublic-access intensive care unit database.
CriticalCare Medicine, 39(5):952.Kenji Sagae and Jun?ichi Tsujii.
2007.
Dependencyparsing and domain adaptation with LR models andparser ensembles.
In Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, pages1044?1050, Prague, Czech Republic.139
