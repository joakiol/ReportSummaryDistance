Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1046?1056,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsMulti-level Structured Models for Document-level Sentiment ClassificationAinur YessenalinaDept.
of Computer ScienceCornell UniversityIthaca, NY, USAainur@cs.cornell.eduYisong YueDept.
of Computer ScienceCornell UniversityIthaca, NY, USAyyue@cs.cornell.eduClaire CardieDept.
of Computer ScienceCornell UniversityIthaca, NY, USAcardie@cs.cornell.eduAbstractIn this paper, we investigate structured mod-els for document-level sentiment classifica-tion.
When predicting the sentiment of a sub-jective document (e.g., as positive or nega-tive), it is well known that not all sentencesare equally discriminative or informative.
Butidentifying the useful sentences automaticallyis itself a difficult learning problem.
This pa-per proposes a joint two-level approach fordocument-level sentiment classification thatsimultaneously extracts useful (i.e., subjec-tive) sentences and predicts document-levelsentiment based on the extracted sentences.Unlike previous joint learning methods forthe task, our approach (1) does not rely ongold standard sentence-level subjectivity an-notations (which may be expensive to obtain),and (2) optimizes directly for document-levelperformance.
Empirical evaluations on moviereviews and U.S. Congressional floor debatesshow improved performance over previous ap-proaches.1 IntroductionSentiment classification is a well-studied and activeresearch area (Pang and Lee, 2008).
One of the mainchallenges for document-level sentiment categoriza-tion is that not every part of the document is equallyinformative for inferring the sentiment of the wholedocument.
Objective statements interleaved with thesubjective statements can be confusing for learningmethods, and subjective statements with conflictingsentiment further complicate the document catego-rization task.
For example, authors of movie reviewsoften devote large sections to (largely objective) de-scriptions of the plot (Pang and Lee, 2004).
In ad-dition, an overall positive review might still includesome negative opinions about an actor or the plot.Early research on document-level sentiment clas-sification employed conventional machine learningtechniques for text categorization (Pang et al, 2002).These methods, however, assume that documents arerepresented via a flat feature vector (e.g., a bag-of-words).
As a result, their ability to identify and ex-ploit subjectivity (or other useful) information at thesentence-level is limited.And although researchers subsequently proposedmethods for incorporating sentence-level subjectiv-ity information, existing techniques have some un-desirable properties.
First, they typically requiregold standard sentence-level annotations (McDon-ald et al (2007), Mao and Lebanon (2006)).
Butthe cost of acquiring such labels can be prohibitive.Second, some solutions for incorporating sentence-level information lack mechanisms for controllinghow errors propagate from the subjective sentenceidentification subtask to the main document classifi-cation task (Pang and Lee, 2004).
Finally, solutionsthat attempt to handle the error propagation problemhave done so by explicitly optimizing for the bestcombination of document- and sentence-level clas-sification accuracy (McDonald et al, 2007).
Opti-mizing for this compromise, when the real goal isto maximize only the document-level accuracy, canpotentially hurt document-level performance.In this paper, we propose a joint two-level modelto address the aforementioned concerns.
We formu-late our training objective to directly optimize for1046document-level accuracy.
Further, we do not requiregold standard sentence-level labels for training.
In-stead, our training method treats sentence-level la-bels as hidden variables and jointly learns to predictthe document label and those (subjective) sentencesthat best ?explain?
it, thus controlling the propaga-tion of incorrect sentence labels.
And by directlyoptimizing for document-level accuracy, our modellearns to solve the sentence extraction subtask onlyto the extent required for accurately classifying doc-ument sentiment.
A software implementation of ourmethod is also publicly available.1For the rest of the paper, we will discuss re-lated work, motivate and describe our model, presentan empirical evaluation on movie reviews and U.S.Congressional floor debates datasets and close withdiscussion and conclusions.2 Related WorkPang and Lee (2004) first showed that sentence-level extraction can improve document-level per-formance.
They used a cascaded approach byfirst filtering out objective sentences and perform-ing subjectivity extractions using a global min-cutinference.
Afterward, the subjective extracts wereconverted into inputs for the document-level senti-ment classifier.
One advantage of their approachis that it avoids the need for explicit subjectiv-ity annotations.
However, like other cascaded ap-proaches (e.g., Thomas et al (2006), Mao andLebanon (2006)), it can be difficult to control howerrors propagate from the sentence-level subtask tothe main document classification task.Instead of taking a cascaded approach, one candirectly modify the training of flat document clas-sifiers using lower level information.
For instance,Zaidan et al (2007) used human annotators to markthe ?annotator rationales?, which are text spans thatsupport the document?s sentiment label.
These an-notator rationales are then used to formulate addi-tional constraints during SVM training to ensure thatthe resulting document classifier is less confident inclassifying a document that does not contain the ra-tionale versus the original document.
Yessenalina etal.
(2010) extended this approach to use automati-cally generated rationales.1http://projects.yisongyue.com/svmsle/A natural approach to avoid the pitfalls associ-ated with cascaded methods is to use joint two-level models that simultaneously solve the sentence-level and document-level tasks (e.g., McDonald etal.
(2007), Zaidan and Eisner (2008)).
Since thesemodels are trained jointly, the sentence-level pre-dictions affect the document-level predictions andvice-versa.
However, such approaches typicallyrequire sentence-level annotations during training,which can be expensive to acquire.
Furthermore,the training objectives are usually formulated as acompromise between sentence-level and document-level performance.
If the goal is to predict well at thedocument-level, then these approaches are solving amuch harder problem that is not exactly aligned withmaximizing document-level accuracy.Recently, researchers within both Natural Lan-guage Processing (e.g., Petrov and Klein (2007),Chang et al (2010), Clarke et al (2010)) andother fields (e.g., Felzenszwalb et al (2008), Yuand Joachims (2009)) have analyzed joint multi-level models (i.e., models that simultaneously solvethe main prediction task along with important sub-tasks) that are trained using limited or no explicitlower level annotations.
Similar to our approach, thelower level labels are treated as hidden or latent vari-ables during training.
Although the training processis non-trivial (and in particular requires a good ini-tialization of the hidden variables), it avoids the needfor human annotations for the lower level subtasks.Some researchers have also recently applied hiddenvariable models to sentiment analysis, but they werefocused on classifying either phrase-level (Choi andCardie, 2008) or sentence-level polarity (Nakagawaet al, 2010).3 Extracting Hidden ExplanationsIn this paper, we take the view that each documenthas a subset of sentences that best explains its sen-timent.
Consider the ?annotator rationales?
gener-ated by human judges for the movie reviews dataset(Zaidan et al, 2007).
Each rationale is a text spanthat was identified to support (or explain) its parentdocument?s sentiment.
Thus, these rationales can beinterpreted as (something close to) a ground truth la-beling of the explanatory segments.
Using a datasetwhere each document contains only its rationales,1047Algorithm 1 Inference Algorithm for (2)1: Input: x2: Output: (y, s)3: s+ ?
argmaxs?S(x) ~wT?
(x,+1, s)4: s?
?
argmaxs?S(x) ~wT?
(x,?1, s)5: if ~wT?
(x,+1, s+) > ~wT?
(x,?1, s?)
then6: Return (+1, s+)7: else8: Return (?1, s?
)9: end ifcross validation experiments using an SVM classi-fier yields 97.44% accuracy ?
as opposed to 86.33%accuracy when using the full text of the original doc-uments.
Clearly, extracting the best supporting seg-ments can offer a tremendous performance boost.We are interested in settings where human-extracted explanations such as annotator rationalesmight not be readily available, or are imperfect.
Assuch, we will formulate the set of extracted sen-tences as latent or hidden variables in our model.Viewing the extracted sentences as latent variableswill pose no new challenges during prediction, sincethe model is expected to predict all labels at testtime.
We will leverage recent advances in traininglatent variable SVMs (Yu and Joachims, 2009) to ar-rive at an effective training procedure.4 ModelIn this section, we present a two-level documentclassification model.
Although our model makespredictions at both the document and sentence lev-els, it will be trained (and evaluated) only with re-spect to document-level performance.
We beginby presenting the feature structure and inferencemethod.
We will then describe a supervised train-ing algorithm based on structural SVMs, and finallydiscuss some extensions and design decisions.Let x denote a document, y = ?1 denote the sen-timent (for us, a binary positive or negative polarity)of a document, and s denote a subset of explanatorysentences in x.
Let ?
(x, y, s) denote a joint fea-ture map that outputs features describing the qual-ity of predicting sentiment y using explanation s fordocument x.
We focus on linear models, so given a(learned) weight vector ~w, we can write the qualityof predicting y (with explanation s) asF (x, y, s; ~w) = ~wT?
(x, y, s), (1)and a document-level sentiment classifier ash(x; ~w) = argmaxy=?1maxs?S(x)F (x, y, s; ~w), (2)where S(x) denotes the collection of feasible expla-nations (e.g., subsets of sentences) for x.Let xj denote the j-th sentence of x.
We proposethe following instantiation of (1),~wT?
(x, y, s) =1N(x)?j?sy ?
~wTpol?pol(xj) + ~wTsubj?subj(xj), (3)where the first term in the summation captures thequality of predicting polarity y on sentences in s,the second term captures the quality of predicting sas the subjective sentences, and N(x) is a normaliz-ing factor (which will be discussed in more detail inSection 4.3).
We represent the weight vector as~w =[~wpol~wsubj], (4)and ?pol(xj) and ?subj(xj) denote the polarity andsubjectivity features of sentence xj , respectively.Note that ?pol and ?subj are disjoint by construc-tion, i.e., ?Tpol?subj = 0.
We will present extensionsin Section 4.5.For example, suppose ?pol and ?subj were bothbag-of-words feature vectors.
Then we might learna high weight for the feature corresponding to theword ?think?
in ?subj since that word is indicativeof the sentence being subjective (but not necessarilyindicating positive or negative polarity).4.1 Making PredictionsAlgorithm 1 describes our inference procedure.
Re-call from (2) that our hypothesis function predictsthe sentiment label that maximizes (3).
To do this,we compare the best set of sentences that explainsa positive polarity prediction with the best set thatexplains a negative polarity prediction.We now specify the structure of S(x).
In this pa-per, we use a cardinality constraint,S(x) = {s ?
{1, .
.
.
, |x|} : |s| ?
f(|x|)}, (5)1048Algorithm 2 Training Algorithm for OP 11: Input: {(x1, y1), .
.
.
, (xN , yN )} //training data2: Input: C //regularization parameter3: Input: (s1, .
.
.
, sN ) //initial guess4: ~w ?
SSVMSolve(C, {(xi, yi, si)}Ni=1)5: while ~w not converged do6: for i = 1, .
.
.
, N do7: si ?
argmaxs?S(xi) ~wT?
(xi, yi, s)8: end for9: ~w ?
SSVMSolve(C, {(xi, yi, si)}Ni=1)10: end while11: Return ~wwhere f(|x|) is a function that depends only on thenumber of sentences in x.
For example, a simplefunction is f(|x|) = |x| ?
0.3, indicating that at most30% of the sentences in x can be subjective.Using this definition of S(x), we can then com-pute the best set of subjective sentences for eachpossible y by computing the joint subjectivity andpolarity score of each sentence xj in isolation,y ?
~wTpol?pol(xj) + ~wTsubj?subj(xj),and selecting the top f(|x|) as s (or fewer, if thereare fewer than f(|x|) that have positive joint score).4.2 TrainingFor training, we will use an approach based on latentvariable structural SVMs (Yu and Joachims, 2009).Optimization Problem 1.min~w,?
?012?~w?2 +CNN?i=1?i (6)s.t.
?i :maxs?Si~wT?
(xi, yi, s) ?maxs??S(xi)~wT?
(xi,?yi, s?)
+ 1?
?i (7)OP 1 optimizes the standard SVM training objec-tive for binary classification.
Each training examplehas a corresponding constraint (7), which is quanti-fied over the best possible explanation of the train-ing polarity label.
Note that we never observe thetrue explanation for the training labels; they are thehidden or latent variables.
The hidden variables arealso ignored in the objective function.As a result, one can interpret OP 1 to be directlyoptimizing a trade-off between model complexity(as measured using the 2-norm) and document-levelclassification error in the training set.
This has twomain advantages over related training approaches.First, it solves the multi-level problem jointly as op-posed to separately, which avoids introducing diffi-cult to control propagation errors.
Second, it doesnot require solving the sentence-level task perfectly,and also does not require precise sentence-leveltraining labels.
In other words, our goal is to learn toidentify the informative (subjective) sentences thatbest explain the training labels to the extent requiredfor good document classification performance.OP 1 is non-convex because of the constraints (7).To solve OP 1, we use the combination of the CCCPalgorithm (Yuille and Rangarajan, 2003) with cut-ting plane training of structural SVMs (Joachims etal., 2009), as proposed in Yu and Joachims (2009).Suppose each constraint (7) is replaced by~wT?
(xi, yi, si) ?
maxs??S(xi)~wT?
(xi,?yi, s?)+1?
?i,where si is some fixed explanation (e.g., an initialguess of the best explanation).
Then OP 1 reducesto a standard structural SVM, which can be solvedefficiently (Joachims et al, 2009).
Algorithm 2 de-scribes our training procedure.
Starting with an ini-tial guess si for each training example, the trainingprocedure alternates between solving an instance ofthe resulting structural SVM (called SSVMSolve inAlgorithm 2) using the currently best known expla-nations si (Line 9), and making a new guess of thebest explanations (Line 7).
Yu and Joachims (2009)showed that this alternating procedure for traininglatent variable structural SVMs is an instance of theCCCP procedure (Yuille and Rangarajan, 2003), andso is guaranteed to converge to a local optimum.For our experiments, we do not train until conver-gence, but instead use performance on a validationset to choose the halting iteration.
Since OP 1 is non-convex, a good initialization is necessary.
To gener-ate the initial explanations, one can use an off-the-shelf sentiment classifier such as OpinionFinder2(Wilson et al, 2005).
For some datasets, there ex-ist documents with annotated sentences, which we2http://www.cs.pitt.edu/mpqa/opinionfinderrelease/1049can treat either as the ground truth or another (verygood) initial guess of the explanatory sentences.4.3 Feature RepresentationLike any machine learning approach, we must spec-ify a useful set of features for the?
vectors describedabove.
We will consider two types of features.Bag-of-words.
Perhaps the simplest approach isto define ?
using a bag-of-words feature representa-tion, with one feature corresponding to each word inthe active lexicon of the corpus.
Using such a featurerepresentation might allow us to learn which wordshave high polarity (e.g., ?great?)
and which are in-dicative of subjective sentences (e.g., ?opinion?
).Sentence properties.
We can incorporate manyuseful features to describe sentence subjectivity.
Forexample, subjective sentences might densely popu-late the end of a document, or exhibit spatial co-herence (so features describing previous sentencesmight be useful for classifying the current sentence).Such features cannot be compactly incorporated intoflat models that ignore the document structure.For our experiments, we normalize each ?subjand ?pol to have unit 2-norm.Joint Feature Normalization.
Another designdecision is the choice of normalization N(x) in (3).Two straightforward choices are N(x) = f(|x|) andN(x) =?f(|x|), where f(|x|) is the size con-straint as described in (5).
In our experiments wetried both and found the square root normalizationto work better in practice; therefore all the experi-mental results are reported using N(x) =?f(|x|).The appendix contains an analysis that sheds lighton when square root normalization can be useful.4.4 Incorporating Proximity InformationAs mentioned in Section 4.3, it is possible (andlikely) for subjective sentences to exhibit spatial co-herence (e.g., they might tend to group together).To exploit this structure, we will expand the featurespace of ?subj to include both the words of the cur-rent and previous sentence as follows,?subj(x, j) =[?subj(xj)?subj(xj?1)].The corresponding weight vector can be written as~w?subj =[~wsubj~wprevSubj].By adding these features, we are essentially assum-ing that the words of the previous sentence are pre-dictive of the subjectivity of the current sentence.Alternative approaches include explicitly ac-counting for this structure by treating subjectivesentence extraction as a sequence-labeling problem,such as in McDonald et al (2007).
Such struc-ture formulations can be naturally encoded in thejoint feature map.
Note that the inference procedurein Algorthm 1 is still tractable, since it reduces tocomparing the best sequence of subjective/objectivesentences that explains a positive sentiment versusthe best sequence that explains a negative sentiment.For this study, we chose not to examine this moreexpressive yet more complex structure.4.5 ExtensionsThough our initial model (3) is simple and intuitive,performance can depend heavily on the quality oflatent variable initialization and the quality of thefeature structure design.
Consider the case wherethe initialization contains only objective sentencesthat do not convey any sentiment.
Then all the fea-tures initially available during training are gener-ated from these objective sentences and are thus use-less for sentiment classification.
In other words, toomuch useful information has been suppressed forthe model to make effective decisions.
To hedgeagainst learning poor models due to using a poorinitialization and/or a suboptimal feature structure,we now propose extensions that incorporate infor-mation from the entire document.We identify the following desirable properties thatany such extended model should satisfy:(A) The model should be linear.
(B) The model should be trained jointly.
(C) The component that models the entire docu-ment should influence which sentences are ex-tracted.The first property stems from the fact that our ap-proach relies on linear models.
The second propertyis desirable since joint training avoids error propaga-tion that can be difficult to control.
The third prop-erty deals with the information suppression issue.10504.5.1 Regularizing Relative to a PriorWe first consider a model that satisfies properties(A) and (C).
Using the representation in (4), we pro-pose a training procedure that regularize ~wpol rela-tive to a prior model.
Suppose we have a weightvector ~w0 which indicated the a priori guess of thecontribution of each corresponding feature, then wecan train our model using OP 2,Optimization Problem 2.min~w,?
?012?~w ?
~w0?2 +CNN?i=1?is.t.
?i :maxs?Si~wT?
(xi, yi, s) ?maxs??S(xi)~wT?
(xi,?yi, s?)
+ 1?
?iFor our experiments, we use~w0 =[~wdoc0],where ~wdoc denotes a weight vector trained to clas-sify the polarity of entire documents.
Then one caninterpret OP 2 as enforcing that the polarity weights~wpol not be too far from ~wdoc.
Note that ~w0 must beavailable before training.
Therefore this approachdoes not satisfy property (B).4.5.2 Extended Feature SpaceOne simple way to satisfy all three aforemen-tioned properties is to jointly model not only po-larity and subjectivity of the extracted sentences,but also polarity of the entire document.
Let ~wdocdenote the weight vector used to model the polar-ity of entire document x (so the document polarityscore is then ~wTdoc?pol(x)).
We can also incorporatethis weight vector into our structured model to com-pute a smoothed polarity score of each sentence via~wTdoc?pol(xj).
Following this intuition, we proposethe following structured model,~wT?
(x, y, s) =yN(x)??
?j?s(~wTpol?pol(xj) + ~wTdoc?pol(xj))??+1N(x)???j?s~wTsubj?subj(xj)?
?+ y ?
~wTdoc?pol(x)where the weight vector is now~w =??~wpol~wsubj~wdoc??
.Training this model via OP 1 achieves that ~wdoc is(1) used to model the polarity of the entire docu-ment, and (2) used to compute a smoothed estimateof the polarity of the extracted sentences.
This sat-isfies all three properties (A), (B), and (C), althoughother approaches are also possible.5 Experiments5.1 Experimental SetupWe evaluate our methods using the Movie Reviewsand U.S. Congressional Floor Debates datasets, fol-lowing the setup used in previous work for compar-ison purposes.3Movie Reviews.
We use the movie reviewsdataset from Zaidan et al (2007) that was originallyreleased by Pang and Lee (2004).
This version con-tains annotated rationales for each review, which weuse to generate an additional initialization duringtraining (described below).
We follow exactly theexperimental setup used in Zaidan et al (2007).4U.S.
Congressional Floor Debates.
We alsouse the U.S. Congressional floor debates transcriptsfrom Thomas et al (2006).
The data was extractedfrom GovTrack (http://govtrack.us), which has allavailable transcripts of U.S. floor debates in theHouse of Representatives in 2005.
As in previ-ous work, only debates with discussions of ?con-troversial?
bills were considered (where the los-ing side had at least 20% of the speeches).
Thegoal is to predict the vote (?yea?
or ?nay?)
for thespeaker of each speech segment.
For our experi-ments, we evaluate our methods using the speaker-based speech-segment classification setting as de-scribed in Thomas et al (2006).53Datasets in the required format for SVMsle are available athttp://www.cs.cornell.edu/?ainur/data.html4Since the rationale annotations are available for nine out of10 folds, we used the 10-th fold as the blind test set.
We trainednine different models on subsets of size eight, used the remain-ing fold as the validation set, and then measured the averageperformance on the final test set.5In the other setting described in Thomas et al (2006)(segment-based speech-segment classification), around 39% of1051Table 1: Summary of the experimental results for the Movie Reviews (top) and U.S. Congressional Floor Debates(bottom) datasets using SVMsle, SVMsle w/ Prior and SVMslefs with and without proximity features.INITIALIZATION SVMsle + Prox.Feat.
SVMsle+ Prox.Feat.
SVMslefs + Prox.Feat.w/ PriorRandom 30% 87.22 85.44 87.61 87.56 89.50 88.22Last 30% 89.72 ?
88.83 90.50 ?
90.00 ?
91.06 ?
91.22 ?OpinionFinder 91.28 ?
90.89 ?
91.72 ?
93.22 ?
92.50 ?
92.39 ?Annot.Rationales 91.61 ?
92.00 ?
92.67 ?
92.00 ?
92.28 ?
93.22 ?INITIALIZATION SVMsle + Prox.Feat.
SVMsle+ Prox.Feat.
SVMslefs + Prox.Feat.w/ PriorRandom 30% 78.84 73.14 78.49 76.40 77.33 73.84Last 30% 73.26 73.95 71.51 73.60 67.79 73.37OpinionFinder 77.33 79.53 77.09 78.60 77.67 77.09?
For Movie Reviews, the SVM baseline accuracy is 88.56%.
A ?
(or ?)
indicates statically significantly better performance thanbaseline according to the paired t-test with p < 0.001 (or p < 0.05).?
For U.S. Congressional Floor Debates, the SVM baseline accuracy is 70.00%.
Statistical significance cannot be calculated becausethe data comes in a single split.Since our training procedure solves a non-convexoptimization problem, it requires an initial guess ofthe explanatory sentences.
We use an explanatoryset size (5) of 30% of the number of sentences ineach document, L = d0.3 ?
|x|e, with a lower cap of1.
We generate initializations using OpinionFinder(Wilson et al, 2005), which were shown to be areasonable substitute for human annotations in theMovie Reviews dataset (Yessenalina et al, 2010).6We consider two additional (baseline) methodsfor initialization: using a random set of sentences,and using the last 30% of sentence in the document.In the Movie Reviews dataset, we also use sentencescontaining human-annotator rationales as a final ini-tialization option.
No such manual annotations areavailable for the Congressional Debates.5.2 Experimental ResultsWe evaluate three versions of our model: the ini-tial model (3) which we call SVMsle (SVMs forSentiment classification with Latent Explanations),SVMsle regularized relative to a prior as described inthe documents in the whole dataset contain only 1-3 sentences,making it an uninteresting setting to analyze with our model.6We select all sentences whose majority vote of Opinion-Finder word-level polarities matches the document?s sentiment.If there are fewer than L sentences, we add sentences startingfrom the end of the document.
If there are more, we removesentences starting from the beginning of the document.Section 4.5.1 which we refer to as SVMsle w/ Prior,7and the feature smoothing model described in Sec-tion 4.5.2 which we call SVMslefs .
Due to the diffi-culty of selecting a good prior, we expect SVMslefs toexhibit the most robust performance.Table 1 shows a comparison of our proposedmethods on the two datasets.
We observe thatSVMslefs provides both strong and robust perfor-mance.
The performance of SVMsle is generally bet-ter when trained using a prior than not in the MovieReviews dataset.
Both extensions appear to hurtperformance in the U.S. Congressional Floor De-bates dataset.
Using OpinionFinder to initialize ourtraining procedure offers good performance acrossboth datasets, whereas the baseline initializationsexhibit more erratic performance behavior.8 Unsur-prisingly, initializing using human annotations (inthe Movie Reviews dataset) can offer further im-provement.
Adding proximity features (as describedin Section 4.4) in general seems to improve perfor-mance when using a good initialization, and hurtsperformance otherwise.7We either used the same value of C to train both standardSVM model and SVMsle w/ Prior or used the best standardSVM model on the validation set to train SVMsle w/ Prior.
Wechose the combination that works the best on the validation set.8Using the random initialization on the U.S. CongressionalFloor Debates dataset offers surprisingly good performance.1052Table 2: Comparison of SVMslefs with previous work onthe Movie Reviews dataset.
We considered two settings:when human annotations are available (Annot.
Labels),and when they are unavailable (No Annot.
Labels).METHOD ACCBaseline SVM 88.56Annot.
Zaidan et al (2007) 92.20Labels SVMslefs 92.28SVMslefs + Prox.Feat.
93.22No Annot.
Yessenalina et al (2010) 91.78Labels SVMslefs 92.50SVMslefs +Prox.Feat.
92.39Table 3: Comparison of SVMslefs with previous work onthe U.S. Congressional Floor Debates dataset for thespeaker-based segment classification task.METHOD ACCBaseline SVM 70.00Prior work Thomas et al (2006) 71.28Bansal et al (2008) 75.00Our work SVMslefs 77.67SVMslefs + Prox.Feat.
77.09Tables 2 and 3 show a comparison of SVMslefs withprevious work on the Movie Reviews and U.S. Con-gressional Floor Debates datasets, respectively.
Forthe Movie Reviews dataset, we considered two set-tings: when human annotations are available, andwhen they are not (in which case we initialized usingOpinionFinder).
For the U.S. Congressional FloorDebates dataset we used only the latter setting, sincethere are no annotations available for this dataset.
Inall cases we observe SVMslefs showing improved per-formance compared to previous results.Training details.
We tried around 10 differentvalues for C parameter, and selected the final modelbased on the validation set.
The training proce-dure alternates between training a standard struc-tural SVM model and using the subsequent modelto re-label the latent variables.
We selected the halt-ing iteration of the training procedure using the val-idation set.
When initializing using human annota-tions for the Movie Reviews dataset, the halting iter-ation is typically the first iteration, whereas the halt-ing iteration is typically chosen from a later iterationFigure 1: Overlap of extracted sentences from differentSVMslefs models on the Movie Reviews training set.Figure 2: Test accuracy on the Movie Reviews dataset forSVMslefs while varying extraction size.when initializing using OpinionFinder.Figure 1 shows the per-iteration overlap of ex-tracted sentences from SVMslefs models initialized us-ing OpinionFinder and human annotations on theMovie Reviews training set.
We can see that train-ing has approximately converged after about 10 it-erations.9 We can also see that both models itera-tively learn to extract sentences that are more similarto each other than their respective initializations (theoverlap between the two initializations is 57%).
Thisis an indicator that our learning problem, despite be-ing non-convex and having multiple local optima,has a reasonably large ?good?
region that can be ap-proached using different initialization methods.Varying the extraction size.
Figure 2 shows howaccuracy on the test set of SVMslefs changes on theMovie Reviews dataset as a function of varying theextraction size f(|x|) from (5).
We can see that per-formance changes smoothly10 (and so is robust), andthat one might see further improvement from more9The number of iterations required to converge is an upperbound on the number of iterations from which to choose thehalting iteration (based on a validation set).10The smoothness will depend on the initialization.1053Table 4: Example ?yea?
speech with Latent Explanations from the U.S. Congressional Floor Debates dataset predictedby SVMslefs with OpinionFinder initialization.
Latent Explanations are preceded by solid circles with numbers denotingtheir preference order (1 being most preferred by SVMslefs ).
The five least subjective sentences are preceded by circleswith numbers denoting the subjectivity order (1 being least subjective according to SVMslefs ).?
Mr. Speaker, I am proud to standon the house floor today to speak infavor of the Stem Cell Research En-hancement Act, legislation which willbring hope to millions of people suffer-ing from disease in this nation.
?
Iwant to thank Congresswoman Degetteand Congressman Castle for their tire-less work in bringing this bill to thehouse floor for a vote.?
The discovery of embryonic stemcells is a major scientific breakthrough.?
Embryonic stem cells have the po-tential to form any cell type in thehuman body.
This could have pro-found implications for diseases such asAlzheimer?s, Parkinson?s, various formsof brain and spinal cord disorders, dia-betes, and many types of cancer.
?
Ac-cording to the Coalition for the Ad-vancement of Medical Research, thereare at least 58 diseases which could po-tentially be cured through stem cell re-search.That is why more than 200 majorpatient groups, scientists, and medicalresearch groups and 80 Nobel Laure-ates support the Stem Cell Research En-hancement Act.
?
They know that thislegislation will give us a chance to findcures to diseases affecting 100 millionAmericans.I want to make clear that I oppose re-productive cloning, as we all do.
I havevoted against it in the past.
?
However,that is vastly different from stem cell re-search and as an ovarian cancer sur-vivor, I am not going to stand in the wayof science.Permitting peer-reviewed Federalfunds to be used for this research,combined with public oversight of theseactivities, is our best assurance thatresearch will be of the highest qualityand performed with the greatest dignityand moral responsibility.
The policyPresident Bush announced in August2001 has limited access to stem celllines and has stalled scientific progress.As a cancer survivor, I know the des-peration these families feel as they waitfor a cure.
?
This congress must notstand in the way of that progress.
?
Wehave an opportunity to change the livesof millions, and I hope we take it.
?
Iurge my colleagues to support this leg-islation.careful tuning of the size constraint.Examining an example prediction.
Our pro-posed methods are not designed to extract inter-pretable explanations, but examining the extractedexplanations might still yield meaningful informa-tion.
Table 4 contains an example speech from theU.S.
Congressional Floor Debates test set, with La-tent Explanations found by SVMslefs highlighted inboldface.
This speech was made in support of theStem Cell Research Enhancement Act.
For com-parison, Table 4 also shows the five least subjectivesentences according to SVMslefs .
Notice that most ofthese ?objective?
sentences can plausibly belong tospeeches made in opposition to bills that limit stemcell research funding.
That is, they do not clearly in-dicate the speaker?s stance towards the specific billin question.
We can thus see that our approach canindeed learn to infer sentences that are essential tounderstanding the document-level sentiment.6 DiscussionMaking good structural assumptions simplifies thedevelopment process.
Compared to methods thatmodify the training of flat document classifiers (e.g.,Zaidan et al (2007)), our approach uses fewer pa-rameters, leading to a more compact and faster train-ing stage.
Compared to methods that use a cascadedapproach (e.g., Pang and Lee (2004)), our approachis more robust to errors in the lower-level subtaskdue to being a joint model.Introducing latent variables makes the trainingprocedure more flexible by not requiring lower-levellabels, but does require a good initialization (i.e., areasonable substitute for the lower-level labels).
Webelieve that the widespread availability of off-the-shelf sentiment lexicons and software, despite beingdeveloped for a different domain, makes this issueless of a concern, and in fact creates an opportunityfor approaches like ours to have real impact.One can incorporate many types of sentence-levelinformation that cannot be directly incorporated intoa flat model.
Examples include scores from anothersentence-level classifier (e.g., from Nakagawa et.
al(2010)) or combining phrase-level polarity scores(e.g., from Choi and Cardie (2008)) for each sen-tence, or features that describe the position of thesentence in the document.Most prior work on the U.S. Congressional FloorDebates dataset focused on using relationships be-tween speakers such as agreement (Thomas et al,2006; Bansal et al, 2008), and used a global min-cut inference procedure.
However, they require all1054test instances to be known in advance (i.e., their for-mulations are transductive).
Our method is not lim-ited to the transductive setting, and instead exploitsa different and complementary structure: the latentexplanation (i.e., only some sentences in the speechare indicative of the speaker?s vote).In a sense, the joint feature structure used inour model is the simplest that could be used.
Ourmodel makes no explicit structural dependencies be-tween sentences, so the choice of whether to extracteach sentence is essentially made independently ofother sentences in the document.
More sophisticatedstructures can be used if appropriate.
For instance,one can formulate the sentence extraction task asa sequence labeling problem similar to (McDonaldet al, 2007), or use a more expressive graphicalmodel such as in (Pang and Lee, 2004; Thomas etal., 2006).
So long as the global inference proce-dure is tractable or has a good approximation al-gorithm, then the training procedure is guaranteedto converge with rigorous generalization guarantees(Finley and Joachims, 2008).
Since any formulationof the extraction subtask will suppress informationfor the main document-level task, one must take careto properly incorporate smoothing if necessary.Another interesting direction is training models topredict not only sentiment polarity, but also whethera document is objective.
For example, one can posea three class problem (?positive?, ?negative?, ?ob-jective?
), where objective documents might not nec-essarily have a good set of (subjective) explanatorysentences, similar to (Chang et al, 2010).7 ConclusionWe have presented latent variable structured mod-els for the document sentiment classification task.These models do not rely on sentence-level an-notations, and are trained jointly (over both thedocument and sentence levels) to directly optimizedocument-level accuracy.
Experiments on two stan-dard sentiment analysis datasets showed improvedperformance over previous results.Our approach can, in principle, be applied to anyclassification task that is well modeled by jointlysolving an extraction subtask.
However, as evi-denced by our experiments, proper training does re-quire a reasonable initial guess of the extracted ex-planations, as well as ways to mitigate the risk ofthe extraction subtask suppressing too much infor-mation (such as via feature smoothing).AcknowledgmentsThis work was supported in part by National ScienceFoundation Grants BCS-0904822, BCS-0624277, IIS-0535099; by a gift from Google; and by the Departmentof Homeland Security under ONR Grant N0014-07-1-0152.
The second author was also supported in part bya Microsoft Research Graduate Fellowship.
The authorsthank Yejin Choi, Thorsten Joachims, Nikos Karampatzi-akis, Lillian Lee, Chun-Nam Yu, and the anonymous re-viewers for their helpful comments.AppendixRecall that all the ?subj and ?pol vectors have unit 2-norm, which is assumed here to be desirable.
We nowshow that using N(x) =?f(|x|) achieves a similarproperty for ?
(x, y, s).
We can write the squared 2-normof ?
(x, y, s) as|?
(x, y, s)|2 =1N(x)2??
?j?sy ?
?pol(xj) + ?subj(xj)??2=1f(|x|)??????j?s?pol(xj)??2+???j?s?subj(xj)??2???
,where the last equality follows from the fact that?pol(xj)T?subj(xj) = 0,due to the two vectors using disjoint feature spaces byconstruction.
The summation of the ?pol(xj) terms iswritten as???j?s?pol(xj)??2=?j?s?i?s?pol(xj)T?pol(xi)?
?j?s?pol(xj)T?pol(xj) (8)=?j?s1 ?
f(|x|),where (8) follows from the sparsity assumption that?i 6= j : ?pol(xj)T?pol(xi) ?
0.A similar argument applies for the ?subj(xj) terms.Thus, by choosing N(x) =?f(|x|) the joint featurevectors ?
(x, y, s) will have approximately equal magni-tude as measured using the 2-norm.1055ReferencesMohit Bansal, Claire Cardie, and Lillian Lee.
2008.
Thepower of negative thinking: Exploiting label disagree-ment in the min-cut classification framework.
In In-ternational Conference on Computational Linguistics(COLING).Ming-Wei Chang, Dan Goldwasser, Dan Roth, and VivekSrikumar.
2010.
Discriminative learning over con-strained latent representations.
In Conference of theNorth American Chapter of the Association for Com-putational Linguistics (NAACL).Yejin Choi and Claire Cardie.
2008.
Learning with com-positional semantics as structural inference for subsen-tential sentiment analysis.
In Empirical Methods inNatural Language Processing (EMNLP).James Clarke, Dan Goldwasser, Ming-Wei Chang, andDan Roth.
2010.
Driving semantic parsing from theworld?s response.
In ACL Conference on Natural Lan-guage Learning (CoNLL), July.Pedro Felzenszwalb, David McAllester, and Deva Ra-manan.
2008.
A discriminatively trained, multiscale,deformable part model.
In IEEE Conference on Com-puter Vision and Pattern Recognition (CVPR).Thomas Finley and Thorsten Joachims.
2008.
Train-ing structural svms when exact inference is intractable.In International Conference on Machine Learning(ICML).Thorsten Joachims, Thomas Finley, and Chun-Nam Yu.2009.
Cutting plane training of structural svms.
Ma-chine Learning, 77(1):27?59.Yi Mao and Guy Lebanon.
2006.
Isotonic conditionalrandom fields and local sentiment flow.
In Neural In-formation Processing Systems (NIPS).Ryan McDonald, Kerry Hannan, Tyler Neylon, MikeWells, and Jeff Reynar.
2007.
Structured models forfine-to-coarse sentiment analysis.
In Annual Meet-ing of the Association for Computational Linguistics(ACL).Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.2010.
Dependency tree-based sentiment classificationusing crfs with hidden variables.
In Conference of theNorth American Chapter of the Association for Com-putational Linguistics (NAACL).Bo Pang and Lillian Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Annual Meeting of the As-sociation for Computational Linguistics (ACL).Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in Infor-mation Retrieval, 2(1-2):1?135.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification using ma-chine learning techniques.
In Empirical Methods inNatural Language Processing (EMNLP).Slav Petrov and Dan Klein.
2007.
Discriminative log-linear grammars with latent variables.
In Neural In-formation Processing Systems (NIPS).Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Getout the vote: Determining support or opposition fromCongressional floor-debate transcripts.
In EmpiricalMethods in Natural Language Processing (EMNLP).Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Empirical Methods in NaturalLanguage Processing (EMNLP).Ainur Yessenalina, Yejin Choi, and Claire Cardie.
2010.Automatically generating annotator rationales to im-prove sentiment classification.
In Annual Meeting ofthe Association for Computational Linguistics (ACL).Chun-Nam Yu and Thorsten Joachims.
2009.
Learningstructural svms with latent variables.
In InternationalConference on Machine Learning (ICML).Alan L. Yuille and Anand Rangarajan.
2003.
Theconcave-convex procedure.
Neural Computation,15(4):915?936, April.Omar F. Zaidan and Jason Eisner.
2008.
Modeling an-notators: a generative approach to learning from an-notator rationales.
In Empirical Methods in NaturalLanguage Processing (EMNLP).Omar F. Zaidan, Jason Eisner, and Christine Piatko.2007.
Using ?annotator rationales?
to improve ma-chine learning for text categorization.
In Conferenceof the North American Chapter of the Association forComputational Linguistics (NAACL).1056
