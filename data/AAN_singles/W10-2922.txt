Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 182?191,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsOnline Entropy-based Model of Lexical Category AcquisitionGrzegorz Chrupa?aSaarland Universitygchrupala@lsv.uni-saarland.deAfra AlishahiSaarland Universityafra@coli.uni-saarland.deAbstractChildren learn a robust representation oflexical categories at a young age.
We pro-pose an incremental model of this processwhich efficiently groups words into lexi-cal categories based on their local contextusing an information-theoretic criterion.We train our model on a corpus of child-directed speech from CHILDES and showthat the model learns a fine-grained setof intuitive word categories.
Furthermore,we propose a novel evaluation approachby comparing the efficiency of our inducedcategories against other category sets (in-cluding traditional part of speech tags) ina variety of language tasks.
We show thecategories induced by our model typicallyoutperform the other category sets.1 The Acquisition of Lexical CategoriesPsycholinguistic studies suggest that early on chil-dren acquire robust knowledge of the abstract lex-ical categories such as nouns, verbs and deter-miners (e.g., Gelman & Taylor, 1984; Kemp etal., 2005).
Children?s grouping of words intocategories might be based on various cues, in-cluding phonological and morphological proper-ties of a word, the distributional information aboutits surrounding context, and its semantic features.Among these, the distributional properties of thelocal context of a word have been thoroughly stud-ied.
It has been shown that child-directed speechprovides informative co-occurrence cues, whichcan be reliably used to form lexical categories(Redington et al, 1998; Mintz, 2003).The process of learning lexical categories bychildren is necessarily incremental.
Human lan-guage acquisition is bounded by memory and pro-cessing limitations, and it is implausible that hu-mans process large volumes of text at once andinduce an optimum set of categories.
Efficient on-line computational models are needed to investi-gate whether distributional information is equallyuseful in an online process of word categoriza-tion.
However, the few incremental models ofcategory acquisition which have been proposedso far are generally inefficient and over-sensitiveto the properties of the input data (Cartwright &Brent, 1997; Parisien et al, 2008).
Moreover, theunsupervised nature of these models makes theirassessment a challenge, and the evaluation tech-niques proposed in the literature are limited.The main contributions of our research aretwofold.
First, we propose an incremental en-tropy model for efficiently clustering words intocategories given their local context.
We train ourmodel on a corpus of child-directed speech fromCHILDES (MacWhinney, 2000) and show that themodel learns a fine-grained set of intuitive wordcategories.
Second, we propose a novel evalua-tion approach by comparing the efficiency of ourinduced categories against other category sets, in-cluding the traditional part of speech tags, in a va-riety of language tasks.
We evaluate our model onword prediction (where a missing word is guessedbased on its sentential context), semantic inference(where the semantic properties of a novel word arepredicted based on the context), and grammatical-ity judgment (where the syntactic well-formednessof a sentence is assessed based on the category la-bels assigned to its words).
The results show thatthe categories induced by our model can be suc-cessfully used in a variety of tasks and typicallyperform better than other category sets.1.1 Unsupervised Models of CategoryInductionSeveral computational models have used distri-butional information for categorizing words (e.g.Brown et al, 1992; Redington et al, 1998; Clark,2000; Mintz, 2002).
The majority of these mod-182els partition the vocabulary into a set of optimumclusters (e.g., Brown et al, 1992; Clark, 2000).The generated clusters are intuitive, and can beused in different tasks such as word predictionand parsing.
Moreover, these models confirm thelearnability of abstract word categories, and showthat distributional cues are a useful source of in-formation for this purpose.
However, (i) they cat-egorize word types rather than word tokens, andas such provide no account of words belonging tomore than one category, and (ii) the batch algo-rithms used by these systems make them implau-sible for modeling human category induction.
Un-supervised models of PoS tagging such as Gold-water & Griffiths (2007) do assign labels to word-tokens, but they still typically use batch process-ing, and what is even more problematic, they hard-wire important aspects of the model, such as thefinal number of categories.Only few previously proposed models processdata incrementally, categorize word-tokens and donot pre-specify a fixed category set.
The modelof Cartwright & Brent (1997) uses an algorithmwhich incrementally merges word clusters so thata Minimum Description Length criterion for atemplate grammar is optimized.
The model treatswhole sentences as contextual units, which sacri-fices a degree of incrementality, as well as makingit less robust to noise in the input.Parisien et al (2008) propose a Bayesian clus-tering model which copes with ambiguity and ex-hibits the developmental trends observed in chil-dren (e.g.
the order of acquisition of different cat-egories).
However, their model is overly sen-sitive to context variability, which results in thecreation of sparse categories.
To remedy this is-sue they introduce a ?bootstrapping?
componentwhere the categories assigned to context words areuse to determine the category of the current targetword.
They also perform periodical cluster reorga-nization.
These mechanisms improve the overallperformance of the model when trained on largeamounts of training data, but they complicate themodel with ad-hoc extensions and add to the (al-ready considerable) computational load.What is lacking is an incremental model of lex-ical category which can efficiently process natu-ralistic input data and gradually build robust cate-gories with little training data.1.2 Evaluation of the Induced CategoriesThere is no standard and straightforward methodfor evaluating the unsupervised models of cate-gory learning (see Clark, 2003, for discussion).Many unsupervised models of lexical category ac-quisition treat the traditional part of speech (PoS)tags as the gold standard, and measure the accu-racy and completeness of their induced categoriesbased on how closely they resemble the PoS cate-gories (e.g.
Redington et al, 1998; Mintz, 2003;Parisien et al, 2008).
However, it is not at allclear whether humans form the same types of cate-gories.
In fact, many language tasks might benefitfrom finer-grained categories than the traditionalPoS tags used for corpus annotation.Frank et al (2009) propose a different, automat-ically generated set of gold standard categories forevaluating an unsupervised categorization model.The gold-standard categories are formed accord-ing to ?substitutability?
: if one word can be re-placed by another and the resulting sentence is stillgrammatical, then there is a good chance that thetwo words belong to the same category.
They ex-tract 3-word frames from the training data, andform the gold standard categories based on thewords that appear in the same frame.
They em-phasize that in order to provide some degree ofgeneralization, different data sets must be used forforming the gold-standard categories and perform-ing the evaluation.
However, the resulting cate-gories are bound to be incomplete, and using themas gold standard inevitably favors categorizationmodels which use a similar frame-based principle.All in all, using any set of gold standard cate-gories for evaluating an unsupervised categoriza-tion model has the disadvantage of favoring oneset of principles and intuitions over another; thatis, assuming that there is a correct set of cate-gories which the model should converge to.
Al-ternatively, automatically induced categories canbe evaluated based on how useful they are in per-forming different tasks.
This approach is taken byClark (2000), where the perplexity of a finite-statemodel is used to compare different category sets.We build on this idea and propose a more gen-eral usage-based approach to evaluating the auto-matically induced categories from a data set, em-phasizing that the ultimate goal of a category in-duction model is to form categories that can be ef-ficiently used in a variety of language tasks.
Weargue that for such tasks, a finer-grained set of cat-183egories might be more appropriate than the coarse-grained PoS categories.
Therefore, we propose anumber of tasks for which we compare the perfor-mance based on various category sets, includingthose induced by our model.2 An Incremental Entropy-based Modelof Category InductionA model of human category acquisition shouldpossess two key features:?
It should process input as it arrives, and incre-mentally update the current set of clusters.?
The set of clusters should not be fixed in ad-vance, but rather determined by the charac-teristics of the input data.We propose a simple algorithm which fulfills thosetwo conditions.Our goal is to categorize word usages based onthe similarity of their form (the content) and theirsurrounding words (the context).
While groupingword usages into categories, we attempt to tradeoff two conflicting criteria.
First, the categoriesshould be informative about the properties of theirmembers.
Second, the number and distribution ofthe categories should be parsimonious.
An appro-priate tool for formalizing both informativenessand parsimony is information-theoretic entropy.The parsimony criterion can be formalized asthe entropy of the random variable (Y ) represent-ing the cluster assignments:H(Y ) = ?NXi=1P (Y = yi) log2(P (Y = yi)) (1)where N is the number of clusters and P (Y = yi)stands for the relative size of the ith cluster.The informativeness criterion can be formalizedas the conditional entropy of training examples(X) given the cluster assignments:H(X|Y ) =NXi=1P (Y = yi)H(X|Y = yi) (2)and H(X|Y = yi) is calculated asH(X|Y = yi) = ?TXj=1[P (X = xj |Y = yi)?
log2(P (X = xj |Y = yi)] (3)where T is the number of word usages in the train-ing set.The two criteria presented by Equations 1 and2 can be combined together as the joint entropy ofthe two random variables X and Y :H(X,Y ) = H(X|Y ) +H(Y ) (4)For a random variableX corresponding to a sin-gle feature, minimizing the joint entropyH(X,Y )will trade off our two desired criteria.The joint entropy will be minimal if each dis-tinct value of variable X is assigned the same cat-egory (i.e.
same value of Y ).
There are manyassignments which satisfy this condition.
Theyrange from putting all values of X in a single cat-egory, to having a unique category for each uniquevalue of X .
We favor the latter solution algorith-mically by creating a new category in case of ties.Finally, since our training examples contain abundle of categorical features, we minimize thejoint entropy simultaneously for all the features.We consider our training examples to be vectorsof random variables (Xj)Mj=1, where each randomvariable corresponds to one feature.
For an incom-ing example we will choose the cluster assignmentwhich leads to the least increase in the joint en-tropy H(Xj , Y ), summed over all the features j:MXj=1H(Xj , Y ) =MXj=1?H(Xj |Y ) +H(Y )?
(5)=MXj=1?H(Xj |Y )?+M ?H(Y )In the next section, we present an incrementalalgorithm which uses this criterion for inducingcategories from a sequence of input data.The Incremental Algorithm.
For each word us-age that the model processes at time t, we need tofind the best category among the ones that havebeen formed so far, as well as a potential new cat-egory.
The decision is made based on the changein the function?Mj=1H(Xj , Y ) (Equation 5) frompoint t ?
1 to point t, as a result of assigning thecurrent input xt to a category y:?Hty =MXj=1?Hty(Xj , Y )?Ht?1(Xj , Y )?
(6)whereHty(X,Y ) is the joint entropy of the assign-ment Y for the input X = {x1, .
.
.
, xt}, after thelast input item xt is assigned to the category y.The winning category y?
is the one that leads to thesmallest increase.
Ties are broken by preferring anew category.y?
=(argminy?
{y}Ni=1 ?Hty if ?yn[?Htyn < ?HtyN+1 ]yN+1 otherwise(7)184where N is the number of categories created up topoint t, and yN+1 represents a new category.Efficiency.
We maintain the relative size P t(y)and the entropy H(Xj |Y = y) for each categoryy over time.
When performing an assignment of xtto a category yi, we only need to update the condi-tional entropies H(Xj |Y = yi) for all features Xjfor this particular category, since other categorieshave not changed.
For a feature Xj at point t, thechange in the conditional entropy for the selectedcategory yi is given by:?Htyi(Xj |Y ) = Htyi(Xj |Y )?Ht?1(Xj |Y )=Xyk 6=yi?P (Y = yk)Ht?1(Xj |Y = yi)??
P t?1(Y = yi)Ht?1(X|Y = yi)?
P t(Y = yi)Ht(Xj |Y = yi)where only the last term depends on the currenttime index t. Therefore, the entropy H(Xj |Y ) ateach step can be efficiently updated by calculatingthis term for the modified category at that step.A number of previous studies have consideredentropy-based criteria for clustering (e.g.
Barbara?et al, 2002; Li et al, 2004).
The main contri-bution of our proposed model is the emphasis onrarely explored combination of the two character-istics we consider crucial for modeling human cat-egory acquisition, incrementality and an open setof clusters.3 Experimental SetupWe evaluate the categories formed by our modelthrough three different tasks.
The first task is wordprediction, where a target word is predicted basedon the sentential context it appears in.
The secondtask is to infer the semantic properties of a novelword based on its context.
The third task is to as-sess the grammaticality of a sentence tagged withcategory labels.
We run our model on a corpus ofchild-directed speech, and use the categories that itinduces from that corpus in the above-mentionedtasks.
For each task, we compare the performanceusing our induced categories against the perfor-mance using other category sets.
In the follow-ing sections, we describe the properties of the datasets used for training and testing the model, andthe formation of other category sets against whichwe compare our model.Data Set Sessions #Sentences #WordsTraining 26?28 22, 491 125, 339Development 29?30 15, 193 85, 361Test 32?33 14, 940 84, 130Table 1: Experimental data3.1 Input DataWe use the Manchester corpus (Theakston et al,2001) from CHILDES database (MacWhinney,2000) as experimental data.
The Manchester cor-pus consists of conversations with 12 children be-tween the ages of eighteen months to three yearsold.
The corpus is manually tagged using 60 PoSlabels.
We use the mother?s speech from tran-scripts of 6 children, remove punctuation, and con-catenate the corresponding sessions.We used data from three sessions as the trainingset, two sessions as the development set, and twosessions as the test set.
We discarded all one-wordsentences from the data sets, as they do not pro-vide any context for our evaluation tasks.
Table 1summarizes the properties of each data set.3.2 Category SetsWe define each word usage in the training or testdata set as a vector of three categorical features:the content feature (i.e., the focus word in a us-age), and two context features (i.e.
the precedingand following bigrams).
We ran our clustering al-gorithm on the training set, which resulted in aset of 944 categories (of which 442 have only onemember).
Table 3 shows two sample categoriesfrom the training set, and Figure 1 shows the sizedistribution of the categories.For each evaluation task, we use the followingcategory sets to label the test set:?H.
The categories induced by our entropy-based model from the training set, as de-scribed above.PoS.
The part-of-speech tags the Manchester cor-pus is annotated with.Words.
The set of all the word types in the dataset (i.e.
assuming that all the usages of thesame word form are grouped together).Parisien.
The induced categories by the model ofParisien et al (2008) from the training set.185Gold PoS Words Parisien ?HVI (0.000) 5.294 5.983 4.806ARI (1.000) 0.139 0.099 0.168Table 2: Comparison against gold PoS tags usingVariation of Information (VI) and Adjusted RandIndex (ARI).Sample Cluster 1going (928)doing (190)back (150)coming (80)looking (76)making (64)playing (55)taking (45).
.
.Sample Cluster 2than (45)more (20)silly (10)bigger (9)frightened (5)dark (4)harder (4)funny (3).
.
.Table 3: Sample categories induced from the train-ing data.
The frequency of each word in the cate-gory is shown in parentheses.For the first two tasks (word prediction and se-mantic inference), we do not use the content fea-ture in labeling the test set, since the assumptionunderlying both tasks is that we do not have ac-cess to the form of the target word.
Therefore,we do not measure the performance of these taskson the Words category set.
However, we do usethe content feature in labeling the test examples ingrammaticality judgment.For completeness, in Table 2 we report theresults of evaluation against Gold PoS tags us-ing two metrics, Variation of Information (Meila,2003) and Adjusted Rand Index (Hubert & Arabie,1985).4 Word PredictionHumans can predict a word based on the context itis used in with remarkable accuracy (e.g.
Lesher etal., 2002).
Different versions of this task such asCloze Test (Taylor, 1953) are used for the assess-ment of native and second language learning.We simulate this task, where a missing word ispredicted based on its context.
We use each of thecategory sets introduced in Section 3.2 to label aword usage in the test set, without using the wordform itself as a feature.
That is, we assume thatthe target word is unknown, and find the best cat-egory for it based only on its surrounding context.5 50 500 5000125102050100Category size frequenciesSizeFrequencyFigure 1: The distribution of the induced cate-gories based on their sizeWe then output a ranked list of the content featurevalues of the selected category as the predictionof the model for the target word.
To evaluate thisprediction, we use the reciprocal rank of the targetword in the predicted list.The third row of Table 4 shows the Mean Re-ciprocal Rank (MRR) over all the word usages inthe test data across different category sets.
The re-sults show that the category labels predicted by ourmodel (?H) perform much better than those ofParisien, but still not as good as the gold-standardpart of speech categories.
The fact that PoS tagsare better here does not necessarily mean that thePoS category set is better for word prediction assuch, since they are manually assigned and thusnoise-free, unlike the automatic category labelspredicted by the two models.
In the second setof experiments described below we try to factor inthe uncertainty about category assignment inher-ent in automatic labeling.Using only the best category output by themodel to produce word predictions is simple andneutral; however, it discards part of the informa-tion learned by the model.
We can predict wordsmore accurately by combining information fromthe whole ranked list of category labels.We use the ?H model to rank the values of thecontent feature in the following fashion: for thecurrent test usage, we rank each cluster assign-ment y by the change in the ?Htyi function thatit causes.
For each of the assignments, we com-pute the relative frequencies P (w|yi) of each pos-sible focus word.
The final rank of the word w incontext h is determined by the sum of the cluster-186Gold PoS Words Parisien ?HWord Prediction (MRR) 0.354 - 0.212 0.309Semantic Inference (MAP) 0.351 - 0.213 0.366Grammaticality Judgment (Accuracy) 0.728 0.685 0.683 0.715Table 4: The performance in each of the three tasks using different category sets.dependent relative frequencies weighted by thenormalized reciprocal ranks of the clusters:P (w|h) =NXi=1P (w|yi)R(yi|h)?1PNi=1 R(yi|h)?1(8)where R(yi|h)?1 is the reciprocal rank of clusteryi for context h according to the model.We compare the performance of the ?H modelwith this word-prediction method to that of ann-gram language model, which is an establishedtechnique for assigning probabilities to wordsbased on their context.
For the language modelwe use several n-gram orders (n = 1 .
.
.
5), andsmooth the n-gram counts using absolute dis-counting (Zhai & Lafferty, 2004).
The probabilityof the word w given the context h is given by thefollowing model of order n:Pn(w|h) = max`0,c(h,w)?
dc(h)?+ ?
(h)Pn?1(w|h) (9)where d is the discount parameter, c(?)
is the fre-quency count function, Pn?1 is the lower-orderback-off distribution, and ?
is the normalizationfactor:?
(h) =(1 if r(h) = 0dr(h) 1c(h) otherwise(10)and r(h) is the number of distinct words that fol-low context h in the training corpus.In addition to the ?H model and the n-gramlanguage models, we also report how well wordscan be predicted from their manually assigned PoStags from CHILDES: for each token we predict themost likely word given the token?s true PoS tagbased on frequencies in the training data.Table 4 summarizes the evaluation results.
The?H model can predict missing words better thanany of the n-gram language models, and evenslightly better than the true POS tags.
Given thesimplicity of our clustering model, this is a veryencouraging result.
Simple n-gram language mod-els are known for providing quite a strong base-line for word prediction; for example, Brown etal.
(1992)?s class-based language model failed toModel MRRLM n = 1 0.1253LM n = 2 0.2884LM n = 3 0.3278LM n = 4 0.3305LM n = 5 0.3297?H 0.3591Gold POS 0.3540Table 5: Mean reciprocal rank on the word predic-tion task on the test setimprove test-set perplexity over a word-based tri-gram model.5 Semantic InferenceSeveral experimental studies have shown that chil-dren and adults can infer (some aspects of) the se-mantic properties of a novel word based on thecontext it appears in (e.g.
Landau & Gleitman,1985; Gleitman, 1990; Naigles & Hoff-Ginsberg,1995).
For example, in an experimental study byFisher et al (2006), two-year-olds watched as ahand placed a duck on a box, and pointed to it as anew word was uttered.
Half of the children heardthe word presented as a noun (This is a corp!
),while half heard it as a preposition (This is acorpmy box!).
After training, children heard a test sen-tence (What else is acorp (my box)?)
while watch-ing two test events: one showed another duck be-side the box, and the other showed a different ob-ject on the box.
Looking-preferences revealed ef-fects of sentence context: subjects in the preposi-tion condition interpreted the novel word as a lo-cation, whereas those in the noun condition inter-preted it as an object.To study a similar effect in our model, we as-sociate each word with a set of semantic features.For nouns, we extract the semantic features fromWordNet 3.0 (Fellbaum, 1998) as follows: Wetake all the hypernyms of the first sense of theword, and the first word in the synset of eachhypernym to the set of the semantic features of187ball?
GAME EQUIPMENT#1?
EQUIPMENT#1?
INSTRUMENTALITY#3, INSTRUMENTATION#1?
ARTIFACT#1, ARTEFACT#1?
WHOLE#2, UNIT#6?
OBJECT#1, PHYSICAL OBJECT#1?
PHYSICAL ENTITY#1?
ENTITY#1ball: { GAME EQUIPMENT#1,EQUIPMENT#1,INSTRUMENTALITY#3,ARTIFACT#1, ... }Figure 2: Semantic features of ball, as extractedfrom WordNet.the target word (see Figure 2 for an example).For verbs, we additionally extract features froma verb-specific resource, VerbNet 2.3 (Schuler,2005).
Due to lack of proper resources for otherlexical categories, we limit our evaluation to nounsand verbs.The semantic features of words are not used inthe formation of lexical categories.
However, ateach point of time in learning, we can associatea semantic profile to a category as the aggregatedset of the semantic features of its members: eachfeature in the set is assigned a count that indicatesthe number of the category members which havethat semantic property.
This is done for each ofthe category sets described in Section 3.2.As in the word-prediction task, we use differ-ent category sets to label each word usage in a testset based only on the context features of the word.When the model encounters a novel word, it canuse the semantic profile of the word?s labeled cat-egory as a prediction of the semantic properties ofthat word.
We can evaluate the quality of this pre-diction by comparing the true meaning represen-tation of the target word (i.e., its set of semanticfeatures according to the lexicon) against the se-mantic profile of the selected category.
We use theMean Average Precision (MAP) (Manning et al,2008) for comparing the ranked list of semanticfeatures predicted by the model with the flat setof semantic features extracted from WordNet andVerbNet.
Average Precision for a ranked list Fwith respect to a set R of correct features is:APR(F ) =1|R||F |Xr=1P (r)?
1R(Fr) (11)where P (r) is precision at rank r and 1R is theindicator function of set R.The middle row of Table 4 shows the MAPscores over all the noun or verb usages in thetest set, based on four different category sets.
Ascan be seen, the categories induced by our model(?H) outperform all the other category sets.
Theword-type categories are particularly unsuitablefor this task, since they provide the least degreeof generalization over the semantic properties ofa group of words.
The categories of Parisienet al (2008) result in a better performance thanword types, but they are still too sparse for thistask.
However, the average score gained by part ofspeech tags is also lower than the one by our cat-egories.
This suggests that too broad categoriesare also unsuitable for this task, since they canonly provide predictions about the most generalsemantic properties, such as ENTITY for nouns,and ACTION for verbs.
These findings again con-firm our hypothesis that a finer-grained set of cat-egories that are extracted directly from the inputdata provide the highest predictive power in a nat-uralistic language task such as semantic inference.6 Grammaticality JudgmentSpeakers of a natural language have a generalagreement on the grammaticality of different sen-tences.
Grammaticality judgment has been viewedas one of the main criteria for measuring howwell a language is learned by a human learner.Experimental studies have shown that children asyoung as five years old can judge the grammati-cality of the sentences that they hear, and that bothchildren?s and adults?
grammaticality judgmentsare influenced by the distributional properties ofwords and their context (e.g., Theakston, 2004).Several methods have been proposed for auto-matically distinguishing between grammatical andungrammatical usages (e.g., Wagner et al, 2007).The ?shallow?
methods are mainly based on n-gram frequencies of words or categories in a cor-pus, whereas the ?deep?
methods treat a parsingfailure as an indication of a grammatical error.Since our focus is on evaluating our category set,we use trigram probabilities as a measure of gram-maticality, using Equation 9 with n = 3.As before, we label each test sentence using dif-ferent category sets, and calculate the probabilityfor each trigram in that sentence.
We define theoverall grammaticality score of a sentence as theminimum of the probabilities of all the trigrams inthat sentence.
Note that, unlike the previous tasks,here we do use the content word as a feature in188labeling a test word usage.
The actual word formaffects the grammaticality of its usage, and this in-formation is available to the human subjects whoevaluate the grammaticality of a sentence.Since we know of no publicly available corpusof ungrammatical sentences, we artificially con-struct one: for each sentence in our test data set,we randomly move one word to another position.1We define the accuracy of this task as the propor-tion of the test usages for which the model calcu-lates a higher grammaticality score for the originalsentence than for its ungrammatical version.The last row of Table 4 shows the accuracy ofthe grammaticality judgment task across differentcategory sets.
As can be seen, the highest accu-racy in choosing the grammatical sentence overthe ungrammatical one is achieved by using thePoS categories (0.728), followed by the categoriesinduced by our model (0.715).
These levels of ac-curacy are rather good considering that some ofthe automatically generated errors are also gram-matical (e.g., there you are vs. you are there, orcan you reach it vs. you can reach it).
The resultsby the other two category sets are lower and veryclose to each other.These results suggest that, unlike the semanticinference task, the grammaticality judgment taskmight require a coarser-grained set of categorieswhich provide a higher level of abstraction.
How-ever, taking into account that the PoS categoriesare manually assigned to the test usages, the dif-ference in their performance might be due to lackof noise in the labeling procedure.
We plan to in-vestigate this matter in future by improving ourcategorization model (as discussed in Section 7).Also, we intend to implement more accurate waysof estimating grammaticality, using an approachsimilar to that described for word prediction taskin Section 4.7 DiscussionWe have proposed an incremental model of lexi-cal category acquisition based on the distributionalproperties of words.
Our model uses an informa-tion theoretic clustering algorithm which attemptsto optimize the category assignments of the in-coming word usages at each point in time.
Themodel can efficiently process the training data, andinduce an intuitive set of categories from child-directed speech.
However, due to the incremen-1We used the software of Foster & Andersen (2009).tal nature of the clustering algorithm, it does notrevise its previous decisions according to the datathat it later receives.
A potential remedy would beto consider merging the clusters that have recentlybeen updated, in order to allow for recovery fromearly mistakes the model has made.We used the categories induced by our modelin word prediction, inferring the semantic prop-erties of novel words, and grammaticality judg-ment.
Our experimental results show that the per-formance in these tasks using our categories iscomparable or better than the performance basedon the manually assigned part of speech tags inour experimental data.
Furthermore, in all thesetasks the performance using our categories im-proves over a previous incremental categorizationmodel (Parisien et al, 2008).
However, the modelof Parisien employs a number of cluster reorgani-zation techniques which improve the overall qual-ity of the clusters after processing a substantialamount of input data.
In future we plan to increasethe size of our training data, and perform a moreextensive comparison with the model of Parisienet al (2008).The promising results of our experiments sug-gest that an information-theoretic approach is aplausible one for modeling the induction of lexi-cal categories from distributional data.
Our resultsimply that in many language tasks, a fine-grainedset of categories which are formed in response tothe properties of the input are more appropriatethan the coarser-grained part of speech categories.Therefore, the ubiquitous approach of using PoScategories as the gold standard in evaluating un-supervised category induction models needs to bereevaluated.
To further investigate this claim, infuture we plan to collect experimental data fromhuman subjects performing our suggested tasks,and measure the correlation between their perfor-mance and that of our model.AcknowledgmentsWe would like to thank Nicolas Stroppa forinsightful comments on our paper, and ChrisParisien for sharing the implementation of hismodel.
Grzegorz Chrupa?a was funded by theBMBF project NL-Search under contract number01IS08020B.
Afra Alishahi was funded by IRTG715 ?Language Technology and Cognitive Sys-tems?
provided by the German Research Founda-tion (DFG).189ReferencesBarbara?, D., Li, Y., & Couto, J.
(2002).
COOL-CAT: an entropy-based algorithm for categori-cal clustering.
In Proceedings of the EleventhInternational Conference on Information andKnowledge Management (pp.
582?589).Brown, P., Mercer, R., Della Pietra, V., & Lai,J.
(1992).
Class-based n-gram models of natu-ral language.
Computational linguistics, 18(4),467?479.Cartwright, T., & Brent, M. (1997).
Syntac-tic categorization in early language acquisition:Formalizing the role of distributional analysis.Cognition, 63(2), 121?170.Clark, A.
(2000).
Inducing syntactic categories bycontext distribution clustering.
In Proceedingsof the 2nd workshop on Learning Language inLogic and the 4th conference on ComputationalNatural Language Learning (pp.
91?94).Clark, A.
(2003).
Combining distributional andmorphological information for part of speechinduction.
In Proceedings of the 10th Confer-ence of the European Chapter of the Associationfor Computational Linguistics (pp.
59?66).Fellbaum, C.
(Ed.).
(1998).
WordNet, an elec-tronic lexical database.
MIT Press.Fisher, C., Klingler, S., & Song, H. (2006).
Whatdoes syntax say about space?
2-year-olds usesentence structure to learn new prepositions.Cognition, 101(1), 19?29.Foster, J., & Andersen, ?.
(2009).
GenERRate:generating errors for use in grammatical errordetection.
In Proceedings of the fourth work-shop on innovative use of nlp for building edu-cational applications (pp.
82?90).Frank, S., Goldwater, S., & Keller, F.(2009).
Eval-uating models of syntactic category acquisitionwithout using a gold standard.
In Proceedingsof the 31st Annual Meeting of the Cognitive Sci-ence Society.Gelman, S., & Taylor, M. (1984).
How two-year-old children interpret proper and commonnames for unfamiliar objects.
Child Develop-ment, 1535?1540.Gleitman, L.(1990).
The structural sources of verbmeanings.
Language acquisition, 1(1), 3?55.Goldwater, S., & Griffiths, T. (2007).
A fullyBayesian approach to unsupervised part-of-speech tagging.
In Proceedings of the 45th An-nual Meeting of the Association for Computa-tional Linguistics (Vol.
45, p. 744).Hubert, L., & Arabie, P. (1985).
Comparing parti-tions.
Journal of classification, 2(1), 193?218.Kemp, N., Lieven, E., & Tomasello, M. (2005).Young Children?s Knowledge of the?
Deter-miner?
and?
Adjective?
Categories.
Journalof Speech, Language and Hearing Research,48(3), 592?609.Landau, B., & Gleitman, L.(1985).
Language andexperience: Evidence from the blind child.
Har-vard University Press Cambridge, Mass.Lesher, G., Moulton, B., Higginbotham, D., & Al-sofrom, B.
(2002).
Limits of human word pre-diction performance.
Proceedings of the CSUN2002.Li, T., Ma, S., & Ogihara, M. (2004).
Entropy-based criterion in categorical clustering.
In Pro-ceedings of the 21st International Conferenceon Machine Learning (p. 68).MacWhinney, B.
(2000).
The CHILDES project:Tools for analyzing talk.
Lawrence Erlbaum As-sociates Inc, US.Manning, C., Raghavan, P., & Schtze, H. (2008).Introduction to Information Retrieval.
Cam-bridge University Press New York, NY, USA.Meila, M. (2003).
Comparing Clusterings by theVariation of Information.
In Learning theoryand kernel machines (pp.
173?187).
Springer.Mintz, T. (2002).
Category induction from distri-butional cues in an artificial language.
Memoryand Cognition, 30(5), 678?686.Mintz, T. (2003).
Frequent frames as a cue forgrammatical categories in child directed speech.Cognition, 90(1), 91?117.Naigles, L., & Hoff-Ginsberg, E. (1995).
Input toVerb Learning: Evidence for the Plausibility ofSyntactic Bootstrapping.
Developmental Psy-chology, 31(5), 827?37.Parisien, C., Fazly, A., & Stevenson, S. (2008).An incremental bayesian model for learningsyntactic categories.
In Proceedings of theTwelfth Conference on Computational NaturalLanguage Learning.Redington, M., Crater, N., & Finch, S.(1998).
Dis-tributional information: A powerful cue for ac-190quiring syntactic categories.
Cognitive Science:A Multidisciplinary Journal, 22(4), 425?469.Schuler, K. (2005).
VerbNet: A broad-coverage,comprehensive verb lexicon.
Unpublished doc-toral dissertation, University of Pennsylvania.Taylor, W. (1953).
Cloze procedure: A new toolfor measuring readability.
Journalism Quar-terly, 30(4), 415?433.Theakston, A.(2004).
The role of entrenchment inchildrens and adults performance on grammati-cality judgment tasks.
Cognitive Development,19(1), 15?34.Theakston, A., Lieven, E., Pine, J., & Rowland, C.(2001).
The role of performance limitations inthe acquisition of verb-argument structure: Analternative account.
Journal of Child Language,28(01), 127?152.Wagner, J., Foster, J., & van Genabith, J.(2007).
Acomparative evaluation of deep and shallow ap-proaches to the automatic detection of commongrammatical errors.
Proceedings of EMNLP-CoNLL-2007.Zhai, C., & Lafferty, J.(2004).
A study of smooth-ing methods for language models applied to in-formation retrieval.
ACM Transactions on In-formation Systems (TOIS), 22(2), 214.191
