Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 208?218,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsEfficient Top-Down BTG Parsing for Machine Translation PreorderingTetsuji NakagawaGoogle Japan Inc.tnaka@google.comAbstractWe present an efficient incremental top-down parsing method for preorderingbased on Bracketing Transduction Gram-mar (BTG).
The BTG-based preorderingframework (Neubig et al, 2012) can beapplied to any language using only par-allel text, but has the problem of compu-tational efficiency.
Our top-down parsingalgorithm allows us to use the early up-date technique easily for the latent vari-able structured Perceptron algorithm withbeam search, and solves the problem.Experimental results showed that the top-down method is more than 10 times fasterthan a method using the CYK algorithm.A phrase-based machine translation sys-tem with the top-down method had statis-tically significantly higher BLEU scoresfor 7 language pairs without relying onsupervised syntactic parsers, compared tobaseline systems using existing preorder-ing methods.1 IntroductionThe difference of the word order between sourceand target languages is one of major problems inphrase-based statistical machine translation.
In or-der to cope with the issue, many approaches havebeen studied.
Distortion models consider word re-ordering in decoding time using such as distance(Koehn et al, 2003) and lexical information (Till-man, 2004).
Another direction is to use more com-plex translation models such as hierarchical mod-els (Chiang, 2007).
However, these approachessuffer from the long-distance reordering issue andcomputational complexity.Preordering (reordering-as-preprocessing) (Xiaand McCord, 2004; Collins et al, 2005) is anotherapproach for tackling the problem, which modifiesthe word order of an input sentence in a source lan-guage to have the word order in a target language(Figure 1(a)).Various methods for preordering have beenstudied, and a method based on Bracketing Trans-duction Grammar (BTG) was proposed by Neubiget al (2012).
It reorders source sentences by han-dling sentence structures as latent variables.
Themethod can be applied to any language using onlyparallel text.
However, the method has the prob-lem of computational efficiency.In this paper, we propose an efficient incremen-tal top-down BTG parsing method which can beapplied to preordering.
Model parameters canbe learned using latent variable Perceptron withthe early update technique (Collins and Roark,2004), since the parsing method provides an easyway for checking the reachability of each parserstate to valid final states.
We also try to useforced-decoding instead of word alignment basedon Expectation Maximization (EM) algorithms inorder to create better training data for preorder-ing.
In experiments, preordering using the top-down parsing algorithmwas faster and gave higherBLEU scores than BTG-based preordering usingthe CYK algorithm.
Compared to existing pre-ordering methods, our method had better or com-parable BLEU scores without using supervisedparsers.2 Previous Work2.1 Preordering for Machine TranslationMany preordering methods which use syntacticparse trees have been proposed, because syntac-tic information is useful for determining the wordorder in a target language, and it can be used torestrict the search space against all the possiblepermutations.
Preordering methods using manu-ally created rules on parse trees have been stud-ied (Collins et al, 2005; Xu et al, 2009), but208Figure 1: An example of preordering.linguistic knowledge for a language pair is nec-essary to create such rules.
Preordering methodswhich automatically create reordering rules or uti-lize statistical classifiers have also been studied(Xia and McCord, 2004; Li et al, 2007; Gen-zel, 2010; Visweswariah et al, 2010; Yang et al,2012; Miceli Barone and Attardi, 2013; Lernerand Petrov, 2013; Jehl et al, 2014).
These meth-ods rely on source-side parse trees and cannot beapplied to languages where no syntactic parsersare available.There are preordering methods that do not needparse trees.
They are usually trained only on auto-matically word-aligned parallel text.
It is possibleto mine parallel text from the Web (Uszkoreit etal., 2010; Antonova and Misyurev, 2011), and thepreordering systems can be trained without man-ually annotated language resources.
Tromble andEisner (2009) studied preordering based on a Lin-ear Ordering Problem by defining a pairwise pref-erence matrix.
Khalilov and Sima?an (2010) pro-posed a method which swaps adjacent two wordsusing a maximum entropy model.
Visweswariahet al (2011) regarded the preordering problem asa Traveling Salesman Problem (TSP) and appliedTSP solvers for obtaining reordered words.
Thesemethods do not consider sentence structures.DeNero and Uszkoreit (2011) presented a pre-ordering method which builds a monolingual pars-ing model and a tree reordering model from par-allel text.
Neubig et al (2012) proposed to traina discriminative BTG parser for preordering di-rectly from word-aligned parallel text by handlingunderlying parse trees with latent variables.
Thismethod is explained in detail in the next subsec-tion.
These two methods can use sentence struc-tures for designing feature functions to score per-mutations.Figure 2: Bracketing transduction grammar.2.2 BTG-based PreorderingNeubig et al (2012) proposed a BTG-based pre-ordering method.
Bracketing Transduction Gram-mar (BTG) (Wu, 1997) is a binary synchronouscontext-free grammar with only one non-terminalsymbol, and has three types of rules (Figure 2):Straight which keeps the order of child nodes,Inverted which reverses the order, and Terminalwhich generates a terminal symbol.1BTG can express word reordering.
For exam-ple, the word reordering in Figure 1(a) can be rep-resented with the BTG parse tree in Figure 1(b).2Therefore, the task to reorder an input source sen-tence can be solved as a BTG parsing task to findan appropriate BTG tree.In order to find the best BTG tree among allthe possible ones, a score function is defined.
Let?
(m) denote the vector of feature functions forthe BTG tree node m, and ?
denote the vector offeature weights.
Then, for a given source sentencex, the best BTG tree z?
and the reordered sentencex?can be obtained as follows:z?
= argmaxz?Z(x)?m?Nodes(z)?
?
?
(m), (1)x?= Proj(z?
), (2)where Z(x) is the set of all the possible BTG treesfor x, Nodes(z) is the set of all the nodes in thetree z, and Proj(z) is the function which gener-ates a reordered sentence from the BTG tree z.The method was shown to improve transla-tion performance.
However, it has a problem ofprocessing speed.
The CYK algorithm, whosecomputational complexity is O(n3) for a sen-1Although Terminal produces a pair of source and targetwords in the original BTG (Wu, 1997), the target-side wordsare ignored here because both the input and the output of pre-ordering systems are in the source language.
In (Wu, 1997),(DeNero and Uszkoreit, 2011) and (Neubig et al, 2012), Ter-minal can produce multiple words.
Here, we produce onlyone word.2There may be more than one BTG tree which repre-sents the same word reordering (e.g., the word reorderingC3B2A1to A1B2C3has two possible BTG trees), and thereare permutations which cannot be represented with BTG(e.g., B2D4A1C3to A1B2C3D4, which is called the 2413pattern).209Figure 3: Top-down BTG parsing.
(0) ?
[[0, 5)], [], 0?
(1) ?
[[0, 2), [2, 5)], [(2, S)], v1?
(2) ?
[[0, 2), [3, 5)], [(2, S), (3, I)], v2?
(3) ?
[[0, 2)], [(2, S), (3, I), (4, I)], v3?
(4) ?
[], [(2, S), (3, I), (4, I), (1, S)], v4?Table 1: Parser states in top-down parsing.tence of length n, is used to find the best parsetree.
Furthermore, due to the use of a complexloss function, the complexity at training time isO(n5) (Neubig et al, 2012).
Since the compu-tational cost is prohibitive, some techniques likecube pruning and cube growing have been applied(Neubig et al, 2012; Na and Lee, 2013).
In thisstudy, we propose a top-down parsing algorithmin order to achieve fast BTG-based preordering.3 Preordering with IncrementalTop-Down BTG Parsing3.1 Parsing AlgorithmWe explain an incremental top-down BTG parsingalgorithm using Figure 3, which illustrates how aparse tree is built for the example sentence in Fig-ure 1.
At the beginning, a tree (span) which coversall the words in the sentence is considered.
Then,a span which covers more than one word is splitin each step, and the node type (Straight or In-verted) for the splitting point is determined.
Thealgorithm terminates after (n ?
1) iterations for asentence with n words, because there are (n ?
1)positions which can be split.We consider that the incremental parser has aparser state in each step, and define the stateas a triple ?P,C, v?.
P is a stack of unre-solved spans.
A span denoted by [p, q) coversthe words xp?
?
?xq?1for an input word sequencex = x0?
?
?x|x|?1.
C is a list of past parser ac-tions.
A parser action denoted by (r, o) representsthe action to split a span at the position betweenxr?1and xrwith the node type o ?
{S, I}, whereS and I indicate Straight and Inverted respectively.v is the score of the state, which is the sum of theInput: Sentence x, feature weights ?, beam width k.Output: BTG parse tree.1: S0?
{?
[[0, |x|)], [], 0? }
// Initial state.2: for i := 1, ?
?
?
, |x| ?
1 do3: S ?
{} // Set of the next states.4: foreach s ?
Si?1do5: S ?
S ?
?x,?
(s) // Generate next states.6: Si?
Topk(S) // Select k-best states.7: s?
= argmaxs?S|x|?1Score(s)8: return Tree(s?
)9: function ?x,?
(?P,C, v?
)10: [p, q)?
P.pop()11: S ?
{}12: for r := p + 1, ?
?
?
, q do13: P??
P14: if r ?
p > 1 then15: P?.push([p, r))16: if q ?
r > 1 then17: P?.push([r, q))18: vS?
v + ?
?
?
(x,C, p, q, r,S)19: vI?
v + ?
?
?
(x,C, p, q, r, I)20: CS?
C; CS.append((r,S))21: CI?
C; CI.append((r, I))22: S ?
S ?
{?P?, CS, vS?, ?P?, CI, vI?
}23: return SFigure 4: Top-down BTG parsing with beamsearch.scores for the nodes constructed so far.
Parsingstarts with the initial state ?
[[0, |x|)], [], 0?, becausethere is one span covering all the words at the be-ginning.
In each step, a span is popped from thetop of the stack, and a splitting point in the spanand its node type are determined.
The new spansgenerated by the split are pushed onto the stack iftheir lengths are greater than 1, and the action isadded to the list.
On termination, the parser hasthe final state ?
[], [c0, ?
?
?
, c|x|?2], v?, because thestack is empty and there are (|x| ?
1) actions intotal.
The parse tree can be obtained from the listof actions.
Table 1 shows the parser state for eachstep in Figure 3.The top-down parsing method can be used withbeam search as shown in Figure 4.
?x,?
(s) is afunction which returns the set of all the possi-ble next states for the state s. Topk(S) returnsthe top k states from S in terms of their scores,Score(s) returns the score of the state s, andTree(s) returns the BTG parse tree constructedfrom s.
?
(x,C, p, q, r, o) is the feature vector forthe node created by splitting the span [p, q) at rwith the node type o, and is explained in Sec-tion 3.3.3.2 Learning AlgorithmModel parameters ?
are estimated from trainingexamples.
We assume that each training example210consists of a sentence x and its word order in atarget language y = y0?
?
?
y|x|?1, where yiis theposition of xiin the target language.
For exam-ple, the example sentence in Figure 1(a) will havey = 0, 1, 4, 3, 2. y can have ambiguities.
Multiplewords can be reordered to the same position onthe target side.
The words whose target positionsare unknown are indicated by position ?1, and weconsider such words can appear at any position.3For example, the word alignment in Figure 5 givesthe target side word positions y = ?1, 2, 1, 0, 0.Statistical syntactic parsers are usually trainedon tree-annotated corpora.
However, corpora an-notated with BTG parse trees are unavailable, andonly the gold standard permutation y is available.Neubig et al (2012) proposed to train BTG parsersfor preordering by regarding BTG trees behindword reordering as latent variables, and we uselatent variable Perceptron (Sun et al, 2009) to-gether with beam search.
In latent variable Percep-tron, among the examples whose latent variablesare compatible with a gold standard label, the onewith the highest score is picked up as a positiveexample.
Such an approach was used for pars-ing with multiple correct actions (Goldberg andElhadad, 2010; Sartorio et al, 2013).Figure 6 describes the training algorithm.4?
(x, s) is the feature vector for all the nodes inthe partial parse tree at the state s, and ?x,?,y(s)is the set of all the next states for the state s.The algorithm adopts the early update technique(Collins and Roark, 2004) which terminates incre-mental parsing if a correct state falls off the beam,and there is no possibility to obtain a correct out-put.
Huang et al (2012) proposed the violation-fixing Perceptron framework which is guaranteedto converge even if inexact search is used, andalso showed that early update is a special caseof the framework.
We define that a parser stateis valid if the state can reach a final state whoseBTG parse tree is compatible with y.
Since thisis a latent variable setting in which multiple statescan reach correct final states, early update occurswhen all the valid states fall off the beam (Ma etal., 2013; Yu et al, 2013).
In order to use early up-date, we need to check the validity of each parser3In (Neubig et al, 2012), the positions of such words werefixed by heuristics.
In this study, the positions are not fixed,and all the possibilities are considered by latent variables.4Although the simple Perceptron algorithm is used for ex-planation, we actually used the Passive Aggressive algorithm(Crammer et al, 2006) with the parameter averaging tech-nique (Freund and Schapire, 1999).state.
We extend the parser state to the four tu-ple ?P,A, v, w?, where w ?
{true, false} is thevalidity of the state.
We remove training exam-ples which cannot be represented with BTG be-forehand and set w of the initial state to true.
Thefunction V alid(s) in Figure 6 returns the validityof state s. One advantage of the top-down pars-ing algorithm is that it is easy to track the validityof each state.
The validity of a state can be cal-culated using the following property, and we canimplement the function ?x,?,y(s) by modifying thefunction ?x,?
(s) in Figure 4.Lemma 1.
When a valid state s, which has [p, q)in the top of the stack, transitions to a state s?bythe action (r, o), s?is also valid if and only if thefollowing condition holds:?i ?
{p, ?
?
?
, r ?
1} yi= ?1 ?
?i ?
{r, ?
?
?
, q ?
1} yi= ?1 ?
(o = S ?
maxi=p,???
,r?1yi?=?1yi?
mini=r,???
,q?1yi?=?1yi)?
(o = I ?
maxi=r,???
,q?1yi?=?1yi?
mini=p,???
,r?1yi?=?1yi).
(3)Proof.
Let piidenote the position of xiafter re-ordering by BTG parsing.
If Condition (3) doesnot hold, there are i and j which satisfy pii<pij?
yi> yj?
yi?= ?1?
yj?= ?1, and piiand pijare not compatible with y.
Therefore, s?is validonly if Condition (3) holds.When Condition (3) holds, a valid permutationcan be obtained if the spans [p, r) and [r, q) areBTG-parsable.
They are BTG-parsable as shownbelow.
Let us assume that y does not have am-biguities.
The class of the permutations whichcan be represented by BTG is known as separablepermutations in combinatorics.
It can be proven(Bose et al, 1998) that a permutation is a sepa-rable permutation if and only if it contains nei-ther the 2413 nor the 3142 patterns.
Since s isvalid, y is a separable permutation.
y does not con-tain the 2413 nor the 3142 patterns, and any sub-sequence of y also does not contain the patterns.Thus, [p, r) and [r, q) are separable permutations.The above argument holds even if y has ambigui-ties (duplicated positions or unaligned words).
Insuch a case, we can always make a word order y?which specializes y and has no ambiguities (e.g.,y?= 2, 1.0, 0.0, 0.1, 1.1 for y = ?1, 1, 0, 0, 1),because s is valid, and there is at least one BTGparse tree which licenses y.
Any subsequence in211Figure 5: An example of word reordering with am-biguities.y?is a separable permutation, and [p, r) and [r, q)are separable permutations.
Therefore, s?is validif Condition (3) holds.For dependency parsing and constituent pars-ing, incremental bottom-up parsing methods havebeen studied (Yamada and Matsumoto, 2003;Nivre, 2004; Goldberg and Elhadad, 2010; Sagaeand Lavie, 2005).
Our top-down approach iscontrastive to the bottom-up approaches.
In thebottom-up approaches, spans which cover individ-ual words are considered at the beginning, thenthey are merged into larger spans in each step, anda span which covers all the words is obtained atthe end.
In the top-down approach, a span whichcovers all the words is considered at the begin-ning, then spans are split into smaller spans ineach step, and spans which cover individual wordsare obtained at the end.
The top-down BTG pars-ing method has the advantage that the validity ofparser states can be easily tracked.The computational complexity of the top-downparsing algorithm is O(kn2) for sentence length nand beam width k, because in Line 5 of Figure 4,which is repeated at most k(n ?
1) times, at most2(n ?
1) parser states are generated, and theirscores are calculated.
The learning algorithm usesthe same decoding algorithm as in the parsingphase, and has the same time complexity.
Notethat the validity of a parser state can be calculatedin O(1) by pre-calculating mini=p,???
,r?yi?=?1yi,maxi=p,???
,r?yi?=?1yi, mini=r,???
,q?1?yi?=?1yi,and maxi=r,???
,q?1?yi?=?1yifor all r for the span[p, q) when it is popped from the stack.3.3 FeaturesWe assume that each word xiin a sentence hasthree attributes: word surface form xwi, part-of-speech (POS) tag xpiand word class xci(Sec-tion 4.1 explains how xpiand xciare obtained).Table 2 lists the features generated for the nodewhich is created by splitting the span [p, q) withthe action (r, o).
o?
is the node type of the par-ent node, d ?
{left, right} indicates whether thisnode is the left-hand-side or the right-hand-sidechild of the parent node, and Balance(p, q, r) re-Input: Training data {?xl, yl?
}L?1l=0,number of iterations T , beam width k.Output: Feature weights ?.1: ??
02: for t := 0, ?
?
?
, T ?
1 do3: for l := 0, ?
?
?
, L?
1 do4: S0?
{?
[[0, |xl|)], [], 0, true?
}5: for i := 1, ?
?
?
, |xl| ?
1 do6: S ?
{}7: foreach s ?
Si?1do8: S ?
S ?
?xl,?,yl(s)9: Si?
Topk(S)10: s?
?
argmaxs?SScore(s)11: s??
argmaxs?S?V alid(s)Score(s)12: if s?/?
Sithen13: break // Early update.14: if s?
?= s?then15: ??
?
+ ?
(xl, s?)?
?
(xl, s?
)16: return ?Figure 6: A training algorithm for latent variablePerceptron with beam search.turns a value among {?<?, ?=?, ?>?}
according tothe relation of the lengths of [p, r) and [r, q).
Thebaseline feature templates are those used by Neu-big et al (2012), and the additional feature tem-plates are extended features that we introduce inthis study.
The top-down parser is fast, and allowsus to use a larger number of features.In order to make the feature generation efficient,the attributes of all the words are converted to their64-bit hash values beforehand, and concatenatingthe attributes is executed not as string manipula-tion but as faster integer calculation to generate ahash value by merging two hash values.
The hashvalues are used as feature names.
Therefore, whenaccessing feature weights stored in a hash tableusing the feature names as keys, the keys can beused as their hash values.
This technique is differ-ent from the hashing trick (Ganchev and Dredze,2008) which directly uses hash values as indices,and no noticeable differences in accuracy were ob-served by using this technique.3.4 Training Data for PreorderingAs described in Section 3.2, each training examplehas y which represents correct word positions afterreordering.
However, only word alignment data isgenerally available, and we need to convert it toy.
Let Aidenote the set of indices of the target-side words which are aligned to the source-sideword xi.
We define an order relation between twowords:xi?
xj?
?a ?
Ai\Aj, ?b ?
Aja ?
b ?
?a ?
Ai, ?b ?
Aj\Aia ?
b.
(4)212Baseline Feature Templateo(q ?
p), oBalance(p, q, r),oxwp?1, oxwp, oxwr?1, oxwr, oxwq?1, oxwq, oxwpxwq?1, oxwr?1xwr,oxpp?1, oxpp, oxpr?1, oxpr, oxpq?1, oxpq, oxppxpq?1, oxpr?1xpr,oxcp?1, oxcp, oxcr?1, oxcr, oxcq?1, oxcq, oxcpxcq?1, oxcr?1xcr.Additional Feature Templateomin(r ?
p, 5)min(q ?
r, 5), oo?, oo?d,oxwp?1xwp, oxwpxwr?1, oxwpxwr, oxwr?1xwq?1, oxwrxwq?1, oxwq?1xwq,oxwr?2xwr?1xwr, oxwpxwr?1xwr, oxwr?1xwrxwq?1, oxwr?1xwrxwr+1,oxwpxwr?1xwrxwq?1,oo?dxwp, oo?dxwr?1, oo?dxwr, oo?dxwq?1, oo?dxwpxwq?1,oxpp?1xpp, oxppxpr?1, oxppxpr, oxpr?1xpq?1, oxprxpq?1, oxpq?1xpq,oxpr?2xpr?1xpr, oxppxpr?1xpr, oxpr?1xprxpq?1, oxpr?1xprxpr+1,oxppxpr?1xprxpq?1,oo?dxpp, oo?dxpr?1, oo?dxpr, oo?dxpq?1, oo?dxppxpq?1,oxcp?1xcp, oxcpxcr?1, oxcpxcr, oxcr?1xcq?1, oxcrxcq?1, oxcq?1xcq,oxcr?2xcr?1xcr, oxcpxcr?1xcr, oxcr?1xcrxcq?1, oxcr?1xcrxcr+1,oxcpxcr?1xcrxcq?1,oo?dxcp, oo?dxcr?1, oo?dxcr, oo?dxcq?1, oo?dxcpxcq?1.Table 2: Feature templates.Then, we sort x using the order relation and as-sign the position of xiin the sorted result to yi.If there are two words xiand xjin x which sat-isfy neither xi?
xjnor xj?
xi(that is, x doesnot make a totally ordered set with the order rela-tion), then x cannot be sorted, and the example isremoved from the training data.
?1 is assigned tothe words which do not have aligned target words.Two words xiand xjare regarded to have the sameposition if xi?
xjand xj?
xi.The quality of training data is important tomake accurate preordering systems, but automat-ically word-aligned data by EM algorithms tendto have many wrong alignments.
We use forced-decoding in order to make training data for pre-ordering.
Given a parallel sentence pair and aphrase table, forced-decoding tries to translate thesource sentence to the target sentence, and pro-duces phrase alignments.
We train the parametersfor forced-decoding using the same parallel dataused for training the final translation system.
In-frequent phrase translations are pruned when thephrase table is created, and forced-decoding doesnot always succeed for the parallel sentences in thetraining data.
Forced-decoding tends to succeedfor shorter sentences, and the phrase-alignmentdata obtained by forced-decoding is biased to con-tain more shorter sentences.
Therefore, we applythe following processing for the output of forced-decoding to make training data for preordering:1.
Remove sentences which contain less than 3or more than 50 words.2.
Remove sentences which contain less than 3phrase alignments.3.
Remove sentences if they contain word 5-grams which appear in other sentences in or-der to drop boilerplates.4.
Lastly, randomly resample sentences fromthe pool of filtered sentences to make thedistribution of the sentence lengths follow anormal distribution with the mean of 20 andthe standard deviation of 8.
The parame-ters were determined from randomly sampledsentences from the Web.4 Experiments4.1 Experimental SettingsWe conduct experiments for 12 language pairs:Dutch (nl)-English (en), en-nl, en-French (fr), en-Japanese (ja), en-Spanish (es), fr-en, Hindi (hi)-en,ja-en, Korean (ko)-en, Turkish (tr)-en, Urdu (ur)-en and Welsh (cy)-en.We use a phrase-based statistical machine trans-lation system which is similar to (Och and Ney,2004).
The decoder adopts the regular distancedistortion model, and also incorporates a maxi-mum entropy based lexicalized phrase reorderingmodel (Zens and Ney, 2006).
The distortion limitis set to 5 words.
Word alignments are learnedusing 3 iterations of IBM Model-1 (Brown et al,1993) and 3 iterations of the HMM alignmentmodel (Vogel et al, 1996).
Lattice-based mini-mum error rate training (MERT) (Macherey et al,2008) is applied to optimize feature weights.
5-gram language models trained on sentences col-lected from various sources are used.The translation system is trained with parallelsentences automatically collected from the Web.The parallel data for each language pair consistsof around 400 million source and target words.
Inorder to make the development data for MERT andtest data (3,000 and 5,000 sentences respectivelyfor each language), we created parallel sentencesby randomly collecting English sentences from theWeb, and translating them by humans into eachlanguage.As an evaluation metric for translation quality,BLEU (Papineni et al, 2002) is used.
As intrin-sic evaluation metrics for preordering, Fuzzy Re-ordering Score (FRS) (Talbot et al, 2011) andKendall?s ?
(Kendall, 1938; Birch et al, 2010;Isozaki et al, 2010) are used.
Let ?idenote the po-sition in the input sentence of the (i+1)-th token ina preordered word sequence excluding unalignedwords in the gold-standard evaluation data.
For213en-ja ja-enTraining Preordering FRS ?
Training Preordering FRS ?(min.)
(sent./sec.)
(min.)
(sent./sec.
)Top-Down (EM-100k) 63 87.8 77.83 87.78 81 178.4 74.60 83.78Top-Down (Basic Feat.)
(EM-100k) 9 475.1 75.25 87.26 9 939.0 73.56 83.66Lader (EM-100k) 1562 4.3 75.41 86.85 2087 12.3 74.89 82.15Table 3: Speed and accuracy of preordering.en-ja ja-enFRS ?
BLEU FRS ?
BLEUTop-Down (Manual-8k) 81.57 90.44 18.13 79.26 86.47 14.26(EM-10k) 74.79 85.87 17.07 72.51 82.65 14.55(EM-100k) 77.83 87.78 17.66 74.60 83.78 14.84(Forced-10k) 76.10 87.45 16.98 75.36 83.96 14.78(Forced-100k) 78.76 89.22 17.88 76.58 85.25 15.54Lader (EM-100k) 75.41 86.85 17.40 74.89 82.15 14.59No-Preordering 46.17 65.07 13.80 59.35 65.30 10.31Manual-Rules 80.59 90.30 18.68 73.65 81.72 14.02Auto-Rules 64.13 84.17 16.80 60.60 75.49 12.59Classifier 80.89 90.61 18.53 74.24 82.83 13.90Table 4: Performance of preordering for various training data.
Bold BLEU scores indicate no statisticallysignificant difference at p < 0.05 from the best system (Koehn, 2004).example, the preordering result ?New York I towent?
for the gold-standard data in Figure 5 has?
= 3, 4, 2, 1.
Then FRS and ?
are calculated asfollows:FRS =B|?| + 1, (5)B =|?|?2?i=0?(y?i=y?i+1?
y?i+1=y?i+1) +?
(y?0=0) + ?
(y?|?|?1=maxiyi), (6)?
=?|?|?2i=0?|?|?1j=i+1?(y?i?
y?j)12|?|(|?| ?
1), (7)where ?
(X) is the Kronecker?s delta functionwhich returns 1 if X is true or 0 otherwise.
Thesescores are calculated for each sentence, and are av-eraged over all sentences in test data.
As above,FRS can be calculated as the precision of word bi-grams (B is the number of the word bigrams whichexist both in the system output and the gold stan-dard data).
This formulation is equivalent to theoriginal formulation based on chunk fragmenta-tion by Talbot et al (2011).
Equation (6) takesinto account the positions of the beginning and theending words (Neubig et al, 2012).
Kendall?s ?
isequivalent to the (normalized) crossing alignmentlink score used by Genzel (2010).We prepared three types of training data forlearning model parameters of BTG-based pre-ordering:Manual-8k Manually word-aligned 8,000 sen-tence pairs.EM-10k, EM-100k These are the data obtainedwith the EM-based word alignment learn-ing.
From the word alignment resultfor phrase translation extraction describedabove, 10,000 and 100,000 sentence pairswere randomly sampled.
Before the sam-pling, the data filtering procedure 1 and 3in Section 3.4 were applied, and also sen-tences were removed if more than half ofsource words do not have aligned targetwords.
Word alignment was obtained bysymmetrizing source-to-target and target-to-source word alignment with the INTERSEC-TION heuristic.5Forced-10k, Forced-100k These are 10,000 and100,000 word-aligned sentence pairs ob-tained with forced-decoding as described inSection 3.4.As test data for intrinsic evaluation of preordering,we manually word-aligned 2,000 sentence pairsfor en-ja and ja-en.Several preordering systems were prepared inorder to compare the following six systems:No-Preordering This is a system without pre-ordering.Manual-Rules This system uses the preorderingmethod based on manually created rules (Xu5In our preliminary experiments, the UNION and GROW-DIAG-FINAL heuristics were also applied to generate thetraining data for preordering, but INTERSECTION per-formed the best.214No- Manual- Auto- Classifier Lader Top-Down Top-DownPreordering Rules Rules (EM-100k) (EM-100k) (Forced-100k)nl-en 34.01 - 34.24 35.42 33.83 35.49 35.51en-nl 25.33 - 25.59 25.99 25.30 25.82 25.66en-fr 25.86 - 26.39 26.35 26.50 26.75 26.81en-ja 13.80 18.68 16.80 18.53 17.40 17.66 17.88en-es 29.50 - 29.63 30.09 29.70 30.26 30.24fr-en 32.33 - 32.09 32.28 32.43 33.00 32.99hi-en 19.86 - - - 24.24 24.98 24.97ja-en 10.31 14.02 12.59 13.90 14.59 14.84 15.54ko-en 14.13 - 15.86 19.46 18.65 19.67 19.88tr-en 18.26 - - - 22.80 23.91 24.18ur-en 14.48 - - - 16.62 17.65 18.32cy-en 41.68 - - - 41.79 41.95 41.86Table 5: BLEU score comparison.Distortion No- Manual- Auto- Classifier Lader Top-Down Top-DownLimit Preordering Rules Rules (EM-100k) (EM-100k) (Forced-100k)en-ja 5 13.80 18.68 16.80 18.53 17.40 17.66 17.88en-ja 0 11.99 18.34 16.87 18.31 16.95 17.36 17.88ja-en 5 10.31 14.02 12.59 13.90 14.59 14.84 15.54ja-en 0 10.03 12.43 11.33 13.09 14.38 14.72 15.34Table 6: BLEU scores for different distortion limits.et al, 2009).
We made 43 precedence rulesfor en-ja, and 24 for ja-en.Auto-Rules This system uses the rule-based pre-ordering method which automatically learnsthe rules from word-aligned data using theVariant 1 learning algorithm described in(Genzel, 2010).
27 to 36 rules were automat-ically learned for each language pair.Classifier This system uses the preorderingmethod based on statistical classifiers (Lernerand Petrov, 2013), and the 2-step algorithmwas implemented.Lader This system uses Latent Derivation Re-orderer (Neubig et al, 2012), which is aBTG-based preordering system using theCYK algorithm.6The basic feature templatesin Table 2 are used as features.Top-Down This system uses the preordering sys-tem described in Section 3.Among the six systems, Manual-Rules, Auto-Rules and Classifier need dependency parsers forsource languages.
A dependency parser basedon the shift-reduce algorithm with beam search(Zhang and Nivre, 2011) is used.
The dependencyparser and all the preordering systems need POStaggers.
A supervised POS tagger based on condi-tional random fields (Lafferty et al, 2001) trainedwith manually POS annotated data is used for nl,en, fr, ja and ko.
For other languages, we use aPOS tagger based on POS projection (T?ackstr?om6lader 0.1.4. http://www.phontron.com/lader/et al, 2013) which does not need POS annotateddata.
Word classes in Table 2 are obtained by us-ing Brown clusters (Koo et al, 2008) (the numberof classes is set to 256).
For both Lader and Top-Down, the beam width is set to 20, and the numberof training iterations of online learning is set to 20.The CPU time shown in this paper is measuredusing Intel Xeon 3.20GHz with 32GB RAM.4.2 Results4.2.1 Training and Preordering SpeedTable 3 shows the training time and preorderingspeed together with the intrinsic evaluation met-rics.
In this experiment, both Top-Down and Laderwere trained using the EM-100k data.
Comparedto Lader, Top-Down was faster: more than 20times in training, and more than 10 times in pre-ordering.
Top-down had higher preordering ac-curacy in FRS and ?
for en-ja.
Although Laderuses sophisticated loss functions, Top-Down usesa larger number of features.Top-Down (Basic feats.)
is the top-downmethod using only the basic feature templates inTable 2.
It was much faster but less accuratethan Top-Down using the additional features.
Top-Down (Basic feats.)
and Lader use exactly thesame features.
However, there are differences inthe two systems, and they had different accuracies.Top-Down uses the beam search-based top-downmethod for parsing and the Passive-Aggressive al-gorithm for parameter estimation, and Lader usesthe CYK algorithm with cube pruning and an on-215line SVM algorithm.
Especially, Lader optimizesFRS in the default setting, and it may be the reasonthat Lader had higher FRS.4.2.2 Performance of Preordering forVarious Training DataTable 4 shows the preordering accuracy and BLEUscores when Top-Down was trained with variousdata.
The best BLEU score for Top-Down was ob-tained by using manually annotated data for en-ja and 100k forced-decoding data for ja-en.
Theperformance was improved by increasing the datasize.4.2.3 End-to-End Evaluation for VariousLanguage PairsTable 5 shows the BLEU score of each system for12 language pairs.
Some blank fields mean thatthe results are unavailable due to the lack of rulesor dependency parsers.
For all the language pairs,Top-Down had higher BLEU scores than Lader.For ja-en and ur-en, using Forced-100k insteadof EM-100k for Top-Down improved the BLEUscores by more than 0.6, but it did not always im-proved.Manual-Rules performed the best for en-ja, butit needs manually created rules and is difficultto be applied to many language pairs.
Auto-Rules and Classifier had higher scores than No-Preordering except for fr-en, but cannot be appliedto the languages with no available dependencyparsers.
Top-Down (Forced-100k) can be appliedto any language, and had statistically significantlybetter BLEU scores than No-Preordering, Manual-Rules, Auto-Rules, Classifier and Lader for 7 lan-guage pairs (en-fr, fr-en, hi-en, ja-en, ko-en, tr-enand ur-en), and similar performance for other lan-guage pairs except for en-ja, without dependencyparsers trained with manually annotated data.In all the experiments so far, the decoder wasallowed to reorder even after preordering was car-ried out.
In order to see the performance withoutreordering after preordering, we conducted exper-iments by setting the distortion limit to 0.
Table 6shows the results.
The effect of the distortion lim-its varies for language pairs and preordering meth-ods.
The BLEU scores of Top-Down were not af-fected largely even when relying only on preorder-ing.5 ConclusionIn this paper, we proposed a top-down BTG pars-ing method for preordering.
The method in-crementally builds parse trees by splitting largerspans into smaller ones.
The method provides aneasy way to check the validity of each parser state,which allows us to use early update for latent vari-able Perceptron with beam search.
In the exper-iments, it was shown that the top-down parsingmethod is more than 10 times faster than a CYK-based method.
The top-down method had betterBLEU scores for 7 language pairs without relyingon supervised syntactic parsers compared to otherpreordering methods.
Future work includes devel-oping a bottom-up BTG parser with latent vari-ables, and comparing the results to the top-downparser.ReferencesAlexandra Antonova and Alexey Misyurev.
2011.Building a Web-Based Parallel Corpus and FilteringOut Machine-Translated Text.
In Proceedings of the4th Workshop on Building and Using ComparableCorpora: Comparable Corpora and the Web, pages136?144.Alexandra Birch, Miles Osborne, and Phil Blunsom.2010.
Metrics for MT Evaluation: Evaluating Re-ordering.
Machine Translation, 24(1):15?26.Prosenjit Bose, Jonathan F. Buss, and Anna Lubiw.1998.
Pattern matching for permutations.
Informa-tion Processing Letters, 65(5):277?283.Peter F. Brown, Vincent J. Della Pietra, StephenA.
Della Pietra, and Robert L. Mercer.
1993.The Mathematics of Statistical Machine Translation:Parameter Estimation.
Computational Linguistics,19(2):263?311.David Chiang.
2007.
Hierarchical Phrase-BasedTranslation.
Computational Linguistics, 33(2):201?228.Michael Collins and Brian Roark.
2004.
IncrementalParsing with the Perceptron Algorithm.
In Proceed-ings of the 42nd Annual Meeting of the Associationfor Computational Linguistics, pages 111?118.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause Restructuring for Statistical MachineTranslation.
In Proceedings of the 43rd AnnualMeeting of the Association for Computational Lin-guistics, pages 531?540.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
On-line Passive-Aggressive Algorithms.
Journal of Ma-chine Learning Research, 7:551?585.John DeNero and Jakob Uszkoreit.
2011.
InducingSentence Structure from Parallel Corpora for Re-ordering.
In Proceedings of the 2011 Conference on216Empirical Methods in Natural Language Process-ing, pages 193?203.Yoav Freund and Robert E. Schapire.
1999.
LargeMargin Classification Using the Perceptron Algo-rithm.
Machine Learning, 37(3):277?296.Kuzman Ganchev and Mark Dredze.
2008.
Small Sta-tistical Models by Random Feature Mixing.
In Pro-ceedings of the ACL-08: HLT Workshop on MobileLanguage Processing, pages 19?20.Dmitriy Genzel.
2010.
Automatically LearningSource-side Reordering Rules for Large Scale Ma-chine Translation.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics,pages 376?384.Yoav Goldberg and Michael Elhadad.
2010.
An Ef-ficient Algorithm for Easy-first Non-directional De-pendency Parsing.
In Human Language Technolo-gies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 742?750.Liang Huang, Suphan Fayong, and Yang Guo.
2012.Structured Perceptron with Inexact Search.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages142?151.Hideki Isozaki, Tsutomu Hirao, Kevin Duh, KatsuhitoSudoh, and Hajime Tsukada.
2010.
AutomaticEvaluation of Translation Quality for Distant Lan-guage Pairs.
In Proceedings of the 2010 Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 944?952.Laura Jehl, Adri`a de Gispert, Mark Hopkins, andBill Byrne.
2014.
Source-side Preordering forTranslation using Logistic Regression and Depth-first Branch-and-Bound Search.
In Proceedings ofthe 14th Conference of the European Chapter of theAssociation for Computational Linguistics, pages239?248.Maurice G. Kendall.
1938.
A New Measure of RankCorrelation.
Biometrika, 30(1/2):81?93.Maxim Khalilov and Khalil Sima?an.
2010.
Sourcereordering using MaxEnt classifiers and supertags.In Proceedings of the 14th Annual Conference of theEuropean Association for Machine Translation.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Pro-ceedings of the 2003 Human Language TechnologyConference of the North American Chapter of theAssociation for Computational Linguistics, pages48?54.Philipp Koehn.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In Proceedings ofthe 2004 Conference on Empirical Methods in Nat-ural Language Processing, pages 388?395.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple Semi-supervised Dependency Pars-ing.
In Proceedings of the 46th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 595?603.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional Random Fields: Prob-abilistic Models for Segmenting and Labeling Se-quence Data.
In Proceedings of the 18th Interna-tional Conference on Machine Learning, pages 282?289.Uri Lerner and Slav Petrov.
2013.
Source-Side Clas-sifier Preordering for Machine Translation.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 513?523.Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,Ming Zhou, and Yi Guan.
2007.
A ProbabilisticApproach to Syntax-based Reordering for Statisti-cal Machine Translation.
In Proceedings of the 45thAnnual Meeting of the Association of ComputationalLinguistics, pages 720?727.Ji Ma, Jingbo Zhu, Tong Xiao, and Nan Yang.
2013.Easy-First POS Tagging and Dependency Parsingwith Beam Search.
In Proceedings of the 51st An-nual Meeting of the Association for ComputationalLinguistics (Volume 2: Short Papers), pages 110?114.Wolfgang Macherey, Franz Och, Ignacio Thayer, andJakob Uszkoreit.
2008.
Lattice-based MinimumError Rate Training for Statistical Machine Trans-lation.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Process-ing, pages 725?734.Valerio Antonio Miceli Barone and Giuseppe Attardi.2013.
Pre-Reordering for Machine Translation Us-ing Transition-Based Walks on Dependency ParseTrees.
In Proceedings of the 8th Workshop on Sta-tistical Machine Translation, pages 164?169.Hwidong Na and Jong-Hyeok Lee.
2013.
A Dis-criminative Reordering Parser for IWSLT 2013.
InProceedings of the 10th International Workshop forSpoken Language Translation, pages 83?86.Graham Neubig, Taro Watanabe, and Shinsuke Mori.2012.
Inducing a Discriminative Parser to OptimizeMachine Translation Reordering.
In Proceedings ofthe 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 843?853.Joakim Nivre.
2004.
Incrementality in DeterministicDependency Parsing.
In Proceedings of the Work-shop on Incremental Parsing: Bringing Engineeringand Cognition Together, pages 50?57.Franz Josef Och and Hermann Ney.
2004.
The Align-ment Template Approach to Statistical MachineTranslation.
Computational Linguistics, 30(4):417?449.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, pages 311?318.Kenji Sagae and Alon Lavie.
2005.
A Classifier-BasedParser with Linear Run-Time Complexity.
In Pro-ceedings of the 9th International Workshop on Pars-ing Technology, pages 125?132.217Francesco Sartorio, Giorgio Satta, and Joakim Nivre.2013.
A Transition-Based Dependency Parser Us-ing a Dynamic Parsing Strategy.
In Proceedings ofthe 51st Annual Meeting of the Association for Com-putational Linguistics, pages 135?144.Xu Sun, Takuya Matsuzaki, Daisuke Okanohara, andJun?ichi Tsujii.
2009.
Latent Variable PerceptronAlgorithm for Structured Classification.
In Proceed-ings of the 21st International Joint Conference onArtificial Intelligence, pages 1236?1242.Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, RyanMcDonald, and Joakim Nivre.
2013.
Token andType Constraints for Cross-Lingual Part-of-SpeechTagging.
Transactions of the Association of Compu-tational Linguistics, 1:1?12.David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Ja-son Katz-Brown, Masakazu Seno, and Franz J. Och.2011.
A Lightweight Evaluation Framework forMachine Translation Reordering.
In Proceedingsof the 6th Workshop on Statistical Machine Trans-lation, pages 12?21.Christoph Tillman.
2004.
A Unigram OrientationModel for Statistical Machine Translation.
In Pro-ceedings of the 2004 Human Language TechnologyConference of the North American Chapter of theAssociation for Computational Linguistics (ShortPapers), pages 101?104.Roy Tromble and Jason Eisner.
2009.
Learning LinearOrdering Problems for Better Translation.
In Pro-ceedings of the 2009 Conference on Empirical Meth-ods in Natural Language Processing, pages 1007?1016.Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, andMoshe Dubiner.
2010.
Large Scale Parallel Docu-ment Mining for Machine Translation.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics, pages 1101?1109.Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,Vijil Chenthamarakshan, and Nandakishore Kamb-hatla.
2010.
Syntax Based Reordering with Au-tomatically Derived Rules for Improved StatisticalMachine Translation.
In Proceedings of the 23rd In-ternational Conference on Computational Linguis-tics, pages 1119?1127.Karthik Visweswariah, Rajakrishnan Rajkumar, AnkurGandhe, Ananthakrishnan Ramanathan, and JiriNavratil.
2011.
A Word Reordering Model for Im-proved Machine Translation.
In Proceedings of the2011 Conference on Empirical Methods in NaturalLanguage Processing, pages 486?496.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based Word Alignment in StatisticalTranslation.
In Proceedings of the 16th Conferenceon Computational Linguistics, pages 836?841.Dekai Wu.
1997.
Stochastic Inversion TransductionGrammars and Bilingual Parsing of Parallel Cor-pora.
Computational Linguistics, 23(3):377?403.Fei Xia and Michael McCord.
2004.
Improving aStatistical MT System with Automatically LearnedRewrite Patterns.
In Proceedings of the 20th Inter-national Conference on Computational Linguistics,pages 508?514.Peng Xu, Jaeho Kang, Michael Ringgaard, and FranzOch.
2009.
Using a Dependency Parser to Im-prove SMT for Subject-Object-Verb Languages.
InProceedings of Human Language Technologies: The2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 245?253.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Sta-tistical Dependency Analysis with Support VectorMachines.
In Proceedings of the 8th InternationalWorkshop on Parsing Technologies, pages 195?206.Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu.2012.
A Ranking-based Approach to Word Reorder-ing for Statistical Machine Translation.
In Proceed-ings of the 50th Annual Meeting of the Associationfor Computational Linguistics, pages 912?920.Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.2013.
Max-Violation Perceptron and Forced Decod-ing for Scalable MT Training.
In Proceedings of the2013 Conference on Empirical Methods in NaturalLanguage Processing, pages 1112?1123.Richard Zens and Hermann Ney.
2006.
DiscriminativeReordering Models for Statistical Machine Transla-tion.
In Proceedings on the Workshop on StatisticalMachine Translation, pages 55?63.Yue Zhang and Joakim Nivre.
2011.
Transition-basedDependency Parsing with Rich Non-local Features.In Proceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics: Short Pa-pers, pages 188?193.218
