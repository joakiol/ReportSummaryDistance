Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 322?332,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsUniversal Morphological Analysis using Structured Nearest NeighborPredictionYoung-Bum KimUniversity of Wisconsin-Madisonybkim@cs.wisc.eduJo?o V. Gra?aL2F INESC-IDLisboa, Portugaljoao.graca@l2f.inesc-id.ptBenjamin SnyderUniversity of Wisconsin-Madisonbsnyder@cs.wisc.eduAbstractIn this paper, we consider the problem of un-supervised morphological analysis from a newangle.
Past work has endeavored to design un-supervised learning methods which explicitlyor implicitly encode inductive biases appropri-ate to the task at hand.
We propose insteadto treat morphological analysis as a structuredprediction problem, where languages with la-beled data serve as training examples for un-labeled languages, without the assumption ofparallel data.
We define a universal morpho-logical feature space in which every languageand its morphological analysis reside.
We de-velop a novel structured nearest neighbor pre-diction method which seeks to find the mor-phological analysis for each unlabeled lan-guage which lies as close as possible in thefeature space to a training language.
We ap-ply our model to eight inflecting languages,and induce nominal morphology with substan-tially higher accuracy than a traditional, MDL-based approach.
Our analysis indicates thataccuracy continues to improve substantially asthe number of training languages increases.1 IntroductionOver the past several decades, researchers in the nat-ural language processing community have focusedmost of their efforts on developing text processingtools and techniques for English (Bender, 2009),a morphologically simple language.
Recently, in-creasing attention has been paid to the wide varietyof other languages of the world.
Most of these lan-guages still pose severe difficulties, due to (i) theirlack of annotated textual data, and (ii) the fact thatthey exhibit linguistic structure not found in English,and are thus not immediately susceptible to manytraditional NLP techniques.Consider the example of nominal part-of-speechanalysis.
The Penn Treebank defines only four En-glish noun tags (Marcus et al, 1994), and as a re-sult, it is easy to treat the words bearing these tagsas completely distinct word classes, with no inter-nal morphological structure.
In contrast, a compara-ble tagset for Hungarian includes 154 distinct nountags (Erjavec, 2004), reflecting Hungarian?s rich in-flectional morphology.
When dealing with such lan-guages, treating words as atoms leads to severe datasparsity problems.Because annotated resources do not exist for mostmorphologically rich languages, prior research hasfocused on unsupervised methods, with a focus ondeveloping appropriate inductive biases.
However,inductive biases and declarative knowledge are no-toriously difficult to encode in well-founded models.Even putting aside this practical matter, a universallycorrect inductive bias, if there is one, is unlikely tobe be discovered by a priori reasoning alone.In this paper, we argue that languages for whichwe have gold-standard morphological analyses canbe used as effective guides for languages lackingsuch resources.
In other words, instead of treatingeach language?s morphological analysis as a de novoinduction problem to be solved with a purely hand-coded bias, we instead learn from our labeled lan-guages what linguistically plausible morphologicalanalyses looks like, and guide our analysis in thisdirection.322More formally, we recast morphological induc-tion as a new kind of supervised structured predic-tion problem, where each annotated language servesas a single training example.
Each language?s nounlexicon serves as a single input x, and the analysisof the nouns into stems and suffixes serves as a com-plex structured label y.Our first step is to define a universal morpholog-ical feature space, into which each language and itsmorphological analysis can be mapped.
We opt fora simple and intuitive mapping, which measures thesizes of the stem and suffix lexicons, the entropy ofthese lexicons, and the fraction of word forms whichappear without any inflection.Because languages tend to cluster into well de-fined morphological groups, we cast our learn-ing and prediction problem in the nearest neighborframework (Cover and Hart, 1967).
In contrast toits typical use in classification problems, where onecan simply pick the label of the nearest training ex-ample, we are here faced with a structured predic-tion problem, where locations in feature space de-pend jointly on the input-label pair (x, y).
Finding anearest neighbor thus consists of searching over thespace of morphological analyses, until a point in fea-ture space is reached which lies closest to one of thelabeled languages.
See Figure 1 for an illustration.To provide a measure of empirical validation, weapplied our approach to eight languages with inflec-tional nominal morphology, ranging in complexityfrom very simple (English) to very complex (Hun-garian).
In all but one case, our approach yieldssubstantial improvements over a comparable mono-lingual baseline (Goldsmith, 2005), which uses theminimum description length principle (MDL) as itsinductive bias.
On average, our method increasesaccuracy by 11.8 percentage points, correspondingto a 42% decrease in error relative to a supervisedupper bound.
Further analysis indicates that accu-racy improves as the number of training languagesincreases.2 Related WorkIn this section, we briefly review prior work on un-supervised morphological induction, as well as mul-tilingual analysis in NLP.Unsupervised Morphological Induction: Unsu-pervised morphology remains an active area of re-search (Schone and Jurafsky, 2001; Goldsmith,2005; Adler and Elhadad, 2006; Creutz and La-gus, 2005; Dasgupta and Ng, 2007; Creutz and La-gus, 2007; Poon et al, 2009).
Many existing algo-rithms derive morpheme lexicons by identifying re-curring patterns in words.
The goal is to optimize thecompactness of the data representation by finding asmall lexicon of highly frequent strings, resulting ina minimum description length (MDL) lexicon andcorpus (Goldsmith, 2001; Goldsmith, 2005).
Laterwork cast this idea in a probabilistic framework inwhich the the MDL solution is equivalent to a MAPestimate in a suitable Bayesian model (Creutz andLagus, 2005).
In all these approaches, a locally op-timal segmentation is identified using a task-specificgreedy search.Multilingual Analysis: An influential line of priormultilingual work starts with the observation thatrich linguistic resources exist for some languagesbut not others.
The idea then is to project linguis-tic information from one language onto others viaparallel data.
Yarowsky and his collaborators firstdeveloped this idea and applied it to the problems ofpart-of-speech tagging, noun-phrase bracketing, andmorphology induction (Yarowsky and Wicentowski,2000; Yarowsky et al, 2000; Yarowsky and Ngai,2001), and other researchers have applied the ideato syntactic and semantic analysis (Hwa et al, 2005;Pad?
and Lapata, 2006) In these cases, the existenceof a bilingual parallel text along with highly accuratepredictions for one of the languages was assumed.Another line of work assumes the existence ofbilingual parallel texts without the use of any super-vision (Dagan et al, 1991; Resnik and Yarowsky,1997).
This idea has been developed and applied toa wide variety tasks, including morphological anal-ysis (Snyder and Barzilay, 2008b; Snyder and Barzi-lay, 2008a), part-of-speech induction (Snyder et al,2008; Snyder et al, 2009b; Naseem et al, 2009),and grammar induction (Snyder et al, 2009a; Blun-som et al, 2009; Burkett et al, 2010).
An evenmore recent line of work does away with the as-sumption of parallel texts and performs joint unsu-pervised induction for various languages through theuse of coupled priors in the context of grammar in-323duction (Cohen and Smith, 2009; Berg-Kirkpatrickand Klein, 2010).In contrast to these previous approaches, themethod proposed in this paper does not assume theexistence of any parallel text, but does assume thatlabeled data exists for a wide variety of languages, tobe used as training examples for our test language.3 Structured Nearest NeighborWe reformulate morphological induction as a super-vised learning task, where each annotated languageserves as a single training example for our language-independent model.
Each such example consistsof an input-label pair (x, y), both of which containcomplex internal structure: The input x ?
X con-sists of a vocabulary list of all words observed in aparticular monolingual corpus, and the label y ?
Yconsists of the correct morphological analysis of allthe vocabulary items in x.1 Because our goal isto generalize across languages, we define a featurefunction which maps each (x, y) pair to a universalfeature space: f : X ?
Y ?
Rd.For each unlabeled input language x, our goal isto predict a complete morphological analysis y ?
Ywhich maximizes a scoring function on the fea-ture space, score : Rd ?
R. This scoring func-tion is trained using the n labeled-language exam-ples: (x, y)1, .
.
.
, (x, y)n, and the resulting predic-tion rule for unlabeled input x is given by:y?
= argmaxy?Yscore(f(x, y))Languages can be typologically categorized bythe type and richness of their morphology.
On theassumption that for each test language, at least onetypologically similar language will be present in thetraining set, we employ a nearest neighbor scoringfunction.
In the standard nearest neighbor classifi-cation setting, one simply predicts the label of theclosest training example in the input space.2 In ourstructured prediction setting, the mapping to the uni-versal feature space depends crucially on the struc-ture of the proposed label y, not simply the input1Technically, the label space of each input, Y , should bethought of as a function of the input x.
We suppress this depen-dence for notational clarity.2More generally the majority label of the k-nearest neigh-bors.x.
We thus generalize nearest-neighbor predictionto the structured scenario and propose the followingprediction rule:y?
= argminy?Ymin`?
f(x, y)?
f(x`, y`) ?, (1)where the index ` ranges over the training languages.In words, we predict the morphological analysis yfor our test language which places it as close as pos-sible in the universal feature space to one of thetraining languages `.Morphological Analysis: In this paper we focuson nominal inflectional suffix morphology.
Considerthe word utiskom in Serbian, meaning impressionwith the instrumental case marking.
A correct analy-sis of this word would divide it into a stem (utisak =impression), a suffix (-om = instrumental case), anda phonological deletion rule on the stem?s penulti-mate vowel (..ak#?
..k#).More generally, as we define it, a morphologicalanalysis of a word type w consists of (i) a stem t, (ii),a suffix f , and (iii) a deletion rule d. Either or bothof the suffix and deletion rule can be NULL.
We al-low three types of deletion rules on stems: deletionof final vowels (..V# ?
..#), deletion of penulti-mate vowels (..V C# ?
..C#), and removals andadditions of final accent marks (e.g.
..a?# ?
..a#).We require that stems be at least three characterslong and that suffixes be no more than four.
And,of course, we require that after (1) applying deletionrule d to stem t, and (2) adding suffix f to the result,we obtain word w.Universal Feature Space: We employ a fairlysimple and minimal set of features, all of whichcould plausibly generalize across a wide range oflanguages.
Consider the set of stems T , suffixes F ,and deletion rules D, induced by the morphologicalanalyses y of the words x.
Our first three featuressimply count the sizes of these three sets.These counting features consider only the rawnumber of unique morphemes (and phonologicalrules) being used, but not their individual frequencyor distribution.
Our next set of features considersthe empirical entropy of these occurrences as dis-tributed across the lexicon of words x by analysis y.324f(x2, y2)f(x1, y1)f(x3, y3)y(t,1)y(t+1,1)y(t+1,2)y(t,2)y(t,3)y(t+1,3)f?x, y(0,?
)?InitializationFigure 1: Structured Nearest Neighbor Search: The inference procedure for unlabeled test language x, when trainedwith three labeled languages, (x1, y1), (x2, y2), (x3, y3).
Our search procedure iteratively attempts to find labels for xwhich are as close as possible in feature space to each of the training languages.
After convergence, the label which isclosest in distance to a training language is predicted, in this case being the label near training language (x3, y3).For example, if the (x, y) pair consists of the ana-lyzed words {kiss, kiss-es, hug}, then the empiricaldistributions over stems, suffixes, and deletion ruleswould be:?
P (t = kiss) = 2/3?
P (t = hug) = 1/3?
P (f = NULL) = 2/3?
P (f = ?es) = 1/3?
P (d = NULL) = 1The three entropy features are defined as the shan-non entropies of these stem, suffix, and deletion ruleprobabilities: H(t), H(f), H(d).3Finally, we consider two simple percentage fea-tures: the percentage of words in x which accordingto y are left unsegmented (i.e.
have the null suf-fix, 2/3 in the example above), and the percentage ofsegmented words which employ a deletion rule (0 inthe example above).
Thus, in total, our model em-ploys 8 universal morphological features.
All fea-tures are scaled to the unit interval and are assumedto have equal weight.3Note that here and throughout the paper, we operate overword types, ignoring their corpus frequencies.3.1 Search AlgorithmThe main algorithmic challenge for our model lies inefficiently computing the best morphological analy-sis y for each language-specific word set x, accord-ing to Equation 1.
Exhaustive search through theset of all possible morphological analyses is impos-sible, as the number of such analyses grows expo-nentially in the size of the vocabulary.
Instead, wedevelop a greedy search algorithm in the followingfashion (the search procedure is visually depicted inFigure 1).At each time-step t, we maintain a set of frontieranalyses {y(t,`)}`, where ` ranges over the traininglanguages.
The goal is to iteratively modify each ofthese frontier analyses y(t,`) ?
y(t+1,`) so that thelocation of the training language in universal featurespace ?
f(x, y(t+1,`)) ?
is as close as possible tothe location of the training language `: f(x`, y`).After iterating this procedure to convergence, weare left with a set of analyses {y(`)}`, each of whichapproximates the analyses which yield minimal dis-tances to a particular training language:y(`) ?
argminy?Y?
f(x, y)?
f(x`, y`) ?
.We finally select from amongst these analyses and325make our prediction:`?
= argmin`?
f(x, y(`))?
f(x`, y`) ?y?
= y(`?
)The main outline of our search algorithm is basedon the MDL-based greedy search heuristic devel-oped and studied by (Goldsmith, 2005).
At a highlevel, this search procedure alternates between indi-vidual analyses of words (keeping the set of stemsand suffixes fixed), aggregate discoveries of newstems (keeping the suffixes fixed), and aggregate dis-coveries of new suffixes (keeping stems fixed).
Asinput, we consider the test words x in our new lan-guage, and we run the search in parallel for eachtraining language (x`, y`).
For each such test-trainlanguage pair, the search consists of the followingstages:Stage 0: InitializationWe initially analyze each word w ?
x accordingto peaks in successor frequency.4 If w?s n-characterprefix w:n has successor frequency > 1 and the sur-rounding prefixes, w:n?1 and w:n+1 both have suc-cessor frequency = 1, then we analyze w as a stem-suffix pair: (w:n, wn+1:).5 Otherwise, we initializew as an unsuffixed stem.
As this procedure tends toproduce an overly large set of suffixes F , we furtherprune F down to the number of suffixes found inthe training language, retaining those which appearwith the largest number of stems.
This initializationstage is carried out once, and afterwards the follow-ing three stages are repeated until convergence.Stage 1: Reanalyze each wordIn this stage, we reanalyze each word (in randomorder).
We use the set of stems T and suffixes Fobtained from the previous stage, and don?t permitthe addition of any new items to these lists.
In-stead, we focus on obtaining better analyses of eachword, while also building up a set of phonologicaldeletion rules D. For each word w ?
x, we con-sider all possible segmentations of w into a stem-4The successor frequency of a string prefix s is defined asthe number of unique characters that occur immediately after sin the vocabulary.5With the restriction that at this stage we only allow suffixesup to length 5, and stems of at least length 3.suffix pair (t, f), for which f ?
F , and where ei-ther t ?
T or some t?
?
T such that t is obtainedfrom t?
using a deletion rule d (e.g.
by deleting afinal or penultimate vowel).
For each such possi-ble analysis y?, we compute the resulting locationin feature space f(x, y?
), and select the analysis thatbrings us closest to our target training language:y = argminy?
?
f(x, y?)?
f(x`, y`) ?
.Stage 2: Find New StemsIn this stage, we keep our set of suffixes F anddeletion rules D from the previous stage fixed, andattempt to find new stems to add to T through an ag-gregate analysis of unsegmented words.
For everystring s, we consider the set of words which are cur-rently unsegmented, and can be analyzed as a stem-suffix pair (s, f) for some existing suffix f ?
F ,and some deletion rule d ?
D. We then considerthe joint segmentation of these words into a newstem s, and their respective suffixes.
As before, wechoose the segmentation if it brings us closer in fea-ture space to our target training language.Stage 3: Find New SuffixesThis stage is exactly analogous to the previousstage, except we now fix the set of stems T and seekto find new suffixes.3.2 A Monolingual Supervised ModelIn order to provide a plausible upper bound on per-formance, we also formulate a supervised monolin-gual morphological model, using the structured per-ceptron framework (Collins, 2002).
Here we as-sume that we are given some training sequence of in-puts and morphological analyses (all within one lan-guage): (x1, y1), (x2, y2), .
.
.
, (xn, yn).
We defineeach input xi to be a noun w, along with a morpho-logical tag z, which specifies the gender, case, andnumber of the noun.
The goal is to predict the cor-rect segmentation of w into stem, suffix, and phono-logical deletion rule: yi = (t, f, d).6To do so, we define a feature function over input-label pairs, (x, y), with the following binary featuretemplates: (1) According to label yi, the stem is t6While the assumption of the correct morphological tag asinput is somewhat unrealistic, this model still gives us a strongupper bound on how well we can expect our unsupervisedmodel to perform.326Type Counts Entropy Percentage# words # stems # suffs # dels stem entropy suff entropy del entropy unseg deletedBG 4833 3112 21 8 11.4 2.7 0.9 .45 .29CS 5836 3366 28 12 11.5 3.2 1.6 .38 .53EN 4178 3453 3 1 11.7 1.0 0.1 .73 .06ET 6371 3742 141 5 11.5 5.0 0.2 .31 .04HU 8051 3746 231 7 11.3 5.8 0.5 .23 .11RO 5578 3297 23 8 11.5 2.9 1.4 .48 .51SL 6111 3172 32 6 11.3 3.2 1.5 .33 .56SR 5849 3178 28 5 11.4 2.9 1.4 .33 .53Table 1: Corpus statistics for the eight languages.
The first four columns give the number of unique word, stem, suffix,and phonological deletion rule types.
The next three columns give, respectively, the entropies of the distributionsof stems, suffixes (including NULL), and deletion rules (including NULL) over word types.
The final two columnsgive, respectively, the percentage of word types occurring with the NULL suffix, and the number of non-NULL suffixwords which use a phonological deletion rule.
Note that the final eight columns define the universal feature space usedby our model.
BG = Bulgarian, CS = Czech, EN = English, ET = Estonian, HU = Hungarian, RO = Romanian, SL =Slovene, SR = Serbian(one feature for each possible stem).
(2) Accord-ing to label yi, the suffix and deletion rule are (f, d)(one feature for every possible pair of deletion rulesand suffixes).
(3) According to label yi and morpho-logical tag z, the suffix, deletion rule, and genderare respectively (f, d,G).
(4) According to label yiand morphological tag z, the suffix, deletion rule,and case are (f, d, C).
(5) According to label yi andmorphological tag z, the suffix, deletion rule, andnumber are (f, d,N).We train a set of linear weights on our fea-tures using the averaged structured perceptron algo-rithm (Collins, 2002).4 ExperimentsIn this section we turn to experimental findings toprovide empirical support for our proposed frame-work.Corpus: To test our cross-lingual model, we ap-ply it to a morphologically analyzed corpus of eightlanguages (Erjavec, 2004).
The corpus includes aroughly 100,000 word English text, Orwell?s novel?Nineteen Eighty Four,?
and its translation intoseven languages: Bulgarian, Czech, Estonian, Hun-garian, Romanian, Slovene, and Serbian.
All thewords in the corpus are tagged with morphologi-cal stems and a detailed morpho-syntactic analysis.Although the texts are parallel, we note that par-allelism is nowhere assumed nor exploited by ourmodel.
See Table 1 for a summary of relevant cor-pus statistics.
As indicated in the table, the raw num-ber of nominal word types varies quite a bit acrossthe languages, almost doubling from 4,178 (English)to 8,051 (Hungarian).
In contrast, the number ofstems appearing within these words is relatively sta-ble across languages, ranging from a minimum of3,112 (Bulgarian) to a maximum of 3,746 (Hungar-ian), an increase of just 20%.In contrast, the number of suffixes across the lan-guages varies quite a bit.
Hungarian and Esto-nian, both Uralic languages with very complex nom-inal morphology, use 231 and 141 nominal suffixes,respectively.
Besides English, the remaining lan-guages employ between 21 and 32 suffixes, and En-glish is the outlier in the other direction, with justthree nominal inflectional suffixes.Baselines and Results: As our unsupervisedmonolingual baseline, we use the Linguistica pro-gram (Goldsmith, 2001; Goldsmith, 2005).
We ap-ply Linguistica?s default settings, and run the ?suffixprediction?
option.
Our model?s search procedureclosely mirrors the one used by Linguistica, withthe crucial difference that instead of attempting togreedily minimize description length, our algorithminstead tries to find the analysis as close as possi-ble in the universal feature space to that of anotherlanguage.To apply our model, we treat each of the eight327LinguisticaOur ModelSupervisedNearest Neighbor Self (oracle) Avg.Accuracy Distance Accuracy Distance Accuracy DistanceBG 68.7 84.0 (RO) 0.13 88.7 0.03 68.6 3.90 94.7CS 60.4 82.8 (BG) 0.40 84.5 0.03 66.3 4.05 93.5EN 81.1 75.8 (BG) 1.29 89.3 0.10 58.3 4.30 93.4ET 51.2 66.6 (HU) 0.35 80.9 0.03 52.8 4.57 86.5HU 64.5 69.3 (ET) 0.81 66.5 1.10 68.0 4.94 94.9RO 65.6 71.0 (CS) 0.11 71.2 0.15 62.3 3.95 89.1SL 61.1 82.8 (SR) 0.07 85.5 0.04 61.7 3.69 95.4SR 64.2 79.1 (SL) 0.06 82.2 0.04 63.0 3.71 94.8avg.
64.6 76.4 0.40 81.1 0.19 62.6 4.14 92.8Table 2: Prediction accuracy over word types for the Linguistica baseline, our cross-lingual model, and the monolin-gual supervised perceptron model.
For our model, we provide both prediction accuracy and resulting distance to thetraining language in three different scenarios: (i) Nearest Neighbor: The training languages include all seven otherlanguages in our data set, and the predictions with minimal distance to a training language are chosen (the nearestneighbor is indicated in parentheses).
(ii) Self (oracle): Each language is trained to minimize the distance to its owngold-standard analysis.
(iii) Average: The feature values of all seven training languages are averaged together tocreate a single objective.languages in turn as the test language, with the otherseven serving as training examples.
For each testlanguage, we iterate the search procedure for eachtraining language (performed in parallel), until con-vergence.
The number of required iterations variesfrom 6 to 36 (depending on the test-training lan-guage pair), and each iteration takes no more than 30seconds of run-time on a 2.4GHz Intel Xeon E5620processor.
We also consider two variants of ourmethod.
In the first (Self (oracle)), we train eachtest language to minimize the distance to its owngold standard feature values.
In the second variant(Avg.
), we average the feature values of all seventraining languages into a single objective.
As a plau-sible upper bound on performance, we implementedthe structured perceptron described in Section 3.2.For each language, we train the perceptron on a ran-domly selected set of 80% of the nouns, and test onthe remaining 20%.The prediction accuracy for all models is calcu-lated as the fraction of word types with correctlypredicted suffixes.
See Table 2 for the results.
Forall languages other than English (which is a mor-phological loner in our group of languages), ourmodel improves over the baseline by a substantialmargin, yielding an average increase of 11.8 abso-lute percentage points, and a reduction in error rela-tive to the supervised upper bound of 42%.
Some ofthe most striking improvements are seen on Serbianand Slovene.
These languages are closely relatedto one another, and indeed our model discovers thatthey are each others?
nearest neighbors.
By guidingtheir morphological analyses towards one another,our model achieves a 21 percentage point increasein the case of Slovene and a 15 percentage point in-crease in the case of Slovene.Perhaps unsurprisingly, when each language?sgold standard feature values are used as its owntarget (Self (oracle) in Table 2), performance in-creases even further, to an average of 81.1%.
By thesame token, the resulting distance in universal fea-ture space between training and test analyses is cutin half under this variant, when compared to the non-oracular nearest neighbor method.
The remainingerrors may be due to limitations of the search proce-dure (i.e.
getting caught in local minima), or to thecoarseness of the feature space (i.e.
incorrect analy-ses might map to the same feature values as the cor-rect analysis).
Finally, we note that minimizing thedistance to the average feature values of the seventraining languages (Avg.
in Table 2) yields subparperformance and very large distances between be-tween predicted analyses and target feature values(4.14 compared to 0.40 for nearest neighbor).
This328LinguisticaGold StandardOur MethodBGCSENETHUROSLSRBGCSENETHUROSLSRBGCSENETHUROSLSRFigure 2: Locations in Feature Space of Linguistica predictions (green squares), gold standard analyses (red tri-angles), and our model?s nearest neighbor predictions (blue circles).
The original 8-dimensional feature space wasreduced to two dimensions using Multidimensional Scaling.result may indicate that the average feature point be-tween training languages is simply unattainable asan analysis of a real lexicon of nouns.Visualizing Locations in Feature Space: Besidesassessing our method quantitatively, we can also vi-sualize the the eight languages in universal featurespace according to (i) their gold standard analyses,(ii) the predictions of our model and (iii) the pre-dictions of Linguistica.
To do so, we reduce the 8-dimensional features space down to two dimensionswhile preserving the distances between the predictedand gold standard feature vectors, using Multidi-mensional Scaling (MDS).
The results of this anal-ysis are shown in Figure 2.
With the exception ofEnglish, our model?s analyses lie closer in featurespace to their gold standard counterparts than thoseof the baseline.
It is interesting to note that Serbianand Slovene, which are very similar languages, haveessentially swapped places under our model?s anal-ysis, as have Estonian and Hungarian (both highlyinflected Uralic languages).
English has (unfortu-nately) been pulled towards Bulgarian, the secondleast inflecting language in our set.Learning Curves: We also measured the perfor-mance of our method as a function of the numberof languages in the training set.
For each target lan-guage, we consider all possible training sets of sizesranging from 1 to 7 and select the predictions whichbring our test language closest in distance to one ofthe languages in the set.
We then average the result-ing accuracy over all training sets of each size.
Fig-ure 3 shows the resulting learning curves averagedover all test languages (left), as well as broken downby test language (right).
The overall trend is clear:as additional languages are added to the training set,test performance improves.
In fact, with only onetraining language, our method performs worse (onaverage) than the Linguistica baseline.
However,with two or more training languages available, ourmethod achieves superior results.Accuracy vs.
Distance: We can gain some in-sight into these learning curves if we consider therelationship between accuracy (of the test languageanalysis) and distance to the training language (ofthe same predicted analysis).
The more training lan-guages available, the greater the chance that we canguide our test language into very close proximity to3291 2 3 4 5 6 7Number of training languages0.550.60.650.70.750.80.85BGCSSLSRENROHUET1 2 3 4 5 6 7Number of training languages0.620.640.660.680.70.720.740.76 Our ModelLinguisticaFigure 3: Learning curves for our model as the number of training languages increases.
The figure on the left showsthe average accuracy of all eight languages for increasingly larger training sets (results are averaged over all trainingsets of size 1,2,3,...).
The dotted line indicates the average performance of the baseline.
The figure on the right showssimilar learning curves, broken down individually for each test language (see Figure 1 for language abbreviations).one of them.
It thus stands to reason that a strong(negative) correlation between distance and accu-racy would lead to increased accuracy with largertraining sets.
In order to assess this correlation, weconsidered all 56 test-train language pairs and col-lected the resulting accuracy and distance for eachpair.
We separately scaled accuracy and distance tothe unit interval for each test language (as some testlanguages are inherently more difficult than others).The resulting plot, shown in Figure 4, shows the ex-pected correlation: When our test language can beguided very closely to the training language, the re-sulting predictions are likely to be good.
If not, thepredictions are likely to be bad.5 Conclusions and Future WorkThe approach presented in this paper recasts mor-phological induction as a structured prediction task.We assume the presence of morphologically labeledlanguages as training examples which guide the in-duction process for unlabeled test languages.
Wedeveloped a novel structured nearest neighbor ap-proach for this task, in which all languages and theirmorphological analyses lie in a universal featurespace.
The task of the learner is to search throughthe space of morphological analyses for the test lan-guage and return the result which lies closest to one0 0.2 0.4 0.6 0.8 1Distance (normalized)00.20.40.60.81Accuracy(normalized)Figure 4: Accuracy vs.
Distance: For all 56 possi-ble test-train language pairs, we computed test accuracyalong with resulting distance in universal feature spaceto the training language.
Distance and accuracy are sep-arately normalized to the unit interval for each test lan-guage, and all resulting points are plotted together.
Aline is fit to the points using least-squares regression.330of the training languages.
Our empirical findingsvalidate this approach: On a set of eight differentlanguages, our method yields substantial accuracygains over a traditional MDL-based approach in thetask of nominal morphological induction.One possible shortcoming of our approach is thatit assumes a uniform weighting of the cross-lingualfeature space.
In fact, some features may be far morerelevant than others in guiding our test language toan accurate analysis.
In future work, we plan to in-tegrate distance metric learning into our approach,allowing some features to be weighted more heavilythan others.
Besides potential gains in prediction ac-curacy, this approach may shed light on deeper rela-tionships between languages than are otherwise ap-parent.ReferencesMeni Adler and Michael Elhadad.
2006.
An un-supervised morpheme-based hmm for hebrew mor-phological disambiguation.
In Proceedings of theACL/CONLL, pages 665?672.Emily M. Bender.
2009.
Linguistically na?ve != lan-guage independent: why NLP needs linguistic typol-ogy.
In Proceedings of the EACL 2009 Workshopon the Interaction between Linguistics and Compu-tational Linguistics, pages 26?32, Morristown, NJ,USA.
Association for Computational Linguistics.Taylor Berg-Kirkpatrick and Dan Klein.
2010.
Phyloge-netic grammar induction.
In Proceedings of the ACL,pages 1288?1297, Uppsala, Sweden, July.
Associationfor Computational Linguistics.P.
Blunsom, T. Cohn, and M. Osborne.
2009.
Bayesiansynchronous grammar induction.
Advances in NeuralInformation Processing Systems, 21:161?168.David Burkett, Slav Petrov, John Blitzer, and Dan Klein.2010.
Learning better monolingual models with unan-notated bilingual text.
In Proceedings of CoNLL.Shay B. Cohen and Noah A. Smith.
2009.
Shared lo-gistic normal distributions for soft parameter tying inunsupervised grammar induction.
In Proceedings ofthe NAACL/HLT.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: theory and experi-ments with perceptron algorithms.
In Proceedings ofEMNLP, pages 1?8.T.
Cover and P. Hart.
1967.
Nearest neighbor patternclassification.
Information Theory, IEEE Transactionson, 13(1):21?27.Mathias Creutz and Krista Lagus.
2005.
Unsupervisedmorpheme segmentation and morphology inductionfrom text corpora using morfessor 1.0.
Publicationsin Computer and Information Science Report A81,Helsinki University of Technology.Mathias Creutz and Krista Lagus.
2007.
Unsupervisedmodels for morpheme segmentation and morphologylearning.
ACM Transactions on Speech and LanguageProcessing, 4(1).Ido Dagan, Alon Itai, and Ulrike Schwall.
1991.
Twolanguages are more informative than one.
In Proceed-ings of the ACL, pages 130?137.Sajib Dasgupta and Vincent Ng.
2007.
Unsuper-vised part-of-speech acquisition for resource-scarcelanguages.
In Proceedings of the EMNLP-CoNLL,pages 218?227.T.
Erjavec.
2004.
MULTEXT-East version 3: Multi-lingual morphosyntactic specifications, lexicons andcorpora.
In Fourth International Conference on Lan-guage Resources and Evaluation, LREC, volume 4,pages 1535?1538.John Goldsmith.
2001.
Unsupervised Learning of theMorphology of a Natural Language.
ComputationalLinguistics, 27(2):153?198.John Goldsmith.
2005.
An algorithm for the unsuper-vised learning of morphology.
Technical report, Uni-versity of Chicago.R.
Hwa, P. Resnik, A. Weinberg, C. Cabezas, and O. Ko-lak.
2005.
Bootstrapping parsers via syntactic projec-tion across parallel texts.
Journal of Natural LanguageEngineering, 11(3):311?325.M.P.
Marcus, B. Santorini, and M.A.
Marcinkiewicz.1994.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational linguistics,19(2):313?330.Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, andRegina Barzilay.
2009.
Multilingual part-of-speechtagging: two unsupervised approaches.
Journal of Ar-tificial Intelligence Research, 36(1):341?385.Sebastian Pad?
and Mirella Lapata.
2006.
Optimal con-stituent alignment with edge covers for semantic pro-jection.
In Proceedings of ACL, pages 1161 ?
1168.Hoifung Poon, Colin Cherry, and Kristina Toutanova.2009.
Unsupervised morphological segmentation withlog-linear models.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, NAACL ?09, pages 209?217, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Philip Resnik and David Yarowsky.
1997.
A perspectiveon word sense disambiguation methods and their eval-uation.
In Proceedings of the ACL SIGLEX Workshop331on Tagging Text with Lexical Semantics: Why, What,and How?, pages 79?86.Patrick Schone and Daniel Jurafsky.
2001.
Knowledge-free induction of inflectional morphologies.
In NAACL?01: Second meeting of the North American Chapter ofthe Association for Computational Linguistics on Lan-guage technologies 2001, pages 1?9, Morristown, NJ,USA.
Association for Computational Linguistics.Benjamin Snyder and Regina Barzilay.
2008a.
Cross-lingual propagation for morphological analysis.
InProceedings of the AAAI, pages 848?854.Benjamin Snyder and Regina Barzilay.
2008b.
Unsuper-vised multilingual learning for morphological segmen-tation.
In Proceedings of the ACL/HLT, pages 737?745.Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, andRegina Barzilay.
2008.
Unsupervised multilinguallearning for POS tagging.
In Proceedings of EMNLP,pages 1041?1050.Benjamin Snyder, Tahira Naseem, and Regina Barzilay.2009a.
Unsupervised multilingual grammar induction.In Proceedings of the ACL, pages 73?81.Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, andRegina Barzilay.
2009b.
Adding more languages im-proves unsupervised multilingual part-of-speech tag-ging: a Bayesian non-parametric approach.
In Pro-ceedings of the NAACL, pages 83?91.David Yarowsky and Grace Ngai.
2001.
Inducing mul-tilingual pos taggers and np bracketers via robust pro-jection across aligned corpora.
In Proceedings of theNAACL, pages 1?8.David Yarowsky and Richard Wicentowski.
2000.
Min-imally supervised morphological analysis by multi-modal alignment.
In ACL ?00: Proceedings of the38th Annual Meeting on Association for Computa-tional Linguistics, pages 207?216, Morristown, NJ,USA.
Association for Computational Linguistics.David Yarowsky, Grace Ngai, and Richard Wicentowski.2000.
Inducing multilingual text analysis tools via ro-bust projection across aligned corpora.
In Proceedingsof HLT, pages 161?168.332
