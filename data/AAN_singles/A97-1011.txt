A non-project ive dependency parserPasi Tapanainen and Timo J~irvinenUniversity of Helsinki, Department of General LinguisticsResearch Unit for Multilingual Language TechnologyP.O.
Box 4, FIN-00014 University of Helsinki, Finland{Pas i. Tapanainen, Timo.
Jarvinen}@l ing.
Hel s inki.
f iAbstractWe describe a practical parser for unre-stricted dependencies.
The parser createslinks between words and names the linksaccording to their syntactic functions.
Wefirst describe the older Constraint Gram-mar parser where many of the ideas comefrom.
Then we proceed to describe the cen-tral ideas of our new parser.
Finally, theparser is evaluated.1 IntroductionWe are concerned with surface-syntactic parsing ofrunning text.
Our main goal is to describe syntac-tic analyses of sentences using dependency links thatshow the he~t-modifier relations between words.
Inaddition, these links have labels that refer to thesyntactic function of the modifying word.
A simpli-fied example is in Figure 1, where the link betweenI and see denotes that I is the modifier of see andits syntactic function is that of subject.
Similarly, amodifies bird, and it is a determiner.see bii ~ d '~b~ birdFigure 1: Dependencies for sentence I see a bird.First, in this paper, we explain some central con-cepts of the Constraint Grammar framework fromwhich many of the ideas are derived.
Then, we givesome linguistic background to the notations we areusing, with a brief comparison to other current de-pendency formalisms and systems.
New formalismis described briefly, and it is utilised in a small toygrammar to illustrate how the formalism works.
Fi-nally, the real parsing system, with a grammar ofsome 2 500 rules, is evaluated.64The parser corresponds to over three man-years ofwork, which does not include the lexical analyser andthe morphological disambiguator, both parts of theexisting English Constraint Grammar parser (Karls-son et al, 1995).
The parsers can be tested viaWWW t .2 BackgroundOur work is partly based on the work done withthe Constraint Grammar  framework that was orig-inally proposed by Fred Karlsson (1990).
A de-tMled description of the English Constraint Gram-mar (ENGCG) is in Karlsson et al (1995).
The basicrule types of the Constraint Grammar (Tapanainen,1996) 2 are REMOVE and SELECT for discarding and se-lecting an alternative reading of a word.
Rules alsohave contextual tests that describe the condition ac-cording to which they may be applied.
For example,the ruleREMOVE (V) IF (-1C DET);discards a verb (V) reading if the preceding word(-1) is unambiguously (C) a determiner (DET).
Morethan one such test can be appended to a rule.The rule above represents a local rule: the testchecks only neighbouring words in a foreknown po-sition before or after the target word.
The test mayalso refer to the positions omewhere in the sentencewithout specifying the exact location.
For instance,SELECT (IMP) IF (NOT *-1 NOM-HEAD);means that a nominal head (NOM-HEAD is a set thatcontains part-of-speech tags that may represent anominal head) may not appear anywhere to the left(NOT *-1).~at http://www.ling.helsinki.f i /~tapanain/dg/~The CG-2 notation here (Tapanainen, 1996) is dif-ferent from the former (Karlsson et al, 1995).
A con-cise introduction to the formalism is also to be found inSamuelsson et al (1996) and Hurskainen (1996).This "anywhere" to the left or right may be re-stricted by BARRIERs, which restrict the area of thetest.
Basically, the barrier can be used to limitthe test only to the current clause (by using clauseboundary markers and "stop words") or to a con-stituent (by using "stop categories") instead of thewhole sentence.
In addition, another test may beadded relative to the unrestricted context positionusing keyword LINK.
For example, the following rulediscards the syntactic function 3 QI-0BJ (indirect ob-ject):REMOVE (@I-OeJ)IF (*-1C VFIN BARRIER SVOOLINK NOT 0 SVOO);The rule holds if the closest finite verb to the left isunambiguously (C) a finite verb (VFIN), and there isno ditransitive verb or participle (subcategorisationSV00) between the verb and the indirect object.
If,in addition, the verb does not take indirect objects,i.e.
there is no SY00 in the same verb (LINg NOT 0SV00), the @I-0BJ reading will be discarded.In essence, the same formalism is used in the syn-tactic analysis in J~rvinen (1994) and Anttila (1995).After the morphological disambiguation, all legiti-mate surface-syntactic labels are added to the set ofmorphological readings.
Then, the syntactic rulesdiscard contextually illegitimate alternatives or se-lect legitimate ones.The syntactic tagset of the Constraint Grammarprovides an underspecific dependency description.For example, labels for functional heads (such as?SUB J, ?0B J, ?I-0BJ) mark the word which is a headof a noun phrase having that function in the clause,but the parent is not indicated.
In addition, the rep-resentation is shallow, which means that, e.g., ob-jects of infinitives and participles receive the sametype of label as objects of finite verbs.
On the otherhand, the non-finite verb forms functioning as ob-jects receive only verbal labels.When using the grammar formalism describedabove, a considerable amount of syntactic ambigu-ity can not be resolved reliably and is therefore leftpending in the parse.
As a consequence, the outputis not optimal in many applications.
For example, itis not possible to reliably pick head-modifier pairsfrom the parser output or collect arguments of verbs,which was one of the tasks we originally were inter-ested in.To solve the problems, we developed a more pow-erful rule formalism which utilises an explicit depen-dency representation.
The basic Constraint Gram-3The convention i  the Constraint Grammar is thatthe tags for syntactic functions begin with the @-sign.65mar idea of introducing the information in a piece-meal fashion is retained, but the integration of dif-ferent pieces of information is more efficient in thenew system.3 Dependency  grammars  in anutshel lOur notation follows the classical model of depen-dency theory (Heringer, 1993) introduced by LucienTesni~re (1959) and later advocated by Igor Mel'~uk(1987).3.1 Un iqueness  and  pro jec t iv i tyIn Tesni~re's and Mel'Suk's dependency notation ev-ery element of the dependency tree has a uniquehead.
The verb serves as the head of a clause andthe top element of the sentence is thus the mainverb of the main clause.
In some other theories, e.g.Hudson (1991), several heads are allowed.Projectivity (or adjacency 4) was not an issue forTesni~re (1959, ch.
10), because he thought hat thelinear order of the words does not belong to the syn-tactic level of representation which comprises thestructural order only.Some early formalisations, c.f.
(Hays, 1964), havebrought he strict projectivity (context-free) require-ment into the dependency framework.
This kindof restriction is present in many dependency-basedparsing systems (McCord, 1990; Sleator and Tem-perley, 1991; Eisner, 1996).But obviously any recognition grammar shoulddeal with non-projective phenomena to the extentthey occur in natural anguages as, for example, inthe analysis hown in Figure 2.
Our system has noin-built restrictions concerning projectivity, thoughthe formalism allows us to state when crossing linksare not permitted.We maintain that one is generally also interestedin the linear order of elements, and therefore it ispresented in the tree diagrams.
But, for some pur-poses, presenting all arguments in a canonical ordermight be more adequate.
This, however, is a matterof output formatting, for which the system makesseveral options available.3.2 Va lency and  categor iesThe verbs (as well as other elements) have a valencythat describes the number and type of the modifiersthey may have.
In valency theory, usually, comple-ments (obligatory) and adjuncts (optional) are dis-tinguished.4D is adjacent to H provided that every word betweenD and H is a subordinate of H (Hudson, 1991).main: <ROOT><SAID>VFIN obj:<JOAN>N SGSG3 VFI IV'~ bj: / ~<DECIDE><WHATEVER><J OHN> <TO>PRON WH N SG INFMARK><SUITS>SG3 VFIN<LIKES> <HER>PRON ACC SG3Figure 2: A dependency structure for the sentence: Joan said whatever John likes to decide suits her.Our notation makes a difference between valency(rule-based) and subcategorisation (lexical): the va-lency tells which arguments are expected; the sub-categorisation tells which combinations are legiti-mate.
The valency merely provides a possibility tohave an argument.
Thus, a verb having three va-lency slots may have e.g.
subcategorisation SV00 orSV0C.
The former denotes: Subject, Verb, indirectObject and Object, and the latter: Subject, Verb,Object and Object Complement.
The default is anominal type of complement, but there might alsobe additional information concerning the range ofpossible complements, e.g., the verb say may havean object (SV0), which may also be realised as ato-infinitive clause, WH-clause, that-clause or quotestructure.The adjuncts are not usually marked in the verbsbecause most of the verbs may have e.g.
spatio-temporal arguments.
Instead, adverbial comple-ments and adjuncts that are typical of particularverbs are indicated.
For instance, the verb decidehas the tag <P/on> which means that the preposi-tional phrase on is typically attached to it.The distinction between the complements and theadjuncts is vague in the implementation; neither thecomplements nor the adjuncts are obligatory.4 In t roduc ing  the  dependenc iesUsually, both the dependent element and its headare implicitly (and ambiguously) present in the Con-straint Grammar type of rule.
Here, we make thisdependency relation explicit.
This is done by declar-ing the heads and the dependents (complement ormodifier) in the context ests.For example, the subject label (@SUB J) is chosenand marked as a dependent of the immediately fol-66lowing auxiliary (AUXMOD) in the following rule:SELECT (@SUBJ) IF (1C AUXMOD HEAD);To get the full benefit of the parser, it is also use-ful to name the valency slot in the rule.
This hastwo effects: (1) the valency slot is unique, i.e.
nomore than one subject is linked to a finite verb 5,and (2) we can explicitly state in rules which kindof valency slots we expect o be filled.
The rule thusis of the form:SELECT (@SUB J)IF (1C AUXMOD HEAD = subject);The rule above works well in an unambiguous con-text but there is still need to specify more tolerantrules for ambiguous contexts.
The ruleINDEX (@SUB J)IF (1C AUXMOD HEAD = subject);differs from the previous rule in that it leaves theother readings of the noun intact and only adds a(possible) subject dependency, while both the previ-ous rules disambiguated the noun reading also.But especially in the rule above, the contextualtest is far from being sufficient o select the subjectreading reliably.
Instead, it leaves open a possibil-ity to attach a dependency from another syntacticfunction, i.e.
the dependency relations remain am-biguous.
The grammar tries to be careful not tointroduce false dependencies but for an obvious rea-son this is not always possible.
If several syntac-tic functions of a word have dependency relations,they form a dependency forest.
Therefore, when thesyntactic function is not rashly disambiguated, thecorrect reading may survive even after illegitimate5 Coordination ishandled via the coordinator that col-lects coordinated subjects in one slot.linking, as the global pruning (Section 5) later ex-tracts dependency links that form consistent trees.Links formed between syntactic labels constitutepartial trees, usually around verbal nuclei.
But anew mechanism is needed to make full use of thestructural information provided by multiple rules.Once a link is formed between labels, it can be usedby the other rules.
For example, when a head of anobject phrase (?0B J) is found and indexed to a verb,the noun phrase to the right (if any) is probably anobject complement (?PCOMPL-0).
It should have thesame head as the existing object if the verb has theproper subcategorisation tag (SV0C).
The followingrule establishes a dependency relation of a verb andits object complement, if the object already exists.INDEX (@PCOMPL-O)IF (*-1 @OBJ BARRIER @NPHEADLINK 0 UP object SVOC HEAD=o-compl);The rule says that a dependency relation (o-?omp1)should be added but the syntactic functions shouldnot be disambiguated (INDEX).
The object comple-ment ?PCOMPL-0 is linked to the verb readings hav-ing the subcategorisation SV0C.
The relation of theobject complement and its head is such that thenoun phrase to the left of the object complement isan object (QOBJ) that has established a dependencyrelation (ob ject )  to the verb.Naturally, the dependency relations may also befollowed downwards (DOWN).
But it is also possible todeclare the last item in a chMn of the links (e.g.
theverb chain would have been wanted) using the key-words TOP and BOTTOM.5 Ambigu i ty  and  prun ingWe pursue the following strategy for linking and dis-ambiguation.
* In the best case, we are sure that some readingis correct in the current context.
In this case,both disambiguation and linking can be doneat the same time (with command SELECT andkeyword HEAD).e The most typical case is that the context givessome evidence about the correct reading, but weknow that there are some rare instances whenthat reading is not correct.
In such a case, weonly add a link.e Sometimes the context gives strong hints as towhat the correct reading can not be.
In sucha case we can remove some readings even ifwe do not know what the correct alternativeis.
This is a fairly typical case in the Con-straint Grammar  framework, but relatively rare67in the new dependency grammar.
In practice,these rules are most likely to cause errors, apartfrom their linguistic interpretation often beingrather obscure.
Moreover, there is no longerany need to remove these readings explicitly byrules, because the global pruning removes read-ings which have not obtained any "extra evi-dence".Roughly, one could say that the REMOVE rules ofthe Constraint Grammar  are replaced by the INDEXrules.
The overall result is that the rules in thenew framework are much more careful than thoseof ENGCG.As already noted, the dependency grammar hasa big advantage over ENGCG in dealing with am-biguity.
Because the dependencies are supposed toform a tree, we can heuristically prune readings thatare not likely to appear in such a tree.
We have thefollowing hypotheses: (1) the dependency forest isquite sparse and a whole parse tree can not alwaysbe found; (2) pruning should favour large (sub)trees;(3) unlinked readings of a word can be removed whenthere is a linked reading present among the alterna-tives; (4) unambiguous subtrees are more likely to becorrect than ambiguous ones; and (5) pruning neednot force the words to be unambiguous.
Instead,we can apply the rules iteratively, and usually someof the rules apply when the ambiguity is reduced.Pruning is then applied agMn, and so on.
Further-more, the pruning mechanism does not contain anylanguage specific statistics, but works on a topolog-ical basis only.Some of the most heuristic rules may be appliedonly after pruning.
This has two advantages: veryheuristic links would confuse the pruning mecha-nism, and words that would not otherwise have ahead, may still get one.6 Toy-grammar  exampleIn this section, we present a set of rules, and showhow those rules can parse the sentence "Joan saidwhatever John likes to decide suits her".
The toygrammar containing 8 rules is presented in Figure 3.The rules are extracted from the real grammar,  andthey are then simplified; some tests are omitted andsome tests are made simpler.
The grammar is ap-plied to the input sentence in Figure 4, where thetags are almost equivalent o those used by theEnglish Constraint Grammar,  and the final resultequals Figure 2, where only the dependencies be-tween the words and certain tags are printed.Some comments concerning the rules in the toygrammar (Figure 3) are in order:INDEX (@SUBJ) IF (1 @+F HEAD --- subj:);INDEX (INF @-FMAINV) IF (-1 INFMARK) (-2 PTC1-COMPL-V + SVO HEAD -- obj:);INDEX (@INFMARK>) IF (1 (INF @-FMAINV) HEAD -- infmark:);SELECT (PRON ACC @OBJ) IF (1C CLB) (-1 @MAINV HEAD -- obj:);INDEX (PRON WH @OH J)IF (*1 @SUBJ BARRIER @NPHEAD-MAINL INK 0 UP subj: @+FL INK 0 TOP v-ch: @MAINVL INK 0 BOTTOM obj: SVO + @-FMAINV HEAD = obj:);INDEX @MAINVIF (*-1 WH BARRIER @MV-CLB/CC L INK -1 @MV-CLB/CC)(*IC @+F BARRIER @SUBJ OR CLB HEAD = subj:);PRUNINGINDEX @MAINVIF (NOT *1 @+F BARRIER SUB J-BARRIER)(*-1 (PRON WH) BARRIER CLB L INK -1 VCOG + SVO + @MAINV HEAD = obj:);INDEX @+FMAINVIF (NOT 0 @+FAUXV) (NOT "1 @+F BARRIER CLB)(0 DOWN subj: @SUBJ L INK NOT *-1 @CS) (@0 (<s>) HEAD = main:);Figure 3: A toy grammar of 8 rules#(1)#(2)#(3)#(4)#(5)#(6)#(*)#(7)#(8)1.
A simple rule shows how the subject (QSUBJ) isindexed to a finite verb by a link named subj.2.
The infinitives preceded by the infinitive markerto can be reliably linked to the verbs with theproper subcategorisation, i.e.
the verb belongsto both categories PTCl-COHPL-V and SV0.3.
The infinitive marker is indexed to the infinitiveby the link named infmaxk.4.
Personal pronouns have morphological mbigu-ity between nominative (NOM) and accusative(ACC) readings.
Here, the accusative readingis selected and linked to the main verb imme-diately to the left, if there is an unambiguousclause boundary immediately to the right.5.
The WH-pronoun is a clause boundary marker,but the only reliable means to find its head isto follow the links.
Therefore, the WH-pronounis not indexed before the appropriate subject islinked to the verb chain which also has a verbalobject.The rule states: the first noun phrase head la-bel to the right is a subject (?SUB J), link subjexists and is followed up to the finite verb (?+F)in a verb chain (v-ch), which is then followedup to the main verb.
Then object or comple-ment links are followed downwards (BOTTOH),.68to the last verbal reading (here decide).
If thena verb with subcategorisation f r objects is en-countered, an object link from the WH-pronounis formed.This kind of rule that starts from word A, fol-lows links up to word B and then down to wordC, introduces a non-projective dependency linkif word B is between words A and C.Note that the conditions TOP and BOTT0X followthe chain of named link, if any, to the upper orlower end of a chain of a multiple (zero or more)links with the same name.
Therefore TOP v-ch:@MAINValways ends with the main verb in theverb chain, whether this be a single finite verblike likes or a chain like would have been liked.The WH-clause itself may function as a subject,object, etc.
Therefore, there is a set of rulesfor each function.
The "WH-clause as subject"rule looks for a finite verb to the right.
No in-tervening subject labels and clause boundariesare allowed.Rules 1-5 are applied in the first round.
Afterthat, the pruning operation disambiguates finiteverbs, and rule 6 will apply.Pruning will be applied once again.
The sen-tence is thus disambiguated both morphologi-cally and morphosyntactically, and a syntactic"< Joan>""joan" N NOM SG @NH @SUBJ @OBJ @I-OBJ @PCOMPL-S @PCOMPL-O @APP @A> @<P @O-ADVL"<said>""say" PCP2 @<P-FMAINV @-FMAINV"say" V PAST VFIN @+FMAINV"say" A ABS ~PCOMPL-S @PCOMPL-O @A> @APP @SUBJ @OBJ @I-OBJ @<P @<NOM"< what ever>""whatever" ADV @ADVL @AD-A>"whatever" DET CENTRAL WH SG/PL @DN>"whatever" <CLB> PRON WH SG/PL @SUBJ ~OBJ @I-OBJ ~PCOMPL-S @PCOMPL-O @<P @<NOM"< John>""john" N NOM SG ~NH @SUBJ ~OBJ ~I-OBJ @PCOMPL-S @PCOMPL-O ~APP ~A> ~<P ~O-ADVL"<likes>""like" N NOM PL @NH @SUBJ @OBJ @I-OBJ @PCOMPL-S @PCOMPL-O @APP @A> ~<P @O-ADVL"like" V PRES SG3 VFIN ~+FMAINV"<to>""to" PREP @<NOM @ADVL"to" INFMARK> ~INFMARK>"<decide>""decide" V SUBJUNCTIVE VFIN @+FMAINV"decide" V IMP VFIN @+FMAINV"decide" V INF ~-FMAINV ~<P-FMAINV"decide" V PRES -SG3 VFIN ~+FMAINV"<suits>""suit" V PRES SG3 VFIN @+FMAINV"suit" N NOM PL @NH @SUBJ @OBJ @I-OBJ @PCOMPL-S @PCOMPL-O @APP @A> @<P @O-ADVL"<her>""she" PRON PERS FEM GEN SG3 @GN>"she" PRON PERS FEM ACC SG3 ~OBJ"<.>"Figure 4: A sentence after morphological analysis.
Each line presents a morphological and @-signs mor-phosyntactic alternatives, e.g.
whatever is ambiguous in 10 ways.
The subcategorisation/valency i formationis not printed here.reading from each word belongs to a subtree ofwhich the root is said or suits.7.
The syntactic relationship between the verbs isestablished by a rule stating that the rightmostmain verb is the (clause) object of a main verbto the left, which Mlows such objects.8.
Finally, there is a single main verb, which isindexed to the root (<s>) (in position GO).7 Eva luat ion7.1 Ef f ic iencyThe evaluation was done using small excerpts ofdata, not used in the development of the system.All text samples were excerpted from three differentgenres in the Bank of English (J~irvinen, 1994) data:American National Public Radio (broadcast), BritishBooks data (literature), and The Independent (news-paper).
Figure 5 lists the samples, their sizes, andthe average and maximum sentence lengths.
Themeasure is in words excluding punctuation.69size avg.
max.
total(w) length length timebroadcast 2281 19 44 12 sec.literature 1920 15 51 8.5 sec.newspaper 1427 19 47 8.5 sec.Figure 5: Benchmark used in the evaluationIn addition, Figure 5 shows the total processingtime required for the syntactic analysis of the sam-ples.
The syntactic analysis has been done in a nor-mal PC with the Linux operating system.
The PChas a Pentium 90 MHz processor and 16 MB of mem-ory.
The speed roughly corresponds to 200 words insecond.
The time does not include morphologicalanMysis and disambiguation 6.6The CG-2 program (Tapanainen, 1996) runs a mod-ified disambiguation grammar of Voutilainen (1995)about 1000 words in second.DG ENGCGsucc.
arab.
succ.
amb.broadcast 97.0 % 3.2 % 96.8 % 12.7 %literature 97.3 % 3.3 % 95.9 % 11.3 %newspaper 96.4 % 3.3 % 94.2 % 13.7 %Figure 6: ENGCG syntax and morphosyntactic levelof the dependency grammar7.2 Compar i son  to  ENGCG syntaxOne obvious point of reference is the ENGCG syn-tax, which shares a level of similar representationwith an almost identical tagset to the new system.In addition, both systems use the front parts of theENGCG system for processing the input.
These in-clude the tokeniser, lexical analyser and morpholog-ical disambiguator.Figure 6 shows the results of the comparison of theENGCG syntax and the morphosyntactic level of thedependency grammar.
Because both systems leavesome amount of the ambiguity pending, two figuresare given: the success rate, which is the percent-age of correct morphosyntactic labels present in theoutput, and the ambiguity rate, which is the percent-age of words containing more than one label.
TheENGCG results compare to those reported elsewhere(J~rvinen, 1994; Tapanainen and J/~rvinen, 1994).The DG success rate is similar or maybe evenslightly better than in ENGCG.
More importantly,the ambiguity rate is only about a quarter of thatin the ENGCG output.
The overall result should beconsidered good in the sense that the output con-tains information about the syntactic functions (seeFigure 4) not only part-of-speech tags.7.3 Dependenc iesThe major improvement over ENGCG is the levelof explicit dependency representation, which makesit possible to excerpt modifiers of certain elements,such as arguments of verbs.
This section evaluatesthe success of the level of dependencies.7.3.1 Unnamed dependenc iesOne of the crude measures to evaluate depen-dencies is to count how many times the correcthead is found.
The results are listed in Fig-ure 7.
Precision is \[ received correct links~ and re-received links Jcall /received correct links ~ The difference betweendesired links.
J"precision and recall is due to the fact that the parserdoes not force a head on every word.
Trying outsome very heuristic methods to assign heads wouldraise recall but lower precision.
A similar measureprecision recallbroadcast 93.4 % 88.0 %literature 96.0 % 88.6 %newspaper 95.3 % 87.9 %Figure 7: Percentages of heads correctly attachedbroadcast precision recall Nsubjects 95 % 89 % 244objects 89 % 83 % 140predicatives 96 % 86 % 57literature precision recall Nsubjects 98 % 92 % 195objects 94 % 91% 118predicatives 97 % 93 % 72newspaper precision recall Nsubjects 95 % 83 % 136objects 94 % 88 % 103predicatives 92 % 96 % 23Figure 8: Rates for main functional dependenciesis used in (Eisner, 1996) except hat every word hasa head, i.e.
the precision equals recall, reported as79.2%.7.3.2 Named dependenc iesWe evaluated our parser against the selected e-pendencies in the test samples.
The samples be-ing rather small, only the most common dependen-cies are evaluated: subject, object and predicative.These dependencies are usually resolved more re-liably than, say, appositions, prepositional attach-ments etc.
The results of the test samples are listedin Figure 8.
It seems the parser leaves some amountof the words unlinked (e.g.
10-15 % of subjects) butwhat it has recognised is generally correct (precision95-98% for subjects).Dekang Lin (1996) has earlier used this kind ofevaluation, where precision and recall were for sub-jects 87 % and 78 %, and for complements (includ-ing objects) 84 % and 72 %, respectively.
The resultsare not strictly comparable because the syntactic de-scription is somewhat different.8 Conc lus ionIn this paper, we have presented some main featuresof our new framework for dependency syntax.
Themost important result is that the new framework al-lows us to describe non-projective dependency gram-mars and apply them efficiently.
This is a property70that will be crucial when we will apply this frame-work to a language having free word-order.Basically, the parsing framework combines theConstraint Grammar f amework (removing ambigu-ous readings) with a mechanism that adds depen-dencies between readings or tags.
This means thatwhile the parser disambiguates it also builds up adependency forest that, in turn, is reduced by otherdisambiguation rules and a global pruning mecha-nism.This setup makes it possible to operate on severallayers of information, and use and combine struc-tural information more efficiently than in the orig-inal Constraint Grammar framework, without anyfurther disadvantage in dealing with ambiguity.First preliminary evaluations are presented.
Com-pared to the ENGCG syntactic analyser, the outputnot only contains more information but it is alsomore accurate and explicit.
The ambiguity rate isreduced to a quarter without any compromise incor-rectness.
We did not have access to other systems,and care must be taken when interpreting the re-sults which are not strictly comparable.
However,the comparison to other current systems uggeststhat our dependency parser is very promising boththeoretically and practically.AcknowledgmentsWe are using Atro Voutilainen's (1995) improvedpart-of-speech disambiguation grammar which runs inthe CG-2 parser.
Voutilainen and Juha Heikkil5 createdthe original ENGCG lexicon.Re ferencesArto Anttila.
1995.
How to recognise subjects inEnglish.
In Karlsson et al, chapt.
9, pp.
315-358.Dekang Lin.
1996.
Evaluation of Principar with theSusanne corpus.
In John Carroll, editor, Work-shop on Robust Parsing, pages 54-69, Prague.Jason M. Eisner.
1996.
Three new probabilisticmodels for dependency parsing: An exploration.In The 16th International Conference on Compu-tational Linguistics, pages 340-345.
Copenhagen.David G. Hays.
1964.
Dependency theory: Aformalism and some observations.
Language,40(4):511-525.Hans Jiirgen Heringer.
1993.
Dependency syntax -basic ideas and the classical model.
In JoachimJacobs, Arnim von Stechow, Wolfgang Sternefeld,and Thee Venneman, editors, Syntax - An In-ternational Handbook of Contemporary Research,volume 1, chapter 12, pages 298-316.
Walter deGruyter, Berlin - New York.Richard Hudson.
1991.
English Word Grammar.Basil Blackwell, Cambridge, MA.Arvi Hurskainen.
1996.
Disambiguation f morpho-logical analysis in Bantu languages.
In The 16thInternational Conference on Computational Lin-guistics, pages 568-573.
Copenhagen.Time J~rvinen.
1994.
Annotating 200 millionwords: the Bank of English project.
In The 15thInternational Conference on Computational Lin-guistics Proceedings, pages 565-568.
Kyoto.Fred Karlsson, Atro Voutilainen, Juha Heikkil~, andArto Anttila, editors.
1995.
Constraint Gram-mar: a language-independent system for parsingunrestricted text, volume 4 of Natural LanguageProcessing.
Mouton de Gruyter, Berlin and N.Y.Fred Karlsson.
1990.
Constraint grammar as aframework for parsing running text.
In Hans Karl-gren, editor, Papers presented to the 13th Interna-tional Conference on Computational Linguistics,volume 3, pages 168-173, Helsinki, Finland.Michael McCord.
1990.
Slot grammar: A system forsimpler construction ofpractical natural languagegrammars.
In lq, Studer, editor, Natural Languageand Logic: International Scientific Symposium,Lecture Notes in Computer Science, pages 118-145.
Springer, Berlin.Igor A. Mel'~uk.
1987.
Dependency Syntax: Theoryand Practice.
State University of New York Press,Albany.Christer Samuelsson, Pasi Tapanainen, and AtroVoutilainen.
1996.
Inducing constraint gram-mars.
In Laurent Miclet and Colin de la Higuera,editors, Grammatical Inference: Learning Syntaxfrom Sentences, volume 1147 of Lecture Notes inArtificial Intelligence, pages 146-155, Springer.Daniel Sleator and Davy Temperley.
1991.
ParsingEnglish with a link grammar.
Technical ReportCMU-CS-91-196, Carnegie Mellon University.Pasi Tapanainen and Time J/irvinen.
1994.
Syn-tactic analysis of natural anguage using linguis-tic rules and corpus-based patterns.
In The 15thInternational Conference on Computational Lin-guistics Proceedings, pages 629-634.
Kyoto.Pasi Tapanainen.
1996.
The Constraint GrammarParser CG-2.
Number 27 in Publications of theDepartment of General Linguistics, University ofHelsinki.Lucien TesniSre.
1959. l~ldments de syntaxe stvuc-turale, l~ditions Klincksieck, Paris.Atro Voutilainen.
1995.
Morphological disambigua-tion.
In Karlsson et al, chapter 6, pages 165-284.71
