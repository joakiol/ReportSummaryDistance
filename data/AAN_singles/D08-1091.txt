Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 867?876,Honolulu, October 2008. c?2008 Association for Computational LinguisticsSparse Multi-Scale Grammarsfor Discriminative Latent Variable ParsingSlav Petrov and Dan KleinComputer Science Division, EECS DepartmentUniversity of California at BerkeleyBerkeley, CA 94720{petrov, klein}@eecs.berkeley.eduAbstractWe present a discriminative, latent variableapproach to syntactic parsing in which rulesexist at multiple scales of refinement.
Themodel is formally a latent variable CRF gram-mar over trees, learned by iteratively splittinggrammar productions (not categories).
Dif-ferent regions of the grammar are refined todifferent degrees, yielding grammars whichare three orders of magnitude smaller thanthe single-scale baseline and 20 times smallerthan the split-and-merge grammars of Petrovet al (2006).
In addition, our discriminativeapproach integrally admits features beyond lo-cal tree configurations.
We present a multi-scale training method along with an efficientCKY-style dynamic program.
On a variety ofdomains and languages, this method producesthe best published parsing accuracies with thesmallest reported grammars.1 IntroductionIn latent variable approaches to parsing (Matsuzakiet al, 2005; Petrov et al, 2006), one models an ob-served treebank of coarse parse trees using a gram-mar over more refined, but unobserved, derivationtrees.
The parse trees represent the desired outputof the system, while the derivation trees representthe typically much more complex underlying syntac-tic processes.
In recent years, latent variable meth-ods have been shown to produce grammars whichare as good as, or even better than, earlier parsingwork (Collins, 1999; Charniak, 2000).
In particular,in Petrov et al (2006) we exhibited a very accuratecategory-splitting approach, in which a coarse ini-tial grammar is refined by iteratively splitting eachgrammar category into two subcategories using theEM algorithm.
Of course, each time the number ofgrammar categories is doubled, the number of bi-nary productions is increased by a factor of eight.As a result, while our final grammars used few cat-egories, the number of total active (non-zero) pro-ductions was still substantial (see Section 7).
In ad-dition, it is reasonable to assume that some genera-tively learned splits have little discriminative utility.In this paper, we present a discriminative approachwhich addresses both of these limitations.We introduce multi-scale grammars, in whichsome productions reference fine categories, whileothers reference coarse categories (see Figure 2).We use the general framework of hidden variableCRFs (Lafferty et al, 2001; Koo and Collins, 2005),where gradient-based optimization maximizes thelikelihood of the observed variables, here parsetrees, summing over log-linearly scored derivations.With multi-scale grammars, it is natural to refineproductions rather than categories.
As a result, acategory such as NP can be complex in some re-gions of the grammar while remaining simpler inother regions.
Additionally, we exploit the flexibilityof the discriminative framework both to improve thetreatment of unknown words as well as to includespan features (Taskar et al, 2004), giving the bene-fit of some input features integrally in our dynamicprogram.
Our multi-scale grammars are 3 ordersof magnitude smaller than the fully-split baselinegrammar and 20 times smaller than the generativesplit-and-merge grammars of Petrov et al (2006).867In addition, we exhibit the best parsing numbers onseveral metrics, for several domains and languages.Discriminative parsing has been investigated be-fore, such as in Johnson (2001), Clark and Curran(2004), Henderson (2004), Koo and Collins (2005),Turian et al (2007), Finkel et al (2008), and, mostsimilarly, in Petrov and Klein (2008).
However, inall of these cases, the final parsing performance fellshort of the best generative models by several per-centage points or only short sentences were used.Only in combination with a generative model wasa discriminative component able to produce highparsing accuracies (Charniak and Johnson, 2005;Huang, 2008).
Multi-scale grammars, in contrast,give higher accuracies using smaller grammars thanprevious work in this direction, outperforming topgenerative models in grammar size and in parsingaccuracy.2 Latent Variable ParsingTreebanks are typically not annotated with fully de-tailed syntactic structure.
Rather, they present onlya coarse trace of the true underlying processes.
Asa result, learning a grammar for parsing requiresthe estimation of a more highly articulated modelthan the naive CFG embodied by such treebanks.A manual approach might take the category NP andsubdivide it into one subcategory NP?S for subjectsand another subcategory NP?VP for objects (John-son, 1998; Klein and Manning, 2003).
However,rather than devising linguistically motivated featuresor splits, latent variable parsing takes a fully auto-mated approach, in which each symbol is split intounconstrained subcategories.2.1 Latent Variable GrammarsLatent variable grammars augment the treebanktrees with latent variables at each node.
This cre-ates a set of (exponentially many) derivations oversplit categories for each of the original parse treesover unsplit categories.
For each observed categoryA we now have a set of latent subcategories Ax.
Forexample, NP might be split into NP1 through NP8.The parameters of the refined productionsAx ?
By Cz, where Ax is a subcategory of A, Byof B, and Cz of C , can then be estimated in var-ious ways; past work has included both generative(Matsuzaki et al, 2005; Liang et al, 2007) and dis-criminative approaches (Petrov and Klein, 2008).We take the discriminative log-linear approach here.Note that the comparison is only between estimationmethods, as Smith and Johnson (2007) show that themodel classes are the same.2.2 Log-Linear Latent Variable GrammarsIn a log-linear latent variable grammar, each pro-duction r = Ax ?
By Cz is associated with amultiplicative weight ?r (Johnson, 2001; Petrov andKlein, 2008) (sometimes we will use the log-weight?r when convenient).
The probability of a derivationt of a sentence w is proportional to the product of theweights of its productions r:P (t|w) ?
?r?t?rThe score of a parse T is then the sum of the scoresof its derivations:P (T |w) =?t?TP (t|w)3 Hierarchical RefinementGrammar refinement becomes challenging when thenumber of subcategories is large.
If each categoryis split into k subcategories, each (binary) produc-tion will be split into k3.
The resulting memory lim-itations alone can prevent the practical learning ofhighly split grammars (Matsuzaki et al, 2005).
Thisissue was partially addressed in Petrov et al (2006),where categories were repeatedly split and somesplits were re-merged if the gains were too small.However, while the grammars are indeed compactat the (sub-)category level, they are still dense at theproduction level, which we address here.As in Petrov et al (2006), we arrange our subcat-egories into a hierarchy, as shown in Figure 1.
Inpractice, the construction of the hierarchy is tightlycoupled to a split-based learning process (see Sec-tion 5).
We use the naming convention that an origi-nal category A becomes A0 and A1 in the first round;A0 then becoming A00 and A01 in the second round,and so on.
We will use x?
?
x to indicate that thesubscript or subcategory x is a refinement of x?.1 We1Conversely, x?
is a coarser version of x, or, in the languageof Petrov and Klein (2007), x?
is a projection of x.868+ 7 .
3+ 5 .
0+ 7 .
3+ 1 2+ 2 .
1S i n g l e  s c a l e p r o d u c t i o n s+ 5 .
0+ 5 .
0+ 7 .
3+ 2 .
1+ 2 .
1+ 2 .
1+ 2 .
1M u l t i  s c a l e p r o d u c t i o n s+ 2 .
1+ 1 2+ 5 .
0?0 010 1 0 0 1 1+ 5 .
0 + 5 .
0 + 7 .
3 + 1 2?0 0 0 1 0 0 1 0 1 1 1 0 1 1 10 0 1 0 1 0 0 1 1+ 2 .
1 + 2 .
1+ 2 .
1+ 2 .
1?r?
?r?0 00*11 00 1 1 10*0 1D T 0 0 0 ?
t h eD T 0 0 1 ?
t h eD T 0 1 0 ?
t h eD T 0 1 1 ?
t h eD T 1 0 0 ?
t h eD T 1 0 1 ?
t h eD T 1 1 0 ?
t h eD T 1 1 1 ?
t h eD T 0 0 ?
t h eD T 0 1 0 ?
t h eD T 0 1 1 ?
t h eD T 1 ?
t h e}}+ 1 2Figure 1: Multi-scale refinement of the DT ?
the production.
The multi-scale grammar can be encoded much morecompactly than the equally expressive single scale grammar by using only the shaded features along the fringe.will also say that x?
dominates x, and x will refer tofully refined subcategories.
The same terminologycan be applied to (binary) productions, which splitinto eight refinements each time the subcategoriesare split in two.The core observation leading to multi-scale gram-mars is that when we look at the refinements of aproduction, many are very similar in weight.
It istherefore advantageous to record productions only atthe level where they are distinct from their childrenin the hierarchy.4 Multi-Scale GrammarsA multi-scale grammar is a grammar in which someproductions reference fine categories, while othersreference coarse categories.
As an example, con-sider the multi-scale grammar in Figure 2, where theNP category has been split into two subcategories(NP0, NP1) to capture subject and object distinc-tions.
Since it can occur in subject and object po-sition, the production NP ?
it has remained unsplit.In contrast, in a single-scale grammar, two produc-tions NP0 ?
it and NP1 ?
it would have been nec-essary.
We use * as a wildcard, indicating that NP?can combine with any other NP, while NP1 can onlycombine with other NP1.
Whenever subcategoriesof different granularity are combined, the resultingconstituent takes the more specific label.In terms of its structure, a multi-scale grammar isa set of productions over varyingly refined symbols,where each production is associated with a weight.Consider the refinement of the production shown inFigure 1.
The original unsplit production (at top)would naively be split into a tree of many subpro-ductions (downward in the diagram) as the grammarcategories are incrementally split.
However, it maybe that many of the fully refined productions sharethe same weights.
This will be especially commonin the present work, where we go out of our way toachieve it (see Section 5).
For example, in Figure 1,the productions DTx ?
the have the same weightfor all categories DTx which refine DT1.2 A multi-scale grammar can capture this behavior with just 4productions, while the single-scale grammar has 8productions.
For binary productions the savings willof course be much higher.In terms of its semantics, a multi-scale grammar issimply a compact encoding of a fully refined latentvariable grammar, in which identically weighted re-finements of productions have been collapsed to thecoarsest possible scale.
Therefore, rather than at-tempting to control the degree to which categoriesare split, multi-scale grammars simply encode pro-ductions at varying scales.
It is hence natural tospeak of refining productions, while consideringthe categories to exist at all degrees of refinement.Multi-scale grammars enable the use of coarse (evenunsplit) categories in some regions of the grammar,while requiring very specific subcategories in others,as needed.
As we will see in the following, this flex-ibility results in a tremendous reduction of grammarparameters, as well as improved parsing time, be-cause the vast majority of productions end up onlypartially split.Since a multi-scale grammar has productionswhich can refer to different levels of the categoryhierarchy, there must be constraints on their coher-ence.
Specifically, for each fully refined produc-tion, exactly one of its dominating coarse produc-tions must be in the grammar.
More formally, themulti-scale grammar partitions the space of fully re-fined base rules such that each r maps to a unique2We define dominating productions and refining productionsanalogously as for subcategories.869i ts a wV P 0N P 1V *S *N P 0V 0 N P *h e rs a wV P 0N P 1V *S *N P 0V 0 N P 1V P 0N P 1V *S *V P *N P 0h e rN P 1sh eN P 0i tN P *s a wV 0Le x i c o n :G ra m m ar :V P * V P *sh eN P 0i tN P *Figure 2: In multi-scale grammars, the categories existat varying degrees of refinement.
The grammar in thisexample enforces the correct usage of she and her, whileallowing the use of it in both subject and object position.dominating rule r?, and for all base rules r?
such thatr?
?
r?, r?
maps to r?
as well.
This constraint is al-ways satisfied if the multi-scale grammar consists offringes of the production refinement hierarchies, in-dicated by the shading in Figure 1.A multi-scale grammar straightforwardly assignsscores to derivations in the corresponding fully re-fined single scale grammar: simply map each refinedderivation rule to its dominating abstraction in themulti-scale grammar and give it the correspondingweight.
The fully refined grammar is therefore triv-ially (though not compactly) reconstructable fromits multi-scale encoding.It is possible to directly define a derivational se-mantics for multi-scale grammars which does notappeal to the underlying single scale grammar.However, in the present work, we use our multi-scale grammars only to compute expectations of theunderlying grammars in an efficient, implicit way.5 Learning Sparse Multi-Scale GrammarsWe now consider how to discriminatively learnmulti-scale grammars by iterative splitting produc-tions.
There are two main concerns.
First, be-cause multi-scale grammars are most effective whenmany productions share the same weight, sparsityis very desirable.
In the present work, we exploitL1-regularization, though other techniques such asstructural zeros (Mohri and Roark, 2006) couldalso potentially be used.
Second, training requiresrepeated parsing, so we use coarse-to-fine chartcaching to greatly accelerate each iteration.5.1 Hierarchical TrainingWe learn discriminative multi-scale grammars in aniterative fashion (see Figure 1).
As in Petrov et al(2006), we start with a simple X-bar grammar froman input treebank.
The parameters ?
of the grammar(production log-weights for now) are estimated in alog-linear framework by maximizing the penalizedlog conditional likelihood Lcond ?R(?
), where:Lcond(?)
= log?iP(Ti|wi)R(?)
=?r|?r|We directly optimize this non-convex objectivefunction using a numerical gradient based method(LBFGS (Nocedal and Wright, 1999) in our imple-mentation).
To handle the non-diferentiability of theL1-regularization term R(?)
we use the orthant-wisemethod of Andrew and Gao (2007).
Fitting the log-linear model involves the following derivatives:?Lcond(?)??r=?i(E?
[fr(t)|Ti] ?
E?
[fr(t)|wi])where the first term is the expected count fr of a pro-duction r in derivations corresponding to the correctparse tree Ti and the second term is the expectedcount of the production in all derivations of the sen-tence wi.
Note that r may be of any scale.
As wewill show below, these expectations can be com-puted exactly using marginals from the chart of theinside/outside algorithm (Lari and Young, 1990).Once the base grammar has been estimated, allcategories are split in two, meaning that all binaryproductions are split in eight.
When splitting an al-ready refined grammar, we only split productionswhose log-weight in the previous grammar deviatesfrom zero.3 This creates a refinement hierarchy overproductions.
Each newly split production r is givena unique feature, as well as inheriting the features ofits parent productions r?
?
r:?r = exp(?r??r?r?
)The parent productions r?
are then removed from thegrammar and the new features are fit as described3L1-regularization drives more than 95% of the featureweights to zero in each round.870V PN PSN PD T N N V B D D T N NV Pi k jS 0 ?
N P 1 V P 0 1I(S0, i, j) I(S11, i, j)Figure 3: A multi-scale chart can be used to efficientlycompute inside/outside scores using productions of vary-ing specificity.above.
We detect that we have split a production toofar when all child production features are driven tozero under L1 regularization.
In such cases, the chil-dren are collapsed to their parent production, whichforms an entry in the multi-scale grammar.5.2 Efficient Multi-Scale InferenceIn order to compute the expected counts needed fortraining, we need to parse the training set, scoreall derivations and compute posteriors for all sub-categories in the refinement hierarchy.
The in-side/outside algorithm (Lari and Young, 1990) is anefficient dynamic program for summing over deriva-tions under a context-free grammar.
It is fairlystraightforward to adapt this algorithm to multi-scale grammars, allowing us to sum over an expo-nential number of derivations without explicitly re-constructing the underlying fully split grammar.For single-scale latent variable grammars, the in-side score I(Ax, i, j) of a fully refined category Axspanning ?i, j?
is computed by summing over allpossible productions r = Ax ?
By Cz with weight?r, spanning ?i, k?
and ?k, j?
respectively:4I(Ax, i, j) =?r?r?kI(By, i, k)I(Cz , k, j)Note that this involves summing over all relevantfully refined grammar productions.The key quantities we will need are marginals ofthe form I(Ax, i, j), the sum of the scores of all fullyrefined derivations rooted at any Ax dominated byAx and spanning ?i, j?.
We define these marginals4These scores lack any probabilistic interpretation, but canbe normalized to compute the necessary expectations for train-ing (Petrov and Klein, 2008).in terms of the standard inside scores of the mostrefined subcategories Ax:I(Ax, i, j) =?x?xI(Ax, i, j)When working with multi-scale grammars, weexpand the standard three-dimensional chart overspans and grammar categories to store the scores ofall subcategories of the refinement hierarchy, as il-lustrated in Figure 3.
This allows us to compute thescores more efficiently by summing only over rulesr?
= Ax?
?
By?
Cz?
?
r:I(Ax, i, j) =?r??r?r?
?r?kI(By, i, k)I(Cz , k, j)=?r??r??r?r?
?kI(By, i, k)I(Cz , k, j)=?r??r??y?y??z?z?
?kI(By, i, k)I(Cz , k, j)=?r??r?
?k?y?y?I(By, i, k)?z?z?I(Cz, k, j)=?r??r?
?kI(By?, i, k)I(Cz?
, k, j)Of course, some of the same quantities are computedrepeatedly in the above equation and can be cachedin order to obtain further efficiency gains.
Due tospace constraints we omit these details, and also thecomputation of the outside score, as well as the han-dling of unary productions.5.3 Feature Count ApproximationsEstimating discriminative grammars is challenging,as it requires repeatedly taking expectations over allparses of all sentences in the training set.
To makethis computation practical on large data sets, weuse the same approach as Petrov and Klein (2008).Therein, the idea of coarse-to-fine parsing (Charniaket al, 1998) is extended to handle the repeated pars-ing of the same sentences.
Rather than computingthe entire coarse-to-fine history in every round oftraining, the pruning history is cached between train-ing iterations, effectively avoiding the repeated cal-culation of similar quantities and allowing the effi-cient approximation of feature count expectations.8716 Additional FeaturesThe discriminative framework gives us a convenientway of incorporating additional, overlapping fea-tures.
We investigate two types of features: un-known word features (for predicting the part-of-speech tags of unknown or rare words) and span fea-tures (for determining constituent boundaries basedon individual words and the overall sentence shape).6.1 Unknown Word FeaturesBuilding a parser that can process arbitrary sen-tences requires the handling of previously unseenwords.
Typically, a classification of rare words intoword classes is used (Collins, 1999).
In such an ap-proach, the word classes need to be manually de-fined a priori, for example based on discriminatingword shape features (suffixes, prefixes, digits, etc.
).While this component of the parsing system israrely talked about, its importance should not be un-derestimated: when using only one unknown wordclass, final parsing performance drops several per-centage points.
Some unknown word features areuniversal (e.g.
digits, dashes), but most of themwill be highly language dependent (prefixes, suf-fixes), making additional human expertise necessaryfor training a parser on a new language.
It is there-fore beneficial to automatically learn what the dis-criminating word shape features for a language are.The discriminative framework allows us to do thatwith ease.
In our experiments we extract prefixesand suffixes of length ?
3 and add those features towords that occur 25 times or less in the training set.These unknown word features make the latent vari-able grammar learning process more language inde-pendent than in previous work.6.2 Span FeaturesThere are many features beyond local tree config-urations which can enhance parsing discrimination;Charniak and Johnson (2005) presents a varied list.In reranking, one can incorporate any such features,of course, but even in our dynamic programming ap-proach it is possible to include features that decom-pose along the dynamic program structure, as shownby Taskar et al (2004).
We use non-local span fea-tures, which condition on properties of input spans(Taskar et al, 2004).
We illustrate our span featureswith the following example and the span ?1, 4?
:0 ?
1 [ Yes 2 ?
3 , ] 4 he 5 said 6 .
7We first added the following lexical features:?
the first (Yes), last (comma), preceding (?)
andfollowing (he) words,?
the word pairs at the left edge ?
?,Yes?, rightedge ?comma,he?, inside border ?Yes,comma?and outside border ?
?,he?.Lexical features were added for each span of lengththree or more.
We used two groups of span features,one for natural constituents and one for syntheticones.5 We found this approach to work slightlybetter than anchoring the span features to particularconstituent labels or having only one group.We also added shape features, projecting thesentence to abstract shapes to capture global sen-tence structures.
Punctuation shape replaces ev-ery non-punctuation word with x and then furthercollapses strings of x to x+.
Our example be-comes #??x?
?,x+.#, and the punctuation featurefor our span is ??[x??,]x.
Capitalization shapeprojects the example sentence to #.X..xx.#, and.[X..
]x for our span.
Span features are a richsource of information and our experiments shouldbe seen merely as an initial investigation of their ef-fect in our system.7 ExperimentsWe ran experiments on a variety of languages andcorpora using the standard training and test splits,as described in Table 1.
In each case, we startwith a completely unannotated X-bar grammar, ob-tained from the raw treebank by a simple right-branching binarization scheme.
We then train multi-scale grammars of increasing latent complexity asdescribed in Section 5, directly incorporating theadditional features from Section 6 into the trainingprocedure.
Hierarchical training starting from a rawtreebank grammar and proceeding to our most re-fined grammars took three days in a parallel im-plementation using 8 CPUs.
At testing time wemarginalize out the hidden structure and extract thetree with the highest number of expected correct pro-ductions, as in Petrov and Klein (2007).5Synthetic constituents are nodes that are introduced duringbinarization.872Training Set Dev.
Set Test SetENGLISH-WSJ Sections Section 22 Section 23(Marcus et al, 1993) 2-21ENGLISH-BROWN see 10% of 10% of the(Francis et al 2002) ENGLISH-WSJ the data6 the data6FRENCH7 Sentences Sentences Sentences(Abeille et al, 2000) 1-18,609 18,610-19,609 19,609-20,610GERMAN Sentences Sentences Sentences(Skut et al, 1997) 1-18,602 18,603-19,602 19,603-20,602Table 1: Corpora and standard experimental setups.We compare to a baseline of discriminativelytrained latent variable grammars (Petrov and Klein,2008).
We also compare our discriminative multi-scale grammars to their generative split-and-mergecousins, which have been shown to produce thestate-of-the-art figures in terms of accuracy and effi-ciency on many corpora.
For those comparisons weuse the grammars from Petrov and Klein (2007).7.1 SparsityOne of the main motivations behind multi-scalegrammars was to create compact grammars.
Fig-ure 4 shows parsing accuracies vs. grammar sizes.Focusing on the grammar size for now, we see thatmulti-scale grammars are extremely compact - evenour most refined grammars have less than 50,000 ac-tive productions.
This is 20 times smaller than thegenerative split-and-merge grammars, which use ex-plicit category merging.
The graph also shows thatthis compactness is due to controlling productionsparsity, as the single-scale discriminative grammarsare two orders of magnitude larger.7.2 AccuracyFigure 4 shows development set results for En-glish.
In terms of parsing accuracy, multi-scalegrammars significantly outperform discriminativelytrained single-scale latent variable grammars andperform on par with the generative split-and-mergegrammars.
The graph also shows that the unknownword and span features each add about 0.5% in finalparsing accuracy.
Note that the span features im-prove the performance of the unsplit baseline gram-mar by 8%, but not surprisingly their contribution6See Gildea (2001) for the exact setup.7This setup contains only sentences without annotation er-rors, as in (Arun and Keller, 2005).90858075100000010000010000Parsingaccuracy(F1)Number of grammar productionsDiscriminative Multi-Scale Grammars+ Lexical Features+ Span FeaturesGenerative Split-Merge GrammarsFlat Discriminative GrammarsFigure 4: Discriminative multi-scale grammars give sim-ilar parsing accuracies as generative split-merge gram-mars, while using an order of magnitude fewer rules.gets smaller when the grammars get more refined.Section 8 contains an analysis of some of the learnedfeatures, as well as a comparison between discrimi-natively and generatively trained grammars.7.3 EfficiencyPetrov and Klein (2007) demonstrates how the ideaof coarse-to-fine parsing (Charniak et al, 1998;Charniak et al, 2006) can be used in the context oflatent variable models.
In coarse-to-fine parsing thesentence is rapidly pre-parsed with increasingly re-fined grammars, pruning away unlikely chart itemsin each pass.
In their work the grammar is pro-jected onto coarser versions, which are then usedfor pruning.
Multi-scale grammars, in contrast, donot require projections.
The refinement hierarchy isbuilt in and can be used directly for coarse-to-finepruning.
Each production in the grammar is associ-ated with a set of hierarchical features.
To obtain acoarser version of a multi-scale grammar, one there-fore simply limits which features in the refinementhierarchy can be accessed.
In our experiments, westart by parsing with our coarsest grammar and al-low an additional level of refinement at each stage ofthe pre-parsing.
Compared to the generative parserof Petrov and Klein (2007), parsing with multi-scalegrammars requires the evaluation of 29% fewer pro-ductions, decreasing the average parsing time persentence by 36% to 0.36 sec/sentence.873?
40 words allParser F1 EX F1 EXENGLISH-WSJPetrov and Klein (2008) 88.8 35.7 88.3 33.1Charniak et al (2005) 90.3 39.6 89.7 37.2Petrov and Klein (2007) 90.6 39.1 90.1 37.1This work w/o span features 89.7 39.6 89.2 37.2This work w/ span features 90.0 40.1 89.4 37.7ENGLISH-WSJ (reranked)Huang (2008) 92.3 46.2 91.7 43.5ENGLISH-BROWNCharniak et al (2005) 84.5 34.8 82.9 31.7Petrov and Klein (2007) 84.9 34.5 83.7 31.2This work w/o span features 85.3 35.6 84.3 32.1This work w/ span features 85.6 35.8 84.5 32.3ENGLISH-BROWN (reranked)Charniak et al (2005) 86.8 39.9 85.2 37.8FRENCHArun and Keller (2005) 79.2 21.2 75.6 16.4This Paper 80.1 24.2 77.2 19.2GERMANPetrov and Klein (2007) 80.8 40.8 80.1 39.1This Paper 81.5 45.2 80.7 43.9Table 2: Our final test set parsing accuracies compared tothe best previous work on English, French and German.7.4 Final ResultsFor each corpus we selected the grammar that gavethe best performance on the development set to parsethe final test set.
Table 2 summarizes our final testset performance, showing that multi-scale grammarsachieve state-of-the-art performance on most tasks.On WSJ-English, the discriminative grammars per-form on par with the generative grammars of Petrovet al (2006), falling slightly short in terms of F1, buthaving a higher exact match score.
When trainedon WSJ-English but tested on the Brown corpus,the discriminative grammars clearly outperform thegenerative grammars, suggesting that the highly reg-ularized and extremely compact multi-scale gram-mars are less prone to overfitting.
All those meth-ods fall short of reranking parsers like Charniak andJohnson (2005) and Huang (2008), which, however,have access to many additional features, that cannotbe used in our dynamic program.When trained on the French and German tree-banks, our multi-scale grammars achieve the bestfigures we are aware of, without any language spe-cific modifications.
This confirms that latent vari-able models are well suited for capturing the syn-tactic properties of a range of languages, and alsoshows that discriminative grammars are still effec-tive when trained on smaller corpora.8 AnalysisIt can be illuminating to see the subcategories thatare being learned by our discriminative multi-scalegrammars and to compare them to generatively es-timated latent variable grammars.
Compared to thegenerative case, the lexical categories in the discrim-inative grammars are substantially less refined.
Forexample, in the generative case, the nominal cate-gories were fully refined, while in the discrimina-tive case, fewer nominal clusters were heavily used.One reason for this can be seen by inspecting thefirst two-way split in the NNP tag.
The genera-tive model split into initial NNPs (San, Wall) andfinal NNPs (Francisco, Street).
In contrast, the dis-criminative split was between organizational entities(Stock, Exchange) and other entity types (September,New, York).
This constrast is unsurprising.
Genera-tive likelihood is advantaged by explaining lexicalchoice ?
New and York occur in very different slots.However, they convey the same information aboutthe syntactic context above their base NP and aretherefore treated the same, discriminatively, whilethe systematic attachment distinctions between tem-porals and named entities are more predictive.Analyzing the syntactic and semantic patternslearned by the grammars shows similar trends.
InTable 3 we compare the number of subcategoriesin the generative split-and-merge grammars to theaverage number of features per unsplit productionwith that phrasal category as head in our multi-scalegrammars after 5 split (and merge) rounds.
Thesequantities are inherently different: the number offeatures should be roughly cubic in the number ofsubcategories.
However, we observe that the num-bers are very close, indicating that, due to the spar-sity of our productions, and the efficient multi-scaleencoding, the number of grammar parameters growslinearly in the number of subcategories.
Further-more, while most categories have similar complex-ity in those two cases, the complexity of the twomost refined phrasal categories are flipped.
Gener-ative grammars split NPs most highly, discrimina-874NPVPPP S SBARADJPADVPQP PRNGenerative 32 24 20 12 12 12 8 7 5subcategoriesDiscriminative 19 32 20 14 14 8 7 9 6production parametersTable 3: Complexity of highly split phrasal categories ingenerative and discriminative grammars.
Note that sub-categories are compared to production parameters, indi-cating that the number of parameters grows cubicly in thenumber of subcategories for generative grammars, whilegrowing linearly for multi-scale grammars.tive grammars split the VP.
This distinction seemsto be because the complexity of VPs is more syntac-tic (e.g.
complex subcategorization), while that ofNPs is more lexical (noun choice is generally higherentropy than verb choice).It is also interesting to examine the automaticallylearned word class features.
Table 4 shows the suf-fixes with the highest weight for a few different cat-egories across the three languages that we experi-mented with.
The learning algorithm has selecteddiscriminative suffixes that are typical derviationalor inflectional morphemes in their respective lan-guages.
Note that the highest weighted suffixes willtypically not correspond to the most common suffixin the word class, but to the most discriminative.Finally, the span features also exhibit clear pat-terns.
The highest scoring span features encouragethe words between the last two punctuation marksto form a constituent (excluding the punctuationmarks), for example ,[x+].
and :[x+].
Wordsbetween quotation marks are also encouraged toform constituents: ??[x+]??
and x[??x+??
]x.Span features can also discourage grouping wordsinto constituents.
The features with the highest neg-ative weight involve single commas: x[x,x+],and x[x+,x+]x and so on (indeed, such spanswere structurally disallowed by the Collins (1999)parser).9 ConclusionsDiscriminatively trained multi-scale grammars givestate-of-the-art parsing performance on a variety oflanguages and corpora.
Grammar size is dramati-cally reduced compared to the baseline, as well as toENGLISH GERMAN FRENCHAdjectives-ous -los -ien-ble -bar -ble-nth -ig -iveNouns-ion -ta?t -te?-en -ung -eur-cle -rei -gesVerbs -ed -st -e?es-s -eht -e?Adverbs -ly -mal -entNumbers -ty -zig ?Table 4: Automatically learned suffixes with the highestweights for different languages and part-of-speech tags.methods like split-and-merge (Petrov et al, 2006).Because fewer parameters are estimated, multi-scalegrammars may also be less prone to overfitting, assuggested by a cross-corpus evaluation experiment.Furthermore, the discriminative framework enablesthe seamless integration of additional, overlappingfeatures, such as span features and unknown wordfeatures.
Such features further improve parsing per-formance and make the latent variable grammarsvery language independent.Our parser, along with trained grammarsfor a variety of languages, is available athttp://nlp.cs.berkeley.edu.ReferencesA.
Abeille, L. Clement, and A. Kinyon.
2000.
Building atreebank for French.
In 2nd International Conferenceon Language Resources and Evaluation.G.
Andrew and J. Gao.
2007.
Scalable training of L1-regularized log-linear models.
In ICML ?07.A.
Arun and F. Keller.
2005.
Lexicalization in crosslin-guistic probabilistic parsing: the case of french.
InACL ?05.E.
Charniak and M. Johnson.
2005.
Coarse-to-Fine N-Best Parsing and MaxEnt Discriminative Reranking.In ACL?05.E.
Charniak, S. Goldwater, and M. Johnson.
1998.
Edge-based best-first chart parsing.
6th Workshop on VeryLarge Corpora.E.
Charniak, M. Johnson, D. McClosky, et al 2006.Multi-level coarse-to-fine PCFG Parsing.
In HLT-NAACL ?06.E.
Charniak.
2000.
A maximum?entropy?inspiredparser.
In NAACL ?00.S.
Clark and J. R. Curran.
2004.
Parsing the WSJ usingCCG and log-linear models.
In ACL ?04.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, UPenn.875J.
Finkel, A. Kleeman, and C. Manning.
2008.
Effi-cient, feature-based, conditional random field parsing.In ACL ?08.W.
N. Francis and H. Kucera.
2002.
Manual of infor-mation to accompany a standard corpus of present-dayedited american english.
In TR, Brown University.D.
Gildea.
2001.
Corpus variation and parser perfor-mance.
EMNLP ?01.J.
Henderson.
2004.
Discriminative training of a neuralnetwork statistical parser.
In ACL ?04.L.
Huang.
2008.
Forest reranking: Discriminative pars-ing with non-local features.
In ACL ?08.M.
Johnson.
1998.
PCFG models of linguistic tree rep-resentations.
Computational Linguistics, 24:613?632.M.
Johnson.
2001.
Joint and conditional estimation oftagging and parsing models.
In ACL ?01.D.
Klein and C. Manning.
2003.
Accurate unlexicalizedparsing.
In ACL ?03, pages 423?430.T.
Koo and M. Collins.
2005.
Hidden-variable modelsfor discriminative reranking.
In EMNLP ?05.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional Random Fields: Probabilistic models for seg-menting and labeling sequence data.
In ICML ?01.K.
Lari and S. Young.
1990.
The estimation of stochas-tic context-free grammars using the inside-outside al-gorithm.
Computer Speech and Language.P.
Liang, S. Petrov, M. I. Jordan, and D. Klein.
2007.
Theinfinite PCFG using hierarchical Dirichlet processes.In EMNLP ?07.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: ThePenn Treebank.
In Computational Linguistics.T.
Matsuzaki, Y. Miyao, and J. Tsujii.
2005.
Probabilis-tic CFG with latent annotations.
In ACL ?05.M.
Mohri and B. Roark.
2006.
Probabilistic context-freegrammar induction based on structural zeros.
In HLT-NAACL ?06.J.
Nocedal and S. J. Wright.
1999.
Numerical Optimiza-tion.
Springer.S.
Petrov and D. Klein.
2007.
Improved inference forunlexicalized parsing.
In HLT-NAACL ?07.S.
Petrov and D. Klein.
2008.
Discriminative log-lineargrammars with latent variables.
In NIPS ?08.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable tree an-notation.
In ACL ?06.W.
Skut, B. Krenn, T. Brants, and H. Uszkoreit.
1997.An annotation scheme for free word order languages.In Conf.
on Applied Natural Language Processing.N.
A. Smith and M. Johnson.
2007.
Weighted and prob-abilistic context-free grammars are equally expressive.Computational Lingusitics.B.
Taskar, D. Klein, M. Collins, D. Koller, and C. Man-ning.
2004.
Max-margin parsing.
In EMNLP ?04.J.
Turian, B. Wellington, and I. D. Melamed.
2007.
Scal-able discriminative learning for natural language pars-ing and translation.
In NIPS ?07.876
