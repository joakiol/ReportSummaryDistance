Finding Representations for Memory-Based LanguageAbstractConstructive induction transforms the representation finstances in order to produce a more accurate model ofthe concept o be learned.
For this purpose, a vari-ety of operators has been proposed in the literature,including a Cartesian product operator forming pair-wise higher-order attributes.
We study the effect ofthe Cartesian product operator on memory-based lan-guage learning, and demonstrate its effect on general-ization accuracy and data compression for a number oflinguistic classification tasks, using k-nearest neighborlearning algorithms.
These results are compared to abaseline approach of backward sequential eliminationof attributes.
It is demonstrated that neither approachconsistently outperforms the other, and that attributeelimination can be used to derive compact representa-tions for memory-based language learning without no-ticeable loss of generalization accuracy.IntroductionLearningStephan Raaijmakersraa i jmakers@in l ,  n lInst i tute for Dutch Lexicology (INL)P.O.
Box 95152300 RA LeidenThe Nether landsDaelemans et al, 1997\].
Successful attribute elimina-tion leads to compact datasets, which possibly increaseclassification speed.
Constructive induction, on theother hand, tries to exploit dependencies between at-tributes, by combining them into complex attributesthat increase accuracy of the classifier.
For instance-based algorithms, this approach as been demonstratedto correct invalid independence assumptions made bythe algorithm \[Pazzani, 1998\]: e.g., for the Naive Bayesclassifier (Duda & Hart, 1973), the unwarranted as-sumption that in general the various attributes a, = v,are independent, and form a joint probability model forthe prediction of the class C:It is a widely held proposition that inductive learn-ing models, such as decision trees \[Quinlan, 1993\] or k-nearest neighbor models \[Aha, Kibler & Albert, 1991\],are heavily dependent upon their representational bi-ases.
Both decision tree algorithms and instance-basedalgorithms have been reported to be vulnerable to ir-relevant or noisy attributes in the representation f ex-emplars, which unnecessarily enlarge the search spacefor classification \[John, 1997\].
In general, there aretwo options for dealing with this problem.
Attributeelimination (or selection) can be applied in order tofind a minimal set of attributes that is maximally in-formative for the concept to be learned.
Attributeelimination can be seen as a radical case of attributeweighting \[Scherf & Brauer, 1997, Aha, 1998\], whereattributes are weighted on a binary scale, as either rel-evant or not; more fine-grained methods of attributeweighting take information-theoretic notions ifito ac-count such as information gain ratio \[Quinlan, 1993.l-I P(a, = v, I C) P(C)tP(C  I a~ = v~A.
.
.Aa ,  = v,) = P(a~ = v~ A... ^ a,, = v,)(I)Constructive induction thus can be used to invent re-lationships between attributes that, apart fl'om possi-bly offering insight into the underlying structure of thelearning task, may boost performance of the resultingclassifier.
Linguistic tasks are sequential by nature, aslanguage processing is a linear process, operating onsequences with a temporal structure (see e.g.
Cleere-mans (1993) for motivation for the temporal structureof finite-state grammar learning).
Learning algorithmslike k-nearest neighbor or decision trees abstract awayfrom this linearity, by treating representations as multi-sets of attribute-value pairs, i.e.
permutation-invariantlists.
Using these algorithms, constructive inductioncannot be used for corrections on the linearity of thelearning task, but it can be used to study attribute in-teraction irrespective of ordering issues.In this paper, the use of constructive induction iscontrasted with attribute elimination for a set of lin-guistic learning tasks.
The linguistic learning domainappears to be deviant from other symbolic domains inbeing highly susceptible to editing.
It has been no-ticed \[Daelemans et al, 1999i\] that editing exceptional24instances from linguistic instance bases tends to harmgeneralization accuracy.
In this study, we apply edit-ing on the level of instance representation.
The centralquestion is whether it is more preferable to correct lin-guistic tasks by combining (possibly noisy or irrelevant)attributes, or by finding informative subsets.Representation TransformationsJohn (1997) contains presentations of various attributeselection approaches.
In Yang & Honovar (1998), agenetic algorithm is used for finding informative at-tribute subsets, in a neural network setting.
Cardie(1996) presents an attribute selection approach to nat-ural language processing (relative pronoun disambigua-tion) incorporating a small set of linguistic biases (tobe determined by experts).Many operators have been proposed in the litera-ture for forming new attributes from existing ones.
Pa-gallo & Hauser (1990) propose boolean operators (likeconjunction and negation) for forming new attributesin a decision tree setting.
Aha (1991) describes IB3-CI, a constructive indiction algorithm for the instance-based classifier IB3.
Aiming at reducing similarity be-tween an exemplar and its misclassifying earest neigh-bor, IB3-CI uses a conjunctive operator forming an at-tribute that discriminates between these two.
Bloedorn& Michalski (1991) present a wide variety of mathe-matical and logical operators within the context of theAQ17-DC1 system.
A general perspective on construc-tive induction is sketched in Bloedorn, Michalski &Wnek (1994).
Keogh & Pazzani (1999) propose correla-tion arcs between attributes, augmenting Naive Bayeswith a graph structure.Pazzani (1998) proposes a Cartesian product oper-ator for joining attributes, and compares its effects ongeneralization accuracy with those of attribute limina-tion, for (a.o.)
the Naive Bayes and PEBLS (Cost &Salzberg, 1993) classifiers.
The Cartesian product oper-ator joins two attributes At and A2 into a new, complexattribute At..42, taking values in the Cartesian product{< a,,aj >1 a ie  Values(At) ^  aj E Values(A.,_)} (2)where Values(A) is the value set of attribute A. TheCartesian product operator has an intrinsic linear in-terpretation: two features joined in a Cartesian prod-uct form an ordered pair with a precedence r lation(the ordered pair < a, b > differs from the ordered pair< b, a >).
This linear interpretation vanishes in learn-ing algorithms that do not discern internal structure inattribute values (like standard nearest neighbor).Pazzani's backward sequential elimination and join-b~g algorithm (BSEJ) finds the optimal representationtransformation by considering each pair of attributesin turn, using leave-one-out cross-validation to deter-mine the effect on generalization accuracy.
Attributejoining carries out an implicit but inevitable limina-tion step: wiping out an attribute being subsumed bya combination.
This reduces the dimensionality of theresult dataset with one dimension.
Following success-ful joining, the BSEJ algorithm carries out an explicitelimination step, attempting todelete very attribute inturn (including the newly constructed attribute) look-ing for the optimal candidate using cross-validation.The algorithm converges when no more transforma-tions can be found that increase generalization accu-racy.
This approach is reported to produce significantaccuracy gain for Naive Bayes and for PEBLS.
Pazzanicontrasts BSEJ with a backward sequential ehminationalgorithm (BSE, backward sequential elimination, pro-gressively eliminating attributes (and thus reducing di-mensionality) until accuracy degrades.
He also investi-gates forward variants of these algorithms, which suc-cessively build more complex representations up to con-vergence.
Both for PEBLS and Naive Bayes, attributejoining appears to be superior to elimination, and thebackward algorithms perform better than the forwardalgorithms.
For k-nearest neighbor algorithms basedon the unweighted overlap metric, BSEJ did not out-perform BSE.Conditioning representation transformations oil theperformance of the original classifier implements awrapper approach (John, 1997; Kohavi & John, 1998),which has proven an accurate, powerful method to mea-sure the effects of data transformations on generaliza-tion accuracy.
The transformation process is wrappedaround the classifier, and no transformation is carriedout that degrades generalization accuracy.In this study, two algorithms, an implementation fBSE and a simplification of the BSEJ algorithm, werewrapped around three types of classifiers: IBI-IG, IB1-IG&MVDM (a classifier related to PEBLS in usingMVDM) and IGTREE \[Daelemans et al, 1997\].
All ofthese classifiers are implemented in the TiMBL package\[Daelemans et al 1999ii\].
IBI-IG is a k-nearest neigh-bor algorithm using a weighted overlap metric, wherethe attributes of instances have their information gainratio as weight.
For instances X and l', distance iscomputed asnACX, Y) = y~ wdiCxi.
Yi) (3)t= lwhere 6 is the overlap metric, and w, is the informationgain ratio (Quinlan, 1993) of attribute i.The PEBLS algorithm can be approximated to a cer-tain extent by combining IBI-IG with the ModifiedValue Difference Metric (MVDM) of Cost & Salzberg25(1993).
The MVDM defines the difference between twovalues x and y respective to a class C, as5(x,y) = Z \[ P(C, ix) - P(C, \[y) \[ (4)z----Ii.e., it uses the probabilities of the various classesconditioned on the two values to determine overlap.Attribute weighting of IBI-IG&MVDM (informationgain ratio based) differs from PEBLS: PEBLS usesperformance-based weighting based on class predicyionstrength, where exemplars are weighted according to anaccuracy or reliability ratio.IGTREE is a tree-based k-nearest neighbor algo-rithm, where information gain is used as a heuristic toinsert nodes in the tree.
For every non-terminal node,a default classification is stored for the path leading toit.
Whenever no exact match can be found for an un-known instance to be classified, the default classificationassociated with the last matching attribute is returnedas classification for the instance.
Although IGTREEsometimes lags behind IBI-IG in accuracy, it providesfor much faster, high quality classifiers.An implementation f the BSE algorithm is outlinedin figure .
It is akin in spirit to the backward elimi-nation algorithm of John (1997).
During every pass, itmeasures the effects on generalization accuracy of elimi-nating every attribute in turn, only carrying out the onewhich maximizes accuracy.
A simplified version of theBSEJ algorithm called backward sequential joining withinformation gain ratio (BSJ-IG) is outlined in figure.N!
It checks the ~ ordered combinations for N fea-tures during each pass, and carries out the one resultingin the maximum gain in accuracy (as a consequence ofthe permutation im, ariance, the total search space ofN!
possible combinations can be halved).
Any twojoined attributes are put on the position with the max-imum information gain ratio of both original positions,after which the remaining candidate position is wipedout.
Again, as the used classifiers are all permutation-invariant with respect o their representations, this isonly a decision procedure to find a target position forthe attribute combination; all candidate positions areequivalent target positions.Unlike the original BSEJ algorithm, BSJ-IG omitsthe additional explicit attribute limination step di-rectly after every attribute joining step, in order to seg-regate the effects of attribute joining as much as possi-ble from those of attribute limination.Both BSE and BSJ-IG algorithms are hill-climbingalgorithms, and, as such, are vulnerable to local lnin-ima.
Ties are resolved randomly by both.26Exper imentsThe effects of forming Cartesian product attributeson generalization accuracy and reduction of dimen-sionality (compression) were compared with those ofbackward sequential elimination of attributes.
Thefollowing 7 linguistic datasets were used.
STRESS isa selection of secondary stress assignment patternsfrom the Dutch version of the Celex lexical database\[Baayen, Piepenbrock & van Rijn, 1993\], on the basisof phonemic representations of syllabified words.
At-tribute values are phonemes.
Also derived from Celexis the DIMIN task, a selection of diminutive formationpatterns for Dutch.
This task consists of assigningDutch diminutive suffixes'to a noun, based on phoneticproperties of (maximally) the last three syllables of thenoun.
Attribute values are phoneme representationsas well as stress markers for the syllables.
The WSJ-NPVP set consists of part-of speech tagged Wall StreetJournal material (Marcus, Santorini & Marcinkiewicz,1993), supplemented with syntactic tags indicatingnoun phrase and verb phrase boundaries (Daelemans etal, 1999iii).
wsJ-POS is a fragment of the Wall StreetJournal part-of-speech tagged material (Marcus, San-torini and Marcinkiewicz, 1993).
Attributes values areparts of speech, which are assigned using a window-ing approach, with a window size of 5.
INL-POS is apart-of-speech tagging task for Dutch, using tl~e Dutch-Tale tagset \[van der Voort van der Kleij et al, 1994\],attribute values are parts of speech.
Using a window-ing approach, on the basis of a 7-cell window, partof speech tags are disambiguated.
GRAPHON consti-tutes a grapheme-to-phoneme learning task for English,based on the Celex lexical database.
Attribute valuesare graphemes ( ingle characters), to be classified asphonemes.
PP-ATTACH, finally, is a prepositional phrase(PP) attachment task for English.. where PP's are at-tached to either noun or verb projections, based onlexical context.
Attribute values are word forms forverb, the head noun of the following nouu phrase, thepreposition of the following PP, and the head noun ofthe PP-internal noun phrase (like bring a t tent ion  toproblem).
The material has been extracted by Rat-naparkhi et al (1994) from the Penn Treebank WallStreet Journal corpus.
Key numerical characteristics ofthe datasets are summarized in table 1.Each of these datasets was subjected to the BSJ-IGand the BSE wrapper algorithms, embedding either theIBI-IG or IGTREE architecture.
Both the Naive Bayesand PEBLS classifier investigated by Pazzani (1998) al-low for certain frequency tendencies hidden in the datato bear on the classification.
This has a smoothing ef-fect on the handling of low-frequency events, which ben-efit from analogies with more reliable higher-frequency!Procedure BSEInput: a training set TOutput: a new training set T' with possibly attributes removedSet  Acc to Accuracy(T) for  the current c lass i f ie rSet  Success to trueWhile (Success) doSetSuccess  to falseFor every attribute A in T doProduce T' by removing A from every instance in TNewAcc=Accuracy(T') for the current classifierIf (NewAcc>Acc)ThenSet Acc to NewAccSet Winner to T'Set Success to trueIf Success equals trueThenSet T to WinnerReturn TFigure h A wrapper implementation f Backward Sequential Elimination (BSE).events.
In order to assess the effects of smoothing, thefollowing additional experiments were carried out.
Em-beddded into BSE and BSJ-IG, the PEBLS approxima-tion IBI-IG with MVDM was applied to three datasets:STRESS, DIMIN and PP-ATTACH, for three values of k (1,3, 7), the size of the nearest neighbor set.
Values for klarger than 1, i.e.
non-singleton nearest neighbor sets.have been found to reproduce some of the smoothing in-herent o statistical back-off models (Daelemans et al.1999ii; Zavrel & Daelemans, 1997).Generalization accuracy for every attribute joiningor elimination step was measured using 10-fold cross-validation, and significance was measured using a two-tailed paired t-test at the .05 level.
All experimentswere carried out on a Digital Alpha XL-266 (Linux) anda Sun UltraSPARC-IIi (Solaris).
Due to slow perfor-mance of the IBI-IG model on certain datasets with theused equipment, IBI-IG experiments with %VSJ-NPVPcould not be completed.Resu l t sThe results show, first of all, that the compressionrates obtained with BSE (average 34.9%) were consis-tently higher than those obtained with BSJ-IG (average28.6%) (table 2).Secondly, BSE and BSJ-IG have com~)arable effectson accuracy.
BSE generally boosts IGTREE perfor-mance to IBI-IG level, and leads to significant accu-racy gains for two datasets, STRESS and PP-A.TTACH(tabel 3).
BSJ-IG does so for the STRESS set (tabel4).
Neither BSE nor BSJ-IG produce any significantgain in accuracy for the IBI-IG classifier.
This general-izes the findings of Pazzani (1998) ibr classifiers basedon unweighted overlap metrics to classifiers based oll aweighted overlap metric.For the classifier IBI-IG&MVDM.
the situation ismore complex (table 5).
First, for k = 1.
BSE and BSJ-IG have comparable accuracy.
For the STRESS and PP-ATTACH sets, both algorithms produce significant andcomparable accuracy gains.
Second, compression byBSE is significantly higher than compression oy BSJ-IG (47.2% vs. 30.6%).For the larger values for k (3, 7), BSJ-IG producessignificant higher accuracies on the STRESS set, outper-forming BSE.
Moreover, BSJ-IG yields a compressionrate comparable to BSE.
BSE compression drops from47.2% to 27.8%.A detailed look at the representations produced byBSE and BSJ-IG reveals the following.27Procedure BSJ-IGInput: a training set TOutput: a new training set T' with possibly higher-order induced attributesSet Acc to  Accuracy(T)  fo r  the  cur rent  c lass i f ie rSet Success to  t rueWhile (Success)  doSet Success to  fa l seFor every  o rdered  combinat ion of two a t t r ibutes  At and Aj in  T doProduce T '  from T by jo in ing  Ai and A~, put t ing  them on the pos i t ionk 6 {i, j} with the la rgest  in fo rmat ion  ga in  ra t io .NewAcc=Accuracy(T') fo r  the  cur rent  c lass i f ie rIf (NewAcc~Acc)ThenSet Acc to NewAccSet Winner to T'Set Success to trueIf Success equals trueThenSet T to WinnerReturn TFigure 2: A wrapper implementation of Backward Sequential Joining with Information Gain ratio (BSJ-IG)* (BSJ-IG) IB I - IG&BSJ - IG and IGTREE&BSJ- IGonly agree on wsJ-POS: they both join the same at-tributes.
For the other datasets, there is no overlapat all.?
(BSE) For the wsJ-POS set, BSE deletes exactly thesame two features that are joined by BSJ-IG for IB1-IG and IGTREE.
For the DIMIN set, IB I - IG&BSEand IGTREE&BSE delete 4 common features.
ForSTRESS, all features deleted by IBI - IG&BSE aredeleted by IGTREE&BSE as well.
On the INL-POSset, three common features are deleted.
Frequently,BSE was found to delete an attribute joined by BSJ-IG.?
( IBI - IG&MVDM, BSJ-IG) BSJ-IG produces no over-lap for D1MIN for the three different classifiers (k =1,3,7).
For STRESS, the k = 1, k = 3 and k = 7classifiers join one common pair of attributes.
Thisis the pair consisting of the nucleus and coda of thelast syllable, indeed a strong feature for stress assign-ment (Daelemans, p.c.).
For PP-ATTACH, the k = 1,k - 3 and k = 7 classifiers identify attribute 4 (thehead noun of the PP-internal noun phrase) for .join-ing with another attribute.
Attribute 4 clearly intro-duces sparseness in the dataset: it has 5~695 possiblevalues, opposed to maximally 4,405 values for theother attributes.
The k = 3 and k = 7 classifiersagree fully here.?
( IBI- IG&MVDM, BSE) On the DIMIN set, the k = 1and k = 3 classifiers differ in 1 attribute eliminationonly.
They display no overlap with k = 7, which elim-inates entirely other attributes.
For STRESS, k = 1and k = 3 classifiers overlap on 3 attributes.
Thethree classifiers delete 1. common attribute (not thenucleus or coda).
For PP, the k = 3 and k = 7 clas-sifters do not eliminate attributes; the k = 1 classi-fier deletes the attribute 4 (PP-internal head noun),and even the first verb-valued attribute.
In doingso, it constitutes a strongly lexicalised model for PP-attachment taking only into account the first headnoun and the following preposition.BSE produced more overlapping results across classi-tiers than BSJ-IG.
IB I - IG&MVDM with BSJ- IG is theonly type of classifier that is able to trap the importantinteraction between ucleus and coda in the STRESS set.Due to lack of domain knowledge, we cannot be cer-tain that other important interactions have ,lot been28Dataset Instances Attributes IBI-IG IGTREESTRESSDIMINWSJ-NPVPGRAPHONWSJ-POSINL-POSPP-ATTACH3,0003,000200,000350,000399,925250,00420,80112128757485.9+0.898.2+0.497.1?0.0896.6+0.0495.9?0.0496.3+0.181.3?0.581.6+1.098.2?0.596.5?0.0896.2?0.0695.9?0.0496.3?0.178.3+0.4Table 1: Number of instmlces, attributes and original accuracies for the datasets.Algorithm BSE BSJ-IGIBI-IG 34.2 23IGTREE 34.7 30.9IBI-IG&MVDM, k=l 47.2 30.6IBI-IG&MVDM, k=3 30.5 33.3IBI-IG&MVDM, k=7 27.8 25Average 34.9 28.6Table 2: Average compression rates.trapped as well; this lies outside the scope of this study.Although firm conclusions cannot be drawn on the basisof three datasets only, the compact and accurate resultsof the k = 3 and k = 7 classifiers may indicate a ten-dency for smoothing algorithms to compensate betterfor eventual non-optimal attribute combinations thanfor eliminated attributes.
This would be in agreementwith Pazzani's findings for PEBLS and Naive Baves.Frequently, cases were observed where BSE elimi-nates attributes that were used for joining by BSJ-IG.This indicates that at least some of the advantages ofattribute joining originate from implicit attribute lim-ination rather than combination, which has also beennoted by Pazzani (1998): removing an attribute mayimprove accuracy more than joining it to another at-tribute.Conc lus ionsThe effects of two representation-changing algorithmson generalization accuracy and data compression weretested for three different types of nearest neighbor clas-sifters, on 7 linguistic learning tasks.
As a consequenceof the permutation-invariance of the used classifiers andthe use of hill-climbing algorithms, a practical samplingof the search space of data transformations was applied.BSE.
an attribute limination algorithm, was found toproduce accurate classifiers, with consistently higherdata compression rates than BSJ-IG, an attribute join-ing algorithm.
The generalization accuracy of BSE iscomparable to that of BSJ-IG.Some evidence hints that attribute joining may bemore succesful - both for compression and accuracy- for classifiers employing smoothing techniques, e.g.PEBLS-Iike algorithms which select a nearest neighborfrom a nearest neighbor set using frequency informa-tion.
This type of classifier was able to trap at leastone important attribute interaction in the STRESS do-main, offering extended insight in the underlying learn-ing task.
Further evidence is needed to confirm thisconjecture, and may shed further li6ht on the questionwhether and how linguistic learning tasks could benefitfrom attribute interaction.
An alternative line of re-search to be pursued will address cla~ssifier models thatallow for linear encoding of linguistic learning tasks:these models will allow investigations into correctionson the linearity of linguistic tasks.AcknowledgementsI would like to thank Michael Pazzani for helpful com-ments, as well as the anonynmus reviewers.
Thanksgo to the Induction of Linguistic Knowledge group ofTilburg University (Antal van den Bosch, Sabine Buch-holz, Walter Daelemans, Jorn Veenstra nd Jal~ub Za-vrel) for valuable feedback and datasets.
INL is ac-knowledged for the INL-POS dataset and access to Sunequipment.Re ferences\[Aha, 1991\] Aha, D. (1991).
Incremental constructiveinduction: an instance-based approach.
PT~ceed-29Dataset IBI-IG&BSESTRESS 10 86.24-1.0DXMIN 4 98.54-0.4WSJ -NPVP _ _GRAPHON = -----ws~-POS 3 96.14-0.04INL-POS 3 96.54-0.09PP-ATTACH 3 82.04-0.7IGTREE&BSE6 84.94-1.0+5 98.44-0.57 96194-0.073 96.14-0.043 96.54-0.093 81.94-0.7+Table 3: Number of remaining attributes and accuracies for BSE.
A '+' indicates a siguificant increase in accuracycompared to the original algorithm; a '_' indicates the experiment could not be completed.DatasetSTRESSDIMINWSJ -NPVPGRAPHONWS.
I -POSINL -POSPP-ATTACHIBI-IG&BSJ-IG IGTREE&BSJ-IG9 86.64-1.0 8 85.24-0.8+6 98.54-0.4 6 98.4-t-0.46 96.94-0.086 96.24-0.054 96.04-0.04 4 96.04-0.044 96.54-0.1 4 96.54-0.1Table 4: Number of remaining attributes and accuracies for BSJ-IG.
A '+' indicates a significant increase in accuracycompared to the original algorithm; a '=' indicates no difference with respect to the original algorithm; a '_' indicatesthe experiment could not be completed.ings of the 8th Machine Learning Workshop.\[Aha, Kibler & Albert, 1991\] Aha, D. , Kibler, D. &Albert, M. (1991).
Instance-based learning algo-rithms.
Machine Learning, 6, 37-66.\[Aha, 1998\] Aha, D. (1998).
Feature weighting for lazylearning algorithms.
In Liu, H. & Motoda.
H.(eds.
), Feature Extraction, Construction and Se-lection.
Boston: Kluwer.\[Baayen, Piepenbrock & van Rijn, 1993\] Baayen, H .
.Piepenbrock, R. & van Rijn, H. (1993).
TheCELEX lexical database on CD-ROM.
LinguisticData Consortium.
Philadelphia, PA.\[Bloedorn&Michalski, 1991\] Bloedorn, E. & Michalski,R.S.
(1991).
Constructive Induction from Data inAQ17-DC1.
MLIgl-12.
Artificial Intelligence Cen-ter, George Mason University.\[Bloedorn, Michalski&Wnek\] Bloedorn, E., Michalski,R.
& Wnek, J.
(1994).
Matching Methods withProblems: A Comparative Analysis of ConstructiveInduction Approaches.
MLI94-12.
Artificial Intelli-gence Center, George Mason University.\[Cardie, 1996\] Cardie, C. (1996).
Automating featureset selection for case-based learning of linguisticknowledge.
Proceedings of the Conference on Em-pirical Methods in Natural Language Processing.University of Pennsylvania.\[Cleeremans, 1993\] Cleeremans, A.
(1993).
Mecha-nisms of implicit learning: connectionist modelsof sequence processing.
Cambridge, Mass.
: MITPress.\[Cost&Salzberg, 1993\] Cost, S. & Salzberg, S. (1993) Aweighted nearest neighbor algorithm for learningwith symbolic features.
Machine Learning, 10, 57-78.\[Daelemans et al, 1997\] Daelemans, W., vail denBosch, A.
& Zavrel, J.
(1997).
IGTREE: usingtrees for compression and classification in lazylearning algorithms.
Artificial Intelligence Review,11, 407-423.\[Daelemans et al, 1999i\] Daelemans, W., van denBosch, A.
& Zavrel, J.
(1999i).
Forgetting ex-ceptions is harmful in language learning.
MachineLearning, 11, 11-43.\[Daelemans et al 1999ii\] Daelemans.
W., Zavrel, J.,van der Sloot, K. & van den Bosch.
A.
(1999ii).30IBI-IG&MVDMDataset k=l k=3 k=7STRESS 86.14-1.0 86.84-1.0 87.04-1.0DIMIN 98.04-0.4 98.44-0.3 98.44-0.3PP-ATTACH 75.74-0.7 76.94-0.7 77.74-0.6IBI-IG&MVDM&BSEDataset k=l k=3 k=7STRESS 7 88.34-1.0+ 7 88.5 4-1.0 9 88.04-1.0DIMIN 6 98.64-0.4 6 98.84-0.4 5 98.7+0.2PP-ATTACH 2 78.14-0.6+IBI-IG&MVDM&BSJ-IGDataset k= 1 k=3 k= 7STRESS 8 89.0+1.0+ 8 89.84-1.0+ 10 89.84-0.7+DIMIN 8 98.44-0.4 7 98.64-0.3 8 98.64-0.2PP-ATTACH 3 77.74-0.5+ 3 77.54-0.5 3 77.7+0.5Table 5: Number of remaining attributes and accuracies for IBI-IG with MVDM, IBI-IG with MVDM and BSE.
andfor IBI-IG with MVDM and BSJ-IG, for k=l, 3, and 7.
A '+' indicates a significant increase in accuracy comparedto the original algorithm; a '=' indicates no difference with respect o the original algorithm.TiMBL: Tilburg Memory Based Learner, ver-sion 2.0, Reference Guide.
Tilburg: Induc-tion of Linguistic Knowledge.
Available fromhttp://ilk, kub.
nl/-ilk/papers/ilk9901, ps.
gz.\[Daelemans, Buchholz & Veensira, 1999iii\]Daelemans, W., Buchholz, S. & Veenstra, J.(1999iii).
Memory-based shallow parsing.
PaperILK, Tilburg.\[Duda&Hart, 1973\] Duda, R. & Hart, P. (1973).
Pat-tern Classification and Scene Analysis.
Wile)"Press.\[John, 1997\] John, G. H., 1997.
Enhancements o thedata mining process.
Ph.D. dissertation.
StanfordUniversity.\[Kohavi & John, 1998\] Kohavi, R. & John, G.H.(1998).
The wrapper approach.
In Liu, H. & Mo-toda, H.
(eds.
), Feature Extraction, Constructionand Selection.
Boston: Kluwer.\[Keogh&Pazzani, 1999\] Keogh, E. J.
& M. J. Paz-zani, 1999.
Learning augmented Bayesian clas-sifters: a comparison of distribution-based andclassification-based approaches.
Proceedings 7thInternational Workshop on AI and Statistics, 225-230.
Ft. Lauderdale, Florida.\[.Marcus.
Santorini & Marcinkiewicz, 1993\] Marcus...M.. Santorini.
B.
& Marcinkiewicz (1993).
Build-ing a large annotated corpus of English: the PennTreebank.
Computational Linguistics 19".
313-330.\[Pagallo& Hauser, 1990\] Pagallo, G., Haussler, D.(1990).
Boolean feature discovery in empiricallearning.
Machine Learning, 5, 71-99.\[Pazzani, 1998\] Pazzani, Michael J.
(1998).
Construc-tive Induction of Cartesian Product Attributes.
InLiu, H. & Motoda, H.
(eds.
), Feature Extraction,Construction and Selection.
Boston: Kluwer.\[Quinlan, 1993\] Quinlan, J. R. (1993).
C4.5: Programsfor Machine Learning.
San Mateo, CA.
: MorganKaufmann.\[Ratnaparkhi eta/., 1994\] Ratnaparkhi.
A.. Reynar, J.& Roulos, S. (1994).
A maxinmm entropy modelfor Prepositional Phrase Attachment.
ProceedingsARPA Workshop on Human Language Technology.Plainsboro.\[Scherf & Brauer, 1997\] Scherf, M.. Brauer, W. (1997).Feature selection by means of a feature weight-ing approach.
Technical report No.
FKI-221-97,Forschungsberichte Kiinstliche Intelligenz.
Institutfiir Informatik, Technische Universitiit Miinchen.\[van der Voort van der Kleij et al, 1994\]van der Voort van der Kleij, J., Raaijmakers, S.,Panhuijsen, M., Meijering, M., Sterkenburg, R.31van (1994).
Een automatisch geanalyseerd corpushedendaags Nederlands ineen flexibel retrievalsys-teem.
(An automatically analysed corpus of con-temporary Dutch in a flexible retrieval system, inDutch.)
Proceedings lnformatiewetenschap 1994.Tilburg.\[Yang & Honovar, 1998\] Yang, J.
& Honovar, V.(1998).
Feature subset selection using a genetic al-gorithm.
In Liu, H. & Motoda, H.
(eds.
), FeatureExtraction, Construction and Selection.
Boston:Kluwer.\[Zavrel & Daelemans, 1997\] Zavrel, J.
& Daelemans,W.
(1997).
Memory-based learning: using similar-ity for smoothing.
Proceedings of the 35th annualmeeting of the ACL.
Madrid.32
