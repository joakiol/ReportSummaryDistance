Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 673?680,Sydney, July 2006. c?2006 Association for Computational LinguisticsContextual Dependencies in Unsupervised Word Segmentation?Sharon Goldwater and Thomas L. Griffiths and Mark JohnsonDepartment of Cognitive and Linguistic SciencesBrown UniversityProvidence, RI 02912{Sharon Goldwater,Tom Griffiths,Mark Johnson}@brown.eduAbstractDeveloping better methods for segment-ing continuous text into words is impor-tant for improving the processing of Asianlanguages, and may shed light on how hu-mans learn to segment speech.
We pro-pose two new Bayesian word segmenta-tion methods that assume unigram and bi-gram models of word dependencies re-spectively.
The bigram model greatly out-performs the unigram model (and previousprobabilistic models), demonstrating theimportance of such dependencies for wordsegmentation.
We also show that previousprobabilistic models rely crucially on sub-optimal search procedures.1 IntroductionWord segmentation, i.e., discovering word bound-aries in continuous text or speech, is of interest forboth practical and theoretical reasons.
It is the firststep of processing orthographies without explicitword boundaries, such as Chinese.
It is also oneof the key problems that human language learnersmust solve as they are learning language.Many previous methods for unsupervised wordsegmentation are based on the observation thattransitions between units (characters, phonemes,or syllables) within words are generally more pre-dictable than transitions across word boundaries.Statistics that have been proposed for measuringthese differences include ?successor frequency?
(Harris, 1954), ?transitional probabilities?
(Saf-fran et al, 1996), mutual information (Sun et al,?This work was partially supported by the followinggrants: NIH 1R01-MH60922, NIH RO1-DC000314, NSFIGERT-DGE-9870676, and the DARPA CALO project.1998), ?accessor variety?
(Feng et al, 2004), andboundary entropy (Cohen and Adams, 2001).While methods based on local statistics arequite successful, here we focus on approachesbased on explicit probabilistic models.
Formulat-ing an explicit probabilistic model permits us tocleanly separate assumptions about the input andproperties of likely segmentations from details ofalgorithms used to find such solutions.
Specifi-cally, this paper demonstrates the importance ofcontextual dependencies for word segmentationby comparing two probabilistic models that dif-fer only in that the first assumes that the proba-bility of a word is independent of its local context,while the second incorporates bigram dependen-cies between adjacent words.
The algorithms weuse to search for likely segmentations do differ,but so long as the segmentations they produce areclose to optimal we can be confident that any dif-ferences in the segmentations reflect differences inthe probabilistic models, i.e., in the kinds of de-pendencies between words.We are not the first to propose explicit prob-abilistic models of word segmentation.
Twosuccessful word segmentation systems based onexplicit probabilistic models are those of Brent(1999) and Venkataraman (2001).
Brent?s Model-Based Dynamic Programming (MBDP) system as-sumes a unigram word distribution.
Venkatara-man uses standard unigram, bigram, and trigramlanguage models in three versions of his system,which we refer to as n-gram Segmentation (NGS).Despite their rather different generative structure,the MBDP and NGS segmentation accuracies arevery similar.
Moreover, the segmentation accuracyof the NGS unigram, bigram, and trigram mod-els hardly differ, suggesting that contextual depen-dencies are irrelevant to word segmentation.
How-673ever, the segmentations produced by both thesemethods depend crucially on properties of thesearch procedures they employ.
We show this byexhibiting for each model a segmentation that isless accurate but more probable under that model.In this paper, we present an alternative frame-work for word segmentation based on the Dirich-let process, a distribution used in nonparametricBayesian statistics.
This framework allows us todevelop extensible models that are amenable tostandard inference procedures.
We present twosuch models incorporating unigram and bigramword dependencies, respectively.
We use Gibbssampling to sample from the posterior distributionof possible segmentations under these models.The plan of the paper is as follows.
In the nextsection, we describe MBDP and NGS in detail.
InSection 3 we present the unigram version of ourown model, the Gibbs sampling procedure we usefor inference, and experimental results.
Section 4extends that model to incorporate bigram depen-dencies, and Section 5 concludes the paper.2 NGS and MBDPThe NGS and MBDP systems are similar in someways: both are designed to find utterance bound-aries in a corpus of phonemically transcribed ut-terances, with known utterance boundaries.
Bothalso use approximate online search procedures,choosing and fixing a segmentation for each utter-ance before moving onto the next.
In this section,we focus on the very different probabilistic mod-els underlying the two systems.
We show that theoptimal solution under the NGS model is the un-segmented corpus, and suggest that this problemstems from the fact that the model assumes a uni-form prior over hypotheses.
We then present theMBDP model, which uses a non-uniform prior butis difficult to extend beyond the unigram case.2.1 NGSNGS assumes that each utterance is generated in-dependently via a standard n-gram model.
Forsimplicity, we will discuss the unigram version ofthe model here, although our argument is equallyapplicable to the bigram and trigram versions.
Theunigram model generates an utterance u accordingto the grammar in Figure 1, soP (u) = p$(1?
p$)n?1n?j=1P (wj) (1)1?
p$ U?W Up$ U?WP (w) W?w ?w ?
?
?Figure 1: The unigram NGS grammar.where u consists of the words w1 .
.
.
wn and p$ isthe probability of the utterance boundary marker$.
This model can be used to find the highest prob-ability segmentation hypothesis h given the data dby using Bayes?
rule:P (h|d) ?
P (d|h)P (h)NGS assumes a uniform prior P (h) over hypothe-ses, so its goal is to find the solution that maxi-mizes the likelihood P (d|h).Using this model, NGS?s approximate searchtechnique delivers competitive results.
However,the true maximum likelihood solution is not com-petitive, since it contains no utterance-internalword boundaries.
To see why not, consider thesolution in which p$ = 1 and each utterance is asingle ?word?, with probability equal to the empir-ical probability of that utterance.
Any other so-lution will match the empirical distribution of thedata less well.
In particular, a solution with ad-ditional word boundaries must have 1 ?
p$ > 0,which means it wastes probability mass modelingunseen data (which can now be generated by con-catenating observed utterances together).Intuitively, the NGS model considers the unseg-mented solution to be optimal because it ranks allhypotheses equally probable a priori.
We know,however, that hypotheses that memorize the inputdata are unlikely to generalize to unseen data, andare therefore poor solutions.
To prevent memo-rization, we could restrict our hypothesis space tomodels with fewer parameters than the number ofutterances in the data.
A more general and mathe-matically satisfactory solution is to assume a non-uniform prior, assigning higher probability to hy-potheses with fewer parameters.
This is in fact theroute taken by Brent in his MBDP model, as weshall see in the following section.2.2 MBDPMBDP assumes a corpus of utterances is gener-ated as a single probabilistic event with four steps:1.
Generate L, the number of lexical types.2.
Generate a phonemic representation for eachtype (except the utterance boundary type, $).6743.
Generate a token frequency for each type.4.
Generate an ordering for the set of tokens.In a final deterministic step, the ordered tokensare concatenated to create an unsegmented cor-pus.
This means that certain segmented corporawill produce the observed data with probability 1,and all others will produce it with probability 0.The posterior probability of a segmentation giventhe data is thus proportional to its prior probabilityunder the generative model, and the best segmen-tation is that with the highest prior probability.There are two important points to note aboutthe MBDP model.
First, the distribution over Lassigns higher probability to models with fewerlexical items.
We have argued that this is neces-sary to avoid memorization, and indeed the unseg-mented corpus is not the optimal solution underthis model, as we will show in Section 3.
Second,the factorization into four separate steps makesit theoretically possible to modify each step in-dependently in order to investigate the effects ofthe various modeling assumptions.
However, themathematical statement of the model and the ap-proximations necessary for the search proceduremake it unclear how to modify the model in anyinteresting way.
In particular, the fourth step usesa uniform distribution, which creates a unigramconstraint that cannot easily be changed.
Since ourresearch aims to investigate the effects of differentmodeling assumptions on lexical acquisition, wedevelop in the following sections a far more flex-ible model that also incorporates a preference forsparse solutions.3 Unigram Model3.1 The Dirichlet Process ModelOur goal is a model of language that preferssparse solutions, allows independent modificationof components, and is amenable to standard searchprocedures.
We achieve this goal by basing ourmodel on the Dirichlet process (DP), a distributionused in nonparametric Bayesian statistics.
Our un-igram model of word frequencies is defined aswi|G ?
GG|?0, P0 ?
DP(?0, P0)where the concentration parameter ?0 and thebase distribution P0 are parameters of the model.Each word wi in the corpus is drawn from adistribution G, which consists of a set of pos-sible words (the lexicon) and probabilities asso-ciated with those words.
G is generated froma DP(?0, P0) distribution, with the items in thelexicon being sampled from P0 and their proba-bilities being determined by ?0, which acts likethe parameter of an infinite-dimensional symmet-ric Dirichlet distribution.
We provide some intu-ition for the roles of ?0 and P0 below.Although the DP model makes the distributionG explicit, we never deal with G directly.
Wetake a Bayesian approach and integrate over allpossible values of G. The conditional probabil-ity of choosing to generate a word from a particu-lar lexical entry is then given by a simple stochas-tic process known as the Chinese restaurant pro-cess (CRP) (Aldous, 1985).
Imagine a restaurantwith an infinite number of tables, each with infiniteseating capacity.
Customers enter the restaurantand seat themselves.
Let zi be the table chosen bythe ith customer.
ThenP (zi|z?i) =??
?n(z?i)ki?1+?0 0 ?
k < K(z?i)?0i?1+?0 k = K(z?i)(2)where z?i = z1 .
.
.
zi?1, n(z?i)k is the number ofcustomers already sitting at table k, and K(z?i) isthe total number of occupied tables.
In our model,the tables correspond to (possibly repeated) lexicalentries, having labels generated from the distribu-tion P0.
The seating arrangement thus specifiesa distribution over word tokens, with each cus-tomer representing one token.
This model is aninstance of the two-stage modeling framework de-scribed by Goldwater et al (2006), with P0 as thegenerator and the CRP as the adaptor.Our model can be viewed intuitively as a cachemodel: each word in the corpus is either retrievedfrom a cache or generated anew.
Summing overall the tables labeled with the same word yieldsthe probability distribution for the ith word givenpreviously observed words w?i:P (wi|w?i) =n(w?i)wii?
1 + ?0+ ?0P0(wi)i?
1 + ?0(3)where n(w?i)w is the number of instances of w ob-served in w?i.
The first term is the probabilityof generating w from the cache (i.e., sitting at anoccupied table), and the second term is the proba-bility of generating it anew (sitting at an unoccu-pied table).
The actual table assignments z?i onlybecome important later, in the bigram model.675There are several important points to note aboutthis model.
First, the probability of generating aparticular word from the cache increases as moreinstances of that word are observed.
This rich-get-richer process creates a power-law distributionon word frequencies (Goldwater et al, 2006), thesame sort of distribution found empirically in nat-ural language.
Second, the parameter ?0 can beused to control how sparse the solutions found bythe model are.
This parameter determines the totalprobability of generating any novel word, a proba-bility that decreases as more data is observed, butnever disappears.
Finally, the parameter P0 canbe used to encode expectations about the natureof the lexicon, since it defines a probability distri-bution across different novel words.
The fact thatthis distribution is defined separately from the dis-tribution on word frequencies gives the model ad-ditional flexibility, since either distribution can bemodified independently of the other.Since the goal of this paper is to investigate therole of context in word segmentation, we chosethe simplest possible model for P0, i.e.
a unigramphoneme distribution:P0(w) = p#(1?
p#)n?1n?i=1P (mi) (4)where word w consists of the phonemesm1 .
.
.
mn, and p# is the probability of theword boundary #.
For simplicity we useda uniform distribution over phonemes, andexperimented with different fixed values of p#.1A final detail of our model is the distributionon utterance lengths, which is geometric.
That is,we assume a grammar similar to the one shown inFigure 1, with the addition of a symmetric Beta( ?2 )prior over the probability of the U productions,2and the substitution of the DP for the standardmultinomial distribution over the W productions.3.2 Gibbs SamplingHaving defined our generative model, we are leftwith the problem of inference: we must determinethe posterior distribution of hypotheses given ourinput corpus.
To do so, we use Gibbs sampling,a standard Markov chain Monte Carlo method(Gilks et al, 1996).
Gibbs sampling is an itera-tive procedure in which variables are repeatedly1Note, however, that our model could be extended to learnboth p# and the distribution over phonemes.2The Beta distribution is a Dirichlet distribution over twooutcomes.WUw1 = w2.w3UWUWw3w2h1: h2:Figure 2: The two hypotheses considered by theunigram sampler.
Dashed lines indicate possibleadditional structure.
All rules except those in boldare part of h?.sampled from their conditional posterior distribu-tion given the current values of all other variablesin the model.
The sampler defines a Markov chainwhose stationary distribution is P (h|d), so afterconvergence samples are from this distribution.Our Gibbs sampler considers a single possibleboundary point at a time, so each sample is froma set of two hypotheses, h1 and h2.
These hy-potheses contain all the same boundaries exceptat the one position under consideration, where h2has a boundary and h1 does not.
The structures areshown in Figure 2.
In order to sample a hypothe-sis, we need only calculate the relative probabili-ties of h1 and h2.
Since h1 and h2 are the same ex-cept for a few rules, this is straightforward.
Let h?be all of the structure shared by the two hypothe-ses, including n?
words, and let d be the observeddata.
ThenP (h1|h?, d) = P (w1|h?, d)= n(h?
)w1 + ?0P0(w1)n?
+ ?0(5)where the second line follows from Equation 3and the properties of the CRP (in particular, that itis exchangeable, with the probability of a seatingconfiguration not depending on the order in whichcustomers arrive (Aldous, 1985)).
Also,P (h2|h?, d)= P (r, w2, w3|h?, d)= P (r|h?, d)P (w2|h?, d)P (w3|w2, h?, d)= nr +?2n?
+ 1 + ?
?n(h?
)w2 + ?0P0(w2)n?
+ ?0?n(h?
)w3 + I(w2 = w3) + ?0P0(w3)n?
+ 1 + ?0(6)where nr is the number of branching rules r =U ?
W U in h?, and I(.)
is an indicator func-tion taking on the value 1 when its argument is676true, and 0 otherwise.
The nr term is derived byintegrating over all possible values of p$, and not-ing that the total number of U productions in h?is n?
+ 1.Using these equations we can simply proceedthrough the data, sampling each potential bound-ary point in turn.
Once the Gibbs sampler con-verges, these samples will be drawn from the pos-terior distribution P (h|d).3.3 ExperimentsIn our experiments, we used the same corpusthat NGS and MBDP were tested on.
The cor-pus, supplied to us by Brent, consists of 9790transcribed utterances (33399 words) of child-directed speech from the Bernstein-Ratner cor-pus (Bernstein-Ratner, 1987) in the CHILDESdatabase (MacWhinney and Snow, 1985).
The ut-terances have been converted to a phonemic rep-resentation using a phonemic dictionary, so thateach occurrence of a word has the same phonemictranscription.
Utterance boundaries are given inthe input to the system; other word boundaries arenot.Because our Gibbs sampler is slow to converge,we used annealing to speed inference.
We beganwith a temperature of ?
= 10 and decreased ?
in10 increments to a final value of 1.
A temperatureof ?
corresponds to raising the probabilities of h1and h2 to the power of 1?
prior to sampling.We ran our Gibbs sampler for 20,000 iterationsthrough the corpus (with ?
= 1 for the final 2000)and evaluated our results on a single sample atthat point.
We calculated precision (P), recall (R),and F-score (F) on the word tokens in the corpus,where both boundaries of a word must be correctto count the word as correct.
The induced lexiconwas also scored for accuracy using these metrics(LP, LR, LF).Recall that our DP model has three parameters:?, p#, and ?0.
Given the large number of knownutterance boundaries, we expect the value of ?
tohave little effect on our results, so we simply fixed?
= 2 for all experiments.
Figure 3 shows the ef-fects of varying of p# and ?0.3 Lower values ofp# cause longer words, which tends to improve re-call (and thus F-score) in the lexicon, but decreasetoken accuracy.
Higher values of ?0 allow morenovel words, which also improves lexicon recall,3It is worth noting that all these parameters could be in-ferred.
We leave this for future work.0.1 0.3 0.5 0.7 0.9505560(a) Varying P(#)1 2 5 10 20 50 100 200 500505560(b) Varying ?0LFFLFFFigure 3: Word (F) and lexicon (LF) F-score (a)as a function of p#, with ?0 = 20 and (b) as afunction of ?0, with p# = .5.but begins to degrade precision after a point.
Dueto the negative correlation between token accuracyand lexicon accuracy, there is no single best valuefor either p# or ?0; further discussion refers to thesolution for p# = .5, ?0 = 20 (though others arequalitatively similar).In Table 1(a), we compare the results of our sys-tem to those of MBDP and NGS.4 Although oursystem has higher lexicon accuracy than the oth-ers, its token accuracy is much worse.
This resultoccurs because our system often mis-analyzes fre-quently occurring words.
In particular, many ofthese words occur in common collocations suchas what?s that and do you, which the system inter-prets as a single words.
It turns out that a full 31%of the proposed lexicon and nearly 30% of tokensconsist of these kinds of errors.Upon reflection, it is not surprising that a uni-gram language model would segment words in thisway.
Collocations violate the unigram assumptionin the model, since they exhibit strong word-to-word dependencies.
The only way the model cancapture these dependencies is by assuming thatthese collocations are in fact words themselves.Why don?t the MBDP and NGS unigram mod-els exhibit these problems?
We have alreadyshown that NGS?s results are due to its search pro-cedure rather than its model.
The same turns outto be true for MBDP.
Table 2 shows the probabili-4We used the implementations of MBDP and NGS avail-able at http://www.speech.sri.com/people/anand/ to obtain re-sults for those systems.677(a) P R F LP LR LFNGS 67.7 70.2 68.9 52.9 51.3 52.0MBDP 67.0 69.4 68.2 53.6 51.3 52.4DP 61.9 47.6 53.8 57.0 57.5 57.2(b) P R F LP LR LFNGS 76.6 85.8 81.0 60.0 52.4 55.9MBDP 77.0 86.1 81.3 60.8 53.0 56.6DP 94.2 97.1 95.6 86.5 62.2 72.4Table 1: Accuracy of the various systems, withbest scores in bold.
The unigram version of NGSis shown.
DP results are with p# = .5 and ?0 =20.
(a) Results on the true corpus.
(b) Results onthe permuted corpus.Seg: True None MBDP NGS DPNGS 204.5 90.9 210.7 210.8 183.0MBDP 208.2 321.7 217.0 218.0 189.8DP 222.4 393.6 231.2 231.6 200.6Table 2: Negative log probabilities (x 1000) un-der each model of the true solution, the solutionwith no utterance-internal boundaries, and the so-lutions found by each algorithm.
Best solutionsunder each model are bold.ties under each model of various segmentations ofthe corpus.
From these figures, we can see thatthe MBDP model assigns higher probability to thesolution found by our Gibbs sampler than to thesolution found by Brent?s own incremental searchalgorithm.
In other words, Brent?s model does pre-fer the lower-accuracy collocation solution, but hissearch algorithm instead finds a higher-accuracybut lower-probability solution.We performed two experiments suggesting thatour own inference procedure does not suffer fromsimilar problems.
First, we initialized our Gibbssampler in three different ways: with no utterance-internal boundaries, with a boundary after everycharacter, and with random boundaries.
Our re-sults were virtually the same regardless of initial-ization.
Second, we created an artificial corpus byrandomly permuting the words in the true corpus,leaving the utterance lengths the same.
The ar-tificial corpus adheres to the unigram assumptionof our model, so if our inference procedure workscorrectly, we should be able to correctly identifythe words in the permuted corpus.
This is exactlywhat we found, as shown in Table 1(b).
While allthree models perform better on the artificial cor-pus, the improvements of the DP model are by farthe most striking.4 Bigram Model4.1 The Hierarchical Dirichlet Process ModelThe results of our unigram experiments suggestedthat word segmentation could be improved bytaking into account dependencies between words.To test this hypothesis, we extended our modelto incorporate bigram dependencies using a hi-erarchical Dirichlet process (HDP) (Teh et al,2005).
Our approach is similar to previous n-grammodels using hierarchical Pitman-Yor processes(Goldwater et al, 2006; Teh, 2006).
The HDP isappropriate for situations in which there are multi-ple distributions over similar sets of outcomes, andthe distributions are believed to be similar.
In ourcase, we define a bigram model by assuming eachword has a different distribution over the wordsthat follow it, but all these distributions are linked.The definition of our bigram language model as anHDP iswi|wi?1 = w,Hw ?
Hw ?wHw|?1, G ?
DP(?1, G) ?wG|?0, P0 ?
DP(?0, P0)That is, P (wi|wi?1 = w) is distributed accord-ing to Hw, a DP specific to word w. Hw is linkedto the DPs for all other words by the fact that theyshare a common base distribution G, which is gen-erated from another DP.5As in the unigram model, we never deal withHw or G directly.
By integrating over them, we geta distribution over bigram frequencies that can beunderstood in terms of the CRP.
Now, each wordtype w is associated with its own restaurant, whichrepresents the distribution over words that followw.
Different restaurants are not completely inde-pendent, however: the labels on the tables in therestaurants are all chosen from a common basedistribution, which is another CRP.To understand the HDP model in terms of agrammar, we consider $ as a special word type,so that wi ranges over ??
?
{$}.
After observingw?i, the HDP grammar is as shown in Figure 4,5This HDP formulation is an oversimplification, since itdoes not account for utterance boundaries properly.
Thegrammar formulation (see below) does.678P2(wi|w?i, z?i) Uwi?1?Wwi Uwi ?wi ?
?
?,wi?1 ?
???
{$}P2($|w?i, z?i) Uwi?1?$ ?wi?1 ?
?
?1 Wwi ?wi ?wi ?
?
?Figure 4: The HDP grammar after observing w?i.withP2(wi|h?i) =n(wi?1,wi) + ?1P1(wi|h?i)nwi?1 + ?1(7)P1(wi|h?i) =???t?
?+ ?2t+?
?twi+?0P0(wi)t?
?+?0 wi ?
?
?t$+ ?2t+?
wi = $where h?i = (w?i, z?i); t$, t??
, and twi are thetotal number of tables (across all words) labeledwith $, non-$, and wi, respectively; t = t$ + t?
?is the total number of tables; and n(wi?1,wi) is thenumber of occurrences of the bigram (wi?1, wi).We have suppressed the superscript (w?i) nota-tion in all cases.
The base distribution shared byall bigrams is given by P1, which can be viewed asa unigram backoff where the unigram probabilitiesare learned from the bigram table labels.We can perform inference on this HDP bigrammodel using a Gibbs sampler similar to our uni-gram sampler.
Details appear in the Appendix.4.2 ExperimentsWe used the same basic setup for our experimentswith the HDP model as we used for the DP model.We experimented with different values of ?0 and?1, keeping p# = .5 throughout.
Some resultsof these experiments are plotted in Figure 5.
Withappropriate parameter settings, both lexicon andtoken accuracy are higher than in the unigrammodel (dramatically so, for tokens), and there isno longer a negative correlation between the two.Only a few collocations remain in the lexicon, andmost lexicon errors are on low-frequency words.The best values of ?0 are much larger than in theunigram model, presumably because all uniqueword types must be generated via P0, but in thebigram model there is an additional level of dis-counting (the unigram process) before reachingP0.
Smaller values of ?0 lead to fewer word typeswith fewer characters on average.Table 3 compares the optimal results of theHDP model to the only previous model incorpo-rating bigram dependencies, NGS.
Due to search,the performance of the bigram NGS model is notmuch different from that of the unigram model.
In100 200 500 1000 2000406080(a) Varying ?0FLF5 10 20 50 100 200 500406080(b) Varying ?1FLFFigure 5: Word (F) and lexicon (LF) F-score (a)as a function of ?0, with ?1 = 10 and (b) as afunction of ?1, with ?0 = 1000.P R F LP LR LFNGS 68.1 68.6 68.3 54.5 57.0 55.7HDP 79.4 74.0 76.6 67.9 58.9 63.1Table 3: Bigram system accuracy, with best scoresin bold.
HDP results are with p# = .5, ?0 =1000, and ?1 = 10.contrast, our HDP model performs far better thanour DP model, leading to the highest published ac-curacy for this corpus on both tokens and lexicalitems.
Overall, these results strongly support ourhypothesis that modeling bigram dependencies isimportant for accurate word segmentation.5 ConclusionIn this paper, we have introduced a new model-based approach to word segmentation that drawson techniques from Bayesian statistics, and wehave developed models incorporating unigram andbigram dependencies.
The use of the Dirichletprocess as the basis of our approach yields sparsesolutions and allows us the flexibility to modifyindividual components of the models.
We havepresented a method of inference using Gibbs sam-pling, which is guaranteed to converge to the pos-terior distribution over possible segmentations ofa corpus.Our approach to word segmentation allows us toinvestigate questions that could not be addressedsatisfactorily in earlier work.
We have shown thatthe search algorithms used with previous modelsof word segmentation do not achieve their ob-679P (h1|h?, d) =n(wl,w1) + ?1P1(w1|h?, d)nwl + ?1?n(w1,wr) + I(wl =w1 =wr) + ?1P1(wr|h?, d)nw1 + 1 + ?1P (h2|h?, d) =n(wl,w2) + ?1P1(w2|h?, d)nwl + ?1?n(w2,w3) + I(wl =w2 =w3) + ?1P1(w3|h?, d)nw2 + 1 + ?1?n(w3,wr) + I(wl =w3, w2 =wr) + I(w2 =w3 =wr) + ?1P1(wr|h?, d)nw3 + 1 + I(w2 =w4) + ?1Figure 6: Gibbs sampling equations for the bigram model.
All counts are with respect to h?.jectives, which has led to misleading results.
Inparticular, previous work suggested that the useof word-to-word dependencies has little effect onword segmentation.
Our experiments indicate in-stead that bigram dependencies can be crucial foravoiding under-segmentation of frequent colloca-tions.
Incorporating these dependencies into ourmodel greatly improved segmentation accuracy,and led to better performance than previous ap-proaches on all measures.ReferencesD.
Aldous.
1985.
Exchangeability and related topics.
In?Ecole d?e?te?
de probabilite?s de Saint-Flour, XIII?1983,pages 1?198.
Springer, Berlin.C.
Antoniak.
1974.
Mixtures of Dirichlet processes with ap-plications to Bayesian nonparametric problems.
The An-nals of Statistics, 2:1152?1174.N.
Bernstein-Ratner.
1987.
The phonology of parent-childspeech.
In K. Nelson and A. van Kleeck, editors, Chil-dren?s Language, volume 6.
Erlbaum, Hillsdale, NJ.M.
Brent.
1999.
An efficient, probabilistically sound al-gorithm for segmentation and word discovery.
MachineLearning, 34:71?105.P.
Cohen and N. Adams.
2001.
An algorithm for segment-ing categorical timeseries into meaningful episodes.
InProceedings of the Fourth Symposium on Intelligent DataAnalysis.H.
Feng, K. Chen, X. Deng, and W. Zheng.
2004.
Acces-sor variety criteria for chinese word extraction.
Computa-tional Lingustics, 30(1).W.R.
Gilks, S. Richardson, and D. J. Spiegelhalter, editors.1996.
Markov Chain Monte Carlo in Practice.
Chapmanand Hall, Suffolk.S.
Goldwater, T. Griffiths, and M. Johnson.
2006.
Interpo-lating between types and tokens by estimating power-lawgenerators.
In Advances in Neural Information Process-ing Systems 18, Cambridge, MA.
MIT Press.Z.
Harris.
1954.
Distributional structure.
Word, 10:146?162.B.
MacWhinney and C. Snow.
1985.
The child language dataexchange system.
Journal of Child Language, 12:271?296.J.
Saffran, E. Newport, and R. Aslin.
1996.
Word segmenta-tion: The role of distributional cues.
Journal of Memoryand Language, 35:606?621.M.
Sun, D. Shen, and B. Tsou.
1998.
Chinese word seg-mentation without using lexicon and hand-crafted trainingdata.
In Proceedings of COLING-ACL.Y.
Teh, M. Jordan, M. Beal, and D. Blei.
2005.
HierarchicalDirichlet processes.
In Advances in Neural InformationProcessing Systems 17.
MIT Press, Cambridge, MA.Y.
Teh.
2006.
A Bayesian interpretation of interpolatedkneser-ney.
Technical Report TRA2/06, National Univer-sity of Singapore, School of Computing.A.
Venkataraman.
2001.
A statistical model for word dis-covery in transcribed speech.
Computational Linguistics,27(3):351?372.AppendixTo sample from the posterior distribution over seg-mentations in the bigram model, we define h1 andh2 as we did in the unigram sampler so that for thecorpus substring s, h1 has a single word (s = w1)where h2 has two (s = w2.w3).
Let wl and wr bethe words (or $) preceding and following s. Thenthe posterior probabilities of h1 and h2 are givenin Figure 6.
P1(.)
can be calculated exactly usingthe equation in Section 4.1, but this requires ex-plicitly tracking and sampling the assignment ofwords to tables.
For easier and more efficient im-plementation, we use an approximation, replacingeach table count twi by its expected value E[twi ].In a DP(?,P ), the expected number of CRP tablesfor an item occurring n times is ?
log n+??
(Anto-niak, 1974), soE[twi ] = ?1?jlogn(wj ,wi) + ?1?1This approximation requires only the bigramcounts, which we must track anyway.680
