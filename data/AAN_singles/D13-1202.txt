Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1960?1970,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsOf words, eyes and brains:Correlating image-based distributional semantic modelswith neural representations of conceptsAndrew J. Anderson, Elia Bruni, Ulisse Bordignon, Massimo Poesio and Marco BaroniCenter for Mind/Brain Sciences (University of Trento, C.so Bettini 31, Rovereto, Italy)first.last@unitn.itAbstractTraditional distributional semantic models ex-tract word meaning representations from co-occurrence patterns of words in text cor-pora.
Recently, the distributional approach hasbeen extended to models that record the co-occurrence of words with visual features inimage collections.
These image-based modelsshould be complementary to text-based ones,providing a more cognitively plausible viewof meaning grounded in visual perception.
Inthis study, we test whether image-based mod-els capture the semantic patterns that emergefrom fMRI recordings of the neural signal.Our results indicate that, indeed, there is asignificant correlation between image-basedand brain-based semantic similarities, and thatimage-based models complement text-basedones, so that the best correlations are achievedwhen the two modalities are combined.
De-spite some unsatisfactory, but explained out-comes (in particular, failure to detect differ-ential association of models with brain areas),the results show, on the one hand, that image-based distributional semantic models can be aprecious new tool to explore semantic repre-sentation in the brain, and, on the other, thatneural data can be used as the ultimate test setto validate artificial semantic models in termsof their cognitive plausibility.1 IntroductionMany recent neuroscientific studies have broughtsupport to the view that concepts are representedin terms of patterns of neural activation over broadareas, naturally encoded as vectors in a neural se-mantic space (Haxby et al 2001; Huth et al 2012).Similar representations are also widely used in com-putational linguistics, and in particular in distribu-tional semantics (Clark, 2012; Erk, 2012; Turneyand Pantel, 2010), that captures meaning in termsof vectors recording the patterns of co-occurrenceof words in large corpora, under the hypothesis thatwords that occur in similar contexts are similar inmeaning.Since the seminal work of Mitchell et al(2008),there has thus being interest in investigating whethercorpus-harvested semantic representations can con-tribute to the study of concepts in the brain.
Therelation is mutually beneficial: From the point ofview of brain activity decoding, a strong correlationbetween corpus-based and brain-derived conceptualrepresentations would mean that we could use theformer (much easier to construct on a very largescale) to make inferences about the second: e.g., us-ing corpus-based representations to reconstruct thelikely neural signal associated to words we have nodirect brain data for.
From the point of view of com-putational linguistics, neural data provide the ulti-mate testing ground for models that strive to cap-ture important aspects of human semantic mem-ory (much more so than the commonly used ex-plicit semantic rating benchmarks).
If we found thata corpus-based model of meaning can make non-trivial predictions about the structure of the semanticspace in the brain, that would make a pretty strongcase for the intriguing idea that the model is approx-imating, in interesting ways, the way in which hu-mans acquire and represent semantic knowledge.1960We take as our starting point the extensive experi-ments reported in Murphy et al(2012), who showedthat purely corpus-based distributional models are atleast as good at brain signal prediction tasks as ear-lier models that made use of manually-generated orcontrolled knowledge sources (Chang et al 2011;Palatucci et al 2009; Pereira et al 2011), and weevaluate a very recent type of distributional model,namely one that is not extracted from textual databut from image collections through automated vi-sual feature extraction techniques.
It has been ar-gued that this new generation of image-based dis-tributional models (Bruni et al 2011; Bruni et al2012b; Feng and Lapata, 2010; Leong and Mihal-cea, 2011) provides a more realistic view of mean-ing, since humans obviously acquire a large propor-tion of their semantic knowledge from perceptualdata.
The first question that we ask, thus, is whetherthe more ?grounded?
image-based models can helpus in interpreting conceptual representations in thebrain.
More specifically, we will compare the per-formance of different image-based representations,and we will test whether text- and image-based rep-resentations are complementary, so that when usedtogether they can better account for patterns in neu-ral data.
Finally, we will check for differences be-tween anatomical regions in the degree to which textand/or image models are effective, as one might ex-pect given the well-known functional specializationsof different anatomical regions.2 Brain dataWe use the data that were recorded and preprocessedby Mitchell et al(2008), available for download intheir supporting online material.1 Full details of theexperimental protocol, data acquisition and prepro-cessing can be found in Mitchell et al(2008) andthe supporting material.
Key points are that therewere nine right-handed adult participants (5 female,age between 18 and 32).
The experimental task wasto actively think about the properties of sixty objectsthat were presented visually, each as a line drawingin combination with a text label.
The entire set ofobjects was presented in a random order in six ses-sions, each object remained on screen for 3 secondswith a seven second fixation gap between presenta-1http://www.cs.cmu.edu/?tom/science2008/tions.Mitchell and colleagues examined 12 categories,five objects per category, for a total of 60 concepts(words).
Due to coverage limitations, we use 51/60words representing 11/12 categories.
Table 1 con-tains the full list of 51 words organized by category.fMRI acquisition and preprocessing Mitchell etal.
(2008) acquired functional images on a SiemensAllegra 3.0T scanner using a gradient echo EPIpulse sequence with TR=1000 ms, TE=30 ms anda 60?
angle.
Seventeen 5-mm thick oblique-axialslices were imaged with a gap of 1-mm betweenslices.
The acquisition matrix was 64?64 with3.125?3.125?5-mm voxels.
They subsequentlycorrected data for slice timing, motion, linear trend,and performed temporal smoothing with a high-passfilter at 190s cutoff.
The data were normalized tothe MNI template brain image, spatially normalizedinto MNI space and resampled to 3?3?6 mm3 vox-els.
The voxel-wise percent signal change relative tothe fixation condition was computed for each objectpresentation.
The mean of the four images acquired4s post stimulus presentation was used for analysis.To create a single representation per object perparticipant, we took the voxel-wise mean of the sixpresentations of each word.
Likewise to create a sin-gle representation per category per participant, wetook the voxel-wise mean of all word models percategory, per participant.Anatomical parcellation Analysis was conductedon the whole brain, and to address the question ofwhether there are differences in models?
effective-ness between anatomical regions, brains were fur-ther partitioned into frontal, parietal, temporal andoccipital lobes.
This partitioning is coarse (each lobeis large and serves many diverse functions), but, foran initial test, appropriate, given that each lobe hasspecialisms that on face value are amenable to inter-pretation by our different distributional models andthe exact nature of specialist processing in localisedareas is often subject to debate (so being overly re-strictive may be risky).
Formulation of the distribu-tional models is described in detail in the Section 3,but for now it is sufficient to know that the Objectmodel is derived from image statistics of the objectdepicted in images, Context from image statistics ofthe background scene, Object&Context is a com-1961Animals Bear, Cat, Cow, Dog HorseBuilding Apartment, Barn, Church, HouseBuilding parts Arch, Chimney, Closet, Door, WindowClothing Coat, Dress, Pants, Shirt, SkirtFurniture Bed, Chair, Desk, Dresser, TableInsect Ant, Bee, Beetle, Butterfly, FlyKitchen utensils Bottle, Cup, Glass, Knife, SpoonMan made objects Bell, Key, Refrigerator, Telephone, WatchTool Chisel, Hammer, ScrewdriverVegetable Celery, Corn, Lettuce, TomatoVehicle Airplane, Bicycle, Car, Train, TruckTable 1: The 51 words represented by the brain and the distributional models, organized by category.bination of the two, and Window2 is a text-basedmodel.The occipital lobe houses the primary visual pro-cessing system and consequently it is reasonableto expect some bias toward image-based semanticmodels.
Furthermore, given that experimental stim-uli incorporated line drawings of the object,and thevisual cortex has a well-established role in process-ing low-level visual statistics including edge detec-tion (Bruce et al 2003), we naturally expected agood performance from Object (formulated fromedge orientation histograms of similar objects).Following Goodale and Milner (1992)?s influ-ential perception-action model (see McIntosh andSchenk (2009) for recent discussion), visual infor-mation is channeled from the occipital lobe in twostreams: a perceptual stream, serving object identi-fication and recognition; and an action stream, spe-cialist in processing egocentric spatial relationshipsand ultimately supporting interaction with the world.The perceptual stream leads to the temporal lobe.Here the fusiform gyrus (shared with the occipitallobe) plays a general role in object categorisation(e.g., animals and tools (Chao et al 1999), faces(Kanwisher and Yovel, 2006), body parts (Peelenand Downing, 2005) and even word form percep-tion (McCandliss et al 2003)).
As the parahip-pocampus is strongly associated with scene repre-sentation (Epstein, 2008), we expect both the Objectand Context models to capture variability in the tem-poral lobe.
Of wider relevance to semantic process-ing, the medial temporal gyrus, inferior temporalgyrus and ventral temporal lobe have generally beenimplicated to have roles in supramodal integrationand concept retrieval (Binder et al 2009).
Giventhis, we expected that incorporating text would alsobe valuable and that the Window2&Object&Contextcombination would be a good model.The visual action stream leads from the occipi-tal lobe to the parietal lobe to support spatial cog-nition tasks and action control (Sack, 2009).
Inthat there seems to be an egocentric frame of ref-erence, placing actor in environment, it is tempt-ing to speculate that the Context model is more ap-propriate than the Object model here.
As the pari-etal lobe also contains the angular gyrus, thoughtto be involved in complex, supra-modal informationintegration and knowledge retrieval (Binder et al2009), we might again forecast that integrating textand image information would boost performance, soWindow2&Context was earmarked as a strong can-didate.The frontal lobe, is traditionally associated withhigh-level processing and manipulation of abstractknowledge and rules and controlled behaviour(Miller et al 2002).
Regarding semantics, the dor-somedial prefrontal cortex has been implicated inself-guided retrieval of semantic information (e.g.,uncued speech production), the ventromedial pre-frontal cortex in motivation and emotional process-ing, the inferior frontal gyrus in phonological andsyntactic processing, (Binder et al 2009) and in-tegration of lexical information (Hagoort, 2005).Given the association with linguistic processing weanticipated a bias in favour of Window2.The four lobes were identified and partitionedusing Tzourio-Mazoyer et al(2002)?s automaticanatomical labelling scheme.1962Voxel selection The set of 500 most stable voxels,both within the whole brain and from within eachregion of interest were identified for analysis.
Themost stable voxels were those showing consistentvariation across the different stimuli between scan-ning sessions.
Specifically, and following a similarstrategy to Mitchell et al(2008), for each voxel, theset of 51 words from each unique pair of scanningsessions were correlated using Pearson?s correlation(6 sessions and therefore 15 unique pairs), and themean of the 15 resulting correlation coefficients wastaken as the measure of stability.
The 500 voxelswith highest mean correlations were selected.3 Distributional modelsDistributional semantic models approximate wordmeaning by keeping track of word co-occurrencestatistics from large textual input, relying on the dis-tributional hypothesis: The meaning of a word canbe induced by the context in which it occurs (Turneyand Pantel, 2010).
Despite their great success, thesemodels still rely on verbal input only, while humansbase their meaning representation also on perceptualinformation (Louwerse, 2011).Thanks to recent developments in computer vi-sion, it is nowadays possible to take the visual per-ceptual channel into account, and build new com-putational models of semantics enhanced with vi-sual information (Feng and Lapata, 2010; Bruni etal., 2011; Leong and Mihalcea, 2011; Bergsma andGoebel, 2011; Bruni et al 2012a).
Given a set oftarget concepts and a collection of images depictingthose concepts, it is indeed possible to first encodethe image content into low-level features, and subse-quently convert it into a higher-level representationbased on the bag-of-visual-words method (Graumanand Leibe, 2011).
Recently, Bruni et al(2012b)have shown that better semantic representations canbe extracted if we first localize the concept in theimage, and then extract distinct higher-level features(visual words) from the box containing the conceptand from the surrounding context.
We also followthis strategy here.In our experiments we utilize both traditional text-based models and experimental image-based mod-els, as well as their combination.3.1 Textual modelsVerb We experiment with the original text-basedsemantic model used to predict fMRI patterns byMitchell et al(2008).
Each object stimulus wordis represented as a 25-dimensional vector, with eachvalue corresponding to the normalized sentence-wide co-occurrence of that word with one of 25manually-picked sensorimotor verbs (such as see,hear, eat, .
.
. )
in a trillion word text corpus.Window2 To create this model, we collect textco-occurrence statistics from the freely availableukWaC and Wackypedia corpora combined (about 3billion words in total).2 As collocates of our distri-butional model we select a set of 30K words, namelythe top 20K most frequent nouns, 5K most frequentadjectives and 5K most frequent verbs.In the tradition of HAL (Lund and Burgess, 1996),the model is based on co-occurrence statistics withcollocates within a fixed-size window of 2 to the leftand right of each target word.
Despite their sim-plicity, narrow-window-based models have shownto achieve state-of-the-art results in various stan-dard semantic tasks (Bullinaria and Levy, 2007)and to outperform both document-based and syntax-based models trained on the same corpus (Bruni etal., 2012a).
Moreover, in Murphy et al(2012) awindow-based model very similar to ours was notsignificantly worse than their best model for braindecoding.
We tried also a few variations, e.g., us-ing a larger window or different transformations onthe raw co-occurrences from those presented below,but with little, insignificant changes in performance.Given that our focus here is on visual information,we only report results for Window2 and its combi-nation with visual models.3.2 Visual modelsOur visual models are inspired by Bruni et al(2012b), that have explored to what extent extract-ing features from images where objects are local-ized results in better semantic representations.
Theyfound that extracting visual features separately fromthe object and its surrounding context leads to bet-ter performance than not using localization, and us-ing only object- and, more surprisingly, context-extracted features also results in performant models2http://wacky.sslmit.unibo.it/1963(especially when evaluating inter-object similarity,the context in which an object is located can signif-icantly contribute to semantic representation, in cer-tain cases carrying even more information than thedepicted object itself).More in detail, with localization the visual fea-tures (visual words) can be extracted from the ob-ject bounding box (in our experiments, the Objectmodel) or from only outside the object box (theContext model).
A combined model is obtainedby concatenating the two feature vectors (the Ob-ject&Context model).Visual model construction pipeline To extractvisual co-occurrence statistics, we use images fromImageNet (Deng et al 2009),3 a very large im-age database organized on top of the WordNet hi-erarchy (Fellbaum, 1998).
ImageNet has more than14 million images, covering 21K WordNet nominalsynsets.
ImageNet stands out for the high quality ofits images, both in terms of resolution and conceptannotations.
Moreover, for around 3K concepts, an-notations of object bounding boxes is provided.
Thislast feature allows us to exploit object localizationwithin our experiments.To build visual distributional models, we utilizethe bag-of-visual-words (BoVW) representation ofimages (Sivic and Zisserman, 2003; Csurka et al2004).
Inspired by NLP, BoVW discretizes the im-age content in terms of a histogram of visual wordcounts.
Differently from NLP, in vision there is not anatural notion of visual words, hence a visual vocab-ulary has to be built from scratch.
The process worksas follows.
First, a large set of low-level features isextracted from a corpus of images.
The low-levelfeature vectors are subsequently clustered into dif-ferent regions (visual words).
Given then a new im-age, each of the low-level feature vectors extractedfrom the patches that compose it is mapped to thenearest visual word (e.g., in terms of Euclidean dis-tance from the cluster centroid) such that the imagecan be represented with a histogram counting the in-stances of each visual word in the image.As low-level features we use SIFT, the Scale In-variant Feature Transform (Lowe, 2004).
SIFT fea-tures are good at capturing parts of objects and aredesigned to be invariant to image transformations3http://www.image-net.org/such as change in scale, rotation and illumination.To construct the visual vocabulary, we cluster theSIFT features into 25K different clusters.4 We addalso spatial information by dividing the image intoseveral subregions, representing each of them interms of BoVW and then stacking the resulting his-tograms (Lazebnik et al 2006).
We use in total 8different regions, obtaining a final vector of 200Kdimensions (25K visual words ?
8 regions).
Sinceeach concept in our dataset is represented by mul-tiple images, we pool the visual word occurrencesacross images by summing them up into a singlevector.To perform the entire visual pipeline we useVSEM, an open library for visual semantics (Bruniet al 2013).53.3 Model transformations and combinationOnce both the textual and the visual models are built,we perform two different transformations on the rawco-occurrence counts.
First, we transform them intononnegative Pointwise Mutual Information (PMI)association scores (Church and Hanks, 1990).
As asecond transformation, we apply dimensionality re-duction to the two matrices.
In particular, we adoptthe Singular Value Decomposition (SVD), one of themost effective methods to approximate the originaldata in lower dimensionality space (Schu?tze, 1997),and reduce the vectors to 50 dimensions.To combine text- and image-based semantic mod-els in a joint representation, we separately normalizetheir vectors to unit length, and concatenate them,along the lines of Bruni et al(2011).
More sophis-ticated combination models have been proposed inthe recent literature on multimodal semantics.
Forexample, Bruni et al(2012a) use SVD as a mix-ing strategy, given its ability to smooth the matricesand uncover latent dimensions.
Another example isSilberer and Lapata (2013), where Canonical Corre-lation Analysis is used.
We reserve the explorationof more advanced combination methods for furtherstudies.Finally, to represent the 11 categories we experi-ment with (see Table 1), we average the vectors ofthe concepts they include.4We use k-means, the most commonly employed clusteringalgorithm for this task.5http://clic.cimec.unitn.it/vsem/19644 ExperimentsA question is posed over how to evaluate the rela-tionship between the different distributional modelsand brain data.
Comparing each model?s predictiveperformance using the same strategy as Mitchell etal.
(2008) (also followed by Murphy et al(2012))is one possibility: they used multiple regression torelate distributional codes to individual voxel activa-tions, thus allowing brain states to be estimated frompreviously unseen distributional codes.
Regressionmodels were trained on 58/60 words and in testingthe regression models estimated the brain state as-sociated with the 2 unseen distributional codes.
Thepredicted brain states were compared with the actualfMRI data, and the process repeated for each per-mutation of left-out words, to build a metric of pre-diction accuracy.
For our purposes, a fair compari-son of models using this strategy is complicated bydifferences in dimensionality between both seman-tic models and lobes (which we compare to otherlobes) in association with the comparatively smallnumber of words in the fMRI data set.
Large dimen-sionality models risk overfitting the data, and it is anuisance to try to reliably correct for the effects ofoverfitting in performance comparisons.
Not least,to thoroughly evaluate all possible cross-validationpermutations is demanding in processing time, andwe have many models to compare.An alternative approach, and that which wehave adopted, is representational similarity analy-sis (Kriegeskorte et al 2008).
Representationalsimilarity analysis circumvents the previous prob-lems by abstracting each fMRI/distributional datasource to a common structure capturing the inter-relationships between each pair of data items (e.g.,words).
Specifically, for each model/participant?sfMRI data/anatomical region, the similarity struc-ture was evaluated by taking the pairwise correla-tion (Pearson?s correlation coefficient) between allunique category or word combinations.
This pro-duced a list of 55 category pair correlations and 121word pair correlations for each data source.
For allbrain data, correlation lists were averaged across thenine participants to produce a single list of meanword pair correlations and a single list of mean cat-egory pair correlations for each anatomical regionand the whole brain.
Then to provide a measure ofsimilarity between models and brain data, the cor-relation lists for respective data sources were them-selves correlated using Spearman?s rank correlation.Statistical significance was tested using a permuta-tion test: The word-pair (or category-pair) labelswere randomly shuffled 10,000 times to estimate anull distribution when the two similarity lists arenot correlated.
The p-value is calculated as the pro-portion of random correlation coefficients that aregreater than or equal to the observed coefficient.5 Results5.1 Category-level analysesDo image models correlate with brain data?
Ta-ble 2 displays results of Spearman?s correlations be-tween the per-category similarity structure of dis-tributional models and brain data.
There is a sig-nificant correlation between every purely image-based model and the occipital, parietal and tempo-ral lobes, and also the whole brain (.38?
?
?.51,all p?.01).
The frontal lobe is less well described.Still, whilst not significant, correlations are onlymarginally above the conventional p = .05 cutoff(all are less than p = .064).
This strongly suggeststhat the answer to our first question is yes: distri-butional models derived from images can be usedto explain concept fMRI data.
Otherwise Window2significantly correlates with the whole brain and allanatomical regions except for the frontal lobe where?=.34, p = .07.
In contrast Verb (the original, par-tially hand-crafted model used by Mitchell and col-leagues) captures inter-relationships poorly and nei-ther correlates with the whole brain or any lobe.Do different models correlate with differentanatomical regions?
2-way ANOVA withoutreplication was used to test for differences in cor-relation coefficients between the five pure-modalitymodels (Verb, Window2, Object, Context and Ob-ject&Context), and the four brain lobes.
This re-vealed a highly significant difference between mod-els F(4,12)=45.2, p<.001.
Post-hoc 2-tailed t-testscomparing model pairs found that Verb differed sig-nificantly from all other models (correlations werelower).
There was a clear difference even when Verb(mean?sd over lobes = .1?.1) was compared to thesecond weakest model, Object (mean?sd=.4?.09),where t =-7.7, p <.01, df=4.
There were no1965Frontal Parietal Occipital Temporal Whole-BrainVerb 0.00 (0.51) 0.06 (0.37) 0.24 (0.10) 0.07 (0.35) 0.17 (0.17)Window2 0.34 (0.06) 0.49 (0.00) 0.47 (0.01) 0.47 (0.00) 0.44 (0.00)Object 0.27 (0.07) 0.38 (0.02) 0.45 (0.00) 0.47 (0.00) 0.43 (0.01)Context 0.33 (0.06) 0.50 (0.00) 0.44 (0.00) 0.44 (0.01) 0.44 (0.01)Object&Context 0.32 (0.05) 0.48 (0.00) 0.51 (0.00) 0.49 (0.00) 0.49 (0.00)Window2&Object 0.32 (0.06) 0.45 (0.00) 0.52 (0.00) 0.53 (0.00) 0.49 (0.00)Window2&Context 0.39 (0.04) 0.57 (0.00) 0.53 (0.00) 0.55 (0.00) 0.51 (0.00)Window2&Object&Context 0.37 (0.04) 0.52 (0.00) 0.55 (0.00) 0.55 (0.00) 0.53 (0.00)Table 2: Matrix of correlations between each pairwise combination of distributional semantic models and brain data.Correlations correspond to the pairwise similarity between the 11 categories.
In each column the first value corre-sponds to Spearman?s rank correlation coefficient and the value in parenthesis is the p-value.Frontal Parietal Occipital Temporal Whole-BrainVerb -0.04 (0.72) 0.09 (0.06) 0.07 (0.20) 0.03 (0.31) 0.07 (0.18)Window2 0.07 (0.13) 0.19 (0.00) 0.12 (0.06) 0.21 (0.00) 0.13 (0.04)Object 0.01 (0.40) 0.08 (0.07) 0.17 (0.01) 0.18 (0.00) 0.17 (0.01)Context 0.04 (0.24) 0.14 (0.01) 0.01 (0.44) 0.12 (0.02) 0.02 (0.38)Object&Context 0.03 (0.31) 0.13 (0.01) 0.10 (0.07) 0.17 (0.00) 0.11 (0.06)Window2&Object 0.04 (0.24) 0.16 (0.00) 0.16 (0.01) 0.23 (0.00) 0.17 (0.00)Window2&Context 0.07 (0.12) 0.20 (0.00) 0.09 (0.11) 0.22 (0.00) 0.11 (0.07)Window2&Object&Context 0.05 (0.18) 0.18 (0.00) 0.12 (0.05) 0.23 (0.00) 0.13 (0.02)Table 3: Matrix of correlations between each pairwise combination of distributional semantic models and brain data.Correlations correspond to the pairwise similarity between the 51 words.
In each column the first value correspondsto Spearman?s rank correlation coefficient and the value in parenthesis is the p-value.other significant differences between models.
How-ever there was a highly significant difference be-tween lobes F(3,12)=13.77, p <.001.
Post-hoc 2-tailed t-tests comparing lobe pairs found that thefrontal lobe yielded significantly different correla-tions (lower) than each other lobe.
When the frontallobe (mean?sd over models = .25?.14) was com-pared to the second weakest anatomical region, theparietal lobe (mean?sd=.38?.19), the differencewas highly significant, t =-8, df=3, p <.01.
Thisintroduces the question of whether this difference incorrelations is the result of differences in neural cat-egory organisation and representation, or differencesin the quality of the signal, which we address next.Category-level inter-correlations between lobeswere all relatively strong and highly significant.
Theoccipital lobe was found to be the most distinct, be-ing similar to the temporal lobe (?=.71, p <.001),but less so to the parietal and frontal lobes (?=.53,p <.001 and ?=.57, p <.001 respectively).
Thetemporal lobe shows roughly similar levels of cor-relation to each other lobe (all .71?
?
?.73, allp <.001).
The frontal and parietal lobes are relatedmost strongly to each other (?=.77, p <.001), to aslightly lesser extent to the temporal lobe (in bothcases ?=.73, p <.001) and least so to the occipitallobe.
These strong relationships are consistent withthere being a broadly similar category organisationacross lobes.To appraise this assertion in the context of thepreviously detected difference between the frontallobe and all other lobes, we examine the raw cat-egory pair similarity matrices derived from the oc-cipital lobe and the frontal lobe (Figure 1).
All thebelow observations are qualitative.
Although it isdifficult to have intuitions about the relative differ-ences between all category pairs (e.g., whether toolsor furniture should be more similar to animals), wemight reasonably expect some obvious similarities.For instance, for animals to be visually similar to in-1966sects and clothing, because all have legs and armsand curves (of course we would not expect a strongrelationship between insects and clothes in functionor other modalities such as sound), buildings to besimilar to building parts and vehicles (hard edgesand windows), building parts to be similar to furni-ture (e.g., from Table 1 we see there is some overlapin category membership between these categories,such as closet and door) and tools to be similar tokitchen utensils.
All of these relationships are main-tained in the occipital lobe, and many are visible inthe frontal lobe (including the similarity between in-sects and clothes), however there are exceptions thatare difficult to explain e.g., within the frontal lobe,building parts are not similar to furniture, kitchenutensils are closer to clothing than to tools and ve-hicles are more similar to clothing than anythingelse.
As such we conclude that category-level rep-resentations were similar across lobes with differ-ences likely due to variation in signal quality be-tween lobes.Are text- and image-based semantic models com-plementary?
Turning to the question of whethertext- and image-derived semantic information canbe complementary, we observe from Table 2 thatthere is not a single instance of a joint model witha weaker correlation than its pure-image counter-part.
The Window2 model showed a stronger cor-relation than the Window2&Object model for thefrontal and parietal lobes, but was weaker than Win-dow2&Object&Context and Window2&Context inall tests and was also weaker than any joint modelin whole-brain comparisons.
The mean?sd correla-tions for all purely image-based results pooled overlobes (3 models * 4 lobes) was .42?.08 in com-parison to .49?.08 for the joint models.
The rel-ative performance of Object vs.
Context vs. Ob-ject&Context on the four different lobes is preservedbetween image-based and joint models: correlatingthe 12 combinations using Spearman?s correlationgives ?=.85, p <.001.
Differences can be statis-tically quantified by pooling all image related cor-relation coefficients for each anatomical region (3models * 4 regions), as for the respective joint mod-els, and comparing with a 2-tailed Wilcoxon signedrank test.
Differences were highly significant (W=0,p <.001,n=12).
This evidence accumulates to sug-Figure 1: Similarity (Pearson correlation) between eachcategory pair in (top) occipital and (bottom) frontal lobes.gest that text and image-derived semantic informa-tion can be complementary in interpreting conceptfMRI data.5.2 Word-level analysesDo image models capture word pair similari-ties?
Per-word results generally corroborate therelationships observed in the previous section inthe sense that Spearman?s correlation between per-word and per-category results for the 40 combina-tions of models and lobes was ?=.78, p <.001.There were differences, most obviously a dramaticdrop in the strength of correlation coefficients forthe per-word results, visible in Table 3.
Subsets1967of per-word image-based models correlated withthree lobes and the whole brain.
Correlations corre-sponding to significance values of p <.05 were ob-served in the temporal and parietal lobes, for Con-text, Object&Context and Window2 whereas Ob-ject was correlated with the occipital and temporallobes (p <.05).
2-way ANOVA without replica-tion was used to test for differences between mod-els and lobes.
This revealed a significant differ-ence between models (F(4,12)=4.05, p=.027).
Post-hoc t-tests showed that the Window2 model signifi-cantly differed from (was stronger than) the Context(t=3.8, p =.03, df=3) and Object&Context models(t =4.5, p =.02, df=3).
There were no other signifi-cant differences between models.
There was again asignificant difference between lobes (F(3,12)=7.89,p < .01), with the frontal lobe showing the weak-est correlations.
Post-hoc 2-tailed t-tests comparinglobe-pairs found that the frontal lobe differed signif-icantly (correlations were weaker) from the parietal(t =-9, p <.001, df=4) and temporal lobes (t =-6.4,p <.01, df=4) but not from the occipital lobe (t =-2.18, p =.09, df=4).
No other significant differencesbetween lobes were observed.Are there differences between models/lobes?Word-level inter-correlations between lobes were allsignificant and the pattern of differences in correla-tion strength largely resembled that of the category-level analyses.
The occipital lobe was again mostsimilar to the temporal lobe (?=.57, p <.001), butless so to the parietal and frontal lobes (?=.47,p <.001 and ?=.34, p <.001 respectively).
Thetemporal lobe this time showed stronger correlationto the parietal (?=.68, p <.001) and frontal lobes(?=.61, p <.001) than the occipital lobe.
The frontaland parietal lobes were again strongly related to oneanother (?=.67, p <.001).
These results echo thecategory-level findings, that word-level brain activ-ity is also organised in a similar way across lobes.Consequently this diminishes our chances of uncov-ering neat interactions between models and brain ar-eas (where for instance the Window2 model corre-lates with the frontal lobe and Object model matchesthe occipital lobe).
It is however noteworthy thatwe can observe some interpretable selectivity inlobe*model combinations.
In particular the Con-text model better matches the parietal lobe than theObject model, which in turn better captures the oc-cipital and temporal lobes (Observations are quali-tative).
Also as we see next, adding text informa-tion boosts performance in both parietal and tempo-ral lobes (see Section 2 on our expectations aboutinformation encoded in the lobes).Does joining text and image models help word-level interpretation?
As concerns the benefits ofjoining Text and Image information, per-word jointmodels were generally stronger than the respectiveimage-based models.
There was one exception:adding text to the Object model weakened corre-lation with the occipital lobe.
Joint models wereexclusively stronger than Window2 for the tempo-ral and occipital lobes, and were stronger in 1/3 ofcases for the frontal and parietal lobes.
In an anal-ogous comparison to the per-category analysis, aWilcoxon signed rank test was used to examine thedifference made by adding text information to imagemodels (pooling 3 models over 4 anatomical areasfor both image and joint models).
The mean?sd ofimage models was .1?.06 whereas for Joint modelsit was .15?.07.
The difference was highly signifi-cant (W=1, p <.001, n=12).6 ConclusionThis study brought together, for the first time, tworecent research lines: The exploration of ?seman-tic spaces?
in the brain using distributional semanticmodels extracted from corpora, and the extensionof the latter to image-based features.
We showedthat image-based distributional semantic measuressignificantly correlate with fMRI-based neural sim-ilarity patterns pertaining to categories of concreteconcepts as well as concrete basic-level concepts ex-pressed by specific words (although correlations, es-pecially at the basic-concept level, are rather low,which might signify the need to develop still moreperformant distributional models and/or noise inher-ent to neural data).
Moreover, image-based mod-els complement a state-of-the-art text-based model,with the best performance achieved when the twomodalities are combined.
This not only presents anoptimistic outlook for the future use of image-basedmodels as an interpretative tool to explore issues ofcognitive grounding, but also demonstrates that theyare capturing useful additional aspects of meaning to1968the text models, which are likely relevant for com-putational semantic tasks.The weak comparative performance of the origi-nal Mitchell et als Verb model is perhaps surprisinggiven its previous success in prediction (Mitchell etal., 2008), but a useful reminder that a good predic-tor does not necessarily have to capture the internalstructure of the data it predicts.The lack of finding organisational differences be-tween anatomical regions differentially described bythe various models is perhaps disappointing, but notuncontroversial, given that the dataset was not origi-nally designed to tease apart visual information fromlinguistic context.
It is however interesting thatin the more challenging word-level analysis somemeaningful trend was visible.
In future experimentsit may prove valuable to configure a fMRI stimulusset where text-based and image-based interrelation-ships are maximally different.
Collecting our ownfMRI data will also allow us to move beyond ex-ploratory analysis, to test sharper predictions aboutdistributional models and their brain area correlates.There are also many opportunities for focusing anal-yses on different subsets of brain regions, with thesemantic system identified by Binder et al(2009) inparticular presenting one interesting avenue for in-vestigation.AcknowledgmentsThis research was partially funded by a Google Re-search Award to the fifth author.ReferencesShane Bergsma and Randy Goebel.
2011.
Using visualinformation to predict lexical preference.
In Proceed-ings of RANLP, pages 399?405, Hissar, Bulgaria.Jeffrey R. Binder, Rutvik H. Desai, William W. Graves,and Lisa L. Conant.
2009.
Where is the semanticsystem?
a critical review and meta-analysis of 120functional neuroimaging studies.
Cerebral Cortexl,12:2767?2796.Vicki Bruce, Patrick R Green, and Georgeson Mark A.2003.
Visual perception: Physiology, psychology, andecology.
Psychology Pr.Elia Bruni, Giang Binh Tran, and Marco Baroni.
2011.Distributional semantics from text and images.
In Pro-ceedings of the EMNLP GEMS Workshop, pages 22?32, Edinburgh, UK.Elia Bruni, Gemma Boleda, Marco Baroni, andNam Khanh Tran.
2012a.
Distributional semantics inTechnicolor.
In Proceedings of ACL, pages 136?145,Jeju Island, Korea.Elia Bruni, Jasper Uijlings, Marco Baroni, and NicuSebe.
2012b.
Distributional semantics with eyes: Us-ing image analysis to improve computational represen-tations of word meaning.
In Proceedings of ACM Mul-timedia, pages 1219?1228, Nara, Japan.Elia Bruni, Ulisse Bordignon, Adam Liska, Jasper Ui-jlings, and Irina Sergienya.
2013.
Vsem: An open li-brary for visual semantics representation.
In Proceed-ings of ACL, Sofia, Bulgaria.John Bullinaria and Joseph Levy.
2007.
Extract-ing semantic representations from word co-occurrencestatistics: A computational study.
Behavior ResearchMethods, 39:510?526.Kai-min Chang, Tom Mitchell, and Marcel Just.
2011.Quantitative modeling of the neural representation ofobjects: How semantic feature norms can account forfMRI activation.
NeuroImage, 56:716?727.Linda L Chao, James V Haxby, and Alex Martin.
1999.Attribute-based neural substrates in temporal cortexfor perceiving and knowing about objects.
Nature neu-roscience, 2(10):913?919.Kenneth Church and Peter Hanks.
1990.
Word asso-ciation norms, mutual information, and lexicography.Computational Linguistics, 16(1):22?29.Stephen Clark.
2012.
Vector space models of lexicalmeaning.
In Shalom Lappin and Chris Fox, editors,Handbook of Contemporary Semantics, 2nd edition.Blackwell, Malden, MA.
In press.Gabriella Csurka, Christopher Dance, Lixin Fan, JuttaWillamowski, and Ce?dric Bray.
2004.
Visual cate-gorization with bags of keypoints.
In In Workshop onStatistical Learning in Computer Vision, ECCV, pages1?22, Prague, Czech Republic.Jia Deng, Wei Dong, Richard Socher, Lia-Ji Li, andLi Fei-Fei.
2009.
Imagenet: A large-scale hierarchi-cal image database.
In Proceedings of CVPR, pages248?255, Miami Beach, FL.Russell A Epstein.
2008.
Parahippocampal and ret-rosplenial contributions to human spatial navigation.Trends in cognitive sciences, 12(10):388?396.Katrin Erk.
2012.
Vector space models of word meaningand phrase meaning: A survey.
Language and Lin-guistics Compass, 6(10):635?653.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge, MA.Yansong Feng and Mirella Lapata.
2010.
Visual infor-mation in semantic representation.
In Proceedings ofHLT-NAACL, pages 91?99, Los Angeles, CA.1969Melvyn A. Goodale and David Milner.
1992.
Separatevisual pathways for perception and action.
Trends inNeurosciences, 15:20?25.Kristen Grauman and Bastian Leibe.
2011.
Visual ObjectRecognition.
Morgan & Claypool, San Francisco.Peter Hagoort.
2005.
On Broca, brain, and bind-ing: a new framework.
Trends in cognitive sciences,9(9):416?423.James Haxby, Ida Gobbini, Maura Furey, Alumit Ishai,Jennifer Schouten, and Pietro Pietrini.
2001.
Dis-tributed and overlapping representations of faces andobjects in ventral temporal cortex.
Science, 293:2425?2430.Alexander Huth, Shinji Nishimoto, An Vu, and Jack Gal-lant.
2012.
A continuous semantic space describes therepresentation of thousands of object and action cate-gories across the human brain.
Neuron, 76(6):1210?1224.Nancy Kanwisher and Galit Yovel.
2006.
The fusiformface area: a cortical region specialized for the percep-tion of faces.
Philosophical Transactions of the RoyalSociety B: Biological Sciences, 361(1476):2109?2128.Nikolaus Kriegeskorte, Marieke Mur, and Peter Ban-dettini.
2008.
Representational similarity analysis?connecting the branches of systems neuroscience.Frontiers in systems neuroscience, 2.Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.2006.
Beyond bags of features: Spatial pyramidmatching for recognizing natural scene categories.
InProceedings of CVPR, pages 2169?2178, Washington,DC.Chee Wee Leong and Rada Mihalcea.
2011.
Goingbeyond text: A hybrid image-text approach for mea-suring word relatedness.
In Proceedings of IJCNLP,pages 1403?1407.Max Louwerse.
2011.
Symbol interdependency in sym-bolic and embodied cognition.
Topics in CognitiveScience, 3:273?302.David G Lowe.
2004.
Distinctive Image Features fromScale-Invariant Keypoints.
International Journal ofComputer Vision, 60:91?110.Kevin Lund and Curt Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, 28:203?208.Bruce D McCandliss, Laurent Cohen, and Stanislas De-haene.
2003.
The visual word form area: expertisefor reading in the fusiform gyrus.
Trends in cognitivesciences, 7(7):293?299.Robert D McIntosh and Thomas Schenk.
2009.
Two vi-sual streams for perception and action: Current trends.Neuropsychologia, 47(6):1391?1396.Earl K Miller, David J Freedman, and Jonathan D Wal-lis.
2002.
The prefrontal cortex: categories, con-cepts and cognition.
Philosophical Transactions ofthe Royal Society of London.
Series B: Biological Sci-ences, 357(1424):1123?1136.Tom Mitchell, Svetlana Shinkareva, Andrew Carlson,Kai-Min Chang, Vincente Malave, Robert Mason, andMarcel Just.
2008.
Predicting human brain activ-ity associated with the meanings of nouns.
Science,320:1191?1195.Brian Murphy, Partha Talukdar, and Tom Mitchell.
2012.Selecting corpus-semantic models for neurolinguisticdecoding.
In Proceedings of *SEM, pages 114?123,Montreal, Canada.Mark Palatucci, Dean Pomerleau, Geoffrey Hinton, andTom Mitchell.
2009.
Zero-shot learning with seman-tic output codes.
In Proceedings of NIPS, pages 1410?1418, Vancouver, Canada.Marius V Peelen and Paul E Downing.
2005.
Selectivityfor the human body in the fusiform gyrus.
Journal ofNeurophysiology, 93(1):603?608.Francisco Pereira, Greg Detre, and Matthew Botvinick.2011.
Generating text from functional brain images.Frontiers in Human Neuroscience, 5(72).
Publishedonline: http://www.frontiersin.org/human_neuroscience/10.3389/fnhum.2011.00072/abstract.Alexander T Sack.
2009.
Parietal cortex and spatial cog-nition.
Behavioural brain research, 202(2):153?161.Hinrich Schu?tze.
1997.
Ambiguity Resolution in NaturalLanguage Learning.
CSLI, Stanford, CA.Carina Silberer and Mirella Lapata.
2013.
Models ofsemantic representation with visual attributes.
In Pro-ceedings of ACL, Sofia, Bulgaria.Josef Sivic and Andrew Zisserman.
2003.
Video Google:A text retrieval approach to object matching in videos.In Proceedings of ICCV, pages 1470?1477, Nice,France.Peter Turney and Patrick Pantel.
2010.
From frequencyto meaning: Vector space models of semantics.
Jour-nal of Artificial Intelligence Research, 37:141?188.N Tzourio-Mazoyer, B Landeau, D Papathanassiou,F Crivello, O Etard, N Delcroix, B Mazoyer, and M Jo-liot.
2002.
Automated anatomical labeling of activa-tions in SPM using a macroscopic anatomical parcel-lation of the MNI MRI single-subject brain.
Neuroim-age, 15(1):273?289.1970
