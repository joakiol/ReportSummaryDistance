Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 443?450, Vancouver, October 2005. c?2005 Association for Computational LinguisticsExtracting Personal Names from Email: Applying Named EntityRecognition to Informal TextEinat Minkov and Richard C. WangLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15217{einat,rcwang}@cs.cmu.eduWilliam W. CohenCtr for Automated Learning & DiscoveryCarnegie Mellon UniversityPittsburgh, PA 15217wcohen@cs.cmu.eduAbstractThere has been little prior work on NamedEntity Recognition for ?informal?
docu-ments like email.
We present two meth-ods for improving performance of per-son name recognizers for email: email-specific structural features and a recall-enhancing method which exploits namerepetition across multiple documents.1 IntroductionNamed entity recognition (NER), the identificationof entity names in free text, is a well-studied prob-lem.
In most previous work, NER has been appliedto news articles (e.g., (Bikel et al, 1999; McCal-lum and Li, 2003)), scientific articles (e.g., (Cravenand Kumlien, 1999; Bunescu and Mooney, 2004)),or web pages (e.g., (Freitag, 1998)).
These genres oftext share two important properties: documents arewritten for a fairly broad audience, and writers takecare in preparing documents.
Important genres thatdo not share these properties include instant messag-ing logs, newsgroup postings and email messages.We refer to these genres as ?informal?
text.Informal text is harder to process automatically.Informal documents do not obey strict grammaticalconventions.
They contain grammatical and spellingerrors.
Further, since the audience is more restricted,informal documents often use group- and task-specific abbreviations and are not self-contained.Because of these differences, existing NER methodsmay require modifications to perform well on infor-mal text.In this paper, we investigate NER for informaltext with an experimental study of the problem ofrecognizing personal names in email?a task that isboth useful and non-trivial.
An application of in-terest is corpus anonymization.
Automatic or semi-automatic email anonymization should allow usinglarge amounts of informal text for research purposes,for example, of medical files.
Person-name extrac-tion and other NER tasks are helpful for automaticprocessing of informal text for a large variety of ap-plications (Culotta et al, 2004; Cohen et al, 2005).We first present four corpora of email text, anno-tated with personal names, each roughly compara-ble in size to the MUC-6 corpus1.
We experimen-tally evaluate the performance of conditional ran-dom fields (CRF) (Lafferty et al, 2001), a state-of-the art machine-learning based NER methods onthese corpora.
We then turn to examine the specialattributes of email text (vs. newswire) and suggestvenues for improving extraction performance.
Oneimportant observation is that email messages ofteninclude some structured, easy-to-recognize names,such as names within a header, names appearing inautomatically-generated phrases, as well as names insignature files or sign-offs.
We therefore suggest aset of specialized structural features for email; thesefeatures are shown to significantly improve perfor-mance on our corpora.We also present and evaluate a novel method forexploiting repetition of names in a test corpus.
Tech-niques for exploiting name repetition within docu-ments have been recently applied to newswire text1Two of these are publicly available.
The others can not bedistributed due to privacy considerations.443(e.g., (Humphreys et al, 1998)), scientific abstracts(e.g., (Bunescu and Mooney, 2004)) and seminar an-nouncements (Sutton and Mccallum, 2004); how-ever, these techniques rely on either NP analysis orcapitalization information to pre-identify candidatecoreferent name mentions, features which are not re-liable in email.
Furthermore, we argue that namerepetition in email should be inferred by examiningmultiple documents in a corpus, which is not com-mon practice.
We therefore present an alternativeefficient scheme for increasing recall in email, us-ing the whole corpus.
This technique is shown toalways improve recall substantially, and to almostalways improve F1 performance.2 CorporaTwo email corpora used in our experiments wereextracted from the CSpace email corpus (Kraut etal., 2004), which contains email messages collectedfrom a management course conducted at CarnegieMellon University in 1997.
In this course, MBA stu-dents, organized in teams of four to six members,ran simulated companies in different market scenar-ios.
We believe this corpus to be quite similar tothe work-oriented mail of employees of a small ormedium-sized company.
This text corpus containsthree header fields: ?From?, ?Subject?, and ?Time?.Mgmt-Game is a subcorpora consisting of all emailswritten over a five-day period.
In the experiments,the first day worth of email was used as a trainingset, the fourth for tuning and the fifth day as a testset.
Mgmt-Teams forms another split of this data,where the training set contains messages betweendifferent teams than in the test set; hence in Mgmt-Teams, the person names appearing in the test setare generally different than those that appear in thetraining set.The next two collections of email were extractedfrom the Enron corpus (Klimt and Yang, 2004).
Thefirst subset, Enron-Meetings, consists of messages infolders named ?meetings?
or ?calendar?2 .
Most butnot all of these messages are meeting-related.
Thesecond subset, Enron-Random, was formed by re-peatedly sampling a user name (uniformly at randomamong 158 users), and then sampling an email from2with two exceptions: (a) six very large files were removed,and (b) one very large ?calendar?
folder was excluded.that user (uniformly at random).Annotators were instructed to include nicknamesand misspelled names, but exclude person namesthat are part of an email address and names that arepart of a larger entity name like an organization orlocation (e.g., ?David Tepper School of Business?
).The sizes of the corpora are given in Table 1.
Welimited training size to be relatively small, reflectinga real-world scenario.Corpus # Documents #Words #NamesTrain Tune Test x1000Mgmt-Teams 120 82 83 105 2,792Mgmt-Game 120 216 264 140 2,993Enron-Meetings 244 242 247 204 2,868Enron-Random 89 82 83 286 5,059Table 1: Summary of the corpora used in the experiments.The number of words and names refer to the whole annotatedcorpora.3 Existing NER MethodsIn our first set of experiments we apply CRF, amachine-learning based probabilistic approach to la-beling sequences of examples, and evaluate it on theproblem of extracting personal names from email.Learning reduces NER to the task of tagging (i.e.,classifying) each word in a document.
We use a setof five tags, corresponding to (1) a one-token entity,(2) the first token of a multi-token entity, (3) the lasttoken of a multi-token entity, (4) any other token ofa multi-token entity and (5) a token that is not partof an entity.The sets of features used are presented in Table2.
All features are instantiated for the focus word, aswell as for a window of 3 tokens to the left and to theright of the focus word.
The basic features includethe lower-case value of a token t, and its capital-ization pattern, constructed by replacing all capitalletters with the letter ?X?, all lower-case letters with?x?, all digits with ?9?
and compressing runs of thesame letter with a single letter.
The dictionary fea-tures define various categories of words includingcommon words, first names, last names 3 and ?rosternames?
4 (international names list, where first and3We used US Census?
lists of the most com-mon first and last names in the US, available fromhttp://www.census.gov/genealogy/www/freqnames.html4A dictionary of 16,623 student names across the country,obtained as part of the RosterFinder project (Sweeney, 2003)444Basic Featurest, lexical value, lowercase (binary form, e.g.
f(t=?hello?
)=1)capitalization pattern of t (binary form, e.g.
f(t.cap=x+)=1)Dictionary FeaturesinCommon: t in common words dictionaryinFirst: t in first names dictionaryinLast: t in last names dictionaryinRoster: t in roster names dictionaryFirst: inFirst ?
?isLast ?
?inCommonLast: ?inFirst ?
inLast ?
?inCommonName: (First ?
Last ?
inRoster) ?
?
inCommonTitle: t in a personal prefixes/suffixes dictionaryOrg: t in organization suffixes dictionaryLoc: t in location suffixes dictionaryEmail Featurest appears in the headert appears in the ?from?
fieldt is a probable ?signoff?(?
after two line breaks and near end of message)t is part of an email address (regular expression)does the word starts a new sentence(?
capitalized after a period, question or exclamation mark)t is a probable initial (X or X.
)t followed by the bigram ?and I?t capitalized and followed by a pronoun within 15 tokensTable 2: Feature setslast names are mixed.)
In addition, we constructedsome composite dictionary features, as specified inTable 2: for example, a word that is in the first-namedictionary and is not in the common-words or last-name dictionaries is designated a ?sure first name?.The common-words dictionary used consists ofbase forms, conjugations and plural forms of com-mon English words, and a relatively small ad-hocdictionary representing words especially common inemail (e.g., ?email?, ?inbox?).
We also use smallmanually created word dictionaries of prefixes andsuffixes indicative of persons (e.g., ?mr?, ?jr?
), loca-tions (e.g., ?ave?)
and organizations (e.g., ?inc?
).Email structure features: We perform a simplifieddocument analysis of the email message and use thisto construct some additional features.
One is an in-dicator as to whether a token t is equal to some to-ken in the ?from?
field.
Another indicates whethera token t in the email body is equal to some tokenappearing in the whole header.
An indicator featurebased on a regular expression is used to mark tokensthat are part of a probable ?sign-off?
(i.e., a name atthe end of a message).
Finally, since the annotationrules do not consider email addresses to be names,we added an indicator feature for tokens that are in-side an email address.l.2.mr l.1.presidentl.2.mrs l.2.drl.1.jr r.2.whol.1.judge r.2.jrr.3.staff l.3.byl.2.ms r.3.presidentr.2.staff l.3.byr.1.family l.3.repl.3.says l.2.repr.3.reporter r.1.administrationl.1.by r.2.homel.2.by r.1.orl.3.name l.1.withl.2.name l.1.thanksl.3.by r.1.pickedr.3.his l.3.meetr.1.ps r.1.startedr.3.home r.1.toldr.1.and l.2.profl.1.called l.2.emailFigure 1: Predictive contexts for personal-name words forMUC-6 (left) and Mgmt-Game (right) corpora.
A features isdenoted by its direction comparing to the focus word (l/r), offsetand lexical value.We experimented with features derived from POStags and NP-chunking of the email, but found thePOS assignment too noisy to be useful.
We did in-clude some features based on approximate linguisticrules.
One rule looks for capitalized words that arenot common words and are followed by a pronounwithin a distance of up to 15 tokens.
(As an exam-ple, consider ?Contact Puck tomorrow.
He should bearound.?).
Another rule looks for words followed bythe bigram ?and I?.
As is common for hand-codedNER rules, both these rules have high precision andlow recall.3.1 Email vs NewswireIn order to explore some of the differences betweenemail and newswire NER problems, we stripped allheader fields from the Mgmt-Game messages, andtrained a model (using basic features only) from theresulting corpus of email bodies.
Figure 1 shows thefeatures most indicative of a token being part of aname in the models trained for the Mgmt-Game andMUC-6 corpora.
To make the list easier to interpret,it includes only the features corresponding to tokenssurrounding the focus word.As one might expect, the important features fromthe MUC-6 dataset are mainly formal name titlessuch as ?mr?, ?mrs?, and ?jr?, as well as job ti-tles and other pronominal modifiers such as ?pres-ident?
and ?judge?.
However, for the Mgmt-Gamecorpus, most of the important features are relatedto email-specific structure.
For example, the fea-tures ?left.1.by?
and ?left.2.by?
are often associatedwith a quoted excerpt from another email message,which in the Mgmt-Game corpus is often markedby mailers with text like ?Excerpts from mail: 7-445Sep-97 Re: paper deadline by Richard Wang?.
Sim-ilarly, features like ?left.1.thanks?
and ?right.1.ps?indicate a ?signoff?
section of an email, as does?right.2.home?
(which often indicates proximity toa home phone number appearing in a signature).3.2 Experimental ResultsWe now turn to evaluate the usefulness of the fea-ture sets described above.
Table 3 gives entity-levelF1 performance 5 for CRF trained models for alldatasets, using the basic features alone (B); the ba-sic and email-tailored features (B+E); the basic anddictionary features (B+D); and, all of the feature setscombined (B+D+E).
All feature sets were tuned us-ing the Mgmt-Game validation subset.
The givenresults relate to previously unseen test sets.Dataset B B+E B+D B+D+EMgmt-Teams 68.1 75.7 82.0 87.9Mgmt-Game 79.2 84.2 90.7 91.9Enron-Meetings 59.0 71.5 78.6 76.9Enron-Random 68.1 70.2 72.9 76.2Table 3: F1 entity-leavel performance for the sets of features,across all datasets, with CRF training.The results show that the email-specific featuresare very informative.
In addition, they show thatthe dictionary features are especially useful.
Thiscan be explained by the relatively weak contextualevidence in email.
While dictionaries are useful innamed entities extraction in general, they are in factmore essential when extracting names from emailtext, where many name mentions are part of headers,names lists etc.
Finally, the results for the combinedfeature set are superior in most cases to any subsetof the features.Overall the level of performance using all fea-tures is encouraging, considering the limited trainingset size.
Performance on Mgmt-Teams is somewhatlower than for Mgmt-Game mainly because (by de-sign) there is less similarity between training andtest sets with this split.
Enron emails seem to beharder than Mgmt-Game emails, perhaps becausethey include fewer structured instances of names.Enron-Meetings emails also contain a number ofconstructs that were not encountered in the Mgmt-Game corpus, notably lists (e.g., of people attendinga meeting), and also include many location and or-5No credit awarded for partially correct entity boundaries.01020304050607080901005  10  15  20  25  30  35Mgmt GameEnron-MeetingsEnron-RandomMUC-6Figure 2: Cumulative percentage of person-name tokens wthat appear in at most K distinct documents as a function of K.ganization names, which are rare in Mgmt-Game.
Alarger set of dictionaries might improve performancefor the Enron corpora.4 Repetition of named entities in emailIn the experiments described above, the extractorshave high precision, but relatively low recall.
Thistypical behavior suggests that some sort of recall-enhancing procedure might improve overall perfor-mance.One family of recall-enhancing techniques arebased on looking for multiple occurrences of namesin a document, so that names which occur in am-biguous contexts will be more likely to be recog-nized.
It is an intuitive assumption that the ways inwhich names repeat themselves in a corpus will bedifferent in email and newswire text.
In news stories,one would expect repetitions within a single docu-ment to be common, as a means for an author to es-tablish a shared context with the reader.
In an emailcorpus, one would expect names to repeat more fre-quently across the corpus, in multiple documents?at least when the email corpus is associated with agroup that works together closely.
In this section wesupport this conjecture with quantitative analysis.In a first experiment, we plotted the percentageof person-name tokens w that appear in at mostK distinct documents as a function of K. Figure2 shows this function for the Mgmt-Game, MUC-6, Enron-Meetings, and Enron-Random datasets.There is a large separation between MUC-6 andMgmt-Game, the most workgroup-oriented emailcorpus.
In MUC-6, for instance, almost 80% of the446Single-Document Repetition0%10%20%30%40%50%60%70%80%90%100%Mgmt Game MgmtTeamsEnronMeetingsEnronRandomMUC-6TokenRecallSDRCRFSDR+CRF(a) SDRMultiple-Document Repetition0%10%20%30%40%50%60%70%80%90%100%Mgmt Game MgmtTeamsEnronMeetingsEnronRandomMUC-6TokenRecallMDRCRFMDR+CRF(b) MDRFigure 3: Upper bounds on recall and recall improvementsassociated with methods that look for terms that re-occur withina single document (SDR) or across multiple documents (MDR).names appear in only a single document, while inMgmt-Game, only 30% of the names appear in onlya single document.
At the other extreme, in MUC-6,only 1.3% of the names appear in 10 or more docu-ments, while in Mgmt-Game, almost 20% do.
TheEnron-Random and Enron-Meetings datasets showdistributions of names that are intermediate betweenMgmt-Game and MUC-6.As a second experiment, we implemented twovery simple extraction rules.
The single documentrepetition (SDR) rule marks every token that oc-curs more than once inside a single document as aname.
Adding tokens marked by the SDR rule tothe tokens marked by the learned extractor generatesa new extractor, which we will denote SDR+CRF.Thus, the recall of SDR+CRF serves as an upperbound on the token recall6 of any recall-enhancing6Token level recall is recall on the task of classifying tokensas inside or outside an entity name.method that improves the extractor by exploitingrepetition within a single document.
Analogously,the multiple document repetition (MDR) rule marksevery token that occurs in more than one documentas a name.
Again, the token recall of MDR+CRFrule is an upper bound on the token recall of anyrecall-enhancing method that exploits token repeti-tion across multiple documents.The left bars in Figure 3 show the recall obtainedby the SDR (top) and the MDR rule (bottom).
TheMDR rule has highest recall for the two Mgmt cor-pora, and lowest recall for the MUC-6 corpus.
Con-versely, for the SDR rule, the highest recall levelobtained is for MUC-6.
The middle bars show thetoken recall obtained by the CRF extractor, usingall features.
The right bars show the token recallof the SDR+CRF and MDR+CRF extractors.
Com-paring them to the other bars, we see that the maxi-mal potential recall gain from a SDR-like method ison MUC-6.
For MDR-like methods, there are largepotential gains on the Mgmt corpora as well as onEnron-Meetings and Enron-Random to a lesser de-gree.
This probably reflects the fact that the Enroncorpora are from a larger and more weakly interact-ing set of users, compared to the Mgmt datasets.These results demonstrate the importance of ex-ploiting repetition of names across multiple docu-ments for entity extraction from email.5 Improving Recall With InferredDictionariesSequential learners of the sort used here classify to-kens from each document independently; moreover,the classification of a word w is independent of theclassification of other occurrences of w elsewhere inthe document.
That is, the fact that a word w has ap-peared somewhere in a context that clearly indicatesthat it is a name does not increase the probability thatit will be classified as a name in other, more ambigu-ous contexts.Recently, sequential learning methods have beenextended to directly utilize information about nameco-occurrence in learning the sequential classifier.This approach provides an elegant solution to mod-eling repetition within a single document.
However,it requires identifying candidate related entities inadvance, applying some heuristic.
Thus, Bunescu &447Mooney (2004) link between similar NPs (requiringtheir head to be identical), and Sutton and Mccallum(2004) connect pairs of identical capitalized words.Given that in email corpora capitalization patternsare not followed to a large extent, there is no ad-equate heuristic that would link candidate entitiesprior to extraction.
Further, it is not clear if a col-lective classification approach can scale to modelingmultiple-document repetition.We suggest an alternative approach of recall-enhancing name matching, which is appropriate foremail.
Our approach has points of similarity tothe methods described by Stevenson and Gaizauskas(2000), who suggest matching text against name dic-tionaries, filtering out names that are also commonwords or appear as non-names in high proportionin the training data.
The approach described hereis more systematic and general.
In a nutshell, wesuggest applying the noisy dictionary of predictednames over the test corpus, and use the approximate(predicted) name to non-name proportions over thetest set itself to filter out ambiguous names.
There-fore, our approach does not require large amount ofannotated training data.
It also does not require worddistribution to be similar between train and test data.We will now describe our approach in detail.5.1 Matching names from dictionaryFirst, we construct a dictionary comprised of allspans predicted as names by the learned model.
Forpersonal names, we suggest expanding this dictio-nary further, using a transformation scheme.
Such ascheme would construct a family of possible varia-tions of a name n: as an example, Figure 4 showsname variations created for the name span ?Ben-jamin Brown Smith?.
Once a dictionary is formed,a single pass is made through the corpus, and ev-ery longest match to some name-variation is markedas a name7.
It may be that a partial name span n1identified by the extractor is subsumed by the fullname span n2 identified by the dictionary-matchingscheme.
In this case, entity-level precision is in-creased, having corrected the entity?s boundaries.7Initials-only variants of a name, e.g., ?bs?
in Figure 4 aremarked as a name only if the ?inSignoff?
feature holds?i.e., ifthey appear near the end of a message in an apparent signature.benjamin brown smith benjamin-brown-s. b. brown s. bbsbenjamin-brown smith benjamin-b.
s. b. b. smith bsbenjamin brown-smith benjamin-smith b. brown-s.benjamin-brown-smith benjamin smith benjaminbenjamin brown s. b. brown smith brownbenjamin-b.
smith benjamin b. s. smithbenjamin b. smith b. brown-smith b. smithbenjamin brown-s. benjamin-s. b. b. sbenjamin-brown s. benjamin s. b. s.Figure 4: Names variants created from the name ?BenjaminBrown Smith?5.2 Dictionary-filtering schemesThe noisy dictionary-matching scheme is suscepti-ble to false positives.
That is, some words predictedby the extractor to be names are in fact non-names.Presumably, these non-names could be removed bysimply eliminating low-confidence predictions ofthe extractor; however, ambiguous words ?that arenot exclusively personal names in the corpus?
mayneed to be identified and removed as well.
We notethat ambiguity better be evaluated in the context ofthe corpus.
For example, ?Andrew?
is a commonfirst name, and may be confidently (and correctly)recognized as one by the extractor.
However, in theMgmt-Game corpus, ?Andrew?
is also the name ofan email server, and most of the occurrences of thisname in this corpus are not personal names.
Thehigh frequency of the word ?Andrew?
in the cor-pus, coupled with the fact that it is only sometimes aname, means that adding this word to the dictionaryleads to a substantial drop in precision.We therefore suggest a measure for filtering thedictionary.
This measure combines two metrics.
Thefirst metric, predicted frequency (PF), estimates thedegree to which a word appears to be used consis-tently as a name throughout the corpus:PF (w) ?
cpf(w)ctf(w)where cpf(w) denotes the number of times that aword w is predicted as part of a name by the extrac-tor, and ctf(w) is the number of occurrences of theword w in the entire test corpus (we emphasize thatestimating this statistic based on test data is valid, asit is fully automatic ?blind?
procedure).Predicted frequency does not assess the likely costof adding a word to a dictionary: as noted above,ambiguous or false dictionary terms that occur fre-quently will degrade accuracy.
A number of statis-tics could be used here; for instance, practitioners448sometimes filter a large dictionary by simply dis-carding all words that occur more than k times in atest corpus.
We elected to use the inverse documentfrequency (IDF) of w to measure word frequency:IDF (w) ?log(N+0.5df(w) )log(N + 1)Here df(w) is the number of documents that containa word w, and N is the total number of documentsin the corpus.
Inverse document frequency is oftenused in the field of information retrieval (Allan et al,1998), and the formula above has the virtue of beingscaled between 0 and 1 (like our PF metric) and ofincluding some smoothing.
In addition to boundingthe cost of a dictionary entry, the IDF formula is initself a sensible filter, since personal names will notappear as frequently as common English words.The joint filter combines these two multiplica-tively, with equal weights:PF.IDF (w) : PF (w) ?
IDF (w)PF.IDF takes into consideration both the probabilityof a word being a name, and how common it is inthe entire corpus.
Words that get low PF.IDF scoresare therefore either words that are highly ambiguousin the corpus (as derived from the extractors?
pre-dictions) or are common words, which were inaccu-rately predicted as names by the extractor.In the MDR method of Figure 3, we imposedan artificial requirement that words must appear inmore than one document.
In the method describedhere, there is no such requirement: indeed, wordsthat appear in a small number of documents aregiven higher weights, due to the IDF factor.
Thusthis approach exploits both single-document andmultiple-document repetitions.In a set of experiments that are not described here,the PF.IDF measure was found to be robust to pa-rameter settings, and also preferable to its separatecomponents in improving recall at minimal cost inprecision.
As described, the PF.IDF values per wordrange between 0 and 1.
One can vary the threshold,under which a word is to be removed from the dic-tionary, to control the precision-recall trade-off.
Wetuned the PF.IDF threshold using the validation sub-sets, optimizing entity-level F1 (a threshold of 0.16was found optimal).In summary, our recall-enhancing strategy is asfollows:1.
Learn an extractor E from the training corpus Ctrain .2.
Apply the extractor E to a test corpus Ctest to assign apreliminary labeling.3.
Build a dictionary S??
including the names n such that(a) n is extracted somewhere in the preliminary label-ing of the test corpus, or is derived from an extractedname applying the name transformation scheme and (b)PF.IDF (n) > ??.4.
Apply the dictionary-matching scheme of Section 5.1, us-ing the dictionary S??
to augment the preliminary label-ing, and output the result.5.3 Experiments with inferred dictionariesTable 4 shows results using the method describedabove.
We consider all of the email corpora and theCRF learner, trained with the full feature set.
Theresults are given in terms of relative change, com-pared to the baseline results generated by the extrac-tors (scoreresult/scorebaseline ?
1) and final value.As expected, recall is always improved.
Entity-level F1 is increased as well, as recall is increasedmore than precision is decreased.
The largest im-provements are for the Mgmt corpora ?the two e-mail datasets shown to have the largest potential im-provement from MDR-like methods in Figure 3.
Re-call improvements are more modest for the Enrondatasets, as was anticipated by the MDR analysis.Another reason for the gap is that extractor baselineperformance is lower for the Enron datasets, so thatthe Enron dictionaries are noisier.As detailed in Section 2, the Mgmt-Teams datasetwas constructed so that the names in the trainingand test set have only minimal overlap.
The perfor-mance improvement on this dataset shows that rep-etition of mostly-novel names can be detected usingour method.
This technique is highly effective whennames are novel, or dense, and is optimal when ex-tractor baseline precision is relatively high.Dataset Precision Recall F1Mgmt-Teams -0.9% / 92.9 +8.5% / 89.8 +3.9% / 91.3Mgmt-Game -0.8% / 94.5 +8.4% / 96.2 +3.8% / 95.4Enron-Meetings -2.5% / 81.1 +4.7% / 74.9 +1.2% / 77.9Enron-Random -3.8% / 79.2 +4.9% / 74.3 +0.7% / 76.7Table 4: Entity-level relative improvement and final result,applying name-matching on models trained with CRF and thefull feature set (F1 baseline given in Table 3).4496 ConclusionThis work applies recently-developed sequentiallearning methods to the task of extraction of namedentities from email.
This problem is of interest as anexample of NER from informal text?text that hasbeen prepared quickly for a narrow audience.We showed that informal text has different char-acteristics from formal text such as newswire.
Anal-ysis of the highly-weighted features selected by thelearners showed that names in informal text havedifferent (and less informative) types of contextualevidence.
However, email also has some structuralregularities which make it easier to extract personalnames.
We presented a detailed description of a setof features that address these regularities and signif-icantly improve extraction performance on email.In the second part of this paper, we analyzedthe way in which names repeat in different typesof corpora.
We showed that repetitions within asingle document are more common in newswiretext, and that repetitions that span multiple docu-ments are more common in email corpora.
Addi-tional analysis confirms that the potential gains inrecall from exploiting multiple-document repetitionis much higher than the potential gains from exploit-ing single-document repetition.Based on this insight, we introduced a simple andeffective method for exploiting multiple-documentrepetition to improve an extractor.
One drawback ofthe recall-enhancing approach is that it requires theentire test set to be available: however, our test setsare of only moderate size (83 to 264 documents),and it is likely that a similar-size sample of unlabeleddata would be available in many practical applica-tions.
The approach substantially improves recalland often improves F1 performance; furthermore, itcan be easily used with any NER method.Taken together, extraction performance is sub-stantially improved by this approach.
The improve-ments seem to be strongest for email corpora col-lected from closely interacting groups.
On theMgmt-Teams dataset, which was designed to reducethe value of memorizing specific names appearingin the training set, F1 performance is improved from68.1% for the out-of-the-box system (or 82.0% forthe dictionary-augmented system) to 91.3%.
For theless difficult Mgmt-Game dataset, F1 performanceis improved from 79.2% for an out-of-the-box CRF-based NER system (or 90.7% for a CRF-based sys-tem that uses several large dictionaries) to 95.4%.As future work, experiments should be expanded toinclude additional entity types and other types of in-formal text, such as blogs and forum postings.ReferencesJ.
Allan, J. Callan, W.B.
Croft, L. Ballesteros, D. Byrd,R.
Swan, and J. Xu.
1998.
Inquery does battle with trec-6.
In TREC-6.D.
M. Bikel, R. L. Schwartz, and R. M. Weischedel.
1999.
Analgorithm that learns what?s in a name.
Machine Learning,34:211?231.R.
Bunescu and R. J. Mooney.
2004.
Relational markov net-works for collective information extraction.
In ICML-2004Workshop on Statistical Relational Learning.W.
W. Cohen, E. Minkov, and A. Tomasic.
2005.
Learning toundertand website update requests.
In IJCAI-05.M.
Craven and J. Kumlien.
1999.
Constructing biologi-cal knowledge bases by extracting information from textsources.
In ISMB-99.A.
Culotta, R. Bekkerman, and A. McCallum.
2004.
Extractingsocial networks and contact information from email and theweb.
In CEAS-04.D.
Freitag.
1998.
Information extraction from html: applica-tion of a general machine learning approach.
In AAAI-98.K.
Humphreys, R. Gaizauskas, S. Azzam, C. Huyck,B.
Mitchell, H. Cunningham, and Y. Wilks.
1998.
Descrip-tion of the LASIE-II system as used for MUC-7.B.
Klimt and Y. Yang.
2004.
Introducing the Enron corpus.
InCEAS-04.R.
E. Kraut, S. R. Fussell, F. J. Lerch, and J.
A. Espinosa.
2004.Coordination in teams: evi-dence from a simulated manage-ment game.
To appear in the Journal of Organizational Be-havior.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Conditionalrandom fields: Probabilistic models for segmenting and la-beling sequence data.
In ICML-01.A.
McCallum and W. Li.
2003.
Early results for named entityrecognition with conditional random fields, feature inductionand web-enhanced lexicons.
In CoNLL-2003.M.
Stevenson and R. Gaizauskas.
2000.
Using corpus-derivednames lists for named entities recognition.
In NAACL-2000.C.
Sutton and A. Mccallum.
2004.
Collective segmentationand labeling of distant entities in information extraction.
InICML workshop on Statistical Relational Learning.L.
Sweeney.
2003.
Finding lists of people on the web.Technical Report CMU-CS-03-168, CMU-ISRI-03-104.http://privacy.cs.cmu.edu/dataprivacy/ projects/rosterfinder/.450
