Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 411?416,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsRACAI: Unsupervised WSD experiments @ SemEval-2, Task #17Radu IonInstitute for AI, Romanian Academy13, Calea 13 Septembrie, Bucharest050711, Romaniaradu@racai.roDan ?tef?nescuInstitute for AI, Romanian Academy13, Calea 13 Septembrie, Bucharest050711, Romaniadanstef@racai.roAbstractThis paper documents the participation of theResearch Institute for Artificial Intelligence ofthe Romanian Academy (RACAI) to the Task17 ?
All-words Word Sense Disambiguationon a Specific Domain, of the SemEval-2 com-petition.
We describe three unsupervised WSDsystems that make extensive use of the Prince-ton WordNet (WN) structure and WordNetDomains in order to perform the disambigua-tion.
The best of them has been ranked the 12thby the task organizers out of 29 judged runs.1 IntroductionReferring to the last SemEval (SemEval-1,(Agirre et al, 2007a)) and to our recent work(Ion and ?tef?nescu, 2009), unsupervised WordSense Disambiguation (WSD) is still at the bot-tom of WSD systems ranking with a significantloss in performance when compared to super-vised approaches.
With Task #17 @ SemEval-2,this observation is (probably 1 ) reinforced butanother issue is re-brought to light: the difficultyof supervised WSD systems to adapt to a givendomain (Agirre et al, 2009).
With general scoreslower with at least 3% than 3 years ago in Task#17 @ SemEval-1 which was a supposedly hard-er task  (general, no particular domain WSD wasrequired for all words), we observe that super-vised WSD is certainly more difficult to imple-ment in a real world application.Our unsupervised WSD approach benefitedfrom the specification of this year?s Task #17which was a domain-limited WSD, meaning thatthe disambiguation would be applied to contentwords drawn from a specific domain: the sur-rounding environment.
We worked under theassumption that a term of the given domain1 At the time of the writing we only know the systems rank-ing without the supervised/unsupervised distinction.would have the same meaning with all its occur-rences throughout the text.
This hypothesis hasbeen put forth by Yarowsky (1993) as the ?onesense per discourse?
hypothesis (OSPD forshort).The task organizers offered a set of back-ground documents with no sense annotations tothe competitors who want to train/tune their sys-tems using data from the same domain as theofficial test set.
Working with the OSPD hypo-thesis, we set off to construct/test domain specif-ic WSD models from/on this corpus using theWordNet Domains (Bentivogli et al, 2004).
Fortesting purposes, we have constructed an in-house gold standard from this corpus that com-prises of 1601 occurrences of 204 terms of the?surrounding environment?
domain that havebeen automatically extracted with the highestconfidence.
We have observed that our goldstandard (which has been independently anno-tated by 3 annotators but on non-overlappingsections which led to having no inter-annotatoragreement scores) obeys the OSPD hypothesiswhich we think that is appropriate to domain-limited WSD.In what follows, we will briefly acknowledgethe usage of WordNet Domains in WSD, we willthen describe the construction of the corpus ofthe background documents including here thecreation of an in-house gold standard, we willthen briefly describe our three WSD algorithmsand finally we will conclude with a discussion onthe ranking of our runs among the 29 evaluatedby the task organizers.2 Related WorkWordNet Domains is a hierarchy of labels thathave been assigned to WN synsets in a one to(possible) many relationship (but the frequentcase is a single WN domain for a synset).
A do-main is the name of an area of knowledge that isrecognized as unitary (Bentivogli et al, 2004).411Thus labels such as ?architecture?, ?sport?
or?medicine?
are mapped onto synsets like?arch(4)-noun?, ?playing(2)-noun?
or ?chron-ic(1)-adjective?
because of the fact that the re-spective concept evokes the domain.WordNet Domains have been used in variousways to perform WSD.
The main usage of thismapping is that the domains naturally create aclustering of the WN senses of a literal thus of-fering a sense inventory that is much coarser thanthe fine sense distinctions of WN.
For instance,senses 1 (?a flat-bottomed motor vehicle that cantravel on land or water?)
and 2 (?an airplanedesigned to take off and land on water?)
of thenoun ?amphibian?
are both mapped to the do-main ?transport?
but the 3rd sense of the samenoun is mapped onto the domains ?ani-mals/biology?
being the ?cold-blooded verte-brate typically living on land but breeding inwater; aquatic larvae undergo metamorphosisinto adult form?
(definitions from version 2.0 ofthe WN).V?zquez et al (2004) use WordNet Domainsto derive a new resource they call the RelevantDomains in which, using WordNet glosses, theyextract the most representative words for a givendomain.
Thus, for a word w and a domain d, theAssociation Ratio formula between w and d is)(P)|(Plog)|(P),(AR 2 wdwdwdw ?=in which, for each synset its gloss has been POStagged and lemmatized.
The probabilities arecomputed counting pairs dw, in glosses (eachgloss has an associated d domain via its synset).Using the Relevant Domains, the WSD proce-dure for a given word w in its context C (a 100words window centered in w), computes a simi-larity measure between two vectors of ARscores: the first vector is the vector of AR scoresof the sentence in which w appears and the otheris the vector of domain scores computed for thegloss of a sense of w (both vectors are norma-lized such that they contain the same domains).The highest similarity gives the sense of w that isclosest to the domain vector of C. With this me-thod, V?zquez et al obtain a precision of 0.54and a recall of 0.43 at the SensEval-2, EnglishAll-Words Task placing them in the 10th positionout of 22 systems where the best one (a super-vised system) achieved a 0.69 precision and anequal recall.Another approach to WSD using the WordNetDomains is that of Magnini et al (2002).
Themethod is remarkably similar to the previous onein that the description of the vectors and the se-lection of the assigned sense is the same.
Whatdiffers, is the weights that are assigned to eachdomain in the vector.
Magnini et al distinguishbetween text vectors (C vectors in the previouspresentation) and sense vectors.
Text (or context)vector weights are computed comparing domainfrequency in the context with the domain fre-quency over the entire corpus (see Magnini et al(2002) for details).
Sense vectors are derivedfrom sense-annotated data which qualifies thismethod as a supervised one.
The results that havebeen reported at the same task the previous algo-rithm participated (SensEval-2, English All-Words Task), are: precision 0.748 and recall0.357 (12th place).Both the methods presented here are very sim-ple and easy to adapt to different domains.
Oneof our methods (RACAI-1, see below) is evensimpler (because it makes the OSPD simplifyingassumption) and performs with approximatelythe same accuracy as any of these methods judg-ing by the rank of the system and the total num-ber of participants.3 Using the Background DocumentscollectionTask #17 organizers have offered a set of back-ground documents for training/tuning/testingpurposes.
The corpus consists of 124 files fromthe ?surrounding environment?
domain that havebeen collected in the framework of the KyotoProject (http://www.kyoto-project.eu/).First, we have assembled the files into a singlecorpus in order to be able to apply some cleaningprocedures.
These procedures involved the re-moval of the paragraphs in which the proportionof letters (Perl character class ?[A-Za-z_-]?
)was less than 0.8 because the text contained a lotof noise in form of lines of numbers and othersymbols which probably belonged to tables.
Thenext stage was to have the corpus POS-tagged,lemmatized and chunked using the TTL web ser-vice (Tufi?
et al, 2008).
The resulting file is anXML encoded corpus which contains 136456sentences with 2654446 tokens out of which348896 are punctuation tokens.In order to test our domain constrained WSDalgorithms, we decided to construct a test setwith the same dimension as the official test set ofabout 2000 occurrences of content words specificto the ?surrounding environment?
domain.
Indoing this, we have employed a simple term ex-412traction algorithm which considers that terms, asopposed to words that are not domain specific,are not evenly distributed throughout the corpus.To formalize this, the corpus is a vector of lem-mas [ ]Nlll ,,,C 21 K=  and for each unique lem-ma Njl j ?
?1, , we compute the mean of theabsolute differences of its indexes in C asmjkjjNkj llkmjmlllfkj?<<??=??=??<?
,,,1)(1?where )( jlf  is the frequency of jl  in C. Wealso compute the standard deviation of these dif-ferences from the mean as2)()(12???=?
?<?jNkjlfkj ?
?in the same conditions as above.With the mean and standard deviation of in-dexes differences of a content word lemma com-puted, we construct a list of all content wordlemmas that is sorted in descending order by thequantity ??
/ which we take as a measure of theevenness of a content word lemma distribution.Thus, lemmas that are in the top of this list arelikely to be terms of the domain of the corpus (inour case, the ?surrounding environment?
do-main).
Table 1 contains the first 20 automaticallyextracted terms along with their term score.Having the list of terms of our domain, wehave selected the first ambiguous 210 (whichhave more than 1 sense in WN) and constructeda test set in which each term has (at least) 10 oc-currences in order to obtain a test corpus with atleast 2000 occurrences of the terms of the ?sur-rounding environment?
domain.
A large part ofthese occurrences have been independentlysense-annotated by 3 annotators which workedon disjoint sets of terms (70 terms each) in orderto finish as soon as possible.
In the end we ma-naged to annotate 1601 occurrences correspond-ing to 204 terms.When the gold standard for the test set wasready, we checked to see if the OSPD hypothesisholds.
In order to determine if it does, we com-puted the average number of annotated differentsenses per term which is 1.36.
In addition, consi-dering the fact that out of 204 annotated terms,145 are annotated with a single sense, we maystate that in this case, the OSPD hypothesisholds.Term Score Term Scoregibbon 15.89 Oceanica 9.41fleet 13.91 orangutan 9.19sub-region 13.01 laurel 9.08Amazon 12.41 coral 9.06roundwood 12.26 polar 9.05biocapacity 12.23 wrasse 8.80footprint 11.68 reef 8.78deen 11.45 snapper 8.67dune 10.57 biofuel 8.53grouper 9.67 vessel 8.35Table 1: The first 20 automatically extracted terms ofthe ?surrounding environment?
domain4 The Description of the SystemsSince we are committed to assign a unique senseper word in the test set, we might as well try toautomatically induce a WSD model from thebackground corpus in which, for each lemmaalong with its POS tag that also exists in WN, asingle sense is listed that is derived from the cor-pus.
Then, for any test set of the same domain,the algorithm would give the sense from theWSD model to any of the occurrences of thelemma.What we actually did, was to find a list ofmost frequent 2 WN domains (frequency countextracted from the whole corpus) for each lemmawith its POS tag, and using these, to list allsenses of the lemma that are mapped onto these 2domains (thus obtaining a reduction of the aver-age number of senses per word).
The steps of thealgorithm for the creation of the WSD model are:1. in the given corpus, for each lemma land its POS-tag p normalized to WNPOS notation (?n?
for nouns, ?v?
forverbs, ?a?
for adjectives and ?b?
for ad-verbs), for each of its senses from WN,increase by 1 each frequency of eachmapped domain;2. for each lemma l with its POS-tag p, re-tain only those senses that map onto themost frequent 2 domains as determinedby the frequency list from the first step.Using our 2.65M words background corpus tobuild such a model (Table 2 contains a sample),we have obtained a decrease in average ambigui-ty degree (the average number of senses per con-tent word lemma) from 2.43 to 2.14.
If we set athreshold of at least 1 for the term score of thelemmas to be included into the WSD model(which selects 12062 lemmas, meaning about 1/3of all unique lemmas in the corpus), we obtain413the same reduction thus contradicting our hypo-thesis that the average ambiguity degree of termswould be reduced more than the average ambigu-ity degree of all words in the corpus.
This resultmight be due to the fact that the ?factotum?
do-main is very frequent (much more frequent thanany of the other domains).Lemma POS:Total no.of WN sensesFirst 2 selecteddomainsSelectedsensesfish n:2 animals,biology1Arctic n:1 geography 1coral n:4 chemistry,animals 2,3,4Table 2: A sample of the WSD model built from thebackground corpusIn what follows, we will present our 3 systemsthat use WSD models derived from the test sets(both the in-house and the official ones).
In theResults section we will explain this choice.4.1 RACAI-1: WordNet Domains-driven,Most Frequent SenseThe first system, as its name suggests, is verysimple: using the WSD model, it chooses themost frequent sense (MFS) of the lemma l withPOS p according to WN (that is, the lowest num-bered sense from the list of senses the lemma hasin the WSD model).Trying this method on our in-house developedtest set, we obtain encouraging results: the over-all accuracy (precision is equal with the recallbecause all test set occurrences are tried) is atleast 4% over the general MFS baseline (senseno.
1 in all cases).
The Results section gives de-tails.4.2 RACAI-2: The Lexical Chains SelectionWith this system, we have tried to select onlyone sense (not necessarily the most frequent one)of lemma l with POS p from the WSD model.The selection procedure is based on lexicalchains computation between senses of the targetword (the word to be disambiguated) and thecontent words in its sentence in a manner thatwill be explained below.We have used the lexical chains descriptionand computation method described in (Ion and?tef?nescu, 2009).
To reiterate, a lexical chain isnot simply a set of topically related words butbecomes a path of synsets in the WordNet hie-rarchy.
The lexical chain procedure is a functionof two WN synsets, LXC(s1, s2), that returns asemantic relation path that one can follow toreach s2 from s1.
On the path from s2 to s1 thereare k synsets (k ?
0) and between 2 adjacent syn-sets there is a WN semantic relation.
Each lexicalchain can be assigned a certain score that we in-terpret as a measure of the semantic similarity(SS) between s1 and s2 (see (Ion and ?tef?nescu,2009) and (Moldovan and Novischi, 2002) formore details).
Thus, the higher the value ofSS(s1, s2), the higher the semantic similarity be-tween s1 and s2.We have observed that using RACAI-1 on ourin-house test set but allowing it to select the mostfrequent 2 senses of lemma l with POS p fromthe WSD model, we obtain a whopping 82%accuracy.
With this observation, we tried to pro-gram RACAI-2 to make a binary selection fromthe first 2 most frequent senses of lemma l withPOS p from the WSD model in order to approachthe 82% percent accuracy limit which wouldhave been a very good result.
The algorithm is asfollows: for a lemma l with POS p and a lemmalc with POS pc from the context (sentence) of l,compute the best lexical chain between any ofthe first 2 senses of l and any of the first 2 sensesof lc according to the WSD model.
If the first 2senses of l are a and b and the first 2 senses of lcare x and y and the best lexical chain score hasbeen found between a and y for instance, thencredit sense a of l with SS(a, y).
Sum over all lcfrom the context of l and select that sense of lwhich has a maximum semantic similarity withthe context.4.3 RACAI-3: Interpretation-based SenseAssignmentThis system tries to generate all the possiblesense assignments (called interpretations) to thelemmas in a sentence.
Thus, in principle, foreach content word lemma, all its WN senses areconsidered thus generating an exponential explo-sion of the sense assignments that can be attri-buted to a sentence.
If we have N content wordlemmas which have k senses on average, we ob-tain a search space of kN interpretations whichhave to be scored.Using the observation mentioned above thatthe first 2 senses of a lemma according to theWSD model yields a performance of 82%, bringsthe search space to 2N but for a large N, it is stilltoo big.The solution we adopted (besides consideringthe first 2 senses from the WSD model) consistsin segmenting the input sentence in M indepen-dent segments of 10 content word lemmas each,which will be processed independently, yielding414a search space of at most 102?M of smaller in-terpretations.
The best interpretation per eachsegment would thus be a part of the best interpre-tation of the sentence.
Next, we describe how wescore an interpretation.For each sense s of a lemma l with POS p(from the first 2 senses of l listed in the WSDmodel) we compute an associated set of contentwords (lemmas) from the following sources:?
all content word lemmas extracted fromthe sense s corresponding gloss (disre-garding the auxiliary verbs);?
all literals of the synset in which lemma lwith sense s exists;?
all literals of the synsets that are linkedwith the synset l(s) by a relation of the fol-lowing type: hypernym, near_antonym,eng_derivative, hyponym, meronym, ho-lonym, similar_to, derived;?
all content word lemmas extracted fromthe glosses corresponding to synsets thatare linked with the l(s) synset by a relationof the following type: hypernym,eng_derivative, similar_to, derived;With this feature set V of a sense s belonging tolemma l with POS p, for a given interpretation (aspecific assignment of senses to each lemma in asegment), its score S (initially 0) is computediteratively (for two adjacent position i and i + 1in the segment) as111 VVV,VVSS +++ ???+?
iiiiiwhere the |X| function is the cardinality functionon the set X and ?
is the assignment operator.5 ResultsIn order to run our WSD algorithms, we had toextract WSD models.
We tested the accuracy ofthe disambiguation (onto the in-house developedgold standard) with RACAI-1 and RACAI-2 sys-tems (RACAI-3 was not ready at that time) withmodels extracted a) from the whole backgroundcorpus and b) from the in-house developed testset (named here the RACAI test set, see section3).
The results are reported in Table 3 along withRACAI-1 system returning the first 2 senses of alemma from the WSD model and the generalMFS baseline.As we can see, the results with the WSD mod-el extracted from the test set are marginally bet-ter than the other results.
This was the reason forwhich we chose to extract the WSD model fromthe official test set as opposed to using the WSDmodel extracted from the background corpus.RACAITest SetBackgroundCorpusRACAI-1 0.647 0.644RACAI-1 (2 senses) 0.825 0.811RACAI-2 0.591 0.582MFS (sense no.
1) 0.602 0.602Table 3: RACAI systems results (accuracy) on theRACAI test setHowever, we did not research the possibility ofadding the official test set to either the RACAItest set or the background corpus and extractWSD models from there.The official test set (named the SEMEVALtest set here) contains 1398 occurrences of con-tent words for disambiguation, out of which 366are occurrences of verbs and 1032 are occur-rences of nouns.
These occurrences correspondto 428 lemmas.
Inspecting these lemmas, wehave found that there are many of them whichare not domain specific (in our case, specific tothe ?surrounding environment?
domain).
Forinstance, the verb to ?be?
is at the top of the listwith 99 occurrences.
It is followed by the noun?index?
with 32 occurrences and by the noun?network?
with 22 occurrences.
With fewer oc-currences follow ?use?, ?include?, ?show?, ?pro-vide?, ?part?
and so on.
Of course, the SEMEV-AL test set includes proper terms of the designat-ed domain such as ?area?
(61 occurrences),?species?
(58 occurrences), ?nature?
(31 occur-rences), ?ocean?, ?sea?, ?water?, ?planet?, etc.Table 4 lists our official results on the SE-MEVAL test set.Precision Recall RankRACAI-1 0.461 0.46 #12RACAI-2 0.351 0.35 #25RACAI-3 0.433 0.431 #18MFS 0.505 0.505 #6Table 4: RACAI systems results (accuracy) on theSEMEVAL test setPrecision is not equal to recall because of the factthat our POS tagger found two occurrences of theverb to ?be?
as auxiliaries which were ignored.The column Rank indicates the place our systemshave in a 29 run ranking of all systems that parti-cipated in Task 17 ?
All-words Word Sense Dis-ambiguation on a Specific Domain, of the Se-415mEval-2 competition which was won by a sys-tem that achieved a precision of 0.57 and a recallof 0.555.The differences with the runs on the RACAItest set are significant but this can be explainedby the fact that our WordNet Domains WSD me-thod cannot cope with general (domain indepen-dent) WSD requirements in which the ?one senseper discourse?
hypothesis does not necessarilyhold.6 ConclusionsRegarding the 3 systems that we entered in theTask #17 @ SemEval-2, we think that the lexicalchains algorithm (RACAI-2) is the most promis-ing even if it scored the lowest of the three.
Weattribute its poor performances to the lexicalchains computation, especially to the weights ofthe WN semantic relations that make up a chain.Also, we will extend our research regarding thecorrectness of lexical chains (the degree to whicha human judge will appreciate as correct or evoc-ative or as common knowledge a semantic pathbetween two synsets).We also want to check if our three systemsmake the same mistakes or not in order to devisea way in which we can combine their outputs.RACAI is at the second participation in theSemEval series of WSD competitions.
We arecommitted to improving the unsupervised WSDtechnology which, we think, is more easilyadaptable and usable in real world applications.We hope that SemEval-3 will reveal significantimprovements in this direction.AcknowledgmentsThe work reported here was supported by theRomanian Ministry of Education and Researchthrough the STAR project (no.
742/19.01.2009).ReferencesEneko Agirre, Llu?s M?rquez and Richard Wicen-towski, Eds., 2007.
Proceedings of Semeval-2007Workshop.
Prague, Czech Republic: Associationfor Computational Linguistics, 2007.Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-baum, Andrea Marchetti, Antonio Toral, Piek Vos-sen. 2009.
SemEval-2010 Task 17: All-words WordSense Disambiguation on a Specific Domain.
InProceedings of NAACL workshop on SemanticEvaluations (SEW-2009).
Boulder,Colorado, 2009.Luisa Bentivogli, Pamela Forner, Bernardo Magniniand Emanuele Pianta.
2004.
Revising WordNetDomains Hierarchy: Semantics, Coverage, andBalancing.
In COLING 2004 Workshop on "Multi-lingual Linguistic Resources", Geneva, Switzer-land, August 28, 2004, pp.
101-108.Radu Ion and Dan ?tef?nescu.
2009.
UnsupervisedWord Sense Disambiguation with Lexical Chainsand Graph-based Context Formalization.
In Zyg-munt Vetulani, editor, Proceedings of the 4th Lan-guage and Technology Conference: Human Lan-guage Technologies as a Challenge for ComputerScience and Linguistics, pages 190?194, Pozna?,Poland, November 6?8 2009.
WydawnictwoPozna?skie Sp.Bernardo Magnini, Carlo Strapparava, GiovanniPezzulo, Alfio Gliozzo.
2002.
The role of domaininformation in Word Sense Disambiguation.
Natu-ral Language Engineering, 8(4), 359?373, De-cember 2002.Dan Moldovan and Adrian Novischi.
2002.
Lexicalchains for question answering.
In Proceedings ofthe 19th International Conference on Computation-al Linguistics, August 24 ?
September 01, 2002,Taipei, Taiwan, pp.
1?7.Dan Tufi?, Radu Ion, Alexandru Ceau?u and Dan?tef?nescu.
2008.
RACAI's Linguistic Web Servic-es.
In Proceedings of the 6th Language Resourcesand Evaluation Conference ?
LREC 2008, Marra-kech, Morocco, May 2008.
ELRA ?
EuropeanLanguage Ressources Association.
ISBN 2-9517408-4-0.Sonia V?zquez, Andr?s Montoyo and German Ri-gau.
2004.
Using Relevant Domains Resource forWord Sense Disambiguation.
In Proceedings of theInternational Conference on Artificial Intelligence(IC-AI'04), Las Vegas, Nevada, 2004.David Yarowsky.
1993.
One sense per collocation.
InARPA Human Language Technology Workshop,pp.
266?271, Princeton, NJ, 1993.416
