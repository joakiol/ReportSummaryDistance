Proceedings of the Second ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 23?27,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsA Core-Tools Statistical NLP CourseDan KleinComputer Science DivisionUniversity of California, BerkeleyBerkeley, CA 94720klein@cs.berkeley.eduAbstractIn the fall term of 2004, I taught anew statistical NLP course focusingon core tools and machine-learning al-gorithms.
The course work was or-ganized around four substantial pro-gramming assignments in which thestudents implemented the importantparts of several core tools, includinglanguage models (for speech rerank-ing), a maximum entropy classifier, apart-of-speech tagger, a PCFG parser,and a word-alignment system.
Usingprovided scaffolding, students built re-alistic tools with nearly state-of-the-art performance in most cases.
Thispaper briefly outlines the coverage ofthe course, the scope of the assign-ments, and some of the lessons learnedin teaching the course in this way.1 IntroductionIn the fall term of 2004, I taught a new sta-tistical NLP course at UC Berkeley which cov-ered the central tools and machine-learning ap-proaches of NLP.
My goal in formulating thiscourse was to create a syllabus and assignmentset to teach in a relatively short time the impor-tant aspects, both practical and theoretical, ofwhat took me years of building research tools tointernalize.
The result was a rather hard coursewith a high workload.
Although the course eval-uations were very positive, and several of thestudents who completed the course were able tojump right into research projects in my group,there?s no question that the broad accessibilityof the course, especially for non-CS students,was limited.As with any NLP course, there were severalfundamental choice points.
First, it?s not possi-ble to cover both core tools and end-to-end ap-plications in detail in a single term.
Since MartiHearst was teaching an applied NLP course dur-ing the same term, I chose to cover tools andalgorithms almost exclusively (see figure 1 for asyllabus).
The second choice point was whetherto organize the course primarily around linguis-tic topics or primarily around statistical meth-ods.
I chose to follow linguistic topics becausethat order seemed much easier to motivate to thestudents (comments on this choice in section 3).The final fundamental choice I made in decid-ing how to target this class was to require bothsubstantial coding and substantial math.
Thischoice narrowed the audience of the class, butallowed the students to build realistic systemswhich were not just toy implementations.I feel that the most successful aspect ofthis course was the set of assignments, so thelargest section below will be devoted to de-scribing them.
If other researchers are inter-ested in using any of my materials, they are en-couraged to contact me or visit my web page(http://www.cs.berkeley.edu/~klein).2 AudienceThe audience of the class began as a mix of CSPhD students (mostly AI but some systems stu-dents), some linguistics graduate students, and23a few advanced CS undergrads.
What becameapparent after the first homework assignment(see section 4.2) was that while the CS studentscould at least muddle through the course withweak (or absent) linguistics backgrounds, thelinguistics students were unable to acquire themath and programming skills quickly enough tokeep up.
I have no good ideas about how to ad-dress this issue.
Moreover, even among the CSstudents, some of the systems students had trou-ble with the math and some of the AI/theorystudents had issues with coding scalable solu-tions.
The course was certainly not optimizedfor broad accessibility, but the approximately80% of students who stuck it out did what I con-sidered to be extremely impressive work.
Forexample, one student built a language modelwhich took the mass reserved for new wordsand distributed it according to a character n-gram model.
Another student invented a non-iterative word alignment heuristic which out-performed IBM model 4 on small and mediumtraining corpora.
A third student built a maxentpart-of-speech tagger with a per-word accuracyof 96.7%, certainly in the state-of-the-art range.3 TopicsThe topics covered in the course are shown infigure 1.
The first week of the course was es-sentially a history lesson about symbolic ap-proaches NLP, both to show their strengths (afull, unified pipeline including predicate logic se-mantic interpretations, while we still don?t havea good notion of probabilistic interpretation)and their weaknesses (many interpretations arisefrom just a few rules, ambiguity poorly han-dled).
From there, I discussed statistical ap-proaches to problems of increasing complexity,spending a large amount of time on tree and se-quence models.As mentioned above, I organized the lecturesaround linguistic topics rather than mathemat-ical methods.
However, given the degree towhich the course focused on such foundationalmethods, this order was perhaps a mistake.
Forexample, it meant that simple word alignmentmodels like IBM models 1 and 2 (Brown etal., 1990) and the HMM model (Vogel et al,1996) came many weeks after HMMs were intro-duced in the context of part-of-speech tagging.I also separated unsupervised learning into itsown sub-sequence, where I now wish I had pre-sented the unsupervised approaches to each taskalong with the supervised ones.I assigned readings from Jurafsky and Mar-tin (2000) and Manning and Schu?tze (1999) forthe first half of the course, but the second halfwas almost entirely based on papers from the re-search literature.
This reflected both increasingsophistication on the part of the students andinsufficient coverage of the latter topics in thetextbooks.4 AssignmentsThe key component which characterized thiscourse was the assignments.
Each assignmentis described below.
They are available foruse by other instructors.
While licensingissues with the data make it impossible to putthe entirety of the assignment materials onthe web, some materials will be linked fromhttp://www.cs.berkeley.edu/~klein, andthe rest can be obtained by emailing me.4.1 Assignment PrinciplesThe assignments were all in Java.
In all cases,I supplied a large amount of scaffolding codewhich read in the appropriate data files, con-structed a placeholder baseline system, andtested that baseline.
The students therefore al-ways began with a running end-to-end pipeline,using standard corpora, evaluated in standardways.
They then swapped out the baselineplaceholder for increasingly sophisticated imple-mentations.
When possible, assignments alsohad a toy ?miniTest?
mode where rather thanreading in real corpora, a small toy corpus wasloaded to facilitate debugging.
Assignmentswere graded entirely on the basis of write-ups.4.2 Assignment 1: Language ModelingIn the first assignment, students built n-gramlanguage models using WSJ data.
Their lan-guage models were evaluated in three ways by24Topics Techniques LecturesClassical NLP Chart Parsing, Semantic Interpretation 2Speech and Language Modeling Smoothing 2Text Categorization Naive-Bayes Models 1Word-Sense Disambiguation Maximum Entropy Models 1Part-of-Speech Tagging HMMs and MEMMs 1Part-of-Speech Tagging CRFs 1Statistical Parsing PCFGs 1Statistical Parsing Inference for PCFGs 1Statistical Parsing Grammar Representations 1Statistical Parsing Lexicalized Dependency Models 1Statistical Parsing Other Parsing Models 1Semantic Representation 2Information Extraction 1Coreference 1Machine Translation Word-to-Word Alignment Models 1Machine Translation Decoding Word-to-Word Models 1Machine Translation Syntactic Translation Models 1Unsupervised Learning Document Clustering 1Unsupervised Learning Word-Level Clustering 1Unsupervised Learning Grammar Induction 2Question Answering 1Document Summarization 1Figure 1: Topics Covered.
Each lecture was 80 minutes.the support harness.
First, perplexity on held-out WSJ text was calculated.
In this evaluation,reserving the correct mass for unknown wordswas important.
Second, their language modelswere used to rescore n-best speech lists (suppliedby Brian Roark, see Roark (2001)).
Finally, ran-dom sentences were generatively sampled fromtheir models, giving students concrete feedbackon how their models did (or did not) capture in-formation about English.
The support code in-tially provided an unsmoothed unigram modelto get students started.
They were then askedto build several more complex language mod-els, including at least one higher-order interpo-lated model, and at least one model using Good-Turing or held-out smoothing.
Beyond these re-quirements, students were encouraged to acheivethe best possible word error rate and perplexityfigures by whatever means they chose.1 Theywere also asked to identify ways in which theirlanguage models missed important trends of En-1After each assignment, I presented in class an hon-ors list, consisting of the students who won on any mea-sure or who had simply built something clever.
I initiallyworried about how these honors announcements wouldbe received, but students really seemed to enjoy hearingwhat their peers were doing, and most students made thehonors list at some point in the term.glish and to suggest solutions.As a second part to assignment 1, studentstrained class-conditional n-gram models (at thecharacter level) to do the proper name identi-fication task from Smarr and Manning (2002)(whose data we used).
In this task, proper namestrings are to be mapped to one of {drug, com-pany, movie, person, location}.
This turnsout to be a fairly easy task since the differentcategories have markedly different character dis-tributions.2 In the future, I will move this partof assignment 1 and the matching part of assign-ment 2 into a new, joint assignment.4.3 Assignment 2: Maximum Entropy /POS TaggingIn assignment 2, students first built a generalmaximum entropy model for multiclass classi-fication.
The support code provided a crippledmaxent classifier which always returned the uni-form distribution over labels (by ignoring thefeatures of the input datum).
Students replacedthe crippled bits and got a correct classifier run-2This assignment could equally well have been doneas a language identification task, but the proper namedata was convenient and led to fun error analysis, sincein good systems the errors are mostly places named afterpeople, movies with place names as titles, and so on.25ning, first on a small toy problem and then onthe proper-name identification problem from as-signment 1.
The support code provided opti-mization code (an L-BFGS optimizer) and fea-ture indexing machinery, so students only wrotecode to calculate the maxent objective functionand its derivatives.The original intention of assignment 2 wasthat students then use this maxent classifier as abuilding block of a maxent part-of-speech taggerlike that of Ratnaparkhi (1996).
The supportcode supplied a most-frequent-tag baseline tag-ger and a greedy lattice decoder.
The studentsfirst improved the local scoring function (keep-ing the greedy decoder) using either an HMMor maxent model for each timeslice.
Once thiswas complete, they upgraded the greedy decoderto a Viterbi decoder.
Since students were, inpractice, generally only willing to wait about 20minutes for an experiment to run, most chose todiscard their maxent classifiers and build gener-ative HMM taggers.
About half of the students?final taggers exceeded 96% per-word tagging ac-curacy, which I found very impressive.
Studentswere only required to build a trigram taggerof some kind.
However, many chose to havesmoothed HMMs with complex emission mod-els like Brants (2000), while others built maxenttaggers.Because of the slowness of maxent taggers?training, I will just ask students to build HMMtaggers next time.
Moreover, with the relationbetween the two parts of this assignment gone, Iwill separate out the proper-name classificationpart into its own assignment.4.4 Assignment 3: ParsingIn assignment 3, students wrote a probabilis-tic chart parser.
The support code read inand normalized Penn Treebank trees using thestandard data splits, handled binarization of n-ary rules, and calculated ParsEval numbers overthe development or test sets.
A baseline left-branching parser was provided.
Students wrotean agenda-based uniform-cost parser essentiallyfrom scratch.
Once the parser parsed cor-rectly with the supplied treebank grammar, stu-dents experimented with horizontal and verticalmarkovization (see Klein and Manning (2003))to improve parsing accuracy.
Students werethen free to experiment with speed-ups to theparser, more complex annotation schemes, andso on.
Most students?
parsers ran at reasonablespeeds (around a minute for 40 word sentences)and got final F1 measures over 82%, which issubstantially higher than an unannotated tree-bank grammar will produce.
While this assign-ment would appear to be more work than theothers, it actually got the least overload-relatedcomplaints of all the assignments.In the future, I may instead have students im-plement an array-based CKY parser (Kasami,1965), since a better understanding of CKYwould have been more useful than knowingabout agenda-based methods for later parts ofthe course.
Moreover, several students wantedto experiment with induction methods whichrequired summing parsers instead of Viterbiparsers.4.5 Assignment 4: Word AlignmentIn assignment 4, students built word alignmentsystems using the Canadian Hansards trainingdata and evaluation alignments from the 2003(and now 2005) shared task in the NAACLworkshop on parallel texts.
The support codeprovided a monotone baseline aligner and eval-uation/display code which graphically printedgold alignments superimposed over guessedalignments.
Students first built a heuristicaligner (Dice, mutual information-based, orwhatever they could invent) and then built IBMmodel 1 and 2 aligners.
They then had a choiceof either scaling up the system to learn fromlarger training sets or implementing the HMMalignment model.4.6 Assignment ObservationsFor all the assignments, I stressed that the stu-dents should spend a substantial amount of timedoing error analysis.
However, most didn?t, ex-cept for in assignment 2, where the support codeprinted out every error their taggers made, bydefault.
For this assignment, students actuallyprovided very good error analysis.
In the fu-ture, I will increase the amount of verbose er-26ror output to encourage better error analysis forthe other assignments ?
it seemed like studentswere reluctant to write code to display errors,but were happy to look at errors as they scrolledby.3A very important question raised by ananonymous reviewer was how effectively imple-menting tried-and-true methods feeds into newresearch.
For students who will not be do-ing NLP research but want to know how thebasic methods work (realistically, this is mostof the audience), the experience of having im-plemented several ?classic?
approaches to coretools is certainly appropriate.
However, evenfor students who intend to do NLP research,this hands-on tour of established methods hasalready shown itself to be very valuable.
Thesestudents can pick up any paper on any of thesetasks, and they have a very concrete idea aboutwhat the data sets look like, why people dothings they way they do, and what kinds of er-ror types and rates one can expect from a giventool.
That?s experience that can take a long timeto acquire otherwise ?
it certainly took me awhile.
Moreover, I?ve had several students fromthe class start research projects with me, and,in each case, those projects have been in someway bridged by the course assignments.
Thismethodology also means that all of the studentsworking with me have a shared implementationbackground, which has facilitated ad hoc collab-orations on research projects.5 ConclusionsThere are certainly changes I will make when Iteach this course again this fall.
I will likelyshuffle the topics around so that word align-ment comes earlier (closer to HMMs for tagging)and I will likely teach dynamic programming so-lutions to parsing and tagging in more depththan graph-search based methods.
Some stu-dents needed remedial linguistics sections andother students needed remedial math sections,and I would hold more such sessions, and ear-3There was also verbose error reporting for assign-ment 4, which displayed each sentence?s guessed and goldalignments in a grid, but since most students didn?t speakFrench, this didn?t have the same effect.lier in the term.
However, I will certainly keepthe substantial implementation component ofthe course, partially in response to very positivestudent feedback on the assignments, partiallyfrom my own reaction to the high quality of stu-dent work on those assignments, and partiallyfrom how easily students with so much hands-on experience seem to be able to jump into NLPresearch.ReferencesThorsten Brants.
2000.
TnT ?
a statistical part-of-speech tagger.
In ANLP 6, pages 224?231.Peter F. Brown, John Cocke, Stephen A. DellaPietra, Vincent J. Della Pietra, Fredrick Jelinek,John D. Lafferty, Robert L. Mercer, and Paul S.Roossin.
1990.
A statistical approach to machinetranslation.
Computational Linguistics, 16(2):79?85.Dan Jurafsky and James H. Martin.
2000.
Speechand Language Processing: An Introduction to Nat-ural Language Processing, Computational Linguis-tics and Speech Recognition.
Prentice Hall, Engle-wood Cliffs, NJ.T.
Kasami.
1965.
An efficient recognition and syn-tax analysis algorithm for context-free languages.Technical Report AFCRL-65-758, Air Force Cam-bridge Research Laboratory, Bedford, MA.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In ACL 41, pages423?430.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of Statistical Natural Language Pro-cessing.
The MIT Press, Cambridge, Mas-sachusetts.Adwait Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In EMNLP 1,pages 133?142.Brian Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguis-tics, 27:249?276.Joseph Smarr and Christopher D. Manning.
2002.Classifying unknown proper noun phrases withoutcontext.
Technical report, Stanford University.Stephan Vogel, Hermann Ney, and Christoph Till-mann.
1996.
HMM-based word alignment in sta-tistical translation.
In COLING 16, pages 836?841.27
