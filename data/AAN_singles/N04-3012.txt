WordNet::Similarity - Measuring the Relatedness of ConceptsTed PedersenDepartment of Computer ScienceUniversity of MinnesotaDuluth, MN 55812tpederse@d.umn.eduSiddharth PatwardhanSchool of ComputingUniversity of UtahSalt Lake City, UT 84102sidd@cs.utah.eduhttp://search.cpan.org/dist/WordNet-Similarityhttp://wn-similarity.sourceforge.netJason MichelizziDepartment of Computer ScienceUniversity of MinnesotaDuluth, MN 55812mich0212@d.umn.eduAbstractWordNet::Similarity is a freely available soft-ware package that makes it possible to mea-sure the semantic similarity and relatedness be-tween a pair of concepts (or synsets).
It pro-vides six measures of similarity, and three mea-sures of relatedness, all of which are based onthe lexical database WordNet.
These measuresare implemented as Perl modules which takeas input two concepts, and return a numericvalue that represents the degree to which theyare similar or related.1 IntroductionWordNet::Similarity implements measures of similarityand relatedness that are all in some way based on thestructure and content of WordNet.Measures of similarity use information found in an is?a hierarchy of concepts (or synsets), and quantify howmuch concept A is like (or is similar to) concept B. Forexample, such a measure might show that an automobileis more like a boat than it is a tree, due to the fact thatautomobile and boat share vehicle as an ancestor in theWordNet noun hierarchy.WordNet is particularly well suited for similarity mea-sures, since it organizes nouns and verbs into hierarchiesof is?a relations.
In version 2.0, there are nine separatenoun hierarchies that include 80,000 concepts, and 554verb hierarchies that are made up of 13,500 concepts.Is?a relations in WordNet do not cross part of speechboundaries, so similarity measures are limited to mak-ing judgments between noun pairs (e.g., cat and dog) andverb pairs (e.g., run and walk).
While WordNet alo in-cludes adjectives and adverbs, these are not organizedinto is?a hierarchies so similarity measures can not beapplied.However, concepts can be related in many ways be-yond being similar to each other.
For example, a wheel isa part of a car, night is the opposite of day, snow is madeup of water, a knife is used to cut bread, and so forth.
Assuch WordNet provides relations beyond is?a, includinghas?part, is?made?of, and is?an?attribute?of.
In addi-tion, each concept is defined by a short gloss that mayinclude an example usage.
All of this information can bebrought to bear in creating measures of relatedness.
Asa result these measures tend to be more flexible, and al-low for relatedness values to be assigned across parts ofspeech (e.g., the verb murder and the noun gun).This paper continues with an overview of the mea-sures supported in WordNet::Similarity, and then pro-vides a brief description of how the package can be used.We close with a summary of research that has employedWordNet::Similarity.2 Similarity MeasuresThree of the six measures of similarity are based on theinformation content of the least common subsumer (LCS)of concepts A and B.
Information content is a measure ofthe specificity of a concept, and the LCS of concepts Aand B is the most specific concept that is an ancestor ofboth A and B.
These measures include res (Resnik, 1995),lin (Lin, 1998), and jcn (Jiang and Conrath, 1997).The lin and jcn measures augment the information con-tent of the LCS with the sum of the information contentof concepts A and B themselves.
The lin measure scalesthe information content of the LCS by this sum, whilejcn takes the difference of this sum and the informationcontent of the LCS.The default source for information content for conceptsis the sense?tagged corpus SemCor.
However, there arealso utility programs available with WordNet::Similaritythat allow a user to compute information content valuesfrom the Brown Corpus, the Penn Treebank, the BritishNational Corpus, or any given corpus of raw text.> similarity.pl --type WordNet::Similarity::lin car#n#2 bus#n#1car#n#2 bus#n#1 0.530371390319309 # railway car versus motor coach> similarity.pl --type WordNet::Similarity::lin car#n bus#ncar#n#1 bus#n#1 0.618486790769613 # automobile versus motor coach> similarity.pl --type WordNet::Similarity::lin --allsenses car#n bus#n#1car#n#1 bus#n#1 0.618486790769613 # automobile versus motor coachcar#n#2 bus#n#1 0.530371390319309 # railway car versus motor coachcar#n#3 bus#n#1 0.208796988315133 # cable car versus motor coachFigure 1: Command Line InterfaceThree similarity measures are based on path lengthsbetween a pair of concepts: lch (Leacock and Chodorow,1998), wup (Wu and Palmer, 1994), and path.
lch findsthe shortest path between two concepts, and scales thatvalue by the maximum path length found in the is?a hi-erarchy in which they occur.
wup finds the depth of theLCS of the concepts, and then scales that by the sum ofthe depths of the individual concepts.
The depth of a con-cept is simply its distance to the root node.
The measurepath is a baseline that is equal to the inverse of the short-est path between two concepts.WordNet::Similarity supports two hypothetical rootnodes that can be turned on and off.
When on, one rootnode subsumes all of the noun concepts, and another sub-sumes all of the verb concepts.
This allows for similaritymeasures to be applied to any pair of nouns or verbs.
Ifthe hypothetical root nodes are off, then concepts mustbe in the same physical hierarchy for a measurement tobe taken.3 Measures of RelatednessMeasures of relatedness are more general in that they canbe made across part of speech boundaries, and they arenot limited to considering is-a relations.
There are threesuch measures in the package: hso (Hirst and St-Onge,1998), lesk (Banerjee and Pedersen, 2003), and vector(Patwardhan, 2003).The hso measures classifies relations in WordNet ashaving direction, and then establishes the relatedness be-tween two concepts A and B by finding a path that isneither too long nor that changes direction too often.The lesk and vector measures incorporate informationfrom WordNet glosses.
The lesk measure finds overlapsbetween the glosses of concepts A and B, as well as con-cepts that are directly linked to A and B.
The vector mea-sure creates a co?occurrence matrix for each word used inthe WordNet glosses from a given corpus, and then repre-sents each gloss/concept with a vector that is the averageof these co?occurrence vectors.4 Using WordNet::SimilarityWordNet::Similarity can be utilized via a command lineinterface provided by the utility program similarity.pl.This allows a user to run the measures interactively.
Inaddition, there is a web interface that is based on thisutility.
WordNet::Similarity can also be embedded withinPerl programs by including it as a module and calling itsmethods.4.1 Command LineThe utility similarity.pl allows a user to measure specificpairs of concepts when given in word#pos#sense form.For example, car#n#3 refers to the third WordNet nounsense of car.
It also allows for the specification of allthe possible senses associated with a word or word#poscombination.For example, in Figure 1, the first command requeststhe value of the lin measure of similarity for the secondnoun sense of car (railway car) and the first noun sense ofbus (motor coach).
The second command will return thescore of the pair of concepts that have the highest similar-ity value for the nouns car and bus.
In the third command,the ?allsenses switch causes the similarity measurementsof all the noun senses of car to be calculated relative tothe first noun sense of bus.4.2 Programming InterfaceWordNet::Similarity is implemented with Perl?s objectoriented features.
It uses the WordNet::QueryData pack-age (Rennie, 2000) to create an object representing Word-Net.
There are a number of methods available that allowfor the inclusion of existing measures in Perl source code,and also for the development of new measures.When an existing measure is to be used, an object ofthat measure must be created via the new() method.
Thenthe getRelatedness() method can be called for a pair ofword senses, and this will return the relatedness value.For example, the program in Figure 2 creates an object ofthe lin measure, and then finds the similarity between the#!/usr/bin/perl -wuse WordNet::QueryData; # use interface to WordNetuse WordNet::Similarity::lin; # use Lin measure$wnObj = new WordNet::QueryData; # create a WordNet object$linObj = new WordNet::Similarity::lin($wnObj); # create a lin object$value = $linObj -> getRelatedness (?car#n#1?, ?bus#n#2?
); # how similar?Figure 2: Programming Interfacefirst sense of the noun car (automobile) and the secondsense of the noun bus (network bus).WordNet::Similarity enables detailed tracing thatshows a variety of diagnostic information specific to eachof the different kinds of measures.
For example, for themeasures that rely on path lengths (lch, wup, path) thetracing shows all the paths found between the concepts.Tracing for the information content measures (res, lin,jcn) includes both the paths between concepts as well asthe least common subsumer.
Tracing for the hso measureshows the actual paths found through WordNet, whilethe tracing for lesk shows the gloss overlaps in Word-Net found for the two concepts and their nearby relatives.The vector tracing shows the word vectors that are usedto create the gloss vector of a concept.5 Software ArchitectureSimilarity.pm is the super class of all modules, and pro-vides general services used by all of the measures such asvalidation of synset identifier input, tracing, and cachingof results.
There are four modules that provide all of thefunctionality required by any of the supported measures:PathFinder.pm, ICFinder.pm, DepthFinder.pm, and LCS-Finder.pm.PathFinder.pm provides getAllPaths(), which finds allof the paths and their lengths between two input synsets,and getShortestPath() which determines the length of theshortest path between two concepts.ICFinder.pm includes the method IC(), which gets theinformation content value of a synset.
probability() andgetFrequency() find the probability and frequency countof a synset based on whatever corpus has been used tocompute information content.
Note that these values arepre?computed, so these methods are simply reading froman information content file.DepthFinder.pm provides methods that read values thathave been pre?computed by the wnDepths.pl utility.
Thisprogram finds the depth of every synset in WordNet,and also shows the is?a hierarchy in which a synset oc-curs.
If a synset has multiple parents, then each possibledepth and home hierarchy is returned.
The depth of asynset is returned by getDepthOfSynset() and getTaxono-myDepth() provides the maximum depth for a given is?ahierarchy.LCSFinder.pm provides methods that find the leastcommon subsumer of two concepts using three differ-ent criteria.
These are necessary since there is multipleinheritance of concepts in WordNet, and different LCScan be selected for a pair of concepts if one or both ofthem have multiple parents in an is?a hiearchy.
getLCS-byIC() chooses the LCS for a pair of concepts that has thehighest information content, getLCSbyDepth() selects theLCS with the greatest depth, and getLCSbyPath() selectsthe LCS that results in the shortest path.6 Related WorkOur work with measures of semantic similarity and relat-edness began while adapting the Lesk Algorithm for wordsense disambiguation to WordNet (Banerjee and Peder-sen, 2002).
That evolved in a generalized approach todisambiguation based on semantic relatedness (Patward-han et al, 2003) that is implemented in the SenseRe-late package (http://senserelate.sourceforge.net), whichutilizes WordNet::Similarity.
The premise behind this al-gorithm is that the sense of a word can be determined byfinding which of its senses is most related to the possiblesenses of its neighbors.WordNet::Similarity has been used by a number ofother researchers in an interesting array of domains.
(Zhang et al, 2003) use it as a source of semantic fea-tures for identifying cross?document structural relation-ships between pairs of sentences found in related docu-ments.
(McCarthy et al, 2004) use it in conjunction witha thesaurus derived from raw text in order to automati-cally identify the predominent sense of a word.
(Jarmaszand Szpakowicz, 2003) compares measures of similarityderived from WordNet and Roget?s Thesaurus.
The com-parisons are based on correlation with human related-ness values, as well as the TOEFL synonym identificationtasks.
(Baldwin et al, 2003) use WordNet::Similarity toprovide an evaluation tool for multiword expressions thatare identified via Latent Semantic Analysis.
(Diab, 2003)combines a number of similarity measures that are thenused as a feature in the disambiguation of verb senses.7 AvailabilityWordNet::Similarity is written in Perl and is freely dis-tributed under the Gnu Public License.
It is avail-able from the Comprehensive Perl Archive Network(http://search.cpan.org/dist/WordNet-Similarity) and viaSourceForge, an Open Source development platform(http://wn-similarity.sourceforge.net).8 AcknowledgementsWordNet::Similarity was preceeded by the distance.plprogram, which was released in June 2002.
This wasconverted into the object oriented WordNet::Similaritypackage, which was first released in April 2003 as ver-sion 0.03.
The most current version as of this writing is0.07, which was released in March 2004.The distance.pl program and all versions of Word-Net::Similarity up to and including 0.06 were designedand implemented by Siddharth Patwardhan as a part ofhis Master?s thesis at the University of Minnesota, Du-luth.
Version 0.07 was designed and implemented by Ja-son Michelizzi as a part of his Master?s thesis.The lesk measure in WordNet::Similarity was origi-nally designed and implemented by Satanjeev Banerjee,who developed this measure as a part of his Master?sthesis at the University of Minnesota, Duluth.
There-after Siddharth Patwardhan ported this measure to Word-Net::Similarity.This work has been partially supported by a NationalScience Foundation Faculty Early CAREER Develop-ment award (#0092784), and by a Grant-in-Aid of Re-search, Artistry and Scholarship from the Office of theVice President for Research and the Dean of the Gradu-ate School of the University of Minnesota.ReferencesT.
Baldwin, C. Bannard, T. Tanaka, and D. Widdows.2003.
An empirical model of multiword expressiondecomposability.
In Proceedings of the of the ACL-2003 Workshop on Multiword Expressions: Analy-sis, Acquisition and Treatment, pages 89?96, Sapporo,Japan.S.
Banerjee and T. Pedersen.
2002.
An adapted Leskalgorithm for word sense disambiguation using Word-Net.
In Proceedings of the Third International Confer-ence on Intelligent Text Processing and ComputationalLinguistics, pages 136?145, Mexico City, February.S.
Banerjee and T. Pedersen.
2003.
Extended gloss over-laps as a measure of semantic relatedness.
In Proceed-ings of the Eighteenth International Joint Conferenceon Artificial Intelligence, pages 805?810, Acapulco,August.M.
Diab.
2003.
Word Sense Disambiguation within aMultilingual Framework.
Ph.D. thesis, The Universityof Maryland.G.
Hirst and D. St-Onge.
1998.
Lexical chains as repre-sentations of context for the detection and correctionof malapropisms.
In C. Fellbaum, editor, WordNet:An electronic lexical database, pages 305?332.
MITPress.M.
Jarmasz and S. Szpakowicz.
2003.
Roget?s thesaurusand semantic similarity.
In Proceedings of the Con-ference on Recent Advances in Natural Language Pro-cessing, pages 212?219, Borovets, Bulgaria.J.
Jiang and D. Conrath.
1997.
Semantic similarity basedon corpus statistics and lexical taxonomy.
In Proceed-ings on International Conference on Research in Com-putational Linguistics, pages 19?33, Taiwan.C.
Leacock and M. Chodorow.
1998.
Combining localcontext and WordNet similarity for word sense identifi-cation.
In C. Fellbaum, editor, WordNet: An electroniclexical database, pages 265?283.
MIT Press.D.
Lin.
1998.
An information-theoretic definition ofsimilarity.
In Proceedings of the International Con-ference on Machine Learning, Madison, August.D.
McCarthy, R. Koeling, and J. Weeds.
2004.
Rank-ing WordNet senses automatically.
Technical ReportCSRP 569, University of Sussex, January.S.
Patwardhan, S. Banerjee, and T. Pedersen.
2003.
Us-ing measures of semantic relatedness for word sensedisambiguation.
In Proceedings of the Fourth Interna-tional Conference on Intelligent Text Processing andComputational Linguistics, pages 241?257, MexicoCity, February.S.
Patwardhan.
2003.
Incorporating dictionary and cor-pus information into a context vector measure of se-mantic relatedness.
Master?s thesis, University of Min-nesota, Duluth, August.J.
Rennie.
2000.
WordNet::QueryData: a Perlmodule for accessing the WordNet database.http://www.ai.mit.edu/people/jrennie/WordNet.P.
Resnik.
1995.
Using information content to evaluatesemantic similarity in a taxonomy.
In Proceedings ofthe 14th International Joint Conference on ArtificialIntelligence, pages 448?453, Montreal, August.Z.
Wu and M. Palmer.
1994.
Verb semantics and lexi-cal selection.
In 32nd Annual Meeting of the Associ-ation for Computational Linguistics, pages 133?138,Las Cruces, New Mexico.Z.
Zhang, J. Otterbacher, and D. Radev.
2003.
Learningcross-document structural relationships using boost-ing.
In Proceedings of the 12th International Con-ference on Information and Knowledge Management,pages 124?130.
