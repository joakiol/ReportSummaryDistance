Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 45?53,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPDesigning a Citation-Sensitive Research Tool:An Initial Study of Browsing-Specific Information NeedsStephen Wan?, Ce?cile Paris?,?
ICT Centre,CSIRO, AustraliaFirstname.Lastname@csiro.auMichael Muthukrishna?, Robert Dale?
?Centre for Language TechnologyFaculty of ScienceMacquarie University, Australiardale@science.mq.edu.auAbstractPractitioners and researchers need to stayup-to-date with the latest advances intheir fields, but the constant growth inthe amount of literature available makesthis task increasingly difficult.
We in-vestigated the literature browsing task viaa user requirements analysis, and identi-fied the information needs that biomed-ical researchers commonly encounter inthis application scenario.
Our analysis re-veals that a number of literature-based re-search tasks are preformed which can beserved by both generic and contextuallytailored preview summaries.
Based on thisstudy, we describe the design of an im-plemented literature browsing support toolwhich helps readers of scientific literaturedecide whether or not to pursue and read acited document.
We present findings froma preliminary user evaluation, suggestingthat our prototype helps users make rele-vance judgements about cited documents.1 IntroductionPractitioners and researchers in all fields facea great challenge in attempting to keep up-to-date with the literature relevant to their work.In this context, search engines provide a usefultool for information discovery; but search is justone modality for gathering information.
We alsoregularly read through documents and expect tofind additional relevant information in referenced(cited or hyperlinked) documents.
This results ina browsing-based activity, where we explore con-nections through related documents.This browsing behaviour is increasingly sup-ported today as publishers of scientific materialdeliver hyperlinked documents via a variety ofmedia including Adobe?s Portable Document For-mat (PDF) as well as the more conventional webhypertext format.
Given appropriate documentdatabases and knowledge of referencing conven-tions, it is relatively straightforward to supportthe automatic downloading of cited documents:such functionality already exists within referencemanagers such as JabRef 1 and Sente2.
This?blind downloading?, however, does not addressthe question of the relevancy of the linked docu-ment for the reader at the time of reading.
Apartfrom the publication details of the reference andthe citation context, readers are provided with verylittle information on the basis of which to de-termine whether the cited document is worth ex-ploring more thoroughly.
Given the potentiallylarge number of citations that may be encountered,this results in the following browsing-specific sce-nario: how can we help a user quickly determinewhether the cited document is indeed worth down-loading, perhaps paying for, and reading?In the study presented here, we focussed on theneeds of biomedical researchers, who are oftentime-poor and yet apparently spend 18% of theirtime gathering and reviewing information (Hersh,2008).
They regularly search through reposito-ries of online scholarly literature to update theirexpert knowledge; in this domain, the penalty fornot staying up-to-date with the latest advances canbe severe, potentially affecting medical experi-ments.
In our work, we found that two thirds of re-searchers regularly engaged in browsing scientificliterature.
Given the prevalent use of the browsingmodality, we believe that novel research tools areneeded to help readers make decisions about therelevance of cited material.To better understand the user?s informationneeds that arise when reading and browsingthrough academic literature, and to ascertain whatNLP techniques we might be able to use tohelp support them, we conducted a user require-1jabref.sourceforge.net2www.thirdstreetsoftware.com45ments analysis.
It revealed a number of commonproblems faced by readers of scientific literature.These served to focus our efforts in designing andimplementing a browsing support tool for scien-tific literature, referred to here as CSIBS.CSIBS helps readers decide which cited docu-ments to read by providing them with informationwhich is useful at the point when citations are en-countered.
The application provides informationabout the cited document and identifies importantsentences in that document, based on the user?scurrent reading context.
The key observation hereis that the reading context can indicate why thereader might be interested in the cited document.In addition to meta-data about the cited document,and its abstract, a contextualised preview is shownwithin the same browser in which the citing docu-ment is being viewed (for example, Adobe Acro-bat Reader or a web browser), thus avoiding aninterruption to the user?s primary reading activ-ity.
This contextualised preview contains impor-tant sentences from the cited document that are re-lated to the reading context.We present related work on understanding in-formation needs in Section 2; we outline our userrequirements analysis in the domain of scientificliterature in Section 3; and the results of the analy-sis and our understanding of the browsing-specificinformation needs are presented in Section 4.
InSection 5, we describe a tool developed to meetthe most pressing of these information needs.
Sec-tion 6 presents a feedback from an initial evalua-tion.
We conclude by discussing our overall find-ings in Section 7.2 Related Work2.1 Information NeedsExisting work on information needs, beginningwith Taylor (1962), typically focuses on mappingfrom a particular query to the underlying inter-est of the user.
In a recent example of suchwork, Henrich and Luedecke (2007) describesmethods for constructing lists of domain-specifickey words which may correspond well to userinterests.
However, we are interested in relat-ing information needs to user tasks in scenariosin which there is no explicit query, as in Bystrmet al (1995); in particular, our work focuses onbrowsing scenarios.
Toms (2000) presents a studyof browsing behaviour over electronic texts andexamines the differences between searching andbrowsing.
In that work, browsing is performedacross multiple news articles where the links be-tween articles are inferred based on topic simi-larity.
In contrast, we consider explicit hyper-text links which are linguistically embedded in thedocument as citations, where the embedding textserves as link anchors.2.2 Information Needs in BiomedicineEly et al (2000) present an overview of the infor-mation needs of practicing clinicians, deriving aset of commonly asked questions.
Although weare interested in doctors as users, the type of in-formation needs presented in this paper relate tothe activity of conducting scientific investigation,rather than that of treating a patient.Task-based analyses of the biomedical domainhave been studied by Bartlett and Neugebauer(2008) and Tran et al (2004).
Their analyses, likeours, are task-based and use qualitative studies touncover the underlying uses of information.
How-ever, the tasks outlined in these related works arefocused on a specific set of information needs in aresearch area: for example, the determination of afunctional analysis of gene sequences.
Our workdiffers in that we wish to take a more general viewin order to elicit information needs to do with sci-entific research, at least at the level of biomedicalsciences.The information needs and tasks of academicusers have been studied previously by Belkin(1994), who focuses on scholarly publications inthe humanities domain.
We perform an investi-gation along similar lines, but with a focus onacademic literature used to conduct scientific re-search.2.3 Using Scientific LiteratureThe genre of academic literature, and the devel-opment of technologies to support researchers asusers, has been studied by several groups work-ing in automatic text summarisation.
Teufel andMoens (2002) describe a summarisation approachthat extracts text from documents and highlightsthe rhetorical role that an extract plays withinthe originating document (for example, stating theAim of an experiment).
Qazvinian and Radev(2008) present an approach to summarising aca-demic documents based on finding citation con-texts in the entire set of published literature for thedocument in question.
Both approaches, however,treat the cited document in isolation of the read-46ing context and do not actively support the readingtask.3 Understanding How ResearchersBrowse through Scientific LiteratureTo determine what readers of scientific literaturewant to know about cited documents, we con-ducted a user requirements analysis.
Our methodis based on Grounded Theory (Glaser and Strauss,1967), a commonly used approach in HumanComputer Interaction (Corbin and Strauss, 2008).We began by interviewing subjects from an appro-priate user demographic and recording their verbaldescriptions about a real scenario situated in theirday-to-day activities.
Following this, we designeda questionnaire for wider participation which pre-sented scenario-based questions attempting to un-cover their information needs and tasks.
Partic-ipants were asked to provide free text answers.The responses were then collated and analysed forcommonalities, bringing to the fore those issuesthat were salient across the participants.
We reporton the questionnaire design and responses in thispaper.Beginning with such a study can reduce therisk of building tools that have only limited util-ity.
This is particularly true of new and less un-derstood application scenarios, such as the one ex-plored here.3.1 Questionnaire DesignAn online questionnaire was used to reach par-ticipants who actively read academic literature.3To encourage participation, the questionnaire waslimited to 10 questions, which were formulated in-dependently of any particular scientific domain.We were explicit about the aims of the question-naire by providing an initial brief, stating that thefeedback from participants would be used to de-velop new tools for browsing through scientific lit-erature.
Within the questionnaire, to prepare par-ticipants for our scenario-based questions, the firstfew questions were basic and concerned the gen-eral usage of scientific literature.
For example,we asked about the high-level reasons for whichthey used scientific literature (e.g., ?To learn abouta new topic?
; ?To update your knowledge on aparticular topic?).
Participants could also specify3The online questionnaire tool, SurveyMonkey(www.surveymonkey.com), was used to implementthe questionnaire as an online interactive form.their own reasons.
In addition, we also asked themabout the frequency of their literature browsing ac-tivity.The main section of the questionnaire consistedof a series of questions, corresponding to the is-sues we wanted to explore:1.
What information needs do researchers haveof a cited document, and what specific tasksdoes this information serve?2.
What makes it difficult for researchers to findthe answers to their questions about citeddocuments?3.
What tasks are potential targets for automa-tion?Questions were to be answered with free textresponses, focussed by presenting a scenario inwhich the researcher encounters a citation whilstreading a scientific publication.
The first questionabove aims to better understand the researchers?information needs and tasks; the second and thirdare concerned with ideas for potential applicationswhich could benefit from NLP and IR research.To address the first research issue, participantswere asked to recall a recent experience in which,while reading a publication, they had encountereda citation.
Within this context, participants wereasked to describe what questions they may havehad of the cited document.
To clarify how thesequestions relate to a specific context of use, re-spondents were then asked to relate the questionsthey identified back to some task undertaken aspart of their research work.Responses regarding the difficulties encoun-tered in satisfying information needs were col-lected with respect to the participants earlier re-sponses.
So as to not bias the participant, thequestion was phrased neutrally.
We asked what as-pects of scientific literature and current technologymade it easy or hard to find answers to the partic-ipants?
personal research questions.
We examinedresponses with the aim of determining how tech-nology might reduce the burden of knowledge dis-covery.
Responses were again focused by usingthe same scenario as in the previous question.The third research issue was explored via twoseparate questions.
The first presented the partici-pants with a scenario in which they had access toa non-expert human assistant who could performone or more simple tasks identified in their ear-lier responses; they were then asked what kinds47of tasks they would delegate to such an assistant.A second, more direct, question was presented re-quiring participants to describe which tools theywould like to use, or to suggest new tools thatwould help them in the future, when it came tobrowsing through scientific literature.Finally, optional questions about the partici-pants?
research backgrounds were presented at theend of the questionnaire.
These were deliberatelyplaced last to reduce barriers to completion.4 Questionnaire Data Analysis4.1 Analysing the ResultsWe recruited users with a background in biomed-ical life sciences since we had access to an ex-tensive corpus of documents in this domain withwhich to build some kind of application.
Note,however, that our questions were not specific tothis domain, and the questionnaire could poten-tially be re-run with participants from a differentscientific background.We contacted 36 users who might be interestedin life sciences publications.
Of these, 24 partici-pants started the questionnaire, and 18 completedit.
Of the 24 participants, two thirds indicated thatthey browsed through academic literature at leastonce a week.The written responses were separately analysedby three of the authors.
Responses to each ques-tion were examined, checking for repeated termsand concepts that could form the basis of clus-tering.
Salient information needs were matchedto corresponding tasks, and commonly mentionedareas of difficulty and suggestions for delega-tion were grouped.
Once each author had per-formed his or her own analysis, the salient group-ings for each question were collaboratively deter-mined, consolidating the three analyses performedin isolation.
The most salient groupings were thenexamined for potential tasks that might be auto-mated.4.2 Questionnaire DataWe now present the results of the analysis.
Theseare organised with respect to each of the three re-search issues.4.2.1 Questions of the Cited DocumentFigure 1 presents the most frequently indicated in-formation needs and the most frequent tasks thatwere identified.
The information needs can beInformation Needs Freq[md] About accessing the full text 9[co] Article details (Definition, Methods, Results) 7[md] About the authors 6[md] About the publication date 5[co] About relevance to own work 4[md] The abstract 3[co] The references 3Participant Task FreqDeciding whether to believe the citation 4Finding baselines for experiments 3Comparing own ideas to article 3Finding information to justify the citation 3Finding information about methods 2Finding additional references 2Updating clinical knowledge 2Conducting a survey of the literature 2Identifying key researchers in the field 2Updating research knowledge 2Figure 1: Principal information needs and tasks ofparticipants with regard to citations.
In the firsttable, information needs are prefixed by ?md?
formeta-data and ?co?
for content-oriented.
?Freq?
in-dicates the number of occurrences in the results.grouped into two main categories.
The first, whichwe refer to as meta-data needs, refers to informa-tion about the document external to the documentcontent itself.
These needs could be met by a se-ries of database queries about the document, in-volving, for example, the author information andthe citation counts for the document.
We notethat, often, the abstract can also be retrieved viaa database query (and thus does not require anyin-depth text analysis of the cited document), al-though technically this is not meta-data.
In termsof the underlying task, this kind of generic infor-mation may be used in deciding whether to trustthe cited source.The second category of information needs,which we refer to as being content-oriented, canbe met by providing information sourced fromwithin the cited document.
This type of informa-tion facilitates multiple tasks.
For example, thesemight include understanding why a document wascited, or finding new baselines to design new ex-periments.
We refer to these tasks in general ascitation-focused, as some underlying informationneed is triggered by the text that the participant hasjust read, whether this is for advancing one?s un-derstanding of a topic, or pursuing a specific lineof scientific inquiry.484.2.2 Difficulties in Finding AnswersThis question required participants to voluntarilyreflect on their own research practices, a processthat is influenced partially by their expertise inresearch and their exposure to different researchtools.
Some responses described features of soft-ware that were appealing, while others related tothe difficulties faced by researchers in finding rel-evant information.
In this paper, we present onlythe subset of responses that concern the difficultiesencountered, since this will influence the function-ality of new research tools.
These responses arepresented in Figure 2.Difficulties FreqFinding the exact text to justify the citation 3Poor writing 2Comparing documents 1Resolving references to the same object 1Figure 2: Difficulties in finding information.In general, the difficulties concerned some kindof analysis of text.
We note that these tasksare largely citation-focused, requiring content-oriented information.
Examples of comments re-garding this task are presented in Figure 3.
For ex-ample, participants wanted to know how the citeddocument compared the citing document from theperspective of experimental design.
However, thecitation-focused task that was most commonlymentioned as difficult was that of justifying cita-tions.
Participants mentioned that reading throughthe entire cited document for this purpose was atedious task, particularly when looking for infor-mation in poorly written documents.4.2.3 Tasks for AutomationOur analysis of responses to the task automationquestions revealed two interesting outcomes: del-egation occurred often with the use of key words,and participants expressed the need for tools toexpress relationships between domain concepts.These are presented in Figure 4.Responses to the question regarding task del-egation revealed that for research-oriented tasks,participants felt the need to direct assistantsthrough the use of key words.
This is consistentto responses to earlier questions detailing whataspects of current technology were attractive, in-cluding user interface conventions such as keyword highlighting.
Otherwise, the other reportedCitation usually does not include the position of the informa-tion in the cited article .
.
.
it might be necessary to read all ofthe article to find it in another reference and so on.If the first report was only citing the second report for a smallpiece of information, that information may be hard to locatein the second report.The original reference may have just cited a very small com-ponent of the second report, either just a comment made inthe discussion or a supplemental figure .
.
.
It may take a whileto locate and justify the citation if it isn?t the major finding ofthe report.If I see a citation in a report that I am interested in, I gen-erally want to know if the cited report actually supports thestatement in the original report.
Very often ?
way too often ?citations do not.
For all important citations I track down theoriginal cited work and verify that it actually says what it issupposed to.Figure 3: Some sample responses from users withregard to justifying citations; emphases added.Automation Possibilities FreqSearch cited document for key words 4Search for further publications using key words 3Refine search using related concepts 6Figure 4: Potential candidates for a new researchtool.delegated task was that of simple database entry ofpublication records.
We interpret these responsesas indicating that participants are not overly will-ing to hand over responsibility for complex tasksto assistants.
If delegation of more research-oriented activities occurs, participants want tounderstand how and why results were obtained.While responses were made assuming delegationto human assistants, we believe that such issuesare even more crucial for results obtained via au-tomated means.Suggested novel features centered upon a bet-ter representation of relationships between do-main concepts to be used for query refinement.Responses included expressions such as ?refinedsearch?, a handling of user-specified ?mind maps?
(for repeated searches), and the use of ?trails?
ex-plaining how results connected to search terms,key words and the author.5 Prototype RequirementsAs a result of these findings, we chose to build atool that meets the two types of information needsrevealed in the initial user requirements study.
The49purpose of the resulting tool, CSIBS, is to helpreaders prioritise which cited documents are worthspending time to download and read further.
Inthis way, CSIBS helps readers to browse and nav-igate through a dense network of cited documents.To facilitate this task in accordance with theelicited user requirements, CSIBS produces analternate version of a published article that hasbeen prepared with pop-up previews of cited doc-uments.
Each preview contains meta-data, the ab-stract and content-oriented information.
It is pro-vided to the user to help perform research tasksthat arise as a consequence of encountering a cita-tion and needing to investigate further.
The pre-view is not intended to serve as a surrogate forthe cited document.
Rather, it is aimed at help-ing readers make relevance judgements about ci-tations.The meta-data helps the user to appraise the ci-tation and to make a value judgement about thework cited.
The abstract provides a generic sum-mary of the cited document, indicating the scopeof the work cited.
The content-oriented informa-tion supports any citation-focused tasks, for exam-ple citation justification, through the provision ofdetailed information sourced from within the citeddocument.
We refer to this as a ContextualisedPreview.
It is constructed using automatic textsummarisation techniques that tailor the resultingsummary to the user?s current interests, here ap-proximately represented by the citation context:that is, the sentence in which the citation is lin-guistically embedded.
We briefly describe CSIBS,in this section; for a full description, see Wan et al(2009).Each preview appears in a pop-up text box ac-tivated by moving the mouse over the citation.The specific interaction (a double click versus a?mouse-over?)
depends on whether the article isdisplayed via a web browser or as a PDF docu-ment.
Figure 5 shows the resulting pop-up for thePDF display.5.1 A Meta-Data Summary and AbstractParticipants often wanted a generic summary out-lining the overall scope and contributions of thecited work.
This is typically available via the ab-stract.
Additionally, CSIBS presents a variety ofmeta-data returned from queries to an online pub-lications database:44www.embase.com?
The full reference: This provides readerswith the date of publication and the journaltitle, amongst other things.?
Author Information: CSIBS can include datato help the reader establish a level of trustin the citation, primarily focusing on infor-mation about the authors?
affiliations and thenumber of related citations in the researcharea.?
The citation count for the cited document:Participants indicated that this was useful inappraising the cited article.These pieces of information were commonly iden-tified as useful in helping readers make valuejudgements about the cited work.
This is perhapsan artifact of the biomedical domain, where re-search has a critical nature and concerns healthand medical issues.5.2 A Contextualised PreviewTo generate the contextualised preview of the citeddocument, the system finds the set of sentencesthat relate to the citation context, employing ap-proaches for summarising documents that exploitanchor text (Wan and Paris, 2008).
FollowingSpark Jones (1998), we specify the purpose of thecontextualised summary along particular dimen-sions, indicated here in italics:?
The situation is tied to a particular context ofuse: an in-browser summary triggered by acitation and its citing context.?
An audience of expert researchers is as-sumed.?
The intended usage of the summary is one ofpreview.
We assume that the reader is makinga relevance judgement as to whether or not todownload (and, if necessary, buy) the citeddocument.
Specifically, the information pre-sented should help the reader determine thelevel of trust to place in the document, un-derstand why the article is cited, and decidewhether or not to read it.?
The summary is intended only to providea partial coverage of the whole document,specifically focused on content that directlyrelates to the citation context.?
The style of the summary is intended to beindicative.
That is, it should present specific50Figure 5: A sample pop-up with an automatically generated summary, triggered by a mouse action overthe citation.
Extracted sentences are grouped together by section titles.
Words that match with thecitation context are coloured and emboldened.details to facilitate a relevance judgement, al-lowing the user to determine if the cited docu-ment can be used to source more informationon a topic, as opposed to just mentioning it inpassing.To create the preview summary, the cited docu-ment is downloaded from a publisher?s database5in its XML form and then segmented into sec-tions, paragraphs and sentences.
Each sentence inthe cited document is compared with the citationcontext in order to find the best justification sen-tences for that particular citation.
Due to the lim-ited space available in the pop-up, the number ofextracted sentences is capped at a predefined limit,currently set to four.
Using vector space methods(Salton and McGill, 1983) weighted with term fre-quency (and omitting stop words), the best match-ing sentence is defined as the one scoring the high-est on the cosine similarity metric with the citationcontext.
The attractiveness of this approach liesin its simplicity, resulting in a fast computation of5www.sciencedirect.coma preview (?
0.03 seconds), making the processamenable to batch processing of multiple docu-ments or, in the future, live generation of previewsat runtime.
To help with the readability of the re-sulting preview, the system also extracts structuralinformation from the cited document.
In particu-lar, for each extracted sentence, the system identi-fies the section in which it belongs; the extractedsentences are then grouped by section, and pre-sented with their section headings, as illustrated inFigure 5.CSIBS focuses on returning precise results, sothat the system does not exacerbate any existinginformation overload problems by burdening thereader with poorly matching sentences.
To achievethis, we currently use exact matches to words inthe citation context; in on-going work, we are ex-ploring methods to relax this constraint withouthurting performance.
In line with our user require-ments analysis, we have designed the tool so thatthe user is able to easily see how the summary wasconstructed.
Matching tokens are highlighted, al-lowing the reader to understand why specific sen-51tences were extracted.6 Initial Feedback6.1 Evaluation OverviewWe built a prototype version of CSIBS and con-ducted a preliminary qualitative evaluation.
Thegoal was to examine how participants would reactto the pop-up previews.
The feedback allows us tofurther clarify our analysis and subsequent devel-opment.We asked participants to view a number of pop-up previews in order to answer the question: Isthe Citation Justified?
This was one of the moredifficult questions that researchers found challeng-ing when making a relevancy judgement.
The ac-tual judgements are not important in this evalua-tion.
Instead, we gauged the reported utility of theprototype based on the participants?
self-reportedconfidence when performing the task.
To capturethis information, participants were asked to scoretheir confidence on a 3-point Likert scale.Three biomedical researchers, all of whom hadtaken part in our original user requirements analy-sis, participated in the evaluation.
Each participantwas shown nine different passages containing a ci-tation context, each situated in a different FEBSLetters6 publication (which was also presented infull to the participants).
At each viewing of a ci-tation context, two supporting texts were providedwith which the participant was asked to answer thecitation justification question.
For all participants,the first supporting text was produced by a base-line system that simply provided the full referenceof the citation.
The second was either the abstractor the contextualised preview, which in this eval-uation was limited to three sentences.
Meta-datawas not presented for this study as we specificallywanted feedback on the citation justification task.The small sample size does not permit hypoth-esis testing.
However, we are encouraged by thecomparable positive gains in self-reported confi-dence scores (Abstract: +1.2 versus CSIBS: +2.2)compared to simply showing the full reference.Since both preview types were positive, we as-sume that these types of information facilitated therelevance judgements.
Participants also reportedthat, for the contextualised preview, 2 out of 3 sen-tences were found to be useful on average.6The journal of the Federation of Europeans BiochemicalSocieties.The qualitative feedback also supported CSIBS.One participant made some particularly interest-ing observations regarding selected sentences andthe structure of the cited document.
Specifically,useful sentences tended to be located deeper in thecited document, for example in the methods sec-tions This participant suggested that, for an expertuser, showing sentences from the earlier sectionsof a publication was not useful; for the same rea-son, the abstract might be too general and not help-ful in justifying a citation.
Finally, this participantremarked that, in those situations where each doc-ument downloaded from a proprietary repositoryincurs a fee, the citation-sensitive previews wouldbe very useful in deciding whether to downloadthe document.7 ConclusionsIn this paper, we presented an analysis ofbrowsing-specific information needs in the do-main of scientific literature.
In this context, usershave information needs that are not realised assearch queries; rather these remain implicit in theminds of users as they browse through hyperlinkeddocuments.
Our analysis sheds light on these in-formation needs, and the tasks being performed intheir pursuit, using a set of scenario-based ques-tions.The analysis revealed two tasks often performedby participants: the appraisal task and the citation-focused task.
CSIBS was designed to support theunderlying needs by providing meta-data informa-tion, the abstract, and a contextualised preview foreach citation.
The user requirement of search re-finement was not directly addressed in this work,but could be met by techniques of query refine-ment in IR, synonym-based expansion in sum-marisation, and of course, additional user speci-fied key terms.
In future work, we will explorethese possibilities.
Our results to date are encour-aging for the use of NLP techniques to supportreaders prioritise which cited documents to readwhen browsing through scientific literature.AcknowledgmentsWe would like to thank all the participants whotook part in our study.
We would also like to thankJulien Blondeau and Ilya Anisimoff, who helpedto implement the prototype.52ReferencesJoan C. Bartlett and Tomasz Neugebauer.
2008.
Atask-based information retrieval interface to supportbioinformatics analysis.
In IIiX ?08: Proceedings ofthe second international symposium on Informationinteraction in context, pages 97?101, New York, NY,USA.
ACM.Nicholas J. Belkin.
1994.
Design principles forelectronic textual resources: Investigating users anduses of scholarly information.
In Current Issues inComputational Linguistics: In Honour of DonaldWalker.Kluwer, pages 1?18.
Kluwer.Katriina Bystrm, Katriina Murtonen, Kalervo Jrvelin,Kalervo Jrvelin, and Kalervo Jrvelin.
1995.
Taskcomplexity affects information seeking and use.In Information Processing and Management, pages191?213.Juliet Corbin and Anselm L. Strauss.
2008.
Basics ofqualitative research : techniques and procedures fordeveloping grounded theory.
Sage, 3rd edition.John W Ely, Jerome A Osheroff, Paul N Gorman,Mark H Ebell, M Lee Chambliss, Eric A Pifer, andP Zoe Stavri.
2000.
A taxonomy of generic clini-cal questions: classification study.
British MedicalJournal, 321:429?432.Barney G. Glaser and Anselm L. Strauss.
1967.
TheDiscovery of Grounded Theory: Strategies for Qual-itative Research.
Aldine de Gruyter, New York.Andreas Henrich and Volker Luedecke.
2007.
Char-acteristics of geographic information needs.
In GIR?07: Proceedings of the 4th ACM workshop on Ge-ographical information retrieval, pages 1?6, NewYork, NY, USA.
ACM.W.
R. Hersh.
2008.
Information Retrieval.
Springer.Information Retrieval for biomedical researchers.Vahed Qazvinian and Dragomir R. Radev.
2008.
Sci-entific paper summarization using citation summarynetworks.
In The 22nd International Conference onComputational Linguistics (COLING 2008), Mach-ester, UK, August.G.
Salton and M. J. McGill.
1983.
Introduction tomodern information retrieval.
McGraw-Hill, NewYork.Karen Spark Jones.
1998.
Automatic summarizing:factors and directions.
In I. Mani and M. Maybury,editors, Advances in Automatic Text Summarisation.MIT Press, Cambridge MA.Robert S Taylor.
1962.
Process of asking questions.American Documentation, 13:391?396, October.Simone Teufel and Marc Moens.
2002.
Summa-rizing scientific articles: experiments with rele-vance and rhetorical status.
Computional Linguis-tics, 28(4):409?445.Elaine G. Toms.
2000.
Understanding and facilitatingthe browsing of electronic text.
International Jour-nal of Human-Computing Studies, 52(3):423?452.D Tran, C Dubay, P Gorman, and W. Hersh.
2004.
Ap-plying task analysis to describe and facilitate bioin-formatics tasks.
Studies in Health Technology andInformatics, 107107(Pt 2):818?22.Stephen Wan and Ce?cile Paris.
2008.
In-browser sum-marisation: Generating elaborative summaries bi-ased towards the reading context.
In The 46th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies: ShortPaper, Columbus, Ohio, June.Stephen Wan, Ce?cile Paris, and Robert Dale.
2009.Whetting the appetite of scientists: Producing sum-maries tailored to the citation context.
In Proceed-ings of the Joint Conference on Digital Libraries.53
