Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 993?998,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsTowards Semi-Automatic Generation of Proposition Banks forLow-Resource LanguagesAlan AkbikIBM ResearchAlmaden Research CenterSan Jose, CA 95120Vishwajeet KumarIIT BombayCS and EngineeeringMumbai, India{akbika,yunyaoli}@us.ibm.com vishwajeetkumar86@gmail.comYunyao LiIBM ResearchAlmaden Research CenterSan Jose, CA 95120AbstractAnnotation projection based on parallel cor-pora has shown great promise in inexpensivelycreating Proposition Banks for languages forwhich high-quality parallel corpora and syn-tactic parsers are available.
In this paper, wepresent an experimental study where we ap-ply this approach to three languages that lacksuch resources: Tamil, Bengali and Malay-alam.
We find an average quality differenceof 6 to 20 absolute F-measure points vis-a-vis high-resource languages, which indicatesthat annotation projection alone is insufficientin low-resource scenarios.
Based on these re-sults, we explore the possibility of using an-notation projection as a starting point for in-expensive data curation involving both expertsand non-experts.
We give an outline of whatsuch a process may look like and present aninitial study to discuss its potential and chal-lenges.1 IntroductionCreating syntactically and semantically annotatedNLP resources for low-resource languages is knownto be immensely costly.
For instance, the Proposi-tion Bank (Palmer et al, 2005) was created by an-notating predicate-argument structures in the PennTreebank (Marcus et al, 1993) with shallow seman-tic labels: frame labels for verbal predicates and rolelabels for arguments.
Similarly, the SALSA (Bur-chardt et al, 2006) resource added FrameNet-styleannotations to the TIGER Treebank (Brants et al,2002), the Chinese Propbank (Xue, 2008) is builton the Chinese Treebank (Xue et al, 2005), andMy  father   bought   a  housemy father a house bought??
????
??
????
??????
?A0A0buy.01 A1A1 buy.01Figure 1: Annotation projection on a pair of very simple sen-tences.
English Propbank frame (buy.01) and role (A0, A1)labels are projected onto aligned Tamil words.
Furthermore,the typed dependencies between the words ?my father?
and ?ahouse?
(dotted lines) are projected onto their Tamil equivalents.so forth.
Since each such layer of annotation typ-ically requires years of manual work, the accumu-lated costs can be prohibitive for low-resource lan-guages.Recent work on annotation projection offers away to inexpensively label a target language corpuswith linguistic annotation (Pado?
and Lapata, 2009).This only requires a word-aligned parallel corpus oflabeled English sentences and their translations inthe target language.
English labels are then auto-matically projected onto the aligned target languagewords.
Refer to Figure 1 for an example.Low-resource languages.
However, previous workthat investigated Propbank annotation projection hasfocused only on languages for which treebanks - andtherefore syntactic parsers - already exist.
Since syn-tactic information is typically used to increase pro-jection accuracy (Pado?
and Lapata, 2009; Akbik etal., 2015), we must expect this approach to workless well for low-resource languages.
In addition,low-resource languages have fewer sources of high-993semantic labels(predicates + roles)ENunlabeled corpusTLParallel corpussemantic labels(projected, noisy)AnnotationprojectionTLAnnotation projection Crowdsourced and expert data curationCrowdagrees?InputCrowdsourceddata curationsemantic labels(crowd cannot curate)semantic labels(curated, final)TLTLExpert datacurationyesnoFigure 2: Proposed process of using annotation projection in a parallel corpus from English (EN) to a target language (TL) as basisfor crowdsourced data curation.
Experts are only involved in cases where the crowd cannot agree on a label.quality parallel data available, further complicatingannotation projection.Contributions.
In this paper, we present a study inwhich we apply annotation projection to three low-resource languages in order to quantify the differ-ence in precision and recall vis-a-vis high-resourcelanguages.
Our study finds overall F1-measure ofgenerated Proposition Banks to be significantly be-low state-of-the-art results, leading us to concludethat annotation projection may at best be a startingpoint for the generation of semantic resources forlow-resource languages.
To explore this idea, weoutline a potential semi-automatic process in whichwe use crowdsourced data curation and limited ex-pert involvement to confirm and correct automati-cally projected labels.
Based on this initial study, wediscuss the potential and challenges of the proposedapproach.2 Annotation ProjectionAnnotation projection takes as input a word-alignedparallel corpus of sentences in a source language(usually English) and their target language trans-lations.
A syntactic parser and a semantic rolelabeler produce labels for the English sentences,which are then projected onto aligned target lan-guage words.
The underlying theory is that paral-lel sentences share a degree of syntactic and, in par-ticular, semantic similarity, making such projectionpossible (Pado?
and Lapata, 2009).State-of-the-art.
Previous work analyzed errors inannotation projection and found that they are of-ten caused by non-literal translations (Akbik et al,2015).
For this reason, previous work defined lexicaland syntactic constraints to increase projection qual-ity.
These include verb filters to allow only verbsto be labeled as frames (Van der Plas et al, 2011),heuristics to ensure that only heads of syntactic con-stituents are labeled as arguments (Pado?
and Lap-ata, 2009) and the use of verb translation dictionar-ies (Akbik et al, 2015) to constrain frame mappings.Adaptation to low-resource languages.
Low-resource languages, however, lack syntactic parsersto identify target language predicate-argument struc-tures.
This requires us to make the following modi-fications to the approach:Target language predicates We define lexical con-straints using verb translation dictionaries.
Thisensures that only target language verbs that arealigned to literal source language translationsare labeled as frames.Target language arguments To identify argu-ments, we project not only the role label ofsource language arguments heads, but theentire argument dependency structure.
This isillustrated in Figure 1: Two dependency arcsare projected from English onto Tamil, givingevidence that arguments A0 and A1 in theTamil sentence each consist of two words.This step produces a target language corpus with se-mantically annotated predicate-argument structure.3 Outline of a Data Curation ProcessAs confirmed in the experiments section of this pa-per, the quality of the Proposition Banks generatedusing annotation projection is significantly lower forlow-resource languages.
We therefore propose touse this approach only as a starting point for an in-expensive curation process as illustrated in Figure 2:994??
????
?????
???
?AM-LOCA0 work.01??
????
?????
???
?Q1: Is ????
meant as in "work"?Q2: Is ??
????
the "worker"?Q3: Is a "co-worker" mentioned       somewhere in this sentence?Tamil sentence with projected labels:Question form:Figure 3: Example of how data curation questions may be for-mulated for the labels projected onto Tamil in Figure 1.Step 1: Crowdsourced data curation.
Previouswork has experimented with different approaches incrowdsourcing to generate frame-semantic annota-tions over text (Hong and Baker, 2011), includingselection tasks (selecting one answer from a list ofoptions) (Fossati et al, 2013) and marking tasks(marking text passages that evoke a certain semanticrole) (Feizabadi and Pado?, 2014).
While these stud-ies only report moderate results on annotator cor-rectness and agreement, our goal is different fromthese works in that we only wish to curate projectedlabels, not generate SRL annotations from scratch.A related project in extending FrameNet with para-phrases (Pavlick et al, 2015) has shown that thecrowd can effectively curate wrong paraphrases byanswering a series of confirm-or-reject questions.For our initial study, we generate human readablequestion-answer pairs (He et al, 2015) using the la-bel descriptions of the English Propbank (see Fig-ure 3).
We generate two types of questions:Label confirmation questions are confirm-or-reject questions on whether projected labelsare correct (e.g.
Q1 and Q2 in Figure 3).Workers further qualify their answers toindicate whether a sequence of words markedas an argument is incomplete.Missing label questions are marking tasks whichask whether any core role labels of a frame aremissing.
For example, the BUY.01 frame has5 core roles (labeled A0 to A4), one of whichis the ?price?
(A3).
Since no ?price?
is labeledin the Tamil sentence in Figure 3, question Q3DATA SET Bengali Malayalam TamilOPENSUBTITLES2016 75K 224K 21KSPOKENTUTORIALS 31K 17K 32KTotal # sentences 106K 241K 53KTable 1: Parallel data sets and number of parallel sentencesused for each language.asks users to add this label if a ?price?
is men-tioned.Our goal is to effectively distribute a large partof the curation workload.
In cases where the crowdunanimously agrees, we remove labels judged to beincorrect and add labels judged to be missing.Step 2: Expert data curation.
We also expect apercentage of questions for which non-experts willgive conflicting answers1.
As Figure 2 shows, suchcases will be passed to experts for further curation.However, for the purpose of scalability, we aim tokeep expert involvement to a minimum.4 Experimental StudyWe report our initial investigations over the follow-ing questions: (1) What are the differences in an-notation projection quality between low- and high-resource languages?
; and (2) Can non-experts beleveraged to at least partially curate projected labels?4.1 Experimental SetupLanguages.
We evaluate three low-resource lan-guages, namely Bengali, an Indo-Aryan language,as well as Tamil and Malayalam, two South Dravid-ian languages.
Between them, they are estimated tohave more than 300 million first language speakers,yet there are few NLP resources available.Data sets.
We use two parallel corpora (see Table 1):OPENSUBTITLES2016 (Tiedemann, 2012), a cor-pus automatically generated from movie subtitles,and SPOKENTUTORIALS, a corpus of technical-domain tutorial translations.Evaluation.
For the purpose of comparison to pre-vious work on high-resource languages, we replicate1Common problems for non-experts that we observe in ourinitial experiments involve ambiguities caused by implicit orcausal role-predicate relationships, as well as figurative usageand hypotheticals.995PRED.
ARGUMENTLANG.
Match P P R F1 %AgreeBengali partial 1.0 0.84 0.68 0.750.67PROJECTED exact 1.0 0.83 0.68 0.75Bengali partial 1.0 0.88 0.69 0.78CURATED exact 1.0 0.87 0.69 0.77Malayalam partial 0.99 0.87 0.65 0.750.65PROJECTED exact 0.99 0.79 0.63 0.7Malayalam partial 0.99 0.92 0.69 0.78CURATED exact 0.99 0.84 0.67 0.74Tamil partial 0.77 0.49 0.59 0.530.75PROJECTED exact 0.77 0.45 0.58 0.5Tamil partial 0.77 0.62 0.67 0.64CURATED exact 0.77 0.58 0.65 0.61Chinese partial 0.97 0.93 0.83 0.88 0.92(Akbik et al, 2015) exact 0.97 0.83 0.81 0.82German partial 0.96 0.95 0.73 0.83 0.92(Akbik et al, 2015) exact 0.96 0.91 0.73 0.81Hindi partial 0.91 0.93 0.66 0.77 0.81(Akbik et al, 2015) exact 0.91 0.58 0.54 0.56Table 2: Estimated precision and recall for Tamil, Bengali andMalayalam before and after non-expert curation.
We list state-of-the-art results for German and Hindi for comparison.earlier evaluation practice and English preprocess-ing steps (Akbik et al, 2015).
After projection, werandomly select 100 sentences for each target lan-guage and pass them to a curation step by 2 non-experts.
We then measure the inter-annotator agree-ment and the quality of the generated PropositionBanks in terms of predicate precision2 and argumentF1-score before and after crowdsourced curation3.4.2 ResultsThe evaluation results are listed in Table 2.
Forcomparison, we include evaluation results reportedfor three high-resource languages: German and Chi-nese, representing average high-resource results, aswell as Hindi, a below-average outlier.
We make thefollowing observations:Lower annotation projection quality.
We find thatthe F1-scores of Bengali, Malayalam and Tamil are2Since we do not ask missing label questions for predicates,we cannot estimate predicate recall.3Following (Akbik et al, 2015), in the exact evaluationscheme, labels marked as correct and complete count as truepositives.
In partial, incomplete correct labels also count astrue positives.??
????
????
,  ??????
??
??????
????
?A0A0discover.01A1A1 discover.01   and     a bit     wild              as        today    discover        dida  wild  one ,  as  I  discovered  today .Figure 4: Example of a projection error.
The verb discover inBengali is a light verb construction.
In addition, the pronoun Iis not explicitly mentioned in the Bengali target sentence.
Thiscauses the pronoun I to be mistakenly aligned to the auxiliaryof the light verb, causing it to be falsely labeled as A0.6, 11 and 31 pp below that of an average high-resource language (as exemplified by German in Ta-ble 2).
Bengali and Malayalam, however, do surpassHindi, for which only a relatively poor dependencyparser was used.
This suggests that syntactic annota-tion projection may be a better method for identify-ing predicate-argument structures in languages thatlack fully developed dependency parsers.Impact of parallel data.
We note a significant im-pact of the size and quality of available parallel dataon overall quality.
For instance, the lowest-scoringlanguage in our experiments, Tamil, use the smallestamount parallel data (see Table 1), most of whichwas from the SPOKENTUTORIALS corpus.
Thisdata is specific to the technical domain and seemsless suited for annotation projection than the moregeneral OPENSUBTITLES2016 corpus.A qualitative inspection of projection errorspoints to a large portion of errors stemming fromtranslation shifts.
For instance, refer to Figure 4for an English-Bengali example of the impact ofeven slight differences in translation: The Englishverb discover is expressed in Bengali as a light verb,while the pronoun I is dropped in the Bengali sen-tence (it is still implicitly evoked through the verbbeing in first person form).
This causes the wordalignment to align the English I to the Bengali auxil-iary, onto which the role label A0 is then incorrectlyprojected.5 DiscussionIn all three languages, we note improvementsthrough curation.
Argument F1-score improves to77% (?2 pp) for Bengali, to 74% (?4 pp) for Malay-996alam, and to 61% (?11 pp) for Tamil on exactmatches.
Especially Tamil improves drastically, al-beit from a much lower initial score than the otherlanguages.
This supports our general observationthat crowd workers are good at spotting obvious er-rors, while they often disagree about more subtledifferences in semantics.
These results indicate thata curation process can at least be partially crowd-sourced.
An interesting question for further investi-gation is to what degree this is possible.
As Table 2shows, non-expert agreement in our initial study wasfar below reported expert agreement, with 25% to35% of all questions problematic for non-experts.A particular focus of our future work is thereforeto quantify to which extent crowd-feedback can bevaluable and how far the involvement of experts canbe minimized for cost-effective resource generation.However, a Proposition Bank generated through thisprocess would be peculiar in several ways:Crowd semantics.
First, generated PropositionBanks would be created in a drastically differentway than current approaches that rely on experts tocreate and annotate frames.
Effectively, the non-expert crowd would, to a large degree, shape theselection and annotation of English frame and roleannotation for new target languages.
An impor-tant question therefore is to what degree an auto-generated Propbank would differ from an expertlycreated one.
In a related line of work (Akbik etal., 2016), we have conducted a preliminary com-parison of an auto-generated Proposition Bank forChinese and the manually created Chinese Proposi-tion Bank (Xue and Palmer, 2005).
Encouragingly,we find a significant overlap between both versions.Future work will further explore the usefulness ofauto-generated Propbanks to train a semantic role la-beler (Akbik and Li, 2016) and their usefulness fordownstream applications in low-resource languages.Partial syntactic annotation.
Second, while cu-ration of semantically labeled predicate-argumentstructure can be formulated as human intelligencetasks, this will not in all likelihood be possible forfull parse trees.
These Propbanks would thereforelack a treebank-style syntactic layer of annotation.Would an existing Propbank facilitate the future taskof creating treebanks for low-resource languages?
Inother words, could the traditional order of first cre-ating treebanks and then Propbanks be reversed?PROPBANK #SENTENCES #LABELS #FRAMESBengali 5,757 17,899 88Malayalam 10,579 26,831 95Tamil 3,486 11,765 68Table 3: Number of labeled sentences, semantic labels anddistinct frames of each auto-generated Propbank (before non-expert curation).6 Conclusion and OutlookWe applied annotation projection to low-resourcelanguages and found a significant drop in qualityvis-a-vis high-resource languages.
We then pro-posed and outlined a curation process for semi-automatically generating Proposition Banks andnoted encouraging results in an initial study.
Toencourage discussion within the research commu-nity, we make our generated Proposition Banks forBengali, Malayalam and Tamil (see Table 3 for anoverview) publicly available4.References[Akbik and Li2016] Alan Akbik and Yunyao Li.
2016.Polyglot: Multilingual semantic role labeling with uni-fied labels.
In ACL 2016, 54th Annual Meeting of theAssociation for Computational Linguistics: Demostra-tion Session, page to appear.
[Akbik et al2015] Alan Akbik, Laura Chiticariu, MarinaDanilevsky, Yunyao Li, Shivakumar Vaithyanathan,and Huaiyu Zhu.
2015.
Generating high qualityproposition banks for multilingual semantic role label-ing.
In ACL 2015, 53rd Annual Meeting of the Asso-ciation for Computational Linguistics Beijing, China,pages 397?407.
[Akbik et al2016] Alan Akbik, Xinyu Guan, and YunyaoLi.
2016.
Multilingual aliasing for auto-generatingproposition banks.
In COLING 2016, the 26th Inter-national Conference on Computational Linguistics (toappear).
[Brants et al2002] Sabine Brants, Stefanie Dipper, SilviaHansen, Wolfgang Lezius, and George Smith.
2002.The tiger treebank.
In Proceedings of the Workshopon Treebanks and Linguistic Theories, volume 168.
[Burchardt et al2006] Aljoscha Burchardt, Katrin Erk,Anette Frank, Andrea Kowalski, Sebastian Pado?, andManfred Pinkal.
2006.
The salsa corpus: a german4Datasets will be made available at this page: http://researcher.watson.ibm.com/researcher/view_group_subpage.php?id=7454997corpus resource for lexical semantics.
In Proceedingsof LREC 2006, Fifth International Conference on Lan-guage Resources and Evaluation, volume 6.
[Feizabadi and Pado?2014] Parvin Sadat Feizabadi and Se-bastian Pado?.
2014.
Crowdsourcing annotation ofnon-local semantic roles.
In EACL, pages 226?230.
[Fossati et al2013] Marco Fossati, Claudio Giuliano, andSara Tonelli.
2013.
Outsourcing framenet to thecrowd.
In Proceedings of the 51st Annual Meetingof the Association for Computational Linguistics (Vol-ume 2: Short Papers), pages 742?747, Sofia, Bulgaria,August.
Association for Computational Linguistics.
[He et al2015] Luheng He, Mike Lewis, and Luke Zettle-moyer.
2015.
Question-answer driven semantic rolelabeling: Using natural language to annotate naturallanguage.
In Proceedings of the 2015 Conference onEmpirical Methods in Natural Language Processing,pages 643?653, Lisbon, Portugal, September.
Associ-ation for Computational Linguistics.
[Hong and Baker2011] Jisup Hong and Collin F Baker.2011.
How good is the crowd at real wsd?
In Proceed-ings of the 5th linguistic annotation workshop, pages30?37.
Association for Computational Linguistics.
[Marcus et al1993] Mitchell P Marcus, Mary AnnMarcinkiewicz, and Beatrice Santorini.
1993.
Build-ing a large annotated corpus of english: The penntreebank.
Computational Linguistics, 19(2):313?330.[Pado?
and Lapata2009] Sebastian Pado?
and Mirella Lap-ata.
2009.
Cross-lingual annotation projection forsemantic roles.
Journal of Artificial Intelligence Re-search, 36(1):307?340.
[Palmer et al2005] Martha Palmer, Daniel Gildea, andPaul Kingsbury.
2005.
The proposition bank: An an-notated corpus of semantic roles.
Computational lin-guistics, 31(1):71?106.
[Pavlick et al2015] Ellie Pavlick, Travis Wolfe, Pushpen-dre Rastogi, Chris Callison-Burch, Mark Dredze, andBenjamin Van Durme.
2015.
Framenet+: Fast para-phrastic tripling of framenet.
In ACL (2), pages 408?413.
The Association for Computer Linguistics.
[Tiedemann2012] Jo?rg Tiedemann.
2012.
Parallel data,tools and interfaces in opus.
In Proceedings of LREC2012, Eighth International Conference on LanguageResources and Evaluation, pages 2214?2218.
[Van der Plas et al2011] Lonneke Van der Plas, PaolaMerlo, and James Henderson.
2011.
Scaling up auto-matic cross-lingual semantic role annotation.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies: short papers-Volume 2, pages 299?304.Association for Computational Linguistics.
[Xue and Palmer2005] Nianwen Xue and Martha Palmer.2005.
Automatic semantic role labeling for chineseverbs.
In IJCAI, volume 5, pages 1160?1165.
Citeseer.
[Xue et al2005] Naiwen Xue, Fei Xia, Fu-Dong Chiou,and Marta Palmer.
2005.
The penn chinese treebank:Phrase structure annotation of a large corpus.
Naturallanguage engineering, 11(02):207?238.
[Xue2008] Nianwen Xue.
2008.
Labeling chinese pred-icates with semantic roles.
Computational linguistics,34(2):225?255.998
