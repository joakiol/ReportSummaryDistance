Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 656?666,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsExample-based Paraphrasing for Improved Phrase-BasedStatistical Machine TranslationAure?lien MaxLIMSI-CNRS & Univ.
Paris SudOrsay, Franceaurelien.max@limsi.frAbstractIn this article, an original view on how toimprove phrase translation estimates is pro-posed.
This proposal is grounded on two mainideas: first, that appropriate examples of agiven phrase should participate more in build-ing its translation distribution; second, thatparaphrases can be used to better estimate thisdistribution.
Initial experiments provide ev-idence of the potential of our approach andits implementation for effectively improvingtranslation performance.1 IntroductionPhrase translation estimation in Statistical Phrase-based Translation (Koehn et al, 2003) is hamperedby the availability of both too many and too fewtraining instances.
Recent results on tera-scale SMT(Lopez, 2008) show that access to many trainingexamples1 can lead to significant improvements intranslation quality.
Also, providing indirect train-ing instances via synonyms or paraphrases for pre-viously unseen phrases can result in gains in trans-lation quality, which are more apparent when littletraining data is originally available (Callison-Burchet al, 2006; Marton et al, 2009; Mirkin et al, 2009;Aziz et al, 2010).
Although there is a consensus onthe importance of using more parallel data in SMT,it has never been formally shown that all additionaltraining instances are actually useful in predictingcontextually appropriate translation hypotheses.1To be more accurate, works such as that of (Lopez, 2008)have recourse to random sampling to build models of a manage-able size in a reasonable amount of time.Attempts at limiting training parallel sentences tothose resembling test data through thematic adapta-tion (Hildebrand et al, 2005) indeed confirm thatlarge quantities of training data cannot compen-sate for the requirement for contextually appropriatetraining instances.
In fact, it is important that phrasetranslation models adequatly reflect contextual pref-erences for each phrase occurrence in a text.
A vari-ety of recent works have used dynamically adaptedtranslation models, where each phrase occurrencehas its own translation distribution (Carpuat and Wu,2007; Stroppa et al, 2007; Max et al, 2008; Gim-pel and Smith, 2008; Haque et al, 2009) derivedfrom local contextual information in the training ex-amples.2 These approaches are supported by thestudy of (Wisniewski et al, 2010) which shows thatphrase-based SMT systems are expressive enough toachieve very high translation performance and there-fore suggests a better scoring of phrases.The apparent tradeoff between the number oftraining examples and their appropriateness in eachindivual context naturally asks for means of increas-ing the number of appropriate examples.
Exploitingcomparable corpora for acquiring translation equiva-lents (Munteanu and Marcu, 2005; Abdul-Rauf andSchwenk, 2009) offers interesting prospects to thisissue, but so far focus has not been so much on con-text appropriateness as on globally increasing thenumber of biphrase examples.2The study of (Carpuat, 2009) shows that the one transla-tion per discourse hypothesis holds in some cases, but to ourknowledge no SMT systems have attempted to exploit it yet.However, in our view, this finding does not contradict the needfor estimating translation distributions at the individual phraselevel, but they should be integrated as additional information.656The approach we take in this article is motivatedby the fact that natural language allows for multipletext views on a given content, and that if two phrasesare good paraphrases in context, then consideringappropriate training examples of one of the phrasescould provide larger quantities of training data fortranslating the other.
In other words, we hypothe-size that there may be more training data to learn aphrase?s translations in a bilingual corpus than whatSMT approaches typically use.In contrast to previous attempts at using para-phrases to improve Statistical Machine Translation,which require external data in the form of additionalparallel bilingual corpora (Callison-Burch et al,2006), monolingual corpora (Marton et al, 2009),lexico-semantic resources (Mirkin et al, 2009; Azizet al, 2010), or sub-sentential (Resnik et al, 2010)or sentential paraphrases of the input (Schroeder etal., 2009), the approach we take here can be endoge-nous with respect to the original training data.
Italso significantly departs from previous work in thatparaphrasing is not simply considered as a way offinding alternative wordings that can be translatedgiven the original training data for out-of-vocabularyphrases only (Callison-Burch et al, 2006; Martonet al, 2009; Mirkin et al, 2009; Aziz et al, 2010),but as a means to better estimate translations for anypossible phrase.
Also, as opposed to the work by(Schroeder et al, 2009; Onishi et al, 2010; Du etal., 2010), we do not encode paraphrases into inputlattices to have them compete against each other tobelong to the source sentential paraphrase that willlead to the highest scoring output sentence3.
Instead,we make use of all contextually appropriate para-phrases of a source phrase, which collectively eval-uate the quality of each translation for that phrase.This work can thus be seen as a contribution to-wards shifting from global phrase translation dis-tributions to contextual translation distributions forcontextually equivalent source units.
The remainderof this paper is organized as followed.
In section 2we review relevant previous works and discuss howthey differ from our approach.
Section 3 provides adescription of the details of our approach.
We de-scribe an experimental setup in section 4 and com-3This highly depends on how well estimated translations foreach independent paraphrase are.ment on our results.
Finally, we discuss our futurework in section 5.2 Relation to previous work2.1 Contextual estimation of phrasetranslationsIn standard approaches to phrase-based SMT, evi-dence of a translation is accumulated uniformely ev-ery time it is found associated with a source phrasein the training corpus.
In addition to the fact thaterrors in automatic word alignment and non literaltranslations often produce useless biphrases, this re-sults in rare but appropriate translations being veryunlikely to be considered during decoding.
Someapproaches on source context modelling (Carpuatand Wu, 2007; Stroppa et al, 2007; Max et al, 2008;Haque et al, 2009) build classifiers offline for thephrases in a test set, so that context similarity canfor example reinforce scores associated with rarebut appropriate translations.
However, heavy offlinecomputation makes scaling to larger corpora an is-sue.
Other approaches (Callison-Burch et al, 2005;Lopez, 2008) instead focus on accessing very largecorpora.
Indexing by suffix arrays is used to allowfast access to phrase instances in the corpus, and ran-dom sampling to avoid collecting the full set of ex-amples has been shown to perform well.
However,these approaches consider all instances of a phraseas equivalent for the estimation of its translations.These works converge on the need for accessinga sufficient number of examples that are relevant forany source phrase in context, fast enough to permiton-the-fly phrase table building.
This paper pro-poses an intermediate step: the full set of phraseexamples is found efficiently, and a measure of theadequacy of each example with a phrase in contextprovides evidence for its translation that depends onthis value of adequacy.
In this way, the translationassociated with an example for a different sense ofa polysemous word would in the best scenario onlybe considered marginally when computing the trans-lation distribution.
As in most previous works, ad-equacy can be approximated by context similaritybetween phrase occurrences and training examples.Ideally, one would stop extracting examples whenenough appropriate examples have been found to es-timate a reliable translation distribution.
(Callison-657sample size 100 500 1K 5K 10k 50k unlim.BLEU score 28.8 28.8 28.8 28.9 29.1 28.9 29.0Figure 1: Effect of number of samples on translationquality (measured on German to English translation onEuroparl data) reported by (Callison-Burch et al, 2005)Burch et al, 2005) measured the impact on transla-tion quality of the sample size in random samplingof source phrase examples in the training corpus toestimate a phrase?s translation probabilities.
As Ta-ble 1 shows, quality (in terms of BLEU scores) al-most remains constant for samples of size 100 ormore.
This apparent confirmation of the efficiencyof random sampling is backed up by the authorswith the following possible explanations: 1) themost probable translations remain the same for dif-ferent sample sizes; 2) misestimated probabilitiesare ruled out by the target language model; and3) longer or less frequent phrases, which are notaffected by sampling, are preferred.
However, assaid previously, random sampling cannot guaranteethat contextually-appropriate examples are selected.In fact, (Lopez, 2008) points out to using discrimi-natively trained models with contextual features ofsource phrases in conjunction with phrase samplingas an open problem.
This work does not attempt todirectly address it, but instead resorts to completeanalysis of the training data to guarantee that allcontextually-appropriate examples are considered.2.2 Using paraphrases for translatingFor some phrases, not enough examples can befound in the training corpus to estimate reliabletranslation probabilities in context.
In such cases,one might be interested in finding more appropri-ate examples, which seems at first impossible us-ing the sole original bilingual corpus.
We can infact consider the set of source phrases that havesimilar translations in context.
This set is roughlymade up of a subset of what can be referred to asparaphrases.
One possible approach to extract lo-cal (i.e.
phrasal) paraphrases precisely exploits sim-ilarity on the target side in another language by ex-tracting source phrases that share common transla-tions (Bannard and Callison-Burch, 2005), but re-cent approaches have combined this approach withSource phrase ParaphrasesBalkan War Balkan war (0.25) Balkans War(0.125) Balkans (0.125) Balkanswar (0.125) war in the Balkans(0.125) Balkan conflict (0.125)British forces British troops (0.29) Britisharmed forces (0.19)Czech president President of the Czech Republic(0.5)Dalai Lama?s of the Dalai Lama (0.27)I don?t see I do not believe (0.18) I do not think(0.18) I do not see (0.15)Figure 2: Examples of paraphrases obtained by pivotingvia French; values indicate paraphrase probability as de-fined in (Bannard and Callison-Burch, 2005).similarity computation in the ?source?
(i.e.
original)language (Callison-Burch, 2008; Max, 2008; Kokand Brockett, 2010).
Figure 2 provides examples ofEnglish paraphrases obtained by automatically piv-oting via French.
As can be seen, some exampleswould be clearly useful to better estimate transla-tions of the original source phrase: (Balkan War?
war in the Balkans) are syntactic variants thatcan generally substitute with each other, (BalkanWar ?
Balkans war) are character-level variants4.Other examples, however, clearly illustrate the needfor validation in context: (Dalai Lama?s ?
of theDalai Lama) require different syntactic contexts,and (I don?t see ?
I do not believe) are only inter-changeable in specific semantic contexts.Previous attempts at exploiting paraphrases inSMT have first concentrated on obtaining transla-tions for phrases absent from the training corpus(Callison-Burch et al, 2006; Marton et al, 2009;Mirkin et al, 2009)5, with modest gains in trans-lation performance as measured by automatic met-rics.
(Callison-Burch et al, 2006) obtain para-phrases by pivoting via additional bilingual corporaand use the translations of known paraphrases totranslate unseen phrases, which requires that the ad-ditional bilingual corpora contain the unseen sourcephrases and that some of the extracted paraphrasesbe present in the original corpus.
(Marton et al,4To our knowledge, most implementations of SMT decodersdo not integrate flexible matching of phrases.5The work by (Mirkin et al, 2009) in fact considers bothparaphrases and entailed texts to increase the number of prop-erly translated texts.6582009) proceed similarly but obtain their paraphrasesfrom comparatively much larger monolingual cor-pora by following the distributionality hypothesis.In both cases, gains are only obtained in very spe-cific conditions where very few training data areavailable and where useful additional knowledge canbe brought in from external resources.
Furthermore,the described implementations do not consider ac-ceptability of the paraphrases in context, as their un-derlying hypothesis is that it might be more desir-able to translate some paraphrase than not to trans-late a given phrase.6 In contrast, the work by (Mirkinet al, 2009) attempts to model context when usingreplacements for words (synonyms or hypernyms).The natural next step that we take here is toexploit the complementarity of the original bilin-gual training corpus for finding paraphrases and themonolingual (source) side of the same corpus forvalidating them in context.
Furthermore, our fo-cus here is not on paraphrasing unseen phrases7, butpossibly any phrase, or any phrase seen less than agiven number of times, or any types of difficult-to-translate phrases (Mohit and Hwa, 2007).The recent work of (Resnik et al, 2010)uses crowdsourcing to obtain paraphrases forsource phrases corresponding to mistranslated targetphrases.
The spotting of the incorrect target phrasesand the paraphrasing of the source phrases can beautomated.
Promising oracle figures are obtained,validating the claim that some variations of the inputsentence might be more easily translated than oth-ers by a given system.
Paraphrases have also beenused to represent alternative inputs encoded in lat-tices using existing (Schroeder et al, 2009) or au-tomatically built paraphrases (Onishi et al, 2010;Du et al, 2010).
In this scenario, paraphrases are infact competing with each other, whereas in our pro-posal paraphrases collectively participate in evalu-ating the quality of each translation for a sourcephrase.
We believe that if two phrases are indeedparaphrases in context, then their respective set oftranslations are both relevant to translate the twophrases.
The target language model neverthelessstill has an important role to play to select appro-6The default strategy for most decoders is to copy out-of-vocabulary tokens into the final text.7Doing it in conjunction with our approach for improvingthe translation of known phrases is part of our future work.priate translations among semantically-compatibletranslations (i.e., target side paraphrases) in the spe-cific context of a generated target hypothesis.Lastly, automatic sentential paraphrasing has alsobeen used in SMT to build alternative referencetranslations for parameter optimization (Madnaniet al, 2008) and to build alternative training cor-pora (Bond et al, 2008).3 Towards better exploitation of trainingcorpora in phrase-based SMTIn typical phrase-based SMT settings (Koehn et al,2003), words from the source side of the corpusare first aligned to words on the target side andbiphrases are extracted from each training sentenceusing some heuristics on the word alignments.
Asource phrase f in a sentence being translated maytherefore be aligned to a variety of target phrases.In the example on Figure 3, f is aligned some num-ber of times in the training corpus to target phrasese1, e2, e3 and e5.
Using the number of times f ispaired with some target phrase ei, count(f, ei), rela-tive frequency estimation can be used to compute theprobability of translation ei given source phrase f :prel(ei|f) =count(f, ei)?j count(f, ej)(1)This value, together with other estimates of howappropriate a translation pair (f, ei) is, are recordedin a phrase table, which typically discards all con-textual information.8 Therefore, the translation dis-tribution of some phrase is globally estimated froma training corpus independently of the actual contextof that phrase.9 On Figure 3, phrase f has at leasttwo distinct senses: one represented by set E , whichin our example corresponds to the appropriate sensefor a particular occurrence of f in a test sentence;and one which corresponds to translation e5.
A typ-ical problem, due to the lack of context modeling,8See (Carpuat and Wu, 2007; Stroppa et al, 2007; Max etal., 2008; Gimpel and Smith, 2008; Haque et al, 2009) for no-table exceptions.9Context is in fact taken into account to some extent by thetarget language model, which should score higher translationsthat are more appropriate given a target translation hypothesisbeing built.
In fact, in this work we consider the target languagemodel as the main source of information for selecting amongacceptable target phrases (target language paraphrases).659Figure 3: Example of possible source equivalents andtranslations for phrase occurrence f ?un bon avocat?
inthe sentence ?L?embauche d?un bon avocat est crucialequelle que soit l?activite??
(?Hiring a good lawyer is cru-cial to any business?).
Set E represents target phrasetypes that are acceptable translations given the particu-lar context of f , and set F represents source phrase typesthat can be in a paraphrasing relation to f depending onthe context they appear in.is that in situations such as ?ei ?
E , count(f, e5)count(f, ei), it is very unlikely that a correct trans-lation will be selected during decoding against theincorrect but much more frequent one.
Taking anextreme view on this issue, it is in fact desirable thatwhen estimating phrase translation probabilities fora phrase f , translations of incompatible senses benot considered.10 Of course, this raises the diffi-cult issue of sense clustering of phrases.
We proposehere an intermediary solution, which consists in con-sidering each occurrence in the training corpus ascounting a number of times that depends on its con-textual similarity with the occurrence of f from thetest file, through the following additional translationmodel :pcont(ei|f) =?
?fk,ei?simcont(C(f), C(fk))?
?fk,ej?simcont(C(f), C(fk))(2)where f is some source phrase to translate and fkan example of f in the training corpus, ?fk, ei?
is a10Put differently, is it more acceptable to copy a source wordin the target hypothesis or to incorrectly translate it when theconfidence about its being incorrect is high?biphrase from the training corpus, C(f) the contextof some source phrase, C(fk) the context of a par-ticular example of f in the training corpus, simphra function indicating the contextual similarity be-tween two phrase contexts, and ej is any possibletranslation of f .The problem of modeling phrase translation ishowever not limited to inappropriate training exam-ples.
For various reasons, legitimate occurrences ofsource phrases may not be considered when buildinga phrase?s translation distribution.
We describe thosecases by considering the possible source phrases pifrom Figure 3:?
p1?s only translation, e1, is a common transla-tion with f ; each contextually-appropriate ex-ample of p1 should reinforce the probability ofe1 being a translation for f .?
Contextually-appropriate examples of p2 canreinforce e3.
Translation e6 should correspondto contextually-inappropriate examples of p2,so e6 should not be considered as a new po-tential translation for f .?
Contrarily to the examples of p2 translating ase6, examples of p3 translating as e4 are muchmore likely of being contextually-appropriatewith f , meaning that f could be substitutedwith most p3 examples.
Therefore, e4, whichwas not initially considered as a possible trans-lation of f , could now be considered as such.?
p4 shares a translation with f , e2, but this isdue to the polysemous nature of this transla-tion.
Again, all examples of p4 should be foundcontextually-inappropriate with f , and theirtranslations should not be considered when es-timating the translations of f .?
Lastly, the case of the common translation e5between f and p5 illustrates a consequence ofthe polysemous nature of the source phrase cor-responding to word sequence f : translationscorresponding to other senses of f should notget reinforced by paraphrase examples such asthose of p5 as these examples should be foundcontextually-inappropriate with f .660We build a separate translation model for transla-tions estimated through paraphrases, defined as fol-lows:ppara(ei|f) =?
?pk,ei?simpara(C(f), C(pk))?
?pk,ej?simpara(C(f), C(pk))(3)where pk is a paraphrase of f , ?pk, ei?
is a biphrasefrom the training corpus such that ei is also a transla-tion of f , C(f) the context of a given source phrasefor which we are estimating the translation distribu-tion, C(pk) the context of a particular example of pkin the training corpus, simpara a function indicat-ing the contextual similarity between a phrase con-text and a paraphrase context, and ej is any possibletranslation of f .Several requirements can be drawn from the pre-vious description:1.
List of potential paraphrases: some mech-anism for finding potential paraphrases forsource phrases is required, and several suchmechanisms could be combined.
Pivoting viabilingual corpora, a natural strategy given theissue at hand, is just one among many differentproposed strategies (Madnani and Dorr, 2010).2.
Contextual similarity measure: a similaritymeasure between the contexts of two phrasesor two potential local paraphrases is required.This automatic measure should ideally be ableto model not only syntactic but also semanticand pragmatic information.3.
Robust translation evaluation: our ap-proach is designed to reinforce estimates forany contextually-appropriate translations of aphrase, as shown by set E on Figure 3.
It istherefore important to have some means of ac-cepting them as subparts of valid translations.Robustness in Machine Translation evaluationis an active domain, and potential candidatesinclude using BLEU-like metrics with multiplereferences, Human-targeted Translation ErrorRate (Snover et al, 2006) and the use of para-phrases for reference translations (Kauchak andBarzilay, 2006).train dev.
test# sent.
# tok.
# sent.
# tok.
# sent.
# tok.en 318K 9.1M 500 14,0K 500 13,6Kfr 318K 10.3M 500 16,1K 500 15,7kFigure 4: Statistics of the corpora used.In this paper, we want to evaluate whether an en-dogenous approach for finding paraphrases can leadto some improvement in translation performance.Note that we will not consider in this initial workthe possibility of adding new translations to phrases(such as e4 for f on Figure 3) as it adds complexityand should be investigated when the other simplercases can be handled successfully.In the following section, we describe experimentsin which the original bilingual corpus is the only re-source used to find potential paraphrases and to esti-mate phrase translations in context.
We chose a verysimple measure of similarity, and let to our futurework the task of improving context modeling.
Asregards evaluation, we will resort to various ways tomeasure the impact of our implementation on trans-lation performance.4 Experiments and results4.1 Data and baseline SMT systemsWe have conducted our experiments using theMOSES11 package to build state-of-the-art phrase-based SMT systems for phrases of up to 5 tokens,using standard parameters and MERT for optimizingmodel weights.
We used a subpart of the Europarlcorpus12 in French and English as our training cor-pus and built baseline MOSES systems (bsl) in bothdirections.
The target side of the training corpuswas used to train 3-gram target language model withmodified Kneser-Ney smoothing.
Held-out datasetswere used for development and testing.
The charac-teristics of all corpora are described in Figure 4.4.2 Example-based Paraphrasing SMT systemsWe also built systems that exploit phrase and para-phrase context under the form of two additionalmodels pcont and ppara described in section 3.
These11http://www.statmt.org/moses12http://www.statmt.org/europarl661phrase table size num.
entriesen?frbaseline 240Mb 2.4Mour systems 5.0Gb 37.5Mfr?enbaseline 193Mb 1.9Mour systems 4.0Gb 30.2MFigure 5: Statistics on the size and the number of entriesof the phrase tables filtered on the development set.models are added to the list of models used to eval-uate the various translations of a phrase in the ap-propriate phrase tables, and are optimized with theother models by standard MERT.In order to model context, we modified the sourcetexts so that each phrase becomes unique in thephrase table, i.e.
it has its own translation distribu-tion.
This is done (as in other works (Carpuat andWu, 2007; Stroppa et al, 2007)) by transformingeach token into a unique token, e.g.
token ?
to-ken@337.
This therefore leads to a significant in-crease in the size of the phrase table, as illustratedon Figure 5, as all occurrences for the same phraseare not factored anymore.13We chose a very simple initial definition of con-text similarity based on the presence of commonn-grams in the immediate vicinity of two phrases.Let lengthleft (resp.
lengthright) be the length ofthe longest common n-gram in the immediate vicin-ity on the left (resp.
right) of two phrases in context(C(f) and C(fi)).
For instance, given the two fol-lowing contexts (phrases under focus are in bold andcommon n-grams are underlined):1. the commission accepts the substance of theamendments@11257 proposed@11258by@11259 the committee on fisheries ...2. this is why we shall support all of the amend-ments put forward by the committee on agri-culture and rural development ...lengthleft = 2 and lengthright = 3.
We furtherdefine length as:13These volumes of data and our available hardware facilitiesfor these experiments led us to initially limit the size of our datasets.
We will discuss in section 5 how we intend to address thislimitation in our future work.length =????????
?lengthleft + lengthrightif lengthleft > 0and lengthright > 00 otherwise(4)We can now define the two similarity functionsused in Equations 2 and 3 that we used for our ex-periments:simcont(C(f), C(fi)) = (1 + length)?
(5)simpara(C(f), C(pi)) = (length)?
(6)The rationale for these functions is the follow-ing.
Exact phrase examples add at least a transla-tion count of 1, i.e.
their translation is always takeninto account to estimate pcont.
Paraphrase exam-ples add a translation count of 0 if length = 0,i.e.
their translation is not taken into account at allif surrounding n-gram similarity is too low.
We used?
= ?
= 1.5.
Algorithm 1 describes how the twomodels are estimated from the training data.foreach phrase f in training file doextract C(f);/* phrase count */foreach unique phrase fi in test(f)) doextract C(fi);compute simcont(C(f), C(fi));end/* paraphrase count */foreach phrase pi in para(f) doforeach unique phrase fj in test(pi) doextract C(fj);compute simpara(C(f), C(fj));endendendestimate pcont and para;Algorithm 1: Model estimation for pcont andppara.
Function test(f) returns all uniquephrases corresponding to phrase f from the testfile.
Function para(f) return all phrases forwhich f is a known paraphrase.We implemented the following strategy to findparaphrases for phrases in the test file.
We extract all662Left context phrase/paraphrase Right contextIS#1 at the moment it is up to each member state to decide, and practice dif-fers considerably from country to countryPE#1 ... as regards the terminal portion in thecycle of nuclear fuel, it isthe responsability of each member state to define its own policy .PT#1 la responsabilite?
de chaqueIS#2 that is why i find it extremely regrettable that the amendment on harmonising the re-registration of cars that have been involvedin accidents ...PE#2 for all these reasons and given your mostexcellent statement , i find ita pity that the new legal base for the daphne pro-gramme is so restrictive ...PT#2 dommage queFigure 6: Examples of paraphrases in context from the development file.
The input sentence (IS) contains a sourcephrase of interest (in bold), the paraphrase example (PE) contains a paraphrase of that source phrase (in bold) forwhich a paraphrase translation (PT) is known.paraphrases p for a phrase f by pivot: all target lan-guage phrases e aligned to f are first extracted, andall source language phrases p aligned to e are ex-tracted.
The following constraints are then appliedto define which paraphrases are kept:?
string p is not included in string f and viceversa (in order to minimize the impact of align-ment errors in the training corpus);?
the paraphrasing probability is greater than afixed threshold: para(f, p) ?
10?2, wherepara(f, p) =?e p(e|f)p(p|e) (Bannard andCallison-Burch, 2005);?
the number of occurrences of phrase f andparaphrase p are equal or less than indepen-dent thresholds: numOccs(f) ?
100 andnumOccs(p) ?
1000.14Figure 6 shows examples of paraphrases in con-text with high similarity with some original phrase,and Figure 7 provides various statistics on the para-phrases extracted on the test file.4.3 Results and analysisAutomatic evaluation results are reported in Table 8for various configurations.
We also wanted to focusour measures on content words, which are known14The first threshold value was chosen as (Callison-Burch etal., 2005) report it to be an optimal sample size for estimatingphrase translation probabilities.
The relatively low value for thesecond threshold was selected to reduce computation time.phrase # phrases # paraphrased # paraphraseslength en fr en fr en fr1 13,620 15,707 458 725 1,824 2,6842 13,120 15,207 4,127 4,481 18,871 19,7003 12,620 14,707 4,782 5,715 24,111 27,3774 12,120 14,208 2,859 4,078 15,071 20,3455 11,623 13,711 1,171 2,275 6,077 12,132Figure 7: Statistics on numbers of phrases, numbersof paraphrased phrases and numbers of paraphrases perphrase length.to be important as regards information content intranslation.
We applied the contrastive lexical eval-uation (CLE) methodology described in (Max etal., 2010), which indicates how many times sourcewords grouped into user-defined classes were cor-rectly translated or not across systems.
These addi-tional results are reported on Figure 9.On English to French translation, both additionalfeatures lead to improvements over the baselinewith all metrics, including CLE, and their combi-nation shows a strong improvement in TER (-1.55).CLE on content words reveals that the para featureseems particularly effective in reducing the numberof words in all categories that only the baseline sys-tem translated correctly.Results on French to English translation are lesspositive: neither cont nor para alone improve overthe baseline with any metrics.
However, their com-bination improves over the baseline with all met-rics except BLEU, including a reduction of -1.07in TER.
Detailed analysis of CLE results showsthat the translation of adjectives and nouns benefited663more from using our two additional models.
Verbs,whose translation improved slightly, are strongly in-flected in French, so finding examples for a givenform is more difficult than for less inflected wordcategories, as is finding paraphrases with the appro-priate inflection.
Also, pivoting via English is onereason why paraphrases obtained via a low-inflectedlanguage can be of varying quality.
Furthermore, thesimplicity of our context modeling may have beenineffective in filtering out some bad examples.
Over-all, para was more effective with the low-inflectedEnglish as the source language, improving over thebaseline with all metrics.These results confirm that translation perfor-mance can be improved by exploiting context andparaphrases in the original training corpus only.
Wenext attempted to measure whether some improve-ment in the quality of the paraphrases used wouldhave some measurable impact on translation perfor-mance.
To this end, we devised a semi-oracle ex-periment in the following way: the source and targettest files were automatically aligned, and for eachsource phrase possible target phrases (i.e., referencetranslations) were extracted, and used as pivots toextract potential paraphrases, which were then fil-tered with the same constraints as previously.
Inthis way, we exploit the information that paraphrasescan at least produce the desired translation, but theymay also propose other incorrect translations and/orbe present in very few examples.
Results appearin the inf rows of Tables 8 and 9.
We obtain themost important improvement over the baseline inBLEU for the two language pairs (resp.
+0.99 and+0.44), though the results for the other metrics forFrench to English translation are more difficult tointerpret.
For this language pair, possible reasonsinclude again that the pivot language may not beappropriate, and also that the limitation to a sin-gle pivot15 may not have produced more monolin-gual variation that might have proved useful.
CLEon English to French, however, reveals significantgains with a relative improvement over the baselineof +116 content words.
Under this condition, thisresult shows that the higher the quality of the para-phrases used, the more translation quality can be im-15Several pivot phrases may in fact have been automaticallyextracted for a given phrase, some of which being possible badcandidates.BLEU NIST TER METEORen?frbsl 30.28 - 6.66 - 57.86 - 54.79 -+cont 31.11 +0.83 6.77 +0.11 57.24 -0.62 55.22 +0.43+para 30.97 +0.69 6.74 +0.08 57.38 -0.48 55.39 +0.60all 30.93 +0.65 6.84 +0.18 56.31 -1.55 55.28 +0.49inf 31.27 +0.99 6.78 +0.12 57.22 -0.64 55.80 +1.01fr?enbsl 29.90 - 6.90 - 54.64 - 61.36 -+cont 29.56 -0.34 6.89 -0.01 54.95 +0.31 60.98 -0.38+para 29.70 -0.20 6.92 +0.02 54.64 +0.00 61.10 -0.26all 29.75 -0.15 7.03 +0.13 53.57 -1.07 61.63 +0.27inf 30.34 +0.44 6.93 +0.03 54.90 +0.26 60.99 -0.37Figure 8: Automatic scores for the MOSES baseline sys-tems (bsl), systems additionnally using the contextualfeature (+cont), systems additionnally using the para-phrasing feature (+para), systems using both features(all), and pivot-informed systems (inf).Adj Adv Noun Verb?en?fr+cont - 74 28 113 60 275+ 55 35 114 85 289 +14+para - 62 12 82 46 202+ 58 32 111 78 279 +77all - 72 25 91 72 260+ 50 37 118 97 302 +42inf - 58 20 108 56 242+ 65 43 147 103 358 +116fr?en+cont - 30 16 80 69 195+ 15 21 69 46 151 -44+para - 32 19 72 60 183+ 12 18 65 43 138 -45all - 21 18 67 61 167+ 30 18 94 48 190 +23inf - 38 21 83 66 208+ 31 23 106 57 217 +9Figure 9: Contrastive lexical evaluation results per part-of-speech measured on the test file.
?-?
(resp.
?+?)
rowsindicate the number of source words that only bsl (resp.the compared system) correctly translated.proved, which is in line with works that make useof human-made paraphrases to improve translationquality (Schroeder et al, 2009; Resnik et al, 2010).Table 10 presents a typology of paraphrases foundin our development set and classifies the impact ofusing them for phrase translation estimation.
As canbe seen, more work is needed to better understandthe characteristics of the phrases that should be para-phrased and of their paraphrases.664Type Impact ExamplesMorphological variants +/- (yugoslav republic?
yugoslavian republic), (go far?
goes far)Synonymy + (duties?
obligations), (to look into?
to study)Grammatical word substitution ?/- (states in the?
the states of the), (amendments by?
amendments to)Word deletion or insertion ?/- (first reading, the?
first reading the), (amendments by?
amendmentsproposed by)Syntactic rewritings + (approval of the majority ?
majority support), (capacity of the euro-pean union?
european union?s ability)Phrasal idiomatic substitutions + (must be said that the?
goes without saying that the), (is fully in line?
is totally coherent), (is amazing?
strikes me)Context-dependent substitu-tions+/- (is not right?
is unacceptable), (offer my?
express my)Alignment and translation prob-lems- (unnecessary if ?
vital if), (the crime ?
organized), (ill-advised ?wise), (to begin by thanking?
to begin by congratulating)Figure 10: Main types of paraphrase pairs found in our dev.
and training corpora.
Pairs shown have length > 0.5 Conclusion and future workWe have introduced an original way of exploitingboth context and paraphrasing for the estimationof phrase translations in phrase-based SMT.
To ourknowledge, this is the first time that paraphrases ac-quired in an endogenous manner have been shownto improve translation performance, which showsthat bilingual corpora can be better exploited thanthey typically are.
Our experiments further showedthe promises of our approach when paraphrases ofhigher quality are available.In the light of our results and our initial typologyof paraphrases presented on Figure 10, as well asprevious work on paraphrasing for SMT, the diffi-cult question of what units should be paraphrasedfor what success should be addressed, taking into ac-count parameters such as language pairs, quantity oftraining data and availability of external resources.Our future work includes three main areas: first,we want to improve the modeling of context, by no-tably working on techniques inspired from Informa-tion Retrieval to quickly access contextually-similarexamples of source phrases in bilingual corpora.Such contextual sampling on large bilingual corporafor phrases and their paraphrases, which could inte-grate more complex linguistic information, will al-low us to assess our approach on more challengingconditions.
This would also allow us to build con-textual models on-the-fly, and experiment with us-ing lattices to encode contextually estimated para-phrases.
Second, we will combine paraphrases ob-tained via different techniques and resources, whichwill allow us to also learn translation distributionsfor phrases absent from the original corpus.
Lastly,we want to also exploit paraphrases for the addi-tional translations that they propose (such as e4 onFigure 3) and that would be contextually similar inthe target language to other existing translations ofa given phrase or that could even represent a newsense of the original phrase.AcknowledgementsThis work was partly supported by ANR projectTrace (ANR-09-CORD-023).
The author would liketo thank the anonymous reviewers for their helpfulquestions and comments.ReferencesSadaf Abdul-Rauf and Holger Schwenk.
2009.
On theUse of Comparable Corpora to Improve SMT perfor-mance.
In Proceedings of EACL, Athens, Greece.Wilker Aziz, Marc Dymetman, Shachar Mirkin, LuciaSpecia, Nicola Cancedda, and Ido Dagan.
2010.Learning an Expert from Human Annotations in Sta-tistical Machine Translation: the Case of Out-of-Vocabulary Words.
In Proceedings of EAMT, Saint-Raphael, France.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with Bilingual Parallel Corpora.
In Proceed-ings of ACL, Ann Arbor, USA.Francis Bond, Eric Nichols, Darren Scott Appling, andMichael Paul.
2008.
Improving statistical machinetranslation by paraphrasing the training data.
In Pro-ceedings of IWSLT, Hawai, USA.Chris Callison-Burch, Colin Bannard, and JoshSchroeder.
2005.
Scaling Phrase-Based Statis-665tical Machine Translation to Larger Corpora andLonger Phrases.
In Proceedings of ACL, Ann Arbor,USA.Chris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006.
Improved Statistical Machine Transla-tion Using Paraphrases.
In Proceedings of NAACL,New York, USA.Chris Callison-Burch.
2008.
Syntactic Constraints onParaphrases Extracted from Parallel Corpora.
In Pro-ceedings of EMNLP, Hawai, USA.Marine Carpuat and Dekai Wu.
2007.
Context-Dependent Phrasal Translation Lexicons for Statisti-cal Machine Translation.
In Proceedings of MachineTranslation Summit XI, Copenhagen, Denmark.Marine Carpuat.
2009.
One Translation Per Discourse.In Proceedings of the NAACL-HLT Workshop on Se-mantic Evaluations, Boulder, USA.Jinhua Du, Jie Jiang, and Andy Way.
2010.
FacilitatingTranslation Using Source Language Paraphrase Lat-tices.
In Proceedings of EMNLP, Cambridge, USA.Kevin Gimpel and Noah A. Smith.
2008.
Rich Source-Side Context for Statistical Machine Translation.
InProceedings of the ACL Workshop on Statistical Ma-chine Translation, Columbus, USA.Rejwanul Haque, Sudip Kumar Naskar, Yanjun Ma, andAndy Way.
2009.
Using Supertags as Source Lan-guage Context in SMT.
In Proceedings of EAMT,Barcelona, Spain.Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,and Alex Waibel.
2005.
Adaptation of the Transla-tion Model for Statistical Machine Translation Basedon Information Retrieval.
In Proceedings of EAMT,Budapest, Hungary.David Kauchak and Regina Barzilay.
2006.
Paraphras-ing for Automatic Evaluation.
In Proceedings ofNAACL HLT, New York, USA.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Pro-ceedings of NAACL HLT, Edmonton, Canada.Stanley Kok and Chris Brockett.
2010.
Hitting the RightParaphrases in Good Time.
In Proceedings of NAACL,Los Angeles, USA.Adam Lopez.
2008.
Tera-Scale Translation Modelsvia Pattern Matching.
In Proceedings of COLING,Manchester, UK.Nitin Madnani and Bonnie J. Dorr.
2010.
GeneratingPhrasal & Sentential Paraphrases: A Survey of Data-Driven Methods.
Computational Linguistics, 36(3).Nitin Madnani, Philip Resnik, Bonnie J. Dorr, andRichard Schwartz.
2008.
Are Multiple ReferenceTranslations Necessary?
Investigating the Value ofParaphrased Reference Translations in Parameter Op-timization.
In Proceedings of AMTA, Waikiki, USA.Yuval Marton, Chris Callison-Burch, and Philip Resnik.2009.
Improved Statistical Machine Translation UsingMonolingually-derived Paraphrases.
In Proceedingsof EMNLP, Singapore.Aure?lien Max, Rafik Makhloufi, and Philippe Langlais.2008.
Explorations in using grammatical dependen-cies for contextual phrase translation disambiguation.In Proceedings of EAMT, Hamburg, Germany.Aure?lien Max, Josep M. Crego, and Franc?ois Yvon.2010.
Contrastive Lexical Evaluation of MachineTranslation.
In Proceedings of LREC, Valletta, Malta.Aure?lien Max.
2008.
Local rephrasing suggestions forsupporting the work of writers.
In Proceedings of Go-TAL, Gothenburg, Sweden.Shachar Mirkin, Lucia Specia, Nicola Cancedda, IdoDagan, Marc Dymetman, and Idan Szpektor.
2009.Source-Language Entailment Modeling for Translat-ing Unknown Terms.
In Proceedings of ACL, Singa-pore.Behrang Mohit and Rebecca Hwa.
2007.
Localizationof Difficult-to-Translate Phrases.
In Proceedings ofthe ACL Workshop on Statistical Machine Translation,Prague, Czech Republic.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving Machine Translation Performance by Exploit-ing Non-parallel Corpora.
Computational Linguistics,31(4).Takashi Onishi, Masao Utiyama, and Eiichiro Sumita.2010.
Paraphrase Lattice for Statistical MachineTranslation.
In Proceedings of ACL, short paper ses-sion, Uppsala, Sweden.Philip Resnik, Olivia Buzek, Chang Hu, Yakov Kron-rod, Alex Quinn, and Benjamin B. Bederson.
2010.Improving Translation via Targeted Paraphrasing.
InProceedings of EMNLP, Cambridge, USA.Josh Schroeder, Trevor Cohn, and Philipp Koehn.
2009.Word Lattices for Multi-Source Translation.
In Pro-ceedings of EACL, Athens, Greece.Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Study ofTranslation Edit Rate with Targeted Human Annota-tion.
In Proceedings of AMTA, Boston, USA.Nicolas Stroppa, Antal van den Bosch, and Andy Way.2007.
Exploiting Source Similarity for SMT usingContext-Informed Features.
In Proceedings of TMI,Skovde, Sweden.Guillaume Wisniewski, Alexandre Allauzen, andFranc?ois Yvon.
2010.
Assessing Phrase-based Trans-lation Models with Oracle Decoding.
In Proceedingsof EMNLP, Cambridge, USA.666
