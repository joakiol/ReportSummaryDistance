Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 395?403,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsIdentifying Text Polarity Using Random WalksAhmed HassanUniversity of Michigan Ann ArborAnn Arbor, Michigan, USAhassanam@umich.eduDragomir RadevUniversity of Michigan Ann ArborAnn Arbor, Michigan, USAradev@umich.eduAbstractAutomatically identifying the polarity ofwords is a very important task in NaturalLanguage Processing.
It has applicationsin text classification, text filtering, analysisof product review, analysis of responsesto surveys, and mining online discussions.We propose a method for identifying thepolarity of words.
We apply a Markov ran-dom walk model to a large word related-ness graph, producing a polarity estimatefor any given word.
A key advantage ofthe model is its ability to accurately andquickly assign a polarity sign and mag-nitude to any word.
The method couldbe used both in a semi-supervised settingwhere a training set of labeled words isused, and in an unsupervised setting wherea handful of seeds is used to define thetwo polarity classes.
The method is exper-imentally tested using a manually labeledset of positive and negative words.
It out-performs the state of the art methods in thesemi-supervised setting.
The results in theunsupervised setting is comparable to thebest reported values.
However, the pro-posed method is faster and does not need alarge corpus.1 IntroductionIdentifying emotions and attitudes from unstruc-tured text is a very important task in Natural Lan-guage Processing.
This problem has a variety ofpossible applications.
For example, there has beena great body of work for mining product reputationon the Web (Morinaga et al, 2002; Turney, 2002).Knowing the reputation of a product is very impor-tant for marketing and customer relation manage-ment (Morinaga et al, 2002).
Manually handlingreviews to identify reputation is a very costly, andtime consuming process given the overwhelmingamount of reviews on the Web.
A list of wordswith positive/negative polarity is a very valuableresource for such an application.Another interesting application is mining onlinediscussions.
A threaded discussion is an electronicdiscussion in which software tools are used to helpindividuals post messages and respond to othermessages.
Threaded discussions include e-mails,e-mail lists, bulletin boards, newsgroups, or Inter-net forums.
Threaded discussions act as a very im-portant tool for communication and collaborationin the Web.
An enormous number of discussiongroups exists on the Web.
Millions of users postcontent to these groups covering pretty much ev-ery possible topic.
Tracking participant attitudetowards different topics and towards other partici-pants is a very interesting task.
For example,Tong(2001) presented the concept of sentiment time-lines.
His system classifies discussion posts aboutmovies as either positive or negative.
This is usedto produce a plot of the number of positive andnegative sentiment messages over time.
All thoseapplications could benefit much from an automaticway of identifying semantic orientation of words.In this paper, we study the problem of automati-cally identifying semantic orientation of any wordby analyzing its relations to other words.
Auto-matically classifying words as either positive ornegative enables us to automatically identify thepolarity of larger pieces of text.
This could bea very useful building block for mining surveys,product reviews and online discussions.
We ap-ply a Markov random walk model to a large se-mantic word graph, producing a polarity estimatefor any given word.
Previous work on identifyingthe semantic orientation of words has addressedthe problem as both a semi-supervised (Takamuraet al, 2005) and an unsupervised (Turney andLittman, 2003) learning problem.
In the semi-supervised setting, a training set of labeled words395is used to train the model.
In the unsupervisedsetting, only a handful of seeds is used to definethe two polarity classes.
The proposed methodcould be used both in a semi-supervised and inan unsupervised setting.
Empirical experimentson a labeled set of words show that the proposedmethod outperforms the state of the art methods inthe semi-supervised setting.
The results in the un-supervised setting are comparable to the best re-ported values.
The proposed method has the ad-vantages that it is faster and it does not need a largetraining corpus.The rest of the paper is structured as follows.In Section 2, we discuss related work.
Section 3presents our method for identifying word polarity.Section 4 describes our experimental setup.
Weconclude in Section 5.2 Related WorkHatzivassiloglou and McKeown (1997) proposeda method for identifying word polarity of adjec-tives.
They extract all conjunctions of adjectivesfrom a given corpus and then they classify eachconjunctive expression as either the same orien-tation such as ?simple and well-received?
or dif-ferent orientation such as ?simplistic but well-received?.
The result is a graph that they clusterinto two subsets of adjectives.
They classify thecluster with the higher average frequency as posi-tive.
They created and labeled their own datasetfor experiments.
Their approach will probablyworks only with adjectives because there is noth-ing wrong with conjunctions of nouns or verbswith opposite polarities (e.g., ?war and peace?,?rise and fall?, ..etc).Turney and Littman (2003) identify word po-larity by looking at its statistical association witha set of positive/negative seed words.
They usetwo statistical measures for estimating association:Pointwise Mutual Information (PMI) and LatentSemantic Analysis (LSA).
To get co-occurrencestatistics, they submit several queries to a searchengine.
Each query consists of the given word andone of the seed words.
They use the search enginenear operator to look for instances where the givenword is physically close to the seed word in the re-turned document.
They present their method as anunsupervised method where a very small amountof seed words are used to define semantic orienta-tion rather than train the model.
One of the lim-itations of their method is that it requires a largecorpus of text to achieve good performance.
Theyuse several corpora, the size of the best performingdataset is roughly one hundred billion words (Tur-ney and Littman, 2003).Takamura et al (2005) proposed using spinmodels for extracting semantic orientation ofwords.
They construct a network of words us-ing gloss definitions, thesaurus, and co-occurrencestatistics.
They regard each word as an electron.Each electron has a spin and each spin has a direc-tion taking one of two values: up or down.
Twoneighboring spins tend to have the same orienta-tion from an energetic point of view.
Their hy-pothesis is that as neighboring electrons tend tohave the same spin direction, neighboring wordstend to have similar polarity.
They pose the prob-lem as an optimization problem and use the meanfield method to find the best solution.
The anal-ogy with electrons leads them to assume that eachword should be either positive or negative.
Thisassumption is not accurate because most of thewords in the language do not have any semanticorientation.
They report that their method couldget misled by noise in the gloss definition and theircomputations sometimes get trapped in a local op-timum because of its greedy optimization flavor.Kamps et al (2004) construct a networkbased on WordNet synonyms and then use theshortest paths between any given word and thewords ?good?
and ?bad?
to determine word polar-ity.
They report that using shortest paths could bevery noisy.
For example.
?good?
and ?bad?
them-selves are closely related in WordNet with a 5-long sequence ?good, sound, heavy, big, bad?.
Agiven word w may be more connected to one setof words (e.g., positive words), yet have a shorterpath connecting it to one word in the other set.
Re-stricting seed words to only two words affects theiraccuracy.
Adding more seed words could help butit will make their method extremely costly fromthe computation point of view.
They evaluate theirmethod only using adjectives.Hu and Liu (2004) use WordNet synonyms andantonyms to predict the polarity of words.
Forany word, whose polarity is unknown, they searchWordNet and a list of seed labeled words to pre-dict its polarity.
They check if any of the syn-onyms of the given word has known polarity.
Ifso, they label it with the label of its synonym.
Oth-erwise, they check if any of the antonyms of thegiven word has known polarity.
If so, they label it396with the opposite label of the antonym.
They con-tinue in a bootstrapping manner till they label allpossible word.
This method is quite similar to theshortest-path method proposed in (Kamps et al,2004).There are some other methods that try to buildlexicons of polarized words.
Esuli and Sebas-tiani (2005; 2006) use a textual representation ofwords by collating all the glosses of the word asfound in some dictionary.
Then, a binary text clas-sifier is trained using the textual representation andapplied to new words.
Kim and Hovy (2004) startwith two lists of positive and negative seed words.WordNet is used to expand these lists.
Synonymsof positive words and antonyms of negative wordsare considered positive, while synonyms of neg-ative words and antonyms of positive words areconsidered negative.
A similar method is pre-sented in (Andreevskaia and Bergler, 2006) whereWordNet synonyms, antonyms, and glosses areused to iteratively expand a list of seeds.
The senti-ment classes are treated as fuzzy categories wheresome words are very central to one category, whileothers may be interpreted differently.
Kanayamaand Nasukawa (2006) use syntactic features andcontext coherency, the tendency for same polari-ties to appear successively , to acquire polar atoms.Other related work is concerned with subjec-tivity analysis.
Subjectivity analysis is the taskof identifying text that present opinions as op-posed to objective text that present factual in-formation (Wiebe, 2000).
Text could be eitherwords, phrases, sentences, or any other chunks.There are two main categories of work on sub-jectivity analysis.
In the first category, subjectivewords and phrases are identified without consider-ing their context (Wiebe, 2000; Hatzivassiloglouand Wiebe, 2000; Banea et al, 2008).
In the sec-ond category, the context of subjective text is used(Riloff and Wiebe, 2003; Yu and Hatzivassiloglou,2003; Nasukawa and Yi, 2003; Popescu and Et-zioni, 2005) Wiebe et al (2001) lists a lot of appli-cations of subjectivity analysis such as classifyingemails and mining reviews.
Subjectivity analysisis related to the proposed method because identi-fying the polarity of text is the natural next stepthat should follow identifying subjective text.3 Word PolarityWe use a Markov random walk model to identifypolarity of words.
Assume that we have a networkof words, some of which are labeled as either pos-itive or negative.
In this network, two words areconnecting if they are related.
Different sources ofinformation could be used to decide whether twowords are related or not.
For example, the syn-onyms of any word are semantically related to it.The intuition behind that connecting semanticallyrelated words is that those words tend to have simi-lar polarity.
Now imagine a random surfer walkingalong the network starting from an unlabeled wordw.
The random walk continues until the surferhits a labeled word.
If the word w is positive thenthe probability that the random walk hits a positiveword is higher and if w is negative then the prob-ability that the random walk hits a negative wordis higher.
Similarly, if the word w is positive thenthe average time it takes a random walk startingat w to hit a positive node is less than the averagetime it takes a random walk starting at w to hit anegative node.In the rest of this section, we will describe howwe can construct a word relatedness graph in Sec-tion 3.1.
The random walk model is described inSection 3.2.
Hitting time is defined in Section?3.3.Finally, an algorithm for computing a sign andmagnitude for the polarity of any given word isdescribed in Section 3.4.3.1 Network ConstructionWe construct a network where two nodes arelinked if they are semantically related.
Severalsources of information could be used as indicatorsof the relatedness of words.
One such importantsource is WordNet (Miller, 1995).
WordNet is alarge lexical database of English.
Nouns, verbs,adjectives and adverbs are grouped into sets ofcognitive synonyms (synsets), each expressing adistinct concept (Miller, 1995).
Synsets are inter-linked by means of conceptual-semantic and lexi-cal relations.The simplest approach is to connect words thatoccur in the same WordNet synset.
We can col-lect all words in WordNet, and add links betweenany two words that occurr in the same synset.
Theresulting graph is a graph G(W,E) where W is aset of word / part-of-speech pairs for all the wordsin WordNet.
E is the set of edges connectingeach pair of synonymous words.
Nodes representword/pos pairs rather than words because the partof speech tags are helpful in disambiguating thedifferent senses for a given word.
For example,397the word ?fine?
has two different meanings whenused as an adjective and as a noun.Several other methods could be used to linkwords.
For example, we can use other WordNetrelations: hypernyms, similar to,...etc.
Anothersource of links between words is co-occurrencestatistics from corpus.
Following the method pre-sented in (Hatzivassiloglou and McKeown, 1997),we can connect words if they appear in a conjunc-tive form in the corpus.
This method is only appli-cable to adjectives.
If two adjectives are connectedby ?and?
in conjunctive form, it is highly likelythat they have the same semantic orientation.
Inall our experiments, we restricted the network toonly WordNet relations.
We study the effect of us-ing co-occurrence statistics to connect words laterat the end of our experiments.
If more than one re-lation exists between any two words, the strengthof the corresponding edge is adjusted accordingly.3.2 Random Walk ModelImagine a random surfer walking along the wordrelatedness graph G. Starting from a word withunknown polarity i , it moves to a node j withprobability Pij after the first step.
The walk con-tinues until the surfer hits a word with a knownpolarity.
Seed words with known polarity act asan absorbing boundary for the random walk.
Ifwe repeat the number of random walks N times,the percentage of time at which the walk ends ata positive/negative word could be used as an in-dicator of its positive/negative polarity.
The aver-age time a random walk starting at w takes to hitthe set of positive/negative nodes is also an indi-cator of its polarity.
This view is closely relatedto the partially labeled classification with randomwalks approach in (Szummer and Jaakkola, 2002)and the semi-supervised learning using harmonicfunctions approach in (Zhu et al, 2003).Let W be the set of words in our lexicon.
Weconstruct a graph whose nodes V are all wordsin W The edges E correspond to relatedness be-tween words We define transition probabilitiesPt+1|t(j|i) from i to j by normalizing the weightsof the edges out of node i, so:Pt+1|t(j|i) = Wij/?kWik (1)where k represents all nodes in the neighborhoodof i. Pt2|t1(j|i) denotes the transition probabilityfrom node i at step t1 to node j at time step t2.We note that the weights Wij are symmetric andthe transition probabilities Pt+1|t(j|i) are not nec-essarily symmetric because of the node out degreenormalization.3.3 First-Passage TimeThe mean first-passage (hitting) time h(i|k) is de-fined as the average number of steps a randomwalker, starting in state i 6= k, will take to en-ter state k for the first time (Norris, 1997).
LetG = (V,E) be a graph with a set of vertices V ,and a set of edges E. Consider a subset of verticesS ?
V , Consider a random walk on G starting atnode i 6?
S. Let Nt denote the position of the ran-dom surfer at time t. Let h(i|S) be the the averagenumber of steps a random walker, starting in statei 6?
S, will take to enter a state k ?
S for the firsttime.
Let TS be the first-passage for any vertex inS.P (TS = t|N0 = i) =?j?Vpij ?
P (TS = t?
1|N0 = j) (2)h(i|S) is the expectation of TS .
Hence:h(i|S) = E(TS |N0 = i)=??t=1t?
P (TS = t|N0 = i)=?
?t=1t?j?VpijP (TS = t?
1|N0 = j)=?j?V??t=1(t?
1)pijP (TS = t?
1|N0 = j)+?j?V?
?t=1pijP (TS = t?
1|N0 = j)=?j?Vpij?
?t=1tP (TS = t|N0 = j) + 1=?j?Vpij ?
h(j|S) + 1 (3)Hence the first-passage (hitting) time can be for-mally defined as:h(i|S) ={0 i ?
S?j?V pij ?
h(j|S) + 1 otherwise(4)3.4 Word Polarity CalculationBased on the description of the random walkmodel and the first-passage (hitting) time above,398we now propose our word polarity identificationalgorithm.
We begin by constructing a word relat-edness graph and defining a random walk on thatgraph as described above.
Let S+ and S?
be twosets of vertices representing seed words that arealready labeled as either positive or negative re-spectively.
For any given word w, we compute thehitting time h(w|S+), and h(w|S?)
for the twosets iteratively as described earlier.
if h(w|S+)is greater than h(w|S?
), the word is classified asnegative, otherwise it is classified as positive.
Theratio between the two hitting times could be usedas an indication of how positive/negative the givenword is.
This is useful in case we need to pro-vide a confidence measure for the prediction.
Thiscould be used to allow the model to abstain fromclassifying words with when the confidence levelis low.Computing hitting time as described earlier maybe time consuming especially if the graph is large.To overcome this problem, we propose a MonteCarlo based algorithm for estimating it.
The algo-rithm is shown in Algorithm 1.Algorithm 1 Word Polarity using Random WalksRequire: A word relatedness graph G1: Given a word w in V2: Define a randomwalk on the graph.
the transi-tion probability between any two nodes i, andj is defined as: Pt+1|t(j|i) = Wij/?k Wik3: Start k independent random walks from wwith a maximum number of steps m4: Stop when a positive word is reached5: Let h?
(w|S+) be the estimated value forh(w|S+)6: Repeat for negative words computingh?(w|S?
)7: if h?
(w|S+) ?
h?(w|S?)
then8: Classify w as positive9: else10: Classify w as negative11: end if4 ExperimentsWe performed experiments on the General In-quirer lexicon (Stone et al, 1966).
We used itas a gold standard data set for positive/negativewords.
The dataset contains 4206 words, 1915 ofwhich are positive and 2291 are negative.
Some ofthe ambiguous words were removed like (Turney,2002; Takamura et al, 2005).We use WordNet (Miller, 1995) as a sourceof synonyms and hypernyms for the word relat-edness graph.
We used 10-fold cross validationfor all tests.
We evaluate our results in terms ofaccuracy.
Statistical significance was tested us-ing a 2-tailed paired t-test.
All reported resultsare statistically significant at the 0.05 level.
Weperform experiments varying the parameters andthe network.
We also look at the performance ofthe proposed method for different parts of speech,and for different confidence levels We compareour method to the Semantic Orientation from PMI(SO-PMI) method described in (Turney, 2002),the Spin model (Spin) described in (Takamura etal., 2005), the shortest path (short-path) describedin (Kamps et al, 2004), and the bootstrapping(bootstrap) method described in (Hu and Liu,2004).4.1 Comparisons with other methodsThis method could be used in a semi-supervisedsetting where a set of labeled words are used andthe system learns from these labeled nodes andfrom other unlabeled nodes.
Under this setting, wecompare our method to the spin model describedin (Takamura et al, 2005).
Table 2 compares theperformance using 10-fold cross validation.
Thetable shows that the proposed method outperformsthe spin model.
The spin model approach usesword glosses, WordNet synonym, hypernym, andantonym relations, in addition to co-occurrencestatistics extracted from corpus.
The proposedmethod achieves better performance by only usingWordNet synonym, hypernym and similar to rela-tions.
Adding co-occurrence statistics slightly im-proved performance, while using glosses did nothelp at all.We also compare our method to the SO-PMImethod presented in (Turney, 2002).
They de-scribe this setting as unsupervised (Turney, 2002)because they only use 14 seeds as paradigm wordsthat define the semantic orientation rather thantrain the model.
After (Turney, 2002), we use ourmethod to predict semantic orientation of words inthe General Inquirer lexicon (Stone et al, 1966)using only 14 seed words.
The network we usedcontains only WordNet relations.
No glosses orco-occurrence statistics are used.
The results com-paring the SO-PMI method with different datasetsizes, the spin model, and the proposed methodusing only 14 seeds is shown in Table 2.
We no-399Table 1: Accuracy for adjectives only for the spinmodel, the bootstrap method, and the randomwalk model.spin-model bootstrap short-path rand-walks83.6 72.8 68.8 88.8tice that the randomwalk method outperforms SO-PMI when SO-PMI uses datasets of sizes 1?
107and 2 ?
109 words.
The performance of SO-PMIand the random walk methods are comparablewhen SO-PMI uses a very large dataset (1 ?
1011words).
The performance of the spin model ap-proach is also comparable to the other 2 meth-ods.
The advantages of the random walk methodover SO-PMI is that it is faster and it does notneed a very large corpus like the one used by SO-PMI.
Another advantage is that the random walkmethod can be used along with the labeled datafrom the General Inquirer lexicon (Stone et al,1966) to get much better performance.
This iscostly for the SO-PMI method because that willrequire the submission of almost 4000 queries to acommercial search engine.We also compare our method to the bootstrap-ping method described in (Hu and Liu, 2004), andthe shortest path method described in (Kamps etal., 2004).
We build a network using only Word-Net synonyms and hypernyms.
We restrict the testset to the set of adjectives in the General Inquirerlexicon (Stone et al, 1966) because this methodis mainly interested in classifying adjectives.
Theperformance of the spin model method, the boot-strapping method, the shortest path method, andthe random walk method for only adjectives isshown in Table 1.
We notice from the table that therandom walk method outperforms both the spinmodel, the bootstrapping method, and the short-est path method for adjectives.
The reported ac-curacy for the shortest path method only considersthe words it could assign a non-zero orientationvalue.
If we consider all words, the accuracy willdrop to around 61%.4.1.1 Varying ParametersAs we mentioned in Section 3.4, we use a param-eter m to put an upper bound on the length of ran-dom walks.
In this section, we explore the impactTable 2: Accuracy for SO-PMI with differentdataset sizes, the spin model, and the randomwalks model for 10-fold cross validation and 14seeds.- CV 14 seedsSO-PMI (1?
107) - 61.3SO-PMI (2?
109) - 76.1SO-PMI (1?
1011) - 82.8Spin Model 91.5 81.9Random Walks 93.1 82.1of this parameter on our method?s performance.Figure 1 shows the accuracy of the randomwalkmethod as a function of the maximum number ofsteps m. m varies from 5 to 50.
We use a net-work built from WordNet synonyms and hyper-nyms only.
The number of samples k was set to1000.
We perform 10-fold cross validation usingthe General Inquirer lexicon.
We notice that themaximum number of steps m has very little im-pact on performance until it rises above 30.
Whenit does, the performance drops by no more than1%, and then it does not change anymore as mincreases.
An interesting observation is that theproposed method performs quite well with a verysmall number of steps (around 10).
We looked atthe dataset to understand why increasing the num-ber of steps beyond 30 negatively affects perfor-mance.
We found out that when the number ofsteps is very large, compared to the diameter of thegraph, the random walk that starts at ambiguouswords, that are hard to classify, have the chanceof moving till it hits a node in the opposite class.That does not happen when the limit on the num-ber of steps is smaller because those walks are thenterminated without hitting any labeled nodes andhence ignored.Next, we study the effect of the random of sam-ples k on our method?s performance.
As explainedin Section 3.4, k is the number of samples usedby the Monte Carlo algorithm to find an estimatefor the hitting time.
Figure 2 shows the accuracyof the random walks method as a function of thenumber of samples k. We use the same settings asin the previous experiment.
the only difference isthat we fix m at 15 and vary k from 10 to 20000(note the logarithmic scale).
We notice that theperformance is badly affected, when the value ofk is very small (less than 100).
We also notice that400after 1000, varying k has very little, if any, effecton performance.
This shows that the Monte Carloalgorithm for computing the random walks hittingtime performs quite well with values of the num-ber of samples as small as 1000.The preceding experiments suggest that the pa-rameter have very little impact on performance.This suggests that the approach is fairly robust(i.e., it is quite insensitive to different parametersettings).Figure 1: The effect of varying the maximumnumber of steps (m) on accuracy.Figure 2: The effect of varying the number of sam-ples (k) on accuracy.4.1.2 Other ExperimentsWe now measure the performance of the proposedmethod when the system is allowed to abstainfrom classifying the words for which it have lowconfidence.
We regard the ratio between the hit-ting time to positive words and hitting time to neg-ative words as a confidence measure and evaluatethe top words with the highest confidence level atdifferent values of threshold.
Figure 4 shows theaccuracy for 10-fold cross validation and for us-ing only 14 seeds at different thresholds.
We no-tice that the accuracy improves by abstaining fromclassifying the difficult words.
The figure showsthat the top 60% words are classified with an ac-curacy greater than 99% for 10-fold cross valida-tion and 92% with 14 seed words.
This may becompared to the work descibed in (Takamura etal., 2005) where they achieve the 92% level whenthey only consider the top 1000 words (28%).Figure 3 shows a learning curve displaying howthe performance of the proposed method is af-fected with varying the labeled set size (i.e., thenumber of seeds).
We notice that the accuracy ex-ceeds 90% when the training set size rises above20%.
The accuracy steadily increases as the la-beled data increases.We also looked at the classification accuracy fordifferent parts of speech in Figure 5. we noticethat, in the case of 10-fold cross validation, theperformance is consistent across parts of speech.However, when we only use 14 seeds all of whichare adjectives, similar to (Turney and Littman,2003), we notice that the performance on adjec-tives is much better than other parts of speech.When we use 14 seeds but replace some of theadjectives with verbs and nouns like (love, harm,friend, enemy), the performance for nouns andverbs improves considerably at the cost of losing alittle bit of the performance on adjectives.
We hada closer look at the results to find out what are thereasons behind incorrect predictions.
We foundtwo main reasons.
First, some words are ambigu-ous and has more than one sense, possible withdifferent orientations.
Disambiguating the senseof words given their context before trying to pre-dict their polarity should solve this problem.
Thesecond reason is that some words have very fewconnection in thesaurus.
A possible solution tothis might be identifying those words and addingmore links to them from glosses of co-occurrencestatistics in corpus.Figure 3: The effect of varying the number ofseeds on accuracy.401Figure 4: Accuracy for words with high confi-dence measure.Figure 5: Accuracy for different parts of speech.5 ConclusionsPredicting the semantic orientation of words isa very interesting task in Natural Language Pro-cessing and it has a wide variety of applications.We proposed a method for automatically predict-ing the semantic orientation of words using ran-dom walks and hitting time.
The proposed methodis based on the observation that a random walkstarting at a given word is more likely to hit an-other word with the same semantic orientation be-fore hitting a word with a different semantic ori-entation.
The proposed method can be used in asemi-supervised setting where a training set of la-beled words is used, and in an unsupervised settingwhere only a handful of seeds is used to define thetwo polarity classes.
We predict semantic orienta-tion with high accuracy.
The proposed method isfast, simple to implement, and does not need anycorpus.AcknowledgmentsThis research was funded by the Office of theDirector of National Intelligence (ODNI), In-telligence Advanced Research Projects Activity(IARPA), through the U.S. Army Research Lab.All statements of fact, opinion or conclusions con-tained herein are those of the authors and shouldnot be construed as representing the official viewsor policies of IARPA, the ODNI or the U.S. Gov-ernment.ReferencesAlina Andreevskaia and Sabine Bergler.
2006.
Min-ing wordnet for fuzzy sentiment: Sentiment tag ex-traction from wordnet glosses.
In Proceedings ofthe 11th Conference of the European Chapter of theAssociation for Computational Linguistics (EACL2006).Carmen Banea, Rada Mihalcea, and Janyce Wiebe.2008.
A bootstrapping method for building subjec-tivity lexicons for languages with scarce resources.In Proceedings of the Sixth International LanguageResources and Evaluation (LREC?08).Andrea Esuli and Fabrizio Sebastiani.
2005.
Deter-mining the semantic orientation of terms throughgloss classification.
In Proceedings of the 14th Con-ference on Information and Knowledge Manage-ment (CIKM 2005), pages 617?624.Andrea Esuli and Fabrizio Sebastiani.
2006.
Senti-wordnet: A publicly available lexical resource foropinion mining.
In Proceedings of the 5th Confer-ence on Language Resources and Evaluation (LREC2006), pages 417?422.Vasileios Hatzivassiloglou and Kathleen R. McKeown.1997.
Predicting the semantic orientation of adjec-tives.
In Proceedings of the eighth conference onEuropean chapter of the Association for Computa-tional Linguistics, pages 174?181.Vasileios Hatzivassiloglou and Janyce Wiebe.
2000.Effects of adjective orientation and gradability onsentence subjectivity.
In COLING, pages 299?305.Minqing Hu and Bing Liu.
2004.
Mining and sum-marizing customer reviews.
In KDD ?04: Proceed-ings of the tenth ACM SIGKDD international con-ference on Knowledge discovery and data mining,pages 168?177.Jaap Kamps, Maarten Marx, Robert J. Mokken, andMaarten De Rijke.
2004.
Using wordnet to mea-sure semantic orientations of adjectives.
In NationalInstitute for, pages 1115?1118.Hiroshi Kanayama and Tetsuya Nasukawa.
2006.Fully automatic lexicon expansion for domain-oriented sentiment analysis.
In Proceedings of the2006 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2006), pages 355?363.Soo-Min Kim and Eduard Hovy.
2004.
Determin-ing the sentiment of opinions.
In Proceedings ofthe 20th international conference on ComputationalLinguistics (COLING 2004), pages 1367?1373.402George A. Miller.
1995.
Wordnet: a lexical databasefor english.
Commun.
ACM, 38(11):39?41.Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi,and Toshikazu Fukushima.
2002.
Mining prod-uct reputations on the web.
In KDD ?02: Proceed-ings of the eighth ACM SIGKDD international con-ference on Knowledge discovery and data mining,pages 341?349.Tetsuya Nasukawa and Jeonghee Yi.
2003.
Senti-ment analysis: capturing favorability using naturallanguage processing.
In K-CAP ?03: Proceedingsof the 2nd international conference on Knowledgecapture, pages 70?77.J.
Norris.
1997.
Markov chains.
Cambridge Univer-sity Press.Ana-Maria Popescu and Oren Etzioni.
2005.
Extract-ing product features and opinions from reviews.
InHLT ?05: Proceedings of the conference on Hu-man Language Technology and Empirical Methodsin Natural Language Processing, pages 339?346.Ellen Riloff and Janyce Wiebe.
2003.
Learning extrac-tion patterns for subjective expressions.
In Proceed-ings of the 2003 conference on Empirical methods innatural language processing, pages 105?112.Philip Stone, Dexter Dunphy, Marchall Smith, andDaniel Ogilvie.
1966.
The general inquirer: A com-puter approach to content analysis.
The MIT Press.Martin Szummer and Tommi Jaakkola.
2002.
Partiallylabeled classification with markov random walks.In Advances in Neural Information Processing Sys-tems, pages 945?952.Hiroya Takamura, Takashi Inui, and Manabu Okumura.2005.
Extracting semantic orientations of words us-ing spin model.
In ACL ?05: Proceedings of the 43rdAnnual Meeting on Association for ComputationalLinguistics, pages 133?140.Richard M. Tong.
2001.
An operational system for de-tecting and tracking opinions in on-line discussion.Workshop note, SIGIR 2001 Workshop on Opera-tional Text Classification.Peter Turney and Michael Littman.
2003.
Measuringpraise and criticism: Inference of semantic orienta-tion from association.
ACM Transactions on Infor-mation Systems, 21:315?346.Peter D. Turney.
2002.
Thumbs up or thumbs down?
:semantic orientation applied to unsupervised classi-fication of reviews.
In ACL ?02: Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, pages 417?424.Janyce Wiebe, Rebecca Bruce, Matthew Bell, MelanieMartin, and Theresa Wilson.
2001.
A corpus studyof evaluative and speculative language.
In Proceed-ings of the Second SIGdial Workshop on Discourseand Dialogue, pages 1?10.Janyce Wiebe.
2000.
Learning subjective adjectivesfrom corpora.
In Proceedings of the SeventeenthNational Conference on Artificial Intelligence andTwelfth Conference on Innovative Applications ofArtificial Intelligence, pages 735?740.Hong Yu and Vasileios Hatzivassiloglou.
2003.
To-wards answering opinion questions: separating factsfrom opinions and identifying the polarity of opinionsentences.
In Proceedings of the 2003 conference onEmpirical methods in natural language processing,pages 129?136.Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.2003.
Semi-supervised learning using gaussianfields and harmonic functions.
In In ICML, pages912?919.403
