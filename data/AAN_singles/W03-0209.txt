Automated Rating of ESL EssaysDeryle LonsdaleBYU Linguistics and English Languagelonz@byu.eduDiane Strong-KrauseBYU Linguistics and English Languagediane strong-krause@byu.eduAbstractTo date, traditional NLP parsers havenot been widely successful in TESOL-oriented applications, particularly in scor-ing written compositions.
Re-engineeringsuch applications to provide the neces-sary robustness for handling ungrammat-ical English has proven a formidable ob-stacle.
We discuss the use of a non-traditional parser for rating compositionsthat attenuates some of these difficulties.Its dependency-based shallow parsing ap-proach provides significant robustness inthe face of language learners?
ungrammat-ical compositions.
This paper discusseshow a corpus of L2 essays for English wasrated using the parser, and how the auto-matic evaulations compared to those ob-tained by manual methods.
The types ofmodifications that were made to the sys-tem are discussed.
Limitations to the cur-rent system are described, future plans fordeveloping the system are sketched, andfurther applications beyond English essayrating are mentioned.1 IntroductionRating constructed response items, particularly es-says, is a time-consuming effort.
This is true inrating essays written by second-language speakers.To make this process more manageable, researchershave investigated how to involve computers in thegrading process.
Several factors suggest why au-tomating scoring might be desirable: (i) practical-ity: essay grading is costly and time-consuming;(ii) consistency: essay grading is somewhat subjec-tive in nature, and consistency may sometimes suf-fer; and (iii) feedback: Providing feedback to a stu-dent is important, and automated scoring can pro-vide ways of generating specific suggestions tailoredto the needs of the author.However, computerized rating of essays writtenby second-language speakers poses unique dilem-mas, particularly for responses written by exami-nees at low levels of language proficiency.
Wherewe expect generally well-formed sentences from na-tive English speaker responses, we find that the ma-jority of the responses by lower proficiency second-language English speakers will be made up of ill-formed sentences.Previous work in automated essay grading andrelated technologies has been surveyed and dis-cussed in several different forums (Burstein andChodorow, 1999; Thompson, 1999; Hearst, 2000;Williams, 2001; Rudner and Gagne, 2001), anda thorough survey of the field has recently beenpublished (Shermis and Burstein, 2003).
Typi-cally these approaches have borrowed techniquesand tools from several natural language processing(NLP) fields.
For example, the knowledge-based en-gines have been used for analyzing essays: parsers(Carbonell and Hayes, 1984; Schneider and McCoy,1998), grammar and spelling checkers (Park et al,1997), discourse processing analyzers (Miltsakakiand Kukich, 2000), and other hand-crafted linguisticknowledge sources.Linkage 1, cost vector = (UNUSED=0 DIS=1 AND=0 LEN=20)+-------------------------------Xp------------------------------++--------------Wd--------------+ || +----------CO---------+ || +--------Xc--------+ | || +-----Jp----+ | | +------Op-----+ || | +--Dmu-+ | +-Sp*i+--PPf-+ +--Dmc-+ || | | | | | | | | | |LEFT-WALL during my schooling.n , I.p have.v taken.v many classes.n .Figure 1: Sample link-parsed sentence with associated cost vector.On the other hand, much work has leveraged sta-tistical methods in detecting properties of student es-says via stylometrics (Aaronson, 2001)1, latent se-mantic indexing (Wiemer-Hastings et al, 1998), andfeature analysis.Finally, mirroring noteworthy progress in otherNLP fields involving data-driven methods, recentwork has involved essay grading via exemplar-basedmachine learning techniques (Chodorow and Lea-cock, 2000).The most visible systems implement one (ormore) of these approaches.
The Project EssayGrade (PEG) system, for example, uses lexically-based metrics in scoring (Page, 2003).
The Intel-ligent Essay Assessor (IEA) uses latent semanticanalysis in calculating its metrics (Landauer et al,2003).
The E-Rater system by Educational TestingServices uses syntactic, discourse, and topical (i.e.vocabulary-based) data analysis (Burstein, 2003).Several criticisms have been aimed at automaticscoring systems on both theoretical and implementa-tional grounds.
For example, many systems exhibitan inherent Achilles?
heel since it is possible to trickthem into evaluating a nonsensical text purely byreverse-engineering the scoring mechanism and de-signing a text that responds to the criteria.
Anotherproblem is the cost of development, which can besubstantial.
In addition, most systems are designedaround certain specific topics in order to focus ter-minology and vocabulary in limited subdomains; in-troducing new subject areas requires building a newmodel, often a nontrivial process.
Thus, many sys-tems are often not adaptable enough to meet the par-ticular needs of an individual, class, teacher, or in-stitution.1This work also uses the Link Grammar parser.The purpose of our research is to explore the useof a particular natural language processing (NLP)approach for automated scoring of essays writtenby ESL students at lower levels of language pro-ficiency Our goal for the system reflects common-sense (though ambitious) criteria: to have the sys-tem?s scores agree with those assigned by humanraters at least as often as human raters agree amongthemselves.2 The parserAs mentioned above, one approach to grading isto use a parsing system, along with its associatedknowledge sources, to analyze the correctness ofa text.
The NLP field has produced a wide rangeof parsers and grammars to support them.
Mostof the widely used and highly accurate parsers areclosely (or even inalienably) tied to a particular syn-tactic theory: XTAG with Tree-Adjoining Gram-mar2, LFG-WB with Lexical-Functional Grammar3,ALE with HPSG or Categorial Grammar4, and soon.
Principled coverage of grammatical phenomenacan thus be tied closely to the linguistic theory inquestion.
Some parsers are designed to skip overungrammatical and disfluent portions of input andhave been successfully applied to speech and dia-logue processing (Rose?, 1997), with perhaps possi-ble future application to rating ESL essays.
Thereare disadvanges to traditional parsers, though, whichoffset their usefulness for automated grading.Consider, for example, that the encoding of anyparser?s phrase-structure component is costly, com-plex, and dependent on significant lexical resources.This precludes involvement of the uninitiated.
Even2See http://www.cis.upenn.edu/?xtag/3See http://www2.parc.com/istl/groups/nltt/medley/4See http://www.cs.toronto.edu/?gpenn/ale.htmlmore serious is the lack of robustness that mostparsers entail.
Most linguistic formalisms focus pre-cisely on what is grammatical, and not on whatis ungrammatical.
This often becomes an archi-tectural assumption in the way parsers are imple-mented.
The result is that such systems are ratherinflexible, particularly in the face of ungrammaticalinput?ungrammaticaly is almost always avoided inboth the theory and in its implementation.
Yet cru-cially for the essay grading ungrammatical input isfrequent and expected.One method used to sidestep the robustness issueis to explicitly encode rules reflecting ungrammati-cality, called ?mal-rules?
(McCoy et al, 1996).
Forexample, the following is a possible mal-rule for anLFG parser:S --> NP (agr ?a) VP (agr ?
?a)This rule says that a sentence can consist of an NPand a VP whose respective agreement features doNOT agree.
While such a technique allows for de-tection of ungrammatical sentences, it introducestwo problems.
First, the computational complex-ity of a parsing system increases as such rules areadded to the phrase-structure component.
Second,maintaining a knowledge base of such informationis a complicated and never-ending proposition, asstudent errors vary in a seemingly infinite numberof ways.In our work we chose to use a different kind ofparser, the link grammer parser (Sleator and Tem-perley, 1993).
This parser has been developed forrobust, efficient processing of dependency-style syn-tax (Grinberg et al, 1995).
Freely available for re-search purposes, it is more robust than traditionalparsers and has been widely used in such NLP appli-cations as information retrieval, speech recognition,and machine translation5.
Written in the C program-ming language, it is comparatively fast and efficient.The link grammar parser does not seek toconstruct constituents in the traditional linguisticsense?instead, it calculates simple, explicit rela-tions between pairs of words.
A link is a targetedrelationship between two words and has two parts:a left side and a right side.
For example, links asso-ciate such word pairs as: subject + verb, verb + ob-ject, preposition + object, adjective + adverbial mod-5For a bibliography see http://link.cs.cmu.edu/link/papers/ifier, and auxiliary + main verb.
Each link has a labelthat expresses the nature of the relationship mediat-ing the two words.
Potential links are specified by aset of technical rules that constrain and permit word-pair associations.
In addition, it is possible to scoreindividual linkages and to penalize unwanted links.Figure 1 shows an example link parse of a sen-tence from a student essay.
Ten links of varioustypes span the various relationships observable inthe sentence.When parses are not possible, the system?s robust-ness allows it to discard words (or alternatively positspelling corrections) in order to arrive at a tenabledescription of the input.
Figure 2 shows two un-grammatical sentences that the parser has nonethe-less coped with.
In the first, it skips over words thatdon?t seem to fit into any grammatical pattern, pars-ing instead a core sentence ?The class is mathemat-ical.?
The cost vector for this sentence records thefact that there were 4 unused words.
In the secondexample, only one word must be discarded to arriveat a reasonable parse.The LG parser as distributed was not completelysuited to handle the grading of ESL students?
es-says, so some modifications had to be made.
Lex-ical items had to be added to the system?s lexiconto cover terms frequently used by students, suchas acronyms: E.L.C.
(the English Language Cen-ter), R.O.C.
(Republic of China), and so on.
Otherconstructions not supported in the standard releasewere also added, for example variant ordering withindates (e.g.
24 May as well as May 24).
The grammaras originally distributed did not allow for optionalcommas where unexpected.
It also did not penalizecertain ungrammatical constructions (e.g.
missingdeterminers, as in ?I am student of English.?)
sincesuch constructions were not anticipated.With the system slightly modified as describedabove, it was well suited to parsing ESL essays.
Twomore example parses of student essay sentences areillustrated in Figure 3.
In the next section we discusshow it was used to score such essays.3 The corpusOur study involved using the LG parser to rate es-says based on the results of a link parse for each sen-tence.
We used ESL essays written by Intensive En-Linkage 1, cost vector = (UNUSED=4 DIS=0 AND=0 LEN=11)+--------------------------------Xp-------------------------------++-----Wd-----+ || +-D*u-+------------Ss-----------+---Ost--+ || | | | | |LEFT-WALL the class.n [most] [important] is.v Mathematical [for] [my] .Linkage 1, cost vector = (UNUSED=1 DIS=0 AND=0 LEN=17)+----------------------------------Xp----------------------------------+| +--------------MVp-------------+ || +----I----+------MVp------+ +----Js----+ |+------Wi-----+-Ox-+ +---Op--+ +--Jp--+ | +--Ds-+ || | | | | | | | | | |LEFT-WALL [it] help.v me make.v friends.n with people.p around the world.n .Figure 2: Link parser results for highly ungrammatical sentences.
Note the discarded words indicated insquare brackets.glish students who spanned a range of ability fromNovice-mid to Intermediate-high.
The essays wereon a variety of assigned topics, and each had to bewritten within a 30-minute time limit.Our corpus consists of 301 human-rated essays intotal consisting of some 50,000 words and 3400 sen-tences.
These were sub-divided further by semesterinto 5 sub-corpora.
The essays exhibited the follow-ing characteristics: each had (on average) 165 wordsand 11.2 sentences, and each sentence had on aver-age 14.75 words.
Note the wide variety of errors inthis typical sentence from one essay:Iwork really hard and occacionallyI don?t have time for have funwhith mt friens but i don?t mindbecausse i knew ,when i grow upi will have a profesion and havea good job and i will be veryhappy.Each essay was given a holistic rating by twohuman judges.
Different raters participated eachsemester, though there was likely a small degree ofoverlap among raters across subsequent semesters.Scores ranged from 1 to 5 with half-points possi-ble (i.e.
essays could receive 1, 1.5, 2, 2.5, 3, 3.5,4, 4.5, and 5).
Occasionally a judge gave a rating of0 indicating that no comprehensible language waspresent.
Inter-rater agreement, where each humanassigned a score within one point of the other, was98% over the corpora.The following categories describe scoring levels:1.
Demonstrates limited ability to write Englishwords and sentences.
Sentences and para-graphs may be incomplete and difficult to fol-low.2.
Writes a simple paragraph with a fair controlof basic, not complex, sentences structures.
Er-rors occur in almost all sentences except for themost basic, formula-type (memorized) struc-tures.
Little detail is present.3.
Writes a fairly long paragraph with relativelysimple sentence structures.
Personal experi-ences and some emotions can be expressed,but much detail is missing.
Frequent errors ingrammar and word use make reading somewhatburdensome.4.
Writes long groups of paragraphs with somecomplex sentence patterns.
Some transitionsare used effectively.
Vocabulary is broaden-ing, but some wrong word use.
Grammar er-rors may detract from meaning.
Some ideasare supported with detail.
Some notion of anintroduction and conclusion is included.5.
Writes complex thoughts using complex sen-tence patterns with effective transitions be-tween ideas and sentences.
Errors in gram-mar exist but do not obscure meaning.
A va-riety of advanced vocabulary words are usedbut some wrong use occurs, including problemswith prepositions and articles.
Ideas are clearlysupported with details.
Effective introductionand conclusion are included.Although judges gave a holistic rating based ona number of features of the essay, the category de-scriptions hint that syntax (and to some degree vo-cabulary use) is the focus for at least categories 1through 3, and even much of category 4.4 Results and analysisThe corpus was partitioned into two classes: the de-velopment set and the test set.
The former was usedto develop and tune the system, and consisted of themost recent set of essays (60 in number) dating fromWinter 2002 semester.
The other 4 (earlier) corporawere used for testing the system.Each essay was sent through the LG parser a sen-tence at a time, and each sentence was given a 5-point score based on the parse?s cost vector.
Anoverall score for each essay was computed as theaverage across sentence scores, after discarding thelowest and highest ones.
The overall score wasthen compared with those of the human raters.
Forthe development set, the system agreed 67% of thetime with human raters, where agreement followsthe standardly accepted definition of falling within1.0 of the closest human?s score.
Note that since hu-mans only gave ratings involving integers and half-steps between them, all computer-generated scoreswere rounded to the nearest integer or half-step asappropriate.The system was then run on the previouslyunseen test corpus, actually consisting of essaysfrom four separate semesters.
The test corpus scoreswere as follows:Fall01: 82 students 69.5% agreementSummer01: 58 students 62.1% agreementWinter01: 36 students 66.7% agreementWinter92: 75 students 62.2% agreementHence over a corpus of some 300 essays and fivesub-corpora, system agreement with human raterswas achieved about 66% of the time.
We now turnto a brief analysis of interesting results that emergedfrom the system?s performance.Generally, the system tended to over-score essayswith very low human scores (i.e.
those in the 1-2 point range).
It also tended to under-score es-says with high human scores and complex run-onsentences.
This reflects the observation that run-onsentences, which were very plentiful, were penal-ized by the system but largely forgiven by humanraters.
Also, the system?s scoring matched humanvalues better for midrange-scored essays, and worsefor extreme examples (i.e.
with average score < 2or > 4.5).
Finally, system panics (when the systemran out of allotted time without successfully parsinga sentence) occurred most frequently when severalconjunctions appeared in a single sentence.It is informative to look at an essay that reflectsone of the most extreme mismatches between humanand computer ratings.
In this case, the two humanraters gave the essay scores of 1 and 2 respectively,whereas the computer scored the essay at 4.40.My free time is very fun.
BecauseI meet my friends.
We goes toplay.
For exmple, I went tomovies, recreation ground, tripand shopping with them.
I can?twrite English.Another observation from the present work is thatperforming a purely syntactic parse does not al-ways assure appropriate ratings.
The current sys-tem?s scoring mechanism occasionally results in ar-tificially high scores.
Consider, for example, thesentence in Figure 4.
Even though there are noegregious syntactic errors, violations of selectionalrestriction, collocation, determiner selection, andverbal aspect render the sentence highly unnatural,though this is undetected by the current parser.
Ad-dition of hand-coded postprocessor rules may helpavoid such situations, and is possible with the parser.5 Future workThere are several ways in which the base system de-scribed in this paper can be improved.
For instance,sentence and essay scores are currently based onstraightforward values from the cost vector, whereasmore sophisticated measures can be implemented.Future work will involve using statistical smooth-ing to improve performance in the extreme (high-scoring and low-scoring) situations.Linkage 1, cost vector = (UNUSED=0 DIS=2 AND=0 LEN=23)+-----------------------------------------Xp----------------------------------------+| +-----------------------MVp-----------------------+ || +---------------MVp--------------+ | || | +-------Jp-------+ +----Js---+ | |+--Wd--+Sp*+-PPf-+--Pg*b--+--MVp-+ +----AN----+ | +---D--+ +-Js+ || | | | | | | | | | | | | |LEFT-WALL I.p ?ve been.v majoring.v in Material engineering.n at my University in Korea .Linkage 1, cost vector = (UNUSED=0 DIS=2 AND=0 LEN=27)+----------------------------------------------Xp----------------------------------------------+| +-----------Wdc-----------+ +------------------Opt-----------------+ || | +--------CO--------+ | +--------------AN-------------+ || | | +-----D*u----+-------Ss------+ | +-------AN-------+ |+--Wc--+ | +--La-+ +--Mp--+--J-+ | | | +----AN---+ || | | | | | | | | | | | | |LEFT-WALL but probably the best.a class.n for.p me was.v medicine.n and first.n aid.n principles.n .Figure 3: Sample link-parsed sentences from the student essay corpus.Linkage 1, cost vector = (UNUSED=0 DIS=1 AND=0 LEN=13)+--------------------------------Xp--------------------------------++------Wd------+---------Ss---------+ +---Jp---+ || +--D*u--+--Mp--+--Jp-+ +--Pg*b--+---MVp--+ +-D*u-+ || | | | | | | | | | |LEFT-WALL the practice.n in English.n is.v progressing.v in the life.n .Figure 4: Sentence illustrating collocational and selectional problems.The system could also achieve more human-likescoring by integrating data-driven, exemplar-basedapproaches.
Training the system to relate salient fea-tures and vector costs of the essays with the corre-sponding human scores can be done using any ofa variety of available techniques, such as memory-based learning or analogical modeling.Finally, further linguistic processing can improvethe system.
Some syntactic improvements can bemade, including specifying licit structures that arenot recognized as well as unacceptable structureswith their associated coses.
As mentioned above,pushing the analysis beyond a syntactic LG parsewill be necessary.
This might involve leveragingresources such as WordNet (Fellbaum, 1998) forproviding lexical semantic information which couldprove useful in scoring compounding strategies, col-locations, and verb argument structure formation.Current research in discourse processing using suchdevices as centering, anaphor/coreference, coher-ence, and topic continuity can also be integrated intothe system as has been done in other scoring pro-grams.In addition, the LG parser is also being developedfor other languages (e.g.
French and Spanish) withthe eventual goal of providing a scoring engine forlearners of these languages as well.6 ConclusionsWe have shown how the output from a non-traditional syntactic parser can be used to grade ESLessays.
With a robust enough parser, reasonable re-sults can be obtained, even for highly ungrammati-cal text.
We anticipate that this foundation can beimproved upon by using other commonly adoptedNLP techniques.
Its applicability should extend toother languages besides English given a comparableLG parser implementation.ReferencesScott Aaronson.
2001.
Stylometric clustering: a compar-ison of data-driven and syntactic features.
Manuscript.http://www.cs.berkeley.edu/?aaronson/sc.doc.Jill Burstein and Martin Chodorow.
1999.
Automatedessay scoring for nonnative English speakers.
In Com-puter Mediated Language Assessment and Evaluationin Natural Language Processing, pages 68?75.
Asso-ciation for Computational Linguistics.Jill Burstein.
2003.
The E-rater scoring engine: Auto-mated essay scoring with natural language processing.In Mark D. Shermis and Jill C. Burstein, editors, Auto-mated Essay Scoring: A Cross-Disciplinary Perspec-tive.
Lawrence Erlbaum, Mahwah, NJ.Jaime G. Carbonell and Phillip J. Hayes.
1984.
Copingwith extragrammaticality.
In Proceedings of COLING?84, pages 437?443.
Association for ComputationalLinguistics.Martin Chodorow and Claudia Leacock.
2000.
An unsu-pervised method for detecting grammatical errors.
InProceedings of ANLP-NAACL 2000, pages 140?147.Morgan Kaufmann Publishers.Christiane Fellbaum.
1998.
WordNet: An electronic lex-ical database.
MIT Press, Cambridge, MA.Dennis Grinberg, John Lafferty, and Daniel Sleator.1995.
A robust parsing algorithm for Link Grammars.Technical Report CMU-CS-95-125, School of Com-puter Science, August.Marti A. Hearst.
2000.
The debate on automatedessay grading.
IEEE Intelligent Systems, Septem-ber/October 2000:22?37.Thomas Landauer, Darrell Laham, and Peter Foltz.
2003.Automated scoring and annotation of essays with theIntelligent Essay Assessor.
In Mark D. Shermis andJill C. Burstein, editors, Automated Essay Scoring:A Cross-Disciplinary Perspective.
Lawrence Erlbaum,Mahwah, NJ.Kathleen F. McCoy, Christopher A. Pennington, andLinda Z. Suri.
1996.
English error correction: A syn-tactic user model based on principled ?mal-rule?
scor-ing.
In Proceedings of the Fifth International Confer-ence on User Modeling, pages 59?66.
User Modeling,Inc.Eleni Miltsakaki and Karen Kukich.
2000.
The role ofcentering theory?s rough-shift in the teaching and eval-uation of writing skills.
In Proceedings of ACL-2000.Association for Computational Linguistics.Ellis Batten Page.
2003.
Project Essay Grade: PEG.
InMark D. Shermis and Jill C. Burstein, editors, Auto-mated Essay Scoring: A Cross-Disciplinary Perspec-tive.
Lawrence Erlbaum, Mahwah, NJ.Jong C. Park, Martha Palmer, and Gay Washburn.
1997.An English grammar checker as a writing aid for stu-dents of English as a Second Language.
In Proceed-ings of the Conference of Applied Natural LanguageProcessing (ANLP).Carolyn Penstein Rose?.
1997.
Robust Interactive Di-alogue Interpretation.
Ph.D. thesis, School of Com-puter Science, Carnegie Mellon University.Lawrence Rudner and Phill Gagne.
2001.
An overviewof three approaches to scoring written essays by com-puter.
Practical Assessment, Research & Evaluation,7(26).David Schneider and Kathleen McCoy.
1998.
Recogniz-ing syntactic errors in the writing of second languagelearners.
In Proceedings of COLING-ACL 1998, pages1198?1204.
Morgan Kaufmann Publishers.Mark D. Shermis and Jill C. Burstein, editors.
2003.Automated Essay Scoring: A Cross-Disciplinary Per-spective.
Lawrence Erlbaum, Mahwah, NJ.Daniel Sleator and Davy Temperley.
1993.
Parsing En-glish with a Link Grammar.
In Third InternationalWorkshop on Parsing Technologies.Clive Thompson.
1999.
New word order: The attackof the incredible grading machine.
Linguafranca: TheReview of Academic Life, 9(5).Peter Wiemer-Hastings, Arthur C. Graesser, and DerekHarter.
1998.
The foundations and architectureof AutoTutor.
Lecture Notes in Computer Science,1452:334?343.Robert Williams.
2001.
Automated essay grading: Anevaluation of four conceptual models.
In Expand-ing Horizons in Teaching and Learning: Proceedingsof the 10th Annual Teaching Learning Forum.
CurtinUniversity of Technology.
