Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 425?432Manchester, August 2008Coordination Disambiguation without Any SimilaritiesDaisuke KawaharaNational Institute of Information andCommunications Technology,3-5 Hikaridai Seika-cho, Soraku-gun,Kyoto, 619-0289, Japandk@nict.go.jpSadao KurohashiGraduate School of Informatics,Kyoto University,Yoshida-Honmachi, Sakyo-ku,Kyoto, 606-8501, Japankuro@i.kyoto-u.ac.jpAbstractThe use of similarities has been one of themain approaches to resolve the ambigui-ties of coordinate structures.
In this pa-per, we present an alternative method forcoordination disambiguation, which doesnot use similarities.
Our hypothesis isthat coordinate structures are supportedby surrounding dependency relations, andthat such dependency relations rather yieldsimilarity between conjuncts, which hu-mans feel.
Based on this hypothesis, webuilt a Japanese fully-lexicalized genera-tive parser that includes coordination dis-ambiguation.
Experimental results on websentences indicated the effectiveness of ourapproach, and endorsed our hypothesis.1 IntroductionThe interpretation of coordinate structures directlyaffects the meaning of the text.
Addressing co-ordination ambiguities is fundamental to natu-ral language understanding.
Previous studies oncoordination disambiguation suggested that con-juncts in coordinate structures have syntactic orsemantic similarities, and dealt with coordinationambiguities using (sub-)string matching, part-of-speech matching, semantic similarities, and soforth (Agarwal and Boggess, 1992).
Semantic sim-ilarities are acquired from thesauri (Kurohashi andNagao, 1994; Resnik, 1999) or distributional simi-larity (Chantree et al, 2005).c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.For instance, consider the following example:(1) eat Caesar salad and Italian pastaThe above methods detect the similarity betweensalad and pasta using a thesaurus or distributionalsimilarity, and identify the coordinate structurethat conjoins salad and pasta.
They do not use theinformation of the word eat.On the other hand, this coordinate structure canbe analyzed by using selectional preference of eat.Since eat is likely to have salad and pasta as its ob-jects, it is plausible that salad and pasta are coor-dinated.
Such selectional preferences are thoughtto support the construction of coordinate structuresand to yield similarity between conjuncts on thecontrary.We present a method of coordination disam-biguation without using similarities.
Coordinatestructures are supported by their surrounding de-pendency relations that provide selectional prefer-ences.
These relations implicitly work as similari-ties, and thus it is not necessary to use similaritiesexplicitly.In this paper, we focus on Japanese.
Coor-dination disambiguation is integrated in a fully-lexicalized generative dependency parser (Kawa-hara and Kurohashi, 2007).
For the selectionalpreferences, we use lexical knowledge, such ascase frames, which is extracted from a large rawcorpus.The remainder of this paper is organized as fol-lows.
Section 2 summarizes previous work relatedto coordination disambiguation and its integrationinto parsing.
Section 3 briefly describes the back-ground of this study.
Section 4 overviews our idea,and section 5 describes our model in detail.
Sec-tion 6 is devoted to our experiments.
Finally, sec-tion 7 gives the conclusions.4252 Related WorkPrevious work on coordination disambiguation hasfocused mainly on finding the scope of coordinatestructures.There are several methods that use similari-ties between the heads of conjuncts.
Similari-ties are obtained from manually assigned seman-tic tags (Agarwal and Boggess, 1992), a the-saurus (Resnik, 1999) and a distributional the-saurus (Chantree et al, 2005).
Other approachesused cooccurrence statistics.
To determine the at-tachments of ambiguous coordinate noun phrases,Goldberg (1999) applied a cooccurrence-basedprobabilistic model, and Nakov and Hearst (2005)used web-based frequencies.
The performance ofthese methods ranges from 50% to 80%.Of the above approaches, Resnik (1999) andNakov and Hearst (2005) considered the statisticsof noun-noun modification.
For example, the co-ordinate structure ?
((mail and securities) fraud)?
isguided by the estimation that mail fraud is a salientcompound nominal phrase.
On the other hand, thecoordinate structure ?
(corn and (peanut butter))?
isled because corn butter is not a familiar concept.They did not use the selectional preferences of thepredicates that the conjuncts depend on.
There-fore, this idea is subsumed into ours.The previously described methods focused oncoordination disambiguation.
Some research hasbeen undertaken that integrated coordination dis-ambiguation into parsing.Several techniques have considered the charac-teristics of coordinate structures in a generative orreranking parser.
Dubey et al (2006) proposedan unlexicalized PCFG parser that modified PCFGprobabilities to condition the existence of syntacticparallelism.
Hogan (2007) improved a generativelexicalized parser by considering the symmetry be-tween words in each conjunct.
As for a rerankingparser, Charniak and Johnson (2005) incorporatedsome features of syntactic parallelism in coordi-nate structures into their MaxEnt reranking parser.Nilsson et al tried to transform the tree rep-resentation of a treebank into a more suitablerepresentation for data-driven dependency parsers(Nilsson et al, 2006; Nilsson et al, 2007).
Oneof their targets is the representation of coordinatestructures.
They succeeded in improving a deter-ministic parser, but failed for a globally optimizeddiscriminative parser.Kurohashi and Nagao proposed a Japanese pars-ing method that included coordinate structure de-tection (Kurohashi and Nagao, 1994).
Theirmethod first detects coordinate structures in a sen-tence, and then determines the dependency struc-ture of the sentence under the constraints of thedetected coordinate structures.
Their method cor-rectly analyzed 97 out of 150 Japanese sentences.Kawahara and Kurohashi (2007) integrated thismethod into a generative parsing model.
Shimboand Hara (2007) considered many features for co-ordination disambiguation and automatically opti-mized their weights, which were heuristically de-termined in Kurohashi and Nagao (1994), using adiscriminative learning model.A number of machine learning-based ap-proaches to Japanese parsing have been developed.Among them, the best parsers are the SVM-baseddependency analyzers (Kudo and Matsumoto,2002; Sassano, 2004).
In particular, Sassano addedsome features to improve his parser by enablingit to detect coordinate structures (Sassano, 2004).However, the added features did not contribute toimproving the parsing accuracy.
Tamura et al(2007) learned not only standard modifier-headrelations but also ancestor-descendant relations.With this treatment, their method can indirectlyimprove the handling of coordinate structures inlimited cases.3 Background3.1 Japanese GrammarLet us first briefly introduce Japanese grammar.The structure of a Japanese sentence can be de-scribed well by the dependency relation betweenbunsetsus.
A bunsetsu is a basic unit of depen-dency, consisting of one or more content words andthe following zero or more function words.
A bun-setsu corresponds to a base phrase in English andeojeol in Korean.
The Japanese language is head-final, that is, a bunsetsu depends on another bun-setsu to its right (but not necessarily the adjacentbunsetsu).For example, consider the following sentence1:(2) ane-tosister-CMIgakkou-nischool-ALLittawent(went to school with (my) sister)1In this paper, we use the following abbreviations:NOM (nominative), ACC (accusative), ABL (ablative),ALL (allative), CMI (comitative), CNJ (conjunction) andTM (topic marker).426This sentence consists of three bunsetsus.
The fi-nal bunsetsu, itta, is a predicate, and the other bun-setsus, ane-to and gakkou-ni, are its arguments.Their endings, to and ni, are postpositions thatfunction as case markers.3.2 TreebankTo evaluate our method, we use a web corpus thatis manually annotated using the criteria of the Ky-oto Text Corpus (Kurohashi and Nagao, 1998).The Kyoto Text Corpus is syntactically annotatedin dependency formalism, and consists of 40KJapanese newspaper sentences.
The web corpus,which is used in our evaluation, consists of 759sentences extracted from the web.Under the annotation criteria of the Kyoto TextCorpus, the last bunsetsu in a pre-conjunct dependson the last bunsetsu in a post-conjunct, as shown inthe dependency trees of Figure 1.4 Our Idea of Addressing CoordinationAmbiguitiesThe target of our approach is nominal coordinatestructures.
Consider, for example, the follow-ing sentence, which contains a nominal coordinatestructure.
(3) jinkou-nopopulation-GENzouka-toincrease-CNJtaiki-noair-GENosen-gapollution-NOMsokushin-saretastimulated(increase of population and pollution of airwere stimulated)In this sentence, the postposition to is a coordinateconjunction2.
In Japanese, a coordinate conjunc-tion is attached to a verb or noun, forming a bun-setsu, like case-marking postpositions.
We call abunsetsu that contains a coordinate conjunction co-ordination key bunsetsu.The coordinate structure in example (3) has fourpossible scopes as depicted in Figure 1.
In thisfigure, our parser generates the constituent wordsaccording to the arrows in the reverse direction.Note that the words that have ?1/2?
marks are gen-erated from multiple words, because they depend2Note that the postposition to can be used as a coordinateconjunction and also a comitative case marker as in exam-ple (2).
The detection of coordinate conjunctions is a task ofcoordination disambiguation as well as the identification ofcoordination scopes.
Both of these tasks are simultaneouslyhandled in our method.on a coordinate structure.
In this case, their gen-erative probabilities, which are described later, areaveraged.The scope patterns in Figure 1 can be written inEnglish as follows:a.
(population increase) and (air pollution)b. population (increase and (air pollution))c. ((population increase) and air) pollutiond.
population (increase and air) pollutionIn (a) and (b), two arguments, zouka (increase)and osen (pollution), are generated from the verbsokushin-sareta (stimulated), and are eligible forthe ga (NOM) words of the verb sokushin-sareta(stimulated).
However, (b) is not appropriate,because we cannot say the nominal compound?jinkou-no osen?
(pollution of population).
In (c)and (d), the heads of conjuncts, zouka (increase)and taiki (air), are generated from osen (pollu-tion).
These cases are also inappropriate, becausewe cannot say the nominal compound ?zouka-noosen?
(pollution of increase).
Accordingly, in thiscase, the correct scope, (a), is derived based on theselectional preferences of predicates and nouns.In this framework, we require selectional prefer-ences.
We use case frames for predicates (Kawa-hara and Kurohashi, 2006) and occurrences ofnoun-noun modifications for nouns.
Both of themare extracted from a large amount of raw text.5 Our Model of CoordinationDisambiguationThis section describes an integrated model of co-ordination disambiguation in a generative parsingframework.
First, we describe resources for selec-tional preferences, and then illustrate our model ofcoordination disambiguation.5.1 Resources for Selectional PreferencesAs the resources of selectional preferences tosupport coordinate structures, we use automati-cally constructed case frames and cooccurrencesof noun-noun modifications.5.1.1 Automatically Constructed CaseFramesWe employ automatically constructed caseframes (Kawahara and Kurohashi, 2006).
Thissection outlines the method for constructing thecase frames.427zouka-toincrease-CNJjinkou-nopopulation-GENzouka-toincrease-CNJtaiki-noair-GENosen-gapollution-NOMsokushin-saretastimulatedC(a) jinkou-nopopuluation-GENzouka-toincrease-CNJtaiki-noair-GENosen-gapollution-NOMsokushin-saretastimulated(b)jinkou-nopopulation-GENzouka-toincrease-CNJtaiki-noair-GENosen-gapollution-NOMsokushin-saretastimulatedC(c) jinkou-nopopulation-GENtaiki-noair-GENosen-gapollution-NOMsokushin-saretastimulatedC(d)C1/21/2Figure 1: Four possible coordination scopes for example (3).
Rounded rectangles represent conjuncts.The solid arrows represent dependency trees.
The dotted arrows represent the additional processes ofgeneration for coordinate structures.
Note that the arrows with coordinate relation (?C?
mark) do notparticipate in generation instead.Table 1: Acquired case frames of yaku.
Examplewords are expressed only in English due to spacelimitation.
The number following each word de-notes its frequency.CS examplesga I:18, person:15, craftsman:10, ?
?
?yaku (1)wo bread:2484, meat:1521, cake:1283, ?
?
?
(bake)de oven:1630, frying pan:1311, ?
?
?yaku (2) ga teacher:3, government:3, person:3, ?
?
?
(have wo fingers:2950difficulty) ni attack:18, action:15, son:15, ?
?
?ga maker:1, distributor:1yaku (3)wo data:178, file:107, copy:9, ?
?
?
(burn)ni R:1583, CD:664, CDR:3, ?
?
?.........A large corpus is automatically parsed, and caseframes are constructed from modifier-head exam-ples in the resulting parses.
The problems of auto-matic case frame construction are syntactic and se-mantic ambiguities.
That is to say, the parsing re-sults inevitably contain errors, and verb senses areintrinsically ambiguous.
To cope with these prob-lems, case frames are gradually constructed fromreliable modifier-head examples.First, modifier-head examples that have no syn-tactic ambiguity are extracted, and they are disam-biguated by a pair consisting of a verb and its clos-est case component.
Such pairs are explicitly ex-pressed on the surface of text, and are thought toplay an important role in sentence meanings.
Forinstance, examples are distinguished not by verbs(e.g., ?yaku?
(bake/broil/have difficulty)), but bypairs (e.g., ?pan-wo yaku?
(bake bread), ?niku-woyaku?
(broil meat), and ?te-wo yaku?
(have diffi-culty)).
Modifier-head examples are aggregated inthis way, and yield basic case frames.Thereafter, the basic case frames are clusteredto merge similar case frames.
For example, since?pan-wo yaku?
(bake bread) and ?niku-wo yaku?
(broil meat) are similar, they are clustered.
Thesimilarity is measured using a thesaurus (The Na-tional Institute for Japanese Language, 2004).Using this gradual procedure, we constructedcase frames from a web corpus (Kawahara andKurohashi, 2006).
The case frames were ob-tained from approximately 500M sentences ex-tracted from the web corpus.
They consisted of90,000 verbs, and the average number of caseframes for a verb was 34.3.In Table 1, some examples of the resulting caseframes of the verb yaku are listed.
In this table,?CS?
indicates a case slot.428ane-tosister-CNJotouto-wobrother-ACCyondainvitedC(b)ane-tosister-CMIotouto-wobrother-ACCyondainvited(a) towo wowoFigure 2: Dependency trees and generation pro-cesses for example (4).
This example sentence hastwo possible dependency structures according tothe interpretation of to: comitative in (a) and co-ordinate conjunction in (b).5.1.2 Cooccurrences of Noun-nounModificationsAdnominal nouns have selectional preferencesto nouns, and thus this characteristic is useful forcoordination disambiguation (Resnik, 1999).
Wecollect dependency relations between nouns fromautomatic parses of the web corpus.
As a re-sult, 10.7M unique dependency relations were ob-tained.5.2 Our ModelWe employ a probabilistic generative dependencyparser (Kawahara and Kurohashi, 2007) as a basemodel.
This base model measures similaritiesbetween conjuncts in the same way as (Kuro-hashi and Nagao, 1994), and calculates probabil-ities of generating these similarities.
Our proposedmodel, however, does not do both of them.
Ourmodel purely depends on selectional preferencesprovided by automatically acquired lexical knowl-edge.Our model gives probabilities to all the possibledependency structures for an input sentence, andselects the structure that has the highest probabil-ity.
For example, consider the following sentence:(4) ane-tosister-CNJotouto-wobrother-ACCyondainvited(invited (my) sister and brother)For this sentence, our model assesses the two de-pendency structures (a) and (b) in Figure 2.
In ourmodel, both of the pre-conjunct and post-conjunctare generated from the predicate.
That is, in (b),both ane (sister) and otouto (brother) with wo(ACC) are generated from yonda (invited).
Toidentify the correct structure, (b), it is essentialthat both ane (sister) and otouto (brother) are el-igible for the accusative words of yonda (invited).Therefore, selectional preferences play an impor-tant role in coordination disambiguation.
On theother hand, in (a), ane (sister) with to (CMI) isgenerated from yonda (invited), and also otouto(brother) with wo (ACC) is generated from yonda.However, yonda is not likely to have the to caseslot, so the probability of (a) is lower than that of(b).
Our model can finally select the correct struc-ture, (b), which has the highest probability.
Thiskind of assessment is also performed to resolvethe scope ambiguities of coordinate structures asshown in Figure 1.This model gives a probability to each possibledependency structure, T , and case structure, L, ofthe input sentence, S, and outputs the dependencyand case structure that have the highest probability.That is to say, the model selects the dependencystructure, Tbest, and the case structure, Lbest, thatmaximize the probability, P (T,L|S):(Tbest, Lbest) = argmax(T,L)P (T,L|S)= argmax(T,L)P (T,L, S)P (S)= argmax(T,L)P (T,L, S) (1)The last equation is derived because P (S) is con-stant.The model considers a clause as a generationunit and generates the input sentence from the endof the sentence in turn.
The probability P (T,L, S)is defined as the product of probabilities for gener-ating clause Cias follows:P (T,L, S) =?Ci?SP (Ci, relihi|Chi) (2)Chiis Ci?s modifying clause, and relihiis the de-pendency relation between Ciand Chi.
The mainclause, Cn, at the end of a sentence does not havea modifying head, but a virtual clause Chn= EOS(End Of Sentence) is added.
Dependency relationrelihiis classified into two types, C (coordinate)and D (normal dependency).Clause Ciis decomposed into its clause type,fi, (including the predicate?s inflection and func-tion words) and its remaining content part Ci?.Clause Chiis also decomposed into its contentpart, Chi?, and its clause type, fhi.P (Ci, relihi|Chi) = P (Ci?, fi, relihi|Chi?, fhi)?
P (Ci?, relihi|fi, Chi?)?
P (fi|fhi)?
P (Ci?|relihi, fi, Chi?)?
P (relihi|fi)?
P (fi|fhi) (3)429Equation (3) is derived using appropriate approx-imations described in Kawahara and Kurohashi(2007).We call P (Ci?|relihi, fi, Chi?)
generative prob-ability of a content part, and P (relihi|fi) gener-ative probability of a dependency relation.
Thefollowing two subsections describe these probabil-ities.5.2.1 Generative Probability of DependencyRelationThe most important feature to determinewhether two clauses are coordinate is a coordina-tion key.
Therefore, we consider a coordinationkey, ki, as clause type fi.
The generative prob-ability of a dependency relation, P (relihi|fi), isdefined as follows:P (relihi|fi) = P (relihi|ki) (4)We classified coordination keys into 52 classes ac-cording to the classification described in (Kuro-hashi and Nagao, 1994).
If type fidoes notcontain a coordination key, the relation is alwaysD (normal dependency), that is, P (relihi|fi) =P (D|?)
= 1.The generative probability of a dependency re-lation was estimated from the Kyoto Text Corpususing maximum likelihood.5.2.2 Generative Probability of Content PartThe generative probability of a content partchanges according to the class of a content part,Ci?.
We classify Ci?into two classes: predicateclause and nominal phrase.If Ci?is a predicate clause, Ci?represents a casestructure.
We consider that a case structure con-sists of a predicate, vi, a case frame, CFl, anda case assignment, CAk.
Case assignment CAkrepresents correspondences between the input casecomponents and the case slots shown in Figure 3.Thus, the generative probability of a content partis decomposed as follows:Pv(Ci?|relihi, fi, Chi?
)= P (vi, CFl, CAk|relihi, fi, Chi?)?
P (vi|relihi, fi, whi)?
P (CFl|vi)?
P (CAk|CFl, fi) (5)These generative probabilities are estimated fromcase frames themselves and parsing results of alarge web corpus.bentou-watabete(lunchbox)(eat)?lunchbox, bread, ?woman, student, ?gataberu1 (eat)Case Frame CFlCaseAssignmentCAk(no correspondence)Dependency Structure of SFigure 3: Example of case assignment.If Ci?is a nominal phrase and consists of a nounni, we consider the following probability insteadof equation (5):Pn(Ci?|relihi, fi, Chi?)
?
P (ni|relihi, fi, whi)This is because a noun does not have a case frameor any case components in the current framework.Since we do not use cooccurrences of coordinatephrases as used in the base model, relihiis alwaysD (normal dependency).
This probability is esti-mated from the cooccurrences of noun-noun mod-ifications using maximum likelihood.6 ExperimentsWe evaluated the dependency structures that wereoutput by our model.
The case frames used in thispaper were automatically constructed from 500MJapanese sentences obtained from the web.In this work, the parameters related to unlexicaltypes were calculated from the Kyoto Text Corpus,which is a small tagged corpus of newspaper ar-ticles, and lexical parameters were obtained froma huge web corpus.
To evaluate the effectivenessof our model, our experiments were conducted us-ing web sentences.
As the test corpus, we used759 web sentences3, which are described in sec-tion 3.2.
We also used the Kyoto Text Corpus asa development corpus to optimize the smoothingparameters.
The system input was automaticallytagged using the JUMAN morphological analyzer4.We used two baseline systems for compara-tive purposes: a rule-based dependency parser(Kurohashi and Nagao, 1994) and the probabilisticgenerative model of dependency, coordinate andcase structure analysis (Kawahara and Kurohashi,2007)5.6.1 Evaluation of Dependency StructuresWe evaluated the dependency structures that wereanalyzed by the proposed model.
Evaluating the3The test set was not used to construct case frames or es-timate probabilities.4http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html5http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html430Table 2: Experimental results of dependency structures.
?all?
represents the accuracy of all the depen-dencies, and ?coordination key?
represents the accuracy of only the coordination key bunsetsus.rule-coord-w/sim prob-coord-w/sim prob-coord-wo/simall 3,821/4,389 (87.1%) 3,852/4,389 (87.8%) 3,877/4,389 (88.3%)coordination key 878/1,106 (79.4%) 881/1,106 (79.7%) 897/1,106 (81.1%)scope ambiguity of coordinate structures is sub-sumed within this dependency evaluation.
The de-pendency structures obtained were evaluated withregard to dependency accuracy ?
the proportionof correct dependencies out of all dependenciesexcept for the last one in the sentence end6.
Ta-ble 2 lists the dependency accuracy.
In this table,?rule-coord-w/sim?
represents a rule-based depen-dency parser; ?prob-coord-w/sim?
represents theprobabilistic parser of dependency, coordinate andcase structure (Kawahara and Kurohashi, 2007);and ?prob-coord-wo/sim?
represents our proposedmodel.
?all?
represents the overall accuracy, and?coordination key?
represents the accuracy of onlythe coordination key bunsetsus.
The proposedmodel, ?prob-coord-wo/sim?, significantly outper-formed both ?rule-coord-w/sim?
and ?prob-coord-w/sim?
(McNemar?s test; p < 0.05) for ?all?.Figure 4 shows some analyses that are cor-rectly analyzed by the proposed method.
Forexample, in sentence (1), our model can rec-ognize the correct coordinate structure that con-joins ?densya-no hassyaaizu?
(departure signalsof trains) and ?keitaidenwa-no tyakushinon?
(ringtones of cell phones).
This is because the caseframe of ?ongaku-ni naru?
(become music) islikely to generate ?hassyaaizu?
(departure signal)and ?tyakushinon?
(ring tone).To compare our results with a state-of-the-artdiscriminative dependency parser, we input thesame test corpus into an SVM-based Japanesedependency parser, CaboCha7(Kudo and Mat-sumoto, 2002).
Its dependency accuracy was86.7% (3,807/4,389), which is close to that of?rule-coord-w/sim?.
This low accuracy is at-tributed to the lack of the consideration of coor-dinate structures.
Though dependency structuresare closely related to coordinate structures, theCaboCha parser failed to incorporate coordinationfeatures.
Another cause of the low accuracy isthe out-of-domain training corpus.
That is, theparser is trained on a newspaper corpus, whereas6Since Japanese is head-final, the second to last bunsetsuunambiguously depends on the last bunsetsu, and the last bun-setsu has no dependency.7http://chasen.org/?taku/software/cabocha/the test corpus is obtained from the web, becauseof the non-availability of a tagged web corpus thatis large enough to train a supervised parser.6.2 DiscussionWe presented a method for coordination dis-ambiguation without using similarities, and thismethod achieved better performance than theconventional approaches based on similarities.Though we do not use similarities, we implicitlyconsider similarities between conjuncts.
This isbecause the heads of pre- and post-conjuncts sharea case marker and a predicate, and thus they are es-sentially similar.
Our idea is related to the notionof distributional similarity.
Chantree et al (2005)applied the distributional similarity proposed byLin (1998) to coordination disambiguation.
Linextracted from a corpus dependency triples of twowords and the grammatical relationship betweenthem, and considered that similar words are likelyto have similar dependency relations.
The differ-ence between Chantree et al (2005) and ours isthat their method does not use the information ofverbs in the sentence under consideration, but useonly the cooccurrence information extracted froma corpus.On the other hand, the disadvantage of ourmodel is that it cannot consider the parallelism ofconjuncts, which still seems to exist in especiallystrong coordinate structures.
Handling of such par-allelism is an open question of our model.The generation process adopted in this workis similar to the design of dependency structuredescribed in Hudson (1990), which lets the con-juncts have a dependency relation to the predi-cate.
Nilsson et al (2006) mentioned this notion,but did not consider this idea in their experimentsof tree transformations for data-driven dependencyparsers.
In addition, it is not necessary for ourmethod to transform dependency trees in pre- andpost-processes, because we just changed the pro-cess of generation in the generative parser.7 ConclusionIn this paper, we first came up with a hypoth-esis that coordinate structures are supported by431?
?
(1) densya-no hassyaaizu-ya, keitaidenwa-no tyakushinon-madega ongaku-ni naru-hodoni, ...train-GEN departure signal cell phone-GEN ring tone-also music-ACC become(departure signals of trains and ring tones of cell phones become music, ...)?
?
(2) nabe-ni dashijiru 3 kappu-to, nokori-no syouyu, mirin, sake-wo irete, ...pot-DAT stock three cups-and remainder-GEN soy mirin sake-ACC pour(pour three cups of stock and remaining soy, mirin and sake to the pot, ...)Figure 4: Examples of correct analyses.
The dotted lines represent the analysis by the baseline, ?prob-coord-w/sim?, and the solid lines represent the analysis by the proposed method, ?prob-coord-wo/sim?.surrounding dependency relations.
Based on thishypothesis, we built an integrated probabilisticmodel for coordination disambiguation and depen-dency/case structure analysis.
This model doesnot make use of similarities to analyze coordinatestructures, but takes advantage of selectional pref-erences from a huge raw corpus and large-scalecase frames.
The experimental results indicatethe effectiveness of our model, and thus supportour hypothesis.
Our future work involves incorpo-rating ellipsis resolution to develop an integratedmodel for syntactic, case, and ellipsis analysis.ReferencesAgarwal, Rajeev and Lois Boggess.
1992.
A simple but use-ful approach to conjunct identification.
In Proceedings ofACL1992, pages 15?21.Chantree, Francis, Adam Kilgarriff, Anne de Roeck, and Al-istair Wills.
2005.
Disambiguating coordinations us-ing word distribution information.
In Proceedings ofRANLP2005.Charniak, Eugene and Mark Johnson.
2005.
Coarse-to-finen-best parsing and maxent discriminative reranking.
InProceedings of ACL2005, pages 173?180.Dubey, Amit, Frank Keller, and Patrick Sturt.
2006.
Inte-grating syntactic priming into an incremental probabilisticparser, with an application to psycholinguistic modeling.In Proceedings of COLING-ACL2006, pages 417?424.Goldberg, Miriam.
1999.
An unsupervised model for statis-tically determining coordinate phrase attachment.
In Pro-ceedings of ACL1999, pages 610?614.Hogan, Deirdre.
2007.
Coordinate noun phrase disambigua-tion in a generative parsing model.
In Proceedings ofACL2007, pages 680?687.Hudson, Richard.
1990.
English Word Grammar.
Blackwell.Kawahara, Daisuke and Sadao Kurohashi.
2006.
Case framecompilation from the web using high-performance com-puting.
In Proceedings of LREC2006.Kawahara, Daisuke and Sadao Kurohashi.
2007.
Proba-bilistic coordination disambiguation in a fully-lexicalizedJapanese parser.
In Proceedings of EMNLP-CoNLL2007,pages 306?314.Kudo, Taku and Yuji Matsumoto.
2002.
Japanese depen-dency analysis using cascaded chunking.
In Proceedingsof CoNLL2002, pages 29?35.Kurohashi, Sadao and Makoto Nagao.
1994.
A syntacticanalysis method of long Japanese sentences based on thedetection of conjunctive structures.
Computational Lin-guistics, 20(4):507?534.Kurohashi, Sadao and Makoto Nagao.
1998.
Building aJapanese parsed corpus while improving the parsing sys-tem.
In Proceedings of LREC1998, pages 719?724.Lin, Dekang.
1998.
Automatic retrieval and clustering ofsimilar words.
In Proceedings of COLING-ACL98, pages768?774.Nakov, Preslav and Marti Hearst.
2005.
Using the web asan implicit training set: Application to structural ambigu-ity resolution.
In Proceedings of HLT-EMNLP2005, pages835?842.Nilsson, Jens, Joakim Nivre, and Johan Hall.
2006.
Graphtransformations in data-driven dependency parsing.
InProceedings of COLING-ACL2006, pages 257?264.Nilsson, Jens, Joakim Nivre, and Johan Hall.
2007.
General-izing tree transformations for inductive dependency pars-ing.
In Proceedings of ACL2007, pages 968?975.Resnik, Philip.
1999.
Semantic similarity in a taxonomy: Aninformation-based measure and its application to problemsof ambiguity in natural language.
Journal of Artificial In-telligence Research, 11:95?130.Sassano, Manabu.
2004.
Linear-time dependency analysisfor Japanese.
In Proceedings of COLING2004, pages 8?14.Shimbo, Masashi and Kazuo Hara.
2007.
A discriminativelearning model for coordinate conjunctions.
In Proceed-ings of EMNLP-CoNLL2007, pages 610?619.Tamura, Akihiro, Hiroya Takamura, and Manabu Oku-mura.
2007.
Japanese dependency analysis using theancestor-descendant relation.
In Proceedings of EMNLP-CoNLL2007, pages 600?609.The National Institute for Japanese Language.
2004.
Bun-ruigoihyo.
Dainippon Tosho, (In Japanese).432
