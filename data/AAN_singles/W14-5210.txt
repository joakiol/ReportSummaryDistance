Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 83?92,Dublin, Ireland, August 23rd 2014.Integrated Tools for Query-driven Developmentof Light-weight Ontologies and Information Extraction ComponentsMartin Toepfer1Georg Fette11Department of Computer Science VIUniversity of W?urzburg, Am HublandW?urzburg, Germanyfirst.last@uni-wuerzburg.dePhilip-Daniel Beck1Peter Kluegl12Frank Puppe12Comprehensive Heart Failure CenterUniversity of W?urzburg, Straubm?uhlweg 2aW?urzburg, Germanypkluegl@uni-wuerzburg.deAbstractThis paper reports on a user-friendly terminology and information extraction development envi-ronment that integrates into existing infrastructure for natural language processing and aims toclose a gap in the UIMA community.
The tool supports domain experts in data-driven and manualterminology refinement and refactoring.
It can propose new concepts and simple relations andincludes an information extraction algorithm that considers the context of terms for disambigua-tion.
With its tight integration of easy-to-use and technical tools for component developmentand resource management, the system is especially designed to shorten times necessary for do-main adaptation of such text processing components.
Search support provided by the tool fostersthis aspect and is helpful for building natural language processing modules in general.
Special-ized queries are included to speed up several tasks, for example, the detection of new terms andconcepts, or simple quality estimation without gold standard documents.
The development en-vironment is modular and extensible by using Eclipse and the Apache UIMA framework.
Thispaper describes the system?s architecture and features with a focus on search support.
Notably,this paper proposes a generic middleware component for queries in a UIMA based workbench.1 IntroductionAccording to general understanding, a specification of relevant concepts, relations, and their types isrequired to build Information Extraction (IE) components.
Named Entity Recognition (NER) systemsin the newspaper domain, for example, try to detect concepts like persons, organizations, or locations.Regarding clinical text, it has been shown that lookup-based approaches (Tanenblatt et al., 2010) canachieve high precision and recall if a terminology exists that maps terms to their meanings.
However,this approach is not directly applicable if such resources are not available for a certain language orsubdomain, or if the domain and its terminology are changing.
Unsupervised methods can help to findand to group the relevant terms of a domain, for example, into concept hierarchies (Cimiano et al., 2005).Nevertheless, automatically derived terminologies are not perfect, and there are many applications thatrequire high precision knowledge resources and representations, for instance, to build up a clinical datawarehouse.
In this case, automatically generated ontologies have to be refined by domain experts likeclinicians, which imposes special requirements on the usability of the tools.There have been several efforts to support ontology extraction and refinement (Cimiano and V?olker,2005), predominantly with text processing based on the GATE (Cunningham et al., 2011) infrastructure.In the Apache UIMA (Ferrucci and Lally, 2004) community1, several tools exist that ease system de-velopment and management.
Much work has, for instance, been spent on pipeline management, ruledevelopment (Kluegl et al., 2009), or evaluation.
Terminology and ontology development support, how-ever, have not gained as much attention in the context of this framework by now.
This is surprising sincethe integration of tools for terminology development and especially terminology generation and infor-mation extraction into existing infrastructure for text processing is promising.
Actually, the approachThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1http://uima.apache.org/83taken in this paper regards terminology creation and information extraction as two related tasks.
Theproposed system aims to assist users in the development of components that extract information withlexical resources gathered during the specification of the concepts of the domain.This paper introduces Edtgar: a user-friendly integrated terminology development environment.
Itprovides many features that help domain experts to construct and refine light-weight ontologies drivenby flexible corpus queries.
In this work, ?light-weight ontology?
means that we focus on simple rela-tions, as well as restricted inference.
We call the knowledge representation ?terminology?
since the toolaims to manage lexical information for information extraction.
The major components of the system area terminology editor, a plug-in concept for terminology extraction and an information extraction API, aswell as support for corpus queries.
Special views show extraction statistics, provide semi-automatic an-notation of gold standard documents, as well as evaluation and deployment support.
The tool comprisesan implementation for terminology induction and an information extraction algorithm that considers con-texts.
In order to keep the system modular and extensible, it integrates into Eclipse2and uses the ApacheUIMA framework.
Apache UIMA provides a well-established framework for text processing, hence, avariety of natural language processing components can easily be integrated, for example, by accessingcomponent repositories like DKPro Core3.
At the technical level, the default processing componentsof the proposed system use a combination of Apache UIMA Ruta4scripts and custom analysis enginesimplemented in Java.
As a consequence, the tight integration of the terminology development tools intoApache UIMA?s Eclipse Tools and Apache UIMA Ruta?s rule engine and workbench allows technicalengineers to use several existing features.The structure of the paper is as follows: Section 2 gives a brief overview of ontology developmentsystems and tools.
Section 3 and 4 introduce the tool and its support for corpus queries.
Results of a casestudy are given in Section 5.
Finally, we conclude in Section 6.2 Related WorkMost of all, our work relates to environments and frameworks for ontology learning, editing, and refine-ment.
We first give a brief overview of such systems with a focus on open source and research relatedsystems5.
Afterwards, we discuss some popular query tools that come into question for integration intonatural language processing environments.OntoLT (Buitelaar et al., 2004) is a plugin for the ontology editor Prot?eg?e.
It aims to derive ontologicalconcepts and relations from plain text by defining XPath expressions over linguistic structures, e.g.,subject object relations under constraints like specific lemmas.
Manual annotation of ontology conceptmentions can be performed with the Prot?eg?e plugin Knowtator6.Very similar to our work is the NeOn toolkit7.
It is an Eclipse based ontology development environ-ment.
There are many plugins available that extend NeOn toolkit?s functionality.
For instance, the GATEWebservice plugin8and its TermRaider component automatically generate ontological information.
Oneof the plugins for NeOn is the work by Cimiano and V?olker, who proposed Text2Onto9(Cimiano andV?olker, 2005), which is a framework that allows to apply ontology learning and change discovery al-gorithms.
Its central data structure is called probabilistic ontology model (POM).
It is not providingstatistics but stores values that represent concept or relation extraction certainty.
Text2Onto?s naturallanguage processing is based on GATE and the rule engine JAPE.
Similar to our work, Text2Onto aimsto provide an easy-to-use user interface.Most of the tools mentioned above either use GATE or proprietary data formats for linguistic represen-tations.
We believe that there is a need for a tool based on UIMA.
Our tool aims to provide an integrated2http://eclipse.org/3https://www.ukp.tu-darmstadt.de/software/dkpro-core/?no_cache=14https://uima.apache.org/ruta.html5There are related commercial solutions, for instance, http://www.poolparty.biz/6http://knowtator.sourceforge.net/7http://neon-toolkit.org8http://neon-toolkit.org/wiki/Gate_Webservice9https://code.google.com/p/text2onto/84development environment that includes terminology development and allows to use already availablefeatures provided by Eclipse plugins related to UIMA.
Thereby, we ease development, introspection,debugging, and testing of all kinds of UIMA annotation engines involved in system and terminologydevelopment.
Hence, the components that extract and refine terminologies based on natural languageprocessing can easily be adapted, for example, segmentation rules written in UIMA Ruta?s workbench.The same argumentation applies to information extraction components or linguistic preprocessors likechunkers that are involved in both tasks.
Contrary to most other tools, we do not consider complexrelations.
Instead, we focus on easy-to-understand inference and relations.In the UIMA community, Fiorelli et al.
proposed the computer-aided ontology development architec-ture (CODA) (Fiorelli et al., 2010) that consists of the tasks: ontology learning, population of ontologies,and linguistic enrichment.
Fiorelli et al.
describe how an integrated ontology development system basedon UIMA could look like, however, their system UIMAST concentrates on ontology population aspects.By contrast, this paper also considers construction tasks and describes tooling for editing and refininglight-weight ontologies either manually or based on document collections.Table 1 compares qualitative features of different query tools which are described below.
Since seman-tic search is an active field of research, we can only give a brief overview focused on popular tools forUIMA and GATE.
We put special emphasis on tools that can be easily integrated into an Eclipse-basedenvironment.Tool Framework Index Syntax IDE IntegrationRuta Query View UIMA noaexpert EclipseLucas / Lucene UIMA yes user-friendly -GATE M?
?mir GATE yes medium -GATE Annic GATE yes medium GATE Developerbauses only UIMA?s annotation indexbhttps://gate.ac.uk/family/developer.htmlTable 1: Qualitative comparison of query tools.The UIMA Ruta workbench contains a query view that enables to search in document collections witharbitrary queries formulated as rules.
For instance, Segment{-CONTAINS(CONCEPT)} matchessegment annotations that do not contain any concept annotation.
With regard to usability for domainexperts, this tool has a drawback: users have to be familiar with Ruta?s syntax which is in general toocomplex for terminology engineers.
They should not have to learn a programming language to pose aquery on a corpus.
Another drawback of the query view is that it has no option to group search results,e.g.
by their covered text.
The Ruta query view is not designed for fast results on very large corpora.It iteratively processes all documents of a folder that match a user-defined filename filter, thus, queriesdo not run as fast as with index structures for the whole corpus.
A combination of rule-based queryformulation and the new middleware (Section 4) would be useful for the community.Apache Lucene10is a popular search engine.
Existing mapping tools like the UIMA Lucene in-dexer Lucas11show how UIMA pipeline results can be indexed with Lucene.
This solution is attractivesince Lucene?s query syntax allows complex query patterns but still remains easy-to-use.
For example,valve -pulmo*searches for documents that contain the term ?valve?
but do not contain terms be-ginning with ?pulmo?.
Inexperienced users have a higher chance to understand the syntax because it ismore similar to the one of web search engines.
Lucene itself does not provide a user interface but thereare tools like Apache Solr, or Apache Stanbol which is a semantic search project based on Apache Solr.Our requirements are similar in certain aspects to Gate m?
?mir12, which is an indexing and retrievaltool for Gate.
It allows to find text passages that match certain annotation or text patterns.
For example,10http://lucene.apache.org/core/11http://svn.apache.org/repos/asf/uima/addons/trunk/Lucas/12http://gate.ac.uk/mimir/85transistor IN {Abstract}13searches for abstracts regarding transistors.
In combination withGATE?s rule engine Jape14, similar functionality can be achieved for terminology development.
A querytool for document processing and retrieval based on Apache Lucene and GATE is Annic15(ANNotationsIn Context) (Aswani et al., 2005).
It provides a viewer for nested annotation structures and features.3 Edtgar: Terminology Development EnvironmentThe system has been developed as part of a medical information extraction project that populates thedata warehouse of a German hospital.
Documents in clinical domains differ from the kind of text that iswidely used for ontology development, hence common approaches to learn ontologies, for example, withlexical patterns, have relatively low entity acceptance rates on clinical documents (Liu et al., 2011).
As aconsequence, ontology learning and enrichment methods and information extraction components have tobe adapted to the domain.
We address this adaptation step integrating new editors, views and applicationlogic into existing tooling for working with UIMA documents.
Some features of our system are espe-cially useful for processing clinical text but they also work for similar domains, such as advertisements,product descriptions, or semi-structured product reviews.In order to assist users during development, we qualitatively analyzed workflows in a project withclinical reports.
As a result, we identified the following steps:1.
Linguistic preprocessing: A technical engineer chooses a component for preprocessing steps liketokenization, sentence detection, part-of-speech tagging, chunking, or parsing.
In our projects, arule engineer typically adapts general rules to fit to a special domain.
He modifies tokenizationrules and lists of abbreviations, and specifies segmentation rules that detect relevant parts of thedocument, annotate sections, subsections, sentences, and segments.2.
Initial terminology generation: an algorithm automatically derives a terminology based on a corpusof plain text documents.3.
Terminology refinement: a domain expert changes the terminology until it meets subjective or ob-jective quality criteria:(a) Automatically extract information according to the current state of the terminology.
(b) Inspect extraction results.
(c) Refine terminology: edit/add new concepts, add/update variants of existing concepts.4.
Annotation of gold standard documents.
Evaluation of the information extraction component andthe coverage of the terminology.Further analysis showed different aspects of improvement that conform with common expectations:1.
Search: terminology engineers frequently need to pose different kinds of search patterns on docu-ment collections.
Some of them are related to quality estimation without a gold standard.2.
Redundancy: modeling concepts independently of each other causes false negatives.
Some conceptshave highly redundant aspects that should be managed in a central way.In the following, we first sketch the terminology model and then briefly report on the terminologyinduction, validation, and information extraction components.
In this work, we put special emphasis onanalysing search tools that can be used in a UIMA-based workbench.
Our approach to pose queries inthe workbench is discussed in Section 4.13from: http://services.gate.ac.uk/mimir/query-session-examples.pdf14http://gate.ac.uk/sale/tao/splitch8.html15http://gate.ac.uk/sale/tao/splitch9.html86Figure 1: Edtgar T-Box Perspective: (A) Toolbar; (B) Project Structure; (C) Terminology Editor with(C1) Concept Tree, (C2) Variants; (D) Information Extraction Statistics; (E) Search View (Lucene); (F)Search Results (Filesystem: TreeViewer mode)3.1 Terminology ModelIn order to describe the information contained in documents of a domain, we follow an attribute-valuemodel extended with concepts of type entity (object) that disambiguate different meanings of attributes.The central aspects of our knowledge representation are given by a tupleO = (T,C,D, V,RT?C, RT?T, RC?V, RV?D)where T , C, D, V contain templates, concepts, dictionaries, and variants, respectively.
The relationRT?Cdefines which concepts use certain templates, RT?Tmodels inter-template references.
Conceptsare the main elements of the terminology.
There are several types of concepts such as objects, attributes,and values.
Each concept is expressed by lexical variants (relation RC?V) which can be grouped bydictionaries (relation RV?D).
Variants can be either simple strings or regular expressions.
Attributesthat belong to the same kind of semantic category typically share the same sets of values.
Ontologiesmodel this kind of relation between concepts typically by inheritance (subclass-of) or instantiation.
In ourterminology model, users can use templates to group concepts and even templates into semantic classesthat share all values that are part of the template.
As a consequence, the template mechanism avoidsredundancy and errors in the terminology.
Templates can also be used just to tag concepts.
To allowusers to store domain specific information, concepts can have arbitrary properties (key-value-pairs).
Allinformation of the model is de-/serializable as a single easy-to-read XML file.3.2 Terminology InductionDeveloping a terminology from scratch is costly, but some domains allow that a considerable amountof attributes and values can be automatically found.
Algorithms that induce terminologies and relationscan be easily plugged into the workbench through certain APIs.
Per default, a simple rule-based ap-proach based on part-of-speech tags is used.
It basically finds different kinds of patterns for attributes(nouns) and values (adjectives).
The algorithm uses the lemmas of lexical items to group concepts and87Figure 2: Ambiguous attributes: the attribute insufficiency (German: Insuffizienz) is both valid for theentities pulmonary valve (German: Pulmonalklappe) and tricuspid valve (German: Trikuspidalklappe).Insufficiency itself can either be negated (German: keine) or minor (German: gering).
Pulmonary valveand tricuspid valve are both regular in this example (German: Klappe zart).creates different variants of the concepts respectively.
Probabilities that tell users how certain a conceptextraction is can be optionally shown if they are provided by the algorithm.3.3 Terminology ValidationTerminologies can become quite large, for example, in medical domains, which makes it difficult tomanage them manually.
For instance, it is important to avoid redundancy because keeping redundantconcepts or variants in sync gets difficult.
We provide different types of validators that check certainaspects of improvement and show errors and warnings.
For example, missing dictionary referencescause insufficient synonym lists and false negative extractions.
We have a validator that creates warningsif a dictionary should be referenced instead of using only some part of this dictionary in a certain concept.There are several other validators, for instance, to detect missing template references.
New validatorscan easily be integrated into the framework.3.4 Information ExtractionSimilar to its terminology induction module, our system has a plug-in mechanism for information ex-traction algorithms.
By default, it contains an information extraction algorithm which allows for context-sensitive disambiguation of terms with multiple meanings.
It can, for example, resolve the correct inter-pretation for ambiguous terms like ?Klappe zart?
or ?Insuffizienz?
as shown in Figure 2.At first, the terminology is transformed into appropriate data structures for inference.
The processingpipeline begins with finding lexemes by matching the text against regular expressions and simple strings.The next annotator segments the document into hierarchically ordered parts such as sections, subsec-tions, sentences, segments, and tokens.
This component is implemented with Ruta rules which enablesrule engineers to adapt this stage to different document types and styles easily.
The following stage isimplemented as a Java analysis engine.
At first, it finds and annotates objects, i.e., entities that are closeto the root of the terminology and belong to the object type in the terminology.
These concepts are typ-ically expressed by unique variants and should not need any disambiguation.
Afterwards, the algorithmtries to assign unresolved attribute entities to objects, or directly annotates special entities.
Finally, thealgorithm performs light-weight inference: first, value extractions are removed when the correspondingattribute has been negated.
Second, presence annotations are added if an attribute entity requires a statusvalue and is not negated in the sentence.3.5 Knowledge-driven Evaluation and Status ReportsSimilar to quality assurance or quality assessment in factories, users can specify assertions for text pro-cessing tasks where system behavior is expected to conform to these assertions (Wittek et al., 2013).Such expectations allow to compute quality estimates even without annotated documents.
To this end,we provide special annotation types for these cases that can be categorized to distinguish different tasksor types of misclassifications.
By now, knowledge-driven evaluation is realized by the contributed searchcommands (see Section 4).
They allow to find, count, and group constraint violations which helps to es-timate the quality of the text processing component and to understand the errors it makes.
For example,the user can expect that all nouns should either be assigned to a concept annotation or listed in a blacklist.Elaboration of this aspect of the tool is planned for future releases.883.6 Edtgar Workbench OverviewEdtgar?s graphical user interface (see Figure 1) provides two perspectives and several views to assist theuser.
The heart of the tool is the terminology editor that allows to create or modify concepts (attributesand values, etc.
), manage variants and dictionaries.
If the main terminology of a project is opened,users can set the active corpus for the project, or trigger several actions.
They can start terminologyinduction/enrichment, information extraction, or run different types of specialized or general searcheson the active corpus, for example, by pressing the corresponding buttons in the toolbar.
The tool alsoprovides several other features that we do not discuss here, e.g., support for semi-automatic gold standardannotation, evaluation, documentation, and much more.3.7 Typesystem HandlingInspecting the results of processed documents is a visual process.
Representing each concept type by adistinct annotation type has a technical advantage because occurrences of a certain type of concept canbe highlighted in the UIMA CAS editor with a different color.
During terminology induction, however,the terminology and the corresponding annotation types do not exist in the typesystem of the processeddocument.
As a result, the presented framework uses a hybrid concept representation.
Engines canuse generic annotation types with an integer id feature to create proposal annotations for terminologygeneration and enrichment.
These annotations are subsequently mapped to type based representationswhen the terminology and its typesystem have been completely defined.
As a natural side-effect ofterminology development, identifiers of concepts may change, for instance, if concepts are rejected ormerged.
The terminology editor keeps identifiers stable as long as possible since both representationschemes have problems with invalid IDs.
An advantage of the ID feature based approach is that it isable to retain invalid references whereas a type-based approach looses information when documents areloaded leniently.Besides concept annotations, the system provides framework types for different purposes.
For in-stance, IgnoredRegion, UnhandledSegment, or Sealed.
They allow to configure irrelevant text detectionfor each subdomain, enable users to find new terms, or that have been manually inspected and containgold standard annotations, respectively.4 Search Tools for the WorkbenchAn important aspect of terminology development is search support.
It should assist users with predefinedqueries, e.g., to find new concepts, synonyms of concepts, or different meanings of concepts.
Technicalusers, however, must be able to perform searches with arbitrary complex patterns.
Several tasks can beregarded as corpus queries, for instance, finding measurement units or abbreviations.In order to support domain experts, we identified three main types of queries that fit to their workflows.First of all, Unhandled Segments Search is a kind of false negative predictor (cf.
Section 3.5).
It listsall segments in the corpus that have not yet been covered either by terminological concepts or excludepatterns.
This is in particular useful in clinical domains where nearly every noun phrase contains relevantinformation.
It can, however, easily be adapted to other domains.
Second, querying specific types ofextracted information is necessary, for instance, to inspect extraction results and to judge their quality.It allows to create partial gold standards and it helps to decide if attributes have ambiguous meaningsdependent on their context.
Third, constrained search supports the terminology engineer when editingconcepts or creating new entries: users often search for attributes to find their value candidates and needto see documents containing synonyms of the attribute but leaving known value terms out.
Finally, ifdomain experts are not familiar with regular expressions, they benefit from fast lexical search to testtheir patterns.Query Tools in EdtgarMost of the queries mentioned above can be implemented as searches for certain annotation types (UIMAType Search).
For instance, segments that do not contain any extracted information (Unhandled SegmentsSearch) can be annotated with a special type, and concept mentions can be annotated with types based on89(a) Search Page for UIMA annotation types: results can begrouped by a certain feature or their covered text.
(b) Search Results Page (TableViewer mode): it shows distinctunhandled segments ordered by frequencyFigure 3: Generic Search Componentsconcept identifiers, e.g., mietas.concept.C24 for the concept with the ID 24.
The system providestwo types of query commands that support annotation type searches.The first query command uses a separate index over a whole corpus and provides very fast retrieval.
Itis based on Apache Lucene, thus Lucene?s user-friendly query syntax can be used.
For instance, querying?insufficiency AND aortic -moderate?
retrieves sentences that contain ?insufficiency?
and ?aortic?
butnot ?moderate?.
The interface to Lucene-based queries can be seen in Figure 1 (E).
The index is based onsentences and contains special fields for extracted concept annotations.
For instance, ?concept:24?
showssentences where the concept with the ID 24 has been extracted.
Indexing can be triggered manually.The second query command iterates over files in the filesystem and uses UIMA?s index structures tofind annotations of requested types.
It integrates into the search framework of Eclipse.
As a consequence,users can easily expand or collapse search results, browse search results, or iteratively open respectivedocuments in a shared editor instance.
The component implements a tree view (confer Figure 1 (F)) anda table view (confer Figure 3b) to show results.
The programmatic search command needs a type name,a type system, and a folder.
As a special feature, search results can be grouped based on their coveredtext or the string value of a feature of the annotations.
Grouping allows to show absolute counts for eachgroup.
It helps users to find and rank relevant candidate terms and phrases.
It can, however, also be usedin other use cases, for example, to list all distinct person mentions in a text collection.
Results can beexported as an HTML report.
Figure 3a shows the generic search page for posing UIMA based queries.The folder can be selected in the script explorer (see Fig.
1 (B)).
The dialog provides a combo box tochoose the type system and auto-completion for the type name and group-by text widgets.Technical users can already use the middleware in combination with rule-based queries when theycreate rule scripts just for this purpose.
These scripts define queries and types for results.
Subsequently,users apply one of the type-based search commands.Only one of the task specific query types in the terminology development environment is not imple-mented as a Lucene search or an annotation type based search command: to support quick testing ofregular expressions, the tool accesses the text search command of Eclipse which allows very fast searchand rapid feedback.5 Case StudyThe main application of the tool is terminology generation and subsequent information extraction for awide range of medical reports.
We evaluated it in a small subdomain of the physical examination con-cerning the heart (cor).
We gathered 200 anonymized documents from a hospital information systemand divided the corpus into a training and a test set (150:50).
For terminology generation, we first ex-tracted all nouns from the training set as attributes.
Then we merged similar entries and deleted irrelevantcandidates.
For each attribute we generated and manually adapted relevant value candidates assigningtemplates wherever possible.
For both tasks, we applied multiple searches and used tool support for fastinspection of the relevant segments in the documents.
The final terminology was used for information90extraction on the test set.
We measured the time necessary for terminology adaption and the precisionand recall of the information extraction on the test set and additionally estimated the recall with a queryas described in Section 4.
The gold standard for information extraction in this feasibility study was de-fined by ourselves.
From the training set, we extracted 20 attributes and 44 boolean and 6 numericalvalues.
Manual correction took about 4 hours.
Microaveraged precision and recall of the informationextraction were 99% and 90% on the test set.
The estimated recall on the test set was 84%.
Roughlyone half of the errors was due to unknown terminology in the test documents.
The other half was mainlyinduced by missing variants of already known concepts.
With a larger training set, these kinds of errorscan be considerably reduced.6 SummaryOntology development and query-driven workflows have not gained as much attention in the UIMAcommunity as, for example, pipeline management, rule development, or evaluation.
Especially if ontolo-gies are developed in order to build information extraction systems, it is desirable to have a workbenchenvironment that integrates both tools for ontology development and information extraction.
The toolsuggested in this paper aims to fill this gap.
It supports the creation of light-weight ontologies for in-formation extraction, that is, it helps to find attributes and their values, and to encode simple relationsbetween concepts.
It integrates into Eclipse and lowers the bridge between different frameworks andtools.
Notably, we assist users in query-driven workflows, which includes a simple way to assess qualitywithout manually annotated documents.
We plan to release the tool under an open source license.AcknowledgmentsThis work was supported by the Competence Network Heart Failure, funded by the German FederalMinistry of Education and Research (BMBF01 EO1004).ReferencesN.
Aswani, V. Tablan, K. Bontcheva, and H. Cunningham.
2005.
Indexing and Querying Linguistic Metadata andDocument Content.
In Proceedings of Fifth International Conference on Recent Advances in Natural LanguageProcessing (RANLP2005), Borovets, Bulgaria.Paul Buitelaar, Daniel Olejnik, and Michael Sintek.
2004.
A Prot?eg?e Plug-In for Ontology Extraction from TextBased on Linguistic Analysis.
In ChristophJ.
Bussler, John Davies, Dieter Fensel, and Rudi Studer, editors, TheSemantic Web: Research and Applications, volume 3053 of Lecture Notes in Computer Science, pages 31?44.Springer Berlin Heidelberg.Philipp Cimiano and Johanna V?olker.
2005.
Text2Onto: A Framework for Ontology Learning and Data-drivenChange Discovery.
In Proceedings of the 10th International Conference on Natural Language Processing andInformation Systems, NLDB?05, pages 227?238, Berlin, Heidelberg.
Springer-Verlag.Philipp Cimiano, Andreas Hotho, and Steffen Staab.
2005.
Learning Concept Hierarchies from Text CorporaUsing Formal Concept Analysis.
J. Artif.
Int.
Res., 24(1):305?339, August.Hamish Cunningham, Diana Maynard, Kalina Bontcheva, Valentin Tablan, Niraj Aswani, Ian Roberts, GenevieveGorrell, Adam Funk, Angus Roberts, Danica Damljanovic, Thomas Heitz, Mark A. Greenwood, Horacio Sag-gion, Johann Petrak, Yaoyong Li, and Wim Peters.
2011.
Text Processing with GATE (Version 6).David Ferrucci and Adam Lally.
2004.
UIMA: An Architectural Approach to Unstructured Information Processingin the Corporate Research Environment.
Natural Language Engineering, 10(3/4):327?348.Manuel Fiorelli, Maria Teresa Pazienza, Steve Petruzza, Armando Stellato, and Andrea Turbati.
2010.
Computer-aided Ontology Development: an integrated environment.
In Ren?e Witte, Hamish Cunningham, Jon Patrick,Elena Beisswanger, Ekaterina Buyko, Udo Hahn, Karin Verspoor, and Anni R. Coden, editors, New Challengesfor NLP Frameworks (NLPFrameworks 2010), pages 28?35, Valletta, Malta, May 22.
ELRA.Peter Kluegl, Martin Atzmueller, and Frank Puppe.
2009.
TextMarker: A Tool for Rule-Based InformationExtraction.
In Christian Chiarcos, Richard Eckart de Castilho, and Manfred Stede, editors, Proceedings of theBiennial GSCL Conference 2009, 2nd UIMA@GSCL Workshop, pages 233?240.
Gunter Narr Verlag.91K.
Liu, W. W. Chapman, G. Savova, C. G. Chute, N. Sioutos, and R. S. Crowley.
2011.
Effectiveness of Lexico-syntactic Pattern Matching for Ontology Enrichment with Clinical Documents.
Methods of Information inMedicine, 50(5):397?407.Michael Tanenblatt, Anni Coden, and Igor Sominsky.
2010.
The ConceptMapper Approach to Named EntityRecognition.
In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani,Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh InternationalConference on Language Resources and Evaluation (LREC?10), Valletta, Malta, may.
European Language Re-sources Association (ELRA).Andreas Wittek, Martin Toepfer, Georg Fette, Peter Kluegl, and Frank Puppe.
2013.
Constraint-driven Evaluationin UIMA Ruta.
In Peter Kluegl, Richard Eckart de Castilho, and Katrin Tomanek, editors, UIMA@GSCL,volume 1038 of CEUR Workshop Proceedings, pages 58?65.
CEUR-WS.org.92
