Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 609?613,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsPredicting Relative Prominence in Noun-Noun CompoundsTaniya MishraAT&T Labs-Research180 Park AveFlorham Park, NJ 07932taniya@research.att.comSrinivas BangaloreAT&T Labs-Research180 Park AveFlorham Park, NJ 07932srini@research.att.comAbstractThere are several theories regarding what in-fluences prominence assignment in Englishnoun-noun compounds.
We have developedcorpus-driven models for automatically pre-dicting prominence assignment in noun-nouncompounds using feature sets based on twosuch theories: the informativeness theory andthe semantic composition theory.
The eval-uation of the prediction models indicate thatthough both of these theories are relevant, theyaccount for different types of variability inprominence assignment.1 IntroductionText-to-speech synthesis (TTS) systems stand togain in improved intelligibility and naturalness ifwe have good control of the prosody.
Typically,prosodic labels are predicted through text analysisand are used to control the acoustic parameters fora TTS system.
An important aspect of prosody pre-diction is predicting which words should be prosod-ically prominent, i.e., produced with greater en-ergy, higher pitch, and/or longer duration than theneighboring words, in order to indicate the for-mer?s greater communicative salience.
Appropriateprominence assignment is crucial for listeners?
un-derstanding of the intended message.
However, theimmense prosodic variability found in spoken lan-guage makes prominence prediction a challengingproblem.
A particular sub-problem of prominenceprediction that still defies a complete solution is pre-diction of relative prominence in noun-noun com-pounds.Noun-noun compounds such as White House,cherry pie, parking lot, Madison Avenue, WallStreet, nail polish, french fries, computer program-mer, dog catcher, silk tie, and self reliance, oc-cur quite frequently in the English language.
In adiscourse neutral context, such constructions usu-ally have leftmost prominence, i.e., speakers producethe left-hand noun with greater prominence than theright-hand noun.
However, a significant portion ?about 25% (Liberman and Sproat, 1992) ?
of themare assigned rightmost prominence (such as cherrypie, Madison Avenue, silk tie, computer program-mer, and self reliance from the list above).
Whatfactors influence speakers?
decision to assign left orright prominence is still an open question.There are several different theories about rela-tive prominence assignment in noun-noun (hence-forth, NN) compounds, such as the structural the-ory (Bloomfield, 1933; Marchand, 1969; Heinz,2004), the analogical theory (Schmerling, 1971;Olsen, 2000), the semantic theory (Fudge, 1984;Liberman and Sproat, 1992) and the informativenesstheory (Bolinger, 1972; Ladd, 1984).1 However, inmost studies, the different theories are examined andapplied in isolation, thus making it difficult to com-pare them directly.
It would be informative and il-luminating to apply these theories to the same taskand the same dataset.For this paper, we focus on two particular the-ories, the informativeness theory and the seman-tic composition theory.
The informativeness theoryposits that the relatively more informative and un-expected noun is given greater prominence in theNN compound than the less informative and morepredictable noun.
The semantic composition theoryposits that relative prominence assignment in NNcompounds is decided according to the semantic re-lationship between the two nouns.We apply these two theories to the task of pre-dicting relative prominence in NN compounds viastatistical corpus-driven methods, within the largercontext of building a system that can predict appro-priate prominence patterns for text-to-speech syn-thesis.
Here we are only focusing on predicting rela-tive prominence of NN compounds in a neutral con-text, where there are no pragmatic reasons (such ascontrastiveness or given/new distinction) for shiftingprominence.1In-depth reviews of the different theories can be found inPlag (2006) and Bell and Plag (2010).6092 Informativeness MeasuresWe used the following five metrics to capture theindividual and relative informativeness of nouns ineach NN compound:?
Unigram Predictability (UP): Defined as thepredictability of a word given a text corpus, itis measured as the log probability of the wordin the text corpus.
Here, we use the maximumlikelihood formulation of this measure.UP = logFreq(wi)?i Freq(wi)(1)This is a very simple measure of word informa-tiveness that has been shown to be effective ina similar task (Pan and McKeown, 1999).?
Bigram Predictability (BP): Defined as the pre-dictability of a word given a previous word, itis measured as the log probability of noun N2given noun N1.BP = log (Prob(N2 | N1)) (2)?
Pointwise Mutual Information (PMI): Definedas a measure of how collocated two words are,it is measured as the log of the ratio of probabil-ity of the joint event of the two words occurringand the probability of them occurring indepen-dent of each other.PMI = logProb(N1, N2)Prob(N1)Prob(N2)(3)?
Dice Coefficient (DC): Dice is another colloca-tion measure used in information retrieval.DC =2?
Prob(N1, N2)Prob(N1) + Prob(N2)(4)?
Pointwise Kullback-Leibler Divergence (PKL):In this context, Pointwise Kullback-Leibler di-vergence (a formulation of relative entropy)measures the degree to which one over-approximates the information content of N2 byfailing to take into account the immediatelypreceding word N1.
(PKL values are alwaysnegative.)
A high absolute value of PKL indi-cates that there is not much information con-tained in N2 if N1 is taken into account.
Wedefine PKL asProb(N2 | N1) logProb(N2 | N1)Prob(N2)(5)Another way to consider PKL is as PMI nor-malized by the predictability of N2 given N1.All except the first the aforementioned five infor-mativeness measures are relative measures.
Ofthese, PMI and Dice Coefficient are symmetric mea-sures while Bigram Predictability and PKL are non-symmetric (unidirectional) measures.3 Semantic Relationship ModelingWe modeled the semantic relationship between thetwo nouns in the NN compound as follows.
Foreach of the two nouns in each NN compound, wemaintain a semantic category vector of 26 elements.The 26 elements are associated with 26 semanticcategories (such as food, event, act, location, arti-fact, etc.)
assigned to nouns in WordNet (Fellbaum,1998).
For each noun, each element of the semanticcategory vector is assigned a value of 1, if the lem-matized noun (i.e., the associated uninflected dic-tionary entry) is assigned the associated semanticcategory by WordNet, otherwise, the element is as-signed a value of 0.
(If a semantic category vector isentirely populated by zeros, then that noun has notbeen assigned any semantic category information byWordNet.)
We expected the cross-product of the se-mantic category vectors of the two nouns in the NNcompound to roughly encode the possible semanticrelationships between the two nouns, which ?
fol-lowing the semantic composition theory ?
corre-lates with prominence assignment to some extent.4 Semantic Informativeness FeaturesFor each noun in each NN compound, we alsomaintain three semantic informativeness features:(1) Number of possible synsets associated with thenoun.
A synset is a set of words that have the samesense or meaning.
(2) Left positional family size and(3) Right positional family size.
Positional familysize is the number of unique NN compounds that in-clude the particular noun, either on the left or on theright (Bell and Plag, 2010).
These features are ex-tracted from WordNet as well.The intuition behind extracting synset counts andpositional family size was, once again, to measurethe relative informativeness of the nouns in NN com-pounds.
Smaller synset counts indicate more spe-cific meaning of the noun, and thus perhaps moreinformation content.
Larger right (or left) posi-tional family size indicates that the noun is present610in the right (left) position of many possible NN com-pounds, and thus less likely to receive higher promi-nence in such compounds.These features capture type-based informative-ness, in contrast to the measures described in Sec-tion 2, which capture token-based informativeness.5 Experimental evaluationFor our evaluation, we used a hand-labeled corpusof 7831 NN compounds randomly selected from the1990 Associated Press newswire, and hand-taggedfor leftmost or rightmost prominence (Sproat, 1994).This corpus contains 64 pairs of NN compounds thatdiffer in terms of capitalization but not in terms ofrelative prominence assignment.
It only containsfour pairs of NN compounds that differ in terms ofcapitalization and in terms of relative prominenceassignment.
Since there is not enough data in thiscorpus to consider capitalization as a feature, we re-moved the case information (by lowercasing the en-tire corpora), and removed any duplicates.
Of thefour pairs that differed in terms of capitalization,we only retained the lower-cased NN compounds.By normalizing Sproat?s hand-labeled corpus in thisway, we created a slightly smaller corpus 7767 ut-terances that was used for the evaluation.For each of the NN compounds in this corpus, wecomputed the three aforementioned feature sets.
Tocompute the informativeness features, we used theLDC English Gigaword corpus.
The semantic cate-gory vectors and the semantic informativeness fea-tures were obtained from Wordnet.
Using each ofthe three feature sets individually as well as com-bined together, we built automatic relative promi-nence prediction models using Boostexter, a dis-criminative classification model based on the boost-ing family of algorithms, which was first proposedin Freund and Schapire (1996).Following an experimental methodology similarto Sproat (1994), we used 88% (6835 samples) ofthe corpus as training data and the remaining 12%(932 samples) as test data.
For each test case, theoutput of the prediction models was either a 0 (indi-cating that the leftmost noun receive higher promi-nence) or a 1 (indicating that the rightmost noun re-ceive higher prominence).
We estimated the modelerror of the different prediction models by comput-ing the relative error reduction from the baseline er-ror.
The baseline error was obtained by assigningthe majority class to all test cases.
We avoided over-fitting by using 5-fold cross validation.5.1 ResultsThe results of the evaluation of the different modelsare presented in Table 1.
In this table, INF denotesinformativeness features (Sec.
2), SRF denotes se-mantic relationship modeling features (Sec.
3) andSIF denotes semantic informativeness features (Sec.4).
We also present the results of building predictionmodels by combining different features sets.These results show that each of the predictionmodels reduces the baseline error, thus indicatingthat the different types of feature sets are each cor-related with prominence assignment in NN com-pounds to some extent.
However, it appears thatsome feature sets are more predictive.
Of the indi-vidual feature sets, SRF and INF features appear tobe more predictive than the SIF features.
Combinedtogether, the three feature sets are most predictive,reducing model error over the baseline error by al-most 33% (compared to 16-22% for individual fea-ture sets), though combining INF with SRF featuresalmost achieves the same reduction in baseline error.Note that none of the three types of feature setsthat we have defined contain any direct lexical infor-mation such as the nouns themselves or their lem-mata.
However, considering that the lexical con-tent of the words is a rich source of information thatcould have substantial predictive power, we includedthe lemmata associated with the nouns in the NNcompounds as additional features to each feature setand rebuilt the prediction models.
An evaluation ofthese lexically-enhanced models is shown in Table2.
Indeed, addition of the lemmatized form of theNN compounds substantially increases the predic-tive power of all the models.
The baseline error isreduced by almost 50% in each of the models ?the error reduction being the greatest (53%) for themodel built by combining all three feature sets.6 Discussion and ConclusionSeveral other studies have examined the main idea ofrelative prominence assignment using one or moreof the theories that we have focused on in this paper(though the particular tasks and terminology usedwere different) and found similar results.
For exam-ple, Pan and Hirschberg (2000) have used some ofthe same informativeness measures (denoted by INFabove) to predict pitch accent placement in word bi-611Feature Av.
baseline Av.
model % ErrorSets error (in %) error (in %) reductionINF 29.18 22.85 21.69SRF 28.04 21.84 22.00SIF 29.22 24.36 16.66INF-SRF 28.52 19.53 31.55INF-SIF 28.04 21.25 24.33SRF-SIF 29.74 21.30 28.31All 28.98 19.61 32.36Table 1: Results of prediction modelsFeature Av.
baseline Av.
model % ErrorSets error (in %) error (in %) reductionINF 28.6 14.67 48.74SRF 28.34 14.29 49.55SIF 29.48 14.85 49.49INF-SRF 28.16 14.81 47.45INF-SIF 28.38 14.16 50.03SRF-SIF 29.24 14.51 50.30All 28.12 13.19 52.95Table 2: Results of lexically-enhanced prediction modelsgrams.
Since pitch accents and perception of promi-nence are strongly correlated, their conclusion thatinformativeness measures are a good predictor ofpitch accent placement agrees with our conclusionthat informativeness measures are useful predictorsof relative prominence assignment.
However, wecannot compare their results to ours directly, sincetheir corpus and baseline error measurement2 weredifferent from ours.Our results are more directly comparable to thoseshown in Sproat (1994).
For the same task as weconsider in this study, besides developing a rule-based system, Sproat also developed a statisticalcorpus-based model.
His feature set was developedto model the semantic relationship between the twonouns in the NN compound, and included the lem-mata related to the nouns.
The model was trainedand tested on the same hand-labeled corpus that weused for this study and the baseline error was mea-sured in the same way.
So, we can directly com-pare the results of our lexically-enhanced SRF-basedmodels to Sproat?s corpus-driven statistical model.2Pan and Hirschberg present error obtained by using aunigram-based predictability model as baseline error.
It is un-clear what is the error obtained by assigning left prominence toall words in their database, which was our baseline error.In his work, Sproat reported a baseline error of 30%and a model error of 16%.
The reported relative im-provement over the baseline error in Sproat?s studywas 46.6%, while our relative improvement usingthe lexically enhanced SRF based model was 49.5%,and the relative improvement using the combinedmodel is 52.95%.Type-based semantic informativeness features ofthe kind that we grouped as SIF were analyzedin Bell and Plag (2010) as potential predictors ofprominence assignment in compound nouns.
Likeus, they too found such features to be predictiveof prominence assignment and that combining themwith features that model the semantic relationship inthe NN compound makes them more predictive.7 ConclusionThe goal of the presented work was predicting rel-ative prominence in NN compounds via statisticalcorpus-driven methods.
We constructed automaticprediction models using feature sets based on twodifferent theories about relative prominence assign-ment in NN compounds: the informativeness theoryand the semantic composition theory.
In doing so,we were able to compare the two theories.Our evaluation indicates that each of these theo-ries is relevant, though perhaps to different degrees.This is supported by the observation that the com-bined model (in Table 1) is substantially more pre-dictive than any of the individual models.
This indi-cates that the different feature sets capture differentcorrelations, and that perhaps each of the theories(on which the feature sets are based) account for dif-ferent types of variability in prominence assignment.Our results also highlight the difference betweenbeing able to use lexical information in prominenceprediction of NN compounds, or not.
Using lexicalfeatures, we can improve prediction over the defaultcase (i.e., assigning prominence to the left noun inall cases) by over 50%.
But if the given input is anout-of-vocabulary NN compound, our non-lexicallyenhanced best model can still improve predictionover the default by about 33%.Acknowledgment We would like to thankRichard Sproat for freely providing the dataset onwhich the developed models were trained and tested.We would also like to thank him for his advice onthis topic.612ReferencesM.
Bell and I. Plag.
2010.
Informativenessis a determinant of compound stress in En-glish.
Submitted for publication.
Obtained fromhttp://www2.uni-siegen.de/?engspra/publicat.html on February 12, 2010.L.
Bloomfield.
1933.
Language, Holt, New York.D.
Bolinger.
1972.
Accent is predictable (if you?re amind-reader).
Language 48.C.
Fellbaum (editor).
1998.
WordNet: An ElectronicLexical Database, The MIT Press, Boston.Y.
Freund and R. E. Schapire, 1996.
Experiments witha new boosting alogrithm.
Machine Learning: Pro-ceedings of the Thirteenth International Conference,pp.
148-156.E.
Fudge.
1984.
English Word-Stress, Allen and Unwin,London and Boston.H.
J. Giegerich.
Compound or phrase?
English noun-plus-noun constructions and the stress criterion.
InEnglish Language and Linguistics, 8:1?24.R.
D. Ladd, 1984.
English compound stress.
In DafyddGibbon and Helmut Richter (eds.)
Intonation, Accentand Rhythm: Studies in 1188 Discourse Phonology,W de Gruyter, Berlin.M.
Liberman and R. Sproat.
1992.
The Stress and Struc-ture of Modified Noun Phrases in English.
In I.
Sag(ed.
), Lexical Matters, pp.
131?181, CSLI Publica-tions, Chicago, University of Chicago Press.H.
Marchand.
The categories and types of present-dayEnglish word-formation, Beck, Munich.S.
Olsen.
2000.
Compounding and stress in English: Acloser look at the boundary between morphology andsyntax.
Linguistische Berichte, 181:55?70.S.
Pan and J. Hirschberg.
2000.
Modeling local contextfor pitch accent prediction.
Proceedings of the 38thAnnual Conference of the Association for Computa-tional Linguistics (ACL-00), pp.
233-240, Hong Kong.ACL.S.
Pan and K. McKeown.
1999.
Word informativenessand automatic pitch accent modeling.
Proceedings ofthe Joint SIGDAT Conference on EMNLP and VLC,pp.
148?157.I.
Plag.
2006.
The variability of compound stress in En-glish: structural, semantic and analogical factors.
En-glish Language and Linguistics, 10.1, pp.
143?172.R.
Sproat.
1994.
English Noun-Phrase Accent Predictionfor Text-to-Speech.
Computer Speech and Language,8, pp.
79?94.R.E.
Schapire, A brief introduction to boosting.
In Pro-ceedings of IJCAI, 1999.S.
F. Schmerling.
1971.
A stress mess.
Studies in theLinguistic Sciences, 1:52?65.613
