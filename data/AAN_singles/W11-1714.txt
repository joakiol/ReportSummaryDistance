Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 111?117,24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational LinguisticsAutomatic?Sentiment?Classification?of?Product?Reviews?Using?Maximal?Phrases?Based?AnalysisMaria?TchalakovaTextkernel?BVNieuwendammerkade?28A?17NL?1022?AB?AmsterdamThe?Netherlandsmaria.tchalakova@gmail.comDale?GerdemannUniversity?of?T?bingenWilhelmstr.?19?2372074?T?bingenGermanydale.gerdemann@googlemail.comDetmar?MeurersUniversity?of?T?bingenWilhelmstr.
?19?2372074?T?bingen?Germanydm@sfs.uni-tuebingen.deAbstractIn ?
this ?
paper ?
we ?
explore ?
the ?
use ?
of ?
phrases?occurring ?
maximally ?
in ?
text ?
as ?
features ?
for?sentiment?classification?of?product?reviews.
?The?goal?is?to?find?in?a?statistical?way?representative?words ?
and ?
phrases ?
used ?
typically ?
in ?
positive?and ?negative ?
reviews.
?The?approach ?does ?not?rely?on?predefined?sentiment?lexicons,?and?the?motivation ?
for ?
this ?
is ?
that ?
potentially ?
every?word ?
could ?
be ?
considered ?
as ?
expressing?something?positive?and/or?negative?in?different?situations,?and?that?the?context?and?the?personal?attitude?of?the?opinion?holder?should?be?taken?into?account?when?determining?the?polarity?of?the ?
phrase, ?
instead ?
of ?
doing ?
this ?
out ?
of?particular?context.1 IntroductionAs?human?beings?we?use?different?ways?to?express?opinions ?
or ?
sentiments.
?
The ?
field ?
of ?
sentiment?analysis?tries?to?identify?the?ways,?in?which?people?express?opinions?or?sentiments?towards?a?particular?target ?
or ?
entity.
?
The ?
entities ?
could ?
be ?
persons,?products,?events,?etc.
?With?the?development?of?the?Internet?technologies?and?robust?search?engines?in?the ?
last ?
decade, ?
people ?
nowadays ?
have ?
a ?
huge?amount?of?free?information.
?Because?of?this?huge?amount, ?
however, ?
the ?
data ?
needs ?
to ?
be ?
first?effectively?processed?so?that?it?could?be?used?in?a?helpful ?
way.
?
The ?
automatic ?
identification ?
of?sentiments?would?make?possible?the?processing?of?large?amounts?of?such?opinionated?data.????
?The?focus?of?this?paper?is?sentiment?classification?at?document?level,?namely?classification?of?product?reviews ?
in ?
the ?
categories ?
positive ?
polarity ?
or?negative?polarity.
?Training?and?testing?data?for?our?experiments ?
is ?
the ?
Multi?Domain ?
Sentiment?Dataset ?
(Blitzer ?
et ?
al., ?
2007), ?
which ?
consists ?
of?product?reviews?of?different?domains,?downloaded?from ?
Amazon1.
?
We ?
explore ?
the ?
use ?
of ?
phrases?occurring ?
maximally ?
in ?
text ?
as ?
features ?
for?sentiment ?
classification ?
of ?
product ?
reviews.
?In?contrast ?
to ?
many ?
related ?
works ?
on ?
sentiment?classification?of?documents,?we?do?not?use?general?polarity ?
lexicons, ?
which ?
contain ?
predefined?positive?and?negative?words.?Very?often?the?same?word?or?phrase?could?express?something?positive?in?one?situation?and?something?negative?
in ?another.?We?identify?words?and?phrases,?which?are?typically?used?in?positive?and?negative?documents?of?some?specific?domains,?based?on?the?frequencies?of?the?words?and?phrases?in?the?domain?specific?corpora.
?After ?
that ?we ?use ?
these ?phrases ?
to ?
classify ?new?sentiment ?
documents ?
from ?
the ?
same ?
type ?
of?documents,?from?which?the?phrases?are?extracted.1http://www.amazon.com/1112 Phrase?ExtractionIn?order?to?extract?distinctive?phrases?we?use?the?approach?of?Burek?and?Gerdemann?
(2009),?who?try?to?identify?phrases,?which?are?distinctive?for?each?of ?
the ?
four ?different ?
categories ?of ?
documents ?
in?their ?
medical ?
data.
?
With ?
distinctive ?
they ?
mean?phrases, ?
which ?
occur ?
predominantly ?
in ?
one?category ?
of ?
the ?
documents ?
or ?
another.
?
The?algorithm?extracts?phrases?of?any?length.
?The?idea?is ?
that ?
if ?
a ?
phrase ?
is ?
distinctive ?
for ?
a ?
particular?category,?it?does?not?matter?how?long?the?phrase?is.
?The?algorithm?looks?for?repeats?of?phrases?of?any?length, ?
and ?
could ?
also ?
count ?
different ?
types ?
of?occurrences ?
of ?
phrases, ?
e.g.
?
maximal, ?
left?maximal,?or?right?maximal.
?Considering?such?types?of?occurrences,?it?is?possible?to?restrict?the?use?of?certain ?
phrases, ?
which ?
might ?
not ?
be ?
much?distinctive ?
and ?
therefore ?
might ?
not ?
be?representative?for?a?category.?Similar?to?Burek?and?Gerdemann?
(2009)?we?experiment?with?using?all?types ?of ?occurrences ?of ?
a ?
phrase ?
as ?
long ?as ?
the?phrase?occurs?maximally?at?
least?one?time?in?the?text.2.1 Distinctiveness?of?PhrasesDistinctive ?
phrases ?
are ?
phrases, ?
which?predominantly ?
occur ?
in ?
one ?
particular ?
type ?
of?documents ?
(Burek ?
and ?
Gerdemann, ?
2009).
?
The?presence?of?such?phrases?in?a?document?is?a?good?indicator?of?the?category?(or?type)?of?the?document.?The?general?rule,?as?Burek?and?Gerdemann?
(2009)?point ?
out, ?
is ?
that ?
if ?
some ?phrases ?
are ?uniformly?distributed ?
in ?
a ?
set ?
of ?
documents ?
with ?
different?categories, ?
then ?
these ?phrases ?are ?not ?distinctive?for?any?of?the?categories?in?the?collection.?On?the?other?hand,?if?particular?phrases?appear?more?often?in?one?category?of?documents?than?in?another,?they?are?good?representatives?for?the?documents?of?this?type,?and?consequently?are?said?to?be?distinctive2.???
?There?are?different?weighting?schemes,?which?one?can?use?to?determine?the?importance?of?a?term?for?the ?
semantics ?
of ?
a ?
document.
?
Burek ?
and?Gerdemann ?
(2009) ?
implement ?
their ?
own ?
scoring?2If ?
the?number?of ?occurrences?of ?
such?phrase?
in ?
the?whole?collection?of?documents?is?very?small,?however,?the?clustering?of?the?phrase?in?some?documents?of?a?specific?category,?may?be?purely?accidental.?(Burek?and?Gerdemann,?2009)function?for?weighting?the?extracted?phrases.?One?of?their?reasons?not?to?use?the?standard?weighting?function?tf?idf?is?that?the?idf?measure?does?not?take?into?account?what?the?category?of?the?documents?is,?in?which?the?term?occurs.
?This?is?important?in?their?case,?because?their?data?consist?of?four?categories,?which ?
could ?
be ?
grouped ?
in ?
two ?
main ?
classes,?namely ?excellent?
and ?good?
on?the?one?hand,?and?fair?
and?poor?on?the?other?hand.
?A?problem?when?using?tf?idf?will ?appear, ?
if ?
there?is?a?rare?phrase,?which ?
occurs ?
in ?
a ?
small ?
number ?
of ?
documents,?however, ?
it ?
clusters ?
in ?documents ?
from ?
the ?
two?different?classes,?for?example,?in?excellent?and?fair,?or ?
in ?good?
and ?poor.
?
This ?
will ?
not ?
be ?
a ?
good?distinctive ?
phrase ?
for ?
this ?
categorization ?
of ?
the?data.
?
Another ?
motivation ?
to ?
develop ?
their ?
own?scoring ?
function ?
is ?
to ?cope ?with ?
the ?problem?of?burstiness?
(see?section?2.2.1).2.2 Extraction?of?PhrasesThis?section?describes?the?algorithm?of?Burek?and?Gerdemann ?
(2009) ?
for ?
extracting ?
distinctive?phrases?and?how?we?have?modified?and?used?it?in?the?context ?of ?our?work.
?We?first ?show?how?the?phrases?are?ranked,?so?that?one?knows?what?phrases?are?more?or?less?distinctive?than?others.2.2.1 The?Scoring?AlgorithmThe ?
extracted ?
phrases ?
are ?
represented ?
by?occurrence ?
vectors.
?
These ?
vectors ?
have ?
two?elements??
?one?for?the?number?of?documents?with?category ?positive ?
polarity, ?
and ?
another ?
for ?
the?negative?polarity.
?Each?element?of?the?vector?stores?the ?number ?
of ?
distinct ?
documents, ?
in ?which ?
the?phrase?occurs.
?For?example,?if?a?phrase?occurs?in?10 ?
positive ?
reviews, ?
and ?
1 ?
negative ?
review, ?
the?occurrence?vector?of?this?phrase?is?<10,?1>?.?This?shows?that?for?the?representation?of?the?phrases?we?take?
into?account?
the?document?frequency?of?
the?phrase,?and?not?its?term?frequency.?The?motivation?behind?this?choice?is?to?cope?with?the?problem?of?burstiness?of?terms.?Madsen?et?al.?
(2005)?explain?burstiness ?
in ?
the ?
following ?
way: ?The ?
term?burstiness ?
(Church?and?Gale, ?1995;?Katz, ?1996) ?describes?the?behavior?of?a?rare?word?appearing?many?times?in?a?single?document.
?Because?of?the ?large ?number ?of ?possible ?words, ?most ?words ?do?not ?appear ?
in ?a ?given ?document.
?However, ?
if ?a ?112word?does?appear?once,?it?is?much?more?likely?to ?appear?again,?i.e.?words?appear?in?bursts.?
?We ?assign ?
a ?
score ?
to ?
a ?
phrase ?
by ?giving ?
the?phrase ?
one ?
point, ?
if ?
the ?
phrase ?
occurs ?
in ?
a?document?with?positive?polarity?and?zero?points,?if?it?occurs?in?a?document?with?negative?polarity.?
?
?Let?us?take?again?the?occurrence?vector?of?<10,?1>?.?According?to?the?way?the?points?are?given,?the?vector?will?be?assigned?a?score?of?10?((1?point?*?10)?
?+?(0?points?*?1)?=?10).
?Is?this?a?good?score,?which ?
indicates ?
that ?
the ?phrase ?
is ?distinctive ?
for?documents?of?category?positive?polarity?
?We?can?answer ?
this ?
question, ?
if ?
we ?
randomly ?
choose?another?phrase,?which?occurs?in?11?documents,?and?see?what?the?probability?is,?that?this?phrase?would?have?a?score,?which?is?higher?than?or?equally?high?to?the?score?of?the?phrase?in?question?
(Burek?and?Gerdemann, ?
2009).
?
In ?
order ?
to ?
calculate ?
this?probability, ?
the ?
scoring ?
method ?
performs ?
a?simulation, ?
in ?
which ?
occurrence ?
vectors ?
for?randomly?chosen?phrases?are?created.?Let?us?pick?randomly?one?phrase,?which?hypothetically?occurs?in?11?reviews.?Let?also,?have?a?data?of?600?positive?reviews?and?600?negative?reviews.
?The?probability?then, ?
that ?
the ?
random?phrase ?would ?occur ?
?
in ?a?positive?or?a?negative?review?is?0.5.
?Based?on?these?probabilities, ?
the ?
simulation ?
process ?
constructs?random?vectors?for?the?random?phrase,?indicating?whether ?
the ?
phrase ?
occurs ?
in ?
a ?
positive ?
or ?
in?negative ?
review.
?
For ?
example, ?
if ?
in ?
a ?
particular?run, ?
the ?
simulation ?
says ?
that ?
the ?
random ?phrase?occurs?in?a?positive?review,?then?we?have?a?random?vector?of?<1,?0>.?Otherwise,?<0,?1>?for?a?negative?review.
?The?program?calculates ?as ?many?random?vectors ?
as ?
the ?number ?of ?
reviews, ?
in ?which ?
the?random?phrase?is?said?to?occur.?In?this?example,?the?number?of?documents?is?11.
?Therefore,?11?random?vectors ?will ?be ?constructed.
?They?may?
look?
like?this:?<1,?0>,?<1,?0>,?<0,?1>?,?<1,?0>,?<1,?0>,?<0,?1>,?<0,?1>,?<0,?1>,?<1,?0>,?<1,?0>,?<0,?1>.
?These?vectors?are?then?summed?up,?and?the?result?vector?<6, ?
5> ?
is ?
the ?
random?
occurrence ?vector ?
for ?
the?random ?
phrase.
?
It ?
tells ?
us ?
that ?
the ?
phrase,?hypothetically, ?
occurs ?
in ?
6 ?
positive ?
and ?
in ?
5?negative?reviews.
?The?score?for?the?random?phrase?is?now?calculated?in?the?same?way?as?for?the?non?random ?
phrases: ?
1 ?
point ?
is ?
given ?
for ?
each?occurrence?of?the?phrase?in?a?positive?review,?and?0?points?otherwise.?So,?the?score?for?this?phrase?is?6?(?((1?point?*?6)??+?(0?points?*?5)?=?6)?).
?This?process?is ?performed?a ?certain ?number ?of ?
times.
?For ?
the?experiments?presented?
in?section?3.2, ?we?run?the?simulation?10,000?times?for?each?extracted?phrase.?This?means?that?10,000?random?vectors?per?phrase?are?created.?
?The ?
last ?
step ?
is ?
to ?
compare ?
the ?
scores ?of ?
the?random?phrase?with?the?score?of?the?actual?phrase,?and?to?see?how?many?of?the?10,000?random?vectors?give ?
a ?
score ?
higher ?
than ?or ?
equally ?
high ?
to ?
the?score?of?the?actual?phrase.
?If?the?number?of?random?vectors,?which?give?a?higher?than?or?equally?high?score ?
to ?
the ?
actual ?
phrase, ?
is ?
bigger ?
than ?
the?number ?
of ?
random ?
vectors, ?
which ?
give ?
a ?
score?lower?than?the?actual?phrase,?then?the?actual?phrase?is?assigned?a?positive?score,?and?the?value?of?this?score ?
is ?
the ?
approximate ?
number ?
of ?
random?vectors, ?
from?which ?higher ?
than?or ?equally ?high?scores?to?the?actual?phrase?score?are?calculated.
?If?the ?number ?
is ?
lower, ?
the ?phrase ?
is ?
assigned ?
the?approximate ?
number ?
of ?
random ?
vectors, ?
from?which?lower?scores?than?the?actual?phrase?score?are?calculated, ?
and ?
a ?
minus ?
sign ?
is ?
attached ?
to ?
the?number,?making?the?score?negative.2.2.2 The?Phrase?Extraction?AlgorithmThe?main?idea?of?the?algorithm?is?that?if?a?phrase?is?distinctive ?
for ?
a ?
particular ?
category, ?
it ?
does ?
not?matter?how?long?the?phrase?is??
?as?long?as?it?helps?for ?
distinguishing ?
one ?
type ?
of ?
document ?
from?another,?it?should?be?extracted.
?In?order?to?extract?phrases ?
in ?
this ?
way, ?
the ?
whole ?
collection ?
of?documents?is?represented?as?one?long?string.?Each?phrase?is?then?a?substring?of?this?string.?It?will?be?very?expensive?to?compute?statistics?(i.e.?tf?and?df)?and?to?run?
the?simulation?process?
(see?2.2.1) ?for?each?substring?in?the?text.
?The?reason?is?that ?the?amount?of?substrings?might?be?huge??
?there?are?a?total ?
of ?
N(N ?
+ ?
1) ?
/ ?
2 ?
substrings ?
in ?
a ?
corpus?
(Yamamoto ?
and ?
Church, ?
2001).
?
Yamamoto ?
and?Church ?
(2001) ?
show ?
how ?
this ?
problem ?
can ?
be?overcome ?
by ?
grouping ?
the ?
substrings ?
into?equivalence?classes?and?performing?operations?(i.e.?computing?statistics)?on?these?classes?instead?of?on?the?individual?elements?of?the?classes.?They?use?for?this?the?suffix?array?data?structure.?The?number?of?the?classes?is?at?most?2N??
?1.1132.2.3 Maximal?Occurrence?of?a?PhraseThe ?
suffix ?
array ?
data ?
structure ?
allows ?
for ?
easy?manipulation?of?the?strings.
?The?algorithm?extracts?phrases ?
if ?
they?
repeat ?
in ?
text, ?and?if ?
the ?phrases?occur?maximally?at ?
least ?once?
in ?
the?
text.
?
If ?
the?phrase?do?not?occur?maximally?at?least?one?time,?then?
it ?may?not ?be?a ?good?linguistic?unit, ?which?could?
stand?on?
its ?own.
?Example?of ?such ?words?might ?
be ?
the ?
different ?
parts ?
of ?
certain ?
named?entities.
?
For ?
instance, ?
the ?
name ?Bugs ?
Bunny.
?
If?Bugs?or?Bunny?never?appear?apart?from?each?other?in?
the ?
text, ?
then?
this ?
imply?
that ?
they?comprise?a?single ?
entity ?
and ?
they ?
should ?
always ?
appear?together?in?the?text.
?In?this?case?it?does?not?make?sense, ?
for ?
example, ?
to ?
count ?
only ?Bugs?
or ?
only?Bunny?
and ?
calculate ?
statistics ?
(e.g.
?
tf ?
or ?
df) ?
for?each?of?them.?They?should?be?grouped?instead?into?a?class.????
Burek ?
and ?
Gerdemann ?
(2009) ?
mention ?
three?different ?
types ?
of ?
occurrences ?
of ?
a ?
phrase: ?
left?maximal, ?
right ?
maximal, ?
and ?
maximal.
?
A ?
left?maximal?occurrence?of?a?phrase?S[i,j]?means?that?the ?
longer ?phrase ?S[i?1,j] ?does ?not ?
repeat ?
in ?
the?corpus ?
(Burek ?
and ?
Gerdemann, ?
2009).
?
For?example, ?
in ?
the ?
sentences ?
below, ?
the ?
phrase?recommend?
is?not?left?maximal,?because?it?can?be?extended?to?the?left?with?the?word?highly:I?highly?recommend?the?book.?You?highly?recommend?this?camera.?
??On?the?other?hand?the?phrase?highly?recommend?is?left?maximal.??
?In?a?similar ?way?we?define?
the?notion?of?right?maximal ?
occurrence ?
of ?
a ?
phrase.
?
A ?
maximal?occurrence?of?a?phrase?is?when?the?occurrence?of?the?phrase?is?both?left?maximal?and?right?maximal?(Burek?and?Gerdemann,?2009).
?The?phrase?highly ?recommend?
in?the?example?sentences?above?is?in?this?sense?maximal.??It?is?not?clear?a?priori?which?of?these?types?should?be?taken?into?account?for?the?successful?realization?of?a?given?application.
?One?could?consider?only?the?left ?maximal, ?only ?
the ?
?
right ?maximal, ?only ?
the?maximal ?
occurrences ?
of ?
the ?
phrases, ?
or ?
all?occurrences.
?
We ?
experimented ?
with ?all?occurrences.
?
Our ?
motivation ?
is ?
that ?
using ?
all?phrases ?would ?give ?us ?
a ?
big ?
enough ?number ?of?distinctive?phrases?and?we?will?most?probably?not?have?a?problem?with?data?sparseness.3 Sentiment ?
Classification ?
of ?
Product?ReviewsFor ?
the ?
experiments ?
presented ?below?we ?used ?
a?supervised ?
machine ?
learning ?
approach, ?
and?different ?
sets ?
of ?
features.
?
Reviews ?
from ?
two?domains,?books?and?cameras?&?photos,?are?used?as?training?and?testing?data.3.1 Choosing ?
Distinctive ?
Phrases ?
for?ClassificationOnce ?
the ?
phrases ?
with ?
which ?
we ?
would ?
like ?
to?represent?the?documents?are?extracted,?we?need?to?consider?two?things?in?the?very?beginning.
?On?the?one ?
hand, ?
the ?
phrases ?
should ?
be ?
as ?
much?distinctive ?
as ?
possible.
?On ?
the ?other ?
hand, ?
even?though ?
a ?
phrase ?
might ?
occur ?
predominantly ?
in?negative ?
reviews, ?
it ?
occurs ?
very ?
often ?
also ?
in?positive ?reviews?
(once?or ?at ?
least ?
several ?
times),?and?vice?versa.?Should?we?consider?such?phrases?
?If?yes,?what?would?be?the?least?acceptable?number?of?occurrences?of?the?phrases?in?the?opposite?type?of ?
reviews?
?
We ?
might ?
choose ?
as ?
distinctive?phrases?those?which?occur?only?in?positive?or?only?in?negative?reviews,?however,?these?phrases?will?be?very?few,?and?we?might?have?the?problem?of?data?sparseness.?On?the?other?hand,?using?all?extracted?phrases?might?bring?a?lot?of?noise,?because?many?of?the?phrases?will?not?be?very?good?characteristics?of?the?data.
?We?experimented?with?several ?different?subsets?of?the?set?of?all?extracted?phrases.??
?
In ?
order ?
to ?
decide ?
what ?
subsets ?
of ?
extracted?phrases?to?use,?we?analyzed?the?set?of?all?extracted?phrases ?paying ?attention?
to ?
their ?vectors ?and ?
the?scores,?trying?to?find?a?trade?off?between?the?two?mentioned?considerations?above.3.2 Experiments???SVM?is?used?as?a?machine?learning?algorithm?for?the?experiments?(the?implementation?of?the?SVM?package?LibSVM3?in?GATE4).3Libsvm:?a?library?for?support?vector?machines,?2001.?software?available?at?http://www.csie.ntu.edu.tw/?cjlin/libsvm.4http://gate.ac.uk/114?
?For?each?experiment?we?first?divide?the?reviews?of?each?domain?into?training?and?testing?data?with?ratio?two?to?one.
?From?this?training?data?we?extract?the ?
distinctive ?
phrases, ?
which ?
are ?
later ?
used ?
as?features ?
to ?
the ?
learning ?algorithm.
?As?evaluation?method?we?apply?the?k?fold?cross?validation?test,?with?k=10.?For?all?experiments?we?used?the?default?tf?idf?weight?for?the?n?grams.
?For?each?domain?we?conduct?five?different?experiments,?each?time?using?different ?
subsets ?
of ?
distinctive ?
phrases.
?
?
All?experiments?were?performed?with?GATE.?
?For?each?domain?the?training?data?from?which?the?phrases ?
are ?
extracted ?
consists ?
of ?
about ?
665?negative?and?665?positive?reviews.?The?testing?data?consists?of?333?negative?and?333?positive?reviews.??
?It?is?interesting?to?notice?that?although?the?results?of ?
the ?
experiments ?
are ?
different, ?
they ?
are ?
very?close?to?each?other,?regardless?of?the?big?difference?in ?
the ?
number ?
of ?
phrases ?
used ?
as ?
features.
?Therefore, ?
we ?
decided ?
to ?
experiment ?
with ?
all?extracted?phrases.?It?turned?out?that?the?results?of?that?experiment?are?the?best.
?This?would?imply?that?the ?
bigger ?
number ?
of ?
phrases ?
is ?
helpful ?
and ?
it?compensates ?
for ?
the ?use ?of ?
phrases ?
that ?
are ?not?much?distinctive.?
?The?results?of?all?experiments?for?domain ?books?are ?
summarized ?
in ?
Table ?
1. ?
The ?
best ?
achieved?results?of?81%?precision,?recall,?and?F?measure?are?given ?
in ?
bold.
?
The ?
rightmost ?
column ?
gives ?
the?number?of?negative?(n.)?and?positive?(p.)?
?phrases?used?in?each?experiment.Experiment Reviews P R F?m Phrases?usedExp1 NegativePositiveOverall0.770.800.780.800.770.780.790.780.781685?n.1116?p.Exp2 NegativePositiveOverall0.750.800.770.800.740.770.770.760.77924?n.568?p.Exp3 NegativePositiveOverall0.760.780.770.780.760.770.770.770.77349?n.
?178?p.Exp4 NegativePositiveOverall0.770.790.780.790.770.780.780.780.7810552?n.
?9084?p.Exp5 NegativePositiveOverall0.800.810.810.810.800.810.800.800.81All:24107?n.?21149?p.Table?1:?Domain?books?
?
Table ?
2 ?
summarizes ?
the ?
results ?
for ?
domain?camera&photos, ?showing?the?best?results?of?86%?precision,?recall,?and?F?measure?in?bold.?
?
Similar ?
to ?
the ?
experiments ?
with ?
reviews ?
of?domain?books,?the?results?for?camera&photos?in?all?five ?
experiments ?
are ?
very ?
close.
?
Again ?
the ?
best?results?are?obtained?when?all?extracted?distinctive?phrases?are?considered.
?Experiment Reviews P R F?m Phrases?usedExp1 NegativePositiveOverall0.850.830.840.830.850.840.840.840.841746n.
?1883?p.Exp2 NegativePositiveOverall0.840.810.830.810.850.830.820.830.831013n.
?1053?p.Exp3 NegativePositiveOverall0.860.830.850.830.870.850.850.850.85384?n.
?432?p.Exp4 NegativePositiveOverall0.850.830.840.830.860.840.840.840.847572?n.
?9821?p.Exp5 NegativePositiveOverall0.860.850.860.850.870.860.860.860.86All:16378?n.
?17951?p.Table?2:?Domain?camera&photos.In ?order ?
to ?evaluate ?how?well ?
the ?
results ?of ?
the?experiments ?
are ?
we ?
performed ?
several ?
more?experiments, ?
in?which?the?texts?were?represented?with?unigrams?
(1?grams) ?and?bigrams?(2?grams).
?Pang?and ?Lee ?
(2008) ?note ?
that: ?whether ?higher?order?n?grams?are?useful?features?appears?to?be?a?matter?of?some?debate.?For?example,?Pang?et?al.
?
(2002) ?report ?
that ?unigrams?outperform?bigrams?when ?
classifying ?
movie ?
reviews ?
by ?
sentiment ?polarity,?but?Dave?et?al.?
(2003)?find?that?in?some?settings, ?
bigrams ?
and ?
trigrams ?
yield ?
better ?product?review?polarity?classification.
?Bekkerman?and ?Allan ?
(2004) ?
review?
the ?
results ?
of ?
different?experiments ?
on ?
text ?
categorization ?
in ?
which ?
n?gram?approaches?were?used,?and?conclude?that?the?use?of?bigrams?for?the?representation?of?texts?does?not ?
show ?
general ?
improvement ?
(Burek ?
and?Gerdemann, ?
2009).
?
It ?
seems ?
intuitive ?
that ?
when?bigrams ?
are ?
used, ?
we ?
would ?
have ?
a ?
better?representation ?
of ?
the ?
texts, ?
because ?
we ?
would?know?what?words?combine?with?what?other?words?in?
the ?
texts.
?However, ?
there ?
is ?a ?data?sparseness?problem.???
?
It ?
seems ?
interesting ?
to ?
compare ?
the ?
results?obtained ?
by ?
representing ?
the ?
texts ?
as ?
unigrams,?bigrams, ?
and ?
distinctive ?
(maximally ?
occurring)?phrases,?because?the?model?based?on?phrases?might?use?both?unigrams?and?bigrams,?and?it?allows?also?any ?
other ?
higher ?
n?grams, ?
that ?
is, ?
more ?
context?115(and?semantics)?of?the?text?is?preserved.??
?
Tables ?
3 ?
and ?
4 ?
present ?
the ?
results ?
of ?
the?experiments?using?bag?of?tokens?(1?gram)?models,?while?Tables?5?and?6?present?the?experiments?with?the?2?gram?models.
?GATE?was?used?as?a?working?environment,?and?SVM?as?learning?algorithm.Reviews Precision Recall F?measureNegative?
0.77 0.82 0.79Positive?
0.82 0.75 0.78Overall 0.79 0.79 0.79Table?3:?Domain?books,?1?gram.Reviews Precision Recall F?measureNegative 0.86 0.84 0.85Positive 0.84 0.86 0.85Overall 0.85 0.85 0.85Table?4:?Domain?camera&photos,?1?gram.Reviews Precision Recall F?measureNegative 0.72 0.80 0.75Positive 0.78 0.69 0.73Overall 0.75 0.75 0.75Table?5:?Domain?books,?2?gram.Reviews Precision Recall F?measureNegative 0.84 0.83 0.83Positive 0.83 0.84 0.83Overall 0.83 0.83 0.83Table?6:?Domain?camera&photos,?2?gram.Features Precision Recall F?measureAll?phrases 0.81 0.81 0.811?gram 0.79 0.79 0.792?gram 0.75 0.75 0.75Table?7:?Comparison,?Domain?books.Features Precision Recall F?measureAll?phrases 0.86 0.86 0.861?gram 0.85 0.85 0.852?gram 0.83 0.83 0.83Table?8:?Comparison,?Domain?camera&photos.Tables?7?and?8?summarize?the?overall?results?using?1?gram?and?2?gram?models?and?a?model?based?on?distinctive ?
phrases ?
for ?
the ?
representation ?
of ?
the?texts.
?
For ?
both ?
domains ?
the ?
best ?
results ?
are?achieved ?
with ?
the ?
model ?
based ?
on ?
phrases ?
(all?phrases).
?
For ?
the ?
domain ?books?
the ?
overall?precision, ?
recall ?
and ?
F?measure ?
results ?
achieved?with ?
that ?
model ?
(81%) ?
are ?
2% ?
higher ?
than ?
the?results?obtained?using?the?1?gram?model,?and?6%?higher?than?the?results?obtained?using?the?2?gram?model.
?
For ?
domain ?cameras ?
& ?
photos, ?
an?improvement?of?1%?and?3%?is?achieved?with?the?phrase?model?in?comparison?with?the?1?gram?and?2?gram?models,?respectively.4 Related?WorkClose?to?our?work?seems?to?be?Funk?et?al.?(2008).
?They?classify?product ?and?company?reviews?
into?one?of?the?1?star?to?5?star?categories.?The?features?to?
the ?
learning?algorithm?
(also?SVM)?are?simple?linguistic ?
features ?
of ?
single ?
tokens.
?
They ?
report?best ?
results ?
with ?
the ?
combinations ?root ?
&?orthography, ?
and ?only ?
root.
?
Another ?
interesting?related?work?is?that?of?Turney?(2002).
?He?uses?an?unsupervised ?
learning ?
algorithm ?
to ?
classify ?
a?review?as?recommended?or?not?recommended.
?The?algorithm ?
extracts ?
phrases ?
from ?
a ?given ?
review,?and?determines?their?pointwise?mutual?information?with?the?words ?excellent?
and ?poor.?Turney?
(2002)?points ?out ?
that ?
the ?contexual ?
information?
is ?very?often?necessary?for?the?correct?determination?of?the?sentiment?polarity?of?a?certain?word.5 ConclusionThis ?
paper ?
presented ?
different ?
experiments ?
on?classifying?product?reviews?of?domains ?books?
and?cameras ?& ?photos?
under ?
the ?
categories ?positive?polarity?
and ?negative ?
polarity?
using ?
distinctive?
(maximally ?
occurring) ?
phrases ?
as ?
features.
?
For?both?domains?best?results ?were?achieved?with?all?extracted ?
distinctive ?
phrases ?
as ?
features.
?
This?approach?outperforms?slightly ?
the?1?gram?and?2?gram?experiments?on?this?data?and?shows?that?the?use?of?phrases?occurring?maximally?in?text?could?be ?
successfully ?
applied ?
in ?
the ?
classification ?
of?sentiment?data?and?that?it?is?worth?experimenting?with?classifying?sentiment?data?without?necessarily?relying?on?general?predefined?sentiment?lexicons.116ReferencesRon?Bekkerman?and?James?Allan.?2004.
?Using?bigrams?in ?
text ?
categorization.
?Technical ?
Report ?
IR?408,?Center ?
of ?
Intelligent ?
Information ?
Retrieval, ?
UMass?Amherst.?John?Blitzer,?Mark?Dredze,?and?Fernando?Pereira.?2007.
?Biographies, ?
bollywood, ?
boomboxes ?
and ?
blenders:?Domain ?
adaptation ?
for ?
sentiment ?
classification.
?
In?Proceedings ?
of ?
the ?
45th ?
Annual ?
Meeting ?
of ?
the ?Association ?
of ?
Computational ?
Linguistics, ?
pp.
?440?447, ?
Prague, ?
Czech ?
Republic.
?
Association ?
for?Computational?Linguistics.Gaston ?
Burek ?
and ?
Dale ?
Gerdemann.
?
2009. ?
Maximal?phrases ?
based ?
analysis ?
for ?
prototyping ?
online?discussion ?
forums ?
postings.
?
In ?Proceedings ?
of ?
the?workshop?on?Adaptation?of?Language?Resources?and ?Technologies ?
to ?
New ?
Domains ?
(AdaptLRTtoND),?Borovets,?Bulgaria.Kenneth ?
W. ?
Church ?
and ?
William ?
A. ?
Gale.
?
1995.?Poisson ?
mixtures.
?Natural ?
Language ?
Engineering,?1:163?190.Kushal?Dave,?Steve?Lawrence?and?David?M.?Pennock.?2003.
?Mining?the?peanut?gallery: ?Opinion?extraction?and ?
semantic ?
classification ?
of ?
product ?
reviews.
?
In?Proceedings?of?WWW,?pp.
?519?528.Adam ?
Funk, ?
Yaoyong ?
Li, ?
Horacio ?
Saggion, ?
Kalina?Bontcheva, ?
and ?
Christian ?
Leibold.
?
2008. ?
Opinion?analysis ?
for ?
business ?
intelligence ?
applications.
?
In?Proceedings ?
of ?First ?
International ?
Workshop ?
on?Ontology?supported?Business ?
Intelligence ?
(OBI2008)?at ?
the ?7th ?
International ?
Semantic ?
Web ?
Conference ?(ISWC),?Karlsruhe,?Germany.Slava?M.?Katz.?1996.?
?Distribution?of?content?words?and?phrases ?
in ?
text ?
and ?
language ?
modelling.
?Natural?Language?Engineering,?2(1):15?59.Rasmus?E.?Madsen,?David?Kauchak,?and?Charles?Elkan.?2005.
?
Modeling ?
word ?
burstiness ?
sing ?
the ?
dirichlet?distribution.
?In?Proceedings?of?the?22nd?International ?Conference?on?Machine?Learning,?pp.
?545?552.Bo?Pang?and ?Lillian ?Lee.
?2008.
?Opinion ?mining ?and?sentiment ?
analysis.
?Foundations ?
and ?
Trends ?
in ?Information?Retrieval,?Vol.?2,?Nos.?1?2?(2008)?1?135.Bo?Pang,?Lillian?Lee.,?and?Shivakumar?Vaithyanathan.?2002.
?
Thumbs ?
up?
?
Sentiment ?
classification ?
using?machine ?
learning ?
techniques.
?
In ?Proceedings ?of ?
the?Conference ?
on ?
Empirical ?
Methods ?
in ?
Natural ?Language?Processing?(EMNLP),?pp.
?79?86.Peter ?D.
?Turney.
?2002.
?Thumbs?up?or ?
thumbs?down?
?Semantic ?
orientation ?
applied ?
to ?
unsupervised?classification?of?reviews.
?In ?Proceedings?of?the?40th?Annual ?
Meeting ?
on ?
Association ?
for ?
Computational ?Linguistics,?pp.?417?424.Mikio?Yamamoto?and?Kenneth?W.?Church.?2001.
?Using?suffix ?
arrays ?
to ?
compute ?
term ?
frequency ?
and?document?frequency?for?all?substrings?in?a?corpus.?In?Computational?
?Linguistics,?27(1):1?30.117
