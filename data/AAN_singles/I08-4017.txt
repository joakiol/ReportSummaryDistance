Unsupervised Segmentation Helps Supervised Learning ofCharacter Tagging for Word Segmentation and Named Entity RecognitionHai Zhao and Chunyu KitDepartment of Chinese, Translation and LinguisticsCity University of Hong KongTat Chee Ave., Kowloon, Hong KongEmail: {haizhao, ctckit}@cityu.edu.hkAbstractThis paper describes a novel character tag-ging approach to Chinese word segmenta-tion and named entity recognition (NER) forour participation in Bakeoff-4.1 It integratesunsupervised segmentation and conditionalrandom fields (CRFs) learning successfully,using similar character tags and feature tem-plates for both word segmentation and NER.It ranks at the top in all closed tests of wordsegmentation and gives promising results forall closed and open NER tasks in the Bake-off.
Tag set selection and unsupervised seg-mentation play a critical role in this success.1 IntroductionA number of recent studies show that character se-quence labeling is a simple but effective formula-tion of Chinese word segmentation and name en-tity recognition for machine learning (Xue, 2003;Low et al, 2005; Zhao et al, 2006a; Chen et al,2006).
Character tagging becomes a prevailing tech-nique for this kind of labeling task for Chinese lan-guage processing, following the current trend of ap-plying machine learning as a core technology in thefield of natural language processing.
In particular,when a full-fledged general-purpose sequence learn-ing model such as CRFs is involved, the only workto do for a given application is to identify an idealset of features and hyperparameters for the purpose1The Fourth International Chinese Language ProcessingBakeoff & the First CIPS Chinese Language Processing Evalu-ation, at http://www.china-language.gov.cn/bakeoff08/bakeoff-08 basic.html.of achieving the best learning model that we canwith available training data.
Our work in this aspectprovides a solid foundation for applying an unsuper-vised segmentation criterion to enrich the supervisedCRFs learning for further performance enhancementon both word segmentation and NER.This paper is intended to present the research forour participation in Bakeoff-4, with a highlight onour strategy to select character tags and feature tem-plates for CRFs learning.
Particularly worth men-tioning is the simplicity of our system in contrast toits success.
The rest of the paper is organized as fol-lows.
The next section presents the technical detailsof the system and Section 3 its evaluation results.Section 4 looks into a few issues concerning charac-ter tag set, unsupervised segmentation, and availablename entities (NEs) as features for open NER test.Section 5 concludes the paper.2 System DescriptionFollowing our previous work (Zhao et al, 2006a;Zhao et al, 2006b; Zhao and Kit, 2007), we con-tinue to apply the order-1 linear chain CRFs (Laf-ferty et al, 2001) as our learning model for Bakeoff-4.
Specifically, we use its implementation CRF++by Taku Kudo2 freely available for research purpose.We opt for a similar set of character tags and featuretemplates for both word segmentation and NER.In addition, two key techniques that we have ex-plored in our previous work are applied.
One is tointroduce more tags in the hope of utilizing moreprecise contextual information to achieve more pre-2http://crfpp.sourceforge.net/106Sixth SIGHAN Workshop on Chinese Language Processing107Sixth SIGHAN Workshop on Chinese Language ProcessingTable 3: Training corpora for assistant learnersTrack CityU NER MSRA NERAss.
Seg.
CityU (Bakeoff-1 to 4) MSRA (Bakeoff-2)ANER-1 CityU(Bakeoff-3) CityU(Bakeoff-3)ANER-2 MSRA(Bakeoff-3) CityU(Bakeoff-4)Table 4: NE lists from Chinese WikipediaCategory NumberPlace name suffix 85Chinese place name 6,367Foreign place name 1,626Chinese family name 573Most common Chinese family name 109Foreign name 2,591Chinese university 515didate s with a score AV (s) is defined asfn(s) = t, if 2t ?
AV (s) < 2t+1,where t is an integer to logarithmize the score.
Thisis to alleviate the sparse data problem by narrowingdown the feature representation involved.
Note thatt is used as a feature value rather than a parameterfor the CRFs training in our system.
For an over-lap character of several word candidates, we onlychoose the one with the greatest AV score to activatethe above feature function for that character.
It isin this way that the unsupervised segmentation out-comes are fit into the CRFs learning.2.3 Features for Open NERThree extra groups of feature template are used forthe open NER beyond those for the closed.The first group includes three segmentation fea-ture templates.
One is character type feature tem-plate T (C?1)T (C0)T (C1), where T (C) is the typeof character C. For this, five character types are de-fined, namdely, number, foreign letter, punctuation,date and time, and others.
The other two are gener-ated respectively by two assistant segmenters (Zhaoet al, 2006a), a maximal matching segmenter basedon a dictionary from Peking University3 and a CRFssegmenter using the 6-tag set and the six n-gram fea-ture templates for training.3It consists of about 108K words of one to four character-slong, available at http://ccl.pku.edu.cn/doubtfire/Course/Chinese%20Information%20Processing/Source Code/Chapter 8/Lexicon full.zip.Table 5: Segmentation results for previous BakeoffsBakeoff-1 AS CityU CTB PKU?AVF .9727 .9473 .8720 .9558ROOVa .7907 .7576 .7022 .7078+AVF .9725 .9554 .9023 .9612ROOV .7597 .7616 .7502 .7208Bakeoff-2 AS CityU MSRA PKU?AVF .9534 .9476 .9735 .9515ROOV .6812 .6920 .7496 .6720+AVF .9570 .9610 .9758 .9540ROOV .6993 .7540 .7446 .6765Bakeoff-3 AS CityU CTB MSRA?AVF .9538 .9691 .9322 .9608ROOV .6699 .7815 .7095 .6658+AVF .9586 .9747 .9431 .9660ROOV .6935 .8005 .7608 .6620aRecall of out-of-vocabulary (OOV) words.The second group comes from the outputs oftwo assistant NE recognizers (ANERs), both trainedwith a corresponding 6-tag set and the same six n-gram feature templates.
They share a similar featurerepresentation as the assistant segmenter.
Table 3lists the training corpora for the assistant CRFs seg-menter and the ANERs for various open NER tests.The third group consists of feature templates gen-erated from seven NE lists acquired from ChineseWikipedia.4 The categories and numbers of theseNE items are summarized in Table 4.3 Evaluation ResultsThe performance of both word segmentation andNER is measured in terms of the F-measure F =2RP/(R + P ), where R and P are the recall andprecision of segmentation or NER.We tested the techniques described above withthe previous Bakeoffs?
data5 (Sproat and Emerson,2003; Emerson, 2005; Levow, 2006).
The evalua-tion results for the closed tests of word segmentationare reported in Table 5 and those for the NER on twocorpora of Bakeoff-3 are in the upper part of Table 7.?+/?AV?
indicates whether AV features are applied.For Bakeoff-4, we participated in all five closedtracks of word segmentation, namely, CityU, CKIP,CTB, NCC, and SXU, and in all closed and openNER tracks of CityU and MSRA.6 The evaluation4http://zh.wikipedia.org/wiki/?
?5http://www.sighan.org6We declare that our team has never been exposed to the108Sixth SIGHAN Workshop on Chinese Language ProcessingTable 6: Evaluation results of word segmentation on Bakeoff-4 data setsFeature Data F P R FIVa PIV RIV FOOV POOV ROOVCityU .9426 .9410 .9441 .9640 .9636 .9645 .7063 .6960 .7168CKIP .9421 .9387 .9454 .9607 .9581 .9633 .7113 .7013 .7216?AV CTB .9634 .9641 .9627 .9738 .9761 .9715 .7924 .7719 .8141(n-gram) NCC .9333 .9356 .9311 .9536 .9612 .9461 .5678 .5182 .6280SXU .9552 .9559 .9544 .9721 .9767 .9675 .6640 .6223 .7116CityU .9510 .9493 .9526 .9667 .9626 .9708 .7698 .7912 .7495CKIP .9470 .9440 .9501 .9623 .9577 .9669 .7524 .7649 .7404+AV*b CTB .9589 .9596 .9583 .9697 .9704 .9691 .7745 .7761 .7730NCC .9405 .9407 .9402 .9573 .9583 .9562 .6080 .5984 .6179SXU .9623 .9625 .9622 .9752 .9764 .9740 .7292 .7159 .7429aF-score for in-vocabulary (IV) words.bHenceforth the official evaluation results in Bakeoff-4 are marked with ?
*?.Table 7: NER evaluation resultsTrack Setting FPER FLOC FORG FNEBakeoff-3CityU ?AV .8849 .9219 .7905 .8807+AV .9063 .9281 .7981 .8918MSRA ?AV .7851 .9072 .8242 .8525+AV .8171 .9139 .8164 .8630Bakeoff-4?AV .8222 .8682 .6801 .8092CityU +AV* .8362 .8677 .6852 .8152Open1* .9125 .9216 .7862 .8869Open2 .9137 .9214 .7853 .8870?AV .9221 .9193 .8367 .8968+AV* .9319 .9219 .8414 .9020MSRA Open* 1.000 .9960 .9920 .9958Open1a .9710 .9601 .9352 .9558Open2b .9699 .9581 .9359 .9548aFor our official submission to Bakeoff-4, we also usedan ANER trained on the MSRA NER training corpus ofBakeoff-3.
This makes our official evaluation results ex-tremely high but trivial, for a part of this corpus is used asthe MSRA NER test corpus for Bakeoff-4.
Presented hereare the results without using this ANER.bOpen2 is the result of Open1 using no NE list feature.results of word segmentation and NER for our sys-tem are presented in Tables 6 and 7, respectively.For the purpose of comparison, the word segmen-tation performance of our system on Bakeoff-4 datausing the 2- and 4-tag sets and the best correspond-ing n-gram feature templates as in (Tsai et al, 2006;Low et al, 2005) are presented in Table 8.7 Thiscomparison reconfirms the conclusion in (Zhao etCityU data sets in any other situation than the Bakeoff.7The templates for the 2-tag set, adopted from (Tsai et al,2006), include C?2, C?1, C0, C1, C?3C?1, C?2C0, C?2C?1,C?1C0, C?1C1 and C0C1.
Those for the 4-tag set, adoptedfrom (Xue, 2003) and (Low et al, 2005), include C?2, C?1,C0, C1, C2, C?2C?1, C?1C0, C?1C1, C0C1and C1C2.al., 2006b) about tag set selection for character tag-ging for word segmentation that the 6-tag set is moreeffective than others, each with its own best corre-sponding feature template set.Table 8: Segmentation F-scores by different tag setsAV Tags CityU CKIP CTB NCC SXU2 .9303 .9277 .9434 .9198 .9454?
4 .9370 .9348 .9481 .9280 .95126 .9426 .9421 .9634 .9333 .95522 .9382 .9319 .9451 .9239 .9485+ 4 .9482 .9423 .9527 .9356 .95936 .9510 .9470 .9589 .9405 .96234 Discussion4.1 Tag Set and Computational CostUsing more labels in CRFs learning is expected tobring in performance enhancement.
Inevitably, how-ever, it also leads to a huge rise of computationalcost for model training.
We conducted a series of ex-periments to study the computational cost of CRFstraining with different tag sets using Bakeoff-3 data.The experimental results are given in Table 9, show-ing that the 6-tag set costs nearly twice as much timeas the 4-tag set and about three times as the 2-tagset.
Fortunately, its memory cost with the six n-gramfeature templates remains very close to that of the 2-and 4-tag sets with the n-gram feature template setsfrom (Tsai et al, 2006; Xue, 2003).However, a 2-tag set is popular in use for wordsegmentation and NER for the reason that CRFstraining is very computationally expensive and alarge tag set would make the situation worse.
Cer-109Sixth SIGHAN Workshop on Chinese Language ProcessingTable 9: Comparison of computational costTags Templates AS CityU CTB MSRATraining time (Minutes)2 Tsai 112 52 16 354 Xue 206 79 28 736 Zhao 402 146 47 117Feature numbers (?106)2 Tsai 13.2 7.3 3.1 5.54 Xue 16.1 9.0 3.9 6.86 Zhao 15.6 8.8 3.8 6.6Memory cost (Giga bytes)2 Tsai 5.4 2.4 0.9 1.84 Xue 6.6 2.8 1.1 2.26 Zhao 6.4 2.7 1.0 2.1tainly, a possible way out of this problem is thecomputer hardware advancement, which is predictedby Moore?s Law (Moore, 1965) to be improving atan exponential rate in general, including processingspeed and memory capacity.
Specifically, CPU canbe made twice faster every other year or even 18months.
It is predictable that computational cost willnot be a problem for CRFs training soon, and the ad-vantages of using a larger tag set as in our approachwill be shared by more others.4.2 Unsupervised Segmentation FeaturesOur evaluation results show that the unsupervisedsegmentation features bring in performance im-provement on both word segmentation and NER forall tracks except CTB segmentation, as highlightedin Table 6.
We are unable explain this yet, and canonly attribute it to some unique text characteristicsof the CTB segmented corpus.
An unsupervised seg-mentation criterion provides a kind of global infor-mation over the whole text of a corpus (Zhao andKit, 2007).
Its effectiveness is certainly sensitive totext characteristics.Quite a number of other unsupervised segmen-tation criteria are available for word discovery inunlabeled texts, e.g., boundary entropy (Tung andLee, 1994; Chang and Su, 1997; Huang and Powers,2003; Jin and Tanaka-Ishii, 2006) and description-length-gain (DLG) (Kit and Wilks, 1999).
We foundthat among them AV could help the CRFs model toachieve a better performance than others, althoughthe overall unsupervised segmentation by DLG wasslightly better than that by AV.
Combining any twoof these criteria did not give any further performanceimprovement.
This is why we have opted for AV forBakeoff-4.4.3 NE List Features for Open NERWe realize that the NE lists available to us are farfrom sufficient for coping with all NEs in Bakeoff-4.
It is reasonable that using richer external NElists gives a better NER performance in many cases(Zhang et al, 2006).
Surprisingly, however, the NElist features used in our NER do not lead to any sig-nificant performance improvement, according to theevaluation results in Table 7.
This is certainly an-other issue for our further inspection.5 ConclusionWithout doubt our achievements in Bakeoff-4 owesnot only to the careful selection of character tag setand feature templates for exerting the strength ofCRFs learning but also to the effectiveness of our un-supervised segmentation approach.
It is for the sakeof simplicity that similar sets of character tags andfeature templates are applied to two distinctive label-ing tasks, word segmentation and NER.
Relying onlittle preprocessing and postprocessing, our systemsimply follows the plain training and test routinesof machine learning practice with the CRFs modeland achieves the best or nearly the best results for alltracks of Bakeoff-4 in which we participated.
Sim-ple is beautiful, as Albert Einstein said, ?Everythingshould be made as simple as possible, but not onebit simpler.?
Our evaluation results also provide evi-dence that simple can be powerful too.AcknowledgementsThe research described in this paper was sup-ported by the Research Grants Council of HongKong S.A.R., China, through the CERG grant9040861 (CityU 1318/03H) and by City Universityof Hong Kong through the Strategic Research Grant7002037.
Dr. Hai Zhao was supported by a Post-doctoral Research Fellowship in the Department ofChinese, Translation and Linguistics, City Univer-sity of Hong Kong.ReferencesJing-Shin Chang and Keh-Yih Su.
1997.
An unsuper-vised iterative method for Chinese new lexicon ex-110Sixth SIGHAN Workshop on Chinese Language Processingtraction.
Computational Linguistics and Chinese Lan-guage Processing, 2(2):97?148.Wenliang Chen, Yujie Zhang, and Hitoshi Isahara.
2006.Chinese named entity recognition with conditionalrandom fields.
In SIGHAN-5, pages 118?121, Sydney,Australia, July 22-23.Thomas Emerson.
2005.
The second international Chi-nese word segmentation bakeoff.
In SIGHAN-4, pages123?133, Jeju Island, Korea, October 14-15.Haodi Feng, Kang Chen, Xiaotie Deng, and WeiminZheng.
2004a.
Accessor variety criteria for Chi-nese word extraction.
Computational Linguistics,30(1):75?93.Haodi Feng, Kang Chen, Chunyu Kit, and Xiaotie Deng.2004b.
Unsupervised segmentation of Chinese cor-pus using accessor variety.
In First InternationalJoint Conference on Natural Language Processing(IJCNLP-04), pages 255?261, Sanya, Hainan Island,China, March 22-24.
Also in K. Y. Su, J. Tsujii, J.H.
Lee & O. Y. Kwong (eds.
), Natural Language Pro-cessing - IJCNLP 2004, LNAI 3248, pages 694-703.Springer.Zellig Sabbetai Harris.
1955.
From phoneme to mor-pheme.
Language, 31(2):190?222.Zellig Sabbetai Harris.
1970.
Morpheme boundarieswithin words.
In Papers in Structural and Transfor-mational Linguistics, page 68?77.Jin Hu Huang and David Powers.
2003.
Chineseword segmentation based on contextual entropy.
InDong Hong Ji and Kim-Ten Lua, editors, PACLIC -17, pages 152?158, Sentosa, Singapore, October, 1-3.COLIPS Publication.Zhihui Jin and Kumiko Tanaka-Ishii.
2006.
Unsuper-vised segmentation of Chinese text by use of branch-ing entropy.
In COLING/ACL?2006, pages 428?435,Sidney, Australia, July 17-21.Chunyu Kit and Yorick Wilks.
1998.
The virtual corpusapproach to deriving n-gram statistics from large scalecorpora.
In Changning Huang, editor, Proceedings of1998 International Conference on Chinese Informa-tion Processing Conference, pages 223?229, Beijing,Nov.
18-20.Chunyu Kit and Yorick Wilks.
1999.
Unsupervisedlearning of word boundary with description lengthgain.
In M. Osborne and E. T. K. Sang, editors,CoNLL-99, pages 1?6, Bergen, Norway.Chunyu Kit and Hai Zhao.
2007.
Improving Chi-nese word segmentation with description length gain.In 2007 International Conference on Artificial Intelli-gence (ICAI?07), Las Vegas, June 25-28.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In ICML?2001, pages 282?289, San Francisco, CA.Gina-Anne Levow.
2006.
The third international Chi-nese language processing bakeoff: Word segmentationand named entity recognition.
In SIGHAN-5, pages108?117, Sydney, Australia, July 22-23.Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo.
2005.A maximum entropy approach to Chinese word seg-mentation.
In SIGHAN-4, pages 161?164, Jeju Island,Korea, October 14-15.Udi Manber and Gene Myers.
1993.
Suffix arrays: Anew method for on-line string searches.
SIAM Journalon Computing, 22(5):935?948.Gordon E. Moore.
1965.
Cramming more componentsonto integrated circuits.
Electronics, 3(8), April 19.Richard Sproat and Thomas Emerson.
2003.
The firstinternational Chinese word segmentation bakeoff.
InSIGHAN-2, pages 133?143, Sapporo, Japan.Richard Tzong-Han Tsai, Hsieh-Chuan Hung, Cheng-Lung Sung, Hong-Jie Dai, and Wen-Lian Hsu.
2006.On closed task of Chinese word segmentation: An im-proved CRF model coupled with character clusteringand automatically generated template matching.
InSIGHAN-5, pages 108?117, Sydney, Australia, July22-23.Cheng-Huang Tung and His-Jian Lee.
1994.
Iden-tification of unknown words from corpus.
Compu-tational Proceedings of Chinese and Oriental Lan-guages, 8:131?145.Nianwen Xue.
2003.
Chinese word segmentation ascharacter tagging.
Computational Linguistics andChinese Language Processing, 8(1):29?48.Suxiang Zhang, Ying Qin, Juan Wen, and Xiaojie Wang.2006.
Word segmentation and named entity recog-nition for SIGHAN Bakeoff3.
In SIGHAN-5, pages158?161, Sydney, Australia, July 22-23.Hai Zhao and Chunyu Kit.
2007.
Incorporating globalinformation into supervised learning for Chinese wordsegmentation.
In PACLING-2007, pages 66?74, Mel-bourne, Australia, September 19-21.Hai Zhao, Chang-Ning Huang, and Mu Li.
2006a.An improved Chinese word segmentation system withconditional random field.
In SIGHAN-5, pages 162?165, Sydney, Australia, July 22-23.Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-LiangLu.
2006b.
Effective tag set selection in Chinese wordsegmentation via conditional random field modeling.In PACLIC-20, pages 87?94, Wuhan, China, Novem-ber 1-3.111Sixth SIGHAN Workshop on Chinese Language Processing
