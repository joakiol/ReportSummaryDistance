Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1860?1870,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsA Graph Degeneracy-based Approach to Keyword Extraction ?Antoine J.-P. Tixier1, Fragkiskos D. Malliaros1,2, Michalis Vazirgiannis11Computer Science Laboratory, E?cole Polytechnique, Palaiseau, France2Department of Computer Science and Engineering, UC San Diego, La Jolla, CA, USA{anti5662,fmalliaros,mvazirg}@lix.polytechnique.frAbstractWe operate a change of paradigm and hy-pothesize that keywords are more likely to befound among influential nodes of a graph-of-words rather than among its nodes high oneigenvector-related centrality measures.
Totest this hypothesis, we introduce unsuper-vised techniques that capitalize on graph de-generacy.
Our methods strongly and sig-nificantly outperform all baselines on twodatasets (short and medium size documents),and reach best performance on the third one(long documents).1 IntroductionKeyword extraction is a central task in NLP.
It findsapplications from information retrieval (notably websearch) to text classification, summarization, and vi-sualization.
In this study, we focus on the task ofunsupervised single-document keyword extraction.Following (Mihalcea and Tarau, 2004), we concen-trate on keywords only, letting the task of keyphrasereconstruction as a post-processing step.More precisely, while we capitalize on a graphrepresentation of text like several previous ap-proaches, we deviate from them by making the as-sumption that keywords are not found among pres-tigious nodes (or more generally, nodes high oneigenvector-related centrality metrics), but ratheramong influential nodes.
Those nodes may not havemany important connections (like their prestigiouscounterparts), but they are ideally placed at the core?This research is supported in part by the OpenPaaS::NGproject.of the network.
In other words, this switches theobjective from capturing the quality and quantity ofsingle node connections, to taking into account thedensity and cohesiveness of groups of nodes.
To op-erate this change of paradigm, we propose severalalgorithms that leverage the concept of graph degen-eracy (Malliaros et al, 2016a).Our contributions are threefold: (1) we proposenew unsupervised keyword extraction techniquesthat reach state-of-the art performance, (2) we ap-ply the K-truss algorithm to the task of keyword ex-traction for the first time, and (3) we report new in-sights on the interplay between window size, graph-of-words structure, and performance.2 Graph-of-Words RepresentationMany ways of encoding text as a graph have beenexplored in order to escape the very limiting term-independence assumption made by the traditionalvector space model.
In this study, we adopt the sem-inal Graph-of-Words representation (GoW) of (Mi-halcea and Tarau, 2004), for its simplicity, high his-torical success, and above all because it was recentlyused in several approaches that reached very goodperformance on various tasks such as informationretrieval (Rousseau and Vazirgiannis, 2013), doc-ument classification (Malliaros and Skianis, 2015;Rousseau et al, 2015), event detection (Meladianoset al, 2015), and keyword extraction (Rousseau andVazirgiannis, 2015).While sophisticated variants of the GoW modelwould be worth exploring (edge weights based onmutual information or word embeddings, adaptivewindow size, etc.
), we aim here at making a bet-1860llllllllllllllmathemataspectcomputer?aidsharetradeproblemstatistanalysipriceprobabilistcharacteristserimethodmodelEdge weights12345Core numbers891011lllinfshellsdifferenceinshellsizes?6?4?202(10 ?
11) (9 ?
10) (8 ?
9)llll0.380.420.460.50 densk?coresdensity11 10 9 8ll l ll lll l llll l2 4 6 8 10 12 147090110130TRnodesTRscoreselbowtop 33%Mathematical aspects of computer-aided sharetrading.
We consider problems of statisticalanalysis of share prices and propose probabilisticcharacteristics to describe the price series.
Wediscuss three methods of mathematical modellingof price series with given probabilisticcharacteristics.Core numbers TR scoresP R F1 P R F1MAIN 0.86 0.55 0.67 ELB 1 0.18 0.31INF 0.83 0.91 0.87PER 1 0.45 0.63DENS 0.88 0.64 0.74mathemat 11 price .1359price 11 share .0948probabilist 11 .0906characterist 11 .0870seri 11 .0860method 11 mathemat .0812model 11 analysi .0633share 10 statist .0595trade 9 method .0569problem 9 problem .0560statist 9 trade .0525analysi 9 model .0493aspect 8 computer-aid .0453computer-aid 8 aspect .0417MAININFDENSELBprobabilistcharacteristseriCR scoresP R F1ELB 0.90 0.82 0.86PER 1 0.45 0.63mathemat 128price 120analysi 119share 118probabilist 112characterist 112statist 108trade 97problem 97seri 94method 85computer-aid 76model 66aspect 65PERELBPER(a)(c)(b)ll l l lll l l l l l l l2 4 6 8 10 12 140.040.080.12CRelbowtop 33%nodesCRscoresFigure 1: (a) Graph-of-words (W = 8) for document #1512 of Hulth2003 decomposed with k-core (non-(nouns andadjectives) in italic).
(b) Keywords extracted by each proposed and baseline method (human assigned keywords inbold).
(c) Selection criterion of each method except main (does not apply).ter use of an existing representation of text, not atproposing a new one.
This is why, to demonstratethe additional skill brought by our proposed meth-ods, we stick to the basic GoW framework.As shown in Figure 1 (a), the GoW representationof (Mihalcea and Tarau, 2004) encodes a piece oftext as an undirected graph where nodes are uniquenouns and adjectives in the document, and wherethere is an edge between two nodes if the terms theyrepresent co-occur within a window of predeter-mined size W that is slided over the entire documentfrom start to finish, overspanning sentences.
Fur-thermore, edges are assigned integer weights match-ing co-occurrence counts.
This fully statistical ap-proach is based on the Distributional Hypothesis(Harris, 1954), that is, on the premise that the re-lationship between words can be determined by thefrequency with which they share local contexts ofoccurrence.3 Graph DegeneracyThe concept of graph degeneracy was introducedby (Seidman, 1983) with the k-core decompositiontechnique and was first applied to the study of cohe-sion in social networks.
Here, we consider it as anumbrella term also encompassing the K-truss algo-rithm (Cohen, 2008).
In what follows, G(V,E) is agraph with |V | nodes and |E| edges.
Note that forgraphs-of-words, the nodes V are labeled accordingto the unique terms they represent.3.1 k-core subgraphA core of order k (or k-core) of G is a maximal con-nected subgraph of G in which every vertex v hasat least degree k (Seidman, 1983).
If the edges areunweighted, the degree of v is simply equal to thecount of its neighbors, while in the weighted case,the degree of v is the sum of the weights of its in-cident edges.
Note that with the definition of GoWpreviously given, node degrees (and thus, k) are in-tegers in both cases since edge weights are integers.1861As shown in Figure 2 (a), the k-core decomposi-tion of G is the set of all its cores from 0 (G itself) tokmax (its main core).
It forms a hierarchy of nestedsubgraphs whose cohesiveness and size respectivelyincrease and decrease with k (Seidman, 1983).
Themain core of G is a coarse approximation of its dens-est subgraph (Wang and Cheng, 2012), and shouldbe seen as a seedbed within which it is possible tofind more cohesive subgraphs (Seidman, 1983).
Fi-nally, the core number of a node is the highest orderof a k-core subgraph that contains this node.3.2 K-truss subgraphK-truss is a triangle-based extension of k-core in-troduced by (Cohen, 2008).
More precisely, the K-truss subgraph of G is its largest subgraph where ev-ery edge belongs to at least K?2 triangles.
In otherwords, every edge in the K-truss joins two verticesthat have at least K ?
2 common neighbors.
Com-pared to k-core, K-truss thus does not iterativelyprune nodes out based on the number of their directlinks, but also based on the number of their sharedconnections.
This more accurately captures cohe-siveness.As a result, the K-trusses are smaller and densersubgraphs than the k-cores, and the maximal K-truss of G better approximates its densest subgraph.In essence, and as illustrated in Figure 2 (b), theK-trusses can be viewed as the essential parts ofthe k-cores, i.e., what is left after the less cohesiveelements have been filtered out (Wang and Cheng,2012).
By analogy with k-core, the K-truss de-composition of G is the set of all its K-trusses fromK = 2 to Kmax, and the truss number of an edgeis the highest order of a truss the edge belongs to.By extension, we define the truss number of a nodeas the maximum truss number of its incident edges.3.3 k-shellDepending on context, we will refer to the k-shell asthe part of the k-core (or truss) that is not includedin the (k + 1)-core (or truss).3.4 Graph degeneracy and spreading influenceIn social networks, it has been shown that the bestspreaders (i.e., the nodes able to propagate informa-tion to a large portion of the network at minimal timeand cost) are not necessarily the highly connectedindividuals (i.e., the hubs), but rather those locatedat the core of the network (i.e., forming dense andcohesive subgraphs with other central nodes), as in-dicated by graph degeneracy algorithms (Kitsak etal., 2010).
Put differently, the spreading influenceof a node is related to its structural position withinthe graph (density and cohesiveness) rather than toits prestige (random walk-based degree).
More re-cently, (Malliaros et al, 2016b) found that the trussnumber is an even better indicator of spreading in-fluence than the core number.
Motivated by thesefindings, we posit that taking cohesiveness into ac-count with the core and truss decomposition of agraph-of-words could improve keyword extractionperformance.
That way, by analogy with the notionof influential spreaders in social networks, we hy-pothesize that influential words in graphs-of-wordswill act as representative keywords.3.5 Complexity(Batagelj and Zavers?nik, 2002) proposed O(|V | +|E|) and O(|E| log |V |) time algorithms for k-coredecomposition in the unweighted (resp.
weighted)case.
These algorithms are both O(|V |) in space.Computing the K-truss decomposition is more ex-pensive, and requires O(|E|1.5) time and O(|V | +|E|) space (Wang and Cheng, 2012).
Finally, build-ing a graph-of-words is linear in time and space:O(|V |W ) and O(|V |+ |E|), respectively.4 Related Work and Point of DepartureTextRank.
One of the most popular approaches tothe task of unsupervised single-document keywordextraction is TextRank (Mihalcea and Tarau, 2004),or TR in what follows.
In TR, the nodes of graphs-of-words are ranked based on a modified version ofthe PageRank algorithm taking edge weights into ac-count, and the top p% vertices are kept as keywords.Limitations.
TR has proven successful and hasbeen widely used and adapted.
However, PageR-ank, which is based on the concept of random walksand is also related to eigenvector centrality, tendsto favor nodes with many important connectionsregardless of any cohesiveness consideration.
Forundirected graphs, it was even shown that PageR-ank values are proportional to node degrees (Grol-musz, 2015).
While well suited to the task of18623-core2-core1-coreCore number c = 1 Core number c = 2 Core number c = 3Main k-core subgraphMain K-truss subgrapha) b)***Figure 2: (a) k-core decomposition illustrative example.
Note that while nodes * and ** have same degree (3), node** makes a more influential spreader as it lies in a higher core than node *.
(b) k-core versus K-truss.
The mainK-truss subgraph can be considered as the core of the main core.prestige-based ranking in the Web and social net-works (among other things), PageRank may thusnot be ideal for keyword extraction.
Indeed, a fun-damental difference when dealing with text is theparamount importance of cohesiveness: keywordsneed not only to have many important connectionsbut also to form dense substructures with these con-nections.
Actually, most of the time, keywords aren-grams (Rousseau and Vazirgiannis, 2015).
There-fore, we hypothesize that keywords are more likelyto be found among the influential spreaders of agraph-of-words ?
as extracted by degeneracy-basedmethods ?
rather than among the nodes high oneigenvector-related centrality measures.Topical vs. network coherence.
Note that, un-like a body of work that tackled the task of key-word extraction and document summarization froma topical coherence perspective (Celikyilmaz andHakkani-Tu?r, 2011; Chen et al, 2012; Christensenet al, 2013), we deal here with network coherence,a purely graph theoretic notion orthogonal to topicalcoherence.Graph degeneracy.
The aforementioned limita-tion of TR motivated the use of graph degeneracy tonot only extract central nodes, but also nodes form-ing dense subgraphs.
More precisely, (Rousseau andVazirgiannis, 2015) applied both unweighted andweighted k-core decomposition on graphs-of-wordsand retained the members of the main cores as key-words.
Best results were obtained in the weightedcase, with small main cores yielding good precisionbut low recall, and significantly outperforming TR.As expected, (Rousseau and Vazirgiannis, 2015) ob-served that cores exhibited the desirable property ofcontaining ?long-distance n-grams?.In addition to superior quantitative performance,another advantage of degeneracy-based techniques(compared to TR, which extracts a constant percent-age of nodes) is adaptability.
Indeed, the size ofthe main core (and more generally, of every level inthe hierarchy) depends on the structure of the graph-of-words, which by nature is uniquely tailored tothe document at hand.
Consequently, the distribu-tion of the number of extracted keywords matchesmore closely that of the human assigned keywords(Rousseau and Vazirgiannis, 2015).Limitations.
Nevertheless, while (Rousseau andVazirgiannis, 2015) made great strides, it suffers thefollowing limitations: (1) k-core is good but notbest in capturing cohesiveness; (2) retaining onlythe main core (or truss) is suboptimal, as one cannotexpect all the gold standard keywords to be foundwithin a unique subgraph ?
actually, many valu-able keywords live in lower levels of the hierarchy(see Figure 1); and (3) the coarseness of the k-core1863decomposition implies to work at a high granular-ity level (selecting or discarding a large group ofwords at a time), which diminishes the flexibility ofthe extraction process and negatively impacts perfor-mance.Research objectives.
To address the aforemen-tioned limitations, we investigate in this study (1)the use of K-truss to get a finer-grained hierarchy ofmore cohesive subgraphs, in order to filter unwantedwords out of the upper levels and improve flexibil-ity; (2) the automated selection of the best level inthe core (or truss) hierarchy to increase recall whilepreserving most of the precision; and (3) the conver-sion of node core (or truss) numbers into ranks, todecrease granularity from the subgraph to the nodelevel, while still leveraging the valuable cohesive-ness information captured by degeneracy.5 Proposed MethodsIn what follows, we introduce the strategies we de-vised to implement our research objectives.5.1 DensityWith the underlying assumption that keywords arefound in cohesive subgraphs-of-words and are notall contained in the main core (or truss), an intu-itive, straightforward stopping criterion when goingdown the core (or truss) hierarchy is a density-basedone.
More precisely, it may be advantageous to godown the hierarchy as long as the desirable cohesive-ness properties of the main core or truss are main-tained, and to stop when these properties are lost.This strategy is more formally detailed in Algorithm1, where G(V,E) is a graph-of-words, levels corre-sponds to the vector of the nlevels unique k-core (ortruss) numbers of V sorted in decreasing order, andthe density of G is defined as:density(G) = |E||V | (|V | ?
1) (1)As can be seen in Figure 1 (c) and as detailed inAlgorithm 2, the elbow is determined as the mostdistant point from the line joining the first and lastpoint of the curve.
When all points are aligned, thetop level is retained (i.e., main core or truss).
Whenthere are only two levels, the one giving the highestdensity is returned.Algorithm 1: dens methodInput : core (or truss) decomposition of GOutput: set of keywords1 D?
empty vector of length nlevels2 for n?
1 to nlevels do3 D[n]?
density(levels[n]-core (or truss))4 end5 kbest ?
levels[elbow(n,D[n])]6 return kbest-core (or truss) of G as keywordsAlgorithm 2: elbowInput : set of |S| ?
2 pointsS ={(x0, y0), ..., (x|S|, y|S|)}Output: xelbow1 line?
{(x0, y0); (x|S|, y|S|)}2 if |S| > 2 then3 distance?
empty vector of length |S|4 s?
15 for (x, y) in S do6 distance[s]?
distance from (x, y) toline7 s?
s+ 18 end9 if ?
!s | distance[s] = max(distance) then10 xelbow ?
x | (x, y) is most distant fromline11 else12 xelbow ?
x013 end14 else15 xelbow ?
x | y is maximum16 end17 return xelbow5.2 InflexionThe Inflexion method (inf in what follows) is anempirically-grounded heuristic that relies on detect-ing changes in the variation of shell sizes (where sizedenotes the number of nodes).
Recall from subsec-tion 3.3 than the k-shell is the part of the k-core(or truss) that does not survive in the (k + 1)-core(or truss), that is, the subgraph of G induced by thenodes with core (or truss) number exactly equal tok.
In simple terms, the inf rule-of-thumb consistsin going down the hierarchy as long as the shells in-1864crease in size, and stopping otherwise.
More pre-cisely, inf is implemented as shown in Algorithm3, by computing the consecutive differences in sizeacross the shells and selecting the first positive pointbefore the drop into the negative half (see Figure 1c).If no point satisfies this requirement, the main core(or truss) is extracted.Algorithm 3: inf methodInput : core (or truss) decomposition of GOutput: set of keywords1 CD?
empty vector of length n?
12 for n?
1 to (nlevels ?
1) do3 kl ?
levels[n+ 1]; ku ?
levels[n]4 CD[n]?size(kl?shell)?size(ku?shell)5 end6 if ?n | (CD[n+ 1] < 0 ?
CD[n] > 0) then7 nbest ?
n8 else9 nbest ?
110 end11 kbest ?
levels[nbest]12 return kbest-core (or truss) as keywordsNote that both dens and inf enjoy the sameadaptability as the main core retention method of(Rousseau and Vazirgiannis, 2015) explained in Sec-tion 4, since the sizes of all the subgraphs in the hi-erarchy suit the structure of the graph-of-words.5.3 CoreRankTechniques based on retaining the kbest-core (ortruss), such as dens and inf previously described, arebetter than retaining only the top level but lack flex-ibility, in that they can only select an entire batchof nodes at a time.
This is suboptimal, because ofcourse not all the nodes in a given group are equallygood.
To address this issue, our proposed CoreRankmethod (CR in what follows) converts nodes core(or truss) numbers into scores, ranks nodes in de-creasing order of their scores, and selects the top p%nodes (CRP) or the nodes before the elbow in thescores curve (CRE).
Note that for simplicity, we stillrefer to the technique as CR even when dealing withtruss numbers.Flexibility is obviously improved by decreasinggranularity from the subgraph level to the nodelevel.
However, to avoid going back to the lim-itations of TR (absence of cohesiveness consider-ations), it is crucial to decrease granularity whileretaining as much of the desirable information en-coded by degeneracy as possible.
To accomplish thistask, we followed (Bae and Kim, 2014) and assignedto each node the sum of the core (or truss) numbersof its neighbors.Our CRE method is outlined in Algorithm 4,where N(v) denotes the set of neighbors of vertexv, and number(v) is the core (or truss) number ofv.
CRP implements the exact same strategy, the onlydifference being nbest ?
round(|V | ?
percentage)at step 8 (where percentage is a real number be-tween 0 and 1).Algorithm 4: CRE methodInput : core (or truss) decomposition of GOutput: set of keywords1 CR?
empty vector of length |V |2 for n?
1 to |V | do3 v ?
V [n]4 CR[n]?
?u?N(v) number(u)5 name(CR[n])?
label(v)6 end7 sort CR in decreasing order8 nbest ?
elbow(n,CR[n])9 return names(CR[1 : nbest]) as keywords6 Experimental Setup6.1 Baseline methodsTextRank (TR).
We used as our first benchmark thesystem of (Mihalcea and Tarau, 2004) discussed inSection 4.
For the sake of fair comparison with ourCRE and CRP methods, we considered two variantsof TR that respectively retain nodes based on boththe elbow (TRE) and percentage criteria (TRP).Main.
Our second baseline is the main core re-tention technique of (Rousseau and Vazirgiannis,2015), also described in Section 4.
This method isreferred to as main in the remainder of this paper.6.2 DatasetsTo evaluate performance, we used three standard,publicly available datasets featuring documents of1865various types and sizes.
Figure 3 shows the distribu-tions of document size and manually assigned key-words for each dataset.The Hulth20031 (Hulth, 2003) dataset containsabstracts drawn from the Inspec database of physicsand engineering papers.
Following our baselines, weused the 500 documents in the validation set and the?uncontrolled?
keywords assigned by human anno-tators.
The mean document size is 120 words andon average, 21 keywords (in terms of unigrams) areavailable for each document.We also used the training set of Marujo20121,containing 450 web news stories of about 440 wordson average, covering 10 different topics from art andculture to business, sport, and technology (Marujo etal., 2012).
For each story, the keyphrases assignedby at least 9 out of 10 Amazon Mechanical Turk-ers are provided as gold standard.
After splitting thekeyphrases into unigrams, this makes for an aver-age of 68 keywords per document, which is muchhigher than for the two other datasets, even the onecomprising long documents (Semeval, see next).The Semeval2 dataset (Kim et al, 2010) offersparsed scientific papers collected from the ACMDigital Library.
More precisely, we used the 100articles in the test set and the corresponding author-and-reader-assigned keyphrases.
Each document isapproximately 1,860 words in length and is associ-ated with about 24 keywords.Notes.
In Marujo2012, the keywords were as-signed in an extractive manner, but many of themare verbs.
In the two other datasets, keywords werefreely chosen by human coders in an abstractive wayand as such, some of them are not present in the orig-inal text.
On these datasets, reaching perfect recall istherefore impossible for our methods (and the base-lines), which by definition all are extractive.6.3 ImplementationBefore constructing the graphs-of-words and pass-ing them to the keyword extraction methods, we per-formed the following pre-processing steps:Stopwords removal.
Stopwords from the1https://github.com/snkim/AutomaticKeyphraseExtraction2https://github.com/boudinfl/centrality_measures_ijcnlp13/tree/master/data050015002500050015002500document size (in words)020406080100140020406080100140number of manuallyassigned keywordsHulth2003 SemevalMarujo2012Hulth2003 SemevalMarujo2012Figure 3: Basic dataset statistics.SMART information retrieval system3 were dis-carded.Part-of-Speech tagging and screening using theopenNLP (Hornik, 2015) R (R Core Team, 2015)implementation of the Apache OpenNLP MaxentPOS tagger.
Then, following (Mihalcea and Tarau,2004), only nouns and adjectives were kept.
ForMarujo2012, as many gold standard keywords areverbs, this step was skipped (note that we did exper-iment with and without POS-based screening but gotbetter results in the second case).Stemming with the R SnowballC package(Bouchet-Valat, 2014) (Porter?s stemmer).
Goldstandard keywords were also stemmed.After pre-processing, graphs-of-words (as de-scribed in Section 2) were constructed for each doc-ument and various window sizes (from 3, increasingby 1, until a plateau in scores was reached).
We usedthe R igraph package (Csardi and Nepusz, 2006)to write graph building and weighted k-core imple-mentation code.
For K-truss, we used the C++ im-plementation offered by (Wang and Cheng, 2012).Finally, for TRP and CRP, we retained thetop 33% keywords on Hulth2003 and Marujo2012(short and medium size documents), whereas on Se-meval (long documents), we retained the top 15 key-words.
This is consistent with our baselines.
In-deed, the number of manually assigned keywords in-creases with document size up to a certain point, andstabilizes afterwards.The code of the implementation and the datasetscan be found here4.3http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list/english.stop4https://github.com/Tixierae/EMNLP_201618665101520253 4 5 6 7 8 9 10 12 144681216Hulth 200313111113CorenumberTrussnumberlllllllllllllllllllllllllllllllllllllllllllllll0500010000200003 4 5 6 7 8 9 10 11 12 13 14Figure 4: Triangle count versus window size(Hulth2003).7 Experimental Results7.1 EvaluationWe computed the standard precision, recall, and F-1measures for each document and averaged them atthe dataset level (macro-averaging).7.2 Precision/Recall trade-offAs shown in Figure 5, our methods dens and infoutperform the baselines by a wide margin on thedatasets containing small and medium size docu-ments (Hulth2003 and Marujo2012).
As expected,this superiority is gained from a drastic improve-ment in recall, for a comparatively lower precisionloss.
TR and main exhibit higher precision thanrecall, which is in accordance with (Rousseau andVazirgiannis, 2015).
The same observation can bemade for our CR method.
For TR, the unbalanceis more severe on the Hulth2003 and Marujo2012datasets (short/medium documents) when the elbowmethod is used (TRE), probably because it tends toretain only a few nodes.
However, on Semeval (longdocuments), using the elbow method (TRE) givesthe best trade-off between precision and recall.
ForCR, still on Semeval, using the elbow method (CRE)even gives better recall than precision.Overall, compared to the baselines, the unbalancebetween precision and recall for our methods is lessextreme or equivalent.
On the Marujo2012 datasetfor instance, our proposed inf method is almost per-fectly balanced and ranks second (significantly bet-ter than all baselines).7.3 Impact of window sizeThe performance of k-core does not dramaticallyincrease with window size, while K-truss exhibitsprecision recall F1-scoredens 48.79 72.78 56.09*inf 48.96 72.19 55.98*CRP 61.53 38.73 45.75CRE 65.33 37.90 44.11main?
51.95 54.99 50.49TRP?
65.43 41.37 48.79TRE?
71.34 36.44 45.77Table 1: Hulth2003, K-truss, W = 11.
*statistical significance at p < 0.001 with respect to all baselines.
?baseline systems.precision recall F1-scoredens 47.62 71.46 52.94*inf 53.88 57.54 49.10*CRP 54.88 36.01 40.75CRE 63.17 25.77 34.41main?
64.05 34.02 36.44TRP?
55.96 36.48 41.44TRE?
65.50 21.32 30.68Table 2: Marujo2012, k-core, W = 13.
*statistical significance at p < 0.001 with respect to all baselines.
?baseline systems.a surprising ?cold start?
behavior and only beginsto kick-in for sizes greater than 4-5.
A possibleexplanation is that the ability of K-truss (whichis triangle-based) to extract meaningful informationfrom a graph depends, up to a certain point, onthe amount of triangles in the graph.
In the caseof graph-of-words, the number of triangles is pos-itively correlated with window size (see Figure 4).It also appears that document size (i.e., graph-of-words structure) is responsible for a lot of perfor-mance variability.
Specifically, on longer docu-ments, performance plateaus at higher window sizes.7.4 Best models comparisonFor each dataset, we retained the degeneracy tech-nique and window size giving the absolute best per-formance in F1-score, and compared all methods un-der these settings (see Tables 1?3).
We tested forstatistical significance in macro-averaged F1 scoresusing the non-parametric version of the t-test, theMann-Whitney U test5.On Hulth2003 and Marujo2012 (short andmedium size documents), our methods dens and infstrongly and significantly outperform all baselines,with respective absolute improvements of more than5.5% with respect to the best performing baseline5https://stat.ethz.ch/R-manual/R-devel/library/stats/html/wilcox.test.html18674 6 8 10 12 140.450.500.550.600.650.700.75PRECISIONl l l l lll l l l l l4 6 8 10 12 140.30.40.50.60.70.8l l l l l l l l l l l l4 6 8 10 12 140.400.450.500.550.60window sizel ll l l l l l l l l lk?trussHulth 2003l l l l l l l l l l l ll l l l l l l l l l l ll l l l l l l l l l l l0.400.450.500.550.604 6 8 10 12 14window size4 6 8 10 12 144 6 8 10 12 140.450.500.550.600.650.700.750.30.40.50.60.70.8k?core5(&$//)6&25(5 10 15 200.450.500.550.600.650.70l l ll l l l l l l l l l l l l l l5 10 15 200.20.30.40.50.60.7l l l l l l l l l l l l l l l l l l5 10 15 200.250.300.350.400.450.500.55l l l l l l l l ll l l l l l l l l5 10 15 200.450.500.550.600.650.70l l ll l l l l l l l l l l l l l l5 10 15 200.20.30.40.50.60.7l l l l l l l l l l l l l l l l l l5 10 15 200.250.300.350.400.450.500.55l l l l l l l l ll l l l l l l l lk?trussMarujo 2012k?corewindow sizewindow sizeTRP l TRE maininf CRP CREdensSemeval k?trussk?core5 10 15 200.10.20.30.40.50.6l l l l l l l l l l l l l l l l l l5 10 15 200.20.30.40.50.60.70.8l l l l l l l l l l l l l l l l l l5 10 15 200.150.200.250.300.350.40l l l l l l l l l l l l l l l l l l5 10 15 200.10.20.30.40.50.6l l l l l l l l l l l l l l l l l l5 10 15 200.20.30.40.50.60.70.8l l l l l l l l l l l l l l l l l l5 10 15 200.150.200.250.300.350.40l l l l l l l l l l l l l l l l l lwindow sizewindow sizeFigure 5: Impact of window size on performance.precision recall F1-scoredens 8.44 79.45 15.06inf 17.70 65.53 26.68CRP 49.67 32.88 38.98*CRE 25.82 58.80 34.86main?
25.73 49.61 32.83TRP?
47.93 31.74 37.64TRE?
33.87 46.08 37.55Table 3: Semeval, K-truss, W = 20.
*statistical significance at p < 0.001 w.r.t.
main.
?baseline systems.(main).
On Semeval, which features larger piecesof text, our CRP technique improves on TRP, thebest performing baseline, by more than 1 %, altoughthe difference is not statistically significant.
How-ever, CRP is head and shoulders above main, with anabsolute gain of 6%.
This suggests that convertingthe cohesiveness information captured by degener-acy into ranks may be valuable for large documents.Finally, the poor performance of the dens and infmethods on Semeval (Table 3) might be explainedby the fact that these methods are only capable ofselecting an entire batch of nodes (i.e., subgraph-of-words) at a time.
This lack of flexibility seemsto become a handicap on long documents for whichthe graphs-of-words, and thus the subgraphs corre-sponding to the k-core (or truss) hierarchy levels, arevery large.
This analysis is consistent with the ob-servation that conversely, approaches that work at afiner granularity level (node level) prove superior onlong documents, such as our proposed CRP methodwhich reaches best performance on Semeval.8 Conclusion and Future WorkOur results provide empirical evidence that spread-ing influence may be a better ?keywordness?
met-ric than eigenvector (or random walk)-based crite-ria.
Our CRP method is currently very basic andleveraging edge weights/direction or combining itwith other scores could yield better results.
Also,more meaningful edge weights could be obtainedby merging local co-occurrence statistics with exter-nal semantic knowledge offered by pre-trained wordembeddings (Wang et al, 2014).
The direct use ofdensity-based objective functions could also provevaluable.1868ReferencesJoonhyun Bae and Sangwook Kim.
2014.
Identifyingand ranking influential spreaders in complex networksby neighborhood coreness.
Physica A: Statistical Me-chanics and its Applications, 395:549?559.Vladimir Batagelj and Matjaz?
Zavers?nik.
2002.
General-ized cores.
arXiv preprint cs/0202039.Milan Bouchet-Valat, 2014.
SnowballC: Snowball stem-mers based on the C libstemmer UTF-8 library.
Rpackage version 0.5.1.Asli Celikyilmaz and Dilek Hakkani-Tu?r.
2011.
Discov-ery of topically coherent sentences for extractive sum-marization.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies-Volume 1, pages 491?499.
Association for Computational Linguistics.Yun-Nung Chen, Yu Huang, Hung-Yi Lee, and Lin-ShanLee.
2012.
Unsupervised two-stage keyword extrac-tion from spoken documents by topic coherence andsupport vector machine.
In 2012 IEEE InternationalConference on Acoustics, Speech and Signal Process-ing (ICASSP), pages 5041?5044.
IEEE.Janara Christensen, Stephen Soderland Mausam, StephenSoderland, and Oren Etzioni.
2013.
Towards coher-ent multi-document summarization.
In HLT-NAACL,pages 1163?1173.
Citeseer.Jonathan Cohen.
2008.
Trusses: Cohesive subgraphsfor social network analysis.
National Security AgencyTechnical Report, page 16.Gabor Csardi and Tamas Nepusz.
2006.
The igraph soft-ware package for complex network research.
Inter-Journal, Complex Systems:1695.Vince Grolmusz.
2015.
A note on the pagerank ofundirected graphs.
Information Processing Letters,115(6):633?634.Zellig S Harris.
1954.
Distributional structure.
Word,10(2-3):146?162.Kurt Hornik, 2015. openNLP: Apache OpenNLP ToolsInterface.
R package version 0.2-5.Anette Hulth.
2003.
Improved automatic keyword ex-traction given more linguistic knowledge.
In Proceed-ings of the 2003 Conference on Empirical Methods inNatural Language Processing (EMNLP), pages 216?223.
Association for Computational Linguistics.Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timo-thy Baldwin.
2010.
Semeval-2010 task 5: Automatickeyphrase extraction from scientific articles.
In Pro-ceedings of the 5th International Workshop on Seman-tic Evaluation, pages 21?26.
Association for Compu-tational Linguistics.Maksim Kitsak, Lazaros K Gallos, Shlomo Havlin,Fredrik Liljeros, Lev Muchnik, H Eugene Stanley, andHerna?n A Makse.
2010.
Identification of influen-tial spreaders in complex networks.
Nature Physics,6(11):888?893.Fragkiskos D Malliaros and Konstantinos Skianis.
2015.Graph-based term weighting for text categorization.In Proceedings of the 2015 IEEE/ACM InternationalConference on Advances in Social Networks Analysisand Mining, pages 1473?1479.
ACM.Fragkiskos D. Malliaros, Apostolos N. Papadopoulos,and Michalis Vazirgiannis.
2016a.
Core decompo-sition in graphs: concepts, algorithms and applica-tions.
In Proceedings of the 19th International Confer-ence on Extending Database Technology, EDBT, pages720?721.Fragkiskos D Malliaros, Maria-Evgenia G Rossi, andMichalis Vazirgiannis.
2016b.
Locating influen-tial nodes in complex networks.
Scientific Reports,6:19307.Luis Marujo, Anatole Gershman, Jaime Carbonell,Robert Frederking, and Jo ao P. Neto.
2012.
Su-pervised topical key phrase extraction of news storiesusing crowdsourcing, light filtering and co-referencenormalization.
In Proceedings of LREC 2012.
ELRA.Polykarpos Meladianos, Giannis Nikolentzos, Franc?oisRousseau, Yannis Stavrakas, and Michalis Vazirgian-nis.
2015.
Degeneracy-based real-time sub-event de-tection in twitter stream.
In Ninth International AAAIConference on Web and Social Media (ICWSM).Rada Mihalcea and Paul Tarau.
2004.
TextRank: bring-ing order into texts.
In Proceedings of the 2004 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP).
Association for ComputationalLinguistics.R Core Team, 2015.
R: A Language and Environmentfor Statistical Computing.
R Foundation for StatisticalComputing, Vienna, Austria.Franc?ois Rousseau and Michalis Vazirgiannis.
2013.Graph-of-word and tw-idf: new approach to ad hocir.
In Proceedings of the 22nd ACM international con-ference on Conference on Information & KnowledgeManagement (CIKM), pages 59?68.
ACM.Franc?ois Rousseau and Michalis Vazirgiannis.
2015.Main core retention on graph-of-words for single-document keyword extraction.
In Advances in Infor-mation Retrieval, pages 382?393.
Springer.Franc?ois Rousseau, Emmanouil Kiagias, and MichalisVazirgiannis.
2015.
Text categorization as a graphclassification problem.
In ACL, volume 15, page 107.Stephen B Seidman.
1983.
Network structure and mini-mum degree.
Social networks, 5(3):269?287.Jia Wang and James Cheng.
2012.
Truss decompositionin massive networks.
Proceedings of the VLDB En-dowment, 5(9):812?823.1869Rui Wang, Wei Liu, and Chris McDonald.
2014.
Corpus-independent generic keyphrase extraction using wordembedding vectors.
In Software Engineering ResearchConference, page 39.1870
