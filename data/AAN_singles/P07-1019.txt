Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144?151,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsForest Rescoring: Faster Decoding with Integrated Language Models ?Liang HuangUniversity of PennsylvaniaPhiladelphia, PA 19104lhuang3@cis.upenn.eduDavid ChiangUSC Information Sciences InstituteMarina del Rey, CA 90292chiang@isi.eduAbstractEfficient decoding has been a fundamentalproblem in machine translation, especiallywith an integrated language model whichis essential for achieving good translationquality.
We develop faster approaches forthis problem based on k-best parsing algo-rithms and demonstrate their effectivenesson both phrase-based and syntax-based MTsystems.
In both cases, our methods achievesignificant speed improvements, often bymore than a factor of ten, over the conven-tional beam-search method at the same lev-els of search error and translation accuracy.1 IntroductionRecent efforts in statistical machine translation(MT) have seen promising improvements in out-put quality, especially the phrase-based models (Ochand Ney, 2004) and syntax-based models (Chiang,2005; Galley et al, 2006).
However, efficient de-coding under these paradigms, especially with inte-grated language models (LMs), remains a difficultproblem.
Part of the complexity arises from the ex-pressive power of the translation model: for exam-ple, a phrase- or word-based model with full reorder-ing has exponential complexity (Knight, 1999).
Thelanguage model also, if fully integrated into the de-coder, introduces an expensive overhead for main-taining target-language boundary words for dynamic?
The authors would like to thank Dan Gildea, JonathanGraehl, Mark Johnson, Kevin Knight, Daniel Marcu, BobMoore and Hao Zhang.
L. H. was partially supported byNSF ITR grants IIS-0428020 while visiting USC/ISI and EIA-0205456 at UPenn.
D. C. was partially supported under theGALE/DARPA program, contract HR0011-06-C-0022.programming (Wu, 1996; Och and Ney, 2004).
Inpractice, one must prune the search space aggres-sively to reduce it to a reasonable size.A much simpler alternative method to incorporatethe LM is rescoring: we first decode without the LM(henceforth ?LM decoding) to produce a k-best listof candidate translations, and then rerank the k-bestlist using the LM.
This method runs much faster inpractice but often produces a considerable numberof search errors since the true best translation (takingLM into account) is often outside of the k-best list.Cube pruning (Chiang, 2007) is a compromise be-tween rescoring and full-integration: it rescores ksubtranslations at each node of the forest, rather thanonly at the root node as in pure rescoring.
By adapt-ing the k-best parsing Algorithm 2 of Huang andChiang (2005), it achieves significant speed-up overfull-integration on Chiang?s Hiero system.We push the idea behind this method further andmake the following contributions in this paper:?
We generalize cube pruning and adapt it to twosystems very different from Hiero: a phrase-based system similar to Pharaoh (Koehn, 2004)and a tree-to-string system (Huang et al, 2006).?
We also devise a faster variant of cube pruning,called cube growing, which uses a lazy versionof k-best parsing (Huang and Chiang, 2005)that tries to reduce k to the minimum neededat each node to obtain the desired number ofhypotheses at the root.Cube pruning and cube growing are collectivelycalled forest rescoring since they both approxi-mately rescore the packed forest of derivations from?LM decoding.
In practice they run an order of144magnitude faster than full-integration with beamsearch, at the same level of search errors and trans-lation accuracy as measured by BLEU.2 PreliminariesWe establish in this section a unified frameworkfor translation with an integrated n-gram languagemodel in both phrase-based systems and syntax-based systems based on synchronous context-freegrammars (SCFGs).
An SCFG (Lewis and Stearns,1968) is a context-free rewriting system for generat-ing string pairs.
Each rule A ?
?, ?
rewrites a pairof nonterminals in both languages, where ?
and ?are the source and target side components, and thereis a one-to-one correspondence between the nonter-minal occurrences in ?
and the nonterminal occur-rences in ?.
For example, the following ruleVP ?
PP (1) VP (2), VP (2) PP (1)captures the swapping of VP and PP between Chi-nese (source) and English (target).2.1 Translation as DeductionWe will use the following example from Chinese toEnglish for both systems described in this section:yu?withSha?lo?ngSharonju?x?
?ngholdle[past]hu?`ta?nmeeting?held a meeting with Sharon?A typical phrase-based decoder generates partialtarget-language outputs in left-to-right order in theform of hypotheses (Koehn, 2004).
Each hypothesishas a coverage vector capturing the source-languagewords translated so far, and can be extended into alonger hypothesis by a phrase-pair translating an un-covered segment.This process can be formalized as a deduc-tive system.
For example, the following deduc-tion step grows a hypothesis by the phrase-pair?yu?
Sha?lo?ng, with Sharon?
:( ???)
: (w, ?held a talk?)(?????)
: (w + c, ?held a talk with Sharon?)
(1)where a ?
in the coverage vector indicates the sourceword at this position is ?covered?
(for simplicitywe omit here the ending position of the last phrasewhich is needed for distortion costs), and where wand w + c are the weights of the two hypotheses,respectively, with c being the cost of the phrase-pair.Similarly, the decoding problem with SCFGs canalso be cast as a deductive (parsing) system (Shieberet al, 1995).
Basically, we parse the input string us-ing the source projection of the SCFG while build-ing the corresponding subtranslations in parallel.
Apossible deduction of the above example is notated:(PP1,3) : (w1, t1) (VP3,6) : (w2, t2)(VP1,6) : (w1 + w2 + c?, t2t1) (2)where the subscripts denote indices in the input sen-tence just as in CKY parsing, w1, w2 are the scoresof the two antecedent items, and t1 and t2 are thecorresponding subtranslations.
The resulting trans-lation t2t1 is the inverted concatenation as specifiedby the target-side of the SCFG rule with the addi-tional cost c?
being the cost of this rule.These two deductive systems represent the searchspace of decoding without a language model.
Whenone is instantiated for a particular input string, it de-fines a set of derivations, called a forest, representedin a compact structure that has a structure of a graphin the phrase-based case, or more generally, a hyper-graph in both cases.
Accordingly we call items like(?????)
and (VP1,6) nodes in the forest, and instan-tiated deductions like(?????)
?
( ???)
with Sharon,(VP1,6) ?
(VP3,6) (PP1,3)we call hyperedges that connect one or more an-tecedent nodes to a consequent node.2.2 Adding a Language ModelTo integrate with a bigram language model, we canuse the dynamic-programming algorithms of Ochand Ney (2004) and Wu (1996) for phrase-basedand SCFG-based systems, respectively, which wemay think of as doing a finer-grained version of thedeductions above.
Each node v in the forest willbe split into a set of augmented items, which wecall +LM items.
For phrase-based decoding, a +LMitem has the form (v a) where a is the last wordof the hypothesis.
Thus a +LM version of Deduc-tion (1) might be:( ???
talk) : (w, ?held a talk?)(?????
Sharon) : (w?, ?held a talk with Sharon?
)1451.01.13.51.0 4.0 7.02.5 8.3 8.52.4 9.5 8.49.2 17.0 15.2(VP held ?
meeting3,6 )(VP held ?
talk3,6 )(VP hold ?
conference3,6 )(PPwith?
Sharon1,3)(PPalong?
Sharon1,3)(PPwith?
Shalong1,3)1.0 4.0 7.0(PPwith?
Sharon1,3)(PPalong?
Sharon1,3)(PPwith?
Shalong1,3)2.52.48.3(PPwith?
Sharon1,3)(PPalong?
Sharon1,3)(PPwith?
Shalong1,3)1.0 4.0 7.02.52.48.39.59.2(PPwith?
Sharon1,3)(PPalong?
Sharon1,3)(PPwith?
Shalong1,3)1.0 4.0 7.02.52.48.39.29.58.5(a) (b) (c) (d)Figure 1: Cube pruning along one hyperedge.
(a): the numbers in the grid denote the score of the resulting+LM item, including the combination cost; (b)-(d): the best-first enumeration of the top three items.
Noticethat the items popped in (b) and (c) are out of order due to the non-monotonicity of the combination cost.where the score of the resulting +LM itemw?
= w + c?
logPlm(with | talk)now includes a combination cost due to the bigramsformed when applying the phrase-pair.Similarly, a +LM item in SCFG-based modelshas the form (va?b), where a and b are boundarywords of the hypothesis string, and ?
is a placeholdersymbol for an elided part of that string, indicatingthat a possible translation of the part of the inputspanned by v starts with a and ends with b.
An ex-ample +LM version of Deduction (2) is:(PP with ?
Sharon1,3 ): (w1, t1) (VP held ?
talk3,6 ): (w2, t2)(VP held ?
Sharon1,6 ): (w, t2t1)where w = w1 +w2 +c??
logPlm(with | talk) witha similar combination cost formed in combining ad-jacent boundary words of antecedents.
This schemecan be easily extended to work with a general n-gram model (Chiang, 2007).
The experiments in thispaper use trigram models.The conventional full-integration approach tra-verses the forest bottom-up and explores all pos-sible +LM deductions along each hyperedge.The theoretical running time of this algorithmis O(|F ||T |(m?1)) for phrase-based models, andO(|F ||T |4(m?1)) for binary-branching SCFG-basedmodels, where |F | is the size of the forest, and |T |is the number of possible target-side words.
Evenif we assume a constant number of translations foreach word in the input, with a trigram model, thisstill amounts to O(n11) for SCFG-based models andO(2nn2) for phrase-based models.3 Cube PruningCube pruning (Chiang, 2007) reduces the searchspace significantly based on the observation thatwhen the above method is combined with beamsearch, only a small fraction of the possible +LMitems at a node will escape being pruned, and more-over we can select with reasonable accuracy thosetop-k items without computing all possible itemsfirst.
In a nutshell, cube pruning works on the ?LMforest, keeping at most k +LM items at each node,and uses the k-best parsing Algorithm 2 of Huangand Chiang (2005) to speed up the computation.For simplicity of presentation, we will use concreteSCFG-based examples, but the method applies to thegeneral hypergraph framework in Section 2.Consider Figure 1(a).
Here k = 3 and we useD(v) to denote the top-k +LM items (in sorted or-der) of node v. Suppose we have computed D(u1)and D(u2) for the two antecedent nodes u1 =(VP3,6) and u2 = (PP1,3) respectively.
Then forthe consequent node v = (VP1,6) we just needto derive the top-3 from the 9 combinations of(Di(u1), Dj(u2)) with i, j ?
[1, 3].
Since the an-tecedent items are sorted, it is very likely that thebest consequent items in this grid lie towards theupper-left corner.
This situation is very similar to k-best parsing and we can adapt the Algorithm 2 ofHuang and Chiang (2005) here to explore this gridin a best-first order.Suppose that the combination costs are negligible,and therefore the weight of a consequent item is justthe product of the weights of the antecedent items.1461: function CUBE(F ) ?
the input is a forest F2: for v ?
F in (bottom-up) topological order do3: KBEST(v)4: return D1(TOP)5: procedure KBEST(v)6: cand ?
{?e,1?
| e ?
IN (v)} ?
for each incoming e7: HEAPIFY(cand) ?
a priority queue of candidates8: buf ?
?9: while |cand | > 0 and |buf | < k do10: item?
POP-MIN(cand)11: append item to buf12: PUSHSUCC(item, cand)13: sort buf to D(v)14: procedure PUSHSUCC(?e, j?, cand )15: e is v ?
u1 .
.
.
u|e|16: for i in 1 .
.
.
|e| do17: j?
?
j + bi18: if |D(ui)| ?
j?i then19: PUSH(?e, j?
?, cand)Figure 2: Pseudocode for cube pruning.Then we know that D1(v) = (D1(u1), D1(u2)),the upper-left corner of the grid.
Moreover, weknow that D2(v) is the better of (D1(u1), D2(u2))and (D2(u1), D1(u2)), the two neighbors of theupper-left corner.
We continue in this way (see Fig-ure 1(b)?
(d)), enumerating the consequent itemsbest-first while keeping track of a relatively smallnumber of candidates (shaded cells in Figure 1(b),cand in Figure 2) for the next-best item.However, when we take into account the combi-nation costs, this grid is no longer monotonic in gen-eral, and the above algorithm will not always enu-merate items in best-first order.
We can see this inthe first iteration in Figure 1(b), where an item withscore 2.5 has been enumerated even though there isan item with score 2.4 still to come.
Thus we riskmaking more search errors than the full-integrationmethod, but in practice the loss is much less signif-icant than the speedup.
Because of this disordering,we do not put the enumerated items directly intoD(v); instead, we collect items in a buffer (buf inFigure 2) and re-sort the buffer into D(v) after it hasaccumulated k items.1In general the grammar may have multiple rulesthat share the same source side but have differenttarget sides, which we have treated here as separate1Notice that different combinations might have the same re-sulting item, in which case we only keep the one with the betterscore (sometimes called hypothesis recombination in MT liter-ature), so the number of items in D(v) might be less than k.method k-best +LM rescoring.
.
.rescoring Alg.
3 only at the root nodecube pruning Alg.
2 on-the-fly at each nodecube growing Alg.
3 on-the-fly at each nodeTable 1: Comparison of the three methods.hyperedges in the ?LM forest.
In Hiero, these hy-peredges are processed as a single unit which wecall a hyperedge bundle.
The different target sidesthen constitute a third dimension of the grid, form-ing a cube of possible combinations (Chiang, 2007).Now consider that there are many hyperedges thatderive v, and we are only interested the top +LMitems of v over all incoming hyperedges.
FollowingAlgorithm 2, we initialize the priority queue candwith the upper-left corner item from each hyper-edge, and proceed as above.
See Figure 2 for thepseudocode for cube pruning.
We use the notation?e, j?
to identify the derivation of v via the hyper-edge e and the jith best subderivation of antecedentui (1 ?
i ?
|j|).
Also, we let 1 stand for a vec-tor whose elements are all 1, and bi for the vectorwhose members are all 0 except for the ith whosevalue is 1 (the dimensionality of either should be ev-ident from the context).
The heart of the algorithmis lines 10?12.
Lines 10?11 move the best deriva-tion ?e, j?
from cand to buf , and then line 12 pushesits successors {?e, j + bi?
| i ?
1 .
.
.
|e|} into cand .4 Cube GrowingAlthough much faster than full-integration, cubepruning still computes a fixed amount of +LM itemsat each node, many of which will not be useful forarriving at the 1-best hypothesis at the root.
It wouldbe more efficient to compute as few +LM items ateach node as are needed to obtain the 1-best hypoth-esis at the root.
This new method, called cube grow-ing, is a lazy version of cube pruning just as Algo-rithm 3 of Huang and Chiang (2005), is a lazy ver-sion of Algorithm 2 (see Table 1).Instead of traversing the forest bottom-up, cubegrowing visits nodes recursively in depth-first or-der from the root node (Figure 4).
First we callLAZYJTHBEST(TOP, 1), which uses the same al-gorithm as cube pruning to find the 1-best +LMitem of the root node using the best +LM items of1471.01.13.51.0 4.0 7.02.1 5.1 8.12.2 5.2 8.24.6 7.6 10.61.0 4.0 7.02.52.48.3(a) h-values (b) true costsFigure 3: Example of cube growing along one hyper-edge.
(a): the h(x) scores for the grid in Figure 1(a),assuming hcombo(e) = 0.1 for this hyperedge; (b)cube growing prevents early ranking of the top-leftcell (2.5) as the best item in this grid.the antecedent nodes.
However, in this case the best+LM items of the antecedent nodes are not known,because we have not visited them yet.
So we re-cursively invoke LAZYJTHBEST on the antecedentnodes to obtain them as needed.
Each invocation ofLAZYJTHBEST(v, j) will recursively call itself onthe antecedents of v until it is confident that the jthbest +LM item for node v has been found.Consider again the case of one hyperedge e. Be-cause of the nonmonotonicity caused by combina-tion costs, the first +LM item (?e,1?)
popped fromcand is not guaranteed to be the best of all combina-tions along this hyperedge (for example, the top-leftcell of 2.5 in Figure 1 is not the best in the grid).
Sowe cannot simply enumerate items just as they comeoff of cand .2 Instead, we need to store up poppeditems in a buffer buf , just as in cube pruning, andenumerate an item only when we are confident that itwill never be surpassed in the future.
In other words,we would like to have an estimate of the best itemnot explored yet (analogous to the heuristic func-tion in A* search).
If we can establish a lower boundhcombo(e) on the combination cost of any +LM de-duction via hyperedge e, then we can form a mono-tonic grid (see Figure 3(a)) of lower bounds on thegrid of combinations, by using hcombo(e) in place ofthe true combination cost for each +LM item x inthe grid; call this lower bound h(x).Now suppose that the gray-shaded cells in Fig-ure 3(a) are the members of cand .
Then the min-imum of h(x) over the items in cand , in this ex-2If we did, then the out-of-order enumeration of +LM itemsat an antecedent node would cause an entire row or column inthe grid to be disordered at the consequent node, potentiallyleading to a multiplication of search errors.1: procedure LAZYJTHBEST(v, j)2: if cand [v] is undefined then3: cand [v]?
?4: FIRE(e,1, cand) foreach e ?
IN (v)5: buf [v]?
?6: while |D(v)| < j and |buf [v]| + |D(v)| < k and|cand [v]| > 0 do7: item?
POP-MIN(cand [v])8: PUSH(item, buf [v])9: PUSHSUCC(item, cand [v])10: bound ?
min{h(x) | x ?
cand [v]}11: ENUM(buf [v],D(v), bound)12: ENUM(buf [v],D(v), +?
)13: procedure FIRE(e, j, cand )14: e is v ?
u1 .
.
.
u|e|15: for i in 1 .
.
.
|e| do16: LAZYJTHBEST(ui, ji)17: if |D(ui)| < ji then return18: PUSH(?e, j?, cand)19: procedure PUSHSUCC(?e, j?, cand )20: FIRE(e, j + bi, cand) foreach i in 1 .
.
.
|e|21: procedure ENUM(buf ,D, bound )22: while |buf | > 0 and MIN(buf ) < bound do23: append POP-MIN(buf ) to DFigure 4: Pseudocode of cube growing.ample, min{2.2, 5.1} = 2.2 is a lower bound onthe cost of any item in the future for the hyperedgee.
Indeed, if cand contains items from multiple hy-peredges for a single consequent node, this is still avalid lower bound.
More formally:Lemma 1.
For each node v in the forest, the termbound = minx?cand [v]h(x) (3)is a lower bound on the true cost of any future itemthat is yet to be explored for v.Proof.
For any item x that is not explored yet, thetrue cost c(x) ?
h(x), by the definition of h. Andthere exists an item y ?
cand[v] along the same hy-peredge such that h(x) ?
h(y), due to the mono-tonicity of h within the grid along one hyperedge.We also have h(y) ?
bound by the definition ofbound.
Therefore c(x) ?
bound .Now we can safely pop the best item from buf ifits true cost MIN(buf ) is better than bound and passit up to the consequent node (lines 21?23); but other-wise, we have to wait for more items to accumulatein buf to prevent a potential search error, for exam-ple, in the case of Figure 3(b), where the top-left cell148(a)1 2 3 4 5(b)1 2 3 4 5Figure 5: (a) Pharaoh expands the hypotheses in thecurrent bin (#2) into longer ones.
(b) In Cubit, hy-potheses in previous bins are fed via hyperedge bun-dles (solid arrows) into a priority queue (shaded tri-angle), which empties into the current bin (#5).
(2.5) is worse than the current bound of 2.2.
The up-date of bound in each iteration (line 10) can be effi-ciently implemented by using another heap with thesame contents as cand but prioritized by h instead.In practice this is a negligible overhead on top ofcube pruning.We now turn to the problem of estimating theheuristic function hcombo .
In practice, computingtrue lower bounds of the combination costs is tooslow and would compromise the speed up gainedfrom cube growing.
So we instead use a much sim-pler method that just calculates the minimum com-bination cost of each hyperedge in the top-i deriva-tions of the root node in ?LM decoding.
This isjust an approximation of the true lower bound, andbad estimates can lead to search errors.
However, thehope is that by choosing the right value of i, these es-timates will be accurate enough to affect the searchquality only slightly, which is analogous to ?almostadmissible?
heuristics in A* search (Soricut, 2006).5 ExperimentsWe test our methods on two large-scale English-to-Chinese translation systems: a phrase-based systemand our tree-to-string system (Huang et al, 2006).1.01.13.51.0 4.0 7.02.5 8.3 8.52.4 9.5 8.49.2 17.0 15.2( ???
meeting)( ???
talk)( ???
conference)withSharonand SharonwithAriel Sharon...Figure 6: A hyperedge bundle represents all +LMdeductions that derives an item in the current binfrom the same coverage vector (see Figure 5).
Thephrases on the top denote the target-sides of appli-cable phrase-pairs sharing the same source-side.5.1 Phrase-based DecodingWe implemented Cubit, a Python clone of thePharaoh decoder (Koehn, 2004),3 and adapted cubepruning to it as follows.
As in Pharaoh, each bini contains hypotheses (i.e., +LM items) covering iwords on the source-side.
But at each bin (see Fig-ure 5), all +LM items from previous bins are firstpartitioned into ?LM items; then the hyperedgesleading from those ?LM items are further groupedinto hyperedge bundles (Figure 6), which are placedinto the priority queue of the current bin.Our data preparation follows Huang et al (2006):the training data is a parallel corpus of 28.3M wordson the English side, and a trigram language model istrained on the Chinese side.
We use the same test setas (Huang et al, 2006), which is a 140-sentence sub-set of the NIST 2003 test set with 9?36 words on theEnglish side.
The weights for the log-linear modelare tuned on a separate development set.
We set thedecoder phrase-table limit to 100 as suggested in(Koehn, 2004) and the distortion limit to 4.Figure 7(a) compares cube pruning against full-integration in terms of search quality vs. search ef-ficiency, under various pruning settings (thresholdbeam set to 0.0001, stack size varying from 1 to200).
Search quality is measured by average modelcost per sentence (lower is better), and search effi-ciency is measured by the average number of hy-potheses generated (smaller is faster).
At each level3In our tests, Cubit always obtains a BLEU score within0.004 of Pharaoh?s (Figure 7(b)).
Source code available athttp://www.cis.upenn.edu/?lhuang3/cubit/1497680848892102 103 104 105 106averagemodelcostaverage number of hypotheses per sentencefull-integration (Cubit)cube pruning (Cubit)0.2000.2050.2100.2150.2200.2250.2300.2350.2400.245102 103 104 105 106BLEUscoreaverage number of hypotheses per sentencePharaohfull-integration (Cubit)cube pruning (Cubit)(a) (b)Figure 7: Cube pruning vs. full-integration (with beam search) on phrase-based decoding.of search quality, the speed-up is always better thana factor of 10.
The speed-up at the lowest search-error level is a factor of 32.
Figure 7(b) makes asimilar comparison but measures search quality byBLEU, which shows an even larger relative speed-upfor a given BLEU score, because translations withvery different model costs might have similar BLEUscores.
It also shows that our full-integration imple-mentation in Cubit faithfully reproduces Pharaoh?sperformance.
Fixing the stack size to 100 and vary-ing the threshold yielded a similar result.5.2 Tree-to-string DecodingIn tree-to-string (also called syntax-directed) decod-ing (Huang et al, 2006; Liu et al, 2006), the sourcestring is first parsed into a tree, which is then re-cursively converted into a target string according totransfer rules in a synchronous grammar (Galley etal., 2006).
For instance, the following rule translatesan English passive construction into Chinese:VPVBDwasVP-Cx1:VBN PPINbyx2:NP-C?
be`i x2 x1Our tree-to-string system performs slightly bet-ter than the state-of-the-art phrase-based systemPharaoh on the above data set.
Although differ-ent from the SCFG-based systems in Section 2, itsderivation trees remain context-free and the searchspace is still a hypergraph, where we can adapt themethods presented in Sections 3 and 4.The data set is same as in Section 5.1, except thatwe also parsed the English-side using a variant ofthe Collins (1997) parser, and then extracted 24.7Mtree-to-string rules using the algorithm of (Galley etal., 2006).
Since our tree-to-string rules may havemany variables, we first binarize each hyperedge inthe forest on the target projection (Huang, 2007).All the three +LM decoding methods to be com-pared below take these binarized forests as input.
Forcube growing, we use a non-duplicate k-best method(Huang et al, 2006) to get 100-best unique transla-tions according to ?LM to estimate the lower-boundheuristics.4 This preprocessing step takes on aver-age 0.12 seconds per sentence, which is negligiblein comparison to the +LM decoding time.Figure 8(a) compares cube growing and cubepruning against full-integration under various beamsettings in the same fashion of Figure 7(a).
At thelowest level of search error, the relative speed-upfrom cube growing and cube pruning compared withfull-integration is by a factor of 9.8 and 4.1, respec-tively.
Figure 8(b) is a similar comparison in termsof BLEU scores and shows an even bigger advantageof cube growing and cube pruning over the baseline.4If a hyperedge is not represented at all in the 100-best?LMderivations at the root node, we use the 1-best ?LM derivationof this hyperedge instead.
Here, rules that share the same sourceside but have different target sides are treated as separate hy-peredges, not collected into hyperedge bundles, since groupingbecomes difficult after binarization.150218.2218.4218.6218.8219.0103 104 105averagemodelcostaverage number of +LM items explored per sentencefull-integrationcube pruningcube growing0.2540.2560.2580.2600.262103 104 105BLEUscoreaverage number of +LM items explored per sentencefull-integrationcube pruningcube growing(a) (b)Figure 8: Cube growing vs. cube pruning vs. full-integration (with beam search) on tree-to-string decoding.6 Conclusions and Future WorkWe have presented a novel extension of cube prun-ing called cube growing, and shown how both can beseen as general forest rescoring techniques applica-ble to both phrase-based and syntax-based decoding.We evaluated these methods on large-scale transla-tion tasks and observed considerable speed improve-ments, often by more than a factor of ten.
We planto investigate how to adapt cube growing to phrase-based and hierarchical phrase-based systems.These forest rescoring algorithms have potentialapplications to other computationally intensive tasksinvolving combinations of different models, forexample, head-lexicalized parsing (Collins, 1997);joint parsing and semantic role labeling (Sutton andMcCallum, 2005); or tagging and parsing with non-local features.
Thus we envision forest rescoring asbeing of general applicability for reducing compli-cated search spaces, as an alternative to simulatedannealing methods (Kirkpatrick et al, 1983).ReferencesDavid Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In Proc.
ACL.David Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2).
To appear.Michael Collins.
1997.
Three generative lexicalised models forstatistical parsing.
In Proc.
ACL.M.
Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,W.
Wang, and I. Thayer.
2006.
Scalable inference andtraining of context-rich syntactic translation models.
InProc.
COLING-ACL.Liang Huang and David Chiang.
2005.
Better k-best parsing.In Proc.
IWPT.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.
Sta-tistical syntax-directed translation with extended domain oflocality.
In Proc.
AMTA.Liang Huang.
2007.
Binarization, synchronous binarization,and target-side binarization.
In Proc.
NAACL Workshop onSyntax and Structure in Statistical Translation.S.
Kirkpatrick, C. D. Gelatt, and M. P. Vecchi.
1983.
Optimiza-tion by simulated annealing.
Science, 220(4598):671?680.Kevin Knight.
1999.
Decoding complexity in word-replacement translation models.
Computational Linguistics,25(4):607?615.Philipp Koehn.
2004.
Pharaoh: a beam search decoder forphrase-based statistical machine translation models.
InProc.
AMTA, pages 115?124.P.
M. Lewis and R. E. Stearns.
1968.
Syntax-directed transduc-tion.
J. ACM, 15:465?488.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-stringalignment template for statistical machine translation.
InProc.
COLING-ACL, pages 609?616.Franz Joseph Och and Hermann Ney.
2004.
The alignmenttemplate approach to statistical machine translation.
Com-putational Linguistics, 30:417?449.Stuart Shieber, Yves Schabes, and Fernando Pereira.
1995.Principles and implementation of deductive parsing.
J. LogicProgramming, 24:3?36.Radu Soricut.
2006.
Natural Language Generation using anInformation-Slim Representation.
Ph.D. thesis, Universityof Southern California.Charles Sutton and Andrew McCallum.
2005.
Joint parsingand semantic role labeling.
In Proc.
CoNLL 2005.Dekai Wu.
1996.
A polynomial-time algorithm for statisticalmachine translation.
In Proc.
ACL.151
