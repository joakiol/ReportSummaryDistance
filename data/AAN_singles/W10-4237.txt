Finding Common Ground: Towards a Surface Realisation Shared TaskAnja BelzNatural Language Technology GroupComputing, Mathematical and Information SciencesUniversity of Brighton, Brighton BN2 4GJ, UKa.s.belz@brighton.ac.ukMike WhiteDepartment of LinguisticsThe Ohio State UniversityColumbus, OH, USAmwhite@ling.osu.eduJosef van Genabith and Deirdre HoganNational Centre for Language TechnologySchool of ComputingDublin City UniversityDublin 9, Ireland{dhogan,josef}@computing.dcu.ieAmanda StentAT&T Labs Research, Inc.,180 Park AvenueFlorham Park, NJ 07932, USAstent@research.att.comAbstractIn many areas of NLP reuse of utility toolssuch as parsers and POS taggers is nowcommon, but this is still rare in NLG.
Thesubfield of surface realisation has perhapscome closest, but at present we still lacka basis on which different surface realis-ers could be compared, chiefly because ofthe wide variety of different input repre-sentations used by different realisers.
Thispaper outlines an idea for a shared task insurface realisation, where inputs are pro-vided in a common-ground representationformalism which participants map to thetypes of input required by their system.These inputs are derived from existing an-notated corpora developed for languageanalysis (parsing etc.).
Outputs (realisa-tions) are evaluated by automatic compari-son against the human-authored text in thecorpora as well as by human assessors.1 BackgroundWhen reading a paper reporting a new NLP sys-tem, it is common these days to find that theauthors have taken an NLP utility tool off theshelf and reused it.
Researchers frequently reuseparsers, POS-taggers, named entity recognisers,coreference resolvers, and many other tools.
Notonly is there a real choice between a range of dif-ferent systems performing the same task, there arealso evaluation methodologies to help determinewhat the state of the art is.Natural Language Generation (NLG) has notso far developed generic tools and methods forcomparing them to the same extent as NaturalLanguage Analysis (NLA) has.
The subfield ofNLG that has perhaps come closest to developinggeneric tools is surface realisation.
Wide-coveragesurface realisers such as PENMAN/NIGEL (Mannand Mathiesen, 1983), FUF/SURGE (Elhadad andRobin, 1996) and REALPRO (Lavoie and Ram-bow, 1997) were intended to be more or less off-the-shelf plug-and-play modules.
But they tendedto require a significant amount of work to adaptand integrate, and required highly specific inputsincorporating up to several hundred features thatneeded to be set.With the advent of statistical techniques in NLGsurface realisers appeared for which it was far sim-pler to supply inputs, as information not providedin the inputs could be added on the basis of like-lihood.
An early example, the Japan-Gloss sys-tem (Knight et al, 1995) replaced PENMAN?s de-fault settings with statistical decisions.
The Halo-gen/Nitrogen developers (Langkilde and Knight,1998a) allowed inputs to be arbitrarily underspec-ified, and any decision not made before the realiserwas decided simply by highest likelihood accord-ing to a language model, automatically trainablefrom raw corpora.The Halogen/Nitrogen work sparked an interestin statistical NLG which led to a range of surfacerealisation methods that used corpus frequenciesin one way or another (Varges and Mellish, 2001;White, 2004; Velldal et al, 2004; Paiva and Evans,2005).
Some surface realisation work looked atdirectly applying statistical models during a lin-guistically informed generation process to prunethe search space (White, 2004; Carroll and Oepen,2005).While statistical techniques have led to realisersthat are more (re)usable, we currently still haveno way of determining what the state of the artis.
A significant subset of statistical realisationwork (Langkilde, 2002; Callaway, 2003; Nakan-ishi et al, 2005; Zhong and Stent, 2005; Cahill andvan Genabith, 2006; White and Rajkumar, 2009)has recently produced results for regenerating thePenn Treebank.
The basic approach in all thiswork is to remove information from the Penn Tree-bank parses (the word strings themselves as wellas some of the parse information), and then con-vert and use these underspecified representationsas inputs to the surface realiser whose task it is toreproduce the original treebank sentence.
Resultsare typically evaluated using BLEU, and, roughlyspeaking, BLEU scores go down as more informa-tion is removed.While publications of work along these lines dorefer to each other and (tentatively) compare BLEUscores, the results are not in fact directly compara-ble, because of the differences in the input repre-sentations automatically derived from Penn Tree-bank annotations.
In particular, the extent to whichthey are underspecified varies from one system tothe next.The idea we would like to put forward withthis short paper is to develop a shared task in sur-face realisation based on common inputs and an-notated corpora of paired inputs and outputs de-rived from various resources from NLA that buildon the Penn Treebank.
Inputs are provided in acommon-ground representation formalism whichparticipants map to the types of input required bytheir system.
These inputs are automatically de-rived from the Penn Treebank and the various lay-ers of annotation (syntactic, semantic, discourse)that have been developed for the documents in it.Outputs (realisations) are evaluated by automaticcomparison against the human-authored text in thecorpora as well as by by human assessors.In the short term, such a shared task wouldmake existing and new approaches directly com-parable by evaluation on the benchmark data asso-ciated with the shared task.
In the long term, thecommon-ground input representation may lead toa standardised level of representation that can actas a link between surface realisers and precedingmodules, and can make it possible to use alterna-tive surface realisers as drop-in replacements foreach other.2 Towards Common InputsOne hugely challenging aspect in developing aSurface Realisation task is developing a commoninput representation that all, or at least a major-ity of, surface realisation researchers are happy towork with.
While many different formalisms havebeen used for input representations to surface re-alisers, one cannot simply use e.g.
van Genabithet al?s automatically generated LFG f-structures,White et als CCG logical forms, Nivre?s depen-dencies, Miyao et al?s HPSG predicate-argumentstructures or Copestake?s MRSs etc., as each ofthem would introduce a bias in favour of one typeof system.One possible solution is to develop a meta-representation which contains, perhaps on multi-ple layers of representation, all the informationneeded to map to any of a given set of realiser in-put representations, a common-ground representa-tion that acts as a kind of interlingua for translatingbetween different input representations.An important issue in deriving input repre-sentations from semantically, syntactically anddiscourse-annotated corpora is deciding what in-formation not to include.
A concern is that mak-ing such decisions by committee may be difficult.One way to make it easier might be to define sev-eral versions of the task, where each version usesinputs of different levels of specificity.Basing a common input representation on whatcan feasibly be obtained from non-NLG resourceswould put everyone on reasonably common foot-ing.
If, moreover, the common input representa-tions can be automatically derived from annota-tions in existing resources, then data can be pro-duced in sufficient quantities to make it feasiblefor participants to automatically learn mappingsfrom the system-neutral input to their own input.The above could be achieved by doing some-thing along the lines of the CoNLL?08 shared taskon Joint Parsing of Syntactic and Semantic De-pendencies, for which the organisers combined thePenn Treebank, Propbank, Nombank and the BBNNamed Entity corpus into a dependency represen-tation.
Brief descriptions of these resources andmore details on this idea are provided in Section 4below.3 EvaluationAs many NLG researchers have argued, there isusually not a single right answer in NLG, but var-ious answers, some better than others, and NLGtasks should take this into account.
If a surfacerealisation task is focused on single-best realiza-tions, then it will not encourage research on pro-ducing all possible good realizations, or multipleacceptable realizations in a ranked list, etc.
Itmay not be the best approach to encourage sys-tems that try to make a single, safe choice; in-stead, perhaps one should encourage approachesthat can tell when multiple choices would be ok,and if some would be better than others.In the long term we need to develop task defi-nitions, data resources and evaluation methodolo-gies that properly take into account the one-to-many nature of NLG, but in the short term it may bemore realistic to reuse existing non-NLG resources(which do not provide alternative realisations) andto adapt existing evaluation methodologies includ-ing intrinsic assessment of Fluency, Clarity andAppropriateness by trained evaluators, and auto-matic intrinsic methods such as BLEU and NIST.One simple way of adapting the latter, for exam-ple, could be to calculate scores for the n best re-alisations produced by a realiser and then to com-pute a weighted average where scores for reali-sations are weighted in inverse proportion to theranks given to the realisations by the realiser.4 DataThere is a wide variety of different annotated re-sources that could be of use in a shared task in sur-face realisation.
Many of these include documentsoriginally included in the Penn Treebank, and thusmake it possible in principle to combine the var-ious levels of annotation into a single common-ground representation.
The following is a (non-exhaustive) list of such resources:1.
Penn Treebank-3 (Marcus et al, 1999): onemillion words of hand-parsed 1989 WallStreet Journal material annotated in TreebankII style.
The Treebank bracketing style al-lows extraction of simple predicate/argumentstructure.
In addition to Treebank-1 mate-rial, Treebank-3 contains documents from theSwitchboard and Brown corpora.2.
Propbank (Palmer et al, 2005): This is a se-mantic annotation of the Wall Street Journalsection of Penn Treebank-2.
More specifi-cally, each verb occurring in the Treebank hasbeen treated as a semantic predicate and thesurrounding text has been annotated for ar-guments and adjuncts of the predicate.
Theverbs have also been tagged with coarsegrained senses and with inflectional informa-tion.3.
NomBank 1.0 (Meyers et al, 2004): Nom-Bank is an annotation project at New YorkUniversity that provides argument structurefor common nouns in the Penn Treebank.NomBank marks the sets of arguments thatoccur with nouns in PropBank I, just as thelatter records such information for verbs.4.
BBN Pronoun Coreference and Entity TypeCorpus (Weischedel and Brunstein, 2005):supplements the Wall Street Journal corpus,adding annotation of pronoun coreference,and a variety of entity and numeric types.5.
FrameNet (Johnson et al, 2002): 150,000sentences annotated for semantic roles andpossible syntactic realisations.
The annotatedsentences come from a variety of sources, in-cluding some PropBank texts.6.
OntoNotes 2.0 (Weischedel et al, 2008):OntoNotes 1.0 contains 674k words of Chi-nese and 500k words of English newswireand broadcast news data.
OntoNotes followsthe Penn Treebank for syntax and PropBankfor predicate-argument structure.
Its seman-tic representation will include word sensedisambiguation for nouns and verbs, witheach word sense connected to an ontology,and coreference.
The current goal is to anno-tate over a million words each of English andChinese, and half a million words of Arabicover five years.There are other resources which may be use-ful.
Zettelmoyer and Collins (2009) have man-ually converted the original SQL meaning an-notations of the ATIS corpus (et al, 1994)?some 4,637 sentences?into lambda-calculus ex-pressions which were used for training and testingtheir semantic parser.
This resource might make agood out-of-domain test set for generation systemstrained on WSJ data.FrameNet, used for semantic parsing, see forexample Gildea and Jurafsky (2002), identifies asentence?s frame elements and assigns semanticroles to the frame elements.
FrameNet data (Bakerand Sato, 2003) was used for training and test setsin one of the SensEval-3 shared tasks in 2004 (Au-tomatic Labeling of Semantic Roles).
There hasbeen some work combining FrameNet with otherlexical resources.
For example, Shi and Mihal-cea (2005) integrated FrameNet with VerbNet andWordNet for the purpose of enabling more robustsemantic parsing.The Semlink project (http://verbs.colorado.edu/semlink/) aims to integrate Propbank,FrameNet, WordNet and VerbNet.Other relevant work includes Moldovan andRus (Moldovan and Rus, 2001; Rus, 2002) whodeveloped a technique for parsing into logicalforms and used this to transform WordNet conceptdefinitions into logical forms.
The same method(with additional manual correction) was used toproduce the test set for another SensEval-3 sharedtask (Identification of Logic Forms in English).4.1 CoNLL 2008 Shared Task DataPerhaps the most immediately promising resourceis is the CoNLL shared task data from 2008 (Sur-deanu et al, 2008) which has syntactic depen-dency annotations, named-entity boundaries andthe semantic dependencies model roles of bothverbal and nominal predicates.
The data consistof excerpts from Penn Treebank-3, BBN PronounCoreference and Entity Type Corpus, PropBank Iand NomBank 1.0.
In CoNLL ?08, the data wasused to train and test systems for the task of pro-ducing a joint semantic and syntactic dependencyanalysis of English sentences (the 2009 CoNLLShared Task extended this to multi-lingual data).It seems feasible that we could reuse the CoNLLdata for a prototype Surface Realisation task,adapting it and inversing the direction of the task,i.e.
mapping from syntactic-semantic dependencyrepresentations to word strings.5 Developing the TaskThe first step in developing a Surface Realisa-tion task could be to get together a workinggroup of surface realisation researchers to developa common-ground input representation automati-cally derivable from a set of existing resources.As part of this task a prototype corpus exempli-fying inputs/outputs and annotations could be de-veloped.
At the end of this stage it would be use-ful to write a white paper and circulate it and theprototype corpus among the NLG (and wider NLP)community for feedback and input.After a further stage of development, it may befeasible to run a prototype surface realisation taskat Generation Challenges 2011, combined with asession for discussion and roadmapping.
Depend-ing on the outcome of all of this, a full-blown taskmight be feasible by 2012.
Some of this work willneed funding to be feasible, and the authors of thispaper are in the process of applying for financialsupport for these plans.6 Concluding RemarksIn this paper we have provided an overview of ex-isting resources that could potentially be used fora surface realisation task, and have outlined ideasfor how such a task might work.
The core ideais to develop a common-ground input representa-tion which participants map to the types of inputrequired by their system.
These inputs are derivedfrom existing annotated corpora developed for lan-guage analysis.
Outputs (realisations) are evalu-ated by automatic comparison against the human-authored text in the corpora as well as by by hu-man assessors.
Evaluation methods are adapted totake account of the one-to-many nature of the re-alisation mapping.The ideas outlined in this paper began as a pro-longed email exchange, interspersed with discus-sions at conferences, among the authors.
This pa-per summarises our ideas as they have evolved sofar, to enable feedback and input from other re-searchers interested in this type of task.ReferencesColin F. Baker and Hiroaki Sato.
2003.
The framenetdata and software.
In Proceedings of ACL?03.A.
Cahill and J. van Genabith.
2006.
Robust PCFG-based generation using automatically acquired LFGapproximations.
In Proc.
ACL?06, pages 1033?44.Charles Callaway.
2003.
Evaluating coverage for largesymbolic NLG grammars.
In Proceedings of the18th International Joint Conference on Artificial In-telligence (IJCAI 2003), pages 811?817.J.
Carroll and S. Oepen.
2005.
High efficiencyrealization for a wide-coverage unification gram-mar.
In Proceedings of the 2nd International JointConference on Natural Language Processing (IJC-NLP?05), volume 3651, pages 165?176.
SpringerLecture Notes in Artificial Intelligence.M.
Elhadad and J. Robin.
1996.
An overview ofSURGE: A reusable comprehensive syntactic real-ization component.
Technical Report 96-03, Deptof Mathematics and Computer Science, Ben GurionUniversity, Beer Sheva, Israel.Deborah Dahl et al 1994.
Expanding the scope of theATIS task: the ATIS-3 corpus.
In Proceedings of theARPA HLT Workshop.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguis-tics, 28(3):245?288.C.
Johnson, C. Fillmore, M. Petruck, C. Baker,M.
Ellsworth, J. Ruppenhoper, and E.Wood.
2002.Framenet theory and practice.
Technical report.K.
Knight, I. Chander, M. Haines, V. Hatzivassiloglou,E.
Hovy, M. Iida, S. Luk, R. Whitney, and K. Ya-mada.
1995.
Filling knowledge gaps in a broad-coverage MT system.
In Proceedings of the Four-teenth International Joint Conference on ArtificialIntelligence (IJCAI ?95), pages 1390?1397.I.
Langkilde and K. Knight.
1998a.
Generationthat exploits corpus-based statistical knowledge.
InProc.
COLING-ACL.
http://www.isi.edu/licensed-sw/halogen/nitro98.ps.I.
Langkilde.
2002.
An empirical verification of cover-age and correctness for a general-purpose sentencegenerator.
In Proc.
2nd International Natural Lan-guage Generation Conference (INLG ?02).B.
Lavoie and O. Rambow.
1997.
A fast and portablerealizer for text generation systems.
In Proceedingsof the 5th Conference on Applied Natural LanguageProcessing (ANLP?97), pages 265?268.W.
Mann and C. Mathiesen.
1983.
NIGEL: A sys-temic grammar for text generation.
Technical Re-port ISI/RR-85-105, Information Sciences Institute.Mitchell P. Marcus, Beatrice Santorini, Mary AnnMarcinkiewicz, and Ann Taylor.
1999.
Treebank-3.
Technical report, Linguistic Data Consortium,Philadelphia.Adam Meyers, Ruth Reeves, and Catherine Macleod.2004.
Np-external arguments a study of argumentsharing in english.
In MWE ?04: Proceedings ofthe Workshop on Multiword Expressions, pages 96?103, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Dan I. Moldovan and Vasile Rus.
2001.
Logic formtransformation of wordnet and its applicability toquestion answering.
In Proceedings of ACL?01.Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic models for disambiguation of anhpsg-based chart generator.
In Proceedings of the9th International Workshop on Parsing Technology(Parsing?05), pages 93?102.
Association for Com-putational Linguistics.D.
S. Paiva and R. Evans.
2005.
Empirically-basedcontrol of natural language generation.
In Proceed-ings ACL?05.M.
Palmer, P. Kingsbury, and D. Gildea.
2005.
Theproposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?106.Vasile Rus.
2002.
Logic Form For WordNet Glossesand Application to Question Answering.
Ph.D. the-sis.Lei Shi and Rada Mihalcea.
2005.
Putting pieces to-gether: Combining framenet, verbnet and wordnetfor robust semantic parsing.
In Proceedings of CI-CLing?05.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
TheCoNLL-2008 shared task on joint parsing of syn-tactic and semantic dependencies.
In CoNLL ?08:Proceedings of the Twelfth Conference on Computa-tional Natural Language Learning, pages 159?177.S.
Varges and C. Mellish.
2001.
Instance-based natu-ral language generation.
In Proceedings of the 2ndMeeting of the North American Chapter of the Asso-ciation for Computational Linguistics (NAACL ?01),pages 1?8.E.
Velldal, S. Oepen, and D. Flickinger.
2004.
Para-phrasing treebanks for stochastic realization rank-ing.
In Proceedings of the 3rd Workshop on Tree-banks and Linguistic Theories (TLT ?04), Tuebin-gen, Germany.Ralph Weischedel and Ada Brunstein.
2005.
Bbn pro-noun coreference and entity type corpus.
Technicalreport, Linguistic Data Consortium.Ralph Weischedel et al 2008.
Ontonotes release 2.0.Technical report, Linguistic Data Consortium.Michael White and Rajakrishnan Rajkumar.
2009.Perceptron reranking for ccg realisation.
In Pro-ceedings of the 2009 Conference on Empririal Meth-ods in Natural Language Processing (EMNLP?09),pages 410?419.M.
White.
2004.
Reining in CCG chart realization.
InA.
Belz, R. Evans, and P. Piwek, editors, Proceed-ings INLG?04, volume 3123 of LNAI, pages 182?191.
Springer.Luke Zettlemoyer and Michael Collins.
2009.
Learn-ing context-dependent mappings from sentences tological forms.
In Proceedings of ACL-IJCNLP?09.H.
Zhong and A. Stent.
2005.
Building surfacerealizers automatically from corpora.
In A. Belzand S. Varges, editors, Proceedings of UCNLG?05,pages 49?54.
