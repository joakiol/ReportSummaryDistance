Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsFormal and Empirical Grammatical InferenceJeffrey Heinz, Colin de la Higuera and Menno van Zaanenheinz@udel.edu, cdlh@univ-nantes.fr, mvzaanen@uvt.nl1Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsOutline of the tutorialI.
Formal GI and learning theory (de la Higuera)II.
Empirical approaches to regular and subregular naturallanguage classes (Heinz)III.
Empirical approaches to nonregular natural languageclasses (van Zaanen)2Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsI Formal GI and learning theoryWhat is grammatical inference?What does learning or having learnt imply?Reasons for considering formal learningSome criteria to study learning in a probabilistic and a nonprobabilistic setting3Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsA simple definitionGrammatical inference is about learning a grammar giveninformation about a languageVocabularyLearning = building, inferringGrammar= finite representation of a possibly infinite set ofstrings, or trees, or graphsInformation=you can learn from text, from an informant, byactively queryingLanguage= possibly infinite set of strings, or trees, or graphs4Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsA Dfa (Ack: Jeffrey Heinz)The (CV)* language representing licit sequences of sounds in manylanguages in the world.
Consonants and vowels must alternate;words must begin with C and must end with V. States show theregular expression indicating its ?good tails?.
(CV )?
V (CV )?CV5Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsA context free grammar and a parse tree(de la Higuera 2010)SNP VPJohn V NPhit Det Nthe ballS ?
NP VPVP?
V NPNP?
Det N6Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsA categorial dependency grammar (Be?chet et al 2011)elle 7?
[pred ],la 7?
[#(?
clit ?
a?
obj)]?clit?a?obj ,lui 7?
[#(?
clit ?
3d ?
obj)]?clit?3d?obj ,a 7?
[#(?
clit ?
3d ?
obj)\#(?clit ?
a ?
obj)\pred\S/aux ?
a ?
d ],donne?e 7?
[aux ?
a ?
d ]?clit?3d?obj?clit?a?obj7Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsA finite state transducer (Ack: Jeffrey Heinz)A subsequential transducer illustrating a common phonological ruleof palatalization ( k ??
>tS / i).
States are labelled with anumber and then the output string given by the ?
function for thatstate.0,?
1,kk:?k:kk, C:kC, V:kVi:>tSiC,V,i k?
= {C ,V , k , i}8Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsSo for example:w t(w)kata katakita >tSitatak taktaki ta>tSi.
.
.9Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsOur definitionGrammatical inference is about learning a grammar giveninformation about a languageQuestionsWhy grammar and not language?Why a and not the?10Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsWhy not write ?learn a language?
?Because you always learn a representation of a languageParadoxTake two learners learning a context-free language, one is learninga quadratic normal form and the other a Greibach normal form,they cannot agree that they have learnt the same thing(undecidable question).Worth thinking about.
.
.
is it a paradox?
Do two English speakersagree they speak the same language?11Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsOur definitionGrammatical inference is about learning a grammar giveninformation about a languageHow can a become the?Ask for the grammar to be the smallest, best (re a score).
?Combinatorial characterisationThe learning problem becomes an optimisation problem!Then we often have theorems saying thatIf our algorithm does solve the optimisation problem, what wehave learnt is correctIf we can prove that we can?t solve the optimisation problem,then the class is not learnable12Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsOptimal with respect of some scoreScore should take into account:SimplicityCoverageUsefulnessWhat scores?Occam argumentCompression argumentKolmogorov complexityMDL argument13Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsMoreoverGI is not only about building a grammar from some data.
It isconcerned with saying something about:the quality of the result,the quality of the learning process,the properties of the process.14Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsNaive exampleSuppose you are building a random number generator.How are you convinced that it works?Because it follows sound principles as defined by numbertheory specialists?Because you have tested and the number 772356191 has beenproduced?Because you have proved that the series of numbers that willbe produced is incompressible?Empirical approachExperimental approachFormal approach15Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsEmpirical approach: using good (safe?)
ideasFor example, genetic algorithms or neural networksOr some mathematical principle (Occam, Kolmogorov,MDL,.
.
.
)Can become a principled approachAlternative point of viewEmpirical approach is about imitating what nature (or humans) do16Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsExperimental approachBenchmarksCompetitionsNecessary but not sufficientHow do we know that all the cases are covered?How do we know that we dont have a hidden bias?17Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsFormal approach: showing that the algorithm has convergedIs impossible:Just one runCan?t prove that 23 is randomBut we can say something about the algorithm:That in the near future, given some string, we can predict ifthis string belongs to the language or not;Choose between defining clearly ?near future?
and acceptingprobable truths (or error bounds) or leaving it undefined andusing identification.18Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsWhat else would we like to say?That if the solution we have returned is not good, then that isbecause the initial data was bad (insufficient, biased)Idea:Blame the data, not the algorithm19Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsSuppose we cannot say anything of the sort?Then that means that we may be terribly wrong even in afavourable settingThus there is a hidden biasHidden bias: the learning algorithm is supposed to be able tolearn anything inside class L1, but can really only learn thingsinside class L2, with L2 ?
L120Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsSaying something about the process itselfKey idea: if there is something to learn and the data is notcorrupt, then, given enough time, we will learn itReplace the notion of learning by that of identifying21Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsIn practise, does it make sense?No, because we never know if we are in the ideal conditions(something to learn + good data + enough of it)Yes, because at least we get to blame the data, not thealgorithm22Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsComplexity issuesComplexity theory should be used: the total or updateruntime, the size of the data needed, the number of mindchanges, the number and weight of errors.
.
.. .
.
should be measured and limited.23Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsA linguistic criterionOne argument appealing to linguists (we hope) is that if thecriteria are not met for some class of languages that a humanis supposed to know how to learn, something is wrongsomewhere(preposterously, the maths can?t be wrong.
.
.
)24Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsNon probabilistic settingsIdentification in the limitResource bounded identification in the limitActive learning (query learning)25Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsIdentification in the limitInformation is presented to the learner who updates itshypothesis after inspecting each piece of dataAt some point, always, the learner will have found the correctconcept and not change from it(Gold 1967 & 1978)26Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsExampleNumber Presentation Analysis of hy-pothesisNew hypothesis(regexp)1 a + a2 aaa + inconsistent a?3 aaaa - inconsistent a(aa)?4 aaaaaa - consistent a(aa)?9234 aaaaaaaa - consistent a(aa)?45623416 aaaaaaaaa + consistent a(aa)?27Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsA presentation isa function ?
: N ?
Xwhere X is some set,and such that ?
is associated to a language L through afunction Yields : Yields(?)
= LIf ?
(N) = ?
(N) then Yields(?)
= Yields(?
)28Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternstext presentationA text presentation of a language L ?
??
is a function?
: N ?
??
such that ?
(N) = L?
is an infinite succession of all the elements of L(note : small technical difficulty with ?
)informed presentationAn informed presentation (or an informant) of L ?
??
is afunction ?
: N ?
??
?
{?,+} such that?
(N) = (L,+) ?
(L,?)?
is an infinite succession of all the elements of ??
labelled toindicate if they belong or not to L29Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsActive presentationThe learner interacts with the environment (modelled as anoracle) through queriesA membership queryLearner presents string xOracle answer yes or noA correction query (Becerra-Bonache et al 2005 & 2008)Learner presents string xOracle answer yes or returns a close correctionAn equivalence queryLearner presents hypothesis HOracle answer yes or returns a counter-example30Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsExample: presentations for {anbn : n ?
N}Legal presentation from text: ?, a2b2, a7b7,.
.
.Illegal presentation from text: ab, ab, ab,.
.
.Legal presentation from informant : (?,+), (abab,?
),(a2b2,+), (a7b7,+), (aab,?
), (abab,?),.
.
.31Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsExample: presentation for SpanishLegal presentation from text: En un lugar de la Mancha.
.
.Illegal presentation from text: GooooolLegal presentation from informant : (en,+), (whatever,-),(un,+), (lugar,+), (lugor,-), (xwszrrzt,-),32Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsWhat happens before convergence?On two occasions I have been asked [by members of Parliament],?Pray, Mr. Babbage, if you put into the machine wrong figures, willthe right answers come out??
I am not able rightly to apprehendthe kind of confusion of ideas that could provoke such a question.Charles Babbage33Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsFurther definitionsGiven a presentation ?, ?n is the set of the first n elements in?.A learning algorithm (learner) A is a function that takes asinput a set ?n and returns a grammar of a language.Given a grammar G , L(G ) is the languagegenerated/recognised/ represented by G .34Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsConvergence to a hypothesisA converges to G with ?
if?n ?
N : A(?n) halts and gives an answer?n0 ?
N : n ?
n0 =?
A(?n) = GIf furthermore L(G ) = Yields(?)
then we have identified.35Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsIdentification in the limitLGPres(L)LYieldsAFigure: The learning setting.from (de la Higuera 2010)36Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsConsistency and conservatismWe say that the learner A is consistent if ?n is consistent withA(?n) ?nA consistent learner is always consistent with the pastConsistency and conservatismWe say that the learner A is conservative if whenever ?
(n + 1)is consistent with A(?n), we have A(?n) = A(?n+1)A conservative learner doesn?t change his mind needlessly37Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsLearning from dataA learner is order dependent if it learns something differentdepending on the order in which it receives the data.Usually an order independent learner is better.38Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsWhat about efficiency?We can try to boundglobal timeupdate timeerrors before converging (IPE)mind changes (MC)queriesgood examples needed (characteristic samples)(Pitt 1989, de la Higuera et al 2008)39Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsDefinition: polynomial number of implicit prediction errorsDenote by G 6|= x if G is incorrect with respect to an elementx of the presentation (i.e.
the learner producing G has madean implicit prediction error.G is polynomially identifiable in the limit from Pres if there existsan identification learner A and a polynomial p() such that givenany G in G, and given any presentation ?
of L(G ),]i : A(?i ) 6|= ?
(i + 1) ?
p(|G |).40Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsDefinition: polynomial characteristic sampleG has polynomial characteristic samples for identification learner Aif there exists a polynomial p() such that: given any G in G, ?Ycorrect sample for G , such that whenever Y ?
?n, A(?n) ?
G and?Y ?
?
p(?G?
)As soon as the CS is in the data, the result is correct;The CS is small.41Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsPolynomial queries(Angluin 1987)Algorithm A learns with a polynomial number of queries if thenumber of queries made before halting with a correctgrammar is polynomial inthe size of the target,the size of the information received.42Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsMain negative resultsCannot learn Nfa, Cfgs from an informant in mostpolynomial settings (Pitt 1989, de la Higuera 1997)Cannot learn Dfa from text (Gold 1967)Cannot learn Dfa from membership nor equivalence queries(Angluin 1981 & 1987).Main positive resultsCan learn Dfa from an informant with polynomial resources(Oncina and Garc?
?a 1992);Can learn Dfa from membership and equivalence queries(Angluin 1987).43Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsProbabilistic settingsPac learning (about learning yes-no machines with fixed butunknown distributions)Identification with probability 1 (about identifyingdistributions)Pac learning distributions (about approximately learningdistributions)44Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsLearning a language from samplingWe have a distribution over ?
?We sample twice:once to learn,once to see how well we have learnedThe Pac setting: Les Valiant, Turing award 201045Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsPac-learning(Valiant 1984, Pitt 1989)L a class of languagesG a class of grammars > 0 and ?
> 0m a maximal length over the stringsn a maximal size of machinesH is -AC (approximately correct)*ifPrD [H(x) 6= G (x)] < 46Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsPolynomial Pac learningThere is a polynomial p(?, ?, ?, ?)
such thatin order to learn -AC machines of size at most n with error atmost ?
we require at most p(m, n, 1?
, 1? )
data and time;we want the errors to be less than  and bad luck to be lessthan ?.
(French radio)Unless there is a surprise there should be no surpriseFrench radio, (after the last primary elections, on 3rd of June2008)First surprise is ?, second surprise is 47Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsResults (Kearns and Valiant 1989, Kearns and Vazirani 1994)Using cryptographic assumptions, we cannot Pac-learn DfaCannot Pac-learn Nfa, Cfgs with membership queries eitherLearning can be seen as finding the encryption function fromexamples (Kearns & Vazirani)48Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsAlternativelyInstead of learning classifiers in a probabilistic world, learndirectly the distributions!Learn probabilistic finite automata (deterministic or not)49Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsNo error (Angluin 1988)This calls for identification in the limit with probability 1Means that the probability of not converging is 0Goal is to identify the structure and the probabilitiesMainly a (nice) theoretic settingResultsIf probabilities are computable, we can learn with probability 1finite state automata (Carrasco and Oncina, 1994)But not with bounded (polynomial) resources (de la Higueraand Oncina, 2004)50Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsWith errorPac definition appliesBut error should be measured by a distance between thetarget distribution and the hypothesisHow do we measure the distance: L1, L2, L?,Kullback-Leibler?51Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsResultsToo easy to learn with L?Too hard to learn with L1Both results hold for the same algorithm!
(de la Higuera andOncina, 2004)Nice algorithms for biased classes of distributions52Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsOpen problemsWe conclude this section on ?what is language learning about?with some open questions:What is a good definition of polynomial identification?How do we deal with shifting targets?
(robustness issues)Alternative views on learnability?Is being learnable a good indicator of being linguisticallyreasonable?Can we learn transducers?
Probabilistic transducers?53Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsII.
GI of Regular PatternsWhy regular?What are the general GI strategies?What are the main results?The main techniques?The main lessons?54Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsLogically Possible Computable PatternsContext-SensitiveMildlyContext-SensitiveContext-FreeRegularFiniteYoruba copyingKobele 2006Swiss GermanShieber 1985English nested embeddingChomsky 1957English consonant clustersClements and Keyser 1983 Kwakiutl stressBach 1975Chumash sibilant harmonyApplegate 197255Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsGI Strategies#1.
Define ?learning?
so that large regions can be learnedContext-SensitiveMildlyContext-SensitiveContext-FreeRegularFiniteYoruba copyingKobele 2006Swiss GermanShieber 1985English nested embeddingChomsky 1957English consonant clustersClements and Keyser 1983 Kwakiutl stressBach 1975Chumash sibilant harmonyApplegate 197256Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsGI Strategies#2.
Target non-superfinite cross-cutting classes(instructor?s bias)Recursively EnumerableContext-SensitiveMildlyContext-SensitiveContext-FreeRegularFiniteYoruba copyingKobele 2006Swiss GermanShieber 1985English nested embeddingChomsky 1957English consonant clustersClements and Keyser 1983 Kwakiutl stressBach 1975Chumash sibilant harmonyApplegate 197257Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsCommon Theme1 Different learning frameworks may better characterize thedata presentations learners actually get (strategy #1).2 Classes of formal languages may exist which bettercharacterize the patterns we are interested in (strategy #2).3 Hard problems are easier to solve with better characterizationsbecause the instance space of the problem is smaller.58Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsWhy Begin with Regular?Insights obtained here can be (and have been) applied fruitfully tononregular classes.Angluin 1982 showed a subclass of regular languages (thereversible languages) was identifiable in the limit from positivedata by an incremental learner.Yokomori?s (2004) Very Simple Languages are a subclass ofthe context-free languages, but draws on ideas from thereversible languages.Similarly, Clark and Eryaud?s (2007) substitutable languages(also subclass of context-free) are also based on insights fromthis paper.59Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsSection Outline1 Targets of Learning2 Learning Frameworks3 State-merging4 Results for learning regular languages, relations, anddistributions60Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsTargets of Learning: Regular LanguagesMultiple grammars (i.e.
representations) for regular languages:1 Regular expressions2 Generalized regular expressions3 Finite state acceptors4 Words which satisfy formulae in monadic second order logic5 Right or left branching rewrite rules6 .
.
.61Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsTargets of Learning: Regular RelationsMultiple grammars (i.e.
representations) for regular relations:Regular expressions (for relations)Generalized regular expressions (for relations)Finite state transducers.
.
.62Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsTargets of Learning: Regular distributionsMultiple grammars (i.e.
representations) for distributions overregular sets and relations:Weighted finite state automataHidden Markov ModelsWeighted right or left branching rewrite rules.
.
.63Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsThis tutorial: Finite State AutomataAcceptors and subsequential transducers admit canonical forms1 The smallest deterministic acceptor, syntactic monoids, .
.
.2 Canonical forms relate to algebraic properties (Nerodeequivalence relation, i.e.
states represent sets of ?good tails?
)3 In contrast, canonical regular expressions have yet to bedetermined.
For example, there are no canonical (e.g.shortest) regular expressions for regular languages.64Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsLearning Frameworks: Main ChoicesSuccess required on which input data streams?All possible vs. some restricted seti.e.
?distribution-free?
vs. ?non distribution-free?What kind of samples?Positive data vs. postive and negative dataOther choices (e.g.
query learning) are not discussed here.65Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsLearning Frameworks: Main Results?Distribution-free?
w/ positive and negative data1 The class of r.e.
languages is identifiable in the limit (Gold1967)2 Non-enumerative algorithms for regular languages:1 Gold (1978)2 RPNI (Oncina and Garc?
?a 1992)66Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsLearning Frameworks: Main Results?Distribution-free?
with positive data only1 No superfinite class (including regular, cf, etc.)
is identifiablein the limit (Gold 1967)2 Not even the finite class is PAC-learnable (Blumer et al 1989)3 No superfinite class is identifiable in the limit with probabilityp (p > 2/3) (Pitt 1985, Wiehagen et al 1986, Angluin 1988)4 But many subregular classes are learnable in this difficultsetting.67Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsLearning Frameworks: Main Results?Distribution-free?
with positive data only: learnable subregularclasses1 reversible languages (Angluin 1982)2 strictly local languages (Garcia et al 1990)3 locally testable and piecewise testable (Garcia and Ruiz 2004)4 left-to-right and right-to-left iterative languages (Heinz 2008)5 strictly piecewise languages (Heinz 2010)6 .
.
.7 subsequential functions (Oncina et al 1993)8 .
.
.68Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsLearning Frameworks: Main Results?Non distribution-free?
w/ positive data only1 The class of r.e.
languages are identifiable in the limit fromcomputable classes of r.e.
texts (Gold 1967)2 The class of r.e.
distributions are identifiable from?approximately computable?
sequences (Angluin 1988, Chaterand Vitany??
2007)3 The class of distributions describable with ProbabilisticDeterministic FSAs (PDFAs) is learnable with probability one(de la Higuera and Thollard 2000)4 The class of distributions describable with PDFAs is learnablein a modified PAC setting (Clark and Thollard, 2004)69Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsLearning regular languages: Key techniqueState-mergingAngluin 1982 (reversible languages)Muggleton 1990 (contextual languages)Garcia et al 1990 (strictly local languages)Oncina et al 1993 (subsequential functions)Clark and Thollard 2004 (PDFA distributions).
.
.70Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsOther techniquesLattice-climbingHeinz 2010 (strictly local languages, strictly piecewiselanguages, many others)Kasprizk and Ko?tzing 2010 (function-distinguishablelanaguages, pattern languages, many others)State-splittingTellier (2008)71Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsOnly so much can be covered.
.
.It?s impossible to be fair to allthose who have contributedand to cover all the variants,even all the algorithms in ashort tutorial.
That?s whythere are books!72Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsOverview of State-merging1 Builds a FSA representation of the input2 Generalize by merging states73Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsIllustrative Example: Stress pattern of Pintupia.
pa?
?a ?earth?
??
?b.
tju?
?aya ?many?
??
?
?c.
ma?
?awa`na ?through from behind?
??
?
?` ?d.
pu?
?iNka`latju ?we (sat) on the hill?
??
?
?` ?
?e.
tja?mul`?mpatju`Nku ?our relation?
??
?
?` ?
?` ?f.
???
?ir`iNula`mpatju ?the fire for our benefitflared up???
?
?` ?
?` ?
?g.
ku?ranju`lul`?mpatju`?a ?the first one who is ourrelation???
?
?` ?
?` ?
?` ?h.
yu?ma?`?Nkama`ratju`?aka ?because of mother-in-law???
?
?` ?
?` ?
?` ?
?Generalization (Hayes (1995:62) citing Hansen and Hansen (1969:163)):Primary stress falls on the initial syllableSecondary stress falls on alternating nonfinal syllables74Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsIllustrative Example: Stress pattern of PintupiGeneralization (Hayes (1995:62) citing Hansen and Hansen (1969:163)):Primary stress falls on the initial syllableSecondary stress falls on alternating nonfinal syllablesMinimal deterministic FSA for Pintupi Stress0 1 234??
??
?`?75Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsStructured representations of Input1 Each word its own FSA (Nondeterministic)2 Prefix Trees (deterministic)3 Suffix Trees (reverse determinstic)76Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsExamples of Prefix and Suffix TreesS =?????
??
???
?
?
??
?
?` ???
?
?` ?
?
??
?
?` ?
?` ???
?PT(S)0 1 2345678??
??`???`??ST(S)01234567891011121316???????`?`??????`?????
?77Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsState-merging InformallyEliminate redundant environments by state-merging.States are identified as equivalent and then merged.All transitions are preserved.This is one way in which generalizations may occur?becausethe post-merged machine accepts everything the pre-mergedmachine accepts, possibly more.Machine A Machine B0 1 2 3a a a 0 1-2 3a aaThe merged machine may not be deterministic.78Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsState-merging FormallyDefinitionGiven an acceptor A = (Q, I ,F , ?)
and a partition pi of its statesstate-merging returns the acceptor A/pi = (Q ?, I ?,F ?, ??
):1 Q ?
= pi (the states are the blocks of pi)2 I ?
= {B ?
pi : I ?
B 6= ?
}3 F ?
= {B ?
pi : F ?
B 6= ?
}4 For all B ?
pi and a ?
?,??
(B , a) = {B ?
?
pi : ?q ?
B , q?
?
B ?
such that q?
?
?
(q, a)}79Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsTheoremTheoremGiven any regular language L, let A(L) denote the minimaldeterministic acceptor recognizing L. There exists a finite sampleS ?
L and a partition pi over PT (S) such that PT (S)/pi = A(L).NotesThe finite sample need only exercise every transition in A(L).What is pi?80Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsIllustrative ExampleLet?s merge states with the same incoming paths of length 2!PT(S)0 1 2345678??
??`???`?
?81Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsResult of State Merging0 1 23-64-75-8???`??`??
?This acceptor is not the canonical acceptor we saw earlier but itrecognizes the same language.Generalization (Hayes (1995:62) citing Hansen and Hansen (1969:163)):Primary stress falls on the initial syllableSecondary stress falls on alternating nonfinal syllables82Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsSummary of Algorithm1 States in the prefix tree are merged if they have the samek-length suffix.u ?
v def??
?x , y ,w such that |w | = k , u = xw , v = yw2 The algorithm then is simply:G = PT (S)/pi?3 This algorithm provably identifies in the limit from positivedata the Strictly (k + 1)-Local class of languages (Garcia etal.
1990).83Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsBack to the Illustrative ExampleResults for stress patterns more generallyOut of 109 distinct stress patterns in the world?s languages(encoded as FSAs), this state-merging strategy works for only44 of themIf we merge states with the same paths up to length 5(!
), only81 are learned.This is the case even permitting very generous input samples.In other words, 44 attested stress patterns are Strictly 3-Local and81 are Strictly 6-Local.
28 are not Strictly 6-Local In fact those 28are not Strictly k-Local for any k (Edlefsen et al 2008).84Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsOther ways to merge statesIf the current structure is ?ill-formed?
then merge states toeliminate source of ill-formednessState equivalence relations1 merge state with same incoming paths of length k (Garcia et.
al 1990)2 recursively eliminate reverse non-determinism (Angluin 1982)3 merge states with same ?contexts?
(Muggleton 1990, Clark and Eryaud2007)4 merge final states (Heinz 2008)5 merge states with same ?neighborhood?
(Heinz 2009)6 .
.
.7 merge states to maximize posterior probability (for HMMs, Stolcke 1994)8 .
.
.85Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsOther ways to merge statesMerge states indiscriminately unless ?ill-formedness?
arisesMerge unless something tells us not to1 unless ?onward subsequentiality?
is lost (for transducers,Oncina et al 1993)2 unless they are ??-distinguishable?
(Clark and Thollard 2004)3 .
.
.86Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsState-merging as inference rulesStrictly k-Local languages (Garcia et al 1990)merge states with same incoming paths of length k?u, v ,w ?
??
: uv ,wv ,?
Prefix(L) and |v | = k?TailsL(uv) = TailsL(wv) ?
L87Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsState-merging as inference rules0-Reversible languages (Angluin 1982)recursively eliminate reverse non-determinism?u, v ,w , y ?
??
: uv ,wv , uy ?
L ?
wy ?
L88Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsState-merging summary1 Distinctions maintained in the prefix tree are lost by statemerging, which results in generalizations.2 The choice of partition corresponds to the generalizationstrategy (i.e.
which distinctions will be maintained and whichwill be lost)Gleitman (1990:12):The trouble is that an observer who notices everythingcan learn nothing for there is no end of categories knownand constructible to describe a situation [emphasis inoriginal].89Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsResults for regular languagesDistribution-free with positive dataIdentification in the limit from positive data1 strictly k-local languages (each state corresponds to suffixes ofup to length k) (Garcia et al 1990)2 reversible languages (acceptors are both forward and reversek-deterministic for some k) (Angluin 1982)3 k-contextual languages (Muggleton 1990)4 .
.
.90Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsRegular relationsRegular relations in CL1 transliteration2 translation3 .
.
.4 anything with finite state transducers91Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsOSTIA (Oncina et al 1993)distribution-free with positive dataOSTIA1 identifies subsequential functions in the limit from positivedata.2 Merges states greedily unless subsequentiality is violated3 If the function is partial, exactness is guaranteed only wherethe function is defined.92Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsOSTIA (Oncina et al 1993)Subsequential relations1 are a subclass of the regular relations, recognizing functions.2 are those which are recognized by subsequential transducers,which are determinstic on the input and which have an?output?
string associated with every state.3 have a canonical form.4 have been generalized to permit up to p outputs for eachinput (Mohri 1997).93Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsOSTIA for learning phonological rulesGildea and Jurafsky 19961 Show that OSTIA doesn?t learn the English tapping rule orGerman word-final devoicing rule from data present inadapted dictionaries of English or German2 Applied additional phonologically motivated heuristics toimprove state-merging choices.What about well-defined subclasses of subsequential relations?94Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsWeighted finite-state automatanon-distribution-free with positive dataThe problemGiven a finite multiset of words drawn independently from thetarget distribution, what grammar accurately describes thedistribution?TheoremThe class of distributions describable with Non-deterministicProbabilistic Finite-State Automata (NPFA) exactly matches theclass of distributions describable with Hidden Markov Models(Vidal et al 2005).95Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsMaximum Likelihood EstimationA : 13a : 0b : 13c : 13MA : 15a : 15b : 15c : 15M??
{bc}M represents a family ofdistributions with 4 parameters.M?
represents a particulardistribution in this family.TheoremFor a sample S and deterministic finite-state acceptor M, counting theparse of S through M and normalizing at each state optimizes themaximum-likelihood estimate.
(Vidal et.
al 2005, de la Higuera 2010) 96Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsStrictly 2-Local Distributions are bigram models?a?b?c ?abcabcabcabcFigure: The structure of a bigram model.
The 16 parameters of thismodel are given by associating probabilities to each transition and to?ending?
at each state.97Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsSubregular distributionsRegularFiniteSome well-definedsubregular class1 When the structure of a Deterministic FSA is known inadvance, MLE is easy to do.2 The DFA represents a subregular class of distributions.98Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsStrictly Piecewise Distributions1 N-gram models can?t describe long-distance dependencies.Long-distance dependencies in phonology1 Consonantal harmony(Jensen 1974, Odden 1994, Hansson 2001, Rose and Walker2004, and many others)2 Vowel harmony(Ringen 1988, Bakovic?
2000, and many others)99Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsSibilant Harmony example from Samala (Inesen?oChumash)[StojonowonowaS] ?it stood upright?
(Applegate 1972:72)cf.
*[stojonowonowaS] andcf.
*[Stojonowonowas]Hypothesis: *[stojonowonowaS] and *[Stojonowonowas] areill-formed because the discontiguous subsequences sS and Ss areill-formed.100Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsStrictly Piecewise languagesRogers et al 20101 solely make distinctions on the basis of potentiallydiscontiguous subsequences up to some length k2 are mathematically natural.
They have several chacterizationsin terms of formal language theory, automata theory, logic,model theory, and the3 algebraic theory of automata (Fu et al 2011)101Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsStrictly Piecewise DistributionsHeinz and Rogers 20101 are defined in terms of the factored automata-theoreticrepresentations (Rogers et al 2010)2 along with the co-emission probability as the product (Vidal etal.
2005)3 Estimation over the factors permits learnability of the patternslike the ones in Samala.Example with ?
= {a, b, c} and k = 2.A0 A1 B0 B1 C0 C1?
?a b cabcabcabcbcacab102Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsSP2 learning results for ChumashTraining corpus 4800 words from a dictionary of SamalaxP(x | y <)s >ts S >tSys 0.0325 0.0051 0.0013 0.0002ts 0.0212 0.0114 0.0008 0.S 0.0011 0.
0.067 0.0359>tS 0.0006 0.
0.0458 0.0314Table: SP2 probabilities of sibilant occuring sometime after another one(collapsing laryngeal distinctions)103Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsLearning larger classes of regular distributionsMore non-distribution-free with positive dataThe class of distributions describable with PDFA1 are identifiable in the limit with probability one (de la Higueraand Thollard 2000).2 are learnable in modified-PAC setting (Clark and Thollard2004).3 The algorithms presented employ state-merging methods.1 This is a (much!)
larger class than that which is describablewith n-gram distributions or with SP distributions.2 To my knowledge these approaches have not been applied totasks in CL.104Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsSummary#1.
Define ?learning?
so that large regions can be learnedContext-SensitiveMildlyContext-SensitiveContext-FreeRegularFiniteYoruba copyingKobele 2006Swiss GermanShieber 1985English nested embeddingChomsky 1957English consonant clustersClements and Keyser 1983 Kwakiutl stressBach 1975Chumash sibilant harmonyApplegate 1972Oncina et al 1993, de la Higuera and Thollard 2000, Clark andThollard 2004, .
.
.105Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsSummary#2.
Target non-superfinite cross-cutting classesRecursively EnumerableContext-SensitiveMildlyContext-SensitiveContext-FreeRegularFiniteYoruba copyingKobele 2006Swiss GermanShieber 1985English nested embeddingChomsky 1957English consonant clustersClements and Keyser 1983 Kwakiutl stressBach 1975Chumash sibilant harmonyApplegate 1972Angluin 1982, Muggleton 1990, Garcia et al 1990, Heinz 2010, .
.
.106Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsHave we put the cart before the horse?1 So far we have discussed algorithms that learn various classesof languages.2 But shouldn?t we first know which classes are relevant for ourgoals?3 E.g.
for phonology, while ?being regular?
may be a necessaryproperty of phonological patterns, it certainly is not sufficient.107Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsHave we put the cart before the horse?Research strategyPatterns ?
Characterizations ?
Learning algorithms1 Identify the range and kind of patterns (linguistics).2 Characterize the range and kind of patterns (computationallinguistics).3 Create learning algorithms for these classes, prove theirsuccess in a variety of settings, and otherwise demonstratetheir success (grammatical inference, formal learning theory,computational linguistics)108Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsSubregular classes of regular setsRegularStar-Free=NonCountingTSL LTTLT PTSL SPProper inclusionrelationships amongsubregular languageclasses.instructor?s hunch forphonologyTSL Tier-based Strictly Local PT Piecewise TestableLTT Locally Threshold Testable SL Strictly LocalLT Locally Testable SP Strictly Piecewise(McNaughton and Papert 1971, Simon 1975, Rogers and Pullum 2007, inpress, Rogers et al 2010, Heinz et al 2011)109Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsConclusion to section 2 part 11 State-merging is a well-studied strategy for inferringautomata, including acceptors, transducers, and weightedacceptors and transducers.2 It has yielded theoretical results in many learning frameworksincluding both distribution-free and non-distribution-freelearning frameworks.110Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsConclusion to section 2 part 21 Many subclasses of regular languages are learnable even in thehardest learning settings.2 Recent advances yield algorithms for large classes(probabilistic DFAs)3 Computational linguists can explore which are relevant tonatural language and consequently which are useful for NLP!4 There is a rich literature in GI which speaks to these classes,and how such patterns in these classes can be learned.111Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsOverviewEmpirical grammatical inferenceFamily of languagesInformation contained in inputOverview of systemsEvaluation issuesFrom empirical to formal GI112Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsIntroductionLanguage learningStarting from family of languagesGiven set of samplesIdentify language that is used to generate samplesFormal grammatical inferenceIdentify family of languages that can be learned efficientlyUnder certain restrictionsEmpirical grammatical inferenceExact underlying family of languages is unknownTarget language is approximation113Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsEmpirical GITry to identify language given samplesE.g.
sentences (syntax), words (morphology), .
.
.Underlying language class is unknownFor algorithm we still need to make a choiceIf identification is impossible, provide approximationEvaluation of empirical GI is different from formal GI114Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsFamily of languagesWhat is the underlying family of languages?Choice has impact on learning algorithmMany possibilitiesUse simple, fixed structures (n-grams)Find probabilitiesExtract structure from treebanksSlightly more flexible structureFind probabilitiesLearn structureFlexible structureFind probabilities115Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsN-grams1 Starting from a plain text or collection of texts (corpus)2 Extract all subsequences of length n (n-grams)3 Count occurrences of n-grams in texts4 Assign probabilities to each n-gram based on countsIssuesUnseen n-gramsBack-off: use n-grams with smaller nSmoothing: adjust probabilities for unseen n-grams116Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsUsing n-gram modelsHow likely is the sentence ?John likes Mary?
?Unigram language modelP(John likes Mary) ?
P(John)P(likes)P(Mary)Bigram language modelP(John likes Mary) ?
P(John|?s?
)P(likes|John)P(Mary|likes)Trigram language modelP(John likes Mary) ?P(John|?s??s?
)P(likes|?s?John)P(Mary|John likes)N-gram language modelP(wn1 ) ?
?nk=1 P(wk |wk?1k?N+1)N-grams provide a probability for each sequenceProbability describes how well sequence fits language117Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsExtract structure from treebanks1 Starting from a treebank (sentences with structure)2 Extract grammar rules that are used to create tree structuresFor instance, context-free grammars (Charniak 1993)or sub-trees (Data-Oriented Parsing) (Bod 1998)3 Count occurrences of grammar rules in treebank4 Assign probabilities to grammar rules based on countsIssuesOver-generalization, ?incorrect?
probabilitiesAdd information on applicability of grammar rules(Johnson 1998)Reestimate probabilities (EM)(Dempster et al1977, Lari and Young 1990)118Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsExtract structure from treeVBPRPHeVB1adoresVB2VBlisteningTOTOtoNNmusicVB ?PRP VB1 VB2PRP?HeVB1?adoresVB2?VB TOVB ?listeningTO ?TO NNTO ?toNN ?musicExtract counts from treebank ?
probabilitiesReestimate probabilitiesImprove fit of grammar and sentences119Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsLearn structure1 Starting from a corpus2 Identify regularities that may serve as grammar rules3 Output:Structure assigned to sentences ?
extract grammarExtracted grammar rules (and probabilities) ?
parseIssuesLearning system has to deal with bothflexibility in structureprobabilities of structure120Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsSummarizing fixed versus flexible structureFixed versus flexible is really a sliding scaleLanguage modelling using n-gramsStructure is very simple and very rigidRequires plain sequences as inputCorresponds to k-testable languages (Garc?
?a 1990)Language modelling using extracted grammar rulesStructure is more flexible, but restricted by treebankRequires structured sequences as inputCorresponds to e.g.
(limited) context-free languages?Learning structure?Structure is flexible, restricted by learning algorithmRequires plain sequences as inputCorresponds to e.g.
context-free languages121Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsEmpirical grammatical inferenceChoices:What type of grammar are we learning?Regular languageK -testable language (n-grams)Context-free language.
.
.What kind of input do we require?Sequence of words (sentence)Sequence of part-of-speech tags(Partial) tree structures.
.
.What kind of output do we want?Structured version of inputExplicit grammarBinary or n-ary (context-free rules).
.
.122Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsOverview of systemsEMILEAlignment-Based Learning (ABL)ADIOSCCM+DMVU-DOP.
.
.123Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsUnderlying approachGiven a collection of plain sentencesOn what basis are we going to assign structure?Should structure be linguistically motivated?or similar to what linguists would assign?Perhaps we can use tests for constituency to find structure124Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsSubstitutabilityElements of the same type are substitutableTest for constituency (Harris, 1951)What is (a family fare)NPReplace noun phrase with another noun phraseWhat is (the payload of an African Swallow)NPLearning by reversing testWhat is (a family fare)XWhat is (the payload of an African Swallow)X125Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsEMILELearns context-free grammarsUsing plain sentencesOriginally used to show formal learnabilityof (a form of) Categorial Grammars in a PAC learning setting(Adriaans 1992, Adriaans and Vervoort 2002, Vervoort 2000)Approach1 Starting from simple sentencesidentify recurring subsequences2 Store recurring subsequences and contexts3 Introduce grammar rules when there is enough evidencePractical implementation allows for several constraintsContext length, subsequence length, .
.
.126Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsExample matrixJohn walksMary walksJohn sees Mary(.)
walks John (.)
(.)
sees Mary .
.
.
contextsJohn x x .
.
.walks x .
.
.Mary x .
.
.sees .
.
.... ... ... ... .
.
.terms127Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsLearn grammar rulesTerms that share (approximately) same context are clustered?John?
and ?Mary?
are grouped togetherOccurrences of terms in cluster are replaced by new symbolModified sequences may again contain terms/contextsTerms may consist of multiple wordsExampleJohn walks ?X walksMary walks ?X walksJohn sees Mary ?X sees XMary slaps John?X slaps X?sees?
and ?slaps?
now also share the same context128Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsAlignment-Based Learning (ABL)Based on substitutability testUsing plain sentencesSimilar to EMILE, butClustered terms are not explicitly replaced by symbolTerms and contexts are always separatedAll terms are considered (and only selected afterwards)Output is structured version of input or grammar(van Zaanen 2000a, b, 2002)129Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsAlignment-Based Learning (ABL)Corpus AlignmentLearningHypothesisSpaceHypothesisSpaceSelectionLearningStructuredCorpusStructuredCorpusGrammarExtractionGrammar130Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsAlignment-Based Learning (ABL)Alignment learningAlign pairs of sentencesUnequal parts of sentences are stored as hypotheses(Clustering)Group hypotheses in same context togetherSelection learningRemove overlapping hypotheses131Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsAlignment learningAlign pairs of sentencesusing edit distance (Wagner and Fischer 1974)or suffixtrees (Geertzen and van Zaanen 2004, Ukkonen 1995)Unequal parts of sentences are stored as hypothesesAlign all sentences in a corpus to all othersExample(Y1 I need (X1a dinner during the flight)X1)Y1(Z1 I need)Z1 (X1to return on (Z2tuesday)Z2)X1(Y1(Z1he wants)Z1 to return on (Z2wednesday)Z2)Y1132Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsSelection LearningAlignment learning can generate overlapping bracketsUnderlying grammar is considered context-freeStructure describes parse according to underlying grammar?Wrong?
brackets have to be removedBased on e.g.
chronological order or statisticsExamplefrom (Y1Tilburg (X2to)Y1 Portland)X2from (X1Portland (Y2to)X1 Tilburg)Y2133Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsADIOSAutomatic Distillation of Structure (ADIOS) (Solan 2005)Idea1 Represent language as a graph2 Compress graph3 As long as possible, find significant patterns in pathsUsing substitutability and significance tests4 (Recursion may be added as a post-processing step)134Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsGraphsees MaryS John walks EMary slaps John135Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsPhases1 InitializationLoad all sentences (as paths) in the graph2 Pattern distilationFind sub-pathsshared by significant number of partially-aligned pathsusing motif-extraction (MEX) algorithm3 GeneralizationGroup all nodes that occur in same pattern togetherCluster words/subsequences similarly to EMILE4 Repeat 2 and 3 until no new patterns are found136Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsGraphS e1 e2 e3 e4 e5 EIf e2 e3 e4 is a significant patternS e1 e2 e3 e4 e5 E137Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsMEXCompute probabilities depending on in-/out-degree of nodesPR(e1; e2) =# paths from e1 to e2# paths to e1PR(e1; e3) =# paths from e1 to e3# paths to e1DR(e1; e3) =PR(e1; e4)PR(e1; e3)PR describes path to the rightsimilarly PL describes path to the leftSignificance is computed based on DR and DL wrt parameterInformally: find significant changes in number of pathsPick most significant pattern138Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsConstituent-Context Model (CCM)Consider all possible binary tree structures on POS sequencesDefine a probability distribution over the possible bracketingsA bracketing is a particular structure on a sequenceP(s,B) = Pbin(B)P(s|B)P(s|B) = ?i ,j :i?jPspan(sij |Bij)Pctx(si?1, sj |Bij)Run (iterative) Expectation-Maximization (EM) algorithmto maximize likelihood ?s?SP(s)(Klein 2002)139Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsDependency Model with Valence (DMV)DMV aims to learn dependency relationsin contrast to CCM which learns context-free grammar rulesDependency parse links words in a head-dependent relationModel describes likelihood ofleft dependenciesright dependenciesstop condition (no more dependencies)Again, iterative EM is used to maximize likelihood of corpus140Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsCCM+DMVCCM and DMV can be combinedBoth models have different view on structureResults of combined system are better than either systemsStrengths of both systems are combined(Klein 2004)141Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsU-DOPSimilar to CCM in that itfinds probability distribution over ?all?
structuresuses POS sequencesU-DOP uses Data-Oriented Parsing (DOP) as formalismExtends probabilistic model of context-free grammarsRequires practical implementation choicesRandom sampling due to huge size of search space(Bod 2006a, b)Procedure1 Generate all possible binary trees on example sentences2 Extract all subtrees3 Estimate probabilities on subtrees using EM142Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsSubtreesSNPPNVPV NPSNP VPV NPSNPPNVPSNP VPVPV NPNPPNRemove either all or no elements on a levelLeads to many subtreesEach subtree receives a probabilityLonger distance dependencies may be modeled143Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsParsingSubtrees can be recombined into a larger treeSimilar to context-free grammar rulesSame parse may be created using different derivationsStatistical model has to take this into accountExampleSNP VPV NP?
NPPN?
NPPN= SNPPNVPV NPPN144Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsUnderlying ideaU-DOP works because span of subtrees reoccur in a corpusLikelihood of ?useful?
spans increaseHence, likelihood of contexts (also subtrees) increaseEssentially, U-DOP uses implied substitutabilitywhile system leans heavily on probabilities145Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsEvaluationBasetreebankExtractsentencesComparetreebanksResultsPlaincorpusLearningsystemLearnedtreebankRecall (completeness)Precision (correctness)F-Score (combination of Precision and Recall)(van Zaanen and Adriaans 2001)146Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsEvaluation settingsAir Travel Information System (ATIS)Taken from Penn Treebank II568 English sentencesExamplelist the flights from baltimore to seattle that stop in minneapolisdoes this flight serve dinnerthe flight should arrive at eleven a.m. tomorrowwhat airline is this147Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsResults on ATISMicro Macro Macro2Precision 47.01 46.18 46.18Recall 44.94 50.98 50.98F-Score 44.60 47.10 48.46ExplanationMicro Count constituents, weighted average per sentenceMacro Count constituents and average per sentenceMacro2 Compute Macro Precision/Recall, average at end148Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsResults on ATISremove remove removesentence empty bothMicro Precision 47.01 47.67 77.10 79.07Micro Recall 44.94 45.30 44.95 45.29Micro F-Score 44.60 45.09 55.31 56.13Macro Precision 46.18 47.66 77.08 81.18Macro Recall 50.98 52.96 51.07 52.80Macro F-Score 47.10 48.62 60.00 62.47Macro2 F-Score 48.46 50.17 61.43 63.99Example(bla bla bla)?bla bla blabla () bla ?bla bla(bla () bla) ?bla bla149Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsEvaluation insightsNo standard evaluation existsbut de facto evaluation datasets ariseATIS (van Zaanen and Adriaans 2001)WSJ10, WSJ40 (WSJ with sentence length limitations)NEGRA10 (German)CTB10 (Chinese)Systems have different input/outputEvaluation settings influence resultsDifferent metrics (micro/macro/macro2)Included constituents (sentence/empty)Formal grammatical inference does not have this problemEvaluation performed through formal proofs150Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsContext-sensitive grammarsLearning context-free grammars is hardIs learning context-sensitive grammars impossible?That dependsTo what degree is the grammar context-sensitive?We may not need ?full?
context-sensitivenessGrammar rules: ?A?
?
??
?Mildly context-sensitive grammars may be enough for NL(Huybrechts 1984, Shieber 1985)Perhaps the full power of context-freeness is not needed151Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsFamily of languagesRegCFCSUnres bFamily to learn152Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsLearning context-sensitive languagesOpen research areaSome work has already been doneAugmented Regular Expressions (Alque?zar 1997)Variants of substitutability (Yoshinaka 2009)Distributional Lattice Grammars (Clark 2010)153Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsRelationship between empirical and formal GIIs there a relationship between empirical GI and formal GI?Example: consider the case of substitutabilityThere are situations in which substitutability breaks:John eats meatJohn eats muchThis suggests that learning based on substitutabilitylearns a different family of languages (not CFG)Non-terminally separated (NTS) languagesSubclass of deterministic context-free grammars154Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsLearning NTS grammarsGrammar G=?
?,V ,P ,S?
is NTS?
is vocabularyV is set of non-terminalsP is set of production rulesS ?
V is the start symbolAdditional restriction:If N ?
VN ??
??
?M ??
?then N ??
?M?In other words:non-terminals correspond exactly with substitutability(Clark and Eyraud 2005, Clark 2006, Clark and Eyraud 2007)155Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsLearning NTS grammarsIt can be shown that NTS grammars areidentifiable in the limitPAC learnableUnfortunately, natural language is not an NTS languageUltimate goal:Find family of languages that fits natural languageand is learnable in the right learning setting156Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsFormal GI and empirical GIRelation between formal GI and empirical GIFormal GI can show learnabilityUnder certain conditionsEmprical GI tries to learn structure from real dataPractically shows possibilities and limitationsUltimate aim: Find family of languages that islearnable under different conditionsfits natural languages157Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patternsCONCLUSIONS1 There have been new strong positive results in a recent pastfor all the cases mentioned (subclasses of regular, PFA,transducers, CFGs, MCSGs)2 Look for ICGI!
It?s the conference where these exciting resultshappen (as well as exciting challenges, competitions,benchmarks etc.
)3 The use of GI techniques both in computational linguisticsand natural language processing is taking place.4 The future is bright!158ReferencesP.
W. Adriaans and M. van Zaanen.
2004.
Computationalgrammar induction for linguists.
Grammars, 7:57?68.Special issue with the theme ?Grammar Induction?.P.
W. Adriaans and M. van Zaanen.
2006.
Computa-tional grammatical inference.
In D. E. Holmes andL.
C. Jain, editors, Innovations in Machine Learning,volume 194 of Studies in Fuzziness and Soft Com-puting, chapter 7.
Springer-Verlag, Berlin Heidelberg,Germany.
To be published.
ISBN: 3-540-30609-9.P.
W. Adriaans and M. Vervoort.
2002.
The EMILE 4.1grammar induction toolbox.
In P. W. Adriaans, H. Fer-nau, and M. van Zaanen, editors, Grammatical Infer-ence: Algorithms and Applications (ICGI); Amster-dam, the Netherlands, volume 2482 of Lecture Notesin AI, pages 293?295, Berlin Heidelberg, Germany,September 23?25.
Springer-Verlag.P.
W. Adriaans.
1992.
Language Learning from a Cat-egorial Perspective.
Ph.D. thesis, University of Ams-terdam, Amsterdam, the Netherlands, November.R.
Alque?zar and A. Sanfeliu.
1997.
Recognition andlearning of a class of context-sensitive languages de-scribed by augmented regular expressions.
PatternRecognition, 30(1):163?182.D.
Angluin and M. Kharitonov.
1991.
When won?t mem-bership queries help?
In Proceedings of 24th ACMSymposium on Theory of Computing, pages 444?454,New York.
ACM Press.D.
Angluin.
1981.
A note on the number of queriesneeded to identify regular languages.
Information andControl, 51:76?87.D.
Angluin.
1982.
Inference of reversible languages.Journal for the Association of Computing Machinery,29(3):741?765.D.
Angluin.
1987a.
Learning regular sets fromqueries and counterexamples.
Information and Con-trol, 39:337?350.D.
Angluin.
1987b.
Queries and concept learning.
Ma-chine Learning Journal, 2:319?342.D.
Angluin.
1988.
Identifying languages from stochas-tic examples.
Technical Report YALEU/DCS/RR-614,Yale University, March.R.
B. Applegate.
1972.
Inesen?o Chumash Grammar.Ph.D.
thesis, University of California, Berkeley.L.
Beccera-Bonache, C. Bibire, and A. Horia Dediu.2005.
Learning DFA from corrections.
In HenningFernau, editor, Proceedings of the Workshop on The-oretical Aspects of Grammar Induction (TAGI), WSI-2005-14, pages 1?11.
Technical Report, University ofTu?bingen.L.
Becerra-Bonache, C. de la Higuera, J. C. Janodet, andF.
Tantini.
2008.
Learning balls of strings from editcorrections.
Journal of Machine Learning Research,9:1841?1870.D.
Be?chet, A. Dikovsky, and A. Fore?t.
2011.
Surles ite?rations disperse?es et les choix itr?e?s pourl?apprentissage incre?mental des types dans les gram-maires de de?pendances.
In Proceedings of Confe?renced?Apprentissage.A.
Blumer, A. Ehrenfeucht, D. Haussler, and M. K.Warmuth.
1989.
Learnability and the Vapnik-Chervonenkis dimension.
J. ACM, 36(4):929?965.R.
Bod.
1998.
Beyond Grammar?An Experience-Based Theory of Language, volume 88 of CSLI Lec-ture Notes.
Center for Study of Language and Infor-mation (CSLI) Publications, Stanford:CA, USA.R.
Bod.
2006a.
An all-subtrees approach to unsuper-vised parsing.
In Proceedings of the 21st InternationalConference on Computational Linguistics (COLING)and 44th Annual Meeting of the Association of Com-putational Linguistics (ACL); Sydney, Australia, pages865?872.
Association for Computational Linguistics.R.
Bod.
2006b.
Unsupervised parsing with u-dop.
InCoNLL-X ?06: Proceedings of the Tenth Conferenceon Computational Natural Language Learning, pages85?92, Morristown, NJ, USA.
Association for Com-putational Linguistics.R.
C. Carrasco and J. Oncina.
1994.
Learning stochasticregular grammars by means of a state merging method.In R. C. Carrasco and J. Oncina, editors, GrammaticalInference and Applications, Proceedings of ICGI ?94,number 862 in LNAI, pages 139?150.
Springer-Verlag.E.
Charniak.
1993.
Statistical Language Learning.Massachusetts Institute of Technology Press, Cam-bridge:MA, USA and London, UK.N.
Chater and P. Vita?nyi.
2007.
?ideal learning?
of natu-ral language: Positive results about learning from pos-itive evidence.
Journal of Mathematical Psychology,51(3):135?163.N.
Chomsky.
1957.
Syntactic Structures.
Mouton & Co.,Printers, The Hague.A.
Clark and R. Eyraud.
2005.
Identification in the limitof substitutable context-free languages.
In S. Jain,H.
U. Simon, and E. Tomita, editors, AlgorithmicLearning Theory: 16th International Conference, ALT2005, volume 3734 of Lecture Notes in Computer Sci-ence, pages 283?296, Berlin Heidelberg, Germany.Springer-Verlag.A.
Clark and R. Eyraud.
2007.
Polynomial identificationin the limit of substitutable context-free languages.Journal of Machine Learning Research, 8:1725?1745.A.
Clark and F. Thollard.
2004.
Pac-learnability of prob-abilistic deterministic finite state automata.
Journal ofMachine Learning Research, 5:473?497.A.
Clark.
2006.
PAC-learning unambiguous NTS lan-guages.
In Y. Sakakibara, S. Kobayashi, K. Sato,T.
Nishino, and E. Tomita, editors, Eighth Interna-tional Colloquium on Grammatical Inference, (ICGI);Tokyo, Japan, number 4201 in Lecture Notes in AI,pages 59?71, Berlin Heidelberg, Germany.
Springer-Verlag.A.
Clark.
2010.
Efficient, correct, unsupervised learn-ing of context-sensitive languages.
In CoNLL ?10:Proceedings of the Fourteenth Conference on Com-putational Natural Language Learning, pages 28?37,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.C.
de la Higuera and J. Oncina.
2004.
Learning proba-bilistic finite automata.
In G. Paliouras and Y. Sakak-ibara, editors, Grammatical Inference: Algorithms andApplications, Proceedings of ICGI ?04, volume 3264of LNAI, pages 175?186.
Springer-Verlag.C.
de la Higuera and F. Thollard.
2000.
Identification inthe limit with probability one of stochastic determinis-tic finite automata.
In A.L.
de Oliveira, editor, Gram-matical Inference: Algorithms and Applications, Pro-ceedings of ICGI ?00, volume 1891 of Lecture Notesin Computer Science, pages 15?24.
Springer-Verlag.C.
de la Higuera, J.-C. Janodet, and F. Tantini.
2008.Learning languages from bounded resources: the caseof the DFA and the balls of strings.
In A. Clark,F.
Coste, and L. Miclet, editors, Grammatical In-ference: Algorithms and Applications, Proceedingsof ICGI ?08, volume 5278 of LNCS, pages 43?56.Springer-Verlag.C.
de la Higuera.
1997.
Characteristic sets for polyno-mial grammatical inference.
Machine Learning Jour-nal, 27:125?138.C.
de la Higuera.
2010.
Grammatical inference: learn-ing automata and grammars.
Cambridge UniversityPress, Cambridge, UK.A.P.
Dempster, N.M. Laird, and D.B.
Rubin.
1977.
Max-imum likelihood from incomplete data via the em al-gorithm.
Journal of the Royal Statistical Society.
Se-ries B (Methodological), 39(1):1?38.Matt Edlefsen, Dylan Leeman, Nathan Myers, NathanielSmith, Molly Visscher, and David Wellcome.
2008.Deciding strictly local (SL) languages.
In Jon Breit-enbucher, editor, Proceedings of the Midstates Con-ference for Undergraduate Research in Computer Sci-ence and Mathematics, pages 66?73.Jie Fu, J. Heinz, and Herbert Tanner.
2011.
An alge-braic characterization of strictly piecewise languages.In The 8th Annual Conference on Theory and Applica-tions of Models of Computation, volume 6648 of Lec-ture Notes in Computer Science.
Springer-Verlag.P.
Garc?
?a and J. Ruiz.
2004.
Learning k-testableand k-piecewise testable languages from positive data.Grammars, 7:125?140.P.
Garcia and E. Vidal.
1990.
Inference of k-testablelanguages in the strict sense and application to syntac-tic pattern recognition.
IEEE Transactions on PatternAnalysis and Machine Intelligence, 12:920?925.P.
Garcia, E. Vidal, and J. Oncina.
1990.
Learning lo-cally testable languages in the strict sense.
In Proceed-ings of the Workshop on Algorithmic Learning Theory,pages 325?338.G.Clements and J. Keyser.
1983.
CV phonology: a gen-erative theory of the syllable.
Cambridge, MA: MITPress.J.
Geertzen and M. van Zaanen.
2004.
Grammati-cal inference using suffix trees.
In G. Paliouras andY.
Sakakibara, editors, Grammatical Inference: Algo-rithms and Applications: Seventh International Collo-quium, (ICGI); Athens, Greece, volume 3264 of Lec-ture Notes in AI, pages 163?174, Berlin Heidelberg,Germany, October 11?13.
Springer-Verlag.D.
Gildea and D. Jurafsky.
1996.
Learning bias andphonological-rule induction.
Computational Linguis-tics, 24(4).L.
Gleitman.
1990.
The structural sources of verb mean-ings.
Language Acquisition, 1(1):3?55.E.
M. Gold.
1967.
Language identification in the limit.Information and Control, 10(5):447?474.E.
M. Gold.
1978.
Complexity of automaton identi-fication from given data.
Information and Control,37:302?320.K.
C. Hansen and L. E. Hansen.
1969.
Pintupi phonol-ogy.
Oceanic Linguistics, 8:153?170.Z.
S. Harris.
1951.
Structural Linguistics.
University ofChicago Press, Chicago:IL, USA and London, UK, 7th(1966) edition.
Formerly Entitled: Methods in Struc-tural Linguistics.B.
Hayes.
1995.
Metrical Stress Theory.
Chicago Uni-versity Press.J.
Heinz and J. Rogers.
2010.
Estimating strictly piece-wise distributions.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 886?896, Uppsala, Sweden, July.
Associa-tion for Computational Linguistics.J.
Heinz.
2008.
Left-to-right and right-to-left iterativelanguages.
In Alexander Clark, Franc?ois Coste, andLauren Miclet, editors, Grammatical Inference: Al-gorithms and Applications, 9th International Collo-quium, volume 5278 of Lecture Notes in ComputerScience, pages 84?97.
Springer.J.
Heinz.
2009.
On the role of locality in learning stresspatterns.
Phonology, 26(2):303?351.J.
Heinz.
2010.
String extension learning.
In Proceed-ings of the 48th Annual Meeting of the Association forComputational Linguistics, pages 897?906, Uppsala,Sweden, July.
Association for Computational Linguis-tics.R.
M. A. C. Huybrechts.
1984.
The weak adequacyof context-free phrase structure grammar.
In G. J.de Haan, M. Trommelen, and W. Zonneveld, editors,Van periferie naar kern, pages 81?99.
Foris, Dor-drecht, the Netherlands.M.
Johnson.
1998.
PCFG models of linguistic tree rep-resentations.
Computational Linguistics, 24(4):613?632, December.A.
Kasprzik and T. Ko?tzing.
2010.
String extensionlearning using lattices.
In Henning Fernau Adrian-Horia Dediu and Carlos Mart?
?n-Vide, editors, Pro-ceedings of the 4th International Conference on Lan-guage and Automata Theory and Applications (LATA2010), volume 6031 of Lecture Notes in Computer Sci-ence, pages 380?391, Trier, Germany.
Springer.M.
Kearns and L. Valiant.
1989.
Cryptographic lim-itations on learning boolean formulae and finite au-tomata.
In 21st ACM Symposium on Theory of Com-puting, pages 433?444.M.
J. Kearns and U. Vazirani.
1994.
An Introduction toComputational Learning Theory.
MIT press.D.
Klein and C. D. Manning.
2002.
A generativeconstituent-context model for improved grammar in-duction.
In 40th Annual Meeting of the Associationfor Computational Linguistics; Philadelphia:PA, USA,pages 128?135.
Association for Computational Lin-guistics, July.
yes.D.
Klein.
2004.
Corpus-based induction of syntacticstructure: Models of dependency and constituency.
In42th Annual Meeting of the Association for Computa-tional Linguistics; Barcelona, Spain, pages 479?486.G.
Kobele.
2006.
Generating Copies: An Investigationinto Structural Identity in Language and Grammar.Ph.D.
thesis, University of California, Los Angeles.K.
Lari and S. J.
Young.
1990.
The estimation of stochas-tic context-free grammars using the inside-outside al-gorithm.
Computer Speech and Language, 4(35?56).R.
McNaughton and S. Papert.
1971.
Counter-Free Au-tomata.
MIT Press.M.
Mohri.
1997.
Finite-state transducers in languageand speech processing.
Computational Linguistics,23(2):269?311.S.
Muggleton.
1990.
Inductive Acquisition of ExpertKnowledge.
Addison-Wesley.J.
Oncina and P.
Garc??a.
1992.
Identifying regular lan-guages in polynomial time.
In H. Bunke, editor, Ad-vances in Structural and Syntactic Pattern Recogni-tion, volume 5 of Series in Machine Perception andArtificial Intelligence, pages 99?108.
World Scientific.J.
Oncina, P.
Garc?
?a, and E. Vidal.
1993.
Learning sub-sequential transducers for pattern recognition tasks.IEEE Transactions on Pattern Analysis and MachineIntelligence, 15:448?458, May.L.
Pitt.
1985.
Probabilistic Inductive Inference.
Ph.D.thesis, Yale University.
Computer Science Depart-ment, TR-400.L.
Pitt.
1989.
Inductive inference, DFA?s, and compu-tational complexity.
In Analogical and Inductive In-ference, number 397 in LNAI, pages 18?44.
Springer-Verlag.J.
Rogers and G. Pullum.
to appear.
Aural pattern recog-nition experiments and the subregular hierarchy.
Jour-nal of Logic, Language and Information.J.
Rogers, J. Heinz, Gil Bailey, Matt Edlefsen, MollyVisscher, David Wellcome, and Sean Wibel.
2010.On languages piecewise testable in the strict sense.
InChristian Ebert, Gerhard Ja?ger, and Jens Michaelis,editors, The Mathematics of Language, volume 6149of Lecture Notes in Artifical Intelligence, pages 255?265.
Springer.S.
M. Shieber.
1985.
Evidence against the context-freeness of natural language.
Linguistics and Philoso-phy, 8(3):333?343.I.
Simon.
1975.
Piecewise testable events.
In AutomataTheory and Formal Languages, pages 214?222.Z.
Solan, D. Horn, E. Ruppin, and S. Edelman.
2005.Unsupervised learning of natural languages.
Proceed-ings of the National Academy of Sciences of the UnitedStates of America, 102(33):11629?11634, August.A.
Stolcke.
1994.
Bayesian Learning of ProbabilisticLanguage Models.
Ph.D. thesis, University of Califor-nia, Berkeley.I.
Tellier.
2008.
How to split recursive automata.
InICGI, pages 200?212.E.
Ukkonen.
1995.
On-line construction of suffix trees.Algorithmica, 14:249?260.L.
G. Valiant.
1984.
A theory of the learnable.
Commu-nications of the Association for Computing Machinery,27(11):1134?1142.M.
van Zaanen and P. W. Adriaans.
2001.
Alignment-Based Learning versus EMILE: A comparison.
InProceedings of the Belgian-Dutch Conference on Ar-tificial Intelligence (BNAIC); Amsterdam, the Nether-lands, pages 315?322, October.M.
van Zaanen.
2000a.
ABL: Alignment-BasedLearning.
In Proceedings of the 18th InternationalConference on Computational Linguistics (COLING);Saarbru?cken, Germany, pages 961?967.
Associationfor Computational Linguistics, July 31?August 4.M.
van Zaanen.
2000b.
Bootstrapping syntax and recur-sion using Alignment-Based Learning.
In P. Langley,editor, Proceedings of the Seventeenth InternationalConference on Machine Learning; Stanford:CA, USA,pages 1063?1070, June 29?July 2.M.
van Zaanen.
2002.
Bootstrapping Structure into Lan-guage: Alignment-Based Learning.
Ph.D. thesis, Uni-versity of Leeds, Leeds, UK, January.Marco R. Vervoort.
2000.
Games, Walks and Grammars.Ph.D.
thesis, University of Amsterdam, Amsterdam,the Netherlands, September.E.
Vidal, F. Thollard, C. de la Higuera, F. Casacuberta,and R. C. Carrasco.
2005a.
Probabilistic finite-statemachines-part I. IEEE Transactions on Pattern Anal-ysis and Machine Intelligence, 27(7):1013?1025.E.
Vidal, F. Thollard, C. de la Higuera, F. Casacuberta,and R. C. Carrasco.
2005b.
Probabilistic finite-statemachines-part II.
IEEE Transactions on Pattern Anal-ysis and Machine Intelligence, 27(7):1026?1039.R.
A. Wagner and M. J. Fischer.
1974.
The string-to-string correction problem.
Journal of the Associationfor Computing Machinery, 21(1):168?173.R.
Wiehagen, R. Frievalds, and E. Kinber.
1984.
On thepower of probabilistic strategies in inductive inference.Theoretical Computer Science, 28:111?133.T.
Yokomori.
2003.
Polynomial-time identification ofvery simple grammars from positive data.
TheoreticalComputer Science, 298(1):179?206.R.
Yoshinaka.
2009.
Learning mildly context-sensitivelanguages with multidimensional substitutability frompositive data.
In R. Gavalda`, G. Lugosi, T. Zeugmann,and S. Zilles, editors, Proceedings of the Workshop onAlgorithmic Learning Theory, volume 5809 of LectureNotes in Computer Science, pages 278?292.
SpringerBerlin / Heidelberg.
