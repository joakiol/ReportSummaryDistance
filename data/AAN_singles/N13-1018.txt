Proceedings of NAACL-HLT 2013, pages 179?189,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsTowards Topic Labeling with Phrase Entailment and AggregationYashar Mehdad Giuseppe Carenini Raymond T. Ng Shafiq JotyDepartment of Computer Science, University of British ColumbiaVancouver, BC, V6T 1Z4, Canada{mehdad, carenini, rng, rjoty}@cs.ubc.caAbstractWe propose a novel framework for topic la-beling that assigns the most representativephrases for a given set of sentences cover-ing the same topic.
We build an entailmentgraph over phrases that are extracted from thesentences, and use the entailment relations toidentify and select the most relevant phrases.We then aggregate those selected phrases bymeans of phrase generalization and merging.We motivate our approach by applying overconversational data, and show that our frame-work improves performance significantly overbaseline algorithms.1 IntroductionGiven text segments about the same topic written indifferent ways (i.e., language variability), topic la-beling deals with the problem of automatically gen-erating semantically meaningful labels for those textsegments.
The potential of integrating topic label-ing as a prerequisite for higher-level analysis hasbeen reported in several areas, such as summariza-tion (Harabagiu and Lacatusu, 2010; Kleinbauer etal., 2007; Dias et al 2007), information extraction(Allan, 2002) and conversation visualization (Liuet al 2012).
Moreover, the huge amount of tex-tual data generated everyday specifically in conver-sations (e.g., emails and blogs) calls for automatedmethods to analyze and re-organize them into mean-ingful coherent clusters.Table 1 shows an example of two human writtentopic labels for a topic cluster collected from a blog1,1http://slashdot.orgText: a: Where do you think the term ?Horse laugh?
comesfrom?b: And that rats also giggled when tickled.c: My hypothesis- if an animal can play, it can ?laugh?
or atleast it is familiar with the concept of ?laughing?.Many animals play.
There are various sorts of humour though.Some involve you laughing because your brain suddenly madea lots of unexpected connections.Possible extracted phrases: animals play, rats have, laugh,Horse laugh, rats also giggle, ratsHuman-authored topic labels: animals which laugh, animallaughterTable 1: Topic labeling example.and possible phrases that can be extracted from thetopic cluster using different approaches.
This ex-ample demonstrates that although most approaches(Mei et al 2007; Lau et al 2011; Branavan et al2007) advocate extracting phrase-level topic labelsfrom the text segments, topically related text seg-ments do not always contain one keyword or keyphrase that can capture the meaning of the topic.
Asshown in this example, such labels do not exist in theoriginal text and cannot be extracted using the exist-ing probabilistic models (e.g., (Mei et al 2007)).The same problem can be observed with many otherexamples.
This suggests the idea of aggregating andgenerating topic labels, instead of simply extractingthem, as a challenging scenario for this field of re-search.Moreover, to generate a label for a topic we haveto be able to capture the overall meaning of a topic.However, most current methods disregard semanticrelations, in favor of statistical models of word dis-tributions and frequencies.
This calls for the integra-179tion of semantic models for topic labeling.Towards the solution of the mentioned problems,in this paper we focus on two novel contributions:1.
Phrase aggregation.
We propose to generatetopic labels using the extracted information by pro-ducing the most representative phrases for each textsegment.
We perform this task in two steps.
First,we generalize some lexically diverse concepts inthe extracted phrases.
Second, we aggregate andgenerate new phrases that can semantically implymore than one original extracted phrase.
For ex-ample, the phrase ?rats also giggle?
and ?horselaugh?
should be merged into a new phrase ?animalslaugh?.
Although our method is still relying on ex-tracting phrases, we move beyond current extractiveapproaches, by generating new phrases through gen-eralization and aggregation of the extracted ones.2.
Building a multidirectional entailment graphover the extracted phrases to identify and select therelevant information.
We set such problem as anapplication-oriented variant of the Textual Entail-ment (TE) recognition task (Dagan and Glickman,2004), to identify the information that are seman-tically equivalent, novel, or more informative withrespect to the content of the others.
In this way, weprune the redundant and less informative text por-tions (e.g., phrases), and produce semantically in-formed phrases for the generation phase.
In the caseof the example in Table 1, we eliminate phrases suchas ?rats have?, ?rats?
and ?laugh?
while keeping?animal play?, ?Horse laugh?
and ?rats also gig-gle?.The experimental results over conversational datasets show that, in all cases, our approach outper-forms other models significantly.
Although conver-sational data are known to be challenging (Careniniet al 2011), we choose to test our method on con-versations because this is a genre in which topicmodeling is critically needed, as conversations lackthe structure and organization of, for instance, editedmonologues.
The results indicate that our frame-work is sufficiently robust to deal with topic labelingin less structured, informal genres (when comparedwith edited monologues).
As an additional result ofour experiments, we show that the identification andselection phase using semantic relations (entailmentgraph) is a necessary step to perform the final step(i.e., the phrase aggregation).2 Topic Labeling FrameworkEach topic cluster contains the sentences that cansemantically represent a topic.
The task of cluster-ing the sentences into a set of coherent topic clus-ters is called topic segmentation (Joty et al 2011),which is out of the scope of this paper.
Our goal is togenerate an understandable label (i.e., a sequence ofwords) that could capture the semantic of the topic,and distinguish a topic from other topics (based ondefinition of a good topic label by (Mei et al 2007)),given a set of topic clusters.
Among possible choicesof word sequences as topic labels, in order to balancethe granularity, we set phrases as valid topic labels.Extract allFilter/selectEntailmentIdentifyPhrase extraction Entailment GraphGeneralizeMergePhraseaggregation?
?
?- -1Figure 1: Topic labeling framework.As shown in Figure 1, our framework consists ofthree main components that we describe in more de-tails in the following sections.2.1 Phrase extractionWe tokenize and preprocess each cluster in the col-lection of topic clusters with lemmas, stems, part-of-speech tags, sense tags and chunks.
We also extractn-grams up to length 5 which do not start or end witha stop word.
In this phase, we do not include anyfrequency count feature in our candidate extractionpipeline.
Once we have built the candidates pool,the next step is to identify a subset containing themost significant of those candidates.
Since most topsystems in key phrase extraction use supervised ap-proaches, we follow the same method (Kim et al2010b; Medelyan et al 2008; Frank et al 1999).Initially, we consider a set of features used in theother systems to determine whether a phrase is likelyto be a key phrase.
However, since our dataset isconversational (more details in Section 3), and thetext segments are not long, we aim for a classifierwith high recall.
Thus, we only use TFxIDF (Saltonand McGill, 1986), position of the first occurrence(Frank et al 1999) and phrase length as our fea-tures.
We merge the training and test data released180for SemEval-2010 Task #5 (Kim et al 2010b),which consists of 244 scientific articles and 3705key phrases, to train a Naive Bayes classifier in or-der to learn a supervised model.
We then apply ourmodel to extract the candidate phrases from the col-lected candidates pool.As a further step, to increase the coverage (re-call) of our extracted phrases and to reduce the num-ber of very short phrases (frequent keywords), wechoose the chunks containing any of the extractedkeywords.
We add those chunks to our extractedphrases and eliminate the associated keywords.2.2 Entailment graphSo far, we have extracted a pool of key phrases fromeach topic cluster.
Many such phrases include re-dundant information which are semantically equiv-alent but vary in lexical choices.
By identifying thesemantic relations between the phrases we can dis-cover the information in one phrase that is seman-tically equivalent, novel, or more/less informativewith respect to the content of the other phrase.We set this problem as a variant of the TextualEntailment (TE) recognition task (Mehdad et al2010b; Adler et al 2012; Berant et al 2011).
Webuild an entailment graph for each topic cluster,where nodes are the extracted phrases and edges arethe entailment relations between nodes.
Given twophrases (ph1 and ph2), we aim at identifying andhandling the following cases:i) ph1 and ph2 express the same meaning (bidirec-tional entailment).
In such cases one of the phrasesshould be eliminated;ii) ph1 is more informative than ph2 (unidirectionalentailment).
In such cases, the entailing phraseshould replace or complement the entailed one;iii) ph1 contains facts that are not present in ph2,and vice-versa (the ?unknown?
cases in TE par-lance).
In such cases, both phrases should remain.Figure 2 shows how entailment relations can helpin selecting the phrases by removing the redun-dant and less informative ones.
For example, thephrase ?animals laugh?
entails ?rats giggle?, ?Horselaugh?
and ?Mice chuckle?,2 but not ?Animals play?.2Assuming that ?animals laugh?
is interpreted as ?all ani-mals laugh?.ratsgiggleHorselaughlaughratsMicechuckleanimalslaughAnimalsplayxx1Figure 2: Building an entailment graph over phrases.
Ar-rows and ?x?
represent the entailment direction and un-known cases respectively.So we can keep ?animals laugh?
and ?Animals play?and eliminate others.
In this way, TE-based phraseidentification method can be designed to distinguishmeaning-preserving variations from true divergence,regardless of lexical choices and structures.Similar to previous approaches in TE (e.g., (Be-rant et al 2011; Mehdad et al 2010b; Mehdad etal., 2010a)), we use supervised method.
To train andbuild the entailment graph, we perform the follow-ing three steps.2.2.1 Training set collectionIn the last few years, TE corpora have been cre-ated and distributed in the framework of severalevaluation campaigns, including the RecognizingTextual Entailment (RTE) Challenge3 and Cross-lingual textual entailment for content synchroniza-tion4 (Negri et al 2012).
However, such datasetscannot directly support our application.
Specifi-cally, our entailment graph is built over the extractedphrases (with max.
length of 5 tokens per phrase),while the RTE datasets are composed of longer sen-tences and paragraphs (Bentivogli et al 2009; Negriet al 2011).In order to collect a dataset which is more similarto the goal of our entailment framework, we decideto select a subset of the sixth and seventh RTE chal-lenge main task (i.e., RTE within a Corpus).
Our3http://pascallin.ecs.soton.ac.uk/Challenges/RTE/4http://www.cs.york.ac.uk/semeval-2013/task8/181dataset choice is based on the following reasons: i)the length of sentence pairs in RTE6 and RTE7 isshorter than the others, and ii) RTE6 and RTE7 maintask datasets are originally created for summariza-tion purpose which is closer to our work.
We sortthe RTE6 and RTE7 dataset pairs based on the sen-tence length and choose the first 2000 samples witha equal number of positive and negative examples.The average length of words in our training data is6.7 words.
There are certainly some differences be-tween our training set and our phrases.
However, thecollected training samples was the closest availabledataset to our purpose.2.2.2 Feature representation and trainingWorking at the phrase level imposes another con-straint.
Phrases are short and in terms of syntacticstructure, they are not as rich as sentences.
This lim-its our features to the lexical level.
Lexical mod-els, on the other hand, are less computationally ex-pensive and easier to implement and often deliver astrong performance for RTE (Sammons et al 2011).Our entailment decision criterion is based onsimilarity scores calculated with a phrase-to-phrasematching process.
Each example pair of phrases(ph1 and ph2) is represented by a feature vector,where each feature is a specific similarity score esti-mating whether ph1 entails ph2.We compute 18 similarity scores for each pair ofphrases.
In order to adapt the similarity scores to theentailment score, we normalize the similarity scoresby the length of ph2 (in terms of lexical items), whenchecking the entailment direction from ph1 to ph2.In this way, we can check the portion of informa-tion/facts in ph2 which is covered by ph1.The first 5 scores are computed based on the exactlexical overlap between the phrases: word overlap,edit distance, ngram-overlap, longest common sub-sequence and Lesk (Lesk, 1986).
The other scoreswere computed using lexical resources: Word-Net (Fellbaum, 1998), VerbOcean (Chklovski andPantel, 2004), paraphrases (Denkowski and Lavie,2010) and phrase matching (Mehdad et al 2011).We used WordNet to compute the word similarityas the least common subsumer between two wordsconsidering the synonymy-antonymy, hypernymy-hyponymy, and meronymy relations.
Then, we cal-culated the sentence similarity as the sum of the sim-ilarity scores of the word pairs in Text and Hypothe-sis, normalized by the number of words in Hypothe-sis.
We also use phrase matching features describedin (Mehdad et al 2011) which consists of phrasalmatching at the level on ngrams (1 to 5 grams).
Therationale behind using different entailment featuresis that combining various scores will yield a bettermodel (Berant et al 2011).To combine the entailment scores and optimizetheir relative weights, we train a Support Vector Ma-chine binary classifier, SVMlight (Joachims, 1999),over an equal number of positive and negative exam-ples.
This results in an entailment model with 95%accuracy over 2-fold and 5-fold cross-validation,which further proves the effectiveness of our fea-ture set for this lexical entailment model.
The reasonthat we gained a very high accuracy is because ourselected sentences are a subset of RTE6 and RTE7with a shorter length (less number of words) whichmakes the entailment recognition task much easierthan recognizing entailment between paragraphs orcomplex long sentences.2.2.3 Graph edge labelingWe set the edge labeling problem as a two-wayclassification task.
Two-way classification castsmultidirectional entailment as a unidirectional prob-lem, where each pair is analyzed checking for en-tailment in both directions (Mehdad et al 2012).
Inthis condition, each original test example is correctlyclassified if both pairs originated from it are cor-rectly judged (?YES-YES?
for bidirectional,?YES-NO?
and ?NO-YES?
for unidirectional entailmentand ?NO-NO?
for unknown cases).
Two-way clas-sification represents an intuitive solution to capturemultidimensional entailment relations.
Moreover,since our training examples are labeled with binaryjudgments, we are not able to train a three-way clas-sifier.2.2.4 Identification and selectionAssigning all entailment relations between the ex-tracted phrase pairs, we are aiming at identifyingrelevant phrases and eliminating the redundant (interms of meaning) and less informative ones.
In or-der to perform this task we follow a set of rules basedon the graph edge labels.
Note that since entailment182# Merging patterns1 merge ( cw11(CPOS=[N|V |J]) ..w1n , cw21(CPOS=[N|V |J]) ..w2n ) = w11..w1n and w22..w2nE.g.
merge ( challenging situation , challenging problem ) = challenging situation and problem2 merge ( w11..cw1n(CPOS=[N|V |J]) , w21..cw2n(CPOS=[N|V |J]) ) = w11..w1n?1 and w21..w2nE.g.
merge ( wet Mars , warm Mars ) = wet and warm Mars3 merge ( w11..cw1n(CPOS=[N|V |J]) , cw21(CPOS=[N|V |J]) ..w2n ) = w11..w1n w22..w2nE.g.
merge ( interesting story , story continues ) = interesting story continues4 merge ( cw11(CPOS=[N|V |J]) ..w1n , w21..cw2n(CPOS=[N|V |J]) ) = w21..w2n w12..w1nE.g.
merge ( LHC shutting down , details about LHC ) = details about LHC shutting down5 merge ( w11Cpos , cw12(CPOS=[N|V |J]) , w13Cpos , w21Cpos , cw22(CPOS=[N|V |J]) , w23Cpos ) = w11 and w21 w22 w23and w13E.g.
merge ( technology grow fast , media grow exponentially ) = technology and media grow exponentially and fastTable 2: Phrase merging patterns.is a transitive relation, our entailment graph is transi-tive i.e., if entail(ph1,ph2) and entail(ph2,ph3) thenentail(ph1,ph3) (Berant et al 2011).Rule 1) If there is a chain of entailing nodes, wekeep the one which is in the root of the chain andeliminate others (e.g.
?animals laugh?
in Figure 2);Rule 2) Among the nodes that are connectedwith bidirectional entailment (semantically equiva-lent nodes) we keep only the one with more outgoingbidirectional and unidirectional entailment relations,respectively;Rule 3) Among the nodes that are connected withunknown entailment (novel information with respectto others) we keep the ones with no incoming entail-ment relation (e.g., ?Animals play?
in Figure 2).Although deleting might be harsh, in our currentframework, we only rely on the performance of anentailment model which gives us a yes/no entailmentdecision.
In future, we are planning to improve ourentailment graph by weighting the edges.
In thisway, we can take advantage of the weights to makea more conservative decision in pruning the entail-ment chains.2.3 Phrase aggregationOnce we have identified and selected the informa-tive phrases, the generation of topic labels can bedone in two steps.
First, we generalize the phrasescontaining the concepts that are lexically connected.Second, we merge the phrases with a set of handwritten linguistically motivated patterns.2.3.1 Phrase generalizationIn this step, we generalize phrases that containconcepts which are lexically connected.
For thispurpose, we search in phrases for different wordswith the same part-of-speech and sense tag.
Then,we find the link between those words in WordNet.
Ifthey are connected and the shortest path connectingthem is less than 3 (estimated over the developmentset), we replace both by their common parent in theWordNet.
In the case that they belong to the samesynset, we can replace one by another.
Note that welimit our search to nouns and verbs.
For example,?rat?
and ?horse?
can be replaced by ?animal?, or?giggle?
and ?chuckle?
can be replaced by ?laugh?.The motivation behind the generalization step is toenrich the common terms between the phrases in fa-vor of increasing the chance that they could mergeto a single phrase.
This also helps to move beyondthe limitation of original lexical choices.2.3.2 Phrase mergingThe goal is to merge the phrases that are con-nected, and to generate a human readable phrase thatcontains more information than a single extractedphrase.
Several approaches have been proposed toaggregate and merge sentences in Natural LanguageGeneration (NLG) (e.g.
(Barzilay and Lapata, 2006;Cheng and Mellish, 2000)), however most of themuse syntactic structure of the sentences.
To mergephrases at the lexical level, we set few common lin-guistically motivated aggregation patterns such as:simple conjunction, and conjunction via shared par-ticipants (Reiter and Dale, 2000).Table 2 demonstrates the merging patterns, wherewij is the jth word (or segment) in phrase i, cwis the common word (or segment) in both phrasesand CPOS is the common part-of-speech tag ofthe corresponding word.
To illustrate, pattern 1183looks for the first segment of each phrase (wi1).If they are same (cwi1) and share the same POStag (CPOS), then we aggregate the first phrase(w11..w1n) and the second phrase removing the firstelement (w22..w2n) by using the connective ?and?.For instance, the aggregation of ?animals laugh?
and?animals play?
results in ?animals laugh and play?.The rest of the patterns follow the same logic and forthe sake of brevity we avoid illustrating each pattern.These patterns are among the most common domainand application independent methods by which twophrases/sentences can be aggregated, as described inthe NLG literature (Reiter and Dale, 2000).In our aggregation pipeline, we group the phrasesbased on their lexical overlap (number of commonwords).
The merging process is conducted over eachgroup in descending order (larger number of wordsin common), in order to increase the chance of merg-ing rules application.
Then, we perform the merg-ing over the resulting generated phrases from eachgroup.
If our phrases cannot be merged (i.e., do notmatch merging patterns), we select them as labelsfor the topic cluster.3 Datasets and Evaluation Metrics3.1 DatasetsTo verify the effectiveness of our approach, we ex-periment with two different conversational datasets.Our interest in dealing with conversational texts de-rives from two reasons.
First, the huge amount oftextual data generated everyday in these conversa-tions validates the need of text analysis frameworksto process such conversational texts effectively.
Sec-ond, conversational texts pose challenges to the tra-ditional techniques, including redundancies, disflu-encies, higher language variabilities and ill-formedsentence structure (Liu et al 2011).Our conversational datasets are from two differ-ent asynchronous media: email and blog.
For email,we use the dataset presented in (Joty et al 2010),where three individuals annotated the publicly avail-able BC3 email corpus (Ulrich et al 2008) with top-ics.
The corpus contains 40 email threads (or conver-sations) at an average of 5 emails per thread.
On av-erage it has 26.3 sentences and 2.5 topics per thread.A topic has an average length of 12.6 sentences.
Intotal, the three annotators found 269 topics in a cor-pus of 1,024 sentences.There are no publicly available blog corpora an-notated with topics.
For this study, we build ourown blog corpus containing 20 blog conversations ofvarious lengths from Slashdot, each annotated withtopics by three human annotators.5 The number ofcomments per conversation varies from 30 to 101with an average of 60.3 and the number of sentencesper conversation varies from 105 to 430 with an av-erage of 220.6.
The annotators first read a conversa-tion and list the topics discussed in the conversationby a short description (e.g., Game contents or size,Bugs or faults) which provides a high-level overviewof the topic.
Then, they assign the most appropriatetopic to each sentence in the conversation.
The shorthigh-level descriptions of the topics serve as refer-ence (or gold) topic labels in our experiments.
Thetarget number of topics was not given in advance andthe annotators were instructed to find as many topicsas needed to convey the overall content structure ofthe conversation.
The annotators found 5 to 23 top-ics per conversation with an average of 10.77.
Thenumber of sentences per topic varies from 11.7 to61.2 with an average of 27.16.
In total, the threeannotators found 512 topics in our blog corpus con-taining 4,411 sentences overall.Note that our annotators performed topic segmen-tation and labeling independently.
In the email cor-pus, the three annotators found 100, 77 and 92 top-ics respectively (269 in total), and in the blog corpus,they found 251, 119 and 192 topics respectively (562in total).
For the evaluation, there is a single goldstandard per topic written by each annotator.
Table1 shows a case in which two annotators selected thesame topical cluster and so we have two labels forthe same cluster.3.2 Evaluation metricsTraditionally, key phrase extraction is evaluated us-ing precision, recall and f-measure based on exactmatches on all the extracted key phrases with goldstandards for a given text.
However, as claimedby (Kim et al 2010a), this approach is not flexibleenough as it ignores the near-misses.
Moreover, inthe case of topic labeling, most of the human written5The new blog corpus annotated with topics will be madepublicly available for research purposes.184topic labels cannot be found in the text.
Recently,(Kim et al 2010a) evaluated the utility of differ-ent n-gram-based metrics for key phrase extractionand showed that the metric R-precision correlatesmost with human judgments.
R-precision normal-izes the approximate matching score by the maxi-mum number of words in the reference and candi-date phrases.
Since this penalize our aggregationphase, where the phrases tend to be longer than orig-inal extracted phrase, we decide to use R-f1 as ourevaluation metric which considers length of both ref-erence and candidate phrases.R?precision = 1kk?i=1overlap(candi, ref)#words(candi)R?recall = 1kk?i=1overlap(candi, ref)#words(ref)R?f1 = 2 ?R?precision ?R?recall(R?precision + R?recall)The metric described above only considers wordoverlap and ignores other semantic relations (e.g.,synonymy, hypernymy) between words.
However,annotators write labels of their own and may usewords that are not directly from the conversation butare semantically related.
Therefore, we propose toalso use another variant of R-f1 that incorporates se-mantic relation between words.
To calculate the Se-mantic R-f1, we count the number of overlaps notonly when they have the same form, but also whenthey are connected in WordNet with a synonymy,hypernymy, hyponymy and entailment relation.Its worth noting that the generalizations phase andthe evaluation method are completely independent.In the generalization step, we try to generalize thephrases which are automatically extracted from thetext segments.
While, in the evaluation, we comparethe human written gold standards with the systemoutput.
Therefore, using WordNet in the generaliza-tion step does not bias the results in the evaluation.4 Experiments and Results4.1 Experimental settingsWe conduct our experiments over the blog and emaildatasets described in Section 3.1, after eliminatingthe development set from the test datasets.
In our ex-periments, the development set was used for the pat-tern extraction and the shortest path threshold con-necting the words in Wordnet in the generalizationphase.
Our test dataset consists of 461 topics (i.e.,clusters and their associated topic labels) from 20blog conversations and 242 topics from 40 emailconversations.For preprocessing our dataset we use OpenNLP6for tokenization, part-of-speech tagging and chunck-ing.
For sense disambiguation, we use the extendedgloss overlap measure with the window size of 5,developed by (Pedersen et al 2005).
We also applySnowball algorithm (Porter, 2001) for stemming.We compare our approach with two strong base-lines.
The first baseline Freq-BL ranks the wordsaccording to their frequencies and select the top 5candidates applying Maximum Marginal Relevancealgorithm (Carbonell and Goldstein, 1998) usingthe same pre- and post-processing as the work by(Mihalcea and Tarau, 2004).
The second baselineLead-BL, ranks the words based on their relevanceto the leading sentences.7 The ranking criteria islog(tfw,Lt + 1)?
log(tfw,t + 1), where tfw,Lt andtfw,t are the number of times word w appears in aset of leading sentences Lt and topic cluster t, re-spectively (Allan, 2002).
The log expressions, as theranking criterion, assign more weights to the wordsin the topic segment, that also appear in the leadingsentences.
This is because topics tend to be intro-duced in the first few sentences of a topical cluster.We also measure the performance of our frameworkat each step in order to compare the effectiveness ofeach phase independently or in combination.4.2 ResultsWe evaluate the performance of different models us-ing the metrics R-f1 and Semantic R-f1 (Sem-R-f1),described in Section 3.2.
Table 4 shows the resultsin percentage for different models.
The results showthat our framework outperforms the baselines signif-6http://opennlp.sourceforge.net/7The key intuitions for this baseline is the leading sentencesof a topic cluster carry the most informative clues for the topiclabels.
Based on our development set, when we consider thefirst three sentences, the coverage of content words that appearin human labeled topics are 39% and 49% for blog and email,respectively.185Blog EmailHuman-authored system generated Human-authored system generatedShutting down the LHC story about the LHC shutting down (#3) How it affects coding it screws my codingtypical shutdown and upgrade times typical and scheduled shutdown (#2) Opinions and preferences of tools opinion about what toolsMARS was warm and wet 3B years ago Mars was warm and wet early history (#3) white on black for disabled users white text on black background (#3)Moon Treaty and outer space treaty Moon and Outer Space Treaty (#2) Contact with Steven email to Steven Pemberton (#3)Table 3: Successful examples of human-authored and system generated labels for blog and email datasets.
The numbernear some examples refers to the aggregation patterns in Table 2.ModelsR-f1 Sem-R-f1blog email blog emailLead-BL 13.5 14.0 34.5 30.1Freq-BL 15.3 13.1 34.7 29.1Extraction-BL 13.9 16.0 31.6 33.2Entailment 12.2 15.6 30.8 33.3Extraction+Aggregation 15.1 18.5 35.5 37.6Extraction+Entailment+Aggregation 17.9 20.4 38.7 41.6Table 4: Results for candidate topic labels on blog andemail corpora.icantly8 in both datasets.On the blog corpus, our key phrase extractionmethod (Extraction-BL) fails to beat the other base-lines (Lead-BL and Freq-BL) in majority of cases(except R-f1 for Lead-BL).
However, in the emaildataset, it improves the performance over both base-lines in both evaluation metrics.
This might be dueto the shorter topic clusters (in terms of number ofsentences) in email corpus which causes a smallernumber of phrases to be extracted.We also observe the effectiveness of the aggre-gation phase.
In all cases, there is a significantimprovement (p < 0.05) after applying the ag-gregation phase over the extracted phrases (Extrac-tion+Aggregation).Note that there is no improvement over the ex-traction phase after the entailment (Entailment row).This is mainly due to the fact that the entailmentphase filters the equivalent phrases.
This affects theresults negatively when such filtered phrases sharemany common words with our human-authoredphrases.
However, the results improve more sig-nificantly (p < 0.01) when the aggregation is con-ducted after the entailment.
This demonstrates that,the combination of these two steps are beneficial fortopic labeling over conversational datasets.In addition, the differences between the results us-8The statistical significance tests was calculated by approx-imate randomization described in (Yeh, 2000).ing R-f1 and Sem-R-f1 metrics suggests the need formore flexible automatic evaluation methods for thistask.
Moreover, although the same trend of improve-ment is observed in blog and email corpora, the dif-ferences between their performance suggest the in-vestigation of specialized methods for various con-versational modalities.0 100 200 300 400 500 60000.20.40.60.81BlogSem-R-f1ExtExt+EntExt+AggExt+Ent+Agg1Figure 3: Sem-R-f1 results distribution after each phaseof our pipeline for blog corpus.
The x-axis represents theexamples sorted based on their Sem-R-f1 score.To further analyze the performance, in Figure 3,we show the Sem-R-f1 results distribution for ourblog dataset.9 We can observe that the aggrega-tion after the entailment phase (bold curve) clearlyincrease the number of correct labels, while suchimprovement can be only achieved when the en-tailment relations is used to identify the relevantphrases.
This further highlights the need of seman-tics in this task.
Comparing both datasets, this ef-fect is more dominant in blogs.
We believe that thisis due to the length of topic clusters.
Presumably,building an entailment graph over a greater pool of9For brevity?s sake we do not show the email dataset graph.186original phrases is more effective to filter the redun-dant information and identify the relevant phrases.5 DiscussionAfter analyzing the results and through manual veri-fication of some cases, we observe that our approachled to some interestingly successful examples.
Table3 shows few generated labels and the human writtentopics for such cases.In general, given that the results are expressed inpercentage, it appears that the performance is stillfar from satisfactory level.
This leaves an interestingchallenge for the research community to tackle.However, this is not always due to the weaknessof our proposed model.
We have identified threedifferent system independent sources of error:10Type 1: Abstractive human-authored labels: thenature of our method is based on extraction (withthe exception of our simple generalization phase)and in many cases the human-written labels cannotbe extracted from the text and require more complexgeneralizations.
In fact, only 9.81% of the labelsin blog and 12.74% of the labels in email appearverbatim in their respective conversations.
Forexample:Human-authored label: meeting schedule and locationGenerated phrases: meeting, Boston area, mid OctoberType 2: Evaluation methods: in this work, weproposed a semantic method to evaluate our system.However, the current evaluation methods fail tocapture the meaning.
For example:Human-authored label: Food choicesGenerated phrase: I would ask what people want to eatType 3: Subjective topic labels: often is not easyfor human to agree on one label for a topic cluster.11For example:Human-authored label 1: Member introductionHuman-authored label 2: Bio of LenGenerated phrases: own intro, Len Kasday, chairIn light of this analysis, we conclude that a morecomprehensive evaluation method (e.g., human eval-uation) could better reveal the potential of our sys-10There are many examples of such cases, however forbrevity we just mention one example for each type.11The mean R-precision agreements computed based on one-to-one mappings of the topic clusters are 20.22 and 36.84 onblog and email data sets, respectively.tem in dealing with topic labeling, specially on con-versational data.6 ConclusionIn this paper, we study the problem of automatictopic labeling, and propose a novel framework to la-bel topic clusters with meaningful readable phrases.Within such framework, this paper makes two maincontributions.
First, in contrast with most currentmethods based on fully extractive models, we pro-pose to aggregate topic labels by means of gener-alizing and merging techniques.
Second, beyondcurrent approaches which disregard semantic infor-mation, we integrate semantics by means of build-ing textual entailment graphs over the topic clusters.To achieve our objectives, we successfully appliedour framework over two challenging conversationaldatasets.
Coherent results on both datasets demon-strate the potential of our approach in dealing withtopic labeling task.Future work will address both the improvement ofour aggregation phase and ranking the output candi-date phrases for each topic cluster.
On one hand,we plan to accommodate more sophisticated NLGtechniques for the aggregation and generation phase.Incorporating a better source of prior knowledge inthe generalization phase (e.g., YAGO or DBpedia) isalso an interesting research direction towards a bet-ter phrase aggregation step.
On the other hand, weplan to apply a ranking strategy to select the top can-didate phrases generated by our framework.AcknowledgmentsWe would like to thank the anonymous reviewersand Frank Tompa for their valuable comments andsuggestions to improve the paper, and the NSERCBusiness Intelligence Network for financial support.Yashar Mehdad also would like to acknowledge theearly discussions on the related topics with MatteoNegri.ReferencesMeni Adler, Jonathan Berant, and Ido Dagan.
2012.Entailment-based text exploration with application tothe health-care domain.
In Proceedings of the ACL2012 System Demonstrations, ACL ?12, pages 79?84,187Stroudsburg, PA, USA.
Association for ComputationalLinguistics.James Allan.
2002.
Topic detection and tracking: event-based information organization.Regina Barzilay and Mirella Lapata.
2006.
Aggregationvia set partitioning for natural language generation.
InProceedings of the main conference on Human Lan-guage Technology Conference of the North AmericanChapter of the Association of Computational Linguis-tics, HLT-NAACL ?06, pages 359?366, Stroudsburg,PA, USA.
Association for Computational Linguistics.Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, DaniloGiampiccolo, and Bernardo Magnini.
2009.
The fifthpascal recognizing textual entailment challenge.
In InProc Text Analysis Conference (TAC09.Jonathan Berant, Ido Dagan, and Jacob Goldberger.2011.
Global learning of typed entailment rules.
InProceedings of ACL, Portland, OR.SRK Branavan, Pawan Deshpande, and Regina Barzilay.2007.
Generating a table-of-contents.
In ACL, vol-ume 45, page 544.Jaime Carbonell and Jade Goldstein.
1998.
The use ofmmr, diversity-based reranking for reordering docu-ments and producing summaries.
In Proceedings ofthe 21st annual international ACM SIGIR conferenceon Research and development in information retrieval,SIGIR ?98, pages 335?336, New York, NY, USA.ACM.Giuseppe Carenini, Gabriel Murray, and Raymond Ng.2011.
Methods for mining and summarizing text con-versations.Hua Cheng and Chris Mellish.
2000.
Capturing the in-teraction between aggregation and text planning in twogeneration systems.
In In Proceedings of the 1st In-ternational Natural Language Generation Conference,186193, Mitzpe.Timothy Chklovski and Patrick Pantel.
2004.
Verbocean:Mining the web for fine-grained semantic verb rela-tions.
In Dekang Lin and Dekai Wu, editors, Proceed-ings of EMNLP 2004, pages 33?40, Barcelona, Spain,July.
Association for Computational Linguistics.I.
Dagan and O. Glickman.
2004.
Probabilistic tex-tual entailment: Generic applied modeling of languagevariability.Michael Denkowski and Alon Lavie.
2010.
Meteor-nextand the meteor paraphrase tables: improved evaluationsupport for five target languages.
In Proceedings ofthe Joint Fifth Workshop on Statistical Machine Trans-lation and MetricsMATR, WMT ?10, pages 339?342,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Gae?l Dias, Elsa Alves, and Jose?
Gabriel Pereira Lopes.2007.
Topic segmentation algorithms for text summa-rization and passage retrieval: an exhaustive evalua-tion.
In Proceedings of the 22nd national conferenceon Artificial intelligence - Volume 2, AAAI?07, pages1334?1339.
AAAI Press.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
Bradford Books.Eibe Frank, Gordon W. Paynter, Ian H. Witten, CarlGutwin, and Craig G. Nevill-Manning.
1999.Domain-specific keyphrase extraction.
In Proceedingsof the Sixteenth International Joint Conference on Ar-tificial Intelligence, IJCAI ?99, pages 668?673, SanFrancisco, CA, USA.
Morgan Kaufmann PublishersInc.Sanda Harabagiu and Finley Lacatusu.
2010.
Us-ing topic themes for multi-document summarization.ACM Trans.
Inf.
Syst., 28(3):13:1?13:47, July.T.
Joachims.
1999.
Making large-scale svm learningpractical.
LS8-Report 24, Universita?t Dortmund, LSVIII-Report.Shafiq Joty, Giuseppe Carenini, Gabriel Murray, andRaymond T. Ng.
2010.
Exploiting conversation struc-ture in unsupervised topic segmentation for emails.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?10, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Shafiq Joty, Gabriel Murray, and Raymond T. Ng.
2011.Supervised topic segmentation of email conversations.In In ICWSM11.
AAAI.Su Nam Kim, Timothy Baldwin, and Min-Yen Kan.2010a.
Evaluating n-gram based evaluation metricsfor automatic keyphrase extraction.
In Proceedings ofthe 23rd International Conference on ComputationalLinguistics, COLING ?10, pages 572?580, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timo-thy Baldwin.
2010b.
Semeval-2010 task 5: Automatickeyphrase extraction from scientific articles.
In Pro-ceedings of the 5th International Workshop on Seman-tic Evaluation, SemEval ?10, pages 21?26, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Thomas Kleinbauer, Stephanie Becker, and TilmanBecker.
2007.
Combining multiple information lay-ers for the automatic generation of indicative meetingabstracts.
In Proceedings of the Eleventh EuropeanWorkshop on Natural Language Generation, ENLG?07, pages 151?154, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Jey Han Lau, Karl Grieser, David Newman, and TimothyBaldwin.
2011.
Automatic labelling of topic models.In ACL, pages 1536?1545.188Michael Lesk.
1986.
Automatic sense disambiguationusing machine readable dictionaries: how to tell a pinecone from an ice cream cone.
In Proceedings of the5th annual international conference on Systems docu-mentation, SIGDOC ?86, pages 24?26, New York, NY,USA.
ACM.F.
Liu, Y. Liu, C. Busso, S. Harabagiu, and V. Ng.
2011.Identifying the Gist of Conversational Text: AutomaticKeyword Extraction and Summarization.
Ph.D. thesis,THE UNIVERSITY OF TEXAS AT DALLAS.Shixia Liu, Michelle X. Zhou, Shimei Pan, YangqiuSong, Weihong Qian, Weijia Cai, and Xiaoxiao Lian.2012.
Tiara: Interactive, topic-based visual text sum-marization and analysis.
ACM Trans.
Intell.
Syst.Technol., 3(2):25:1?25:28, February.O.
Medelyan, I.H.
Witten, and D. Milne.
2008.
Topicindexing with wikipedia.
In Proceedings of the AAAIWikiAI workshop.Y.
Mehdad, A. Moschitti, and F.M.
Zanzotto.
2010a.Syntactic semantic structures for textual entailmentrecognition.
Association for Computational Linguis-tics.Yashar Mehdad, Matteo Negri, and Marcello Federico.2010b.
Towards cross-lingual textual entailment.
InHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics, HLT ?10,pages 321?324, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Yashar Mehdad, Matteo Negri, and Marcello Federico.2011.
Using bilingual parallel corpora for cross-lingual textual entailment.
In Proceedings of the 49thAnnual Meeting of the Association for ComputationalLinguistics: Human Language Technologies - Volume1, HLT ?11, pages 1336?1345, Stroudsburg, PA, USA.Association for Computational Linguistics.Yashar Mehdad, Matteo Negri, and Marcello Federico.2012.
Detecting semantic equivalence and informa-tion disparity in cross-lingual documents.
In Proceed-ings of the 50th Annual Meeting of the Association forComputational Linguistics: Short Papers - Volume 2,ACL ?12, pages 120?124, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.2007.
Automatic labeling of multinomial topic mod-els.
In Proceedings of the 13th ACM SIGKDD inter-national conference on Knowledge discovery and datamining, KDD ?07, pages 490?499, New York, NY,USA.
ACM.R.
Mihalcea and P. Tarau.
2004.
TextRank: Bringingorder into texts.
In Proceedings of EMNLP-04andthe 2004 Conference on Empirical Methods in Natu-ral Language Processing, July.Matteo Negri, Luisa Bentivogli, Yashar Mehdad, DaniloGiampiccolo, and Alessandro Marchetti.
2011.
Di-vide and conquer: crowdsourcing the creation of cross-lingual textual entailment corpora.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?11, pages 670?679,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Matteo Negri, Alessandro Marchetti, Yashar Mehdad,Luisa Bentivogli, and Danilo Giampiccolo.
2012.Semeval-2012 task 8: cross-lingual textual entailmentfor content synchronization.
In Proceedings of theFirst Joint Conference on Lexical and ComputationalSemantics - Volume 1: Proceedings of the main con-ference and the shared task, and Volume 2: Proceed-ings of the Sixth International Workshop on Seman-tic Evaluation, SemEval ?12, pages 399?407, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.T.
Pedersen, S. Banerjee, and S. Patwardhan.
2005.Maximizing Semantic Relatedness to Perform WordSense Disambiguation.
Research Report UMSI2005/25, University of Minnesota Supercomputing In-stitute, March.Martin F. Porter.
2001.
Snowball: A language for stem-ming algorithms.Ehud Reiter and Robert Dale.
2000.
Building naturallanguage generation systems.Gerard Salton and Michael J. McGill.
1986.
Introductionto modern information retrieval.Mark Sammons, V.G.Vinod Vydiswaran, and Dan Roth.2011.
Recognizing Textual Entailment.
In Daniel M.Bikel and Imed Zitouni, editors, Multilingual Natu-ral Language Applications: From Theory to Practice.Prentice Hall, Jun.Jan Ulrich, Gabriel Murray, and Giuseppe Carenini.2008.
A publicly available annotated corpus for su-pervised email summarization.Alexander Yeh.
2000.
More accurate tests for the statis-tical significance of result differences.
In Proceedingsof the 18th conference on Computational linguistics -Volume 2, COLING ?00, pages 947?953, Stroudsburg,PA, USA.
Association for Computational Linguistics.189
