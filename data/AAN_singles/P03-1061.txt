Morphological Analysis of a Large Spontaneous Speech Corpus in JapaneseKiyotaka Uchimoto?
Chikashi Nobata?
Atsushi Yamada?Satoshi Sekine?
Hitoshi Isahara?
?Communications Research Laboratory3-5, Hikari-dai, Seika-cho, Soraku-gun,Kyoto, 619-0289, Japan{uchimoto,nova,ark,isahara}@crl.go.jp?New York University715 Broadway, 7th floorNew York, NY 10003, USAsekine@cs.nyu.eduAbstractThis paper describes two methods for de-tecting word segments and their morpho-logical information in a Japanese sponta-neous speech corpus, and describes howto tag a large spontaneous speech corpusaccurately by using the two methods.
Thefirst method is used to detect any type ofword segments.
The second method isused when there are several definitions forword segments and their POS categories,and when one type of word segments in-cludes another type of word segments.
Inthis paper, we show that by using semi-automatic analysis we achieve a precisionof better than 99% for detecting and tag-ging short words and 97% for long words;the two types of words that comprise thecorpus.
We also show that better accuracyis achieved by using both methods than byusing only the first.1 IntroductionThe ?Spontaneous Speech: Corpus and Process-ing Technology?
project is sponsoring the construc-tion of a large spontaneous Japanese speech corpus,Corpus of Spontaneous Japanese (CSJ) (Maekawaet al, 2000).
The CSJ is a collection of mono-logues and dialogues, the majority being mono-logues such as academic presentations and simu-lated public speeches.
Simulated public speechesare short speeches presented specifically for the cor-pus by paid non-professional speakers.
The CSJ in-cludes transcriptions of the speeches as well as audiorecordings of them.
One of the goals of the projectis to detect two types of word segments and cor-responding morphological information in the tran-scriptions.
The two types of word segments weredefined by the members of The National Institute forJapanese Language and are called short word andlong word.
The term short word approximates a dic-tionary item found in an ordinary Japanese dictio-nary, and long word represents various compounds.The length and part-of-speech (POS) of each are dif-ferent, and every short word is included in a longword, which is shorter than a Japanese phrasal unit,a bunsetsu.
If all of the short words in the CSJwere detected, the number of the words would beapproximately seven million.
That would be thelargest spontaneous speech corpus in the world.
Sofar, approximately one tenth of the words have beenmanually detected, and morphological informationsuch as POS category and inflection type have beenassigned to them.
Human annotators tagged everymorpheme in the one tenth of the CSJ that has beentagged, and other annotators checked them.
The hu-man annotators discussed their disagreements andresolved them.
The accuracies of the manual taggingof short and long words in the one tenth of the CSJwere greater than 99.8% and 97%, respectively.
Theaccuracies were evaluated by random sampling.
Asit took over two years to tag one tenth of the CSJ ac-curately, tagging the remainder with morphologicalinformation would take about twenty years.
There-fore, the remaining nine tenths of the CSJ must betagged automatically or semi-automatically.In this paper, we describe methods for detectingthe two types of word segments and correspondingmorphological information.
We also describe howto tag a large spontaneous speech corpus accurately.Henceforth, we call the two types of word segmentsshort word and long word respectively, or merelymorphemes.
We use the term morphological anal-ysis for the process of segmenting a given sentenceinto a row of morphemes and assigning to each mor-pheme grammatical attributes such as a POS cate-gory.2 Problems and Their SolutionsAs we mentioned in Section 1, tagging the whole ofthe CSJ manually would be difficult.
Therefore, weare taking a semi-automatic approach.
This sectiondescribes major problems in tagging a large sponta-neous speech corpus with high precision in a semi-automatic way, and our solutions to those problems.One of the most important problems in morpho-logical analysis is that posed by unknown words,which are words found in neither a dictionary nora training corpus.
Two statistical approaches havebeen applied to this problem.
One is to find un-known words from corpora and put them into adictionary (e.g., (Mori and Nagao, 1996)), and theother is to estimate a model that can identify un-known words correctly (e.g., (Kashioka et al, 1997;Nagata, 1999)).
Uchimoto et al used both ap-proaches.
They proposed a morphological analysismethod based on a maximum entropy (ME) model(Uchimoto et al, 2001).
Their method uses a modelthat estimates how likely a string is to be a mor-pheme as its probability, and thus it has a potentialto overcome the unknown word problem.
Therefore,we use their method for morphological analysis ofthe CSJ.
However, Uchimoto et al reported that theaccuracy of automatic word segmentation and POStagging was 94 points in F-measure (Uchimoto etal., 2002).
That is much lower than the accuracy ob-tained by manual tagging.
Several problems led tothis inaccuracy.
In the following, we describe theseproblems and our solutions to them.?
Fillers and disfluenciesFillers and disfluencies are characteristic ex-pressions often used in spoken language, butthey are randomly inserted into text, so detect-ing their segmentation is difficult.
In the CSJ,they are tagged manually.
Therefore, we firstdelete fillers and disfluencies and then put themback in their original place after analyzing atext.?
Accuracy for unknown wordsThe morpheme model that will be describedin Section 3.1 can detect word segments andtheir POS categories even for unknown words.However, the accuracy for unknown words islower than that for known words.
One of thesolutions is to use dictionaries developed for acorpus on another domain to reduce the num-ber of unknown words, but the improvementachieved is slight (Uchimoto et al, 2002).
Webelieve that the reason for this is that defini-tions of a word segment and its POS categorydepend on a particular corpus, and the defi-nitions from corpus to corpus differ word byword.
Therefore, we need to put only wordsextracted from the same corpus into a dictio-nary.
We are manually examining words thatare detected by the morpheme model but thatare not found in a dictionary.
We are alsomanually examining those words that the mor-pheme model estimated as having low proba-bility.
During the process of manual exami-nation, if we find words that are not found ina dictionary, those words are then put into adictionary.
Section 4.2.1 will describe the ac-curacy of detecting unknown words and showhow much those words contribute to improvingthe morphological analysis accuracy when theyare detected and put into a dictionary.?
Insufficiency of featuresThe model currently used for morphologicalanalysis considers the information of a targetmorpheme and that of an adjacent morphemeon the left.
To improve the model, we need toconsider the information of two or more mor-phemes on the left of the target morpheme.However, too much information often leads toovertraining the model.
Using all the informa-tion makes training the model difficult whenthere is too much of it.
Therefore, the bestway to improve the accuracy of the morpholog-ical information in the CSJ within the limitedtime available to us is to examine and revisethe errors of automatic morphological analysisand to improve the model.
We assume that thesmaller the probability estimated by a modelfor an output morpheme is, then the greaterthe likelihood is that the output morpheme iswrong.
Therefore, we examine output mor-phemes in ascending order of their probabili-ties.
The expected improvement of the accu-racy of the morphological information in thewhole of the CSJ will be described in Sec-tion 4.2.1Another problem concerning unknown wordsis that the cost of manual examination is highwhen there are several definitions for word seg-ments and their POS categories.
Since thereare two types of word definitions in the CSJ,the cost would double.
Therefore, to reduce thecost, we propose another method for detectingword segments and their POS categories.
Themethod will be described in Section 3.2, andthe advantages of the method will be describedin Section 4.2.2The next problem described here is one that wehave to solve to make a language model for auto-matic speech recognition.?
PronunciationPronunciation of each word is indispensable formaking a language model for automatic speechrecognition.
In the CSJ, pronunciation is tran-scribed separately from the basic form writ-ten by using kanji and hiragana characters asshown in Fig.
1.
Text targeted for morpho-Basic form Pronunciation0017 00051.425-00052.869 L:(F??)
(F??)?????
????????
?0018 00053.073-00054.503 L:????
???
?0019 00054.707-00056.341 L:????????
?????????
?Well, I?m going to talk about morphological analysis.
?Figure 1: Example of transcription.logical analysis is the basic form of the CSJand it does not have information on actual pro-nunciation.
The result of morphological anal-ysis, therefore, is a row of morphemes thatdo not have information on actual pronuncia-tion.
To estimate actual pronunciation by usingonly the basic form and a dictionary is impossi-ble.
Therefore, actual pronunciation is assignedto results of morphological analysis by align-ing the basic form and pronunciation in theCSJ.
First, the results of morphological anal-ysis, namely, the morphemes, are transliteratedinto katakana characters by using a dictionary,and then they are aligned with pronunciationin the CSJ by using a dynamic programmingmethod.In this paper, we will mainly discuss methods fordetecting word segments and their POS categories inthe whole of the CSJ.3 Models and AlgorithmsThis section describes two methods for detectingword segments and their POS categories.
The firstmethod uses morpheme models and is used to detectany type of word segment.
The second method usesa chunking model and is only used to detect longword segments.3.1 Morpheme ModelGiven a tokenized test corpus, namely a set ofstrings, the problem of Japanese morphologicalanalysis can be reduced to the problem of assign-ing one of two tags to each string in a sentence.
Astring is tagged with a 1 or a 0 to indicate whetherit is a morpheme.
When a string is a morpheme, agrammatical attribute is assigned to it.
A tag desig-nated as a 1 is thus assigned one of a number, n, ofgrammatical attributes assigned to morphemes, andthe problem becomes to assign an attribute (from 0to n) to every string in a given sentence.We define a model that estimates the likelihoodthat a given string is a morpheme and has a gram-matical attribute i(1 ?
i ?
n) as a morphememodel.
We implemented this model within an MEmodeling framework (Jaynes, 1957; Jaynes, 1979;Berger et al, 1996).
The model is represented byEq.
(1):p?
(a|b) =exp(?i,j?i,jgi,j(a, b))Z?
(b)(1)Short word Long wordWord Pronunciation POS Others Word Pronunciation POS Others??
(form) ????
(keitai) Noun ?????
(morphologicalanalysis)?????????
(keitaisokaiseki) Noun?
(element) ?
(so) Suffix??
(analysis)????
(kaiseki) Noun?
?
(ni) PPP case marker ????
(about) ????
(nitsuite) PPP case marker,compoundword??
(relate) ??
(tsui) Verb KA-GYO, ADF, eu-phonic change?
?
(te) PPP conjunctive?
?
(o) Prefix ??????
(talk) ???????
(ohanashiitasi) Verb SA-GYO,ADF??
(talk) ???
(hanashi) Verb SA-GYO, ADF???
(do) ???
(itashi) Verb SA-GYO, ADF??
??
(masu) AUX ending form ??
??
(masu) AUX ending formPPP : post-positional particle , AUX : auxiliary verb , ADF : adverbial formFigure 2: Example of morphological analysis results.Z?
(b) =?aexp(?i,j?i,jgi,j(a, b)), (2)where a is one of the categories for classification,and it can be one of (n+1) tags from 0 to n (This iscalled a ?future.?
), b is the contextual or condition-ing information that enables us to make a decisionamong the space of futures (This is called a ?his-tory.?
), and Z?
(b) is a normalizing constant deter-mined by the requirement that?ap?
(a|b) = 1 forall b.
The computation of p?
(a|b) in any ME modelis dependent on a set of ?features?
which are binaryfunctions of the history and future.
For instance, oneof our features isgi,j(a, b) ={1 : if has(b, fj) = 1 & a = aifj= ?POS(?1)(Major) : verb,?
?0 : otherwise.
(3)Here ?has(b, fj)?
is a binary function that returns1 if the history b has feature fj.
The features usedin our experiments are described in detail in Sec-tion 4.1.1.Given a sentence, probabilities of n tags from 1to n are estimated for each length of string in thatsentence by using the morpheme model.
From allpossible division of morphemes in the sentence, anoptimal one is found by using the Viterbi algorithm.Each division is represented as a particular divisionof morphemes with grammatical attributes in a sen-tence, and the optimal division is defined as a di-vision that maximizes the product of the probabil-ities estimated for each morpheme in the division.For example, the sentence ??????????????????
in basic form as shown in Fig.
1 isanalyzed as shown in Fig.
2.
???????
is ana-lyzed as three morphemes, ???
(noun)?, ??
(suf-fix)?, and ???
(noun)?, for short words, and as onemorpheme, ??????
(noun)?
for long words.In conventional models (e.g., (Mori and Nagao,1996; Nagata, 1999)), probabilities were estimatedfor candidate morphemes that were found in a dic-tionary or a corpus and for the remaining stringsobtained by eliminating the candidate morphemesfrom a given sentence.
Therefore, unknown wordswere apt to be either concatenated as one word or di-vided into both a combination of known words anda single word that consisted of more than one char-acter.
However, this model has the potential to cor-rectly detect any length of unknown words.3.2 Chunking ModelThe model described in this section can be appliedwhen several types of words are defined in a cor-pus and one type of words consists of compounds ofother types of words.
In the CSJ, every long wordconsists of one or more short words.Our method uses two models, a morpheme modelfor short words and a chunking model for longwords.
After detecting short word segments andtheir POS categories by using the former model,long word segments and their POS categories are de-tected by using the latter model.
We define four la-bels, as explained below, and extract long word seg-ments by estimating the appropriate labels for eachshort word according to an ME model.
The four la-bels are listed below:Ba: Beginning of a long word, and the POS cat-egory of the long word agrees with the shortword.Ia: Middle or end of a long word, and the POS cat-egory of the long word agrees with the shortword.B: Beginning of a long word, and the POS categoryof the long word does not agree with the shortword.I: Middle or end of a long word, and the POS cat-egory of the long word does not agree with theshort word.A label assigned to the leftmost constituent of a longword is ?Ba?
or ?B?.
Labels assigned to other con-stituents of a long word are ?Ia?, or ?I?.
For exam-ple, the short words shown in Fig.
2 are labeled asshown in Fig.
3.
The labeling is done deterministi-cally from the beginning of a given sentence to itsend.
The label that has the highest probability as es-timated by an ME model is assigned to each shortword.
The model is represented by Eq.
(1).
In Eq.
(1), a can be one of four labels.
The features used inour experiments are described in Section 4.1.2.Short word Long wordWord POS Label Word POS??
Noun Ba ?????
Noun?
Suffix I??
Noun Ia?
PPP Ba ????
PPP??
Verb I?
PPP Ia?
Prefix B ??????
Verb??
Verb Ia???
Verb Ia??
AUX Ba ??
AUXPPP : post-positional particle , AUX : auxiliary verbFigure 3: Example of labeling.When a long word that does not include a shortword that has been assigned the label ?Ba?
or ?Ia?,this indicates that the word?s POS category differsfrom all of the short words that constitute the longword.
Such a word must be estimated individually.In this case, we estimate the POS category by us-ing transformation rules.
The transformation rulesare automatically acquired from the training corpusby extracting long words with constituents, namelyshort words, that are labeled only ?B?
or ?I?.
A ruleis constructed by using the extracted long word andthe adjacent short words on its left and right.
Forexample, the rule shown in Fig.
4 was acquired inour experiments.
The middle division of the con-sequent part represents a long word ????
(auxil-iary verb), and it consists of two short words ???
(post-positional particle) and ???
(verb).
If severaldifferent rules have the same antecedent part, onlythe rule with the highest frequency is chosen.
If norules can be applied to a long word segment, rulesare generalized in the following steps.1.
Delete posterior context2.
Delete anterior and posterior contexts3.
Delete anterior and posterior contexts and lexi-cal entries.If no rules can be applied to a long word segment inany step, the POS category noun is assigned to thelong word.4 Experiments and Discussion4.1 Experimental ConditionsIn our experiments, we used 744,204 short wordsand 618,538 long words for training, and 63,037short words and 51,796 long words for testing.Those words were extracted from one tenth of theCSJ that already had been manually tagged.
Thetraining corpus consisted of 319 speeches and thetest corpus consisted of 19 speeches.Transcription consisted of basic form and pronun-ciation, as shown in Fig.
1.
Speech sounds werefaithfully transcribed as pronunciation, and also rep-resented as basic forms by using kanji and hiraganacharacters.
Lines beginning with numerical digitsare time stamps and represent the time it took toproduce the lines between that time stamp and thenext time stamp.
Each line other than time stampsrepresents a bunsetsu.
In our experiments, we usedonly the basic forms.
Basic forms were tagged withseveral types of labels such as fillers, as shown inTable 1.
Strings tagged with those labels were han-dled according to rules as shown in the rightmostcolumns in Table 1.Since there are no boundaries between sentencesin the corpus, we selected the places in the CSJ thatAnterior context Target words Posterior contextEntry ??
(it, go) ?
(te)?
(mi, try) ??
(tai, want)POS Verb PPP Verb AUXLabel Ba B I BaAntecedent part?Anterior context Long word Posterior context??
(it, go) ??
(temi, try) ??
(tai, want)Verb AUX AUXConsequent partFigure 4: Example of transformation rules.Table 1: Type of labels and their handling.Type of Labels Example RulesFillers (F??)
delete allDisfluencies (D?)?????
(D2?)?
delete allNo confidence intranscription(?
?????)
leave a candidateEntirely (?)
delete allSeveral can- (?
???,????)
leave the formerdidates exist candidateCitation on sound orwords(M?)?
(M?)???
leave a candidateForeign, archaic, ordialect words(O???????)
leave a candidatePersonal name, dis-criminating words,and slander????
(R??)???
leave a candidateLetters and theirpronunciation inkatakana strings(A????
;EU) leave the formercandidateStrings that cannotbe written in kanjicharacters(K?
(F??)??;?)
leave the latter can-didateare automatically detected as pauses of 500 ms orlonger and then designated them as sentence bound-aries.
In addition to these, we also used utteranceboundaries as sentence boundaries.
These are au-tomatically detected at places where short pauses(shorter than 200 ms but longer than 50 ms) followthe typical sentence-ending forms of predicates suchas verbs, adjectives, and copula.4.1.1 Features Used by Morpheme ModelsIn the CSJ, bunsetsu boundaries, which are phraseboundaries in Japanese, were manually detected.Fillers and disfluencies were marked with the labels(F) and (D).
In the experiments, we eliminated fillersand disfluencies but we did use their positional infor-mation as features.
We also used as features, bun-setsu boundaries and the labels (M), (O), (R), and(A), which were assigned to particular morphemessuch as personal names and foreign words.
Thus, theinput sentences for training and testing were charac-ter strings without fillers and disfluencies, and bothboundary information and various labels were at-tached to them.
Given a sentence, for every stringwithin a bunsetsu and every string appearing in adictionary, the probabilities of a in Eq.
(1) were es-timated by using the morpheme model.
The outputwas a sequence of morphemes with grammatical at-tributes, as shown in Fig.
2.
We used the POS cate-gories in the CSJ as grammatical attributes.
We ob-tained 14 major POS categories for short words and15 major POS categories for long words.
Therefore,a in Eq.
(1) can be one of 15 tags from 0 to 14 forshort words, and it can be one of 16 tags from 0 to15 for long words.Table 2: Features.Number Feature Type Feature value(Number of value) (Short:Long)1 String(0) (113,474:117,002)2 String(-1) (17,064:32,037)3 Substring(0)(Left1) (2,351:2,375)4 Substring(0)(Right1) (2,148:2,171)5 Substring(0)(Left2) (30,684:31,456)6 Substring(0)(Right2) (25,442:25,541)7 Substring(-1)(Left1) (2,160:2,088)8 Substring(-1)(Right1) (1,820:1,675)9 Substring(-1)(Left2) (11,025:12,875)10 Substring(-1)(Right2) (10,439:13,364)11 Dic(0)(Major) Noun, Verb, Adjective, .
.
.
Unde-fined (15:16)12 Dic(0)(Minor) Common noun, Topic marker, Ba-sic form.
.
.
(75:71)13 Dic(0)(Major&Minor) Noun&Common noun,Verb&Basic form, .
.
.
(246:227)14 Dic(-1)(Minor) Common noun, Topic marker, Ba-sic form.
.
.
(16:16)15 POS(-1) Noun, Verb, Adjective, .
.
.
(14:15)16 Length(0) 1, 2, 3, 4, 5, 6 or more (6:6)17 Length(-1) 1, 2, 3, 4, 5, 6 or more (6:6)18 TOC(0)(Beginning) Kanji, Hiragana, Number,Katakana, Alphabet (5:5)19 TOC(0)(End) Kanji, Hiragana, Number,Katakana, Alphabet (5:5)20 TOC(0)(Transition) Kanji?Hiragana,Number?Kanji,Katakana?Kanji, .
.
.
(25:25)21 TOC(-1)(End) Kanji, Hiragana, Number,Katakana, Alphabet (5:5)22 TOC(-1)(Transition) Kanji?Hiragana,Number?Kanji,Katakana?Kanji, .
.
.
(16:15)23 Boundary Bunsetsu(Beginning), Bun-setsu(End), Label(Beginning),Label(End), (4:4)24 Comb(1,15) (74,602:59,140)25 Comb(1,2,15) (141,976:136,334)26 Comb(1,13,15) (78,821:61,813)27 Comb(1,2,13,15) (156,187:141,442)28 Comb(11,15) (209:230)29 Comb(12,15) (733:682)30 Comb(13,15) (1,549:1,397)31 Comb(12,14) (730:675)The features we used with morpheme models inour experiments are listed in Table 2.
Each featureconsists of a type and a value, which are given in therows of the table, and it corresponds to j in the func-tion gi,j(a, b) in Eq.
(1).
The notations ?(0)?
and?(-1)?
used in the feature-type column in Table 2 re-spectively indicate a target string and the morphemeto the left of it.
The terms used in the table are ba-sically as same as those that Uchimoto et al used(Uchimoto et al, 2002).
The main difference is thefollowing one:Boundary: Bunsetsu boundaries and positional in-formation of labels such as fillers.
?(Begin-ning)?
and ?(End)?
in Table 2 respectively indi-cate whether the left and right side of the targetstrings are boundaries.We used only those features that were found three ormore times in the training corpus.4.1.2 Features Used by a Chunking ModelWe used the following information as featureson the target word: a word and its POS cate-gory, and the same information for the four clos-est words, the two on the left and the two onthe right of the target word.
Bigram and tri-gram words that included a target word plus bigramand trigram POS categories that included the tar-get word?s POS category were used as features.
Inaddition, bunsetsu boundaries as described in Sec-tion 4.1.1 were used.
For example, when a targetword was ???
in Fig.
3, ??
?, ???
?, ??
?, ???
?, ??
?, ?Suffix?, ?Noun?, ?PPP?, ?Verb?, ?PPP?,???&?
?, ??&??
?, ??
&??
&?
?, ??&??&?
?, ?Noun&PPP?, ?PPP&Verb?, ?Suf-fix&Noun&PPP?, ?PPP&Verb&PPP?, and ?Bun-setsu(Beginning)?
were used as features.4.2 Results and Discussion4.2.1 Experiments Using Morpheme ModelsResults of the morphological analysis obtained byusing morpheme models are shown in Table 3 and4.
In these tables, OOV indicates Out-of-Vocabularyrates.
Shown in Table 3, OOV was calculated as theproportion of words not found in a dictionary to allwords in the test corpus.
In Table 4, OOV was cal-culated as the proportion of word and POS categorypairs that were not found in a dictionary to all pairsin the test corpus.
Recall is the percentage of mor-phemes in the test corpus for which the segmentationand major POS category were identified correctly.Precision is the percentage of all morphemes identi-fied by the system that were identified correctly.
TheF-measure is defined by the following equation.F ?
measure =2?
Recall ?
PrecisionRecall + PrecisionTable 3: Accuracies of word segmentation.Word Recall Precision F OOVShort 97.47% (61,44463,037) 97.62% (61,44462,945) 97.54 1.66%99.23% (62,55363,037) 99.11% (62,55363,114) 99.17 0%Long 96.72% (50,09551,796) 95.70% (50,09552,346) 96.21 5.81%99.05% (51,30651,796) 98.58% (51,30652,047) 98.81 0%Table 4: Accuracies of word segmentation and POStagging.Word Recall Precision F OOVShort 95.72% (60,34163,037) 95.86% (60,34162,945) 95.79 2.64%97.57% (61,50563,037) 97.45% (61,50563,114) 97.51 0%Long 94.71% (49,05851,796) 93.72% (49,05852,346) 94.21 6.93%97.30% (50,39651,796) 96.83% (50,39652,047) 97.06 0%Tables 3 and 4 show that accuracies would im-prove significantly if no words were unknown.
Thisindicates that all morphemes of the CSJ could be an-alyzed accurately if there were no unknown words.The improvements that we can expect by detectingunknown words and putting them into dictionariesare about 1.5 in F-measure for detecting word seg-ments of short words and 2.5 for long words.
For de-tecting the word segments and their POS categories,for short words we expect an improvement of about2 in F-measure and for long words 3.Next, we discuss accuracies obtained when un-known words existed.
The OOV for long wordswas 4% higher than that for short words.
In gen-eral, the higher the OOV is, the more difficult de-tecting word segments and their POS categoriesis.
However, the difference between accuraciesfor short and long words was about 1% in recalland 2% in precision, which is not significant whenwe consider that the difference between OOVs forshort and long words was 4%.
This result indi-cates that our morpheme models could detect bothknown and unknown words accurately, especiallylong words.
Therefore, we investigated the recallof unknown words in the test corpus, and foundthat 55.7% (928/1,667) of short word segments and74.1% (2,660/3,590) of long word segments weredetected correctly.
In addition, regarding unknownwords, we also found that 47.5% (791/1,667) ofshort word segments plus their POS categories and67.3% (2,415/3,590) of long word segments plustheir POS categories were detected correctly.
Therecall of unknown words was about 20% higher forlong words than for short words.
We believe thatthis result mainly depended on the difference be-tween short words and long words in terms of thedefinitions of compound words.
A compound wordis defined as one word when it is based on the def-inition of long words; however it is defined as twoor more words when it is based on the definition ofshort words.
Furthermore, based on the definition ofshort words, a division of compound words dependson its context.
More information is needed to pre-cisely detect short words than is required for longwords.
Next, we extracted words that were detectedby the morpheme model but were not found in a dic-tionary, and investigated the percentage of unknownwords that were completely or partially matched tothe extracted words by their context.
This percent-age was 77.6% (1,293/1,667) for short words, and80.6% (2,892/3,590) for long words.
Most of the re-maining unknown words that could not be detectedby this method are compound words.
We expect thatthese compounds can be detected during the manualexamination of those words for which the morphememodel estimated a low probability, as will be shownlater.The recall of unknown words was lower than thatof known words, and the accuracy of automatic mor-phological analysis was lower than that of manualmorphological analysis.
As previously stated, toimprove the accuracy of the whole corpus we takea semi-automatic approach.
We assume that thesmaller the probability is for an output morphemeestimated by a model, the more likely the outputmorpheme is wrong, and we examine output mor-phemes in ascending order of their probabilities.
Weinvestigated how much the accuracy of the wholecorpus would increase.
Fig.
5 shows the relation-ship between the percentage of output morphemeswhose probabilities exceed a threshold and their9394959697989910020 30 40 50 60 70 80 90 100Precision(%)Output Rates (%)"short_without_UKW""long_without_UKW""short_with_UKW""long_with_UKW"Figure 5: Partial analysis.precision.
In this figure, ?short without UKW?,?long without UKW?
?, ?short with UKW?, and?long with UKW?
represent the precision for shortwords detected assuming there were no unknownwords, precision for long words detected assumingthere were no unknown words, precision of shortwords including unknown words, and precision oflong words including unknown words, respectively.When the output rate in the horizontal axis in-creases, the number of low-probability morphemesincreases.
In all graphs, precisions monotonouslydecrease as output rates increase.
This means thattagging errors can be revised effectively when mor-phemes are examined in ascending order of theirprobabilities.Next, we investigated the relationship between thepercentage of morphemes examined manually andthe precision obtained after detected errors were re-vised.
The result is shown in Fig.
6.
Precisionrepresents the precision of word segmentation andPOS tagging.
If unknown words were detected andput into a dictionary by the method described in thefourth paragraph of this section, the graph line forshort words would be drawn between the graph lines?short without UKW?
and ?short with UKW?, andthe graph line for long words would be drawn be-tween the graph lines ?long without UKW?
and?long with UKW?.
Based on test results, we canexpect better than 99% precision for short wordsand better than 97% precision for long words in thewhole corpus when we examine 10% of output mor-939495969798991000 20 40 60 80 100 120Precision(%)Examined Morpheme Rates (%)"short_without_UKW""long_without_UKW""short_with_UKW""long_with_UKW"Figure 6: Relationship between the percentage ofmorphemes examined manually and precision ob-tained after revising detected errors (when mor-phemes with probabilities under threshold and theiradjacent morphemes are examined).01020304050600 5 10 15 20 25 30 35 40 45 50Error RatesinExaminedMorphemes(%)Examined Morpheme Rates (%)"short_without_UKW""short_with_UKW""long_without_UKW""long_with_UKW"Figure 7: Relationship between percentage of mor-phemes examined manually and error rate of exam-ined morphemes.phemes in ascending order of their probabilities.Finally, we investigated the relationship betweenpercentage of morphemes examined manually andthe error rate for all of the examined morphemes.The result is shown in Fig.
7.
We found that about50% of examined morphemes would be found as er-rors at the beginning of the examination and about20% of examined morphemes would be found aserrors when examination of 10% of the whole cor-pus was completed.
When unknown words were de-tected and put into a dictionary, the error rate de-creased; even so, over 10% of examined morphemeswould be found as errors.4.2.2 Experiments Using Chunking ModelsResults of the morphological analysis of longwords obtained by using a chunking model areshown in Table 5 and 6.
The first and second linesTable 5: Accuracies of long word segmentation.Model Recall Precision FMorph 96.72% (50,09551,796) 95.70% (50,09552,346) 96.21Chunk 97.65% (50,58051,796) 97.41% (50,58051,911) 97.54Chunk 98.84% (51,19351,796) 98.66% (51,19351,888) 98.75Table 6: Accuracies of long word segmentation andPOS tagging.Model Recall Precision FMorph 94.71% (49,05851,796) 93.72% (49,05852,346) 94.21Chunk 95.59% (49,51351,796) 95.38% (49,51351,911) 95.49Chunk 98.56% (51,05151,796) 98.39% (51,05151,888) 98.47Chunk w/o TR 92.61% (47,96851,796) 92.40% (47,96851,911) 92.51TR : transformation rulesshow the respective accuracies obtained when OOVswere 5.81% and 6.93%.
The third lines show the ac-curacies obtained when we assumed that the OOVfor short words was 0% and there were no errors indetecting short word segments and their POS cate-gories.
The fourth line in Table 6 shows the accuracyobtained when a chunking model without transfor-mation rules was used.The accuracy obtained by using the chunkingmodel was one point higher in F-measure than thatobtained by using the morpheme model, and it wasvery close to the accuracy achieved for short words.This result indicates that errors newly produced byapplying a chunking model to the results obtainedfor short words were slight, or errors in the resultsobtained for short words were amended by apply-ing the chunking model.
This result also shows thatwe can achieve good accuracy for long words by ap-plying a chunking model even if we do not detectunknown long words and do not put them into a dic-tionary.
If we could improve the accuracy for shortwords, the accuracy for long words would be im-proved also.
The third lines in Tables 5 and 6 showthat the accuracy would improve to over 98 pointsin F-measure.
The fourth line in Tables 6 shows thattransformation rules significantly contributed to im-proving the accuracy.Considering the results obtained in this sectionand in Section 4.2.1, we are now detecting short andlong word segments and their POS categories in thewhole corpus by using the following steps:1.
Automatically detect and manually examineunknown words for short words.2.
Improve the accuracy for short words in thewhole corpus by manually examining shortwords in ascending order of their probabilitiesestimated by a morpheme model.3.
Apply a chunking model to the short words todetect long word segments and their POS cate-gories.As future work, we are planning to use an activelearning method such as that proposed by Argamon-Engelson and Dagan (Argamon-Engelson and Da-gan, 1999) to more effectively improve the accuracyof the whole corpus.5 ConclusionThis paper described two methods for detectingword segments and their POS categories in aJapanese spontaneous speech corpus, and describeshow to tag a large spontaneous speech corpus accu-rately by using the two methods.
The first method isused to detect any type of word segments.
We foundthat about 80% of unknown words could be semi-automatically detected by using this method.
Thesecond method is used when there are several defi-nitions for word segments and their POS categories,and when one type of word segments includes an-other type of word segments.
We found that betteraccuracy could be achieved by using both methodsthan by using only the first method alone.Two types of word segments, short words andlong words, are found in a large spontaneous speechcorpus, CSJ.
We found that the accuracy of auto-matic morphological analysis for the short wordswas 95.79 in F-measure and for long words, 95.49.Although the OOV for long words was much higherthan that for short words, almost the same accuracywas achieved for both types of words by using ourproposed methods.
We also found that we can ex-pect more than 99% of precision for short words,and 97% for long words found in the whole corpuswhen we examined 10% of output morphemes in as-cending order of their probabilities as estimated bythe proposed models.In our experiments, only the information con-tained in the corpus was used; however, more appro-priate linguistic knowledge than that could be used,such as morphemic and syntactic rules.
We wouldlike to investigate whether such linguistic knowl-edge contributes to improved accuracy.ReferencesS.
Argamon-Engelson and I. Dagan.
1999.
Committee-BasedSample Selection For Probabilistic Classifiers.
Artificial In-telligence Research, 11:335?360.A.
L. Berger, S. A. Della Pietra, and V. J. Della Pietra.
1996.
AMaximum Entropy Approach to Natural Language Process-ing.
Computational Linguistics, 22(1):39?71.E.
T. Jaynes.
1957.
Information Theory and Statistical Me-chanics.
Physical Review, 106:620?630.E.
T. Jaynes.
1979.
Where do we Stand on Maximum Entropy?In R. D. Levine and M. Tribus, editors, The Maximum En-tropy Formalism, page 15.
M. I. T. Press.H.
Kashioka, S. G. Eubank, and E. W. Black.
1997.
Decision-Tree Morphological Analysis Without a Dictionary forJapanese.
In Proceedings of NLPRS, pages 541?544.K.
Maekawa, H. Koiso, S. Furui, and H. Isahara.
2000.
Sponta-neous Speech Corpus of Japanese.
In Proceedings of LREC,pages 947?952.S.
Mori and M. Nagao.
1996.
Word Extraction from Cor-pora and Its Part-of-Speech Estimation Using DistributionalAnalysis.
In Proceedings of COLING, pages 1119?1122.M.
Nagata.
1999.
A Part of Speech Estimation Method forJapanese Unknown Words Using a Statistical Model of Mor-phology and Context.
In Proceedings of ACL, pages 277?284.K.
Uchimoto, S. Sekine, and H. Isahara.
2001.
The UnknownWord Problem: a Morphological Analysis of Japanese UsingMaximum Entropy Aided by a Dictionary.
In Proceedingsof EMNLP, pages 91?99.K.
Uchimoto, C. Nobata, A. Yamada, S. Sekine, and H. Isahara.2002.
Morphological Analysis of The Spontaneous SpeechCorpus.
In Proceedings of COLING, pages 1298?1302.
