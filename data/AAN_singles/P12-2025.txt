Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 125?129,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsCross-lingual Parse Disambiguation based on Semantic CorrespondenceLea FrermannDepartment of Computational LinguisticsSaarland Universityfrermann@coli.uni-saarland.deFrancis BondLinguistics and Multilingual StudiesNanyang Technological Universitybond@ieee.orgAbstractWe present a system for cross-lingual parsedisambiguation, exploiting the assumptionthat the meaning of a sentence remains un-changed during translation and the fact thatdifferent languages have different ambiguities.We simultaneously reduce ambiguity in multi-ple languages in a fully automatic way.
Eval-uation shows that the system reliably discardsdispreferred parses from the raw parser output,which results in a pre-selection that can speedup manual treebanking.1 IntroductionTreebanks, sets of parsed sentences annotated with asytactic structure, are an important resource in NLP.The manual construction of treebanks, where a hu-man annotator selects a gold parse from all parsesreturned by a parser, is a tedious and error prone pro-cess.
We present a system for simultaneous and ac-curate partial parse disambiguation of multiple lan-guages.
Using the pre-selected set of parses returnedby the system, the treebanking process for multiplelanguages can be sped up.The system operates on an aligned parallel cor-pus.
The languages of the parallel corpus are con-sidered as mutual semantic tags: As the meaning ofa sentence stays constant during translation, we areable to resolve ambiguities which exist in only oneof the langauges by only accepting those interpreta-tions which are licensed by the other language.In particular, we select one language as the tar-get language, translate the other language?s seman-tics for every parse into the target language and thusalign maximally similar semantic representations.The parses with the most overlapping semantics areselected as preferred parses.As an example consider the English sentence Theyclosed the shop at five, which has the following twointerpretations due to PP attachment ambiguity:1(1) ?At five, they closed the shop?close(they, shop); at(close, 5)(2) ?The shop at five was closed by them?close(they, shop); at(shop, 5)The Japanese translation is also ambiguous, but ina completely different way: it has the possibility ofa zero pronoun (we show the translated semantics).
(3) ?karehe?raPL?waTOP?55?jihour?niat?miseshop?woACC?
?shimeclose?taPAST?At 5 o?clock, they closed the shop.
?close(they, shop); at(close, 5)(4) ?At 5 o?clock, as for them, someone closed the shop.
?close(?, shop); at(close, 5)topic(they,close)We show the semantic representation of the ambi-guity with each sentence.
Both languages are disam-biguated by the other language as only the Englishinterpretation (1) is supported in Japanese, and onlythe Japanese interpretation (3) leads to a grammati-cal English sentence.2 Related WorkThere is no group using exactly the same approachas ours: automated parallel parse disambiguationon the basis of semantic analyses.
Zhechev and1In fact it has four, as they can be either plural or the androg-ynous singular, this is also disambiguated by the Japanese.125Way (2008) automatically generate parallel tree-banks for training of statistical machine translation(SMT) systems through sub-tree alignment.
We donot aim to carry out the complete treebanking pro-cess, but to optimize speed and precision of manualcreation of high-quality treebanks.Wu (1997) and others have tried to simultane-ously learn grammars from bilingual texts.
Burkettand Klein (2008) induce node-alignments of syntac-tic trees with a log-linear model, in order to guidebilingual parsing.
Chen et al (2011) translate anexisting treebank using an SMT system and thenproject parse results from the treebank to the otherlanguage.
This results in a very noisy treebank, thatthey then clean.
These approaches align at the syn-tactic level (using CFGs and dependencies respec-tively).In contrast to the above approaches, we assumethe existence of grammars and use a semantic rep-resentation as the appropriate level for cross-lingualprocessing.
We compare semantic sub-structures, asthose are more straightforwardly comparable acrossdifferent languages.
As a consequence, our systemis applicable to any combination of languages.
Theinput is plain parallel text, neither side needs to betreebanked.3 Materials and MethodsWe use grammars within the grammatical frame-work of head-driven phrase-structure grammar(HPSG Pollard and Sag (1994)), with the seman-tic representation of minimal recursion semantics(MRS; Copestake et al (2005)).
We use two large-scale HPSG grammars and a Japanese-English ma-chine translation system, all of which were de-veloped in the DELPH-IN framework:2 The En-glish Resource Grammar (ERG; Flickinger (2000))is used for English parsing, and Jacy (Bender andSiegel, 2004) for parsing Japanese.
For Japaneseto English translation we use Jaen, a semantic-transfer based machine translation system (Bondet al, 2011).3.1 Semantic Interface and AlignmentFor the alignment, we convert the MRS struc-tures into simplified elementary dependency graphs2http://www.delph-in.net/x4:pronoun_q[]e2:_close_v_c[ARG1 x4:pron, ARG2 x9:_shop_n_of]x9:_the_q[]e8:_at_p_temp[ARG1 e2, ARG2 x16:_num_hour(5)]x16:_def_implicit_q[]Figure 1: EDG for They closed the shop at five.
(EDGs), which abstract away information aboutgrammatical properties of relations and scopal in-formation.
Preliminary experiments showed that theformer kind of information did not contribute to dis-ambiguation performance, as number is typicallyunderspecified in Japanese.
As we only consider lo-cal information in the alignment, scopal informationcan be ignored as well.
An example EDG is dis-played in Figure 1.An EDG consists of a bag of elementary predi-cates (EPs) which are themselves composed of re-lations.
Each line in Figure 1 corresponds to oneEP.
Relations are the elementary building blocks ofthe EDG, and loosely correspond to words of thesurface string.
EPs consist either of atomic rela-tions (corresponding to quantifiers), or a predicate-argument structure which is composed of several re-lations.
During alignment, we only consider non-atomic EPs, as quantifiers should be considered asgrammatical properties of (lexical) relations, whichwe chose to ignore.Given the EDG representations of the translatedJapanese sentence, and the original target languageEDGs, we can straightforwardly align by matchingsubstructures of different granularity.Currently, we align at the predicate level.
We areexperimenting with aligning further dependency re-lation based tuples, which would allow us to resolvemore structural ambiguities.3.2 The Disambiguation SystemAmbiguity in the analyses for both languages is re-duced on the basis of the semantic analyses returnedfor each sentence-pair, and a reduced set of pre-ferred analyses is returned for both languages.
Foreach sentence-pair, we (1) parse the English andthe Japanese sentence (MRSE and MRSJ ) (2) trans-fer the Japanese MRS analyses to English MRSs(MRSJE) (3) convert the top 11 translated MRSs126and the original English MRSs to EDGs3 (EDGEand EDGJE) (4) align every possible E and JE EDGcombination and determine the set of best aligninganalyses (5) from those, create language specific setsof preferred parses.We are comparing semantic representations of thesame language, the English text from the bilingualcorpus and the English machine translation of theJapanese text.
In order to increase robustness ofour alignment system we not only consider com-plete translations, but also accept partially translatedMRSs in case no complete translation could be pro-duced.
This step significantly increases the recall,while the partial MRSs proved to be informativeenough for parse disambiguation.4 Evaluation and ResultsWe evaluate our model on the task of parse disam-biguation.
We use full sentence match as evaluationmetric, a challenging target.The Tanaka corpus is used for training and testing(Tanaka, 2001).
It is an open corpus of Japanese-English sentence pairs.
We use version (2008-11)which contains 147,190 sentence pairs.
We hold out4,500 sentence pairs each for development and test.For each sentence, we compare the number of the-oretically possible alignments with the number ofpreferred alignments returned by our system.
Onaverage, ambiguity is reduced down to 30%.
ForEnglish 3.76 and for Japanese 3.87 parses out of(at most) 11 analyses remain in the partially disam-biguated list: both languages benefit equally fromthe disambiguation.We evaluate disambiguation accuracy by countingthe number of times the gold parse was present in thepartially disambiguated set (full sentence match).Table 1 shows the alignment accuracy results.The correct parse is included in the reduced setin 80% of the cases for Japanese, and for 82% ofthe cases in English.
We match atomic relationswhen aligning the semantic structures, which is avery generic method applicable to the vast major-ity of sentence pairs.
This leads to a recall score of3These are ranked with a model trained on a hand-treebanked set.
The cutoff was determined empirically: Forboth languages the gold parse is included in the top 11 parses inmore than 97% of the cases.English JapanesePrec F Prec FIncluded 0.820 0.897 0.804 0.887First Rank 0.659 0.791 0.676 0.803MRR 0.713 0.829 0.725 0.837Table 1: Accuracy and F-scores for disambiguation per-formance of our system.
Recall was 99% in every case.?Included?
: inclusion of the gold parse in the reduced setof parses or not.
?First Rank?
: ranking of the preferredparse as top in the reduced list.
?MRR?
: mean reciprocalrank of the gold parse in the list.99%, and an F-Score of 89.7% and 88.7% for En-glish and Japanese, respectively.The reduced list of parser analyses can be furtherranked by the parse ranking model which is includedin the parsers of the respective languages (the samemodels with which we determined the top 11 analy-ses).
Given this ranking, we can evaluate how oftenthe preferred parse is ranked top in our partially dis-ambiguated list; results are shown in the two bottomlines of Table 1.A ranked list of possible preferred parses whosetop rank corresponds with a high probability to thegold parse should further speed up the manual tree-banking process.Performance in the context of the whole pipelineThe performance of parsers and MT systemstrongly influences the end-to-end results of the pre-sented system.
In the results given above, this in-fluence is ignored.
We lose around 29% of our databecause no parse could be produced in one or bothlanguages, or no translation could be produced.
anda further 5% of the sentences did not have the goldparse in the original set of analyses (before align-ment): our system could not possibly select the cor-rect parse in those cases.5 DiscussionOur system builds on the output of two parsers anda machine translation system.
We reduce ambiguityfor all sentence pairs where a parse could be cre-ated for both languages, and for which there was atleast a partial translation.
For these sentences, thecross-lingual alignment component achieves a recallof above 99%, such that we do not lose any addi-127tional data.
The parsers and the MT system includea parse ranking system trained on human gold anno-tations.
We use these models in parsing and transla-tion to select the top 11 analyses.
Our system thusdepends on a range of existing technologies.
How-ever, these technologies are available for a range oflanguages, and we use them for efficient extensionof linguistic resources.The effectiveness of cross-lingual parse disam-biguation on the basis of semantic alignment highlydepends on the languages of choice.
Given that weexploit the differences between languages, pairs ofless related languages should lead to better disam-biguation performance.
Furthermore, disambiguat-ing with more than two languages should improveperformance.
Some ambiguities may be shared be-tween languages.
4One weakness when considering the disam-biguated sentences as training for a parse rankingmodel is that the translation fails on similar kinds ofsentences, so there are some phenomena which weget no examples of ?
the automatically trained tree-bank does not have a uniform coverage of phenom-ena.
Our models may not discriminate some phe-nomena at all.Our system provides large amounts of automati-cally annotated data at the only cost of CPU time:so far we have disambiguated 25,000 sentences: 10times more than the existing hand annotated golddata.
Using the parser output for speeding up man-ual treebanking is most effective if the gold parse isreliably included in the reduced set of parses.
In-creasing precision by accepting more than only themost overlapping parses may lead to more effectivemanual treebanking.The alignment method we propose does not makeany language-specific assumptions, nor is it limitedto align two languages only.
The algorithm is veryflexible, and allows for straightforward explorationof different numbers and combinations of languages.6 Conclusion and Future WorkTranslating a sentence into a different languagechanges its surface form, but not its meaning.
In4For example the PP attachment ambiguity in John said thathe went on Tuesday where either the saying or the going couldhave happened on Tuesday holds in both English and Japanese.parallel corpora, one language can be viewed as asemantic tag of the other language and vice versa,which allows for disambiguation of phenomenawhich are ambiguous in only one of the languages.We use the above observations for cross-lingualparse disambiguation.
We experimented with thelanguage pair of English and Japanese, and wereable to accurately reduce ambiguity in parser anal-yses simultaneously for both languages to 30% ofthe starting ambiguity.
The remaining parses can beused as a pre-selection to speed up the manual tree-banking process.We started working on an extrinsic evaluation ofthe presented system by training a discriminativeparse ranking model on the output of our alignmentprocess.
Augmenting the Gold training data withour data improves the model.
Our next step willbe to evaluate the system as part of the treebankingprocess, and optimize the parameters such as disam-biguation precision vs. amount of disambiguation.As no language-specific assumptions are hardcoded in our disambiguation system, it would bevery interesting to apply the system to different lan-guage pairs as well as groups of more than two lan-guages.
Using a group of languages for disambigua-tion will likely lead to increased and more accuratedisambiguation, as more constraints are imposed onthe data.Probably the most important goal for future workis improving the recall achieved in the complete dis-ambiguation pipeline.
Many sentence-pairs cannotbe disambiguated because either no parse can begenerated for one or both languages, or no (par-tial) translation can be produced.
Following theidea of partial translations, partial parses may be avalid backoff.
For purposes of cross-lingual align-ment, partial structures may contribute enough in-formation for disambiguation.
There has been workregarding partial parsing in the HPSG community(Zhang and Kordoni, 2008), which we would like toexplore.
There is also current work on learning moretypes and instances of transfer rules (Haugereid andBond, 2011).Finally, we would like to investigate more align-ment methods, such as dependency relation basedalignment which we started experimenting with, orEDM-based metrics as presented in (Dridan andOepen, 2011).128AcknowledgmentsThis research was supported in part by the ErasmusMundus Action 2 program MULTI of the EuropeanUnion, grant agreement number 2009-5259-5 andthe the joint JSPS/NTU grant on Revealing MeaningUsing Multiple Languages.
We would like to thankTakayuki Kuribayashi and Dan Flickinger for theirhelp with the treebanking.ReferencesEmily M. Bender and Melanie Siegel.
2004.
Im-plementing the syntax of Japanese numeral clas-sifiers.
In Proceedings of the IJC-NLP-2004.Francis Bond, Stephan Oepen, Eric Nichols, DanFlickinger, Erik Velldal, and Petter Haugereid.2011.
Deep open-source machine translation.Machine Translation, 25(2):87?105.David Burkett and Dan Klein.
2008.
Two languagesare better than one (for syntactic parsing).
In Pro-ceedings of EMNLP, 2008.Wenliang Chen, Jun?ichi Kazama, Min Zhang,Yoshimasa Tsuruoka, Yujie Zhang, Yiou Wang,Kentaro Torisawa, and Haizhou Li.
2011.
SMThelps bitext dependency parsing.
In Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP2011), pages 73?83.
Edinburgh.Ann Copestake, Dan Flickinger, Carl Pollard, andIvan A.
Sag.
2005.
Minimal recursion semantics ?an introduction.
Research on Language and Com-putation, 3:281?332.Rebecca Dridan and Stephan Oepen.
2011.
Parserevaluation using elementary dependency match-ing.
In Proceedings of IWPT.Dan Flickinger.
2000.
On building a more efficientgrammar by exploiting types.
Natural LanguageEngineering, 6(1):15?28.
(Special Issue on Effi-cient Processing with HPSG).Petter Haugereid and Francis Bond.
2011.
Extract-ing transfer rules for multiword expressions fromparallel corpora.
In Proceedings of the Work-shop on Multiword Expressions: from Parsingand Generation to the Real World.Carl Pollard and Ivan A.
Sag.
1994.
HeadDriven Phrase Structure Grammar.
University ofChicago Press, Chicago.Yasuhito Tanaka.
2001.
Compilation of a multilin-gual parallel corpus.
In Proceedings of PACLING2001.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel cor-pora.
Computational Linguistics, 23(3):377?403.Yi Zhang and Valia Kordoni.
2008.
Robust parsingwith a large HPSG grammar.
In Proceedings ofthe Sixth International Conference on LanguageResources and Evaluation (LREC?08).Ventsislav Zhechev and Andy Way.
2008.
Auto-matic generation of parallel treebanks.
In Pro-ceedings of the 22nd International Conference onComputational Linguistics (Coling 2008).129
