Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 239?247,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsFacilitating Mental Modeling in Collaborative Human-Robot Interactionthrough Adverbial CuesGordon Briggs and Matthias ScheutzHuman-Robot Interaction LaboratoryTufts University, Medford, MA 02155, USA{gbriggs,mscheutz}@cs.tufts.eduAbstractMental modeling is crucial for natural human-robot interactions (HRI).
Yet, effective mech-anisms that enable reasoning about and com-munication of mental states are not available.We propose to utilize adverbial cues, routinelyemployed by humans, for this goal and presenta novel algorithm that integrates adverbialmodifiers with belief revision and expression,phrasing utterances based on Gricean conver-sational maxims.
The algorithm is demon-strated in a simple HRI scenario.1 IntroductionAdvances in robotics and autonomous systems arepaving the way for the development of robots thatcan take on increasingly complex tasks without theneed of minute human supervision.
As a resultof this greater autonomy, the interaction styles be-tween humans and robots are slowly shifting fromthose of humans micromanaging robot behaviors(e.g., via remote controls) to more higher-level in-teractions (e.g., verbal commands) which are re-quired for many mixed initiative tasks where hu-mans and robots work together in teams (e.g., insearch and rescue missions).
In order for these jointhuman-robot interactions to be productive and effi-cient, robots must have the ability to communicate innatural and human-like ways (Scheutz et al, 2007).Natural human-like communication in robots, how-ever, requires us to tackle several challenges, includ-ing the development of robust natural language (NL)competencies and the ability to understand and uti-lize a variety of affective, gestural, and other non-linguistic cues that are indicative of the interlocu-tor?s mental states.
Hence, natural human-like in-teraction also requires the construction and mainte-nance of mental models of other agents, especiallyin the context of collaborative team tasks where ac-tions among multiple agents must be coordinated,often through natural language dialogues.Several recent efforts are aimed at endowingrobots with natural language processing capabilitiesto allow for verbal instructions as a first step (e.g.,(Brenner, 2007; Dzifcak et al, 2009; Kress-Gazit etal., 2008; Rybski et al, 2007; Kollar et al, 2010)).Independently, user modeling has been extensivelyexplored in order to generate more natural and pro-ductive human-machine interactions (Kobsa, 2001),including adapting the natural language output of di-alogue systems based on mental models of human-users (Wahlster and Kobsa, 1989).
However, thereis currently no integrated robotic architecture thatincludes explicit mechanisms for efficiently convey-ing natural language information about the robot?s?mental states?
(i.e., beliefs, goals, intentions) to ahuman teammate.
Yet, such mechanisms are notonly desirable to make the robot?s behavior more in-tuitive and predictable to humans, but can also becrucial for team success (e.g., quick updates on goalachievement or early corrections of wrong humanassumptions).We propose a novel integrated belief revision andexpression algorithm that allows robots to track andupdate the beliefs of their interlocutors in a waythat respects Gricean maxims about language usage.The algorithm explicitly models and updates task-relevant beliefs and intentions of all participating239agents.
Whenever a discrepancy is detected betweena human belief (as implied in a natural language ex-pression uttered by the human) and the robot?s men-tal model of the human, the robot generates a naturallanguage response that corrects the discrepancy inthe most effective way.
To achieve effectiveness, therobot uses linguistic rules about the pragmatic im-plications of adverbial modifiers like ?yet?, ?still?,?already?, and others that are used by humans to ef-fectively communicate their beliefs and intentions.The rest of the paper is organized as follows.
Westart with a motivation of our approach based onGricean maxims.
Then, we introduce formalizationsof linguistic devices that humans use to generate ef-fective task-based dialogue interactions and presentour algorithm for generating appropriate utterancesin response to human queries.
Next we use a simpleremote human-robot interaction scenario to demon-strate the operation of the algorithm, followed by adiscussion and summary of our contributions.2 MotivationJoint activity often requires agents to monitor andkeep track of each others?
mental states to ensure ef-fective team performance.
For example, searchersduring rescue operations in disaster zones typicallycoordinate their (distributed and remote) activitiesthrough spoken natural language interactions viawireless audio links to keep team members informedof discoveries and plans of other team members.
Co-ordination as part of joint activities requires two im-portant processes in an agent: (1) building and main-taining a mental model of the other agents?
beliefsand intentions (based on perceived, communicated,and inferred information), which is critical for sit-uational awareness (Lison et al, 2010); and (2) ac-tively supporting the maintenance of others?
mentalmodels of oneself (e.g., by proactively communicat-ing new information to the other agents in ways thatwill allow them to update their mental models).Cohen et al (1990), for example, discuss thenecessity of various communicative acts that serveto synchronize agent belief-models.
These com-municative acts include both linguistic and non-linguistic cues, such as utterances of confirmation(?okay.?)
or signals that indicate intention (puttingon a turn-signal).
In addition to utilizing explicitcues to synchronize belief-models, humans employvarious other mechanisms to convey informationabout one?s own belief-state, in particular, variouslinguistic devices.
A simple, but very powerful lin-guistic mechanism is the use of adverbial cues.Consider a scenario where one agent wants toknow the location of another agent, e.g., whetherthe agent is at home.
A straightforward way to ob-tain this information is to simply ask ?Are you athome??
The other agent can then answer ?yes?
or?no?
accordingly.
Now, suppose the first agent knewthat the second agent was planning to be at home atsome point.
In that case, the agent might ask ?Areyou at home yet??
Note that semantically both ques-tions have the same meaning, but their pragmaticimplications are different as the second implies thatthat agent 1 knows that agent 2 was planning to beat home, while no such implication can be inferredfrom the first query.
Conversely, suppose that agent2 responded ?not yet?
in the first example (insteadof ?no?).
While the semantic meaning is the same as?no?, ?not yet?
communicates to agent 1 that agent2 has the goal to be home.
In general, adverbs like?yet?
can be used to convey information about one?s(or somebody else?s) beliefs concerning mutually-recognized goals and intentions.
Not surprisingly,humans use them regularly and with ease to aid theirinterlocutors with maintaining an accurate model oftheir beliefs and goals.The challenges that need to be addressed to al-low robots to have the above kinds of linguistic ex-changes are: (1) how to formalize the functionalroles of adverbial modifiers in different sentencetypes, and how to use the formalized principles to(2) perform belief updates and (3) generate effectivenatural language responses that are natural, succinct,and complete.
To tackle these three challenges, weturn to Gricean principles that have long be used inpragmatics as guiding principles of human commu-nicative exchanges.3 NL Understanding and GenerationGrice (1975) proposed four general principles to aidin the pragmatic analysis of utterances.
Phrased asrules, it is unsurprising that they have been usedas an inspiration for NL generation systems before.Dale and Reiter (1995) have enlisted the maxims in240their design of an algorithm to generate referring ex-pressions, while others have cited Gricean influencein utterance selection for intelligent tutor systems(Eugenio et al, 2008).
The particular maxims weconsidered are the maxims of quality (G1), quantity(G2), and relevance (G3): (G1) requires one to notsay what one believes is false or for which one lacksadequate evidence; (G2) requires one to make con-tributions as informative as necessary for the currentpurposes of the exchange, but not more informative;and (G3) tersely states ?be relevant.
?Our approach to belief-model synchronizationand utterance selection is based on the above max-ims and attempts to select the most appropriate re-sponse to another agent?s query based on relevanceof semantic content.
It uses speech pragmatic mean-ing postulates for linguistic devices such as adver-bial modifiers to search for a succinct and naturallinguistic representation that captures the intendedupdates.
Rather than explicitly communicating eachand every proposition that needs to be communi-cated to a human to allow the person to updatetheir mental model of the robot, the algorithm makesheavy use of ?implied meanings?, i.e., propositionsthat humans will infer from the way the informa-tion is phrased linguistically.
This allows for muchshorter messages to be communicated than other-wise possible and addresses the second maxim ofquantity.3.1 Formalizing pragmatic implicationsWe start by introducing four types of sentencesas they are found in typical dialogue interac-tions: statements (expressed through declarativesentences), questions (expressed through interrog-ative sentences), commands (expressed throughimperative sentences) and acknowledgments (ex-pressed through words like ?okay?, ?yes?, ?no?,etc.).
For simplicity, we restrict the discussion toone predicate at(?, ?)
which states that agent ?
isin location ?.3.1.1 StatementsWe will use the form Stmt(?, ?, ?, ?)
to ex-press that agent ?
communicates ?
to agent ?
us-ing adverbial modifiers in a set ?.
For exam-ple, Stmt(A2, A1,?at(A2, home), yet) means thatagent A2 tells A1 that it is not at home yet.
Notethat we are indifferent about the exact linguistic rep-resentation of ?
here as the goal is to capture thepragmatic implications.If ?
informs ?
that it is at ?
without any adverbialmodifiers or additional contextual information, thenwe can assume using (G1) that ?
is indeed at thatlocation:[[Stmt(?, ?, at(?, ?
), {})]]c := at(?, ?)
(1)Here we use [[..]]c to denote the ?pragmatic mean-ing?
of an expression in context c, which includestask, goal, belief and discourse aspects.
Next, weinductively define the pragmatic meanings for sev-eral adverbial modifiers ?still?, ?already?, ?now?,and ?not yet?
(the meanings of compound expres-sions such as at(?, ?1) ?
?at(?, ?2) are defined re-cursively in the usual way).If ?
states that it is ?still?
at ?, one can infer that?
is at ?
and that ?
will not be at ?
at some point inthe future:[[Stmt(?, ?, at(?, ?
), {still})]]c := (2)[[Stmt(?, ?, at(?, ?
)), {}]]c ?
Future(?at(?, ?
))If ?
states that it is ?already?
at ?, one can infer that?
is at ?
and that ?
had a goal (expressed via the?G?
operator) to be at ?
at some point in the past:[[Stmt(?, at(?, ?
), {already})]]c := (3)[[Stmt(?, ?, at(?, ?
), {}]]c ?
Past(G(?, at(?, ?
)))If ?
states that it is ?now?
at ?, one can infer that ?is at ?
and that ?
had not been at ?
at some point inthe past:[[Stmt(?, ?, at(?, ?
), {now})]]c := (4)[[Stmt(?, ?, at(?, ?
)), {}]]c ?
Past(?at(?, ?
))If ?
states that it is ?not...yet?
at ?, one can inferthat ?
is not at ?, but has an intention to be at ?.
[[Stmt(?, ?,?at(?, ?
), {yet})]]c := (5)?at(?, ?)
?G(?, at(?, ?
))Even in our limited domain, one must be cog-nizant of the ambiguities that arise from how ad-verbial cues are deployed.
In addition to the simplepresence of an adverbial cue, the location of the ad-verb in a sentence and prosodic factors may affectthe intended meaning of the utterance.
For instance,consider the statements: (a) I am now at ?
; (b) I am241at ?
now; (c) I am still at ?
; and (d) I am still at ?.Statement (a) is a simple situational update utteranceas described above, while (b) could be construed asa statement akin to ?I am already at ?.
Statement(d) could be interpreted as additionally signaling thefrustration of the agent, beyond conveying the infor-mation from (c).It should also be noted that our analysis of theseadverbial cues is to be understood in the limitedcontext of these simple task-related predicates (e.g.at(?, ?)).
Formal definition of these adverbial cuesin general cases is beyond the scope of this paper.For instance, ?yet?
could be used in a context whenthe predicate is not intended by the agent to which itapplies (e.g.
?Has Bill been fired yet??).
In this case,it would probably be incorrect to infer that the agentBill had a goal to be fired.
Instead an inference couldbe made regarding the probabilistic judgments of theinterlocutors regarding the topic agent?s future state.However, in the context of this paper, it is assumedthat ?yet?
is used in the context of goals intended byagents.3.1.2 QuestionsHere we will limit the discussion to two questiontypes, the ?where?
question (regarding locations)and simple ?Yes-No?
questions.If ?
asks ?
about its location in the general sense(?where are you??
), then one can infer that ?
has anintention to know (expressed via the ?IK?
operator,see (Perrault and Allen, 1980)) where ?
is located:[[Askloc(?, ?, {})]]c := IK(?, at(?, ?))
(6)for some ?.If ?
asks ?
whether it is at ?, then one can inferthat ?
has an intention to know whether ?
is at ?
:[[Askyn(?, ?, at(?, ?
), {})]]c := IK(?, at(?, ?))
(7)If ?
is asked by ?
whether it is ?still?
at ?, ?
can in-fer that ?
believes (expressed via the ?B?
operator)that ?
is currently at ?
:[[Askyn(?, ?, at(?, ?
), {still})]]c := (8)[[Askyn(?, ?, at(?, ?
), {})]]c ?B(?, at(?, ?
))If ?
is asked by ?
whether it is at ?
?yet?, ?
caninfer that ?
believes that ?
has a goal to be at ?
:[[Askyn(?, ?, at(?, ?
), {yet})]]c := (9)[[Askyn(?, ?, at(?, ?
), {})]]c ?B(?,G(?, at(?, ?
)))3.1.3 Question-Answer PairsNext, we consider how discourse context as pro-vided by question-answer pairs can further specifythe pragmatic implications.If ?
asks ?
whether it is at ?
withany set of adverbial modifiers ?
(i.e.,Prior(Askyn(?, ?, at(?, ?
), ?))
?
c), and ?responds by stating that it is ?still?
at ?, then onecan infer that ?
has the belief that ?
was at ?
in therecent past:[[Stmt(?, ?, at(?, ?
), {still})]]c := (10)[[Stmt(?, ?, at(?, ?
), {})]]c?B(?,RecPast(at(?, ?
)))where Prior(Askyn(?, ?, at(?, ?
), ?))
?
c.
Also,RecPast(?)
denotes that ?
was true in the recentpast, as distinct from ?
holding at some arbitrarypoint in the past (i.e.
Past(?)).
This distinction isnecessary as it only makes sense to use the adverbialcue at this point if agent ?
believed at(?, ?)
at somerelative and recent point in the past.
Formalizing thiswould require keeping track of the points in time atwhich certain propositions are believed.
To avoidcommitting to a particular temporal modeling sys-tem, we make the simplifying assumption that theRecPast operator is not applied in rules (10) and(11), which is sufficient for the very simple interac-tions examined in this paper.If ?
asks ?
whether it is at ?
with any set of ad-verbial modifiers ?, and ?
responds by stating that itis ?now?
at ?, then one can infer that ?
has the beliefthat ?
is was not at ?
in the recent past:[[Stmt(?, ?, at(?, ?
), {now})]]c := (11)[[Stmt(?, ?, at(?, ?
), {})]]c?B(?,RecPast(?at(?, ?
)))where Prior(Askyn(?, ?, at(?, ?
), ?))
?
c.3.1.4 CommandsWe also briefly describe how command process-ing (which we have studied elsewhere in muchgreater detail (Dzifcak et al, 2009)) can be aug-mented with the inclusion of pragmatic meanings.If ?
orders ?
to travel to ?, then one can infer that ?has a goal for ?
to be at ?
and that ?
intends to knowwhether ?
has received its new goal:[[Cmd(?, ?, at(?, ?
), {})]]c := (12)G(?, at(?, ?
))?IK(?,G(?, at(?, ?
)))242It would be an oversimplification to assume thatthe proposition G(?, at(?, ?))
is immediately un-derstood by all listening agents.
In order to generatethe appropriate goal belief in the target agent, ad-ditional inference rules need to be considered.
Thefollowing rule states that ?
will instantiate the goalG(?, at(?, ?))
when it believes ?
has the same goaland it believes authority(?, ?
), which denotes that?
has command authority over ?
:G(?, at(?, ?))
?
authority(?)
?G(?, at(?, ?
))Other agents would have to wait for an acknowledg-ment that this inference has indeed taken place (as ?could have not heard the initial command utterance).These acknowledgment utterances are described inthe subsequent section.3.1.5 AcknowledgmentsFinally, we consider typical forms of acknowledg-ment.
If ?
utters an acknowledgment (e.g., ?OK.?
)when the previous utterance was a positive statementof location by ?, then one can infer ?
no longer hasthe intention to know ?
?s location:[[Ack(?, ?, {})]]c := ?IK(?, at(?, ?))
(13)for some ?
where for any MPrior(Stmt(?, ?, at(?, ?
), {M})) ?
c.If ?
utters an acknowledgment (e.g., ?OK.?)
whenthe previous utterance was a command by ?
to be at?, then one can infer that[[Ack(?, ?, {})]]c := (14)G(?, at(?, ?))
?G(?, at(?, ?))?
?IK(?,G(?, at(?, ?
)where Prior(Cmd(?, ?, at(?, ?
), {M})) ?
c forany M .We should note here that the distinction betweenexplicitly not intending-to-know and the lack of anintention-to-know has been blurred in the aboverules for the sake of simplicity.
As describedin the subsequent section, agent beliefs are re-moved when contradicted in the current system (i.e.Remove(?,B?)
?
(??)
?
B?).
A more com-prehensive belief update system should allow for amechanism to remove beliefs without the need forexplicit contradiction.3.2 Agent Modeling and Belief UpdatesBelief updates occur whenever an agent ?
receivesan utterance Utt from another agent ?
in contextc.
First, [[Utt]]c is computed using the pragmaticprinciples and definitions developed in Section 3.1.For simplicity, we assume that agents adhere tothe Gricean maxim of quality and, therefore, donot communicate information they do not believe.Hence, all propositions ?
?
[[Utt]]c are assumedto be true and to the extent that they are inconsis-tent with existing beliefs of ?
as determined by ?
?sinference algorithm ?b?, the conflicting beliefs areremoved from the agent?s sets of beliefs Belself (bhere denotes some finite bound on the inference al-gorithm, e.g., resources, computation time, etc.
).1To model other agents hearing the utterance, agent?
derives the set B?B?
= {?|B(?, ?)
?
Belself}for all other agents ?
6= ?.
The agent updates thesebelief sets by applying the same rules as it does toBelself .It should be noted that these belief update rulesare indeed simplifications designed to avoid the is-sue of resolving conflicting information from dif-ferent sources.
These belief update rules would beproblematic, for instance, when agents have incor-rect beliefs (and proceed to communicate them), asno method for belief disputation exists.
For the pur-pose of illustrating the implementation and utility ofadverbial cues, however, they should suffice.
Weset up our environment and rule sets such that theautonomous agent has perfect information about it-self (specifically location), and no utterances existsto communicate propositions that are not about one-self.3.3 Sentence GenerationDepending on the sentence type ?
received (and theextent to which meanings can be resolved, an issuewe will not address in this paper), different responsesentence types are appropriate (e.g., a yes-no ques-1Note that we are not making any assumption about a partic-ular inference algorithm or its (as it will, in general, depend onthe expressive power of the employed logic to represent mean-ings), only that if a contradiction can be reached using the in-ference algorithm, the existing belief needs to be removed (oth-erwise existing beliefs are taken to be consistent with the impli-cations of the utterance).
In our implemented system, we use asimplified version of the resolution inference principle.243tion requires a statement answering the question).The generation of an appropriate response proceedsin two steps.
First, based on the agent?s current setof beliefs Belself , we determine the set of proposi-tions ?comm that the agent has an interest in con-veying.
Second, we attempt to find the smallest ut-terance Utt given a set of pragmatic principles (asspecified in Section 3.1) that communicates one ormore of these propositions and implies the rest forrecipient ?.3.3.1 What to sayIn obtaining a set ?comm of propositions to com-municate, ?
may obey the Gricean maxim of qual-ity by adding a proposition ?
to ?comm only if?
?
Belself .
The maxims of relevance and quan-tity are heeded by restricting believed propositionsto be conveyed solely to those that either correct afalse belief of ?
or provide ?
some piece of infor-mation it wants to know.
Specifically, we find theset of all propositions used to correct false beliefs?rev, defined as:?
?
?rev ?
?
?, ?
:B(?, ?)
?
?
?
Belself ?
(?
?b?
??
)The set of all propositions other agents want toknow, ?IK , can be defined as:?
?
?IK ?
?
?, ?
: ?
?
Belself?IK(?, ?
?
Belself ) ?
(?
?b?
?
?
?
?b?
??
)The final set of propositions to convey is obtainedby merging these two sets, ?comm = ?rev ?
?IK .Note that this set is always consistent because propo-sitions are added to ?rev and ?IK if and only if theyexist in Belself , which is maintained to be consis-tent.3.3.2 How to say itOnce ?comm has been obtained, ?must select po-tential utterances to produce.
It starts by generatingan initial set Utt0 of utterances that in the presentcontext c imply some subset of ?comm:(u ?
Utt0) ?
??
?
?comm??
?
?
: ([[u]]c ?b?
?
)Currently, this is achieved by searching throughthe set of all utterances defined by rules such as thosefound in Section 3.1.
Note that while this approachis feasible for our quite limited domain, more effi-cient methods for identifying candidate utterancesmust be developed as the number of understood ut-terances grows.Applying the maxim of quality, this set can bepruned of all utterances that are defined by addi-tional propositions that we either have no evidencefor (?unsupported?)
or explicitly believe to be false:False(?)
?
??
: ?
?
Belself ?
(?
?b?
??)NoSupp(?)
?
???
: ?
?
Belself ?
(?
?b?
?
)Using these conditions, we can generate a new sub-set of utterance candidates Utt1:(u ?
Utt1) ?
???
: ([[u]]c ?b?
?)?(False(?)
?NoSupp(?
)))Applying the maxim of quantity, utterances thatrevise or add the most beliefs to other agent belief-spaces ought to be favored:RevBel(?, ?)
???
: B(?, ?)
?
Bself ?
(?
?b?
??
)AddBel(?, ?)
?
B(?, ?)
6?
BelselfUsing these definitions, we can derive the?correction-score?
of an utterance by countingthe number of propositions ?
?
[[u]]c that revise oradd a belief for ?.If multiple candidate utterances still exist at thispoint, we can again apply the maxim of quantity tofavor utterances that convey the most (true) informa-tion.
Because all definitions with false propositionshave been eliminated, we can simply count the num-ber of true propositions derived from the utterance,thereby favoring semantically richer utterances.
Atthis point, if multiple candidate utterances are stillavailable, the difference is of stylistic nature onlyand we may choose an arbitrary one.
Note that thecorrect usage of adverbial modifiers emerges natu-rally from these rules as utterances that include in-appropriate adverbs are removed in Utt1, while ut-terances that include appropriate adverbial cues aresubsequently favored.2444 Case StudyWe now demonstrate the operation of the proposedalgorithm in a simple joint activity scenario wherea robot (R) is located at nav-point 1 and correctlyknows its location, having the initial belief-spaceBR = {at(R,N1)}.
The remote human operatorstarts by asking:O: R, where are you?R updates its beliefs based on this question:u := parse(?O: R, where are you??)?
u := Askloc(O,R, {})[[u]]c := {IK(O, at(R,N1)), IK(O, at(R,N2)),IK(O, at(R,N3))}Pcontra := contradictedTerms([[u]]c, Bself )BR := (BR ?
Pcontra) + [[u]]cBRBO := (BRBO ?
Pcontra) + [[u]]cwhich yields a new belief-space:BR := {at(R,N1), IK(O, at(R,N1)),IK(O, at(R,N2)), IK(O, at(R,N3)),B(O, IK(O, at(R,N1))), B(O, IK(O, at(R,N2))),B(O, IK(O, at(R,N3)))}Next, R proceeds to respond.
For compactness, werefer below to utterance candidates according to theindex of the applicable rules from Section 3.1, sothat u13 denotes Ack(?, ?, {}).BRBO := {IK(O, at(R,N1)), IK(O, at(R,N2))IK(O, at(R,N3))}?rev := {}; ?IK := {at(R,N1)}?comm := {at(R,N1)};?
Utt0 := {u1, u2, u3, u4}R now has an initial set of candidate utterances,which it prunes using the rules from Section 3.3.2.
[[u1]]c := at(R,N1)[[u2]]c := at(R,N1) ?
Future(?at(R,N1))[[u3]]c := at(R,N1) ?
Past(G(R,N1))[[u4]]c := at(R,N1) ?
Past(?at(R,N1))?
Utt1 := {u1}Thus, R chooses the utterance of the form,Stmt(R,O, at(R,N1), {}), and responds:R: I am at N1.Finally, R processes its own utterance so that it canupdate its beliefs according to rule (1):BR := {at(R,N1), IK(O, at(R,N1)),IK(O, at(R,N2)), IK(O, at(R,N3)),B(O, IK(O, at(R,N1))), B(O, IK(O, at(R,N2))),B(O, IK(O, at(R,N3))), B(O, at(R,N1)}When the operator responds:O: Okay.R also processes this acknowledgment to update itsbeliefs according to rule (13):BR := {at(R,N1), B(O, at(R,N1)}R proceeds to respond, but finds that it has nothingto convey.BRBO := {at(R,N1)}; ?rev := {}; ?IK := {}; ?comm := {};?
Utt0 := {}Thus, R generates no utterance.
Now let us supposethat R moves to N2, and enough time elapses suchthat the operator forfeits his/her conversational turn.R then proceeds to generate an utterance.BR := {at(R,N2), Past(at(R,N1))}BRBO := {at(R,N1)}?rev := {at(R,N2)}; ?IK := {}?comm := {at(R,N2)}?
Utt0 := {u1, u2, u3, u4}[[u1]]c := at(R,N2)[[u2]]c := at(R,N2) ?
Future(?at(R,N2))[[u3]]c := at(R,N2) ?G(R,N2)[[u4]]c := at(R,N2) ?
Past(?at(R,N2))?
Utt1 := {u1, u4}So, R must now resolve which of these candidateutterances to select by choosing the one that revisesthe most beliefs of O, or failing that, the one that hasthe most true propositions.at(R,N2)?
?at(R,N1)?
NumRev([[u1]]c) := 1;NumRev([[u4]]c) := 1;NumTrue([[u1]]c) := 1;NumTrue([[u4]]c) := 2;?
Uttfinal := u4Thus, R chooses the utterance of the form,Stmt(R,O, at(R,N2), {now}), and responds:R: I am now at N2.R again processes its own utterance to update its be-liefs according to rule (4).
If O then asks:O: R, are you still at N2?R updates its beliefs according to rule (10):BR := {at(R,N2), B(O, at(R,N2)),Past(at(R,N1)), B(O,Past(at(R,N1))),IK(O, at(R,N2)), B(O, IK(O, at(R,N2)))}Next, R generates a response:?rev := {}; ?IK := {at(R,N2)}?comm := {at(R,N2)};?
Utt0 := {u1, u2, u10, u11}[[u1]]c := at(R,N2)[[u2]]c := at(R,N2) ?
Future(?at(R,N2))[[u10]]c := at(R,N2) ?B(O, at(R,N2))[[u11]]c := at(R,N2) ?B(O,?at(R,N2))?
Utt1 := {u1, u10}?
NumRev([[u1]]c) := 0;NumRev([[u10]]c) := 0;NumTrue([[u1]]c) := 1;NumTrue([[u10]]c) := 2;?
Uttfinal := u10replying with the utterance:R: I am still at N2.245and processes its own utterance to updates its beliefsaccording to rule (10).
O?s acknowledgment:O: Okay.causes R to update its beliefs according to rule (13):BR := {at(R,N2), B(O, at(R,N2)),Past(at(R,N1)), B(O,Past(at(R,N1)))}R does not generate a response as there are no be-liefs to revise or intentions to know.
Now supposeR moves back to N1, without O?s knowledge, afterwhich O commands:O: R, go to N1.R, updates its belief according to rule (12):BR := {at(R,N1), B(O, at(R,N2)),Past(at(R,N2)), B(O,Past(at(R,N2))),G(R, at(R,N1)), G(O, at(R,N1)),IK(O,G(R, at(R,N1))),B(O,G(R, at(R,N1))),B(O, IK(O,G(R, at(R,N1)))),B(O,G(O, at(R,N1)))}and proceeds to generate a response:?rev := {at(R,N1)}?IK := {G(R, at(R,N1))}?comm := {at(R,N1), G(R, at(R,N1))}?
Utt0 := {u1, u2, u3, u4}[[u1]]c := at(R,N1)[[u2]]c := at(R,N1) ?
Future(?at(R,N1))[[u3]]c := at(R,N1) ?G(R, at(R,N1))[[u4]]c := at(R,N1) ?
Past(?at(R,N1))?
Utt1 := {u1, u3, u4}?
NumRev([[u1]]c) := 1;NumRev([[u3]]c) := 2;NumRev([[u4]]c) := 1?
Uttfinal := u3Thus, R responds:R: I am already at N1.5 Discussion and Related WorkWhile the above case study was kept simple due tospace restrictions, it demonstrates the utility of ourutterance generation method in adapting NL outputat the sentence-level based on a mental-model of aninterlocutor.
In particular, we adapted utterances byemploying adverbial modifiers, which serve to makethe speaker?s belief-space more transparent and nat-ural, which was the main motivation for the devel-opment of the formal framework with rules for ad-verbial modifiers in the first place.
Other examplesof adaptations that are intended to make an auto-mated system?s reasoning and internal state repre-sentations more open and clear to human-users in-clude the sentence-level adaptation of restaurant rec-ommendations (Walker et al, 2007) and the adapta-tion of query-phrasing in a robotic context (Kruijffand Brenner, 2009).
In addition to conveying in-formation about one?s own mental state, pragmaticprinciples and rules, such as those we have pre-sented, may be deployed to reason about the in-tentions and beliefs of others (Perrault and Allen,1980).The current system, while a promising step to-wards more natural task-based dialogue interactions,has several limitations.
Aside from lexical and se-mantic limitations, the currently implemented ad-verbial modifiers are restricted to very simple pred-icates.
Clearly, these restrictions will have to beaddressed and the formal definitions will have tobe widened.
Moreover, the system currently doesnot handle situations where a human?s mental statechanges without the robot?s knowledge, which cancause misunderstandings that need to be detectedand corrected effectively.
Additionally, agents canbe mistaken about their beliefs.
Real-world com-plexities such as these suggest the inclusion of han-dling uncertainty in a belief modeling system (Lisonet al, 2010), potentially by assigning beliefs confi-dence values.
This is clearly an important topic forfuture work.User-model based adaptation of NL output at thesentence level that includes multi-modal compo-nents (Walker et al, 2004) has also not been ad-dressed.
Further study is required to determinewhether our Gricean-inspired utterance selectionmethod can also be applied to non-linguistic com-munication modalities.
Finally, the current sys-tem can only handle simple perceptual updates andhas limitations when handling multi-robot dialogues(neither of which are discussed here for space rea-sons).
The challenges of perceptual updates that willhave to be addressed are investigated in the con-text of a plan-based situated dialogue system forrobots in (Brenner, 2007) and extensions to multi-robot scenarios are explored in (Brenner and Kruijff-Korbayova, 2008).6 ConclusionCompetency in mental modeling is a crucial com-ponent in the development of natural, human-likeinteraction capabilities for robots in mixed initia-tive settings.
We showed that the ability to under-246stand and employ adverbial modifiers can help bothin constructing mental models of human operatorsand conveying one?s own mental state to others.To this end, we made three contributions.
First,we introduced a framework for formalizing differentsentence types and the pragmatic meanings of ad-verbial modifiers.
Second, we showed how one canperform belief updates based on implied meaningsof adverbial modifiers.
And third, we introduceda novel algorithm for generating effective responsesthat obey three Gricean maxims and aid the listenerin appropriate belief updates.
The core properties ofthe algorithm are that it corrects false or missing be-liefs in other agents, that it provides an agent withinformation that is wanted, that it never generates anutterance that implies false propositions, and that itfirst favors utterances that convey more (true) propo-sitions after favoring utterances that revise or addmore beliefs to the listener?s belief-space.
Finally,we demonstrated our algorithm responding to basicoperator queries in a simple case study, correctly us-ing adverbial cues to sound more natural and conveymore information regarding its beliefs.There are extensive avenues to pursue futurework.
For instance, we plan to extend the algo-rithm to include multi-modal perceptual integrationas well as multi-agent multi-dialogue capabilities.A variety of empirical evaluations would be desir-able to evaluate the efficacy and naturalness of theproposed adverbial cues in simulated and real HRItasks.
Additionally, empirical evaluations could alsobe performed to observe additional cues to incorpo-rate into the system.7 AcknowledgmentsThis work was supported by an ONR MURI grant#N00014-07-1-1049 to the second author.
We wishto extend our thanks to Paul Schermerhorn and theanonymous reviewers for providing valuable feed-back.ReferencesA.
Kobsa.
2001.
Generic User Modeling Systems.
UserModeling and User-Adapted Interaction 11: 49?63.B.
Di Eugenio, et al 2008.
Be Brief, And They ShallLearn: Generating Concise Language Feedback for aComputer Tutor.
International Journal of Artificial In-telligence in Education 18(4).C.
R. Perrault and J. F. Allen.
1980.
A Plan-Based Anal-ysis of Indirect Speech Acts.
American Journal ofComputational Linguistics, 6(3-4):167?182.G.
M. Kruijff and M. Brenner.
2009.
Phrasing Questions.AAAI 2009 Spring Symposium.H.
Kress-Gazit and G. E. Fainekos and G. J. Pappas 2008Translating Structured English to Robot ControllersAdvanced Robotics 22, 12, 1343?1359H.
P. Grice.
1975.
Logic and conversation.
Syntax andSemantics, 3(1):43?58.J.
Dzifcak, M. Scheutz, C. Baral, and P. Schermerhorn.2009.
What to do and how to do it: Translating Nat-ural Language Directives into Temporal and DynamicLogic Representation for Goal Management and Ac-tion Execution.
ICRA.M.
A. Walker, et al 2004.
Generation and evaluation ofuser tailored responses in multimodal dialogue.
Cog-nitive Science 28: 811?840.M.
Walker, et al 2007.
Individual and Domain Adap-tation in Sentence Planning for Dialogue.
Journal ofArtificial Intelligence Research.
30: 413?456.M.
Scheutz, et al 2007.
First Steps toward NaturalHuman-Like HRI.
Autonomous Robots 22(4):411?423.M.
Brenner.
2007.
Situation-Aware Interpretation,Planning and Execution of User Commands by Au-tonomous Robots.
RO-MAN 2007.M.
Brenner and I. Kruijff-Korbayova.
2008.
Continualplanning and acting in dynamic multiagent environ-ments.
12th SEMDIAL Workshop.P.
R. Cohen, et al 1990.
Task-Oriented Dialogue as aConsequence of Joint Activity.
Pacific Rim Interna-tional Conference on Artificial Intelligence.P.
Lison, C. Ehrler, and G. M. Kruijff.
2010.
Belief Mod-elling for Situation Awareness in Human-Robot Inter-action.
19th IEEE International Symposium.R.
Dale and E. Reiter.
1995.
Computational Interpre-tations of the Gricean Maxims in the Generation ofReferring Expressions.
Cognitive Science, 18(1):233?263.P.
Rybski, K. Yoon, J. Stolarz, and M. Veloso.
2007Interactive robot task training through dialog anddemonstration HRI, 49?56T.
Kollar and S. Tellex and D. Roy and N. Roy 2010Toward Understanding Natural Language DirectionsHRI, 259-266W.
Wahlster and A. Kobsa.
1989.
User Models in Dia-log Systems.
User Models in Dialog Systems, 4?34.Springer-Verlag, Berlin.247
