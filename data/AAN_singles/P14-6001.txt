Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials, pages 1?3,Baltimore, Maryland, USA, 22 June 2014.c?2014 Association for Computational LinguisticsGaussian Processes for Natural Language ProcessingTrevor CohnComputing and Information SystemsThe University of Melbournetrevor.cohn@gmail.comDaniel Preot?iuc-Pietro and Neil LawrenceDepartment of Computer ScienceThe University of Sheffield{daniel,n.lawrence}@dcs.shef.ac.uk1 IntroductionGaussian Processes (GPs) are a powerful mod-elling framework incorporating kernels andBayesian inference, and are recognised as state-of-the-art for many machine learning tasks.Despite this, GPs have seen few applications innatural language processing (notwithstandingseveral recent papers by the authors).
We arguethat the GP framework offers many benefits overcommonly used machine learning frameworks,such as linear models (logistic regression, leastsquares regression) and support vector machines.Moreover, GPs are extremely flexible and canbe incorporated into larger graphical models,forming an important additional tool for proba-bilistic inference.
Notably, GPs are one of thefew models which support analytic Bayesian in-ference, avoiding the many approximation errorsthat plague approximate inference techniques incommon use for Bayesian models (e.g.
MCMC,variational Bayes).1GPs accurately model notjust the underlying task, but also the uncertaintyin the predictions, such that uncertainty can bepropagated through pipelines of probabilisticcomponents.
Overall, GPs provide an elegant,flexible and simple means of probabilistic infer-ence and are well overdue for consideration of theNLP community.This tutorial will focus primarily on regressionand classification, both fundamental techniques ofwide-spread use in the NLP community.
WithinNLP, linear models are near ubiquitous, becausethey provide good results for many tasks, supportefficient inference (including dynamic program-ming in structured prediction) and support simpleparameter interpretation.
However, linear mod-els are inherently limited in the types of relation-ships between variables they can model.
Often1This holds for GP regression, but note that approximateinference is needed for non-Gaussian likelihoods.non-linear methods are required for better under-standing and improved performance.
Currently,kernel methods such as Support Vector Machines(SVM) represent a popular choice for non-linearmodelling.
These suffer from lack of interoper-ability with down-stream processing as part of alarger model, and inflexibility in terms of parame-terisation and associated high cost of hyperparam-eter optimisation.
GPs appear similar to SVMs, inthat they incorporate kernels, however their prob-abilistic formulation allows for much wider appli-cability in larger graphical models.
Moreover, sev-eral properties of Gaussian distributions (closureunder integration and Gaussian-Gaussian conju-gacy) means that GP (regression) supports analyticformulations for the posterior and predictive infer-ence.This tutorial will cover the basic motivation,ideas and theory of Gaussian Processes and severalapplications to natural language processing tasks.GPs have been actively researched since the early2000s, and are now reaching maturity: the fun-damental theory and practice is well understood,and now research is focused into their applica-tions, and improve inference algorithms, e.g., forscaling inference to large and high-dimensionaldatasets.
Several open-source packages (e.g.
GPyand GPML) have been developed which allow forGPs to be easily used for many applications.
Thistutorial aims to promote GPs, emphasising theirpotential for widespread application across manyNLP tasks.2 OverviewOur goal is to present the main ideas and theorybehind Gaussian Processes in order to increaseawareness within the NLP community.
The firstpart of the tutorial will focus on the basics of Gaus-sian Processes in the context of regression.
TheGaussian Process defines a prior over functionswhich applied at each input point gives a response1value.
Given data, we can analytically infer theposterior distribution of these functions assumingGaussian noise.This tutorial will contrast two main applicationssettings for regression: interpolation and extrapo-lation.
Interpolation suits the use of simple radialbasis function kernels which bias towards smoothlatent functions.
For extrapolation, however, thechoice of the kernel is paramount, encoding ourprior belief about the type of function wish tolearn.
We present several different kernels, includ-ing non-stationary and kernels for structured data(string and tree kernels).
One of the main issuesfor kernel methods is setting the hyperparameters,which is often done in the support vector literatureusing grid search on held-out validation data.
Inthe GP framework, we can compute the probabil-ity of the data given the model which involves theintegral over the parameter space.
This marginallikelihood or Bayesian evidence can be used formodel selection using only training data, where bymodel selection we refer either to choosing from aset of given covariance kernels or choosing fromdifferent model hyperparameters (kernel parame-ters).
We will present the key algorithms for type-II maximum likelihood estimation with respect tothe hyper-parameters, using gradient ascent on themarginal likelihood.Many problems in NLP involve learning froma range of different tasks.
We present multi-tasklearning models by representing intra-task transfersimply and explicitly as a part of a parameterisedkernel function.
GPs are an extremely flexibleprobabilistic framework and have been success-fully adapted for multi-task learning, by modellingmultiple correlated output variables (Alvarez etal., 2011).
This literature develops early workfrom geostatistics (kriging and co-kriging), onlearning latent continuous spatio-temporal modelsfrom sparse point measurements, a problem set-ting that has clear parallels to transfer learning (in-cluding domain adaptation).In the application section, we start by present-ing an open-source software package for GP mod-elling in Python: GPy.2The first application weapproach the regression task of predicting user in-fluence on Twitter based on a range or profile andword features (Lampos et al., 2014).
We exem-plify how to identifying which features are bestfor predicting user impact by optimising the hy-2http://github.com/SheffieldML/GPyperparameters (e.g.
RBF kernel length-scales) us-ing Automatic Relevance Determination (ARD).This basically gives a ranking in importance ofthe features, allowing interpretability of the mod-els.
Switching to a multi-task regression setting,we present an application to Machine TranslationQuality Estimation.
Our method shows large im-provements over previous state-of-the-art (Cohnand Specia, 2013).
Concepts in automatic kernelselection are exemplified in an extrapolation re-gression setting, where we model word time seriesin Social Media using different kernels (Preot?iuc-Pietro and Cohn, 2013).
The Bayesian evidencehelps to select the most suitable kernel, thus giv-ing an implicit classification of time series.In the final section of the tutorial we give abrief overview of advanced topics in the field ofGPs.
First, we look at non-conjugate likelihoodsfor modelling classification, count and rank data.This is harder than regression, as Bayesian pos-terior inference can no longer be solved analyti-cally.
We will outline strategies for non-conjugateinference, such as expectation propagation and theLaplace approximation.
Second, we will outlinerecent work on scaling GPs to big data using vari-ational inference to induce sparse kernel matrices(Hensman et al., 2013).
Finally ?
time permitting?
we will finish with unsupervised learning in GPsusing the latent variable model (Lawrence, 2004),a non-linear Bayesian analogue of principle com-ponent analysis.3 Outline1.
GP Regression (60 mins)(a) Weight space view(b) Function space view(c) Kernels2.
NLP Applications (60 mins)(a) Sparse GPs: Predicting user impact(b) Multi-output GPs: Modelling multi-annotator data(c) Model selection: Identifying temporalpatterns in word frequencies3.
Further topics (45 mins)(a) Non-congjugate likelihoods: classifica-tion, counts and ranking(b) Scaling GPs to big data: Sparse GPs andstochastic variational inference2(c) Unsupervised inference with the GP-LVM4 InstructorsTrevor Cohn3is a Senior Lecturer and ARC Fu-ture Fellow at the University of Melbourne.
Hisresearch deals with probabilistic machine learn-ing models, particularly structured prediction andnon-parametric Bayesian models.
He has recentlypublished several seminal papers on Gaussian Pro-cess models for NLP with applications rangingfrom translation evaluation to temporal dynamicsin social media.Daniel Preot?iuc-Pietro4is a final year PhD stu-dent in Natural Language Processing at the Uni-versity of Sheffield.
His research deals with ap-plying Machine Learning models to model largevolumes of data, mostly coming from Social Me-dia.
Applications include forecasting future be-haviours of text, users or real world quantities (e.g.political voting intention), user geo-location andimpact.Neil Lawrence5is a Professor at the Universityof Sheffield.
He is one of the foremost experts onGaussian Processes and non-parametric Bayesianinference, with a long history of publications andinnovations in the field, including their applicationto multi-output scenarios, unsupervised learning,deep networks and scaling to big data.
He has beenprogramme chair for top machine learning confer-ences (NIPS, AISTATS), and has run several pasttutorials on Gaussian Processes.ReferencesMauricio A. Alvarez, Lorenzo Rosasco, and Neil D.Lawrence.
2011.
Kernels for vector-valued func-tions: A review.
Foundations and Trends in MachineLearning, 4(3):195?266.Trevor Cohn and Lucia Specia.
2013.
Modelling anno-tator bias with multi-task Gaussian processes: an ap-plication to machine translation quality estimation.In Proceedings of the 51st annual meeting of the As-sociation for Computational Linguistics, ACL.James Hensman, Nicolo Fusi, and Neil D. Lawrence.2013.
Gaussian processes for big data.
In Proceed-ings of the 29th Conference on Uncertainty in Artifi-cial Intelligence, UAI.3http://staffwww.dcs.shef.ac.uk/people/T.Cohn4http://www.preotiuc.ro5http://staffwww.dcs.shef.ac.uk/people/N.LawrenceVasileios Lampos, Nikolaos Aletras, Daniel Preot?iuc-Pietro, and Trevor Cohn.
2014.
Predicting and char-acterising user impact on Twitter.
In Proceedings ofthe 14th Conference of the European Chapter of theAssociation for Computational Linguistics, EACL.Neil D. Lawrence.
2004.
Gaussian process latent vari-able models for visualisation of high dimensionaldata.
NIPS, 16(329-336):3.Daniel Preot?iuc-Pietro and Trevor Cohn.
2013.
A tem-poral model of text periodicities using Gaussian Pro-cesses.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,EMNLP.Kashif Shah, Trevor Cohn, and Lucia Specia.
2013.An investigation on the effectiveness of features fortranslation quality estimation.
In Proceedings of theMachine Translation Summit.3
