First Joint Conference on Lexical and Computational Semantics (*SEM), pages 441?448,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsTakeLab: Systems for Measuring Semantic Text SimilarityFrane S?aric?, Goran Glavas?, Mladen Karan,Jan S?najder, and Bojana Dalbelo Bas?ic?University of ZagrebFaculty of Electrical Engineering and Computing{frane.saric, goran.glavas, mladen.karan, jan.snajder, bojana.dalbelo}@fer.hrAbstractThis paper describes the two systems fordetermining the semantic similarity of shorttexts submitted to the SemEval 2012 Task 6.Most of the research on semantic similarityof textual content focuses on large documents.However, a fair amount of information is con-densed into short text snippets such as socialmedia posts, image captions, and scientific ab-stracts.
We predict the human ratings of sen-tence similarity using a support vector regres-sion model with multiple features measuringword-overlap similarity and syntax similarity.Out of 89 systems submitted, our two systemsranked in the top 5, for the three overall eval-uation metrics used (overall Pearson ?
2ndand 3rd, normalized Pearson ?
1st and 3rd,weighted mean ?
2nd and 5th).1 IntroductionNatural language processing tasks such as text clas-sification (Sebastiani, 2002), text summarization(Lin and Hovy, 2003; Aliguliyev, 2009), informa-tion retrieval (Park et al, 2005), and word sense dis-ambiguation (Schu?tze, 1998) rely on a measure ofsemantic similarity of textual documents.
Researchpredominantly focused either on the document sim-ilarity (Salton et al, 1975; Maguitman et al, 2005)or the word similarity (Budanitsky and Hirst, 2006;Agirre et al, 2009).
Evaluating the similarity ofshort texts such as sentences or paragraphs (Islamand Inkpen, 2008; Mihalcea et al, 2006; Oliva etal., 2011) received less attention from the researchcommunity.
The task of recognizing paraphrases(Michel et al, 2011; Socher et al, 2011; Wan etal., 2006) is sufficiently similar to reuse some of thetechniques.This paper presents the two systems for auto-mated measuring of semantic similarity of shorttexts which we submitted to the SemEval-2012 Se-mantic Text Similarity Task (Agirre et al, 2012).
Wepropose several sentence similarity measures builtupon knowledge-based and corpus-based similarityof individual words as well as similarity of depen-dency parses.
Our two systems, simple and syn-tax, use supervised machine learning, more specif-ically the support vector regression (SVR), to com-bine a large amount of features computed from pairsof sentences.
The two systems differ in the set offeatures they employ.Our systems placed in the top 5 (out of 89 sub-mitted systems) for all three aggregate correlationmeasures: 2nd (syntax) and 3rd (simple) for overallPearson, 1st (simple) and 3rd (syntax) for normal-ized Pearson, and 2nd (simple) and 5th (syntax) forweighted mean.The rest of the paper is structured as follows.
InSection 2 we describe both knowledge-based andcorpus-based word similarity measures.
In Section3 we describe in detail the features used by our sys-tems.
In Section 4 we report the experimental resultscross-validated on the development set as well as theofficial results on all test sets.
Conclusions and ideasfor future work are given in Section 5.2 Word Similarity MeasuresApproaches to determining semantic similarity ofsentences commonly use measures of semantic sim-441ilarity between individual words.
Our systems usethe knowledge-based and the corpus-based (i.e., dis-tributional lexical semantics) approaches, both ofwhich are commonly used to measure the semanticsimilarity of words.2.1 Knowledge-based Word SimilarityKnowledge-based word similarity approaches relyon a semantic network of words, such as Word-Net.
Given two words, their similarity can be esti-mated by considering their relative positions withinthe knowledge base hierarchy.All of our knowledge-based word similarity mea-sures are based on WordNet.
Some measures usethe concept of a lowest common subsumer (LCS)of concepts c1 and c2, which represents the lowestnode in the WordNet hierarchy that is a hypernymof both c1 and c2.
We use the NLTK library (Bird,2006) to compute the PathLen similarity (Leacockand Chodorow, 1998) and Lin similarity (Lin, 1998)measures.
A single word often denotes several con-cepts, depending on its context.
In order to computethe similarity score for a pair of words, we take themaximum similarity score over all possible pairs ofconcepts (i.e., WordNet synsets).2.2 Corpus-based Word SimilarityDistributional lexical semantics models determinethe meaning of a word through the set of all con-texts in which the word appears.
Consequently, wecan model the meaning of a word using its distribu-tion over all contexts.
In the distributional model,deriving the semantic similarity between two wordscorresponds to comparing these distributions.
Whilemany different models of distributional semanticsexist, we employ latent semantic analysis (LSA)(Turney and Pantel, 2010) over a large corpus to es-timate the distributions.For each word wi, we compute a vector xi usingthe truncated singular value decomposition (SVD)of a tf-idf weighted term-document matrix.
The co-sine similarity of vectors xi and xj estimates thesimilarity of the corresponding words wi and wj .Two different word-vector mappings were com-puted by processing the New York Times AnnotatedCorpus (NYT) (Sandhaus, 2008) and Wikipedia.Aside from lowercasing the documents and remov-ing punctuation, we perform no further preprocess-Table 1: Evaluation of word similarity measuresMeasure ws353 ws353-sim ws353-relPathLen 0.29 0.61 -0.05Lin 0.33 0.64 -0.01Dist (NYT) 0.50 0.50 0.51Dist (Wikipedia) 0.62 0.66 0.55ing (e.g., no stopwords removal or stemming).
Uponremoving the words not occurring in at least twodocuments, we compute the tf-idf.
The word vec-tors extracted from NYT corpus and Wikipedia havea dimension of 200 and 500, respectively.We compared the measures by computing theSpearman correlation coefficient on the Word-Sim3531 data set, as well as its similarity and re-latedness subsets described in (Agirre et al, 2009).Table 1 provides the results of the comparison.3 Semantic Similarity of SentencesOur systems use supervised regression with SVR asa learning model, where each system exploits differ-ent feature sets and SVR hyperparameters.3.1 PreprocessingWe list all of the preprocessing steps our systemsperform.
If a preprocessing step is executed by onlyone of our systems, the system?s name is indicatedin parentheses.1.
All hyphens and slashes are removed;2.
The angular brackets (< and >) that enclose thetokens are stripped (simple);3.
The currency values are simplified, e.g.,$US1234 to $1234 (simple);4.
Words are tokenized using the Penn Treebankcompatible tokenizer;5.
The tokens n?t and ?m are replaced with not andam, respectively (simple);6.
The two consecutive words in one sentence thatappear as a compound in the other sentence arereplaced by the said compound.
E.g., cater pil-lar in one sentence is replaced with caterpil-lar only if caterpillar appears in the other sen-tence;1http://www.cs.technion.ac.il/?gabr/resources/data/wordsim353/wordsim353.html4427.
Words are POS-tagged using Penn Treebankcompatible POS-taggers: NLTK (Bird, 2006)for simple, and OpenNLP2 for syntax;8.
Stopwords are removed using a list of 36 stop-words (simple).While we acknowledge that some of the prepro-cessing steps we take may not be common, we didnot have the time to determine the influence of eachindividual preprocessing step on the results to eitherwarrant their removal or justify their presence.Since, for example, sub-par, sub par and subparare treated as equal after preprocessing, we believe itmakes our systems more robust to inputs containingsmall orthographic differences.3.2 Ngram Overlap FeaturesWe use many features previously seen in paraphraseclassification (Michel et al, 2011).
Several featuresare based on the unigram, bigram, and trigram over-lap.
Before computing the overlap scores, we re-move punctuation and lowercase the words.
We con-tinue with a detailed description of each individualfeature.Ngram OverlapLet S1 and S2 be the sets of consecutive ngrams(e.g., bigrams) in the first and the second sentence,respectively.
The ngram overlap is defined as fol-lows:ngo(S1, S2) = 2 ?
(|S1||S1 ?
S2|+|S2||S1 ?
S2|)?1(1)The ngram overlap is the harmonic mean of the de-gree to which the second sentence covers the firstand the degree to which the first sentence covers thesecond.
The overlap, defined by (1), is computed forunigrams, bigrams, and trigrams.Additionally we observe the content ngram over-lap ?
the overlap of unigrams, bigrams, and tri-grams exclusively on the content words.
The con-tent words are nouns, verbs, adjectives, and adverbs,i.e., the lemmas having one of the following part-of-speech tags: JJ, JJR, JJS, NN, NNP, NNS, NNPS,RB, RBR, RBS, VB, VBD, VBG, VBN, VBP, andVBZ.
Intuitively, the function words (prepositions,2http://opennlp.apache.org/conjunctions, articles) carry less semantics than con-tent words and thus removing them might eliminatethe noise and provide a more accurate estimate ofsemantic similarity.In addition to the overlap of consecutive ngrams,we also compute the skip bigram and trigram over-lap.
Skip-ngrams are ngrams that allow arbitrarygaps, i.e., ngram words need not be consecutive inthe original sentence.
By redefining S1 and S2 torepresent the sets of skip ngrams, we employ eq.
(1)to compute the skip-n gram overlap.3.3 WordNet-Augmented Word OverlapOne can expect a high unigram overlap between verysimilar sentences only if exactly the same words (orlemmas) appear in both sentences.
To allow forsome lexical variation, we use WordNet to assignpartial scores to words that are not common to bothsentences.
We define the WordNet augmented cov-erage PWN (?, ?
):PWN (S1, S2) =1|S2|?w1?S1score(w1, S2)score(w, S) =??
?1 if w ?
Smaxw??Ssim(w,w?)
otherwisewhere sim(?, ?)
represents the WordNet path lengthsimilarity.
The WordNet-augmented word over-lap feature is defined as a harmonic mean ofPWN (S1, S2) and PWN (S2, S1).Weighted Word OverlapWhen measuring sentence similarities we givemore importance to words bearing more content, byusing the information contentic(w) = ln?w?
?C freq(w?
)freq(w)where C is the set of words in the corpus andfreq(w) is the frequency of the word w in the cor-pus.
We use the Google Books Ngrams (Michel etal., 2011) to obtain word frequencies because of itsexcellent word coverage for English.
Let S1 and S2be the sets of words occurring in the first and secondsentence, respectively.
The weighted word cover-age of the second sentence by the first sentence is443given by:wwc(S1, S2) =?w?S1?S2 ic(w)?w??S2ic(w?
)The weighted word overlap between two sen-tences is calculated as the harmonic mean of thewwc(S1, S2) and wwc(S2, S1).This measure proved to be very useful, but itcould be improved even further.
Misspelled frequentwords are more frequent than some correctly spelledbut rarely used words.
Hence dealing with mis-spelled words would remove the inappropriate heavypenalty for a mismatch between correctly and incor-rectly spelled words.Greedy Lemma Aligning OverlapThis measure computes the similarity betweensentences using the semantic alignment of lem-mas.
First we compute the word similarity be-tween all pairs of lemmas from the first and thesecond sentence, using either the knowledge-basedor the corpus-based semantic similarity.
We thengreedily search for a pair of most similar lemmas;once the lemmas are paired, they are not consideredfor further matching.
Previous research by Lavieand Denkowski (2009) proposed a similar alignmentstrategy for machine translation evaluation.
Afteraligning the sentences, the similarity of each lemmapair is weighted by the larger information content ofthe two lemmas:sim(l1, l2) = max(ic(l1), ic(l2)) ?
ssim(l1, l2) (2)where ssim(l1, l2) is the semantic similarity be-tween lemmas l1 and l2.The overall similarity between two sentences isdefined as the sum of similarities of paired lemmasnormalized by the length of the longer sentence:glao(S1, S2) =?
(l1,l2)?P sim(l1, l2)max(length(S1), length(S2))where P is the set of lemma pairs obtained by greedyalignment.
We take advantage of greedy align over-lap in two features: one computes glao(?, ?)
by us-ing the Lin similarity for ssim(?, ?)
in (2), while theother feature uses the distributional (LSA) similarityto calculate ssim(?, ?
).Vector Space Sentence SimilarityThis measure is motivated by the idea of composi-tionality of distributional vectors (Mitchell and La-pata, 2008).
We represent each sentence as a sin-gle distributional vector u(?)
by summing the dis-tributional (i.e., LSA) vector of each word w in thesentence S: u(S) =?w?S xw, where xw is thevector representation of the word w. Another sim-ilar representation uW (?)
uses the information con-tent ic(w) to weigh the LSA vector of each wordbefore summation: uW (S) =?w?S ic(w)xw.The simple system uses |cos(u(S1), u(S2))| and|cos(uW (S1), uW (S2))| for the vector space sen-tence similarity features.3.4 Syntactic FeaturesWe use dependency parsing to identify the lemmaswith the corresponding syntactic roles in the twosentences.
We also compute the overlap of the de-pendency relations of the two sentences.Syntactic Roles SimilarityThe similarity of the words or phrases having thesame syntactic roles in the two sentences may be in-dicative of their overall semantic similarity (Oliva etal., 2011).
For example, two sentences with very dif-ferent main predicates (e.g., play and eat) probablyhave a significant semantic difference.Using Lin similarity ssim(?, ?
), we obtain the sim-ilarity between the matching lemmas in a sentencepair for each syntactic role.
Additionally, for eachrole we compute the similarity of the chunks thatlemmas belong to:chunksim(C1, C2) =?l1?C1?l2?C2ssim(l1, l2)where C1 and C2 are the sets of chunks ofthe first and second sentence, respectively.
Thefinal similarity score of two chunks is theharmonic mean of chunksim(C1, C2)/|C1| andchunksim(C1, C2)/|C2| .Syntactic roles that we consider are predicates (p),subjects (s), direct (d), and indirect (i) (i.e., preposi-tional) objects, where we use (o) to mean either (d)or (i).
The Stanford dependency parser (De Marn-effe et al, 2006) produces the dependency parse ofthe sentence.
We infer (p), (s), and (d) from the syn-tactic dependencies of type nsubj (nominal subject),444nsubjpass (nominal subject passive), and dobj (di-rect object).
By combining the prep and pobj de-pendencies (De Marneffe and Manning, 2008), weidentify (i).
Since the (d) in one sentence often se-mantically corresponds to (i) in the other sentence,we pair all (o) of one sentence with all (o) of theother sentence and define object similarity betweenthe two sentences as the maximum similarity amongall (o) pairs.
Because the syntactic role might beabsent from both sentences (e.g., the object in sen-tences ?John sings?
and ?John is thinking?
), we in-troduce additional binary features indicating if thecomparison for the syntactic role in question exists.Many sentences (especially longer ones) have twoor more (p).
In such cases it is necessary to alignthe corresponding predicate groups (i.e., the (p) withits corresponding arguments) between the two sen-tences, while also aggregating the (p), (s), and (o)similarities of all aligned (p) pairs.
The similarityof two predicate groups is defined as the sum of (p),(s), and (o) similarities.
In each iteration, the greedyalgorithm pairs all predicate groups of the first sen-tence with all predicate groups of the second sen-tence and searches for a pair with the maximum sim-ilarity.
Once the predicate groups of two sentenceshave been aligned, we compute the (p) similarity asa weighted sum of (p) similarities for each predicatepair group.
The weight of each predicate group pairequals the larger information content of two predi-cates.
The (s) and (o) similarities are computed inthe same manner.Syntactic Dependencies OverlapSimilar to the ngram overlap features, we measurethe overlap between sentences based on matchingdependency relations.
A similar measure has beenproposed in (Wan et al, 2006).
Two syntactic depen-dencies are considered equal if they have the samedependency type, governing lemma, and dependentlemma.
Let S1 and S2 be the set of all dependencyrelations in the first and the second sentence, respec-tively.
Dependency overlap is the harmonic meanbetween |S1 ?
S2|/|S1| and |S1 ?
S2|/|S2| .
Con-tent dependency overlap computes the overlap in thesame way, but considers only dependency relationsbetween content lemmas.Similarly to weighted word overlap, we com-pute the weighted dependency relations overlap.The weighted coverage of the second sentence de-pendencies with the first sentence dependencies isgiven by:wdrc(S1, S2) =?r?S1?S2 max(ic(g(r)), ic(d(r)))?r?S2 max(ic(g(r)), ic(d(r)))where g(r) is the governing word of the dependencyrelation r, d(r) is the dependent word of the depen-dency relation r, and ic(l) is the information con-tent of the lemma l. Finally, the weighted depen-dency relations overlap is the harmonic mean be-tween wdrc(S1, S2) and wdrc(S2, S1).3.5 Other FeaturesAlthough we primarily focused on developing thengram overlap and syntax-based features, someother features significantly improve the performanceof our systems.Normalized DifferencesOur systems take advantage of the following fea-tures that measure normalized differences in a pairof sentences: (A) sentence length, (B) the nounchunk, verb chunk, and predicate counts, and (C)the aggregate word information content (see Nor-malized differences in Table 2).Numbers OverlapThe annotators gave low similarity scores to manysentence pairs that contained different sets of num-bers, even though their sentence structure was verysimilar.
Socher et al (2011) improved the perfor-mance of their paraphrase classifier by adding thefollowing features that compare the sets of num-bers N1 and N2 in two sentences: N1 = N2,N1?N2 6= ?, and N1 ?
N2?N2 ?
N1.
We replacethe first two features with log (1 + |N1|+ |N2|) and2?
|N1 ?N2|/(|N1|+ |N2|) .
Additionally, the num-bers that differ only in the number of decimal placesare treated as equal (e.g., 65, 65.2, and 65.234 aretreated as equal, whereas 65.24 and 65.25 are not).Named Entity FeaturesShallow NE similarity treats capitalized words asnamed entities if they are longer than one character.If a token in all caps begins with a period, it is clas-sified as a stock index symbol.
The simple system445Table 2: The usage of feature setsFeature set simple syntaxNgram overlap + +Content-ngram overlap - +Skip-ngram overlap - +WordNet-aug. overlap + -Weighted word overlap + +Greedy align.
overlap - +Vector space similarity + -Syntactic roles similarity - +Syntactic dep.
overlap - +Normalized differences* A,C A,BShallow NERC + -Full NERC - +Numbers overlap + +* See Section 3.5uses the following four features: the overlap of cap-italized words, the overlap of stock index symbols,and the two features indicating whether these namedentities were found in either of the two sentences.In addition to the overlap of capitalized words, thesyntax system uses the OpenNLP named entity rec-ognizer and classifier to compute the overlap of en-tities for each entity class separately.
We recognizethe following entity classes: persons, organizations,locations, dates, and rudimentary temporal expres-sions.
The absence of an entity class from both sen-tences is indicated by a separate binary feature (onefeature for each class).Feature Usage in TakeLab SystemsSome of the features presented in the previous sec-tions were used by both of our systems (simple andsyntax), while others were used by only one of thesystems.
Table 2 indicates the feature sets used forthe two submitted systems.4 Results4.1 Model TrainingFor each of the provided training sets we trained aseparate Support Vector Regression (SVR) modelusing LIBSVM (Chang and Lin, 2011).
To ob-tain the optimal SVR parameters C, g, and p, oursystems employ a grid search with nested cross-Table 3: Cross-validated results on train setsMSRvid MSRpar SMTeuroparlsimple 0.8794 0.7566 0.7802syntax 0.8698 0.7144 0.7308validation.
Table 3 presents the cross-validated per-formance (in terms of Pearson correlation) on thetraining sets.
The models tested on the SMTnewstest set were trained on the SMTeuroparl train set.For the OnWn test set, the syntax model was trainedon the MSRpar set, while the simple system?s modelwas trained on the union of all train sets.
The finalpredictions were trimmed to a 0?5 range.Our development results indicate that theweighted word overlap, WordNet-augmented wordoverlap, the greedy lemma alignment overlap, andthe vector space sentence similarity individuallyobtain high correlations regardless of the devel-opment set in use.
Other features proved to beuseful on individual development sets (e.g., syntaxroles similarity on MSRvid and numbers overlapon MSRpar).
More research remains to be done inthorough feature analysis and systematic featureselection.4.2 Test Set ResultsThe organizers provided five different test sets toevaluate the performance of the submitted systems.Table 4 illustrates the performance of our systemson individual test sets, accompanied by their rank.Our systems outperformed most other systems onMSRvid, MSRpar, and OnWN sets (Agirre et al,2012).
However, they performed poorly on theSMTeuroparl and SMTnews sets.
While the corre-lation scores on the MSRvid and MSRpar test setscorrespond to those obtained using cross-validationon the corresponding train sets, the performance onthe SMT test sets is drastically lower than the cross-validated performance on the corresponding trainset.
The sentences in the SMT training set are signif-icantly longer (30.4 tokens on average) than the sen-tences in both SMT test sets (12.3 for SMTeuroparland 13.5 for SMTnews).
Also there are several re-peated pairs of extremely short and identical sen-tences (e.g., ?Tunisia?
?
?Tunisia?
appears 17 times446Table 4: Results on individual test setssimple syntaxMSRvid 0.8803 (1) 0.8620 (8)MSRpar 0.7343 (1) 0.6985 (2)SMTeuroparl 0.4771 (26) 0.3612 (63)SMTnews 0.3989 (46) 0.4683 (18)OnWN 0.6797 (9) 0.7049 (6)Table 5: Aggregate performance on the test setsAll ALLnrm Meansimple 0.8133 (3) 0.8635 (1) 0.6753 (2)syntax 0.8138 (2) 0.8569 (3) 0.6601 (5)in the SMTeuroparl test set).
The above measure-ments indicate that the SMTeuroparl training set wasnot representative of the SMTeuroparl test set for ourchoice of features.Table 5 outlines the aggregate performance of oursystems according to the three aggregate evaluationmeasures proposed for the task (Agirre et al, 2012).Both systems performed very favourably comparedto the other systems, achieving very high rankingsregardless of the aggregate evaluation measure.The implementation of simple system is availableat http://takelab.fer.hr/sts.5 Conclusion and Future WorkIn this paper we described our submission to theSemEval-2012 Semantic Textual Similarity Task.We have identified some high performing featuresfor measuring semantic text similarity.
Althoughboth of the submitted systems performed very wellon all but the two SMT test sets, there is still roomfor improvement.
The feature selection was ad-hocand more systematic feature selection is required(e.g., wrapper feature selection).
Introducing ad-ditional features for deeper understanding (e.g., se-mantic role labelling) might also improve perfor-mance on this task.AcknowledgmentsThis work was supported by the Ministry of Science,Education and Sports, Republic of Croatia under theGrant 036-1300646-1986.
We would like to thankthe organizers for the tremendous effort they put intoformulating this challenging task.ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.
Astudy on similarity and relatedness using distributionaland wordnet-based approaches.
In Proceedings of Hu-man Language Technologies: The 2009 Annual Con-ference of the North American Chapter of the Associa-tion for Computational Linguistics, pages 19?27.
As-sociation for Computational Linguistics.Eneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
SemEval-2012 Task 6: Apilot on semantic textual similarity.
In Proceedingsof the 6th International Workshop on Semantic Eval-uation (SemEval 2012), in conjunction with the FirstJoint Conference on Lexical and Computational Se-mantics (*SEM 2012).
ACL.Ramiz M. Aliguliyev.
2009.
A new sentence similaritymeasure and sentence based extractive technique forautomatic text summarization.
Expert Systems withApplications, 36(4):7764?7772.Steven Bird.
2006.
NLTK: the natural language toolkit.In Proceedings of the COLING/ACL on Interactivepresentation sessions, COLING-ACL ?06, pages 69?72.
Association for Computational Linguistics.Alexander Budanitsky and Graeme Hirst.
2006.
Evalu-ating wordnet-based measures of lexical semantic re-latedness.
Computational Linguistics, 32(1):13?47.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM:A library for support vector machines.
ACM Transac-tions on Intelligent Systems and Technology, 2:27:1?27:27.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Marie-Catherine De Marneffe and Christopher D. Man-ning.
2008.
The Stanford typed dependencies repre-sentation.
In Coling 2008: Proceedings of the work-shop on Cross-Framework and Cross-Domain ParserEvaluation, pages 1?8.
Association for ComputationalLinguistics.Marie-Catherine De Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
In Pro-ceedings of LREC, volume 6, pages 449?454.Aminul Islam and Diana Inkpen.
2008.
Semantictext similarity using corpus-based word similarity andstring similarity.
ACM Transactions on KnowledgeDiscovery from Data (TKDD), 2(2):10.447Alon Lavie and Michael Denkowski.
2009.
The ME-TEOR metric for automatic evaluation of machinetranslation.
Machine translation, 23(2):105?115.Claudia Leacock and Martin Chodorow.
1998.
Com-bining local context and WordNet similarity for wordsense identification.
WordNet: An electronic lexicaldatabase, 49(2):265?283.Chin-Yew Lin and Eduard Hovy.
2003.
Automaticevaluation of summaries using n-gram co-occurrencestatistics.
In Proceedings of the 2003 Conferenceof the North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology-Volume 1, pages 71?78.
Association forComputational Linguistics.Dekang Lin.
1998.
An information-theoretic defini-tion of similarity.
In Proceedings of the 15th inter-national conference on Machine Learning, volume 1,pages 296?304.
San Francisco.Ana G. Maguitman, Filippo Menczer, Heather Roinestad,and Alessandro Vespignani.
2005.
Algorithmic detec-tion of semantic similarity.
In Proceedings of the 14thinternational conference on World Wide Web, pages107?116.
ACM.Jean-Baptiste Michel, Yuan Kui Shen, Aviva P. Aiden,Adrian Veres, Matthew K. Gray, et al 2011.
Quan-titative analysis of culture using millions of digitizedbooks.
Science, 331(6014):176.Rada Mihalcea, Courtney Corley, and Carlo Strapparava.2006.
Corpus-based and knowledge-based measuresof text semantic similarity.
In Proceedings of the na-tional conference on artificial intelligence, volume 21,page 775.
Menlo Park, CA; Cambridge, MA; London;AAAI Press; MIT Press; 1999.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
Proceedings of ACL-08: HLT, pages 236?244.Jesu?s Oliva, Jo?se Ignacio Serrano, Mar?
?a DoloresDel Castillo, and A?ngel Iglesias.
2011.
SyMSS: Asyntax-based measure for short-text semantic similar-ity.
Data & Knowledge Engineering.Eui-Kyu Park, Dong-Yul Ra, and Myung-Gil Jang.
2005.Techniques for improving web retrieval effectiveness.Information processing & management, 41(5):1207?1223.Gerard Salton, Andrew Wong, and Chung-Shu Yang.1975.
A vector space model for automatic indexing.Communications of the ACM, 18(11):613?620.Evan Sandhaus.
2008.
The New York Times annotatedcorpus.
Linguistic Data Consortium, Philadelphia,6(12):e26752.Hinrich Schu?tze.
1998.
Automatic word sense discrimi-nation.
Computational linguistics, 24(1):97?123.Fabrizio Sebastiani.
2002.
Machine learning in auto-mated text categorization.
ACM computing surveys(CSUR), 34(1):1?47.Richard Socher, Eric H. Huang, Jeffrey Pennington, An-drew Y. Ng, and Christopher D. Manning.
2011.
Dy-namic pooling and unfolding recursive autoencodersfor paraphrase detection.
Advances in Neural Infor-mation Processing Systems, 24.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of semantics.Journal of Artificial Intelligence Research, 37(1):141?188.Stephen Wan, Mark Dras, Robert Dale, and Ce?cile Paris.2006.
Using dependency-based features to take the?para-farce?
out of paraphrase.
In Proceedings of theAustralasian Language Technology Workshop, volume2006.448
