Coling 2008: Companion volume ?
Posters and Demonstrations, pages 51?54Manchester, August 2008Scaling up Analogical LearningPhilippe LanglaisUniversite?
de Montre?al / Dept.
I.R.O.C.P.
6128, Que?bec, H3C3J7, Canadafelipe@iro.umontreal.caFranc?ois YvonUniv.
Paris Sud 11 & LIMSI-CNRSF-91401 Orsay, Franceyvon@limsi.frAbstractRecent years have witnessed a growing in-terest in analogical learning for NLP ap-plications.
If the principle of analogicallearning is quite simple, it does involvecomplex steps that seriously limit its ap-plicability, the most computationally de-manding one being the identification ofanalogies in the input space.
In this study,we investigate different strategies for ef-ficiently solving this problem and studytheir scalability.1 IntroductionAnalogical learning (Pirrelli and Yvon, 1999) be-longs to the family of lazy learning techniques(Aha, 1997).
It allows to map forms belong-ing to an input space I into forms of an outputspace O, thanks to a set of known observations,L = {(i, o) : i ?
I, o ?
O}.
I(u) and O(u)respectively denote the projection of an observa-tion u into the input space and output space: ifu ?
(i, o), then I(u) ?
i and O(u) ?
o.
For anincomplete observation u ?
(i, ?
), the inference ofO(u) involves the following steps:1. building EI(u) the set of analogical tripletsof I(u), that is EI(u) = {(s, v, w) ?
L3:[I(s) : I(v) = I(w) : I(u) ]}2. building the set of solutions to the target equa-tions formed by projecting source triplets:EO(u) = {t ?
O : [O(s) : O(v) = O(w) :t ] ,?
(s, v, w) ?
EI(u)}3. selecting candidates among EO(u).c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.where [x : y = z : t ] denotes an analogical pro-portion, that is a relation between these four items,meaning that ?x is to y as z is to t?, in a sense tobe specified.
See (Lepage, 1998) or (Stroppa andYvon, 2005) for possible interpretations.Analogical learning has recently regained someinterest in the NLP community.
Lepage and De-noual (2005) proposed a machine translation sys-tem entirely based on the concept of formal anal-ogy, that is, analogy on forms.
Stroppa andYvon (2005) applied analogical learning to sev-eral morphological tasks also involving analogieson words.
Langlais and Patry (2007) applied it tothe task of translating unknown words in severalEuropean languages, an idea investigated as wellby Denoual (2007) for a Japanese to English trans-lation task.If the principle of analogical learning is quitesimple, it does involve complex steps that seriouslylimit its applicability.
As a matter of fact, we areonly aware of studies where analogical learning isapplied to restricted tasks, either because they ar-bitrarily concentrate on words (Stroppa and Yvon,2005; Langlais and Patry, 2007; Denoual, 2007)or because they focus on limited data (Lepage andDenoual, 2005; Denoual, 2007).In this study, we investigate different strategiesfor making step 1 of analogical learning tractable.We propose a data-structure and algorithms thatallow to control the balance between speed andrecall.
For very high-dimensional input spaces(hundreds of thousand of elements), we proposea heuristic which reduces computation time with alimited impact on recall.2 Identifying input analogical relations2.1 Existing approachesA brute-force approach for identifying the inputtriplets that define an analogy with the incompleteobservation u = (t , ?)
consists in enumerating51triplets in the input space and checking for an ana-logical relation with the unknown form t :EI(u) = { ?x, y, z?
: ?x, y, z?
?
I3,[x : y = z : t ] }This amounts to check o(|I|3) analogies, which ismanageable for toy problems only.Langlais and Patry (2007) deal with an inputspace in the order of tens of thousand forms (thetypical size of a vocabulary) using following strat-egy for EI(u).
It consists in solving analogicalequations [y : x = t : ? ]
for some pairs ?x, y?belonging to the neighborhood1 of I(u), denotedN (t).
Those solutions that belong to the inputspace are the z-forms retained.EI(u) = { ?x, y, z?
: ?x, y?
?
N (t)2,[y : x = t : z] }This strategy (hereafter named LP) directly fol-lows from a symmetrical property of an analogy([x : y = z : t ] ?
[y : x = t : z]), and reducesthe search procedure to the resolution of a numberof analogical equations which is quadratic with thenumber of pairs ?x, y?
sampled.2.2 Exhaustive tree-count searchThe strategy we propose here exploits a prop-erty on character counts that an analogical relationmust fulfill (Lepage, 1998):[x : y = z : t ] ?
|x|c+ |t |c= |y|c+ |z|c?c ?
Awhere A is the alphabet on which the forms arebuilt, and |x|cstands for the number of occur-rences of character c in x .
In the sequel, we de-note C(?x, t?)
= {?y, z?
?
I2 : |x|c+ |t |c=|y|c+ |z|c?c ?
A} the set of pairs that satisfythe count property with respect to ?x, t?
.The strategy we propose consists in first select-ing an x-form in the input space.
This enforces aset of necessary constraints on the counts of char-acters that any two forms y and z must satisfy for[x : y = z : t ] to be true.
By considering all formsx in turn,2 we collect a set of candidate triplets fort .
A verification of those that define with t a anal-ogy must then be carried out.
Formally, we built:EI(u) = { ?x, y, z?
: x ?
I,?y, z?
?
C(?x, t?
),[x : y = z : t ] }1The authors proposed to sample x and y among the clos-est forms in terms of edit-distance to I(u) .2Anagram forms do not have to be considered separately.This strategy will only work if (i) the numberof quadruplets to check is much smaller than thenumber of triplets we can form in the input space(which happens to be the case in practice), and if(ii) we can efficiently identify the pairs ?y, z?
thatsatisfy a set of constraints on character counts.
Tothis end, we propose to organize the input spacethanks to a data structure called a tree-count (seeSection 3), which is easy to built and supports effi-cient runtime retrieval.2.3 Sampled tree-count searchAs shown in (Langlais and Yvon, 2008), usingtree-count to constraint the search allows to ex-haustively solve step 1 for reasonably large inputspaces.
Computing analogies in very large inputspace (hundreds of thousand forms) however re-mains computationally demanding, as the retrievalalgorithm must be carried out o(I) times.
In thiscase, we propose to sample the x-forms:EI(u) = { ?x, y, z?
: x ?
N (t),?y, z?
?
C(?x, t?
),[x : y = t : z] }There is unfortunately no obvious way of se-lecting a good subset N (t) of input forms, asanalogies does not necessarily entail the similar-ity of ?diagonal?
forms, as illustrated by the anal-ogy [une pomme verte : des pommes vertes =une voiture rouge : des voitures rouges], whichinvolves singular/plural commutations in Frenchnominal groups.
In this situation, randomly se-lecting a subset of the input space seems to be areasonable strategy (hereafter RAND).For some analogies however, the first andlast forms share some sequences of charac-ters.
This is obvious in [dream : dreamer =dreams : dreamers], but can be more subtle, asin our first example [This guy drinks too much :This boat sinks = These guys drank too much :These boats sank ] where the diagonal termsshare some n-grams reminiscent of the number(This/These) and tense (drink /drank ) commuta-tions involved.We thus propose a sampling strategy (hereafterEV) which selects x-forms that share with t somesequences of characters.
To this end, input formsare represented in a vector space whose dimen-sions are frequent character n-grams, retaining thek-most frequent n-grams, where n ?
[min;max].A form is thus encoded as a binary vector of52dimension k, in which ith coefficient indicateswhether the form contains an occurrence of the ithn-gram.3 At runtime, we select the N forms thatare the closest to a given form t , according to adistance4.
Figure 1 illustrates some forms selectedby this process.
For comparison purposes, we alsotested a sampling strategy which consists in select-ing the x-forms that are closest to the source formt , according to the usual edit-distance (hereafterED).establish a report ?
order to establish a ?
hastabled this report ?
is about the report ?
basisof the report ?
other problem is that ?
problemthat arises ?
problem is that thoseFigure 1: The 8 nearest neighbors of to establisha report in a vector space computed from an inputspace of over a million phrases.3 The tree-count data-structureA tree-count is a tree which encodes a set of forms.Nodes are labeled by an alphabetical symbol andcontain a (possibly empty) set of pointers to forms.A vertice from a node n labeled c to a node m isweighted by the count of c in the forms encodedby m, that is, the set of forms that can be reachedfrom this node and its descendants.
Thus, a pathin a tree-count represents a set of constraints onthe counts of the characters encountered along thispath.
This structure allows for instance the identi-fication of anagrams in a set of forms: it suffices tosearch the tree-count for nodes that contain morethan one pointer to forms in the vocabulary.An example of a tree-count is provided in Fig-ure 2 for a small set of forms.
The node doublecircled in this figure is labeled by the character dand encodes the 6 input forms that contain 1 oc-currence of ?o?
and 1 occurrence of ?s?.
One formis os , referenced by the pointer m , the other fiveforms are found by descending the tree from thisnode; among which gods and dogs , two anagramsencoded by the leave which set of pointers is b, k.3.1 Construction timeThe construction of a tree-count from a set offorms only needs an arbitrary order on the char-acters of the alphabet.
This is the order in whichwe will encounter them while descending the3Typical values are min=max=3 and k=20000.4We used the Manhattan distance in this study.?u?pa,l?an?gb,kc?
k?
yf,ig,h s?
le,jdmd?
m?
t?
s?
o01 211 111100 111211201Figure 2: The tree-count encoding the set:{soup(a), gods(b), odds(c), sos(d), solo(e),tokyo(f), moot(g), moto(h), kyoto(i), oslo(j),dogs(k), opus(l), os(m), a(n)}.
The character la-beling a node is represented in a box; the counts ofeach character labels each vertice.
Roman lettersin nodes represent pointers to input forms; greeksymbols label internal nodes.tree.
The lack of space prevents us to report theconstruction algorithm (see (Langlais and Yvon,2008)), but it is important to note that it only in-volves a simple traversal of the input forms and istherefore time efficient.
Also worth mentioning,our construction procedure only stores necessarynodes.
This means that when enumerating char-acters in order, we only store zero-count nodes asrequired.
As a result, the depth of a tree-count istypically much lower than the size of the alphabet.3.2 Retrieval timeThe retrieval of C(?x, t?)
can be performed bytraversing the tree-count while maintaining a fron-tier, that is, the set of pairs of nodes in the tree-count that satisfy the constraints on counts encoun-tered so far.
Imagine, for instance, that we arelooking for the pairs of forms that contain exactly3 occurrences of characters o , 2 of characters sand 1 character l , and no other character.
Start-ing from the root node labelled by o , there is onlyone pair of nodes that satisfy the constraint on o:the frontier is therefore {(?, ?)}.
The constrainton s leads to the frontier {(d, ?)}
(since the countof t must be null).
Finally, descending this nodeyields the frontier {(m, (e, j))}, which identifiesthe pairs (os, solo) and (os, oslo) to be the only53ones satisfying the initial set of constraints.The complexity of retrieval is mainly dominatedby the size of the frontier built while traversing atree-count.
In practice, because of the sparsity ofthe space we manipulate in NLP applications, re-trieval is also a fast operation.4 Checking for an analogyStroppa (2005) provides a dynamic programmingalgorithm for checking that a quadruplet is an anal-ogy, whose complexity is o(|x| ?
|y| ?
|z| ?
|t |).5Depending on the application, a large number ofcalls to this algorithm must be performed duringstep 1 of analogical learning.
The following prop-erty helps cutting down the computations:[x : y = z : t ] ?
(x[1] ?
{y[1], z[1]}) ?
(t [1] ?
{y[1], z[1]})(x[$] ?
{y[$], z[$]}) ?
(t [$] ?
{y[$], z[$]})where ?
[$] denotes the last character of ?.
A simpleand efficient trick consists in calling the analogychecking routine only for those triplets that passthis test.5 DiscussionWe investigated the aforementioned search strate-gies by translating 1 000 new words (resp.
phrases)thanks to a translation table populated with pairs ofwords (resp.
pairs of phrases).
We studied the scal-ability of each strategy by varying the size of thetransfer table (small, medium, large).
Precise fig-ures can be found in (Langlais and Yvon, 2008);we summarize here the main outcomes.On the word-task, we compared the tree-countsearch strategy to the LP one.
On the largest word-set (84 000 input words), the former (exact) strat-egy could find an average of 34 597 input analogiesfor 964 test-words at an average response time of1.2 seconds per word, while with the latter strat-egy, an average of 56 analogies could be identifiedfor 890 test-words, in an average of 6.3 seconds.On the sequence-task, where input spaces aremuch larger, we compared the various samplingstrategies presented in Section 2.3.
We set N, thenumber of sampled input forms, to 103 for allsampling strategies.
On the medium size dataset(293 000 input phrases), both ED and RAND per-form badly compared to EV.
With the two for-mer filtering strategies, we could at best identify5In this study, we used the definition of a formal analogyprovided by Stroppa and Yvon (2005).
Lepage (1998) pro-poses a less general definition, which is faster to check.17 input analogies for 38% of the test-phrases (atan average response time of 9 seconds), while withEV, an average 46 analogies could be identified for75% of the test-phrases (in 3 seconds on average).Finally, we checked that the approach we pro-posed scales to very large datasets (several mil-lions of input phrases), which to the best of ourknowledge is simply out of the reach of existingapproaches.
This opens up interesting prospectsfor analogical learning, such as enriching a phrase-based table of the kind being used in statistical ma-chine translation.AcknowledgmentThis study has been accomplished while the firstauthor was visiting Te?le?com ParisTech.ReferencesAha, David A.
1997.
Editorial.
Artificial IntelligenceReview, 11(1-5):7?10.
Special Issue on Lazy Learn-ing.Denoual, Etienne.
2007.
Analogical translation ofunknown words in a statistical machine translationframework.
In Machine Translation Summit, XI,Copenhagen, Sept. 10-14.Langlais, Philippe and Alexandre Patry.
2007.
Trans-lating unknown words by analogical learning.
InEMNLP-CoNLL, pages 877?886, Prague, Czech Re-public, June.Langlais, Philippe and Franc?ois Yvon.
2008.
Scalingup analogies.
Technical report, Te?le?com ParisTech,France.Lepage, Yves and ?Etienne Denoual.
2005.
Purestever example-based machine translation: Detailedpresentation and assessment.
Machine Translation,29:251?282.Lepage, Yves.
1998.
Solving analogies on words: analgorithm.
In COLING-ACL, pages 728?734, Mon-treal, Canada.Pirrelli, Vitto and Franc?ois Yvon.
1999.
The hiddendimension: a paradigmatic view of data-driven NLP.Journal of Experimental & Theroretical Artifical In-telligence, 11:391?408.Stroppa, Nicolas and Franc?ois Yvon.
2005.
An ana-logical learner for morphological analysis.
In 9thConf.
on Computational Natural Language Learning(CoNLL), pages 120?127, Ann Arbor, MI, June.Stroppa, Nicolas.
2005.
De?finitions et caracte?risationsde mode`les a` base d?analogies pour l?apprentissageautomatique des langues naturelles.
Ph.D. thesis,ENST, Paris, France, Nov.54
