c?
2002 Association for Computational LinguisticsIntroduction to the Special Issue onSummarizationDragomir R. Radev?
Eduard Hovy?University of Michigan USC/ISIKathleen McKeown?Columbia University1.
Introduction and DefinitionsAs the amount of on-line information increases, systems that can automatically sum-marize one or more documents become increasingly desirable.
Recent research hasinvestigated types of summaries, methods to create them, and methods to evaluatethem.
Several evaluation competitions (in the style of the National Institute of Stan-dards and Technology?s [NIST?s] Text Retrieval Conference [TREC]) have helped de-termine baseline performance levels and provide a limited set of training material.Frequent workshops and symposia reflect the ongoing interest of researchers aroundthe world.
The volume of papers edited by Mani and Maybury (1999) and a book(Mani 2001) provide good introductions to the state of the art in this rapidly evolvingsubfield.A summary can be loosely defined as a text that is produced from one or moretexts, that conveys important information in the original text(s), and that is no longerthan half of the original text(s) and usually significantly less than that.
Text here isused rather loosely and can refer to speech, multimedia documents, hypertext, etc.The main goal of a summary is to present the main ideas in a document in lessspace.
If all sentences in a text document were of equal importance, producing a sum-mary would not be very effective, as any reduction in the size of a document wouldcarry a proportional decrease in its informativeness.
Luckily, information content in adocument appears in bursts, and one can therefore distinguish between more and lessinformative segments.
Identifying the informative segments at the expense of the restis the main challenge in summarization.Of the many types of summary that have been identified (Borko and Bernier 1975;Cremmins 1996; Sparck Jones 1999; Hovy and Lin 1999), indicative summaries providean idea of what the text is about without conveying specific content, and informativeones provide some shortened version of the content.
Topic-oriented summaries con-centrate on the reader?s desired topic(s) of interest, whereas generic summaries reflectthe author?s point of view.
Extracts are summaries created by reusing portions (words,sentences, etc.)
of the input text verbatim, while abstracts are created by regenerating?
Assistant Professor, School of Information, Department of Electrical Engineering and Computer Scienceand Department of Linguistics, University of Michigan, Ann Arbor.
E-mail: radev@umich.edu.?
ISI Fellow and Senior Project Leader, Information Sciences Institute of the University of SouthernCalifornia, Marina del Rey, CA.
E-mail: hovy@isi.edu.?
Professor, Department of Computer Science, New York University, New York, NY.
E-mail:kathy@cs.columbia.edu.400Computational Linguistics Volume 28, Number 4the extracted content.
Extraction is the process of identifying important material inthe text, abstraction the process of reformulating it in novel terms, fusion the processof combining extracted portions, and compression the process of squeezing out unim-portant material.
The need to maintain some degree of grammaticality and coherenceplays a role in all four processes.The obvious overlap of text summarization with information extraction, and con-nections from summarization to both automated question answering and natural lan-guage generation, suggest that summarization is actually a part of a larger picture.In fact, whereas early approaches drew more from information retrieval, more re-cent approaches draw from the natural language field.
Natural language generationtechniques have been adapted to work with typed textual phrases, in place of se-mantics, as input, and this allows researchers to experiment with approaches to ab-straction.
Techniques that have been developed for topic-oriented summaries are nowbeing pushed further so that they can be applied to the production of long answersfor the question-answering task.
However, as the articles in this special issue show,domain-independent summarization has several specific, difficult aspects that make ita research topic in its own right.2.
Major ApproachesWe provide a sketch of the current state of the art of summarization by describingthe general areas of research, including single-document summarization through ex-traction, the beginnings of abstractive approaches to single-document summarization,and a variety of approaches to multidocument summarization.2.1 Single-Document Summarization through ExtractionDespite the beginnings of research on alternatives to extraction, most work todaystill relies on extraction of sentences from the original document to form a summary.The majority of early extraction research focused on the development of relativelysimple surface-level techniques that tend to signal important passages in the sourcetext.
Although most systems use sentences as units, some work with larger passages,typically paragraphs.
Typically, a set of features is computed for each passage, andultimately these features are normalized and summed.
The passages with the highestresulting scores are sorted and returned as the extract.Early techniques for sentence extraction computed a score for each sentence basedon features such as position in the text (Baxendale 1958; Edmundson 1969), wordand phrase frequency (Luhn 1958), key phrases (e.g., ?it is important to note?)
(Ed-mundson 1969).
Recent extraction approaches use more sophisticated techniques fordeciding which sentences to extract; these techniques often rely on machine learningto identify important features, on natural language analysis to identify key passages,or on relations between words rather than bags of words.The application of machine learning to summarization was pioneered by Kupiec,Pedersen, and Chen (1995), who developed a summarizer using a Bayesian classifierto combine features from a corpus of scientific articles and their abstracts.
Aone etal.
(1999) and Lin (1999) experimented with other forms of machine learning and itseffectiveness.
Machine learning has also been applied to learning individual features;for example, Lin and Hovy (1997) applied machine learning to the problem of de-termining how sentence position affects the selection of sentences, and Witbrock andMittal (1999) used statistical approaches to choose important words and phrases andtheir syntactic context.401Radev, Hovy, and McKeown Summarization: IntroductionApproaches involving more sophisticated natural language analysis to identify keypassages rely on analysis either of word relatedness or of discourse structure.
Someresearch uses the degree of lexical connectedness between potential passages and theremainder of the text; connectedness may be measured by the number of shared words,synonyms, or anaphora (e.g., Salton et al 1997; Mani and Bloedorn 1997; Barzilayand Elhadad 1999).
Other research rewards passages that include topic words, that is,words that have been determined to correlate well with the topic of interest to the user(for topic-oriented summaries) or with the general theme of the source text (Buckleyand Cardie 1997; Strzalkowski et al 1999; Radev, Jing, and Budzikowska 2000).Alternatively, a summarizer may reward passages that occupy important positionsin the discourse structure of the text (Ono, Sumita, and Miike 1994; Marcu 1997b).
Thismethod requires a system to compute discourse structure reliably, which is not possiblein all genres.
This technique is the focus of one of the articles in this special issue (Teufeland Moens 2002), which shows how particular types of rhetorical relations in the genreof scientific journal articles can be reliably identified through the use of classification.An open-source summarization environment, MEAD, was recently developed at theJohns Hopkins summer workshop (Radev et al 2002).
MEAD allows researchers toexperiment with different features and methods for combination.Some recent work (Conroy and O?Leary 2001) has turned to the use of hiddenMarkov models (HMMs) and pivoted QR decomposition to reflect the fact that theprobability of inclusion of a sentence in an extract depends on whether the previoussentence has been included as well.2.2 Single-Document Summarization through AbstractionAt this early stage in research on summarization, we categorize any approach thatdoes not use extraction as an abstractive approach.
Abstractive approaches have usedinformation extraction, ontological information, information fusion, and compression.Information extraction approaches can be characterized as ?top-down,?
since theylook for a set of predefined information types to include in the summary (in con-trast, extractive approaches are more data-driven).
For each topic, the user predefinesframes of expected information types, together with recognition criteria.
For example,an earthquake frame may contain slots for location, earthquake magnitude, number ofcasualties, etc.
The summarization engine must then locate the desired pieces of infor-mation, fill them in, and generate a summary with the results (DeJong 1978; Rau andJacobs 1991).
This method can produce high-quality and accurate summaries, albeit inrestricted domains only.Compressive summarization results from approaching the problem from the pointof view of language generation.
Using the smallest units from the original document,Witbrock and Mittal (1999) extract a set of words from the input document and thenorder the words into sentences using a bigram language model.
Jing and McKeown(1999) point out that human summaries are often constructed from the source docu-ment by a process of cutting and pasting document fragments that are then combinedand regenerated as summary sentences.
Hence a summarizer can be developed toextract sentences, reduce them by dropping unimportant fragments, and then use in-formation fusion and generation to combine the remaining fragments.
In this specialissue, Jing (2002) reports on automated techniques to build a corpus representing thecut-and-paste process used by humans; such a corpus can then be used to train anautomated summarizer.Other researchers focus on the reduction process.
In an attempt to learn rules forreduction, Knight and Marcu (2000) use expectation maximization to train a systemto compress the syntactic parse tree of a sentence in order to produce a shorter but402Computational Linguistics Volume 28, Number 4still maximally grammatical version.
Ultimately, this approach can likely be used forshortening two sentences into one, three into two (or one), and so on.Of course, true abstraction involves taking the process one step further.
Abstractioninvolves recognizing that a set of extracted passages together constitute somethingnew, something that is not explicitly mentioned in the source, and then replacing themin the summary with the (ideally more concise) new concept(s).
The requirement thatthe new material not be in the text explicitly means that the system must have accessto external information of some kind, such as an ontology or a knowledge base, and beable to perform combinatory inference (Hahn and Reimer 1997).
Since no large-scaleresources of this kind yet exist, abstractive summarization has not progressed beyondthe proof-of-concept stage (although top-down information extraction can be seen asone variant).2.3 Multidocument SummarizationMultidocument summarization, the process of producing a single summary of a setof related source documents, is relatively new.
The three major problems introducedby having to handle multiple input documents are (1) recognizing and coping withredundancy, (2) identifying important differences among documents, and (3) ensuringsummary coherence, even when material stems from different source documents.In an early approach to multidocument summarization, information extractionwas used to facilitate the identification of similarities and differences (McKeown andRadev 1995).
As for single-document summarization, this approach produces more of abriefing than a summary, as it contains only preidentified information types.
Identity ofslot values are used to determine when information is reliable enough to include in thesummary.
Later work merged information extraction approaches with regeneration ofextracted text to improve summary generation (Radev and McKeown 1998).
Importantdifferences (e.g., updates, trends, direct contradictions) are identified through a set ofdiscourse rules.
Recent work also follows this approach, using enhanced informationextraction and additional forms of contrasts (White and Cardie 2002).To identify redundancy in text documents, various similarity measures are used.A common approach is to measure similarity between all pairs of sentences and thenuse clustering to identify themes of common information (McKeown et al 1999; Radev,Jing, and Budzikowska 2000; Marcu and Gerber 2001).
Alternatively, systems measurethe similarity of a candidate passage to that of already-selected passages and retainit only if it contains enough new (dissimilar) information.
A popular such measure ismaximal marginal relevance (MMR) (Carbonell, Geng, and Goldstein 1997; Carbonelland Goldstein 1998).Once similar passages in the input documents have been identified, the infor-mation they contain must be included in the summary.
Rather than simply listingall similar sentences (a lengthy solution), some approaches will select a representa-tive passage to convey information in each cluster (Radev, Jing, and Budzikowska2000), whereas other approaches use information fusion techniques to identify repet-itive phrases from the clusters and combine the phrases into the summary (Barzilay,McKeown, and Elhadad 1999).
Mani, Gates, and Bloedorn (1999) describe the use ofhuman-generated compression and reformulation rules.Ensuring coherence is difficult, because this in principle requires some understand-ing of the content of each passage and knowledge about the structure of discourse.In practice, most systems simply follow time order and text order (passages fromthe oldest text appear first, sorted in the order in which they appear in the input).To avoid misleading the reader when juxtaposed passages from different dates allsay ?yesterday,?
some systems add explicit time stamps (Lin and Hovy 2002a).
Other403Radev, Hovy, and McKeown Summarization: Introductionsystems use a combination of temporal and coherence constraints to order sentences(Barzilay, Elhadad, and McKeown 2001).
Recently, Otterbacher, Radev, and Luo (2002)have focused on discourse-based revisions of multidocument clusters as a means forimproving summary coherence.Although multidocument summarization is new and the approaches describedhere are only the beginning, current research also branches out in other directions.
Re-search is beginning on the generation of updates on new information (Allan, Gupta,and Khandelwal 2001).
Researchers are currently studying the production of longeranswers (i.e., multidocument summaries) from retrieved documents, focusing on suchtypes as biographies of people, descriptions of multiple events of the same type(e.g., multiple hurricanes), opinion pieces (e.g., editorials and letters discussing a con-tentious topic), and causes of events.
Another challenging ongoing topic is the gener-ation of titles for either a single document or set of documents.
This challenge will beexplored in an evaluation planned by NIST in 2003.2.4 EvaluationEvaluating the quality of a summary has proven to be a difficult problem, principallybecause there is no obvious ?ideal?
summary.
Even for relatively straightforward newsarticles, human summarizers tend to agree only approximately 60% of the time, mea-suring sentence content overlap.
The use of multiple models for system evaluationcould help alleviate this problem, but researchers also need to look at other methodsthat can yield more acceptable models, perhaps using a task as motivation.Two broad classes of metrics have been developed: form metrics and content met-rics.
Form metrics focus on grammaticality, overall text coherence, and organizationand are usually measured on a point scale (Brandow, Mitze, and Rau 1995).
Content ismore difficult to measure.
Typically, system output is compared sentence by sentenceor fragment by fragment to one or more human-made ideal abstracts, and as in in-formation retrieval, the percentage of extraneous information present in the system?ssummary (precision) and the percentage of important information omitted from thesummary (recall) are recorded.
Other commonly used measures include kappa (Car-letta 1996) and relative utility (Radev, Jing, and Budzikowska 2000), both of which takeinto account the performance of a summarizer that randomly picks passages from theoriginal document to produce an extract.
In the Document Understanding Conference(DUC)-01 and DUC-02 summarization competitions (Harman and Marcu 2001; Hahnand Harman 2002), NIST used the Summary Evaluation Environment (SEE) interface(Lin 2001) to record values for precision and recall.
These two competitions, run alongthe lines of TREC, have served to establish overall baselines for single-document andmultidocument summarization and have provided several hundred human abstractsas training material.
(Another popular source of training material is the Ziff-Davis cor-pus of computer product announcements.)
Despite low interjudge agreement, DUChas shown that humans are better summary producers than machines and that, forthe news article genre, certain algorithms do in fact do better than the simple baselineof picking the lead material.The largest task-oriented evaluation to date, the Summarization Evaluation Con-ference (SUMMAC) (Mani et al 1998; Firmin and Chrzanowski 1999) included threetests: the categorization task (how well can humans categorize a summary comparedto its full text?
), the ad hoc task (how well can humans determine whether a full text isrelevant to a query just from reading the summary?)
and the question task (how wellcan humans answer questions about the main thrust of the source text from readingjust the summary?).
But the interpretation of the results is not simple; studies (Jing etal.
1998; Donaway, Drummey, and Mather 2000; Radev, Jing, and Budzikowska 2000)404Computational Linguistics Volume 28, Number 4show how the same summaries receive different scores under different measures orwhen compared to different (but presumably equivalent) ideal summaries created byhumans.
With regard to interhuman agreement, Jing et al find fairly high consistencyin the news genre only when the summary (extract) length is fixed relatively short.Marcu (1997a) provides some evidence that other genres will deliver less consistency.With regard to the lengths of the summaries produced by humans when not con-strained by a particular compression rate, both Jing and Marcu find great variation.Nonetheless, it is now generally accepted that for single news articles, systems producegeneric summaries indistinguishable from those of humans.Automated summary evaluation is a gleam in everyone?s eye.
Clearly, when anideal extract has been created by human(s), extractive summaries are easy to evalu-ate.
Marcu (1999) and Goldstein et al (1999) independently developed an automatedmethod to create extracts corresponding to abstracts.
But when the number of availableextracts is not sufficient, it is not clear how to overcome the problems of low inter-human agreement.
Simply using a variant of the Bilingual Evaluation Understudy(BLEU) scoring method (based on a linear combination of matching n-grams betweenthe system output and the ideal summary) developed for machine translation (Pap-ineni et al 2001) is promising but not sufficient (Lin and Hovy 2002b).3.
The Articles in this IssueThe articles in this issue move beyond the current state of the art in various ways.Whereas most research to date has focused on the use of sentence extraction for sum-marization, we are beginning to see techniques that allow a system to extract, merge,and edit phrases, as opposed to full sentences, to generate a summary.
Whereas manysummarization systems are designed for summarization of news, new algorithms aresummarizing much longer and more complex documents, such as scientific journalarticles, medical journal articles, or patents.
Whereas most research to date has fo-cused on text summarization, we are beginning to see a move toward summarizationof speech, a medium that places additional demands on the summarization process.Finally, in addition to providing full summarization systems, the articles in this issuealso focus on tools that can aid in the process of developing summarization systems,on computational efficiency of algorithms, and on techniques needed for preprocessingspeech.The four articles that focus on summarization of text share a common theme:Each views the summarization process as consisting of two phases.
In the first, mate-rial within the original document that is important is identified and extracted.
In thesecond, this extracted material may be modified, merged, and edited using genera-tion techniques.
Two of the articles focus on the extraction stage (Teufel and Moens2002; Silber and McCoy 2002), whereas Jing (2002) examines tools for automaticallyconstructing resources that can be used for the second stage.Teufel and Moens propose significantly different techniques for sentence extractionthan have been used in the past.
Noting the difference in both length and structurebetween scientific articles and news, they claim that both the context of sentences anda more focused search for sentences is needed in order to produce a good summarythat is only 2.5% of the original document.
Their approach is to provide a summarythat focuses on the new contribution of the paper and its relation to previous work.They rely on rhetorical relations to provide information about context and to identifysentences relating to, for example, the aim of the paper, its basis in previous work,or contrasts with other work.
Their approach features the use of corpora annotatedboth with rhetorical relations and with relevance; it uses text categorization to extract405Radev, Hovy, and McKeown Summarization: Introductionsentences corresponding to any of seven rhetorical categories.
The result is a set ofsentences that situate the article in respect to its original claims and in relation to otherresearch.Silber and McCoy focus on computationally efficient algorithms for sentence ex-traction.
They present a linear time algorithm to extract lexical chains from a sourcedocument (the lexical-chain approach was originally developed by Barzilay and El-hadad [1997] but used an exponential time algorithm).
This approach facilitates theuse of lexical chains as an intermediate representation for summarization.
Barzilay andElhadad present an evaluation of the approach for summarization with both scientificdocuments and university textbooks.Jing advocates the use of a cut-and-paste approach to summarization in whichphrases, rather than sentences, are extracted from the original document.
She showsthat such an approach is often used by human abstractors.
She then presents an auto-mated tool that is used to analyze a corpus of paired documents and abstracts writtenby humans, in order to identify the phrases within the documents that are used inthe abstracts.
She has developed an HMM solution to the matching problem.
Thedecomposition program is a tool that can produce training and testing corpora forsummarization, and its results have been used for her own summarization program.Saggion and Lapalme (2002) describe a system, SumUM, that generates indicative-informative summaries from technical documents.
To build their system, Saggion andLapalme have studied a corpus of professionally written (short) abstracts.
They havemanually aligned the abstracts and the original documents.
Given the structured formof technical papers, most of the information in the abstracts was also found in either theauthor abstract (20%) or in the first section of the paper (40%) or the headlines or cap-tions (23%).
Based on their observations, the authors have developed an approach tosummarization, called selective analysis, which mimics the human abstractors?
routine.The four components of selective analysis are indicative selection, informative selection,indicative generation, and informative generation.The final article in the issue (Zechner 2002) is distinct from the other articles inthat it addresses problems in summarization of speech.
As in text summarization,Zechner also uses sentence extraction to determine the content of the summary.
Giventhe informal nature of speech, however, a number of significant steps must be takenin order to identify useful segments for extraction.
Zechner develops techniques forremoving disfluencies from speech, for identifying units for extraction that are insome sense equivalent to sentences, and for identifying relations such as question-answer across turns in order to determine when units from two separate turns shouldbe extracted as a whole.
This preprocessing yields a transcript on which standardtechniques for extraction in text (here the use of MMR [Carbonell and Goldstein 1998]to identify relevant units) can operate successfully.Though true abstractive summarization remains a researcher?s dream, the successof extractive summarizers and the rapid development of compressive and similartechniques testifies to the effectiveness with which the research community can addressnew problems and find workable solutions to them.ReferencesAllan, James, Rahul Gupta, and VikasKhandelwal.
2001.
Temporal summariesof news topics.
In Proceedings of the 24thAnnual International ACM SIGIR Conferenceon Research and Development in InformationRetrieval, pages 10?18.Aone, Chinatsu, Mary Ellen Okurowski,James Gorlinsky, and Bjornar Larsen.1999.
A trainable summarizer withknowledge acquired from robust NLPtechniques.
In I. Mani and M. T. Maybury,editors, Advances in Automatic TextSummarization.
MIT Press, Cambridge,pages 71?80.Barzilay, Regina and Michael Elhadad.
1997.406Computational Linguistics Volume 28, Number 4Using lexical chains for textsummarization.
In Proceedings of theACL/EACL?97 Workshop on IntelligentScalable Text Summarization, pages 10?17,Madrid, July.Barzilay, Regina and Michael Elhadad.
1999.Using lexical chains for textsummarization.
In I. Mani and M. T.Maybury, editors, Advances in AutomaticText Summarization.
MIT Press,Cambridge, pages 111?121.Barzilay, Regina, Noe?mie Elhadad, andKathy McKeown.
2001.
Sentence orderingin multidocument summarization.
InProceedings of the Human LanguageTechnology Conference.Barzilay, Regina, Kathleen McKeown, andMichael Elhadad.
1999.
Informationfusion in the context of multi-documentsummarization.
In Proceedings of the 37thAnnual Meeting of the Association forComputational Linguistics, College Park,MD, 20?26 June, pages 550?557.Baxendale, P. B.
1958.
Man-made index fortechnical literature?An experiment.
IBMJournal of Research and Development,2(4):354?361.Borko, H. and C. Bernier.
1975.
AbstractingConcepts and Methods.
Academic Press,New York.Brandow, Ron, Karl Mitze, and Lisa F. Rau.1995.
Automatic condensation ofelectronic publications by sentenceselection.
Information Processing andManagement, 31(5):675?685.Buckley, Chris and Claire Cardie.
1997.Using empire and smart forhigh-precision IR and summarization.
InProceedings of the TIPSTER Text Phase III12-Month Workshop, San Diego, CA,October.Carbonell, Jaime, Y. Geng, and JadeGoldstein.
1997.
Automatedquery-relevant summarization anddiversity-based reranking.
In Proceedingsof the IJCAI-97 Workshop on AI in DigitalLibraries, pages 12?19.Carbonell, Jaime G. and Jade Goldstein.1998.
The use of MMR, diversity-basedreranking for reordering documents andproducing summaries.
In Alistair Moffatand Justin Zobel, editors, Proceedings of the21st Annual International ACM SIGIRConference on Research and Development inInformation Retrieval, Melbourne,Australia, pages 335?336.Carletta, Jean.
1996.
Assessing agreement onclassification tasks: The kappa statistic.Computational Linguistics, 22(2):249?254.Conroy, John and Dianne O?Leary.
2001.Text summarization via hidden Markovmodels.
In Proceedings of the 24th AnnualInternational ACM SIGIR Conference onResearch and Development in InformationRetrieval, pages 406?407.Cremmins, Edward T. 1996.
The Art ofAbstracting.
Information Resources Press,Arlington, VA, second edition.DeJong, Gerald Francis.
1978.
Fast Skimmingof News Stories: The FRUMP System.
Ph.D.thesis, Yale University, New Haven, CT.Donaway, R. L., K. W. Drummey, andL.
A. Mather.
2000.
A comparison ofrankings produced by summarizationevaluation measures.
In Proceedings of theWorkshop on Automatic Summarization,ANLP-NAACL2000, Association forComputational Linguistics, 30 April,pages 69?78.Edmundson, H. P. 1969.
New methods inautomatic extracting.
Journal of theAssociation for Computing Machinery,16(2):264?285.Firmin, T. and M. J. Chrzanowski.
1999.
Anevaluation of automatic textsummarization systems.
In I. Mani andM.
T. Maybury, editors, Advances inAutomatic Text Summarization.
MIT Press,Cambridge, pages 325?336.Goldstein, Jade, Mark Kantrowitz, Vibhu O.Mittal, and Jaime G. Carbonell.
1999.Summarizing text documents: Sentenceselection and evaluation metrics.
InResearch and Development in InformationRetrieval, pages 121?128, Berkeley, CA.Hahn, Udo and Donna Harman, editors.2002.
Proceedings of the DocumentUnderstanding Conference (DUC-02).Philadelphia, July.Hahn, Udo and Ulrich Reimer.
1997.Knowledge-based text summarization:Salience and generalization operators forknowledge base abstraction.
In I. Maniand M. Maybury, editors, Advances inAutomatic Text Summarization.
MIT Press,Cambridge, pages 215?232.Harman, Donna and Daniel Marcu, editors.2001.
Proceedings of the DocumentUnderstanding Conference (DUC-01).
NewOrleans, September.Hovy, E. and C.-Y.
Lin.
1999.
Automatedtext summarization in SUMMARIST.
InI.
Mani and M. T. Maybury, editors,Advances in Automatic Text Summarization.MIT Press, Cambridge, pages 81?94.Jing, Hongyan.
2002.
Using hidden Markovmodeling to decompose human-writtensummaries.
Computational Linguistics,28(4), 527?543.Jing, Hongyan and Kathleen McKeown.1999.
The decomposition ofhuman-written summary sentences.
In407Radev, Hovy, and McKeown Summarization: IntroductionM.
Hearst, F. Gey, and R. Tong, editors,Proceedings of SIGIR?99: 22nd InternationalConference on Research and Development inInformation Retrieval, University ofCalifornia, Berkeley, August,pages 129?136.Jing, Hongyan, Kathleen McKeown, ReginaBarzilay, and Michael Elhadad.
1998.Summarization evaluation methods:Experiments and analysis.
In IntelligentText Summarization: Papers from the 1998AAAI Spring Symposium, Stanford, CA,23?25 March.
Technical Report SS-98-06.AAAI Press, pages 60?68.Knight, Kevin and Daniel Marcu.
2000.Statistics-based summarization?Step one:Sentence compression.
In Proceedings of the17th National Conference of the AmericanAssociation for Artificial Intelligence(AAAI-2000), pages 703?710.Kupiec, Julian, Jan O. Pedersen, andFrancine Chen.
1995.
A trainabledocument summarizer.
In Research andDevelopment in Information Retrieval,pages 68?73.Lin, C. and E. Hovy.
1997.
Identifying topicsby position.
In Fifth Conference on AppliedNatural Language Processing, Associationfor Computational Linguistics, 31March?3 April, pages 283?290.Lin, Chin-Yew.
1999.
Training a selectionfunction for extraction.
In Proceedings ofthe Eighteenth Annual International ACMConference on Information and KnowledgeManagement (CIKM), Kansas City, 6November.
ACM, pages 55?62.Lin, Chin-Yew.
2001.
Summary evaluationenvironment.http://www.isi.edu/cyl/SEE.Lin, Chin-Yew and Eduard Hovy.
2002a.From single to multi-documentsummarization: A prototype system andits evaluation.
In Proceedings of the 40thConference of the Association ofComputational Linguistics, Philadelphia,July, pages 457?464.Lin, Chin-Yew and Eduard Hovy.
2002b.Manual and automatic evaluation ofsummaries.
In Proceedings of the DocumentUnderstanding Conference (DUC-02)Workshop on Multi-Document SummarizationEvaluation at the ACL Conference,Philadelphia, July, pages 45?51.Luhn, H. P. 1958.
The automatic creation ofliterature abstracts.
IBM Journal of ResearchDevelopment, 2(2):159?165.Mani, Inderjeet.
2001.
AutomaticSummarization.
John Benjamins,Amsterdam/Philadelphia.Mani, Inderjeet and Eric Bloedorn.
1997.Multi-document summarization by graphsearch and matching.
In Proceedings of theFourteenth National Conference on ArtificialIntelligence (AAAI-97), Providence, RI.American Association for ArtificialIntelligence, pages 622?628.Mani, Inderjeet, Barbara Gates, and EricBloedorn.
1999.
Improving summaries byrevising them.
In Proceedings of the 37thAnnual Meeting of the Association forComputational Linguistics (ACL 99), CollegePark, MD, June, pages 558?565.Mani, Inderjeet, David House, G. Klein,Lynette Hirshman, Leo Obrst, The?re`seFirmin, Michael Chrzanowski, and BethSundheim.
1998.
The TIPSTER SUMMACtext summarization evaluation.
TechnicalReport MTR 98W0000138, The MitreCorporation, McLean, VA.Mani, Inderjeet and Mark Maybury, editors.1999.
Advances in Automatic TextSummarization.
MIT Press, Cambridge.Marcu, Daniel.
1997a.
From discoursestructures to text summaries.
InProceedings of the ACL?97/EACL?97 Workshopon Intelligent Scalable Text Summarization,Madrid, July 11, pages 82?88.Marcu, Daniel.
1997b.
The Rhetorical Parsing,Summarization, and Generation of NaturalLanguage Texts.
Ph.D. thesis, University ofToronto, Toronto.Marcu, Daniel.
1999.
The automaticconstruction of large-scale corpora forsummarization research.
In M. Hearst,F.
Gey, and R. Tong, editors, Proceedings ofSIGIR?99: 22nd International Conference onResearch and Development in InformationRetrieval, University of California,Berkeley, August, pages 137?144.Marcu, Daniel and Laurie Gerber.
2001.
Aninquiry into the nature of multidocumentabstracts, extracts, and their evaluation.
InProceedings of the NAACL-2001 Workshop onAutomatic Summarization, Pittsburgh, June.NAACL, pages 1?8.McKeown, Kathleen, Judith Klavans,Vasileios Hatzivassiloglou, ReginaBarzilay, and Eleazar Eskin.
1999.Towards multidocument summarizationby reformulation: Progress and prospects.In Proceedings of the 16th NationalConference of the American Association forArtificial Intelligence (AAAI-1999), 18?22July, pages 453?460.McKeown, Kathleen R. and Dragomir R.Radev.
1995.
Generating summaries ofmultiple news articles.
In Proceedings of the18th Annual International ACM SIGIRConference on Research and Development inInformation Retrieval, Seattle, July,pages 74?82.Ono, K., K. Sumita, and S. Miike.
1994.408Computational Linguistics Volume 28, Number 4Abstract generation based on rhetoricalstructure extraction.
In Proceedings of theInternational Conference on ComputationalLinguistics, Kyoto, Japan, pages 344?348.Otterbacher, Jahna, Dragomir R. Radev, andAirong Luo.
2002.
Revisions that improvecohesion in multi-document summaries:A preliminary study.
In ACL Workshop onText Summarization, Philadelphia.Papineni, K., S. Roukos, T. Ward, and W-J.Zhu.
2001.
BLEU: A method for automaticevaluation of machine translation.Research Report RC22176, IBM.Radev, Dragomir, Simone Teufel, HoracioSaggion, Wai Lam, John Blitzer, ArdaC?elebi, Hong Qi, Elliott Drabek, andDanyu Liu.
2002.
Evaluation of textsummarization in a cross-lingualinformation retrieval framework.Technical Report, Center for Languageand Speech Processing, Johns HopkinsUniversity, Baltimore, June.Radev, Dragomir R., Hongyan Jing, andMalgorzata Budzikowska.
2000.Centroid-based summarization ofmultiple documents: Sentence extraction,utility-based evaluation, and user studies.In ANLP/NAACL Workshop onSummarization, Seattle, April.Radev, Dragomir R. and Kathleen R.McKeown.
1998.
Generating naturallanguage summaries from multipleon-line sources.
Computational Linguistics,24(3):469?500.Rau, Lisa and Paul Jacobs.
1991.
Creatingsegmented databases from free text fortext retrieval.
In Proceedings of the 14thAnnual International ACM-SIGIR Conferenceon Research and Development in InformationRetrieval, New York, pages 337?346.Saggion, Horacio and Guy Lapalme.
2002.Generating indicative-informativesummaries with SumUM.
ComputationalLinguistics, 28(4), 497?526.Salton, G., A. Singhal, M. Mitra, andC.
Buckley.
1997.
Automatic textstructuring and summarization.Information Processing & Management,33(2):193?207.Silber, H. Gregory and Kathleen McCoy.2002.
Efficiently computed lexical chainsas an intermediate representation forautomatic text summarization.Computational Linguistics, 28(4), 487?496.Sparck Jones, Karen.
1999.
Automaticsummarizing: Factors and directions.
InI.
Mani and M. T. Maybury, editors,Advances in Automatic Text Summarization.MIT Press, Cambridge, pages 1?13.Strzalkowski, Tomek, Gees Stein, J. Wang,and Bowden Wise.
1999.
A robustpractical text summarizer.
In I. Mani andM.
T. Maybury, editors, Advances inAutomatic Text Summarization.
MIT Press,Cambridge, pages 137?154.Teufel, Simone and Marc Moens.
2002.Summarizing scientific articles:Experiments with relevance and rhetoricalstatus.
Computational Linguistics, 28(4),409?445.White, Michael and Claire Cardie.
2002.Selecting sentences for multidocumentsummaries using randomized localsearch.
In Proceedings of the Workshop onAutomatic Summarization (including DUC2002), Philadelphia, July.
Association forComputational Linguistics, NewBrunswick, NJ, pages 9?18.Witbrock, Michael and Vibhu Mittal.
1999.Ultra-summarization: A statisticalapproach to generating highly condensednon-extractive summaries.
In Proceedingsof the 22nd Annual International ACM SIGIRConference on Research and Development inInformation Retrieval, Berkeley,pages 315?316.Zechner, Klaus.
2002.
Automaticsummarization of open-domainmultiparty dialogues in diverse genres.Computational Linguistics, 28(4), 447?485.
