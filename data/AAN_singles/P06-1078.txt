Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 617?624,Sydney, July 2006. c?2006 Association for Computational LinguisticsIncorporating speech recognition confidence intodiscriminative named entity recognition of speech dataKatsuhito Sudoh Hajime Tsukada Hideki IsozakiNTT Communication Science LaboratoriesNippon Telegraph and Telephone Corporation2-4 Hikaridai, Seika-cho, Keihanna Science City, Kyoto 619-0237, Japan{sudoh,tsukada,isozaki}@cslab.kecl.ntt.co.jpAbstractThis paper proposes a named entity recog-nition (NER) method for speech recogni-tion results that uses confidence on auto-matic speech recognition (ASR) as a fea-ture.
The ASR confidence feature indi-cates whether each word has been cor-rectly recognized.
The NER model istrained using ASR results with named en-tity (NE) labels as well as the correspond-ing transcriptions with NE labels.
In ex-periments using support vector machines(SVMs) and speech data from Japanesenewspaper articles, the proposed methodoutperformed a simple application of text-based NER to ASR results in NER F-measure by improving precision.
Theseresults show that the proposed method iseffective in NER for noisy inputs.1 IntroductionAs network bandwidths and storage capacitiescontinue to grow, a large volume of speech dataincluding broadcast news and PodCasts is becom-ing available.
These data are important informa-tion sources as well as such text data as newspaperarticles and WWW pages.
Speech data as infor-mation sources are attracting a great deal of inter-est, such as DARPA?s global autonomous languageexploitation (GALE) program.
We also aim to usethem for information extraction (IE), question an-swering, and indexing.Named entity recognition (NER) is a key tech-nique for IE and other natural language process-ing tasks.
Named entities (NEs) are the proper ex-pressions for things such as peoples?
names, loca-tions?
names, and dates, and NER identifies thoseexpressions and their categories.
Unlike text data,speech data introduce automatic speech recogni-tion (ASR) error problems to NER.
Although im-provements to ASR are needed, developing a ro-bust NER for noisy word sequences is also impor-tant.
In this paper, we focus on the NER of ASRresults and discuss the suppression of ASR errorproblems in NER.Most previous studies of the NER of speechdata used generative models such as hiddenMarkov models (HMMs) (Miller et al, 1999;Palmer and Ostendorf, 2001; Horlock and King,2003b; Be?chet et al, 2004; Favre et al, 2005).On the other hand, in text-based NER, better re-sults are obtained using discriminative schemessuch as maximum entropy (ME) models (Borth-wick, 1999; Chieu and Ng, 2003), support vec-tor machines (SVMs) (Isozaki and Kazawa, 2002),and conditional random fields (CRFs) (McCal-lum and Li, 2003).
Zhai et al (2004) applied atext-level ME-based NER to ASR results.
Thesemodels have an advantage in utilizing various fea-tures, such as part-of-speech information, charac-ter types, and surrounding words, which may beoverlapped, while overlapping features are hard touse in HMM-based models.To deal with ASR error problems in NER,Palmer and Ostendorf (2001) proposed an HMM-based NER method that explicitly models ASR er-rors using ASR confidence and rejects erroneousword hypotheses in the ASR results.
Such rejec-tion is especially effective when ASR accuracy isrelatively low because many misrecognized wordsmay be extracted as NEs, which would decreaseNER precision.Motivated by these issues, we extended their ap-proach to discriminative models and propose anNER method that deals with ASR errors as fea-617tures.
We use NE-labeled ASR results for trainingto incorporate the features into the NER model aswell as the corresponding transcriptions with NElabels.
In testing, ASR errors are identified byASR confidence scores and are used for the NER.In experiments using SVM-based NER and speechdata from Japanese newspaper articles, the pro-posed method increased the NER F-measure, es-pecially in precision, compared to simply applyingtext-based NER to the ASR results.2 SVM-based NERNER is a kind of chunking problem that canbe solved by classifying words into NE classesthat consist of name categories and such chunk-ing states as PERSON-BEGIN (the beginning ofa person?s name) and LOCATION-MIDDLE (themiddle of a location?s name).
Many discrimi-native methods have been applied to NER, suchas decision trees (Sekine et al, 1998), ME mod-els (Borthwick, 1999; Chieu and Ng, 2003), andCRFs (McCallum and Li, 2003).
In this paper, weemploy an SVM-based NER method in the follow-ing way that showed good NER performance inJapanese (Isozaki and Kazawa, 2002).We define three features for each word: theword itself, its part-of-speech tag, and its charac-ter type.
We also use those features for the twopreceding and succeeding words for context de-pendence and use 15 features when classifying aword.
Each feature is represented by a binaryvalue (1 or 0), for example, ?whether the previousword is Japan,?
and each word is classified basedon a long binary vector where only 15 elementsare 1.We have two problems when solving NERusing SVMs.
One, SVMs can solve only atwo-class problem.
We reduce multi-class prob-lems of NER to a group of two-class problemsusing the one-against-all approach, where eachSVM is trained to distinguish members of aclass (e.g., PERSON-BEGIN) from non-members(PERSON-MIDDLE, MONEY-BEGIN, ... ).
In thisapproach, two or more classes may be assigned toa word or no class may be assigned to a word.
Toavoid these situations, we choose class c that hasthe largest SVM output score gc(x) among all oth-ers.The other is that the NE label sequence must beconsistent; for example, ARTIFACT-ENDmust follow ARTIFACT-BEGIN orSpeech dataNE-labeledtranscriptionsTranscriptions ASR resultsASR-basedtraining dataText-basedtraining dataManualtranscription ASRNE labelingSetting ASRconfidencefeature to 1Alignment&identifyingASR errorsand NEsFigure 1: Procedure for preparing training data.ARTIFACT-MIDDLE.
We use a Viterbi search toobtain the best and consistent NE label sequenceafter classifying all words in a sentence, basedon probability-like values obtained by applyingsigmoid function sn(x) = 1/(1 + exp(?
?nx)) toSVM output score gc(x).3 Proposed method3.1 Incorporating ASR confidence into NERIn the NER of ASR results, ASR errors cause NEsto be missed and erroneous NEs to be recognized.If one or more words constituting an NE are mis-recognized, we cannot recognize the correct NE.Even if all words constituting an NE are correctlyrecognized, we may not recognize the correct NEdue to ASR errors on context words.
To avoidthis problem, we model ASR errors using addi-tional features that indicate whether each word iscorrectly recognized.
Our NER model is trainedusing ASR results with a feature, where featurevalues are obtained through alignment to the cor-responding transcriptions.
In testing, we estimatefeature values using ASR confidence scores.
Inthis paper, this feature is called the ASR confidencefeature.Note that we only aim to identify NEs that arecorrectly recognized by ASR, and NEs containingASR errors are not regarded as NEs.
Utilizing er-roneous NEs is a more difficult problem that is be-yond the scope of this paper.3.2 Training NER modelFigure 1 illustrates the procedure for preparingtraining data from speech data.
First, the speech618data are manually transcribed and automaticallyrecognized by the ASR.
Second, we label NEsin the transcriptions and then set the ASR con-fidence feature values to 1 because the words inthe transcriptions are regarded as correctly recog-nized words.
Finally, we align the ASR results tothe transcriptions to identify ASR errors for theASR confidence feature values and to label cor-rectly recognized NEs in the ASR results.
Notethat we label the NEs in the ASR results that existin the same positions as the transcriptions.
If a partof an NE is misrecognized, the NE is ignored, andall words for the NE are labeled as non-NE words(OTHER).
Examples of text-based and ASR-basedtraining data are shown in Tables 1 and 2.
Sincethe name Murayama Tomiichi in Table 1 is mis-recognized in ASR, the correctly recognized wordMurayama is also labeled OTHER in Table 2.
An-other approach can be considered, where misrec-ognized words are replaced by word error symbolssuch as those shown in Table 3.
In this case, thosewords are rejected, and those part-of-speech andcharacter type features are not used in NER.3.3 ASR confidence scoring for using theproposed NER modelASR confidence scoring is an important techniquein many ASR applications, and many methodshave been proposed including using word poste-rior probabilities on word graphs (Wessel et al,2001), integrating several confidence measures us-ing neural networks (Schaaf and Kemp, 1997),using linear discriminant analysis (Kamppari andHazen, 2000), and using SVMs (Zhang and Rud-nicky, 2001).Word posterior probability is a commonly usedand effective ASR confidence measure.
Word pos-terior probability p([w; ?, t]|X) of word w at timeinterval [?, t] for speech signal X is calculated asfollows (Wessel et al, 2001):p([w; ?, t]|X)=?W?W [w;?,t]{p(X|W ) (p(W ))?
}?p(X) , (1)where W is a sentence hypothesis, W [w; ?, t] isthe set of sentence hypotheses that include w in[?, t], p(X|W ) is a acoustic model score, p(W )is a language model score, ?
is a scaling param-eter (?<1), and ?
is a language model weight.?
is used for scaling the large dynamic range ofWord Confidence NE labelMurayama 1 PERSON-BEGINTomiichi 1 PERSON-ENDshusho 1 OTHERwa 1 OTHERnento 1 DATE-SINGLETable 1: An example of text-based training data.Word Confidence NE labelMurayama 1 OTHERshi 0 OTHERni 0 OTHERichi 0 OTHERshiyo 0 OTHERwa 1 OTHERnento 1 DATE-SINGLETable 2: An example of ASR-based training data.Word Confidence NE labelMurayama 1 OTHER(error) 0 OTHER(error) 0 OTHER(error) 0 OTHER(error) 0 OTHERwa 1 OTHERnento 1 DATE-SINGLETable 3: An example of ASR-based training datawith word error symbols.p(X|W )(p(W ))?
to avoid a few of the top hy-potheses dominating posterior probabilities.
p(X)is approximated by the sum over all sentence hy-potheses and is denoted asp(X) =?W{p(X|W ) (p(W ))?}?
.
(2)p([w; ?, t]|X) can be efficiently calculated using aforward-backward algorithm.In this paper, we use SVMs for ASR confidencescoring to achieve a better performance than whenusing word posterior probabilities as ASR confi-dence scores.
SVMs are trained using ASR re-sults, whose errors are known through their align-ment to their reference transcriptions.
The follow-ing features are used for confidence scoring: theword itself, its part-of-speech tag, and its wordposterior probability; those of the two precedingand succeeding words are also used.
The worditself and its part-of-speech are also represented619by a set of binary values, the same as with anSVM-based NER.
Since all other features are bi-nary, we reduce real-valued word posterior prob-ability p to ten binary features for simplicity: (if0 < p ?
0.1, if 0.1 < p ?
0.2, ... , and if0.9 < p ?
1.0).
To normalize SVMs?
outputscores for ASR confidence, we use a sigmoid func-tion sw(x) = 1/(1 + exp(??wx)).
We use thesenormalized scores as ASR confidence scores.
Al-though a large variety of features have been pro-posed in previous studies, we use only these sim-ple features and reserve the other features for fur-ther studies.Using the ASR confidence scores, we estimatewhether each word is correctly recognized.
If theASR confidence score of a word is greater thanthreshold tw, the word is estimated as correct, andwe set the ASR confidence feature value to 1; oth-erwise we set it to 0.3.4 Rejection at the NER levelWe use the ASR confidence feature to suppressASR error problems; however, even text-basedNERs sometimes make errors.
NER performanceis a trade-off between missing correct NEs andaccepting erroneous NEs, and requirements dif-fer by task.
Although we can tune the parame-ters in training SVMs to control the trade-off, itseems very hard to find appropriate values for allthe SVMs.
We use a simple NER-level rejectionby modifying the SVM output scores for the non-NE class (OTHER).
We add constant offset value toto each SVM output score for OTHER.
With a largeto, OTHER becomes more desirable than the otherNE classes, and many words are classified as non-NE words and vice versa.
Therefore, to works as aparameter for NER-level rejection.
This approachcan also be applied to text-based NER.4 ExperimentsWe conducted the following experiments relatedto the NER of speech data to investigate the per-formance of the proposed method.4.1 SetupIn the experiment, we simulated the procedureshown in Figure 1 using speech data from theNE-labeled text corpus.
We used the trainingdata of the Information Retrieval and ExtractionExercise (IREX) workshop (Sekine and Eriguchi,2000) as the text corpus, which consisted of 1,174Japanese newspaper articles (10,718 sentences)and 18,200 NEs in eight categories (artifact, or-ganization, location, person, date, time, money,and percent).
The sentences were read by 106speakers (about 100 sentences per speaker), andthe recorded speech data were used for the exper-iments.
The experiments were conducted with 5-fold cross validation, using 80% of the 1,174 ar-ticles and the ASR results of the correspondingspeech data for training SVMs (both for ASR con-fidence scoring and for NER) and the rest for thetest.We tokenized the sentences into words andtagged the part-of-speech information using theJapanese morphological analyzer ChaSen 1 2.3.3and then labeled the NEs.
Unreadable to-kens such as parentheses were removed in to-kenization.
After tokenization, the text cor-pus had 264,388 words of 60 part-of-speechtypes.
Since three different kinds of charac-ters are used in Japanese, the character typesused as features included: single-kanji(words written in a single Chinese charac-ter), all-kanji (longer words written in Chi-nese characters), hiragana (words writtenin hiragana Japanese phonograms), katakana(words written in katakana Japanese phono-grams), number, single-capital (wordswith a single capitalized letter), all-capital,capitalized (only the first letter is capital-ized), roman (other roman character words), andothers (all other words).
We used all the fea-tures that appeared in each training set (no featureselection was performed).
The chunking states in-cluded in the NE classes were: BEGIN (beginningof a NE), MIDDLE (middle of a NE), END (endingof a NE), and SINGLE (a single-word NE).
Therewere 33 NE classes (eight categories * four chunk-ing states + OTHER), and therefore we trained 33SVMs to distinguish words of a class from wordsof other classes.
For NER, we used an SVM-basedchunk annotator YamCha 2 0.33 with a quadratickernel (1 + ~x ?
~y)2 and a soft margin parameterof SVMs C=0.1 for training and applied sigmoidfunction sn(x) with ?n=1.0 and Viterbi search tothe SVMs?
outputs.
These parameters were exper-imentally chosen using the test set.We used an ASR engine (Hori et al, 2004) witha speaker-independent acoustic model.
The lan-1http://chasen.naist.jp/hiki/ChaSen/ (in Japanese)2http://www.chasen.org/?taku/software/yamcha/620guage model was a word 3-gram model, trainedusing other Japanese newspaper articles (about340 M words) that were also tokenized usingChaSen.
The vocabulary size of the word 3-grammodel was 426,023.
The test-set perplexity overthe text corpus was 76.928.
The number of out-of-vocabulary words was 1,551 (0.587%).
223(1.23%) NEs in the text corpus contained such out-of-vocabulary words, so those NEs could not becorrectly recognized by ASR.
The scaling param-eter ?
was set to 0.01, which showed the best ASRerror estimation results using word posterior prob-abilities in the test set in terms of receiver operatorcharacteristic (ROC) curves.
The language modelweight ?
was set to 15, which is a commonly usedvalue in our ASR system.
The word accuracy ob-tained using our ASR engine for the overall datasetwas 79.45%.
In the ASR results, 82.00% of theNEs in the text corpus remained.
Figure 2 showsthe ROC curves of ASR error estimation for theoverall five cross-validation test sets, using SVM-based ASR confidence scoring and word posteriorprobabilities as ASR confidence scores, whereTrue positive rate= # correctly recognized words estimated as correct# correctly recognized wordsFalse positive rate= # misrecognized words estimated as correct# misrecognized words .In SVM-based ASR confidence scoring, we usedthe quadratic kernel and C=0.01.
Parameter ?w ofsigmoid function sw(x) was set to 1.0.
These pa-rameters were also experimentally chosen.
SVM-based ASR confidence scoring showed better per-formance in ASR error estimation than simpleword posterior probabilities by integrating mul-tiple features.
Five values of ASR confidencethreshold tw were tested in the following experi-ments: 0.2, 0.3, 0.4, 0.5, and 0.6 (shown by blackdots in Figure 2).4.2 Evaluation metricsEvaluation was based on an averaged NER F-measure, which is the harmonic mean of NER pre-cision and recall:NER precision = # correctly recognized NEs# recognized NEsNER recall = # correctly recognized NEs# NEs in original text.0204060801000  20  40  60  80  100Truepositverate(%)False positive rate (%)=0.3=0.4SVM-basedconfidencescoringWord posterior probabilitytwtttw=0.2tw=0.6w=0.5wFigure 2: SVM-based confidence scoring outper-forms word posterior probability for ASR error es-timation.A recognized NE was accepted as correct if andonly if it appeared in the same position as its refer-ence NE through alignment, in addition to havingthe correct NE surface and category, because thesame NEs might appear more than once.
Compar-isons of NE surfaces did not include differencesin word segmentation because of the segmentationambiguity in Japanese.
Note that NER recall withASR results could not exceed the rate of the re-maining NEs after ASR (about 82%) because NEscontaining ASR errors were always lost.In addition, we also evaluated the NER perfor-mance in NER precision and recall with NER-level rejection using the procedure in Section 3.4,by modifying the non-NE class scores using offsetvalue to.4.3 Compared methodsWe compared several combinations of featuresand training conditions for evaluating the effect ofincorporating the ASR confidence feature and in-vestigating differences among training data: text-based, ASR-based, and both.Baseline does not use the ASR confidence fea-ture and is trained using text-based training dataonly.NoConf-A does not use the ASR confidencefeature and is trained using ASR-based trainingdata only.621Method Confidence Training Test F-measure (%) Precision (%) Recall (%)Baseline Text ASR 67.00 70.67 63.70NoConf-A Not used ASR ASR 65.52 78.86 56.05NoConf-TA Text+ASR ASR 66.95 77.55 58.91Conf-A ASR ASR?
67.69 76.69 60.59Proposed Used Text+ASR ASR?
69.02 78.13 61.81Conf-Reject Used?
Text+ASR ASR?
68.77 77.57 61.78Conf-UB Used Text+ASR ASR??
73.14 87.51 62.83Transcription Not used Text Text 84.04 86.27 81.93Table 4: NER results in averaged NER F-measure, precision, and recall without considering NER-levelrejection (to = 0).
ASR word accuracy was 79.45%, and 82.00% of NEs remained in ASR results.
(?Unconfident words were rejected and replaced by word error symbols, ?tw = 0.4, ?
?ASR errors wereknown.
)NoConf-TA does not use the ASR confidencefeature and is trained using both text-based andASR-based training data.Conf-A uses the ASR confidence feature and istrained using ASR-based training data only.Proposed uses the ASR confidence feature andis trained using both text-based and ASR-basedtraining data.Conf-Reject is almost the same as Proposed,but misrecognized words are rejected and replacedwith word error symbols, as described at the endof Section 3.2.The following two methods are for reference.Conf-UB assumes perfect ASR confidence scor-ing, so the ASR errors in the test set are known.The NER model, which is identical to Proposed,is regarded as the upper-boundary of Proposed.Transcription applies the same model as Base-line to reference transcriptions, assuming word ac-curacy is 100%.4.4 NER ResultsIn the NER experiments, Proposed achieved thebest results among the above methods.
Table4 shows the NER results obtained by the meth-ods without considering NER-level rejection (i.e.,to = 0), using threshold tw = 0.4 for Conf-A,Proposed, and Conf-Reject, which resulted in thebest NER F-measures (see Table 5).
Proposedshowed the best F-measure, 69.02%.
It outper-formed Baseline by 2.0%, with a 7.5% improve-ment in precision, instead of a recall decrease of1.9%.
Conf-Reject showed slightly worse resultsMethod tw F (%) P (%) R (%)0.2 66.72 71.28 62.710.3 67.32 73.68 61.98Conf-A 0.4 67.69 76.69 60.590.5 67.04 79.64 57.890.6 64.48 81.90 53.140.2 68.08 72.54 64.140.3 68.70 75.11 63.31Proposed 0.4 69.02 78.13 61.810.5 68.17 80.88 58.930.6 65.39 83.00 53.960.2 68.06 72.49 64.140.3 68.61 74.88 63.31Conf-Reject 0.4 68.77 77.57 61.780.5 67.93 80.23 58.910.6 64.93 82.05 53.73Table 5: NER results with varying ASR confi-dence score threshold tw for Conf-A, Proposed,and Conf-Reject.
(F: F-measure, P: precision, R:recall)than Proposed.
Conf-A resulted in 1.3% worse F-measure than Proposed.
NoConf-A and NoConf-TA achieved 7-8% higher precision than Base-line; however, their F-measure results were worsethan Baseline because of the large drop of recall.The upper-bound results of the proposed method(Conf-UB) in F-measure was 73.14%, which was4% higher than Proposed.Figure 3 shows NER precision and recall withNER-level rejection by to for Baseline, NoConf-TA, Proposed, Conf-UB, and Transcription.
In thefigure, black dots represent results with to = 0,as shown in Table 4.
By all five methods, we62202040608010050  60  70  80  90  100Recall (%)Precision (%)BaselineNoConf-TAProposedConf-UBTranscriptionFigure 3: NER precision and recall with NER-level rejection by toobtained higher precision with to > 0.
Pro-posed achieved more than 5% higher precisionthan Baseline on most recall ranges and showedhigher precision than NoConf-TA on recall rangeshigher than about 35%.5 DiscussionThe proposed method effectively improves NERperformance, as shown by the difference betweenProposed and Baseline in Tables 4 and 5.
Improve-ment comes from two factors: using both text-based and ASR-based training data and incorpo-rating ASR confidence feature.
As shown by thedifference between Baseline and the methods us-ing ASR-based training data (NoConf-A, NoConf-TA, Conf-A, Proposed, Conf-Reject), ASR-basedtraining data increases precision and decreasesrecall.
In ASR-based training data, all wordsconstituting NEs that contain ASR errors are re-garded as non-NE words, and those NE exam-ples are lost in training, which emphasizes NERprecision.
When text-based training data are alsoavailable, they compensate for the loss of NEexamples and recover NER recall, as shown bythe difference between the methods without text-based training data (NoConf-A, Conf-A) and thosewith (NoConf-TA, Proposed).
The ASR confi-dence feature also increases NER recall, as shownby the difference between the methods withoutit (NoConf-A, NoConf-TA) and with it (Conf-A,Proposed).
This suggests that the ASR confidencefeature helps distinguish whether ASR error influ-ences NER and suppresses excessive rejection ofNEs around ASR errors.With respect to the ASR confidence feature, thesmall difference between Conf-Reject and Pro-posed suggests that ASR confidence is a moredominant feature in misrecognized words than theother features: the word itself, its part-of-speechtag, and its character type.
In addition, the dif-ference between Conf-UB and Proposed indicatedthat there is room to improve NER performancewith better ASR confidence scoring.NER-level rejection also increased precision, asshown in Figure 3.
We can control the trade-off between precision and recall with to accord-ing to the task requirements, even in text-basedNER.
In the NER of speech data, we can ob-tain much higher precision using both ASR-basedtraining data and NER-level rejection than usingeither one.6 Related workRecent studies on the NER of speech data considermore than 1-best ASR results in the form of N-bestlists and word lattices.
Using many ASR hypothe-ses helps recover the ASR errors of NE words in1-best ASR results and improves NER accuracy.Our method can be extended to multiple ASR hy-potheses.Generative NER models were used for multi-pass ASR and NER searches using word lattices(Horlock and King, 2003b; Be?chet et al, 2004;Favre et al, 2005).
Horlock and King (2003a)also proposed discriminative training of their NERmodels.
These studies showed the advantage ofusing multiple ASR hypotheses, but they do notuse overlapping features.Discriminative NER models were also appliedto multiple ASR hypotheses.
Zhai et al (2004) ap-plied text-based NER to N-best ASR results, andmerged the N-best NER results by weighted vot-ing based on several sentence-level results such asASR and NER scores.
Using the ASR confidencefeature does not depend on SVMs and can be usedwith their method and other discriminative mod-els.7 ConclusionWe proposed a method for NER of speech datathat incorporates ASR confidence as a featureof discriminative NER, where the NER model623is trained using both text-based and ASR-basedtraining data.
In experiments using SVMs,the proposed method showed a higher NER F-measure, especially in terms of improving pre-cision, than simply applying text-based NER toASR results.
The method effectively rejected erro-neous NEs due to ASR errors with a small drop ofrecall, thanks to both the ASR confidence featureand ASR-based training data.
NER-level rejectionalso effectively increased precision.Our approach can also be used in other tasksin spoken language processing, and we expect itto be effective.
Since confidence itself is not lim-ited to speech, our approach can also be applied toother noisy inputs, such as optical character recog-nition (OCR).
For further improvement, we willconsider N-best ASR results or word lattices as in-puts and introduce more speech-specific featuressuch as word durations and prosodic features.Acknowledgments We would like to thankanonymous reviewers for their helpful comments.ReferencesFre?de?ric Be?chet, Allen L. Gorin, Jeremy H. Wright,and Dilek Hakkani-Tu?r.
2004.
Detecting and ex-tracting named entities from spontaneous speech in amixed-initiative spoken dialogue context: How MayI Help You?
Speech Communication, 42(2):207?225.Andrew Borthwick.
1999.
A Maximum Entropy Ap-proach to Named Entity Recognition.
Ph.D. thesis,New York University.Hai Leong Chieu and Hwee Tou Ng.
2003.
Named en-tity recognition with a maximum entropy approach.In Proc.
CoNLL, pages 160?163.Beno?
?t Favre, Fre?de?ric Be?chet, and Pascal Noce?ra.2005.
Robust named entity extraction from largespoken archives.
In Proc.
HLT-EMNLP, pages 491?498.Takaaki Hori, Chiori Hori, and Yasuhiro Minami.2004.
Fast on-the-fly composition for weightedfinite-state transducers in 1.8 million-word vocab-ulary continuous-speech recognition.
In Proc.
IC-SLP, volume 1, pages 289?292.James Horlock and Simon King.
2003a.
Discrimi-native methods for improving named entity extrac-tion on speech data.
In Proc.
EUROSPEECH, pages2765?2768.James Horlock and Simon King.
2003b.
Named en-tity extraction from word lattices.
In Proc.
EU-ROSPEECH, pages 1265?1268.Hideki Isozaki and Hideto Kazawa.
2002.
Efficientsupport vector classifiers for named entity recogni-tion.
In Proc.
COLING, pages 390?396.Simo O. Kamppari and Timothy J. Hazen.
2000.
Wordand phone level acoustic confidence scoring.
InProc.
ICASSP, volume 3, pages 1799?1802.Andrew McCallum and Wei Li.
2003.
Early results fornamed entity recognition with conditional randomfields, feature induction and web-enhanced lexicons.In Proc.
CoNLL, pages 188?191.David Miller, Richard Schwartz, Ralph Weischedel,and Rebecca Stone.
1999.
Named entity extractionfrom broadcast news.
In Proceedings of the DARPABroadcast News Workshop, pages 37?40.David D. Palmer and Mari Ostendorf.
2001.
Im-proving information extraction by modeling errorsin speech recognizer output.
In Proc.
HLT, pages156?160.Thomas Schaaf and Thomas Kemp.
1997.
Confidencemeasures for spontaneous speech recognition.
InProc.
ICASSP, volume II, pages 875?878.Satoshi Sekine and Yoshio Eriguchi.
2000.
Japanesenamed entity extraction evaluation - analysis of re-sults.
In Proc.
COLING, pages 25?30.Satoshi Sekine, Ralph Grishman, and Hiroyuki Shin-nou.
1998.
A decision tree method for finding andclassifying names in Japanese texts.
In Proc.
theSixth Workshop on Very Large Corpora, pages 171?178.Frank Wessel, Ralf Schlu?ter, Klaus Macherey, andHermann Ney.
2001.
Confidence measures forlarge vocabulary continuous speech recognition.IEEE Transactions on Speech and Audio Process-ing, 9(3):288?298.Lufeng Zhai, Pascale Fung, Richard Schwartz, MarineCarpuat, and Dekai Wu.
2004.
Using N-best listsfor named entity recognition from chinese speech.In Proc.
HLT-NAACL, pages 37?40.Rong Zhang and Alexander I. Rudnicky.
2001.
Wordlevel confidence annotation using combinations offeatures.
In Proc.
EUROSPEECH, pages 2105?2108.624
