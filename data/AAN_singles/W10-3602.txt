Proceedings of the 1st Workshop on South and Southeast Asian Natural Language Processing (WSSANLP), pages 8?16,the 23rd International Conference on Computational Linguistics (COLING), Beijing, August 2010Thai Sentence-Breaking for Large-Scale SMTGlenn Slaydenthai-language.comglenn@thai-language.comMei-Yuh HwangMicrosoft Researchmehwang@microsoft.comLee SchwartzMicrosoft Researchleesc@microsoft.comAbstractThai language text presents challengesfor integration into large-scale multi-language statistical machine translation(SMT) systems, largely stemming fromthe nominal lack of punctuation and in-ter-word space.
For Thai sentence break-ing, we describe a monolingual maxi-mum entropy classifier with features thatmay be applicable to other languagessuch as Arabic, Khmer and Lao.
We ap-ply this sentence breaker to our large-vocabulary, general-purpose, bidirec-tional Thai-English SMT system, andachieve BLEU scores of around 0.20,reaching our threshold of releasing it as afree online service.1 IntroductionNLP research has consolidated around the notionof the sentence as the fundamental unit of trans-lation, a consensus which has fostered the devel-opment powerful statistical and analytical ap-proaches which incorporate an assumption ofdeterministic sentence delineation.
As such sys-tems become more sophisticated, languages forwhich this assumption is challenged receive in-creased attention.
Thai is one such language,since it uses space neither to distinguish syl-lables from words or affixes, nor to unambi-guously signal sentence boundaries.Written Thai has no sentence-end punctuation,but a space character is always present betweensentences.
There is generally no space betweenwords, but a space character may appear within asentence according to linguistic or prescriptiveorthographic motivation (Wathabunditkul 2003),and these characteristics disqualify sentence-breaking (SB) methods used for other languages,such as Palmer and Hearst (1997).
Thai SB hastherefore been regarded as the task of classifyingeach space that appears in a Thai source text aseither sentence-breaking (sb) or non-sentence-breaking (nsb).Several researchers have investigated ThaiSB.
Along with a discussion of Thai word break-ing (WB), Aroonmanakun (2007) examines theissue.
With a human study, he establishes thatsentence breaks elicited from Thai informantsexhibit varying degrees of consensus.
Mittra-piyanuruk and Sornlertlamvanich (2000) definepart-of-speech (POS) tags for sb and nsb andtrain a trigram model over a POS-annotated cor-pus.
At runtime, they use the Viterbi algorithmto select the POS sequence with the highestprobability, from which the corresponding spacetype is read back.
Charoenpornsawat and Sornler-tlamvanich (2001) apply Winnow, a multiplica-tive trigger threshold classifier, to the problem.Their model has ten features: the number ofwords to the left and right, and the left-two andright-two POS tags and words.We present a monolingual Thai SB based on amaximum entropy (ME) classifier (Ratnaparkhi1996; Reynar and Ratnaparkhi, 1997) which issuitable for sentence-breaking SMT training dataand runtime inputs.
Our model uses a four tokenwindow of Thai lemmas, plus categorical fea-tures, to describe the proximal environment ofthe space token under consideration, allowingruntime classification of space tokens with pos-sibly unseen contexts.As our SB model relies on Thai WB, we re-view our approach to this problem, plus relatedpreprocessing, in the next section.
Section 2 alsodiscusses the complementary operation to WB,namely, the re-spacing of Thai text generated bySMT output.
Section 3 details our SB model andevaluates its performance.
We describe the inte-gration of this work with our large-scale SMTsystem in Section 4.
We draw conclusions inSection 5.82 Pre- and Post-processingAs will be shown in Section 3, our sentencebreaker relies on Thai WB.
In turn, with the aimof minimizing WB errors, we perform Unicodecharacter sequence normalization prior to WB.As output byproducts, our WB analysis readilyidentifies certain types of named entities whichwe propagate into our THA-ENG SMT; in thissection, we briefly summarize these preliminaryprocessing steps, and we conclude the sectionwith a discussion of Thai text re-spacing.2.1 Character Sequence NormalizationThai orthography uses an alphabet of 44 conso-nants and a number of vowel glyphs and tonemarks.
The four Thai tone marks and some Thaivowel characters are super- and/or sub-scriptedwith respect to a base character.
For example,the ??
?
sequence consists of three code points:?
?
?
?
?.
When two or more of these combiningmarks are present on the same base character, theordering of these code points in memory shouldbe consistent so that orthographically identicalentities are recognized as equivalent by comput-er systems.
However, some computer word pro-cessors do not enforce the correct sequence or donot properly indicate incorrect sequences to theuser visually.
This often results in documentswith invalid byte sequences.Correcting these errors is desirable for SMTinputs.
In order to normalize Thai input charactersequences to a canonical Unicode form, we de-veloped a finite state transducer (FST) whichdetects and repairs a number of sequencing er-rors which render Thai text either orthographi-cally invalid, or not in a correct Unicode se-quence.For example, a superscripted Thai tone markshould follow a super- or sub-scripted Thai vo-wel when they both apply to the same consonant.When the input has the tone mark and the vowelglyph swapped, the input can be fully repaired:?
?
?
?
?
?
?
?
?
?
?
?
?????
?
?
?
?
?
?
?
?
?
?
?
?
?
???
?Figure 1.
Two unambiguous repairsOther cases are ambiguous.
The occurrence ofmultiple adjacent vowel glyphs is an error wherethe intention may not be clear.
We retain thefirst-appearing glyph, unless it is a pre-posedvowel, in which case we retain the last-appearinginstance.
These two treatments are contrasted inFigure 2.
Miscoding (Figure 3) is another varietyof input error that is readily repaired.????
?
??????
?
?
?Figure 2.
Two ambiguous repairsWithin the Infoquest Thai newswire corpus, alow-noise corpus, about 0.05% of the lines exhi-bit at least one of the problems mentioned here.For some chunks of broad-range web scrapeddata, we observe rates as high as 4.1%.
Thismeasure is expected to under-represent the utilityof the filter to WB, since Thai text streams, lack-ing intra-word spacing and permitting two un-written vowels, have few re-alignment check-points, allowing tokenization state machines tolinger in misaligned states.?
?
?
?
?
?
???
?
????
?
?
?
?
?
?
?
?Figure 3.
Two common mis-codings2.2 Uniscribe Thai TokenizationThai text does not normally use the space cha-racter to separate words, except in certain specif-ic contexts.
Although Unicode offers the Zero-Width Space (ZWSP) as one solution for indicat-ing word breaks in Thai, it is infrequently used.Programmatic tokenization has become a stapleof Thai computational linguistics.
The problemhas been well studied, with precision and recallnear 95% (Haruechaiyasak et al 2008).In our SMT application, both the sentencebreaker and the SMT system itself require ThaiWB, and we use the same word breaker for thesetasks (although the system design currently pro-hibits directly passing tokens between these twocomponents).
Our method is to apply post-processing heuristics to the output of Uniscribe(Bishop et al 2003), which is provided as part ofthe Microsoft?
WindowsTM operating systeminterface.
Our heuristics fall into two categories:?re-gluing?
words that Uniscribe broke too ag-gressively, and a smaller class of cases of furtherbreaking of words that Uniscribe did not break.Re-gluing is achieved by comparing Uniscribeoutput against a Thai lexicon in which desiredbreaks within a word are tagged.
Underbreakingby Uniscribe is less common and is restricted toa number of common patterns which are repairedexplicitly.92.3 Person Name EntitiesIn written Thai, certain types of entities employprescriptive whitespace patterns.
By removingthese recognized patterns from consideration, SBprecision can be improved.
Furthermore, be-cause our re-gluing procedure requires a lookupof every syllable proposed by Uniscribe, it isefficient to consider, during WB, additionalprocessing that can be informed by the samelookup.
Accordingly, we briefly mention someof the entity types that our WB identifies, focus-ing on those that incorporate distinctive spacingpatterns.Person names in Thai adhere to a conventionfor the use of space characters.
This helps Thaireaders to identify the boundaries of multi-syllable surnames that they may not have seenbefore.
The following grammar summarizes theprescriptive conventions for names appearing inThai text:<name-entity> ::= <honorific>  <full-name><full-name> ::= <first-name> [<last-name>]<first-name> ::= <name-text> space<last-name> ::= <name-text> space<name-text> ::= <thai-alphabetic-char>+<thai-alphabetic-char> ::= ?
| ?
| ?
| ?
| ...Figure 4.
Name entity recognition grammarThe re-glue lookup also determines if a sylla-ble matches one of the following predefined spe-cial categories: name-introducing honorific (h),Thai or foreign given name (g), token which islikely to form part of a surname (s), or tokenwhich aborts the gathering of a name (i.e.
is un-likely to form part of a name)..../???/???/?/???/??
?/ /?/????/??/?
?/ /???/...???
???
?
?
??
???
?
?
???
??
?
?
??
?h g0 g1 g2 sp0 s0 s1 s2 s3 sp1thatMr.<oov>hitbeloved<oov>stable<oov><oov>sai d...that Mr. Chiranut Winichotkun said...Figure 5.
Thai person-name entity recognitionFigure 5 shows a Thai name appearing withina text fragment, with Uniscribe detected tokenboundaries indicated by slashes.
In the third rowwe have identified the special category, if any,for each token.
The fourth line shows the Eng-lish translation gloss, or <oov> if none.
The bot-tom row is the desired translation output.Our name identifier first notes the presence ofan honorific {h} ???
followed by a pattern oftokens {g0-gn}, {s0-sn} and spaces {sp0, sp1}that is compatible with a person name and sur-name of sensible length.Next, we determine which of those tokens inthe ranges {g} and {s} following the honorificdo not have a gloss translation (i.e., are notfound in the lexicon).
These tokens are indicatedby <oov> in the gloss above.
When the numberof unknown tokens exceeds a threshold, we hy-pothesize that these tokens form a name.
Thelack of lexical morphology in Thai facilitatesthis method because token (or syllable) lookupgenerally equates with the lookup of a stemmedlemma.2.4 Calendar Date EntitiesOur WB also identifies Thai calendar dates, asthese also exhibit a pattern which incorporatesspaces.
As a prerequisite to identifying dates, wemap Thai orthographic digits {?
?
?
?
?
?
??
?
?}
to Arabic digits 0 through 9, respec-tively.
For example, our system would interpretthe input text ????
as equivalent to ?2540.?.../??/???/??/?
/14/ ?????
?/ /???
?/ /???/...??
???
???
sp 14 ??????
sp ????
sp ??
?on day which  14 March  2540  and...on March 14th, 1997 and...Figure 6.
Date entity recognitionFigure 6 shows a fragment of Thai text whichcontains a calendar date for which our systemwill emit a single token.
As shown in the exam-ple, our system detects and adjusts for the use ofThai Buddhist year dates when necessary.
Ga-thering of disparate and optional parts of theThai date is summarized by the grammar in Fig-ure 7.<date-entity> ::= [<cardinal-words>] [space] <date><cardinal-words> ::= ?????
?| ??
?<date> ::= month-date [space] year<year> ::= <tha-digit> <tha-digit> <tha-digit> <tha-digit><year> ::= <ara-digit> <ara-digit> <ara-digit> <ara-digit><month-date> ::= <day> [space] <month><day> ::= <thai-digit>+<day> ::= <ara-digit>+<month> ::= <month-full> | <month-abbr><month-full> ::= ??????
| ?????????
?| ??????
| ...<month-abbr> ::= ?.?.
| ?.?.
| ?.??.
| ...<tha-digit> ::= ?
| ?
| ?
| ?
| ?
| ?
| ?
| ?
| ?
| ?<ara-digit> ::= 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9Figure 7.
Date recognition grammar102.5 Thai Text Re-spacingTo conclude this section, we mention an opera-tion complementary to Thai WB, whereby Thaiwords output by an SMT system must be re-spaced in accordance with Thai prescriptiveconvention.
As will be mentioned in Section 4.2,for each input sentence, our English-Thai systemhas access to an English dependency parse tree,as well as links between this tree and a Thaitransfer dependency tree.
After using these linksto transfer syntactic information to the Thai tree,we are able to apply prescriptive spacing rules(Wathabunditkul 2003) as closely as possible.Human evaluation showed satisfactory resultsfor this process.3 Maximum Entropy Sentence-BreakingWe now turn to a description of our statisticalsentence-breaking model.
We train an ME clas-sifier on features which describe the proximalenvironment of the space token under considera-tion and use this model at runtime to classifyspace tokens with possibly unseen contexts.3.1 ModelingUnder the ME framework, let B={sb, nsb}represent the set of possible classes we are inter-ested in predicting for each space token in theinput stream.
Let C={linguistic contexts}represent the set of possible contexts that we canobserve, which must be encoded by binary fea-tures, ??
(?, ?
), 1 ?
?
?
?, such as:??
(?, ?)
= ?
1 if the previous word is English ???
?
= ??
?.0 otherwise.This feature helps us learn that the space after anEnglish word is usually not a sentence boundary.??
(?, ?)
= ?1 if the distance to the previous honorificis less than 15 tokens ???
?
= ??
?0 otherwise.This feature enables us to learn that spaceswhich follow an honorific are less likely to marksentence boundaries.
Assume the joint probabili-ty p(b,c) is modeled by?
(?, ?)
= ??
????(?,?)???
?where we have k free parameters {??}
to esti-mate and Z is a normalization factor to make?
?
(?, ?)
= 1.?,?
The ME learning algorithmfinds a solution {??}
representing the most un-certain commitmentmax  ?(?)
= ???
(?, ?)
log ?
(?, ?
)that satisfies the observed distribution ???
(?, ?)
ofthe training data??
(?, ?)??
(?, ?)
= ?
???
(?, ?)??
(?, ?
), 1 ?
?
?
?
.This is solved via the Generalized Iterative Scal-ing algorithm (Darroch and Ratcliff 1972).
Atrun-time, a space token is considered an sb, ifand only if p(sb|c) > 0.5, where?(??|?)
= ?(?
?, ?)?(?
?, ?)
+ ?(??
?, ?)
.3.2 Feature SelectionThe core context of our model, {w, x, y, z}, is awindow spanning two tokens to the left (posi-tions w and x) and two tokens to the right (posi-tions y and z) of a classification candidate spacetoken.c token characteristicyk Yamok (syllable reduplication) symbol ?sp space??
Thai numeric digitsnum Arabic numeric digitsABC Sequence of all capital ASCII characterscnn single character (derived from hex)ckkmmnn single character (derived from UTF8 hex)ascii any amount of non-Thai text(Thai text) Thai word (derived from lemma)Table 1.
Categorical and derived feature namesThe possible values of each of the windowpositions {w, x, y, z} are shown in Table 1,where the first match to the token at the desig-nated position is assigned as the feature value forthat position.
Foreign-text tokens plus any inter-vening space are merged, so a single ?ascii?
fea-ture may represent an arbitrary amount of non-Thai script with interior space.Figure 8 shows an example sentence that hasbeen tokenized.
Token boundaries are indicatedby slashes.
Although there are three space tokensin the original input, we extract four contexts.The shaded boxes in the source text?and theshaded line in the figure?indicate the single sbcontext that is synthesized by wrapping, to bedescribed in Section 3.4.For each context, in addition to the {w, x, y, z}features, we extract two more features indicatedby {l ,r} in Figure 8.
They are the number of11tokens between the previous space token (wrap-ping as necessary) and the current one, and thenumber of tokens between the current space to-ken and the next space token (wrapping as ne-cessary).
These features do not distinguishwhether the bounding space token is sb or nsb.This is because, processing left-to-right, it ispermissible to use a feature such as ?number oftokens since last sb,?
but not ?number of tokensuntil next sb,?
which would be available duringtraining but not at runtime.???????????????????
R1C1  ????????????????????????????????
A1?R1C1 reference style was converted to A1 reference style.?__/??????/???/????/???/??
?/  /R1C1/   /???/????/??/????/??????/???/????/???/??
?/  /A1/__b c=w c=x c=y c=z c=l c=rnsb ???
???
ABC sp 5 1nsb sp ABC ???
????
1 9nsb ???
???
ABC sp 9 1sb sp ABC ??????
???
1 5Figure 8.
A Thai sentence and the training contexts extracted.
Hig-hlighting shows the context for sb.In addition to the above core features, ourmodel emits certain extra features only if theyappear:?
An individual feature for each English punc-tuation mark, since these are sometimes usedin Thai.
For example, there is one feature forthe sentence end period (i.e.
full-stop);?
The current nest depth for paired glyphs withdirectional variation, such as brackets, braces,and parentheses;?
The current parity value for paired glyphswithout directional distinction such as?straight?
quotation marks.The following example illustrates paired direc-tional glyphs (in this case, parentheses):.../??????????/?
/(/??????/__/??
?/)/  /?????
?/  /???????/??
?/......Unilever (Thailand) Ltd. disclosed that...b c=w c=x c=y c=z c=pnnsb ( ??????
??? )
1Figure 9.
Text fragment illustrating paired directional glyphs andthe context for the highlighted spaceIn Figure 9, the space between ???????country?
and ???
?Thai,?
generates an nsbcontext which includes the features shown,where ?pn?
is an extra feature which indicatesthe parenthesis nesting level.
This feature helpsthe model learn that spaces which occur withinparentheses are likely to be nsb.Parity features for the non-directional pairedglyphs, which do nest, are true binary features.Since these features have only two possible val-ues (inside or outside), they are only emittedwhen their value is ?inside,?
that is, when thespace under consideration occurs between such apair.3.3 Sentence Breaker Training CorpusThai corpora which are marked with sentencebreaks are required for training.
We assembled acorpus of 361,802 probable sentences.
This cor-pus includes purchased, publicly available, andweb-crawled content.
In total it contains 911,075spaces, a figure which includes one inter-sentence space per sentence, generated as de-scribed below.3.4 Out-of-context SentencesFor SB training, paragraphs are first tokenizedinto words as described in Section 2.2.
Thisprocess does not introduce new spaces betweentokens; only original spaces in the text are classi-fied as sb/nsb and used for the context featuresdescribed below.
To keep this distinction clear,token boundaries are indicated by a slash ratherthan space in the examples shown in this paper.For 91% of our training sentences, the para-graphs from which they originate are inaccessi-ble.
In feature extraction for each of these sen-tences, we wrap the sentence?s head around to itstail to obtain its sb context.
In other words, for asentence of tokens t0-tn-1, the context of sb (thelast space) is given by{ w=tn-2, x=tn-1, y=t0, z=t1 }.This process was illustrated in Figure 8.
Al-though not an ideal substitute for sentences incontext, this ensures that we extract at least onesb context per sentence.
The number of nsb con-texts extracted per sentence is equal to the num-ber of interior space tokens in the original sen-tence.
Sentence wrapping is not needed whentraining with sentence-delimited paragraphsources.
Contexts sb and nsb are extracted fromthe token stream of the entire paragraph andwrapping is used only to generate one additionalsb for the entire paragraph.123.5 Sentence Breaker EvaluationAlthough evaluation against a single-domaincorpus does not measure important design re-quirements of our system, namely resilience tobroad-domain input texts, we evaluated againstthe ORCHID corpus (Charoenporn et al 1997)for the purpose of comparison with the existingliterature.
Following the methodology of the stu-dies cited below, we use 10-fold ?10% averagedtesting against the ORCHID corpus.Our results are consistent with recent work us-ing the Winnow algorithm, which itself com-pares favorably with the probabilistic POS tri-gram approach.
Both of these studies use evalua-tion metrics, attributed to Black and Taylor(1997), which aim to more usefully measure sen-tence-breaker utility.
Accordingly, the followingdefinitions are used in Table 2:space-correct =  (#correct sb+#correct nsb)total # of space tokensfalse break= #sb false positivestotal # of space tokensIt was generally possible to reconstruct preci-sion and recall figures from these published re-sults1 and we present a comprehensive table ofresults.
Reconstructed values are marked with adagger and the optimal result in each category ismarked in boldface.Mittrapiyanuruket alCharoenpornsawatet alOur resultmethod POS Trigram Winnow MaxEnt#sb in reference 10528 1086?
2133#space tokens 33141 3801 7227nsb-precision 90.27?
91.48?
93.18nsb-recall 87.18?
97.56?
94.41sb-precision 74.35?
92.69?
86.21sb-recall 79.82 77.27 83.50?space-correct?
85.26 89.13 91.19?false-break?
8.75 1.74 3.94Table 2.
Evaluation of Thai Sentence Breakers againstORCHIDFinally, we would be remiss in not acknowl-edging the general hazard of assigning sentencebreaks in a language such as Thai, where source1 Full results for Charoenpornsawat et al are reconstructed basedon remarks in their text, including that ?the ratio of the number of[nsb to sb] is about 5:2.?text authors may intentionally include or omitspaces in order to create syntactic or semanticambiguity.
We defer to Mittrapiyanuruk andSornlertlamvanich (2000) and Aroonmanakun(2007) for informed commentary on this topic.4 SMT System and IntegrationThe primary application for which we developedthe Thai sentence breaker described in this workis the Microsoft?
BING?
general-domain ma-chine translation service.
In this section, we pro-vide a brief overview of this large-scale SMTsystem, focusing on Thai-specific integrationissues.4.1 OverviewLike many multilingual SMT systems, our sys-tem is based on hybrid generative/discriminativemodels.
Given a sequence of foreign words, f, itsbest translation is the sequence of target words,e, that maximizes??
= argmax?
?(?|?)
=  argmax?
?(?|?)?(?
)= argmaxe  { log ?(?|?)
+ log ?(?
)}where the translation model ?(?|?)
is computedon dozens to hundreds of features.
The targetlanguage model (LM), ?(?
), is represented by asmoothed n-grams (Chen 1996) and sometimesmore than one LM is adopted in practice.
Toachieve the best performance, the log likelihoodsevaluated by these features/models are linearlycombined.
After ?(?|?)
and ?(?)
are trained, thecombination weights ??
are tuned on a held-outdataset to optimize an objective function, whichwe set to be the BLEU score (Papineni et al2002):{???}
= max{??}
BLEU({??
}, {?})??
=  argmaxe  {???log???(?|?)
+???log???(?
)}where {r} is the set of gold translations for thegiven input source sentences.
To learn ??
we usethe algorithm described by Och (2003), wherethe decoder output at any point is approximatedusing n-best lists, allowing an optimal linesearch to be employed.4.2 Phrasal and Treelet TranslationSince we have a high-quality real-time rule-based English parser available, we base our Eng-13lish-to-Thai translation (ENG-THA) on the?treelet?
concept suggested in Menezes andQuirk (2008).
This approach parses the sourcelanguage into a dependency tree which includespart-of-speech labels.Lacking a Thai parser, we use a purely statis-tical phrasal translator after Pharaoh (Koehn2004) for THA-ENG translation, where weadopt the name and date translation described inSections 2.3 and 2.4.We also experimented with phrasal ENG-THA translation.
Though we actually achieved aslightly better BLEU score than treelet for thistranslation direction, qualitative human evalua-tion by native speaker informants was mixed.We adopted the treelet ENG-THA in the finalsystem, for its better re-spacing (Section 2.5).4.3 Training, Development and Test DataNaturally, our system relies on parallel text cor-pora to learn the mapping between two languag-es.
The parallel corpus contains sentence pairs,corresponding to translations of each other.
ForThai, quality corpora are generally not availablein sufficient quality for training a general-domain SMT system.
For the ENG-THA pair,we resort to Internet crawls as a source of text.We first identify paired documents, break eachdocument into sentences, and align sentences inone document against those in its parallel docu-ment.
Bad alignments are discarded.
Only sen-tence pairs with high alignment confidence arekept in our parallel corpus.
Our sentence align-ment algorithm is based on Moore (2002).For our ENG-THA translation system, we as-sembled three resources: a parallel training cor-pus, a development bitext (also called the lamb-da set) for training the feature combinationweights {??
}, and a test corpus for BLEU andhuman evaluation.
Both the lambda and the testsets have single reference translations per sen-tence.Data Set #Sentences(ENG||THA) training 725K(ENG,THA) lambda 2K(ENG,THA) test 5KTHA LM text 10.3MENG LM text 45.6MTable 3.
Corpus size of parallel and monolingual dataAlthough it is well known that language trans-lation pairs are not symmetric, we use thesesame resources to build our THA-ENG transla-tion system due to the lack of additional corpora.Our parallel MT corpus consists of approx-imately 725,000 English-Thai sentence pairsfrom various sources.
Additionally we have 9.6million Thai sentences, which are used to train aThai 4-gram LM for ENG-THA translation, to-gether with the Thai sentences in the parallelcorpus.
Trigrams and 4-grams that occur onlyonce are pruned, and n-gram backoff weights arere-normalized after pruning, with the survivingKN smoothed probabilities intact (Kneser andNey 1995).
Similarly, a 4-gram ENG LM istrained for THA-ENG translation, on a total of45.6M English sentences.For both the lambda and test sets, THA LMincurs higher out-of-vocabulary (OOV) rates(1.6%) than ENG LM (0.7%), due to its smallertraining set and thus smaller lexicon.
Both trans-lation directions define the maximumphrase/treelet length to be 4 and the maximumre-ordering jump to be 4 as well.4.4 BLEU ScoresTo evaluate our end-to-end performance, wecompute case insensitive 4-gram BLEU scores.Translation outputs are WB first according to theThai/English tokenizer, before BLEU scores arecomputed.
The BLEU scores on the test sets areshown in Table 4.
We are not aware of any pre-viously published BLEU results for either direc-tion of this language pair.BLEUTHA-ENG 0.233ENG-THA 0.194Table 4.
Four-gram case-insensitive BLEU scores.Figures 10 and 11 illustrate sample outputs forthe each translation direction, with referencetranslations.INPUT: ???????????????????????????
???
????
??????????????????????????
?????????????????????????
?OUTPUT: In Thailand a Orchid approximately 175 type ifextinct from Thailand.
It means extinct from the world.REF: In Thailand, there are about 175 species of Orchid.
Ifthey disappear from Thailand, they will be gone from theworld.Figure 10.
THA-ENG Sample Translation Output14INPUT: In our nation the problems and barriers we face arejust problems and barriers of law not selection or develop-ment.OUTPUT: ??????????????????
???????????????????????????????????????????????????????????????????????????
?REF: ?????????????????????????????
???????????????????????????????
????????????????????????????????????????????????
?Figure 11.
ENG-THA Sample Translation OutputAlthough the translation quality is far from beingperfect, SMT is making good process on build-ing useful applications.5 Conclusion and Future WorkOur maximum entropy model for Thai sentence-breaking achieves results which are consistentwith contemporary work in this task, allowing usto overcome this obstacle to Thai SMT integra-tion.
This general approach can be applied toother South-East Asian languages in which spacedoes not deterministically delimit sentenceboundaries.In Arabic writing, commas are often used toseparate sentences until the end of a paragraphwhen a period is finally used.
In this case, thecomma character is similar to the space token inThai where its usage is ambiguous.
We can usethe same approach (perhaps with different lin-guistic features) to identify which commas aresentence-breaking and which are not.Our overall system incorporates a range of in-dependent solutions to problems in Thai textprocessing, including character sequence norma-lization, tokenization, name and date identifica-tion, sentence-breaking, and Thai text re-spacing.
We successfully integrated each solu-tion into an existing large-scale SMT frame-work, obtaining sufficient quality to release theThai-English language pair in a high-volume,general-domain, free public online service.There remains much room for improvement.We need to find or create true Thai-English di-rectional corpora to train the lambdas and to testour models.
The size of our parallel corpus forThai should increase by at least an order of mag-nitude, without loss of bitext quality.
With alarger corpus, we can consider longer phraselength, higher-order n-grams, and longer re-ordering distance.ReferencesW.
Aroonmanakun.
2007.
Thoughts on Wordand Sentence Segmentation in Thai.
In Pro-ceedings of the Seventh International Sympo-sium on Natural Language Processing, Pat-taya, Thailand, 85-90.F.
Avery Bishop, David C. Brown and David M.Meltzer.
2003.
Supporting MultilanguageText Layout and Complex Scripts with Win-dows 2000. http://www.microsoft.com/typo-graphy/developers/uniscribe/intro.htmA.
W. Black and P. Taylor.
1997.
AssigningPhrase Breaks from Part-of-Speech Se-quences.
Computer Speech and Language,12:99-117.Thatsanee Charoenporn, Virach Sornlertlamva-nich, and Hitoshi Isahara.
1997.
Building AThai Part-Of-Speech Tagged Corpus (ORC-HID).Paisarn Charoenpornsawat and Virach Sornler-tlamvanich.
2001.
Automatic sentence breakdisambiguation for Thai.
In InternationalConference on Computer Processing ofOriental Languages (ICCPOL), 231-235.S.
F. Chen and J. Goodman.
1996.
An empiricalstudy of smoothing techniques for languagemodeling.
In Proceedings of the 34th AnnualMeeting on Association for ComputationalLinguistics, 310-318.
Morristown, NJ: ACL.J.
N. Darroch and D. Ratcliff.
1972.
GeneralizedIterative Scaling for Log-Linear Models.
TheAnnals of Mathematical Statistics, 43(5):1470-1480.Choochart Haruechaiyasak, Sarawoot Kon-gyoung, and Matthew N. Dailey.
2008.
AComparative Study on Thai Word Segmenta-tion Approaches.
In Proceedings of ECTI-CON 2008.
Pathumthani, Thailand: ECTI.Reinhard Kneser and Hermann Ney.
1995.
Im-proved Backing-Off for M-Gram LanguageModeling.
In Proceedings of InternationalConference on Acoustics, Speech and SignalProcesing (ICASSP), 1:181-184.Philipp Koehn.
2004.
Pharaoh: a Beam SearchDecoder for Phrase-Based Statistical MachineTranslation Models.
In Proceedings of the As-15sociation of Machine Translation in the Amer-icas (AMTA-2004).Arul Menezes, and Chris Quirk.
2008.
SyntacticModels for Structural Word Insertion and De-letion during Translation.
In Proceedings ofthe 2008 Conference on Empirical Methods inNatural Language Processing.P.
Mittrapiyanuruk and V. Sornlertlamvanich.2000.
The Automatic Thai Sentence Extrac-tion.
In Proceedings of the Fourth Symposiumon Natural Language Processing, 23-28.Robert C. Moore.
2002.
Fast and Accurate Sen-tence Alignment of Bilingual Corpora.
In Ma-chine Translation: From Research to RealUsers (Proceedings, 5th Conference of the As-sociation for Machine Translation in theAmericas, Tiburon, California), Springer-Verlag, Heidelberg, Germany, 135-244Franz Josef Och.
2003.
Minimum error ratetraining in statistical machine translation.
InProceedings of the 41th Annual Meeting of theAssociation for Computational Linguistics.Stroudsburg, PA: ACL.David D. Palmer and Marti A. Hearst.
1997.Adaptive Multilingual Sentence BoundaryDisambiguation.
Computational Linguistics,23:241-267.Kishore Papineni, Salim Roukos, Todd Ward,and Wei-jing Zhu.
2002.
BLEU: a method forautomatic evaluation of machine translation.In Proceedings of the 40th Annual meeting ofthe Association for Computational Linguistics,311?318.
Stroudsburg, PA: ACL.Adwait Ratnaparkhi, 1996.
A Maximum EntropyModel for Part-of-Speech Tagging.
In Pro-ceedings of the Conference on Empirical Me-thods in Natural Language Processing, 133-142.Jeffrey C. Reynar and Adwait Ratnaparkhi.1997.
A Maximum Entropy Approach to Iden-tifying Sentence Boundaries, In Proceedingsof the Fifth Conference on Applied NaturalLanguage Processing, 16-19.Suphawut Wathabunditkul.
2003.
Spacing in theThai Language.
http://www.thailanguage.com/ref/spacing16
