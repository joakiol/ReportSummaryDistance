Proceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 9?12,Columbus, June 2008. c?2008 Association for Computational LinguisticsBART: A Modular Toolkit for Coreference ResolutionYannick VersleyUniversity of Tu?bingenversley@sfs.uni-tuebingen.deSimone Paolo PonzettoEML Research gGmbHponzetto@eml-research.deMassimo PoesioUniversity of Essexpoesio@essex.ac.ukVladimir EidelmanColumbia Universityvae2101@columbia.eduAlan JernUCLAajern@ucla.eduJason SmithJohns Hopkins Universityjsmith@jhu.eduXiaofeng YangInst.
for Infocomm Researchxiaofengy@i2r.a-star.edu.sgAlessandro MoschittiUniversity of Trentomoschitti@dit.unitn.itAbstractDeveloping a full coreference system ableto run all the way from raw text to seman-tic interpretation is a considerable engineer-ing effort, yet there is very limited avail-ability of off-the shelf tools for researcherswhose interests are not in coreference, or forresearchers who want to concentrate on aspecific aspect of the problem.
We presentBART, a highly modular toolkit for de-veloping coreference applications.
In theJohns Hopkins workshop on using lexicaland encyclopedic knowledge for entity dis-ambiguation, the toolkit was used to ex-tend a reimplementation of the Soon et al(2001) proposal with a variety of additionalsyntactic and knowledge-based features, andexperiment with alternative resolution pro-cesses, preprocessing tools, and classifiers.1 IntroductionCoreference resolution refers to the task of identify-ing noun phrases that refer to the same extralinguis-tic entity in a text.
Using coreference informationhas been shown to be beneficial in a number of othertasks, including information extraction (McCarthyand Lehnert, 1995), question answering (Morton,2000) and summarization (Steinberger et al, 2007).Developing a full coreference system, however, isa considerable engineering effort, which is why alarge body of research concerned with feature en-gineering or learning methods (e.g.
Culotta et al2007; Denis and Baldridge 2007) uses a simpler butnon-realistic setting, using pre-identified mentions,and the use of coreference information in summa-rization or question answering techniques is not aswidespread as it could be.
We believe that the avail-ability of a modular toolkit for coreference will sig-nificantly lower the entrance barrier for researchersinterested in coreference resolution, as well as pro-vide a component that can be easily integrated intoother NLP applications.A number of systems that perform coreferenceresolution are publicly available, such as GUITAR(Steinberger et al, 2007), which handles the fullcoreference task, and JAVARAP (Qiu et al, 2004),which only resolves pronouns.
However, literatureon coreference resolution, if providing a baseline,usually uses the algorithm and feature set of Soonet al (2001) for this purpose.Using the built-in maximum entropy learnerwith feature combination, BART reaches 65.8%F-measure on MUC6 and 62.9% F-measure onMUC7 using Soon et al?s features, outperformingJAVARAP on pronoun resolution, as well as theSoon et al reimplementation of Uryupina (2006).Using a specialized tagger for ACE mentions andan extended feature set including syntactic features(e.g.
using tree kernels to represent the syntacticrelation between anaphor and antecedent, cf.
Yanget al 2006), as well as features based on knowledgeextracted from Wikipedia (cf.
Ponzetto and Smith, inpreparation), BART reaches state-of-the-art resultson ACE-2.
Table 1 compares our results, obtainedusing this extended feature set, with results fromNg (2007).
Pronoun resolution using the extendedfeature set gives 73.4% recall, coming near special-ized pronoun resolution systems such as (Denis andBaldridge, 2007).9Figure 1: Results analysis in MMAX22 System ArchitectureThe BART toolkit has been developed as a tool toexplore the integration of knowledge-rich featuresinto a coreference system at the Johns Hopkins Sum-mer Workshop 2007.
It is based on code and ideasfrom the system of Ponzetto and Strube (2006), butalso includes some ideas from GUITAR (Steinbergeret al, 2007) and other coreference systems (Versley,2006; Yang et al, 2006).
1The goal of bringing together state-of-the-art ap-proaches to different aspects of coreference res-olution, including specialized preprocessing andsyntax-based features has led to a design that is verymodular.
This design provides effective separationof concerns across several several tasks/roles, in-cluding engineering new features that exploit dif-ferent sources of knowledge, designing improved orspecialized preprocessing methods, and improvingthe way that coreference resolution is mapped to amachine learning problem.Preprocessing To store results of preprocessingcomponents, BART uses the standoff format of theMMAX2 annotation tool (Mu?ller and Strube, 2006)with MiniDiscourse, a library that efficiently imple-ments a subset of MMAX2?s functions.
Using ageneric format for standoff annotation allows the useof the coreference resolution as part of a larger sys-tem, but also performing qualitative error analysisusing integrated MMAX2 functionality (annotation1An open source version of BART is available fromhttp://www.sfs.uni-tuebingen.de/?versley/BART/.diff, visual display).Preprocessing consists in marking up nounchunks and named entities, as well as additional in-formation such as part-of-speech tags and mergingthese information into markables that are the start-ing point for the mentions used by the coreferenceresolution proper.Starting out with a chunking pipeline, whichuses a classical combination of tagger and chun-ker, with the Stanford POS tagger (Toutanova et al,2003), the YamCha chunker (Kudoh and Mat-sumoto, 2000) and the Stanford Named Entity Rec-ognizer (Finkel et al, 2005), the desire to use richersyntactic representations led to the development ofa parsing pipeline, which uses Charniak and John-son?s reranking parser (Charniak and Johnson, 2005)to assign POS tags and uses base NPs as chunkequivalents, while also providing syntactic trees thatcan be used by feature extractors.
BART also sup-ports using the Berkeley parser (Petrov et al, 2006),yielding an easy-to-use Java-only solution.To provide a better starting point for mention de-tection on the ACE corpora, the Carafe pipelineuses an ACE mention tagger provided by MITRE(Wellner and Vilain, 2006).
A specialized mergerthen discards any base NP that was not detected tobe an ACE mention.To perform coreference resolution proper, themention-building module uses the markables cre-ated by the pipeline to create mention objects, whichprovide an interface more appropriate for corefer-ence resolution than the MiniDiscourse markables.These objects are grouped into equivalence classesby the resolution process and a coreference layer iswritten into the document, which can be used for de-tailed error analysis.Feature Extraction BART?s default resolver goesthrough all mentions and looks for possible an-tecedents in previous mentions as described by Soonet al (2001).
Each pair of anaphor and candi-date is represented as a PairInstance object,which is enriched with classification features by fea-ture extractors, and then handed over to a machinelearning-based classifier that decides, given the fea-tures, whether anaphor and candidate are corefer-ent or not.
Feature extractors are realized as sepa-rate classes, allowing for their independent develop-10Figure 2: Example system configurationment.
The set of feature extractors that the systemuses is set in an XML description file, which allowsfor straightforward prototyping and experimentationwith different feature sets.Learning BART provides a generic abstractionlayer that maps application-internal representationsto a suitable format for several machine learningtoolkits: One module exposes the functionality ofthe the WEKA machine learning toolkit (Wittenand Frank, 2005), while others interface to special-ized state-of-the art learners.
SVMLight (Joachims,1999), in the SVMLight/TK (Moschitti, 2006) vari-ant, allows to use tree-valued features.
SVM Classi-fication uses a Java Native Interface-based wrapperreplacing SVMLight/TK?s svm classify pro-gram to improve the classification speed.
Also in-cluded is a Maximum entropy classifier that isbased upon Robert Dodier?s translation of Liu andNocedal?s (1989) L-BFGS optimization code, witha function for programmatic feature combination.2Training/Testing The training and testing phasesslightly differ from each other.
In the training phase,the pairs that are to be used as training exampleshave to be selected in a process of sample selection,whereas in the testing phase, it has to be decidedwhich pairs are to be given to the decision functionand how to group mentions into equivalence rela-tions given the classifier decisions.This functionality is factored out into the en-2see http://riso.sourceforge.netcoder/decoder component, which is separate fromfeature extraction and machine learning itself.
Itis possible to completely change the basic behav-ior of the coreference system by providing newencoders/decoders, and still rely on the surround-ing infrastructure for feature extraction and machinelearning components.3 Using BARTAlthough BART is primarily meant as a platform forexperimentation, it can be used simply as a corefer-ence resolver, with a performance close to state ofthe art.
It is possible to import raw text, performpreprocessing and coreference resolution, and eitherwork on the MMAX2-format files, or export the re-sults to arbitrary inline XML formats using XSLstylesheets.Adapting BART to a new coreferentially anno-tated corpus (which may have different rules formention extraction ?
witness the differences be-tween the annotation guidelines of MUC and ACEcorpora) usually involves fine-tuning of mention cre-ation (using pipeline and MentionFactory settings),as well as the selection and fine-tuning of classi-fier and features.
While it is possible to make rad-ical changes in the preprocessing by re-engineeringcomplete pipeline components, it is usually possi-ble to achieve the bulk of the task by simply mix-ing and matching existing components for prepro-cessing and feature extraction, which is possible bymodifying only configuration settings and an XML-11BNews NPaper NWireRecl Prec F Recl Prec F Recl Prec Fbasic feature set 0.594 0.522 0.556 0.663 0.526 0.586 0.608 0.474 0.533extended feature set 0.607 0.654 0.630 0.641 0.677 0.658 0.604 0.652 0.627Ng 2007?
0.561 0.763 0.647 0.544 0.797 0.646 0.535 0.775 0.633?
: ?expanded feature set?
in Ng 2007; Ng trains on the entire ACE training corpus.Table 1: Performance on ACE-2 corpora, basic vs. extended feature setbased description of the feature set and learner(s)used.Several research groups focusing on coreferenceresolution, including two not involved in the ini-tial creation of BART, are using it as a platformfor research including the use of new informationsources (which can be easily incorporated into thecoreference resolution process as features), differentresolution algorithms that aim at enhancing globalcoherence of coreference chains, and also adaptingBART to different corpora.
Through the availabilityof BART as open source, as well as its modularityand adaptability, we hope to create a larger com-munity that allows both to push the state of the artfurther and to make these improvements available tousers of coreference resolution.Acknowledgements We thank the CLSP at JohnsHopkins, NSF and the Department of Defense forensuring funding for the workshop and to EMLResearch, MITRE, the Center for Excellence inHLT, and FBK-IRST, that provided partial support.Yannick Versley was supported by the DeutscheForschungsgesellschaft as part of SFB 441 ?Lin-guistic Data Structures?
; Simone Paolo Ponzetto hasbeen supported by the Klaus Tschira Foundation(grant 09.003.2004).ReferencesCharniak, E. and Johnson, M. (2005).
Coarse-to-fine n-bestparsing and maxent discriminative reranking.
In Proc.
ACL2005.Culotta, A., Wick, M., and McCallum, A.
(2007).
First-orderprobabilistic models for coreference resolution.
In Proc.HLT/NAACL 2007.Denis, P. and Baldridge, J.
(2007).
A ranking approach to pro-noun resolution.
In Proc.
IJCAI 2007.Finkel, J. R., Grenager, T., and Manning, C. (2005).
Incorpo-rating non-local information into information extraction sys-tems by Gibbs sampling.
In Proc.
ACL 2005, pages 363?370.Joachims, T. (1999).
Making large-scale SVM learning prac-tical.
In Scho?lkopf, B., Burges, C., and Smola, A., editors,Advances in Kernel Methods - Support Vector Learning.Kudoh, T. and Matsumoto, Y.
(2000).
Use of Support VectorMachines for chunk identification.
In Proc.
CoNLL 2000.Liu, D. C. and Nocedal, J.
(1989).
On the limited memorymethod for large scale optimization.
Mathematical Program-ming B, 45(3):503?528.McCarthy, J. F. and Lehnert, W. G. (1995).
Using decision treesfor coreference resolution.
In Proc.
IJCAI 1995.Morton, T. S. (2000).
Coreference for NLP applications.
InProc.
ACL 2000.Moschitti, A.
(2006).
Making tree kernels practical for naturallanguage learning.
In Proc.
EACL 2006.Mu?ller, C. and Strube, M. (2006).
Multi-level annotation oflinguistic data with MMAX2.
In Braun, S., Kohn, K., andMukherjee, J., editors, Corpus Technology and LanguagePedagogy: New Resources, New Tools, New Methods.
PeterLang, Frankfurt a.M., Germany.Ng, V. (2007).
Shallow semantics for coreference resolution.
InProc.
IJCAI 2007.Petrov, S., Barett, L., Thibaux, R., and Klein, D. (2006).
Learn-ing accurate, compact, and interpretable tree annotation.
InCOLING-ACL 2006.Ponzetto, S. P. and Strube, M. (2006).
Exploiting semantic rolelabeling, WordNet and Wikipedia for coreference resolution.In Proc.
HLT/NAACL 2006.Qiu, L., Kan, M.-Y., and Chua, T.-S. (2004).
A public referenceimplementation of the RAP anaphora resolution algorithm.In Proc.
LREC 2004.Soon, W. M., Ng, H. T., and Lim, D. C. Y.
(2001).
A machinelearning approach to coreference resolution of noun phrases.Computational Linguistics, 27(4):521?544.Steinberger, J., Poesio, M., Kabadjov, M., and Jezek, K. (2007).Two uses of anaphora resolution in summarization.
Informa-tion Processing and Management, 43:1663?1680.
Specialissue on Summarization.Toutanova, K., Klein, D., Manning, C. D., and Singer, Y.(2003).
Feature-rich part-of-speech tagging with a cyclic de-pendency network.
In Proc.
NAACL 2003, pages 252?259.Uryupina, O.
(2006).
Coreference resolution with and withoutlinguistic knowledge.
In Proc.
LREC 2006.Versley, Y.
(2006).
A constraint-based approach to noun phrasecoreference resolution in German newspaper text.
In Kon-ferenz zur Verarbeitung Natu?rlicher Sprache (KONVENS2006).Wellner, B. and Vilain, M. (2006).
Leveraging machine read-able dictionaries in discriminative sequence models.
In Proc.LREC 2006.Witten, I. and Frank, E. (2005).
Data Mining: Practical ma-chine learning tools and techniques.
Morgan Kaufmann.Yang, X., Su, J., and Tan, C. L. (2006).
Kernel-based pronounresolution with structured syntactic knowledge.
In Proc.CoLing/ACL-2006.12
