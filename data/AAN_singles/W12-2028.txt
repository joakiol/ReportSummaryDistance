The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 242?250,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsHOO 2012 Error Recognition and Correction Shared Task:Cambridge University Submission ReportEkaterina KochmarComputer LaboratoryUniversity of Cambridgeek358@cl.cam.ac.uk?istein AnderseniLexIR LtdCambridgeand@ilexir.co.ukTed BriscoeComputer LaboratoryUniversity of Cambridgeejb@cl.cam.ac.ukAbstractPrevious work on automated error recognitionand correction of texts written by learners ofEnglish as a Second Language has demon-strated experimentally that training classifierson error-annotated ESL text generally outper-forms training on native text alone and thatadaptation of error correction models to thenative language (L1) of the writer improvesperformance.
Nevertheless, most extant mod-els have poor precision, particularly when at-tempting error correction, and this limits theirusefulness in practical applications requiringfeedback.We experiment with various feature types,varying quantities of error-corrected data, andgeneric versus L1-specific adaptation to typi-cal errors using Na?
?ve Bayes (NB) classifiersand develop one model which maximizes pre-cision.
We report and discuss the results for8 models, 5 trained on the HOO data and3 (partly) on the full error-coded CambridgeLearner Corpus, from which the HOO data isdrawn.1 IntroductionThe task of detecting and correcting writing errorsmade by learners of English as a Second Language(ESL) has recently become a focus of research.The majority of previous papers in this areahave presented machine learning methods with mod-els being trained on well-formed native Englishtext (Eeg-Olofsson and Knutsson, 2003; De Feliceand Pulman, 2008; Gamon et al, 2008; Han et al,2006; Izumi et al, 2003; Tetreault and Chodorow,2008; Tetreault et al, 2010).
However, some recentapproaches have explored ways of using annotatednon-native text either by incorporating error-taggeddata into the training process (Gamon, 2010; Hanet al, 2010), or by using native language-specificerror statistics (Rozovskaya and Roth, 2010b; Ro-zovskaya and Roth, 2010c; Rozovskaya and Roth,2011).
Both approaches show improvements overthe models trained solely on well-formed native text.Training a model on error-tagged non-nativetext is expensive, as it requires large amounts ofmanually-annotated data, not currently publicallyavailable.
In contrast, using native language-specificerror statistics to adapt a model to a writer?s first ornative language (L1) is less restricted by the amountof training data.Rozovskaya and Roth (2010b; 2010c) show thatadapting error corrections to the writer?s L1 and in-corporating artificial errors, in a way that mimicsthe typical error rates and confusion patterns of non-native text, improves both precision and recall com-pared to classifiers trained on native data only.
Theapproach proposed in Rozovskaya and Roth (2011)uses L1-specific error correction patterns as a dis-tribution on priors over the corrections, incorporat-ing the appropriate priors into a generic Na?
?ve Bayes(NB) model.
This approach is both cheaper to im-plement, since it does not require a separate classi-fier to be trained for every L1, and more effective,since the priors condition on the writer?s L1 as wellas on the possible confusion sets.Some extant approaches have achieved good re-sults on error detection.
However, error correctionis much harder and on this task precision remains242low.
This is a disadvantage for applications suchas self-tutoring or writing assistance, which requirefeedback to the user.
A high proportion of error-ful suggestions is likely to further confuse learnersand/or non-native writers rather than improve theirwriting or assist learning.
Instead a system whichmaximizes precision over recall returning accuratesuggestions for a small proportion of errors is likelyto be more helpful (Nagata and Nakatani, 2010).In section 2 we describe the data used for train-ing and testing the systems we developed.
In sec-tion 3 we describe the preprocessing of the ESL textundertaken to provide a source of features for theclassifiers.
We also discuss the feature types thatwe exploit in our classifiers.
In section 4 we de-scribe and report results for a high precision systemwhich makes no attempt to generalize from train-ing data.
In section 5 we describe our approach toadapting multiclass NB classifiers to characteristicerrors and L1s.
We also report the performance ofsome of these NB classifiers on the training and testdata.
In section 6 we report the official results ofall our submitted runs on the test data and also onthe HOO training data, cross-validated where appro-priate.
Finally, we briefly discuss our main results,further work, and lessons learnt.2 Cambridge Learner CorpusThe Cambridge Learner Corpus1 (CLC) is a largecorpus of learner English.
It has been developedby Cambridge University Press in collaboration withCambridge Assessment, and contains examinationscripts written by learners of English from 86 L1backgrounds.
The scripts have been produced bylanguage learners taking Cambridge Assessment?sESL examinations.2The linguistic errors committed by the learnershave been manually annotated using a taxonomy of86 error types (Nicholls, 2003).
Each error has beenmanually identified and tagged with an appropriatecode, specifying the error type, and a suggested cor-rection.
Additionally, the scripts are linked to meta-data about examination and learner.
This includesthe year of examination, the question prompts, the1http://www.cup.cam.ac.uk/gb/elt/catalogue/subject/custom/item3646603/Cambridge-International-Corpus-Cambridge-Learner-Corpus2http://www.cambridgeesol.org/learner?s L1, as well as the grades obtained.
The cur-rent version of the CLC contains about 20M wordsof error-annotated scripts from a wide variety of ex-aminations.The HOO training and test datasets are drawnfrom the CLC.
The training dataset is a reformatted1000-script subset of a publically-available subset ofCLC scripts produced by learners sitting the FirstCertficate in English (FCE) examination.3 This ex-amination assesses English at an upper-intermediatelevel, so many learners sitting this exam still man-ifest a number of errors motivated by the conven-tions of their L1s.
The CLC-FCE subcorpus was ex-tracted, anonymized, and made available as a set ofXML files by Yannakoudakis et al (2011).4The HOO training dataset contains scripts fromFCE examinations undertaken in the years 2000 and2001 written by speakers of 16 L1s.
These scriptscan be divided into two broad L1 typological groups,Asian (Chinese, Thai, Korean, Japanese) and Euro-pean (French, Spanish, Italian, Portuguese, Catalan,Greek, Russian, Polish).
The latter can be furthersubdivided into Slavic (Russian, Polish) and Ro-mance.
In turn, the Romance languages differ in ty-pological relatedness with, for example, Portugueseand Spanish being closer than Spanish and French.Error coding which is not relevant to preposition ordeterminer errors has been removed from the train-ing data so that only six error type annotations areretained for training: incorrect, missing or unnec-essary determiners (RD, MD, UD) and prepositions(RT, MT, UT).One consequence of this reformatting is that thecontexts of these errors often contain further errorsof different types that are no longer coded.
The ideais that errors should be considered in their naturalhabitat, and that correcting and copy-editing the sur-rounding text would create an artificial task.
On theother hand, not correcting anything makes it difficultin some cases and nigh impossible in others to de-termine whether a given determiner or preposition iscorrect or not.
The error-coding in the CLC in suchcases (provided the writer?s intent is deemed recov-erable) depends not only on the original text, but alsoon the correction of nearby errors.3http://www.cambridgeesol.org/exams/general-english/fce.html4http://ilexir.co.uk/applications/clc- fce-dataset/243Certain errors even appear as a direct result ofcorrecting others: for instance, the phrase to sleepin tents has been corrected to to sleep in a tent inthe CLC; this ends up as a ?correction?
to to sleepin a tents in the HOO dataset.
This issue is diffi-cult to avoid given that the potential solutions are alllabour-intensive (explicit indication of dependenciesbetween error annotations, completely separate errorannotation for different types of errors, or manual re-moval of spurious errors after extraction of the typesof error under consideration), and we mention it heremainly to explain the origin of some surprising an-notations in the dataset.A more HOO-specific problem is the ?
[removalof] elements [from] some of [the] files [...] todispose of nested edits and other phenomena thatcaused difficulties in the preprocessing of the data?
(Dale et al, 2012).
This approach unfortunatelyleads to mutilated sentences such as I think if wewear thistoevery wherespace ships.
This mean.
re-placing the original I think if we wear this clothes wewill travel to every where easier than we use cars,ships, planes and space ships.
This mean the engi-neering will find the way to useless petrol for it, so itmust useful in the future.The HOO test set consists of 100 responses toindividual prompts from FCE examinations set be-tween 1993 and 2009, also drawn from the CLC.As a side effect of removing the test data from thefull CLC, we have discovered that the distribution ofL1s, examination years and exam prompts is differ-ent from the training data.
There are 27 L1s exem-plified, a superset of the 16 seen in the HOO train-ing data; about half are Romance, and the rest arewidely distributed with Asian and Slavic languagesless well represented than in the training data.In the experiments reported below, we make useof both the HOO training data and the full 20Mwords of error-annotated CLC, but with the HOOtest data removed, to train our systems.
Wheneverwe use the larger training set we refer to this as thefull CLC below.3 Data PreprocessingWe parsed the training and test data (see Section2) using the Robust Accurate Statistical Parsing(RASP) system with the standard tokenization andMy friend was (MD: a) good studentGrammatical Relations (GRs):(ncsubj be+ed:3 VBDZ friend:2 NN1 )(xcomp be+ed:3 VBDZ student:6 NN1)(ncmod student:6 NN1 good:5 JJ)(det friend:2 NN1 My:1 APP$)*(det student:6 NN1 a:4 AT1)Figure 1: RASP GR outputsentence boundary detection modules and the unlex-icalized version of the parser (Briscoe et al, 2006)in order to broaden the space of candidate fea-tures types.
The features used in our experimentsare mainly motivated by the fact that lexical andgrammatical features have been shown in previouswork to be effective for error detection and correc-tion.
We believe RASP is an appropriate tool touse with ESL text because the PoS tagger deploysa well-developed unknown word handling mecha-nism, which makes it relatively robust to noisy in-put such as misspellings, and because the parser de-ploys a hand-coded grammar which indicates un-grammaticality of sentences and markedness of con-structions and is encoded entirely in terms of PoStag sequences.
We utilize the open-source versionof RASP embedded in an XML-handling pipelinethat allows XML-encoded metadata in the CLC andHOO training data to be preserved in the output,but ensures that unannotated text is passed to RASP(Andersen et al, 2008).Relevant output of the system is shown in Fig-ure 1 for a typical errorful example.
The grammati-cal relations (GRs) form a connected, directed graphof typed bilexical head-dependent relations (where anon-fragmentary analysis is found).
Nodes are lem-matized word tokens with associated PoS tag andsentence position number.
Directed arcs are labelledwith GR types.
In the factored representation shownhere, each line represents a GR type, the head node,the dependent node, and optional subtype informa-tion either after the GR type or after the dependent.In this example, the asterisked GR would be missingin the errorful version of the sentence.
We extract themost likely analysis for each sentence based on themost probable tag sequence found by the tagger.Extraction of the lexical and grammatical infor-244mation from the parser output is easier when a deter-miner or preposition is present than when it is miss-ing.
During training, for all nouns, we checked for adet relation to a determiner, and whenever no detGR is present, we checked whether the noun is pre-ceded by an MD annotation in the XML file.
Formissing prepositions, we have only extracted caseswhere a noun is governed by a verb with a dobjrelation, and cases where a noun is governed by an-other noun with an ncmod (non-clausal modifier)relation.
For example, in It?s been a long time sinceI last wrote you, in absence of the preposition to theparser would ?recognize?
a dobj relation betweenyou and wrote, and this case would be used as atraining example for a missing preposition, while Itrusted him with the same dobj relation betweentrusted and him would be used as a training exam-ple to correct unwanted use of a preposition as in Itrusted *to him.3.1 Feature TypesIn all the experiments and system configurationsdescribed below, we used a similar set of featuresbased on the following feature templates.For determiner errors:?
Noun lemma: lemma of the noun that gov-erns the determiner?
Noun PoS: PoS tag of the noun?
Distance from Noun: distance in num-ber of words to the governed determiner?
Head lemma: head lemma in the shortestgrammatical relation in which the noun is de-pendent?
Head PoS: as defined above, but with PoS tagrather than lemma?
Distance from Head: distance in num-ber of words to the determiner from head, asdefined above (for Head lemma)?
GR type to Noun: a GR between Headand Noun.For instance for the example shown in Figure 1, thenoun lemma is student, the noun PoS is NN1, thedistance from the noun is 2, the head lemma is be,the head PoS is VBDZ, and the distance from thehead is 1, while the GR type to the noun is xcomp.For preposition errors:?
Preposition (P): target preposition?
Head lemma (H): head lemma of the GR inwhich the preposition is dependent?
Dependent lemma (D): dependentlemma of the GR in which the preposition ishead.For instance, in I am looking forward to your reply,P is to, H is look and D is reply.In contrast to work by Rozovskaya and Roth,amongst others, we have not used word context fea-tures, but instead focused on grammatical context in-formation for detecting and correcting errors.
Wealso experimented with some other feature types,such as n-grams consisting of the head, prepositionand dependent lemmas, but these did not improveperformance on the cross-validated HOO trainingdata, perhaps because they are sparser and the train-ing set is small.
However, there are many other po-tential feature types, such as PoS n-grams or syn-tactic rule types, and so forth that we don?t explorehere, despite their probable utility.
Our main focusin these experiments is not on optimal feature engi-neering but rather on the issues of classifier adaptionto errors and high precision error correction.4 A Simple High Precision CorrectionSystemWe have experimented with a number of approachesto maximizing precision and have not outperformeda simple model that doesn?t generalize from thetraining data using machine learning techniques.
Weleverage the large amount of error-corrected text inthe full CLC to learn reliable contexts in which er-rors occur and their associated corrections.
For theHOO shared task, we tested variants of this approachfor missing determiner (MD) and incorrect prepo-sition (RT) errors.
Better performing features andthresholds used to define contexts were found bytesting variants on the HOO training data.
The fea-ture types from section 3.1 deployed for the MDsystem submitted for the official run were Noun245lemma, Noun PoS, GR types to Noun andGR types from Noun (set of GRs which hasthe noun as head).
For the RT system, all three P, H,and D features were used to define contexts.
A con-text is considered reliable if it occurs at least twicein the full CLC and more than 75% of the time itoccurs with an error.The performance of this system on the trainingdata was very similar to performance on the test data(in contrast to our other runs).
We also explored L1-specific and L1-group variants of these systems; forinstance, we split the CLC data into Asian and Eu-ropean languages, trained separate systems on each,and then applied them according to the L1 meta-data supplied with the HOO training data.
However,all these systems performed worse than the best un-adapted system.The results for the generic, unadapted MD and RTsystems appear as run 0 in Tables 4?9 below.
Thesefigures are artefactually low as we don?t attempt todetect or correct UD, UT, RD or MT errors.
Theactual results computed from the official runs solelyfor MD errors are for detection, recognition and cor-rection: 83.33 precision and 7.63 recall, which givesan F-measure of 13.99; the RT system performed at66.67 precision, 8.05 recall and 14.37 F-measure onthe detection, recognition and correction tasks.
De-spite the low recall, this was our best submitted sys-tem in terms of official correction F-score.5 Na?
?ve Bayes (NB) (Un)AdaptedMulticlass ClassifiersRozovskaya and Roth (2011) demonstrate on adifferent dataset that Na?
?ve Bayes (NB) can out-perform discriminative classifiers on prepositionerror detection and correction if the prior is adaptedto L1-specific estimates of error-correction pairs.They compare the performance of an unadaptedNB multiclass classifier, in which the prior for apreposition is defined as the relative probabilityof seeing a specific preposition compared to apredefined subset of the overall PoS class (whichthey call the Conf(usion) Set):prior(p) =C(p)?q?ConfSet C(q),to the performance of the same NB classfier withan adapted prior which calculates the probability ofa correct preposition as:prior(c, p,L1) =CL1(p, c)CL1(p),where CL1(p) is the number of times prepositionp is seen in texts written by learners with L1 astheir native language, and CL1(p, c) is the numberof times c is the correct preposition when p is used.We applied Rozovskaya and Roth?s approach todeterminers as well as prepositions, and experi-mented with priors calculated in the same way forL1 groups as well as specific L1s.
We also com-pared L1-adaptation to generic adaption to correc-tions, calculated as:prior(c, p) =C(p, c)C(p),We have limited the set of determiners and prepo-sitions that our classifiers aim to detect and correct,if necessary.
Our confusions sets contain:?
Determiners: no determiner, the, a, an;?
Prepositions: no preposition, in, of, for,to, at, with, on, about, from, by, after.Therefore, for determiners, our systems were onlyaimed at detecting and correcting errors in the use ofarticles, and we have not taken into account any er-rors in the use of possessive pronouns (my, our, etc.
),demonstratives (this, those, etc.
), and other types ofdeterminers (any, some, etc.).
For prepositions, it iswell known that a set of about 10 of the most fre-quent prepositions account for more than 80% of allprepositional usage (Gamon, 2010).We have calculated the upper bounds for the train-ing and test sets when the determiner and preposi-tion confusion sets are limited this way.
The upperbound recall for recognition (i.e., ability of the clas-sifier to recognize that there is an error, dependent onthe fact that only the chosen determiners and prepo-sitions are considered) is calculated as the propor-tion of cases where the incorrect, missing or unnec-essary determiner or preposition is contained in ourconfusion set.
For the training set, it is estimated at91.95, and for the test at 93.20.
Since for correction,246the determiner or preposition suggested by the sys-tem should also be contained in our confusion set,upper bound recall for correction is slightly lowerthan that for recognition, and is estimated at 86.24for the training set, and at 86.39 for the test set.These figures show that the chosen candidates dis-tribute similarly in both datasets, and that a systemaimed at recognition and correction of only thesefunction words can obtain good performance on thefull task.The 1000 training scripts were divided into 5 por-tions pseudo-randomly to ensure that each portioncontained approximately the same number of L1-specific scripts in order not to introduce any L1-related bias.
The results on the training set pre-sented below were averaged across 5 runs, where ineach run 4 portions (about 800 scripts) were usedfor training, and one portion (about 200 scripts) wasused for testing.We treated the task as multi-class classification,where the number of classes equates to the size ofour confusion set, and when the classifier?s decisionis different from the input, it is considered to be er-rorful.
For determiners, we used the full set of fea-tures described in section 3.1, whereas for preposi-tions, we have tried two different feature sets: onlyhead lemma (H), or H with the dependent lemma (D).We ran the unadapted and L1-adapted NB classi-fiers on determiners and prepositions using the fea-tures defined above.
The results of these preliminaryexperiments are presented below.5.1 Unadapted and L1-adapted NB classifiersTables 1 to 3 below present results averaged overthe 5 runs for the unadapted classifiers.
We reportthe results in terms of recall, precision and F-scorefor detection, recognition and correction of errors asdefined for the HOO shared task.5We have experimented with two types of L1-specific classification: classifier1 below is acombination of 16 separate multiclass NB classi-fiers, each trained on a specific L1 and applied tothe corresponding parts of the data.
Classifier2is a replication of the classifier presented in Ro-zovskaya and Roth (2011), which uses the priors5For precise definitions of these measures seewww.correcttext.org/hoo2012adapted to the writer?s L1 and to the chosen deter-miner or preposition at decision time.
The priorsused for these runs were estimated from the HOOtraining data.We present only the results of the systems that useH+D features for prepositions, since these systemsoutperform systems using H only.
Tables 1, 2 and3 below show the comparative results of the threeclassifiers averaged over 5 runs, with all errors, de-terminer errors only, and preposition errors only, re-spectively.Detection Recognition CorrectionR P F R P F R P FU 60.69 21.32 31.55 50.57 17.73 26.25 34.38 12.05 17.85C1 64.51 16.17 25.85 50.25 12.56 20.10 30.95 7.74 12.39C2 33.74 16.51 22.15 28.50 13.96 18.72 16.51 8.10 10.85Table 1: All errors included.
Unadapted classifier (U) vs.two L1-adapted classifiers (C1 and C2).
Results on thetraining set.Detection Recognition CorrectionR P F R P F R P FU 54.42 33.25 41.25 50.09 30.60 30.83 40.70 24.84 30.83C1 61.19 20.25 30.42 52.20 17.27 25.94 40.57 13.43 20.17C2 40.56 15.88 22.81 37.24 14.58 20.94 23.20 9.08 13.04Table 2: Determiner errors.
Unadapted classifier (U) vs.two L1-adapted classifiers (C1 and C2).
Results on thetraining set.Detection Recognition CorrectionR P F R P F R P FU 65.71 16.89 26.87 50.90 13.09 20.83 28.95 7.45 11.84C1 66.96 13.86 22.97 48.51 10.05 16.65 22.70 4.70 7.79C2 27.45 17.06 21.00 21.00 13.07 16.09 10.79 6.73 8.27Table 3: Preposition errors.
Unadapted classifier (U) vs.two L1-adapted classifiers (C1 and C2).
Results on thetraining set.The results show some improvement with a com-bination of classifiers trained on L1-subsets in termsof recall for detection and recognition of errors, anda slight improvement in precision using L1-specificpriors for preposition errors.
However, in general,unadapted classifiers outperform L1-adapted classi-fiers with identical feature types.
Therefore, we havenot included L1-specific classifiers in the submittedset of runs.5.2 Submitted systemsFor the official runs, we trained various versions ofthe unadapted and generic adapted NB classifiers.247We trained all the adapted priors on the full CLCdataset in the expectation that this would yield moreaccurate estimates.
We trained the unadapted priorsand the NB features as before on the HOO trainingdataset.
We also trained the NB features on the fullCLC dataset and tested the impact of the preposi-tion feature D (dependent lemma of the GR from thepreposition, i.e., the head of the preposition comple-ment) with the different training set sizes.
For allruns we used the full set of determiner features de-scribed in section 3.1.The full set of multiclass NB classifiers submittedis described below:?
Run1: unadapted, trained on the HOO data.
Hfeature for prepositions;?
Run2: unadapted, trained on the HOO data.
Hand D features for prepositions;?
Run3: a combination of the NB classifierstrained for each of the used candidate wordsseparately.
H and D features are used for prepo-sitions;?
Run4: generic adapted, trained on HOO data.H feature for prepositions;?
Run5: generic adapted, trained on HOO data.H and D features for prepositions;?
Run6: unadapted, trained on the full CLC.
Hfeature for prepositions;?
Run7: unadapted, trained on the full CLC.
Hand D features for prepositions.The classifiers used for runs 1 and 2 differ fromthe ones used for runs 6 and 7 only in the amountof training data.
None of these classifiers involveany adaptation.
The classifiers used for runs 4 and5 involve prior adaptation to the input determineror preposition, adjusted at decision time.
In run3, a combination of classifiers trained on the inputdeterminer- or preposition-specific partitions of theHOO training data are used.
At test time, the appro-priate classifier from this set is applied depending onthe preposition or determiner chosen by the learner.To limit the number of classes for the classifiersused in runs 1?3 and 6?7, we have combined thetraining cases for determiners a and an in one classa/an; after classification one of the variants is chosendepending on the first letter of the next word.
How-ever, for the classifiers used in runs 4?5, we usedpriors including confusions between a and an.The results for these runs on the training data areshown in Tables 4 to 6 below.Detection Recognition CorrectionR P F R P F R P F0 5.54 81.08 10.37 5.32 77.95 9.97 4.90 71.70 9.171 60.14 18.57 28.37 48.21 14.88 22.74 32.71 10.09 15.432 60.69 21.32 31.55 50.57 17.73 26.25 34.38 12.05 17.853 50.09 27.54 35.52 45.99 25.23 32.57 28.78 15.80 20.394 25.39 25.48 25.39 22.10 22.23 22.13 12.23 12.33 12.265 31.17 22.33 25.94 26.28 18.88 21.90 14.50 10.46 12.116 62.41 10.73 18.31 49.95 8.57 14.63 32.66 5.60 9.577 62.92 11.60 19.59 52.29 9.61 16.24 34.32 6.31 10.66Table 4: Training set results, all errorsDetection Recognition CorrectionR P F R P F R P F0 5.02 82.98 9.46 5.02 82.98 9.46 4.81 79.57 9.071?2 54.42 33.25 41.25 50.09 30.60 30.83 40.70 24.84 30.833 58.50 62.22 60.22 57.41 61.07 59.11 46.33 49.25 47.684?5 34.93 31.09 32.68 33.66 30.01 31.52 19.74 17.66 18.516?7 58.65 8.11 14.24 53.90 7.43 13.06 40.61 5.60 9.84Table 5: Training set results, determiner errorsDetection Recognition CorrectionR P F R P F R P F0 5.87 78.30 10.93 5.59 74.49 10.40 4.97 66.28 9.251 64.71 14.04 23.06 46.54 10.11 16.61 25.86 5.61 9.222 65.71 16.89 26.87 50.90 13.09 20.83 28.95 7.45 11.843 42.63 16.53 23.81 36.18 14.04 20.22 13.74 5.35 7.704 16.85 19.27 17.97 12.24 14.03 13.06 5.81 6.67 6.215 27.49 16.89 20.88 19.96 12.30 15.19 10.03 6.20 7.656 64.69 14.03 23.06 46.51 10.10 16.60 25.83 5.61 9.227 65.68 16.89 26.87 50.87 13.09 20.82 28.92 7.44 11.84Table 6: Training set results, preposition errorsThe results on the training data show that useof the D feature improves the performance of allthe preposition classifiers.
Use of the full CLC fortraining improves recall, but does not improve pre-cision for prepositions, while for determiners pre-cision of the classifiers trained on the full CLCis much worse.
Adaptation of the classifiers withdeterminer/preposition-specific priors slightly im-proves precision on prepositions, but is damagingfor recall.
Therefore, in terms of F-score, unadaptedclassifiers outperform adapted ones.
The over-all best-performing system on the cross-validatedtraining data is Run3, which is trained on thedeterminer/preposition-specific data subsets and ap-248plies an input-specific classifier to test data.
How-ever, the result is due to improved performance ondeterminers, not prepositions.6 Official Evaluation ResultsThe results presented below are calculated using theevaluation tool provided by the organizers, imple-menting the scheme specified in the HOO sharedtask.
The results on the test set, presented in Ta-bles 7?9 are from the final official run after correc-tion of errors in the annotation and score calculationscripts.Detection Recognition CorrectionR P F R P F R P F0 4.86 76.67 9.15 4.65 73.33 8.75 4.65 73.33 8.751 34.46 13.04 18.92 22.83 8.64 12.54 13.53 5.12 7.432 35.73 14.04 20.16 23.47 9.22 13.24 12.26 4.82 6.923 19.24 12.10 14.86 14.59 9.18 11.27 5.71 3.59 4.414 9.51 14.95 11.63 7.19 11.30 8.79 5.29 8.31 6.465 15.43 14.31 14.85 10.78 10.00 10.38 6.77 6.28 6.516 55.60 11.15 18.58 41.86 8.40 13.99 28.54 5.73 9.547 56.66 11.59 19.24 42.49 8.69 14.43 27.27 5.58 9.26Table 7: Test set results, all errorsDetection Recognition CorrectionR P F R P F R P F0 4.37 83.33 8.30 4.37 83.33 8.30 4.37 83.33 8.301?2 8.73 7.61 8.13 4.80 4.18 4.47 4.37 3.80 4.073 6.11 11.29 7.93 5.24 9.68 6.80 5.24 9.68 6.804?5 6.11 9.72 7.51 4.80 7.64 5.90 4.80 7.64 5.906?7 51.09 8.53 14.63 44.10 7.37 12.63 35.37 5.91 10.13Table 8: Test set results, determiner errorsDetection Recognition CorrectionR P F R P F R P F0 5.33 72.22 9.92 4.92 66.67 9.16 4.92 66.67 9.161 57.79 14.29 22.91 39.75 9.83 15.76 22.13 5.47 8.772 59.43 15.41 24.47 40.98 10.63 16.88 19.67 5.10 8.103 29.10 11.31 16.28 23.36 9.08 13.07 6.15 2.39 3.444 12.71 19.75 15.46 9.43 14.65 11.47 5.74 8.92 6.985 24.18 16.12 19.34 16.39 10.93 13.12 8.61 5.74 6.896 57.79 14.29 22.91 39.75 9.83 15.76 22.13 5.47 8.777 59.43 15.41 24.47 40.98 10.63 16.88 19.67 5.10 8.10Table 9: Test set results, preposition errorsThe test set results for NB classifiers (Runs 1?7) are significantly worse than our preliminary re-sults obtained on the training data partitions, espe-cially for determiners.
Use of additional trainingdata (Runs 6 and 7) improves recall, but does not im-prove precision.
Adaptation to the input prepositionimproves precision as compared to the unadaptedclassifier for prepositions (Run 4), whereas trainingon the determiner-specific subsets improves preci-sion for determiners (Run 3).
However, generallythese results are worse than the results of the similarclassifiers on the training data subsets.We calculated the upper bound recall for our clas-sifiers on the test data.
The upper bound recall onthe test data is 93.20 for recognition, and 86.39 forcorrection, given our confusion sets for both deter-miners and prepositions.
However, the actual upperbound recall is 71.82, with upper bound recall ondeterminers at 71.74 and on prepositions at 71.90,because 65 out of 230 determiner errors, and 68 outof 243 preposition errors are not considered by ourclassifiers, primarily because when the parser fails tofind a full analysis, the grammatical context is oftennot recovered accurately enough to identify missinginput positions or relevant GRs.
This is an inher-ent weakness of using only parser-extracted featuresfrom noisy and often ungrammatical input.
Takingthis into account, some models (Runs 1, 2, 6 and 7)achieved quite high recall.We suspect the considerable drop in precision isexplained by the differences in the training and testdata.
The training set contains answers from learnersof a smaller group of L1s from one examination yearto a much more restricted set of prompts.
The well-known weaknesses of generative NB classifiers mayprevent effective exploitation of the additional infor-mation in the full CLC over the HOO training data.Experimentation with count weighting schemes andoptimized interpolation of adapted priors may wellbe beneficial (Rennie et al, 2003).AcknowledgementsWe thank Cambridge ESOL, a division of Cam-bridge Assessment for a partial grant to the first au-thor and a research contract with iLexIR Ltd. Wealso thank them and Cambridge University Press forgranting us access to the CLC for research purposes.References?istein Andersen.
2011 Semi-automatic ESOL errorannotation.
English Profile Journal, vol2:e1.
DOI:10.1017/S2041536211000018, Cambridge UniversityPress.
?istein Andersen, Julian Nioche, Ted Briscoe, andJohn Carroll.
2008 The BNC parsed with249RASP4UIMA.
In 6th Int.
Conf.
on Language Re-sources and Evaluation (LREC), Marrakech, Moroc-ccoTed Briscoe, John A. Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Proceed-ings of COLING/ACL, vol 6.Robert Dale, Ilya Anisimoff and George Narroway 2012HOO 2012: A Report on the Preposition and Deter-miner Error Correction Shared Task.
In Proceedingsof the Seventh Workshop on Innovative Use of NLP forBuilding Educational Applications Montreal, Canada,June.Rachele De Felice and Stephen G. Pulman.
2008.A classifier-based approach to preposition and deter-miner error correction in L2 English.
In Proceed-ings of the 22nd International Conference on Compu-tational Linguistics (COLING 2008), pages 169?176,Manchester, UK, -August.Jens Eeg-Olofsson and Ola Knutsson.
2003.
Automaticgrammar checking for second language learners - theuse of prepositions.
In Nodalida.Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-der Klementiev, William Dolan, Dmitriy Belenko, andLucy Vanderwende.
2008.
Using contextual spellertechniques and language modeling for ESL error cor-rection.
In Proceedings of IJCNLP, Hyderabad, India,January.Michael Gamon.
2010.
Using Mostly Native Data toCorrect Errors in Learners?
Writing: A Meta-ClassifierApproach.
In Proceedings of NAACL 2010, pages163?171, Los Angeles, USA, June.Na-Rae Han, Martin Chodorow, and Claudia Leacock.2006.
Detecting errors in English article usage by non-native speakers.
Journal of Natural Language Engi-neering, 12(2):115?129.Na-Rae Han, Joel R. Tetreault, Soo-Hwa Lee, and Jin-Young Ha.
2010.
Using an Error-Annotated LearnerCorpus to Develop an ESL/EFL Error CorrectionSystem.
In Proceedings of the Seventh conferenceon International Language Resources and Evaluation(LREC-10), Valletta, Malta, May.Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thep-chai Supnithi, and Hitoshi Isahara.
2003.
Automaticerror detection in the Japanese learners?
English spo-ken data.
In The Companion Volume to the Proceed-ings of 41st Annual Meeting of the Association forComputational Linguistics, pages 145?148, Sapporo,Japan, July.Ekaterina Kochmar.
2011.
Identification of a Writer?sNative Language by Error Analysis University ofCambridge, MPhil Dissertation.Claudia Leacock, Martin Chodorow, Michael Gamon,and Joel Tetreault.
2010.
Automated GrammaticalError Detection for Language Learners.
Morgan &Claypool Publishers.John Lee and Stephanie Seneff.
2008.
An analysis ofgrammatical errors in non-native speech in English.
InProceedings of the 2008 Spoken Language TechnologyWorkshop.Ryo Nagata and Kazuhide Nakatani.
2010 Evaluatingperformance of grammatical error detection to max-imize learning effect.
In Proceedings of Int.
Conf.on Computational Linguistics (Coling-10), Poster Ses-sion, pages 894?900, Beijing, China.Diane Nicholls.
2003.
The Cambridge Learner Corpus:Error coding and analysis for lexicography and ELT.In Proceedings of the Corpus Linguistics conference,pages 572?581.Jason Rennie, Lawrence Shih, Jaime Teevan, andDavid Karger.
2003 Tackling the Poor Assumtionsof Naive Bayes Text Classifiers.
20th Int.
Conferenceon Machine Learning (ICML-2003) Washington, DCAlla Rozovskaya and Dan Roth.
2010a.
Annotating ESLErrors: Challenges and Rewards.
In Proceedings ofthe NAACL Workshop on Innovative Use of NLP forBuilding Educational Applications.Alla Rozovskaya and Dan Roth.
2010b.
Generating Con-fusion Sets for Context-Sensitive Error Correction.
Inof the Conference on Empirical Methods in NaturalLanguage Processing (EMNLP).Alla Rozovskaya and Dan Roth.
2010c.
TrainingParadigms for Correcting Errors in Grammar and Us-age.
In Proceedings of the Annual Meeting of theNorth American Association of Computational Lin-guistics (NAACL).Alla Rozovskaya and Dan Roth.
2011.
Algorithm Selec-tion and Model Adaptation for ESL Correction Tasks.In Proceedings of the Annual Meeting of the Associa-tion of Computational Linguistics (ACL).Joel R. Tetreault and Martin Chodorow.
2008.
The upsand downs of preposition error detection in ESL writ-ing.
In Proceedings of the 22nd International Con-ference on Computational Linguistics (Coling 2008),pages 865?872, Manchester, UK, August.Joel R. Tetreault, Jennifer Foster, Martin Chodorow.2010.
Using Parse Features for Preposition Selectionand Error Detection.
In ACL.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A New Dataset and Method for AutomaticallyGrading ESOL Texts.
In The 49th Annual Meetingof the Association for Computational Linguistics: Hu-man Language Technologies.250
