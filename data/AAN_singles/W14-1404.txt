Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 28?36,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsThe Phenogrammar of CoordinationChris WorthDepartment of LinguisticsThe Ohio State Universityworth@ling.osu.eduAbstractLinear Categorial Grammar (LinCG) is asign-based, Curryesque, relational, logi-cal categorial grammar (CG) whose cen-tral architecture is based on linear logic.Curryesque grammars separate the ab-stract combinatorics (tectogrammar) oflinguistic expressions from their concrete,audible representations (phenogrammar).Most of these grammars encode linear or-der in string-based lambda terms, in whichthere is no obvious way to distinguish rightfrom left.
Without some notion of direc-tionality, grammars are unable to differen-tiate, say, subject and object for purposesof building functorial coordinate struc-tures.
We introduce the notion of a phe-nominator as a way to encode the termstructure of a functor separately from its?string support?.
This technology is thenemployed to analyze a range of coordina-tion phenomena typically left unaddressedby Linear Logic-based Curryesque frame-works.1 OverviewFlexibility to the notion of constituency in con-junction with introduction (and composition) ruleshas allowed categorial grammars to successfullyaddress an entire host of coordination phenomenain a transparent and compositional manner.
While?Curryesque?
CGs as a rule do not suffer fromsome of the other difficulties that plague LambekCGs, many are notably deficient in one area: co-ordination.
Lest we throw the baby out with thebathwater, this is an issue that needs to be ad-dressed.
We take the following to be an exemplarysubset of the relevant data, and adopt a fragmentmethodology to show how it may be analyzed.
(1) Tyrion and Joffrey drank.
(2) Joffrey whined and sniveled.
(3) Tyrion slapped and Tywin chastised Jof-frey.The first example is a straightforward instanceof noun phrase coordination.
The second and thirdare both instances of what has become known inthe categorial grammar literature as ?functor co-ordination?, that is, the coordination of linguisticmaterial that is in some way incomplete.
The thirdis particularly noteworthy as being an example ofa ?right node raising?
construction, whereby theargument Joffrey serves as the object to both ofthe higher NP-Verb complexes.
We will show thatall three examples can be given an uncomplicatedaccount in the Curryesque framework of LinearCategorial Grammar (LinCG), and that (2) and(3) have more in common than not.Section 1 provides an overview of the data andand central issues surrounding an analysis of co-ordination in Curryesque grammars.
Section 2 in-troduces the reader to the framework of LinCG,and presents the technical innovations at the heartof this paper.
Section 3 gives lexical entries andderivations for the examples in section 1, and sec-tion 4 discusses our results and suggests some di-rections for research in the near future, with refer-ences following.1.1 Curryesque grammars and LinearCategorial GrammarWe take as our starting point the Curryesque (af-ter Curry (1961)) tradition of categorial grammars,making particular reference to those originatingwith Oehrle (1994) and continuing with AbstractCategorial Grammar (ACG) of de Groote (2001),Muskens (2010)?s Lambda Grammar (?G), Kub-ota and Levine?s Hybrid Type-Logical Catego-rial Grammar (Kubota and Levine, 2012) andto a lesser extent the Grammatical Frameworkof Ranta (2004), and others.
These dialects28of categorial grammar make a distinction be-tween Tectogrammar, or ?abstract syntax?, andPhenogrammar, or ?concrete syntax?.
Tec-togrammar is primarily concerned with the struc-tural properties of grammar, among them co-occurrence, case, agreement, tense, and so forth.Phenogrammar is concerned with computing apre-phonological representation of what will even-tually be produced by the speaker, and encom-passes word order, morphology, prosody, and thelike.Linear Categorial Grammar (LinCG) is a sign-based, Curryesque, relational, logical categorialgrammar whose central architecture is based onlinear logic.
Abbreviatory overlap has beena regrettably persistent problem, and LinCG isthe same in essence as the framework varyinglycalled Linear Grammar (LG) and Pheno-Tecto-Differentiated Categorial Grammar (PTDCG), anddeveloped in Smith (2010), Mihalicek (2012),Martin (2013), Pollard and Smith (2012), and Pol-lard (2013).
In LinCG, the syntax-phonology andsyntax-semantics interfaces amount to noting thatthe logics for the phenogrammar, the tectogram-mar, and the semantics operate in parallel.
Thisstands in contrast to ?syntactocentric?
theories ofgrammar, where syntax is taken to be the fun-damental domain within which expressions com-bine, and then phonology and semantics are ?readoff?
of the syntactic representation.
LinCG is con-ceptually different in that it has relational, ratherthan functional, interfaces between the three com-ponents of the grammar.
Since we do not interpretsyntactic types into phenogrammatical or semantictypes, this allows us a great deal of freedom withineach logic, although in practice we maintain afairly tight connection between all three compo-nents.
Grammar rules take the form of derivationalrules which generate triples called signs, and theybind together the three logics so that they operateconcurrently.
While the invocation of a grammarrule might simply be, say, point-wise application,the ramifications for the three systems can in prin-ciple be different; one can imagine expressionswhich exhibit type asymmetry in various ways.By way of example, one might think of ?focus?as an operation which has reflexes in all three as-pects of the grammar: it applies pitch accents tothe target string(s) in the phenogrammar (the dif-ference between accented and unaccented wordsbeing reflected in the phenotype), it creates ?low-ering?
operators in the tectogrammar (that is, ex-pressions which scope within a continuation), andit ?focuses?
a particular meaningful unit in the se-mantics.
A focused expression might share its tec-totype ((NP ( S) ( S) with, say, a quantifiednoun phrase, but the two could have different phe-notypes, reflecting the accentuation or lack thereofby placing the resulting expression in the domainof prosodic boundary phenomena or not.
Never-theless, the system is constrained by the fact thatthe tectogrammar is based on linear logic, so if wetake some care when writing grammar rules, weshould still find resource sensitivity to be at theheart of the framework.1.2 Why coordination is difficult forCurryesque grammarsMost Curryesque CGs encode linear order inlambda terms, and there is no obvious way to dis-tinguish ?right?
from ?left?
by examining the types(be they linear or intuitionistic).1This is not aproblem when we are coordinating strings directly,as de Groote and Maarek (2007) show, but an anal-ysis of the more difficult case of functor coordi-nation remains elusive.2Without some notion ofdirectionality, grammars are unable to distinguishbetween, say, subject and object.
This would seemto predict, for example, that ?s.
s ?
SLAPPED ?JOFFREY and ?s.
TYRION ?
SLAPPED ?
s wouldhave the same syntactic category (NP ( S in thetectogrammar, and St ?
St in the phenogram-mar), and would thus be compatible under coor-dination, but this is generally not the case.
Whatwe need is a way to examine the structure of alambda term independently of the specific stringconstants that comprise it.
To put it another way,in order to coordinate functors, we need to be ableto distinguish between what Oehrle (1995) callstheir string support, that is, the string constantswhich make up the body of a particular functionalterm, and the linearization structure such functorsimpose on their arguments.2 Linear Categorial Grammar (LinCG)Curryesque grammars separate the notion of linearorder from the abstract combinatorics of linguis-1A noteworthy exception is Ranta?s Grammatical Frame-work (GF), explored in, e.g.
Ranta (2004) and Ranta(2009).
GF also makes distinctions between tectogrammarand phenogrammar, though it has a somewhat different con-ception of each.2A problem explicitly recognized by Kubota (2010) insection 3.2.1.29tic expressions, and as such base their tectogram-mars around logics other than bilinear logic; theGrammatical Framework is based on Martin-L?oftype theory, and LinCG and its cousins ACG and?G use linear logic.
Linear logic is generally de-scribed as being ?resource-sensitive?, owing to thelack of the structural rules of weakening and con-traction.
Resource sensitivity is an attractive no-tion, theoretically, since it allows us to describeprocesses of resource production, consumption,and combination in a manner which is agnosticabout precisely how resources are combined.
Cer-tain problems which have been historically trickyfor Lambek categorial grammars (medial extrac-tion, quantifier scope, etc.)
are easily handled byLinCG.Since a full introduction to the framework is re-grettably impossible given current constraints, werefer the interested reader to the references in sec-tion 1.1, which contain a more in-depth discus-sion of the potential richness of the architectureof LinCG.
We do not wish to say anything newabout the semantics or the tectogrammar of coor-dination in the current discussion, so we will ex-pend our time fleshing out the phenogrammaticalcomponent of the framework, and it is to this topicthat we now turn.2.1 LinCG PhenogrammarLinCG grammar rules take the form of tripartiteinference rules, indicating what operations takeplace pointwise within each component of thesigns in question.
There are two main gram-mar rules, called application (App) for combiningsigns, and abstraction (Abs) for creating the po-tential for combination through hypothetical rea-soning.
Aside from the lexical entries given asaxioms of the theory, it is also possible to obtaintyped variables using the rule of axiom (Ax), andwe make use of this rule in the analysis of rightnode raising found in section 3.4.
While the tec-togrammar of LinCG is based on a fragment oflinear logic, the phenogrammatical and semanticcomponents are based on higher order logic.
Sincewe are concerned only with the phenogrammaticalcomponent here, we have chosen to simplify theexposition by presenting only the phenogrammat-ical part of the rules of application and abstraction:Axf : A ` f : A?
` f : A?
B ?
` a : AApp?,?
` (f a) : B?, x : A ` b : BAbs?
` ?x : A. b : A?
BWe additionally stipulate the following familiaraxioms governing the conversion and reduction oflambda terms:3` ?x : A. b = ?y : A.
[y/x]b (?-conversion)` (?x.
b a) = [a/x]b (?-reduction)As is common to any number of Curryesqueframeworks, we encode the phenogrammaticalparts of LinCG signs with typed lambda termsconsisting of strings, and functions over strings.4We axiomatize our theory of strings in the familiarway:`  : St` ?
: St?
St?
St` ?stu : St. s ?
(t ?
u) = (s ?
t) ?
u` ?s : St.  ?
s = s = s ?
The first axiom asserts that the empty string  isa string.
The second axiom asserts that concate-nation, written ?, is a (curried) binary function onstrings.
The third axiom represents the fact thatconcatenation is associative, and the fourth, thatthe empty string is a two-sided identity for con-catenation.
Because of the associativity of con-catenation, we will drop parentheses as a matterof convention.The phenogrammar of a typical LinCG sign willresemble the following (with one complication tobe added shortly):` ?s.
s ?
SNIVELED : St?
StSince we treat St as the only base type, we willgenerally omit typing judgments in lambda termswhen no confusion will result.
Furthermore, weuse SMALL CAPS to indicate that a particular con-stant is a string.
So, the preceding lexical entryprovides us with a function from some string s, tostrings, which concatenates the string SNIVELEDto the right of s.2.1.1 PhenominatorsThe center of our analysis of coordination isthe notion of a phenominator (short for pheno-combinator), a particular variety of typed lambdaterm.
Intuitively, phenominators serve the samepurpose for LinCG that bilinear (slash) types dofor Lambek categorial grammars.
Specifically,3The exact status of the rule of ?-conversion with respectto this framework is currently unclear, and since we do notmake use of it, we omit its discussion here.4although other structures have been proposed, e.g.
thenode sets found in Muskens (2001).30they encode the linearization structure of a func-tor, that is, where arguments may eventually oc-cur with respect to its string support.
To put it an-other way, a phenominator describes the structurea functor ?projects?, in terms of linear order.From a technical standpoint, we would like todefine a phenominator as a closed monoidal linearlambda term, i.e.
a term containing no constantsother than concatenation and the empty string.The idea is that phenominators are the terms ofthe higher order theory of monoids, and they insome ways describe the abstract ?shape?
of pos-sible string functions.
For those accustomed tothinking of ?syntax?
as being word order, thenphenominators can be thought of as a kind of syn-tactic combinator.
In practice, we will make useonly of what we call the unary phenominators, thetypes of which we will refer to using the sort ?
(with ?
used by custom as a metavariable overunary phenominators, i.e.
terms whose type is in?).
These are not unary in the strict sense, but theywill have as their centerpiece one particular stringvariable, which will be bound with the highestscope.
We will generally abbreviate phenomina-tors by the construction with which they are mostcommonly associated: VP for verb phrases andintransitive verbs, TV for transitive verbs, DTVfor ditransitive verbs, QNP for quantified nounphrases, and RNR for right node raising construc-tions.
Here are examples of some of the most com-mon phenominators we will make use of and theabbreviations we customarily use for them:Phenominator Abbreviation?s.s (omitted)?vs.s ?
v VP?vst.t ?
v ?
s TV?vstu.u ?
v ?
s ?
t DTV?vP.
(P v) QNP?vs.v ?
s RNRAs indicated previously, the first argument of aphenominator always corresponds to what we re-fer to (after Oehrle (1995)) as the string supportof a particular term.
With the first argument dis-pensed with, we have chosen the argument orderof the phenominators out of general concern forwhat we perceive to be fairly uncontroversial cat-egorial analyses of English grammatical phenom-ena.
That is, transitive verbs take their object ar-guments first, and then their subject arguments, di-transitives take their first and second object argu-ments, followed by their subject argument, etc.
Aslong as the arguments in question are immediatelyadjacent to the string support at each successiveapplication, it is possible to permute them to someextent without losing the general thrust of the anal-ysis.
For example, the choice to have transitiveverbs take their object arguments first is insignifi-cant.5Since strings are implicitly under the imageof the identity phenominator ?s.s, we will consis-tently omit this subscript.We will be able to define a function we callsay, so that it will have the following property:` ?s : St.??
: ?.say (?
s) = sThat is, say is a left inverse for unary phenomi-nators.The function say is defined recursively via cer-tain objects we call vacuities.
The idea of a vacu-ity is that it be in some way an ?empty argument?to which a functional term may apply.
If we aredealing with functions taking string arguments, itseems obvious that the vacuity on strings shouldbe the empty string .
If we are dealing withsecond-order functions taking St?
St arguments,for example, quantified noun phrases like every-one, then the vacuity on St ?
St should be theidentity function on strings, ?s.s.
Higher vacuitiesthan these become more complicated, and defin-ing all of the higher-order vacuities is not entirelystraightforward, as certain types are not guaran-teed to have a unique vacuity.
Fortunately, we cando it for any higher-function taking as an argumentanother function under the image of a phenomina-tor ?
then the vacuity on such a function is just thephenominator applied to the empty string.6Thecentral idea is easily understood when one askswhat, say, a vacuous transitive verb sounds like.The answer seems to be: by itself, nothing, butit imposes a certain order on its arguments.
Onepractical application of this clause is in analyzingso-called ?argument cluster coordination?, wherethis definition will ensure that the argument clustergets linearized in the correct manner.
This analy-sis is regrettably just outside the scope of the cur-rent inquiry, though the notion of the phenomina-5Since we believe it is possible to embed Lambek catego-rial grammars in LinCG, this fact reflects that the calculus weare dealing with is similar to the associative Lambek Calcu-lus.6A reviewer suggests that this concept may be related tothe ?context passing representation?
of Hughes (1995), andthe association of a nil term with its continuation with re-spect to contexts is assuredly evocative of the association ofthe vacuity on a phenominator-indexed type with the contin-uation of  with respect to a phenominator.31tor can be profitably employed to provide exactlysuch an analysis by adopting and reinterpreting acategorial account along the lines of the one givenin Dowty (1988).We formally define vacuities as follows:vacSt?St=def?s.svac??=def(?
)The reader should note that as a special case of thesecond clause, we havevacSt= vacSt?s.s= (?s.s ) = This in turn enables us to define say:saySt=def?s.ssay?1?
?2=def?k : ?1?
?2.
say?2(k vac?1)say(?1??2)?=defsay?1?
?2For an expedient example, we can apply say toour putative lexical entry from earlier, and verifythat it will reduce to the string SNIVELED as de-sired:saySt?St?s.
s ?
SNIVELED= ?k : St?
St.(saySt(k vacSt)) ?s.
s ?
SNIVELED= saySt(?s.
s ?
SNIVELED vacSt)= saySt(?s.
s ?
SNIVELED )= saySt ?
SNIVELED= sayStSNIVELED= ?s.s SNIVELED= SNIVELED2.1.2 Subtyping by unary phenominatorsIn order to augment our type theory with therelevant subtypes, we turn to Lambek and Scott(1986), who hold that one way to do subtyping isby defining predicates that amount to the charac-teristic function of the particular subtype in ques-tion, and then ensuring that these predicates meetcertain axioms embedding the subtype into the su-pertype.
We will be able to write such predicatesusing phenominators.
A unary phenominator isone which has under its image a function whosestring support is a single contiguous string.
Withthis idea in place, we are able to assign subtypesto functional types in the following way.For ?
a (functional) type, we write ??
(with ?
aphenominator) as shorthand for ??
?, where:?
?= ?f : ?
.
?s : St.f = (?
s)Then ?
?constitutes a subtyping predicate in themanner of Lambek and Scott (1986).
For example,let ?
= St?
St and ?
= ?vs.s?v.
Let us considerthe following (putative) lexical entry (pheno only):` ?s?.
s??
SNIVELED : (St?
St)VPThen our typing is justified along the followinglines:??
::= (St?
St)VP::= (St?
St)?vs.s?v::= (St?
St)?f :St?St.
?t:St.f=(?vs.s?v t)So applying the subtyping predicate to the term inquestion, we have(?f : St?
St. ?t : St.f = (?vs.s ?
v t) ?s?.
s??
SNIVELED)= ?t : St.?s?.
s??
SNIVELED = (?vs.s ?
v t)= ?t : St.?s?.
s??
SNIVELED = ?s.
s ?
t= ?t : St.?s.
s ?
SNIVELED = ?s.
s ?
twhich is true with t = SNIVELED, and the term isshown to be well-typed.3 AnalysisThe basic strategy underlying our analysis of coor-dination is that in order to coordinate two linguis-tic signs, we need to track two things: their lin-earization structure, and their string support.
If wehave access to the linearization structure of eachconjunct, then we can check to see that it is thesame, and the signs are compatible for coordina-tion.
Furthermore, we will be able to maintain thisstructure independent of the actual string supportof the individual signs.Phenominators simultaneously allow us tocheck the linearization structure of coordinationcandidates and to reconstruct the relevant lin-earization functions after coordination has takenplace.
The function say addresses the secondpoint.
For a given sign, we can apply say to itin order to retrieve its string support.
Then, wewill be able to directly coordinate the resultingstrings by concatenating them with a conjunctionin between.
Finally, we can apply the phenomi-nator to the resulting string and retrieve the newlinearization function, containing the entire coor-dinate structure as its string support.3.1 Lexical entriesIn LinCG, lexical entries constitute the (nonlogi-cal) axioms of the proof theory.
First we considerthe simplest elements of our fragment, the phenosfor the proper names Joffrey, Tyrion, and Tywin:(4) a.
` JOFFREY : Stb.
` TYRION : Stc.
` TYWIN : StNext, we consider the intransitive verbs drank,sniveled and whined.
:32(5) a.
` ?s.
s ?
DRANK : (St?
St)VPb.
` ?s.
s ?
SNIVELED : (St?
St)VPc.
` ?s.
s ?
WHINED : (St?
St)VPEach of these is a function from strings to strings,seeking to linearize its ?subject?
string argument tothe left of the verb.
They are under the image ofthe ?verb phrase?
phenominator ?vs.s ?
v.The transitive verbs chastised and slapped seekto linearize their first string argument to the right,resulting in a function under the image of the VPphenominator, and their second argument to theleft, resulting in a string.
(6) a.
` ?st.
t ?
CHASTISED ?
s: (St?
St?
St)TVb.
` ?st.
t ?
SLAPPED ?
s: (St?
St?
St)TVTechnically, this type could be written (St ?
(St ?
St)VP)TV, but for the purposes of coordina-tion, the present is sufficient.
Each of these entriesis under the image of the ?transitive verb?
phe-nominator ?vst.t ?
v ?
s.Finally, we come to the lexical entry schema forand:(7) ` ?c1: ??.
?c2: ??.?
((say?
?c2) ?
AND ?
(say?
?c1)): ???
???
?
?We note first that it takes two arguments of iden-tical types ?
, and furthermore that these must beunder the image of the same phenominator ?.
Itthen returns an expression of the same subtype.7This mechanism bears more detailed examination.First, each conjunct is subjected to the functionsay, which, given its type, will return the stringsupport of the conjunct.
Then, the resulting stringsare concatenated to either side of the string AND.Finally, the phenominator of each argument is ap-plied to the resulting string, creating a functionidentical to the linearization functions of each ofthe conjuncts, except with the coordinated stringin the relevant position.3.2 String coordinationString coordination is direct and straightforward.Since string-typed terms are under the image of7Since ?
occurs within both the body of the term and thesubtyping predicate, we note that this effectively takes us intothe realm of dependent types.
Making the type theory of thephenogrammar precise is an ongoing area of research, and weare aware that constraining the type system is of paramountimportance for computational tractability.the identity phenominator, and since sayStis alsodefined to be the identity on strings, the lexical en-try we obtain for and simply concatenates eachargument string to either side of the string AND.We give the full term reduction here, although thisversion of and can be shown to be equal to the fol-lowing:` ?c1c2: St. c2?
AND ?
c1: St?
St?
StSince our terms at times become rather large,we will adopt a convention where proof trees aregiven with numerical indexes instead of sequents,with the corresponding sequents following below(at times on multiple lines).
We will from timeto time elide multiple steps of reduction, noting inpassing the relevant definitions to consider whenreconstructing the proof.61 23 4571.
` ?c1: St. ?c2: St.?s.s ((sayStc2) ?
AND ?
(sayStc1)): St?
St?
St2.
` JOFFREY : St3.
` ?c2: St.?s.s ((sayStc2) ?
AND ?
(sayStJOFFREY))= ?c2: St.?s.s ((sayStc2) ?
AND ?
(?s.s JOFFREY))= ?c2: St. ?s.s ((sayStc2)?AND?JOFFREY): St?
St4.
` TYRION : St5.
` ?s.s ((sayStTYRION) ?
AND ?
JOFFREY) :St= ?s.s ((?s.s TYRION)?AND?JOFFREY) : St= ?s.s (TYRION ?
AND ?
JOFFREY) : St= TYRION ?
AND ?
JOFFREY : St6.
` ?s.
s ?
DRANK : (St?
St)VP7.
` TYRION ?
AND ?
JOFFREY ?
DRANK : St3.3 Functor coordinationHere, in order to understand the term appearingin each conjunct, it is helpful to notice that thefollowing equality holds (with f a function fromstrings to strings, under the image of the VP phe-nominator):f : (St?
St)VP` say(St?St)VPf= saySt?Stf= saySt(f vacSt)= saySt(f )= ?s.s (f )= (f ) : St33This says that to coordinate VPs, we will first needto reduce them to their string support by feedingtheir linearization functions the empty string.
Forthe sake of brevity, this term reduction will beelided from steps 5 and 8 in the derivations be-low.
Steps 2 and 6 constitute the hypothesizingand subsequent withdrawal of an ?object?
stringargument t?, as do steps 10 and 14 (s?).
Format-ting restrictions prohibit rule-labeling on the prooftrees, so we note that these are each instances ofthe rules of axiom (Ax) and abstraction (Abs), re-spectively.1 23 45 671.
` ?c1: (St?
St)VP.
?c2: (St?
St)VP.
?vs.s ?
v((say(St?St)VPc2) ?
AND ?
(say(St?St)VPc1)): (St?
St)VP?
(St?
St)VP?
(St?
St)VP2.
` ?s.
s ?
SNIVELED : (St?
St)VP3.
` ?c2: (St?
St)VP.
?vs.s ?
v((say(St?St)VPc2) ?
AND?
(say(St?St)VP?s.
s ?
SNIVELED))...= ?c2: (St?
St)VP.
?vs.s ?
v((say(St?St)VPc2) ?
AND ?
SNIVELED): (St?
St)VP?
(St?
St)VP4.
` ?s.
s ?
WHINED : (St?
St)VP5.
` ?vs.s ?
v ((say(St?St)VP?s.
s ?
WHINED)?
AND ?
SNIVELED)...= (?vs.s ?
v WHINED ?
AND ?
SNIVELED)= ?s.
s ?
WHINED ?
AND ?
SNIVELED: (St?
St)VP6.
` JOFFREY : St7.
` JOFFREY ?
WHINED ?
AND ?
SNIVELED : St3.4 Right node raisingIn the end, ?right node raising?
constructions proveonly to be a special case of functor coordination.The key here is the licensing of the ?rightward-looking?
functors, which are under the image ofthe phenominator ?vs.v ?
s. As was the case withthe ?leftward-looking?
functor coordination exam-ple in section 3.3, this analysis is essentially thesame as the well-known Lambek categorial gram-mar analysis originating in Steedman (1985) andcontinuing in Dowty (1988) and Morrill (1994).The difference is that we encode directionality inthe phenominator, rather than in the type.
Sinceour system does not include function compositionas a rule, but as a theorem, we will need to makeuse of hypothetical reasoning in order to permutethe order of the string arguments in order to con-struct expressions with the correct structure.8As was the case with the functor coordina-tion example in section 3.3, applying say to theconjuncts passes them the empty string, reducingthem to their string support, as shown here:f : (St?
St)RNR` say(St?St)RNRf= saySt?Stf= saySt(f vacSt)= saySt(f )= ?s.s (f )= (f ) : StAs before, this reduction is elided in the proofgiven below, occurring in steps 8 and 15.71 23 45689 1011 12131415 16171.
` ?st.
t ?
CHASTISED ?
s : (St?
St?
St)TV2.
t?
: St ` t?
: St3.
t?
: St ` ?t.
t ?
CHASTISED ?
t?
: (St?
St)VP4.
` TYWIN : St5.
t?
: St ` TYWIN ?
CHASTISED ?
t?
: St6.
` ?t?.
TYWIN ?CHASTISED ?t?
: (St?
St)RNR7.
` ?c1: (St?
St)RNR.
?c2: (St?
St)RNR.
?vs.v ?
s ((say(St?St)RNRc2)?
AND ?
(say(St?St)RNRc1)): (St?
St)RNR?
(St?
St)RNR?
(St?
St)RNR8.
` ?c2: (St?
St)RNR.
?vs.v ?
s((say(St?St)RNRc2) ?
AND?(say(St?St)RNR?t?.
TYWIN?CHASTISED?t?
))...= ?c2: (St?
St)RNR.
?vs.v ?
s8Regrettably, space constraints prohibit a discussion veri-fying the typing for the ?right node raised?
terms.
The readercan verify that the terms are in fact well-typed given the sub-typing schema in section 2.1.2.
It is possible to write infer-ence rules that speak directly to the introduction and elimina-tion of the relevant functional subtypes, but these are omittedhere for the sake of brevity.34((say(St?St)RNRc2)?
AND ?
TYWIN ?
CHASTISED): (St?
St)RNR?
(St?
St)RNR9.
` ?st.
t ?
SLAPPED ?
s : (St?
St?
St)TV10.
s?
: St ` s?
: St11.
s?
: St ` ?t.
t ?
SLAPPED ?
s?
: (St?
St)VP12.
` TYRION : St13.
s?
: St ` TYRION ?
SLAPPED ?
s?
: St14.
` ?s?.
TYRION ?
SLAPPED ?
s?
: (St?
St)RNR15.
` ?vs.v ?
s ((say(St?St)RNR?s?.
TYRION ?
SLAPPED ?
s?)?
AND ?
TYWIN ?
CHASTISED)...= (?vs.v ?
s TYRION ?
SLAPPED?
AND ?
TYWIN ?
CHASTISED)= ?s.
TYRION ?
SLAPPED ?
AND?
TYWIN ?
CHASTISED ?
s : (St?
St)RNR16.
` JOFFREY : St17.
` TYRION ?
SLAPPED ?
AND?
TYWIN ?
CHASTISED ?
JOFFREY : St4 DiscussionWe provide a brief introduction to the frameworkof Linear Categorial Grammar (LinCG).
One ofthe primary strengths of categorial grammar ingeneral has been its ability to address coordina-tion phenomena.
Coordination presents a uniquelyparticular problem for grammars which distin-guish between structural combination (tectogram-mar) and the actual linear order of the strings gen-erated by such grammars (part of phenogrammar).Due to the inability to distinguish ?directionality?in string functors within a standard typed lambdacalculus, a general analysis of coordination seemsdifficult.We have elaborated LinCG?s concept ofphenogrammar by introducing phenominators,closed monoidal linear lambda terms.
We haveshown how the recursive function say provides aleft inverse for unaryphenominators, and we havedefined a more general notion of an ?empty cate-gory?
known as a vacuity, which say is definedin terms of.
It is then possible to describe sub-types of functional types suitable to make the rele-vant distinctions.
These technologies enable us togive analyses of various coordination phenomenain LinCG, extending the empirical coverage of theframework.4.1 Future workIt is possible to give an analysis of argument clus-ter coordination using phenominators, instantiat-ing the lexical entry for and with ?
as the type(St?
St?
St?
St)DTV?
(St?
St)VPand ?
as?vPs.
s?
(P)?v, and using hypothetical reason-ing.
Regrettably, the necessity of brevity prohibitsa detailed account here.Given that phenominators provide access to thestructure of functional terms which concatenatestrings to the right and left of their string support,it is our belief that any Lambek categorial gram-mar analysis can be recast in LinCG by an algo-rithmic translation of directional slash types intophenominator-indexed functional phenotypes, andwe are currently in the process of evaluating a po-tential translation algorithm from directional slashtypes to phenominators.
This should in turn pro-vide us with most of the details necessary to de-scribe a system which emulates the HTLCG ofKubota and Levine (2012), which provides anal-yses of various gapping phenomena, greatly in-creasing the overall empirical coverage.There are a number of coordination phenomenathat require modifications to the tectogrammaticalcomponent.
We would like to be able to analyzeunlike category coordinations like rich and an ex-cellent cook in the manner of Bayer (1996), as wellas Morrill (1996), which would require the addi-tion of some variety of sum types in the tectogram-mar.
Further muddying the waters is so-called ?it-erated?
or ?list?
coordination, which requires theability to generate coordinate structures contain-ing a number of conjuncts with no coordinatingconjunction, as in Thurston, Kim, and Steve.It is our intent to extend the use of phenom-inators to analyze intonation as well, and weexpect that they can be fruitfully employed togive accounts of focus, association with focus,contrastive topicalization, ?in-situ?
topicalization,alternative questions, and any number of otherphenomena which are at least partially realizedprosodically.AcknowledgementsI am grateful to Carl Pollard, Bob Levine, YusukeKubota, Manjuan Duan, Gerald Penn, the TTNLS2014 committee, and two anonymous reviewersfor their comments.
Any errors or misunderstand-ings rest solely on the shoulders of the author.35ReferencesSamuel Bayer.
1996.
The coordination of unlike cate-gories.
Language, pages 579?616.Haskell B. Curry.
1961.
Some Logical Aspects ofGrammatical Structure.
In Roman.
O. Jakobson, ed-itor, Structure of Language and its Mathematical As-pects, pages 56?68.
American Mathematical Soci-ety.Philippe de Groote and Sarah Maarek.
2007.
Type-theoretic Extensions of Abstract Categorial Gram-mars.
In Reinhard Muskens, editor, Proceedingsof Workshop on New Directions in Type-TheoreticGrammars.Philippe de Groote.
2001.
Towards Abstract Catego-rial Grammars.
In Association for ComputationalLinguistics, 39th Annual Meeting and 10th Confer-ence of the European Chapter, Proceedings of theConference, pages 148?155.David Dowty.
1988.
Type raising, functional composi-tion, and non-constituent conjunction.
In Richard T.Oehrle, Emmon Bach, and Deirdre Wheeler, editors,Categorial Grammars and Natural Language Struc-tures, volume 32 of Studies in Linguistics and Phi-losophy, pages 153?197.
Springer Netherlands.John Hughes.
1995.
The design of a pretty-printing li-brary.
In Advanced Functional Programming, pages53?96.
Springer.Yusuke Kubota and Robert Levine.
2012.
Gappingas like-category coordination.
In D. B?echet andA.
Dikovsky, editors, Logical Aspects of Computa-tional Linguistics (LACL) 2012.Yusuke Kubota.
2010.
(In)flexibility of Constituency inJapanese in Multi-Modal Categorial Grammar withStructured Phonology.
Ph.D. thesis, The Ohio StateUniversity.J.
Lambek and P.J.
Scott.
1986.
Introduction tohigher order categorical logic.
Cambridge Univer-sity Press.Scott Martin.
2013.
The Dynamics of Sense and Impli-cature.
Ph.D. thesis, The Ohio State University.Vedrana Mihalicek.
2012.
Serbo-Croatian Word Or-der: A Logical Approach.
Ph.D. thesis, The OhioState University.Glyn V. Morrill.
1994.
Type Logical Grammar.Kluwer Academic Publishers.Glyn Morrill.
1996.
Grammar and logic*.
Theoria,62(3):260?293.Reinhard Muskens.
2001.
Lambda grammars and thesyntax-semantics interface.
In Proceedings of theThirteenth Amsterdam Colloquium, pages 150?155.Universiteit van Amsterdam.Reinhard Muskens.
2010.
New Directions in Type-Theoretic Grammars.
Journal of Logic, Lan-guage and Information, 19(2):129?136.
DOI10.1007/s10849-009-9114-9.Richard Oehrle.
1994.
Term-Labeled Categorial TypeSystems.
Linguistics and Philosophy, 17:633?678.Dick Oehrle.
1995.
Some 3-dimensional systems oflabelled deduction.
Logic Journal of the IGPL, 3(2-3):429?448.Carl Pollard and E. Allyn Smith.
2012.
A unifiedanalysis of the same, phrasal comparatives, and su-perlatives.
In Anca Chereches, editor, Proceedingsof SALT 22, pages 307?325.
eLanguage.Carl Pollard.
2013.
Agnostic Hyperintensional Se-mantics.
Synthese.
to appear.Aarne Ranta.
2004.
Grammatical Framework: AType-Theoretical Grammar Formalism.
Journal ofFunctional Programming, 14:145?189.Aarne Ranta.
2009.
The GF resource grammar library.Linguistic Issues in Language Technology, 2(1).E.
Allyn Smith.
2010.
Correlational Comparison inEnglish.
Ph.D. thesis, The Ohio State University.Mark Steedman.
1985.
Dependency and co?ordinationin the grammar of dutch and english.
Language,pages 523?568.36
