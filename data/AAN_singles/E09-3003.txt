Proceedings of the EACL 2009 Student Research Workshop, pages 19?27,Athens, Greece, 2 April 2009. c?2009 Association for Computational LinguisticsCombining a Statistical Language Model with Logistic Regression toPredict the Lexical and Syntactic Difficulty of Texts for FFLThomas L. Franc?oisAspirant FNRSCENTAL (Center for Natural Language Processing)Universite?
catholique de Louvain1348 Louvain-la-Neuve, Belgiumthomas.francois@uclouvain.beAbstractReading is known to be an essential taskin language learning, but finding the ap-propriate text for every learner is far fromeasy.
In this context, automatic procedurescan support the teacher?s work.
Sometools exist for English, but at present thereare none for French as a foreign language(FFL).
In this paper, we present an origi-nal approach to assessing the readabilityof FFL texts using NLP techniques andextracts from FFL textbooks as our cor-pus.
Two logistic regression models basedon lexical and grammatical features areexplored and give quite good predictionson new texts.
The results shows a slightsuperiority for multinomial logistic re-gression over the proportional odds model.1 IntroductionThe current massive mobility of people has putincreasing pressure on the language teaching sec-tor, in terms of the availability of instructors andsuitable teaching materials.
The development ofIntelligent Computer Aided Language Learning(ICALL) has helped both these needs, while theInternet has increasingly been used as a source ofexercises.
Indeed, it allows immediate access to ahuge number of texts which can be used for edu-cational purposes, either for classical reading com-prehension tasks, or as a corpus for the creation ofvarious automatically generated exercises.However, the strength of the Internet is also itsmain flaw : there are so many texts available to theteacher that he or she can get lost.
Having gatheredsome documents suitable in terms of subject mat-ter, teachers still have to check if their readabil-ity levels are suitable for their students : a highlytime-consuming task.
This is where NLP applica-tions able to classify documents according to theirreading difficulty level can be invaluable.Related research will be discussed in Section 2.In Section 3, the distinctive features of the cor-pus used in this study and a difficulty scale suit-able for FFL text classification are described.
Sec-tion 4 focuses on the independent linguistic vari-ables considered in this research, while the statis-tical techniques used for predictions are coveredin Section 5.
Section 6 gives some details of theimplementations, and Section 7 presents the firstresults of our models.
Finally, Section 8 sums upthe contribution of this article before providing aprogramme for future work and improvement ofthe results.2 Related researchThe measurement of the reading difficulty of textshas been a major concern in the English-speakingliterature since the 1920s and the first formula de-veloped by Lively and Pressey (1923).
The fieldof readability has since produced many formulaebased on simple lexical and syntactic measuressuch as the average number of syllables per word,the average length of sentences in a piece of text(Flesch, 1948; Kincaid et al, 1975), or the per-centage of words not on a list combined with theaverage sentence length (Chall and Dale, 1995).French-speaking researchers discovered thefield of readability in 1956 through the work ofAndre?
Conquet, La lisibilite?
(1971), and the firsttwo formulae for French were adapted from Flesch(1948) by Kandel and Moles (1958) and de Land-sheere (1963).
Both of these researchers stayedquite close to the Flesch formula, and in so doingthey failed to take into account some specificitiesof the French language.Henry (1975) was the first to introduce spe-cific formulae for French.
He used a larger setof variables to design three formulae : a com-plete, an automatic and a short one, each of which19was adapted for three different educational lev-els.
His formulae are by far the best and most fre-quently used in the French-speaking world.
Later,Richaudeau (1979) suggested a criteria of ?lin-guistic efficiency?
based on experiments on short-term memory, while Mesnager (1989) coined whatis still, to the best of our knowledge, the most re-cent specific formula for French, with children asits target.Compared to the mass of studies in English,readability in French has never enthused the re-search community.
The cultural reasons for thisare analysed by Bosse?-Andrieu (1993) (who basi-cally argues that the idea of measuring text diffi-culty objectively seems far too pragmatic for theFrench spirit).
It follows that there is little cur-rent research in this field: in Belgium, the Fleschformula is still used to assess the readability ofarticles in journalism studies.
This example alsoshows that the French-specific formulae are notmuch used, probably because of their complexity(Bosse?-Andrieu, 1993).Of course, if there is little work on French read-ability, there is even less on French as a foreignlanguage.
We only know the study of Cornaire(1988), which tested the adaptation of Henry?sshort formula to French as a foreign language,and that of Uitdenbogerd (2005), which developeda new measure for English-speaking learners ofFrench, stressing the importance of cognates whendeveloping a new formula for a related language.Therefore, we had to draw our inspiration fromthe English-speaking world, which has recentlyexperienced a revival of interest in research onreadability.
Taking advantage of the increasingpower of computers and the development of NLPtechniques, researchers have been able to exper-iment with more complex variables.
Collins-Thompson et al (2005) presented a variation of amultinomial naive Bayesian classifier they calledthe ?Smoothed Unigram?
model.
We retainedfrom their work the use of language models in-stead of word lists to measure lexical complex-ity.
Schwarm and Ostendorf (2005) developeda SVM categoriser combining a classifier basedon trigram language models (one for each levelof difficulty), some parsing features such as av-erage tree height, and variables traditionally usedin readability.
Heilman et al (2007) extended the?Smoothed Unigram?
model by the recognition ofsyntactic structures, in order to assess L2 Englishtexts.
Later, they improved the combination oftheir various lexical and grammatical features us-ing regression methods (Heilman et al, 2008).
Wealso found regression methods to be the most ef-ficient of the statistical models with which we ex-perimented.
In this article, we consider some waysto adapt these various ideas to the specific case ofFFL readability.3 Corpus descriptionIn the development of a new readability formula,the first step is to collect a corpus labelled byreading-difficulty level, a task that implies agree-ment on the difficulty scale.
In the US, a com-mon choice is the 12 American grade levels corre-sponding to primary and secondary school.
How-ever, this scale is less relevant for FFL educationin Europe.
So, we looked for another scale.Given that we are looking for an automatic wayof measuring text complexity for FFL learners par-ticipating in an educational programme, an obvi-ous choice was the difficulty scale used for assess-ing students?
levels in Europe, that is the Com-mon European Framework of Reference for Lan-guages (CEFR) (Council of Europe, 2001) .
TheCEFR has six levels: A1 (Breakthrough); A2(Waystage); B1 (Threshold); B2 (Vantage); C1(Effective Operational Proficiency) and C2 (Mas-tery).
However differences in learners?
skills canbe quite substantial at lower levels, so we dividedeach of the A1, A2 and B1 grades in two, thus ob-taining a total of nine levels.We still needed to find a corpus labelled accord-ing to these nine classes.
Unlike traditional ap-proaches, based on a limited set of texts usuallystandardised by applying a closure test to a targetpopulation, our NLP-oriented approach required alarge number of texts on which the statistical mod-els could be trained.
For that reason we opted forFFL textbooks as a corpus.
With the appearance ofthe CEFR, FFL textbooks have undergone a kindof standardisation and their levels have been clari-fied.
It is thus feasible to gather a large number ofdocuments already labelled in terms of the CEFRscale by experts with an educational background.However, not every textbook can be used as adocument source.
Likewise, not all the materialfrom FFL textbooks is appropriate.
We establishedthe following criteria for selecting textbooks andtexts:?
The CEFR was published in 2001, so only20textbooks published since then were con-sidered.
This restriction also ensures thatthe language resembles present-day spokenFrench.?
The target population for our formula isyoung people and adults.
Therefore, onlytextbooks intended for this public were used.?
We retained only those texts made up of com-plete sentences, linked to a reading compre-hension task.
So, all the transcriptions oflistening comprehension tasks were ignored.Similarly, all instructions to the students wereexcluded, because there is no guarantee thelanguage employed there is the same as therest of the textbook material (metalinguisticterms and so on can be found there).Up to now, using these criteria, we have gath-ered more than 1,500 documents containing about440,000 tokens.
Texts cover a wide variety of sub-jects ranging from French literature to newspaperarticles, as well as numerous dialogues, extractsfrom plays, cooking recipes, etc.
The goal is tohave as wide a coverage as possible, to achievemaximum generalisability of the formula, and alsoto check what sort of texts it does not fit (e.g.
sta-tistical descriptive analyses have considered songsand poems as outliers).4 Selection of lexical and syntacticvariablesAny text classification tasks require an object(here a text) to be parameterised into variables,whether qualitative or quantitative.
These inde-pendent variables must correlate as strongly aspossible with the dependent variable represent-ing difficulty in order to explain the text?s com-plexity, and they should also account for the var-ious dimensions of the readability phenomenon.Traditional approaches to readability have beensharply criticised with respect to this second re-quirement by Kintsch and Vipond (1979) andKemper (1983), who both insist on the impor-tance of including the conceptual properties oftexts (such as the relations between propositionsand the ?inference load?).
However, these newapproaches have not resulted in any easily repro-ducible computational models, leading current re-searchers to continue to use the classic semanticand grammatical variables, enhancing them withNLP techniques.Because this research only spans the last year,attempts to discover interesting variables are stillat an early stage.
We explored the efficiency ofsome traditional features such as the type-tokenratio, the number of letters per word, and the av-erage sentence length, and found that, on our cor-pus, only the word length and sentence length cor-related significantly with difficulty.
Then, we addtwo NLP-oriented features, as described below: astatistical language model and a measure of tensedifficulty.4.1 The language modelThe lexical difficulty of a text is quite an elaboratephenomenon to parameterise.
The logistic regres-sion models we used in this study require us to re-duce this complex reality to just one number, thechallenge being to achieve the most informativenumber.
Some psychological work (Howes andSolomon, 1951; Gerhand and Barry, 1998; Brys-baert et al, 2000) suggests that there is a strong re-lationship between the frequency of words and thespeed with which they are recognised.
We there-fore opted to model the lexical difficulty for read-ing as the global probability of a text T (with Ntokens) occurring:P (T ) = P (t1)P (t2 | t1)?
?
?P (tn | t1, t2, .
.
.
, tn?1) (1)This equation raises two issues :1.
Estimating the conditional probabilities.
Itis well-known that it is impossible to trainsuch a model on a corpus, even the largestone, because some sequences in this equa-tion are unlikely to be encountered more thanonce.
However, following Collins-Thompsonand Callan (2005), we found that a simplesmoothed unigram model could give good re-sults for readability.
Thus, we assumed thatthe global probability of a text T could be re-duced to:P (T ) =n?i=1p(ti) (2)where p(ti) is the probability of meeting thetoken ti in French; and n is the number oftokens in a text.2.
Deciding what is the best linguistic unit toconsider.
The equations introduced above use21tokens, as is traditional in readability formu-lae, but the inflected nature of French sug-gests that lemmas may be a better alternative.Using tokens means that words taking numer-ous inflected forms (such as verbs), have theiroverall probability split between these differ-ent forms.
Consequently, compared to sel-dom ?
or never ?
inflected words (such as ad-verbs, prepositions, conjunctions), they seemless frequent than they really are.
Second, us-ing tokens presupposes a theoretical positionaccording to which learners are not able tolink an inflected form with its lemma.
Sucha view seems highly questionable for the ma-jority of regular forms.In order to settle this issue, we trained threelanguage models: one with lemmas (LM1),another with inflected forms disambiguatedaccording to their tags (LM2), and a thirdone with inflected forms (LM3).
The ex-periment was not very conclusive, since themodels all correlated with the dependent vari-able to a similar extent, having Pearson?s rcoefficients of ?0.58, ?0.58, and ?0.59 re-spectively.
However, three factors militate infavour of the lemma model: as well as the-oretical likelihood, it is the model which ismost sensitive to outliers and most prone tomeasurement error.
This suggests that, if wecan reduce this error, the lemma model mayprove to be the best predictor of the three.As a consequence of these considerations, wedecided to compute the difficulty of the text by us-ing Equation 2 adapted for lemmas and, for com-putational reasons, the logarithm of the probabili-ties:P (T ) = exp(n?i=1log[p(lemi)]) (3)The resulting value is still correlated with thelength of the text, so it has to be normalised bydividing it by N (the number of words in the text).These operations give in a final value suitable forthe logistic regression model.
More informationabout the origin and smoothing of the probabilitiesis given in Section 6.4.2 Measuring the tense difficultyHaving considered the complexity of a text?s syn-tactic structures through the traditional factor ofthe ?mean number of words per sentence?, we de-cided to also take into account the difficulty ofthe conjugation of the verbs in the text.
For thispurpose, we created 11 variables, each represent-ing one tense or class of tenses: conditional, fu-ture, imperative, imperfect, infinitive, past partici-ple, present participle, present, simple past, sub-junctive present and subjunctive imperfect.The question then arose as to whether it wouldbe better to treat these variables as binary or con-tinuous.
Theoretical justifications for a binary pa-rameterisation lie in the fact that a text becomesmore complex for a L2 language learner whenthere is a large variety of tenses, especially dif-ficult ones.
The proportion of each tense seemsless significant.
For this reason, we opted for bi-nary variables.
The other way of parameterisingthe data should nevertheless be tested in furtherresearch.5 The regression modelsBy the end of the parameterisation stage, each textof the corpus has been reduced to a vector com-prising the 14 following predictive variables : theresult of the language model, the average numberof letters per word1, the average number of wordsper sentence and the 11 binary variables for tensecomplexity.Each vector also has a label representing thelevel of the text, which is the dependent variablein our classification problem.
From a statisti-cal perspective, this variable may be consideredas a nominal, ordinal, or interval variable, eachlevel of measurement being linked to a particu-lar regression technique: multiple linear regres-sion for interval data; a popular cumulative logitmodel called proportional odds for ordinal data;and multinomial logistic regression for nominalvariables.
Therefore, identifying the best scale ofmeasurement is an important issue for readability.From a theoretical perspective, viewing the lev-els of difficulty as an interval scale would implythat they are ordered and evenly spaced.
How-ever, most FFL teachers would disagree with thisassumption: it is well known that the higher levelstake longer to complete than the earlier ones.
So, amore realistic position is to consider text difficultyas an ordinal variable (since the CEFR levels are1Pearson?s r coefficient between the language model andthe average number of letters in the words was ?0.68.
Thissuggests that there is some independent information in thelength of the words that can be used for prediction.22ordered).
The third alternative, treating the levelsas a nominal scale, is not intuitively obvious to alanguage teacher, because it suggests that there isno particular order to the CEFR levels.From a practical perspective, things are not soclear.
Traditional approaches have usually vieweddifficulty as an interval scale and applied mul-tiple linear regression.
Recent NLP perspectivehave either considered difficulty as an ordinal vari-able (Heilman et al, 2008), making use of logis-tic regression, or as a nominal one, implementingclassifiers such as the naive Bayes, SVM or deci-sion tree.
Such a variety of practices convinced usthat we should experiment with all three scales ofmeasurement.In an exploratory phase, we compared regres-sion methods and decision tree classifiers on thesame corpus.
We found that regression was moreprecise and more robust, due to the current lim-ited size of the corpus.
Linear regression wasdiscarded because it gave poor results during thetest phase.
So we retained two logistic regressionmodels, the PO model and the MLR model, whichare presented in the next section.5.1 Proportional odds (PO) modelLogistic regression is a statistical technique firstdeveloped for binary data.
It generally de-scribes the probability of a 0 or 1 outcome withan S-shaped logistic function (see Hosmer andLemeshow (1989) for details).
Adaptation of thelogistic regression for J ordinal classes involvesa model with J ?
1 response curves of the sameshape.
For a fixed class j, each of these responsefunctions is comparable to a logistic regressioncurve for a binary response with outcomes Y ?
jand Y > j (Agresti, 2002), where Y is the depen-dent variable.The PO model can be expressed as:logit[P (Y ?
j | x)] = ?j + ?
?x (4)In Equation 4, x is the vector containing the inde-pendent variables, ?j is the intercept parameter forthe jth level and ?
is the vector of regression co-efficients.
From this formula, the particularity ofthe PO model can be observed: it has the same set,?, of parameters for each level.
So, the responsefunctions only differ in their intercepts, ?j .
Thissimplification is only possible under the assump-tion of ordinality.Using this cumulative model, when 2 ?
j ?
J ,the estimated probability of a text Y belonging tothe class j can be computed as:P (Y = j | x) = logit[P (Y ?
j | x)]?logit[P (Y ?
j ?
1 | x)] (5)When j = 1, P (Y = 1 | x) is equal to P (Y ?
j |x).We said above that this model involves a simpli-fication, based on the proportional odds assump-tion.
This assumption needs to be tested with thechi-squared form of the score test (Agresti, 2002).The lower the chi-squared value, the better the POmodel fits the data.5.2 Multinomial logistic regressionMultinomial logistic regression is also called?baseline category?, because it compares eachclass Y with a reference category, often the firstone (Y1), in order to regress to the binary case.Each pair of classes (Yj , Y1) can then be describedby the ratio (Agresti, 2002, p. 268):logP (Y = j | x)P (Y = 1 | x) = ?j + ?j?x (6)where the notation is as given above.
On the ba-sis of these J-1 regression equations, it is possibleto compute the probability of a text belonging todifficulty level j using the values of its featurescontained in the vector x.
This may be calculatedusing the equation (Agresti, 2002, p. 271):P (Y = j | x) =exp(?j + ?j ?x)1 + ?Jh=2 exp(?h + ?j ?x)(7)Notice that for the baseline category (here, j = 1),?1 and ?1 = 0.
Thus, when looking for the proba-bility of a text belonging to the baseline level, it iseasy to compute the numerator, since exp(0) = 1.The value of the denominator is the same for eachj.Heilman et al (2008) drew attention to the factthat the MLR model multiplies the number of pa-rameters by J ?
1 compared to the PO model.Because of this, they recommend using the POmodel.6 Implementation of the modelsHaving covered the theoretical aspects of ourmodel, we will now describe some of the partic-ularities of our implementation.236.1 The language model: probabilities andsmoothingFor our language model, we need a list of Frenchlemmas with their frequencies of occurrence.
Get-ting robust estimates for a large number of lem-mas requires a very large corpus and is a time-consuming process.
We used Lexique3, a lexiconprovided by New et al (2001) and developed fromtwo corpora: the literary corpus Frantext contain-ing about 15 million of words; and a corpus of filmsubtitles (New et al, 2007), with about 50 millionwords.
The authors drew up a list of more than50,000 tagged lemmas, each of which is associ-ated with two frequency estimates, one from eachcorpus.We decided to use the frequencies from the sub-title corpus, because we think it gives a more ac-curate image of everyday language, which is thelanguage FFL teaching is mainly concerned with.The frequencies were changed into probabilities,and smoothed with the Simple Good-Turing al-gorithm described by Gale and Sampson (1995).This step is necessary to solve another well-knownproblem in language models: the appearance ina new text of previously unseen lemmas.
In thiscase, since the logarithm of probabilities is used,an unseen lemma would result in a infinite value.In order to prevent this, a smoothing process isused to shift some of the model?s probability massfrom seen lemmas to unseen ones.Once we had obtained a good estimate of theprobabilities, we could analyse the texts in the cor-pus.
Each of them was lemmatised and tagged us-ing the TreeTagger (Schmid, 1994).
This NLP toolallows us to distinguish between homographs thatcan represent different levels of difficulty.
For in-stance, the word actif is quite common as an ad-jective, but the noun is infrequent and is only usedin the business lexicon.
This distinction is possiblebecause Lexique3 provides tagged lemmas.6.2 Variable selectionHaving gathered the values for the 14 dependentvariables, it was possible to train the two statis-tical models.2 However, an essential requirementprior to training is feature selection.
This proce-dure, described by Hosmer and Lemeshow (1989),consists of examining models with one, two, three,2All statistical computations were performed with theMASS package (Venables and Ripley, 2002) of the R soft-ware.etc., variables and comparing them to the fullmodel according to some specified criteria so asto select one that is both efficient and parsimo-nious.
For logistic regression, the criterion se-lected is the AIC (Akaike?s Information Criterion)of the model.
This can be obtained from:AIC = ?2log-likelihood + 2k (8)where k is the number of parameters in the model,and the log-likelihood value is the result of a calcu-lation detailed by Hosmer and Lemeshow (1989).We applied the stepwise algorithm to our data,trying both a backward and a forward procedure.They converged to a simpler model containingonly 10 variables: the value obtained from our lan-guage model, the number of letters per word, thenumber of words per sentence, the past participle,the present participle, and the imperfect, infinitive,conditional, future and present subjunctive tenses.Presumable the imperative and present tenses areso common that they do not have much discrim-inative power.
On the other hand, the imperfectsubjunctive is so unusual that it is not useful for aclassification task.
However, the non-appearanceof the simple past is surprising, since it is a nar-rative tense which is not usually introduced untilan advanced stage in the learning of French.
Thisphenomenon deserves further investigation in thefuture.7 First resultsTo the best of our knowledge, no one has pre-viously applied NLP technologies to the specificissue of the readability of texts for FFL learn-ers.
So, any comparisons with previous studies aresomewhat flawed by the fact that neither the targetpopulation nor the scale of difficulty is the same.However, our results can be roughly compared tosome of the numerous studies on L1 English read-ability presented in Section 2.
Before making thiscomparison, we will analyse the predictive abilityof the two models.7.1 Models evaluationThe evaluation measures most commonly em-ployed in the literature are Pearson?s product-moment correlation coefficient, prediction accu-racy as defined by Tan et al (2005), and adjacentaccuracy.
Adjacent accuracy is defined by Heil-man et al (2008) as ?the proportion of predictionsthat were within one level of the human-assigned24Measure PO model MLR modelResults on training foldsCorrel.
0.786 0.777Exact Acc.
32.5% 38%Adj.
Acc.
70% 71.3%Results on test foldsCorrel.
0.783 0.772Exact Acc.
32.4% 38%Adj.
Acc.
70% 71.2%Table 1: Mean Pearson?s r coefficient, exact andadjacent accuracies for both models with the ten-fold cross-validation evaluation.label for the given text?.
They defended this mea-sure by arguing that even human-assigned readinglevels are not always consistent.
Nevertheless, itshould not be forgotten that it can give optimisticvalues when the number of classes is small.Exploratory analysis of the corpus highlightedthe importance of having a similar number of textsper class.
This requirement made it impossibleto use all the texts from the corpus.
Some 465texts were selected, distributed across the 9 levelsin such a way that each level contained about 50texts.
Within each class, an automatic procedurediscarded outliers located more than 3?
from themean, leaving 440 texts.
Both models were trainedon these texts.The results on the training corpus were promis-ing, but might be biased.
So, we turned to aten-fold cross-validation process which guaranteesmore reliable values for the three evaluation mea-sures we had chosen, as well as a better insightinto the generalisability of the two models.
Theresulting evaluation measures for training and testfolds are shown in Table 1.
The similarity betweenthem clearly shows that, with 440 observations,both the models were quite robust.
On this corpus,multinomial logistic regression was significantlymore accurate (with 38% of texts correctly classi-fied against 32.4% for the PO model), while Pear-son?s R was slightly higher for the PO model.These results suggest that the exact accuracymay be a better indicator of performance than thecorrelation coefficient.
However they conflict withHeilman et al?s (2008) conclusion that the POmodel performed better than the MLR one.
Thisdiscrepancy might arise because the PO modelwas less accurate for exact predictions, but betterwhen the adjacent accuracy by level was taken intoaccount.
However, the data in Table 2 do not sup-port this hypothesis; rather they confirm the supe-riority of the MLR model when adjacent accuracyis considered.
In fact, PO model?s lower perfor-mance seems to be due to a lack of fit to the data,as revealed by the result of the score test for theproportional-odds assumption.
This yielded a p-value below 0.0001, clearly showing that the POmodel was not a good fit to the corpus.There remains one last issue to be discussed be-fore comparing our results to those of other stud-ies: the empirical evidence for tense being a goodpredictor of reading difficulty.
We selected tensesbecause of our experience as FLE teacher ratherthan on theoretical or empirical grounds.
How-ever we found that exact accuracy decreased by10% when the tense variables were omitted fromthe models.
Further analysis showed that the tensecontributed significantly to the adjacent accuracyof classifying the C1 and C2 texts.7.2 Comparison with other studiesAs stated above, it is not easy to compare ourresults with those of previous studies, since thescale, population of interest and often the lan-guage are different.
Furthermore, up till now, wehave not been able to run the classical formu-lae for French (such as de Landsheere (1963) orHenry (1975)) on our corpus.
So we are limited tocomparing our evaluation measures with those inthe published literature.With multinomial logistic regression, we ob-tained a mean adjacent accuracy of 71% for 9classes.
This result seems quite good comparedto similar research on L1 English by Heilman etal.
(2008).
Using more complex syntactic fea-tures, they obtained an adjacent accuracy of 52%with a PO model, and 45% with a MLR model.However, they worked with 12 levels, which mayexplain their lower percentage.For French, Collins-Thompson and Callan(2005) reported a Pearson?s R coefficient of 0.64for a 5-classes naive Bayes classifier while we ob-tained 0.77 for 9 levels with MLR.
This differ-ence might be explained by the tagging or the useof better-estimated probabilities for the languagemodel.
Further research on this point to determinethe specificities of an efficient approach to Frenchreadability appears very promising.25Level A1 A1+ A2 A2+ B1 B1+ B2 C1 C2 MeanPO model 91% 91% 67% 68% 53% 55% 56% 86% 68% 70%MLR model 93% 90% 69% 51% 59% 56% 64% 88% 73% 71%Table 2: Mean adjacent accuracy per level for PO model and MLR model (on the test folds).8 Discussion and future researchThis paper has proposed the first readability ?for-mula?
for French as a foreign language using NLPand statistical models.
It takes into account someparticularities of French such as its inflected na-ture.
A new scale to assess FFL texts within theCECR framework, and a new criteria for the cor-pus involving the use of textbooks, have also beenproposed.
The two logistic models applied to a440-text corpus gave results consistent with the lit-erature.
They also showed the superiority of theMLR model over the PO model.
Since Heilmanet al (2008) found the opposite, and the intuitiveview is that levels should be described by an ordi-nal scale of measurement, this issue clearly needsfurther investigation.This research is still in progress, and furtheranalyses are planned.
The predictive capacity ofsome other lexical and grammatical features willbe explored.
At the lexical level, statistical lan-guage models seems to be best, and tagging thetexts to work with lemmas turned out to be effi-cient for French, although it has not been shownto be superior to disambiguated inflected forms.Moreover, due to their higher sensibility to con-text, smoothed n-grams might represent an alter-native to lemmas.Once the best unit has been selected, someother issues remain: it is not clear whether amodel using the probabilities of this unit in thewhole language or probabilities per level (Collins-Thompson and Callan, 2005) would be more ef-ficient.
We also wonder whether the L1 frequen-cies of words are similar to those in L2 ?
FFLtextbooks use a controlled vocabulary, linked tospecific situational tasks, which suggests that it ishighly possible that the frequencies of words inFFL differ from those in mother-tongue French.Grammatical features have been taken into ac-count through simple parameterisation.
Morecomplex measures (such as the presence of somesyntactic structures (Heilman et al, 2007) or thecharacteristics of a syntactic-parsing tree) havebeen explored in the literature.
We hope that in-cluding such factors may result in improved accu-racy for our model.
However, these techniques areprobably dependent on the quality of the parser?sresults.
Parsers for French are less accurate thanthose for English, which may generate some noisein the analysis.Finally, we intend to explore the performanceof other classification techniques.
Logistic regres-sion was the most efficient of the statistical mod-els we tested, but as our corpus grows, more andmore data is becoming available, and data min-ing approaches may become applicable to the text-categorization problem for FFL readability.
Sup-port vector machines have already been shown tobe useful for readability purposes (Schwarm andOstendorf, 2005).
We also want to try aggregatingapproaches such as boosting, bagging, and randomforests (Breiman, 2001), since they claim to be ef-fective when the sample is not perfectly represen-tative of the population (which could be true forour data).
These analyses would aim to illuminatesome of the assets and flaws of each of the statis-tical models considered.AcknowledgmentsThomas L. Franc?ois is supported by the Bel-gian Fund for Scientific Research (FNRS), as isthe research programme from which this materialcomes.I would like to thank my directors, Prof.Ce?drick Fairon and Prof. Anne-Catherine Simon,my colleagues, Laure Cuignet and the anonymousreviewers for their valuable comments.ReferencesAlan Agresti.
2002.
Categorical Data Analysis.
2ndedition.
Wiley-Interscience, New York.J.
Bosse?-Andrieu.
1993.
La question de la lisi-bilite?
dans les pays anglophones et les pays fran-cophones.
Technostyle, Association canadienne desprofesseurs de re?daction technique et scientifique,11(2):73?85.L.
Breiman.
2001.
Random forests.
Machine Learn-ing, 45(1):5?32.26M.
Brysbaert, M. Lange, and I.
Van Wijnendaele.2000.
The effects of age-of-acquisition andfrequency-of-occurrence in visual word recognition:Further evidence from the Dutch language.
Euro-pean Journal of Cognitive Psychology, 12(1):65?85.J.S.
Chall and E. Dale.
1995.
Readability Revisited:The New Dale-Chall Readability Formula.
Brook-line Books, Cambridge.K.
Collins-Thompson and J. Callan.
2005.
Predict-ing reading difficulty with statistical language mod-els.
Journal of the American Society for InformationScience and Technology, 56(13):1448?1462.A.
Conquet.
1971.
La lisibilite?.
Assemble?e Perma-nente des CCI de Paris, Paris.C.M.
Cornaire.
1988.
La lisibilite?
: essai d?applicationde la formule courte d?Henry au franc?ais languee?trange`re.
Canadian Modern Language Review,44(2):261?273.Council of Europe and Education Committee andCouncil for Cultural Co-operation.
2001.
CommonEuropean Framework of Reference for Languages:Learning, Teaching, Assessment.
Press Syndicate ofthe University of Cambridge.G.
De Landsheere.
1963.
Pour une application destests de lisibilite?
de Flesch a` la langue franc?aise.
LeTravail Humain, 26:141?154.R.
Flesch.
1948.
A new readability yardstick.
Journalof Applied Psychology, 32(3):221?233.W.A.
Gale and G. Sampson.
1995.
Good-Turing fre-quency estimation without tears.
Journal of Quanti-tative Linguistics, 2(3):217?237.S.
Gerhand and C. Barry.
1998.
Word frequencyeffects in oral reading are not merely age-of-acquisition effects in disguise.
Journal of Experi-mental Psychology.
Learning, Memory, and Cogni-tion, 24(2):267?283.M.
Heilman, K. Collins-Thompson, J. Callan, andM.
Eskenazi.
2007.
Combining lexical and gram-matical features to improve readability measures forfirst and second language texts.
In Proceedings ofNAACL HLT, pages 460?467.M.
Heilman, K. Collins-Thompson, and M. Eskenazi.2008.
An analysis of statistical models and fea-tures for reading difficulty prediction.
Associationfor Computational Linguistics, The 3rd Workshopon Innovative Use of NLP for Building EducationalApplications:1?8.G.
Henry.
1975.
Comment mesurer la lisibilite?.
Labor.D.W.
Hosmer and S. Lemeshow.
1989.
Applied Logis-tic Regression.
Wiley, New York.D.H.
Howes and R.L.
Solomon.
1951.
Visual durationthreshold as a function of word probability.
Journalof Experimental Psychology, 41(40):1?4.L.
Kandel and A. Moles.
1958.
Application de l?indicede Flesch a` la langue franc?aise.
Cahiers ?Etudes deRadio-Te?le?vision, 19:253?274.S.
Kemper.
1983.
Measuring the inference loadof a text.
Journal of Educational Psychology,75(3):391?401.J.
Kincaid, R.P.
Fishburne, R. Rodgers, andB.
Chissom.
1975.
Derivation of new read-ability formulas for navy enlisted personnel.Research Branch Report, 85.W.
Kintsch and D. Vipond.
1979.
Reading compre-hension and readability in educational practice andpsychological theory.
Perspectives on Memory Re-search, pages 329?366.B.A.
Lively and S.L.
Pressey.
1923.
A method formeasuring the vocabulary burden of textbooks.
Ed-ucational Administration and Supervision, 9:389?398.J.
Mesnager.
1989.
Lisibilite?
des textes pour en-fants: un nouvel outil?
Communication et Lan-gages, 79:18?38.B.
New, C. Pallier, L. Ferrand, and R. Matos.
2001.Une base de donne?es lexicales du franc?ais con-temporain sur internet: LEXIQUE.
LAnne?e Psy-chologique, 101:447?462.B.
New, M. Brysbaert, J. Veronis, and C. Pallier.
2007.The use of film subtitles to estimate word frequen-cies.
Applied Psycholinguistics, 28(04):661?677.F.
Richaudeau.
1979.
Une nouvelle formule de lisi-bilite?.
Communication et Langages, 44:5?26.H.
Schmid.
1994.
Probabilistic part-of-speech taggingusing decision trees.
In Proceedings of InternationalConference on New Methods in Language Process-ing, volume 12.
Manchester, UK.S.E.
Schwarm and M. Ostendorf.
2005.
Reading levelassessment using support vector machines and sta-tistical language models.
Proceedings of the 43rdAnnual Meeting on Association for ComputationalLinguistics, pages 523?530.P.-N. Tan, M. Steinbach, and V. Kumar.
2005.
Intro-duction to Data Mining.
Addison-Wesley, Boston.S.
Uitdenbogerd.
2005.
Readability of French as aforeign language and its uses.
In Proceedings of theAustralian Document Computing Symposium, pages19?25.W.N.
Venables and B.D.
Ripley.
2002.
Modern Ap-plied Statistics with S. Springer, New York.27
