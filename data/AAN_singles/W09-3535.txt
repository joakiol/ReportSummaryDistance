Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 168?176,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPTag Confidence Measure for Semi-Automatically UpdatingNamed Entity RecognitionKuniko Saito and Kenji ImamuraNTT Cyber Space Laboratories, NTT Corporation1-1 Hikarinooka, Yokosuka-shi, Kanagawa, 239-0847, Japan{saito.kuniko, imamura.kenji}@lab.ntt.co.jpAbstractWe present two techniques to reduce ma-chine learning cost, i.e., cost of manuallyannotating unlabeled data, for adaptingexisting CRF-based named entity recog-nition (NER) systems to new texts ordomains.
We introduce the tag posteriorprobability as the tag confidence measureof an individual NE tag determined bythe base model.
Dubious tags are auto-matically detected as recognition errors,and regarded as targets of manual correc-tion.
Compared to entire sentence poste-rior probability, tag posterior probabilityhas the advantage of minimizing systemcost by focusing on those parts of thesentence that require manual correction.Using the tag confidence measure, thefirst technique, known as active learning,asks the editor to assign correct NE tagsonly to those parts that the base modelcould not assign tags confidently.
Activelearning reduces the learning cost by66%, compared to the conventionalmethod.
As the second technique, wepropose bootstrapping NER, which semi-automatically corrects dubious tags andupdates its model.1 IntroductionMachine learning, especially supervised learning,has achieved great success in many natural lan-guage tasks, such as part-of-speech (POS) tag-ging, named entity recognition (NER), and pars-ing.
This approach automatically encodes lin-guistic knowledge as statistical parameters(models) from large annotated corpora.
In theNER task, which is the focus of this paper, se-quential tagging1 based on statistical models is1Tags are assigned to each input unit (e.g., word) one by one.similarly used; studies include Conditional Ran-dom Fields (CRFs; Lafferty et al, 2001, Suzukiet al, 2006).
However, the manual costs incurredin creating annotated corpora are extremely high.On the other hand, Consumer Generated Me-dia (CGM) such as blog texts has attracted a lotof attention recently as an informative resourcefor information retrieval and information extrac-tion tasks.
CGM has two distinctive features;enormous quantities of new texts are generatedday after day, and new vocabularies and topicscome and go rapidly.
The most effective ap-proach to keep up with new linguistic phenom-ena is creating new annotated corpora for modelre-training at short intervals.
However, it is diffi-cult to build new corpora expeditiously becauseof the high manual costs imposed by traditionalschemes.To reduce the manual labor and costs, vari-ous learning methods, such as active learning(Shen et al, 2004, Laws and Sch?tze, 2008),semi-supervised learning (Suzuki and Isozaki,2008) and bootstrapping (Etzioni, 2005) havebeen proposed.
Active learning automaticallyselects effective texts to be annotated from hugeraw-text corpora.
The correct answers are thenmanually annotated, and the model is re-trained.In active learning, one major issue is data selec-tion, namely, determining which sample data ismost effective.
The data units used in conven-tional methods are sentences.Automatically creating annotated corporawould dramatically decrease the manual costs.
Infact, there always are some recognition errors inany automatically annotated corpus and the edi-tor has to correct errors one by one.
Since sen-tences are used as data units, the editor has to payattention to all tags in the selected sentence be-cause it is not obvious where the recognition er-ror is.
However, it is a waste of manual effort to168annotate all tags because most tags must be la-beled correctly by the base model2.In this paper, we propose a confidence meas-ure based on tag posterior probability for theNER task.
Our method does not use the confi-dence of a sentence, but instead computes theconfidence of the tag assigned to each word.
Thetag confidence measure allows the sentence towhich the base model might assign an incorrecttag to be selected automatically.
Active learningbecomes more efficient because we correct onlythose tags that have low confidence (cf.
Sec.
4).We can realize the same effect as activelearning if we can automatically correct the se-lected data based upon our tag confidence meas-ure.
Our proposal "Semi-Automatically UpdatingNER" automatically corrects erroneous data byusing a seed NE list generated from other infor-mation sources.
Semi-Automatically UpdatingNER easily keeps up with new words because itenables us to update the model simply by provid-ing a new NE list (cf.
Sec.
5).2 Named Entity Recognition TaskThe NER task is to recognize entity names suchas organizations and people.
In this paper, we use17 NE tags based on the IOB2 scheme (Sang andDe Meulder, 1999) combined with eightJapanese NE types defined in the IREXworkshop (IREX 1999) as shown in Table 1.For example, ???
(Tokyo)/?
(City)/?(in)?
is labeled like this:??
?/B-<LOC> ?/I-<LOC> ?/O?.This task is regarded as the sequential taggingproblem, i.e., assigning NE tag sequencesnttT L1= to word sequences nwwW L1= .Recently, discriminative models such asConditional Random Fields (CRFs) have beensuccessfully applied to this task (Lafferty et al,2001).
In this paper, we use linear-chain CRFsbased on the Minimum Classification Errorframework (Suzuki et al, 2006).
The posteriorprobability of a tag sequence is calculated asfollows:))},,(),((exp{)(1)|(11iibbbiiaaanittfwtfWZWTP?=??+?
?= ???
(1)where iw  and it are the i-th word and itscorresponding NE tag, respectively.
),( iia wtf2 A base model is the initial model trained with the initialannotated corpora.and ),( 1 iib ttf ?
is a feature function 3 .
a?
andb?
is a parameter to be estimated from thetraining data.
Z(W) is a normalization factorover all candidate paths expressed as follows:))}.,(),((exp{)(11iibbbiiaaaniTttfwtfWZ?=??+?
?= ????
(2)The best tag sequence that maximizes Formula(1) is located using the Viterbi algorithm.Table 1.
NE Types and Tags.NE Types NE TagsPERSON B-<PSN> I-<PSN>LOCATION B-<LOC> I-<LOC>ORGANIZATION B-<ORG> I-<ORG>ARTIFACT B-<ART> I-<ART>DATE B-<DAT> I-<DAT>TIME B-<TIM> I-<TIM>MONEY B-<MNY> I-<MNY>PERCENT B-<PCT> I-<PCT>outside an NE O3 Error Detection with Tag ConfidenceMeasure3.1 Tag Posterior ProbabilityIt is quite natural to consider sentence posteriorprobability as a confidence measure of the esti-mated tag sequences.
We focus on tag posteriorprobability, and regard it as the confidencemeasure of the decoded tag itself.
Our methodtries to detect the recognition error of each tag byreferring to the tag confidence measure.Figure 1 overviews the calculation of tagconfidence measure.
The confidence score of tagji,t , which is a candidate tag for word iw , iscalculated as follows:,W)|T,P(t=W)|P(t Tji,ji, ?
(3)where ?T ji, W)|T,P(t is the summation of all NEtag sequences that pass through ji,t .
This prob-ability is generally called the marginal probabil-ity.
k,=j L1,  represents the number of NEtags shown in Table 1( i.e., k=17 in this paper).The tag confidence score of ji,t  can be cal-culated efficiently using forward and backward3 We used n-grams (n=1, 2, 3) of surface forms and parts-of-speech within a five word window and 2-gram combina-tions of NE tags as the feature set.169Figure 1.
Overview of the tag confidence measure calculation.The W ord  SequenceThe Tag C andidates1w 12 ?iww K iw ni ww K1+<s> 1,1t 1,11,2 ?itt K 1,it 1,1,1 ni tt K+ </s>jt ,1 jnji tt ,,1 K+M M M M M Mjij tt ,1,2 ?KMMkt ,1 kit , knki tt ,,1 K+kik tt ,1,2 ?Kjit ,ji ,?
ji ,?algorithms as follows (Manning and Sch?tze,1999):,?
?Z(W)=W)|P(t ji,ji,ji, ?1            (4)where)}},,(),(exp{{1,1,iibbbiiaaakkijittfwtf????+???=?????(5))}},,(),(exp{{111,1,++++??+???=?iibbbiiaaakkijittfwtf????
(6)1,0, =?
j                            (7)1.1, =?
j+n                           (8)In this manner, the confidence scores of alltags of each word in a given sentence are calcu-lated.
The rejecter then refers to the highest tagconfidence score in judging whether the decodedNE tag is correct or incorrect.3.2 RejecterThe rejecter tries to detect dubious tags in theNER result derived by the method described inSection 2.
For each word, the rejecter refers tothe decoded tag td, which maximizes Formula (1),and the most confident tag t1, in terms of the pos-terior probability as defined in Formula (4).
Thejudgment procedure is as follows:[1] If td is NOT identical to t1, then td is deter-mined to be dubious, and so is rejected as anincorrect tag.4[2] Else, if the confidence score of t1, called cs1,is below the predefined threshold, td is de-termined to be dubious, and so is rejected asan incorrect tag.
[3] Otherwise, td is accepted as a correct tag.4 The decoded tag td rarely disagrees with the most confi-dent tag t1 due to a characteristic of the CRFs.Increasing the threshold also increases thenumber of rejected tags and manual annotationcost.
In practice, the threshold should be empiri-cally set to achieve the lowest judgment errorrate using development data.
There are two typesof judgment errors: false acceptance and falserejection.
False rejection is to reject a correct tag,and false acceptance is to accept an incorrect tagin error.
The judgment error rate is taken as theratio of these two types of errors in all instances.4 Active LearningTag-wise recognition error detection is also help-ful for data selection in active learning.
If a sen-tence contains several rejected tags, it containssome new information which the base modeldoes not have.
In other words, this sentence isworth learning.
Our approach, then, is to basedata selection (sentence selection) on the pres-ence of rejected tags.
However, it is not neces-sary to check and correct all tags in each selectedsentence.
We only have to check and correct therejected tags to acquire the annotated sentences.Figure 2 shows our active learning scheme.ModelRe-trainingSelected DataBase Data(Labeled)Additional Data(Unlabeled)BaseModelRecognition Error DetectorMorphologicalAnalyzerNER DecoderCalculation of theTag Confidence MeasureData SelectionManuallyCorrected DataUpdatedModelModel LearningCorrect the RejectedTags by HandRejecterFigure 2.
Active Learning Scheme170Figure 3.
Learning Curves.0.60.620.640.660.680.70.720.740.760.780.80 0.2 0.4 0.6 0.8 1Word Check RateF-measureTag Base(proposed)Sentence BaseFirst, the NER decoder assigns an NE tag to eachword5 of the additional data using the base modeltrained with the base data.
The recognition errordetector then determines whether each tag can beconfidently accepted as described in Section 3.
Inthis step, the confidence score is calculated usingthe same base model used for NER decoding.Next, the sentences with at least one rejected tagare selected.
Only the rejected tags are manuallychecked and corrected.
Finally, the model is re-trained and updated with the merged data con-sisting of the manually corrected data and thebase data.4.1 ExperimentsWe evaluated the efficiency of our active learn-ing method from the perspective of learning cost.A blog corpus consisting of 45,694 sentences inblog articles on the WWW was prepared for theexperiments.
This corpus was divided into foursegments as shown in Table 2.
All sentenceswere manually annotated including additionaldata.
For additional data, these tags were initiallyhidden and used only for simulating manual cor-rection as shown below.
Development data wasused for optimizing the threshold by measuringthe rejecter?s judgment error rate as described inSubsection 3.2.Table 2.
Data Used for Active Learning.Base Data 11,553 sentences, 162,227 wordsDevelopment Data 1,000 sentences,  19,710 wordsAdditional Data 32,163 sentences, 584,077 wordsTest Data 978 sentences,  17,762 wordsWe estimated the learning cost from the rateof hand-labeled tags.
The Word Check Rate(WCR) represents the ratio of the number of thewords in the additional data that need to bemanually checked and annotated, to the totalnumber of words in the additional data, and isexpressed as follows:WCR= Checked Tags / Total Words.The system obtained various sizes of selecteddata as the rejecter changed its threshold from0.1 to 1.0 for data selection.
Only the rejectedtags in the selected data were replaced with thetags originally assigned by hand (i.e., correcttags).
This procedure simulates manual correc-tion.
The manually corrected data was mergedwith the base data to update the base model.5 The morphological analyzer segments an input sen-tence into a word sequence and assigns parts-of-speech to each word.We compared our method with data selectionbased on the sentence confidence measure.Posterior probabilities of sentences were used asthe confidence measure, and low-confidencescoring sentences were selected.
In contrast toour active learning method, all tags in the se-lected sentences were replaced with the correcttags in this case.We evaluated the effectiveness of the up-dated models against the test data by F-measureas follows:.2precision+recallprecisionrecall=F??
(9)4.2 Results and Discussions4.2.1 Learning Curves and AccuraciesFigure 3 shows learning curves of two activelearning methods; one is based on our tag confi-dence measure (Tag Based selection), and theother is based on the sentence confidence meas-ure (Sentence Based selection).
In order to reachthe F-measure of approximately 0.76, SentenceBased selection requires approximately 60% ofthe entire data set to be checked by hand.
In con-trast, Tag Based selection requires only 20% orthereabouts.
In other words, our Tag Based selec-tion technique basically matches the performanceof Sentence Based selection with only 1/3 of thelearning cost.4.2.2 Types of Tag ReplacementWe further investigated the effects of tag-basedjudgment from the results of an experiment onour Tag Based selection.
We categorized tag re-placements of the rejected tags into the followingfour types:?
No Change: the rejected tag is replacedwith the same tag.?
O-to-BI: the rejected tag is an O-tag.
It isreplaced with a B-tag or an I-tag.171?
BI-to-O: the rejected tag is a B-tag or an I-tag.
It is replaced with an O-tag.?
BI-to-BI: the rejected tag is a B-tag or an I-tag.
It is replaced with another B-tag or I-tag.Table 3 shows the distribution of these fourcategories in the selected data for the thresholdof 0.5.
This threshold achieves the lowest judg-ment error rate given the development set.The rate of No Change replacement type isthe highest.
This means that the rejecter rejectedtoo many tags, which actually did not need to bechecked by hand.
Although this result does nothave a negative influence on the accuracy of theupdated model, it is not preferable from thelearning cost perspective.
Further considerationshould be given in order to improve the rejecter'sjudgment.O-to-BI type accounts for the 2nd highest per-centage of all replacements: it is almost one thirdof all changes.
Excluding No Change type (i.e.,among O-to-BI, BI-to-O and BI-to-BI types), O-to-BI type makes up nearly 60% of these threereplacement types.
This result shows that therewere many new NEs not recognized by the basemodel in the selected data.Table 3.
The Distribution of Replacement Types.Replacement Type Frequency %No Change 13,253 43.6O-to-BI 10,042 33.0BI-to-O 2,419 8.0BI-to-BI 4,688 15.4Total 30,402 100.05 Bootstrapping for NERAs mentioned in Section 4, we have to correct anO-tag to a B-tag or an I-tag in many cases, al-most 60% of all actual corrections.
This situationarises from a characteristic of the NER task.
Inthe NER task, most NE tags in the entire corpusare O-tags.
In fact, we found that 91 % of all tagswere O-tags in the additional data discussed inSection 4.
Thus, when a new NE appears in asentence, this new NE is often mistakenly givenan O-tag by the base model.The fact that only O-tags are dominant im-plies that we have a chance to find a correct B-tag or I-tag when we look up the 2nd candidate.This is because one of these top two candidatesis inevitably a B-tag or an I-tag.
Thus, it is valu-able to consider what the NEXT preferable tag iswhen the most preferable tag is rejected.We examined in detail the accuracy of the tagcandidates when the threshold is 0.5 as summa-rized in Table 4.
When the top tag (i.e., the tagwith the highest tag confidence score) is accepted,its accuracy is 94 %, obviously high.
On theother hand, the top tag?s accuracy is only 43 %when it is rejected.
However, focusing both onthe top tag and on the 2nd tag provides an oppor-tunity to correct the rejected tag in this case.
Ifwe consider these top two tags together when the1st tag is rejected, the possibility of finding thecorrect tag is 72 %, relatively high.
This suggeststhat the system is capable of correcting the re-jected tag automatically by using the top two tagcandidates.
On this background, automatic cor-rection is attempted for re-training the modelthrough the use of a bootstrapping scheme.Table 4.
Accuracy of the Tags.Rejecter?s Judgment of the Top TagACCEPT REJECTTop Tag Top Tag 2nd Tag94 % 43 % 29 %Figure 4 shows an example of the top two tagcandidates and their tag confidence scores whenthe top tag?s confidence score is lower than thethreshold (=0.5).
We call this lattice the ?taggraph?
in this paper.
The system failed to recog-nize the movie title ?3??????
(?Sancho-me no Yuuhi?, which means ?Sunset on ThirdStreet?)
as ARTIFACT only with the top tagcandidates.
However, it may find a correct tagsequence using the top two tag candidates(shaded cells in Figure 4).
Once the system iden-tifies the correct tag sequence automatically inthe tag graph, the sequence is used as a manuallyannotated sequence.
We introduce this new tech-nique, Semi-Automatically Updating NER.Figure 4.
The Top Two Tag Candidates withTag Confidence Measures.Top Tag 2nd TagTag score Tag score??
(Today) B-<DAT> 0.95?(?)
O 0.983(Third) O 0.47 B-<ART> 0.36??
(Street) O 0.38 I-<ART> 0.36?
(on) O 0.49 I-<ART> 0.38??
(Sunset) I-<ART> 0.39 O 0.34?(?)
O 0.99?
(is) O 0.99??
(broadcast) O 0.991725.1 Semi-Automatically Updating NERFigure 5.
Semi-Automatically UpdatingNER Scheme.Selected Datawith Tag GraphsModelRe-trainingBase Data(Labeled)Additional Data(Unlabeled)BaseModelRecognition Error DetectorMorphologicalAnalyzerNER DecoderCalculation of theTag Confidence MeasureData SelectionAutomaticallyCorrected DataUpdatedModelModel LearningRejecterAutomatic CorrectionSeed NE ListBy extracting the correct tag sequence in eachtag graph as shown in Figure 4, it is possible toobtain automatically corrected data, which alsoserve as new training data.
Based on this idea,we propose Semi-Automatically Updating NER,which is hereafter simply referred to as UpdatingNER.Figure 5 overviews Updating NER.
The re-jecter produces the sentences with tag graphsbased on the tag confidence measure.
In this newprocedure, however, the rejecter?s role differsfrom that described in Section 4 as follows:[1] When the highest confidence score cs1 equalsor exceeds the threshold, the rejecter acceptsonly the top candidate tag t1, otherwise itgoes to Step 2.
[2] When cs1 is less than the threshold, the re-jecter accepts not only the top tag t1 but alsothe 2nd tag t2.Sentences that contain the 2nd candidates areselected in data selection for subsequent process-ing.
The correct tag sequence in each tag graph isidentified in automatic correction as follows:[1] Select the tag sequence that has the longest6and consistent NE from the tag graph.
[2] If the longest NE also exists in a seed NE list,which will be described below, the systemextracts the entire sentence with its tag se-quence as corrected data.In Step 1, the system selects one preferabletag sequence based on the longest NE match.
Inthe tag graph shown in Figure 4, there are 16possible sequences because four words ?3?, ???
(Street)?, ?
(on)?
and ??
??(Sunset)?
eachhave two tag candidates; O or B for ?3?, O or Ifor ?
(Street)?
and ?
(on)?, and I or O for ???
???(Sunset)?.
For example, ?B I I I?, ?B I I O?,?B I O O?, ?O O O I?, ?O O O O?
and the rest.Because the sequence ?B I I I?
constructs thelongest NE, the system selects the tag sequencethat contains the ARTIFACT ?3?????
.
?Other sequences that contain partial NEs such as?3?, ?3 ?, ?3??
???
?, which are all ARTI-FACTs, are ignored.In Step 2, the system judges whether the tagsequence selected in Step 1 is indeed correct.6 By longest, we mean the longest tag sequence that doesnot include any O-tags.However, the system requires some hints tojudge the correctness, so we need to prepare aseed NE list, which contains surface forms andNE types.
This list can be created by manuallyannotation of possible NEs or automatic genera-tion from other sources such as dictionaries.When the same NE exists both in the selected tagsequence and the seed NE list, the system re-gards the selected tag sequence as reliable andextracts it as automatically corrected data.
Fi-nally, the model is updated by merging theautomatically corrected data with the base data.Bootstrapping means that data selection andcorrection of the selected data are completelyautomatic; we still have to prepare the seed NElist somehow.
Thus the learning cost is quite lowbecause we only need to provide an NE list as aseed.
Updating NER is capable of modifying themodel to keep up with the emergence of newnamed entities.
Therefore, it is effective to ana-lyze the large amount of texts that emerge every-day, such as blogs on the WWW.5.2 ExperimentsWe tested our Updating NER with a largeamount of blog texts from the WWW.
Oneweek?s worth of blog texts was crawled on theWWW to generate the additional data.
Table 5shows the statistics of the data used in our ex-periments.
The test data contained only the blogtexts generated in December 2006, and the basedata is about a half year older than the test data.Therefore, it is difficult for the base model torecognize new NEs in the test data.
One week?s173worth of December 2006 blog texts were pre-pared for bootstrapping.
The overlap between thetest data and the additional data was removed inadvance.
We set the rejecter?s threshold at 0.5and selected the data with tag graphs from theadditional data.Japanese Wikipedia entries were used as theseed NE list.
The titles of Wikipedia articleswere regarded as surface forms.
NE types wereestimated from the category sections of each arti-cle, based on heuristic rules prepared in advance.We collected 104,296 entries as a seed NE list.Using this seed list, Updating NER extractedthe seed NE and its context from the selecteddata automatically.
If the system found a match,it extracted the sentence with its tag sequencefrom the selected data.
The automatically cor-rected data was then merged with the base data inorder to re-train the base model.For comparison, we evaluated the effect of theseed NE list itself.
If there is a sequence of wordsthat can be found in the seed list, then that se-quence is always recognized as a NE.
Note thatthe other words are simply decoded using thebase model.
We call this method ?user diction-ary?.
Here, we use recall and precision to evalu-ate the accuracy of the model.Table 5.
Data Description for Updating NER.Base Data(blog in Sep. 04-Jun.
06)43,716 sentences746,304 wordsAdditional Data(one week?s blog in Dec. 06)240,474 sentences3,677,077 wordsSelected Data from theAdditional Data113,761 sentences2,466,464 wordsTest Data(blog in Dec.06)1,609 sentences21,813 words5.3 ResultsTable 6 shows the details of accuracy results re-garding the following four NE types: PERSON,LOCATION, ORGANIZATION, and ARTI-FACT, which are referred to hereafter as PSN,LOC, ORG and ART, respectively.
Although weadded Wikipedia as a user dictionary to the basemodel, it only slightly improved the recall.
Infact, it has no positive and sometimes a negativeeffect on precision (e.g., ART decreased from0.666 to 0.619).
This indicates that adding an NElist as a dictionary is not enough to improve theaccuracy of a NER system.
This is because theNER system cannot discriminate an NE fromsurrounding unrelated words.
It simply extractsmatched sequences of words, so it overestimatesthe number of NEs.On the contrary, our Updating NER improvedboth recall and precision (e.g., the recall and theprecision in ART improved from 0.320 to 0.364and from 0.666 to 0.694, respectively.).
Thismeans that not only the NE list but also the con-texts are actually needed to retrain the model.Our Updating NER scheme has the advantage offinding the reliable context of a seed NE listautomatically.
Although some manual effort isneeded to provide a seed NE list, its associatedcost is lower than the cost of annotating the en-tire training data.
Thus, we regard Updating NERas a promising solution for reducing learningcost in practical NER systems.As shown in Table 6, neither user dictionarymethod nor Updating NER improves the accu-racy in ORG.
We assume that this is caused bythe distribution of NE types in the seed NE list.In the seed list selected from the Wikipedia en-tries, PSN-type is dominant (74%).
ORG-type isscant at only 11%, so the system did not haveenough chances to retrain the ORG-type.
Rather,it might be the case that the system had a ten-dency to recognize ORG-type as PSN-type be-cause peoples' names are often used as organiza-tion names.
Further investigation is needed toclarify the impact of the distribution and thequality of the seed NE list.Table 6.
Details of Accuracy.PSN LOC ORG ARTrec.
0.640 0.737 0.688 0.320Base Modelprec.
0.699 0.811 0.652 0.666rec.
0.686 0.729 0.688 0.354+Wikipedia(user dic.)
prec.
0.716 0.815 0.654 0.619rec.
0.649 0.747 0.678 0.364+Wikipedia(UpdatingNER) prec.
0.728 0.822 0.632 0.6945.4 DiscussionsCompared to conventional machine learningtechniques, the most distinctive feature of Updat-ing NER is that the system can focus on the toptwo candidates when the confidence score of thetop candidate is low.
This feature actually has agreat advantage in the NER task, because thesystem is capable of determining what the nextpreferable tag is when a new NE appears whichis assigned an O-tag by the base model.Updating NER, however, has one weak point.That is, the following two strict conditions arerequired to correct the selected data automati-cally.
First, the correct tag sequence must appearin tag graphs (i.e., as one of the top two tag can-didates).
Second, the NE must also appear in theseed NE list.
These conditions decrease the174chance of extracting sentences with correct tagsequences from the selected data.To overcome this weakness, one practicalapproach is to use Updating NER in combinationwith active learning.
In the case of active learn-ing, we do not need the correct tags in the toptwo candidates.
The editor can assign correcttags without considering the order of candidates.In short, active learning has broad coverage interms of learning, while Updating NER does not.Therefore, active learning is suitable for improv-ing the performance level of the entire basemodel.
Updating NER has the advantage of stay-ing current with new named entities whichemerge every day on the WWW.
In practical use,for example, it will be better to update the modelevery week with Updating NER to keep up withnew named entities, and occasionally performactive learning (every six months or so) to en-hance the entire model.
In the future, we plan toevaluate the efficiency of our two learning meth-ods in practical applications, such as domain ad-aptation and acquisition of hot trend NE wordsfrom blog texts on the WWW.6 Related WorksTo date, there have been many related works onactive learning not only for the NER task (Shenet al, 2004, Laws and Sch?tze, 2008) but alsofor other tasks, such as POS tagging (Engelsonand Dagan, 1996), text classification (Lewis andCatlett, 1994), parsing (Hwa, 2000), and confu-sion set disambiguation (Banko and Brill, 2001).Active learning aims at effective data selectionbased on criterion measures, such as the confi-dence measure.
Most previous works focus onthe Sentence-Based criterion evaluation and dataselection.
Our proposal differs from those previ-ous works in that we focus on the Tag-Basedstrategy, which judges whether each tag shouldbe accepted or rejected.
This approach maxi-mizes the effectiveness of manual annotation byleaving the accepted tags in without any manualcorrection.
As a result, our Tag-based approachreduces the manual annotation cost by 66 %,compared to the Sentence-Base method.Semi-supervised learning has become an ac-tive area in machine learning; it utilizes not onlyannotated corpora but also huge amounts of plaintext for model training.
Several studies adaptedsemi-supervised learning to suit NLP tasks, suchas word sense disambiguation (Yarowsky, 1995),text classification (Fujino et al, 2008), andchunking and NER (Suzuki and Isozaki, 2008).Suzuki and Isozaki (2008) suggest that a GIGA-word size plain text corpus may further improvethe performance of the state-of-the-art NLP sys-tem.
In this paper, however, we aim at modeladaptation to the CGM domain to keep up withthe new linguistic phenomena that are emergingevery day.
Because it is difficult to obtain GIGA-word size plain text sets that reflect such newlinguistic phenomena, it is not practical to di-rectly apply this approach to our task.Bootstrapping is similar to semi-supervisedlearning in that it also allows the use of plain text(Etzioni 2005, Pantel and Pennacchioti 2006).
Inthis learning method, it is possible to extract newinstances automatically from plain text withsmall seed data prepared manually.
Our UpdatingNER is similar to bootstrapping in that it extractsnew annotated corpora automatically from plaintext data starting with a seed NE list.
However,the goal of conventional bootstrapping is to de-velop a new dictionary or thesaurus by extractingnew instances.
On the contrary, our goal is toacquire a new NE and its surrounding context ina sentence, not to build a NE dictionary (i.e., cor-rect tag sequence).
It is the tag sequence and nota single NE that is needed for model training.Updating NER is a novel approach in the pointof applying bootstrapping to the framework ofsupervised learning.
This approach is quite effec-tive in that it has the advantage of reducing learn-ing cost compared with active learning becauseonly a seed NE list is needed.7 ConclusionsTo reduce machine learning cost, we introducedtwo techniques that are based on a tag confidencemeasure determined from tag posterior probabil-ity.
Dubious tags are automatically detected asrecognition errors using the tag confidencemeasure.
This approach maximizes the effective-ness of manual annotation by leaving the confi-dent tags in without any manual correction.We first applied this technique to activelearning by correcting error tags manually.
Wefound that it matches the performance of thelearning method based on the sentence confi-dence measure with only 1/3 of the learning cost.Next, we proposed Semi-Automatic Updat-ing NER which has a bootstrap learning scheme,by expanding the scope from the top tag candi-date to include the 2nd candidate.
With this newscheme, it is possible to collect auto-labeled datafrom a large data source, such as blog texts onthe WWW, by simply providing a seed NE list.175ReferencesM.
Banko and E. Brill.
2001.
Scaling to Very VeryLarge Corpora for Natural Language Disambigua-tion.
In Proc.
of ACL-2001, pages 26-33.S.
A. Engelson and I. Dagan.
1999.
Committee-BasedSample Selection for Probabilistic Classifiers.Journal of Artificial Intelligence Research,vol.11(1999), pages 335-360.O.
Etzioni, M. Cafarella, D. Downey, A. Popescu, T.Shaked, S. Soderland, D. S. Weld, and A. Yates.2005.
Unsupervised Named-Entity Extraction fromthe Web: An Experimental Study.
Artificial Intelli-gence, 165(1), pages 91-134.A.
Fujino, N. Ueda, and K. Saito.
2008.
Semisuper-vised Learning for a Hybrid Generative/Discriminative Classifier Based on the MaximumEntropy Principle.
IEEE Transactions on PatternAnalysis and Machine Intelligence (TPAMI), 30(3),pages 424-437.R.
Hwa.
2000.
Sample Selection for StatisticalGrammer Induction.
In Proc.
of EMNLP/VLC-2000,pages 45-52.IREX Committee (ed.
), 1999.
In Proc.
of the IREXworkshop.
http://nlp.cs.nyu.edu/irex/J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional Random Fields: Probabilistic Models forSegmenting and Labeling Sequence Data.
In Proc.of ICML-2001.
pages 282-289.F.
Laws and H. Sch?tze.
2008.
Stopping Criteria forActive Learning of Named Entity Recognition.
InProc.
of COLING-2008, pages 465-472.D.
Lewis and J. Gatlett.
1994.
Heterogeneous uncer-tainty sampling for supervised learning.
In Proc.
ofICML-1994, pages 148-156.C.
D. Manning and H. Sch?tze.
1999.
Foundations ofStatistical Natural Language Processing.
The MITPress.P.
Pantel and M. Pennacchiotti.
2006.
Espresso: Lev-eraging Generic Patterns for Automatically Har-vesting Semantic Relations.
In Proc.
of COLING-ACL-2006, pages 113-120.E.
F. T. K. Sang and F. De Meulder.
1999.
Represent-ing text chunks.
In Proc.
of EACL-1999, pages173-179.D.
Shen, J. Zhang, J. Su, G. Zhou, and C. L. Tan.2004.
Multi-Criteria-based Active Learning forNamed Entity Recognition.
In Proc.
of ACL-2004,pages 589-596.J.
Suzuki and H. Izozaki.
2008.
Semi-Supervised Se-quential Labeling and Segmentation using Giga-word Scale Unlabeled Data.
In Proc.
of ACL-2008,pages 665-673.J.
Suzuki, E. McDermott, and H. Isozaki.
2006.
Train-ing Conditional Random Fields with MultivariateEvaluation Measures.
In Proc.
of COLING-ACL-2006.
pages 617-624.D.
Yarowsky.
1995.
Unsupervised Word Sense Dis-ambiguation Rivaling Supervised Methods.
In Proc.of ACL-1995, pages 189-196.X.
Zhu.
2007.
Semi-Supervised Learning, ICML-2007 Tutorial.176
