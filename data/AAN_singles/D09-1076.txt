Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 727?736,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPSynchronous Tree Adjoining Machine TranslationSteve DeNeefe and Kevin KnightUSC Information Sciences Institute4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292 USA{sdeneefe,knight}@isi.eduAbstractTree Adjoining Grammars have well-knownadvantages, but are typically considered toodifficult for practical systems.
We demon-strate that, when done right, adjoining im-proves translation quality without becomingcomputationally intractable.
Using adjoiningto model optionality allows general translationpatterns to be learned without the clutter ofendless variations of optional material.
Theappropriate modifiers can later be spliced in asneeded.In this paper, we describe a novel methodfor learning a type of Synchronous Tree Ad-joining Grammar and associated probabilitiesfrom aligned tree/string training data.
We in-troduce a method of converting these gram-mars to a weakly equivalent tree transducerfor decoding.
Finally, we show that adjoiningresults in an end-to-end improvement of +0.8BLEU over a baseline statistical syntax-basedMTmodel on a large-scale Arabic/EnglishMTtask.1 IntroductionStatistical MT has changed a lot in recent years.We have seen quick progress from manuallycrafted linguistic models to empirically learnedstatistical models, from word-based models tophrase-based models, and from string-based mod-els to tree-based models.
Recently there is a swingback to incorporating more linguistic informationagain, but this time linguistic insight carefullyguides the setup of empirically learned models.Shieber (2007) recently argued that proba-bilistic Synchronous Tree Adjoining Grammars(Shieber and Schabes, 1990) have the right com-bination of properties that satisfy both linguistsand empirical MT practitioners.
So far, though,most work in this area has been either more lin-guistic than statistical (Abeille et al, 1990) orstatistically-based, but linguistically light (Nessonet al, 2006).Current tree-based models that integrate lin-guistics and statistics, such as GHKM (Galley etal., 2004), are not able to generalize well froma single phrase pair.
For example, from the datain Figure 1, GHKM can learn rule (a) to translatenouns with two pre-modifiers, but does not gener-alize to learn translation rules (b) - (d) without theoptional adjective or noun modifiers.
Likewise,none of these rules allow extra material to be intro-duced, e.g.
?Pakistan?s national defense minister?.In large enough training data sets, we see manyexamples of all the common patterns, but the rarerpatterns have sparse statistics or poor coverage.NPJJnationalNNdefenseNNministerwzyr AldfAE AlwTnY(a)NPJJ1NN2NN3?
NN3NN2JJ1(b)NPNN1NN2?
NN2NN1(c)NPJJ1NN2?
NN2JJ1(d)NPNN1?
NN1Figure 1: Rule (a) can be learned from this trainingexample.
Arguably, the more general rules (b) -(d) should also be learnable.To mitigate this problem, the parse trees usedas training data for these systems can be binarized(Wang et al, 2007).
Binarization allows rules withpartial constituents to be learned, resulting in moregeneral rules, richer statistics, and better phrasalcoverage (DeNeefe et al, 2007), but no principledrequired vs. optional decision has been made.
Thismethod?s key weakness is that binarization alwayskeeps adjacent siblings together, so there is no wayto group the head with a required complement ifoptional information intervenes between the two.Furthermore, if all kinds of children are consid-ered equally optional, then we have removed im-portant syntactic constraints, which may end uppermitting too much freedom.
In addition, spu-rious alignments may limit the binarization tech-727nique?s effectiveness.In this paper, we present a method of learninga type of probabilistic Synchronous Tree Adjoin-ing Grammar (STAG) automatically from a cor-pus of word-aligned tree/string pairs.
To learn thisgrammar we use linguistic resources to make therequired vs. optional decision.
We then directlymodel the optionality in the translation rules bylearning statistics for the required parts of the ruleindependently from the optional parts.
We alsopresent a method of converting these rules into awell-studied tree transducer formalism for decod-ing purposes.
We then show that modeling option-ality using adjoining results in a statistically sig-nificant BLEU gain over our baseline syntax-basedmodel with no adjoining.2 Translation Model2.1 Synchronous Tree Insertion GrammarsTree Adjoining Grammars (TAG), introduced byJoshi et al (1975) and Joshi (1985), allow inser-tion of unbounded amounts of material into thestructure of an existing tree using an adjunctionoperation.
Usually they also include a substitutionoperation, which has a ?fill in the blank?
seman-tics, replacing a substitution leaf node with a tree.Figure 2 visually demonstrates TAG operations.Shieber and Schabes (1990) offer a synchronousversion of TAG (STAG), allowing the construc-tion of a pair of trees in lockstep fashion using theTAG operations of substitution and adjunction ontree pairs.
To facilitate this synchronous behav-ior, links between pairs of nodes in each tree pairdefine the possible sites for substitution and ad-junction to happen.
One application of STAG ismachine translation (Abeille et al, 1990).One negative aspect of TAG is the compu-tational complexity: O(n6) time is requiredfor monolingual parsing (and thus decoding),and STAG requires O(n12) for bilingual parsing(which might be used for training the model di-rectly on bilingual data).
Tree Insertion Grammars(TIG) are a restricted form of TAG that was in-troduced (Schabes and Waters, 1995) to keep thesame benefits as TAG (adjoining of unboundedmaterial) without the computational complexity?TIG parsing is O(n3).
This reduction is due to alimitation on adjoining: auxiliary trees can onlyintroduce tree material to the left or the right ofthe node adjoined to.
Thus an auxiliary tree canbe classified by direction as left or right adjoining.adjunctionNPDTtheNPNN?NPJJ?
NP*substitution substitutionNNministerJJdefense=?NPDTtheNPJJdefenseNPNNministerFigure 2: TAG grammars use substitution and ad-junction operations to construct trees.
Substitu-tion replaces the substitution node (marked with?)
with another tree.
Adjunction inserts an aux-iliary tree?a special kind of tree fragment with afoot node (marked with *)?into an existing tree ata permitted non-terminal node.
Note that in TAG,adjunctions are permitted at any non-terminal withthe same label as the root and foot node of theauxiliary tree, while in STAG adjunctions are re-stricted to linked sites.Nesson et al (2006) introduce a probabilis-tic, synchronous variant of TIG and demonstrateits use for machine translation, showing resultsthat beat both word-based and phrase-based MTmodels on a limited-vocabulary, small-scale train-ing and test set.
Training the model uses anO(n6) bilingual parsing algorithm, and decodingis O(n3).
Though this model uses trees in the for-mal sense, it does not create Penn Treebank (Mar-cus et al, 1993) style linguistic trees, but uses onlyone non-terminal label (X) to create those trees us-ing six simple rule structures.The grammars we use in this paper share someproperties in common with those of Nesson et al(2006) in that they are of the probabilistic, syn-chronous tree-insertion variety.
All pairs of sites(both adjunction and substitution in our case) areexplicitly linked.
Adjunction sites are restricted bydirection: at each linked site, the source and targetside each specify one allowed direction.
The re-sult is that each synchronous adjunction site can beclassified into one of four direction classes: {LR,LL, RR, RL}.
For example, LR means the sourceside site only allows left adjoining trees and thetarget side site only allows right adjoining trees.There are several important differences betweenour grammars and the ones of Nesson et al (2006):Richer, Linguistic Trees: Our grammars have a728Penn Treebank-style linguistic tree on the En-glish (target) side, and a hierarchical structureusing only a single non-terminal symbol (X)on the source side.
We believe this providesthe rich information needed in the target lan-guage without over-constraining the model.Substitution Sites/Non-lexical trees: We useboth substitution and adjunction (Nessonet al (2006) only used adjunction) and donot require all trees to contain lexical itemsas is commonly done in TIG (Schabes andWaters, 1995).Single Adjunction/Multiple Sites: Each non-terminal node in a tree may allow multipleadjunction sites, but every site only allows atmost one adjunction,1 a common assumptionfor TAG as specified in the Vijay-Shanker(1987) definition.Here are some examples of automaticallylearned translation rules with interpretations ofhow they work:1. simple lexical rules for translating words orphrases:INwithout?
?XAlAinterpretation: translate the Arabic word?AlA?
as the preposition ?without?2.
rules with substitution for translating phraseswith holes (substitution sites are designatedby an arrow and numeric subscript, e.g.NP?1):PPPPINofNP?1?
?XX?1interpretation: insert ?of?
to turn a nounphrase into a prepositional phrase3.
simple adjoining rules for inserting optionalmodifiers (adjoining sites are designated by1An adjoined rule may itself have adjoining sites allowingfurther adjunction.an alphabetic subscript before or after a non-terminal to indicate direction of adjoining,e.g.aNP):aNPJJ?1NP*?
?XX* XaX?1interpretation: adjoin an adjective before anoun in English but after in Arabic, and al-lowing further adjoinings in those same di-rections afterward4.
rules with multiple adjunction and substitu-tion sites:aSNP?1 bScVPdVPeVBD?2NP?3?
?XaXX?2XX?1e,bXd,cX?3interpretation: translate an Arabic sentence inVSO form into an English sentence in SVOform, with multiple adjoining options2.2 Generative StoryWhen we use these rules to translate from a for-eign sentence f into an English sentence e, weuse several models together in a log-linear fash-ion, but our primary model is a joint model ofP (etree, ftree), which is our surrogate for directlymodeling P (e|f).
This can be justified becauseP (e|f) =P (e,f)P (f), and P (f) is fixed for a givenforeign sentence.
Therefore:argmaxeP (e|f) = argmaxeP (e, f)?
yield(argmaxetreeP (etree, ftree))?
yield(argmaxetreeP (detree,ftree))where detree,ftreeis a derivation tree of rules thatgenerates etreeand ftree.
In other words, e, thehighest probability translation of f , can be approx-imated by taking the yield of the highest proba-bility tree etreethat is a translation of the high-est probability tree of f .
This can further be ap-proximated by the highest probability derivationof rules translating between f and e via trees.Now we define the probability of generatingdetree,ftree.
Starting with an initial symbol pair729representing a rule with a single substitution site,2?TOP?, X?
?, a tree pair can be generated by thefollowing steps:1.
For each substitution site siin the current ruler1:(a) Choose with probabilityPsub(r2|?labelL(si), labelR(si)?)
a ruler2having root node labels labelL(si)and labelR(si) that match the left andright labels at si.2.
For each adjunction site si,r1in the currentrule r1:(a) Choose with rule-specific probabilityPifadj(decisionadjoin|si,r1, r1) choosewhether or not to adjoin at the currentsite si,r1.
(b) If we are adjoining at sitesi,r1, choose with probabilityPadj(r2|d, ?labelL(si,r1), labelR(si,r1)?
)a rule r2of direction class d havingroot node labels labelL(si,r1) andlabelR(si,r1) that match the left andright labels at si,r1.3.
Recursively process each of the added rulesFor all substitution rules rs, adjoining rules ra,and adjoining sites si,r, the probability of a deriva-tion tree using these rules is the product of all theprobabilities used in this process, i.e.
:Pderiv=?rs(Psub(rs|?rootL(rs), rootR(rs)?)
?
?si,rsPifadj(decisionadjoin|si,rs, rs))?
?ra(Padj(ra|dir(ra), ?rootL(ra), rootR(ra)?)
?
?si,raPifadj(decisionadjoin|si,ra, ra))Note that while every new substitution site re-quires an additional rule to be added, adjunctionsites may or may not introduce an additional rulebased on the rule-specific Pifadj probability.
Thisallows adjunction to represent linguistic optional-ity.2Here and in the following, we use site as shorthand forsynchronous site pair.3 Learning the ModelInstead of using bilingual parsing to directly trainour model from strings as done by Nesson et al(2006), we follow the method of Galley et al(2004) by dividing the training process into steps.First, we word align the parallel sentences andparse the English (target) side.
Then, we transformthe aligned tree/string training data into derivationtrees of minimal translation rules (Section 3.1).
Fi-nally, we learn our probability models Psub, Pifadj ,and Padjby collecting counts over the derivationtrees (Section 3.2).
This method is quick enoughto allow us to scale our learning process to large-scale data sets.3.1 Generating Derivation Trees and RulesThere are four steps in transforming the trainingdata into derivation trees and rules, the first twooperating only on the English parse tree itself:3A.
Marking Required vs.
Optional.
For eachconstituent in the English parse tree, we mark chil-dren as (H)ead, (R)equired, or (O)ptional elements(see step (a) in Figure 3).
The choice of head, re-quired, or optional has a large impact on the gen-erality and applicability of our grammar.
If allchildren are considered required, the result is thesame as the GHKM rules of Galley et al (2004)and has the same problem?lots of low count,syntactically over-constrained rules.
Too manyoptional children, on the other hand, allows un-grammatical output.
Our proposed model is a lin-guistically motivated middle ground: we considerthe linguistic heads and complements selected byCollins?
(2003) rules to be required and all otherchildren to be optional.B.
Parse tree to TIG tree.
Next, we re-structure the English tree to form a TIG deriva-tion where head and required elements are substi-tutions, and optional elements are adjunctions (seestep (b) in Figure 3).
To allow for adjoining be-tween siblings under a constituent, we first do ahead-out binarization of the tree.
This is followedby excising4 any children marked as optional andreplacing them with an adjunction site, as shownin Figure 4.
Note that we excise a chain of op-tional children as one site with each optional child3These first two steps were inspired by the method Chiang(2003) used to automatically extract a TIG from an Englishparse tree.4Excising is the opposite of adjoining: extracting out anauxiliary rule from a tree to form two smaller trees.730SADVP , NP VP .
(a)=?SADVPO,ONPRVPH.O(b)=?SADVP , NP VP .Figure 3: Parse tree to TIG transformation: (a) mark constituent children with (H)ead, (R)equired, and(O)ptional, then (b) restructure the tree so that head and required elements are substitutions, while op-tional elements are adjoined (shown with dotted lines).NT1NT1ABCNT2XYZ=?NT1ABC NT1NT1* NT2XYZNT1NT3XYZNT1NT2DEFNT1ABC=?NT1NT1NT1NT3XYZNT1*NT2DEFNT1*ABC(a) excising one optional child (XYZ) (b) excising a series of optional children (DEF, then XYZ)Figure 4: Two examples of excising auxiliary trees from a head-out binarized parse tree: (a) excising oneoptional left branch, (b) excising a chain of optional branches in the same (right) direction into a seriesof adjunctions.
In both examples, the ?ABC?
child is the head, while the other children are optional.adjoined to the previous child, as in Figure 4(b).C.
Extracting rules and derivation trees.
Wenow have a TIG derivation tree, with each elemen-tary tree attached to its parent by a substitution oradjunction link.
We can now extract synchronousrules allowed by the alignments and syntactic con-stituents.
This can be done using a method in-spired by the rule-extraction approach of Galley etal.
(2004), but instead of directly operating on theparse tree we process the English TIG derivationtree.
In bottom-up fashion, we visit each elemen-tary tree in the derivation, allowing a rule rootedat this tree to be extracted if its words or thoseof its descendants are aligned such that they arethe English side of a self-contained parallel phrase(i.e., the foreign text of this phrase is not aligned toEnglish leaves outside of the set of descendants).Otherwise, this elementary tree is rejoined with itsparent to form a larger elementary tree.
At the endof this process we have a new set of linked ele-mentary trees which make up the English side ofthe grammar, where each substitution or adjunc-tion link becomes a substitution or adjunction sitein the synchronous grammar.On the foreign side we start with the foreign textof the self-contained parallel phrase and replaceany parts of this phrase covered by substituted oradjoined children of the English side tree with sub-stitution sites or adjunction site markers.
Fromthis, we produce a tree with a simple, regular formby placing all items under a root node labeled X.In the case of more than one foreign word or sub-stitution site, we introduce an intermediate level ofX-labeled non-terminals to allow for possible ad-junction between elements, otherwise the adjoin-ing sites attach to the single root node.
We attachall foreign-side adjoining sites to be left adjoining,except on the right side of the right-hand child.It is possible to have the head child tree on theEnglish side not aligned to anything, while the ad-joined children are.
This may lead to rules with noforeign non-terminal from which to anchor the ad-junctions, so in this case, we attach adjoined childelementary trees starting from the head and mov-ing out until we attach a some child with a non-empty foreign side.D.
Generalizing rules.
We need to clarifywhat makes one rule distinct from another.
Con-sider the example in Figure 5, which shows se-lected rules learned in the case of two differentnoun phrases.
If the noun phrase consists of justa single noun, we learn rule (a), while if the nounphrase also has an adjective, we learn rules (b) and(c).
Since adjoining the adjective is optional, we731consider rules (a) and (c) to be the same rule, thelatter with an adjoining seen, and the former withthe same adjoining not seen.3.2 Statistical ModelsOnce we have the derivation trees and list of rules,we learn our statistical models using maximumlikelihood estimation.
By counting and normal-izing appropriately over the entire corpus, we canstraightforwardly learn the Psuband Padjdistribu-tions.
However, recall that in our model Pifadj is arule-specific probability, which makes it more dif-ficult to estimate accurately.
For common rules,we see plenty of examples of adjoining, while forother rules, we need to learn from only a handfulof examples.
Smoothing and generalization are es-pecially important for these low frequency cases.Two options present themselves for how to esti-mate adjoining:(a) A joint model of adjoining.
We assume thatadjoining decisions are made in combinationwith each other, and so learn non-zero proba-bilities only for adjoining combinations seenin data(b) An independent model of adjoining.
We as-sume adjoining decisions are made indepen-dently, and learn a model for each adjoiningsite separatelyOption (a) may be sufficient for frequent rules,and will accurately model dependencies betweendifferent kinds of adjoining.
However, it does notallow us to generalize to unseen patterns of adjoin-ing.
Consider the low frequency situation depictedin Figure 6, rules (d)-(f).
We may have seen thisrule four times, once with adjoining site a, twicewith adjoining sites a and b, and once with a thirdadjoining site c. The joint model will give a zeroprobability to unseen patterns of adjoining, e.g.
noadjoining at any site or adjoining at site b alone.Even if we use a discounting method to give a non-zero probability to unseen cases, we still have noway to distinguish one from another.Option (b) allows us to learn reasonable esti-mates for these missing cases by separating outadjoining decisions and letting each speak for it-self.
To properly learn non-zero probabilities forunseen cases5 we use add k smoothing (k = 12).5For example, low frequency rules may have always beenobserved with a single adjoining pattern, and never withoutadjoining.A weakness of this approach still remains: ad-joining is not a truly independent process, as weobserve empirically in the data.
In real data, fre-quent rules have many different observed adjoin-ing sites (10 or 20 in some cases), many of whichrepresent already infrequent sites in combinationsnever seen together.
To reduce the number of in-valid combinations produced, we only allow ad-joinings to be used at the same time if they haveoccurred together in the training data.
This restric-tion makes it possible to do less adjoining than ob-served, but not more.
For the example in Figure 6,in addition to the observed patterns, we would alsoallow site b to be used alone, and we would allowno adjoinings, but we would not allow combina-tions of site c with either a or b.
Later, we willsee that this makes the decoding process more ef-ficient.Because both option (a) and (b) above havestrengths and weaknesses, we also explore a thirdoption which builds upon the strengths of each:(c) A log-linear combination of the joint modeland independent model.
We assume the prob-ability has both a dependent and indepen-dent element, and learn the relative weightbetween them automaticallyTo help smooth this model we add two addi-tional binary features: one indicating adjoiningpatterns seen in data and one indicating previouslyunseen patterns.4 DecodingTo translate with these rules, we do a monolingualparse using the foreign side of the rules (constrain-ing the search using non-terminal labels from bothsides), while keeping track of the English sidestring and structure for language modeling pur-poses.
This produces all valid derivations of ruleswhose foreign side yield is the input string, fromwhich we simply choose the one with the high-est log-linear model score.
Though this processcould be done directly using a specialized parsingalgorithm, we note that these rules have weaklyequivalent counterparts in the Synchronous TreeSubstitution Grammar (STSG) and Tree-to-stringtransducer (xLNTs6) worlds, such that each STIGrule can be translated into one equivalent rule, plussome helper rules to model the adjoin/no-adjoin6xLNTs is shorthand for extended linear non-deleting top-down tree-to-string transducer.732Case 1: Case 2:NPNNhealthAlSHp?
(a)NPNN?1?
?XX?1NPJJnationalNPNNdefenseAldfAE AlwTnY?(b)NPJJ?1NP*?
?XX* XX?1(c)aNPNN?1?
?XaX?1Figure 5: Selected rules learned in two cases.
Rule (a) and (c) are considered the same rule, where (c)has the optional synchronous adjoining site marked with a.
From these (limited) examples alone wewould infer that adjective adjoining happens half the time, and is positioned before the noun in English,but after the noun in Arabic (thus the positioning of site a).(d)aQPbIN?1??aXbX?1(e)aQPIN?1??aXX?1(f)cQPIN?1?
?XcX?1(seen once) (seen twice) (seen once)Figure 6: For a low frequency rule, we may see only a few different adjoining patterns, but we want toinfer more.decision.
Conversion to a better known and ex-plored formalism allows us to take advantage ofexisting code and algorithms.
Here we describethe conversion process to xLNTs rules, thoughconversion to STSG is similar.Algorithm 1 describes the process of convertingone of our automatically learned STIG rules.
Oneach side of the rule, we traverse the tree in a top-down, left-to-right order, recording words, substi-tution sites, and adjoining sites in the order en-countered (left adjoinings before the node?s chil-dren and right adjoinings after).
We make thesewords and sites as the children under a single rootnode.
The substitution sites are given states madeup of a combination of their source and target la-bels as are the roots of non-adjoining rules.
Ad-joining sites are labeled with a combination of therule id and a site id.
Adjoining rule roots are la-beled with a combination of the source and targetroot labels and the direction class.
To allow for theadjoining/no-adjoining decision, two helper rulesare created for each adjoining site, their root statea combination of the rule and site ids.
One of theserules has only epsilon leaf nodes (representing noadjoining), while the other has leaf nodes and astate that match with the corresponding adjoiningrule root (labeled with the site?s source and targetlabels and the direction class).For each rule, the algorithm generates onemain rule and pairs of helper rules to facilitateadjoining/non-adjoining.
For computational effi-ciency reasons, our decoder supports neither ep-silon rules nor non-binary rules.
So we remove ep-silons using an exponential expansion of the rules:combine each main rule with an adjoining or non-adjoining helper rule for each adjunction site, thenremove epsilon-only branches.
For k adjunctionsites this could possibly results in 2k rules.
But asdiscussed previously (at the end of Section 3.2),we only allow subsets of adjoining combinationsseen in training data, so this number is substan-tially lower for large values of k.5 ExperimentsAll experiments are trained with a subset (171,000sentences or 4 million words) of the Arabic-English training data from the constrained datatrack of the NIST 2008 MT Evaluation, leav-ing out LDC2004T18, LDC2007E07, and the UNdata.
The training data is aligned using the LEAFtechnique (Fraser and Marcu, 2007).
The Englishside of the training data is parsed with an imple-mentation of Collins Model 2 (Collins, 2003)then head-out binarized.
The tuning data (1,178sentences) and devtest data (1,298 sentences) are733Input: Synchronous TIG rule r with j adjoining sites, S ?
T , where S and T are treesOutput: a weakly equivalent xLNTs rule S?
?
t1.
.
.
tn, where S?
is a one-level tree, and 2 ?
jhelper rules for adjoiningRun time: O(|S| + |T |)beginrules ?
{}, lhs-state ?
concat(?q?, get-root(S), get-root(T ))site-and-word-list-s ?
get-sites-and-words-in-order(S)site-and-word-list-t ?
get-sites-and-words-in-order(T )if r is adjoining then lhs-state ?
concat(lhs-state, get-adjoin-dir(S), get-adjoin-dir(T ))lhs?
construct-LHS(lhs-state, get-root(S), site-and-word-list-s)rhs?
construct-RHS(add-states(id(r), site-and-word-list-t))add(rules, ?lhs ?
rhs?)
/* main rule */foreach adjoining site i ?
1 .
.
.
k dolhs-state ?
concat(?q?, id(r), i), rhs-state ?
concat(?q?, lhs-root)lhs-root ?
concat(source-label(i), target-label(i), source-dir(i), target-dir(i))lhs ?
construct-LHS(lhs-state, lhs-root, lhs-root)rhs ?
construct-RHS({(rhs-state, lhs-root)})rhs-eps ?
construct-RHS(!
)add(rules, {?lhs ?
rhs?, ?lhs ?
rhs-eps?})
/* helper rules for site i */return rulesendfunction get-sites-and-words-in-order(node)y ?
{}if node is substitution site or word then append site or word to y elseappend left adjoining sites to y in outside-to-inside orderforeach child c of node do append result of get-yield(c) to yappend right adjoining sites to y in inside-to-outside orderreturn yendfunction add-states(ruld-id, node-list)foreach substitution or adjunction site siand in node-list doif siis substitution site then state = concat(?q?, source-site-label(si), target-site-label(si))else state = concat(?q?, rule-id, i)replace siwith (state, si)return modified node-listendAlgorithm 1: Conversion from synchronous TIG rules to weakly equivalent xLNTs rulesBLEUdescription DevTest NIST06(1) baseline: all required (GHKM minimal, head-out binarized parse trees) 48.0 47.0(2) joint adjoining prob model alone (only observed adjoining patterns) 48.0 46.6(3) independent adjoining prob model alone (only observed adjoining patterns) 48.1 46.7(4) independent adjoining prob model alone (with new adjoining patterns) 48.5 47.6(5) independent model alone + features (adjoining pattern, direction) 48.4 47.7(6) log-linear combination of joint & independent models + features 48.7 47.8Table 1: End-to-end MT results show that the best adjoining model using a log-linear combinationof joint and independent models (line 6) outperforms the baseline (line 1) by +0.7 and +0.8 BLEU, astatistically significant difference at the 95% confidence level.734made up of newswire documents drawn from theNIST MT evaluation data from 2004, 2005, and2006 (GALE part).
We use the newswire docu-ments from the NIST part of the 2006 evaluationdata (765 sentences) as a held-out test set.We train our feature weights using max-BLEU(Och, 2003) and decode with a CKY-based de-coder that supports language model scoring di-rectly integrated into the search.In addition to Psub, Padj, and Pifadj , weuse several other features in our log-linearmodel during decoding, including: lexical andphrase-based translation probabilities, a modelsimilar to conditional probability on the trees(P (ftree(rule)|etree(rule))), a probability modelfor generating the top tree non-terminal, a 5-gramlanguage model7, and target length bonus.
Wealso have several binary features?lexical rule,rule with missing or spurious content words?andseveral binary indicator features for specializedrules: unknown word rules; name, number, anddate translation rules; and special fail-safe mono-tone translation rules in case of parse failures andextremely long sentences.Table 1 shows the comparison between ourbaseline model (minimal GHKM on head-out bi-narized parse trees) and different models of ad-joining, measured with case-insensitive, NIST-tokenized BLEU (IBM definition).
The top section(lines 1?4) compares the joint adjoining probabil-ity model to the independent adjoining probabil-ity model and seen vs. unseen adjoining combi-nations.
While the joint model results in a BLEUscore at the same level as our baseline (line 2),the independent model (line 4) improves BLEU by+0.5 and +0.6, which are significant differencesat the 95% confidence level.
Since with the in-dependent model we introduce both new adjoin-ing patterns and a different probability model foradjoining (each site is independent), we also usethe independent model with only previously seenadjoining patterns (line 3).
The insignificant dif-ference in BLEU between lines 2 and 3 leads usto think that the new adjoining patterns are wherethe improvement comes from, rather than the in-dependent probability model alone.We also test several other features and combi-nations.
First, we add binary features to indicatea new adjoining combination vs. one previously7The 5-gram LM was trained on 2 billion words of auto-matically selected collections taken from the NIST 08 allow-able data.seen in data.
We also add features to indicate thedirection class of adjoining to test if there is a sys-tematic bias toward particular directions.
Thesefeatures cause no significant difference in score(line 5).
We also add the joint-adjoining proba-bility as a feature, allowing it to be combined in alog-linear fashion with the independent probabil-ity (line 6).
This results in our best BLEU gain:+0.7 and +0.8 over our non-adjoining baseline.6 ConclusionWe have presented a novel method for learningthe rules and probabilities for a new statistical,linguistically-informed, syntax-based MT modelthat allows for adjoining.
We have described amethod to translate using this model.
And we havedemonstrated that linguistically-motivated adjoin-ing improves the end-to-end MT results.There are many potential directions for researchto proceed.
One possibility is to investigate othermethods of making the required vs. optional de-cision, either using linguistic resources such asCOMLEX or automatically learning the distinc-tion using EM (as done for tree binarization byWang et al (2007)).
In addition, most ideas pre-sented here are extendable to rules with linguistictrees on both sides (using insights from Lavie etal.
(2008)).
Also worth investigating is the directintegration of bilingual dictionaries into the gram-mar (as suggested by Shieber (2007)).
Lastly, rulecomposition and different amounts of lexicaliza-tion (Galley et al, 2006; Marcu et al, 2006; De-Neefe et al, 2007) or context modeling (Marin?o etal., 2006) have been successful with other mod-els.AcknowledgmentsWe thank David Chiang for suggestions aboutadjoining models, Michael Pust and Jens-So?nkeVo?ckler for developing parts of the experimen-tal framework, and other colleagues at ISI fortheir helpful input.
We also thank the anony-mous reviewers for insightful comments and sug-gestions.
This research is financially supportedunder DARPA Contract No.
HR0011-06-C-0022,BBN subcontract 9500008412.ReferencesAnne Abeille, Yves Schabes, and Aravind K. Joshi.1990.
Using lexicalized TAGs for machine trans-lation.
In Proc.
COLING, volume 3.735David Chiang.
2003.
Statistical parsing with an auto-matically extracted tree adjoining grammar.
Data-Oriented Parsing.Michael Collins.
2003.
Head-driven statistical modelsfor natural language parsing.
Computational Lin-guistics, 29(4).Steve DeNeefe, Kevin Knight, Wei Wang, and DanielMarcu.
2007.
What can syntax-based MT learnfrom phrase-based MT?
In Proc.
EMNLP-CoNLL.Alexander Fraser and Daniel Marcu.
2007.
Getting thestructure right for word alignment: LEAF.
In Proc.EMNLP-CoNLL.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proc.
HLT-NAACL.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proc.ACL.Aravind K. Joshi, L. S. Levy, and M. Takahashi.
1975.Tree adjunct grammars.
Journal of Computer andSystem Sciences, 10(1).Aravind K. Joshi.
1985.
How much context-sensitivity is necessary for characterizing structuraldescriptions?tree adjoining grammars.
NaturalLanguage Processing?Theoretical, Computational,and Psychological Perspectives.Alon Lavie, Alok Parlikar, and Vamshi Ambati.
2008.Syntax-driven learning of sub-sentential translationequivalents and translation rules from parsed parallelcorpora.
In Proc.
SSST.Daniel Marcu, Wei Wang, Abdessamad Echihabi, andKevin Knight.
2006.
SPMT: Statistical machinetranslation with syntactified target language phrases.In Proc.
EMNLP.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn treebank.
Computa-tional Linguistics, 19(2).Jose?
B. Marin?o, Rafael E. Banchs, Josep M. Crego,Adria` de Gispert, Patrik Lambert, Jose?
A. R. Fonol-losa, and Marta R. Costa-jussa`.
2006.
N-gram-based machine translation.
Computational Linguis-tics, 32(4).Rebecca Nesson, Stuart M. Shieber, and AlexanderRush.
2006.
Induction of probabilistic synchronoustree-insertion grammars for machine translation.
InProc.
AMTA.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
ACL.Yves Schabes and Richard C. Waters.
1995.
Treeinsertion grammar: A cubic-time, parsable formal-ism that lexicalizes context-free grammar withoutchanging the trees produced.
Computational Lin-guistics, 21(4).Stuart M. Shieber and Yves Schabes.
1990.
Syn-chronous tree-adjoining grammars.
In Proc.
COL-ING.Stuart M. Shieber.
2007.
Probabilistic synchronoustree-adjoining grammars for machine translation:The argument from bilingual dictionaries.
In Proc.SSST Wkshp., NAACL-HLT.Kumar Vijay-Shanker.
1987.
A study of tree adjoininggrammars.
Ph.D. thesis.Wei Wang, Kevin Knight, and Daniel Marcu.
2007.Binarizing syntax trees to improve syntax-basedma-chine translation accuracy.
In Proc.
EMNLP andCoNLL.736
