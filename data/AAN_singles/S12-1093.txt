First Joint Conference on Lexical and Computational Semantics (*SEM), pages 631?634,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsBUAP: Three Approaches for Semantic Textual SimilarityMaya Carrillo, Darnes Vilarin?o, David Pinto, Mireya Tovar, Saul Leo?n, Esteban CastilloBeneme?rita Universidad Auto?noma de Puebla,Faculty of Computer Science14 Sur & Av.
San Claudio, CUPuebla, Puebla, Me?xico{cmaya, darnes, dpinto, mtovar}@cs.buap.mxsaul.ls@live.com, ecjbuap@gmail.comAbstractIn this paper we describe the three approacheswe submitted to the Semantic Textual Similar-ity task of SemEval 2012.
The first approachconsiders to calculate the semantic similar-ity by using the Jaccard coefficient with termexpansion using synonyms.
The second ap-proach uses the semantic similarity reportedby Mihalcea in (Mihalcea et al, 2006).
Thethird approach employs Random Indexing andBag of Concepts based on context vectors.
Weconsider that the first and third approaches ob-tained a comparable performance, meanwhilethe second approach got a very poor behav-ior.
The best ALL result was obtained withthe third approach, with a Pearson correlationequal to 0.663.1 IntroductionFinding the semantic similarity between two sen-tences is very important in applications of naturallanguage processing such as information retrievaland related areas.
The problem is complex due to thesmall number of terms involved in sentences whichare tipically less than 10 or 15.
Additionally, it is re-quired to ?understand?
the meaning of the sentencesin order to determine the ?semantic?
similarity oftexts, which is quite different of finding the lexicalsimilarity.There exist different works at literature dealingwith semantic similarity, but the problem is far tobe solved because of the aforementioned issues.In (Mihalcea et al, 2006), for instance, it is pre-sented a method for measuring the semantic simi-larity of texts, using corpus-based and knowledge-based measures of similarity.
The approaches pre-sented in (Shrestha, 2011) are based on the VectorSpace Model, with the aim to capture the contex-tual behavior, senses and correlation, of terms.
Theperformance of the method is better than the base-line method that uses vector based cosine similaritymeasure.In this paper, we present three different ap-proaches for the Textual Semantic Similarity task ofSemeval 2012 (Agirre et al, 2012).
The task is de-scribed as follows: Given two sentences s1 and s2,the aim is to compute how similar s1 and s2 are,returning a similarity score, and an optional confi-dence score.
The approaches should provide valuesbetween 0 and 5 for each pair of sentences.
Thesevalues roughly correspond to the following consid-erations, even when the system should output realvalues:5: The two sentences are completely equivalent,as they mean the same thing.4: The two sentences are mostly equivalent, butsome unimportant details differ.3: The two sentences are roughly equivalent, butsome important information differs/missing.2: The two sentences are not equivalent, but sharesome details.1: The two sentences are not equivalent, but areon the same topic.0: The two sentences are on different topics.631The description of the runs submitted to the com-petition follows.2 Experimentation setupThe three runs submitted to the competition usecompletely different mechanisms to find the degreeof semantic similarity between two sentences.
Theapproaches are described as follows:2.1 Approach BUAP-RUN-1: Term expansionwith synonymsLet s1 = w1,1w1,2...w1,|s1| and s2 =w2,1w2,2...w2,|s2| be two sentences.
The synonymsof a given word wi,k, expressed as synonyms(wi,k),are obtained from online dictionaries by extractingthe synonyms of wi,k.
A better matching betweenthe terms contained in the text fragments and theterms at the dictionary are obtained by stemming allthe terms (using the Porter stemmer).In order to determine the semantic similarity be-tween any pair of terms of the two sentences (w1,iand w2,j) we use Eq.
(1).sim(w1,i, w2,j) =????????
?1 if (w1,i == w2,j) ||w1,i ?
synonyms(w2,j) ||w2,j ?
synonyms(w1,i)0 otherwise(1)The similarity between sentences s1 and s2 is cal-culated as shown in Eq.
(2).similarity(s1, s2) =5 ?
?ni=1?nj=1 sim(w1i, w2j)|s1 ?
s2|(2)2.2 Approach BUAP-RUN-2In this approach, the similarity of s1 and s2 is calcu-lated as shown in Eq.
(3) (Mihalcea et al, 2006).similarity(s1, s2) = 12 (?w?{s1}(maxSim(w,s2)?idf(w))?w?{s1}idf(w)+?w?{s2}(maxSim(w,s1)?idf(w))?w?
{s2}idf(w) )(3)where idf(w) is the inverse document frequency ofthe word w, and maxSim(w, s2) is the maximumlexical similarity between the word w in sentence s2and all the words in sentence s2 calculated by meansof the Eq.
(4) reported by (Wu and Palmer, 1994).The sentence terms are assumed to be concepts, LCSis the depth of the least common subsumer, and theequation is calculated using the NLTK libraries1.Simwup =2 ?
depth(LCS)depth(concept1) + depth(concept2)(4)2.3 Approach BUAP-RUN-3: RandomIndexing and Bag of ConceptsThe vector space model (VSM) for document rep-resentation supporting search is probably the mostwell-known IR model.
The VSM assumes that termvectors are pair-wise orthogonal.
This assumptionis very restrictive because words are not indepen-dent.
There have been various attempts to buildrepresentations for documents that are semanticallyricher than only vectors based on the frequency ofterms occurrence.
One example is Latent Seman-tic Indexing (LSI), a method of word co-occurrenceanalysis to compute semantic vectors (context vec-tors) for words.
LSI applies singular-value decom-position (SVD) to the term-document matrix in or-der to construct context vectors.
As a result the di-mension of the produced vector space will be signif-icantly smaller; consequently the vectors that repre-sent terms cannot be orthogonal.
However, dimen-sion reduction techniques such as SVD are expen-sive in terms of memory and processing time.
Per-forming the SVD takes time O (nmz), where n isthe vocabulary size, m is the number of documents,and z is the number of nonzero elements per columnin the words-by-documents matrix.
As an alterna-tive, there is a vector space methodology called Ran-dom Indexing (RI) (Sahlgren, 2005), which presentsan efficient, scalable, and incremental method forbuilding context vectors.
Its computational com-plexity is O (nr) where n is as previously describedand r is the vector dimension.
Particularly, we applyRI to capture the inherent semantic structure usingBag of Concepts representation (BoC) as proposedby Sahlgren and Co?ster (Sahlgren and Co?ster, 2004),where the meaning of a term is considered as thesum of contexts in which it occurs.1http://www.nltk.org/6322.3.1 Random IndexingRandom Indexing (RI) is a vector space method-ology that accumulates context vectors for wordsbased on co-occurrence data.
The technique can bedescribed as:?
First a unique random representation known asindex vector is assigned to each context (docu-ment).
Index vectors are binary vectors with asmall number of non-zero elements, which areeither +1 or -1, with equal amounts of both.For example, if the index vectors have twentynon-zero elements in a 1024-dimensional vec-tor space, they have ten +1s and ten -1s.
Indexvectors serve as indices or labels for documents?
Index vectors are used to produce context vec-tors by scanning through the text and everytime a target word occurs in a context, the in-dex vector of the context is added to the con-text vector of the target word.
Thus, at eachencounters of the target word t with a context cthe context vector of t is updated as follows: ct+ = ic where ct is the context vector of t and icis the index vector of c. In this way, the contextvector of a word keeps track of the contexts inwhich it occurred.RI methodology is similar to latent semantic in-dexing (LSI) (Deerwester et al, 1990).
However,to reduce the co-occurrence matrix no dimension re-duction technique such as SVD is needed, since thedimensionality d of the random index vectors is pre-established as a parameter (implicit dimension re-duction).
Consequently d does not change once ithas been set; as a result, the dimensionality of con-text vectors will never change with the addition ofnew data.2.3.2 Bag of ConceptsBag of Concepts (BoC) is a recent representa-tion scheme proposed by Sahlgren and Co?ster in(Sahlgren and Co?ster, 2004), which is based on theperception that the meaning of a document can beconsidered as the union of the meanings of its terms.This is accomplished by generating term contextvectors from each term within the document, andgenerating a document vector as the weighted sumof the term context vectors contained within thatdocument.
Therefore, we use RI to represent themeaning of a word as the sum of contexts (entiredocuments) in which it occurs.
Illustrating this tech-nique, suppose you have two documents: D1: A manwith a hard hat is dancing, and D2: A man wearinga hard hat is dancing.
Let us suppose that they haveindex vectors ID1 and ID2, respectively: the contextvector for hat will be the ID1 + ID2, because thisword appears in both documents.
Once the contextvectors have been built by RI, they are used to repre-sent the document as BoC.
For instance, supposingCV1, CV2, CV3, .
.
.
and CV8, are the context vec-tors of each word in D1, then document D1 will berepresented as the weighted sum of these eight con-text vectors.2.3.3 ImplementationThe sentences of each file were processed to gen-erate the BoC representations of them.
BoC rep-resentations were generated by first stemming allwords in the sentences.
We then used random index-ing to produce context vectors for each word in thefiles (i.e.
STS.input.MSRpar, STS.input.MSRvid,etc.
), each file was considered a different corpus anddocuments were the sentences in them.
The dimen-sion of the context vectors was fixed at 2048, de-termined by experimentation using the training set.These context vectors were then tf ?
idf -weighted,according to the corpus, and added up for each sen-tence, to produce BoC representations.
Thereforethe similarity values were calculated by the cosinefunction.
Finally cosine values were multiplied by 5to produce values between 0 and 5.3 Experimental resultsIn Table 1 we show the results obtained by thethree approaches submitted to the competition.
Thecolumns of Table 1 stand for:?
ALL: Pearson correlation with the gold stan-dard for the five datasets, and correspondingrank.?
ALLnrm: Pearson correlation after the systemoutputs for each dataset are fitted to the goldstandard using least squares, and correspondingrank.633Run ALL Rank ALLnrmRankNrmMean RankMeanMSRparMSRvidSMTeurOn -WNSMT-newsBUAP-RUN-10.4997 63 0.7568 62 0.4892 57 0.4037 0.6532 0.4521 0.605 0.4537BUAP-RUN-2-0.026 89 0.5933 89 0.0669 89 0.1109 0.0057 0.0348 0.1788 0.1964BUAP-RUN-30.663 25 0.7474 64 0.488 59 0.4018 0.6378 0.4758 0.5691 0.4057Table 1: Results of approaches of BUAP in Task 6.?
Mean: Weighted mean across the 5 datasets,where the weight depends on the number ofpairs in the dataset.Followed by Pearson for individual datasets.At this moment, we are not aware of the reasonsbecause the second approach obtained a very poorperformance.
The way in which the idf(w) is calcu-lated could be one of the reasons, because the corpusused is relatively small and also from a different do-main.
With respect to the other two approaches, weconsider that they (first and third) obtained a com-parable performance, even when the third approachobtained the best ALL result with a Pearson correla-tion equal to 0.663.4 Discussion and conclusionWe have presented three different approaches fortackling the problem of Semantic Textual Similarity.The use of term expansion by synonyms performedwell in general and obtained a comparable behaviorthan the third approach which used random index-ing and bag of concepts.
It is interesting to observethat these two approaches performed similar whenthe two term expansion mechanism are totally dif-ferent.
As further, it is important to analyze the poorbehavior of the second approach.
We would like alsoto introduce semantic relationships other than syn-onyms in the process of term expansion.AcknowledgmentsThis project has been partially supported byprojects CONACYT #106625, #VIAD-ING11-II,PROMEP/103.5/11/4481 and VIEP #PIAD-ING11-II.ReferencesE.
Agirre, D. Cer, M. Diab, and B. Dolan.
2012.SemEval-2012 Task 6: Semantic Textual Similarity.In Proceedings of the 6th International Workshop onSemantic Evaluation (SemEval 2012).Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-dauer, George W. Furnas, and Richard A. Harshman.1990.
Indexing by Latent Semantic Analysis.
Jour-nal of the American Society of Information Science,41(6):391?407.Rada Mihalcea, Courtney Corley, and Carlo Strapparava.2006.
Corpus-based and knowledge-based measuresof text semantic similarity.
In proceedings of AAAI?06,pages 775?780.Magnus Sahlgren and Rickard Co?ster.
2004.
Using bag-of-concepts to improve the performance of supportvector machines in text categorization.
In Proceedingsof the 20th international conference on ComputationalLinguistics, COLING ?04, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.M.
Sahlgren.
2005.
An Introduction to Random Index-ing.
Methods and Applications of Semantic IndexingWorkshop at the 7th International Conference on Ter-minology and Knowledge Engineering, TKE 2005.Prajol Shrestha.
2011.
Corpus-based methods for shorttext similarity.
In TALN 2011, Montpellier, France.Zhibiao Wu and Martha Palmer.
1994.
Verb semanticsand lexical selection.
In 32nd.
Annual Meeting of theAssociation for Computational Linguistics, pages 133?138, New Mexico State University, Las Cruces, NewMexico.634
