Proceedings of the TextGraphs-7 Workshop at ACL, pages 20?24,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsGraph Based Similarity Measures for Synonym Extraction from Parsed TextEinat MinkovDep.
of Information SystemsUniversity of HaifaHaifa 31905, Israeleinatm@is.haifa.ac.ilWilliam W. CohenSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213wcohen@cs.cmu.eduAbstractWe learn graph-based similarity measures forthe task of extracting word synonyms from acorpus of parsed text.
A constrained graphwalk variant that has been successfully ap-plied in the past in similar settings is shown tooutperform a state-of-the-art syntactic vector-based approach on this task.
Further, we showthat learning specialized similarity measuresfor different word types is advantageous.1 IntroductionMany applications of natural language processingrequire measures of lexico-semantic similarity.
Ex-amples include summarization (Barzilay and El-hadad, 1999), question answering (Lin and Pantel,2001), and textual entailment (Mirkin et al, 2006).Graph-based methods have been successfully ap-plied to evaluate word similarity using available on-tologies, where the underlying graph included wordsenses and semantic relationships between them(Hughes and Ramage, 2007).
Another line of re-search aims at eliciting semantic similarity measuresdirectly from freely available corpora, based on thedistributional similarity assumption (Harria, 1968).In this domain, vector-space methods give state-of-the-art performance (Pado?
and Lapata, 2007).Previously, a graph based framework has beenproposed that models word semantic similarity fromparsed text (Minkov and Cohen, 2008).
The un-derlying graph in this case describes a text cor-pus as connected dependency structures, accord-ing to the schema shown in Figure 1.
The toygraph shown includes the dependency analysis oftwo sentences: ?a major environmental disaster isFigure 1: A joint graph of dependency structuresunder way?, and ?combat the environmental catas-trophe?.
In the graph, word mentions (in circles)and word types (in squares) are both representedas nodes.
Each word mention is linked to itscorresponding word type; for example, the nodes?environmental3?
and ?environmental204?
representdistinct word mentions and both nodes are linkedto the word type ?environmental?.1 For every edgein the graph, there exists an edge in the oppo-site direction (not shown in the figure).
In thisgraph, the terms disaster and catastrophe are re-lated due to the connecting path disaster ??
disaster3amod?inverse??
environmental3 ??
environmental ?
?environmental204 amod??
catastrophe204 ??
catastrophe .Given a query, which consists of a word of inter-est (e.g., ?disaster?
), various graph-based similaritymetrics can be used to assess inter-node relatedness,so that a list of nodes ranked by their similarity tothe query is returned to the user.
An advantage ofgraph-based similarity approaches is that they pro-duce similarity scores that reflect structural infor-1We will sometimes refer to word types as terms.20mation in the graph (Liben-Nowell and Kleinberg,2003).
Semantically similar terms are expected toshare connectivity patterns with the query term inthe graph, and thus appear at the top of the list.Notably, different edge types, as well as the pathstraversed, may have varying importance for differ-ent types of similarity sought.
For example, in theparsed text domain, noun similarity and verb sim-ilarity are associated with different syntactic phe-nomena (Resnik and Diab, 2000).
To this end, weconsider a path constrained graph walk (PCW) al-gorithm, which allows one to learn meaningful pathsgiven a small number of labeled examples and incor-porates this information in assessing node related-ness in the graph (Minkov and Cohen, 2008).
PCWhave been successfully applied to the extraction ofnamed entity coordinate terms, including city andperson names, from graphs representing newswiretext (Minkov and Cohen, 2008), where the special-ized measures learned outperformed the state-of-the-art dependency vectors method (Pado?
and Lap-ata, 2007) for small- and medium-sized corpora.In this work, we apply the path constrained graphwalk method to the task of eliciting general wordrelatedness from parsed text, conducting a set of ex-periments on the task of synonym extraction.
Whilethe tasks of named entity extraction and synonymextraction from text have been treated separately inthe literature, this work shows that both tasks can beaddressed using the same general framework.
Ourresults are encouraging: the PCW model yields su-perior results to the dependency vectors approach.Further, we show that learning specialized similar-ity measures per word type (nouns, verbs and adjec-tives) is preferable to applying a uniform model forall word types.2 Path Constrained Graph WalksPCW is a graph walk variant proposed recently thatis intended to bias the random walk process to fol-low meaningful edge sequences (paths) (Minkovand Cohen, 2008).
In this approach, rather than as-sume fixed (possibly, uniform) edge weight param-eters ?
for the various edge types in the graph, theprobability of following an edge of type ?
from nodex is evaluated dynamically, based on the history ofthe walk up to x.The PCW algorithm includes two components.First, it should provide estimates of edge weightsconditioned on the history of a walk, based on train-ing examples.
Second, the random walk algorithmhas to be modified to maintain historical informationabout the walk compactly.In learning, a dataset of N labelled examplequeries is provided.
The labeling schema is binary,where a set of nodes considered as relevant answersto an example query ei, denoted as Ri, is specified,and graph nodes that are not explicitly included inRi are assumed irrelevant to ei.
As a starting point,an initial graph walk is applied to generate a rankedlist of graph nodes li for every example query ei.
Apath-tree T is then constructed that includes all ofthe acyclic paths up to length k leading to the topM+ correct and M?
incorrect nodes in each of theretrieved lists li.
Every path p is associated witha maximum likelihood probability estimate Pr(p)of reaching a correct node based on the number oftimes the path was observed in the set of correct andincorrect target nodes.
These path probabilities arepropagated backwards in the path tree to reflect theprobability of reaching a correct node, given an out-going edge type and partial history of the walk.Given a new query, a constrained graph walk vari-ant is applied that adheres both to the topology of thegraph G and the path tree T .
In addition to trackingthe graph node that the random walker is at, PCWmaintains pointers to the nodes of the path tree thatrepresent the walk histories in reaching that graphnode.
In order to reduce working memory require-ments, one may prune paths that are associated withlow probability of reaching a correct node.
This of-ten leads to gains in accuracy.3 Synonym ExtractionWe learn general word semantic similarity measuresfrom a graph that represents a corpus of parsed text(Figure 1).
In particular, we will focus on evalu-ating word synonymy, learning specialized modelsfor different word types.
In the experiments, wemainly compare PCW against the dependency vec-tors model (DV), due to Pado?
and Lapata (2007).In the latter approach, a word wi is representedas a vector of weighted scores, which reflect co-occurrence frequency with words wj , as well as21properties of the dependency paths that connect theword wi to word wj .
In particular, higher weightis assigned to connecting paths that include gram-matically salient relations, based on the obliquenessweighting hierarchy (Keenan and Comrie, 1977).For example, co-occurrence of word wi with wordwj over a path that includes the salient subject rela-tion receives higher credit than co-occurrences overa non-salient relation such as preposition.
In addi-tion, Pado?
and Lapata suggest to consider only asubset of the paths observed that are linguisticallymeaningful.
While the two methods incorporatesimilar intuitions, PCW learns meaningful paths thatconnect the query and target terms from examples,whereas DV involves manual choices that are task-independent.3.1 DatasetTo allow effective learning, we constructed a datasetthat represents strict word synonymy relations formultiple word types.
The dataset consists of 68 ex-amples, where each example query consists of a sin-gle term of interest, with its synonym defined as asingle correct answer.
The dataset includes nounsynonym pairs (22 examples), adjectives (24) andverbs (22).
Example synonym pairs are shown inTable 1.
A corpus of parsed text was constructedusing the British National Corpus (Burnard, 1995).The full BNC corpus is a 100-million word col-lection of samples of written and spoken contem-porary British English texts.
We extracted rele-vant sentences, which contained the synonymouswords, from the BNC corpus.
(The number of ex-tracted sentences was limited to 2,000 per word.
)For infrequent words, we extracted additional ex-ample sentences from Associated Press (AP) arti-cles included in the AQUAINT corpus (Bilotti et al,2007).
(Sentence count was complemented to 300per word, where applicable.)
The constructed cor-pus, BNC+AP, includes 1.3 million words overall.This corpus was parsed using the Stanford depen-dency parser (de Marneffe et al, 2006).2.
The parsedcorpus corresponds to a graph that includes about0.5M nodes and 1.7M edges.2http://nlp.stanford.edu/software/lex-parser.shtmlNouns movie : filmmurderer : assassinVerbs answered : repliedenquire : investigateAdjectives contemporary : moderninfrequent : rareTable 1: Example word synonym pairs: the left words areused as the query terms.3.2 ExperimentsGiven a query like {term=?movie?
}, we would liketo get synonymous words, such as film, to appearat the top of the retrieved list.
In our experimentalsetting, we assume that the word type of the queryterm is known.
Rather than rank all words (terms) inresponse to a query, we use available (noisy) part ofspeech information to narrow down the search to theterms of the same type as the query term, e.g.
for thequery ?film?
we retrieve nodes of type ?
=noun.We applied the PCW method to learn separatemodels for noun, verb and adjective queries.
Thepath trees were constructed using the paths leadingto the node known to be a correct answer, as wellas to the otherwise irrelevant top-ranked 10 terms.We required the paths considered by PCW to in-clude exactly 6 segments (edges).
Such paths rep-resent distributional similarity phenomena, allowinga direct comparison against the DV method.
In con-ducting the constrained walk, we applied a thresh-old of 0.5 to truncate paths associated with lowerprobability of reaching a relevant response, follow-ing on previous work (Minkov and Cohen, 2008).We implemented DV using code made available byits authors,3 where we converted the syntactic pat-terns specified to Stanford dependency parser con-ventions.
The parameters of the DV method wereset to medium context and oblique edge weightingscheme, which were found to perform best (Pado?and Lapata, 2007).
In applying a vector-space basedmethod, a similarity score needs to be computed be-tween every candidate from the corpus and the queryterm to construct a ranked list.
In practice, we usedthe union of the top 300 words retrieved by PCW ascandidate terms for DV.We evaluate the following variants of DV: hav-3http://www.coli.uni-saarland.de/?
pado/dv.html22Nouns Verbs Adjs AllCO-Lin 0.34 0.37 0.37 0.37DV-Cos 0.24 0.36 0.26 0.29DV-Lin 0.45 0.49 0.54 0.50PCW 0.47 0.55 0.47 0.49PCW-P 0.53 0.68 0.55 0.59PCW-P-U 0.49 0.65 0.50 0.54Table 2: 5-fold cross validation results: MAPing inter-word similarity computed using Lin?s mea-sure (Lin, 1998) (DV-Lin), or using cosine similarity(DV-Cos).
In addition, we consider a non-syntacticvariant, where a word?s vector consists of its co-occurrence counts with other terms (using a win-dow of two words); that is, ignoring the dependencystructure (CO-Lin).Finally, in addition to the PCW model describedabove (PCW), we evaluate the PCW approach in set-tings where random, noisy, edges have been elimi-nated from the underlying graph.
Specifically, de-pendency links in the graph may be associated withpointwise mutual information (PMI) scores of thelinked word mention pairs (Manning and Schu?tze,1999); edges with low scores are assumed to rep-resent word co-occurrences of low significance, andso are removed.
We empirically set the PMI scorethreshold to 2.0, using cross validation (PCW-P).4In addition to the specialized PCW models, we alsolearned a uniform model over all word types in thesesettings; that is, this model is trained using the unionof all training examples, being learned and tested us-ing a mixture of queries of all types (PCW-P-U).3.3 ResultsTable 2 gives the results of 5-fold cross-validationexperiments in terms of mean average precision(MAP).
Since there is a single correct answer perquery, these results correspond to the mean recipro-cal rank (MRR).5 As shown, the dependency vec-tors model applied using Lin similarity (DV-Lin)performs best among the vector-based models.
Theimprovement achieved due to edge weighting com-4Eliminating low PMI co-occurrences has been shown to bebeneficial in modeling lexical selectional preferences recently,using a similar threshold value (Thater et al, 2010).5The query?s word inflections and words that are seman-tically related but not synonymous were discarded from theranked list manually for evaluation purposes.pared with the co-occurrence model (CO-Lin) islarge, demonstrating that syntactic structure is veryinformative for modeling word semantics (Pado?
andLapata, 2007).
Interestingly, the impact of applyingthe Lin similarity measure versus cosine (DV-Cos)is even more profound.
Unlike the cosine measure,Lin?s metric was designed for the task of evaluatingword similarity from corpus statistics; it is based onthe mutual information measure, and allows one todownweight random word co-occurrences.Among the PCW variants, the specialized PCWmodels achieve performance that is comparable tothe state-of-the-art DV measure (DV-Lin).
Further,removing noisy word co-occurrences from the graph(PCW-P) leads to further improvements, yieldingthe best results over all word types.
Finally, thegraph walk model that was trained uniformly for allword types (PCW-P-U) outperforms DV-Lin, show-ing the advantage of learning meaningful paths.
No-tably, the uniformly trained model is inferior toPCW trained separately per word type in the samesettings (PCW-P).
This suggests that learning spe-cialized word similarity metrics is beneficial.4 DiscussionWe applied a path constrained graph walk variant tothe task of extracting word synonyms from parsedtext.
In the past, this graph walk method has beenshown to perform well on a related task, of extract-ing named entity coordinate terms from text.
Whilethe two tasks are typically treated distinctly, we haveshown that they can be addressed using the sameframework.
Our results on a medium-sized cor-pus were shown to exceed the performance of de-pendency vectors, a syntactic state-of-the-art vector-space method.
Compared to DV, the graph walk ap-proach considers higher-level information about theconnecting paths between word pairs, and are adap-tive to the task at hand.
In particular, we showed thatlearning specialized graph walk models for differentword types is advantageous.
The described frame-work can be applied towards learning other flavorsof specialized word relatedness models (e.g., hyper-nymy).
Future research directions include learningword similarity measures from graphs that integratecorpus statistics with word ontologies, as well as im-proved scalability (Lao and Cohen, 2010).23ReferencesRegina Barzilay and Michael Elhadad.
1999.
Text sum-marizations with lexical chains, in Inderjeet Mani andMark Maybury, editors, Advances in Automatic TextSummarization.
MIT.Matthew W. Bilotti, Paul Ogilvie, Jamie Callan, and EricNyberg.
2007.
Structured retrieval for question an-swering.
In SIGIR.Lou Burnard.
1995.
Users Guide for the British NationalCorpus.
British National Corpus Consortium, OxfordUniversity Computing Service, Oxford, UK.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InLREC.Zellig Harria.
1968.
Mathematical Structures of Lan-guage.
Wiley, New York.Thad Hughes and Daniel Ramage.
2007.
Lexical seman-tic relatedness with random graph walks.
In EMNLP.Edward Keenan and Bernard Comrie.
1977.
Nounphrase accessibility and universal grammar.
Linguis-tic Inquiry, 8.Ni Lao and William W. Cohen.
2010.
Fast query exe-cution for retrieval models based on path constrainedrandom walks.
In KDD.Liben-Nowell and J. Kleinberg.
2003.
The link predic-tion problem for social networks.
In CIKM.Dekang Lin and Patrick Pantel.
2001.
Discovery of infer-ence rules for question answering.
Natural LanguageEngineering, 7(4).Dekang Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In COLING-ACL.Chris Manning and Hinrich Schu?tze.
1999.
Founda-tions of Statistical Natural Language Processing.
MITPress.Einat Minkov and William W. Cohen.
2008.
Learninggraph walk based similarity measures for parsed text.In EMNLP.Shachar Mirkin, Ido Dagan, and Maayan Geffet.
2006.Integrating pattern-based and distributional similaritymethods for lexical entailment acquisition.
In ACL.Sebastian Pado?
and Mirella Lapata.
2007.
Dependency-based construction of semantic space models.
Compu-tational Linguistics, 33(2).Philip Resnik and Mona Diab.
2000.
Measuring verbsimilarity.
In Proceedings of the Annual Meeting ofthe Cognitive Science Society.Stefan Thater, Hagen F?
?urstenau, and Manfred Pinkal.2010.
Contextualizing semantic representations usingsyntactically enriched vector models.
In ACL.24
