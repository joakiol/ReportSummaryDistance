Learning Sequence-to-Sequence Correspondences from Parallel Corporavia Sequential Pattern MiningKaoru Yamamoto?
and Taku Kudo?
and Yuta Tsuboi?
and Yuji Matsumoto?
?Genomic Sciences Center, The Institute of Physical and Chemical Research1-7-22-E209, Suehiro-cho, Tsurumi-ku, Yokohama, 230-0045 Japankaorux@gsc.riken.go.jp?Graduate School of Information Science, Nara Institute of Science and Technology8916-5 Takayama, Ikoma, Nara, 630-0192 Japantaku-ku@is.aist-nara.ac.jp, matsu@is.aist-nara.ac.jp?Tokyo Research Laboratory, IBM Japan, Ltd.1623-14 Shimotsuruma, Yamato-shi, Kanagawa-ken, 242-8502 Japanyutat@jp.ibm.comAbstractWe present an unsupervised extraction ofsequence-to-sequence correspondences fromparallel corpora by sequential pattern mining.The main characteristics of our method aretwo-fold.
First, we propose a systematic wayto enumerate all possible translation pair can-didates of rigid and gapped sequences withoutfalling into combinatorial explosion.
Second,our method uses an efficient data structure andalgorithm for calculating frequencies in a con-tingency table for each translation pair candi-date.
Our method is empirically evaluated us-ing English-Japanese parallel corpora of 6 mil-lion words.
Results indicate that it works wellfor multi-word translations, giving 56-84% ac-curacy at 19% token coverage and 11% typecoverage.1 IntroductionThis paper addresses the problem of identifying ?multi-word?
(sequence-to-sequence) translation correspon-dences from parallel corpora.
It is well-known that trans-lation does not always proceed by word-for-word.
Thishighlights the need for finding multi-word translation cor-respondences.Previous works that focus on multi-word transla-tion correspondences from parallel corpora include nounphrase correspondences (Kupiec, 1993), fixed/flexiblecollocations (Smadja et al, 1996), n-gram word se-quences of arbitrary length (Kitamura and Matsumoto,1996), non-compositional compounds (Melamed, 2001),captoids (Moore, 2001), and named entities 1.In all of these approaches, a common problem seems tobe an identification of meaningful multi-word translationunits.
There are a number of factors which make han-dling of multi-word units more complicated than it ap-pears.
First, it is a many-to-many mapping which poten-tially leads to a combinatorial explosion.
Second, multi-word translation units are not necessarily contiguous, soan algorithm should not be hampered by the word adja-cency constraint.
Third, word segmentation itself is am-biguous for non-segmented languages such as Chinese orJapanese.
We need to resolve such ambiguity as well.In this paper, we apply sequential pattern mining tosolve the problem.
First, the method effectively avoids aninherent combinatorial explosion by concatenating pairsof parallel sentences into single bilingual sequences andapplying a pattern mining algorithm on those sequences.Second, it covers both rigid (gap-less) and gapped se-quences.
Third, it achieves a systematic way of enumer-ating all possible translation pair candidates, single- ormulti-word.
Note that some are overlapped to accountfor word segmentation ambiguity.
Our method is bal-anced by a conservative discovery of translation corre-spondences with the rationale that direct associations willwin over indirect ones, thereby resolving the ambiguity.2 Our Basic IdeaOur approach is illustrated in Figure 1.
We concatenatecorresponding parallel sentences into bilingual sequencesto which sequential pattern mining is applied.
By doingso, we obtain the following effects:?
It exhaustively generates all possible translation can-1As of this writing, we learn that Moore will present hisresults on named entity at EACL 2003.Parallel CorpusJapanesePreprocessingEnglishPreprocessingSequential Pattern MiningBilingual Sequence DatabaseMonolingual Patterns with Independet FrequencyBilingual Patterns with Co-occurrence FrequencySequence-to-Sequence Correspondence DiscoveryBilingual ExpressionsConcatenateFigure 1: Our Approachdidates, both rigid and gapped sequences, yet avoid-ing combinatorial explosion.?
It achieves an efficient calculation of a contingencytable in a single running of sequential pattern min-ing.In what follows, we describe sequential pattern miningand each module in Figure 1.2.1 Sequential Pattern MiningSequential pattern mining discovers frequent subse-quences as patterns in a sequence database (Agrawaland Srikant, 1995).
Here, a subsequence is an order-preserving item sequence where some gaps betweenitems are allowed.
In this paper, we write the support ofsubsequence s in sequence database S as supportS(s),meaning the occurrence frequency of s in S. The prob-lem is defined as follows:Given a set of sequences S, where each sequence con-sists of items, and a given a user-specified minimum sup-port ?, sequential pattern mining is to find all of thesubsequences whose occurrence frequency in the set S isno less than ?.A sequential pattern is different from N-gram patternin that the former includes a pattern with and withoutgaps and does not impose any limit on its length.
Thesecharacteristics in sequential pattern mining leads us to theidea of concatenating corresponding parallel sentencesinto a bilingual sequence database from which bilingualsequential patterns are mined efficiently.minsup = 2A00 C01 D02A10 B11 C12C20 B21 A22A30 A31 B32A00 C01 D02A10 B11 C12A22A30 A31 B32B11 C12B21 A22B32C01 D02C12C20 B21 A22ABCA10 B11 C12A30 B32BCA00 C01 D02A10 C12A    4AB 2AC 2B     3C     3sequence db pattern projected db pattern projected dbpattern projected dbpattern projected dbpattern projected dbsup= 4sup= 3sup= 3sup= 2sup= 2resultsDFigure 2: A Sample Execution of PrefixSpan2.2 Bilingual Lexicon Extraction2.2.1 Bilingual Sequence DatabaseFor each parallel sentence, we undergo language-dependent preprocessing, such as word segmentation andpart-of-speech tagging.
Then we concatenate the mono-lingual sequences into a single bilingual sequence, anda collection of bilingual sequences becomes a sequencedatabase S.2.2.2 Sequential Pattern MiningA single run of sequential pattern mining takes care ofidentifying and counting translation candidate patterns ?rigid and gapped, some of which are overlapped ?
in thebilingual sequence database.
All English subsequencessatisfying the minimum support ?
will be generated (e.g.,?e1?, ?e1e2?, ?e1e3?
?
?
?, indicated by Ei ).
Similarly, allJapanese and bilingual subsequences with support ?
?will be generated (indicated by Jj and EiJj respectively).It is important to point out that for any bilingual patternEiJj , corresponding English pattern Ei and Japanesepattern Jj that form constituents of the bilingual patternare always recognized and counted.PrefixSpanIn order to realize sequential pattern mining, we usePrefixSpan algorithm (Pei et al, 2001).
The general ideais to divide the sequence database by frequent prefix andto grow the prefix-spanning patterns in depth-first searchfashion.We introduce some concepts.
Let ?
be a sequentialpattern in the sequence database S. Then, we refer to the?-projected database, S|?, as the collection of postfixesof sequences in S w.r.t prefix ?.A running example of PrefixSpan with the minimumsupport ?
= 2 (i.e., mining of sequential patterns withfrequency ?
2) is shown in Figure 2.
Each item in a se-function PrefixSpan (?, S|?
)beginB ?
{b|(s ?
S|?, b ?
s)?
(supportS |?(?b?)
?
?)?
(projectable(?, b))}foreach b ?
Bbegin(S|?
)|b ?
{?i, s?
?|(?i, s?
?
S|?)?
(s?
= postfix(s, b))}call PrefixSpan (?b, (S|?
)|b)endendFigure 3: Pseudo Code of PrefixSpanquence database is indicated by eij where e is an item,i is a sequence id, j is the offset for the postfix of se-quence id i.
First, frequent sequential patterns with length1 are selected.
This gives A,B and C. The support ofD is less than the minimum support 2, so D-projecteddatabase will not be created.
For projections drawn withbold lines in Figure 2, we proceed with a frequent prefixA.
Since A satisfies the minimum support 2, it creates aA-projected database derived from the sequence databaseS (S|A).
From S|A, frequent items B,C are identified,subsequently forming prefix patterns AB and AC, andcorresponding projected databases of the postfixes S|AB ,S|AC .
We continue with projection recursively to mineall sequential patterns satisfying the minimum supportcount 2.PrefixSpan is described in Figure 3.
The predicate pro-jectable is designed to encode if a projection is feasiblein an application domain.
The original PrefixSpan givesa predicate that always returns true.There are a number of possibilities for projectable toreflect linguistic constraints.
A default projectable pred-icate covers both rigid and gapped sequences satisfyingthe minimum support.
If we care for word adjacency, theprojectable should return true only when the last item ofthe mined pattern and the first item of a postfix sequencein the projected database are contiguous.
Another possi-bility is to prevent a certain class of words from being anitem of a sequence.
For example, we may wish to finda sequence consisting only of content words.
In such acase, we should disallow projections involving functionalword item.2.2.3 Sequence-to-Sequence CorrespondenceThe effect of sequential pattern mining from bilingualsequence database can better be seen in a contingencytable shown in Table 1.
Frequencies of a bilingual patternEiJj , an English pattern Ei, and a Japanese pattern Jjcorrespond to a, a + b, and a + c respectively.
Sincewe know the total number of bilingual sequences N =a + b + c + d, values of b, c and d can be calculatedimmediately.Table 1: Contingency TableJj ?
JjEi a b a + b?
Ei c da + c NThe contingency table is used for calculating a sim-ilarity (or association) score between Ei and Jj .
Forthis present work, we use Dunning?s log-likelihood ratiostatistics (Dunning, 1993) defined as follows:sim = a log a+ b log b+ c log c+ d log d?
(a+ b) log (a+ b)?
(a+ c) log (a+ c)?
(b+ d) log (b+ d)?
(c+ d) log (c+ d)+(a+ b+ c+ d) log (a+ b+ c+ d)For each bilingual pattern EiJj , we compute its similarityscore and qualify it as a bilingual sequence-to-sequencecorrespondence if no equally strong or stronger associ-ation for monolingual constituent is found.
This step isconservative and the same as step 5 in Moore (2001) orstep 6(b) in Kitamura and Matsumoto (1996).
Our im-plementation uses a digital trie structure called DoubleArray for efficient storage and retrieval of sequential pat-terns (Aoe, 1989).For non-segmented language, a word unit depends onresults of morphological analysis.
In case of Japanesemorphological analysis, ChaSen (Matsumoto et al, 2000)tends to over-segment words, while JUMAN (Kurohashiet al, 1994) tends to under-segment words.
It is diffi-cult to define units of correspondences only consultingthe Japanese half of parallel corpora.
A parallel sentence-pair may resolve some Japanese word segmentation am-biguity, however, we have no way to rank for word unitswith the same degree of segmentation ambiguity.
In-stead, we assume that frequently co-occurred sequence-to-sequence pairs in the entire parallel corpora are trans-lation pairs.
Using the global frequency of monolingualand bilingual sequences in the entire parallel corpora, wehave better chance to rank for the ties, thereby resolv-ing ambiguity in the monolingual half.
To follow thisintuition, we generate overlapped translation candidateswhere ambiguity exists, and extract ones with high asso-ciation scores.Sequential pattern mining takes care of translation can-didate generation as well as efficient counting of the gen-erated candidates.
This characteristic is well-suited forour purpose in generating overlapped translation candi-dates of which frequencies are efficiently counted.3 Experimental Results3.1 DataWe use the English-Japanese parallel corpora that areautomatically aligned from comparable corpora of thenews wires (Utiyama and Isahara, 2002).
There are150,000 parallel sentences which satisfy their proposedsentence similarity.
We use TnT (Brants, 2000) for En-glish POS tagging and ChaSen (Matsumoto et al, 2000)for Japanese morphological analysis, and label each to-ken to either content or functional depending on its part-of-speech.Table 2: Statistics of 150,000 parallel sentencesJapanese Englishcontent (token) 2,039,656 2,257,806content (type) 47,316 57,666functional (token) 2,660,855 1,704,189functional (type) 1,811 3863.2 Evaluation CriteriaWe evaluate our sequence-to-sequence correspondenceby accuracy and coverage, which we believe, similar cri-teria to (Moore, 2001) and (Melamed, 2001) 2.
Let Cseqbe the set of correct bilingual sequences by a humanjudge, Sseq be the set of bilingual sequences identifiedby our system, Ctoken be the multiset of items coveredby Cseq , Ttoken be the multiset of items in the bilingualsequence database, Ctype be the set of items covered byCseq , and Ttype be the set of items in the bilingual se-quence database.
Then, our evaluation metrics are givenby:accuracy = |Cseq||Sseq|token coverage = |Ctoken||Ttoken|2We would like to examine how many distinct translationpairs are correctly identified (accuracy) and how well the iden-tified subsequences can be used for partial sequence alignmentin the original parallel corpora (coverage).
Since all the correcttranslation pairs in our parallel corpora are not annotated, thesum of true positives and false negatives remain unknown.
Forthis reason, we avoid to use evaluation terms precision and re-call to emphasize the difference.
There are many variations ofevaluation criteria used in the literature.
At first, we try to useMoore?s criteria to present a direct comparison.
Unfortunately,we are unclear about frequency for multi-words in the parallelcorpora, which seems to require for the denominator of his cov-erage formula.
Further, we also did not split train/test corpusfor cross-validation.
Our method is an unsupervised learning,and the learning does not involve tuning parameters of a prob-abilistic model for unseen events.
So we believe results usingentire parallel corpora give indicative material for evaluation.type coverage = |Ctype||Ttype|In order to calculate accuracy, each translation pair iscompared against the EDR (Dictionary, 1995).
All theentries appeared in the dictionary were assumed to becorrect.
The remaining list was checked by hand.
Ahuman judge was asked to decide ?correct?, ?nearmiss?,or ?incorrect?
for each proposed translation pair with-out any reference to the surrounding context.
Distinc-tion between ?nearmiss?
and ?incorrect?
is that the for-mer includes translation pairs that are partially correct3.In Tables 3, 4, and 5, accuracy is given as a range froma combination of ?correct?
and ?nearmiss?
to a combi-nation of ?nearmiss?
and ?incorrect?.
Having calculatedthe total accuracy, accuracies for single-word translationpairs only and for multi-word translation pairs only arecalculated accordingly.3.3 ResultsOur method is implemented in C++, and executed ona 2.20 GHz Penntium IV processor with 2GB mem-ory.
For each experiment, we set the minimum support(minsup) and the maximum length (maxpat) of pat-terns.
All experiments target bilingual sequences of con-tent words only, since we feel that functional word cor-respondences are better dealt with by consulting the sur-rounding contexts in the parallel corpora4.
An executionof bilingual sequence databases compiled from 150,000sentences, takes less than 5 mins with minsup = 3 andmaxpat = 3, inferring 14312 translation pairs.Given different language pair, different genre of text,different evaluation criteria, we find it difficult to di-rectly compare our result with previous high-accuracy ap-proaches such as (Moore, 2001).
Below, we give an ap-proximate comparison of our empirical results.3.3.1 Rigid SequencesTable 3 shows a detailed result of rigid sequences withminsup = 3, maxpat = 3.
In total, we obtain 14312translation pairs, out of which we have 6567 single-word3We include ?not sure?
ones for a single-word translation.Those are entries which are correct in some context, but debat-able to include in a dictionary by itself.
As for multi-word trans-lation, we include pairs that can become ?correct?
in at most 2rewriting steps.4Inclusion of functional word items in bilingual sequencesis debatable.
We have conducted an preliminary experiment ofapprox 10,000 sentences taken from a English?Japanese dic-tionary.
As sentences are shorter and more instructive, we getgrammatical collocations such as ?impressed with / ni kanmei?
and ?apologize for / koto owabi?
or phrasal expressions suchas ?for your information / go sanko?
and ?on behalf of / wodaihyo shi?.
However, we felt that it was not practical to in-clude functional words in this work, since the parallel corporais large-scale and interesting translation pairs in newspaper arenamed entities comprised of mostly content words.Table 3: Result of Rigid Sequence Only with minsup = 3, maxpat = 3.
Accuracy is given as a range from a combinationof ?correct?
and ?nearmiss?
to a combination of ?nearmiss?
and ?incorrect?.
The left side of slash gives a tigherevaluation and the right side of slash gives a looser evaluation.minsup maxpat extracted correct total single-word multi-word token typesequence sequence accuracy accuracy accuracy coverage coverage3 3 1000 927 / 988 0.927 / 0.988 0.942 / 0.988 0.824 / 0.984 0.142 0.0183 3 2000 1836 / 1969 0.918 / 0.986 0.953 / 0.992 0.742 / 0.945 0.164 0.0353 3 3000 2723 / 2932 0.908 / 0.977 0.951 / 0.991 0.732 / 0.923 0.174 0.0503 3 4000 3563 / 3882 0.891 / 0.971 0.951 / 0.990 0.695 / 0.909 0.179 0.0643 3 5000 4330 / 4825 0.866 / 0.965 0.948 / 0.989 0.656 / 0.903 0.182 0.0763 3 6000 5052 / 5752 0.842 / 0.959 0.945 / 0.990 0.618 / 0.891 0.184 0.0873 3 7000 5776 / 6656 0.825 / 0.951 0.941 / 0.989 0.607 / 0.879 0.186 0.0983 3 8000 6350 / 7463 0.794 / 0.933 0.938 / 0.987 0.568 / 0.848 0.187 0.1043 3 9000 7034 / 8345 0.782 / 0.927 0.935 / 0.985 0.562 / 0.844 0.188 0.113Table 4: Result of Rigid Sequences Only with minsup = 10 and minsup = 5.minsup maxpat extracted correct total single-word multi-word token typesequence sequence accuracy accuracy accuracy coverage coverage10 3 4467 3989 / 4341 0.893 / 0.972 0.946 / 0.988 0.712 / 0.918 0.085 0.0115 3 7654 6325 / 7271 0.826 / 0.950 0.937 / 0.986 0.618 / 0.882 0.188 0.10610 10 4518 4002 / 4392 0.886 / 0.972 0.947 / 0.988 0.690 / 0.921 0.183 0.0735 10 8007 6383 / 7387 0.797 / 0.922 0.938 / 0.986 0.563 / 0.817 0.188 0.106Table 5: Result of Rigid and Gapped Sequences with minsup = 10.
A default projectable constraint in Figure 3 is used.minsup maxpat extracted correct total single-word multi-word token typesequence sequence accuracy accuracy accuracy coverage coverage10 3 5792 4503 / 4979 0.777 / 0.860 0.950 / 0.989 0.530 / 0.674 0.085 0.012Table 6: Comparison between Table 4 and Table 5 with minsup = 10, maxpat = 3single-word single-word single-word multi-word multi-word multi-wordcorrect wrong all correct wrong allBoth 3239 167 3406 554 181 735Rigid only 25 18 43 171 112 283Gapped only 0 2 2 710 937 1649Table 7: Length Distribution of 171 correct Rigid multi-word Sequences Only (left) vs.
Length Distribution of 112wrong Rigid multi-word Sequences Only (right)HHHHEJ 1 2 31 n/a 16 02 15 110 63 5 7 12HHHHEJ 1 2 31 n/a 11 02 19 29 193 3 24 7Table 8: Length Distribution of 710 correct Rigid and Gapped multi-word Sequences (left) vs.
Length Distribution of937 wrong Rigid and Gapped multi-word Sequences (right)HHHHEJ 1 2 31 n/a 17 02 45 546 153 9 43 35HHHHEJ 1 2 31 n/a 30 22 36 229 2393 15 162 226translation pairs and 7745 multi-word translation pairs.In this paper, we evaluate only the top 9000 pairs sortedby the similarity score.For single-word translation, we get 93-99% accuracyat 19% token coverage and 11% type coverage.
This im-plies that about 1/5 of content word tokens in the paral-lel corpora can find their correspondence with high ac-curacy.
We cannot compare our word alignment resultto (Moore, 2001), since the real rate of tokens that canbe aligned by single-word translation pairs is not explic-itly mentioned.
Although our main focus is sequence-to-sequence correspondences, the critical question remainsas to what level of accuracy can be obtained when ex-tending coverage rate, for example to 36%, 46% and90%.
Our result appears much inferior to Moore (2001)and Melamed (2001) in this respect and may not reach36% type coverage.
A possible explanation for the poorperformance is that our algorithm has no mechanism tocheck mutually exclusive constraints between translationcandidates derived from the same paired parallel sen-tence.For general multi-word translation, our method seemsmore comparable to Moore (2001).
Our method performs56-84% accuracy at 11% type coverage.
It seems bet-ter than ?compound accuracy?
which is his proposal ofhypothesizing multi-word occurrences, being 45-54% at12% type coverage.
However it is less favorable to ?mul-tiword accuracy?
provided by Microsoft parsers, being73-76% accuracy at 12% type coverage (Moore, 2001).The better performance could be attributed to our redun-dant generation of overlapped translation candidates inorder to account for ambiguity.
Although redundancyintroduces noisier indirect associations than one-to-onemapping, our empirical result suggests that there is still agood chance of direct associations being selected.Table 4 shows results of rigid sequences with a higherminimum support and a longer maximum length.
Com-paring with Table 3, setting a higher minimum supportproduces a slightly more cost-effective results.
For ex-ample, minsup = 10,maxpat = 3, there are 4467 pairsextracted with 89.3-97.1% accuracy, while the top 4000pairs in minsup = 3,maxpat = 3 are extracted with89.1-97.1% accuracy.
Table 4 reveals a drop in multi-word accuracy when extending minpat, indicating thatcare should be given to the length of a pattern as well asa cutoff threshold.Our analysis suggests that an iterative method by con-trolling minsup and maxpat appropriately seems bet-ter than a single execution cycle of finding correspon-dences.
It can take mutually exclusive constraints intoaccount more easily which will improve the overall per-formance.
Another interesting extension is to incorporatemore linguistically motivated constraints in generation ofsequences.
Yamamoto et al (2001) reports that N-gramtranslation candidates that do not go beyond the chunkboundary boosts performance.
Had we performed a lan-guage dependent chunking in preparation of bilingual se-quences, such a chunk boundary constraint could be sim-ply represented in the projectable predicate.
The issuesare left for future research.3.3.2 Gapped SequencesOne of advantages in our method is a uniform genera-tion of both rigid and gapped sequences simultaneously.Gapped sequences are generated and extracted withoutrecording offset and without distinguisting compositionalcompounds from non-compositional compounds.
Al-though non-compositional compounds are rare and moredifficult to extract, compositional compounds are stilluseful as collocational entires in bilingual dictionary.There are positive and negarive effects in our gappedsequences using sequential pattern mining.
Suppose wehave English sequences of ?My best friend wishes yourfather to visit ?
?
??
and ??
?
?
best wishes for success?.Then, we obtain a pattern ?best wishes?
that should becounted separately.
However, if we have sequences of?staying at Hilton hotel?
and ?staying at Kyoto Miyakohotel?, then we will obtain a kind of a phrasal template?staying at hotel?
where the individual name of hotel,Hilton or Kyoto Miyako, is abstracted.
Usefulness ofsuch gapped sequences is still open, but we empericallyevaluate the result of gapped sequences with minsup =10 and maxpat = 3 shown in Table 5.Comparing Table 4 and 5, we lose the multi-word ac-curacy substantially.
Table 6 is a breakdown of rigid andgapped sequences with minsup = 10, maxpat = 3.The ?Both?
row lists the number of pairs found, under acategory described in the column head, in both rigid andgapped sequences.
The ?Rigid only?
row counts for thoseonly found in rigid sequences, while the ?Gapped only?row counts for those only found in gapped sequence.
Welearn that the decrease in multi-word accuracy is due toan increase in the portion of wrong pairs in sequences;57% (937 / 1649) in gapped sequences whilst 40% (112 /283) in rigid sequences.However, gapped sequences have contributed to anincrease in the absolute number of correct multi-wordtranslation pairs (+539 correct pairs).
In order to gain abetter insight, we summarizes the length combination be-tween English pattern and Japanese pattern as reportedin Tables 7 and 8.
It reveals that the word adjacencyconstraint in rigid sequences are too stringent.
By relax-ing the constraint, 436 (546 - 110) correct 2-2 translationpairs are encountered, though 200 (229 - 29) wrong 2-2pairs are introduced at the same time.
At this particularinstance of minsup = 10 and maxpat = 3, consider-ing gapped sequence of length 3 seems to introduce morenoise.Admittedly, we still require further analysis as tosearching a break-even point of rigid/gapped sequences.Our preliminary finding supports the work on collocationby Smadja et al (1996) in that gapped sequences are alsoan important class of multi-word translations.4 Related WorkMoore (2001) presents insightful work which is closestto ours.
His method first computes an initial associationscore, hypothesizes an occurrence of compounds, fusesit to a single token, recomputes association scores as ifall translations are one-to-one mapping, and returns thehighest association pairs.
As for captoids, he also com-putes association of an inferred compound and its con-stituent words.
He also uses language-specific features(e.g.
capital letters, punctuation symbols) to identifylikely compound candidates.Our method is quite different in dealing with com-pounds.
First, we outsource a step of hypothesizing com-pounds to language-dependent preprocessors.
The reasonis that an algorithm will become complicated if language-specific features are directly embedded.
Instead, we pro-vide an abstract interface, namely the projectable predi-cate in sequential pattern mining, to deal with language-specific constraints.
Second, we allow items being re-dundantly counted and translation pair candidates beingoverlapped.
This sharply contrasts with Moore?s methodof replacing an identified compound to a single token foreach sentence pair.
In his method, word segmentationambiguity must be resolved before hypothesizing com-pounds.
Our method reserves a possibility for word seg-mentation ambiguity and resolves only when frequentlyco-occured sequence-to-sequence pairs are identified.Since we compute association scores independently, itis difficult to impose mutually exclusive constraints be-tween translation candidates derived from a paired par-allel sentence.
Hence, our method tends to suffer fromindirect association when the association score is low, aspointed out by Melamed (2001).
Although our methodrelies on an empirical observation that ?direct associa-tions are usually stronger than indirect association?, itseems effective enough for multi-word translation.
bal-anced by aAs far as we know, our method is the first attemptto make an exhaustive enumeration of rigid and gappedtranslation candidates of both languages possible, yetavoiding combinatorial explosion.
Previous approacheseffectively narrow down its search space by some heuris-tics.
Kupiec (1993) focuses on noun-phrase translationsonly, Smadja et al (1996) limits to find French transla-tion of English collocation identified by his Xtract sys-tem, and Kitamura and Matsumoto (1996) can exhaus-tively enumerate only rigid word sequences.Many of works mentioned in the last paragraph aswell as ours extract non-probabilistic translation lexicons.However, there are research works which go beyondword-level translations in statistical machine translation.One notable work is that of Marcu and Wong (2002),which is based on a joint probability model for statisticalmachine translation where word equivalents and phrase(rigid sequence) equivalents are automatically learnedform bilingual corpora.Our method does not iterate an extraction process asshown in Figure 1.
This could be a cause of poor perfor-mance in single-word translation pairs, since there is nomechanism for imposing mutually exclusion constrains.An interesting question then is what kind of iterationshould be performed to improve performance.
Prob-abilistic translation lexicon acquisition often uses EMtraining on Viterbi alignments, e.g.
(Marcu and Wong,2002), while non-probabilistic ones employ a greedy al-gorithm that extracts translation pairs that give higher as-sociation scores than a predefined threshold where thethreshold is monotonically decreasing as the algorithmproceeds, e.g.
(Kitamura and Matsumoto, 1996).
Theissue is left for future work.Last but not least, no previous works give an explicitmention to an efficient calculation of each cell in a con-tingency table.
Our approach completes the process bya single run of sequential pattern mining.
Since speeddoes not affect results of accuracy and coverage, its sig-nificance is often ignored.
However, it will be importantwhen we handle with corpora of large size.5 ConclusionsWe have proposed an effective method to find sequence-to-sequence correspondences from parallel corpora by se-quential pattern mining.
As far as multi-word translationis concerned, our method seems to work well, giving 56-84% accuracy at 19% token coverage and 11% type cov-erage.In this work, we choose English-Japanese pair and em-pirically evaluate our method.
However, we believe themethod is applicable to any language pair with appropri-ate language-specific preprocessing tools.
As by-productof our experiment, we obtain Japanese-English parallelcorpora of 150,000 sentences where alignment of vali-dated subsequence correspondences are back-annotated.This was accomplished by looking up to a Double Arraydictionary of sequential patterns constructed in the ex-traction method.
This shows that our method can be use-ful not only to development of semi-automatic lexicon fordata-driven machine translation, but also to annotation ofcorresponding subsequences in translation memory sys-tem.AcknowledgementWe would like to thank Masao Utiyama of CRL forcreating a large-scale English-Japanese parallel corpora,Thorsten Brants and ChaSen development team for mak-ing NLP tools publicly available.
In addition, we wouldlike to thank anonymous reviewers for useful commentsthat have helped preparation of this paper.ReferencesR.
Agrawal and R. Srikant.
1995.
Mining sequentialpatterns.
Proc.
1995 International Conference of VeryLarge DataBases (VLDB?95), pages 3?14.J.
Aoe.
1989.
An Efficient Digital Search Algorithm byUsing a Double-Array Structure.
IEEE Transactionson Software Engineering Vol.
15, 9, pages 1066?1077.T.
Brants.
2000.
TnT ?
A Statistical Part-of-Speech Tag-ger.
6th Applied Natural Language Processing Con-ference, pages 224?231.EDR Electronic Dictionary.
1995.http://www.iijnet.or.jp/edr.T.
Dunning.
1993.
Accurate Methods for the Statistics ofSurprise and Coincidence.
Computational Linguistics,Vol.19, No.1, pages 61?74.M.
Kitamura and Y. Matsumoto.
1996.
Automatic Ex-traction of Word Sequence Correspondences in Paral-lel Corpora.
Proc.
of the 4th Annual Workshop on VeryLarge Corpora (WVLC-4), pages 79?87.J.
Kupiec.
1993.
An Algorithm for Finding Noun PhraseCorrespondences in Bilingual Corpora.
31st AnnualMeeting of the Association for Computational Linguis-tics, pages 23?30.S.
Kurohashi, T. Nakamura, Y. Matsumoto, and M. Na-gao.
1994.
Improvements of japanese morphologicalanalyzer juman.
SNLR: Proceedings of the Interna-tional Workshop on Sharable Natural Language Re-sources, pages 22?28.D.
Marcu and W. Wong.
2002.
A Phrase-Based, JointProbability Model for Statistical Machine Translation.Proceedings of the 2002 Conference on EmpiricalMethods in Natural Language Processing, pages 133?139.Y.
Matsumoto, A. Kitamuchi, T. Yamashita, H. Matsuda,K.
Takaoka, and M. Asahara.
2000.
Morphologicalanalysis system chasen version 2.2.1 manual.
Nara In-stitute of Science and Technology.I.D.
Melamed.
2001.
Empirical Methods for ExploitingParallel Texts.
MIT Press.R.C.
Moore.
2001.
Towards a Simple and AccurateStatistical Approach to Learning Translation Relation-ships among Words.
ACL Workshop on Data-DrivenMachine Translation, pages 79?86.J.
Pei, B. Han, J. Mortazavi-Asl, H. Pinto, Q. Chen,U.
Dayal, and M. Hau.
2001.
Prefixspan: Mining se-quential patterns efficiently by prefix-projected patterngrowth.
Proc.
of International Conference of Data En-gineering (ICDE2001), pages 215?224.F.
Smadja, K.R.
McKeown, and V. Hatzuvassiloglou.1996.
Translating Collocations for Bilingual Lexi-cons: A Statistical Approach.
Computational Linguis-tics, 22(1):1?38.M.
Utiyama and H. Isahara.
2002.
Alingment ofJapanese?English News Articles and Sentences (inJapanese).
IPSJ SIG-NL 151, pages 15?21.K.
Yamamoto, Y. Matsumoto, and Kitamura M. 2001.
AComparative Study on Translation Units for BilingualLexicon Extraction.
ACL Workshop on Data-DrivenMachine Translation, pages 87?94.
