Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 1?8,Sydney, July 2006. c?2006 Association for Computational LinguisticsImproving Context Vector Models by Feature Clustering for Auto-matic Thesaurus ConstructionJia-Ming YouInstitute of Information ScienceAcademia Sinicaswimming@hp.iis.sinica.edu.twKeh-Jiann ChenInstitute of Information ScienceAcademia Sinicakchen@iis.sinica.edu.twAbstractThesauruses are useful resources for NLP;however, manual construction of thesau-rus is time consuming and suffers lowcoverage.
Automatic thesaurus construc-tion is developed to solve the problem.Conventional way to automatically con-struct thesaurus is by finding similarwords based on context vector modelsand then organizing similar words intothesaurus structure.
But the context vec-tor methods suffer from the problems ofvast feature dimensions and data sparse-ness.
Latent Semantic Index (LSI) wascommonly used to overcome the prob-lems.
In this paper, we propose a featureclustering method to overcome the sameproblems.
The experimental results showthat it performs better than the LSI mod-els and do enhance contextual informa-tion for infrequent words.1 IntroductionThesaurus is one of the most useful linguisticresources.
It provides information more than justsynonyms.
For example, in WordNet (Fellbaum,1998), it also builds up relations between syno-nym sets, such as hyponym, hypernym.
There aretwo Chinese thesauruses Cilin(1983) andHownet1.
Cilin provides synonym sets with sim-ple hierarchical structure.
Hownet uses someprimitive senses to describe word meanings.
Thecommon primitive senses provide additional re-lations between words implicitly.
However,many words occurred in contemporary news cor-pora are not covered by Chinese thesauruses.1 http://www.HowNet.com(Dong Zhendong, DongQiang:HowNet)Therefore, we intend to create a thesaurusbased on contemporary news corpora.
The com-mon steps to automatically construct a thesaurusinclude a) contextual information extraction, b)finding synonym words and c) organizing syno-nym words into a thesaurus.
The approach isbased upon the fact that word meaning lays on itscontextual behavior.
If words act similarly incontext, they may share the same meaning.However, the method can only handle frequentwords rather than infrequent ones.
In fact most ofvocabularies occur infrequently, one has to dis-cover extend information to overcome the datasparseness problem.
We will introduce the con-ventional approaches for automatic thesaurusconstruction in section 2.
Follow a discussionabout the problems and solutions of context vec-tor models in section 3.
In section 4, we use twoperformance evaluation metrics, i.e.
discrimina-tion and nonlinear interpolated precision, toevaluate our proposed method.2 Conventional approaches for auto-matic thesaurus constructionThe conventional approaches for automatic the-saurus construction include three steps: (1) Ac-quire contextual behaviors of words from cor-pora.
(2) Calculate the similarity between words.
(3) Finding similar words and then organizinginto a thesaurus structure.2.1 Acquire word sense knowledgeOne can model word meanings by their co-occurrence context.
The common ways to extractco-occurrence contextual words include simplewindow based and syntactic dependent based(You, 2004).
Obviously, syntactic dependentrelations carry more accurate information thanwindow based.
Also, it can bring additional in-formation, such as POS (part of speech) and se-mantic roles etc.
To extract the syntactic de-1;1 )(log)()rdentropy(wo ?= -=mkikwordpikwordpipended relation, a raw text has to be segmented,POS tagged, and parsed.
Then the relation ex-tractor identifies the head-modifier relationsand/or head-argument relations.
Each relationcould be defined as a triple (w, r, c), where w isthe thesaurus term, c is the co-occurred contextword and r is the relation between w and c.Then context vector of a word is representeddifferently by different models, such as: tf,weight-tf, Latent Semantic Indexing (LSI)(Deerwester, S.,et al, 1990) and ProbabilisticLSI (Hofmann, 1999).
The context vectors ofword x can be express by:a) tf model: word x = }tf...,,2tf,1{tf xnxx ,where xitf isthe term frequency of the ith context word whengiven word x.b) weight-tf model: assume there are n contex-tual words and m target words.
word x=,where weighti,  we used here, is defined as[logm-entropy(wordi)]/logm)(wordikpis the co-occurrence probability of wordk whengiven wordi.c) LSI or PLSI models: using tf or weighted-tfco-occurrence matrix and by adopting LSI orPLSI to reduce the dimension of the matrix.2.2 Similarity between wordsThe common similarity functions includea) Adopting simple frequency feature, such ascosine, which computes the angle between twocontext vectors;b) Represent words by the probabilistic distribu-tion among contexts, such as Kull-Leiber diver-gence (Cover and Thomas, 1991).The first step is to convert the co-occurrencematrix into a probabilistic matrix by simple for-mula.?
===?
===== n 1k yktfyitfxiq},xn,...qx2q,x1{q wordyn1kxktfxitfxip},xn,...px2p,x1{pwordxqpThen calculate the distance between probabil-istic vectors by sums up the all probabilistic dif-ference among each context word so called crossentropy.Due to the original KL distance is asymmetricand is not defined when zero frequency occurs.Some enhanced KL models were developed toprevent these problems such as Jensen-Shannon(Jianhua, 1991), which introducing a probabilis-tic variable m, or ?
-Skew Divergence (Lee,1999), by adopting adjustable variable ?.
Re-search shows that Skew Divergence achievesbetter performance than other measures.
(Lee,2001)))1(||(yxS  rgence)D(SkewDive yxxKL aaa -+==2/)(,2/)}||()||({y)x,(JS Shannon)-D(JensenyxmmyKLmxKL+=+==To convert distance to similarity value, weadopt the formula inspired by Mochihashi, andMatsumoto 2002.2.3 Organize similar words into thesaurusThere are several clustering methods can be usedto cluster similar words.
For example, by select-ing N target words as the entries of a thesaurus,then extract top-n similar words for each entry;adopting HAC(Hierarchical agglomerative clus-tering, E.M. Voorhees,1986) method to clusterthe most similar word pairs in each clusteringloop.
Eventually, these similar words will beformed into synonyms sets.3 Difficulties and SolutionsThere are two difficulties of using context vectormodels.
One is the enormous dimensions of con-}weight...tf2weight2tf,1weight1{tf nxxxn ???yxxy)cos(x,?
?= y)},distance(exp{wordy)(wordx,similarity yx?-= lii21 qplog)(q)KL(p, :Distance KL ?
?== ipni2textual words, and the other is data sparsenessproblem.
Conventionally LSI or PLSI methodsare used to reduce feature dimensions by map-ping literal words into latent semantic classes.The researches show that it?s a promisingmethod (April Kontostathis, 2003).
However thelatent semantic classes also smooth the informa-tion content of feature vectors.
Here we proposeda different approach to cope with the feature re-duction and data sparseness problems.3.1 Feature ClusteringReduced feature dimensions and data sparsenesscause the problem of inaccurate contextual in-formation.
In general, one has to reduce the fea-ture dimensions for computational feasibility andalso to extend the contextual word information toovercome the problem of insufficient contextinformation.In our experiments, we took the clustered-feature approaches instead of LSI to cope withthese two problems and showed better perform-ances.
The idea of clustered-feature approachesis by adopting the classes of clustering result ofthe frequent words as the new set of featureswhich has less feature dimensions and contextwords are naturally extend to their class mem-bers.
We followed the steps described in section2 to develop the synonyms sets.
First, the syntac-tic dependent relations were extracted to createthe context vectors for each word.
We adoptedthe skew divergence as the similarity function,which is reported to be the suitable similarityfunction (Masato, 2005), to measure the distancebetween words.We used HAC algorithm to develop the syno-nyms classes, which is a greedy method, simplyto cluster the most similar word pairs at eachclustering iteration.The HAC clustering process:While  the similarity of the most similar word pair(wordx, wordy) is greater than a threshold ?then cluster wordx, wordy together and replace it withthe centroid   between wordx and wordyRecalculate the similarity between other words andthe  centroid3.2 Clustered-Feature VectorsWe obtain the synonyms sets S from above HACmethod.
Let the extracted synonyms sets S = { S1,S2,?SR} which contains R synonym classes;ijS stands for the jth element of the ith synonymclass;  the ith synonym class Si contains Qi ele-ments.The feature extension processing transformsthe coordination from literal words to synonymssets.
Assume there are N contextual words{C1,C2,?CN}, and the first step is to transformthe context vector of of Ci to the distribution vec-tor among S. Then the new feature vector is thesummation of the distribution vectors among Sof its all contextual words.The new feature vector of wordj =?= ?Ni 1jitf Distribution_Vector_among_S( iC ),where jitf  is the term frequency of the contextword Ci occurs with wordj.Distribution_Vector_among_S( iC )= { }RSiPSiPSiP ,..., 21 ,.S synonyms  at the  of rdscontext woofon distributi  themeans,(Ci)1),(where,jjthCifreqQjqCijqSfreqSiPj?==Due to the transformed coordination no longerstands for either frequency or probability, we usesimple cosine function to measure the similaritybetween these transformed clustered-feature vec-tors.4 EvaluationTo evaluate the performance of the feature clus-tering method, we had prepared two sets of test-ing data with high and low frequency words re-spectively.
We want to see the effects of featurereduction and feature extension for both frequentand infrequent words.?????????????
?=RQRRRQQSSSSSSSSS.....................S2122222111121134.1 Discrimination RatesThe discrimination rate is used to examine thecapability of distinguishing the correlation be-tween words.
Given a word pair (wordi,wordj),one has to decide whether the word pair is simi-lar or not.
Therefore, we will arrange two differ-ent word pair sets, related and unrelated, to esti-mate the discrimination.
By given the formulabelow,where Na and Nb are respectively the numbersof synonym word pairs and unrelated word pairs.As well as, na and nb are the numbers of correctlabeled pairs in synonyms and unrelated words.4.2 Nonlinear interpolated precisionThe Nap evaluation is used to measure the per-formance of restoring words to taxonomy, asimilar task of restoring words in WordNet(Dominic Widdows, 2003).The way we adopted Nap evaluation is to re-construct a partial Chinese synonym set, andmeasure the structure resemblance betweenoriginal synonyms and the reconstructed one.
Bydoing so, one has to prepare certain number ofsynonyms sets from Chinese taxonomy, and tryto reclassify these words.Assume there are n testing words distributedin R synonyms sets.
Let i1R stands for the repre-sented word of the ith synonyms set.
Then wewill compute the similarity ranking between eachrepresented word and the rest n-1 testing words.By given formulaijS  represents the jth similar word of i1R  amongthe rest n-1 words?????
?= 0synonym are R and  S if,1 i1ijijZThe NAP value means how many percentsynonyms can be identified.
The maximum valueof NAP is 1, means the extracted similar wordsare exactly match to the synonyms.5 ExperimentsThe context vectors were derived from a 10year news corpus from The Central NewsAgency.
It contains nearly 33 million sentences,234 million word tokens, and we extracted 186million syntactic relations from this corpus.
Dueto the low reliability of infrequent data, only therelation triples (w, r, c), which occurs more than3 times and POS of w and c must be noun orverb, are used.
It results that nearly 30,000 highfrequent nouns and verbs are used as the contex-tual features.
And with feature clustering2, thecontextual dimensions were reduced from 30,988literal words to 12,032 semantic classes.In selecting testing data, we consider thewords that occur more than 200 times as highfrequent words and the frequencies range from40 to 200 as low frequent words.DiscriminationFor the discrimination experiments, we randomlyextract high frequent word pairs which include500 synonym pairs and 500 unrelated word pairsfrom Cilin (Mei et.
al, 1983).
At the mean time,we also prepare equivalent low frequency data.We use a mathematical technique SingularValue Decomposition (SVD) to derive principalcomponents and to implement LSI models withrespect to different feature dimensions from 100to 1000.
We compare the performances of differ-ent models.
The results are shown in the follow-ing figures.Figure1.
Discrimination for high frequent wordsThe result shows that for the high frequentdata, although the feature clustering method didnot achieve the best performance, it perform-ances better at related data and a balanced per-formance at unrelated data.
The tradeoffs be-2 Some feature clustering results are listed in the Ap-pendix??????
+= NbnbNana21ratetion Discrimina,1R1NAP111n1jR1i????????
+= ???
-=-==jkikij ZjZ4tween related recalls and unrelated recalls areclearly shown.
Another observation is that nomatter of using LSI or literal word features (tf orweight_tf), the performances are comparable.Therefore, we could simply use any method tohandle the high frequent words.Figure2 Discrimination for low frequent wordFor the infrequent words experiments, neitherLSI nor weighted-tf performs well due to insuffi-cient contextual information.
But by introducingfeature clustering method, one can gain more 6%accuracy for the related data.
It shows featureclustering method could help gather more infor-mation for the infrequent words.Nonlinear interpolated precisionFor the Nap evaluation, we prepared two testingdata from Cilin and Hownet.
In the high frequentwords experiments, we extract 1311 wordswithin 352 synonyms sets from Cilin and 2981words within 570 synonyms sets from Hownet.Figure 3.
Nap performance for high frequent wordsIn high frequent experiments, the results showthat the models retaining literal form performbetter than dimension reduction methods.
Itmeans in the task of measuring similarity of highfrequent words using literal contextual featurevectors is more precise than using dimensionreduction feature vectors.In the infrequent words experiments, we canonly extract 202 words distributed in 62 syno-nyms sets from Cilin and 1089 words within 222synonyms sets.
Due to fewer testing words, LSIwas not applied in this experiment.Figure 4.
Nap performance for low frequent wordsIt shows with insufficient contextual informa-tion, the feature clustering method could not helpin recalling synonyms because of dimensionalreduction.6.
Error Analysis and ConclusionUsing context vector models to construct thesau-rus suffers from the problems of large featuredimensions and data sparseness.
We propose afeature clustering method to overcome the prob-lems.
The experimental results show that it per-forms better than the LSI models in distinguish-ing related/unrelated pairs for the infrequent data,and also achieve relevant scores on other evalua-tions.Feature clustering method could raise the abil-ity of discrimination, but not robust enough toimprove the performance in extracting synonyms.It also reveals the truth that it?s easy to distin-guish whether a pair is related or unrelated oncethe word pair shares the same sense in theirsenses.
However, it?s not the case when seekingsynonyms.
One has to discriminate each sensefor each word first and then compute the similar-ity between these senses to achieve synonyms.Because feature clustering method lacks the abil-ity of senses discrimination of a word, themethod can handle the task of distinguishing cor-relation pairs rather than synonyms identification.Also, after analyzing discrimination errorsmade by context vector models, we found thatsome errors are not due to insufficient contextualinformation.
Certain synonyms have dissimilarcontextual contents for different reasons.
Weobserved some phenomenon of these cases:5a) Some senses of synonyms in testing data arenot their dominant senses.Take guang1hua2 (??)
for example, it has asense of ?splendid?
which is similar to the senseof guang1mang2 (??
).
Guang1hua2 andguang1mang2 are certainly mutually changeablein a certain degree, guang1hua2jin4shi4 (????)
and guang1mang2jin4shi4 (????
), orxi2ri4guang1hua2 ( ?
?
?
? )
andxi2ri4guang1mang2 (????).
However, thedominated contextual sense of guang1hua2 ismore likely to be a place name, likeguang1hua2shi4chang3( ?
?
?
? )
orhua1lian2guang1hua2 (????)
etc3.b) Some synonyms are different in usages forpragmatic reasons.Synonyms with different contextual vectorscould be result from different perspective views.For example, we may view wai4jie4 (??)
as acontainer image with viewer inside, but on theother hand, yi3wai4 (??)
is an omnipotenceperspective.
This similar meaning but differentperspective makes distinct grammatical usageand different collocations.Similarly, zhong1shen1 (??)
and sheng1ping2( ?
? )
both refer to ?life-long time?.zhong1shen1 explicates things after a time point,which differs from sheng1ping2, showing mat-ters before a time point.c) Domain specific usages.For example, in medical domain news ,wa1wa1(??)
occurs frequently with bo1li2 (??)
refer3 This may due to different genres.
In newspapers theproper noun usage of guang1hua2 is more commonthan in a literature text.to kind of illness.
Then the corpus reinterpretwa1wa1 (??)
as a sick people, due to it occurswith medical term.
But the synonym of wa1wa1(??
), xiao3peng2you3(??? )
stands formoney in some finance news.
Therefore, themeanings of words change from time to time.
It?shard to decide whether meaning is the right an-swer when finding synonyms.With above observations, our future researcheswill be how to distinguish different word sensesfrom its context features.
Once we could distin-guish the corresponding features for differentsenses, it will help us to extract more accuratesynonyms for both frequent and infrequent words.ReferencesApril Kontostathis,  William M. Pottenger 2003. ,  AFramework for Understanding LSI Performance ,  Inthe Proceedings of the ACM SIGIR Workshop onMathematical/Formal Methods in Information Re-trieval, Annual International SIGIR Conference, 2003.Christiance Fellbaum, editor 1998,WordNet: An alec-tronic lwxical database.
MIT press, Cambrige MA.Deerwester, S.,et al 1990 Indexing by Latent Seman-tic Analysis.
Jorunal of the American Society for In-formation Science, 41(6):391-407Dominic Widdows.
2003.
Unsupervised methods fordeveloping taxonomies by combining syntactic andstatistical information.
In Proceeding of HLT-NAACL2003 Main papers, pp, 197-204.E.M.
Voorhees, ?Implement agglomerative hierarchi-cal clustering algorithm for use in document re-trieval?, Information Processing & Management.
, no.22 pp.46-476,1986Hofmann, T.1999.
Probabilistic Latent  SemanticIndexing.
Proc.of the 22nd International conference onResearch and Development in Information Retrieval(SIGIR?99),50-57James R.Curran and Marc Moens.
2002.
Improve-ments in Automatic Thesaurus Extraction.
Proceed-ings of the Workshop of the ACL Special InterestGroup on the Lexicon (SIGLEX), pp.
59-66Jia-Ming You and Keh-Jiann Chen, 2004 AutomaticSemantic Role Assignment for a Tree Structure, Pro-Wai4jie4??Yi3wai4?
?Omnipotence viewer?Zhong1shen1???sheng1ping2?
?viewer6ceedings of 3rd ACL SIGHAN WorkshopJiahua Lin.
1991.
Divergence measures based on theShannon Entropy.
IEEE transactions on InformationTheory, 37(1): 145-151Lillian Lee.
2001.
On the effectiveness of the skewdivergence for statistical language analysis.
In Artifi-cial Intelligence and Statistics 2001, page 65-72.Lillian Lee.
1999.
Measure of distributional similarity.In Proceeding of the 37th Annual Meeting of the As-sociation for Computational Linguistics (ACL-1999),page 23-32.Masato Hagiwara, Yasuhiro Ogawa, and KatsuhikoToyama.
2005.
PLSI Utilization for Automatic The-saurus Construction.
IJCNLP 2005, LNAI 651, pp.334-345.Mei,Jiaju,Yiming Lan, Yunqi Gao, Yongxian Ying(1983) ?????
[ A Dictionary of Syno-nyms],Shanghai Cishu Chubanshe.Mochihashi, D., Matsumoto, Y.2002.
ProbabilisticRepresentation of Meanings.
IPSJ SIG Notes NaturalLanguage, 2002-NL-147:77-84.T.Cover and J.Thomas, 1991.
Element of InformationTheory.
Wiley & sons, New York7Appendix:Some feature clustering results??
??????
???
???
?????
??????
???
????
??
???
?????
??
?????
??
??
??????
????
??????
??
??
???
??
??
??
??
????
?????
??
??????
??????
??
??
??
????
??
??
??
?????
??????
?????
??
????
??
???
?????
???
??????
??
??
??
????
?????
???
??
????
??
??
??
???
????
??
??
??????
?????
??
???
??
???
??
?????
??
??
??
??
????
????
????
??
??
???
???
??
???
????
??
??
??
??
??
??
????
??
???
???
??????
???
???
??
???
???
??
???
???
???
?????
???
??
???
??
??
?????
??????
???
??????
???????
??
????
??
??
?????
??
????
??
????
????
??
??
????
???????
??
???
??
????
??
??????
??
?????
????
???
????
??
??
??
???
??
????
??
??
??
??
??
??
????
????
??
????
??
?????
???
??
????
??
??
????
??
????
??
???
???
??
??
??
??
??
??
???
??
????
??
??
????
???
??
??
????
??
??
?????
?????
??
?????
??
?????
???
????
?????
????
??
??
??????
????
??
??
?????
???
??
??
??
??
?????
??
????
??
???
??
??
?
?8
