Max-Margin ParsingBen TaskarComputer Science Dept.Stanford Universitybtaskar@cs.stanford.eduDan KleinComputer Science Dept.Stanford Universityklein@cs.stanford.eduMichael CollinsCS and AI LabMITmcollins@csail.mit.eduDaphne KollerComputer Science Dept.Stanford Universitykoller@cs.stanford.eduChristopher ManningComputer Science Dept.Stanford Universitymanning@cs.stanford.eduAbstractWe present a novel discriminative approach to parsinginspired by the large-margin criterion underlying sup-port vector machines.
Our formulation uses a factor-ization analogous to the standard dynamic programs forparsing.
In particular, it allows one to efficiently learna model which discriminates among the entire space ofparse trees, as opposed to reranking the top few candi-dates.
Our models can condition on arbitrary features ofinput sentences, thus incorporating an important kind oflexical information without the added algorithmic com-plexity of modeling headedness.
We provide an efficientalgorithm for learning such models and show experimen-tal evidence of the model?s improved performance overa natural baseline model and a lexicalized probabilisticcontext-free grammar.1 IntroductionRecent work has shown that discriminativetechniques frequently achieve classification ac-curacy that is superior to generative techniques,over a wide range of tasks.
The empirical utilityof models such as logistic regression and sup-port vector machines (SVMs) in flat classifica-tion tasks like text categorization, word-sensedisambiguation, and relevance routing has beenrepeatedly demonstrated.
For sequence taskslike part-of-speech tagging or named-entity ex-traction, recent top-performing systems havealso generally been based on discriminative se-quence models, like conditional Markov mod-els (Toutanova et al, 2003) or conditional ran-dom fields (Lafferty et al, 2001).A number of recent papers have consid-ered discriminative approaches for natural lan-guage parsing (Johnson et al, 1999; Collins,2000; Johnson, 2001; Geman and Johnson,2002; Miyao and Tsujii, 2002; Clark and Cur-ran, 2004; Kaplan et al, 2004; Collins, 2004).Broadly speaking, these approaches fall into twocategories, reranking and dynamic programmingapproaches.
In reranking methods (Johnsonet al, 1999; Collins, 2000; Shen et al, 2003),an initial parser is used to generate a numberof candidate parses.
A discriminative modelis then used to choose between these candi-dates.
In dynamic programming methods, alarge number of candidate parse trees are repre-sented compactly in a parse tree forest or chart.Given sufficiently ?local?
features, the decod-ing and parameter estimation problems can besolved using dynamic programming algorithms.For example, (Johnson, 2001; Geman and John-son, 2002; Miyao and Tsujii, 2002; Clark andCurran, 2004; Kaplan et al, 2004) describe ap-proaches based on conditional log-linear (max-imum entropy) models, where variants of theinside-outside algorithm can be used to effi-ciently calculate gradients of the log-likelihoodfunction, despite the exponential number oftrees represented by the parse forest.In this paper, we describe a dynamic pro-gramming approach to discriminative parsingthat is an alternative to maximum entropyestimation.
Our method extends the max-margin approach of Taskar et al (2003) tothe case of context-free grammars.
The presentmethod has several compelling advantages.
Un-like reranking methods, which consider onlya pre-pruned selection of ?good?
parses, ourmethod is an end-to-end discriminative modelover the full space of parses.
This distinctioncan be very significant, as the set of n-bestparses often does not contain the true parse.
Forexample, in the work of Collins (2000), 41% ofthe correct parses were not in the candidate poolof ?30-best parses.
Unlike previous dynamicprogramming approaches, which were based onmaximum entropy estimation, our method in-corporates an articulated loss function whichpenalizes larger tree discrepancies more severelythan smaller ones.1Moreover, like perceptron-based learning, itrequires only the calculation of Viterbi trees,rather than expectations over all trees (for ex-ample using the inside-outside algorithm).
Inpractice, it converges in many fewer iterationsthan CRF-like approaches.
For example, whileour approach generally converged in 20-30 iter-ations, Clark and Curran (2004) report exper-iments involving 479 iterations of training forone model, and 1550 iterations for another.The primary contribution of this paper is theextension of the max-margin approach of Taskaret al (2003) to context free grammars.
Weshow that this framework allows high-accuracyparsing in cubic time by exploiting novel kindsof lexical information.2 Discriminative ParsingIn the discriminative parsing task, we want tolearn a function f : X ?
Y, where X is a setof sentences, and Y is a set of valid parse treesaccording to a fixed grammar G. G maps aninput x ?
X to a set of candidate parses G(x) ?Y.2We assume a loss function L : X ?
Y ?Y ?
R+.
The function L(x, y, y?)
measures thepenalty for proposing the parse y?
for x when yis the true parse.
This penalty may be defined,for example, as the number of labeled spans onwhich the two trees do not agree.
In general weassume that L(x, y, y?)
= 0 for y = y?.
Givenlabeled training examples (xi, yi) for i = 1 .
.
.
n,we seek a function f with small expected losson unseen sentences.The functions we consider take the followinglinear discriminant form:fw(x) = arg maxy?G(x)?w,?
(x, y)?,1This articulated loss is supported by empirical suc-cess and theoretical generalization bound in Taskar et al(2003).2For all x, we assume here that G(x) is finite.
Thespace of parse trees over many grammars is naturally in-finite, but can be made finite if we disallow unary chainsand empty productions.where ?
?, ??
denotes the vector inner product,w ?
Rd and ?
is a feature-vector representationof a parse tree ?
: X ?
Y ?
Rd (see examplesbelow).3Note that this class of functions includesViterbi PCFG parsers, where the feature-vectorconsists of the counts of the productions usedin the parse, and the parameters w are the log-probabilities of those productions.2.1 Probabilistic EstimationThe traditional method of estimating the pa-rameters of PCFGs assumes a generative gram-mar that defines P (x, y) and maximizes thejoint log-likelihood ?i log P (xi, yi) (with someregularization).
A alternative probabilisticapproach is to estimate the parameters dis-criminatively by maximizing conditional log-likelihood.
For example, the maximum entropyapproach (Johnson, 2001) defines a conditionallog-linear model:Pw(y | x) =1Zw(x)exp{?w,?
(x, y)?
},where Zw(x) =?y?G(x) exp{?w,?
(x, y)?
}, andmaximizes the conditional log-likelihood of thesample, ?i log P (yi | xi), (with some regular-ization).2.2 Max-Margin EstimationIn this paper, we advocate a different estima-tion criterion, inspired by the max-margin prin-ciple of SVMs.
Max-margin estimation has beenused for parse reranking (Collins, 2000).
Re-cently, it has also been extended to graphicalmodels (Taskar et al, 2003; Altun et al, 2003)and shown to outperform the standard max-likelihood methods.
The main idea is to foregothe probabilistic interpretation, and directly en-sure thatyi = arg maxy?G(xi)?w,?
(xi, y)?,for all i in the training data.
We define themargin of the parameters w on the example iand parse y as the difference in value betweenthe true parse yi and y:?w,?
(xi, yi)?
?
?w,?
(xi, y)?
= ?w,?i,yi ?
?i,y?,3Note that in the case that two members y1 and y2have the same tied value for ?w,?
(x, y)?, we assume thatthere is some fixed, deterministic way for breaking ties.For example, one approach would be to assume somedefault ordering on the members of Y.where ?i,y = ?
(xi, y), and ?i,yi = ?
(xi, yi).
In-tuitively, the size of the margin quantifies theconfidence in rejecting the mistaken parse y us-ing the function fw(x), modulo the scale of theparameters ||w||.
We would like this rejectionconfidence to be larger when the mistake y ismore severe, i.e.
L(xi, yi, y) is large.
We can ex-press this desideratum as an optimization prob-lem:max ?
(1)s.t.
?w,?i,yi ?
?i,y?
?
?Li,y ?y ?
G(xi);||w||2 ?
1,where Li,y = L(xi, yi, y).
This quadratic pro-gram aims to separate each y ?
G(xi) fromthe target parse yi by a margin that is propor-tional to the loss L(xi, yi, y).
After a standardtransformation, in which maximizing the mar-gin is reformulated as minimizing the scale ofthe weights (for a fixed margin of 1), we get thefollowing program:min 12?w?2 + C?i?i (2)s.t.
?w,?i,yi ?
?i,y?
?
Li,y ?
?i ?y ?
G(xi).The addition of non-negative slack variables ?iallows one to increase the global margin by pay-ing a local penalty on some outlying examples.The constant C dictates the desired trade-offbetween margin size and outliers.
Note that thisformulation has an exponential number of con-straints, one for each possible parse y for eachsentence i.
We address this issue in section 4.2.3 The Max-Margin DualIn SVMs, the optimization problem is solved byworking with the dual of a quadratic programanalogous to Eq.
2.
For our problem, just as forSVMs, the dual has important computationaladvantages, including the ?kernel trick,?
whichallows the efficient use of high-dimensional fea-tures spaces endowed with efficient dot products(Cristianini and Shawe-Taylor, 2000).
More-over, the dual view plays a crucial role in cir-cumventing the exponential size of the primalproblem.In Eq.
2, there is a constraint for each mistakey one might make on each example i, which rulesout that mistake.
For each mistake-exclusionconstraint, the dual contains a variable ?i,y.
In-tuitively, the magnitude of ?i,y is proportionalto the attention we must pay to that mistake inorder not to make it.The dual of Eq.
2 (after adding additionalvariables ?i,yi and renormalizing by C) is givenby:max C?i,y?i,yLi,y ?12???????????
?C?i,y(Ii,y ?
?i,y)?i,y????????????2s.t.
?y?i,y = 1, ?i; ?i,y ?
0, ?i, y, (3)where Ii,y = I(xi, yi, y) indicates whether y isthe true parse yi.
Given the dual solution ?
?,the solution to the primal problem w?
is sim-ply a weighted linear combination of the featurevectors of the correct parse and mistaken parses:w?
= C?i,y(Ii,y ?
?
?i,y)?i,y.This is the precise sense in which mistakes withlarge ?
contribute more strongly to the model.3 Factored ModelsThere is a major problem with both the pri-mal and the dual formulations above: since eachpotential mistake must be ruled out, the num-ber of variables or constraints is proportional to|G(x)|, the number of possible parse trees.
Evenin grammars without unary chains or empty el-ements, the number of parses is generally ex-ponential in the length of the sentence, so wecannot expect to solve the above problem with-out any assumptions about the feature-vectorrepresentation ?
and loss function L.For that matter, for arbitrary representa-tions, to find the best parse given a weight vec-tor, we would have no choice but to enumerateall trees and score them.
However, our gram-mars and representations are generally struc-tured to enable efficient inference.
For exam-ple, we usually assign scores to local parts ofthe parse such as PCFG productions.
Suchfactored models have shared substructure prop-erties which permit dynamic programming de-compositions.
In this section, we describe howthis kind of decomposition can be done over thedual ?
distributions.
The idea of this decom-position has previously been used for sequencesand other Markov random fields in Taskar etal.
(2003), but the present extension to CFGsis novel.For clarity of presentation, we restrict thegrammar to be in Chomsky normal form (CNF),where all rules in the grammar are of the form?A ?
B C?
or ?A ?
a?, where A,B and C areSNPDTTheNNscreenVPVBDwasNPNPDTaNNseaPPINofNPNNred01234560 1 2 3 4 5 6 7DTNNVBDDTNNINNNNPNPPPVPSNPr = ?NP, 3, 5?q = ?S ?
NP VP, 0, 2, 7?
(a) (b)Figure 1: Two representations of a binary parse tree: (a) nested tree structure, and (b) grid of labeled spans.non-terminal symbols, and a is some terminalsymbol.
For example figure 1(a) shows a treein this form.We will represent each parse as a set of twotypes of parts.
Parts of the first type are sin-gle constituent tuples ?A, s, e, i?, consisting ofa non-terminal A, start-point s and end-pointe, and sentence i, such as r in figure 1(b).
Inthis representation, indices s and e refer to po-sitions between words, rather than to wordsthemselves.
These parts correspond to the tra-ditional notion of an edge in a tabular parser.Parts of the second type consist of CF-rule-tuples ?A ?
B C, s,m, e, i?.
The tuple specifiesa particular rule A ?
B C, and its position,including split point m, within the sentence i,such as q in figure 1(b), and corresponds to thetraditional notion of a traversal in a tabularparser.
Note that parts for a basic PCFG modelare not just rewrites (which can occur multipletimes), but rather anchored items.Formally, we assume some countable set ofparts, R. We also assume a function R whichmaps each object (x, y) ?
X ?
Y to a finitesubset of R. Thus R(x, y) is the set of parts be-longing to a particular parse.
Equivalently, thefunction R(x, y) maps a derivation y to the setof parts which it includes.
Because all rules arein binary-branching form, |R(x, y)| is constantacross different derivations y for the same inputsentence x.
We assume that the feature vectorfor a sentence and parse tree (x, y) decomposesinto a sum of the feature vectors for its parts:?
(x, y) =?r?R(x,y)?
(x, r).In CFGs, the function ?
(x, r) can be any func-tion mapping a rule production and its posi-tion in the sentence x, to some feature vectorrepresentation.
For example, ?
could includefeatures which identify the rule used in the pro-duction, or features which track the rule iden-tity together with features of the words at po-sitions s,m, e, and neighboring positions in thesentence x.In addition, we assume that the loss functionL(x, y, y?)
also decomposes into a sum of localloss functions l(x, y, r) over parts, as follows:L(x, y, y?)
=?r?R(x,y?
)l(x, y, r).One approach would be to define l(x, y, r) tobe 0 only if the non-terminal A spans wordss .
.
.
e in the derivation y and 1 otherwise.
Thiswould lead to L(x, y, y?)
tracking the number of?constituent errors?
in y?, where a constituent isa tuple such as ?A, s, e, i?.
Another, more strictdefinition would be to define l(x, y, r) to be 0if r of the type ?A ?
B C, s,m, e, i?
is in thederivation y and 1 otherwise.
This definitionwould lead to L(x, y, y?)
being the number of CF-rule-tuples in y?
which are not seen in y.4Finally, we define indicator variables I(x, y, r)which are 1 if r ?
R(x, y), 0 otherwise.
Wealso define sets R(xi) = ?y?G(xi)R(xi, y) for thetraining examples i = 1 .
.
.
n. Thus, R(xi) isthe set of parts that is seen in at least one ofthe objects {(xi, y) : y ?
G(xi)}.4 Factored DualThe dual in Eq.
3 involves variables ?i,y forall i = 1 .
.
.
n, y ?
G(xi), and the objec-tive is quadratic in these ?
variables.
In addi-tion, it turns out that the set of dual variables?i = {?i,y : y ?
G(xi)} for each example i isconstrained to be non-negative and sum to 1.It is interesting that, while the parameters wlose their probabilistic interpretation, the dualvariables ?i for each sentence actually form akind of probability distribution.
Furthermore,the objective can be expressed in terms of ex-pectations with respect to these distributions:C?iE?i [Li,y]?12?????????
?C?i?i,yi ?E?i [?i,y]?????????
?2.We now consider how to efficiently solvethe max-margin optimization problem for afactored model.
As shown in Taskar et al(2003), the dual in Eq.
3 can be reframed using?marginal?
terms.
We will also find it useful toconsider this alternative formulation of the dual.Given dual variables ?, we define the marginals?i,r(?)
for all i, r, as follows:?i,r(?i) =?y?i,yI(xi, y, r) = E?i [I(xi, y, r)] .Since the dual variables ?i form probability dis-tributions over parse trees for each sentence i,the marginals ?i,r(?i) represent the proportionof parses that would contain part r if they weredrawn from a distribution ?i.
Note that thenumber of such marginal terms is the numberof parts, which is polynomial in the length ofthe sentence.Now consider the dual objective Q(?)
inEq.
3.
It can be shown that the original ob-jective Q(?)
can be expressed in terms of these4The constituent loss function does not exactly cor-respond to the standard scoring metrics, such as F1 orcrossing brackets, but shares the sensitivity to the num-ber of differences between trees.
We have not thoroughlyinvestigated the exact interplay between the various losschoices and the various parsing metrics.
We used theconstituent loss in our experiments.marginals as Qm(?(?
)), where ?(?)
is the vectorwith components ?i,r(?i), and Qm(?)
is definedas:C?i,r?R(xi)?i,rli,r ?12???????????
?C?i,r?R(xi)(Ii,r ?
?i,r)?i,r???????????
?2where li,r = l(xi, yi, r), ?i,r = ?
(xi, r) and Ii,r =I(xi, yi, r).This follows from substituting the factoreddefinitions of the feature representation ?
andloss function L together with definition ofmarginals.Having expressed the objective in terms of apolynomial number of variables, we now turn tothe constraints on these variables.
The feasibleset for ?
is?
= {?
: ?i,y ?
0, ?i, y?y?i,y = 1, ?i}.Now let ?m be the space of marginal vectorswhich are feasible:?m = {?
: ??
?
?
s.t.
?
= ?(?
)}.Then our original optimization problem can bereframed as max??
?m Qm(?
).Fortunately, in case of PCFGs, the domain?m can be described compactly with a polyno-mial number of linear constraints.
Essentially,we need to enforce the condition that the ex-pected proportions of parses having particularparts should be consistent with each other.
Ourmarginals track constituent parts ?A, s, e, i?
andCF-rule-tuple parts ?A ?
B C, s,m, e, i?
Theconsistency constraints are precisely the inside-outside probability relations:?i,A,s,e =?B,Cs<m<e?i,A?B C,s,m,eand?i,A,s,e =?B,Ce<m?ni?i,B?AC +?B,C0?m<s?i,B?CAwhere ni is the length of the sentence.
In ad-dition, we must ensure non-negativity and nor-malization to 1:?i,r ?
0;?A?i,A,0,ni = 1.The number of variables in our factored dualfor CFGs is cubic in the length of the sentence,Model P R F1GENERATIVE 87.70 88.06 87.88BASIC 87.51 88.44 87.98LEXICAL 88.15 88.62 88.39LEXICAL+AUX 89.74 90.22 89.98Figure 2: Development set results of the variousmodels when trained and tested on Penn treebanksentences of length ?
15.Model P R F1GENERATIVE 88.25 87.73 87.99BASIC 88.08 88.31 88.20LEXICAL 88.55 88.34 88.44LEXICAL+AUX 89.14 89.10 89.12COLLINS 99 89.18 88.20 88.69Figure 3: Test set results of the various models whentrained and tested on Penn treebank sentences oflength ?
15.while the number of constraints is quadratic.This polynomial size formulation should be con-trasted with the earlier formulation in Collins(2004), which has an exponential number ofconstraints.5 Factored SMOWe have reduced the problem to a polynomialsize QP, which, in principle, can be solved us-ing standard QP toolkits.
However, althoughthe number of variables and constraints in thefactored dual is polynomial in the size of thedata, the number of coefficients in the quadraticterm in the objective is very large: quadratic inthe number of sentences and dependent on thesixth power of sentence length.
Hence, in ourexperiments we use an online coordinate descentmethod analogous to the sequential minimal op-timization (SMO) used for SVMs (Platt, 1999)and adapted to structured max-margin estima-tion in Taskar et al (2003).We omit the details of the structured SMOprocedure, but the important fact about thiskind of training is that, similar to the basic per-ceptron approach, it only requires picking upsentences one at a time, checking what the bestparse is according to the current primal anddual weights, and adjusting the weights.6 ResultsWe used the Penn English Treebank for all ofour experiments.
We report results here foreach model and setting trained and tested ononly the sentences of length ?
15 words.
Asidefrom the length restriction, we used the stan-dard splits: sections 2-21 for training (9753 sen-tences), 22 for development (603 sentences), and23 for final testing (421 sentences).As a baseline, we trained a CNF transforma-tion of the unlexicalized model of Klein andManning (2003) on this data.
The resultinggrammar had 3975 non-terminal symbols andcontained two kinds of productions: binary non-terminal rewrites and tag-word rewrites.5 Thescores for the binary rewrites were estimated us-ing unsmoothed relative frequency estimators.The tagging rewrites were estimated with asmoothed model of P (w|t), also using the modelfrom Klein and Manning (2003).
Figure 3 showsthe performance of this model (generative):87.99 F1 on the test set.For the basic max-margin model, we usedexactly the same set of allowed rewrites (andtherefore the same set of candidate parses) as inthe generative case, but estimated their weightsaccording to the discriminative method of sec-tion 4.
Tag-word production weights were fixedto be the log of the generative P (w|t) model.That is, the only change between genera-tive and basic is the use of the discriminativemaximum-margin criterion in place of the gen-erative maximum likelihood one.
This changealone results in a small improvement (88.20 vs.87.99 F1).On top of the basic model, we first added lex-ical features of each span; this gave a lexicalmodel.
For a span ?s, e?
of a sentence x, thebase lexical features were:?
xs, the first word in the span?
xs?1, the preceding adjacent word?
xe?1, the last word in the span?
xe, the following adjacent word?
?xs?1, xs??
?xe?1, xe??
xs+1 for spans of length 3These base features were conjoined with thespan length for spans of length 3 and below,since short spans have highly distinct behaviors(see the examples below).
The features are lex-ical in the sense than they allow specific words5Unary rewrites were compiled into a single com-pound symbol, so for example a subject-gapped sentencewould have label like s+vp.
These symbols were ex-panded back into their source unary chain before parseswere evaluated.and word pairs to influence the parse scores, butare distinct from traditional lexical features inseveral ways.
First, there is no notion of head-word here, nor is there any modeling of word-to-word attachment.
Rather, these features pickup on lexical trends in constituent boundaries,for example the trend that in the sentence Thescreen was a sea of red., the (length 2) spanbetween the word was and the word of is un-likely to be a constituent.
These non-head lex-ical features capture a potentially very differ-ent source of constraint on tree structures thanhead-argument pairs, one having to do morewith linear syntactic preferences than lexicalselection.
Regardless of the relative merit ofthe two kinds of information, one clear advan-tage of the present approach is that inference inthe resulting model remains cubic, since the dy-namic program need not track items with distin-guished headwords.
With the addition of thesefeatures, the accuracy jumped past the genera-tive baseline, to 88.44.As a concrete (and particularly clean) exam-ple of how these features can sway a decision,consider the sentence The Egyptian presidentsaid he would visit Libya today to resume thetalks.
The generative model incorrectly consid-ers Libya today to be a base np.
However, thisanalysis is counter to the trend of today being aone-word constituent.
Two features relevant tothis trend are: (constituent ?
first-word =today ?
length = 1) and (constituent ?
last-word = today ?
length = 1).
These features rep-resent the preference of the word today for beingthe first and and last word in constituent spansof length 1.6 In the lexical model, however,these features have quite large positive weights:0.62 each.
As a result, this model makes thisparse decision correctly.Another kind of feature that can usefully beincorporated into the classification process isthe output of other, auxiliary classifiers.
Forthis kind of feature, one must take care that itsreliability on the training not be vastly greaterthan its reliability on the test set.
Otherwise,its weight will be artificially (and detrimentally)high.
To ensure that such features are as noisyon the training data as the test data, we splitthe training into two folds.
We then trained theauxiliary classifiers in jacknife fashion on each6In this length 1 case, these are the same feature.Note also that the features are conjoined with only onegeneric label class ?constituent?
rather than specific con-stituent types.fold, and using their predictions as features onthe other fold.
The auxiliary classifiers werethen retrained on the entire training set, andtheir predictions used as features on the devel-opment and test sets.We used two such auxiliary classifiers, givinga prediction feature for each span (these classi-fiers predicted only the presence or absence of abracket over that span, not bracket labels).
Thefirst feature was the prediction of the genera-tive baseline; this feature added little informa-tion, but made the learning phase faster.
Thesecond feature was the output of a flat classi-fier which was trained to predict whether sin-gle spans, in isolation, were constituents or not,based on a bundle of features including the listabove, but also the following: the preceding,first, last, and following tag in the span, pairsof tags such as preceding-first, last-following,preceding-following, first-last, and the entire tagsequence.Tag features on the test sets were taken froma pretagging of the sentence by the tagger de-scribed in Toutanova et al (2003).
While theflat classifier alone was quite poor (P 78.77 /R 63.94 / F1 70.58), the resulting max-marginmodel (lexical+aux) scored 89.12 F1.
To sit-uate these numbers with respect to other mod-els, the parser in Collins (1999), which is genera-tive, lexicalized, and intricately smoothed scores88.69 over the same train/test configuration.It is worth considering the cost of this kind ofmethod.
At training time, discriminative meth-ods are inherently expensive, since they all in-volve iteratively checking current model perfor-mance on the training set, which means parsingthe training set (usually many times).
In ourexperiments, 10-20 iterations were generally re-quired for convergence (except the basic model,which took about 100 iterations.)
There areseveral nice aspects of the approach describedhere.
First, it is driven by the repeated extrac-tion, over the training examples, of incorrectparses which the model currently prefers overthe true parses.
The procedure that providesthese parses need not sum over all parses, noreven necessarily find the Viterbi parses, to func-tion.
This allows a range of optimizations notpossible for CRF-like approaches which mustextract feature expectations from the entire setof parses.7 Nonetheless, generative approaches7One tradeoff is that this approach is more inherentlysequential and harder to parallelize.are vastly cheaper to train, since they must onlycollect counts from the training set.On the other hand, the max-margin approachdoes have the potential to incorporate manynew kinds of features over the input, and thecurrent feature set alows limited lexicalizationin cubic time, unlike other lexicalized models(including the Collins model which it outper-forms in the present limited experiments).7 ConclusionWe have presented a maximum-margin ap-proach to parsing, which allows a discriminativeSVM-like objective to be applied to the parsingproblem.
Our framework permits the use of arich variety of input features, while still decom-posing in a way that exploits the shared sub-structure of parse trees in the standard way.
Ona test set of ?
15 word sentences, the feature-rich model outperforms both its own naturalgenerative baseline and the Collins parser onF1.
While like most discriminative models it iscompute-intensive to train, it allows fast pars-ing, remaining cubic despite the incorporationof lexical features.
This trade-off between thecomplexity, accuracy and efficiency of a parsingmodel is an important area of future research.AcknowledgementsThis work was supported in part by the Depart-ment of the Interior/DARPA under contractnumber NBCHD030010, a Microsoft GraduateFellowship to the second author, and NationalScience Foundation grant 0347631 to the thirdauthor.ReferencesY.
Altun, I. Tsochantaridis, and T. Hofmann.2003.
Hidden markov support vector ma-chines.
In Proc.
ICML.S.
Clark and J. R. Curran.
2004.
Parsingthe wsj using ccg and log-linear models.
InProceedings of the 42nd Annual Meeting ofthe Association for Computational Linguis-tics (ACL ?04).M.
Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. the-sis, University of Pennsylvania.M.
Collins.
2000.
Discriminative reranking fornatural language parsing.
In ICML 17, pages175?182.M.
Collins.
2004.
Parameter estimation for sta-tistical parsing models: Theory and practiceof distribution-free methods.
In Harry Bunt,John Carroll, and Giorgio Satta, editors, NewDevelopments in Parsing Technology.
Kluwer.N.
Cristianini and J. Shawe-Taylor.
2000.
AnIntroduction to Support Vector Machines andOther Kernel-Based Learning Methods.
Cam-bridge University Press.S.
Geman and M. Johnson.
2002.
Dynamicprogramming for parsing and estimation ofstochastic unification-based grammars.
InProceedings of the 40th Annual Meeting of theAssociation for Computational Linguistics.M.
Johnson, S. Geman, S. Canon, Z. Chi, andS.
Riezler.
1999.
Estimators for stochastic?unification-based?
grammars.
In Proceed-ings of ACL 1999.M.
Johnson.
2001.
Joint and conditional es-timation of tagging and parsing models.
InACL 39.R.
Kaplan, S. Riezler, T. King, J. Maxwell,A.
Vasserman, and R. Crouch.
2004.
Speedand accuracy in shallow and deep stochasticparsing.
In Proceedings of HLT-NAACL?04).D.
Klein and C. D. Manning.
2003.
Accurateunlexicalized parsing.
In ACL 41, pages 423?430.J.
Lafferty, A. McCallum, and F. Pereira.2001.
Conditional random fields: Probabi-listic models for segmenting and labeling se-quence data.
In ICML.Y.
Miyao and J. Tsujii.
2002.
Maximumentropy estimation for feature forests.
InProceedings of Human Language TechnologyConference (HLT 2002).J.
Platt.
1999.
Using sparseness and analyticQP to speed training of support vector ma-chines.
In NIPS.L.
Shen, A. Sarkar, and A. K. Joshi.
2003.
Us-ing ltag based features in parse reranking.
InProc.
EMNLP.B.
Taskar, C. Guestrin, and D. Koller.
2003.Max margin Markov networks.
In NIPS.K.
Toutanova, D. Klein, C. D. Manning, andY.
Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
InNAACL 3, pages 252?259.
