Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 221?224,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsAdapting a WSJ-Trained Parser to Grammatically Noisy TextJennifer Foster, Joachim Wagner and Josef van GenabithNational Centre for Language TechnologyDublin City UniversityIrelandjfoster, jwagner, josef@computing.dcu.ieAbstractWe present a robust parser which is trained ona treebank of ungrammatical sentences.
Thetreebank is created automatically by modify-ing Penn treebank sentences so that they con-tain one or more syntactic errors.
We eval-uate an existing Penn-treebank-trained parseron the ungrammatical treebank to see how itreacts to noise in the form of grammatical er-rors.
We re-train this parser on the trainingsection of the ungrammatical treebank, lead-ing to an significantly improved performanceon the ungrammatical test sets.
We show howa classifier can be used to prevent performancedegradation on the original grammatical data.1 IntroductionThe focus in English parsing research in recent yearshas moved from Wall Street Journal parsing to im-proving performance on other domains.
Our re-search aim is to improve parsing performance ontext which is mildly ungrammatical, i.e.
text whichis well-formed enough to be understood by peopleyet which contains the kind of grammatical errorsthat are routinely produced by both native and non-native speakers of a language.
The intention is notto detect and correct the error, but rather to ignoreit.
Our approach is to introduce grammatical noiseinto WSJ sentences while retaining as much of thestructure of the original trees as possible.
Thesesentences and their associated trees are then usedas training material for a statistical parser.
It is im-portant that parsing on grammatical sentences is notharmed and we introduce a parse-probability-basedclassifier which allows both grammatical and un-grammatical sentences to be accurately parsed.2 BackgroundVarious strategies exist to build robustness into theparsing process: grammar constraints can be relaxed(Fouvry, 2003), partial parses can be concatenated toform a full parse (Penstein Rose?
and Lavie, 1997),the input sentence can itself be transformed until aparse can be found (Lee et al, 1995), and mal-rulesdescribing particular error patterns can be includedin the grammar (Schneider and McCoy, 1998).
For aparser which tends to fail when faced with ungram-matical input, such techniques are needed.
The over-generation associated with a statistical data-drivenparser means that it does not typically fail on un-grammatical sentences.
However, it is not enoughto return some analysis for an ungrammatical sen-tence.
If the syntactic analysis is to guide semanticanalysis, it must reflect as closely as possible whatthe person who produced the sentence was trying toexpress.
Thus, while statistical, data-driven parsinghas solved the robustness problem, it is not clear thatit is has solved the accurate robustness problem.The problem of adapting parsers to accuratelyhandle ungrammatical text is an instance of the do-main adaptation problem where the target domain isgrammatically noisy data.
A parser can be adaptedto a target domain by training it on data from the newdomain ?
the problem is to quickly produce high-quality training material.
Our solution is to simplymodify the existing training material so that it re-sembles material from the noisy target domain.In order to tune a parser to syntactically ill-formedtext, a treebank is automatically transformed into anungrammatical treebank.
This transformation pro-cess has two parts: 1. the yield of each tree is trans-formed into an ungrammatical sentence by introduc-ing a syntax error; 2. each tree is minimally trans-formed, but left intact as much as possible to reflectthe syntactic structure of the original ?intended?
sen-221tence prior to error insertion.
Artificial ungrammati-calities have been used in various NLP tasks (Smithand Eisner, 2005; Okanohara and Tsujii, 2007)The idea of an automatically generated ungram-matical treebank was proposed by Foster (2007).Foster generates an ungrammatical version of theWSJ treebank and uses this to train two statisticalparsers.
The performance of both parsers signifi-cantly improves on the artificially created ungram-matical test data, but significantly degrades on theoriginal grammatical test data.
We show that itis possible to obtain significantly improved perfor-mance on ungrammatical data without a concomi-tant performance decline on grammatical data.3 Generating Noisy TreebanksGenerating Noisy Sentences We apply the errorintroduction procedure described in detail in Foster(2007).
Errors are introduced into sentences by ap-plying the operations of word substitution, deletionand insertion.
These operations can be iterativelyapplied to generate increasingly noisy sentences.We restrict our attention to ungrammatical sentenceswith a edit-distance of one or two words from theoriginal sentence, because it is reasonable to expecta parser?s performance to degrade as the input be-comes more ill-formed.
The operations of substitu-tion, deletion and insertion are not carried out en-tirely at random, but are subject to some constraintsderived from an empirical study of ill-formed En-glish sentences (Foster, 2005).
Three types of wordsubstitution errors are produced: agreement errors,real word spelling errors and verb form errors.
Anyword that is not an adjective or adverb can be deletedfrom any position within the input sentence, butsome part-of-speech tags are favoured over others,e.g.
it is more likely that a determiner will be deletedthan a noun.
The error creation procedure can insertan arbitrary word at any position within a sentencebut it has a bias towards inserting a word directly af-ter the same word or directly after a word with thesame part of speech.
The empirical study also in-fluences the frequency at which particular errors areintroduced, with missing word errors being the mostfrequent, followed by extra word errors, real wordspelling errors, agreement errors, and finally, verbform errors.
Table 1 shows examples of the kind ofill-formed sentences that are produced when we ap-ply the procedure to Wall Street Journal sentences.Generating Trees for Noisy Sentences The treestructures associated with the modified sentences arealso modified, but crucially, this modification is min-imal, since a truly robust parser should return ananalysis for a mildly ungrammatical sentence thatremains as similar as possible to the analysis it re-turns for the original grammatical sentence.Assume that (1) is an original treebank tree for thesentence A storm is brewing.
Example (2) is then thetree for the ungrammatical sentence containing anis/it confusion.
No part of the original tree structureis changed apart from the yield.
(1) (S (NP A storm) (VP (VBZ is) (VP (VBG brewing))))(2) (S (NP A storm) (VP (VBZ it) (VP (VBG brewing))))An example of a missing word error is shown in(3) and (4).
A pre-terminal dominating an emptynode is introduced into the tree at the point wherethe word has been omitted.
(3) (S (NP Annotators) (VP (VBP parse) (NP the sentences)))(4) (S (NP Annotators) (VP (-NONE- 0) (NP the sentences)))An example of an extra word error is shown in (5),(6) and (7).
For this example, two ungrammaticaltrees, (6) and (7), are generated because there aretwo possible positions in the original tree where theextra word can be inserted which will result in a treewith the yield He likes of the cake and which will notresult in the creation of any additional structure.
(5) (S (NP He) (VP (VBZ likes) (NP (DT the) (NN cake))))(6) (S (NP He) (VP (VBZ likes) (IN of) (NP (DT the) (NNcake))))(7) (S (NP He) (VP (VBZ likes) (NP (IN of) (DT the) (NNcake))))4 Parser Adaptation ExperimentsIn order to obtain training data for our parsing ex-periments, we introduce syntactic noise into theusual WSJ training material, Sections 2-21, usingthe procedures outlined in Section 3, i.e.
for everysentence-tree pair in WSJ2-21, we introduce an er-ror into the sentence and then transform the tree sothat it covers the newly created ungrammatical sen-tence.
For 4 of the 20 sections in WSJ2-21, we applythe noise introduction procedure to its own output to222Error Type WSJ00Missing Word likely to bring new attention to the problem ?
likely to new attention to the problemExtra Word the $ 5.9 million it posted ?
the $ 5.9 million I it postedReal Word Spell Mr Vinken is chairman of Elsevier?
Mr. Vinken if chairman of ElsevierAgreement this event took place 35 years ago?
these event took place 35 years agoVerb Form But the Soviets might still face legal obstacles?
But the Soviets might still faces legal obstaclesTable 1: Automatically Generated Ungrammatical Sentencescreate even noisier data.
Our first development set isa noisy version of WSJ00, Noisy00, produced by ap-plying the noise introduction procedure to the 1,921sentences in WSJ00.
Our second development set isan even noisier version of WSJ00, Noisiest00, whichis created by applying our noise introduction proce-dure to the output of Noisy00.
We apply the sameprocess to WSJ23 to obtain our two test sets.For all our parsing experiments, we use the June2006 version of the two-stage parser reported inCharniak and Johnson (2005).
Evaluation is carriedout using Parseval labelled precision/recall.
For ex-tra word errors, there may be more than one goldstandard tree (see (6) and (7)).
When this happensthe parser output tree is evaluated against all goldstandard trees and the maximum f-score is chosen.We carry out five experiments.
In the first ex-periment, E0, we apply the parser, trained on well-formed data, to noisy input.
The purpose of E0 is toascertain how well a parser trained on grammaticalsentences, can ignore grammatical noise.
E0 pro-vides a baseline against which the subsequent ex-perimental results can be judged.
In the E1 experi-ments, the parser is retrained using the ungrammati-cal version of WSJ2-21.
In experiment E1error, theparser is trained on ungrammatical material only,i.e.
the noisy version of WSJ2-21.
In experimentE1mixed, the parser is trained on grammatical andungrammatical material, i.e.
the original WSJ2-21 ismerged with the noisy WSJ2-21.
In the E2 experi-ments, a classifier is applied to the input sentence.If the sentence is classified as ungrammatical, a ver-sion of the parser that has been trained on ungram-matical data is employed.
In the E2ngram experi-ment, we train a J48 decision tree classifier.
Follow-ing Wagner et al (2007), the decision tree featuresare part-of-speech n-gram frequency counts, with nranging from 2 to 7 and with a subset of the BNCas the frequency reference corpus.
The decision treeis trained on the original WSJ2-21 and the ungram-matical WSJ2-21.
In the E2prob experiment, the in-put sentence is parsed with two parsers, the origi-nal parser (the E0 parser) and the parser trained onungrammatical material (either the E1error or theE1mixed parser).
A very simple classifier is usedto decide which parser output to choose: if the E1parser returns a higher parse probability for the mostlikely tree than the E0 parser, the E1 parser output isreturned.
Otherwise the E0 parser output is returned.The baseline E0 results are in the first column ofTable 2.
As expected, the performance of a parsertrained on well-formed input degrades when facedwith ungrammatical input.
It is also not surprisingthat its performance is worse on Noisiest00 (-8.8%f-score) than it is on Noisy00 (-4.3%) since the Nois-iest00 sentences contain two errors rather than one.The E1 results occupy the second and thirdcolumns of Table 2.
An up arrow indicates a sta-tistically significant improvement over the baselineresults, a down arrow a statistically significant de-cline and a dash a change which is not statisticallysignificant (p < 0.01).
Training the parser on un-grammatical data has a positive effect on its perfor-mance on Noisy00 and Noisiest00 but has a negativeeffect on its performance on WSJ00.
Training on acombination of grammatical and ungrammatical ma-terial gives the best results for all three developmentsets.
Therefore, for the E2 experiments we use theE1mixed parser rather than the E1error parser.The E2 results are shown in the last two columnsof Table 2 and the accuracy of the two classifiers inTable 3.
Over the three test sets, the E2prob classi-fier outperforms the E2ngram classifier.
Both classi-fiers misclassify approximately 45% of the Noisy00sentences.
However, the sentences misclassified bythe E2prob classifier are those that are handled wellby the E0 parser, and this is reflected in the pars-ing results for Noisy00.
An important feature of the223Dev Set P R F P R F P R F P R F P R FE0 E1-error E1-mixed E2prob E2ngramWSJ00 91.5 90.3 90.9 91.0?
89.4 ?
90.2 91.3?
89.8 ?
90.5 91.5?
90.2?
90.9 91.3?
89.9?
90.6Noisy00 87.5 85.6 86.6 89.4 ?
86.6 ?
88.0 89.4 ?
86.8 ?
88.1 89.1 ?
86.8 ?
87.9 88.7?
86.2?
87.5Noisiest00 83.5 80.8 82.1 87.6 ?
83.6 ?
85.6 87.6 ?
83.8 ?
85.7 87.2 ?
83.7 ?
85.4 86.6?
83.0?
84.8Table 2: Results of Parsing ExperimentsDevelopment Set E2prob E2ngramWSJ00 76.7% 63.3%Noisy00 55.1% 55.6%Noisiest00 70.2% 66.0%Table 3: E2 Classifier AccuracyTest Set P R F P R FE0 E2probWSJ23 91.7 90.8 91.3 91.7?
90.7?
91.2Noisy23 87.4 85.6 86.5 89.2 ?
87.0 ?
88.1Noisiest23 83.2 80.8 82.0 87.4 ?
84.1 ?
85.7Table 4: Final Results for Section 23 Test SetsE2prob classifier is that its use results in a constantperformance on the grammatical data - with no sig-nificant degradation from the baseline.Taking the E2prob results as our optimum, wecarry out the same experiment again on our WSJ23test sets.
The results are shown in Table 4.
The sameeffect can be seen for the test sets as for the devel-opment sets - a significantly improved performanceon the ungrammatical data without an accompany-ing performance decrease for the grammatical data.The Noisy23 breakdown by error type is shown inTable 5.
The error type which the original parser ismost able to ignore is an agreement error.
For this er-ror type alone, the ungrammatical training materialseems to hinder the parser.
The biggest improve-ment occurs for real word spelling errors.5 ConclusionWe have shown that it is possible to tune a WSJ-trained statistical parser to ungrammatical text with-Error Type P R F P R FE0 E2-probMissing Word 88.5 83.7 86.0 88.9 84.3 86.5Extra Word 87.2 89.4 88.3 89.2 89.7 89.4Real Word Spell 84.3 83.0 83.7 89.5 88.2 88.9Agreement 90.4 88.8 89.6 90.3 88.6 89.4Verb Form 88.6 87.0 87.8 89.1 87.9 88.5Table 5: Noisy23: Breakdown by Error Typeout affecting its performance on grammatical text.This has been achieved using an automatically gen-erated ungrammatical version of the WSJ treebankand a simple binary classifier which compares parseprobabilities.
The next step in this research is to seehow the method copes on ?real?
errors - this will re-quire manual parsing of a suitably large test set.Acknowledgments We thank the IRCSET Em-bark Initiative (postdoctoral fellowship P/04/232)for supporting this research.ReferencesEugene Charniak and Mark Johnson.
2005.
Course-to-fine n-best-parsing and maxent discriminative reranking.
In Pro-ceedings of ACL-2005.Jennifer Foster.
2005.
Good Reasons for Noting Bad Gram-mar: Empirical Investigations into the Parsing of Ungram-matical Written English.
Ph.D. thesis, University of Dublin,Trinity College.Jennifer Foster.
2007.
Treebanks gone bad: Parser evaluationand retraining using a treebank of ungrammatical sentences.IJDAR, 10(3-4), December.Frederik Fouvry.
2003.
Robust Processing for Constraint-based Grammar Formalisms.
Ph.D. thesis, University of Es-sex.Kong Joo Lee, Cheol Jung Kweon, Jungyun Seo, and Gil ChangKim.
1995.
A robust parser based on syntactic information.In Proceedings of EACL-1995.Daisuke Okanohara and Jun?ichi Tsujii.
2007.
A discrimi-native language model with pseudo-negative examples.
InProceedings of ACL-2007.Carolyn Penstein Rose?
and Alon Lavie.
1997.
An efficient dis-tribution of labor in a two stage robust interpretation process.In Proceedings of EMNLP-1997.David Schneider and Kathleen McCoy.
1998.
Recognizingsyntactic errors in the writing of second language learners.In Proceedings of ACL/COLING-1998.Noah A. Smith and Jason Eisner.
2005.
Contrastive Estima-tion: Training Log-Linear Models on Unlabeled Data.
InProceedings of ACL-2005.Joachim Wagner, Jennifer Foster, and Josef van Genabith.2007.
A comparative evaluation of deep and shallow ap-proaches to the automatic detection of common grammaticalerrors.
In Proceedings of EMNLP-CoNLL-2007.224
