Proceedings of NAACL-HLT 2013, pages 32?40,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsSimultaneous Word-Morpheme Alignment forStatistical Machine TranslationElif Eyigo?zComputer ScienceUniversity of RochesterRochester, NY 14627Daniel GildeaComputer ScienceUniversity of RochesterRochester, NY 14627Kemal OflazerComputer ScienceCarnegie Mellon UniversityPO Box 24866, Doha, QatarAbstractCurrent word alignment models for statisti-cal machine translation do not address mor-phology beyond merely splitting words.
Wepresent a two-level alignment model that dis-tinguishes between words and morphemes, inwhich we embed an IBM Model 1 inside anHMM based word alignment model.
Themodel jointly induces word and morphemealignments using an EM algorithm.
We eval-uated our model on Turkish-English paralleldata.
We obtained significant improvement ofBLEU scores over IBM Model 4.
Our resultsindicate that utilizing information from mor-phology improves the quality of word align-ments.1 IntroductionAll current state-of-the-art approaches to SMT relyon an automatically word-aligned corpus.
However,current alignment models do not take into accountthe morpheme, the smallest unit of syntax, beyondmerely splitting words.
Since morphology has notbeen addressed explicitly in word alignment models,researchers have resorted to tweaking SMT systemsby manipulating the content and the form of whatshould be the so-called ?word?.Since the word is the smallest unit of translationfrom the standpoint of word alignment models, thecentral focus of research on translating morphologi-cally rich languages has been decomposition of mor-phologically complex words into tokens of the rightgranularity and representation for machine transla-tion.
Chung and Gildea (2009) and Naradowsky andToutanova (2011) use unsupervised methods to findword segmentations that create a one-to-one map-ping of words in both languages.
Al-Onaizan et al(1999), C?mejrek et al(2003), and Goldwater andMcClosky (2005) manipulate morphologically richlanguages by selective lemmatization.
Lee (2004)attempts to learn the probability of deleting or merg-ing Arabic morphemes for Arabic to English trans-lation.
Niessen and Ney (2000) split German com-pound nouns, and merge German phrases that cor-respond to a single English word.
Alternatively,Yeniterzi and Oflazer (2010) manipulate words ofthe morphologically poor side of a language pairto mimic having a morphological structure similarto the richer side via exploiting syntactic structure,in order to improve the similarity of words on bothsides of the translation.We present an alignment model that assumes in-ternal structure for words, and we can legitimatelytalk about words and their morphemes in line withthe linguistic conception of these terms.
Our modelavoids the problem of collapsing words and mor-phemes into one single category.
We adopt a two-level representation of alignment: the first level in-volves word alignment, the second level involvesmorpheme alignment in the scope of a given wordalignment.
The model jointly induces word andmorpheme alignments using an EM algorithm.We develop our model in two stages.
Our initialmodel is analogous to IBM Model 1: the first levelis a bag of words in a pair of sentences, and the sec-ond level is a bag of morphemes.
In this manner,we embed one IBM Model 1 in the scope of anotherIBM Model 1.
At the second stage, by introducingdistortion probabilities at the word level, we developan HMM extension of the initial model.We evaluated the performance of our model on the32Turkish-English pair both on hand-aligned data andby running end-to-end machine translation experi-ments.
To evaluate our results, we created gold wordalignments for 75 Turkish-English sentences.
Weobtain significant improvement of AER and BLEUscores over IBM Model 4.
Section 2.1 introducesthe concept of morpheme alignment in terms of itsrelation to word alignment.
Section 2.2 presentsthe derivation of the EM algorithm and Section 3presents the results of our experiments.2 Two-level Alignment Model (TAM)2.1 Morpheme AlignmentFollowing the standard alignment models of Brownet al(1993), we assume one-to-many alignment forboth words and morphemes.
A word alignment aw(or only a) is a function mapping a set of word po-sitions in a source language sentence to a set ofword positions in a target language sentence.
A mor-pheme alignment am is a function mapping a set ofmorpheme positions in a source language sentenceto a set of morpheme positions in a target languagesentence.
A morpheme position is a pair of integers(j, k), which defines a word position j and a relativemorpheme position k in the word at position j. Thealignments below are depicted in Figures 1 and 2.aw(1) = 1 am(2, 1) = (1, 1) aw(2) = 1Figure 1 shows a word alignment between two sen-tences.
Figure 2 shows the morpheme alignment be-tween same sentences.
We assume that all unalignedmorphemes in a sentence map to a special null mor-pheme.A morpheme alignment am and a word alignmentaw are compatible if and only if they satisfy the fol-lowing conditions: If the morpheme alignment ammaps a morpheme of e to a morpheme of f , then theword alignment aw maps e to f .
If the word align-ment aw maps e to f , then the morpheme alignmentam maps at least one morpheme of e to a morphemeof f .
If the word alignment aw maps e to null, thenall of its morphemes are mapped to null.
In sum, amorpheme alignment am and a word alignment aware compatible if and only if:?
j, k,m, n ?
N+, ?
s, t ?
N+[am(j, k) = (m,n)?
aw(j) = m] ?
[aw(j) = m?
am(j, s) = (m, t)] ?
[aw(j) = null?
am(j, k) = null] (1)Please note that, according to this definition of com-patibility, ?am(j, k) = null?
does not necessarily im-ply ?aw(j) = null?.A word alignment induces a set of compati-ble morpheme alignments.
However, a morphemealignment induces a unique word alignment.
There-fore, if a morpheme alignment am and a word align-ment aw are compatible, then the word alignment isaw is recoverable from the morpheme alignment am.The two-level alignment model (TAM), likeIBM Model 1, defines an alignment between wordsof a sentence pair.
In addition, it defines a mor-pheme alignment between the morphemes of a sen-tence pair.The problem domain of IBM Model 1 is definedover alignments between words, which is depictedas the gray box in Figure 1.
In Figure 2, the smallerboxes embedded inside the main box depict the newproblem domain of TAM.
Given the word align-ments in Figure 1, we are presented with a newalignment problem defined over their morphemes.The new alignment problem is constrained by thegiven word alignment.
We, like IBM Model 1, adopta bag-of-morphemes approach to this new problem.We thus embed one IBM Model 1 into the scope ofanother IBM Model 1, and formulate a second-orderinterpretation of IBM Model 1.TAM, like IBM Model 1, assumes that words andmorphemes are translated independently of theircontext.
The units of translation are both words andmorphemes.
Both the word alignment aw and themorpheme alignment am are hidden variables thatneed to be learned from the data using the EM algo-rithm.In IBM Model 1, p(e|f), the probability of trans-lating the sentence f into e with any alignment iscomputed by summing over all possible word align-ments:p(e|f) =?ap(a, e|f)33Figure 1: Word alignment Figure 2: Morpheme alignmentIn TAM, the probability of translating the sentencef into e with any alignment is computed by sum-ming over all possible word alignments and all pos-sible morpheme alignments that are compatible witha given word alignment aw:p(e|f) =?awp(aw, e|f)?amp(am, e|aw, f) (2)where am stands for a morpheme alignment.
Sincethe morpheme alignment am is in the scope of agiven word alignment aw, am is constrained by aw.In IBM Model 1, we compute the probability oftranslating the sentence f into e by summing overall possible word alignments between the words of fand e:p(e|f) = R(e, f)|e|?j=1|f |?i=0t(ej |fi) (3)where t(ej | fi) is the word translation probabilityof ej given fi.
R(e, f) substitutesP (le|lf )(lf+1)le for easyreadability.1In TAM, the probability of translating the sen-tence f into e is computed as follows:WordR(e, f)|e|?j=1|f |?i=0(t(ej |fi)R(ej , fi)|ej |?k=1|fi|?n=0t(ekj |fni ))Morphemewhere fni is the nth morpheme of the word at po-sition i.
The right part of this equation, the con-tribution of morpheme translation probabilities, is1le = |e| is the number of words in sentence e and lf = |f |.in the scope of the left part.
In the right part, wecompute the probability of translating the word fiinto the word ej by summing over all possible mor-pheme alignments between the morphemes of ej andfi.
R(ej , fi) is equivalent to R(e, f) except for thefact that its domain is not the set of sentences butthe set of words.
The length of words ej and fi inR(ej , fi) are the number of morphemes of ej and fi.The left part, the contribution of word transla-tion probabilities alone, equals Eqn.
3.
Therefore,canceling the contribution of morpheme translationprobabilities reduces TAM to IBM Model 1.
Inour experiments, we call this reduced version ofTAM ?word-only?
(IBM).
TAM with the contribu-tion of both word and morpheme translation proba-bilities, as the equation above, is called ?word-and-morpheme?.
Finally, we also cancel out the con-tribution of word translation probabilities, which iscalled ?morpheme-only?.
In the ?morpheme-only?version of TAM, t(ej |fi) equals 1.
Bellow is theequation of p(e|f) in the morpheme-only model.p(e|f) =R(e, f)|e|?j=1|f |?i=0|ej |?k=1|fi|?n=0R(ej , fi)t(ekj |fni ) (4)Please note that, although this version of the two-level alignment model does not use word translationprobabilities, it is also a word-aware model, as mor-pheme alignments are restricted to correspond to avalid word alignment according to Eqn.
1.
Whenpresented with words that exhibit no morphology,the morpheme-only version of TAM is equivalent toIBM Model 1, as every single-morpheme word is it-self a morpheme.Deficiency and Non-Deficiency of TAM Wepresent two versions of TAM, the word-and-34morpheme and the morpheme-only versions.
Theword-and-morpheme version of the model is defi-cient whereas the morpheme-only model is not.The word-and-morpheme version is deficient, be-cause some probability is allocated to cases wherethe morphemes generated by the morpheme modeldo not match the words generated by the wordmodel.
Moreover, although most languages exhibitmorphology to some extent, they can be input to thealgorithm without morpheme boundaries.
This alsocauses deficiency in the word-and-morpheme ver-sion, as single morpheme words are generated twice,as a word and as a morpheme.Nevertheless, we observed that the deficient ver-sion of TAM can perform as good as the non-deficient version of TAM, and sometimes performsbetter.
This is not surprising, as deficient word align-ment models such as IBM Model 3 or discriminativeword alignment models work well in practice.Goldwater and McClosky (2005) proposed a mor-pheme aware word alignment model for languagepairs in which the source language words corre-spond to only one morpheme.
Their word alignmentmodel is:P (e|f) =K?k=0P (ek|f)where ek is the kth morpheme of the word e. Themorpheme-only version of our model is a general-ization of this model.
However, there are major dif-ferences in their and our implementation and exper-imentation.
Their model assumes a fixed number ofpossible morphemes associated with any stem in thelanguage, and if the morpheme ek is not present, itis assigned a null value.The null word on the source side is also a nullmorpheme, since every single morpheme word is it-self a morpheme.
In TAM, the null word is the nullmorpheme that all unaligned morphemes align to.2.2 Second-Order CountsIn TAM, we collect counts for both word translationsand morpheme translations.
Unlike IBM Model 1,R(e, f) = P (le|lf )(lf+1)le does not cancel out in the countsof TAM.
To compute the conditional probabilityP (le|lf ), we assume that the length of word e (thenumber of morphemes of word e) varies accordingto a Poisson distribution with a mean that is linearwith length of the word f .P (le|lf ) = FPoisson(le, r ?
lf )=exp(?r ?
lf )(r ?
lf )lele!FPoisson(le, r ?
lf ) expresses the probability that thereare le morphemes in e if the expected number ofmorphemes in e is r ?
lf , where r =E[le]E[lf ]is the rateparameter.
Since lf is undefined for null words, weomit R(e, f) for null words.We introduce T (e|f), the translation probabilityof e given f with all possible morpheme alignments,as it will occur frequently in the counts of TAM:T (e|f) = t(e|f)R(e, f)|e|?k=1|f |?n=0t(ek|fn)The role of T (e|f) in TAM is very similar to therole of t(e|f) in IBM Model 1.
In finding the Viterbialignments, we do not take max over the values inthe summation in T (e|f).2.2.1 Word CountsSimilar to IBM Model 1, we collect counts forword translations over all possible alignments,weighted by their probability.
In Eqn.
5, the countfunction collects evidence from a sentence pair(e, f) as follows: For all words ej of the sentence eand for all word alignments aw(j), we collect countsfor a particular input word f and an output word eiff ej = e and faw(j) = f .cw(e|f ; e, f , aw) =?1?j?|e|s.t.e=ejf=faw(j)T (e|f)|f |?i=0T (e|fi)(5)2.2.2 Morpheme CountsAs for morpheme translations, we collect countsover all possible word and morpheme alignments,weighted by their probability.
The morpheme countfunction below collects evidence from a word pair(e, f) in a sentence pair (e, f) as follows: For allwords ej of the sentence e and for all word align-ments aw(j), for all morphemes ekj of the word ejand for all morpheme alignments am(j, k), we col-lect counts for a particular input morpheme g and an35output morpheme h iff ej = e and faw(j) = f andh = ekj and g = fam(j,k).cm(h|g; e, f , aw, am) =?1?j?|e|s.t.e=ejf=faw(j)?1?k?|e|s.t.h=ekjg=fam(j,k)T (e|f)|f |?i=0T (e|fi)t(h|g)|f |?i=1t(h|f i)The left part of the morpheme count function is thesame as the word-counts in Eqn.
5.
Since it does notcontain h or g, it needs to be computed only once foreach word.
The right part of the equation is familiarfrom the IBM Model 1 counts.2.3 HMM ExtensionWe implemented TAM with the HMM extension(Vogel et al 1996) at the word level.
We redefinep(e|f) as follows:R(e, f)?aw|e|?j=1(p(s(j ) |C (faw (j?1 ))) t(ej |faw(j))R(ej , faw(j))?am|ej |?k=1t(ekj |fam(j,k)))where the distortion probability depends on the rel-ative jump width s(j) = aw(j ?
1) ?
aw(j),as opposed to absolute positions.
The distortionprobability is conditioned on class of the previousaligned word C (faw(j?1)).
We used the mkclstool in GIZA (Och and Ney, 2003) to learn the wordclasses.We formulated the HMM extension of TAM onlyat the word level.
Nevertheless, the morpheme-onlyversion of TAM also has an HMM extension, as itis also a word-aware model.
To obtain the HMMextension of the morpheme-only version, substitutet(ej |faw(j)) with 1 in the equation above.For the HMM to work correctly, we must han-dle jumping to and jumping from null positions.
Welearn the probabilities of jumping to a null positionfrom the data.
To compute the jump probability froma null position, we keep track of the nearest previoussource word that does not align to null, and use theposition of the previous non-null word to calculatethe jump width.
For this reason, we use a total of2lf ?
1 words for the HMM model, the positions> lf stand for null positions between the words of f(Och and Ney, 2003).
We do not allow null to nulljumps.
In sum, we enforce the following constraints:P (i+ lf + 1|i?)
= p(null|i?
)P (i+ lf + 1|i?
+ lf + 1) = 0P (i|i?
+ lf + 1) = p(i|i?
)In the HMM extension of TAM, we performforward-backward training using the word counts inEqn.
5 as the emission probabilities.
We calculatethe posterior word translation probabilities for eachej and fi such that 1 ?
j ?
le and 1 ?
i ?
2lf ?
1as follows:?j(i) =?j(i)?j(i)2lf?1?m=1?j(m)?j(m)where ?
is the forward and ?
is the backward prob-abilities of the HMM.
The HMM word counts, inturn, are the posterior word translation probabilitiesobtained from the forward-backward training:cw(e|f ; e, f , aw) =?1?j?|e|s.t.e=ejf=faw(j)?j(aw(j))Likewise, we use the posterior probabilities in HMMmorpheme counts:cm(h|g; e, f , aw, am) =?1?j?|e|s.t.e=ejf=faw(j)?1?k?|e|s.t.h=ekjg=fam(j,k)?j(aw(j))t(h|g)|f |?i=1t(h|f i)The complexity of the HMM extension of TAM isO(n3m2), where n is the number of words, and mis the number of morphemes per word.2.4 Variational BayesMoore (2004) showed that the EM algorithm is par-ticularly susceptible to overfitting in the case of rarewords when training IBM Model 1.
In order to pre-vent overfitting, we use the Variational Bayes ex-tension of the EM algorithm (Beal, 2003).
This36(a) Kas?m 1996?da, Tu?rk makamlar?, I?c?is?leri Bakanl?g??
bu?nyesinde bir kay?p kis?ileri arama birimi olus?turdu.
(b) Kas?m+Noun 1996+Num?Loc ,+Punc Tu?rk+Noun makam+Noun?A3pl?P3sg ,+Punc I?c?is?i+Noun?A3pl?P3sg Bakanl?k+Noun?P3sg bu?nye+Noun?P3sg?Loc bir+Det kay?p+Adj kis?i+Noun?A3pl?Acc ara+Verb?Inf2 birim+Noun?P3sg olus?+Verb?Caus?Past .+Punc(c) In November 1996 the Turkish authorities set up a missing persons search unit within the Ministry of theInterior.
(d) in+IN November+NNP 1996+CD the+DT Turkish+JJ author+NN?ity+N|N.
?NNS set+VB?VBD up+RPa+DT miss+VB?VBG+JJ person+NN?NNS search+NN unit+NN within+IN the+DT minister+NN?y+N|N.
of+IN the+DT interior+NN .+.
(e) In+IN November+NNP 1996+CD the+DT Turkish+JJ authorities+NNS set+VBD up+RP a+DT missing+JJpersons+NNS search+NN unit+NN within+IN the+DT Ministry+NNP of+IN the+DT Interior+NNP .+.Figure 3: Turkish-English data examplesamounts to a small change to the M step of the orig-inal EM algorithm.
We introduce Dirichlet priors ?to perform an inexact normalization by applying thefunction f(v) = exp(?
(v)) to the expected countscollected in the E step, where ?
is the digammafunction (Johnson, 2007).
?x|y =f(E[c(x|y)] + ?
)f(?j E[c(xj |y)] + ?
)We set ?
to 10?20, a very low value, to have the ef-fect of anti-smoothing, as low values of ?
cause thealgorithm to favor words which co-occur frequentlyand to penalize words that co-occur rarely.3 Experimental Setup3.1 DataWe trained our model on a Turkish-English paral-lel corpus of approximately 50K sentences, whichhave a maximum of 80 morphemes.
Our paralleldata consists mainly of documents in internationalrelations and legal documents from sources such asthe Turkish Ministry of Foreign Affairs, EU, etc.
Wefollowed a heavily supervised approach in morpho-logical analysis.
The Turkish data was first morpho-logically parsed (Oflazer, 1994), then disambiguated(Sak et al 2007) to select the contextually salient in-terpretation of words.
In addition, we removed mor-phological features that are not explicitly marked byan overt morpheme ?
thus each feature symbol be-yond the root part-of-speech corresponds to a mor-pheme.
Line (b) of Figure 3 shows an example ofa segmented Turkish sentence.
The root is followedby its part-of-speech tag separated by a ?+?.
Thederivational and inflectional morphemes that followthe root are separated by ???s.
In all experiments,we used the same segmented version of the Turkishdata, because Turkish is an agglutinative language.For English, we used the CELEX database(Baayen et al 1995) to segment English words intomorphemes.
We created two versions of the data:a segmented version that involves both derivationaland inflectional morphology, and an unsegmentedPOS tagged version.
The CELEX database providestags for English derivational morphemes, which in-dicate their function: the part-of-speech category themorpheme attaches to and the part-of-speech cate-gory it returns.
For example, in ?sparse+ity?
= ?spar-sity?, the morpheme -ity attaches to an adjective tothe right and returns a noun.
This behavior is repre-sented as ?N|A.?
in CELEX, where ?.?
indicates theattachment position.
We used these tags in additionto the surface forms of the English morphemes, inorder to disambiguate multiple functions of a singlesurface morpheme.The English sentence in line (d) of Figure 3 ex-hibits both derivational and inflectional morphology.For example, ?author+ity+s?=?authorities?
has bothan inflectional suffix -s and a derivational suffix -ity,whereas ?person+s?
has only an inflectional suffix -s.For both English and Turkish data, the dashes inFigure 3 stand for morpheme boundaries, thereforethe strings between the dashes are treated as indi-37Words MorphemesTokens Types Tokens TypesEnglish Der+Inf 1,033,726 27,758 1,368,188 19,448English POS 1,033,726 28,647 1,033,726 28,647Turkish Der+Inf 812,374 57,249 1,484,673 16,713Table 1: Data statisticsvisible units.
Table 1 shows the number of words,the number of morphemes and the respective vocab-ulary sizes.
The average number of morphemes insegmented Turkish words is 2.69, and the averagelength of segmented English words is 1.57.3.2 ExperimentsWe initialized our baseline word-only model with 5iterations of IBM Model 1, and further trained theHMM extension (Vogel et al 1996) for 5 iterations.We call this model ?baseline HMM?
in the discus-sions.
Similarly, we initialized the two versions ofTAM with 5 iterations of the model explained inSection 2.2, and then trained the HMM extension ofit as explained in Section 2.3 for 5 iterations.To obtain BLEU scores for TAM models andour implementation of the word-only model, i.e.baseline-HMM, we bypassed GIZA++ in the Mosestoolkit (Och and Ney, 2003).
We also ran GIZA++(IBM Model 1?4) on the data.
We translated 1000sentence test sets.4 Results and DiscussionWe evaluated the performance of our model in twodifferent ways.
First, we evaluated against goldword alignments for 75 Turkish-English sentences.Second, we used the word Viterbi alignments of ouralgorithm to obtain BLEU scores.Table 2 shows the AER (Och and Ney, 2003) ofthe word alignments of the Turkish-English pair andthe translation performance of the word alignmentslearned by our models.
We report the grow-diag-final (Koehn et al 2003) of the Viterbi alignments.In Table 2, results obtained with different versionsof the English data are represented as follows: ?Der?stands for derivational morphology, ?Inf?
for inflec-tional morphology, and ?POS?
for part-of-speechtags.
?Der+Inf?
corresponds to the example sen-tence in line (d) in Figure 3, and ?POS?
to line (e).?DIR?
stands for models with Dirichlet priors, and?NO DIR?
stands for models without Dirichlet pri-ors.
All reported results are of the HMM extensionof respective models.Table 2 shows that using Dirichlet priors hurtsthe AER performance of the word-and-morphememodel in all experiment settings, and benefits themorpheme-only model in the POS tagged experi-ment settings.In order to reduce the effect of nondeterminism,we run Moses three times per experiment setting,and report the highest BLEU scores obtained.
Sincethe BLEU scores we obtained are close, we did a sig-nificance test on the scores (Koehn, 2004).
Table 2visualizes the partition of the BLEU scores into sta-tistical significance groups.
If two scores within thesame column have the same background color, or theborder between their cells is removed, then the dif-ference between their scores is not statistically sig-nificant.
For example, the best BLEU scores, whichare in bold, have white background.
All scores in agiven experiment setting without white backgroundare significantly worse than the best score in that ex-periment setting, unless there is no border separatingthem from the best score.In all experiment settings, the TAM Models per-form better than the baseline-HMM.
Our experi-ments showed that the baseline-HMM benefits fromDirichlet priors to a larger extent than the TAM mod-els.
Dirichlet priors help reduce the overfitting inthe case of rare words.
The size of the word vo-cabulary is larger than the size of the morphemevocabulary.
Therefore the number of rare words islarger for words than it is for morphemes.
Conse-quently, baseline-HMM, using only the word vocab-38BLEUEN to TRBLEUTR to ENAERDer+Inf POS Der+Inf POS Der+Inf POSNODIRTAMMorph only 22.57 22.54 29.30 29.45 0.293 0.276Word & Morph 21.95 22.37 28.81 29.01 0.286 0.282WORDIBM 4 21.82 21.82 27.91 27.91 0.357 0.370Base-HMM 21.78 21.38 28.22 28.02 0.381 0.375IBM 4 Morph 17.15 17.94 25.70 26.33 N/A N/ADIRTAMMorph only 22.18 22.52 29.32 29.98 0.304 0.256Word & Morph 22.43 21.62 29.21 29.11 0.338 0.317WORDIBM 4 21.82 21.82 27.91 27.91 0.357 0.370Base-HMM 21.69 21.95 28.76 29.13 0.381 0.377IBM 4 Morph 17.15 17.94 25.70 26.33 N/A N/ATable 2: AER and BLEU Scoresulary, benefits from the use of Dirichlet priors morethan the TAM models.In four out of eight experiment settings, themorpheme-only model performs better than theword-and-morpheme version of TAM.
However,please note that our extensive experimentationwith TAM models revealed that the superiorityof the morpheme-only model over the word-and-morpheme model is highly dependent on segmenta-tion accuracy, degree of segmentation, and morpho-logical richness of languages.Finally, we treated morphemes as words andtrained IBM Model 4 on the morpheme segmentedversions of the data.
To obtain BLEU scores, wehad to unsegment the translation output: we con-catenated the prefixes to the morpheme to the right,and suffixes to the morpheme to the left.
Since thisprocess creates malformed words, the BLEU scoresobtained are much lower than the scores obtained byIBM Model 4, the baseline and the TAM Models.5 ConclusionWe presented two versions of a two-level alignmentmodel for morphologically rich languages.
We ob-served that information provided by word transla-tions and morpheme translations interact in a waythat enables the model to be receptive to the par-tial information in rarely occurring words throughtheir frequently occurring morphemes.
We obtainedsignificant improvement of BLEU scores over IBMModel 4.
In conclusion, morphologically awareword alignment models prove to be superior to theirword-only counterparts.Acknowledgments Funded by NSF award IIS-0910611.
Kemal Oflazer acknowledges the gen-erous support of the Qatar Foundation throughCarnegie Mellon University?s Seed Research pro-gram.
The statements made herein are solely theresponsibility of this author(s), and not necessarilythat of Qatar Foundation.ReferencesYaser Al-Onaizan, Jan Curin, Michael Jahr, KevinKnight, John Lafferty, Dan Melamed, Franz-JosefOch, David Purdy, Noah A. Smith, and DavidYarowsky.
1999.
Statistical machine translation.39Technical report, Final Report, JHU Summer Work-shop.R.H.
Baayen, R. Piepenbrock, and L. Gulikers.
1995.The CELEX Lexical Database (Release 2) [CD-ROM].Linguistic Data Consortium, University of Pennsylva-nia [Distributor], Philadelphia, PA.Matthew J. Beal.
2003.
Variational Algorithms for Ap-proximate Bayesian Inference.
Ph.D. thesis, Univer-sity College London.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19(2):263?311.Tagyoung Chung and Daniel Gildea.
2009.
Unsu-pervised tokenization for machine translation.
InEMNLP, pages 718?726.Martin C?mejrek, Jan Cur??
?n, and Jir???
Havelka.
2003.Czech-English dependency-based machine transla-tion.
In EACL, pages 83?90, Morristown, NJ, USA.Association for Computational Linguistics.Sharon Goldwater and David McClosky.
2005.
Improv-ing statistical MT through morphological analysis.
InHLT-EMNLP.Mark Johnson.
2007.
Why doesn?t EM find good HMMPOS-taggers?
In EMNLP-CoNLL, pages 296?305,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In HLT-NAACL.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In EMNLP, pages388?395.Young-suk Lee.
2004.
Morphological analysis for statis-tical machine translation.
In HLT-NAACL, pages 57?60.Robert C. Moore.
2004.
Improving IBM word alignmentmodel 1.
In ACL, pages 518?525, Barcelona, Spain,July.Jason Naradowsky and Kristina Toutanova.
2011.
Unsu-pervised bilingual morpheme segmentation and align-ment with context-rich Hidden Semi-Markov Models.In Proceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics: Human Lan-guage Technologies, pages 895?904, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.Sonja Niessen and Hermann Ney.
2000.
Improving SMTquality with morpho-syntactic analysis.
In Computa-tional Linguistics, pages 1081?1085, Morristown, NJ,USA.
Association for Computational Linguistics.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Kemal Oflazer.
1994.
Two-level description of Turkishmorphology.
Literary and Linguistic Computing, 9(2).Has?im Sak, Tunga Gu?ngo?r, and Murat Sarac?lar.
2007.Morphological disambiguation of Turkish text withperceptron algorithm.
In CICLing, pages 107?118,Berlin, Heidelberg.
Springer-Verlag.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In COLING, pages 836?841.Reyyan Yeniterzi and Kemal Oflazer.
2010.
Syntax-to-morphology mapping in factored phrase-based statis-tical machine translation from English to Turkish.
InACL, pages 454?464, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.40
