Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 643?648,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsSemantic Parsing for Single-Relation Question AnsweringWen-tau Yih Xiaodong He Christopher MeekMicrosoft ResearchRedmond, WA 98052, USA{scottyih,xiaohe,meek}@microsoft.comAbstractWe develop a semantic parsing frameworkbased on semantic similarity for open do-main question answering (QA).
We focuson single-relation questions and decom-pose each question into an entity men-tion and a relation pattern.
Using convo-lutional neural network models, we mea-sure the similarity of entity mentions withentities in the knowledge base (KB) andthe similarity of relation patterns and re-lations in the KB.
We score relationaltriples in the KB using these measuresand select the top scoring relational tripleto answer the question.
When evaluatedon an open-domain QA task, our methodachieves higher precision across differentrecall points compared to the previous ap-proach, and can improve F1by 7 points.1 IntroductionOpen-domain question answering (QA) is an im-portant and yet challenging problem that remainslargely unsolved.
In this paper, we focus on an-swering single-relation factual questions, whichare the most common type of question observed invarious community QA sites (Fader et al, 2013),as well as in search query logs.
We assumedsuch questions are answerable by issuing a single-relation query that consists of the relation and anargument entity, against a knowledge base (KB).Example questions of this type include: ?Who isthe CEO of Tesla??
and ?Who founded Paypal?
?While single-relation questions are easier tohandle than questions with more complex andmultiple relations, such as ?When was the child ofthe former Secretary of State in Obama?s admin-istration born?
?, single-relation questions are stillfar from completely solved.
Even in this restricteddomain there are a large number of paraphrases ofthe same question.
That is to say that the problemof mapping from a question to a particular relationand entity in the KB is non-trivial.In this paper, we propose a semantic parsingframework tailored to single-relation questions.At the core of our approach is a novel semanticsimilarity model using convolutional neural net-works.
Leveraging the question paraphrase datamined from the WikiAnswers corpus by Fader etal.
(2013), we train two semantic similarity mod-els: one links a mention from the question to anentity in the KB and the other maps a relation pat-tern to a relation.
The answer to the question canthus be derived by finding the relation?entity tripler(e1, e2) in the KB and returning the entity notmentioned in the question.
By using a general se-mantic similarity model to match patterns and re-lations, as well as mentions and entities, our sys-tem outperforms the existing rule learning system,PARALEX (Fader et al, 2013), with higher pre-cision at all the recall points when answering thequestions in the same test set.
The highest achiev-able F1score of our system is 0.61, versus 0.54 ofPARALEX.The rest of the paper is structured as follows.We first survey related work in Sec.
2, followed bythe problem definition and the high-level descrip-tion of our approach in Sec.
3.
Sec.
4 details oursemantic models and Sec.
5 shows the experimen-tal results.
Finally, Sec.
6 concludes the paper.2 Related WorkSemantic parsing of questions, which maps nat-ural language questions to database queries, isa critical component for KB-supported QA.
Anearly example of this research is the semanticparser for answering geography-related questions,learned using inductive logic programming (Zelleand Mooney, 1996).
Research in this line origi-nally used small, domain-specific databases, suchas GeoQuery (Tang and Mooney, 2001; Liang et643al., 2013).
Very recently, researchers have starteddeveloping semantic parsers for large, general-domain knowledge bases like Freebase and DB-pedia (Cai and Yates, 2013; Berant et al, 2013;Kwiatkowski et al, 2013).
Despite significantprogress, the problem remains challenging.
Mostmethods have not yet been scaled to large KBsthat can support general open-domain QA.
In con-trast, Fader et al (2013) proposed the PARALEXsystem, which targets answering single-relationquestions using an automatically created knowl-edge base, ReVerb (Fader et al, 2011).
Byapplying simple seed templates to the KB andby leveraging community-authored paraphrases ofquestions from WikiAnswers, they successfullydemonstrated a high-quality lexicon of pattern-matching rules can be learned for this restrictedform of semantic parsing.The other line of work related to our approachis continuous representations for semantic simi-larity, which has a long history and is still anactive research topic.
In information retrieval,TF-IDF vectors (Salton and McGill, 1983), latentsemantic analysis (Deerwester et al, 1990) andtopic models (Blei et al, 2003) take the bag-of-words approach, which captures well the contex-tual information for documents, but is often toocoarse-grained to be effective for sentences.
Ina separate line of research, deep learning basedtechniques have been proposed for semantic un-derstanding (Mesnil et al, 2013; Huang et al,2013; Shen et al, 2014b; Salakhutdinov and Hin-ton, 2009; Tur et al, 2012).
We adapt the workof (Huang et al, 2013; Shen et al, 2014b) for mea-suring the semantic distance between a questionand relational triples in the KB as the core compo-nent of our semantic parsing approach.3 Problem Definition & ApproachIn this paper, we focus on using a knowledgebase to answer single-relation questions.
A single-relation question is defined as a question com-posed of an entity mention and a binary rela-tion description, where the answer to this ques-tion would be an entity that has the relation withthe given entity.
An example of a single-relationquestion is ?When were DVD players invented?
?The entity is dvd-player and the relation isbe-invent-in.
The answer can thus be de-scribed as the following lambda expression:?x.
be-invent-in(dvd-player, x)Q?
RP ?M (1)RP ?
when were X invented (2)M ?
dvd players (3)when were X invented?
be-invent-in (4)dvd players?
dvd-player (5)Figure 1: A potential semantic parse of the ques-tion ?When were DVD players invented?
?A knowledge base in this work can be simplyviewed as a collection of binary relation instancesin the form of r(e1, e2), where r is the relation ande1and e2are the first and second entity arguments.Single-relation questions are perhaps the easiestform of questions that can directly be answeredby a knowledge base.
If the mapping of the re-lation and entity in the question can be correctlyresolved, then the answer can be derived by a sim-ple table lookup, assuming that the fact exists inthe KB.
However, due to the large number of para-phrases of the same question, identifying the map-ping accurately remains a difficult problem.Our approach in this work can be viewed as asimple semantic parser tailored to single-relationquestions, powered by advanced semantic similar-ity models to handle the paraphrase issue.
Given aquestion, we first separate it into two disjoint parts:the entity mention and the relation pattern.
Theentity mention is a subsequence of consecutivewords in the question, where the relation patternis the question where the mention is substitutedby a special symbol.
The mapping between thepattern and the relation in the KB, as well as themapping between the mention and the entity aredetermined by corresponding semantic similaritymodels.
The high-level approach can be viewedas a very simple context-free grammar, which isshown in Figure 1.The probability of the rule in (1) is 1 sincewe assume the input is a single-relation ques-tion.
For the exact decomposition of the ques-tion (e.g., (2), (3)), we simply enumerate all com-binations and assign equal probabilities to them.The performance of this approach depends mainlyon whether the relation pattern and entity mentioncan be resolved correctly (e.g., (4), (5)).
To deter-64415K 15K 15K 15K 15K500 500 500max max......... max500......Word hashing layer: ftConvolutional layer: htMax pooling layer: vSemantic layer: y<s>             w1              w2           wT             <s>Word sequence: xtWord hashing matrix: WfConvolution matrix: WcMax pooling operationSemantic projection matrix: Ws... ...500Figure 2: The CNNSM maps a variable-lengthword sequence to a low-dimensional vector in alatent semantic space.
A word contextual windowsize (i.e., the receptive field) of three is used in theillustration.
Convolution over word sequence vialearned matrix Wcis performed implicitly via theearlier word hashing layer?s mapping with a localreceptive field.
The max operation across the se-quence is applied for each of 500 feature dimen-sions separately.mine the probabilities of such mappings, we pro-pose using a semantic similarity model based onconvolutional neural networks, which is the tech-nical focus in this paper.4 Convolutional Neural Network basedSemantic ModelFollowing (Collobert et al, 2011; Shen et al,2014b), we develop a new convolutional neuralnetwork (CNN) based semantic model (CNNSM)for semantic parsing.
The CNNSM first uses aconvolutional layer to project each word within acontext window to a local contextual feature vec-tor, so that semantically similar word-n-grams areprojected to vectors that are close to each otherin the contextual feature space.
Further, since theoverall meaning of a sentence is often determinedby a few key words in the sentence, CNNSM usesa max pooling layer to extract the most salient lo-cal features to form a fixed-length global featurevector.
The global feature vector can be then fedto feed-forward neural network layers to extractnon-linear semantic features.
The architecture ofthe CNNSM is illustrated in Figure 2.
In what fol-lows, we describe each layer of the CNNSM indetail, using the annotation illustrated in Figure 2.In our model, we leverage the word hash-ing technique proposed in (Huang et al, 2013)where we first represent a word by a letter-trigram count vector.
For example, given aword (e.g., cat), after adding word boundary sym-bols (e.g., #cat#), the word is segmented into a se-quence of letter-n-grams (e.g., letter-trigrams: #-c-a, c-a-t, a-t-#).
Then, the word is representedas a count vector of letter-trigrams.
For exam-ple, the letter-trigram representation of ?cat?
is:In Figure 2, the word hashing matrix Wfde-notes the transformation from a word to its letter-trigram count vector, which requires no learning.Word hashing not only makes the learning morescalable by controlling the size of the vocabulary,but also can effectively handle the OOV issues,sometimes due to spelling mistakes.
Given theletter-trigram based word representation, we rep-resent a word-n-gram by concatenating the letter-trigram vectors of each word, e.g., for the t-thword-n-gram at the word-n-gram layer, we have:lt=[fTt?d, ?
?
?
, fTt, ?
?
?
, fTt+d]T, t = 1, ?
?
?
, Twhere ftis the letter-trigram representation of thet-th word, and n = 2d + 1 is the size of the con-textual window.
The convolution operation canbe viewed as sliding window based feature extrac-tion.
It captures the word-n-gram contextual fea-tures.
Consider the t-th word-n-gram, the convo-lution matrix projects its letter-trigram representa-tion vector ltto a contextual feature vector ht.
Asshown in Figure 2, htis computed byht= tanh(Wc?
lt), t = 1, ?
?
?
, Twhere Wcis the feature transformation matrix, asknown as the convolution matrix, which are sharedamong all word n-grams.
The output of the con-volutional layer is a sequence of local contextualfeature vectors, one for each word (within a con-textual window).
Since many words do not havesignificant influence on the semantics of the sen-tence, we want to retain in the global feature vectoronly the salient features from a few key words.
Forthis purpose, we use a max operation, also knownas max pooling, to force the network to retain only645the most useful local features produced by the con-volutional layers.
Referring to the max-poolinglayer of Figure 2, we havev(i) = maxt=1,???
,T{ft(i)}, i = 1, ?
?
?
,Kwhere v(i) is the i-th element of the max pool-ing layer v, ht(i) is the i-th element of the t-thlocal feature vector ht.
K is the dimensionalityof the max pooling layer, which is the same asthe dimensionality of the local contextual featurevectors {ht}.
One more non-linear transformationlayer is further applied on top of the global featurevector v to extract the high-level semantic repre-sentation, denoted by y.
As shown in Figure 2, wehave y = tanh(Ws?
v), where v is the global fea-ture vector after max pooling, Wsis the semanticprojection matrix, and y is the vector representa-tion of the input query (or document) in latent se-mantic space.
Given a pattern and a relation, wecompute their relevance score by measuring thecosine similarity between their semantic vectors.The semantic relevance score between a pattern Qand a relation R is defined as the cosine score oftheir semantic vectors yQand yR.We train two CNN semantic models from sets ofpattern?relation and mention?entity pairs, respec-tively.
Following (Huang et al, 2013), for everypattern, the corresponding relation is treated as apositive example and 100 randomly selected otherrelations are used as negative examples.
The set-ting for the mention?entity model is similar.The posterior probability of the positive relationgiven the pattern is computed based on the cosinescores using softmax:P (R+|Q) =exp(?
?
cos(yR+ , yQ))?R?exp(?
?
cos(yR?, yQ))where ?
is a scaling factor set to 5.
Model train-ing is done by maximizing the log-posteriori us-ing stochastic gradient descent.
More detail canbe found in (Shen et al, 2014a).5 ExperimentsIn order to provide a fair comparison to previ-ous work, we experimented with our approachusing the PARALAX dataset (Fader et al, 2013),which consists of paraphrases of questions minedfrom WikiAnswers and answer triples from Re-Verb.
In this section, we briefly introduce thedataset, describe the system training and evalua-tion processes and, finally, present our experimen-tal results.5.1 Data & Model TrainingThe PARALEX training data consists of ap-proximately 1.8 million pairs of questions andsingle-relation database queries, such as ?Whenwere DVD players invented?
?, paired withbe-invent-in(dvd-player,?).
For eval-uation, the authors further sampled 698 questionsthat belong to 37 clusters and hand labeled the an-swer triples returned by their systems.To train our two CNN semantic models, wederived two parallel corpora based on the PAR-ALEX training data.
For relation patterns, we firstscanned the original training corpus to see if therewas an exact surface form match of the entity (e.g.,dvd-player would map to ?DVD player?
in thequestion).
If an exact match was found, then thepattern would be derived by replacing the mentionin the question with the special symbol.
The corre-sponding relation of this pattern was thus the rela-tion used in the original database query, along withthe variable argument position (i.e., 1 or 2, indicat-ing whether the answer entity was the first or sec-ond argument of the relation).
In the end, we de-rived about 1.2 million pairs of patterns and rela-tions.
We then applied these patterns to all the 1.8million training questions, which helped discover160 thousand new mentions that did not have theexact surface form matches to the entities.When training the CNNSM for the pattern?relation similarity measure, we randomly split the1.2 million pairs of patterns and relations into twosets: the training set of 1.19 million pairs, andthe validation set of 12 thousand pairs for hyper-parameter tuning.
Data were tokenized by re-placing hyphens with blank spaces.
In the ex-periment, we used a context window (i.e., the re-ceptive field) of three words in the convolutionalneural networks.
There were 15 thousand uniqueletter-trigrams observed in the training set (usedfor word hashing).
Five hundred neurons wereused in the convolutional layer, the max-poolinglayer and the final semantic layer, respectively.We used a learning rate of 0.002 and the train-ing converged after 150 iterations.
A similar set-ting was used for the CNNSM for the mention?entity model, which was trained on 160 thousandmention-entity pairs.5.2 ResultsWe used the same test questions in the PARALEXdataset to evaluate whether our system could find646F1Precision Recall MAPCNNSMpm0.57 0.58 0.57 0.28CNNSMp0.54 0.61 0.49 0.20PARALEX 0.54 0.77 0.42 0.22Table 1: Performance of two variations of our sys-tems, compared with the PARALEX system.the answers from the ReVerb database.
Becauseour systems might find triples that were not re-turned by the PARALEX systems, we labeled thesenew question?triple pairs ourselves.Given a question, the system first enumeratedall possible decompositions of the mentions andpatterns, as described earlier.
We then computedthe similarity scores between the pattern and allrelations in the KB and retained 150 top-scoringrelation candidates.
For each selected relation, thesystem then checked all triples in the KB that hadthis relation and computed the similarity score be-tween the mention and corresponding argumententity.
The product of the probabilities of thesetwo models, which are derived from the cosinesimilarity scores using softmax as described inSec.
4, was used as the final score of the triple forranking the answers.
The top answer triple wasused to compute the precision and recall of the sys-tem when reporting the system performance.
Bylimiting the systems to output only answer tripleswith scores higher than a predefined threshold, wecould control the trade-off between recall and pre-cision and thus plot the precision?recall curve.Table 1 shows the performance in F1, preci-sion, recall and mean average precision of our sys-tems and PARALEX.
We provide two variationshere.
CNNSMpmis the full system and consistsof two semantic similarity models for pattern?relation and mention?entity.
The other model,CNNSMp, only measures the similarity betweenthe patterns and relations, and maps a mention toan entity when they have the same surface form.Since the trade-off between precision and re-call can be adjusted by varying the threshold, itis more informative to compare systems on theprecision?recall curves, which are shown in Fig-ure 3.
As we can observe from the figure, theprecision of our CNNSMpmsystem is consistentlyhigher than PARALEX across all recall regions.The CNNSMmsystem also performs similarly toCNNSMpmin the high precision regime, but is in-ferior when recall is higher.
This is understandable0.50.60.70.80.910  0.1  0.2  0.3  0.4  0.5  0.6PrecisionRecallCNNSMpm  CNNSMp  ParalexFigure 3: The precision?recall curves of the twovariations of our systems and PARALEX.since the system does not match mentions withentities of different surface forms (e.g., ?RobertHooke?
to ?Hooke?).
Notice that the highest F1values of them are 0.61 and 0.56, compared to0.54 of PARALEX.
Tuning the thresholds using avalidation set would be needed if there is a metric(e.g., F1) that specifically needs to be optimized.6 ConclusionsIn this work, we propose a semantic parsingframework for single-relation questions.
Com-pared to the existing work, our key insight is tomatch relation patterns and entity mentions usinga semantic similarity function rather than lexicalrules.
Our similarity model is trained using convo-lutional neural networks with letter-trigrams vec-tors.
This design helps the model go beyond bag-of-words representations and handles the OOV is-sue.
Our method achieves higher precision on theQA task than the previous work, PARALEX, con-sistently at different recall points.Despite the strong empirical performance, oursystem has room for improvement.
For in-stance, due to the variety of entity mentions inthe real world, the parallel corpus derived fromthe WikiAnswers data and ReVerb KB may notcontain enough data to train a robust entity link-ing model.
Replacing this component with adedicated entity linking system could improvethe performance and also reduce the number ofpattern/mention candidates when processing eachquestion.
In the future, we would like to extendour method to other more structured KBs, such asFreebase, and to explore approaches to extend oursystem to handle multi-relation questions.647ReferencesJonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on Freebase fromquestion-answer pairs.
In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing, pages 1533?1544, Seattle, Wash-ington, USA, October.
Association for Computa-tional Linguistics.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet alocation.
the Journal of ma-chine Learning research, 3:993?1022.Qingqing Cai and Alexander Yates.
2013.
Large-scale semantic parsing via schema matching and lex-icon extension.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 423?433,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.Ronan Collobert, Jason Weston, Leon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
Journal of Machine Learning Research.Scott Deerwester, Susan Dumais, Thomas Landauer,George Furnas, and Richard Harshman.
1990.
In-dexing by latent semantic analysis.
Journal of theAmerican Society for Information Science, 41(6).Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In Proceedings of the Conference of Em-pirical Methods in Natural Language Processing(EMNLP ?11), Edinburgh, Scotland, UK, July 27-31.Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.2013.
Paraphrase-driven learning for open questionanswering.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguis-tics (Volume 1: Long Papers), pages 1608?1618,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,Alex Acero, and Larry Heck.
2013.
Learning deepstructured semantic models for web search usingclickthrough data.
In Proceedings of the 22nd ACMinternational conference on Conference on informa-tion & knowledge management, pages 2333?2338.ACM.Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and LukeZettlemoyer.
2013.
Scaling semantic parsers withon-the-fly ontology matching.
In Proceedings ofthe 2013 Conference on Empirical Methods in Natu-ral Language Processing, pages 1545?1556, Seattle,Washington, USA, October.
Association for Compu-tational Linguistics.Percy Liang, Michael I Jordan, and Dan Klein.
2013.Learning dependency-based compositional seman-tics.
Computational Linguistics, 39(2):389?446.Gr?egoire Mesnil, Xiaodong He, Li Deng, and YoshuaBengio.
2013.
Investigation of recurrent-neural-network architectures and learning methods for spo-ken language understanding.
In Interspeech.Ruslan Salakhutdinov and Geoffrey Hinton.
2009.
Se-mantic hashing.
International Journal of Approxi-mate Reasoning, 50(7):969?978.Gerard Salton and Michael J. McGill.
1983.
Intro-duction to Modern Information Retrieval.
McGrawHill.Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,and Gr?egoire Mesnil.
2014a.
A convolutional latentsemantic model for web search.
Technical ReportMSR-TR-2014-55, Microsoft Research.Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,and Gr?egoire Mesnil.
2014b.
Learning semanticrepresentations using convolutional neural networksfor web search.
In Proceedings of the CompanionPublication of the 23rd International Conference onWorld Wide Web Companion, pages 373?374.Lappoon Tang and Raymond Mooney.
2001.
Usingmultiple clause constructors in inductive logic pro-gramming for semantic parsing.
In Machine Learn-ing: ECML 2001, pages 466?477.
Springer.Gokhan Tur, Li Deng, Dilek Hakkani-Tur, and Xi-aodong He.
2012.
Towards deeper understanding:deep convex networks for semantic utterance classi-fication.
In Acoustics, Speech and Signal Processing(ICASSP), 2012 IEEE International Conference on,pages 5045?5048.
IEEE.John Zelle and Raymond Mooney.
1996.
Learningto parse database queries using inductive logic pro-gramming.
In Proceedings of the National Confer-ence on Artificial Intelligence, pages 1050?1055.648
