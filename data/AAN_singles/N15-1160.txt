Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1403?1409,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsLarge-scale Native Language Identification with Cross-Corpus EvaluationShervin MalmasiCentre for Language TechnologyMacquarie UniversitySydney, NSW, Australiashervin.malmasi@mq.edu.auMark DrasCentre for Language TechnologyMacquarie UniversitySydney, NSW, Australiamark.dras@mq.edu.auAbstractWe present a large-scale Native LanguageIdentification (NLI) experiment on new data,with a focus on cross-corpus evaluation toidentify corpus- and genre-independent lan-guage transfer features.
We test a new corpusand show it is comparable to other NLI cor-pora and suitable for this task.
Cross-corpusevaluation on two large corpora achieves goodaccuracy and evidences the existence of reli-able language transfer features, but lower per-formance also suggests that NLI models arenot completely portable across corpora.
Fi-nally, we present a brief case study of fea-tures distinguishing Japanese learners?
En-glish writing, demonstrating the presence ofcross-corpus and cross-genre language trans-fer features that are highly applicable to SLAand ESL research.1 IntroductionNative Language Identification, the task of deter-mining the native language (L1) of an author basedon a second language (L2) text, has received muchattention recently.
Much of this is motivated by Sec-ond Language Acquisition (SLA) as NLI, often ac-complished via machine learning methods, can beused to study language transfer effects.Most NLI research hitherto has focused on identi-fying linguistic phenomena that can capture trans-fer effects, with little effort towards interpretingdiscriminant features.
Some researchers have nowshifted their focus to developing data-driven meth-ods for the automatic extraction and ranking of lin-guistic features that distinguish specific L1s (Swan-son and Charniak, 2014).Such methods could be used not only to confirmexisting SLA hypotheses, but also to create newones.
This hypothesis formulation is an inherentlydifficult problem requiring copious amounts of data.Contrary to this requirement, researchers have longnoted the paucity of suitable corpora1for this task(Brooke and Hirst, 2011).
This is one of the researchissues addressed by this work.Furthermore, deriving SLA hypotheses from asingle corpus may not be entirely useful for SLAresearch.
Many variables like genre and topic areconstant within a corpus, restricting the validity ofsuch cross-validation studies to those dimensions.An alternative, potentially more helpful approach,is to identify transfer features that reliably distin-guish an L1 across multiple corpora of differing gen-res and domains.
A cross-corpus methodology maybe a more promising avenue to finding features thatgeneralize to diverse text sources, but requires addi-tional large corpora.
It is also a more realistic ap-proach, and one we pursue in this work.Accordingly, the aims of the present work are to:(1) test a large new corpus suitable for NLI, (2)perform within-corpus evaluation with a compara-tive analysis against equivalent corpora, (3) performcross-corpus evaluation to determine the efficiencyof corpus independent features and (4) analyze thefeatures?
utility for SLA & ESL research.2 Background and MotivationNLI work has been growing in recent years, usinga wide range of syntactic and more recently, lexicalfeatures to distinguish the L1.
A detailed review ofNLI methods is omitted here for reasons of space,but a thorough exposition is presented in the reportfrom the very first NLI Shared Task that was held in2013 (Tetreault et al, 2013).Most English NLI work has been done using twocorpora.
The International Corpus of Learner En-1An ideal NLI corpus should have multiple L1s, be balancedby topic, proficiency, texts per L1 and be large in size.1403glish (Granger et al, 2009) was widely used un-til recently, despite its shortcomings2being widelynoted (Brooke and Hirst, 2012a).
More recently,TOEFL11, the first corpus designed for NLI wasreleased (Blanchard et al, 2013).
While it is thelargest NLI dataset available, it only contains argu-mentative essays, limiting analyses to this genre.Research has also expanded to use non-Englishlearner corpora (Malmasi and Dras, 2014a; Mal-masi and Dras, 2014c).
Recently, Malmasi and Dras(2014b) introduced the Chinese Learner Corpus forNLI and their results indicate that feature perfor-mance may be similar across corpora and even L1-L2 pairs.
This is a claim that we will test here.NLI is now also moving towards using features togenerate SLA hypotheses.
Swanson and Charniak(2014) approach this by using both L1 and L2 datato identify features exhibiting non-uniform usage inboth datasets, creating lists of candidate transfer fea-tures.
Malmasi and Dras (2014d) propose a differentmethod, using linear SVM weights to extract lists ofoverused and underused linguistic features for eachL1 group.Cross-corpus studies have been conducted forvarious data-driven NLP tasks, including parsing(Gildea, 2001), WSD (Escudero et al, 2000) andNER (Nothman et al, 2009).
While most such ex-periments show a drop in performance, the effectvaries widely across tasks, making it hard to predictthe expected drop for NLI.
We aim to address thisquestion using large training and testing data.3 EFCamDat: A new corpus for NLIThe EF Cambridge Open Language Database(EFCAMDAT) is an English L2 corpus that was re-leased recently (Geertzen et al, 2013).
It is com-posed of texts submitted to Englishtown, an onlineschool used by thousands of learners daily.This corpus is notable for its size, containingsome 550k texts from numerous nationalities, mak-ing it an ideal candidate for NLI research.
Whilethe TOEFL11 is made of argumentative essays, EF-CAMDAT has a much wider range of genres includ-ing writing emails, descriptions, letters, reviews, in-structions and more.In this work we present an application of NLI tothis new data.
As some of the texts can be short, weuse the methodology of Brooke and Hirst (2011) toconcatenate and create texts with at least 300 tokens,much like the TOEFL11.2The issues exist as the corpus was not designed for NLI.CommonArabic, Chinese, French, German, Italian,Japanese, Korean, Spanish, TurkishEFCAMDAT Portuguese, RussianTOEFL11 Hindi, TeluguTable 1: The 11 L1 classes extracted from the EFCAMDATcorpus, compared to the TOEFL11 corpus.
The first 9classes are common between both.From the data we choose 850 texts from each ofthe top 11 nationalities.
This subset of EFCAMDATthus consists of 9,350 documents totalling approxi-mately 3.2m tokens.
This is an average of 337 to-kens per text, close to the 348 tokens per text inTOEFL11.This also provides us with the same number ofclasses as the TOEFL11, as shown in Table 1, fa-cilitating direct performance comparisons.
The ta-ble also indicates the 9 classes common to both cor-pora.
This subset of common classes enables us toperform large-scale cross-corpus validation experi-ments that have not been possible until now.4 MethodologyWe use the standard NLI classification approach.
Alinear Support Vector Machine is used for classi-fication and feature vectors are created using rel-ative frequency values.
We also combine featureswith a mean probability ensemble classifier (Polikar,2006, ?4.2) using the probabilities assigned to eachclass.
We compare results with a random base-line and the oracle baseline used by Malmasi et al(2015).
The oracle correctly classifies a text if anyensemble member correctly predicts its label anddefines an upper-bound for classification accuracy.We avoid using lexical features as EFCAMDAT isnot topic balanced.
We extract the following topic-independent feature types:Function words are topic-independent grammat-ical words such as prepositions which indicate therelations between other words.
They are known tobe useful for NLI.
Frequencies of 400 English func-tion words3are extracted as features.
We also applyfunction word bigrams as described in Malmasi etal.
(2013).Context-free Grammar Production Rules areextracted after parsing each sentence.
Each rule isa classification feature (Wong and Dras, 2011) andcaptures global syntactic patterns.3Like previous work, this also includes stop words,which we sourced from the Onix Text Retrieval Toolkit:http://www.lextek.com/manuals/onix/stopwords1.html1404POS-1 POS-2 POS-3FW PR0102030405060Accuracy(%)EFCamDat TOEFL11 CLCFigure 1: Comparing EFCAMDAT feature performancewith the TOEFL11 and Chinese Learner Corpus (CLC).POS-1/2/3: POS uni/bi/trigrams, FW: Function Words,PR: CFG ProductionsPart-of-Speech (POS) n-grams of size 1?3 areextracted as features.
They capture preferences forword classes and their localized ordering patterns.5 Within-Corpus EvaluationOur first experiment applies 10-fold cross-validationwithin the corpus to assess feature efficacy.
The re-sults are shown in the first column of Table 2.All features perform substantially higher than the9% baseline.
POS trigrams are the best single fea-ture (53%), suggesting there exist significant inter-class syntactic differences.
Next, we also combinedall features using a classifier ensemble, which hasbeen shown to be helpful for NLI (Tetreault et al,2012).
This yields the best accuracy of 65% againstan upper-bound of 87% set by the oracle.We also compare these results to those from theTOEFL11 and Chinese Learner Corpus (CLC).
Asshown in Figure 1, we find that feature performanceis nearly identical across corpora.
Consistent withthe results in Malmasi and Dras (2014b), this seemsto suggest an invariant degree of transfer across dif-ferent learners and L1-L2 pairs.Figure 2 shows the confusion matrix.
German isthe most correctly classified L1, while the highestconfusion is between Japanese?Korean, followed bySpanish?Portuguese and French?Italian.
This is notsurprising given their syntactic similarity as well asbeing typologically related in case of the latter two.6 Large-scale Cross-Corpus EvaluationOur second experiment tests the cross-corpus effi-cacy of the features by training on EFCAMDAT andtesting on TOEFL11,4and vice versa.
As the corpustexts are from different genres, this approach enables4The 9 common classes discussed in ?3 are used.ARA CHI FRE GER ITA JPN KOR SPA TUR RUS PORPredicted labelARACHIFREGERITAJPNKORSPATURRUSPORTrue labelConfusion Matrix080160240320400480560640Figure 2: EFCAMDAT 11-class confusion matrix.Arabic German JapaneseSaudi Germany JapanArabia Berlin TokyoArabic Hamburg OsakaMohammed Frankfurt NagoyaAli Munich YenTable 3: Selected items from the top 15 most discrimina-tive words for Arabic/German/Japanese.us to test the cross-corpus and cross-genre general-izability of our features.Results are shown in the second and third columnof Table 2.
While lower than the cross-validationresults which were on 11 classes vs 9 here, theresults are far greater than the baseline.
The ac-curacy for training on EFCAMDAT and testing onTOEFL11 is higher (33.45%) than the other wayaround (28.42%), even though TOEFL11 is the largercorpus.
This is possibly because EFCAMDAT hasnumerous genres while TOEFL11 does not.
Thecross-corpus oracle is also over 20% lower, despitean increase in the random baseline, showing somefeatures are not portable across corpora.
Training onTOEFL11 yields a lower oracle.Although a performance drop was expected dueto the big genre differences, results suggest the pres-ence of some corpus-independent features that cap-ture cross-linguistic influence.
However, they alsosuggest that a large portion of the features helpfulfor NLI are genre-dependent.Previously, word n-grams have been applied insmall-scale cross-corpus studies and found to be thebest feature (Brooke and Hirst, 2012b).
Word n-grams have been previously used in NLI and are be-lieved to capture lexical transfer effects which havebeen previously noted by researchers and linguists1405Classification FeatureEFCAMDAT10-fold CVTrain EFCAMDATTest TOEFL11Train TOEFL11Test EFCAMDATRandom Baseline 9.09 11.11 11.11Oracle Baseline 86.84 64.92 62.43Function Word unigrams 52.01 27.14 21.77Function Word bigrams 47.92 29.21 22.63Production Rules 49.12 30.73 23.91Part-of-Speech unigrams 33.21 23.42 16.71Part-of-Speech bigrams 50.43 31.02 23.09Part-of-Speech trigrams 53.05 32.38 25.55Ensemble (All features) 64.95 33.45 28.42Word unigrams ?
41.82 42.48Table 2: Classification accuracy (%) for our within- and cross-corpus experiments.
(Odlin, 1989).
The effects are mediated not only bycognates and word form similarities, but also seman-tics and meanings.
Other NLI studies have also pro-vided empirical evidence for this hypothesis (Mal-masi and Cahill, 2015).However, issues stemming from topic bias5havealso limited their use in NLI (Brooke and Hirst,2012a), although use could be justified in cross-corpus scenarios due to the lower risk of topic-biasacross corpora.
We applied word unigrams to ourcross-corpus experiment, achieving an accuracy of41.8% for training on the EFCAMDAT and test-ing on TOEFL11 and 42.5% for the reverse setting.These are the best results in this setup.To check for any topic-bias effects, we inspectedthe most discriminative features for each L1 classusing the method proposed by Malmasi and Dras(2014d).
This analysis revealed that the top featureswere mostly cultural and geographic references re-lated to the author?s country.
Table 3 contains wordsselected from the top 15 most discriminative fea-tures found in the cross-corpus experiment for threeL1s.
We observe that most of these are toponymsor culture-specific terms such as names and curren-cies.
These results reveal another potential issuewith using lexical features.
Although this isn?t topic-bias, the features do not represent genuine linguis-tic differences or lexical transfer effects between L1groups.
In practical scenarios, this could also makeNLI systems vulnerable to content-based manipula-tion.
The exclusion of proper nouns is one way tocombat this.7 A Case Study of Japanese LearnersTo demonstrate the utility of this cross-corpus ap-proach we present a brief case study of features that5Due to correlations between text topics and L1 classes.characterize English writings of Japanese learners.We extracted the most discriminative cross-corpusfeatures of Japanese learner texts using the methodof Malmasi and Dras (2014d).Table 4 contains the top production rule features.The first rule shows a preference for having a subor-dinate clause before the main clause.
The next tworules show that Japanese learners tend to begin theirsentences with adverbs and conjunctions.
This pref-erence for placing information at the start of sen-tences is most likely rooted in the fact that Japaneseis an SOV head-final language6where dependentclauses generally precede the main clause and rel-ative clauses precede the noun they modify.
The in-fluence of this head-direction parameter on Englishacquisition has been previously investigated (Flynn,1989).In contrast, it is quite common for the main clauseto precede the subordinate clause in English.
Otherresearch has also noted that Japanese speakers havea ?long before short?
preference7(Yamashita andChang, 2001).
This is also evidence by anotherhighly discriminative rule for this L1: S ?
S ,CC S .Japanese writers also seem more likely to splitlonger arguments into multiple shorter sentences, assuggested by our third production rule.
It has alsobeen noted that Japanese and Korean sentences inthe TOEFL11 have the shortest mean length (Ciminoet al, 2013, p. 211).Turning to POS trigrams, the POS tag sequenceVBZ JJ NN is strongly linked to Japanese learn-6Contrasted with English which is SVO.7This refers to how conjuncts are ordered: short-before-longin English, long-before-short in Japanese.
Our findings suggestthat Japanese writers transfer this internal order-preference intotheir L2 English writing.1406Production Rule Example SentenceS ?
SBAR , NP VP .
If you have spare time, you?ll think of shopping.S ?
ADVP , NP VP .
Therefore, the online studying system is really convenient for me.S ?
CC NP VP .
But I?m not good at English.
/ But it wasn?t comfortable and cosy.Table 4: The top 3 cross-corpus production rule features for Japanese L1 with example lexicalizations.Overuse Underusehowever perhapsthough somebodycannot everythingtherefore behindsuch uponinto betweenTable 5: English function words overused and underusedby Japanese learners in their writing.ers.
It represents a third person verb, such as is orhas followed by an adjective and a noun.
A briefanalysis reveals that this is commonly observed inJapanese learner texts because the sequence is miss-ing a determiner before the noun phrase.8This likelystems from the fact that Japanese learners have diffi-culty with English articles, often failing to use them(Butler, 2002; Thomas, 1989).
Its prominence in theranked list shows that it is a common issue acrossdistinct learners and genres.The top overused and underused function wordsare listed in Table 5.
The words however and there-fore are highly relevant; Japanese writers often usethese to start sentences, possibly due to the above-mentioned production rules.
The word into is alsopredictive and seems to be used in places where in ismore appropriate.
This may be due to the Japanesewords for in, to and into being similar.9In the under-use list, perhaps is never used by Japanese learners.Other words here are low-frequency in Japanese L1texts in both corpora.8 DiscussionIn this work we presented the first application of oneof the largest and newest publicly available learnercorpora to NLI.
Cross-validation experiments mir-rored the performance of other corpora and demon-strated its utility for the task.
We believe this willmotivate future work by equipping researchers witha large-scale corpus that is highly suitable for NLI.8Example lexicalizations from EFCAMDAT include ?Shewears black top?
and ?This area is famous park.
?9All use the particle ni, see Takenobu et al (2005)Next, results from the largest cross-corpus NLIevaluation to date were presented, providing strongevidence for the presence of transfer features thatgeneralize across learners, corpora, topics and gen-res.
However, the fact that the cross-corpus accuracyis lower than within-corpus cross-validation high-lights that a large portion of the features are highlycorpus-specific.
This suggests that NLI models arenot entirely portable across corpora.
Practical appli-cations of NLI to forensic linguistics or SLA mustbe robust to input from numerous sources and theirassociated variations, and this finding highlights theneed for a cross-corpus approach.To demonstrate how this methodology could beused for SLA, an examination of the cross-corpusfeatures effective in classifying texts of Japaneselearners was conducted.
Through feature analysis,we were able to link these patterns of syntactic pro-ductions, article use and lexical choices to L1-basedSLA hypotheses.Our output lists hundreds of features, not includedor examined here due to space limitations, whoseanalysis would allow SLA researchers to exploreand generate new hypotheses, specially by combin-ing multiple syntactic feature types.A shortcoming here is that we did not balancetexts by proficiency to match the TOEFL11.
We ex-pect that a more even sampling of proficiency or us-ing proficiency-segregated models will yield higheraccuracy and features more representative of stu-dents at each proficiency level.Directions for future work are manifold.
The nextphase of this research will focus on developing toolsto derive and browse ranked lists of the most dis-criminative cross-corpus features, which will thenbe used to formulate SLA hypotheses.
Subject toavailability of data, this could be expanded to a mul-tiple cross-corpus methodology, using three or morecorpora.
Its application to other languages besidesEnglish is also of interest.NLI is a young but rapidly growing field of re-search and this study is but a first step in shiftingefforts towards a more interpretive approach to thetask.
We hope that the new dataset and directionspresented here will galvanize future work.1407ReferencesDaniel Blanchard, Joel Tetreault, Derrick Higgins, AoifeCahill, and Martin Chodorow.
2013.
TOEFL11: ACorpus of Non-Native English.
Technical report, Ed-ucational Testing Service.Julian Brooke and Graeme Hirst.
2011.
Native languagedetection with ?cheap?
learner corpora.
Presented atthe Conference of Learner Corpus Research, Univer-sity of Louvain, Belgium.Julian Brooke and Graeme Hirst.
2012a.
Measuring in-terlanguage: Native language identification with L1-influence metrics.
In Proceedings of the Eight Interna-tional Conference on Language Resources and Eval-uation (LREC?12), pages 779?784, Istanbul, Turkey,May.Julian Brooke and Graeme Hirst.
2012b.
Robust, Lexi-calized Native Language Identification.
In Proc.
Inter-nat.
Conf.
on Computat.
Linguistics (COLING).Yuko Goto Butler.
2002.
Second language learners?
the-ories on the use of english articles.
Studies in secondlanguage acquisition, 24(03):451?480.Andrea Cimino, Felice Dell?Orletta, Giulia Venturi, andSimonetta Montemagni.
2013.
Linguistic profilingbased on general?purpose features and native languageidentification.
In Proceedings of the Eighth Workshopon Innovative Use of NLP for Building EducationalApplications, pages 207?215, Atlanta, Georgia, June.Association for Computational Linguistics.Gerard Escudero, Llu?
?s M`arquez, and German Rigau.2000.
An empirical study of the domain dependenceof supervised word sense disambiguation systems.
InProceedings of the 2000 Joint SIGDAT conferenceon Empirical methods in natural language processingand very large corpora, pages 172?180.
Associationfor Computational Linguistics.Suzanne Flynn.
1989.
The role of the head-initial/head-final parameter in the acquisition of English relativeclauses by adult Spanish and Japanese speakers.
Lin-guistic perspectives on second language acquisition,pages 89?108.Jeroen Geertzen, Theodora Alexopoulou, and Anna Ko-rhonen.
2013.
Automatic Linguistic Annotation ofLarge Scale L2 Databases: The EF-Cambridge OpenLanguage Database (EFCamDat).Daniel Gildea.
2001.
Corpus variation and parser per-formance.
In Proceedings of the 2001 Conference onEmpirical Methods in Natural Language Processing,pages 167?202.Sylviane Granger, Estelle Dagneaux, Fanny Meunier,and Magali Paquot.
2009. International Corpus ofLearner English (Version 2).
Presses Universitaires deLouvain, Louvian-la-Neuve.Shervin Malmasi and Aoife Cahill.
2015.
Measur-ing Feature Diversity in Native Language Identifica-tion.
In Proceedings of the Tenth Workshop on Inno-vative Use of NLP for Building Educational Applica-tions, Denver, Colorado, June.
Association for Com-putational Linguistics.Shervin Malmasi and Mark Dras.
2014a.
Arabic NativeLanguage Identification.
In Proceedings of the Ara-bic Natural Language Processing Workshop (EMNLP2014), pages 180?186, Doha, Qatar, October.
Associ-ation for Computational Linguistics.Shervin Malmasi and Mark Dras.
2014b.
Chinese Na-tive Language Identification.
pages 95?99, Gothen-burg, Sweden, April.
Association for ComputationalLinguistics.Shervin Malmasi and Mark Dras.
2014c.
Finnish Na-tive Language Identification.
In Proceedings of theAustralasian Language Technology Workshop (ALTA),pages 139?144, Melbourne, Australia.Shervin Malmasi and Mark Dras.
2014d.
LanguageTransfer Hypotheses with Linear SVM Weights.
InProceedings of the 2014 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 1385?1390, Doha, Qatar, October.
Associationfor Computational Linguistics.Shervin Malmasi, Sze-Meng Jojo Wong, and Mark Dras.2013.
NLI Shared Task 2013: MQ Submission.
InProceedings of the Eighth Workshop on InnovativeUse of NLP for Building Educational Applications,pages 124?133, Atlanta, Georgia, June.
Associationfor Computational Linguistics.Shervin Malmasi, Joel Tetreault, and Mark Dras.
2015.Oracle and Human Baselines for Native LanguageIdentification.
In Proceedings of the Tenth Workshopon Innovative Use of NLP for Building EducationalApplications, Denver, Colorado, June.
Association forComputational Linguistics.Joel Nothman, Tara Murphy, and James R Curran.
2009.Analysing Wikipedia and gold-standard corpora forNER training.
In Proceedings of the 12th Conferenceof the European Chapter of the Association for Com-putational Linguistics, pages 612?620.
Association forComputational Linguistics.Terence Odlin.
1989.
Language Transfer: Cross-linguistic Influence in Language Learning.
CambridgeUniversity Press, Cambridge, UK.Robi Polikar.
2006.
Ensemble based systems in deci-sion making.
Circuits and Systems Magazine, IEEE,6(3):21?45.Ben Swanson and Eugene Charniak.
2014.
Data drivenlanguage transfer hypotheses.
EACL 2014, page 169.1408Tokunaga Takenobu, Koyama Tomofumi, and Saito Sug-uru.
2005.
Meaning of japanese spatial nouns.
In Pro-ceedings of the Second ACL-SIGSEM Workshop on theLinguistic Dimensions of Prepositions and their Usein Computational Linguistics Formalisms and Appli-cations, pages 93?100.Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-tin Chodorow.
2012.
Native Tongues, Lost andFound: Resources and Empirical Evaluations in Na-tive Language Identification.
In Proceedings of COL-ING 2012, pages 2585?2602, Mumbai, India, Decem-ber.
The COLING 2012 Organizing Committee.Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013.A Report on the First Native Language IdentificationShared Task.
In Proceedings of the Eighth Workshopon Innovative Use of NLP for Building EducationalApplications, pages 48?57, Atlanta, Georgia, June.Association for Computational Linguistics.Margaret Thomas.
1989.
The acquisition of English ar-ticles by first-and second-language learners.
AppliedPsycholinguistics, 10(03):335?355.Sze-Meng Jojo Wong and Mark Dras.
2011.
ExploitingParse Structures for Native Language Identification.In Proceedings of the 2011 Conference on Empiri-cal Methods in Natural Language Processing, pages1600?1610, Edinburgh, Scotland, UK., July.
Associa-tion for Computational Linguistics.Hiroko Yamashita and Franklin Chang.
2001.
?Longbefore short?
preference in the production of a head-final language.
Cognition, 81(2):B45?B55.1409
