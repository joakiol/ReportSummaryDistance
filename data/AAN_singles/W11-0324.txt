Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 210?218,Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational LinguisticsA Normalized-Cut Alignment Model for Mapping Hierarchical SemanticStructures onto Spoken DocumentsXiaodan ZhuInstitute for Information TechnologyNational Research Council CanadaXiaodan.Zhu@nrc-cnrc.gc.caAbstractWe propose a normalized-cut model for theproblem of aligning a known hierarchicalbrowsing structure, e.g., electronic slides oflecture recordings, with the sequential tran-scripts of the corresponding spoken docu-ments, with the aim to help index and accessthe latter.
This model optimizes a normalized-cut graph-partitioning criterion and considerslocal tree constraints at the same time.
The ex-perimental results show the advantage of thismodel over Viterbi-like, sequential alignment,under typical speech recognition errors.1 IntroductionLearning semantic structures of written text has beenstudied in a number of specific tasks, which include,but not limited to, those finding semantic represen-tations for individual sentences (Ge and Mooney,2005; Zettlemoyer and Collins, 2005; Lu et al,2008), and those constructing hierarchical structuresamong sentences or larger text blocks (Marcu, 2000;Branavan et al, 2007).
The inverse problem of thelatter kind, e.g., aligning certain form of already-existing semantic hierarchies with the correspondingtext sequence, is not so much a prominent problemfor written text as it is for spoken documents.
In thispaper, we study a specific type of such a problem, inwhich a hierarchical browsing structure, i.e., elec-tronic slides of oral presentations, have already ex-isted, the goal being to impose such a structure ontothe transcripts of the corresponding speech, with theaim to help index and access spoken documents assuch.Navigating audio documents is often inherentlymuch more difficult than browsing text; an obvi-ous solution, in relying on human beings?
ability toread text, is to conduct a speech-to-text conversionthrough automatic speech recognition (ASR).
Im-plicitly, solutions as such change the conventionalspeaking-for-hearing construals: now speech can beread through its transcripts, though, in most cases,it was not intended for this purpose, which in turnraises a new set of problems.The convenience and efficiency of reading tran-scripts (Stark et al, 2000; Munteanu et al, 2006)are first affected by errors produced in transcrip-tion channels for various reasons, though if the goalis only to browse salient excerpts, recognition er-rors on the extracts can be reduced by consider-ing ASR confidence scores (Xie and Liu, 2010;Hori and Furui, 2003; Zechner and Waibel, 2000):trading off the expected salience of excerpts withtheir recognition-error rate could actually result inthe improvement of excerpt quality in terms of theamount of important content being correctly pre-sented (Zechner and Waibel, 2000).Even if transcription quality were not a problem,browsing transcripts is not straightforward.
Whenintended to be read, written documents are almostalways presented as more than uninterrupted stringsof text.
Consider that for many written docu-ments, e.g., books, indicative structures such as sec-tion/subsection headings and tables-of-contents arestandard constituents created manually to help read-ers.
Structures of this kind, even when existing, arerarely aligned with spoken documents completely.This paper studies the problem of imposing a210known hierarchical browsing structure, e.g., theelectronic slides of lecture recordings, onto the se-quential transcripts of the corresponding spokendocument, with the aim to help index and hence ac-cess the latter more effectively.
Specifically, we pro-pose a graph-partitioning approach that optimizes anormalized-cut criterion globally, in traversing thegiven hierarchical semantic structures.
The exper-imental results show the advantage of this modelover Viterbi-like, sequential alignment, under typi-cal speech recognition errors.2 Related workFlat structures of spoken documents Much pre-vious work, similar to its written-text counterpart,has attempted to find certain flat structures of spokendocuments, such as topic and slide boundaries.
Forexample, the work of (Chen and Heng, 2003; Rud-darraju, 2006; Zhu et al, 2008) aims to find slideboundaries in the corresponding lecture transcripts.Malioutov et al (2007) developed an approach todetecting topic boundaries of lecture recordings byfinding repeated acoustic patterns.
None of thiswork, however, has involved hierarchical structuresof a spoken document.
Research has also resortedto other multimedia channels, e.g., video (Liu et al,2002; Wang et al, 2003; Fan et al, 2006), to detectslide transitions.
This type of research, however, isunlikely to recover semantic structures in more de-tails than slide boundaries.Hierarchical structures of spoken documentsRecently, research has started to align hierarchicalbrowsing structures with spoken documents, giventhat inferring such structures directly from spokendocuments is still too challenging.
Zhu et al (2010)investigates bullet-slide alignment by first sequen-tializing bullet trees with a pre-order walk beforeconducting alignment, through which the problemis reduced to a string-to-string alignment problemand an efficient Viterbi-like method can be naturallyapplied.
In this paper, we use such a sequentialalignment as our baseline, which takes a standarddynamic-programming process to find the optimalpath on an M-by-N similarity matrix, where M andN denote the number of bullets and utterances in alecture, respectively.
Specifically, we chose the paththat maps each bullet to an utterance to achieve thehighest total bullet-utterance similarity score; thispath can be found within a standard O(MN2) timecomplexity.A pre-order walk of the hierarchical tree is a natu-ral choice, since speakers of presentations often fol-low such a order in developing their talk; i.e., theyoften talk about a bullet first and then each of its chil-dren in sequence.
A pre-order walk is also assumedby Branavan et al (2007) in their table-of-contentgeneration task, a problem in which a hierarchicalstructure has already been assumed (aligned) with aspan of written text, but the title of each node needsto be generated.In principle, such a sequential-alignment ap-proach allows a bullet to be only aligned to one ut-terance in the end, which does not model the basicproperties of the problem well, where the content ina bullet is often repeated not only when the speakertalks about it but also, very likely, when he discussesthe descendant bullets.
Second, we suspect thatspeech recognition errors, when happening on thecritical anchoring words that bridging the alignment,would make a sequential-alignment algorithm muchless robust, compared with methods based on many-to-many alignment.
This is very likely to happen,considering that domain-specific words are likely tobe the critical words in deciding the alignment, butthey are also very likely to be mis-recognized by anASR system at the same time, e.g., due to out-of-vocabulary issue or language-model sparseness.
Wewill further discuss this in more details later in ourresult section.
Third, the hierarchical structures arelost in the sequentialization of bullets, though someremedy could be applied, e.g., by propagating a par-ent bullet?s information onto its children (Zhu et al,2010).On the other hand, we should also note that thebenefit of formulating the problem as a sequentialalignment problem is its computational efficiency:the solution can be calculated with conventionalViterbi-like algorithms.
This property is also impor-tant for the task, since the length of a spoken docu-ment, such as a lecture, is often long enough to makeinefficient algorithms practically intractable.An important question is therefore how to, in prin-ciple, model the problem better.
The second is howtime efficient the model is.
Malioutov and Barzi-lay (2006) describe a dynamic-programming version211of a normalized-cut-based model in solving a topicsegmentation problem for spoken documents.
In-spired by their work, we will propose a model basedon graph partitioning in finding the correspondencebetween bullets and the regions of transcripts thatdiscuss them; the proposed model runs in polyno-mial time.
We will empirically show its benefit onboth improving the alignment performance over asequential alignment and its robustness to speechrecognition errors.3 ProblemWe are given a speech sequence U = u1, u2, ..., uN ,where ui is an utterance, and the corresponding hi-erarchical structure, which, in our work here, is asequence of lecture slides containing a set of slide ti-tles and bullets, B = {b1, b2, ..., bM}, organized in atree structure T (<,?,?
), where < is the root of thetree that concatenates all slides of a lecture; i.e., eachslide is a child of the root < and each slide?s bulletsform a subtree.
In the rest of this paper, the wordbullet means both the title of a slide (if any) and anybullet in it, if not otherwise noted.
?
is the set ofnodes of the tree (both terminal and non-terminals,excluding the root <), each corresponding to a bulletbm in the slides.
?
is the edge set.
With the defini-tions, our task is herein to find the triple (bi, uj , uk),denoting that a bullet bi is mapped to a region of lec-ture transcripts that starts from the jth utterance ujand ends at the kth, inclusively.
Constrained by thetree structure, the transcript region corresponding toan ancestor bullet contains those corresponding toits descendants; i.e., if a bullet bi is the ancestor ofanother bullet bn in the tree, the acquired boundarytriples (bi, uj1 , uk1) and (bi, uj2 , uk2) should satisfyj1 ?
j2 and k1 ?
k2.
Figure 1 shows a slide, itsstructure, and the correspondence between one ofits bullets and a region of transcribed utterances (theroot that concatenates all such slides of a lecture to-gether is not shown here).4 A graph-partitioning approachThe generative process of lecture speech, with re-gard to a hierarchical structure (here, bullet trees),is characterized in general by a speaker?s producingdetailed content for each bullet when discussing it,during which sub-bullets, if any, are talked about re-Figure 1: A slide, its tree structure, and the correspon-dence between one of its bullets and a region of tran-scribed utterances (uj, uj+1..., uk).cursively.
By its nature of the problem, words in abullet could be repeated multiple times, even whenthe speaker traverses to talk about the descendantbullets in the depth of the sub-trees.
In principle,a model would be desirable to consider such proper-ties between a slide bullet, including all its descen-dants, and utterance transcripts, as well as the con-straints of bullet trees.
We formulate the problemof finding the correspondence between bullets andtranscripts as a graph-partitioning problem, as de-tailed below.The correspondence between bullets and tran-scribed utterances is evidenced by the similaritiesbetween them.
In a graph that contains a set of bul-lets and utterances as its vertices and similarities be-tween them as its edges, our aim is to place bound-aries to partition the graph into smaller ones in orderto obtain triples, e.g., (bi, uj , uk), that optimize cer-tain criterion.
Inspired by the work of (Malioutovand Barzilay, 2006; Shi and Malik, 2000), we op-timize a normalized-cut score, in which the totalweight of edges being cut by the boundaries is mini-mized, normalized by the similarity between the bul-let bi and the entire vertices, as well as between thetranscript region uj , ..., uk and the entire vertices,respectively.Consider a simple two-set case first, in which aboundary is placed on a graph G = (V,E) to sepa-rate its vertices V into two sets, A and B, with all theedges between these two sets being removed.
Theobjective, as we have mentioned above, is to mini-mize the following normalized-cut score:212Ncut(A,B) = cut(A,B)assoc(A,V ) +cut(A,B)assoc(B,V ) (1)where,cut(A,B) =?a?A,b?Bw(a, b)assoc(A,V ) =?a?A,v?Vw(a, v)assoc(B,V ) =?b?B,v?Vw(b, v)In equation (1), cut(A,B) is the total weight ofthe edges being cut, i.e., those connecting A withB, while assoc(A,V ) and assoc(B,V ) are the totalweights of the edges that connect A with all verticesV , and B with V , respectively; w(a, b) is an edgeweight between a vertex a and b.In general, minimizing such a normalized-cutscore has been shown to be NP-complete.
In ourproblem, however, the solution is constrained bythe linearity of segmentation on transcripts, simi-lar to that in (Malioutov and Barzilay, 2006).
Insuch a situation, a polynomial-time algorithm exists.Malioutov and Barzilay (2006) describe a dynamic-programming algorithm to conduct topic segmenta-tion for spoken documents.
We modify the methodto solve our alignment problem here, which, how-ever, needs to cope with the bipartite graphs betweenbullets and transcribed sentences rather than sym-metric similarity matrices among utterances them-selves.
We also need to integrate this in consideringthe hierarchical structures of bullet trees.We first consider a set of sibling bullets, b1, ..., bm,that appear on the same level of a bullet tree andshare the same parent bp.
For the time being, weassume the corresponding region of transcripts hasalready been identified for bp, say u1, ..., un.
Weconnect each bullet in b1, ..., bm with utterances inu1, ..., un by their similarity, which results in a bi-partite graph.
Our task here is to place m ?
1boundaries onto the bipartite graph to partition thegraph into m bipartite graphs and obtain triples, e.g.,(bi, uj , uk), to align bi to uj, ..., uk , where bi ?
{b1, ..., bm} and uj, uk ?
{u1, ..., bn} and j <= k.Since we have all descendant bullets to help the par-titioning, when constructing the bipartite graph, weactually include also all descendant bullets of eachbullet bi, but ignoring their orders within each bi.We will revisit this in more details later.
We findoptimal normalized cuts in a dynamic-programmingprocess with the following recurrence relation:C[i, k] = minj?k{C[i?
1, j] +D[i, j + 1, k]} (2)B[i, k] = argminj?k{C[i?1, j]+D[i, j +1, k]} (3)In equation (2) and (3), C[i, k] is the opti-mal/minimal normalized-cut value of aligning thefirst i sibling bullets, b1, ..., bi, with the first k ut-terances, u1, ..., bk , while B[i, k] records the back-tracking indices corresponding to the optimal pathyielding the current C[i, k].
As shown in equation(2), C[i, k] is computed by updating C[i?
1, j] withD[i, j + 1, k], for all possible j s.t.
j ?
k, whereD[i, j +1, k] is a normalized-cut score for the triple(bi, uj+1, uk) and is defined as follows:D[i, j + 1, k] = cut(Ai,j+1,k, V \Ai,j+1,k)assoc(Ai,j+1,k, V )(4)where Ai,j+1,k is the vertex set that contains thebullet bi (including its descendant bullets, if any,as discussed above) and the utterances uj+1, ..., uk ;V \ Ai,j+1,k is its complement set.Different from the topic segmentation problem(Malioutov et al, 2007), we need to remember thenormalized-cut values between any region uj , ..., ukand any bullet bi in our task, so we need to usethe additional subscript i in Ai,j+1,k, while in topicsegmentation, the computation of both cut(.)
andassoc(.)
is only dependant on the left boundary jand right boundary k. Note that the similarity matrixhere is not symmetric as it is in topic segmentation,but m by n, where m is the number of bullets, whilen is the number of utterances.For any triple (bi, uj+1, uk), there are two differ-ent types of edges being cut: those between Bindef={bi} (again, including bi and all its descendant bul-lets) and Uout def= {u1, ..., uj , uk+1, ..., um}, as wellas those between Bout def= {b1, ..., bi?1, bi+1, ..., bm}and Uin def= {uj+1, ..., uk}.
We discriminatethese two types of edges.
Accordingly, cut(.)
and213assoc(.)
in equation (4) are calculated with equation(5) and (6) below by linearly combining the weightsof these two types of edges with ?, whose value isdecided with a small held-out data.cut(Ai,j+1,k, V \ Ai,j+1,k) =?
?b?Bin,u?Uoutw(b, u)+(1?
?)?b??Bout,u?
?Uinw(b?, u?)
(5)assoc(Ai,j+1,k, V ) = ?
?b?Bin,u?Vw(b, u)+(1?
?)?b??Uin,u?
?Vw(b?, u?)
(6)In addition, different form that in topic segmen-tation, where a segment must not be empty, weshall allow a bullet bi to be aligned to an emptyregion, to model the situation that a bullet is notdiscussed by the speaker.
To do so, we made j inequation (2) and (3) above to be able to equal tok in the subscript, i.e., j ?
k. Specifically, whenj = k, the set Ai,j+1,k has no internal edges, andD[i, j + 1, k] is either equal to 1, or often not de-fined if assoc(Ai,j+1,k, V ) = 0.
For the latter, wereset D[i, j + 1, k] to be 1.A visual example of partitioning sibling bulletsb1, b2, and b3 is shown in Figure 2, in which thedescendant bullets of them (here, b4, b5, and b6) arealso considered.
Note that we only show direct chil-dren of b1 here, while, as discussed above, all de-scendant bullets, if any, will be considered.Figure 2: A visual example of partitioning sibling bulletsb1, b2, and b3.Up to now, we have only considered partition-ing sibling bullets by assuming the boundaries oftheir parent on lecture transcripts have already beengiven, where the sibling bullets and the correspond-ing transcripts form a bipartite graph.
When parti-tioning the entire bullet trees and all utterances for alecture, the graph contains not only a bipartite graphbut also the hierarchical trees themselves.
We de-couple this two parts of graph by a top-down traver-sal of the bullet trees: starting from the root, for eachnode on the bullet tree, we apply the normalized-cutalgorithm discussed above to find the correspondingregions of transcripts for all its direct children, andrepeat this process recursively.
In each visit to parti-tion a group of sibling bullets, to allow the first childto have a different starting point from its parent bul-let (the speaker may spend some time on the parentbullet itself before talking about each child bullet),we inserted an extra child in front of the first childand copy the text of the parent bullet to it.
Note thatin each visit to partition a group of sibling bullets,the solution found is optimal on that level, which,again, results in a powerful model since all descen-dant bullets, if any, are all considered.
For exam-ple, processing high-level bullets first is expectedto benefit from the richer information of using alltheir descendants in helping find the boundaries ontranscripts accurately.
Recall that we have discussedabove how to incorporate the descendant bullets intothis process.
It would also dramatically reduce thesearching space of partitioning lower-level bullets.As far as computational complexity is concerned,the graph-partitioning method discussed above ispolynomial, O(MN2), with M and N denotingthe number of bullets and utterances in a lecture,respectively.
Note that M is often much smallerthan N , M  N .
In more details, the loop ker-nel of the algorithm is computing D[i, j, k].
Thisin total needs to compute 12 (MN2) values, whichcan be pre-calculated and stored before dynamic-programming decoding runs; the later, as normal, isO(MN2), too.5 Experiment set-up5.1 CorpusOur experiment uses a corpus of four 50-minutethird-year university lectures taught by the same in-structor on the topics of human-computer interac-tion (HCI), which contain 119 slides composed of214921 bullets prepared by the lecturer himself.
Theautomatic transcripts of the speech contain approxi-mately 30,000 word tokens, roughly equal to a 120-page double-spaced essay in length.
The lecturer?svoice was recorded with a head-mounted micro-phone with a 16kHz sampling rate and 16-bit sam-ples, while students?
comments and questions werenot recorded.
The speech is split into utterances bypauses longer than 200ms, resulting in around 4000utterances.
The slides and automatic transcripts ofone lecture were held out to decide the value of ?
indifferentiating the two different types of edges be-ing cut, as discussed in Section 4.
The boundariesbetween adjacent slides were marked manually dur-ing the lectures were recorded, by the person whooversaw the recording process, while the boundariesbetween bullets within a slide were annotated after-wards by another human annotator.5.2 Building the graphsThe lecture speech was first transcribed into text au-tomatically with ASR models.
The first ASR modelis a baseline with its acoustic model trained on theWSJ0 and WSJ1 subsets of the 1992 developmentset of the Wall Street Journal (WSJ) dictation cor-pus, which contains 30 hours of data spoken by283 speakers.
The language model was trained onthe Switchboard corpus, which contains 2500 tele-phone conversations involving about 500 English-native speakers, which was suggested to be suit-able for the conversational style of lectures, e.g.,by (Munteanu et al, 2007; Park et al, 2005).
Thewhole model yielded a word error rate (WER) at0.48.
In the remainder of this paper, we call themodel as ASR Model 1.The second model is an advanced one using thesame acoustic model.
However, its language modelwas trained on domain-related documents obtainedfrom the Web through searching the words appear-ing on slides, as suggested by Munteanu et al(2007).
This yielded a WER of 0.43, which is atypical WER for lectures and conference presenta-tions (Leeuwis et al, 2003; Hsu and Glass, 2006;Munteanu et al, 2007), though a lower WER ispossible in a more ideal condition (Glass et al,2007), e.g., when the same course from the previoussemester by the same instructor is available.
The 3-gram language models were trained using the CMU-CAM Language Modelling Toolkit (Clarkson andRosenfeld, 1997), and the transcripts were generatedwith the SONIC toolkit (Pellom, 2001).
The out-of-vocabulary rates are 0.3% in the output of ASRModel 1 and 0.1% in that of Model 2, respectively.Both bullets and automatic transcripts werestemmed and stop words in them were removed.
Wethen calculated the similarity between a bullet andan utterance with the number of overlapping wordsshared, normalized by their lengths.
Note that usingseveral other typical metrics, e.g., cosine, resultedin a similar trend of performance change?our con-clusions below are consistent under these situations,though the specific performance scores (i.e., wordoffsets) are different.
Finally, the similarities be-tween bullets and utterances yielded a single M-by-N similarity matrix for each lecture to be aligned,with M and N denoting the number of bullets inslides and utterances in transcripts, respectively.5.3 Evaluation metricThe metric used in our evaluation isstraightforward?automatically acquired bound-aries on transcripts for each slide bullet arecompared against the corresponding gold-standardboundaries to calculate offsets measured in numberof words.
The offset scores are averaged over allboundaries to evaluate model performance.
Thoughone may consider that different bullets may be ofdifferent importance, in this paper we do not useany heuristics to judge this and we treat all bulletsequally in our evaluation.Note that topic segmentation research often usesmetrics such as Pk and WindowDiff (Malioutovet al, 2007; Beeferman et al, 1999; Pevsner andHearst, 2002).
Our problem here, as an alignmentproblem, has an exact 1-to-1 correspondence be-tween a gold and automatic boundary, in which wecan directly measure the exact offset of each bound-ary.6 Experimental resultsTable 1 presents the experimental results obtainedon the automatic transcripts generated by the ASRmodels discussed above, with WERs at 0.43 and0.48, respectively, which are typical WERs for lec-tures and conference presentations in realistic and215less controlled situations.
SEQ-ALN in the tablestands for the Viterbi-like, sequential alignment dis-cussed above in section 2, while G-CUT is thegraph-partitioning approach proposed in this paper.The values in the table are the average word-offsetscores counted after stop-words having been re-moved.WER=0.43 WER=0.48SEQ-ALN 15.22 20.38G-CUT 13.41 16.77Offs.
Reduction 12% 18%Table 1: The average word offsets of automatic bound-aries from the gold-standard.Table 1 shows that comparing these twopolynomial-time models, G-CUT reduces the aver-age offsets of SEG-ALN under both WERs.
On thetranscripts with 0.48 WER, the average word-offsetscore is reduced by approximately 18% from 20.38to 16.77, while for the transcripts with WER at 0.43,the offset reduction is 12%, from 15.22 to 13.41.Since both models use exactly the same input simi-larity matrices, the differences between their resultsconfirm the advantage of the modeling principle be-hind the proposed approach.
Although the graph-partitioning model could be extended further, e.g.,with the approach in (Zhu et al, 2010), our primaryinterest here is the principle modeling advantage ofthis normalized-cut framework.The results in Table 1 also suggest that the graph-partitioning model is more robust to speech recog-nition errors: when WERs increase from 0.43 to0.48, the error of G-CUT increases by 25%, from13.41 to 16.77, while that of SEQ-ALN increases by44%, from 15.22 to 20.38.
We due this to the factthat the graph-partitioning model considers multiplealignments between bullets, including their descen-dants, and the transcribed utterances, where mis-matching between bullet and transcript words, e.g.,that caused by recognition errors, is less likely toimpact the graph-partitioning method, which basesits optimization criterion on multiple alignments,e.g., when calculating cut(.)
and assoc(.)
in equa-tion (5) and (6).
Recall that the ASR Model 2 in-cludes domain-specific Web data to train the lan-guage models, which were acquired by using bul-let words to search the Web.
It is expected to in-crease the recognition accuracy on domain words,particularly those appearing on the slides.
There-fore, Model 2 is likely to particularly increase thecorrect matching between bullets and transcript.The results in Table 1 also show the usefulnessof better ASR modeling on the structure-imposingtask here.
As discussed in the introduction sec-tion earlier, browsing automatic transcripts of longspoken documents, such as lectures, is affected byboth speech recognition errors and lack of browsingstructures.
Table 1 shows that the improvement insolving the first problem also helps the second.Last, from a pragmatic viewpoint of system de-velopment, the graph-partitioning algorithm is sim-ple to implement: the essence of equation (2)-(6) isto find the optimal normalized-cut score character-ized by computing D[i, j + 1, k] and updating theformulae with it, which is not much more compli-cate to build than the baseline.
Also, the practicalspeed difference between these two types of modelsis not obvious on our dataset.7 ConclusionThis paper proposes a graph-partitioning approachfor aligning a known hierarchical structure with thetranscripts of the corresponding spoken documentthrough optimizing a normalized-cut criterion.
Thisapproach models the basic properties of the prob-lem and is quadratic-time.
Experimental resultsshow both its advantage on improving the alignmentperformance over a standard sequential-alignmentbaseline and its robustness to speech recognition er-rors, while both take as input exactly the same simi-larity matrices.
From a pragmatic viewpoint of sys-tem development, this graph-partitioning-based al-gorithm is simple to implement.
We believe immedi-ate further work such as combining the normalized-cut model with CYK-like dynamic programing totraverse the semantic trees in alignment could helpus further understand the problem, though suchmodels need much more memory in practice if notproperly optimized and have a higher time complex-ity.
Also, topic-segmentation (cohesion) models canbe naturally combined with the alignment model dis-cussed here.
We will study such problems as ourimmediate future work.216ReferencesD.
Beeferman, A. Berger, and J. Lafferty.
1999.
Statisti-cal models for text segmentation.
Machine Learning,34(1-3):177?210.S.
Branavan, Deshpande P., and Barzilay R. 2007.
Gen-erating a table-of-contents: A hierarchical discrimina-tive approach.
In Proc.
of Annual Meeting of the As-sociation for Computational Linguistics.Y.
Chen and W. J. Heng.
2003.
Automatic synchroniza-tion of speech transcript and slides in presentation.
InProc.
International Symposium on Circuits and Sys-tems.P.
Clarkson and R. Rosenfeld.
1997.
Statistical languagemodeling using the cmu-cambridge toolkit.
In Proc.
ofISCA European Conf.
on Speech Communication andTechnology, pages 2707?2710.Q.
Fan, K. Barnard, A. Amir, A. Efrat, and M. Lin.
2006.Matching slides to presentation videos using sift andscene background.
In Proc.
of ACM InternationalWorkshop on Multimedia Information Retrieval, pages239?248.R.
Ge and R. J. Mooney.
2005.
A statistical semanticparser that integrates syntax and semantics.
In Proc.of Computational Natural Language Learnine, pages9?16.J.
Glass, T. Hazen, S. Cyphers, I. Malioutov, D. Huynh,and R. Barzilay.
2007.
Recent progress in the mitspoken lecture processing project.
Proc.
of AnnualConference of the International Speech Communica-tion Association, pages 2553?2556.C.
Hori and S. Furui.
2003.
A new approach to auto-matic speech summarization.
IEEE Transactions onMultimedia, 5(3):368?378.B.
Hsu and J.
Glass.
2006.
Style and topic languagemodel adaptation using hmm-lda.
In Proc.
of Confer-ence on Empirical Methods in Natural Language Pro-cessing.E.
Leeuwis, M. Federico, and M. Cettolo.
2003.
Lan-guage modeling and transcription of the ted corpus lec-tures.
In Proc.
of IEEE International Conference onAcoustics, Speech and Signal Processing.T.
Liu, R. Hjelsvold, and J. R. Kender.
2002.
Analysisand enhancement of videos of electronic slide presen-tations.
In Proc.
IEEE International Conference onMultimedia and Expo.W.
Lu, H. T. Ng, W. S. Lee, and L. S. Zettlemoyer.
2008.A generative model for parsing natural language tomeaning representations.
In Proc.
of Empirical Meth-ods in Natural Language Processing, pages 783?792.I.
Malioutov and R. Barzilay.
2006.
Minimum cut modelfor spoken lecture segmentation.
In Proc.
of Interna-tional Conference on Computational Linguistics andAnnual Meeting of the Association for ComputationalLinguistics.I.
Malioutov, A.
Park, R. Barzilay, and J.
Glass.
2007.Making sense of sound: Unsupervised topic segmen-tation over acoustic input.
In Proc.
of Annual Meet-ing of the Association for Computational Linguistics,pages 504?511.D.
Marcu.
2000.
The theory and practice of discourseparsing and summarization.
The MIT Press.C.
Munteanu, R. Baecker, G. Penn, E. Toms, andE.
James.
2006.
Effect of speech recognition accu-racy rates on the usefulness and usability of webcastarchives.
In Proc.
of ACM Conference on Human Fac-tors in Computing Systems, pages 493?502.C.
Munteanu, G. Penn, and R. Baecker.
2007.
Web-based language modelling for automatic lecture tran-scription.
In Proc.
of Annual Conference of the Inter-national Speech Communication Association.A.
Park, T. Hazen, and J.
Glass.
2005.
Automatic pro-cessing of audio lectures for information retrieval.
InProc.
of IEEE Conf.
on Acoustics, Speech, and SignalProcessing, pages 497?500.B.
L. Pellom.
2001.
Sonic: The university of coloradocontinuous speech recognizer.
Tech.
Rep. TR-CSLR-2001-01, University of Colorado.L.
Pevsner and M. Hearst.
2002.
A critique and im-provement of an evaluation metric for text segmenta-tion.
Computational Linguistics, 28:19?36.R.
Ruddarraju.
2006.
Indexing Presentations Using Mul-tiple Media Streams.
Ph.D. thesis, Georgia Institute ofTechnology.
M.S.
Thesis.J.
Shi and J. Malik.
2000.
Normalized cuts and imagesegmentation.
IEEE Trans.
Pattern Anal.
Mach.
In-tell., 22.L.
Stark, S. Whittaker, and J. Hirschberg.
2000.
Find-ing information in audio: A new paradigm for audiobrowsing and retrieval.
In Proc.
of International Con-ference on Spoken Language Processing.F.
Wang, C. W. Ngo, and T. C. Pong.
2003.
Synchroniza-tion of lecture videos and electronic slides by videotext analysis.
In Proc.
of ACM International Confer-ence on Multimedia.S.
Xie and Y. Liu.
2010.
Using confusion networks forspeech summarization.
In Proc.
of International Con-ference on Human Language Technology and AnnualMeeting of North American Chapter of the Associationfor Computational Linguistics.K.
Zechner and A. Waibel.
2000.
Minimizing word er-ror rate in textual summaries of spoken language.
InProc.
of Applied Natural Language Processing Con-ference and Meeting of the North American Chapter ofthe Association for Computational Linguistics, pages186?193.217L.
S. Zettlemoyer and M. Collins.
2005.
Learning tomap sentences to logical form: Structured classifica-tion with probabilistic categorial grammars.
In Proc.of Uncertainty in Artificial Intelligence, pages 658?666.X.
Zhu, X.
He, C. Munteanu, and G. Penn.
2008.
Us-ing latent dirichlet alocation to incorporate domainknowledge for topic transition detection.
In Proc.
ofAnnual Conference of the International Speech Com-munication Association.X.
Zhu, C. Cherry, and G. Penn.
2010.
Imposing hierar-chical browsing structures onto spoken documents.
InProc.
of International Conference on ComputationalLinguistics.218
