Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1119?1130,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsDemographic Dialectal Variation in Social Media: A Case Study ofAfrican-American EnglishSu Lin Blodgett?
Lisa Green?
Brendan O?Connor?
?College of Information and Computer Sciences ?Department of LinguisticsUniversity of Massachusetts AmherstAbstractThough dialectal language is increasinglyabundant on social media, few resources existfor developing NLP tools to handle such lan-guage.
We conduct a case study of dialectallanguage in online conversational text by in-vestigating African-American English (AAE)on Twitter.
We propose a distantly supervisedmodel to identify AAE-like language from de-mographics associated with geo-located mes-sages, and we verify that this language fol-lows well-known AAE linguistic phenomena.In addition, we analyze the quality of existinglanguage identification and dependency pars-ing tools on AAE-like text, demonstrating thatthey perform poorly on such text compared totext associated with white speakers.
We alsoprovide an ensemble classifier for languageidentification which eliminates this disparityand release a new corpus of tweets containingAAE-like language.Data and software resources are available at:http://slanglab.cs.umass.edu/TwitterAAE1 IntroductionOwing to variation within a standard language, re-gional and social dialects exist within languagesacross the world.
These varieties or dialects differfrom the standard variety in syntax (sentence struc-ture), phonology (sound structure), and the inven-tory of words and phrases (lexicon).
Dialect com-munities often align with geographic and sociolog-ical factors, as language variation emerges withindistinct social networks, or is affirmed as a markerof social identity.As many of these dialects have traditionally ex-isted primarily in oral contexts, they have histor-ically been underrepresented in written sources.Consequently, NLP tools have been developed fromtext which aligns with mainstream languages.
Withthe rise of social media, however, dialectal languageis playing an increasingly prominent role in onlineconversational text, for which traditional NLP toolsmay be insufficient.
This impacts many applica-tions: for example, dialect speakers?
opinions maybe mischaracterized under social media sentimentanalysis or omitted altogether (Hovy and Spruit,2016).
Since this data is now available, we seek toanalyze current NLP challenges and extract dialectallanguage from online data.Specifically, we investigate dialectal language inpublicly available Twitter data, focusing on African-American English (AAE), a dialect of StandardAmerican English (SAE) spoken by millions of peo-ple across the United States.
AAE is a linguisticvariety with defined syntactic-semantic, phonolog-ical, and lexical features, which have been the sub-ject of a rich body of sociolinguistic literature.
Inaddition to the linguistic characterization, referenceto its speakers and their geographical location orspeech communities is important, especially in lightof the historical development of the dialect.
Not allAfrican-Americans speak AAE, and not all speakersof AAE are African-American; nevertheless, speak-ers of this variety have close ties with specific com-munities of African-Americans (Green, 2002).
Dueto its widespread use, established history in the soci-olinguistic literature, and demographic associations,AAE provides an ideal starting point for the devel-opment of a statistical model that uncovers dialectal1119language.
In fact, its presence in social media is at-tracting increasing interest for natural language pro-cessing (J?rgensen et al, 2016) and sociolinguistic(Stewart, 2014; Eisenstein, 2015; Jones, 2015) re-search.1 In this work we:?
Develop a method to identifydemographically-aligned text and lan-guage from geo-located messages (?2), basedon distant supervision of geographic censusdemographics through a statistical modelthat assumes a soft correlation betweendemographics and language.?
Validate our approach by verifying that textaligned with African-American demographicsfollows well-known phonological and syntac-tic properties of AAE, and document the pre-viously unattested ways in which such text di-verges from SAE (?3).?
Demonstrate racial disparity in the efficacyof NLP tools for language identification anddependency parsing?they perform poorly onthis text, compared to text associated withwhite speakers (?4, ?5).?
Improve language identification for U.S. on-line conversational text with a simple en-semble classifier using our demographically-based distant supervision method, aiming toeliminate racial disparity in accuracy rates(?4.2).?
Provide a corpus of 830,000 tweets alignedwith African-American demographics.2 Identifying AAE from DemographicsThe presence of AAE in social media and thegeneration of resources of AAE-like text for NLPtasks has attracted recent interest in sociolinguis-tic and natural language processing research; Jones(2015) shows that nonstandard AAE orthography onTwitter aligns with historical patterns of African-American migration in the U.S., while J?rgensenet al (2015) investigate to what extent it supportswell-known sociolinguistics hypotheses about AAE.1Including a recent linguistics work-shop: http://linguistlaura.blogspot.co.uk/2016/06/using-twitter-for-linguistic-research.htmlBoth, however, find AAE-like language on Twit-ter through keyword searches, which may not yieldbroad corpora reflective of general AAE use.
Morerecently, J?rgensen et al (2016) generated a largeunlabeled corpus of text from hip-hop lyrics, subti-tles from The Wire and The Boondocks, and tweetsfrom a region of the southeast U.S.
While this cor-pus does indeed capture a wide variety of language,we aim to discover AAE-like language by utiliz-ing finer-grained, neighborhood-level demographicsfrom across the country.Our approach to identifying AAE-like text isto first harvest a set of messages from Twitter,cross-referenced against U.S. Census demographics(?2.1), then to analyze words against demograph-ics with two alternative methods, a seedlist approach(?2.2) and a mixed-membership probabilistic model(?2.3).2.1 Twitter and Census dataIn order to create a corpus of demographically-associated dialectal language, we turn to Twitter,whose public messages contain large amounts of ca-sual conversation and dialectal speech (Eisenstein,2015).
It is well-established that Twitter can be usedto study both geographic dialectal varieties2 and mi-nority languages.3Some methods exist to associate messages withauthors?
races; one possibility is to use birth recordstatistics to identify African-American-associatednames, which has been used in (non-social media)social science studies (Sweeney, 2013; Bertrand andMullainathan, 2003).
However, metadata about au-thors is fairly limited on Twitter and most other so-cial media services, and many supplied names areobviously not real.Instead, we turn to geo-location and induce adistantly supervised mapping between authors andthe demographics of the neighborhoods they livein (O?Connor et al, 2010; Eisenstein et al, 2011b;Stewart, 2014).
We draw on a set of geo-locatedTwitter messages, most of which are sent on mo-bile phones, by authors in the U.S. in 2013.
(Theseare selected from a general archive of the ?Gar-denhose/Decahose?
sample stream of public Twit-2For example, of American English (Huang et al, 2015;Doyle, 2014).3For example, Lynn et al (2015) develop POS corpora andtaggers for Irish tweets; see also related work in ?4.1.1120ter messages (Morstatter et al, 2013)).
Geo-located users are a particular sample of the userbase(Pavalanathan and Eisenstein, 2015), but we expectit is reasonable to compare users of different raceswithin this group.We look up the U.S. Census blockgroup geo-graphic area that the message was sent in; block-groups are one of the smallest geographic areas de-fined by the Census, typically containing a popula-tion of 600?3000 people.
We use race and ethnic-ity information for each blockgroup from the Cen-sus?
2013 American Community Survey, definingfour covariates: percentages of the population thatare non-Hispanic whites, non-Hispanic blacks, His-panics (of any race), and Asian.4 Finally, for eachuser u, we average the demographic values of alltheir messages in our dataset into a length-four vec-tor pi(census)u .
Under strong assumptions, this couldbe interpreted as the probability of which race theuser is; we prefer to think of it as a rough proxy forlikely demographics of the author and the neighbor-hood they live in.Messages were filtered in order to focus on ca-sual conversational text; we exclude tweets whoseauthors had 1000 or more followers, or that (a) con-tained 3 or more hashtags, (b) contained the strings?http?, ?follow?, or ?mention?
(messages designedto generate followers), or (c) were retweeted (ei-ther containing the string ?rt?
or marked by Twitter?smetadata as re-tweeted).Our initial Gardenhose/Decahose stream archivehad 16 billion messages in 2013; 90 million weregeo-located with coordinates that matched a U.S.Census blockgroup.
59.2 million tweets from 2.8million users remained after pre-processing; eachuser is associated with a set of messages and aver-aged demographics pi(census)u .2.2 Direct Word-Demographic AnalysisGiven a set of messages and demographics associ-ated with their authors, a number of methods couldbe used to infer statistical associations between lan-guage and demographics.Direct word-demographic analysis methods usethe pi(census)u quantities to calculate statistics at theword level in a single pass.
An intuitive approachis to calculate the average demographics per word.4See appendix for additional details.For a token in the corpus indexed by t (across thewhole corpus), let u(t) be the author of the messagecontaining that token, andwt be the word token.
Theaverage demographics of word type w is:5pi(softcount)w ?
?t 1{wt = w}pi(census)u(t)?t 1{wt = w}We find that terms with the highest piw,AA values (de-noting high average African-American demograph-ics of their authors?
locations) are very non-standard,while Stewart (2014) and Eisenstein (2013) findlarge piw,AA associated with certain AAE linguisticfeatures.One way to use the piw,k values to construct a cor-pus is through a seedlist approach.
In early experi-ments, we constructed a corpus of 41,774 users (2.3million messages) by first selecting the n = 100highest-piw,AA terms occurring at least m = 3000times across the data set, then collecting all tweetsfrom frequent authors who have at least 10 tweetsand frequently use these terms, defined as the casewhen at least p = 20% of their messages containat least one of the seedlist terms.
Unfortunately, then,m, p thresholds are ad-hoc.2.3 Mixed-MembershipDemographic-Language ModelThe direct word-demographics analysis gives use-ful validation that the demographic information mayyield dialectal corpora, and the seedlist approachcan assemble a set of users with heavy dialectalusage.
However, the approach requires a numberof ad-hoc thresholds, cannot capture authors whoonly occasionally use demographically-aligned lan-guage, and cannot differentiate language use at themessage-level.
To address these concerns, we de-velop a mixed-membership model for demographicsand language use in social media.The model directly associates each of the four de-mographic variables with a topic; i.e.
a unigram lan-guage model over the vocabulary.6 The model as-sumes an author?s mixture over the topics tends to5 piw,k has the flavor of ?soft counts?
in multinomial EM.By changing the denominator to ?t pi(census)u(t) , it calculates aunigram language model that sums to one across the vocabulary.This hints at a more complete modeling approach (?2.3).6To build the vocabulary, we select all words used by at least20 different users, resulting in 191,873 unique words; otherwords are mapped to an out-of-vocabulary symbol.1121u?
z w???mt?
?m ?
Dir(?piu), ?
?
Dir(?/V )zt ?
?m, wz ?
?ztFigure 1: Mixed-membership model for users (u), messages(m) and tokens (t).
Observed variables have a double lined bor-der.be similar to their Census-associated demographicweights, and that every message has its own topicdistribution.
This allows for a single author to usedifferent types of language in different messages, ac-commodating multidialectal authors.
The message-level topic probabilities ?m are drawn from an asym-metric Dirichlet centered on pi(census)u , whose scalarconcentration parameter ?
controls whether authors?language is very similar to the demographic prior, orcan have some deviation.
A token t?s latent topic ztis drawn from ?m, and the word itself is drawn from?zt , the language model for the topic (Figure 1).Thus the model learns demographically-alignedlanguage models for each demographic category.The model is much more tightly constrained than atopic model?for example, if ?
?
?, ?
becomesfixed and the likelihood is concave as a function of?
?but it still has more joint learning than a directcalculation approach, since the inference of a mes-sages?
topic memberships ?m is affected not just bythe Census priors, but also by the language used.
Atweet written by an author in a highly AA neigh-borhood may be inferred to be non-AAE-aligned ifit uses non-AAE-associated terms; as inference pro-ceeeds, this information is used to learn sharper lan-guage models.We fit the model with collapsed Gibbs sampling(Griffiths and Steyvers, 2004) with repeated sampleupdates for each token t in the corpus,p(zt = k | w, z?t) ?Nwk + ?/VNk + ?Nmk + ?piukNm + ?where Nwk is the number of tokens where word woccurs under topic z = k, Nmk is the number oftokens in the current message with topic k, etc.
; allcounts exclude the current t position.
We observedconvergence of the log-likelihood within 100 to 200iterations, and ran for 300 total.7 We average to-gether count tables from the last 50 Gibbs samplesfor analysis of posterior topic memberships at theword, message, and user level; for example, the pos-terior probability a particular user u uses topic k,P (z = k | u), can be calculated as the fraction oftokens with topic k within messages authored by u.We considered ?
to be a fixed control parameter;setting it higher increases the correlations betweenP (z = k | u) and pi(census)u,k .
We view the selec-tion of ?
as an inherently difficult problem, sincethe correlation between race and AAE usage is al-ready complicated and imperfect at the author-level,and census demographics allow only for rough as-sociations.
We set ?
= 10 which yields posterioruser-level correlations of P (z = AA | u) againstpiu,AA to be approximately 0.8.This model has broadly similar goals as non-latent, log-linear generative models of text that con-dition on document-level covariates (Monroe et al,2008; Eisenstein et al, 2011a; Taddy, 2013).
Theformulation here has the advantage of fast inferencewith large vocabularies (since the partition functionnever has to be computed), and gives probabilisticadmixture semantics at arbitrary levels of the data.This model is also related to topic models wherethe selection of ?
conditions on covariates (Mimnoand McCallum, 2008; Ramage et al, 2011; Robertset al, 2013), though it is much simpler without fulllatent topic learning.In early experiments, we used only two classes(AA and not AA), and found Spanish terms beingincluded in the AA topic.
Thus we turned to fourrace categories in order to better draw out non-AAElanguage.
This removed Spanish terms from theAA topic; interestingly, they did not go to the His-panic topic, but instead to Asian, along with otherforeign languages.
In fact, the correlation betweenusers?
Census-derived proportions of Asian popu-lations, versus this posterior topic?s proportions, isonly 0.29, while the other three topics correlateto their respective Census priors in the range 0.83to 0.87.
This indicates the ?Asian?
topic actuallyfunctions as a background topic (at least in part).Better modeling of demographics and non-English7Our straightforward single core implementation (in Julia)spends 80 seconds for each iteration over 586 million tokens.1122language interactions is interesting potential futurework.By fitting the model to data, we can directly ana-lyze unigram probabilities within the model param-eters ?, but for other analyses, such as analyzinglarger syntactic constructions and testing NLP tools,we require an explicit corpus of messages.To generate a user-based AA-aligned corpus, wecollected all tweets from users whose posteriorprobability of using AA-associated terms under themodel was at least 80%, and generated a correspond-ing white-aligned corpus as well.
In order to removethe effects of non-English languages, and given un-certainty about what the model learned in the His-panic and Asian-aligned demographic topics, we fo-cused only on AA- and white-aligned language byimposing the additional constraint that each user?scombined posterior proportion of Hispanic or Asianlanguage was less than 5%.
Our two resulting usercorpora contain 830,000 and 7.3 million tweets, forwhich we are making their message IDs availablefor further research (in conformance with the Twit-ter API?s Terms of Service).
In the rest of the work,we refer to these as the AA- and white-aligned cor-pora, respectively.3 Linguistic ValidationBecause validation by manual inspection of our AA-aligned text is impractical, we turn to the well-studied phonological and syntactic phenomena thattraditionally distinguish AAE from SAE.
We val-idate our model by reproducing these phenomena,and document a variety of other ways in which ourAA-aligned text diverges from SAE.3.1 Lexical-Level VariationWe begin by examining how much AA- and white-aligned lexical items diverge from a standard dictio-nary.
We used SCOWL?s largest wordlist with level1 variants as our dictionary, totaling 627,685 words.8We calculated, for each word w in the model?svocabulary, the ratiork(w) =p(w|z = k)p(w|z 6= k)where the p(.|.)
probabilities are posterior infer-ences, derived from averaged Gibbs samples of the8http://wordlist.aspell.net/sufficient statistic count tables Nwk.We selected heavily AA- and white-aligned wordsas those where rAA(w) ?
2 and rwhite(w) ?
2,respectively.
We find that while 58.2% of heav-ily white-aligned words were not in our dictionary,fully 79.1% of heavily AA-aligned words were not.While a high number of out-of-dictionary lexicalitems is expected for Twitter data, this disparitysuggests that the AA-aligned lexicon diverges fromSAE more strongly than the white-aligned lexicon.3.2 Internet-Specific Orthographic VariationWe performed an ?open vocabulary?
unigram anal-ysis by ranking all words in the vocabulary byrAA(w) and browsed them and samples of their us-age.
Among the words with high rAA, we observe anumber of Internet-specific orthographic variations,which we separate into three types: abbreviations(e.g.
llh, kmsl), shortenings (e.g.
dwn, dnt), andspelling variations which do not correlate to theword?s pronunciation (e.g.
axx, bxtch).
These varia-tions do not reflect features attested in the literature;rather, they appear to be purely orthographic vari-ations highly specific to AAE-speaking communi-ties online.
They may highlight previously unknownlinguistic phenomena; for example, we observe thatthoe (SAE though) frequently appears in the role ofa discourse marker instead of its standard SAE us-age (e.g.
Girl Madison outfit THOE).
This new useof though as a discourse marker, which is difficultto observe using the SAE spelling amidst many in-stances of the SAE usage, is readily identifiable inexamples containing the thoe variant.
Thus, non-standard spellings provide valuable windows into avariety of linguistic phenomena.In the next section, we turn to variations which doappear to arise from known phonological processes.3.3 Phonological VariationMany phonological features are closely associatedwith AAE (Green, 2002).
While there is not a per-fect correlation between orthographic variations andpeople?s pronunciations, Eisenstein (2013) showsthat some genuine phonological phenomena, includ-ing a number of AAE features, are accurately re-flected in orthographic variation on social media.We therefore validate our model by verifying thatspellings reflecting known AAE phonological fea-tures align closely with the AA topic.1123AAE Ratio SAEsholl 1802.49 sureiont 930.98 I don?twea 870.45 wheretalmbout 809.79 talking aboutsumn 520.96 somethingTable 1: Of 31 phonological variant words, top five by ratiorAA(w).
SAE translations are shown for reference.We selected 31 variants of SAE words fromprevious studies of AAE phonology on Twitter(J?rgensen et al, 2015; Jones, 2015).
These varia-tions display a range of attested AAE phonologicalfeatures, such as derhotacization (e.g.
brotha), dele-tion of initial g and d (e.g.
iont), and realization ofvoiced th as d (e.g.
dey) (Rickford, 1999).Table 1 shows the top five of these words by theirrAA(w) ratio.
For 30 of the 31 words, r ?
1, andfor 13 words, r ?
100, suggesting that our modelstrongly identifies words displaying AAE phonolog-ical features with the AA topic.
The sole excep-tion is the word brotha, which appears to have beenadopted into general usage as its own lexical item.3.4 Syntactic VariationWe further validate our model by verifying that it re-produces well-known AAE syntactic constructions,investigating three well-attested AAE aspectual orpreverbal markers: habitual be, future gone, andcompletive done (Green, 2002).
Table 2 shows ex-amples of each construction.To search for the constructions, we tagged the cor-pora using the ARK Twitter POS tagger (Gimpelet al, 2011; Owoputi et al, 2013),9 which J?rgensenet al (2015) show has similar accuracy rates on bothAAE and non-AAE tweets, unlike other POS tag-gers.
We searched for each construction by search-ing for sequences of unigrams and POS tags char-acterizing the construction; e.g.
for habitual be wesearched for the sequences O-be-V and O-b-V. Non-standard spellings for the unigrams in the patternswere identified from the ranked analysis of ?3.2.We examined how a message?s likelihood of us-ing each construction varies with the message?s pos-terior probability of AA.
We split all messages intodeciles based on the messages?
posterior probabil-9Version 0.3.2: http://www.cs.cmu.edu/?ark/TweetNLP/Construction Example RatioO-be/b-V I be tripping bruh 11.94gone/gne/gon-V Then she gon besingle Af14.26done/dne-V I done laughed sohard that I?m weak8.68Table 2: AAE syntactic constructions and the ratios of theiroccurrences in the AA- vs. white-aligned corpora (?2.3).l lll llll ll0.0000.0020.004Posterior Probability of AAProportion ofTweetswithConstruction0.0000.0020.0040.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0l O?be?Vgone?Vdone?VFigure 2: Proportion of tweets containing AAE syntactic con-structions by messages?
posterior probability of AA.
On the x-axis, 0.1 refers to the decile [0, 0.1).ity of AA.
From each decile, we sampled 200,000messages and calculated the proportion of messagescontaining the three syntactic constructions.For all three constructions, we observed the clearpattern that as messages?
posterior probabilities ofAA increase, so does their likelihood of contain-ing the construction.
Interestingly, for all threeconstructions, frequency of usage peaks at approx-imately the [0.7, 0.8) decile.
One possible reasonfor the decline in higher deciles might be tendencyof high-AA messages to be shorter; while the meannumber of tokens per message across all deciles inour samples is 9.4, the means for the last two decilesare 8.6 and 7.1, respectively.Given the important linguistic differences be-tween our demographically-aligned subcorpora, wehypothesize that current NLP tools may behave dif-ferently.
We investigate this hypothesis in ?4 and ?5.4 Lang ID Tools on AAE4.1 Evaluation of Existing ClassifiersLanguage identification, the task of classifying themajor world language in which a message is writ-ten, is a crucial first step in almost any web or social1124AAE White-Alignedlangid.py 13.2% 7.6%Twitter-1 8.4% 5.9%Twitter-2 24.4% 17.6%Table 3: Proportion of tweets in AA- and white-aligned corporaclassified as non-English by different classifiers.
(?4.1)media text processing pipeline.
For example, in or-der to analyze the opinions of U.S. Twitter users, onemight throw away all non-English messages beforerunning an English sentiment analyzer.Hughes et al (2006) review language identifica-tion methods; social media language identificationis challenging since messages are short, and alsouse non-standard and multiple (often related) lan-guages (Baldwin et al, 2013).
Researchers havesought to model code-switching in social media lan-guage (Rosner and Farrugia, 2007; Solorio and Liu,2008; Maharjan et al, 2015; Zampieri et al, 2013;King and Abney, 2013), and recent workshops havefocused on code-switching (Solorio et al, 2014)and general language identification (Zubiaga et al,2014).
For Arabic dialect classification, work hasdeveloped corpora in both traditional and Roman-ized script (Cotterell et al, 2014; Malmasi et al,2015) and tools that use n-gram and morphologicalanalysis to identify code-switching between dialectsand with English (Elfardy et al, 2014).We take the perspective that since AAE is a di-alect of American English, it ought to be classi-fied as English for the task of major world languageidentification.
Lui and Baldwin (2012) developlangid.py, one of the most popular open source lan-guage identification tools, training it on over 97 lan-guages from texts including Wikipedia, and evalu-ating on both traditional corpora and Twitter mes-sages.
We hypothesize that if a language identifica-tion tool is trained on standard English data, it mayexhibit disparate performance on AA- versus white-aligned tweets.
Since language identifiers are typi-cally based on character n-gram features, they mayget confused by the types of lexical/orthographic di-vergences seen in ?3.
To evaluate this hypothesis,we compare the behavior of existing language iden-tifiers on our subcorpora.We test langid.py as well as the output of Twitter?sin-house identifier, whose predictions are includedin a tweet?s metadata (from 2013, the time of datacollection); the latter may give a language code ora missing value (unk or an empty/null value).
Werecord the proportion of non-English predictions bythese systems; Twitter-1 does not consider missingvalues to be a non-English prediction, and Twitter-2does.We noticed emojis had seemingly unintendedconsequences on langid.py?s classifications, so re-moved all emojis by characters from the relevantUnicode ranges.
We also removed @-mentions.User-level analysis We begin by comparing theclassifiers?
behavior on the AA- and white-alignedcorpora.
Of the AA-aligned tweets, 13.2% wereclassified by langid.py as non-English; in contrast,7.6% of white-aligned tweets were classified assuch.
We observed similar disparities for Twitter-1and Twitter-2, illustrated in Table 3.It turns out these ?non-English?
tweets are, for themost part, actually English.
We sampled and anno-tated 50 tweets from the tweets classified as non-English by each run.
Of these 300 tweets, only 3could be unambiguously identified as written in alanguage other than English.Message-level analysis We examine how a mes-sage?s likelihood of being classified as non-Englishvaries with its posterior probability of AA.
As in?3.4, we split all messages into deciles based onthe messages?
posterior probability of AA, and pre-dicted language identifications on 200,000 sampledmessages from each decile.For all three systems, the proportion of messagesclassified as non-English increases steadily as themessages?
posterior probabilities of AA increase.As before, we sampled and annotated from thetweets classified as non-English, sampling 50 tweetsfrom each decile for each of the three systems.
Ofthe 1500 sampled tweets, only 13 (?0.87%) couldbe unambiguously identified as being in a languageother than English.4.2 Adapting Language Identification for AAENatural language processing tools can be improvedto better support dialects; for example, J?rgensenet al (2016) use domain adaptation methods to im-prove POS tagging on AAE corpora.
In this sec-tion, we contribute a fix to language identification tocorrectly identify AAE and other social media mes-sages as English.1125l ll lll l lll0.000.100.200.30Posterior Probability of AA%ofTweetsPredicted to beNon?Englishl langid.pyTwitter?1Twitter?20.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.00.000.100.200.30Figure 3: Proportion of tweets classified as non-English bymessages?
posterior probability of AA.
On the x-axis, 0.1 refersto the decile [0, 0.1).4.2.1 Ensemble ClassifierWe observed that messages where our model in-fers a high probability of AAE, white-aligned,or ?Hispanic?-aligned language almost always arewritten in English; therefore we construct a simpleensemble classifier by combining it with langid.py.For a new message ~w, we predict itsdemographic-language proportions ??
via poste-rior inference with our trained model, given asymmetric ?
prior over demographic-topic pro-portions (see appendix for details).
The ensembleclassifier, given a message, is as follows:?
Calculate langid.py?s prediction y?.?
If y?
is English, accept it as English.?
If y?
is non-English, and at least one of themessage?s tokens are in demographic model?svocabulary: Infer ??
and return English only ifthe combined AA, Hispanic, and white poste-rior probabilities are at least 0.9.
Otherwisereturn the non-English y?
decision.Another way to view this method is that we are ef-fectively training a system on an extended Twitter-specific English language corpus softly labeled byour system?s posterior inference; in this respect, itis related to efforts to collect new language-specificTwitter corpora (Bergsma et al, 2012) or minoritylanguage data from the web (Ghani et al, 2001).4.2.2 EvaluationOur analysis from ?4.1 indicates that this methodwould correct erroneous false negatives for AAEMessage set langid.py EnsembleHigh AA 80.1% 99.5%High White 96.8% 99.9%General 88.0% 93.4%Table 4: Imputed recall of English messages in 2014 messages.For the General set these are an approximation; see text.messages in the training set for the model.
We fur-ther confirm this by testing the classifier on a sampleof 2.2 million geolocated tweets sent in the U.S. in2014, which are not in the training set.In addition to performance on the entire sample,we examine our classifier?s performance on mes-sages whose posterior probability of using AA- orwhite-associated terms was greater than 0.8 withinthe sample, which in this section we will call highAA and high white messages, respectively.
Our clas-sifier?s precision is high across the board, at 100%across manually annotated samples of 200 messagesfrom each sample.10 Since we are concerned aboutthe system?s overall recall, we impute recall (Ta-ble 4) by assuming that all high AA and high whitemessages are indeed English.
Recall for langid.pyalone is calculated by nN , where n is the numberof messages predicted to be English by langid.py,and N is the total number of messages in the set.
(This is the complement of Table 3, except evalu-ated on the test set.)
We estimate the ensemble?s re-call as n+mN , where m = (nflip)P (English | flip)is the expected number of correctly changed classifi-cations (from non-English to English) by the ensem-ble and the second term is the precision (estimatedas 1.0).
We observe the baseline system has consid-erable difference in recall between the groups whichis solved by the ensemble.We also apply the same calculation to the generalset of all 2.2 million messages; the baseline classifies88% as English.
This is a less accurate approxima-tion of recall since we have observed a substantialpresence of non-English messages.
The ensembleclassifies an additional 5.4% of the messages as En-glish; since these are all (or nearly all) correct, this10We annotated 600 messages as English, not English, or notapplicable, from 200 sampled each from general, high AA, andhigh white messages.
Ambiguous tweets which were too short(e.g.
?Gm?)
or contained only named entities (e.g.
?Tennessee?
)were excluded from the final calculations.
The resulting sam-ples have 197/197, 198/198, and 200/200 correct English clas-sifications, respectively.1126reflects at least a 5.4% gain to recall.5 Dependency Parser EvaluationGiven the lexical and syntactic variation of AAEcompared to SAE, we hypothesize that syntac-tic analysis tools also have differential accuracy.J?rgensen et al (2015) demonstrate this for part-of-speech tagging, finding that SAE-trained taggers haddisparate accuracy on AAE versus non-AAE tweets.We assess a publicly available syntactic depen-dency parser on our AAE and white-aligned corpora.Syntactic parsing for tweets has received some re-search attention; Foster et al (2011) create a cor-pus of constituent trees for English tweets, and Konget al (2014)?s Tweeboparser is trained on a Twittercorpus annotated with a customized unlabeled de-pendency formalism; since its data was uniformlysampled from tweets, we expect it may have low dis-parity between demographic groups.We focus on widely used syntactic representa-tions, testing the SyntaxNet neural network-baseddependency parser (Andor et al, 2016),11 which re-ports state-of-the-art results, including for web cor-pora.
We evaluate it against a new manual an-notation of 200 messages, 100 randomly sampledfrom each of the AA- and white-aligned corpora de-scribed in ?2.3.SyntaxNet outputs grammatical relations con-forming to the Stanford Dependencies (SD) system(de Marneffe and Manning, 2008), which we used toannotate messages using Brat,12 comparing to pre-dicted parses for reference.
Message order was ran-domized and demographic inferences were hiddenfrom the annotator.
To increase statistical power rel-ative to annotation effort, we developed a partial an-notation approach to only annotate edges for the rootword of the first major sentence in a message.
Gen-erally, we found that that SD worked well as a de-scriptive formalism for tweets?
syntax; we describehandling of AAE and Internet-specific non-standardissues in the appendix.
We evaluate labeled recallof the annotated edges for each message set:Parser AA Wh.
DifferenceSyntaxNet 64.0 (2.5) 80.4 (2.2) 16.3 (3.4)CoreNLP 50.0 (2.7) 71.0 (2.5) 21.0 (3.7)11Using the publicly available mcparseface model: https://github.com/tensorflow/models/tree/master/syntaxnet12http://brat.nlplab.org/Bootstrapped standard errors (from 10,000 messageresamplings) are in parentheses; differences are sta-tistically significant (p < 10?6 in both cases).The white-aligned accuracy rate of 80.4% isbroadly in line with previous work (compare to theparser?s unlabeled accuracy of 89% on English WebTreebank full annotations), but parse quality is muchworse on AAE tweets at 64.0%.
We test the StanfordCoreNLP neural network dependency parser (Chenand Manning, 2014) using the english SD modelthat outputs this formalism;13 its disparity is worse.Soni et al (2014) used a similar parser14 on Twittertext; our analysis suggests this approach may sufferfrom errors caused by the parser.6 Discussion and ConclusionWe have presented a distantly supervised probabilis-tic model that employs demographic correlations ofa dialect and its speaker communities to uncover di-alectal language on Twitter.
Our model can alsoclose the gap between NLP tools?
performance ondialectal and standard text.This represents a case study in dialect identifica-tion, characterization, and ultimately language tech-nology adaptation for the dialect.
In the case ofAAE, dialect identification is greatly assisted sinceAAE speakers are strongly associated with a demo-graphic group for which highly accurate governmen-tal records (the U.S. Census) exist, which we lever-age to help identify speaker communities.
The no-tion of non-standard dialectal language implies thatthe dialect is underrepresented or underrecognizedin some way, and thus should be inherently diffi-cult to collect data on; and of course, many otherlanguage communities and groups are not necessar-ily officially recognized.
An interesting directionfor future research would be to combine distant su-pervision with unsupervised linguistic models to au-tomatically uncover such underrecognized dialectallanguage.Acknowledgments: We thank Jacob Eisenstein, Taylor Jones,Anna J?rgensen, Dirk Hovy, and the anonymous reviewers fordiscussion and feedback.13pos,depparse options in version 2015-04-20, using tok-enizations output by SyntaxNet.14The older Stanford englishPCFG model with dependencytransform (via pers.
comm.
).1127ReferencesDaniel Andor, Chris Alberti, David Weiss, AliakseiSeveryn, Alessandro Presta, Kuzman Ganchev,Slav Petrov, and Michael Collins.
Globally nor-malized transition-based neural networks.
arXivpreprint arXiv:1603.06042, 2016.Timothy Baldwin, Paul Cook, Marco Lui, AndrewMacKinlay, and Li Wang.
How noisy social me-dia text, how diffrnt social media sources?
InInternational Joint Conference on Natural Lan-guage Processing, pages 356?364, 2013.Shane Bergsma, Paul McNamee, Mossaab Bag-douri, Clayton Fink, and Theresa Wilson.Language identification for creating language-specific Twitter collections.
In Proceedings of theSecond Workshop on Language in Social Media,pages 65?74.
Association for Computational Lin-guistics, 2012.Marianne Bertrand and Sendhil Mullainathan.
AreEmily and Greg more employable than Lakishaand Jamal?
A field experiment on labor marketdiscrimination.
Technical report, National Bureauof Economic Research, 2003.Danqi Chen and Christopher Manning.
A fastand accurate dependency parser using neural net-works.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Process-ing (EMNLP), pages 740?750, Doha, Qatar, Oc-tober 2014.
Association for Computational Lin-guistics.
URL http://www.aclweb.org/anthology/D14-1082.Ryan Cotterell, Adithya Renduchintala, NaomiSaphra, and Chris Callison-Burch.
An AlgerianArabic-French code-switched corpus.
In Work-shop on Free/Open-Source Arabic Corpora andCorpora Processing Tools Workshop Programme,page 34, 2014.M.
C. de Marneffe and C. D. Manning.
Stanfordtyped dependencies manual.
Technical report, lastrevised April 2015 edition, 2008.Gabriel Doyle.
Mapping dialectal variation byquerying social media.
In Proceedings of EACL,pages 98?106, 2014.Jacob Eisenstein.
Phonological factors in social me-dia writing.
In Proc.
of the Workshop on Lan-guage Analysis in Social Media, pages 11?19,2013.Jacob Eisenstein.
Identifying regional dialects in on-line social media.
In C. Boberg, J. Nerbonne, andD.
Watt, editors, Handbook of Dialectology.
Wi-ley, 2015.Jacob Eisenstein, Amr Ahmed, and Eric P. Xing.Sparse additive generative models of text.
In Pro-ceedings of ICML, pages 1041?1048, 2011a.Jacob Eisenstein, Noah A. Smith, and Eric P.Xing.
Discovering sociolinguistic associationswith structured sparsity.
In Proceedings ofthe 49th Annual Meeting of the Association forComputational Linguistics: Human LanguageTechnologies-Volume 1, pages 1365?1374.
Asso-ciation for Computational Linguistics, 2011b.Heba Elfardy, Mohamed Al-Badrashiny, and MonaDiab.
Aida: Identifying code switching in infor-mal Arabic text.
Proceedings of EMNLP 2014,page 94, 2014.Jennifer Foster, Ozlem Cetinoglu, Joachim Wag-ner, Joseph Le Roux, Stephen Hogan, JoakimNivre, Deirdre Hogan, and Josef van Genabith.#hardtoparse: POS tagging and parsing the Twit-terverse.
In Proc.
of AAAI-11 Workshop onAnalysing Microtext, 2011.Rayid Ghani, Rosie Jones, and Dunja Mladenic?.Mining the web to create minority language cor-pora.
In Proceedings of the Tenth InternationalConference on Information and Knowledge Man-agement, pages 279?286.
ACM, 2001.Kevin Gimpel, Nathan Schneider, BrendanO?Connor, Dipanjan Das, Daniel Mills, JacobEisenstein, Michael Heilman, Dani Yogatama,Jeff Flanigan, and Noah A. Smith.
Part-of-speechtagging for Twitter: Annotation, features, andexperiments.
In Proceedings of the 49th AnnualMeeting of the Association for ComputationalLinguistics: Human Language Technologies,pages 42?47.
Association for ComputationalLinguistics, 2011.Lisa J.
Green.
African American English: A Lin-guistic Introduction.
Cambridge University Press,2002.T.L.
Griffiths and M. Steyvers.
Finding scientifictopics.
Proceedings of the National Academy of1128Sciences of the United States of America, 101(Suppl 1):5228, 2004.Dirk Hovy and L. Shannon Spruit.
The social im-pact of natural language processing.
In Proceed-ings of the 54th Annual Meeting of the Associ-ation for Computational Linguistics (Volume 2:Short Papers), pages 591?598.
Association forComputational Linguistics, 2016. doi: 10.18653/v1/P16-2096.
URL http://aclweb.org/anthology/P16-2096.Yuan Huang, Diansheng Guo, Alice Kasakoff, andJack Grieve.
Understanding US regional linguis-tic variation with Twitter data analysis.
Comput-ers, Environment and Urban Systems, 2015.Baden Hughes, Timothy Baldwin, Steven Bird,Jeremy Nicholson, and Andrew MacKinlay.
Re-considering language identification for writtenlanguage resources.
In Proceedings of the FifthInternational Conference on Language Resourcesand Evaluation (LREC?06).
European LanguageResources Association (ELRA), 2006.
URL http://aclweb.org/anthology/L06-1274.Taylor Jones.
Toward a description of AfricanAmerican Vernacular English dialect regions us-ing ?Black Twitter?.
American Speech, 90(4):403?440, 2015.Anna J?rgensen, Dirk Hovy, and Anders S?gaard.Learning a POS tagger for AAVE-like language.In Proceedings of NAACL.
Association for Com-putational Linguistics, 2016.Anna Katrine J?rgensen, Dirk Hovy, and AndersS?gaard.
Challenges of studying and processingdialects in social media.
In Proceedings of theWorkshop on Noisy User-generated Text, pages 9?18, 2015.Ben King and Steven P Abney.
Labeling the lan-guages of words in mixed-language documentsusing weakly supervised methods.
In Proceed-ings of HLT-NAACL, pages 1110?1119, 2013.Lingpeng Kong, Nathan Schneider, SwabhaSwayamdipta, Archna Bhatia, Chris Dyer,and Noah A. Smith.
A dependency parserfor tweets.
In Proceedings of the 2014Conference on Empirical Methods in Natu-ral Language Processing (EMNLP), pages1001?1012, Doha, Qatar, October 2014.
As-sociation for Computational Linguistics.
URLhttp://www.aclweb.org/anthology/D14-1108.M.
Lui and T. Baldwin.
langid.
py: Anoff-the-shelf language identification tool.
InProceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics(ACL 2012), Demo Session, Jeju, Republicof Korea, 2012.
URL http://www.aclweb.org/anthology-new/P/P12/P12-3005.pdf.Teresa Lynn, Kevin Scannell, and Eimear Maguire.Minority language Twitter: Part-of-speech tag-ging and analysis of Irish tweets.
Proceedings ofACL-IJCNLP 2015, page 1, 2015.Suraj Maharjan, Elizabeth Blair, Steven Bethard,and Thamar Solorio.
Developing language-taggedcorpora for code-switching tweets.
In The 9th Lin-guistic Annotation Workshop held in conjuncionwith NAACL 2015, page 72, 2015.Shervin Malmasi, Eshrag Refaee, and Mark Dras.Arabic dialect identification using a parallel mul-tidialectal corpus.
In International Conferenceof the Pacific Association for Computational Lin-guistics, pages 35?53.
Springer, 2015.David Mimno and Andrew McCallum.
Topicmodels conditioned on arbitrary features withDirichlet-Multinomial regression.
In Uncertaintyin Artificial Intelligence, pages 411?418, 2008.B.
L. Monroe, M. P. Colaresi, and K. M. Quinn.Fightin?
Words: Lexical feature selection andevaluation for identifying the content of politicalconflict.
Political Analysis, 16(4):372, 2008.Fred Morstatter, Jrgen Pfeffer, Huan Liu, andKathleen Carley.
Is the sample good enough?Comparing data from twitter?s streaming apiwith Twitter?s Firehose.
In International AAAIConference on Weblogs and Social Media,2013.
URL http://www.aaai.org/ocs/index.php/ICWSM/ICWSM13/paper/view/6071.Brendan O?Connor, Jacob Eisenstein, Eric P. Xing,and Noah A. Smith.
A mixture model of demo-graphic lexical variation.
In NIPS Workshop onMachine Learning for Social Computing, 2010.Olutobi Owoputi, Brendan O?Connor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah A.Smith.
Improved part-of-speech tagging for on-1129line conversational text with word clusters.
InProceedings of NAACL, 2013.Umashanthi Pavalanathan and Jacob Eisenstein.Confounds and consequences in geotagged Twit-ter data.
In Proceedings of Empirical Methodsfor Natural Language Processing (EMNLP), Lis-bon, September 2015.
URL http://www.aclweb.org/anthology/D/D15/D15-1256.pdf.Daniel Ramage, Christopher D. Manning, and Su-san Dumais.
Partially labeled topic models forinterpretable text mining.
In Proceedings of the17th ACM SIGKDD International Conference onKnowledge Discovery and Data Mining, pages457?465, 2011.John Russell Rickford.
African American Vernacu-lar English: Features, Evolution, Educational Im-plications.
Wiley-Blackwell, 1999.Margaret E Roberts, Brandon M Stewart, DustinTingley, and Edoardo M Airoldi.
The structuraltopic model and applied social science.
In Ad-vances in Neural Information Processing SystemsWorkshop on Topic Models: Computation, Appli-cation, and Evaluation, 2013.Mike Rosner and Paulseph-John Farrugia.
A tag-ging algorithm for mixed language identificationin a noisy domain.
In Eighth Annual Conferenceof the International Speech Communication Asso-ciation, 2007.Thamar Solorio and Yang Liu.
Learning to pre-dict code-switching points.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing, pages 973?981.
Associa-tion for Computational Linguistics, 2008.Thamar Solorio, Elizabeth Blair, Suraj Mahar-jan, Steven Bethard, Mona Diab, MahmoudGhoneim, Abdelati Hawwari, Fahad AlGhamdi,Julia Hirschberg, Alison Chang, and PascaleFung.
Overview for the first shared task on lan-guage identification in code-switched data.
InProceedings of the First Workshop on Compu-tational Approaches to Code Switching, pages62?72, Doha, Qatar, October 2014.
Associationfor Computational Linguistics.
URL http://www.aclweb.org/anthology/W14-3907.Sandeep Soni, Tanushree Mitra, Eric Gilbert, andJacob Eisenstein.
Modeling factuality judgmentsin social media text.
In Proceedings of the 52ndAnnual Meeting of the Association for Compu-tational Linguistics (Volume 2: Short Papers),pages 415?420, Baltimore, Maryland, June 2014.Association for Computational Linguistics.
URLhttp://www.aclweb.org/anthology/P14-2068.Ian Stewart.
Now we stronger than ever: African-American syntax in Twitter.
Proceedings ofEACL, page 31, 2014.Latanya Sweeney.
Discrimination in online ad de-livery.
ACM Queue, 11(3):10, 2013.Matt Taddy.
Multinomial inverse regression for textanalysis.
Journal of the American Statistical As-sociation, 108(503):755?770, 2013.Marcos Zampieri, Binyam Gebrekidan Gebre, andSascha Diwersy.
N-gram language models andpos distribution for the identification of Spanishvarieties.
Proceedings of TALN2013, pages 580?587, 2013.Arkaitz Zubiaga, Inaki San Vincente, PabloGamallo, Jose Ramom Pichel, Inaki Algeria,Nora Aranberri, Aitzol Ezeiza, and Victor Fresno.Overview of TweetLID: Tweet language identi-fication at SEPLN 2014.
In Proceedings of theTweet Language Identification Workshop, Girona,Spain, September 2014.
Spanish Society for Nat-ural Language Processing.
URL http://ceur-ws.org/Vol-1228/.1130
