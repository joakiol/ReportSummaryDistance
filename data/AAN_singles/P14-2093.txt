Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 569?573,Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational LinguisticsEffective Selection of Translation Model Training DataLe Liu  Yu Hong*  Hao Liu  Xing Wang  Jianmin YaoSchool of Computer Science & Technology, Soochow University, China{20124227052, hongy, 20134227035, 20114227047, jyao}@suda.edu.cnAbstractData selection has been demonstrated tobe an effective approach to addressingthe lack of high-quality bitext for statisti-cal machine translation in the domain ofinterest.
Most current data selectionmethods solely use language modelstrained on a small scale in-domain data toselect domain-relevant sentence pairsfrom general-domain parallel corpus.
Bycontrast, we argue that the relevance be-tween a sentence pair and target domaincan be better evaluated by the combina-tion of language model and translationmodel.
In this paper, we study and exper-iment with novel methods that applytranslation models into domain-relevantdata selection.
The results show that ourmethods outperform previous methods.When the selected sentence pairs areevaluated on an end-to-end MT task, ourmethods can increase the translation per-formance by 3 BLEU points.
*1 IntroductionStatistical machine translation depends heavilyon large scale parallel corpora.
The corpora arenecessary priori knowledge for training effectivetranslation model.
However, domain-specificmachine translation has few parallel corpora fortranslation model training in the domain of inter-est.
For this, an effective approach is to automat-ically select and expand domain-specific sen-tence pairs from large scale general-domain par-allel corpus.
The approach is named Data Selec-tion.
Current data selection methods mostly uselanguage models trained on small scale in-domain data to measure domain relevance andselect domain-relevant parallel sentence pairs toexpand training corpora.
Related work in litera-ture has proven that the expanded corpora cansubstantially improve the performance of ma-* Corresponding authorchine translation (Duh et al, 2010; Haddow andKoehn, 2012).However, the methods are still far from satis-factory for real application for the following rea-sons:?
There isn?t ready-made domain-specificparallel bitext.
So it?s necessary for data se-lection to have significant capability in min-ing parallel bitext in those assorted free texts.But the existing methods seldom ensureparallelism in the target domain while se-lecting domain-relevant bitext.?
Available domain-relevant bitext needs keephigh domain-relevance at both the sides ofsource and target language.
But it?s difficultfor current method to maintain two-sideddomain-relevance when we aim at enhanc-ing parallelism of bitext.In a word, current data selection methods can?twell maintain both parallelism and domain-relevance of bitext.
To overcome the problem,we first propose the method combining transla-tion model with language model in data selection.The language model measures the domain-specific generation probability of sentences, be-ing used to select domain-relevant sentences atboth sides of source and target language.
Mean-while, the translation model measures the trans-lation probability of sentence pair, being used toverify the parallelism of the selected domain-relevant bitext.2 Related WorkThe existing data selection methods are mostlybased on language model.
Yasuda et al (2008)and Foster et al (2010) ranked the sentence pairsin the general-domain corpus according to theperplexity scores of sentences, which are com-puted with respect to in-domain language models.Axelrod et al (2011) improved the perplexity-based approach and proposed bilingual cross-entropy difference as a ranking function with in-and general- domain language models.
Duh et al(2013) employed the method of (Axelrod et al,5692011) and further explored neural language mod-el for data selection rather than the conventionaln-gram language model.
Although previousworks in data selection (Duh et al, 2013; Koehnand Haddow, 2012; Axelrod et al, 2011; Fosteret al, 2010; Yasuda et al, 2008) have gainedgood performance, the methods which onlyadopt language models to score the sentencepairs are sub-optimal.
The reason is that a sen-tence pair contains a source language sentenceand a target language sentence, while the existingmethods are incapable of evaluating the mutualtranslation probability of sentence pair in the tar-get domain.
Thus, we propose novel methodswhich are based on translation model and lan-guage model for data selection.3 Training Data Selection MethodsWe present three data selection methods forranking and selecting domain-relevant sentencepairs from general-domain corpus, with an eyetowards improving domain-specific translationmodel performance.
These methods are based onlanguage model and translation model, which aretrained on small in-domain parallel data.3.1 Data Selection with Translation ModelTranslation model is a key component in statisti-cal machine translation.
It is commonly used totranslate the source language sentence into thetarget language sentence.
However, in this paper,we adopt the translation model to evaluate thetranslation probability of sentence pair and de-velop a simple but effective variant of translationmodel to rank the sentence pairs in the general-domain corpus.
The formulations are detailed asbelow:(   )(    )?
?
(     )(1)?
(   )(2)Where  (   ) is the translation model, which isIBM Model 1 in this paper, it represents thetranslation probability of target language sen-tence   conditioned on source language sentence.
and    are the number of words in sentenceand  respectively.
(     )  is the translationprobability of word    conditioned on word   andis estimated from the small in-domain paralleldata.
The parameter   is a constant and is as-signed with the value of 1.0.   is the length-normalized IBM Model 1, which is used to scoregeneral-domain sentence pairs.
The sentence pairwith higher score is more likely to be generatedby in-domain translation model, thus, it is morerelevant to the in-domain corpus and will be re-mained to expand the training data.3.2 Data Selection by Combining Transla-tion and Language modelAs described in section 1, the existing data selec-tion methods which only adopt language modelto score sentence pairs are unable to measure themutual translation probability of sentence pairs.To solve the problem, we develop the seconddata selection method, which is based on thecombination of translation model and languagemodel.
Our method and ranking function areformulated as follows:(   )   (   )   ( )        (3)?
(   )?
( )(4)Where  (   ) is a joint probability of sentenceand   according to the translation model  (   )and language model  ( ), whose parameters areestimated from the small in-domain text.
is theimproved ranking function and used to score thesentence pairs with the length-normalized trans-lation model  (   )and language model  ( ).The sentence pair with higher score is more simi-lar to in-domain corpus, and will be picked out.3.3 Data Selection by BidirectionallyCombining Translation and LanguageModelsAs presented in subsection 3.2, the method com-bines translation model and language model torank the sentence pairs in the general-domaincorpus.
However, it does not evaluate the inversetranslation probability of sentence pair and theprobability of target language sentence.
Thus, wetake bidirectional scores into account and simplysum the scores in both directions.?
(   )?
( )?
(   )?
( )(5)Again, the sentence pairs with higher scores arepresumed to be better and will be selected to in-corporate into the domain-specific training data.This approach makes full use of two translationmodels and two language models for sentencepairs ranking.5704 Experiments4.1 CorporaWe conduct our experiments on the Spoken Lan-guage Translation English-to-Chinese task.
Twocorpora are needed for the data selection.
The in-domain data is collected from CWMT09, whichconsists of spoken dialogues in a travel setting,containing approximately 50,000 parallel sen-tence pairs in English and Chinese.
Our general-domain corpus mined from the Internet contains16 million sentence pairs.
Both the in- and gen-eral- domain corpora are identically tokenized (inEnglish) and segmented (in Chinese)1.
The de-tails of corpora are listed in Table 1.
Additionally,we evaluate our work on the 2004 test set of?863?
Spoken Language Translation task (?863?SLT), which consists of 400 English sentenceswith 4 Chinese reference translations for each.Meanwhile, the 2005 test set of ?863?
SLT task,which contains 456 English sentences with 4 ref-erences each, is used as the development set totune our systems.Bilingual Cor-pus#sentence #tokenEng Chn Eng ChnIn-domain 50K 50K 360K 310KGeneral-domain 16M 16M 3933M 3602MTable 1.
Data statics4.2 System settingsWe use the NiuTrans 2  toolkit which adoptsGIZA++ (Och and Ney, 2003) and MERT (Och,2003) to train and tune the machine translationsystem.
As NiuTrans integrates the mainstreamtranslation engine, we select hierarchical phrase-based engine (Chiang, 2007) to extract the trans-lation rules and carry out our experiments.Moreover, in the decoding process, we use theNiuTrans decoder to produce the best outputs,and score them with the widely used NIST mt-eval131a3  tool.
This tool scores the outputs inseveral criterions, while the case-insensitiveBLEU-4 (Papineni et al, 2002) is used as theevaluation for the machine translation system.4.3 Translation and Language modelsOur work relies on the use of in-domain lan-guage models and translation models to rank thesentence pairs from the general-domain bilingualtraining set.
Here, we employ ngram language1http://www.nlplab.com/NiuPlan/NiuTrans.YourData.ch.html2http://www.nlplab.com/NiuPlan/NiuTrans.ch.html#download3 http://ww.itl.nist.gov/iad/mig/toolsmodel and IBM Model 1 for data selection.
Thus,we use the SRI Language Modeling Toolkit(Stolcke, 2002) to train the in-domain 4-gramlanguage model with interpolated modifiedKneser-Ney discounting (Chen and Goodman,1998).
The language model is only used to scorethe general-domain sentences.
Meanwhile, weuse the language model training scripts integrat-ed in the NiuTrans toolkit to train another 4-gramlanguage model, which is used in MT tuning anddecoding.
Additionally, we adopt GIZA++ to getthe word alignment of in-domain parallel dataand form the word translation probability table.This table will be used to compute the translationprobability of general-domain sentence pairs.4.4 Baseline SystemsAs described above, by using the NiuTranstoolkit, we have built two baseline systems tofulfill ?863?
SLT task in our experiments.
TheIn-domain baseline trained on spoken languagecorpus has 1.05 million rules in its hierarchical-phrase table.
While, the General-domain baselinetrained on 16 million sentence pairs has a hierar-chical phrase table containing 1.7 billion transla-tion rules.
These two baseline systems areequipped with the same language model which istrained on large-scale monolingual target lan-guage corpus.
The BLEU scores of the In-domain and General-domain baseline system arelisted in Table 2.CorpusHierarchicalphraseDev TestIn-domain 1.05M 15.01 21.99General-domain 1747M 27.72 34.62Table 2.
Translation performances of In-domain andGeneral-domain baseline systemsThe results show that General-domain systemtrained on a larger amount of bilingual resourcesoutperforms the system trained on the in-domaincorpus by over 12 BLEU points.
The reason isthat large scale parallel corpus maintains morebilingual knowledge and language phenomenon,while small in-domain corpus encounters datasparse problem, which degrades the translationperformance.
However, the performance of Gen-eral-domain baseline can be improved further.We use our three methods to refine the general-domain corpus and improve the translation per-formance in the domain of interest.
Thus, webuild several contrasting systems trained on re-fined training data selected by the following dif-ferent methods.571?
Ngram: Data selection by 4-gram LMs withKneser-Ney smoothing.
(Axelrod et al,2011)?
Neural net: Data selection by RecurrentNeural LM, with the RNNLM Tookit.
(Duhet al, 2013)?
Translation Model (TM): Data selectionwith translation model: IBM Model 1.?
Translation model and Language Model(TM+LM): Data selection by combining 4-gram LMs with Kneser-Ney smoothing andIBM model 1(equal weight).?
Bidirectional TM+LM: Data selection bybidirectionally combining translation andlanguage models (equal weight).4.5 Results of Training Data SelectionWe adopt five methods for extracting domain-relevant parallel data from general-domain cor-pus.
Using the scoring methods, we rank the sen-tence pairs of the general-domain corpus andselect only the top N = {50k, 100k, 200k, 400k,600k, 800k, 1000k} sentence pairs as refinedtraining data.
New MT systems are then trainedon these small refined training data.
Figure 1shows the performances of systems trained onselected corpora from the general-domain corpus.The horizontal coordinate represents the numberof selected sentence pairs and vertical coordinateis the BLEU scores of MT systems.Figure 1.
Results of the systems trained on only a sub-set of the general-domain parallel corpus.From Figure 1, we conclude that these five da-ta selection methods are effective for domain-specific translation.
When top 600k sentencepairs are picked out from general-domain corpusto train machine translation systems, the systemsperform higher than the General-domain baselinetrained on 16 million parallel data.
The resultsindicate that more training data for translationmodel is not always better.
When the domain-specific bilingual resources are deficient, thedomain-relevant sentence pairs will play an im-portant role in improving the translation perfor-mance.Additionally, it turns out that our methods(TM, TM+LM and Bidirectional TM+LM) areindeed more effective in selecting domain-relevant sentence pairs.
In the end-to-end SMTevaluation, TM selects top 600k sentence pairsof general-domain corpus, but increases thetranslation performance by 2.7 BLEU points.Meanwhile, the TM+LM and BidirectionalTM+LM have gained 3.66 and 3.56 BLEU pointimprovements compared against the general-domain baseline system.
Compared with themainstream methods (Ngram and Neural net),our methods increase translation performance bynearly 3 BLEU points, when the top 600k sen-tence pairs are picked out.
Although, in the fig-ure 1, our three methods are not performing bet-ter than the existing methods in all cases, theiroverall performances are relatively higher.
Wetherefore believe that combining in-domaintranslation model and language model to scorethe sentence pairs is well-suited for domain-relevant sentence pair selection.
Furthermore, weobserve that the overall performance of ourmethods is gradually improved.
This is becauseour methods are combining more statistical char-acteristics of in-domain data in ranking and se-lecting sentence pairs.
The results have proventhe effectiveness of our methods again.5 ConclusionWe present three novel methods for translationmodel training data selection, which are based onthe translation model and language model.
Com-pared with the methods which only employ lan-guage model for data selection, we observe thatour methods are able to select high-quality do-main-relevant sentence pairs and improve thetranslation performance by nearly 3 BLEU points.In addition, our methods make full use of thelimited in-domain data and are easily implement-ed.
In the future, we are interested in applying20.0022.0024.0026.0028.0030.0032.0034.0036.0038.0040.000 200 400 600 800 1000Axelord et al(2011) Duh et al(2013)TM TM+LMBidirectional TM+LM572our methods into domain adaptation task of sta-tistical machine translation in model level.AcknowledgmentsThis research work has been sponsored by twoNSFC grants, No.61373097 and No.61272259,and one National Science Foundation of Suzhou(Grants No.
SH201212).ReferenceAmittai Axelrod, Xiaodong He, and Jianfeng Gao.2011.
Domain adaptation via pseudo in-domain da-ta selection.
In Proceedings of the 2011 Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 355?362, Edinburgh, Scotland,UK, July.
Association for Computational Linguis-tics.Peter F.Brown, Vincent J. Della Pietra, Stephen A.Della Pietra and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Computational linguistics,1993, 19(2): 263-311.Stanley Chen and Joshua Goodman.
1998.
An Empir-ical Study of Smoothing Techniques for LanguageModeling.
Technical Report 10-98, Computer Sci-ence Group, Harvard University.Moore Robert C, Lewis William.
2010.
Intelligentselection of language model training data.
In Pro-ceedings of the ACL 2010 Conference Short Pa-pers.
Association for Computational Linguistics,2010: 220-224.Chiang David.
A hierarchical phrase-based model forstatistical machine translation.
2005.
In Proceed-ings of the 43rd Annual Meeting on Association forComputational Linguistics, pages: 263-270.
Asso-ciation for Computational Linguistics.Kevin Duh, Graham Neubig, Katsuhito Sudoh andHajime Tsukada.
Adaptation Data Selection usingNeural Language Models: Experiments in MachineTranslation.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics, pages 678-683, Sofia, Bulgaria, August 4-9 2013.Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada.2010.Analysis of translation model adaptation forstatistical machine translation.
In Proceedings ofthe International Workshop on Spoken LanguageTranslation (IWSLT) - Technical Papers Track.George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative Instance Weighting for DomainAdaptation in Statistical Machine Translation.
Em-pirical Methods in Natural Language Processing.Barry Haddow and Philipp Koehn.
2012.
Analysingthe effect of out-of-domain data on smt systems.
InProceedings of the Seventh Workshop on StatisticalMachine Translation, pages 422?432, Montreal,Canada, June.
Association for Computational Lin-guistics.Och, Franz Josef, and Hermann Ney.
A systematiccomparison of various statistical alignment models.Computational linguistics 29.1 (2003): 19-51.Och, Franz Josef.
Minimum error rate training in sta-tistical machine translation.
Proceedings of the 41stAnnual Meeting on Association for ComputationalLinguistics-Volume 1.
Association for Computa-tional Linguistics, 2003.Philipp Koehn and Barry Haddow.
2012.
Towardseffective use of training data in statistical machinetranslation.
In WMT.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: A method for auto-matic evaluation of machine translation.
In ACL.Andreas Stolcke.
2002.
SRILM - An extensible lan-guage modeling toolkit.
Spoken Language Pro-cessing.Tong Xiao, Jingbo Zhu, Hao Zhang and Qiang Li.NiuTrans: an open source toolkit for phrase-basedand syntax-based machine translation.
In Proceed-ings of the ACL 2012 System Demonstrations.
As-sociation for Computational Linguistics, 2012: 19-24.Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto,and Eiichiro Sumita.
2008.
Method of selectingtraining data to build a compact and efficient trans-lation model.
International Joint Conference onNatural Language Processing.573
