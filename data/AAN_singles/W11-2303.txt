Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 22?31,Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational LinguisticsTowards technology-assisted co-construction with communication partnersBrian Roark?, Andrew Fowler?, Richard Sproat?, Christopher Gibbons?, Melanie Fried-Oken?
?Center for Spoken Language Understanding ?Child Development & Rehabilitation CenterOregon Health & Science University{roark,fowlera,sproatr}@cslu.ogi.edu {gibbons,mfo}@ohsu.eduAbstractIn this paper, we examine the idea oftechnology-assisted co-construction, wherethe communication partner of an AAC usercan make guesses about the intended mes-sages, which are included in the user?s wordcompletion/prediction interface.
We run somehuman trials to simulate this new interfaceconcept, with subjects predicting words as theuser?s intended message is being generated inreal time with specified typing speeds.
Re-sults indicate that people can provide substan-tial keystroke savings by providing word com-pletion or prediction, but that the savings arenot as high as n-gram language models.
In-terestingly, the language model and humanpredictions are complementary in certain keyways ?
humans doing a better job in somecircumstances on contextually salient nouns.We discuss implications of the enhanced co-construction interface for real-time messagegeneration in AAC direct selection devices.1 IntroductionIndividuals who cannot use standard keyboards fortext entry because of physical disabilities have anumber of alternative text entry methods that per-mit typing.
Referred to as keyboard emulationwithin augmentative and alternative communication(AAC), there are many different access options forthe user, ranging from direct selection of letters withany anatomical pointer (e.g., head, eyes) to use of abinary switch ?
triggered by button-press, eye-blinkor even through event related potentials (ERP) suchas the P300 detected in EEG signals.
These optionsallow the individual to indirectly select a symbolbased on some process for scanning through alter-natives (Lesher et al, 1998).
Typing speed is a chal-lenge, yet is critically important for usability, andas a result there is a significant line of research intothe utility of statistical language models for improv-ing typing speed (McCoy et al, 2007; Koester andLevine, 1996; Koester and Levine, 1997; Koesterand Levine, 1998).
Methods of word, symbol,phrase and message prediction via statistical lan-guage models are widespread in both direct selec-tion and scanning devices (Darragh et al, 1990; Liand Hirst, 2005; Trost et al, 2005; Trnka et al,2006; Trnka et al, 2007; Wandmacher and Antoine,2007; Todman et al, 2008).
To the extent that thepredictions are accurate, the number of keystrokesrequired to type a message can be dramatically re-duced, greatly speeding typing.AAC devices for spontaneous and novel text gen-eration are intended to empower the user of the sys-tem, to place them in control of their own com-munication, and reduce their reliance on others formessage formulation.
As a result, all such devices(much like standard personal computers) are builtfor a single user, with a single keyboard and/or alter-native input interface, which is driven by the user ofthe system.
The unilateral nature of these high tech-nology solutions to AAC stands in contrast to com-mon low technology solutions, which rely on collab-oration between the individual formulating the mes-sage and their communication partner.
Many adultswith acquired neurological conditions rely on com-munication partners for co-construction of messages(Beukelman et al, 2007).One key reason why low-tech co-constructionmay be preferred to high-tech stand-alone AAC sys-tem solutions is the resulting speed of communica-tion.
Whereas spoken language reaches more thanone hundred words per minute and an average speedtypist using standard touch typing will achieve ap-proximately 35 words per minute, a user of an AACdevice will typically input text in the 3-10 words perminute range.
With a communication partner guess-22ing the intended message and requesting confirma-tion, the communication rate can speed up dramati-cally.
For face-to-face communication ?
a modalitythat is currently very poorly served by AAC devices?
such a speedup is greatly preferred, despite anypotential authorship questions.Consider the following low-tech scenario.
Sandyis locked-in, with just a single eye-blink serving toprovide binary yes/no feedback.
Sandy?s commu-nication partner, Kim, initiates communication byverbally stepping through an imagined row/columngrid, first by number (to identify the row); then byletter.
In such a way, Sandy can indicate the firstdesired symbol.
Communication can continue inthis way until Kim has a good idea of the word thatSandy intends and proposes the word.
If Sandy saysyes, the word has been completed, much as auto-matic word completion may occur within an AACdevice.
But Kim doesn?t necessarily stop with wordcompletion; subsequent word prediction, phrase pre-diction, in fact whole utterance prediction can fol-low, driven by Kim?s intuitions derived from knowl-edge of Sandy, true sensitivity to context, topic, so-cial protocol, etc.
It is no wonder that such methodsare often chosen over high-tech alternatives.In this paper, we present some preliminary ideasand experiments on an approach to providing tech-nology support to this sort of co-construction duringtyping.
The core idea is to provide an enhanced in-terface to the communication partner (Kim in the ex-ample above), which does not allow them to directlycontribute to the message construction, but ratherto indirectly contribute, by predicting what they be-lieve the individual will type next.
Because most textgeneration AAC devices typically already rely uponsymbol, word and phrase prediction from statisticallanguage models to speed text input, the predictionsof the conversation partner could be used to influ-ence (or adapt) the language model.
Such adaptationcould be as simple as assigning high probability towords or symbols explicitly predicted by the com-munication partner, or as complex as deriving thetopic or context from the partner?s predictions andusing that context to improve the model.Statistical language models in AAC devices cancapture regularities in language, e.g., frequent wordcollocations or phrases and names commonly usedby an individual.
People, however, have access tomuch more information than computational mod-els, including rich knowledge of language, any rel-evant contextual factors that may skew prediction,familiarity with the AAC user, and extensive worldknowledge ?
none of which can be easily included inthe kinds of simple statistical models that constitutethe current state of the art.
People are typically quitegood at predicting what might come next in a sen-tence, particularly if it is part of a larger discourse ordialogue.
Indeed, some of the earliest work lookingat statistical models of language established the en-tropy of English by asking subjects to play a simplelanguage guessing game (Shannon, 1950).
The so-called ?Shannon game?
starts with the subject guess-ing the first letter of the text.
Once they have guessedcorrectly, it is uncovered, and the subject guessesthe next letter, and so on.
A similar game could beplayed with words instead of letters.
The number ofguesses required is a measure of entropy in the lan-guage.
People are understandably very good at thisgame, often correctly predicting symbols on the firsttry for very long stretches of text.
No purely com-putational model can hope to match the contextualsensitivity, partner familiarity, or world knowledgethat a human being brings to such a task.A co-construction scenario differs from a Shan-non game in terms of the time constraints underwhich it operates.
The communication partner insuch a scenario must offer completions and predic-tions to the user in a way that actually speeds com-munication relative to independent text generation.Given an arbitrary amount of time, it is clear thatpeople have greater information at their disposal forpredicting subsequent content; what happens undertime constraints is less clear.
Indeed, in this paperwe demonstrate that the time constraints put humansubjects at a strong disadvantage relative to languagemodels in the scenarios we simulated.
While it isfar from clear that this disadvantage will also applyin scenarios closer to the motivating example givenabove, it is certainly the case that providing usefulinput is a challenging task.The principal benefit of technology-assisted co-construction with communication partners is makinguse of the partner?s knowledge of language and con-text, as well as their familiarity with the AAC userand the world, to yield better predictions of likelycontinuations than are currently made by the kinds23of relatively uninformed (albeit state of the art) com-putational language models.
A secondary benefit isthat such an approach engages the conversation part-ner in a high utility collaboration during the AACuser?s turn, rather than simply sitting and waiting forthe reply to be produced.
Lack of engagement is aserious obstacle to successful conversation in AAC(Hoag et al, 2004).
The slow speed of AAC input isitself a contributing factor to AAC user dissatisfac-tion with face-to-face conversation, one of the mostcritical modes of human social interaction, and theone least served by current technology.
Because ofthe slow turnaround, the conversation partner tendsto lose focus and interest in the conversation, leadingto shorter and less satisfying exchanges than thoseenjoyed by those using spoken language.
A systemwhich leverages communication partner predictionswill more fully engage the conversation partner inthe process, rather than forcing them to wait for aresponse with nothing to do.Importantly, an enhanced interface such as thatproposed here provides predictive input from thecommunication partner, but not direct compositionalinput.
The responsibility of selecting symbols andwords during text entry remains with the AAC user,as the sole author of the text.
In the preliminaryexperiments presented later in the paper, we simu-late a direct selection typing system with word pre-diction, and measure the utility of human generatedword completions and predictions relative to n-grammodels.
In such a scenario, n-gram predictions canbe replaced or augmented by human predictions.This illustrates how easily technology assisted co-construction with communication partners could po-tentially be integrated into a user?s interface.Despite the lack of speedup achieved versus n-gram models in the results reported below, the po-tential for capturing communication partner intu-itions about AAC user intended utterances seems acompelling topic for future research.2 Background and Related WorkOver the past forty years, there has been a vastarray of technological solutions to aid AAC userswho present with severe speech and physical im-pairments, from methods for generating possibleresponses, to techniques for selecting among re-sponses.
The simplest methods to generate lan-guage involve the use of pre-stored phrases, such as?hello?, ?thank you?, ?I love you?, etc., which areavailable on many AAC devices.
Some studies haveindicated that use of such phrases improves the per-ception of fluid communication (McCoy et al, 2007;Hoag et al, 2008).Prediction options vary in AAC devices, rang-ing from letter-by-letter prediction ?
see Higgin-botham (1992) and Lesher et al (1998) for somereviews ?
to word-based prediction.
Some systemscan be quite sophisticated, for example incorporat-ing latent semantic analysis to aid in the better mod-eling of discourse-level information (Wandmacherand Antoine, 2007).
The WebCrawler project in Jef-frey Higginbotham?s lab uses topic-related wordlistsmined from the Web to populate a user?s AAC de-vice with terminology that is likely to be of utility tothe current topic of conversation.Going beyond word prediction, there has beenan increased interest in utterance-based approaches(Todman et al, 2008), which extend predictionfrom the character or word level to the levelof whole sentences.
For example, systems likeFrameTalker/Contact (Higginbotham and Wilkins,1999; Wilkins and Higginbotham, 2006) populatethe AAC device with pre-stored phrases that can beorganized in various ways.
In a similar vein, re-cent work reported in Wisenburn and Higginbotham(2008; 2009) proposed a novel method that uses au-tomatic speech recognition (ASR) on the speech ofthe communication partner, extracts noun phrasesfrom the speech, and presents those noun phrases onthe AAC device, with frame sentences that the AACuser can select.
Thus if the communication partnersays ?Paris?, the AAC user will be able to selectfrom phrases like ?Tell me more about Paris?
or ?Iwant to talk about Paris?.
This can speed up the con-versation by providing topically-relevant responses.Perhaps the most elaborate system of this kind is theHow Was School Today system (Reiter et al, 2009).This system, which is geared towards children withsevere communication disabilities, uses data fromsensors, the Web, and other sources as input for anatural language generation system.
The system ac-quires information about the child?s day in school:which classes he or she attended, what activitiesthere were, information about visitors, food choicesat the cafeteria, and so forth.
The data are then used24to generate natural language sentences, which areconverted to speech via a speech synthesizer.
At theend of the day, the child uses a menu to select sen-tences that he or she wants the system to utter, andthereby puts together a narrative that describes whathe/she did.
The system allows for vastly more rapidoutput than a system where the child constructs eachsentence from scratch.Perhaps the closest work to what we are proposingis the study of non-disabled adults in Cornish andHigginbotham (No Date), where one of the adultsplayed the role of an AAC user, and the other a non-disabled communication partner.
The participantscompleted a narrative, a map and a puzzle task.
Ofinterest was the relative amount of co-constructionof the other?s utterances by each partner, and inparticular its relation to which of the partners wasthe one initiating the attempt to achieve a commonground with the other speaker ?
the ?groundedcontribution owner?.
In all tasks both the commu-nication partner and the AAC user co-constructedeach other?s contributions, but there was the great-est asymmetry between the two users in the puzzletask.In what follows, we will first describe a prelim-inary experiment of word completion for a simu-lated AAC user, using sentences from the Enronemail corpus and the New York Times.
We thenwill present results for word completion and pre-diction within the context of dialogs in the Switch-board corpus.
While we ultimately believe thatthe potential for co-construction goes far beyondsimple word completion/prediction, these experi-ments serve as a first indication of the challengesto an enhanced technology-assisted interface for co-construction with communication partners duringnovel text generation.3 Preliminary experimentIn this section, we present a preliminary experimentto evaluate the potential utility of our technology-assisted co-construction scenario.
The experiment isakin to a Shannon Game (Shannon, 1950), but witha time limit for guesses imposed by the speed of typ-ing.
For the current experiment we chose 5 secondsper keystroke as the simulated typing speed: targetsentences appeared one character at a time, everyfive seconds.
The subjects?
task was to provide aFigure 1: Preliminary experimental interface in terminalwindow, with 4 predicted completions and cursor belowcompletion for the current word.
If the correct wordis provided by the subject, it is selected by the sim-ulated AAC user as the next keystroke.For this preliminary experiment, we used a sim-ple program running in the terminal window of aMac laptop.
Figure 1 shows a screenshot from thisprogram in operation.
The target string is displayedat the top of the terminal window, one character ata time, with the carat symbol showing white spaceword boundaries.
Predicted word completions aremade by typing with a standard qwerty keyboard;and when the enter key is pressed, the word that hasbeen typed is aligned with the current incompleteword.
If it is consistent with the prefix of the wordthat has been typed, it remains as a candidate forcompletion.
When the current five second intervalhas passed, the set of accumulated predictions arefiltered to just those which are consistent with thenew letter that the user would have typed (e.g., ?i?in Figure 1).
If the correct word completion for thetarget string is present, it is selected with the follow-ing keystroke.
Otherwise the following letter willbe typed (with the typical 5-second delay) and theinterface proceeds as before.Three able-bodied, adult, literate subjects wererecruited for this initial experiment, and all threecompleted trials with both Enron email and NewYork Times target strings.
The Enron datacomes from the Enron email dataset (http://www-2.cs.cmu.edu/?enron/) and the NY Times data fromthe English Gigaword corpus (LDC2007T07).
Bothcorpora were pre-processed to remove duplicate data(e.g., spam or multiple recipient emails), tabulardata and other material that does not represent writ-ten sentences.
Details on this normalization can befound in Roark (2009).
Both corpora consist of writ-ten sentences, one heavily edited (newspaper), theother less formal (email); and both are large enoughto allow for robust statistical language modeling.25Ngram training TestingTask sents words sents words charsNYT 1.9M 35.6M 10 201 1199Enron 0.6M 6.1M 10 102 528Table 1: Statistics for each task of n-gram training corpussize and test set size in terms of sentences, words andcharacters (baseline keystrokes)The two corpora were split into training and test-ing sets, to allow for training of n-gram languagemodels to compare word completion performance.To ensure fair comparison between n-gram and hu-man word completion performance, no sentences inthe test sets were seen in the training data.
Fromeach test corpus, we extracted sets of 10 contiguoussentences at periodic intervals, to use as test or prac-tice sets.
Each subject used a 10 sentence practiceset from the NY Times to become familiar with thetask and interface; then performed the word com-pletion task on one 10 sentence set from the NYTimes and one 10 sentence set from the Enron cor-pus.
Statistics of the training and test sets are givenin Table 1.Language models were n-gram word-based mod-els trained from the given corpora using Kneser-Neysmoothing (Kneser and Ney, 1995).
We performedno pruning on the models.We evaluate in terms of keystroke savings per-centage.
Let k be the baseline number of keystrokeswithout word completion, which is the number ofcharacters in the sample, i.e., 1 keystroke per char-acter.
With a given word completion method, let c bethe number of keystrokes required to enter the text,i.e., if the word completion method provides correctwords for selection, those will reduce the number ofkeystrokes required1.
Then keystroke savings per-centage is 100?
(k?c)/k, the percentage of originalkeystrokes that were saved with word completion.Table 2 shows the keystroke savings percentage onour two tasks for three n-gram language models (un-igram, bigram and trigram) and our three subjects.It is clear from this table that the n-gram languagemodels are achieving much higher keystroke savingsthan our three human subjects.
Further, our threesubjects performed quite similarly, not only in com-1Each word completion requires a selection keystroke, butsaves the keystrokes associated with the remaining charactersin the selected word.N-gram SubjectTask 1g 2g 3g 1 2 3NYT 47.4 54.5 56.0 36.5 32.0 32.9Enron 54.4 61.4 64.4 34.5 32.0 34.1Table 2: Keystroke savings percentage for test set acrossmodels and subjectsparison with each other, but across the two tasks.On the face of it, the relatively poor performanceof the human predictors might be surprising, giventhat the original Shannon game was intended to es-tablish a lower bound on the entropy of English.
Theassumption has always been that people have betterlanguage models than we can hope to learn automat-ically.
However, in contrast to the original Shannongame, our predictions are carried out with a fairlytight time limit, i.e., predictions need to be madewithin a fairly short period in order to be made avail-able to individuals for word completion.
The timelimit within the current scenario is one factor thatseems to be putting the subjects at a disadvantagecompared to automated n-gram models on this task.There are a couple of additional reasons why n-gram models are performing better on these tasks.First, they are specific domains with quite ampletraining data for the language models.
As theamount of training data decreases ?
which wouldcertainly be the case for individual AAC users ?
theefficacy of the n-gram models decrease.
Second,there is a 1-character advantage of n-gram modelsrelative to human predictions in this approach.
Tosee this point clearly, consider the position at thestart of the string.
N-gram models can (for prac-tical purposes) instantaneously provide predictionsfor that word.
But our subjects must begin typingthe words that they are predicting for this positionat the same time the individual is making their firstkeystroke.
Those predictions do not become opera-tive until after that keystroke.
Hence the time over-head of prediction places a lag relative to what ispossible for the n-gram model.
We will return tothis point in the discussion section at the end of thepaper.There are some scenarios, however, where thesubjects did provide word completions prior to thetrigram language model in both domains.
Interest-ingly, a fairly large fraction of these words werefaster than n-gram for more than one of the three26NY Times Enroncompany cranbury creditor hearingcreditors denied facility suggestionsfoothill jamesway jamesways stairsplan proposal sandler savingsstock stockholders warrantsTable 3: Words completed using subject suggestions withfewer keystrokes than trigram model.
Bold indicatesmore than one subject was faster for that word.subjects.
Table 3 shows the list of these words forour trials.
These tended to be longer, open-classwords with high topical importance.
In addition,they tended to be words with common word pre-fixes, which lead to higher confusability in the n-gram model.
Of course, common prefixes also leadto higher confusability in our subjects, yet they ap-pear to be able to leverage their superior context sen-sitivity to yield effective disambiguation earlier thanthe n-gram model in these cases.Based on these results, we designed a second ex-periment, with a few key changes from this prelim-inary experiment, including an improved interface,the ability to predict as well as complete, and a do-main that is closer to a proposed model for this co-construction task.4 Switchboard experimentBased on the preliminary experiment, we created anew protocol and ran seven able-bodied, adult, lit-erate subjects.
We changed the interface and do-main in ways that we believed would make a dif-ference in the ability of subjects to compete with n-gram models in keystroke savings.
What remainedthe same was the timing of the interface: charactersfor target strings were displayed every five seconds.Word completions were then evaluated for consis-tency with what had been typed, and if the correctword was present, the word was completed and re-vealed, and typing continued.Data Our primary motivating case for technology-assisted co-construction comes from face-to-face di-alog, yet the corpora from which target strings wereextracted in the preliminary experiments were fromlarge corpora of text produced under very differentconditions.
One corpus that does represent a varied-topic, conversational dialog scenario is the Switch-board corpus (Godfrey et al, 1992), which containstranscripts of both sides of telephone conversations.The idea in using this data was to provide some num-ber of utterances of dialog context (from the 10 pre-vious dialog turns), and then ask subjects to provideword completions for some number of subsequentutterances.While the Switchboard corpus does represent thekind of conversational dialog we are interested in, itis a spoken language corpus, yet we are modelingwritten (typed) language.
The difference betweenwritten and spoken language does present somethingof an issue for our task.
To mitigate this mismatchsomewhat, we made use of the Switchboard sectionof the Penn Treebank (Marcus et al, 1993), whichcontains syntactic annotations of the Switchboardtranscripts, including explicit marking of disfluen-cies (?EDITED?
non-terminals in the treebank), in-terjections or parentheticals such as ?I mean?
or?you know?.
Using these syntactic annotations, weproduced edited transcripts that omit much of thespoken language specific phenomena, thus provid-ing a closer approximation to the kind of written di-alogs we would like to simulate.
In addition, we de-cased the corpus and removed all characters exceptthe following: the 26 letters of the English alphabet,the apostrophe, the space, and the dash.Interface Figure 2 shows the graphical user inter-face that was created for these trials.
In the upperbox, ten utterances from the context of the dialog arepresented, with an indication of which speaker (A orB) took the turn.
Participants are asked to first readthis context and then press enter to begin the session.Below this box, the current utterance is displayed,along with which of the two participants is currentlyproducing the utterance.
As in the previous experi-ment, the string is displayed one character at a timein this region.
Below this is a text box where wordcompletions and predictions are entered.
Finally, atthe bottom of the interface, Figure 2 shows two ofthe five rows of current word completions (left col-umn) and next word predictions (right column).Perhaps the largest departure from the preliminaryexperiment is the ability to not only complete thecurrent word but also to provide predictions aboutthe subsequent word.
The subject uses a space de-limiter to indicate whether predictions are for thecurrent word or for the subsequent word.
Wordspreceding a space are taken as current word com-pletions; the first word after a space is taken as a27Figure 2: Experimental graphical user interfacesubsequent word prediction.
To just predict the sub-sequent word, one can lead with a space, which re-sults in no current word completion and whatevercomes after the space as next word prediction.
Oncethe current word is complete, any words on the sub-sequent word prediction list are immediately shiftedto the word completion list.
We limited current andnext word predictions to five.We selected ten test dialogs, and subjects pro-duced word completions and predictions for threeutterances per dialog, for a total of thirty utterances.We selected the test dialogs to conform to the fol-lowing characteristics:1.
Each group of three utterances was consecutiveand spoken by the same person.2.
Each utterance contained more than 15 charac-ters of text.3.
Each group of three utterances began turn-initially; the first of the three utterances wasalways immediately after the other speaker inthe corpus had spoken at least two consecutiveutterances of 15 characters or more.4.
Each group of three utterances was far enoughinto its respective conversation that there wasenough text to provide the ten lines of contextrequired above.Language models used to contrast with humanperformance on this task were trained separately forevery conversation in the test set.
For each conver-sation, Kneser-Ney smoothed n-gram models werebuilt using all other conversations in the normalizedSwitchboard corpus.
Thus no conversation is in itsown training data.
Table 4 shows statistics of train-ing and test sets.Table 5 shows the results for n-gram models andour seven subjects on this test.
Despite the differ-ences in the testing scenario from the preliminaryexperiment, we can see that the results are very sim-ilar to what was found in that experiment.
Also sim-ilar to the previous trial was the fact that a large per-centage of tokens for which subjects provided fasterword completion than the trigram model were fasterfor multiple subjects.
Table 6 shows the nine wordsthat were completed faster by more than half of thesubjects than the trigram model.
Thus, while there issome individual variation in task performance, sub-jects were fairly consistent in their ability to predict.5 DiscussionIn this paper we presented two experiments thatevaluated a new kind of technology-assisted co-construction interface for communication partnersduring time-constrained text generation.
ResultsNgram training TestingTask sents words sents words charsSWBD 0.66M 3.7M 30 299 1501Table 4: Statistics for the Switchboard task of n-gramtraining corpus size and test set size in terms of utter-ances, words and characters (baseline keystrokes)28N-gram SubjectTask 1g 2g 3g 1 2 3 4 5 6 7Switchboard 51.0 59.0 60.0 28.7 33.1 28.4 28.6 34.1 31.8 32.5Table 5: Keystroke savings percentage for Switchboard test set across models and subjectsapplied can?t comesevery failure namedphysics should supervisorTable 6: Words completed in more than half of theSwitchboard trials using subject suggestions with fewerkeystrokes than trigram model.from both experiments are negative, in terms of theability of our human subjects to speed up communi-cation via word prediction under time constraints be-yond what is achievable with n-gram language mod-els.
These results are somewhat surprising givenconventional wisdom about the superiority of hu-man language models versus their simplified compu-tational counterparts.
One key reason driving the di-vergence from conventional wisdom is the time con-straint on production of predictions.
Another is theartificiality of the task and relative unfamiliarity ofthe subjects with the individuals communicating.While these results are negative, there are reasonswhy they should not be taken as an indictment ofthe approach as a whole, rather an indication of thechallenges faced by this task.
First, we would stressthe fact that we have not yet tested the approach in asituation where the user knows the speaker well, andtherefore can be presumed to have knowledge wellbeyond general knowledge of English and generaltopical knowledge.
In future work we are planningexperiments based on interactions between peoplewho have a close relationship with each other.
Insuch a scenario, we can expect that humans wouldhave an advantage over statistical language models,for which appropriate training data would not, in anycase, be available.None of the domains that we evaluated were a per-fect match to the application: the text data was notdialog, and the dialogs were spoken rather than writ-ten language.
Further, the tasks that we evaluated inthis paper are quite rigid compared to what mightbe considered acceptable in real use.
For example,our task required the prediction of a particular wordtype, whereas in actual use synonyms or other waysof phrasing the same information will likely be quiteacceptable to most AAC users.
In such an applica-tion, the task is not to facilitate production of a spe-cific word string, rather production of an idea whichmight be realized variously.
We were interested inthe tasks reported here as a first step towards under-standing the problem, and among the lessons learnedare the shortcomings of these very tasks.Another take-away message relates to the util-ity of the new interface itself.
The subjects inthese trials had the difficult task of quickly pre-dicting intended words; this is also a communica-tion task that may be assisted.
Providing access towhat n-gram models are predicting may allow thecommunication partner to quickly select or winnowdown the options.
Further, it is apparent that singleword completions or predictions is not where com-munication partners are going to achieve order-of-magnitude speedups in communication; rather suchspeedups may be realized in facilitation of largerphrase or whole utterance production, particularlywhen the communication is between familiar part-ners on known topics.In summary, this paper presented preliminary re-sults on the ability of human subjects to provideword completion and prediction information to usersof AAC systems, through simulation of such a newinterface concept.
While the subjects were notable to match n-gram language models in termsof keystroke reduction, we did see consistent per-formance across many subjects and across severaldomains, yielding real keystroke reductions on thestimulus strings.
Ultimately, the tasks were not asrepresentative of real co-construction scenarios a wewould have liked, but they serve to illustrate thechallenges of such an application.AcknowledgmentsThis research was supported in part by NIH Grant#1R01DC009834-01.
Any opinions, findings, con-clusions or recommendations expressed in this pub-lication are those of the authors and do not necessar-ily reflect the views of the NIH.29ReferencesD.R.
Beukelman, S. Fager, L. Ball, and A. Dietz.
2007.AAC for adults with acquired neurological conditions:A review.
Augmentative and Alternative Communica-tion, 23(3):230?242.Jennifer Cornish and Jeffrey Higginbotham.
No Date.Assessing AAC interaction III: Effect of task typeon co-construction & message repair.
AAC-RERC, available from http:aac-rerc.psu.edu/_userfiles/asha3.pdf.J.J.
Darragh, I.H.
Witten, and M.L.
James.
1990.
Thereactive keyboard: A predictive typing aid.
Computer,23(11):41?49.J.J.
Godfrey, E.C.
Holliman, and J. McDaniel.
1992.Switchboard: A telephone speech corpus for researchand develpment.
In Proceedings of ICASSP, volume I,pages 517?520.D.
Jeffery Higginbotham and David Wilkins.
1999.Frametalker: A system and method for utilizing com-munication frames in augmented communication tech-nologies.
US Patent No.
5,956,667.D.
Jeffery Higginbotham.
1992.
Evaluation of keystrokesavings across five assistive communication technolo-gies.
Augmentative and Alternative Communication,8:258?272.Linda A. Hoag, Jan L. Bedrosian, Kathleen F. McCoy,and Dallas Johnson.
2004.
Informativeness and speedof message delivery trade-offs in augmentative andalternative communication.
Journal of Speech, Lan-guage, and Hearing Research, 47:1270?1285.Linda A. Hoag, Jan L. Bedrosian, Kathleen F. Mc-Coy, and Dallas Johnson.
2008.
Hierarchy ofconversational rule violations involving utterance-based augmentative and alternative communicationsystems.
Augmentative and Alternative Communica-tion, 24(2):149?161.R.
Kneser and H. Ney.
1995.
Improved backing-off form-gram language modeling.
In Proceedings of theIEEE International Conference on Acoustics, Speech,and Signal Processing (ICASSP), pages 181?184.Heidi H. Koester and Simon Levine.
1996.
Ef-fect of a word prediction feature on user perfor-mance.
Augmentative and Alternative Communica-tion, 12(3):155?168.Heidi H. Koester and Simon Levine.
1997.
Keystroke-level models for user performance with word predic-tion.
Augmentative and Alternative Communication,13(4):239257.Heidi H. Koester and Simon Levine.
1998.
Modelsimulations of user performance with word predic-tion.
Augmentative and Alternative Communication,14(1):25?36.G.W.
Lesher, B.J.
Moulton, and D.J.
Higginbotham.1998.
Techniques for augmenting scanning commu-nication.
Augmentative and Alternative Communica-tion, 14:81?101.J.
Li and G. Hirst.
2005.
Semantic knowledge in wordcompletion.
In Proceedings of the 7th InternationalACM Conference on Computers and Accessibility.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Kathleen F. McCoy, Jan L. Bedrosian, Linda A. Hoag,and Dallas E. Johnson.
2007.
Brevity and speed ofmessage delivery trade-offs in augmentative and alter-native communication.
Augmentative and AlternativeCommunication, 23(1):76?88.Ehud Reiter, Ross Turner, Norman Alm, Rolf Black,Martin Dempster, and Annalu Waller.
2009.
Us-ing NLG to help language-impaired users tell storiesand participate in social dialogues.
In 12th EuropeanWorkshop on Natural Language Generation, pages 1?8.
Association for Computational Linguistics.B.
Roark.
2009.
Open vocabulary language modelingfor binary response typing interfaces.
TechnicalReport #CSLU-09-001, Center for Spoken LanguageProcessing, Oregon Health & Science University.cslu.ogi.edu/publications/ps/roark09.pdf.C.E.
Shannon.
1950.
Prediction and entropy of printedEnglish.
Bell System Technical Journal, 30:50?64.John Todman, Norman Alm, D. Jeffery Higginbotham,and Portia File.
2008.
Whole utterance approaches inAAC.
Augmentative and Alternative Communication,24(3):235?254.K.
Trnka, D. Yarrington, K.F.
McCoy, and C. Pennington.2006.
Topic modeling in fringe word prediction forAAC.
In Proceedings of the International Conferenceon Intelligent User Interfaces, pages 276?278.K.
Trnka, D. Yarrington, J. McCaw, K.F.
McCoy, andC.
Pennington.
2007.
The effects of word predic-tion on communication rate for AAC.
In Proceed-ings of HLT-NAACL; Companion Volume, Short Pa-pers, pages 173?176.H.
Trost, J. Matiasek, and M. Baroni.
2005.
The lan-guage component of the FASTY text prediction sys-tem.
Applied Artificial Intelligence, 19(8):743?781.T.
Wandmacher and J.Y.
Antoine.
2007.
Methods to in-tegrate a language model with semantic informationfor a word prediction component.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 506?513.David Wilkins and D. Jeffery Higginbotham.
2006.
Theshort story of Frametalker: An interactive AAC de-vice.
Perspectives on Augmentative and AlternativeCommunication, 15(1):18?21.30Bruce Wisenburn and D. Jeffery Higginbotham.
2008.An AAC application using speaking partner speechrecognition to automatically produce contextually rel-evant utterances: Objective results.
Augmentative andAlternative Communication, 24(2):100?109.Bruce Wisenburn and D. Jeffery Higginbotham.
2009.Participant evaluations of rate and communication ef-ficacy of an AAC application using natural languageprocessing.
Augmentative and Alternative Communi-cation, 25(2):78?89.31
