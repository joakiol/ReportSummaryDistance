INQUERY System OverviewJohn Broglio James P. Callan W. Bruce CroftComputer  Science Depar tmentUn ivers i ty  of Massachuset tsAmherst, MA 01003-4610, USA{broglio, callan, croft}@cs.umass.edu1.
Descr ip t ion  o f  F ina l  Sys tem1.1.
ApproachThe T IPSTER project in the Information Retrieval Lab-oratory of the Computer Science Department, Universityof Massachusetts, Amherst (which includes MCC as asubcontractor), has focused on the following goals:?
Improving the effectiveness of information retrievaltechniques for large, full-text databases,?
Improving the effectiveness of routing techniquesappropriate for long-term information eeds, and?
Demonstrating the effectiveness of these retrievaland routing techniques for Japanese full textdatabases \[5\].Our general approach to achieving these goals has beento use improved representations of text and informationneeds in the framework of a new model of retrieval.
Thismodel uses Bayesian networks to describe how text andqueries should be used to identify relevant documents\[7, 4, 8\].
Retrieval (and routing) is viewed as a proba-bilistic inference process which compares text represen-tations based on different forms of linguistic and sta-tistical evidence to representations of information eedsbased on similar evidence from natural anguage queriesand user interaction.
Learning techniques are used tomodify the initial queries both for short-term and long-term information eeds (relevance feedback and routing,respectively).This approach (generally known as the inference netmodel and implemented in the INQUERY system \[1\])emphasizes retrieval based on combination of evidence.Different text representations ( uch as words, phrases,paragraphs, or manually assigned keywords) and differ-ent versions of the query (such as natural anguage andBoolean) can be combined in a consistent probabilisticframework.
This type of "data fusion" has been knownto be effective in the information retrieval context for anumber of years, and was one of the primary motivationsfor developing the inference net approach.Another feature of the inference net approach is the abil-ity to capture complex structure in the network repre-senting the information eed (i.e.
the query).
A practi-cal consequence of this is that complex Boolean queriescan be evaluated as easily as natural language queriesand produce ranked output.
It is also possible to repre-sent "rule-based" or "concept-based" queries in the sameprobabilistic framework.
This has led to us concentrat-ing on automatic analysis of queries and techniques forenhancing queries rather than on in-depth analysis of thedocuments in the database.
In general, it is more effec-tive (as well as efficient) to analyze short query textsthan millions of document exts.
The results of thequery analysis are represented in the INQUERY querylanguage which contains a number of operators, such as#SUM, #AND, #OR,  #NOT,  #PHRASE,  and #SYN.These operators implement.different methods of combin-ing evidence.Some of the specific research issues we are addressing aremorphological analysis in English and Japanese, wordsense disambiguation i English, the use of phrases andother syntactic structure in English and Japanese, theuse of feature recognizers (for example, company, coun-try and people name recognizers) in representing doc-uments and queries, analyzing natural anguage queriesto build structured representations of information eeds,learning techniques appropriate for routing and struc-tured queries, techniques for acquiring domain knowl-edge by corpus analysis, and probability estimation tech-niques for indexing.The T IPSTER and TREC evaluations have made it clearthat a lot remains to be learned about retrieval and rout-ing in large, full-text databases based on complex infor-mation needs.
On the other hand, we have made con-siderable progress in developing effective techniques forthis environment, and the evaluations have shown thatgood levels of performance can be achieved.1 .2 .
P rocess ing  F lowThe main processes in INQUERY are document index-ing, query processing, query evaluation and relevance47feedback.
We will give a brief description of these pro-cesses.In the document indexing process, documents are parsedand index terms representing the content of documentsare identified.
INQUERY supports a variety of indexingtechniques including simple word-based indexing, index-ing based on part-of-speech tagging and phrase identifi-cation, and indexing by domain-dependent features uchas company names, dates, locations, etc.
The last typeof indexing is a first step towards integrating detectionand extraction systems.In more detail, the document structure is used to iden-tify which parts will be used for indexing.
The first stepof this process is to scan for word tokens.
Most typesof words (including numbers) are indexed, although astopword list is used to remove very common words.Stopwords can be indexed, however, if they are capi-talized (but not at the start of sentences) or joined withother words (e.g.
"the The-1 system").
Words are thenstemmed to conflate variants.
We have developed a newstemming algorithm that has a number of advantagesfor operational systems.
A number of feature recogniz-ers written with the UNIX utility f lex  are then used toidentify objects such as company names and mark theirpresence in the document using "meta" index terms.
Acompany name such as IBM in the text, for example,will result in a meta-term ~COMPANY being recordedat that position in the text.
The use of these meta-termsextends the range of queries that can be specified.
Thiscompletes the usual processing for document text.The document indexing process also involves buildingthe compressed inverted files that are necessary for effi-cient performance with very large databases.
Since posi-tional information is stored, overhead rates are typicallyabout 40% of the original database size.Query processing involves a series of steps to identifythe important concepts and structure describing a user'sinformation eed.
INQUERY is unique in that it canrepresent and use complex structured escriptions in aprobabilistic framework.
Many of the steps in query pro-cessing are the same as those done in document indexing.In addition, a part-of-speech tagger is to used to identifycandidate search phrases.
Domain-dependent featuresare recognized and meta-terms inserted into the queryrepresentation.
The relative importance of query con-cepts is also estimated, and relationships between con-cepts are suggested based on simple grammar ules.
Anevaluation of some of the query processing techniques ipresented in \[2\].INQUERY is also capable of expanding the query us-ing relationships between concepts found by either us-ing manually specified domain knowledge in the formof a simple thesaurus or by corpus analysis.
TheWORDFINDER system is a version of INQUERYthat retrieves concepts that are related to the query.WORDFINDER is constructed by identifying noungroups in the text and representing them by the wordsthat are closely associated with them (i.e.
occur in thesame text windows).
Concept "documents" are thenstored in INQUERY.
This technique has shown consid-erable promise in retrieval experiments.-The query evaluation process uses the inverted files andthe query represented as an inference net to produce adocument ranking.
The evaluation involves probabilis-tic inference based on the operators defined in the IN-QUERY language.
These operators define new conceptsand how to calculate the belief in those concepts usinglinguistic and statistical evidence.
We are constantlyexperimenting with and refining these operators (for ex-ample, the operator defining a phrase-based concept) inorder to improve retrieval performance.
The efficiencyof retrieval is comparable to commercial information re-trieval systems.The relevance feedback process uses information fromuser evaluations of retrieved documents to modify theoriginal query in ad-hoc retrieval or routing environ-ments.
The INQUERY system, because it can repre-sent structured queries, supports a wide range of learn-ing techniques for query modification \[6\].
In general, newwords and phrases are identified in the sample of relevantdocuments.
These are added to the original query andall the terms in the query are then reweighted.
Withthe amount of relevance information available in TIP-STER, relatively simple automatic techniques appear toproduce good levels of effectiveness.
We are also investi-gating the effect of using more limited information andmore complex learning techniques, such as neural net-works.The Japanese version of INQUERY only differs fromthe English version in the low-level language process-ing and some aspects of the interface.
We have carriedout experiments using both character and word-basedrepresentations of documents in combination with word-based processing of queries that are represented usingthe INQUERY language.
These experiments have shownthat character-based representations, which are efficientto produce, are surprisingly effective.1 .3 .
Descr ip t ion  o f  Key  SubsystemsAs described above, INQUERY has five key subsystems:48Document will describe marketing strategies carriedout by U.S. companies for their agricultural chem-icals, report predictions for market share of suchchemicals, or report market statistics for the chemi-cals.
pesticide, herbicide, fungicide, insecticide, fer-tilizer, predicted sales, market share, stimulate de-mand, price cut, volume of salesFigure 1: Stemming example: Query text.?
document indexing,?
WORDFINDER,?
query processing,?
query evaluation,?
relevance feedback, and routing.Each subsystem is described in more detail below.Document  Index ing :  The text-processing modulesfor document indexing are frequently described as docu-ment parsers or document parsers and feature recogniz-ers.
The purpose of all document parsing modules isto generate transactions whereby words, or terms, andtheir locations are stored in the document indexes.
Thephases of document parsing and term recognition are (1)layout analysis, (2) lexical analysis, (3) syntactic analy-sis, and (4) concept identification.
Each of these phasesmay generate multiple transactions for each document,each transaction recording term, document and the loca-tions in the document where term is found.INQUERY has a set of default text processing modules.We will discuss the default module behavior, but thesemodules can be easily replaced if some behavior otherthan the default is desired.The minimal document layout analysis must identify thebeginnings and end of documents, and the beginning andend of each text segment.
Text is distinguished from for-matting information, cataloguing information or any in-formation to be excluded for retrieval purposes.
Option-market strateg carr #usa compan #company agri-cultur chemic report predict market share chemic re-port market statist market agrochem #usa pesticidherbicid fungicid insecticid fertil predict sale stimuldemand price cut volum saleFigure 2: Stemming example: Query text, after stop-word and stop-phrase removal, as stemmed by the Porterstemmer.marketing strategy carry #usa company #companyagriculture chemical report prediction market sharechemical report market statistic marketing agro-chemic #usa pesticide herbicide fungicide insecticidefertilizer predict sale stimulate demand price cut vol-ume saleFigure 3: Stemming example: Query text, after stop-word and stop-phrase removal, as stemmed by theKSTEM stemmer.ally, title, document identification code and other specialfields may be identified, and a document layout formatmay be enforced.
The default parser elies on a subset ofStandardized General Mark-up Language (SGML), butany layout analysis module may be substituted in or-der to recognize whatever document style is required.The only requirements are that the layout analysis mod-ule indicate to the indexing system when to begin andend indexing for that document, and which section(s) totreat as text to be indexed.The function of a lezical analysis module is to identifyand record word boundaries, recognize stopwords andstem the words as desired, and generate transactions sothe words will be indexed for retrieval.
In theory, ev-ery word in the document collection will be indexed.
Inpractice, it is helpful to identify very common words,such as operators or closed-class words, which do notcarry any meaningful information for retrieval purposes(although they may offer significant information for textor content extraction).
These stopwords are not indexed,although they are retained in the text so that subsequenttextual analysis (syntactic analysis, feature recognition)may make use of them.
Stemming is performed to con-flare words that have the same root form or stem, inspite of different endings (Figures 1 and 2).
The currentversion of INQUERY uses the Porter stemmer.A new stemmer, which is more sensitive to the true mor-phological word stems, has been developed (Figure 3).The KSTEM stemmer has certain advantages over thePorter stemmer.
The Porter stemmer produces tringsthat are not necessarily English words.
Because thestemmed representation is sometimes unintelligible, itis difficult to get feedback from a human user by pre-senting a processed query for modification.
In addition,the Porter stemmer sometimes conflates words that arerelated by only spelling.
Thus the phrases "a conflictover foreign policy" and "a conflict between foreignersand the police" would produce the same query networkwith the Porter stemmer.The KSTEM stemmer always uses English words as its49John Davenport, 52 years old, was appointed chiefexecutive officer of this international telecommuni-cations concern's U.S. subsidiary, Cable & Wire-less North America Inc. Mr. Davenport, who suc-ceeds John Zrno, is currently general manager of thegroup's operations in Bermuda.Figure 4: Indexing example: Original document ext.stemming output.
It is more conservative as well, con-flaring only words that are on the same derivational path.Concept recognition is an important step in automaticclassification and glossary construction.
The base set ofrecognizers which are delivered with INQUERY include:U.S.  c i ty  recogn izer :  For each mention of a U. S.city in the text, generates a transaction forthe special term #CITY .Count ry  recogn izer :  For each mention of a countryin the text, generates a transaction for either#USA or #FOREIGNCOUNTRY.Company name recogn izer :  For each citation of acompany in the text, generates a transactionfor the special term #COMPANY.Person  name recogn izer :  For each mention of anidentifiable person's name in the text, gen-erates a transaction in a special person file.Figures 4 and 5 illustrate the role that these recognizersplay in document indexing.WORDFINDER:  WORDFINDER is a process,analagous to document indexing, in which simple noungroups are indexed by the words that occur near them inthe document ext.
The result of this indexing processis a database in which one or more words can be used toretrieve simple noun groups.
This database can be usedfor query expansion, as a method of adding concepts hatare related to a set of query terms.WORDFINDER is based on the assumption that con-cepts that have a similar lexical context may be relatedsemantically.
For example, the words "connectionism"and "neural networks" occur in similar lexical contexts,but rarely in the same documents.
The semantic rela-tionship captured is not necessarily synonymy, becauseWORDFINDER also might relate "connectionism" and"back propagation", which co-occur but have differentmeanings.The major steps in constructing a WORDFINDERdatabase are:John Davenport #PERSON,  52 years old, appointedchief executive officer international telecommunica-tions concern U.S. #USA subsidiary, Cable WirelessNorth America Inc. #COMPANY Mr. Davenport,#PERSON succeeds John Zrno, #PERSON cur-rently general manager group's operations Bermuda#FOREIGNCOUNTRY.Figure 5: Indexing example: Document ext indexed.1.
Extract simple sequences of nouns, using part ofspeech tagging (e.g.
\[3\]).2.
For each noun sequence, collect the significant wordsthat occur in any document within a window of nwords containing the sequence.
The words collectedform a feature vector representing the lexical con-text of the sequence.3.
Queries are expanded by adding noun groups thatare close in feature space to the words in the originalquery (see Figures 6 and 7).In Figures 6 and 7 the floating point numbers to theleft of each query expansion term are belief values.
Theabsolute magnitude of a behef value is not meaningful.Query:l l5.1 : Impact of the 1986 Immigration Law -will report specific consequence consequences of theU.S.
's Immigration Reform and Control Act of 1986.0.5114620.5019360.4991200.4989640.4980540.4924530.4909930.4894480.4887540.4877620.4871870.4832450.4826870.4804490.4802220.4786250.4784370.4777980.4759950.475995illegal immigrationillegalsundocumented aliensamnesty programimmigration reform laweditorial-page articlenaturalization servicecivil finesnew immigration lawlegal immigrationemployer sanctionssimpson-mazzoh immigration reformstatutesapplicabilityseeking amnestylegal statusimmigration actundocumented workersguest workersweeping immigration lawFigure 6: Query expansion example: Concepts discov-ered automatically for topic 115.50Query:132.1 : "Stealth" Aircraft - will provide cost,technical, and/or performance data on U.S. "stealth"aircraft projects.0.529560 northrop corp.0.528570 tactical fighter0.525970 aerospace companies0.520621 flying wing design0.519889 enemy radar0.517714 stealth bomber0.517441 development program0.515699 radar-evading aircraft0.514796 bat-winged aircraft0.513967 cost overruns0.
'512299 expensive plane0.508889 stealth fighter0.506980 radar-evasion standards0.505602 full-scale production0.505052 palmdale0.503255 radar-evading0.503105 pentagon official0.500766 flying wing0.498029 air force officials0.496971 development costsFigure 7: Query expansion example: Concepts discov-ered automatically for topic 132.It is the result of the complex process of query evalu-ation combining term weights, document and collectionfrequencies and query network structure.
Within in thesame query expansion set, however, the relative magni-tude of belief values is significant, Further research willdetermine if a weighting scheme for expansion terms canbe developed based on the belief values.There is an advantage to using WORDFINDER for au-tomatic query expansion.
Since the query expansionconcepts are derived from the text of the collection it-self, WORDFINDER automatically creates collection-specific concept links.
Further research will compare thevalue of generic WORDFINDER databases to collection-specific ones.WORDFINDER is sensitive to several parameters, in-cluding the size of the window, decisions about whatwords to include in the window, and whether lowand/or high frequency noun groups are removed fromthe database.
Experiments have shown that gettingthese parameters right is important o the effectivenessof the concepts returned.
The current implementation ofWORDFINDER shows promise on the T IPSTER data,but further work is necessary to build a WORDFINDERthat would be effective on a variety of document collec-tions.Query  P rocess ing ;  Queries can be made to IN-QUERY by using either natural language or a struc-tured query language or a mixture of the two.
Natu-ral language queries are interpreted by text processingmodules which ensure that the handling of lexical anal-ysis and concept recognition match that of the index-ing subsystem.
The resulting modified queries are con-verted to the structured query language by applying the#SUM operator to the terms in the query.
Equations1-6 (below, in the discussion of the Retrieval engine) de-scribe #SUM and the other operators in INQUERY'sstructured query language.
Query operators permit thetext-processing system or the user interface to supplystructural information with the query, including phrasegrouping or proximity requirements.
Query text is gen-erally converted to lower case and checked for stopwordsand stemmed, if necessary, to produce canonical wordforms.Query text processing must minimally mirror the index-ing text processing.
But because query texts are muchshorter than document collections, it is practical to ex-periment with more thorough textual analysis at the re-search and development stage.
All text processing isexperimental nd the sequence of operations is adjustedfrequently as more is learned about the effects of thisprocessing.
Text processing methods which prove suc-cessful with queries can then be integrated into the in-dexing process.
This reduces the need to repeatedly in-dex large document collections in order to make smallexperimental djustments.For this reason, much T IPSTER query processing is per-formed as preprocessing rather than in the body of theINQUERY program.
Preprocessing stages are made upof sed, awk, f lex  (or lex)  scripts and C code.
Querytext processing that has been integrated into INQUERYis made up of f lex  feature analyzers.Currently INQUERY has a small number of internalquery text processors.
One handles hyphenated wordsand groups of capitalized words by enclosing them inthese proximity operators.
Orthographic lues such ashyphenation and capitalization, when reliable, are verygood clues to phrasal grouping.
Hyphens are generallydiscarded uring indexing so that an expression such asIvan-Contra or voice-activated become Ivan Contra andvoice activated, respectively.
In query processing, thesymmetrical procedure is to remove the hyphen and tophrase the words as #i ( Ivan Contra ) or #1 ( vo iceac t ivated  ), requiring that the two words be found ad-jacent in a document for the query operator to be satis-fied.
Groups of capitahzed words are similarly phrasedso that the phrase House of Representatives in a queryis transformed to #3( House Representat ives  ).51<hum> Number: 106<dom> Domain: Law and Government<tit le> Topic: U.S. Control of Insider Trading<desc> Description:Document will report proposed or enacted changes to U.S. laws and regulations designed to prevent insidertrading?<con> Concept(s):1. insider trading2.
securities law, bill, legislation, regulation, rule3.
Insider Trading Sanctions Act, Insider Trading and Securities Fraud Enforcement Act4.
Securities and Exchange Commission, SEC, Commodity Futures Trading Commission, CFTC, NationalAssociation of Securities Dealers, NASD<fat> Factor(s):<nat> Nationality: U.S.Figure 8: Query processing example: Original query.Another of INQUERY's query text processors removesphrases that discuss the retrieval process rather than thedesired material, such as "customer eports that .
.
. "
or"A relevant document must contain?
?
.".
This behavioris domain dependent, so the program's rules should bemodified or augmented for different types of use.What follows is a description of the text preprocessingmodules that have been used for T IPSTER queries.
Theorder of their use is not fixed, but it can be significant?For example, it was found useful to phrase a hyphenatedcompound such as wordl-word~ as #1 ( wordl word2?
.
.  )
and to phrase a group of capitalized words as #3 (wordl word2 .
.
.
).
We have experimented with remov-ing names of countries and some capitalized expressionsfrom medium-sized phrases (e.g.,#PHRASE ( wordl cap i ta l i zed-group  word2.
.
.
)#PHRASE ( wordl word2.
.
. )
cap i ta l i zed-group .Obviously, it makes a difference whether you process hy-phenated and capitalized words before or after you gen-erate the larger #PHRASE.The input and output description for a processor or pre-processor is the same: query text.
There are no restric-tions.
However, text which is tagged with part of speechtags contains more information and more "noise".
So,for each query text preprocessor, there are two versions,one which operates on tagged text and one for untaggedtext.There are two main kinds of query styles: a natural an-guage query and a keyword or key concept query.
Forexample, the <desc> and <nar r> fields of a T IPSTERtopic (see Figure 8) represent natural language queriesof varying levels of abstraction.
The <con>, <t i t le>and <fat> fields represent key concepts in the query?The main difference between the two types of processingis that the key concept query has more controlled infor-mation.
The phrasing and emphasis are already givenand do not have to be conjectured from the languagestructure?
It is valuable to discover how to treat bothstyles of query, because a good user interface will make iteasy for a user to input both styles.
For example, a usermay enter a prose query and then highlight the impor-tant words and phrases in the query in some convenientmanner.
These highlighted words would then be treatedas key concepts in the query processing?Natural language query fields are tagged for syntacticcategory by a part-of-speech (POS) tagger?
Currentlywe use the tagger developed by Ken Church \[3\].
Thistagger is proprietary software and not available with IN-QUERY.
We have developed our own POS tagger, whichwe are beginning to use now.
Additionally, we changeoperator phrases to single words in order to simplify laterprocessing?
An example of this simplification is replac-ing the phrase in order to with the infinitive particle toor replacing with respect o with the word regarding.
Thegoal of this replacement is to remove phrases which re-semble noun phrases syntactically but which are reallysyntactic operators (e.g., phrasal prepositions) with nosubstantive content.When the text is tagged and the potentially irrelevantmaterial has been removed, syntactically-based noungroup capture is performed.
Certain kinds of nounphrase patterns are enfolded in a #PHRASE operator(Figure 9):1.
A noun phrase which contains more than one modi-fying adjective and noun is enclosed in a #PHRASEoperator;52#WSUM ( 1.0!Terms from <title> field:2.0 #UW50 ( Control of Insider Trading )2.0 #PHRASE ( #USA Control ) 5.0 #PHRASE ( Insider Trading )!
Terms from <con> field:2.0 #PHRASE(  securities law) 2.0 bill 2.0 legislation 2.0 regulation2.0 rule 2.0 #3(  Insider Trading Sanctions Act)2.0 #3(  Insider Trading and Securities Fraud Enforcement Act )2.0 #3(  Securities and Exchange Commission) 2.0 SEC2.0 #3(Commodity Futures Trading Commission) 2.0 CFTC2.0 #3(  National Association of Securities Dealers) NASD!
Terms from <desc> field:1.0 proposed 1.0 enacted 1.0 changes 1.0 #PHRASE ( #USA laws )1.0 regulations 1.0 designed 1.0 prevent2.0 #NOT(#FOREIGNCOUNTRY)Figure 9: Query processing example: Automatically processed query.2.
A head noun with no premodifiers and followed bya prepositional phrase is enclosed in a #PHRASEoperator with the head noun of the prepositionalphrase;All text in the query is searched for constraint expres-sions.
Among these expressions are the words company,hog U. S. or a restriction in the nationality section of the<fac> field to U.S. or other nationality.
A restriction toU.S.
nationality as the area of interest is implemented bypenalizing documents for references to foreign countries.A restriction to other nationalities is implemented byrepeating that country as a term.
This asymmetry de-pends on the fact that the document collection is drawnsolely from U.S. sources, and therefore the U.S., as thedefault area of interest, is rarely referred to unless a gov-ernment body or foreign policy implementation is underdiscussion (Figure 9).There is some recognition of simple time expressions,such as since 198$ which are expanded to the set of yearswhich might be intended by the phrase in question.Countries are recognized as such and are handled so thatexpressions like South Africa are phrased as #1 ( southa f r i ca  ) even when they appear in the middle of alarger group of capitalized words.
In addition, propernames uch as country names are moved out of the scopeof #PHRASE operators, since it generally increases theeffectiveness of a #PHRASE to reduce the number ofwords in it.
Nationality constraints can better be main-tained within the scope of the larger and more tolerant#SUM operator.
For example the phrase"import ban on South African diamonds"becomes by stages,#PHRASE ( import ban on #SYN (#I (southa f r i can)  #I (south a f r i ca ) )  diamonds)and finally#SUM (#SYN (#1(south afr ican) #l(southaf r i ca ) )  #PHRASE(import ban on diamonds)).Key concept query processing is different from prosequery processing since the concept separation providedby the user can presumably be trusted.
Instead of usinga part-of-speech tagger, we rely on comma delimitationof concepts, and #PHRASE the words found betweeneach pair of delimiters (Figure 9: Terms from <con>field).Additionally, if any constraints were found anywhere lsein the query, e.g., a mention of the word company or anexclusionary geographical constraint (e.g., not USA oronly USA), the query will be modified according to theseconstraints.
For example (Figure 9),only USA~#NOT (#FOREIGNCOUNTE?
)andnot USA =~ #NOT ( #USA ).If the word company is found in a query, then a secondcopy of the key concepts (the <con> field), is producedwhere each item in the field appears in an unorderedwindow operator with the feature #COMPANY.
For ex-ample, if the word South Africa appears as a key concept(and company appears omewhere in the query), then thepreprocessor would produce the term #OWS0( #COMPANY#1 ( south a f r i ca ) )  which would match any document53#WSUM (1.02.0 #UWS0 (Control of Insider Trading )3.0 #3 ( Insider Trading )1.0 #3(  securities law) 1.0 #3(  Insider Trading Sanctions Act)1.0 #3(  Insider Trading and Securities Fraud Enforcement Act )1.0 #3(  Securities and Exchange Commission) 1.0 SEC1.0 #3(Commodity Futures Trading Commission) 1.0 CFTC1.0 #3(  National Association of Securities Dealers) 1.0 NASD2.0 #uw50( #syn(bill aw regulation rules) insider trading)1.0 #3(increasing penalties) 1.0 #3(closing loopholes)1.O #NOT(Boesky) 1.0 #NOT(Milken)1.0 #NOT(#3(Drexel  Burnham Lambert)2.0 #NOT (#FOREIGNCOUNTRY)  )Figure 10: Query processing example: Manually modified query.which had a company name within fifty words of SouthAfrica.We have experimented with manual modification of pro-cessed queries in order to measure the feasibility andeffectiveness of simple user adjustments to automaticquery processing output.
We have explored simple mod-ifications uch as adding a term from the Narrative field,deleting a term, and constraining existing teims to ap-pear near each other in a document (Figure 10).
This hasproved to be generally effective in increasing the qualityof retrieval results.Ret r ieva l  Engine:  Once a query net is formed, theretrieval subsystem can rank documents according to thebelief that they are relevant o the query.
INQUERY'smethods of combining evidence are summarized below,and are documented more fully in \[9\].belnot(Q) = 1 -p lbelow(Q) = 1 - (1 -p l ) .
.
.
.
.
(1 -p ,~)beland(Q) = P l 'P2" .
.
.
.p ,~belm~x(Q) = max(pl,p2,...,pn)(wlPl + w2p2 + .. .
+ wnpn)wq bel  .
.
.
.
.
.
(Q)  =(Wl +w2 +.
.
.+w~)beL,,m(Q) = (pl +pz  +- - -+pn)nand belief in its arguments as presented in Equations 1-6.Re levance  Feedback  and Routing: The INQUERYsystem is able to refine queries automatically based uponrelevance feedback by a user.
The general approach isfor the system to select terms from relevant documents,add them to the query, and then reweight all of the queryterms \[6\].
Experiments have been conducted with a va-riety of algorithms for term selection and weighting.Early experiments \[6\] showed that ranking terms by theproduct of their frequency in relevant documents (rdf)and their inverse document frequency (idf) was best onsmall and medium-sized collections with relatively smallnumbers of relevance judgements.
The number of termsadded was set empirically at 5.
Term weights were de-termined by their frequency in relevant documents (rtf).The INQUERY system still uses this model for interac-tive relevance feedback, where the number of relevancejudgements per query is generally low (e.g.
< 15).
(1) Our approach to the routing portion of our T IPSTER(2) work was based initially upon our existing relevance feed-(3) back mechanisms.
Routing profiles were constructed bya two step process.
The first step was to produce auto-(4) matically a query representing each T IPSTER topic, as(5) described in the Query Processing section above.
Thesecond step was to modify the query, using relevancefeedback.
This modified query was then used as a rout-(6) ing profile in the routing experiments.Node belief scores are calculated as a combination ofterm frequency (tf) and inverse document frequency(idf) weights.
The values are normalized to remain be-tween 0 and 1, and are further modified by tf and beliefdefault values which the user may define at program in-vocation.
Calculation of a belief or a given query opera-tor is dependent on the type of operator and the numberExperiments with the creating routing profiles showedthat better esults could be obtained by replacing the idfcomponent of the term selection algorithm with log ~,where df is the number of documents in which the termoccurs (document frequency) and tf is the frequency ofthe term in the collection.
The number of terms added tothe query was also increased, from 5 to 30.
This modified54#q051 =#WSUM( 1.000000 .433963 dougla 30.622835 sub-sid 14.105722 mcdonnel 2.856207 spain 22.664160 boe30.620134 european 5.776313 g.m.b.h.
8.629494 34014.828697 messerschmit 24.899202 industri 7.524240jet 28.518532 aerospac 6.187950 unfair 34.157051 air-craft 5.245394 construccion 5.942457 330 12.249618boelkow 5.435017 west 5.136472 franc 8.268916aerospatial 5.439325 aeronautica 6.971968 jetlin11.957228 blohm 9.611669 german 10.252533 mbb25.656782 consortium 16.704779 british 138.805618airbu 10.874762 plane 2.533194 plc 2.73149 #UWS(#company #foreigncountry ) 6.74627 #UWS0( 330airbu ) 6.36442 #UWS0( aid airbu ) 6.03555#UWS0( airbu messerschmit ) 8.87131 #UWS0( air-craft subsid ) 7.39724 #UW5( british aerospac )11.1438 #UW50( british airbu ) 3.45497 #UWS0(competitor airbu ) 6.27218 #UW50( cost airbu )20.7534 #UW50( european airbu ) 4.8756 #UW50(g.m.b.h, airbu ) 14.6286 #UWS0( german airbu )23.6137 #UW50( govern airbu ) 4.41921 #UW5(govern european ) p3.63681 #UW50( help airbu )4.04575 #UW5( mcdonnel boe ) 8.33751 #UW5( mc-donnel dougla ) 3.19623 #UWS( offic u.s. ) 8.1083#UW50( partner airbu ) 4.9825 #UW50( price airbu) 6.19649 #UW50( project airbu ) 10.3209 #UW50(say airbu ) 18.1742 #UW50( subsid airbu ) 15.8317#UW50( trade airbu ) 25.5183 #UW50( u.s. airbu) 5.23789 #UW5( u.s. trade ) 2.04795 #UW5(wall street ) 11.3886 #UW50( west airbu ) 6.19697#UW5( west german ) )Figure I l: Routing profile created automatically fromrelevant documents.algorithm appears effective ven with small numbers ofrelevance judgements.The addition of proximity operators further improves theaverage precision of routing profiles.
INQUERY consid-ers every pair of terms within a distance of n in a relevantdocument as a potential source of a proximity operatorto add to a query.
Experiments with values of n rangingfrom 3 to 50 showed that a range of values is superiorto any single value.
The resulting set of pairs, whichcan be quite large, is filtered to remove pairs that occurrarely in relevant documents.
The resulting set of pairsare ranked by the formula\[R\[ \[N RI " rg f\[R I is the number of relevant documents, ndf is thenumber of non-relevant documents in which the pair co-occur, and \[NR\[ is the number of non-relevant docu-ments, In our T IPSTER experiments, 10 proximity op-erators with n = 5, and 20 proximity operators withn : 50, were added to the query.
These operators wereintended to capture phrase-level and paragraph-level co-occur rence .The T IPSTER document collection differs from previ-ously available document collections in that it containsmany more documents and many more relevance judge-ments per query.
One might expect having more rele-vance judgements to improve the reliability of the statis-tics obtained by analyzing relevant documents, but it isnot clear that this is so.
Unofficial experiments showedthat INQUERY's performance improved steadily as thenumber of relevant documents used was increased toabout 275-300 documents.
After 300 relevant docu-ments, performance began to degrade slowly.
Furtherwork is required to understand this behavior.It can be argued that several hundred relevant docu-ments are a better epresentation f a user's interest hanthe query that retrieved them along with irrelevant doc-uments.
We found that better results were obtained bydiscarding the user's original query and creating a com-pletely new routing query using the relevance feedbackmethods described above.
Figure 11 shows a query cre-ated by this method.In addition to the number of relevance judgements, itis unusual to have relevance judgements from a diverseset of systems.
In an operational setting, even over longterm use, one is likely to only have relevance judgementsresulting from use with a single system.
We found thatrestricting INQUERY's attention to only those relevantdocuments that it retrieved reduced the number of rele-vance judgements to reach a given level of performance.Using relevant documents retrieved by many systems(e.g.
the TREC systems) eventually ielded similar per-formance, but required analysis of many more relevantdocuments.The routing experiments show that it is feasible to au-tomatically construct relatively accurate profiles in anoperational setting.
Profiles can be created from a set ofrelevant documents, or from repeated interaction with auser.
Either approach will yield relatively accurate rout-ing profiles.
The experiments also showed that, evenwhen large numbers of relevant documents are availablefor analysis, a combination of automatic query formationand manual query construction by a user is superior toeither approach alone.1 .4 .
Hardware /So f tware  Requ i rementsINQUERY was developed to run under the UNIX oper-ating system, on workstations manufactured by Digital55Equipment Corporation (DEC), and SUN Microsystems(SUN).
It has also been ported to the MS-DOS operatingsystem (with and without the Windows graphical userinterface) on personal computers containing the Intel 386and 486 microprocessors.
These hardware platforms in-clude 16, 32 and 64 bit architectures.
Ports to othercomputers and operating systems are not expected topresent any serious problems.The INQUERY system consists of several major sub-systems, as described in Section above.
The apph-cation programmer's interface (API) for document re-trieval applies to all INQUERY document collections,whether local or remote (available on another machineon a network).
An apphcation can be configured for aclient/server environment with just a few compile-timeand hnk-time decisions.
Chents can run on any of theplatforms mentioned above.
The server processes areconstrained currently to run on a UNIX workstation.The DecNet and TCP/ IP  networking protocols are bothsupported.
Additional network protocols can be addedeasily.Compile-time and link-time options are also available toenable a programmer to use only those modules neces-sary for a particular application.
For example, if a clientprogram will access only remote databases, the code foraccessing local databases can be eliminated, resulting ina substantially smaller executable program.The amount of memory and disk space required for doc-ument retrieval depends on the size of the documentcollection.
For a collection of N bytes, INQUERY re-quires about 5N bytes of disk space to build its docu-ment database.
Once the database is built, INQUERYrequires about 1.5N bytes of disk space to store the docu-ment database.
Memory requirements are more difficultto predict, because they depend upon the characteristicsof the document collection, the complexity of the queries,and the hardware characteristics of the computer.
ForUNIX workstations, a very rough estimate is that IN-QUERY requires about ~ bytes of virtual memory.
Areasonable amount of physical memory is ~0" Thereforea 2 gigabyte collection would need about 135 MB of vir-tual memory and about 32 MB of physical memory.
ForPCs running DOS, about ~ bytes of physical memoryis needed.Although INQUERY's appetite for memory and diskspace is not unreasonable when compared with compa-rable information retrieval systems, it can be reduced.Experiments have been conducted with an in-memoryapproach to document indexing that eliminates the needfor a separate sort of the indexing transactions.
The ad-vantages of this approach are its simphcity for the user,and a reduction of the peak disk space usage from about5N bytes to 1.9N bytes.
The disadvantage is that per-manent disk usage is increased from 1.5N bytes to 1.9Nbytes.
Experiments are also being conducted with a newapproach to document retrieval that will allow a user orsystem administrator to control the amount of memoryconsumed by INQUERY, essentially trading memory forresponse time.INQUERY is implemented in the ANSI standard ver-sion of the C programming language.
The f lex  andyacc programs are required to create the lexical scan-ners and parsers needed during document indexing andquery processing.
These scanners and parsers are pro-vided with each release of INQUERY, so f lex  and yaccare not needed except to customize the system.1 .5 .
Speed & ThroughputThe INQUERY system builds document collections au-tomatically at about 40-50 megabytes per CPU hour ona SUN SPARCserver 690 UNIX system with 128 MB ofphysical memory.
Speed varies with the size of the doc-ument collection, because transaction sorting takes timeproportional to nlog ft.On the same UNIX system, document retrieval takes anaverage of about 1 CPU second per query term on a 1gigabyte document collection.
The time varies widely,depending upon the frequency of the term in the collec-tion and the type of query language operators used.1 .6 .
Key  Innovat ions  o f  F ina l  Sys temThe following are the key innovations developed uringthe course of the T IPSTER project:1.
An inference net retrieval model for large, heteroge-neous databases..3.4.Query processing techniques that transform com-plex queries into INQUERY structures.Query expansion techniques using WORDFINDER.New techniques for handling noun phrases in theretrieval model.5.
Techniques for automatic onstruction of profiles forrouting.6.
Techniques for combining document and paragraph-level representations.7.
Techniques for combining queries.8.
A new stemming algorithm (KSTEM).569.
Techniques for integrating feature xtraction and in-dexing.10.
New approaches to word indexing.11.
Indexing and query processing techniques forJapanese.2.
Original P ro ject /System GoalsThe original goal of this project as described in the Uni-versity of Massachusetts proposal was to design and im-plement algorithms for document detection that wouldachieve significant improvements in retrieval effective-ness relative to conventional techniques.
We defined sig-nificant improvements as at least 10% increases in av-erage precision figures.
In addition, these algorithmsshould be portable, extensible, trainable, improvable,and robust.To achieve these goals, we decided to focus on algorithmsfor text representation, acquisition of information eeds,and retrieval, starting from a basis of statistical retrievaltechniques.
In particular, we emphasized the develop-ment of new text representation techniques based onnatural language processing, coupled with the develop-ment of a probabilistic retrieval model using Bayesianinference networks.We proposed to do work in the following areas:1.
Text Representation.
(a) Theories of text representation quality.
(b) Morphological processing - improving the in-dexing technology for English and providingbasic indexing for Japanese.
(c) Word senses - reducing ambiguity in text rep-resentation by identification of word senses inqueries and documents.
(d) Syntactic phrases - indexing by phrases formedusing syntactic riteria, augmented using clus-ter analysis.
(e) Statistical phrases - indexing by groups ofindex terms formed using statistical criteria,both in English and Japanese.2.
Retrieval Models.
(a) Theories of retrieval viewed as inference.
(b) Basic inference networks - implementing andevaluating retrieval and routing strategies inEnglish and Japanese.
(c) Combining multiple sources of evidence - usinginference networks to combine representationsproduced by the text representation research.3.
Acquisition of Information Needs.
(a) Query formulation - acquiring additional infor-mation to improve the  accuracy of queries inEnglish and Japanese.
(b) Relevance feedback - develol~ing feedback algo-rithms for inference networks, for both Englishand Japanese.
(c) Profile formation - investigating strategies foreffective long-term profile formation using rel-evance feedback.3.
Evolution of System Over 2 YearsOver the two years of the project, the project goals andthe main areas of work (text representation, retrievalmodels, and acquisition of information eeds) have notchanged.
The focus of some of the work in those areashas shifted somewhat.In the text representation area, considerable work hasbeen done on morphological processing for both Englishand Japanese.
In English, this work has been directed atimproving the basic indexing and stemming algorithms.In Japanese, character-based indexing has emerged as analternative to word-based indexing.
There has also beensubstantial work done on phrases.
The shift here hasbeen from parsing documents to locate phrases at indextime, to parsing queries to determine which phrases areimportant and then looking for evidence of the presenceof those phrases in documents.
Two major changes havebeen the lowering in priority of word sense disambigua-tion algorithms, and a different approach to clustering.Our initial experiments with word sense disambiguationwere not promising and for that reason we have delayedthe implementation of the algorithm until the end of theproject.
With regard to clustering, we are now group-ing nouns and noun phrases by the similarity of theircontexts in the test collections.
The system for conceptretrieval and comparison that results from this approachis WORDFINDER.The retrieval model area has been essentially carried outas planned.
Experiments evaluating retrieval and rout-ing have been done on schedule, and the idea of com-bining multiple sources of evidence has turned out to becentral both in our work, and in other aspects of theT IPSTER project.
One aspect of this that we haye paidparticular attention to is paragraph-based retrieval.In the area of information eed acquisition, we have car-ried out extensive xperiments with relevance feedback57and routing as planned.
In query formulation, we haveplaced the most emphasis on experiments with simplemanual modifications to automatically processed queriesand query expansion.4.
AccomplishmentsThe TIPSTER evaluations have demonstrated that theINQUERY approach to retrieval and routing is botheffective and efficient.
We have shown that the prob-abilistic framework is portable, trainable and improv-able.
The extensibility and robustness of this approachare further demonstrated in technology transfer effortsinvolving INQUERY.
Apart from these general accom-plishments, however, we can be more specific about thelessons that have been learned in the major areas ofwork.Indexing:?
Stopwords are sometimes necessary (e.g.
Ms. The,sit-in).
In order to reduce the number of queriesthat fail due to incomplete indexing, we have ex-tended the basic indexing algorithms to include in-dexing of stopwords when they are capitalized butnot at the start of sentences, and when they arejoined to other words.?
It is sometimes difficult to decide what is a word(e.g.
numbers, special characters).
We have carriedout a number of experiments o determine the mostflexible and efficient way of indexing word tokens.?
"Real" paragraph boundaries often do not indicatecontent shift in documents.
Our experiments withparagraph-based retrieval indicate that real para-graphs are no more effective than text windows, al-though there are collection-specific exceptions, uchas detecting Wall Street Journal articles that covermultiple short topics.?
Feature recognition can add significant overhead.We have done the first experiments on the impactof including simple extraction (e.g.
company names,dates, locations) in the indexing process.
We haveattempted to reduce the indexing overhead to a min-imum in order to support high-volume updates (e.g.routing).?
"Justified" stemming is hard (effective stems vs.understandable stems).
We have developed anew stemming algorithm that produces much moreunderstandable stems than the current standard(Porter).
The effectiveness of the new algorithmvaries across collections and we are continuing towork towards consistent improvements.Query processing:?
Sophisticated query processing produces ignificantimprovements.
We have developed a variety ofquery processing techniques that together have im-proved the overall system effectiveness considerably.?
Large queries resulted in less emphasis on word dis-ambiguation.
The T IPSTER topics are much longerthan typical IR queries and the mutual disambigua-tion produced by the presence of so many terms hasmade word sense disambiguation a marginal tech-nique.
We continue to study this technique withother collections.?
Large queries also makes query expansion difficult.Simple query expansion techniques, uch as using ageneral thesaurus, are not effective in this environ-ment.?
Automatic query expansion still looks promising.Query expansion based on the WORDFINDER sys-tem has produced the most significant results oftheir type to date.?
Extracting query terms/phrases from the narrativeis hard.
The abstract nature of the narrative sec-tion of the T IPSTER topics makes automatic pro-cessing difficult.
This is a promising area for futureresearch.?
Feature extraction/recognition is most effective innarrow domains.
Our experiments with includ-ing extraction in the indexing and retrieval pro-cess showed only small effectiveness improvementsin TIPSTER.
Our experience with other collectionshave shown more promise.?
Manual queries can improve results, but usuallyonly in combination with automatically processedqueries.
In general, we have shown that automati-cally processed queries are competitive with hand-crafted queries.Retrieval:?
Improvements depend heavily on baseline.
In gen-eral, it is easy to get large percentage improvementsif a baseline search with poor performance is used.The improvements obtained using INQUERY areoften small, but the overall effectiveness i  consis-tently better than other approaches.?
Phrases were not as effective as on other collec-tions.
Despite considerable efforts on developing thephrase model, the overall improvements obtained58using phrases are small (but consistent).
We arecontinuing to work on this issue.?
Estimation was a significant problem for large, het-erogeneous databases.
The estimation functionsused for previous, smaller test collections provedto be inadequate for TIPSTER.
We have developednew forms of these functions which have resulted insignificant improvements.?
Paragraph-level retrieval can help in combinationwith document-level retrieval.
In full-text collec-tions, the notion of a local match and a global matchis important.
We have shown that paragraph-level matching can produce significant improve-ments in effectiveness in two situations.
One is whenparagraph-level connections between query conceptsare specified manually, the other is when automaticparagraph-level matching is combined (in the IN-QUERY framework) with document-level matching.Routing:?
Automatic onstruction of routing profiles has con-sistcntly outperformed manually specified profiles inour experiments.
Combining these forms of profileresults in further improvements.?
Proximity pairs are important in the automaticprofile.
We compared simple word-based learn-ing with learning structure in the form of phraseand paragraph-level proximities and found that thestructured profiles perform better.?
Routing is different han relevance feedback.
Tech-niques that were superior in relevance feedback ex-periments (small amounts of training data), havenot been the best in routing experiments (largeamounts of training data).?
Amount of training data has significant, but limitedeffect on performance.
We have shown that goodperformance can be obtained with limited amountsof training data or relevance judgements.
This hasimportant implications for practical applications.Japanese:?
INQUERY works well with Japanese with minorchanges.
The only differences between the Japaneseand English versions of INQUERY are the modulesfor the morphological processing and the interface.The operators in the query language are identical,although there is some evidence that a Japanese-specific phrase operator may be more effective.Character-based indexing can be competitive withword-based indexing.
Indexing all characters ismuch faster than segmenting Japanese into words,and our retrieval experiments have shown the effec-tiveness levels to be similar.
The best performancewas obtained using combinations of both represen-tations.Query processing is at least as important inJapanese as in English.
Parsing a Japanese queryand constructing the appropriate INQUERY queryfrom that parse is a crucial part of getting effectiveperformance, particularly with character-based in-dexing.
Experiments with different approaches toquery processing are continuing as more topics areobtained.5.
Eva luat ion  SummaryIn this section we discuss the results obtained in the 24month evaluation of the T IPSTER project.Table 1 shows the results of the ad-hoc runs.
INQ009 isthe baseline result obtained using automatic query pro-cessing on the T IPSTER topics, excluding the Narrativefield.
INQ010 and INQ041 are the results obtained bymanually modifying the queries produced for INQ009.In the case of INQ010, the queries were modified byadding natural language structures from the narrativethat were judged to be important, and by deleting termsjudged to be not relevant.
For INQ041, the query mod-ifier was allowed to add any structure that was thoughtto be appropriate.
Examples of this type of structureare paragraph-level proximity constraints between queryterms.
The results show that manual modification haslittle effect on performance, with the main effect beingan increase in precision at low recall levels for INQ041.In previous evaluations, manual modifications resulted insignificant improvements.
The result reported here maybe due to the difficulty of the topics in the third set.
Theresults for INQ010 also indicate that using NLP tech-niques to analyze the Narrative section of a topic maynot improve the query.
The results labelled INQ015 andINQ01fi were for an early version of the WORDFINDERquery expansion system.
Although these results show nosignificant differences, other WORDFINDER results arepresente d later.Given that automatically processed queries had similarperformance to manually produced queries, the next ex-periment combined these two version of the informationneed using the INQUERY framework.
The result of thiscombination (INQ044) was significantly better than ei-ther of the individual sets of queries, and retained thehigh precision of the manual queries with the high recallof the automatic queries.59Recall INQ0090 82.710 60.020 54.330 48.440 44.050 38.060 33.070 27.480 21.490 14.5100 3.3avg 38.8Table 1: Ad-hoc results.INQO09 - baselineINQ010 - official manual (simulated NLP)INQ015 - Wordfinder 1INQ016 - Wordfinder 2INQ041 - Unofficial manualPrecision (% change) - 50 queriesINQ010 INQ015 INQ016 INQ04177.6 (-6.2)63.0 (+5.1)56.1 (+3.2)49.3 (+1.9)43.8 (-0.4)38.2 (+0.4)32.3 (-1.9)26.6 (-3.0)21.0 (-1.8)13.9 (-4.3)3.3 (+o.3)36.6 (-0.5)82.5 (-0.3)60.3 (+0.6)54.3 (-o.1)48.5 (+0.3)44.1 (+0.2)38.3 (+0.7)33.1 (+0.4)27.6 (+0.8)21.7 (+1.3)14.6 (+0.9)3.3 (-o.1)38.9 (+0.3)82.8 (+0.0)60.2 (+0.3)54.2 (-0.2)48.4 (+0.0)43.9 (-0.2)38.2 (+0.4),33.2 (+0.7)27.5 (+0.2)21.6 (+1.0)14.6 (+0.6)3.3 (-0.1)38.9 (+0.2)87.4 (+5.6)63.1 (+5.3)57.1 (+5.o)51.1 (+5.6)43.3 (-1.6)36.9 (-3.1)30.8 (-6.4)25.0 (-8.7)19.3 (-9.8)13.3 (-0.6)1.6 (-49.7)39.o (+0.5)Another set of experiments that were done with the ad-hoc queries was to combine the results of document-level representations with paragraph-level representa-tions.
This was done using the #WSUM operator in IN-QUERY.
A significant improvement in performance wasobtained when the paragraph-level results were weightedat 1/2 the importance of the document-level results.
Theperformance of the paragraph-level search on its own waspoor, and we are currently working on improving this.Table 4 shows the results of the routing experiments.INQ026 is the result of using the automatically processedversion of the original queries with no relevance feed-back.
INQ020 is the result of using simple techniquesfor reweighting terms and adding new terms (thirty wereadded) based on feedback from relevant documents inthe earlier databases.
It can be seen that these feedbacktechniques result in significant improvements.
INQ022shows the result of using the manually modified versionof the query (no relevance feedback), and INQ021 givesthe combination of the manual queries and the queriesproduced using simple relevance feedback.
Once againthe combination results in an improvement, althoughthis is small for this experiment due to the relatively poorperformance of the manual queries.
INQ023 and INQ024show the result of using more complex relevance feedbacktechniques in which proximity structures (paragraph andphrase level) were extracted from relevant documents aswell as simple terms.
Twenty paragraph-level proximi-ties, ten phrase-level proximities and thirty terms wereadded.
Both of these produced significant improvements.The best result (INQ024) was from a run where the orig-inal query was ignored and all terms came from relevantdocuments.Tables 5 and 6 show additional results usingWORDFINDER.
TipC and TipT were the results of us-ing topics 51-100 expanded using the best 5 conceptsfrom WORDFINDER.
These queries were run againstthe first T IPSTER disk.
For TipC, the query usedto search WORDFINDER was the concepts from eachtopic, whereas for TipT, it was the Description field.The results show substantial improvements, and is oneof the best results ever obtained for automatic query ex-pansion.
The results also show that even the relativelyshort Description field retrieved good concepts for ex-pansion.
These results were all obtained using a trainingset of 250,000 documents from WSJ, AP and Ziff to buildthe WORDFINDER database.
The results in T3T, T3Cand T5C were obtained using a smaller 50,000 documentcollection as the basis for WORDFINDER.
The results,although not as good as with the larger database, arestill significant.
It appears that a smaller text windowgives better performance.Table 7 gives an indication of the progress of the IN-QUERY system over the 2 year time frame of the TIP-STER project.
The baseline in this case has had all sys-60Table 2: Ad-hoc combination results.
INQ044 was a combination of baseline automatic (INQ009) and manual(INQ041).
Combination done using #SUM operator.Recall0 82.710 60.020 54.330 48.440 44.050 38.060 33.07O 27.480 21.490 14.5100 3.3avgPrecision (% change) - 50 queriesINQ009 INQ041 INQ04487.4 (+5.6)63.1 (+5.3)57.1 (+5.o)51.1 (+5.6)43.3 (-1.6)36.9 (-3.1)30.8 (-6.4)25.0 (-8.7)19.3 (-9.8)13.3 (-8.6)1.6 (-49.7)88.0 (+6.4)64.1 (+6.9)58.5 (+7.6)52.8 (+9.2)46.4 (+5.5)4o.8 (+7.2)35.7 (+8.3)29.5 (+7.6)23.5 (+10.0)16.0 (+10.2)3.5 (+8.3)38.8 39.0 (+0.5)  41.7 (+7.5)tem refinements from this period removed.
In general,these improvements are consistently better than 10%,but in the case where WORDFINDER is included, thereis nearly a 30% improvement.Table 8 shows that even small changes can still produceimprovements.
We have in general resisted tuning IN-QUERY to the T IPSTER collection.
We have, how-ever, had to change the probability estimation formulaeconsiderably to produce good results for a large, het-erogeneous database.
The improvement in Table 8 isthe result of running experiments on the earlier parts ofT IPSTER, and more improvements of this kind can beexpected.Tables 9 and 10 give the results of experiments on a smallJapanese database using 27 of our own queries and rele-vance judgements.
The results compare word-based andcharacter-based indexing with different forms of queryprocessing.
The results show that word-based indexingperforms well at low recall levels, but that averaged overall recall levels, it is significantly lower.
The best perfor-mance has been obtained using a combination of bothrepresentations (this result is not shown here).An overall summary of what has been learned over thetwo years of the T IPSTER project is given in the Accom-plishments ection.
Here, we will very briefly summarizethe major results:?
INQUERY has been shown to be an effective, flexi-ble and efficient retrieval and  routing engine.?
T IPSTER topics are very different to typical IRqueries, and new query processing techniques pro-duced good results.?
IR techniques, which are primarily based on wordsand statistics, produced very good results whenscaled up for the T IPSTER database and topics.?
For ad-hoc queries, automatic processing producesvery good results.
Manual modification can resultin significant benefits, but topics 101-150 were dif-ferent in that respect.?
Paragraph-based retrieval can help performance,but only when used in combination with document-based results.?
Automatic query expansion techniques producedencouraging results, but more work is needed to re-fine the techniques and understand the size of train-ing set needed.?
Automatic relevance feedback produces very goodresults for routing when there are large numbers ofrelevance judgements available.
Manual queries arenot competitive, but can help in combination.?
Combining document representations and queries,for which INQUERY was designed, can result invery significant performance improvements.
Thiswas also shown to be true for Japanese.Re ferences\[1\] J. P. Callan, W. B. Croft, and S. M. Harding.
TheINQUERY retrieval system.
In Proceedings of the61Table 3: Paragraph-based retrievalQueryid (Num): INQ009 INQ011(BASELINE) (DOC) (PAR)Total number of documents over all queriesRetrieved: 50000 50000Relevant: 11547 11547Rel_ret: 8651 7814Interpolated Recall - Precision Averages:INQ012(DOC+PAR/2)50000115478851at 0.00 0.8275 0.7067 0.8400at 0.10 0.5945 0.4722 0.6101at 0.20 0.5383 0.4218 0.5512at 0.30 0.4775 0.3673 0.4985at 0.40 0.4290 0.3170 0.4468at 0.50 0.3720 0.2659 0.3864at 0.60 0.3133 0.2055 0.3265at 0.70 0.2316 0.1393 0.2482at 0.80 0.1388 0.0758 0.1652at 0.90 0.0588 0.0293 0.0758at 1.00 0.0095 0.0046 0.0082Average precision (non-interpolated) over all rel does0.3480 0.2511 (-27.9) 0.3629 (+4.3)Third International Conference on Database and Ez-pert Systems Applications, pages 78-83, Valencia,Spain, 1992.
Springer-Verlag.\[2\] James P. Callan and W. Bruce Croft.
An evalua-tion of query processing strategies using the tipstercollection.
In Proceedings of the 16 th ACM SIGIRInternational Conference on Research and Develop-ment in Information Retrieval, pages 347-356.
ACM,June 1993.\[3\] Kenneth Church.
A stochastic parts program andnoun phrase parser for unrestricted text.
In Pro-ceedings of the 2nd Conference on Applied NaturalLanguage Processing, pages 136-143, 1988.\[4\] W. Bruce Croft and Howard R. Turtle.
Text retrievaland inference.
In P. Jacobs, editor, Tezt-Based Intel-ligent Systems, pages 127-156.
Lawrence Erlbaum,1992.\[5\] Hides Fujii and W. B. Croft.
A comparison of index-ing techniques for Japanese text retrieval.
In Proceed-ings of the A CM SIGIR Conference on Research andDevelopment in Information Retrieval, pages 237-246, 1993.\[6\] David Haines and W. B. Croft.
Relevance feedbackand inference networks.
In Proceedings of the A CMSIGIR Conference on Research and Development inInformation Retrieval, pages 2-11, 1993.\[7\] H. R. Turtle and W.  B. Croft.
Evaluation of an in-ference network-based retrieval model.
A CM Trans-actions on Information Systems, 9(3):187-222, 1991.\[8\] H. R. Turtle and W. B. Croft.
A comparison of textretrieval models.
Computer Journal, 1992.
To ap-pear.\[9\] Howard R. Turtle and W. Bruce Croft.
Efficientprobabilistic inference for text retrieval.
In RIAO$ Conference Proceedings, pages 644-661, Barcelona,Spain, April 1991.62Table 4: Routing results.INQ026 - Automatic basefineINQ020 - Relevance feedbackINQ021 - Relevance feedback + manual queryINQ022 - Manual queryINQ023 - Relevance feedback w/proxINQ024 - Relevance feedback w/prox and w/o original queryPrecision (% change) - 50 queriesINQ026 INQ020 INQ021 INQ022 INQ023 INQ024 Recall0102O3040506070809O100avg78.755.746.540.535.531.126.922.618.011.21.580.7 (+2.5)62.7 (+12.5)54.5 (+17.2)48.9 (+20.6)44.4 (+25.0)39.1 (+25.6)32.1 (+19.3)27.7 (+22.2)22.2 (+23.5)14.4 (+29.0)2.8 (+84.3)84.7 (+7.6)64.9 (+16.6)54.2 (+16.5)48.7 (+20.2)43.7 (+23.2)38.6 (+24.2)33.4 (+24.2)27.9 (+23.2)22.8 (+26.9)14.9 (+33.7)3.0 (+97.4)81.5 (+3.7)61.0 (+9.5)48.0 (+3.4)40.2 (-0.7)35.6 (+0.4)30.9 (-0.7)26.1 (-3.0)22.2 (-2.0)17.9 (-0.5)10.9 (-2.2)2.1 (+42.2)81.1 (+3.1)66.5 (+19.3)58.1 (+25.0)50.4 (+24.4)45.9 (+29.4)41.3 (+32.8)34.6 (+28.6)30.3 (+33.6)23.9 (+33.1)15.0 (+34.5)2.9 (+91.8)82.5 (+4.9)65.8 (+18.1)57.1 (+23.0)50.4 (+24.4)44.9 (+26.5)40.5 (+30.1)35.9 (+33.6)31.5 (+39.0)25.8 (+43.5)17.5 (+56.7)3.4 +127.2)33.5 39.0 (+16.6) 39.7 (+18.7) 34.2 (+2.3) 40.9 (+22.2) 41.4 (+23.7)Set-2:TipC:TipT:Recall0102030405060708090100avgTable 5: Wordfinder esults.Baseline using set 2 queries on firstTIPSTER database.Queries expanded using concepts to searchWORDFINDER, first 5 unique conceptsadded to query, WORDFINDER built using250,000 documents from WSJ, AP and Ziff.Queries expanded using topic anddescription.Precision (% change)set-2 TipC87.5 86.865.8 70.057,4 62.651.4 56.545.8 52.439,7 45.934,5 40.828,6 35.422.1 28.214,2 19.63,4 5.340,9 45.8(-0.8)(+6.5)(+9.1)(+10.0)(+14.4)(+15.4)(+18.5)(+23.7)(+27.8)(+38.1)(+57.6)- 50 queriesTipT87.9 (+0.5)68.9 (+4.8)61.5 (+7.2)56.0 (+9.1)51.0 (+11.4)46.3 (+16.5)41.0 (+19.1)35.4 (+24.0)29.3 (+32.7)19.3 (+35.7)4.8 (+41.9)(+11.8) 45.6 (+11.4)63Set-2:T3T:T3C:T5C:Table 6: Wordfinder esults.Baseline using set 2 queries on first T IPSTER databaseQueries expanded using topic and description to search WORDFINDER, first 5 unique conceptsadded to query, WORDFINDER built using 50,000 documents from WSJ, AP and Ziff, text" window of 3 sentences.Queries expanded using conceptsText window of 5 sentencesRecall set-20 87.5 84.9 (-3.0)10 65.8 67.9 (+3.2)20 57.4 59.1 (+3.1)30 51.4 55.1 (+7.3)40 45.8 50.6 (+10.6)50 39.7 45.5 (+14.5)60 34.5 40.1 (+16.3)70 28.6 34.6 (+21.0)80 22.1 28.3 (+28.0)90 14.2 19.4 (+36.2)100 3.4 4.6 (+36.5)avg 40.9Precision (% change) - 50 queriesT3T T3C T5C87.2 (-0.3)69.8 (+6.1)61.3 (+6.8)56.1 (+9.2)50.8 (+11.0)45.2 (+13.8)40.1 (+16.4)34.7 (+21.5)27.8 (+26.0)19.1 (+34.6)4.8 (+40.6)87.6 (+0.1)68.4 (+4.0)00.4 (+5.4)55.2 (+7.4)50.3 (+9.8)45.0 (+13.1)39.6 (+15.0)34.4 (+20.2)27.8 (+25.6)19.1 (+34.3)4.7 (+39.2)44.6 (+8.8)  45.2 (+10.4) 44.8 (+9.4)Table 7: Improvements during T IPSTER project.
Queries for topics 51-100, run against Volume 1 of the collection.Recall0 81.8I0 56.420 48.130 43.340 38.550 34.360 29.970 25.
I80 19.590 12.4100 2.5avgPrecision (% change) - 50 queriesset-2-basic TipC86.8 (+6.1)70.0 (+24.1)62.6 (+30.2)56.5 (+30.5)52.4 (+36.1)45.9 (+33.8)40.8 (+36.5)35.4 (+41.0)28.2 (+44.6)19.6 (+58.1)5.3 (+112.0)35.6 45.8 (+28.7)64Table 8: Optimizing estimation formulae.Recall INQ0090 82.710 60.020 54.330 48.440 44.050 38.060 33.070 27.480 21.490 14.5100 3.3avg 38.8Precision (% change) - 50 queriesINQ009a82.8 (+o.1)60.0 (+o.1)54.9 (+1.0)49.1 (+1.5)44.6 (+1.4)38.8 (+2.1)33.9 (+2.7)28.0 (+1.9)21.7 (+1.4)14.9 (+2.5)3.3 (+1.0)39.3 (+1.2)Table 9: Japanese results: Character-based indexing on a database of 1100 documents.NLQ(C)Short(C)Long(C)Joined(C)Character-based natural anguage queryCharacters combined at word levelCharacters combined at phrase levelCharacters combined at complex phrase levelRecall NLQ10 i 65.3 67.8 (+3.7)20 I 61.6 61.9 (+0.5)30 i 54.7 52.5 (-4.1)40 49.4 47.3 (-4.3)50 47.1 41.6 (-11.6)60 42.4 38.0 (-10.5)70 38.1 34.0 (-10.7)80 34.8 30.3 (-13.0)90 29.4 23.4 (-20.6)I00 15.5 15.0 (-3.2)avg 43.8Precision (% change) - 27 queriesShort (C) Long (C) Joined (C)71.3 (+9.1)62.1 (+0.8)54.3 (-0.9)47.6 (-3.7)41.8 (-11.3)37.2 (-12.4)33.2 (-12.8)29.8 (-14.5)24.8 (-15.7)14.5 (-6.5)71.3 (+9.1)66.0 (+7.2)56.7 (+3.6)51.5 (+4.2)47.9 (+1.8)42.9 (+I.0)38.3 (+0.4)34.8 (-0.0)29.4 (+0.0)15.5 (+o.o)41.2 (-6.1) 41.7 (-5.0) 45.4 (+3.6)65Table 10: Japanese results: Word-based indexing on a database of 1100 documents.NLQ(W):Long(W):Joined(W):Words (from JUMAN) in natural language queryWords combined at phrase levelWords combined at complex phrase levelRecall102030405060708090100avgPrecision (% change ) - 27 queriesNLQ(W) Long (W) Joined (W)75.467.453.244.839.336.531.028.220.913.175.6 (+0.3)67.5 (+0.1)55.6 (+4.5)44.8 (+0.1)38.1 (-3.1)34.1 (-6.4)30.1 (-2.8)26.7 (-5.5)20.1 (-3.7)12.8 (-1.9)77.7 (+3.1)68.5 (+1.6)54.7 (+2.9)46.7 (+4.3)40.7 (+3.6)36.9 (+1.3)31.3 (+1.1)28.2 (+0.0)20.9 (+0.0)13.1 (+0.0)41.0 40.5 (-1.0) 41.9 (+2.2)66INQUERY System OverviewJohn Broglio James P. CaUan W. Bruce CroftComputer  Science Depar tmentUniversity of MassachusettsAmherst ,  MA 01003-4610, USA{broglio , callan, croft}@cs.umass.edu1.
B ib l iographyN.J.
Belkin, C. Cool, W.B.
Croft and J.P.Callas.
"The Effect of Multiple Query Rep-resentations on Information Retrieval Sys-tem Performance.
Proceedings of the S i -teen& Annual International ACM SIGIRConference on Research and Development inInformation Retrieval, pages 339-346, 1993.J.
Broglio and W.B.
Croft.
Query Pro-cessing for Retrieval from Large Text Bases.DARPA Human Language Technology Work-shop, 1993.J.
Broglio, J. Callas, J. Chmaj, and W.B.Croft.
Information Retrieval in a CustomerSupport Situation.
Eleventh National Con-ference on Artificial Intelligence: AI in Ser-vice and Support - Bridging the Gap be-tween Research and Applications, pages 17-23, 1993.E.W.
Brown, J.P. Callas, W.B.
Croft, andJ.E.B.
Moss.
Supporting Full-Text Informa-tion Retrieval with a Persistent Object Store.EDBT, 1994J.P.
Callan and W.B.
Croft.
An Approach toIncorporating CBR Concepts in IR Systems.AAAI  Spring Symposium: Case-Based Rea-soning and Information Retrieval - Ezplor-ing the Opportunities for Technology Shar-ing, pages 28-34, 1993.J.P.
Callas and W.B.
Croft.
An Evalu-ation of Query Processing Strategies usingthe TIPSTER Collection.
Proceedings of theSizteenth Annual International ACM SIGIRConference on Research and Development inInformation Retrieval, pages 347-356, 1993.W.B.
Croft and M.H.
Utt.
Discovering Re-lationships by Feature Extraction from TextDatabases.
UMass Technical Report 93-61,1993.W.B.
Croft.
Knowledge-Based and Statisti-cal Approaches to Text Retrieval.
IEEE Ez-pert, pages 8-12, April 1993.H.
Fujii and W.B.
Croft.
A Comparisonof Indexing Techniques for Japanese TextRetrieval.
Proceedings of the Sizteenth An-nual International ACM SIGIR Conferenceon Research and Development  InformationRetrieval, pages 237-247, 1993.D.
Haines and W.B.
Croft.
Relevance Feed-back and Inference Networks.
Proceedings ofthe Sizteenth Annual International ACM SI-GIR Conference on Research and Develop-ment in Information Retrieval, pages 2-11,1993.R.
Krovetz.
Viewing Morphology as an In-ference Process.
Proceedings of the SizteenthAnnual International ACM SIGIR Confer-ence on Research and Development in Infor-mation Retrieval, pages 191-203, 1993.H.
Turtle and W.B.
Croft.
A Comparisonof Text Retrieval Models, Computer Journal35(3): pages 279-290, 1992.67
