Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 428?437,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsDecision Trees for Lexical Smoothing in Statistical MachineTranslationRabih Zbib?and Spyros Matsoukas and Richard Schwartz and John MakhoulBBN Technologies, 10 Moulton Street, Cambridge, MA 02138, USA?
Massachusetts Institute of Technology, 77 Massachusetts Ave., Cambridge, MA 02139, USAAbstractWe present a method for incorporat-ing arbitrary context-informed word at-tributes into statistical machine trans-lation by clustering attribute-qualifiedsource words, and smoothing theirword translation probabilities using bi-nary decision trees.
We describe twoways in which the decision trees areused in machine translation: by us-ing the attribute-qualified source wordclusters directly, or by using attribute-dependent lexical translation probabil-ities that are obtained from the trees,as a lexical smoothing feature in the de-coder model.
We present experimentsusing Arabic-to-English newswire data,and using Arabic diacritics and part-of-speech as source word attributes, andshow that the proposed method im-proves on a state-of-the-art translationsystem.1 IntroductionModern statistical machine translation (SMT)models, such as phrase-based SMT or hierar-chical SMT, implicitly incorporate source lan-guage context.
It has been shown, however,that such systems can still benefit from theexplicit addition of lexical, syntactic or otherkinds of context-informed word features (Vick-rey et al, 2005; Gimpel and Smith, 2008;Brunning et al, 2009; Devlin, 2009).
But thebenefit obtained from the addition of attributeinformation is in general countered by the in-crease in the model complexity, which in turnresults in a sparser translation model when es-timated from the same corpus of data.
Theincrease in model sparsity usually results in adeterioration of translation quality.In this paper, we present a method for usingarbitrary types of source-side context-informedword attributes, using binary decision trees todeal with the sparsity side-effect.
The deci-sion trees cluster attribute-dependent sourcewords by reducing the entropy of the lexi-cal translation probabilities.
We also presentanother method where, instead of clusteringthe attribute-dependent source words, the de-cision trees are used to interpolate attribute-dependent lexical translation probability mod-els, and use those probabilities to compute afeature in the decoder log-linear model.The experiments we present in this paperwere conducted on the translation of Arabic-to-English newswire data using a hierarchicalsystem based on (Shen et al, 2008), and usingArabic diacritics (see section 2.3) and part-of-speech (POS) as source word attributes.
Pre-vious work that attempts to use Arabic dia-critics in machine translation runs against thesparsity problem, and appears to lose most ofthe useful information contained in the dia-critics when using partial diacritization (Diabet al, 2007).
Using the methods proposedin this paper, we manage to obtain consistentimprovements from diacritics against a strongbaseline.
The methods we propose, though,are not restrictive to Arabic-to-English trans-lation.
The same techniques can also be usedwith other language pairs and arbitrary wordattribute types.
The attributes we use in thedescribed experiments are local; but long dis-tance features can also be used.In the next section, we review relevant pre-vious work in three areas: Lexical smoothingand lexical disambiguation techniques in ma-chine translation; using decision trees in nat-ural language processing, and especially ma-chine translation; and Arabic diacritics.
Wepresent a brief exposition of Arabic orthogra-428phy, and refer to previous work on automaticdiacritization of Arabic text.
Section 3 de-scribes the procedure for constructing the deci-sion trees, and the two methods for using themin machine translation.
In section 4 we de-scribe the experimental setup and present ex-perimental results.
Finally, section 5 concludesthe paper and discusses future directions.2 Previous Work2.1 Lexical Disambiguation andLexical SmoothingVarious ways have been proposed to improvethe lexical translation choices of SMT systems.These approaches typically incorporate localcontext information, either directly or indi-rectly.The use of Word Sense Disambiguation(WSD) has been proposed to enhance ma-chine translation by disambiguating the sourcewords (Cabezas and Resnick, 2005; Carpuatand Wu, 2007; Chan et al, 2007).
WSDusually requires that the training data be la-beled with senses, which might not be avail-able for many languages.
Also, WSD is tra-ditionally formulated as a classification prob-lem, and therefore does not naturally lend it-self to be integrated into the generative frame-work of machine translation.
Carpuat and Wu(2007) formulate the SMT lexical disambigua-tion problem as a WSD task.
Instead of learn-ing from word sense corpora, they use the SMTtraining data, and use local context features toenhance the lexical disambiguation of phrase-based SMT.Sarikaya et al (2007) incorporate contextmore directly by using POS tags on the targetside to model word context.
They augmentedthe target words with POS tags of the worditself and its surrounding words, and used theaugmented words in decoding and for languagemodel rescoring.
They reported gains on Iraqi-Arabic-to-English translation.Finally, using word-to-word context-free lex-ical translation probabilities has been shownto improve the performance of machine trans-lation systems, even those using much moresophisticated models.
This feature, usuallycalled lexical smoothing, has been used inphrase-based systems (Koehn et al, 2003).Och et al (2004) also found that includingIBM Model 1 (Brown et al, 1993) word prob-abilities in their log-linear model works betterthan most other higher-level syntactic featuresat improving the baseline.
The incorporationof context on the source or target side en-hances the gain obtained from lexical smooth-ing.
Gimpel and Smith (2008) proposed us-ing source-side lexical features in phrase-basedSMT by conditioning the phrase probabilitieson those features.
They used word context,syntactic features or positional features.
Thefeatures were added as components into thelog-linear decoder model, each with a tunableweight.
Devlin (2009) used context lexical fea-tures in a hierarchical SMT system, interpolat-ing lexical counts based on multiple contexts.It also used target-side lexical features.The work in the paper incorporates con-text information based on the reduction of thetranslation probability entropy.2.2 Decision TreesDecision trees have been used extensively invarious areas of machine learning, typicallyas a way to cluster patterns in order to im-prove classification (Duda et al, 2000).
Theyhave, for instance, been long used success-fully in speech recognition to cluster context-dependent phoneme model states (Young etal., 1994).Decision trees have also been used in ma-chine translation, although to a lesser extent.In this respect, our work is most similar to(Brunning et al, 2009), where the authors ex-tended word alignment models for IBM Model1 and Hidden Markov Model (HMM) align-ments.
They used decision trees to cluster thecontext-dependent source words.
Contexts be-longing to the same cluster were grouped to-gether during Expectation Maximization (EM)training, thus providing a more robust proba-bility estimate.
While Brunning et al (2009)used the source context clusters for word align-ments, we use the attribute-dependent sourcewords directly in decoding.
The approach wepropose can be readily used with any align-ment model.Stroppa et al (2007) presented a general-ization of phrase-based SMT (Koehn et al,2003) that also takes into account source-side context information.
They conditionedthe target phrase probability on the source429phrase as well as source phrase context, suchas bordering words, or part-of-speech of bor-dering words.
They built a decision tree foreach source phrase extracted from the train-ing data.
The branching of the tree nodeswas based on the different context features,branching on the most class-discriminative fea-tures first.
Each node is associated with theset of aligned target phrases and correspond-ing context-conditioned probabilities.
The de-cision tree thus smoothes the phrase probabil-ities based on the different features, allowingthe model to back off to less context, or nocontext at all depending on the presence ofthat context-dependent source phrase in thetraining data.
The model, however, did notprovide for a back-off mechanism if the phrasepair was not found in the extracted phrase ta-ble.
The method presented in this paper differsin various aspects.
We use context-dependentinformation at the source word level, ratherthan the phrase level, thus making it readilyapplicable to any translation model and notjust phrase-based translation.
By incorporat-ing context at the word level, we can decodedirectly with attribute-augmented source data(see section 3.2).2.3 Arabic DiacriticsSince an important part of the experimentsdescribed in this paper use diacritized Arabicsource, we present a brief description of Arabicorthography, and specifically diacritics.The Arabic script, like that of most otherSemitic languages, only represents consonantsand long vowels using letters1.
Short vowelscan be written as small marks written aboveor below the preceding consonant, called di-acritics.
The diacritics are, however, omit-ted from written text, except in special cases,thus creating an additional level of lexical am-biguity.
Readers can usually guess the cor-rect pronunciation of words in non-diacritizedtext from the sentence and discourse context.Grammatical case on nouns and adjectives arealso marked using diacritics at the end ofwords.
Arabic MT systems use undiacritizedtext, since most available Arabic data is undi-acritized.1Such writing systems are sometimes referred to asAbjads (See Daniels, Peter T., et al eds.
The World'sWriting Systems Oxford.
(1996), p.4.
)Automatic diacritization of Arabic has beendone with high accuracy, using various genera-tive and discriminative modeling techniques.For example, Ananthakrishnan et al (2005)used a generative model that incorporatesword level n-grams, sub-word level n-gramsand part-of-speech information to perform di-acritization.
Nelken and Shieber (2005) mod-eled the generative process of dropping dia-critics using weighted transducers, then usedViterbi decoding to find the most likely gener-ator.
Zitouni et al (2006) presented a methodbased on maximum entropy classifiers, us-ing features like character n-grams, word n-grams, POS and morphological segmentation.Habash and Rambow (2007) determined vari-ous morpho-syntactic features of the word us-ing SVM classifiers, then chose the correspond-ing diacritization.
The experiments in thispaper use the automatic diacritizer by SakhrSoftware.
The diacritizer determines word di-acritics through rule-based morphological andsyntactic analysis.
It outputs a diacritizationfor both the internal stem and case endingmarkers of the word, with an accuracy of 97%for stem diacritization and 91% for full dia-critization (i.e., including case endings).There has been work done on using dia-critics in Automatic Speech Recognition, e.g.
(Vergyri and Kirchhoff, 2004).
However, theonly previous work on using diacritization forMT is (Diab et al, 2007), which used the di-acritization system described in (Habash andRambow, 2007).
It investigated the effectof using full diacritization as well as partialdiacritization on MT results.
The authorsfound that using full diacritics deteriorates MTperformance.
They used partial diacritiza-tion schemes, such as diacritizing only passiveverbs, keeping the case endings diacritics, oronly gemination diacritics.
They also saw nogain in most configurations.
The authors ar-gued that the deterioration in performance iscaused by the increase in the size of the vo-cabulary, which in turn makes the translationmodel sparser; as well as by errors during theautomatic diacritization process.4303 Decision Trees for Source WordAttributes3.1 Growing the Decision TreeIn this section, we describe the procedurefor growing the decision trees using context-informed source word attributes.The attribute-qualified source-side of theparallel training data is first aligned to thetarget-side data.
If S is the set of attribute-dependent forms of source word s, and tj is atarget word aligned to si ?
S, then we define:p (tj |si) =count(si,tj)count(si)(1)where count(si, tj) is the count of alignmentlinks between si and tj .A separate binary decision tree is grown foreach source word.
We start by including all theattribute-dependent forms of the source wordat the root of the tree.
We split the set of at-tributes at each node into two child nodes, bychoosing the splitting that maximizes the re-duction in weighted entropy of the probabilitydistribution in (1).
In other words, at node n,we choose the partition (S?1 , S?2) such that:(S?1 , S?2) =argmax(S1,S2)S1?S2=S{h(S)?
(h(S1) + h(S2))}(2)where h(S) is the entropy of the probabil-ity distribution p(tj |si ?
S), weighted by thenumber of samples in the training data of thesource words in S. We only split a node if theentropy is reduced by more than a threshold?h.
This step is repeated recursively until thetree cannot be grown anymore.Weighting the entropy by the source wordcounts gives more weight to the context-dependent source words with a higher numberof samples in the training data, sine the lex-ical translation probability estimates for fre-quent words can be trusted better.
The ratio-nale behind the splitting criterion used is thatthe split that reduces the entropy of the lexicaltranslation probability distribution the mostis also the split that best separates the list offorms of the source word in terms of the targetword translation.
For a source word that hasmultiple meanings, depending on its context,the decision tree will tend to implicitly sepa-rate those meanings using the information inthe lexical translation probabilities.Although we describe this method as grow-ing one decision tree for each word, and usingone attribute type at a time, a decision treecan clearly be constructed for multiple words,and more than one attribute type can be usedin the same decision tree.3.2 Trees for Source Word ClusteringThe source words could be augmented to ex-plicitly incorporate the word attributes (dia-critics or other attribute types).
The aug-mented source will be less ambiguous if theattributes do in fact contain disambiguatinginformation.
This, in principle, helps machinetranslation performance.
The flip side is thatthe resulting increase in vocabulary size in-creases the translation model sparsity, usuallywith a detrimental effect on translation.To mitigate the effect of the increase in vo-cabulary, decision trees can be use to clusterthe attribute-augmented source words.
Morespecifically, a decision tree is grown for eachsource word as described in the previous sec-tion, using a predefined entropy threshold ?h.When the tree cannot be expanded anymore,its leaf nodes will contain a multi-set parti-tioning of the list of attribute-dependent formsof that source word.
Each of the clusters istreated as an equivalence class, and all formsin that class are mapped to a unique form (e.g.an arbitrarily chosen member of the cluster).The mappings are used to map the tokens inthe parallel training data before alignment isrun on the mapped data.
The test data isalso mapped consistently.
This clustering pro-cedure will only keep the attribute-dependentforms of the source words that decrease the un-certainty in the translation probabilities, andare thus useful for translation.The experiments we report on use diacriticsas an attribute type.
The various diacritizedforms of a source word are thus used to trainthe decision trees.
The resulting clusters areused to map the data into a subset of the vo-cabulary that is used in translation trainingand decoding (see section 4.2 for results).
Di-acritics are obviously specific to Arabic.
Butthis method can be used with other attributetypes, by first appending the source words with431{sijona,sijni}sjn{sijona,sijni,sajona,sajonu,sajana} {sajana}{sajona,sajonu}Figure 1: Decision tree for source word sjn usingdiacritics as an attribute.their context (e.g.
attach to each source wordits part-of-speech tag or context), and thentraining decision trees and mapping the sourceside of the data.Figure 1 shows an example of a decisiontree for the Arabic word sjn2using diacriticsas a source attribute.
The root contains thevarious diacritized forms (sijona `prison AC-CUSATIVE', sijoni `prison DATIVE', sajona`imprisonment ACCUSATIVE.
', sajoni `im-prisonment ACCUSATIVE.
', sajana `he im-prisoned ').
The leaf nodes contain theattribute-dependent clusters.3.3 Trees for Lexical SmoothingAs mentioned in section 2.1, lexical smoothing,computed from word-to-word translation prob-abilities, is a useful feature, even in SMT sys-tems that use sophisticated translation mod-els.
This is likely due to the robustness ofcontext-free word-to-word translation proba-bility estimates compared to the probabilitiesof more complicated models.
In those models,the rules and probabilities are estimated frommuch larger sample spaces.In our system, the lexical smoothing featureis computed as follows:f(U)=?tj?T (U)(1??si?{S(U)?NULL}(1?p?
(tj |si)))(3)where U is the modeling unit specific to thetranslation model used.
For a phrase-basedsystem, U is the phrase pair, and for a hierar-chical system U is the translation rule.
S (U)2Examples are written using Buckwalter transliter-ation.sjn{sijona,sijni,sajona,sajonu,sajana} {sajana}{sijona}{sijoni}{sajona}{sajonu}{sijona}{sijoni}Figure 2: Decision tree for source word sjn grownfully using diacritics.is the set of terminals on the source side of U,and T (U) is the set of terminals on its tar-get.
The NULL term in the equation aboveaccounts for unaligned target words, which wefound in our experiments to be beneficial.
Oneway of interpreting equation (3) is that f (U)is the probability that for each target word tjin U, tj is a likely translation of at least oneword si on the source side.
The feature valueis then used as a component in the log-linearmodel, with a tunable weight.In this work, we generalize the lexicalsmoothing feature to incorporate the sourceword attributes.
A tree is grown for eachsource word as described in section 3.1, butusing an entropy threshold ?h = 0.
In otherwords, the tree is grown all the way until eachleaf node contains one attribute-dependentform of the source word.
Each node in thetree contains a cluster of attribute-dependentforms of the source word, and a correspondingattribute-dependent lexical translation prob-ability distribution.
The lexical translationprobability models at the root nodes are thoseof the regular attribute-independent lexicaltranslation probabilities.
The models at theleaf nodes are the most fine-grained, since theyare conditioned on only one attribute value.Figure 2 shows a fully grown decision tree forthe same source word as the example in Figure1.The lexical probability distribution at theleafs are from sparser data than the originaldistributions, and are therefore less robust.
Toaddress this, the attribute-dependent lexical432smoothing feature is estimated by recursivelyinterpolating the lexical translation probabil-ities up the tree.
The probability distribu-tion pn at each node n is interpolated withthe probability of its parent node as follows:pn ={pn if n is root,wnpn + (1?
wn)pm otherwisewhere m is the parent of n(4)A fraction of the parent probability mass isthus given to the probability of the child node.If the probability estimate of an attribute-dependent form of a source word with a cer-tain target word t is not reliable, or if theprobability estimate is 0 (because the sourceword in this context is not aligned with t),then the model gracefully backs off by usingthe probability estimates from other attribute-dependent lexical translation probability mod-els of the source word.The interpolation weight is a logistic regres-sion function of the source word count at anode n:wn =11 + e????
log(count(Sn))(5)The weight varies depending on the countof the attribute-qualified source word in eachnode, thus reflecting the confidence in the es-timates of each node's distribution.
The twoglobal parameters of the function, a bias ?
anda scale ?
are tuned to maximize the likelihoodof a set of alignment counts from a heldoutdata set of 179K sentences.
The tuning is doneusing Powell's method (Brent, 1973).During decoding, we use the probability dis-tribution at the leaves to compute the featurevalue f(R) for each hierarchical rule R. Wetrain and decode using the regular, attribute-independent source.
The source word at-tributes are used in the decoder only to in-dex the interpolated probability distributionneeded to compute f (R).4 Experiments4.1 Experimental SetupAs mentioned before, the experiments we re-port on use a string-to-dependency-tree hier-archical translation system based on the modeldescribed in (Shen et al, 2008).
Forward andLikelihood %baseline -1.29 -Diacs.dec.
trees-1.25 +2.98%POS dec.trees-1.24 +3.41%Table 1: Normalized likelihood of the test set algn-ments without decision trees, then with decision treesusing diacritics and part-of-speech respectively.backward context-free lexical smoothing areused as decoder features in all the experiments.Other features such as rule probabilities anddependency tree language model (Shen et al,2008) are also used.
We use GIZA++ (Ochand Ney, 2003) for word alignments.
The de-coder model parameters are tuned using Mini-mum Error Rate training (Och, 2003) to max-imize the IBM BLEU score (Papineni et al,2002).For training the alignments, we use 27Mwords from the Sakhr Arabic-English Paral-lel Corpus (SSUSAC27).
The language modeluses 7B words from the English Gigaword andfrom data collected from the web.
A 3-gramlanguage model is used during decoding.
Thedecoder produces an N-best list that is re-ranked using a 5-gram language model.We tune and test on two separate data setsconsisting of documents from the following col-lections: the newswire portion of NIST MT04,MT05, MT06, and MT08 evaluation sets, theGALE Phase 1 (P1) and Phase 2 (P2) evalu-ation sets, and the GALE P2 and P3 develop-ment sets.
The tuning set contains 1994 sen-tences and the test set contains 3149 sentences.The average length of sentences is 36 words.Most of the documents in the two data setshave 4 reference translations, but some haveonly one.
The average number of referencetranslations per sentence is 3.94 for the tun-ing set and 3.67 for the test set.In the next section, we report on measure-ments of the likelihood of test data, and de-scribe the translation experiments in detail.4.2 ResultsIn order to assess whether the decision treesare in fact helpful in decreasing the uncer-tainty in the lexical translation probabilities43354.254.354.454.554.654.754.854.955MT Score  in BLEU5454.154.254.354.454.554.654.754.854.95502550100Entropy ThresholdFigure 3: BLEU scores of the clustering experimentsas a function of the entropy threshold on tuning set.on unseen data, we compute the likelihoodof the test data with respect to these prob-abilities with and without the decision treesplitting.
We align the test set with its ref-erence using GIZA++, and then obtain thelink count l_count(si, tj) for each alignmentlink i = (si,ti) in the set of alignment links I.We calculate the normalized likelihood of thealignments:L = log??
(?ip(ti | si)l_count(si,ti)) 1|I|?
?=1|I|?i?Il_count(si, ti) log p?
(ti | si) (6)where p?
(ti | si) is the probability for the wordpair (ti, si) in equation (4).
If the same in-stance of source word si is aligned to two tar-get words ti and tj , then these two links arecounted separately.
If a source in the test setis out-of-vocabulary, or if a word pair (ti, si)is aligned in the test alignment but not in thetraining alignments (and thus has no probabil-ity estimate), then it is ignored in the calcula-tion of the log-likelihood.Table 1 shows the likelihood for the baselinecase, where one lexical translation probabilitydistribution is used per source word.
It alsoshows the likelihoods calculated using the lex-ical distributions in the leaf nodes of the de-cision trees, when either diacritics or part-of-speech are used as an attribute type.
The tableshows an increase in the likelihood of 2.98% us-ing diacritics, and 3.41% using part-of-speech.The translation result tables present MTscores in two different metrics: TranslationEdit Rate (Snover et al, 2006) and IBMTER BLEUTestbaseline 40.14 52.05full diacritics 40.31 52.39+0.17 +0.34dec.
trees, diac (?h = 50) 39.75 52.60-0.39 +0.55Table 2: Results of experiments using decision treesto cluster source words.BLEU.
The reader is reminded that a higherBLEU score and a lower TER are desired.
Thetables also show the difference in scores be-tween the baseline and each experiment.
It isworth noting that the gains reported are rela-tive to a strong baseline that uses a state-of-the-art system with many features, and a fairlylarge training corpus.The decision tree clustering experiment asdescribed in section 3.2 depends on a globalparameter, namely the threshold in entropy re-duction ?h.
We tune this parameter manuallyon a tuning set.
Figure 3 shows the BLEUscores as a function of the threshold value, withdiacritics as an attribute type.
The most gainis obtained for an entropy threshold of 50.The fully diacritized data has an average of1.78 diacritized forms per source word.
The av-erage weighted by the number of occurrences is6.28, which indicates that words with more di-acritized forms tend to occur more frequently.After clustering using a value of ?h = 50,the average number of diacritized forms be-comes 1.11, and the occurrence weighted av-erage becomes 3.69.
The clustering proce-dure thus seems to eliminate most diacritizedforms, which likely do not contain helpful dis-ambiguating information.Table 2 lists the detailed results of experi-ments using diacritics.
In the first experiment,we show that using full diacritization results ina small gain on the BLEU score and no gain onTER, which is somewhat consistent with theresult obtained by Diab et al (2007).
The nextexperiment shows the results of clustering thediacritized source words using decision treesfor the entropy threshold of 50.
The TER lossof the full diacritics becomes a gain, and theBLEU gain increases.
This confirms our spec-ulation that the use of fully diacritized data in-434TER BLEUTestbaseline 40.14 52.05dec.
trees, diacs 39.75 52.55-0.39 +0.50dec.
trees, POS 40.05 52.40-0.09 +0.35dec.
trees, diacs, no interpolation 39.98 52.09-0.16 +0.04Table 3: Results of experiments using the word attribute-dependent lexical smoothing feature.creases the model sparsity, which undoes mostof the benefit obtained from the disambiguat-ing information that the diacritics contain.
Us-ing the decision trees to cluster the diacritizedsource data prunes diacritized forms that donot decrease the entropy of the lexical trans-lation probability distributions.
It thus findsa sweet-spot between the negative effect of in-creasing the vocabulary size and the positiveeffect of disambiguation.In our experiments, using diacritics withcase endings gave consistently better scorethan using diacritics with no case endings, de-spite the fact that they result in a higher vo-cabulary size.
One possible explanation is thatdiacritics not only help in lexical disambigua-tion, but they might also be indirectly help-ing in phrase reordering, since the diacritics onthe final letter indicate the word's grammaticalfunction.The results from using decision trees to in-terpolate attribute-dependent lexical smooth-ing features are summarized in table 3.
Inthe first experiment, we show the results ofusing diacritics to estimate the interpolatedlexical translation probabilities.
The resultsshow a gain of +0.5 BLEU points and 0.39TER points.
The gain is statistically signifi-cant with a 95% confidence level.
Using part-of-speech as an attribute gives a smaller, butstill statistically significant gain.
We also rana control experiment, where we used diacritic-dependent lexical translation probabilities ob-tained from the decision trees, but did not per-form the probability interpolation of equation(4).
The gains mostly disappear, especially onBLEU, showing the importance of the inter-polation step for the proper estimation of thelexical smoothing feature.5 Conclusion and Future DirectionsWe presented in this paper a new method forincorporating explicit context-informed wordattributes into SMT using binary decisiontrees.
We reported on experiments on Arabic-to-English translation using diacritized Ara-bic and part-of-speech as word attributes, andshowed that the use of these attributes in-creases the likelihood of source-target wordpairs of unseen data.
We proposed two spe-cific ways in which the results of the decisiontree training process are used in machine trans-lation, and showed that they result in bettertranslation results.For future work, we plan on using multi-ple source-side attributes at the same time.Different attributes could have different dis-ambiguating information, which could pro-vide more benefit than using any of the at-tributes alone.
We also plan on investigat-ing the use of multi-word trees; trees for wordclusters can for instance be grown insteadof growing a separate tree for each sourceword.
Although the experiments presentedin this paper use local word attributes, noth-ing in principle prevents this method from be-ing used with long-distance sentence context,or even with document-level or discourse-levelfeatures.
Our future plans include the investi-gation of using such features as well.AcknowledgmentThis work was supported by DARPA/IPTOContract No.
HR0011-06-C-0022 under theGALE program.The views, opinions, and/or findings con-tained in this article are those of the authorand should not be interpreted as representingthe official views or policies, either expressed435or implied, of the Defense Advanced ResearchProjects Agency or the Department of Defense.A pproved for Public Release, Distribution Un-limited.ReferencesS.
Ananthakrishnan, S. Narayanan, and S. Ban-galore.
2005.
Automatic diacritization of ara-bic transcripts for automatic speech recognition.Kanpur, India.R.
Brent.
1973.
Algorithms for MinimizationWithout Derivatives.
Prentice-Hall.P.
Brown, V. Della Pietra, S. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of sta-tistical machine translation: Parameter estima-tion.
Computational Linguistics, 19:263311.J.
Brunning, A. de Gispert, and W. Byrne.
2009.Context-dependent alignment models for statis-tical machine translation.
In NAACL '09: Pro-ceedings of the 2009 Human Language Technol-ogy Conference of the North American Chapterof the Association for Computational Linguis-tics, pages 110118.C.
Cabezas and P. Resnick.
2005.
Using WSDtechniques for lexical selection in statistical ma-chine translation.
In Technical report, Insti-tute for Advanced Computer Studies (CS-TR-4736, LAMP-TR-124, UMIACS-TR-2005-42),College Park, MD.M.
Carpuat and D. Wu.
2007.
Improving statis-tical machine translation using word sense dis-ambiguation.
In EMNLP-CoNLL-2007: Pro-ceedings of the 2007 Joint Conference on Em-pirical Methods in Natural Language Processingand Computational Natural Language Learning,Prague, Czech Republic.Y.
Chan, H. Ng, and D. Chiang.
2007.
Wordsense disambiguation improves statistical ma-chine translation.
In Proceedings of the 45thAnnual Meeting of the Association for Compu-tational Linguistics (ACL).J.
Devlin.
2009.
Lexical features for statisticalmachine translation.
Master's thesis, Universityof Maryland, December 2009.M.
Diab, M. Ghoneim, and N. Habash.
2007.
Ara-bic diacritization in the context of statistical ma-chine translation.
InMT Summit XI, pages 143149, Copenhagen, Denmark.R.
O. Duda, P. E. Hart, and D. G. Stork.
2000.Pattern Classification.
Wiley-Interscience Pub-lication.K.
Gimpel and N. A. Smith.
2008.
Rich source-side context for statistical machine translation.In StatMT '08: Proceedings of the Third Work-shop on Statistical Machine Translation, pages917, Columbus, Ohio.N.
Habash and O. Rambow.
2007.
Arabic diacriti-zation through full morphological tagging.
InProceedings of the 2007 Human Language Tech-nology Conference of the North American Chap-ter of the Association for Computational Lin-guistics, pages 5356, Rochester, New York.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statis-tical phrase-based translation.
In Proceedings ofthe 2003 Human Language Technology Confer-ence of the North American Chapter of the As-sociation for Computational Linguistics, pages4854, Edmonton, Canada.R.
Nelken and S. M. Shieber.
2005.
Arabic dia-critization using weighted finite-state transduc-ers.
In Proceedings of the 2005 ACL Workshopon Computational Approaches to Semitic Lan-guages, Ann Arbor, Michigan.F.
J. Och and H. Ney.
2003.
A systematic com-parison of various statistical alignment models.Computational Linguistics, 29(1):1951.F.
J. Och, D. Gildea, S. Khudanpur, A. Sarkar,K.
Yamada, A. Fraser, S. Kumar, L. Shen,D.
Smith, K. Eng, V. Jain, Z. Jin, and D. R.Radev.
2004.
A smorgasbord of features for sta-tistical machine translation.
In HLT-NAACL,pages 161168.F.
J. Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedingsof the 41st Annual Meeting of the Associationfor Computational Linguistics (ACL), Sapporo,Japan.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.2002.
Bleu: a method for automatic evalua-tion of machine translation.
In Proceedings ofthe 40th Annual Meeting of the Association forComputational Linguistics (ACL), Philadelphia,PA.Ruhi Sarikaya, Yonggang Deng, and Yuqing Gao.2007.
Context dependent word modeling for sta-tistical machine translation using part-of-speechtags.
In Proceedings of INTERSPEECH 2007fs,Antwerp, Belgium.L.
Shen, J. Xu, and R. Weischedel.
2008.
A newstring-to-dependency machine translation algo-rithm with a target dependency language model.In Proceedings of the 46th Annual Meeting ofthe Association for Computational Linguistics(ACL), Columbus, Ohio.M.
Snover, B. Dorr, R. Schwartz, J. Makhoul, andL.
Micciulla.
2006.
A study of translation error436rate with targeted human annotation.
In Pro-ceedings of the 7th Conf.
of the Association forMachine Translation in the Americas (AMTA2006), pages 223231, Cambridge, MA.N.
Stroppa, A. van den Bosch, and A Way.2007.
Exploiting source similarity for SMT us-ing context-informed features.
In Proceedings ofthe 11th International Conference on Theoreti-cal and Methodological Issues in Machine Trans-lation (TMI-07), pages 231240.D.
Vergyri and K. Kirchhoff.
2004.
Automaticdiacritization of arabic for acoustic modeling inspeech recognition.
In Semitic '04: Proceedingsof the Workshop on Computational Approachesto Arabic Script-based Languages, pages 6673,Geneva, Switzerland.D.
Vickrey, L. Biewald, M. Teyssier, and D. Koller.2005.
Word-sense disambiguation for machinetranslation.
In HLT '05: Proceedings of theconference on Human Language Technology andEmpirical Methods in Natural Language Pro-cessing, Vancouser, BC, Canada.S.J.
Young, J.J. Odell, and P.C.
Woodland.
1994.Tree-based state tying for high accuracy acousticmodelling.
In HLT'94: Proceedings of the Work-shop on Human Language Technology, pages307312.I.
Zitouni, J. S. Sorensen, and Ruhi Sarikaya.
2006.Maximum entropy based restoration of arabicdiacritics.
In Proceedings of the 21st Interna-tional Conference on Computational Linguisticsand the 44th annual meeting of the Associationfor Computational Linguistics, pages 577584,Sydney, Australia.437
