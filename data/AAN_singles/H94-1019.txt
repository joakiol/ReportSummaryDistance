AUTOMATIC EVALUATION OF COMPUTER GENERATED TEXT:A PROGRESS REPORT ON THE TEXTEVAL PROJECTChris Brew, Henry S. ThompsonHuman Communication Research CentreUniversity of Edinburgh2 Buccleuch Place, Edinburgh EH8 9LW, SCOTLANDhthompson@edinburgh.ac.ukABSTRACTWe present results of experiments designed to assess theusefulness of a new technique for the evaluation oftranslation quality, comparing human rankings withautomatic measures.
The basis of our approach isthe useof a standard set and the adoption of a statistical view oftranslation quality.
This approach as the ability toprovide evaluations which avoid dependence on anyparticular theory of translation, which are thereforepotentially more objective than previous techniques.
Thework presented here was supported by the Science andEngineering and the Social and Economic ResearchCouncils of Great Britain, and would not have beenpossible without he gracious assistance of Ian Mason ofHeriot Watt University, Edinburgh.INTRODUCTIONThe TextEval  project aims to explore anddevelop a new approach to the automaticeva luat ion  of computer -generated  texts,based on the use of  standard sets.We believethat  fast, accurate and automatic evaluationmethods are vital to the development of anylarge piece of natural  anguage software, andnote that  cur rent  methods,  which involveextensive intervention by human experts, aretoo cost ly to be a rout ine part  of thedevelopment cycle.
For pragmatic reasons weare working on translations, but in principlethe techniques would apply to any body ofsu i tab ly  comparab le  texts ,  such  asalternative versions of an instruction sheet.Our purpose in this p resentat ion  is todescr ibe a f ramework  for the automat icevaluat ion of t rans lat ion quality.
We wills tar t  by present ing  the resu l ts  of anevaluat ion experiment,  hen use the resultsof this to assess the usefulness of severalalternative automatic metrics.TRANSLAT ION QUAL ITY .A l though it is not c lear precisely whattranslators'  subjective judgements are basedon when they report  on the qual i ty  oftranslations, it is worthy of comment hatsuch judgements can be elicited at all, sinceattempts to prescribe detailed guidelines forthe scientific evaluation of translation qualityhave been problematic  since the ALPACreport \[1\].
It is arguable that any part icularset  of gu ide l ines  will inappropr ia te lyconstrain the eva luator  by imposing thetheoretical preconceptions of the author of theguidelines.
In the cur rent  project we areexplor ing the possib i l i ty  of p roduc ingautomat ic  evaluat ions wi thout  any priorcommitment  to a theory of t ranslat ion.Ourmethod of evaluat ion depends only on theessential ly statist ical  hypothesis that  goodtranslat ions will tend to be more similar toeach other than will bad ones.
Pre-requisitesof this approach are the avai labi l i ty of asuitable corpus of translations and the choiceof a s imi lar i ty  metric.
In this paper themetrics which we describe will be (negativelog) probabilities Strict ly this makes themdissimilarity metrics.Once we have establ ished a metric, we mayapply two approaches to the generation of arank ordering.
In the first approach we findan appropr iate method of combining thee lements  of a set of t ranslat ions,  thenmeasure  the distance of each individualtranslat ion from the composites formed bythe respective remainders  of the standardset.
Under  our hypothesis the better  thetranslat ion is the lower will be its distancefrom the composite formed by the remainingdata.
In  the second approach we start  bygenerat ing a pairwise distance matrix, thenuse multi-dimensional scaling \[3,4(p 498)\] toreduce the data to one dimension.
Thisproduces not only a l inear ordering over theitems tested but also a measure of the extentto which this l inear ordering captuures there lat ionsh ips  descr ibed by the d istancematr ix.We will in t roduce  our f ramework  bydescribing an experiment on the extraction of108translation quality judgements from humanbeings, along with the analysis whichdemonstrates  that  these judgements  doindeed reflect some objective reality about thetranslations.
We will then carry out a verysimilar analysis of results obtained fromautomatic metrics.HUMAN EXPERIMENTSSubjects were a class of final year translationstudents from an established translat ioncourse at a Scottish University.
We restrictedour attention to translations made by nativespeakers of English.
It can safely be assumedthat these students have a high level ofcompetence in both French and English.
InExper iment 1 they were asked to makejudgements about the quality of translationsprepared by others, while Experiment 2required them to assess the quality oftranslations produced by classmates.
Sincewe are taking a theory neutral approach, wedid not offer the subjects any guidelines fortheir judgements other than asking them toassess the quality of the translations.
Sincetranslation courses assume basic linguisticcompetence and concentrate their efforts onshowing translators how to preserve themessage of a text in translation \[5\],one mightexpect that  the basis on which theirjudgements are made might be on globalimpressions of the translation rather than onsmall details.
We took as our starting pointthe technique of Magnitude Estimation \[7\],long used in the social sciences for evaluativetasks where forced choice scoring is difficutltor inappropriate.
It is ideal for our purposesas it is robust, val idatable and orderinsensitive.Exper iment  1The f i rst  exper iment  made use oft rans lat ions which had previously beenelicited by electronic mail for use in a pilotstudy \[8\].
The volunteers who submittedt rans la t ions  di f fered cons iderably  inbackground and experience of translation,and there were concomitant substantialdifferences in the quality of the translationswhich they produced.
The original corpusconsisted of 44 translations of the same piece(a report on the opportunities and dangersprovided by Europe's peculiar position as amul t i l i ngua l  communi ty ) .
.
For theexperiment reported here we selected 10translations panning the quality range ofthe corpus as a whole.
From each of theset rans la t ions  we drew an ext rac tcorresponding to 111 words of French.
Thetranslations varied in length from 86 to 129words .Subjects were asked to respond in twomodalities, line production and numericalestimation.
The use of two modalities wasoriginally motivated by the requirements ofalternat ive analytical  method based onmagnitude stimation.
For present purposesit can be regarded as a somewhat elaboratemeans of eliciting two judgements of eachtranslation from each rater.Under both modalities the subject is asked tocompare a series of translat ions with areference translation which remains presentthroughout the sequence of tests.
In the caseof line production the reference translation isassociated with a pre-drawn line of aparticular length, and the subject is asked toindicate an assessment of the current targettranslation by producing a line which islonger or shorter than the line associatedwith the reference translation.
In the case ofnumerical  est imation the the referencetranslation is described by a number, and thesubject is asked to indicate their assessmentby producing a number whose ratio to thereference number best reflects the relativequality of the target ranslations.24 subjects took part.
The first part of thesession was taken up with a calibrationexercise designed to familiarize the subjectswith the responses required.
In the secondpart of the session we asked for judgementsof the 10 translations under both modalities,allowing 90 seconds for each judgement.Because of the limited time available, ratingwas paced by the experimenterANALYSISExper iment  1In order to establish whether  there isconsistency between the we use Kendall'scoefficient of concordance \[4 pp 454-456\].
Thisis a measure of the agreement in rankingbetween the raters, and is associated with aZ2 statistic for which a significance test isavailable.For both line production andnumerical estimation the results were highlysignificant.,,(For Line Production Kendall'sW = 0.3049%z(23) = 7,0,.121 p < 0.001) NumericEstimation Kendal ls W = 0.3263 %2(23) =10975.0.54 p < 0.001)indicating that there is ameasure of agreement between raters.
Itwould be a surprise if this were otherwise.We also carried out an experiment as abaseline for our hypothesis about similaritymetrics.The probabilistic distance metricwhich we used was the matched t-test \[4 pp287-:290\].
For each pair of translations weused the t-test to calculate the probability thatthese translations were rated the same by oursubjects, then transformed the probabilitiesinto negative log space to form the distancemetric.We then reduced this data to a singledimension using multidimensional scaling.We wished to assess the extent to which thel inear  sca le  produced by themul t id imens iona l  scal ing describes thevariability in the data.
For line production ther 2 value was 0.693, indicating that the singlel inear dimension accounts for this proportionof the variance in the pairwise distances.
Fornumer ica l  est imat ion the correspondingf igure was r 2 = .645.
Multidimensionalscal ing is essent ia l ly  an exploratorytechnique, therefore it is inappropriate toread too much into these figures, but they arelarge enough to suggest that there is someconnection between the input data and theresults of scaling.For numerica l  est imation the Spearmanrank correlation between the result producedby scaling and the original data was 0.95 andPearson's p was 0.96.
For line production thecorresponding statistics were Spearman p =0.41 Pearson p = 0.098, but these low figureswere entirely due to the fact that scaling hadassigned a very low score to a single veryhigh qua l i ty  t rans la t ion .
Our basichypothesis allows, and indeed encouragesthis, because all that the scaling processknows is that this translation is very differentfrom the others.
In supplying only inter-t rans lat ion distances we have effectivelysuppressed the in format ion that  thistranslation is better rather than worse thanthe general i ty of translations.
Once thisaberrant ranslation is ignored the statisticsbecome Spearman p=0.95 Pearson p=0.93,indicating the same good fit as in the lineproduction case.In general, whenever we apply the saclingtechnique we need to be alert to the possibilitythat outliers will be classified in the wrongextreme region of the scale.
For the purposesof evaluation of machine translations it isunlikely that this will be a major problem,since it is unlikely that any current systemwill produce results much better than thoseof the human beings in the standard set.
Onthe other hand, it is certainly possible that wewill encounter the converse problem, whichis that machine translations may be so bad asto cast doubt on the relavance of comparisonsbased on the differences between humantranslations, which are typically of muchhigher quality.Exper iment  2In Experiment 2 the same procedure wasfollowed as for Experiment 1,In both casesthe texts which were used were previouslyunseen by the raters, having been given as anexercise only to the other half of the subjectpool.
Since slightly more time was available,rating was self paced, although raters wereadvised to spend no more than two minuteson each translation.
The text was 127 wordsof French extracted from a report oneconomic prospects for Europe, with thelength of the translat ion extracts usedranging from 87 to 143 words.The standardset consisted of 14 translations, and the totalnumber of raters was also 14Exper iment  3In Experiment 3, which was carried out inthe same session as Experiment 2, the textwas 137 words of French, extracted from theannual report of a European initiative for thed isseminat ion of technical  information.When translated this produces between 115and 155 words of English.
In this experimentthere were 9 raters providing judgements ona set of 16 t rans lat ions .Analys is  forExperiments 2 and 3In experiments 2 and 3 we again foundprima facie evidence that the raters wereagreeing on something.
For experiment 2 thecoefficient of concordance was 0_.188 for 14subjects using line production (Z'2(12)= 31.61p < 0.01) and 0.206 (Zz(12) = 34.61 p < 0.001)using numerical estimation.For experiment3 ~he figure for line production was 0.145(Zz(8) = 18.5333 DF= 8 p < 0.Q25), and fornumerical estimation 0.25026 ZZ(8) = 32.0333p < 0.001).In experiment 2 mult idimensional scalingproduced r 2 values of 0.882 for l ineproduct ion and 0.906 for numer ica lest imation, while in exper iment  3 the110corresponding results were 0.791 for lineproduct ion  and 0.743 for numer ica lestimation.The correlations with the original input datawere all significant (p< 0.01) For the 14subjects of Experiment 2 line production gaveSpearman p = 0.88, Pearson p = 0.94, whilenumerical estimation gave Spearman p = -0.96 Pearson p = -0.98 (The negativecorrelation is not a problem, since multi-dimensional scaling involves an arbitrarychoice of direction for the linear scale).
Forthe 9 subjects of Experiment 3 line productiongave Spearman p = 0.85 Pearson p = 0.94while numerical estimation gave Spearmanp = 0.65 Pearson p = 0.88.
In the case ofnumer ica l  est imat ion mul t id imens iona lscaling has been slightly less successful inmatch ing  the or iginal  data,  but allcorrelations are still significant..Discuss ionThe experiments which we have carried outsuggest that when our subjects providedratings of translations they are achieving ameasure  of agreement ,and  thatmultidimensional scaling is indeed capableof recovering an appropriate l inear orderfrom a matr ix of probabilistic distancemeasures.AUTOMATIC METRICSIn this section we illustrate our approach tothe construction of automatically applicablemetrics.The first type of model is a simplemult inomia l .
In this model we focusexclusively on the frequency distribution ofthe words within a corpus.
Given twomul t inomia l  d istr ibut ions a techniquedescribed by Dunning \[2\] makes it possible tocalculate the log probability that the twodistr ibutions are drawn from the samemodel.
This is a distance metric analogous tothe t-test which we used in the  analysis ofhuman judgements.
In  one variant of ourtechnique, which we call the direct approach,we measure  the probabil ity that  eacht rans la t ion  is drawn from the samedistribution as a composite formed from theremainder of the standard set, while in theother variant we calculate pairwise distancesbetween each version, again using mult i -dimensional scaling to reduce the matrix to alinear order.
If the results of either of theseapproaches are a good match for humanperformance, then there is some suggestionthat  word population inf luences subjectjudgements.As an alternative to simply counting words,we have used the Xerox part-of-speech tagger\[6\] to assign part-of-speech tags to the wordsof the translations.
We can now apply thesame multinomial techniques as we did withwords.
What we are doing here is to collapseacross the equivalence classes which thetagger has identified.
This metric is ofinterest,  since if it matches  humanperformance better than does the word-basedmetric then there is some suggestion thatword-class stat ist ics inf luence subjectjudgements.\]The final multinomial model which we haveconsidered is again based upon informationwhich is available within the Xerox part-of-speech tagger, but instead of collapsingacross the parts of speech actually assignedwe use only information contained in thetagger's lexicon.This takes the form ofambiguity classes, which are statementsabout the sets of possible parts of speechwhich can in principle be assigned to aparticular lexical item.
In contrast to themetric based on tags, but like that based onwords, this metric is insensitive to actualway in which words are used in a giventranslation,but depends only on the wordsused.
The difference from the word-basedmetric is simply that we have used thetagger 's  lexicon to col lapse acrossequivalence classes of similar words.EXPERIMENTS WITH AUTOMATICMETRICSIn these exper iments we explored theusefulness of the three types of multinomialmodel , using both the direct approach (inwhich each translation is reduced to a vectorof counts, and each count compared againstthe aggregate counts for the rest of thecorpus), and the mult idimensional scalingapproach based on the reduction of pairwisecounts to a single linear scale.Exper iment  4In this experiment we used the 10 textswhich had previously been used forExperiment 1.
Under both modalities thedirect approach produced small negativecorrelations for all types of mult inomialmodel, but none of the correlations weresignificant.111Mult idimensional  scaling of the distancematrices produced by part-of-speech tagging,word counting and the counting of lexicalambiguity classes yielded the informationthat the proportion of variance accounted forin the reduction to a linear scale was r 2 = .492for part of speech tags, r2= .508 for words andr2= .473 for classes.Although t is indicatesthat there was some linear component to thescales induced by the distance metrics,correlation analysis revealed that for thistranslation at least none of these scalesseemed to mimic human performance.Exper iment  5Exper iment  5 used the 14 translationspreviously used in Experiment 2.
The directapproach again failed to reveal any metricwhich correlated significantly with humanperformance using either line production ornumerical estimation.In the scaling approach we found that thelinear reduction of the tag counts accountedfor a larger fraction of the variance in itsmatrix than did the ambiguity classes intheirs.
The linear reduction of the word countdistance accounted for so little of the variancethat the l inear scale must be viewed asworthless irrespective of any correlation withhuman rat ings  (Tags r 2 = .465,Classesr2=.258,Words r z = .083).For line production the linear scale based onpart of speech tags correlates ignificantlywith the human data (p= .4713 p =0.0495).
Theother two metrics did not show significantcorrelations.For numerical  estimation thesame pattern emerged , with only the metricbased on tags correlating significantly withhuman performance(p = -0.6159 p = 0.0362).Exper iment  6This used the 16 t rans la t ions  fromexperiment 3, again subjecting them to thethree multinomial distance metrics based ontags, classes and words.
In the directapproach, for line production only the ratingbased on tags was significantly correlated (p=-0.5679; p = 0.0109) while for numericalestimation both tags and ambiguity classesproduced significant results (For tags: p = -0.5555 ; p = 0.0029.For classes: p=0.2107 ; p =0.0477)The scaling approach showed the samepattern as experiment 5 The tag-based metricgenerated a linear scale which accounted for0.673 of the variance in the distance matrix,while ambiguity classes did substantial lyworse (r 2 = .288) and the linear scale derivedfrom words was essentially worthless (r2=.071).As in Experiment 4, neither the metricbased on ambiguity classes nor that based ontags produced significant correlations withthe human data, and while marginal lysignificant correlations did emerge for theword based metric, these have to bediscounted because of the extremely low r 2value produced in scaling.FURTHER WORKOnce we have access to numer ica linformation about human preferences wecan be more sophisticated about the way inwhich we form the composite standardagainst which each version is assessed.
Inpart icular ,  ra ther  than summing thefrequencies with which words, tags orclasses occur, we can generate a weightedcombination of the frequencies of theindividual elements,  where the weightsreflect the rat ings assigned by humanbeings.
Preliminary results indicate that theresults of the multinomial approach are notgreatly improved by this manipulation.There are other ways of moving beyondsimple multinomial models.
One is the use oflimited context to extend the mult inomialapproach to bigrams and beyond.
A second isto make direct use of the f requencyinformation encoded in the Xerox tagger toprovide ratings of translat ions.
A thirdapproach is to rad ica l ly  extend themult inomial approach to allow counts, ofarb i t rary text features,  including wordfrequencies, bigrams,and so on,then to selecta subset of the various features which closelymimics human judgements.
Given the smallsize of the available training corpus (a total ofunder 10,000 words) it will be necessary tokeep the number of degrees of freedom fairlysmall if over-training is to be avoided.In essence the mult inomia l  approachprovides a metric of texts analogous to thepower spectrum of a speech signal.
There isc lear ly room for metr ics  which areanalogous to phase information, i.e.
thosewhich take account of the connectionsbetween words.
Such measures, such as thedistance between successive occurrences ofthe same word, or that  between an ananaphor and its antecedent, are of particularinterest because students of translation are112taught  \[5\] to pay part icu lar  at tent ion to theproblem of ensur ing  that  the i r  t rans la t iondoes not appear  as a collection of apparent lyunre la ted  sentences.
It is an open quest ionwhether  they  in fact make  use of th ist ra in ing  in car ry ing  out  the judgementsdescribed here, but  if they do it should be thecase that  metr ics of textual  cohesion will be auseful  complement  to the essent ia l ly  lexicalmetr ics described here.We are  a lso pursu ing  the  approachintroduced in \[8\] of us ing str ing edit distanceas the basis for automat ic  eva luat ion ,  andhope to report  on this at the meeting.CONCLUSIONSThere is some evidence that  rat ings based onmul t inomia ls  are capable of captur ing  somehuman intu i t ions about t rans lat ion quality.A l though this varies from text to text, it doesappear  that  the part  of speech in format ionwhich can be obtained by automat ic  taggingrepresents  a promis ing  way of co l laps ingacross equ iva lence  c lasses of words.
By,contrast,  the resu l ts  of mu l t id imens iona lscal ing us ing word counts suggest hat  thereis too much i r re levant  in format ion in thesecounts to allow an automat ic  system to makemuch use of them in ra t ing  translat ions.Both the direct approach and that  us ingmul t id imens iona l  sca l ing  show somesuccess ,  a l though each fa i led on thet rans lat ion for which the other succeeded.REFERENCES1 ALPAC.
1966.
Language and Machines:Computers in Translation and Linguistics.
Areport by the Automatic Language ProcessingAdvisory Committee,Division of BehavioralSciences, National Research Council, NationalAcademy of Sciences, Washington D.C.2 Coxon A.P.
1982.
The User's Guide toMultidimensional Scaling, Exeter, NH, SagePublications.3 Dunning, Ted.
1993.
"Accurate Methods for theStatistics of Surprise and Coincidence"Computational Linguistics 19(3) pp 61-74, March,4 Hatch, E. and Lazaraton, A.
1991.
The ResearchManual: Design and Statistics for AppliedLinguistics Newbury House, London.5 Hatim, B. and Mason, I.
1991.
Discourse and theTranslator Longman, London.8Kupiec, J.
1992 "Robust part-of-speech taggingusing a hidden Markov model."
Computer Speechand Language, 6(3):225-242, July.Lodge, M. 1981.
Magnitude Scaling: QuantitativeMeasurement of Opinions.
Sage Publications,Beverley Hills, London.Thompson, H. S. 1991.
"Automatic Evaluation ofTranslation Quality: Outline of Methodology andReport on Pilot Experiment".
In K. Falkedal (ed)Report from the Evaluators Forum.
ISSCO,Geneva, to appear.113
