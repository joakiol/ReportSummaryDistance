PLSI Utilization for Automatic Thesaurus ConstructionMasato Hagiwara, Yasuhiro Ogawa, and Katsuhiko ToyamaGraduate School of Information Science, Nagoya University,Furo-cho, Chikusa-ku, Nagoya, JAPAN 464-8603{hagiwara, yasuhiro, toyama}@kl.i.is.nagoya-u.ac.jpAbstract.
When acquiring synonyms from large corpora, it is important to dealnot only with such surface information as the context of the words but also theirlatent semantics.
This paper describes how to utilize a latent semantic model PLSIto acquire synonyms automatically from large corpora.
PLSI has been shownto achieve a better performance than conventional methods such as tf?idf andLSI, making it applicable to automatic thesaurus construction.
Also, various PLSItechniques have been shown to be effective including: (1) use of Skew Divergenceas a distance/similarity measure; (2) removal of words with low frequencies, and(3) multiple executions of PLSI and integration of the results.1 IntroductionThesauri, dictionaries in which words are arranged according to meaning, are one of themost useful linguistic sources, having a broad range of applications, such as informationretrieval and natural language understanding.
Various thesauri have been constructedso far, including WordNet [6] and Bunruigoihyo [14].
Conventional thesauri, however,have largely been compiled by groups of language experts, making the constructionand maintenance cost very high.
It is also difficult to build a domain-specific thesaurusflexibly.
Thus it is necessary to construct thesauri automatically using computers.Many studies have been done for automatic thesaurus construction.
In doing so,synonym acquisition is one of the most important techniques, although a thesaurus gen-erally includes other relationships than synonyms (e.g., hypernyms and hyponyms).
Toacquire synonyms automatically, contextual features of words, such as co-occurrenceand modification are extracted from large corpora and often used.
Hindle [7], for ex-ample, extracted verb-noun relationships of subjects/objects and their predicates from acorpus and proposed a method to calculate similarity of two words based on their mu-tual information.
Although methods based on such raw co-occurrences are simple yeteffective, in a naive implementation some problems arise: namely, noises and sparse-ness.
Being a collection of raw linguistic data, a corpus generally contains meaninglessinformation, i.e., noises.
Also, co-occurrence data extracted from corpora are often verysparse, making them inappropriate for similarity calculation, which is also known as the?zero frequency problem.?
Therefore, not only surface information but also latent se-mantics should be considered when acquiring synonyms from large corpora.Several latent semantic models have been proposed so far, mainly for informationretrieval and document indexing.
The most commonly used and prominent ones are La-tent Semantic Indexing (LSI) [5] and Probabilistic LSI (PLSI) [8].
LSI is a geometricR.
Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
334?345, 2005.c?
Springer-Verlag Berlin Heidelberg 2005PLSI Utilization for Automatic Thesaurus Construction 335model based on the vector space model.
It utilizes singular value decomposition of theco-occurrence matrix, an operation similar to principal component analysis, to auto-matically extract major components that contribute to the indexing of documents.
It canalleviate the noise and sparseness problems by a dimensionality reduction operation,that is, by removing components with low contributions to the indexing.
However, themodel lacks firm, theoretical basis [9] and the optimality of inverse document frequency(idf) metric, which is commonly used to weight elements, has yet to be shown [13].On the contrary, PLSI, proposed by Hofmann [8], is a probabilistic version of LSI,where it is formalized that documents and terms co-occur through a latent variable.PLSI puts no assumptions on distributions of documents or terms, while LSI performsoptimal model fitting, assuming that documents and terms are under Gaussian distribu-tion [9].
Moreover, ad hoc weighting such as idf is not necessary for PLSI, although itis for LSI, and it is shown experimentally to outperform the former model [8].This study applies the PLSI model to the automatic acquisition of synonyms by es-timating each word?s latent meanings.
First, a number of verb-noun pairs were collectedfrom a large corpus using heuristic rules.
This operation is based on the assumption thatsemantically similar words share similar contexts, which was also employed in Hindle?swork [7] and has been shown to be considerably plausible.
Secondly, the co-occurrencesobtained in this way were fit into the PLSI model, and the probability distribution oflatent classes was calculated for each noun.
Finally, similarity for each pair of nounscan be calculated by measuring the distances or the similarity between two probabilitydistributions using an appropriate distance/similarity measure.
We then evaluated anddiscussed the results using two evaluation criteria, discrimination rates and scores.This paper also discusses basic techniques when applying PLSI to the automaticacquisition of synonyms.
In particular, the following are discussed from methodologicaland experimental views: (1) choice of distance/similarity measures between probabilitydistributions; (2) filtering words according to their frequencies of occurrence; and (3)multiple executions of PLSI and integration of the results.This paper is organized as follows: in Sect.
2 a brief explanation of the PLSI modeland calculation is provided, and Sect.
3 outlines our approach.
Sect.
4 shows the resultsof comparative experiments and basic techniques.
Sect.
5 concludes this paper.2 The PLSI ModelThis section provides a brief explanation of the PLSI model in information retrievalsettings.
The PLSI model, which is based on the aspect model, assumes that documentd and term w co-occur through latent class z, as shown in Fig.
1 (a).The co-occurrence probability of documents and terms is given by:P (d, w) = P (d)?zP (z|d)P (w|z).
(1)Note that this model can be equivalently rewritten asP (d, w) =?zP (z)P (d|z)P (w|z), (2)336 M. Hagiwara, Y. Ogawa, and K. Toyamad z wP(d) P(z|d) P(w|z)d z wP(z)P(d|z) P(w|z)(a) (b)Fig.
1.
PLSI model asymmetric (a) and symmetric (b) parameterizationcorpusco-occurrence(v, c, n)(eat, obj, lunch)(eat, obj, hamburger)(have, obj, breakfast)??
?PLSI modelz nP(v) P(z|v) P(n|z)latent classnoun(v,c)verb+case0.00.10.20.30.40.50.61 2 3 4 5 6 7 8 9 10lunch0 .00 .10 .20 .30 .40 .50 .61 2 3 4 5 6 7 8 9 10breakfast),( 21 wwsimlatent class distributionsimilarity calculationP(z|n)P(z|n)Fig.
2.
Outline of our approachwhose graphical model representation is shown in Fig.
1 (b).
This is a symmetric pa-rameterization with respect to documents and terms.
The latter parameterization is usedin the experiment section because of its simple implementation.Theoretically, probabilities P (d), P (z|d), P (w|z) are determined by maximumlikelihood estimation, that is, by maximizing the likelihood of document termco-occurrence:L =?d,wN(d, w) log P (d, w), (3)where N(d, w) is the frequency document d and term w co-occur.While the co-occurrence of document d and term w in the corpora can be observeddirectly, the contribution of latent class z cannot be directly seen in this model.
Forthe maximum likelihood estimation of this model, the EM algorithm [1], which is usedfor the estimation of systems with unobserved (latent) data, is used.
The EM algorithmperforms the estimation iteratively, similar to the steepest descent method.3 ApproachThe original PLSI model, as described above, deals with co-occurrences of documentsand terms, but it can also be applied to verbs and nouns in the corpora.
In this way, latentPLSI Utilization for Automatic Thesaurus Construction 337John gave presents to his colleagues.John    gave    presents    to    his    colleagues.NNPNPVBDVPNNS TO NNSNPPRP$PPNPS[John]    gave    [presents]    to    [his    colleagues]NP S VPNPVPVBDPPVPVBD NPTO PP(a) Original sentence(b) Parsing result(c) Dependency structure(d) Co-occurrence extraction from dependenciesJohn    gaveNP S VP(?give?, subj, ?John?
)gave   presentsVBDVP NP(?give?, obj, ?present?
)gave   to   his   colleaguesTO PP NP(?give?, ?to?, ?colleague?
)PPVPVBDnNP S VPv (v, subj, n)(v, obj, n)(v, prep, n)but (v, obj, n) when the verb is ?be?
+ past participle.nNP VP baseVPvnNP PPprepPP* VP baseVPvRule 1?Rule 2?Rule 3?
(e) Rules for co-occurrence identificationFig.
3.
Co-occurrence extractionclass distribution, which can be interpreted as latent ?meaning?
corresponding to eachnoun, is obtained.
Semantically similar words are then obtained accordingly, becausewords with similar meaning have similar distributions.
Fig.
2 outlines our approach,and the following subsections provide the details.3.1 Extraction of Co-occurrenceWe adopt triples (v, c, n) extracted from the corpora as co-occurrences fit into the PLSImodel, where v, c, and n represent a verb, case/preposition, and a noun, respectively.The relationships between nouns and verbs, expressed by c, include case relation (sub-ject and object) as well as what we call here ?prepositional relation,?
that is, a co-occurrence through a preposition.
Take the following sentence for example:John gave presents to his colleagues.First, the phrase structure (Fig.
3(b)) is obtained by parsing the original sentence(Fig.
3(a)).
The resulting tree is then used to derive the dependency structure (Fig.
3(c)),using Collins?
method [4].
Note that dependencies in baseNPs (i.e., noun phrases thatdo not contain NPs as their child constituents, shown as the groups of words enclosedby square brackets in Fig.
3(c)), are ignored.
Also, we introduced baseVPs, that is,sequences of verbs 1, modals (MD), or adverbs (RB), of which the last word must bea verb.
BaseVPs simplify the handling of sequences of verbs such as ?might not be?1 Ones expressed as VB, VBD, VBG, VBN, VBP, and VBZ by the Penn Treebank POS tag set[15].338 M. Hagiwara, Y. Ogawa, and K. Toyamaand ?is always complaining.?
The last word of a baseVP represents the entire baseVPto which it belongs.
That is, all the dependencies directed to words in a baseVP areredirected to the last verb of the baseVP.Finally, co-occurrences are extracted and identified by matching the dependencypatterns and the heuristic rules for extraction, which are all listed in Fig.
3 (e).
Forexample, since the label of the dependency ?John?
??gave?
is ?NP S VP?, the noun?John?
is identified as the subject of the verb ?gave?
(Fig.
3(d)).
Likewise, the de-pendencies ?presents???gave?
and ?his colleagues???to???gave?
are identified as averb-object relation and prepositional relation through ?to?.A simple experiment was conducted to test the effectiveness of this extractionmethod, using the corpus and the parser mentioned in the experiment section.
Co-occurrence extraction was performed for the 50 sentences randomly extracted fromthe corpus, and precision and recall turned out to be 88.6% and 78.1%, respectively.
Inthis context, precision is more important than recall because of the substantial size ofthe corpus, and some of the extraction errors result from parsing error caused by theparser, whose precision is claimed to be around 90% [2].
Therefore, we conclude thatthis method and its performance are sufficient for our purpose.3.2 Applying PLSI to Extracted Co-occurence DataWhile the PLSI model deals with dyadic data (d, w) of document d and term w, the co-occurrences obtained by our method are triples (v, c, n) of a verb v, a case/prepositionc, and a noun n. To convert these triples into dyadic data (pairs), verb v and case/preposition c are paired as (v, c) and considered a new ?virtual?
verb v. This enables itto handle the triples as the co-occurrence (v, n) of verb v and noun n to which the PLSImodel becomes applicable.
Pairing verb v and case/preposition c also has a benefit thatsuch phrasal verbs as ?look for?
or ?get to?
can be naturally treated as a single verb.After the application of PLSI, we obtain probabilities P (z), P (v|z), and P (n|z).Using Bayes theorem, we then obtain P (z|n), which corresponds to the latent classdistribution for each noun.
In other words, distribution P (z|n) represents the featuresof meaning possessed by noun n. Therefore, we can calculate the similarity betweennouns n1 and n2 by measuring the distance or similarity between the two correspond-ing distribution, P (z|n1) and P (z|n2), using an appropriate measure.
The choice ofmeasure affects the synonym acquisition results and experiments on comparison of dis-tance/similarity measures are detailed in Sect.
4.3.4 ExperimentsThis section includes the results of comparison experiments and those on the basic PLSItechniques.4.1 ConditionsThe automatic acquisition of synonyms was conducted according to the method de-scribed in Sect.
3, using WordBank (190,000 sentences, 5 million words) [3] as a cor-PLSI Utilization for Automatic Thesaurus Construction 339pus.
Charniak?s parser [2] was used for parsing and TreeTagger [16] for stemming.
Atotal of 702,879 co-occurrences was extracted by the method described in Sect.
3.1.When using EM algorithm to implement PLSI, overfitting, which aggravates theperformance of the resultant language model, occasionally occurs.
We employed thetempered EM (TEM) [8] algorithm, instead of a naive one, to avoid this problem.
TEMalgorithm is closely related to the deterministic annealing EM (DAEM) algorithm [17],and helps avoid local extrema by introducing inverse temperature ?.
The parameter wasset to ?
= 0.86, considering the results of the preliminary experiments.As the similarity/distance measure and frequency threshold tf , Skew Divergence(?
= 0.99) and tf = 15 were employed in the following experiments in response to theresults from the experiments described in Sects.
4.3 and 4.5.
Also, because estimationby EM algorithm is started from the random parameters and consequently the PLSIresults change every time it is executed, the average performance of the three executionswas recorded, except in Sect.
4.6.4.2 Measures for PerformanceThe following two measures, discrimination rate and scores, were employed for theevaluation of automated synonym acquisition.Discrimination rate Discrimination rate, originally proposed by Kojima et al [10], isthe rate (percentage) of pairs (w1, w2) whose degree of association between two wordsw1, w2 is successfully discriminated by the similarity derived by a method.
Kojimaet al dealt with three-level discrimination of a pair of words, that is, highly related(synonyms or nearly synonymous), moderately related (a certain degree of association),and unrelated (irrelevant).
However, we omitted the moderately related level and limitedthe discrimination to two-level: high or none, because of the high cost of preparing atest set that consists of moderately related pairs.The calculation of discrimination rate follows these steps: first, two test sets, oneof which consists of highly related word pairs and the other of unrelated ones, wereprepared, as shown in Fig.
4.
The similarity between w1 and w2 is then calculated foreach pair (w1, w2) in both test sets via the method under evaluation, and the pair islabeled highly related when similarity exceeds a given threshold t and unrelated whenthe similarity is lower than t. The number of pairs labeled highly related in the highlyrelated test set and unrelated in the unrelated test set are denoted na and nb, respectively.The discrimination rate is then given by:12(naNa+nbNb), (4)where Na and Nb are the numbers of pairs in highly related and unrelated test sets,respectively.
Since the discrimination rate changes depending on threshold t, maximumvalue is adopted by varying t.We created a highly related test set using the synonyms in WordNet [6].
Pairs in aunrelated test set were prepared by first choosing two words randomly and then con-firmed by hand whether the consisting two words are truly irrelevant.
The numbers ofpairs in the highly and unrelated test sets are 383 and 1,124, respectively.340 M. Hagiwara, Y. Ogawa, and K. Toyama(answer, reply)(phone, telephone)(sign, signal)(concern, worry)(animal, coffee)(him, technology)(track, vote)(path, youth)?
?highly related unrelatedFig.
4.
Test-sets for discrimination rate calcula-tionbase word: computerrank synonym sim sim?
rel.
(p) p ?
sim?1 equipment 0.6 0.3 B(0.5) 0.152 machine 0.4 0.2 A(1.0) 0.203 Internet 0.4 0.2 B(0.5) 0.104 spray 0.4 0.2 C(0.0) 0.005 PC 0.2 0.1 A(1.0) 0.10total 2.0 1.0 0.55Table 5.
Procedure for score calculationScores We propose a score which is similar to precision used for information retrievalevaluation, but different in that it considers the similarity of words.
This extension isbased on the notion that the more accurately the degrees of similarity are assigned tothe results of synonym acquisition, the higher the score values should be.Described in the following, along with Table 5, is the procedure for score calcula-tion.
Table 5 shows the obtained synonyms and their similarity with respect to the baseword ?computer.?
Results are obtained by calculating the similarity between the baseword and each noun, and ranking all the nouns in descending order of similarity sim.The highest five are used for calculations in this example.The range of similarity varies based on such factors as the employed distance/similarity measure, which unfavorably affects the score value.
To avoid this, the val-ues of similarity are normalized such that their sum equals one, as shown in the columnsim?
in Fig.
5.
Next, the relevance of each synonym to the base word is checked andevaluated manually, giving them three-level grades: highly related (A), moderately re-lated (B), and unrelated (C), and relevance scores p = 1.0, 0.5, 0.0 are assigned foreach grade, respectively (?rel.(p)?
column in Fig.
5).
Finally, each relevance score p ismultiplied by corresponding similarity sim?
, and the products (the p ?
sim?
columnin Fig.
5) are totaled and then multiplied by 100 to obtain a score, which is 55 in thiscase.
In actual experiments, thirty words chosen randomly were adopted as base words,and the average of the scores of all base words was employed.
Although this exampleconsiders only the top five words for simplicity, the top twenty words were used forevaluation in the following experiments.4.3 Distance/Similarity Measures of Probability DistributionThe choice of distance measure between two latent class distributions P (z|ni), P (z|nj)affects the performance of synonym acquisition.
Here we focus on the following sevendistance/similarity measures and compare their performance.?
Kullback-Leibler (KL) divergence [12]: KL(p || q) = ?x p(x) log(p(x)/q(x))?
Jensen-Shannon (JS) divergence [12]: JS(p, q) = {KL(p || m)+KL(q || m)}/2,m = (p + q)/2?
Skew Divergence [11]: s?
(p || q) = KL(p || ?q + (1 ?
?)p)?
Euclidean distance: euc(p, q) = ||p ?
q||?
L1 distance: L1(p, q) =?x |p(x) ?
q(x)|PLSI Utilization for Automatic Thesaurus Construction 341?
Inner product: p ?
q =?x p(x)q(x)?
Cosine: cos(p, q) = (p ?
q)/||p|| ?
||q||KL divergence is widely used for measuring the distance between two probabil-ity distributions.
However, it has such disadvantages as asymmetricity and zero fre-quency problem, that is, if there exists x such that p(x) = 0, q(x) = 0, the distance isnot defined.
JS divergence, in contrast, is considered the symmetrized KL divergenceand has some favorable properties: it is bounded [12] and does not cause the zero fre-quency problem.
Skew Divergence, which has recently been receiving attention, hasalso solved the zero frequency problem by introducing parameter ?
and mixing thetwo distributions.
It has shown that Skew Divergence achieves better performance thanthe other measures [11].
The other measures commonly used for calculation of thesimilarity/distance of two vectors, namely Euclidean distance, L1 distance (also calledManhattan Distance), inner product, and cosine, are also included for comparison.Notice that the first five measures are of distance (the more similar p and q, the lowervalue), whereas the others, inner product and cosine, are of similarity (the more similarp and q, the higher value).
We converted distance measure D to a similarity measuresim by the following expression:sim(p, q) = exp{?
?D(p, q)}, (5)inspired by Mochihashi and Matsumoto [13].
Parameter ?
was determined in such away that the average of sim doesn?t change with respect to D. Because KL divergenceand Skew Divergence are asymmetric, the average of both directions (e.g.
for KL diver-gence, 12 (KL(p||q) + KL(q||p))) is employed for the evaluation.Figure 6 shows the performance (discrimination rate and score) for each measure.
Itcan be seen that Skew Divergence with parameter ?
= 0.99 shows the highest perfor-mance of the seven, with a slight difference to JS divergence.
These results, along withseveral studies, also show the superiority of Skew Divergence.
In contrast, measures forvectors such as Euclidean distance achieved relatively poor performance compared tothose for probability distributions.4.4 Word Filtering by FrequenciesIt may be difficult to estimate the latent class distributions for words with low frequen-cies because of a lack of sufficient data.
These words can be noises that may degregatethe results of synonym acquisition.
Therefore, we consider removing such words withlow frequencies before the execution of PLSI improves the performance.
More specif-ically, we introduced threshold tf on the frequency, and removed nouns ni such that?j tfij < tf and verbs vj such that?i tfij < tf from the extracted co-occurrences.The discrimination rate change on varying threshold tf was measured and shownin Fig.
7 for d = 100, 200, and 300.
In every case, the rate increases with a moderateincrease of tf , which shows the effectiveness of the removal of low frequency words.We consequently fixed tf = 15 in other experiments, although this value may dependon the corpus size in use.342 M. Hagiwara, Y. Ogawa, and K. Toyama50.0%55.0%60.0%65.0%70.0%75.0%80.0%KLJS s(0.99)s(0.95)s(0.90)Euc.dist.L1dist.cosineinnerprod.distance/similarity measuredis crimina tionrate(%)5.07.09.011.013.015.017.019.021.023.025.0scoredisc.
ratescoreFig.
6.
Performances of distance/similaritymeasures70.0%71.0%72.0%73.0%74.0%75.0%76.0%77.0%78.0%0 5 10 15 20 25 30thresholddiscriminationrate(%)d=100d=200d=300Fig.
7.
Discrimination rate measured by varyingthreshold tf4.5 Comparison Experiments with Conventional MethodsHere the performances of PLSI and the following conventional methods are compared.In the following, N and M denote the numbers of nouns and verbs, respectively.?
tf: The number of co-occurrence tf ij of noun ni and verb vj is used directly forsimilarity calculation.
The corresponding vector ni to noun ni is given by:ni = t[tf i1 tfi2 ... tfiM ].
(6)?
tf?idf: The vectors given by tf method are weighted by idf.
That is,n?i =t[tf i1 ?
idf1 tf i2 ?
idf2 ... tf iM ?
idfM ], (7)where idfj is given byidfj =log(N/dfj)maxk log(N/dfk), (8)using dfj , the number of distinct nouns that co-occur with verb vj .?
tf+LSI: A co-occurrence matrix X is created using vectors ni defined by tf:X = [n1 n2 ... nN ], (9)to which LSI is applied.?
tf?idf+LSI : A co-occurrence matrix X?
is created using vectors n?i defined bytf?idf:X?
= [n?1 n?2 ... n?N ], (10)to which LSI is applied.?
Hindle?s method: The method described in [7] is used.
Whereas he deals onlywith subjects and objects as verb-noun co-occurrence, we used all the kinds ofco-occurrence mentioned in Sect.
3.1, including prepositional relations.PLSI Utilization for Automatic Thesaurus Construction 34360.0%62.0%64.0%66.0%68.0%70.0%72.0%74.0%76.0%78.0%number of latent classesdis crimina tionra te(%)PLSItf?idf+LSItf+LSItf?idftfHindletftf?idfHindle0 500 10005.07.09.011.013.015.017.019.021.023.0number of latent classesscoreHindletf?idftf0 500 1000Fig.
8.
Performances of PLSI and conventional methodsThe values of discrimination rate and scores are calculated for PLSI as well as themethods described above, and the results are shown in Fig.
8.
Because the number oflatent classes d must be given beforehand for PLSI and LSI, the performances of thelatent semantic models are measured varying d from 50 to 1,000 with a step of 50.
Thecosine measure is used for the similarity calculation of tf, tf?idf, tf+LSI, and tf?idf+LSI.The results reveal that the highest discrimination rate is achieved by PLSI, with thelatent class number of approximately 100, although LSI overtakes with an increase ofd.
As for the scores, the performance of PLSI stays on top for almost all the values ofd, strongly suggesting the superiority of PLSI over the conventional method, especiallywhen d is small, which is often.The performances of tf and tf+LSI, which are not weighted by idf, are consistentlylow regardless of the value of d. PLSI and LSI distinctly behave with respect to d,especially in the discrimination rate, whose cause require examination and discussion.4.6 Integration of PLSI ResultsIn maximum likelihood estimation by EM algorithm, the initial parameters are set tovalues chosen randomly, and likelihood is increased by an iterative process.
Therefore,the results are generally local extrema, not global, and they vary every execution, whichis unfavorable.
To solve this problem, we propose to execute PLSI several times andintegrate the results to obtain a single one.To achieve this, PLSI is executed several times for the same co-occurrence dataobtained via the method described in Sect.
3.1.
This yields N values of similaritysim1(ni, nj), ..., simN (ni, nj) for each noun pair (ni, nj).
These values are integratedusing one of the following four schemes to obtain a single value of similarity sim(ni, nj).?
arithmetic mean: sim(ni, nj) = 1N?Nk=1 simk(ni, nj)?
geometric mean:sim(ni, nj) = N?
?Nk=1 simk(ni, nj)?
maximum: sim(ni, nj) = maxk simk(ni, nj)?
minimum: sim(ni, nj) = mink simk(ni, nj)344 M. Hagiwara, Y. Ogawa, and K. Toyama70.0%71.0%72.0%73.0%74.0%75.0%76.0%77.0%78.0%dis crimina tionrate(%)15.017.019.021.023.025.0scoredisc .
ratescore1 2 3 arith.meangeo.meanmax minbefore integration after integrationFig.
9.
Integration result for N = 371.0%72.0%73.0%74.0%75.0%76.0%77.0%1 2 3 4 5 6 7 8 9 10Ndiscriminationrate(%)15.017.019.021.023.025.027.029.031.0scoreintegrated (disc.
score)maximum (disc.
score)average (disc.
rate)integrated (score)maximum (score)average (score)Fig.
10.
Integration results varying NIntegration results are shown in Fig.
9, where the three sets of performance on theleft are the results of single PLSI executions, i.e., before integration.
On the right arethe results after integration by the four schemes.
It can be observed that integrationimproves the performance.
More specifically, the results after integration are as good orbetter than any of the previous ones, except when using the minimum as a scheme.An additional experiment was conducted that varied N from 1 to 10 to confirm thatsuch performance improvement is always achieved by integration.
Results are shown inFig.
10, which includes the average and maximum of the N PLSI results (unintegrated)as well as the performance after integration using arithmetic average as the scheme.The results show that the integration consistently improves the performance for all 2 ?N ?
10.
An increase of the integration performance was observed for N ?
5, whereasincreases in the average and maximum of the unintegrated results were relatively low.It is also seen that using N > 5 has less effect for integration.5 ConclusionIn this study, automatic synonym acquisition was performed using a latent semanticmodel PLSI by estimating the latent class distribution for each noun.
For this purpose,co-occurrences of verbs and nouns extracted from a large corpus were utilized.
Discrim-ination rates and scores were used to evaluate the current method, and it was found thatPLSI outperformed such conventional methods as tf?idf and LSI.
These results makePLSI applicable for automatic thesaurus construction.
Moreover, the following tech-niques were found effective: (1) employing Skew Divergence as the distance/similaritymeasure between probability distributions; (2) removal of words with low frequencies,and (3) multiple executions of PLSI and integration of the results.As future work, the automatic extraction of the hierarchical relationship of wordsalso plays an important role in constructing thesauri, although only synonym relation-ships were extracted this time.
Many studies have been conducted for this purpose, butextracted hyponymy/hypernymy relations must be integrated in the synonym relationsto construct a single thesaurus based on tree structure.
The characteristics of the latentclass distributions obtained by the current method may also be used for this purpose.PLSI Utilization for Automatic Thesaurus Construction 345In this study, similarity was calculated only for nouns, but one for verbs can beobtained using an identical method.
This can be achieved by pairing noun n and case /preposition c of co-occurrence (v, c, n), not v and c as previously done, and executingPLSI for the dyadic data (v, (c, n)).
By doing this, the latent class distributions for eachverb v, and consequently the similarity between them, are obtained.Moreover, although this study only deals with verb-noun co-occurrences, other in-formation such as adjective-noun modifications or descriptions in dictionaries may beused and integrated.
This will be an effective way to improve the performance of auto-matically constructed thesauri.References1.
Bilmes, J.
1997.
A gentle tutorial on the EM algorithm and its application to parameterestimation for gaussian mixture and hidden markov models.
Technical Report ICSI-TR-97-021, International Computer Science Institute (ICSI), Berkeley, CA.2.
Charniak, E. 2000.
A maximum-entropy-inspired parser.
NAACL 1, 132?139.3.
Collins.
2002.
Collins Cobuild Major New Edition CD-ROM.
HarperCollins Publishers.4.
Collins, M. 1996.
A new statistical parser based on bigram lexical dependencies.
Proc.
of34th ACL, 184?191.5.
Deerwester, S., et al 1990.
Indexing by Latent Semantic Analysis.
Journal of the AmericanSociety for Information Science, 41(6):391?407.6.
Fellbaum, C. 1998.
WordNet: an electronic lexical database.
MIT Press.7.
Hindle, D. 1990.
Noun classification from predicate-argument structures.
Proc.
of the 28thAnnual Meeting of the ACL, 268?275.8.
Hofmann, T. 1999.
Probabilistic Latent Semantic Indexing.
Proc.
of the 22nd InternationalConference on Research and Development in Information Retrieval (SIGIR ?99), 50?57.9.
Hofmann, T. 2001.
Unsupervised Learning by Probabilistic Latent Semantic Analysis.
Ma-chine Learning, 42:177?196.10.
Kojima, K., et.
al.
2004.
Existence and Application of Common Threshold of the Degree ofAssociation.
Proc.
of the Forum on Information Technology (FIT2004) F-003.11.
Lee, L. 2001.
On the Effectiveness of the Skew Divergence for Statistical Language Analysis.Artificial Intelligence and Statistics 2001, 65?72.12.
Lin, J.
1991.
Divergence measures based on the shannon entropy.
IEEE Transactions onInformation Theory, 37(1):140?151.13.
Mochihashi, D., Matsumoto, Y.
2002.
Probabilistic Representation of Meanings.
IPSJ SIG-Notes Natural Language, 2002-NL-147:77?84.14.
The National Institute of Japanese Language.
2004.
Bunruigoihyo.
Dainippontosho.15.
Santorini, B.
1990.
Part-of-Speech Tagging Guidelines for the Penn Treebank Project.ftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gz16.
Schmid, H. 1994.
Probabilistic Part-of-Speech Tagging Using Decision Trees.
Proc.
of theFirst International Conference on New Methods in Natural Language Processing (NemLap-94), 44?49.17.
Ueda, N., Nakano, R. 1998.
Deterministic annealing EM algorithm.
Neural Networks,11:271?282.
