A Perspective on Word Sense Disambiguation Methodsand Their EvaluationPhilip ResnikDept.
of Linguistics/UMIACSUniversity of MarylandCollege Park, MD 20742resnik@umiacs, umd.
eduDavid YarowskyDept.
of Computer Science/CLSPJohns Hopkins UniversityBaltimore, MD 21218yarowsky@cs, jhu.
eduAbstractIn this position paper, we make severalobservations about the state of the art inautomatic word sense disambiguation.
Mo-tivated by these observations, we offer sev-eral specific proposals to the community re-garding improved evaluation criteria, com-mon training and testing resources, and thedefinition of sense inventories.1 IntroductionWord sense disambiguation (WSD) is perhaps thegreat open problem at the lexical level of naturallanguage processing.
If one requires part-of-speechtagging for some task, it is now possible to obtainhigh performance off the shelf; if one needs morpho-logical analysis, software and lexical data are not toohard to find.
In both cases, performance of state-of-the-art systems is respectable, if not perfect, andthe fundamentals of the dominant approaches (noisychannel models for tagging, two-level morphology)are by now well understood.
For word sense disam-biguation, we have a far longer way to go.2 Observat ionsObservation 1.
Evaluation of  word sense dis-ambiguation systems is not yet standardized.Evaluation of many natural language processingtasks including part-of-speech tagging and parsinghas become fairly standardized, with most reportedstudies using common training and testing resourcessuch as the Brown Corpus and Penn Treebank.
Per-formance measures include a fairly well recognizedsuite of metrics including crossing brackets and pre-cision/recall of non-terminal label placement.
Sev-eral researchers (including Charniak, Collins andMagerman) have facilitated contrastive evaluation oftheir parsers by even training and testing on identi-cal segments of the Treebank.
Government fundingagencies have accelerated this process, and even thetask of anaphora resolution has achieved an evalua-tion standard under the MUC-6 program.In contrast, most previous work in word sense dis-ambiguation has tended to use different sets of poly-semous words, different corpora and different eval-uation metrics.
Some clusters of studies have usedcommon test suites, most notably the 2094-word Hnedata of Leacock et al (1993), shared by Lehman(1994) and Mooney (1996) and evaluated on the sys-tem of Gale, Church and Yarowsky (1992).
Also, re-searchers have tended to keep their evaluation dataand procedures somewhat standard across their ownstudies for internally consistent comparison.
Never-theless, there are nearly as many test suites as thereare researchers in this field.Observation 2.
The  potential for WSD variesby task.
As Wilks and Stevenson (1996) empha-size, disambiguating word senses is not an end initself, but rather an intermediate capability that isbelieved -- but not yet proven -- to improve natu-ral language applications.
It would appear, however,that different major applications of language differ intheir potential to make use of successful word senseinformation.
In information retrieval, even perfectword sense information may be of only limited util-ity, largely owing to the implicit disambiguation thattakes place when,multiple words within a querymatch multiple words within a document (Krovetzand Croft, 1992).
In speech recognition, sense in-formation is potentially most relevant in the form ofword equivalence classes for smoothing in languagemodels, but smoothing based on equivalence classesof contexts (e.g.
(Bahl et al, 1983; Katz, 1987)) hasa far better track record than smoothing based Onclasses of words (e.g.
(Brown et al, 1992)).The potential for using word senses in machinetranslation seems rather more promising.
At thelevel of monolingual lexical information useful forhigh quality machine translation, for example, there79is good reason to associate information about syn-tactic realizations of verb meanings with verb sensesrather than verb tokens (Don' and Jones, 1996a;1996b).
And of course unlike machine translation orspeech recognition, the human process followed incompleting the task takes exp\]\]icit account of wordsenses, in that translators make use of correspon-dences in bilingual dictionaries organized accordingto word senses.Observation 3.
Adequately large sense-tagged data sets are difficult to obtain.
Avail-ability of data is a significant factor contributingto recent advances in part-of-speech tagging, pars-ing, etc.
For the most successful approaches to suchproblems, correctly annotated ata are crucial fortraining learning-based algorithms.
Regardless ofwhether or not learning is involved, the prev~illngevaluation methodology requires correct est sets inorder to rigorously assess the quality of algorithmsand compare their performance.Unfortunately, ofthe few sense-annotated corporacurrently available, virtually all are tagged collec-tions of a single ambiguous word such as line ortank.
The only broad-coverage annotation of all thewords in a subcorpus i the WordNet semantic on-cordance (Miller et ai., 1994).
This represents a veryimportant contribution to the field, providing thefirst large-scale, balanced ata set for the study ofthe distributional properties of polysemy in English.However, its utility as a tr~inlng and evaluation re-source for supervised sense taggers is currently some-what limited by its token-by-token sequential tag-ging methodology, ielding too few tagged instancesof the large majority of polysemous words (typicallyfewer than 10 each), rather than providing muchlarger training/testing sets for a selected subset ofthe vocabulary.
In addition, sequential ~nnotationforces annotators to repeatedly refamiliarize them-selves with the sense inventories of each word, slow-ing ~nnotation speed and lowering intra- and inter-annotator agreement rates.
Nevertheless, the Word-Net semantic hierarchy itself is a central trainingresource for a variety of sense disambiguation algo-rithms and the existence of a corpus tagged in thissense inventory is a very useful complementary e-source, even if small.The other major potential source of sense-taggeddata comes from parallel aligned bilingual corpora.Here, translation distinctions can provide a practi-cal correlate to sense distinctions, as when instancesof the English word duty translated to the Frenchwords devoir and droit correspond to the mono-lingual sense distinction between dUty/OBLIGATIONand duty/TAX.
Current offerings of parallel bilingualcorpora re limited, but as their availability and di-versity increase they offer the possibility of limitless'~agged" training data without he need for manualannotation.Given the data requirements for supervised learn-ing algorithms and the current paucity of such data,we believe that unsupervised and minimally super-vised methods offer the primary near-term hopefor broad-coverage s nse tagging.
However, we seestrong future potential for supervised algorithms us-ing many types of aligned bilingual corpora for manytypes of sense distinctions.Observation 4.
The field has narrowed downapproaches, but  only a little.
In the area of part-of-speech tagging, the noisy channel model domi-nates (e.g.
(Bald and Mercer, 1976; Jelinek, 1985;Church, 1988)), with transformational role-basedmethods (Brill, 1993) and grammatico-statistical hy-brids (e.g.
(Tapanainen and Voutilainen, 1994)) alsohaving a presence.
Regardless of which of these ap-proaches one takes, there seems to be consensus onwhat makes part-of-speech tagging successful:?
The inventory of tags is small and fairly standard.?
Context outside the current sentence has little in-fluence.?
The within-sentence d pendencies are very local.?
Prior (decontextuaUzed) probabilities dominate inmany cases.?
The task can generally be accomplished success-fully using only tag-level models without lexicalsensitivities besides the priors.?
Standard annotated corpora of adequate size havelong been available.Table 1: Some properties of the POS tagging task.In contrast, approaches to WSD attempt o takeadvantage of many different sources of information(e.g.
see (McRoy, 1992; Ng and Lee, 1996; Bruceand Wiebe, 1994)); it seems possible to obtain ben-efit from sources ranging from local collocationalclues (Yarowsky, 1993) to membership in semanti-cally or topically related word classes (?arowsky,1992; Resnik, 1993) to consistency of word usageswithin a discourse (Gale et al, 1992); and disam-bignation seems highly lexically sensitive, in effectrequiring specialized isamhignators for each poly-semous word.803 Proposa lsProposal 1.
A better evaluation criterion.
Atpresent, the standard for evaluation of word sensedisambiguation algorithms is the "exact match" cri-terion, or simple accuracy:% correct = 100 x # exactly matched sense tags# assigned sense tagsDespite its appealing simplicity, this criterion suf-fers some obvious drawbacks.
For example, considerthe context:... bought an interest in Lydak Corp .... (1)and assume the existence of 4 hypothetical systemsthat assign the probability distribution in Table 2 tothe 4 major senses of interest.Sense(1) monetary (e.g.
on a loan)(2) stake or share ,tffi correct(3) benefit/advantage/sake(4) intellectual curiositySystem1 2 3 4.47 .85 .28 1.00.42 .05 .24 .00.06 .05 .24 .00.05 .05 .24 .00Table 2: Probability distributions assigned by fourhypothetical systems to the example context (1)above.Each of the systems assigns the incorrect classifi-cation (sense 1) given the correct sense 2 Ca stake orshare).
However System 1 has been able to nearlyrule out senses 3 and 4 and assigns reasonably highprobability to the correct sense, but is given thesame penalty as other systems that either have ruledout the correct sense (systems 2 and 4) or effectivelyclaim ignorance (system 3).If we intend to use the output of the sense tag-ger as input to another probabilistic system, suchas a speech recognizer, topic classifier or IR system,it is important hat the sense tagger yield proba-bilities with its classifications that are as accurateand robust as possible.
If the tagger is confidentin its answer, it should assign high probability toits chosen classification.
If it is less confident, buthas effectively ruled out several options, the assignedprobability distribution should reflect his too.A solution to this problem comes from the speechcommunity, where cross-entropy (or its related mea-sures perplexity and Kullback-Leibler distance) areused to evaluate how well a model assigns probabil-ities to its predictions.
The easily computable for-mula for cross entropy isN-- N i~-~1 log2 Pr~4 (ca, \[wi, c?ntext')where N is the number of test instances and Pr~t isthe probability assigned by the algorithm A to thecorrect sense, c.sl of polysemous word wi in contexti.Crucially, given the hypothetical case above, thesense disambiguation algorithm in System 1 wouldget much of the credit for assigning high probabil-ity, even if not the highest probability, to the correctsense.
Just as crucially, an algorithm would be pe-nalized heavily for assigning very low probability tothe correct sense, I as illustrated below:Illustration of Cross- SystemEntropy Calculation 1 2 3 4PrA(csi\[wl, contexti) .42 .05 .24 .00-log2Pr.4(cs~lw~,context~ ) 1.25 4.32 2.05 ovIn aggregate, optimal performance is achieved un-der this measure by systems that assign as accuratea probability estimate as possible to their classifica-tions, neither too conservative (System 3) nor toooverconfident (Systems 2 and 4).This evaluation measure does not necessarily ob-viate the exact match criterion, and the two couldbe used in conjunction with each other since theymake use of the same test data.
However, a measurebased on cross-entropy or perplexity would providea fairer test, especially for the common case whereseveral fine-grained senses may be correct and it isnearly impossible to select exactly the sense chosenby the human annotator.Finally, not all classification algorithms returnprobability values.
For these systems, and for thosethat yield poorly estimated values, a variant ofthe cross entropy measure without the log term(~ ~'\]~I Pr-a(csdwi, contexti)) can be used to mea-sure improvement in restricting and/or roughly or-dering the possible classification set without exces-sive penalties for poor or absent probability esti-mates.
In the latter case, when the assigned tag isgiven probability 1 and all other senses probability0, this measure is equivalent to simple % correct.Proposa l  2.
Make  eva luat ion  sens i t ive  tosenmnt ic /conmaun lcat ive  d is tance betweensubsenses .Current WSD evaluation metrics also fail to takeinto account semantic/communicative distance be-tween senses when assigning penalties for incorrectlabels.
This is most evident when word senses arenested or arranged hierarchically, as shown in theexample sense inventory for bank in Table 3.1The extreme case of assigning 0 probability to thecorrect sense is given a penalty of oo by the cross-entropymeasure.81I Bank - REPOSITORY1.1 Financial BankI.la - the institutionI.lb - the bnllding1.2 General Supply/Reserve/InventoryII Bank - GEOGRAPHICALII.1 ShorelineII.2 Ridge/EmbanlcmentIII Bank - ARRAY/GROUP/ROWTable 3: Example sense inventory for bankAn erroneous classification between close siblingsin the sense hierarchy should be given relatively littlepenalty, while misclassifications across homographsshould receive a much greater penalty.
The penaltymatrix distance (subsensel, subsense2) could capturesimple hierarchical distance (e.g.
(Resnik, 1995;Richardson et al, 1994)), derived from a single se-mantic hierarchy such as WordNet, or be based ona weighted average of simple hierarchical distancesfrom multiple sources uch as sense/subsense hi rar-chies in several dictionaries.
A very simple exampleof such a distance matrix for the bank sense hierarchyis given in Table 4.i I. la I.lb 1.2 II.l II.2 IIII .
la  0 1 2 4 4 4Lib 1 0 2 4 4 41.2 2 2 0 4 4 4II.1 4 4 4 0 1 4II.2 4 4 4 1 0 4III 4 4 :t 4 4 0Table 4: Example distance/cost matrix for bankPenalties could also be based on general pairwisef~nctional communicative distance: errors betweensubtle sense differences would receive little penaltywhile gross errors likely to result in misunderstand-ing would receive a large penalty.
Such communica-tive distance matrices could be derived from severalsources.
They could be based on psycholinguisticdata, such as experimentally derived estimates ofsimilarity or confusability (Miller and Charles, 1991;Resnik, 1995).
They could be based on a given task,e.g.
in speech synthesis only those sense distinctionerrors corresponding to pronunciation distinctions(e.g.
bass-/bms/vs, bass-/beIs/) would be penal-ized.
For the machine-translation application, onlythose sense differences lexicalized ifferently in thetarget language would he penalized, with the penaltyproportional to communicative distance.
2 In gen-2Such distance could be based on the weighted % ofall languages that lexicalize the two subsenses differently.eral such a distance matrix could support arbitrarycommunicative cost/penalty functions, dynamicallychangible according to task.There are several ways in which such a (hierar-chical) distance penalty weighting could be utilizedalong with the cross-entropy measure.
The simplestis to minimize the mean distance/cost between theassigned sense (as~) and correct sense (csl) over allN examples as an independent figure of merit:1 Ndistance(csi, asi)However, one could also use a metric such as thefollowing that measures efficacy of probability as-signment in a manner that penalizes probabilitiesassigned to incorrect senses weighted by the commu-nicative distance/cost between that incorrect senseand the correct one:~1 distance(cs,, sj) x Pr~(sjlw,,context,)i-- j=where for any test example i, we consider all Sisenses (sj) of word wi, weighting the probabilitymass assigned by the classifier ,4 to incorrect senses(PrA(sjlwi,context~)) by the communicative dis-tance or cost of that misclassification.
3Note that in the special case of sense tagging with-out probability estimates (all are either 0 or 1), thisformula is equivalent o the previous one (simplemean distance or cost mlnlmlzation).Proposal  3.
A framework for common evalua-tion and test set  generat ion .
Supervised and un-supervised sense disambiguation methods have dif-ferent needs regarding system development and eval-uation.
Although unsupervised methods may beevaluated (with some limitations) by a sequentiallytagged corpus such as the WordNet semantic on-cordance (with a large number of polysemous wordsrepresented but with few examples of each), super-vised methods require much larger data sets focusedon a subset of polysemous words to provide ade-quately large training and testing material.
It ishoped that US and international sources will seefit to fund such a data annotation effort.
To fa-cilitate discussion of this issue, the following is aproposed framework for providing this data, satisfy-ing the needs of both supervised and unsupervisedtagging research.3Although this function enumerates over all 8i sensesof wi, because distance(cs~,cs~) -- 0 this function onlypenalizes probability mass assigned to incorrect sensesfor the given example.mmmmmmmmmmmnmmmmmmmmmmmmmm821.
Select/Collect a very large (e.g., N = 1 billionwords), diverse unannotated corpus.2.
Select a sense inventory (e.g.
WordNet,LDOCE) with respect o which algorithms willbe evaluated (see Proposal 4).3.
Pick a subset of R < N (e.g., 100M) words ofunannotated text, and release it to the commu-nity as a training set.4.
Pick a smaller subset of S < R < N (e.g.,10M) words of text as the source of the test set.Generate the test set as follows:(a) Select a set of M (e.g., 100) ambiguouswords that will be used as the basis forthe evaluation, mithout elling the researchcommunity what those words will be.
(b) For each of the M words, annotate all avail-able instances of that word in the test cor-pus.
Make sure each annotator tags all in-stances of a single word, e.g.
using a con-cordance tool, as opposed to going throughthe corpus sequentially.
(c) For each of the M words, compute valu-ation statistics using individual annotatorsagainst other annotators.
(d) For each of the M words, go through thecases where annotators disagreed and makea consensus choice, by vote if necessary.5.
Instruct participants in the evaluation to'~reeze" their code; that is, from this point onno changes may be made.6.
Have each participating algorithm do WSD onthe full S-word test corpus.7.
Evaluate the performance of each algorithmconsidering only instances of the M words an-notated as the basis for the evaluation.
Com-pare exact match, cross-entropy, and inter-judge reliability measures (e.g.
Cohen's ~) us-ing annotator-vs-annotator results as an upperbound.8.
Release this year's S-word test corpus as a de-velopment corpus for those algorithms that re-quire supervised training, so they can partici-pate from now on, being evaluated in the futurevia cross-validation.9.
Go back to Step 3 for next year's evaluation.There are a number of advantages to thisparadigm, in comparison with simply trying to an-notate large corpora with word sense information.First, it combines an emphasis on broad coveragewith the advantages of evaluating on a limited setof words, as is done traditionally in the WSD lit-erature.
Step 4a can involve any form of criteria(frequency, level of ambiguity, part of speech, etc.
)to narrow down to set of candidate words, and thenemploy random selection among those candidates.At the same time, it avoids a common criticism ofstudies based on evaluating using small sets of words,namely that there is not enough attention being paidto scalability.
In this evaluation paradigm, algo-rithms must be able to sense tag all words in thecorpus meeting specified criteria, because there is noway to know in advance which words will be used tocompute the figure(s) of merit.Second, the process avoids some of the problemsthat arise in using exhaustively annotated corporafor evaluation.
By focusing on a relatively small setof polysemous words, much larger data sets for eachcan be produced.
This focus will also allow moreattention to be paid to selecting and vetting com-prehensive and robust sense inventories, includingdetailed specifications and definitions for each.
Fur-thermore, by having annotators focus on one word atat time using concordance software, the initial levelof consistency is likely to be far higher than that ob-tained by a process in which one jumps from wordto word to word by going sequentially through atext, repeatedly refamiliarizing oneself with differentsense inventories at each word.
Finally, by comput-ing inter-annotator statistics blindly and then allow-ing annotators to confer on disagreements, a cleanertest set can be obtained without sacrificing trust-worthy upper bounds on performance.Third, the experience of the Penn Treebank andother annotation efforts has demonstrated that itis difflcult to select and freeze a comprehensive tagset for the entire vocabulary in advance.
Study-ing and writing detailed sense tagging uidelines foreach word is comparable to the effort required tocreate a new dictionary.
By focusing on only 100 orso polysemous words per evaluation, the annotatingorganization can afford to do a multi-pass tudy ofand detailed tagging guidelines for the sense inven-tory present in the data for each target word.
Thiswould be prohibitively expensive to do for the fullEnglish vocabulary.
Also, by utilizing different setsof words in each evaluation, such factors as the levelof detail and the sources of the sense inventories maychange without worrying about maintaining consis-tency with previous data.Fourth, both unsupervised and supervised WSDalgorithms are better accommodated in terms of theamount of data available.
Unsupervised algorithms83Target WordNet EnglishWord Sense # description Spanish French German Italian1 inter&, in t~t  interesser~ditointerest(noun)3,4drug la(noun)bank(noun)lbfire 1(t. verb)5monetary(e.g.
on loan)stake/shareintellectualcuriositybenefit,advantagemedicinenarcoticshorelineembankmentfinancial inst.supply/reservebank buildingarray/rowinterds,participacidninterns,provecho, inte-rns, beneficiomedicamento,droganarcdticadrogaribera, orillaloma, cuestabancobancobaJicohilera, baterladismiss despedir,from job echararouse, provoke excitar,enardecerdischarge weapn dispararbake pottery cocerintdr~tparticipationintdr~tintdr~tmedicamentZinsenAnteilInteresseInteresseMedikament,Arzheimittelinteresseinteresseint~essemedicinadrogue Drogue, drogaP~u~banc, rivetalus, terasseUferErdwallBankBankBankReihebanquebanquebanquerang, batterierenvoyer feuernenflammer, befiiigelnanimer entziindenl~:her abfeuerncuire brennenJapaneserisE,risokurikenkaushin,kySmiriekikusurimayakusponda,riva kishimuccio teib5banca gink5banca gink5bauca gink5batteria retsulicenziare kubi nishimasuaccendere kSfuninfiammare saserusparare happ5 s.cuocere yakuTable 5: Mapping between cross-finguistic sense labels and established lexiconscan be given very large quantities of training data:since they require no annotation the value of R canbe quite large.
And although supervised algorithmsare typically plagued by sparse data, this approachwill yield much larger training and testing sets perword, facilitating the exploration and developmentof data intensive supervised algorithms.Proposal  4.
A multUlngual sense inventoryfor evaluation.
One of the most fraught issuesin applied lexical semantics i how to define wordsenses.
Although we certainly do not propose adefinitive answer to that question, we suggest herea general purpose criterion that can be applied toexisting sources of word senses in a way that, wesuggest, makes sense both for target applicationsand for evaluation, and is compatible with the majorsources of available training and test data.The essence of the proposal is to restrict a wordsense inventory to those distinctions that are typ-ically lezicalized cross-linguistically.
This cuts amiddle ground between restricting oneself to homo-graphs within a single language, which tends towarda very coarse-grained distinction, and an attempt toexpress all the fine-grained istinctions made in alanguage, as found in monolingual dictionaries.
Inpractice the idea would be to define a set of tar-get languages (and associated bilingual dictionaries),and then to require that any sense distinction bereAliT~d lexically in a minimum subset of those lan-guages.
This would eliminate many distinctions thatare arguably better treated as regular polysemy.
Forexample, table can be used to refer to both a physicalobject and a group of people:(1) a.
The waiter put the food on the table.b.
Then he told another table their foodwas almost ready.c.
He finally brought appetizers to the tablean hour later.In German the two meanings can actually be lexi-calized ifferently (Tisch vs. Tischrunde).
However,as such sense distinctions are typically conflated intoa single word in most languages, and because venGerman can use Tisch in both cases, one could plau-sibly argue for a common sense inventory for evalu-ation that conflates these meanings.A useful reference source for both training andevaluation would be a table linking sense numbersin established lexical resources (such as WordNet orLDOCE) with these crosslinguistic translation dis-tinctions.
An example of such a map is given inTable 5.
A comparable mapping could readily be84extracted semi-automatically from bilingual dictio-naries or from the EuroWordNet effort (Bloksma etal., 1996) which provides both semantic hierarchiesand interlingnal node linkages, currently for the lan-guages Spanish, Italian, Dutch and English.
We notethat the table follows many lexical resources, such asthe original WordNet, in being organized at the toplevel according to parts of speech.
This seems to usa sensible approach to take for sense inventories, es-pecially in light of Wilks and Stevenson's (1996) ob-servation that part-of-speech tagging accomplishesmuch of the work of semantic disambignation, atleast at the level of homographs.Although cross-linguistic divergence is a signifi-cant problem, and 1-1 translation maps do not existfor all sense-language pairs, this table suggests howmultiple parallel bilingual corpora for different lan-gnage pairs can be used to yield sets of training datacovering different subsets of the English sense inven-tory, that in aggregate may yield tagged data forall given sense distinctions when any one languagealone may not be adequate.For example, a German-English parallel corpuscould yield tagged data for Senses 1 and 2 for in-terest, and the presence of certain Spanish words(provecho, beneficio) aligned with interest in aSpanish-English corpus will tag some instances ofSense 5, with a Japanese-English aligned corpus po-tentially providing data for the remaining sense dis-tinctions.
In some cases it will not be possible tofind any language (with adequate on-line parallelcorpora) that lexicalize some subtle English sensedistinctions differently, but this may be evidencethat the distinction is regular or subtle enough tobe excluded or handled by other means.Note that Table 5 is not intended for direct usein machine translation.
Also note that when twoword senses are in a cell they axe not necessarilysynonyms.
In some cases they realize differences inmeaning or contextual usage that are salient to thetarget language.
However, at the level of sense dis-tinction given in the table, they correspond to thesame word senses in English and the presence of ei-ther in an aligned bilingual corpus will indicate thesame English word sense.Monolingual sense tagging of another languagesuch as Spanish would yield a similar map, such asdistinguishing the senses of the Spanish word dedo,which can mean 'finger' or 'toe'.
Either English orGerman could be used to distinguish these senses,but not Italian or French, which share the same senseambiguity.It would also be helpful for Table 5 to includealignments between multiple monolingual sense rep-resentations, such as COBUILD sense numbers,LDOCE tags or WordNet synsets, to support thesharing and leveraging of results between multiplesystems.
This brings to the fore an existing problem,of course: different sense inventories lead to differ-ent algorithmic biases.
For example, WordNet as asense inventory would tend to bias an evaluation infavor of algorithms that take advantage of taxonomicstructure; LDOCE might bias in favor of algorithmsthat can take advantage of topical/subject codes,and so forth.
Unfortunately we have no solution topropose for the problem of which representation (ifany) should be the ultimate standard, and leave itas a point for discussion.4 Conc lus ionsThe most important of our observations about thestate of the art in word sense disambiguation is thatit is still a hard, open problem, for which the fieldhas not yet narrowed much.
We have made severalsuggestions that we believe will help assess progressand advance the state of the art.
In summary:?
We proposed that the accepted standard forWSD evaluation include a cross-entropy likemeasure that tests the accuracy of the probabil-ities assigned to sense tags and offers a mecha-nism for assigning partial credit.?
We suggested a paradigm for common evalua-tion that combines the benefits of traditional"interesting word" evaluations with an empha-sis on broad cov~age and scalability.?
We outlined a criterion that should help in de-terminirlg a suitable sense inventory to use forcomparison of algorithms, compatible with bothhierarchical sense partitions and multilinguallymotivated sense distinctions.ReferencesL.
Bahl and R. Mercer.
1976.
Part-of-speech as-sigument by a statistical decision algorithm.
InInternational Symposium on Information Theory,Ronneby, Sweden.L.
Bald, F. Jelinek, and R. Mercer.
1983.
A max-imum likelihood approach to continuous speechrecognition.
IEEE Transactions on Pattern Anal-ysis and Machine Intelligence, PAMI-5(2):179-190.L.
Bloksma, P. Dfez-Orzas and P. Vossen.
1996.User Requirements and Functional Specificationof the EuroWordNet Project.
http:/lw~m, let.uva.
nll-er, m.85E.
Brill.
1993.
A Corpus-Based Approach to Lan-guage Learning.
Ph.D. thesis, Computer and In-formation Science, University of Pennsylvania.P.
Brown, V. Della Pietra, P. deSouza, J. Lai, andR.
Mercer.
1992.
Class-based n-gram modelsof natural anguage.
Computational Linguistics,18(4):467-480.R.
Bruce and J. Wiebe.
1994.
Word-sense disam-biguation using decomposable models.
In Proceed-ings of the 3~nd Annual Meeting of the Associa-tion .for Computational Linguistics, Las Cruces.K.
Church.
1988.
A stochastic parts program andnoun phrase parser for unrestricted texts.
In Pro-ceedings of the Second Conference on Applied Nat-ural Language Processing, Austin, Texas.B.
Dorr and D. Jones.
1996a.
Acquisition of seman-tic lexicons: Using word sense disarnbiguation toimprove precision.
In Proceedings ofthe SIGLEXWorkshop on Breadth and Depth of Semantic Lex-icons, Santa Cruz, CA.B.
Dorr and D. Jones.
1996b.
Role of word sense dis-ambiguation i lexical acquisition: Predicting se-mantics from syntactic ues.
In Proceedings oftheInternational Conference on Computational Lin-guistics, Copenhagen, Denmark.W.
Gale, K. Church, and D. Yarowsky.
1992.One sense per discourse.
Proceedings of the 4thDARPA Speech and Natural Language Workshop.W.
Gale, K. Church, and D. Yarowsky.
A methodfor disambiguating word senses in a large corpus.Computers and the Humanities, 26:415-439, 1992.F.
Jelinek.
1985.
Markov source modeling of textgeneration.
In J. Skwirzinski, editor, Impact ofProcessing Techniques on Communication.
Dor-drecht.S.
Katz.
1987.
Estimation of probabilities fromsparse data for the language model componentof a speech recognizer.
IEEE Transactions onAcoustics, Speech and Signal Processing, ASSP-35(3):400-401.It.
Krovetz and W. B. Croft.
1992.
Lexical ambigu-ity and information retrieval.
ACM Transactionson Information Systems, 10(2):115-141.C.
Leaeock, G. Towell and E. Voorhees.
1993.Corpus-based statistical sense resolution.
InProceedings, ARPA Human Language TechnologyWorkshop, pp.
260-265, Plainsboro, NJ.J.
Lehman.
1994.
Toward the essential nature ofstatistical knowledge in sense resolution.
In Pro-ceedings of the TvJelfth National Conference onArtificial Intelligence, pp.
734-471.86R.
Mooney.
1996.
Comparative experiments on dis-am~i~mating word senses: An illustration of therole of bias in machine learning.
In Proceedings ofthe Con\]erence onEmpirical Methods in NaturalLanguage Processing, Philadelphia.S.
McRoy.
1992.
Using multiple knowledge sourcesfor word sense discrimination.
ComputationalLinguistics, 18(1):1-30.G.
Miller and W. Charles.
1991.
Contextual corre-lates of semantic similarity.
Language and Cogni-tive Processes, 6(1):1-28.G.
Miller, M. Chodorow, S. Landes, C. Leacock,and R. Thomas.
1994.
Using a semantic con-cordance for sense identification.
In Proceedingsof the ARPA Human Language Technology Work-shop, San Francisco.
Morgan Kaufmarm.H.
Ng and H. Lee.
1996.
Integrating multipleknowledge sources to disambiguate word sense:An exemplar-based approach.
In Proceedings o.fthe 34th Annual Meeting o.f the Society for Com-putational Linguistics, pp.
40-47, Santa Cruz, CA.P.
Resnik.
1993.
Selection and ln\]ormation: AClass-Based Approach to Le~cal Relationships.Ph.D.
thesis, University of Pennsylvania.
( f tp : / /ftp.
cis.
upenn, edu/pub/ircs/tr/93-42, ps.
Z).P.
Resnik.
1995.
Using information content to eval-uate semantic similarity in a taxonomy.
In Pro-ceedings of the I~th International Joint Confer-ence on Artificial Intelligence (IJCAI-95).
(cmp-lg/9511007).R.
Richardson, A. Smeaton, and J. Murphy.
1994.Using WordNet as a knowledge base for measur-ing semantic similarity between words.
Work-ing Paper CA-1294, Dublin City University,School of Computer Applications, Dublin, Ireland.f tp : / / f tp ,  compapp, dcu.
ie/pub/w-papers/1994/CA1294.
ps.
Z.P.
Tapanainen and A. Voutilainen.
1994.
Taggingaccurately - don't guess if you know.
In Proceed-ings of ANLP '94.Y.
Wilks and M. Stevenson.
1996.
The grammarof sense: Is word-sense tagging much more thanpart-of-speech tagging?
cmp-lg/9607028.D.
Yarowsky.
1992.
Word-sense disambiguation us-ing statistical models of Roget's categories trainedon large corpora.
In Proceedings ofCOLING-9~,pp.
454-460, Nantes, Prance.D.
Yarowsky.
1993.
One sense per collocation.
Pro-ccedings of the ARIA Human Language Technol-ogy Workshop, Morgan Kaufmann, pp.
266-271.
