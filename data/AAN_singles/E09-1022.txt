Proceedings of the 12th Conference of the European Chapter of the ACL, pages 184?192,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsLearning to Interpret Utterances Using Dialogue HistoryDavid DeVaultInstitute for Creative TechnologiesUniversity of Southern CaliforniaMarina del Rey, CA 90292devault@ict.usc.eduMatthew StoneDepartment of Computer ScienceRutgers UniversityPiscataway, NJ 08845-8019Matthew.Stone@rutgers.eduAbstractWe describe a methodology for learning adisambiguation model for deep pragmaticinterpretations in the context of situatedtask-oriented dialogue.
The system accu-mulates training examples for ambiguityresolution by tracking the fates of alter-native interpretations across dialogue, in-cluding subsequent clarificatory episodesinitiated by the system itself.
We illus-trate with a case study building maxi-mum entropy models over abductive in-terpretations in a referential communica-tion task.
The resulting model correctly re-solves 81% of ambiguities left unresolvedby an initial handcrafted baseline.
A keyinnovation is that our method draws exclu-sively on a system?s own skills and experi-ence and requires no human annotation.1 IntroductionIn dialogue, the basic problem of interpretation isto identify the contribution a speaker is making tothe conversation.
There is much to recognize: thedomain objects and properties the speaker is refer-ring to; the kind of action that the speaker is per-forming; the presuppositions and implicatures thatrelate that action to the ongoing task.
Neverthe-less, since the seminal work of Hobbs et al (1993),it has been possible to conceptualize pragmatic in-terpretation as a unified reasoning process that se-lects a representation of the speaker?s contributionthat is most preferred according to a backgroundmodel of how speakers tend to behave.In principle, the problem of pragmatic interpre-tation is qualitatively no different from the manyproblems that have been tackled successfully bydata-driven models in NLP.
However, while re-searchers have shown that it is sometimes possi-ble to annotate corpora that capture features of in-terpretation, to provide empirical support for the-ories, as in (Eugenio et al, 2000), or to buildclassifiers that assist in dialogue reasoning, as in(Jordan and Walker, 2005), it is rarely feasibleto fully annotate the interpretations themselves.The distinctions that must be encoded are subtle,theoretically-loaded and task-specific?and theyare not always signaled unambiguously by thespeaker.
See (Poesio and Vieira, 1998; Poesioand Artstein, 2005), for example, for an overviewof problems of vagueness, underspecification andambiguity in reference annotation.As an alternative to annotation, we argue herethat dialogue systems can and should preparetheir own training data by inference from under-specified models, which provide sets of candi-date meanings, and from skilled engagement withtheir interlocutors, who know which meanings areright.
Our specific approach is based on contribu-tion tracking (DeVault, 2008), a framework whichcasts linguistic inference in situated, task-orienteddialogue in probabilistic terms.
In contributiontracking, ambiguous utterances may result in alter-native possible contexts.
As subsequent utterancesare interpreted in those contexts, ambiguities mayramify, cascade, or disappear, giving new insightinto the pattern of activity that the interlocutor isengaged in.
For example, consider what happensif the system initiates clarification.
The interlocu-tor?s answer may indicate not only what they meannow but also what they must have meant earlierwhen they used the original ambiguous utterance.Contribution tracking allows a system to accu-mulate training examples for ambiguity resolutionby tracking the fates of alternative interpretationsacross dialogue.
The system can use these ex-amples to improve its models of pragmatic inter-pretation.
To demonstrate the feasibility of thisapproach in realistic situations, we present a sys-tem that tracks contributions to a referential com-munication task using an abductive interpretation184model: see Section 2.
A user study with this sys-tem, described in Section 3, shows that this sys-tem can, in the course of interacting with its users,discover the correct interpretations of many poten-tially ambiguous utterances.
The system therebyautomatically acquires a body of training data inits native representations.
We use this data to builda maximum entropy model of pragmatic interpre-tation in our referential communication task.
Aftertraining, we correctly resolve 81% of the ambigu-ities left open in our handcrafted baseline.2 Contribution trackingWe continue a tradition of research that uses sim-ple referential communication tasks to explore theorganization and processing of human?computerand mediated human?human conversation, includ-ing recently (DeVault and Stone, 2007; Gergleet al, 2007; Healey and Mills, 2006; Schlangenand Ferna?ndez, 2007).
Our specific task is a two-player object-identification game adapted from theexperiments of Clark and Wilkes-Gibbs (1986)and Brennan and Clark (1996); see Section 2.1.To play this game, our agent, COREF, inter-prets utterances as performing sequences of task-specific problem-solving acts using a combinationof grammar-based constraint inference and abduc-tive plan recognition; see Section 2.2.
Crucially,COREF?s capabilities also include the ambiguitymanagement skills described in Section 2.3, in-cluding policies for asking and answering clarifi-cation questions.2.1 A referential communication taskThe game plays out in a special-purpose graphicalinterface, which can support either human?humanor human?agent interactions.
Two players worktogether to create a specific configuration of ob-jects, or a scene, by adding objects into the sceneone at a time.
Their interfaces display the same setof candidate objects (geometric objects that differin shape, color and pattern), but their locations areshuffled.
The shuffling undermines the use of spa-tial expressions such as ?the object at bottom left?.Figures 1 and 2 illustrate the different views.11Note that in a human?human game, there are literallytwo versions of the graphical interface on the separate com-puters the human participants are using.
In a human?agentinteraction, COREF does not literally use the graphical inter-face, but the information that COREF is provided is limitedto the information the graphical interface would provide to ahuman participant.
For example, COREF is not aware of thelocations of objects on its partner?s screen.Present: [c4, Agent], Active: [] Skip this objectContinue (next object) or You (c4:)c4: brown diamondc4: yesHistoryCandidate Objects    Your sceneFigure 1: A human user plays an object identifi-cation game with COREF.
The figure shows theperspective of the user (denoted c4).
The user isplaying the role of director, and trying to identifythe diamond at upper right (indicated to the userby the blue arrow) to COREF.Present: [c4, Agent], Active: [] Skip this object or You (Agent:)c4: brown diamondc4: yesHistoryCandidate Objects    Your sceneFigure 2: The conversation of Figure 1 fromCOREF?s perspective.
COREF is playing the roleof matcher, and trying to determine which objectthe user wants COREF to identify.As in the experiments of Clark and Wilkes-Gibbs (1986) and Brennan and Clark (1996), oneof the players, who plays the role of director,instructs the other player, who plays the role ofmatcher, which object is to be added next to thescene.
As the game proceeds, the next target ob-ject is automatically determined by the interfaceand privately indicated to the director with a bluearrow, as shown in Figure 1.
(Note that the corre-sponding matcher?s perspective, shown in Figure2, does not include the blue arrow.)
The director?sjob is then to get the matcher to click on (their ver-sion of) this target object.To achieve agreement about the target, the twoplayers can exchange text through an instant-messaging modality.
(This is the only communi-185cation channel.)
Each player?s interface providesa real-time indication that their partner is ?Active?while their partner is composing an utterance, butthe interface does not show in real-time what isbeing typed.
Once the Enter key is pressed, theutterance appears to both players at the bottom ofa scrollable display which provides full access toall the previous utterances in the dialogue.When the matcher clicks on an object they be-lieve is the target, their version of that object is pri-vately moved into their scene.
The director has novisible indication that the matcher has clicked onan object.
However, the director needs to click theContinue (next object) button (see Fig-ure 1) in order to move the current target into thedirector?s scene, and move on to the next targetobject.
This means that the players need to discussnot just what the target object is, but also whetherthe matcher has added it, so that they can coordi-nate on the right moment to move on to the nextobject.
If this coordination succeeds, then afterthe director and matcher have completed a seriesof objects, they will have created the exact samescene in their separate interfaces.2.2 Interpreting user utterancesCOREF treats interpretation broadly as a prob-lem of abductive intention recognition (Hobbs etal., 1993).2 We give a brief sketch here to high-light the content of COREF?s representations, thesources of information that COREF uses to con-struct them, and the demands they place on disam-biguation.
See DeVault (2008) for full details.COREF?s utterance interpretations take theform of action sequences that it believes wouldconstitute coherent contributions to the dialoguetask in the current context.
Interpretations are con-structed abductively in that the initial actions inthe sequence need not be directly tied to observ-able events; they may be tacit in the terminologyof Thomason et al (2006).
Examples of such tacitactions include clicking an object, initiating a clar-ification, or abandoning a previous question.
Asa concrete example, consider utterance (1b) fromthe dialogue of Figure 1, repeated here as (1):(1) a. COREF: is the target round?b.
c4: brown diamondc.
COREF: do you mean dark brown?d.
c4: yes2In fact, the same reasoning interprets utterances, buttonpresses and the other actions COREF observes!In interpreting (1b), COREF hypothesizes that theuser has tacitly abandoned the agent?s question in(1a).
In fact, COREF identifies two possible inter-pretations for (1b):i2,1= ?
c4:tacitAbandonTasks[2],c4:addcr[t7,rhombus(t7)],c4:setPrag[inFocus(t7)],c4:addcr[t7,saddlebrown(t7)]?i2,2= ?
c4:tacitAbandonTasks[2],c4:addcr[t7,rhombus(t7)],c4:setPrag[inFocus(t7)],c4:addcr[t7,sandybrown(t7)]?Both interpretations begin by assuming thatuser c4 has tacitly abandoned the previous ques-tion, and then further analyze the utterance as per-forming three additional dialogue acts.
When a di-alogue act is preceded by tacit actions in an inter-pretation, the speaker of the utterance implicatesthat the earlier tacit actions have taken place (De-Vault, 2008).
These implicatures are an importantpart of the interlocutors?
coordination in COREF?sdialogues, but they are a major obstacle to annotat-ing interpretations by hand.Action sequences such as i2,1 and i2,2 are coher-ent only when they match the state of the ongoingreferential communication game and the seman-tic and pragmatic status of information in the dia-logue.
COREF tracks these connections by main-taining a probability distribution over a set of di-alogue states, each of which represents a possi-ble thread that resolves the ambiguities in the di-alogue history.
For performance reasons, COREFentertains up to three alternative threads of inter-pretation; COREF strategically drops down to thesingle most probable thread at the moment eachobject is completed.
Each dialogue state repre-sents the stack of processes underway in the ref-erential communication game; constituent activi-ties include problem-solving interactions such asidentifying an object, information-seeking interac-tions such as question?answer pairs, and ground-ing processes such as acknowledgment and clari-fication.
Dialogue states also represent pragmaticinformation including recent utterances and refer-ents which are salient or in focus.COREF abductively recognizes the intention Iof an actor in three steps.
First, for each dia-logue state sk, COREF builds a horizon graph ofpossible tacit action sequences that could be as-sumed coherently, given the pending tasks (De-Vault, 2008).Second, COREF uses the horizon graph andother resources to solve any constraints associ-186ated with the observed action.
This step instanti-ates any free parameters associated with the actionto contextually relevant values.
For utterances,the relevant constraints are identified by parsingthe utterance using a hand-built, lexicalized tree-adjoining grammar.
In interpreting (1b), the parseyields an ambiguity in the dialogue act associatedwith the word ?brown?, which may mean eitherof the two shades of brown in Figure 1, whichCOREF distinguishes using its saddlebrownand sandybrown concepts.Once COREF has identified a set of interpre-tations {it,1, ..., it,n} for an utterance o at time t,the last step is to assign a probability to each.
Ingeneral, we conceive of this following Hobbs etal.
(1993): the agent should weigh the differentassumptions that went into constructing each in-terpretation.3 Ultimately, this process should bemade sensitive to the rich range of factors thatare available from COREF?s deep representationof the dialogue state and the input utterance?thisis our project in this paper.
However, in our initialimplemented prototype, COREF assigned theseprobabilities using a simple hand-built model con-sidering only NT, the number of tacit actions ab-ductively assumed to occur in an interpretation:P (I = it,j |o, St = sk) ?1NT(it,j) + 1(1)In effect, this is a ?null hypothesis?
that assignsrelatively uniform weights to different abductivehypotheses.2.3 Interactive disambiguationCOREF uses its probabilistic model of context inorder to tolerate ambiguity as it moves forwardwith its dialogues and to resolve ambiguity overtime.
We have put particular effort into COREF?sskills with three kinds of ambiguity: word-senseambiguities, where COREF finds multiple resolu-tions for the domain concept evoked by the useof a lexical item, as in the interaction (1) of Fig-ure 1; referential ambiguities, where COREF takesa noun phrase to be compatible with multiple ob-jects from the display; and speech act ambiguities,where alternative interpretations communicate orimplicate different kinds of contributions to theongoing task.The resolution of ambiguity may involve somecombination of asking questions of the user, ag-3Though note that Hobbs et al do not explicitly construetheir weights in terms of probabilities.Figure 3: A human user plays the director role inan object identification game with COREF.
Thefigure shows the perspective of the user.gregating information provided by the user acrossmultiple turns of dialogue, and strategically drop-ping threads of interpretation.
For example,COREF represents the context resulting from (1b)in terms of two states: one from interpretation i2,1and one from i2,2.
COREF asks a clarificationquestion (1c); the user?s explicit answer yes allowsCOREF to discard one of the possible states andallocate all its probability mass to the other one.The dialogue in (2) shows an alternative case.
(2) a. c1: color is orangeb.
c1: shape is diamondc.
COREF: okThe example is taken from the setting shown inFigure 3.
In this case, COREF finds two colors onthe screen it thinks the user could intend to evokewith the word orange; the peachy orange of the di-amond and circle on the top row and the brighterorange of the solid and empty squares in the mid-dle column.
COREF responds to the ambiguity byintroducing two states which track the alternativecolors.
Immediately COREF gets an additionaldescription from the user, and adds the constraintthat the object is a diamond.
As there is no brightorange diamond, there is no way to interpret theuser?s utterance in the bright orange state; COREFdiscards this state and allocates all its probabilitymass to the other one.3 Inferring the fates of interpretationsOur approach is based on the observation thatCOREF?s contribution tracking can be viewed asassigning a fate to every dialogue state it enter-tains as part of some thread of interpretation.
In187particular, if we consider the agent?s contributiontracking retrospectively, every dialogue state canbe assigned a fate of correct or incorrect, where astate is viewed as correct if it or some of its descen-dants eventually capture all the probability massthat COREF is distributing across the viable sur-viving states, and incorrect otherwise.In general, there are two ways that a state canend up with fate incorrect.
One way is that thestate and all of its descendants are eventually de-nied any probability mass due to a failure to in-terpret a subsequent utterance or action as a co-herent contribution from any of those states.
Inthis case, we say that the incorrect state was elimi-nated.
The second way a state can end up incorrectis if COREF makes a strategic decision to drop thestate, or all of its surviving descendants, at a timewhen the state or its descendants were assignednonzero probability mass.
In this case we say thatthe incorrect state was dropped.
Meanwhile, be-cause COREF drops all states but one after eachobject is completed, there is a single hypothesizedstate at each time t whose descendants will ulti-mately capture all of COREF?s probability mass.Thus, for each time t, COREF will retrospectivelyclassify exactly one state as correct.Of course, we really want to classify interpre-tations.
Because we seek to estimate P (I =it,j |o, St = sk), which conditions the probabilityassigned to I = it,j on the correctness of statesk, we consider only those interpretations arisingin states that are retrospectively identified as cor-rect.
For each such interpretation, we start fromthe state where that interpretation is adopted andtrace forward to a correct state or to its last surviv-ing descendant.
We classify the interpretation thesame way as that final state, either correct, elimi-nated, or dropped.We harvested a training set using this method-ology from the transcripts of a previous evaluationexperiment designed to exercise COREF?s ambi-guity management skills.
The data comes from20 subjects?most of them undergraduates par-ticipating for course credit?who interacted withCOREF over the web in three rounds of the ref-erential communication each.
The number of ob-jects increased from 4 to 9 to 16 across rounds;the roles of director and matcher alternated in eachround, with the initial role assigned at random.Of the 3275 sensory events that COREF in-terpreted in these dialogues, from the (retrospec-N Percentage N Percentage0 10.53 5 0.211 79.76 6 0.122 7.79 7 0.093 0.85 8 0.064 0.58 9 0.0Figure 4: Distribution of degree of ambiguity intraining set.
The table lists percentage of eventsthat had a specific number N of candidate inter-pretations constructed from the correct state.tively) correct state, COREF hypothesized 0 inter-pretations for 345 events, 1 interpretation for 2612events, and more than one interpretation for 318events.
The overall distribution in the number ofinterpretations hypothesized from the correct stateis given in Figure 4.4 Learning pragmatic interpretationWe capture the fate of each interpretation it,j in adiscrete variable F whose value is correct, elimi-nated, or dropped.
We also represent each inten-tion it,j , observation o, and state sk in terms offeatures.
We seek to learn a functionP (F = correct | features(it,j),features(o),features(sk))from a set of training examples E = {e1, ..., en}where, for l = 1..n, we have:el = ( F = fate(it,j), features(it,j),features(o), features(sk)).We chose to train maximum entropy models(Berger et al, 1996).
Our learning framework isdescribed in Section 4.1; the results in Section 4.2.4.1 Learning setupWe defined a range of potentially useful features,which we list in Figures 5, 6, and 7.
These fea-tures formalize pragmatic distinctions that plau-sibly provide evidence of the correct interpreta-tion for a user utterance or action.
You mightannotate any of these features by hand, but com-puting them automatically lets us easily explore amuch larger range of possibilities.
To allow thesevarious kinds of features (integer-valued, binary-valued, and string-valued) to interface to the max-imum entropy model, these features were con-verted into a much broader class of indicator fea-tures taking on a value of either 0.0 or 1.0.188feature set descriptionNumTacitActions The number of tacit actions in it,j .TaskActions These features represent the action type (function symbol) ofeach action ak in it,j = ?A1 : a1, A2 : a2, ..., An : an?, as astring.ActorDoesTaskAction For each Ak : ak in it,j = ?A1 : a1, A2 : a2, ..., An : an?, afeature indicates that Ak (represented as string ?Agent?
or?User?)
has performed action ak (represented as a stringaction type, as in the TaskActions features).Presuppositions If o is an utterance, we include a string representation of eachpresupposition assigned to o by it,j .
The predicate/argumentstructure is captured in the string, but any gensym identifierswithin the string (e.g.
target12) are replaced withexemplars for that identifier type (e.g.
target).Assertions If o is an utterance, we include a string representation of eachdialogue act assigned to o by it,j .
Gensym identifiers arefiltered as in the Presuppositions features.Syntax If o is an utterance, we include a string representation of thebracketed phrase structure of the syntactic analysis assigned too by it,j .
This includes the categories of all non-terminals inthe structure.FlexiTaskIntentionActors Given it,j = ?A1 : a1, A2 : a2, ..., An : an?, we include a singlestring feature capturing the actor sequence ?A1, A2, ..., An?
init,j (e.g.
?User, Agent, Agent?
).Figure 5: The interpretation features, features(it,j), available for selection in our learned model.feature set descriptionWords If o is an utterance, we include features that indicate thepresence of each word that occurs in the utterance.Figure 6: The observation features, features(o), available for selection in our learned model.feature set descriptionNumTasksUnderway The number of tasks underway in sk.TasksUnderway The name, stack depth, and current task state for each taskunderway in sk.NumRemainingReferents The number of objects yet to be identified in sk.TabulatedFacts String features representing each proposition in theconversational record in sk (with filtered gensym identifiers).CurrentTargetConstraints String features for each positive and negative constraint on thecurrent target in sk (with filtered gensym identifiers).
E.g.
?positive: squareFigureObject(target)?
or?negative: solidFigureObject(target)?.UsefulProperties String features for each property instantiated in the experimentinterface in sk.
E.g.
?squareFigureObject?,?solidFigureObject?, etc.Figure 7: The dialogue state features, features(sk), available for selection in our learned model.189We used the MALLET maximum entropy clas-sifier (McCallum, 2002) as an off-the-shelf, train-able maximum entropy model.
Each run involvedtwo steps.
First, we applied MALLET?s featureselection algorithm, which incrementally selectsfeatures (as well as conjunctions of features) thatmaximize an exponential gain function which rep-resents the value of the feature in predicting in-terpretation fates.
Based on manual experimenta-tion, we chose to have MALLET select about 300features for each learned model.
In the secondstep, the selected features were used to train themodel to estimate probabilities.
We used MAL-LET?s implementation of Limited-Memory BFGS(Nocedal, 1980).4.2 EvaluationWe are generally interested in whether COREF?sexperience with previous subjects can be lever-aged to improve its interactions with new sub-jects.
Therefore, to evaluate our approach, whilemaking maximal use of our available data set, weperformed a hold-one-subject-out cross-validationusing our 20 human subjects H = {h1, ..., h20}.That is, for each subject hi, we trained a modelon the training examples associated with subjectsH \ {hi}, and then tested the model on the exam-ples associated with subject hi.To quantify the performance of the learnedmodel in comparison to our baseline, we adaptthe mean reciprocal rank statistic commonly usedfor evaluation in information retrieval (Vorhees,1999).
We expect that a system will use the prob-abilities calculated by a disambiguation model todecide which interpretations to pursue and how tofollow them up through the most efficient interac-tion.
What matters is not the absolute probabilityof the correct interpretation but its rank with re-spect to competing interpretations.
Thus, we con-sider each utterance as a query; the disambigua-tion model produces a ranked list of responses forthis query (candidate interpretations), ordered byprobability.
We find the rank r of the correct in-terpretation in this list and measure the outcomeof the query as 1r .
Because of its weak assump-tions, our baseline disambiguation model actuallyleaves many ties.
So in fact we must compute anexpected reciprocal rank (ERR) statistic that aver-ages 1r over all ways of ordering the correct inter-pretation against competitors of equal probability.Figure 8 shows a histogram of ERR acrossERR range Hand-builtmodelLearnedmodels1 20.75% 81.76%[12 , 1) 74.21% 16.35%[13 ,12) 3.46% 1.26%[0, 13) 1.57% 0.63%mean(ERR) 0.77 0.92var(ERR) 0.02 0.03Figure 8: For the 318 ambiguous sensory events,the distribution of the expected reciprocal of rankof the correct interpretation, for the initial, hand-built model and the learned models in aggregate.the ambiguous utterances from the corpus.
Thelearned models correctly resolve almost 82%,while the baseline model correctly resolves about21%.
In fact, the learned models get much of thisimprovement by learning weights to break the tiesin our baseline model.
The overall performancemeasure for a disambiguation model is the meanexpected reciprocal rank across all examples in thecorpus.
The learned model improves this metric to0.92 from a baseline of 0.77.
The difference is un-ambiguously significant (Wilcoxon rank sum testW = 23743.5, p < 10?15).4.3 Selected featuresFeature selection during training identified a vari-ety of syntactic, semantic, and pragmatic featuresas useful in disambiguating correct interpretations.Selections were made from every feature set inFigures 5, 6, and 7.
It was often possible to iden-tify relevant features as playing a role in successfuldisambiguation by the learned models.
For exam-ple, the learned model trained on H \ {c4} deliv-ered the following probabilities for the two inter-pretations COREF found for c4?s utterance (1b):P (I = i2,1|o, S2 = s8923) = 0.665P (I = i2,2|o, S2 = s8923) = 0.335The correct interpretation, i2,1, hypothesizes thatthe user means saddlebrown, the darker of thetwo shades of brown in the display.
Among thefeatures selected in this model is a Presupposi-tions feature (see Figure 5) which is present justin case the word ?brown?
is interpreted as mean-ing saddlebrown rather than some other shade.This feature allows the learned model to preferto interpret c4?s use of ?brown?
as meaning this190darker shade of brown, based on the observed lin-guistic behavior of other users.5 Results in contextOur work adds to a body of research learning deepmodels of language from evidence implicit in anagent?s interactions with its environment.
It sharesmuch of its motivation with co-training (Blum andMitchell, 1998) in improving initial models byleveraging additional data that is easy to obtain.However, as the examples of Section 2.3 illustrate,COREF?s interactions with its users offer substan-tially more information about interpretation thanthe raw text generally used for co-training.
Closerin spirit is AI research on learning vocabularyitems by connecting user vocabulary to the agent?sperceptual representations at the time of utterance(Oates et al, 2000; Roy and Pentland, 2002; Co-hen et al, 2002; Yu and Ballard, 2004; Steelsand Belpaeme, 2005).
Our framework augmentsthis information about utterance context with ad-ditional evidence about meaning from linguisticinteraction.
In general, dialogue coherence is animportant source of evidence for all aspects of lan-guage, for both human language learning (Saxtonet al, 2005) as well as machine models.
For exam-ple, Bohus et al (2008) use users?
confirmationsof their spoken requests in a multi-modal interfaceto tune the system?s ASR rankings for recognizingsubsequent utterances.Our work to date has a number of limitations.First, although 318 ambiguous interpretations didoccur, this user study provided a relatively smallnumber of ambiguous interpretations, in machinelearning terms; and most (80.2%) of those that didoccur were 2-way ambiguities.
A richer domainwould require both more data and a generative ap-proach to model-building and search.Second, this learning experiment has been per-formed after the fact, and we have not yet inves-tigated the performance of the learned model in afollow-up experiment in which COREF uses thelearned model in interactions with its users.A third limitation lies in the detection of?correct?
interpretations.
Our scheme some-times conflates the user?s actual intentions withCOREF?s subsequent assumptions about them.
IfCOREF decides to strategically drop the user?sactual intended interpretation, our scheme maymark another interpretation as ?correct?.
Alterna-tive approaches may do better at harvesting mean-ingful examples of correct and incorrect interpre-tations from an agent?s dialogue experience.
Ourapproach also depends on having clear evidenceabout what an interlocutor has said and whetherthe system has interpreted it correctly?evidencethat is often unavailable with spoken input orinformation-seeking tasks.
Thus, even when spo-ken language interfaces use probabilistic inferencefor dialogue management (Williams and Young,2007), new techniques may be needed to minetheir experience for correct interpretations.6 ConclusionWe have implemented a system COREF thatmakes productive use of its dialogue experience bylearning to rank new interpretations based on fea-tures it has historically associated with correct ut-terance interpretations.
We present these results asa proof-of-concept that contribution tracking pro-vides a source of information that an agent canuse to improve its statistical interpretation process.Further work is required to scale these techniquesto richer dialogue systems, and to understand thebest architecture for extracting evidence from anagent?s interpretive experience and modeling thatevidence for future language use.
Nevertheless,we believe that these results showcase how judi-cious system-building efforts can lead to dialoguecapabilities that defuse some of the bottlenecks tolearning rich pragmatic interpretation.
In particu-lar, a focus on improving our agents?
basic abilitiesto tolerate and resolve ambiguities as a dialogueproceeds may prove to be a valuable technique forimproving the overall dialogue competence of theagents we build.AcknowledgmentsThis work was sponsored in part by NSF CCF-0541185 and HSD-0624191, and by the U.S.Army Research, Development, and EngineeringCommand (RDECOM).
Statements and opinionsexpressed do not necessarily reflect the position orthe policy of the Government, and no official en-dorsement should be inferred.
Thanks to our re-viewers, Rich Thomason, David Traum and JasonWilliams.191ReferencesAdam L. Berger, Stephen Della Pietra, and VincentJ.
Della Pietra.
1996.
A maximum entropy ap-proach to natural language processing.
Computa-tional Linguistics, 22(1):39?71.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Pro-ceedings of the 11th Annual Conference on Compu-tational Learning Theory, pages 92?100.Dan Bohus, Xiao Li, Patrick Nguyen, and GeoffreyZweig.
2008.
Learning n-best correction modelsfrom implicit user feedback in a multi-modal localsearch application.
In The 9th SIGdial Workshop onDiscourse and Dialogue.Susan E. Brennan and Herbert H. Clark.
1996.
Con-ceptual pacts and lexical choice in conversation.Journal of Experimental Psychology, 22(6):1482?1493.Herbert H. Clark and DeannaWilkes-Gibbs.
1986.
Re-ferring as a collaborative process.
In Philip R. Co-hen, Jerry Morgan, and Martha E. Pollack, editors,Intentions in Communication, pages 463?493.
MITPress, Cambridge, Massachusetts, 1990.Paul R. Cohen, Tim Oates, Carole R. Beal, and NiallAdams.
2002.
Contentful mental states for robotbaby.
In Eighteenth national conference on Artifi-cial intelligence, pages 126?131, Menlo Park, CA,USA.
American Association for Artificial Intelli-gence.David DeVault and Matthew Stone.
2007.
Managingambiguities across utterances in dialogue.
In Pro-ceedings of the 11th Workshop on the Semantics andPragmatics of Dialogue (Decalog 2007), pages 49?56.David DeVault.
2008.
Contribution Tracking: Par-ticipating in Task-Oriented Dialogue under Uncer-tainty.
Ph.D. thesis, Department of Computer Sci-ence, Rutgers, The State University of New Jersey,New Brunswick, NJ.Barbara Di Eugenio, Pamela W. Jordan, Richmond H.Thomason, and Johanna D. Moore.
2000.
Theagreement process: An empirical investigation ofhuman-human computer-mediated collaborative di-alogue.
International Journal of Human-ComputerStudies, 53:1017?1076.Darren Gergle, Carolyn P.
Rose?, and Robert E. Kraut.2007.
Modeling the impact of shared visual infor-mation on collaborative reference.
InCHI 2007 Pro-ceedings, pages 1543?1552.Patrick G. T. Healey and Greg J.
Mills.
2006.
Partic-ipation, precedence and co-ordination in dialogue.In Proceedings of Cognitive Science, pages 1470?1475.Jerry R. Hobbs, Mark Stickel, Douglas Appelt, andPaul Martin.
1993.
Interpretation as abduction.
Ar-tificial Intelligence, 63:69?142.Pamela W. Jordan and Marilyn A. Walker.
2005.Learning content selection rules for generating ob-ject descriptions in dialogue.
JAIR, 24:157?194.Andrew McCallum.
2002.
MALLET: AMAchine learning for LanguagE toolkit.http://mallet.cs.umass.edu.Jorge Nocedal.
1980.
Updating quasi-newton matriceswith limited storage.
Mathematics of Computation,35(151):773?782.Tim Oates, Zachary Eyler-Walker, and Paul R. Co-hen.
2000.
Toward natural language interfaces forrobotic agents.
In Proc.
Agents, pages 227?228.Massimo Poesio and Ron Artstein.
2005.
Annotating(anaphoric) ambiguity.
In Proceedings of the Cor-pus Linguistics Conference.Massimo Poesio and Renata Vieira.
1998.
A corpus-based investigation of definite description use.
Com-putational Linguistics, 24(2):183?216.Deb Roy and Alex Pentland.
2002.
Learning wordsfrom sights and sounds: A computational model.Cognitive Science, 26(1):113?146.Matthew Saxton, Carmel Houston-Price, and NatashaDawson.
2005.
The prompt hypothesis: clarifica-tion requests as corrective input for grammatical er-rors.
Applied Psycholinguistics, 26(3):393?414.David Schlangen and Raquel Ferna?ndez.
2007.
Speak-ing through a noisy channel: Experiments on in-ducing clarification behaviour in human?human di-alogue.
In Proceedings of Interspeech 2007.Luc Steels and Tony Belpaeme.
2005.
Coordinatingperceptually grounded categories through language.a case study for colour.
Behavioral and Brain Sci-ences, 28(4):469?529.Richmond H. Thomason, Matthew Stone, and DavidDeVault.
2006.
Enlightened update: Acomputational architecture for presupposition andother pragmatic phenomena.
For the OhioState Pragmatics Initiative, 2006, available athttp://www.research.rutgers.edu/?ddevault/.Ellen M. Vorhees.
1999.
The TREC-8 question an-swering track report.
In Proceedings of the 8th TextRetrieval Conference, pages 77?82.Jason Williams and Steve Young.
2007.
Partiallyobservable markov decision processes for spokendialog systems.
Computer Speech and Language,21(2):393?422.Chen Yu and Dana H. Ballard.
2004.
A multimodallearning interface for grounding spoken language insensory perceptions.
ACM Transactions on AppliedPerception, 1:57?80.192
