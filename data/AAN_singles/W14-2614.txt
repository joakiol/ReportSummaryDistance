Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 79?83,Baltimore, Maryland, USA.
June 27, 2014.c?2014 Association for Computational LinguisticsCredibility Adjusted Term Frequency: A Supervised Term WeightingScheme for Sentiment Analysis and Text ClassificationYoon KimNew York Universityyhk255@nyu.eduOwen Zhangzhonghua.zhang2006@gmail.comAbstractWe provide a simple but novel supervisedweighting scheme for adjusting term fre-quency in tf-idf for sentiment analysisand text classification.
We compare ourmethod to baseline weighting schemes andfind that it outperforms them on multiplebenchmarks.
The method is robust andworks well on both snippets and longerdocuments.1 IntroductionBaseline discriminative methods for text classifi-cation usually involve training a linear classifierover bag-of-words (BoW) representations of doc-uments.
In BoW representations (also known asVector Space Models), a document is representedas a vector where each entry is a count (or binarycount) of tokens that occurred in the document.Given that some tokens are more informative thanothers, a common technique is to apply a weight-ing scheme to give more weight to discriminativetokens and less weight to non-discriminative ones.Term frequency-inverse document frequency (tf-idf ) (Salton and McGill, 1983) is an unsupervisedweighting technique that is commonly employed.In tf-idf, each token i in document d is assignedthe following weight,wi,d= tfi,d?
logNdfi(1)where tfi,dis the number of times token i occurredin document d, N is the number of documents inthe corpus, and dfiis the number of documents inwhich token i occurred.Many supervised and unsupervised variants oftf-idf exist (Debole and Sebastiani (2003); Mar-tineau and Finin (2009); Wang and Zhang (2013)).The purpose of this paper is not to performan exhaustive comparison of existing weightingschemes, and hence we do not list them here.
In-terested readers are directed to Paltoglou and Thel-wall (2010) and Deng et al.
(2014) for comprehen-sive reviews of the different schemes.In the present work, we propose a simple butnovel supervised method to adjust the term fre-quency portion in tf-idf by assigning a credibil-ity adjusted score to each token.
We find thatit outperforms the traditional unsupervised tf-idfweighting scheme on multiple benchmarks.
Thebenchmarks include both snippets and longer doc-uments.
We also compare our method againstWang and Manning (2012)?s Naive-Bayes SupportVector Machine (NBSVM), which has achievedstate-of-the-art results (or close to it) on manydatasets, and find that it performs competitivelyagainst NBSVM.
We additionally find that thetraditional tf-idf performs competitively againstother, more sophisticated methods when used withthe right scaling and normalization parameters.2 The MethodConsider a binary classification task.
Let Ci,kbethe count of token i in class k, with k ?
{?1, 1}.Denote Cito be the count of token i over bothclasses, and y(d)to be the class of document d.For each occurrence of token i in the training set,we calculate the following,s(j)i={Ci,1Ci, if y(d)= 1Ci,?1Ci, if y(d)= ?1(2)Here, j is the j-th occurrence of token i. Sincethere are Cisuch occurrences, j indexes from 1 toCi.
We assign a score to token i by,s?i=1CiCi?j=1s(j)i(3)Intuitively, s?iis the average likelihood of mak-ing the correct classification given token i?s occur-rence in the document, if i was the only token in79the document.
In a binary classification case, thisreduces to,s?i=C2i,1+ C2i,?1C2i(4)Note that by construction, the support of s?iis[0.5, 1].2.1 Credibility AdjustmentSuppose s?i= s?j= 0.75 for two different tokensi and j, but Ci= 5 and Cj= 100.
Intuition sug-gests that s?jis a more credible score than s?i, andthat s?ishould be shrunk towards the populationmean.
Let s?
be the (weighted) population mean.That is,s?
=?iCi?
s?iC(5)where C is the count of all tokens in the corpus.We define credibility adjusted score for token i tobe,si=C2i,1+ C2i,?1+ s?
?
?C2i+ ?
(6)where ?
is an additive smoothing parameter.
IfCi,k?s are small, then si?
s?
(otherwise, si?
s?i).This is a form of Buhlmann credibility adjustmentfrom the actuarial literature (Buhlmann and Gisler,2005).
We subsequently define tf , the credibilityadjusted term frequency, to be,tfi,d= (0.5 + s?i) ?
tfi,d(7)and tf is replaced with tf .
That is,wi,d= tfi,d?
logNdfi(8)We refer to above as cred-tf-idf hereafter.2.2 Sublinear ScalingIt is common practice to apply sublinear scaling totf .
A word occurring (say) ten times more in adocument is unlikely to be ten times as important.Paltoglou and Thelwall (2010) confirm that sub-linear scaling of term frequency results in signif-icant improvements in various text classificationtasks.
We employ logarithmic scaling, where tf isreplaced with log(tf) + 1.
For our method, tf issimply replaced with log(tf) + 1.
We found vir-tually no difference in performance between logscaling and other sublinear scaling methods (suchas augmented scaling, where tf is replaced with0.5 +0.5+tfmax tf).2.3 NormalizationUsing normalized features resulted in substan-tial improvements in performance versus usingun-normalized features.
We thus use?x(d)=x(d)/||x(d)||2in the SVM, where x(d)is the fea-ture vector obtained from cred-tf-idf weights fordocument d.2.4 Naive-Bayes SVM (NBSVM)Wang and Manning (2012) achieve excellent(sometimes state-of-the-art) results on manybenchmarks using binary Naive Bayes (NB) log-count ratios as features in an SVM.
In their frame-work,wi,d= 1{tfi,d} log(dfi,1+ ?
)/?i(dfi,1+ ?
)(dfi,?1+ ?
)/?i(dfi,?1+ ?
)(9)where dfi,kis the number of documents that con-tain token i in class k, ?
is a smoothing parameter,and 1{?}
is the indicator function equal to one iftfi,d> 0 and zero otherwise.
As an additionalbenchmark, we implement NBSVM with ?
= 1.0and compare against our results.13 Datasets and Experimental SetupWe test our method on both long and short textclassification tasks, all of which were used to es-tablish baselines in Wang and Manning (2012).Table 1 has summary statistics of the datasets.
Thesnippet datasets are:?
PL-sh: Short movie reviews with one sen-tence per review.
Classification involves de-tecting whether a review is positive or nega-tive.
(Pang and Lee, 2005).2?
PL-sub: Dataset with short subjective moviereviews and objective plot summaries.
Clas-sification task is to detect whether the sen-tence is objective or subjective.
(Pang andLee, 2004).And the longer document datasets are:1Wang and Manning (2012) use the same ?
but they dif-fer from our NBSVM in two ways.
One, they use l2hingeloss (as opposed to l1loss in this paper).
Two, they in-terpolate NBSVM weights with Multivariable Naive Bayes(MNB) weights to get the final weight vector.
Further, theirtokenization is slightly different.
Hence our NBSVM resultsare not directly comparable.
We list their results in table 2.2https://www.cs.cornell.edu/people/pabo/movie-review-data/.
All the PL datasets are available here.80Dataset Length Pos Neg TestPL-sh 21 5331 5331 CVPL-sub 24 5000 5000 CVPL-2k 746 1000 1000 CVIMDB 231 12.5k 12.5k 25kAthR 355 480 377 570XGraph 276 584 593 784Table 1: Summary statistics for the datasets.Length is the average number of unigram tokens(including punctuation) per document.
Pos/Neg isthe number of positive/negative documents in thetraining set.
Test is the number of documents inthe test set (CV means that there is no separatetest set for this dataset and thus a 10-fold cross-validation was used to calculate errors).?
PL-2k: 2000 full-length movie reviews thathas become the de facto benchmark for sen-timent analysis (Pang and Lee, 2004).?
IMDB: 50k full-length movie reviews (25ktraining, 25k test), from IMDB (Maas et al.,2011).3?
AthR, XGraph: The 20-Newsgroup dataset,2nd version with headers removed.4Clas-sification task is to classify which topic adocument belongs to.
AthR: alt.atheism vsreligion.misc, XGraph: comp.windows.x vscomp.graphics.3.1 Support Vector Machine (SVM)For each document, we construct the feature vec-tor x(d)using weights obtained from cred-tf-idfwith log scaling and l2normalization.
For cred-tf-idf, ?
is set to 1.0.
NBSVM and tf-idf (also withlog scaling and l2normalization) are used to es-tablish baselines.
Prediction for a test document isgiven byy(d)= sign (wTx(d)+ b) (10)In all experiments, we use a Support Vector Ma-chine (SVM) with a linear kernel and penalty pa-rameter of C = 1.0.
For the SVM, w, b are ob-tained by minimizing,wTw+CN?d=1max(0, 1?y(d)(wTx(d)+b)) (11)using the LIBLINEAR library (Fan et al., 2008).3http://ai.stanford.edu/ amaas/data/sentiment/index.html4http://people.csail.mit.edu/jrennie/20Newsgroups3.2 TokenizationWe lower-case all words but do not perform anystemming or lemmatization.
We restrict the vo-cabulary to all tokens that occurred at least twicein the training set.4 Results and DiscussionFor PL datasets, there are no separate test sets andhence we use 10-fold cross validation (as do otherpublished results) to estimate errors.
The standardtrain-test splits are used on IMDB and Newsgroupdatasets.4.1 cred-tf-idf outperforms tf-idfTable 2 has the comparison of results for the dif-ferent datasets.
Our method outperforms the tra-ditional tf-idf on all benchmarks for both uni-grams and bigrams.
While some of the differ-ences in performance are significant at the 0.05level (e.g.
IMDB), some are not (e.g.
PL-2k).
TheWilcoxon signed ranks test is a non-parametrictest that is often used in cases where two classi-fiers are compared over multiple datasets (Dem-sar, 2006).
The Wilcoxon signed ranks test indi-cates that the overall outperformance is significantat the <0.01 level.4.2 NBSVM outperforms cred-tf-idfcred-tf-idf did not outperform Wang and Manning(2012)?s NBSVM (Wilcoxon signed ranks test p-value = 0.1).
But it did outperform our own im-plementation of NBSVM, implying that the ex-tra modifications by Wang and Manning (2012)(i.e.
using squared hinge loss in the SVM and in-terpolating between NBSVM and MNB weights)are important contributions of their methodology.This was especially true in the case of shorter doc-uments, where our uninterpolated NBSVM per-formed significantly worse than their interpolatedNBSVM.4.3 tf-idf still performs wellWe find that tf-idf still performs remarkably wellwith the right scaling and normalization parame-ters.
Indeed, the traditional tf-idf outperformedmany of the more sophisticated methods thatemploy distributed representations (Maas et al.
(2011); Socher et al.
(2011)) or other weightingschemes (Martineau and Finin (2009); Deng et al.
(2014)).81Method PL-sh PL-sub PL-2k IMDB AthR XGraphtf-idf-uni 77.1 91.5 88.1 88.6 85.8 88.4tf-idf-bi 78.0 92.3 89.2 90.9 86.5 88.0Our cred-tfidf-uni 77.5 91.8 88.7 88.8 86.5 89.8results cred-tfidf-bi 78.6 92.8 89.7 91.3 87.4 88.9NBSVM-uni 75.5 89.9 87.0 85.9 86.7 88.5NBSVM-bi 76.0 90.5 89.5 90.5 86.7 88.1MNB-uni 77.9 92.6 83.5 83.6 85.0 90.0Wang & MNB-bi 79.0 93.6 85.9 86.6 85.1 91.2Manning NBSVM-uni 78.1 92.4 87.8 88.3 87.9 91.2NBSVM-bi 79.4 93.2 89.5 91.2 87.7 90.7Appr.
Tax.
* - - 90.2 - - -Str.
SVM* - - 92.4 - - -aug-tf-mi - - 87.8 88.0 - -Other Disc.
Conn. - - - 91.4 - -results Word Vec.
* - 88.6 88.9 88.9 - -LLR - - 90.4 - - -RAE 77.7 - - - - -MV-RNN 79.0 - - - - -Table 2: Results of our method (cred-tf-idf ) against baselines (tf-idf, NBSVM), using unigrams andbigrams.
cred-tf-idf and tf-idf both use log scaling and l2normalization.
Best results (that do not useexternal sources) are underlined, while top three are in bold.
Rows 7-11 are MNB and NBSVM resultsfrom Wang and Manning (2012).
Our NBSVM results are not directly comparable to theirs (see footnote1).
Methods with * use external data or software.
Appr.
Tax: Uses appraisal taxonomies from WordNet(Whitelaw et al., 2005).
Str.
SVM: Uses OpinionFinder to find objective versus subjective parts of thereview (Yessenalina et al., 2010).
aug-tf-mi: Uses augmented term-frequency with mutual informationgain (Deng et al., 2014).
Disc.
Conn.: Uses discourse connectors to generate additional features (Trivediand Eisenstein, 2013).
Word Vec.
: Learns sentiment-specific word vectors to use as features combinedwith BoW features (Maas et al., 2011).
LLR: Uses log-likelihood ratio on features to select features(Aue and Gamon, 2005).
RAE: Recursive autoencoders (Socher et al., 2011).
MV-RNN: Matrix-VectorRecursive Neural Networks (Socher et al., 2012).5 Conclusions and Future WorkIn this paper we presented a novel supervisedweighting scheme, which we call credibility ad-justed term frequency, to perform sentiment anal-ysis and text classification.
Our method outper-forms the traditional tf-idf weighting scheme onmultiple benchmarks, which include both snippetsand longer documents.
We also showed that tf-idfis competitive against other state-of-the-art meth-ods with the right scaling and normalization pa-rameters.From a performance standpoint, it would be in-teresting to see if our method is able to achieveeven better results on the above tasks with propertuning of the ?
parameter.
Relatedly, our methodcould potentially be combined with other super-vised variants of tf-idf, either directly or throughensembling, to improve performance further.ReferencesA.
Aue, M. Gamon.
2005.
Customizing sentimentclassifiers to new domains: A case study.
Proceed-ings of the International Conference on Recent Ad-vances in NLP, 2011.H.
Buhlmann, A. Gisler.
2005.
A Course in Credi-bility Theory and its Applications Springer-Verlag,Berlin.F.
Debole, F. Sebastiani.
2003.
Supervised TermWeighting for Automated Text Categorization Pro-ceedings of the 2003 ACM symposium on AppliedComputing.
784?788.J.
Demsar.
2006.
Statistical Comparison of classifiersover multiple data sets.
Journal of Machine Learn-ing Research, 7:1-30.
2006.Z.
Deng, K. Luo, H. Yu.
2014.
A study of supervisedterm weighting scheme for sentiment analysis Ex-82pert Systems with Applications.
Volume 41, Issue 7,3506?3513.R.
Fan, K. Chang, J. Hsieh, X. Wang, C. Lin.
2008.
LI-BLINEAR: A library for large linear classification.Journal of Machine Learning Research, 9:1871?1874, June.A.
Maas, R. Daly, P. Pham, D. Huang, A. Ng, C. Potts.2011.
Learning Word Vectors for Sentiment Analy-sis.
In Proceedings of ACL 2011.J.
Martineau, T. Finin.
2009.
Delta TFIDF: An Im-proved Feature Space for Sentiment Analysis.
ThirdAAAI International Conference on Weblogs and So-cial MediaG.
Paltoglou, M. Thelwall.
2010.
A study of Infor-mation Retrieval weighting schemes for sentimentanalysis.
In Proceedings of ACL 2010.B.
Pang, L. Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proceedings of ACL2004.B.
Pang, L. Lee.
2005.
Seeing stars: Exploiting classrelationships for sentiment categorization with re-spect to rating scales.
In Proceedings of ACL 2005.R.
Socher, J. Pennington, E. Huang, A. Ng, C. Man-ning.
2011.
Semi-Supervised Recursive Autoen-coders for Predicting Sentiment Distributions.
InProceedings of EMNLP 2011.R.
Socher, B. Huval, C. Manning, A. Ng.
2012.
Se-mantic Compositionality through Recursive Matrix-Vector Spaces.
In Proceedings of EMNLP 2012.R.
Trivedi, J. Eisenstein.
2013.
Discourse Connec-tors for Latent Subjectivity in Sentiment Analysis.In Proceedings of NAACL 2011.G.
Salton, M. McGill.
1983.
Introduction to ModernInformation Retrieval.
McGraw-Hill.S.
Wang, C. Manning.
2012.
Baselines and Bigrams:Simple, Good Sentiment and Topic Classification.In proceedings of ACL 2012.D.
Wang, H. Zhang.
2013.
Inverse-Category-Frequency Based Supervised Term WeightingSchemes for Text Categorization.
Journal of Infor-mation Science and Engineering 29, 209?225.C.
Whitelaw, N. Garg, S. Argamon.
2005.
Using ap-praisal taxonomies for sentiment analysis.
In Pro-ceedings of CIKM 2005.A.
Yessenalina, Y. Yue, C. Cardie.
2010.
Multi-level Structured Models for Document-level Senti-ment Classification.
In Proceedings of ACL 2010.83
