Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 41?48,Rochester, New York, April 2007. c?2007 Association for Computational LinguisticsMachine Translation as Tree LabelingMark HopkinsDepartment of LinguisticsUniversity of Potsdam, Germanyhopkins@ling.uni-potsdam.deJonas KuhnDepartment of LinguisticsUniversity of Potsdam, Germanykuhn@ling.uni-potsdam.deAbstractWe present the main ideas behind a newsyntax-based machine translation system,based on reducing the machine translationtask to a tree-labeling task.
This tree la-beling is further reduced to a sequence ofdecisions (of four varieties), which can bediscriminatively trained.
The optimal treelabeling (i.e.
translation) is then foundthrough a simple depth-first branch-and-bound search.
An early system foundedon these ideas has been shown to becompetitive with Pharaoh when both aretrained on a small subsection of the Eu-roparl corpus.1 MotivationStatistical machine translation has, for a while now,been dominated by the phrase-based translation par-adigm (Och and Ney, 2003).
In this paradigm,sentences are translated from a source language toa target language through the repeated substitutionof contiguous word sequences (?phrases?)
from thesource language for word sequences in the targetlanguage.
Training of the phrase translation modelbuilds on top of a standard statistical word align-ment over the training corpus for identifying corre-sponding word blocks, assuming no further linguis-tic analysis of the source or target language.
In de-coding, these systems then typically rely on n-gramlanguage models and simple statistical reorderingmodels to shuffle the phrases into an order that iscoherent in the target language.There are limits to what such an approach can ul-timately achieve.
Machine translation based on adeeper analysis of the syntactic structure of a sen-tence has long been identified as a desirable objec-tive in principle (consider (Wu, 1997; Yamada andKnight, 2001)).
However, attempts to retrofit syn-tactic information into the phrase-based paradigmhave not met with enormous success (Koehn et al,2003; Och et al, 2003)1, and purely phrase-basedmachine translation systems continue to outperformthese syntax/phrase-based hybrids.In this work, we try to make a fresh start withsyntax-based machine translation, discarding thephrase-based paradigm and designing a machinetranslation system from the ground up, using syntaxas our central guiding star.
Evaluation with BLEUand a detailed manual error analysis of our nascentsystem show that this new approach might well havethe potential to finally realize some of the promisesof syntax.2 Problem FormulationWe want to build a system that can learn to translatesentences from a source language to a destinationlanguage.
As our first step, we will assume that thesystem will be learning from a corpus consisting oftriples ?f, e, a?, where: (i) f is a sentence from oursource language, which is parsed (the words of thesentence and the nodes of the parse tree may or maynot be annotated with auxiliary information), (ii) e isa gold-standard translation of sentence f (the wordsof sentence e may or may not be annotated with aux-iliary information), and (iii) a is an automatically-generated word alignment (e.g.
via GIZA++) be-tween source sentence f and destination sentence e.1(Chiang, 2005) also reports that with his hierarchical gen-eralization of the phrase-based approach, the addition of parserinformation doesn?t lead to any improvements.41Figure 1: Example translation object.Let us refer to these triples as translation objects.The learning task is: using the training data, pro-duce a scoring function P that assigns a score toevery translation object ?f, e, a?, such that this scor-ing function assigns a high score to good transla-tions, and a low score to poor ones.
The decodingtask is: given scoring function P and an arbitrarysentence f from the source language, find transla-tion object ?f, e, a?
that maximizes P (?f, e, a?
).To facilitate matters, we will map translation ob-jects to an alternate representation.
In (Galley et al,2003), the authors give a semantics to every trans-lation object by associating each with an annotatedparse tree (hereafter called a GHKM tree) represent-ing a specific theory about how the source sentencewas translated into the destination sentence.In Figure 1, we show an example translation ob-ject and in Figure 2, we show its associated GHKMtree.
The GHKM tree is simply the parse tree f ofthe translation object, annotated with rules (hereafterreferred to as GHKM rules).
We will not describe indepth the mapping process from translation object toGHKM tree.
Suffice it to say that the alignment in-duces a set of intuitive translation rules.
Essentially,a rule like: ?not 1?
ne 1 pas?
(see Figure 2) means:if we see the word ?not?
in English, followed by aphrase already translated into French, then translatethe entire thing as the word ?ne?
+ the translatedphrase + the word ?pas.?
A parse tree node gets la-beled with one of these rules if, roughly speaking,its span is still contiguous when projected (via thealignment) into the target language.Formally, what is a GHKM tree?
Define a rule el-ement as a string or an indexed variable (e.g.
x1,x4, x32).
A GHKM rule of rank k (where k isa non-negative integer) is a pair ?Rs, Rd?, wheresource list Rs and destination list Rd are both listsof rule elements, such that each variable of Xk ,{x1, x2, ..., xk} appears exactly once in Rs and ex-actly once in Rd.
Moreover, in Rs, the variables ap-pear in ascending order.
In Figure 2, some of thetree nodes are annotated with GHKM rules.
Forclarity, we use a simplified notation.
For instance,rule ?
?x1, x2, x3?, ?x3, ?,?, x1, x2??
is represented as?1 2 3 ?
3 , 1 2?.
We have also labeled the nodeswith roman numerals.
When we want to refer to aparticular node in later examples, we will refer to it,e.g., as t(i) or t(vii).A rule node is a tree node annotated with aGHKM rule (for instance, nodes t(i) or t(v) of Fig-ure 2, but not node t(iv)).
A tree node t2 is reachablefrom tree node t1 iff node t2 is a proper descendantof node t1 and there is no rule node (not includingnodes t1, t2) on the path from node t1 to node t2.Define the successor list of a tree node t as the listof rule nodes and leaves reachable from t (ordered inleft-to-right depth-first search order).
For Figure 2,the successor list of node t(i) is ?t(ii), t(v), t(xiii)?,and the successor list of node t(v) is ?t(vii), t(viii)?.The rule node successor list of a tree node is its suc-cessor list, with all non-rule nodes removed.Define the signature of a parse tree node t as theresult of taking its successor list, replacing the jthrule node with variable xj , and replacing every non-rule node with its word label (observe that all non-rule nodes in the successor list are parse tree leaves,and therefore they have word labels).
For Figure 2,the signature of node t(i) is ?x1, x2, x3?, and the sig-nature of node t(v) is ?
?am?, x1?.Notice that the signature of every rule node in Fig-ure 2 coincides with the source list of its GHKMrule.
This is no accident, but rather a requirement.Define a GHKM tree node as a parse tree nodewhose children are all GHKM tree nodes, and whoseGHKM rule?s source list is equivalent to its signa-ture (if the node is a rule node).Given these definitions, we can proceed to definehow a GHKM tree expresses a translation theory.Suppose we have a list S = ?s1, ..., sk?
of strings.Define the substitution of string list S into rule ele-42Figure 2: GHKM tree equivalent of example translation object.
The light gray nodes are rule nodes of theGHKM tree.ment r as:r[S] = si if r is indexed var xir otherwiseNotice that this operation always produces astring.
Define the substitution of string list S intorule element list R = ?r1, ..., rj?
as:R[S] = concat(r1[S], r2[S], ..., rj [S])where concat(s1, ..., sk) is the spaced concatenationof strings s1, ..., sk (e.g., concat( ?hi?, ?there? )
=?hi there?).
This operation also produces a string.Finally, define the translation of GHKM tree nodet as:?
(t) , Rd[??
(t1), ..., ?(tk)?
]where ?t1, ..., tk?
is the rule node successor list ofGHKM tree node t.For Figure 2, the rule node successor list of nodet(viii) is ?t(xi)?.
So:?
(t(viii)) = ?
?ne?, x1, ?pas??[??(t(xi))?
]= ?
?ne?, x1, ?pas??[??vais??
]= ?ne vais pas?A similar derivation gives us:?
(t(i)) = ?aujourd?hui , je ne vais pas?In this way, every GHKM tree encodes a transla-tion.
Given this interpretation of a translation object,the task of machine translation becomes somethingconcrete: label the nodes of a parsed source sentencewith a good set of GHKM rules.3 Probabilistic ApproachTo achieve this ?good?
labeling of GHKM rules,we will define a probabilistic generative model Pof GHKM trees, which will serve as our scoringfunction.
We would like to depart from the stan-dard probabilistic approach of most phrase-basedtranslators, which employ very simple probabilitymodels to enable polynomial-time decoding.
In-stead, we will use an alternative probabilistic ap-proach (an assignment process), which sacrificespolynomial-time guarantees in favor of a more flexi-ble and powerful model.
This sacrifice of guaranteedpolynomial-time decoding does not entail the sacri-fice of good running time in practice.3.1 Assignment ProcessesAn assignment process builds a sequence of vari-able assignments (called an assignment history) byrepeatedly iterating the following steps.
First, it re-quests a variable name (say x22) from a so-namedvariable generator.
It takes this variable nameand the assignment history built so far and com-presses this information into a set of features (say{f2, f6, f80}) using a feature function.
These fea-tures are then mapped to a probability distribution bya function (say p7) requested from a so-named distri-bution generator.
The iteration ends by assigning tothe chosen variable a value (say v4) drawn from thisdistribution.
In the above running example, the iter-ation assigns v4 to x22, which was drawn accordingto distribution p7({f2, f6, f80}).
The process endswhen the variable generator produces the reservedtoken STOP instead of a variable name.
At this43Var Assignment Distribution Featuresx23 true p4 {}x7 ?the?
p10 {f12, f102}x8 blue p2 {f5, f55}x51 red p2 {f5, f15, f50}x19 7.29 p5 {f2}x30 false p4 {f2, f5, f7}x1 ?man?
p10 {f1, f2, f12}x102 blue p2 {f1, f55, f56}Figure 3: A example assignment history generatedby an assignment process.point, the assignment history built so far (like theexample in Figure 3) is returned.Formally, define a variable signature as a pair?
= ?X, V ?, where X is a set of variable namesand V is a set of values.
Define a variable assign-ment of signature ?X, V ?
as a pair ?x, v?, for vari-able x ?
X and value v ?
V .
Define an assignmenthistory of signature ?
as an ordered list of variableassignments of ?.
The notation H(?)
represents theset of all assignment histories of signature ?.We define a feature function of signature ?
=?X, V ?
as a function f that maps every pair of setX ?H(?)
to a set of assignments (called features)of an auxiliary variable signature ?f .We define an assignment process of signature?
= ?X, V ?
as a tuple ?f, P, gx, gp?, where: (i) f isa feature function of ?, (ii) P = {p1, ..., pk} is a fi-nite set of k functions (called the feature-conditionaldistributions) that map each feature set in range(f)to a probability distribution over V , (iii) gx is a func-tion (called the variable generator) mapping eachassignment history in the set H(?)
to either a vari-able name in X or the reserved token STOP , and(iv) gp is a function (called the distribution gener-ator) mapping each assignment history in the setH(?)
to a positive integer between 1 and k.An assignment process probabilistically generatesan assignment history of signature ?
in the follow-ing way:1. h?
empty list2.
Do until gx(h) = STOP :(a) Let x = gx(h) and let j = gp(h).
(b) Draw value v probabilistically from distri-bution pj(f(x, h)).
(c) Append assignment ?x, v?
to history h.3.
Return history h.3.2 TrainingGiven all components of an assignment processof signature ?
except for the set P of feature-conditional distributions, the training task is to learnP from a training corpus of assignment histories ofsignature ?.
This can be achieved straightforwardlyby taking the feature vectors generated by a partic-ular distribution and using them to discriminativelylearn the distribution.
For instance, say that our cor-pus consists of the single history given in Figure ?
?.To learn distribution p2, we simply take the threevariable assignments produced by p2 and feed thesefeature vectors to a generic discriminative learner.We prefer learners that produce distributions (ratherthan hard classifiers) as output, but this is not re-quired.3.3 DecodingNotice that an assignment process of signature ?
in-duces a probability distribution over the set H(?)
ofall assignment histories of ?.
The decoding ques-tion is: given a partial assignment history h, whatis the most probable completion of the history, ac-cording to this induced distribution?
We will usethe natural naive search space for this question.
Thenodes of this search space are the assignment his-tories of H(?).
The children of the search noderepresenting history h are those histories that can begenerated from h in one iteration of the assignmentprocess.
The value of a search node is the proba-bility of its assignment history (according to the as-signment process).
To decode, we begin at the noderepresenting history h, and search for the highest-value descendant that represents a complete assign-ment history (i.e.
an assignment history terminatedby the STOP token).This is, potentially, a very large and intractiblesearch space.
However, if most assignment deci-sions can be made with relative confidence, then thegreat majority of search nodes have values whichare inferior to those of the best solutions.
Thestandard search technique of depth-first branch-and-bound search takes advantage of search spaces withthis particular characteristic by first finding greedygood-quality solutions and using their values to opti-mally prune a significant portion of the search space.44Figure 4: Partial GHKM tree, after rule nodes have been identified (light gray).
Notice that once we identifythe rule node, the rule left-hand sides are already determined.Depth-first branch-and-bound search has the follow-ing advantage: it finds a good (suboptimal) solutionin linear time and continually improves on this solu-tion until it finds the optimal.
Thus it can be run ei-ther as an optimal decoder or as a heuristic decoder,since we can interrupt its execution at any time to getthe best solution found so far.
Additionally, it takesonly linear space to run.4 Generative ModelWe now return to where we left off at the end of Sec-tion 2, and devise an assignment process that pro-duces a GHKM tree from an unlabeled parse tree.This will give us a quality measure that we can useto produce a ?good?
labeling of a given parse treewith GHKM rules (i.e., the probability of such a la-beling according to the assignment process).The simplest assignment process would have avariable for each node of the parse tree, and thesevariables would all be assigned by the same feature-conditional distribution over the space of all possibleGHKM rules.
The problem with such a formulationis that such a distribution would be inachievably dif-ficult to learn.
We want an assignment process inwhich all variables can take only a very small num-ber of possible values, because it will be much eas-ier to learn distributions over such variables.
Thismeans we need to break down the process of con-structing a GHKM rule into simpler steps.Our assignment process will begin by sequen-tially assigning a set of boolean variables (which wewill call rule node indicator variables), one for eachnode in the parse tree.
For parse tree node t, we de-note its corresponding rule node indicator variablexrt .
Variable xrt is assigned true iff the parse treenode t will be a rule node in the GHKM tree.In Figure 3.3, we show a partial GHKM tree af-ter these assignments are made.
The key thing toobserve is that, after this sequence of boolean deci-sions, the LHS of every rule in the tree is alreadydetermined!
To complete the tree, all we need to dois to fill in their right-hand sides.Again, we could create variables to do this di-rectly, i.e.
have a variable for each rule whose do-main is the space of possible right-hand sides for itsestablished left-hand sides.
But this is still a wide-open decision, so we will break it down further.For each rule, we will begin by choosing thetemplate of its RHS, which is a RHS in whichall sequences of variables are replaced with anempty slot into which variables can later be placed.For instance, the template of ?
?ne?, x1, ?pas??
is?
?ne?, X, ?pas??
and the template of ?x3, ?,?, x1, x2?is ?X, ?,?, X?, where X represents the empty slots.Once the template is chosen, it simply needs to befilled with the variables from the LHS.
To do so, weprocess the LHS variables, one by one.
By default,they are placed to the right of the previously placedvariable (the first variable is placed in the first slot).We repeatedly offer the option to push the variableto the right until the option is declined or it is nolonger possible to push it further right.
If the vari-able was not pushed right at all, we repeatedly offerthe option to push the variable to the left until theoption is declined or it is no longer possible to pushit further left.
Figure 4 shows this generative storyin action for the rule RHS ?x3, ?,?, x1, x2?.These are all of the decisions we need to make45Decision to make Decision RHS so farRHS template?
X , X X , Xdefault placement of var 1 1 , Xpush var 1 right?
yes X , 1default placement of var 2 X , 1 2push var 2 left?
no X , 1 2default placement of var 3 X , 1 2 3push var 3 left?
yes X , 1 3 2push var 3 left?
yes X , 3 1 2push var 3 left?
yes 3 , 1 2Figure 5: Trace of the generative story for the right-hand side of a GHKM rule.in order to label a parse tree with GHKM rules.
No-tice that, aside from the template decisions, all of thedecisions are binary (i.e.
feasible to learn discrimi-natively).
Even the template decisions are not terri-bly large-domain, if we maintain a separate feature-conditional distribution for each LHS template.
Forinstance, if the LHS template is ?
?not?, X?, thenRHS template ?
?ne?, X, ?pas??
and a few other se-lect candidates should bear most of the probabilitymass.5 EvaluationIn this section, we evaluate a preliminary English-to-German translation system based on the ideasoutlined in this paper.
We first present a quantia-tive comparison with the phrase-based approach, us-ing the BLEU metric; then we discuss two con-crete translation examples as a preliminary qualita-tive evaluation.
Finally, we present a detailed man-ual error analysis.Our data was a subset of the Europarl corpus con-sisting of sentences of lengths ranging from 8 to 17words.
Our training corpus contained 50000 sen-tences and our test corpus contained 300 sentences.We also had a small number of reserved sentencesfor development.
The English sentences were parsedusing the Bikel parser (Bikel, 2004), and the sen-tences were aligned with GIZA++ (Och and Ney,2000).
We used the WEKA machine learning pack-age (Witten and Frank, 2005) to train the distribu-tions (specifically, we used model trees).For comparison, we also trained and evaluatedPharaoh (Koehn, 2005) on this limited corpus, us-ing Pharaoh?s default parameters.
Pharaoh achieveda BLEU score of 11.17 on the test set, whereas oursystem achieved a BLEU score of 11.52.
What isnotable here is not the scores themselves (low due tothe size of the training corpus).
However our systemmanaged to perform comparably with Pharaoh in avery early stage of its development, with rudimen-tary features and without the benefit of an n-gramlanguage model.Let?s take a closer look at the sentences producedby our system, to gain some insight as to its currentstrengths and weaknesses.Starting with the English sentence (note that alldata is lowercase):i agree with the spirit of those amendments .Our system produces:ichIstimmevotediethe.FEMgeistspirit.MASCdieserthesea?nderungsantra?gechange-proposalszuto..The GHKM tree is depicted in Figure 5.
The keyfeature of this translation is how the English phrase?agree with?
is translated as the German ?stimme... zu?
construction.
Such a feat is difficult to pro-duce consistently with a purely phrase-based sys-tem, as phrases of arbitrary length can be placed be-tween the words ?stimme?
and ?zu?, as we can seehappening in this particular example.
By contrast,Pharaoh opts for the following (somewhat less de-sirable) translation:ichIstimmevotemitwithdemthe.MASCgeistspirit.MASCdieserthesea?nderungsantra?gechange-proposals..A weakness in our system is also evident here.The German noun ?Geist?
is masculine, thus oursystem uses the wrong article (a problem thatPharaoh, with its embedded n-gram language model,does not encounter).In general, it seems that our system is superior toPharaoh at figuring out the proper way to arrange thewords of the output sentence, and inferior to Pharaohat finding what the actual translation of those wordsshould be.Consider the English sentence:we shall submit a proposal along these lines beforethe end of this year .46Figure 6: GHKM tree output for the first test sentence.Here we have an example of a double verb: ?shallsubmit.?
In German, the second verb should go atthe end of the sentence, and this is achieved by oursystem (translating ?shall?
as ?werden?, and ?sub-mit?
as ?vorlegen?
).wirwewerdenwilleinea.FEMvorschlagproposal.MASCinindieserthesehaushaltslinienbudget-linesvorbeforediethe.FEMendeend.NEUTdieserthis.FEMjahresyear.NEUTvorlegensubmit..Pharaoh does not manage this (translating ?sub-mit?
as ?unterbreiten?
and placing it mid-sentence).werdenwillwirweunterbreitensubmiteineavorschlagproposalinindieserthesehaushaltslinienbudget-linesvorbeforeendeenddieserthis.FEMjahryear.NEUT..It is worth noting that while our system gets theword order of the output system right, it makes sev-eral agreement mistakes and (like Pharaoh) doesn?tget the translation of ?along these lines?
right.To have a more systematic basis for comparison,we did a manual error analysis for 100 sentencesfrom the test set.
A native speaker of German (in thepresent pilot study one of the authors) determinedthe editing steps required to transform the systemoutput into an acceptable translation ?
both in termsof fluency and adequacy of translation.
In order toavoid a bias for our system, we randomized the pre-sentation of output from one of the two systems.We defined the following basic types of edits, withfurther subdistinctions depending on the word type:ADD, DELETE, CHANGE and MOVE.
A special typeTRANSLATE-untranslated was assumed for untrans-lated source words in the output.
For the CHANGE,more fine-grained distinctions were made.2 A sin-gle MOVE operation was assumed to displace an en-tire phrase; the distance of the movement in termsof the number of words was calculated.
The table inFigure 7 shows the edits required for correcting theoutput of the two systems on 100 sentences.We again observe that our system, which is atan early stage of development and contrary to thePharaoh system does not include an n-gram lan-guage model trained on a large corpus, alreadyyields promising results.
The higher proportionof CHANGE operations, in particular CHANGE-inflection and CHANGE-function-word edits is pre-sumably a direct consequence of providing a lan-guage model or not.
An interesting observation isthat our system currently tends to overtranslate, i.e.,redundantly produce several translations for a word,which leads to the need of DELETE operations.
ThePharaoh system had a tendency to undertranslate, of-ten with crucial words missing.2CHANGE-inflection: keeping the lemma and category thesame, e.g.
taken ?
takes; CHANGE-part-of-speech: choos-ing a different derivational form, e.g., judged ?
judgement;CHANGE-function-word: e.g., in ?
from; CHANGE-content-word: e.g., opinion ?
consensus.47TL-MT PharaohADD-function-word 40 49ADD-content-word 17 35ADD-punctuation 12 13ADD (total) 69 97DELETE-function-word 37 18DELETE-content-word 22 10DELETE-punctuation 13 15DELETE-untranslated 2 1DELETE (total) 74 44CHANGE-content-word 24 19CHANGE-function-word 44 26CHANGE-inflection 101 80CHANGE-part-of-speech 4 10CHANGE (total) 173 135TRANSLATE-untranslated 34 1MOVE (distance)1 16 172 12 163 13 114 3 6?
5 7 5MOVE (total) 51 55TOTAL # EDITS 401 332edits-per-word ratio 0.342 0.295Figure 7: Edits required for an acceptable systemoutput, based on 100 test sentences.6 DiscussionIn describing this pilot project, we have attemptedto give a ?big picture?
view of the essential ideasbehind our system.
To avoid obscuring the presen-tation, we have avoided many of the implementationdetails, in particular our choice of features.
Thereare exactly four types of decisions that we need totrain: (1) whether a parse tree node should be a rulenode, (2) the RHS template of a rule, (3) whether arule variable should be pushed left, and (4) whethera rule variable should be pushed right.
For each ofthese decisions, there are a number of possible fea-tures that suggest themselves.
For instance, recallthat in German, typically the second verb of a doubleverb (such as ?shall submit?
or ?can do?)
gets placedat the end of the sentence or clause.
So when thesystem is considering whether to push a rule?s nounphrase to the left, past an existing verb, it would beuseful for it to consider (as a feature) whether thatverb is the first or second verb of its clause.This system was designed to be very flexible withthe kind of information that it can exploit as fea-tures.
Essentially any aspect of the parse tree, orof previous decisions that have been taken by theassignment process, can be used.
Furthermore, wecan mark-up the parse tree with any auxiliary infor-mation that might be beneficial, like noun gender orverb cases.
The current implementation has hardlybegun to explore these possibilities, containing onlyfeatures pertaining to aspects of the parse tree.Even in these early stages of development, thesystem shows promise in using syntactic informa-tion flexibly and effectively for machine translation.We hope to develop the system into a competitivealternative to phrase-based approaches.ReferencesDaniel M. Bikel.
2004.
Intricacies of Collins?
parsing model.Computational Linguistics, 30(4):479?511.David Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In Proceedings of ACL, pages263?270.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2003.
What?s in a translation rule?
In Proc.
NAACL.Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003.
Sta-tistical phrase-based translation.
In Proceedings of the Hu-man Language Technology Conference 2003 (HLT-NAACL2003), Edmonton, Canada.Philipp Koehn.
2005.
Pharaoh: a beam search decoder forphrase-based statistical machine translation models.
In Pro-ceedings of the Sixth Conference of the Association for Ma-chine Translation in the Americas, pages 115?124.F.
J. Och and H. Ney.
2000.
Improved statistical alignmentmodels.
In Proc.
ACL, pages 440?447, Hongkong, China,October.Franz Josef Och and Hermann Ney.
2003.
A systematic com-parison of various statistical alignment models.
Computa-tional Linguistics, 29(1):19?51.F.
J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,A.
Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, VirenJain, Z.Jin, and D. Radev.
2003.
Syntax for statistical ma-chine translation.
Technical report, Center for Language andSpeech Processing, Johns Hopkins University, Baltimore.Summer Workshop Final Report.Ian H. Witten and Eibe Frank.
2005.
Data Mining: Practicalmachine learning tools and techniques.
Morgan Kaufmann.Dekai Wu.
1997.
Stochastic inversion transduction grammarsand bilingual parsing of parallel corpora.
ComputationalLinguistics, 23(3):377?403.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statis-tical translation model.
In Proceedings of the 39th AnnualMeeting of the Association for Computational Linguistics,pages 523?530.48
