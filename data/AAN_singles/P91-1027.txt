AUTOMATIC ACQUISITION OF SUBCATEGORIZATIONFRAMES FROM UNTAGGED TEXTMichael R. BrentMIT AI Lab545 Technology SquareCambridge, Massachusetts 02139michael@ai.mit.eduABSTRACTThis paper describes an implemented programthat takes a raw, untagged text corpus as itsonly input (no open-class dictionary) and gener-ates a partial list of verbs occurring in the textand the subcategorization frames (SFs) in whichthey occur.
Verbs are detected by a novel tech-nique based on the Case Filter of Rouvret andVergnaud (1980).
The completeness of the outputlist increases monotonically with the total numberof occurrences of each verb in the corpus.
Falsepositive rates are one to three percent of observa-tions.
Five SFs are currently detected and moreare planned.
Ultimately, I expect to provide alarge SF dictionary to the NLP community and totrain dictionaries for specific corpora.1 INTRODUCTIONThis paper describes an implemented programthat takes an untagged text corpus and generatesa partial list of verbs occurring in it and the sub-categorization frames (SFs) in which they occur.So far, it detects the five SFs shown in Table 1.SF Good Example Bad ExampleDescriptiondirect objectdirect object& clausedirect object& infinitiveclauseinfinitivegreet themtell him he's afoolwant him toattendknow I'll attendhope to attend*arrive them*hope him he's afool*hope him toattend*want I'll attend*greet to attendTable 1: The five subcategorization frames (SFs)detected so farThe SF acquisition program has been testedon a corpus of 2.6 million words of the Wall StreetJournal (kindly provided by the Penn Tree Bankproject).
On this corpus, it makes 5101 observa-tions about 2258 orthographically distinct verbs.False positive rates vary from one to three percentof observations, depending on the SF.1.1 WHY IT  MATTERSAccurate parsing requires knowing the sub-categorization frames of verbs, as shown by (1).
(1) a. I expected \[nv the man who smoked NP\]to eat ice-creamh.
I doubted \[NP the man who liked to eatice-cream NP\]Current high-coverage parsers tend to use eithercustom, hand-generated lists of subcategorizationframes (e.g., Hindle, 1983), or published, hand-generated lists like the Ozford Advanced Learner'sDictionary of Contemporary English, Hornby andCovey (1973) (e.g., DeMarcken, 1990).
In eithercase, such lists are expensive to build and to main-tain in the face of evolving usage.
In addition,they tend not to include rare usages or specializedvocabularies like financial or military jargon.
Fur-ther, they are often incomplete in arbitrary ways.For example, Webster's Ninth New Collegiate Dic-tionary lists the sense of strike meaning 'go occurto", as in "it struck him that .
.
.
", but it does notlist that same sense of hit.
(My program discov-ered both.
)1.2 WHY IT 'S  HARDThe initial priorities in this research were:.
Generality (e.g., minimal assumptions aboutthe text).
Accuracy in identifying SF occurrences?
Simplicity of design and speedEfficient use of the available text was not a highpriority, since it was felt that plenty of text wasavailable ven for an inefficient learner, assumingsufficient speed to make use of it.
These priorities209had a substantial influence on the approach taken.They are evaluated in retrospect in Section 4.The first step in finding a subcategorizationframe is finding a verb.
Because of widespread andproductive noun/verb ambiguity, dictionaries arenot much use - -  they do not reliably exclude thepossibility oflexical ambiguity.
Even if they did, aprogram that could only learn SFs for unambigu-ous verbs would be of limited value.
Statisticaldisambiguators make dictionaries more useful, butthey have a fairly high error rate, and degrade inthe presence of many unfamiliar words.
Further,it is often difficult to understand where the error iscoming from or how to correct it.
So finding verbsposes a serious challenge for the design of an accu-rate, general-purpose algorithm for detecting SFs.In fact, finding main verbs is more difficultthan it might seem.
One problem is distinguishingparticiples from adjectives and nouns, as shownbelow.
(2) a. John has \[~p rented furniture\](comp.
: John has often rented apart-ments)b. John was smashed (drunk) last night(comp.
: John was kissed last night)c. John's favorite activity is watching TV(comp.
: John's favorite child is watchingTV)In each case the main verb is have or be in a con-text where most parsers (and statistical disam-biguators) would mistake it for an auxiliary andmistake the following word for a participial mainverb.A second challenge to accuracy is determin-ing which verb to associate a given complementwith.
Paradoxically, example (1) shows that ingeneral it isn't possible to do this without alreadyknowing the SF.
One obvious strategy would beto wait for sentences where there is only one can-didate verb; unfortunately, it is very difficult toknow for certain how many verbs occur in a sen-tence.
Finding some of the verbs in a text reliablyis hard enough; finding all of them reliably is wellbeyond the scope of this work.Finally, any system applied to real input, nomatter how carefully designed, will occasionallymake errors in finding the verb and determiningits subcategorizatiou frame.
The more times agiven verb appears in the corpus, the more likelyit is that one of those occurrences will cause anerroneous judgment.
For that reason any learn-ing system that gets only positive examples andmakes a permanent judgment on a single examplewill always degrade as the number of occurrencesincreases.
In fact, making a judgment based onany fixed number of examples with any finite errorrate will always lead to degradation with corpus-size.
A better approach is to require a fixed per-centage of the total occurrences of any given verbto appear with a given SF before concluding thatrandom error is not responsible for these observa-tions.
Unfortunately, determining the cutoff per-centage requires human intervention and samplingerror makes classification unstable for verbs withfew occurrences in the input.
The sampling er-ror can be dealt with (Brent, 1991) but predeter-mined cutoff percentages stir require eye-bailingthe data.
Thus robust, unsupervised judgmentsin the face of error pose the third challenge to de-veloping an accurate learning system.1.3 HOW IT'S DONEThe architecture ofthe system, and that of this pa-per, directly reflects the three challenges describedabove.
The system consists of three modules:1.
Verb detection: Finds some occurrences ofverbs using the Case Filter (Rouvret andVergnaud, 1980), a proposed rule of gram-mar .2.
SF detection: Finds some occurrences offive subcategorization frames using a simple,finite-state grammar for a fragment of En-glish.3.
SF decision: Determines whether a verb isgenuinely associated with a given SF, orwhether instead its apparent occurrences inthat SF are due to error.
This is done usingstatistical models of the frequency distribu-tions.The following two sections describe and eval-uate the verb detection module and the SF de-tection module, respectively; the decision module,which is still being refined, will be described ina subsequent paper.
The final two sections pro-vide a brief comparison to related work and drawconclusions.2 VERB DETECT IONThe technique I developed for finding verbs isbased on the Case Filter of Rouvret and Verguaud(1980).
The Case Filter is a proposed rule of gram-mar which, as it applies to English, says that ev-ery noun-phrase must appear either immediatelyto the left of a tensed verb, immediately to theright of a preposition, or immediately to the r ightof a main verb.
Adverbs and adverbial phrases(including days and dates) are ignored for the pur-poses of case adjacency.
A noun-phrase that sat-isfies the Case Filter is said to "get case" or "havecase", while one that violates it is said to "lackcase".
The program judges an open-class wordto be a main verb if it is adjacent o a pronoun orproper name that would otherwise lack case.
Sucha pronoun or proper name is either the subject or210the direct object of the verb.
Other noun phrasesare not used because it is too difficult to determinetheir right boundaries accurately.The two criteria for evaluating the perfor-mance of the main-verb detection technique areefficiency and accuracy.
Both were measured us-ing a 2.6 million word corpus for which the PennTreebank project provides hand-verified tags.Efficiency of verb detection was assessed byrunning the SF detection module in the normalmode, where verbs were detected using the CaseFilter technique, and then running it again withthe Penn Tags substituted for the verb detectionmodule.
The results are shown in Table 2.
NoteSFdirect objectdirect object&: clausedirect object& infinitiveclauseinfinitiveOccurrencesFound3,59194310739367Control8,6063813,59714,14411,880Efficiency40%25%8%5%3%Table 2: Efficiency of verb detection for each ofthe five SFs, as tested on 2.6 million words of theWall Street Journal and controlled by the PennTreehank's hand-verified taggingthe substantial variation among the SFs: for theSFs "direct object" and "direct object & clause"efficiency is roughly 40% and 25%, respectively;for "direct object & infinitive" it drops to about8%; and for the intransitive SFs it is under 5%.The reason that the transitive SFs fare better isthat the direct object gets case from the preced-ing verb and hence reveals its presence - -  intran-sitive verbs are harder to find.
Likewise, clausesfare better than infinitives because their subjectsget case from the main verb and hence reveal it,whereas infinitives lack overt subjects.
Anotherobvious factor is that, for every SF listed aboveexcept "direct object" two verbs need to be found- -  the matrix verb and the complement verb - -  ifeither one is not detected then no observation isrecorded.Accuracy was measured by looking at thePenn tag for every word that the system judgedto be a verb.
Of approximately 5000 verb tokensfound by the Case Filter technique, there were28 disagreements with the hand-verified tags.
Myprogram was right in 8 of these cases and wrongin 20, for a 0.24% error-rate beyond the rate us-ing hand-verified tags.
Typical disagreements inwhich my system was right involved verbs thatare ambiguous with much more frequent nouns,like mold in "The Soviet Communist Party has thepower to shape corporate development and moldit into a body dependent upon it ."
There wereseveral systematic constructions in which the Penntags were right and my system was wrong, includ-ing constructions like "We consumers are..." andpseudo-clefts like '~vhat you then do is you makethem think .... (These examples are actual textfrom the Penn corpus.
)The extraordinary accuracy of verb detection- -  within a tiny fraction of the rate achieved bytrained human taggers - -  and it's relatively lowefficiency are consistent with the priorities laid outin Section 1.2.2.1 SF DETECTIONThe obvious approach to finding SFs like "VNP to V" and "V to V" is to look for occurrences ofjust those patterns in the training corpus; but theobvious approach fails to address the attachmentproblem illustrated by example (1) above.
Thesolution is based on the following insights:?
Some examples are clear and unambiguous.?
Observations made in clear cases generalizeto all cases.?
It is possible to distinguish the clear casesfrom the ambiguous ones with reasonable ac-curacy.?
With enough examples, it pays to wait forthe clear cases.Rather than take the obvious approach of lookingfor "V NP to V' ,  my approach is to wait for clearcases like "V PRONOUN to V ' .
The advantagescan be seen by contrasting (3) with (1).
(3) a. OK I expected him to eat ice-creamb.
* I doubted him to eat ice-creamMore generally, the system recognizes linguisticstructure using a small finite-state grammar thatdescribes only that fragment of English that ismost useful for recognizing SFs.
The grammarrelies exclusively on closed-class lexical items suchas pronouns, prepositions, determiners, and aux-iliary verbs.The grammar for detecting SFs needs todistinguish three types of complements: directobjects, infinitives, and clauses.
The gram-mars for each of these are presented in Fig-ure 1.
Any open-class word judged to he averb (see Section 2) and followed immediatelyby matches for <DO>, <clause>, <infinitives,<DO><clanse>, or <DO><inf> is assigned thecorresponding SF.
Any word ending in "ly" or211<clause> := that?
(<subj -pron> I <subj -ob j -pron><tensed-verb><sub j -pron> := I J he \[ she \[ I \[ they<sub j -ob j -p ron> := you,  i t ,  yours ,  hers ,  ours ,  the i rs<DO> := <obj-pron><obj-pron> := me \[ him \[ us \[ them<infinitive> := to  <prev ious ly -noted-un in f lec ted-verb>I his I <proper-name>)Figure 1: A non-recursive (finite-state) grammar for detecting certain verbal complements.
"?"
indicatesan optional element.
Any verb followed immediately expressions matching <DO>,  <clause>, <infinitive>,<DO> <clause>, or <DO> <infinitive> is assigned the corresponding SF.belonging to a list of 25 irregular adverbs is ig-nored for purposes of adjacency.
The notation"T' follows optional expressions.
The categoryprev ious ly -noted-un in f lec ted-verb  is specialin that it is not fixed in advance - -  open-class non-adverbs are added to it when they occur followingan unambiguous modal.
I This is the only case inwhich the program makes use of earlier decisions- -  literally bootstrapping.
Note, however, thatambiguity is possible between mass nouns and un-inflected verbs, as in to fish.Like the verb detection algorithm, the SF de-tection algorithm is evaluated in terms of efficiencyand accuracy.
The most useful estimate of effi-ciency is simply the density of observations in thecorpus, shown in the first column of Table 3.
TheSFdirect objectdirect object& clausedirect object& infinitiveclauseinfinitiveoccurrencesfound3,59194310739367% error1.5%2.0%1.5%0.5%3.0%Table 3: SF detector error rates as tested on 2.6million words of the Wall Street Journalaccuracy of SF detection is shown in the second1If there were room to store an unlimited numberof uninflected verbs for later reference then the gram-mar formalism would not be finite-state.
In fact, afixed amount of storage, sufficient o store all the verbsin the language, is allocated.
This question is purelyacademic, however - -  a hash-table gives constant-timeaverage performance.column of Table 3.
2 The most common sourceof error was purpose adjuncts, as in "John quitto pursue a career in finance," which comes fromomitting the in order from "John quit in order topursue a career in finance."
These purpose ad-juncts were mistaken for infinitival complements.The other errors were more sporadic in nature,many coming from unusual extrapositions or otherrelatively rare phenomena.Once again, the high accuracy and low ef-ficiency are consistent with the priorities of Sec-tion 1.2.
The throughput rate is currently aboutten-thousand words per second on a Sparcsta-tion 2, which is also consistent with the initial pri-orities.
Furthermore, at ten-thousand words persecond the current density of observations i notproblematic.3 RELATED WORKInterest in extracting lexical and especiallycollocational information from text has risen dra-matically in the last two years, as sufficiently largecorpora and sufficiently cheap computation havebecome available.
Three recent papers in this areaare Church and Hanks (1990), Hindle (1990), andSmadja and McKeown (1990).
The latter two areconcerned exclusively with collocation relationsbetween open-class words and not with grammat-ical properties.
Church is also interested primar-ily in open-class collocations, but he does discussverbs that tend to be followed by infinitives withinhis mutual information framework.Mutual information, as applied by Church,is a measure of the tendency of two items to ap-pear near one-another - - their observed frequencyin nearby positions is divided by the expectationof that frequency if their positions were randomand independent.
To measure the tendency of averb to be followed within a few words by an in-finitive, Church uses his statistical disambiguator2Error rates computed by hand verification of 200examples for each SF using the tagged mode.
Theseare estimated independently of the error rates for verbdetection.212(Church, 1988) to distinguish between to as aninfinitive marker and to as a preposition.
Thenhe measures the mutual information between oc-currences of the verb and occurrences of infinitivesfollowing within a certain number of words.
Unlikeour system, Church's approach does not aim to de-cide whether or not a verb occurs with an infiniti-val complement - - example (1) showed that beingfollowed by an infinitive is not the same as takingan infinitival complement.
It might be interestingto try building a verb categorization scheme basedon Church's mutual information measure, but tothe best of our knowledge no such work has beenreported.4 CONCLUSIONSThe ultimate goal of this work is to providethe NLP community with a substantially com-plete, automatically updated ictionary of subcat-egorization frames.
The methods described abovesolve several important problems that had stoodin the way of that goal.
Moreover, the results ob-tained with those methods are quite encouraging.Nonetheless, two obvious barriers till stand on thepath to a fully automated SF dictionary: a deci-sion algorithm that can handle random error, andtechniques for detecting many more types of SFs.Algorithms are currently being developed toresolve raw SF observations into genuine lexicalproperties and random error.
The idea is to auto-matically generate statistical models of the sourcesof error.
For example, purpose adjuncts like "Johnquit to pursue a career in finance" are quite rare,accounting for only two percent of the apparentinfinitival complements.
Furthermore, they aredistributed across a much larger set of matrixverbs than the true infinitival complements, soanygiven verb should occur with a purpose adjunctextremely rarely.
In a histogram sorting verbs bytheir apparent frequency of occurrence with in-finitival complements, those that in fact have ap-peared with purpose adjuncts and not true sub-categorized infinitives will be clustered at the lowfrequencies.
The distributions of such clusters canbe modeled automatically and the models used foridentifying false positives.The second requirement for automaticallygenerating a full-scale dictionary is the ability todetect many more types of SFs.
SFs involvingcertain prepositional phrases are particularly chal:lenging.
For example, while purpose adjuncts(mistaken for infinitival complements) are rela-tively rare, instrumental djuncts as in "John hitthe nail with a hammer" are more common.
Theproblem, of course, is how to distinguish themfrom genuine, subcategorized PPs headed by with,as in "John sprayed the lawn with distilled wa-ter".
The hope is that a frequency analysis likethe one planned for purpose adjuncts will workhere as well, but how successful it will be, and ifsuccessful how large a sample size it will require,remain to be seen.The question of sample size leads back to anevaluation of the initial priorities, which favoredsimplicity, speed, and accuracy, over efficient useof the corpus.
There are various ways in whichthe high-priority criteria can be traded off againstefficiency.
For example, consider (2c): one mightexpect that the overwhelming majority of occur-rences of "is V-ing" are genuine progressives, whilea tiny minority are cases copula.
One might alsoexpect that the occasional copula constructionsare not concentrated around any one present par-ticiple but rather distributed randomly among alarge population.
If those expectations are truethen a frequency-modeling mechanism like the onebeing developed for adjuncts ought to prevent hemistaken copula from doing any harm.
In thatcase it might be worthwhile to admit "is V-ing',where V is known to be a (possibly ambiguous)verb root, as a verb, independent of the Case Fil-ter mechanism.ACKNOWLEDGMENTSThanks to Don Hindle, Lila Gleitman, and JaneGrimshaw for useful and encouraging conversa-tions.
Thanks also to Mark Liberman, MitchMarcus and the Penn Treebank project at theUniversity of Pennsylvania for supplying taggedtext.
This work was supported in part by NationalScience Foundation grant DCR-85552543 under aPresidential Young Investigator Award to Profes-sor Robert C. Berwick.Re ferences\[Brent, 1991\] M. Brent.
Semantic Classification ofVerbs from their Syntactic Contexts: An Imple-mented Classifier for Stativity.
In Proceedings ofthe 5th European A CL Conference.
Associationfor Computational Linguistics, 1991.\[Church and Hanks, 1990\] K. Church andP.
Hanks.
Word association orms, mutual in-formation, and lexicography.
Comp.
Ling., 16,1990.\[Church, 1988\] K. Church.
A Stochastic PartsProgram and Noun Phrase Parser for Unre-stricted Text.
In Proceedings of the 2nd ACLConference on Applied NLP.
ACL, 1988.\[DeMarcken, 1990\] C. DeMarcken.
Parsing theLOB Corpus.
In Proceedings of the A CL.
As-socation for Comp.
Ling., 1990.\[Gleitman, 1990\] L. Gleitman.
The structuralsources of verb meanings.
Language Acquisition,1(1):3-56, 1990.213\[Hindle, 1983\] D. Hindle.
User Manual for Fid-ditch, a Deterministic Parser.
Technical Report7590-142, Naval Research Laboratory, 1983.\[Hindle, 1990\] D. Hindle.
Noun cl~sification frompredicate argument structures.
In Proceedingsof the 28th Annual Meeting of the ACL, pages268-275.
ACL, 1990.\[Hornby and Covey, 1973\] A. Hornby andA.
Covey.
Ozford Advanced Learner's Dictio-nary of Contemporary English.
Oxford Univer-sity Press, Oxford, 1973.\[Levin, 1989\] B. Levin.
English Verbal Diathe-sis.
Lexicon Project orking Papers no.
32, MITCenter for Cognitive Science, MIT, Cambridge,MA., 1989.\[Pinker, 1989\] S. Pinker.
Learnability and Cogni-tion: The Acquisition of Argument Structure.MIT Press, Cambridge, MA, 1989.\[Rouvret and Vergnaud, 1980\] A. Rouvret and J-R Vergnaud.
Specifying Reference to the Sub-ject.
Linguistic Inquiry, 11(1), 1980.\[Smadja and McKeown, 1990\]F. Smadja and K. McKeown.
Automaticallyextracting and representing collocations for lan-guage generation.
In 28th Anneal Meeting ofthe Association for Comp.
Ling., pages 252-259.ACL, 1990.\[Zwicky, 1970\] A. Zwicky.
In a Manner of Speak-ing.
Linguistic Inquiry, 2:223-233, 1970.214
