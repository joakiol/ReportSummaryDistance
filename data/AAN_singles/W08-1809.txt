Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 66?73Manchester, UK.
August 2008Topic Indexing and Retrieval for Factoid QAKisuh AhnSchool of InformaticsUniversity of EdinburghEdinburgh, UKk.ahn@sms.ed.ac.ukBonnie WebberSchool of InformaticsUniversity of EdinburghEdinburgh, UKbonnie@inf.ed.ac.ukAbstractThe method of Topic Indexing and Re-trieval for QA persented in this paperenables fast and efficent QA for ques-tions with named entity answers.
This isachieved by identifying all possible namedentity answers in a corpus off-line andgathering all possible evidence for their di-rect retrieval as answer candidates usingstandard IR techniques.
An evaluation ofthis method on 377 TREC questions pro-duced a score of 0.342 in Accuracy and0.413 in Mean Reciprocal Rank (MRR).1 IntroductionMany textual QA systems use InformationRetrieval to retrieve a subset of the docu-ments/passages from the source corpus in order toreduce the amount of text that needs to be inves-tigated in finding the correct answers.
This useof Information Retrieval (IR) plays an importantrole, since it imposes an upper bound on the per-formance of the entire QA system: Subsequent an-swer extraction operations cannot make up for thefailure of IR to fetch text that contains correct an-swers.
Several techniques have been developed tocut down the amount of text that must be retrievedin order to ensure against the loss of answer mate-rial, but processing any text for downstream oper-ations still takes up valuable on-line time.In this paper, we present a method, Topic Index-ing and Retrieval for QA, that turns factoid Ques-tion Answering into fine-grained Information Re-trieval, where answer candidates are directly re-c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.trieved instead of documents/passages.
The pri-mary claim here is that for simple named entityanswers, this can make for fast and accurate re-trieval.2 The Overall IdeaThe answers to many factoid questions are namedentities ?
eg, ?Who is the president of India?
?,?Where was Eric Clapton born?
?, etc.
The basicidea of this paper?s central method, Topic Indexingand Retrieval for Question Answering (or TOQAsubsequently), is to extract such expressions off-line from a textual corpus as potential answers andgather evidence that supports their direct retrievalas answers to questions using off-the-shelf IR.Central here is the notion of topics.
Underthis method, any named entities (proper names)found in a corpus are regarded as potential an-swers.
However, named entities are not just treatedas words or phrases but as topics with three kindsof information useful for Question Answering.First, as a locus of information, a topic has tex-tual content which talks about this topic.
Thiscomprises the set of all sentences from the cor-pus that mention this topic.
Textual content is im-portant because it provides the means to judge thetopic?s suitability as an answer to a question viatextual similarity between the question and somepart of the topic?s textual content.Second, a topic has an ontological type (ortypes).
This type information is very important forQA because the question requires the answer to beof certain type.
A topic must be of the same type(or some compatible type via ISA relation) in or-der to be considered as an answer candidate.
Forexample, the question, ?Who is the president of In-dia??
requires the answer to be of type PERSON(or more specifically, PRESIDENT).66Finally, a topic has relations to other topics.
Forexample, the topic, ?Dolly the sheep?, is closelyrelated to the topic, ?Ian Wilmut?.
While the pre-cise nature of this relation may vary, the frequentco-occurence of two topics in sentences can be re-garded as an evidence that the two are related.
Re-lated topics are useful for question answering be-cause they reduce the search space.
For exam-ple, the answer to the question, e.g.
?Who cre-ated Dolly the sheep??
can be found among all thetopics that are related to the topic contained in thequestion (or question topic), e.g.
?Dolly?
here.These three kinds of information are the basematerial for Question Answering using topics:they provide the means to directly retrieve answersto questions.3 PreprocessingThis section describes the technical details of howto collect these three kinds of information used fortopic based QA, and how to process and store themoff-line in order to enable fast and efficient on-line question answering.
The stored material con-sists of (1) a Topic Repository, which stores topicswith their variant names and ontological types, (2)a topic document collection that stores the textualcontent of topics, and (3) a set of indices createdby indexing the topic document collection for fastand efficient retrieval.3.1 The Make Up of Topic RepositoryThe Topic Repository stores topics, along theirvariant names and their ontological types, in hashtables for fast look-up.
Building a topic repos-itory requires identifying topics within the givencorpus.
For this we have used the C&C named en-tity recogniser (Curran and Clark, 2003), which isrun on pos-tagged and chunked documents in thecorpus to identify and extract named entities as po-tential topics.
This also identifies the base type of asubset of named entities as PERSON, LOCATIONand ORGANISATION.
This is stored for later usein building type-separated indices.
When a namedentity is identified, we first check whether it repre-sents a topic already found in the topic repository.This is done by checking the topic-name hash ta-ble in the repository, which serves as the main datastorage for the variant names of topics.To resolve a target named entity to the appro-priate topic, we use Wikipedia?s Redirect table,which contains many common variant names forthe same topic.
The topic-name hash table is up-dated accordingly.
Hash table entries consist ofpairs like (?George Clooney?, 1745442), where thename ?George Clooney?
is one of the names thatbelong to the unique topic with the ID number of1745442.
We currently do nothing to disambiguatetopics, so different individuals with the same namewill all be considered the same topic.Fine-grained ontological types of topics areidentified and stored as well in a separate topic-type table.
In order to discover fine-grainedtopic types, the ontology database Yago is used(Suchanek et al, 2007).
Yago contains such infor-mation for Wikipedia topics, derived by mappingthe category information about target topic sup-plied by a Wikipedia user to the appropriate Word-Net concept.
(Wikipedia categories are not consis-tent and uniform, and they are more like tags thatcharacterise a topic rather than strictly classify it.
)Using this ontology to look up the type(s) of eachtopic-type (i.e.
the corresponding WordNet con-cept) and by tracing up the WordNet concept hier-archy, we created a fine-grained, multi-level (withrespect to ISA) topic-type hash table for all the top-ics in the topic repository.The topic-type hash table not only contains theontological type of a topic, but also a significantamount of world knowledge typically associated tothe topic, due to the nature of Wikipedia categoriesas descriptive tags.
For example, ?Bill Gates?
isidentified as ?CEO?
(a title-role), and ?Pusan?
as?a province of Korea?
(geographical knowledge).Such diverse and significant knowledge, as well asthe breadth and the depth of the fine types con-tained in the topic-type hash table, enable a verypowerful match between the answer type from aquestion to that of a candidate topic.The set of fine-grained answer types used herediffers from the set of answer types such as Li andRoth (2002) used elsewhere in that the set is open-ended, and new types can be added for an entity atany time.The topic repository is used in re-ranking an-swer candidates by the fine-grained anwer typeand for question topic identification, as well as inbuilding topic document collection to be explainednext.3.2 The Topic Document CollectionAs noted, the textual content of a topic is the set ofall sentences in a corpus that mentions this topic.67(Since anaphora resolution is not yet performed,the sentences that only mention a particular topicanaphorically are missed.)
Such set of sentences isassembled into one file per topic.
This can thenbe regarded as a document on its own with thetopic name as its title.
We henceforth call sucha document, a topic document.
Figure 1 illustratesa topic document for the topic, Dolly the sheep.The topic document collection thus created for alltopics identified can be regarded as a reorganisedcorpus with respect to the original corpus as theFigure ??
illustrates.Figure 1: An Example Topic Document: Dolly thesheepThe topic document collection for the full set oftopics is a subset of the original corpus, reorga-nized around topics.
The process of creating thetopic document collection (which we refer to asthe topic document method) is actually performedat the same time as the creation of Topic Reposi-tory.
Any sentence that contains identifiable topicsis appended to the topic document of each topicit contains.
The topic document collection so cre-ated is central to our Question Answering becauseretrieving a topic document (specifically, its topic)equates to generating an answer candidate for agiven question.
Hence, via topic documents, fine-grained IR can be used to retrieve answers directly.In order to facilitate such retrieval, however, atopic document collection needs to be indexed.In our implemented system (described in Section5), this is done using the indexing module of theLemur Toolkit.
For type specific retrieval, threeseparate indices corresponding to PERSON, LO-CATION and ORGANISATION are created ac-cording to the base types of the topics identifiedat the time of Named Entity Recognition.
In ad-dition, an index for all topic documents regardlessof types, TOTAL, is also created for questions fromwhich the answer type cannot be determined or forwhich their answer types differ from the three basetypes.
Of note here is that separate indices arecreated only for these base types, as we have notexplored separate indexing by fine-grained answertypes.
These fine-types are only used for rerankingafter the candidate topics have been retrieved fromthe base indices.At the time of retrieval, an appropriate index isto be chosen depending on the answer type iden-tified from the question.
This is discussed in thenext section.4 Topic Retrieval and Reranking for QAThe goal is to retrieve a ranked list of topic doc-uments (indicated by their topics) as answers to agiven question.
In order to do this, the query forthe IR operation must be formulated from the ques-tion, and the specific answer type must be identi-fied both for retrieval and for any re-ranking of theretrieved list of topics.Thus, the first necessary operation is QuestionAnalysis.
Question Analysis identifies the ques-tion type (eg, definition question, factoid question,list question, etc); the answer type, and the ques-tion topics (if any) and produces a shallow parseof the question text (pos-tagged and chunked) forquery formulation.
(Identifying the question typeis a formality since the method only deals with fac-toid questions.
)The question topic identification is straightfor-ward: Any proper name present in a particularquestion is a question topic.
For answer typeidentification, we use a simple rule based algo-rithm that looks at the WH-word (e.g.
?Where?means location), the head noun of a WH-phrasewith ?Which?
or ?What?
(e.g.
?Which president?means the answer type is of president), and if themain verb is a copula, the head of the post-copulanoun phrase (e.g.
for ?Who is the president ..?,here again ?president?
is the answer type.)
Word-Net is used to identify the base type of the an-swer type identified from the question when it isnot one of the base types (PERSON, LOCATION,ORGANISATION).
For example, ?president?
istraced to its base type, ?PERSON?.68Next is the retrieval of topics as answer candi-dates for a given question.
This involves: (1) iden-tifying the appropriate index, (2) formulating thequery, and (3) the actual retrieval operation.
An ap-propriate index is chosen based on the base answertype.
For example, for the question, ?Who is thepresident of Germany?
?, the answer type is iden-tified as ?president?.
But since the answer type,?president?, is not the base type, WordNet is usedto trace from ?president?
to a base type (PERSON)and the corresponding index is selected (becauseseparate indices exist only for base types).
If noneof the three base types is found by this process, thetotal index is used.Retrieval uses the InQuery retrieval systemwithin the Lemur Tool Kit (Ogilvie and Callan,2002).
InQuery supports a powerful and flexiblestructured query language.
Taking advantage ofthis, a structured query is formulated from the tar-get question.
So for example, the parsed form ofthe question, ?Who is the president of Germany?
?is used to generate the following query\sum(is president of germany\phrase(president of germany)).In this example, ?president of germany?
forms aphrase, and it is inserted as part of the query el-ement with the ?\phrase?
operator.
However, theindividual keywords are also included as bag ofwords since we have found it to give better perfor-mance in the trials that we have run.
The overalloperator is then enclosed by the ?\sum?
operatorthat gives the overall score of the query with re-spect to a document.
With this query, search isperformed and a ranked list of topics is retrieved.This ranked list is then run through the followingoperations:1.
Filtering the retrieved list of topics to removequestion topics if present.2.
Re-ranking with respect to topic type, prefer-ring the topic that matches the fine answertype.3.
Choosing the highest ranking topic as the an-swer to the question.The question topic, in the above example, ?Ger-many?, is filtered out if it is found in the list of top-ics retrieved (using topic-name hash table), whichcan happen as it is one of the keywords in thequery.
For the remaining topics in the list, the typesof each topic are fetched using the topic-type hashtable and matched up to the specific answer type.Re-ranking is performed according to the follow-ing rules:?
Topics whose type precisely matches the an-swer type are ranked higher than any othertopics whose types do not precisely match theanswer type.?
Topics whose type do not precisely matchthe answer type but still matches the basetype traced from the answer type are rankedhigher than any other topics whose types donot match the answer type at all.Based on these simple rules, the highest-rankingtopic is chosen as the answer.
Because of the de-tailed and precise type information stored for eachtopic, we find this simple procedure works wellenough.
However, a more sophisticated answercandidate reranking strategy is conceivable basedon giving different weights to different degree ofmatch for an answer type.5 Bi-Topic IndexingThe method described thus far ignores questiontopics except for filtering them out during post-processing.
However, we mentioned in Section 2that related topics can be exploited in answeringquestions.To take advantage of question topics withinTopic Indexing and Retrieval, we have adoptedthe solution of constructing bi-topic documents incontrast to the original topic documents with sin-gle topics.
An example of a bi-topic document isthe following Figure 2, which represents the twotopics (Dolly, Ian Wilmut).
Such a bi-topic docu-ment represents the general relation between twotopics via the context in which they co-occur.
(Asalready noted, the precise character of the rela-tion is ignored.)
The terms that more frequentlyappear in such document characterise the relationbetween the two topics in statistical fashion, andthis document would be given a higher score forretrieval with respect to a question, if the ques-tion contains such a relatively frequently appear-ing term.
For example, in scoring the bi-topic doc-ument pertaining to (Dolly, Ian Wilmut) bi-topicdocument with respect to the question, ?Who cre-ated the first cloned sheep, Dolly?
?, the frequentlyappearing term in the document, ?cloned?
would69give a very high mark for this document with re-spect to this question.Figure 2: A Bi-Topic Document: (Dolly, IanWilmut)We construct a bi-topic document collectionis a recursive application of the topic documentmethod first on the original documents and thento the resulting topic documents.
So given a singletopic document, e.g.
for ?Dolly?, the same topicdocument generating process is then applied to thisdocument.
This generates a new set of topic doc-uments that, in addition to having their own top-ics, e.g.
?Ian Wilmut?, will also contain the topic?Dolly?
since the original topic document has thetopic ?Dolly?
in its every sentence.
The result-ing bi-topic documents would comprise (Dolly, IanWilmut), (Dolly, Bonnie), (Dolly, Roslin), etc., allas bi-topic documents.
These topic documents allconcern the topic ?Dolly?, which we call the an-chor topic, and indexing these amounts to creatinga ?Dolly?
(anchored) index.
Separate indices forbase types as in the case of the single topic doc-uments need not be created since the number ofbi-topic documents anchored to one topic is somemagnitude smaller compared to the number of to-tal single topic documents.QA using a bi-topic document index is essen-tially the same as for the single topic document in-dex, except in selecting the appropriate anchoredindex using the question topic identified from thequestion.
So the ?Dolly?
index is chosen if thequestion topic is ?Dolly?, as in the question, e.g.
?Who created the fist cloned sheep, Dolly??.
Re-ranking based on fine-grained answer types canstill be performed although question topic filteringis no longer necessary.This bi-topic method has the draw-back of gen-erating a lot of documents and corresponding in-dices since the number of bi-topic documents isthe product of the number of topics with all theassociated topics.
This takes a lot of space for stor-age and time for generating such a collection.
Forthe evaluation to be described in the next section,we have created bi-topic documents and indicesthat only pertain to questions (ie only for the ques-tion topics within the test set) due to the limitationof space.
To be able to scale this method gener-ally, XML information retrieval technique mightbe applicable as this supports richer retrieval ele-ments other than whole documents and thereforethe bi-topic documents pertaining to one anchortopic could be all embedded within one topic doc-ument.
This is one area we would like to explorefurther in the future.The next section characterises and compares theperformance of single topic and bi-topic documentbased methods.6 Evaluation6.1 The Evaluation SettingsEvaluation has been carried out to determinewhether Topic Indexing and Retrieval using a sim-ple and efficient IR technique for direct answer re-trieval can indeed make for an accurate QA sys-tem.
This has also iluminated those features of themethod that contribute to QA performance.The questions and the corpus (AQUAINT) usedfor the evaluation are taken from the TREC QAtrack.
377 questions that have single proper namesas answers (ie, excluding list questions, ?other?questions and questions without answers) were se-lected from the TREC 2003/2004/2005 questions.Questions from TREC 2004 and TREC 2005 aregrouped around what are called targets.
A tar-get is basically the question topic, e.g.
?Whenwas he born??
where ?he?
refers to the target,e.g.
?Fred Durst?.
One of the experimental setupstakes account of these targets by employing the Bi-topic method discussed in Section 5.
This retrievalstrategy is also applied to questions from TREC2003 (that come with no targets), by identifyingthe question topic in a question and extracting itas a target automatically, in order to see whetherit can benefit the QA performance even when thetarget is not provided manually.70The actual evaluation of the method consists ofthree experiments, each of which tests a differentsetting.
The common elements for all three arethe core answer retrieval system.
The aspects thatdifferentiate the three settings are: (1) whether ornot a fine-grained answer type is used for rerank-ing, (2) whether single topic documents or bi-topicdocuments are retrieved.6.2 The Core Evaluation SystemThe common core system that implements the an-swer retrieval method comprises (1) a questionanalysis module that analyses the question andproduces the question type, answer type, the ques-tion topics and the shallow parse of the questiontext and (2) a retrieval module that generates thestructured query, selects the appropriate index andretrieves the top 100 topics as answer candidates.This core system performs the basic retrieval op-erations, to which we add further operations suchas answer-type based reranking and target specificretrieval.
The addition of some of these featuresdistinguish different setups for the evaluation.Setup A involves just the core system on singletopic document indexing of the AQUAINT corpus,as described in Section 3.2.
The resulting topicdocuments are divided into the three base types(PERSON, LOCATION, ORGANISATION), plusOTHER, as summarised in Table 1.
Some ex-amples of entities belong to type OTHER includemedicines, roller coasters and software.KIND NUMPERSON 117370ORGANISATION 67559LOCATION 48194OTHER 17942TOTAL 251065Table 1: Number of Topic Docs per TypesSetup B is basically the same as setup A, ex-cept for the addition of fine-grained answer typere-ranking on the one hundred topics retrieved asanswer candidates.
That is, elements of this listare re-ranked depending on whether their fine-grained answer type matches the fine-grained an-swer type identified from the question.
Note herethat only the coarse answer type (PERSON, LO-CATION, ORGANISATION, TOTAL) was usedfor retrieval, as opposed to the fine-grained typesuch as PRESIDENT or COMPANY, due to theA@N A B C1 0.233:88 0.340:128 0.342:1292 0.316:119 0.406:153 0.443:1673 0.366:138 0.438:165 0.485:1834 0.401:151 0.467:176 0.501:1895 0.430:162 0.491:185 0.515:19410 0.472:178 0.523:197 0.549:20715 0.496:187 0.533:201 0.560:21120 0.512:193 0.541:204 0.560:211ACC 0.233 0.340 0.342MRR 0.306 0.395 0.413Table 2: Results for all setups for all questionsfact that separate indices exist only for these coarsetypes.
The identification of the fine type of a can-didate topic is done by looking up this informationin the topic-type hash table as mentioned in Sec-tion 3.
Again the resulting top candidate is pickedas the definite answer.The final setup is setup C. Setup C exploits ques-tion topics (targets), as described in Section 5.
Tar-gets are explicitly provided in TREC 2004 andTREC 2005 question set.
For the TREC 2003, thequestions, which do not come with explicit targets,the system automatically extracts a target from thequestion using a very simple rule: any proper namein the question is regarded as a target.
The point ofthis setup is to test the effectiveness of the bi-topicmethod discussed in Section 5.
The core retrievalprocedure is the same as in setup B, except thatthe index on which the retrieval is performed is se-lected based on the question topic.
In Section 5,we mentioned that a set of indices were built withrespect to ?anchor topics?.
So the question topicidentified from the question (or provided as de-fault) acts as the anchor topic and the index thatcorresponds to this anchor topic gets chosen.
Therest of the process is the same as setup B, and re-trieved topics are re-ranked according to the fine-grained answer type.6.3 Overall ResultsTable 2 summarises the results of the experimentsacross all setups and across all the questions eval-uated.
The leftmost column indicates the cut-offpoint (ie, 5 indicates the top-5 answer candidates,10 indicates the top-10 answer candidates, etc.
).The other columns indicate the A@N performancescore data for setup A, setup B and setup C respec-tively at each cut-off point.
Each entry comprises71A@N B-C C-B C ?
B1 60 61 682 61 75 923 61 79 1044 63 76 1135 66 75 11910 69 79 12815 68 78 13320 69 76 135Table 3: Overlap between B and Ctwo scores separated by a colon, representing theratio of correctly answered questions over all ques-tions and the number of correctly answered ques-tions.
The last two rows summarise the results bygiving the accuracy (ACC), which is equivalent tothe correctness rate at A@1 and the Mean Recip-rocal Rank score (MRR).From this table, it can be seen that both setup Band setup C produced results that are superior tosetup A in all measures: accuracy, A@N (for N upto 20) and MRR.In order to verify whether the differences inscores indicate statistical significance, we haveperformed Wilcoxon Matched Signed Rank Test(Wilcoxon, 1945) on the test data (the differencesin ranks for all the questions between setups).
Thistest is suited for testing two related samples whenan underlying distribution cannot be assumed (un-like t-test) as with the data here.
The statisticaltest shows that the difference between setup B andsetup A is indeed significant (p = 1.763e ?
08,for P threshold at 0.05) and that the differencebetween setup C and setup A is also significant(p = 4.244e?08).
So setup B and setup C performsignificantly better than setup A.Setup C performs slightly better than setup B,both in accuracy (0.342 vs. 0.340) and in MRR(0.413 vs 0.395), but the statistical test shows,this difference is not statistically significant (p =0.5729).
However, as the Table 3 shows, setupB and setup C correctly answered different ques-tions.
(Setup B answered most of the questionsthat were correctly answered by setup A, as wellas questions that were not correctly answered bysetup A).
Thus, a further investigation is neededto understand performance differences between se-tups B and C.The execution time for each question takes lessthan one second for both single-topic and bi-topicdocument indices based retrieval on a single CPU(P4 3.2 Mhz HT) with 512 MB of memory, andthe reranking operation did not add any significantamount of time to it.7 Related WorkIn this section, we discuss some of the works onnovel indexing techniques for QA that relate to thiswork.In predictive annotation (Prager et al, 1999), thetext of the target corpus is pre-processed in sucha way that phrases that are potential answers aremarked and annotated with respect to their answertypes (or QA-tokens as they call them) includingPERSON$1, DURATION$, etc.
Then the text isindexed not only with ordinary terms but also withthese QA-tokens as indexing elements.
The mainadvantage of this approach is that QA-tokens areused as part of the query enhancing the passage re-trieval performance.
Our work in this paper usesthe same predictive annotation technique but dif-fers in that the named entities are indexed as topicsand are retrieved directly as answer candidates.Similar to our approach, Kim et al (2001) ap-plies predictive annotation method to retrieve an-swers directly rather than supporting text.
For ev-ery potential answer in the corpus, a set of textspans up to three sentences long (the sentence inwhich it appears, plus whatever following sen-tences that are linked to this sentence via lexicalchain totalling no more than three sentences insize) is stored and later sued to retrieve a potentialanswer.
Although similar to our work, the maindifference is in the way the textual evidence is ag-gregated.
In Topic Indexing and Retrieval, all theevidence (aka textual content) available through-out the corpus for a possible answer is aggregated,whereas Kim uses text spans up three sentenceslong from a single document connected by a co-reference chain for each answer candidate.
Also,topic relations are not exploited as in our work (viaBi-topic documents).Fleischman et al (2002) also retrieves answersdirectly.
In what they call the answer repositoryapproach to Question Answering, highly preciserelational information is extracted from the textcollection using text mining techniques based onpart of speech patterns.
The extracted concept-instance pairs of person name-title such as (Bill1In their notation, the Dollar sign at the end indicates thatthis is a QA token rather than a term.72Gates, Chairman of Microsoft) are used eithersolely or in conjunction with a common QA sys-tem in producing answers.
(Jijkoun et al (2004)follows a similar approach.)
This basically Infor-mation Extraction approach taken here can com-plement our own work for the benefit of increasedprecision for select types of questions.In Clifton and Teahan (2004), their knowledgeframework based QA system, QITEKAT, prestorespossible answers along with their correspondingquestion templates based on manual and automaticregular expression patterns.
That the potentialquestions are stored as well the answers make thisapproach different from our approach.The bi-topic method in this paper has some sim-ilarity to Katz and Lin (2000).
Here, ternary re-lations are extracted off-line using manually con-structed regular expression patterns on a target textand stored in a database for the use in QuestionAnswering such as in the START QA system (Katzet al, 2002).
With bi-topic documents in this pa-per, instead of the precise relations between thetwo topics, the aggregate context between two par-ticular topics are captured by assembling all state-ments that mention these two topics together in onefile.
While this does not give the exact character-istics of the relations involved, it does give somestatistical characterization between the two topicsto the benefit for QA.8 ConclusionIn this paper, we have presented the method ofTopic Indexing and Retrieval for QA.
The methodeffectively turns document retrieval of IR into di-rect answer retrieval by indexing potential answers(topics) via topic documents.
We claimed thatthe method can be applied in answering simplenamed-entity questions.
The evaluation results in-deed show that the method is effective for this typeof question, with MRR of 0.413 and accuracy of0.342 (best run: setup C).ReferencesClifton, Terence and William Teahan.
2004.
Bangor atTREC 2004: Question answering track.
In Proceed-ings TREC 2004.Curran, J. and S. Clark.
2003.
Language independentner using a maximum entropy tagger.
In Proceed-ings of the Seventh Conference on Natual LanguageLearning (CoNLL-03), pages 164?167.Fleischman, Michael, Eduard Hovy, and AbdessamadEchihabi.
2002.
Offline strategies for online ques-tion answering: Answering questions before they areasked.
In Proceedings of the 41th Annual Meeting ofthe Association for Computational Linguistics (ACL-2003).Jijkoun, Valentin, Maarten de Rijke, and Jori Mur.2004.
Information extraction for question answer-ing: improving recall through syntactic patterns.In COLING ?04: Proceedings of the 20th inter-national conference on Computational Linguistics,page 1284, Morristown, NJ, USA.
Association forComputational Linguistics.Katz, Boris and Jimmy Lin.
2000.
REXTOR: A systemfor generating relations from natural language.
InProceedings of the ACL 2000 Workshop on RecentAdvances in NLP and IR.Katz, B., S. Felshin, D. Yuret, A. Ibrahim, J. Lin,G.
Marton, A. McFarland, and B. Temelkuran.
2002.Omnibase: Uniform access to heterogeneous data forquestion answering.Kim, Harksoo, Kyungsun Kim, Gary Geunbae Lee,and Jungyun Seo.
2001.
MAYA: A fast question-answering system based on a predictive answer in-dexer.
In Proceedings of the 39th Annual Meeting ofthe Association for Computational Linguistics (ACL-2001) Workshop on Open-Domain Question Answer-ing.Li, X. and D. Roth.
2002.
Learning question clas-sifiers.
In Proceeding of the 19th InternationalConference on Computational Linguistics (COL-ING?02).Ogilvie, P. and J. Callan.
2002.
Experiments usingthe lemur toolkit.
In Proceeding of the 2001 TextRetrieval Conference (TREC 2001), pages 103?108.Prager, John, Dragomir Radev, Eric Brown, Anni Co-den, and Valerie Samn.
1999.
The use of predic-tive annotation for question answering in TREC8.
InProceedings of the Eighth Text REtrieval Conference(TREC-8).Suchanek, Fabian M., Gjergji Kasneci, and Ger-hard Weikum.
2007.
Yago: A core of seman-tic knowledge - unifying WordNet and Wikipedia.In Williamson, Carey L., Mary Ellen Zurko, andPrashant J. Patel-Schneider, Peter F. Shenoy, edi-tors, 16th International World Wide Web Conference(WWW 2007), pages 697?706, Banff, Canada.
ACM.Wilcoxon, F. 1945.
Individual comparisons by rankingmethods.
Biometrics, (1):80?83.73
