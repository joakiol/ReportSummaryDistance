RECENT IMPROVEMENTS IN THE CMU SPOKEN LANGUAGEUNDERSTANDING SYSTEMWayne Ward and Sunil IssarSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh PA 15217ABSTRACTWe have been developing a spoken language system to recognizeand understand spontaneous speech.
It is difficult for such systemsto achieve good coverage of the lexicon and grammar that subjectsmight use because spontaneous speech often contains disfluenciesand ungrammatical constructions.
Our goal is to respond appropri-ately to input, even though coverage is not complete.
The naturallanguage component of our system is oriented toward the extractionof information relevant to a task, and seeks to directly optimize thecorrectness of the extracted information (and therefore the systemresponse).
We use a flexible frame-based parser, which parses asmuch of the input as possible.
This approach leads both to highaccuracy and robustness.
We have implemented a version of thissystem for the Air Travel Information Service (ATIS) task, which isbeing used by several ARPA-funded sites to develop and evaluatespeech understanding systems.
Users are asked to perform a taskthat requires getting information from an Air Travel database.
Inthis paper, we describe recent improvements in our system resultingfrom our efforts to improve the coverage given a limited amount oftraining data.
These improvements address a number of problemsincluding enerating an adequate l xicon and grammar for the rec-ognizer, generating and generalizing an appropriate grammar for theparser, and dealing with ambiguous parses.1.
INTRODUCTIONUnderstanding spontaneous speech presents everal problems thatare not found either in recognizing read speech or in parsing writtentext.
Since the users are not familiar with the lexicon and grammarused by the system, itis very difficult for a speech understanding sys-tem to achieve good coverage of the lexicon and grammar that sub-jects might use.
Spontaneous speech often contains ungrammaticalconstructions, stutters, filled pauses, restarts, repeats, interjections,etc.
This causes problems both for the recognizer and the parser.Stochastic language models tend to produce more robust recognitionthan grammar based models.
They can be smoothed to allow forunseen word sequences and their scope is short enough to "get backon track" after an error.
The parsing and understanding componentalso must be robust o the phenomena in spontaneous speech and torecognition errors.
Even though the speech is disfluent and gramat-ically ill-formed, the relevant information is still consistent most ofthe time.
We therefore try to model the information i  an utterancerather than its grammatical structure.
The natural language compo-nent of our system is oriented toward the extraction of informationrelevant to a task, and seeks to directly optimize the correctness oftheextracted information (and therefore the system response).
We usea flexible frame-based parser, which parses as much of the input aspossible.
This approach leads both to high accuracy and robustness.We have implemented a version of this system for the ARPA AirTravel Information Service (ATIS) task.
Users are asked to performa task that requires getting information from an Air Travel database.They must interact with the system by voice to find a solution.
Inthis paper, we describe recent improvements in our system resultingfrom our efforts to increase the coverage given a limited amount oftraining data.
These improvements address a number of problemsmentioned above:?
Generating and generalizing an appropriate grammar for theparser* Generating a lexicon and language model for the recognizer?
Resolving ambiguous parses with contextIn addition, we improved the basic performance of the parser andadded a rejection mechanism.2.
SYSTEM OVERVIEWThe CMU spoken language understanding system is called Phoenix,and has been described in previous papers \[4, 3\].
It is neccessaryhere to give a brief description of the system in order to understandthe context within which we were making changes.Our system has a loose coupfing between the recognition and parsingstages.
The recognizer uses stochastic language models to producea single word string hypothesis.
This hypothesis then passed to aparsing module which uses emantic grammars to produce asemanticrepresentation forthe input utterance.
We use a stochastic languagemodel in recognition because of its robustness and efficiency.
Theparsing stage must then be tolerant of mistakes due to disfluenciesand misrecognitions.The parser outputs aframe with slots filled from the current utterance.Information from the current frame is integrated with informationfrom previous frames to form the current context.
A set of tests isapplied to determine whether to reset context.
For example if newdepart and arrive locations are specified, old context is cleared.
Thecurrent context is then used to build an SQL query.2.1.
RecognitionThe CMU Sphinx-II system \[2\] uses semi-continuous HiddenMarkov Models to model context-dependent phones (triphones), in-eluding between-word context.
The phone models are based onsenones, that is, observation distributions are shared between corre-sponding states in similar models.
The system uses four codebooks:Mel-scale cepstra, 1st and 2nd difference cepstra, and power.
Anobservation probability is computed using a mixture of the top 4distributions from each codebook.213The recognizer processes an utterance in four steps.1.
It makes a forward time-synchronous pass using full between-word models, Viterbi scoring and a bigram language model.This produces a word lattice where words have one begin timebut several end times.2.
It then makes a backward pass which uses the end times fromthe words in the first pass and produces a second lattice whichcontains multiple begin times for words.3.
An A* algorithm is used to generate the set of N-best hypothe-ses for the utterance from these two lattices.
An N of 100 wasused for these tests.4.
The set of N-best hypotheses i  then reseored using a trigramlanguage model.
The best scoring hypothesis after escoring isoutput as the result.2.2.
ParsingOur NL understanding system (Phoenix) is designed forr0bust in-formation extraction.
It uses a simple frame mechanism to representtask semantics.
Frames are associated with the various types of ac-tions that can be taken by the system.
Slots in a frame represent thevarious pieces of information relevant to the action that may be spec-ified by the subject.
For example, the most frequently used framein ATIS is the one corresponding to a request to display some typeof flight information.
Slots in the frame specify what informationis to be displayed (flights, fares, times, airlines, etc), how it is to betabulated (a list, a count, ete) and the constraints hat are to be used(date ranges, time ranges, price ranges, ete).The Phoenix system uses Recursive Transition Networks to encodesemantic grammars.
The grammars specify word patterns ( equencesof words) which correspond to semantic tokens understood by thesystem.
A subset of tokens are considered as top-level tokens, whichmeans they can be recognized independently of surrounding context.Nets call other nets to produee a semantic parse tree.
The top-leveltokens appear as slots in the frame structures.
The frames erve toassociate a set of semantic tokens with a function.
Information isoften represented redundantly in different nets.
Some nets repre-sent more complex bindings between tokens, while others representsimple stand-alone values.
There is not one large sentential-levelgrammar, but separate grammars for each slot (there are approxi-mately 70 of these in our ATIS system).
The parse is flexible at theslot level in that it allows slots to be filled independent of order.
Itis not necessary, to represent all different orders in which the slotpatterns could occur.on frames.
Many different frames, and several different versions ofa frame, are pursued simultaneously.
The score for each frame hy-pothesis is the number of words that it accounts for.
A file of wordsnot to be counted in the score is included.
At the end of an utter-ante the parser picks the best scoring frame as the result.
There is aheuristic procedure for resolving ties.
The output of the parser is theframe name and the parse trees for its filled slots.3.
LANGUAGE MODEL GENERATIONOur system uses two different types of language models, a bigramfor speech recognition and and semantic grammar for parsing \[4, 3\].3.1.
Parsing GrammarThe frame structures and patterns for the Recursive Transition Net-works were developed by processing transcripts ofsubjects perform-ing scenarios of the ATIS task.
The data, which consists of around20000 utterances from the ATIS2 and ATIS3 corpora, were gatheredby several sites.
A subset of this data (around 10000 utterances) hasbeen annotated with reference answers.
The details of data collectionand annotation are described by DaM \[1\].The goal of our system is to extract all relevant information from theutterance.
We do not attempt to parse each and every word in theutterance.
However, we have a problem with the grammar coverageif we miss one of the content words in the utterance.
Let us lookat some of the sentences from the December 1993 ARPA evaluationwhere grammar coverage was a problem:(x0s032sx) list *AIRPORT-DESIGNATIONS for flightsfrom st. petersburg(8k8012sx) find a flight round trip from los angelesto tacoma washington with a stopover in san francisco*NOT -EXCEEDING the price of three hundred ollarsfor june tenth nineteen inety three(g02084sx) list *THE -ORIGINS for alaska irlines(g0d014sx) *ARE -SNACKS *SERVED on tower air(i0k05esx) list *THE -DISTANCES to downtown(i0k0eesx) list *THOSE -DISTANCES from los angelesto downtownIn these sentences, the words preceded by '-' did not occur in thegrammar.
However, we could parse the following sentences whichare similar to these sentences.
The differences are highlighted inbold letters.Our semantic grammars arc written to allow flexibility in the patternmatch.
The patterns for a semantic token consist of mandatorywords or tokens which arc necessary tothe meaning of the token andoptional elements.
The patterns arc also written to overgenerate inways that do not change the semantics.
This overgeneration not onlymakes the pattern matches more flexible but also serves to make thenetworks maller.The parser operates by matching the word patterns for slots againstthe input text.
A set of possible interpretations are pursued simul-taneously.
The system is implemented as a top-down RecursiveTransition Network chart parser for slots.
As slot fillers (semanticphrases) are recognized, they are added to frames to which they ap-ply.
The algorithm is basically adynamic programming beam searchlist airport designation for flights from st. petersburgfind a flight round trip from los angeles to tacoma wash-ington with a stopover in san francisco not more thanthe price of three hundred ollars for june tenth nineteenninety threelist the origin for alaska irlinesare snack sewed on tower airlist the distance to downtownlist those distance from los angeles to downtownSome of the words that occurred in the test set, but were not coveredby our grammar are as follows:214ALONG COMBINATION DESIGNATIONSDISTANCES EVER EXCEEDING INFORMATIONSORIGINS RIGHT SEATAC SHOWEDSNACKS SOUTHERN TIP TOLD VERYWANNA YEAHAlthough we have used much of the training data in developing thegrammar, we have mainly focused on the annotated data.
For the an-notated ata, we can compare our answers with the reference answersand determine whether we have parsed the sentence r asonably.
Thisis consistent with the goal of our system, which is to extract all rel-evant information from the utterance.
For the unannotated data, weneed to manually look at either the output of the parser or the wordsmissed to decide if the parse is reasonable.We had on overall error rate of 5.8% on the November 1992 ARPAevaluation, and an error rate of around 4% on the ATIS2 annotateddata.
However, our error rate on the ATIS3 annotated data wasaround 18% for a nearly identical system.
In the November 1992ARPA evaluation, we had an error rate of 5.6% on the Class A setand an error rate of 6.1% on the Class D set.
However, 75% of theClass A errors and 69% of the Class D errors in the evaluation setwere caused by the lack of grammar coverage.
We do not have exactresults, but believe that the overwhelming umber of errors in thetraining set are still caused by lack of grammatical coverage.We try to generalize the grammar to parse strings that are syntac-tically similar to the utterances in the training data.
In our currentsystem, this is achieved by manually adding synonyms and usingother completion techniques.
We add other words that are related tothe words in the training corpus, for example, if we see Monday inthe training corpus, we add the word Mondays as well as other dayslike Tuesday.
If we had been more thorough in our completion, wewould have correctly parsed the sentences from the December 1993evaluation that we mentioned above.
The process can be vastly im-proved by automating this completion process based on synonyms,antonyms, plurals, possessives and other semantic classes.
We couldalso use morphemes toderive new words from the words used intraining data.3.2.
Recognition Language ModelThe lexicon used by the recognizer is initially based on the trainingdata.
We augment this lexicon using completion techniques de-scribed above, as well as adding the words from the grammar usedby the parser.
We also added many city names that are not in thedatabase.
The recognition dictionary contained 2924 unique wordsplus 10 nonspeech events.
Currently, we allow nonspeech eventsto occur between any words, like a silence.
Since we have addedwords to the lexicon which were not observed in the training data,we need to generate a bigram with appropriate probabilities for thesewords.
Initially, we used a backed-off class bigram.
The class bi-gram used 1573 word classes and was smoothed with the standardback-offmethod.
In generating the class-based models, the probabil-ity of a word given a class was determined bythe unigram frequencyof the words rather than treating them as equi-probable.
We thencompared this to a bigram language model created by interpolatingtwo different bigram models: 1) a backed-off word bigram and 2) abacked-off class bigram.
Our initial experiments on an unseen testset indicated that the perplexity decreased from 21.7 to 20.58 whenwe used an interpolated bigram instead of a class bigram.
Whenthe recognizer was run on this test set (516 utterances) with the twolanguage models, word error ate was reduced from 11% to 10% byusing the interpolated bigram (compared tothe class based).
This isand error reduction of about 9%.We then tried to use the parser grammar to help smooth the bigram.The hope was to improve the match between the language modelsused during recognition and parsing and to get a better estimate of theprobabilities of unseen bigrams.
A word-pair grammar was gener-ated from the parser grammar, and probabilities added by assumingequi-probable transitions from a word to all of its successors.
Thisshould give a better probability to a bigram which had not beenseen, but was acceptable tothe parser, than to an unseen bigram notallowed by the parser.
We then interpolated the class-based, word-based and grammar-based bigrarns.
However, when evaluated on atest set, a recognizer using this model was not significantly differentthan using the interpolated word and class bigrams.The trigram language model used the same word classes as thebigram, also smoothed by the back-off method, but not interpolatedwith a word-level model.
We felt that we did not have enough datato train word trigrams.4.
PARSINGAs described earlier, the system is implemented as a chart parser forslot fillers.
As tokens (semantic phrases) are recognized, they areadded to frames to which they apply.
This process naturally producespartial interpretations.
Words which don't fit into a interpretation areleft out.
In many cases the partial interpretation is sufficient for thesystem to take the desired action.
If not, it still provides agood basisto begin a clarification dialog with the user.
The system can givemeaningful feedback to the user about what was understood andprompt for the most relevant missing information.
The algorithmalso produces multiple interpretations.
Many different frames, andseveral different versions of a frame, are pursued simultaneously.
Inearlier papers \[4\], we have described some of the heuristics used bythe parser to select he best interpretation for an utterance.
If thescore is below a certain threshold (based on the number of words inthe utterance) it does not generate any parse.The implementation of achart mechanism had other advantages.
Thesystem does not allow overlapping phrases in a frame, two differentslots could not use a given word in the input.
The previous version ofthe system used a subsumption process to favor matching the longeststrings possible, but did not keep substrings ofthe longest string.
Thisled to a phrase overlap roblem if one slot could start with the sameword that could end another slot in the same frame.
For example inthe utterance "Show flights from Boston", if "show flights" matchedone slot and "flights from Boston" matched another slot, both couldnot be assigned to the same frame since they overlap on the word"flights".
The grammar writer could avoid the problem, but had tobe careful to do so.
The chart algorithm efficiently produces andkeeps ubstrings.
Now, it is not neccessary tobias for longer strings,the overlap roblem is solved by producing both parses at very littleextra cost.
In the example, we would produce "show flights" "fromBoston" and "show" "flights from Boston", assuming the grammarsallowed these phrases.5.
Alternate InterpretationsSometimes heuristics fail to identify the best interpretation, sincethey do not use additional information available in the context.
Wefound it helpful to sometimes pursue alternate interpretations, that215is interpretations which were not the best according to the heuristicsused by the parser.
In this section, we will describe when alternateinterpretations are needed.One of the design decisions in our system was to use task knowledgein the backend instead of in the parser.
The parser only uses domainindependent heuristics.
This sometimes leads to ambiguities.
Forexample, Show me the fares could refer to fares for flights or faresfor ground transportations.
In general, heuristics correctly identifythe parse.
However, consider the following two sentences:I NEED FARES FROM MILWAUKEE TO LONGBEACH AIRPORT (q0g0b7ss)I NEED FARES FROM MILWAUKEE TO GENERALMITCHELL INTERNATIONAL AIRPORTThese sentences are syntactically identical.
The user is most likelyasking for flight fares in the first sentence.
In the second sentence,the user is most likely asking for the cost of ground transportation.However, the system needs domain knowledge to make these dis-tinctions; it needs to know that Long Beach airport is in California,while Generel Mitchell International is in Milwaukee.
Since theparser has no domain knowledge, it cannot parse both of these sen-tences correctly.
However, the backend has domain knowledge andnotices that one of the parses is incorrect.We address this problem by generating a beam of interpretations.The parser still produces the single best interpretation, but keepstrack of a number of other interpretations.
Whenever the backendnotices a problem, it asks the parser for another interpretation.
Theparser then selects the next best interpretation.
However, the scoreof the new interpretation must be within a certain threshold of thebest score.We next look at the parses for the above mentioned sentences:i need fares from milwaukee to general mitchell inter-national airport\[transport.select_field\] \[list.spec\] I NEED \[ground-fare\]FARES \[city_airport\] FROM \[city\] \[cityname\] MIL-WAUKEE TO \[airport_name\] GENERAL MITCHELLINTERNATIONAL AIRPORTi need fares from milwaukee to long beach airport\[transport_select_field\] \[list_spec\] I NEED \[ground.fare\]FARES \[city_airport\] FROM \[city\] \[cityname\] MIL-WAUKEE TO \[airport.for_city\] \[city\] \[cityname\] LONGBEACH AIRPORTERROR correction: Frame Changed\[flight_field.list\] \[list..spec\] I NEED \[flight_fields_exist\]\[fare\] FARES \[flight=type\] FROM \[depart.loc\] \[city\]\[cityname\] MILWAUKEE TO \[arrive_loc\] \[city\] [city-name\] LONG BEACHis processed by the recognizer (SPREC), transcripts ofthe utterancesare processed by the NL portion of the system, and then the speechinput is processed by the entire system.
Processing transcripts showsthe NL coverage of the system and gives a baseline measure of howwell it would do if recognition were perfect.
Processing startingwith the speech input then shows how much performance is lost dueto recognition errors.
The evaluation measures the error rate foreach process.
The measure used for the SPREC test is word errorrate.
This is the sum of all insertion, substitution and deletion errors.The NL and SLS sytems are scored on whether they produced acorrect answer from the database.
For these tests an answer is eithercorrect (if it agrees with annotated database responses) or incorrect(if it does no0.
We had an error rate of 4.4% for SPREC, 9.3% forNL and 13.2% for SLS.
These were the best results reported for theevaluation.
So, for 9.3% of the transcript input, our system produceda wrong answer (there is no indication of whether it was close).
Theword error rate of 4.4% for the recognition gave a sentence rrorrate of 22%.
That is, 22% of the utterances contained at least onerecognition error.
This number becomes 20% if class X utterancesare excluded.
There were 773 non-X utterances in this test set,so approximately 154 of the sentence hypotheses produced by therecognizer contained errors.
Approximately 30 of these led to anerror for the SLS system (when the transcript had been correctlyprocessed).7.
ACKNOWLEDGEMENTSThis research was sponsored by the Department of the Navy, NavalResearch Laboratory under Grant No.
N00014-93-1-2005.
Theviews and conclusions contained in this document are those off theauthors and should not be interpreted asrepresenting the official poli-cies, either expressed or implied, of the U.S. Government.
We thankRaj Reddy and the rest of the speech group for their contributions tothis work.References1.
Deborah A. Dahl, Madeline Bates, Michael Brown, WilliamFisher, Kate Hunicke-Smith, David Pallctt, Christine Pao,Alexander Rudnicky, and Elizabeth Shfibcrg.
Expanding thescope of the ATIS task: The ATIS-3 corpus.
In Proceedingsof the DARPA Human Language Technology Workshop, March1994.2.
Xuedong Huang, Fileno Alleva, Mei-Yuh Hwang, and RonaldRosenfeld.
An overview of the SPHINX-II speech recogni-tion system.
In Proceedings ofthe DARPA Human LanguageTechnology Workshop, March 1993.3.
Sunil Issar and Wayne Ward.
CMU's robust spoken languageunderstanding system.
In Proceedings ofEurospeech, Septem-ber 1993.4.
Wayne Ward.
The CMU air travel information service: Un-derstanding spontaneous speech.
In Proceedings ofthe DARPASpeech and Natural Language Workshop, ages 127-129, June1990.This error correction mechanism was used twice in the December1993 evaluation set, and both times it worked correctly.6.
RESULTS AND CONCLUSIONIn the December 1993 ARPA evaluation, systems from a number ofARPA sites were evaluated.
The evaluation has three parts, speech216
