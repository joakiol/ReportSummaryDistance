Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 135?144,ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational LinguisticsA General-Purpose Rule Extractor for SCFG-Based Machine TranslationGreg Hanneman and Michelle Burroughs and Alon LavieLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213 USA{ghannema, mburroug, alavie}@cs.cmu.eduAbstractWe present a rule extractor for SCFG-basedMT that generalizes many of the contraintspresent in existing SCFG extraction algo-rithms.
Our method?s increased rule coveragecomes from allowing multiple alignments, vir-tual nodes, and multiple tree decompositionsin the extraction process.
At decoding time,we improve automatic metric scores by signif-icantly increasing the number of phrase pairsthat match a given test set, while our experi-ments with hierarchical grammar filtering in-dicate that more intelligent filtering schemeswill also provide a key to future gains.1 IntroductionSyntax-based machine translation systems, regard-less of the underlying formalism they use, dependon a method for acquiring bilingual rules in that for-malism to build the system?s translation model.
Inmodern syntax-based MT, this formalism is oftensynchronous context-free grammar (SCFG), and theSCFG rules are obtained automatically from paralleldata through a large variety of methods.Some SCFG rule extraction techniques requireonly Viterbi word alignment links between thesource and target sides of the input corpus (Chi-ang, 2005), while methods based on linguistic con-stituency structure require the source and/or targetside of the input to be parsed.
Among such tech-niques, most retain the dependency on Viterbi wordalignments for each sentence (Galley et al, 2004;Zollmann and Venugopal, 2006; Lavie et al, 2008;Chiang, 2010) while others make use of a general,corpus-level statistical lexicon instead of individualalignment links (Zhechev and Way, 2008).
Eachmethod may also place constraints on the size, for-mat, or structure of the rules it returns.This paper describes a new, general-purpose ruleextractor intended for cases in which two parse treesand Viterbi word alignment links are provided foreach sentence, although compatibility with single-parse-tree extraction methods can be achieved bysupplying a flat ?dummy?
parse for the missing tree.Our framework for rule extraction is thus most sim-ilar to the Stat-XFER system (Lavie et al, 2008;Ambati et al, 2009) and the tree-to-tree situationconsidered by Chiang (2010).
However, we signif-icantly broaden the scope of allowable rules com-pared to the Stat-XFER heuristics, and our approachdiffers from Chiang?s system in its respect of the lin-guistic constituency constraints expressed in the in-put tree structure.
In summary, we attempt to extractthe greatest possible number of syntactically moti-vated rules while not allowing them to violate ex-plicit constituent boundaries on either the source ortarget side.
This is achieved by allowing creation ofvirtual nodes, by allowing multiple decompositionsof the same tree pair, and by allowing extraction ofSCFG rules beyond the minimial set required to re-generate the tree pair.After describing our extraction method and com-paring it to a number of existing SCFG extractiontechniques, we present a series of experiments ex-amining the number of rules that may be producedfrom an input corpus.
We also describe experimentson Chinese-to-English translation that suggest thatfiltering a very large extracted grammar to a more135Figure 1: Sample input for our rule extraction algorithm.
It consists of a source-side parse tree (French) and a target-side parse tree (English) connected by a Viterbi word alignment.moderate-sized translation model is an importantconsideration for obtaining strong results.
Finally,this paper concludes with some suggestions for fu-ture work.2 Rule Extraction AlgorithmWe begin with a parallel sentence consisting of asource-side parse tree S, a target-side parse tree T ,and a Viterbi word alignment between the trees?leaves.
A sample sentence of this type is shown inFigure 1.
Our goal is to extract a number of SCFGrules that are licensed by this input.2.1 Node AlignmentOur algorithm first computes a node alignment be-tween the parallel trees.
A node s in tree S is alignedto a node t in tree T if the following constraints aremet.
First, all words in the yield of s must eitherbe aligned to words within the yield of t, or theymust be unaligned.
Second, the reverse must alsohold: all words in the yield of t must be aligned towords within the yield of s or again be unaligned.This is analogous to the word-alignment consistencyconstraint of phrase-based SMT phrase extraction(Koehn et al, 2003).
In Figure 1, for example, theNP dominating the French words les voitures bleuesis aligned to the equivalent English NP node domi-nating blue cars.As in phrase-based SMT, where a phrase in onelanguage may be consistent with multiple possiblephrases in the other language, we allow parse nodesin both trees to have multiple node alignments.
Thisis in contrast to one-derivation rule extractors suchas that of Lavie et al (2008), in which each node136in S may only be aligned to a single node in T andvice versa.
The French NP node Ma me`re, for exam-ple, aligns to both the NNP and NP nodes in Englishproducing Mother.Besides aligning existing nodes in both parse treesto the extent possible, we also permit the introduc-tion of ?virtual?
nodes into either tree.
Virtual nodesare created when two or more contiguous children ofan existing node are aligned consistently to a node ora similar set of two or more contiguous children ofa node in the opposite parse tree.
Virtual nodes maybe aligned to ?original?
nodes in the opposite tree orto other virtual nodes.In Figure 1, the existing English NP node bluecars can be aligned to a new virtual node in Frenchthat dominates the N node voitures and the AP nodebleues.
The virtual node is inserted as the parentof N and AP, and as the child of the NP node di-rectly above.
In conjunction with node alignmentsbetween existing nodes, this means that the EnglishNP blue cars is now aligned twice: once to the orig-inal French NP node and once to the virtual nodeN+AP.
We thus replicate the behavior of ?growinginto the gaps?
from phrase-based SMT in the pres-ence of unaligned words.
As another example, a vir-tual node in French covering the V node avait andthe ADV node toujours could be created to alignconsistently with a virtual node in English coveringthe VBD node had and the ADVP node always.Since virtual nodes are always created out of chil-dren of the same node, they are always consis-tent with the existing syntactic structure of the tree.Within the constraints of the existing tree structureand word alignments, however, all possible virtualnodes are considered.
This is in keeping with ourphilosophy of allowing multiple alignments with-out violating constituent boundaries.
Near the topof the trees in Figure 1, for example, French virtualnodes NP+VN+NP (aligned to English NP+VP) andVN+NP+PU (aligned to VP+PU) both exist, eventhough they overlap.
In our procedure, we do allow alimit to be placed the number of child nodes that canbe combined into a virtual node.
Setting this limitto two, for instance, will constrain node alignmentto the space of possible synchronous binarizationsconsistent with the Viterbi word alignments.2.2 Grammar ExtractionGiven the final set of node alignments between thesource tree and the target tree, SCFG rules are ob-tained via a grammar extraction step.
Rule extrac-tion proceeds in a depth-first manner, such that rulesare extracted and cached for all descendents of asource node s before rules in which s is the left-handside are considered.
Extracting rules where sourcenode s is the left-hand side consists of two phases:decomposition and combination.The first phase is decomposition of node s intoall distinct sets D = {d1, d2, .
.
.
, dn} of descendentnodes such that D spans the entire yield of node s,where di ?
D is node-aligned or is an unaligned ter-minal for all i, and di has no ancestor a where a is adescendent of s and a is node-aligned.
Each D thusrepresents the right-hand side of a minimal SCFGrule rooted at s. Due to the introduction of overlap-ping virtual nodes, the decomposition step may in-volve finding multiple sets of decomposition pointswhen there are multiple nodes with the same span atthe same level of the tree.The second phase involves composition of allrules derived from each element of D subject to cer-tain constraints.
Rules are constructed using s, theset of nodes Ts = {t | s is aligned to t}, and eachdecomposed node set D. The set of left-hand sidesis {s} ?
Ts, but there may be many right-hand sidesfor a given t and D. Define rhs(d) as the set ofright-hand sides of rules that are derived from d, plusall alignments of d to its aligned set Td.
If d is aterminal, word alignments are used in the place ofnode alignments.
To create a set of right-hand sides,we generate the set R = rhs(d1) ?
.
.
.
?
rhs(dn).For each r ?
R, we execute a combine operationsuch that combine(r) creates a new right-hand sideby combining the component right-hand sides andrecalculating co-indexes between the source- andtarget-side nonterminals.
Finally, we insert any un-aligned terminals on either side.We work through a small example of grammar ex-traction using Figure 2, which replicates a fragmentof Figure 1 with virtual nodes included.
The En-glish node JJ is aligned to the French nodes A andAP, the English node NNS is aligned to the Frenchnode N and the virtual node D+N, and the Englishnode NP is aligned to the French node NP and the137Figure 2: A fragment of Figure 1 with virtual nodes (sym-bolized by dashed lines) added on the French side.
NodesD, N, and AP are all original children of the French NP.virtual node N+AP.
To extract rules from the Frenchnode NP, we consider two potential decompositions:D1 = {D+N,AP} and D2 = {les,N+AP}.
Sincethe French NP is aligned only to the English NP, theset of left-hand sides is {NP::NP}, where we use thesymbol ?::?
to separate the source and target sidesof joint nonterminal label or a rule.In the next step, we use cached rules andalignments to generate all potential right-hand-sidepieces from these top-level nodes:rhs(D+N) ={[D+N1] :: [NNS1],[les voitures] :: [cars]}rhs(AP) =???
[AP1] :: [JJ1],[A1] :: [JJ1],[bleues] :: [blue]??
?rhs(les) = ?rhs(N+AP) =???????????????????
[N+AP1] :: [NP1],[N1 AP2] :: [JJ2 NNS1],[N1 A2] :: [JJ2 NNS1],[voitures AP1] :: [JJ1 cars],[voitures A1] :: [JJ1 cars],[N1 bleues] :: [blue NNS1],[voitures bleues] :: [blue cars]??????????????????
?Next we must combine these pieces.
For example,from D1 we derive the full right-hand sides1.
combine([les voitures]::[cars], [bleues]::[blue])= [les voitures bleues]::[blue cars]2. combine([les voitures]::[cars], [A1]::[JJ1])= [les voitures A1]::[JJ1 cars]3. combine([les voitures]::[cars], [AP1]::[JJ1])= [les voitures AP1]::[JJ1 cars]4. combine([D+N1]::[NNS1], [bleues]::[blue])= [D+N1 bleues]::[blue NNS1]5. combine([D+N1]::[NNS1], [A1]::[JJ1])= [D+N1 A2]::[JJ2 NNS1]6. combine([D+N1]::[NNS1], [AP1]::[JJ1])= [D+N1 AP2]::[JJ2 NNS1]Similarly, we derive seven full right-hand sides fromD2.
Since rhs(les) is empty, rules derived haveright-hand sides equivalent to rhs(N+AP) with theunaligned les added on the source side to com-plete the span of the French NP.
For example,combine([N+AP1]::[NP1]) = [les N+AP1]::[NP1].In the final step, the left-hand side is added to eachfull right-hand side.
Thus,NP :: NP?
[les voitures A1] :: [JJ1 cars]is one example rule extracted from this tree.The number of rules can grow rapidly: if the parsetree has a branching factor of b and a depth of h,there are potentially O(2bh) rules extracted.
To con-trol this, we allow certain constraints on the rules ex-tracted that can short-circuit right-hand-side forma-tion.
We allow separate restrictions on the numberof items that may appear on the right-hand side ofphrase pair rules (maxp) and hierarchical grammarrules (maxg).
We also optionally allow the exclu-sion of parallel unary rules ?
that is, rules whoseright-hand sides consist solely of a pair of alignednonterminals.138Tree Multiple Virtual MultipleSystem Constraints Alignments Nodes DerivationsHiero No ?
?
YesStat-XFER Yes No Some NoGHKM Yes No No YesSAMT No No Yes YesChiang (2010) No No Yes YesThis work Yes Yes Yes YesTable 1: Comparisons between the rule extractor described in this paper and other SCFG rule extraction methods.3 Comparison to Other MethodsTable 1 compares the rule extractor described in Sec-tion 2 to other SCFG extraction methods describedin the literature.
We include comparisons of ourwork against the Hiero system (Chiang, 2005), theStat-XFER system rule learner most recently de-scribed by Ambati et al (2009), the composed ver-sion of GHKM rule extraction (Galley et al, 2006),the so-called Syntax-Augmented MT (SAMT) sys-tem (Zollmann and Venugopal, 2006), and a Hiero?SAMT extension with source- and target-side syntaxdescribed by Chiang (2010).
Note that some of thesemethods make use of only target-side parse trees ?or no parse trees at all, in the case of Hiero ?
butour primary interest in comparison is the constraintsplaced on the rule extraction process rather than thefinal output form of the rules themselves.
We high-light four specific dimensions along these lines.Tree Constraints.
As we mentioned in this pa-per?s introduction, we do not allow any part of ourextracted rules to violate constituent boundaries inthe input parse trees.
This is in contrast to Hiero-derived techniques, which focus on expanding gram-mar coverage by extracting rules for all spans inthe input sentence pair that are consistently word-aligned, regardless of their correspondence to lin-guistic constituents.
Practitioners of both phrase-based and syntax-based SMT have reported severegrammar coverage issues when rules are required toexactly match parse constituents (Koehn et al, 2003;Chiang, 2010).
In our work, we attempt to improvethe coverage of the grammar by allowing multiplenode alignments, virtual nodes, and multiple treedecompositions rather than ignoring structure con-straints.Multiple Alignments.
In contrast to all other ex-traction methods in Table 1, ours allows a node inone parse tree to be aligned with multiple nodesin the other tree, as long as the word-alignmentand structure constraints are satisfied.
However, wedo not allow a node to have multiple simultaneousalignments ?
a single node alignment must be cho-sen for extracting an individual rule.
In practice,this prevents extraction of ?triangle?
rules where thesame node appears on both the left- and right-handside of the same rule.1Virtual Nodes.
In keeping with our philosophyof representing multiple alignments, our use of mul-tiple and overlapping virtual nodes is less restrictivethan the single-alignment constraint of Stat-XFER.Another key difference is that Stat-XFER requiresall virtual nodes to be aligned to original nodes inthe other language, while we permit virtual?virtualnode alignments.
In respecting existing tree struc-ture constraints, our virtual node placement is morerestrictive than SAMT or Chiang, where extractednodes may cross existing constituent boundaries.Multiple Derivations.
Galley et al (2006) ar-gued that breaking a single tree pair into multipledecompositions is important for correct probabilitymodeling.
We agree, and we base our rule extrac-tor?s acquisition of multiple derivations per tree pairon techniques from both GHKM and Hiero.
Morespecifically, we borrow from Hiero the idea of cre-ating hierarchical rules by subtracting and abstract-ing all possible subsets of smaller phrases (alignednodes in our case) from larger phrases.
Like GHKM,1Figure 2 includes a potential triangle rule, D+N :: NNS ?
[les N1] :: [NNS1], where the English NNS node appears onboth sides of the rule.
It is simultaneously aligned to the FrenchD+N and N nodes.139we do this exhaustively within some limit, althoughin our case we use a rank limit on a rule?s right-handside rather than a limit on the depth of the subn-ode subtractions.
Our constraint achieves the goalof controlling the size of the rule set while remainingflexibile in terms of depth depending on the shape ofthe parse trees.4 ExperimentsWe conducted experiments with our rule extrac-tor on the FBIS corpus, made up of approximately302,000 Chinese?English sentence pairs.
We parsedthe corpus with the Chinese and English grammarsof the Berkeley parser (Petrov and Klein, 2007) andword-aligned it with GIZA++ (Och and Ney, 2003).The parsed and word-aligned FBIS corpus served asthe input to our rule extractor, which we ran with anumber of different settings.First, we acquired a baseline rule extraction(?xfer-orig?)
from our corpus using an implementa-tion of the basic Stat-XFER rule learner (Lavie et al,2008), which decomposes each input tree pair into asingle set of minimal SCFG rules2 using only origi-nal nodes in the parse trees.
Next, we tested the ef-fect of allowing multiple decompositions by runningour own rule learner, but restricting its rules to alsoonly make use of original nodes (?compatible?).
Fi-nally, we investigated the total number of extractablerules by allowing the creation of virtual nodes fromup to four adjacent sibling nodes and placing twodifferent limits on the length of the right-hand side(?full-short?
and ?full-long?).
These configurationsare summarized in Table 2.Rule Set maxp maxg Virtual Unaryxfer-orig 10 ?
No Yescompatible 10 5 No Yesfull-short 5 5 Yes Nofull-long 7 7 Yes NoTable 2: Rule sets considered by a Stat-XFER baseline(?xfer-orig?)
and our own rule extractor.2In practice, some Stat-XFER aligned nodes produce tworules instead of one: a minimal hierarchical SCFG rule is al-ways produced, and a phrase pair rule will also be produced fornode yields within the maxp cutoff.4.1 Rules ExtractedAs expected, we find that allowing multiple decom-positions of each tree pair has a significant effect onthe number of extracted rules.
Table 3 breaks the ex-tracted rules for each configuration down into phrasepairs (all terminals on the right-hand side) and hier-archical rules (containing at least one nonterminalon the right-hand side).
We also count the num-ber of extracted rule instances (tokens) against thenumber of unique rules (types).
The results showthat multiple decomposition leads to a four-fold in-crease in the number of extracted grammar rules,even when the length of the Stat-XFER baselinerules is unbounded.
The number of extracted phrasepairs shows a smaller increase, but this is expected:the number of possible phrase pairs is proportionalto the square of the sentence length, while the num-ber of possible hierarchical rules is exponential, sothere is more room for coverage improvement in thehierarchical grammar.With virtual nodes included, there is again a largejump in both the number of extracted rule tokens andtypes, even at relatively short length limits.
Whenboth maxp and maxg are set to 7, our rule ex-tractor produces 1.5 times as many unique phrasepairs and 20.5 times as many unique hierarchicalrules as the baseline Stat-XFER system, and nearlytwice the number of hierarchical rules as when us-ing length limits of 5.
Ambati et al (2009) showedthe usefulness of extending rule extraction from ex-act original?original node alignments to cases inwhich original?virtual and virtual?original align-ments were also permitted.
Our experiments con-firm this, as only 60% (full-short) and 54% (full-long) of our extracted rule types are made up of onlyoriginal?original node alignments.
Further, we finda contribution from the new virtual?virtual case: ap-proximately 8% of the rules extracted in the ?full-long?
configuration from Table 3 are headed by avirtual?virtual alignment, and a similar number havea virtual?virtual alignment on their right-hand sides.All four of the extracted rule sets show Zipfiandistributions over rule frequency counts.
In the xfer-orig, full-short, and full-long configurations, be-tween 82% and 86% of the extracted phrase pairrules, and between 88% and 92% of the extractedhierarchical rules, were observed only once.
These140Extracted Instances Unique RulesRule Set Phrase Hierarchical Phrase Hierarchicalxfer-orig 6,646,791 1,876,384 1,929,641 767,573compatible 8,709,589 6,657,590 2,016,227 3,590,184full-short 10,190,487 14,190,066 2,877,650 8,313,690full-long 10,288,731 22,479,863 2,970,403 15,750,695Table 3: The number of extracted rule instances (tokens) and unique rules (types) produced by the Stat-XFER system(?xfer-orig?)
and three configurations of our rule extractor.percentages are remarkably consistent despite sub-stantial changes in grammar size, meaning that ourmore exhaustive method of rule extraction does notproduce a disproportionate number of singletons.3On the other hand, it does weaken the average countof an extracted hierarchical rule type.
From Table 3,we can compute that the average phrase pair countremains at 3.5 when we move from xfer-orig to thetwo full configurations; however, the average hier-archical rule count drops from 2.4 to 1.7 (full-short)and finally 1.4 (full-long).
This likely again reflectsthe exponential increase in the number of extractablehierarchical rules compared to the quadratic increasein the phrase pairs.4.2 Translation ResultsThe grammars obtained from our rule extractor canbe filtered and formatted for use with a variety ofSCFG-based decoders and rule formats.
We car-ried out end-to-end translation experiments with thevarious extracted rule sets from the FBIS corpus us-ing the open-source decoder Joshua (Li et al, 2009).Given a source-language string, Joshua translates byproducing a synchronous parse of it according to ascored SCFG and a target-side language model.
Asignificant engineering challenge in building a realMT system of this type is selecting a more moderate-sized subset of all extracted rules to retain in the finaltranslation model.
This is an especially importantconsideration when dealing with expanded rule setsderived from virtual nodes and multiple decomposi-tions in each input tree.In our experiments, we pass all grammars through3The compatible configuration is somewhat of an outlier.
Ithas proportionally fewer singleton phrase pairs (80%) than theother variants, likely because it allows multiple alignments andmultiple decompositions without allowing virtual nodes.two preprocessing steps before any translationmodel scoring.
First, we noticed that English car-dinal numbers and punctuation marks in many lan-guages tend to receive incorrect nonterminal labelsduring parsing, despite being closed-class items withclearly defined tags.
Therefore, before rule extrac-tion, we globally correct the nodel labels of all-numeral terminals in English and certain punctua-tion marks in both English and Chinese.
Second,we attempt to reduce derivational ambiguity in caseswhere the same SCFG right-hand side appears inthe grammar after extraction with a large number ofpossible left-hand-side labels.
To this end, we sortthe possible left-hand sides by frequency for eachunique right-hand side, and we remove the least fre-quent 10 percent of the label distribution.Our translation model scoring is based on the fea-ture set of Hanneman et al (2010).
This includesthe standard bidirectional conditional maximum-likelihood scores at both the word and phrase levelon the right-hand side of rules.
We also includemaximum-likelihood scores for the left-hand-sidelabel given all or part of the right-hand side.
Usingstatistics local to each rule, we set binary indicatorfeatures for rules whose frequencies are ?
3, plusfive additional indicator features according to theformat of the rule?s right-hand side, such as whetherit is fully abstract.
Since the system in this paperis not constructed using any non-syntactic rules, wedo not include the Hanneman et al (2010) ?not la-belable?
maximum-likelihood features or the indica-tor features related to non-syntactic labels.Beyond the above preprocessing and scoringcommon to all grammars, we experiment with threedifferent solutions to the more difficult problem ofselecting a final translation grammar.
In any case,we separate phrase pair rules from hierarchical rules141Rule Set Filter BLEU TER METxfer-orig 10k 24.39 68.01 54.35xfer-orig 5k+100k 25.95 66.27 54.77compatible 10k 24.28 65.30 53.58full-short 10k 25.16 66.25 54.33full-short 100k 25.51 65.56 54.15full-short 5k+100k 26.08 64.32 54.58full-long 10k 25.74 65.52 54.55full-long 100k 25.53 66.24 53.68full-long 5k+100k 25.83 64.55 54.35Table 4: Automatic metric results using different rulesets, as well as different grammar filtering methods.and include in the grammar all phrase pair rulesmatching a given tuning or testing set.
Any im-provement in phrase pair coverage during the extrac-tion stage is thus directly passed along to decoding.For hierarchical rules, we experiment with retain-ing the 10,000 or 100,000 most frequently extractedunique rules.
We also separate fully abstract hier-archical rules from partially lexicalized hierarchicalrules, and in a further selection technique we retainthe 5,000 most frequent abstract and 100,000 mostfrequent partially lexicalized rules.Given these final rule sets, we tune our MT sys-tems on the NIST MT 2006 data set using the min-imum error-rate training package Z-MERT (Zaidan,2009), and we test on NIST MT 2003.
Both setshave four reference translations.
Table 4 presentscase-insensitive evaluation results on the test set ac-cording to the automatic metrics BLEU (Papineni etal., 2002), TER (Snover et al, 2006), and METEOR(Lavie and Denkowski, 2009).4 The trend in theresults is that including a larger grammar is gener-ally better for performance, but filtering techniquesalso play a substantial role in determining how wella given grammar will perform at run time.We first compare the results in Table 4 for dif-ferent rule sets all filtered the same way at decod-ing time.
With only 10,000 hierarchical rules in use(?10k?
), the improvements in scores indicate that animportant contribution is being made by the addi-tional phrase pair coverage provided by each suc-4For METEOR scoring we use version 1.0 of the metric,tuned to HTER with the exact, stemming, and synonymy mod-ules enabled.cessive rule set.
The original Stat-XFER rule ex-traction provides 244,988 phrase pairs that matchthe MT 2003 test set.
This is already increased to520,995 in the compatible system using multiple de-compositions.
With virtual nodes enabled, the fullsystem produces 766,379 matching phrase pairs upto length 5 or 776,707 up to length 7.
These systemsboth score significantly higher than the Stat-XFERbaseline according to BLEU and TER, and the ME-TEOR scores are likely statistically equivalent.Across all configurations, we find that changingthe grammar filtering technique ?
possibly com-bined with retuned decoder feature weights ?
alsohas a large influence on automatic metric scores.Larger hierarchical grammars tend to score better, insome cases to the point of erasing the score differ-ences between rule sets.
From this we conclude thatmaking effective use of the extracted grammar, nomatter its size, with intelligent filtering techniquesis at least as important as the number and type ofrules extracted overall.
Though the filtering resultsin Table 4 are still somewhat inconclusive, the rel-ative success of the ?5k+100k?
setting shows thatfiltering fully abstract and partially lexicalized rulesseparately is a reasonable starting approach.
Whilefully abstract rules do tend to be more frequently ob-served in grammar extraction, and thus more reliablyscored in the translation model, they also have theability to overapply at decoding time because theiruse is not restricted to any particular lexical context.5 Conclusions and Future WorkWe demonstrated in Section 4.1 that the generalSCFG extraction algorithm described in this paperis capable of producing very large linguistically mo-tivated rule sets.
These rule sets can improve auto-matic metric scores at decoding time.
At the sametime, we see the results in Section 4.2 as a spring-board to more advanced and more intelligent meth-ods of grammar filtering.
Our major research ques-tion for future work is to determine how to make thebest runtime use of the grammars we can extract.As we saw in Section 2, multiple decompositionsof a single parse tree allow the same constituent tobe built in a variety of ways.
This is generally goodfor coverage, but its downside at run time is that thedecoder must manage a larger number of competing142derivations that, in the end, produce the same outputstring.
Grammar filtering that explicitly attempts tolimit the derivational ambiguity of the retained rulesmay prevent the translation model probabilities ofcorrect outputs from getting fragmented into redun-dant derivations.
So far we have only approximatedthis by using fully abstract rules as a proxy for themost derivationally ambiguous rules.Filtering based on the content of virtual nodesmay also be a reasonable strategy for selecting use-ful grammar rules and discarding those whose con-tributions are less necessary.
For example, we findin our current output many applications of rulesinvolving virtual nodes that consist of an open-class category and a mark of punctuation, such asVBD+COMMA and NN+PU.
While there is noth-ing technically wrong with these rules, they may notbe as helpful in translation as rules for nouns andadjectives such as JJ+NNP+NN or NNP+NNP in flatnoun phrase structures such as former U.S. presidentBill Clinton.A final concern in making use of our large ex-tracted grammars is the effect virtual nodes haveon the size of the nonterminal set.
The Stat-XFERbaseline grammar from our ?xfer-orig?
configura-tion uses a nonterminal set of 1,577 unique labels.In our rule extractor so far, we have adopted the con-vention of naming virtual nodes with a concatena-tion of their component sibling labels, separated by?+?s.
With the large number of virtual node labelsthat may be created, this gives our ?full-short?
and?full-long?
extracted grammars nonterminal sets ofaround 73,000 unique labels.
An undesirable conse-quence of such a large label set is that a particularSCFG right-hand side may acquire a large varietyof left-hand-side labels, further contributing to thederivational ambiguity problems discussed above.In future work, the problem could be addressed byreconsidering our naming scheme for virtual nodes,by allowing fuzzy matching of labels at translationtime (Chiang, 2010), or by other techniques aimedat reducing the size of the overall nonterminal set.AcknowledgmentsThis research was supported in part by U.S. NationalScience Foundation grants IIS-0713402 and IIS-0915327 and the DARPA GALE program.
We thankVamshi Ambati and Jon Clark for helpful discus-sions regarding implementation details of the gram-mar extraction algorithm.
Thanks to Chris Dyer forproviding the word-aligned and preprocessed FBIScorpus.
Finally, we thank Yahoo!
for the use ofthe M45 research computing cluster, where we ranmany steps of our experimental pipeline.ReferencesVamshi Ambati, Alon Lavie, and Jaime Carbonell.
2009.Extraction of syntactic translation models from paral-lel data using syntax from source and target languages.In Proceedings of the 12th Machine Translation Sum-mit, pages 190?197, Ottawa, Canada, August.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Meeting of the ACL, pages 263?270,Ann Arbor, MI, June.David Chiang.
2010.
Learning to translate with sourceand target syntax.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 1443?1452, Uppsala, Sweden, July.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
InHLT-NAACL 2004: Main Proceedings, pages 273?280, Boston, MA, May.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proceed-ings of the 21st International Conference on Compu-tational Linguistics and 44th Annual Meeting of theACL, pages 961?968, Sydney, Australia, July.Greg Hanneman, Jonathan Clark, and Alon Lavie.
2010.Improved features and grammar selection for syntax-based MT.
In Proceedings of the Joint Fifth Workshopon Statistical Machine Translation and MetricsMATR,pages 82?87, Uppsala, Sweden, July.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of HLT-NAACL 2003, pages 48?54, Edmonton,Alberta, May?June.Alon Lavie and Michael J. Denkowski.
2009.
TheMETEOR metric for automatic evaluation of machinetranslation.
Machine Translation, 23(2?3):105?115.Alon Lavie, Alok Parlikar, and Vamshi Ambati.
2008.Syntax-driven learning of sub-sentential translationequivalents and translation rules from parsed parallelcorpora.
In Proceedings of the Second ACL Work-shop on Syntax and Structure in Statistical Transla-tion, pages 87?95, Columbus, OH, June.143Zhifei Li, Chris Callison-Burch, Chris Dyer, JuriGanitkevitch, Sanjeev Khudanpur, Lane Schwartz,Wren N.G.
Thornton, Jonathan Weese, and Omar F.Zaidan.
2009.
Joshua: An open source toolkit forparsing-based machine translation.
In Proceedings ofthe Fourth Workshop on Statistical Machine Transla-tion, pages 135?139, Athens, Greece, March.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automatic eva-lution of machine translation.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics, pages 311?318, Philadelphia, PA,July.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of NAACLHLT 2007, pages 404?411, Rochester, NY, April.Matthew Snover, Bonnie Dorr, Richard Schwartz, LinneaMicciulla, and John Makhoul.
2006.
A study of trans-lation edit rate with targeted human annotation.
InProceedings of the Seventh Conference of the Associ-ation for Machine Translation in the Americas, pages223?231, Cambridge, MA, August.Omar F. Zaidan.
2009.
Z-MERT: A fully configurableopen source tool for minimum error rate training ofmachine translation systems.
The Prague Bulletin ofMathematical Linguistics, 91:79?88.Ventsislav Zhechev and Andy Way.
2008.
Automaticgeneration of parallel treebanks.
In Proceedings of the22nd International Conference on Computational Lin-guistics, pages 1105?1112, Manchester, England, Au-gust.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProceedings of the Workshop on Statistical MachineTranslation, pages 138?141, New York, NY, June.144
