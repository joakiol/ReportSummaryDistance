What Makes Writing Great?
First Experiments on Article QualityPrediction in the Science Journalism DomainAnnie LouisUniversity of PennsylvaniaPhiladelphia, PA 19104lannie@seas.upenn.eduAni NenkovaUniversity of PennsylvaniaPhiladelphia, PA 19104nenkova@seas.upenn.eduAbstractGreat writing is rare and highly admired.Readers seek out articles that are beautifullywritten, informative and entertaining.
Yetinformation-access technologies lack capabil-ities for predicting article quality at this level.In this paper we present first experiments onarticle quality prediction in the science jour-nalism domain.
We introduce a corpus ofgreat pieces of science journalism, along withtypical articles from the genre.
We imple-ment features to capture aspects of great writ-ing, including surprising, visual and emotionalcontent, as well as general features related todiscourse organization and sentence structure.We show that the distinction between greatand typical articles can be detected fairly ac-curately, and that the entire spectrum of ourfeatures contribute to the distinction.1 IntroductionMeasures of article quality would be hugely bene-ficial for information retrieval and recommendationsystems.
In this paper, we describe a dataset of NewYork Times science journalism articles which wehave categorized for quality differences and presenta system that can automatically make the distinction.Science journalism conveys complex scientificideas, entertaining and educating at the same time.Consider the following opening of a 2005 article byDavid Quammen from Harper?s magazine:One morning early last winter a small item appeared inmy local newspaper announcing the birth of an extraordi-nary animal.
A team of researchers at Texas A&M Uni-versity had succeeded in cloning a whitetail deer.
Neverdone before.
The fawn, known as Dewey, was developingnormally and seemed to be healthy.
He had no mother,just a surrogate who had carried his fetus to term.
Hehad no father, just a ?donor?
of all his chromosomes.
Hewas the genetic duplicate of a certain trophy buck outof south Texas whose skin cells had been cultured in alaboratory.
One of those cells furnished a nucleus that,transplanted and rejiggered, became the DNA core of anegg cell, which became an embryo, which in time be-came Dewey.
So he was wildlife, in a sense, and in an-other sense elaborately synthetic.
This is the sort of news,quirky but epochal, that can cause a person with a mouth-ful of toast to pause and marvel.
What a dumb idea, Imarveled.The writing is clear and well-organized but thetext also contains creative use of language and aclever story-like explanation of the scientific con-tribution.
Such properties make science journalisman attractive genre for studying writing quality.
Sci-ence journalism is also a highly relevant domain forinformation retrieval in the context of educationalas well as entertaining applications.
Article qualitymeasures can hugely benefit such systems.Prior work indicates that three aspects of articlequality can be successfully predicted: a) whethera text meets the acceptable standards for spelling(Brill and Moore, 2000), grammar (Tetreault andChodorow, 2008; Rozovskaya and Roth, 2010) anddiscourse organization (Barzilay et al 2002; Lap-ata, 2003); b) has a topic that is interesting to a par-ticular user.
For example, content-based recommen-dation systems standardly represent user interest us-ing frequent words from articles in a user?s historyand retrieve other articles on the same topics (Paz-341Transactions of the Association for Computational Linguistics, 1 (2013) 341?352.
Action Editor: Mirella Lapata.Submitted 12/2012; Revised 3/2013, 5/2013; Published 7/2013.
c?2013 Association for Computational Linguistics.zani et al 1996; Mooney and Roy, 2000); and c) iseasy to read for a target readership.
Shorter words(Flesch, 1948), less complex syntax (Schwarm andOstendorf, 2005) and high cohesion between sen-tences (Graesser et al 2004) typically indicate eas-ier and more ?readable?
articles.Less understood is the question of what makes anarticle interesting and beautifully written.
An earlyand influential work on readability (Flesch, 1948)also computed an interest measure with the hypoth-esis that interesting articles would be easier to read.More recently, McIntyre and Lapata (2009) foundthat people?s ratings of interest for fairy tales can besuccessfully predicted using token-level scores re-lated to syntactic items and categories from a psy-cholinguistic database.
But large scale studies of in-terest measures for adult educated readers have notbeen carried out.Further, there have been little attempts to measurearticle quality in a genre-specific setting.
But it isreasonable to expect that properties related to theunique aspects of a genre should contribute to theprediction of quality in the same way that domain-specific spelling and grammar correction (Cucerzanand Brill, 2004; Bao et al 2011; Dale and Kilgar-riff, 2010) techniques have been successful.Here we address the above two issues by develop-ing measures related to interesting and well-writtennature specifically for science journalism.
Centralto our work is a corpus of science news articles withtwo categories: written by popular journalists andtypical articles in science columns (Section 2).
Weintroduce a set of genre-specific features related tobeautiful writing, visual nature and affective content(Section 3) and show that they have high predictiveaccuracies, 20% above the baseline, for distinguish-ing our quality categories (Section 4).
Our final sys-tem combines the measures for interest and genre-specific features with those proposed for identifyingreadable, well-written and topically interesting arti-cles, giving an accuracy of 84% (Section 5).2 Article quality corpusOur corpus1 contains chosen articles from the largerNew York Times (NYT) corpus (Sandhaus, 2008),the latter containing a wealth of metadata about each1Available from http://www.cis.upenn.edu/?nlp/corpora/scinewscorpus.htmlarticle including author information and manuallyassigned topic tags.2.1 General corpusThe articles in the VERY GOOD category include allcontributions to the NYT by authors whose writingappeared in ?The Best American Science Writing?anthology published annually since 1999.
Articlesfrom the science columns of leading newspapers arenominated and expert journalists choose a set theyconsider exceptional to appear in these anthologies.There are 63 NYT articles in the anthology (betweenyears 1999 and 2007) that are also part of the digitalNYT corpus; these articles form the seed set of theVERY GOOD category.We further include in the VERY GOOD categoryall other science articles contributed to NYT by theauthors of the seed examples.
Science articles byother authors not in our seed set form the TYPICALcategory.
We perform this expansion by first creat-ing a relevant set of science articles.
There is nosingle meta-data tag in the NYT which refers to allthe science articles.
So we use the topic tags fromthe seed articles as an initial set of research tags.We then compute the minimal set of research tagsthat cover all best articles.
We greedily add tagsinto the minimal set, at each iteration choosing thetag that is present in the majority of articles that re-main uncovered.
This minimal set contains 14 tagssuch as ?Medicine and Health?, ?Space?, ?Research?,?Physics?
and ?Evolution?.We collect articles from the NYT which have atleast one of the minimal set tags.
However, evena cursory mention of a research topic results in aresearch-related tag being assigned to the article.
Sowe also use a dictionary of research-related termsto determine whether the article passes a minimumthreshold for research content.
We created this dic-tionary manually and it contains the following wordsand their morphological variants (total 63 items).We used our intuition about a few categories of re-search words to create this list.
The category isshown in capital letters below.PEOPLE: researcher, scientist, physicist, biologist, economist,anthropologist, environmentalist, linguist, professor, dr, studentPROCESS: discover, found, experiment, work, finding, study,question, project, discussTOPIC: biology, physics, chemistry, anthropology, primatology342PUBLICATIONS: report, published, journal, paper, author, issueOTHER: human, science, research, knowledge, university, lab-oratory, labENDINGS: -ology -gist, -list, -mist, -uist, -phyThe items in the ENDINGS category are usedto match word suffixes.
An article is consideredscience-related if at least 10 of its tokens match thedictionary and in addition, at least 5 unique wordsfrom the dictionary are matched.
Since the time spanof the best articles is 1999 to 2007, we limit our col-lection to this timespan.
In addition, we only con-sider articles that are at least 500 words long.
Thisfiltered set of 23,709 articles form the relevant set ofscience journalism.The 63 seed samples of great writing were con-tributed by about 40 authors.
Some authors havemultiple articles selected for the best writing bookseries, supporting the idea that these authors producehigh quality pieces that can be considered distinctfrom typical articles.
Separating the articles fromthese authors gives us 3,467 extra samples of VERYGOOD writing.
In total, the VERY GOOD set has3,530 articles.
The remaining articles from the rele-vant set, 20,242, written by about 3000 other authorsform the TYPICAL category.2.2 Topic-paired corpusThe general corpus of science writing introduced sofar contains articles on diverse topics including bi-ology, astronomy, religion and sports.
The VERYGOOD and TYPICAL categories created above al-low us to study writing quality without regard totopic.
However a typical information retrieval sce-nario would involve comparison between articles ofthe same topic, i.e.
relevant to the same query.
Toinvestigate how quality differentiation can be donewithin topics, we created another corpus where wepaired articles of VERY GOOD and TYPICAL quality.For each article in the VERY GOOD category, wecompute similarity with all articles in the TYPICALset.
This similarity is computed by comparing thetopic words (computed using a loglikelihood ratiotest (Lin and Hovy, 2000)) of the two articles.
Weretain the most similar 10 TYPICAL articles for eachVERY GOOD article.
We enumerate all pairs of VERYGOOD with matched up TYPICAL ARTICLES (10 innumber) giving a total of 35,300 pairs.There are two distinguishing aspects of our cor-pus.
First, the average quality of articles is high.They are unlikely to have spelling, grammar and ba-sic organization problems allowing us to investigatearticle quality rather than the detection of errors.Second, our corpus contains more realistic samplesof quality differences for IR or article recommen-dation compared to prior work, where system pro-duced texts and permuted versions of an original ar-ticle were used as proxies for lower quality text.2.3 TasksWe perform two types of classification tasks.
Wedivide our corpus into development and test sets forthese tasks in the following way.Any topic: Here the goal is to separate out VERYGOOD versus TYPICAL articles without regard totopic.
The setting roughly corresponds to pickingout an interesting article from an archive or a day?snewspaper.
The test set contains 3,430 VERY GOODarticles and we randomly sample 3,430 articles fromthe TYPICAL category to comprise the negative set.Same topic: Here we use the topic-paired VERYGOOD and TYPICAL articles.
The goal is to predictwhich article in the pair is the VERY GOOD one.
Thistask is closer to a information retrieval setting, wherearticles similar in topic (retrieved for a user query)need to be distinguished for quality.
For test set, weselected 34,300 pairs.Development data: We randomly selected 100VERY GOOD articles and their paired (10 each)TYPICAL articles from the topic-normalized corpus.Overall, these constitute 1,000 pairs which we usefor developing the same-topic classifier.
From theseselected pairs we take the 100 VERY GOOD articlesand sample 100 unique articles from the TYPICALarticles making up the pairs.
These 200 articles areused to tune the any-topic classifier.3 Facets of science writingIn this section, we discuss six prominent facets ofscience writing which we hypothesized will havean impact on text quality.
These are the presenceof passages of highly visual nature, people-orientedcontent, use of beautiful language, sub-genres, sen-timent or affect, and the depth of research descrip-tion.
Several other properties of science writingcould also be relevant to quality such as the use of343humor, metaphor, suspense and clarity of explana-tions and we plan to explore these in future work.We describe how we computed features related toeach property and tested how these features are dis-tributed in the VERY GOOD and TYPICAL categories.To do this analysis, we randomly sampled 1,000 ar-ticles from each of the two categories as representa-tive examples.
We compute the value of each featureon these articles and use a two-sided t-test to checkif the mean value of the feature is higher in one classof articles.
A p-value less than 0.05 is taken to in-dicate significantly different trend for the feature inthe VERY GOOD versus TYPICAL articles.Note that our feature computation step is nottuned for the quality prediction task in any way.Rather we aim to represent each facet as accuratelyas possible.
Ideally we would require manual anno-tations for each facet (visual, sentiment nature etc.
)to achieve this goal.
At this time, we simply checksome chosen features?
values on a random collectionof snippets from our corpus and check if they behaveas intended without resorting to these annotations.3.1 Visual nature of articlesSome texts create an image in the reader?s mind.
Forexample, the snippet below has a high visual effect.When the sea lions approached close, seemingly as curiousabout us as we were about them, their big brown eyes wereencircled by light fur that looked like makeup.
One sea lionplayed with a conch shell as if it were a ball.Such vivid descriptions can engage and entertaina reader.
Kosslyn (1980) found that people spon-taneously form images of concrete words that theyhear and use them to answer questions or performother tasks.
Books written for student science jour-nalists (Blum et al 2006; Stocking, 2010) also em-phasize the importance of visual descriptions.We measure the visual nature of a text by count-ing the number of visual words.
Currently, the onlyresource of imagery ratings for words is the MRCpsycholinguistic database (Wilson, 1988).
It con-tains a list of 3,394 words rated for their ability toinvoke an image, so the list contains both words thatare highly visual along with words that are not visualat all.
With a cutoff value we adopted, of 4.5 for theGilhooly-Logie and 350 for the Bristol Norms2 we2The visual words resource in MRC contains two lists?obtain 1,966 visual words.
So the coverage of thatlexicon is likely to be low for our corpus.We collect a larger set of visual words from a cor-pus of tagged images from the ESP game (von Ahnand Dabbish, 2004).
The corpus contains 83,904total images and 27,466 unique tags.
The averagenumber of tags per picture is 14.5.
The tags werecollected in a game setting where two users individ-ually saw the same image and had to guess wordsrelated to it.
The players increased their scores whenthe word guessed by one player matched that of theother.
Due to the simple annotation method, thereis considerable noise and non-visual words assignedas tags.
So we performed filtering to find high pre-cision image words and also group them into topics.We use Latent Dirichlet Allocation (Blei et al2003) to cluster image tags into topics.
We treat eachpicture as a document and the tags assigned to thepicture are the document?s contents.
We use sym-metric priors set to 0.01 for both topic mixture andword distribution within each topic.
We filter out the30 most common words in the corpus, words that ap-pear in less than four pictures and images with fewerthan five tags.
The remaining words are clusteredinto 100 topics with the Stanford Topic ModelingToolbox3 (Ramage et al 2009).
We did not tune thenumber of topics and choose the value of 100 basedon the intuition that the number of visual topics islikely to be small.To select clean visual clusters, we make the as-sumption that visual words are likely to be clusteredwith other visual terms.
Topics that are not visualare discarded altogether.
We use the manual an-notations available with the MRC database to de-termine which clusters are visual.
For each of the100 topics from the topic model, we examine thetop 200 words with highest probability in that topic.We compute the precision of each topic as the pro-portion of these 200 words that match the MRC listof visual words (1,966 words using the cutoff men-tioned above).
Only those topics which had a pre-cision of at least 25% were retained, resulting in 68visual topics.
Some example topics, with manuallycreated headings, include:landscape.
grass, mountain, green, hill, blue, field,brown, sand, desert, dirt, landscape, skyGilhooly-Logie and Bristol Norms.3http://nlp.stanford.edu/software/tmt/tmt-0.4/344jewellery.
silver, white, diamond, gold, necklace,chain, ring, jewel, wedding, diamonds, jewelryshapes.
round, ball, circles, logo, dots, square, dot,sphere, glass, hole, oval, circleCombining these 68 topics, there are 5,347 uniquevisual words because topics can overlap in the list ofmost probable words.
2,832 words from this set arenot present in the MRC database.
Some examplesof new words in our list are ?daffodil?, ?sailor?, ?hel-met?, ?postcard?, ?sticker?, ?carousel?, ?kayak?, and?camouflage?.
For later experiments we consider the5,347 words as the visual word set and also keepthe information about the top 200 words in the 68selected topics.
We compute two classes of featuresone based on all visual words and the other on visualtopics.
We consider only the adjectives, adverbs,verbs and common nouns in an article as candidatewords for computing visual quality.Overall visual use: We compute the propor-tion of candidate words that match the visualword list as the TOTAL VISUAL feature.
We alsocompute the proportions based only on the first200 words of the article (BEG VISUAL), the last200 words (END VISUAL) and the middle region(MID VISUAL) as features.
We also divide the arti-cle into five equally sized bins of words where eachbin captures consecutive words in the article.
Withineach bin we compute the proportion of visual words.We treat these values as a probability distributionand compute its entropy (ENTROPY VISUAL).
Weexpected these position features to indicate how theplacement of visual words is related to quality.Topic-based features: We also compute what pro-portion of the words we identify as visual matchesthe list under each topic.
The maximum proportionfrom a single topic (MAX TOPIC VISUAL) is a fea-ture.
We also compute a greedy cover set of top-ics for the visual words in the article.
The topicthat matches the most visual words is added first,and the next topic is selected based on the remain-ing unmatched words.
The number of topics neededto cover 50% of the article?s visual words is theTOPIC COVER VISUAL feature.
These features cap-ture the mix of visual words from different topics.Disregarding topic information, we also compute afeature NUM PICTURES which is the number of im-ages in the corpus where 40% of the image?s tags arematched in the article.We found 8 features to vary significantly be-tween the two types of articles.
The fea-tures with significantly higher values in VERYGOOD articles are: BEG VISUAL, END VISUAL,MAX TOPIC VISUAL.
The features with signifi-cantly higher values in the TYPICAL articles are:TOTAL VISUAL, MID VISUAL, ENTROPY VISUAL,TOPIC COVER VISUAL, NUM PICTURES.It appears that the simple expectation that VERYGOOD articles contain more visual words overalldoes not hold true here.
However the great writ-ing samples have a higher degree of visual contentin the beginning and ends of articles.
Good articlesalso have lower entropy for the distribution of vi-sual words indicating that they appear in localizedpositions in contrast to being distributed throughout.The topic-based features further indicate that for theVERY GOOD articles, the visual words come fromonly a few topics (compared to TYPICAL articles)and so may evoke a coherent image or scene.3.2 The use of people in the storyWe hypothesized that articles containing researchfindings that directly affect people in some way, andtherefore involve explicit references to people in thestory, will make a bigger impact on the reader.
Forexample, the most frequent topic among our VERYGOOD samples is ?medicine and health?
where ar-ticles are often written from the view of a patient,doctor or scientist.
An example is below.Dr.
Remington was born in Reedville, Va., in 1922, to Maudand P. Sheldon Remington, a school headmaster.
Charles spenthis boyhood chasing butterflies alongside his father, also a col-lector.
During his graduate studies at Harvard, he founded theLepidopterists?
Society with an equally butterfly-smitten under-graduate, Harry Clench.We approximate this facet by computing the num-ber of explicit references to people, relying on threesources of information about animacy of words.
Thefirst is named entity (NE) tags (PERSON, ORGANI-ZATION and LOCATION) returned by the StanfordNE recognition tool (Finkel et al 2005).
We alsocreated a list of personal pronouns such as ?he?, ?my-self?
etc.
which standardly indicate animate entities(animate pronouns).Our third resource contains the number of timesdifferent noun phrases (NP) were followed by eachof the relative pronouns ?who?, ?where?
and ?which?.345These counts for 664,673 noun phrases were col-lected by Ji and Lin (2009) from the Google NgramCorpus (Lin et al 2010).
We use a simple heuris-tic to obtain a list of animate (google animate) andinanimate nouns (google inanimate) from this list.The head of each NP is taken as a candidate noun.If the noun does not occur with ?who?
in any of thenoun phrases where it is the head, then it is inani-mate.
In contrast, if it appears only with ?who?
inall noun phrases, it is animate.
Otherwise, for eachNP where the noun is a head, we check whether thecount of times the noun phrase appeared with ?who?is greater than each of the occurrences of ?which?,?where?
and ?when?
(taken individually) with thatnoun phrase.
If the condition holds for at least onenoun phrase, the noun is marked as animate.When computing the features for an article, weconsider all nouns and pronouns as candidate words.If the word is a pronoun and appears in our list of an-imate pronouns, it is assigned an ?animate?
label and?inanimate?
otherwise.
If the word is a proper nounand tagged with the PERSON NE tag, we mark itas ?animate?
and if it is a ORGANIZATION or LO-CATION tag, the word is ?inanimate?.
For commonnouns, we check it if appears in the google animateand inanimate lists.
Any match is labelled accord-ingly as ?animate?
and ?inanimate?.
Note that thisprocedure may leave some nouns without any labels.Our features are counts of animate tokens(ANIM), inanimate tokens (INAMIN) and both thesecounts normalized by total words in the article(ANIM PROP, INANIM PROP).
Three of these fea-tures had significantly higher mean values in theTYPICAL category of articles: ANIM, ANIM PROP,INANIM PROP.
We found upon observation that sev-eral articles that talk about government policies in-volve a lot of references to people but are often in theTYPICAL category.
These findings suggest that the?human?
dimension might need to be computed notonly based on simple counts of references to peoplebut also involve finer distinctions between them.3.3 Beautiful languageBeautiful phrasing and word choice can entertainthe reader and leave a positive impression.
Multi-ple studies in the education genre (Diederich, 1974;Spandel, 2004) note that when teachers and expertadult readers graded student writing, word choiceand phrasing always turn out as a significant factorsinfluencing the raters?
scores.We implement a method for detecting creativelanguage based on a simple idea that creative wordsand phrases are sometimes those that are used in un-usual contexts and combinations or those that soundunusual.
We compute measures of unusual languageboth at the level of individual words and for the com-bination of words in a syntactic relation.Word level measures: Unusual words in an ar-ticle are likely to be those with low frequenciesin a background corpus.
We use the full set ofarticles (not only science) from year 1996 in theNYT corpus as a background (these do not over-lap with our corpus for article quality).
We also ex-plore patterns of letters and phoneme sequences withthe idea that unusual combination of characters andphonemes could create interesting words.
We usedthe CMU pronunciation dictionary (Weide, 1998) toget the phoneme information for words and built a 4-gram model of phonemes on the background corpus.Laplace smoothing is used to compute probabilitiesfrom the model.
However, the CMU dictionary doesnot contain phoneme information for several wordsin our corpus.
So we also compute an approximatemodel using the letters in the words and obtain an-other 4-gram model.4 Only words that are longerthan 4 characters are used in both models and we fil-ter out proper names, named entities and numbers.During development, we analyzed the articlesfrom an entire year of NYT, 1997, with the threemodels to identify unusual words.
Below is the listof words with lowest frequency and those with high-est perplexity under the phoneme and letter models.Low frequency.
undersheriff, woggle, ahmok,hofman, volga, oceanaut, trachoma, baneful, truffler,acrimal, corvair, entomopterHigh perplexity-phoneme model.
showroom, yahoo,dossier, powwow, plowshare, oomph, chihuahua, iono-sphere, boudoir, superb, zaire, oeuvreHigh perplexity-letter model.
kudzu, muumuu, qi-pao, yugoslav, kohlrabi, iraqi, yaqui, yakuza, jujitsu, oeu-vre, yaohan, kaffiyehFor computing the features, we consider onlynouns, verbs, adjectives and adverbs.
We alsorequire that the words are at least 5 letters long4We found that higher order n-grams provided better pre-dictions of unusual nature during development.346and do not contain a hyphen5.
Three types ofscores are computed.
FREQ NYT is the aver-age of word frequencies computed from the back-ground corpus.
The second set of features arebased on the phoneme model.
We compute theaverage perplexity of words under the model,AVR PHONEME PERP ALL.
In addition, we also or-der the words in an article based on decreasing per-plexity values and the average perplexity of the top10, 20 and 30 words in this list are added as fea-tures (AVR PHONEME PERP 10, 20, 30).
We ob-tain similar features from the letter n-gram model(AVR CHAR PERP ALL, AVR CHAR PERP 10, 20,30).
In phoneme features, we ignore words that donot have an entry in the CMU dictionary.Word pair measures: Next we attempt to detect un-usual combinations of words.
We do this calculationonly for certain types of syntactic relations?a) nounsand their adjective modifiers, b) verbs with adverbmodifiers, c) adjacent nouns in a noun phrase andd) verb and subject pairs.
Counts for co-occurrenceagain come from NYT 1996 articles.
The syntacticrelations are obtained using the constituency and de-pendency parses from the Stanford parser (Klein andManning, 2003; De Marneffe et al 2006).
To avoidthe influence of proper names and named entities,we replace them with tags (NNP for proper namesand PERSON, ORG, LOC for named entities).We treat the words for which the dependencyholds as a (auxiliary word, main word) pair.
Foradjective-noun and adverb-verb pairs, the auxiliaryis the adjective or adverb; for noun-noun pairs, it isthe first noun; and for verb-subject pairs, the auxil-iary is the subject.
Our idea is to compute usualnessscores based on frequency with which a particularpair of words appears in the background.Specifically, we compute the conditional proba-bility of the auxiliary word given the main wordas the score for likelihood of observing the pair.We consider the main word as related to the articletopic, so we use the conditional probability of auxil-iary given main word and not the other way around.However, the conditional probability has no infor-mation about the frequency of the auxiliary word.
Sowe apply ideas from interpolation smoothing (Chen5We noticed that in this genre several new words are createdusing hyphen to concatenate common words.ADJ-NOUN ADV-VERBhypoactive NNP suburbs saidplasticky woman integral waspsychogenic problems collective doyoplait television physiologically dosubminimal level amuck runehatchery investment illegitimately putNOUN-NOUN SUBJ-VERBspecification today blog saidauditory system briefer saidpal programs hr saidsteganography programs knucklehead saidwastewater system lymphedema haveautism conference permissions haveTable 1: Unusual word-pairs from different categoriesand Goodman, 1996) and compute the conditionalprobability as a interpolated quantity together withthe unigram probability of the auxiliary word.p?
(aux|main) = ??p(aux|main)+(1??
)?p(aux)The unigram and conditional probabilities arealso smoothed using Laplace method.
We train thelambda values to optimize data likelihood using theBaum Welch algorithm and use the pairs from NYT1997 year articles as a development set.
The lambdavalues across all types of pairs tended to be lowerthan 0.5 giving higher weight to the unigram proba-bility of the auxiliary word.Based on our observations on the developmentset, we picked a cutoff of 0.0001 on the proba-bility (0.001 for adverb-verb pairs) and considerphrases with probability below this value as un-usual.
For each test article, we compute the num-ber of unusual phrases (total for all categories)as a feature (SURP) and also this value normal-ized by total number of word tokens in the article(SURP WD) and normalized by number of phrases(SURP PH).
We also compute features for indi-vidual pair types and in each case, the number ofunusual phrases is normalized by the total wordsin the article (SURP ADJ NOUN, SURP ADV VERB,SURP NOUN NOUN, SURP SUBJ VERB).A list of the top unusual words under the differentpair types are shown in Table 1.
These were com-puted on pairs from a random set of articles from ourcorpus.
Several of the top pairs involve hyphenatedwords which are unusual by themselves, so we onlyshow in the table the top words without hyphens.347Most of these features are significantlydifferent between the two classes.
Thosewith higher values in the VERY GOODset include: AVR PHONEME PERP ALL,AVR CHAR PERP (ALL, 10), SURP, SURP PH,SURP WD, SURP ADJ NOUN, SURP NOUN NOUN,SURP SUBJ VERB.
The FREQ NYT feature hashigher value in the TYPICAL class.All these trends indicate that unusual phrases areassociated with the VERY GOOD category of articles.3.4 Sub-genresThere are several sub-genres within science writing(Stocking, 2010): short descriptions of discoveries,longer explanatory articles, narratives, stories aboutscientists, reports on meetings, review articles andblog posts.
Naturally, some of these sub-genres willbe more appealing to readers.
To investigate thisaspect, we compute scores for some sub-genres ofinterest?narrative, attribution and interview.Narrative texts typically have characters andevents (Nakhimovsky, 1988), so we look for entitiesand past tense in the articles.
We count the numberof sentences where the first verb in surface order isin the past tense.
Then among these sentences, wepick those which have either a personal pronoun or aproper noun before the target verb (again in surfaceorder).
The proportion of such sentences in the textis taken as the NARRATIVE score.We also developed a measure to identify the de-gree to which the article?s content is attributed to ex-ternal sources as opposed to the author?s own state-ments.
Attribution to other sources is frequent inthe news domain since many comments and opin-ions are not the views of the journalist (Semetko andValkenburg, 2000).
For science news, attribution be-comes more important since the research findingswere obtained by scientists and reported in a second-hand manner by the journalists.
The ATTRIB scoreis the proportion of sentences in the article that havea quote symbol, or the words ?said?
and ?says?.We also compute a score to indicate if the articleis the account of an interview.
There are easy cluesin NYT for this genre with paragraphs in the inter-view portion of the article beginning with either ?Q.?
(question) or ?A.?
(answer).
We count the total num-ber of ?Q.?
and ?A.?
prefixes combined and dividethe value by the total number of sentences (INTER-VIEW).
When either the number of ?Q.?
tags is zeroor ?A.?
tags is zero, the score is set to zero.All three scores are significantly higher for theTYPICAL class.3.5 Affective contentSome articles, for example those detailing researchon health, crime, ethics, can provoke emotional re-actions in readers as shown in the snippet below.Medicine is a constant trade-off, a struggle to cure the dis-ease without killing the patient first.
Chemotherapy, for exam-ple, involves purposely poisoning someone ?
but with the ex-pectation that the short-term injury will be outweighed by theeventual benefits.We compute affect-related features using threelexicons.
The MPQA (Wilson et al 2005) and Gen-eral Inquirer (Stone et al 1966) give lists of positiveand negative sentiment words.
The third resourceis emotion-related words from FrameNet (Baker etal., 1998).
The sizes of these lexicon are 8,221,5,395, and 653 words respectively.
We computethe counts of positive, negative, polar, and emotionwords, each normalized by the total number of con-tent words in the article (POS PROP, NEG PROP, PO-LAR PROP, EMOT PROP).
We also include the pro-portion of emotion and polar words taken together(POLAR EMOT PROP) and the ratio between countof positive and negative words (POS BY NEG).The features with higher values in the VERYGOOD class are NEG PROP, POLAR PROP,EMOT POLAR PROP.
In TYPICAL articles,POS BY NEG, EMOT PROP have higher values.VERY GOOD articles have more sentiment words,mostly skewed towards negative sentiment.3.6 Amount of research contentFor a lay audience, a science writer presents only themost relevant findings and methods from a researchstudy and interleaves research information with de-tails about the relevance of the finding, people in-volved in the research and general information aboutthe topic.
As a result, the degree of explicit researchdescriptions in the articles varies considerably.To test how this aspect is associated with qual-ity, we count references to research methods and re-searchers in the article.
We use the research dictio-nary that we introduced in Section 2 as the sourceof research-related words.
We count the total num-348ber of words in the article that match the dictionary(RES TOTAL) and also the number of unique match-ing words (RES UNIQ).
We also normalize thesecounts by the total words in the article and createfeatures RES TOTAL PROP and RES UNIQ PROP.All four features have significantly higher valuesin the VERY GOOD articles which indicate that greatarticles are also associated with a great amount ofdirect research content and explanations.4 Classification accuracyWe trained classifiers using all the above features forfor the two settings??any-topic?
and ?same-topic?
in-troduced in Section 2.3.
The baseline random accu-racy in both cases is 50%.
We use a SVM classi-fier with a radial basis kernel (R Development CoreTeam, 2011) and parameters were tuned using crossvalidation on the development data.The best parameters were then used to classify thetest set in a 10 fold cross-validation setting.
We di-vide the test set into 10 parts, train on 9 parts andtest on the held-out data.
The average accuracies inthe 10 experiments are 75.3% accuracy for the ?any-topic?
setup, and 68% accuracy for the topic-paired?same-topic?
setup.
These accuracies are consider-able improvements over the baseline.The ?same-topic?
data contains article pairs withvarying similarity.
So we investigate the relationshipbetween topic similarity and accuracy of predictionmore closely for this setting.
We divide the articlepairs into bins based on the similarity value.
Wecompute the 10-fold cross validation predictions us-ing the different feature classes above and collect thepredicted values across all the folds.
Then we com-pute accuracy of examples within each bin.
Theseresults are plotted in Figure 1. int-science refers tothe full set of features and the results from the sixfeature classes are also indicated.As the similarity increases, the prediction task be-comes harder.
The combination of all features gives66% accuracy for pairs above 0.4 similarity and 74%when the similarity is less than 0.15.
Most individ-ual feature classes also show a similar trend.
Thisresult is understandable because articles on simi-lar topics could exhibit similar properties.
For ex-ample, two articles about ?controversies surround-ing vaccination?
are likely to have similar levels ofpeople-oriented nature or written in a narrative styleFigure 1: Accuracy on pairs with different similarityin the same way as two space-related articles areboth likely to contain high visual content.
There arehowever two exceptions?affect and research.
Forthese features, the accuracies improve with highersimilarity; affect features give 51% for pairs withsimilarity 0.1 and 62% for pairs above 0.4 similar-ity, accuracy of research features goes from 52% to57% for the same similarity values.
This finding il-lustrates that even articles on very similar topics canbe written differently, with the articles by the excel-lent authors associated with greater degree of senti-ment, and deeper study of the research problem.5 Combining aspects of article qualityWe now compare and combine the genre-specificinterest-science features (41 total) with those dis-cussed in work on readability, well-written nature,interest and topic classification.Readability (16 features): We test the full set ofreadability features studied in Pitler and Nenkova(2008), involving token-type ratio, word and sen-tence length, language model features, cohesionscores and syntactic estimates of complexity.Well-written nature (23 features): For well-written nature, we use two classes of features, bothrelated to discourse.
One is the probabilities of dif-ferent types of entity transitions from the Entity Gridmodel (Barzilay and Lapata, 2008) which we com-pute with the Brown Coherence Toolkit (Elsner etal., 2007).
The other class of features are those de-fined in Pitler and Nenkova (2008) for likelihoodsand counts of explicit discourse relations.
We iden-tified the relations for texts in our corpus using the349AddDiscourse tool (Pitler and Nenkova, 2009).Interesting fiction (22 features): are those intro-duced by McIntyre and Lapata (2009) for predictinginterest ratings on fairy tales.
They include counts ofsyntactic items and relations, and token categoriesfrom the MRC psycholinguistic database.
We nor-malize each feature by the total words in the article.Content: features are based on the words presentin the articles.
Word features are standard incontent-based recommendation systems (Pazzani etal., 1996; Mooney and Roy, 2000) where they areused to pick out articles similar to those which a userhas already read.
In our work the features are themost frequent n words in our corpus after removingthe 50 most frequent ones.
The word?s count in thearticle is the feature value.
Note that word featuresindicate topic as well as other content in the articlesuch as sentiment and research.
A random sample ofthe word features for n = 1000 is shown below andreflects this aspect.
?matter, series, wear, nation, ac-count, surgery, high, receive, remember, support, worry,enough, office, prevent, biggest, customer?.Table 2 compares the accuracies of SVM classi-fiers trained on features from different classes andtheir combinations.6 The readability, well-writtennature and interesting fiction classes provide goodaccuracies 60% and above.
The genre-specificinteresting-science features are individually muchstronger than these classes.
Different writing as-pects (without content) are clearly complementaryand when combined give 76% to 79% accuracy forthe ?any-topic?
task and 74% for the topic pairs task.The simple bag of words features work remark-ably well giving 80% accuracy in both settings.
Asmentioned before these word features are a mix oftopic indicators as well as other content of the ar-ticles, ie., they also implicitly indicate animacy, re-search or sentiment.
But the high accuracy of wordfeatures above all the writing categories indicatesthat topic plays an important role in article quality.However, despite the high accuracy, word featuresare not easily interpretable in different classes re-lated to writing as we have done with other writingfeatures.
Further, the total set of writing features is6For classifiers involving content features, we did not tunethe SVM parameters because of the small size of developmentdata compared to number of features.
Default SVM settingswere used instead.Feature set Any Topic SameInteresting-science 75.3 68.0Readable 65.5 63.0Well-written 59.1 59.9Interesting-fiction 67.9 62.8Readable + well-writ 64.7 64.3Readable + well-writ + Int-fict 71.0 70.3Readable + well-writ + Int-sci 79.5 73.2All writing aspects 76.7 74.7Content (500 words) 81.7 79.4Content (1000 words) 81.2 82.1Combination: Writing (all) + Content (1000w)In feature vector 82.6* 84.0*Sum of confidence scores 81.6 84.9Oracle 87.6 93.8Table 2: Accuracy of different article quality aspectsonly 102 in contrast to 1000 word features.
In ourinterest-science feature set, we aimed to highlighthow well some of the aspects considered importantto good science writing can predict quality ratings.We also combined writing and word features tomix topic with writing related predictors.
We do thecombination in three ways a) word and writing fea-tures are included together in the feature vector; b)two separate classifiers are trained (one using wordfeatures and the other using writing ones) and thesum of confidence measures is used to decide on thefinal class; c) an oracle method: two classifiers aretrained just as in option (b) but when they disagreeon the class, we pick the correct label.
The oraclemethod gives a simple upper bound on the accuracyobtainable by combination.
These values are 87%for ?any-topic?
and a higher 93.8% for ?same-topic?.The automatic methods, both feature vector combi-nation and classifier combination also give better ac-curacies than only the word or writing features.
Theaccuracies for the folds from 10 fold cross valida-tion in the feature vector combination method werealso found to be significantly higher than those fromword features only (using a paired Wilcoxon signed-rank test).
Therefore both topic and writing featuresare clearly useful for identifying great articles.6 ConclusionOur work is a step towards measuring overall arti-cle quality by showing the complementary benefitsof general and domain-specific writing measures aswell as indicators of article topic.
In future we planto focus on development of more features as well asbetter methods for combining different measures.350ReferencesC.
F. Baker, C. J. Fillmore, and J.
B. Lowe.
1998.The berkeley framenet project.
In Proceedings ofCOLING-ACL, pages 86?90.Z.
Bao, B. Kimelfeld, and Y. Li.
2011.
A graph ap-proach to spelling correction in domain-centric search.In Proceedings of ACL-HLT, pages 905?914.R.
Barzilay and M. Lapata.
2008.
Modeling local coher-ence: An entity-based approach.
Computational Lin-guistics, 34(1):1?34.R.
Barzilay, N. Elhadad, and K. McKeown.
2002.Inferring strategies for sentence ordering in multi-document summar ization.
Journal of Artificial Intel-ligence Research, 17:35?55.D.M.
Blei, A.Y.
Ng, and M.I.
Jordan.
2003.
Latentdirichlet alcation.
the Journal of machine Learningresearch, 3:993?1022.D.
Blum, M. Knudson, and R. M. Henig, editors.
2006.A field guide for science writers: the official guide ofthe national association of science writers.
OxfordUniversity Press, New York.E.
Brill and R.C.
Moore.
2000.
An improved error modelfor noisy channel spelling correction.
In Proceedingsof ACL, pages 286?293.S.
F. Chen and J. Goodman.
1996.
An empirical studyof smoothing techniques for language modeling.
InProceedings of ACL, pages 310?318.S.
Cucerzan and E. Brill.
2004.
Spelling correction as aniterative process that exploits the collective knowledgeof web users.
In Proceedings of EMNLP, pages 293?300.R.
Dale and A. Kilgarriff.
2010.
Helping our own:text massaging for computational linguistics as a newshared task.
In Proceedings of INLG, pages 263?267.M.
C. De Marneffe, B. MacCartney, and C. D. Man-ning.
2006.
Generating typed dependency parses fromphrase structure parses.
In Proceedings of LREC, vol-ume 6, pages 449?454.P.
Diederich.
1974.
Measuring Growth in English.
Na-tional Council of Teachers of English.M.
Elsner, J. Austerweil, and E. Charniak.
2007.
A uni-fied local and global model for discourse coherence.In Proceedings of NAACL-HLT, pages 436?443.J.
R. Finkel, T. Grenager, and C. Manning.
2005.
In-corporating non-local information into information ex-traction systems by gibbs sampling.
In Proceedings ofACL, pages 363?370.R.
Flesch.
1948.
A new readability yardstick.
Journal ofApplied Psychology, 32:221 ?
233.A.C.
Graesser, D.S.
McNamara, M.M.
Louwerse, andZ.
Cai.
2004.
Coh-Metrix: Analysis of text on co-hesion and language.
Behavior Research Methods In-struments and Computers, 36(2):193?202.H.
Ji and D. Lin.
2009.
Gender and animacy knowledgediscovery from web-scale n-grams for unsupervisedperson name detection.
In Proceedings of PACLIC.D.
Klein and C.D.
Manning.
2003.
Accurate unlexical-ized parsing.
In Proceedings of ACL, pages 423?430.S.M.
Kosslyn.
1980.
Image and mind.
Harvard Univer-sity Press.M.
Lapata.
2003.
Probabilistic text structuring: Experi-ments with sentence ordering.
In Proceedings of ACL,pages 545?552.C.
Lin and E. Hovy.
2000.
The automated acquisition oftopic signatures for text summarization.
In Proceed-ings of COLING, pages 495?501.D.
Lin, K. W. Church, H. Ji, S. Sekine, D. Yarowsky,S.
Bergsma, K. Patil, E. Pitler, R. Lathbury, V. Rao,K.
Dalwani, and S. Narsale.
2010.
New tools for web-scale n-grams.
In Proceedings of LREC.N.
McIntyre and M. Lapata.
2009.
Learning to tell tales:A data-driven approach to story generation.
In Pro-ceedings of ACL-IJCNLP, pages 217?225.R.
J. Mooney and L. Roy.
2000.
Content-based bookrecommending using learning for text categorization.In Proceedings of the fifth ACM conference on Digitallibraries, pages 195?204.A.
Nakhimovsky.
1988.
Aspect, aspectual class, and thetemporal structure of narrative.
Computational Lin-guistics, 14(2):29?43, June.M.
Pazzani, J. Muramatsu, and D. Billsus.
1996.
Syskill& Webert: Identifying interesting web sites.
In Pro-ceedings of AAAI, pages 54?61.E.
Pitler and A. Nenkova.
2008.
Revisiting readabil-ity: A unified framework for predicting text quality.
InProceedings of EMNLP, pages 186?195.E.
Pitler and A. Nenkova.
2009.
Using syntax to dis-ambiguate explicit discourse connectives in text.
InProceedings of ACL-IJCNLP, pages 13?16.R Development Core Team, 2011.
R: A Language andEnvironment for Statistical Computing.
R Foundationfor Statistical Computing.D.
Ramage, D. Hall, R. Nallapati, and C.D.
Manning.2009.
Labeled lda: A supervised topic model forcredit attribution in multi-labeled corpora.
In Proceed-ings of EMNLP, pages 248?256.A.
Rozovskaya and D. Roth.
2010.
Generating confu-sion sets for context-sensitive error correction.
In Pro-ceedings of EMNLP, pages 961?970.E.
Sandhaus.
2008.
The new york times annotated cor-pus.
Corpus number LDC2008T19, Linguistic DataConsortium, Philadelphia.S.
Schwarm and M. Ostendorf.
2005.
Reading level as-sessment using support vector machines and statisticallanguage models.
In Proceedings of ACL, pages 523?530.351H.A.
Semetko and P.M. Valkenburg.
2000.
Framing eu-ropean politics: A content analysis of press and televi-sion news.
Journal of communication, 50(2):93?109.V.
Spandel.
2004.
Creating Writers Through 6-TraitWriting: Assessment and Instruction.
Allyn and Ba-con, Inc.S.
H. Stocking.
2010.
The New York Times Reader: Sci-ence and Technology.
CQ Press, Washington DC.P.
J.
Stone, J. Kirsh, and Cambridge Computer Asso-ciates.
1966.
The General Inquirer: A Computer Ap-proach to Content Analysis.
MIT Press.J.
R. Tetreault and M. Chodorow.
2008.
The ups anddowns of preposition error detection in esl writing.
InProceedings of COLING, pages 865?872.L.
von Ahn and L. Dabbish.
2004.
Labeling images witha computer game.
In Proceedings of CHI, pages 319?326.R.
L. Weide.
1998.
The cmu pronunciation dictio-nary, release 0.6. http://www.speech.cs.cmu.edu/cgi-bin/cmudict.T.
Wilson, J. Wiebe, and P. Hoffmann.
2005.
Recogniz-ing contextual polarity in phrase-level sentiment anal-ysis.
In Proceedings of HLT-EMNLP, pages 347?354.M.
Wilson.
1988.
MRC psycholinguistic database:Machine-usable dictionary, version 2.00.
BehaviorResearch Methods, 20(1):6?10.352
