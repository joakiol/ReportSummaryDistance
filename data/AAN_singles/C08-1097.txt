Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 769?776Manchester, August 2008A Fully-Lexicalized Probabilistic Modelfor Japanese Zero Anaphora ResolutionRyohei Sasano?Graduate School of Information Scienceand Technology, University of Tokyoryohei@nlp.kuee.kyoto-u.ac.jpDaisuke KawaharaNational Institute of Informationand Communication Technologydk@nict.go.jpSadao KurohashiGraduate School of Infomatics,Kyoto Universitykuro@i.kyoto-u.ac.jpAbstractThis paper presents a probabilistic modelfor Japanese zero anaphora resolution.First, this model recognizes discourse en-tities and links all mentions to them.
Zeropronouns are then detected by case struc-ture analysis based on automatically con-structed case frames.
Their appropriateantecedents are selected from the entitieswith high salience scores, based on thecase frames and several preferences onthe relation between a zero pronoun andan antecedent.
Case structure and zeroanaphora relation are simultaneously de-termined based on probabilistic evaluationmetrics.1 IntroductionAnaphora resolution is one of the most importanttechniques in discourse analysis.
In English, def-inite noun phrases such as the company and overtpronouns such as he are anaphors that refer to pre-ceding entities (antecedents).
On the other hand,in Japanese, anaphors are often omitted and theseomissions are called zero pronouns.
We focuson zero anaphora resolution of Japanese web cor-pus, in which anaphors are often omitted and zeroanaphora resolution plays an important role in dis-course analysis.Zero anaphora resolution can be divided intotwo phases.
The first phase is zero pronoun detec-tion and the second phase is zero pronoun resolu-tion.
Zero pronoun resolution is similar to coref-c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.
* Research Fellow of the Japan Society for the Promotion ofScience (JSPS)erence resolution and pronoun resolution, whichhave been studied for many years (e.g.
Soon etal.
(2001); Mitkov (2002); Ng (2005)).
Isozaki andHirao (2003) and Iida et al (2006) focused on zeropronoun resolution assuming perfect pre-detectionof zero pronouns.
However, we consider that zeropronoun detection and resolution have a tight rela-tion and should not be handled independently.
Ourproposed model aims not only to resolve zero pro-nouns but to detect zero pronouns.Zero pronouns are not expressed in a text andhave to be detected prior to identifying their an-tecedents.
Seki et al (2002) proposed a proba-bilistic model for zero pronoun detection and res-olution that uses hand-crafted case frames.
Inorder to alleviate the sparseness of hand-craftedcase frames, Kawahara and Kurohashi (2004) in-troduced wide-coverage case frames to zero pro-noun detection that are automatically constructedfrom a large corpus.
They use the case frames asselectional restriction for zero pronoun resolution,but do not utilize the frequency of each example ofcase slots.
However, since the frequency is shownto be a good clue for syntactic and case structureanalysis (Kawahara and Kurohashi, 2006), we con-sider the frequency also can benefit zero pronoundetection.
Therefore we propose a probabilisticmodel for zero anaphora resolution that fully uti-lizes case frames.
This model directly consid-ers the frequency and estimates case assignmentsfor overt case components and antecedents of zeropronoun simultaneously.In addition, our model directly links each zeropronoun to an entity, while most existing mod-els link it to a certain mention of an entity.
Inour model, mentions and zero pronouns are treatedsimilarly and all of them are linked to correspond-ing entities.
In this point, our model is similar to769Table 1: Examples of Constructed Case Frames.case slot examples generalized examples with ratega (subjective) he, driver, friend, ?
?
?
[CT:PERSON]:0.45, [NE:PERSON]:0.08, ?
?
?tsumu (1)wo (objective) baggage, luggage, hay, ?
?
?
[CT:ARTIFACT]:0.31, ?
?
?
(load)ni (dative) car, truck, vessel, seat, ?
?
?
[CT:VEHICLE]:0.32, ?
?
?tsumu (2)ga (subjective) player, children, party, ?
?
?
[CT:PERSON]:0.40, [NE:PERSON]:0.12, ?
?
?
(accumulate)wo (objective) experience, knowledge, ?
?
?
[CT:ABSTRACT]:0.47, ?
?
?.........ga (subjective) company, Microsoft, firm, ?
?
?
[NE:ORGANIZATION]:0.16, [CT:ORGANIZATION]:0.13, ?
?
?hanbai (1) wo (objective) goods, product, ticket, ?
?
?
[CT:ARTIFACT]:0.40, [CT:FOOD]:0.07, ?
?
?
(sell) ni (dative) customer, company, user, ?
?
?
[CT:PERSON]:0.28, ?
?
?de (locative) shop, bookstore, site ?
?
?
[CT:FACILITY]:0.40, [CT:LOCATION]:0.39, ?
?
?.........the coreference model proposed by Luo (2007) andthat proposed by Yang et al (2008).
Due to thischaracteristic, our model can utilize informationbeyond a mention and easily consider salience (theimportance of an entity).2 Construction of Case FramesCase frames describe what kinds of cases eachpredicate has and what kinds of nouns can fillthese case slots.
We construct case frames froma large raw corpus by using the method proposedby Kawahara and Kurohashi (2002), and use themfor case structure analysis and zero anaphora res-olution.
This section shows how to construct thecase frames.2.1 Basic MethodAfter a large corpus is parsed by a Japanese parser,case frames are constructed from modifier-headexamples in the resulting parses.
The problems ofcase frame construction are syntactic and seman-tic ambiguities.
That is to say, the parsing resultsinevitably contain errors and predicate senses areintrinsically ambiguous.
To cope with these prob-lems, case frames are gradually constructed fromreliable modifier-head examples.First, modifier-head examples that have no syn-tactic ambiguity are extracted, and they are disam-biguated by coupling a predicate and its closestcase component.
Such couples are explicitly ex-pressed on the surface of text, and can be consid-ered to play an important role in sentence mean-ings.
For instance, examples are distinguished notby predicates (e.g., ?tsumu (load/accumulate))?,but by couples (e.g., ?nimotsu-wo tsumu (load bag-gage)?
and ?keiken-wo tsumu (accumulate experi-ence))?.
Modifier-head examples are aggregated inthis way, and yield basic case frames.Thereafter, the basic case frames are clusteredto merge similar case frames.
For example, since?nimotsu-wo tsumu (load baggage)?
and ?busshi-wo tsumu (load supplies)?
are similar, they areclustered.
The similarity is measured using athesaurus (The National Language Institute forJapanese Language, 2004).
Using this gradual pro-cedure, we constructed case frames from approx-imately 1.6 billion sentences extracted from theweb.
In Table 1, some examples of the resultingcase frames are shown.2.2 Generalization of ExamplesBy using case frames that are automatically con-structed from a large corpus, sparseness problemis alleviated to some extent, but still remains.
Forinstance, there are thousands of named entities(NEs), which cannot be covered intrinsically.
Todeal with this sparseness problem, we general-ize the examples of case slots.
Kawahara andKurohashi also give generalized examples suchas ?agent?
but only a few types.
We generalizecase slot examples based on categories of commonnouns and NE classes.First, we use the categories that Japanese mor-phological analyzer JUMAN1adds to commonnouns.
In JUMAN, about twenty categories are de-fined and tagged to common nouns.
For example,?ringo (apple),?
?inu (dog)?
and ?byoin (hospi-tal)?
are tagged as ?FOOD,?
?ANIMAL?
and ?FA-CILITY,?
respectively.
For each category, we cal-culate the rate of categorized example among allcase slot examples, and add it to the case slot as?[CT:FOOD]:0.07.
?We also generalize NEs.
We use a commonstandard NE definition for Japanese provided byIREX workshop (1999).
IREX defined eight NEclasses as shown in Table 2.
We first recognizeNEs in the source corpus by using an NE recog-nizer (Sasano and Kurohashi, 2008), and then con-struct case frames from the NE-recognized corpus.1http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html770Table 2: Definition of NE in IREX.NE class ExamplesORGANIZATION NHK Symphony OrchestraPERSON Kawasaki KenjiroLOCATION Rome, SinuijuARTIFACT Nobel PrizeDATE July 17, April this yearTIME twelve o?clock noonMONEY sixty thousand dollarsPERCENT 20%, thirty percentsAs well as categories, for each NE class, we calcu-late the NE rate among all case slot examples, andadd it to the case slot as ?[NE:PERSON]:0.12.
?The generalized examples are also included inTable 1.
This information is utilized to estimate thecase assignment probability, which will be men-tioned in Section 3.2.3.3 Zero Anaphora Resolution ModelIn this section, we propose a probabilistic modelfor Japanese zero anaphora resolution.3.1 OverviewThe outline of our model is as follows:1.
Parse an input text using the Japanese parserKNP2and recognize NEs.2.
Conduct coreference resolution and link eachmention to an entity or create new entity.3.
For each sentence, from the end of the sen-tence, analyze each predicate by the follow-ing steps:(a) Select a case frame temporarily.
(b) Consider all possible correspondencebetween each input case component andan case slot of the selected case frame.
(c) Regard case slots that have no corre-spondence as zero pronoun candidates.
(d) Consider all possible correspondencebetween each zero pronoun candidateand an existing entity.
(e) For each possible case frame, estimateeach correspondence probabilistically,and select the most likely case frame andcorrespondence.In this paper, we concentrate on three case slotsfor zero anaphora resolution: ?ga (subjective),?
?wo (objective)?
and ?ni (dative),?
which coverabout 90% of zero anaphora.2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.htmlMorphological analysis, NE recognition, syn-tactic analysis and coreference resolution are con-ducted as pre-processes for zero anaphora resolu-tion.
Therefore, the model has already recognizedexisting entities before zero anaphora resolution.For example, let us consider the following text:(i) Toyota-wa 1997-nen hybrid car Prius-wohatsubai(launch).
2000-nen-karaha kaigai(overseas)-demo hanbai(sell)-shiteiru.
(Toyota launched the hybrid car Prius in 1997.
?1started selling ?2overseas in 2000.
)Figure 1 shows the analysis process for this text.There are three mentions3in the first sentence, andthe two mentions, hybrid car and Prius, appear inapposition.
Thus, after the pre-processes, two enti-ties, {Toyota} and {hybrid-car, Prius}, are created.Then, case structure analysis for the predicatehatsubai (launch) is conducted.
First, one of thecase frames of hatsubai (launch) is temporarily se-lected and each input case component is assignedto an appropriate case slot.
For instance, case com-ponent Toyota is assigned to ga case slot and Priusis assigned to wo case slot4.
In this case, thoughthere is a mention hybrid-car that is not a casecomponent of hatsubai (launch) by itself, it refersto the same entity as Prius refers.
Thus, there is noentity that is not linked to hatsubai (launch), andno further analysis is conducted.Now, let us consider the second sentence.
Amention kaigai (overseas) appears and a new entity{kaigai} is created.
Then, case structure analysisfor the predicate hanbai (sell) is conducted.
Thereis only one overt case component kaigai (over-seas), and it is assigned to a case slot of the se-lected case frame of hanbai (sell).
For instance,the case frame hanbai(1) in Table 1 is selected andkaigai (overseas) is assigned to de (locative) caseslot.
In this case, the remaining case slots ga, woand ni are considered as zero pronouns, and allpossible correspondences between zero pronounsand remaining entities are considered.
As a resultof probabilistic estimation, the entity {Toyota} isassigned to ga case, the entity {hybrid-car, Prius}is assigned to wo case and no entity is assigned toni case.Now, we show how to estimate the correspon-dence probabilistically in the next subsection.3In this paper, we do not consider time expressions, suchas 1997, as mentions.4Note that since there are some non case-making postposi-tions in Japanese, such as ?wa?
and ?mo,?
several correspon-dences can be considered.771Toyota-waPrius-wohybrid carhatsubai.kaigai-demohanbai-shiteiru.1997-nen2000-nen-karawa{Toyota, ??
}{hybrid car,Prius, ?2 }{kaigai}Entities(overseas)(launch)(sell)hatsubai (launch)gasubjectivecompany, SONY, firm, ?
[NE:ORGANIZATION] 0.15, ?woobjectiveproduct, CD, model, car,  ?
[CT:ARTIFACT] 0.40, ?delocativearea, shop, world, Japan, ?
[CT:FACILITY] 0.13, ?hanbai (sell)gasubjectivecompany, Microsoft, ?
[NE:ORGANIZATION] 0.16, ?woobjectivegoods, product, ticket, ?
[CT:ARTIFACT] 0.40, ?nidativecustomer, company, user, ?
[CT:PERSON] 0.28, ?delocativeshop, bookstore, site, ?
[CT:FACILITY] 0.40, ?
:direct case assignment:indirect case assignment (zero anaphora)Case framesInput sentencesToyota launched the hybrid car Prius in 1997.
?
?started selling ?2 overseas in 2000.Figure 1: An Example of Case Assignment CAk.3.2 Probabilistic ModelThe proposed model gives a probability to eachpossible case frame CF and case assignment CAwhen target predicate v, input case componentsICC and existing entities ENT are given.
It alsooutputs the case frame and case assignment thathave the highest probability.
That is to say, ourmodel selects the case frame CFbestand the caseassignment CAbestthat maximize the probabilityP (CF,CA|v, ICC,ENT ):(CFbest, CAbest)= argmaxCF,CAP (CF,CA|v, ICC,ENT ) (1)Though case assignment CA usually representscorrespondences between input case componentsand case slots, in our model it also representscorrespondences between antecedents of zero pro-nouns and case slots.
Hereafter, we call the formerdirect case assignment (DCA) and the latter indi-rect case assignment (ICA).
Then, we transformP (CFl, CAk|v, ICC,ENT ) as follows:P (CFl, CAk|v, ICC,ENT )=P (CFl|v, ICC,ENT )?
P (DCAk|v, ICC,ENT,CFl)?
P (ICAk|v, ICC,ENT,CFl, DCAk)?P (CFl|v, ICC) ?
P (DCAk|ICC,CFl)?
P (ICAk|ENT,CFl, DCAk) (2)=P (CFl|v)?P (DCAk, ICC|CFl)/P (ICC|v)?
P (ICAk|ENT,CFl, DCAk) (3)(?
P (CFl|v, ICC) =P (CFl, ICC|v)P (ICC|v)=P (ICC|CFl, v) ?
P (CFl|v)P (ICC|v)=P (ICC|CFl) ?
P (CFl|v)P (ICC|v),(?
CFlcontains the information about v.)P (DCAk|ICC,CFl)=P (DCAk, ICC|CFl)P (ICC|CFl))Equation (2) is derived because we assume thatthe case frame CFland direct case assignmentDCAkare independent of existing entities ENT ,and indirect case assignment ICAkis independentof input case components ICC.Because P (ICC|v) is constant, we can say thatour model selects the case frame CFbestand thedirect case assignment DCAbestand indirect caseassignment ICAbestthat maximize the probabilityP (CF,DCA, ICA|v, ICC,ENT ):(CFbest, DCAbest, ICAbest) =argmaxCF,DCA,ICA(P (CF |v) ?
P (DCA, ICC|CF )?P (ICA|ENT,CF,DCA))(4)The probability P (CFl|v), called generativeprobability of a case frame, is estimated fromcase structure analysis of a large raw corpus.
Thefollowing subsections illustrate how to calculateP (DCAk, ICC|CFl) and P (ICAk|ENT,CFl,DCAk).7723.2.1 Generative Probability of Direct CaseAssignmentFor estimation of generative probability of di-rect case assignment P (DCAk, ICC|CFl), wefollow Kawahara and Kurohashi?s (2006) method.They decompose P (DCAk, ICC|CFl) into thefollowing product depending on whether a caseslot sjis filled with an input case component orvacant:P (DCAk, ICC|CFl) =?sj:A(sj)=1P (A(sj) = 1, nj, cj|CFl, sj)?
?sj:A(sj)=0P (A(sj) = 0|CFl, sj)=?sj:A(sj)=1{P (A(sj) = 1|CFl, sj)?
P (nj, cj|CFl, sj, A(sj) = 1)}?
?sj:A(sj)=0P (A(sj) = 0|CFl, sj) (5)where the function A(sj) returns 1 if a case slot sjis filled with an input case component; otherwise0, njdenotes the content part of the case compo-nent, and cjdenotes the surface case of the casecomponent.The probabilities P (A(sj) = 1|CFl, sj) andP (A(sj) = 0|CFl, sj) are called generative prob-ability of a case slot, and estimated from casestructure analysis of a large raw corpus as well asgenerative probability of a case frame.The probability P (nj, cj|CFl, sj, A(sj) = 1) iscalled generative probability of a case componentand estimated as follows:P (nj, cj|CFl, sj, A(sj) = 1)?P (nj|CFl, sj, A(sj)=1)?P (cj|sj, A(sj)=1) (6)P (nj|CFl, sj, A(sj) = 1) means the gener-ative probability of a content part njfrom acase slot sjin a case frame CFl, and esti-mated by using the frequency of a case slotexample in the automatically constructed caseframes.
P (cj|sj, A(sj) = 1) is approximated byP (cj|case type of(sj), A(sj)=1) and estimatedfrom the web corpus in which the relationship be-tween a surface case marker and a case slot is an-notated by hand.3.2.2 Probability of Indirect Case AssignmentTo estimate probability of indirect case assign-ment P (ICAk|ENT,CFl, DCAk) we also de-compose it into the following product dependingTable 3: Location Classes of Antecedents.intra-sentence: case components ofL1: parent predicate of VzL2: parent predicate of Vz?
(parallel)L3: child predicate of VzL4: child predicate of Vz(parallel)L5: parent predicate of parent noun phrase of VzL6: parent predicate of parent predicate of Vz(parallel)L7: other noun phrases following VzL8: other noun phrases preceding Vzinter-sentence: noun phrases inL9: 1 sentence beforeL10: 2 sentences beforeL11: 3 sentences beforeL12: more than 3 sentences beforeon whether a case slot sjis filled with an entityentjor vacant:P (ICAk|ENT,CFl, DCAk) =?sj:A?
(sj)=1P (A?
(sj) = 1, entj|ENT,CFl, sj)??sj:A?
(sj)=0P (A?
(sj) = 0|ENT,CFl, sj) (7)where the function A?
(sj) returns 1 if a case slotsjis filled with an entity entj; otherwise 0.
Notethat we only consider case slots ga, wo and ni thatis not filled with an input case component.
Weapproximate P (A?
(sj) = 1, entj|ENT,CFl, sj)and P (A?
(sj) = 0|ENT,CFl, sj) as follows:P (A?
(sj) = 1, entj|ENT,CFl, sj)?
P (A?
(sj) = 1, entj|entj, CFl, sj)= P (A?
(sj) = 1|entj, CFl, sj) (8)P (A?
(sj) = 0|ENT,CFl, sj)?
P (A?
(sj) = 0|case type of(sj)) (9)Equation (8) is derived because we assumeP (A?
(sj) = 1|CFl, sj) is independent of exist-ing entities that are not assigned to sj.
Equation(9) is derived because we assume P (A?
(sj) = 0)is independent of ENT and CFl, and only de-pends on the case type of sj, such as ga, wo and ni.P (A?
(sj)=0|case type of(sj)) is the probabilitythat a case slot has no correspondence after zeroanaphora resolution and estimated from anaphoricrelation tagged corpus.Let us consider the probability P (A?
(sj) =1|entj, CFl, sj).
We decompose entjinto contentpart njm, surface case cjnand location class ljn.Here, location classes denote the locational rela-tions between zero pronouns and their antecedents.We defined twelve location classes as described inTable 3.
In Table 3, Vzmeans a predicate that hasa zero pronoun.
Note that we also consider the773locations of zero pronouns that are linked to thetarget entity as location class candidates.
Now weroughly approximate P (A?
(sj)=1|entj, CFl, sj)as follows:P (A?= 1|entj, CFl, sj)=P (A?= 1|njm, cjn, ljn, CFl, sj)=P (njm, cjn, ljn|CFl, sj,A?=1)?P (A?=1|CFl, sj)P (njm, cjn, ljn|CFl, sj)?P (njm|CFl, sj, A?=1)P (njm|CFl, sj)?P (cjn|CFl, sj, A?=1)P (cjn|CFl, sj)?P (ljn|CFl, sj, A?=1)P (ljn|CFl, sj)?P (A?=1|CFl, sj) (10)?P (njm|CFl, sj, A?=1)P (njm)?P (cjn|case type of(sj), A?=1)P (cjn)?
P (A?=1|ljn, case type of(sj)) (11)(?P (ljn|CFl, sj, A?=1)P (ljn|CFl, sj)?P (A?=1|CFl, sj)=P (A?=1, ljn|CFl, sj)P (ljn|CFl, sj)=P (A?=1|CFl, ljn, sj))Note that because entjis often mentioned morethan one time, there are several combinations ofcontent part njm, surface case cjnand locationclass ljncandidates.
We select the pair of m and nwith the highest probability.Equation (10) is derived because we as-sume njm, cjnand ljnare independent of eachother.
Equation (11) is derived because we ap-proximate P (A?= 1|CFl, ljn, sj) as P (A?=1|ljn, case type of(sj)), and assume P (njm) andP (cjn) are independent of CFland sj.
Since theseapproximation is too rough, specifically, P (njm)and P (cjn) tend to be somewhat smaller thanP (njm|CFl, sj) and P (cjn|CFl, sj) and equation(11) often becomes too large, we introduce aparameter ?(?
1) and use the ?-times value asP (A?= 1|entj, CFl, sj).The first term of equation (11) represents howlikely an entity that contains njmas a content partis considered to be an antecedent, the second termrepresents how likely an entity that contains cjnasa surface case is considered to be an antecedent,and the third term gives the probability that anentity that appears in location class ljnis an an-tecedent.The probabilities P (njm) and P (cjn) are esti-mated from a large raw corpus.
The probabili-ties P (cjn|case type of(sj)) and P (A?= 1|ljn,case type of(sj)) are estimated from the webcorpus in which the relationship between an an-tecedent of a zero pronoun and a case slot, and therelationship between its surface case marker and acase slot are annotated by hand.
Then, let us con-sider the probability P (njm|CFl, sj, A?
(sj) = 1)in the next subsection.3.2.3 Probability of Component Part of ZeroPronounP (njm|CFl, sj, A?=1) is similar to P (nj|CFl,sj, A=1) and can be estimated approximately fromcase frames using the frequencies of case slot ex-amples.
However, while A?
(sj) = 1 means sjisnot filled with input case component but filled withan entity as the result of zero anaphora resolution,case frames are constructed by extracting only theinput case component.
Therefore, the content partof a zero anaphora antecedent njmis often not in-cluded in the case slot examples.
To cope with thisproblem, we utilize generalized examples.When one mention of an entity is tagged anycategory or recognized as an NE, we also use thecategory or the NE class as the content part of theentity.
For examples, if an entity {Prius} is recog-nized as an artifact name and assigned to wo caseof the case frame hanbai(1) in Table 1, the systemalso calculates:P (NE :ARTIFACT |hanbai(1),wo, A?
(wo)=1)P (NE :ARTIFACT )besides:P (Prius|hanbai(1),wo, A?
(wo) = 1)P (Prius)and uses the higher value.3.3 Salience ScorePrevious works reported the usefulness of saliencefor anaphora resolution (Lappin and Leass, 1994;Mitkov et al, 2002).
In order to consider salienceof an entity, we introduce salience score, which iscalculated by the following set of simple rules:?
+2 : mentioned with topical marker ?wa?.?
+1 : mentioned without topical marker ?wa?.?
+0.5 : assigned to a zero pronoun.?
?0.7 : beginning of each sentence.For examples, we consider the salience score ofthe entity {Toyota} in (i) in Section 3.1.
In thefirst sentence, since {Toyota} is mentioned withtopical marker ?wa?, the salience score is 2.
At thebeginning of the second sentence it becomes 1.4,774Table 4: Data for Parameter Estimation.probability dataP (nj) raw corpusP (cj) raw corpusP (cj|case type of(sj), A(sj)=1) tagged corpusP (cj|case type of(sj), A?
(sj)=1) tagged corpusP (nj|CFl, sj, A(sj)=1) case framesP (nj|CFl, sj, A?
(sj)=1) case framesP (CFl|vi) case structure analysisP (A(sj)={0, 1} |CFl, sj) case structure analysisP (A?
(sj)=0|case type of(sj)) tagged corpusP (A?
(sj)=1|lj, case type of(sj)) tagged corpusTable 5: Experimental Results.R P FKawahara & Kurohashi .230 (28/122) .173 (28/162) .197Proposed (?
= 1) .426 (52/122) .271 (52/192) .331(?
= 1/2) .410 (50/122) .373 (50/134) .391(?
= 1/4) .295 (36/122) .419 (36/86) .346and after assigned to the zero pronoun of ?hanbai?it becomes 1.9.
Note that we use the salience scorenot as a probabilistic clue but as a filter to considerthe target entity as a possible antecedent.
When weuse the salience score, we only consider the entitiesthat have the salience score no less than 1.4 Experiments4.1 SettingWe created an anaphoric relation-tagged corpusconsisting of 186 web documents (979 sentences).We selected 20 documents for test and used theother 166 documents for calculating several proba-bilities.
Since the anaphoric relations in some webdocuments were not so clear and too difficult torecognize, we did not select such documents fortest.
In the 20 test documents, 122 zero anaphorarelations were tagged between one of the mentionsof the antecedent and the target predicate that hadthe zero pronoun.Each parameter for proposed model was esti-mated using maximum likelihood from the datadescribed in Table 4.
The case frames were auto-matically constructed from web corpus comprising1.6 billion sentences.
The case structure analysiswas conducted on 80 million sentences in the webcorpus, and P (nj) and P (cj)were calculated fromthe same 80 million sentences.In order to concentrate on zero anaphora resolu-tion, we used the correct morphemes, named enti-ties, syntactic structures and coreferential relationsthat were annotated by hand.
Since correct corefer-ential relations were given, the number of createdentities was same between the gold standard andthe system output because zero anaphora resolu-tion did not create new entities.4.2 Experimental ResultsWe conducted experiments of zero anaphora reso-lution.
As the parameter ?
introduced in Section3.2.2., we tested 3 values 1, 1/2, and 1/4.
Forcomparison, we also tested Kawahara and Kuro-hashi?s (2004) model.
The experimental results areshown in Table 5, in which recall R, precision Pand F-measure F were calculated by:R =# of correctly recognized zero anaphora# of zero anaphora tagged in corpus,P =# of correctly recognized zero anaphora# of system outputted zero anaphora,F =21/R + 1/P.Kawahara and Kurohashi?s model achieved al-most 50% as F-measure against newspaper arti-cles.
However, as a result of our experimentagainst web documents, it achieved only about20% as F-measure.
This may be because anaphoricrelations in web documents were not so clear asthose in newspaper articles and more difficult torecognize.
As to the parameter ?, the larger ?tended to output more zero anaphora, and the high-est F-measure was achieved against ?
= 1/2.When using ?
= 1/2, there were 72 (=122?50)zero pronouns that were tagged in the corpus andnot resolved correctly.
Only 12 of them were cor-rectly detected and assigned to a wrong entity, thatis, 60 of them were not even detected.
Therefore,we can say our recall errors were mainly caused bythe low recall of zero pronoun detection.In order to confirm the effectiveness of gener-alized examples of case slots and salience score,we also conducted experiments under several con-ditions.
We set ?
= 1/2 in these experiments.
Theresults are shown in Table 6, in which CT meansgeneralized categories, NE means generalized NEsand SS means salience score.Without using any generalized examples, the F-measure is less than Kawahara and Kurohashi?smethod, which use similarity to deal with sparse-ness of case slot examples, and we can con-firm the effectiveness of the generalized examples.While generalized categories much improved theF-measure, generalized NEs contribute little.
Thismay be because the NE rate is smaller than com-mon noun rate, and so the effect is limited.We also confirmed that the salience score filterimproved F-measure.
Moreover, by using saliencescore filter, the zero anaphora resolution becomesabout ten times faster.
This is because the system775Table 6: Experiments under Several Conditions.CT NE SS R P F?.131 (16/122) .205 (16/78) .160?
?.164 (20/122) .247 (20/81) .197?
?.402 (49/122) .368 (49/133) .384?
?.385 (47/122) .196 (47/240) .260?
?
?.410 (50/122) .373 (50/134) .391can avoid checking entities with low salience asantecedent candidates.4.3 Comparison with Previous WorksWe compare our accuracies with (Seki et al,2002).
They achieved 48.9% in precision, 88.2%in recall, and 62.9% in F-measure for zero pro-noun detection, and 54.0% accuracy for antecedentestimation on 30 newspaper articles, that is, theyachieved about 34% in F-measure for whole zeropronoun resolution.
It is difficult to directly com-pare their results with ours due to the differenceof the corpus, but our method achieved 39% inF-measure and we can confirm that our modelachieves reasonable performance considering thetask difficulty.5 ConclusionIn this paper, we proposed a probabilistic modelfor Japanese zero anaphora resolution.
By us-ing automatically constructed wide-coverage caseframes that include generalized examples and in-troducing salience score filter, our model achievesreasonable performance against web corpus.
Asfuture work, we plan to conduct large-scale ex-periments and integrate this model to a fully-lexicalized probabilistic model for Japanese syn-tactic and case structure analysis (Kawahara andKurohashi, 2006).ReferencesIida, Ryu, Kentaro Inui, and Yuji Matsumoto.
2006.Exploiting syntactic patterns as clues in zero-anaphora resolution.
In Proceedings of COL-ING/ACL 2006, pages 625?632.IREX Committee, editor.
1999.
Proc.
of the IREXWorkshop.Isozaki, Hideki and Tsutomu Hirao.
2003.
Japanesezero pronoun resolution based on ranking rules andmachine learning.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP), pages 184?191.Kawahara, Daisuke and Sadao Kurohashi.
2002.Fertilization of Case Frame Dictionary for RobustJapanese Case Analysis.
In Proceedings of the 19thInternational Conference on Computational Linguis-tics, pages 425?431.Kawahara, Daisuke and Sadao Kurohashi.
2004.Zero pronoun resolution based on automatically con-structed case frames and structural preference of an-tecedents.
In Proceedings of the 1st InternationalJoint Conference on Natural Language Processing(IJCNLP-04), pages 334?341.Kawahara, Daisuke and Sadao Kurohashi.
2006.
Afully-lexicalized probabilistic model for japanesesyntactic and case structure analysis.
In Proceedingsof the Human Language Technology Conference ofthe NAACL, Main Conference, pages 176?183.Lappin, Shalom and Herbert J. Leass.
1994.
An algo-rithm for pronominal anaphora resolution.
Compu-tational Linguistics, 20(4):535?562.Luo, Xiaoqiang.
2007.
Coreference or not: Atwin model for coreference resolution.
In HumanLanguage Technologies 2007: The Conference ofthe North American Chapter of the Association forComputational Linguistics; Proceedings of the MainConference, pages 73?80.Mitkov, Ruslan, Richard Evans, and Constantin Or?asan.2002.
A new, fully automatic version of mitkov?sknowledge-poor pronoun resolution method.
In Pro-ceedings of the Third International Conference onIntelligent Text Processing and Computational Lin-guistics (CICLing-2002).Ng, Vincent.
2005.
Machine learning for coreferenceresolution: From local classification to global rank-ing.
In Proceedings of the 43rd Annual Meetingof the Asssociation for Computational Linguistics,pages 157?164.Sasano, Ryohei and Sadao Kurohashi.
2008.
Japanesenamed entity recognition using structural natural lan-guage processing.
In Proceedings of the 3rd Interna-tional Joint Conference on Natural Language Pro-cessing (IJCNLP-08), pages 607?612.Seki, Kazuhiro, Atsushi Fujii, and Tetsuya Ishikawa.2002.
A probabilistic method for analyzing Japaneseanaphora integrating zero pronoun detection and res-olution.
In Proceedings of the 19th InternationalConference on Computational Linguistics, pages911?917.Soon, Wee Meng, Hwee Tou Ng, and DanielChung Yong Lim.
2001.
A machine learning ap-proach to coreference resolution of noun phrases.Computational Linguistics, 27(4):521?544.The National Language Institute for Japanese Lan-guage.
2004.
Bunruigoihyo.
Dainippon Tosho, (InJapanese).Yang, Xiaofeng, Jian Su, Jun Lang, Ghew Lim Tan,Ting Liu, and Sheng Li.
2008.
An entity-mentionmodel for coreference resolution with inductive logicprogramming.
In Proceedings of ACL-08: HLT,pages 843?851.776
