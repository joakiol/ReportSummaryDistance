Adaptation of Maximum Entropy Capitalizer: Little Data Can Help a LotCiprian Chelba and Alex AceroMicrosoft ResearchOne Microsoft WayRedmond, WA 98052{chelba,alexac}@microsoft.comAbstractA novel technique for maximum ?a posteriori?
(MAP) adaptation of maximum entropy (MaxEnt)and maximum entropy Markov models (MEMM) ispresented.The technique is applied to the problem of recov-ering the correct capitalization of uniformly casedtext: a ?background?
capitalizer trained on 20Mwdsof Wall Street Journal (WSJ) text from 1987 isadapted to two Broadcast News (BN) test sets ?one containing ABC Primetime Live text and theother NPR Morning News/CNN Morning Editiontext ?
from 1996.The ?in-domain?
performance of the WSJ capi-talizer is 45% better than that of the 1-gram base-line, when evaluated on a test set drawn from WSJ1994.
When evaluating on the mismatched ?out-of-domain?
test data, the 1-gram baseline is outper-formed by 60%; the improvement brought by theadaptation technique using a very small amount ofmatched BN data ?
25-70kwds ?
is about 20-25%relative.
Overall, automatic capitalization error rateof 1.4% is achieved on BN data.1 IntroductionAutomatic capitalization is a practically relevantproblem: speech recognition output needs to becapitalized; also, modern word processors performcapitalization among other text proofing algorithmssuch as spelling correction and grammar checking.Capitalization can be also used as a preprocessingstep in named entity extraction or machine trans-lation.
We study the impact of using increasingamounts of training data as well as using a smallamount of adaptation data on this simple problemthat is well suited to data-driven approaches sincevast amounts of ?training?
data are easily obtainableby simply wiping the case information in text.As in previous approaches, the problem is framedas an instance of the class of sequence labelingproblems.
A case frequently encountered in prac-tice is that of using mismatched ?
out-of-domain,in this particular case we used Broadcast News ?test data.
For example, one may wish to use a capi-talization engine developed on newswire text for e-mail or office documents.
This typically affects neg-atively the performance of a given model, and moresophisticated models tend to be more brittle.
In thecapitalization case we have studied, the relative per-formance improvement of the MEMM capitalizerover the 1-gram baseline drops from in-domain ?WSJ ?
performance of 45% to 35-40% when usedon the slightly mismatched BN data.In order to take advantage of the adaptation datain our scenario, a maximum a-posteriori (MAP)adaptation technique for maximum entropy (Max-Ent) models is developed.
The adaptation procedureproves to be quite effective in further reducing thecapitalization error of the WSJ MEMM capitalizeron BN test data.
It is also quite general and couldimprove performance of MaxEnt models in any sce-nario where model adaptation is desirable.
A furtherrelative improvement of about 20% is obtained byadapting the WSJ model to Broadcast News (BN)text.
Overall, the MEMM capitalizer adapted to BNdata achieves 60% relative improvement in accuracyover the 1-gram baseline.The paper is organized as follows: the next sec-tion frames automatic capitalization as a sequencelabeling problem, presents previous approaches aswell as the widespread and highly sub-optimal 1-gram capitalization technique that is used as a base-line in most experiments in this work and others.The MEMM sequence labeling technique is brieflyreviewed in Section 3.
Section 4 describes theMAP adaptation technique used for the capitaliza-tion of out-of-domain text.
The detailed mathemat-ical derivation is presented in Appendix A.
The ex-perimental results are presented in Section 5, fol-lowed by conclusions and suggestions for futurework.2 Capitalization as Sequence TaggingAutomatic capitalization can be seen as a sequencetagging problem: each lower-case word receives atag that describes its capitalization form.
Similar tothe work in (Lita et al, 2003), we tag each word ina sentence with one of the tags:?
LOC lowercase?
CAP capitalized?
MXC mixed case; no further guess is made as tothe capitalization of such words.
A possibilityis to use the most frequent one encountered inthe training data.?
AUC all upper case?
PNC punctuation; we decided to have a sep-arate tag for punctuation since it is quite fre-quent and models well the syntactic context ina parsimonious wayFor training a given capitalizer one needs to convertrunning text into uniform case text accompanied bythe above capitalization tags.
For example,PrimeTime continues on ABC .PERIODNow ,COMMA from Los Angeles ,COMMADiane Sawyer .PERIODbecomesprimetime_MXC continues_LOC on_LOCabc_AUC .period_PNCnow_CAP ,comma_PNC from_LOC los_CAPangeles_CAP ,comma_PNC diane_CAPsawyer_CAP .period_PNCThe text is assumed to be already segmented intosentences.
Any sequence labeling algorithm canthen be trained for tagging lowercase word se-quences with capitalization tags.At test time, the uniform case text to be capital-ized is first segmented into sentences1 after whicheach sentence is tagged.2.1 1-gram capitalizerA widespread algorithm used for capitalization isthe 1-gram tagger: for every word in a given vo-cabulary (usually large, 100kwds or more) use themost frequent tag encountered in a large amount oftraining data.
As a special case for automatic capi-talization, the most frequent tag for the first word ina sentence is overridden by CAP, thus capitalizingon the fact that the first word in a sentence is mostlikely capitalized2.1Unlike the training phase, the sentence segmenter at testtime is assumed to operate on uniform case text.2As with everything in natural language, it is not hard tofind exceptions to this ?rule?.Due to its popularity, both our work and thatof (Lita et al, 2003) uses the 1-gram capitalizer asa baseline.
The work in (Kim and Woodland, 2004)indicates that the same 1-gram algorithm is used inMicrosoft Word 2000 and is consequently used asa baseline for evaluating the performance of theiralgorithm as well.2.2 Previous WorkWe share the approach to capitalization as sequencetagging with that of (Lita et al, 2003).
In their ap-proach, a language model is built on pairs (word,tag) and then used to disambiguate over all possibletag assignments to a sentence using dynamic pro-gramming techniques.The same idea is explored in (Kim and Woodland,2004) in the larger context of automatic punctuationgeneration and capitalization from speech recogni-tion output.
A second approach they consider forcapitalization is the use a rule-based tagger as de-scribed by (Brill, 1994), which they show to outper-form the case sensitive language modeling approachand be quite robust to speech recognition errors andpunctuation generation errors.Departing from their work, our approach buildson a standard technique for sequence tagging,namely MEMMs, which has been successfully ap-plied to part-of-speech tagging (Ratnaparkhi, 1996).The MEMM approach models the tag sequence Tconditionally on the word sequence W , which has afew substantial advantages over the 1-gram taggingapproach:?
discriminative training of probability modelP (T |W ) using conditional maximum likeli-hood is well correlated with tagging accuracy?
ability to use a rich set of word-level fea-tures in a parsimonious way: sub-word fea-tures such as prefixes and suffixes, as well asfuture words3 are easily incorporated in theprobability model?
no concept of ?out-of-vocabulary?
word: sub-word features are very useful in dealing withwords not seen in the training data?
ability to integrate rich contextual features intothe modelMore recently, certain drawbacks of MEMM mod-els have been addressed by the conditional randomfield (CRF) approach (Lafferty et al, 2001) whichslightly outperforms MEMMs on a standard part-of-speech tagging task.
In a similar vein, the work3Relative to the current word, whose tag is assigned a prob-ability value by the MEMM.of (Collins, 2002) explores the use of discrimina-tively trained HMMs for sequence labeling prob-lems, a fair baseline for such cases that is often over-looked in favor of the inadequate maximum likeli-hood HMMs.The work on adapting the MEMM model param-eters using MAP smoothing builds on the Gaussianprior model used for smoothing MaxEnt models, aspresented in (Chen and Rosenfeld, 2000).
We arenot aware of any previous work on MAP adapta-tion of MaxEnt models using a prior, be it Gaus-sian or a different one, such as the exponential priorof (Goodman, 2004).
Although we do not have aformal derivation, the adaptation technique shouldeasily extend to the CRF scenario.A final remark contrasts rule-based approachesto sequence tagging such as (Brill, 1994) withthe probabilistic approach taken in (Ratnaparkhi,1996): having a weight on each feature in the Max-Ent model and a sound probabilistic model allowsfor a principled way of adapting the model to a newdomain; performing such adaptation in a rule-basedmodel is unclear, if at all possible.3 MEMM for Sequence LabelingA simple approach to sequence labeling is the max-imum entropy Markov model.
The model assigns aprobability P (T |W ) to any possible tag sequenceT = t1.
.
.
tn = T n1for a given word sequenceW = w1.
.
.
wn.
The probability assignment isdone according to:P (T |W ) =n?i=1P (ti|xi(W,Ti?11))where ti is the tag corresponding to word i andxi(W,Ti?11) is the conditioning information at posi-tion i in the word sequence on which the probabilitymodel is built.The approach we took is the one in (Rat-naparkhi, 1996), which uses xi(W,T i?11) ={wi, wi?1, wi+1, ti?1, ti?2}.
We note that the prob-ability model is causal in the sequencing of tags (theprobability assignment for ti only depends on previ-ous tags ti?1, ti?2) which allows for efficient algo-rithms that search for the most likely tag sequenceT ?
(W ) = arg maxT P (T |W ) as well as ensures aproperly normalized conditional probability modelP (T |W ).The probability P (ti|xi(W,T i?11)) is modeledusing a maximum entropy model.
The next sectionbriefly describes the training procedure; for detailsthe reader is referred to (Berger et al, 1996).3.1 Maximum Entropy State Transition ModelThe sufficient statistics that are ex-tracted from the training data are tuples(y,#, x) = (ti,#, xi(W,Ti?11)) where ti isthe tag assigned in context xi(W,T i?11) ={wi, wi?1, wi+1, ti?1, ti?2} and # denotes thecount with which this event has been observed inthe training data.
By way of example, the eventassociated with the first word in the example inSection 2 is (*bdw* denotes a special boundarytype):MXC 1currentword=primetimepreviousword=*bdw*nextword=continuest1=*bdw* t1,2=*bdw*,*bdw*prefix1=p prefix2=pr prefix3=prisuffix1=e suffix2=me suffix3=imeThe maximum entropy probability model P (y|x)uses features which are indicator functions of thetype:f(y, x) = {1,0,if y = MXC and x.wi= primetimeo/wAssuming a set of features F whose cardinality isF , the probability assignment is made according to:p?
(y|x) = Z?1(x,?)
?
exp[F?i=1?ifi(x, y)]Z(x,?)
=?yexp[F?i=1?ifi(x, y)]where ?
= {?1.
.
.
?F } ?
RF is the set of real-valued model parameters.3.1.1 Feature SelectionWe used a simple count cut-off feature selection al-gorithm which counts the number of occurrences ofall features in a predefined set after which it discardsthe features whose count is less than a pre-specifiedthreshold.
The parameter of the feature selection al-gorithm is the threshold value; a value of 0 will keepall features encountered in the training data.3.1.2 Parameter EstimationThe model parameters ?
are estimated such thatthe model assigns maximum log-likelihood to thetraining data subject to a Gaussian prior centeredat 0, ?
?
N (0, diag(?2i )), that ensures smooth-ing (Chen and Rosenfeld, 2000):L(?)
=?x,yp?
(x, y) log p?
(y|x) (1)?F?i=1?2i2?2i+ const(?
)As shown in (Chen and Rosenfeld, 2000) ?
and re-derived in Appendix A for the non-zero mean case?
the update equations are:?
(t+1)i = ?
(t)i + ?i, where ?i satisfies:?x,yp?
(x, y)fi(x, y) ?
?i?2i=?i?2i+ (2)?x,yp?(x)p?
(y|x)fi(x, y)exp(?if#(x, y))In our experiments the variances are tied to ?i = ?whose value is determined by line search on devel-opment data such that it yields the best tagging ac-curacy.4 MAP Adaptation of Maximum EntropyModelsIn the adaptation scenario we already have a Max-Ent model trained on the background data and wewish to make best use of the adaptation data by bal-ancing the two.
A simple way to accomplish this isto use MAP adaptation using a prior distribution onthe model parameters.A Gaussian prior for the model parameters ?has been previously used in (Chen and Rosen-feld, 2000) for smoothing MaxEnt models.
Theprior has 0 mean and diagonal covariance: ?
?N (0, diag(?2i )).
In the adaptation scenario, theprior distribution used is centered at the parametervalues ?0 estimated from the background data in-stead of 0: ?
?
N (?0, diag(?2i )).The regularized log-likelihood of the adaptationtraining data becomes:L(?)
=?x,yp?
(x, y) log p?
(y|x) (3)?F?i=1(?i ?
?0i )22?2i+ const(?
)The adaptation is performed in stages:?
apply feature selection algorithm on adaptationdata and determine set of features Fadapt.?
build new model by taking the union of thebackground and the adaptation feature sets:F = Fbackground ?
Fadapt; each of thebackground features receives the correspond-ing weight ?i determined on the backgroundtraining data; the new featuresFadapt \ Fbackground4 introduced in the modelreceive 0 weight.
The resulting model is thusequivalent with the background model.?
train the model such that the regularized log-likelihood of the adaptation training data ismaximized.
The prior mean is set at ?0 =?background ?
0; ?
denotes concatenation be-tween the parameter vector for the backgroundmodel and a 0-valued vector of length |Fadapt\Fbackground| corresponding to the weights forthe new features.As shown in Appendix A, the update equations arevery similar to the 0-mean case:?x,yp?
(x, y)fi(x, y) ?
(?i ?
?0i )?2i=?i?2i+ (4)?x,yp?(x)p?
(y|x)fi(x, y)exp(?if#(x, y))The effect of the prior is to keep the model param-eters ?i close to the background ones.
The cost ofmoving away from the mean for each feature fi isspecified by the magnitude of the variance ?i: asmall variance ?i will keep the weight ?i close toits mean; a large variance ?i will make the regu-larized log-likelihood (see Eq.
3) insensitive to theprior on ?i, allowing the use of the best value ?i formodeling the adaptation data.Another observation is that not only the featuresobserved in the adaptation data get updated: evenif Ep?
(x,y)[fi] = 0, the weight ?i for feature fi willstill get updated if the feature fi triggers for a con-text x encountered in the adaptation data and somepredicted value y ?
not necessarily present in theadaptation data in context x.In our experiments the variances were tied to?i = ?
whose value was determined by line searchon development data drawn from the adaptationdata.
The common variance ?
will thus balanceoptimally the log-likelihood of the adaptation datawith the ?0 mean values obtained from the back-ground data.Other tying schemes are possible: separate val-ues could be used for the Fadapt \ Fbackground andFbackground feature sets, respectively.
We did notexperiment with various tying schemes althoughthis is a promising research direction.4.1 Relationship with Minimum DivergenceTrainingAnother possibility to adapt the background modelis to do minimum KL divergence (MinDiv) train-4We use A \B to denote set difference.ing (Pietra et al, 1995) between the backgroundexponential model B ?
assumed fixed ?
and anexponential model A built using the Fbackground ?Fadapt feature set.
It can be shown that, if wesmooth the A model with a Gaussian prior on thefeature weights that is centered at 0 ?
followingthe approach in (Chen and Rosenfeld, 2000) forsmoothing maximum entropy models ?
then theMinDiv update equations for estimating A on theadaptation data are identical to the MAP adaptationprocedure we proposed5.However, we wish to point out that the equiva-lence holds only if the feature set for the new modelA is Fbackground ?
Fadapt.
The straightforward ap-plication of MinDiv training ?
by using only theFadapt feature set for A ?
will not result in anequivalent procedure to ours.
In fact, the differ-ence in performance between this latter approachand ours could be quite large since the cardinalityof Fbackground is typically several orders of mag-nitude larger than that of Fadapt and our approachalso updates the weights corresponding to featuresin Fbackground \ Fadapt.
Further experiments areneeded to compare the performance of the two ap-proaches.5 ExperimentsThe baseline 1-gram and the background MEMMcapitalizer were trained on various amounts ofWSJ (Paul and Baker, 1992) data from 1987 ?
filesWS87_{001-126}.
The in-domain test data usedwas file WS94_000 (8.7kwds).As for the adaptation experiments, two differentsets of BN data were used, whose sizes are summa-rized in Table 1:1.
BN CNN/NPR data.
The train-ing/development/test partition consisted of a3-way random split of file BN624BTS.
Theresulting sets are denoted CNN-trn/dev/tst,respectively2.
BN ABC Primetime data.
The training set con-sisted of file BN623ATS whereas the develop-ment/test set consisted of a 2-way random splitof file BN624ATS5.1 In-Domain ExperimentsWe have proceeded building both 1-gram andMEMM capitalizers using various amounts of back-ground training data.
The model sizes for the 1-gram and MEMM capitalizer are presented in Ta-ble 2.
Count cut-off feature selection has been used5Thanks to one of the anonymous reviewers for pointing outthis possible connection.Data set Partitiontrain devel testWSJ 2-20M ?
8.7kCNN 73k 73k 73kABC 25k 8k 8kTable 1: Background and adaptation training, devel-opment, and test data partition sizesfor the MEMM capitalizer with the threshold set at5, so the MEMM model size is a function of thetraining data.
The 1-gram capitalizer used a vocab-ulary of the most likely 100k wds derived from thetraining data.Model No.
Param.
(103)Training Data Size (106) 2.0 3.5 20.01-gram 100 100 100MEMM 76 102 238Table 2: Background models size (number of pa-rameters) for various amounts of training dataWe first evaluated the in-domain and out-of-domain relative performance of the 1-gram and theMEMM capitalizers as a function of the amount oftraining data.
The results are presented in Table 3.The MEMM capitalizer performs about 45% betterModel Test Data Cap ERR (%)Training Data Size (106) 2.0 3.5 20.01-gram WSJ-tst 5.4 5.2 4.4MEMM WSJ-tst 2.9 2.5 2.31-gram ABC-dev 3.1 2.9 2.6MEMM ABC-dev 2.2 2.0 1.61-gram CNN-dev 4.4 4.2 3.5MEMM CNN-dev 2.7 2.5 2.1Table 3: Background models performance on in-domain (WSJ-test) and out-of-domain (BN-dev)data for various amounts of training datathan the 1-gram one when trained and evaluated onWall Street Journal text.
The relative performanceimprovement of the MEMM capitalizer over the 1-gram baseline drops to 35-40% when using out-of-domain Broadcast News data.
Both models benefitfrom using more training data.5.2 Adaptation ExperimentsWe have then adapted the best MEMM model builton 20Mwds on the two BN data sets (CNN/ABC)and compared performance against the 1-gram andthe unadapted MEMM models.There are a number of parameters to be tunedon development data.
Table 4 presents the varia-tion in model size with different count cut-off valuesfor the feature selection procedure on the adaptationdata.
As can be seen, very few features are added tothe background model.
Table 5 presents the varia-tion in log-likelihood and capitalization accuracy onthe CNN adaptation training and development data,respectively.
The adaptation procedure was foundCut-off 0 5 106No.
features 243,262 237,745 237,586Table 4: Adapted model size as a function of countcut-off threshold used for feature selection on CNN-trn adaptation data; the entry corresponding to thecut-off threshold of 106 represents the number offeatures in the background modelto be insensitive to the number of reestimation it-erations, and, more surprisingly, to the number offeatures added to the background model from theadaptation data, as shown in 5.
The most sensitiveparameter is the prior variance ?2, as shown in Fig-ure 1; its value is chosen to maximize classificationaccuracy on development data.
As expected, lowvalues of ?2 result in no adaptation at all, whereashigh values of ?2 fit the training data very well, andresult in a dramatic increase of training data log-likelihood and accuracies approaching 100%.Cut- LogL Cap ACC (%)off ?2 (nats) CNN-trn CNN-dev0 0.01 -4258.58 98.00 97.980 3.0 -1194.45 99.63 98.625 0.01 -4269.72 98.00 97.985 3.0 -1369.26 99.55 98.60106 0.01 -4424.58 98.00 97.96106 3.0 -1467.46 99.52 98.57Table 5: Adapted model performance for variouscount cut-off and ?2 variance values; log-likelihoodand accuracy on adaptation data CNN-trn as wellas accuracy on held-out data CNN-dev; the back-ground model results (no new features added) arethe entries corresponding to the cut-off threshold of106Finally, Table 6 presents the results on test datafor 1-gram, background and adapted MEMM.
Ascan be seen, the background MEMM outperformsthe 1-gram model on both BN test sets by about35-40% relative.
Adaptation improves performanceeven further by another 20-25% relative.
Overall,the adapted models achieve 60% relative reductionin capitalization error over the 1-gram baseline onboth BN test sets.
An intuitively satisfying resultis the fact that the cross-test set performance (CNN0 1 2 3 4 5 6?4500?4000?3500?3000?2500?2000?1500?1000?2 varianceLogL(train)Adaptation: Training LogL (nats) with ?20 1 2 3 4 5 697.59898.59999.5100?2 varianceAccuracyAdaptation: Training and Development Capitalization Accuracy with ?2Figure 1: Variation of training data log-likelihood,and training/development data (- -/?
line) capitaliza-tion accuracy as a function of the prior variance ?2Cap ERR (%)Model Adapt Data ABC-tst CNN-tst1-gram ?
2.7 3.7MEMM ?
1.8 2.2MEMM ABC-trn 1.4 1.7MEMM CNN-trn 2.4 1.4Table 6: Background and adapted models perfor-mance on BN test data; two adaptation/test sets areused: ABC and CNNadapted model evaluated on ABC data and the otherway around) is worse than the adapted one.6 Conclusions and Future WorkThe MEMM tagger is very effective in reducingboth in-domain and out-of-domain capitalization er-ror by 35%-45% relative over a 1-gram capitaliza-tion model.We have also presented a general technique foradapting MaxEnt probability models.
It was shownto be very effective in adapting a backgroundMEMM capitalization model, improving the accu-racy by 20-25% relative.
An overall 50-60% reduc-tion in capitalization error over the standard 1-grambaseline is achieved.
A surprising result is that theadaptation performance gain is not due to addingmore, domain-specific features but rather makingbetter use of the background features for modelingthe in-domain data.As expected, adding more background trainingdata improves performance but a very small amountof domain specific data also helps significantly ifone can make use of it in an effective way.
The?There?s no data like more data?
rule-of-thumbcould be amended by ?..., especially if it?s the rightdata!
?.As future work we plan to investigate the bestway to blend increasing amounts of less-specificbackground training data with specific, in-domaindata for this and other problems.Another interesting research direction is to ex-plore the usefulness of the MAP adaptation of Max-Ent models for other problems among which wewish to include language modeling, part-of-speechtagging, parsing, machine translation, informationextraction, text routing.AcknowledgmentsSpecial thanks to Adwait Ratnaparkhi for makingavailable the code for his MEMM tagger and Max-Ent trainer.ReferencesA.
L. Berger, S. A. Della Pietra, and V. J. DellaPietra.
1996.
A Maximum Entropy Approachto Natural Language Processing.
ComputationalLinguistics, 22(1):39?72, March.Eric Brill.
1994.
Some Advances inTransformation-Based Part of Speech Tag-ging.
In National Conference on ArtificialIntelligence, pages 722?727.Stanley F. Chen and Ronald Rosenfeld.
2000.
ASurvey of Smoothing Techniques for MaximumEntropy Models.
IEEE Transactions on Speechand Audio Processing, 8(1):37?50.Michael Collins.
2002.
Discriminative TrainingMethods for Hidden Markov Models: Theoryand Experiments with Perceptron Algorithms.In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, pages1?8, University of Pennsylvania, Philadelphia,PA, July.
ACL.Joshua Goodman.
2004.
Exponential Priors forMaximum Entropy Models.
In Daniel Marcu Su-san Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 305?312, Boston, Massachusetts, USA, May 2 - May7.
Association for Computational Linguistics.Ji-Hwan Kim and Philip C. Woodland.
2004.
Au-tomatic Capitalization Generation for Speech In-put.
Computer Speech and Language, 18(1):67?90, January.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional Random Fields:Probabilistic Models for Segmenting and La-beling Sequence Data.
In Proc.
18th Interna-tional Conf.
on Machine Learning, pages 282?289.
Morgan Kaufmann, San Francisco, CA.L.
Lita, A. Ittycheriah, S. Roukos, and N. Kamb-hatla.
2003. tRuEcasIng.
In Proccedings ofACL, pages 152?159, Sapporo, Japan.Doug B. Paul and Janet M. Baker.
1992.
The designfor the Wall Street Journal-based CSR corpus.
InProceedings of the DARPA SLS Workshop.
Febru-ary.S.
Della Pietra, V. Della Pietra, and J. Lafferty.1995.
Inducing features of random fields.
Tech-nical Report CMU-CS-95-144, School of Com-puter Science, Carnegie Mellon University, Pitts-burg, PA.Adwait Ratnaparkhi.
1996.
A Maximum EntropyModel for Part-of-Speech Tagging.
In Eric Brilland Kenneth Church, editors, Proceedings ofthe Conference on Empirical Methods in Natu-ral Language Processing, pages 133?142.
Asso-ciation for Computational Linguistics, Somerset,New Jersey.AppendixA Modified IIS for MaxEnt MAPAdaptation Using a Gaussian PriorThe regularized log-likelihood of the training data?
to be maximized by the MAP adaptation trainingalgorithm ?
is:L(?)
=?x,yp?
(x, y) log p?
(y|x)?F?i=1(?i ?
?0i )22?2i+ const(?)=?x,yp?
(x, y)F?i=1?ifi(x, y) ??x,yp?
(x, y) log?y?exp[F?i=1?ifi(x, y?
)]?F?i=1(?i ?
?0i )22?2i+ const(?)=?x,yp?
(x, y)F?i=1?ifi(x, y) ??xp?
(x) log?y?exp[F?i=1?ifi(x, y?
)]?F?i=1(?i ?
?0i )22?2i+ const(?
)where the last equality holds because the argumentof the log is independent of y.The derivation of the updates follows very closelythe one in (Chen and Rosenfeld, 2000) for smooth-ing a MaxEnt model by using a Gaussian priorwith 0 mean and diagonal covariance matrix.
Ateach iteration we seek to find a set of updates for?, ?
= {?i}, that increase the regularized log-likelihood L(?)
by the largest amount possible.After a few basic algebraic manipulations, thedifference in log-likelihood caused by a ?
changein the ?
values becomes:L(?
+ ?)
?
L(?)=?x,yp?
(x, y)F?i=1?ifi(x, y) ??xp?
(x) log?yp?
(y|x) exp[F?i=1?ifi(x, y)]?F?i=12(?i ?
?0i )?i + ?i22?2iFollowing the same lower bounding technique asin (Chen and Rosenfeld, 2000) by using log x ?x?1 and Jensen?s inequality for the U -convexity ofthe exponential we obtain:L(?
+ ?)
?
L(?)??x,yp?
(x, y)F?i=1?ifi(x, y) + 1 ??x,yp?(x)p?
(y|x)F?i=1fi(x, y)f#(x, y)exp(?if#(x, y))?F?i=12(?i ?
?0i )?i + ?i22?2i= A(?,?
)where f#(x, y) =?Fi=1 fi(x, y).
Taking the firstand second partial derivative of A(?,?)
with re-spect to ?i we obtain, respectively:?A(?,?)?
?i= Ep?
(x,y)[fi] ?
(?i ?
?0i )?2i+?i?2i?
Ep?(x)p?
(y|x)[fi ?
exp(?if#)]and?2A(?,?)?
?i2= ?1?2i?Ep?(x)p?
(y|x)[fi ?
f#?
exp(?if#)]< 0Since A(0,?)
= 0 and ?2A(?,?)?
?i2< 0, by solvingfor the unique root ??
of ?A(?,?)?
?i= 0 we obtainthe maximum value of A(?,?)
?
which is non-negative and thus guarantees that the regularizedlog-likelihood does not decrease at each iteration:L(?
+ ??)
?
L(?)
?
0.Solving for the root of ?A(?,?)?
?i= 0 results in theupdate Eq.
4 and is equivalent to finding the solutionto:Ep?
(x,y) [fi] ?
(?i ?
?0i )?2i=?i?2i+?x,yp?(x)p?
(y|x)fi(x, y)exp(?if#(x, y))A convenient way to solve this equation is to substi-tute ?i = exp(?i) and ai = Ep?
(x,y) [fi] ?(?i?
?0i)?2iand then use Newton?s method for finding the solu-tion to ai = f(?i), where f(?)
is:f(?)
=log ??2i+?x,yp?(x)p?
(y|x)fi(x, y)?f#(x,y)The summation on the right hand side reduces toaccumulating the coefficients of a polynomial in?
whose maximum degree is the highest possiblevalue of f#(x, y) on any context x encountered inthe training data and any allowed predicted valuey ?
Y .
