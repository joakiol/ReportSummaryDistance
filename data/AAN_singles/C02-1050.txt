Bidirectional Decoding for Statistical Machine TranslationTaro WATANABE ??
and Eiichiro SUMITA ?
{taro.watanabe, eiichiro.sumita}@atr.co.jp?
ATR Spoken Language Translation Research Laboratories ?
Department of Information Science2-2-2 Hikaridai Seika-cho, Kyoto UniversitySoraku-gun, Kyoto 619-0288 JAPAN Sakyo-ku, Kyoto 606-8501, JAPANAbstractThis paper describes the right-to-left decodingmethod, which translates an input string by gen-erating in right-to-left direction.
In addition, pre-sented is the bidirectional decoding method, thatcan take both of the advantages of left-to-right andright-to-left decoding method by generating outputin both ways and by merging hypothesized partialoutputs of two directions.
The experimental resultson Japanese and English translation showed thatthe right-to-left was better for Englith-to-Japanesetranslation, while the left-to-right was suitable forJapanese-to-English translation.
It was also ob-served that the bidirectional method was better forEnglish-to-Japanese translation.1 IntroductionThe statistical approach to machine translation re-gards the machine translation problem as the maxi-mum likelihood solution of a translation target textgiven a translation source text.
According to theBayes Rule, the problem is transformed into thenoisy channel model paradigm, where the transla-tion is the maximum a posteriori solution of a dis-tribution for a channel target text given a channelsource text and a prior distribution for the channelsource text (Brown et al, 1993).Although there exists efficient algorithms to es-timate the parameters for the statistical machinetranslation (SMT), one of the problems of SMT isthe search algorithms for the translation given a se-quence of words.
There exists stack decoding al-gorithm (Berger et al, 1996), A* search algorithm(Och et al, 2001; Wang and Waibel, 1997) anddynamic-programming algorithms (Tillmann andNey, 2000; Garcia-Varea and Casacuberta, 2001),and all translate a given input string word-by-wordand render the translation in left-to-right, with prun-ing technologies assuming almost linearly alignedtranslation source and target texts.
The algorithmsproposed above cannot deal with drastically differ-ent word correspondence, such as Japanese and En-glish translation, where Japanese is SOV while SVOin English.
Germann et al (2001) suggested greedymethod and integer programming decoding, thoughthe first method suffer from the similar problem asdescribed above and the second is impractical forthe real-world application.This paper presents two decoding methods, oneis the right-to-left decoding based on the left-to-right beam search algorithm, which generates out-puts from the end of a sentence.
The second one isthe bidirectional decoding method which decodes inboth of the left-to-right and right-to-left directionsand merges the two hypothesized partial sentencesinto one.
The experimental results of Japanese andEnglish translation indicated that the right-to-leftdecoding was better for English-to-Japanese trans-lation, while the left-to-right decoding was betterfor Japanese-to-English decoding.
The above re-sults could be justified by the structural differenceof Japanese and English, where English takes theprefix structure that places emphasis at the begin-ning of a sentence, hence prefers left-to-right de-coding.
On the other hand, Japanese takes postfixstructure, setting attention around the end of a sen-tence, therefore favors right-to-left decoding.
Thebidirectional decoding, which can take both of thebenefits of decoding method, was superior to mono-directional decoding methods.The next section briefly describes the SMT fo-cusing on the IBM Model 4.
Then, the Section 3presents decoding algorithms in three direction, left-to-right, right-to-left and bi-direction.
The Section4 presents the results of Japanese and English trans-lation followed by discussions.2 Statistical Machine TranslationStatistical machine translation regards machinetranslation as a process of translating a source lan-NULL0 could1 you2 recommend3 another4 hotel5hoka no hoteru o shokaishi teitadake masu kaa = (4, 4, 5, 0, 3, 1, 1, 0)Figure 1: An example of alignment for Japanese and English sentencesguage text (f) into a target language text (e) with thefollowing formula:e = arg maxeP(e|f)The Bayes Rule is applied to the above to derive:e = arg maxeP(f|e)P(e)The translation process is treated as a noisy chan-nel model, like those used in speech recognition inwhich there exists e transcribed as f, and a trans-lation is to infer the best e from f in terms ofP(f|e)P(e).
The former term, P(f|e), is a translationmodel representing some correspondence betweenbilingual text.
The latter, P(e), is the languagemodel denoting the likelihood of the channel sourcetext.
In addition, a word correspondence model,called alignment a, is introduced to the translationmodel to represent a positional correspondence ofthe channel target and source words:e = arg maxe?aP(f, a|e)P(e)An example of an alignment is shown in Figure 1,where the English sentence ?could you recommendanother hotel?
is mapped onto the Japanese ?hokano hoteru o shokaishi teitadake masu ka?, and both?hoka?
and ?no?
are aligned to ?another?, etc.
TheNULL symbol at index 0 is also a lexical entry inwhich no morpheme is aligned from the channeltarget morpheme, such as ?masu?
and ?ka?
in thisJapanese example.2.1 IBM Model 4The IBM Model 4, main focus in this paper, is com-posed of the following models (see Figure 2):?
Lexical Model ?
t( f |e) : Word-for-word trans-lation model, representing the probability of asource word f being translated into a targetword e.?
Fertility Model ?
n(?|e) : Representing theprobability of a source word e generating ?words.?
Distortion Model ?
d : The probability of dis-tortion.
In Model 4, the model is decomposedinto two sets of parameters:?
d1( j ?
c?i|A(ei),B( f j)) : Distortion prob-ability for head words.
The head wordis the first of the target words generatedfrom a source word a cept, that is thechannel source word with fertility morethan and equal to one.
The head word po-sition j is determined by the word classesof the previous source word, A(ei), andtarget word, B( f j), relative to the centroidof the previous source word, c?i .?
d>1( j ?
j?|B( f j)) : Distortion probabil-ity for non-head words.
The position ofa non-head word j is determined by theword class and relative to the previous tar-get word generated from the cept ( j?).?
NULL Translation Model ?
p1 : A fixed prob-ability of inserting a NULL word after deter-mining each target word f .For details, refer to Brown et al (1993).2.2 Search ProblemThe search problem of statistical machine trans-lation is to induce the maximum likely channelsource sequence, e, given f and the model, P(f|e) =?a P(f, a|e) and P(e).
For the space of a is ex-tremely large, |a|l+1, where the l is the output length,an approximation of P(f|e) ' P(f, a|e) is used whenexploring the possible candidates of translation.This problem is known to be NP-Complete(Knight, 1999), for the re-ordering property in themodel further complicates the search.
One of thesolution is the left-to-right generation of output byconsuming input words in any-order.
Under thisconstraint, many researchers had contributed algo-rithms and associated pruning strategies, such asBerger et al (1996), Och et al (2001), Wang andWaibel (1997), Tillmann and Ney (2000) Garcia-Varea and Casacuberta (2001) and Germann et al(2001), though they all based on almost linearlyTranslation ModelLexical Model?t( f j|ei)Fertility Model?n(?i |ei)Distortion ModelHead ?
d1( j ?
c?i|A(e?i )B( f j))Non-Head ?
d1>( j ?
j?|B( f j))NULL Translation Model(m?
?0?0)pm?2?00 p?01Figure 2: Translation Model (IBM Model 4)aligned language pairs, and not suitable for lan-guage pairs with totally different alignment corre-spondence, such as Japanese and English.3 Decoding AlgorithmsThe decoding methods presented in this paper ex-plore the partial candidate translation hypothesesgreedily, as presented in Tillmann and Ney (2000)and Och et al (2001), and operation applied to eachhypothesis is similar to those explained in Bergeret al (1996), Och et al (2001) and Germann etal.
(2001).
The algorithm is depicted in Algorithm1 where C = { jk : k = 1...|C|} represents a setof input string position 1.
The algorithm assumestwo kinds of partial hypotheses2, translated partiallyfrom an input string, one is an open hypothesis thatcan be extended by raising the fertility.
The otheris a close hypothesis that is to be extended by in-serting a string e?
to the hypothesis.
The e?
is a se-quence of output word, consisting of a word with thefertility more than one (translation of f j) and otherwords with zero fertility.
The translation of f j canbe computed either by inverse translation table (Ochet al, 2001; Al-Onaizan et al, 1999).
The list ofzero fertility words can be obtained from the viterbialignment of training corpus (Germann et al, 2001).The extension operator applied to an open hypothe-sis (e,C) is:?
align j to ei ?
this creates a new hypothesisby raising the fertility of ei by consuming theinput word f j.
The generated hypothesis canbe treated as either closed or open, that meansto stop raising the fertility or raise the fertilityfurther more.The operators applied to a close hypothesis are:1For simplicity, the dependence of alignment, a is omitted.2There exist a complete hypothesis, that is a candidate oftranslation.Algorithm 1 Beam Decoding Searchinput source string: f1 f2... fmfor all cardinality c = 0, 1, ...m ?
1 dofor all (e,C) where |C| = c dofor all j = 1, ...m and j < C doif (e,C) is open thenalign j to ei and keep it openalign j to ei and close itelsealign j to NULLinsert e?, align from j and open itinsert e?, align from j and close itend ifend forend forend for?
align j to NULL ?
raise the fertility for theNULL word.?
insert e?, align from j ?
this operator insert astring e?
and align one input word f j to one ofthe word in e?.
After this operation, the newhypothesis can be regarded as either open orclosed.Pruning is inevitable in the process of decoding,and applied is the beam search pruning, in which themaximum number of hypotheses to be consideredis limited.
In addition, fertility pruning is also in-troduced which suppress the word with large num-ber of fertility.
The skipping based criteria, such asintroduced by Och et al (2001), is not appropri-ate for the language pairs with drastically differentalignment, such as Japanese and English, hence wasnot considered in this paper.
Depending on the out-put generation direction, the algorithm can generateeither in left-to-right or right-to-left, by alternatingsome constraints of insertion of output words.e1 ... el e?1 ... e?l?f1 f2 ... f j ... fme e?Figure 3: string insertion operator for left-to-rightdecoding method.
A string e?
was appended afterthe partial output string, e, and the last word in e?was aligned from f j.e?1 ... e?l?
e1 ... elf1 f2 ... f j ... fme?
eFigure 4: string insertion operation for right-to-leftdecoding method.
A string e?
was prepended beforethe partial output string, e, and the first word in e?was aligned from f j.3.1 Left-to-Right DecodingThe left-to-right decoding enforces the restrictionwhere the insertion of e?
is allowed after the par-tially generated e, and alignment from the inputword f j is restricted to the end of the word of e?.Hence, the operator applied to an open hypothesisraise the fertility for the word at the end of e (referto Figure 3).The language which place emphasis around thebeginning of a sentence, such as English, will besuitable in this direction, for the Language Modelscore P(e) can estimate what should come first.Hence, the decoder can discriminate a hypothesisbetter or not.3.2 Right-to-Left DecodingThe right-to-left decoding does the reverse of theleft-to-right decoding, in which the insertion of e?is allowed only before the e and the f j is alignedto the beginning of the word of e?
(see Figure 4).Therefore, the open hypothesis is extended by rais-ing the fertility of the beginning of the word of e. Inprepending a string to a partial hypothesis, an align-ment vector should be reassigned so that the valuescan point out correct index.Again, the right-to-left direction is suitable forthe language which enforces stronger constraints atthe end of sentence, such as Japanese, similar to thereason mentioned above.e f 1 ... ei ... eblbef eb(a) merging two open hy-pothesese f 1 ... e f l f e?
eb1 ... eblbef eb(b) merging two close hypotheses with in-serted e?Figure 5: Merging left-to-right and right-to-lefthypotheses (ef and eb) in bidirectional decodingmethod.
Figure 5(a) merge two open hypotheses,while Figure 5(b) merge them with inserted zero fer-tility words.3.3 Bidirectional DecodingThe bidirectional decoding decode the input wordsin both direction, one with left-to-right decodingmethod up to the cardinality of dm/2e and right-to-left direction up to the cardinality of bm/2c, wherem is the input length.
Then, the two hypotheses aremerged when both are open and can share the sameoutput word e, which resulted in raising the fertilityof e. If both of them are closed hypotheses, thenan additional sequence of zero fertility words (orNULL sequence) are inserted (refer to Figure 5).3.4 Computational ComplexityThe computational complexity for the left-to-rightand right-to-left is the same, O(|E|3m22m), as re-ported by Tillmann and Ney (2000), in which |E|is the size of the vocabulary for output sentences 3.The bidirectional method involves merging of twohypotheses, hence additional O(( mm/2)) is required.3.5 Effects of Decoding DirectionThe decoding algorithm generating in left-to-rightdirection fills the output sequence from the begin-ning of a sentence by consuming the input words inany order and by selecting the corresponding trans-lation.Therefore, the languages with prefix structure,such as English, German or French, can take thebenefits of this direction, because the languagemodel/translation model can differentiate ?good?hypotheses to ?bad?
hypotheses around the begin-ning of the output sentences.
Therefore, the nar-rowing the search space by the beam search crite-3The term |E|3 is the case for trigram language model.ria (pruning) would not affect the overall quality.On the other hand, if right-to-left decoding methodwere applied to such a language above, the dif-ference of good hypotheses and bad hypotheses issmall, hence the drop of hypotheses would affect thequality of translation.The similar statement can hold for postfix lan-guages, such as Japanese, where emphasis is placedaround the end of a sentence.
For such languages,right-to-left decoding will be suitable but left-to-right decoding will degrade the quality of transla-tion.The bidirectional decoding is expected to take thebenefits of both of the directions, and will show thebest results in any kind of languages.4 Experimental ResultsThe corpus for this experiment consists of 172,481bilingual sentences of English and Japanese ex-tracted from a large-scale travel conversation corpus(Takezawa et al, 2002).
The statistics of the corpusare shown in Table 1.
The database was split intothree parts: a training set of 152,183 sentence pairs,a validation set of 10,148, and a test set of 10,150.The translation models, both for the Japanese-to-English (J-E) and English-to-Japanese (E-J) trans-lation, were trained toward IBM Model 4 on thetraining set and cross-validated on validation set toterminate the iteration by observing perplexity.
Inmodeling IBM Model 4, POSs were used as wordclasses.From the viterbi alignments of the training cor-pus, A list of possible insertion of zero fertilitywords were extracted with frequency more than 10,around 1,300 sequences of words for both of the J-E and E-J translations.
The test set consists of 150Japanese sentences varying by the sentence lengthof 6, 8 and 10.
The translation was carried outby three decoding methods:left-to-right, right-to-left and bidirectional one.The translation results were evaluated by word-error-rate (WER) and position independent word-error-rate (PER) (Watanabe et al, 2002; Och et al,2001).
The WER is the measure by penalizing in-sertion/deletion/replacement by 1.
The PER is theone similar to WER but ignores the positions, al-lowing the reordered outputs, hence can estimate theaccuracy for the tranlslation word selection.
It hasbeen also evaluated by subjective evaluation (SE)with the criteria ranging from A(perfect) to D(non-Table 1: Statistics on a travel conversation corpusJapanese English# of sentences 172,481# of words 1,186,620 1,005,080vocabulary size 22,801 15,768avg.
sentence length 6.88 5.833-gram perplexity 26.16 36.92Table 3: Comparison of the three decoders by theratio each decoder produced search errors.J-E E-JLtoR 11.3 12.0RtoL 59.3 34.0Bi 15.3 15.3sense) 4 (Sumita et al, 1999).Table 2 summarizes the results of decoding byleft-to-right, right-to-left and bidirectional methodevaluated with WER, PER and SE.
Table 3 showsthe ratio of producing search errors, computed bycomparing the translation model and lnguage modelscores for the outputs from three decoding methods.Sample Japanese-to-English translations performedby the decoders is presented in Figure 6.5 DiscussionsFrom Table 2, the left-to-right decoding method per-formed better than the right-to-left one in Japanese-to-English translation as expected in Section 3.5.Furthermore, the bidirectional decoding methodwas slightly better than the left-to-right one, for itcould combine the benefits of both directions.Similar analysis could hold for English-to-Japanese translation, and the right-to-left decodingmethod was slightly superior to the left-to-right onein terms of WER/PER scores, though the SE scoredropped from 8.7% to 6.7% in C-ranked sentences.Overall quality measured by the SE rate for ac-cepted senteces, ranging from A to C, dropped from68.0% into 66.0%.
In addition, the bidirectionalmethod in English-to-Japanese translation was notevaluated as high as those in Japanese-to-Englishtranslation: the results were closer to the left-to-right method.
This might be due to the nature of lan-4The meanings of the symbol are follows: A ?
perfect:no problem in either information or grammar; B ?
fair: easyto understand but some important information is missing or itis grammatically flawed; C ?
acceptable: broken but under-standable with effort; D ?
nonsense: important informationhas been translated incorrectly.Table 2: Summary of results for Japanese-to-English (J-E) and English-to-Japanese (E-J) translations byleft-to-right (LtoR), right-to-left (RtoL) and bidirectional (Bi) decoding methods.Trans.
Alg.
WER PER SEA B C DJ-E LtoR 70.0 64.8 26.7% 23.3% 20.0% 30.0%RtoL 74.6 66.9 21.3% 24.7% 18.0% 36.0%Bi 69.9 63.7 27.3% 22.7% 20.7% 29.3%E-J LtoR 66.2 57.6 49.3% 10.0% 8.7% 32.0%RtoL 64.0 56.1 49.3% 10.0% 6.7% 34.0%Bi 66.0 58.0 48.7% 8.0% 10.0% 33.3%input: suri ni saifu o sura re mashi ta(i had my pocket picked)LtoR: here ?s my wallet was stolenRtoL: here ?s my wallet was stolenBi: i had my wallet stoleninput: sumimasen ga terasu no seki ga ii no desu ga(excuse me but can we have a table on the terrace)LtoR: excuse me i ?d like a seat on the terraceRtoL: i ?d prefer excuse meBi: i ?d like a seat on the terraceinput: nan ji ni owaru no desu(what time will it be over)LtoR: what time should i be at the endRtoL: it ?s what time will it be overBi : at what time is it endinput: nimotsu o ue ni age te morae masu ka(will you put my luggage on the rack)LtoR: could you put my baggage hereRtoL: do you have overhead luggageBi: could you put my baggageinput: ee ani to imouto ga hitori zutsu i masu(yes i have a brother and a sister)LtoR: yes brother and sister there a daughterRtoL: you ?re yes brother and sister daughterBi: yes my daughter is there a brother and sisterFigure 6: Examples of Japanese-to-English translationguage model employed for this experiment, for thelanguage model probabilities were assigned basedon the left history, not the right history.
It is ex-pected that the use of the suitable language modelcontext direction corresponding to a generation di-rection would assign appropriate probability, hencewould be able to differentiate better hypotheses.Table 3 indicats that the right-to-left decodingmethod produced more errors than other methodsregardless of translaiton directions.
This is ex-plained by the use of the left history languagemodel, not the right context one, as stated above.Nevertheless, the search error decreased from 59.3into 34.0 by alternating the translation direction forthe right-to-left decoding method, which still sup-ports the use of the correct rendering direction fortranslation target language.6 ConclusionThe decoding methods for statistical machine trans-lation presented here varies the output directions,left-to-right, right-to-left and bi-direction, and wereexperimented with drastically different languagepairs, English and Japanese.
The results indicatedthat the left-to-right decoding method was suit-able for Japanese-to-English translation while theright-to-left decoding method fit with English-to-Japanese translation.
In addition, the bidirectionaldecoding method was superior to mono-directionaldecoding method for Japanese-to-English transla-tion.
This suggests that the translation output gen-eration should match with the underlying linguisticstructure for the output language.AcknowledgementThe research reported here was supported in part bya contract with the Telecommunications Advance-ment Organization of Japan entitled, ?A study ofspeech dialogue translation technology based on alarge corpus?.ReferencesYaser Al-Onaizan, Jan Curin, Michael Jahr, KevinKnight, John Lafferty, Dan Melamed, Frantz-Josef Och, David Purdy, Noah A. Smith, andDavid Yarowsky.
1999.
Statistical machinetranslation final report, jhu workshop 1999, 12.A.
Berger, P. Brown, S. Pietra, V. Pietra, J. Gillett,A.
Kehler, and R. Mercer.
1996.
Language trans-lation apparatus and method of using context-based translation models.
Technical report,United States Patent, Patent Number 5510981,April.Peter F. Brown, Stephen A. Della Pietra, VincentJ.
Della Pietra, and Robert L. Mercer.
1993.The mathematics of statistical machine transla-tion: Parameter estimation.
Computational Lin-guistics, 19(2):263?311.Ismael Garcia-Varea and Francisco Casacuberta.2001.
Search algorithms for statistical machinetranslation based on dynamic programming andpruning techniques.
In MT Summit VIII, Santiagode Compostela, Galicia, Spain, september.Ulrich Germann, Michael Jahr, Kevin Knight,Daniel Marcu, and Kenji Yamada.
2001.
Fast de-coding and optimal decoding for machine trans-lation.
In Proc.
of ACL-01, Toulouse, France.Kevin Knight.
1999.
Decoding complexity inword-replacement translation models.
Computa-tional Linguistics, 25(4):607?615.Franz Josef Och, Nicola Ueffing, and Hermann Ney.2001.
An efficient a* search algorithm for statis-tical machine translation.
In Proc.
of the ACL-2001 Workshop on Data-Driven Machine Trans-lation, pages 55?62, Toulouse, France, July.Eiichiro Sumita, Setsuo Yamada, Kazuhide Ya-mamoto, Michael Paul, Hideki Kashioka, KaiIshikawa, and Satoshi Shirai.
1999.
Solutionsto problems inherent in spoken-language transla-tion: The ATR-MATRIX approach.
In MachineTranslation Summit VII, pages 229?235.Toshiyuki Takezawa, Eiichiro Sumita, FumiakiSugaya, Hirofumi Yamamoto, and Seiichi Ya-mamoto.
2002.
Toward a broad-coverage bilin-gual corpus for speech translation of travel con-versations in the real world.
In Proc.
of LREC2002, pages 147?152, Las Palmas, Canary Is-lands, Spain, May.Christoph Tillmann and Hermann Ney.
2000.
Wordre-ordering and dp-based search in statistical ma-chine translation.
In Proc.
of the COLING 2000,July-August.Ye-Yi Wang and Alex Waibel.
1997.
Decoding al-gorithm in statistical machine translation.
In Pro-ceedings of the 35th Annual Meeting of the Asso-ciation for Computational Linguistics.Taro Watanabe, Kenji Imamura, and EiichiroSumita.
2002.
Statistical machine translationbased on hierarchical phrase alignment.
In Proc.of TMI 2002, Keihanna, Japan, March.
