Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 649?655,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsOn WordNet Semantic Classes and Dependency ParsingKepa Bengoetxea?, Eneko Agirre?, Joakim Nivre?,Yue Zhang*, Koldo Gojenola?
?University of the Basque Country UPV/EHU / IXA NLP Group?Uppsala University / Department of Linguistics and Philology?
Singapore University of Technology and Designkepa.bengoetxea@ehu.es, e.agirre@ehu.es,joakim.nivre@lingfil.uu.se, yue zhang@sutd.edu.sg,koldo.gojenola@ehu.esAbstractThis paper presents experiments withWordNet semantic classes to improve de-pendency parsing.
We study the effectof semantic classes in three dependencyparsers, using two types of constituency-to-dependency conversions of the EnglishPenn Treebank.
Overall, we can say thatthe improvements are small and not sig-nificant using automatic POS tags, con-trary to previously published results usinggold POS tags (Agirre et al, 2011).
Inaddition, we explore parser combinations,showing that the semantically enhancedparsers yield a small significant gain onlyon the more semantically oriented LTHtreebank conversion.1 IntroductionThis work presents a set of experiments to investi-gate the use of lexical semantic information in de-pendency parsing of English.
Whether semanticsimprove parsing is one interesting research topicboth on parsing and lexical semantics.
Broadlyspeaking, we can classify the methods to incor-porate semantic information into parsers in two:systems using static lexical semantic repositories,such as WordNet or similar ontologies (Agirre etal., 2008; Agirre et al, 2011; Fujita et al, 2010),and systems using dynamic semantic clusters au-tomatically acquired from corpora (Koo et al,2008; Suzuki et al, 2009).Our main objective will be to determinewhether static semantic knowledge can help pars-ing.
We will apply different types of semantic in-formation to three dependency parsers.
Specifi-cally, we will test the following questions:?
Does semantic information in WordNet helpdependency parsing?
Agirre et al (2011)found improvements in dependency parsingusing MaltParser on gold POS tags.
In thiswork, we will investigate the effect of seman-tic information using predicted POS tags.?
Is the type of semantic information relatedto the type of parser?
We will test threedifferent parsers representative of successfulparadigms in dependency parsing.?
How does the semantic information relate tothe style of dependency annotation?
Most ex-periments for English were evaluated on thePenn2Malt conversion of the constituency-based Penn Treebank.
We will also examinethe LTH conversion, with richer structure andan extended set of dependency labels.?
How does WordNet compare to automati-cally obtained information?
For the sake ofcomparison, we will also perform the experi-ments using syntactic/semantic clusters auto-matically acquired from corpora.?
Does parser combination benefit from seman-tic information?
Different parsers can use se-mantic information in diverse ways.
For ex-ample, while MaltParser can use the semanticinformation in local contexts, MST can in-corporate them in global contexts.
We willrun parser combination experiments with andwithout semantic information, to determinewhether it is useful in the combined parsers.After introducing related work in section 2, sec-tion 3 describes the treebank conversions, parsersand semantic features.
Section 4 presents the re-sults and section 5 draws the main conclusions.2 Related workBroadly speaking, we can classify the attempts toadd external knowledge to a parser in two sets:using large semantic repositories such as Word-Net and approaches that use information automat-ically acquired from corpora.
In the first group,Agirre et al (2008) trained two state-of-the-artconstituency-based statistical parsers (Charniak,6492000; Bikel, 2004) on semantically-enriched in-put, substituting content words with their seman-tic classes, trying to overcome the limitations oflexicalized approaches to parsing (Collins, 2003)where related words, like scissors and knife, can-not be generalized.
The results showed a signi-cant improvement, giving the first results over bothWordNet and the Penn Treebank (PTB) to showthat semantics helps parsing.
Later, Agirre et al(2011) successfully introduced WordNet classes ina dependency parser, obtaining improvements onthe full PTB using gold POS tags, trying differentcombinations of semantic classes.
MacKinlay etal.
(2012) investigate the addition of semantic an-notations in the form of word sense hypernyms, inHPSG parse ranking, reducing error rate in depen-dency F-score by 1%, while some methods pro-duce substantial decreases in performance.
Fu-jita et al (2010) showed that fully disambiguatedsense-based features smoothed using ontologicalinformation are effective for parse selection.On the second group, Koo et al (2008) pre-sented a semisupervised method for training de-pendency parsers, introducing features that incor-porate word clusters automatically acquired froma large unannotated corpus.
The clusters includestrongly semantic associations like {apple, pear}or {Apple, IBM} and also syntactic clusters like{of, in}.
They demonstrated its effectiveness independency parsing experiments on the PTB andthe Prague Dependency Treebank.
Suzuki et al(2009), Sagae and Gordon (2009) and Canditoand Seddah (2010) also experiment with the samecluster method.
Recently, T?ackstr?om et al (2012)tested the incorporation of cluster features fromunlabeled corpora in a multilingual setting, givingan algorithm for inducing cross-lingual clusters.3 Experimental FrameworkIn this section we will briefly describe the PTB-based datasets (subsection 3.1), followed by thedata-driven parsers used for the experiments (sub-section 3.2).
Finally, we will describe the differenttypes of semantic representation that were used.3.1 Treebank conversionsPenn2Malt1performs a simple and direct conver-sion from the constituency-based PTB to a depen-dency treebank.
It obtains projective trees and hasbeen used in several works, which allows us to1http://w3.msi.vxu.se/ nivre/research/Penn2Malt.htmlcompare our results with related experiments (Kooet al, 2008; Suzuki et al, 2009; Koo and Collins,2010).
We extracted dependencies using standardhead rules (Yamada and Matsumoto, 2003), and areduced set of 12 general dependency tags.LTH2(Johansson and Nugues, 2007) presentsa conversion better suited for semantic process-ing, with a richer structure and a more fine-grainedset of dependency labels (42 different dependencylabels), including links to handle long-distancephenomena, giving a 6.17% of nonprojective sen-tences.
The results from parsing the LTH outputare lower than those for Penn2Malt conversions.3.2 ParsersWe have made use of three parsers representativeof successful paradigms in dependency parsing.MaltParser (Nivre et al, 2007) is a determinis-tic transition-based dependency parser that obtainsa dependency tree in linear-time in a single passover the input using a stack of partially analyzeditems and the remaining input sequence, by meansof history-based feature models.
We added twofeatures that inspect the semantic feature at the topof the stack and the next input token.MST3represents global, exhaustive graph-based parsing (McDonald et al, 2005; McDon-ald et al, 2006) that finds the highest scoring di-rected spanning tree in a graph.
The learning pro-cedure is global since model parameters are setrelative to classifying the entire dependency graph,in contrast to the local but richer contexts usedby transition-based parsers.
The system can betrained using first or second order models.
Thesecond order projective algorithm performed beston both conversions, and we used it in the rest ofthe evaluations.
We modified the system in or-der to add semantic features, combining them withwordforms and POS tags, on the parent and childnodes of each arc.ZPar4(Zhang and Clark, 2008; Zhang andNivre, 2011) performs transition-based depen-dency parsing with a stack of partial analysisand a queue of remaining inputs.
In contrast toMaltParser (local model and greedy deterministicsearch) ZPar applies global discriminative learn-ing and beam search.
We extend the feature set ofZPar to include semantic features.
Each set of se-mantic information is represented by two atomic2http://nlp.cs.lth.se/software/treebank converter3http://mstparser.sourceforge.net4www.sourceforge.net/projects/zpar650Base WordNet WordNet Clustersline SF SSMalt 88.46 88.49 (+0.03) 88.42 (-0.04) 88.59 (+0.13)MST 90.55 90.70 (+0.15) 90.47 (-0.08) 90.88 (+0.33)?ZPar 91.52 91.65 (+0.13) 91.70 (+0.18)?
91.74 (+0.22)Table 1: LAS results with several parsing algo-rithms, Penn2Malt conversion (?
: p <0.05, ?
: p<0.005).
In parenthesis, difference with baseline.feature templates, associated with the top of thestack and the head of the queue, respectively.
ZParwas directly trained on the Penn2Malt conversion,while we applied the pseudo-projective transfor-mation (Nilsson et al, 2008) on LTH, in order todeal with non-projective arcs.3.3 Semantic informationOur aim was to experiment with different types ofWordNet-related semantic information.
For com-parison with automatically acquired information,we will also experiment with bit clusters.WordNet.
We will experiment with the seman-tic representations used in Agirre et al (2008) andAgirre et al (2011), based on WordNet 2.1.
Word-Net is organized into sets of synonyms, calledsynsets (SS).
Each synset in turn belongs to aunique semantic file (SF).
There are a total of 45SFs (1 for adverbs, 3 for adjectives, 15 for verbs,and 26 for nouns), based on syntactic and seman-tic categories.
For example, noun SFs differen-tiate nouns denoting acts or actions, and nounsdenoting animals, among others.
We experimentwith both full SSs and SFs as instances of fine-grained and coarse-grained semantic representa-tion, respectively.
As an example, knife in itstool sense is in the EDGE TOOL USED AS ACUTTING INSTRUMENT singleton synset, andalso in the ARTIFACT SF along with thousandsof words including cutter.
These are the two ex-tremes of semantic granularity in WordNet.
Foreach semantic representation, we need to deter-mine the semantics of each occurrence of a targetword.
Agirre et al (2011) used i) gold-standardannotations from SemCor, a subset of the PTB, togive an upper bound performance of the semanticrepresentation, ii) first sense, where all instancesof a word were tagged with their most frequentsense, and iii) automatic sense ranking, predictingthe most frequent sense for each word (McCarthyet al, 2004).
As we will make use of the full PTB,we only have access to the first sense information.Clusters.
Koo et al (2008) describe a semi-Base WordNet WordNet Clustersline SF SSMalt 84.95 85.12 (+0.17) 85.08 (+0.16) 85.13 (+0.18)MST 85.06 85.35 (+0.29)?
84.99 (-0.07) 86.18 (+1.12)?ZPar 89.15 89.33 (+0.18) 89.19 (+0.04) 89.17 (+0.02)Table 2: LAS results with several parsing algo-rithms in the LTH conversion (?
: p <0.05, ?
: p<0.005).
In parenthesis, difference with baseline.supervised approach that makes use of cluster fea-tures induced from unlabeled data, providing sig-nificant performance improvements for superviseddependency parsers on the Penn Treebank for En-glish and the Prague Dependency Treebank forCzech.
The process defines a hierarchical cluster-ing of the words, which can be represented as abinary tree where each node is associated to a bit-string, from the more general (root of the tree) tothe more specific (leaves).
Using prefixes of vari-ous lengths, it can produce clusterings of differentgranularities.
It can be seen as a representation ofsyntactic-semantic information acquired from cor-pora.
They use short strings of 4-6 bits to representparts of speech and the full strings for wordforms.4 ResultsIn all the experiments we employed a baseline fea-ture set using word forms and parts of speech, andan enriched feature set (WordNet or clusters).
Wefirstly tested the addition of each individual se-mantic feature to each parser, evaluating its contri-bution to the parser?s performance.
For the combi-nations, instead of feature-engineering each parserwith the wide array of different possibilities forfeatures, as in Agirre et al (2011), we adoptedthe simpler approach of combining the outputs ofthe individual parsers by voting (Sagae and Lavie,2006).
We will use Labeled Attachment Score(LAS) as our main evaluation criteria.
As in pre-vious work, we exclude punctuation marks.
Forall the tests, we used a perceptron POS-tagger(Collins, 2002), trained on WSJ sections 2?21, toassign POS tags automatically to both the training(using 10-way jackknifing) and test data, obtaininga POS tagging accuracy of 97.32% on the test data.We will make use of Bikel?s randomized parsingevaluation comparator to test the statistical signi-cance of the results.
In all of the experiments theparsers were trained on sections 2-21 of the PTBand evaluated on the development set (section 22).Finally, the best performing system was evaluatedon the test set (section 23).651Parsers LAS UASBest baseline (ZPar) 91.52 92.57Best single parser (ZPar + Clusters) 91.74 (+0.22) 92.63Best combination (3 baseline parsers) 91.90 (+0.38) 93.01Best combination of 3 parsers:3 baselines + 3 SF extensions 91.93 (+0.41) 92.95Best combination of 3 parsers:3 baselines + 3 SS extensions 91.87 (+0.35) 92.92Best combination of 3 parsers:3 baselines + 3 cluster extensions 91.90 (+0.38) 92.90Table 3: Parser combinations on Penn2Malt.Parsers LAS UASBest baseline (ZPar) 89.15 91.81Best single parser (ZPar + SF) 89.33 (+0.15) 92.01Best combination (3 baseline parsers) 89.15 (+0.00) 91.81Best combination of 3 parsers:3 baselines + 3 SF extensions 89.56 (+0.41)?
92.23Best combination of 3 parsers:3 baselines + 3 SS extensions 89.43 (+0.28) 93.12Best combination of 3 parsers:3 baselines + 3 cluster extensions 89.52 (+0.37)?
92.19Table 4: Parser combinations on LTH (?
: p <0.05,?
: p <0.005).4.1 Single ParsersWe run a series of experiments testing each indi-vidual semantic feature, also trying different learn-ing configurations for each one.
Regarding theWordNet information, there were 2 different fea-tures to experiment with (SF and SS).
For the bitclusters, there are different possibilities, depend-ing on the number of bits used.
For Malt and MST,all the different lengths of bit strings were used.Given the computational requirements and the pre-vious results on Malt and MST, we only tested allbits in ZPar.
Tables 1 and 2 show the results.Penn2Malt.
Table 1 shows that the only signifi-cant increase over the baseline is for ZPar with SSand for MST with clusters.LTH.
Looking at table 2, we can say that the dif-ferences in baseline parser performance are accen-tuated when using the LTH treebank conversion,as ZPar clearly outperforms the other two parsersby more than 4 absolute points.
We can see thatSF helps all parsers, although it is only significantfor MST.
Bit clusters improve significantly MST,with the highest increase across the table.Overall, we see that the small improvementsdo not confirm the previous results on Penn2Malt,MaltParser and gold POS tags.
We can also con-clude that automatically acquired clusters are spe-cially effective with the MST parser in both tree-bank conversions, which suggests that the type ofsemantic information has a direct relation to theparsing algorithm.
Section 4.3 will look at the de-tails by each knowledge type.4.2 CombinationsSubsection 4.1 presented the results of the base al-gorithms and their extensions based on semanticfeatures.
Sagae and Lavie (2006) report improve-ments over the best single parser when combiningthree transition-based models and one graph-basedmodel.
The same technique was also used by thewinning team of the CoNLL 2007 Shared Task(Hall et al, 2007), combining six transition-basedparsers.
We used MaltBlender5, a tool for mergingthe output of several dependency parsers, using theChu-Liu/Edmonds directed MST algorithm.
Afterseveral tests we noticed that weighted voting byeach parser?s labeled accuracy gave good results,using it in the rest of the experiments.
We traineddifferent types of combination:?
Base algorithms.
This set includes the 3 base-line algorithms, MaltParser, MST, and ZPar.?
Extended parsers, adding semantic informa-tion to the baselines.
We include the threebase algorithms and their semantic exten-sions (SF, SS, and clusters).
It is known (Sur-deanu and Manning, 2010) that adding moreparsers to an ensemble usually improves ac-curacy, as long as they add to the diver-sity (and almost regardless of their accuracylevel).
So, for the comparison to be fair, wewill compare ensembles of 3 parsers, takenfrom sets of 6 parsers (3 baselines + 3 SF,SS, and cluster extensions, respectively).In each experiment, we took the best combina-tion of individual parsers on the development setfor the final test.
Tables 3 and 4 show the results.Penn2Malt.
Table 3 shows that the combina-tion of the baselines, without any semantic infor-mation, considerably improves the best baseline.Adding semantics does not give a noticeable in-crease with respect to combining the baselines.LTH (table 4).
Combining the 3 baselines doesnot give an improvement over the best baseline, asZPar clearly outperforms the other parsers.
How-ever, adding the semantic parsers gives an increasewith respect to the best single parser (ZPar + SF),which is small but significant for SF and clusters.4.3 AnalysisIn this section we analyze the data trying to under-stand where and how semantic information helpsmost.
One of the obstacles of automatic parsersis the presence of incorrect POS tags due to auto-5http://w3.msi.vxu.se/users/jni/blend/652LAS on sentences LAS on sentencesPOS tags Parser LAS test set without POS errors with POS errorsGold ZPar 90.45 91.68 89.14Automatic ZPar 89.15 91.62 86.51Automatic Best combination of 3 parsers: 89.56 (+0.41) 91.90 (+0.28) 87.06 (+0.55)3 baselines + 3 SF extensionsAutomatic Best combination of 3 parsers: 89.43 (+0.28) 91.95 (+0.33) 86.75 (+0.24)3 baselines + 3 SS extensionsAutomatic Best combination of 3 parsers: 89.52 (+0.37) 91.92 (+0.30) 86.96 (+0.45)3 baselines + 3 cluster extensionsTable 5: Differences in LAS (LTH) for baseline and extended parsers with sentences having cor-rect/incorrect POS tags (the parentheses show the difference w.r.t ZPar with automatic POS tags).matic tagging.
For example, ZPar?s LAS score onthe LTH conversion drops from 90.45% with goldPOS tags to 89.12% with automatic POS tags.
Wewill examine the influence of each type of seman-tic information on sentences that contain or notPOS errors, and this will clarify whether the incre-ments obtained when using semantic informationare useful for correcting the negative influence ofPOS errors or they are orthogonal and constitutea source of new information independent of POStags.
With this objective in mind, we analyzed theperformance on the subset of the test corpus con-taining the sentences which had POS errors (1,025sentences and 27,300 tokens) and the subset wherethe sentences had (automatically assigned) correctPOS tags (1,391 sentences and 29,386 tokens).Table 5 presents the results of the best singleparser on the LTH conversion (ZPar) with goldand automatic POS tags in the first two rows.
TheLAS scores are particularized for sentences thatcontain or not POS errors.
The following threerows present the enhanced (combined) parsersthat make use of semantic information.
As thecombination of the three baseline parsers did notgive any improvement over the best single parser(ZPar), we can hypothesize that the gain comingfrom the parser combinations comes mostly fromthe addition of semantic information.
Table 5 sug-gests that the improvements coming from Word-Net?s semantic file (SF) are unevenly distributedbetween the sentences that contain POS errors andthose that do not (an increase of 0.28 for sentenceswithout POS errors and 0.55 for those with er-rors).
This could mean that a big part of the in-formation contained in SF helps to alleviate theerrors performed by the automatic POS tagger.
Onthe other hand, the increments are more evenlydistributed for SS and clusters, and this can bedue to the fact that the semantic information isorthogonal to the POS, giving similar improve-ments for sentences that contain or not POS errors.We independently tested this fact for the individ-ual parsers.
For example, with MST and SF thegains almost doubled for sentences with incorrectPOS tags (+0.37 with respect to +0.21 for sen-tences with correct POS tags) while the gains ofadding clusters?
information for sentences withoutand with POS errors were similar (0.91 and 1.33,repectively).
This aspect deserves further inves-tigation, as the improvements seem to be relatedto both the type of semantic information and theparsing algorithm.We did an initial exploration butit did not give any clear indication of the types ofimprovements that could be expected using eachparser and semantic data.5 ConclusionsThis work has tried to shed light on the contribu-tion of semantic information to dependency pars-ing.
The experiments were thorough, testing twotreebank conversions and three parsing paradigmson automatically predicted POS tags.
Comparedto (Agirre et al, 2011), which used MaltParser onthe LTH conversion and gold POS tags, our resultscan be seen as a negative outcome, as the improve-ments are very small and non-significant in mostof the cases.
For parser combination, WordNetsemantic file information does give a small sig-nificant increment in the more fine-grained LTHrepresentation.
In addition we show that the im-provement of automatic clusters is also weak.
Forthe future, we think tdifferent parsers, eitherhat amore elaborate scheme is needed for word classes,requiring to explore different levels of generaliza-tion in the WordNet (or alternative) hierarchies.AcknowledgmentsThis research was supported by the the BasqueGovernment (IT344- 10, S PE11UN114), the Uni-versity of the Basque Country (GIU09/19) andthe Spanish Ministry of Science and Innovation(MICINN, TIN2010-20218).653ReferencesEneko Agirre, Timothy Baldwin, and David Martinez.2008.
Improving parsing and PP attachment per-formance with sense information.
In Proceedingsof ACL-08: HLT, pages 317?325, Columbus, Ohio,June.
Association for Computational Linguistics.Eneko Agirre, Kepa Bengoetxea, Koldo Gojenola, andJoakim Nivre.
2011.
Improving dependency pars-ing with semantic classes.
In Proceedings of the49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 699?703, Portland, Oregon, USA, June.Association for Computational Linguistics.Daniel M. Bikel.
2004.
Intricacies of collins?
parsingmodel.
Computational Linguistics, 30(4):479?511.Marie Candito and Djam?e Seddah.
2010.
Pars-ing word clusters.
In Proceedings of the NAACLHLT 2010 First Workshop on Statistical Parsing ofMorphologically-Rich Languages, pages 76?84, LosAngeles, CA, USA, June.
Association for Computa-tional Linguistics.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the 1st NorthAmerican chapter of the Association for Computa-tional Linguistics conference, NAACL 2000, pages132?139, Stroudsburg, PA, USA.
Association forComputational Linguistics.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedingsof the 2002 Conference on Empirical Methods inNatural Language Processing, pages 1?8.
Associ-ation for Computational Linguistics, July.Michael Collins.
2003.
Head-driven statistical modelsfor natural language parsing.
Computational Lin-guistics, 29(4):589?637, December.Sanae Fujita, Francis Bond, Stephan Oepen, andTakaaki Tanaka.
2010.
Exploiting semantic infor-mation for hpsg parse selection.
Research on Lan-guage and Computation, 8(1):122.Johan Hall, Jens Nilsson, Joakim Nivre, Glsen Eryigit,Beta Megyesi, Mattias Nilsson, and Markus Saers.2007.
Single malt or blended?
a study in multi-lingual parser optimization.
In Proceedings of theCoNLL Shared Task EMNLP-CoNLL.Richard Johansson and Pierre Nugues.
2007.
Ex-tended constituent-to-dependency conversion forEnglish.
In Proceedings of NODALIDA 2007, pages105?112, Tartu, Estonia, May 25-26.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 1?11, Uppsala, Sweden,July.
Association for Computational Linguistics.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency parsing.In Proceedings of ACL-08: HLT, pages 595?603,Columbus, Ohio, June.
Association for Computa-tional Linguistics.Andrew MacKinlay, Rebecca Dridan, Diana McCarthy,and Timothy Baldwin.
2012.
The effects of seman-tic annotations on precision parse ranking.
In FirstJoint Conference on Lexical and Computational Se-mantics (*SEM), page 228236, Montreal, Canada,June.
Association for Computational Linguistics.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Finding predominant word sensesin untagged text.
In Proceedings of the 42nd Meet-ing of the Association for Computational Linguistics(ACL?04), Main Volume, pages 279?286, Barcelona,Spain, July.R.
McDonald, K. Crammer, and F. Pereira.
2005.
On-line large-margin training of dependency parsers.
InProceedings of ACL.R.
McDonald, K. Lerman, and F. Pereira.
2006.
Mul-tilingual dependency analysis with a two-stage dis-criminative parser.
In Proceedings of CoNLL 2006.Jens Nilsson, Joakim Nivre, and Johan Hall.
2008.Generalizing tree transformations for inductive de-pendency parsing.
In Proceedings of the 45th Con-ference of the ACL.Joakim Nivre, Johan Hall, Jens Nilsson, Chanev A.,Glsen Eryiit, Sandra Kbler, Marinov S., and EdwinMarsi.
2007.
Maltparser: A language-independentsystem for data-driven dependency parsing.
NaturalLanguage Engineering.Kenji Sagae and Andrew Gordon.
2009.
Clusteringwords by syntactic similarity improves dependencyparsing of predicate-argument structures.
In Pro-ceedings of the Eleventh International Conferenceon Parsing Technologies.Kenji Sagae and Alon Lavie.
2006.
Parser com-bination by reparsing.
In Proceedings of the Hu-man Language Technology Conference of the NorthAmerican Chapter of the ACL.Mihai Surdeanu and Christopher D. Manning.
2010.Ensemble models for dependency parsing: Cheapand good?
In Proceedings of the North Ameri-can Chapter of the Association for ComputationalLinguistics Conference (NAACL-2010), Los Ange-les, CA, June.Jun Suzuki, Hideki Isozaki, Xavier Carreras, andMichael Collins.
2009.
An empirical study of semi-supervised structured conditional models for depen-dency parsing.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 551?560, Singapore, August.
As-sociation for Computational Linguistics.654Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-reit.
2012.
Cross-lingual word clusters for directtransfer of linguistic structure.
In Proceedings ofthe 2012 Conference of the North American Chap-ter of the Association for Computational Linguis-tics: Human Language Technologies, pages 477?487, Montr?eal, Canada, June.
Association for Com-putational Linguistics.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statis-tical dependency analysis with support vector ma-chines.
In Proceedings of the 8th IWPT, pages 195?206.
Association for Computational Linguistics.Yue Zhang and Stephen Clark.
2008.
A tale of twoparsers: Investigating and combining graph-basedand transition-based dependency parsing.
In Pro-ceedings of the 2008 Conference on Empirical Meth-ods in Natural Language Processing, pages 562?571, Honolulu, Hawaii, October.
Association forComputational Linguistics.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 188?193, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.655
