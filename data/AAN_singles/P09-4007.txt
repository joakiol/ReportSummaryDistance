Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 25?28,Suntec, Singapore, 3 August 2009.c?2009 ACL and AFNLPDemonstration of Joshua: An Open Source Toolkitfor Parsing-based Machine Translation?Zhifei Li, Chris Callison-Burch, Chris Dyer?, Juri Ganitkevitch+, Sanjeev Khudanpur,Lane Schwartz?, Wren N. G. Thornton, Jonathan Weese, and Omar F. ZaidanCenter for Language and Speech Processing, Johns Hopkins University?
Computational Linguistics and Information Processing Lab, University of Maryland+ Human Language Technology and Pattern Recognition Group, RWTH Aachen University?
Natural Language Processing Lab, University of MinnesotaAbstractWe describe Joshua (Li et al, 2009a)1,an open source toolkit for statistical ma-chine translation.
Joshua implements allof the algorithms required for transla-tion via synchronous context free gram-mars (SCFGs): chart-parsing, n-gram lan-guage model integration, beam- and cube-pruning, and k-best extraction.
The toolkitalso implements suffix-array grammar ex-traction and minimum error rate training.It uses parallel and distributed computingtechniques for scalability.
We also pro-vide a demonstration outline for illustrat-ing the toolkit?s features to potential users,whether they be newcomers to the fieldor power users interested in extending thetoolkit.1 IntroductionLarge scale parsing-based statistical machinetranslation (e.g., Chiang (2007), Quirk et al(2005), Galley et al (2006), and Liu et al (2006))has made remarkable progress in the last fewyears.
However, most of the systems mentionedabove employ tailor-made, dedicated software thatis not open source.
This results in a high barrierto entry for other researchers, and makes experi-ments difficult to duplicate and compare.
In thispaper, we describe Joshua, a Java-based general-purpose open source toolkit for parsing-based ma-chine translation, serving the same role as Moses(Koehn et al, 2007) does for regular phrase-basedmachine translation.
?This research was supported in part by the Defense Ad-vanced Research Projects Agency?s GALE program underContract No.
HR0011-06-2-0001 and the National ScienceFoundation under grants No.
0713448 and 0840112.
Theviews and findings are the authors?
alone.1Please cite Li et al (2009a) if you use Joshua in yourresearch, and not this demonstration description paper.2 Joshua ToolkitWhen designing our toolkit, we applied generalprinciples of software engineering to achieve threemajor goals: Extensibility, end-to-end coherence,and scalability.Extensibility: Joshua?s codebase consists ofa separate Java package for each major aspectof functionality.
This way, researchers can focuson a single package of their choosing.
Fuur-thermore, extensible components are defined byJava interfaces to minimize unintended inter-actions and unseen dependencies, a common hin-drance to extensibility in large projects.
Wherethere is a clear point of departure for research,a basic implementation of each interface isprovided as an abstract class to minimizework necessary for extensions.End-to-end Cohesion: An MT pipeline con-sists of many diverse components, often designedby separate groups that have different file formatsand interaction requirements.
This leads to a largenumber of scripts for format conversion and tofacilitate interaction between the components, re-sulting in untenable and non-portable projects, andhindering repeatability of experiments.
Joshua, onthe other hand, integrates the critical componentsof an MT pipeline seamlessly.
Still, each compo-nent can be used as a stand-alone tool that does notrely on the rest of the toolkit.Scalability: Joshua, especially the decoder, isscalable to large models and data sets.
For ex-ample, the parsing and pruning algorithms are im-plemented with dynamic programming strategiesand efficient data structures.
We also utilize suffix-array grammar extraction, parallel/distributed de-coding, and bloom filter language models.Joshua offers state-of-the-art quality, havingbeen ranked 4th out of 16 systems in the French-English task of the 2009 WMT evaluation, both inautomatic (Table 1) and human evaluation.25System BLEU-4google 31.14lium 26.89dcu 26.86joshua 26.52uka 25.96limsi 25.51uedin 25.44rwth 24.89cmu-statxfer 23.65Table 1: BLEU scores for top primary systems onthe WMT-09 French-English Task from Callison-Burch et al (2009), who also provide human eval-uation results.2.1 Joshua Toolkit FeaturesHere is a short description of Joshua?s main fea-tures, described in more detail in Li et al (2009a):?
Training Corpus Sub-sampling: We sup-port inducing a grammar from a subsetof the training data, that consists of sen-tences needed to translate a particular testset.
To accomplish this, we make use of themethod proposed by Kishore Papineni (per-sonal communication), outlined in further de-tail in (Li et al, 2009a).
The method achievesa 90% reduction in training corpus size whilemaintaining state-of-the-art performance.?
Suffix-array Grammar Extraction: Gram-mars extracted from large training corporaare often far too large to fit into availablememory.
Instead, we follow Callison-Burchet al (2005) and Lopez (2007), and use asource language suffix array to extract onlyrules that will actually be used in translatinga particular test set.
Direct access to the suffixarray is incorporated into the decoder, allow-ing rule extraction to be performed for eachinput sentence individually, but it can also beexecuted as a standalone pre-processing step.?
Grammar formalism: Our decoder as-sumes a probabilistic synchronous context-free grammar (SCFG).
It handles SCFGsof the kind extracted by Hiero (Chiang,2007), but is easily extensible to more gen-eral SCFGs (as in Galley et al (2006)) andclosely related formalisms like synchronoustree substitution grammars (Eisner, 2003).?
Pruning: We incorporate beam- and cube-pruning (Chiang, 2007) to make decodingfeasible for large SCFGs.?
k-best extraction: Given a source sentence,the chart-parsing algorithm produces a hy-pergraph representing an exponential num-ber of derivation hypotheses.
We implementthe extraction algorithm of Huang and Chi-ang (2005) to extract the k most likely deriva-tions from the hypergraph.?
Oracle Extraction: Even within the largeset of translations represented by a hyper-graph, some desired translations (e.g.
the ref-erences) may not be contained due to pruningor inherent modeling deficiency.
We imple-ment an efficient dynamic programming al-gorithm (Li and Khudanpur, 2009) for find-ing the oracle translations, which are mostsimilar to the desired translations, as mea-sured by a metric such as BLEU.?
Parallel and distributed decoding: Wesupport parallel decoding and a distributedlanguage model that exploit multi-core andmulti-processor architectures and distributedcomputing (Li and Khudanpur, 2008).?
Language Models: We implement three lo-cal n-gram language models: a straightfor-ward implementation of the n-gram scoringfunction in Java, capable of reading stan-dard ARPA backoff n-gram models; a na-tive code bridge that allows the decoder touse the SRILM toolkit to read and score n-grams2; and finally a Bloom Filter implemen-tation following Talbot and Osborne (2007).?
Minimum Error Rate Training: Joshua?sMERT module optimizes parameter weightsso as to maximize performance on a develop-ment set as measured by an automatic evalu-ation metric, such as BLEU.
The optimizationconsists of a series of line-optimizations us-ing the efficient method of Och (2003).
Moredetails on the MERT method and the imple-mentation can be found in Zaidan (2009).32The first implementation allows users to easily try theJoshua toolkit without installing SRILM.
However, usersshould note that the basic Java LM implementation is not asscalable as the SRILM native bridge code.3The module is also available as a standalone applica-tion, Z-MERT, that can be used with other MT systems.26?
Variational Decoding: spurious ambiguitycauses the probability of an output stringamong to be split among many derivations.The goodness of a string is measured bythe total probability of its derivations, whichmeans that finding the best output string iscomputationally intractable.
The standardViterbi approximation is based on the mostprobable derivation, but we also implementa variational approximation, which considersall the derivations but still allows tractabledecoding (Li et al, 2009b).3 Demonstration OutlineThe purpose of the demonstration is 4-fold: 1) togive newcomers to the field of statistical machinetranslation an idea of the state-of-the-art; 2) toshow actual, live, end-to-end operation of the sys-tem, highlighting its main components, targetingpotential users; 3) to illustrate, through visual aids,the underlying algorithms, for those interested inthe technical details; and 4) to explain how thosecomponents can be extended, for potential powerusers who want to be familiar with the code itself.The first component of the demonstration willbe an interactive user interface, where arbitraryuser input in a source language is entered into aweb form and then translated into a target lan-guage by the system.
This component specificallytargets newcomers to SMT, and demonstrates thecurrent state of the art in the field.
We will havetrained multiple systems (for multiple languagepairs), hosted on a remote server, which will bequeried with the sample source sentences.Potential users of the system would be inter-ested in seeing an actual operation of the system,in a similar fashion to what they would observeon their own machines when using the toolkit.
Forthis purpose, we will demonstrate three main mod-ules of the toolkit: the rule extraction module, theMERT module, and the decoding module.
Eachmodule will have a separate terminal window ex-ecuting it, hence demonstrating both the module?sexpected output as well as its speed of operation.In addition to demonstrating the functionalityof each module, we will also provide accompa-nying visual aids that illustrate the underlying al-gorithms and the technical operational details.
Wewill provide visualization of the search graph and(Software and documentation at: http://cs.jhu.edu/?ozaidan/zmert.
)the 1-best derivation, which would illustrate thefunctionality of the decoder, as well as alterna-tive translations for phrases of the source sentence,and where they were learned in the parallel cor-pus, illustrating the functionality of the grammarrule extraction.
For the MERT module, we willprovide figures that illustrate Och?s efficient linesearch method.4 Demonstration RequirementsThe different components of the demonstrationwill be spread across at most 3 machines (Fig-ure 1): one for the live ?instant translation?
userinterface, one for demonstrating the different com-ponents of the system and algorithmic visualiza-tions, and one designated for technical discussionof the code.
We will provide the machines our-selves and ensure the proper software is installedand configured.
However, we are requesting thatlarge LCD monitors be made available, if possi-ble, since that would allow more space to demon-strate the different components with clarity thanour laptop displays would provide.
We will alsorequire Internet connectivity for the live demon-stration, in order to gain access to remote serverswhere trained models will be hosted.ReferencesChris Callison-Burch, Colin Bannard, and JoshSchroeder.
2005.
Scaling phrase-based statisti-cal machine translation to larger corpora and longerphrases.
In Proceedings of ACL.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
InProceedings of the Fourth Workshop on StatisticalMachine Translation, pages 1?28, Athens, Greece,March.
Association for Computational Linguistics.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Jason Eisner.
2003.
Learning non-isomorphic treemappings for machine translation.
In Proceedingsof ACL.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Pro-ceedings of the ACL/Coling.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of the International Work-shop on Parsing Technologies.27We will rely on 3 workstations:one for the instant translationdemo, where arbitrary input istranslated from/to a language pairof choice (top); one for runtimedemonstration of the system, witha terminal window for each of thethree main components of thesystems, as well as visual aids,such as derivation trees (left); andone (not shown) designated fortechnical discussion of the code.Remote serverhosting trainedtranslation modelsJHUGrammar extractionDecoderMERTFigure 1: Proposed setup of our demonstration.
When this paper is viewed as a PDF, the reader mayzoom in further to see more details.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the ACL-2007 Demo and Poster Ses-sions.Zhifei Li and Sanjeev Khudanpur.
2008.
A scalabledecoder for parsing-based machine translation withequivalent language model state maintenance.
InProceedings Workshop on Syntax and Structure inStatistical Translation.Zhifei Li and Sanjeev Khudanpur.
2009.
Efficientextraction of oracle-best translations from hyper-graphs.
In Proceedings of NAACL.Zhifei Li, Chris Callison-Burch, Chris Dyer, JuriGanitkevitch, Sanjeev Khudanpur, Lane Schwartz,Wren Thornton, Jonathan Weese, and Omar Zaidan.2009a.
Joshua: An open source toolkit for parsing-based machine translation.
In Proceedings of theFourth Workshop on Statistical Machine Transla-tion, pages 135?139, Athens, Greece, March.
As-sociation for Computational Linguistics.Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.2009b.
Variational decoding for statistical machinetranslation.
In Proceedings of ACL.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment templates for statistical machinetranslation.
In Proceedings of the ACL/Coling.Adam Lopez.
2007.
Hierarchical phrase-based trans-lation with suffix arrays.
In Proceedings of EMNLP-CoLing.Franz Josef Och.
2003.
Minimum error rate trainingfor statistical machine translation.
In Proceedingsof ACL.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency treelet translation: Syntactically in-formed phrasal smt.
In Proceedings of ACL.David Talbot and Miles Osborne.
2007.
Randomisedlanguage modelling for statistical machine transla-tion.
In Proceedings of ACL.Omar F. Zaidan.
2009.
Z-MERT: A fully configurableopen source tool for minimum error rate training ofmachine translation systems.
The Prague Bulletin ofMathematical Linguistics, 91:79?88.28
