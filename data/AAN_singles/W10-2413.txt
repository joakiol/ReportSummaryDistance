Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 81?84,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsPhrase-based Transliteration System with Simple HeuristicsAvinesh PVS and Ankur ParikhIIIT HyderabadLanguage Technologies Research Centre{avinesh,shaileshkumar.parikh}@students.iiit.ac.inAbstractThis paper presents modeling of translit-eration as a phrase-based machine transla-tion system.
We used a popular phrase-based machine translation system forEnglish-Hindi machine transliteration.
Wehave achieved an accuracy of 38.1% on thetest set.
We used some basic rules to mod-ulate the existing phrased-based transliter-ation system.
Our experiments show thatphrase-based machine translation systemscan be adopted by modulating the systemto fit the transliteration problem.1 IntroductionTransliteration is the practice of converting a textfrom one writing system into another in a system-atic way.
Most significantly it is used in MachineTranslation (MT) systems, Information Retrievalsystems where a large portion of unknown words(out of vocabulary) are observed.
Named enti-ties (NE), technical words, borrowed words andloan words constitute the majority of the unknownwords.
So, transliteration can also be termed asthe process of obtaining the phonetic translationof names across various languages (Shishtla et al,2009).
Transcribing the words from one languageto another without the help of bilingual dictionaryis a challenging task.Previous work in transliteration include(Surana and Singh, 2009) who propose a translit-eration system using two different approachesof transliterating the named entities based ontheir origin.
(Sherif and Kondrak, 2007) usethe Viterbi based monotone search algorithm forsearching possible candidate sub-string translit-erations.
(Malik, 2006) solved some specialcases of transliteration for Punjabi using a set oftransliteration rules.In the recent years Statistical Machine Trans-lation (SMT) systems (Brown et al, 1990), (Ya-mada and Knight, 2001), (Chiang, 2005), (Char-niak et al, 2003) have been in focus.
It is easyto develop a MT system for a new pair of lan-guage using an existing SMT system and a par-allel corpora.
It isn?t a surprise to see SMT beingattractive in terms of less human labour as com-pared to other traditional systems.
These SMTsystems have also become popular in the transliter-ation field (Finch and Sumita, 2008), (Finch andSumita, 2009), (Rama and Gali, 2009).
(Finchand Sumita, 2008) use a bi-directional decoderwhereas (Finch and Sumita, 2009) use a machinetranslation system comprising of two phrase-baseddecoders.
The first decoder generated from firsttoken of the target to the last.
The second decodergenerated the target from last to first.
(Rama andGali, 2009) modeled the phrase-based SMT sys-tem using minimum error rate training (MERT) forlearning model weights.In this paper we present a phrase-based ma-chine transliteration technique with simple heuris-tics for transliterating named entities of English-Hindi pair using small amount of training and de-velopment data.
The structure of our paper is asfollows.
Section 2 describes the modeling of trans-lation problem to transliteration.
Modeling of theparameters and the heuristics are presented in Sec-tion 3.
Section 4 and 5 we give a brief descriptionabout the data-set and error-analysis.
Finally weconclude in Section 6.2 Modeling ApproachTransliteration can be viewed as a task ofcharacter-level machine translation process.
Boththe problems involve transformation of source to-kens in one language to target tokens in anotherlanguage.Transliteration differs from machine translation intwo ways (Finch and Sumita, 2009):1.
Reordering of the target tokens is generally81h a n m A nu ah a n u m a nHANUMANhanumAnahanumanhanumAnaInput Lowercase After Giza Alignmentshhaannuum aAnn amPost?ProcessingFigure 1: English-Hindi transliteration example through our system(To represent Hindi font roman scriptis used)abscent in transliteration.2.
Number of token types (vocabulary) in thedata is relatively very less and finite as com-pared to the translation data.The work in this paper is related to the work of(Rama and Gali, 2009) who also use SMT directlyto transliterate.
We can model the translationproblem to transliteration problem by replacingwords with characters.
So instead of sentenceslet us assume a given word is represented as asequence of characters of the source languageF=f1,f2,f3,...fn which needs to be transcribed asa sequence of characters in the target languageE=e1,e2,e3,...em.
1The best possible target language sequence ofcharacters among the possible candidate charac-ters can be represented as:Ebest = ArgmaxE P(E|F)The above equation can be represented in termsof noisy channel model using Bayes Rule:Ebest = ArgmaxE P(F|E) ?
P(E)Here P(F|E) represents the transcription modelwhere as P(E) represents the language model i.ethe character n-gram of the target language.
Theabove equation returns the best possible outputsequence of characters for the given sequence ofcharacters F.We used some heuristics on top of Moses toolkit, which is a publicly available tool provided by(Hoang et al, 2007).1F,E is used to name source and target language sequencesas used in conventional machine translation notations3 Method3.1 Pre-processingFirstly the data on the English side is converted tolowercase to reduce data sparsity.
Each characterof the words in the training and development dataare separated with spaces.
We also came acrossmulti-word sequences which posed a challenge forour approach.
We segmented the multi-words intoseparate words, such that they would be transliter-ated as different words.3.2 Alignment and Post ProcessingParallel word lists are given to GIZA++ for char-acter alignments.
We observed grow-diag-final-and as the best alignment heuristic.
From thedifferences mentioned above between translitera-tion and translation we came up with some simpleheuristics to do post processing on the GIZA++alignments.1.
As reordering of the target tokens is not al-lowed in transliteration.
Crossing of the arcsduring the alignments are removed.As shown in Fig 1. above.The second A ?
a is removed as it was cross-ing the arcs.2.
If the target character is aligned to NULLcharacter on the source side then the NULLis removed, and the target language characteris aligned to the source character aligned toprevious target character.From Fig 1.n ?
nNULL ?
ato82n ?
na3.3 Training and Parameter TuningThe language models and translation models werebuilt on the combined training and the develop-ment data.
But the learning of log-linear weightsduring the MERT step is done using developmentdata separately.
It is obvious that the system wouldperform better if it was trained on the combineddata.
8-gram language model and a maximumphrase length of 7 is used during training.The transliteration systems were modeled usingthe minimum error rate training procedure intro-duced by (Och, 2003).
We used BLUE score as aevaluation metric for our convenience during tun-ing.
BLUE score is commonly used to evaluatemachine translation systems and it is a function ofgeometric mean of n-gram precision.
It was ob-served that improvement of the BLUE score alsoshowed improvements in ACC.4 Experiments and ResultsTraining data of 9975 words is used to buildthe system models, while the development dataof 1974 words is used for tuning the log-linearweights for the translation engines.
Our accuracieson test-data are reported in Table 1.
Due to timeconstraints we couldn?t focus on multiple correctanswers in the training data, we picked just thefirst one for our training.
Some of the translationfeatures like word penalty, phrase penalty, reorderparameters don?t play any role in transliterationprocess hence we didn?t include them.Before the release of the test-data we tested thesystem without tuning i.e.
default weights wereused on the development data.
Later once the test-data was released the system was tuned on the de-velopment data to model the weights.
We evalu-ated our system on ACC which accounts for WordAccuracy for top-1, Mean F-score, Mean Recipro-cal Rank (MRR).Table 1: Evaluation on Test DataMeasure ResultACC 0.381Mean F-score 0.860MRR 0.403MAPref 0.3815 Error AnalysisFrom the reference corpora we examined that ma-jority of the errors were due to foreign originwords.
As the phonetic transcription of thesewords is different from the other words.
We alsoobserved from error analysis that the correct tar-get sequence of characters were occurring at lowerrank in the 20-best list.
We would like to see howdifferent ranking mechanisms like SVM re-ranketc would help in boosting the correct accuraciesof the system.6 ConclusionIn this paper we show that the usage of someheuristics on top of popular phrase-based machinetranslation works well for the task of translit-eration.
First the source and target charactersare aligned using GIZA++.
Then some heuris-tics are used to modify the alignments.
Thesemodified alignments are used during estimationof the weights during minimum error rate train-ing (MERT).
Finally the Hindi characters are de-coded using the beam-search based decoder.
Wealso produced the 20-best outputs using the n-bestlist provided by moses toolkit.
It is very interestingto see how simple heuristics helped in performingbetter than other systems.ReferencesPeter F. Brown, John Cocke, Stephen A. Della Pietra,Vincent J. Della Pietra, Fredrick Jelinek, John D.Lafferty, Robert L. Mercer, and Paul S. Roossin.1990.
A statistical approach to machine translation.COMPUTATIONAL LINGUISTICS, 16(2):79?85.Eugene Charniak, Kevin Knight, and Kenji Yamada.2003.
Syntax-based language models for statisticalmachine translation.
In MT Summit IX.
Intl.
Assoc.for Machine Translation.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In In ACL,pages 263?270.Andrew Finch and Eiichiro Sumita.
2008.
Phrase-based machine transliteration.
In In Proc.
3rd Int?l.Joint Conf NLP, volume 1.Andrew Finch and Eiichiro Sumita.
2009.
Translit-eration by bidirectional statistical machine transla-tion.
In NEWS ?09: Proceedings of the 2009 NamedEntities Workshop: Shared Task on Transliteration,pages 52?56, Morristown, NJ, USA.
Association forComputational Linguistics.83Hieu Hoang, Alexandra Birch, Chris Callison-burch,Richard Zens, Rwth Aachen, Alexandra Constantin,Marcello Federico, Nicola Bertoldi, Chris Dyer,Brooke Cowan, Wade Shen, Christine Moran, andOndej Bojar.
2007.
Moses: Open source toolkit forstatistical machine translation.
pages 177?180.M.
G. Abbas Malik.
2006.
Punjabi machine translit-eration.
In ACL-44: Proceedings of the 21st Inter-national Conference on Computational Linguisticsand the 44th annual meeting of the Association forComputational Linguistics, pages 1137?1144, Mor-ristown, NJ, USA.
Association for ComputationalLinguistics.Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In ACL ?03:Proceedings of the 41st Annual Meeting on Asso-ciation for Computational Linguistics, pages 160?167, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Taraka Rama and Karthik Gali.
2009.
Modeling ma-chine transliteration as a phrase based statistical ma-chine translation problem.
In NEWS ?09: Proceed-ings of the 2009 Named Entities Workshop: SharedTask on Transliteration, pages 124?127, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Tarek Sherif and Grzegorz Kondrak.
2007.
Substring-based transliteration.
In Proceedings of the 45th An-nual Meeting of the Association of ComputationalLinguistics, pages 944?951, Prague, Czech Repub-lic, June.
Association for Computational Linguis-tics.Praneeth Shishtla, V. Surya Ganesh, SethuramalingamSubramaniam, and Vasudeva Varma.
2009.
Alanguage-independent transliteration schema usingcharacter aligned models at news 2009.
In NEWS?09: Proceedings of the 2009 Named Entities Work-shop: Shared Task on Transliteration, pages 40?43, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Harshit Surana and Anil Kumar Singh.
2009.
Digitiz-ing The Legacy of Indian Languages.
ICFAI Books,Hyderabad.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
pages 523?530.84
