Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 167?174,Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005Novel Reordering Approaches in Phrase-Based Statistical MachineTranslationStephan Kanthak, David Vilar, Evgeny Matusov, Richard Zens, and Hermann NeyThe authors are with the Lehrstuhl fu?r Informatik VI,Computer Science Department, RWTH Aachen University,D-52056 Aachen, Germany.E-mail: {kanthak,vilar,matusov,zens,ney}@informatik.rwth-aachen.de.AbstractThis paper presents novel approaches toreordering in phrase-based statistical ma-chine translation.
We perform consistentreordering of source sentences in train-ing and estimate a statistical translationmodel.
Using this model, we follow aphrase-based monotonic machine transla-tion approach, for which we develop an ef-ficient and flexible reordering frameworkthat allows to easily introduce different re-ordering constraints.
In translation, weapply source sentence reordering on wordlevel and use a reordering automaton as in-put.
We show how to compute reorderingautomata on-demand using IBM or ITGconstraints, and also introduce two newtypes of reordering constraints.
We furtheradd weights to the reordering automata.We present detailed experimental resultsand show that reordering significantly im-proves translation quality.1 IntroductionReordering is of crucial importance for machinetranslation.
Already (Knight et al, 1998) use full un-weighted permutations on the level of source wordsin their early weighted finite-state transducer ap-proach which implemented single-word based trans-lation using conditional probabilities.
In a refine-ment with additional phrase-based models, (Kumaret al, 2003) define a probability distribution overall possible permutations of source sentence phrasesand prune the resulting automaton to reduce com-plexity.A second category of finite-state translation ap-proaches uses joint instead of conditional probabili-ties.
Many joint probability approaches originate inspeech-to-speech translation as they are the naturalchoice in combination with speech recognition mod-els.
The automated transducer inference techniquesOMEGA (Vilar, 2000) and GIATI (Casacuberta etal., 2004) work on phrase level, but ignore the re-ordering problem from the view of the model.
With-out reordering both in training and during search,sentences can only be translated properly into a lan-guage with similar word order.
In (Bangalore et al,2000) weighted reordering has been applied to tar-get sentences since defining a permutation model onthe source side is impractical in combination withspeech recognition.
In order to reduce the computa-tional complexity, this approach considers only a setof plausible reorderings seen on training data.Most other phrase-based statistical approacheslike the Alignment Template system of Benderet al (2004) rely on (local) reorderings which areimplicitly memorized with each pair of source andtarget phrases in training.
Additional reorderings onphrase level are fully integrated into the decodingprocess, which increases the complexity of the sys-tem and makes it hard to modify.
Zens et al (2003)reviewed two types of reordering constraints for thistype of translation systems.In our work we follow a phrase-based transla-tion approach, applying source sentence reorderingon word level.
We compute a reordering graph on-demand and take it as input for monotonic trans-lation.
This approach is modular and allows easyintroduction of different reordering constraints andprobabilistic dependencies.
We will show that it per-forms at least as well as the best statistical machinetranslation system at the IWSLT Evaluation.167In the next section we briefly review the basictheory of our translation system based on weightedfinite-state transducers (WFST).
In Sec.
3 we in-troduce new methods for reordering and alignmentmonotonization in training.
To compare differ-ent reordering constraints used in the translationsearch process we develop an on-demand com-putable framework for permutation models in Sec.
4.In the same section we also define and analyze un-restricted and restricted permutations with some ofthem being first published in this paper.
We con-clude the paper by presenting and discussing a richset of experimental results.2 Machine Translation using WFSTsLet fJ1 and eIi be two sentences from a source andtarget language.
Assume that we have word levelalignments A of all sentence pairs from a bilingualtraining corpus.
We denote with e?J1 the segmenta-tion of a target sentence eI1 into J phrases such thatfJ1 and e?J1 can be aligned to form bilingual tuples(fj , e?j).
If alignments are only functions of targetwords A?
: {1, .
.
.
, I} ?
{1, .
.
.
, J}, the bilingualtuples (fj , e?j) can be inferred with e. g. the GIATImethod of (Casacuberta et al, 2004), or with ournovel monotonization technique (see Sec.
3).
Eachsource word will be mapped to a target phrase of oneor more words or an ?empty?
phrase ?.
In particular,the source words which will remain non-aligned dueto the alignment functionality restriction are pairedwith the empty phrase.We can then formulate the problem of finding thebest translation e?I1 of a source sentence fJ1 :e?I1 = argmaxeI1Pr(fJ1 , eI1)= argmaxe?J1?A?APr(fJ1 , e?J1 , A)?= argmaxe?J1maxA?APr(A) ?
Pr(fJ1 , e?J1 |A)?= argmaxe?J1maxA?A?fj :j=1...JPr(fj , e?j |fj?11 , e?j?11 , A)= argmaxe?J1maxA?A?fj :j=1...Jp(fj , e?j |fj?1j?m, e?j?1j?m, A)In other words: if we assume a uniform distri-bution for Pr(A), the translation problem can bemapped to the problem of estimating an m-gram lan-guage model over a learned set of bilingual tuples(fj , e?j).
Mapping the bilingual language model to aWFST T is canonical and it has been shown in (Kan-thak et al, 2004) that the search problem can then berewritten using finite-state terminology:e?I1 = project-output(best(fJ1 ?
T )) .This implementation of the problem as WFSTs maybe used to efficiently solve the search problem inmachine translation.3 Reordering in TrainingWhen the alignment function A?
is not monotonic,target language phrases e?
can become very long.For example in a completely non-monotonic align-ment all target words are paired with the last alignedsource word, whereas all other source words formtuples with the empty phrase.
Therefore, for lan-guage pairs with big differences in word order, prob-ability estimates may be poor.This problem can be solved by reordering eithersource or target training sentences such that align-ments become monotonic for all sentences.
Wesuggest the following consistent source sentence re-ordering and alignment monotonization approach inwhich we compute optimal, minimum-cost align-ments.First, we estimate a cost matrix C for each sen-tence pair (fJ1 , eI1).
The elements of this matrix cijare the local costs of aligning a source word fj to atarget word ei.
Following (Matusov et al, 2004), wecompute these local costs by interpolating state oc-cupation probabilities from the source-to-target andtarget-to-source training of the HMM and IBM-4models as trained by the GIZA++ toolkit (Och et al,2003).
For a given alignment A ?
I ?
J , we definethe costs of this alignment c(A) as the sum of thelocal costs of all aligned word pairs:c(A) =?
(i,j)?Acij (1)The goal is to find an alignment with the minimumcosts which fulfills certain constraints.3.1 Source Sentence ReorderingTo reorder a source sentence, we require thealignment to be a function of source words A1:{1, .
.
.
, J} ?
{1, .
.
.
, I}, easily computed from thecost matrix C as:A1(j) = argmini cij (2)168We do not allow for non-aligned source words.
A1naturally defines a new order of the source words fJ1which we denote by f?J1 .
By computing this permu-tation for each pair of sentences in training and ap-plying it to each source sentence, we create a corpusof reordered sentences.3.2 Alignment MonotonizationIn order to create a ?sentence?
of bilingual tuples(f?J1 , e?J1 ) we required alignments between reorderedsource and target words to be a function of targetwords A2 : {1, .
.
.
, I} ?
{1, .
.
.
, J}.
This align-ment can be computed in analogy to Eq.
2 as:A2(i) = argminj c?ij (3)where c?ij are the elements of the new cost matrixC?
which corresponds to the reordered source sen-tence.
We can optionally re-estimate this matrix byrepeating EM training of state occupation probabili-ties with GIZA++ using the reordered source corpusand the original target corpus.
Alternatively, we canget the cost matrix C?
by reordering the columns ofthe cost matrix C according to the permutation givenby alignment A1.In alignment A2 some target words that were pre-viously unaligned in A1 (like ?the?
in Fig.
1) maynow still violate the alignment monotonicity.
Themonotonicity of this alignment can not be guaran-teed for all words if re-estimation of the cost matri-ces had been performed using GIZA++.The general GIATI technique (Casacuberta et al,2004) is applicable and can be used to monotonizethe alignment A2.
However, in our experimentsthe following method performs better.
We makeuse of the cost matrix representation and computea monotonic minimum-cost alignment with a dy-namic programming algorithm similar to the Lev-enshtein string edit distance algorithm.
As costs ofeach ?edit?
operation we consider the local align-ment costs.
The resulting alignment A3 representsa minimum-cost monotonic ?path?
through the costmatrix.
To make A3 a function of target words wedo not consider the source words non-aligned in A2and also forbid ?deletions?
(?many-to-one?
sourceword alignments) in the DP search.An example of such consistent reordering andmonotonization is given in Fig.
1.
Here, we re-order the German source sentence based on the ini-tial alignment A1, then compute the function of tar-get words A2, and monotonize this alignment to A3the very beginning of May would suit me .the very beginning of May would suit me .sehr gut Anfang Mai w?rde passen mir .sehr gut Anfang Mai w?rde passen mir .the very beginning of May would suit me .mir sehrw?rde gut Anfang Mai passen ..Mai|of_May w?rde|would passen|suit mir|me |.sehr|the_very gut|$ Anfang|beginningAAA123Figure 1: Example of alignment, source sentence re-ordering, monotonization, and construction of bilin-gual tuples.with the dynamic programming algorithm.
Fig.
1also shows the resulting bilingual tuples (f?j , e?j).4 Reordering in SearchWhen searching the best translation e?J1 for a givensource sentence fJ1 , we permute the source sentenceas described in (Knight et al, 1998):e?I1 = project-output(best(permute(fJ1 ) ?
T ))Permuting an input sequence of J symbols re-sults in J !
possible permutations and representingthe permutations as a finite-state automaton requiresat least 2J states.
Therefore, we opt for computingthe permutation automaton on-demand while apply-ing beam pruning in the search.4.1 Lazy Permutation AutomataFor on-demand computation of an automaton in theflavor described in (Kanthak et al, 2004) it is suffi-cient to specify a state description and an algorithmthat calculates all outgoing arcs of a state from thestate description.
In our case, each state representsa permutation of a subset of the source words fJ1 ,which are already translated.This can be described by a bit vector bJ1 (Zenset al, 2002).
Each bit of the state bit vector corre-sponds to an arc of the linear input automaton and isset to one if the arc has been used on any path fromthe initial to the current state.
The bit vectors of twostates connected by an arc differ only in a single bit.Note that bit vectors elegantly solve the problem ofrecombining paths in the automaton as states with169the same bit vectors can be merged.
As a result, afully minimized permutation automaton has only asingle initial and final state.Even with on-demand computation, complexityusing full permutations is unmanagable for long sen-tences.
We further reduce complexity by addition-ally constraining permutations.
Refer to Figure 2 forvisualizations of the permutation constraints whichwe describe in the following.4.2 IBM ConstraintsThe IBM reordering constraints are well-known inthe field of machine translation and were first de-scribed in (Berger et al, 1996).
The idea behindthese constraints is to deviate from monotonic trans-lation by postponing translations of a limited num-ber of words.
More specifically, at each state wecan translate any of the first l yet uncovered wordpositions.
The implementation using a bit vector isstraightforward.
For consistency, we associate win-dow size with the parameter l for all constraints pre-sented here.4.3 Inverse IBM ConstraintsThe original IBM constraints are useful for a largenumber of language pairs where the ability to skipsome words reflects the differences in word orderbetween the two languages.
For some other pairs,it is beneficial to translate some words at the end ofthe sentence first and to translate the rest of the sen-tence nearly monotonically.
Following this idea wecan define the inverse IBM constraints.
Let j be thefirst uncovered position.
We can choose any posi-tion for translation, unless l ?
1 words on positionsj?
> j have been translated.
If this is the case wemust translate the word in position j.
The inverseIBM constraints can also be expressed byinvIBM(x) = transpose(IBM(transpose(x))) .As the transpose operation can not be computedon-demand, our specialized implementation uses bitvectors bJ1 similar to the IBM constraints.4.4 Local ConstraintsFor some language pairs, e.g.
Italian ?
English,words are moved only a few words to the left orright.
The IBM constraints provide too many alter-native permutations to chose from as each word canbe moved to the end of the sentence.
A solution thatallows only for local permutations and therefore hasa)0000 10001 11002 11103 11114b)00001000101002 110021010310110311103110141011141111413210114 2c)000010001010020010300014 1001410103 11002111110121111311102 443d)00001000101002 110021010311110311014 1111432Figure 2: Permutations of a) positions j = 1, 2, 3, 4of a source sentence f1f2f3f4 using a window sizeof 2 for b) IBM constraints, c) inverse IBM con-straints and d) local constraints.very low complexity is given by the following per-mutation rule: the next word for translation comesfrom the window of l positions1 counting from thefirst yet uncovered position.
Note, that the local con-straints define a true subset of the permutations de-fined by the IBM constraints.4.5 ITG ConstraintsAnother type of reordering can be obtained using In-version Transduction Grammars (ITG) (Wu, 1997).These constraints are inspired by bilingual bracket-ing.
They proved to be quite useful for machinetranslation, e.g.
see (Bender et al, 2004).
Here,we interpret the input sentence as a sequence of seg-ments.
In the beginning, each word is a segment ofits own.
Longer segments are constructed by recur-sively combining two adjacent segments.
At each1both covered and uncovered170Chinese English Japanese English Italian Englishtrain sentences 20 000 20 000 66107words 182 904 160 523 209 012 160 427 410 275 427 402singletons 3 525 2 948 4 108 2 956 6 386 3 974vocabulary 7 643 6 982 9 277 6 932 15 983 10 971dev sentences 506 506 500words 3 515 3 595 4 374 3 595 3 155 3 253sentence length (avg/max) 6.95 / 24 7.01 / 29 8.64 / 30 7.01 / 29 5.79 / 24 6.51 / 25test sentences 500 500 506words 3 794 ?
4 370 ?
2 931 3 595sentence length (avg/max) 7.59 / 62 7.16 / 71 8.74 / 75 7.16 / 71 6.31 / 27 6.84 / 28Table 1: Statistics of the Basic Travel Expression (BTEC) corpora.combination step, we either keep the two segmentsin monotonic order or invert the order.
This pro-cess continues until only one segment for the wholesentence remains.
The on-demand computation isimplemented in spirit of Earley parsing.We can modify the original ITG constraints tofurther limit the number of reorderings by forbid-ding segment inversions which violate IBM con-straints with a certain window size.
Thus, the re-sulting reordering graph contains the intersection ofthe reorderings with IBM and the original ITG con-straints.4.6 Weighted PermutationsSo far, we have discussed how to generate the per-mutation graphs under different constraints, but per-mutations were equally probable.
Especially for thecase of nearly monotonic translation it is make senseto restrict the degree of non-monotonicity that weallow when translating a sentence.
We propose asimple approach which gives a higher probabilityto the monotone transitions and penalizes the non-monotonic ones.A state description bJ1 , for which the followingcondition holds:Mon(j) : bj?
= ?(j?
?
j) ?
1 ?
j?
?
Jrepresents the monotonic path up to the word fj .
Ateach state we assign the probability ?
to that out-going arc where the target state description fullfillsMon(j+1) and distribute the remaining probabilitymass 1?
?
uniformly among the remaining arcs.
Incase there is no such arc, all outgoing arcs get thesame uniform probability.
This weighting schemeclearly depends on the state description and the out-going arcs only and can be computed on-demand.5 Experimental Results5.1 Corpus StatisticsThe translation experiments were carried out on theBasic Travel Expression Corpus (BTEC), a multilin-gual speech corpus which contains tourism-relatedsentences usually found in travel phrase books.We tested our system on the so called Chinese-to-English (CE) and Japanese-to-English (JE) SuppliedTasks, the corpora which were provided during theInternational Workshop on Spoken Language Trans-lation (IWSLT 2004) (Akiba et al, 2004).
In ad-dition, we performed experiments on the Italian-to-English (IE) task, for which a larger corpus waskindly provided to us by ITC/IRST.
The corpusstatistics for the three BTEC corpora are given inTab.
1.
The development corpus for the Italian-to-English translation had only one reference transla-tion of each Italian sentence.
A set of 506 sourcesentences and 16 reference translations is used asa development corpus for Chinese-to-English andJapanese-to-English and as a test corpus for Italian-to-English tasks.
The 500 sentence Chinese andJapanese test sets of the IWSLT 2004 evaluationcampaign were translated and automatically scoredagainst 16 reference translations after the end of thecampaign using the IWSLT evaluation server.5.2 Evaluation CriteriaFor the automatic evaluation, we used the crite-ria from the IWSLT evaluation campaign (Akiba etal., 2004), namely word error rate (WER), position-independent word error rate (PER), and the BLEUand NIST scores (Papineni et al, 2002; Doddington,2002).
The two scores measure accuracy, i. e. largerscores are better.
The error rates and scores werecomputed with respect to multiple reference transla-17140424446485052545658601  2  3  4  5  6  7  8  9reordering constraints window sizeINV-IBMIBMITGLOCAL464748495051525354551  2  3  4  5  6  7  8  9reordering constraints window sizeINV-IBMIBMITGLOCALFigure 3: Word error rate [%] as a function of the reordering window size for different reordering constraints:Japanese-to-English (left) and Chinese-to-English (right) translation.tions, when they were available.
To indicate this, wewill label the error rate acronyms with an m. Bothtraining and evaluation were performed using cor-pora and references in lowercase and without punc-tuation marks.5.3 ExperimentsWe used reordering and alignment monotonizationin training as described in Sec.
3.
To estimate thematrices of local alignment costs for the sentencepairs in the training corpus we used the state occupa-tion probabilities of GIZA++ IBM-4 model trainingand interpolated the probabilities of source-to-targetand target-to-source training directions.
After thatwe estimated a smoothed 4-gram language model onthe level of bilingual tuples fj , e?j and represented itas a finite-state transducer.When translating, we applied moderate beampruning to the search automaton only when using re-ordering constraints with window sizes larger than 3.For very large window sizes we also varied the prun-ing thresholds depending on the length of the inputsentence.
Pruning allowed for fast translations andreasonable memory consumption without a signifi-cant negative impact on performance.In our first experiments, we tested the four re-ordering constraints with various window sizes.
Weaimed at improving the translation results on the de-velopment corpora and compared the results withtwo baselines: reordering only the source trainingsentences and translation of the unreordered test sen-tences; and the GIATI technique for creating bilin-gual tuples (fj , e?j) without reordering of the sourcesentences, neither in training nor during translation.5.3.1 Highly Non-Monotonic Translation (JE)Fig.
3 (left) shows word error rate on theJapanese-to-English task as a function of the win-dow size for different reordering constraints.
Foreach of the constraints, good results are achievedusing a window size of 9 and larger.
This can beattributed to the Japanese word order which is verydifferent from English and often follows a subject-object-verb structure.
For small window sizes, ITGor IBM constraints are better suited for this task, forlarger window sizes, inverse IBM constraints per-form best.
The local constraints perform worst andrequire very large window sizes to capture the mainword order differences between Japanese and En-glish.
However, their computational complexity islow; for instance, a system with local constraintsand window size of 9 is as fast (25 words per sec-ond) as the same system with IBM constraints andwindow size of 5.
Using window sizes larger than10 is computationally expensive and does not sig-nificantly improve the translation quality under anyof the constraints.Tab.
2 presents the overall improvements in trans-lation quality when using the best setting: inverseIBM constraints, window size 9.
The baseline with-out reordering in training and testing failed com-pletely for this task, producing empty translationsfor 37 % of the sentences2.
Most of the originalalignments in training were non-monotonic whichresulted in mapping of almost all Japanese words to?
when using only the GIATI monotonization tech-nique.
Thus, the proposed reordering methods are ofcrucial importance for this task.2Hence a NIST score of 0 due to the brevity penalty.172mWER mPER BLEU NISTReordering: [%] [%] [%]BTEC Japanese-to-English (JE) devnone 59.7 58.8 13.0 0.00in training 57.8 39.4 14.7 3.27+ 9-inv-ibm 40.3 32.1 45.1 8.59+ rescoring* 39.1 30.9 53.2 9.93BTEC Chinese-to-English (CE) devnone 55.2 52.1 24.9 1.34in training 54.0 42.3 23.0 4.18+ 7-inv-ibm 47.1 39.4 34.5 6.53+ rescoring* 48.3 40.7 39.1 8.11Table 2: Translation results with optimal reorder-ing constraints and window sizes for the BTECJapanese-to-English and Chinese-to-English devel-opment corpora.
*Optimized for the NIST score.mWER mPER BLEU NIST[%] [%] [%]BTEC Japanese-to-English (JE) testAT 41.9 33.8 45.3 9.49WFST 42.1 35.6 47.3 9.50BTEC Chinese-to-English (CE) testAT 45.6 39.0 40.9 8.55WFST 46.4 38.8 40.8 8.73Table 3: Comparison of the IWSLT-2004 automaticevaluation results for the described system (WFST)with those of the best submitted system (AT).Further improvements were obtained with arescoring procedure.
For rescoring, we produceda k-best list of translation hypotheses and used theword penalty and deletion model features, the IBMModel 1 lexicon score, and target language n-grammodels of the order up to 9.
The scaling factors forall features were optimized on the development cor-pus for the NIST score, as described in (Bender etal., 2004).5.3.2 Moderately Non-Mon.
Translation (CE)Word order in Chinese and English is usually sim-ilar.
However, a few word reorderings over quitelarge distances may be necessary.
This is especiallytrue in case of questions, in which question wordslike ?where?
and ?when?
are placed at the end ofa sentence in Chinese.
The BTEC corpora containmany sentences with questions.The inverse IBM constraints are designed to per-form this type of reordering (see Sec.
4.3).
As shownin Fig.
3, the system performs well under these con-mWER mPER BLEU NISTReordering: [%] [%] [%]none 25.6 22.0 62.1 10.46in training 28.0 22.3 58.1 10.32+ 4-local 26.3 20.3 62.2 10.81+ weights 25.3 20.3 62.6 10.79+ 3-ibm 27.2 20.5 61.4 10.76+ weights 25.2 20.3 62.9 10.80+ rescoring* 22.2 19.0 69.2 10.47Table 4: Translation results with optimal reorderingconstraints and window sizes for the test corpus ofthe BTEC IE task.
*Optimized for WER.straints already with relatively small window sizes.Increasing the window size beyond 4 for these con-straints only marginally improves the translation er-ror measures for both short (under 8 words) and longsentences.
Thus, a suitable language-pair-specificchoice of reordering constraints can avoid the hugecomputational complexity required for permutationsof long sentences.Tab.
2 includes error measures for the best setupwith inverse IBM constraints with window size of 7,as well as additional improvements obtained by a k-best list rescoring.The best settings for reordering constraints andmodel scaling factors on the development corporawere then used to produce translations of the IWSLTJapanese and Chinese test corpora.
These trans-lations were evaluated against multiple referenceswhich were unknown to the authors.
Our system(denoted with WFST, see Tab.
3) produced resultscompetitive with the results of the best system at theevaluation campaign (denoted with AT (Bender etal., 2004)) and, according to some of the error mea-sures, even outperformed this system.5.3.3 Almost Monotonic Translation (IE)The word order in the Italian language does notdiffer much from the English.
Therefore, the abso-lute translation error rates are quite low and translat-ing without reordering in training and search alreadyresults in a relatively good performance.
This is re-flected in Tab.
4.
However, even for this languagepair it is possible to improve translation quality byperforming reordering both in training and duringtranslation.
The best performance on the develop-ment corpus is obtained when we constrain the re-odering with relatively small window sizes of 3 to 4and use either IBM or local reordering constraints.173On the test corpus, as shown in Tab.
4, all error mea-sures can be improved with these settings.Especially for languages with similar word orderit is important to use weighted reorderings (Sec.
4.6)in order to prefer the original word order.
Introduc-tion of reordering weights for this task results in no-table improvement of most error measures using ei-ther the IBM or local constraints.
The optimal prob-ability ?
for the unreordered path was determinedon the development corpus as 0.5 for both of theseconstraints.
The results on the test corpus using thissetting are also given in Tab.
4.6 ConclusionIn this paper, we described a reordering frameworkwhich performs source sentence reordering on wordlevel.
We suggested to use optimal alignment func-tions for monotonization and improvement of trans-lation model training.
This allowed us to translatemonotonically taking a reordering graph as input.We then described known and novel reordering con-straints and their efficient finite-state implementa-tions in which the reordering graph is computed on-demand.
We also utilized weighted permutations.We showed that our monotonic phrase-based trans-lation approach effectively makes use of the reorder-ing framework to produce quality translations evenfrom languages with significantly different word or-der.
On the Japanese-to-English and Chinese-to-English IWSLT tasks, our system performed at leastas well as the best machine translation system.AcknowledgementThis work was partially funded by the DeutscheForschungsgemeinschaft (DFG) under the project?Statistische Textu?bersetzung?
(Ne572/5) and by theEuropean Union under the integrated project TC-STAR ?
Technology and Corpora for Speech toSpeech Translation (IST-2002-FP6-506738).ReferencesY.
Akiba, M. Federico, N. Kando, H. Nakaiwa, M. Paul,and J. Tsujii.
2004.
Overview of the IWSLT04 Evalu-ation Campaign.
Proc.
Int.
Workshop on Spoken Lan-guage Translation, pp.
1?12, Kyoto, Japan.S.
Bangalore and G. Riccardi.
2000.
Stochastic Finite-State Models for Spoken Language Machine Transla-tion.
Proc.
Workshop on Embedded Machine Transla-tion Systems, pp.
52?59.O.
Bender, R. Zens, E. Matusov, and H. Ney.
2004.Alignment Templates: the RWTH SMT System.
Proc.Int.
Workshop on Spoken Language Translation, pp.79?84, Kyoto, Japan.A.
L. Berger, P. F. Brown, S. A. Della Pietra, V. J. DellaPietra, J. R. Gillett, A. S. Kehler, and R. L. Mercer.1996.
Language Translation Apparatus and Methodof Using Context-based Translation Models.
UnitedStates Patent 5510981.F.
Casacuberta and E. Vidal.
2004.
Machine Transla-tion with Inferred Stochastic Finite-State Transducers.Computational Linguistics, vol.
30(2):205-225.G.
Doddington.
2002.
Automatic Evaluation of MachineTranslation Quality Using n-gram Co-OccurrenceStatistics.
Proc.
Human Language Technology Conf.,San Diego, CA.S.
Kanthak and H. Ney.
2004.
FSA: an Efficient andFlexible C++ Toolkit for Finite State Automata usingOn-demand Computation.
Proc.
42nd Annual Meet-ing of the Association for Computational Linguistics,pp.
510?517, Barcelona, Spain.K.
Knight and Y. Al-Onaizan.
1998.
Translation withFinite-State Devices.
Lecture Notes in Artificial Intel-ligence, Springer-Verlag, vol.
1529, pp.
421?437.S.
Kumar and W. Byrne.
2003.
A Weighted Finite StateTransducer Implementation of the Alignment TemplateModel for Statistical Machine Translation.
Proc.
Hu-man Language Technology Conf.
NAACL, pp.
142?149, Edmonton, Canada.E.
Matusov, R. Zens, and H. Ney.
2004.
Symmetric WordAlignments for Statistical Machine Translation.
Proc.20th Int.
Conf.
on Computational Linguistics, pp.
219?225, Geneva, Switzerland.F.
J. Och and H. Ney.
2003.
A Systematic Comparison ofVarious Statistical Alignment Models.
ComputationalLinguistics, vol.
29, number 1, pp.
19?51.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.BLEU: a Method for Automatic Evaluation of MachineTranslation.
Proc.
40th Annual Meeting of the Associ-ation for Computational Linguistics, Philadelphia, PA,pp.
311?318.J.
M. Vilar, 2000.
Improve the Learning of Sub-sequential Transducers by Using Alignments and Dic-tionaries.
Lecture Notes in Artificial Intelligence,Springer-Verlag, vol.
1891, pp.
298?312.D.
Wu.
1997.
Stochastic Inversion TransductionGrammars and Bilingual Parsing of Parallel Corpora.Computational Linguistics, 23(3):377?403.R.
Zens, F. J. Och and H. Ney.
2002.
Phrase-Based Sta-tistical Machine Translation.
In: M. Jarke, J. Koehler,G.
Lakemeyer (Eds.
): KI - Conference on AI, KI 2002,Vol.
LNAI 2479, pp.
18-32, Springer Verlag.R.
Zens and H. Ney.
2003.
A Comparative Study onReordering Constraints in Statistical Machine Trans-lation.
Proc.
Annual Meeting of the Associationfor Computational Linguistics, pp.
144?151, Sapporo,Japan.174
