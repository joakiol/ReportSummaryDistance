Proceedings of NAACL HLT 2007, Companion Volume, pages 33?36,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsGeneralized Graphical Abstractions for Statistical Machine TranslationKarim Filali and Jeff Bilmes?Departments of Computer Science & Engineering and Electrical EngineeringUniversity of WashingtonSeattle, WA 98195, USA{karim@cs,bilmes@ee}.washington.eduAbstractWe introduce a novel framework for theexpression, rapid-prototyping, and eval-uation of statistical machine-translation(MT) systems using graphical mod-els.
The framework extends dynamicBayesian networks with multiple con-nected different-length streams, switchingvariable existence and dependence mech-anisms, and constraint factors.
We haveimplemented a new general-purpose MTtraining/decoding system in this frame-work, and have tested this on a variety ofexisting MT models (including the 4 IBMmodels), and some novel ones as well,all using Europarl as a test corpus.
Wedescribe the semantics of our representa-tion, and present preliminary evaluations,showing that it is possible to prototypenovel MT ideas in a short amount of time.1 IntroductionWe present a unified graphical model frameworkbased on (Filali and Bilmes, 2006) for statistical ma-chine translation.
Graphical models utilize graphicaldescriptions of probabilistic processes, and are capa-ble of quickly describing a wide variety of differentsets of model assumptions.
In our approach, eitherphrases or words can be used as the unit of transla-tion, but as a first step, we have only implementedword-based models since our main goal is to show?This material was supported by NSF under Grant No.
ISS-0326276.the viability of our graphical model representationand new software system.There are several important advantages to a uni-fied probabilistic framework for MT including: (1)the same codebase can be used for training and de-coding without having to implement a separate de-coder for each model; (2) new models can be pro-totyped quickly; (3) combining models (such as ina speech-MT system) is easier when they are en-coded in the same framework; (4) sharing algo-rithms across different disciplines (e.g., the MT andthe constraint-satisfaction community) is facilitated.2 Graphical Model FrameworkA Graphical Model (GM) represents a factorizationof a family of joint probability distributions over aset of random variables using a graph.
The graphspecifies conditional independence relationships be-tween the variables, and parameters of the modelare associated with each variable or group thereof.There are many types of graphical models.
For ex-ample, Bayesian networks use an acyclic directedgraph and their parameters are conditional probabili-ties of each variable given its parents.
Various formsof GM and their conditional independence proper-ties are defined in (Lauritzen, 1996).Our graphical representation, which we callMulti-dynamic Bayesian Networks (MDBNs) (Filaliand Bilmes, 2006), is a generalization of dynamicBayesian networks (DBNs) (Dean and Kanazawa,1988).
DBNs are an appropriate representation forsequential (for example, temporal) stochastic pro-cesses, but can be very difficult to apply when de-pendencies have arbitrary time-span and the exis-tence of random variables is contingent on the val-33ues of certain variables in the network.
In (Filaliand Bilmes, 2006), we discuss inference and learn-ing in MDBNs.
Here we focus on representationand the evaluation of our new implementation andframework.
Below, we summarize key features un-derlying our framework.
In section 3, we explainhow these features apply to a specific MT model.?
Multiple DBNs can be represented along withrules for how they are interconnected ?
the rule de-scription lengths are fixed (they do not grow with thelength of the DBNs).?
Switching dependencies (Geiger and Hecker-man, 1996): a variable X is a switching parent ofY if X influences what type of dependencies Y haswith respect to its other parents, e.g., an alignmentvariable in IBM Models 1 and 2 is switching.?
Switching existence: A variable X is ?switch-ing existence?
with respect to variable Y if the valueof X determines whether Y exists.
An example is afertility variable in IBM Models 3 and above.?
Constraints and aggregation: BN semantics canencode various types of constraints between groupsof variables (Pearl, 1988).
For example, in the con-struct A ?
B ?
C where B is observed, B canconstrain A and C to be unequal.
We extend thosesemantics to support a more efficient evaluation ofconstraints under some variable order conditions.3 GM Representation of IBM MT ModelsIn this section we present a GM representation forIBM model 3 (Brown et al, 1993) in fig.
1.
Model 3is intricate enough to showcase some of the featuresof our graphical representation but not as complexas, and thus is easier to describe, than model 4.
Ourchoice of representing IBM models is not becausewe believe they are state of the art MT models?although they are still widely used in producingalignments and as features in log-linear models?but because they provide a good initial testbed forour architecture.The topmost random variable (RV), ?, is a hid-den switching existence variable corresponding tothe length of the English string.
The box abutting?
includes all the nodes whose existence depends onthe value of ?.
In the figure, ?
= 3, thus resultingin three English words e1, ..., e3, connected using asecond-order Markov chain.
To each English wordei corresponds a conditionally dependent fertility ?i,which indicates how many times ei is used by wordsin the French string.
Each ?i in turn grants existenceto a set of RVs under it.
Given the fertilities (the fig-ure depicts the case ?1 = 3, ?2 = 1, ?3 = 0), foreach word ei, ?i French word RVs are granted exis-tence and are denoted by the tablet ?i1, ?i2, .
.
.
, ?i?iof ei.
The values of ?
variables need to match theactual observed French sequence f1, .
.
.
, fm.
This isrepresented as a shared constraint between all the f ,?, and ?
variables which have incoming edges intothe observed variable v. v?s conditional probabilitytable is such that it is one only when the associatedconstraint is satisfied.
The variable ?i,k is a switch-ing dependency parent with respect to the constraintvariable v and determines which fj participates inan equality constraint with ?i,k.In the null word sub-model, the constraint thatsuccessive permutation variables be ordered is im-plemented using the observed child w of ?0i and?0(i+1).
The probability of w being unity is one onlywhen the constraint is satisfied and zero otherwise.The bottom variable m is a switching existencenode (observed to be 6 in the figure) with cor-responding French word sequence and alignmentvariables.
The French sequence participates in thev constraint described above, while the alignmentvariables aj ?
{1, .
.
.
, ?
}, j ?
1, .
.
.
,m constrainthe fertilities to take their unique allowable values(for the given alignment).
Alignments also restrictthe domain of permutation variables, pi, using theconstraint variable x.
Finally, the domain size ofeach aj has to lie in the interval [0, ?]
and that is en-forced by the variable u.
The dashed edges connect-ing the alignment a variables represent an extensionto implement an M3/M-HMM hybrid.14 ExperimentsWe have developed (in C++) a new entirely self-contained general-purpose MT training/decodingsystem based on our framework, of which we pro-vide a preliminary evaluation in this section.
Al-though the framework is perfectly capable of rep-resenting phrase-based models, we restrict ourselvesto word-based models to show the viability of graph-ical models for MT and will consider different trans-lation units in future work.
We perform MT ex-1We refer to the HMM MT model in (Vogel et al, 1996) asM-HMM to avoid any confusion.34mf1a1?0 ?1 ?2 ?3e1 e2 e3f2a2f3a3f4a4f5a5f6a6m?
?01 ?02 ?11 ?12 ?13 ?21?01 ?02 ?11 ?12 ?13 ?21u vxyw?Figure 1: Unrolled Model 3 graphical model with fertilityassignment ?0 = 2, ?1 = 3, ?2 = 1, ?3 = 0.periments on a English-French subset of the Eu-roparl corpus used for the ACL 2005 SMT evalu-ations (Koehn and Monz, 2005).
We train an En-glish language model on the whole training set us-ing the SRILM toolkit (Stolcke, 2002) and trainMT models mainly on a 10k sentence pair sub-set of the ACL training set.
We test on the 2000sentence test set used for the same evaluations.For comparison, we use the MT training program,GIZA++ (Och and Ney, 2003), the phrase-base de-coder, Pharaoh (Koehn et al, 2003), and the word-based decoder, Rewrite (Germann, 2003).For inference we use a backtracking depth-firstsearch inference method with memoization that ex-tends Value Elimination (Bacchus et al, 2003).
Thesame inference engine is used for both training anddecoding.
As an admissible heuristic for decod-ing, we compute, for each node V with ConditionalProbability Table c, the largest value of c over allpossible configurations of V and its parents (Filaliand Bilmes, 2006).Decoder BLEU (%)500 1000 1500 2000Rewrite 25.3 22.3 21.7 22.01Pharaoh 20.4 18.1 17.7 18.05M-HMM 19.9 16.9 15.6 12.5Table 1: BLEU scores on first 500, 1000, 1500, and2000 sentences (ordered from shortest to longest) ofthe ACL05 English-French 2000 sentence test set us-ing a 700k sent train set.
The last row is our MDBNsystem?s simulation of a M-HMM model.Table 1 compares MT performance between (1)Pharaoh (which uses beam search), (2) our system,and (3) Rewrite (hill-climbing).
(1) and (2) makeuse of a fixed lexical table2 learned using an M-HMM model specified using our tool, and neitheruses minimum error rate training.
(3) uses Model4 parameters learned using GIZA++.
This compari-son is informative because Rewrite is a special pur-pose model 4 decoder and we would expect it toperform at least as well as decoders not written fora specific IBM model.
Pharaoh is more general inthat it only requires, as input, a lexical table fromany given model.3 Our MDBN system is not tai-lored for the translation task.
Pharaoh was able todecode the 2000 sentences of the test set in 5000son a 3.2GHz machine; Rewrite took 84000s, and weallotted 400000s for our engine (200s per sentence).We attribute the difference in speed and BLEU scorebetween our system and Pharaoh to the fact ValueElimination searches in a depth-first fashion overthe space of partial configurations of RVs, whilePharaoh expands partial translation hypotheses in abest-first search manner.
Thus, Pharaoh can take ad-vantage of knowledge about the MT problem?s hy-pothesis space while the GM is agnostic with respectto the structure of the problem?something that isdesirable from our perspective since generality isa main concern of ours.
Moreover, the MDBN?sheuristic and caching of previously explored sub-trees have not yet proven able to defray the cost,associated with depth-first search, of exploring sub-trees that do not contain any ?good?
configurations.Table 2 shows BLEU scores of different MT mod-els trained using our system.
We decode usingPharaoh because the above speed difference in itsfavor allowed us to run more experiments and fo-cus on the training aspect of different models.
M1,M2, M-HMM, M3, and M4 are the standard IBMmodels.
M2d and M-Hd are variants in whichthe distortion between the French and English po-sitions is used instead of the absolute alignment po-sition.
M-Hdd is a second-order M-HMM model(with distortion).
M3H (see fig 1) is a variant ofmodel 3 that uses first-order dependencies betweenalignment variables.
M-Hr is another HMM modelthat uses the relative distortion between the currentalignment and the previous one.
This is similarto the model implemented by GIZA except we did2Pharaoh?s phrases are single words only.3It does, however, use simple hard-coded distortion and fer-tility models.35BLEU(%)Giza train MDBN train10k 700k 10k 700kM1 15.67 18.04 14.53 17.74M2 15.84 18.52 15.74M2d NA NA 15.75M-HMM NA NA 15.87M-Hd NA NA 15.99 18.05M-Hdd NA NA 15.55M-Hr 16.98 19.57 16.04M3 16.78 19.38 15.32M3H NA NA 15.67M4 16.81 19.51 15.00M4H NA NA 15.20Table 2: BLEU scores for various models trainedusing GM and GIZA (when applicable).
All modelsare decoded using Pharaoh.not include the English word class dependency.
Fi-nally, model M4H is a simplified model 4, in whichonly distortions within each tablet are modeled but aMarkov dependency is also used between the align-ment variables.Table 2 also shows BLEU scores obtained bytraining equivalent IBM models using GIZA andthe standard training regimen of initializing highermodels with lower ones (we use the same sched-ules for our GM training, but only transfer lexical ta-bles).
The main observation is that GIZA-trained M-HMM, M3 and 4 have about 1% better BLEU scoresthan their corresponding MDBN versions.
We at-tribute the difference in M3/4 scores to the fact weuse a Viterbi-like training procedure (i.e., we con-sider a single configuration of the hidden variablesin EM training) while GIZA uses pegging (Brown etal., 1993) to sum over a set of likely hidden variableconfigurations in EM.While these preliminary results do not show im-proved MT performance, nor would we expect themto since they are on simulated IBM models, we findvery promising the fact that this general-purposegraphical model-based system produces competitiveMT results on a computationally challenging task.5 Conclusion and Future WorkWe have described a new probabilistic frameworkfor doing statistical machine translation.
We havefocused so far on word-based translation.
In fu-ture work, we intend to implement phrase-based MTmodels.
We also plan to design better approximateinference strategies for training highly connectedgraphs such as IBM models 3 and 4, and some novelextensions.
We are also working on new best-firstsearch generalizations of our depth-first search in-ference to improve decoding time.
As there has beenincreased interest in end-to-end task such as speechtranslation, dialog systems, and multilingual search,a new challenge is how best to combine the complexcomponents of these systems into one framework.We believe that, in addition to the finite-state trans-ducer approach, a graphical model framework suchas ours would be well suited for this scientific andengineering endeavor.ReferencesF.
Bacchus, S. Dalmao, and T. Pitassi.
2003.
Value elimina-tion: Bayesian inference via backtracking search.
In UAI-03,pages 20?28, San Francisco, CA.
Morgan Kaufmann.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.Mercer.
1993.
The mathematics of statistical machine trans-lation: Parameter estimation.
Computational Linguistics,19(2):263?311.T.
Dean and K. Kanazawa.
1988.
Probabilistic temporal rea-soning.
AAAI, pages 524?528.Karim Filali and Jeff Bilmes.
2006.
Multi-dynamic Bayesiannetworks.
In Advances in Neural Information ProcessingSystems 19.
MIT Press, Cambridge, MA.Dan Geiger and David Heckerman.
1996.
Knowledge repre-sentation and inference in similarity networks and bayesianmultinets.
Artif.
Intell., 82(1-2):45?74.Ulrich Germann.
2003.
Greedy decoding for statistical Ma-chine Translation in almost linear time.
In HLT-NAACL,pages 72?79, Edmonton, Canada, May.
ACL.P.
Koehn and C. Monz.
2005.
Shared task: Statistical machinetranslation between European languages.
pages 119?124,Ann Arbor, MI, June.
ACL.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statistical phrase-based translation.
In HLT-NAACL, May.S.L.
Lauritzen.
1996.
Graphical Models.
Oxford Science Pub-lications.F.
J. Och and H. Ney.
2003.
A systematic comparison of vari-ous statistical alignment models.
Computational Linguistics,29(1):19?51.J.
Pearl.
1988.
Probabilistic Reasoning in Intelligent Systems:Networks of Plausible Inference.
Morgan Kaufmann.A.
Stolcke.
2002.
SRILM ?
an extensible language modelingtoolkit.
In Proc.
Int.
Conf.
on Spoken Language Processing.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-based wordalignment in statistical translation.
In COLING, pages 836?841.36
