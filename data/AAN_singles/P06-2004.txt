Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 25?32,Sydney, July 2006. c?2006 Association for Computational LinguisticsThe Effect of Corpus Size in Combining Supervised andUnsupervised Training for DisambiguationMichaela AttererInstitute for NLPUniversity of Stuttgartatterer@ims.uni-stuttgart.deHinrich Schu?tzeInstitute for NLPUniversity of Stuttgarthinrich@hotmail.comAbstractWe investigate the effect of corpus sizein combining supervised and unsuper-vised learning for two types of attach-ment decisions: relative clause attach-ment and prepositional phrase attach-ment.
The supervised component isCollins?
parser, trained on the WallStreet Journal.
The unsupervised com-ponent gathers lexical statistics froman unannotated corpus of newswiretext.
We find that the combined sys-tem only improves the performance ofthe parser for small training sets.
Sur-prisingly, the size of the unannotatedcorpus has little effect due to the noisi-ness of the lexical statistics acquired byunsupervised learning.1 IntroductionThe best performing systems for many tasks innatural language processing are based on su-pervised training on annotated corpora suchas the Penn Treebank (Marcus et al, 1993)and the prepositional phrase data set first de-scribed in (Ratnaparkhi et al, 1994).
How-ever, the production of training sets is ex-pensive.
They are not available for manydomains and languages.
This motivates re-search on combining supervised with unsu-pervised learning since unannotated text is inample supply for most domains in the majorlanguages of the world.
The question ariseshow much annotated and unannotated datais necessary in combination learning strate-gies.
We investigate this question for two at-tachment ambiguity problems: relative clause(RC) attachment and prepositional phrase(PP) attachment.
The supervised componentis Collins?
parser (Collins, 1997), trained onthe Wall Street Journal.
The unsupervisedcomponent gathers lexical statistics from anunannotated corpus of newswire text.The sizes of both types of corpora, anno-tated and unannotated, are of interest.
Wewould expect that large annotated corpora(training sets) tend to make the additional in-formation from unannotated corpora redun-dant.
This expectation is confirmed in ourexperiments.
For example, when using themaximum training set available for PP attach-ment, performance decreases when ?unanno-tated?
lexical statistics are added.For unannotated corpora, we would expectthe opposite effect.
The larger the unanno-tated corpus, the better the combined systemshould perform.
While there is a general ten-dency to this effect, the improvements in ourexperiments reach a plateau quickly as the un-labeled corpus grows, especially for PP attach-ment.
We attribute this result to the noisinessof the statistics collected from unlabeled cor-pora.The paper is organized as follows.
Sections2, 3 and 4 describe data sets, methods andexperiments.
Section 5 evaluates and discussesexperimental results.
Section 6 compares ourapproach to prior work.
Section 7 states ourconclusions.2 Data SetsThe unlabeled corpus is the Reuters RCV1corpus, about 80,000,000 words of newswiretext (Lewis et al, 2004).
Three different sub-sets, corresponding to roughly 10%, 50% and100% of the corpus, were created for experi-ments related to the size of the unannotatedcorpus.
(Two weeks after Aug 5, 1997, wereset apart for future experiments.
)The labeled corpus is the Penn Wall StreetJournal treebank (Marcus et al, 1993).
We25created the 5 subsets shown in Table 1 for ex-periments related to the size of the annotatedcorpus.unlabeled R100% 20/08/1996?05/08/1997 (351 days)50% 20/08/1996?17/02/1997 (182 days)10% 20/08/1996?24/09/1996 (36 days)labeled WSJ50% sections 00?12 (23412 sentences)25% lines 1 ?
292960 (11637 sentences)5% lines 1 ?
58284 (2304 sentences)1% lines 1 ?
11720 (500 sentences)0.05% lines 1 ?
611 (23 sentences)Table 1: Corpora used for the experiments:unlabeled Reuters (R) corpus for attachmentstatistics, labeled Penn treebank (WSJ) fortraining the Collins parser.The test set, sections 13-24, is larger than inmost studies because a single section does notcontain a sufficient number of RC attachmentambiguities for a meaningful evaluation.which-clauses subset highA lowA totaldevelop set (sec 00-12) 71 211 282test set (sec 13-24) 71 193 264PP subset verbA nounA totaldevelop set (sec 00-12) 5927 6560 12487test set (sec 13-24) 5930 6273 12203Table 2: RC and PP attachment ambigui-ties in the Penn Treebank.
Number of in-stances with high attachment (highA), low at-tachment (lowA), verb attachment (verbA),and noun attachment (nounA) according tothe gold standard.All instances of RC and PP attachmentswere extracted from development and testsets, yielding about 250 RC ambiguities and12,000 PP ambiguities per set (Table 2).
AnRC attachment ambiguity was defined as asentence containing the pattern NP1 Prep NP2which.
For example, the relative clause in Ex-ample 1 can either attach to mechanism or toSystem.
(1) ... the exchange-rate mechanism of theEuropean Monetary System, whichlinks the major EC currencies.A PP attachment ambiguity was defined asa subtree matching either [VP [NP PP]] or [VPNP PP].
An example of a PP attachment am-biguity is Example 2 where either the approvalor the transaction is performed by written con-sent.
(2) .
.
.
a majority .
.
.
have approved thetransaction by written consent .
.
.Both data sets are available for download(Web Appendix, 2006).
We did not use thePP data set described by (Ratnaparkhi et al,1994) because we are using more context thanthe limited context available in that set (seebelow).3 MethodsCollins parser.
Our baseline method forambiguity resolution is the Collins parser asimplemented by Bikel (Collins, 1997; Bikel,2004).
For each ambiguity, we check whetherthe attachment ambiguity is resolved correctlyby the 5 parsers corresponding to the differenttraining sets.
If the attachment ambiguity isnot recognized (e.g., because parsing failed),then the corresponding ambiguity is excludedfor that instance of the parser.
As a result, thesize of the effective test set varies from parserto parser (see Table 4).Minipar.
The unannotated corpus is ana-lyzed using minipar (Lin, 1998), a partial de-pendency parser.
The corpus is parsed and allextracted dependencies are stored for later use.Dependencies in ambiguous PP attachments(those corresponding to [VP NP PP] and [VP[NP PP]] subtrees) are not indexed.
An ex-periment with indexing both alternatives forambiguous structures yielded poor results.
Forexample, indexing both alternatives will createa large number of spurious verb attachmentsof of, which in turn will result in incorrect highattachments by our disambiguation algorithm.For relative clauses, no such filtering is nec-essary.
For example, spurious subject-verbdependencies due to RC ambiguities are rarecompared to a large number of subject-verbdependencies that can be extracted reliably.Inverted index.
Dependencies extractedby minipar are stored in an inverted index(Witten et al, 1999), implemented in Lucene(Lucene, 2006).
For example, ?john subjbuy?, the analysis returned by minipar forJohn buys, is stored as ?john buy john<subj26subj<buy john<subj<buy?.
All words, de-pendencies and partial dependencies of a sen-tence are stored together as one document.This storage mechanism enables fast on-linequeries for lexical and dependency statistics,e.g., how many sentences contain the depen-dency ?john subj buy?, how often does johnoccur as a subject, how often does buy havejohn as a subject and car as an object etc.Query results are approximate because doubleoccurrences are only counted once and struc-tures giving rise to the same set of dependen-cies (a piece of a tile of a roof of a house vs.a piece of a roof of a tile of a house) cannotbe distinguished.
We believe that an invertedindex is the most efficient data structure forour purposes.
For example, we need not com-pute expensive joins as would be required in adatabase implementation.
Our long-term goalis to use this inverted index of dependenciesas a versatile component of NLP systems inanalogy to the increasingly important role ofsearch engines for association and word countstatistics in NLP.A total of three inverted indexes were cre-ated, one each for the 10%, 50% and 100%Reuters subset.Lattice-Based Disambiguation.
Ourdisambiguation method is Lattice-BasedDisambiguation (LBD, (Atterer and Schu?tze,2006)).
We formalize a possible attachmentas a triple < R, i,X > where X is (theparse of) a phrase with two or more possibleattachment nodes in a sentence S, i is one ofthese attachment nodes and R is (the relevantpart of a parse of) S with X removed.
Forexample, the two attachments in Example 2are represented as the triples:< approvedi1 the transactioni2 , i1,by consent >,< approvedi1 the transactioni2 , i2,by consent >.We decide between attachment possibilitiesbased on pointwise mutual information, thewell-known measure of how surprising it is tosee R and X together given their individualfrequencies:MI(< R, i,X >) = log2 P (<R,i,X>)P (R)P (X)for P (< R, i,X >), P (R), P (X) 6= 0MI(< R, i,X >) = 0 otherwisewhere the probabilities of the dependencystructures < R, i,X >, R and X are estimatedon the unlabeled corpus by querying the in-0:pMN:pNN:pMN:pN:pN MN:pMN:pMN:pMNMN:pMNFigure 1: Lattice of pairs of potential attach-ment site (NP) and attachment phrase (PP).M: premodifying adjective or noun (upper orlower NP), N: head noun (upper or lower NP),p: Preposition.verted index.
Unfortunately, these structureswill often not occur in the corpus.
If this isthe case we back off to generalizations of Rand X.
The generalizations form a lattice asshown in Figure 1 for PP attachment.
For ex-ample, MN:pMN corresponds to commercialtransaction by unanimous consent, N:pM totransaction by unanimous etc.
For 0:p we com-pute MI of the two events ?noun attachment?and ?occurrence of p?.
Points in the lattice inFigure 1 are created by successive eliminationof material from the complete context R:X.A child c directly dominated by a parent pis created by removing exactly one contextualelement from p, either on the right side (theattachment phrase) or on the left side (the at-tachment node).
For RC attachment, general-izations other than elimination are introducedsuch as the replacement of a proper noun (e.g.,Canada) by its category (country) (see below).The MI of each point in the lattice is com-puted.
We then take the maximum over allMI values of the lattice as a measure of theaffinity of attachment phrase and attachmentnode.
The intuition is that we are looking forthe strongest evidence available for the attach-ment.
The strongest evidence is often not pro-vided by the most specific context (MN:pMNin the example) since contextual elements likemodifiers will only add noise to the attachmentdecision in some cases.
The actual syntacticdisambiguation is performed by computing theaffinity (maximum over MI values in the lat-tice) for each possible attachment and select-ing the attachment with highest affinity.
(The27default attachment is selected if the two valuesare equal.)
The second lattice for PP attach-ment, the lattice for attachment to the verb,has a structure identical to Figure 1, but theattachment node is SV instead of MN, whereS denotes the subject and V the verb.
So thesupremum of that lattice is SV:pMN and theinfimum is 0:p (which in this case correspondsto the MI of verb attachment and occurrenceof the preposition).LBD is motivated by the desire to use asmuch context as possible for disambiguation.Previous work on attachment disambiguationhas generally used less context than in thispaper (e.g., modifiers have not been used forPP attachment).
No change to LBD is neces-sary if the lattice of contexts is extended byadding additional contextual elements (e.g.,the preposition between the two attachmentnodes in RC, which we do not consider in thispaper).4 ExperimentsThe Reuters corpus was parsed with miniparand all dependencies were extracted.
Threeinverted indexes were created, correspondingto 10%, 50% and 100% of the corpus.1 Fiveparameter sets for the Collins parser were cre-ated by training it on the WSJ training setsin Table 1.
Sentences with attachment am-biguities in the WSJ corpus were parsed withminipar to generate Lucene queries.
(We chosethis procedure to ensure compatibility of queryand index formats.)
The Lucene queries wererun on the three indexes.
LBD disambigua-tion was then applied based on the statisticsreturned by the queries.
LBD results are ap-plied to the output of the Collins parser bysimply replacing all attachment decisions withLBD decisions.4.1 RC attachmentThe lattice for LBD in RC attachment isshown in Figure 2.
When disambiguatingan RC attachment, two instances of thelattice are formed, one for NP1 and one1In fact, two different sets of inverted indexes werecreated, one each for PP and RC disambiguation.
TheRC index indexes all dependencies, including ambigu-ous PP dependencies.
Computing the RC statisticson the PP index should not affect the RC results pre-sented here, but we didn?t have time to confirm thisexperimentally for this paper.for NP2 in NP1 Prep NP2 RC.
Figure 2shows the maximum possible lattice.
Ifcontextual elements are not present in acontext (e.g., a modifier), then the latticewill be smaller.
The supremum of the lat-tice corresponds to a query that includesthe entire NP (including modifying adjec-tives and nouns)2, the verb and its object.Example: exchange rate<nn<mechanim&& mechanism<subj<link &&currency<obj<link.C:V[empty]MC:VC:VOMn:VMN:VO Nf:VOMn:VO N:VO MN:V Nf:VMC:VOMNf:VOn:Vn:VOMNf:VN:VFigure 2: Lattice of pairs of potential attach-ment site NP and relative clause X. M: pre-modifying adjective or noun, Nf: head nounwith lexical modifiers, N: head noun only, n:head noun in lower case, C: class of NP, V:verb in relative clause, O: object of verb inthe relative clause.To generalize contexts in the lattice, the fol-lowing generalization operations are employed:?
strip the NP of the modifying adjec-tive/noun (weekly report ?
report)?
use only the head noun of the NP (Catas-trophic Care Act ?
Act)?
use the head noun in lower case (Act ?
act)?
for named entities use a hypernym of the NP(American Bell Telephone Co. ?
company)?
strip the object from X (company have sub-sidiary ?
company have)The most important dependency for disam-2From the minipar output, we use all adjectives thatmodify the NP via the relation mod, and all nouns thatmodify the NP via the relation nn.28biguation is the noun-verb link, but the otherdependencies also improve the accuracy ofdisambiguation (Atterer and Schu?tze, 2006).For example, light verbs like make and haveonly provide disambiguation information whentheir objects are also considered.Downcasing and hypernym generalizationswere used because proper nouns often causesparse data problems.
Named entity classeswere identified with LingPipe (LingPipe,2006).
Named entities identified as companiesor organizations are replaced with company inthe query.
Locations are replaced with coun-try.
Persons block RC attachment becausewhich-clauses do not attach to person names,resulting in an attachment of the RC to theother NP.query MI+exchange rate?nn?mechanism 12.2+mechanism?subj?link +currency?obj?link+exchange rate?nn?mechanism 4.8+mechanism?subj?link+mechanism?subj?link +currency?obj?link 10.2mechanism?subj?link 3.4+European Monetary System?subj?link 0+currency?obj?link+System?subj?link +currency?obj?link 0European Monetary System?subj?link 0System?subj?link 0+system?subj?link +currency?obj?link 0system?subj?link 1.2+company?subj?link +currency?obj?link 0company?subj?link -1.1empty 3Table 3: Queries for computing high attach-ment (above) and low attachment (below) forExample 1.Table 3 shows queries and mutual informa-tion values for Example 1.
The highest valuesare 12.2 for high attachment (mechanism) and3 for low attachment (System).
The algorithmtherefore selects high attachment.The value 3 for low attachment is the de-fault value for the empty context.
This valuereflects the bias for low attachment: the ma-jority of relative clauses are attached low.
Ifall MI-values are zero or otherwise low, thisprocedure will automatically result in low at-tachment.33We experimented with a number of values (2, 3,and 4) on the development set.
Accuracy of the algo-rithm was best for a value of 3.
The results presentedhere differ slightly from those in (Atterer and Schu?tze,2006) due to a coding error.Decision list.
For increased accuracy, LBDis embedded in the following decision list.1.
If minipar has already chosen high attach-ment, choose high attachment (this only oc-curs if NP1 Prep NP2 is a named entity).2.
If there is agreement between the verb andonly one of the NPs, attach to this NP.3.
If one of the NPs is in a list of person entities,attach to the other NP.44.
If possible, use LBD.5.
If none of the above strategies was successful(e.g.
in the case of parsing errors), attachlow.4.2 PP attachmentThe two lattices for LBD applied to PP at-tachment were described in Section 3 and Fig-ure 1.
The only generalization operation usedin these two lattices is elimination of contex-tual elements (in particular, there is no down-casing and named entity recognition).
Notethat in RC attachment, we compare affinitiesof two instances of the same lattice (the oneshown in Figure 2).
In PP attachment, wecompare affinities of two different lattices sincethe two attachment points (verb vs. noun) aredifferent.
The basic version of LBD (with theuntuned default value 0 and without decisionlists) was used for PP attachment.5 Evaluation and DiscussionEvaluation results are shown in Table 4.
Thelines marked LBD evaluate the performanceof LBD separately (without Collins?
parser).LBD is significantly better than the baselinefor PP attachment (p < 0.001, all tests are?2 tests).
LBD is also better than baselinefor RC attachment, but this result is not sig-nificant due to the small size of the data set(264).
Note that the baseline for PP attach-ment is 51.4% as indicated in the table (upperright corner of PP table), but that the base-line for RC attachment is 73.1%.
The differ-ence between 73.1% and 76.1% (upper rightcorner of RC table) is due to the fact that forRC attachment LBD proper is embedded in adecision list.
The decision list alone, with an4This list contains 136 entries and was semiauto-matically computed from the Reuters corpus: An-tecedents of who relative clauses were extracted, andthe top 200 were filtered manually.29RC attachmentTrain data # Coll.
only 100% R 50% R 10% R 0% RLBD 264 78.4% 78.0% 76.9% 76.1%50% 251 71.7% 78.5% 78.1% 76.9% 76.1%25% 250 70.0% 78.0% 77.6% 76.4% 76.4%5% 238 68.9% 78.2% 77.7% 76.9% 76.1%1% 245 67.8% 78.8% 78.4% 77.1% 76.7%0.05% 194 60.8% 76.8% 76.3% 75.8% 73.7%PP attachmentTrain data # Coll.
only 100% R 50% R 10% R 0% RLBD 12203 73.4% 73.4% 73.0% 51.4%50% 11953 82.8% 73.6% 73.6% 73.2% 51.7%25% 11950 81.5% 73.6% 73.7% 73.3% 51.7%5% 11737 77.4% 74.1% 74.2% 73.7% 52.3%1% 11803 72.9% 73.6% 73.6% 73.2% 51.6%0.05% 8486 58.0% 73.9% 73.8% 74.0% 52.8%Table 4: Experimental results.
Results for LBD (without Collins) are given in the first lines.
#is the size of the test set.
The baselines are 73.1% (RC) and 51.4% (PP).
The combined methodperforms better for small training sets.
There is no significant difference between 10%, 50% and100% for the combination method (p < 0.05).unlabeled corpus of size 0, achieves a perfor-mance of 76.1%.The bottom five lines of each table evalu-ate combinations of a parameter set trainedon a subset of WSJ (0.05% ?
50%) and a par-ticular size of the unlabeled corpus (100% ?0%).
In addition, the third column gives theperformance of Collins?
parser without LBD.Recall that test set size (second column) variesbecause we discard a test instance if Collins?parser does not recognize that there is an am-biguity (e.g., because of a parse failure).
Asexpected, performance increases as the size ofthe training set grows, e.g., from 58.0% to82.8% for PP attachment.The combination of Collins and LBD is con-sistently better than Collins for RC attach-ment (not statistically significant due to thesize of the data set).
However, this is notthe case for PP attachment.
Due to the goodperformance of Collins?
parser for even smalltraining sets, the combination is only superiorfor the two smallest training sets (significantfor the smallest set, p < 0.001).The most surprising result of the experi-ments is the small difference between the threeunlabeled corpora.
There is no clear pattern inthe data for PP attachment and only a smalleffect for RC attachment: an increase between1% and 2% when corpus size is increased from10% to 100%.We performed an analysis of a sample of in-correctly attached PPs to investigate why un-labeled corpus size has such a small effect.
Wefound that the noisiness of the statistics ex-tracted from Reuters were often responsiblefor attachment errors.
The noisiness is causedby our filtering strategy (ambiguous PPs arenot used, resulting in undercounting), by theapproximation of counts by Lucene (Luceneovercounts and undercounts as discussed inSection 3) and by minipar parse errors.
Parseerrors are particularly harmful in cases likethe impact it would have on prospects, where,due to the extraction of the NP impact, mini-par attaches the PP to the verb.
We didnot filter out these more complex ambiguouscases.
Finally, the two corpora are from dis-tinct sources and from distinct time periods(early nineties vs. mid-nineties).
Many topic-and time-specific dependencies can only bemined from more similar corpora.The experiments reveal interesting dif-ferences between PP and RC attachment.The dependencies used in RC disambiguationrarely occur in an ambiguous context (e.g.,most subject-verb dependencies can be reli-ably extracted).
In contrast, a large propor-tion of the dependencies needed in PP dis-ambiguation (verb-prep and noun-prep depen-dencies) do occur in ambiguous contexts.
An-other difference is that RC attachment is syn-tactically more complex.
It interacts withagreement, passive and long-distance depen-30dencies.
The algorithm proposed for RC ap-plies grammatical constraints successfully.
Afinal difference is that the baseline for RC ismuch higher than for PP and therefore harderto beat.5An innovation of our disambiguation systemis the use of a search engine, lucene, for serv-ing up dependency statistics.
The advantageis that counts can be computed quickly anddynamically.
New text can be added on anongoing basis to the index.
The updated de-pendency statistics are immediately availableand can benefit disambiguation performance.Such a system can adapt easily to new topicsand changes over time.
However, this archi-tecture negatively affects accuracy.
The un-supervised approach of (Hindle and Rooth,1993) achieves almost 80% accuracy by usingpartial dependency statistics to disambiguateambiguous sentences in the unlabeled corpus.Ambiguous sentences were excluded from ourindex to make index construction simple andefficient.
Our larger corpus (about 6 times aslarge as Hindle et al?s) did not compensate forour lower-quality statistics.6 Related WorkOther work combining supervised and unsu-pervised learning for parsing includes (Char-niak, 1997), (Johnson and Riezler, 2000), and(Schmid, 2002).
These papers present inte-grated formal frameworks for incorporating in-formation learned from unlabeled corpora, butthey do not explicitly address PP and RC at-tachment.
The same is true for uncorrectedcolearning in (Hwa et al, 2003).Conversely, no previous work on PP and RCattachment has integrated specialized ambi-guity resolution into parsing.
For example,(Toutanova et al, 2004) present one of thebest results achieved so far on the WSJ PPset: 87.5%.
They also integrate supervisedand unsupervised learning.
But to our knowl-edge, the relationship to parsing has not beenexplored before ?
even though application toparsing is the stated objective of most work onPP attachment.5However, the baseline is similarly high for the PPproblem if the most likely attachment is chosen perpreposition: 72.2% according to (Collins and Brooks,1995).With the exception of (Hindle and Rooth,1993), most unsupervised work on PP attach-ment is based on superficial analysis of theunlabeled corpus without the use of partialparsing (Volk, 2001; Calvo et al, 2005).
Webelieve that dependencies offer a better basisfor reliable disambiguation than cooccurrenceand fixed-phrase statistics.
The difference to(Hindle and Rooth, 1993) was discussed abovewith respect to analysing the unlabeled cor-pus.
In addition, the decision procedure pre-sented here is different from Hindle et al?s.LBD uses more context and can, in princi-ple, accommodate arbitrarily large contexts.However, an evaluation comparing the perfor-mance of the two methods is necessary.The LBD model can be viewed as a back-off model that combines estimates from sev-eral ?backoffs?.
In a typical backoff model,there is a single more general model to backoff to.
(Collins and Brooks, 1995) also presenta model with multiple backoffs.
One of its vari-ants computes the estimate in question as theaverage of three backoffs.
In addition to themaximum used here, testing other combina-tion strategies for the MI values in the lattice(e.g., average, sum, frequency-weighted sum)would be desirable.
In general, MI has notbeen used in a backoff model before as far aswe know.Previous work on relative clause attachmenthas been supervised (Siddharthan, 2002a; Sid-dharthan, 2002b; Yeh and Vilain, 1998).6(Siddharthan, 2002b)?s accuracy for RC at-tachment is 76.5%.77 ConclusionPrevious work on specific types of ambiguities(like RC and PP) has not addressed the in-tegration of specific resolution algorithms intoa generic statistical parser.
In this paper, wehave shown for two types of ambiguities, rel-ative clause and prepositional phrase attach-ment ambiguity, that integration into a sta-tistical parser is possible and that the com-6Strictly speaking, our experiments were not com-pletely unsupervised since the default value and themost frequent attachment were determined based onthe development set.7We attempted to recreate Siddharthan?s trainingand test sets, but were not able to based on the de-scription in the paper and email communication withthe author.31bined system performs better than either com-ponent by itself.
However, for PP attachmentthis only holds for small training set sizes.
Forlarge training sets, we could only show an im-provement for RC attachment.Surprisingly, we only found a small effectof the size of the unlabeled corpus on disam-biguation performance due to the noisiness ofstatistics extracted from raw text.
Once theunlabeled corpus has reached a certain size (5-10 million words in our experiments) combinedperformance does not increase further.The results in this paper demonstrate thatthe baseline of a state-of-the-art lexicalizedparser for specific disambiguation problemslike RC and PP is quite high compared torecent results for stand-alone PP disambigua-tion.
For example, (Toutanova et al, 2004)achieve a performance of 87.6% for a train-ing set of about 85% of WSJ.
That num-ber is not that far from the 82.8% achievedby Collins?
parser in our experiments whentrained on 50% of WSJ.
Some of the super-vised approaches to PP attachment may haveto be reevaluated in light of this good perfor-mance of generic parsers.ReferencesMichaela Atterer and Hinrich Schu?tze.
2006.
Alattice-based framework for enhancing statisti-cal parsers with information from unlabeled cor-pora.
In CoNLL.Daniel M. Bikel.
2004.
Intricacies of Collins?parsing model.
Computational Linguistics,30(4):479?511.Hiram Calvo, Alexander Gelbukh, and Adam Kil-garriff.
2005.
Distributional thesaurus vs.WordNet: A comparison of backoff techniquesfor unsupervised PP attachment.
In CICLing.Eugene Charniak.
1997.
Statistical parsing witha context-free grammar and word statistics.
InAAAI/IAAI, pages 598?603.Michael Collins and James Brooks.
1995.
Prepo-sitional attachment through a backed-off model.In Third Workshop on Very Large Corpora.
As-sociation for Computational Linguistics.Michael Collins.
1997.
Three generative, lexi-calised models for statistical parsing.
In ACL.Donald Hindle and Mats Rooth.
1993.
Structuralambiguity and lexical relations.
ComputationalLinguistics, 19(1):103?120.Rebecca Hwa, Miles Osborne, Anoop Sarkar, andMark Steedman.
2003.
Corrected co-trainingfor statistical parsers.
In Workshop on the Con-tinuum from Labeled to Unlabeled Data in Ma-chine Learning and Data Mining, ICML.Mark Johnson and Stefan Riezler.
2000.
Ex-ploiting auxiliary distributions in stochasticunification-based grammars.
In NAACL.David D. Lewis, Yiming Yang, Tony G. Rose, andFan Li.
2004.
RCV1: A new benchmark collec-tion for text categorization research.
J. Mach.Learn.
Res., 5.Dekang Lin.
1998.
Dependency-based evaluationof MINIPAR.
In Workshop on the Evaluation ofParsing Systems, Granada, Spain.LingPipe.
2006. http://www.alias-i.com/lingpipe/.Lucene.
2006. http://lucene.apache.org.Mitchell P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large natural language corpus of English:the Penn treebank.
Computational Linguistics,19:313?330.Adwait Ratnaparkhi, Jeff Reynar, and SalimRoukos.
1994.
A maximum entropy model forprepositional phrase attachment.
In HLT.Helmut Schmid.
2002.
Lexicalization of proba-bilistic grammars.
In Coling.Advaith Siddharthan.
2002a.
Resolving attach-ment and clause boundary ambiguities for sim-plifying relative clause constructs.
In StudentResearch Workshop, ACL.Advaith Siddharthan.
2002b.
Resolving relativeclause attachment ambiguities using machinelearning techniques and wordnet hierarchies.
In4th Discourse Anaphora and Anaphora Resolu-tion Colloquium.Kristina Toutanova, Christopher D. Manning, andAndrew Y. Ng.
2004.
Learning random walkmodels for inducing word dependency distribu-tions.
In ICML.Martin Volk.
2001.
Exploiting the WWW as acorpus to resolve pp attachment ambiguities.
InCorpus Linguistics 2001.Web Appendix.
2006. http://www.ims.uni-stuttgart.de/?schuetze/colingacl06/apdx.html.Ian H. Witten, Alistair Moffat, and Timothy C.Bell.
1999.
Managing Gigabytes: Compressingand Indexing Documents and Images.
MorganKaufman.Alexander S. Yeh and Marc B. Vilain.
1998.
Someproperties of preposition and subordinate con-junction attachments.
In Coling.32
