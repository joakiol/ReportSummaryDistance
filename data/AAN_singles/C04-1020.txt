Representing discourse coherence: A corpus-based analysisFlorian WOLFMIT NE20-448Cambridge, MA 02139, USAfwolf@mit.eduEdward GIBSONMIT NE20-459Cambridge, MA, 02139, USAegibson@mit.eduAbstractWe present a set of discourse structure relationsthat are easy to code, and develop criteria for anappropriate data structure for representing theserelations.
Discourse structure here refers toinformational relations that hold between sentencesin a discourse (cf.
Hobbs, 1985).
We evaluatedwhether trees are a descriptively adequate datastructure for representing coherence.
Trees arewidely assumed as a data structure for representingcoherence but we found that more powerful datastructures are needed: In coherence structures ofnaturally occurring texts, we found many differentkinds of crossed dependencies, as well as manynodes with multiple parents.
The claims aresupported by statistical results from a database of135 texts from the Wall Street Journal and the APNewswire that were hand-annotated withcoherence relations, based on the annotationschema presented in this paper.1 IntroductionAn important component of natural languagediscourse understanding and production is having arepresentation of discourse structure.
A coherentlystructured discourse here is assumed to be acollection of sentences that are in some relation toeach other.
This paper aims to present a set ofdiscourse structure relations that are easy to code,and to develop criteria for an appropriate datastructure for representing these relations.Discourse structure relations here refer toinformational relations that hold between sentencesor other non-overlapping segments in a discoursemonologue.
That is, discourse structure relationsreflect how the meaning conveyed by onediscourse segment relates to the meaning conveyedby another discourse segment (cf.
Hobbs, 1985;Marcu, 2000; Webber et al, 1999).Accounts of discourse structure vary greatly withrespect to how many discourse relations theyassume, ranging from two (Grosz & Sidner, 1986)to over 400 different coherence relations, reportedin Hovy and Maier (1995).
However, Hovy andMaier (1995) argue that taxonomies with morerelations represent subtypes of taxonomies withfewer relations.
This means that differenttaxonomies can be compatible with each other.We describe an account with a small number ofrelations in order to achieve more generalizablerepresentations of discourse structures; however,the number is not so small that informationalstructures that we are interested in are obscured.The next section will describe in detail the set ofcoherence relations we use, which are mostlybased on Hobbs (1985).
Additionally, we try tomake as few a priori theoretical assumptions aboutrepresentational data structures as possible.
Theseassumptions will be outlined in the next section.Importantly, however, we do not assume a treedata structure to represent discourse coherencestructures.
In fact, a major goal of this paper is toshow that trees do not seem adequate to representdiscourse structures.2 Collecting a database of texts annotatedwith coherence relationsThis section describes (1) how we definediscourse segments, (2) which coherence relationswe used to connect the discourse segments, and (3)how the annotation procedure worked.2.1 Discourse segmentsDiscourse segments can be defined as non-overlapping spans of prosodic units (Hirschberg &Nakatani, 1996), intentional units (Grosz & Sidner,1986), phrasal units (Lascarides & Asher, 1993), orsentences (Hobbs, 1985).
We adopted a sentenceunit-based definition of discourse segments.However, we also assume that contentfulcoordinating and subordinating conjunctions (cf.Table 1) can delimit discourse segments.2.2 Coherence relationsWe assume a set of coherence relations that issimilar to that of Hobbs (1985) and Kehler (2002).Table 1 shows the coherence relations we assume,along with contentful conjunctions that can signalthe coherence relation.cause-effect becauseviolated expectation although; butcondition if?then; as long assimilarity (and) similarlycontrast but; howeverelaboration also, furthermoreattribution ?said, according to?temporal sequence before; afterwardsTable 1.
Coherence relations with contentfulconjunctions for determining coherence relations.Below are examples of each coherence relation.
(1) Cause-Effect[There was bad weather at the airport]a [and soour flight got delayed.
]b(2) Violated Expectation[The weather was nice]a [but our flight gotdelayed.
]b(3) Condition[If the new software works,]a [everyone will behappy.
]b(4) Similarity[There is a train on Platform A.
]a [There isanother train on Platform B.
]b(5) Contrast[John supported Bush]a [but Susan opposedhim.
]b(6) Elaboration[A probe to Mars was launched this week.
]a [TheEuropean-built ?Mars Express?
is scheduled toreach Mars by late December.
]b(7) Attribution[John said that]a [the weather would be nicetomorrow.
]b(8) Temporal Sequence[Before he went to bed,]a [John took a shower.
]bThe same relation, illustrated by (9), is anepiphenomenon of assuming contiguous distinctelements of text.
(a) is the first segment and (c) isthe second segment of what is actually one singlediscourse segment, separated by the interveningdiscourse segment (b), which is in an attributionrelation with (a) (and therefore also with (c), since(a) and (c) are actually one single discoursesegment).
(9) Same[The economy,]a [according to some analysts,]b[is expected to improve by early next year.
]cCause-effect, violated expectation, condition,elaboration, temporal sequence, and attributionare asymmetrical or directed relations, whereassimilarity, contrast, temporal sequence, and sameare symmetrical or undirected relations (Mann &Thompson, 1988; Marcu, 2000).
The directions ofasymmetrical or directed relations are as follows:cause ?
effect for cause-effect; cause ?
absenteffect for violated expectation; condition ?consequence for condition; elaborating ?elaborated for elaboration, and source ?
attributedfor attribution.2.3 Coding procedureIn order to code the coherence relations of a text,annotators used a procedure consisting of threesteps.
In Step One, a text is segmented intodiscourse segments as described above.
In StepTwo, adjacent discourse segments that aretopically related are grouped together.
Forexample, if a text discusses inventions ininformation technology, there could be groups of afew discourse segments each talking aboutinventions by specific companies.
There mightalso be subgroups of several discourse segmentseach talking about specific inventions at specificcompanies.
Thus, marking groups determines apartially hierarchical structure for the text.
In StepThree, coherence relations are determined betweendiscourse segments and groups of discoursesegments.
Each previously unconnected (group of)discourse segment(s) is tested to see if it connectsto any of the (groups of) discourse segments in thealready existing representation of discoursestructure.In order to help determine the coherence relationbetween (groups of) discourse segments, the(groups of) discourse segments underconsideration are connected with a contentfulconjunction like the ones shown in Table 1.
Ifusing a contentful conjunction to connect (groupsof) discourse segments results in an acceptablepassage, this is used as evidence that the coherencerelation corresponding to the contentfulconjunction holds between the (groups of)discourse segments under consideration.2.4 Statistics on annotated databaseIn order to evaluate hypotheses aboutappropriate data structures for representingcoherence structures, we annotated 135 texts, fromthe Wall Street Journal 1987-1989 and the APNewswire 1989 (Harman & Liberman, 1993), withthe coherence relations described above.
For the135 texts, the mean number of words was 545(min.
: 161; max.
: 1409; median: 529), the meannumber of discourse segments was 61 (min.
: 6;max.
: 143; median: 60).Each text was independently annotated by twoannotators.
In order to determine inter-annotatoragreement for the database of annotated texts, wecomputed kappa statistics (Carletta, 1996).
For allannotations of the 135 texts, the agreement was88.45%, per chance agreement was 24.86%, andkappa was 84.63%.
Annotator agreement did notdiffer by text length (?2 = 1.27; p < 0.75), arclength (?2 < 1), or kind of coherence relation (?2 <1).3 Data structures for representing coherencerelationsMost accounts of discourse coherence assumetree structures to represent coherence relationsbetween discourse segments in a text (Carlson etal., 2002; Corston-Oliver, 1998; Lascarides &Asher, 1993; Longacre, 1983; Grosz & Sidner,1986; Mann & Thompson, 1988; Marcu, 2000;Polanyi, 1988; van Dijk & Kintsch, 1983; Walker,1998; Webber et al, 1999).
Other accountsassume less constrained graphs (Hobbs, 1985).The proponents of tree structures argue that treesare easier to formalize and derive than lessconstrained graphs (Marcu, 2000).
We testedwhether coherence structures of naturallyoccurring texts can be represented by trees, i.e.
ifthese structures are free of crossed dependencies ornodes with multiple parents.
However, we found alarge number of both crossed dependencies as wellas nodes with multiple parents in the coherencestructures of naturally occurring texts.
Thereforewe argue for less constrained graphs as anappropriate data structure for representingcoherence, where an ordered array of nodesrepresents discourse segments and labeled directedarcs represent the coherence relations that holdbetween these discourse segments.1  The followingtwo sections will give examples of coherencestructures with crossed dependencies and nodeswith multiple parents.
The section after that willpresent statistical results from our database of 135coherence-annotated texts.3.1 Crossed dependenciesCrossed dependencies are rampant and occur inmany different forms in the coherence structures ofnaturally occurring texts.
Here we will give someexamples.
Consider the text passage in (10).1 Other accounts also acknowledge examples thatcannot be represented in tree structures (Webber et al,1999).
In order to maintain trees, these accountsdistinguish non-anaphoric coherence structures,represented in a tree, and anaphoric coherencestructures, which are not subject to tree constraints.However, e.g., Haliday & Hasan (1976) stress theimportance of anaphoric links as a cue for coherencestructures.
Therefore, by Occam?s Razor, we assume asingle level of representation for coherence rather thanmultiple levels.Figure 1 represents the coherence relations in (10).The arrowheads of the arcs represent directionalityfor asymmetrical relations (elaboration) andbidirectionality for symmetrical relations(contrast).
(10) Example text (from SAT practicing materials)0.
Schools tried to teach students history ofscience.1.
At the same time they tried to teach them howto think logically and inductively.2.
Some success has been reached in the first ofthese aims.3.
However, none at all has been reached in thesecond.Figure 1.
Coherence graph for (10).The coherence structure for (10) can be derivedas follows:  there is a contrast relation between 0and 1; 0 and 1 describe teaching different things tostudents.
There is another contrast relationbetween 2 and 3; 2 and 3 describe varying degreesof success (some vs. none).
2 provides moredetails (the degree of success) about the teachingdescribed in 0, so there is an elaboration relationbetween 2 and 0.
Furthermore, in anotherelaboration relation, 3 provides more details (thedegree of success) about the teaching described in1.
In the resultant coherence structure for (10),there is a crossed dependency between {2, 0} and{3, 1}.In order to be able to represent the crosseddependency in the coherence structure of (10) in atree without violating validity assumptions abouttree structures, one might consider augmenting atree with feature propagation (Shieber, 1986) orwith a coindexation mechanism (Chomsky, 1973).But the problem is that both the tree structure itselfas well as the features and coindexations representthe same kind of information (coherence relations).It is unclear how one could decide which part of atext coherence structure should be represented bythe tree structure and which by the augmentation.As pointed out above, coherence structures ofnaturally occurring texts contain many differentkinds of crossed dependencies.
This is importantbecause it means that one cannot simply makespecial provisions to account for list-like structureslike the structure of (10) and otherwise assume treestructures.
As an example of a non-list-likestructure with a crossed dependency (between {3,1} and {2, 0-1}), consider (11).contrelab elabcontr0 1 2 3(11) Example text0.
Susan wanted to buy some tomatoes1.
and she also tried to find some basil2.
because her recipe asked for theseingredients.3.
The basil would probably be quite expensiveat this time of the year.Figure 2.
Coherence graph for (11).The coherence structure for (11) can be derivedas follows:  there is a parallel relation between 0and 1; 0 and 1 both describe shopping for groceryitems.
There is a cause-effect relation between 2and 0-1; 2 describes the cause for the shoppingdescribed by 0 and 1.
Furthermore, there is anelaboration relation between 3 and 1; 3 providesdetails about the basil in 1.
(12) from the AP Newswire1989 corpus is anexample with a similar structure:(12) Example text (from text ap890109-0012)0.
The flight Sunday took off from HeathrowAirport at 7:52pm1.
and its engine caught fire 10 minutes later,2.
the Department of Transport said.3.
The pilot told the control tower he had theengine fire under control.Figure 3.
Coherence graph for (12).The coherence structure for (12) can be derivedas follows: 1 and 0 are in a temporal sequencerelation; 0 describes the takeoff that happensbefore the engine fire described by 1 occurs.
2 and0-1 are in an attribution relation; 2 mentions thesource of what is said in 0-1.
3 and 1 are in anelaboration relation; 3 provides more detail aboutthe engine fire in 1.
The resulting coherencestructure, shown in Figure 3, contains a crosseddependency between {3, 1} and {2, 0-1}.3.2 Nodes with multiple parentsIn addition to crossed dependencies, manycoherence structures of natural texts include nodeswith multiple parents.
Such nodes cannot berepresented in tree structures.
For instance, in thecoherence structure of (10), nodes 0 and 2 havetwo parents.
Similarly, in the coherence structureof (13) from the AP Newswire 1989, node 1 hasone attribution and one condition ingoing arc (cf.Figure 4).
(13) Example text (from text ap890103-0014)0.
?Sure I?ll be polite,?1.
promised one BMW driver2.
who gave his name only as Rudolf.3.
?As long as the trucks and the timid stay outof the left lane.
?Figure 4.
Coherence graph for (13).The coherence structure for (13) can be derivedas follows:  1 states the source of what is stated in0 and in 3, so there are attribution relationsbetween 1 and 0 and 1 and 3 respectively.
2 and 1are in an elaboration relation; 2 providesadditional detail about the BMW driver in 1.
3 and0 are in a condition relation; 3 states the BMWdriver?s condition for being polite, stated in 0; thecondition relation is also indicated by the phrase?as long as?.4 Statistics4.1 Crossed dependenciesAn important question is how frequent thephenomena discussed in the previous sections are.The more frequent they are, the more urgent theneed for a data structure that can adequatelyrepresent them.This section reports counts on crosseddependencies in the annotated database of 135texts.
In order to track the frequency of crosseddependencies for the coherence structure graph ofeach text, we counted the minimum number of arcsthat would have to be deleted in order to make thecoherence structure graph free of crosseddependencies (i.e.
the minimum number of arcsthat participate in crossed dependencies).
Theexample graph in Figure 10 illustrates this process.This graph contains the following crosseddependencies: (1, 3} crosses with {0, 2} and {2,4}.
By deleting {1, 3}, both crossed dependenciescan be eliminated.
The crossed dependency countfor the graph in Figure 5 is thus ?one?.Figure 5.
Example graph with crosseddependencies.On average for the 135 annotated texts, 12.5% ofarcs in a coherence graph have to be deleted inorder to make the graph free of crosseddependencies (min.
: 0%; max.
: 44.4%; median:10.9%).
Seven texts out of 135 had no crossed0 1 2 3 4condelabattr0 1 2 3attrce elabpar0 1 2 30-1elabts0 1 2 30-1attrdependencies.
The mean number of arcs for thecoherence graphs of these texts was 36.9 (min.
: 8;max.
: 69; median: 35).
The mean number of arcsfor the other 128 coherence graphs (those withcrossed dependencies) was 125.7 (min.
: 20; max.
:293; median: 115.5).
Thus, the graphs with nocrossed dependencies have significantly fewer arcsthan those graphs that have crossed dependencies(?2=15330.35; p < 10-4).
Text length is hence alikely explanation for why these seven texts had nocrossed dependencies.Linear regressions show that the more arcs agraph has, the higher the number of crosseddependencies (R2 = 0.39; p < 10-4).
Also, thelonger a text, the more crossed dependencies are inits coherence structure graph (for text length indiscourse segments: R2 = .29, p < 10-4; for textlength in words: R2 = .24, p < 10-4).Another important question is whether certaintypes of coherence relations participate more orless frequently in crossed dependencies than othertypes of coherence relations.
In other words, thequestion is whether the frequency distribution overtypes of coherence relations is different for arcsparticipating in crossed dependencies compared tothe overall frequency distribution over types ofcoherence relations in the whole database.Results from our database indicate that theoverall distribution over types of coherencerelations participating in crossed dependencies isnot different from the distribution over types ofcoherence relations overall.
This is confirmed by alinear regression, which shows a significantcorrelation between the two distributions ofpercentages (R2 = 0.84; p < .0001).
Notice that theoverall distribution includes only arcs with lengthgreater than one, since arcs of length one could notparticipate in crossed dependencies.However, some types of coherence relationsoccur considerably less frequently in crosseddependencies than overall in the database.
Theproportion of same relations is 15.21 times greater,and the percentage of condition relations is 5.93times greater overall than in crossed dependencies.We do not yet understand the reason for thesedifferences, and plan to address this question infuture research.Another question is how great the distance or arclength typically is between sentences thatparticipate in crossed dependencies.
It is possible,for instance, that crossed dependencies primarilyinvolve long-distance arcs and that more localcrossed dependencies are disfavored.
However,the distribution over arc lengths is practicallyidentical for the overall database and for coherencerelations participating in crossed dependencies (R2= 0.937; p < 10-4), with short-distance relationsbeing more frequent than long-distance relationsfor coherence relations overall as well as for thoseparticipating in crossed dependencies.
The arclengths are normalized in order to take into accountthe length of a text; the absolute length of an arc isdivided by the maximum length that that arc couldhave, given its position in a text.
Furthermore, weexclude arcs of (absolute) length 1 from the overalldistribution, since such arcs could not participate incrossed dependencies.Taken together, statistical results on crosseddependencies suggest that crossed dependenciesare too frequent to be ignored by accounts ofcoherence.
Furthermore, the results suggest thatany type of coherence relation can participate in acrossed dependency.
However, there are somecases where knowing the type of coherencerelation that an arc represents can be informative asto how likely that arc is to participate in a crosseddependency.
The statistical results reported herealso suggest that crossed dependencies occurprimarily locally, as evidenced by the distributionover lengths of arcs participating in crosseddependencies.4.2 Nodes with multiple parentsAbove we provided examples of coherencestructure graphs that contain nodes with multipleparents.
Nodes with multiple parents are anotherreason why trees are inadequate for representingnatural language coherence structures.
The meanin-degree (=mean number of parents) of all nodesin the investigated database of 135 texts is 1.6(min.
: 1; max.
: 12; median: 1).
41% of all nodes inthe database have an in-degree greater than 1.
Thissuggests that even if a mechanism could be derivedfor representing crossed dependencies in(augmented) tree graphs, nodes with multipleparents present another significant problem fortrees representing coherence structures.
Resultsfrom our database indicate that the overalldistribution over types of coherence relationsingoing to nodes with multiple parents issignificantly correlated with the distribution overtypes of coherence relations overall (R2 = 0.967; p< 10-4).As for crossed dependencies, we also comparedarc lengths.
Here, we compared the length of arcsthat are ingoing to nodes with multiple parents tothe overall distribution of arc length.
Again, wecompared normalized arc lengths.
By contrast tothe comparison for crossed dependencies, weincluded arcs of (absolute) length 1 because sucharcs can be ingoing to nodes with either single ormultiple parents.
The distribution over arc lengthsis practically identical for the overall database andfor arcs ingoing to nodes with multiple parents (R2= 0.993; p < 10-4), suggesting a strong locality biasfor coherence relations overall as well as for thoseparticipating in crossed dependencies.In sum, statistical results on nodes with multipleparents suggest that they are a frequentphenomenon, and that they are not limited tocertain kinds of coherence relations.
Additionally,the statistical results reported here suggest thatingoing arcs to nodes with multiple parents areprimarily local.5 ConclusionThe goals of this paper have been to present a setof coherence relations that are easy to code, and toillustrate the inadequacy of trees as a data structurefor representing discourse coherence structures.We have developed a coding scheme with highinter-annotator reliability and used that scheme toannotate 135 texts with coherence relations.
Aninvestigation of these annotations has shown thatdiscourse structures of naturally occurring textscontain various kinds of crossed dependencies aswell as nodes with multiple parents.
Bothphenomena cannot be represented using trees,which implies that existing databases of coherencestructures that use trees are not descriptivelyadequate.Our statistical results suggest that crosseddependencies and nodes with multiple parents arenot restricted phenomena that could be ignored oraccommodated with a few exception rules.Furthermore, even if one could find a way ofaugmenting tree structures to account for crosseddependencies and nodes with multiple parents,there would have to be a mechanism for unifyingthe tree structure with the augmentation features.Thus, in terms of derivational complexity, treeswould just shift the burden from having to derive aless constrained data structure to having to derive aunification of trees and features or coindexation.Because trees are neither a descriptivelyadequate data structure for representing coherencestructures nor easier to derive, we argue for lessconstrained graphs as a data structure forrepresenting coherence structures.
Such lessconstrained graphs would have the advantage ofbeing able to adequately represent coherencestructures in one single data structure (cf.
Skut etal., 1997).
Furthermore, they are at least notharder to derive than (augmented) tree structures.The greater descriptive adequacy might in factmake them easier to derive.
However, this is stillan open issue and will have to be addressed infuture research.ReferencesJean Carletta.
1996.
Assessing agreement onclassifi-cation tasks: the kappa statistic.Computational Linguistics, 22(2): 249-254.Lynn Carlson, Daniel Marcu, and Mary E.Okurowski.
2002.
RST Discourse Treebank.Philadelphia, PA: LDC.Noam Chomsky.
1973.
Conditions ontransformations.
In: Anderson, S. & Kiparsky,P., eds., A Festschrift for Morris Halle, 232-286.New York: Holt, Rinehart and Winston.Simon Corston-Oliver.
1998.
Computingrepresentations of the structure of writtendiscourse.
Microsoft Research TechnicalReport MSR-TR-98-15.
Redmont, WA, USA.Barbara J. Grosz and Candace L. Sidner.
1986.Attention, intentions, and the structure ofdiscourse.
Computational Linguistics, 12(3):175-204.Michael A.K.
Haliday and Ruqaiya Hasan.
1976.Cohesion in English.
Longman, London.Donna Harman and Mark Liberman.
1993.TIPSTER complete.
Philadelphia, PA: LDC.Marti Hearst.
1997.
TextTiling: Segmenting textinto multi-paragraph subtopic passages.Computational Linguistics, 23(1): 33-64.Julia Hirschberg and Christine H. Nakatani.
1996.A prosodic analysis of discourse segments indirection-giving monologues.
In: Proceedingsof the 34th Annual Meeting of the ACL, 286-293.
Santa Cruz, CA, USA.Jerry R. Hobbs.
1985.
On the coherence andstructure of discourse.
CSLI Technical Report85-37.
Stanford, CA, USA.Eduard Hovy and Elisabeth Maier.
1995.Parsimonious or profligate: How many andwhich discourse relations?
Unpublishedmanuscript.Andrew Kehler.
2002.
Coherence, reference, andthe theory of grammar.
Stanford, CA: CSLIPublications.Alex Lascarides and Nicholas Asher.
1993.Temporal interpretation, discourse relations, andcommon sense entailment.
Linguistics andPhilosophy, 16(5): 437-493.Robert E. Longacre.
1983.
The grammar ofdiscourse.
New York: Plenum Press.William C. Mann and Sandra A. Thompson.
1988.Rhetorical structure theory: Toward a functionaltheory of text organization.
Text, 8(3): 243-281.Daniel Marcu.
2000.
The theory and practice ofdiscourse parsing and summarization.Cambridge, MA: MIT Press.Mitchell Marcus, Grace Kim, Mary A.Marcinkiewicz, Robert MacIntyre, Ann Bies,Mark Ferguson, Karen Katz and BrittaSchasberger.
1994.
The Penn Treebank:Annotating predicate argument structure.
In:Proceedings of the ARPA Human LanguageTechnology Workshop.
San Francisco, CA:Morgan Kaufman.Livia Polanyi.
1988.
A formal model of thestructure of discourse.
Journal of Pragmatics,12: 601-638.Stuart M. Shieber.
1985.
Evidence against thecontext-freeness of natural language.Linguistics and Philosophy, 8: 333-343.Stuart M. Shieber.
1986.
An introduction tounification-based approaches to grammar.Stanford University: CSLI Lecture Notes 4.Wojciech Skut, Brigitte Krenn, Thorsten Brantsand Hans Uszkoreit.
1997.
An annotationscheme for free word order languages.
In:Proceedings of the 5th ANLP Conference.Washington, DC, USA.Teun A. van Dijk and Walter Kintsch.
1983.Strategies of discourse comprehension.
NewYork: Academic.Marilyn A. Walker.
1998.
Centering, anaphoraresolution, and discourse structure.
In: Prince,E., Joshi, A.K.
& Walker, M.A., eds., CenteringTheory in discourse.
Oxford: Oxford UniversityPress.Bonnie L. Webber, Alastair Knott, Stone, M. &Joshi, A.K.
1999.
Discourse relations: Astructural and presuppositional account usinglexicalized TAG.
In: Proceedings of the 37thAnnual Meeting of the ACL, 41-48.
CollegePark, MD, USA.
