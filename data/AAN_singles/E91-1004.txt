7 earl: A P robab i l i s t i cDav id  M.  MagermanCS Del)a, r tmcntS ta .
hn'd U, f ivcrs i tyS tanford ,  CA 94305magc,'n mn(i~cs.sl, a.n ford.c(I uChar t  Parser*Mitche l l  P .
MarcusCIS l )epar tment\[.lnivcrsil,y of l )cnnsylvania.P i f i ladelphia.
,  PA 19104mi tch  ?21 in c.(:is, u I)enn .eduAbst ractThis i)al)er describes a Ilatural language i)ars -ing algorith,n for unrestricted text which uses aprol)al)ility-I~ased scoring function to select the"l)est" i)arse of a sclfl,ence.
The parser, T~earl,is a time-asynchronous I)ottom-ul) chart parserwith Earley-tyl)e tol)-down prediction which l)ur -sues the highest-scoring theory iu the chart, wherethe score of a theory represents im extent o  whichthe context of the sentence predicts that interpre-tation.
This parser dilrers front previous attemi)tsat stochastic parsers in that it uses a richer form ofconditional prol)alfilities I)ased on context o l)re-diet likelihood.
T>carl also provides a frameworkfor i,lcorporating the results of previous work ini)art-of-spe(;ch assignrlmn|., unknown word too<l-ois, and other probal)ilistic models of lingvisticfeatures into one parsing tool, interleaving thesetechniques instead of using the traditional pipelinea,'chitecture, lu preliminary tests, "Pearl has I)ee.,ist,ccessl'ul at resolving l)art-of-speech and word (insl)eech processing) ambiguity, d:etermining cate-gories for unknown words, and selecting correctparses first using a very loosely fitting cove,'inggrammar, lIn t roduct ionAll natural language grammars are alnbiguous.
Eventightly fitting natural anguage grammars are ambigu-ous in some ways.
Loosely fitting grammars, which arenecessary for handling the variability and complexityof unrestricted text and speech, are worse.
Tim stan-dard technique for dealing with this ambiguity, pruning?This work was p,~rtially supported by DARPA grantNo.
N01114-85-1(0018, ONR contract No.
N00014-89-C-0171 by DARPA and AFOSR jointly under grant No.AFOSR-90-0066, and by ARO grant No.
DAAL 03-89-C(1031 PRI.
Special thanks to Carl Weir and Lynettellirschman at Unisys for their valued input, guidance andsupport.I'Fhe grammar used for our experiments i  the string~ra.mmar used in Unisys' PUNI ) IT  natura.I language iin-dt'rsl.a ndi n/4 sysl.tml.gra.nunars I)y hand, is painful, time-consuming, andusually arbitrary.
The solution which many peoplehave proposed is to use stochastic models to grain sta-tistical grammars automatically from a large corpus.Attempts in applying statistical techniques to nat-ura, I iangt, age parsi,lg have exhibited varying degreesof success.
These successful and unsuccessful attemptshave suggested to us that:.
Stochastic techniques combined with traditional lin-guistic theories can  (and indeed must) provide a so-lull|on to the natural language understanding prob-lem.
* In order for stochastic techniques to be effective,they must be applied with restraint (poor estimatesof context arc worse than none\[7\]).- Interactive, interleaved architectvres are preferableto pipeline architectures in NLU systems, becausethey use more of the available information in thedecision-nmkiug process.Wc have constructed a stoch~tic parser,/)earl,  whichis based on these ideas.The development of the 7~earl parser is an effort tocombine the statistical models developed recently intoa single tool which incorporates all of these models intothe decisiou-making component of a parser, While wehave only attempted to incorporate a few simple sta-tistical models into this parser, ~earl is structured ina way which allows any nt, mber of syntactic, semantic,and ~other knowledge sources to contribute to parsingdecisions.
The current implementation of "Pearl usesChurclFs part-of-speech assignment trigram model, asimple probabilistic unknown word model, and a con-d i t iona l  probability model for grammar ules based onpart-of-speech trigrams and parent rules.By combining multiple knowledge sources and usinga chart-parsing framework, 7~earl attempts to handlea number of difficult problems.
7%arl has the capa-bility to parse word lattices, an ability which is usefulin recognizing idioms in text processing, as well as inspeech processing.
The parser uses probabilistic train-ing from a corpus to disambiguate between grammati-cally ac(-i:ptal)h', structures, such ;m determining i)repo --15 -sitional l)hrase attachment and conjunction scope.
Fi-nally, ?earl maintains a well-formed substring I,ablewithin its chart to allow for partial parse retrieval.
Par-tial parses are usefid botll for error-message nerationaud for pro(-cssitlg lulgrattUllal,i('al or illCOllll)h;I,e .
'~;l|-I,(~llCes.ht i)reliluinary tests, ?ear l  has shown protnisillg re-suits in ha,idling part-of-speech ~ussignnlent,, preposi-t, ional I)hrase ;d,l, achnlcnl., ait(I Ilnknowlt wor(I catego-riza6on.
Trained on a corpus of 1100 sentences fromthe Voyager direction-linding system 2 and using thestring gra,ulm~r from l,he I)UNDIT l,aug,,age IhM,.r-sl.atJ(ling Sysl,cuh ?carl  correcl, y i)a.rse(I 35 out of/10 or88% of scIitellces ele('tcd frolu Voyager sentcil(:~}.~ tierused in the traini,lg data.
We will describe the detailsof this exl)crimelfl, al,cr.In this I)al)cr , wc will lirsl, explain our contribu-l, ion l,o the sl,ochastic ,nodels which are used in ?earl:a context-free granunar with context-sensitive condi-l, ional probal)ilities.
Then, we will describe the parser'sarchitecture and the parsing algorithtn, l"ina.lly, wewill give the results of some exi)erinlents we performedusing ?earl  which explore its capabilities.Us ing  Stat i s t i cs  to  ParseRecent work involving conl,ext-free a,.I context-sensitive probal)ilistic gramnlars I)rovide little hope forthe success of processing unrestricted text osing I)roba.-bilistic teclmiques.
Wo,'ks I)y C, Ititrao and Grishman\[3}and by Sharmau, .Iclinek, aml Merce,'\[12\] exhil)il, ac-cllracy I'atos Iowq;r than 50% using supervised train-iny.
Supervised trailfiug for probal)ilisl, ic C, FGs re-quires parsed corpora, which is very costly in time andman-power\[2\].l i l  otn" illw~sl, igatiolls, w,~ hav,~ Iliad(; two ol)s(~rval,iolmwhich al,tcinl)t to Cxl)laiit l.h(' lack-hlstt'r i)erfornmnceof statistical parsing tecluti(lUeS:?
Sinq)l~: llrol)al)ilistic ( :l,'(;s i)rovidc ycncTnl infornm-lion about how likely a constr0ct is going to appearanywhere in a sample of a language.
This averagelikelihood is often a poor estimat;e of probability.?
Parsing algorithnls which accumulate I)rol)abilitiesof parse theories by simply multiplying the,n over-penalize infrequent constructs.
?earl  avoids the first pitfall" by t,sing a context-sensitive conditional probability CFG, where cot ttextof a theory is determi,ted by the theories which pre-dicted it and the i)art-of-sl)eech sequences in the inputs,ml,ence.
To address the second issue, Pearl scoreseach theory by usi.g the geometric mean of Lhe con-textl,al conditional probalfilities of all of I.he theorieswhich have contributed to timt theory.
This is e(lt, iva-lent to using the sum of the logs of l.hese probal)ilities.~Spcclnl thanks to Victor Zue at Mlq" h)r the use of theSl)(:c(:h da.t;r from MIT's Voyager sysl, Clll.CFG wi th  context -sens i t i ve  cond i t iona lprobab i l i t iesIn a very large parsed corpus of English text, onefinds I, Imt, I,be most freq.ently occurring noun phrasestructure in I, Iw text is a nomt plu'asc containing adeterminer followed by a noun.
Simple probabilisticCFGs dictate that, given this information, "determinernoun" should be the most likely interpretation of aIlOUn phrase.Now, consider only those noun phrases which oc-cur as subjects of a senl,ence.
In a given corpus, younlighl, liml that pronouns occur just as fre(luently as"lletermincr nou,,"s in the subject I)ositiou.
This typeof information can easily be cai)tnred by conditionall)robalfilities.Finally, tmsume that the sentence begins with a pro-noun followed by a verb.
In l.his case, it is quite clearthat, while you can probably concoct a sentence whichfit, s this description and does not have a pronoun fora subject, I,he first, theory which you should pursue isone which makes this hypothesis.The context-sensitive conditional probabilities which?earl  uses take into account he irnmediate parent ofa theory 3 and the part-of-speech trigram centered atthe beginning of the theory.For example, consider the sentence:My first love was named ?earl .
(no subliminal propaganda intended)A theory which tries to interpret "love" as a verb willbe scored based ou the imrl,-of-speecll trigranl "adjec-tive verb verb" and the parent theory, probably "S --+NP VP."
A theory which interprets "love" as a nounwill be scored based on the trigram "adjective nounw~rl)."
AIl,llo.gll Io.xical prollabilities favor "love" asa verb, I, he comlitional i)robabilities will heavily favor"love" as a noun in tiffs context.
4Us ing  the  Geometr i c  Mean o f  TheoryScoresAccording to probability theory, the likelihood of twoindependent events occurring at the same time is theproduct of their individual probabilities.
Previous sta-tistical parsing techniques apply this definition to thecooceurrence of two theories in a parse, and claim thatthe likelihood of the two theories being correct is theproduct of the probabilities of the two theories.3The parent of a theory is defined as a theory with aCF rule which co.tains the left-hand side of tile theory.For instance, if "S ---, NP VP" and "NP --+ det n" are twogrammar rules, the first rule can be a parent of tile second,since tl,e left-hand side of tile second "NP" occurs in theright-hand side of the first rule.4In fact, tile part-of-speech tagging model which is Msoused in ~earl will heavily favor "love" as a noun.
We ignorethis behavior to demonstrate the benefits of the trigramco.ditioni.g.16-'l?his application of probal)ility theory ignores twovital observations el)out the domain of statistical pars-ing:?
Two CO,lstructs .occurring in the same sentence are,lot n,:ccssa,'ily indel)cndc.nt (and frequ~ml.ly are not).If the indel)el/de//e,, ;msuniption is violated, then tileprodl,ct of individual probabilities has no meaningwith ,'espect o the joint probability of two events.?
SiilCe sl,al,isl,i(:al l iarshig sllil't:rs froln Sl)ars,~ data,l irol i i l .
I) i l i l ,y esl, in latcs of low frequency evenl.s w i l li l sual ly  lie i i iaccurate estil i iaLes.
I,;xl, relue underesl, i-ili;i.I,l:s of  I, ll,~ l ike l ihood of  low frl~qlmlicy \[Welll.s wi l li)rolhl('e l i i is l~; idhig .ioint l i rohal i i l i l ,y  estiulates.Froln tiios~; ol iserval.
ioi is, w(; have de.l, er lnhled that  csti-lilal,hig.ioinl, liroha.I)ilil,ies of I,li(~ories usilig iliilividilallirohldJilil,ies is Leo dillicull, with the availalih.
', data.IvVe haw, foulid I,ha.I, the geoinel, ric niean of these prob-ahilit,y esl, inial,cs provides an accurate a.,~sl;ssiilellt of aIJll~Ol'y's vialiilil.y.The Actua l  Theory  Scor ing  Funct ionIn a departure front standard liractice, and perhapsagailisl.
I)el.l.er .iu(Ignienl,, we will inehlde a precise( Icsei ' i l l t ioi i  ( i f  I, he theory scoring funct io l i  used l iy'-Pearl.
This scoring fuiiction l,rics to soiw; some of thelirolih)lliS listed in lirevious at,telUlitS at tirobabilisticparsii,g\[.l\]\[12\]:?
Theory scores shouhl not deliend on thc icngth ofthe string which t, hc theory spans.?
~l)al'S(~ data  (zero-fr~:qllelicy eVl;lltS) ~llid evell zero-prolJahility ew;nts do occur, and shouhl not result inzero scoring Lheorics.?
Theory scores should not discrinfinate against un-likely COlistriicts wJl,'.n the context liredicts theln.The raw score of a theory, 0 is calculated by takiugI,he.
i)rodul:l, of the ?onditiona.I i)rol)ability of that the-ory's (',1"(i ride giw;il the conl,ext (whel'l ~, COlitelt is itI)iirl,-of-sl)(~ech I,rigraln a.n(I a l)areiit I,heol'y's rule) alidI, he score of tim I, rigrani:,5'C:r aw(0) = "P(r {tics I(/'oPl 1'2 ), ruic parent ) sc(pol,!
1)2 )llere, the score of a trigram is the product of themutual infornlation of the part-of-speech trigram, 5POPII~2, and tile lexical prol)ability of the word at theIoeat io i l  of Pi l ie ing assigiled that liart-of-specch pi .sIn the case of anlhiguil,y (part-of-speech ambiguity orinuitil)le parent theories), the inaxinuim value of thislirothict is used.
The score of a partial theory or a conl-I)lete theory is the geometric liieali of the raw scores ofall of the theories which are contained in that theory.'
The  l i i l l t i la l  iliforlll;ll.iOll el r ~ part-of-sl)eech tr igram,l l op ip i l  is (lelincd to lie li(|lillll/'2) where x is li l ly l i l l rt  -7 )( Pl izP1 )7)( I l l  ) 'of-speech.
See \[4\] for tintiler exlila.n,%l, ioli.GTlie trigrani .~coring funcl.ion actually ilsed by tileparser is SOill(:wh;il, tiler(: (:onllili(:al,t~d I, Ilall this.Theory  Length  Independence  This scoring func-tion, although heuristic in derivation, provides anlethod Ibr evaluating the value of a theory, regardlessof its length.
When a rule is first predicted (Earley-styh;), its score is just its raw score, which relireseutshow uiuch {,lie context predicts it.
llowever, when theparse process hypothesizes interpretations of tile sen-teuce which reinforce this theory, the geornetric nleanof all of the raw scorn of the rule's subtree is used,rcllrescnting the ow,rall likelihood or I.he i.heory giventhe coutcxt of the sentence.Low-freqlte l tcy Ew:nts  AII.hol,gll sonic statisticalnatural language aplili('ations enllAoy backing-off e.s-timatitm tcchni(lues\[ll\]\[5\] to handle low-freql,eneyevents, "Pearl uses a very sintple estilnation technique,reluctantly attributed to Chl,rcl,\[7\].
This techniqueestiniatcs the probability of au event by adding 0.5 toevery frequency count.
~ Low-scoring theories will bepredicted by the Earley-style parscr.
And, if no otherhypothesis i suggested, these theories will be pt, rsued.If a high scoring theory advauces a theory with a verylow raw score, the resulting theory's core will be thegeonletric nlean of all of the raw scores of theories con-tained in that thcory, and thus will I)e nluch higherthan the low-scoring theory's core.Example  o f  Scor ing Funct ion  As an example ofhow the conditional-probability-b<~sed scoring flinctionhandles anlbiguity, consider the sentenceFruit, flies like a banana.i,i the dontain of insect studies.
Lexical probabilitiesshould indicate that the word "flies" is niore likely tobe a plural noun than an active verb.
This informationis incorporated in the trigram scores, llowever, whenthe interliretationS --+ .
NP VPis proposed, two possible NPs will be parsed,NP ~ nolnl (fruit)all dNP -+ noun nouu (fruit flies).Sitlce this sentence is syntactically ambiguous, if thefirst hypothesis i tested first, the parser will interpretthis sentence incorrectly.ll0wever, this will not happen in this donlain.
Since"fruit flies" is a common idiom in insect studies, thescore of its trigram, noun noun verb, will be muchgreater than the score of the trigram, noun verb verb.Titus, not only will the lexical probability of the word"flies/verb" be lower than that of "flies/noun," but alsotile raw score of "NP  --+ noun (fruit)" will be lower than7We are not deliberately avoiding using ,'ill probabil-ity estinlatioll techniques, o,,ly those backillg-off tech-aiques which use independence assunlptions that frequentlyprovide misleading information when applied to naturalliillgU age.- 17 -that of "NP -+ nolln nolln (fruit flies)," because of thedifferential between the trigram score~s.So, "NP -+ noun noun" will I)e used first to advancethe "S --+ NI ) VP" rid0.. Further, even if the I)arsera(lva.llCeS I)ol,h NII hyliol,h(++ses, I,he "S --+ NP .
V I ' "rule IlSilig "N I j ---+ liOllll i iOlln" wil l  have a higher s(:orel, hau the "S --+ INIP .
V l  )'' rule using "NP  -+ notul.
"I n ter leaved Arch i tec ture  in Pear lThe interleaved architecture implemented in Pearl pro-vides uiany advantages over the tradil,ionai pilielinear('hil,~+.
(:l.ln'e, l iut  it, also iiil.rodu(-~,s c,:rl,a.ili risks.
I)('+-('iSiOllS abo l l t  word alld liarl,-of-sl)ee('h a ln l i igu i ty  ca.iiI)e dolaye(I until synl,acl, ic I)rocessiug can disanlbiguatel,h~;ni.
And,  using I,he al)llroprial,e score conibhia.tionflilicl,iolis, the scoring of  aliihigliOllS ('hoi(:es Call directI, li~ parser towards I, he most likely inl,erl)re.tal, ioii elli-cicutly.I lowevcr, with these delayed decisions COllieS a vasl,ly~Jlllal'g~'+lI sl'arch spa(:('. '
l ' \] le elf<;ctivelio.ss (if the i)arsi'.rdellen(Is on a, nla:ioril,y of  tile theories having very lowscores I)ased ou either uul ikely syntactic strllCtllres orlow scoring h lput  (SilCii as low scores from a speechrecognizer or low lexical I)robabilil,y).
hi exl:)eriulenl,swe have i)erforn}ed, tliis \]las been the case.The  Pars ing  A lgor i thmT'earl is a time-asynchronous I)ottom-up chart parserwith Earley-tyi)e top-down i)rediction.
The signifi-cant difference I)etween Pearl and non-I)robabilisticbol,tOllHI I) i)arsers is tha.t instead of COml)letely gener-at ing all grammatical interpretations of a word striug,Tcarl pursues i.he N highest-scoring incoml)lete theo-ries ill the chart  al.
each I);mS. Ilowcw~r, Pearl I)a.,'scswilhoul pruniny.
All, hough it is o l l ly  mlVallcing the Nhil~hest-scorhig \]iiieOlill)h~l.~" I,Jieories, it reta.his the lowerSCOl'illg tl leorics ill its agl~ll(la.
I f  I, he higher scorhlgth(,ories do not g(~lleral,e vial)It all,crnal.iw~s, the lowerSCOl'illg l, lteori~'s IIHly I)(~ IISOd Oil SIliiSC~tllmllt i)a.
'~scs.The liarsing alg(u'ithill begins with the inl)ut wordlati,ice.
An 11 x It cha.rl, is allocated, where It iS thehmgl, h of the Iongesl, word sl,rillg in l,lie lattice, l,?xicali'uh~s for I,he inl iut word lal.l, ice a, re inserted into thecha.rt.
Using Earley-tyl)e liredicLi6u, a st;ntence is pre-(licl.ed at, the beg inu i l ig  of tim SClitence, and all of thetheories which are I)re(licl.c(I l)y l, hat initial sentenceare inserted into the chart.
These inconll)lete thee-tics are scored accordiug to the context-sensitive con-ditional probabilities and the trigram part-of-speechnlodel.
The incollll)lel.e theories are tested in order byscore, until N theories are adwl.nced, s The rcsult.iugadvanced theories arc scored aud predicted for, andI, he new iuconll)lete predicted theories are scored andaWe believe thai, N depends on tile perl)lcxity of thegralillllar used, lint for the string grammar used for ourCXl)criment.s we ,tsctl N=3.
\["or the purl)oses of training, ahigher N shouhl I)(: tlS(:(I ill order to generaL(: //|ore I)a.rs(:s.added to the chart.
This process continues until ancoml)lete parse tree is determined, or until the parserdecides, heuristically, that it should not continue.
Theheuristics we used for determining that no parse canI)e Ibun(I Ibr all inlmt are I)ased on tile highest scoringincomplete theory ill the chart, the number of passesthe parser has made, an(I the size of the chart.T'- ear l ' s  Capab i l i t iesBesides nsing statistical methods to guide tile parserl,hrough I,h,' I)arsing search space, Pearl also performsother functions which arc crucial to robustly processingUlU'estricted uatural language text aud speech.Hand l ing  Unknown Words  Pearl uses a very sim-ple I)robal)ilistic unknown word model to hypol.h(nsizecategories for unknown words.
When word which isunknown to the systenl's lexicon, tile word is assumedto I)e a.ny one of the open class categories.
The lexicali)rol);d)ility givell a (-atcgory is the I)rol)ability of thatcategory occurring in the training corpus.Id iom Process ing  and Lat, t ice Pars ing  Since theparsing search space can be simplified by recognizingidioms, Pearl allows tile input string to i,iclude idiomsthat span more than one word in tile sentence.
This isaccoml)lished by viewing the input sentence as a wordla.ttice instead of a word string.
Since idion}s tend to beuuand)igttous with respect to part-of-speech, they aregenerally favored over processing the individual wordsthat make up the idiom, since the scores of rules con-taining the words will ten(I to be less thau 1, whilea syntactically apl)rol)riate, unambiguous idiom willhave a score of close to 1.The ahility to parse a scnl.epce wil, h multiple wordhyl)otlmses and word I)oulidary hyl)othcses makesPeaH very usehd in the domain of spoken languageprocessing.
By delayiug decisions about word selectionI)ut maintaining scoring information from a sl)eech rec-ognizer, tlic I>a.rser can use granmlaticai information inword selection without slowing the speech recognitionpro(~ess.
Because of Pearl 's interleaved architecture,one could easily incorporate scoring information froma speech rccogniz, cr into the set of scoring functionsused in tile parser.
Pearl could also provide feedbackto the specch recognizer about the grammaticality offragnmitt hypotheses to guide the recognizer's search.Par t ia l  Parses  The main advantage of chart-basedparsiug over other parsing algorithms is that the parsercan also recognize well-formed substrings within thesentence in the course of pursuing a complete parse.Pearl takes fidl advantage of this characteristic.
OncePearl is given the input sentence, it awaits instructionsa.s to what type of parse should be attempted for thisi,lput.
A standard parser automatically attempts toproduce asentence (S) spanning tile entire input string.llowever, if this fails, the semantic interpreter might beable to (Icriw-' some mealfiug from the sentence if given18-aon-ow'.rhq~pirig noun, w~.rb, and prepositional phrases.If a s,,nte,,ce f~tils I,o parse,, requests h)r p;trLial parsesof the input string call be made by specifying a rangewhich the parse l.ree should cover and the category(NP, VI', etc.
).Tile al)ilil.y I.o llrodil('c i)artial parses allows the sys-tem i.o hai ld le  ,nult.iple sentence inl~ul.s.
In both speechalld I.~'x|.
proc~ssing, il.
is difficult to know where the(qld Of ;I S('llI,CIICe is.
For illsta.llCe~ ouc CaUllOt reli-ably d,'l.eriiiitw wholl ;t slmakcr t(~.rlnillat?.s a selll,c,.aceia free speech.
Aml in text processing, abbreviationsand quoted expressions produce anlbiguity abotll, sen-t,,.nc,, teriilinatioil.
Wh,~ll this aildfiguil,y exists, .p,'a,'lcan I),, qucri~'d for partial p;i.rse I.rccs for the given in-pill., wh(,re l.ll(~ goal category is a sen(elite.
Tin,s, ifI.hc word sl.rittg ix a cl.ually two COmldcl.c S~'ld.elwcs, I.Impars~,r call r,'l.urn I.his itd'orm;d.ioll.
Ilow~,w,r, if I.hcword sl, r-itJg is oilly ()tic SCIItI~.IlCC, tllell it colilld~,l,c parsel.i't',, is retul'ned at lit.tie extra cost.Tra i ,ml l i l i ty  ( ) l . '
of I.he lim;ior adva,d,agcs of theI~rohabilistic pars,,i's ix ti'ainalfility.
The c(mditic, tm.Iprobabilities used by T'earl are estimated by using fre-quem:ies froth a large corpus of parsed  sellte|lce~, rlahepars~,d seill.enccs Ira,st be parsed ttSillg I.he grallima.rIbrmalism which the ` pearl will use.Assuming l.he g,'ammar is not rccursive in an un-constrained way, the parser can be traim~'d in an unsu-pervised mode.
This is accomplished by framing thepars~,r wil.hotlt the scoring functions, and geueratinglilall~" parse trees for each sentence.
Previous work 9has dclllonstrated that the correct information froththese parse l.rc~s will I)~" reinforced, while the i,lcorrectsubstructure will not.
M ultiple passes of re-Lra.iniqg its-ing frequency data.
from the previous pass shouhl causet,lw fro(lllency I.abh,s 1.o conw'.rge to a stable sta.te.
ThisJLvI)ol.hcsis has not yet beell tesl.cd.
TMAn alternal.iw~ 1.o completely unsupervised trainingis I.o I.akc a parsed corpus for any domain of the same\] ; l l lgi l ; Igl '  IlSilig l,h,~ Salli,~ gra.i i l l l ia.r, all<l liS~: I, he fl'~:-i I I Ip l lCy dal,a f ro l l i  I.hal, corpllS ;is I, hc i l i i l ,  ial I,ra.iliiilgji i lal, er ia l  for I, he l iew corpus.
Th is  a l lproach shoulds,)i'vt~ () l i ly I,o i i i i n i ln i ze  I, he l i l l i i l ber  of  UliSUllCrvisedpasses reqi l i red for l.lio f reqi le i lcy dal, a I,o converge.Pre l iminary  Eva luat ionWhile we haw; ,rot yet done ~-xte,miw~' testing of all ofthe Cal)abilities of "/)carl, we perforumd some simpletests to determine if its I~erformance is at least con-sistent with the premises ,port which it is based.
TheI.cst s,'ntcnces used for this evaluation are not fi'om the?This is a.u Unl~,,blishcd result, reportedly due to Fu-jisaki a.t IBM .\]apitll.l0 In fact, h~r certain grail|liiars, th(.'
fr(.~qllClicy I.~tl)les maynot conw:rge at all, or they may converge to zero, withthe g,','tmmar gc,tcrati,lg no pa.rscs for the entire corpus.This is a worst-case sccl,ario whicl, we do oct a,lticipatehalq~cning.training data on which the parser was trained.
Using.p,'arl's cont(.
'xt-free gr;unmar,  i,h~.~e t st sentences pro-duced an average of 64 parses per sentence, with somesentences producing over 100 parses.Unknown Word  Par t -o f - speechAss ignmentTo determine how "Pearl hamlles unknown words, weremow'd live words from the lexicon, i, kuow, lee, de-scribe, aml station, and tried to parse the 40 samplesentences I,sing the simple unknown word model pre-vie,rely d,:scribcd.I,i this test, the pl'onollll, il W~L,'q assigncd the cor-rect.
i)art-of-speech 9 of 10 I.iiiies it occurred in the test,s'~'nt~mces.
The nouns, lee and slalion, were correctlyI.~tggcd 4 of 5 I.inics.
And the w;rbs, kltow and describe,were corl'~cl.ly I,aggcd :l of :l tiilles.pronoun 90%nou,i 80%verb 100%'overall 89% ....Figure 1: Performance on Unknown Words in Test Sen-I, encesWhile this accuracy is expected for unknown wordsin isolation, based oil the accuracy of the part-of-speech tagging model, the performance is expected todegrade for sequences of unk,lown words.P repos i t iona l  Phrase  At tachmentAcc0rately determining prepositional phrase attach-nlent in general is a difficult and well-documentedproblem, llowever, based on experience with severaldifferent donmins, we have found prel)ositional phraseattachment to be a domain-specific pheuomenon forwhich training ca,t I)e very helpfld.
For insta,tce, inthe dirccl.ion-li,ldi,,g do,lmin, from aml to prepositionalphrases generally attach to the preceding verb andnot to any noun phrase.
This tende,icy is capturediu the training process for .pearl and is used to guidethe parscr to the more likely attach,nent with respectto ~he domain.
This does not mean that Pearl willgel.
the correct parse when the less likely attachme\]ttis correct; in fact, .pearl will invariably get this casewrong, llowever, based on the premise that this is theless likely attachment, his will produce more correctanalyses than incorrect.
And, using a more sophisti-cated statistical model, this pcrfornla,lcc an easily beimproved.
"Pearl's performance on prepositional phrase attach-meat was very high (54/55 or 98.2% correct).
The rea-so,i the accuracy rate was so high is that/.lie direction-finding domain is very consistent in it's use of individ-t,al prepositions.
The accuracy rate is not expectedto be as high in other domains, although it certainly- 19 -should be higher than 50% and we would expect it tobc greater than 75 %, although wc have nol.
performedany rigorous tests on other (Ionmius to verify this.i,.ro,,ositio., I to i o,,Accuracy R,ate 92 % 100 % 100 % 98.2 %I"igure 2: Accl,racy Rate for Prepositional Phr;~se At-I.achnlcnt, I)y l)repositionOvera l l  Pars ing  AccuracyThe 40 test sentences were parsed by 7)earl and thehighest scoring parse for each sentence was comparedto the correct parse produced by I'UNI)rr.
Of these 40s~llt.encos, "\])~'.
;I.l'I I),'odu('ed p;t.rsr: tl'(?t:s for :18 of ti,enl,al ld :15 of  I, he.sc i)a.rsc tree's wt~t'\[~" {:(liliv;i.I(:lll, I o I,hc cor-I'~:Cl, I)al'Se i)roducetl by I)ulldil,, for an overall at;cura(:yM; i t ly  of  Lilt: I,(?st SelltellCCS W(?l't.
~ I Iot ( l i l l i cu l t  I,o i)arscfor ex is t ing  l)arsers, but  \]hOSt had some granunat ica latl l l ) ig l l i l ,y  which wouhl  pro( l l lce l l l i l i t i l ) le  i)arses.
I l lfact, on 2 of tile 3 sciitences which were iucorrectlyi)arsed, "POal'l i)roduced the corl't~ct i);ll'SC ;is well, butthe correct i)a,'se did not have the h ighest  score .Future  WorkThe "Pearl parser takes advantage ofdonmin-depen(lentinformation to select the most approi)riate interpreta-tion of an inpul,.
Ilowew'.r, i,he statistical measure usedto disalnbiguate these interpretations is sensitive tocertain attributes of the grammatical formalism used,as well as to the part-of-si)eech categories used to la-I)el lexical entries.
All of the exl)erimcnts performed onT'carl titus fa," have been using one gra.l inrla.r, one  pa.rl.-of-speech tag set, and one donlaiu (hecause of avail-ability constra.ints).
Future experime.nl,s are I)lannedto evalua.l,e "Pearl's i)erforma.nce on dii\[cre.nt domaius,as well as on a general corpus of English, arid ott digfi~rent grammars, including a granunar derived fi'om anlanually parsed corl)us.Conc lus ionThe probal)ilistic parser which we have described pro-vides a I)latform for exploiting the useful informa-tion made available by statistical models in a mannerwhich is consistent with existing grammar formalismsand parser desigus.
7)carl can bc trained to use anycontext-free granurlar, ;iccompanied I)y tile al)l)ropri-ate training matc,'ial.
Anti, the parsing algorithm isvery similar to a standard bottom-t,I) algorithm, withthe exception of using theory scores to order the search.More thorough testing is necessary to inclosure7)carl's performance in tcrms of i)arsing accuracy, part-of-sl)eech assignnmnt, unknown word categorization,kliom processing cal)al)ilil.ies, aml even word selectionin speech processing.
With the exception of word se-lection, preliminary tesl.s how /)earl performs thesettLsks with a high degree of accuracy.References\[1\] Ayuso, D., Bobrow, It,  el.
al.
1990.
'lbwards Un-derstanding Text with a Very Large Vocabulary.In Proceedings of the June 1990 DARPA Speechand Natural Language Workshop.
llidden Valley,Pennsylvania.\[2\] Brill, E., Magerman, D., Marcus, M., anti San-torini, I1.
1990.
Deducing Linguistic Strl,cturefi'om the Statistics of Large Corl)ora.
In Proceed-ings of the June 1990 I)A IU)A Speech and NaturalLanguage Workshop.
llidden Valley, Pennsylva-Ilia.\[3\] C'hil, rao, M. and (.
','ishnla, i, IL 1990.
SI,atisti-cal Parsing of Messages.
hi Proceedings of theJ utle 1990 I)A R.PA Speech and Natural LanguageWorkshoiL Iliddeu Valley, Pennsylvania.\[4} Church, K. 1988.
A Stochastic Parts Programand Noun Phra.se Parser for Unrestricted Tcxt.
InProcee(li*lgs of the Second Confereuce on AppliedNatural I,at.~gt,age Processing.
Austin, 'l~xas.\[5\] Chu,'dl, K. and Gale, W. 1990.
Enhanced Good-Turing and Cat-Cal: Two New Methods for Es-timating Probal)ilitics of English Bigrams.
Com-pulers, Speech and Language.\[6\] Fano, R.. 1961.
Transmission of \[nformalion.
NewYork, New York: MIT Press.\[7\] Gale, W. A. and Church, K. 1990.
Poor Estimatesof Context are Worse than None.
In Proceedingsof the June 1990 I)AR.PA Speech and NaturalI,anguage Workshol).
llidden Valley, Pennsylva-nia.\[8\] llin(lle, I).
1988.
Acquiring a Noun Classificationfrom Predicate-Argument Structures.
Bell Labo-ratories.\[9\] llindle, D. and R.ooth, M. 1990.
Structural Ambi-guity and l,exical R.clations.
hi Proceedings of theJ uuc 1990 I)A I)d~A SI)ccch and Natural LanguageWorkshop.
llid(len Valley, Pennsylvania.\[10\] Jelinek, F. 1985.
Self-organizing Language Mod-eling for Speech li.ecognition.
IBM R.eport.\[l 1\] Katz, S. M. 1987.
Estimation of Probabilities fromSparse Data for the Language Model Compo-nent of a SI)eech R.ecognizer.
IEEE Trausaclionson Acouslics, Speech, aud Signal Processing, Vol.ASSP-35, No.
3.\[12\] Sharman, IL A., Jelinek, F., and Mercer, R. 1990.In Proceedings of tile June 1990 DARPA Speechand Natural Language Workshop.
11idden Valley,Pennsylvauia.- 20 -
