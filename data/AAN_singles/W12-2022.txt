The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 190?200,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsShort Answer Assessment: Establishing Links Between Research StrandsRamon Ziai Niels Ott Detmar MeurersSFB 833 / Seminar fu?r SprachwissenschaftUniversita?t Tu?bingen{rziai,nott,dm}@sfs.uni-tuebingen.deAbstractA number of different research subfields areconcerned with the automatic assessment ofstudent answers to comprehension questions,from language learning contexts to computerscience exams.
They share the need to evaluatefree-text answers but differ in task setting andgrading/evaluation criteria, among others.This paper has the intention of fosteringsynergy between the different research strands.It discusses the different research strands,details the crucial differences, and exploresunder which circumstances systems can becompared given publicly available data.
To thatend, we present results with the CoMiC-ENContent Assessment system (Meurers et al,2011a) on the dataset published by Mohleret al (2011) and outline what was necessaryto perform this comparison.
We concludewith a general discussion on comparability andevaluation of short answer assessment systems.1 IntroductionShort answer assessment systems compare students?responses to questions with manually defined targetresponses or answer keys in order to judge theappropriateness of the responses, or in order toautomatically assign a grade.
A number ofapproaches have emerged in recent years, each ofthem with different aims and different backgrounds.In this paper, we will draw a map of the short answerassessment landscape, highlighting the similaritiesand differences between approaches and the data usedfor evaluation.
We will provide an overview of 12systems and sketch their attributes.
Subsequently,we will zoom into the comparison of two of them,namely CoMiC-EN (Meurers et al, 2011a) and theone which we call the Texas system (Mohler et al,2011) and discuss the issues that arise with thisendeavor.
Returning to the bigger picture, we willexplore how such systems could be compared ingeneral, in the belief that meaningful comparisonof approaches across research strands will be animportant ingredient in advancing this relatively newresearch field.2 The short answer assessment landscape2.1 General aspectsResearchers from all directions have settled in thelandscape of short answer assessment, each of themwith different backgrounds and different goals.
Inthis section, we aim at providing an overview ofthese research villages, also hoping to construct aroad network that may connect them.Most approaches to short answer assessment aresituated in an educational context.
Some focus onGCSE1 tests, others aim at university assessmenttests in the medical domain.
Another strandof approaches focuses on language teaching andlearning.
All of these approaches share one theme:they assess short texts written by students.
Thesemay be answers to questions that ask for knowledgeacquired in a course, e.g., in computer science, or toreading comprehension questions in second language1The General Certificate of Secondary Education (GCSE) isan academic qualification in the United Kingdom, usually takenat the age of 14?16.190learning.
While thematically related, short answerassessment is different from essay grading.
Shortanswers are formulated by students in a much morecontrolled setting.
Not only are they short, theyusually are supposed to contain only a few facts thatanswer only one question.Another common theme of these approaches isthat they compare the student answers to one or morepreviously defined correct answers that are eithergiven in natural language as target answers or as a listof concepts in an answer key.
The ways of technicallyconducting these comparisons vary widely, as wediscuss below in Section 2.2.There also are conceptual differences betweenthe approaches.
Some systems focus on assessingwhether or not the student has properly answeredthe question.
They put the spot on comparing themeaning of target answers and student answers; theyaim at being tolerant of form errors such as spellingor grammar errors.
Others aim at giving a grade asaccurate as possible, therefore not only assessingmeaning but also performing grading similar tohuman teachers.
This can also include modules thattake into account form errors.These two views on a similar task are also reflectedin the annotation of the data used in experiments:Systems performing meaning comparison usuallyoperate with labels specifying the relations betweentarget answers and student answers.
Grading systemsnaturally aim at producing numerical grades.
Sincelabels are on a nominal scale, and grades are onan ordinal scale (or even treated as being on aninterval scale), the difference between meaningcomparison and grading results in a whole stringof other differences in methodology.Researchers also enter the short answer landscapefrom different home countries: Some projects areinterested in the strategies and mechanics of meaningcomparison, others aim at reducing the load and costsof large-scale assessment tests, and yet others aimat improving intelligent tutoring systems, requiringadditional components that provide useful feedbackto students using these systems.2.2 ApproachesTable 1 summarizes the features of the short answerassessment systems discussed hereafter.One of the earlier systems is WebLAS, presentedby Bachman et al (2002).
A human task creator feedsthe system with scores for model answers.
Regularexpressions are then created automatically from thesemodel answers.
Since each regular expression isassociated with a score, matching the expressionagainst a student answer yields a score for that answer.Bachman et al (2002) do not provide an evaluationstudy based on data.Another earlier system is CarmelTC by Rose?
etal.
(2003).
It has been designed as a componentin the Why2 tutorial dialogue system (VanLehn etal., 2002).
Even though Rose?
et al (2003) positionCarmelTC in the context of essay grading, it may beconsidered to deal with short answers: in their data,the average length of a student response is approx.48 words.
Their system is designed to performtext classification on single sentences in the studentresponses, where each class of text represents onepossible model response, plus an additional class for?no match?.
They combine decision trees operatingon an automatic syntactic analysis, a Naive Bayestext classifier, and a bag-of-words approach.
In a50-fold cross validation experiment with one physicsquestion, six classes and 126 student responses,hand-tagged by two annotators, CarmelTC reachesan F-measure value of 0.85.
They do not report on abaseline.
Concerning the quality of the gold standard,they report that conflicts in the annotation have beenresolved.C-Rater (Leacock and Chodorow, 2003) is basedon a paraphrase recognition approach.
It employscorrect answer models consisting of essential pointsformulated in natural language.
C-Rater aims atautomatic scoring and focuses on meaning, thustolerating form errors.
Leacock and Chodorow(2003) present two pilot studies, one of them dealingwith reading comprehension.
From 16,625 studentanswers with an average length of 43 words, theydrew a random sample of 100 answers to each ofthe seven questions.
This sample was scored byone human judge using a three-way scoring system(full credit, partial credit, no credit).
Their systemachieved 84% agreement with the gold standard.Information about the distribution of the scoringcategories is given indirectly: A baseline system thatassigns scores randomly would have achieved 47%accuracy.191System Goal Technique Domain Lang.WebLAS (Bachman et al, 2002) Assessment oflanguage abilityAuto-generated regularexpressionsForeign languageteachingENCarmelTC (Rose?
et al, 2003) Automatic grading Text classification Physics ENC-Rater (Leacock and Chodorow,2003)Assessment test Paraphrase recognition Mathematics,Reading comp.ENIAT (Mitchell et al, 2003) Assessment,Automatic gradingInformation extractionw/ handwritten patternsMedical ENOxford (Pulman and Sukkarieh,2005)Assessment,automatic gradingInformation extractionw/ handwritten patternsGCSE exams ENAtenea (Pe?rez et al, 2005) Automatic grading N-gram overlap, LatentSemantic AnalysisComputer science ESLogic-based System (Makatchevand VanLehn, 2007)Meaning comparison First-order logic,machine learningPhysics ENCAM (Bailey and Meurers, 2008),CoMiC-EN (Meurers et al, 2011a)Meaning comparison Alignment, machinelearningReading comp.
inforeign languageENFacets System (Nielsen et al, 2009) Meaning comparison& tutoring systemsAlignment of facets,machine learningElementaryschool scienceclassesENTexas (Mohler et al, 2011) Automatic grading Graph alignment,semantic similarityComputer science ENCoMiC-DE (Meurers et al, 2011b) Meaning comparison Alignment, machinelearningReading comp.
inforeign languageDECoSeC-DE (Hahn and Meurers,2012)Meaning comparison Alignment viaLexical-ResourceSemanticsReading comp.
inforeign languageDETable 1: Short Answer Assessment systems and their FeaturesInformation extraction templates form the core ofthe Intelligent Assessment Technologies system (IAT,Mitchell et al 2003).
These templates are createdmanually in a special-purpose authoring tool byexploring sample responses.
They allow for syntacticvariation, e.g., filling the subject slot in a sentencewith different equivalent concepts.
The templatescorresponding to a question are then matched againstthe student answer.
Unlike other systems, IATadditionally features templates for explicitly invalidanswers.
They tested their approach with a progresstest that has to be taken by medicine students.Approximately 800 students each plowed through270 test items.
The automatically graded responsesthen were moderated: Human judges streamlined theanswers to achieve a more consistent grading.
Thisstep already had been done before with tests gradedby humans.
Mitchell et al (2003) state that theirsystem reaches 99.4% accuracy on the full datasetafter the manual adjustment of the templates viathe moderation process.
Summarizing, they reportan error of ?between 5 and 5.5%?
in inter-graderagreement and an error of 5.8% in automatic gradingwithout the moderation step, though it is not entirelyclear which data these statistics correspond to.
Noinformation on the distribution of grades or a randombaseline is provided.The Oxford system (Pulman and Sukkarieh, 2005)is another one to employ an information extractionapproach.
Again, templates are constructed manually.Motivated by the necessary robustness to processlanguage with grammar mistakes and spelling errors,they use shallow analyses in their pre-processing.In order to overcome the hassle of manually con-structing templates, they also investigated machinelearning techniques.
However, the automaticallygenerated templates were outperformed by themanually created ones.
Furthermore, they state thatmanually created templates can be equipped withmessages provided to the student as feedback in atutoring system.
For evaluating their system, theyused factual science questions and the corresponding192student answers from GCSE tests.
200 gradedanswers for each of nine questions served as atraining set, while another 60 answers served as atest set.
They report that their system achieves anaccuracy of 84%.
With inconsistencies in the humangrading removed, it achieves 93%.
However, they donot report on the level of inter-grader agreement oron a random baseline.Pe?rez et al (2005) present the Atenea system,a combined approach that makes use of LatentSemantic Analysis (LSA, Landauer et al 1998) andn-gram overlap.
While n-gram overlap supportscomparing target responses and student responseswith differing word order, it does not deal withsynonyms and related terms.
Hence, they use LSA toadd a component that deals with semantic relatednessin the comparison step.
As a test corpus, theycollected nine different questions from computerscience exams.
A tenth question ?
[consists] of aset of definitions of ?Operating System?
obtainedfrom the Internet.?
Altogether, they gathered 924student responses and 44 target responses writtenby teachers.
Since their LSA module had beentrained on English but their data were in Spanish,they chose to use Altavista Babelfish to translate thedata into English.
They do not provide informationabout the distribution of scores and about inter-graderagreement.
Atenea achieves a Pearson?s correlationof r = 0.554 with the scores in the gold standard.The approach by Makatchev and VanLehn (2007),which we refer to as the Logic-based System,enters the landscape from the direction of artificialintelligence.
It is related to CarmelTC and itsdataset, but follows a different route: targetresponses are manually encoded in first-orderpredicate language.
Similar logic representationsare constructed automatically for student answers.They explore various strategies for matching thesetwo logic representation on the basis of 16 semanticclasses.
In an evaluation experiment, they tested thesystem on 293 ?natural language utterances?
withten-fold cross validation.
The test data are skewedtowards the ?empty?
label that indicates that noneof the 16 semantic labels could be attached.
Theydo not report on other properties of the dataset suchas number of annotators or number of questions towhich the student answers were given.
Their winningconfiguration yields a F-measure value of 0.4974.While Makatchev and VanLehn (2007) position theirapproach in the context of the Why2 tutorial dialoguesystem, their use of semantic classes seems to makethem more related to meaning comparison than tograding.The Content Assessment Module (CAM) pre-sented in Bailey (2008) and Bailey and Meurers(2008) utilizes an approach that is different fromthe systems discussed so far: Following a three-stepstrategy, the system first automatically generateslinguistic annotations for questions, target responsesand student responses.
In an alignment phase, theseannotations are then used to map from elements(words, lemmas, chunks, dependency triples) inthe student responses to elements in the targetresponses.
Finally, a machine learning classifierjudges on the basis of this alignment, whetheror not the student has answered the questioncorrectly.
The data used for evaluation was madeavailable as the Corpus of Reading ComprehensionExercises in English (CREE, Meurers et al 2011a).This corpus consists of 566 responses producedby intermediate ESL learners at The Ohio StateUniversity as part of their regular assignments.Students had access to their textbooks and typicallyanswered questions in one to three sentences.
Allresponses were labelled as either appropriate orinappropriate by two independent annotators, alongwith a detailed diagnosis code specifying the natureof the inappropriateness (missing concept, extraconcept, blend, non-answer).
In leave-one-outevaluation on the development set containing 311responses to 47 different questions, CAM achieved87% accuracy on the binary judgment (responsecorrect/incorrect).
For the test set containing 255responses to 28 questions, the approach achieved88%.
However, the distribution of categories in thedata is heavily skewed with 71% of the responsesmarked as correct in the development set and 84% inthe test set.
The best result obtained on a balancedset with leave-one-out-testing is 78%.
Meurers etal.
(2011a) present a re-implementation of CAMcalled CoMiC-EN (Comparing Meaning in Contextin English), achieving an accuracy of 87.6% on theCREE development set and 88.4% on the test set.With their Facets System, Nielsen et al (2009)establish a connection to the field of RecognizingTextual Entailment (RTE, Dagan et al 2009).
In193a number of friendly challenges, RTE research hasspawned numerous systems that try to automaticallyanswer the following question: Given a text and ahypothesis, is the hypothesis entailed by the text?Short answers assessment can be seen as a RTE taskin which the target response corresponds to the textand the student response to the hypothesis.
Nielsen etal.
(2009) base their system on what they call facets.These facets are meaning representations of partsof sentences.
They are constructed automaticallyfrom dependency and semantic parses of the targetresponses.
Each facet in the target response is thenlooked up in the corresponding student responseand equipped with one of five labels2 ranging fromunaddressed (the student did not mention the factin this facet) to expressed (the student named thefact).
This step is taken via machine learning.From a tutoring system in real-life operation, theygathered responses from third- to sixth-grade studentsanswering questions for science classes.
Twoannotators worked on these data, producing 142,151facets.
Furthermore, all facets were looked up inthe corresponding student responses and annotatedaccordingly, using the mentioned set of labels.
Thebest result of the Facets System is 75.5% accuracy onone of the held-out test sets.
With ten-fold crossvalidation on the training set, it achieves 77.1%accuracy.
The majority label baselines are 51.1% and54.6% respectively.
Providing this more fine-grainedanalysis of facets that are searched for in studentresponses, Nielsen et al (2009) claim to ?enablemore intelligent dialogue control?
in tutoring systems.From the point of view of grading vs. meaningcomparison, their approach can be counted towardsthe latter, since their labels can be conflated toproduce a single yes/no decision.Another recent approach is described by Mohler etal.
(2011), hereafter referred to as the Texas system.Student responses and target responses are annotatedusing a dependency parser.
Thereupon, subgraphs ofthe dependency structures are constructed in order tomap one response to the other.
These alignmentsare generated using machine learning.
Dealingwith subgraphs allows for variation in word orderbetween the two responses that are to be compared.2In human annotation, they use eight labels, which aregrouped into five broader categories as used by their system.In order to account for meaning, they combinelexical semantic similarity with the aforementionedalignment.
They make use of several WordNet-basedmeasures and two corpus-based measures, namelyLatent Semantic Analysis and Explicit SemanticAnalysis (ESA, Gabrilovich and Markovitch 2007).For evaluating their system, Mohler et al (2011)collected student responses from an online learningenvironment.
80 questions from ten introductorycomputer science assignments spread across twoexams were gathered together with 2,273 studentresponses.
These responses were graded by twohuman judges on a scale from zero to five.
Thejudges fully agreed in 57% of all cases, theirPearson correlation computes to r = 0.586.
Thegold standard has been created by computing thearithmetic mean of the two judgments for eachresponse.
The Texas system achieves r = 0.518 anda Root Mean Square Error of 0.978 as its best result.Mohler et al (2011) mention that ?
[t]he dataset isbiased towards correct answers?.
Data are publiclyavailable.
We used these in an evaluation experimentwith the CoMiC-EN system, discussed in Section 3.While almost all short answer assessment researchhas targeted answers written in English, there aretwo recent approaches dealing with German answers.The CoMiC-EN reimplementation of CAM discussedabove was motivated by the need for a modulararchitecture supporting a transfer of the system toGerman, resulting in its counterpart named CoMiC-DE (Meurers et al, 2011b).
The German systemutilizes the same strategies as the English one,but with language-dependent processing modulesbeing replaced.
Meurers et al (2011b) evaluatedCoMiC-DE on a subset of the Corpus of ReadingComprehension Questions in German (CREG, Ott etal.
2012), collected in collaboration with the Germanprograms at The Ohio State University and theUniversity of Kansas.
Like in CREE, all responsesare rated by two annotators with both binary anddetailed diagnosis codes.3 The aforementionedsubset contains 1,032 learner responses and 223target responses to 177 questions.
Furthermore, itfeatures an even distribution of correct and incorrectanswers according to the judgement of two human3In CREG, correct answers as well as incorrect ones can belabelled with missing concept, extra concept, or blend.194annotators.
On that subset, CoMiC-DE achieved anaccuracy of 84.6% in the binary classification task.CREG is freely available for research purposes undera Creative Commons by-nc-sa license.Hahn and Meurers (2012) present the CoSeC-DEapproach based on Lexical Resource Semantics(LRS, Richter and Sailer 2003).
In a first step,they create LRS representations from POS-taggedand dependency-parsed data.
These underspecifiedLRS representations of student responses and targetresponses are then aligned.
Using A* as heuristicsearch algorithm, a best alignment is computed andequipped with a numeric score representing thequality of the alignment of the formulae.
If thisbest alignment scores higher than a threshold, thesystem judges student response and target responseto convey the same meaning.
The alignmentand comparison mechanism does not utilize anylinguistic representations other than the LRSsemantic formulae.
These semantic representationsabstract away from surface features, e.g., by treatingactive and passive voice equally.
Hahn and Meurers(2012) claim that that ?
[semantic representations]more clearly expose those distinction which do makea difference in meaning.?
They evaluate the approachon the above-mentioned subset of CREG containing1,032 learner responses and report an accuracy of86.3%.3 A concrete system comparisonAfter discussing the broad landscape of Short AnswerEvaluation systems, the main characteristics anddifferences, we now turn to a comparison of twoconcrete systems, namely CoMiC-EN (Meurerset al, 2011a) and the Texas system Mohler etal.
(2011), to explore what is involved in such aconcrete comparison of two systems from differentcontexts.
While CoMiC-EN was developed withmeaning comparison in mind, the purpose of theTexas system is answer grading.
We pick thesetwo systems because they constitute recent andinteresting instances of their respective fields andthe corresponding data are freely available.3.1 DataIn evaluating the Texas system, Mohler et al (2011)used a corpus of ten assignments and two exams froman introductory computer science class.
In total, theTexas corpus consists of 2,442 responses, which werecollected using an online learning platform.
Eachresponse is rated by two annotators with a numericalgrade on a 0?5 scale.
Annotators were not given anyspecific instructions besides the scale itself, whichresulted in an exact agreement of 57.7%.
In order toarrive at a gold standard rating, the numerical averageof the two ratings was computed.
The data exist inraw, sentence-segmented and parsed versions and arefreely available for research use.
Table 2 presentsa breakdown of the score counts and distributionstatistics of the Texas corpus.
A bias towards correctanswers can be observed, which is also mentioned byMohler et al (2011).Score #0.000 240.500 31.000 231.500 461.750 12.000 932.250 22.500 1253.000 164Score #3.250 13.500 1873.625 13.750 14.000 2204.125 24.500 3104.750 15.000 1238x = 4.19, s = 1.11Table 2: Details on the gold standard scores in the Texascorpus.
Non-integer scores result from averaging betweenraters and normalization onto the 0?5 scale.3.2 ApproachesCoMiC-EN uses a three-step approach to meaningcomparison.
Annotation uses NLP to enrich thestudent and target answers, as well as the questiontext, with linguistic information on different levels(words, chunks, dependency triples) and types ofabstraction (tokens, lemmas, distributional vectors,etc.).
Alignment maps elements of the learner answerto elements of the target response using annotation.The global alignment solution is computed using theTraditional Marriage Algorithm (Gale and Shapley,1962).
Finally, Classification analyzes the possiblealignments and labels the learner response with abinary or detailed diagnosis code.
The features usedin the classification step are shown in Table 3.For the Texas system, Mohler et al (2011) used acombination of bag-of-words (BOW) features and195Features Description1.
Keyword Overlap Percent of keywords aligned(relative to target)2./3.
Token Overlap Percent of alignedtarget/learner tokens4./5.
Chunk Overlap Percent of alignedtarget/learner chunks6./7.
Triple Overlap Percent of alignedtarget/learner triples8.
Token Match Percent of token alignmentsthat were token-identical9.
Similarity Match Percent of token alignmentsthat were similarity-resolved10.
Type Match Percent of token alignmentsthat were type-resolved11.
Lemma Match Percent of token alignmentsthat were lemma-resolved12.
Synonym Match Percent of token alignmentsthat were synonym-resolved13.
Variety of Match Number of kinds of(0-5) token-level alignmentsTable 3: Features used in the CoMiC-EN systemdependency graph alignment in connection withtwo different machine learning approaches.
Amongthe BOW features are WordNet-based similaritymeasures such as the one by Lesk (1986) and vectorspace measures such as tf ?
idf (Salton and McGill,1983) and the more advanced LSA (Landauer et al,1998).
The dependency graph alignment approachbuilds on a node-to-node matching stage whichcomputes a score for each possible match betweennodes of the student and target response.
In the nextstage, the optimal graph alignment is computed basedon the node-to-node scores using the Hungarianalgorithm.Mohler et al (2011) also employ a techniquethey call ?question demoting?, which refers to theexclusion of words from the alignment processif they already appeared in the question string.Incidentally, the technique is also used in the earlierCAM system (Bailey and Meurers, 2008), but called?Givenness filter?
there, following the long traditionof research on givenness (Schwarzschild, 1999) as anotion of information structure investigated in formalpragmatics.To produce the final system score, the Texassystem uses two machine learning techniques basedon Support Vector Machines (SVMs), SVMRank andSupport Vector Regression (SVR).
Both techniquesare trained with several combinations of thedependency alignment and BOW features.
Whilewith SVR one trains a function to produce a score onthe 0?5 scale itself, SVMRank produces a ranking ofstudent answers which does not produce a 0?5 grade.Therefore, Mohler et al (2011) employ isotonicregression to map the ranking to the 0?5 scale.In terms of performance, Mohler et al (2011)report that the SVMRank system produces a bettercorrelation measure (r = 0.518) while the SVRsystem yields a better RMSE (0.978).3.3 EvaluationWe now turn to the evaluation of CoMiC-EN on theTexas corpus as it is a publicly available dataset.
Asmentioned before, CoMiC-EN performs meaningcomparison based on a system of categories whilethe Texas system is a scoring approach, trying topredict a grade.
While the former is a classificationtask, the latter is better characterized as a regressionproblem because of the desired numerical outcome.Of course, one could simply pretend that individualgrades are classes and treat scoring as a classificationtask.
However, a classification approach has noknowledge of numerical relationships, i.e., it doesnot ?know?
that 4 is a higher grade than 3 and amuch higher grade than 1 (assuming a 0?5 scale).As a result, if an evaluation metric such as Pearsoncorrelation is used, classification systems are at adisadvantage because some misclassifications arepunished more than others.
We discuss this pointfurther in Section 4.For these reasons, to obtain a more interestingcomparison, we modified CoMiC-EN to performscoring instead of meaning comparison.
This meansthat the memory-based learning approach CoMiC-EN had employed so far was no longer applicable andhad to be replaced with a regression-capable learningstrategy.
We chose Support Vector Regression (SVR)using libSVM4 since that is one of the methodsemployed by Mohler et al (2011).
However, all otherparts of CoMiC-EN such as the processing pipelineand the alignment approach and the extracted featuresremained the same.4http://www.csie.ntu.edu.tw/?cjlin/libsvm196The evaluation procedure was carried out as a12-fold cross-validation due to the 12 assignmentsin the Texas corpus.
For each fold, one completeassignment was held out as test set.
Parameters forthe SVR were determined using a grid search usingthe tools provided with libSVM.
As kernel function,we used a linear kernel as it was also used in theevaluation of the Texas system and thus constitutesa vital part of the evaluation setup.
In general, wedesigned to evaluation procedure to be as close aspossible to the Texas one.Table 4 presents detailed results on the 12 foldsas well as the overall results and a baseline whichalways predicts the median value 5.Assignment # responses r RMSE1 203 0.416 0.9582 210 0.349 1.2213 217 0.335 0.9694 210 0.338 1.2125 112 0.010 1.0306 182 0.646 0.7027 182 0.265 0.9918 189 0.521 0.9429 189 0.220 0.94210 168 0.699 0.99011 (exam) 300 0.436 1.07612 (exam) 280 0.619 1.165Median Baseline 2442 ?
1.375Overall 2442 0.405 1.016Table 4: Detailed results of CoMiC-EN on Texas corpusThe CoMiC-EN system on the Texas data set doesnot quite reach the level achieved by the Texas systemon their data set.
We obtained a Pearson correlationof r = 0.405 and an RMSE of 1.016 over all 12 folds.However, let us keep in mind the objective of thisexperiment as exemplifying the process needed todirectly compare two systems from different researchstrands on the same dataset.4 Comparability of approaches & datasetsIt seems clear that for systems to be comparableand results to be reproducible, datasets must bepublicly available, as is the case with the Texascorpus.
However, data availability alone does notensure meaningful comparison.
Depending on thecontext the corpus was drawn from, datasets willdiffer just like the corresponding systems:?
Data source: Reading comprehension task inlanguage learning setting, language tutoringcontext, automated grading of short answerexams?
Language properties: Native vs. learnerlanguage, domain-specific language (e.g., com-puter science)?
Assessment scheme: nominal vs. interval scaleEspecially the last point deserves some furtherdiscussion.
Depending on the kind of assessmentscheme, which in turn is motivated by the task,different evaluation methods may be chosen.
Scoringsystems are often evaluated using a correlation metricin order to capture the systems?
tendency to assignsimilar but not necessary equal grades as the humanraters.
Conversely, with category-based schemes oneusually reports accuracy, which expresses how manyitems were classified correctly.The question that arises is how a system comingfrom one paradigm can be compared to one fromthe other paradigm in a meaningful way.
One mightargue that the tasks are simply too different: scoringmight take form errors into account while meaningcomparison by definition does not.
Moreover,while classification labels say something explicit andabsolute about a piece of data, grades by definitionare relative to the scale they come from.
It thus seemsimpossible to somehow unify the two schemes as theyexpress fundamentally different ideas.However, the strategies systems use to tacklescoring or meaning comparison are undoubtedlysimilar and should be comparable, as we argue in thispaper.
So in order for researchers to learn from otherapproaches and also compare their results to those ofother systems which tackle a different task, changesto systems seem necessary and should be preferredover changes to the gold standard data.
In the casepresented here, a meaning comparison system wasturned into a scoring system by changing the machinelearning component from classification to regression,which requires a certain level of system modularity.Having compared the two systems using Pearsoncorrelation and RMSE, it also makes sense toconsider the relevance of these evaluation metrics.For example, it is the case that pairwise correlationassumes a normal distribution whereas datasets like197the Texas corpus are heavily skewed towards correctanswers (see Table 2).
Mohler et al (2011) also notethat in distributions with zero variance, correlation isundefined, which is not a problem as such but limitsthe use of correlation as evaluation metric.
Mohleret al (2011) propose that RMSE is better suited tothe task since it captures the relative error a systemmakes when trying to predict scores.
However,RMSE is scale-dependent and thus RMSE valuesacross different studies cannot be compared.
Wecan only suggest that in order to sufficiently describea system?s performance, several metrics need to bereported.Finally, an important point concerns the qualityof gold standards.
Given the relatively low inter-annotator agreement in the Texas corpus (r =0.586, RMSE = 0.659) it seems fair to ask whetheranswers without perfect agreement should be used intraining and testing systems at all.
In the CREEand CREG corpora, answers with disagreementamong the annotators have either been excludedfrom experiments or resolved by an additional judge.This approach is also supported by recent literature(cf., e.g., Beigman and Beigman Klebanov 2009;Beigman Klebanov and Beigman 2009).
However,for the Texas corpus, Mohler et al (2011) have optedto use the arithmetic mean of the two graders as goldstandard.
While mathematically a viable solution,it seems questionable whether the mean is reliablewith only two graders, especially if they have notoperated on the grounds of explicit guidelines.
Itwould be interesting to see whether in this case, asystem trained on more, singly annotated data wouldperform better than one on less, doubly annotateddata, as argued for by Dligach et al (2010).
In anycase, if many disagreements occur, one should askthe question whether the annotation task is definedwell enough and whether machines should really beexpected to perform it consistently if humans havetrouble doing so.5 ConclusionWe discussed several issues in the comparison ofshort answer evaluation systems.
To that end, wegave an overview of the existing systems and pickedtwo for a concrete comparison on the same data, theCoMiC-EN system (Meurers et al, 2011a) and theTexas system (Mohler et al, 2011).
In comparingthe two, it was necessary to turn CoMiC-EN intoa scoring system because the Texas corpus asthe chosen gold standard contains numeric scoresassigned by humans.
Taking a step back fromthe concrete comparison, we gave a more generaldescription of what is necessary to compare shortanswer evaluation systems.
We observed that moredatasets need to be publicly available in order forperformance comparisons to have meaning, a pointalso made earlier by Pulman and Sukkarieh (2005).Moreover, we noted how datasets differ in similaraspects as systems do, such as task context andassessment scheme.
We then criticized the use ofcorrelation measures as evaluation metrics for shortanswer scoring.
Finally, we discussed the importanceof gold standard quality.We conclude that it is interesting and relevantto compare short answer evaluation systems evenif the concrete task they tackle, such as grading ormeaning comparison, is not the same.
However, theavailability and quality of the datasets will decideto what extent systems can sensibly be compared.For progress to be made in this area, more publiclyavailable datasets and systems are needed.
Theupcoming SemEval-2013 task on ?Textual entailmentand paraphrasing for student input assessment?5will hopefully become one important step into thisdirection (see also Dzikovska et al 2012).AcknowledgementsWe are grateful to the three anonymous BEAreviewers for their detailed and helpful comments.ReferencesLyle Bachman, Nathan Carr, Greg Kamei, Mikyung Kim,Michael Pan, Chris Salvador, and Yasuyo Sawaki.2002.
A reliable approach to automatic assessment ofshort answer free responses.
In Proceedings of the 19thInternational Conference on Computational Linguistics(COLING 2002), pages 1?4.Stacey Bailey and Detmar Meurers.
2008.
Diagnosingmeaning errors in short answers to reading compre-hension questions.
In Joel Tetreault, Jill Burstein,and Rachele De Felice, editors, Proceedings of the5http://www.cs.york.ac.uk/semeval-2013/task4/1983rd Workshop on Innovative Use of NLP for BuildingEducational Applications (BEA-3) at ACL?08, pages107?115, Columbus, Ohio.Stacey Bailey.
2008.
Content Assessment in IntelligentComputer-Aided Language Learning: Meaning ErrorDiagnosis for English as a Second Language.
Ph.D.thesis, The Ohio State University.Eyal Beigman and Beata Beigman Klebanov.
2009.Learning with annotation noise.
In Proceedings of theJoint Conference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on NaturalLanguage Processing of the AFNLP, volume 1, pages280?287.
Association for Computational Linguistics.Beata Beigman Klebanov and Eyal Beigman.
2009.
Fromannotator agreement to noise models.
ComputationalLinguistics, 35(4):495?503.Ido Dagan, Bill Dolan, Bernardo Magnini, and DanRoth.
2009.
Recognizing textual entailment: Rational,evaluation and approaches.
Natural LanguageEngineering, 15(4):i?xvii, 10.Dmitriy Dligach, Rodney D. Nielsen, and Martha Palmer.2010.
To annotate more accurately or to annotate more.In Proceedings of the Fourth Linguistic AnnotationWorkshop, LAW IV ?10, pages 64?72, Stroudsburg, PA,USA.
Association for Computational Linguistics.Myroslava O. Dzikovska, Rodney D. Nielsen, and ChrisBrew.
2012.
Towards effective tutorial feedbackfor explanation questions: A dataset and baselines.In Proceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for ComputationalLinguistics: Human Language Technologies.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing semantic relatedness using wikipedia-basedexplicit semantic analysis.
In Proceedings of the20th International Joint Conference on ArtificialIntelligence, pages 6?12.David Gale and Lloyd S. Shapley.
1962.
Collegeadmissions and the stability of marriage.
AmericanMathematical Monthly, 69:9?15.Michael Hahn and Detmar Meurers.
2012.
Evaluatingthe meaning of answers to reading comprehensionquestions: A semantics-based approach.
In Pro-ceedings of the 7th Workshop on Innovative Use ofNLP for Building Educational Applications (BEA-7) atNAACL-HLT 2012, Montreal.Thomas Landauer, Peter Foltz, and Darrell Laham.
1998.An introduction to latent semantic analysis.
DiscourseProcesses, 25:259?284.Claudia Leacock and Martin Chodorow.
2003.
C-rater: Automated scoring of short-answer questions.Computers and the Humanities, 37:389?405.Michael Lesk.
1986.
Automatic sense disambiguationusing machine readable dictionaries: how to tell apine cone from an ice cream cone.
In Proceedingsof the 5th annual international conference on Systemsdocumentation, pages 24?26, Toronto, Ontario,Canada.Maxim Makatchev and Kurt VanLehn.
2007.
Combiningbaysian networks and formal reasoning for semanticclassification of student utterances.
In Proceedingsof the International Conference on AI in Education(AIED), Los Angeles, July.Detmar Meurers, Ramon Ziai, Niels Ott, and StaceyBailey.
2011a.
Integrating parallel analysis modulesto evaluate the meaning of answers to readingcomprehension questions.
IJCEELL.
Special Issue onAutomatic Free-text Evaluation, 21(4):355?369.Detmar Meurers, Ramon Ziai, Niels Ott, and Janina Kopp.2011b.
Evaluating answers to reading comprehensionquestions in context: Results for german and therole of information structure.
In Proceedings of theTextInfer 2011 Workshop on Textual Entailment, pages1?9, Edinburgh, Scotland, UK, July.
Association forComputational Linguistics.Tom Mitchell, Nicola Aldrige, and Peter Broomhead.2003.
Computerized marking of short-answerfree-text responses.
Paper presented at the 29thannual conference of the International Association forEducational Assessment (IAEA), Manchester, UK.Michael Mohler, Razvan Bunescu, and Rada Mihalcea.2011.
Learning to grade short answer questionsusing semantic similarity measures and dependencygraph alignments.
In Proceedings of the 49thAnnual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages752?762, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.Rodney D. Nielsen, Wayne Ward, and James H. Martin.2009.
Recognizing entailment in intelligent tutoringsystems.
Natural Language Engineering, 15(4):479?501.Niels Ott, Ramon Ziai, and Detmar Meurers.
2012.Creation and analysis of a reading comprehensionexercise corpus: Towards evaluating meaning incontext.
In Thomas Schmidt and Kai Wo?rner,editors, Multilingual Corpora and Multilingual CorpusAnalysis, Hamburg Studies in Multilingualism (HSM).Benjamins, Amsterdam.
to appear.Diana Pe?rez, Enrique Alfonseca, Pilar Rodr?
?guez, AlfioGliozzo, Carlo Strapparava, and Bernardo Magnini.2005.
About the effects of combining latent semanticanalysis with natural language processing techniquesfor free-text assessment.
Revista signos, 38(59):325?343.Stephen G. Pulman and Jana Z. Sukkarieh.
2005.Automatic short answer marking.
In Jill Burstein andClaudia Leacock, editors, Proceedings of the Second199Workshop on Building Educational Applications UsingNLP, pages 9?16, Ann Arbor, Michigan, June.Association for Computational Linguistics.Frank Richter and Manfred Sailer.
2003.
Basic conceptsof lexical resource semantics.
In Arnold Beckmannand Norbert Preining, editors, ESSLLI 2003 ?
CourseMaterial I, volume 5 of Collegium Logicum, pages87?143, Wien.
Kurt Go?del Society.Carolyn Penstein Rose?, Antonio Roque, DumisizweBhembe, and Kurt VanLehn.
2003.
A hybridapproach to content analysis for automatic essaygrading.
In Proceedings of the 2003 Conferenceof the North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology: companion volume of the Proceedingsof HLT-NAACL 2003?short papers - Volume 2,NAACL-Short ?03, pages 88?90, Edmonton, Canada.Association for Computational Linguistics.Gerard Salton and Michael J. McGill.
1983.
Introductionto modern information retrieval.
McGraw-Hill, NewYork.Roger Schwarzschild.
1999.
GIVENness, AvoidF andother constraints on the placement of accent.
NaturalLanguage Semantics, 7(2):141?177.Kurt VanLehn, Pamela W. Jordan, Carolyn Penstein Rose?,Dumisizwe Bhembe, Michael Boettner, Andy Gaydos,Maxim Makatchev, Umarani Pappuswamy, MichealRingenberg, Antonio Roque, Stephanie Siler, andRamesh Srivastava.
2002.
The architecture of why2-atlas: A coach for qualitative physics essay writing.In Proceedings of the 6th International Conferenceon Intelligent Tutoring Systems, volume 2363, pages158?167, Biarritz, France and San Sebastian, Spain,June 2-7.
Springer LNCS.200
