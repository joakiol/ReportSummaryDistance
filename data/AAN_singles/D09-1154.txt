Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1484?1492,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPDiscovery of Term Variation in Japanese Web Search QueriesHisami Suzuki, Xiao Li, and Jianfeng GaoMicrosoft Research, RedmondOne Microsoft Way, Redmond, WA 98052 USA{hisamis,xiaol,jfgao}@microsoft.comAbstractIn this paper we address the problem of identi-fying a broad range of term variations in Japa-nese web search queries, where these varia-tions pose a particularly thorny problem due tothe multiple character types employed in itswriting system.
Our method extends the tech-niques proposed for English spelling correc-tion of web queries to handle a wider range ofterm variants including spelling mistakes, va-lid alternative spellings using multiple charac-ter types, transliterations and abbreviations.The core of our method is a statistical modelbuilt on the MART algorithm (Friedman,2001).
We show that both string and semanticsimilarity features contribute to identifyingterm variation in web search queries; specifi-cally, the semantic similarity features used inour system are learned by mining user sessionand click-through logs, and are useful not onlyas model features but also in generating termvariation candidates efficiently.
The proposedmethod achieves 70% precision on the termvariation identification task with the recallslightly higher than 60%, reducing the errorrate of a na?ve baseline by 38%.1 IntroductionIdentification of term variations is fundamentalto many NLP applications: words (or more gen-erally, terms) are the building blocks of NLP ap-plications, and any robust application must beable to handle variations in the surface represen-tation of terms, be it a spelling mistake, validspelling variation, or abbreviation.
In search ap-plications, term variations can be used for queryexpansion, which generates additional queryterms for better matching with the terms in thedocument set.
Identifying term variations is alsouseful in other scenarios where semantic equiva-lence of terms is sought, as it represents a veryspecial case of paraphrase.This paper addresses the problem of identify-ing term variations in Japanese, specifically forthe purpose of query expansion in web search,which appends additional terms to the originalquery string for better retrieval quality.
Queryexpansion has been shown to be effective in im-proving web search results in English, where dif-ferent methods of generating the expansion termshave been attempted, including relevance feed-back (e.g., Salton and Buckley, 1990), correctionof spelling errors (e.g., Cucerzan and Brill, 2004),stemming or lemmatization (e.g., Frakes, 1992),use of manually- (e.g., Aitchison and Gilchrist,1987) or automatically- (e.g., Rasmussen 1992)constructed thesauri, and Latent Semantic Index-ing (e.g., Deerwester et al 1990).
Though manyof these methods can be applied to Japanesequery expansion, there are unique problemsposed by Japanese search queries, the most chal-lenging of which is that valid alternative spel-lings of a word are extremely common due to themultiple script types employed in the language.For example, the word for 'protein' can be spelledas ?????
?, ????
?, ??
?, ???
?and so on, all pronounced tanpakushitsu but us-ing combinations of different script types.
Wegive a detailed description of the problem posedby the Japanese writing system in Section 2.Though there has been previous work on ad-dressing specific subsets of spelling alterationswithin and across character types in Japanese,there has not been any comprehensive solutionfor the purpose of query expansion.Our approach to Japanese query expansion isunique in that we address the problem compre-hensively: our method works independently ofthe character types used, and targets a wide rangeof term variations that are both orthographicallyand semantically similar, including spelling er-rors, valid alternative spellings, transliterationsand abbreviations.
As described in Section 4, wedefine the problem of term variation identifica-1484tion as a binary classification task, and build twotypes of classifiers according to the maximumentropy model (Berger et al, 1996) and theMART algorithm (Friedman, 2001), where allterm similarity metrics are incorporated as fea-tures and are jointly optimized.
Another impor-tant contribution of our approach is that we de-rive our semantic similarity models by mininguser query logs, which has been explored for thepurposes of collecting related words (e.g., Joneset al, 2006a), improving search results ranking(e.g., Craswell and Szummer, 2007) and learningquery intention (e.g., Li et al, 2008), but not forthe task of collecting term variations.
We showthat our semantic similarity models are not onlyeffective in the term variation identification task,but also for generating candidates of term varia-tions much more efficiently than the standardmethod whose candidate generation is based onedit distance metrics.2 Term Variations in JapaneseIn this section we give a summary of the Japa-nese writing system and the problem it poses foridentifying term variations, and define the prob-lem we want to solve in this paper.2.1 The Japanese Writing SystemThere are four different character types that areused in Japanese text: hiragana, katakana, kanjiand Roman alphabet.
Hiragana and katakana arethe two subtypes of kana characters, which aresyllabic character sets, each with about 50 basiccharacters.
There is a one-to-one correspondencebetween hiragana and katakana characters, and,as they are phonetic, they can be unambiguouslyconverted into a sequence of Roman characters.For example, the word for 'mackerel' is spelled inhiragana as ??
or in katakana as ?
?, both ofwhich can be transcribed in Roman characters assaba, which is how the word is pronounced.Kanji characters, on the other hand, are ideo-graphic and therefore numerous ?
more than5,000 are in common usage.
One difficulty inhandling Japanese kanji is that each character hasmultiple pronunciations, and the correct pronun-ciation is determined by the context in which thecharacter is used.
For instance, the character ?
isread as kou in the word ??
ginkou 'bank', gyouin ?
'column', and i or okona in ???
itta'went' or okonatta 'done' depending on the con-text in which the word is used.1  Proper namereadings are particularly difficult to disambiguate,as their pronunciation cannot be inferred fromthe context (they tend to have the same grammat-ical function) or from the dictionary (they tend tobe out-of-vocabulary).
Therefore, in Japanese,computing a pronunciation-based edit distancemetric is not straightforward, as it requires esti-mating the readings of kanji characters.2.2 Term Variation by Character TypeSpelling variations are commonly observed bothwithin and across character types in Japanese.Within a character type, the most prevalent is thevariation observed in katakana words.
Katakanais used to transliterate words from English andother foreign languages, and therefore reflectsthe variations in the sound adaptation from thesource language.
For example, the word'spaghetti' is transliterated into six differentforms (??????
supagetti, ??????
?supagettii, ??????
supagettei, ????
?supageti, ??????
supagetii, ????
?supagetei) within a newspaper corpus (Masuya-ma et al, 2004).Spelling variants are also prevalent acrosscharacter types: in theory, a word can be spelledusing any of the character types, as we have seenin the example for the word 'protein' in Section 1.Though there are certainly preferred charactertypes for spelling each word, variations are stillvery common in Japanese text and search queries.Alterations are particularly common among hira-gana, katakana and kanji (e.g.
??~?
?~ ?
sa-ba 'mackerel'), and between katakana and Romanalphabet (e.g.
??????
fedekkusu fedex).This latter case constitutes the problem of transli-teration, which has been extensively studied inthe context of machine translation (e.g.
Knightand Graehl, 1998; Bilac and Tanaka, 2004; Brillet al, 2001).2.3 Term Variation by Re-write CategoriesTable 1 shows the re-write categories of relatedterms observed in web query logs, drawing onour own data analysis as well as on previouswork such as Jones et al (2006a) and Okazaki etal.
(2008b).
Categories 1 though 9 representstrictly synonymous relations; in addition, termsin Categories 1 through 5 are also similar ortho-graphically or in pronunciation.
Categories 101 In a dictionary of 200K entries, we find that on averageeach kanji character has 2.5 readings, with three characters(?,?,?)
with as many as 11 readings.1485through 12, on the other hand, specify non-synonymous relations.Different sets out of these categories can beuseful for different purposes.
For example, Joneset al(2006a; 2006b) target al of these categories,as their goal is to collect related terms as broadlyas possible for the application of sponsoredsearch, i.e., mapping search queries to a smallcorpus of advertiser listings.
Okazaki et al(2008b) define their task narrowly, to focusingon spelling variants and inflection, as they aim atbuilding lexical resources for the specific domainof medical text.For web search, a conservative definition ofthe task as dealing only with spelling errors hasbeen successful for English; a more general defi-nition using related words for query expansionhas been a mixed blessing as it compromises re-trieval precision.
A comprehensive review onthis topic is provided by Baeza-Yates and Ribei-ro-Neto (1999).
In this paper, therefore, we adopta working definition of the term variation identi-fication task as including Categories 1 through 5,i.e., those that are synonymous and also similarin spelling or in pronunciation.2 This definition isreasonably narrow so as to make automatic dis-covery of term variation pairs realistic, whilecovering all common cases of term variation inJapanese, including spelling variants and transli-terations.
It is also appropriate for the purpose ofquery expansion: because term variation definedin this manner is based on spelling or pronuncia-tion similarity, their meaning and function tend2 In reality, Category 3 (Inflection) is extremely rare in Jap-anese web queries, because nouns do not inflect in Japanese,and most queries are nominals.to be completely equivalent, as opposed to Cate-gories 6 through 9, where synonymy is morecontext- or user-dependent.
This will ensure thatthe search results by query expansion will avoidthe problem of compromised precision.3 Related WorkIn information retrieval, the problem of vocabu-lary mismatch between the query and the termsin the document has been addressed in manyways, as mentioned in Section 1, achieving vary-ing degrees of success in the retrieval task.
Inparticular, our work is closely related to researchin spelling correction for English web queries(e.g., Cucerzan and Brill, 2004; Ahmad andKondrak, 2005; Li et al, 2006; Chen et al, 2007).Among these, Li et al (2006) and Chen et al(2007) incorporate both string and semantic simi-larity in their discriminative models of spellingcorrection, similarly to our approach.
In Li et al(2006), semantic similarity was computed as dis-tributional similarity of the terms using querystrings in the log as context.
Chen et al (2007)point out that this method suffers from the datasparseness problem in that the statistics for rarerterms are unreliable, and propose using websearch results as extended contextual information.Their method, however, is expensive as it re-quires web search results for each query-candidate pair, and also because their candidateset, generated using an edit distance function andphonetic similarity from query log data, is im-practically large and must be pruned by using alanguage model.
Our approach differs from thesemethods in that we exploit user query logs toderive semantic knowledge of terms, which isCategories Example in English Example in Japanese1.
Spelling mistake aple ~ apple ????
guuguru ~ ????
gu-guru 'google'2.
Spelling variant color ~ colour ??~??~?
; ?????~???????
(Cf.
Sec.2.2)3.
Inflection matrix ~ matrices ??
tsukuru 'make' ~ ???
tsukutta 'made'4.
Transliteration  ??????
~ fedex 'Fedex'5.
Abbreviation/Acronymmacintosh ~ mac ????
sekaiginkou ~ ??
segin 'World Bank'; ??????
makudonarudo ~ ???
makku 'McDonald's'6.
Alias republican party ~ gop ????
furansu ~ ?
futsu 'France'7.
Translation ????????
pakisutantaishikan ~ Pakistan embassy8.
Synonym carcinoma ~ cancer ?
koyomi ~ ?????
karendaa 'calendar'9.
Abbreviation(user specific)mini ~ mini cooper ???????
kuronekoyamato ~ ????
kuroneko(name of a delivery service company)10.
Generalization nike shoes ~ shoes ????
??
shibikku buhin 'Civic parts' ~ ?
??
ku-ruma buhin 'car parts'11.
Specification ipod ~ ipod nano ???
toukyoueki 'Tokyo station' ~ ??????
tou-kyouekijikokuhyou 'Tokyo station timetable'12.
Related windows ~ microsoft ???
toyota 'Toyota' ~ ???
honda 'Honda'Table 1: Categories of Related Words Found in Web Search Logs1486used both for the purpose of generating a candi-date set efficiently and as features in the termvariation identification model.Acquiring semantic knowledge from a largequantity of web query logs has become popularin recent years.
Some use only query strings andtheir counts for learning word similarity (e.g.,Sekine and Suzuki, 2007; Komachi and Suzuki,2008), while others use additional information,such as the user session information (i.e., a set ofqueries issued by the same user within a timeframe, e.g., Jones et al, 2006a) or the URLsclicked as a result of the query (e.g., Craswelland Szummer, 2007; Li et al, 2008).
This addi-tional data serves as an approximation to themeaning of the query; we use both user sessionand click-through data for discovering term vari-ations.Our work also draws on some previous workon string transformation, including spelling nor-malization and transliteration.
In addition to thesimple Levenshtein distance, we also use genera-lized string-to-string edit distance (Brill andMoore, 2000), which we trained on aligned kata-kana-English word pairs in the same manner asBrill et al (2001).
As mentioned in Section 2.2,our work also tries to address the individualproblems targeted by such component technolo-gies as Japanese katakana variation, English-to-katakana transliteration and katakana-to-Englishback-transliteration in a unified framework.4 Discriminative Model of IdentifyingTerm VariationRecent work in spelling correction (Ahmed andKondrak, 2005; Li et al, 2006; Chen et al, 2007)and normalization (Okazaki et al, 2008b) formu-lates the task in a discriminative framework:??
= argmax?
?gen  ?
?(?|?
)This model consists of two components: gen(q)generates a list of candidates C(q) for an inputquery q, which are then ranked by the rankingfunction P(c|q).
In previous work, gen(q) is typi-cally generated by using an edit distance functionor using a discriminative model trained for itsown purpose (Okazaki et al, 2008b), often incombination with a pre-complied lexicon.
In thecurrent work, we generate the list of candidatesby learning pairs of queries and their re-writecandidates automatically from query session andclick logs, which is far more robust and efficientthan using edit distance functions.
We describeour candidate generation method in detail in Sec-tion 5.1.Unlike the spelling correction and normaliza-tion tasks, our goal is to identify term variations,i.e., to determine whether each query-candidatepair (q,c) constitutes a term variation or not.
Weformulate this problem as a binary classificationtask.
There are various choices of classifiers forsuch a task: we chose to build two types of clas-sifiers that make a binary decision based on theprobability distribution P(c|q) over a set of fea-ture functions fi(q,c).
In maximum entropyframework, this is defined as:?
?
?
=exp ????
?, ??
?=1exp ????
?, ??
?=1?where ?1,?, ?k are the feature weights.
The op-timal set of feature weights ?
* is computed bymaximizing the log-likelihood of the trainingdata.
We used stochastic gradient descent fortraining the model with a Gaussian prior.The second classifier is built on MART(Friedman, 2001), which is a boosting algorithm.At each boosting iteration, MART builds a re-gression tree to model the functional gradient ofthe cost function (which is cross entropy in ourcase), evaluated on all training samples.
MARThas three main parameters: M, the total numberof boosting iterations, L, the number of leafnodes for each regression tree, and v, the learningrate.
The optimal values of these parameters canbe chosen based on performance on a validationset.
In our experiments, we found that the per-formance of the algorithm is relatively insensi-tive to these parameters as long as they are in areasonable range: given the training set of a fewthousand samples or more, as in our experiments,M=100, L=15, and v=0.1 usually give good per-formance.
Smaller trees and shrinkage may beused if the training data set is smaller.The classifiers output a binary decision ac-cording to P(c|q): positive when P(c|q) > 0.5 andnegative otherwise.5 Experiments5.1 Candidate GenerationWe used a set of Japanese query logs collectedover one year period in 2007 and 2008.
Morespecifically, we used two different extracts of logdata for generating term variation candidatepairs:Query session data.
From raw query logs, weextracted pairs of queries q1 and q2 such that theyare (i) issued by the same user; (ii) q2 followswithin 3 minutes of issuing q1; and (iii) q2 gener-ated at least one click of a URL on the result1487page while q1 did not result in any click.
We thenscored each query pair (q1,q2) in this subset usingthe log-likelihood ratio (LLR, Dunning, 1993)between q1 and q2, which measures the mutualdependence within the context of web searchqueries (Jones et al, 2006a).
After applying anLLR threshold (LLR > 15) and a count cutoff(we used only the top 15 candidate q2 accordingto the LLR value for each q1), we obtained a listof 47,139,976 pairs for the 14,929,497 distinct q1,on average generating 3.2 candidates per q13.
Wetook this set as comprising query-candidate pairsfor our model, along with the set extracted byclick-through data mining explained below.Click-through data.
This data extract is basedon the idea that if two queries led to the sameURLs being repeatedly clicked, we can reasona-bly infer that the two queries are semanticallyrelated.
This is similar to computing the distribu-tional similarity of terms given the context inwhich they appear, where context is most oftendefined as the words co-occurring with the terms.Here, the clicked URLs serve as their context.One challenge in using the URLs as contex-tual information is that the contextual representa-tion in this format is very sparse, as user clicksare rare events.
To learn query similarities fromincomplete click-through data, we used the ran-dom walk algorithm similar to the one describedin Craswell and Szummer (2007).
Figure 1 illu-strates the basic idea: initially, document ?3 hasa click-through link consisting of query ?2 only;the random walk algorithm adds the link from ?3to ?1 , which has a similar click pattern as ?2 .Formally, we construct a click graph which is abipartite-graph representation of click-throughdata.
We use  ??
?=1?
to represent a set of querynodes and  ??
?=1?a set of document nodes.
Wefurther define an  ?
?
?
matrix ?
in which ele-ment ???
represents the click count associatedwith  ??
,??
.
This matrix can be normalized tobe a query-to-document transition matrix, de-3 We consider each query as an unbreakable term in thispaper, so term variation is equivalent to query variation.noted by ?, where ???
= ?(1)(??
|??)
is the prob-ability that ??
transits to ??
in one step.
Similarly,we can normalize the transpose of ?
to be adocument-to-query transition matrix, denoted by?, where ??
,?
= ?(1)(??|??
).
It is easy to see thatusing ?
and ?
we can compute the probability oftransiting from any node to any other node in ?steps.
In this work, we use a simple measurewhich is the probability that one query transits toanother in two steps, and the correspondingprobability matrix is given by ?
?.We used this probability and ranked all pairsof queries in the same raw query logs as in thequery session data described above to generateadditional candidates for term variation pairs.20,308,693 pairs were extracted after applyingthe count cutoff of 5, generating on average 6.8candidates for 2,973,036 unique queries.It is interesting to note that these two data ex-tracts are quite complementary: of all the datagenerated, only 4.2% of the pairs were found inboth the session and click-through data.
We be-lieve that this diversity is attributable to the na-ture of the extracts: the session data tends to col-lect the term pairs that are issued by the sameuser as a result of conscious re-writing effort,such as typing error corrections and query speci-fications (Categories 1 and 11 in Table 1), whilethe click-though data collects the terms issued bydifferent users, possibly with different intentions,and tends to include many spelling variants, syn-onyms and queries with different specificity(Categories 2, 8, 10 and 11).5.2 FeaturesWe used the same set of features for the maxi-mum entropy and MART models, which are giv-en in Table 2.
They are divided into three maintypes: string similarity features (1-16), semanticsimilarity features (17, 18), and character typefeatures (19-39).
Among the string similarityfeatures, half of them are based on Levenshteindistance applied to surface forms (1-8), while theother half is based on Levenshtein and string-to-string edit distance metrics computed over theRomanized form of the query, reflecting its pro-nunciation.
The conversion into Roman charac-ters was done deterministically for kana charac-ters using a simple mapping table.
For Romaniz-ing kanji characters, we used the function availa-ble from Windows IFELanguage API (versionFigure 1.
Random Walk Algorithm14882).4 The character equivalence table mentioned inthe features 3,4,7,8 is a table of 643 pairs of cha-racters that are known to be equivalent, includingkanji allography (same kanji in different graphi-cal styles).
The alpha-beta edit distance (11, 12,15, 16) is the string-to-string edit distance pro-posed in Brill and Moore (2001), which wetrained over about 60K parallel English-to-katakana Wikipedia title pairs, specifically tocapture the edit operations between English andkatakana words, which are different from the editoperations between two Japanese words.
Seman-tic similarity features (17, 18) use the LLR scorefrom the session data, and the click-though pairprobability described in the subsection above.Finally, features 19-39 capture the script types ofthe query-candidate pair.
We first defined sixbasic character types for each query or candidate:Hira (hiragana only), Kata (katakana only), Kanji(kanji only), Roman (Roman alphabet only),MixedNoKanji (includes more than one charac-ter sets but not kanji) and Mixed (includes morethan one character sets with kanji).
We then de-rived 21 binary features by concatenating thesebasic character type features for the combination4 http://msdn.microsoft.com/en-us/library/ms970129.aspx.We took the one-best conversion result from the API.
Theconversion accuracy on a randomly sampled 100 kanji que-ries was 89.6%.of query and candidate strings.
For example, ifboth the query and candidate are in hiragana,BothHira will be on; if the query is Mixed andthe candidate is Roman, then RomanMixed willbe on.
Punctuation characters and Arabic numer-als were treated as being transparent to charactertype assignment.
The addition of these features ismotivated by the assumption that appropriatetypes of edit distance operations might dependon different character types for the query-candidate pair.Since the dynamic ranges of different featurescan be drastically different, we normalized eachfeature dimension to a normal variable with zero-mean and unit-variance.
We then used the samenormalized features for both the maximum en-tropy and the MART classifiers.5.3 Training and Evaluation DataIn order to generate the training data for the bi-nary classification task, we randomly sampledthe query session (5,712 samples) and click-through data (6,228 samples), and manually la-beled each pair as positive or negative: the posi-tive label was assigned when the term pair fellinto Categories 1 through 5 in Table 1; otherwiseit was assigned a negative label.
Only 364 (6.4%)and 244 (3.9%) of the samples were positive ex-amples for the query session and click-throughdata respectively, which makes the baseline per-String similarity features (16 real-valued features)1.
Lev distance on surface form2.
Lev distance on surface form normalized by q1 length3.
Lev distance on surface form using character equivalence table4.
Lev distance on surface form normalized by  q1 length using character equivalence table5.
Lev distance on surface form w/o space6.
Lev distance on surface form normalized q1 length w/o space7.
Lev distance on surface form using  character equivalence table w/o space8.
Lev distance on surface form normalized by q1 using character equivalence table  w/o space9.
Lev distance on Roman10.
Lev distance on Roman normalized by q1 length11.
Alpha-beta edit distance on Roman12.
Alpha-beta edit distance on Roman normalized by q1 length13.
Lev distance  on Roman w/o space14.
Lev distance  on Roman normalized by q1 length w/o space15.
Alpha-beta edit distance on Roman w/o space16.
Alpha-beta edit distance on Roman normalized by q1 length w/o spaceFeatures for semantic similarity (2 real-valued features)17.
LLR score18.
Click-though data probabilityCharacter type features (21 binary features)19.
BothHira, 20.
BothKata, 21.
BothRoman, 22.
BothKanji, 23.
BothMixedNoKanji, 24.
BothMixed,25.
HiraKata, 26.
HiraKanji, 27.
HiraRoman, 28.
HiraMixedNoKanji, 29.
HiraMixed, 30.
KataKanji,31.KataRoman, 32.
KataMixedNoKanji, 33.
KataMixed, 34.
KanjiRoman, 35.
KanjiMixedNoKanji,36.
KanjiMixed, 37.
RomanMixedNoKanji, 38.
RomanMixed, 39.
MixedNoKanjiMixedTable 2: Classifier Features1489formance of the classifier quite high (alwayspredict the negative label ?
the accuracy will be95%).
Note, however, that these data sets includeterm variation candidates much more efficientlythan a candidate set generated by the standardmethod that uses an edit distance function with athreshold.
For example, there is a query-candidate pair q=????
kafuujouhou 'house-style information' c= ?
?
?
?
kafunjouhou'pollen information') in the session data extract,the first one of which is likely to be a mis-spelling of the second.5 If we try to find candi-dates for the query ????
using an edit dis-tance function naively with a threshold of 2 fromthe queries in the log, we end up collecting alarge amount of completely irrelevant set of can-didates such as ????
taifuujouhou 'typhooninformation', ???
kabu jouhou 'stock informa-tion', ????
kouu jouhou 'rainfall information'and so on ?
as many as 372 candidates werefound in the top one million most frequent que-ries in the query log from the same period; forrarer queries these numbers will only be worse.Computing the edit distance based on the pro-nunciation will not help here: the examplesabove are within the edit distance of 2 even interms of Romanized strings.Another advantage of generating the annotateddata using the result of query log data mining isthat the annotation process is less prone to sub-jectivity than creating the annotation fromscratch.
As Cucerzan and Brill (2004) point out,the process of manually creating a spelling cor-rection candidate is seriously flawed as the inten-tion of the original query is completely lost: forthe query gogle, it is not clear out of context ifthe user meant goggle, google, or gogle.
Usingdata mined from query logs solves this problem:an annotator can safely assume that if gogle-goggle appears in the candidate set, it is verylikely to be a valid term variation intended by theuser.
This makes the annotation more robust andefficient: the inter-annotator agreement rate for2,000 query pairs by two annotators was 95.7%on our data set, each annotator spending onlyabout two hours to annotate 2,000 pairs.5.4 Results and DiscussionIn order to compare the performance of two clas-sifiers, we first built maximum entropy andMART classifiers as described in Section 4 using5 ????
does not make any sense in Japanese; on theother hand, information about cedar pollen is commonlysought after in spring due to widespread pollen allergy.all the features in Section 5.2.
We run five expe-riments using different random split of trainingand test data: in each run,  we used 10,000 sam-ples for training and the remaining 1,940 samplesfor testing, and measured the performance of thetwo classifiers on the task of term variation iden-tification in terms of the error rate i.e., 1?accuracy.
The results, average over five runs,were 4.18 for the maximum entropy model, and3.07 for the MART model.
In all five runs, theMART model outperformed the maximum en-tropy classifier.
This is not surprising given thesuperior performance of tree-boosting algorithmspreviously reported on similar classificationtasks (e.g., Hastie et al, 2001).
In our task wheredifferent types of features are likely to performbetter when they are combined (such as semanticfeatures and character types features), MARTwould be a better fit than linear classifiers  be-cause the decision trees generated by MART op-timally combines features in the local sense.
Inwhat follows, we only discuss the results pro-duced by MART for further experiments.
Notethat the baseline classifier, which always predictsthe label to be negative, achieves 95.04% in ac-curacy (or 4.96% error rate), which sounds ex-tremely high, but in fact this baseline classifier isuseless for the purpose of collecting term varia-tions, as it learns none of them by classifying allsamples as negative.For evaluating the contribution of differenttypes of features in Section 5.2, we performedfeature ablation experiments using MART.
Table3 shows the results in error rate by variousMART classifiers using different combination offeatures.
The results in this table are also aver-aged over five run with random training/test datasplit.
From Table 3, we can see that the best per-formance was achieved by the model using allfeatures (line A of the table), which reduces thebaseline error rate (4.96%) by 38%.
The im-provement is statistically significant according tothe McNemar test (P < 0.05).
Models that usestring edit distance features only (lines B and C)did not perform well: in particular, the modelthat uses surface edit distance features onlyFeatures Error rate (%)A.
All features (1-39 in Table 2) 3.07B.
String features only (1-16) 3.49C.
Surface string features only (1-8) 4.9D.
No semantic feats (1-16,19-39) 3.28E.
No character type feats (1-18) 3.5Table 3: Results of Features Ablation ExperimentsUsing MART Model1490without considering the term pronunciation per-formed horribly (line C), which confirms the re-sults reported by Jones et al (2006b).
However,unlike Jones et al (2006b), we see a positivecontribution of semantic features: the use of se-mantic features reduced the error rate from 3.28(line D) to 3.07 (line A), which is statisticallysignificant.
This may be attributable to the natureof semantic information used in our experiments:we used the user session and click-though data toextract semantic knowledge, which may be se-mantically more specific than the probability ofword substitution in a query collection as awhole, which is used by Jones et al (2006b).Finally, the character type features also contri-buted to reducing the error rate (lines A and E).In particular, the observation that the addition ofsemantic features without the character type fea-tures (comparing lines B and E) did not improvethe error rate indicates that the character typefeatures are also important in bringing about thecontribution of semantic features.Figure 2 displays the test data precision/recallcurve of one of the runs of MART that uses allfeatures.
The x-axis of the graph is the confi-dence score of classification P(c|q), which wasset to 0.5 for the results in Table 3.
At this confi-dence, the model achieves 70% precision withthe recall slightly higher than 60%.
In the graph,we observe a familiar trade-off between preci-sion and recall, which is useful for practical ap-plications that may favor one over the other.In order to find out where the weaknesses ofour classifiers lie, we performed a manual erroranalysis on the same MART run whose resultsare shown in Figure 2.
Most of the classificationerrors are false negatives, i.e., the model failed topredict a case of term variation as such.
The mostconspicuous error is the failure to capture ab-breviations, such as failing to capture the altera-tion between ?????
juujoochuugakkou'Juujoo middle school' and ???
juujoochuu,which our edit distance-based features fail as thelength difference between a term and its abbrevi-ation is significant.
Addition of more targetedfeatures for this subclass of term variation (e.g.,Okazaki et al, 2008a) is called for, and will beconsidered in future work.
Mistakes in the Ro-manization of kanji characters were not alwayspunished as the query and the candidate stringmay contain the same mistake, but when theyoccurred in either in the query or the candidatestring (but not in both), the result was destruc-tive: for example, we assigned a wrong Romani-zation on ???
as suiginnakari ?mercury lamp?,as opposed to the correct suiginntou, which caus-es the failure to capture the alteration with ???
suiginntou, (a misspelling of ???).
UsingN-best (N>1) candidate pronunciations for kanjiterms or using all possible pronunciations forkanji characters might reduce this type of error.Finally, the features of our models are the editdistance functions themselves, rather than theindividual edit rules or operations.
Using theseindividual operations as features in the classifica-tion task directly has been shown to perform wellon spelling correction and normalization tasks(e.g., Brill and Moore, 2000; Okazaki et al,2008b).
Okazaki et al?s (2008b) method of gene-rating edit operations may not be viable for ourpurposes, as they assume that the original andcandidate strings are very similar in their surfacerepresentation ?
they target only spelling variantsand inflection in English.
One interesting futureavenue to consider is to use the edit distancefunctions in our current model to select a subsetof query-candidate pairs that are similar in termsof these functions, separately for the surface andRomanized forms, and use this subset to alignthe character strings in these query-candidatepairs as described in Brill and Moore (2000), andadd the edit operations derived in this manner tothe term variation identification classifier as fea-tures.6 ConclusionIn this paper we have addressed the problem ofacquiring term variations in Japanese query logsfor the purpose of query expansion.
We generateterm variation candidates efficiently by miningquery log data, and our best classifier, based onthe MART algorithm, can make use of both edit-distance-based and semantic features, and canidentify term variation with the precision of 70%at the recall slightly higher than 60%.
Our nextFigure 2: Precision/Recall Curve of MART01020304050607080901000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1confidenceprecision/recall(%)precisionrecall1491goal is to use and evaluate the term variation col-lected by the proposed method in an actualsearch scenario, as well as improving the per-formance of our classifier by using individual,character-dependent edit operations as features inclassification.ReferencesAhmad, Farooq, and Grzegorz Kondrak.
2005.
Learn-ing a spelling error model from search query logs.In Proceedings of EMNLP, pp.955-962.Aitchison, J. and A. Gilchrist.
1987.
Thesaurus Con-struction: A Practical Manual.
Second edition.ASLIB, London.Aramaki, Eiji, Takeshi Imai, Kengo Miyo, and Kazu-hiko Ohe.
2008.
Orthographic disambiguation in-corporating transliterated probability.
In Proceed-ings of IJCNLP, pp.48-55.Baeza-Yates, Ricardo, and Berthier Ribeiro-Neto.1999.
Modern Information Retrieval.
AddisonWesley.Berger, A.L., S. A. D. Pietra, and V. J. D. Pietra.
1996.A maximum entropy approach to natural languageprocessing.
Computational Linguistics, 22(1): 39-72.Bilac, Slaven, and Hozumi Tanaka.
2004.
A hybridback-transliteration system for Japanese.
In Pro-ceedings of COLING, pp.597-603.Brill, Eric, Gary Kacmarcik and Chris Brockett.
2001.Automatically harvesting katakana-English termpairs from search engine query logs.
In Proceed-ings of the Sixth Natural Language Processing Pa-cific Rim Symposium (NLPRS-2001), pp.393-399.Brill, Eric, and Robert C. Moore.
2000.
An improvederror model for noisy channel spelling.
In Proceed-ings of ACL, pp.286-293.Chen, Qing, Mu Li and Ming Zhou.
2007.
Improvingquery spelling correction using web search results.In Proceedings of EMNLP-CoNLL, pp.181-189.Craswell, Nick, and Martin Szummer.
2007.
Randomwalk on the click graph.
In Proceedings of SIGIR.Cucerzan, Silviu, and Eric Brill.
2004.
Spelling cor-rection as an iterative process that exploits the col-lective knowledge of web users.
In Proceedings ofEMNLP, pp.293-300.Deerwester, S., S.T.
Dumais, T. Landauer andHarshman.
1990.
Indexing by latent semantic anal-ysis.
In Journal of the American Society for Infor-mation Science, 41(6): 391-407.Dunning, Ted.
1993.
Accurate methods for the statis-tics of surprise and coincidence.
ComputationalLinguistics, 19(1): 61-74.Frakes, W.B.
1992.
Stemming algorithm.
InW.B.Frakes and R.Baeza-Yates (eds.
), InformationRetrieval: Data Structure and Algorithms, Chapter8.
Prentice Hall.Friedman, J.
2001.
Greedy function approximation: agradient boosting machine.
Annals of Statistics,29(5).Jones, Rosie, Benjamin Rey, Omid Madani and WileyGreiner.
2006a.
Generating query substitutions.
InProceedings of WWW, pp.387?396.Jones, Rosie, Kevin Bartz, Pero Subasic and Benja-min Rey.
2006b.
Automatically generating relatedaueries in Japanese.
Language Resources andEvaluation 40: 219-232.Hastie, Trevor, Robert Tibshirani and Jerome Fried-man.
2001.
The Elements of Statistical Learning.Springer.Knight, Kevin, and Jonathan Graehl.
1998.
Machinetransliteration.
Computational Linguistics, 24(4):599-612.Komachi, Mamoru and Hisami Suzuki.
2008.
Mini-mally supervised learning of semantic knowledgefrom query logs.
In Proceedings of IJCNLP,pp.358?365.Li, Mu, Muhua Zhu, Yang Zhang and Ming Zhou.2006.
Exploring distributional similarity basedmodels for query spelling correction.
In Proceed-ings of COLING/ACL, pp.1025-1032.Li, Xiao, Ye-Yi Wang and Alex Acero.
2008.
Learn-ing query intent from regularized click graphs.
InProceedings of SIGIR.Masuyama, Takeshi, Satoshi Sekine, and Hiroshi Na-kagawa.
2004.
Automatic construction of Japanesekatakana variant list from large corpus.
In Proceed-ings COLING, pp.1214-1219.Okazaki, Naoaki, Mitsuru Ishizuka and Jun?ichi Tsujii.2008a.
A discriminative approach to Japanese ab-breviation extraction.
In Proceedings of IJCNLP.Okazaki, Naoaki, Yoshimasa Tsuruoka, Sophia Ana-niadou and Jun?ichi Tsujii.
2008b.
A discriminativecandidate generator for string transformations.
InProceedings of EMNLP.Rasmussen, E. 1992.
Clustering algorithm.
InW.B.Frakes and R.Baeza-Yates (eds.
), InformationRetrieval: Data Structure and Algorithms, Chapter16.
Prentice Hall.Salton, G., and C. Buckley.
1990.
Improving retrievalperformance by relevance feedback.
Journal of theAmerican Society for Information Science, 41(4):288-297.Sekine, Satoshi, and Hisami Suzuki.
2007.
Acquiringontological knowledge from query logs.
In Pro-ceedings of WWW, pp.1223-12241492
