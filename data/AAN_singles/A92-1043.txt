Learning a Scanning Understanding for"Real-world" Library CategorizationSte fan  Wermter*Computer  Sc ience Depar tmentUn ivers i ty  of  Hamburg2000 Hamburg  50Federa l  Repub l i c  of  GermanyAbst ractThis paper describes, compares, and evaluates three dif-ferent approaches for learning a semantic lassification oflibrary titles: 1) syntactically condensed titles, 2) com-plete titles, and 3) titles without insignificant words areused for learning the classification in connectionist re-current plausibility networks.
In particular, we demon-strate in this paper that automatically derived featurerepresentations and recurrent plausibility networks canscale up to several thousand library titles and reach al-most perfect classification accuracy (>98%) comparedto a real-world library classification.1 In t roduct ionOur goal is to examine hybrid symbolic/connectionistand connectionist approaches for classifying a substan-tial number of real-world title phrases.
These approachesare embedded in the framework of SCAN (Wermter 92),a Symbolic Connectionist Approach for Natural lan-guage phrases, aimed towards a scanning understandingof natural anguage rather than focusing on an in-depthunderstanding.
For our experiments we took an existingclassification from the online catalog of the main libraryat Dortmund University and as a first subclassificationwe selected titles from three classes: "computer science"(CS), "history/politics" (HP), and "materials/geology"(MG).2 P reprocess ing  o f  T i t le  Phrases2.1 Symbol ic  Syntact i c  Condensat ionThe first approach used syntactic ondensation based ona chart parser and a headnoun extractor.
The symbolicchart parser built a syntactic structure for a title usinga context-free grammar and a syntactic lexicon.
Thenthe headnoun extractor etrieved the sequence of head-nouns for building a compound noun.
For instance, thecompound noun "software access guidelines" was gen-erated from "guidelines on subject access to microcom-puter software".
This headnoun extractor was motivated*This research was supported in part by the Fed-eral Secretary for Research and Technology under cont ract#01IV101AO and by the Computer Science Department ofDortmund University.by the close relationship between oun phrases and com-pound nouns and by the importance of nouns as contentwords (Finin 80).Each noun in a compound noun was representedwith 16 binary manually encoded semantic features, likemeasuring-event, changing-event, scientific-field, prop-erty, mechanism etc.
The set of semantic features hadbeen developed as a noun representation for a relatedscientific technical domain and had been used for struc-tural disambiguation (Wermter 89).
The first approachcontained a relatively small set of 76 titles since for eachnoun 16 features had to be determined manually and foreach word in the title the syntactic ategory had to bein the lexicon which contained 900 entries.2.2 Unrest r i c ted  Complete  PhrasesIn our second approach, we used an automatically ac-quired significance vector for each word based on theoccurrence of the words in certain classes.
Each valuev(w, ci) in a significance vector epresented the frequencyof occurrence of word w in class ci divided by the totalfrequency of word w in all classes.
These significancevectors were computed for the words of 2493 library ti-tles from the three classes CS, HP, and MG.2.3 E l iminat ion  of  Ins igni f icant  WordsIn the third approach we analyzed the most frequentwords in the 2493 titles of the second approach.
Weeliminated words that occured more than five times inour corpus and that were prepositions, conjunctions, ar-ticles, and pronouns.
Words were represented with thesame significance vectors as in the second approach.
Thiselimination of frequently occuring domain-independentwords was expected to make classification easier sincemany domain-independent i significant words were re-moved from the titles.3 The  Arch i tec ture  o f  the  Recur rentP laus ib i l i ty  NetworkThe semantic lassification was learned by using a con-nectionist recurrent plausibility network.
A recurrentplausibility network is similar to a simple recurrent net-work (Elman 89) but instead of learning to predictwords, recurrent connections support the assignment ofplausible classes (see figure 1).
The recurrent plausi-bility network was trained in a supervised mode using251the backpropagation learning algorithm (Rumelhart etal.
86).
In each training step the feature representa-tion of a word and its preceding context was presentedto the network in the word bank and context bank to-gether with the desired class.
A unit in the output layerreceived the value 1 if the unit represented the particularclass of the title, otherwise the unit received the value 0.The real-valued hidden layer represented the context ofpreceding words.
At the beginning of a title the contextbank was initialized with values of 0 since there was nopreceding context.
After the first word had been pre-sented the context bank was initialized with the valuesof the hidden layer that encoded the reduced precedingcontext.Classes!
!Hidden layer!Word bank Context bankSemantic nput featuresFigure 1: Recurrent Plausibility Network for Titles4 Resu l t s  and  Conclus ionsFor the first approach the 76 titles were divided into 61titles for training and 15 titles for testing.
For the 2493titles in the second and third approach we used 1249 ti-tles for training and 1244 for testing.
Using these train-ing and test sets we examined ifferent network archi-tectures and training parameters.
For the first approacha configuration with 6 hidden units and a learning rateof 0.001 showed the smallest number of errors on thetraining and test set.
For the second and third approach3 hidden units and the learning rate 0.0001 performedbest.Below we show example titles, the titles after pre-processing, and their sequential class assignment.
Thefirst two titles illustrate that two titles with the same fi-nal headnoun ("design") are assigned to different classesdue to their different learned preceding context.
Thethird title illustrates the second approach of classifyingan unrestricted complete phrase.
The network first as-signs the CS class for the initial phrase "On the op-erating experience of the..." since such initial represen-tations have occurred in the CS class.
However, whenmore specific knowledge is available ("doppler sodar sys-tem...") the assigned class is changed to the MG class.In the fourth example the same title is shown for thethird approach which eliminates insignificant domain-independent words.
In general, the second and thirdapproach ave the potential to deal with unanticipatedgrammatical nd even ungrammatical titles since theydo not rely on a predefined grammar.1.
Title: Design of relational database schemes bydeleting attributes in the canonical decomposition;Approachl: Compound noun: Decomposition (CS)attribute (CS) scheme (CS) design (CS)2.
Title: Design of bulkheads for controlling water inunderground mines; Approach1: Compound noun:Mine (MG) water (MG) bulkhead (MG) design(MG)3.
Title: On the operating experience of the dopplersodar system at the Forschungszentrum Juelich;Approach2: Unrestricted complete title: On (CS)the (CS) operating (CS) experience (CS) of (CS)the (CS) doppler (MG) sodar (MG) system (MG) at(MG) the (MG) Forschungszentrum (MG) Juelich(MG)4.
Title: On the operating experience Of the dopplersodar system at the Forschungszentrum Juelich;Approach3: Unrestricted reduced title:operating (CS) experience (CS) doppler (MG) so-dar (MG) system (MG) Forschungszentrum (MG)Juelich (MG)The overall performance of the three approaches asrecorded in the best found configuration is summarizedin table 1.
The first approach performed worst for clas-sifying new titles from the test set alhough the titlesin the training set were learned completely.
The secondapproach performed better on the test set for a muchbigger training and test set of unrestricted phrases.
Thethird approach demonstrated that the elimination of in-significant words from unrestricted phrases can improveperformance for the big set of titles.Performance Approach1 Approach2 Approach3Training 100% 98.4% 99.9%Testing 93% 97.7% 99.4%Table 1: Performance for Semantic ClassificationIn conclusion, we described and evaluated three dif-ferent approaches for semantic lassification which usehybrid symbolic/connectionist andconnectionist repre-sentations.
Our results show that recurrent plausibilitynetworks and automatically acquired feature representa-tions can provide an efficient basis for learning and gen-eralizing a scanning understanding of real-world libraryclassifications.ReferencesElman J.L.
1989.
Structured representations and connection-ist models.
Proceedings o\] the Eleventh Annual Conferenceo\] the Cognitive Science Society, Ann Arbor.Finin T.W.
1980.
The semantic Interpretation of Com-pound Nominals.
PhD Thesis.
University of Illinois atUrbana-Champaign.Rumelhart D.E., Hinton G.E., Williams R.J. 1986.
Learn-ing Internal Representations by Error Propagation.
In:Rumelhart D.E., McClelland J.L.
(Eds.)
Parallel distributedProcessing Vol.
1.
MIT Press, Cambridge, MA.Wermter, S. 1989.
Integration of Semantic and Syntac-tic Constraints for Structural Noun Phrase Disambiguation.Proceedings o\] the Eleventh International Joint Conlerenceon Artificial Intelligence, Detroit.Wermter, S. 1992 (forthcoming).
Scanning Understand-ing: A Symbolic Connectionist Approach .for Natural Lan-guage Phrases.
Technical Report.
University of Hamburg.252
