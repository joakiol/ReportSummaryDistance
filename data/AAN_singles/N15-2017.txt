Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 126?132,Denver, Colorado, June 1, 2015. c?2015 Association for Computational LinguisticsSpeeding Document Annotation with Topic ModelsForough Poursabzi-Sangdeh and Jordan Boyd-GraberComputer ScienceUniversity of Colorado Boulder{forough.poursabzisangdeh, Jordan.Boyd.Graber}@colorado.eduAbstractDocument classification and topic modelsare useful tools for managing and under-standing large corpora.
Topic models areused to uncover underlying semantic andstructure of document collections.
Catego-rizing large collection of documents requireshand-labeled training data, which is timeconsuming and needs human expertise.
Webelieve engaging user in the process of docu-ment labeling helps reduce annotation timeand address user needs.
We present an in-teractive tool for document labeling.
Weuse topic models to help users in this pro-cedure.
Our preliminary results show thatusers can more effectively and efficiently ap-ply labels to documents using topic modelinformation.1 IntroductionMany fields depend on texts labeled by humanexperts; computational linguistics uses such an-notation to determine word senses and senti-ment (Kelly and Stone, 1975; Kim and Hovy,2004); social science uses ?coding?
to scale upand systemetize content analysis (Budge, 2001;Klingemann et al, 2006).
In general text clas-sification is a standard tool for managing largedocument collections.However, these labeled data have to come fromsomewhere.
The process for creating a broadlyapplicable, consistent, and generalizable label setand then applying them to the dataset is longand difficult, requiring expensive annotators toexamine large swaths of the data.We present a user interactive tool for documentlabeling that uses topic models to help users as-sign appropriate labels to documents (Section 2).In Section 3, we describe our user interface andexperiments on Congressional Bills data set.
Wealso explain an evaluation metric to assess thequality of assigned document labels.
In prelimi-nary results, we show that annotators can morequickly label a document collection given a topicmodeling overview.
While engaging user in theprocess of content-analysis has been studied be-fore(as we discuss in Section 4), in Section 4 wedescribe how our new framework allows for moreflexibility and interactivity.
Finally, in Section 5,we discuss the limitation of our framework andhow we plan to extend it in future.2 Interactive Document LabelingWe propose an alternative framework for assign-ing labels to documents.
We use topic models togive an overview of the document contents to theuser.
Users can create a label set incrementally,see the content of documents, assign labels todocuments, and classify documents.
They cango back and forth in these steps and edit labelset or document labels and re-classify.Having labeled documents is necessary for au-tomatic text classification.
With a large collec-tion of unstructured documents, labeling can beexcruciating since it is essential to label enoughdocuments in different labels to obtain accept-able accuracy.
Topic models are a solution toreduce this effort since they provide some infor-mation about the underlying theme of corpus.Given a fixed number of topics, topic models126ClassificationTopic ModelsStart with LDADocument Labellinglabel 1label 2label m...label 1 label 2 label m...SLDA/LDAFigure 1: Interactive document labeling: Start with lda topic modeling, show users relevant documents foreach topic, get user labels, classify documents, and use slda to generate topics1.
Repeat this until the user issatisfied with labels.output (i) a set of words for each topic (Topicwords) and (ii) a distribution over topics for eachdocument (Document?s Topic Distribution).Topic words can be used to reveal the contentof a topic and thus content of documents with ahigh probability of that topic.
Therefore, assum-ing the number of topics is chosen carefully, topdocuments for each topic are similar in contentand can be labeled appropriately.Thus, rather than showing an unstructuredcollection of documents to the user, providingthe topic words and highly relevant documents tothat topic helps them in the process of documentlabeling, both in the step of choosing appropriatelabel names and choosing appropriate documentto assign a label to.
Another way to think aboutthis is that if the topics are perfect (they are nottoo general or too detailed), all labels associatedwith the topic?s high relevant documents can beviewed as subjects explaining the topic.
Table 1provides an example of how topic models canhelp a user craft document labels.Having a set of user labeled documents, classifi-cation algorithms can be used to predict the labelof unseen documents.
Next, classification resultsare shown.
Users can change document labels.They can also edit/delete label set and re-runthe classifier.
The explained procedure can berepeated iteratively until satisfaction is achievedwith existing (document,label) pairs.
Figure 1shows the explained procedure.3 Experiments with InteractiveLabeling InterfaceData: In our experiments, we need a labeledcorpus to be able to assess the quality of user-generated labels.
We chose US CongressionalBills corpus (Adler and Wilkerson, 2006).
Gov-Track provides bill texts along with the discussedcongressional issues as labels.
Example of labelsare ?education?, ?agriculture?, ?health?, and?defense?.
There are total of 19 unique labels.We use the 112thcongress, which has 12274 doc-uments.
We remove bills with no assigned goldlabel or that are short.
We end with 6528 docu-ments.Topic Modeling: To generate topics, we useMallet (McCallum, 2002) to apply lda on thedata.
A set of extra stop words are generatedbased on tf-idf scores to avoid displaying non-informative words to the user.Features and Classification: A crucial stepfor text classification is to extract useful featuresto represent documents.
Some common featuresfor text classification are n-grams, which makesthe dimensionality very high and classificationslower.
Since response time is very important inuser interactive systems, instead of n-grams, we1Currently, we are not using slda.
We just use theoriginal topics generated by lda.
The idea behind sldais explained in Section 5.127Topic Words Document Title Document Labels16 dod, sbir,afghanistan, phase,sttr, missile, com-bat, capabilities,command, elementsHR 4243 IH 112th CONGRESS 2d Session H. R. 4243To strengthen the North Atlantic Treaty Organiza-tion.military19historic,conveyance, dated,monument,depicted, generally,boundary, creek,preservation,recreationHR 4334 IH 112th CONGRESS 2d Session H. R. 4334To establish a monument in Dona Ana County, NewMexico, and for other purposes.wildlifeS 617 IS 112th CONGRESS 1st Session S. 617 Torequire the Secretary of the Interior to convey certainFederal land to Elko County, Nevada, and to takeland into trust for the Te-moak Tribe of WesternShoshone Indians of Nevada, and for other purposes.natureTable 1: An example of topic words and the labels user has assigned to top documents for that topic.use topic probabilities as features, which reducesthe dimensionality and classification time signifi-cantly.
User can choose 10, 15, 25, or 50 topics.We want to show the label probabilities gener-ated by classifier to users.
We use Liblinear (Fanet al, 2008) to run L2 regularized logistic regres-sion for classifying documents and generatinglabel probabilities.Interface: We start with the web-based in-terface of Hu et al (2014) for interactive topicmodeling.
The existing interface starts with ask-ing user information, corpus name, and numberof topics they want to explore.
Then it displaystopic words and the most relevant documents foreach topic.
Also, the user can see the content ofdocuments.
Users can create new labels and/oredit/delete an existing label.When seeing a document, user has 3 options:1.
Create a new label and assign that label tothe document.2.
Choose an existing label for the document.3.
Skip the document.At any point, the user can run the classifier.After classification is finished, the predicted la-bels along with the certainty is shown for eachdocument.
User can edit/delete document la-bels and re-run classifier as many times as theydesire.
We Refer to this task as Topic GuidedAnnotation(TGA).Figure 2 shows a screenshot of the interfacewhen choosing a label for a document.3.1 EvaluationWe introduce an interactive framework for docu-ment labeling using topic models.
In this section,we evaluate our system.Our goal is to measure whether showing usersa topic modeling overview of the corpus helpsthem apply labels to documents more effectivelyand efficiently.
Thus, we compare user-generatedlabels (considering labels assigned by user andclassifier altogether) with gold labels of US Con-gressional Bills provided by GovTrack.
Sinceuser labels can be more specific than gold labels,we want each user label to be ?pure?
in goldlabels.
Thus, we use the purity score (Zhao andKarypis, 2001) to measure how many gold labelsare associated with each user label.
Purity scoreispurity(U ,G ) =1N?kmaxj|Uk?Gj|, (1)whereU = {U1, U2, ..., UK} is the user clusteringof documents, G = {G1, G2, ..., GJ} is gold clus-tering of documents, and N is the total numberof documents.
Moreover, we interpret UkandGjas the set of documents in user cluster UKor gold cluster Gj.
Figure 3 shows an exampleof purity calculation for a clustering, given goldlabels.Purity is an external metric for cluster evalua-tion.
A very bad labeling has a purity score closeto 0 and a perfect labeling has purity score of 1.128Figure 2: A screenshot of interactive document labeling interface.
The user sees topic words and the mostrelevant documents for each topic.
The user has created two labels: ?Education?
and ?Health?
and sees thecontent of a documents.
The user can create a new label and assign the new label to the document, or chooseone of the two existing labels to assign to the document, or skip the document and view the previous or nextdocument.Figure 3: An example of computing purity: Clusterscorrespond to user labels and different shapes corre-spond to different associated gold labels.
Majoritygold label numbers for three clusters are 4(U1), 3(U2),and 5(U3).
Purity is117?
(4 + 3 + 5) ?
0.71.The higher this score, the higher the quality ofuser labels.To evaluate TGA, We did a study on twodifferent users.
For User 1, we chose 15 topics andfor User 2, we chose 25 topics.
They were askedto stop labeling whenever they were satisfiedwith the predicted document labels.We compare the user study results with a base-line.
Our baseline ignores topic modeling infor-mation for choosing documents to labels.
It con-siders the scenario when users are given a largedocument collection and are asked to categorizethe documents without any other information.Thus, we show randomly chosen documents tousers and want them to apply label to them.
Allusers can go back and edit or delete document la-bels, or refuse to label a document if they find itconfusing.
After each single labeling, we use thesame features and classifier that we used for userstudy with topic models to classify documents.Then we calculate purity for user labels withrespect to gold labels.
Figure 4 shows the purityscore over different number of labeled documentsfor User 1, User 2, and baseline.User 1 did the labeling in 6 rounds, whereasUser 2 did total of 7 rounds.
User 1 ended with116 labeled documents and user 2 had 42 labeleddocuments in the end.User 2 starts with a label set of size 9 and labels11 documents.
Two documents are labeled as?wildlife?, other two are labeled as ?tax?, and allother documents have unique labels.
This meansthat even if there are very few instance per label,baseline is outperformed.
This is an evidence of129Number of labeled documents0 20 40 60 80 100 120Purity0.10.20.30.4baselineTGA 2TGA 1Figure 4: Purity score over number of labeled docu-ments.
TGA 1 and TGA 2 refer to results for User1 and User 2.User 1 Baseline User 2 Baseline36 12 11 1250 52 17 1858 60 20 3882 109 23 40103 > 116 30 112116 > 116 35 11642 115(a) (b)Table 2: The number of required labeled documentsfor baseline to get the same purity score as (a) User1 (b) User 2, in each roundchoosing informative documents to assign labelswith the help of topic models.
On the other hand,User 1 starts with a label set of size 7 and labels36 documents and is outperformed by baselinesignificantly.
One reason for this is that assigningtoo many documents relevant to a topic, with thesame label doesn?t provide any new informationto the classifier and thus the user could get thesame purity score with a lower number of labeleddocuments, which would lead to outperformingbaseline.
User 1 outperforms the baseline in thesecond (8 labels and 50 labeled documents) andthird round (9 labels and 58 labeled documents)slightly.
In the fourth round, user creates morelabels.
With total of 13 labels and 82 labeleddocuments, the gap between user?s purity scoreand baseline gets larger.
Both users outperformbaseline in the final round.To see how topic models help speed up labelingprocess, we compare the number of user labeleddocuments with the approximate number of re-quired labeled documents to get the same purityscore in baseline.
Table 2 shows the results forUser 1 and User 2.User 1 starts with man labeled documents andbaseline can achieve the same performance withone third of the labeled documents.
As the userkeeps labeling more documents, the performanceimproves and baseline needs more labeled docu-ments to get the same level of purity.
For User2, baseline on average needs over two times asmany labeled documents to achieve the samepurity score as user labels.
These tables indicatethat topic models help users choose documentsto assign labels to and achieve an acceptableperformance with fewer labeled documents.4 Related WorkTopic Models such as Latent Dirichlet Allocation(Blei et al, 2003, lda) are unsupervised learningalgorithms and are a useful tool for understand-ing the content of large collection of documents.The topics found by these models are the set ofwords that are observed together in many doc-uments and they introduce correlation amongwords.
Top words in each topic explain the se-mantics of that topic.
Moreover, each documentis considered a mixture of topics.
Top topicsfor each document explain the semantics of thatdocument.When all documents are assigned a la-bel, supervised topic models can be used.slda (Mcauliffe and Blei, 2008) is a supervisedtopic model that generates topics that give anoverview of both document contents and assignedlabels.
Perotte et al (2011) extend slda and in-troduce hslda, which is a model for large-scalemultiply-labeled documents and takes advantageof hierarchical structure in label space.
hsldais used for label prediction.
In general, super-vised topic models help users understand labeleddocument collections.Text classification predicts a label for docu-ments and help manage document collections.There are known classifiers as well as featureextraction methods for this task.
However, pro-viding an initial set of labeled documents for bothtext classification and supervised topic modelsstill requires lots of time and human effort.Active learning (Settles, 2010), reduces theamount of required labeled data by having a130learner which actively queries the label for spe-cific documents and collects a labeled trainingset.
In a user interactive system, the activelearner queries document labels from users (Set-tles, 2010).
In other words, the learner suggestssome documents to the user and wants the userto assign a label to those.
Settles (2011) dis-cusses that having interactive users in annotationprocess along with active learning, reduces theamount of annotation time while still achievingacceptable performance.
In more detail, theypresents an interactive learning framework to getuser annotations and produce accurate classifiersin less time.
The shortcoming of active learningis that they don?t provide any overview infor-mation of corpus, like topic model approachesdo.Nevertheless, new methods in both analysisand evaluation are needed.
Classification algo-rithms restrict document labels to a predefinedlabel set.
Grimmer and Stewart (2013) showthat to be able to use the output of automatictext analysis in political science, we need care-ful validation methods.
There has been somework done on bringing user in this task for re-fining and evaluating existing methods.
Hu etal.
(2014) show that topic models are not perfectfrom the user view and introduce a framework tointeractively get user feedback and refine topicmodels.
Chuang et al (2013) present an inter-active visualization for exploring documents bytopic models to address user needs.We bring these tools together to speed up anno-tation process.
We believe having users engagedin content analysis, not only reduces the amountof annotation time, but also helps to achieve usersatisfaction.
We propose an iterative and userinteractive procedure for document annotation.We use topic models to provide some high-levelinformation about the corpus and guid users inthis task.
We show top words and documentsfor each topic to the user and have them start la-beling documents.
Users can create/edit/deletelabels.
Then users can run a classifier to predictthe labels for the unlabeled documents.
Theycan change document labels and re-classify docu-ments iteratively, until satisfaction is achieved.5 Future WorkThere are some obvious directions that will ex-pand this ongoing research.
First, we are plan-ning to use active learning to better aid clas-sification.
We expect that active learning willreduce the number of required labeled documentswhile still getting a high purity score and usersatisfaction.Second, we will use supervised topic mod-els (Mcauliffe and Blei, 2008, slda) instead oflda after the first round to update topics basedon document labels.
slda uses labeled docu-ments to find topics that explain both docu-ment content and their associated labels.
Webelieve using slda instead of lda after the firstround will give users more information about theoverview of documents and help them further forapplying labels to documents.Third, we want to allow the user to refineand correct labels further.
Our existing interfaceallows the user to delete a label or edit a label.We believe it is also important for users to mergelabels if they think the labels are too specific.
Inaddition, we believe a crucially important step isto generate the label set.
Giving the user someinformation about the range of documents canhelp them generate a better label set.
One otheroption is to suggest labels to users based on topicmodels (Lau et al, 2010).Fourth, we will explore other corpora suchas European Parliament corpus (Koehn, 2005).To our knowledge, there are no true labels forEuroparl corpus and using our interactive toolcan help users find the categorized informationthey need.Finally, for evaluating our method, in additionto using the correct labeling and purity score, wewill conduct a user experiment with more usersinvolved.
Since the task of labeling congressdata set requires some political knowledge, wewill choose annotators who have some politicalscience background.AcknowledgmentsWe thank the anonymous reviewers for theirinsightful comments.
We thank Dr. NiklasElmqvist for his advice for revising the user inter-131face.
We also thank Alvin Grissom II for helpingus in the user study.
This work was supportedby NSF Grant NCSE-1422492.
Any opinions,findings, results, or recommendations expressedhere are of the authors and do not necessarilyreflect the view of the sponsor.ReferencesE Scott Adler and John Wilkerson.
2006.
Congres-sional bills project.
NSF, 880066:00880061.David M. Blei, Andrew Ng, and Michael Jordan.
2003.Latent Dirichlet alocation.
Journal of MachineLearning Research, 3.Ian Budge.
2001.
Mapping policy preferences: esti-mates for parties, electors, and governments, 1945-1998, volume 1.
Oxford University Press.Jason Chuang, Yuening Hu, Ashley Jin, John DWilkerson, Daniel A McFarland, Christopher DManning, and Jeffrey Heer.
2013.
Document ex-ploration with topic modeling: Designing inter-active visualizations to support effective analysisworkflows.
In NIPS Workshop on Topic Models:Computation, Application, and Evaluation.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
The Journalof Machine Learning Research, 9:1871?1874.Justin Grimmer and Brandon M Stewart.
2013.
Textas data: The promise and pitfalls of automatic con-tent analysis methods for political texts.
PoliticalAnalysis, page mps028.Yuening Hu, Jordan Boyd-Graber, Brianna Satinoff,and Alison Smith.
2014.
Interactive topic model-ing.
Machine learning, 95(3):423?469.Edward F Kelly and Philip J Stone.
1975.
Com-puter recognition of English word senses, volume 13.North-Holland.Soo-Min Kim and Eduard Hovy.
2004.
Determiningthe sentiment of opinions.
In Proceedings of the20th international conference on ComputationalLinguistics, page 1367.
Association for Computa-tional Linguistics.Hans-Dieter Klingemann, Andrea Volkens, JudithBara, Ian Budge, Michael D McDonald, et al 2006.Mapping policy preferences II: estimates for par-ties, electors, and governments in Eastern Europe,European Union, and OECD 1990-2003.
OxfordUniversity Press Oxford.Philipp Koehn.
2005.
Europarl: A parallel corpusfor statistical machine translation.
In MT summit,volume 5, pages 79?86.Jey Han Lau, David Newman, Sarvnaz Karimi, andTimothy Baldwin.
2010.
Best topic word selectionfor topic labelling.
In Coling 2010: Posters, pages605?613, Beijing, China, August.Jon D Mcauliffe and David M Blei.
2008.
Supervisedtopic models.
In Advances in neural informationprocessing systems, pages 121?128.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://www.cs.umass.edu/ mccallum/mallet.Adler J. Perotte, Frank Wood, Noemie Elhadad, andNicholas Bartlett.
2011.
Hierarchically supervisedlatent Dirichlet alocation.
In Advances in NeuralInformation Processing Systems 24, pages 2609?2617.Burr Settles.
2010.
Active learning literature survey.University of Wisconsin, Madison, 52(55-66):11.Burr Settles.
2011.
Closing the loop: Fast, interac-tive semi-supervised annotation with queries onfeatures and instances.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1467?1478.
Association for Com-putational Linguistics.Ying Zhao and George Karypis.
2001.
Criterionfunctions for document clustering: Experimentsand analysis.
Technical report, Citeseer.132
