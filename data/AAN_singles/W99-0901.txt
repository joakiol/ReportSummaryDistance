Hid ing  a Semant ic  H ie rarchy  in a Markov  ModelSteven AbneyAT&:T Labs180 Park AveFlorham Park, NJ 07932abney@research, att.
comMarc LightThe MITRE Corporation202 Burl ington RoadBedford, MA 01730 USAlight@mitre, orgAbstractWe introduce a new model of selectionalpreference induction.
Unlike previous ap-proaches, we provide a stochastic genera-tion model for the words that appear asarguments of a predicate.
More specifi-cally, we define a hidden Markov modelwith the general shape of a given seman-tic class hierarchy.
This model has a num-ber of attractive features, among them thatselectional preference can be seen as dis-tributions over words.
Initial results arepromising.
However, unsupervised param-eter estimation has proven problematic.
Acentral problem is word sense ambiguity inthe training corpora.
We describe attemptsto modify the forward-backward algorithm,an EM algorithm, to handle such disam-biguation.
Although these attempts wereunsuccessful at improving performance, webelieve they give insight into the nature ofthe bottlenecks and into the behavior of theEM algorithm.1 IntroductionWe describe here an approach to inducing selectionalpreferences from text corpora.
In the traditionalview, a predicate constrains its arguments by select-ing for particular semantic lasses, or concepts.
Se-lectional restriction of the traditional sort can becharacterized as a relation p(v, r, c) over predicatesv, syntactic roles r, and argument concepts c. Indi-vidual instances (v, r, c) are selectional tuples.
Ex-amples are given in table 1.Of more interest to computational linguistics isselectional preference, a continuous-valued gener-alization of selectional restriction.
Selectional pref-erence is a mapping ~ : (v,r,c) ~ a that mapseach tuple (v,r,c) to a real number a, the degreePredicate Role Argument Classsplatter subj CAUSAL-AGENTsplatter obj FLUIDsplatter on SURFACETable 1: Selectional tuplesof preference of v for c with respect o role r. Pos-itive degrees of preference are intended to correlatewith intuitive judgments of "plausibility" or "typi-cality," and negative judgments are intended to cor-relate with intuitive judgments of "implausibility.
"We have chosen to characterize such selectionalpreference as a side-effect of a stochastic model forgenerating what we will call co-occurrence tuples:triples (v, r, n) for v a predicate, r a syntactic role,and n the headword of the argument filling the roler with respect o v. An example of a co-occurrencetuple is (splatter, obj, water).
Co-occurrence tuplescan be obtained from text corpora, and can be usedto make inferences about the probability of selec-tional tuples.
For example, the co-occurrence tuple(splatter, obj, water) may be taken as evidence forthe selectional tuple (splatter, obj, FLUID).
Moreconcretely, such co-occurrence tuples make up thetraining corpora, from which we train our stochasticmodels.For this study, we have used the British NationalCorpus (100M words), from which we have extractedco-occurrence tuples using the Cass parser (Abney,1997).
By way of illustration, table 2 shows the val-ues of n in tuples (eat, obj, n) along with their freq-uencies in the corpus.
This "subcorpus" would beused to train a stochastic model specific to the ob-ject role of the verb eat and is the first of two inputsto our induction process.There are two problems with such training data:it is noisy and it contains ambiguity.
The noiseis sometimes due to tagging or parsing errors, andsometimes due to metaphorical uses.
Examples frommeat 45 bucket 1 ice 2tape 1 investment 1 soup 2proportion 2 kitchen 1 fry 4root 4 salad 2 top 1bread 14 feast 1 scrap 2majority 2 sauce 1 sugar 1principle 1 food 77 hole 2roll 4 pack 1 bag 2race 1 mouthful 3 dinner 11sheep 1 salt 1 meal 46trout 2 pasta 1 slice 7dish 2 spaghetti 6 chicken 5stick 1 egg 18 average 1sandwich 13 yogurt 1 mustard 1breakfast 30 garlic 1Table 2: Objects of eat in the BNCtable 2 include investment, average, tape, and race'.However, note that the "good" examples uch as foodand meal are much greater in number and frequency'.Thus, the signal is stronger than the noise in mostcases and most reasonably robust training methodswill be able to handle the noise.The second problem, that of word sense ambigu-ity, is more difficult.
The word bread in table 2 pro-vides an example.
Bread can be used to refer to afood, e.g., the multigrain bread in Germany is won-derlul, but it can also refer to money, e.g., I couldreally use some bread since my car just broke down.For this reason, it is not immediately clear whichconcepts the 14 tokens of bread provide evidence for.If the wrong choice is made for a high frequencyword, incorrect selectional preferences will result.The model we propose represents this sort of un-certainty in a natural way: the two senses of breadare represented as different paths through a stochas-tic model, both of which generate the same obser-vation.
This stochastic model is a hidden Markovmodel (HMM) which has the shape of a given se-mantic hierarchy.
Figure 1 shows an example hi-erarchy.
In the work discussed here, we made useof the WordNet semantic hierarchy (Miller, 1990).This hierarchy is the second input to our inductionprocess.We hoped that the forward-backward algorithm,an EM algorithm, would properly disambiguateword senses in the training data as a side effect ofits quest to maximize the likelihood of the trainingdata given the model.
However, for reasons we willdiscuss in section 4, this was not the case.In the following section we discuss work on se-lectional preference induction that also assumes asinput (i) subcorpora corresponding to predicate rolepair and (ii) a semantic lass hierarchy.
Then weTOPLOCA~ON ENTITYLEc'E.FORM CAUSAL-AGENTPERSON e~ ~ "~'~BEEs ~ m , , r ~ ZWORKERBEE WORKER FEMALE MALEFigure 1: Example Semantic Class Hierarchyformally define our stochastic model.
Next we lookat a number of ultimately unsuccessful attempts tomodify the forward-backward algorithm to performeffective word-sense disambiguation of the trainingdata.
Despite these problems we did obtain someencouraging results which we present at the end ofthe paper.2 Re la ted  workThere' have been a number of attempts to deriveselectional preferences using parsed corpora and asemantic class hierarchy.
Our work is closely re-lated to that of (Resnik, 1993).
His system pro-vides a distribution over classes conditioned on apredicate-role pair: p( clv, r ).
It estimates p( clv , r )as f (v , r , c ) /~c '  f (v,r ,c ' ) ,  where f (v , r ,c)  is in turnapproximated by allocating the frequency of the co-occurrence tuple (v, r, n) among the classes C(n) towhich the senses of n belong.
For example, sup-pose the word bread has two senses, BREAD andMONEY.
Suppose further that BREAD is a hyponymof BAKED-GOODS, FOOD, ARTIFACT, and TOP, andMONEY is a hyponym solely of TOP.
Then C(bread)is (BREAD, BAKED-GOODS, FOOD, ARTIFACT, TOP,MONEY).
Tokens of bread are taken as ambiguous ev-idence for all concepts in C(bread); the weight of ev-idence is divided uniformly across C(bread).
Henceeach token of (eat, obj, bread) counts as 1/6 of atoken of (eat, obj, BREAD), 1/6 of a token of (eat,obj, BAKED-GOODS), and so on.
Such a uniform al-lotment is does not reflect empirical distributions ofsenses, which are Zipf-like, but does produce reason-able results.
It is important o note that Resnik isnot very explicit about how the probability p(clv, r)is to be interpreted; there is no explicit stochasticgeneration model involved.Resnik uses p(clv, r) to quantify selectional pref-erence by comparing it to p(c), the marginal proba-bility of class c appearing as an argument.
He mea-sures the difference between these distributions astheir relative entropy (D).
The total amount of "se-2lection" that a predicate v imposes on the filler ofrole r is quantified as D(p(c\[v,r)\[\[p(c)).
The selec-tional preference of v for c in role r is quantifiedas the contribution of the c to the total amount ofselection:p( c\[v, r) log eJs.ly_ffO_ p(c)selpref(v, r c) = Dfp(c'lv, r)llP(C'))The class or classes produced as the output for thepredicate are those with the highest selpref value.Other work on the induction of selectional pref-erences includes (Li and Abe, 1995).
They charac-terize the selectional restriction of a predicate witha horizontal cut through a semantic hierarchy, anduse the principle of Minimum Description Length(MDL) to choose a cut that optimally balances im-plicity and descriptive adequacy.
More specifically, acut is a set of concepts that partition the set of nounsbelonging to the hierarchy.
A cut is deemed simplerif it cuts the hierarchy at a higher place (i.e., the cutcontains fewer concepts), and descriptive adequacyis measured by comparing the actual distribution ofnouns filling a slot (v, r) to the closest approxima-tion one can obtain by estimating p(nlc ) for only theconcepts c in the cut.
Again, the intended stochasticgeneration model is not clear.As mentioned, the interpretation of expressionssuch as p(clv, r) is obscure in these previous mod-els.
Without clarity about what stochastic processis producing the data, it is difficult to gauge howwell probabilities are being estimated.
In addition,?
having an explicit stochastic generation model en-ables one to do a number of things.
First, onecan experiment with different methods of eliminat-ing word sense ambiguity in the training corpusin a principled fashion.
Second, it is often possi-ble to calculate a number of useful distributions.From our model, the following distributions canbe efficiently estimated: Pr(word\[predicate, role),Pr (word\[ semantic-class, predicate, role),and Pr( word-senselword ,predicate,role).
Thesedistributions can be used directly to help solveambiguity resolution problems such as syntac-tic structure disambiguation.
In addition, thePr(wordlpredicate, role ) distribution can be seenas a very specific language model, i.e., a languagemodel for the head of the argument of the predicate.3 Our  S tochast i c  Generat ion  Mode lOur model generates co-occurrence tuples (e.g., (eat,obj, bee\])) as follows.
The probability p(v, r, n)of a co-occurrence tuple can be expressed asp(v,r)p(nlv, r ).
Our central concern is the condi-tional probability p(nlv, r).
We associate a separateHMM with each pair (v,r) in order to characterizethe distribution p(nlv ,r).
Thus, the HMM for (eat,obj) would be different han that for (drink, subj).That  is, the general structure of the HMM would bethe same but the parameters would be different.The states and transitions of the HMMs are iden-tified with the nodes and arcs of a given semanticclass hierarchy.
The nodes of the hierarchy representsemantic lasses (concepts), and the arcs representhyponymy (that is, the "is-a" relation).
Some con-cepts are expressible as words: these concepts areword senses.
A sense may be expressible by multi-ple words (synonyms) and, conversely, a single wordmay be an expression of more than one sense (wordsense ambiguity).
For expository reasons, we assumethat all and only the terminal nodes of the hierarchyare word senses.
In actuality, the only constraint oursystem places on the shape of the hierarchy is thatit have a single root.A "run" of one of our HMMs begins at the rootof the semantic hierarchy.
A child concept is chosenin accordance with the HMM's transition probabil-ities.
This is done repeatedly until a terminal node(word sense) c is reached, at which point a word wis emitted in accordance with the probability of ex-pressing sense c as word w. Hence, each HMM "run"can be identified with a path through the hierarchyfrom the root to a word sense, plus the word thatwas generated from the word sense.
Also, every ob-servation sequence generated by our HMMs consistsof a single noun: each run leads to a final state, atwhich point exactly one word is emitted.More formally, a concept graph is given, and anexpressibility relation from nodes to words.
Tilenodes of the graph are identified with concepts C ={c l , .
.
.
,cn},  and the expressibility relation relatesconcepts to words kY = {wl , .
.
.
,wm}.
The HMMconsists of a set of states {ql,...,qn}, which weidentify with the nodes of the concept graph; a set ofpossible emissions which we identify with W U {e}(that is, we permit non-emitting states); and threeparameter matrices:A = {aij} The transition probabilities.
The valueaij represents the probability of making a tran-sition from state qi to state qj.
aij is nonzeroonly if there is an arc in the concept graph fromconcept ci to concept cj.B = {bj(k)} The emission probabilities.
The valuebj(k) represents the probability of emittingword Wk while in state q/.
States correspondingto nonterminal nodes in the concept graph arenon-emitting (that is, they emit e with prob-ability 1), and states corresponding to termi-3nal nodes are emitting states (they emit e withprobability 0).~- = {Tri} The initial state distribution, rri is identi-cally 1 for the start state (corresponding to theroot node), and 0 for all other states.As mentioned, we associate an HMM with eachpair (v,r).
Each HMM has the same structure, de-termined by the semantic hierarchy.
Where theydiffer is in the values of the associated parameters.To estimate parameters, we require a training sam-ple of observation sequences.
Since each observationsequence consists of a single word, a training sampleis simply a collection of word tokens.
The trainingsample consists of the nouns filling the associated"slot" (v , r ) - - that  is, a token of the noun n is in-cluded in the training sample for each token of thetuple (v,r ,n)  that occurs in the corpus.
Table 2provides an example corpus.This approach permits us to address both wordsense disambiguation and selectional preference.
Anambiguous word is one that could have been gen-erated by means of more than one state sequence.For a given ambiguous word n appearing in a slot(v,r), we can readily compute the posterior prob-ability that word sense c was used to generate n,according to the (v, r) model.
We can disambiguateby choosing the word sense with maximum poste-rior probability, or we can use the probabilities ina more sophisticated model that uses more contex-tual information than just the slot in which the wordappears.Selectional preferences for (v, r), can be extractedfrom these models by calculating the distributionover classes p(clv, r) from the model trained for (v, r)and tile distribution p(c) from a model trained onall nouns.
One can then follow Resnik and use sel-pref(v,r,c) as defined above.
These distributions canbe calculated by considering our HMMs with addi-tional transitions going from all leaf states to theroot state.
Such HMMs are ergodic and thus theprobability of being in a particular state at a timet converges to a single value as t approaches oo.These steady-state probabilities can be put entirelyin terms of the parameters of the model.
Thus, oncean HMM has been trained, the steady state proba-bilities can be easily calculated.
Because of the cor-respondence between states and classes, these steadystate distributions can be interpreted as a distribu-tion over classes.As mentioned earlier, another way of thinkingabout selectional preference is as a distribution overwords.
For example, the selectional preference of theverb eat for its direct object would be expressed byhigh probabilities for words like breakfast, meat, andbread and low probabilities for words like thought,computer, and break.
This conception of selectionalpreference is related to language modeling in speechrecognition.
In fact, the selectional preference of apredicate-role pair can be thought of as a very spe-cific language model.
This way of thinking aboutselectional preferences i useful because it points topossible applications in speech recognition.4 Parameter  Es t imat ionWe had originally hoped that after turning our se-mantic hierarchy into an HMM as described above,we could simply run the standard forward-backwardalgorithm on the training corpus and we would geta useful model.
Unfortunately, there are a numberof reasons why this does not work.
We will describethese problems and our attempted solutions in thecontext of disambiguating the words in the train-ing data with multiple word senses, a fundamentaltask in the estimation of selectional preferences.
Ineach of the three sub-sections below we describe aproblem we discovered and an attempted solution.In the end, we were not able to produce a systemthat performed better than Resnik's system on hisword-sense disambiguation evaluation.
This evalua-tion is an indirect way of testing whether the trainingmethod is word sense disambiguating the trainingcorpora correctly.
However, when we derived fromour models a ranked list of classes using p(clv ,r)and Divergence as described above, we obtained verygood lists.
We present some representative lists andthe results on Resnik's evaluation in section 5.In addition, we think the attempted solutions areinstructive and provide insight into the nature of theproblem and the behavior of the EM algorithm.4.1 Smooth ingIt was our original hope that, by treating the choiceof word sense as just another hidden variable in theHMM, word-sense disambiguation would be accom-plished as a side effect of EM estimation.
In fact,however, there is no pressure in the model in favorof parameter settings in which occurrences of an am-biguous word are all accounted for by a single wordsense.
If the initial parameter settings account foran ambiguous word as a mixture of word senses, theconverged model does likewise.
This should come asno surprise to those with experience using EM, but isnot usually stated very clearly in the literature: theEM algorithm estimates a mixture model and (intu-itively speaking) strongly prefers mixtures contain-ing small amounts of many solutions over mixturesthat are dominated by any one solution.,4c.psilon = 115 TOPCOGNITION FIX)D cpsilon = 20C p p , ~  FLESH FRUIT BREAD DAIRY~SSENCE "" ' i ~ i J iI " "  ~t  ?
gpple ~S?I r.~?~~to lFigure 2: SmoothingFor example, consider Figure 2.
We assume aminiature training corpus, containing one instanceeach of four words, meat, apple, bagel, cheese.The word meat is ambiguous, having both senseESSENCE and sense FLESH.
The training corpusis perfectly accounted for by the weights in Figure 2,and this is indeed a fixed point of the EM algorithm.One would like to introduce some pressure towardconsolidating word occurrences under a single wordsense.
Further, one would like the set of word sensesone ends up with to be as closely related as possible.In Figure 2, for example, one would like word meatto shift as much of its weight as possible to senseFLESH,  not sense the ESSENCE.We sought to accomplish this in a natural way bysmoothing transition probabilities, as follows.
Thetransition probabilities out of a given state consti-tute a probability distribution.
At a given iterationof the EM algorithm, the "empirical" distributionfor a given state is the distribution of counts acrossoutgoing transitions, where the counts are estimatedusing the model produced by the previous iteration.
(Hence the scare quotes around empirical.
For wantof a better term, let us call this distribution pseudo-empirical.
)For example, assume the parameter settingsshown in Figure 2 to be the output of the previousiteration, and assume that each word appears oncein the training corpus.
Then the (estimated) countfor the path through transition FOOD ~ FLESH is1/2, and the count for the paths through transitionsFOOD --~ FRUIT ,  FOOD ~ BREAD,  FOODDAIRY  is 1 each.
Hence, the total count for thestate FOOD is 3.5.
Dividing each transition countby the count for state FOOD yields the pseudo-empirical probabilities {1/7, 2/7, 2/7, 2/7}.The pseudo-empirical probabilities would nor-mally be installed as transition weights in the newmodel.
Instead, we mix them with the uniformdistribution {1/4, 1/4, 1/4, 1/4}.
Let p(t) be thepseudo-empirical probability of transition t, and letu(t) be the uniform probability of transition t. In-stead of setting the new weight for t to p(t), we setit to E u(t) ?
(1 - v)p(t).Crucially, we make the mixing parameter, e, afunction of the total count for the state.
Intuitively,if there is a lot of empirical evidence for the distribu-tion, we rely on it, and if there is not much empiricalevidence, we mix in a larger proportion of the uni-form distribution.
To be precise, we compute ~ as1/ (c+ 1), for c the total count of the state.
This hasthe desirable property that E is 1 when c is 0, anddecreases exponentially with increasing c.It is probably not immediately obvious howsmoothing in this manner helps to prune undesiredword senses.
To explain, consider what happens inFigure 2.
There are two paths from the root tothe word meat, one leading through the word senseESSENCE and the other leading through the wordsense FLESH.
In the "previous" model (i.e., theweights shown), each of those paths has the sameweight (namely, 1/8), hence each instance of theword meat in the training corpus is taken as evi-dence in equal parts for word senses ESSENCE andFLESH.The difference lies in the states COGNIT IONand FOOD.
Words apple, bagel, and cheese, alongwith half of meat, provide evidence for the stateFOOD,  giving it a total count of 31/2; but the onlyevidence for state COGNIT ION is the other half ofmeat, giving it a total count of 1/2.
The new distri-bution for COGNIT ION has a large admixture ofthe uniform distribution, whereas the distribution ofFOOD has a much smaller uniform component.The large proportion of uniform probability forthe state COGNIT ION causes much of its probabil-ity mass to be "bled off" onto siblings of ESSENCE(not shown, but indicated by the additional outgoingedges from COGNIT ION) .
Since none of these sib-ling are attested in the training corpus, this makesCOGNIT ION's  fit to the training corpus very poor.Intuitively, this creates pressure for TOP to reducethe weight it apportions to COGNIT ION and in-crease its weight for FOOD;  doing so improves themodel's overall fit to the training corpus.This decreases the relative count for the wordsense ESSENCE in the next iteration, increasingthe pressure to shift weight from COGNIT ION toFOOD.
Ultimately, an equilibrium is reached inwhich most of the count for word meat is assigned tothe word sense FLESH.
(What prevents a total shiftto the word Sense FLESH is smoothing at TOP ,which keeps a small amount of weight on COGNI -T ION.
In a large hierarchy, this translates to a van-ishingly small amount of weight on ESSENCE.
)5TOPCOGNIT ION F(XID cpsilon = 3/5q'~,~llon IIFLESH PLANT-PART BREAD DAIRYESSENCE I ."
" " "  I ~ !!
- .~p~.  "
II~g?l ?.hces?~?arFigure 3: Imbalanced Senses4.2 Sense Ba lanc ingIn Figure 2, our smoothing method produces the de-sired bias for the corpus meat, apple, bagel, cheese.However, in different circumstances the bias pro-duced is not the desired one.
Consider training thehierarchy in Figure 3 on a corpus made up of onetoken of meat.The hierarchy in Figure 3 differs from the hier-archy in Figure 2 in that meat has three senses,two of which share a prefix path, i.e., the transi-tion from TOP to FOOD.
When training on thecorpus of one token of meat, 2/3 of the count wouldgo down the FOOD side and the other third downthe COGNITION side; thus, with respect to theforward-backward algorithm, there is little differencebetween the current example and the previous one.Therefore, the two senses of meat under FOOD willbe preferred.
Intuitively this is wrong, because thereis no information in the corpus on which to derivea bias for any one sense and we would like our pa-rameter settings to reflect this.
In addition, this isalso not simply a border case problem, since if meatis very frequent, as in the corpus in Table 2, it couldeasily happen that such an a priori bias for certainsenses of meat drowns out the bias that should resultfrom the other words in the corpus.In concrete terms, the problem is the sharedpath prefix that exists for the senses under FOOD,namely the transition from TOP to FOOD.
Moreabstractly, the problem is that the hierarchy is notbalanced with respect o the senses of meat--if therewere another sense under ESSENCE there would beno problem (see Figure 4).One can simulate such a phantom sense within theforward-backward algorithm.
First the count for thetransitions in the prefix path have to be reduced.This can be done by modifying the E step such thatthe expectation, Ew(X i , j ) ,  for the random variable,Xi~j,  which corresponds to the transition from statei to state j for a single token of word w, is calculatedTOPCOGNrHON FOODf FLESH PLANT-PART BREAD DAIRYESSENCE... f l  .
.
.
.
.  "
' "  I ' / -  T TFigure 4: Sense BalancingTOPC( )G NITION F(R)D PL,-MWT- PARTESSENCE FLESH.,ofFigure 5: Graph with Reentrancyas follows.Ew(Xi.j)w)where Ew() is the expectation based on the modeland corpus and D(j,w) is the number of uniquepaths starting at j and ending in a state that cangenerate w. One then sums over all tokens of thecorpus to get the expectation for the corpus.The second step is to reduce tile probability of thepaths to the sister sense of the phantom sense, e.g.,COGNIT IONs  ESSENCE.
This can be achieved byincreasing the normalization factor used in the Mstep:: Ew( (r,w) -Once again, we focus on the contribution of a singletoken of a word w and thus the normalization factorused in the M step would be the sum Aw over thetokens in the corpus.
The state r is the starting stateof the model, i.e., the state corresponding to theroot of the hierarchy.
The exception to this formulaoccurs when D(r,w) -D( i ,w)  = O, in which caseA,~ = Ew.There are other ways of modifying the algorithmto simulate the phantom sense.
However, thismethod is easy and efficient o implement since theE and M steps remain simple local calculations--theonly global information comes through the functiond which can be efficiently and easily computed.Another kind of sense imbalance is shown in Fig-ure 5.
This imbalance can be corrected by further6")~)pABSTRACT43BJECT F(X)D* *"C()GNI'rlON\[ :i'ESSENCE s?
.
, I?
t*Figure 6: Path on the Right is Preferred Due to itsShorter Lengthmodifying the E step as follows:v(j, w)u(j)where U(j) is the number of unique paths up tothe root from j.4.3 Length  and  Width  Ba lanc ingMost of the example hierarchies/models we haveconsidered so far have been balanced with respectto length and width, i.e., the length of the paths tothe generating states has been uniform and the num-ber of transitions out of a state has been uniformacross states.
It turns out that uniform length andwidth are important characteristics with respect oour modified forward-backward algorithm: shorterpaths are preferred to longer ones (see Figure 6) andpaths that go through states with few exiting tran-sitions are preferred to ones that go through stateswith many (see Figure 7).
In fact, short paths arepreferred to longer ones by the standard forward-backward algorithm, since in an HMM the proba-bilities of events in a sequence are multiplied to getthe probability of the sequence as a whole.
Widthonly comes into play when one introduces mooth-ing.
Remember that in our smoothing, we mix in theuniform probability.
Consider the transitions com-ing out of the state COGNITION in Figure 7; thereare four transitions and thus the uniform probabil-ity would be 1/4.
In contrast, the transitions comingout of the state FOOD in the same figure numberonly 2 and thus the uniform distribution would be1/2.
If there are many transitions the probabilitymixed for the uniform distribution will be smallerthan if there were fewer transitions.We can solve the problem by balancing the hier-archy: all paths that result in generating a symbolshould be of the same length and all distributionsshould contain the same number of members.
As inTOPCC~NITION FOODESSENCE FLESH.
,alFigure 7: Path on the Right is Preferred Due to itsRelatively Narrow Distributionsthe previous ection, we can simulate this balancingby modifying the forward-backward algorithm.First, to balance for width, the smoothing can bemodified as follows: instead of mixing in the uni-form probability for a particular parameter, alwaysmix in the same probability, namely the uniformprobability of the largest distribution, umax (i.e.,the state with the largest number of exiting tran-sitions; in Figure 7, this maximum uniform proba-bility would be 1/4).
Thus the smoothing formulabecomes E u,~x + (1 - c)p(t).
This modification hasthe following effect: it is as if there are always thesame number of transitions out of a class.
Widthbalancing for emission parameters i performed inan analogous fashion.Let us turn to length balancing.
Conceptually,in order to balance for length, extra transitions andstates need to be added to short paths so that theyare as long as the maximum length path of the hi-erarchy.
It should be noted that we are only con-cerned with paths that end in a state that generateswords.
The extension of short paths can be simu-lated by multiplying the probability of a path by afactor that is dependent on its length:Pr~b(p) = Prob(p)umax (lengLh'~* -len~Lh(p) )This additional factor can be worked into the for-ward and backward variable calculations o thatthere is no loss in efficiency.
It is, thus, as iflengthmaz - length(p) states have been added andthat each of these states has Urnax -1  exiting transi-tions.5 P re l iminary  Resu l tsAs mentioned above, we tested our trained modelson a word-sense disambiguation evaluation, reason-ing that if it performed poorly on this evaluation,then it must not be disambiguating the training cor-pus very well.
The bottom line is that we werenot able to advance the state of the art - - the per-formance results are comparable to, but not better7than, those obtained by Resnik.
We used the train-ing sets, test sets, and evaluation method describedin (Resnik, 1997).
1 Table 3 presents performanceresults.
The Random method is simply to randomlypick a sense with a uniform distribution.
The FirstSense method is to always pick the most commonsense as listed in WordNet.
The HMM smoothedmethod is to use models trained with smoothingbut no balancing modifications.
HMM balanced usessmoothing and all three balancing modifications.MethodRandom 28.5%First Sense 82.8%Resnik 44.3%HMM smoothed 35.6%HMM balanced 42.3%Table 3: Word Sense Disambiguation ResultsNext we give examples of the preferences derivedfrom trained models for three verbs, , representedas weights on classes.
These are typical rather thanbest-case xamples.
We have not yet attempted anyformal evaluation of these lists.eat 0.321048 food0.245948 substance0.209142 nutriment0.156176 object0.144745 entity0.072242 mealabandon 0.078877 content0.061569 psychological feature0.057775 idea0.056840 cognition0.038888 plan0.025118 activity0.023805 attempt0.023058 act0.021834 beliefbreak 0.033223 object0.020298 law0.020287 law of nature0.018689 substance0.016658 ice0.016407 solid0.015359 guidance0.014416 rule0.014345 entity0.014334 crystal1We would like to thank Philip Kesnik for providingus with the training and test data that he used in theabove mentioned work.6 Conc lus ionIn the last section , we showed why the straight-forward application of an EM algorithm, namelythe forward-backward algorithm, would not disam-biguate the sensese of input words as desired.
Thus,we introduced a type of smoothing which producedthe desired bias in the example at hand.
Then weshowed how this smoothing, when used on certaingraphs, produced unwanted biases which then neces-sitated further modifications in the E and M stepsof the algorithm.
In the end, even with smoothing,sense, length, and width balancing, the performanceof the EM-like estimation was disappointing.One possible lesson is that EM itself is inappro-priate for this problem.
Despite the fact that it hasbecome the default method for uncovering hiddenstructure in NLP problems, it essentially averagestogether many possible solutions.
Possibly, a lesslinear method that eventually commits to one oranother hypothesis about hidden structure may bemore appropriate in this case.In conclusion, this paper has made the followingcontributions: it has shown how a stochastic gener-ation model can make use of a semantic lass hierar-chy, it has provided a negative result with respect oparameter estimation for this model, and in doing sohas provided an interesting illustration of the innerworkings of the forward-backward algorithm.ReferencesAbney, Steven 1997.
Partial parsing via finite-statecascades.
Natural Language Engineering, 2(4).Li, Hang and Abe, Naoki 1995.
Generalizing caseframes using a thesaurus and the MDL principle.In Proceedings ofRecent Advances in Natural Lan-guage Processing.Miller, George 1990.
WordNet: An On-Line LexicalDatabase.
International Journal of Lexicography,3(4).Rabiner, L. R. A tutorial on Hidden Markov Mod-els and selected applications in speech recognition.Proceedings of the IEEE, 77(2):257-285, February1989.Resnik, Philip Selection and Information: A Class-Based Approach to Lexical Relationships.
PhDthesis, University of Pennsylvania, Philadelphia,PN, 1993.Resnik, Philip Selectional preference and sense dis-ambiguation.
In Proceedings of the ANLP-97Workshop: Tagging Text with Lexical Semantics:Why, What, and How?, Washington, D.C., 1997.8
