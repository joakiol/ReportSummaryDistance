A Statistical Approach to Machine Aided Translationof Terminology BanksJyun-Sheng ChangDepartment ofComputer ScienceNational Tsing Hua University, lqsinchu, 30043, Taiwanjschang@cs.nthu.edu.twAndrew ChangResearch and DevelopmentDeuchemie Industries, Inc.26, Kuangfu South Road, Hsinchu Industrial ParkHsinchu, TaiwanTsuey-Fen Lin and Sur-Jin KerDepartment ofComputer ScienceSooChow University, Taipei, TaiwanAbstract"l\]fis paper reports on a new statistical approach tomachine aided translation of terminology bank.
The textin the bank is hyphenated and then dissected into roots of1 to 3 syllables.
Both hyphenation and dissection aredone with a set of initial probabilities of syllables androots.
The probabilities are repeatedly revised using anEM algorithm.
Alter each iteration of hyphenation ordissectioh, the resulting syllables and roots are countedsubsequently to yield more precise estimation ofprobability.
The set of roots rapidly converges to a set ofmost likely roots.
Preliminary experhuents have shownpromising results.
From a terminology bank of more than4,000 terms, the algorithm extracts 223 general andchemical roots, of which 91% are actually roots.
Thealgoritlun dissects a word into roots with aromld 86% hitrate.
The set of roots and their "hand-translation are thenused iu a compositional translation of the terminologybank.
One can expect he translation of terminology bankusing this approach to be more cost-effective, consistent,and with a better closure.1.
IntroductionExisting machine translators work well for limiteddomains (Slocum, 1985).
Wlmn an MT system istransported to another domain, among other things, thedomain specific terms have to be acquired andtranslated before the system can do any reasonablework again (Knowles, 1982).
Current ways of handlingthis porting process are largely manual.
Usually oneeither gleans domain specific tenns from large amountof document at once and translates them one by one byhand, or translated each unkalown term when it appears.l l iese previous approaches all involve large amount ofeffort of  more than one person.
The long and tediousprocess may often result in inconsistent translation.Furthermore, no dictiolmry is complete, but still wehope tlmt the translation system produces sometrunslation when encountering an unknown word.However, U'anslation of  terms on a one-for-one basisIres no closure.
When eneounteruig an unknown term,however similar to a known one, the system will not beable to fall softly and produce some kind of  reasonablyacceptable translation like a human translator does.Similar consideration motives a text-to-speech researchon producing pronunciation for an mflulown wordsthrough morphological decomposition (Black et al1991).This paper reports on a project experimenting on a newapproach to this problem.
The project involvesstatistical lexical acquisition from a large corpus ofdocument o build a terminology bank, and automaticextraction of roots from tile tenuinology bank.
Theidea is to perform htmlan translation of these roots andto translate a term by composing the translation of itsconstituent roots.
This idea is similar to the root-oriented dictiotmry proposed ill (Tufts and Popescu,1991).
Certain mnoant of postedithlg is expected.However, over all, we expect this method to savesignificant mnom~t of  human effort, produce moreconsistent translatioa, and resolt in better closure suchthat the system can fall gracefully whan encounteringan unknown word.
"lhe rest of  the paper will tocns on the acquisition ofroots from a terminology bank.
Section 2 statesfonnally the problem.
Section 3 describes our approachAcr?s DE COLING-92, NANTES.
23-28 AOtrr 1992 9 2 1 PROC.
OF COLlNG-92, NANTES, AUG. 23-28, 1992to root acquisition.
Section 4 describes the setup of ourexperiments and reports some preliminary results.Section 5 concludes the paper with some remarks andpoints out directions for future research.2.
The Problem of Root AcquisitionSuppose that we have a large amount of terms through amanual or automatic lexical acquisition process.
Inthese terms, there is always certain degree ofredundancy in the form of repeated occurrence ofcertain general or domain specific roots in differentwords (or words in noun-noun compounds).
In order totake advantage of the redundancy and reduce the effortof translating these terms, there is the need fordiscovering the roots automatically.
So given a set ofterms, we are supposed to produce a list of roots thatappear more than twice in the terminology bank.
Forexample, givenacidimeter acidity amide antibiotic antiblockingcyanoacrylate gloss glossmeterhydroxybonzylmoth hydrometer mildewmildewiclde polyacryl polyacrylamidepolyacrylonitdle polyacrylsulfone acrylalkydpacrylate polyacrylate polyamide polyolpolytributyltinacrylatewe are suppose to produceacryl, amide, amine, anti, block, cide, gloss,meter, mildew, hydro, el, polyAfter hand translation, we getacryl ~Jt~anti I~block I?icide ~ ~Jgloss Y~meter 1~mildew 1~hydro 7J~el ~polyNow we are in a position to translate the originalterminology bank by the composition of  the translatedroo~:antiblocking I~ ~glossmeter ~hydrometer 7 J~mildowicide t~ ~ ~Jpolyacryl ~ J~polyol ~B~3.
Root AcquisitionA root can be anywhere between one and up to I 1characters (such as phosphazene in pho~phazene,polyaryloxyphosphazene, a d polyphosphazene).
Tocarry out a statistical analysis on a letter by letter basiswould mean searching for scarce roots (102-103) in avery large search space (1015).
However, a root can beeither from one to 3 syllables long and there are butabout some 2,000 syllables.
So if we analyze the data assyllables, the search space is drastically reduced (1010).So, we choose to hyphenate words in the terminologybank first and extract only roots that are made of I to 3syllables.If we had in advance the appearing frequency of thesyllables and roots in the terminology bank, we couldsimply use them to compute the most likelyhyphenation or dissection.
After the whole term banksare hyphenated and dissected, we can then not onlyproduce the list of the most likely roots in theterminology bank, but also produce the frequency countof each syllable or root.
However, in most cases, we donot have the frequency count of syllables and roots inthe first place, a dilemma.Both hyphenation and root dissection are attacked usingthe EM algorithm (Dempster et at.
1977).
In brief, theEM algorithm for the root dissection problem workslike this: given some initial estimate of the rootprobability, any dissection of all the terms in theterminology bank into roots can be evaluated accordingto this set of initial root probability.
We can computetile most likely dissection of terms into roots using tileinitial root probabilities.
We then re-estimate theprobability of any root according to this dissection.Repeated applications of  the process lead to probabilitythat assign ever greater probability to correct dissectionof term into roots.
This algorithm leads to a local butacceptable maximum.3.1.
HyphenationPrevious methods Ibr hyphenation are all based on rulesabout the nature of characters (consonant or vowel)and can only achieve about 90% hit rate (Knuth, 1985;Smith, 1989).
The other 10% is done using anexception dictionary.
These hyphenation algorithms arenot feasible for our purpose because of  the low rate andreliance on an exception dictionary.
Therefore, we havedeveloped a statistical approach to hyphenation.
Tileidea is to collect frequency count of  syllables incorrectly hyphenated words.
Then we use the frequencyto estimate the likelihood of a syllable inACTES DE COLING-92.
NAMES, 23-28 AO~r 1992 9 2 2 PROC.
OF COL1NG-92, NANTES, AUG. 23-28, 1992Algorithm 1.
HyphenationInput:Word = WlW 2 ... W n the word to hyphenateSylProb - probability of syllablesOutput:Pos - positiOns of hyphensLocal:prob - probability of optimal hyphenatiOn at apositiOnprev- previous hyphenation positiOn1.
prob\[0\] = 1 .
; prev\[0\] = 0;2.
For i= l tondo3&43.
j* = max prob\[i-j\] x SylProb(Sj)Jwhere SI = WH,1 WH,2.. Wi.4.
prob\[i\] = prob\[i-j*\] * SylProb(Sj*);prevli\] = j*;5.
Compute Pos by tracing back the linked liststarting from prev\[len\].Algorithm 2.
Root DissectionInput:Word - the ~rd  to dissectRootProb - the estimated root probabilitiesOutput:Pos - the starting positions of rootsLocal:prob - probability of optimal dissection at apositionprev - previOus dissecting position1.
Hyphenate Word into n syllables.Word = S 1 S 2 ... S n2.
prob\[0\] = 1; prey\[0\] = 0;3.
For i := l tondo4&54.
j" = max prob\[i-j\] x RootProb(Ri)j=1,3where Rj = SH,1SH,2...S i5.
prob\[i\] :- prob\[i-j*\] x RootProb(Rl,);prev\[i\] =j*6.
Compute Pos by tracing back prev links startingfrom prev\[n\].a possible hyphenation and choose the hyphenation thatconsists of  a most likely sequence of  syllables.
Theoptimization process is done through a dynamicprogramming algorithm described in Algorithm 1.3,2.
Root DissectionChie can set the initial estimate of  the probability ofsingle-, bi-, and tri-syllabl?
roots as follows:polychloroprene flexible foampolychioroprene rubberpolycondensatepolycondensationpolycondensation resinpolydiallylphthalatepolydienepolydimethyl butadienepolydimethylsiloxanepolydiolefinpolydioxyarylene diphenyl silanepolydioxycycloalkylene diphenylpolydiphenylsulphonemaleimidepolydispersitypolyelectrolytepolyenepolyepichlorohydriopolyepichiorohydrin rubberpolyesterpolyester acrylate resinpolyester amidepolyester dithiolF igure  1.
An excerpt  f rom a chemical  terminologybanka241 a.
13 ab25 ac354ac.
1 act.
1 ad47 aer6a f7  ag59 age.
15air4air.
2 a1141 al.
26 am 49an 106 an.
8 ance.
7 and.
2Figure 2.
Independent syllable probabilitiesabi 3 abil 3 able.9abra 6 absorb 2 absorp 9accel 8 accep 3 ace 40acene.
4 aci 7 acid.
177acous 2 acri 3 acro 3Figure 3.
Syllable bigramsProb(R) = SylProb(S),for is a single-syllable root R = S= Bigram(S i $2),for a bi-syllable root R = SIS 2= Min(Bigram(StS2) , Bigram(S2,S3))for a tri-syllable root R = S18283,The root dissection is done using Algorithm 2 which issimilar to the hyphenation algorithm.ACTES DE COLING-92.
NANTES, 23-28 AOIYf 1992 9 2 3 PROC.
OF COLING-92, NANTES, AUO.
23-28, 1992a 2 a.
2 able.
4abrasion.
3abrasive.
3 absorption.
7accelo 4 aceno.
2 acetal.
6acetate.
2 acetic.
4 aceto 3acety 5 acetyl.
2 acid.
177acidi 22 acoustic.
2 acridine.
2acryl 3 acryl.
2 acrylat 4Figure 4.
Roots extracted after the first iterationplastic 5 plasticisation plasticised plasticiserplasticityPO 6 antipode epichlopohyddn pored porositypoteniometricpolari 4 polarisation polarity polarizationpoly 302 polyacetal polyacrolein polyacrylamidepolyacrylatepolymer 28 copolymedzation polymedsationcopolymedsatepolymer.
14 prepolymer terpolymer photopolymerbiopolymerport.
4 export import supportposition, 5 composition decompositionpre 18 prechrome precipitate precipitatedprecipitationprene.
5 chloroprene polychloroprenepr/3 prileshajev primary pdmedessFigure 5.
Roots extracted after the last iteration4.
Exper imental  ResultsThe experiment has been carried out on a personalcomputer running a C-H- compiler under DOS 5.0.
Theterminology bank consists of more than 4,000 lines ofchemical terms compiled by a leading chemicalcompany hi Germany for internal use.
Each lineconsists of from 1 to 5 words and a word can be anywhere from 1 to 15 syllables long or 2 to 31 characterslong.The initial syllable probabilities used in thehyphenation algorithm are the appearance counts ofsome 1,800 distinct syllables in a partially hyphenateddata, which is the result of running Latex (Knuth 1986)on the terminology bank itself.The root dissection algorithm uses the syllableprobability and bigram of syllables to start the EMalgoritlun.
Small segments of the bigram and rootprobabilities produced in the first iteration are shown inFigure 2 and Figure 3 respectively.To facilitate human translation, in the last iteration, weproduce the exemplary words along side with the rootfound.
A small segment is shown in Figure 5.Following the terminology of research in informationretrieval, we can evaluate the performance of this rootextraction method:precision =number of correct rootsnumber of roots foundnumber of correct rootsrecall =number of actual rootsThese two numbers can be calculated for allappearances of roots or for the set of distinct rootsrespectively.
We have extracted 223 distinct and morefrequently occurring root and 203 of them are validroots.
To analyze precision and recall for alloccurrences, we have randomly sampled 100 terms, inwhich a domain expert identified 237 roots and ouralgorithm split into 195 valid roots in 226 proposedroots.
Thus, counting all occurrences of root, theprecision and recall rates are as follows:precision recall86.3%=(195/226) 82.3%=(195/237)If distinct roots are counted, the precision and recallrates are as follows:precision recall91.0%=(203/223) Not available5.
Concluding RemarksOur approach is very similar to the research onidentifying Chinese words in the absence of delimiters(such as spaces in English) by Sproat and Shih (1990).They have used a greedy method and the wordsidentified are limited to 2-syllable words.
Incomparison, we use a global optimization algorithmthrough dynamic programming and identify roots up to3 syllables long.The results have shown that statistical approaches arevery robust and through an EM algorithm, we canextract roots effectively to cut down cost in translation,achieve better consistency and closure.The limitations of the current approach ineinde thefollowing: (1) Some roots do not end at syllableboundary and that results in acquisition of incompleteroots or no acquisition at all.
(2) Currently, we are notperforming any kind of prefix or suffix analysis.Therefore, some words having the same charactersequence are incorrectly split.
That results in overgeneration of roots.AcrEs DE COLING-92, NANTES, 23-28 AOt\]T I992 9 2 4 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992We are now working on the following: (1) Changingthe root splitting algorithm in the target languageprocess from syllable-based to letter-based.
(2)Translation of roots.
(3) Formulation of the process ofgenerating teml translation in Chinese from translatedroots.Tufts, D. and O. Popeseu.
A Unified Management andProcessing of Word-forms, Idioms and AnalyticalCompounds, In Proceedings of the 5th Conference ofthe European Chapter of the ACL, pages 95-100, 1991.AcknowledgmentThis research was supported by the National ScienceCouncil, Taiwan, under Contracts NSC 81-0408-E007-13 and -529.ReferencesBlack, A.W., J. van de Plassche and B. Williams.Analysis of Unknown Words through MorphologicalDecomposition, I  Proceedings of the 5th Conference ofthe European Chapter of the ACL, pages 101-107,1991.Chang, J.S., C.D.
Chen, and S.D.
Chang.
Chinese WordSegmentation through Constraint Satisfaction andStatistical Optimization, In Proceedings of ROCComputational Linguistics Conference, pages 147-166,Kenting, Taiwan, 1991, (in Chinese).Chang, J.S., S.D.
Chen, and J.D.
Chen.
Conversion ofPhonemic Input to Text through Cott~traint Satisfaction,In Proceedings of Internal Conference on ComputerProcessing of Chinese and Oriental Languages, pages30-36, Nankuan, Taiwan, 1991.Dempster, A.P., N.M. Laird and D.B.
Rubin.
MaximumLikelihood from incomplete Data via the EMalgorithm, J. of the Royal Statistical Society 39, pages1-38, 1977.Knowles, F.E.
The pivotal Role the Various Dictionaryin an MT system, in Practical Experience of MachineTranslation, V. Lawson, Ed.
North-Holland,Amsterdam, pages 149-162, 1982.Knuth, D. The TeXbook, Prentice Hall, Reading,Massachusetts, 1985.Sloeum, J. ,4 Survey of Machine Translation,Computational Linguistics 11, pages 1-15, 1985.Smith, A.
Text Processing, MIT Press, 1989.Sproat, R. and Ch.
Shill.
d Statistical Method forFinding Word Boundaries in Chinese Text, ComputerProcessing of Chinese and Oriental Languages, pages336-351, 1990.ACTE.S DE COLING-92, NANTES, 23-28 AOt~'r 1992 9 2 5 PRO?:.
OF COLING-92, NANTES, AUG. 23-28, 1992
