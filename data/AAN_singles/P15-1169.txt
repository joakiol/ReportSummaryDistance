Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1754?1764,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsAn analysis of the user occupational class through Twitter contentDaniel Preot?iuc-Pietro1, Vasileios Lampos2and Nikolaos Aletras21Computer & Information Science, University of Pennsylvania2Department of Computer Science, University College Londondanielpr@sas.upenn.edu, {v.lampos,n.aletras}@ucl.ac.ukAbstractSocial media content can be used as acomplementary source to the traditionalmethods for extracting and studying col-lective social attributes.
This study focuseson the prediction of the occupational classfor a public user profile.
Our analysis isconducted on a new annotated corpus ofTwitter users, their respective job titles,posted textual content and platform-relatedattributes.
We frame our task as classifi-cation using latent feature representationssuch as word clusters and embeddings.
Theemployed linear and, especially, non-linearmethods can predict a user?s occupationalclass with strong accuracy for the coars-est level of a standard occupation taxon-omy which includes nine classes.
Com-bined with a qualitative assessment, thederived results confirm the feasibility ofour approach in inferring a new user at-tribute that can be embedded in a multitudeof downstream applications.1 IntroductionThe growth of online social networks provides theopportunity to analyse user text in a broader context(Tumasjan et al, 2010; Bollen et al, 2011; Lam-pos and Cristianini, 2012).
This includes the socialnetwork (Sadilek et al, 2012), spatio-temporal in-formation (Lampos and Cristianini, 2010) and per-sonal attributes (Al Zamal et al, 2012).
Previousresearch has analysed language differences in userattributes like location (Cheng et al, 2010), gender(Burger et al, 2011), impact (Lampos et al, 2014)and age (Rao et al, 2010), showing that languageuse is influenced by them.
Therefore, user text al-lows us to infer these properties.
This user profilingis important not only for sociolinguistic studies, butalso for other applications: recommender systemsto provide targeted advertising, analysts who studydifferent opinions in each social class or integra-tion in text regression tasks such as voting intention(Lampos et al, 2013).Social status reflected through a person?s occu-pation is a factor which influences language use(Bernstein, 1960; Bernstein, 2003; Labov, 2006).Therefore, our hypothesis is that language use insocial media can be indicative of a user?s occu-pational class.
For example, executives may writemore frequently about business or financial news,while people in manufacturing positions could re-fer more to their personal interests and less to jobrelated activities.
Similarly, we expect some cate-gories of people, like those working in sales andcustomer services, to be more social or to use moreinformal language.Focusing on the microblogging platform of Twit-ter, we explore our hypothesis by studying thetask of predicting a user?s occupational class givenplatform-related attributes and generated content,i.e.
tweets.
That has direct applicability in a broadrange of areas from sociological studies, whichanalyse the behaviour of different occupations, torecruiting companies that target people for new jobopportunities.
For this study, we created a publiclyavailable data set of users, including their profileinformation and historical text content as well asa label to an occupational class from the ?Stan-dard Occupational Classification?
taxonomy (seeSection 2).We frame our task as classification, aiming toidentify the most likely job class for a given userbased on profile and a variety of textual features:general word embeddings and clusters (or ?topics?
).Both linear and non-linear classification methodsare applied with a focus on those that can assist in-terpretation and offer qualitative insights.
We findthat text features, especially word clusters, leadto good predictive performance.
Accuracy for ourbest model is well above 50% for 9-way classifi-1754cation, outperforming competitive methods.
Thebest results are obtained using the Bayesian non-parametric framework of Gaussian Processes (Ras-mussen and Williams, 2006), which also accom-modates feature interpretation via the AutomaticRelevance Determination.
This allows us to get in-sight into differences in language use across jobclasses and, finally, assess our original hypothesisabout the thematic divergence across them.2 Standard Occupational ClassificationTo enable the user occupation study, we adopt astandardised job classification taxonomy for map-ping Twitter users to occupations.
The Standard Oc-cupational Classification (SOC)1is a UK govern-ment system developed by the Office of NationalStatistics for classifying occupations.
Jobs are cate-gorised hierarchically based on skill requirementsand content.
The SOC scheme includes nine majorgroups coded with a digit from 1 to 9.
Each ma-jor group is divided into sub-major groups codedwith 2 digits, where the first digit indicates the ma-jor group.
Each sub-major group is further dividedinto minor groups coded with 3 digits and finally,minor groups are divided into unit groups, codedwith 4 digits.
The unit groups are the leaves of thehierarchy and represent specific jobs related to thegroup.Table 1 shows a part of the SOC hierarchy.
In to-tal, there are 9 major groups, 25 sub-major groups,90 minor groups and 369 unit groups.
Althoughother hierarchies exist, we use the SOC becauseit has been published recently (in 2010), includesnewly introduced jobs, has a balanced hierarchyand offers a wide variety of job titles that werecrucial in our data set creation.3 DataTo the best of our knowledge there are no pub-licly available data sets suitable for the task weaim to investigate.
Thus, we have created a newone consisting of Twitter users mapped to their oc-cupation, together with their profile informationand historical tweets.
We use the account?s profileinformation to capture users with self-disclosedoccupations.
The potential self-selection bias is ac-knowledged, but filtering content via self disclosure1http://www.ons.gov.uk/ons/guide-method/classifications/current-standard-classifications/soc2010/index.html; accessed on 24/02/2015.Major Group 1 (C1): Managers, Directors and Senior OfficialsSub-major Group 11: Corporate Managers and DirectorsMinor Group 111: Chief Executives and Senior OfficialsUnit Group 1115: Chief Executives and Senior Officials?Job: chief executive, bank managerUnit Group 1116: Elected Officers and RepresentativesMinor Group 112: Production Managers and DirectorsMinor Group 113: Functional Managers and DirectorsMinor Group 115: Financial Institution Managers and DirectorsMinor Group 116: Managers and Directors in Transport and LogisticsMinor Group 117: Senior Officers in Protective ServicesMinor Group 118: Health and Social Services Managers and DirectorsMinor Group 119: Managers and Directors in Retail and WholesaleSub-major Group 12: Other Managers and ProprietorsMajor Group (C2): Professional Occupations?Job: mechanical engineer, pediatristMajor Group (C3): Associate Professional and Technical Occupations?Job: system administrator, dispensing opticianMajor Group (C4): Administrative and Secretarial Occupations?Job: legal clerk, company secretaryMajor Group (C5): Skilled Trades Occupations?Job: electrical fitter, tailorMajor Group (C6): Caring, Leisure and Other Service Occupations?Job: nursery assistant, hairdresserMajor Group (C7): Sales and Customer Service Occupations?Job: sales assistant, telephonistMajor Group (C8): Process, Plant and Machine Operatives?Job: factory worker, van driverMajor Group (C9): Elementary Occupations?Job: shelf stacker, bartenderTable 1: Subset of the SOC classification hierarchy.is widespread when extracting large-scale data foruser attribute inference (Pennacchiotti and Popescu,2011; Coppersmith et al, 2014).Similarly to Hecht et al (2011), we first assessthe proportion of Twitter accounts with a clear men-tion to their occupation by annotating the user de-scription field of a random set of 500 users.
Therewere chosen from the random 1% sample, having atleast 200 tweets in their history and with a majorityof English tweets.
There, we can identify the fol-lowing categories: no description (12.2%), randominformation (22%), user information but not occu-pation related (45.8%), and job related information(20%).To create our data set, we thus use the user de-scription field to search for self-disclosed job titlesprovided by the 4-digit SOC unit groups, sincethey contain specific job titles.
We queried Twit-ter?s Search API to retrieve for each job title a max-imum of 200 accounts which best matched occupa-tion keywords.
Then, we aggregated the accountsinto the 3-digit (minor) categories.
To remove po-tential ambiguity in the retrieved set, we manuallyinspected accounts in each minor category and fil-tered out those that belong to companies, containno description or the description provided does notindicate that the user has a job corresponding tothe minor category.
In total, around 50% of theaccounts were removed by manual inspection per-1755formed by the authors.
We also removed users inmultiple categories and or users that have tweetedless than 50 times in their history.
Finally, we elim-inated all 3-digit categories that contained less than45 user accounts after this filtering.
This processproduced a total number of 5,191 users from 55 mi-nor groups (22 sub-major groups), spread across allnine major SOC groups.
The distribution of usersacross these nine groups is: 9.7%, 34.5%, 20.6%,3.8%, 16.7%, 6.1%, 1.4%, 4.2%, and 3% (follow-ing the ordering of Table 1).
In our data set themost well represented minor occupational groupsare ?Functional Managers and Directors?
(184 users?
code 113), ?Therapy Professionals?
(159 users ?code 222) and ?Quality and Regulatory Profession-als?
(158 users ?
code 246), whereas the least rep-resented ones are ?Textile and Garment Trades?
(45users ?
code 541), ?Elementary Security Occupa-tions?
(46 users ?
code 924), ?Elementary CleaningOccupations?
(47 users ?
code 923).
The mean num-ber of users in the minor classes is equal to 94.4with a standard deviation of 35.6.
For these users,we have collected all their tweets, going as far backas the latest 3,200, and their profile information.The final data set consists of 10,796,836 tweets col-lected around 5 August 2014 and is openly avail-able.2A separate Twitter data set is used as a referencecorpus in order to build the feature representationsdetailed in Section 4.
This data set is an extractfrom the Twitter Gardenhose stream (a 10% repre-sentative sample of the entire Twitter stream) from2 January to 28 February 2011.
Based on this con-tent, we also build the vocabulary for the text fea-tures, containing the most frequent 71,555 words.We tokenise and filter for English using the Trend-miner preprocessing pipeline (Preot?iuc-Pietro et al,2012).4 FeaturesIn this section, we overview the features used inthe occupational class prediction task.
They aredivided into two types: (1) user level features, (2)textual features.4.1 User Level Features (UserLevel)The user level features are based on the generaluser information or aggregated statistics about thetweets.
Table 2 introduces the 18 features in this2http://www.sas.upenn.edu/?danielpr/jobs.tar.gzu1number of followersu2number of friendsu3number of times listedu4follower/friend ratiou5proportion of non-duplicate tweetsu6proportion of retweeted tweetsu7average no.
of retweets/tweetu8proportion of retweets doneu9proportion of hashtagsu10proportion of tweets with hashtagsu11proportion of tweets with @-mentionsu12proportion of @-repliesu13no.
of unique @-mentions in tweetsu14proportion of tweets with linksu15no.
of favourites the account madeu16avg.
number of tweets/dayu17total number of tweetsu18proportion of tweets in EnglishTable 2: User level attributes for a Twitter user.category.4.2 Textual FeaturesThe textual features are derived from the aggre-gated set of user?s tweets.
We use our referencecorpus to represent each user as a distribution overthese features.
We ignore the bio field from build-ing textual features to avoid introducing biasesfrom our data collection method.
While this is a re-striction, our analysis showed that in less than 20%of the cases the information in the bio is directlyrelevant to the occupation.4.2.1 SVD Word Embeddings (SVD-E)We use a more abstract representation of wordsthan simple unigram counts in order to aid inter-pretability of our analysis.
We compute a wordto word similarity matrix from our reference cor-pus.
Normalised Pointwise Mutual Information(NPMI) (Bouma, 2009) is used to compute word toword similarity.
NPMI is an information theoreticmeasure indicating which words co-occur in thesame context, where the context is represented bya whole tweet:NPMI(x, y) = ?
log P(x, y) ?
logP(x, y)P(x) ?
P(y).
(1)We then perform singular value decomposition(SVD) on the word to word similarity matrix andobtain an embedding of words into a low dimen-sional space.
In our experiments we tried the fol-lowing dimensionalities: 30, 50, 100 and 200.
Thefeature representation for each user is obtainedsumming over each of the embedding dimensionsacross all words.17564.2.2 NPMI Clusters (SVD-C)We use the NPMI matrix described in the previousparagraph to create hard clusters of words.
Theseclusters can be thought as ?topics?, i.e.
words thatare semantically similar.
From a variety of cluster-ing techniques we choose spectral clustering (Shiand Malik, 2000; Ng et al, 2002), a hard-clusteringapproach which deals well with high-dimensionaland non-convex data (von Luxburg, 2007).
Spectralclustering is based on applying SVD to the graphLaplacian and aims to perform an optimal graphpartitioning on the NPMI similarity matrix.
Thenumber of clusters needs to be pre-specified.
Weuse 30, 50, 100 and 200 clusters ?
numbers werechosen a priori based on previous work (Lamposet al, 2014).
The feature representation is the stan-dardised number of words from each cluster.Although there is a loss of information comparedto the original representation, the clusters are veryuseful in the model analysis step.
Embeddings arehard to interpret because each dimension is an ab-stract notion, while the clusters can be interpretedby presenting a list of the most frequent or repre-sentative words.
The latter are identified using thefollowing centrality metric:Cw=?x?cNPMI(w, x)|c| ?
1, (2)where c denotes the cluster and w the target word.4.2.3 Neural Embeddings (W2V-E)Recently, there has been a growing interest in neu-ral language models, where the words are projectedinto a lower dimensional dense vector space via ahidden layer (Mikolov et al, 2013b).
These modelsshowed they can provide a better representationof words compared to traditional language models(Mikolov et al, 2013c) because they capture syntac-tic information rather than just bag-of-context, han-dling non-linear transformations.
In this low dimen-sional vector space, words with a small distance areconsidered semantically similar.
We use the skip-gram model with negative sampling (Mikolov et al,2013a) to learn word embeddings on the Twitterreference corpus.
In that case, the skip-gram modelis factorising a word-context PMI matrix (Levy andGoldberg, 2014).
We use a layer size of 50 and theGensim implementation.33http://radimrehurek.com/gensim/models/word2vec.html4.2.4 Neural Clusters (W2V-C)Similar to the NPMI cluster, we use the neuralembeddings in order to obtain clusters of relatedwords, i.e.
?topics?.
We derive a word to word simi-larity matrix using cosine similarity on the neuralembeddings.
We apply spectral clustering on thismatrix to obtain 30, 50, 100 and 200 word clusters.5 Classification with Gaussian ProcessesIn this section, we briefly overview Gaussian Pro-cess (GP) for classification, highlighting our mo-tivation for using this method.
GPs formulate aBayesian non-parametric machine learning frame-work which defines a prior on functions (Ras-mussen and Williams, 2006).
The properties ofthe functions are given by a kernel which modelsthe covariance in the response values as a functionof its inputs.
Although GPs form a powerful learn-ing tool, they have only recently been used in NLPresearch (Cohn and Specia, 2013; Preot?iuc-Pietroand Cohn, 2013) with classification applicationslimited to (Polajnar et al, 2011).Formally, GP methods aim to learn a functionf : Rd?
R drawn from a GP prior given theinputs x ?
Rd:f(x) ?
GP(m(x), k(x,x?))
, (3)wherem(?)
is the mean function (here 0) and k(?, ?
)is the covariance kernel.
Usually, the Squared Ex-ponential (SE) kernel (a.k.a.
RBF or Gaussian) isused to encourage smooth functions.
For the multi-dimensional pair of inputs (x,x?
), this is:kard(x,x?)
= ?2exp[d?i?(xi?
x?i)22l2i], (4)where liare lengthscale parameters learnt onlyusing training data by performing gradient as-cent on the type-II marginal likelihood.
Intuitively,the lengthscale parameter licontrols the variationalong the i input dimension, i.e.
a low value makesthe output very sensitive to input data, thus mak-ing that input more useful for the prediction.
If thelengthscales are learnt separately for each inputdimension the kernel is named SE with AutomaticRelevance Determination (ARD) (Neal, 1996).Binary classification using GPs ?squashes?
thereal valued latent function f(x) output through alogistic function: pi(x) , P(y = 1|x) = ?
(f(x))in a similar way to logistic regression classification.The object of the GP inference is the distribution1757of the latent variable corresponding to a test casex?
:P(f?|x,y, x?)
=?P(f?|x, x?, f)P(f |x,y)df ,(5)where P(f |x,y) = P(y|f)P(f |x)/P(y|x) is theposterior over the latent variables.
If the likelihoodP(y|f) is Gaussian, the combination with a GPprior P(f |x) gives a posterior GP over functions.In binary classification, the distribution over thelatent f?is combined with the logistic function toproduce the prediction:p?i?=??(f?
)P(f?|x,y, x?)df?.
(6)This results in a non-Gaussian likelihood in theposterior formulation and therefore, exact infer-ence is infeasible for classification models.
Multi-ple approximations exist that make the computa-tion tractable (Gibbs and Mackay, 1997; Williamsand Barber, 1998; Neal, 1999).
In our experimentswe opt to use the Expectation Propagation (EP)method (Minka, 2001) which approximates the non-Gaussian joint posterior with a Gaussian one.
EPoffers very good empirical results for many differ-ent likelihoods, although it has no proof of con-vergence.
The complexity for the inference step isO(n3).
Given that our data set is very large and thenumber of features is high, we conduct inferenceusing the fully independent training conditional(FITC) approximation (Snelson and Ghahramani,2006) with 500 random inducing points.
We referthe interested reader to Rasmussen and Williams(2006) for further information on GP classification.Although we could use multi-class classificationmethods, in order to provide insight, we perform aseparate one-vs-all classification for each class andthen determine a label through the occupationalclass that has the highest likelihood.6 ExperimentsThis section presents the experimental results forour task.
We first compare the accuracy of our clas-sification methods on held out data using each fea-ture set and conduct a standard error analysis.
Wethen use the interpretability of the ARD length-scales from the GP classifier to further analyse therelevant features.6.1 Predictive AccuracyWe assign users to one of nine possible classes (seethe ?Major Groups?
on Table 1) using one set ofFeature LR SVM GPMost frequent class 34.4% 34.4% 34.4%UserLevel 34.0% 31.5% 34.2%SVD-E-30 36.3% 35.0% 39.8%SVD-E-50 36.7% 36.9% 38.6%SVD-E-100 40.8% 41.9% 40.9%SVD-E-200 40.0% 43.1% 43.8%SVD-C-30 36.9% 36.5% 38.2%SVD-C-50 37.7% 38.3% 40.5%SVD-C-100 40.4% 42.1% 44.6%SVD-C-200 44.2% 47.9% 48.2%W2V-E-50 42.5% 49.0% 48.4%W2V-C-30 40.0% 46.0% 47.1%W2V-C-50 42.3% 48.5% 47.9%W2V-C-100 44.4% 48.7% 51.3%W2V-C-200 46.9% 51.7% 52.7%Table 3: 9-way classification accuracy on held-outdata for our 3 methods.
Textual features are ob-tained using SVD or Word2Vec (W2V).
E repre-sents embeddings, C clusters.
The final numberdenotes the amount of clusters or the size of theembedding.features at a time.
Experiments combining featuresyielded only minor improvements.
We apply com-mon linear and non-linear methods together withour proposed GP classifier.
The linear method islogistic regression (LR) with Elastic Net regulari-sation (Freedman, 2009) and the non-linear one isformulated by a Support Vector Machine (SVM)with an RBF kernel (Vapnik, 1998).
The accuracyof our classifiers is measured on held-out data.
Ourdata set is divided into stratified training (80%),validation (10%) and testing (10%) sets.
The val-idation set was used to learn the LR and SVMhyperparameters, while the GP did not use this setat all.
We report results using all three methods andall feature sets in Table 3.We first observe that user level features (User-Level; see Section 4.1) are not useful for predictingthe job class.
This finding indicates that general so-cial behaviour or user impact are likely to be spreadevenly across classes.
It also highlights the diffi-culty of the task and motivates the use of deepertextual features.The textual features (see Section 4.2) improveperformance as compared to the most frequent classbaseline.
We also notice that the embeddings (SVD-E and W2V-E) have lower performance than theclusters (SVD-C and W2V-C) in most of the cases.This is expected, as adding word vectors to rep-resent a user?s text may overemphasise commonwords.
The size of the embedding also increasesperformance.
The W2V features show better ac-1758Rank Topic # Label Topic (most central words; most frequent words) MRR ?
(l)1 116 Artsarchival, stencil, canvas, minimalist, illustration, paintings, abstract, designs,lettering, steampunk; art, design, print, collection, poster, painting, custom, logo,printing, drawing.43 1.352 105 Healthchemotherapy, diagnosis, disease, inflammation, diseases, arthritis, symptoms,patients, mrsa, colitis; risk, cancer, mental, stress, patients, treatment, surgery,disease, drugs, doctor.20 2.763 153 Beauty Careexfoliating, cleanser, hydrating, moisturizer, moisturiser, shampoo, lotions,serum, moisture, clarins; beauty, natural, dry, skin, massage, plastic, spray,facial, treatments, soap.19 3.694 21HigherEducationundergraduate, doctoral, academic, students, curriculum, postgraduate, enrolled,master?s, admissions, literacy; students, research, board, student, college,education, library, schools, teaching, teachers.18 3.215 158SoftwareEngineeringintegrated, data, implementation, integration, enterprise, configuration,open-source, cisco, proprietary, avaya; service, data, system, services, access,security, development, software, testing, standard.17 3.107 186 Footballbardsley, etherington, gallas, heitinga, assou-ekotto, lescott, pienaar, warnock,ridgewell, jenas; van, foster, cole, winger, terry, reckons, youngster, rooney,fielding, kenny.16 3.118 124 Corporateconsortium, institutional, firm?s, acquisition, enterprises, subsidiary, corp,telecommunications, infrastructure, partnership; patent, industry, reports, global,survey, leading, firm, 2015, innovation, financial.15 2.449 96 Cookingparmesan, curried, marinated, zucchini, roasted, coleslaw, salad, tomato, spinach,lentils; recipe, meat, salad, egg, soup, sauce, beef, served, pork, rice.15 3.0012 164ElongatedWordsyaaayy, wooooo, woooo, yayyyyy, yaaaaay, yayayaya, yayy, yaaaaaaay,wooohooo, yaayyy; wait, till, til, yay, ahhh, hoo, woo, woot, whoop, woohoo.11 3.4716 176 Politicsreligious, colonialism, christianity, judaism, persecution, fascism, marxism,nationalism, communism, apartheid; human, culture, justice, religion, democracy,religious, humanity, tradition, ancient, racism.08 3.09Table 4: Topics, represented by their most central and most frequent 10 words, sorted by their ARDlengthscale MRR across the nine GP-based occupation classifiers.
?
(l) denotes the average lengthscalefor a topic across these classifiers.
Topic labels are manually created.1 2 3 4 5 6 7 8 9123456789 0.00.20.40.6Figure 1: Confusion matrix of the prediction results.Rows represent the actual occupational class (C 1?9) and columns the predicted class.curacy than the SVD on the NPMI matrix.
Thisis consistent with previous work that showed theefficiency of word2vec and the ability of those em-beddings to capture non-linear relationships andsyntactic features (Mikolov et al, 2013a; Mikolovet al, 2013b; Mikolov et al, 2013c).LR has a lower performance than the non-linearmethods, especially when using clusters as features.GPs usually outperform SVMs by a small margin.However, these offer the advantages of not usingthe validation set and the interpretability propertieswe highlight in the next section.
Although we onlydraw our focus on major occupational classes, thedata set alows the study of finer granularities of oc-cupation classes in future work.
For example, pre-diction performance for sub-major groups reaches33.9% accuracy (15.6% majority class, 22 classes)and 29.2% accuracy for minor groups (3.4% major-ity class, 55 classes).6.2 Error AnalysisTo illustrate the errors made by our classifiers, Fig-ure 1 shows the confusion matrix of the classi-fication results.
First, we observe that class 4 ismany times classified as class 2 or 3.
This can beexplained by the fact that classes 2, 3 and 4 con-tain similar types of occupations, e.g.
doctors andnurses or accountants and assistant accountants.However, with very few exceptions, we notice thatonly adjacent classes get misclassified, suggesting1759that our model captures the general user skill level.6.3 Qualitative AnalysisThe word clusters that were built from a referencecorpus and then used as features in the GP classi-fier, give us the opportunity to extract some qual-itative derivations from our predictive task.
Forthe rest of the section we use the best performingmodel of this type (W2V-C-200) in order to anal-yse the results.
Our main assumption is that theremight be a divergence of language and topic us-age across occupational classes following previousstudies in sociology (Bernstein, 1960; Bernstein,2003).
Knowing that the inferred GP lengthscalehyperparameters are inversely proportional to fea-ture (i.e.
topic) relevance (see Section 5), we canuse them to rank the topic importance and giveanswers to our hypothesis.Table 4 shows 10 of the most informative top-ics (represented by the top 10 most central andfrequent words) sorted by their ARD lengthscaleMean Reciprocal Rank (MRR) (Manning et al,2008) across the nine classifiers.
Evidently, theycover a broad range of thematic subjects, includ-ing potentially work specific topics in different do-mains such as ?Corporate?
(Topic #124), ?SoftwareEngineering?
(#158), ?Health?
(#105), ?Higher Ed-ucation?
(#21) and ?Arts?
(#116), as well as topicscovering recreational interests such as ?Football?
(#186), ?Cooking?
(#96) and ?Beauty Care?
(#153).The highest ranked MRR GP lengthscales onlyhighlight the topics that are the most discrimina-tive of the particular learning task, i.e.
which topicused alone would have had the best performance.To examine the difference in topic usage acrossoccupations, we illustrate how six topics are cov-ered by the users of each class.
Figure 2 shows theCumulative Distribution Functions (CDFs) acrossthe nine different occupational classes for these sixtopics.
CDFs indicate the fraction of users havingat least a certain topic proportion in their tweets.
Atopic is more prevalent in a class, if the CDF lineleans towards the bottom-right corner of the plot.
?Higher Education?
(#21) is more prevalent inclasses 1 and 2, but is also discriminative for classes3 and 4 compared to the rest.
This is expected be-cause the vast majority of jobs in these classesrequire a university degree (holds for all of the jobsin classes 2 and 3) or are actually jobs in highereducation.
On the other hand, classes 5 to 9 have asimilar behaviour, tweeting less on this topic.
Wealso observe that words in ?Corporate?
(#124) areused more as the skill required for a job gets higher.This topic is mainly used by people in classes 1and 2 and with less extent in classes 3 and 4, in-dicating that people in these occupational classesare more likely to use social media for discussionsabout corporate business.There is a clear trend of people with more skilledjobs to talk about ?Politics?
(#176).
Indeed, highlyranked politicians and political philosophers areparts of classes 1 and 2 respectively.
Neverthe-less, this pattern expands to the entire spectrumof the investigated occupational classes, providingfurther proof-of-concept for our methodology, un-der the assumption that the theme of politics ismore attractive to the higher skilled classes ratherthan the lower skilled occupations.
By examining?Arts?
(#116), we see that it clearly separates class5, which includes artists, from all others.
This topicappears to be relevant to most of the classifica-tion tasks and it is ranked first according to theMRR metric.
Moreover, we observe that peoplewith higher skilled jobs and education (classes 1?3)post more content about arts.
Finally, we examinetwo topics containing words that can be used inmore informal occasions, i.e.
?Elongated Words?
(#164) and ?Beauty Care?
(#153).
We observe asimilar pattern in both topics by which users withlower skilled jobs tweet more often.1 2 3 4 5 6 7 8 9123456789 0.000.010.020.03Figure 3: Jensen-Shannon divergence in the topicdistributions between the different occupationalclasses (C 1?9).The main conclusion we draw from Figure 2 isthat there exists a topic divergence between users inthe lower vs. higher skilled occupational classes.
Toexamine this distinction better, we use the Jensen-Shannon divergence (JSD) to quantify the differ-ence between the topic distributions across every17600.001 0.01 0.0500.20.40.60.81User probabilityHigher Education (#21)C1C2C3C4C5C6C7C8C9 0.001 0.01 0.0500.20.40.60.81 Corporate (#124)C1C2C3C4C5C6C7C8C90.001 0.01 0.0500.20.40.60.81User probabilityPolitics (#176)C1C2C3C4C5C6C7C8C9 0.001 0.01 0.0500.20.40.60.81 Arts (#116)C1C2C3C4C5C6C7C8C90.001 0.01 0.0500.20.40.60.81Topic proportionUser probabilityBeauty Care (#153)C1C2C3C4C5C6C7C8C9 0.001 0.01 0.0500.20.40.60.81Topic proportionElongated Words (#164)C1C2C3C4C5C6C7C8C9Figure 2: CDFs for six of the most important topics; the x-axis is on the log-scale for display purposes.
Apoint on a CDF line indicates the fraction of users (y-axis point) with a topic proportion in their tweetslower or equal to the corresponding x-axis point.
The topic is more prevalent in a class, if the CDF lineleans closer to the bottom-right corner of the plot.class pair.
Figure 3 visualises these differences.There, we confirm that adjacent classes use simi-lar topics of discussion.
We also notice that JSDincreases as the classes are further apart.
Two maingroups of related classes, with a clear separationfrom the rest, are identified: classes 1?2 and 6?9.For the users belonging to these two groups, wecompute their topic usage distribution (for the toptopics listed in Table 4).
Then, we assess whetherthe topic usage distributions of those super-classesof occupations have a statistically significant dif-ference by performing a two-sample Kolmogorov-Smirnov test.
We enumerate the group topic usagemeans in Table 5; all differences were indeed sta-tistically significant (p < 10?5).
From this compar-ison, we conclude that users in the higher skilledclasses have a higher representation in all top topicsbut ?Beauty Care?
and ?Elongated Words?.
Hence,the original hypothesis about the difference in theusage of language between upper and lower occu-pational classes is reconfirmed in this more generictesting.
A very noticeable difference occurs for the1761Topics C 1?2 C 6?9Arts 4.95 2.79Health 4.45 2.13Beauty Care 1.40 2.24Higher Education 6.04 2.56Software Engineering 6.31 2.54Football 0.54 0.52Corporate 5.15 1.41Cooking 2.81 2.49Elongated Words 1.90 3.78Politics 2.14 1.06Table 5: Comparison of mean topic usage forsuper-sets (classes 1?2 vs. 6?9) of the occupationalclasses; all values were multiplied by 103.
The dif-ference between the topic usage distributions wasstatistically significant (p < 10?5).?Corporate?
topic, whereas ?Football?
registers thelowest distance.7 Related WorkOccupational class prediction has been studied inthe past in the areas of psychology and economics.French (1959) investigated the relation between var-ious measures on 232 undergraduate students andtheir future occupations.
This study concluded thatoccupational membership can be predicted fromvariables such as the ability of subjects in usingmathematical and verbal symbols, their family eco-nomic status, body-build and personality compo-nents.
Schmidt and Strauss (1975) also studied therelationship between job types (five classes) andcertain demographic attributes (gender, race, expe-rience, education, location).
Their analysis identi-fied biases or discrimination which possibly existin different types of jobs.
Sociolinguistic and so-ciology studies deduct that social status is an im-portant factor in determining the use of language(Bernstein, 1960; Bernstein, 2003; Labov, 2006).Differences arise either due to language use or dueto the topics people discuss as parts of various so-cial domains.
However, a large scale investigationof this hypothesis has never been attempted.Relevant to our task is a relation extraction ap-proach proposed by Li et al (2014) aiming to ex-tract user profile information on Twitter.
They useda weakly supervised approach to obtain informa-tion for job, education and spouse.
Nonetheless,the information relevant to the job attribute re-gards the employer of a user (i.e.
the name of acompany) rather than the type of occupation.
Inaddition, Huang et al (2014) proposed a methodto classify Sina Weibo users to twelve predefinedoccupations using content based and network fea-tures.
However, there exist significant differencesfrom our task since this inference is based on a dis-tinct platform, with an ambiguous distribution overoccupations (e.g.
more than 25% related to me-dia), while the occupational classes are not generic(e.g.
media, welfare and electronic are three of thetwelve categories).
Most importantly, the appliedmodel did not allow for a qualitative interpreta-tion.
Filho et al (2014) inferred the social class ofsocial media users by combining geolocation infor-mation derived from Foursquare and Twitter posts.Recently, Sloan et al (2015) introduced tools forthe automated extraction of demographic data (age,occupation and social class) from the profile de-scriptions of Twitter users using a similar methodto our data set extraction approach.
They showedthat it is feasible to build a data set that matchesthe real-world UK occupation distribution as givenby the SOC.8 ConclusionsOur paper presents the first large-scale systematicstudy on language use on social media as a factorfor inferring a user?s occupational class.
To addressthis problem, we have also introduced an exten-sive labelled data set extracted from Twitter.
Wehave framed prediction as a classification task and,to this end, we used the powerful, non-linear GPframework that combines strong predictive perfor-mance with feature interpretability.
Results showthat we can achieve a good predictive accuracy,highlighting that the occupation of a user influencestext use.
Through a qualitative analysis, we haveshown that the derived topics capture both occupa-tion specific interests as well as general class-basedbehaviours.
We acknowledge that the derivationsof this study, similarly to other studies in the field,are reflecting the Twitter population and may expe-rience a bias introduced by users self-mentioningtheir occupations.
However, the magnitude, occupa-tional diversity and face validity of our conclusionssuggest that the presented approach is useful forfuture downstream applications.1762AcknowledgementsDP-P acknowledges the support from TempletonReligion Trust, grant TRT-0048.
VL and NA ac-knowledge the support from EPSRC (UK) projectEP/K031953/1.
We thank Mark Stevenson for hiscritical comments on early drafts of this paper.ReferencesFaiyaz Al Zamal, Wendy Liu, and Derek Ruths.
2012.Homophily and Latent Attribute Inference: InferringLatent Attributes of Twitter Users from Neighbors.In Proc.
of 6th International Conference on Weblogsand Social Media, pages 387?390.Basil Bernstein.
1960.
Language and social class.British Journal of Sociology, pages 271?276.Basil Bernstein.
2003.
Class, codes and control: Ap-plied studies towards a sociology of language, vol-ume 2.
Psychology Press.Johan Bollen, Huina Mao, and Xiaojun Zeng.
2011.Twitter mood predicts the stock market.
Journal ofComputational Science, 2(1):1?8.Gerlof Bouma.
2009.
Normalized (pointwise) mutualinformation in collocation extraction.
In BiennialGSCL Conference, pages 31?40.D.
John Burger, John Henderson, George Kim, andGuido Zarrella.
2011.
Discriminating Gender onTwitter.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Processing,EMNLP, pages 1301?1309.Zhiyuan Cheng, James Caverlee, and Kyumin Lee.2010.
You are where you tweet: a content-based ap-proach to geo-locating twitter users.
In Proceedingsof the 19th ACM Conference on Information andKnowledge Management, CIKM, pages 759?768.Trevor Cohn and Lucia Specia.
2013.
Modelling an-notator bias with multi-task gaussian processes: Anapplication to machine translation quality estimation.In 51st Annual Meeting of the Association for Com-putational Linguistics, ACL, pages 32?42.Glen Coppersmith, Craig Harman, and Mark Dredze.2014.
Measuring post traumatic stress disorder intwitter.
In International Conference on Weblogs andSocial Media, ICWSM.Renato Miranda Filho, Guilherme R. Borges, Jus-sara M. Almeida, and Gisele L. Pappa.
2014.
Infer-ring user social class in online social networks.
InProceedings of the 8th Workshop on Social NetworkMining and Analysis, SNAKDD?14, pages 10:1?10:5.David Freedman.
2009.
Statistical models: theory andpractice.
Cambridge University Press.Wendell L French.
1959.
Can a man?s occupationbe predicted?
Journal of Counseling Psychology,6(2):95.Mark Gibbs and David J. C. Mackay.
1997.
Variationalgaussian process classifiers.
IEEE Transactions onNeural Networks, 11:1458?1464.Brent Hecht, Lichan Hong, Bongwon Suh, and Ed H.Chi.
2011.
Tweets from justin bieber?s heart: Thedynamics of the location field in user profiles.
InProceedings of the SIGCHI Conference on HumanFactors in Computing Systems, CHI.Yanxiang Huang, Lele Yu, Xiang Wang, and Bin Cui.2014.
A multi-source integration framework foruser occupation inference in social media systems.World Wide Web, pages 1?21.William Labov.
2006.
The Social Stratification of En-glish in New York City.
Cambridge University Press,second edition.Vasileios Lampos and Nello Cristianini.
2010.
Track-ing the flu pandemic by monitoring the Social Web.In Proc.
of the 2nd International Workshop on Cog-nitive Information Processing, pages 411?416.Vasileios Lampos and Nello Cristianini.
2012.
Now-casting Events from the Social Web with StatisticalLearning.
ACM Transactions on Intelligent Systemsand Technology, 3(4):72:1?72:22.Vasileios Lampos, Daniel Preot?iuc-Pietro, and TrevorCohn.
2013.
A user-centric model of voting in-tention from Social Media.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics, ACL, pages 993?1003.Vasileios Lampos, Nikolaos Aletras, Daniel Preot?iuc-Pietro, and Trevor Cohn.
2014.
Predicting and char-acterising user impact on Twitter.
In Proceedings ofthe 14th Conference of the European Chapter of theAssociation for Computational Linguistics, EACL,pages 405?413.Omer Levy and Yoav Goldberg.
2014.
Neural wordembeddings as implicit matrix factorization.
In Ad-vances in Neural Information Processing Systems,NIPS, pages 2177?2185.Jiwei Li, Alan Ritter, and Eduard H. Hovy.
2014.Weakly supervised user profile extraction from twit-ter.
In Proceedings of the 52nd Annual Meeting ofthe Association for Computational Linguistics, ACL,pages 165?174.Christopher D. Manning, Prabhakar Raghavan, andHinrich Sch?utze.
2008.
Introduction to InformationRetrieval.
Cambridge University Press.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word represen-tations in vector space.
In Proceedings of Workshopat the International Conference on Learning Repre-sentations, ICLR.1763Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeffrey Dean.
2013b.
Distributed represen-tations of words and phrases and their composition-ality.
In Advances in Neural Information ProcessingSystems, NIPS, pages 3111?3119.Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.2013c.
Linguistic regularities in continuous spaceword representations.
In Proceedings of the 2010annual Conference of the North American Chap-ter of the Association for Computational Linguistics,NAACL, pages 746?751.Thomas P. Minka.
2001.
Expectation propagation forapproximate bayesian inference.
In Proceedings ofthe 17th Conference in Uncertainty in Artificial In-telligence, UAI ?01.Radford M. Neal.
1996.
Bayesian Learning for NeuralNetworks.
Springer-Verlag New York, Inc.Radford M. Neal.
1999.
Regression and classificationusing gaussian process priors.
Bayesian Statistics 6,pages 475?501.Andrew Y. Ng, Michael I. Jordan, and Yair Weiss.2002.
On spectral clustering: Analysis and an algo-rithm.
In Advances in Neural Information Process-ing Systems, NIPS, pages 849?856.Marco Pennacchiotti and Ana-Maria Popescu.
2011.A machine learning approach to twitter user classifi-cation.
ICWSM, pages 281?288.Tamara Polajnar, Simon Rogers, and Mark Girolami.2011.
Protein interaction detection in sentences viagaussian processes; a preliminary evaluation.
Inter-national Journal of Data Mining and Bioinformatics,5(1):52?72.Daniel Preot?iuc-Pietro and Trevor Cohn.
2013.
A tem-poral model of text periodicities using Gaussian Pro-cesses.
EMNLP.Daniel Preot?iuc-Pietro, Sina Samangooei, Trevor Cohn,Nicholas Gibbins, and Mahesan Niranjan.
2012.Trendminer: An architecture for real time analysisof social media text.
In Workshop on Real-TimeAnalysis and Mining of Social Streams (RAMSS),ICWSM.Delip Rao, David Yarowsky, Abhishek Shreevats, andManaswi Gupta.
2010.
Classifying Latent User At-tributes in Twitter.
In Proceedings of the 2nd In-ternational Workshop on Search and Mining User-generated Contents, SMUC, pages 37?44.Carl Edward Rasmussen and Christopher K. I.Williams.
2006.
Gaussian Processes for MachineLearning.
The MIT Press.Adam Sadilek, Henry Kautz, and Vincent Silenzio.2012.
Modeling Spread of Disease from Social In-teractions.
In Proc.
of 6th International Conferenceon Weblogs and Social Media, pages 322?329.Peter Schmidt and Robert P Strauss.
1975.
The predic-tion of occupation using multiple logit models.
In-ternational Economic Review, 16(2):471?86.Jianbo Shi and Jitendra Malik.
2000.
Normalized cutsand image segmentation.
Transactions on PatternAnalysis and Machine Intelligence, 22(8):888?905.Luke Sloan, Jeffrey Morgan, Pete Burnap, and MatthewWilliams.
2015. Who tweets?
Deriving the demo-graphic characteristics of age, occupation and so-cial class from twitter user meta-data.
PloS one,10(3):e0115545.Edward Snelson and Zoubin Ghahramani.
2006.Sparse gaussian processes using pseudo-inputs.
InAdvances in Neural Information Processing Systems,NIPS, pages 1257?1264.Andranik Tumasjan, Timm Oliver Sprenger, Philipp GSandner, and Isabell M Welpe.
2010.
PredictingElections with Twitter: What 140 Characters Revealabout Political Sentiment.
In Proc.
of 4th Inter-national Conference on Weblogs and Social Media,pages 178?185.Vladimir N Vapnik.
1998.
Statistical learning theory.Wiley, New York.Ulrike von Luxburg.
2007.
A tutorial on spectral clus-tering.
Statistics and computing, 17(4):395?416.Christopher K.I Williams and David Barber.
1998.Bayesian classification with gaussian processes.IEEE Transactions on Pattern Analysis and MachineIntelligence, 20 (12):1342?1351.1764
