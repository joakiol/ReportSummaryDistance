Balancing Clarity and Efficiency in Typed Feature Logic through DelayingGerald PennUniversity of Toronto10 King?s College Rd.Toronto M5S 3G4Canadagpenn@cs.toronto.eduAbstractThe purpose of this paper is to re-examine the bal-ance between clarity and efficiency in HPSG design,with particular reference to the design decisionsmade in the English Resource Grammar (LinGO,1999, ERG).
It is argued that a simple generaliza-tion of the conventional delay statements used inlogic programming is sufficient to restore much ofthe functionality and concomitant benefit that theERG elected to forego, with an acceptable althoughstill perceptible computational cost.1 MotivationBy convention, current HPSGs consist, at the veryleast, of a deductive backbone of extended phrasestructure rules, in which each category is a descrip-tion of a typed feature structure (TFS), augmentedwith constraints that enforce the principles of gram-mar.
These principles typically take the form ofstatements, ?for all TFSs, ?
holds,?
where ?
isusually an implication.
Historically, HPSG useda much richer set of formal descriptive devices,however, mostly on analogy to developments inthe use of types and description logics in program-ming language theory (A?
?t-Kac?i, 1984), which hadserved as the impetus for HPSG?s invention (Pol-lard, 1998).
This included logic-programming-stylerelations (Ho?hfeld and Smolka, 1988), a powerfuldescription language in which expressions could de-note sets of TFSs through the use of an explicitdisjunction operator, and the full expressive powerof implications, in which antecedents of the above-mentioned ?
principles could be arbitrarily com-plex.Early HPSG-based natural language processingsystems faithfully supported large chunks of thisricher functionality, in spite of their inability to han-dle it efficiently ?
so much so that when the de-signers of the ERG set out to select formal descrip-tive devices for their implementation with the aimof ?balancing clarity and efficiency,?
(Flickinger,2000), they chose to include none of these ameni-ties.
The ERG uses only phrase-structure rules andtype-antecedent constraints, pushing all would-bedescription-level disjunctions into its type system orrules.
In one respect, this choice was successful, be-cause it did at least achieve a respectable level ofefficiency.
But the ERG?s selection of functionalityhas acquired an almost liturgical status within theHPSG community in the intervening seven years.Keeping this particular faith, moreover, comes at aconsiderable cost in clarity, as will be argued below.This paper identifies what it is precisely aboutthis extra functionality that we miss (modularity,Section 2), determines what it would take at a mini-mum computationally to get it back (delaying, Sec-tion 3), and attempts to measure exactly how muchthat minimal computational overhead would cost(about 4 ?s per delay, Section 4).
This study hasnot been undertaken before; the ERG designers?decision was based on largely anecdotal accountsof performance relative to then-current implemen-tations that had not been designed with the inten-tion of minimizing this extra cost (indeed, the ERGbaseline had not yet been devised).2 Modularity: the cost in claritySemantic types and inheritance serve to organizethe constraints and overall structure of an HPSGgrammar.
This is certainly a familiar, albeit vaguejustification from programming languages research,but the comparison between HPSG and modernprogramming languages essentially ends with thisstatement.Programming languages with inclusional poly-morphism (subtyping) invariably provide functionsor relations and allow these to be reified as meth-ods within user-defined subclasses/subtypes.
InHPSG, however, values of features must necessar-ily be TFSs themselves, and the only method (im-plicitly) provided by the type signature to act onthese values is unification.
In the absence of othermethods and in the absence of an explicit disjunc-tion operator, the type signature itself has the re-sponsibility of not only declaring definitional sub-fin-wh-fill-rel-clinf-wh-fill-rel-cl red-rel-cl simp-inf-rel-clfin-hd-fill-ph inf-hd-fill-phwh-rel-cl non-wh-rel-cl hd-fill-ph hd-comp-phinter-cl rel-cl hd-adj-ph hd-nexus-phclause non-hd-ph hd-phheaded phrasephraseFigure 1: Relative clauses in the ERG (partial).class relationships, but expressing all other non-definitional disjunctions in the grammar (as subtyp-ing relationships).
It must also encode the neces-sary accoutrements for implementing all other nec-essary means of combination as unification, such asdifference lists for appending lists, or the so-calledqeq constraints of Minimal Recursion Semantics(Copestake et al, 2003) to encode semantic embed-ding constraints.Unification, furthermore, is an inherently non-modular, global operation because it can only bedefined relative to the structure of the entire par-tial order of types (as a least upper bound).
Ofcourse, some partial orders are more modularizablethan others, but legislating the global form that typesignatures must take on is not an easy property toenforce without more local guidance.The conventional wisdom in programming lan-guages research is indeed that types are responsi-ble for mediating the communication between mod-ules.
A simple type system such as HPSG?s can thusonly mediate very simple communication.
Modernprogramming languages incorporate some degree ofparametric polymorphism, in addition to subtyping,in order to accommodate more complex communi-cation.
To date, HPSG?s use of parametric types hasbeen rather limited, although there have been somerecent attempts to apply them to the ERG (Penn andHoetmer, 2003).
Without this, one obtains type sig-natures such as Figure 1 (a portion of the ERG?s forrelative clauses), in which both the semantics of thesubtyping links themselves (normally, subset inclu-sion) and the multi-dimensionality of the empiricaldomain?s analysis erode into a collection of arbi-trary naming conventions that are difficult to vali-date or modify.A more avant-garde view of typing in program-ming languages research, inspired by the Curry-Howard isomorphism, is that types are equivalentto relations, which is to say that a relation can me-diate communication between modules through itsarguments, just as a parametric type can through itsparameters.
The fact that we witness some of thesemediators as types and others as relations is sim-ply an intensional reflection of how the grammarwriter thinks of them.
In classical HPSG, relationswere generally used as goals in some proof resolu-tion strategy (such as Prolog?s SLD resolution), buteven this has a parallel in the world of typing.
Usingthe type signature and principles of Figure 2, for ex-appendbase appendrecArg1: e list Arg1:ne listJunk:appendappendArg1: listArg2: listArg3: list?appendbase =?
Arg2 : L ?
Arg3 : L.appendrec =?
Arg1 : [H |L1] ?Arg2 : L2 ?
Arg3 : [H |L3] ?Junk : (append ?
A1 : L1 ?A2 : L2 ?
Arg3 : L3).Figure 2: Implementing SLD resolution over the ap-pend relation as sort resolution.ample, we can perform proof resolution by attempt-ing to sort resolve every TFS to a maximally spe-cific type.
This is actually consistent with HPSG?suse of feature logic, although most TFS-based NLPsystems do not sort resolve because type inferenceunder sort resolution is NP-complete (Penn, 2001).Phrase structure rules, on the other hand, whilethey can be encoded inside a logic programming re-lation, are more naturally viewed as algebraic gen-erators.
In this respect, they are more similar tothe immediate subtyping declarations that grammarwriters use to specify type signatures ?
both chartparsing and transitive closure are instances of all-source shortest-path problems on the same kind ofalgebraic structure, called a closed semi-ring.
Theonly notion of modularity ever proven to hold ofphrase structure rule systems (Wintner, 2002), fur-thermore, is an algebraic one.3 Delaying: the missing link offunctionalityIf relations are used in the absence of recursive datastructures, a grammar could be specified using rela-tions, and the relations could then be unfolded off-line into relation-free descriptions.
In this usage,relations are just macros, and not at all inefficient.Early HPSG implementations, however, used quitea lot of recursive structure where it did not need tobe, and the structures they used, such as lists, buriedimportant data deep inside substructures that madeparsing much slower.
Provided that grammar writ-ers use more parsimonious structures, which is agood idea even in the absence of relations, there isnothing wrong with the speed of logic programmingrelations (Van Roy, 1990).Recursive datatypes are also prone to non-termination problems, however.
This can happenwhen partially instantiated and potentially recur-sive data structures are submitted to a proof reso-lution procedure which explores the further instan-tiations of these structures too aggressively.
Al-though this problem has received significant atten-tion over the last fifteen years in the constraint logicprogramming (CLP) community, no true CLP im-plementation yet exists for the logic of typed fea-ture structures (Carpenter, 1992, LTFS).
Some as-pects of general solution strategies, including in-cremental entailment simplification (A?
?t-Kaci et al,1992), deterministic goal expansion (Doerre, 1993),and guard statements for relations (Doerre et al,1996) have found their way into the less restrictivesorted feature constraint systems from which LTFSdescended.
The CUF implementation (Doerre et al,1996), notably, allowed for delay statements to beattached to relation definitions, which would waituntil each argument was at least as specific as somevariable-free, disjunction-free description before re-solving.In the remainder of this section, a method ispresented for reducing delays on any inequation-free description, including variables and disjunc-tions, to the SICStus Prolog when/2 primitive(Sections 3.4).
This method takes full advan-tage of the restrictions inherent to LTFS (Sec-tion 3.1) to maximize run-time efficiency.
In ad-dition, by delaying calls to subgoals individuallyrather than the (universally quantified) relation defi-nitions themselves,1 we can also use delays to post-pone non-deterministic search on disjunctive de-scriptions (Section 3.3) and to implement complex-antecedent constraints (Section 3.2).
As a result,this single method restores all of the functionalitywe were missing.For simplicity, it will be assumed that the targetlanguage of our compiler is Prolog itself.
This is in-consequential to the general proposal, although im-plementing logic programs in Prolog certainly in-volves less effort.1Delaying relational definitions is a subcase of this func-tionality, which can be made more accessible through some ex-tra syntactic sugar.3.1 Restrictions inherent to LTFSLTFS is distinguished by its possession of appro-priateness conditions that mediate the occurrence offeatures and types in these records.
Appropriatenessconditions stipulate, for every type, a finite set offeatures that can and must have values in TFSs ofthat type.
This effectively forces TFSs to be finite-branching terms with named attributes.
Appropri-ateness conditions also specify a type to which thevalue of an appropriate feature is restricted (a valuerestriction).
These conditions make LTFS very con-venient for linguistic purposes because the combi-nation of typing with named attributes allows for avery terse description language that can easily makereference to a sparse amount of information in whatare usually extremely large structures/records:Definition: Given a finite meet semi-lattice of types,Type, a fixed finite set of features, Feat, and a count-able set of variables, Var, ?
is the least set of de-scriptions that contains:?
v, v ?
Var ,?
?, ?
?
Type ,?
F : ?, F ?
Feat , ?
?
?,?
?1 ?
?2, ?1, ?2 ?
?, and?
?1 ?
?2, ?1, ?2 ?
?.A nice property of this description language isthat every non-disjunctive description with a non-empty denotation has a unique most general TFS inits denotation.
This is called its most general satis-fier.We will assume that appropriateness guaranteesthat there is a unique most general type, Intro(F)to which a given feature, F, is appropriate.
This iscalled unique feature introduction.
Where uniquefeature introduction is not assumed, it can be addedautomatically in O(F ?T ) time, where F is the num-ber of features and T is the number of types (Penn,2001).
Meet semi-latticehood can also be restoredautomatically, although this involves adding expo-nentially many new types in the worst case.3.2 Complex Antecedent ConstraintsIt will be assumed here that all complex-antecedentconstraints are implicitly universally quantified, andare of the form:?
=?
(?
?
?
)where ?, ?
are descriptions from the core descrip-tion language, ?, and ?
is drawn from a definiteclause language of relations, whose arguments arealso descriptions from ?.
As mentioned above, theERG uses the same form, but where ?
can only be atype description, ?
, and ?
is the trivial goal, true.The approach taken here is to allow for arbitraryantecedents, ?, but still to interpret the implica-tions of principles using subsumption by ?, i.e., forevery TFS (the implicit universal quantification isstill there), either the consequent holds, or the TFSis not subsumed by the most general satisfier of?.
The subsumption convention dates back to theTDL (Krieger and Scha?fer, 1994) and ALE (Car-penter and Penn, 1996) systems, and has earlier an-tecedents in work that applied lexical rules by sub-sumption (Krieger and Nerbone, 1991).
The Con-Troll constraint solver (Goetz and Meurers, 1997)attempted to handle complex antecedents, but useda classical interpretation of implication and no de-ductive phrase-structure backbone, which created avery large search space with severe non-terminationproblems.Within CLP more broadly, there is some relatedwork on guarded constraints (Smolka, 1994) and oninferring guards automatically by residuation of im-plicational rules (Smolka, 1991), but implicit uni-versal quantification of all constraints seems to beunique to linguistics.
In most CLP, constraints on aclass of terms or objects must be explicitly posted toa store for each member of that class.
If a constraintis not posted for a particular term, then it does notapply to that term.The subsumption-based approach is sound withrespect to the classical interpretation of implicationfor those principles where the classical interpreta-tion really is the correct one.
For completeness,some additional resolution method (in the form ofa logic program with relations) must be used.
As isnormally the case in CLP, deductive search is usedalongside constraint resolution.Under such assumptions, our principles can beconverted to:trigger(?)
=?
v ?
whenfs((v = ?
), ((v = ?)??
))Thus, with an implementation of type-antecedentconstraints and an implementation of whenfs/2(Section 3.3), which delays the goal in its secondargument until v is subsumed by (one of) the mostgeneral satisfier(s) of description ?, all that remainsis a method for finding the trigger, the most effi-cient type antecedent to use, i.e., the most generalone that will not violate soundness.
trigger(?)
canbe defined as follows:?
trigger(v) = ?,?
trigger(?)
= ?
,?
trigger(F : ?)
= Intro(F),?
trigger(?1?
?2) = trigger(?1)ttrigger(?2),and?
trigger(?1?
?2) = trigger(?1)utrigger(?2),where t and u are respectively unification and gen-eralization in the type semi-lattice.In this and the next two subsections, we can useFigure 3 as a running example of the various stagesof compilation of a typical complex-antecedent con-straint, namely the Finiteness Marking Principle forGerman (1).
This constraint is stated relative to thesignature shown in Figure 4.
The description to theleft of the arrow in Figure 3 (1) selects TFSs whosesubstructure on the path SYNSEM:LOC:CAT satisfiestwo requirements: its HEAD value has type verb,and its MARKING value has type fin.
The princi-ple says that every TFS that satisfies that descrip-tion must also have a SYNSEM: LOC: CAT: HEAD:VFORM value of type bse.To find the trigger in Figure 3 (1), we can observethat the antecedent is a feature value description(F:?
), so the trigger is Intro(SYNSEM), the uniqueintroducer of the SYNSEM feature, which happensto be the type sign.
We can then transform this con-straint as above (Figure 3 (2)).
The cons and goaloperators in (2)?
(5) are ALE syntax, used respec-tively to separate the type antecedent of a constraintfrom the description component of the consequent(in this case, just the variable, X), and to separatethe description component of the consequent fromits relational attachment.
We know that any TFSsubsumed by the original antecedent will also besubsumed by the most general TFS of type sign, be-cause sign introduces SYNSEM.3.3 Reducing Complex ConditionalsLet us now implement our delay predicate,whenfs(V=Desc,Goal).
Without loss ofgenerality, it can be assumed that the first argumentis actually drawn from a more general conditionallanguage, including those of the form Vi = Desciclosed under conjunction and disjunction.
It canalso be assumed that the variables of each Desc i aredistinct.
Such a complex conditional can easily beconverted into a normal form in which each atomicconditional contains a non-disjunctive description.Conjunction and disjunction of atomic conditionalsthen reduce as follows (using the Prolog conventionof comma for AND and semi-colon for OR):whenfs((VD1,VD2),Goal) :-whenfs(VD1,whenfs(VD2,Goal)).whenfs((VD1;VD2),Goal) :-whenfs(VD1,(Trigger = 0 -> Goal; true)),whenfs(VD2,(Trigger = 1 -> Goal; true)).The binding of the variable Trigger is necessaryto ensure that Goal is only resolved once in case the(1) synsem:loc:cat:(head:verb,marking:fin) =?
synsem:loc:cat:head:vform:bse.
(2) sign cons X goalwhenfs((X=synsem:loc:cat:(head:verb,marking:fin)),(X=synsem:loc:cat:head:vform:bse)).
(3) sign cons X goalwhentype(sign,X,(farg(synsem,X,SynVal),whentype(synsem,SynVal,(farg(loc,SynVal,LocVal),whentype(local,LocVal,(farg(cat,LocVal,CatVal),whenfs((CatVal=(head:verb,marking:fin)),(X=synsem:loc:cat:head:vform:bse)))))))).
(4) sign cons X goal(whentype(sign,X,(farg(synsem,X,SynVal),whentype(synsem,SynVal,(farg(loc,SynVal,LocVal),whentype(local,LocVal,(farg(cat,LocVal,CatVal),whentype(category,CatVal,(farg(head,CatVal,HdVal),whentype(verb,HdVal,whentype(category,CatVal,(farg(marking,CatVal,MkVal),whentype(fin,MkVal,(X=synsem:loc:cat:head:vform:bse)))))))))))))).
(5) sign cons X goal(farg(synsem,X,SynVal),farg(loc,SynVal,LocVal),farg(cat,LocVal,CatVal),farg(head,CatVal,HdVal),whentype(verb,HdVal,(farg(marking,CatVal,MkVal),whentype(fin,MkVal,(X=synsem:loc:cat:head:vform:bse))))).
(6) sign(e list( ),e list( ),SynVal,DelayVar)(7) whentype(Type,FS,Goal) :-functor(FS,CurrentType,Arity),(sub type(Type,CurrentType) -> call(Goal); arg(Arity,FS,DelayVar), whentype(Type,DelayVar,Goal)).Figure 3: Reduction stages for the Finiteness Marking Principle.bse ind fin inf verb nounvform marking headVFORM:vformsignQRETR:listQSTORE:listSYNSEM:synsemsynsemLOC:localcategoryHEAD:headMARKING:markinglocalCAT:category?Figure 4: Part of the signature underlying the constraint in Figure 3.goals for both conditionals eventually unsuspend.For atomic conditionals, we must thread twoextra arguments, VsIn, and VsOut, which trackwhich variables have been seen so far.
Delayingon atomic type conditionals is implemented by aspecial whentype/3 primitive (Section 3.4), andfeature descriptions reduce using unique featureintroduction:whenfs(V=T,Goal,Vs,Vs) :-type(T) -> whentype(T,V,Goal).whenfs(V=(F:Desc),Goal,VsIn,VsOut):-unique introducer(F,Intro),whentype(Intro,V,(farg(F,V,FVal),whenfs(FVal=Desc,Goal,VsIn,VsOut))).farg(F,V,FVal) binds FVal to the argumentposition of V that corresponds to the feature F onceV has been instantiated to a type for which F isappropriate.In the variable case, whenfs/4 simply binds thevariable when it first encounters it, but subsequentoccurrences of that variable create a suspensionusing Prolog when/2, checking for identity withthe previous occurrences.
This implements aprimitive delay on structure sharing (Section 3.4):whenfs(V=X,Goal,VsIn,VsOut) :-var(X),(select(VsIn,X,VsOut)-> % not first X - waitwhen(?=(V,X),((V==X) -> call(Goal) ; true)); % first X - bindVsOut=VsIn,V=X,call(Goal)).In practice, whenfs/2 can be partially evalu-ated by a compiler.
In the running example, Fig-ure 3, we can compile the whenfs/2 subgoal in(2) into simpler whentype/2 subgoals, that delayuntil X reaches a particular type.
The second case ofwhenfs/4 tells us that this can be achieved by suc-cessively waiting for the types that introduce eachof the features, SYNSEM, LOC, and CAT.
As shownin Figure 4, those types are sign, synsem and local,respectively (Figure 3 (3)).The description that CatVal is suspended on isa conjunction, so we successively suspend on eachconjunct.
The type that introduces both HEAD andMARKING is category (4).
In practice, static anal-ysis can greatly reduce the complexity of the re-sulting relational goals.
In this case, static analy-sis of the type system tells us that all four of thesewhentype/2 calls can be eliminated (5), since Xmust be a sign in this context, synsem is the leastappropriate type of any SYNSEM value, local is theleast appropriate type of any LOC value, and cate-gory is the least appropriate type of any CAT value.3.4 Primitive delay statementsThe two fundamental primitives typically providedfor Prolog terms, e.g., by SICStus Prolog when/2,are: (1) suspending until a variable is instantiated,and (2) suspending until two variables are equatedor inequated.
The latter corresponds exactly tostructure-sharing in TFSs, and to shared variablesin descriptions; its implementation was already dis-cussed in the previous section.
The former, if car-ried over directly, would correspond to delaying un-til a variable is promoted to a type more specificthan ?, the most general type in the type semi-lattice.
There are degrees of instantiation in LTFS,however, corresponding to long subtyping chainsthat terminate in ?.
A more general and usefulprimitive in a typed language with such chains issuspending until a variable is promoted to a partic-ular type.
whentype(Type,X,Goal), i.e., de-laying subgoal Goal until variable X reaches Type,is then the non-universally-quantified cousin of thetype-antecedent constraints that are already used inthe ERG.How whentype(Type,X,Goal) is imple-mented depends on the data structure used for TFSs,but in Prolog they invariably use the underlying Pro-log implementation of when/2.
In ALE, for ex-ample, TFSs are represented with reference chainsthat extend every time their type changes.
Onecan simply wait for a variable position at the endof this chain to be instantiated, and then com-pare the new type to Type.
Figure 3 (6) showsa schematic representation of a sign-typed TFSwith SYNSEM value SynVal, and two other ap-propriate feature values.
Acting upon this as itssecond argument, the corresponding definition ofwhentype(Type,X,Goal) in Figure 3 (7) de-lays on the variable in the extra, fourth argumentposition.
This variable will be instantiated to a sim-ilar term when this TFS promotes to a subtype ofsign.As described above, delaying until the antecedentof the principle in Figure 3 (1) is true or false ul-timately reduces to delaying until various featurevalues attain certain types using whentype/3.
ATFS may not have substructures that are specificenough to determine whether an antecedent holdsor not.
In this case, we must wait until it is knownwhether the antecedent is true or false before ap-plying the consequent.
If we reach a deadlock,where several constraints are suspended on theirantecedents, then we must use another resolutionmethod to begin testing more specific extensions ofthe TFS in turn.
The choice of these other methodscharacterizes a true CLP solution for LTFS, all ofwhich are enabled by the method presented in thispaper.
In the case of the signature in Figure 4, oneof these methods may test whether a marking-typedsubstructure is consistent with either fin or inf.
If itis consistent with fin, then this branch of the searchmay unsuspend the Finiteness Marking Principle ona sign-typed TFS that contains this substructure.4 Measuring the cost of delayingHow much of a cost do we pay for using delay-ing?
In order to answer this question definitively,we would need to reimplement a large-scale gram-mar which was substantially identical in every wayto the ERG but for its use of delay statements.
Theconstruction of such a grammar is outside the scopeof this research programme, but we do have accessto MERGE,2 which was designed to have the sameextensional coverage of English as the ERG.
Inter-nally, the MERGE is quite unlike the ERG.
Its TFSsare far larger because each TFS category carries in-side it the phrase structure daughters of the rule thatcreated it.
It also has far fewer types, more fea-ture values, a heavy reliance on lists, about a thirdas many phrase structure rules with daughter cate-gories that are an average of 32% larger, and manymore constraints.
Because of these differences, thisversion of MERGE runs on average about 300 timesslower than the ERG.On the other hand, MERGE uses delaying for allthree of the purposes that have been discussed in thispaper: complex antecedents, explicit whenfs/2calls to avoid non-termination problems, and ex-plicit whenfs/2 calls to avoid expensive non-deterministic searches.
While there is currently nodelay-free grammar to compare it to, we can popopen the hood on our implementation and mea-sure delaying relative to other system functions onMERGE with its test suite.
The results are shown inFigure 5.
These results show that while the per callper sent.avg.
avg.
%Function ?s avg.
parse/ call # calls timePS rules 1458 410 0.41Chart access 13.3 13426 0.12Relations 4.0 1380288 1.88Delays 2.6 3633406 6.38Path compression 2.0 955391 1.31Constraints 1.6 1530779 1.62Unification 1.5 37187128 38.77Dereferencing 0.5 116731777 38.44Add type MGSat 0.3 5131391 0.97Retrieve feat.
val.
0.02 19617973 0.21Figure 5: Run-time allocation of functionality inMERGE.
Times were measured on an HP Omni-book XE3 laptop with an 850MHz Pentium II pro-cessor and 512MB of RAM, running SICStus Pro-log 3.11.0 on Windows 98 SE.cost of delaying is on a par with other system func-tions such as constraint enforcement and relationalgoal resolution, delaying takes between three andfive times more of the percentage of sentence parse2The author sincerely thanks Kordula DeKuthy and Det-mar Meurers for their assistance in providing the version ofMERGE (0.9.6) and its test suite (1347 sentences, average wordlength 6.3, average chart size 410 edges) for this evaluation.MERGE is still under development.time because it is called so often.
This reflects, inpart, design decisions of the MERGE grammar writ-ers, but it also underscores the importance of havingan efficient implementation of delaying for large-scale use.
Even if delaying could be eliminated en-tirely from this grammar at no cost, however, a 6%reduction in parsing speed would not, in the presentauthor?s view, warrant the loss of modularity in agrammar of this size.5 ConclusionIt has been shown that a simple generalization ofconventional delay statements to LTFS, combinedwith a subsumption-based interpretation of impli-cational constraints and unique feature introductionare sufficient to restore much of the functionalityand concomitant benefit that has been routinely sac-rificed in HPSG in the name of parsing efficiency.While a definitive measurement of the computa-tional cost of this functionality has yet to emerge,there is at least no apparent indication from theexperiments that we can conduct that disjunction,complex antecedents and/or a judicious use of recur-sion pose a significant obstacle to tractable grammardesign when the right control strategy (CLP withsubsumption testing) is adopted.ReferencesH.
A?
?t-Kaci, A. Podelski, and G. Smolka.
1992.A feature-based constraint system for logic pro-gramming with entailment.
In Proceedings ofthe International Conference on Fifth GenerationComputer Systems.H.
A??t-Kac?i.
1984.
A Lattice-theoretic Approach toComputation based on a Calculus of Partially Or-dered Type Structures.
Ph.D. thesis, University ofPennsylvania.B.
Carpenter and G. Penn.
1996.
Compiling typedattribute-value logic grammars.
In H. Bunt andM.
Tomita, editors, Recent Advances in ParsingTechnologies, pages 145?168.
Kluwer.B.
Carpenter.
1992.
The Logic of Typed FeatureStructures.
Cambridge.A.
Copestake, D. Flickinger, C. Pollard, and I. Sag.2003.
Minimal Recursion Semantics: An intro-duction.
Journal submission, November 2003.J.
Doerre, M. Dorna, J. Junger, and K. Schneider,1996.
The CUF User?s Manual.
IMS Stuttgart,2.0 edition.J.
Doerre.
1993.
Generalizing Earley deductionfor constraint-based grammars.
Technical ReportR1.2.A, DYANA Deliverable.D.
Flickinger.
2000.
On building a more efficientgrammar by exploiting types.
Natural LanguageEngineering, 6(1):15?28.T.
Goetz and W.D.
Meurers.
1997.
Interleav-ing universal principles and relational constraintsover typed feature logic.
In Proceedings of the35th ACL / 8th EACL, pages 1?8.M.
Ho?hfeld and G. Smolka.
1988.
Definite re-lations over constraint languages.
LILOG Re-port 53, IBM Deutschland.H.-U.
Krieger and J. Nerbone.
1991.
Feature-basedinheritance networks for computational lexicons.In Proceedings of the ACQUILEX Workshop onDefault Inheritance in the Lexicon, number 238in University of Cambridge, Computer Labora-tory Technical Report.H.-U.
Krieger and U. Scha?fer.
1994.
TDL ?a type description language for HPSG part1: Overview.
Technical Report RR-94-37,Deutsches Forschungszentrum fu?r Ku?nstliche In-telligenz (DFKI), November.LinGO.
1999.
The LinGO grammar and lexicon.Available on-line at http://lingo.stanford.edu.G.
Penn and K. Hoetmer.
2003.
In search of epis-temic primitives in the english resource grammar.In Proceedings of the 10th International Confer-ence on Head-driven Phrase Structure Grammar,pages 318?337.G.
Penn.
2001.
Tractability and structural closuresin attribute logic signatures.
In Proceedings ofthe 39th ACL, pages 410?417.C.
J. Pollard.
1998.
Personal communiciation to theauthor.G.
Smolka.
1991.
Residuation and guarded rulesfor constraint logic programming.
Technical Re-port RR-91-13, DFKI.G.
Smolka.
1994.
A calculus for higher-orderconcurrent constraint programming with deepguards.
Technical Report RR-94-03, DFKI.P.
Van Roy.
1990.
Can Logic Programming Exe-cute as Fast as Imperative Programming?
Ph.D.thesis, University of California, Berkeley.S.
Wintner.
2002.
Modular context-free grammars.Grammars, 5(1):41?63.
