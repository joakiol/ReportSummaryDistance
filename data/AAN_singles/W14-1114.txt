Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 90?95,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsReducing VSM data sparseness by generalizing contexts:application to health text miningAmandine P?erinetINSERM, U1142, LIMICS, Paris, FranceSorbonne Universit?es, UPMC Univ Paris 06, Paris, FranceUniversit?e Paris 13, Sorbonne Paris Cit?e, Villetaneuse, Franceamandine.perinet@edu.univ-paris13.frThierry HamonLIMSI-CNRS, Orsay, FranceUniversit?e Paris 13, Sorbonne Paris Cit?eVilletaneuse, Francehamon@limsi.frAbstractVector Space Models are limited with lowfrequency words due to few available con-texts and data sparseness.
To tackle thisproblem, we generalize contexts by inte-grating semantic relations acquired withlinguistic approaches.
We use three meth-ods that acquire hypernymy relations on aEHR corpus.
Context Generalization ob-tains the best results when performed withhypernyms, the quality of the relations be-ing more important than the quantity.1 IntroductionDistributional Analysis (DA) (Harris, 1954; Firth,1957) computes a similarity between target wordsfrom the contexts shared by those two words.This hypothesis is applied with geometric meth-ods, such as the Vector Space Model (VSM) (Tur-ney and Pantel, 2010).
The advantage of the VSMis that the similarity of word meaning can be easilyquantified by measuring their distance in the vec-tor space, or the cosine of the angle between them(Mitchell and Lapata, 2010).
On the other hand,a major inconvenience is data sparseness withinthe matrix that represents the vector space (Tur-ney and Pantel, 2010).
The data sparseness prob-lem is the consequence of the word distribution ina corpus (Baroni et al., 2009): in any corpus, mostof the words have a very low frequency and ap-pear only a few times.
Thus, those words have alimited set of contexts and similarity is difficult tocatch.
Thus, methods based on DA perform betterwhen more information is available (Weeds andWeir, 2005; van der Plas, 2008) and are efficientwith large corpora of general language.
But withspecialized texts, as EHR texts that are usually ofsmaller size, reducing data sparseness is a majorissue and methods need to be adapted.Semantic grouping of contexts should decreasetheir diversity, and thus increase the frequency ofthe remaining generalized contexts.
We assumethat generalizing contexts may influence the distri-butional context frequencies.
Information for gen-eralization can be issued from existing resourcesor can be computed by linguistic approaches.
Inthis paper, we propose to use semantic relationsacquired by relation acquisition methods to groupwords in contexts.
We define a method thatswitches words in DA contexts for their hierar-chical parent or morphosyntactic variant that havebeen computed on the corpus with linguistic ap-proaches before applying the VSM method.In the following, we first present the relatedwork, then our method and we finally describe thedifferent experiments we led.
The results obtainedon the EHR corpus are then evaluated in terms ofprecision and MAP, and analyzed.2 Related workOur approach relates with works that influencedistributional contexts to improve the performanceof VSMs.
Some of them intend to change theway to consider contexts; Broda et al.
(2009) donot use the raw context frequency in DA, but theyfirst rank contexts according to their frequency,and take the rank into account.
Other modelsuse statistical language models to determine themost likely substitutes to represent the contexts(Baskaya et al., 2013).
They assign probabilities toarbitrary sequences of words that are then used tocreate word pairs to feed a co-occurrence model,before performing a clustered algorithm (Yuret,2012).
The limit of such methods is that their per-formance is proportional to vocabulary size and re-quires the availability of training data.Influence on contexts may also be performed byembedding additional semantic information.
Thesemantic relations may be issued from an exist-ing resource or automatically computed.
Witha method based on bootstrapping, Zhitomirsky-Geffet and Dagan (2009) modify the weights of90the elements in contexts relying on the seman-tic neighbors found with a distributional similar-ity measure.
Based on this work, Ferret (2013)uses a set of examples selected from an origi-nal distributional thesaurus to train a supervisedclassifier.
This classifier is then applied forreranking the neighbors of the thesaurus selec-tion.
Within Vector Space Model, Tsatsaronis andPanagiotopoulou (2009) use a word thesaurus tointerpret the orthogonality of terms and measuresemantic relatedness.With the same purpose of solving the problem ofdata sparseness, other methods are based on di-mensionality reduction, such as Latent SemanticAnalysis (LSA) in (Pad?o and Lapata, 2007) orNon-negative Matrix Factorization (NMF) (Zhenget al., 2011).
Matrix decomposition techniques areusually applied to reduce the dimensionality of theoriginal matrix, thereby rendering it more infor-mative (Mitchell and Lapata, 2010).Our approach differs from the aforementionedones in that we add semantic information in con-texts to reduce the number of contexts and to in-crease their frequency.
Contrary to these latter ap-proaches, we do not reduce the contexts by remov-ing information but by generalyzing informationand integrating extra semantic knowledge.3 VSM and context generalizationThe contexts in which occurs a target word haveassociated frequencies which may be used to formprobability estimates.
The goal of our method isto influence the distributional context frequenciesby generalizing contexts.Step 1: target and context definition Duringthis step, we define targets and contexts, with dif-ferent constraints for their extraction.
To adaptour method to specialized texts, we identify terms(specific terminological entities that denote anevent) with a term extractor (YATEA (Aubin andHamon, 2006)).
Target words are both nounsand terms (T).
Their distributional contexts corre-spond to a graphical window of n number of wordsaround the targets (Wilks et al., 1990; Sch?utze,1998; Lund and Burgess, 1996).
We consider twodifferent window sizes defined in section 4.Linguistic approaches During the generaliza-tion process, we use three existing linguistic ap-proaches: two that acquire hypernymy relationsand one to get morphosyntactic variants.
Lexico-syntactic Patterns (LSP) acquire hypernymy re-lations.
We use the patterns defined by (Hearst,1992).
Lexical Inclusion (LI) acquires hypernymyrelations and uses the syntactic analysis of theterms.
Based on the hypothesis that if a term islexically included in another, generally there is ahypernymy relation between the two terms (kid-ney transplant - cadaveric kidney transplant) (Bo-denreider et al., 2001).
Terminological Variation(TV) acquires both hypernyms and synonyms.
TVuses rules that define a morpho-syntactic transfor-mation, mainly the insertion (blood transfusion -blood cell transfusion (Jacquemin, 1996).Step 2: context generalization Once targetsand contexts are defined, we generalize contextswith the relations acquired by the three linguis-tic approaches we mentioned.
To integrate therelations in contexts, we replace words in con-text by their hypernym or morphosyntactic variant.We define two rules: (1) if the context matcheswith one hypernym, context is replaced by thishypernym.
(2) if the context matches with sev-eral hypernyms or variants, we take the hypernymor variant frequency into account, and choose themost frequent hypernym/variant.
The generaliza-tion step is individually or sequentially performedwhen several relation sets are available.Step 3: computation of semantic similarityAfter the generalization step, similarity betweentarget words is computed.
As we previously de-crease diversity in contexts, we choose a mea-sure that favors words appearing in similar con-texts.
We use the Jaccard Index (Grefenstette,1994) which normalizes the number of contextsshared by two words by the total number of con-texts of those two words.Parameter: thresholds The huge number of re-lations we obtain after computing similarity be-tween targets leads us to remove the supposedwrong relations with three thresholds: (i) numberof shared lemmatized contexts (2 for a large win-dow, 1 for a small window) ; (ii) number of thelemmatized contexts (2 for a large window, 1 fora small window) ; (iii) number of the lemmatizedtargets (3 for both window sizes).
For each pa-rameter, the threshold is automatically computed,according to the corpus, as the mean of the valuesof parameters on the corpus.
And we experimenttwo thresholds on similarity score we empiricallydefined : sim > 0.001 and sim > 0.0005.914 ExperimentsIn this section, we present the material we use forthe experiments and evaluation, and the distribu-tional parameter values of the VSM automaticallydetermined from the data.
We then describe thegeneralization sets we experiment and the evalua-tion measures we used for evaluation.4.1 CorpusWe use the collection of anonymous clinical En-glish texts provided by the 2012 i2b2/VA chal-lenge (Sun et al., 2013).The corpus is pre-processed within the Ogmiosplatform (Hamon et al., 2007).
We perform mor-phosyntactic tagging and lemmatization with TreeTagger (Schmid, 1994), and term extraction withYATEA (Aubin and Hamon, 2006).4.2 Distributional parametersWe consider two window sizes: a large windowof 21 words (?
10 words, centered on the tar-get, henceforth W21) and a narrow one of 5 words(?
2 words, centered on the target, W5).The window size influences on the type, thevolume and the quality of the acquired relations.Generally, the smaller windows allow to acquiremore relevant contexts for a target, but increasethe data sparseness problem (Rapp, 2003).
Theygive better results for classical types of relations(eg.
synonymy), whereas larger windows are moreappropriate for domain relations (eg.
colloca-tions)(Sahlgren, 2006; Peirsman et al., 2008).4.3 Generalizing distributional contextsWe define several sets of context generalization.We experiment in step 2 different ways of gener-alizing contexts.
We use as a baseline the VSMwithout any generalization in the contexts (VS-Monly), and compare the generalization sets to it.Regarding context generalization, we first ex-ploit the relations acquired from only one linguis-tic approach.
We apply the method describedat the section 3 (step 2) by separately using thethree different sets of relations automatically ac-quired.
Distributional contexts are replaced bytheir hypernym acquired with lexico-syntactic pat-terns (VSM/LSP) and lexical inclusion (VSM/LI),and by their morphosyntactic variants acquiredwith terminological variation (VSM/TV).
Then,we replace contexts with relations acquired by twoapproaches (TV then LI, LSP then TV, etc.).
Thisgeneralization is done sequentially: we generalizeall the contexts with the relations acquired by onemethod (e.g.
LI), and then with the relations ac-quired by another method (e.g.
TV).
And finally,similarly to what we perform with two methods,we experiment the generalization of contexts byrelations acquired with the three different linguis-tic approaches (e.g.
LSP then LI then TV).
Weexperiment all the possible combinations.
Withboth the single and multiple generalization, weaim at evaluating the contribution of each methodbut also the impact of the order of the methods.4.4 EvaluationIn order to evaluate the quality of the acquired re-lations, we compare our relations to the 53,203UMLS relations between terms occurring in ourEHR corpus.
We perform the evaluation withthe Mean Average Precision (MAP) (Buckley andVoorhees, 2005) and the macro-precision com-puted for each target word: semantic neighborsfound in the resource by the total semantic neigh-bors acquired by our method.
We consider threesets of neighbors: precision after examining 1(P@1), 5 (P@5) and 10 (P@10) neighbors.5 Results and discussionBest results are obtained with a large window of21 words, with a precision P@1 of 0.243 against0.032 for a 5 word window, both for VSMonly,with a threshold of 0.001.
Thus, a high thresh-old on the similarity score is not always relevant.We observe on this corpus that the generalizationwith the several linguistic approaches does not im-prove the results.
For instance, VSM/LI obtains0.250 of P@1 with a > 0.001 threshold, and thisprecision is the same with VSM/LI+TV and withVSM/LI+LSP.
This is an interesting behavior, dif-ferent from what have been observed so far onmore general French corpora that contains cook-ing recipes (P?erinet and Hamon, 2013).We discuss here the results we obtain for terms,for the two thresholds on the similarity score: alow and a higher thresholds, with relations with asimilarity above 0.0005 and above 0.001.
We ob-serve that with a higher threshold, the precision ishigher, with a P@1 of 0.243 against 0.187 for thelower threshold (when considering VSMonly).
Asfor the number of relations acquired, with a lowerthreshold we obtain more relations (3,936 rela-tions acquired for the baseline) than with a higher92threshold (326 relations for the baseline).We evaluate precison after examining threegroups of neighbors.
The best results are ob-tained with P@1, and in most cases, precision de-creases when we consider more neighbors: themore neighbors we consider, the lower precisionis.
For a 0.001 threshold, the generalized experi-ment sets obtain a higher precision than VSMonly,in any case.
While for a 0005 threshold, the use ofLI to generalize contexts decreases the precision.We also observe that when considering generali-sation with TV or LSP only, or their combination,the P@10 is slightly better than P@5.The MAP values are higher when the thresoldon the similarity measure is low, with 0.446 forVSM/LI against 0.089 with the > 0.001 thresh-old.
It means that some correct relations are notwell ranked with the similarity score, but are stillpresent.
We observe that the MAP values arealways higher with the generalization sets thanwith the baseline with both thresholds: 0.089 forVSM/LI, 0.446 for VSM/LI+LSP, etc.Comparison of the experimental sets Whenconsidering the relations found in the UMLS, weobserve that the generalization with LSP bringsthe same relations that the baseline VSMonly plus22 relations, the generalization with TV brings 16more relations that VSMonly, and finally that thegeneralization with LI decreases the number of re-lations acquired.
When the generalization of thecontexts is performed with LI, only with LI or withLI combined to another method, it decreases thenumber of relations acquired as well as the num-ber of relations found in the resource.
On the con-trary, generalizing contexts with LSP increases thenumber of relations acquired as well as the num-ber of relations found in the UMLS resource.
Weobtain the highest number of relations when gener-alizing contexts with LSP, with 454 relations, andthe highest precision with 0.273 for P@1.Comparing those results with the relations ac-quired with the linguistic approaches on the EHRcorpus shows a correlation between the quality ofthe relations acquired with the generalized sets andthe relations used for generalization.
Indeed, LIgives the highest number of relations with 14,437relations, then TV gives 631 relations, and fi-nally LSP acquires only three relations: pancre-atic complication - necrosis, pancreatic complica-tion - abscess, gentle laxative - milk of magnesia.With these relations, if the second term (eg.necrosis) is found in the context, it is replaced bythe first term (eg.
pancreatic complication).
Thesethree relations used for generalization give betterresults in terms of precision that the many relationsgiven by the two other approaches.
We could de-duce that the number of relations may not be as im-portant as their quality when they are used for gen-eralization.
But when the LSP are used after TV orLI, they do not improve the results.
From this ob-servation, we make the hypothesis that these sec-ond terms may have already been replaced duringthe generalization with LI or TV.
To confirm or re-ject this hypothesis, we look closer to the relationsacquired with TV and LI.
In TV, we find no rela-tion including any of these second terms.
On thecontrary, with LI, we found the relation milk - milkof magnesia that inhibits one of the three relationsacquired with the LSP.We deduce that even if the quality of the re-lations used for generalization is more importantthan their number, the number of relations stillmatters.
If generalization is first performed witha great number of relations, then a small numberof relations used for generalization is not enoughand does not improve the results.6 Conclusion and perspectivesIn this work, we face the problem of data sparese-ness of distributional methods.
This problem espe-cially arises from specialized corpora which havea smaller size and in which words and terms havelower frequencies.To achieve this goal, we propose to generalizedistributional contexts with hypernyms and vari-ants acquired by three existing approaches.
We fo-cus on the acquistion of relations between terms.We experimented several generalization sets, us-ing one, two or the three methods sequentiallyto replace words in context by their hypernym orvariant.
Evaluation of the method has been per-formed on an EHR English text collection.
Gen-eralization obtains the best results when realizedwith hypernyms.
The quality of the relations mat-ters much more than their number: few but goodrelations used to generalize contexts give better re-sults than many relations of poorer quality.
Forfuture work, we plan to use for generalization re-lations issued from different distributional and ter-minological resources.
Finally, we will intend tocombine the methods before normalization.93ReferencesSophie Aubin and Thierry Hamon.
2006.
Improvingterm extraction with terminological resources.
InAdvances in Natural Language Processing, number4139 in LNAI, pages 380?387.
Springer.Marco Baroni, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
2009.
The wacky wide web:a collection of very large linguistically processedweb-crawled corpora.
Language Resources andEvaluation, 43(3):209?226.Osman Baskaya, Enis Sert, Volkan Cirik, and DenizYuret.
2013.
Ai-ku: Using substitute vectors andco-occurrence modeling for word sense inductionand disambiguation.
In Proceedings of SemEval -2013, pages 300?306, Atlanta, Georgia, USA.
As-sociation for Computational Linguistics.Olivier Bodenreider, Anita Burgun, and Thomas Rind-flesch.
2001.
Lexically-suggested hyponymic rela-tions among medical terms and their representationin the umls.
In TIA 2001, pages 11?21.Bartosz Broda, Maciej Piasecki, and Stan Szpakowicz.2009.
Rank-based transformation in measuring se-mantic relatedness.
In Yong Gao and Nathalie Jap-kowicz, editors, Canadian Conference on AI, vol-ume 5549, pages 187?190.
Springer.Chris Buckley and Ellen Voorhees.
2005.
Retrievalsystem evaluation.
In Ellen Voorhees and DonnaHarman, editors, TREC: Experiment and Evaluationin Information Retrieval, chapter 3.
MIT Press.Olivier Ferret.
2013.
S?election non supervis?ee derelations s?emantiques pour am?eliorer un th?esaurusdistributionnel.
In TALN 2013, pages 48?61, LesSables d?Olonne, France.J.R.
Firth.
1957.
A synopsis of linguistic theory 1930-1955.
Studies in linguistic analysis, pages 1?32.Gregory Grefenstette.
1994.
Corpus-derived first, sec-ond and third-order word affinities.
In Sixth EuralexInternational Congress, pages 279?290.T.
Hamon, A. Nazarenko, T. Poibeau, S. Aubin, andJ.
Derivi`ere.
2007.
A robust linguistic platform forefficient and domain specific web content analysis.In RIAO 2007, Pittsburgh, USA.Zellig Harris.
1954.
Distributional structure.
Word,10(23):146?162.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In InternationalConference on Computational Linguistics, pages539?545, Nantes, France.Christian Jacquemin.
1996.
A symbolic and surgi-cal acquisition of terms through variation.
In CoRR,pages 425?438.K.
Lund and C. Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instru-mentation, and Computers, 28:203?208.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1429.Sebastian Pad?o and Mirella Lapata.
2007.Dependency-based construction of semanticspace models.
Comput.
Linguist., 33(2):161?199.Yves Peirsman, Heylen Kris, and Geeraerts Dirk.2008.
Size matters.
tight and loose context def-initions in english word space models.
In ESS-LLI Workshop on Distributional Lexical Semantics,Hamburg, Germany.Amandine P?erinet and Thierry Hamon.
2013.
Hybridacquisition of semantic relations based on contextnormalization in distributional analysis.
In Proceed-ings of TIA 2013, pages 113?120, Paris, France.Reinhard Rapp.
2003.
Word sense discovery based onsense descriptor dissimilarity.
In MT Summit?2003,pages 315?322.Magnus Sahlgren.
2006.
The Word-Space Model:Using Distributional Analysis to Represent Syntag-matic and Paradigmatic Relations between Wordsin High-Dimensional Vector Spaces.
Ph.D. thesis,Stockholm University, Stockholm, Sweden.H.
Schmid.
1994.
Probabilistic part-of-speech taggingusing decision trees.
In New Methods in LanguageProcessing, pages 44?49, Manchester, UK.Hinrich Sch?utze.
1998.
Automatic word sense dis-crimination.
Comput.
Linguist., 24(1):97?123.Weiyi Sun, Anna Rumshisky, and?Ozlem Uzuner.2013.
Evaluating temporal relations in clinical text:2012 i2b2 challenge.
JAMIA, 20(5):806?813.George Tsatsaronis and Vicky Panagiotopoulou.
2009.A generalized vector space model for text retrievalbased on semantic relatedness.
In EACL 2009,pages 70?78, Stroudsburg, PA, USA.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of seman-tics.
JAIR, 37:141?188.Lonneke van der Plas.
2008.
Automatic lexico-semantic acquisition for question answering.
Th`esede doctorat, University of Groningen, Groningen.Julie Weeds and David Weir.
2005.
Co-occurrenceretrieval: A flexible framework for lexical distribu-tional similarity.
Comput.
Linguist., 31(4):439?475.Yorick A. Wilks, Dan, James E. Mcdonald, TonyPlate, and Brian M. Slator.
1990.
Providing ma-chine tractable dictionary tools.
Journal of MachineTranslation, 2.94Deniz Yuret.
2012.
Fastsubs: An efficient and exactprocedure for finding the most likely lexical substi-tutes based on an n-gram language model.
IEEESignal Process.
Lett., 19(11):725?728.Wenbin Zheng, Yuntao Qian, and Hong Tang.
2011.Dimensionality reduction with category informationfusion and non-negative matrix factorization for textcategorization.
In Hepu Deng, Duoqian Miao, Jing-sheng Lei, and Fu Lee Wang, editors, AICI, volume7004 of LNCS, pages 505?512.Maayan Zhitomirsky-Geffet and Ido Dagan.
2009.Bootstrapping distributional feature vector quality.Comput.
Linguist., 35(3):435?461.95
