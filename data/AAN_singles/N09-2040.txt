Proceedings of NAACL HLT 2009: Short Papers, pages 157?160,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsAnswer Credibility:  A Language Modeling Approachto Answer ValidationProtima Banerjee   Hyoil HanCollege of Information Science and TechnologyDrexel UniversityPhiladelphia, PA 19104pb66@drexel.edu, hyoil.han@acm.orgAbstractAnswer Validation is a topic of significant in-terest within the Question Answering commu-nity.
In this paper, we propose the use oflanguage modeling methodologies for AnswerValidation, using corpus-based methods that donot require the use of external sources.
Specifi-cally, we propose a model for Answer Credibil-ity which quantifies the reliability of a sourcedocument that contains a candidate answer andthe Question?s Context Model.1 IntroductionIn recent years, Answer Validation has become atopic of significant interest within the QuestionAnswering community.
In the general case, onecan describe Answer Validation as the process thatdecides whether a Question is correctly answeredby an Answer according to a given segment of sup-porting Text.
Magnini et al (Magnini, 2002) pre-sents an approach to Answer Validation that usesredundant information sources on the Web; theypropose that the number of Web documents inwhich the question and the answer co-occurred canserve as an indicator of answer validity.
Other re-cent approaches to Answer Validation Exercise inthe Cross-Language Evaluation Forum (CLEF)(Peters, 2008) make use of textual entailmentmethodologies for the purposes of Answer Valida-tion.In this paper, we propose the use of language mod-eling methodologies for Answer Validation, usingcorpus-based methods that do not require the useof external sources.
Specifically, we propose thedevelopment of an Answer Credibility score whichquantifies reliability of a source document thatcontains a candidate answer with respect to theQuestion?s Context Model.
Unlike many textualentailment methods, our methodology has the ad-vantage of being applicable to question types forwhich hypothesis generation is not easily accom-plished.The remainder of this paper describes our work inprogress, including our model for Answer Credi-bility, our experiments and results to date, and fu-ture work.2 Answer CredibilityCredibility has been extensively studied in the fieldof information science (Metzger, 2002).
Credibil-ity in the computational sciences has been charac-terized as being synonymous with believability,and has been broken down into the dimensions oftrustworthiness and expertise.Our mathematical model of Answer Credibilityattempts to quantify the reliability of a source us-ing the semantic Question Context.
The semanticQuestion Context is built using the Aspect-BasedRelevance Language Model that was presented in(Banerjee, 2008) and (Banerjee, 2009).
This modelbuilds upon the Relevance Based Language Model(Lavrenko, 2001) and Probabilisitic Latent Seman-tic Analysis (PLSA) (Hofmann, 1999) to provide amechanism for relating sense disambiguated Con-cept Terms (CT) to a query by their likelihood ofrelevance.The Aspect-Based Relevance Language Modelassumes that for every question there exists an un-157derlying relevance model R, which is assignedprobabilities P(z|R) where z is a latent aspect of theinformation need, as defined by PLSA.
Thus, wecan obtain a distribution of aspects according totheir likelihood of relevancy to the user?s informa-tion need.
By considering terms from the aspectsthat have the highest likelihood of relevance (eg.highest P(z|R) values), we can build a distributionthat models a semantic Question Context.We define Answer Credibility to be a similaritymeasure between the Question Context (QC) andthe source document from which the answer wasderived.
We consider the Question Context to be adocument, which has a corresponding documentlanguage model.
We then use the well-knownKullback-Leibler divergence method (Lafferty,2001) to compute the similarity between the Ques-tion Context document model and the documentmodel for a document containing a candidate an-swer:Here, P(w|QC) is the language model of the Ques-tion Context, P(w|d) is the language model o thedocument containing the candidate answer.
Toinsert this model into the Answer Validation proc-ess, we propose an interpolation technique thatmodulates the answer score during the process us-ing Answer Credibility.3 Experimental SetupThe experimental methodology we used is shownas a block diagram in Figure 1.
To validate ourapproach, we used the set of all factoid questionsfrom the Text Retrieval Conference (TREC) 2006Question Answering Track (Voorhees, 2006).The OpenEphyra Question Answering testbed(Schlaefer, 2006) was then used as the frameworkfor our Answer Credibility implementation.OpenEphyra uses a baseline Answer Validationmechanism which uses documents retrieved usingYahoo!
search to support candidate answers foundin retrieved passages.
In our experiments, we con-structed the Question Context according to themethodology described in (Banerjee, 2008).
Ourexperiments used the Lemur Language Modelingtoolkit (Strohman, 2005) and the Indri search en-gine (Ogilvie, 2001) to construct the QuestionContext and document language models.Figure 1:  Experiment MethodologyWe then inserted an Answer Credibility filter intothe OpenEphyra processing pipeline which modu-lates the OpenEphyra answer score according tothe following formula:Here score is the original OpenEphyra answerscore and score' is the modulated answer score.
Inthis model, ?
is an interpolation constant which weset using the average of the P(z|R) values for thoseaspects that are included in the Question Context.For the purposes of evaluating the effectiveness ofour theoretical model, we use the accuracy andMean Reciprocal Rank (MRR) metrics (Voorhees,2005).4 ResultsWe compare the results of the baselineOpenEphyra Answer Validation approach againstthe results after our Answer Credibility processinghas been included as a part of the OpenEphyrapipeline.
Our results are presented in Table 1 andTable 2.To facilitate interpretation of our results, we sub-divided the set of factoid questions into categoriesby their question words, following the example of(Murdock, 2006).
The light grey shaded cells inboth tables indicate categories for which improve-ments were observed after our Answer Credibilitymodel was applied.
The dark grey shaded cells inboth tables indicate categories for which no changewas observed.
The paired Wilcoxon signed rank)|()|(log)|(dwPQCwPQCwPibilityAnswerCredCTw?
?=AQUIANTCorpusTREC 2006,Questions QueriesBackground?Concepts?in CorpusQuery termsQuery contextQueryContextualizationPLSAQA QuestionAnalysisQA AnswerExtractionQA AnswerSelection andValidationOpenEphyra QA TestBedCandidateDocumentsInformationRetrievalTREC 2006Judgements AnswersEvaluationibilityAnswerCredscorescore **)1(' ??
+?=158test was used to measure significance in improve-ments for MRR; the shaded cells in Table 2 indi-cate results for which the results were significant(p<0.05).
Due to the binary results for accuracy atthe question level (eg.
a question is either corrector incorrect), the Wilcoxon test was found to beinappropriate for measuring statistical significancein accuracy.Table 1:  Average MRR of Baseline vs. Baseline IncludingAnswer CredibilityTable 2:  Average Accuracy of Baseline vs. Baseline In-cluding Answer CredibilityOur results show the following:?
A 5% improvement in accuracy over the base-line for ?what?-type questions.?
An overall improvement of 13% in accuracyfor ?who?-type questions, which include the?who,?
?who is?
and ?who was?
categories?
A 9% improvements in MRR for ?what?
typequestions?
An overall improvement of 25% in MRR for?who?-type questions, which include the?who,?
?who is?
and ?who was?
categories?
Overall, 7 out of 13 categories (58%) per-formed at the same level or better than thebaseline5 DiscussionIn this section, we examine some examples ofquestions that showed improvement to better un-derstand and interpret our results.First, we examine a ?who?
type question whichwas not correctly answered by the baseline system,but which was correctly answered after includingAnswer Credibility.
For the question ?Who is thehost of the Daily Show??
the baseline system cor-rectly determined the answer was ?Jon Stewart?but incorrectly identified the document that thisanswer was derived from.
For this question, theQuestion Context included the terms ?stewart,??comedy,?
?television,?
?news,?
and ?kilborn.?
(Craig Kilborn was the host of Daily Show until1999, which makes his name a logical candidatefor inclusion in the Question Context since theAQUAINT corpus spans 1996-2000).
In this case,the correct document that the answer was derivedfrom was actually ranked third in the list.
The An-swer Credibility filter was able to correctly in-crease the answer score of that document so that itwas ranked as the most reliable source for the an-swer and chosen as the correct final result.Next, we consider a case where the correct answerwas ranked at a lower position in the answer list inthe baseline results and correctly raised higher,though not to the top rank, after the application ofour Answer Credibility filter.
For the question?What position did Janet Reno assume in 1993?
?the correct answer (?attorney general?)
was ranked5 in the list in the baseline results.
However, inthis case the score associated with the answer waslower than the top-ranked answer by an order ofmagnitude.
The Question Context for this questionincluded the terms ?miami,?
?elian,?
?gonzales,??boy,?
?attorney?
and ?justice.?
After the applica-tion of our Answer Credibility filter, the score andrank of the correct answer did increase (which con-QuestionCategoryQuestionCountBaselineMRRBaseline + An-swer Credibil-ity MRRHow 20 0.33 0.28how many 58 0.21 0.16how much 6 0.08 0.02in what 47 0.68 0.60What 114 0.30 0.33what is 28 0.26 0.26When 29 0.30 0.19Where 23 0.37 0.37where is 6 0.40 0.40Which 17 0.38 0.26Who 17 0.51 0.63who is 14 0.60 0.74who was 24 0.43 0.55QuestionCategoryQuestionCountBaselineAccuracyBaseline +AnswerCredibilityAccuracyHow 20 0.25 0.20how many 58 0.12 0.07how much 6 0.00 0.00in what 47 0.64 0.55What 114 0.23 0.28what is 28 0.18 0.18When 29 0.21 0.10Where 23 0.30 0.30where is 6 0.33 0.33Which 17 0.29 0.18Who 17 0.47 0.59who is 14 0.57 0.71who was 24 0.38 0.50159tributed to an increase in MRR), but the increasewas not enough to overshoot the original top-ranked answer.Categories for which the Answer Credibility hadnegative effect included ?how much?
and ?howmany?
questions.
For these question types, thecorrect answer or correct document was frequentlynot present in the answer list.
In this case, the An-swer Credibility filter had no opportunity to in-crease the rank of correct answers or correctdocuments in the answer list.
This same reasoningalso limits our applicability to questions that re-quire a date in response.Finally, it is important to note here that the verynature of news data makes our methodology appli-cable to some categories of questions more thanothers.
Since our methodology relies on the abilityto derive semantic relationships via a statisticalexamination of text, it performs best on those ques-tions for which some amount of supporting infor-mation is available.6   Conclusions and Future WorkIn conclusion, we have presented a work in pro-gress that uses statistical language modeling meth-ods to create a novel measure called AnswerCredibility for the purpose of Answer Validation.Our results show performance increases in bothaccuracy and MRR for ?what?
and ?who?
typequestions when Answer Credibility is included as apart of the Answer Validation process.
Our goalsfor the future include further development of theAnswer Credibility model to include not onlyterms from a Question Context, but terms that canbe deduced to be in an Answer Context.ReferencesBanerjee, P., Han, H. 2008.
"Incorporation of Corpus-Specific Semantic Information into Question AnsweringContext," CIKM 2008 - Ontologies and InformationSystems for the Semantic Web Workshop, Napa Valley,CA.Banerjee, P., Han, H  2009.
"Modeling Semantic Ques-tion Context for Question Answering," To appear inFLAIRS 2009.Hofmann, T. 1999.
"Probabilistic latent semantic index-ing," Proceedings of the 22nd Annual InternationalSIGIR.Lafferty, J. and Zhai, C. 2001.
"Document languagemodels, query models, and risk minimization for infor-mation retrieval," in Proceedings of the 24th AnnualInternational ACM SIGIR, New Orleans, Louisiana: pp.111-119.Lavrenko, V. and Croft, W. B.
2001.
"Relevance basedlanguage models," Proceedings of the 24th annual inter-national ACM SIGIR, pp.
120-127.Magnini, B., Negri, M., Prevete, R. Tanev, H.
2002.
"Is It the Right Answer?
Exploiting Web Redundancyfor Answer Validation," in Association for Computa-tional Lingustistics (ACL) 2002, Philadelphia, PA, pp.425-432.Metzger, M.  2007.
"Making sense of credibility on theWeb: Models for evaluating online information andrecommendations for future research," Journal of theAmerican Society of Information Science and Technol-ogy (JASIST), vol.
58, p. 2078.Murdock, V.  2006.
Exploring Sentence Retrieval.VDM Verlag.Ogilvie, P. and Callan, J. P. 2001.
"Experiments Usingthe Lemur Toolkit," in Online Proceedings of the 2001Text Retrieval Conference (TREC).Peters, C.  2008.
?What happened in CLEF 2008:  In-troduction to the Working Notes.?
http://www.clef-campaign.org/2008/working_notes.Schlaefer, N., Gieselmann, P., Schaaf, T., & A., W.2006.
A Pattern Learning Approach to Question An-swering within the Ephyra Framework, In Proceedingsof the Ninth International Conference on Text, Speechand Dialogue (TSD).Strohman, T., Metzler, D., Turtle, H., and Croft, W. B.2005.
"Indri: A language model-based search engine forcomplex queries," International Conference on Intelli-gence Analysis McLean, VA.Voorhees, E. M. and Harman, D. K. 2005.
TREC: Ex-periment and Evaluation in Information Retrieval (Digi-tal Libraries and Electronic Publishing): The MIT Press.Voorhees, E. M. 2006.
"Overview of the TREC 2006Question Answering Track," in Online Proceedings of2006 Text Retrieval Conference (TREC).160
