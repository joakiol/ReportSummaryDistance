Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 165?168,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsUnsupervised Learning of Acoustic Sub-word UnitsBalakrishnan Varadarajan?
and Sanjeev Khudanpur?Center for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, MD 21218{bvarada2,khudanpur}@jhu.eduEmmanuel DupouxLaboratoire de Science Cognitiveet Psycholinguistique75005, Paris, Franceemmanuel.dupoux@gmail.comAbstractAccurate unsupervised learning of phonemesof a language directly from speech is demon-strated via an algorithm for joint unsupervisedlearning of the topology and parameters ofa hidden Markov model (HMM); states andshort state-sequences through this HMM cor-respond to the learnt sub-word units.
Thealgorithm, originally proposed for unsuper-vised learning of allophonic variations withina given phoneme set, has been adapted tolearn without any knowledge of the phonemes.An evaluation methodology is also proposed,whereby the state-sequence that aligns toa test utterance is transduced in an auto-matic manner to a phoneme-sequence andcompared to its manual transcription.
Over85% phoneme recognition accuracy is demon-strated for speaker-dependent learning fromfluent, large-vocabulary speech.1 Automatic Discovery of Phone(me)sStatistical models learnt from data are extensivelyused in modern automatic speech recognition (ASR)systems.
Transcribed speech is used to estimate con-ditional models of the acoustics given a phoneme-sequence.
The phonemic pronunciation of wordsand the phonemes of the language, however, arederived almost entirely from linguistic knowledge.In this paper, we investigate whether the phonemesmay be learnt automatically from the speech signal.Automatic learning of phoneme-like units has sig-nificant implications for theories of language ac-quisition in babies, but our considerations here aresomewhat more technological.
We are interested indeveloping ASR systems for languages or dialects?
This work was partially supported by National ScienceFoundation Grants No?IIS-0534359 and OISE-0530118.for which such linguistic knowledge is scarce ornonexistent, and in extending ASR techniques torecognition of signals other than speech, such as ma-nipulative gestures in endoscopic surgery.
Hence analgorithm for automatically learning an inventory ofintermediate symbolic units?intermediate relativeto the acoustic or kinematic signal on one end andthe word-sequence or surgical act on the other?isvery desirable.Except for some early work on isolated word/digitrecognition (Paliwal and Kulkarni, 1987; Wilponet al, 1987, etc), not much attention has beenpaid to automatic derivation of sub-word units fromspeech, perhaps because pronunciation lexicons arenow available1 in languages of immediate interest.What has been investigated is automatically learn-ing allophonic variations of each phoneme due toco-articulation or contextual effects (Takami andSagayama, 1992; Fukada et al, 1996); the phonemeinventory is usually assumed to be known.The general idea in allophone learning is to be-gin with an inventory of only one allophone perphoneme, and incrementally refine the inventory tobetter fit the speech signal.
Typically, each phonemeis modeled by a separate HMM.
In early stages ofrefinement, when very few allophones are available,it is hoped that ?similar?
allophones of a phonemewill be modeled by shared HMM states, and thatsubsequent refinement will result in distinct statesfor different allophones.
The key therefore is to de-vise a scheme for successive refinement of a modelshared by many allophones.
In the HMM setting,this amounts to simultaneously refining the topol-ogy and the model parameters.
A successive statesplitting (SSS) algorithm to achieve this was pro-posed by Takami and Sagayama (1992), and en-1See http://www.ldc.upenn.edu/Catalog/byType.jsp165hanced by Singer and Ostendorf (1996).
Improve-ments in phoneme recognition accuracy using thesederived allophonic models over phonemic modelswere obtained.In this paper, we investigate directly learning theallophone inventory of a language from speech with-out recourse to its phoneme set.
We begin with aone-state HMM for all speech sounds and modifythe SSS algorithm to successively learn the topol-ogy and parameters of HMMs with even larger num-bers of states.
States sequences through this HMMare expected to correspond to allophones.
The mostlikely state-sequence for a speech segment is inter-preted as an ?allophonic labeling?
of that speech bythe learnt model.
Performance is measured by map-ping the resultant state-sequence to phonemes.One contribution of this paper is a significant im-provement in the efficacy of the SSS algorithm asdescribed in Section 2.
It is based on observingthat the improvement in the goodness of fit by upto two consecutive splits of any of the current HMMstates can be evaluated concurrently and efficiently.Choosing the best subset of splits from among theseis then cast as a constrained knapsack problem, towhich an efficient solution is devised.
Another con-tribution of this paper is a method to evaluate theaccuracy of the resulting ?allophonic labeling,?
asdescribed in Section 3.
It is demonstrated that ifa small amount of phonetically transcribed speechis used to learn a Markov (bigram) model of state-sequences that arise from each phone, an evalua-tion tool results with which we may measure phonerecognition accuracy, even though the HMM labelsthe speech signal not with phonemes but merely astate-sequence.
Section 4 presents experimental re-sults, where the performance accuracies with differ-ent learning setups are tabulated.
We also see how aslittle as 5 minutes of speech is adequate for learningthe acoustic units.2 An Improved and Fast SSS AlgorithmThe improvement of the SSS algorithm of Takamiand Sagayama (1992), renamed ML-SSS by Singerand Ostendorf (1996), proceeds roughly as follows.1.
Model all the speech2 using a 1-state HMMwith a diagonal-covariance Gaussian.
(N=1.
)2Note that the original application of SSS was for learningFigure 1: Modified four-way split of a state s.2.
For each HMM state s, compute the gain in log-likelihood (LL) of the speech by either a con-textual or a temporal split of s into two statess1 and s2.
Among the N states, select and andsplit the one that yields the most gain in LL.3.
If the gain is above a threshold, retain the splitand set N = N + 1; furthermore, if N is lessthan desired, re-estimate all parameters of thenew HMM, and go to Step 2.Note that the key computational steps are the for-loop of Step 2 and the re-estimation of Step 3.Modifications to the ML-SSS Algorithm: Wemade the following modifications that are favorablein terms of greater speed and larger search space,thereby yielding a gain in likelihood that is poten-tially greater than the original ML-SSS.1.
Model all the speech using a 1-state HMM witha full-covariance Gaussian density.
Set N = 1.2.
Simultaneously replace each state s of theHMM with the 4-state topology shown in Fig-ure 1, yielding a 4N -state HMM.
If the state shad parameters (?s,?s), then means of its 4-state replacement are ?s1 = ?s?
?
= ?s4 and?s2 = ?s +?
= ?s3 , with ?
= ?
?v?, where ?
?and v?
are the principal eigenvalue and eigen-vector of ?s and 0 <  1 is typically 0.2.3.
Re-estimate all parameters of this (overgrown)HMM.
Gather the Gaussian sufficient statisticsfor each of the 4N states from the last passof re-estimation: the state occupancy pisi .
Thesample mean ?si , and sample covariance ?si .4.
Each quartet of states (see Figure 1) that re-sulted from the same original state s can bethe allophonic variations of a phoneme; hence the phrase ?allthe speech?
meant all the speech corresponding separately toeach phoneme.
Here it really means all the speech.166merged back in different ways to produce 3, 2or 1 HMM states.
There are 6 ways to end upwith 3 states, and 7 to end up with 2 states.
Re-tain for further consideration the 4 state split ofs, the best merge back to 3 states among the 6ways, the best merge back to 2 states among the7 ways, and the merge back to 1 state.5.
Reduce the number of states from 4N toN+?by optimally3 merging back quartets that causethe least loss in log-likelihood of the speech.6.
Set N = N + ?.
If N is less than the desiredHMM size, retrain the HMM and go to Step 2.Observe that the 4-state split of Figure 1 permits aslight look-ahead in our scheme in the sense that thegoodness of a contextual or temporal split of two dif-ferent states can be compared in the same iterationwith two consecutive splits of a single state.
Also,the split/merge statistics for a state are gathered inour modified SSS assuming that the other states havealready been split, which facilitates consideration ofconcurrent state splitting.
If s1, .
.
.
, sm are mergedinto s?, the loss of log-likelihood in Step 4 is:d2m?i=1pisi log |?s?| ?d2m?i=1pisi log |?si | , (1)where ?s?
=?mi=1 pisi(?si + ?si?
?si)?mi=1 pisi?
?s??
?s?.Finally, in selecting the best ?
states to add to theHMM, we consider many more ways of splitting theN original states than SSS does.
E.g.
going up fromN = 6 toN+?
= 9 HMM states could be achievedby a 4-way split of a single state, a 3-way split of onestate and 2-way of another, or a 2-way split of threedistinct states; all of them are explored in the processof merging from 4N = 24 down to 9 states.
Yet, likeSSS, no original state s is permitted to merge withanother original state s?.
This latter restriction leadsto an O(N5) algorithm for finding the best states tomerge down4.
Details of the algorithm are ommitedfor the sake of brevity.In summary, our modified ML-SSS algorithm canleap-frog by ?
states at a time, e.g.
?
= ?N , com-pared to the standard algorithm, and it has the benefitof some lookahead to avoid greediness.3This entails solving a constrained knapsack problem.4This is a restricted version of the 0-1 knapsack problem.3 Evaluating the Goodness of the LabelsThe HMM learnt in Section 2 is capable of assign-ing state-labels to speech via the Viterbi algorithm.Evaluating whether these labels are linguisticallymeaningful requires interpreting the labels in termsof phonemes.
We do so as follows.Some phonetically transcribed speech is labeledwith the learnt HMM, and the label sequences cor-responding to each phone segment are extracted.Since the HMM was learnt from unlabeled speech,the labels and short label-sequences usually corre-spond to allophones, not phonemes.
Therefore, foreach triphone, i.e.
each phone tagged with its left-and right-phone context, a simple bigram model oflabel sequences is estimated.
An unweighted ?phoneloop?
that accepts all phone sequences is created,and composed with these bigram models to cre-ate a label-to-phone transducer capable of mappingHMM label sequences to phone sequences.Finally, the test speech (not used for HMM learn-ing, nor for estimating the bigram model) is treatedas having been ?generated?
by a source-channelmodel in which the label-to-phone transducer is thesource?generating an HMM state-sequence?andthe Gaussian densities of the learnt HMM states con-stitute the channel?taking the HMM state-sequenceas the channel input and generating the observedspeech signal as the output.
Standard Viterbi decod-ing determines the most likely phone sequence forthe test speech, and phone accuracy is measured bycomparison with the manual phonetic transcription.4 Experimental Results4.1 Impact of the Modified State SplittingThe ML-SSS procedure estimates 2N differentN+1-state HMMs to grow from N to N+1 states.Our procedure estimates one 4N state HMM togrow to N+?, making it hugely faster for large N .Table 1 compares the log-likelihood of the train-ing speech for ML-SSS and our procedure.
The re-sults validate our modifications, demonstrating thatat least in the regimes feasible for ML-SSS, there isno loss (in fact a tiny gain) in fitting the speech data,and a big gain in computational effort5.5ML-SSS with ?=1 was impractical beyond N=22.167# of states SSS (?
= 1) ?
= 3 ?
= N8 -7.14 -7.13 -7.1310 -7.08 -7.06 -7.0622 -6.78 -6.76 N/A40 N/A -6.23 -6.20Table 1: Aggressive state splitting does not cause anydegradation in log-likelihood relative to ML-SSS.4.2 Unsupervised Learning of Sub-word UnitsWe used about 30 minutes of phonetically tran-scribed Japanese speech from one speaker6 providedby Maekawa (2003) for our unsupervised learningexperiments.
The speech was segmented via silencedetection into 800 utterances, which were furtherpartitioned into a 24-minute training set (80%) and6-minute test set (20%).Our first experiment was to learn an HMM fromthe training speech using our modified ML-SSS pro-cedure; we tried N = 22, 70 and 376.
For each N ,we then labeled the training speech using the learntHMM, used the phonetic transcription of the train-ing speech to estimate label-bigram models for eachtriphone, and built the label-to-phone transducer asdescribed in Section 3.
We also investigated (i) usingonly 5 minutes of training speech to learn the HMM,but still labeling and using all 24 minutes to buildthe label-to-phone transducer, and (ii) setting aside5 minutes of training speech to learn the transducerand using the rest to learn the HMM.
For each learntHMM+transducer pair, we phonetically labeled thetest speech.The results in the first column of Table 2 suggestthat the sub-word units learnt by the HMM are in-deed interpretable as phones.
The second columnsuggests that a small amount of speech (5 minutes)may be adequate to learn these units consistently.The third column indicates that learning how to mapthe learnt (allophonic) units to phones requires rela-tively more transcribed speech.4.3 Inspecting the Learnt Sub-word UnitsThe most frequent 3-, 4- and 5-state sequences in theautomatically labeled speech consistently matchedparticular phones in specific articulatory contexts, as6We heeded advice from the literature indicating that au-tomatic methods model gross channel- and speaker-differencesbefore capturing differences between speech sounds.HMM 24 min 5 min 19 minlabel-to-phone 24 min 24 min 5 min27 states 71.4% 70.9% 60.2%70 states 84.4% 84.7% 75.8%376 states 87.2% 86.8% 76.6%Table 2: Phone recognition accuracy for different HMMsizes (N), and with different amounts of speech used tolearn the HMM labeler and the label-to-phone transducer.shown below, i.e.
the HMM learns allophones.HMM labels L-contxt Phone R-contxt11, 28, 32 vowel t [e|a|o]15, 17, 2 [g|k] [u|o] [?
]3, 17, 2 [k|t|g|d] a [k|t|g|d]31, 5, 13, 5 vowel [s|sj|sy] vowel17, 2, 31, 11 [g|t|k|d] [a|o] [t|k]3, 30, 22, 34 [?]
a silence6, 24, 8, 15, 22 [?]
o silence4, 3, 17, 2, 21 [k|t] a [k|t]4, 17, 24, 2, 31 [s|sy|z] o [t|d][t|d] o [s|sy|z]For instance, the label sequence 3, 17, 2, corre-sponds to an ?a?
surrounded by stop consonants{t, d, k, g}; further restricting the sequence to4, 3, 17, 2, 21, results in restricting the context to theunvoiced stops {t, k}.
That such clusters are learntwithout knowledge of phones is remarkable.ReferencesT.
Fukada, M. Bacchiani, K. K. Paliwal, and Y. Sagisaka.1996.
Speech recognition based on acoustically de-rived segment units.
In ICSLP, pages 1077?1080.K.
Maekawa.
2003.
Corpus of spontaneous japanese:its design and evaluation.
In ISCA/IEEE Workshop onSpontaneous Speech Processing and Recognition.K.
K. Paliwal and A. M. Kulkarni.
1987.
Segmenta-tion and labeling using vector quantization and its ap-plication in isolated word recognition.
Journal of theAcoustical Society of India, 15:102?110.H.
Singer and M. Ostendorf.
1996.
Maximum likelihoodsuccessive state splitting.
In ICASSP, pages 601?604.J.
Takami and S. Sagayama.
1992.
A successive statesplitting algorithm for efficient allophone modeling.In ICASSP, pages 573?576.J.
G. Wilpon, B. H. Juang, and L. R. Rabiner.
1987.
Aninvestigation on the use of acoustic sub-word units forautomatic speech recognition.
In ICASSP, pages 821?824.168
