Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 13?18,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsAn Open Source Toolkit for Quantitative Historical LinguisticsJohann-Mattis ListResearch Center Deutscher SprachatlasPhilipps-University Marburgmattis.list@uni-marburg.deSteven MoranDepartment of General LinguisticsUniversity of Zurichsteven.moran@uzh.chAbstractGiven the increasing interest and devel-opment of computational and quantitativemethods in historical linguistics, it is im-portant that scholars have a basis for doc-umenting, testing, evaluating, and shar-ing complex workflows.
We present anovel open-source toolkit for quantitativetasks in historical linguistics that offersthese features.
This toolkit also servesas an interface between existing softwarepackages and frequently used data for-mats, and it provides implementations ofnew and existing algorithms within a ho-mogeneous framework.
We illustrate thetoolkit?s functionality with an exemplaryworkflow that starts with raw languagedata and ends with automatically calcu-lated phonetic alignments, cognates andborrowings.
We then illustrate evaluationmetrics on gold standard datasets that areprovided with the toolkit.1 IntroductionSince the turn of the 21st century, there has been anincreasing amount of research that applies compu-tational and quantitative approaches to historical-comparative linguistic processes.
Among theseare: phonetic alignment algorithms (Kondrak,2000; Proki?
et al 2009), statistical tests for ge-nealogical relatedness (Kessler, 2001), methodsfor phylogenetic reconstruction (Holman et al2011; Bouckaert et al 2012), and automatic de-tection of cognates (Turchin et al 2010; Steiner etal., 2011), borrowings (Nelson-Sathi et al 2011),and proto-forms (Bouchard-C?t?
et al 2013).In contrast to traditional approaches to languagecomparison, quantitative methods are often em-phasized as advantageous with regard to objectiv-ity, transparency and replicability of results.
Itis striking then that given the multitude of newapproaches, very few are publicly available asexecutable code.
Thus in order to replicate astudy, researchers have to rebuild workflows frompublished descriptions and reimplement their ap-proaches and algorithms.
These challenges makethe replication of results difficult, or even impos-sible, and they hinder not only the evaluation andcomparison of existing algorithms, but also the de-velopment of new approaches that build on them.Another problem is that quantitative approachesthat have been released as software are largely in-compatible with each other and they show greatdifferences in regard to their input and out for-mats, application range and flexibility.1 Given thebreadth of research questions involved in deter-mining language relatedness, this is not surprising.Furthermore, the linguistic datasets upon whichmany analyses and tools are based are only ?
if atall ?
available in disparate formats that need man-ual or semi-automatic re-editing before they canbe used as input elsewhere.
Scholars who wantto analyze a dataset with different approaches of-ten have to (time-consumingly) convert it into var-ious input formats and they have to familiarizethemselves with many different kinds of software.As a result, errors may occur during data conver-sion processes and the output from different toolsmust also be converted into a comparable format.For the comparison of different output formats or1There is the STARLING database program for lexicosta-tistical and glottochronological analyses (Starostin, 2000).TheRug/L04 software aligns sound sequences and calculatesphonetic distances using the Levensthein distance (Kleiweg,2009; Levenshtein, 1966).
The ASJP-Software also com-putes the Levenshtein distance (Holman et al 2011), but itsresults are based on previously executed phonetic analyses.The ALINE software carries out pairwise alignment analy-ses (Kondrak, 2000).
There are also software packages fromevolutionary biology, which are adapted for linguistic pur-poses, such as MrBayes (Ronquist and Huelsenbeck, 2003),PHYLIP (Felsenstein, 2005), and SplitsTree (Huson, 1998).13for the evaluation of competing quantitative ap-proaches, gold standard datasets are desirable.Towards a solution to these problems, we havedeveloped a toolkit that (a) serves as an interfacebetween existing software packages and data for-mats frequently used in quantitative approaches,(b) provides high-quality implementations of newand existing approaches within a homogeneousframework, and (c) offers a solid basis for test-ing, documenting, evaluating, and sharing com-plex workflows in quantitative historical linguis-tics.
We call this open source toolkit LingPy.2 LingpyLingPy is written in Python3 and is freely avail-able online.2 The Lingpy website contains an API,documentation, tutorials, example scripts, work-flows, and datasets that can be used for training,testing, and comparing results from different algo-rithms.
We use Python because it is flexible andobject-oriented, it is easy to write C extensionsfor scientific computing, and it is approachableto non-programmers (Knight et al 2007).
Apartfrom a large number of different functions for com-mon automatic tasks, LingPy offers specific mod-ules for implementing general workflows that areused in historical linguistics and which partiallymimic the basic aspects of the traditional compar-ative method (Trask, 2000, 64-67).
Figure 1 il-lustrates the interaction between different modulesalong with the data they produce.
In the followingsubsections, these modules will be introduced inthe order of a typical workflow to illustrate the ba-sic capabilities of the LingPy toolkit in more detail.2.1 Input FormatsThe basic input format read by LingPy is a tab-delimited text file in which the first line (theheader) indicates the values of the columns and allwords are listed in the following rows.
The for-mat is very flexible.
No specific order of columnsor rows is required.
Any additional data can bespecified by the user, as long as it is in a separatecolumn.
Each row represents a word that has to becharacterized by a minimum of four values that aregiven in separate columns: (1) ID, an integer thatis used to uniquely identify the word during calcu-lations, (2) CONCEPT, a gloss which indicates themeaning of the word and which is used to align thewords semantically, (3) WORD, the orthographic2http://lingpy.orgRaw dataTokenizeddataOrthographic parsingCognatesetsAlignmentsCognatedetectionPhoneticalignment (PA)OutputformatsPAPatchycognatesetsborrowingdetectionPAFigure 1: Basic Workflow in LingPyrepresentation of the word,3 and (4) TAXON, thename of the language (or dialect) inwhich thewordoccurs.
Basic output formats are essentially thesame, the difference being that the results of cal-culations are added as separate columns.
Table 1illustrates the basic structure of the input formatfor a dataset covering 325 concepts translated into18 Dogon language varieties taken from the Do-gon comparative lexical spreadsheet (Heath et al2013).42.2 Parsing and Unicode HandlingGiven a dataset in the basic LingPy input for-mat, the first step towards sound-based normal-ization for automatically identifying cognates andsound changes with quantitative methods is toparse words into tokens.
Orthographic tokeniza-tion is a non-trivial task, but it is needed to at-3By this we mean a textual representation of the word,whether in a document or language-specific orthography orin some form of broad or narrow transcription, etc.4This tokenized dataset and analyses that are discussed in thiswork are available for download from the LingPy website.14ID CONCEPT WORD TAXON... ... ... ...1239 file (tool) ki?:ra?
Toro_Tegu1240 file (tool) di?:s?
: Ben_Tey1241 file (tool) ki?r?l Bankan_Tey1242 file (tool) di?:ju?
Jamsay... ... ... ...1249 file (tool) bi?mbu?
Tommo_So1250 file (tool) bi?mbu?
Dogul_Dom1251 file (tool) di?:zu?
Yanda_Dom1252 file (tool) bi?:mbye?
Mombo... ... ... ...Table 1: Basic Input Format of LingPytain interoperability across different orthographiesor transcription systems and to enable the com-parative analysis of languages.
LingPy includesa parser that takes as input a dataset and an op-tional orthography profile, i.e.
a description ofthe Unicode code points, characters, graphemesand orthographic rules that are needed to ade-quately model a writing system for a language va-riety as described in a particular document (Moran,2012, 331).
The LingPy parser first normalizes allstrings into UnicodeNormalization FormD,whichdecomposes all character sequences and reordersthem into one canonical order.
This step is nec-essary because sequences of Unicode charactersmay differ in their visual and logical orders.
Next,if no orthography profile is specified, the parserwill use a regular expression match \X for Uni-code grapheme clusters, i.e.
combining charactersequences typified by a base character followed byone or more Combing Diacritical Marks.
How-ever, another layer of tokenization is usually re-quired to match linguistic graphemes, or what Uni-code calls ?tailored grapheme clusters?.
Table 2 il-lustrates the different technological and linguisticlevels involved in orthographic parsing.5code points t s h o ?
?
?
?
?
?
s h i?characters?
t s h o???
s h igraphemes tsh o???
sh iTable 2: Tokens for the string <tsh??
?shi>So, given the dataset illustrated in Table 1 andan orthography profile that defines the phone-mic units in the Dogon comparative lexicon, the5Note that even when a linguist transcribes a word with theInternational Phonetic Alphabet (IPA; a transcription systemwith one-to-one symbol-to-sound correspondences), explicitdefinitions for phonemes are needed because some IPA dia-critics are encoded as Unicode Spacing Modifier Letters, i.e.characters that are not specified as how they combine with abase character, such as aspiration.LingPy parser produces the IPA tokenized outputshown in Table 3.ID ... WORD TOKENS ...... ... ... ... ...1239 ...
ki?:ra?
# k i?
: r a?
# ...1240 ...
di?:s?
: # d i?
: s ?
: # ...1241 ... ki?r?l # k i?
r ?
l # ...1242 ...
di?:ju?
# d i?
: ?
u?
# ...... ... ... ... ...1249 ... bi?mbu?
# b i?
m b u?
# ...1250 ... bi?mbu?
# b i?
m b u?
# ...1251 ...
di?:zu?
# d i?
: z u?
# ...1252 ...
bi?:mbye?
# b i?
: m b j e?
# ...... ... ... ... ...Table 3: Orthographic Parsing in LingPy2.3 Phonetic AlignmentsAlthough less common in traditional historical lin-guistics, phonetic alignment plays a crucial rolein automatic approaches, with alignment analysesbeing currently used in many different subfields,such as dialectology (Proki?
et al 2009), phyloge-netic reconstruction (Holman et al 2011) and cog-nate detection (List, 2012a).
Furthermore, align-ment analyses are very useful for data visualiza-tion, since they directly show which sound seg-ments correspond in cognate words.LingPy offers implementations for many dif-ferent approaches to pairwise and multiple pho-netic alignment.
Among these, there are stan-dard approaches that are directly taken from evo-lutionary biology and can be applied to linguisticdata with only slight modifications, such as theNeedleman-Wunsch algorithm (Needleman andWunsch, 1970) and the Smith-Waterman algo-rithm (Smith and Waterman, 1981).
Furthermore,there are novel approaches that use more com-plex sequence models in order to meet linguistic-specific requirements, such as the Sound-Class-based phonetic Alignment (SCA) method (List,2012b).
Figure 2 shows a plot of the multi-ple alignment of the counterparts of the concept?stool?
in eight Dogon languages.
The colorscheme for the sound segments follows the soundclass distinction of Dolgopolsky (1964).2.4 Automatic Cognate DetectionThe identification of cognates plays an impor-tant role in both traditional and quantitative ap-proaches in historical linguistics.
Most quantita-tive approaches dealing with phylogenetic recon-struction are based on previously identified cog-nate sets distributed over the languages being in-15Taxon AlignmentBen_Tey t u?
?
g u?
r - u?
mBankan_Tey t u?
?
g u?
r - u?
-Jamsay t u?
?
- u?
r?
- u?
-Perge_Tegu t u?
?
- u?
r?
- u?
mGourou t u?
m - u?
r - u?
-Yorno_So t ??
?
- ??
- - - -Tommo_So t u?
?
g u?
r - u?
-Tebul_Ure t u?
?
g u?
r g ??
-XXX XXX XXX XXX XXX XXX XXX XXX XXXFigure 2: Multiple Phonetic Alignment in LingPyvestigated (Bouckaert et al 2012; Bouchard-C?t?et al 2013).
Since the traditional approach to cog-nate detection within the framework of the com-parative method is very time-consuming and diffi-cult to evaluate for the non-expert, automatic ap-proaches to cognate detection can play an impor-tant role in objectifying phylogenetic reconstruc-tions.Currently, LingPy offers four alternative ap-proaches to cognate detection in multilingualwordlists.
Themethod by Turchin et al(2010) em-ploys sound classes as proposed by Dolgopolsky(1964) and assigns words that match in their firsttwo consonant classes to the same cognate set.
TheNED method calculates the normalized edit dis-tance between words and groups them into cognatesets using a flat cluster algorithm.6 The SCA andthe LexStat methods (List, 2012a) use the samestrategy for clustering, but the distances for theSCA method are calculated with help of the SCAalignment method (List, 2012b), and the distancesfor the LexStat method are derived from previ-ously identified regular sound correspondences.Table 4 shows a small section of the results fromthe LexStat analysis of the Dogon data.
As shown,LingPy follows the STARLING approach in dis-playing cognate judgments by assigning cognatewords the same cognate ID (COGID).
In Table4, the words judged to be cognate are shaded inthe same color.
The full results are posted on theLingPy website.2.5 Automatic Borrowing DetectionAutomatic approaches for borrowing detectionare still in their infancy in historical linguistics.LingPy provides a full reimplementation (alongwith specifically linguistic modifications) of theminimal lateral network (MLN) approach (Nelson-Sathi et al 2011).
This approach searches for cog-nate sets which are not compatible with a given ref-6The normalized edit distance is calculated by dividing theedit distance (Levenshtein, 1966) by the length of the smallersequence, see Holman et al(2011) for details.ID CONCEPT WORD TAXON COGID... ... ... ... ...1239 file (tool) ki?:ra?
Toro_Tegu 681240 file (tool) di?:s?
: Ben_Tey 691241 file (tool) ki?r?l Bankan_Tey 681242 file (tool) di?:ju?
Jamsay 69... ... ... ... ...1249 file (tool) bi?mbu?
Tommo_So 701250 file (tool) bi?mbu?
Dogul_Dom 701251 file (tool) di?:zu?
Yanda_Dom 691252 file (tool) bi?:mbye?
Mombo 70... ... ... ... ...Table 4: Cognate Detection in LingPyerence tree topology.
Incompatible (patchy) cog-nate sets often point to either borrowings or wrongcognate assessments in the data.
The results canbe visualized by connecting all taxa of the refer-ence tree for which patchy cognate sets can be in-ferred with lateral links.
In Figure 3, the methodhas been applied again to the Dogon dataset.
Cog-nate judgments for this analysis were carried outwith help of LingPy?s LexStat method.
The treetopology was calculated using MrBayes.2.6 Output FormatsThe output formats supported by LingPy can be di-vided into three different classes.
The first classconsists of text-based formats that can be usedfor manual correction and inspection by import-ing the data into spreadsheet programs, or sim-ply editing and reviewing the results in a texteditor.
The second class consists of specificformats for third-party toolkits, such as PHY-LIP, SplitsTree, MrBayes, or STARLING.
LingPycurrently offers support for PHYLIP?s distancecalculations (DST-format), for tree-representation(Newick-format), for complex representations ofcharacter data (Nexus-format), and for the im-port into STARLING databases (CSV with STAR-LING markup).
The third class consists of newapproaches to the visualization of phonetic align-ments, cognate sets, and phylogenetic networks.In fact, all plots in this paper were created withLingPy?s output formats.3 EvaluationIn order to improve the performance of quantita-tive approaches, it is of crucial importance to testand evaluate them.
Evaluation is usually done bycomparing how well a given approach performson a reference dataset, i.e.
a gold standard, wherethe results of the analysis are known in advance.LingPy comes with a module for the evaluation of16Ben TeyTomo Kan DiangassagouToro TeguTebul UreJamsay MondoroYanda DomNangaTiranigeBankan TeyJamsayPerge TeguGourouBunogeDogul DomMomboYorno SoTommo SoTogo Kan1919InferredLinksFigure 3: Borrowing Detection in LingPybasic tasks in historical linguistics, such as pho-netic alignment and cognate detection.
This mod-ule offers both common evaluation measures thatare used to assess the accuracy of the respectivemethods and gold standard datasets encoded in theLingPy input format.In Figure 4, the performance of the four above-mentioned approaches to automatic cognate de-tection are compared with the gold standard cog-nate judgments of a dataset covering 207 con-cepts translated into 20 Indo-European languagestaken from the Indo-European Lexical Cognacy(IELex) database (Bouckaert et al 2012).7 Thepair scores, implemented in LingPy after the de-scription in Bouchard-C?t?
et al(2013), were usedas an evaluation measure.
For all approaches wechose the respective thresholds that tend to yieldthe best results on all of the gold standards.
Asshown in Figure, both the SCA and LexStat meth-ods show a higher accuracy than the Turchin andNED methods, with LexStat slightly outperform-ing SCA.
However, the generally bad performance7Gold standard here means that the cognate judgments werecarried out manually by the compilers of the IELex database.of all approaches on this dataset shows that there isa clear need for improving automatic cognate de-tection approaches, especially in cases of remoterelationship, such as Indo-European.Precision Recall F-Score0.20.30.40.50.60.70.8 TurchinNEDSCALexStatFigure 4: Evaluating Cognate Detection Methods4 ConclusionQuantitative approaches in historical linguisticsare still in their infancy, far away from being ableto compete with the intuition of trained historical17linguists.
The toolkit we presented is a first at-tempt to close the gap between quantitative andtraditional methods by providing a homogeneousframework that serves as an interface between ex-isting packages and at the same time provides high-quality implementations of new approaches.ReferencesA.
Bouchard-C?t?, D. Hall, T. L. Griffiths, andD.
Klein.
2013.
Automated reconstruction of an-cient languages using probabilistic models of soundchange.
PNAS, 110(11):4224?4229.R.
Bouckaert, P. Lemey, M. Dunn, S. J. Greenhill,A.
V. Alekseyenko, A. J. Drummond, R. D. Gray,M.
A. Suchard, and Q. D. Atkinson.
2012.
Map-ping the origins and expansion of the Indo-Europeanlanguage family.
Science, 337(6097):957?960, Aug.A.
B. Dolgopolsky.
1964.
Gipoteza drevnej?ego rod-stva jazykovych semej Severnoj Evrazii s verojatnos-tej to?ky zrenija [A probabilistic hypothesis concern-ing the oldest relationships among the language fam-ilies of Northern Eurasia].
Voprosy Jazykoznanija,2:53?63.J.
Felsenstein.
2005.
Phylip (phylogeny inferencepackage) version 3.6.
Distributed by the author.
De-partment of Genome Sciences, University of Wash-ington, Seattle.J.
Heath, S Moran, K. Prokhorov, L. McPherson, andB.
Canslter.
2013.
Dogon comparative lexicon.URL: http://www.dogonlanguages.org.E.
W. Holman, C. H. Brown, S. Wichmann, A. M?ller,V.
Velupillai, H. Hammarstr?m, S. Sauppe, H. Jung,D.
Bakker, P. Brown, O. Belyaev, M. Urban,R.
Mailhammer, J.-M.
List, and D. Egorov.
2011.Automated dating of the world?s language familiesbased on lexical similarity.
Current Anthropology,52(6):841?875.D.
H. Huson.
1998.
SplitsTree.
Analyzing and visu-alizing evolutionary data.
Bioinformatics, 14(1):68?73.B.
Kessler.
2001.
The significance of word lists.
Sta-tistical tests for investigating historical connectionsbetween languages.
CSLI Publications, Stanford.P.
Kleiweg.
2009.
RuG/L04.
Software for dialecto-metrics and cartography.
Distributed by the Author.Rijksuniversiteit Groningen.
Faculteit der Letteren,September.R.
Knight, P. Maxwell, A. Birmingham, J. Carnes,J.
G. Caporaso, B. Easton, M. Eaton, M. Hamady,H.
Lindsay, Z. Liu, C. Lozupone, D. McDonald,M.
Robeson, R. Sammut, S. Smit, M. Wakefield,J.
Widmann, S. Wikman, S. Wilson, H. Ying, andG.
Huttley.
2007.
PyCogent.
A toolkit for makingsense from sequence.
Genome Biology, 8(8):R171.G.
Kondrak.
2000.
A new algorithm for the align-ment of phonetic sequences.
In Proceedings ofthe 1st North American chapter of the Associationfor Computational Linguistics conference, NAACL2000, pages 288?295, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.V.
I. Levenshtein.
1966.
Binary codes capable of cor-recting deletions, insertions, and reversals.
SovietPhysics Doklady, 10(8):707?710.J.-M.
List.
2012a.
LexStat.
Automatic detection ofcognates in multilingual wordlists.
InProceedings ofthe EACL 2012 Joint Workshop of LINGVIS & UN-CLH, pages 117?125.
Association for ComputationalLinguistics.J.-M.
List.
2012b.
SCA.
Phonetic alignment basedon sound classes.
In M. Slavkovik and D. Las-siter, editors, New directions in logic, language, andcomputation, number 7415 in LNCS, pages 32?51.Springer, Berlin and Heidelberg.S.
Moran.
2012.
Phonetics information base and lexi-con.
Ph.D. thesis, University of Washington.S.
B. Needleman and C. D. Wunsch.
1970.
A genemethod applicable to the search for similarities inthe amino acid sequence of two proteins.
Journalof Molecular Biology, 48:443?453, July.S.
Nelson-Sathi, J.-M.
List, H. Geisler, H. Fangerau,R.
D. Gray, W. Martin, and T. Dagan.
2011.
Net-works uncover hidden lexical borrowing in Indo-European language evolution.
Proceedings of theRoyal Society B, 278(1713):1794?1803.J.
Proki?, M. Wieling, and J. Nerbonne.
2009.
Multi-ple sequence alignments in linguistics.
In Proceed-ings of the EACL 2009 Workshop on Language Tech-nology and Resources for Cultural Heritage, SocialSciences, Humanities, and Education, pages 18?25.Association for Computational Linguistics.F.
Ronquist and J. P. Huelsenbeck.
2003.
MrBayes 3.Bayesian phylogenetic inference under mixed mod-els.
Bioinformatics, 19(12):1572?1574.T.
F. Smith and M. S. Waterman.
1981.
Identifica-tion of common molecular subsequences.
Journal ofMolecular Biology, 1:195?197.S.
A. Starostin.
2000.
The STARLING database pro-gram.
URL: http://starling.rinet.ru.L.
Steiner, P. F. Stadler, and M. Cysouw.
2011.A pipeline for computational historical linguistics.Language Dynamics and Change, 1(1):89?127.R.
L. Trask.
2000.
The dictionary of historicaland comparative linguistics.
Edinburgh UniversityPress, Edinburgh.P.
Turchin, I. Peiros, and M. Gell-Mann.
2010.
An-alyzing genetic connections between languages bymatching consonant classes.
Journal of LanguageRelationship, 3:117?126.18
