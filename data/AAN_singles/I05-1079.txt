Exploiting Lexical Conceptual Structurefor Paraphrase GenerationAtsushi Fujita1, Kentaro Inui2, and Yuji Matsumoto21 Graduate School of Informatics, Kyoto Universityfujita@pine.kuee.kyoto-u.ac.jp2 Graduate School of Information Science, Nara Institute of Science and Technology{inui, matsu}@is.naist.jpAbstract.
Lexical Conceptual Structure (LCS) represents verbs as semanticstructures with a limited number of semantic predicates.
This paper attempts toexploit how LCS can be used to explain the regularities underlying lexical andsyntactic paraphrases, such as verb alternation, compound word decomposition,and lexical derivation.
We propose a paraphrase generation model which trans-forms LCSs of verbs, and then conduct an empirical experiment taking the para-phrasing of Japanese light-verb constructions as an example.
Experimental resultsjustify that syntactic and semantic properties of verbs encoded in LCS are usefulto semantically constrain the syntactic transformation in paraphrase generation.1 IntroductionAutomatic paraphrasing has recently been attracting increasing attention due to its po-tential in a broad range of natural language processing tasks.
For example, a system thatis capable of simplifying a given text, or showing the user several alternative expres-sions conveying the same content, would be useful for assisting a reader.There are several classes of paraphrase that exhibit a degree of regularity.
For exam-ple, paraphrasing associated with verb alternation, lexical derivation, compound worddecomposition, and paraphrasing of light-verb constructions (LVC(s)) all fall into suchclasses.
Examples1 (1) and (2) appear to exhibit the same transformation pattern, inwhich a compound noun is transformed into a verb phrase.
Likewise, paraphrases in-volving an LVC as in (3) and (4) (from [4]) have considerable similarities.
(1) s. His machine operation is very good.t.
He operates the machine very well.
(2) s. My son?s bat control is unskillful yet.t.
My son controls his bat poorly yet.
(3) s. Steven made an attempt to stop playing.t.
Steven attempted to stop playing.
(4) s. It had a noticeable effect on the trade.t.
It noticeably affected the trade.1 For each example, ?s?
and ?t?
denote an original sentence and its paraphrase, respectively.
Notethat our target language is Japanese.
English examples are used for an explanatory purpose.R.
Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
908?919, 2005.c?
Springer-Verlag Berlin Heidelberg 2005Exploiting Lexical Conceptual Structure for Paraphrase Generation 909However, the regularity we find in these examples is not so simple that it cannot becaptured only in syntactic terms.
For example, the transformation pattern as in (1) and(2) does not apply to another compound noun ?machine translation.?
We can also finda range of varieties in paraphrasing of LVCs as we describe in Section 3.In spite of this complexity, the regularity each paraphrase class exhibits were ex-plained by recent advances in lexical semantics, such as the Lexical Conceptual Struc-ture (LCS) [8] and the Generative Lexicon [17].
According to the LCS, for instance,a wide variety of paraphrases including word association within compounds, transitiv-ity alternation, and lexical derivation, were explained by means of the syntactic andsemantic properties of the verb involved.
The systematicity underlying such linguisticaccounts is intriguing also from the engineering viewpoint as it could enable us to takea more theoretically motivated but still practical approach to paraphrase generation.The issue we address in this paper is to empirically clarify (i) what types of regular-ities underlying paraphrases can be explained by means of lexical semantics and how,and (ii) how lexical semantics theories can be enhanced with feedback from practicaluse, namely, paraphrase generation.
We make an attempt to exploit the LCS among sev-eral lexical semantics frameworks, and propose a paraphrase generation model whichutilizes LCS combining with syntactic transformation.2 Lexical Conceptual Structure2.1 Basic FrameworkAmong several frameworks of lexical semantics, we focus on the Lexical ConceptualStructure (LCS) [8] due to the following reasons.
First, several studies [9,3,19] haveshown that the theory of the LCS provides a systematic explanation of semantic de-composition as well as syntax determines.
In particular, Kageyama [9] has shown thateven a simple typology of LCS can explain a wide variety of linguistic phenomena in-cluding word association within compounds, transitivity alternation, and lexical deriva-tion.
Second, large-scale LCS dictionaries have been developed through practical useon machine translation and compound noun analysis [3,19].
The LCS dictionary forEnglish [3] (4,163-verbs with 468 LCS types) was tailored based on a verb classifica-tion [12] with an expansion for the semantic role delivered to arguments.
For Japanese,Takeuchi et al [19] developed a 1,210-verbs LCS dictionary (with 12 LCS types) calledthe T-LCS dictionary, following Kageyama?s analysis [9].
In this paper, we make use ofthe current version of the T-LCS dictionary, because it provides a set of concrete rulesfor LCS assignment, which ensures the reliability of the dictionary.Examples of LCS in the T-LCS dictionary are shown in Table 1.
An LCS consistsof a combination of semantic predicates (?CONTROL,?
?BE AT,?
etc.)
and their argu-ment slots (x, y, and z).
Each argument slot corresponds to a semantic role, such as?Agent,?
?Theme,?
and ?Goal,?
depending on its surrounding semantic predicates.
Letus take ?yakusu (to translate)?
as an example.
The inner structure ?
[y BE AT z]?
de-notes the state of affairs where z (?Goal?)
indicates the state or physical location of y(?Theme?).
The predicate ?BECOME?
expresses a change of y.
In the case of examplephrase in Table 1, the change of the language of the book is represented.
The leftmost910 A. Fujita, K. Inui, and Y. MatsumotoTable 1.
Examples of LCSLCS for verb (example verb)example Japanese phrase[y BE AT z] (ichi-suru (to locate), sonzai-suru (to exist))gakkou-ga kawa-no chikaku-ni ichi-suru.school-NOM river-GEN near-DAT to locate-PRESThe school (Theme) locates near the river (Goal).
[BECOME [y BE AT z]] (houwa-suru (to become saturate), bunpu-suru (to be distributed))kono-hana-ga sekaiju-ni bunpu-suru.this flower-NOM all over the world-DAT to distribute-PRESThis flower (Theme) is distributed all over the world (Goal).
[x CONTROL [BECOME [y BE AT z]]] (yakusu (to translate), shoukai-suru (to introduce))kare-ga hon-o nihongo-ni yakusu.he-NOM book-ACC Japanese-DAT to translate-PRESHe (Agent) translates the book (Theme) into Japanese (Goal).
[x ACT ON y] (unten-suru (to drive), sousa-suru (to operate))kare-ga kikai-o sousa-suru.he-NOM machine-ACC to operate-PRESHe (Agent) operates the machine (Theme).
[y MOVE TO z] (ido-suru (to move), sen?i-suru (to propagate))ane-ga tonarimachi-ni ido-suru.my sister-NOM neighboring town-DAT to move-PRESMy sister (Theme) moves to a neighboring town (Goal).part ?
[x CONTROL .
.
.]?
denotes that the ?Agent?
causes the state change.
The differ-ence between ?BECOME BE AT?
and ?MOVE TO?
is underlying their telicity: the for-mer indicates telic, and thus the verb can be perfective, while the latter atelic.
Likewise,?CONTROL?
implicates a state change, while ?ACT ON?
merely denotes an action.
Thefollowing are examples of syntactic and semantic properties represented in LCS:?
Semantic role of argument (e.g.
?
[x CONTROL .
.
.]?
indicates x=?Agent?)?
Syntactic case particle pattern (e.g.
?
[y MOVE TO z]?
indicates y=NOM, z=DAT)?
Aspectual property (e.g.
?MOVE TO?
is atelic (?
?ket-tearu (to kick-PERF)?
), while?BECOME BE AT?
is telic (?oi-tearu (to place-PERF).?))?
Focus of statement(e.g.
x is focused in ?
[x CONTROL .
.
.
]?, while z in ?
[z BE WITH .
.
.]?)?
Semantic relations in lexical derivation?
transitivity alternation (?kowasu (to break (vt))?
?
?kowareru (to break (vi))?)?
lexical active-passive alternation (?oshieru (to teach)?
?
?osowaru (to betaught)?
)2.2 Disambiguation in LCS AnalysisIn principle, a verb is associated with more than one LCS if it has multiple senses.The mapping from syntactic case assignments to argument slots in LCS is also many-Exploiting Lexical Conceptual Structure for Paraphrase Generation 911to-many in general.
In the case of Japanese, the case particle ?ni?
tends to be highlyambiguous as demonstrated in (5).
(5) a. shuushin-jikan-o yoru-11ji-ni henkou-shita.bedtime-ACC 11 p.m.-DAT (complement) to change-PASTI changed my bedtime to 11 p.m.b.
yoru-11ji-ni yuujin-ni mail-o okut-ta.11 p.m.-DAT (adjunct) friends-DAT (complement) mail-ACC to send-PASTI sent a mail to my friends at 11 p.m.Resolution of these sorts of ambiguity is called semantic parsing and has been ac-tively studied by many researchers recently [6,2] as semantically annotated corpora andlexical resources such as the FrameNet [1] and the Proposition Bank [16] have becomeavailable.
Relying on the promising results of this trend of research, we do not addressthe issue of semantic parsing in this paper to focus our attention on the generation sideof the whole problem.3 Paraphrasing of Light-Verb ConstructionsIn this paper, we focus our discussion on one class of paraphrases, i.e., paraphrasing oflight-verb constructions (LVCs).
Sentence (6s) shows an example of an LVC.
An LVCis a verb phrase (?kandou-o atae-ta (made an impression),?
c.f., Figure 1) that consistsof a light-verb (?atae-ta (to give-PAST)?)
that syntactically governs a deverbal noun(?kandou (an impression)?).
A paraphrase of (6s) is shown in sentence (6t), where thedeverbal noun functions as the main verb with its verbalized form (?kandou-s-ase-ta (tobe impressed-CAUSATIVE-PAST)?).
(6) s. eiga-ga kare-ni saikou-no kandou-o atae-ta.film-NOM him-DAT supreme-GEN impression-ACC to give-PASTThe film made an supreme impression on him.t.
eiga-ga kare-o saikou-ni kandou-s-ase-ta.film-NOM him-ACC supreme-DAT to be impressed-CAUSATIVE-PASTThe film supremely impressed him.Example (6) indicates that we need an information to determine how the voice of targetsentence must be changed and how the case particles of the nominal elements must bereassigned.
These decisions depend not only on the syntactic and semantic attributes ofthe light-verb, but also on those of the deverbal noun [14].
LVC paraphrasing is thus anovel challenging material for exploiting LCS.Figure 1 demonstrates tree representations of source and target expressions involvedin LVC paraphrasing, taking (6) as an example.
To generate this type of paraphrase, weneed a computational model that is capable of the following operations:Change of the dependence: Change the dependences of the elements (a) and (b) dueto the elimination of the original modifiee, the light-verb.
This operation can bedone by just making them dependent on the resultant verb.912 A. Fujita, K. Inui, and Y. Matsumotosaikou-nii - ikare-or -eiga-gai -*kandou-o-kare-nir - i(c) <noun>+GENsaikou-noi -*(d) <adjective> or <embedded clause>(a) <adverb>(b) <noun>+ <case particle>LVCkandou-s-ase-ta-- -t<deverbal noun>+ <verbal suffixes>(a)(b)(d)eiga-gai -atae-tat -t**(c)<deverbal noun>+ <case particle><light-verb>+ <verbal suffixes>Fig.
1.
Dependency structure showing the range which the LVC paraphrasing affects.
The ovalobjects denote Japanese base-chunks so-called bunsetsu.Re-conjugation: Change the conjugation form of the elements (d) and occasionally(c), according to the syntactic category change of their modifiee: the given deverbalnoun is verbalized.
This operation can be carried out independently of the LVCparaphrasing.Selection of the voice: Choose the voice of the target sentence among active, passive,causative, etc.
In example (6), the causative (the auxiliary verb ?ase?)
is chosen.The decision depends on the syntactic and semantic attributes of both the givenlight-verb and the deverbal noun [14].Reassignment of the cases: Assign the case particles of the elements (b) and (c), thearguments of the main verb.
In (6), the syntactic case of ?kare (him),?
which wasoriginally assigned dative case ?ni?
is changed to accusative ?o.
?Among these operations, this paper focuses on the last two, namely handling the ele-ment (b), the sibling cases of the deverbal noun.
Triangles in both trees in Figure 1 indi-cate the range which we handle.
Henceforth, elements outside of the triangles, namely,(a), (c), and (d), are used only for explanatory purposes.4 LCS-Based Paraphrase Generation ModelFigure 2 illustrates how our model paraphrases the LVC, taking (7) as an example.
(7) s. Ken-ga eiga-ni shigeki-o uke-ta.Ken-NOM film-DAT inspiration-ACC to receive-PASTKen received an inspiration from the film.t.
Ken-ga eiga-ni shigeki-s-are-ta.Ken-NOM film-DAT to inspire-PASSIVE-PASTKen was inspired by the film.The generation process consists of the following three steps:Step 1.
Semantic analysis: The model first analyzes a given input sentence includingan LVC to obtain its LCS representation.
In Figure 2, this step generates LCSV 1 byfilling arguments of LCSV 0 with nominal elements.Exploiting Lexical Conceptual Structure for Paraphrase Generation 913LCSdic.i .Step 2LCS transformationttr f r ti+Step 3Surface generationtrf  r ti?ukeru?
(to receive)z-NOM y-ACC x-DAT?shigeki-suru?
(to inspire)x?-NOM y?-ACCStep 1Semantic analysistti  l iLCSN0LCSV0LCSV1LCSN1Input sentenceParaphrased sentenceKen-NOM film-DAT to inspire-PASSIVEfil t  i i IKen-NOM film-DAT inspiration-ACC to receive-ACTfil i i ti t  i[BECOME [[Ken]z BE WITH[[inspiration]y MOVE FROM [film]x TO [Ken]z]]][  [[ ]  I[[i i ti ]    [fil ]  [ ] ]]][[film]x?
ACT ON [Ken]y?
][[fil ]   [ ] ][BECOME [[Ken]z BE WITH ?
][  [[ ]  I  ][[film]x?
ACT ON [Ken]y?
][[fil ]   [ ] ]LCSS[BECOME [[Ken]z BE WITH[[inspiration]y MOVE FROM [film]x TO [Ken]z]]][  [[ ]  I[[i i ti ]    [fil ]  [ ] ]]]Fig.
2.
LCS-based paraphrase generation modelStep 2.
Semantic transformation (LCS transformation): The model then transfersthe obtained semantic structure to another semantic structure so that the target struc-ture consists of the LCS of the verbalized form of the deverbal noun.
In our exam-ple, this step generates LCSN1 together with the supplement ?
[BECOME [.
.
.]]?.
Werefer to such a supplement as LCSS .Step 3.
Surface generation: Having obtained the target LCS representation, the modelfinally generates the output sentence from it.
LCSS triggers another syntactic alter-nation such as passivization and causativization.The idea is to use the LCS representation as a semantic representation and to re-trieve semantic constraints to relieve the syntactic underspecificity underlying the LVCparaphrasing.
Each step consists of a handful of linguistically explainable rules, andthus is scalable when the typology and resource of LCS is given.
The rest of this sec-tion elaborates on each step, differentiating symbols to denote arguments; x, y, and zfor LCSV , and x?, y?, and z?
for LCSN .4.1 Semantic AnalysisGiven an input sentence (a simple clause with an LVC), the model first looks up theLCS template LCSV 0 for the given light-verb in the T-LCS dictionary, and then appliesthe case assignment rule below to obtain its LCS representation LCSV 1:?
In the case of the LCSV 0 having argument x, fill the leftmost argument of theLCSV 0 with the nominative case of the input, the second leftmost with the ac-cusative, and the rest with the dative case.?
Otherwise, fill arguments y and z of the LCSV 0 with the nominative and the dativecases, respectively.This rule is proposed in [19] instead of semantic parsing in order to tentativelyautomate LCS-based processing.
In the example shown in Figure 2, LCSV 0 for the914 A. Fujita, K. Inui, and Y. Matsumoto[BECOME [[Ken]z BE WITH[[inspiration]y MOVE FROM [film]x TO [Ken]z]]][  [[ ]  I[[i i ti ]    [fil ]  [ ] ]]][[film]x?
ACT ON [Ken]y?
][[fil ]   [ ] ][BECOME [[Ken]z BE WITH ?
][  [[ ]  I  ]+LCSV1LCSN1Predicate andargument matchingTreatment ofnon-transferred predicatesLCSSFig.
3.
An example of LCS transformationgiven light-verb ?ukeru (to receive)?
has argument x, thus the nominative case, ?Ken,?fills the leftmost argument z.
Accordingly, the accusative (?shigeki (inspiration)?)
andthe dative (?eiga (film)?)
fill y and x, respectively.4.2 LCS TransformationThe second step matches LCSV 1 with the another LCS for the verbalized form of thedeverbal noun LCSN0 to generate the target LCS representation LCSN1.
Figure 3 showsa more detailed view of this process for the example shown in Figure 2.Muraki [14] described that the direction of action and the focus of statement areimportant clues to determine the voice in LVC paraphrasing.
We therefore incorporatethe below assumptions into matching process.
The model first matches predicates inLCSV 1 and LCSN0, assuming that the agentive argument x is relevant to the direc-tion of action.
We classify the semantic predicates into the following three groups: (i)agentive predicates (involve argument x): ?CONTROL,?
?ACT ON,?
?ACT TO,?
?ACT,?and ?MOVE FROM TO,?
(ii) state of affair predicates (involve only argument y or z):?MOVE TO,?
?BE AT,?
and ?BE WITH,?
and (iii) aspectual predicates (with no argu-ment): ?BECOME,?
and allowed any pair of predicates in the same group to match.
Inour example, ?MOVE FROM TO?
matches ?ACT ON?
as shown in Figure 3.Having matched the predicates, the model then fills each argument slot in LCSN0with its corresponding argument in LCSV 1.
In Figure 3, argument z is matched withy?, and x with x?.
As a result, ?Ken?
and ?eiga?
come to y?
and x?
slots, respectively.When an argument is filled with another LCS, arguments within the inner LCS are alsotaken into account.
Likewise, we introduced some exceptional rules assuming that theinput sentences are periphrastic.
For instance, arguments filled with the implicit filler(e.g.
?name?
for ?to sign?
is usually not expressed in Japanese) and the deverbal noun,which is already represented by LCSN0 are never matched.
Argument z in LCSV 1 isallowed to match with y?
in LCSN0.LCS representations have right-embedding structures, and inner-embedded pred-icates denote the state of affairs.
We thus prioritize the rightmost predicates in thismatching process.
In other words, the proceeds from the rightmost inner predicatesto the outer ones, and the matching process is repeated until the leftmost predicate inLCSN0 or that in LCSV 1 matched.If LCSV 1 has any non-transferred part LCSS when the predicate and argumentmatching has been completed, it represents the semantic content that is not expressedby LCSN1 and needs to be expressed by auxiliary linguistic devices such as voiceauxiliaries.
As described in Section 2.1, the leftmost part specifies the focus of state-Exploiting Lexical Conceptual Structure for Paraphrase Generation 915ment.
The model thus attaches LCSS to LCSN0 as a supplement, and then use it todetermine auxiliaries in the next step, the surface generation.
In the case of Figure 3,?
[BECOME [[Ken]z BE WITH]]?
in LCSV 1 remains non-transferred and be attached.4.3 Surface GenerationThe model again applies the aforementioned case assignment rule to generate a sentencefrom the resultant LCS.
From the LCSN1 in Figure 2, sentence (8) is generated.
(8) eiga-ga Ken-o shigeki-shi-ta.film-NOM Ken-ACC to inspire-PASTThe film inspired Ken.The model then makes the final decision on the selection of the voice and the reas-signment of the cases.
As we described above, the attached structure LCSS is a clue todetermine what the focus is.
We therefore use the following decision list:1.
If the leftmost argument of LCSS has the same value as the leftmost argument inLCSN1, the viewpoints of LCSS and LCSN1 are same.
Thus, the active voice isselected and the case structure is left as is.2.
If the leftmost argument of LCSS has the same value as either z?
or y?
in LCSN1,the model makes the argument a subject (nominative).
That is, the passive voice isselected and case alternation (passivization) is applied.3.
If LCSS has ?BE WITH?
and its argument has the same value as x?
in LCSN1, thecausative voice is selected and case alternation (causativization) is applied.4.
If LCSS has an agentive predicate, and its argument is filled with a value differentfrom those of the other arguments, then the causative voice is selected and casealternation (causativization) is applied.5.
Otherwise, active voice is selected and thus no modification is applied.The example in Figure 2 satisfies the second condition, thus the model chooses ?s-are-ru (PASSIVE)?
and passivizes the sentence (8).
As a result, ?Ken?
becomes to bethe nominative ?ga?
as in (7t).5 Experiment5.1 Paraphrase Generation and EvaluationTo conduct an empirical experiment, we collected the following data sets.
Note thatmore than one LCS was assigned to a verb if it was polysemous.Deverbal nouns: We regard ?sahen-nouns?
and adverbial forms of verbs as deverbalnouns.
We retrieved 1,210 deverbal nouns from the T-LCS dictionary.
The set con-sists of (i) activity nouns (e.g., ?sasoi (invitation)?
and ?odoroki (surprise)?
), (ii) Sino-Japanese verbal nouns (e.g., ?kandou (impression)?
and ?shigeki (inspiration)?
), and(iii) English borrowings (e.g., ?drive?
and ?support?
).Tuples of light-verb and case particle: A verb takes different meanings when it con-stitutes LVCs with different case particles, and not every tuple of a light-verb v and a916 A. Fujita, K. Inui, and Y. Matsumotocase particle c functions as an LVC.
We therefore tailored an objective collection oftuples ?v, c?
from corpus in the following manner:Step 1.
From a corpus consisting of 25 million parsed sentences of newspaper articles,we collected 876,101 types of triplet ?v, c, n?, where v, c, and n denote a base formof verb, a case particle, and an deverbal noun.Step 2.
For each of the 50 most frequent ?v, c?
tuples, we extracted the 10 most fre-quent triplets ?v, c, n?.Step 3.
Each ?v, c, n?
was manually evaluated to determine whether it functioned as anLVC.
If any of 10 triplets functioned as an LVC, the tuple ?v, c?
was merged intothe list of light-verbs, assigning an LCS according to the linguistic tests examinedin [19].
As a result, we collected 40 types of ?v, c?
for light-verbs.Paraphrase examples: A collection of paraphrase examples, pairs of an LVC and itscorrect paraphrase, were constructed in the following way:Step 1.
From the 876,101 types of triplet ?v, c, n?
collected above, 23,608 types of?v, c, n?
were extracted, whose components, n and ?v, c?, were in the dictionaries.Step 2.
For each of the 245 most frequent ?v, c, n?, the 3 most frequent simple clausesincluding the ?v, c, n?
were extracted from the same corpus.Step 3.
Two native speakers of Japanese, adults graduated from university, were em-ployed to build a gold-standard collection.
711 out of 735 sentences were manuallyparaphrased in the manner of LVC, while the remaining 24 sentences were notbecause ?v, c, n?
within them did not function as LVCs.The real coverage of these 245 ?v, c, n?
with regard to all LVCs among the corpus fallsin the range between the below two:Lower bound: If every ?v, c, n?
is an LVC, the coverage of the collection is estimatedat 6.47% (492,737 / 7,621,089) of tokens.Upper bound: If the dictionaries cover all light-verbs and deverbal nouns, the collec-tion covers 24.1% (492,737 / 2,044,387) of tokens.In the experiment, our model generated all the possible paraphrases when a givenverb was polysemous with multiple entries in the T-LCS dictionary.
As a result, themodel generated 822 paraphrases from the 735 input sentences, at least one for eachinput.
We then classified the resultant paraphrases as correct and incorrect by compar-ing them with the gold-standard, where we ignored ordering of syntactic cases, andobtained 624 correct and 198 incorrect paraphrases Recall, precision, and F-measure(?
= 0.5) were 0.878 (624 / 711), 0.759 (624 / 822), and 0.814, respectively.As the baseline, we employed a statistical language model developed in [5].
Amongall the combinations of the voice and syntactic cases, the baseline model selects theone that has the highest probability.
Although the model is trained on a large amountof data, the generated expression often falls out of the vocabulary.
In such a case, theprobability cannot be calculated, and the model outputs nothing for the given sentence.As a result of an application of this baseline model to the same set of input sentences,we obtained 320 correct and 215 incorrect paraphrases (Recall: 0.450 (320 / 711), Pre-cision: 0.598 (320 / 535), and F-measure: 0.514).
The significant improvement indicatesthat our lexical-semantics-based account benefited on the decisions we considered.Exploiting Lexical Conceptual Structure for Paraphrase Generation 917The language model can also be complementary used to our LCS-based paraphrasegeneration.
By filtering implausible paraphrases out, 66 incorrect and 15 correct para-phrases were filtered, and the performance was further improved (Recall: 0.857, Preci-sion: 0.822, and F-measure: 0.839).5.2 DiscussionAlthough the performance has room for further improvement, we think the perfor-mance is reasonably high under the current stage of the T-LCS dictionary.
In otherwords, the tendency of errors does not so differ from our expectation.
As we expectedin Section 2.2, the ambiguity of dative case ?ni?
(c.f.
(5)) occupied the largest portionof errors (78 / 198).
This was because the case assignment was performed by a rule in-stead of semantic parsing.
Each rule in our model has been created relying on a set oflinguistic tests used in the theory of LCS and our linguistic intuition on handling LCS.However, the rule set was not sufficiently sophisticated, so that led to 59 errors.
Equally,30 errors occurred due to the immature typology of the T-LCS dictionary.We consider the improvement of the LCS typology as the primal issue, becauseour transformation rules depend on it.
For the moment, we have the following twosuggestions.
First, more variety of semantic roles should be handled step by step.
Forexample, we need to handle the object of ?eikyou-suru (to affect),?
which is marked bynot accusative but dative.
Second, the necessity of ?Source?
is inconsistent.
Verbs suchas ?hairu (to enter)?
do not require this argument (?BECOME BE AT?)
, while someother verbs, such as ?ukeru (to receive),?
explicitly require it (?MOVE FROM TO?).
Thetelicity of ?MOVE FROM TO?
should also be discussed.
With such a feedback fromthe application and an extensive investigation into lexicology, we have to enhance thetypology, and enlarge the dictionary preserving its consistency.6 Related WorkThe paraphrases associated with LVCs are not idiosyncratic to Japanese but also appearcommonly in other languages such as English, French, and Spanish [13,7,4] as shownin (3) and (4).
Our approach raises an interesting issue of whether the paraphrasing ofLVCs can be modeled in an analogous way across languages.Iordanskaja et al [7] proposed a set of paraphrasing rules including one for LVCparaphrasing based on the Meaning-Text Theory introduced by [13].
The model seemedto properly handle LVC paraphrasing, because their rules were described according tothe deep semantic analysis and heavily relied on what were called lexical functions,such as lexical derivation (e.g., S0(affect) = effect ) and light-verb generation (e.g.,Oper1(attempt) = make).
To take this approach, however, a vast amount of lexicalknowledge to form each lexical function is required, because they only virtually specifyall the choices relevant to LVC paraphrasing for every combination of deverbal nounand light-verb individually.
In contrast, our approach is to employ lexical semanticsto provide a general account of those classes of choices, and thus contributes to theknowledge development in terms of reducing human-labor and preserving consistency.Kaji et al [10] proposed a paraphrase generation model which utilized an monolin-gual dictionary for human.
Given an input LVC, their model paraphrases it referring to918 A. Fujita, K. Inui, and Y. Matsumotothe glosses of both the deverbal noun and light-verb, and a manually assigned semanticfeature of the light-verb.
Their model looks robust due to the availability of resource.However, their model fails to explain the difference between examples (7) and (9) inthe voice selection, because it selects the voice based only on the light-verb irrespec-tive of the deverbal noun: the light-verb ?ukeru (to receive)?
is always mapped to thepassive voice.
(9) s. musuko-ga kare-no hanashi-ni kandou-o uke-ta.son-NOM his-GEN talk-DAT impression-ACC to receive-PASTMy son was given a good impression by his talk.t.
musuko-ga kare-no hanashi-ni kandou-shi-ta.son-NOM his-GEN talk-DAT to be impressed-PASTMy son was impressed by his talk.In their model, the target expression is restricted only to the LVC itself (c.f., Figure 1).Hence, their model is unable to reassign the case particles as we saw in example (6).There is another trend in the research of paraphrase generation: i.e., the automaticparaphrase acquisition from existing lexical resources such as ordinary dictionaries,parallel/comparable corpora, and non-parallel corpora.
This type of approach may beable to reduce the cost of resource development.
However, there are drawbacks thatmust be overcome before they can work practically.
First, automatic methods requirelarge amounts of training data.
The issue is how to collect enough large size of data atlow cost.
Second, automatically extracted knowledge tends to be rather noisy, requiringmanual correction and maintenance.
In contrast, our approach, which focuses on theregularity underlying paraphrases, is a complementary avenue to develop and maintainknowledge resources that cover a sufficiently wide range of paraphrases.Previous case studies [14,18,11] have employed some syntactic properties of verbsto constrain syntactic transformations in paraphrase generation: e.g.
subject agentiv-ity, aspectual property, passivizability, and causativizability.
Several classifications ofverbs have also been proposed [12,15] based on various types of verb alternation andsyntactic case patterns.
In contrast, the theory of lexical semantics integrates syntacticand semantic properties including those above, and gives a perspective to formalize andmaintain the syntactic and semantic properties of words.7 ConclusionIn this paper, we explored what sorts of lexical properties encoded in LCS can explainthe regularity underlying paraphrases.
Based on an existing LCS dictionary, we builtan LCS-based paraphrase generation model, and conducted an empirical experiment onparaphrasing of LVC.
The experiment confirmed that the proposed model was capableof generating paraphrases accurately in terms of selecting the voice and reassigning thesyntactic cases, and revealed potential difficulties that we have to overcome toward apractical use of our lexical-semantics-based account.
To make our model more accu-rate, we need further discussion on (i) the enhancement of the T-LCS dictionary withfeedback from experiments, (ii) the LCS transformation algorithm, and (iii) the seman-tic parsing.
Another goal is to practically clarify what extent can be done by LCS forother classes of paraphrase, such as those exemplified in Section 1.Exploiting Lexical Conceptual Structure for Paraphrase Generation 919References1.
C. F. Baker, C. J. Fillmore, and J.
B. Lowe.
The Berkeley FrameNet project.
In Proceedingsof the 36th Annual Meeting of the Association for Computational Linguistics and the 17thInternational Conference on Computational Linguistics (COLING-ACL), pages 86?90, 1998.2.
X. Carreras and L. Ma`rques.
Introduction to the CoNLL-2004 shared task: semantic rolelabeling.
In Proceedings of 8th Conference on Natural Language Learning (CoNLL), pages89?97, 2004.3.
B. J. Dorr.
Large-scale dictionary construction for foreign language tutoring and interlingualmachine translation.
Machine Translation, 12(4):271?322, 1997.4.
M. Dras.
Tree adjoining grammar and the reluctant paraphrasing of text.
Ph.D. thesis,Division of Information and Communication Science, Macquarie University, 1999.5.
A. Fujita, K. Inui, and Y. Matsumoto.
Detection of incorrect case assignments in automat-ically generated paraphrases of Japanese sentences.
In Proceedings of the 1st InternationalJoint Conference on Natural Language Processing (IJCNLP), pages 14?21, 2004.6.
D. Gildea and D. Jurafsky.
Automatic labeling of semantic roles.
Computational Linguistics,28(3):245?288, 2002.7.
L. Iordanskaja, R. Kittredge, and A. Polgue`re.
Lexical selection and paraphrase in a meaning-text generation model.
In C. L. Paris, W. R. Swartout, and W. C. Mann, editors, NaturalLanguage Generation in Artificial Intelligence and Computational Linguistics, pages 293?312.
Kluwer Academic Publishers, 1991.8.
R. Jackendoff.
Semantic structures.
The MIT Press, 1990.9.
T. Kageyama.
Verb semantics.
Kurosio Publishers, 1996.
(in Japanese).10.
N. Kaji and S. Kurohashi.
Recognition and paraphrasing of periphrastic and overlappingverb phrases.
In Proceedings of the 4th International Conference on Language Resourcesand Evaluation (LREC) Workshop on Methodologies and Evaluation of Multiword Units inReal-world Application, 2004.11.
K. Kondo, S. Sato, and M. Okumura.
Paraphrasing by case alternation.
IPSJ Journal,42(3):465?477, 2001.
(in Japanese).12.
B. Levin.
English verb classes and alternations: a preliminary investigation.
Chicago Press,1993.13.
I. Mel?c?uk and A. Polgue`re.
A formal lexicon in meaning-text theory (or how to do lexicawith words).
Computational Linguistics, 13(3-4):261?275, 1987.14.
S. Muraki.
Various aspects of Japanese verbs.
Hitsuji Syobo, 1991.
(in Japanese).15.
A. Oishi and Y. Matsumoto.
Detecting the organization of semantic subclasses of Japaneseverbs.
International Journal of Corpus Linguistics, 2(1):65?89, 1997.16.
M. Palmer, D. Gildea, and P. Kingsbury.
The Proposition Bank: an annotated corpus ofsemantic roles.
Computational Linguistics, 31(1):71?106, 2005.17.
J. Pustejovsky.
The generative lexicon.
The MIT Press, 1995.18.
S. Sato.
Automatic paraphrase of technical papers?
titles.
IPSJ Journal, 40(7):2937?2945,1999.
(in Japanese).19.
K. Takeuchi, K. Kageura, and T. Koyama.
An LCS-based approach for analyzing Japanesecompound nouns with deverbal heads.
In Proceedings of the 2nd International Workshop onComputational Terminology (CompuTerm), pages 64?70, 2002.
