Proceedings of the Third Workshop on Statistical Machine Translation, pages 9?17,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsRich Source-Side Context for Statistical Machine TranslationKevin Gimpel and Noah A. SmithLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USA{kgimpel,nasmith}@cs.cmu.eduAbstractWe explore the augmentation of statistical ma-chine translation models with features of thecontext of each phrase to be translated.
Thiswork extends several existing threads of re-search in statistical MT, including the use ofcontext in example-based machine translation(Carl and Way, 2003) and the incorporation ofword sense disambiguation into a translationmodel (Chan et al, 2007).
The context fea-tures we consider use surrounding words andpart-of-speech tags, local syntactic structure,and other properties of the source languagesentence to help predict each phrase?s transla-tion.
Our approach requires very little compu-tation beyond the standard phrase extractionalgorithm and scales well to large data sce-narios.
We report significant improvementsin automatic evaluation scores for Chinese-to-English and English-to-German translation,and also describe our entry in the WMT-08shared task based on this approach.1 IntroductionMachine translation (MT) by statistical modeling ofbilingual phrases is one of the most successful ap-proaches in the past few years.
Phrase-based MTsystems are straightforward to train from parallelcorpora (Koehn et al, 2003) and, like the origi-nal IBM models (Brown et al, 1990), benefit fromstandard language models built on large monolin-gual, target-language corpora (Brants et al, 2007).Many of these systems perform well in competitiveevaluations and scale well to large-data situations(NIST, 2006; Callison-Burch et al, 2007).
End-to-end phrase-based MT systems can be built entirelyfrom freely-available tools (Koehn et al, 2007).We follow the approach of Koehn et al (2003),in which we translate a source-language sentence finto the target-language sentence e?
that maximizes alinear combination of features and weights:1?e?, a??
= argmax?e,a?score(e,a,f) (1)= argmax?e,a?M?m=1?mhm(e,a,f) (2)where a represents the segmentation of e and finto phrases and a correspondence between phrases,and each hm is a R-valued feature with learnedweight ?m.
The translation is typically found us-ing beam search (Koehn et al, 2003).
The weights?
?1, ..., ?M ?
are typically learned to directly mini-mize a standard evaluation criterion on developmentdata (e.g., the BLEU score; Papineni et al, (2002))using numerical search (Och, 2003).Many features are used in phrase-based MT, butnearly ubiquitous are estimates of the conditionaltranslation probabilities p(eji | f`k) and p(f`k | eji )for each phrase pair ?eji ,flk?
in the candidate sen-tence pair.2 In this paper, we add and evaluate fea-1In the statistical MT literature, this is often referred to as a?log-linear model,?
but since the score is normalized during nei-ther parameter training nor decoding, and is never interpreted asa log-probability, it is essentially a linear combination of featurefunctions.
Since many of the features are actually probabilities,this linear combination is closer to a mixture model.2We will use xji to denote the subsequence of x containingthe ith through jth elements of x, inclusive.9tures that condition on additional context features onthe source (f ) side:p(eji | Phrase = f`k,Context = ?fk?11 ,fF`+1, ...?
)The advantage of considering context is well-known and exploited in the example-basedMT com-munity (Carl and Way, 2003).
Recently researchershave begun to use source phrase context informa-tion in statistical MT systems (Stroppa et al, 2007).Statistical NLP researchers understand that condi-tioning a probability model on more information ishelpful only if there are sufficient training data to ac-curately estimate the context probabilities.3 Sparsedata are often the death of elaborate models, thoughthis can be remedied through careful smoothing.In this paper we leverage the existing linearmodel (Equation 2) to bring source-side context intophrase-based MT in a way that is robust to datasparseness.
We interpret the linear model as a mix-ture of many probability estimates based on differentcontext features, some of which may be very sparse.The mixture coefficients are trained in the usual way(?minimum error-rate training,?
Och, 2003), so thatthe additional context is exploited when it is usefuland ignored when it isn?t.The paper proceeds as follows.
We first review re-lated work that enriches statistical translation mod-els using context (?2).
We then propose a setof source-side features to be incorporated into thetranslation model, including the novel use of syntac-tic context from source-side parse trees and globalposition within f (?3).
We explain why analogoustarget-side features pose a computational challenge(?4).
Specific modifications to the standard trainingand evaluation paradigm are presented in ?5.
Exper-imental results are reported in ?6.2 Related WorkStroppa et al (2007) added souce-side context fea-tures to a phrase-based translation system, includingconditional probabilities of the same form that weuse.
They consider up to two words and/or POS tagsof context on either side.
Because of the aforemen-tioned data sparseness problem, they use a decision-3An illustrative example is the debate over the use of bilex-icalized grammar rules in statistical parsing (Gildea, 2001;Bikel, 2004).tree classifier that implicitly smooths relative fre-quency estimates.
The method improved over a stan-dard phrase-based baseline trained on small amountsof data (< 50K sentence pairs) for Italian?
Englishand Chinese ?
English.
We explore a significantlylarger space of context features, a smoothing methodthat more naturally fits into the widely used, error-driven linear model, and report a more comprehen-sive experimental evaluation (including feature com-parison and scaling up to very large datasets).Recent research on the use of word-sense dis-ambiguation in machine translation also points to-ward our approach.
For example, Vickrey et al(2005) built classifiers inspired by those used inword sense disambiguation to fill in blanks ina partially-completed translation.
Gime?nez andMa`rquez (2007) extended the work by consideringphrases and moved to full translation instead of fill-ing in target-side blanks.
They trained an SVM foreach source language phrase using local features ofthe sentences in which the phrases appear.
Carpuatand Wu (2007) and Chan et al (2007) embeddedstate-of-the-art word sense disambiguation modulesinto statistical MT systems, achieving performanceimprovements under several automatic measures forChinese ?
English translation.Our approach is also reminiscent of example-based machine translation (Nagao, 1984; Somers,1999; Carl and Way, 2003), which has for manyyears emphasized use of the context in which sourcephrases appear when translating them.
Indeed, likethe example-based community, we do not begin withany set of assumptions about which kinds of phrasesrequire additional disambiguation (cf.
the applica-tion of word-sense disambiguation, which is moti-vated by lexical ambiguity).
Our feature-rich ap-proach is omnivorous and can exploit any linguisticanalysis of an input sentence.3 Source-Side Context FeaturesAdding features to the linear model (Equation 2)that consider more of the source sentence requireschanging the decoder very little, if at all.
The reasonis that the source sentence is fully observed, so theinformation to be predicted is the same as before?the difference is that we are using more clues tocarry out the prediction.10We see this as an opportunity to include manymore features in phrase-based MT without increas-ing the cost of decoding at runtime.
This discussionis reminiscent of an advantage gained by movingfrom hidden Markov models to conditional randomfields for sequence labeling tasks.
While the samecore algorithm is used for decoding with both mod-els, a CRF allows inclusion of features that considerthe entire observed sequence?i.e., more of the ob-servable context of each label to be predicted.
Al-though this same advantage was already obtainedin statistical MT through the transition from ?noisychannel?
translation models to (log-)linear models,the customary set of features used in most phrase-based systems does not take full advantage of theobserved data.The standard approach to estimating the phrasetranslation conditional probability features is via rel-ative frequencies (here e and f are phrases):p(e | f) =count(e,f)?e?
count(e?,f)Our new features all take the form p(e |f ,f context), where e is the target language phrase,f is the source language phrase, and f context is thecontext of the source language phrase in the sentencein which it was observed.
Like the context-bare con-ditional probabilities, we estimate probability fea-tures using relative frequencies:p(e | f ,f context) =count(e,f ,f context)?e?
count(e?,f ,f context)Since we expect that adding conditioning vari-ables will lead to sparser counts and therefore morezero estimates, we compute features for many dif-ferent types of context.
To combine the manydifferently-conditioned features into a single model,we provide them as features to the linear model(Equation 2) and use minimum error-rate training(Och, 2003) to obtain interpolation weights ?m.This is similar to an interpolation of backed-off es-timates, if we imagine that all of the different con-texts are differently-backed off estimates of the com-plete context.
The error-driven weight training ef-fectively smooths one implicit context-rich estimatep(e | f ,f context) so that all of the backed-off es-timates are taken into account, including the orig-inal p(e | f).
Our approach is asymmetrical; wehave not, for example, estimated features of the formp(f ,f context | e).We next discuss the specific source-side contextfeatures used in our model.3.1 Lexical Context FeaturesThe most obvious kind of context of a source phrasef `k is the m-length sequence before it (fk?1k?m) andthe m-length sequence after it (f `+m`+1 ).
We includecontext features for m ?
{1, 2}, padding sentenceswith m special symbols at the beginning and at theend.
For each value of m, we include three features:?
p(e | f ,fk?1k?m), the left lexical context;?
p(e | f ,f `+m`+1 ), the right lexical context;?
p(e | f ,fk?1k?m,f`+m`+1 ), both sides.3.2 Shallow Syntactic FeaturesLexical context features, especially when m > 1,are expected to be sparse.
Representing the contextby part-of-speech (POS) tags is one way to over-come that sparseness.
We used the same set of thelexical context features described above, but withPOS tags replacing words in the context.
We alsoinclude a feature which conditions on the POS tagsequence of the actual phrase being translated.3.3 Syntactic FeaturesIf a robust parser is available for the source lan-guage, we can include context features from parsetrees.
We used the following parse tree features:?
Is the phrase (exactly) a constituent??
What is the nonterminal label of the lowest nodein the parse tree that covers the phrase??
What is the nonterminal label or POS of the high-est nonterminal node that ends immediately be-fore the phrase?
Begins immediately after thephrase??
Is the phrase strictly to the left of the root word,does it contain the root word, or is it strictly tothe right of the root word?
(Requires a parse withhead annotations.
)We also used a feature that conditions on both fea-tures in the third bullet point above.11S[support] Syntactic Features:Not a constituentNP covers phraseVBP to leftPP to rightRight of root wordPPShallowLexicalPhrasePPNPVPIN     DT       NN        ,  PRP VBP     DT   NN   IN  DT      NN         IN     NN     CC    NN    , ...NPSBARNPNPNPNP.........Positional Features:Not at startNot at endSecond fifth of sentenceCovers 18.5% of sentence(quantized to 20%)Against this background ,  we   support   the report  of  the committee   on transport and tourism , which...In dieser Hinsicht unterst?tzen wir   den Bericht des Ausschusses   f?r Verkehr und Fremdenverkehr , in...SyntacticNPPPFigure 1: A (partial) sentence pair from the WMT-07 Europarl training corpus.
Processing of the data (parsing, wordalignment) was done as discussed in ?6.
The phrase pair of interest is boxed and context features are shown in dottedshapes.
The context features help determine whether the phrase should be translated as ?der Bericht des Ausschusses?
(nominative case) or ?den Bericht des Ausschusses?
(accusative case).
See text for details.3.4 Positional FeaturesWe include features based on the position of thephrase in the source sentence, the phrase length, andthe sentence length.
These features use informationfrom the entire source sentence, but are not syntac-tic.
For a phrase f `k in a sentence f of length n:?
Is the phrase at the start of the sentence (k = 1)??
Is the phrase at the end of the sentence (` = n)??
A quantization of r =k+ `?k+12n , the relative po-sition in (0, 1) of the phrase?s midpoint within f .We choose the smallest q ?
{0.2, 0.4, 0.6, 0.8, 1}such that q > r.?
A quantization of c = `?k+1n , the fraction of thewords in f that are covered by the phrase.
Wechoose the smallest q ?
{ 140 ,120 .110 ,15 ,13 , 1} suchthat q > c.An illustration of the context features is shownin Fig.
1.
Consider the phrase pair ?the reportof the committee?/?den Bericht des Ausschusses?extracted by our English ?
German baseline MTsystem (described in ?6.3).
The German word?Bericht?
is a masculine noun; therefore, it takes thearticle ?der?
in the nominative case, ?den?
in the ac-cusative case, and ?dem?
in the dative case.
Thesethree translations are indeed available in the phrasetable for ?the report of the committee?
(see Table 1,?no context?
column), with relatively high entropy.The choice between ?den?
and ?der?
must be madeby the language model alone.Knowing that the phrase follows a verb, or ap-pears to the right of the sentence?s root word, orwithin the second fifth of the sentence should help.Indeed, a probability distribution that conditions oncontext features gives more peaked distributions thatgive higher probability to the correct translation,given this context, and lower probability given someother contexts (see Table 1).4 Why Not Target-Side Context?While source context is straightforward to exploitin a model, including target-side context featuresbreaks one of the key independence assumptionsmade by phrase-based translation models: the trans-lations of the source-side phrases are conditionallyindependent of each other, given f , thereby requir-ing new algorithms for decoding (Marino et al,2006).We suggest that target-side context may alreadybe well accounted for in current MT systems.
In-deed, language models pay attention to the localcontext of phrases, as do reordering models.
The re-cent emphasis on improving these components of atranslation system (Brants et al, 2007) is likely duein part to the widespread availability of NLP toolsfor the language that is most frequently the target:English.
We will demonstrate that NLP tools (tag-12Shallow: 2 POS on left Syntax: of root Positional: rel.
pos.g no context ?
?PRP VBP?
?VBN IN?
?right left ?2nd fifth 1st fifthden bericht des ausschusses 0.3125 1.0000 0.3333 0.5000 0.0000 0.6000 0.0000der bericht des ausschusses 0.3125 0.0000 0.0000 0.1000 0.6667 0.2000 0.6667dem bericht des ausschusses 0.2500 0.0000 0.6667 0.3000 0.1667 0.0000 0.1667Table 1: Phrase table entries for ?the report of the committee?
and their scores under different contexts.
These are thetop three phrases in the baseline English ?
German system (?no context?
column).
Contexts from the source sentencein Fig.
1 (starred) predict correctly; we show also alternative contexts that give very different distributions.gers and parsers) for the source side can be used toimprove the translation model, exploiting analysistools for other languages.5 ImplementationThe additional data required to compute the contextfeatures is extracted along with the phrase pairs dur-ing execution of the standard phrase extraction algo-rithm, affecting phrase extraction and scoring timeby a constant factor.
We avoid the need to modifythe standard phrase-based decoder to handle contextfeatures by appending a unique identifier to each to-ken in the sentences to be translated.
Then, we pre-compute a phrase table for the phrases in these sen-tences according to the phrase contexts.
To avoidextremely long lists of translations of common to-kens, we filter the generated phrase tables, remov-ing entries for which the estimate of p(e | f) < c,for some small c. In our experiments, we fixedc = 0.0002.
This filtering reduced time for exper-imentation dramatically and had no apparent effecton the translation output.
We did not perform anyfiltering for the baseline system.6 ExperimentsIn this section we present experimental results usingour context-endowed phrase translation model witha variety of different context features, on Chinese ?English, German ?
English, and English ?
Ger-Chinese ?
English (UN)Context features BLEU NIST METEORNone 0.3715 7.918 0.6486Lexical 0.4030 8.367 0.6716Shallow 0.3807 7.981 0.6523Lexical + Shallow 0.4030 8.403 0.6703Syntactic 0.3823 7.992 0.6531Positional 0.3775 7.958 0.6510Table 2: Chinese ?
English experiments: training andtesting on UN data.
Boldface marks scores significantlyhigher than ?None.
?man translation tasks.
Dataset details are given inAppendices A (Chinese) and B (German).Baseline We use the Moses MT system (Koehn etal., 2007) as a baseline and closely follow the exam-ple training procedure given for the WMT-07 andWMT-08 shared tasks.4 In particular, we performword alignment in each direction using GIZA++(Och and Ney, 2003), apply the ?grow-diag-final-and?
heuristic for symmetrization and use a max-imum phrase length of 7.
In addition to the twophrase translation conditionals p(e | f) and p(f |e), we use lexical translation probabilities in eachdirection, a word penalty, a phrase penalty, a length-based reordering model, a lexicalized reorderingmodel, and an n-gram language model, SRILM im-plementation (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998).
Mini-mum error-rate (MER) training (Och, 2003) was ap-plied to obtain weights (?m in Equation 2) for thesefeatures.
A recaser is trained on the target side of theparallel corpus using the script provided withMoses.All output is recased and detokenized prior to evalu-ation.Evaluation We evaluate translation output usingthree automatic evaluation measures: BLEU (Pap-ineni et al, 2002), NIST (Doddington, 2002), andMETEOR (Banerjee and Lavie, 2005, version 0.6).5All measures used were the case-sensitive, corpus-level versions.
The version of BLEU used was thatprovided by NIST.
Significance was tested using apaired bootstrap (Koehn, 2004) with 1000 samples(p < 0.05).64http://www.statmt.org/wmt085METEOR details: For English, we use exact matching,Porter stemming, and WordNet synonym matching.
For Ger-man, we use exact matching and Porter stemming.
These are thesame settings that were used to evaluate systems for the WMT-07 shared task.6Code implementing this test for these metrics can be freelydownloaded at http://www.ark.cs.cmu.edu/MT.13Chinese ?
EnglishTesting on UN Testing on News (NIST 2005)Context features BLEU NIST METEOR BLEU NIST METEORTraining on in-domain data only:None 0.3715 7.918 0.6486 0.2700 7.986 0.5314Training on all data:None 0.3615 7.797 0.6414 0.2593 7.697 0.5200Lexical 0.3898 8.231 0.6697 0.2522 7.852 0.5273Shallow: ?
1 POS tag 0.3611 7.713 0.6430 0.2669 8.243 0.5526Shallow: ?
2 POS tags 0.3657 7.808 0.6455 0.2591 7.843 0.5288Lexical + Shallow 0.3886 8.245 0.6675 0.2628 7.881 0.5290Syntactic 0.3717 7.899 0.6531 0.2653 8.123 0.5403Lexical + Syntactic 0.3926 8.224 0.6636 0.2572 7.774 0.5234Positional 0.3647 7.766 0.6469 0.2648 7.891 0.5275All 0.3772 8.176 0.6582 0.2566 7.775 0.5225Feature selection (see Sec.
6.4) 0.3843 8.079 0.6594 0.2730 8.059 0.5343Table 3: Chinese ?
English experiments: first row shows baseline performance when training only on in-domaindata for each task; all other rows show results when training on all data (UN and News).
Left half shows results whentuning and testing on UN test sets; right half shows results when tuning on NIST 2004 News test set and testing onNIST 2005.
For feature selection, an additional set of unseen data was used: 2000 held-out sentences from the UNdata for the left half and the NIST 2003 test set for the right half.
Boldface marks scores that are significantly higherthan the first row, in-domain baseline.6.1 Chinese ?
EnglishFor our Chinese ?
English experiments, two kindsof data were used: UN proceedings, and newswireas used in NIST evaluations.UN Data UN data results are reported in Ta-ble 2.
Significant improvements are obtained on allthree evaluation measures?e.g., more than 3 BLEUpoints?using lexical or lexical and shallow fea-tures.
While improvements are smaller for other fea-tures and feature combinations, performance is notharmed by conditioning on context features.
Notethat using syntactic features gave 1 BLEU point ofimprovement.News Data In News data experiments, none of ourfeatures obtained BLEU performance statisticallydistinguishable from the baseline of 0.2700 BLEU(neither better, nor worse).
The News training cor-pus is less than half the size of the UN training cor-pus (in words); unsurprisingly, the context featureswere too sparse to be helpful.
Further, newswire areless formulaic and repetitive than UN proceedings,so contexts do not generalize as well from trainingto test data.
Fortunately, our ?error-minimizing mix-ture?
approach protects the BLEU score, which the?m are tuned to optimize.Combined UN + News Data Our next experi-ment used all of the available training data (> 200Mwords on each side) to train the models, in-domain?m tuning, and testing for each domain separately;see Table 3.
Without context features, trainingon mixed-domain data consistently harms perfor-mance.
With contexts that include lexical features,the mixed-domain model significantly outperformsthe in-domain baseline for UN data.
These resultssuggest that context features enable better use of out-of-domain data, an important advantage for statis-tical MT since parallel data often arise from verydifferent sources than those of ?real-world?
transla-tion scenarios.
On News data, context features didnot give a significant advantage on the BLEU score,though syntactic and ?
1 POS contexts did give sig-nificant NIST and METEOR improvements over thein-domain baseline.6.2 German ?
EnglishWe do not report full results for this task, becausethe context features neither helped nor hurt perfor-mance significantly.
We believe this is due to datasparseness resulting from the size of the training cor-pus (26M German words), German?s relatively richmorphology, and the challenges of German parsing.14English ?
GermanContext features BLEU NIST METEORNone 0.2069 6.020 0.2811Lexical 0.2018 6.031 0.2772Shallow 0.2017 5.911 0.2748Syntactic 0.2077 6.049 0.2829Positional 0.2045 5.930 0.2772Lex.
+ Shal.
+ Syn.
0.2045 6.061 0.2817All 0.2053 6.009 0.2797Feature selection 0.2080 6.009 0.2807Table 4: English ?
German experiments: trainingand testing on Europarl data.
WMT-07 Europarl paralleltraining data was used for training, dev06 was used fortuning, devtest06 was used for feature selection and de-velopmental testing, and test07 was used for final testing.Boldface marks scores significantly higher than ?None.
?6.3 English ?
GermanEnglish ?
German results are shown in Table 4.The baseline system here is highly competitive, hav-ing scored higher on automatic evaluation measuresthan any other system in the WMT-07 shared task(Callison-Burch et al, 2007).
Though most resultsare not statistically significant, small improvementsdo tend to come from syntactic context features.Comparing with the German?
English experiment,we attribute this effect to the high accuracy of theEnglish parser compared to the German parser.6.4 Feature SelectionTranslation performance does not always increasewhen features are added to the model.
This mo-tivates the use of feature selection algorithms tochoose a subset of features to optimize perfor-mance.
We experimented with several feature se-lection algorithms based on information-theoreticquantities computed among the source phrase, thetarget phrase, and the context, but found that a sim-ple forward variable selection algorithm (Guyon andElisseeff, 2003) worked best.
In this procedure, westart with no context features and, at each iteration,add the single feature that results in the largest in-crease in BLEU score on an unseen developmentset after ?m tuning.
The algorithm terminates if nofeatures are left or if none result in an increase inBLEU.
We ran this algorithm to completion for thetwo Chinese?
English tune/test sets (training on alldata in each case) and the English ?
German task;see Tables 3 and 4.
In all cases, the algorithm fin-ishes after ?
4 iterations.Feature selection for Chinese ?
English (UN)first chose the lexical feature ?1 word on each side,?then the positional feature indicating which fifth ofthe sentence contains the phrase, and finally the lex-ical feature ?1 word on right.?
For News, the fea-tures chosen were the shallow syntactic feature ?1POS on each side,?
then the positional beginning-of-sentence feature, then the position relative to theroot (a syntactic feature).
For English ?
German,the shallow syntactic feature ?2 POS on left,?
thenthe lexical feature ?1 word on right?
were selected.In the case where context features were mosthelpful (Chinese ?
English UN data), we foundfeature selection to be competitive at 2.28 BLEUpoints above the no-context baseline, but not the bestachieved.
In the other two cases (Chinese?
EnglishNews and English ?
German Europarl), our bestresults were achieved using these automatically se-lected features, and in the Chinese ?
English Newscase, improvements on all three scores (including1.37 BLEU points) are significant compared to theno-context baseline trained on the same data.6.5 WMT-08 Shared Task: English ?
GermanSince we began this research before the releaseof the data for the WMT-08 shared task, we per-formed the majority of our experiments using thedata released for the WMT-07 shared task (see Ap-pendix B).
To prepare our entry for the 2008 sharedtask, we trained a baseline system on the 2008 datausing a nearly identical configuration.7 Table 5 com-pares performance of the baseline system (with nocontext features) to performance with the two con-text features chosen automatically as described in?6.4.
In addition to the devtest06 data, we report re-sults on the 2007 and 2008 Europarl test sets.
Mostimprovements were statistically significant.7 Future WorkIn future work, we plan to apply more sophisticatedlearning algorithms to rich-feature phrase table esti-mation.
Context features can also be used as condi-tioning variables in other components of translation7The only differences were the use of a larger max sentencelength threshold of 55 tokens instead of 50, and the use of thebetter-performing ?englishFactored?
Stanford parser model.15devtest06 test07 test08System BLEU NIST METEOR BLEU NIST METEOR BLEU NIST METEORBaseline 0.2009 5.866 0.2719 0.2051 5.957 0.2782 0.2003 5.889 0.2720Context 0.2039 5.941 0.2784 0.2088 6.036 0.2826 0.2016 5.956 0.2772Table 5: English ?
German shared task system results using WMT-08 Europarl parallel data for training, dev06 fortuning, and three test sets, including the final 2008 test set.
The row labeled ?Context?
uses the top-performing featureset {2 POS on left, 1 word on right}.
Boldface marks scores that are significantly higher than the baseline.models, including the lexicalized reordering modeland the lexical translation model in the Moses MTsystem, or hierarchical or syntactic models (Chiang,2005).
Additional linguistic analysis (e.g., morpho-logical disambiguation, named entity recognition,semantic role labeling) can be used to define newcontext features.8 ConclusionWe have described a straightforward, scalablemethod for improving phrase translation models bymodeling features of a phrase?s source-side context.Our method allows incorporation of features fromany kind of source-side annotation and barely affectsthe decoding algorithm.
Experiments show perfor-mance rivaling or exceeding strong, state-of-the-artbaselines on standard translation tasks.
Automaticfeature selection can be used to achieve performancegains with just two or three context features.
Per-formance is strongest when large in-domain trainingsets and high-accuracy NLP tools for the source lan-guage are available.AcknowledgmentsThis research was supported in part by an ARCSaward to the first author, NSF IIS-0713265, su-percomputing resources provided by Yahoo, and aGoogle grant.
We thank Abhaya Agarwal, AshishVenugopal, and Andreas Zollmann for helpful con-versations and Joy Zhang for his Chinese segmenter.We also thank the anonymous reviewers for helpfulcomments.A Dataset Details (Chinese-English)We trained on data from the NIST MT 2008 con-strained Chinese-English track: Hong KongHansards and news (LDC2004T08), Sino-rama (LDC2005T10), FBIS (LDC2003E14),Xinhua (LDC2002E18), and financial news(LDC2006E26)?total 2.5M sents., 66M Chinesewords, 68M English.
For news experiments, thenewswire portion of the NIST 2004 test set was usedfor tuning, the full NIST 2003 test set was used fordevelopmental testing and feature selection, and theNIST 2005 test set was used for testing (900-1000sents.
each).
We also used the United Nations paral-lel text (LDC2004E12), divided into training (4.7Msents.
; words: 136M Chinese, 144M English),tuning (2K sents.
), and test sets (2K sents.).
Weremoved sentence pairs where either side was longerthan 80 words, segmented all Chinese text automat-ically,8 and parsed/tagged using the Stanford parserwith the pre-trained ?xinhuaPCFG?
model (Kleinand Manning, 2003).
Trigram language modelswere trained on the English side of the parallelcorpus along with approximately 115M words fromthe Xinhua section of the English Gigaword corpus(LDC2005T12), years 1995?2000 (total 326Mwords).B Dataset Details (German-English)For German ?
English experiments, we used dataprovided for the WMT-07 shared task (1.1M sents.,26M German words, 27M English).
We used dev06for tuning, devtest06 for feature selection and de-velopmental testing, and test07 for final testing(2K sents.
each).
We removed sentence pairswhere either side was longer than 50 words andparsed/tagged the German and English data usingthe Stanford parser (Klein andManning, 2003) (withpre-trained ?germanFactored?
and ?englishPCFG?models).
5-gram language models were trained onthe entire target side of the parallel corpus (37MGerman words, 38M English).8Available at http://projectile.is.cs.cmu.edu/research/public/tools/segmentation/lrsegmenter/lrSegmenter.perl.16ReferencesS.
Banerjee and A. Lavie.
2005.
METEOR: An auto-matic metric for MT evaluation with improved corre-lation with human judgments.
In Proc.
of ACL Work-shop on Intrinsic and Extrinsic Evaluation Measuresfor MT and/or Summarization.D.
M. Bikel.
2004.
A distributional analysis of a lexical-ized statistical parsing model.
In Proc.
of EMNLP.T.
Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean.2007.
Large language models in machine translation.In Proc.
of EMNLP-CoNLL.P.
F. Brown, J. Cocke, S. A. Della Pietra, V. J. DellaPietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, andP.
S. Roossin.
1990.
A statistical approach to machinetranslation.
Computational Linguistics, 16(2):79?85.C.
Callison-Burch, P. Koehn, C. Fordyce, and C. Monz,editors.
2007.
Proc.
of the 2nd Workshop on Statisti-cal Machine Translation.M.
Carl and A.Way.
2003.
Recent Advances in Example-Based Machine Translation.
Kluwer Academic.M.
Carpuat and D. Wu.
2007.
Improving statistical ma-chine translation using word sense disambiguation.
InProc.
of EMNLP-CoNLL.Y.
Chan, H. Ng, and D. Chiang.
2007.
Word sense dis-ambiguation improves statistical machine translation.In Proc.
of ACL.S.
Chen and J. Goodman.
1998.
An empirical study ofsmoothing techniques for language modeling.
Techni-cal report 10-98, Harvard University.D.
Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In Proc.
of ACL.G.
Doddington.
2002.
Automatic evaluation of machinetranslation quality using n-gram co-occurrence statis-tics.
In Proc.
of HLT.D.
Gildea.
2001.
Corpus variation and parser perfor-mance.
In Proc.
of EMNLP.J.
Gime?nez and L. Ma`rquez.
2007.
Context-awarediscriminative phrase selection for statistical machinetranslation.
In Proc.
of the 2ndWorkshop on StatisticalMachine Translation.I.
Guyon and A. Elisseeff.
2003.
An introduction to vari-able and feature selection.
Journal of Machine Learn-ing Research, 3:1157?1182.D.
Klein and C. D. Manning.
2003.
Fast exact inferencewith a factored model for natural language parsing.
InAdvances in NIPS 15.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisti-cal phrase-based translation.
In Proc.
of HLT-NAACL,pages 127?133.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkit forstatistical machine translation.
In ACL demonstrationsession.P.
Koehn.
2004.
Statistical significance tests for machinetranslation evaluation.
In Proc.
of EMNLP.Jose?
B. Marino, Rafael E. Banchs, Josep M. Crego, Adria`de Gispert, Patrik Lambert, Jose?
A. R. Fonollosa, andMarta R. Costa-jussa`.
2006.
N -gram-based machinetranslation.
Computational Linguistics, 32(4):527?549.M.
Nagao.
1984.
A framework of a mechanical trans-lation between Japanese and English by analogy prin-ciple.
In Proc.
of the International NATO Symposiumon Artificial and Human Intelligence.
Elsevier North-Holland, Inc.NIST.
2006.
NIST 2006 machine translation evaluationofficial results.F.
Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1).F.
J. Och.
2003.
Minimum error rate training for sta-tistical machine translation.
In Proc.
of ACL, pages160?167.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proc.
of ACL.H.
Somers.
1999. Review article: Example-based ma-chine translation.
Machine Translation, 14(2).A.
Stolcke.
2002.
SRILM?an extensible language mod-eling toolkit.
In Proc.
of ICSLP.N.
Stroppa, A. van den Bosch, and A.
Way.
2007.Exploiting source similarity for SMT using context-informed features.
In Proc.
of TMI.D.
Vickrey, L. Biewald, M. Teyssier, and D. Koller.
2005.Word-sense disambiguation for machine translation.In Proc.
of HLT-EMNLP.17
