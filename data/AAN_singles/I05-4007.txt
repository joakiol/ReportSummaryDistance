Cross-lingual Conversion of Lexical Semantic Relations: BuildingParallel WordnetsChu-Ren Huang1 , I-Li Su1, Jia-Fei Hong1, Xiang-Bing Li21.
Institute of Linguistics2.
Institute of Information ScienceAcademia Sinica,No.128 Academic Sinica Road, SEC.2 Nankang,Taipei 115, Taiwan1.
{ churen, isu, jiafei }@gate.sinica.edu.twAbstractParallel wordnets built uponcorrespondences between differentlanguages can play a crucial role inmultilingual knowledge processing.
Sincethere is no homomorphism between pairs ofmonolingual wordnets, we must rely onlexical semantic relation (LSR) mappings toensure conceptual cohesion.
In this paper,we propose and implement a model forbootstrapping parallel wordnets based onone monolingual wordnet and a set ofcross-lingual lexical semantic relations.
Inparticular, we propose a set of inferencerules to predict Chinese wordnet structurebased on English wordnet andEnglish-Chinese translation relations.
Weshow that this model of parallel wordnetbuilding is effective and achieves higherprecision in LSR prediction.1 IntroductionA knowledgebase which systemizeslexical and conceptual information ofhuman knowledge is a basic infrastructurefor Natural Language Processing (NLP)applications.
Wordnets, pioneered by thePrinceton WordNet (WN, Fellbaum 1998),and greatly enriched by EuroWordnet (EWN,Vossen 1998), have become the standard fora lexical knowledgebase enriched withlexical semantic relations.
In addition to themultilingual architecture of EWN, there aresome proposals to construct the expansionfor monolingual wordnets to parallelwordnet systems, such as Pianta and Girardi(2002).
However, the construction ofmultilingual wordnets eventually faces thechallenge of low-density languages, whichis dealt with in Huang, et al (2002).Low-density languages, as opposed tohigh-density languages, usually refer tolanguages that are not spoken by a largenumber of people.
However, there is neithera direct correspondence between languagepopulation and language technology, nor anobjective population number that definesdensity level.
In this work, we use theavailability of language resources instead todefine language density.
That is, low-densitylanguages are languages that do not haveenough language resources to support fullyautomated language processing, such asmachine translation.
In our current line ofwork, we (Huang et al 2002) refer tolow-density languages as those which do nothave enough existing resources forsemi-automatic construction of monolingualwordnet.There are two alternative approaches tobuild parallel wordnets, as shown in Figure1.
The first approach relies on two fullyannotated monolingual wordnets withsynsets and LSR?s.
The second approachrequires only one fully annotated WN inaddition to LSR-based cross-lingualtranslation correspondences.48Figure1.
Two Approaches to Building Bilingual WordnetsApproach I maps and pairs Language Asynsets with Language B synsets andannotates cross-lingual LSR?s.
The result isa fully annotated parallel wordnet.
ApproachII maps language A synsets to language Bthrough translation equivalents.
Afterlanguage B synsets are thus established,language B LSR?s are predicted based oncorresponding LSR?s in language A.
A newset of monolingual LSR?s is bootstrappedand predicted basing on inference rulesgoverned by translation LSR?s (T-LSR?s).
Ingeneral, approach I applies to high-densitylanguages while approach II applies tolow-density languages.
In this paper, we willfocus on the application of approach II tobuild a Chinese Wordnet with conceptualcohesion.The current model was first explored inHuang et al (2003).
This previous studycovered 210 lemmas, consisted of the topranked lemmas in each part-of-speech(POS).
The translation LSR?s discussed inthe previous model were antonymy,hypernymy and hyponymy.
In this currentwork, we expand our study to all possibleLSR?s as well as to all the bilingual lexicalpairs in our English-Chinese translationequivalents databases.
Moreover, the LSR?sin Princeton WordNet are again used as thebasis for bootstrapping.
In addition, weestablish a set of evaluation for the results.The approach will be evaluated in term ofboth the precision of prediction and theconfidence of prediction.
We aim to showthat T-LSR?s bootstrapped approach doesprovide an effective model for buildingparallel wordnets for low-density languages.After the introduction, the main part ofthis paper consists of the following sections:in section 2, we briefly introduce theexisting resources required for this work.We discuss methodology of T-LSRbootstrapping step by step in section 3.
Aseries of LSR-predicting inference rules arealso given in this section.
In section 4, weplan to evaluate the results of ourexperiment and demonstrate the feasibilityof maintaining conceptual cohesion incross-lingual LSR mapping.2 Required Resources: ECTED andWNAs we mentioned above, the T-LSRapproach to parallel wordnet requires twolanguage resources: a fully annotatedmonolingual wordnet and a set of translationLSR?s to map the wordnet information tothe target language.
In our current study, weuse the English WN as the source of synsetand LSR information.
The semantic relationbetween an English synset and its Chinesetranslation is based on The English-ChineseTranslation Equivalents Database (ECTED,Huang et al 2002).2.1 The English-Chinese TranslationEquivalents Databases (ECTED)The basic idea of ECTED is to providethe Chinese translation equivalents for eachAPPROACH IGiven fully annotatedmonolingual wordentswith synsets and LSRsFully annotatedparallel wordnetAPPROACH IIGiven fully annotated WNof language A; andbilingual translationequivalents annotatedwith LSRMap LSR-annotatedsynsets in Language A toLanguage B throughtranslation LSRs (T-LSR?s)Grow LSR links amongLanguage B synsets byusing language A LSRand cross-lingual LSRinference rulesMap and pair Language A andLanguage B synsets withcross-lingual LSRs49WN English synset.
Our ECTED wasbootstrapped with a combined lexicalknowledgebase integrating at least fourEnglish-Chinese or Chinese-Englishbilingual resources.
Based on this combinedLKB, a group of translators chose (orcreated) up to three best translationequivalents for each WN synset.
In addition,for each English-Chinese translationequivalent, a lexical semantic relation isannotated.
In addition to synonym, thesemantic relations marked includingantonym, hypernym, hyponym, holonym,meronym, and near-synonym.
We use allsemantic relations, with the exception ofantonymy, in this study.2.2 Wordnet (WN)The Cognitive Science Laboratory ofPrinceton University created WN, a lexicalknowledgebase for English, in 1990(Fellbaum, 1998).
Synsets (a group ofform-meaning pairs sharing same sense) arethe main units used in WN to organize thelexicon conceptually.
Each sense can beexpanded either by gloss or context.
It iseasy for users to distinguish each sense bysimply checking the synonyms, the examplesentences or explanation.
Nouns, verbs,adjectives and adverbs are the main lexicalcategories to classify all the lexicons.
Suchclassification of lexicons is based on theprinciples in psycholinguistics.
Besides, thesemantic relations of each sense in WN arealso expressed like a Word-network.
Inother words, WN resembles an ontologysystem and links all the semantic relationsof words.
Therefore, English WN is not justa lexical knowledgebase but also anontological system that expresses thesemantic relations and the concepts ofwords.The current version of WN is Wordnet2.0, but Wordnet 1.6 is more widely used bythe most applications in NLP and linguisticresearch.
Therefore, after considering thecompatibility with other applications, weconnected the ECTED with Wordnet 1.6.However, we are still working on keepingupdating our systems by using the content inthe new version of WN.
We believe this willkeep the information updated and shortenthe gap caused by the different versions ofWN.3 Inferring Lexical SemanticRelations for WN and ECTEDAs we mentioned above, WN does notonly express the knowledge of lexicons butalso cover the semantic relations of lexicons.Therefore, in order to present such semanticrelations clearly and logically, Huang (2002)proposed to use cross-lingual LexicalSemantic Relations (LSRs) to predict thesemantic relations in the target language.The proposed framework is shown inDiagram 1.Diagram 1.
Translation-mediated LSR (the complete model)In Diagram1, EW1 and EW2 are headwords for two different English synsets.CW1 and CW2 are translation equivalentsin ECTED for these two head words.
LSR iand ii are the T-LSRs stipulating thesemantic relations between the head wordsand their Chinese TEs.
In WN, each synsetis linked to a network of their synsetsthrough a number of LSR?s.
Hence, we useLSR x to represent the semantic relationCW1 EW1(Synset number)EW2(Synset number)CW2yixiix = EW1-EW2y = CW1-CW2i = Translation LSRii = Translation LSRThe unknown LSR y = i+x+ii50between EW1 and EW2.
The four LSR?sform a closed network that includes threeknow LSR?s: two T-LSRs, i and ii, and oneEnglish LSR, x, from WN.
The onlyunknown LSR is y, the semantic relationbetween CW1 and CW2.
Huang et al(2002)claimed that LSR y can be inferred as afunctional combination of the three LSRs - i,x and ii.Language translation does not onlyinvolve the semantic correspondences butalso the human decision in choosingtranslation equivalents that are affected bythe social and cultural factors.
Our mainpriority in this paper is to infer the lexicalsemantic information across differentlanguage rather than the translationalidiosyncrasies, so the elements regardingtranslational idiosyncrasies are excludedhere.
In order to simplify the complexity ofLSR combination and get a better predictionof LSR, here, we only take account of thesituations when LSR ii is exactly equivalent,EW2=CW2 or ii=0.
Therefore, we have areduced model of the translation-mediatedLSR Prediction as shown in diagram 2.Diagram 2.
Translation-mediated LSR (the reduced model)Synonym, hypernym, hyponym,holonym, meronym and near-synonym arethe main semantic relations that we willdiscuss in the following sections.
First of all,we would like to discuss the foundationalsituation of LSR prediction, synonym, asshown in diagram 3.
When translation LSR iis exactly equivalent, i.e.
CW1=EW1, andLSR ii is also exactly equivalent, i.e.EW2=CW2, the LSR combination, LSR y,is directly inherited the semantic relation ofLSR x.Diagram 3.
Translation-mediated LSR (When TEs are synonymous)CW1 EW1(Synset number)EW2(Synset number)=CW2 (ii=0)yixThe unknown LSR y= i + xCW1 EW1(Synset number)EW2(Synset number)=CW2 (ii=0)yCW1=EW1(i=0)xThe unknown LSR y= 0 + x = x51Diagram 4.
Examples of the LSR (When TEs are synonymous)As shown in diagram 4 above,according to the ECTED, the English headword ?thin?
is exactly equivalent with?shou4?
in Chinese.
The LSR x betweenEW1 and EW2 in WN is marked ?ANT?which means ?fat?
is the antonym of ?thin.
?Therefore, according to the prediction indiagram 3, we can infer that the CW2(fei2pang4de5) is the antonym of CW1(shou4).
The above inference can also beapplied to another example in diagram 4.The LSR prediction in WN plays a verycrucial role in determining the unknownLSR y.
Even an English head word mayhave more than one sense, it is still veryclear to infer the LSR between the TEs.However, there is a potential problem withinthis inference.
If a head word has more thanone Chinese TEs which can all correspondto the head word, there might be a problemto consider whether those TEs are reallysynonyms.However, the situation is not alwaysthat ideal as above.
When the Chinesetranslation equivalents and the correspondedEnglish synset have a non-identicalsemantic relation, CW1?EW1, theprediction of LSR y needs to be consideredfurther and carefully.fei2pang4d fat (00934421A)chubby(00935062A) = feng1man3de5y =NSYNCW1=EW1(i=0)x = NSYNshou4 thin (00936334A)fat(00934421A) = fei2pang4de5y = ANTCW1=EW1(i=0)x= ANT52Diagram 5.
Predicting LSR (Hypernym) and its exampleDiagram 6.
Predicting LSR(Hyponym) and its exampleLogically, hypernym and hyponym aresymmetric semantic relations.
For instance,if A is a hypernym of B, B is a hyponym ofA.
For instance, as shown in diagram 5, theEnglish word ?nick?
is the hypernym of theChinese term ?shang1kou3?
and ?cut?
is thehypernym of ?nick?
in WN and the exacttranslation equivalent of ?cut?
in Chinese is?jian3kai1.?
According to the logicality,?jian3kai1?
is the hypernym of?shang1kou3.?
The example of hyponym isshown in diagram 6.
Due to the variedsemantic relations in WN, the inferences ofLSRs , the unknown LSR y = i + x ,forhypernym, hyponym, near-synonym,holonym, and mernoym are listed as below:Hypernym(HYP)(a) IF x=ANTLSR y =HYP +ANT =ANT (CW2 is theantonym of CW1.
)(b) IF x=HYPLSR y = HYP+HYP =HYP (CW2 is thehypernym of CW1.
)(c) IF x= NSYNLSR y = HYP+NSYN =HYP (CW2 is thehypernym of CW1.
)(d) IF x = HOLLSR y = HYP+HOL =HOL (CW2 is theholonym of CW1.
)(e) IF x = all other LSRLSR y = HYP +all other LSRs = ?
(Undecided)Hyponym(HPO)(a) IF x=ANTLSR y =HPO +ANT =ANT (CW2 is theantonym of CW1.
)(b) IF x=HPOLSR y = HPO+HPO =HPO (CW2 is thehyponym of CW1.
)(c) IF x= NSYNLSR y = HPO+NSYN =HPO (CW2 is thehyponym of CW1.
)(d) IF x = MERLSR y = HPO+MER =MER (CW2 is themeronym of CW1.
)(e) IF x = all other LSRLSR y = HPO +all other LSRs = ?
(Undecided)gao1dian3 pastry(05670938N)baklava(05674827N)=guo3ren2mi4tang2qian1ceng2bing3yi= HPOx= HPOThe unknown LSR y= i + x=HPO +HPO =HPO(?guo3ren2mi4tang2qian1ceng2bing3?
is thehyponym of ?gao1dian3?
)shang1kou3 nick(00248910N)cut(00248688N)=jian3kai1yi= HYP (?nick?
is the hypernym of ?shang1kou3?
)x= HYP (?cut?
is the hypernym of ?nick?
)The unknown LSR y= i + x=HYP +HYP =HYP(?jian3kai1?
is the hypernym of ?shang1kou3?
)53Near-Synonym(NSYN)(a) IF x=ANTLSR y = NSYN+ANT =ANT (CW2 is theantonym of CW1.
)(b) IF x=HYPLSR y = NSYN+HYP =HYP (CW2 is thehypernym of CW1.
)(c) IF x=HPOLSR y = NSYN+HPO =HPO (CW2 is thehyponym of CW1.
)(d) IF x= NSYNLSR y = NSYN+NSYN =NSYN (CW2 isthe near-synonym of CW1.
)(e) IF x = MERLSR y = NSYN+MER =MER (CW2 is themeronym of CW1.
)(f) IF x = HOLLSR y = NSYN+HOL =HOL (CW2 is theholonym of CW1.
)Holonym(HOL)(a) IF x=ANTLSR y = HOL+ANT =ANT (CW2 is theantonym of CW1.
)(b) IF x=HYPLSR y = HOL+HYP =HYP (CW2 is thehypernym of CW1.
)(c) IF x= NSYNLSR y = HOL+NSYN =HOL (CW2 is theholonym of CW1.
)(d) IF x = HOLLSR y = HOL+HOL =HOL (CW2 is theholonym of CW1.
)(e) IF x = all other LSRLSR y = HPO +all other LSRs = ?
(Undecided)Meronym(MER)(a) IF x=ANTLSR y = MER+ANT =ANT (CW2 is theantonym of CW1.
)(b) IF x=HPOLSR y = MER+HPO =HPO (CW2 is thehyponym of CW1.
)(c) IF x= NSYNLSR y = MER+NSYN =MER (CW2 is themeronym of CW1.
)(d) IF x = MERLSR y = MER+MER =MER (CW2 is themeronym of CW1.
)(e) IF x = all other LSRLSR y = HPO +all other LSRs = ?
(Undecided)4 Implementation and EvaluationWN 1.6 contains 99,642 Englishsynsets and expands to 157,507 Englishlemma tokens.
On the other hand, the totalnumber of Chinese lemma types found inour ECTED is 108,533.
Hence, eachChinese lemma type translates roughly 1.1English synsets in average.In comparing the two approaches toparallel wordnet building, we treat atbaseline the cases where the translation LSRis synonymy.
In others words, these are thecases where both approach I and approach IIwill make highly accurate predictions (e.g.Huang, et al 2003).
However, if the T-LSRis other than synonymy, we expect theprediction based on source language LSRwill be much lower.In our study, there are in total 372,927lexical semantic relations that canpotentially be bootstrapped when the T-LSRis one of the five semantic relations in study.These are expanded from the followingtypes of translations equivalence relations:11,396 translation near-synonyms, 2,782translation hypernyms, 2,106 translationhyponyms, 252 translation meronyms and145 translations holonyms.
For evaluation,due to constraints on resources, weexhaustively check the types with less than300 lemmas, while randomly checked closeto 300 lemmas for the other types.We first introduce the baseline modelwhere synonym is assumed.
This is wheresource language LSR?s will be mappeddirectly to target languages.
We have shownthat if the T-LSR is really synonymy, theprecision will be 62.7%.
However, when theT-LSR?s are different, the baseline precisionis much lower.
In Table 1, such na?veprediction is manually classes into threetypes: Correct, Incorrect, and Others.?Correct?
means that the prediction isverified.
?Incorrect?
means the assignedLSR is wrong.
Two scenarios are possible.One is that there is a possible prediction andanother one is the correct LSR is differentfrom the predicted one.
?Others?
refers toexceptional cases where these is no lexicaltranslation, or the source language LSR iswrongly assigned and so on.
Table 1 showsthat the baseline for non-synonymousT-LSR is only 47% in average, and rangefrom 30% to 65% for each semantic relation.54Correct Incorrect Others TotalNSYN 400 51% 379 49% 0 0% 779 100%HYP 178 65% 72 27% 22 8% 272 100%HPO 402 40% 285 28% 330 32% 1017 100%HOL 48 30%  108 69% 2 1% 158 100%MER 52 56% 32 34% 9 10% 93 100%Total 1079 47% 877 37% 363 16% 2319 100%Table 1 Baseline Results (assuming synonym)Table 2 shows the comparison betweenthe T-LSR model and the baseline.
It showsthat there is improvement of 17.8% inaverage and that there is gain in precisionfor each LSR type.
The improvement variesfrom just below 2% to 39%.Baseline T-LSR Difference ImprovementNSYN 400 51% 556 71% 156 20 % 156/400 39 %HYP 178 65% 184 66%  6 2.2% 6/178  3.4%HPO 402 40% 409 40%  7 0.7% 7/402  1.7%HOL 48 30% 64 41% 16 10.1% 16/48 33.3%MER 52 56% 58 62% 6 6.5% 6/52 11.5%Total 1079 47% 1271 55% 191 8.2% 191/1080 17.7%Table 2 Precision of using the LSR inferences5 ConclusionIt is interesting to note that the classeswith least improvements are hypernymy andhyponymy.
Since these are the classicalIS-A relations, we hypothesize that theirpredictions are similar to the baselinerelation of synonym.
If we take these tworelations out, the T-LSR model withinference rules has a precision difference of17.3% (178/1030), as well as animprovement of 35.6% (178/500).
These aresubstantial improvements over the baselinemodel.
The result will be reinforced whenthe evaluation is completed.
We will alsoanalyze the prediction based on each T-LSRto give a more explanatory account as well ameasure confidence or prediction.
The resultoffers strong support for T-LSR as a modelfor bootstrapping parallel wordnets with alow-density target language.ReferencesFellbaum, C.
(ed.)
1998.
Wordent: An ElectronicLexical Database.
Cambridge, MA: MIT Press.Huang, Chu-Ren, D.B.
Tsai, J.Lin, S. Tseng, K.J.Chen and Y. Chuang.
2001 Definition and Testfor Lexical Semantic Relation in Chinese.
[inChinese] Paper presented at the Second ChineseLexical Semantics Workshop.
May 2001,Beijing, China.Huang, Chu-Ren, I-Ju E. Tseng, Dylan B.S.
Tsai.2002.
Translating Lexical Semantic Relations:The first step towards Multilingual Wordnets.Proceedings of the COLONG2002 Workshop?SemaNet:Building and Using SemanticNetworks?, ed.
By Grace Ngai, Pascale Fung,and Kenneth W. Church, 2-8.Huang, Chu-Ren, Elanna I. J. Tseng, Dylan B.S.Tsai, Brian Murphy.
2003 Cross-lingualPortability of Semantic Relations: BootstrappingChinese WordNet with English WordNetRelations.
pp.509-531.Pianta, Emanuel, L. Benitivogli, C. Girardi.2002 MultiWordnet: Developing an alignednultilingual database.
Proceedings of the 1stInternational WordNet Conference, Maysore,Inda, pp.293-302.Tsai, D.B.S., Chu-Ren Huang, J.Lin, K.J.
Chenand Y. Chuang.
2002.
Definition and Test forLexical Semantic Relation in Chinese.
[?????????????!?]
Journal of Chinese Information Processing[??????].
16.4.21-31.Vossen P.
(ed.).
1998.
EuroWordNet: Amultilingual database with lexical semanticnetworks.
Norwell, MA: Kluwer AcademicPublisher55
