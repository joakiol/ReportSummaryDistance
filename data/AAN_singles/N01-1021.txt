A Probabilistic Earley Parser as a Psycholinguistic ModelJohn HaleDepartment of Cognitive ScienceThe Johns Hopkins University3400 North Charles Street; Baltimore MD 21218-2685hale@cogsci.jhu.eduAbstractIn human sentence processing, cognitive load can bedened many ways.
This report considers a deni-tion of cognitive load in terms of the total probabilityof structural options that have been disconrmed atsome point in a sentence: the surprisal of word wigiven its prex w0...i?1on a phrase-structural lan-guage model.
These loads can be eciently calcu-lated using a probabilistic Earley parser (Stolcke,1995) which is interpreted as generating predictionsabout reading time on a word-by-word basis.
Un-der grammatical assumptions supported by corpus-frequency data, the operation of Stolcke?s probabilis-tic Earley parser correctly predicts processing phe-nomena associated with garden path structural am-biguity and with the subject/object relative asym-metry.IntroductionWhat is the relation between a person?s knowledge ofgrammar and that same person?s application of thatknowledge in perceiving syntactic structure?
Theanswer to be proposed here observes three principles.Principle 1 The relation between the parser andgrammar is one of strong competence.Strong competence holds that the human sentenceprocessing mechanism directly uses rules of gram-mar in its operation, and that a bare minimum ofextragrammatical machinery is necessary.
This hy-pothesis, originally proposed by Chomsky (Chom-sky, 1965, page 9) has been pursued by many re-searchers (Bresnan, 1982) (Stabler, 1991) (Steed-man, 1992) (Shieber and Johnson, 1993), and standsin contrast with an approach directed towards thediscovery of autonomous principles unique to theprocessing mechanism.Principle 2 Frequency affects performance.The explanatory success of neural network andconstraint-based lexicalist theories (McClelland andSt.
John, 1989) (MacDonald et al, 1994) (Tabor etal., 1997) suggests a statistical theory of languageperformance.
The present work adopts a numericalview of competition in grammar that is grounded inprobability.Principle 3 Sentence processing is eager.\Eager" in this sense means the experimental situa-tions to be modeled are ones like self-paced readingin which sentence comprehenders are unrushed andno information is ignored at a point at which it couldbe used.The proposal is that a person?s diculty per-ceiving syntactic structure be modeled by word-to-word surprisal (Attneave, 1959, page 6) which canbe directly computed from a probabilistic phrase-structure grammar.
The approach taken here usesa parsing algorithm developed by Stolcke.
In thecourse of explaining the algorithm at a very highlevel I will indicate how the algorithm, interpretedas a psycholinguistic model, observes each principle.After that will come some simulation results, andthen a conclusion.1 Language modelsStolcke?s parsing algorithm was initially applied as acomponent of an automatic speech recognition sys-tem.
In speech recognition, one is often interestedin the probability that some word will follow, giventhat a sequence of words has been seen.
Given somelexicon of all possible words, a language model as-signs a probability to every string of words fromthe lexicon.
This denes a probabilistic language(Grenander, 1967) (Booth and Thompson, 1973)(Soule, 1974) (Wetherell, 1980).A language model helps a speech recognizer focusits attention on words that are likely continuationsof what it has recognized so far.
This is typicallydone using conditional probabilities of the formP (Wn= wnjW1= w1; : : : Wn?1= wn?1)the probability that the nth word will actually bewngiven that the words leading up to the nth havebeen w1; w2; : : : wn?1.
Given some nite lexicon, theprobability of each possible outcome for Wncan beestimated using that outcome?s relative frequency ina sample.Traditional language models used for speech are n-gram models, in which n ?
1 words of history serveas the basis for predicting the nth word.
Such mod-els do not have any notion of hierarchical syntacticstructure, except as might be visible through an n-word window.Aware that the n-gram obscures manylinguistically-signicant distinctions (Chomsky,1956, section 2.3), many speech researchers (Jelinekand Laerty, 1991) sought to incorporate hierar-chical phrase structure into language modeling (see(Stolcke, 1997)) although it was not until the late1990s that such models were able to signicantlyimprove on 3-grams (Chelba and Jelinek, 1998).Stolcke?s probabilistic Earley parser is one wayto use hierarchical phrase structure in a languagemodel.
The grammar it parses is a probabilisticcontext-free phrase structure grammar (PCFG),e.g.1:0 S !
NP VP0:5 NP !
Det N0:5 NP !
NP VP......see (Charniak, 1993, chapter 5)Such a grammar denes a probabilistic language interms of a stochastic process that rewrites strings ofgrammar symbols according to the probabilities onthe rules.
Then each sentence in the language of thegrammar has a probability equal to the product ofthe probabilities of all the rules used to generate it.This multiplication embodies the assumption thatrule choices are independent.
Sentences with morethan one derivation accumulate the probability of allderivations that generate them.
Through recursion,innite languages can be specied; an importantmathematical question in this context is whether ornot such a grammar is consistent { whether it assignssome probability to innite derivations, or whetherall derivations are guaranteed to terminate.Even if a PCFG is consistent, it would appear tohave another drawback: it only assigns probabili-ties to complete sentences of its language.
This is asinconvenient for speech recognition as it is for mod-eling reading times.Stolcke?s algorithm solves this problem by com-puting, at each word of an input string, the prexprobability.
This is the sum of the probabilities of allderivations whose yield is compatible with the stringseen so far.
If the grammar is consistent (the proba-bilities of all derivations sum to 1.0) then subtractingthe prex probability from 1.0 gives the total proba-bility of all the analyses the parser has disconrmed.If the human parser is eager, then the \work" doneduring sentence processing is exactly this disconr-mation.2 Earley parsingThe computation of prex probabilities takes advan-tage of the design of the Earley parser (Earley, 1970)which by itself is not probabilistic.
In this section Iprovide a brief overview of Stolcke?s algorithm butthe original paper should be consulted for full details(Stolcke, 1995).Earley parsers work top-down, and propagatepredictions conrmed by the input string back upthrough a set of states representing hypotheses theparser is entertaining about the structure of the sen-tence.
The global state of the parser at any one timeis completely dened by this collection of states, achart, which denes a tree set.
A state is a recordthat species the current input string position processed sofar a grammar rule a \dot-position" in the rule representing howmuch of the rule has already been recognized the leftmost edge of the substring this rule gen-eratesAn Earley parser has three main functions, pre-dict, scan and complete, each of which can enternew states into the chart.
Starting from a dummystart state in which the dot is just to the left of thegrammar?s start symbol, predict adds new states forrules which could expand the start symbol.
In thesenew predicted states, the dot is at the far left-handside of each rule.
After prediction, scan checks theinput string: if the symbol immediately followingthe dot matches the current word in the input, thenthe dot is moved rightward, across the symbol.
Theparser has \scanned" this word.
Finally, completepropagates this change throughout the chart.
If, asa result of scanning, any states are now present inwhich the dot is at the end of a rule, then the lefthand side of that rule has been recognized, and anyother states having a dot immediately in front ofthe newly-recognized left hand side symbol can nowhave their dots moved as well.
This happens overand over until no new states are generated.
Parsingnishes when the dot in the dummy start state ismoved across the grammar?s start symbol.Stolcke?s innovation, as regards prex probabili-ties is to add two additional pieces of information toeach state: , the forward, or prex probability, and?
the \inside" probability.
He notes thatpath An (unconstrained) Earley path,or simply path, is a sequence of Earleystates linked by prediction, scanning,or completion.constrained A path is said to be con-strained by, or generate a string x ifthe terminals immediately to the leftof the dot in all scanned states, in se-quence, form the string x.. .
.The signicance of Earley paths is thatthey are in a one-to-one correspondencewith left-most derivations.
This will al-low us to talk about probabilities of deriva-tions, strings and prexes in terms of theactions performed by Earley?s parser.
(Stolcke, 1995, page 8)This correspondence between paths of parser op-erations and derivations enables the computation ofthe prex probability { the sum of all derivationscompatible with the prex seen so far.
By the cor-respondence between derivations and Earley paths,one would need only to compute the sum of all pathsthat are constrained by the observed prex. Butthis can be done in the course of parsing by storingthe current prex probability in each state.
Then,when a new state is added by some parser opera-tion, the contribution from each antecedent state {each previous state linked by some parser operation{ is summed in the new state.
Knowing the prexprobability at each state and then summing for allparser operations that result in the same new stateeciently counts all possible derivations.Predicting a rule corresponds to multiplying bythat rule?s probability.
Scanning does not alter anyprobabilities.
Completion, though, requires knowing?, the inside probability, which records how probablewas the inner structure of some recognized phrasalnode.
When a state is completed, a bottom-up con-rmation is united with a top-down prediction, sothe  value of the complete-ee is multiplied by the?
value of the complete-er.Important technical problems involving left-recursive and unit productions are examined andovercome in (Stolcke, 1995).
However, these com-plications do not add any further machinery to theparsing algorithm per se beyond the grammar rulesand the dot-moving conventions: in particular, thereare no heuristic parsing principles or intermediatestructures that are later destroyed.
In this respectthe algorithm observes strong competence { princi-ple 1.
In virtue of being a probabilistic parser itobserves principle 2.
Finally, in the sense that pre-dict and complete each apply exhaustively at eachnew input word, the algorithm is eager, satisfyingprinciple 3.3 ParallelismPsycholinguistic theories vary regarding the amountbandwidth they attribute to the human sentenceprocessing mechanism.
Theories of initial parsingpreferences (Fodor and Ferreira, 1998) suggest thatthe human parser is fundamentally serial: a func-tion from a tree and new word to a new tree.
Thesetheories explain processing diculty by appealingto \garden pathing" in which the current analysisis faced with words that cannot be reconciled withthe structures built so far.
A middle ground is heldby bounded-parallelism theories (Narayanan and Ju-rafsky, 1998) (Roark and Johnson, 1999).
In thesetheories the human parser is modeled as a functionfrom some subset of consistent trees and the newword, to a new tree subset.
Garden paths arise inthese theories when analyses fall out of the set oftrees maintained from word to word, and have tobe reanalyzed, as on strictly serial theories.
Finally,there is the possibility of total parallelism, in whichthe entire set of trees compatible with the input ismaintained somehow from word to word.
On sucha theory, garden-pathing cannot be explained by re-analysis.The probabilistic Earley parser computes allparses of its input, so as a psycholinguistic theoryit is a total parallelism theory.
The explanationfor garden-pathing will turn on the reduction in theprobability of the new tree set compared with theprevious tree set { reanalysis plays no role.
Beforeillustrating this kind of explanation with a specicexample, it will be important to rst clarify the na-ture of the linking hypothesis between the operationof the probabilistic Earley parser and the measuredeects of the human parser.4 Linking hypothesisThe measure of cognitive eort mentioned earlier isdened over prexes: for some observed prex, thecognitive eort expended to parse that prex is pro-portional to the total probability of all the struc-tural analyses which cannot be compatible with theobserved prex.
This is consistent with eagernesssince, if the parser were to fail to infer the incom-patibility of some incompatible analysis, it wouldbe delaying a computation, and hence not be eager.This prex-based linking hypothesis can be turnedinto one that generates predictions about word-by-word reading times by comparing the total eortexpended before some word to the total eort af-ter: in particular, take the comparison to be a ratio.Making the further assumption that the probabili-ties on PCFG rules are statements about how di-cult it is to disconrm each rule1, then the ratio of1This assumption is inevitable given principles 1 and 2.
Ifthere were separate processing costs distinct from the opti-mization costs postulated in the grammar, then strong com-petence is violated.
Dening all grammatical structures asequally easy to disconrm or perceive likewise voids the grad-edness of grammaticality of any content.the  value for the previous word to the  value forthe current word measures the combined dicultyof disconrming all disconrmable structures at agiven word { the denition of cognitive load.
Scal-ing this number by taking its log gives the surprisal,and denes a word-based measure of cognitive eortin terms of the prex-based one.
Of course, if thelanguage model is sensitive to hierarchical structure,then the measure of cognitive eort so dened willbe structure-sensitive as well.5 Plausibility of ProbabilisticContext-Free GrammarThe debate over the form grammar takes in the mindis clearly a fundamental one for cognitive science.Much recent psycholinguistic work has generated awealth of evidence that frequency of exposure to lin-guistic elements can aect our processing (Mitchellet al, 1995) (MacDonald et al, 1994).
However,there is no clear consensus as to the size of the ele-ments over which exposure has clearest eect.
Gib-son and Pearlmutter identify it as an \outstandingquestion" whether or not phrase structure statisticsare necessary to explain performance eects in sen-tence comprehension:Are phrase-level contingent frequency con-straints necessary to explain comprehen-sion performance, or are the remainingtypes of constraints sucient.
If phrase-level contingent frequency constraints arenecessary, can they subsume the eects ofother constraints (e.g.
locality) ?
(Gibson and Pearlmutter, 1998, page 13)Equally, formal work in linguistics has demon-strated the inadequacy of context-free grammars asan appropriate model for natural language in thegeneral case (Shieber, 1985).
To address this criti-cism, the same prex probabilities could be comput-ing using tree-adjoining grammars (Nederhof et al,1998).
With context-free grammars serving as theimplicit backdrop for much work in human sentenceprocessing, as well as linguistics2 simplicity seems asgood a guide as any in the selection of a grammarformalism.6 Garden-pathing6.1 A celebrated exampleProbabilistic context-free grammar (1) will help il-lustrate the way a phrase-structured language model2Some important work in computational psycholinguistics(Ford, 1989) assumes a Lexical-Functional Grammar wherethe c-structure rules are essentially context-free and haveattached to them \strengths" which one might interpret asprobabilities.could account for garden path structural ambiguity.Grammar (1) generates the celebrated garden pathsentence \the horse raced past the barn fell" (Bever,1970).
English speakers hearing these words one byone are inclined to take \the horse" as the subject of\raced," expecting the sentence to end at the word\barn."
This is the main verb reading in gure 1.SNPthe horseVPVBDracedPPINpastNPDTtheNNbarnFigure 1: Main verb readingThe human sentence processing mechanism ismetaphorically led up the garden path by the mainverb reading, when, upon hearing \fell" it is forcedto accept the alternative reduced relative readingshown in gure 2.SNPNPDTtheNNhorseVPVBNracedPPINpastNPDTtheNNbarnVPVBDfellFigure 2: Reduced relative readingThe confusion between the main verb and the re-duced relative readings, which is resolved upon hear-ing \fell" is the empirical phenomenon at issue.As the parse trees indicate, grammar (1) analyzesreduced relative clauses as a VP adjoined to an NP3.In one sample of parsed text4 such adjunctions areabout 7 times less likely than simple NPs made up ofa determiner followed by a noun.
The probabilitiesof the other crucial rules are likewise estimated bytheir relative frequencies in the sample.3See section 1.24 of the Treebank style guide4The sample, starts at sentence 93 of section 16 ofthe Treebank and goes for 500 sentences (12924 words)For information about the Penn Treebank project seehttp://www.cis.upenn.edu/~ treebank/(1)1.0 S ?
NP VP .0.876404494831 NP ?
DT NN0.123595505169 NP ?
NP VP1.0 PP ?
IN NP0.171428571172 VP ?
VBD PP0.752380952552 VP ?
VBN PP0.0761904762759 VP ?
VBD1.0 DT ?
the0.5 NN ?
horse0.5 NN ?
barn0.5 VBD ?
fell0.5 VBD ?
raced1.0 VBN ?
raced1.0 IN ?
pastThis simple grammar exhibits the essential characterof the explanation: garden paths happen at pointswhere the parser can disconrm alternatives that to-gether comprise a great amount of probability.
Notethe category ambiguity present with raced which canshow up as both a past-tense verb (VBD) and a pastparticiple (VBN).Figure 3 shows the reading time predictions5 derivedvia the linking hypothesis that reading time at wordn is proportional to the surprisal log(?n?1?n).the horse raced past the barn fell2468101214Log[previous prefixcurrent prefix ] garden-pathing01.0.1906840.064130301.5.90627Figure 3: Predictions of probabilistic Earley parseron simple grammarAt \fell," the parser garden-paths: up until thatpoint, both the main-verb and reduced-relativestructures are consistent with the input.
The prexprobability before \fell" is scanned is more than 10times greater than after, suggesting that the proba-bility mass of the analyses disconrmed at that pointwas indeed great.
In fact, all of the probability as-signed to the main-verb structure is now lost, andonly parses that involve the low-probability NP rulesurvive { a rule introduced 5 words back.6.2 A comparisonIf this garden path eect is truly a result of both themain verb and the reduced relative structures be-ing simultaneously available up until the nal verb,5Whether the quantitative values of the predicted read-ing times can be mapped onto a particular experiment in-volves taking some position on the oft-observed (Gibson andSchu?tze, 1999) imperfect relationship between corpus fre-quency and psychological norms.then the eect should disappear when words inter-vene that cancel the reduced relative interpretationearly on.To examine this possibility, consider now a dier-ent example sentence, this time from the languageof grammar (2).
(2)0.574927953937 S ?
NP VP0.425072046063 S ?
VP1.0 SBAR ?
WHNP S0.80412371161 NP ?
DT NN0.082474226966 NP ?
NP SBAR0.113402061424 NP ?
NP VP0.11043 VP ?
VBD PP0.141104 VP ?
VBD NP PP0.214724 VP ?
AUX VP0.484663 VP ?
VBN PP0.0490798 VP ?
VBD1.0 PP ?
IN NP1.0 WHNP ?
who1.0 DT ?
the0.33 NN ?
boss0.33 NN ?
banker0.33 NN ?
buy-back0.5 IN ?
about0.5 IN ?
by1.0 AUX ?
was0.74309393 VBD ?
told0.25690607 VBD ?
resigned1.0 VBN ?
toldThe probabilities in grammar (2) are estimated fromthe same sample as before.
It generates a sentencecomposed of words actually found in the sample,\the banker told about the buy-back resigned."
Thissentence exhibits the same reduced relative clausestructure as does \the horse raced past the barnfell.
"SNPNPDTtheNNbankerVPVBNtoldPPabout the buy-backVPVBDresignedGrammar (2) also generates6 the subject relative\the banker who was told about the buy-back re-signed."
Now a comparison of two conditions is pos-sible.MV and RC the banker told about the buy-back re-signed6This grammar also generates active and simple passivesentences, rating passive sentences as more probable than theactives.
This is presumably a fact about the writing stylefavored by the Wall Street Journal.the banker who was told about the buy-backresigned123456Log[previous prefixcurrent prefix ] Subject Relative Clause0.7985471.599463.599913.453670.4980821.32120.1.599465.87759Figure 4: Mean 10.5the banker told about the buy-back resigned123456Log[previous prefixcurrent prefix ] Reduced Relative Clause0.7985471.599460.6222621.32120.1.599466.67629Figure 5: Mean: 16.44RC only the banker who was told about the buy-back resignedThe words who was cancel the main verb reading,and should make that condition easier to process.This asymmetry is borne out in graphs 4 and 5.
At\resigned" the probabilistic Earley parser predictsless reading time in the subject relative conditionthan in the reduced relative condition.This comparison veries that the same sorts ofphenomena treated in reanalysis and bounded paral-lelism parsing theories fall out as cases of the present,total parallelism theory.6.3 An entirely empirical grammarAlthough they used frequency estimates provided bycorpus data, the previous two grammars were par-tially hand-built.
They used a subset of the rulesfound in the sample of parsed text.
A grammar in-cluding all rules observed in the entire sample sup-ports the same sort of reasoning.
In this grammar,instead of just 2 NP rules there are 532, along with120 S rules.
Many of these generate analyses com-patible with prexes of the reduced relative clause atvarious points during parsing, so the expectation isthat the parser will be disconrming many more hy-potheses at each word than in the simpler example.Figure 6 shows the reading time predictions derivedfrom this much richer grammar.Because the terminal vocabulary of this richergrammar is so much larger, a comparatively largeamount of information is conveyed by the nouns\banker" and \buy-back" leading to high surprisalthe banker told about the buy-backresigned .2468101214Log[previous prefixcurrent prefix ]grammar from Wall Street Journal sample3.1397911.93699.590688.590212.9274711.949612.92146.50046Figure 6: Predictions of Earley parser on richergrammarvalues at those words.
However, the garden patheect is still observable at \resigned" where the pre-x probability ratio is nearly 10 times greater thanat either of the nouns.
Amid the lexical eects, theprobabilistic Earley parser is aected by the samestructural ambiguity that aects English speakers.7 Subject/Object asymmetryThe same kind of explanation supports an accountof the subject-object relative asymmetry (cf.
refer-ences in (Gibson, 1998)) in the processing of unre-duced relative clauses.
Since the Earley parser isdesigned to work with context-free grammars, thefollowing example grammar adopts a GPSG-styleanalysis of relative clauses (Gazdar et al, 1985, page155).
The estimates of the ratios for the two S[+R]rules are obtained by counting the proportion of sub-ject relatives among all relatives in the Treebank?sparsed Brown corpus7.
(3)0.33 NP ?
SPECNP NBAR0.33 NP ?
you0.33 NP ?
me1.0 SPECNP ?
DT0.5 NBAR ?
NBAR S[+R]0.5 NBAR ?
N1.0 S ?
NP VP0.86864638 S[+R] ?
NP[+R] VP0.13135362 S[+R] ?
NP[+R] S/NP1.0 S/NP ?
NP VP/NP1.0 VP/NP ?
V NP/NP1.0 VP ?
V NP1.0 V ?
saw1.0 NP[+R] ?
who1.0 DT ?
the1.0 N ?
man1.0 NP/NP ?
7In particular, relative clauses in the Treebank are ana-lyzed asNP ?
NP SBAR (rule 1)SBAR ?
WHNP S (rule 2)where the S con-tains a trace *T* coindexed with the WHNP.
The total num-ber of structures in which both rule 1 and rule 2 apply is5489.
The total number where the rst child of S is null is4768.
This estimate puts the total number of object relativesat 721 and the frequency of object relatives at 0.13135362 andthe frequency of subject relatives at 0.86864638.Grammar (3) generates both subject and object rela-tive clauses.
S[+R]?NP[+R] VP is the rule that gen-erates subject relatives and S[+R] ?
NP[+R] S/NPgenerates object relatives.
One might expect thereto be a greater processing load for object relatives assoon as enough lexical material is present to deter-mine that the sentence is in fact an object relative8.The same probabilistic Earley parser (modied tohandle null-productions) explains this asymmetry inthe same way as it explains the garden path eect.Its predictions, under the same linking hypothesisas in the previous cases, are depicted in graphs 7and 8.
The mean surprisal for the object relative isabout 5.0 whereas the mean surprisal for the subjectrelative is about 2.1.the man who saw you saw me12345Log[previous prefixcurrent prefix ] Subject Relative Clause1.5994601.0.2031591.599461.1.59946Figure 7: Subject relative clausethe man who you saw saw me12345Log[previous prefixcurrent prefix ] Object Relative Clause1.5994601.4.5279301.1.59946Figure 8: Object relative clauseConclusionThese examples suggest that a \total-parallelism"parsing theory based on probabilistic grammar cancharacterize some important processing phenomena.In the domain of structural ambiguity in particular,the explanation is of a dierent kind than in tradi-tional reanalysis models: the order of processing isnot theoretically signicant, but the estimate of itsmagnitude at each point in a sentence is.
Resultswith empirically-derived grammars suggest an ar-mative answer to Gibson and Pearlmutter?s ques-8The dierence in probability between subject and objectrules could be due to the work necessary to set up storagefor the ller, eectively recapitulating the HOLD Hypothesis(Wanner and Maratsos, 1978, page 119)tion: phrase-level contingent frequencies can do thework formerly done by other mechanisms.Pursuit of methodological principles 1, 2 and 3has identied a model capable of describing some ofthe same phenomena that motivate psycholinguisticinterest in other theoretical frameworks.
Moreover,this recommends probabilistic grammars as an at-tractive possibility for psycholinguistics by provid-ing clear, testable predictions and the potential fornew mathematical insights.ReferencesFred Attneave.
1959.
Applications of InformationTheory to Psychology: A summary of basic con-cepts, methods and results.
Holt, Rinehart andWinston.Thomas G. Bever.
1970.
The cognitive basis forlinguistic structures.
In J.R. Hayes, editor, Cog-nition and the Development of Language, pages279{362.
Wiley, New York.Taylor L. Booth and Richard A. Thompson.
1973.Applying probability measures to abstract lan-guages.
IEEE Transactions on Computers, C-22(5).Joan Bresnan.
1982.
Introduction: Grammars asmental representations of language.
In Joan Bres-nan, editor, The Mental Representation of Gram-matical Relations, pages xvii,lii.
MIT Press, Cam-bridge, MA.Eugene Charniak.
1993.
Statistical Language Learn-ing.
MIT Press.Ciprian Chelba and Frederick Jelinek.
1998.
Ex-ploiting syntactic structure for language mod-elling.
In Proceedings of COLING-ACL ?98, pages225{231, Montreal.Noam Chomsky.
1956.
Three models for the de-scription of language.
IRE Transactions on In-formation Theory, 2(3):113{124.Noam Chomsky.
1965.
Aspects of the Theory ofSyntax.
MIT Press, Cambridge MA.Jay Earley.
1970.
An ecient context-free pars-ing algorithm.
Communications of the Associa-tion for Computing Machinery, 13(2), February.Janet Dean Fodor and Fernanda Ferreira, editors.1998.
Reanalysis in sentence processing, vol-ume 21 of Studies in Theoretical Psycholingustics.Kluwer, Dordrecht.Marilyn Ford.
1989.
Parsing complexity and a the-ory of parsing.
In Greg N. Carlson and Michael K.Tanenhaus, editors, Linguistic Structure in Lan-guage Processing, pages 239{272.
Kluwer.Gerald Gazdar, Ewan Klein, Georey Pullum, andIvan Sag.
1985.
Generalized Phrase StructureGrammar.
Harvard University Press, Cambridge,MA.Edward Gibson and Neal J. Pearlmutter.
1998.Constraints on sentence processing.
Trends inCognitive Sciences, 2:262{268.Edward Gibson and Carson Schu?tze.
1999.
Disam-biguation preferences in noun phrase conjunctiondo not mirror corpus frequency.
Journal of Mem-ory and Language.Edward Gibson.
1998.
Linguistic complexity: local-ity of syntactic dependencies.
Cognition, 68:1{76.Ulf Grenander.
1967.
Syntax-controlled probabili-ties.
Technical report, Brown University Divisionof Applied Mathematics, Providence, RI.Frederick Jelinek and John D. Laerty.
1991.
Com-putation of the probability of initial substringgeneration by stochastic context-free grammars.Computational Linguistics, 17(3).Maryellen C. MacDonald, Neal J. Pearlmutter, andMark S. Seidenberg.
1994.
Lexical nature of syn-tactic ambiguity resolution.
Psychological Review,101(4):676{703.James McClelland and Mark St. John.
1989.
Sen-tence comprehension: A PDP approach.
Lan-guage and Cognitive Processes, 4:287{336.Don C. Mitchell, Fernando Cuetos, Martin M.B.Corley, and Marc Brysbaert.
1995.
Exposure-based models of human parsing: Evidence forthe use of coarse-grained (nonlexical) statisti-cal records.
Journal of Psycholinguistic Research,24(6):469{488.Srini Narayanan and Daniel Jurafsky.
1998.Bayesian models of human sentence processing.In Proceedings of the 19th Annual Conferenceof the Cognitive Science Society, University ofWisconsin-Madson.Mark-Jan Nederhof, Anoop Sarkar, and GiorgioSatta.
1998.
Prex probabilities from stochas-tic tree adjoining grammars.
In Proceedings ofCOLING-ACL ?98, pages 953{959, Montreal.Brian Roark and Mark Johnson.
1999.
Broad cover-age predictive parsing.
Presented at the 12th An-nual CUNY Conference on Human Sentence Pro-cessing, March.Stuart Shieber and Mark Johnson.
1993.
Variationson incremental interpretation.
Journal of Psy-cholinguistic Research, 22(2):287{318.Stuart Shieber.
1985.
Evidence against the context-freeness of natural language.
Linguistics and Phi-losophy, 8:333{343.Stephen Soule.
1974.
Entropies of probabilisticgrammars.
Information and Control, 25(57{74).Edward Stabler.
1991.
Avoid the pedestrian?s para-dox.
In Robert C. Berwick, Steven P. Abney, andCarol Tenny, editors, Principle-Based Parsing:computation and psycholinguistics, Studies in Lin-guistics and Philosophy, pages 199{237.
Kluwer,Dordrecht.Mark Steedman.
1992.
Grammars and processors.Technical Report TR MS-CIS-92-52, University ofPennsylvania CIS Department.Andreas Stolcke.
1995.
An ecient probabilis-tic context-free parsing algorithm that computesprex probabilities.
Computational Linguistics,21(2).Andreas Stolcke.
1997.
Linguistic knowledge andempirical methods in speech recognition.
AI Mag-azine, 18(4):25{31.Whitney Tabor, Cornell Juliano, and MichaelTanenhaus.
1997.
Parsing in a dynamical sys-tem: An attractor-based account of the interac-tion of lexical and structural constraints in sen-tence processing.
Language and Cognitive Pro-cesses, 12(2/3):211{271.Eric Wanner and Michael Maratsos.
1978.
An ATNapproach to comprehension.
In Morris Halle, JoanBresnan, and George A. Miller, editors, LinguisticTheory and Psychological Reality, chapter 3, pages119{161.
MIT Press, Cambridge, Massachusetts.C.S.
Wetherell.
1980.
Probabilistic languages: A re-view and some open questions.
Computing Sur-veys, 12(4).
