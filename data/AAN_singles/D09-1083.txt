Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 793?802,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPLearning Term-weighting Functions for Similarity MeasuresWen-tau YihMicrosoft ResearchRedmond, WA, USAscottyih@microsoft.comAbstractMeasuring the similarity between twotexts is a fundamental problem in manyNLP and IR applications.
Among the ex-isting approaches, the cosine measure ofthe term vectors representing the origi-nal texts has been widely used, where thescore of each term is often determinedby a TFIDF formula.
Despite its sim-plicity, the quality of such cosine similar-ity measure is usually domain dependentand decided by the choice of the term-weighting function.
In this paper, we pro-pose a novel framework that learns theterm-weighting function.
Given the la-beled pairs of texts as training data, thelearning procedure tunes the model pa-rameters by minimizing the specified lossfunction of the similarity score.
Com-pared to traditional TFIDF term-weightingschemes, our approach shows a significantimprovement on tasks such as judging thequality of query suggestions and filteringirrelevant ads for online advertising.1 IntroductionMeasuring the semantic similarity between twotexts is an important problem that has many use-ful applications in both NLP and IR communi-ties.
For example, Lin (1998) defined a similar-ity measure for automatic thesaurus creation froma corpus.
Mihalcea et al (2006) developed sev-eral corpus-based and knowledge-based word sim-ilarity measures and applied them to a paraphraserecognition task.
In the domain of web search, dif-ferent methods of measuring similarity betweenshort text segments have recently been proposedfor solving problems like query suggestion and al-ternation (Jones et al, 2006; Sahami and Heilman,2006; Metzler et al, 2007; Yih and Meek, 2007).Among these similarity measures proposed invarious applications, the vector-based methods arearguably the most widely used.
In this approach,the text being compared with is first representedby a term vector, where each term is associatedwith a weight that indicates its importance.
Thesimilarity function could be cosine (i.e., the innerproduct of two normalized unit term vectors, orequivalently a linear kernel), or other kernel func-tions such as the Gaussian kernel.There are essentially two main factors that de-cide the quality of a vector-based similarity mea-sure.
One is the vector operation that takes as in-put the term vectors and computes the final simi-larity score (e.g., cosine).
The other is how theseterm vectors are constructed, including the termselection process and how the weights are deter-mined.
For instance, a TFIDF scheme for mea-suring document similarity may follow the bag-of-words strategy to include all the words in the doc-ument when constructing the term vectors.
Theweight of each term is simply the product of itsterm frequency (i.e., the number of occurrencesin the document) and inverse document frequency(i.e., the number of documents in a collection thatcontain this term).Despite its simplicity and reasonable perfor-mance, such approach suffers from several weak-nesses.
For instance, the similarity measure is notdomain-dependent and cannot be easily adjustedto better fit the final objective, such as being ametric value used for clustering or providing betterranking results.
Researchers often need to experi-ment with variants of TFIDF formulas and differ-ent term selection strategies (e.g., removing stop-words or stemming) to achieve acceptable perfor-mance (Manning et al, 2008).
In addition, whenmore information is available, such as the positionof a term in the document or whether a term is partof an anchor text, incorporating it in the similaritymeasure in a principled manner may not be easy.793In this paper, we propose a general term-weighting learning framework, TWEAK, thatlearns the term-weighting function for the vector-based similarity measures.
Instead of using afixed formula to decide the weight of each term,TWEAK uses a parametric function of features ofeach term, where the model parameters are learnedfrom labeled data.
Although the weight of eachterm conceptually represents its importance withrespect to the document, tuning the model param-eters to optimize for such objectives may not bethe best strategy due to two reasons.
While thelabel of whether a pair of texts is similar is not dif-ficult to collect from human annotators1, the labelof whether a term in a document is important isoften very ambiguous and hard to decide.
Evenif such annotation issue can be resolved, aligningthe term weights with the true importance of eachterm may not necessarily lead to our real objec-tive ?
deriving a better similarity measure for thetarget application.
Therefore, our learning frame-work, TWEAK, assumes that we are given only thelabels of the pairs of texts being compared, suchas whether the two texts are considered similar byhuman subjects.TWEAK is flexible in choosing various lossfunctions that are close to the true objectives,while still maintaining the simplicity of the vector-based similarity measures.
For example, a systemthat implements the TFIDF cosine measure caneasily replace the original term-weighting scoreswith the ones output by TWEAK without changingother portions of the algorithm.
TWEAK is alsonovel compared to other existing learning meth-ods for similarity measures.
For instance, we donot learn the scores of all the terms in the vocab-ulary directly, which is one of the methods pro-posed by Bilenko and Mooney (2003).
Becausethe vocabulary size is typically large in the textdomain (e.g., all possible words in English), learn-ing directly the term-weighting scores may sufferfrom the data sparsity issue and cannot general-ize well in practice.
Instead, we focus on learningthe model parameters for features that each termmay have, which results in a much smaller fea-ture space.
TWEAK also differs from the modelcombination approach proposed by Yih and Meek(2007), where the output scores of different simi-larity measures are combined via a learned linear1As argued in (Sheng et al, 2008), low-cost labels maynowadays be provided by outsourcing systems such as Ama-zon?s Mechanical Turk or online ESP games.function.
In contrast, TWEAK effectively learnsa new similarity measure by tuning the term-weighting function and can potentially be comple-mentary to the model combination approach.As will be demonstrated in our experiments, inapplications such as judging the relevance of dif-ferent query suggestions and determining whethera paid-search ad is related to the user query,TWEAK can incorporate various kinds of term?document information and learn a term-weightingfunction that significantly outperforms the tradi-tional TFIDF scheme in several evaluation met-rics, when using the same vector operation (i.e.,cosine) and the same set of terms.We organize the rest of the paper as follows.Sec.
2 first gives a high-level view of our term-weighting learning framework.
We then formallydefine our model and present the loss functionsthat can be optimized for in Sec.
3.
Experimentson target applications are presented in Sec.
4.
Fi-nally, we compare our approach with some relatedwork in Sec.
5 and conclude the paper in Sec.
6.2 Problem StatementTo simplify the description, assume that the textswe are comparing are two documents.
A generalarchitecture of vector-based similarity measurescan be formally described as follows.
Given twodocuments Dpand Dq, a similarity function mapsthem to a real-valued number, where a highervalue indicates these two documents are seman-tically more related, considered by the measure.Suppose a pre-defined vocabulary set V ={t1, t2, ?
?
?
, tn} consists of all possible terms (e.g.,tokens, words) that may occur in the documents.Each document Dpis represented by a term vectorof length n: vp= (s1p, s2p, ?
?
?
, snp), where sip?
Ris the weight of term ti, and is determined by theterm-weighting function tw that depends on theterm and the document (i.e., sip?
tw(ti, Dp)).The similarity between documents Dpand Dqis then computed by a vector operation functionfsim: (vp,vq) ?
R, illustrated in Fig.
1.Determining the specific functions fsimand tweffectively decides the final similarity measure.For example, the functions that construct the tra-ditional TFIDF cosine similarity can be:fsim(vp,vq) ?vp?
vq||vp|| ?
||vq||(1)tw(ti, Dp) ?
tf(ti, Dp) ?
log(Ndf(ti))(2)794Figure 1: A general architecture of vector-basedsimilarity measureswhere N is the size of the document collection forderiving document frequencies, tf and df are thefunctions computing the term frequency and doc-ument frequency, respectively.In contrast, TWEAK also takes a specified vec-tor function fsimbut assumes a parametric term-weighting function tww.
Given the training data,it learns the model parameters w that optimize forthe designated loss function.3 ModelAs a specific instantiation of our learning frame-work, the term-weighting function used in this pa-per is a linear combination of features extractedfrom the input term and document.
In particular,the weight of term tiwith respect to document Dpissip= tww(ti, Dp) ?
?jwj?j(ti, Dp), (3)where ?jis the j-th feature function and wjis thecorresponding model parameter.As for the vector operation function fsim, weuse the same cosine function (Eq.
1).
Notice thatwe choose these functional forms for their sim-plicity and good empirical performance shown inpreliminary experiments.
However, other smoothfunctions can certainly be used.The choice of loss function for training modelparameters depends on the true objective in thetarget application.
In this work, we consider twodifferent learning settings: learning directly thesimilarity metric and learning the preference or-dering, and compare several loss functions exper-imentally.3.1 Learning Similarity MetricIn this setting, we assume that the learning al-gorithm is given a set of document pairs.
Eachof them is associated with a label that indicateswhether these two documents are similar (e.g., abinary label where 1 means similar and 0 oth-erwise) or the degree of similarity (e.g., a real-valued label ranges from 0 to 1), considered by thehuman subjects.
A training set of m examples canbe denoted as {(y1, (Dp1, Dq1)), (y2, (Dp2, Dq2)),?
?
?, (ym, (Dpm, Dqm))}, where ykis the labeland (Dpk, Dqk) is the pair of documents to com-pare.
Following the vector construction describedin Eq.
3, let vp1,vq1, ?
?
?
,vpm,vqmbe the corre-sponding term vectors of these documents.We consider two commonly used loss functions,sum-of-squares error and log loss2:Lsse(w) =12m?k(yk?
fsim(vpk,vqk))2 (4)Llog(w) =m?k?yklog(fsim(vpk,vqk))?
(1 ?
yk) log(1 ?
fsim(vpk,vqk)) (5)Eq.
4 and Eq.
5 can further be regularized byadding ?2||w||2 in the loss function, which mayimprove the performance empirically and alsoconstrain the range of the final term-weightingscores.
Learning the model parameters for min-imizing these loss functions can be done us-ing standard gradient-based optimization methods.We choose the L-BFGS (Nocedal and Wright,2006) method in our experiments for its guaran-tee to find a local minimum and fast convergence.The derivation of gradients is fairly straightfor-ward, which we skip here.Notice that other loss functions can also be usedin this framework.
Interested readers can refer to,say, (Bishop, 1995), for other loss functions andtheir theoretical justifications.3.2 Learning Preference OrderingIn many applications where the similarity measureis applied, the goal is to obtain a ranked list of thecandidate elements.
For example, in the task of2Although in theory the cosine function may return a neg-ative value and make the log-loss uncomputable, this canbe easily avoided in practice by selecting appropriate ini-tial model parameters and by constraining the term-weightingscores to be non-negative.795filtering irrelevant ads, a good similarity measureis expected to rank appropriate ads higher thanthe irrelevant ones.
A desired trade-off of false-positive (mistakenly filtered good ads) and false-negative (unfiltered bad ads) can be achieved byselecting a decision threshold.
The exact valueof the similarity measure, in this case, is not cru-cial.
For these applications, it is more important ifthe model parameters can better predict the pair-wise preference.
Learning preference ordering isalso motivated by the observation that preferenceannotations are generally more reliable than cat-egorical similarity labels (Carterette et al, 2008)and has been advocated recently by researchers(e.g., Burges et al (2005)).In the setting of learning preference ordering,we assume that each training example consistsof two pairs of documents, associated with a la-bel indicating which pair of documents is consid-ered more preferable.
A training set of m exam-ples can be formally denoted as {(y1, (xa1, xb1)),(y2, (xa2, xb2)), ?
?
?, (ym, (xam, xbm))}, wherexak= (Dpak, Dqak) and xbk= (Dpbk, Dqbk) aretwo pairs of documents and yk?
{0, 1} indicatesthe pairwise order preference, where 1 means xakshould be ranked higher than xbkand 0 otherwise.We use a loss function that is very similar tothe one proposed by Dekel et al (2004) for labelranking.
Let ?kbe the difference of the similarityscores of these two document pairs.
Namely,?k= fsim(vpak,vqak) ?
fsim(vpbk,vqbk)The loss function L, which can be shown to upperbound the pairwise accuracy (i.e., the 0-1 loss ofthe pairwise predictions), is:L(w) =m?k=1log(1+exp(?yk??k?(1?yk)?(?
?k)))(6)Similarly, Eq.
6 can be regularized by adding?2||w||2 in the loss function.4 ExperimentsWe demonstrate how to apply our term-weightinglearning framework, TWEAK, to measuring sim-ilarity for short text segments and to judgingthe relevance of an ad landing page given anquery.
In addition, we compare experimentally theperformance of using different training settings,loss functions and features against the traditionalTFIDF term-weighting scheme.4.1 Similarity for Short Text SegmentsJudging the similarity between two short text seg-ments is a crucial problem for many search and on-line advertising applications.
For instance, queryreformulation or query substitution needs to mea-sure the similarity between two queries.
A prod-uct keyword recommendation system needs to de-termine whether the given product name and thesuggested keyword is related.Because the length of the text segment is typi-cally short, ranging from a single word to a dozenwords, naively applying methods based on wordoverlapping such as the Jaccard coefficient leadsto poor results (Sahami and Heilman, 2006; Yihand Meek, 2007).
To overcome this difficulty, Sa-hami and Heilman (2006) proposes a Web-kernelfunction, which first expands the short text seg-ment by issuing it to a search engine as the query,and then collectes the snippets of the top results toconstruct a pseudo-document.
TFIDF term vectorsof the pseudo-documents are used to represent theoriginal short text segments and the cosine scoreof these two vectors is used as the similarity mea-sure.In this section, we apply TWEAK to thisproblem by replacing the TFIDF term-weightingscheme with the learned term-weighting function,when constructing the vectors from the pseudo-documents.
Our target application is query sug-gestion ?
automatically presenting queries that arerelated to the one issued by the user.
In particu-lar, we would like to use our similarity measureas a filter to determine whether queries suggestedby various algorithms and heuristics are indeedclosely related to the target query.4.1.1 Task & DataOur query suggestion dataset has been previouslyused in (Metzler et al, 2007; Yih and Meek, 2007)and is collected in the following way.
From thesearch logs of a commercial search engine, a ran-dom sample of 363 thousand queries from the top1 million most frequent queries in late 2005 werefirst taken as the query and suggestion candidates.Among them, 122 queries were chosen randomlyas our target queries; each of them had up to 100queries used as suggestions, generated by variousquery suggestion mechanisms.Given these pairs of query and suggestions, hu-man annotators judged the level of similarity usinga 4-point scale ?
Excellent, Good, Fair and Bad,796where Excellent and Good suggestions are consid-ered clearly related to the query intent, while theother two categories mean the suggestions are ei-ther too general or totally unrelated.
In the end,4,852 query/suggestion pairs that had effective an-notations were collected.
The distribution of thefour labels is: Excellent - 5%, Good - 12%, Fair -44% and Bad - 39%.For the simplicity of both presentation and im-plementation, query/suggestion pairs labeled asExcellent or Good are treated as positive examplesand the rest as negative ones.
Notice that TWEAKis not restricted in using only binary labels.
Forinstance, the pairwise preference learning settingonly needs to know which pair of objects beingcompared is more preferred.
The model and algo-rithm do not have to change regardless of whetherthe label reflects the degree of similarity (e.g, theoriginal 4-scale labels) or binary categories.
Forthe metric learning setting, an ordinal regressionapproach (e.g, (Herbrich et al, 2000)) can be ap-plied for multi-category labels.We used the same query expansion method asdescribed in (Sahami and Heilman, 2006).
Eachquery/suggestion was first issued to a commercialsearch engine.
The result page with up to 200snippets (i.e., titles and summaries) was used asthe pseudo-document to create the term vector thatrepresents the original query/suggestion.
As de-scribed earlier in Eq.
3, the weight of each termis a linear function of a set of predefined features,which are described next.4.1.2 FeaturesBecause the pseudo-documents are constructedusing the search result snippets instead of regularweb documents, special formatting or link infor-mation provided by HTML is not very meaning-ful.
Therefore, we focused on using features thatare available for plain-text documents, including:?
Bias: 1 for all examples.?
TF: We used log(tf + 1) as the term fre-quency feature, where tf is the number oftimes the term occurs in the original pseudo-document.?
DF: We used log(df + 1) as the documentfrequency feature, where df is the number ofdocuments in our collection that contain thisterm.?
QF: The search engine query log reflects thedistribution of the words/phrases in whichpeople are interested (Goodman and Car-valho, 2005; Yih et al, 2006).
We took a logfile with the most frequent 7.5 million queriesand used log(qf + 1) as feature, where qf isthe query frequency.?
Cap: A capitalized word may indicate beingpart of a proper noun or being more impor-tant.
When the term is capitalized in at leastone occurrence in the pseudo-document, thevalue of this feature is 1; otherwise, it is 0.?
Loc & Len: The beginning of a regular doc-ument often contains a summary with impor-tant words.
In the pseudo-documents cre-ated using search snippets, words that occurin the beginning come from the top results,which are potentially more relevant to theoriginal query/suggestion.
We created twospecific features using this location informa-tion.
Let loc be the word position of the targetterm and len be the total number of words ofthis pseudo-document.
The logarithmic valuelog(loc + 1) and the ratio loc/len were bothused as features.
In order for the learning pro-cedure to adjust the scaling, the logarithmicvalue of the document length, log(len + 1),was also used.4.1.3 ResultsWe conducted the experiments using 10-foldcross-validation.
The whole query/suggestionpairs were first split into 10 subsets of roughlyequal sizes.
Pairs with the same target query wereput in the same subset.
In each round, one subsetwas used for testing.
95% of the remaining datawas used for training the model and 5% was usedas the development set.
We trained six modelswith different values of the regularization hyper-parameter ?
?
{0.003, 0.01, 0.03, 0.1, 0.3, 1} anddetermined which model to use based on its per-formance on the development set, although the re-sult actually did not vary a lot as ?
changed.We compared three learning configurations?
metric learning with sum-of-squares error(Metricsse) and log loss (Metriclog) and thepairwise preference learning (Preference).
Thelearned term-weighting functions were used tocompare with the Web-kernel similarity function,which implemented the TFIDF term-weightingscheme using Eq.
2.797Table 1: The AUC scores, mean averaged preci-sion and precision at 3 of similarity measures us-ing different term-weighting functions.
The num-bers with the ?
sign are statistically significantlybetter compared to the Web-kernel method.Method AUC MAP Prec@3Web-kernel 0.732 0.540 0.556Metricsse0.775?
0.590 0.553Metriclog0.781?
0.585 0.545Preference 0.782?
0.597?
0.570We evaluated these models using three differentevaluation metrics: the AUC score, precision atk and MAP (mean averaged precision).
The areaunder the ROC curve (AUC) is typically used tojudge the overall quality of a ranking function.
Ithas been shown equivalent to the averaged accu-racy of the pairwise preference predictions of allpossible element pairs in the sequence, and can becalculated by the the following Wilcoxon-Mann-Whitney statistic (Cortes and Mohri, 2004):A(f ;x,y) =?i,j:yi>yjIf(xi)>f(xj)+12If(xi)=f(xj),where f is the similarity measure, x is the se-quence of compared elements and y is the labels.Another metric that is commonly used in a rank-ing scenario is precision at k, which computesthe accuracy of the top-ranked k elements and ig-nores the rest.
We used k = 3 in our task, whichmeans that for each target query, we selected threesuggestions with the highest similarity scores andcomputed the averaged accuracy.One issue of precision at k is that it does notprovide an overall quality measure of the rankingfunction.
Therefore, we also present MAP (meanaveraged precision), which is a single number thatsummarizes the performance of the ranking func-tion by considering both precision and recall, andhas been shown reliable in evaluating various in-formation retrieval tasks (Manning et al, 2008).Suppose there are m relevant elements in a se-quence, where r1, r2, ?
?
?
, rmare their locations.The averaged precision is then:AP =1mm?j=1Prec(rj),where Prec(rj) is the precision at rj.
We com-puted the averaged precision values of the 10 testsets in our cross-validation setting and report theirmean value.As shown in Table 1, all three learned term-weighting functions lead to better similarity mea-sures compared to the TFIDF scheme in terms ofthe AUC and MAP scores, where the preferenceorder learning setting performs the best.
However,for the precision at 3 metric, only the preferencelearning setting has a higher score than the TFIDFscheme, but the difference is not statistically sig-nificant3.
This is somewhat understandable sincethe design of our loss function focuses on the over-all quality instead of only the performance of thetop ranked elements.4.2 Query/Page SimilarityMeasuring whether a page is relevant to a givenquery is the main problem in information retrievaland has been studied extensively.
Instead of re-trieving web pages that are relevant to the queryaccording to the similarity measure, our goal isto implement a paid-search ad filter for commer-cial search engines.
In this scenario, textual adswith bid keywords that match the query can en-ter the auction and have a chance to be shown onthe search result page.
However, as the advertisersmay bid on keywords that are not related to theiradvertisements, it is important for the system to fil-ter irrelevant ads to ensure that users only receiveuseful information.
For this purpose, we measurethe similarity between the query and the ad land-ing page (i.e., the page pointed by the ad) and re-move the ad when the score of its landing page isbelow a pre-selected threshold4.Given a pair of query and ad landing page,while the query term vector is constructed usingthe same query expansion technique described inSec.
4.1, the page term vector can be created di-rectly from the web page since it is a regular doc-ument that contains enough content.
As usual,our goal is to produce a better similarity measureby learning the term-weighting functions for thesetwo types of vectors jointly.3We conducted a paired-t test on the 10 individualscores from the cross-validation results of each learned term-weighting function versus the Web-kernel method.
The re-sults are considered statistically significant when the p-valueis lower than 0.05.4One may argue that the filter should measure the simi-larity between the query and ad-text.
However, an ad willnot provide useful information to the user if the final destina-tion page is not relevant to the query, even if its ad-text looksappealing.7984.2.1 DataWe first collected a random sample of queries andpaid-search ads shown on a commercial search en-gine during 2008, as well as the ad landing pages.Judged by several human annotators, each pagewas labeled as relevant or not compared to the is-sued query.
After removing some pairs where thequery intent was not clear or the landing page wasno longer available, we managed to collect 13,341query/page pairs with reliable labels.
Amongthem, 8,309 were considered relevant and 5,032were labeled irrelevant.4.2.2 FeaturesIn this experiment, we tested the effect of usingdifferent features and experimented with three fea-ture sets: TF&DF, Plain-text and HTML.
TF&DFcontains only log(tf +1), log(df +1) and the biasfeature.
The goal of using this feature set is totest whether we can learn a better term-weightingfunction given the same amount of information asthe TFIDF scheme has.
The second feature set,Plain-text, consists of all the features described inSec.
4.1.2.
As mentioned earlier, this set of fea-tures can be used for regular text documents thatdo not have special formatting information.
Fi-nally, feature set HTML is composed of all thefeatures used in Plain-text plus features extractedfrom some special properties of web documents,including:?
Hypertext: The anchor text in an HTMLdocument usually provides important infor-mation.
If there is at least one occurrence ofthe term that appears in some anchor text, thevalue of this feature is 1; otherwise, it is 0.?
URL: A web document has a uniquely usefulproperty ?
the name of the document, whichis its URL.
If the term is a substring of theURL, then the value of this feature is 1; oth-erwise, it is 0.?
Title: The value of this feature is 1 when theterm is part of the title; otherwise, it is 0.?
Meta: Besides Title, several meta tags usedin the HTML header explicitly show the im-portant words selected by the page author.Specifically, whether the term is part of ameta-keyword is used as a binary feature.Whether the term is in the meta-descriptionsegment is also used.Table 2: The AUC scores, true-positive rates atfalse-positive rates 0.1 and 0.2 of the ad filterbased on different term-weighting functions.
Thedifference between any pair of numbers of thesame evaluation metric is statistically significant.Method AUC TPRfnr=0.1TPRfnr=0.2TFIDF 0.794 0.527 0.658TF&DF 0.806 0.430 0.639Plain-text 0.832 0.503 0.704HTML 0.855 0.568 0.750Because the term vector that represents thequery is created from the pseudo-document (i.e., acollection of search snippets), the values of theseHTML-specific features are all 0 for the queryterm vector.
This set of features are only useful fordeciding the weights of the terms in a page termvector.4.2.3 ResultsWe split our data into 10 subsets and conductedthe experiments using the same 10-fold cross-validation setting described in Sec.
4.1.3, includ-ing how we used the development set to select theregularization hyper-parameter ?.
The pairs thathave the same target query were again put in thesame subsets.
We used only the preference or-dering learning setting for its good performanceshown in the previous set of experiments.
Modelscompared here were learned from the three dif-ferent sets of features, as well as the same fixedTFIDF term-weighting formula (i.e., Eq.
2) usedin Sec.
4.1.
Table 2 reports the averaged resultsof the 10 testing sets in AUC, as well as the true-positive rates at two low false-positive rate points(FPR=0.1 and FPR=0.2).
The difference betweenany pair of numbers of the same evaluation metricis statistically significant5.As we can see from the table, having more fea-tures does lead to a better term-weighting func-tion.
With all features (i.e., HTML), the modelachieves the highest AUC score among all con-figurations.
Features available in plain-text doc-uments (i.e., Plain-text) other than term frequencyand document frequency can still improve the per-formance significantly.
When only the TF and DFfeatures are available, the learned term-weightingfunction still outperforms the TFIDF scheme, al-5We conduct paired-t tests as described in Sec.
4.1.3.
Allthe p-values after Bonferroni correction are less than 0.01.799Figure 2: ROC Curves of the ad filters using dif-ferent term-weighting functions00.20.40.60.810 0.1 0.2 0.3 0.4 0.5 0.6TruePositiveRateFalse Positive RateROC CurvesTFIDFTF&DFPlain-textHTMLthough the improvement gain is much smallercompared to the other two settings.Notice that the behaviors of these models at dif-ferent false-positive regions varies from the tra-ditional TFIDF scheme.
At a low false-positivepoint (e.g., FPR=10%), only the model that usesall features performs better than TFIDF.
This phe-nomenon can be clearly observed from the ROCcurves plotted in Fig.
2, where the models weretrained using half of the data and applied to theother half to generate the similarity scores.
If onlythe performance at a very low false-positive ratematters, TWEAK can still be easily adjusted bymodifying the loss function using techniques suchas training with utility (Domingos, 1999; Morik etal., 1999).5 Related WorkOur term-weighting learning framework can beanalogous to the ?Siamese?
architecture for learn-ing jointly two neural networks that share the sameset of model weights (Bromley et al, 1993).
Forinstance, a term vector can be viewed as a verylarge single-layer neural network, where each termin the vocabulary is a node that takes as input thefeatures and outputs the learned term-weightingscore.
Previous applications of this learning ma-chine are typically problems in image processingor computer vision.
For example, Chopra et al(2005) designed an algorithm to learn a similar-ity metric for face verification, which is based onthe difference between two vectors.
In our earlierexperiments (not reported in this paper) of usingvector difference instead of cosine, we did not ob-serve positive outcomes.
We hypothesize that be-cause the length of the term vector in our problemcan be extremely large (i.e., the size of the vocab-ulary), a similarity measure based on vector differ-ence can easily be affected by terms that do not oc-cur in both documents, even when the co-occurredterms have very large weights.Learning similarity measures for text has alsobeen proposed by several researchers.
For in-stance, Bilenko and Mooney (2003) applied SVMsto directly learn the weights of co-occurred wordsin two text records, which are then used formeasuring similarity for duplicate detection.
Al-though this approach worked moderately well inthe database domain, it may not be suitable to han-dle general text similarity problems for two rea-sons.
First, the vocabulary size is typically large,which results in a very high dimensional featurespace for the learning problem.
It is very likelythat some rarely used and yet important terms oc-cur in the testing documents but not in the trainingdata.
The weights of those terms may not be reli-able or even be learned.
Second, this learning ap-proach can only learn the importance of the termsfrom the labels of whether two texts are consideredsimilar, how to incorporate the basic informationof these terms such as the position or query logfrequency is not clear.An alternative learning approach is to combinemultiple similarity measures with learned coeffi-cients (Yih and Meek, 2007), or to apply the tech-nique of kernel alignment (Cristianini et al, 2002)to combining a set of kernel functions for tun-ing a more appropriate kernel based on labeleddata.
This type of approaches can be viewedas constructing an ensemble of different existingsimilarity measures without modifying the termweighting function, and may not generate math-ematically equivalent similarity functions as de-rived by TWEAK.
Although learning in this ap-proach is usually very fast due to the model formand the small number of parameters to learn, itsimprovement is limited by the quality of the in-dividual similarity measures.
In spite of the fun-damental difference between our approach andthis combination method, it is worth noticing thatthese two approaches are in fact complementaryto each other.
Having a newly learned term-weighting function effectively provides a new sim-ilarity measure and therefore can be combinedwith other measures.8006 ConclusionsIn this paper, we presented a novel term-weightinglearning framework, TWEAK, for improving sim-ilarity measures based on term vectors.
Given thelabels of text pairs for training, our method learnsthe model parameters to calculate the score of eachterm, optimizing the desired loss function that issuitable for the target application.
As we demon-strated in the experiments, TWEAK with differ-ent features and training settings significantly out-performs the traditional TFIDF term-weightingscheme.TWEAK also enjoys several advantages com-pared to existing methods.
From an engineer-ing perspective, adopting the new term-weightingscores produced by our model is straightforward.If a similarity measure has been implemented,the algorithm need not be changed ?
only theterm vectors need to be updated.
From the learn-ing perspective, additional information regard-ing each term with respect to the document cannow be incorporated easily via feature functions.Weights (i.e., model parameters) of these featuresare learned in a principled way instead of beingadjusted manually.
Finally, TWEAK is potentiallycomplementary to other methods for improvingthe similarity measure, such as model combinationof various types of similarity measures (Yih andMeek, 2007) or different term vector constructionmethods such as Latent Semantic Analysis (Deer-wester et al, 1990).In the future, we plan to explore more vector op-erations other than the inner-product (i.e., cosine)as well as different functional forms of the term-weighting function (e.g.
log-linear instead of lin-ear).
Designing new loss functions to better fit thetrue objectives in various target applications andstudying the quality of a similarity measure basedon both term-weighting learning and model com-bination are also on our agenda.
In terms of appli-cations, we would like to apply TWEAK in otherproblems such as paraphrase recognition and near-duplicate detection.AcknowledgmentsThe author thanks the anonymous reviewers fortheir valuable comments and is grateful to AselaGunawardana, Chris Meek, John Platt and MishaBilenko for many useful discussions.ReferencesMikhail Bilenko and Raymond J. Mooney.
2003.Adaptive duplicate detection using learnable stringsimilarity measures.
In Proceedings of KDD-2003,pages 39?48.Christopher M. Bishop.
1995.
Neural Networks forPattern Recognition.
Oxford University Press.Jane Bromley, James W. Bentz, Le?on Bottou, Is-abelle Guyon, Yann LeCun, Cliff Moore, EduardSa?ckinger, and Roopak Shah.
1993.
Signature ver-ification using a ?Siamese?
time delay neural net-work.
International Journal Pattern Recognitionand Artificial Intelligence, 7(4):669?688.Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,Matt Deeds, Nicole Hamilton, and Greg Hullender.2005.
Learning to rank using gradient descent.
InProceedings of the 22nd International Conferenceon Machine learning (ICML-05), pages 89?96.Ben Carterette, Paul N. Bennett, David MaxwellChickering, and Susan Dumais.
2008.
Here orthere: Preference judgments for relevance.
In Pro-ceedings of the 30th European Conference on Infor-mation Retrieval (ECIR 2008).Sumit Chopra, Raia Hadsell, and Yann LeCun.
2005.Learning a similarity metric discriminatively, withapplication to face verification.
In Proceedings ofCVPR-2005, pages 539?546.Corinna Cortes and Mehryar Mohri.
2004.
AUC opti-mization vs. error rate minimization.
In Advancesin Neural Information Processing Systems (NIPS2003).Nello Cristianini, John Shawe-Taylor, Andre Elisseeff,and Jaz Kandola.
2002.
On kernel-target algnment.In Advances in Neural Information Processing Sys-tems 14, pages 367?373.
MIT Press.Scott Deerwester, Susan Dumais, George Furnas,Thomas Landauer, and Richard Harshman.
1990.Indexing by latent semantic analysis.
Journalof the American Society for Information Science,41(6):391?407.Ofer Dekel, Christopher D. Manning, and YoramSinger.
2004.
Log-linear models for label ranking.In Advances in Neural Information Processing Sys-tems (NIPS 2003).Pedro Domingos.
1999.
MetaCost: A general methodfor making classifiers cost-sensitive.
In Proceedingsof KDD-1999, pages 155?164.Joshua Goodman and Vitor R. Carvalho.
2005.
Im-plicit queries for email.
In Proceedings of the 2ndconference on Email and Anti-Spam (CEAS-2005).Ralf Herbrich, Thore Graepel, and Klaus Obermayer.2000.
Large margin rank boundaries for ordinalregression.
Advances in Large Margin Classifiers,pages 115?132.801Rosie Jones, Benjamin Rey, Omid Madani, and WileyGreiner.
2006.
Generating query substitutions.
InProceedings of the 15th World Wide Web Confer-ence.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proc.
of COLING-ACL 98.Christopher D. Manning, Prabhakar Raghavan, andHinrich Schu?tze.
2008.
Introduction to InformationRetrieval.
Cambridge University Pres.Donald Metzler, Susan Dumais, and Christopher Meek.2007.
Similarity measures for short segments oftext.
In Proceedings of the 29th European Confer-ence on Information Retrieval (ECIR 2007).Rada Mihalcea, Courtney Corley, and Carlo Strappa-rava.
2006.
Corpus-based and knowledge-basedmeasures of text semantic similarity.
In Proceedingsof AAAI-2006.Katharina Morik, Peter Brockhausen, and ThorstenJoachims.
1999.
Combining statistical learningwith a knowledge-based approach ?
a case study inintensive care monitoring.
In Proceedings of the Six-teenth International Conference on Machine Learn-ing (ICML-1999), pages 268?277.Jorge Nocedal and Stephen Wright.
2006.
NumericalOptimization.
Springer, 2nd edition.Mehran Sahami and Timothy D. Heilman.
2006.
Aweb-based kernel function for measuring the simi-larity of short text snippets.
In Proceedings of the15th World Wide Web Conference.Victor S. Sheng, Foster Provost, and Panagiotis G.Ipeirotis.
2008.
Get another label?
Improving dataquality and data mining using multiple, noisy label-ers.
In Proceedings of KDD-2008, pages 614?622.Wen-tau Yih and Christopher Meek.
2007.
Improvingsimilarity measures for short segments of text.
InProceedings of AAAI-2007, pages 1489?1494.Wen-tau Yih, Joshua Goodman, and Vitor Carvalho.2006.
Finding advertising keywords on web pages.In Proceedings of the 15th World Wide Web Confer-ence.802
