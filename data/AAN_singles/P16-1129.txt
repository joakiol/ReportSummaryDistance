Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1361?1371,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsTowards Constructing Sports News from Live Text CommentaryJianmin Zhang Jin-ge Yao Xiaojun WanInstitute of Computer Science and Technology, Peking University, Beijing 100871, ChinaKey Laboratory of Computational Linguistic (Peking University), MOE, China{zhangjianmin2015, yaojinge, wanxiaojun}@pku.edu.cnAbstractIn this paper, we investigate the possibil-ity to automatically generate sports newsfrom live text commentary scripts.
As apreliminary study, we treat this task as aspecial kind of document summarizationbased on sentence extraction.
We for-mulate the task in a supervised learningto rank framework, utilizing both tradi-tional sentence features for generic docu-ment summarization and novelly designedtask-specific features.
To tackle the prob-lem of local redundancy, we also propose aprobabilistic sentence selection algorithm.Experiments on our collected data fromfootball live commentary scripts and cor-responding sports news demonstrate thefeasibility of this task.
Evaluation resultsshow that our methods are indeed appro-priate for this task, outperforming severalbaseline methods in different aspects.1 IntroductionThere are a huge number of sports games playedeach day.
It is demanding and challenging to writecorresponding news reports instantly after variousgames.
Meanwhile, live text commentary servicesare available on the web and becoming increas-ingly popular for sports fans who do not have ac-cess to live video streams due to copyright reasons.Some people may also prefer live texts on portabledevices.
The emergence of live texts has producedhuge amount of text commentary data.
To the bestof our knowledge, there exists few studies aboututilizing this rich data source.Manually written sports news for match reportusually share the same information and vocabularyas live texts for the corresponding sports game.Sports news and commentary texts can be treatedas two different sources of descriptions for thesame sports events.
It is tempting to investigatewhether we can utilize the huge amount of livetexts to automatically construct sports news, typ-ically in a form of match report.
Building such asystem will largely relax the burden of sports newseditors, making them free from repetitive tediousefforts for writing while producing sports newsmore efficiently.In this work, we study the possibility to con-struct sports news in the form of match reportsfrom given live text commentary scripts.
As a con-crete example we collect live text data and corre-sponding news reports for football (called soccermore often in the United States) games and con-duct our study thereby.
However, our methodsand discussions made in this paper can be triviallyadapted to other types of sports games as well.As a preliminary study, we treat this task asa special kind of document summarization: ex-tracting sentences from live texts to form a matchreport as generated news.
However, generatingsports news from live texts is still challenging dueto some unique properties of live text commentaryscripts.
For almost every minute of the game thereare usually several sentences describing variouskinds of events.
Texts are ordered and organizedby the timeline, without apparent highlights formany important events1.
Descriptions are usuallyin short sentences, which is not helpful for sen-tence scoring and selection in general.
The com-mentators may tend to use similar, repeated wordsdescribing the same type of key events, which maybring additional challenges to traditional summa-rization methods that are designed to avoid literalrepetitions in nature.
As a result, naively treatingthe task as an ordinary document summarization1Some live texts services may use different textual formatfor scoring events, which is not enough for our more generalpurposes.1361problem can hardly lead to the construction of rea-sonable sports news reports.To overcome these difficulties, we explore somespecific features of live text commentary scriptsand formulate a system based on supervised learn-ing to rank models for this task.
In order to tacklethe local redundancy issue, we also propose aprobabilistic sentence selection strategy.We summarize our contributions as follows:?
We originally study the task of sports newsconstruction from live text commentary andwe build datasets for supervised learning andevaluation for this task.?
We formulate the task in a learning to rankframework, utilizing both traditional featuresfor document summarization and novel task-specific features during supervised learning.?
We propose a probabilistic sentence selectionalgorithm to address the issue of local redun-dancy in description.?
We conduct a series of experiments on a realdataset and the evaluation results verify theperformance of our system.
Results suggestthat constructing sports news from live textsis feasible and our proposed methods can out-perform a few strong baselines.2 Problem Statement2.1 Task DescriptionIn this work, we treat the task of constructingsports news from live text commentary as a spe-cial kind of document summarization: extractingsentences from live text scripts to form a matchreport.Formally, given a piece of live text commen-tary containing a collection of candidate sen-tences S = {s1, s2, .
.
.
, sn} describing a partic-ular sports game G, we need to extract sentencesto form a summary of G which are suitable to beformed as sports news.
The total length should notexceed a pre-specified length budget B.The overall framework of generic documentsummarization can still be retained for this prelim-inary study.
We first rank all candidate sentencesaccording to a sentence scoring scheme and thenselect a few sentences according to certain criteriato form the final generated news.2.2 Data CollectionTo the best of our knowledge, there does not ex-ist off-the-shelf datasets for evaluating sports newsconstruction.
Therefore we have to build a newdataset for this study.
We will focus on live textscripts for football (soccer) games as a concreteinstance, since football live texts are the easiestto collect.
Note that the methods and discussionsdescribed in this paper can trivially generalize toother types of sports games.Meanwhile, live text commentary services areextremely popular in China, where sports fansin many cases do not have access to live videostreams due to copyright reasons.
The most influ-ential football live services are Sina Sports Live2and 163 Football Live3.
For evaluation purposeswe need to simultaneously collect both live textsand news texts describing the same sports games.Due to the convenience and availability of paralleldata collection, we build our dataset from Chinesewebsites.
For most football games, there existboth live text scripts recorded after the games andhuman-written news reports on both Sina Sportsand 163 Football.
We crawl live text commentaryscripts for 150 football matches on Sina SportsLive.
Figure 1 displays an example of the formatof the live texts, containing the main commentarytext along with information of the current timelineand scoreline.??????????????
(Lewandowski passes the ball to the right and finds M?ller)???
42'(first half 42') 2-0?????????
(M?ller stops the ball and gets a direct shot)???
43'(first half 43') 2-0??????????????
(Fast reaction from Cech to tip the ball over the bar)???
43'(first half 43') 2-0Text Commentary Scoreline TimelineFigure 1: Illustration of the live text formatFor every match, two different correspondingsports news reports are collected from Sina SportsLive and 163 Football Matches Live, respec-tively.
These news reports are manually writtenby professional editors and therefore suitable to betreated as gold-standard news for our task.
The av-erage number of sentences in the live texts for onematch is around 242, containing around 4,590 Chi-nese characters for that match.
The gold-standardnews reports contain 1,185 Chinese characters onaverage, forming around 32 sentences.For both the gold-standard news and live textcommentary scripts, we split them into sentences2http://match.sports.sina.com.cn/3http://goal.sports.163.com/1362and then use a Chinese word segmentation tool4to segment the sentences into word sequences.
Foreach sentence, we compute its TFIDF vector forcalculating literal cosine similarity when used.3 Constructing Sports News via SentenceExtractionWe build a system to automatically constructmatch reports from live text commentary.
Sincewe have described the new challenges for this task,we may design a number of relevant features toaddress them.
In this work, we cast the probleminto supervised sentence extraction.
Supervisedapproaches, especially those based on learning torank (LTR), can better utilize the power of vari-ous task-dependent features (Shen and Li, 2011;Wang et al, 2013).
For a given specific sportsgame, we extract features from all candidate sen-tences in the corresponding live texts and score thesentences using a learning to rank (LTR) modellearned from the training data (Section 3.1).
Thenwe select a few of them according to the rankingscores to form the constructed news (Section 3.3).3.1 Training Data FormatSupervised sentence scoring models based on LTRrequire input training data in the format of (xi, yi)for each candidate sentence si, where xiis the fea-ture vector and yiis the preference score.
Thefeature vector x is described in Section 3.2.
Thescore y will be defined to reflect the importance,or the tendency to be included in the final news re-port, of the candidate sentence.
In this work wefirst calculate a group of ROUGE-2 F-scores (cf.Section 4.4.1) of the candidate sentence, treatingeach sentence in the gold-standard news as refer-ence.
The score y of the candidate sentence is thenset to be the maximum among those ROUGE-2 F-scores.
Later we will see that this scores can in-deed serve as good learning targets.3.2 FeaturesIn this work, we extract both common featureswhich have been widely used for generic docu-ment summarization (Shen and Li, 2011; Wang etal., 2013) and novel task-specific features aimingat proper sports news generation from live broad-cast script.
The features are described as follows.4We use the ICTCLAS toolkit for word segmentation inthis work: http://ictclas.nlpir.org/3.2.1 Basic FeaturesPosition: The position of each candidate sentence.Suppose there are n sentences in a document.
Forthe i-th sentence, its position feature is computedas 1?i?1n.Length: The number of words contained in thesentence after stopwords removal.Number of stopwords: The Number of stop-words contained in each sentence.
Sentences withmany stopwords should be treated as less impor-tant candidates.Sum of word weights: The sum of TF-IDFweights for each word in a sentence.Similarity to the Neighboring Sentences: Wecalculate the average cosine similarity of a candi-date sentence to its previous N and the next Nneighboring sentences.
We set N as 1 and 2 hereto get two different features.3.2.2 Task-specific FeaturesThe task we study has some unique propertiescompared with generic document summarization.For instance, in live text commentary for sportsgames such as football matches, the scripts notonly contain descriptive texts but also the score-line and timeline information.
Such informationcan be utilized to judge the quality of candidatesentences as well.
We extract a rich set of newfeatures, which can be grouped into four types:Explicit highlight markers: Explicit highlightmarker words in a sentence are usually good in-dicators for its importance.
Sentences with moremarker words are more probable to be extractedand contained in news or reports for the games.For example, words such as ???
(scores)?
and???
(red card)?
in a sentence may indicate thatthe sentence is describing important events andwill be more likely to be extracted.
We collect ashort list of 25 explicit highlight marker words5.For each marker word we create a binary feature todenote the presence or absence of that markers ineach candidate sentence.
We also use the numberof markers as one feature, with the intuition thatcontaining more marker words typically suggestsmore important sentences.Scoreline features: An audience of sportsgames typically pays more attention on score-line changes, especially those deadlock-breakingscores that break the game from ties.
We use three5We include the full list of marker words in the supple-mentary materials due to the space limit.1363binary features to describe the scoreline informa-tion of each candidate sentence:?
An indicator feature on whether there wasa change of scoreline when the narrator orcommentator was producing that sentence.?
An indicator feature on whether the distancebetween the candidate sentence and the pre-vious closest sentence with a change of score-line is less than or equal to 5.?
An indicator feature showing whether thegame was a draw or not at that time.To better describe these features we give an exam-ple in Figure 2, where S1-S3 corresponds to theabove three binary features, respectively.Text Commentary Timeline Scoreline S1 S2 S3Both sides take advantages ofcounter attacks.
32' 1-1 0 0 01-2??
33' 1-2 1 1 1Alexis!!
33' 1-2 0 1 1?zil finds the teammate bylinefollowed by a low cross to far post,Alexis sends the ball into the net!34' 1-2 0 1 1Leicester players are unhappy.
34' 1-2 0 1 1Figure 2: An example of scoreline featuresTimeline features: The timestamp on each sen-tence can reflect the progress of a sports game.We divide a match into five different stages as???
(not started)?, ????
(first half)?, ?????
(half-time)?, ????
(second half)?
and ???(full-time)?.
Then we use five binary featuresto represent whether the sentence was describing aspecific stage.
We also use the specific time-stamp(in integral minutes) of the candidate sentence inthe match as an additional feature.
Suppose thereare n minutes of the match (typically 90 minutesfor football), for sentences on the time-stamp ofthe i-th minute , this feature is computed asin.Player popularity: Sports fans usually focusmore on the performance of the star players or in-form players during the games.
We design twofeatures to utilize player information described ina candidate sentence: the number of players con-tained in the sentence and the sum of their popu-larity measurements.
In this work the popularityof a player is measured using search engines fornews: we use the name of a certain player as in-put query to Baidu News6, and use the numberof recent news retrieved to measure this player?spopularity.6http://news.baidu.com/3.3 Sentence SelectionOnce we have the trained LTR model, we canimmediately construct news reports by selectingsentences with the highest scores.
Unfortunatelythis simple strategy will suffer from redundancyin commentary, since the LTR scores are pre-dicted independently for each sentence and assign-ing high scores for repeated commentary texts de-scribing the same key event.
Therefore, specialcare is needed in sentence selection.
In princi-ple, any In this work we propose a probabilisticapproach based on determinantal point processes(Kulesza and Taskar, 2012, DPPs).
This approachcan naturally integrate the predicted scores fromthe LTR model while trying to avoid certain re-dundancy by producing more diverse extractions7.
We first review some background knowledgeon the model.
More details can be found in thecomprehensive survey (Kulesza and Taskar, 2012)covering this topic.3.3.1 Determinantal Point ProcessesDeterminantal point processes (DPPs) are distri-butions over subsets that jointly prefer quality ofeach item and diversity of the whole subset.
For-mally, a DPP is a probability measure definedon all possible subsets of a group of items Y ={1, 2, .
.
.
, N}.
For every Y ?
Y we have:P(Y ) =det(LY)det(L+ I)where L is a positive semidefinite matrix typi-cally called an L-ensemble.
LY?
[Lij]i,j?Yde-notes the restriction of L to the entries indexedby elements of Y , and det(L?)
= 1.
The termdet(L + I) is the normalization constant whichhas a succinct closed-form and easy to compute.We can define the entries of L as follows:Lij= qi?>i?jqj= qi?
sim(i, j) ?
qj(1)where we can think of qi?
R+as the quality ofan item i and ?i?
Rnwith ?
?i?2= 1 denotesa normalised feature vector such that sim(i, j) ?
[?1, 1] measures similarity between item i anditem j.
This simple definition gives rise to a distri-bution that places most of its mass on sets that areboth high quality and diverse.
This is intuitive in a7Many other approaches can also be used to achieve simi-lar effect, such as submodular maximization (Lin and Bilmes,2010).
We leave the comparison with these alternatives forfuture work study.1364geometric sense since determinants are closely re-lated to volumes; in particular, det(LY) is propor-tional to the volume spanned by the vectors qi?ifor i ?
Y .
Thus, item sets with both high-qualityand diverse items will have the highest probability(Figure 3).
(a)(b) (c)Figure 3: (a) The DPP probability of a set Y de-pends on the volume spanned by vectors qi?ifori ?
Y (b) As length increases, so does volume.
(c)As similarity increases, volume decreases.3.3.2 Sentence SelectionIn this work we formulate the sentence selectionproblem as maximum a posteriori (MAP) infer-ence for DPPs, i.e.
finding argmaxYlog det(LY).It is known that MAP inference for DPPs is NP-hard (Gillenwater et al, 2012).
Therefore weadopt the greedy approximate inference procedureused by Kulesza and Taskar (2011) which is fastand performs reasonably well in practice.The remaining question is how to define the L-ensemble matrix L, or equivalently how to de-fine itemwise quality qiand pairwise similaritysim(i, j), where each item corresponds to a can-didate sentence.
Since we have predicted scoresfor all candidates with the LTR model, we simplyset qito be the ranking score for sentence i.The definition of sim(i, j) is more subtle since itdirectly address specific types of redundancy.
Themost straightforward definition is to use literal co-sine similarity.
This is used for traditional sum-marization problems (Kulesza and Taskar, 2011).However, the problem for constructing sportsnews from live broadcast script is rather different.A live broadcast script may use literally similarsentences to describe similar types of events hap-pened at different time stamps.
Simply removingsentences that are similar in content may becomeharmful to the preservation of important events8.One typical redundancy that we found in thisstudy is local description redundancy.
In live texts,8Using cosine similarity for all similarity-dependentmethods performs poorly in our experiments.
Therefore wewill not discuss cosine similarity in more details later.an important event (such as goals) may be stressedmultiple times consecutively by the commentator.Therefore in this study we use local literal sim-ilarity as a first attempt.
Formally, the pairwisesimilarity is defined as:sim(i, j) ={0, if max{|ip?
jp|, |it?
jt|} > 1,cos(i, j), otherwise,where the subscripts ipand itdenotes positionand timestamp for sentence i, respectively.
Inother words we treat sentences written consecu-tively within one minute as local descriptions andonly calculate literal cosine similarity for them.4 Experimental Setup4.1 Data PreparationAs described earlier in Section 2.2, we evaluate theperformance of different systems on our collecteddataset.
To utilize the dataset more sufficiently anddraw more reliable conclusions, we perform cross-validation during evaluation.
Specifically, we ran-domly divide the dataset into three parts with equalsizes, i.e.
each has 50 pairs of live texts and gold-standard news.
Each time we set one of them asthe test set and use the remaining two parts fortraining and validation.
We will mainly report theaveraged results from all three folds.
For unsuper-vised baselines the results are calculated similarlyvia averaging the performance on the test set.4.2 Learning to RankFor predicting ranking scores we use the RandomForest (RF) (Breiman, 2001) ensemble ranker ofLambdaMart (Wu et al, 2010), implemented inRankLib9.
We set the number of iterations to 300and the sampling rate to 0.3.
Using different val-ues did not show real differences.4.3 Compared Baseline MethodsOur system is compared with several baselines,typically traditional summarization approaches:HeadTail: Using head and tail sentences only.Commentators usually describe some basic infor-mation of the two sides at the beginning and sum-marize the scoring events in the end of commen-tary.
This baseline resembles the baseline of lead-ing sentences for traditional summarization.9http://sourceforge.net/p/lemur/wiki/RankLib/; In prelim-inary experiments, we contrasted RF with support vector re-gression predictor as well as other pairwise and listwise LTRmodels.
We found that RF consistently outperformed others.1365Centroid: In centroid-based summarization(Radev et al, 2000), a pseudo-sentence of the doc-ument called centroid is calculated.
The centroidconsists of words with TFIDF scores above a pre-defined threshold.
The score of each sentence isdefined by summing the scores based on differentfeatures including cosine similarity of sentenceswith the centroid, position weight and cosine sim-ilarity with the first sentence.LexRank: LexRank (Erkan and Radev, 2004)computes sentence importance based on the con-cept of eigenvector centrality in a graph represen-tation of sentences.
In this model, a connectivitymatrix based on intra-sentence cosine similarity isused as the adjacency matrix of the graph repre-sentation of sentences.ILP: Integer linear programming (ILP) ap-proaches (Gillick et al, 2008) cast document sum-marization as combinatorial optimization.
An ILPmodel selects sentences by maximizing the sum offrequency-induced weights of bigram concepts10contained in the summary.Highlight: This method is designed to show theeffect of using merely the explicit highlight mark-ers described in Section 3.2.2.
The importance ofa sentence is represented by the number of high-light markers it includes.For fair comparisons the length of each con-structed news report is limited to be no more than1,000 Chinese characters, roughly the same withthe average length of the gold-standard news.
Notethat we do not use the traditional MMR redun-dancy removal algorithm based on literal similar-ity (Carbonell and Goldstein, 1998) since we findonly ignorable differences between using MMR ornot for all systems.4.4 Evaluation Methods and Metrics4.4.1 Automatic EvaluationSimilar to the evaluation for traditional summa-rization tasks, we use the ROUGE metrics (Linand Hovy, 2003) to automatically evaluate thequality of produced summaries given the gold-standard reference news.
The ROUGE metricsmeasure summary quality by counting the preci-sion, recall and F-score of overlapping units, suchas n-grams and skip grams, between a candidatesummary and the reference summaries.We use the ROUGE-1.5.5 toolkit to perform the10We also tried words rather than bigrams but foundslightly worse performance.evaluation.
In this paper we report the F-scores ofthe following metrics in the experimental results:ROUGE-1 (unigram-based), ROUGE-2 (bigram-based) and ROUGE-SU4 (based on skip bigramswith a maximum skip distance of 4).4.4.2 Pyramid EvaluationWe also conduct manual pyramid evaluation inthis study.
Specifically, we use the modified pyra-mid scores as described in (Passonneau et al,2005) to manually evaluate the summaries gener-ated by different methods.
We randomly sample20 games from the data set and manually annotatefacts on the gold-standard news.
The annotatedfacts are mostly describing specific events hap-pened during the game, e.g.
?????????
(Ivanovic is shown the yellow card) and ?????????
(Neymar takes the corner).
Each factis treated as a Summarization Content Unit, (SCU)(Nenkova and Passonneau, 2004).
The numberof occurrences for each SCU in the gold-standardnews is regarded as the weight of this SCU.5 Results and Analysis5.1 Comparison with Baseline MethodsThe average performance on all three folds of dif-ferent methods are displayed in Table 1.Method R-1 R-2 R-SU4HeadTail 0.30147 0.07779 0.10336Centroid 0.32508 0.08113 0.11245LexRank 0.31284 0.06159 0.09376ILP 0.32552 0.07285 0.10378Highlight 0.34687 0.08748 0.11924RF 0.38559 0.11887 0.14907RF+DPP 0.39391 0.11986 0.15097Table 1: Comparison results of different methodsAs we can see from the results, our learningto rank approach based on RF achieves signif-icantly (< 0.01 significance level for pairwise-t testing) better results compared with traditionalunsupervised summarization approaches11.
TheILP model, which is believed to be suitable formulti-document summarization, did not performwell in our settings.
Head and tail sentences areinformative but merely using them lacks specificdescriptions for procedural events, therefore not11We also conducted experiments on using our proposedfeatures to calculate LexRank, but did not observe real differ-ence compared with normal LexRank.
This suggest that theperformance gain comes from supervised learning to rank ap-proach, not merely from the features.1366providing competitive results either.The comparison between RF and RF+DPPshows the effectiveness of our sentence selectionstrategy.
However, the increase is still limited12.This may become reasonable later when we dis-cuss more about the errors from our systems.Merely using highlight markers to constructnews also provides competitive results, but infe-rior to supervised models.
This suggests that thehighlight marker features are relatively strong in-dicators for good sentences while merely usingthese features may not be sufficient.Table 2 shows the average pyramid scores forthe systems in comparison.
The ?Gold-standard?row denotes manually written news report and islisted for reference.
We can see our learning torank systems based on RF constructs news withthe highest pyramid scores.Method Pyramid scoresHeadTail 0.13657Centroid 0.30663LexRank 0.28756ILP 0.20867Highlight 0.41121RF 0.53766RF+DPP 0.62500Gold-standard 0.88329Table 2: Average Pyramid scoresOverall, the experimental results indicate thatour system can generate much better news thanthe baselines in both automatic and manual eval-uations.
We include examples of our constructednews reports in the supplementary materials.5.2 Feature ValidationDifferent groups of features may play differentroles in the LTR models.
In order to validatethe impact of both the traditional features and thenovel task-specific features, we conduct experi-ments with different combinations by removingeach group of features respectively.
Table 3 showsthe results, with ?w/o?
denotes experiments with-out the corresponding group of features.Method R-1 R-2 R-SU4RF 0.38559 0.11887 0.14907RF-w/o novel 0.37297 0.10964 0.14021RF-w/o trad.
0.36314 0.09910 0.13102Table 3: Results of feature validation12Significance level < 0.05 for pairwise-t testing only forROUGE-1.We can observe that both the traditional featuresand the novel features contribute useful informa-tion for learning to rank models.
Due to the na-ture of the sentence extraction approach, featuresdesigned for traditional document summarizationare still playing an indispensable role for our task,although they might be important in this work fordifferent reasons.
For example, position featuresare indicative for traditional summarization sincesentences appearing in the very beginning or theend are more probable as summarizing sentences.For sports commentary, positions are closely re-lated to timeline in a more coarse fashion.
Certaintypes of key events, for example player substitu-tions and even scores, may tend to happen in cer-tain period in a game rather than uniformly spreadout in every minute.5.3 Room for Improvements5.3.1 Upper BoundsTo get a rough estimate of what is actually achiev-able in terms of the final ROUGE scores, welooked at different ?upper bounds?
under variousscenarios (Table 4).
We first evaluate one refer-ence news with the other reference news served asthe gold-standard result.
The results are given inthe row labeled reference of Table 4.
This providesa reasonable estimate of human performance.Second, in sentence extraction we restrict theconstructed news to sentences from the origi-nal commentary texts themselves.
We use thegreedy algorithm to extract sentences that max-imize ROUGE-2F scores.
The resulting perfor-mance is given in the row extract of Table 4.We observe numerically superior scores comparedwith reference.
This is not strange since we are in-tentionally optimizing ROUGE scores.
And alsothis suggests that the sentence extraction approachfor sports news construction is rather reasonable,in terms of information overlap.Method R-1 R-2 R-SU4reference 0.44725 0.15265 0.18064extract 0.43270 0.16872 0.18622target 0.40987 0.15901 0.17941target+DPP 0.41536 0.15994 0.18232RF+DPP 0.39391 0.11986 0.15097Table 4: Upper bounds on ROUGE scoresThird, we use the partial ROUGE-2 values,i.e.
the targets used to train LTR models (cf.Section 3.1) for greedy selection and DPP selec-tion, with results listed in the row target and tar-1367Time Live Text Commentary Script55 ???
?
?
?
??
?
??
??
??
?
???
Neymar wins a free kick in a good position.56 ???
~ ~ ~  Neymar!!!!!!
!56 ???
?
???
??
??
?
??
??
?
?
??
?
?
?
??
????
The free kick from Neymar goes directly into the top left corner!
The keeper can do nothing.56 ?
?
?
?
??
?
??
?
??
??
??
?
The ball drops quickly and flies to the goal.Figure 4: Case I: short and noisy sentencesTime Live Text Commentary ScriptFT ?????????????????????????
From a quick free kick from Chelsea, Costa scored the only goal of the game!!
!FT ????????????????????????????
Chelsea dominated the game but found it difficult against Norwich?s defense.FT??????????????????????????????????????
?From an error from the opponent, Hazard was fouled.
Willian launches a quick free kick and assists Costa for the lethal strike.Figure 5: Case II: summarizing sentencesget+DPP of Table 4.
This validates that using par-tial ROUGE-2 as the training target for LTR mod-els is somewhat reasonable for this study.5.3.2 Error AnalysisIn this preliminary study, we use LTR models andprobabilistic sentence selection procedure.
Whilereasonable performance has been achieved, thereexist certain types of errors as we found in the con-structed news results.Error I: First, sentences in live commentaryare mostly short, and sometimes noisy.
Some-times an important event has been described us-ing a number of consecutive short sentences.
OurLTR models failed to generate high scores forsuch sentences and therefore will cause some lackof information.
Figure 4 illustrates an exampleof this type of error in the constructed news re-port.
All the sentences are describing a key scor-ing event.
However, none of them were selected toconstruct the news because our LTR model assignslow scores for these short sentences.
Meanwhilethe second sentence can be treated as noisy.Error II: Second, commentators are likely tosummarize important events during the game, notat the point when the event happens.
Our sentenceselection algorithm can only address local redun-dancy, while this issue is more global.
Figure 5 il-lustrates an example of this case in the constructednews report.
The only goal of the match is de-scribed during full-time (FT).
Our method redun-dantly included this in the final constructed newseven it had already selected that event.These two issues are highly non-trivial and havenot been well addressed in the method we exploredin this paper.
We leave them for further study inthe future.5.3.3 Readability AssessmentIn this work we only consider sentence extrac-tion.
Unlike traditional summarization tasks,sports commentary texts are describing a differ-ent specific action in almost every sentence.
De-scriptive coherence becomes a more difficult chal-lenge in this scenario.
We conduct manual evalu-ation on systems in comparison along with man-ually written news reports (gold-standard).
Threevolunteers who are fluent in Chinese were askedto perform manual ratings on three factors: co-herence (Coh.
), non-redundancy (NR) and overallreadability (Read.).
The ratings are in the formatof 1-5 numerical scores (not necessarily integral),with higher scores denote better quality.
The re-sults are shown in Table 5.Method Coh.
NR Read.HeadTail 3.47 3.07 3.56Centroid 2.87 3.72 2.66LexRank 2.90 3.23 2.43ILP 2.87 3.23 2.50Highlight 3.36 3.72 3.06RF 3.23 3.64 3.13RF+DPP 3.23 3.87 3.06Gold-Standard 4.67 4.23 4.77Table 5: Manual readability ratingsThe differences between systems in terms ofreadability factors are not as large as informationcoverage suggested by ROUGE metrics and pyra-mid scores.
Meanwhile, while we can observe thatour approaches outperforms the unsupervised ex-tractive summarization approaches in coherenceand readability for certain level, the results alsoclearly suggest that there still exists large room forimprovements in terms of the readability factors.6 DiscussionsThe general challenges for the particular task ofsports news generation are mostly addressed inthose designed features in the learning to rankframework.
We utilize the timeline and score-line information, while also keep traditional fea-tures such as sentence length.
Experimental re-sults show that our framework indeed outperformsstrong traditional summarization baselines, whilestill having much room for improvement.We might also notice that there may exist someissues if merely using automatic metrics to eval-uate the overall quality of the generated news re-ports.
The ROUGE metrics are mainly based on1368ngram overlaps.
For sports texts most of the pro-portions are dominated by proper names, certaintypes of actions or key events, etc.
Compared withtraditional summarization tasks, it might be eas-ier to achieve high ROUGE scores with an em-phasize on selecting important entities.
In our ex-periments, methods with higher ROUGE scorescan indeed achieve better coverage of importantunits such as events, as shown in pyramid scoresin Table 2.
However, we can also observe fromTable 5 that automatic metrics currently cannotreflect readability factors very well.
Generallyspeaking, while big difference in ROUGE maysuggest big difference in overall quality, smallerROUGE differences may not be that indicativeenough.
Therefore, it is interesting to find alter-native automatic metrics in order to better reflectthe general quality for this task.7 Related WorkTo the best of our knowledge, generation of sportsnews from live text commentary is not a well-studied task in related fields.
One related study fo-cused on generating textual summaries for sportsevents from status updates in Twitter (Nicholset al, 2012).
There also exists earlier work ongeneration of sports highlight frames from sportsvideos, focusing on a very different type of data(Tjondronegoro et al, 2004).
Bouayad-Agha etal.
(2011) and Bouayad-Agha et al (2012) con-structed an ontology-based knowledge base forthe generation of football summaries, using pre-defined extraction templates.Our task is closely related to document sum-marization, which has been studied quite inten-sively.
Various approaches exist to challenge thedocument summarization task, including centroid-based methods, link analysis and graph-basedalgorithms (Erkan and Radev, 2004; Wan etal., 2007), combinatorial optimization techniquessuch as integer linear programming (Gillick etal., 2008) and submodular optimization (Lin andBilmes, 2010).
Supervised models includinglearning to rank models (Metzler and Kanungo,2008; Shen and Li, 2011; Wang et al, 2013)and regression (Ouyang et al, 2007; Galanis andMalakasiotis, 2008; Hong and Nenkova, 2014)have also been adapted in the scenario of docu-ment summarization.Since sports live texts contain timeline informa-tion, summarization paradigms that utilize time-line and temporal information (Yan et al, 2011;Ng et al, 2014; Li et al, 2015) are also conceptu-ally related.
Supervised approaches related to thiswork have also been applied for timeline summa-rization, including linear regression for importantscores (Tran et al, 2013a) and learning to rankmodels (Tran et al, 2013b).
In this preliminarywork we only use the timestamps in the definitionof similarity for sentence selection.
More craftedusages will be explored in the future.8 Conclusion and Future WorkIn this paper we study a challenging task to au-tomatically construct sports news from live textcommentary.
Using football live texts as an in-stance, we collect training data jointly from livetext commentary services and sports news portals.We develop a system based on learning to rankmodels, with several novel task-specific features.To generate the final news summary and tacklethe local redundancy problem, we also propose aprobabilistic sentence selection method.
Experi-mental results demostrate that this task is feasibleand our proposed methods are appropriate.As a preliminary work, we only perform sen-tence extraction in this work.
Since sports newsand live commentary are in different genres, somepost-editing rewritings will make the system gen-erating more natural descriptions for sports news.We would like to extend our system to producesports news beyond pure sentence extraction.Another important direction is to focus on theconstruction of datasets in larger scale.
One fea-sible approach is to use a speech recognition sys-tem on live videos or broadcasts of sports gamesto collect huge amount of transcripts as our rawdata source.
Although more data can be eas-ily collected in this case, the noisiness of audiotranscripts may bring some additional challenges,therefore worthwhile for further study.AcknowledgmentsThis work was supported by National Natural Sci-ence Foundation of China (61331011), NationalHi-Tech Research and Development Program (863Program) of China (2015AA015403) and IBMGlobal Faculty Award Program.
We thank theanonymous reviewers for helpful comments andKui Xu from our group for his help in calculat-ing player popularity features.
Xiaojun Wan is thecorresponding author of this paper.1369ReferencesNadjet Bouayad-Agha, Gerard Casamayor, and LeoWanner.
2011.
Content selection from an ontology-based knowledge base for the generation of footballsummaries.
In Proceedings of the 13th EuropeanWorkshop on Natural Language Generation, pages72?81.
Association for Computational Linguistics.Nadjet Bouayad-Agha, Gerard Casamayor, SimonMille, and Leo Wanner.
2012.
Perspective-orientedgeneration of football match summaries: Old tasks,new challenges.
ACM Transactions on Speech andLanguage Processing (TSLP), 9(2):3.Leo Breiman.
2001.
Random forests.
Machine learn-ing, 45(1):5?32.Jaime Carbonell and Jade Goldstein.
1998.
The use ofmmr, diversity-based reranking for reordering doc-uments and producing summaries.
In Proceedingsof the 21st annual international ACM SIGIR confer-ence on Research and development in informationretrieval, pages 335?336.
ACM.G?unes Erkan and Dragomir R Radev.
2004.
Lexrank:Graph-based lexical centrality as salience in textsummarization.
Journal of Artificial IntelligenceResearch, pages 457?479.Dimitrios Galanis and Prodromos Malakasiotis.
2008.Aueb at tac 2008.
In Proceedings of the TAC 2008Workshop.Jennifer Gillenwater, Alex Kulesza, and Ben Taskar.2012.
Near-optimal map inference for determinantalpoint processes.
In Advances in Neural InformationProcessing Systems, pages 2735?2743.Dan Gillick, Benoit Favre, and Dilek Hakkani-Tur.2008.
The icsi summarization system at tac 2008.
InProceedings of the Text Understanding Conference.Kai Hong and Ani Nenkova.
2014.
Improvingthe estimation of word importance for news multi-document summarization.
In Proceedings of the14th Conference of the European Chapter of the As-sociation for Computational Linguistics, pages 712?721, Gothenburg, Sweden, April.
Association forComputational Linguistics.Alex Kulesza and Ben Taskar.
2011.
Learning deter-minantal point processes.
In UAI.Alex Kulesza and Ben Taskar.
2012.
Determinantalpoint processes for machine learning.
Foundationsand Trends in Machine Learning, 5(2?3).Chen Li, Yang Liu, and Lin Zhao.
2015.
Improvingupdate summarization via supervised ilp and sen-tence reranking.
In Proceedings of the 2015 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 1317?1322, Denver, Col-orado, May?June.
Association for ComputationalLinguistics.Hui Lin and Jeff Bilmes.
2010.
Multi-document sum-marization via budgeted maximization of submod-ular functions.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 912?920.
Association for Computa-tional Linguistics.Chin-Yew Lin and Eduard Hovy.
2003.
Auto-matic evaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003Conference of the North American Chapter of theAssociation for Computational Linguistics on Hu-man Language Technology-Volume 1, pages 71?78.Association for Computational Linguistics.Donald Metzler and Tapas Kanungo.
2008.
Ma-chine learned sentence selection strategies for query-biased summarization.
In SIGIR Learning to RankWorkshop, pages 40?47.Ani Nenkova and Rebecca Passonneau.
2004.
Evalu-ating content selection in summarization: The pyra-mid method.Jun-Ping Ng, Yan Chen, Min-Yen Kan, and ZhoujunLi.
2014.
Exploiting timelines to enhance multi-document summarization.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages923?933, Baltimore, Maryland, June.
Associationfor Computational Linguistics.Jeffrey Nichols, Jalal Mahmud, and Clemens Drews.2012.
Summarizing sporting events using twitter.
InProceedings of the 2012 ACM international confer-ence on Intelligent User Interfaces, pages 189?198.ACM.You Ouyang, Sujian Li, and Wenjie Li.
2007.
Devel-oping learning strategies for topic-based summariza-tion.
In Proceedings of the sixteenth ACM confer-ence on Conference on information and knowledgemanagement, pages 79?86.
ACM.Rebecca J Passonneau, Ani Nenkova, Kathleen McK-eown, and Sergey Sigelman.
2005.
Applying thepyramid method in duc 2005.
In Proceedings of theDocument Understanding Conference (DUC 05),Vancouver, BC, Canada.Dragomir R Radev, Hongyan Jing, and MalgorzataBudzikowska.
2000.
Centroid-based summariza-tion of multiple documents: sentence extraction,utility-based evaluation, and user studies.
In Pro-ceedings of the 2000 NAACL-ANLP Workshop onAutomatic summarization, pages 21?30.
Associationfor Computational Linguistics.Chao Shen and Tao Li.
2011.
Learning to rank forquery-focused multi-document summarization.
InData Mining (ICDM), 2011 IEEE 11th InternationalConference on, pages 626?634.
IEEE.1370Dian Tjondronegoro, Yi-Ping Phoebe Chen, and BinhPham.
2004.
Integrating highlights for more com-plete sports video summarization.
IEEE multimedia,11(4):22?37.Giang Binh Tran, Mohammad Alrifai, and DatQuoc Nguyen.
2013a.
Predicting relevant newsevents for timeline summaries.
In Proceedings ofthe 22nd international conference on World WideWeb Companion, pages 91?92.
International WorldWide Web Conferences Steering Committee.Giang Binh Tran, Tuan A Tran, Nam-Khanh Tran, Mo-hammad Alrifai, and Nattiya Kanhabua.
2013b.Leveraging learning to rank in an optimizationframework for timeline summarization.
In SIGIR2013 Workshop on Time-aware Information Access(TAIA.Xiaojun Wan, Jianwu Yang, and Jianguo Xiao.2007.
Manifold-ranking based topic-focused multi-document summarization.
In IJCAI, volume 7,pages 2903?2908.Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Flo-rian, and Claire Cardie.
2013.
A sentence com-pression based framework to query-focused multi-document summarization.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages1384?1394, Sofia, Bulgaria, August.
Association forComputational Linguistics.Qiang Wu, Christopher JC Burges, Krysta M Svore,and Jianfeng Gao.
2010.
Adapting boosting forinformation retrieval measures.
Information Re-trieval, 13(3):254?270.Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan,Xiaoming Li, and Yan Zhang.
2011.
Timeline gen-eration through evolutionary trans-temporal summa-rization.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, pages 433?443, Edinburgh, Scotland, UK., July.Association for Computational Linguistics.1371
