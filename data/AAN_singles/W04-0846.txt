Context Clustering for Word Sense Disambiguation Based onModeling Pairwise Context SimilaritiesCheng Niu, Wei Li, Rohini K. Srihari, Huifeng Li, Laurie CristCymfony Inc.600 Essjay Road, Williamsville, NY 14221.
USA.
{cniu, wei, rohini, hli, lcrist}@cymfony.comAbstractTraditionally, word sense disambiguation(WSD) involves a different context model foreach individual word.
This paper presents anew approach to WSD using weaklysupervised learning.
Statistical models are nottrained for the contexts of each individualword, but for the similarities between contextpairs at category level.
The insight is that thecorrelation regularity between the sensedistinction and the context distinction can becaptured at category level, independent ofindividual words.
This approach only requiresa limited amount of existing annotated trainingcorpus in order to disambiguate the entirevocabulary.
A context clustering scheme isdeveloped within the Bayesian framework.
Amaximum entropy model is then trained torepresent the generative probabilitydistribution of context similarities based onheterogeneous features, including triggerwords and parsing structures.
Statisticalannealing is applied to derive the final contextclusters by globally fitting the pairwisecontext similarity distribution.
Benchmarkingshows that this new approach significantlyoutperforms the existing WSD systems in theunsupervised category, and rivals supervisedWSD systems.1 IntroductionWord Sense Disambiguation (WSD) is one of thecentral problems in Natural Language Processing.The difficulty of this task lies in the fact thatcontext features and the corresponding statisticaldistribution are different for each individual word.Traditionally, WSD involves modeling thecontexts for each word.
[Gale et al 1992] uses theNa?ve Bayes method for context modeling whichrequires a manually truthed corpus for eachambiguous word.
This causes a serious KnowledgeBottleneck.
The situation is worse whenconsidering the domain dependency of wordsenses.
To avoid the Knowledge Bottleneck,unsupervised or weakly supervised learningapproaches have been proposed.
These include thebootstrapping approach [Yarowsky 1995] and thecontext clustering approach [Schutze 1998].Although the above unsupervised or weaklysupervised learning approaches are less subject tothe Knowledge Bottleneck, some weakness exists:i) for each individual keyword, the sense numberhas to be provided and in the bootstrapping case,seeds for each sense are also required; ii) themodeling usually assumes some form of evidenceindependency, e.g.
the vector space model used in[Schutze 1998] and [Niu et al 2003]: this limits theperformance and its potential enhancement; iii)most WSD systems either use selectionalrestriction in parsing relations, and/or  triggerwords which co-occur within a window size of theambiguous word.
We previously at-temptedcombining both types of evidence but onlyachieved limited improvement due to the lack of aproper modeling of information over-lapping [Niuet al 2003].This paper presents a new algorithm thataddresses these problems.
A novel contextclustering scheme based on modeling thesimilarities between pairwise contexts at categorylevel is presented in the Bayesian framework.
Agenerative maximum entropy model is then trainedto represent the generative probability distributionof pairwise context similarities based onheterogeneous features that cover both co-occurring words and parsing structures.
Statisticalannealing is used to derive the final contextclusters by globally fitting the pairwise contextsimilarities.This new algorithm only requires a limitedamount of existing annotated corpus to train thegenerative maximum entropy model for the entirevocabulary.
This capability is based on theobservation that a system does not necessarilyrequire training data for word A in order todisambiguate A.
The insight is that the correlationregularity between the sense distinction and thecontext distinction can be captured at categorylevel, independent of individual words.In what follows, Section 2 formulates WSD as acontext clustering task based on the pairwiseAssociation for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of Systemscontext similarity model.
The context clusteringalgorithm is described in Sections 3 and 4,corresponding to the two key aspects of thealgorithm, i.e.
the generative maximum entropymodeling and the annealing-based optimization.Section 5 describes benchmarks and conclusion.2 Task Definition and Algorithm DesignGiven n  mentions of a key word, we firstintroduce the following symbols.
iC  refers to thei -th context.
iS  refers to the sense of the i -thcontext.
jiCS ,  refers to the context similaritybetween the i -th context and the j -th context,which is a subset of the predefined contextsimilarity features.
?f  refers to the ?
-thpredefined context similarity feature.
So jiCS ,takes the form of { }?f .The WSD task is defined as the hard clusteringof multiple contexts of the key word.
Its finalsolution is represented as { }MK ,  where K refersto the number of distinct senses, and M representsthe many-to-one mapping (from contexts to acluster) such that ( ) K].
[1,j n],[1,i j,iM ?
?=For any given context pair, a set of contextsimilarity features are defined.
With n mentions ofthe same key word, 2)1( ?nn  context similarities[ ] [ )( )ijniCS ji ,1,,1 , ??
are computed.
The WSD taskis formulated as searching for { }MK ,  whichmaximizes the following conditional probability:{ }( ) [ ] [ )( )ijniCSMK ji ,1,,1       }{,Pr , ?
?Based on Bayesian Equity, this is equivalent tomaximizing the joint probability in Eq.
(1), whichcontains a prior probability distribution of WSD,{ }( )MK ,Pr .
{ }( ) [ ] [ )( ){ }( ) { }( ){ }( ) { }( )MKMKCSMKMKCSijniCSMKijNijijiji,Pr,Pr,Pr,}{Pr,1,,1       }{,,Pr1,1,1,,,??====??
(1)Because there is no prior knowledge availableabout what solution is preferred, it is reasonable totake an equal distribution as the prior probabilitydistribution.
So WSD is equivalent to searching for{ }MK ,  which maximizes Expression (2).
{ }( )?
?==1,1,1, ,PrijNiji MKCS    (2)where{ }( ) ( ) ( ) ( )( )?=== otherwise ,PrjMiM if ,Pr,Pr,,,jijijijiji SSCSSSCSMKCS(3)To learn the conditional probabilities ( )jiji SSCS =|Pr ,  and ( )jiji SSCS ?|Pr ,  in Eq.
(3), amaximum entropy model is trained.
There are twomajor advantages of this maximum entropy model:i) the model is independent of individual words; ii)the model takes no information independenceassumption about the data, and hence is powerfulenough to utilize heterogeneous features.
With thelearned conditional probabilities in Eq.
(3), for agiven { }MK ,  candidate, we can compute theconditional probability of Expression (2).
In thefinal step, optimization is performed to search for{ }MK ,  that maximizes the value of Expression(2).3 Maximum Entropy ModelingThis section presents the definition of contextsimilarity features, and how to estimate thegenerative probabilities of context similarity ( )jiji SSCS =,Pr  and ( )jiji SSCS ?,Pr  usingmaximum entropy modeling.Using the Senseval-2 training corpus,1 we haveconstructed Corpus I and Corpus II for each Part-of-speech (POS) tag.
Corpus I is constructed usingcontext pairs involving the same sense of a word.Corpus II is constructed using context pairs thatrefer to different senses of a word.
Each corpuscontains about 18,000 context pairs.
The instancesin the corpora are represented as pairwise contextsimilarities, taking the form of { }?f .
The twoconditional probabilities ( )jiji SSCS =,Pr  and ( )jiji SSCS ?,Pr  can be represented as( )}{Pr maxEntI ?f  and ( )}{Pr maxEntII ?f  which aregenerative probabilities by maximum entropy forCorpus I and Corpus II.We now present how to compute the contextsimilarities.
Each context contains the followingtwo categories of features:i) Trigger words centering around the key wordwithin a predefined window size equal to 50tokens to both sides of the key word.
Triggerwords are learned using the same technique asin [Niu et al 2003].ii) Parsing relationships associated with the keyword automatically decoded by our parser1 Note that the words that appear in the Senseval-3lexical sample evaluation are removed in the corpusconstruction process.InfoXtract [Srihari et al 2003].
Therelationships being utilized are listed below.Noun: subject-of, object-of, complement-of,has-adjective-modifier, has-noun-modifier, modifier-of, possess,possessed-by, appositive-ofVerb: has-subject, has-object, has-complement, has-adverb-modifier,has-prepositional-modifierAdjective: modifier-of, has-adverb-modifierBased on the above context features, thefollowing three categories of context similarityfeatures are defined:(1) Context similarity based on a vector spacemodel using co-occurring trigger words: thetrigger words centering around the key wordare represented as a vector, and the tf*idfscheme is used to weigh each trigger word.The cosine of the angle between two resultingvectors is used as a context similaritymeasure.
(2) Context similarity based on Latentsemantic analysis (LSA) using trigger words:LSA [Deerwester et al 1990] is a techniqueused to uncover the underlying semanticsbased on co-occurrence data.
Using LSA,each word is represented as a vector in thesemantic space.
The trigger words arerepresented as a vector summation.
Then thecosine of the angle between the two resultingvector summations is computed, and used as acontext similarity measure.
(3) LSA-based Parsing Structure Similarity:each relationship is in the form of )(wR?
.Using LSA, each word w  is represented assemantic vector ( )wV .
Then, the similaritybetween )( 1wR?
and )( 2wR?
is represented asthe cosine of angle between ( )1wV  and ( )2wV .Two special values are assigned to twoexceptional cases: i) when  no relationship?R  is decoded in both contexts; ii) when therelationship ?R is decoded only for onecontext.To facilitate the maximum entropy modeling inthe later stage, the resulting similarity measure isdiscretized into 10 integer values.
Now thepairwise context similarity is a set of similarityfeatures, e.g.
{VSM-Similairty-equal-to-2, LSA-Trigger-Words-Similarity-equal-to-1, LSA-Subject-Similarity-equal-to-2}.In addition to the three categories of basiccontext similarity features defined above, we alsodefine induced context similarity features bycombining basic context similarity features usingthe logical AND operator.
With induced features,the context similarity vector in the previousexample is represented as{VSM-Similairty-equal-to-2, LSA- Trigger-Words-Similarity-equal-to-1, LSA-Subject-Similarity-equal-to-2,[VSM-Similairty-equal-to-2 and LSA-Trigger -Words-Similarity-equal-to-1], [VSM-Similairty-equal-to-2 and LSA-Subject-Similarity-equal-to-2],??
?,[VSM-Similairty-equal-to-2 and LSA-Trigger -Words-Similarity-equal-to-1 and LSA-Subject-Similarity-equal-to-2]}.The induced features provide direct and fine-grained information, but suffer from less samplingspace.
To make the computation feasible, weregulate 3 as the maximum number of logical ANDin the induced features.
Combining basic featuresand induced features under a smoothing scheme,maximum entropy modeling may achieve optimalperformance.Now the maximum entropy modeling can beformulated as follows: given a pairwise contextsimilarity }{ ?f , the generative probability of}{ ?f in Corpus I or Corpus II is given as( ){ }??=?
?fffwZf1}{Pr maxEnt         (4)where Z is the normalization factor, fw  is theweight associated with feature f .
The IterativeScaling algorithm combined with Monte Carlosimulation [Pietra, Pietra, & Lafferty 1995] is usedto train the weights in this generative model.Unlike the commonly used conditional maximumentropy modeling which approximates the featureconfiguration space as the training corpus[Ratnaparkhi 1998], Monte Carlo techniques arerequired in the generative modeling to simulate thepossible feature configurations.
The exponentialprior smoothing scheme [Goodman 2003] isadopted.
The same training procedure is performedusing Corpus I and Corpus II to estimate( )}{Pr maxEntI if  and ( )}{Pr maxEntII if  respectively.4 Statistical AnnealingWith the maximum entropy modeling presentedabove, the WSD task is performed as follows: i)for a given set of contexts, the pairwise contextsimilarity measures are computed; ii) for eachcontext similarity }{ if , the two generativeprobabilities ( )}{Pr maxEntI if  and ( )}{Pr maxEntII if  arecomputed; iii) for a given WSD candidatesolution{ }MK , , the conditional probability (2) canbe computed.
Optimization based on statisticalannealing (Neal 1993) is used to search for { }MK ,which maximizes Expression (2).The optimization process consists of two steps.First, a local optimal solution{ }0, MK is computedby a greedy algorithm.
Then by setting { }0, MK asthe initial state, statistical annealing is applied tosearch for the global optimal solution.
To reducethe search time, we set the maximum value of Kto 5.5 Benchmarking and ConclusionTo enter the Senseval-3 evaluation, weimplemented the following procedure to map thecontext clusters to Senseval-3 standards: i) processthe Senseval-3 training corpus and testing corpususing our parser; ii) for each word to bebenchmarked, retrieve the related contexts fromthe corpora and cluster them; iii) Based on 10% ofthe sense tags in the Senseval-3 training corpus(10% data correspond roughly to an average of 2-3instances for each sense), the context cluster ismapped onto the most frequent WSD senseassociated with the cluster members.
By design,the context clusters correspond to distinct senses,therefore, we do not allow multiple context clustersto be mapped onto one sense.
In case multipleclusters correspond to one sense, only the largestcluster is retained; iv), each instance in the testingcorpus is tagged with the same sense as the one towhich its context cluster corresponds.We are not able to compare our performancewith other systems in Senseval-3 because at thetime of writing, the Senseval-3 evaluation resultsare not publicly available.
As a note, comparedwith the Senseval-2 English Lexical Sampleevaluation, the benchmarks of our new algorithm(Table 1) are significantly above the performanceof the WSD systems in the unsupervised category,and rival the performance of the supervised WSDsystems.Table 1.
Senseval-3 Lexical Sample EvaluationAccuracyCategory Fine grain (%) Coarse grain (%)Adjective (5) 49.1 64.8Noun (20) 57.9 66.6Verb (32) 55.3 66.3Average 56.3% 66.4%6 AcknowledgementsThis work was supported by the Navy SBIRprogram under contract N00178-03-C-1047.ReferencesGale, W., K. Church, and D. Yarowsky.
1992.
AMethod for Disambiguating Word Senses in aLarge Corpus.
Computers and the Humanities,26.Yarowsky, D. 1995.
Unsupervised Word SenseDisambiguation Rivaling Supervised Methods.In Proceedings of ACL 1995.Schutze, H. 1998.
Automatic Word SenseDisambiguation.
Computational Linguistics, 23.C.
Niu, Zhaohui Zheng, R. Srihari, H. Li, and W.Li 2003.
Unsupervised Learning for Verb SenseDisambiguation Using Both trigger Words andParsing Relations.
In Proceeding of PACLING2003, Halifax, Canada.Deerwester, S., S. T. Dumais, G. W. Furnas, T. K.Landauer, and R. Harshman.
1990.
Indexing byLatent Semantic Analysis.
In Journal of theAmerican Society of Information ScienceGoodman, J.
2003.
Exponential Priors forMaximum Entropy Models.Neal, R.M.
1993.
Probabilistic Inference UsingMarkov Chain Monte Carlo Methods.
TechnicalReport, Univ.
of Toronto.Pietra, S. D., V. D. Pietra, and J. Lafferty.
1995.Inducing Features Of Random Fields.
In IEEETransactions on Pattern Analysis and MachineIntelligence.Adwait Ratnaparkhi.
(1998).
Maximum EntropyModels for Natural Language AmbiguityResolution.
Ph.D. Dissertation.
University ofPennsylvania.Srihari, R., W. Li, C. Niu and T. Cornell.
2003.InfoXtract: A Customizable Intermediate LevelInformation Extraction Engine.
In Proceedingsof HLT/NAACL 2003 Workshop on SEALTS.Edmonton, Canada.
