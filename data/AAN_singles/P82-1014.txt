PROCESSING ENGLISH WITH AGENERALIZED PHRASE STRUCTURE GRAMMARJean Mark Gawron, Jonathan King, John Lamping, Egon Loebner,Eo Anne Paulson, Geoffrey K. Pullum, Ivan A.
Sag, and Thomas WasowComputer Research CenterHewlett Packard Company1501 Page Mill RoadPalo Alto, CA 94304ABSTRACTThis paper describes a natural languageprocessing system implemented at Hewlett-Packard'sComputer Research Center.
The system's maincomponents are: a Generalized Phrase StructureGrammar (GPSG); a top-down parser; a logictransducer that outputs a f i r s t -order  logicalrepresentation; and a "disambiguator" that usessortal information to convert "normal-form"f i r s t -order  logical expressions into the querylanguage for HIRE, a relational database hosted inthe SPHERE system.
We argue that theoreticaldevelopments in GPSG syntax and in Montaguesemantics have specific advantages to bring to thisdomain of computational l inguistics.
The syntaxand semantics of the system are total lydomain-independent, and thus, in principle,highly portable.
We discuss the prospects forextending domain-independence to the lexicalsemantics as well, and thus to the logical semanticrepresentations.I .
INTRODUCTIONThis paper is an inter im progress report onl inguistic research carried out at Hewlett-PackardLaboratories since the summer of 1981.
Theresearch had three goals: (1) demonstrating thecomputational tractabi l i ty of Generalized PhraseStructure Grammar (GPSG), (2) implementing aGPSG system covering a large fragment of English,and (3) establishing the feasibi l i ty of using GPSGfor interactions with an inferencing knowledgebase.Section 2 describes the general architectureof the system.
Section 3 discusses the grammarand the lexicon.
A brief dicussion of the parsingtechnique used in found in Section 4.
Section 5discusses the semantics of the system, and Section6 presents ~ detailed example of a parse-treecomplete with semantics.
Some typical examplesthat the system can handle are given in theAppendix.The system is based on recent developmentsin syntax and semantics, reflecting a modular viewin which grammatical structure an~ abstract logicalstructure have independent status.
Theunderstanding of a sentence occurs in a number ofstages, distinct from each other and governed bydifferent principles of organization.
We areopposed to the idea that language understandingcan be achieved without detailed syntacticanalysis.
There is, of course, a massivepragmatic component to human l inguist icinteraction.
But we hold that pragmatic inferencemakes use of a logically pr ior grammatical andsemantic analysis.
This can be f ru i t fu l l y  modeledand exploited even in the complete absence of anymodeling of pragmatic inferencing capabil ity.However, this does not entail an incompatibil itybetween our work and research on modelingdiscourse organization and conversationalinteraction directly= Ultimately, a successfullanguage understanding system wilt require bothkinds of research, combining the advantages ofprecise, grammar-driven analysis of utterancestructure and pragmatic inferencing based ondiscourse structures and knowledge of the world.We stress, however, that our concerns at thisstage do not extend beyond the specification of asystem that can eff ic iently extract literal meaningfrom isolated sentences of arb i t rar i ly  complexgrammatical structure.
Future systems will exploitthe literal meaning thus extracted in moreambitious applications that involve pragmaticreasoning and discourse manipulation.The system embodies two features thatsimultaneously promote extensibi l i ty,  facil itatemodification, and increase efficiency.
The f i r s t  isthat its grammar is context-free in the informalsense sometimes (rather misleadingly) used indiscussions of the autonomy of grammar andpragmatics: the syntactic rules and the semantictranslation rules are independent of the specificapplication domain.
Our rules are not devised adhoc with a part icular application or type ofinteraction in mind.
Instead, they are motivatedby recent theoretical developments in naturallanguage syntax, and evaluated by the usuall inguistic canons of simplicity and general ity.
Nochanges in the knowledge base or other exigenciesderiving from a particular context of applicationcan introduce a problem for the grammar (asdist inct,  of course, from the lexicon).The second relevant feature is that thegrammar i r  the- system is context-free in the senseof formal language theory.
This makes theextensive mathematical l i terature on context-freephrase structure grammars (CF-PSG's) direct lyrelevant to the enterprise, and permits util izationof all the well-known techniques for thecomputational implementation of context-freegrammars.
It might seem anachronistic to base alanguage understanding system on context-free74parsing.
As Pratt (1975, 423) observes: " I t  isfashionable these days to want to avoid allreference to context- free grammars beyond warningstudents that they are unf i t  for computerconsumption as far as computational l inguist ics isconcerned."
Moreover, widely accepted argumentshave been given in the l inguistics l i terature to theeffect that some human languages are not evenweakly context- f ree and thus cannot possibly bedescribed by a CF-PSG.
However, Gazdar andPullum (1982) answer all of these arguments,showing that they are either formally inval id orempirically unsupported or both.
It seemsappropriate, therefore, to take a renewed interestin the possibi l i ty of CF-PSG description of humanlanguages, both in computational l inguist ics and inl inguist ic  research generally.2.
COMPONENTS OF THE SYSTEMThe l inguist ic basis of the GPSG l inguist icsystem resides in the work reported in Gazdar(1981, 1982) and Gazdar, Pullum, and Sag (1981).
1These papers argue on empirical and theoreticalgrounds that context-freeness is a desirableconstraint on grammars.
It clearly would not beso desirable, however, if (1) it led to lostgeneralizations or (2) it resulted in anunmanageable number of rules in the grammar.Gazdar (1982) proposes a way of simultaneouslyavoiding these two problems.
Linguist icgeneralizations can be captured in a context- freegrammar with a metagrammor, i .e.
a higher- levelgrammar that generates the actual grammar as itslanguage.
The metagrammar has two kinds ofstatements:(1) Rule schemata.
These arebasically like ordinary rules, except thatthey contain variables ranging overcategories and features.
(2) Metarules.
These are implicationalstatements, wr i t ten in the form ===>B,which capture relations between rules.
Ametarule ===>t~ is interpreted as saying,"for every rule that is an instantiation ofthe schema =, there is a corresponding ruleof form \[5."
Here 13 will be @(~), where 8i ssome mapping specified part ly  by thegeneral theory of grammar and part ly  inthe metarule formulation.
For instance,it is taken to be part of the theory ofgrammar that @ preserves unchanged thesubcategorization (rule name) features ofrules (cf.
below).The GPSG system also assumes theRule-to-Rule Hypothesis, f i rst  advanced byRichard Montague, which requires that eachsyntactic rule be associated with a single semanticI.
See also Gazdar, Pullum, Sag, and Wasow(1982) for some fur ther  discussion and comparisonwith other work in the l inguist ic l i terature.translation rule.
The syntax-semantics match isrealized as follows: each rule is a t r ip le  consistingof a rule name, a syntactic statement (~ormally alocal condition on node admissibi l i ty) ,  and asemantic translation, specifying how theh igher -order  logic representations of the daughternodes combine to yield the correct translation forthe mother.
=The present GPSG system has f ivecomponents :1.
Grammara.
Lexiconb.
Rules and Metarules2.
Parser and Grammar Compiler3.
Semantics Handler4.
Disambiguator5.
HIRE database3.
GRAMMAR AND LEXICONThe grammar that has been implemented thusfar  is only a subset of a much larger GPSGgrammar that we have defined on paper.
Itnevertheless describes a broad sampling of thebasic constructions of English, including a var ietyof prepositional phrase construct ions,  noun-nouncompounds, the auxi l iary  system, genit ives,questions and relative clauses, passives, andexistential sentences.Each entry  in the lexicon contains two kindsof information about a lexical item, syntactic andsemantic.
The syntactic part of an entry consistsof a syntactic feature specification; this includes,inter alia, information about any i r regularmorphology the item may have, and what is knownin the l inguist ic  l i terature as strictsubcategorization information.
In our terms thelatter is information l inking lexical items of apart icular  category to specific environments inwhich that category is introduced by phrasestructure rules.
Presence in the lexical entry foran item I of the feature R (where R is the nameof a rule) indicates that / may appear instructures admitted by R, and absence indicatesthat it may not.The semantic information in a lexical entry issometimes simple, direct ly l inking a lexical itemwith some HIRE predicate or relation.
With verbsor preposit ions, there is also a specification ofwhat case roles to associate with part iculararguments (cf.
below for discussion of case roles).Expressions that make a complex logicalcontr ibut ion to the sentence in which they appearwitl in general have complicated translations.Thus every has the translation-2.
There is a theoretical issue here aboutwhether semantic translation rules need to bestipulated for each syntactic rule or whether thereis a general way of predict ing their  form.
SeeKlein and Sag (t981) for an attempt to develop thelatter view, which is not at present implementedin our system.75(LAMBDA P (LAMBDA Q ((FORALL X (P X))--> (Q x ) ) ) ) ,This indicates that it denotes a function whichtakes as argument a set P, and returns the set ofproperties that are true of all members of that set(cf.
below for sl ightly more detailed discussion).A typical rule looks like this:<VPI09: V\] -> V N\]!
N!I2: ((V N!
!2) N!!
)>The exclamation marks here are our notation forthe bars in an X-bar  category system.
(SeeJackendoff (1977) for a theory of thistype-- though one which differs on points of detailfrom ours.)
The rule has the form <a: b: c>.Here a is the name 'VP109'; b is a condition thatwill admit a node labeled 'V!'
if it has threedaughter nodes labeled respectively 'V' (verb) ,'N i t '  (noun phrase at the second bar level), and'NI! '
(the numeral 2 being merely an index topermit reference to a specific symbol in thesemantics, the metarules, and the rule compiler,and is not a part of the category label); and c isa semantic translation rule stating that the Vconstituent translates as a function expressiontaking as its argument the translation of thesecond N!
!,  the result being a function expressionto be applied to the translation of the f i rst  N!!
.By a general convention in the theory ofgrammar, the rule name is one of the featurevalues marked on the lexical head of any rule thatintroduces a lexical category (as this oneintroduces V).
Only verbs marked with thatfeature value satisfy this rule.
For example, if weinclude in the lexicon the word give and assign toit the feature VPI09, then this rule would generatethe verb phrase gave Anne a job.A typical metarule is the passive metarule,which looks like this ( ignoring semantics):<PAS:  <V!
-> V NI!
W > => <V!
-> V \ [PAS\ ]  W>>W is a string variable ranging over zero or morecategory symbols.
The metarule has the form <N:<A> => <B>>, where N is a name and <A> and <B >are schemata that have rules as their  instantiationswhen appropriate substitutions are made for thefree variables.
This metarule says that for everyrule that expands a verb phrase as verb followedby noun phrase followed by anything else( including nothing else), there is another rule thatexpands verb phrase as verb with passivemorphology followed by whatever followed the nounphrase in the given rule.
The metarule PAS wouldapply to grammar rule VP109 given above, yieldingthe rule:<VP109: V!
-> V\[PAS\] N{!>As we noted above, the rule number feature ispreserved here, so we get Anne was given a job,where the passive verb phrase is given a job,but not *Anne was hired a job.
3Passive sentences are thus analyzed direct ly,and not reduced to the form of active sentences inthe course of being analyzed, in the way that isfamiliar from work on transformational grammarsand on ATN's.
However, this does not mean thatno relation between passives and their activecounterparts is expressed in the system, becausethe rules for analyzing passives are in a senseder ivat ively defined on the basis of' rules foranalyzing actives.More di f f icult  than treating passives and thelike, and often cited as l i teral ly impossible within acontext- free grammar'," is treat ing constructionslike questions and relative clauses.
The apparentd i f f icu l ty  resides in the fact that in a question likeWhich employee has Personnel reported that Annethinks has performed outstandingly?, the portionbeginning with the third word must constitute astr ing analyzable as a sentence except that at somepoint it must lack a third person singular nounphrase in a position where such a noun phrasecould otherwise have occurred.
If it lacks nonoun phrase, we get ungrammatical strings of thetype *Which employee has Personnel reported thatAnne thinks Montague has performedoutstandingly?.
If it lacks a noun phrase at aposition where the verb agreement indicatessomething other than a singular one is required,we get ungrammaticalities like *Which employee hasPersonnel reported that Anne thinks haveperformed outstandingly?.
The problem is thusone of guaranteeing a grammatical dependencyacross a context that may be arb i t rar i ly  wide,while keeping the grammar context- free.
Thetechnique used is introduced into the l inguisticl i terature by Gazdar (1981).
It involves anaugmentation of the nonterminal vocabulary of thegrammar that permits constituents with "gaps" tobe treated as not belonging to the same categoryas similar constituents without gaps.
This wouldbe an unwelcome and inelegant enlargement of thegrammar if it had to be done by means ofcase-by-case stipulation, but again the use of ametagrammar avoids this.
Gazdar (1981) proposesa new set of syntactic categories of the form a/B,where ~ and 15 are categories from the basicnonterminal vocabulary of the grammar.
These arecalled slash categories.
A slash category e/B maybe thought of as representing a constituent ofcategory = with a missing internal occurrence of !5.We employ a method of introducing slash categoriesthat was suggested by Sag (1982): a metarulestating that for every rule introducing some Bunder = there is a parallel rule introducing 15/~under =/~.
In other words, any constituent canhave a gap of type ~" if one of its daughterconstituents does too.
Wherever this would lead toa daughter constituent with the label \[/~' in some3.
~ regard was given a job not as a passiveverb phrase itself but as a verb phrase containingthe verb be plus a passive verb phrase containinggiven and a job.4.
See Pullum and Gazdar (1982) for references.76rule, another metarule allows a parallel rulewithout the ~'/;r, and therefore defines rules thatallow for  actual gaps--i.e., missing constituents.In this way, complete sets of rules for describingthe unbounded dependencies found in interrogat iveand relative clauses can readily be wr i t ten.
Evenlong-distance agreement facts can be (and are)captured, since the morphosyntactic featuresrelevant to a specific case of agreement arepresent in the feature composition of any given ~'.4.
PARSINGThe system is init ial ized by expanding outthe grammar .
That is, t i le metarules are appliedto the rules to produce the ful l  rule set, which isthen compiled and used by the parser.
Metarulesare not consulted dur ing the process of parsing.One might well wonder about the possible benefitsof the other alternative: a parser that made themetarule-derived rules to order each time theywere needed, instead of consult ing a precompiledl ist.
This possibi l i ty has been explored by Kay(1982).
Kay draws an analogy between metarulesand phonological rules, modeling both by means off in i te state transducers.
We believe that this lineis worth pursuing;  however, the GPSG systemcur rent ly  operates off a precompiled set of rules.Application of ten metarules to for ty  basicrules yielded 283 grammar rules in the 1/1/82version of the GPSG system.
Since then thegrammar has been expanded somewhat, though thecurrent  version is stil l  undergoing somedebugging,  and the number of rules is unstable.The size of the grammar-plus-metarules systemgrows by a factor of f ive or six through the rulecompilation.
The great practical advantage ofusing a metarule-induced grammar is, therefore,that the work of designing and revising the systemof l inguist ic  rules can proceed on a body ofstatements that is under twenty percent of the sizeit would be if it were formulated as a simple l ist ofcontext- f ree rules.The system uses a standard type oftop-down parser with no Iookahead, augmenteds l ight ly  to prevent it from looking for a givenconst ituent start ing in a given spot more thanonce.
It produces, in parallel, all legal parsetrees for  a sentence, with semantic translationsassociated with each node.5.
SEMANTICSThe semantics handler uses the translationrule associated with a node to construct itssemantics from the semantics of its daughters.This construction makes crucial use of a procedurethat we call Cooper storage (after Robin Cooper;see below).
In the spir i t  of current  research informal semantics, each syntactic constituent isassociated direct ly with a single logic expression(modulo Cooper Storage), rather than any programor procedure for producing such an expression.Our semantic analysis thus embraces the principleof "surface composit ionality."
The semanticrepresentations derived at each node are referredto as the Logical Representation (LR).The disambiguator provides the crucialtransit ion from LR to HIRoE queries; thedisambiguator uses information about the sort, ordomoin of definition, of various terms in the logicalrepresentation.
One of the most importantfunctions of the disambiguator is to eliminateparses that do not make sense in the conceptualscheme of HIRE.HIRE is a relational database with a certainamount of inferencin9 capabi l i ty.
It is implementedin SPHERE, a database system which is adescendant of FOL (described in Weyhrauch(1980)).
Many of the relation-names output bythe disambiguator are derived relations defined byaxioms in SPHERE.
The SPHERE environment wasimportant for this application, since it wasessential to have something that could processf i r s t -o rder  logical output,  and SPHERE does justthat.
A noticeable recent trend in database theoryhas been a move toward an interd isc ip l inarycomingling of mathematical logic and relationaldatabase technology (see especially Gallaire andMinker (1978) and Gallaire, Minker and Nicolas(198\])) .
We regard it as an important fact aboutthe GPSG system that l inks computationall inguist ics to f i r s t -o rder  logical representationjust as the work referred to above has linkedf i r s t -o rder  logic to relational database theory.
Webelieve that SPHERE offers promising prospects fora knowledge representation system that ispr incipled and general in the way that we havetr ied to exemplify in our syntactic and semanticrule system.
Filman, Lamping and Montalvo (\]982)present details of some capabilities of SPHERE thatwe have not as yet exploited in our work,involv ing the use of multiple contexts to representviewpoints,  beliefs, and modalities, which aregeneral ly regarded as insuperable stumbling-blocksto f i r s t -o rder  logic approaches.Thus far the l inguist ic  work we havedescribed has been in keeping with GPSGpresented in the papers cited above.
However twosemantic innovations have been introduced tofaci l itate the disambiguator's translation from LR toa HIRE query.
As a result the l inguist ic  systemversion of LR has two new properties:(1) The intensional logic of the publishedwork was set aside and LR was designed to be anextensional f i r s t -o rder  language.
Althoughconstituent translations bui l t  up on the way to aroot node may be second-order, the system-maintains f i r s t -o rder  reducib i l i ty .
Thisreducib i l i ty  is i l lustrated by the fol lowing analysisof noun phrases as second-order properties(essentially the analysis of Montague (\]970)).
Forexample, the proper name Egon and the quantif iednoun phrase every opplicant are both translated assets of properties:77Egon = LAMBDA P (P EGON)Every applicant = LAMBDA P (FORALL X((APPLICANT X) --> (P X) ) )Egon is translated as the set of propertiest rue of Egon, and every applicant, as the set ofproperties t rue of all applicants.
Since basicpredicates in the logic are f i r s t -order ,  neither ofthe above expressions can be made the direct?
argument of any basic predicate; instead theargument is some unique ent i ty- level  variablewhich is later bound to the quant i f ier -expressionby quant i fy ing in.
This technique is essentiallythe storage device proposed in Cooper (1975).One advantage of this method of "deferr ing"  theintroduct ion into the interpretat ion process ofphrases with quant i f ier  meanings is that it allowsfor a natural,  nonsyntactic treatment of scopeambiguities.
Another is that with a logic limited tof i r s t -o rder  predicates, there is sti l l  a naturaltreatment for coordinated noun phrases ofapparently heterogeneous semantics, such as Egonand every applicant.
(2) HIRE represents events as objects.
Allobjects in the knowledge base, including events,belong to various sorts.
For our purposes, a sortis a set.
HIRE relations are declared as propertiesof entities within part icular  sorts.
For example,there is an employment sort, consisting of variouspart icular  employment events, and anemployment.employee relation as well asemployment .organization and employment.managerrelations.
More conventional relations, likeemployee.manager are defined as joins of the basicevent relations.
This allows the semantics to makesome fa i r ly  obvious connections between verbs andevents (between, say, the verb work and eventsof employment), and to represent di f ferentrelations between a verb and its arguments asd i f ferent  f i r s t -o rder  relations between an eventand its part ic ipants.
Although the lexicaltreatment sketched here is clearly domaindependent (the English verb work doesn'tnecessarily involve employment events),  it waschosen primari ly to simplify the ontology of a f i r s timplementation.
As an alternative, one mightconsider associating work with events of a sortlabor, one of whose subsorts was an employmentevent, def ining employments as those laborsassociated with an organization.Whichever choice one makes about the basicevent-types of verbs, the mapping from verbs toHIRE relations cannot be direct.
Consider asentence like Anne work5 for Egon.
The HIRErepresentation will predicate theemployment.manager relation of a part icularemployment event and a part icular  manager, andthe employment.employee relation of that sameevent and ,~knl,~.
Yet where Egon in this exampleis picked out with the employment .managerrelation, the sentence Anne worl<s for HP will needto pick out HP with the employment.organizationrelation.
I n order to accomodate thismany-to-many mapping between a verb andpart icu lar  relations in a knowledge base, thelexicon stipulates special relations that l ink averb to its eventual arguments.
Following Fillmore(1968), these mediating relations are called caseroles.The disambiguator narrows the case rolesdown to specific knowledge base relations.
Totake a simple example, Anne works for HP has alogical representation reducible to:(EXISTS SIGMA (AND (EMPLOYMENT SIGMA)(AG SIGMA ANNE)(LOC SIGMA HP)))Here SIGMA is a variable over s i tuat ions  or eventinstantiations, s The formula may be read, "Thereis an employment-situation whose Agent is Anneand whose Location is HP."
The lexical entry forwork supplies the information that its subject is anAgent and its complement a Location.
Thedisambiguator now needs to fu r ther  specify thecase roles as HIRE relations.
It does this bytreat ing each atomic formula in the expressionlocally, using the fact that Anne is a person inorder to interpret  AG, and the fact that HP isan organization in order to interpret  LOC.
In thiscase, it interprets the AG role asemployment.employee and the LOC role asemployment.organization.The advantages of using the roles in LogicalRepresentation, rather than going direct ly  topredicates in a knowledge base, include (1) theabi l i ty  to interpret  at least some prepositionalphrases, those known as adjuncts, wi thoutsubcategorizing verbs specially for them, since thecase role may be supplied either by a verb or apreposition.
(2) the option of interpret ing'vague' verbs such as have and give using caseroles without event types.
These verbs, then,become "pure ly"  relational.
For example, therepresentation of Egon gave Montague a job wouldbe:(EXISTS SIGMA (AND ((SO EGON) SIGMA)((POS MONTAGUE) SIGMA)(EMPLOYMENT SIGMA)))Here SO 'source' wil l  pick out the sameemployment.manager relation it did in the exampleabove; and POS 'possession' is the same relation asthat associated with have.
Here the s i tuat ion-typeis supplied by the translation of the noun job.
Itis important to realize that this representation isderived without giv ing the noun phrase a job anyspecial treatment.
The lexical entry for givecontains the information that the subject is thesource of the direct object, and the direct objectthe possession of the indirect object.
If therewere lamps in our knowledge base, the derivedrepresentation of Egon gave Montague a lamp wouldsimply be the above formula with the predicatelamp replacing employment.
The possessionrelation would hold between Montague and some5.
Our work in this domain has been influencedby the recent papers of Barwise and Perry on"situation semantics"; see e.c .
Barwise and Perry(1982)).78lamp, and the disambiguator would retr ievewhatever knowledge-base relation kept track ofsuch matters.Two active research goals o f  the currentproject are to give all lexical entries domainindependent representations, and to make allknowledge base-specific predicates and relationsthe exclusive province of the disambiguator.
Oneimportant means to that end is case roles, whichallow us a level of abstract, purely " l inguist ic"relations to mediate between logical representationsand HIRE queries.
Another is the use of generalevent types such as labor, to replace event-typesspecific to HIRE, such as employments.
The caseroles maintain a separation between the domainrepresentation language and LR.
Insofar as thatseparation is achieved, then absolute portabi l i tyof the system, up to and including the lexicon, isan attainable goal.Absolute portabi l i ty  obviously has immediatepractical benefits for any system that expects tohandle a large fragment of English, since theeffort in moving from one application to anotherwill be limited to " tun ing"  the disambiguator to anew ontology, and adding "specialized" vocabulary.The actual rules governing the production off i r s t -o rder  logical representations make noreference to the facts of HIRE.
The questionremains of just how portable the current  lexiconis; the answer is that much of it is already domainindependent.
Quantif iers like every (as we saw inthe discussion of NP semantics) are expressed aslogical constants; verbs like give are expressedent i re ly in terms of the case relations that holdamong the i r  arguments.
Verbs like work can beabstracted away from the domain by a simpleextension.
The obvious goal is to t ry  to givedomain independent representations to a corevocabulary of English that could be used in avar iety of application domains.6.
AN EXAMPLEWe shall now give a s l ight ly  more detailedi l lustrat ion of how the syntax and compositionalsemantics rules work.
We are stil l  s impl i fyingconsiderably, since we have selected an examplewhere rote frames are not involved, and we arenot employing features on nodes.
Here we havethe grammar of a t r iv ia l  subset of English:<$1: S -> NP VP: (NP Vp)>"<NPI: NP -> DET N: (DET N)><VPI: VP -> V NP: iV  NP)><VP2: VP -> V A: A>Suppose that the lexicon associated with the aboverules is:<every:DET: (LAMBDA P (LAMBDA Q(FORALL X ((P X)IMPLIES (Q X) ) ) ) )><applicant: N: APPLICANT><interviewed: V\[(RULE VP1)\] :  INTERVIEW><Bill: NP: (LAMBDA P (P BILL))><is: V\[(RULE MP2)\]: (BE)><competent: A: (LAMBDA Y(EXPERT.LEVEL HIGH Y))>The syntax of a lexical entry  is <L: C: T>, whereL is the spell ing of the item, C is its grammaticalcategory and feature specification ( if  other thanthe default set) and T is its translation into LR.Consider how we assign an LR to a sentencelike Every applicant is competent.
The translationof every supplies most of the st ructure of theuniversal quantif ication needed in LR.
Itrepresents a function from properties to functionsfrom properties to t ru th  values, so when appliedto applicant it yields a const ituent,  namely everyapplicant, which has one of the property slotsf i l led, and represents a funct ion from propertiesto t ruth-va lues ;  it is:(LAMBDA P (FORALL X((APPLICANT X) IMPLIES (P X) ) ) )This funct ion can now be applied to the funct iondenoted by competent, i.e.
( LAMBDA Y(EXPERT.LEVEL HIGH Y))This yields:(FORALL X((APPLICANT X)IMPLIES(LAMBDA Y(EXPERT.LEVEL HIGH Y))  X))And after one more lambda-conversion, wehave:( FORALL X((APPLICANT X)IMPLIES(EXPERT.LEVEL HIGH X) ) )Fig.
1 shows one parse tree that would begenerated by the above rules, together with itslogical translat ion.
The sentence is Bil linterviewed every applicant.
The complicatedtranslation of the VP is necessary becauseINTERVIEW is a one-place predicate that takes anent i ty - type  argument, not the type of functionthat every applicant denotes.
We thus defercombining the NP translation with the verb byusing Cooper storage.
A translation with a storedNP is represented above in angle-brackets.
Noticethat at the S node the NP every applicant is sti l lstored, but the subject is not stored.
It hasd i rect ly  combined with the VP, by taking the VPas an argument.
INTERVIEW is itself a two-placepredicate, but one of its argument places has beenfi l led by a place-holding variable, X1.
There isth~Js ~ only one slot left.
The translation can nowbe completed via the operations of StorageRetrieval and lambda conversion.
First, we simplifythe part of the semantics that isn't in storage:79Fig.
1.
A typical parse treeS<((LAMBDA P (P BILL))(INTERVIEW X1)),<(LAMBDA P (FORALL X ((APPLICANT X) IMPLIES (P X)) ) )  >>NP((LAMBDA P (P BILL)))VP<(INTERVIEW X1)(LAMBDA P (FORALL X((APPLICANT X)IMPLIES (P X))))>Bill VINTERVIEWI interviewedNP(LAMBDA P (FORALL X((APPLICANT X)IMPLIES (P X)) ) )~ i  ICANTDET applicantLAMBDA Q(LAMBDA P(FORALL X ((Q X)IMPLIES (P X)) ) )every((LAMBDA P (P BILL))(INTERVIEW X1)) :>((INTERVIEW Xl)  BILL)The function (LAMBDA P (P BILL)) has beenevaluated with P set to the value (INTERVIEWX1); this is a. conventional lambda-conversion.The rule for storage retrieval is to make aone-place predicate of the sentence translation bylambda-binding the placeholding variable, and thento apply the NP translation as a function to theresult.
The S-node translation above becomes:((LAMBDA P(FORALL X((APPLICANT X) IMPLIES (P X)) ) )(LAMBDA X1 ((INTERVIEW X1) BILL)))\[lambda-conversion\] ==>(FORALL X ((APPLICANT X) IMPLIES((LAMBDA X1((INTERVIEW X1) BILL)) X)))\[lambda-conversion\] : :>(FORALL X ((APPLICANT X) IMPLIES(((INTERVIEW X) BILL))))This is the desired final result.7.
CONCLUSIONWhat we have outlined is a natural languagesystem that is a direct implementation of alinguistic theory.
We have argued that in thiscase the linguistic theory has the special appeal ofcomputational tractabil ity (promoted by itscontext-freeness), and that the system as a wholeoffers the hope of a happy marriage of linguistictheory, mathematical logic, and advancedcomputer applications.
The system's theoreticalunderpinnings give it compatibility with currentresearch in Generalized Phrase Structure Grammar,and its augmented f i rst  order logic gives itcompatibility with a whole body of ongoingresearch in the field of model-theoretic semantics.The work done thus far is only the f i rststep on the road to a robust and practical naturallanguage processor, but the guiding principlethroughout has been extensibil ity, both of thegrammar, and of the applicability to variousspheres of computation.ACKNOWLEDGEMENTGrateful acknowledgement is given to twobrave souls, Steve Gadol and Bob Kanefsky, whohelped give this system some of its credibil ity byimplementing the actual hook-up with HIRE.Thanks are also due Robert Filman and BertRaphael for helpful comments on an early versionof this paper.
And a special thanks is dueRichard Weyhrauch, for encouragement, wiseadvice, and comfort in times of debugging.80APPENDIXThis appendix lists some sentences that areactually translated into HIRE and answered by thecurrent system.
Declarative sentences presentedto the system are evaluated with respect withtheir truth value in the usual way, and thus alsofunction as queries.SIMPLE SENTENCES1.
HP employs Egon.2.
Egon works for HP.3.
HP offered Montague the position.4.
HP gave Montague a job.5.
Montague got a job from HP.6.
Montague's job is at HP7.
HP's offer was to Capulet.8.
Montague had a meeting with Capulet.9.
Capulet has an offer from Xerox.10.
Capulet is competent.IMPERATIVES AND QUESTIONS11.
Find the programmers in CRCwho attended the meeting.12.
How many applicants for theposition are there?13.
Which manager interviewed Capulet?14.
Whose job did Capulet accept?15.
Who is a department manager?16.
Is there a LISP programmerwho Xerox hired?17.
Whose job does Montague have?18.
How many applicantsdid Capulet interview?RELATIVE CLAUSES19.
The professor whose student Xeroxhired visited HP.20.
The manager Montague met with hiredthe student who attended Berkeley.NOUN-NOUN COMPOUNDS21.
Some Xerox programmers visited HP.22.
Montague interviewed a job applicant.23.
Who are the department managers?24.
How many applicants have a LISPprogramming background?COORDINATION25.
Who did Montague interview and visit?26.
Which department's position didevery programmer and a managerfrom Xerox apply for?PASSIVE AND EXISTENTIAL SENTENCES27.
Egon was interviewed by Montague.28.
There is a programmerwho knows LISP in CRC.INFINITIVAL COMPLEMENTS29.
Montague managed to get a job at HP.30.
HP failed to hire a programmerwith Lisp programming background.REFERENCESBarwise, Jon, and John Perry.
1981.
"Situations and attitudes."
Journal ofPhilosophy 78, 668-692.Cooper, Robin.
1975.
Montague's SemanticTheory and Transformational Syntax.Doctoral dissertation, University ofMassachusetts, Amherst.Fillmore, Charles.
1968.
"The Case for Case.
"In Bach, Emmon and Robert Harms.Universals in Linguistic Theory.
NewYork: Holt, Rinehart and Winston.Filman, Robert E., John Lamping, and FanyaNlontalvo.
1982.
"Metalanguage andMetareasoning."
Submitted forpresentation at the AAAI NationalConference on Artificial Intelligence,Carnegie-Mellon University, Pittsburgh,Pennsylvania.Gallaire, Herv$, and Jack Minker, eds.
1978.Logic and Data Bases.
New York: PlenumPress.Gallaire, Herv$, Jack Minker, and Jean MarieNicolas, eds.
1981.
Advances in DateBase Theory.
New York: Plenum Press.Gazdar, Gerald.
1981.
"UnboundedDependencies and Coordinate Structure.
"Linguistic Inquiry 12, 155-184.Gazdar, Gerald.
1982.
"Phrase StructureGrammar."
In Pauline Jacobson andGeoffrey K. Pullum, eds.
The Nature ofSyntactic Representation.
Dordrecht: D.Reidel.Gazdar, Gerald, Geoffrey K. Pullum, and IvanA.
Sag.
In press.
"Auxiliaries andRelated Phenomena."
Language.Gazdar, Gerald, Geoffrey K. Pullum, Ivan A.Sag, and Thomas Wasow.
1982.
"Coordination and TransformationalGrammar".
Linguistic Inquiry 13.Jackendoff, Ray.
1977.
~" Syntax.
Cambridge:MIT Press.Kay, Martin.
1982.
"When Metarules are notMetarules."
Ms. Xerox Palo Alto ResearchCenter.Montague, Richard.
1970.
"The ProperTreatment of Quantification in English.
"in Richmond Thomason, ed.
1974.
FormalPhilosophy.
New Haven: Yale UniversityPress.Pratt, Vaughan R. 1975.
"LINGOL aprogress report."
Advance Papers of theFourth /nternational Joint Conference onArtificia/ /nte//igence, Tbilisi, Georgia,USSR, 3-8 September 1975.
Cambridge,MA: Artificial Intelligence Laboratory.422-428.Pullum, Geoffrey K. and Gerald Gazdar.1982.
Natural languages and context-freelanguages.
Linguistics and phitos.ophy 4.Sag, Ivan A.
1982.
"Coordination, Extraction,and Generalized Phrase StructureGrammar."
Linguistic Inquiry 13.Weyhrauch, Richard W. 1980.
"Prolegomena toa theory of mechanized formal reasoning.
"Artificial Intelligence, 1, pp.
133-170.81
