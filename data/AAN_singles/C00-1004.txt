Extended Models  and Tools for High-performance Part-of-speechTaggerMasayuki Asahara and Yuji MatsumotoGraduate School of Information Science, Nara Institute of Science and Technology8916-5, Taka.yama-cho, Ikoma-shi, Nara, 630-0101, Japan{masayu-a,matsu}@is.
a ist -nara,  ac.
jpAbst rac tStatistical part-of-st)eeeh(POS) taggers achieve highaccuracy and robustness when based oil large, scalemaimally tagged eorl)ora.
Ilowever, enhancementsof the learning models are necessary to achieve bet-ter 1)erforma.nce.
We are develol)ing a learningtool for a Jalmnese morphological analyzer calledCh, aScn.
Currently we use a fine-grained POS tagset with about 500 tags.
To al)l)ly a normal tri-gram model on the tag set, we need unrealistic sizeof eorl)ora.
Even, for a hi-gram model, we ean-no~, 1)ret)are a ll loderate size of an mmotated cor-pus, when we take all the tags as distinct.
A usualtechnique to Col)e with such fine-grained tags is toreduce the size of the tag set 1)y grouping the setof tags into equivalence classes.
We introduce theconcept of position-wise 9rouping where the tag setis t)artitioned into dill'el'lint equivalence classes ateach t)osition in the.
conditional 1)rohabilities in theMarkov Model.
Moreover, to eoi)e with the dataSl)arsen(?ss prot)lem caused 1) 3, exceptional t)henon>ena, we introduce several other techniques uch asword-level statistics, smoothing of word-level an(lP()S-level statistics and a selective tri-gram model.To help users determine probabilistic 1)arameters, weintroduce an error-driven method for the pm'mneterselection.
We then give results of exl)eriments to seethe effect of the tools applied to an existing Jat)anesemorphological nalyzer.1 IntroductionAlong with the increasing awfilability of mmotatedeorl)ora, a number of statistic P()S tat ters  havebeen developed which achieve high accuracy and ro-bustness.
On the other hand, there is still continu-ing demand for the iinprovement of learning lnod-els when sufficient quantity of annotated corporaare not available in the users domains or languages.Flexible tools for easy tuning of leanfing modelsare in demand.
We present such tools in this pa-per.
Our tools are originally intended for use withthe Japanese morphological nalyzer, ChaSen (Mat-sumoto et al, 1999), which at present is a statis-tical tagger based on the w~riable memory lengthMarker Model (lion et al, 1.994).
We first give abrief overview of the features of the learning tools.The t)art-of-speech tag set we use is a slightlymodified version of tile IPA POS tag set (RWCP,2000) with about 500 distinct POS tags.
The realtag set is even larger since some words are treatedas distim't P()S tags.
The size of the tag set is unre-alistic for buihting tri-grmn rules and even bi-gramrules which take all the tags as distinct.
The usualtechnique for coping with such fine-grained tags is toreduce the size of the tag set by groul)ing the set oftags into equivalence classes (Jelinek, 1.998).
We in-troduce the concept of position-wise grouping wherethe tag set is partitioned into different equivalence(;lasses at each position in the conditiolml probabili-lies in the Marker Model.
This feature is especiallyuseflfl for ,lapanese language analysis ince Jal)aneseis a highly (:onjugated language, where conjugationfOl'lllS have a great etfeet on the succeeding mor-1)homes, trot have little to do with the t)receding nlor-phemes.
Moreover, in colloquial anguage, a numberof eolltrael;ed expressio11s are eoinmon, where two ormore morphemes are central:ted into a single word.The contracted word behaves as belonging to dif-ferent t)arts-of-st)eech by connecting to the previousword or to the next word.
Position-wise grouping en-ables users to grouI) such words differently accordingto the positions in which they appear.Data sparseness i  always a serious problem whendealing with a large tag set.
Since it is unrealistic toadopt a simple POS tri-gram model to our tag set,we base our model on a hi-gram model and augmentit with selective tri-grams.
By selective tri-gram, wemean that only special contexts are conditioned bytri-gram model and are mixed with the ordinary bi-grmn model.
We also incorporate some smoothingtechniques for coping with the data sparseness prob-lel l l .By eolnbining these methods, we constructed thelearning tools for a high-lmrformance statistical mor-phological analyzer that are able to learn the prob-ability i)arameters with only a moderate size tagged('orl)us.The rest of this paper is structured as follows.21Section 2 discusses the basic concet)ts of tile statisti-cal morphological nalysis and some problems of thestatistical approach.
Section 3 presents the charac-teristics of the our learning tools.
Section 4 reportsthe result of some experiments and tile accuracy ofthe tagger in several settings.
Section 5 discusses re-lated works.
Finally, section 6 gives conclusions anddiscusses future works.Throughout this paper, we use morphologicalanalysis instead of part-of-speecll tagging sinceJapanese is an agglutinative language.
This is thestandard ternfinology in Japanese literatures.2 Pre l iminar ies2.1 Stat i s t i ca l  morphological analysisThe POS tagging problem or the Japanese morpho-logical analysis problem must do tokenization andfind the sequence of POS tags T = t l , .
?., t:,~ tot theword sequence W = wl , .
.
.
,  w,~ in the int)ut stringS.
Tile target is to find T that maxinfizes tile fol-lowing probability:Using the Bayes' rule of probability theory,P (W,T)  can be decomposed as a sequence of tileproducts of tag probabilities and word probabilities.P (TIW) P(T, W) = argmax P(W)= argu~}.xP(T,W)= .,': uv/xP(WlT)F(T)We assumed that tile word probability is con-strained only by its trig, and that the tag probabilityis constrained only by its preceding tags, either withthe t)i-grmn or the tri-gram model:P(WIT)  = HP(wi lt i )i=1P(T) = f l  P(tilti_l)i=1P(T) = fl P(ti\[ti-2,i-1) )i=1The values are estimated from tile frequencies intagged corpora using maximum likelihood estima-tion:p(w lti) - F (w"L)  r(t )F( t i _ , , tdP(t lt -l) -F(t;i-2,1,i-1, ti) =F(ti-2, ti-1)Using these parameters, tim most probable tag se-quence is determined using the Viterbi algorithm.2.2 H ierarch ica l  Tag SetWe use the IPA POS tag set (RWCP, 2000).
Thistag set consist of three eleinents: tile part -of  speech,the type of conjugation and tile form of conjugation(the latter two elements are necessary only for wordsthat conjugate).Tile POS tag set has a hierarchical structure: Thetop POE level consists of 15 categories(e.g., Noun,Verb, .
.
.
).
The second and lower levels are th, e sub-division level.
For example, Noun is fllrther subdi-vided into common nouns(general), llroper nomls,numerals, and so on.
Proper Noun is sul)dividedinto General, Person, Organization and Place.
Per-son and Place are subdivided again.
The bottomlevel of tile subdivision level is th.c word level, whichis conceptually regarded as a part of the subdivisionlevel.In the Japanese language, verbs, adjectives andauxiliary verbs have conjugation.
These are catego-rized into a fixed set of conjugation types(CTYPE),each of which has a fxed set of conjugal;ionforms(CFORM).
It is known that in Japanese thatthe CFORM varies according to the words appear-ing in the succeeding position.
Thus, at tile condi-tional position of the estimated tag probabilities, theCFORM plays an important role, while in the caseof other positions, they need not be distinguished.Figure 1 illustrates tile structure of the tag set.2.3 P rob lems in s tat is t ica l  mode lsOn the one hand, most of the i)rol)lems in statisticalnatural language processing stem fi'om the sparse-hess of training data.
In our case, tile nuinber of themost fine-grained tags (disregarding the word level)is about 500.
Even when we use the bi-gram model,we suffer from the data sparseness problem.
Thesituation is nmch worse in the case of the tri-grmnmodel.
This may be remedied by reducing tile tagset by grouping the tags into a smaller tag set.On the other hand, there are various kinds of ex-ceptions in language phenomena.
Some words havedifferent contextual features fi'om others in the sametag.
Such exceptions require a word or some group ofwords to be taken itself as a distinct part-of-speechor its statistics to be taken in distinct contexts.
Inour statistical earning tools, those exceptions arehandled by position-wise grouping, word-level statis-tics, smoothing of word-level and POS-level, and se-lective tri-gram model, which are described in turnill the next section.
These features enable users to22POS.,o 17571"rile subdivision level ~ I  I I Il he  word level GL~ WI~yO\] NL~ .
.
.
.
.
.
.
.
.
.
(;TYPE Godul>"K" G?dan"lS" I l l l  No Conjugation No ConjugationType Typetbrm form .... No Conjugation No ConjugationFigure 1: The examples of the hierarchical tag setadjust tile balance between fine and coarse grainedmodel settings.3 Features  o f  the  too lsThis section overviews characteristic timtures of timlearning tools for coping with the above, mentionedprolflems.a.
:t Pos i t ion -w ise  group ing  of  POS tagsSince we use a very fine-grained tag set, it is impor-tant to classit'y them into some equiva.lence classesl;o reduce the, size.
of i)rotmbilistic lmramelers.
More-over, as is discussed in the 1)revious ection, somewords or P()S bo, lmves ditli;rently according to Ihel)osition they at)pear.
In 3at)aneso, tbr instance,the CF()I/M play an iml)ortmlt role only to dis-mnbiguate the words at their succeeding position.In other words, the CFORM should be taken intoaccount only when they appear at tim position of1,i-1 in either bi-gram or tri-grain model ( t i - i  inI'(t~lt~__~ ) and P(t,\]t~_.,,t~_l)).
This means thatwhen the statistics of verbs are, taken, they should begrouped diflbrently according to the positions.
Not(',that, we named the positions; The current positionmeans the position of ti in the hi-gram statisticsP(tiIti-1) or the tri-grmn statistics P(till, i_.,, ti-~).The preceding position means the position of ti-1.The second preceding position means the position ofti-.2.There are quite a few contracted t~rms ill col-loquial expressions.
For example, auxiliary verb"chau" is a contracted tbrms consisting of two words"te(particle) + simau(auxiliary verb)" and behavesquite differently from other words.
One way to learnits statistical behavior is to collect various us~ges ofthe word and add the data to the training data aftercorrectly mmotating them.
In contrast, the idea ofpoint-wise grouping provides a nice alternative so-hltion to this problem.
By simply group this wordinto the same equivalence class of "te" for the cur-relfl; 1)osition I,i and grou I) it into the same equiva-le, nt class of "simau" for the t)rece(ling position ti-1in P ( t i \ ] t i - .
), it learns the statistical behavior fromthese classes.We now describe the point-wise grouping ill anlore precise way.
l?or simplicity, we assume, bi-gram model.
Let 7- = {A,/3,---} be ttw, originaltag set.
\?e introduce two partitions of the tag set,one is fin' the current position T ~ = {A (',/)~,..-},mid the other is for the preceding 1)osition T v ={AV,13v, ..
.}.
We define the equivalence mal>l)ingof the current position: I ( ' (T -~ T"), and anothermapping of the t)rece(ling position: U'('\]~ -4 "yv).lqgure, 2 shows an exalnple of the lmrtitiollS bythose, mapl)ings, where the equivalence mappingsI c = {el --> A c,L?
-9 A",C ~ A':,\]) -+ B",E -5W, .
.
.
}fv = {A --+ AV, \]\] -4 AJ', C -4 B v, D --+ B v, E -~Cl,...}Supl)ose we express the equivalence class to whichthe tag t, belongs as It\]" for the current position and\[t,\]v for the preceding position, then:= F(w , \[td\[W)1)(ti l l ,  i _ l )  =3.2 Word- leve l  s tat is t icsSeine words behave ditt'erently flom other wordseven in tile same POS.
Especially Japanese particles,auxiliary verbs a.nd some affixes are known to havedifferent e(mtextual behavior.
The tools can define2303gkuo - -b(DThe Preceding Positon Tag SetALB C~Dlc ?
FIG\[ H----~- -  B"  / D"- -~ .
.
.
.
.
.
.
.
.
.
', .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
i .
.
.
.
.
.
.
.03F-o = <n"EBOI-The Precedin( Position Tag SetWb 1Biil..ll,ll.q Wbr, B ,~Figure 2: Position-wise grouping of tagssome words as distinct POS and their statistics aretaken individually.The tag set T extends to a new tag set T ~xt thatdefines some words as individual POSs (the wordlevel).
Modification to the probability formulas forsuch word level tags is straightforward.Note that the statistics for POS level should bemodified when some words in the same group areindividuated.
Suppose that the tags A and B aredefined in tile T and some words l , l~ , .
.
.
,  l'l~,~ E Aand l, tZb~ , .
.
.
, I:F~,,~ C 17 arc individuated in 7" ~xt.
\Vedefine tags Ae,~,l, Bext~ C T ~:t as follows:Figure 3: the word extended tag setWe define two smoothing coetIicients: A~ is thesmoothing ratio for the current position and /~j, isthe smoothing ratio of the preceding position.
Thosevalues can be defined for each word.Suppose the word wi is individuated and its POSis ti.
If the current position is smoothed, then thetag probability is defined as follows (note that wiitself is an individuated tag):/5(wilti_l ) = ((1 - A~)P(tilti_\]) + A~P(wilti_l))If the word at the preceding positions is smoothed(assume ti-\] is the POS of wi-1):.
ao~ = ,4  \ 0v ,~, .
.
.
,~ , ,~ , ,}Bcxt = /) \ {wv~,...,wt,,,~}To estimate the probability for tile comlectionA-B, tile frequency F(Aea:t, Bext)  is used rather thanthe total frequency F(A ,  B) .
Figure 3 illustrate thetag set extension of this situation.These tag set extension is actually a special case ofposition-wise grouping.
The equivalence mappingsare fl'om all word level tags to T ~t.
The mappingI ~ maps all the words ill A~xt into A~t  and mapseach of {W~, .
.
.
,  14(~., } into itself.
In the same way,I p maps all the words in B~.~,t into B~:~t and mapseach of {Wb,, .
.
.
,  Wb,, } into itself.3.3 Smooth ing  of  word and POS levelstat ist icsWhen a word is individuated while its occurrencefrequency is not high, x~e have to accumulate in-stances to obtain enough statistics.
Another solu-tion is to smooth the word level statistics with POSlevel statistics.
In order to back-off the st)arseness ofthe words, we use the statistics of the POS to whichthe words belong.P( t i \ ]w i -1 )  = (1 - ~Xp)P(t i l t i - l )  + A~,F(t i lwi-~)If the both words of the positions is extend:~-  /~p( (1  - -  ~c)\]~(ti\[~Oi_l ) ~- )tcr('ll)i\]'ll)i_ 1 ))+(1 - Av)((1 - A~)P(t i l t i_ \ ]  ) + A~P(wi l t i - t ) )3.4 Select ive t r i -gram mode lSimple tri-gram models are not feasible for a largetag set.
As a matter of fact, only limited eases re-quire as long contexts as tri-grams.
We 1)rot)ose totake into account Olfly limited tri-gram instances,which we call sclcctive tri-flrams.
Our model isa mixture of such tri-gram statistics with bi-gramones .The idea of mixture of different context lengthis not new.
Markov Models with varial)le memorylength are proposed by Ron(Ron et al, 1994), inwhictl a mixtm'e model of n-grams with various valueof n is presented as well as its learning algorithms.In such a model, the set of contexts (the set of statesof the automata) should be mutually disjoint for theautomata to be deterministic and well-defined.We give a little different interpretation to tri-grmnstatistics.
We consider a tri-grmn as an exceptional24,, ......... ,o,,,o.
A IA~C The preceding position A I BF-B hIECA,.....;;....,.
A B I( .......Figure 4: Selective tr i-gramcontext.
Wheii a bi-grani context and a tri-granicontext have sonie intersection, the tri-gram contextis regarded as an exception within the, l)i-graui con-text.
In this sense, all tim cont, exts are, mutual lydisjoint as well in our niodel, and it is possii)h, toconvert our model into Ron's tormulal;ion.
I{owevei,we think that oul" %rnmlation is iilore straighforwardif the longer COlltex(;s ~-/1"(!
interln'eted as exc, el)i;ionsto (;lie shorter (;onte, xts.\Ve, assume that th(; grouping at the current 1)o-sit;ion ('7 -~) share the same grouping of the t)i-graincase.
But for the l)re(:eding l)osition and (;lie s(!
(:ond1)receding 1)osition, we can deline ditl'erent groupingsof tag sets fl'om those of the bi-gram case.
We intro-duce the two new tag sets tbr the preceding positions:The tag sol; of the preceding position:"P/- {W", S/,...}The tag set of the s(!
(',()ii(l pre(:(!ding l>().~ition:.T),, ' _ { A I',' , H~,#.
.
.
}We define the equiv~tlence mal)l)ing for the 1)re -ceding position: I p' (7- 4 "Y p'), and tile nml)ping fortile second 1)receding position: I s';/ (7- -4  "Y Jm' ).
As-stoning that an equivalence classes for 1, detined bythe mapping I pp' is expressed as \[t\] pj/, the, tri-granit)robal)ility is defined natural ly as fl)llows:P(t i l t~- ' , ,  t i - i  ) - -  s ' ( \ [ I ,d" lb ,<- .
_ , \ ] ' " " , \ [ l ,~- , \ ] ' " )F(\[t.~_.4'"', \[l.~_, \],", It,;\]")F(\[ti_~\]m", rl.
,1,.>'~ L ' t - - J  J \]Figure 4 shows an image of fl'equency counts fortr i-gram model.In case some hi-gram COlltext overlaps with a tri-grmn context, the bi-graln statistics are taken byexcluding the tri-gram statistics.For (:xmnl)h:, if we inchide (.lie tri-grmn contextA - C - \]7 in our model, then the slat,)sties of the hi-grail) COilteX(; C-  13 is taken as folh>ws (F  stands fortrue, frequency in training corpora while F '  standsfor estimated frequency to lie used for 1)robat.>ilitycalculation):s,"(c, . )
= s~'(c, J3) - F (A ,  C, ix)Since selection of t i i -gram contexts is not easytask, the tools supports the selection based on mlerror-driven method.
We omit the detail because ofthe sl)a(:e limitation.3.5 Est i inat io l i  for unseen  words  in eor i )usSince not all the words in (;lie dictionary appearin the training corpus, |;lie occurrence probabilityi>f miseen words should 1)e allocated ill $Olil(: way.There are a number of method for estimating un-se, en events.
Our era'rent ool adopts Lidstone's lawof succession, wlfieh add ;~ fixed (:omit to each obser-vat)Oil.~'('.,10 = F(,., ,  t) + ~E, ,~ F(,.,  t) + ~: .
ItlAt 1)resent, l;he de, fault frequency (:omit, (t is set (o0.5.4 Exper iments  and  Eva luat ionFor evaluating how the 1)rol)osed extension lint)rovesa normal t)i-glain model, we condllcted several ex-periments.
We group verl)s according (o the con-jugation forms at the preceding i/osition, take wordlevel statistics for all l/articles, auxiliary verbs andsynll)ols, each of which is smoothed with the illlliie,-dial:ely higher P()S level.
Selective, tri-grani contextsare defined for dist:riniinating a few notoriously ;lil/-hi~uous particle "no" and auxiliary ve, i'l)s "nai" and"aruY This is a very simple extension but sufficestbr evaluating the ett'ect of the learning tools.We use 5-tbld cross ewfluation over (;he RWCPtagged corpus (RXVCP, 2000).
The corpus (:late sizeis 37490 sentences(958678 words).
The errors of thecorpus are manually rood)tied.
The annotated cor-pus is divided into the traiifing data set(29992 sen-tences, 80%) and the test data set(7498 se, ntence, s20%).
Experinients were repeated 5 times, and thereslllts \v(;r(; averaged.The, evaluation is done at the following 3 levels:?
le, vell: only word segmentation (tokenizati(m)is ewduated?
level2: word segmentation mid (;lie toI) levelpart-of-speech are ewfluated?
level3: all infornmtion is taken into at, count forevaluationUsing the tools, we create the following six models:D: hernial bi-granl modelD,,, :  D + word level statistics for particles, etc.25Table 1: Results for test data (F-value %)datasetD 98.69 98.12 96.91Dw 98.75 98.24 97.22Dw.~t 98.80 98.26 97.20Dws 98.76 98.27 97.23Dwqt 98.78 98.35 97.27Table 2: Results for learning data (F-value %)datasetD 98.84 98.36 92.36D,o 98.96 98.58 97.81Dwq 98.92 98.46 97.6tDw~ 98.96 98.58 97.80D,vgt 98.92 98.55 97.70Dwo: Dw + groupilLgDw,: D,o + smoothing of word level with POSlevelD,~,at: Dwo + selective tri-grmnThe smoothing rate between the part-of-sl)eechand the words is fixed to 0.9 for each word.To evahmte the results, we use tile F-value definedby the tbllowing formulae:number of cor rect  words12ccall =number of words in  corpusnumber of cor rec t  wordsPrecision = number of words by system output(/32 + 1) - l~,ecall.
PrecisionF~ = /32 ?
(Precision + 12ccall)For each model, we evaluate the F-value (with ~ =1) for tim learlfing data and test data at; each level.The results are given in the Tables 1 and 2.From the results the tbllowing observation is pos-sible:Smoothing improve on grouping dataset in testdata slightly.
But in tile other enviromnents theaccuracy isn't improved.
In this experiment, thesmoothing rate for all words is fixed.
We need tomake the different rate for each word in the futurework.The grouping performs good result for the testdataset.
It is natural that the grouping is not goodfor learning dataset since all the word level statisticsare learned in the case of learning dataset.Finally, the selective tri-gram (only 25 rulesadded) achieves non-negligible improvement atlevel2 and level3.
Compared with the normal bi-gram Inodel, it improves about 0.35% on level3 andabout 0.2% on level2.5 Re la ted  workCutting introduced grouping of words into equiv-a.lence classes based on the set of possible tags toreduce the number of the parameters (Cutting etal., 1992) .
Schmid used tile equivaleuce classes forsmoothing.
Their classes define not a partition ofPOS tags, but mixtures of some POS tags (Schmid,1995).Brill proposed a transfbrmation-based method.
Inthe selection of tri-gram contexts we will use a sim-ilar technique (Brill, 1995) .Haruno constructed variable length models basedon the mistake-driven methods, and mixed these tagmodels.
They do not have grouping or smoothingfacilities (Haruno and Matsumoto, 1997).Kitauchi presented a method to determine refine-meat of the tag set by a mistake-driven technique.Their inethod determines the tag set according tothe hierarchical definition of tags.
Word level dis-crimination and grouping beyond the hierarchicaltag structure are out of scope of their method (Ki-tauchi el; al., 1999).6 Conc lus ion  and  Future  worksWe proposed several extensions to the statisticalmodel for Japanese morphological nalysis.
We alsogave preliminary experiments and showed tile effectsof the extensions.Counting some words individually and smooth-ing them with POS level statistics alleviate the datasparseness problem.
Position-wise grouping enablesan eflk~ctive refiimment of the probability parametersettings.
Using selective tri-grain provides an easydescription of exceptional language phenomena.In our future work, we will develol) a method to re-fine the models automatically or semi-automatically.For example, error-driven methods will be applica-ble to the selection of the words to be individuatedand the useflfl tri-gram contexts.For the morphological nalyzer Ch, aSen, we are us-ing the mixture modeh Position-wise grouping usedfor conjugation.
Smoothing of tile word level andthe POS level used tbr particles.The analyzer and the learning tools are availablepublicly i .ReferencesE.
Brill.
1995.
Transformation-Based Error-DrivenLearning and Natural Language Processing: ACase Study ill Part-ofSpeecll Tagging.
Compu-tational Linguistics, 21(4):543 565.D.
Cutting, J. Kut)iec, J. Pedersen, and P. Sibun.1992.
A pracl;ical part-of-speech tagger.
In Pro-cccdings of the Third Conference on Applied Nat-ural Language Processing.1 http : l / e l .
aist-nara, ac.
jpllab/nlt/chasen/26M.
tlaruno and 3;.
Matsumoto.
1997.
Mistake-\])riven Mixtur(; of Iti(;rarchical '.
?~g; Contcxl; Tre(~s.In 35th, Annual Meeting of the Association for(7om, puational Linguistics and 8th Co'nfcv('.ncc ofth, c European Chapter of tit(: Association for Com-putational Linguistics, 1)ages 230 237, July.F.
Jelinek.
1998.
Statistical Methods for @cechRecognition.
MIT Press.A.
Kitauchi, T. Utsm'o, and Y. Matsmnoto.
1999.Probabilistic Model Le~arning tbr JatmlmSC' Mor-1)hological Analysis 1)3' lgrror-driven Feat;,rc Se-lection (in .lal)mmse).
'J}'(t~t.sa(:l/io'l~, f \]'nfi)rmatio'nl'roccssi'ng Sci('ty of ,\]apa'n, 40(5):2325 2337, 5.Y.
Matsmnoto, A. Kitau('hi, T. Ymmtshita, Y.
1\]i-mno, H. M~tsuda, and 54.
Asahm'a.
1999.Japanese MorphologicM Analyzer ChaSen UsersMa.mml version 2.0.
Technical l/el)oft NAIST-IS-T1~99012, Nma Institute of Science mM ~ibx:lmol-ogy ~l~(:lmicM lR,eport.l).
I~.on, Y.
Singer, and N. Tishby.
1994.
\]A~A/I'll -ing Prolml)ilistic Automal a with Vm'iM)lc MemoryLength.
In COLT-g4, tinges 35 ~16.\]/,WCP.
2000.
I{\YC Tt~xt l)atabas(~.http:/ /www, rwcp.
or.
j p /wswg/rwcdb/ text / .H.
Schmid.
1995.
Inproveln(;nts In l)art-of-S1)e(.,(:hTagging With an Applic~tion To (-lermmL InIM6'L SIGDA'\]' workshop, tinges ~'17- 50.27
