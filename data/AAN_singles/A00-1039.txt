Unsupervised Discovery of Scenario-Level Patterns forInformation ExtractionRoman Yangarber  Ra lph  Gr i shmanroman?as, nyu.
edu grishman?cs, nyu.
eduCourant Inst i tute of Courant Inst i tute ofMathematical  Sciences Mathematical  SciencesNew York University New York UniversityPas i  Tapana inen  )~ Si l ja  Hut tunen $tapanain?conexor, fi sihuttun~ling.helsinki, fit Conexor Oy :~ University of HelsinkiHelsinki, F inland F in landAbst rac tInformation Extraction (IE) systems are com-monly based on pattern matching.
Adaptingan IE system to a new scenario entails theconstruction of a new pattern base---a time-consuming and expensive process.
We haveimplemented a system for finding patterns au-tomatically from un-annotated text.
Startingwith a small initial set of seed patterns proposedby the user, the system applies an incrementaldiscovery procedure to identify new patterns.We present experiments with evaluations whichshow that the resulting patterns exhibit highprecision and recall.0 I n t roduct ionThe task of Information Extraction (I-E) isthe selective extraction of meaning from freenatural language text.
I "Meaning" is under-stood here in terms of a fixed set of semanticobjects--entities, relationships among entities,and events in which entities participate.
Thesemantic objects belong to a small number oftypes, all having fixed regular structure, withina fixed and closely circumscribed subject do-main.
The extracted objects are then stored ina relational database.
In this paper, we use thenomenclature accepted in current IE literature;the term subject domain denotes a class of tex-tual documents to be processed, e.g., "businessnews," and scenario denotes the specific topicof interest within the domain, i.e., the set offacts to be extracted.
One example of a sce-nario is "management succession," the topic ofMUC-6 (the Sixth Message Understanding Con-ference); in this scenario the system seeks toidentify events in which corporate managers left1For general references on IE, cf., e.g., (Pazienza,1997; muc, 1995; muc, 1993).their posts or assumed new ones.
We will con-sider this scenario in detail in a later sectiondescribing experiments.IE systems today are commonly based on pat-tern matching.
The patterns are regular ex-pressions, stored in a "pattern base" containinga general-purpose component and a substantialdomain- and scenario-specific component.Portability and performance are two majorproblem areas which are recognized as imped-ing widespread use of IE.
This paper presents anovel approach, which addresses both of theseproblems by automatically discovering goodpatterns for a new scenario.
The viability ofour approach is tested and evaluated with anactual IE system.In the next section we describe the problem inmore detail in the context of our IE system; sec-tions 2 and 3 describe our algorithm for patterndiscovery; section 4 describes our experimentalresults, followed by comparison with prior workand discussion, in section 5.1 The  IE  Sys temOur IE system, among others, contains a a back-end core engine, at the heart of which is aregular-e~xpression pattern matcher.
The enginedraws on attendant knowledge bases (KBs) ofvarying degrees of domain-specificity.
The KBcomponents are commonly factored out to makethe systems portable to new scenarios.
Thereare four customizable knowledge bases in our IEsystem: the Lexicon contains general dictionar-ies and scenario-specific terms; the concept basegroups terms into classes; the predicate base de-scribes the logical structure of events to be ex-tracted, and the pattern base contains patternsthat catch the events in text.Each KB has a. substantial domain-specificcomponent, which must be modified when mov-282ing to new domains and scenarios.
The systemallows the user (i.e.
scenario developer) to startwith example sentences in text which containevents of interest, the candidates, and general-ize them into patterns.
However, the user isultimately responsible for finding all the can-didates, which amounts to manually processingexample sentences in a very large training cor-pus.
Should s/he fail to provide an exampleof a particular class of syntactic/semantic con-struction, the system has no hope of recoveringthe corresponding events.
Our experience hasshown that (1) the process of discovering candi-dates is highly expensive, and (2) gaps in pat-terns directly translate into gaps in coverage.How can the system help automate the pro-cess of discovering new good candidates?
Thesystem should find examples of all common lin-guistic constructs relevant o a scenario.
Whilethere has been prior research on identifying theprimary lexical patterns of a sub-language orcorpus (Grishman et al, 1986; Riloff, 1996), thetask here is more complex, since we are typi-cally not provided in advance with a sub-corpusof relevant passages; these passages must them-selves be found as part of the discovery process.The difficulty is that one of the best indicationsof the relevance of the passages i precisely thepresence of these constructs.
Because of thiscircularity, we propose to acquire the constructsand passages in tandem.2 So lu t ionWe outline our procedure for automatic ac-quisition of patterns; details are elaborated inlater sections.
The procedure is unsupervisedin that it does not require the training corpusto be manually annotated with events of inter-est, nor a pro-classified corpus with relevancejudgements, nor any feedback or interventionfrom the user 2.
The idea is to combine IR-styledocument selection with an iterative relaxationprocess; this is similar to techniques used else-where in NLP, and is inspired in large part, ifremotely, by the work of (Kay and RSscheisen,1993) on automatic alignment of sentences andwords in a bilingual corpus.
There, the reason-ing was: sentences that are translations of each2however, it may be supervised after each iteration,where the user can answer yes/no questions to improvethe quality of the resultsother are good indicators that words they con-tain are translation pairs; conversely, words thatare translation pairs indicate that the sentenceswhich contain them correspond to one another.In our context, we observe that documentsthat are relevant to the scenario will neces-sarily contain good patterns; conversely, goodpatterns are strong indicators of relevant docu-ments.
The outline of our approach is as follows...Given: (1) a large corpus of un-annotatedand un-classified documents in the domain;(2) an initial set of trusted scenario pat-terns, as chosen ad hoc by the user--theseed; as will be seen, the seed can be quitesmall--two or three patterns eem to suf-fice.
(3) an initial (possibly empty) set ofconcept classesThe pattern set induces a binary partition(a split) on the corpus: on any document,either zero or more than zero patterns willmatch.
Thus the universe of documents, U,is partitioned into the relevant sub-corpus,R, vs. the non-relevant sub-corpus, R =U - R, with respect o the given patternset.
Actually, the documents are assignedweights which are 1 for documents matchedby the trusted seed, and 0 otherwise.
32.
Search for new candidate patterns:(a) Automatically convert each sentencein the corpus,into a set of candidatepatterns, 4(b) Generalize each pattern by replacingeach lexical item which is a member ofa concept class by the class name.
(c) Working from the relevant documents,select those patterns whose distribu-tion is strongly correlated with otherrelevant documents (i.e., much more3R represents he trusted truth through the discoveryiterations, since it was induced by the manually-selectedseed.4Here, for each clause in the sentence we extract atuple of its major roles: the head of the subject, theverb group, the object, object complement, asdescribedbelow.
This tuple is considered to be a pattern for thepresent purposes of discovery; it is a skeleton for therich, syntactically transformed patterns our system usesin the extraction phase.283densely distributed among the rele-vant documents than among the non-relevant ones).
The idea is to considerthose candidate patterns, p, whichmeet the density, criterion:IHnRI IRI - - > >IHnUI IUIwhere H = H(p) is the set of docu-ments where p hits.
(d) Based on co-occurrence with the cho-sen patterns, extend the conceptclasses.3.
Optional: Present he new candidates andclasses to the user for review, retainingthose relevant o the scenario.4.
The new pattern set induces a new parti-tion on the corpus.
With this pattern set,return to step 1.
Repeat he procedure un-til no more patterns can be added.3 Methodo logy3.1 Pre-proeess ing:  Normal i za t ionBefore applying the discovery procedure, wesubject the corpus to several stages o f  pre-processing.
First, we apply a name recognitionmodule, and replace each name with a tokendescribing its class, e.g.
C-Person, C-Company,etc.
We collapse together all numeric expres-sions, currency values, dates, etc., using a singletoken to designate ach of these classes.3.2 Syntact ic  Analys isWe then apply a parser to perform syntacticnormalization to transform each clause into acommon predicate-argument structure.
We usethe general-purpose d pendency parser of En-glish, based on the FDG formalism (Tapanainenand J~rvinen, 1997) and developed by the Re-search Unit for Multilingual Language Technol-ogy at the University of Helsinki, and ConexorOy.
The parser (modified to understand thename labels attached in the previous step) isused for reducing such variants as passive andrelative clauses to a tuple, consisting of severalelements.1.
For each claus, the first element is the sub-ject, a "semantic" subject of a non-finitesentence or agent of the passive.
52.
The second element is the verb.3.
The third element is the object, certainobject-like adverbs, subject of the passiveor subject complement 64.
The fourth element is a phrase whichrefers to the object or the subject.
Atypical example of such an argument isan object complement, such as Com-pany named John Smith pres ident .
An-other instance is the so-called copredica-tire (Nichols, 1978), in the parsing system(J~irvinen and Tapanainen, 1997).
A co-predicative refers to a subject or an object,though this distinction is typically difficultto resolve automatically/Clausal tuples also contain a locative modifier,and a temporal modifier.
We used a corpus of5,963 articles from the Wall Street Journal, ran-domly chosen.
The parsed articles yielded a to-tal of 250,000 clausal tuples, of which 135,000were distinct.3.3 Genera l i za t ion  and  Concept  ClassesBecause tuples may not repeat with sufficientfrequency to obtain reliable statistics, each tu-ple is reduced to a set of pairs: e.g., a verb-object pair, a subject-object pair, etc.
Eachpair is used as a generalized pattern duringthe candidate selection stage.
Once we haveidentified pairs which are relevant o the sce-nario, we use them to construct or augment con-cept classes, by grouping together the missingroles, (for example, a class of verbs which oc-cur with a relevant subject-object pair: "com-pany (hire/fire/expel...} person").
This is sim-ilar to work by several other groups whichaims to induce semantic lasses through syn-tactic co-occurrence analysis (Riloff and Jones,1999; Pereira et al, 1993; Dagan et al, 1993;Hirschman et al, 1975), although in .our casethe contexts are limited to selected patterns,relevant o the scenario.SE.g., " John sleeps", "John is appointed byCompany" ,  "I saw a dog which sleeps", "She askedJohn  to buy a car".6E.g., " John is appointed by Company", "John is thepres ident  of Company", "I saw a dog which sleeps",The dog  which I saw sleeps.7For example, "She gave us our coffee black",  "Com-pany appointed John Smith as pres ident" .2843.4 Pattern DiscoveryHere we present he results from experimentswe conducted on the MUC-6 scenario, "man-agement succession".
The discovery procedurewas seeded with a small pattern set, namely:Subject Verb Direct ObjectC-Company C-Appoint C-PersonC-Person C-ResignDocuments are assigned relevance scores ona scale between 0 and 1.
The seed patternsare accepted as ground truth; thus the docu-ments they match have relevance 1.
On sub-sequent iterations, the newly accepted patternsare not trusted as absolutely.
On iteration um-ber i q- 1, each pattern p is assigned a precisionmeasure, based on the relevance of the docu-ments it matches:Here C-Company and C-Person denote se-mantic classes containing named entities of thecorresponding semantic types.
C-Appoirlt de-notes a class of verbs, containing four verbs{ appoint, elect, promote, name}; C-Resign ={ resign, depart, quit, step-down }.During a single iteration, we compute thescore s, L(p), for each candidate pattern p:L(p) = Pc(P)" log {H A R\] (1)where R denotes the relevant subset, and H --H(p) the documents matching p, as above, and\[gnR\[ Pc(P) -- Igl is the conditional probability ofrelevance.
We further impose two support cri-teria: we distrust such frequent patterns where\[HA U{ > a\[U\[ as uninformative, and rare pat-terns for which \[H A R\[ </3  as noise.
?
At theend of each iteration, the system selects the pat-tern with the highest score, L(p), and adds it tothe seed set.
The documents which the winningpattern hits are added to the relevant set.
Thepattern search is then restarted.3.5 Re-computat lon  of  DocumentRelevanceThe above is a simplification of the actual pro-cedure, in several important respects.Only generalized patterns are considered forcandidacy, with one or more slots filled withwild-cards.
In computing the score of the gen-eralized pattern, we do not take into considera-tion all possible values of the wild-card role.
Weinstead constrain the wild-card to those valueswhich themselves in turn produce patterns withhigh scores.
These values then become membersof a new class, which is output in tandem withthe winning pattern 1?Ssimilarly to (Riloff, 1996)?U denotes the universe of documents.
We used c~ =0.i and ~----- 2.1?The classes are currently unused by subsequent i er-ations; this important issue is considered in future work.Preci+l(p) = 1 {H(p){ ~ Reli(d) (2)dEH(p)where Reli(d) is the relevance of the documentfrom the previous iteration, and H(p) is the setof documents where p matched.
More generally,if K is a classifier consisting of a set of patterns,we can define H(K) as the set of documentswhere all of patterns p E K match, and the"cumulative" precision 11 of K asPreci+l(K) = 1 ~ Reli(d) (3)IH(K)\[ riCH(K)Once the new winning pattern is accepted,the relevance scores of the documents are re-adjusted as follows.
For each document d whichis matched by some (non-empty) subset of thecurrently accepted patterns, we can view thatsubset of patterns as a classifier K d = {py}.These patterns determine the new relevancescore of the documentReli+l(d) = max (Rel~(d),Prec~+l(Kd)) (4)This ensures that the relevance score growsmonotonically, and only when there is sufficientpositive evidence, as the patterns in effect vote"conjunctively" on the documents.
The resultswhich follow use this measure.Thus in the formulas above, R is not sim-ply the count of the relevant documents, butis rather their cumulative relevance.
The twoformulas, (3) and (4), capture the mutual de-pendency of patterns and documents; this re-computation and growing of precision and rele-vance scores is at the heart of the procedure.11Of course, this measure is defined only whenH(K) # 0.2854 Resu l ts  1An objective measure of goodness of a pattern o.
9is not trivial to establish since the patterns can-not be used for extraction directly, without be- o. sing properly incorporated into the knowledgebase.
Thus, the discovery procedure does not o. vlend itself easily to MUC-style evaluations, ince0.6  a pattern lacks information about which eventsit induces and which slots its arguments should 0.5fill.However, it is possible to apply some objec- o. ative measures of performance.
One way we eval-uated the system is by noting that in addition o.to growing the pattern set, the procedure alsogrows the relevance of documents.
The latter o.
2can be objectively evaluated.0.1We used a test corpus of 100 MUC-6 formal-training documents (which were included in the omain development corpus of about 6000 docu-ments) plus another 150 documents picked atrandom from the main corpus and judged byhand.
These judgements constituted the groundtruth and were used only for evaluation, (not inthe discovery procedure).4.1 Text  F i l ter ingFigure 1 shows the recall/precision measureswith respect to the test corpus of 250 docu-ments, over a span of 60 generations, tartingwith the seed set in table 3.4.
The Seed pat-terns matched 184 of the 5963 documents, yield-ing an initial recall of .11 and precision of .93;by the last generation it searched through 982documents with non-zero relevance, and endedwith .80 precision and .78 recall.
This facet ofthe discovery procedure is closely related to theMUC '%ext-filtering" sub-task, where the sys-tems are judged at the level of documents ratherthan event slots.
It is interesting to compare theresults with other MUC-6 participants, shownanonymously in figure 2.
Considering recall andprecision separately, the discovery procedure at-tains values comparable to those achieved bysome of the participants, all of which were ei-ther heavily-supervised or manually coded sys-tems.
It is important o bear in mind that thediscovery procedure had no benefit of trainingmaterial, or any information beyond the seedpattern set.I I I I I I......i"{+X'N+~v i P ~ e c i s i o n  ' i" "  ... .
i .
.~ .
i Re~aa - - -?- - -.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
....... iiiiiiiiiiiiiilEi   ........... ......../.... .
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
'."
.
.
.
.
.
.
.
.
": .
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
r .
.
.
.
.
.
.
.
.
.
~ .. .
.
.
.
.
.
.
.
!
.
.
.
.
.
.
.
.
.2111111ji.. iii121122;1211111;ii122221ilSiiii12112121SiiiiSiii: .
.
.
.
.
.0 I0  20  30  40  50  60  70G e n e r a t i o n  #80Figure h Recall/Precision curves for Manage-ment Succession4.2 Cho ice  of  Test  CorpusFigure 2 shows two evaluations of our discoveryprocedure, tested against the original MUC-6corpus of 100 documents, and against our testcorpus, which consists of an additional 150 doc-uments judged manually.
The two plots in thefigure show a slight difference in results, indi-cating that in some sense, the MUC corpus wasmore "random", or that our expanded corpuswas somewhat skewed in favor of more commonpatterns that the system is able to find moreeasily.4.3 Cho ice  of  Eva luat ion  Met r i cThe graphs shown in Figures 1 and 2 are basedon an "objective" measure we adopted duringthe experiments.
This is the same measure ofrelevance used internally by the discovery proce-dure on each iteration (relative to the "truth" ofrelevance scores of the previous iteration), andis not quite the standard measure used for textfiltering in IR.
According to this measure, thesystem gets a score for each document based onthe relevance which it assigned to the document.Thus if the system .assigned relevance of X per-cent to a relevant document, it only received X2860( I )0 .
90 .
80 .
7I I  I I I I I I : : .
, .
: ?.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
- - -  !
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.i i i :: i i i i i ~.
.
.
.
.
.
i .
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
\[ .
.
.
.
.
.
.
?
.. .
.
.
.
.
.
f .
.
.
.
.
.
.
.
.
.
.
.
T .
.
.
.
.
.
.. .
.
.
.
.
J .
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
; .
.
.
.
.
.
.
.
.
.
.
..: .
.
.
.
.
.
.
.
i .
.
.
.
:0 6 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
'7 .
.
.
.
.
.
.  '
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.iii0 .
5 .
.
.
.
.
.
.
'.
.
.
.
.
.
.
.
, .
.
.
.
.
.
.
.
~" .
.
.
.
.
.
.
, .
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
* .
.
.
.
.
.
.
"=.
.
.
.
.
.
.
.
, .
.
.
.
.
.
.i z0 .
4  i I0 0 .
1 0 .
2 0 .
3 0 .
4 0 .
5 0 .
6 0 .
7 0 .
8 0 .
9  1R e c a l lFigure 2: Precision vs. RecallIi .
i ~ iB  i. .
.
.
.
.
e .
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
!
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
~ .
.
.  '
.
.
.
.
.
.
.
.
!
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
0 .
9i i i i i im~ !
!!
i i D iE  c!
i i i i i i i i0 .
'7  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~" .
.
.
.
.
.
.
.
.
.
.
.
.
.
.C o n ~ i n ~ o u -  ?
~ut i - -o  f0 .
6 .
.
.
.
.
.
~ .
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.  '
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
; .
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.0 .
5 ...... ~ ........ , ........ ~ ....... ~ ........ ~ ....... ~ ........ !
........ : ........ 4 .......0 .
40 0 .
i 0 .
2 0 .
3 0 .
4 0 .
5 0 .
6 0 .
7 0 .
8 0 .
9  1R e c a l lFigure 3: Results on the MUC corpuspercent on the recall score for classifying thatdocument correctly.
Similarly, if the system as-signed relevance Y to an irrelevant document,it was penalized only for the mis-classified Ypercent on the precision score.
To make our re-sults more comparable to those of other MUCcompetitors, we chose a cut-off point and forcethe system to make a binary relevance decisionon each document.
The cut-off of 0.5 seemedoptimal from empirical observations.
Figure 3shows a noticeable improvement in scores, whenusing our continuous, "objective" measure, vs.the cut-off measure, with the entire graph essen-tially translated to the right for a gain of almost10 percentage points of recall.4.4 Eva luat ing  Pat ternsAnother effective, if simple, measure of perfor-mance is  how many of the patterns the pro-cedure found, and comparing them with thoseused by an extraction engine which was manu-ally constructed for the same task.
Our MUC-6system used approximately 75 clause level pat-terns, with 30 distinct verbal heads.
In oneconservative experiment, we observed that thediscovery procedure found 17 of these verbs, or57%.
However, it also found at least 8 verbs themanual system lacked, which seemed relevant tothe scenario:company-bring-person-\[as?officer\] 12person-come-\[to+eompanv\]-\[as+oZScer\]person-rejoin- company-\[as + o25cer\]person-{ ret  , conti,  e, remai, ,stay}-\[as + o25cer\]person-pursue-interestAt the risk of igniting a philosophical de-bate over what is or is not relevant o a sce-nario, we note that the first four of these verbsare evidently essential to the scenario in thestrictest definition, since they imply changes ofpost.
The next three are "staying" verbs, andare actually also needed, since higher-level infer-ences required in tracking events for long-rangemerging over documents, require knowledge ofpersons occupying posts, rather than only as-suming or leaving them.
The most curious oneis "person-pursue-interesf'; urprisingly, it toois useful, even in the strictest MUC sense, cf.,(muc, 1995).
Systems are judged on filling aslot called "other-organization", i dicating fromor to which company the person came or went.This pattern is consistently used in text to indi-nbracketed  const i tuents  a re  outs ide  o f  the  cent ra lSVO t r ip le t ,  inc luded here  fo r  c la r i ty .287cate that the person left to pursue other, undis-closed interests, the knowledge of which wouldrelieve the system from seeking other informa-tion in order to fill this slot.
This is to say thathere strict evaluation is elusive.5 D iscuss ion  and  Cur rent  WorkSome of the prior research as emphasized in-teractive tools to convert examples to extractionpatterns, cf.
(Yangarber and Grishman, 1997),while others have focused on methods for au-tomatically converting a corpus annotated withextraction examples into such patterns (Lehn-ert et al, 1992; Fisher et al, 1995; Miller etal., 1998).
These methods, however, do not re-duce the burden of finding the examples to an-notate.
With either approach, the portabilitybottleneck is shifted from the problem of build-ing patterns to that of finding good candidates.The prior work most closely related to thisstudy is (Riloff, 1996), which, along with (Riloff,1993), seeks automatic methods for filling slotsin event templates.
However, the prior workdiffers from that presented here in several cru-cial respects; firstly, the prior work does not at-tempt to find entire events, after the fashionof MUC's highest-level scenario-template task.Rather the patterns produced by those systemsidentify NPs that fill individual slots, withoutspecifying how these slots may be combinedat a later stage into complete vent templates.The present work focuses on directly discoveringevent-level, multi-slot relational patterns.
Sec-ondly, the prior work either relies on a set ofdocuments with relevance judgements to findslot fillers where they are relevant o events,(Riloff, 1996), or utilizes an un-classified cor-pus containing a very high proportion of rele-vant documents o find all instances of a seman-tic class, (Riloff and Jones, 1999).
By contrast,our procedure requires no relevance judgements,and works on the assumption that the corpus isbalanced and the proportion of relevant docu-ments is small.
Classifying documents by hand,although admittedly easier than tagging eventinstances in text for automatic training, is stilla formidable task.
When we prepared the testcorpus, it took 5 hours to mark 150 short doc-uments.The presented results indicate that ourmethod of corpus analysis can be used to rapidlyidentify a large number of relevant patternswithout pre-classifying a large training corpus.We are at the early stages of understandinghow to optimally tune these techniques, andthere are number of areas that need refinement.We are working on capturing the rich informa-tion about concept classes which is currently re-turned as part of our pattern discovery proce-dure, to build up a concept dictionary in tandemwith the pattern base.
We are also consider-ing the proper selection of weights and thresh-olds for controlling the rankings of patterns anddocuments, criteria for terminating the itera-tion process, and for dynamic adjustments ofthese weights.
We feel that the generalizationtechnique in pattern discovery offers a greatopportunity for combating sparseness of data,though this requires further research.
Lastly,we are studying these algorithms under severalunrelated scenarios to determine to what extentscenario-specific phenomena affect their perfor-mance.ReferencesIdo Dagan, Shaul Marcus, and ShaulMarkovitch.
1993.
Contextual word simi-larity and estimation from sparse data.
InProceedings of the 31st Annual Meeting ofthe Assn.
for Computational Linguistics,pages 31-37, Columbus, OH, June.David Fisher, Stephen Soderland, Joseph Mc-Carthy, Fang-fang Feng, and Wendy Lehnert.1995.
Description of the UMass system asused for MUC-6.
In Proc.
Si;zth Message Un-derstanding Conf.
(MUC-6), Columbia, MD,November.
Morgan Kaufmann.R.
Grishman, L. Hirschman, and N.T.
Nhan.1986.
Discovery procedures for sublanguageselectional patterns: Initial experiments.Computational Linguistics, 12(3):205-16.Lynette Hirschman, Ralph Grishman, andNaomi Sager.
1975.
Grammatically-basedautomatic word class formation.
InformationProcessing and Management, 11(1/2):39-57.Timo J/irvinen and Pasi Tapanainen.
1997.
Adependency parser for English.
Technical Re-port TR-1, Department of General Linguis-tics, University of Helsinki, Finland, Febru-ary.Martin Kay and Martin RSscheisen.
1993.288Text-translation alignment.
ComputationalLinguistics, 19(1).W.
Lehnert, C. Cardie, D. Fisher, J. McCarthy,E.
Riloff, and S. Soderland.
1992.
Univer-sity of massachusetts: MUC-4 test resultsand analysis.
In Proc.
Fourth Message Un-derstanding Conf., McLean, VA, June.
Mor-gan Kaufmann.Scott Miller, Michael Crystal, Heidi Fox,Lance Ramshaw, Richard Schwartz, RebeccaStone, Ralph Weischedel, and the Annota-tion Group.
1998.
Algorithms that learn toextract information; BBN: Description of theSIFT system as used for MUC-7.
In Proc.
ofthe Seventh Message Understanding Confer-ence, Fairfax, VA.1993.
Proceedings of the Fifth Message Un-derstanding Conference (MUC-5), Baltimore,MD, August.
Morgan Kaufmann.1995.
Proceedings of the Sixth Message Un-derstanding Conference (MUC-6), Columbia,M_D, November.
Morgan Kaufmann.Johanna Nichols.
1978.
Secondary predicates.Proceedings of the 4th Annual Meeting ofBerkeley Linguistics Society, pages 114-127.Maria Teresa Pazienza, editor.
1997.
Infor-mation Extraction.
Springer-Verlag, LectureNotes in Artificial Intelligence, Rome.Fernando Pereira, Naftali Tishby, and LillianLee.
1993.
Distributional clustering of En-glish words.
In Proceedings of the 31st An-nual Meeting of the Assn.
for ComputationalLinguistics, pages 183-190, Columbus, OH,June.Ellen Riloff and Rosie Jones.
1999.
Learn-ing dictionaries for information extraction bymulti-level bootstrapping.
In Proceedings ofSixteenth National Conference on ArtificialIntelligence (AAAI-99), Orlando, Florida,Ellen Riloff.
1993.
Automatically construct-ing a dictionary for information extractiontasks.
In Proceedings of Eleventh NationalConference on Artificial Intelligence (AAAI-93), pages 811-816.
The AAAI Press/MITPress.Ellen Riloff.
1996.
Automatically generatingextraction patterns from untagged text.
InProceedings of Thirteenth National Confer-ence on Artificial Intelligence (AAAL96),pages 1044-1049.
The AAAI Press/MITPress.Pasi Tapanainen and Timo J~rvinen.
1997.
Anon-projective dependency parser.
In Pro-ceedings of the 5th Conference on AppliedNatural Language Processing, pages 64-71,Washington, D.C., April.
ACL.Roman Yangarber and Ralph Grishman.
1997.Customization of information extraction sys-tems.
In Paola Velardi, editor, InternationalWorkshop on Lexically Driven InformationExtraction, pages 1-11, Frascati, Italy, July.Universit?~ di Roma.289
