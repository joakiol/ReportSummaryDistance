Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 118?123,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsMultiplicative Representations for Unsupervised Semantic Role InductionYi Luan?
?Yangfeng Ji?Hannaneh Hajishirzi?Boyang Li?
?Department of Electrical Engineering, University of Washington?School of Interactive Computing, Georgia Institute of Technology?Disney Research{luanyi,hannaneh}@uw.edu, jiyfeng@gatech.edu, boyang.li@disney.comAbstractIn unsupervised semantic role labeling,identifying the role of an argument is usu-ally informed by its dependency relationwith the predicate.
In this work, we pro-pose a neural model to learn argumentembeddings from the context by explic-itly incorporating dependency relations asmultiplicative factors, which bias argu-ment embeddings according to their de-pendency roles.
Our model outperformsexisting state-of-the-art embeddings in un-supervised semantic role induction on theCoNLL 2008 dataset and the SimLex999word similarity task.
Qualitative resultsdemonstrate our model can effectively biasargument embeddings based on their de-pendency role.1 IntroductionSemantic role labeling (SRL) aims to identifypredicate-argument structures of a sentence.
Thefollowing example shows the arguments labeledwith the roles A0 (typically the agent of an action)and A1 (typically the patient of an action), as wellas the predicate in bold.
[Little WillyA0] broke [a windowA1].As manual annotations are expensive and time-consuming, supervised approaches (Gildea andJurafsky, 2002; Xue and Palmer, 2004; Pradhanet al, 2005; Punyakanok et al, 2008; Das et al,2010; Das et al, 2014) to this problem are heldback by limited coverage of available gold anno-tations (Palmer and Sporleder, 2010).
SRL per-formance decreases remarkably when applied toout-of-domain data (Pradhan et al, 2008).Unsupervised SRL offer a promising alternative(Lang and Lapata, 2011; Titov and Klementiev,2012; Garg and Henderson, 2012; Lang and La-pata, 2014; Titov and Khoddam, 2015).
It is com-monly formalized as a clustering problem, whereeach cluster represents an induced semantic role.Such clustering is usually performed through man-ually defined semantic and syntactic features de-fined over argument instances.
However, the rep-resentation based on these features are usuallysparse and difficult to generalize.Inspired by the recent success of distributedword representations (Mikolov et al, 2013; Levyand Goldberg, 2014; Pennington et al, 2014), weintroduce two unsupervised models that learn em-beddings of arguments, predicates, and syntac-tic dependency relations between them.
The em-beddings are learned by predicting each argumentfrom its context, which includes the predicate andother arguments in the same sentence.
Drivenby the importance of syntactic dependency rela-tions in SRL, we explicitly model dependenciesas multiplicative factors in neural networks, yield-ing more succinct models than existing represen-tation learning methods employing dependencies(Levy and Goldberg, 2014; Woodsend and Lap-ata, 2015).
The learned argument embeddings arethen clustered and are evaluated by the clusters?agreement with ground truth labels.On unsupervised SRL, our models outperformthe state of the art by Woodsend and Lapata (2015)on gold parses and Titov and Khoddam (2015) onautomatic parses.
Qualitative results suggest ourmodel is effective in biasing argument embeddingstoward a specific dependency relation.2 Related WorkThere has been growing interest in using neu-ral networks and representation learning for su-pervised and unsupervised SRL (Collobert et al,2011; Hermann et al, 2014; Zhou and Xu, 2015;118ut-k(a) SYMDEPEt-k ??ut+k????
upvt Et+k ?
?Dt ?
?ut-k(b) ASYMDEPEt-k ??ut+k????
upvt Et+k ?
?Dt ??
A car is hit by another car .NMOD?
SBJ?
VC?PMOD?NMOD?LGS?u1/v1D1/E1Look-upD2/E2u2/v2Look-upLook-up Look-up up Look-up (C)Figure 1: (a): The SYMDEP model.
(b): The ASYMDEP model.
(c): An example of how embeddingsrelate to the parse tree.
In SYMDEP, the biasing of dependency is uniformly applied to all argumentembeddings.
In ASYMDEP, they are concentrated on one side of the dot product.FitzGerald et al, 2015).
Closely related to ourwork, Woodsend and Lapata (2015) concatenateone hot features of dependency, POS-tag and a dis-tributed representation for head word and projectthe concatenation onto a dense feature vectorspace.
Instead of using dependency relations asone-hot vectors, we explicitly model the multi-plicative compositionality between arguments anddependencies, and investigate two different com-positionality configurations.Our model is related to Levy and Goldberg(2014) who use dependency relations in learn-ing word embeddings.
In comparison, our mod-els separate the representation of dependency rela-tions and arguments, thereby allow the same wordin different relations to share weights in order toreduce model parameters and data sparsity.3 ApproachMost unsupervised approaches to SRL performthe following two steps: (1) identifying the ar-guments of the predicate and (2) assigning argu-ments to unlabeled roles, such as argument clus-ters.
Step (1) can be usually tackled with heuristicrules (Lang and Lapata, 2014).
In this paper, wefocus on tackling step (2) by creating clusters ofarguments that belongs to the same semantic role.As we assume PropBank-style roles (Kingsburyand Palmer, 2002), our models allocate a separateset of role clusters for each predicate and assign itsarguments to the clusters.
We evaluate the resultsby the overlapping between the induced clustersand PropBank-style gold labels.The example below suggests that SRL requiresmore than just lexical embeddings.
[A carA1] is hit by [another carA0].The A0 and A1 roles are very similar lexically, buttheir dependency relations to the predicate differ.To allow the same lexical embedding to shift ac-cording to different relations to the predicate, wepropose the following models.3.1 ModelsFollowing the framework of CBOW (Mikolov etal., 2013), our models predict an argument byits context, which includes surrounding argumentsand the predicate.Let vtbe the embedding of the tthargumentin a sentence, and utthe embedding of the ar-gument when it is part of the context.
Letupbe the embedding of the predicate.
uc={ut?k, .
.
.
,ut?1,ut+1, .
.
.
,ut+k} are the vectorssurrounding the tthargument with a window ofsize k.1The prediction of the tthargument is:p(vt|up,uc) ?
exp(f(vt)?g(up,uc)) (1)where f(?)
and g(?)
are two transformation func-tions of the target argument embedding and con-text vectors respectively.We further associate a dependency relation witheach argument (explained in more details in ?4.1).Let matrix Dtencode the biasing effect of the de-pendency relation between the tthargument andits predicate, and Etbe the corresponding depen-dency matrix for the tthargument if it is used as acontext.
We define a ?
operator:vt?Dt, tanh (Dtvt)ut?Et, tanh (Etut) ,(2)where tanh(?)
is the element-wise tanh function.Eq.
2 composes an argument and its dependencywith a multiplicative nonlinear operation.
Themultiplicative formulation encourages the decou-pling of dependencies and arguments, which is1To be precise, the embeddings are indexed by the argu-ments, which are then indexed by their positions, like uw(t).Here we omit w. The same convention applies to dependencymatrices, which are indexed by the dependency label first.119useful in learning representations tightly focusedon lexical and relational semantics, respectively.Symmetric-Dependency.
In our first model,we apply the dependency multiplication to all ar-guments.
We havef1(vt) = vt?Dt(3)g1(up,uc) = up?Ep+?ui?ucui?Ei(4)This model is named Symmetric-Dependency(SYMDEP) for the symmetric use of ?.
Since thepredicate does not have an dependency with itself,we let Ep= I .
Generally, ?i,Ei6= I .Asymmetric-Dependency.
An alternativemodel is to concentrate the dependency relations?effects by shifting the dependency of the predictedargument from f(?)
to g(?
), thereby move all ?operations to construct context vector:g2(up,uc) = (up?Ep+?ui?ucui?Ei)?Dt(5)f2(vt) = vt(6)This model is named Asymmetric-Dependency orASYMDEP.
Figure 1 shows the two models sideby side.
Note that Eq.
5 actually defines a feed-forward neural network structure g2(up,uc) forpredicting arguments.
Consider the predictionfunction defined in Eq.
1, these two models willbe equivalent if we eliminate all nonlinearities in-troduced by tanh(?
).3.2 Clustering ArgumentsIn the final step of semantic role induction, weperform agglomerative clustering on the learnedembeddings of arguments.
We first create a num-ber of seed clusters based on syntactic positions(Lang and Lapata, 2014), which are hierarchicallymerged.
Similar to Lang and Lapata (2011), wedefine the similarity between clusters as the cosinesimilarity (CosSim) between the centroids witha penalty for clustering two arguments from thesame sentence into the same role.
Consider twoclusters C and C?with the centroids x and y re-spectively, their similarity is:S(C,C?)
= CosSim(x,y)??
?pen(C,C?)
(7)where ?
is heuristically set to 1.To compute the penalty, let V (C,C?)
be the setof arguments ai?
C such that aiappears in thesame sentence with another argument aj?
C?.We havepen(C,C?)
=|V (C,C?
)|+ |V (C?, C)||C|+ |C?|(8)where | ?
| is set cardinality.
When this penalty islarge, the clusters C and C?will appear dissimilar,so it becomes difficult to merge them into the samecluster, preventing aiand ajfrom appearing in thesame cluster.4 ExperimentsWe evaluate our models in unsupervised SRL andcompare the effectiveness our approach in model-ing dependency relations with the previous work.4.1 SetupOur models are trained on 24 million tokens and1 million sentences from the North AmericanNews Text corpus (Graff, 1995).
We use MATE(Bj?orkelund et al, 2009) to parse the dependencytree and identify predicates and arguments.
Em-beddings of head words are the only feature we usein clustering.
Dependency matrices are restrictedto contain only diagonal terms.
The vocabularysizes for arguments and predicates are 10K and5K respectively.
We hand-picked the dimensionof embeddings to be 50 for all models.We take the first dependency relation on thepath from an argument?s head word to the predi-cate as its dependency label, considering the de-pendency?s direction.
For example, the label forthe first car in Figure 1(c) is SBJ?1.
We use neg-ative sampling (Mikolov et al, 2013) to approx-imate softmax in the objective function.
ForSYMDEP, we sample both the predicted argumentand dependency.
For ASYMDEP, we sample onlythe argument.
Models are trained using AdaGrad(Duchi et al, 2011) with L2 regularization.
Allembeddings are randomly initialized.2Baselines.
We compare against several baselinesusing representation learning: CBOW and Skip-Gram (Mikolov et al, 2013), GloVe (Penningtonet al, 2014), L&G (Levy and Goldberg, 2014) andArg2vec (Woodsend and Lapata, 2015).
Similar toours, L&G and Arg2vec both encode dependencyrelations in the embeddings.
We train all modelson the same dataset as ours using publicly avail-2Resulted embeddings can be downloaded from https://bitbucket.org/luanyi/unsupervised-srl.120able code3, and then apply the same clustering al-gorithm.
Introduced by Lang and Lapata (2014),SYNTF is a strong baseline that clusters argumentsbased on purely syntactic cues: voice of the verb,relative position to the predicate, syntactic rela-tions, and realizing prepositions.
The window sizefor Arg2vec and our models are set to 1, while allother embeddings are set to 2.
We also employtwo state-of-the-art methods from Titov and Kle-mentiev (2012) (T&K12) and Titov and Khoddam(2015) (T&K15).4.2 SRL ResultsFollowing common practices (Lang and Lapata,2014), we measure the overlap of induced seman-tic roles and their gold labels on the CoNLL 2008training data (Surdeanu et al, 2008).
We reportpurity (PU), collocation (CO), and their harmonicmean (F1) evaluated on gold arguments in two set-tings of gold parses and automatic parses from theMaltParser (Nivre et al, 2007).
Table 1 shows theresults.4SYMDEP and ASYMDEP outperform all repre-sentation learning baselines for SRL.
T&K12 out-performs our models on gold parsing because theyuse a strong generative clustering method, whichshared parameters across verbs in the clusteringstep.
In addition, T&K15 incorporates feature-rich latent structure learning.
Nevertheless, ourmodels perform better with automatic parses, in-dicating the robustness of our models under noisein automatic parsing.
Future work involves moresophisticated clutering techniques (Titov and Kle-mentiev, 2012) as well as incorporating feature-rich models (Titov and Khoddam, 2015) to im-prove performance further.Table 1 shows that including dependency rela-tions (L&G, Arg2vec, SYMDEP, and ASYMDEP)improves performance.
Additionally, our mod-els achieve the best performance among those,showing the strength of modeling dependenciesas multiplicative factors.
Arg2vec learns wordembedings from the context features which areconcatenation of syntactic features (dependencyreations and POS tags) and word embedings.
L&Gtreats each word-dependency pair as a separate to-3Except that Arg2vec is reimplemented since there is nopublic code online.4The numbers reported for Arg2vec with gold parsing(80.7) is different from Woodsend and Lapata (2015) (80.9)since we use a different clustering method and different train-ing data.Gold parses Automatic parsesModel PU CO F1 PU CO F1SYNTF 81.6 78.1 79.8 77.0 71.5 74.1Skip-Gram 86.6 74.7 80.2 84.3 72.4 77.9CBOW 84.6 74.9 79.4 84.0 71.5 77.2GloVe 84.9 74.1 79.2 83.0 70.8 76.5L&G 87.0 75.6 80.9 86.6 71.3 78.2Arg2vec 84.0 77.7 80.7 86.9 71.4 78.4SYMDEP 85.3 77.9 81.4 81.9 76.6 79.2ASYMDEP 85.6 78.3 81.8 82.9 75.2 78.9T&K12 88.7 78.1 83.0 86.2 72.7 78.8T&K15 79.7 86.2 82.8 - - -SYM1DEP 83.8 77.4 80.5 82.3 74.8 78.4Table 1: Purity, collocation and F1 measures forthe CoNLL-2008 data set.ken, leading to a large vocabulary (142k in ourdataset) and potentially data scarcity.
In compar-ison, SYMDEP and ASYMDEP formulate the de-pendency as the weight matrix of the second non-linear layer, leading to a deeper structure with lessparameters compared to previous work.Qualitative results.
Table 2 demonstrates the ef-fectiveness of our models qualitatively.
For exam-ple, we identify that car is usually the subject ofcrash and unload, and the object of sell and pur-chase.
In comparison, CBOW embeddings do notreflect argument-predicate relations.Ablation Study.
To further understand the ef-fects of the multiplicative representation on un-supervised SRL, we create an ablated modelSYM1DEP, where we force all dependencies inSYMDEP to use the same matrix.
The networkhas the same structure as SYMDEP, but the depen-dency information is removed.
Its performance onSRL is shown at the bottom of Table 1.
SYM1DEPperforms slightly worse than Arg2vec.
This sug-gests that the performance gain in SYMDEP canbe attributed to the use of dependency informationinstead of the way of constructing context.4.3 Word Similarity ResultsAs a further evaluation of the learned embed-dings, we test if similarities between word em-beddings agree with human annotation from Sim-Lex999 (Hill et al, 2015).
Table 3 showsthat SYMDEP outperforms Arg2vec on bothnouns and verbs, suggesting multiplicative depen-dency relations are indeed effective.
However,ASYMDEP performs better than SYMDEP on nounsimilarity but much worse on verb similarity.
Weexplore this further in an ablation study.121Argument SYMDEP (SBJ) SYMDEP (OBJ) CBOWcar crash, roar, capsize, land, lug, un-load, bounce, shipsell, purchase, buy, retrieve, board,haul, lease, unloadtrain, splash, mail, shelter, jet, ferry,drill, ticketvictim injure, die, protest, complain,weep, hospitalize, shout, sufferinsult, assault, stalk, avenge, harass,interview, housing, apprehendvoid, murder, kidnap, widow, mas-sacre, surge, sentence, defectteacher teach, mentor, educate, note, rem-inisce, say, learn, lecturehire, bar, recruit, practice, assault,enlist, segregate, encouragecoach, mentor, degree, master, guide,pilot, partner, captainstudent learn, resurface, object, enroll,note, protest, deem, teachteach, encourage, educate, assault,segregate, enroll, attend, administergraduate, degree, mortgage, engi-neer, mentor, pilot, partner, pioneerTable 2: The 8 most similar predicates to a given argument in a given dependency role.Model Nouns VerbsL&G 31.4 27.2Arg2vec 38.2 31.4SYMDEP 39.2 36.5ASYMDEP 39.7 15.3ASYM1DEP 33.2 24.2Table 3: A POS-based analysis of the various em-beddings.
Numbers are the Spearman?s ?
scoresof each model on nouns and verbs of SimLex999.Ablation Study.
We create an ablated modelto explore the reason for ASYMDEP?s perfor-mance on verb similarity.
ASYM1DEP is basedon ASYMDEP where we force all dependency re-lations for the predicted argument vtto use thesame matrix Di.
The aim of this experiment is tocheck the negative influence of asymmetric depen-dency matrix to verb embedding.
The results areshown at the bottom of Table 3.
By keeping Didependency independent, performance on verbs issignificantly improved with the cost of noun per-formance.5 ConclusionsWe present a new unsupervised semantic role la-beling approach that learns embeddings of argu-ments by predicting each argument from its con-text and considering dependency relation as a mul-tiplicative factor.
Two proposed neural networksoutperform current state-of-the-art embeddings onunsupervised SRL and the SimLex999 word simi-larity task.
As an effective model for dependencyrelations, our multiplicative argument-dependencyfactor models encourage the decoupling of argu-ment and dependency representations.
Disentan-gling linguistic factors in similar manners may beworth investigating in similar tasks such as framesemantic parsing and event detection.ReferencesAnders Bj?orkelund, Love Hafdell, and Pierre Nugues.2009.
Multilingual semantic role labeling.
In Pro-ceedings of the Thirteenth Conference on Compu-tational Natural Language Learning: Shared Task,pages 43?48.R.
Collobert, J. Weston, L. Bottou, M. Karlen,K.
Kavukcuoglu, and P. Kuksa.
2011.
Natural lan-guage processing (almost) from scratch.
Journal ofMachine Learning Research, 12:2493?2537.Dipanjan Das, Nathan Schneider, Desai Chen, andNoah A. Smith.
2010.
Probabilistic frame-semanticparsing.
In Proceedings of the Human LanguageTechnologies Conference of the North AmericanChapter of the Associattion for Computational Lin-guistics, pages 948?956.Dipanjan Das, Desai Chen, Andr?e FT Martins, NathanSchneider, and Noah A. Smith.
2014.
Frame-semantic parsing.
Computational Linguistics,40(1):9?56.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
The Journal of Ma-chine Learning Research, 12:2121?2159.Nicholas FitzGerald, Oscar Tckstrm, KuzmanGanchev, and Dipanjan Das.
2015.
Semantic rolelabeling with neural network factors.
In Proceed-ings of the 2015 Conference on Empirical Methodsin Natural Language Processing.Hagen F?urstenau and Mirella Lapata.
2012.
Semi-supervised semantic role labeling via structuralalignment.
Computational Linguistics, 38(1):135?171.Nikhil Garg and James Henderson.
2012.
Unsuper-vised semantic role induction with global role or-dering.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguis-tics, pages 145?149.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguis-tics, 28(9):245?288.David Graff.
1995.
North american news text corpus.122Karl Moritz Hermann, Dipanjan Das, Jason Weston,and Kuzman Ganchev.
2014.
Semantic frame iden-tification with distributed word representations.
InProceedings of the 52nd Annual Meeting of the As-sociation for Computational Linguistics.Felix Hill, Roi Reichart, and Anna Korhonen.
2015.Simlex-999: Evaluating semantic models with (gen-uine) similarity estimation.
Computational Linguis-tics.Paul Kingsbury and Martha Palmer.
2002.
From Tree-Bank to PropBank.
In Proceedings of the Third In-ternational Conference on Language Resources andEvaluation, pages 1989?1993.
Las Palmas.Meghana Kshirsagar, Sam Thomson, Nathan Schnei-der, Jaime Carbonell, Noah A. Smith, and ChrisDyer.
2015.
Frame-semantic role labeling withheterogeneous annotations.
In Proceedings of the53rd Annual Meeting of the Association for Compu-tational Linguistics and the 7th International JointConference on Natural Language Processing, Bei-jing, China.Joel Lang and Mirella Lapata.
2011.
Unsupervisedsemantic role induction via split-merge clustering.In Proceedings of Human Language TechnologiesConference of the North American Chapter of theAssociation for Computational Linguistics, pages1117?1126.Joel Lang and Mirella Lapata.
2014.
Similarity-driven semantic role induction via graph partition-ing.
Computational Linguistics, 40(3):633?669.Omer Levy and Yoav Goldberg.
2014.
Dependen-cybased word embeddings.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics, volume 2, pages 302?308.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeffrey Dean.
2013.
Distributed represen-tations of words and phrases and their composition-ality.
In Proceedings of NIPS.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, G?ulsen Eryigit, Sandra K?ubler, SvetoslavMarinov, and Erwin Marsi.
2007.
Maltparser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(2):95?135.Alexis Palmer and Caroline Sporleder.
2010.
Eval-uating FrameNet-style semantic parsing: the roleof coverage gaps in FrameNet.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics, pages 928?936.Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning.
2014.
GloVe: Global vectorsfor word representation.
In Proceedings of the 2014Conference on Empirical Methods in Natural Lan-guage Processing, pages 1532?1543.Sameer Pradhan, Kadri Hacioglu, Wayne Ward,James H. Martin, and Daniel Jurafsky.
2005.
Se-mantic role chunking combining complementarysyntactic views.
In Proceedings of the Ninth Confer-ence on Computational Natural Language Learning,pages 217?220.Sameer S. Pradhan, Wayne Ward, and James H. Mar-tin.
2008.
Towards robust semantic role labeling.Computational Linguistics, 34(2):289?310.Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
2008.The importance of syntactic parsing and inference insemantic role labeling.
Computational Linguistics,34(2):257?287.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s M`arquez, and Joakim Nivre.
2008.
Theconll-2008 shared task on joint parsing of syntacticand semantic dependencies.
In Proceedings of theTwelfth Conference on Computational Natural Lan-guage Learning, CoNLL ?08, pages 159?177.Ivan Titov and Ehsan Khoddam.
2015.
Unsupervisedinduction of semantic roles within a reconstruction-error minimization framework.
Proceedings of the53rd Annual Meeting of the Association for Compu-tational Linguistics.Ivan Titov and Alexandre Klementiev.
2012.
ABayesian approach to unsupervised semantic role in-duction.
In Proceedings of the 13th Conference ofthe European Chapter of the Association for Com-putational Linguistics, pages 12?22.Kristian Woodsend and Mirella Lapata.
2015.
Dis-tributed representations for unsupervised semanticrole labeling.
In Proceedings of the 2015 Confer-ence on Empirical Methods in Natural LanguageProcessing.Nianwen Xue and Martha Palmer.
2004.
Calibratingfeatures for semantic role labeling.
In Proceedingsof the 2004 Conference on Empirical Methods inNatural Language Processing, pages 88?94.Jie Zhou and Wei Xu.
2015.
End-to-end learning ofsemantic role labeling using recurrent neural net-works.
In Proceedings of the 53rd Conference of theAssociation for Computational Linguistics, pages1127?1137.123
