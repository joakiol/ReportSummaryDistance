Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 746?754,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPWhat lies beneath: Semantic and syntactic analysisof manually reconstructed spontaneous speechErin FitzgeraldJohns Hopkins UniversityBaltimore, MD, USAerinf@jhu.eduFrederick JelinekJohns Hopkins UniversityBaltimore, MD, USAjelinek@jhu.eduRobert FrankYale UniversityNew Haven, CT, USAbob.frank@yale.eduAbstractSpontaneously produced speech text oftenincludes disfluencies which make it diffi-cult to analyze underlying structure.
Suc-cessful reconstruction of this text wouldtransform these errorful utterances intofluent strings and offer an alternate mech-anism for analysis.Our investigation of naturally-occurringspontaneous speaker errors aligned tocorrected text with manual semantico-syntactic analysis yields new insight intothe syntactic and structural semanticdifferences between spoken and recon-structed language.1 IntroductionIn recent years, natural language processing taskssuch as machine translation, information extrac-tion, and question answering have been steadilyimproving, but relatively little of these systemsbesides transcription have been applied to themost natural form of language input: spontaneousspeech.
Moreover, there has historically been lit-tle consideration of how to analyze the underlyingsemantico-syntactic structure of speech.A system would accomplish reconstruction ofits spontaneous speech input if its output wereto represent, in flawless, fluent, and content-preserved English, the message that the speakerintended to convey (Fitzgerald and Jelinek, 2008;Fitzgerald et al, 2009).
Examples of such recon-structions are seen in the following sentence-likeunits (SUs).EX1: that?s uh that?s a reliefbecomesthat?s a reliefEX2: how can you do that without + it?s a catch-22becomeshow can you do that without <ARG>it?s a catch-22EX3: they like video games some kids dobecomessome kids like video gamesIn EX1, reconstruction requires only the dele-tion of a simple filled pause and speaker repetition(or reparandum (Shriberg, 1994)).
The second ex-ample shows a restart fragment, where an utter-ance is aborted by the speaker and then restartedwith a new train of thought.
Reconstruction hererequires1.
Detection of an interruption point (denoted+ in the example) between the abandonedthought and its replacement,2.
Determination that the abandoned portioncontains unique and preservable content andshould be made a new sentence rather than bedeleted (which would alter meaning)3.
Analysis showing that a required argumentmust be inserted in order to complete the sen-tence.Finally, in the third example EX3, in order to pro-duce one of the reconstructions given, a systemmust1.
Detect the anaphoric relationship between?they?
and ?some kids?2.
Detect the referral of ?do?
to ?like video games?3.
Make the necessary word reorderings anddeletion of the less informative lexemes.These examples show varying degrees of diffi-culty for the task of automatic reconstruction.
Ineach case, we also see that semantic analysis of thereconstruction is more straightforward than of the746original string directly.
Such analysis not only in-forms us of what the speaker intended to commu-nicate, but also reveals insights into the types of er-rors speakers make when speaking spontaneouslyand where these errors occur.
The semantic la-beling of reconstructed sentences, when combinedwith the reconstruction alignments, may yield newquantifiable insights into the structure of disfluentnatural speech text.In this paper, we will investigate this relation-ship further.
Generally, we seek to answer twoquestions:?
What generalizations about the underlyingstructure of errorful and reconstructed speechutterances are possible??
Are these generalizations sufficiently robustas to be incorporated into statistical modelsin automatic systems?We begin by reviewing previous work in the au-tomatic labeling of structural semantics and moti-vating the analysis not only in terms of discoverybut also regarding its potential application to auto-matic speech reconstruction research.
In Section 2we describe the Spontaneous Speech Reconstruc-tion (SSR) corpus and the manual semantic rolelabeling it includes.
Section 3 analyzes structuraldifferences between verbatim and reconstructedtext in the SSR as evaluated by a combination ofmanual and automatically generated phrasal con-stituent parses, while Section 4 combines syntacticstructure and semantic label annotations to deter-mine the consistency of patterns and their compar-ison to similar patterns in the Wall Street Journal(WSJ)-based Proposition Bank (PropBank) corpus(Palmer et al, 2005).
We conclude by offering ahigh level analysis of discoveries made and sug-gesting areas for continued analysis in the future.Expanded analysis of these results is described in(Fitzgerald, 2009).1.1 Semantic role labelingEvery verb can be associated with a set of coreand optional argument roles, sometimes called aroleset.
For example, the verb ?say?
must have asayer and an utterance which is said, along withan optionally defined hearer and any number oflocative, temporal, manner, etc.
adjunctival argu-ments.The task of predicate-argument labeling (some-times called semantic role labeling or SRL) as-signs a simplewho didwhat towhom when, where,some kids?
??
?ARG0like???
?predicatevideo games?
??
?ARG1Figure 1: Semantic role labeling for the sentence?some kids like video games?.
According to Prop-Bank specifications, core arguments for each pred-icate are assigned a corresponding label ARG0-ARG5 (where ARG0 is a proto-agent, ARG1 is aproto-patient, etc.
(Palmer et al, 2005)).why, how, etc.
structure to sentences (see Figure1), often for downstream processes such as infor-mation extraction and question answering.
Reli-ably identifying and assigning these roles to gram-matical text is an active area of research (Gildeaand Jurafsky, 2002; Pradhan et al, 2004; Prad-han et al, 2008), using training resources like theLinguistic Data Consortium?s Proposition Bank(PropBank) (Palmer et al, 2005), a 300k-wordcorpus with semantic role relations labeled forverbs in the WSJsection of the Penn Treebank.A common approach for automatic semanticrole labeling is to separate the process into twosteps: argument identification and argument label-ing.
For each task, standard cue features in au-tomatic systems include verb identification, anal-ysis of the syntactic path between that verb andthe prospective argument, and the direction (to theleft or to the right) in which the candidate argu-ment falls in respect to its predicate.
In (Gildeaand Palmer, 2002), the effect of parser accuracyon semantic role labeling is quantified, and con-sistent quality parses were found to be essentialwhen automatically identifying semantic roles onWSJ text.1.2 Potential benefit of semantic analysis tospeech reconstructionWith an adequate amount of appropriately anno-tated conversational text, methods such as thosereferred to in Section 1.1 may be adapted fortranscriptions of spontaneous speech in future re-search.
Furthermore, given a set of semanticrole labels on an ungrammatical string, and armedwith the knowledge of a set of core semantico-syntactic principles which constrain the set of pos-sible grammatical sentences, we hope to discoverand take advantage of new cues for constructionerrors in the field of automatic spontaneous speechreconstruction.7472 DataWe conducted our experiments on the Spon-taneous Speech Reconstruction (SSR) corpus(Fitzgerald and Jelinek, 2008), a 6,000 SU set ofreconstruction annotations atop a subset of Fisherconversational telephone speech data (Cieri et al,2004), including?
manual word alignments between corre-sponding original and cleaned sentence-likeunits (SUs) which are labeled with transfor-mation types (Section 2.1), and?
annotated semantic role labels on predicatesand their arguments for all grammatical re-constructions (Section 2.2).The fully reconstructed portion of the SSR cor-pus consists of 6,116 SUs and 82,000 words to-tal.
While far smaller than the 300,000-word Prop-Bank corpus, we believe that this data will be ad-equate for an initial investigation to characterizesemantic structure of verbatim and reconstructedspeech.2.1 Alignments and alteration labelsIn the SSR corpus, words in each reconstructedutterance were deleted, inserted, substituted, ormoved as required to make the SU as grammaticalas possible without altering the original meaningand without the benefit of extrasentential context.Alignments between the original words and theirreconstructed ?source?
words (i.e.
in the noisychannel paradigm) are explicitly defined, and foreach alteration a corresponding alteration labelhas been chosen from the following.- DELETE words: fillers, repetitions/revisions,false starts, co-reference, leading conjuga-tion, and extraneous phrases- INSERT neutral elements, such as functionwords like ?the?, auxiliary verbs like ?is?, orundefined argument placeholders, as in ?hewants <ARG>?- SUBSTITUTE words to change tense or num-ber, correct transcriber errors, and replacecolloquial phrases (such as: ?he was like...?
?
?he said...?
)- REORDER words (within sentence bound-aries) and label as adjuncts, arguments, orother structural reorderingsUnchanged original words are aligned to the cor-responding word in the reconstruction with an arcmarked BASIC.2.2 Semantic role labeling in the SSR corpusOne goal of speech reconstruction is to developmachinery to automatically reduce an utterance toits underlying meaning and then generate cleantext.
To do this, we would like to understandhow semantic structure in spontaneous speech textvaries from that of written text.
Here, we can takeadvantage of the semantic role labeling includedin the SSR annotation effort.Rather than attempt to label incomplete ut-terances or errorful phrases, SSR annotators as-signed semantic annotation only to those utter-ances which were well-formed and grammaticalpost-reconstruction.
Therefore, only these utter-ances (about 72% of the annotated SSR data) canbe given a semantic analysis in the following sec-tions.
For each well-formed and grammatical sen-tence, all (non-auxiliary and non-modal) verbswere identified by annotators and the correspond-ing predicate-argument structure was labeled ac-cording to the role-sets defined in the PropBankannotation effort1.We believe the transitive bridge between thealigned original and reconstructed sentences andthe predicate-argument labels for those recon-structions (described further in Section 4) mayyield insight into the structure of speech errors andhow to extract these verb-argument relationshipsin verbatim and errorful speech text.3 Syntactic variation between originaland reconstructed stringsAs we begin our analysis, we first aim to under-stand the types of syntactic changes which occurduring the course of spontaneous speech recon-struction.
These observations are made empiri-cally given automatic analysis of the SSR corpusannotations.
Syntactic evaluation of speech andreconstructed structure is based on the followingresources:1. the manual parse Pvm for each verbatim orig-inal SU (from SSR)2. the automatic parse Pva of each verbatimoriginal SU1PropBank roleset definitions for given verbs can be re-viewed at http://www.cs.rochester.edu/?gildea/Verbs/.7483.
the automatic parse Pra of each reconstructedSUWe note that automatic parses (using the stateof the art (Charniak, 1999) parser) of verbatim,unreconstructed strings are likely to contain manyerrors due to the inconsistent structure of ver-batim spontaneous speech (Harper et al, 2005).While this limits the reliability of syntactic obser-vations, it represents the current state of the art forsyntactic analysis of unreconstructed spontaneousspeech text.On the other hand, automatically obtainedparses for cleaned reconstructed text are morelikely to be accurate given the simplified and morepredictable structure of these SUs.
This obser-vation is unfortunately not evaluable without firstmanually parsing all reconstructions in the SSRcorpus, but is assumed in the course of the follow-ing syntax-dependent analysis.In reconstructing from errorful and disfluenttext to clean text, a system makes not only surfacechanges but also changes in underlying constituentdependencies and parser interpretation.
We canquantify these changes in part by comparing theinternal context-free structure between the twosets of parses.We compare the internal syntactic structure be-tween sets Pva and Pra of the SSR check set.Statistics are compiled in Table 1 and analyzed be-low.?
64.2% of expansion rules in parses Pvaalso occur in reconstruction parses Pra , and92.4% (86.8%) of reconstruction parse Praexpansions come directly from the verbatimparses Pva (from columns one and two of Ta-ble 1).?
Column three of Table 1 shows the rule typesmost often dropped from the verbatim stringparses Pva in the transformation to recon-struction.
The Pva parses select full clausenon-terminals (NTs) for the verbatim parseswhich are not in turn selected for automaticparses of the reconstruction (e.g.
[SBAR ?S] or [S ?
VP]).
This suggests that theserules may be used to handle errorful struc-tures not seen by the trained grammar.?
Rule types in column four of Table 1 are themost often ?generated?
in Pra (as they areunseen in the automatic parse Pva).
Sincerules like [S ?
NP VP], [PP ?
IN NP],and [SBAR ?
IN S] appear in a recon-struction parse but not corresponding verba-tim parse at similar frequencies regardless ofwhether Pvm or Pva are being compared, weare more confident that these patterns are ef-fects of the verbatim-reconstruction compar-ison and not the specific parser used in anal-ysis.
The fact that these patterns occur in-dicates that it is these common rules whichare most often confounded by spontaneousspeaker errors.?
Given a Levenshtein alignment between al-tered rules, the most common changes withina given NT phrase are detailed in column fiveof Table 1.
We see that the most com-mon aligned rule changes capture the mostbasic of errors: a leading coordinator (#1and 2) and rules proceeded by unnecessaryfiller words (#3 and 5).
Complementary rules#7 and 9 (e.g.
VP ?
[rule]/[rule SBAR] andVP?
[rule SBAR]/[rule]) show that comple-menting clauses are both added and removed,possibly in the same SU (i.e.
a phrase shift),during reconstruction.4 Analysis of semantics for speechFigure 2: Manual semantic role labeling for thesentence ?some kids like video games?
and SRLmapped onto its verbatim source string ?they likevideo games and stuff some kids do?To analyze the semantic and syntactic patternsfound in speech data and its corresponding recon-structions, we project semantic role labels fromstrings into automatic parses, and moreover fromtheir post-reconstruction source to the verbatimoriginal speech strings via the SSR manual wordalignments, as shown in Figures 2.The automatic SRL mapping procedure fromthe reconstructed string Wr to related parses Praand Pva and the verbatim original string Wv is asfollows.749Pva rules Pra rules Pva rules most Pra rules most Levenshtein-aligned expansionin Pra in Pva frequently dropped frequently added changes (Pva/Pra)1.
NP?
PRP 1.
S?
NP VP 1.
S?
[ CC rule] / [rule]2.
ROOT?
S 2.
PP?
IN NP 2.
S?
[ CC NP VP] / [ NP VP]3.
S?
NP VP 3.
ROOT?
S 3.
S?
[ INTJ rule] / [rule]4.
INTJ?
UH 4.
ADVP?
RB 4.
S?
[ NP rule] / [rule]64.2% 92.4% 5.
PP?
IN NP 5.
S?
NP ADVP VP 5.
S?
[ INTJ NP VP] / [ NP VP]6.
ADVP?
RB 6.
SBAR?
IN S 6.
S?
[ NP NP VP] / [ NP VP]7.
SBAR?
S 7.
SBAR?
S 7.
VP?
[rule] / [rule SBAR]8.
NP?
DT NN 8.
S?
ADVP NP VP 8.
S?
[ RB rule] / [rule]9.
S?
VP 9.
S?
VP 9.
VP?
[rule SBAR] / [rule]10.
PRN?
S 10.
NP?
NP SBAR 10.
S?
[rule] / [ ADVP rule]Table 1: Internal syntactic structure removed and gained during reconstruction.
This table comparesthe rule expansions for each verbatim string automatically parsed Pva and the automatic parse of thecorresponding reconstruction in the SSR corpus (Pra).1.
Tag each reconstruction word wr ?
stringWr with the annotated SRL tag twr .
(a) Tag each verbatim word wv ?
stringWvaligned to wr via a BASIC, REORDER,or SUBSTITUTE alteration label with theSRL tag twr as well.
(b) Tag each verbatim word wv alignedto wr via a DELETE REPETITIONor DELETE CO-REFERENCE alignmentwith a shadow of that SRL tag twr (seethe lower tags in Figure 2 for an exam-ple)Any verbatim original word wv with anyother alignment label is ignored in this se-mantic analysis as SRL labels for the alignedreconstruction word wr do not directly trans-late to them.2.
Overlay tagged words of string Wv and Wrwith the automatic (or manual) parse of thesame string.3.
Propagate labels.
For each constituent inthe parse, if all children within a syntacticconstituent expansion (or all but EDITED orINTJ) has a given SRL tag for a given pred-icate, we instead tag that NT (and not chil-dren) with the semantic label information.4.1 Labeled verbs and their argumentsIn the 3,626 well-formed and grammatical SUs la-beled with semantic roles in the SSR, 895 distinctverb types were labeled with core and adjunct ar-guments as defined in Section 1.1.
The most fre-quent of these verbs was the orthographic form ?
?s?which was labeled 623 times, or in roughly 5%of analyzed sentences.
Other forms of the verb?to be?, including ?is?, ?was?, ?be?, ?are?, ?re?, ?
?m?,and ?being?, were labeled over 1,500 times, or ata rate of nearly one in half of all well-formed re-constructed sentences.
The verb type frequenciesroughly follow a Zipfian distribution (Zipf, 1949),where most verb words appear only once (49.9%)or twice (16.0%).On average, 1.86 core arguments (ARG[0-4])are labeled per verb, but the specific argumenttypes and typical argument numbers per predicateare verb-specific.
For example, the ditransitiveverb ?give?
has an average of 2.61 core argumentsfor its 18 occurrences, while the verb ?divorced?
(whose core arguments ?initiator of end of mar-riage?
and ?ex-spouse?
are often combined, as in?we divorced two years ago?)
was labeled 11 timeswith an average of 1.00 core arguments per occur-rence.In the larger PropBank corpus, annotated atopWSJ news text, the most frequently reported verbroot is ?say?, with over ten thousand labeled ap-pearances in various tenses (this is primarily ex-plained by the genre difference between WSJ andtelephone speech)2; again, most verbs occur twoor fewer times.4.2 Structural semantic statistics in cleanedspeechA reconstruction of a verbatim spoken utterancecan be considered an underlying form, analogous2The reported PropBank analysis ignores past and presentparticiple (passive) usage; we do not do this in our analysis.750to that of Chomskian theory or Harris?s concep-tion of transformation (Harris, 1957).
In this view,the original verbatim string is the surface form ofthe sentence, and as in linguistic theory should beconstrained in some manner similar to constraintsbetween Logical Form (LF) and Surface Structure(SS).Most common syntacticData SRL Total categories, with rel.
frequencyPva 10110 NP (50%) PP (6%)Pra ARG1 8341 NP (58%) SBAR (9%)PB05 Obj-NP (52%) S (22%)Pva 4319 NP (90%) WHNP (3%)Pra ARG0 4518 NP (93%) WHNP (3%)PB05 Subj-NP (97%) NP (2%)Pva 3836 NP (28%) PP (13%)Pra ARG2 3179 NP (29%) PP (18%)PB05 NP (36%) Obj-NP (29%)Pva 931 ADVP (25%) NP (20%)Pra TMP 872 ADVP (27%) PP (18%)PB05 ADVP (26%) PP-in (16%)Pva 562 MD (58%) TO (18%)Pra MOD 642 MD (57%) TO (19%)PB05 MD (99%) ADVP (1%)Pva 505 PP (47%) ADVP (16%)Pra LOC 489 PP (54%) ADVP (17%)PB05 PP-in (59%) PP-on (10.0%)Table 2: Most frequent phrasal categories for com-mon arguments in the SSR (mapping SRLs ontoPva parses).
PB05 refers to the PropBank data de-scribed in (Palmer et al, 2005).Most common argumentData NT Total labels, with rel.
frequencyPva 10541 ARG1 (48%) ARG0 (37%)Pra NP 10218 ARG1 (47%) ARG0 (41%)PB05 ARG2 (34%) ARG1 (24%)PB05 Subj-NP ARG0 (79%) ARG1 (17%)PB05 Obj-NP ARG1 (84%) ARG2 (10%)Pva PP 1714 ARG1 (34%) ARG2 (30%)Pra 1777 ARG1 (31%) ARG2 (30%)PB05 PP-in LOC (48%) TMP (35%)PB05 PP-at EXT (36%) LOC (27%)Pva 1519 ARG2 (21%) ARG1 (19%)Pra ADVP 1444 ARG2 (22%) ADV (20%)PB05 TMP (30%) MNR (22%)Pva 930 ARG1 (61%) ARG2 (14%)Pra SBAR 1241 ARG1 (62%) ARG2 (12%)PB05 ADV (36%) TMP (30%)Pva 523 ARG1 (70%) ARG2 (16%)Pra S 526 ARG1 (72%) ARG2 (17%)PB05 ARG1 (76%) ADV (9%)Pva 449 MOD (73%) ARG1 (18%)Pra MD 427 MOD (86%) ARG1 (11%)PB05 MOD (97%)Adjuncts (3%)Table 3: Most frequent argument categories forcommon syntactic phrases in the SSR (mappingSRLs onto Pva parses).In this section, we identify additional trendswhich may help us to better understand these con-straints, such as the most common phrasal cate-gory for common arguments in common contexts?
listed in Table 2 ?
and the most frequent seman-tic argument type for NTs in the SSR ?
listed inTable 3.4.3 Structural semantic differences betweenverbatim speech and reconstructedspeechWe now compare the placement of semantic rolelabels with reconstruction-type labels assigned inthe SSR annotations.These analyses were conducted on Pra parses ofreconstructed strings, the strings upon which se-mantic labels were directly assigned.Reconstructive deletionsQ: Is there a relationship between speaker er-ror types requiring deletions and the argumentshadows contained within?
Only two deletiontypes ?
repetitions/revisions and co-references ?have direct alignments between deleted text andpreserved text and thus can have argument shad-ows from the reconstruction marked onto the ver-batim text.Of 9,082 propagated deleted repetition/ revisionphrase nodes from Pva , we found that 31.0% of ar-guments within were ARG1, 22.7% of argumentswere ARG0, 8.6% of nodes were predicates la-beled with semantic roles of their own, and 8.4%of argument nodes were ARG2.
Just 8.4% of?delete repetition/revision?
nodes were modifier(vs. core) arguments, with TMP and CAU labelsbeing the most common.Far fewer (353) nodes from Pva representeddeleted co-reference words.
Of these, 57.2% of ar-gument nodes were ARG1, 26.6% were ARG0 and13.9% were ARG2.
7.6% of ?argument?
nodeshere were SRL-labeled predicates, and 10.2%were in modifier rather than core arguments, themost prevalent were TMP and LOC.These observations indicate to us that redun-dant co-references are far most likely to occur forARG1 roles (most often objects, though also sub-jects for copular verbs (i.e.
?to be?)
and others) andappear more likely than random to occur in coreargument regions of an utterance rather than in op-tional modifying material.Reconstructive insertions751Q: When null arguments are inserted into re-constructions of errorful speech, what seman-tic role do they typically fill?
Three types ofinsertions were made by annotators during the re-construction of the SSR corpus.
Inserted functionwords, the most common, were also the most var-ied.
Analyzing the automatic parses of the recon-structions Pra , we find that the most commonlyassigned parts-of-speech (POS) for these elementswas fittingly IN (21.5%, preposition), DT (16.7%,determiner) and CC (14.3%, conjunction).
Inter-estingly, we found that the next most commonPOS assignments were noun labels, which may in-dicate errors in SSR labeling.Other inserted word types were auxiliary or oth-erwise neutral verbs, and, as expected, most POSlabels assigned by the parses were verb types,mostly VBP (non-third person present singular).About half of these were labeled as predicates withcorresponding semantic roles; the rest were unla-beled which makes sense as true auxiliary verbswere not labeled in the process.Finally, around 147 insertion types made wereneutral arguments (given the orthographic form<ARG>).
32.7% were common nouns and 18.4%of these were labeled personal pronouns PRP.
An-other 11.6% were adjectives JJ.
We found that 22(40.7%) of 54 neutral argument nodes directly as-signed as semantic roles were ARG1, and another33.3% were ARG0.
Nearly a quarter of insertedarguments became part of a larger phrase serv-ing as a modifier argument, the most common ofwhich were CAU and LOC.Reconstructive substitutionsQ: How often do substitutions occur in the an-alyzed data, and is there any semantic con-sistency in the types of words changed?
230phrase tense substitutions occurred in the SSR cor-pus.
Only 13 of these were directly labeled aspredicate arguments (as opposed to being part ofa larger argument), 8 of which were ARG1.
Mor-phology changes generally affect verb tense ratherthan subject number, and with no real impact onsemantic structure.Colloquial substitutions of verbs, such as ?hewas like...?
?
?he said...?, yield more unusual seman-tic analysis on the unreconstructed side as non-verbs were analyzed as verbs.Reconstructive word re-orderingsQ: How is the predicate-argument labeling af-fected?
If reorderings occur as a phrase, whattype of phrase?
Word reorderings labeled asargument movements occurred 136 times in the3,626 semantics-annotated SUs in the SSR corpus.Of these, 81% were directly labeled as argumentsto some sentence-internal predicate.
52% of thosearguments were ARG1, 17%were ARG0, and 13%were predicates.
11% were labeled as modifyingarguments rather than core arguments, which mayindicate confusion on the part of the annotatorsand possibly necessary cleanup.More commonly labeled than argument move-ment was adjunct movement, assigned to 206phrases.
54% of these reordered adjuncts were notdirectly labeled as predicate arguments but werewithin other labeled arguments.
The most com-monly labeled adjunct types were TMP (19% of allarguments), ADV (13%), and LOC (11%).Syntactically, 25% of reordered adjuncts wereassigned ADVP by the automatic parser, 19% wereassigned NP, 18% were labeled PP, and remainingcommon NT assignments included IN, RB, andSBAR.Finally, 239 phrases were labeled as being re-ordered for the general reason of fixing the gram-mar, the default change assignment given by theannotation tool when a word was moved.
Thiscategory was meant to encompass all movementsnot included in the previous two categories (argu-ments and adjuncts), including moving ?I guess?from the middle or end of a sentence to the be-ginning, determiner movement, etc.
Semantically,63% of nodes were directly labeled as predicatesor predicate arguments.
34% of these were PRED,28% were ARG1, 27% were ARG0, 8% wereARG2, and 8% were roughly evenly distributedacross the adjunct argument types.Syntactically, 31% of these changes were NPs,16% were ADVPs, and 14% were VBPs (24% wereverbs in general).
The remaining 30% of changeswere divided amongst 19 syntactic categories fromCC to DT to PP.4.4 Testing the generalizations required forautomatic SRL for speechThe results described in (Gildea and Palmer, 2002)show that parsing dramatically helps during thecourse of automatic SRL.
We hypothesize thatthe current state-of-art for parsing speech is ade-quate to generally identify semantic roles in spon-752taneously produced speech text.
For this to be true,features for which SRL is currently dependent onsuch as consistent predicate-to-parse paths withinautomatic constituent parses must be found to ex-ist in data such as the SSR corpus.The predicate-argument path is defined as thenumber of steps up and down a parse tree (andthrough which NTs) which are taken to traversethe tree from the predicate (verb) to its argument.For example, the path from predicate VBP?
?like?to the argument ARG0 (NP ?
?some kids?)
mightbe [VBP ?
VP ?
S ?
NP].
As trees grow morecomplex, as well as more errorful (as expectedfor the automatic parses of verbatim speech text),the paths seen are more sparsely observed (i.e.
theprobability density is less concentrated at the mostcommon paths than similar paths seen in the Prop-Bank annotations).
We thus consider two pathsimplifications as well:?
compressed: only the source, target, and rootnodes are preserved in the path (so the pathabove becomes [VBP ?
S ?
NP])?
POS class clusters: rather than distinguish,for example, between different tenses ofverbs in a path, we consider only the first let-ter of each NT.
Thus, clustering compressedoutput, the new path from predicate to ARG0becomes [V ?
S ?
N].The top paths were similarly consistent regardlessof whether paths are extracted from Pra , Pvm , orPva (Pva results shown in Table 4), but we see thatthe distributions of paths are much flatter (i.e.
agreater number and total relative frequency of pathtypes) going from manual to automatic parses andfrom parses of verbatim to parses of reconstructedstrings.5 DiscussionIn this work, we sought to find generalizationsabout the underlying structure of errorful and re-constructed speech utterances, in the hopes of de-termining semantic-based features which can beincorporated into automatic systems identifyingsemantic roles in speech text as well as statisti-cal models for reconstruction itself.
We analyzedsyntactic and semantic variation between originaland reconstructed utterances according to manu-ally and automatically generated parses and man-ually labeled semantic roles.Argument Path from Predicate FreqVBP ?
VP ?
S ?
NP 4.9%Predicate- VB ?
VP ?
VP ?
S ?
NP 3.9%Argument VB ?
VP ?
NP 3.8%Paths VBD ?
VP ?
S ?
NP 2.8%944 more path types 84.7%VB ?
S ?
NP 7.3%VB ?
VP ?
NP 5.8%Compressed VBP ?
S ?
NP 5.3%VBD ?
S ?
NP 3.5%333 more path types 77.1%V ?
S ?
N 25.8%V ?
V ?
N 17.5%POS class+ V ?
V ?
A 8.2%compressed V ?
V ?
V 7.7%60 more path types 40.8%Table 4: Frequent Pva predicate-argument pathsSyntactic paths from predicates to argumentswere similar to those presented for WSJ data(Palmer et al, 2005), though these patterns de-graded when considered for automatically parsedverbatim and errorful data.
We believe that auto-matic models may be trained, but if entirely depen-dent on automatic parses of verbatim strings, anSRL-labeled resource much bigger than the SSRand perhaps even PropBank may be required.6 Conclusions and future workThis work is an initial proof of concept that au-tomatic semantic role labeling (SRL) of verbatimspeech text may be produced in the future.
This issupported by the similarity of common predicate-argument paths between this data and the Prop-Bank WSJ annotations (Palmer et al, 2005) andthe consistency of other features currently empha-sized in automatic SRL work on clean text data.To automatically semantically label speech tran-scripts, however, is expected to require additionalannotated data beyond the 3k utterances annotatedfor SRL included in the SSR corpus, though it maybe adequate for initial adaptation studies.This new ground work using available corporato model speaker errors may lead to new intelli-gent feature design for automatic systems for shal-low semantic labeling and speech reconstruction.AcknowledgmentsSupport for this work was provided by NSF PIREGrant No.
OISE-0530118.
Any opinions, find-ings, conclusions, or recommendations expressedin this material are those of the authors and donot necessarily reflect the views of the supportingagency.753ReferencesEugene Charniak.
1999.
A maximum-entropy-inspired parser.
In Proceedings of the Annual Meet-ing of the North American Association for Compu-tational Linguistics.Christopher Cieri, Stephanie Strassel, MohamedMaamouri, Shudong Huang, James Fiumara, DavidGraff, Kevin Walker, and Mark Liberman.
2004.Linguistic resource creation and distribution forEARS.
In Rich Transcription Fall Workshop.Erin Fitzgerald and Frederick Jelinek.
2008.
Linguis-tic resources for reconstructing spontaneous speechtext.
In Proceedings of the Language Resources andEvaluation Conference.Erin Fitzgerald, Keith Hall, and Frederick Jelinek.2009.
Reconstructing false start errors in sponta-neous speech text.
In Proceedings of the AnnualMeeting of the European Association for Computa-tional Linguistics.Erin Fitzgerald.
2009.
Reconstructing SpontaneousSpeech.
Ph.D. thesis, The Johns Hopkins University.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguis-tics, 28(3):245?288.Daniel Gildea and Martha Palmer.
2002.
The neces-sity of parsing for predicate argument recognition.In Proceedings of the Annual Meeting of the Associ-ation for Computational Linguistics.Mary Harper, Bonnie Dorr, John Hale, Brian Roark,Izhak Shafran, Matthew Lease, Yang Liu, MatthewSnover, Lisa Yung, Anna Krasnyanskaya, and RobinStewart.
2005.
Structural metadata and parsingspeech.
Technical report, JHU Language Engineer-ing Workshop.Zellig S. Harris.
1957.
Co-occurrence and transforma-tion in linguistic structure.
Language, 33:283?340.Martha Palmer, Paul Kingsbury, and Daniel Gildea.2005.
The Proposition Bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?106, March.Sameer Pradhan, Wayne Ward, Kadri Hacioglu, JamesMartin, and Dan Jurafsky.
2004.
Shallow semanticparsing using support vector machines.
In Proceed-ings of the Human Language Technology Confer-ence/North American chapter of the Association ofComputational Linguistics (HLT/NAACL), Boston,MA.Sameer Pradhan, James Martin, and Wayne Ward.2008.
Towards robust semantic role labeling.
Com-putational Linguistics, 34(2):289?310.Elizabeth Shriberg.
1994.
Preliminaries to a Theoryof Speech Disfluencies.
Ph.D. thesis, University ofCalifornia, Berkeley.George K. Zipf.
1949.
Human Behavior and the Prin-ciple of Least-Effort.
Addison-Wesley.754
