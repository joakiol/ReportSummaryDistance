Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 394?401,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsIPA and STOUT: Leveraging Linguistic and Source-basedFeatures for Machine Translation EvaluationMeritxell Gonz`alez, Alberto Barr?on-Cede?noTALP Research Center,Technical University of Catalonia{mgonzalez,albarron}@lsi.upc.eduLlu?
?s M`arquezQatar Computing Research InstituteQatar Foundationlmarquez@qf.org.qaAbstractThis paper describes the UPC submissionsto the WMT14 Metrics Shared Task: UPC-IPA and UPC-STOUT.
These metricsuse a collection of evaluation measures in-tegrated in ASIYA, a toolkit for machinetranslation evaluation.
In addition to somestandard metrics, the two submissions takeadvantage of novel metrics that considerlinguistic structures, lexical relationships,and semantics to compare both source andreference translation against the candidatetranslation.
The new metrics are availablefor several target languages other than En-glish.
In the the official WMT14 evalua-tion, UPC-IPA and UPC-STOUT scoredabove the average in 7 out of 9 languagepairs at the system level and 8 out of 9 atthe segment level.1 IntroductionEvaluating Machine Translation (MT) quality is adifficult task, in which even human experts mayfail to achieve a high degree of agreement whenassessing translations.
Conducting manual evalu-ations is impractical during the development cy-cle of MT systems or for transation applicationsaddressed to general users, such as online transla-tion portals.
Automatic evaluation measures bringvaluable benefits in such situations.
Compared tomanual evaluation, automatic measures are cheap,more objective, and reusable across different testsets and domains.Nonetheless, automatic metrics are far fromperfection: when used in isolation, they tend tostress specific aspects of the translation quality andneglect others (particularly during tuning); theyare often unable to capture little system improve-ments (enhancements in very specific aspects ofthe translation process); and they may make un-fair comparisons when they are not able to reflectreal differences among the quality of different MTsystems (Gim?enez, 2008).ASIYA, the core of our approach, is an open-source suite for automatic machine translationevaluation and output analysis.1It provides a richset of heterogeneous metrics and tools to evalu-ate and analyse the quality of automatic transla-tions.
The ASIYA core toolkit was first releasedin 2009 (Gim?enez and M`arquez, 2010a) and hasbeen continuously improved and extended sincethen (Gonz`alez et al., 2012; Gonz`alez et al., 2013).In this paper we first describe the most recentenhancements to ASIYA: (i) linguistic-based met-rics for French and German; (ii) an extended setof source-based metrics for English, Spanish, Ger-man, French, Russian, and Czech; and (iii) the in-tegration of mechanisms to exploit the alignmentsbetween sources and translations.
These enhance-ments are all available in ASIYA since version 3.0.We have used them to prepare the UPC submis-sions to the WMT14 Metrics Task: UPC-IPA andUPC-STOUT, which serve the purpose of testingtheir usefulness in a real comparative setting.The rest of the paper is structured as follows.Section 2 describes the new reference-based met-rics developed, including syntactic parsers for lan-guages other than English.
Section 3 gives thedetails of novel source-based metrics, developedfor almost all the language pairs in this challenge.Section 4 explains our simple metrics combina-tion strategy and analyses the results obtained withboth approaches, UPC-IPA and UPC-STOUT,when applied to the WMT13 dataset.
Finally, Sec-tion 5 summarises our main contributions.2 Reference-based MetricsWe recently added a new set of metrics to ASIYA,which estimate the similarity between reference(ref ) and candidate (cand) translations.
The met-1http://asiya.lsi.upc.edu394rics rely either on structural linguistic informa-tion (Section 2.1), on a semantic mapping (Sec-tion 2.2), or on word n-grams (Section 2.3).2.1 Parsing-based MetricsOur initial set of parsing-based metrics is a follow-up of the proposal by Gim?enez and M`arquez(2010b): it leverages the structural informationprovided by linguistic processors to compute sev-eral similarity cues between two analyzed sen-tences.
ASIYA includes plenty of metrics that cap-ture syntactic and semantic aspects of a transla-tion.
New metrics based on linguistic structuralinformation for French and German and upgradedversions of the parsers for English and Spanish areavailable since version 3.0.2In the WMT14 evaluation, we opt for metricsbased on shallow parsing (SP), constituency pars-ing (CP), and dependency parsing (DPm)3.
Mea-sures based on named entities (NE) and semanticroles (SR) were used to analyse translations intoEnglish as well.
The nomenclature used belowfollows the same patterns as in the ASIYA?s man-ual (Gonz`alez and Gim?enez, 2014).
The manualdescribes every family of metrics in detail.
Next,we briefly depict the concrete metrics involved inour submissions to the WMT14 Shared Task.The set of SP metrics is available for English,German, French, Spanish and Catalan.
Theymeasure the lexical overlapping between parts-of-speech elements in the candidate and referencetranslations.
For instance, SP-Op(VB) measuresthe proportion of correctly translated verbs; andthe coarser SP-Op(*) averages the overlapping be-tween the words for each part of speech.
We alsouse NIST (Doddington, 2002) to compute accu-mulated scores over sequences of n = 1..5 partsof speech (SP-pNIST).Similarly, CP metrics analyse similarities be-tween constituent parse trees associated to can-didate and reference translations.
For instance,CP-STMi5 and CP-STM4 compute, respectively,the proportion of (individual) length-5 and accu-mulated up to length-4 matching sub-paths of thesyntactic tree (Liu and Gildea, 2005).
CP-Oc(*)computes the lexical overlap averaged over all thephrase constituents.
Constituent trees are obtainedusing the parsers of Charniak and Johnson (2005),2Equivalent resources were previously available for En-glish, Catalan, and Spanish.3ASIYA includes two dependency parsers; the m identifiesthe metrics calculated using the MALT parser.Bonsai v3.2 (Candito et al., 2010b), and Berke-ley Parser (Petrov et al., 2006; Petrov and Klein,2007) for English, French, and German, respec-tively.Measures based on dependency parsing (DPm)?
available for English and French thanks tothe MALT parser (Nivre et al., 2007)?
capturethe similarities between dependency tree items(i.e., heads and modifiers).
The pre-trained mod-els for French were obtained from the FrenchTreebank (Candito et al., 2010a) and used totrain the Bonsai parser, which in turn uses theMALT parser.
For instance, DPm-HWCM w-3 re-trieves average accumulated proportion of match-ing word-chains (Liu and Gildea, 2005) upto length 3; and DPm-HWCMi c-3 computesthe proportion of matching category-chains oflength 3.2.2 Explicit-Semantics MetricAdditionally, we borrowed a metric originally pro-posed in the field of Information Retrieval: ex-plicit semantic analysis (ESA) (Gabrilovich andMarkovitch, 2007).
ESA is a similarity metricthat relies on a large corpus of general knowl-edge to represent texts.
Our knowledge corporaare composed of ?
100K Wikipedia articles from2010 for the following target languages: English,French and German.
In this case, ref and candtranslations are both mapped onto the Wikipediacollection W .
The similarities between each textand every article a ?
W are computed on the ba-sis of the cosine measure in order to compose asimilarities vector that represents the text.
That is:~ref = {sim(ref, a) ?a ?W} , (1)~cand = {sim(cand, a)?a ?W} .
(2)As the i-th elements in both~ref and~cand representthe similarity of ref and cand sentences to a com-mon article, the similarity between ref and candcan be estimated by computing sim(~ref,~cand).2.3 Language-Independent Resource-FreeMetricWe consider a simple characterisation based onword n-grams.
Texts are broken down into over-lapping word sequences of length n, with 1-wordshifting.
The similarity between cand and refis computed on the basis of the Jaccard coeffi-cient (Jaccard, 1901).
We used this metric for thepairs English?Russian and Russian-English, con-sidering n = 2 (NGRAM-jacTok2ngram).
For the395rest of the pairs we opt for the character-n-grammetrics described in Section 3.1, but they showedno positive results in the English?Russian pair dur-ing our tuning experiments.3 Source-based MetricsWe enhance our evaluation module by includinga set of new metrics that compare the source textagainst the translations.
The metrics can be di-vided into two subsets: those that do not requireany external resources (Section 3.1) and those thatdepend on a parallel corpus (Section 3.2).3.1 Language-Independent Resource-FreeMetricsWe opted for two characterisations that allow forthe comparison of texts across languages withoutexternal resources nor language-related knowl-edge ?as far as the languages use the same writ-ing system.4The first characterisation is character n-grams;proposed by McNamee and Mayfield (2004) forcross-language information retrieval between Eu-ropean languages.
Texts are broken down intooverlapping character sequences of length n, with1-character shifting.
We opt for case-folded bi-grams (NGRAM-cosChar2ngrams), as they al-lowed for the best performance across all the pairs(except for From/To Russian pairs) during tuning.The second characterisation (NGRAM-jacCognates) is based on the concept ofcognateness; originally proposed for bitextsalignment (Simard et al., 1992).
A word is apseudo-cognate candidate if (i) it has only lettersand |w| ?
4, (ii) it contains at least one digit, or(iii) it is a single punctuation mark.
src and candsentences are then represented as word vectors,containing only those words fulfilling one of theprevious conditions.
In the case of (i) the word iscut down to its leading four characters only.In both cases (character n-grams and cognate-ness) cand translations are compared against srcsentences on the basis of the cosine similaritymeasure.3.2 Parallel-Corpus MetricsWe consider two metrics that make use of parallelcorpora: length factor and alignment.4Previous research showed that transliteration is agood short-cut when dealing with different writing sys-tems (Barr?on-Cede?no et al., 2014).Table 1: Length factor parameters as estimated onthe WMT13 parallel corpora.pair ?
?
pair ?
?en?cs 0.972 0.245 cs?en 1.085 0.273en?de 1.176 0.926 de?en 0.961 0.463en?fr 1.158 0.411 fr?en 0.914 0.313en?ru 1.157 0.678 ru?en 1.069 0.668The length factor (LeM) is rooted in the fact thatthe length of a text and its translation tend to pre-serve a certain length correlation.
For instance,translations from English into Spanish or Frenchtend to be longer than their source.
Similar mea-sures were proposed during the statistical machinetranslation early days, both considering character-and word-level lengths (Gale and Church, 1993;Brown et al., 1991).
Pouliquen et al.
(2003) de-fines the length factor as:%(d?)
= e?0.5(|d?||dq|???
)2, (3)where ?
and ?
represent the mean and standarddeviation of the character lengths between trans-lations of texts from L into L?.
This is a stochas-tic normal distribution that results in higher valuesas the length of the target text approaches the ex-pected value given the source.
Table 1 includesthe values for each language pair, as estimated onthe WMT13 parallel corpora.
Note that this metricwas not applied to Hindi?English since this lan-guage pair was not present in the WMT13 chal-lenge.The last of our newly-added measures relieson the word alignments calculated between thesentence pairs src?cand and src?ref.
We trainedalignment models for each language pair using theBerkeley Aligner5, and devised three variants ofan ALGN metric, which compute: (i) the propor-tion of aligned words between src and cand (AL-GNs); (ii) the proportion of aligned words betweencand and ref, calculated as the combination of thealignments src?cand and src?ref (ALGNr); and(iii) the ratio of shared alignments between src?cand and src?ref (ALGNp).4 Experimental ResultsThe tuning and selection of the different met-rics to build UPC-IPA and UPC-STOUT was5https://code.google.com/p/berkeleyaligner396conducted considering the WMT13 Metrics Taskdataset (Mach?a?cek and Bojar, 2013), and the re-sources distributed for the WMT13 TranslationTask (Bojar et al., 2013).
Table 2 gives acomplete list of these metrics grouped by fami-lies.
First, we calculated the Pearson?s correla-tion with the human judgements for all the met-rics in the current version of the ASIYA repos-itory, including standard MT evaluation metrics,such as METEOR (Denkowski and Lavie, 2011),GTM (Melamed et al., 2003), -TERp-A (Snoveret al., 2009) (a variant of TER tuned towards ade-quacy), WER (Nie?en et al., 2000) and PER (Till-mann et al., 1997).
We selected the best perform-ing metrics (i.e., those resulting in high Pearsoncoefficients) in each family across all the From/ToEnglish translation language pairs, including thenewly developed measures ?even if they per-formed poorly compared to others (see This is howthe UPC-STOUT metrics sets for both from En-glish and To English translation pairs were com-posed6(see Table 3).Table 2: Metrics considered in the experimentsseparated by families according to the type ofgrammatical items they use.1.
-WER 17.
DPm-HWCM r-12.
-PER 18.
DPm-Or(*)3.
-TERp-A 19.
SR-Or(*)4.
METEOR-ex 20.
SR-Or5.
METEOR-pa 21.
SR-Orv(*)6.
GTM-3 22.
SR-Orv7.
SP-Op(*) 23.
NE-Oe(*)8.
SP-pNIST 24.
NE-Oe(**)9.
CP-STMi-5 25.
ESA10.
CP-STMi-2 26.
NGRAM-jacTok2ngrams11.
CP-STMi-3 27.
NGRAM-jacCognates12.
CP-STM-4 28.
NGRAM-cosChar2ngrams13.
CP-Oc(*) 29.
LeM14.
DPm-HWCM w-3 30.
ALGNp15.
DPm-HWCM c-3 31.
ALGNs16.
DPm-HWCMi c-3 32.
ALGNrTable 3: Metrics considered in each system.7BAS: 1?6 SYN: 7?18SEM: 19?25 SRC: 26?32IPA: 1?9, 25?31 STOUT: 1?326Parser-based measures are not present in Czech nor Rus-sian as target languages, ALGN is not available for Frenchpairs, and ESA is not applied to Russian as target.The metric sets included in UPC-IPA are lightversions of the UPC-STOUT ones.
They werecomposed following different criteria, dependingon the translation direction.
Parsing-based mea-sures were already available in the previous ver-sion of ASIYA when translating into English ?they are known to be robust across domains andare usually good indicators of translation qual-ity (Gim?enez and M`arquez, 2007).
So, in orderto assess the gain achieved with these measureswith respect the new ones, UPC-IPA neglects themeasures based on structural information obtainedfrom parsers.
In contrast, this distinction was notsuitable for the From English pairs since the num-ber of resources and measures varies for each lan-guage.
Hence, in this latter case, UPC-IPA usedonly the subset of measures from UPC-STOUTthat required no or little resources.In summary, when English is the target lan-guage, UPC-IPA uses the baseline evaluationmetrics along with the length factor, alignments-based metrics, character n-grams, and ESA.
In ad-dition to the above metrics, UPC-STOUT usesthe linguistic-based metrics over parsing trees,named entities, and semantic roles.
When Englishis the source language, UPC-IPA relies on thebasic collection of metrics and character n-gramsonly.
UPC-STOUT includes the alignment-basedmetrics, length factor, ESA, and the syntacticparsers applied to both German and French.In all cases (metric sets and language pairs),the translation quality score is computed as theuniformly-averaged linear combination (ULC) ofall the individual metrics for each sentence in thetestset.
Its calculation implies the normalizationof heterogeneous scores (some of them are neg-ative or unbounded), into the range [0, 1].
As aconsequence, the scores of UPC-IPA and UPC-STOUT constitute a natural way of ranking dif-ferent translations, rather than an overall qualityestimation measure.
We opt for this linear combi-nation for simplicity.
The discussion below sug-gests that a more sophisticated method for weighttuning (e.g., relying on machine learning methods)would be required for each language pair, domainand/or task since different metric families performnotably different for each subtask.We complete our experimentation by eval-uating more configurations: BAS, a baseline7These are the full sets of measures for each configura-tion.
However, each specific subset for From/To English canvary slightly depending on the available resources.397Table 4: System-level Pearson correlation for automatic metrics over translations From/To English.WMT13 en?fr en?de en?es en?cs en?ru fr?en de?en es?en cs?en ru?enUPC-IPA 93.079 85.147 88.702 85.259 70.345 96.755 94.660 95.065 94.316 72.083UPC-STOUT 94.274 90.193 73.314 84.743 70.544 96.916 96.208 96.704 96.666 74.050BAS 92.502 84.251 90.051 86.584 67.655 95.777 96.506 95.98 96.539 71.536SYN 95.68 87.297 96.965 n/a n/a 96.291 96.592 96.052 95.238 73.083BAS+SYN 94.584 87.786 95.162 n/a n/a 96.684 97.057 96.101 96.402 72.800SEM 89.735 83.647 35.694 95.067 n/a 95.629 96.601 98.021 96.595 76.158BAS+SEM 92.254 87.005 47.321 89.107 n/a 96.337 97.534 97.568 97.371 74.804SRC 14.465 -16.796 -22.466 -49.981 39.527 13.405 -51.371 71.64 -73.254 68.766BAS+SRC 93.637 76.401 83.754 64.742 54.128 95.395 90.889 93.299 89.216 71.882WMT13-Best 94.745 93.813 96.446 86.036 81.194 98.379 97.789 99.171 83.734 94.768WMT13-Worst 78.787 -45.461 87.677 69.151 61.075 95.118 92.239 79.957 60.918 82.058Table 5: Segment-level Kendall?s ?
correlation for automatic metrics over translations From/To English.WMT13 en?fr en?de en?es en?cs en?ru fr?en de?en es?en cs?en ru?enUPC-IPA 18.625 14.901 17.057 7.805 15.132 22.832 25.769 26.907 21.207 19.904UPC-STOUT 19.488 15.012 17.166 8.545 15.279 23.090 27.117 26.848 21.332 19.100BAS 19.477 13.589 16.975 8.449 15.599 24.060 28.259 28.381 23.346 20.983SYN 16.554 14.970 16.444 n/a n/a 22.365 24.289 23.889 20.232 17.679BAS+SYN 19.112 16.016 18.122 n/a n/a 23.940 28.068 27.988 23.180 19.659SEM 12.184 9.249 10.871 3.808 n/a 17.282 19.083 20.859 15.186 14.971BAS+SEM 19.167 13.291 15.857 7.732 n/a 22.024 25.788 26.360 21.427 19.117SRC 2.745 2.481 1.152 1.992 5.247 2.181 1.154 8.700 -4.023 16.267BAS+SRC 18.32 13.017 15.698 7.666 13.619 22.292 24.948 26.780 17.603 20.707WMT13-Best 21.897 19.459 20.699 11.283 18.899 26.836 29.565 24.271 21.665 25.584WMT13-Worst 16.753 13.910 3.024 4.431 13.166 14.008 14.542 14.494 9.667 13.178with standard and commonly used MT metrics;SYN, the reference-based syntactic metrics; SEM,the reference-based semantic metrics; SRC, thesource-based metrics; and the combination ofBAS with every other configuration: BAS+SYN,BAS+SEM, and BAS+SRC.
Their purpose is toevaluate the contribution of the newly developedsets of metrics with respect to the baseline.
Thecomposition of the different configurations is sum-marised in Tables 2 and 3.Evaluation results are shown in Tables 4 and 5.For each configuration and language pair, we showthe correlation coefficients obtained at the system-and the segment-level, respectively.
As customarywith the WMT13 dataset, Pearson correlation wascomputed at the system-level, whereas Kendall?s?
was used to estimate segment-level rank correla-tions.
Additionally to the two submitted and sevenextra configurations, we include the coefficientsobtained with the Best and Worst systems reportedin the official WMT13 evaluation for each lan-guage pair.Although the results of our two submitted sys-tems are not radically different to each other,UPC-STOUT consistently outperforms UPC-IPA.
The currently available version of ASIYA, in-cluding the new metrics, allows for a performanceclose to the top-performing evaluation measures inlast year?s challenge, even with our na?
?ve combi-nation strategy.It is worth noting that no configuration be-haves the same way throughout the different lan-guages.
In some cases (e.g., with the SRC config-uration), the bad performance can be explained bythe weaknesses of the necessary resources whencomputing certain metrics.
When analysed in de-tail, the cause can be ascribed to different metricfamilies in each case.
As a result, it is clear thatspecific configurations are necessary for evaluat-ing different languages and domains.
We plan toapproach these issues as part of our future work.When looking at the system-level figures, onecan observe that the SEM set allows for a con-siderable improvement over the baseline system.The further inclusion of the SYN set ?whenavailable?, tends to increase the quality of theestimations, mainly when English is the sourcelanguage.
These properties impact on some ofthe UPC-STOUT configurations.
In contrast,when looking at the segment-level scores, while398Table 6: System-level Pearson correlation resultsin the WMT14 Metrics shared tasken?fr en?de en?cs en?ruUPC-IPA 93.7 13.0 96.8 92.2UPC-STOUT 93.8 14.8 93.8 92.1WMT14-Best 95.9 19.8 98.8 94.2WMT14-Worst 88.8 1.1 93.8 90.3fr?en de?en hi?en cs?en ru?enUPC-IPA 96.6 89.4 91.5 82.4 80.0UPC-STOUT 96.8 91.4 89.8 94.7 82.5WMT14-Best 98.1 94.2 97.6 99.3 86.1WMT14-Worst 94.5 76.0 41.1 74.1 -41.7the SYN measures still tend to provide some gainover the baseline, the SEM ones do not.
Finally, itmerits some attention the good results achieved bythe baseline for translations into English.
We mayremark here that our baseline included also thebest performing state-of-the-art metrics, includingall the variants of METEOR, that reported goodresults in the WMT13 challenge.Tables 6 and 7 show the official results obtainedby UPC-IPA and UPC-STOUT in WMT14.8The best and worst figures for each language pairare included for comparison ?the worst perform-ing submission at segment level is neglected as itseems to be a dummy (Mach?a?cek and Bojar, 2014to appear).
Both UPC-IPA and UPC-STOUTconfigurations resulted in different performancesdepending on the language pair.
UPC-STOUTscored above the average for all the language pairsexcept for en?cs at both system and segment level,and en?ru at system level.
Although the evaluationresults are not directly comparable to the WMT13ones, one can note that the results were notablybetter for pairs that involved Czech and Russian,and worse for those that involved French and Ger-man at system level.
Analysing the impact of theevaluation methods and building comparable re-sults in order to address a study on configurationsfor different languages is part of our future work.5 ConclusionsThis paper describes the UPC submission to theWMT14 metrics for automatic machine transla-tion evaluation task.
The core of our evaluationsystem is ASIYA, a toolkit for MT evaluation.
Be-sides the formerly available metrics in ASIYA, weexperimented with new metrics for machine trans-8At the time of submitting this paper, the evaluation re-sults for WMT14 Metrics Task were provisional.Table 7: Segment-level Kendall?s ?
correlation re-sults in the WMT14 Metrics shared tasken?fr en?de en?cs en?ruUPC-IPA 26.3 21.7 29.7 42.6UPC-STOUT 27.8 22.4 28.1 42.5WMT14-Best 29.7 25.8 34.4 44.0WMT14-Worst 25.4 18.5 28.1 38.1fr?en de?en hi?en cs?en ru?enUPC-IPA 41.2 34.1 36.7 27.4 32.4UPC-STOUT 40.3 34.5 35.1 27.5 32.4WMT14-Best 43.3 38.1 43.8 32.8 36.4WMT14-Worst 31.1 22.5 23.7 18.7 21.2lation evaluation, with especial focus on transla-tion from English into other languages.As previous work on English as target languagehas proven, syntactic and semantic analysis cancontribute positively to the evaluation of automatictranslations.
For this reason, we integrated a set ofnew metrics for different languages, aimed at eval-uating a translation from different perspectives.Among the novelties, (i) new shallow metrics, bor-rowed from Information Retrieval, were includedto compare the candidate translation against boththe reference translation (monolingual compari-son) and the source sentence (cross-language com-parison), including explicit semantic analysis andthe lexical-based characterisations character n-grams and pseudo-cognates; (ii) new parsers forother languages than English were applied to com-pare automatic and reference translation at thesyntactic level; (iii) an experimental metric basedon alignments; and (iv) a metric based on the cor-relation of the translations?
expected lengths wasincluded as well.
Our preliminary experimentsshowed that the combination of these and standardMT evaluation metrics allows for a performanceclose to the best in last year?s competition for somelanguage pairs.The new set of metrics is already availablein the current version of the toolkit ASIYAv3.0 (Gonz`alez and Gim?enez, 2014).
Our currentefforts are focused on the exploitation of more so-phisticated methods to combine the contributionsof each metric, and the extension of the list of sup-ported languages.AcknowledgementsThis work was funded by the Spanish Ministryof Education and Science (TACARDI project,TIN2012-38523-C02-00).399ReferencesAlberto Barr?on-Cede?no, Monica Lestari Paramita, PaulClough, and Paolo Rosso.
2014.
A Comparisonof Approaches for Measuring Cross-Lingual Sim-ilarity of Wikipedia Articles.
Advances in Infor-mation Retrieval.
Proceedings of the 36th EuropeanConference on IR Research, LNCS (8416):424?429.Springer-Verlag.Ond?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut, andLucia Specia.
2013.
Findings of the 2013 Work-shop on Statistical Machine Translation.
In Pro-ceedings of the Eighth Workshop on Statistical Ma-chine Translation, pages 1?44, Sofia, Bulgaria, Au-gust.
Association for Computational Linguistics.Peter F. Brown, Jennifer C. Lai, and Robert L. Mer-cer.
1991.
Aligning Sentences in Parallel Corpora.In Douglas E. Appelt, editor, Proceedings of the29th Annual Meeting of the Association for Com-putational Linguistics (ACL 1991), pages 169?176,Berkeley, CA, USA.
Association for ComputationalLinguistics.Marie Candito, Benot Crabb, and Pascal Denis.
2010a.Statistical French dependency parsing: treebankconversion and first results.
In The seventh interna-tional conference on Language Resources and Eval-uation (LREC), Valletta, Malta.Marie Candito, Joakim Nivre, Pascal Denis, and En-rique Henestroza Anguiano.
2010b.
Benchmark-ing of Statistical Dependency Parsers for French.
InProc.
23rd Intl.
COLING Conference on Computa-tional Linguistics: Poster Volume, pages 108?116,Beijing, China.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-Fine N-best Parsing and MaxEnt DiscriminativeReranking.
In Proc.
43rd Annual Meeting of theAssociation for Computational Linguistics (ACL),pages 173?180, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic Metric for Reliable Optimizationand Evaluation of Machine Translation Systems.
InProceedings of the 6th Workshop on Statistical Ma-chine Translation, pages 85?91, Stroudsburg, PA,USA.
Association for Computational Linguistics.George Doddington.
2002.
Automatic Evaluationof Machine Translation Quality Using N-gram Co-Occurrence Statistics.
In Proceedings of the 2nd In-ternational Conference on Human Language Tech-nology, pages 138?145, San Francisco, CA, USA.Morgan Kaufmann Publishers Inc.E.
Gabrilovich and S. Markovitch.
2007.
ComputingSemantic Relatedness Using Wikipedia-based Ex-plicit Semantic Analysis.
In 20th International JointConference on Artificial Intelligence, pages 1606?1611, San Francisco, CA, USA.William A. Gale and Kenneth, W. Church.
1993.
AProgram for Aligning Sentences in Bilingual Cor-pora.
Computational Linguistics, 19:75?102.Jes?us Gim?enez and Llu?
?s M`arquez.
2007.
Linguis-tic Features for Automatic Evaluation of Heteroge-neous MT Systems.
In Proc.
of 2nd Workshop onstatistical Machine Translation (WMT07), ACL?07,Prague, Czech Republic.Jes?us Gim?enez and Llu?
?s M`arquez.
2010a.
Asiya:An Open Toolkit for Automatic Machine Translation(Meta-)Evaluation.
The Prague Bulletin of Mathe-matical Linguistics, 94:77?86.Jes?us Gim?enez and Llu?
?s M`arquez.
2010b.
LinguisticMeasures for Automatic Machine Translation Eval-uation.
Machine Translation, 24(3?4):77?86.Jes?us Gim?enez.
2008.
Empirical Machine Translationand its Evaluation.
Ph.D. thesis, Universitat Politc-nica de Catalunya, July.Meritxell Gonz`alez and Jes?us Gim?enez.
2014.Asiya: An Open Toolkit for Automatic Ma-chine Translation (Meta-)Evaluation, v3.0, Febru-ary.
http://asiya.lsi.upc.edu.Meritxell Gonz`alez, Jes?us Gim?enez, and Llu??sM`arquez.
2012.
A Graphical Interface for MT Eval-uation and Error Analysis.
In Proc.
Annual Meet-ing of the Association for Computational Linguis-tics (ACL).
System Demonstration, pages 139?144,Jeju, South Korea, July.
Association for Computa-tional Linguistics.Meritxell Gonz`alez, Laura Mascarell, and Llu??sM`arquez.
2013. tSearch: Flexible and FastSearch over Automatic translation for ImprovedQuality/Error Analysis.
In Proc.
51st Annual Meet-ing of the Association for Computational Linguis-tics (ACL).
System Demonstration, pages 181?186,Sofia, Bulgaria, August.Paul Jaccard.
1901.?Etude comparative de la distribu-tion florale dans une portion des Alpes et des Jura.Bulletin del la Soci?et?e Vaudoise des Sciences Na-turelles, 37:547?579.Ding Liu and Daniel Gildea.
2005.
Syntactic Fea-tures for Evaluation of Machine Translation.
In Pro-ceedings of ACL Workshop on Intrinsic and Extrin-sic Evaluation Measures for MT and/or Summariza-tion, pages 25?32, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Matou?s Mach?a?cek and Ond?rej Bojar.
2013.
Results ofthe WMT13 metrics shared task.
In Proceedings ofthe Eighth Workshop on Statistical Machine Trans-lation, pages 45?51, Sofia, Bulgaria, August.
Asso-ciation for Computational Linguistics.Matou?s Mach?a?cek and Ond?rej Bojar.
2014 (to appear).Results of the WMT14 metrics shared task.
In Pro-ceedings of the Ninth Workshop on Statistical Ma-chine Translation, Baltimare, US, June.
Associationfor Computational Linguistics.400Paul McNamee and James Mayfield.
2004.
CharacterN-Gram Tokenization for European Language TextRetrieval.
Information Retrieval, 7(1-2):73?97.I.
Dan Melamed, Ryan Green, and Joseph P. Turian.2003.
Precision and Recall of Machine Transla-tion.
In Proceedings of the Joint Conference on Hu-man Language Technology and the North AmericanChapter of the Association for Computational Lin-guistics (HLT-NAACL), pages 61?63, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Sonja Nie?en, Franz Josef Och, Gregor Leusch, andHermann Ney.
2000.
An Evaluation Tool for Ma-chine Translation: Fast Evaluation for MT Research.In Proceedings of the 2nd International Conferenceon Language Resources and Evaluation (LREC).Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, G?ulsen Eryigit, Sandra K?ubler, SvetoslavMarinov, and Erwin Marsi.
2007.
MaltParser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(2):95?135.Slav Petrov and Dan Klein.
2007.
Improved Infer-ence for Unlexicalized Parsing.
In Proc.
HumanLanguage Technologies (HLT), pages 404?411.
As-sociation for Computational Linguistics, April.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and in-terpretable tree annotation.
In 21st InternationalConference on Computational Linguistics and the44th annual meeting of the Association for Com-putational Linguistics, ACL-44, pages 433?440,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Bruno Pouliquen, Ralf Steinberger, and Camelia Ignat.2003.
Automatic Identification of Document Trans-lations in Large Multilingual Document Collections.In Proceedings of the International Conference onRecent Advances in Natural Language Processing(RANLP-2003), pages 401?408, Borovets, Bulgaria.Michel Simard, George F. Foster, and Pierre Isabelle.1992.
Using Cognates to Align Sentences in Bilin-gual Corpora.
In Proceedings of the Fourth Interna-tional Conference on Theoretical and Methodologi-cal Issues in Machine Translation.Matthew Snover, Nitin Madnani, Bonnie Dorr, andRichard Schwartz.
2009.
Fluency, adequacy, orHTER?
Exploring different human judgments with atunable MT metric.
In Proceedings of the 4th Work-shop on Statistical Machine Translation, pages 259?268, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Christoph Tillmann, Stefan Vogel, Hermann Ney,A.
Zubiaga, and H. Sawaf.
1997.
Accelerated DPbased Search for Statistical Translation.
In Proceed-ings of European Conference on Speech Communi-cation and Technology, pages 2667?2670.401
