Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 33?40Manchester, August 2008Improving Alignments for Better Confusion Networksfor Combining Machine Translation SystemsNecip Fazil Ayan and Jing Zheng and Wen WangSRI InternationalSpeech Technology and Research Laboratory (STAR)333 Ravenswood AvenueMenlo Park, CA 94025{nfa,zj,wwang}@speech.sri.comAbstractThe state-of-the-art system combinationmethod for machine translation (MT) isthe word-based combination using confu-sion networks.
One of the crucial steps inconfusion network decoding is the align-ment of different hypotheses to each otherwhen building a network.
In this paper, wepresent new methods to improve alignmentof hypotheses using word synonyms and atwo-pass alignment strategy.
We demon-strate that combination with the new align-ment technique yields up to 2.9 BLEUpoint improvement over the best input sys-tem and up to 1.3 BLEU point improve-ment over a state-of-the-art combinationmethod on two different language pairs.1 IntroductionCombining outputs of multiple systems perform-ing the same task has been widely explored invarious fields such as speech recognition, wordsense disambiguation, and word alignments, and ithad been shown that the combination approachesyielded significantly better outputs than the in-dividual systems.
System combination has alsobeen explored in the MT field, especially withthe emergence of various structurally different MTsystems.
Various techniques include hypothesisselection from different systems using sentence-level scores, re-decoding source sentences usingphrases that are used by individual systems (Rostiet al, 2007a; Huang and Papineni, 2007) andc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.word-based combination techniques using confu-sion networks (Matusov et al, 2006; Sim et al,2007; Rosti et al, 2007b).
Among these, confu-sion network decoding of the system outputs hasbeen shown to be more effective than the others interms of the overall translation quality.One of the crucial steps in confusion networkdecoding is the alignment of hypotheses to eachother because the same meaning can be expressedwith synonymous words and/or with a differentword ordering in different hypotheses.
Unfortu-nately, all the alignment algorithms used in confu-sion network decoding are insensitive to synonymsof words when aligning two hypotheses to eachother.
This paper extends the previous alignmentapproaches to handle word synonyms more effec-tively to improve alignment of different hypothe-ses.
We also present a two-pass alignment strategyfor a better alignment of hypotheses with similarwords but with a different word ordering.We evaluate our system combination approachusing variants of an in-house hierarchical MT sys-tem as input systems on two different languagepairs: Arabic-English and Chinese-English.
Evenwith very similar MT systems as inputs, we showthat the improved alignments yield up to an abso-lute 2.9 BLEU point improvement over the bestinput system and up to an absolute 1.3 BLEUpoint improvement over the old alignments in aconfusion-network-based combination.The rest of this paper is organized as follows.Section 2 presents an overview of previous sys-tem combination techniques for MT.
Section 3 dis-cusses the confusion-network-based system com-bination.
In Section 4, we present the new hy-pothesis alignment techniques.
Finally, Section 5presents our experiments and results on two lan-guage pairs.332 Related WorkSystem combination for machine translation canbe done at three levels: Sentence-level, phrase-level or word-level.Sentence-level combination is done by choosingone hypothesis amongmultipleMT system outputs(and possibly among n-best lists).
The selectioncriterion can be a combination of translation modeland language model scores with multiple compar-ison tests (Akiba et al, 2002), or statistical confi-dence models (Nomoto, 2004).Phrase-level combination systems assume thatthe input systems provide some internal informa-tion about the system, such as phrases used by thesystem, and the task is to re-decode the source sen-tence using this additional information.
The firstexample of this approach was the multi-engine MTsystem (Frederking and Nirenburg, 1994), whichbuilds a chart using the translation units insideeach input system and then uses a chart walk algo-rithm to find the best cover of the source sentence.Rosti et al (2007a) collect source-to-target corre-spondences from the input systems, create a newtranslation option table using only these phrases,and re-decode the source sentence to generate bet-ter translations.
In a similar work, it has beendemonstrated that pruning the original phrase ta-ble according to reliable MT hypotheses and en-forcing the decoder to obey the word orderings inthe original system outputs improves the perfor-mance of the phrase-based combination systems(Huang and Papineni, 2007).
In the absence ofsource-to-target phrase alignments, the sentencescan be split into simple chunks using a recursivedecomposition as input to MT systems (Mellebeeket al, 2006).
With this approach, the final outputis a combination of the best chunk translations thatare selected by majority voting, system confidencescores and language model scores.The word-level combination chooses the besttranslation units from different translations andcombine them.
The most popular method forword-based combination follows the idea behindthe ROVER approach for combining speech recog-nition outputs (Fiscus, 1997).
After reorderinghypotheses and aligning to each other, the com-bination system builds a confusion network andchooses the path with the highest score.
The fol-lowing section describes confusion-network-basedsystem combination in detail.?
2005 SRI InternationalConfusion Network ExampleHypothesis 1: she went homeHypothesis 2: she was at schoolHypothesis 3: at home was sheshe school#eps# homeatwentwasshe homewas school#eps#atwent she#eps#was#eps# wentat#eps# home #eps#school<s><s><s> </s></s></s>Figure 1: Alignment of three hypotheses to eachother using different hypotheses as skeletons.3 System Combination with ConfusionNetworksThe general architecture of a confusion-network-based system combination is as follows:1.
Extract n-best lists from MT systems.2.
Pick a skeleton translation for each segment.3.
Reorder all the other hypotheses by aligningthem to the skeleton translation.4.
Build a confusion network from the re-ordered translations for each segment.5.
Decode the confusion network using vari-ous arc features and sentence-level scoressuch as LM score and word penalty.6.
Optimize feature weights on a held-out testset and re-decode.In this framework, the success of confusion net-work decoding for system combination depends ontwo important choices: Selection of the skeletonhypothesis and alignment of other hypotheses tothe skeleton.For selecting the best skeleton, two commonmethods are choosing the hypothesis with the Min-imum Bayes Risk with translation error rate (TER)(Snover et al, 2006) (i.e., the hypothesis with theminimum TER score when it is used as the ref-erence against the other hypotheses) (Sim et al,2007) or choosing the best hypotheses from eachsystem and using each of those as a skeleton inmultiple confusion networks (Rosti et al, 2007b).In this paper, we use the latter since it performsslightly better than the first method in our exper-iments.
An example confusion network on threetranslations is presented in Figure 1.1The major difficulty when using confusion net-works for system combination for MT is aligningdifferent hypotheses to the skeleton since the word1In this paper, we use multiple confusion networks that areattached to the same start and end node.
Throughout the restof the paper, the term confusion network refers to one networkamong multiple networks used for system combination.34order might be different in different hypothesesand it is hard to align words that are shifted fromone hypothesis to another.
Four popular methodsto align hypotheses to each other are as follows:1.
Multiple string-matching algorithm basedon Levenshtein edit distance (Bangalore etal., 2001)2.
A heuristic-based matching algorithm (Ja-yaraman and Lavie, 2005)3.
Using GIZA++ (Och and Ney, 2000) withpossibly additional training data (Matusovet al, 2006)4.
Using TER (Snover et al, 2006) betweenthe skeleton and a given hypothesis (Sim etal., 2007; Rosti et al, 2007b)None of these methods takes word synonymsinto account during alignment of hypotheses.2Inthis work, we extend the TER-based alignmentto use word stems and synonyms using the pub-licly available WordNet resource (Fellbaum, 1998)when aligning hypotheses to each other and showthat this additional information improves the align-ment and the overall translation significantly.4 Confusion Networks with WordSynonyms and Two-pass AlignmentWhen building a confusion network, the goal is toput the same words on the same arcs as much aspossible.
Matching similar words between two hy-potheses is necessary to achieve this goal.When we align two different hypotheses usingTER, it is necessary that two words have the iden-tical spelling to be considered a match.
However,in natural languages, it is possible to represent thesame meaning using synonyms of words in pos-sibly different positions.
For example, in the fol-lowing sentences, ?at the same time?
and ?in themeantime?, ?waiting for?
and ?expect?, and ?set?and ?established?
correspond to each other, re-spectively:Skeleton: at the same time expect israelto abide by the deadlines set by .Hypothesis: in the meantime , we arewaiting for israel to abide by theestablished deadlines .Using TER, synonymous words might bealigned to each other if they appear in the same po-2Note that the approach by Matusov et al (2006) at-tempts to align synonyms and different morphological formsof words to each other but this is done implicitly, relying onthe parallel text to learn word alignments.sition in two hypotheses but this is less likely whentwo words appear in different positions.
With-out knowing that two words are synonyms of eachother, they are considered two separate words dur-ing TER alignment.Our goal is to create equivalence classes foreach word in the given translations and modify thealignment algorithm to give priority to the match-ing of words that are in the same equivalence class.In this paper, the equivalence classes are generatedusing WordNet by extracting synonyms of eachword in the translations.To incorporate matching of word synonyms intothe alignment, we followed three steps:1.
Use WordNet to extract synonyms of thewords that appear in all hypotheses.2.
Augment each skeleton word with all syn-onymous words that appear in all the hy-potheses.3.
Modify TER script to handle words withalternatives using an additional synonymmatching operation.In the following subsections, we describe howeach of these tasks is performed.4.1 Extracting Synonyms from WordNetThe first step is to use WordNet to extract syn-onyms of each word that appears in all hypotheses.This is simply done using the publicly availableWordNet processing tools to extract all synonymsof the given word.
To allow matching words thathave the same stem or variations of the same wordwith different part-of-the-speech (POS) tags, weextract all synonyms of the given word regardlessof their POS tag in the given translation.3In the example above, it is clear that the verbswait and expect have the same meaning but TERis unable to align these two words to each otherbecause of different word positions.
Using Word-Net, we get the following two entries for the wordswaiting and expect, indicating that two words canbe considered as synonyms of each other:expect: Sense 1: expect, anticipate ?
evalu-ate, pass judgment, judge.
Sense 2: ask, require,expect ?
demand.
Sense 3: expect, look, await,wait.
Sense 4: expect ?
see, consider, reckon,3Our goal is to add as many synonyms as possible to in-crease the chances of a word aligning to one of its possiblesynonyms rather than to any other word.
Therefore, we donot distinguish between the synonyms of the same word ac-cording to their confidence value or their POS tag.35ONE-PASS ALIGNMENTkhaddam receives italian house speakerkhaddam receives the head of the italian chamber of deputieskhaddam will meet with the president of the italian chamber of deputiesTWO-PASS ALIGNMENTkhaddam receives italian house speakerkhaddam receives of the head italian chamber of the deputieskhaddam will meet with president of the italian chamber of the deputiesFigure 2: Example alignment of three hypotheses with one-pass and two-pass alignmentview, regard.
Sense 5: expect ?
expect, look,await, wait.
Sense 6: have a bun in the oven, bear,carry, gestate, expect ?
give birth, deliver, bear,birth, have.waiting: Sense 1: wait.
Sense 2: wait, holdoff, hold back ?
act, move.
Sense 3: wait, expect,look, await.
Sense 4: wait, waitress ?
work.We should note that WordNet contains onlyopen-class words, i.e., nouns, verbs, adjectives,and adverbs.
There are no entries for determin-ers, prepositions, pronouns, conjunctions, and par-ticles.
For better matching of these additional POStags, we manually created a different equivalenceclass for each POS tag that is not included in theWordNet so that words with the same POS tag canbe considered synonymous.4.2 Augmenting ReferencesAfter extracting the synonyms of each word in thegiven translations, the next step is to augment eachreference word with its synonyms.
To avoid over-generation of synonyms, we make the assumptionthat words wiand wjare synonyms of each otheronly if wiappears in the synonym list of wj, andwjappears in the synonym list of wi.
To makethe alignment task more efficient and faster, we re-strict the synonym list to only words that appearin the given translations.
In our running exam-ple, the augmented (extended) skeleton accordingto the second hypothesis is as follows:Extended skeleton: at the same time meantimeexpect waiting israel to abide by thedeadlines set established by .4.3 Modifications to TER ScriptThe final step is to modify TER script to favormatching of a word to its synonyms rather than toany other word.
To achieve this goal, we modi-fied the publicly available TER script, TERCOM(Snover et al, 2006), to match words in the sameequivalence class at an additional synonym cost.In its original implementation, TERCOM builds ahash table for the n-grams that appear in both thereference and the hypothesis translation to deter-mine possible shifts of words.
To allow synony-mous words to be shifted and aligned to each other,we extend the hash table for all possible synonymsof words in the skeleton.
Formally, if the skeletonincludes two consecutive words wisiand wjsj,where si(sj) is a synonym of wi(wj), we putall four possible combinations to the hash table:wiwj, wisj, siwj, and sisj.4To give higher priority to the exact matchingof words (which has zero cost during edit dis-tance computation), we used a slightly higher costfor synonym matching, a cost of 0.1.5All theother operations (i.e., insertion, deletion, substitu-tion and shifting of words) have a cost of 1.0.4.4 Two-pass Alignment StrategyWhen building a confusion network, the usualstrategy is first to align each hypothesis to theskeleton separately and reorder them so that theword ordering in the given hypothesis matches theword ordering in the skeleton translation.
Next aconfusion network is built between all these re-ordered hypotheses.One of the major problems with this process oc-curs when the hypotheses include additional wordsthat do not appear in the skeleton translation, asdepicted in Figure 2.
Since the alignments of twodifferent hypotheses are done independently, twohypotheses other than the skeleton may not alignperfectly, especially when the additional words ap-pear in different positions.To overcome this issue, we employ a two-passalignment strategy.
In the first pass, we align allhypotheses to the skeleton independently and builda confusion network.
Next an intermediate refer-ence sentence is created from the confusion net-work generated in the first pass.
To create this in-termediate reference, we find the best position foreach word that appears in the confusion network4Note that the hash table is built in an iterative fashion.We consider adding a new n-gram only if the previous n?
1words appear in the hypothesis as well.5Synonym matching cost was determined empirically, try-ing different costs from 0 to 0.5.36WITHOUT SYNONYM MATCHING and ONE-PASS ALIGNMENT:at the same time expect israel to abide byat the same time we expect israel to abide byat the same time , we are waiting for israel to abide byat the same time we expect israel to abide byat the same time , we expect israel to abide byat the same time , waiting for israel to comply within the meantime , waiting for israel to abide byWITH SYNONYM MATCHING and TWO-PASS ALIGNMENT:at the same time expect israel to abide byat the same time we expect israel to abide byat the same time , we are waiting for israel to abide byat the same time we expect israel to abide byat the same time , we expect israel to abide byat the same time , waiting for israel to comply within the meantime , waiting for israel to abide byFigure 3: Example alignment via confusion networks with and without synonym matching and two-passalignment (using the first sentence as the skeleton)using majority voting.
The second pass uses thisintermediate reference as the skeleton translationto generate the final confusion network.When we create the intermediate reference, thenumber of positions for a given word is boundedby the maximum number of occurrences of thesame word in any hypothesis.
It is possible thattwo different words are mapped to the same po-sition in the intermediate reference.
If this is thecase, these words are treated as synonyms whenbuilding the second confusion network, and the in-termediate reference looks like the extended refer-ence in Section 4.2.Finally, Figure 3 presents our running examplewith the old alignments versus the alignments withsynonym matching and two-pass alignment.4.5 FeaturesEach word in the confusion network is representedby system-specific word scores.
For computingscores, each hypothesis is assigned a score basedon three different methods:1.
Uniform weighting: Each hypothesis in then-best list has the same score of 1/n.2.
Rank-based weighting: Each hypothesis isassigned a score of 1/(1+r), where r is therank of the hypothesis.3.
TM-based weighting: Each hypothesis isweighted by the score that is assigned to thehypothesis by the translation model.The total score of an arc with word w for a givensystem S is the sum of all the scores of the hy-potheses in system S that contain the word w inthe given position.
The score for a specific arc be-tween nodes niand njis normalized by the sum ofthe scores for all the arcs between niand nj.Our experiments demonstrated that rank-basedweighting performs the best among all threeweighting methods although the differences aresmall.
In the rest of the paper, we report only theresults with rank-based weighting.Besides the arc scores, we employ the followingfeatures during decoding:1.
Skeleton selection features for each system,2.
NULL-word (or epsilon) insertion score,3.
Word penalty, and4.
Language model score.Skeleton selection feature is intended to helpchoose the best skeleton among the input systems.NULL-word feature controls the number of ep-silon arcs used in the chosen translation duringthe decoding and word penalty feature controlsthe length of the translation.
For language modelscores, we used a 4-gram LM that we used to trainthe input systems.5 Evaluation and ResultsIn this section, we describe how we train the inputsystems and how we evaluate the proposed systemcombination method.5.1 Systems and DataTo evaluate the impact of the new alignments,we tested our system combination approach usingthe old alignments and improved alignments ontwo language pairs: Arabic-English and Chinese-English.
We ran the system combination on threesystem outputs that were generated by an in-househierarchical phrase-based decoder, as in (Chiang,2007).
The major difference between the three sys-tems is that they were trained on different subsetsof the available training data using different wordalignments.For generating the system outputs, first a hier-archical phrase-based decoder was used to gener-37Data for Training/Tuning/TestingArabic-English Chinese-English# of segments # of tokens # of segments # of tokensTraining Data (System1) 14.8M 170M 9.1M 207MTraining Data (System2) 618K 8.1M 13.4M 199MTraining Data (System3) 2.4M 27.5M 13.9M 208MTuning Set (Input Systems) 1800 51K 1800 51KTuning Set (System Combination) 1259 37K 1785 55KTest Set - NIST MTEval?05 1056 32K 1082 32KTest Set - NIST MTEval?06 1797 45K 1664 41KTest Set - NIST MTEval?08 1360 43K 1357 34KTable 1: Number of segments and source-side words in the training and test data.ate three sets of unique 3000-best lists.
Nine fea-tures were used in the hierarchical phrase-basedsystems under the log-linear model framework: a4-gram language model (LM) score (trained onnearly 3.6 billion words using the SRILM toolkit(Stolcke, 2002)), conditional rule/phrase probabil-ities and lexical weights (in both directions), rulepenalty, phrase penalty, and word penalty.
Rulesand phrases were extracted in a similar manneras described in (Chiang, 2007) from the trainingdata with word alignments generated by GIZA++.The n-best lists were then re-scored with three ad-ditional LMs: a count-based LM built from theGoogle Tera word corpus, an almost parsing LMbased on super-ARV tagging, and an approximatedfull-parser LM (Wang et al, 2007).For Arabic-English, the first system was trainedon all available training data (see Table 1 for de-tails), with long sentences segmented into multiplesegments based on IBM model 1 probabilities (Xuet al, 2005).
The second system was trained on asmall subset of the training data, which is mostlynewswire.
The third system was trained on an au-tomatically extracted subset of the training data ac-cording to n-gram overlap in the test sets.For Chinese-English, the first system used allthe training data without any sentence segmenta-tion.
The second system used all training data af-ter IBM-1 based sentence segmentation, with dif-ferent weightings on different corpora.
The thirdsystem is the same as the second system exceptthat it used different word alignment symmetriza-tion heuristics (grow-diag-final-and vs. grow-diag-final (Koehn et al, 2003)).5.2 Empirical ResultsAll input systems were optimized on a ran-domly selected subset of the NIST MTEval?02,MTEval?03, and MTEval?04 test sets using min-System MT?05 MT?06 MT?08System 1 53.4 43.8 43.2System 2 53.9 46.0 42.8System 3 56.1 45.3 43.3No Syns, 1-pass 56.7 47.5 44.9w/Syns, 2-pass 57.9 48.4 46.2Table 2: Lowercase BLEU scores (in percentages)on Arabic NIST MTEval test sets.imum error rate training (MERT) (Och, 2003)to maximize BLEU score (Papineni et al, 2002).System combination was optimized on the rest ofthis data using MERT to maximize BLEU score.As inputs to the system combination, we used 10-best hypotheses from each of the re-ranked n-bestlists.
To optimize system combination, we gener-ated unique 1000-best lists from a lattice we cre-ated from the input hypotheses, and used MERT ina similar way to MT system optimization.We evaluated system combination with im-proved alignments on three different NISTMTEval test sets (MTEval?05, MTEval?06 NISTportion, and MTEval?08).
The final MT outputswere evaluated using lowercased BLEU scores.6Tables 2 and 3 present the BLEU scores (in per-centages) for the input systems and for differentcombination strategies on three test sets in Arabic-English and Chinese-English, respectively.On Arabic-English, the combination with syn-onym matching and two-pass alignment yields ab-solute improvements of 1.8 to 2.9 BLEU point onthree test sets over the best input system.
Whencompared to the combination algorithm with theold alignments (i.e., 1-pass alignment with no syn-onymmatching), the improved alignments yield anadditional improvement of 0.9 to 1.3 BLEU point6We used the NIST script (version 11b) for BLEU with itsdefault settings: case-insensitive matching of up to 4-grams,and the shortest reference sentence for the brevity penalty.38System MT?05 MT?06 MT?08System 1 35.8 34.3 27.6System 2 35.9 34.2 27.8System 3 36.0 34.3 27.8No Syns, 1-pass 38.1 36.5 27.9w/Syns, 2-pass 38.6 37.0 28.3No Syns, 1-pass, tuning set w/webtext 28.4w/Syns, 2-pass, tuning set w/webtext 29.3Table 3: Lowercase BLEU scores (in percentages)on Chinese NIST MTEval test sets.on the three test sets.For Chinese-English, the improvements over theprevious combination algorithm are smaller.
Thenew combination system yields up to an absolute2.6 BLEU point improvement over the best inputsystem and up to 0.5 BLEU point improvementover the previous combination algorithm on threedifferent test sets.
Note that for Arabic-English,the individual systems show a high variance intranslation quality when compared to Chinese-English systems.
This might explain why the im-provements on Chinese-English are modest whencompared to Arabic-English results.We also noticed that system combinationyielded much smaller improvement on ChineseMTEval?08 data when compared to other testsets, regardless of the alignment method (only 0.5BLEU point over the best input system).
Wesuspected that this might happen because of amismatch between the genres of the test set andthe tuning set (the amount of web text data inMTEval?08 test set is high although the tuning setdoes not include any web text data).
To test thishypothesis, we created a new tuning set for systemcombination, which consists of 2000 randomly se-lected sentences from the previous MTEval testsets and includes web text data.
Using this newtuning set, combination with the improved align-ments yields a BLEU score of 29.3 on MTEval?08data (an absolute improvement of 1.5 BLEU pointover the best input system, and 0.9 BLEU pointimprovement over the combination with the oldalignments).
These new results again validate theusefulness of the improved alignments when thetuning set matches the genre of the test set.5.3 A Comparison of the Impact of SynonymMatching and Two-pass AlignmentOne last evaluation investigated the impact of eachcomponent on the overall improvement.
For thisSynon.
2-pass MT?05 MT?06 MT?08No No 56.7 47.5 44.9Yes No 57.3 47.8 45.2No Yes 57.7 48.0 45.9Yes Yes 57.9 48.4 46.2Table 4: Comparison of Synonym Matching andTwo-pass Alignment on Arabic-Englishpurpose, we ran system combination by turning onand off each component.
Table 4 presents the sys-tem combination results in terms of BLEU scoreson Arabic-English test sets when each componentis used on its own or when they are used together.The results indicate that synonym matching onits own yields improvements of 0.3-0.6 BLEUpoints over not using synonym matching.
Two-pass alignment turns out to be more useful thansynonym matching, yielding an absolute improve-ment of up to 1 BLEU point over one-pass align-ment.6 ConclusionsWe presented an extension to the previous align-ment approaches to handle word synonyms moreeffectively in an attempt to improve the align-ments between different hypotheses during confu-sion network decoding.
We also presented a two-pass alignment strategy for a better alignment ofhypotheses with similar words but with a differentword ordering.We evaluated our system combination ap-proach on two language pairs: Arabic-Englishand Chinese-English.
Combination with improvedalignments yielded up to an absolute 2.9 BLEUpoint improvement over the best input system andup to an absolute 1.3 BLEU point improvementover combination with the old alignments.
It isworth noting that these improvements are obtainedusing very similar input systems.
We expect thatthe improvements will be higher when we usestructurally different MT systems as inputs to thecombiner.Our future work includes a more effective useof existing linguistic resources to handle alignmentof one word to multiple words (e.g., al-nahayanvs.
al nahyan, and threaten vs. pose threat)and matching of similar (but not necessarily syn-onymous) words (polls vs. elections).
We arealso planning to extend word lattices to includephrases from the individual systems (i.e., not justthe words) for more grammatical outputs.39Acknowledgments This material is based upon worksupported by the Defense Advanced Research ProjectsAgency (DARPA) under Contract No.
HR0011-06-C-0023.ReferencesAkiba, Yasuhiro, Taro Watanabe, and Eiichiro Sumita.2002.
Using language and translation models to se-lect the best among outputs from multiple MT sys-tems.
In Proc.
of the 19th Intl.
Conf.
on Computa-tional Linguistics (COLING?2002).Bangalore, Srinivas, German Bordel, and GiuseppeRiccardi.
2001.
Computing consensus translationfrom multiple machine translation systems.
In Proc.of IEEE Automatic Speech Recognition and Under-standing Workshop (ASRU?2001).Chiang, David.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Fellbaum, Christiane.
1998.
WordNet: An ElectronicLexical Database.
Bradford Books, March.
Avail-able at http://wordnet.princeton.edu.Fiscus, Jonathan G. 1997.
A post-processing systemto yield reduced word error rates: Recognizer outputvoting error reduction (ROVER).
In Proc.
of IEEEAutomatic Speech Recognition and UnderstandingWorkshop (ASRU?1997).Frederking, Robert and Sergei Nirenburg.
1994.Three heads are better than one.
In Proc.
of the4th Conf.
on Applied Natural Language Processing(ANLP?1994).Huang, Fei and Kishore Papineni.
2007.
Hierarchi-cal system combination for machine translation.
InProc.
of the Conf.
on Empirical Methods in NaturalLanguage Processing (EMNLP?2007).Jayaraman, Shyamsundar and Alon Lavie.
2005.Multi-engine machine translation guided by explicitword matching.
In Proc.
of the 10th Annual Conf.
ofthe European Association for Machine Translation(EAMT?2005).Koehn, Philipp, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proc.
of theHuman Language Technology and the Meeting of theNorth American Chapter of the Association for Com-putational Linguistics Conf.
(HLT/NAACL?2003).Matusov, Evgeny, Nicola Ueffing, and Hermann Ney.2006.
Computing consensus translation from multi-ple machine translation systems using enhanced hy-potheses alignment.
In Proc.
of the 11th Conf.
of theEuropean Chapter of the Association for Computa-tional Linguistics (EACL?2006).Mellebeek, Bart, Karolina Owczarzak, Josef Van Gen-abith, and Andy Way.
2006.
Multi-engine machinetranslation by recursive sentence decomposition.
InProc.
of the 7th Conf.
of the Association for MachineTranslation in the Americas (AMTA?2006).Nomoto, Tadashi.
2004.
Multi-engine machine trans-lation with voted language model.
In Proc.
of the42nd Annual Meeting of the Association for Compu-tational Linguistics (ACL?04).Och, Franz J. and Hermann Ney.
2000.
Improved sta-tistical alignment models.
In Proc.
of the 38th An-nual Meeting of the Association for ComputationalLinguistics (ACL?2000).Och, Franz J.
2003.
Minimum error rate training in sta-tistical machine translation.
In Proc.
of the 41st An-nual Meeting of the Association for ComputationalLinguistics (ACL?2003).Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proc.
of the40th Annual Meeting of the Association for Compu-tational Linguistics (ACL?2002).Rosti, Antti-Veikko, Necip Fazil Ayan, Bing Xiang,Spyros Matsoukas, Richard Schwartz, and BonnieDorr.
2007a.
Combining outputs from multiple ma-chine translation systems.
In Proc.
of the HumanLanguage Technology and the Meeting of the NorthAmerican Chapter of the Association for Computa-tional Linguistics Conf.
(HLT/NAACL?2007).Rosti, Antti-Veikko, Spyros Matsoukas, and RichardSchwartz.
2007b.
Improved word-level system com-bination for machine translation.
In Proc.
of the45th Annual Meeting of the Association for Compu-tational Linguistics (ACL?2007).Sim, Khe Chai, William J. Byrne, Mark J.F.
Gales,Hichem Sahbi, and Phil C. Woodland.
2007.Consensus network decoding for statistical machinetranslation system combination.
In Proc.
of the 32ndIntl.
Conf.
on Acoustics, Speech, and Signal Process-ing (ICASSP?2007).Snover, Matthew, Bonnie Dorr, Rich Schwartz, LinneaMicciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proc.
of the 7th Conf.
of the Association for Ma-chine Translation in the Americas (AMTA?2006).Stolcke, Andreas.
2002.
SRILM ?
an extensible lan-guage modeling toolkit.
In Proc.
of the Intl.
Conf.
onSpoken Language Processing (ICSLP?2002).Wang, Wen, Andreas Stolcke, and Jing Zheng.
2007.Reranking machine translation hypotheses withstructured and web-based language models.
In Proc.of the IEEE Automatic Speech Recognition and Un-derstanding Workshop (ASRU?2007).Xu, Jia, Richard Zens, and Hermann Ney.
2005.Sentence segmentation using IBM word alignmentmodel 1.
In Proc.
of the 10th Annual Conf.
ofthe European Association for Machine Translation(EAMT?2005).40
