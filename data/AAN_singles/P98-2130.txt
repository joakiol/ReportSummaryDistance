Formal aspects and parsing issues of dependency theoryVincenzo Lombardo and Leonardo LesmoDipartimento di Informatica and Centro di Scienza CognitivaUniversita' di Torinoc.so Svizzera 185 - 10149 Torino - Italy{vincenzo, lesmo}@di.unito.itAbstractThe paper investigates the problem of providing aformal device for the dependency approach tosyntax, and to link it with a parsing model.
Afterreviewing the basic tenets of the paradigm and thefew existing mathematical results, we describe adependency formalism which is able to deal withlong-distance dependencies.
Finally, we present anEarley-style parser for the formalism and discuss the(polynomial) complexity results.1.
IntroductionMany authors have developed dependencytheories that cover cross-linguistically the mostsignificant phenomena of natural language syntax:the approaches range from generative formalisms(Sgall et al 1986), to lexically-based descriptions(Mel'cuk 1988), to hierarchical organizations oflinguistic knowledge (Hudson 1990) (Fraser,Hudson 1992), to constrained categorial grammars(Milward 1994).
Also, a number of parsers havebeen developed for some dependency frameworks(Covington 1990) (Kwon, Yoon 1991) (Sleator,Temperley 1993) (Hahn et al 1994) (Lombardo,Lesmo 1996), including a stochastic treatment(Eisner 1996) and an object-oriented parallelparsing method (Neuhaus, Hahn 1996).However, dependency theories have never beenexplicitly linked to formal models.
Parsers andapplications usually refer to grammars built arounda core of dependency oncepts, but there is a greatvariety in the description of syntactic onstraints,from rules that are very similar to CFG productions(Gaifman 1965) to individual binary relations onwords or syntactic ategories (Covington 1990)(Sleator, Temperley 1993).knowS UB/  SCOMPI ~ l ikesSUB?
~OBJJohn beansFigure 1.
A dependency tree for the sentence "I knowJohn likes beans".
The leftward or rightward orientationof the edges represents the order constraints: thedependents that precede (respectively, follow) the headstand on its left (resp.
right).The basic idea of dependency is that the syntacticstructure of a sentence is described in terms ofbinary relations (dependency relations) on pairs ofwords, a head (parent), and a dependent (daughter),respectively; these relations usually form a tree, thedependency tree (fig.
1).The linguistic merits of dependency s ntax havebeen widely debated (e.g.
(Hudson 1990)).Dependency syntax is attractive because of theimmediate mapping of dependency trees on thepredicate-arguments structure and because of thetreatment of free-word order constructs (Sgall et al1986) (Mercuk 1988).
Desirable properties oflexicalized formalisms (Schabes 1990), like finiteambiguity and decidability of string acceptance,intuitively hold for dependency s ntax.On the contrary, the formal studies ondependency theories are rare in the literature.Gaifman (1965) showed that projectivedependency grammars, expressed by dependencyrules on syntactic ategories, are weakly equivalentto context-free grammars.
And, in fact, it ispossible to devise O(n 3) parsers for this formalism(Lombardo, Lesmo 1996), or other projectivevariations (Milward 1994) (Eisner 1996).
On thecontrolled relaxation of projective constraints, Nasr(1995) has introduced the condition of pseudo?projectivity, which provides ome controlled looserconstraints on arc crossing in a dependency tree,and has developed a polynomial parser based on agraph-structured stack.
Neuhaus and Broker (1997)have recently showed that the general recognitionproblem for non-projective dependency grammars(what hey call discontinuous DG) is NP-complete.They have devised a discontinuous DG withexclusively lexical categories (no traces, as mostdependency theories do), and dealing with freeword order constructs through a looser subtreeordering.
This formalism, considered as the moststraightforward extension to a projectiveformalism, permits the reduction of the vertexcover problem to the dependency recognitionproblem, thus yielding the NP-completeness result.However, even if banned from the dependencyliterature, the use of non lexical categories i only anotational variant of some graph structures alreadypresent in some formalisms (see, e.g., WordGrammar (Hudson 1990)).
This paper introduces alexicalized dependency formalism, which deals787with long distance dependencies, and a polynomialparsing algorithm.
The formalism is projective, andcopes with long-distance dependency phenomenathrough the introduction of non lexical categories.The non lexical categories allow us to keepinalterate the condition of projectivity, encoded inthe notion of derivation.
The core of the grammarrelies on predicate-argument structures associatedwith lexical items, where the head is a word anddependents are categories linked by edges labelledwith dependency relations.
Free word orderconstructs are dealt w i th  by constrainingdisplacements via a set data structure in thederivation relation.
The introduction of non lexicalcategories also permits the resolution of  theinconsistencies pointed out by Neuhaus and Brokerin Word Grammar (1997).The parser is an Earley type parser with apolynomial  complexity,  that encodes thedependency trees associated with a sentence.The paper is organized as follows.
The nextsection presents a formal dependency s stem thatdescribes the linguistic knowledge.
Section 3presents an Earley-type parser: we illustrate thealgorithm, trace an example, and discuss thecomplexity results.
Section 4 concludes the paper.2.
A dependency formalismThe basic idea of dependency is that thesyntactic structure of a sentence is described interms of binary relations (dependency relations) onpairs of words, a head (or parent), and a dependent(daughter), respectively; these relations form a tree,the dependency tree.
In this section we introduce aformal dependency system.
The formalism isexpressed via dependency rules which describe onelevel of a dependency tree.
Then, we introduce anotion of derivation that allows us to define thelanguage generated by a dependency grammar ofthis form.The grammar and the lexicon coincide, since therules are lexicalized: the head of the rule is a wordof a certain category, i.e.
the lexical anchor.
Fromthe linguistic point of view we can recognize twotypes of dependency rules: primitive dependencyrules, which represent subcategorization frames,and non-primitive dependency rules, which resultfrom the application of lexical metarules toprimitive and non-primitive dependency rules.Lexical metarules (not dealt with in this paper)obey general principles of linguistic theories.A dependency grammar is a six-tuple <W, C, S,D, I, H>, whereW is a finite set of symbols (words of a naturallanguage);C is a set of syntactic ategories (among which thespecial category E);S is a non-empty set of root categories (C ~ S);D is the set of dependency relations, e.g.
SUB J,OBJ, XCOMP, P-OBJ, PRED (among which thespecial relation VISITOR1);I is a finite set of symbols (among which thespecial symbol 0), called u-indices;H is a set of dependency rules of the formx:X (<rlYlUl'Cl> ... <ri-1Yi-lUi-l'Ci-1 > #<a'i+lYi+lUi+fl;i+l> ... <rmYmum'~m>)1) xe W, is the head of the rule;2) Xe C, is its syntactic ategory;3) an element <r i Yi ui xj> is a d-quadruple(which describbs a-de-pefident); he sequenceof d-quads, including the symbol #(representing the linear position of the head,# is a special symbol), is called the d-quadsequence.
We have that3a) r ieD, j e {1 ..... i-l, i+l ..... m};3b) Yje C,j e {1 ..... i-l, i+l ..... m};3c)ujeI ,  j e {1 ..... i-l, i+l ..... m};3d) x\] is a (possibly empty) set of triples <u,r, Y>, called u-triples, where ue I, re D,YeC.Finally, it holds that:I) For each ue I that appears in a u-triple <u, r,Y>~Uj, there exists exact ly  one d-quad<riYiu:\]xi> in the same rule such that u=ui, i ~j.II) For each u=ui of a d-quad <riYiuixi>, thereexists exactly one u-triple <u, r, Y>e x j, i~j, inthe same rule.Intuitively, a dependency rule constrains one node(head) and its dependents in a dependency tree: thed-quad sequence states the order of elements, boththe head (# position) and the dependents (d-quads).The grammar is lexicalized, because eachdependency rule has a lexical anchor in its head(x:X).
A d-quad <rjYjujxj> identifies a dependentof category Yi, co-rm-ect6d with the head via adependency relation ri.
Each element of the d-quadsequence is possibly fissociated with a u-index (uj)and a set of u-triples (x j).
Both uj and xjcan be millelements, i.e.
0 and ~,  respectively.
A u-triple (x-component of the d-quad) <u, R, Y> bounds thearea of the dependency tree where the trace can belocated.
Given the constraints I and II, there is aone-to-one correspondence b tween the u-indicesand the u-triples of the d-quads.
Given that adependency rule constrains one head and its directdependents in the dependency tree, we have thatthe dependent indexed by uk is coindexed with aThe relation VISITOR (Hudson 1990) accounts fordisplaced elements and, differently from the otherrelations, is not semantically interpreted.788trace node in the subtree rooted by the dependentcontaining the u-triple <Uk, R, Y>.Now we introduce a notion of derivation for thisformalism.
As one dependency rule can be usedmore than once in a derivation process, it isnecessary to replace the u-indices with uniquesymbols (progressive integers) before the actualuse.
The replacement must be consistent in the uand the x components.
When all the indices in therule are replaced, we say that the dependency rule(as well as the u-triple) is instantiated.A triple consisting of a word w (a W) or the tracesymbol e(~W) and two integers g and v is a wordobject of the grammar.Given a grammar G, the set of word objects of G isWx(G)={ ~tXv / IX, v_>0, xe Wu {e} }.A pair consisting of a category X (e C) and a stringof instantiated u-triples T is a category object of thegrammar (X(T)).A 4-tuple consisting of a dependency relation r(e D), a category object X(yl), an integer k, a set ofinstantiated u-triples T2 is a derivation object of thegrammar.
Given a grammar G, the set of derivationobjects of G isCx(G) = {<r,Y(y1),u,y2> /re D, Ye C, u is an integer,)'1 ,T2 are strings of instantiated u-triples }.Let a,~e Wx(G)* and Ve (Wx(G) u Cx(G) )*.
Thederivation relation holds as follows:1)a <R,X(Tp),U, Tx> ~ :=~ ?I<rl,Y l(Pl),U 1,'~1><:r2,Y2(P2),u 2,'~2>.
.
.<ri-1 ,Y i-1 (Pi-1),u i-1 ,'q-1 >uX0<ri+l,Yi+l(Pi+l),Ui+l,'Ci+l >.
, ,<rm,Ym(Pm),U m,'Cm>where x:X (<rlYlUl'Cl> ... <ri-lYi-lUi-lZi-l> #<ri+lYi+lUi+l'~i+l> ... <rmY mUm'Cm >) is adependency rule, and Pl u ... u Pm--'q'p u Tx.2)ct <r,X( <j,r,X>),u,O> =, a uejWe define =~* as the reflexive, transitive closureof ~ .Given a grammar G, L'(G) is the language ofsequences of word objects:L'(G)={ae Wx(G)* /<TOP, Q(i~), 0, 0> :=>* (x and Qe S(G)}where TOP is a dummy dependency relation.
Thelanguage generated by the grammar G, L(G), isdefined through the function t:L(G)={we Wx(G)* / w=t(ix) and oce L'(G)},where t is defined recursively ast(-)  = -;( tWv a) = w t(at(~tev a) = t(a).where - is me empty sequence.As an example, consider the grammarGI=<W(G1) = {I, John, beans, know, likes}C(G 1) = { V, V+EX, N }S(GO = {v, V+EX}D(G 1) = {SUB J, OBJ, VISITOR, TOP}1((31) = {0, Ul}T(G1) >,where T(G1) includes the following dependencyrules:1.
I: N (#);2.
John: N (#);3. beans: N (#);4. likes: V (<SUBJ, N, 0, 0># <OBJ, N, 0, 0)>);5. knOW: V+EX (<VISITOR, N, ul, 0><SUB\]', N, 0, 0>#<SCOMP, V, 0, {<ul,OBJ, N>}>).A derivation for the sentence "Beans I know Johnlikes" is the following:<TOP, V+EX(O), 0, 0> ::~<VISITOR, N(IEl), I, 0> <SUBJ, N(~), 0, 0> know<SCOMP, V(~), 0, {<I,OBJ,N>}> :=~lbeans <SUBJ, N(O), 0, 0> know<SCOMP, V (O), 0, {<I,OBJ,_N'>}>lbeans I know <SCOMP, V(O), 0, {<I,OBJ,N>}> =:~lbeans I know <SUBJ, N(O), 0, O>likes<OBJ, N(<I,OBJ,N>), 0, 0> =:~lbeans I know John likes<OBJ, N(<I,OBJ,N>), 0, 0>1beans I know John likes elThe dependency tree corresponding to thisderivation is in fig.
2.3.
Parsing issuesIn this section we describe an Earley-style parserfor the formalism in section 2.
The parser is an off-line algorithm: the first step scans the inputsentence to select he appropriate dependency rulesknowVISITO SUB" SCOMPJohn E1Figure 2.
Dependency tree of the sentence "Beans Iknow John likes", given the grammar G1.789from the grammar.
The selection is carried out bymatching the head of the rules with the words ofthe sentence.
The second step follows Earley'sphases on the dependency rules, together with thetreatment of u-indices and u-triples.
This off-linetechnique is not uncommon in lexicalizedgrammars, since each Earley's prediction wouldwaste much computation time (a grammar factor)in the body of the algorithm, because dependencyrules do not abstract over categories (cf.
(Schabes1990)).In order to recognize asentence of n words, n+lsets Si of items are built.
An item represents asubtree of the total dependency tree associated withthe sentence.
An item is a 5-tuple <Dotted-rule,Position, g-index, v-index, T-stack>.
Dotted- ru le  isa dependency rule with a dot between two d-quadsof the d-quad sequence.
Pos i t ion  is the inputposition where the parsing of the subtreerepresented by this item began (the leftmostposition on the spanned string).
It-index and v-index are two integers that correspond to theindices of a word object in a derivation.
T-s tack  isa stack of sets of u-triples to be satisfied yet: thesets of u-triples (including empty sets, whenapplicable) provided by the various items arestacked in order to verify the consumption of oneu-triple in the appropriate subtree (cf.
the notion ofderivation above).
Each time the parser predicts adependent, the set of u-triples associated with it ispushed onto the stack.
In order for an item to enterthe completer phase, the top of T-s tack  must be theempty set, that means that all the u-triplesassociated with that item have been satisfied.
Thesets of u-triples in T-stack are always inserted atthe top, after having checked that each u-triple isnot already present in T-stack (the check neglectsthe u-index).
In case a u-triple is present, thedeeper u-triple is deleted, and the T-stack will onlycontain the u-triple in the top set (see the derivationrelation above).
When satisfying a u-triple, the T-stack is treated as a set data structure, since theformalism does not pose any constraint on theorder of consumption of the u-triples.Following Earley's style, the general idea is tomove the dot rightward, while predicting thedependents of the head of the rule.
The dot canadvance across a d-quadruple <rjYiui'q> or acrossthe special symbol #.
The d-q-ua-d-irhmediatelyfollowing the dot can be indexed uj.
This isacknowledged by predicting an item (representingthe subtree rooted by that dependent), and insertinga new progressive integer in the fourth componentof the item (v-index).
xj is pushed onto T-stack: thesubstructure ooted by a node of category Yi mustcontain the trace nodes of the type licensed by theu-triples.
The prediction of a trace occurs as in thecase 2) of the derivation process.
When an item Pcontains a dotted rule with the dot at its end and aT-stack with the empty set @ as the top symbol, theparser looks for the items that can advance the dot,.given the completion of the dotted ependency rulem P. Here is the algorithm.Sentence: w0 w 1 ... wn-1Grammar G=<W,C,S,D,I,H>initializationfor each x:Q(5)e H(G), where Qe S(G)replace ach u-index in 8 with a progressive integer;INSERT <x:Q(* 8), 0, 0, 0, \[\]> into S Obodyfor each set S i (O~_i.gn) dofor each P--<y: Y('q ?
5), j, Ix, v, T-stack> in S i---> completer (including pseudocompleter).if 5 is the empty sequence and TOP(T-stack)=Ofor each <x: X(Tt ?
<R, Y, u x, Xx> 4),j~, Ix', v', T-stack'> in SjT-stack" <- POP(T-stack);INSERT <x: X(~ <IL Y, Ux, Xx> ?
Q,j', Ix', v', T-stack"> into S i;---> predictor:if 5 = <R & Z & u& "c5> I1 thenfor each rule z: Z8(0)replace ach u-index in 0 with a prog.
integer;T-stack' <- PUSH-UNION(x& T-stack);INSERT <z: Z5(, 0), i, 0, u& T-stack'> into Si;.
.
.
.
.
.
.
.
.
.
.
> pseudopredictor:if <u, R & Z 8> e UNION (set i I set i in T-stack);DELETE <u, R& Z&> from T-stack;T-stack' <- PUSH(Q, T-stack);INSERT<e: ZS(.
#), i, u. uS.
T-stack'> into Si;---> scanner  :if~=# Tl thenify=w iINSERT <y: Y(7 # ""q),j, Ix, v, T-stack> into Si+l........... > pseudoscanner  :elseify=EINSERT < E: Y(# "), j, IX, v, T-stack> into Si;end forend for,terminationif<x: Q(ct ,).
0. is.
v. \[\]> e s n, where Qa S(G)then accept else reject endif.At the beginning ( in i t ia l i za t ion) ,  the parserinitializes the set So, by inserting all the dottedrules (x:Q(8)e H(G)) that have a head of a rootcategory (Qe S(G)).
The dot precedes the whole d-quad sequence ($).
Each u-index of the rule isreplaced by a progressive integer, in both the u and790the % components of the d-quads.
Both IX and v-indices are null (0), and T-stack is empty (\[\]).The body consists of an external loop on thesets Si (0 < i < n) and an inner loop on the singleitems of the set Si.
LetP = <y: Y(~ * 8),j, IX, v, T-stack>be a generic item.
Following Eafley's schema, theparser invokes three phases on the item P:completer, predictor and scanner.
Because of thederivation of traces (8) from the u-triples in T-stack, we need to add some code (the so-calledpseudo-phases) that deals with completion,prediction and scanning of these ntities.Completer: When 8 is an empty sequence (all thed-quad sequence has been traversed) and the topof T-stack is the empty set O (all the triplesconcerning this item have been satisfied), thedotted rule has been completely analyzed.
Thecompleter looks for the items in S i which werewaiting for completion (return items; j is theretum Position of the item P).
The return itemsmust contain a dotted rule where the dotimmediately precedes a d-quad <R,Y,ux,Xx>,where Y is the head category of the dotted rule inthe item P. Their generic form is <x: X(X *<R,Y,ux,Xx> 4), J', IX', v', T-stack'>.
These itemsfrom Sj are inserted into Si after havingadvanced the dot to the right of <R,Y,ux,%x>.Before inserting the items, we need updating theT-stack component, because some u-triples couldhave been satisfied (and, then, deleted from theT-stack).
The new T-stack" is the T-stack of thecompleted item after popping the top element E~.Predictor: If the dotted rule of P has a d-quad<Rs,Z&us,xS> immediately after the dot, then theparser is expecting a subtree headed by a word ofcategory ZS.
This expectation is encoded byinserting a new item (predicted item) in the set,for each rule associated with 28 (of the formz:Zs(0)).
Again, each u-index of the new item (d-quad sequence 0) is replaced by a progressiveinteger.
The v-index component of the predicteditem is set to us.
Finally, the parser prepares thenew T-stack', by pushing the new u-triplesintroduced by %8, that are to be satisfied by theitems predicted after the current item.
Thisoperation is accomplished by the primitivePUSH-UNION, which also accounts for the nonrepetition of u-triples in T-stack.
As stated in thederivation relation through the UNIONoperation, there cannot be two u-triples with thesame relation and syntactic ategory in T-stack.In case of a repetition of a u-triple, PUSH deletesthe old u-triple and inserts the new one (with thesame u-index) in the topmost set.
Finally,INSERT joins the new item to the set Si.The pseudopredictor accounts for thesatisfaction of the the u-triples when theappropriate conditions hold.
The current d-quadin P, <Rs,Zs,us, xS>, can be the dependent whichsatisfies the u-triple <u,Rs,Zs> in T-stack (theUNION operation gathers all the u-triplesscattered through the T-stack): in addition toupdating T-stack (PUSH(O,T-stack)) andinserting the u-index u8 in the v component asusual, the parser also inserts the u-index u in theIX component to coindex the appropriate distantelement.
Then it inserts an item (trace item) witha fictitious dotted ependency rule for the trace.Scanner: When the dot precedes the symbol #, theparser can scan the current input word wi (if y,the head of the item P, is equal to it), orpseudoscan a trace item, respectively.
The resultis the insertion of a new item in the subsequentset (Si+l) or the same set (Si), respectively.At the end of the external loop (termination), thesentence is accepted if an item of a root category Qwith a dotted rule completely recognized, spanningthe whole sentence (Position=0), an empty T-stackmust be in the set Sn.3.1.
An exampleIn this section, we trace the parsing of thesentence "Beans I know John likes".
In thisexample we neglect he problem of subject-verbagreement: i  can be coded by inserting the AGRfeatures in the category label (in a similar way tothe +EX feature in the grammar G1); the commentson the right help to follow the events; the separatorsymbol Ihelps to keep trace of the sets in the stack;finally, we have left in plain text the d-quadsequence of the dotted rules; the other componentsof the items appear in boldface.S0<know: V.F.,X (* <VISITOR.
N, 1, 6><SUB J, N, 0, 6>#<SCOMP, V, 0, <1, OBJ, N>>),O, O, O,H><likes: V (* <SUBJ, N, O, 6>#<OBJ, V, O, 0>),0, 0, 0,H><beans: N (* #), 0, 0, 1,\[O\]><I:N (* #),0,0, 1, \[O\]><John: N (* #), 0, 0, 1, \[0\]><beans: N (* #), 0, 0, 0, \[0\]><I:N (* #),0,0,0, \[~\]><John: N (* # ), 0, 0, 0, \[0 l>(initialization)(initialization)(predictor "know" )(predictor "know" )(predictor "know" )(predictor "likes" )(predictor "likes" )(predictor "likes" )Sl \[beans\]<beans: N (# o), O, O, 1, \ [0l> Ocarmer)791<beans: N (# *), 0, 0, 0, \[@\]> OcanneO<know: V+EX (<VISITOR, N, 1, Q~> (completer "beans")* <SUBJ, N, O, 9>#<SCOMP, V, 0, <1, OBL N>>),0, 0, 0, ~><likes: V (<SUB J, N, 0, 9> (completer "beans")*#<OBJ, V, 0, Q)>),0, 0, 0, \[\]><beans: N (* # ), 1, 0, 0,\[O\]> (predictor "know" )<I: N (* # ), 1, 0, 0, \[?~\]> (predictor "know")<John: N (* # ), 1, 0, 0, \[~\]> (predictor "know" )$2 \[I\]<I: N (# *), 1, 0, 0, \[~10\]> (scanner)<know: V+v.x (<VISITOR, N, 1, O> (completer"I")<SUB J, N, 0, Q~>*#<SCOMP0 V, 0, <1, OBJ, N>>),0, 0, 0, H>$3 \[know\]<know: V +EX (<VISITOR, N, 1 .9> (scanner)<SUB J, N, 0, ~>#?
<SCOMP, V, 0, <1, OBJ, N>>),0, 0, 0, H><likes: V ( ?
<SUB J, N, 0.
9> (predictor "know" )#<OBJ, V, 0, ~>),3, 0, 0, \[ {<1, OBJ, N>}\]><beans: N (?
#), 3, 0, 2,\[{<1, OBJ, N>}I~\]> (p. "know" )<I: N (?
#),3, 0, 2, \[{<1, OBJ, N>}I~\]> (p. "know")<John: N (* # ), 3, 0, 2, \[{<1, OBJ, N>} 1~\]> (p. "know")<beans: N (?
# ), 3, 0, 0, \[{<1, OBJ, N>} I~\]> (p. "likes" )<I:N (* #), 3, 0, 0, \[{<1, OBJ, N>}I~\]> (p. "likes" )<John: N (* # ), 3, 0, 0, \[{<1, OBJ, N>} I~l> (p. "likes" )$4 \[John\]<John: N (# *), 3, 0, 2, \[ {<1, OBJ, N>} 19\]> (scanner)<John: N (# *), 3, 0, 0, \[ {<1, OBJ, N>} I~\]> (scanner)<know: V +EX (<VISITOR, N, 2, 9> (completer "John")* <SUBJ, N, 0, 9>#<SCOMP, V, 0, <2, OBJ, N>>),3, 0, 0, \[{<1, OBJ, N>}\]><likes: V (<SUB J, N, 0, ~> (completer "John")*#<OBJ, V, 0, ~>),3, 0, 0, \[ {<1, OBJ, N>}\]>$5 \[likes\]<likes: V (<SUB J, N, 0, 9> (scanner)#?
<OBL N, 0, ~>) ,0, 0, 0, \[{<1, OBJ, N>}\]><beans: N (* #), 5, 0, 0, \[{<1, OBJ, N>} I~\]><I: N (?
#), 5, 0, 0, \[{<1, OBJ, N>}I~\]><John: N ( ?
# ), 5, 0, 0, \[{<1, OBJ, N>} 1~\]><~: N (?
#), 5, 0, 0, \[O\]><E: N (# *), 5, 0, 0, \[~\]><likes: V (<SUB J, N, 0, Q~>#<OBJ, N, 0, O>* ),3, 0, 0, \[ ?~'\]><know: V+EX (<VISITOR.
N, 1, O><SUB J, N, 0, ~>#<SCOMP, V, 0, <I, OBJ.
N>> ?
),0, 0, 0, \[\]>(13.
"likes" )(p. "likes" )(p. "likes" )(pseudopredietor)(pseudopredictor)(completer)(completer)3.2.
Complexity resultsThe parser has a polynomial complexity.
Thespace complexity of the parser, i.e.
the number ofitems, is O(n 3+ aDllCl).
Each item is a 5-tuple<Dotted-rule, Position, Ix-index, v-index, T-stack>:Dotted rules are in a number which is a constant ofthe grammar, but in off-line parsing this number isbounded by O(n).
Position is bounded by O(n).
Ix-index and v-index are two integers that keep traceof u-triple satisfaction, and do not add an owncontribution to the complexity count.
T-stack has anumber of elements which depends on themaximum length of the chain of predictions.
Sincethe number of rules is O(n), the size of the stack isO(n).
The elements of T-stack contain all the u-triples introduced up to an item and which are tobe satisfied (deleted) yet.
A u-triple is of the form<u,R,Y>: u is an integer that is ininfluent, Ra D,Ya C. Because of the PUSH-UNION operation onT-stack, the number of possible u-triples catteredthroughout the elements of T-stack is IDIICI.
Thenumber of different stacks is given by thedispositions of IDllCI u-triples on O(n) elements;so, O(nlOllCl).
Then, the number of items in a set ofitems is bounded by O(n 2+ IDIICI) and there are nsets of items (O(n 3+ IDI ICI)).The time complexity of the parser is O(n 7+3 IDIICI).
Each of the three phases executes an INSERTof an item in a set.
The cost of  the INSERToperation depends on the implementation f the setdata structure; we assume it to be linear (O(n 2+IDIICI)) to make easy calculations.
The phasecompleter xecutes at most O(n 2+ IDIICI)) actionsper each pair of items (two for-loops).
The pairs ofitems are O(n 6+2 IDI ICI).
But to execute the actionof the completer, one of the sets must have theindex equal to one of the positions, so O(n 5 + 21DIICI).
Thus, the completer costs O(n 7+3 ID\[ ICI).
The792phase predictor, executes O(n) actions for eachitem to introduce the predictions ("for each rule"loop); then, the loop of the pseudopredictor isO(IDIICI) (UNION+DELETE), a grammar factor.Finally it inserts the new item in the set (O(n 2+IDIICI)).
The total number of items is O(n 3+ tDJ )o)and, so, the cost of the predictor O(n (i + 21DI Cl).The phase scanner executes the INSERT operationper item, and the items are at most O(n 3+ IDI ICI).THUS, the scanner costs O(n s+2 IDI I?1).
The totalcomplexity of the algorithm is O(n 7+3 IDttCt).We are conscious that the (grammar dependent)exponent can be very high, but the treatment of theset data structure for the u-triples requiresexpensive operations (cf.
a stack).
Actually thisformalism is able to deal a high degree of freeword order (for a comparable r sult, see (Becker,Rambow 1995)).
Also, the complexity factor dueto the cardinalities of the sets D and C is greatlyreduced if we consider that linguistic constraintsrestrict he displacement of several categories andrelations.
A better estimation of complexity canonly be done when we consider empirically theimpact of the linguistic constraints in writing awide coverage grammar.4.
ConclusionsThe paper has described a dependencyformalism and an Earley-type parser with apolynomial complexity.The introduction of non lexical categories in adependency formalism allows the treatment oflong distance dependencies and of free word order,and to aovid the NP-completeness.
The grammarfactor at the exponent can be reduced if wefurtherly restrict he long distance dependenciesthrough the introduction of a more restrictive datastructure than the set, as it happens in someconstrained phrase structure formalisms (Vijay-Schanker, Weir 1994).A compilation step in the parser can produceparse tables that account for left-cornerinformation (this optimization of the Earleyalgorithm has already been proven fruitful in(Lombardo, Lesmo 1996)).ReferencesBecker T., Rambow O., Parsing non-immediatedominance relations, Proc.
IWPT 95, Prague,1995, 26-33.Covington M. A., Parsing DiscontinuousConstituents in Dependency Grammar,Computational Linguistics 16, 1990, 234-236.Earley J., An Efficient Context-free ParsingAlgorithm.
CACM 13, 1970, 94-102.Eisner J., Three New Probabilistic Models forDependency Parsing: An Exploration, Proc.COLING 96, Copenhagen, 1996, 340-345.Fraser N.M., Hudson R. A., Inheritance in WordGrammar, Computational Linguistics 18,1992, 133-158.Gaifman H., Dependency Systems and PhraseStructure Systems, Information and Control 8,1965, 304-337.Hahn U., Schacht S., Broker N., Concurrent,Object-Oriented Natural Language Parsing:The ParseTalk Model, CLIF Report 9/94,Albert-Ludwigs-Univ., Freiburg, Germany (alsoin Journal of Human-Computer Studies).Hudson R., English Word Grammar, BasilBlackweU, Oxford, 1990.Kwon H., Yoon A., Unification-BasedDependency Parsing of Governor-FinalLanguages, Proc.
IWPT 91, Cancun, 1991, 182-192.Lombardo V., Lesmo L., An Earley-typerecognizer for dependency grammar, Proc.COLING 96, Copenhagen, 1996, 723-728.Mercuk I., Dependency Syntax: Theory andPractice, SUNY Press, Albany, 1988.Milward D., Dynamic Dependency Grammar,Linguistics and Phylosophy, December 1994.Nasr A., A formalism and a parser for lexicalizeddependency grammar, Proc.
IWPT 95, Prague,1995, 186-195.Neuhaus P., Broker N., The Complexity ofRecognition of Linguistically AdequateDependency Grammars, Proc.
ACL/EACL97,Madrid, 1997, 337-343.Neuhaus P., Hahn U., Restricted Parallelism inObject-Oriented Parsing, Proc.
COLING 96,Copenhagen, 1996, 502-507.Rambow O., Joshi A., A Formal Look atDependency Grammars and Phrase-StructureGrammars, with Special Consideration ofWord-Order Phenomena, Int.
Workshop on TheMeaning-Text Theory, Darmstadt, 1992.Schabes Y., Mathematical nd ComputationalAspects Of Lexicalized Grammars, Ph.D.Dissertation MS-CIS-90-48, Dept.
of Computerand Information Science, University ofPennsylvania, Philadelphia (PA), August 1990.SgaU P., Haijcova E., Panevova J., The Meaning ofSentence in its Semantic and PragmaticAspects, Dordrecht Reidel Publ.
Co., Dordrecht,1986.Sleator D. D., Temperley D., Parsing English witha Link Grammar, Proc.
of IWPT93, 1993, 277-29i.Vijay-Schanker K., Weir D. J., Parsing someconstrained grammar formalisms,Computational Linguistics 19/4, 1994793
