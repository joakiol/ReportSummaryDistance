Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 272?277,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsImproving the Recognizability of Syntactic Relations UsingContextualized ExamplesAditi MuralidharanComputer Science DivisionUniversity of California, BerkeleyBerkeley, CAasm@berkeley.eduMarti A. HearstSchool of InformationUniversity of California, BerkeleyBerkeley, CAhearst@berkeley.eduAbstractA common task in qualitative data analy-sis is to characterize the usage of a linguis-tic entity by issuing queries over syntac-tic relations between words.
Previous in-terfaces for searching over syntactic struc-tures require programming-style queries.User interface research suggests that it iseasier to recognize a pattern than to com-pose it from scratch; therefore, interfacesfor non-experts should show previews ofsyntactic relations.
What these previewsshould look like is an open question thatwe explored with a 400-participant Me-chanical Turk experiment.
We foundthat syntactic relations are recognized with34% higher accuracy when contextual ex-amples are shown than a baseline of nam-ing the relations alone.
This suggeststhat user interfaces should display contex-tual examples of syntactic relations to helpusers choose between different relations.1 IntroductionThe ability to search over grammatical relation-ships between words is useful in many non-scientific fields.
For example, a social scientisttrying to characterize different perspectives on im-migration might ask how adjectives applying to?immigrant?
have changed in the last 30 years.
Ascholar interested in gender might search a col-lection to find out whether different nouns enterinto possessive relationships with ?his?
and ?her?
(Muralidharan and Hearst, 2013).
In other fields,grammatical queries can be used to develop pat-terns for recognizing entities in text, such as med-ical terms (Hirschman et al, 2005; MacLean andHeer, 2013), and products and organizations (Cu-lotta and McCallum, 2005), and for coding quali-tative data such as survey results.Most existing interfaces for syntactic search(querying over grammatical and syntactic struc-tures) require structured query syntax.
For exam-ple, the popular Stanford Parser includes Tregex,which allows for sophisticated regular expressionsearch over syntactic tree structures (Levy and An-drew, 2006).
The Finite Structure Query tool forquerying syntactically annotated corpora requiresits queries to be stated in first order logic (Kepser,2003).
In the Corpus Query Language (Jakubiceket al, 2010), a query is a pattern of attribute-value pairs, where values can include regular ex-pressions containing parse tree nodes and words.Several approaches have adopted XML represen-tations and the associated query language familiesof XPATH and SPARQL.
For example, LPath aug-ments XPath with additional tree operators to giveit further expressiveness (Lai and Bird, 2010).However, most potential users do not have pro-gramming expertise, and are not likely to be atease composing rigidly-structured queries.
Onesurvey found that even though linguists wishedto make very technical linguistic queries, 55% ofthem did not know how to program (Soehn etal., 2008).
In another (Gibbs and Owens, 2012),humanities scholars and social scientists are fre-quently skeptical of digital tools, because they areoften difficult to use.
This reduces the likelihoodthat existing structured-query tools for syntacticsearch will be usable by non-programmers (Ogdenand Brooks, 1983).A related approach is the query-by-examplework seen in the past in interfaces to database sys-tems (Androutsopoulos et al, 1995).
For instance,the Linguist?s Search Engine (Resnik et al, 2005)uses a query-by-example strategy in which a usertypes in an initial sentence in English, and the sys-tem produces a graphical view of a parse tree asoutput, which the user can alter.
The user can ei-ther click on the tree or modify the LISP expres-sion to generalize the query.
SPLICR also contains272a graphical tree editor tool (Rehm et al, 2009).According to Shneiderman and Plaisant (2010),query-by-example has largely fallen out of favoras a user interface design approach.
A downsideof QBE is that the user must manipulate an exam-ple to arrive at the desired generalization.More recently auto-suggest, a faster techniquethat does not require the manipulation of query byexample, has become a widely-used approach insearch user interfaces with strong support in termsof its usability (Anick and Kantamneni, 2008;Ward et al, 2012; Jagadish et al, 2007).
A listof selectable options is shown under the searchbar, filtered to be relevant as the searcher types.Searchers can recognize and select the option thatmatches their information need, without having togenerate the query themselves.The success of auto-suggest depends uponshowing users options they can recognize.
How-ever, we know of no prior work on how to dis-play grammatical relations so that they can beeasily recognized.
One current presentation (notused with auto-suggest) is to name the relationand show blanks where the words that satisfy itwould appear as in X is the subject of Y (Muralid-haran and Hearst, 2013); we used this as the base-line presentation in our experiments because it em-ploys the relation definitions found in the Stan-ford Dependency Parser?s manual (De Marneffe etal., 2006).
Following the principle of recognitionover recall, we hypothesized that showing contex-tualized usage examples would make the relationsmore recognizable.Our results confirm that showing examples inthe form of words or phrases significantly im-proves the accuracy with which grammatical re-lationships are recognized over the standard base-line of showing the relation name with blanks.
Ourfindings also showed that clausal relationships,which span longer distances in sentences, bene-fited significantly more from example phrases thaneither of the other treatments.These findings suggest that a query interface inwhich a user enters a word of interest and the sys-tem shows candidate grammatical relations aug-mented with examples from the text will be moresuccessful than the baseline of simply naming therelation and showing gaps where the participatingwords appear.2 ExperimentWe gave participants a series of identificationtasks.
In each task, they were shown a list of sen-tences containing a particular syntactic relation-ship between highlighted words.
They were askedto identify the relationship type from a list of fouroptions.
We presented the options in three differ-ent ways, and compared the accuracy.We chose Amazon?s Mechanical Turk (MTurk)crowdsourcing platform as a source of study par-ticipants.
The wide range of backgrounds pro-vided by MTurk is desirable because our goal is tofind a representation that is understandable to mostpeople, not just linguistic experts or programmers.This platform has become widely used for bothobtaining language judgements and for usabilitystudies (Kittur et al, 2008; Snow et al, 2008).Our hypothesis was:Grammatical relations are identifiedmore accurately when shown with ex-amples of contextualizing words orphrases than without.To test it, participants were given a series ofidentification tasks.
In each task, they were showna list of 8 sentences, each containing a particu-lar relationship between highlighted words.
Theywere asked to identify the relationship from a listof 4 choices.
Additionally, one word was chosenas a focus word that was present in all the sen-tences, to make the relationship more recognizable(?life?
in Figure 1).The choices were displayed in 3 different ways(Figure 1).
The baseline presentation (Figure 1a)named the linguistic relation and showed a blankspace with a pink background for the varying wordin the relationship, the focus word highlighted inyellow and underlined, and any necessary addi-tional words necessary to convey the relationship(such as ?of?
for the prepositional relationship?of?, the third option).The words presentation showed the baseline de-sign, and in addition beneath was the word ?Exam-ples:?
followed by a list of 4 example words thatcould fill in the pink blank slot (Figure 1b).
Thephrases presentation again showed the baselinedesign, beneath which was the phrase ?Patternslike:?
and a list of 4 example phrases in whichfragments of text including both the pink and theyellow highlighted portions of the relationship ap-peared (Figure 1c).273(a) The options as they appear in the baseline condition.
(b) The same options as they appear in the words condition.
(c) The same options in the phrases condition, shown as they appeared in an identification task for the relationshipamod(life, ) (where different adjectives modify the noun ?life?).
The correct answer is ?adjective modifier?
(4th option),and the remaining 3 options are distractors.Figure 1: The appearance of the choices shown in the three experiment conditions.Method: We used a between-subjects design.The task order and the choice order were not var-ied: the only variation between participants wasthe presentation of the choices.
To avoid the pos-sibility of guessing the right answer by pattern-matching, we ensured that there was no overlapbetween the list of sentences shown, and the ex-amples shown in the choices as words or phrases.Tasks: The tasks were generated using theStanford Dependency Parser (De Marneffe et al,2006) on the text of Moby Dick by HermanMelville.
We tested the 12 most common gram-matical relationships in the novel in order to coverthe most content and to be able to provide as manyreal examples as possible.
These relationships fellinto two categories, listed below with examples.Clausal or long-distance relations:?
Adverbial clause: I walk while talking?
Open clausal complement: I love to sing?
Clausal complement: he saw us leave?
Relative clause modifier: the letter I wrotereachedNon-clausal relations:?
Subject of verb: he threw the ball?
Object of verb: he threw the ball?
Adjective modifier red ball?
Preposition (in): a hole in a bucket?
Preposition (of): the piece of cheese?
Conjunction (and) mind and body274?
Adverb modifier: we walk slowly?
Noun compound: Mr. BrownWe tested each of these 12 relations with 4 dif-ferent focus words, 2 in each role.
For example,the Subject of Verb relation was tested in the fol-lowing forms:?
(Ahab, ): the sentences each contained?Ahab?, highlighted in yellow, as the subject ofdifferent verbs highlighted in pink.?
(captain, )?
( , said): the sentences each containedthe verb ?said?, highlighted in yellow, but withdifferent subjects, highlighted in pink.?
( , stood)To maximize coverage, yet keep the total tasktime reasonable (average 6.8 minutes), we dividedthe relations above into 4 task sets, each testingrecognition of 3 different relations.
Each of rela-tions was tested with 4 different words, making atotal of 12 tasks per participant.Participants: 400 participants completed thestudy distributed randomly over the 4 task sets andthe 3 presentations.
Participants were paid 50c(U.S.) for completing the study, with an additional50c bonus if they correctly identified 10 or moreof the 12 relationships.
They were informed of thepossibility of the bonus before starting.To gauge their syntactic familiarity, we alsoasked them to rate how familiar they were withthe terms ?adjective?
(88% claimed they could de-fine it), ?infinitive?
(43%), and ?clausal comple-ment?
(18%).
To help ensure the quality of effort,we included a multiple-choice screening question,?What is the third word of this sentence??
The 27participants (out of 410) who answered incorrectlywere eliminated.Results: The results (Figure 2) confirm our hy-pothesis.
Participants in conditions that showedexamples (phrases and words) were significantlymore accurate at identifying the relations thanparticipants in the baseline condition.
We usedthe Wilcoxson signed-rank test, an alternative tothe standard T-test that does not assume sam-ples are normally distributed.
The average suc-cess rate in the baseline condition was 41%,which is significantly less accurate than words:52%, (p=0.00019, W=6136), and phrases: 55%,(p=0.00014, W=5546.5).Clausal relations operate over longer distancesin sentences, and so it is to be expected that show-ing longer stretches of context would perform bet-0?0.1?0.2?0.3?0.4?0.5?0.6?0.7?0.8?Overall?
Clausal Relations?
Non-Clausal Relations?
Adverb Modifier?Average Recognition Success Rate per Relation?Baseline?
Phrases?
Words?Figure 2: Recognition rates for different types ofrelations under the 3 experiment conditions, with95% confidence intervals.ter in these cases; that is indeed what the re-sults showed.
Phrases significantly outperformedwords and baseline for clausal relations.
The av-erage success rate was 48% for phrases, whichis significantly more than words: 38%, (p=0.017W=6976.5) and baseline: 24%, (p=1.9?10?9W=4399.0), which was indistinguishable fromrandom guessing (25%).
This is a strong improve-ment, given that only 18% of participants reportedbeing able to define ?clausal complement?.For the non-clausal relations, there was no sig-nificant difference between phrases and words,although they were both overall significantly bet-ter than the baseline (words: p=0.0063 W=6740,phrases: p=0.023 W=6418.5).
Among these rela-tions, adverb modifiers stood out (Figure 2), be-cause evidence suggested that words (63% suc-cess) made the relation more recognizable thanphrases (47% success, p=0.056, W=574.0) ?
butthe difference was only almost significant, due tothe smaller sample size (only 96 participants en-countered this relation).
This may be because thewords are the most salient piece of information inan adverbial relation ?
adverbs usually end in ?ly??
and in the phrases condition the additional infor-mation distracts from recognition of this pattern.3 ConclusionsThe results imply that user interfaces for syntacticsearch should show candidate relationships aug-mented with a list of phrases in which they occur.A list of phrases is the most recognizable presenta-tion for clausal relationships (34% better than thebaseline), and is as good as a list of words for theother types of relations, except adverb modifiers.For adverb modifiers, the list of words is the most275recognizable presentation.
This is likely becauseEnlglish adverbs usually end in ?-ly?
are thereforea distinctive set of words.The list of candidates can be ordered by fre-quency of occurrence in the collection, or by aninterestingness measure given the search word.
Asthe user becomes more familiar with a given re-lation, it may be expedient to shorten the cuesshown, and then re-introduce them if a relationhas not been selected after some period of timehas elapsed.
If phrases are used, there is a tradeoffbetween recognizability and the space required todisplay the examples of usage.
However, it is im-portant to keep in mind that because the sugges-tions are populated with items from the collectionitself, they are informative.The best strategy, phrases, had an overall suc-cess rate of only 55%, although the intended userbase may have more familiarity with grammaticalrelations than the participants did, and thereforemay perform better in practice.
Nonetheless, thereis room for improvement in scores, and it may bethat additional visual cues, such as some kind ofbracketing, will improve results.
Furthermore, thecurrent study did not test three-word relationshipsor more complex combinations of structures, andthose may require improvements to the design.4 AcknowledgementsWe thank Bj?orn Hartmann for his helpful com-ments.
This work is supported by National En-dowment for the Humanities grant HK-50011.ReferencesI Androutsopoulos, GD Ritchie, and P Thanisch.
1995.Natural language interfaces to databases?an intro-duction.
Natural Language Engineering, 1(01):29?81.Peter Anick and Raj Gopal Kantamneni.
2008.
A lon-gitudinal study of real-time search assistance adop-tion.
In Proceedings of the 31st annual internationalACM SIGIR conference on Research and develop-ment in information retrieval, pages 701?702.
ACM.Aron Culotta and Andrew McCallum.
2005.
Reduc-ing labeling effort for structured prediction tasks.
InAAAI, pages 746?751.Marie-Catherine De Marneffe, Bill MacCartney,Christopher D Manning, et al 2006.
Generat-ing typed dependency parses from phrase structureparses.
In LREC, volume 6, pages 449?454.Fred Gibbs and Trevor Owens.
2012.
Building betterdigital humanities tools.
DH Quarterly, 6(2).Lynette Hirschman, Alexander Yeh, ChristianBlaschke, and Alfonso Valencia.
2005.
Overviewof biocreative: critical assessment of informationextraction for biology.
BMC bioinformatics,6(Suppl 1):S1.HV Jagadish, Adriane Chapman, Aaron Elkiss,Magesh Jayapandian, Yunyao Li, Arnab Nandi, andCong Yu.
2007.
Making database systems usable.In Proceedings of the 2007 ACM SIGMOD interna-tional conference onManagement of data, pages 13?24.
ACM.Milos Jakubicek, Adam Kilgarriff, Diana McCarthy,and Pavel Rychl`y.
2010.
Fast syntactic searching invery large corpora for many languages.
In PACLIC,volume 24, pages 741?747.Stephan Kepser.
2003.
Finite structure query: Atool for querying syntactically annotated corpora.
InEACL, pages 179?186.Aniket Kittur, Ed H Chi, and Bongwon Suh.
2008.Crowdsourcing user studies with mechanical turk.In Proceedings of the SIGCHI conference on humanfactors in computing systems, pages 453?456.
ACM.Catherine Lai and Steven Bird.
2010.
Querying lin-guistic trees.
Journal of Logic, Language and Infor-mation, 19(1):53?73.Roger Levy and Galen Andrew.
2006.
Tregex and tsur-geon: tools for querying and manipulating tree datastructures.
In LREC, pages 2231?2234.Diana Lynn MacLean and Jeffrey Heer.
2013.
Iden-tifying medical terms in patient-authored text: acrowdsourcing-based approach.
Journal of theAmerican Medical Informatics Association.Aditi Muralidharan and Marti A Hearst.
2013.
Sup-porting exploratory text analysis in literature study.Literary and Linguistic Computing, 28(2):283?295.William C Ogden and Susan R Brooks.
1983.
Querylanguages for the casual user: Exploring the mid-dle ground between formal and natural languages.In Proceedings of the SIGCHI conference on Hu-man Factors in Computing Systems, pages 161?165.ACM.Georg Rehm, Oliver Schonefeld, Andreas Witt, ErhardHinrichs, and Marga Reis.
2009.
Sustainability ofannotated resources in linguistics: A web-platformfor exploring, querying, and distributing linguisticcorpora and other resources.
Literary and LinguisticComputing, 24(2):193?210.Philip Resnik, Aaron Elkiss, Ellen Lau, and HeatherTaylor.
2005.
The web in theoretical linguistics re-search: Two case studies using the linguists searchengine.
In Proc.
31st Mtg.
Berkeley Linguistics So-ciety, pages 265?276.276Ben Shneiderman and Catherine Plaisant.
2010.
De-signing The User Interface: Strategies for EffectiveHuman-Computer Interaction, 5/e (Fifth Edition).Addison Wesley.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y Ng.
2008.
Cheap and fast?but is itgood?
: evaluating non-expert annotations for natu-ral language tasks.
In Proceedings of the conferenceon empirical methods in natural language process-ing, pages 254?263.
Association for ComputationalLinguistics.Jan-Philipp Soehn, Heike Zinsmeister, and GeorgRehm.
2008.
Requirements of a user-friendly,general-purpose corpus query interface.
Sustain-ability of Language Resources and Tools for NaturalLanguage Processing, 6:27.David Ward, Jim Hahn, and Kirsten Feist.
2012.
Au-tocomplete as research tool: A study on providingsearch suggestions.
Information Technology and Li-braries, 31(4):6?19.277
