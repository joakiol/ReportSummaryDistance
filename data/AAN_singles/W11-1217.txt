An Expectation Maximization Algorithm for Textual Unit AlignmentRadu IonResearch Institute for AICalea 13 Septembrie nr.
13Bucharest 050711, Romaniaradu@racai.roAlexandru Ceau?uDublin City UniversityGlasnevin, Dublin 9, Ireland[address3]aceausu@computing.dcu.ieElena IrimiaResearch Institute for AICalea 13 Septembrie nr.
13Bucharest 050711, Romaniaelena@racai.roAbstractThe paper presents an Expectation Maximiza-tion (EM) algorithm for automatic generationof parallel and quasi-parallel data from anydegree of comparable corpora ranging fromparallel to weakly comparable.
Specifically,we address the problem of extracting relatedtextual units (documents, paragraphs or sen-tences) relying on the hypothesis that, in agiven corpus, certain pairs of translationequivalents are better indicators of a correcttextual unit correspondence than other pairs oftranslation equivalents.
We evaluate ourmethod on mixed types of bilingual compara-ble corpora in six language pairs, obtainingstate of the art accuracy figures.1 IntroductionStatistical Machine Translation (SMT) is in a con-stant need of good quality training data both fortranslation models and for the language models.Regarding the latter, monolingual corpora is evi-dently easier to collect than parallel corpora andthe truth of this statement is even more obviouswhen it comes to pairs of languages other thanthose both widely spoken and computationallywell-treated around the world such as English,Spanish, French or German.Comparable corpora came as a possible solu-tion to the problem of scarcity of parallel corporawith the promise that it may serve as a seed forparallel data extraction.
A general definition ofcomparability that we find operational is given byMunteanu and Marcu (2005).
They say that a (bi-lingual) comparable corpus is a set of paired doc-uments that, while not parallel in the strict sense,are related and convey overlapping information.Current practices of automatically collectingdomain-dependent bilingual comparable corporafrom the Web usually begin with collecting a listof t terms as seed data in both the source and thetarget languages.
Each term (in each language) isthen queried on the most popular search engine andthe first N document hits are retained.
The finalcorpus will contain t ?
N documents in each lan-guage and in subsequent usage the documentboundaries are often disregarded.At this point, it is important to stress out theimportance of the pairing of documents in a com-parable corpus.
Suppose that we want to word-align a bilingual comparable corpus consisting ofM documents per language, each with k words,using the IBM-1 word alignment algorithm (Brownet al, 1993).
This algorithm searches for eachsource word, the target words that have a maxi-mum translation probability with the source word.Aligning all the words in our corpus with no regardto document boundaries, would yield a time com-plexity of      operations.
The alternative wouldbe in finding a 1:p (with p a small positive integer,usually 1, 2 or 3) document assignment (a set ofaligned document pairs) that would enforce the ?nosearch outside the document boundary?
conditionwhen doing word alignment with the advantage ofreducing the time complexity to      operations.When M is large, the reduction may actually bevital to getting a result in a reasonable amount oftime.
The downside of this simplification is theloss of information: two documents may not becorrectly aligned thus depriving the word-alignment algorithm of the part of the search spacethat would have contained the right alignments.128Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 128?135,49th Annual Meeting of the Association for Computational Linguistics,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsWord alignment forms the basis of the phrasealignment procedure which, in turn, is the basis ofany statistical translation model.
A comparablecorpus differs essentially from a parallel corpus bythe fact that textual units do not follow a transla-tion order that otherwise greatly reduces the wordalignment search space in a parallel corpus.
Giventhis limitation of a comparable corpus in generaland the sizes of the comparable corpora that wewill have to deal with in particular,  we have de-vised one variant of an Expectation Maximization(EM) algorithm (Dempster et al, 1977) that gener-ates a 1:1 (p = 1) document assignment from a par-allel and/or comparable corpus using only pre-existing translation lexicons.
Its generality wouldpermit it to perform the same task on other textualunits such as paragraphs or sentences.In what follows, we will briefly review the lit-erature discussing document/paragraph alignmentand then we will present the derivation of the EMalgorithm that generates 1:1 document alignments.We will end the article with a thorough evaluationof the performances of this algorithm and the con-clusions that arise from these evaluations.2 Related WorkDocument alignment and other types of textualunit alignment have been attempted in various sit-uations involving extracting parallel data fromcomparable corpora.
The first case study is offeredby Munteanu and Marcu (2002).
They align sen-tences in an English-French comparable corpus of1.3M of words per language by comparing suffixtrees of the sentences.
Each sentence from eachpart of the corpus is encoded as a suffix tree whichis a tree that stores each possible suffix of a stringfrom the last character to the full string.
Using thismethod, Munteanu and Marcu are able to detectcorrect sentence alignments with a precision of95% (out of 100 human-judged and randomly se-lected sentences from the generated output).
Therunning time of their algorithm is approximately100 hours for 50000 sentences in each of the lan-guages.A popular method of aligning sentences in acomparable corpus is by classifying pairs of sen-tences as parallel or not parallel.
Munteanu andMarcu (2005) use a Maximum Entropy classifierfor the job trained with the following features: sen-tence lengths and their differences and ratios, per-centage of the words in a source sentence that havetranslations in a target sentence (translations aretaken from pre-existing translation lexicons), thetop three largest fertilities, length of the longestsequence of words that have translations, etc.
Thetraining data consisted of a small parallel corpus of5000 sentences per language.
Since the number ofnegative instances (50002 ?
5000) is far more largethan the number of positive ones (5000), the nega-tive training instances were selected randomly outof instances that passed a certain word overlap fil-ter (see the paper for details).
The classifier preci-sion is around 97% with a recall of 40% at theChinese-English task and around 95% with a recallof 41% for the Arabic-English task.Another case study of sentence alignment thatwe will present here is that of Chen (1993).
Heemploys an EM algorithm that will find a sentencealignment in a parallel corpus which maximizesthe translation probability for each sentence beadin the alignment.
The translation probability to bemaximized by the EM procedure considering eachpossible alignment  is given by(     )   ( )?
([])The following notations were used:   is theEnglish corpus (a sequence of English sentences),is the French corpus, [] is a sentence bead(a pairing of m sentences in English with nsentences in French),  ([]   [])is the sentence alignment (a sequence of sentencebeads) and p(L) is the probability that an alignmentcontains L beads.
The obtained accuracy is around96% and was computed indirectly by checkingdisagreement with the Brown sentence aligner(Brown et al, 1991) on randomly selected 500disagreement cases.The last case study of document and sentencealignment from ?very-non-parallel corpora?
is thework from Fung and Cheung (2004).
Their contri-bution to the problem of textual unit alignmentresides in devising a bootstrapping mechanism inwhich, after an initial document pairing and conse-quent sentence alignment using a lexical overlap-ping similarity measure, IBM-4 model (Brown etal., 1993) is employed to enrich the bilingual dic-tionary that is used by the similarity measure.
The129process is repeated until the set of identifiedaligned sentences does not grow anymore.
Theprecision of this method on English-Chinese sen-tence alignment is 65.7% (out of the top 2500 iden-tified pairs).3 EMACCWe propose a specific instantiation of the well-known general EM algorithm for aligning differenttypes of textual units: documents, paragraphs, andsentences which we will name EMACC (an acro-nym for ?Expectation Maximization Alignment forComparable Corpora?).
We draw our inspirationfrom the famous IBM models (specifically fromthe IBM-1 model) for word alignment (Brown etal., 1993) where the translation probability (eq.
(5))is modeled through an EM algorithm where thehidden variable a models the assignment (1:1 wordalignments) from the French sequence of words (?indexes) to the English one.By analogy, we imagined that between two setsof documents (from now on, we will refer to doc-uments as our textual units but what we presenthere is equally applicable ?
but with different per-formance penalties ?
to paragraphs and/or sentenc-es) ?
let?s call them   and  , there is an assignment(a sequence of 1:1 document correspondences1),the distribution of which can be modeled by a hid-den variable   taking values in the set {true, false}.This assignment will be largely determined by theexistence of word translations between a pair ofdocuments, translations that can differentiate be-tween one another in their ability to indicate a cor-rect document alignment versus an incorrect one.In other words, we hypothesize that there are cer-tain pairs of translation equivalents that are betterindicators of a correct document correspondencethan other translation equivalents pairs.We take the general formulation and derivationof the EM optimization problem from (Borman,2009).
The general goal is to optimize  (   ), thatis to find the parameter(s)   for which  (   ) ismaximum.
In a sequence of derivations that we arenot going to repeat here, the general EM equationis given by:1 Or ?alignments?
or ?pairs?.
These terms will be used withthe same meaning throughout the presentation.?
(      )    (     )(1)where  ?
(      )   .
At step n+1, we try toobtain a new parameter      that is going to max-imize (the maximization step) the sum over z (theexpectation step) that in its turn depends on thebest parameter    obtained at step n. Thus, inprinciple, the algorithm should iterate over the setof all possible   parameters, compute the expecta-tion expression for each of these parameters andchoose the parameter(s) for which the expressionhas the largest value.
But as we will see, in prac-tice, the set of all possible parameters has a dimen-sion that is exponential in terms of the number ofparameters.
This renders the problem intractableand one should back off to heuristic searches inorder to find a near-optimal solution.We now introduce a few notations that we willoperate with from this point forward.
We suggestto the reader to frequently refer to this section inorder to properly understand the next equations:?
is the set of source documents,     is thecardinal of this set;?
is the set of target documents with     itscardinal;?
is a pair of documents,      and;?
is a pair of translation equivalents?
?
such that    is a lexical item thatbelongs to    and    is a lexical item thatbelongs to   ;?
is the set of all existing translationequivalents pairs ?
?.
is the transla-tion probability score (as the one given forinstance by GIZA++ (Gao and Vogel,2008)).
We assume that GIZA++ transla-tion lexicons already exist for the pair oflanguages of interest.In order to tie equation 1 to our problem, we de-fine its variables as follows:?
is the sequence of 1:1 document align-ments of the form              ,{              }.
We call   an assign-ment which is basically a sequence of 1:1document alignments.
If there are     1:1document alignments in   and if        ,then the set of all possible assignments has130the cardinal equal to     () where n!
isthe factorial function of the integer n and./ is the binomial coefficient.
It is clearnow that with this kind of dimension of theset of all possible assignments (or   pa-rameters), we cannot simply iterate over itin order to choose the assignment thatmaximizes the expectation;?
*          + is the hidden variable thatsignals if a pair of documents     repre-sents a correct alignment (true) or not(false);?
is the sequence of translation equivalentspairs    from T in the order they appearin each document pair from  .Having defined the variables in equation 1 thisway, we aim at maximizing the translation equiva-lents probability over a given assignment,  (   ).In doing so, through the use of the hidden variablez, we are also able to find the 1:1 document align-ments that attest for this maximization.We proceed by reducing equation 1 to a formthat is readily amenable to software coding.
Thatis, we aim at obtaining some distinct probabilitytables that are going to be (re-)estimated by theEM procedure.
Due to the lack of space, we omitthe full derivation and directly give the generalform of the derived EM equation,   (   )     (      )- (2)Equation 2 suggests a method of updating the as-signment probability  (      )  with the lexicalalignment probability  (   ) in an effort to pro-vide the alignment clues that will ?guide?
the as-signment probability towards the correctassignment.
All it remains to do now is to definethe two probabilities.The lexical document alignment probability(   ) is defined as follows:(   )  ??
(   |   )(3)where  (       )  is the simplified lexical docu-ment alignment probability which is initially equalto  (   ) from the set  .
This probability is to beread as ?the contribution    makes to the correct-ness of the     alignment?.
We want that thealignment contribution of one translation equiva-lents pair    to distribute over the set of all possi-ble document pairs thus enforcing that?
(   |   ){              }(4)The summation over   in equation 3 is actuallyover all translation equivalents pairs that are to befound only in the current     document pair andthe presence of the product        ensures that westill have a probability value.The assignment probability  (      ) is alsodefined in the following way:(      )  ?
(        )(5)for which we enforce the condition:?
(        ){             }(6)Using equations 2, 3 and 5 we deduce the final,computation-ready EM equation[  ??
(       )?
(        )]?
[?
(       )(        )](7)As it is, equation 7 suggests an exhaustive searchin the set of all possible   parameters, in order tofind the parameter(s) for which the expression thatis the argument of ?argmax?
is maximum.
But, aswe know from section 3, the size of this this set isprohibitive to the attempt of enumerating eachassignment and computing the expectation expres-sion.
Our quick solution to this problem was todirectly construct the ?best?
assignment2 using a2 We did not attempt to find the mathematical maximum of theexpression from equation 7 and we realize that the conse-131greedy algorithm: simply iterate over all possible1:1 document pairs and for each document pair{              }  compute the align-ment count (it?s not a probability so we call it a?count?
following IBM-1 model?s terminology)?
(   |   )(        )Then, construct the best 1:1 assignment      bychoosing those pairs     for which we have countswith the maximum values.
Before this cycle(which is the basic EM cycle) is resumed, we per-form the following updates:(        )   (        )?
(   |   )(7a)(   |   )  ?
(   |   )(7b)and normalize the two probability tables withequations 6 and 4.
The first update is to be inter-preted as the contribution the lexical documentalignment probability makes to the alignmentprobability.
The second update equation aims atboosting the probability of a translation equivalentif and only if it is found in a pair of documents be-longing to the best assignment so far.
In this way,we hope that the updated translation equivalentwill make a better contribution to the discovery ofa correct document alignment that has not yet beendiscovered at step n + 1.Before we start the EM iterations, we need toinitialize the probability tables  (        ) and(   |   ) .
For the second table we used theGIZA++ scores that we have for the     translationequivalents pairs and normalized the table withequation 4.
For the first probability table we have(and tried) two choices:?
(D1) a uniform distribution:;?
(D2) a lexical document alignment meas-ure  (   ) (values between 0 and 1) that iscomputed directly from a pair of docu-quence of this choice and of the greedy search procedure is notfinding the true optimum.ments     using the    translation equiva-lents pairs from the dictionary  :(   )?
(  )        ?
(  )(8)where      is the number of words in documentand    (  ) is the frequency of word    in docu-ment    (please note that, according to section 3,is not a random pair of words, but a pair oftranslation equivalents).
If every word in thesource document has at least one translation (of agiven threshold probability score) in the targetdocument, then this measure is 1.
We normalizethe table initialized using this measure with equa-tion 6.EMACC finds only 1:1 textual units alignmentsin its present form but a document pair     can beeasily extended to a document bead following theexample from (Chen, 1993).
The main differencebetween the algorithm described by Chen and oursis that the search procedure reported there is inva-lid for comparable corpora in which no pruning isavailable due to the nature of the corpus.
A secondvery important difference is that Chen only relieson lexical alignment information, on the parallelnature of the corpus and on sentence lengths corre-lations while we add the probability of the wholeassignment which, when initially set to the D2 dis-tribution, produces a significant boost of the preci-sion of the alignment.4 Experiments and EvaluationsThe test data for document alignment was com-piled from the corpora that was previously collect-ed in the ACCURAT project3 and that is known tothe project members as the ?Initial ComparableCorpora?
or ICC for short.
It is important to knowthe fact that ICC contains all types of comparablecorpora from parallel to weakly comparable docu-ments but we classified document pairs in threeclasses: parallel (class name: p), strongly compa-rable (cs) and weakly comparable (cw).
We haveconsidered the following pairs of languages: Eng-lish-Romanian (en-ro), English-Latvian (en-lv),English-Lithuanian (en-lt), English-Estonian (en-et), English-Slovene (en-sl) and English-Greek3 http://www.accurat-project.eu/132(en-el).
For each pair of languages, ICC also con-tains a Gold Standard list of document alignmentsthat were compiled by hand for testing purposes.We trained GIZA++ translation lexicons forevery language pair using the DGT-TM4 corpus.The input texts were converted from their Unicodeencoding to UTF-8 and were tokenized using atokenizer web service described by Ceau?u (2009).Then, we applied a parallel version of GIZA++(Gao and Vogel, 2008) that gave us the translationdictionaries of content words only (nouns, verbs,adjective and adverbs) at wordform level.
For Ro-manian, Lithuanian, Latvian, Greek and English,we had lists of inflectional suffixes which we usedto stem entries in respective dictionaries and pro-cessed documents.
Slovene remained the only lan-guage which involved wordform level processing.The accuracy of EMACC is influenced by threeparameters whose values have been experimentallyset:?
the threshold over which we use transla-tion equivalents from the dictionary   fortextual unit alignment; values for thisthreshold (let?s name it ThrGiza) arefrom the ordered set *             +;?
the threshold over which we decide to up-date the probabilities of translation equiva-lents with equation 7b; values for thisthreshold (named ThrUpdate) are fromthe same ordered set *             +;?
the top ThrOut% alignments from thebest assignment found by EMACC.
Thisparameter will introduce precision and re-call with the ?perfect?
value for recallequal to ThrOut%.
Values for this pa-rameter are from the set *         +.We ran EMACC (10 EM steps) on every possiblecombination of these parameters for the pairs oflanguages in question on both initial distributionsD1 and D2.
For comparison, we also performed abaseline document alignment using the greedy al-gorithm of EMACC with the equation 8 supplyingthe document similarity measure.
The following 4tables report a synthesis of the results we have ob-tained which, because of the lack of space, wecannot give in full.
We omit the results of EMACCwith D1 initial distribution because the accuracy4 http://langtech.jrc.it/DGT-TM.htmlfigures (both precision and recall) are always lower(10-20%) than those of EMACC with D2.cs P/R Prms.
P/R Prms.
#en-ro1/0.690470.40.40.70.85714/0.857140.40.4142en-sl0.96666/0.288070.40.40.30.83112/0.831120.40.41302en-el0.97540/0.292380.0010.80.30.80098/0.800980.0010.41407en-lt0.97368/0.291910.40.80.30.72978/0.729780.40.41507en-lv0.95757/0.286750.40.40.30.79854/0.798540.0010.81560en-et0.88135/0.264420.40.80.30.55182/0.551820.40.41987Table 1: EMACC with D2 initial distribution on strong-ly comparable corporacs P/R Prms.
P/R Prms.
#en-ro1/0.690470.40.70.85714/0.857140.4142en-sl0.97777/0.291390.0010.30.81456/0.814560.40.1302en-el0.94124/0.281480.0010.30.71851/0.718510.0011407en-lt0.95364/0.285140.0010.30.72673/0.726730.0011507en-lv0.91463/0.273220.0010.30.80692/0.806920.0011560en-et0.87030/0.261000.40.30.57727/0.577270.41987Table 2: D2 baseline algorithm on strongly comparablecorporacw P/R Prms.
P/R Prms.
#en-ro1/0.294110.40.0010.30.66176/0.661760.40.001168en-sl0.73958/0.221640.40.40.30.42767/0.427670.40.41961en-el0.15238/0.045450.0010.80.30.07670/0.076700.0010.81352en-lt0.55670/0.166150.40.80.30.28307/0.283070.40.81325en-lv0.23529/0.070450.40.40.30.10176/0.101760.40.41511en-et0.59027/0.176340.40.80.30.27800/0.278000.40.81483Table 3: EMACC with D2 initial distribution on weaklycomparable corpora133cw P/R Prms.
P/R Prms.
#en-ro0.85/0.250.40.30.61764/0.617640.4168en-sl0.65505/0.196240.40.30.39874/0.398740.41961en-el0.11428/0.034280.40.30.06285/0.062850.41352en-lt0.60416/0.180120.40.30.24844/0.248440.41325en-lv0.13071/0.039210.40.30.09803/0.098030.41511en-et0.48611/0.145220.0010.30.25678/0.256780.41483Table 4: D2 baseline algorithm on weakly comparablecorporaIn every table above, the P/R column gives themaximum precision and the associated recallEMACC was able to obtain for the correspondingpair of languages using the parameters (Prms.
)from the next column.
The P/R column gives themaximum recall with the associated precision thatwe obtained for that pair of languages.The Prms.
columns contain parameter settingsfor EMACC (see Tables 1 and 3) and for the D2baseline algorithm (Tables 2 and 4): in Tables 1and 3 values for ThrGiza, ThrUpdate andThrOut are given from the top (of the cell) to thebottom and in Tables 2 and 4 values of ThrGizaand ThrOut are also given from top to bottom(the ThrUpdate parameter is missing because theD2 baseline algorithm does not do re-estimation).The # column contains the size of the test set: thenumber of documents in each language that have tobe paired.
The search space is # * # and the goldstandard contains # pairs of human aligned docu-ment pairs.To ease comparison between EMACC and theD2 baseline for each type of corpora (strongly andweakly comparable), we grayed maximal valuesbetween the two: either the precision in the P/Rcolumn or the recall in the P/R column.In the case of strongly comparable corpora (Ta-bles 1 and 2), we see that the benefits of re-estimating the probabilities of the translationequivalents (based on which we judge documentalignments) begin to emerge with precisions for allpairs of languages (except en-sl) being better thanthose obtained with the D2 baseline.
But the realbenefit of re-estimating the probabilities of transla-tion equivalents along the EM procedure is visiblefrom the comparison between Tables 3 and 4.
Thus,in the case of weakly comparable corpora, inwhich EMACC with the D2 distribution is clearlybetter than the baseline (with the only exception ofen-lt precision), due to the significant decrease inthe lexical overlap, the EM procedure is able toproduce important alignment clues in the form ofre-estimated (bigger) probabilities of translationequivalents that, otherwise, would have been ig-nored.It is important to mention the fact that the re-sults we obtained varied a lot with values of theparameters ThrGiza and ThrUpdate.
We ob-served, for the majority of studied language pairs,that lowering the value for ThrGiza and/orThrUpdate (0.1, 0.01, 0.001?
), would negative-ly impact the performance of EMACC due to thefact of introducing noise in the initial computationof the D2 distribution and also on re-estimating(increasing) probabilities for irrelevant translationequivalents.
At the other end, increasing thethreshold for these parameters (0.8, 0.85, 0.9?
)would also result in performance decreasing due tothe fact that too few translation equivalents (bethey all correct) are not enough to pinpoint correctdocument alignments since there are great chancesfor them to actually appear in all document pairs.So, we have experimentally found that there is acertain balance between the degree of correctnessof translation equivalents and their ability to pin-point correct document alignments.
In other words,the paradox resides in the fact that if a certain pairof translation equivalents is not correct but the re-spective words appear only in documents whichcorrectly align to one another, that pair is very im-portant to the alignment process.
Conversely, if apair of translation equivalents has a very highprobability score (thus being correct) but appearsin almost every possible pair of documents, thatpair is not informative to the alignment process andmust be excluded.
We see now that the EMACCaims at finding the set of translation equivalentsthat is maximally informative with respect to theset of document alignments.We have introduced the ThrOut parameter inorder to have better precision.
This parameter actu-ally instructs EMACC to output only the top (ac-cording to the alignment score probability(        )) ThrOut% of the document align-ments it has found.
This means that, if all are cor-rect, the maximum recall can only be ThrOut%.134But another important function of ThrOut is torestrict the translation equivalents re-estimation(equation 7b) for only the top ThrOut% align-ments.
In other words, only the probabilities oftranslation equivalents that are to be found in topThrOut% best alignments in the current EM stepare re-estimated.
We introduced this restriction inorder to confine translation equivalents probabilityre-estimation to correct document alignmentsfound so far.Regarding the running time of EMACC, we canreport that on a cluster with a total of 32 CPUcores (4 nodes) with 6-8 GB of RAM per node, thetotal running time is between 12h and 48h per lan-guage pair (about 2000 documents per language)depending on the setting of the various parameters.5 ConclusionsThe whole point in developing textual unit align-ment algorithms for comparable corpora is to beable to provide good quality quasi-aligned data toprograms that are specialized in extracting paralleldata from these alignments.
In the context of thispaper, the most important result to note is thattranslation probability re-estimation is a good toolin discovering new correct textual unit alignmentsin the case of weakly related documents.
We alsotested EMACC at the alignment of 200 parallelparagraphs (small texts of no more than 50 words)for all pairs of languages that we have consideredhere.
We can briefly report that the results are bet-ter than the strongly comparable document align-ments from Tables 1 and 2 which is a promisingresult because one would think that a significantreduction in textual unit size would negatively im-pact the alignment accuracy.AcknowledgementsThis work has been supported by the ACCURATproject (http://www.accurat-project.eu/) funded bythe European Community?s Seventh FrameworkProgram (FP7/2007-2013) under the Grant Agree-ment n?
248347.
It has also been partially support-ed by the Romanian Ministry of Education andResearch through the STAR project (no.742/19.01.2009).ReferencesBorman, S. 2009.
The Expectation Maximization Algo-rithm.
A short tutorial.
Online at:http://www.isi.edu/natural-language/teaching/cs562/2009/readings/B06.pdfBrown, P. F., Lai, J. C., and Mercer, R. L. 1991.
Align-ing sentences in parallel corpora.
In Proceedings ofthe 29th Annual Meeting of the Association forComputational Linguistics, pp.
169?176, June 8-21,1991, University of California, Berkeley, California,USA.Brown, P. F., Pietra, S. A. D., Pietra, V. J. D., and Mer-cer, R. L. 1993.
The Mathematics of Statistical Ma-chine Translation: Parameter Estimation.Computational Linguistics, 19(2): 263?311.Ceau?u, A.
2009.
Statistical Machine Translation forRomanian.
PhD Thesis, Romanian Academy (in Ro-manian).Chen, S. F. 1993.
Aligning Sentences in Bilingual Cor-pora Using Lexical Information.
In Proceedings ofthe 31st Annual Meeting on Association for Compu-tational Linguistics, pp.
9?16, Columbus, Ohio,USA.Dempster, A. P., Laird, N. M., and Rubin, D. B.
1977.Maximum likelihood from incomplete data via theEM algorithm.
Journal of the Royal Statistical Socie-ty, 39(B):1?38.Fung, P., and Cheung, P. 2004.
Mining Very-Non-Parallel Corpora: Parallel Sentence and Lexicon Ex-traction via Bootstrapping and EM.
In Proceedings ofEMNLP 2004, Barcelona, Spain: July 2004.Gao, Q., and Vogel, S. 2008.
Parallel implementationsof word alignment tool.
ACL-08 HLT: Software En-gineering, Testing, and Quality Assurance for Natu-ral Language Processing, pp.
49?57, June 20, 2008,The Ohio State University, Columbus, Ohio, USA.Munteanu, D. S., and Marcu, D. 2002.
Processing com-parable corpora with bilingual suffix trees.
In Pro-ceedings of the 2002 Conference on EmpiricalMethods in Natural Language Processing (EMNLP2002), pp.
289?295, July 6-7, 2002, University ofPennsylvania, Philadelphia, USA.Munteanu, D. S., and Marcu, D. 2005.
Improving ma-chine translation performance by exploiting non-parallel corpora.
Computational Linguistics,31(4):477?504.135
