Synchronous Dependency Insertion GrammarsA Grammar Formalism for Syntax Based Statistical MTYuan Ding     and     Martha PalmerDepartment of Computer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104, USA{yding, mpalmer}@linc.cis.upenn.eduAbstractThis paper introduces a grammar formalismspecifically designed for syntax-based sta-tistical machine translation.
The synchro-nous grammar formalism we propose inthis paper takes into consideration the per-vasive structure divergence between lan-guages, which many other synchronousgrammars are unable to model.
A Depend-ency Insertion Grammars (DIG) is a gen-erative grammar formalism that capturesword order phenomena within the depend-ency representation.
Synchronous Depend-ency Insertion Grammars (SDIG) is thesynchronous version of DIG which aims atcapturing structural divergences across thelanguages.
While both DIG and SDIG havecomparatively simpler mathematical forms,we prove that DIG nevertheless has a gen-eration capacity weakly equivalent to thatof CFG.
By making a comparison to TAGand Synchronous TAG, we show how suchformalisms are linguistically motivated.
Wethen introduce a probabilistic extension ofSDIG.
We finally evaluated our current im-plementation of a simplified version ofSDIG for syntax based statistical machinetranslation.1 IntroductionDependency grammars have a long history andhave played an important role in machine translation(MT).
The early use of dependency structures in ma-chine translation tasks mainly fall into the categoryof transfer based MT, where the dependency struc-ture of the source language is first analyzed, thentransferred to the target language by using a set oftransduction rules or a transfer lexicon, and finallythe linear form of the target language sentence isgenerated.While the above approach seems to be plausible,the transfer process demands intense human effort increating a working transduction rule set or a transferlexicon, which largely limits the performance andapplication domain of the resultant machine transla-tion system.In the early 1990s, (Brown et.
al.
1993) intro-duced the idea of statistical machine translation,where the word to word translation probabilities andsentence reordering probabilities are estimated froma large set of parallel sentence pairs.
By having theadvantage of leveraging large parallel corpora, thestatistical MT approach outperforms the traditionaltransfer based approaches in tasks for which ade-quate parallel corpora is available (Och, 2003).However, a major criticism of this approach is that itis void of any internal representation for syntax orsemantics.In recent years, hybrid approaches, which aim atapplying statistical learning to structured data, beganto emerge.
Syntax based statistical MT approachesbegan with (Wu 1997), who introduced a polyno-mial-time solution for the alignment problem basedon synchronous binary trees.
(Alshawi et al, 2000)extended the tree-based approach by representingeach production in parallel dependency trees as afinite-state transducer.
(Yamada and Knight, 2001,2002) model translation as a sequence of operationstransforming a syntactic tree in one language intothe string of the second language.The syntax based statistical approaches havebeen faced with the major problem of pervasivestructural divergence between languages, due to bothsystematic differences between languages (Dorr,1994) and the vagaries of loose translations in realcorpora.
While we would like to use syntactic in-formation in both languages, the problem of non-isomorphism grows when trees in both languages arerequired to match.To allow the syntax based machine translationapproaches to work as a generative process, certainisomorphism assumptions have to be made.
Hence areasonable question to ask is: to what extent shouldthe grammar formalism, which we choose to repre-sent syntactic language transfer, assume isomor-phism between the structures of the two languages?
(Hajic et al, 2002) allows for limited non-isomorphism in that n-to-m matching of nodes in thetwo trees is permitted.
However, even after extend-ing this model by allowing cloning operations onsubtrees, (Gildea, 2003) found that parallel treesover-constrained the alignment problem, andachieved better results with a tree-to-string modelusing one input tree than with a tree-to-tree modelusing two.At the same time, grammar theoreticians haveproposed various generative synchronous grammarformalisms for MT, such as Synchronous ContextFree Grammars (S-CFG) (Wu, 1997) or Synchro-nous Tree Adjoining Grammars (S-TAG) (Shieberand Schabes, 1990).
Mathematically, generativesynchronous grammars share many good propertiessimilar to their monolingual counterparts such asCFG or TAG (Joshi and Schabes, 1992).
If such asynchronous grammar could be learnt from parallelcorpora, the MT task would become a mathemati-cally clean generative process.However, the problem of inducing a synchronousgrammar from empirical data was never solved.
Forexample, Synchronous TAGs, proposed by (Shieberand Schabes, 1990), which were introduced primar-ily for semantics but were later also proposed fortranslation.
From a formal perspective, Syn-TAGscharacterize the correspondences between languagesby a set of synchronous elementary tree pairs.
Whileexamples show that this formalism does capture cer-tain cross language structural divergences, there isnot, to our knowledge, any successful statisticallearning method to learn such a grammar from em-pirical data.
We believe that this is due to the limitedability of Synchronous TAG to model structure di-vergences.
This observation will be discussed laterin Section 5.We studied the problem of learning synchronoussyntactic sub-structures (parallel dependency treelets)from unaligned parallel corpora in (Ding and Palmer,2004).
At the same time, we would like to formalizea synchronous grammar for syntax based statisticalMT.
The necessity of a well-defined formalism andcertain limitations of the current existing formalisms,motivate us to design a new synchronous grammarformalism which will have the following properties:1.
Linguistically motivated: it should be able tocapture most language phenomena, e.g.
compli-cated word orders such as ?wh?
movement.2.
Without the unrealistic word-to-word isomor-phism assumption: it should be able to capturestructural variations between the languages.3.
Mathematically rigorous: it should have a welldefined formalism and a proven generation ca-pacity, preferably context free or mildly contextsensitive.4.
Generative: it should be ?generative?
in amathematical sense.
This property is essentialfor the grammar to be used in statistical MT.Each production rule should have its own prob-ability, which will allow us to decompose theoverall translation probability.5.
Simple: it should have a minimal number ofdifferent structures and operations so that it willbe learnable from the empirical data.In the following sections of this paper, we intro-duce a grammar formalism that satisfies the aboveproperties: Synchronous Dependency InsertionGrammar (SDIG).
Section 2 gives an informal lookat the desired capabilities of a monolingual versionDependency Insertion Grammar (DIG) by address-ing the problems with previous dependency gram-mars.
Section 3 gives the formal definition of theDIG and shows that it is weakly equivalent to Con-text Free Grammar (CFG).
Section 4 shows howDIG is linguistically motivated by making a com-parison between DIG and Tree Adjoining Grammar(TAG).
Section 5 specifies the Synchronous DIGand Section 6 gives the probabilistic extension ofSDIG.2 Issues with Dependency Grammars2.1 Dependency Grammars and Statistical MTAccording to (Fox, 2002), dependency represen-tations have the best phrasal cohesion propertiesacross languages.
The percentage of head crossingsper chance is 12.62% and that of modifier crossingsper chance is 9.22%.
Observing this fact, it is rea-sonable to propose a formalism that handles lan-guage transfer based on dependency structures.What is more, if a formalism based on depend-ency structures is made possible, it will have thenice property of being simple, as expressed in thefollowing table:CFG TAG DGNode# 2n 2n nLexicalized?
NO YES YESNode types 2 2 1*Operation types 1 2 1*(*: will be shown later in this paper)Figure 1.The simplicity of a grammar is very important forstatistical modeling, i.e.
when it is being learnedfrom the corpora and when it is being used in ma-chine translation decoding, we don?t need to condi-tion the probabilities on two different node types oroperations.At the same time, dependency grammars are in-herently lexicalized in that each node is one word.Statistical parsers (Collins 1999) showed perform-ance improvement by using bilexical probabilities,i.e.
probabilities of word pair occurrences.
This iswhat dependency grammars model explicitly.2.2 A Generative Grammar?Why do we want the grammar for statistical MTto be generative?
First of all, generative models havelong been studied in the machine learning commu-nity, which will provide us with mathematically rig-orous algorithms for training and decoding.
Second,CFG, the most popular formalism in describingnatural language phenomena, is generative.
Certainideas and algorithms can be borrowed from CFG ifwe make the formalism generative.While there has been much previous work informalizing dependency grammars and in its appli-cation to the parsing task, until recently (Joshi andRambow, 2003), little attention has been given to theissue of making the proposed dependency grammargenerative.
And in machine translation tasks, al-though using dependency structures is an old idea,little effort has been made to propose a formalgrammar which views the composition and decom-position of dependency trees as a generative processfrom a formal perspective.There are two reasons for this fact: (1) The?pure?
dependency trees do not have nonterminals.The standard solution to this problem was intro-duced as early as (Gaifman 1965), where he pro-posed adding syntactic categories to each node onthe dependency tree.
(2) However, there is a deeperproblem with dependency grammar formalisms, asobserved by (Rambow and Joshi 1997).
In the de-pendency representation, it is hard to handle com-plex word order phenomena without resorting toglobal word order rules, which makes the grammarno longer generative.
This will be explored in thenext subsection (2.3).2.3 Non-projectivityNon-projectivity has long been a major obstaclefor anyone who wants to formalize dependencygrammar.
When we draw projection lines from thenodes in the dependency trees to a linear representa-tion of the sentence, if we cannot do so without hav-ing one or more projection lines going across at leastone of the arcs of the dependency tree, we say thedependency tree is non-projective.A typical example for non-projectivity is ?wh?movement, which is illustrated below.Figure 2.Our solution for this problem is given in section4 and in the next section we will first give the formaldefinition of the monolingual Dependency InsertionGrammar.3 The DIG Formalism3.1 Elementary TreesFormally, the Dependency Insertion Grammar isdefined as a six tuple ),,,,,( RSBALC .
C  is a setof syntactic categories and L  is a set of lexicalitems.
A  is a set of Type-A trees and B  is a set ofType-B trees (defined later).
S  is a set of the start-ing categories of the sentences.
R  is a set of wordorder rules local to each node of the trees.Each node in the DIG has three fields:A Node consists of:1.
One lexical item2.
One corresponding category3.
One local word order rule.We define two types of elementary trees in DIG:Type-A trees and Type-B trees.
Both types of treeshave one or more nodes.
One of the nodes in anelementary tree is designated as the head of the ele-mentary tree.Type-A trees are also called ?root lexicalizedtrees?.
They roughly correspond to the ?
trees inTAG.
Type-A trees have the following properties:Properties of a Type-A elementary tree:1.
The root is lexicalized.2.
The root is designated as the head of thetree3.
Any lexicalized node can take a set ofunlexicalized nodes as its arguments.4.
The local word order rule specifies therelative order between the current nodeand all its immediate children, includingthe unlexicalized arguments.Here is an example of a Type-A elementary treefor the verb ?like?.
Note that the head node ismarked with (@).Please note that the placement of the dependencyarcs reflects the relative order between the parentand all its immediate children.Figure 3Type-B trees are also called ?root unlexicalizedtrees?.
They roughly correspond to ?
trees in TAGand have the following properties:Properties of a Type-B elementary tree:1.
The root is the ONLY unlexicalized node2.
One of the lexicalized nodes is desig-nated as the head of the tree3.
Similar to Type-A trees, each node alsohave a word order rule that specifies therelative order between the current nodeand all its immediate children.Here is and example of a Type-B elementary tree forthe adverb ?really?Figure 43.2 The Unification OperationWe define only one type of operation: unificationfor any DIG derivation:Unification Operation:When an unlexicalized node and a headnode have the same categories, they canbe merged into one node.This specifies that an unlexicalized node cannotbe unified with a non-head node, which guaranteeslimited complexity when a unification operationtakes place.After unification,1.
If the resulting tree is a Type-A tree, its rootbecomes the new root;2.
If the resulting tree is a Type-B tree, the rootnode involved in the unification operation be-comes the new root.Here is one example for the unification operationwhich adjoins the adverb ?really?
to the verb ?like?
:Figure 5Note that for the above unification operation thedependency tree on the right hand side is just one ofthe possible resultant dependency trees.
The stringsgenerated by the set of possible resultant depend-ency trees should all be viewed as the language)(DIGL  generated by the DIG grammar.Also note that the definition of DIG is preservedthrough the unification operation, as we have:1.
(Type-A) (unify) (Type A)  =  (Type-A)2.
(Type-A) (unify) (Type B)  =  (Type-A)3.
(Type-B) (unify) (Type B)  =  (Type-B)3.3 Comparison to Other ApproachesThere are two major differences between our de-pendency grammar formalism and that of (Joshi andRambow, 2003):1.
We only define one unification operation,whereas (Joshi and Rambow, 2003) defined twooperations: substitution and adjunction.2.
We introduce the concept of ?heads?
in the DIGso that the derivation complexity is significantlysmaller.3.4 Proof of Weak Equivalence between DIGand CFGWe prove the weak equivalence between DIG andCFG by first showing that the language that a DIGgenerates is a subset of one that a CFG generates,i.e.
)()( CFGLDIGL ?
.
And then we show theopposite is also true: )()( DIGLCFGL ?
.3.4.1 )()( CFGLDIGL ?The proof is given constructively.
First, for eachType-A tree, we ?insert?
a ?waiting for Type-B tree?argument at each possible slot underneath it with thecategory B.
This process is shown below:Figure 6Then we ?flatten?
the Type-A tree to its linearform according to the local word order rule, whichdecides the relative ordering between the parent andall its children at each of the nodes.
And we get:}.{}{}.{}{}.{}.
{ 100HnjiHHHCBNTwCNTwCBNTwCNTwCBNTCANTLLL?y nww L0 is the strings of lexical itemsy }.
{ HCANT  is the nonterminal created forthis Type-A tree, and HC is the category of thehead (root).y }{ jCNT  is the nonterminal for each categoryy }.
{ HCBNT  is the nonterminal for each ?Type-B site?Similarly, for each Type-B tree we can create?Type-B site?
under its head node.
So we have:nHiHR wCBNTwCBNTwCRBNT }.{}.{}.
{ 0 LL?Then we create the production to take arguments:}.
{}{ CANTCNT ?And the production rules to take Type-B trees:}.{}.{}.
{ CBNTCRBNTCBNT ?}.{}.{}.
{ CRBNTCBNTCBNT ?Hence, a DIG can be converted to a CFG.3.4.2 )()( DIGLCFGL ?It is known that a context free grammar can be con-verted to Greibach Normal Form, where each pro-duction will have the form:*aVA ?
, where V  is the set of nonterminalsWe simply construct a corresponding Type-Adependency tree as follows:Figure 74 Compare DIG to TAGA Tree Adjoining Grammars is defined as a fivetuple ),,,,( SAINT?
, where ?
is a set of terminals,NT  is a set of nonterminals, I  is a finite set of fi-nite initial trees (?
trees), A  is a finite set of auxil-iary trees ( ?
trees), and S  is a set of startingsymbols.
The TAG formalism defines two opera-tions, substitution and adjunction.A TAG derives a phrase-structure tree, called the?derived tree?
and at the same time, in each step ofthe derivation process, two elementary trees areconnected through either the substitution or adjunc-tion operation.
Hence, we have a ?derivation tree?which represents the syntactic and/or logical relationbetween the elementary trees.
Since each elementarytree of TAG has exactly one lexical node, we canview the derivation tree as a ?Deep Syntactic Repre-sentation?
(DSynR).
This representation closely re-sembles the dependency structure of the sentence.Here we show how DIG models different opera-tions of TAG and hence handles word order phe-nomena gracefully.We categorize the TAG operations into three dif-ferent types: substitution, non-predicative adjunctionand predicative adjunction.z SubstitutionWe model the TAG substitution operation byhaving the embedded tree replaces the non-terminalthat is in accordance with its root.
An example forthis type is the substitution of NP.Figure 8a Substitution in TAGFigure 8b Substitution through DIG unificationz Non-predicative AdjunctionIn TAG, this type of operation includes all ad-junctions when the embedded tree does not contain apredicate, i.e.
the root of the embedded tree is not anS.
For example, the trees for adverbs are with rootVP and are adjoined to non-terminal VPs in the ma-trix tree.Figure 9a Non-predicative Adjunction in TAGLike[V]@[N]John[N]really[adv]@[V] Like[V]@[N]John[N] really[adv]Figure 9b Non-predicative Adjunction through DIGunificationz Predicative AdjunctionThis type of operation adjoins an embedded treewhich contains a predicate, i.e.
with a root S, to thematrix tree.
A typical example is the sentence: Whodoes John think Mary likes?This example is non-projective and has ?wh?movement.
In the TAG sense, the tree for ?doesJohn think?
is adjoined to the matrix tree for ?WhoMary likes?.
This category of operation has someinteresting properties.
The dependency relation ofthe embedded tree and the matrix tree is inverted.This means that if tree T1 is adjoined to T2, in non-predicative adjunction, T1 depends on T2, but inpredicative adjunction, T2 depends on T1.
In theabove example, the tree with ?like?
depends on thetree with ?think?.Figure 10a ?Wh?
movement through TAG(predicative) adjunction operationOur solution is quite simple: when we are con-structing the grammar, we invert the arc that pointsto a predicative clause.
Despite the fact that the re-sulting dependency trees have certain arcs inverted,we will still be able to use localized word order rulesand derive the desired sentence with the simple uni-fication operation.
As shown below:Figure 10b ?Wh?
movement through unificationSince TAG is mildly context sensitive, and wehave shown in Section 3 that DIG is context free, weare not claiming the two grammars are weakly orstrongly equivalent.
Also, please note DIG does nothandle all the non-projectivity issues due to its CFGequivalent generation capacity.5 Synchronous DIG5.1 Definition(Wu, 1997) introduced synchronous binary treesand (Shieber, 1990) introduced synchronous treeadjoining grammars, both of which view the transla-tion process as a synchronous derivation process ofparallel trees.
Similarly, with our DIG formalism,we can construct a Synchronous DIG by synchroniz-ing both structures and operations in both languagesand ensuring synchronous derivations.Properties of SDIG:1.
The roots of both trees of the source andtarget languages are aligned, and have thesame category2.
All the unlexicalized nodes of both treesare aligned and have the same category.3.
The two heads of both trees are alignedand have the same category.Synchronous Unification Operation:By the above properties of SDIG, we canshow that unification operations are synchro-nized in both languages.
Hence we can havesynchronous unification operations.5.2 Isomorphism AssumptionSo how is SDIG different from other synchro-nous grammar formalisms?As we know, a synchronous grammar derivesboth source and target languages through a series ofsynchronous derivation steps.
For any tree-basedsynchronous grammar, the synchronous derivationwould create two derivation trees for both languageswhich have isomorphic structure.
Thus a synchro-nous grammar assumes certain isomorphism be-tween the two languages which we refer to as the?isomorphism assumption?.Now we examine the isomorphism assumptionsin S-CFG and S-TAG:y For S-CFG, the substitutions for all the non-terminals need to be synchronous.
Hence theisomorphism assumption for S-CFG is isomor-phic phrasal structure.y For S-TAG, all the substitution and adjunctionoperations need to be synchronous, and thederivation trees of both languages are isomor-phic.
The derivation tree for TAG is roughlyequivalent to a dependency tree.
Hence theisomorphism assumption for S-TAG is an iso-morphic dependency structure.As shown by real translation tasks, both of thoseassumptions would fail due to structural divergencesbetween languages.On the other hand SDIG does NOT assume wordlevel isomorphism or isomorphic dependency trees.Since in the SDIG sense, the parallel dependencytrees are in fact the ?derived?
form rather than the?derivation?
form.
In other words, SDIG assumesthe isomorphism lies deeper than the dependencystructure.
It is ?the derivation tree of DIG?
that isisomorphic.The following ?pseudo-translation?
example il-lustrates how SDIG captures structural divergencebetween the languages.
Suppose we want to translate:y [Source] The girl kissed her kitty cat.y [Target] The girl gave a kiss to her cat.Figure 11Note that both S-CFG and S-TAG won?t be ableto handle such structural divergence.
However,when we view each of the two sentences as derivedfrom three elementary trees in DIG, we can have asynchronous derivation, as shown below:6 The Probabilistic Extension to SDIG andStatistical MTThe major reason to construct an SDIG is to havea generative model for syntax based statistical MT.By relying on the assumption that the derivation treeof DIG represents the probability dependency graph,we can build a graphical model which captures thefollowing two statistical dependencies:1.
Probabilities of Elementary Tree unification (inthe target language)2.
Probabilities of Elementary Tree transfer (be-tween languages), i.e.
the probability of twoelementary trees being pairedET-f3ET-f1ET-f2ET-f4ET-e3ET-e1ET-e2ET-e4Figure 12The above graph shows two isomorphic deriva-tion trees for two languages.
ET stands for elemen-tary trees and dotted arcs denote the conditionaldependence assumptions).
Under the above model,the best translation is: )()|(maxarg* ePefPee= ;And ?=iii eETfETPefP ))(|)(()|( ; also wehave ( )?=iii eETParenteETPeP ))((|)()( .Hence, we can have PSDIG (probabilistic syn-chronous Dependency Insertion Grammar).
Giventhe dynamic programming property of the abovegraphical model, an efficient polynomial timeViterbi decoding algorithm can be constructed.7 Current ImplementationTo test our idea, we implemented the above syn-chronous grammar formalism in a Chinese-Englishmachine translation system.
The actual implementa-tion of the synchronous grammar used in the systemis a scaled-down version of the SDIG introducedabove, where all the word categories are treated asone.
The reason for this simplification is that wordcategory mappings across languages are not straight-forward.
Defining the word categories so that theycan be consistent between the languages is a majorgoal for our future research.The uni-category version of the SDIG is inducedusing the algorithm in (Ding and Palmer, 2004),which is a statistical approach to extracting paralleldependency structures from large scale parallel cor-pora.
An example is given in Figure 12.
We canconstruct the parallel dependency trees as shown inFigure 13a.
The expected output of the above ap-proach is shown in Figure 13b.
(e) stands for anempty node trace.y [English]  I have been here since 1947.y [Chinese] Wo 1947  nian   yilai   yizhi     zhu   zai  zheli.I             year   since  always  live   in     hereFigure13a.InputFigure 13b.
Output(5 parallel elementary tree pairs)We build a decoder for the model in Section 6 forour machine translation system.
The decoder isbased on a polynomial time decoding algorithm forfast non-isomorphic tree-to-tree transduction (Un-published by the time of this paper).We use an automatic syntactic parser (Collins,1999; Bikel, 2002) to produce the parallel unalignedsyntactic structures.
The parser was trained using thePenn English/Chinese Treebanks.
We then used thealgorithm in (Xia 2001) to convert the phrasal struc-ture trees into dependency trees.The following table shows the statistics of thedatasets we used.
(Genre, number of sentence pairs,number of Chinese/English words, type and usage).Dataset Xinhua FBIS NISTGenre News News NewsSent# 56263 21003 206Chn W# 1456495 522953 26.3 averageEng W# 1490498 658478 32.5 averageType unaligned unaligned multi-referenceUsage training training testingFigure 14The training set consists of Xinhua newswiredata from LDC and the FBIS data.
We filtered bothdatasets to ensure parallel sentence pair quality.
Weused the development test data from the 2001 NISTMT evaluation workshop as our test data for the MTsystem performance.
In the testing data, each inputChinese sentence has 4 English translations as refer-ences, so that the result of the MT system can beevaluated using Bleu and NIST machine translationevaluation software.1-gram 2-gram 3-gram 4-gramNIST: 4.3753 4.9773 5.0579 5.0791BLEU: 0.5926 0.3417 0.2060 0.1353Figure 15The above table shows the cumulative Bleu andNIST n-gram scores for our current implementation;with the final Bleu score 0.1353 with average inputsentence length of 26.3 words.In comparison, in (Yamada and Knight, 2002),which was a phrasal structure based statistical MTsystem for Chinese to English translation, the Bleuscore reported for short sentences (less than 14words) is 0.099 to 0.102.Please note that the Bleu/NIST scorers, whilebased on n-gram matching, do not model syntax dur-ing evaluation, which means a direct comparisonbetween a syntax based MT system and a stringbased statistical MT system using the above scorerwould favor the string based systems.We believe that our results can be improved us-ing a more sophisticated machine translation pipe-line which has separate components that handlespecific language phenomena such as named entities.Larger training corpora can also be helpful.8 ConclusionFinally, let us review whether the proposed SDIGformalism has achieved the goals we setup in Sec-tion 1 of this paper for a grammar formalism for Sta-tistical MT applications:1.
Linguistically motivated: DIG captures word-order phenomena within the CFG domain.2.
SDIG dropped the unrealistic word-to-wordisomorphism assumption and is able to capturestructural divergences.3.
DIG is weakly equivalent to CFG.4.
DIG and SDIG are generative grammars.5.
They have both simple formalisms, only onetype of node, and one type of operation.9 Future WorkWe observe from our testing results that the cur-rent simplified uni-category version of SDIG suffersfrom various grammatical errors, both in grammarinduction and decoding, therefore our future workshould focus on word category consistency betweenthe languages so that a full-fledged version of SDIGcan be used.10 AcknowledgementsOur thanks go Aravind Joshi, Owen Rambow,Dekai Wu and all the anonymous reviewers of theprevious versions of the paper, who gave us invalu-able advices, suggestions and feedbacks.ReferencesHiyan Alshawi, Srinivas Bangalore, and Shona Douglas.2000.
Learning dependency translation models as col-lections of finite state head transducers.
ComputationalLinguistics, 26(1): 45-60.Daniel M. Bikel.
2002.
Design of a multi-lingual, paral-lel-processing statistical parsing engine.
In Proceedingsof HLT 2002.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematics ofstatistical machine translation: parameter estimation.Computational Linguistics, 19(2): 263-311.Michael John Collins.
1999.
Head-driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania, Philadelphia.Yuan Ding and Martha Palmer.
2004.
Automatic Learn-ing of Parallel Dependency Treelet Pairs, in Proceed-ings of The First International Joint Conference onNatural Language Processing (IJCNLP-04).Bonnie J. Dorr.
1994.
Machine translation divergences: Aformal description and proposed solution.
Computa-tional Linguistics, 20(4): 597-633.Heidi J.
Fox.
2002.
Phrasal cohesion and statistical ma-chine translation.
In Proceedings of EMNLP-02, pages304-311Daniel Gildea.
2003.
Loosely tree based alignment formachine translation.
In Proceedings of ACL-03Jan Hajic, et al 2002.
Natural language generation in thecontext of machine translation.
Summer workshop finalreport, Center for Language and Speech Processing,Johns Hopkins University, Baltimore.Aravind Joshi and Owen Rambow.
2003.
A formalism ofdependency grammar based on Tree Adjoining Gram-mar.
In Proceedings of the first international confer-ence on meaning text theory (MTT 2003), June 2003.Aravind K. Joshi and Yves Schabes.
Tree-adjoininggrammars and lexicalized grammars.
In Maurice Nivatand Andreas Podelski, editors, Tree Automata and Lan-guages.
Elsevier Science, 1992.Franz Josef Och.
2003.
Minimum Error Rate Training inStatistical Machine Translation.
In Proceedings ofACL-03), pages 160-167.Owen Rambow and Aravind Joshi.
1997.
A formal lookat dependency grammars and phrase structures.
In LeoWanner, editor, Recent Trends in Meaning-Text Theory,pages 167-190.S.
M. Shieber and Y. Schabes.
1990.
Synchronous Tree-Adjoining Grammars, Proceedings of the 13th COLING,pp.
253-258, August 1990.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):3-403.Fei Xia.
2001.
Automatic grammar generation from twodifferent perspectives.
Ph.D. thesis, University of Penn-sylvania, Philadelphia.Kenji Yamada and Kevin Knight.
2001.
A syntax basedstatistical translation model.
In Proceedings of ACL-01Kenji Yamada and Kevin Knight.
2002.
A decoder forsyntax-based statistical MT.
In Proceedings of ACL-02
