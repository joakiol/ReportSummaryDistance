Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1336?1345,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsUsing Bilingual Parallel Corporafor Cross-Lingual Textual EntailmentYashar MehdadFBK - irst and Uni.
of TrentoPovo (Trento), Italymehdad@fbk.euMatteo NegriFBK - irstPovo (Trento), Italynegri@fbk.euMarcello FedericoFBK - irstPovo (Trento), Italyfederico@fbk.euAbstractThis paper explores the use of bilingual par-allel corpora as a source of lexical knowl-edge for cross-lingual textual entailment.
Weclaim that, in spite of the inherent difficul-ties of the task, phrase tables extracted fromparallel data allow to capture both lexical re-lations between single words, and contextualinformation useful for inference.
We experi-ment with a phrasal matching method in or-der to: i) build a system portable across lan-guages, and ii) evaluate the contribution oflexical knowledge in isolation, without inter-action with other inference mechanisms.
Re-sults achieved on an English-Spanish corpusobtained from the RTE3 dataset support ourclaim, with an overall accuracy above averagescores reported by RTE participants on mono-lingual data.
Finally, we show that using par-allel corpora to extract paraphrase tables re-veals their potential also in the monolingualsetting, improving the results achieved withother sources of lexical knowledge.1 IntroductionCross-lingual Textual Entailment (CLTE) has beenproposed by (Mehdad et al, 2010) as an extensionof Textual Entailment (Dagan and Glickman, 2004)that consists in deciding, given two texts T and H indifferent languages, if the meaning of H can be in-ferred from the meaning of T. The task is inherentlydifficult, as it adds issues related to the multilingualdimension to the complexity of semantic inferenceat the textual level.
For instance, the reliance of cur-rent monolingual TE systems on lexical resources(e.g.
WordNet, VerbOcean, FrameNet) and deepprocessing components (e.g.
syntactic and semanticparsers, co-reference resolution tools, temporal ex-pressions recognizers and normalizers) has to con-front, at the cross-lingual level, with the limitedavailability of lexical/semantic resources coveringmultiple languages, the limited coverage of the ex-isting ones, and the burden of integrating language-specific components into the same cross-lingual ar-chitecture.As a first step to overcome these problems,(Mehdad et al, 2010) proposes a ?basic solution?,that brings CLTE back to the monolingual scenarioby translating H into the language of T. Despite theadvantages in terms of modularity and portability ofthe architecture, and the promising experimental re-sults, this approach suffers from one main limitationwhich motivates the investigation on alternative so-lutions.
Decoupling machine translation (MT) andTE, in fact, ties CLTE performance to the availabil-ity of MT components, and to the quality of thetranslations.
As a consequence, on one side trans-lation errors propagate to the TE engine hamperingthe entailment decision process.
On the other sidesuch unpredictable errors reduce the possibility tocontrol the behaviour of the engine, and devise ad-hoc solutions to specific entailment problems.This paper investigates the idea, still unexplored,of a tighter integration of MT and TE algorithms andtechniques.
Our aim is to embed cross-lingual pro-cessing techniques inside the TE recognition pro-cess in order to avoid any dependency on externalMT components, and eventually gain full control ofthe system?s behaviour.
Along this direction, we1336start from the acquisition and use of lexical knowl-edge, which represents the basic building block ofany TE system.
Using the basic solution proposedby (Mehdad et al, 2010) as a term of comparison,we experiment with different sources of multilinguallexical knowledge to address the following ques-tions:(1) What is the potential of the existing mul-tilingual lexical resources to approach CLTE?To answer this question we experiment with lex-ical knowledge extracted from bilingual dictionar-ies, and from a multilingual lexical database.
Suchexperiments show two main limitations of these re-sources, namely: i) their limited coverage, and ii)the difficulty to capture contextual information whenonly associations between single words (or at mostnamed entities and multiword expressions) are usedto support inference.
(2) Does MT provide useful resources or tech-niques to overcome the limitations of existing re-sources?
We envisage several directions in whichinputs from MT research may enable or improveCLTE.
As regards the resources, phrase and para-phrase tables extracted from bilingual parallel cor-pora can be exploited as an effective way to cap-ture both lexical relations between single words, andcontextual information useful for inference.
As re-gards the algorithms, statistical models based on co-occurrence observations, similar to those used inMTto estimate translation probabilities, may contributeto estimate entailment probabilities in CLTE.
Focus-ing on the resources direction, the main contribu-tion of this paper is to show that the lexical knowl-edge extracted from parallel corpora allows to sig-nificantly improve the results achieved with othermultilingual resources.
(3) In the cross-lingual scenario, can we achieveresults comparable to those obtained in mono-lingual TE?
Our experiments show that, althoughCLTE seems intrinsically more difficult, the resultsobtained using phrase and paraphrase tables are bet-ter than those achieved by average systems on mono-lingual datasets.
We argue that this is due to thefact that parallel corpora are a rich source of cross-lingual paraphrases with no equivalents in monolin-gual TE.
(4) Can parallel corpora be useful also for mono-lingual TE?
To answer this question, we experimenton monolingual RTE datasets using paraphrase ta-bles extracted from bilingual parallel corpora.
Ourresults improve those achieved with the most widelyused resources in monolingual TE, namely Word-Net, Verbocean, and Wikipedia.The remainder of this paper is structured as fol-lows.
Section 2 shortly overviews the role of lexicalknowledge in textual entailment, highlighting a gapbetween TE and CLTE in terms of available knowl-edge sources.
Sections 3 and 4 address the first threequestions, giving motivations for the use of bilingualparallel corpora in CLTE, and showing the results ofour experiments.
Section 5 addresses the last ques-tion, reporting on our experiments with paraphrasetables extracted from phrase tables on the monolin-gual RTE datasets.
Section 6 concludes the paper,and outlines the directions of our future research.2 Lexical resources for TE and CLTEAll current approaches to monolingual TE, ei-ther syntactically oriented (Rus et al, 2005), orapplying logical inference (Tatu and Moldovan,2005), or adopting transformation-based techniques(Kouleykov and Magnini, 2005; Bar-Haim et al,2008), incorporate different types of lexical knowl-edge to support textual inference.
Such informationranges from i) lexical paraphrases (textual equiva-lences between terms) to ii) lexical relations pre-serving entailment between words, and iii) word-level similarity/relatedness scores.
WordNet, themost widely used resource in TE, provides all thethree types of information.
Synonymy relationscan be used to extract lexical paraphrases indicat-ing that words from the text and the hypothesis en-tail each other, thus being interchangeable.
Hy-pernymy/hyponymy chains can provide entailment-preserving relations between concepts, indicatingthat a word in the hypothesis can be replacedby a word from the text.
Paths between con-cepts and glosses can be used to calculate simi-larity/relatedness scores between single words, thatcontribute to the computation of the overall similar-ity between the text and the hypothesis.Besides WordNet, the RTE literature documentsthe use of a variety of lexical information sources(Bentivogli et al, 2010; Dagan et al, 2009).These include, just to mention the most popular1337ones, DIRT (Lin and Pantel, 2001), VerbOcean(Chklovski and Pantel, 2004), FrameNet (Baker etal., 1998), and Wikipedia (Mehdad et al, 2010;Kouylekov et al, 2009).
DIRT is a collection of sta-tistically learned inference rules, that is often inte-grated as a source of lexical paraphrases and entail-ment rules.
VerbOcean is a graph of fine-grainedsemantic relations between verbs, which are fre-quently used as a source of precise entailment rulesbetween predicates.
FrameNet is a knowledge-baseof frames describing prototypical situations, and therole of the participants they involve.
It can beused as an alternative source of entailment rules,or to determine the semantic overlap between textsand hypotheses.
Wikipedia is often used to extractprobabilistic entailment rules based word similar-ity/relatedness scores.Despite the consensus on the usefulness of lexi-cal knowledge for textual inference, determining theactual impact of these resources is not straightfor-ward, as they always represent one component incomplex architectures that may use them in differ-ent ways.
As emerges from the ablation tests re-ported in (Bentivogli et al, 2010), even the mostcommon resources proved to have a positive impacton some systems and a negative impact on others.Some previous works (Bannard and Callison-Burch,2005; Zhao et al, 2009; Kouylekov et al, 2009)indicate, as main limitations of the mentioned re-sources, their limited coverage, their low precision,and the fact that they are mostly suitable to capturerelations mainly between single words.Addressing CLTE we have to face additional andmore problematic issues related to: i) the strongerneed of lexical knowledge, and ii) the limited avail-ability of multilingual lexical resources.
As regardsthe first issue, it?s worth noting that in the monolin-gual scenario simple ?bag of words?
(or ?bag of n-grams?)
approaches are per se sufficient to achieveresults above baseline.
In contrast, their applica-tion in the cross-lingual setting is not a viable so-lution due to the impossibility to perform direct lex-ical matches between texts and hypotheses in differ-ent languages.
This situation makes the availabilityof multilingual lexical knowledge a necessary con-dition to bridge the language gap.
However, withthe only exceptions represented by WordNet andWikipedia, most of the aforementioned resourcesare available only for English.
Multilingual lexi-cal databases aligned with the EnglishWordNet (e.g.MultiWordNet (Pianta et al, 2002)) have been cre-ated for several languages, with different degrees ofcoverage.
As an example, the 57,424 synsets of theSpanish section of MultiWordNet algned to Englishcover just around 50% of the WordNet?s synsets,thus making the coverage issue even more problem-atic than for TE.
As regards Wikipedia, the cross-lingual links between pages in different languagesoffer a possibility to extract lexical knowledge use-ful for CLTE.
However, due to their relatively smallnumber (especially for some languages), bilinguallexicons extracted from Wikipedia are still inade-quate to provide acceptable coverage.
In addition,featuring a bias towards named entities, the infor-mation acquired through cross-lingual links can atmost complement the lexical knowledge extractedfrom more generic multilingual resources (e.g bilin-gual dictionaries).3 Using Parallel Corpora for CLTEBilingual parallel corpora represent a possible solu-tion to overcome the inadequacy of the existing re-sources, and to implement a portable approach forCLTE.
To this aim, we exploit parallel data to: i)learn alignment criteria between phrasal elementsin different languages, ii) use them to automaticallyextract lexical knowledge in the form of phrase ta-bles, and iii) use the obtained phrase tables to createmonolingual paraphrase tables.Given a cross-lingual T/H pair (with the text inl1 and the hypothesis in l2), our approach leveragesthe vast amount of lexical knowledge provided byphrase and paraphrase tables to map H into T. Weperform such mapping with two different methods.The first method uses a single phrase table to di-rectly map phrases extracted from the hypothesis tophrases in the text.
In order to improve our system?sgeneralization capabilities and increase the cover-age, the second method combines the phrase tablewith two monolingual paraphrase tables (one in l1,and one in l2).
This allows to:1. use the paraphrase table in l2 to find para-phrases of phrases extracted from H;2. map them to entries in the phrase table, and ex-tract their equivalents in l1;13383. use the paraphrase table in l1 to find para-phrases of the extracted fragments in l1;4. map such paraphrases to phrases in T.With the second method, phrasal matches betweenthe text and the hypothesis are indirectly performedthrough paraphrases of the phrase table entries.The final entailment decision for a T/H pair is as-signed considering a model learned from the similar-ity scores based on the identified phrasal matches.In particular, ?YES?
and ?NO?
judgements are as-signed considering the proportion of words in thehypothesis that are found also in the text.
This wayto approximate entailment reflects the intuition that,as a directional relation between the text and the hy-pothesis, the full content of H has to be found in T.3.1 Extracting Phrase and Paraphrase TablesPhrase tables (PHT) contain pairs of correspond-ing phrases in two languages, together with associa-tion probabilities.
They are widely used in MT as away to figure out how to translate input in one lan-guage into output in another language (Koehn et al,2003).
There are several methods to build phrase ta-bles.
The one adopted in this work consists in learn-ing phrase alignments from a word-aligned bilingualcorpus.
In order to build English-Spanish phrase ta-bles for our experiments, we used the freely avail-able Europarl V.4, News Commentary and UnitedNations Spanish-English parallel corpora releasedfor the WMT101.
We run TreeTagger (Schmid,1994) for tokenization, and used the Giza++ (Ochand Ney, 2003) to align the tokenized corpora atthe word level.
Subsequently, we extracted the bi-lingual phrase table from the aligned corpora usingthe Moses toolkit (Koehn et al, 2007).
Since the re-sulting phrase table was very large, we eliminatedall the entries with identical content in the two lan-guages, and the ones containing phrases longer than5 words in one of the two sides.
In addition, in or-der to experiment with different phrase tables pro-viding different degrees of coverage and precision,we extracted 7 phrase tables by pruning the initialone on the direct phrase translation probabilities of0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.
The resulting1http://www.statmt.org/wmt10/phrase tables range from 76 to 48 million entries,with an average of 3.9 words per phrase.Paraphrase tables (PPHT) contain pairs of corre-sponding phrases in the same language, possibly as-sociated with probabilities.
They proved to be use-ful in a number of NLP applications such as naturallanguage generation (Iordanskaja et al, 1991), mul-tidocument summarization (McKeown et al, 2002),automatic evaluation of MT (Denkowski and Lavie,2010), and TE (Dinu and Wang, 2009).One of the proposed methods to extract para-phrases relies on a pivot-based approach usingphrase alignments in a bilingual parallel corpus(Bannard and Callison-Burch, 2005).
With thismethod, all the different phrases in one language thatare aligned with the same phrase in the other lan-guage are extracted as paraphrases.
After the extrac-tion, pruning techniques (Snover et al, 2009) canbe applied to increase the precision of the extractedparaphrases.In our work we used available2 paraphrasedatabases for English and Spanish which have beenextracted using the method previously outlined.Moreover, in order to experiment with differentparaphrase sets providing different degrees of cov-erage and precision, we pruned the main paraphrasetable based on the probabilities, associated to its en-tries, of 0.1, 0.2 and 0.3.
The number of phrase pairsextracted varies from 6 million to about 80000, withan average of 3.2 words per phrase.3.2 Phrasal Matching MethodIn order to maximize the usage of lexical knowledge,our entailment decision criterion is based on similar-ity scores calculated with a phrase-to-phrase match-ing process.A phrase in our approach is an n-gram composedof up to 5 consecutive words, excluding punctua-tion.
Entailment decisions are estimated by com-bining phrasal matching scores (Scoren) calculatedfor each level of n-grams , which is the numberof 1-grams, 2-grams,..., 5-grams extracted from Hthat match with n-grams in T. Phrasal matches areperformed either at the level of tokens, lemmas, orstems, can be of two types:2http://www.cs.cmu.edu/ alavie/METEOR13391.
Exact: in the case that two phrases are identicalat one of the three levels (token, lemma, stem);2.
Lexical: in the case that two different phrasescan be mapped through entries of the resourcesused to bridge T and H (i.e.
phrase tables, para-phrases tables, dictionaries or any other sourceof lexical knowledge).For each phrase in H, we first search for exactmatches at the level of token with phrases in T. Ifno match is found at a token level, the other levels(lemma and stem) are attempted.
Then, in case offailure with exact matching, lexical matching is per-formed at the same three levels.
To reduce redun-dant matches, the lexical matches between pairs ofphrases which have already been identified as exactmatches are not considered.Once matching for each n-gram level has beenconcluded, the number of matches (Mn) and thenumber of phrases in the hypothesis (Nn) are usedto estimate the portion of phrases in H that arematched at each level (n).
The phrasal matchingscore for each n-gram level is calculated as follows:Scoren =MnNnTo combine the phrasal matching scores obtainedat each n-gram level, and optimize their relativeweights, we trained a Support Vector Machine clas-sifier, SVMlight (Joachims, 1999), using each scoreas a feature.4 Experiments on CLTETo address the first two questions outlined in Sec-tion 1, we experimented with the phrase matchingmethod previously described, contrasting the effec-tiveness of lexical information extracted from par-allel corpora with the knowledge provided by otherresources used in the same way.4.1 DatasetThe dataset used for our experiments is an English-Spanish entailment corpus obtained from the orig-inal RTE3 dataset by translating the English hy-pothesis into Spanish.
It consists of 1600 pairsderived from the RTE3 development and test sets(800+800).
Translations have been generated bythe CrowdFlower3 channel to Amazon MechanicalTurk4 (MTurk), adopting the methodology proposedby (Negri and Mehdad, 2010).
The method relieson translation-validation cycles, defined as separatejobs routed to MTurk?s workforce.
Translation jobsreturn one Spanish version for each hypothesis.
Val-idation jobs ask multiple workers to check the cor-rectness of each translation using the original En-glish sentence as reference.
At each cycle, the trans-lated hypothesis accepted by the majority of trust-ful validators5 are stored in the CLTE corpus, whilewrong translations are sent back to workers in anew translation job.
Although the quality of the re-sults is enhanced by the possibility to automaticallyweed out untrusted workers using gold units, we per-formed a manual quality check on a subset of the ac-quired CLTE corpus.
The validation, carried out bya Spanish native speaker on 100 randomly selectedpairs after two translation-validation cycles, showedthe good quality of the collected material, with only3 minor ?errors?
consisting in controversial but sub-stantially acceptable translations reflecting regionalSpanish variations.The T-H pairs in the collected English-Spanishentailment corpus were annotated using TreeTagger(Schmid, 1994) and the Snowball stemmer6 with to-ken, lemma, and stem information.4.2 Knowledge sourcesFor comparison with the extracted phrase and para-phrase tables, we use a large bilingual dictionaryand MultiWordNet as alternative sources of lexicalknowledge.Bilingual dictionaries (DIC) allow for precisemappings between words in H and T. To createa large bilingual English-Spanish dictionary weprocessed and combined the following dictionariesand bilingual resources:- XDXF Dictionaries7: 22,486 entries.3http://crowdflower.com/4https://www.mturk.com/mturk/5Workers?
trustworthiness can be automatically determinedby means of hidden gold units randomly inserted into jobs.6http://snowball.tartarus.org/7http://xdxf.revdanica.com/1340Figure 1: Accuracy on CLTE by pruning the phrase tablewith different thresholds.- Universal dictionary database8: 9,944 entries.- Wiktionary database9: 5,866 entries.- Omegawiki database10: 8,237 entries.- Wikipedia interlanguage links11: 7,425 entries.The resulting dictionary features 53,958 entries,with an average length of 1.2 words.MultiWordNet (MWN) allows to extract mappingsbetween English and Spanish words connected byentailment-preserving semantic relations.
The ex-traction process is dataset-dependent, as it checksfor synonymy and hyponymy relations only betweenterms found in the dataset.
The resulting collectionof cross-lingual words associations contains 36,794pairs of lemmas.4.3 Results and DiscussionOur results are calculated over 800 test pairs of ourCLTE corpus, after training the SVM classifier over800 development pairs.
This section reports thepercentage of correct entailment assignments (accu-racy), comparing the use of different sources of lex-ical knowledge.Initially, in order to find a reasonable trade-off be-tween precision and coverage, we used the 7 phrasetables extracted with different pruning thresholds8http://www.dicts.info/9http://en.wiktionary.org/10http://www.omegawiki.org/11http://www.wikipedia.org/MWN DIC PHT PPHT Acc.
?x 55.00 0.00x 59.88 +4.88x 62.62 +7.62x x 62.88 +7.88Table 1: Accuracy results on CLTE using different lexicalresources.
(see Section 3.1).
Figure 1 shows that with the prun-ing threshold set to 0.05, we obtain the highest re-sult of 62.62% on the test set.
The curve demon-strates that, although with higher pruning thresholdswe retain more reliable phrase pairs, their smallernumber provides limited coverage leading to lowerresults.
In contrast, the large coverage obtained withthe pruning threshold set to 0.01 leads to a slightperformance decrease due to probably less precisephrase pairs.Once the threshold has been set, in order toprove the effectiveness of information extractedfrom bilingual corpora, we conducted a series of ex-periments using the different resources mentioned inSection 4.2.As it can be observed in Table 1, the highestresults are achieved using the phrase table, bothalone and in combination with paraphrase tables(62.62% and 62.88% respectively).
These resultssuggest that, with appropriate pruning thresholds,the large number and the longer entries containedin the phrase and paraphrase tables represent an ef-fective way to: i) obtain high coverage, and ii) cap-ture cross-lingual associations between multiple lex-ical elements.
This allows to overcome the bias to-wards single words featured by dictionaries and lex-ical databases.As regards the other resources used for compari-son, the results show that dictionaries substantiallyoutperform MWN.
This can be explained by thelow coverage of MWN, whose entries also repre-sent weaker semantic relations (preserving entail-ment, but with a lower probability to be applied)than the direct translations between terms containedin the dictionary.Overall, our results suggest that the lexical knowl-edge extracted from parallel data can be successfullyused to approach the CLTE task.1341Dataset WN VO WIKI PPHT PPHT 0.1 PPHT 0.2 PPHT 0.3 AVGRTE3 61.88 62.00 61.75 62.88 63.38 63.50 63.00 62.37RTE5 62.17 61.67 60.00 61.33 62.50 62.67 62.33 61.41RTE3-G 62.62 61.5 60.5 62.88 63.50 62.00 61.5 -Table 2: Accuracy results on monolingual RTE using different lexical resources.5 Using parallel corpora for TEThis section addresses the third and the fourth re-search questions outlined in Section 1.
Buildingon the positive results achieved on the cross-lingualscenario, we investigate the possibility to exploitbilingual parallel corpora in the traditional monolin-gual scenario.
Using the same approach discussedin Section 4, we compare the results achieved withEnglish paraphrase tables with those obtained withother widely used monolingual knowledge resourcesover two RTE datasets.For the sake of completeness, we report in thissection also the results obtained adopting the ?basicsolution?
proposed by (Mehdad et al, 2010).
Al-though it was presented as an approach to CLTE,the proposed method brings the problem back to themonolingual case by translating H into the languageof T. The comparison with this method aims at ver-ifying the real potential of parallel corpora againstthe use of a competitive MT system (Google Trans-late) in the same scenario.5.1 DatasetWe experiment with the original RTE3 and RTE5datasets, annotated with token, lemma, and stem in-formation using the TreeTagger and the Snowballstemmer.In addition to confront our method with the solu-tion proposed by (Mehdad et al, 2010) we translatedthe Spanish hypotheses of our CLTE dataset into En-glish using Google Translate.
The resulting datasetwas annotated in the same way.5.2 Knowledge sourcesWe compared the results achieved with paraphrasetables (extracted with different pruning thresh-olds12) with those obtained using the three most12We pruned the paraphrase table (PPHT), with probabilitiesset to 0.1 (PPHT 0.1), 0.2 (PPHT 0.2), and 0.3 (PPHT 0.3)widely used English resources for Textual Entail-ment (Bentivogli et al, 2010), namely:WordNet (WN).
WordNet 3.0 has been usedto extract a set of 5396 pairs of words connected bythe hyponymy and synonymy relations.VerbOcean (VO).
VerbOcean has been usedto extract 18232 pairs of verbs connected by the?stronger-than?
relation (e.g.
?kill?
stronger-than?injure?
).Wikipedia (WIKI).
We performed Latent Se-mantic Analysis (LSA) over Wikipedia using thejLSI tool (Giuliano, 2007) to measure the relat-edness between words in the dataset.
Then, wefiltered all the pairs with similarity lower than 0.7 asproposed by (Kouylekov et al, 2009).
In this waywe obtained 13760 word pairs.5.3 Results and DiscussionTable 2 shows the accuracy results calculated overthe original RTE3 and RTE5 test sets, training ourclassifier over the corresponding development sets.The first two rows of the table show that prunedparaphrase tables always outperform the other lexi-cal resources used for comparison, with an accuracyincrease up to 3%.
In particular, we observe that us-ing 0.2 as a pruning threshold provides a good trade-off between coverage and precision, leading to ourbest results on both datasets (63.50% for RTE3, and62.67% for RTE5).
It?s worth noting that these re-sults, compared with the average scores reported byparticipants in the two editions of the RTE Challenge(AVG column), represent an accuracy improvementof more than 1%.
Overall, these results confirm ourclaim that increasing the coverage using context sen-sitive phrase pairs obtained from large parallel cor-pora, results in better performance not only in CLTE,1342but also in the monolingual scenario.The comparison with the results achieved onmonolingual data obtained by automatically trans-lating the Spanish hypotheses (RTE3-G row in Ta-ble 2) leads to four main observations.
First, we no-tice that dealing with MT-derived inputs, the optimalpruning threshold changes from 0.2 to 0.1, leadingto the highest accuracy of 63.50%.
This suggeststhat the noise introduced by incorrect translationscan be tackled by increasing the coverage of theparaphrase table.
Second, in line with the findingsof (Mehdad et al, 2010), the results obtained overthe MT-derived corpus are equal to those we achieveover the original RTE3 dataset (i.e.
63.50%).
Third,the accuracy obtained over the CLTE corpus usingcombined phrase and paraphrase tables (62.88%, asreported in Table 1) is comparable to the best re-sult gained over the automatically translated dataset(63.50%).
In all the other cases, the use of phraseand paraphrase tables on CLTE data outperformsthe results achieved on the same data after transla-tion.
Finally, it?s worth remarking that applying ourphrase matching method on the translated datasetwithout any additional source of knowledge wouldresult in an overall accuracy of 62.12%, which islower than the result obtained using only phrase ta-bles on cross-lingual data (62.62%).
This demon-strates that phrase tables can successfully replaceMT systems in the CLTE task.In light of this, we suggest that extracting lexi-cal knowledge from parallel corpora is a preferablesolution to approach CLTE.
One of the main rea-sons is that placing a black-box MT system at thefront-end of the entailment process reduces the pos-sibility to cope with wrong translations.
Further-more, the access to MT components is not easy (e.g.Google Translate limits the number and the size ofqueries, while open source MT tools cover few lan-guage pairs).
Moreover, the task of developing afull-fledged MT system often requires the availabil-ity of parallel corpora, and is much more complexthan extracting lexical knowledge from them.6 Conclusion and Future WorkIn this paper we approached the cross-lingual Tex-tual Entailment task focusing on the role of lexi-cal knowledge extracted from bilingual parallel cor-pora.
One of the main difficulties in CLTE raisesfrom the lack of adequate knowledge resources tobridge the lexical gap between texts and hypothe-ses in different languages.
Our approach builds onthe intuition that the vast amount of knowledge thatcan be extracted from parallel data (in the form ofphrase and paraphrase tables) offers a possible so-lution to the problem.
To check the validity of ourassumptions we carried out several experiments onan English-Spanish corpus derived from the RTE3dataset, using phrasal matches as a criterion to ap-proximate entailment.
Our results show that phraseand paraphrase tables allow to: i) outperform the re-sults achieved with the few multilingual lexical re-sources available, and ii) reach performance levelsabove the average scores obtained by participants inthe monolingual RTE3 challenge.
These improve-ments can be explained by the fact that the lexi-cal knowledge extracted from parallel data providesgood coverage both at the level of single words, andat the level of phrases.As a further contribution, we explored the appli-cation of paraphrase tables extracted from paralleldata in the traditional monolingual scenario.
Con-trasting results with those obtained with the mostwidely used resources in TE, we demonstrated theeffectiveness of paraphrase tables as a mean to over-come the bias towards single words featured by theexisting resources.Our future work will address both the extractionof lexical information from bilingual parallel cor-pora, and its use for TE and CLTE.
On one side,we plan to explore alternative ways to build phraseand paraphrase tables.
One possible direction is toconsider linguistically motivated approaches, suchas the extraction of syntactic phrase tables as pro-posed by (Yamada and Knight, 2001).
Another in-teresting direction is to investigate the potential ofparaphrase patterns (i.e.
patterns including part-of-speech slots), extracted from bilingual parallelcorpora with the method proposed by (Zhao et al,2009).
On the other side we will investigate moresophisticated methods to exploit the acquired lexi-cal knowledge.
As a first step, the probability scoresassigned to phrasal entries will be considered to per-form weighted phrase matching as an improved cri-terion to approximate entailment.1343AcknowledgmentsThis work has been partially supported by the EC-funded project CoSyne (FP7-ICT-4-24853).ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
Proceedingsof COLING-ACL.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with Bilingual Parallel Corpora.
Proceed-ings of the 43rd Annual Meeting of the Association forComputational Linguistics (ACL 2005).Roy Bar-haim , Jonathan Berant , Ido Dagan , IddoGreental , Shachar Mirkin , Eyal Shnarch , and IdanSzpektor.
2008.
Efficient semantic deduction and ap-proximate matching over compact parse forests.
Pro-ceedings of the TAC 2008 Workshop on Textual Entail-ment.Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa TrangDang, and Danilo Giampiccolo.
2010.
The SixthPASCAL Recognizing Textual Entailment Challenge.Proceedings of the the Text Analysis Conference (TAC2010).Timothy Chklovski and Patrick Pantel.
2004.
Verbocean:Mining the web for fine-grained semantic verb rela-tions.
Proceedings of Conference on Empirical Meth-ods in Natural Language Processing (EMNLP-04).Ido Dagan and Oren Glickman.
2004.
Probabilistic tex-tual entailment: Generic applied modeling of languagevariability.
Proceedings of the PASCAL Workshop ofLearning Methods for Text Understanding and Min-ing.Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.2009.
Recognizing textual entailment: Rational, eval-uation and approaches.
Journal of Natural LanguageEngineering , Volume 15, Special Issue 04, pp i-xvii.Michael Denkowski and Alon Lavie.
2010.
Extendingthe METEOR Machine Translation Evaluation Metricto the Phrase Level.
Proceedings of Human LanguageTechnologies (HLT-NAACL 2010).Georgiana Dinu and Rui Wang.
2009.
Inference Rulesand their Application to Recognizing Textual Entail-ment.
Proceedings of the 12th Conference of the Eu-ropean Chapter of the ACL (EACL 2009).Claudio Giuliano.
2007. jLSI a tool for la-tent semantic indexing.
Software avail-able at http://tcc.itc.it/research/textec/tools-resources/jLSI.html.Lidija Iordanskaja, Richard Kittredge, and Alain Polg re..1991.
Lexical selection and paraphrase in a meaningtext generation model.
Natural Language Generationin Articial Intelligence and Computational Linguistics.Thorsten Joachims.
1999.
Making large-scale supportvector machine learning practical.Philipp Koehn, Franz Josef Och, and Daniel Marcu 2003.Statistical Phrase-Based Translation.
Proceedings ofHLT/NAACL.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
Proceed-ings of the Conference of the Association for Compu-tational Linguistics (ACL).Milen Kouleykov and Bernardo Magnini.
2005.
Treeedit distance for textual entailment.
Proceedings ofRALNP-2005, International Conference on Recent Ad-vances in Natural Language Processing.Milen Kouylekov, Yashar Mehdad, and Matteo Negri.2010.
Mining Wikipedia for Large-Scale Repositoriesof Context-Sensitive Entailment Rules.
Proceedingsof the Language Resources and Evaluation Conference(LREC 2010).Yashar Mehdad, Alessandro Moschitti and Fabio Mas-simo Zanzotto.
2010.
Syntactic/semantic structuresfor textual entailment recognition.
Proceedings of the11th Annual Conference of the North American Chap-ter of the Association for Computational Linguistics(NAACL HLT 2010).Dekang Lin and Patrick Pantel.
2001.
DIRT - Discoveryof Inference Rules from Text..
Proceedings of ACMConference on Knowledge Discovery and Data Mining(KDD-01).Kathleen R. McKeown, Regina Barzilay, David Evans,Vasileios Hatzivassiloglou, Judith L. Klavans, AniNenkova, Carl Sable, Barry Schiffman, and SergeySigelman.
2002.
Tracking and summarizing news ona daily basis with Columbias Newsblaster.
Proceed-ings of the Human Language Technology Conference..Yashar Mehdad, Matteo Negri, and Marcello Federico.2010.
Towards Cross-Lingual Textual Entailment.Proceedings of the 11th Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics (NAACL HLT 2010).Dan Moldovan and Adrian Novischi.
2002.
Lexicalchains for question answering.
Proceedings of COL-ING.Matteo Negri and Yashar Mehdad.
2010.
Creating a Bi-lingual Entailment Corpus through Translations withMechanical Turk: $100 for a 10-day Rush.
Proceed-ings of the NAACL 2010 Workshop on Creating Speechand Language Data With Amazons Mechanical Turk .Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):1951.1344Emanuele Pianta, Luisa Bentivogli, and Christian Gi-rardi.
2002.
MultiWordNet: Developing and AlignedMultilingual Database.
Proceedings of the First Inter-national Conference on Global WordNet.Vasile Rus, Art Graesser, and Kirtan Desai 2005.Lexico-Syntactic Subsumption for Textual Entailment.Proceedings of RANLP 2005.Helmut Schmid 2005.
Probabilistic Part-of-Speech Tag-ging Using Decision Trees.
Proceedings of the In-ternational Conference on New Methods in LanguageProcessing.Marta Tatu andDan Moldovan.
2005.
A semantic ap-proach to recognizing textual entailment.
Proceed-ings of the Human Language Technology Conferenceand Conference on Empirical Methods in Natural Lan-guage Processing (HLT/EMNLP 2005).Matthew Snover, Nitin Madnani, Bonnie Dorr, andRichard Schwartz.
2009.
Fluency, Adequacy, orHTER?
Exploring Different Human Judgments witha Tunable MT Metric.
Proceedings of WMT09.Rui Wang and Yi Zhang,.
2009.
Recognizing Tex-tual Relatedness with Predicate-Argument Structures.Proceedings of the Conference on Empirical Methodsin Natural Language Processing (EMNLP 2009).Kenji Yamada and Kevin Knight 2001.
A Syntax-BasedStatistical Translation Model.
Proceedings of the Con-ference of the Association for Computational Linguis-tics (ACL).Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.2009.
Extracting Paraphrase Patterns from BilingualParallel Corpora.
Journal of Natural Language Engi-neering , Volume 15, Special Issue 04, pp 503-526.1345
