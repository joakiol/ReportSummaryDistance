Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 266?274,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsRandomized Significance Tests in Machine TranslationYvette Graham Nitika Mathur Timothy BaldwinDepartment of Computing and Information SystemsThe University of Melbourneygraham@unimelb.edu.au, nmathur@student.unimelb.edu.au, tb@ldwin.netAbstractRandomized methods of significance test-ing enable estimation of the probabilitythat an increase in score has occurred sim-ply by chance.
In this paper, we examinethe accuracy of three randomized meth-ods of significance testing in the contextof machine translation: paired bootstrapresampling, bootstrap resampling and ap-proximate randomization.
We carry outa large-scale human evaluation of sharedtask systems for two language pairs toprovide a gold standard for tests.
Re-sults show very little difference in accu-racy across the three methods of signif-icance testing.
Notably, accuracy of alltest/metric combinations for evaluation ofEnglish-to-Spanish are so low that there isnot enough evidence to conclude they areany better than a random coin toss.1 IntroductionAutomatic metrics, such as BLEU (Papineni etal., 2002), are widely used in machine translation(MT) as a substitute for human evaluation.
Suchmetrics commonly take the form of an automaticcomparison of MT output text with one or morehuman reference translations.
Small differencesin automatic metric scores can be difficult to inter-pret, however, and statistical significance testingprovides a way of estimating the likelihood that ascore difference has occurred simply by chance.For several metrics, such as BLEU, standard sig-nificance tests cannot be applied due to scoresnot comprising the mean of individual sentencescores, justifying the use of randomized methods.Bootstrap resampling was one of the early ran-domized methods proposed for statistical signifi-cance testing of MT (Germann, 2003; Och, 2003;Kumar and Byrne, 2004; Koehn, 2004), to assessfor a pair of systems how likely a difference inBLEU scores occurred by chance.
Empirical testsdetailed in Koehn (2004) show that even for testsets as small as 300 translations, BLEU confidenceintervals can be computed as accurately as if theyhad been computed on a test set 100 times as large.Approximate randomization was subsequentlyproposed as an alternate to bootstrap resam-pling (Riezler and Maxwell, 2005).
Theoreticallyspeaking, approximate randomization has an ad-vantage over bootstrap resampling, in that it doesnot make the assumption that samples are repre-sentative of the populations from which they aredrawn.
Both methods require some adaptation inorder to be used for the purpose of MT evalua-tion, such as combination with an automatic met-ric, and therefore it cannot be taken for grantedthat approximate randomization will be more ac-curate in practice.
Within MT, approximate ran-domization for the purpose of statistical testing isalso less common.Riezler and Maxwell (2005) provide a compar-ison of approximate randomization with bootstrapresampling (distinct from paired bootstrap resam-pling), and conclude that since approximate ran-domization produces higher p-values for a set ofapparently equally-performing systems, it moreconservatively concludes statistically significantdifferences, and recommend preference of approx-imate randomization over bootstrap resamplingfor MT evaluation.
Conclusions drawn from ex-periments provided in Riezler and Maxwell (2005)are oft-cited, with experiments interpreted as ev-idence that bootstrap resampling is overly opti-mistic in reporting significant differences (Riezlerand Maxwell, 2006; Koehn and Monz, 2006; Gal-ley and Manning, 2008; Green et al., 2010; Monz,2011; Clark et al., 2011).Our contribution in this paper is to revisit sta-tistical significance tests in MT ?
namely, boot-strap resampling, paired bootstrap resampling and266approximate randomization ?
and find problemswith the published formulations.
We redress theseissues, and apply the tests in statistical testing oftwo language pairs.
Using human judgments oftranslation quality, we find only very minor differ-ences in significance levels across the three tests,challenging claims made in the literature about rel-ative merits of tests.2 Revisiting Statistical Significance Testsfor MT EvaluationFirst, we revisit the formulations of bootstrapresampling and approximate randomization al-gorithms as presented in Riezler and Maxwell(2005).
At first glance, both methods appear tobe two-tailed tests, with the null hypothesis thatthe two systems perform equally well.
To facili-tate a two-tailed test, absolute values of pseudo-statistics are computed before locating the abso-lute value of the actual statistic (original differ-ence in scores).
Using absolute values of pseudo-statistics is not problematic in the approximaterandomization algorithm, and results in a reason-able two-tailed significance test.
However, thebootstrap algorithm they provide uses an addi-tional shift-to-zero method of simulating the nullhypothesis.
The way in which this shift-to-zeroand absolute values of pseudo-statistics are ap-plied is non-standard.
Combining shift-to-zeroand absolute values of pseudo-statistics resultsin all pseudo-statistics that fall below the meanpseudo-statistic to be omitted from computation ofcounts later used to compute p-values.
The ver-sion of the bootstrap algorithm, as provided in thepseudo-code, is effectively a one-tailed test, andsince this does not happen in the approximate ran-domization algorithm, experiments appear to com-pare p-values from a one-tailed bootstrap test di-rectly with those of a two-tailed approximate ran-domization test.
This inconsistency is not recog-nized, however, and p-values are compared as ifboth tests are two-tailed.A better comparison of p-values would first re-quire doubling the values of the one-sided boot-strap, leaving those of the two-sided approximaterandomization algorithm as-is.
The results of thetwo tests on this basis are extremely close, andin fact, in two out of the five comparisons, thoseof the bootstrap would have marginally higher p-values than those of approximate randomization.As such, it is conceivable to conclude that the ex-periments actually show no substantial differencein Type I error between the two tests, which is con-sistent with results published in other fields of re-search (Smucker et al., 2007).
We also note thatthe pseudo-code contains an unconventional com-putation of mean pseudo-statistics, ?B, for shift-to-zero.Rather than speculate over whether these is-sues with the original paper were simply presen-tational glitches or the actual basis of the experi-ments reported on in the paper, we present a nor-malized version of the two-sided bootstrap algo-rithm in Figure 1, and report on the results of ourown experiments in Section 4.
We compare thismethod with approximate randomization and alsopaired bootstrap resampling (Koehn, 2004), whichis widely used in MT evaluation.
We carry outevaluation over a range of MT systems, not onlyincluding pairs of systems that perform equallywell, but also pairs of systems for which onesystem performs marginally better than the other.This enables evaluation of not only Type I error,but the overall accuracy of the tests.
We carry outa large-scale human evaluation of all WMT 2012shared task participating systems for two languagepairs, and collect sufficient human judgments tofacilitate statistical significance tests.
This hu-man evaluation data then provides a gold-standardagainst which to compare randomized tests.
Sinceall randomized tests only function in combina-tion with an automatic MT evaluation metric, wepresent results of each randomized test across fourdifferent MT metrics.3 Randomized Significance Tests3.1 Bootstrap ResamplingBootstrap resampling provides a way of estimat-ing the population distribution by sampling withreplacement from a representative sample (Efronand Tibshirani, 1993).
The test statistic is takenas the difference in scores of the two systems,SX?
SY, which has an expected value of 0 underthe null hypothesis that the two systems performequally well.
A bootstrap pseudo-sample consistsof the translations by the two systems (Xb, Yb) ofa bootstrapped test set (Koehn, 2004), constructedby sampling with replacement from the originaltest set translations.
The bootstrap distributionSbootof the test statistic is estimated by calculat-ing the value of the pseudo-statistic SXb?
SYbforeach pseudo-sample.267Set c = 0Compute actual statistic of score differences SX?
SYon test dataCalculate sample mean ?B=1BB?b=1SXb?
SYboverbootstrap samples b = 1, ..., BFor bootstrap samples b = 1, ..., BSample with replacement from variable tuples testsentences for systems X and YCompute pseudo-statistic SXb?
SYbon bootstrap dataIf |SXb?
SYb?
?B| ?
|SX?
SY|c = c+ 1If c/B ?
?Reject the null hypothesisFigure 1: Two-sided bootstrap resampling statisti-cal significance test for automatic MT evaluationSet c = 0Compute actual statistic of score differences SX?
SYon test dataFor random shuffles r = 1, ..., RFor sentences in test setShuffle variable tuples between systems X and Ywith probability 0.5Compute pseudo-statistic SXr?
SYron shuffled dataIf SXr?
SYr?
SX?
SYc = c+ 1If c/R ?
?Reject the null hypothesisFigure 2: Approximate randomization statisticalsignificance test for automatic MT evaluationThe null hypothesis distribution SH0can be es-timated from Sbootby applying the shift method(Noreen, 1989), which assumes that SH0has thesame shape but a different mean than Sboot.
Thus,Sbootis transformed into SH0by subtracting themean bootstrap statistic from every value in Sboot.Once this shift-to-zero has taken place, the nullhypothesis is rejected if the probability of observ-ing a more extreme value than the actual statisticis lower than a predetermined p-value ?, which istypically set to 0.05.
In other words, the score dif-ference is significant at level 1?
?.Figure 3 provides a one-sided implementationof bootstrap resampling, whereH0is that the scoreof System X is less than or equal to the score ofSet c = 0Compute actual statistic of score differences SX?
SYon test dataCalculate sample mean ?B=1BB?b=1SXb?
SYboverbootstrap samples b = 1, ..., BFor bootstrap samples b = 1, ..., BSample with replacement from variable tuples testsentences for systems X and YCompute pseudo-statistic SXb?
SYbon bootstrap dataIf SXb?
SYb?
?B?
SX?
SYc = c+ 1If c/B ?
?Reject the null hypothesisFigure 3: One-sided Bootstrap resampling statisti-cal significance test for automatic MT evaluationSet c = 0For bootstrap samples b = 1, ..., BIf SXb< SYbc = c+ 1If c/B ?
?Reject the null hypothesisFigure 4: Paired bootstrap resampling randomizedsignificance testSystem Y .
Figure 5 includes a typical example ofbootstrap resampling applied to BLEU, for a pairof systems for which differences in scores are sig-nificant, while Figure 6 shows the same for ME-TEOR but for a pair of systems with no significantdifference in scores.3.2 Approximate RandomizationUnlike bootstrap, approximate randomizationdoes not make any assumptions about the popula-tion distribution.
To simulate a distribution for thenull hypothesis that the scores of the two systemsare the same, translations are shuffled between thetwo systems so that 50% of each pseudo-sampleis drawn from each system.
In the context of ma-chine translation, this can be interpreted as eachtranslation being equally likely to have been pro-duced by one system as the other (Riezler andMaxwell, 2005).The test statistic is taken as the difference inscores of the two systems, SX?
SY.
If there is268?0.015 ?0.005 0.005 0.0150100200300400Paired Bootstrap Res.
BLEUoriginc = 13?0.015 ?0.005 0.005 0.0150100200300400Bootstrap Resampling BLEUactual statisticc = 14?0.015 ?0.005 0.005 0.0150100200300400Approximate Randomization  BLEUactual statisticc = 11Figure 5: Pseudo-statistic distributions for a typical pair of systems with close BLEU scores for eachrandomized test (System F vs. System G).
?0.015 ?0.005 0.005 0.0150100200300400Paired Bootstrap Res.
METEORoriginc = 269?0.015 ?0.005 0.005 0.0150100200300400Bootstrap Resampling METEORactual statisticc = 275?0.015 ?0.005 0.005 0.0150100200300400Approximate Randomization  METEORactual statisticc = 260Figure 6: Pseudo-statistic distributions of METEOR with randomized tests (System D vs. System A).a total of S sentences, then a total of 2Sshuffles ispossible.
If S is large, instead of generating all 2Spossible combinations, we instead generate sam-ples by randomly permuting translations betweenthe two systems with equal probability.
The distri-bution of the test statistic under the null hypoth-esis is approximated by calculating the pseudo-statistic, SXr?
SYr, for each sample.
As before,the null hypothesis is rejected if the probability ofobserving a more extreme value than the actualtest statistic is lower than ?.Figure 2 provides a one-sided implementationof approximate randomization for MT evaluation,where the null hypothesis is that the score of Sys-tem X is less than or equal to the score of SystemY .
Figure 5 shows a typical example of pseudo-statistic distributions for approximate randomiza-tion for a pair of systems with a small but signifi-cant score difference according to BLEU, and Fig-ure 6 shows the same for METEOR applied to apair of systems where no significant difference isconcluded.3.3 Paired Bootstrap ResamplingPaired bootstrap resampling (Koehn, 2004) isshown in Figure 4.
Unlike the other two random-ized tests, this method makes no attempt to simu-late the null hypothesis distribution.
Instead, boot-strap samples are used to estimate confidence in-tervals of score differences, with confidence inter-vals not containing 0 implying a statistically sig-nificant difference.We compare what takes place with the two othertests, by plotting differences in scores for boot-strapped samples, SXb?
SYb, as shown in Fig-ure 5 for BLEU and Figure 6 for METEOR.
Insteadof computing counts with reference to the actualstatistic, the line through the origin provides thecut-off for counts.269Adequacy Fluency Combinedp-valueSystem.ASystem.BSystem.CSystem.DSystem.E System.FSystem.G System.H System.JSystem.K System.LSystem.M System.ASystem.BSystem.CSystem.DSystem.E System.FSystem.G System.H System.JSystem.K System.LSystem.M System.ASystem.BSystem.CSystem.DSystem.E System.FSystem.G System.H System.JSystem.K System.LSystem.MSystem.MSystem.LSystem.KSystem.JSystem.HSystem.GSystem.FSystem.ESystem.DSystem.CSystem.BSystem.AFigure 7: Human evaluation pairwise significance tests for Spanish-to-English systems (colored cellsdenote scores for System row being significantly greater than System column .4 EvaluationIn order to evaluate the accuracy of the three ran-domized significance significance tests, we com-pare conclusions reached in a human evaluationof shared task participant systems.
We carry outa large-scale human evaluation of all participatingsystems from WMT 2012 (Callison-Burch et al.,2012) for the Spanish-to-English and English-to-Spanish translation tasks.
Large numbers of hu-man assessments of translations were collected us-ing Amazon?s Mechanical Turk, with strict qual-ity control filtering (Graham et al., 2013).
A to-tal of 82,100 human adequacy assessments and62,400 human fluency assessments were collected.After the removal of quality control items andfiltering of judgments from low-quality workers,this resulted in an average of 1,280 adequacy and1,013 fluency assessments per system for Spanish-to-English (12 systems), and 1,483 adequacy and1,534 fluency assessments per system for English-to-Spanish (11 systems).
To remove bias with re-spect to individual human judge preference scor-ing severity/leniency, scores provided by each hu-man assessor were standardized according to themean and standard deviation of all scores providedby that individual.Significance tests were carried out over thescores for each pair of systems separately foradequacy and fluency assessments using theWilcoxon rank-sum test.
Figure 7 shows pairwisesignificance test results for fluency, adequacy andthe combination of the two tests, for all pairs ofSpanish-to-English systems.
Combined fluencyand adequacy significance test results are con-structed as follows: if a system?s adequacy score issignificantly greater than that of another, the com-bined conclusion is that it is significantly better,at that significance level.
Only when a tie in ad-equacy scores occurs are fluency judgments usedto break the tie.
In this case, p-values from signifi-cance tests applied to fluency scores of that systempair are used.
For example, in Figure 7, adequacyscores of System B are not significantly greaterthan those of Systems C, D and E, while fluencyscores for System B are significantly greater thanthose of the three other systems.
The combined re-sult for each pair of systems is therefore taken asthe p-value from the corresponding fluency signif-icance test.We use the combined human evaluation pair-wise significant tests as a gold standard againstwhich to evaluate the randomized methods of sta-tistical significance testing.
We evaluate pairedbootstrap resampling (Koehn, 2004) and bootstrapresampling as shown in Figure 3 and approxi-mate randomization as shown in Figure 2, eachin combination with four automatic MT metrics:BLEU (Papineni et al., 2002), NIST (NIST, 2002),METEOR (Banerjee and Lavie, 2005) and TER(Snover et al., 2006).4.1 Results and DiscussionFigure 8 shows the outcome of pairwise random-ized significance tests for each metric for Spanish-to-English systems, and Table 1 shows numbers ofcorrect conclusions and accuracy of each test.When we compare conclusions made by thethree randomized tests for Spanish-to-English sys-tems, there is very little difference in p-values forall pairs of systems.
For both BLEU and NIST,270Paired Bootst.
Resamp.
Bootst.
Resamp.
Approx.
Rand.?
Conc.
Acc.
(%) Conc.
Acc.
(%) Conc.
Acc.
(%)0.05BLEU 53 80.3 [68.7, 89.1] 53 80.3 [68.7, 89.1] 53 80.3 [68.7, 89.1]NIST 54 81.8 [70.4, 90.2] 54 81.8 [70.4, 90.2] 54 81.8 [70.4, 90.2]METEOR 52 78.8 [67.0, 87.9] 52 78.8 [67.0, 87.9] 52 78.8 [67.0, 87.9]TER 52 78.8 [67.0, 87.9] 52 78.8 [67.0, 87.9] 52 78.8 [67.0, 87.9]0.01BLEU 51 77.3 [65.3, 86.7] 51 77.3 [65.3, 86.7] 51 77.3 [65.3, 86.7]NIST 51 77.3 [65.3, 86.7] 51 77.3 [65.3, 86.7] 51 77.3 [65.3, 86.7]METEOR 53 80.3 [68.7, 89.1] 53 80.3 [68.7, 89.1] 53 80.3 [68.7, 89.1]TER 51 77.3 [65.3, 86.7] 51 77.3 [65.3, 86.7] 51 77.3 [65.3, 86.7]0.001BLEU 48 72.7 [60.4, 83.0] 48 72.7 [60.4, 83.0] 48 72.7 [60.4, 83.0]NIST 48 72.7 [60.4, 83.0] 48 72.7 [60.4, 83.0] 48 72.7 [60.4, 83.0]METEOR 53 80.3 [68.7, 89.1] 53 80.3 [68.7, 89.1] 52 78.8 [67.0, 87.9]TER 50 75.8 [63.6, 85.5] 51 77.3 [65.3, 86.7] 52 78.8 [67.0, 87.9]Table 1: Accuracy of randomized significance tests for Spanish-to-English MT with four automaticmetrics, based on the WMT 2012 participant systems.Paired Bootst.
Resamp.
Bootst.
Resamp.
Approx.
Rand.?
Conc.
Acc.
(%) Conc.
Acc.
(%) Conc.
Acc.
(%)0.05BLEU 34 61.8 [47.7, 74.6] 34 61.8 [47.7, 74.6] 34 61.8 [47.7, 74.6]NIST 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3]METEOR 31 56.4 [42.3, 69.7] 31 56.4 [42.3, 69.7] 31 56.4 [42.3, 69.7]TER 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3]0.01BLEU 33 60.0 [45.9, 73.0] 33 60.0 [45.9, 73.0] 33 60.0 [45.9, 73.0]NIST 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3]METEOR 31 56.4 [42.3, 69.7] 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3]TER 30 54.5 [40.6, 68.0] 30 54.5 [40.6, 68.0] 30 54.5 [40.6, 68.0]0.001BLEU 33 60.0 [45.9, 73.0] 33 60.0 [45.9, 73.0] 33 60.0 [45.9, 73.0]NIST 33 60.0 [45.9, 73.0] 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3]METEOR 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3]TER 30 54.5 [40.6, 68.0] 30 54.5 [40.6, 68.0] 31 56.4 [42.3, 69.7]Table 2: Accuracy of randomized significance tests for English-to-Spanish MT with four automaticmetrics, based on the WMT 2012 participant systems.all three randomized methods produce p-valuesso similar that when ?
thresholds are applied, allthree tests produce precisely the same set of pair-wise conclusions for each metric.
When tests arecombined with METEOR and TER, similar resultsare observed: at the ?
thresholds of 0.05 and 0.01,precisely the same conclusions are drawn for bothmetrics combined with each of the three tests, andat most a difference of two conclusions at the low-est ?
level.Table 2 shows the accuracy of each test on theEnglish-to-Spanish data, showing much the sameset of conclusions at all ?
levels.
For BLEU andNIST, all three tests again produce precisely thesame conclusions, at p < 0.01 there is at most asingle different conclusion for METEOR, and onlyat the lowest p-value level is there a single differ-ence for TER.271METEORTERNISTBLEUPaired Bootstrap Bootstrap ApproximateResampling Resampling RandomizationSystem.MSystem.LSystem.KSystem.JSystem.HSystem.GSystem.FSystem.ESystem.DSystem.CSystem.BSystem.ASystem.MSystem.LSystem.KSystem.JSystem.HSystem.GSystem.FSystem.ESystem.DSystem.CSystem.BSystem.ASystem.MSystem.LSystem.KSystem.JSystem.HSystem.GSystem.FSystem.ESystem.DSystem.CSystem.BSystem.ASystem.ASystem.BSystem.CSystem.DSystem.ESystem.FSystem.GSystem.HSystem.JSystem.KSystem.LSystem.MSystem.ASystem.BSystem.CSystem.DSystem.ESystem.FSystem.GSystem.HSystem.JSystem.KSystem.LSystem.MSystem.ASystem.BSystem.CSystem.DSystem.ESystem.FSystem.GSystem.HSystem.JSystem.KSystem.LSystem.MSystem.MSystem.LSystem.KSystem.JSystem.HSystem.GSystem.FSystem.ESystem.DSystem.CSystem.BSystem.AFigure 8: Automatic metric pairwise randomized significance test results for Spanish-to-English systems(colored cells denote scores for System row significantly greater than System column).Finally, we examine which combination of met-ric and test is most accurate for each languagepair at the conventional significance level of p <0.05.
For Spanish-to-English evaluation, NISTcombined with any of the three randomized testsis most accurate, making 54 out of 66 (82%) cor-rect conclusions.
For English-to-Spanish, BLEUin combination with any of the three randomizedtests, is most accurate at 62%.
For both languagepairs, however, differences in accuracy for metrics272are not significant (Chi-square test).For English-to-Spanish evaluation, an accuracyas low as 62% should be a concern.
This levelof accuracy for significance testing ?
only makingthe correct conclusion in 6 out of 10 tests ?
actsas a reminder that no matter how sophisticated thesignificance test, it will never make up for flaws inan underlying metric.
When we take into accountthe fact that lower confidence limits all fall below50%, significance tests based on these metrics forEnglish-to-Spanish are effectively no better than arandom coin toss.5 ConclusionsWe provided a comparison of bootstrap resam-pling and approximate randomization significancetests for a range of automatic machine trans-lation evaluation metrics.
To provide a gold-standard against which to evaluate randomizedtests, we carried out a large-scale human evalua-tion of all shared task participating systems for theSpanish-to-English and English-to-Spanish trans-lation tasks from WMT 2012.
Results showed formany metrics and significance levels that all threetests produce precisely the same set of conclu-sions, and when conclusions do differ, it is com-monly only by a single contrasting conclusion,which is not significant.
For English-to-SpanishMT, the results of the different MT evaluation met-ric/significance test combinations are not signifi-cantly higher than a random baseline.AcknowledgementsWe wish to thank the anonymous reviewers for their valuablecomments.
This research was supported by funding from theAustralian Research Council.ReferencesS.
Banerjee and A. Lavie.
2005.
METEOR: An au-tomatic metric for mt evaluation with improved cor-relation with human judgements.
In Proc.
Wkshp.Intrinsic and Extrinsic Evaluation Measures for Ma-chine Translation and/or Summarization, pages 65?73, Ann Arbor, MI.
ACL.C.
Callison-Burch, P. Koehn, C. Monz, M. Post,R.
Soricut, and L. Specia.
2012.
Findings of the2012 Workshop on Statistical Machine Translation.In Proc.
7th Wkshp.
Statistical Machine Translation,pages 10?51, Montreal, Canada.
ACL.J.
H. Clark, C. Dyer, A. Lavie, and N. A. Smith.2011.
Better hypothesis testing for statistical ma-chine translation: Controlling for optimizer instabil-ity.
In Proc.
of the 49th Annual Meeting of the As-soc.
Computational Linguistics: Human LanguageTechnologies: short papers-Volume 2, pages 176?181, Portland, OR.
ACL.B.
Efron and R. J. Tibshirani.
1993.
An Introductionto the Bootstrap.
Chapman & Hall, New York City,NY.M.
Galley and C. D. Manning.
2008.
A simple andeffective hierarchical phrase reordering model.
InProc.
of the Conference on Empirical Methods inNatural Language Processing, pages 848?856, Ed-inburgh, Scotland.
ACL.U.
Germann.
2003.
Greedy decoding for statisti-cal machine translation in almost linear time.
InProc.
of the 2003 Conference of the North AmericanChapter of the Assoc.
Computational Linguistics onHuman Language Technology-Volume 1, pages 1?8,Edmonton, Canada.
ACL.Y.
Graham, T. Baldwin, A. Moffat, and J. Zobel.
2013.Continuous measurement scales in human evalua-tion of machine translation.
In Proc.
7th Linguis-tic Annotation Wkshp.
& Interoperability with Dis-course, pages 33?41, Sofia, Bulgaria.
ACL.S.
Green, M. Galley, and C. D. Manning.
2010.
Im-proved models of distortion cost for statistical ma-chine translation.
In Human Language Technolo-gies: The 2010 Annual Conference of the NorthAmerican Chapter of the Assoc.
Computational Lin-guistics, pages 867?875, Los Angeles, CA.
ACL.P.
Koehn and C. Monz.
2006.
Manual and automaticevaluation of machine translation between Europeanlanguages.
In Proceedings of the Workshop on Sta-tistical Machine Translation, pages 102?121, NewYork City, NY.
ACL.P.
Koehn.
2004.
Statistical significance tests for ma-chine translation evaluation.
In Proc.
of Empiri-cal Methods in Natural Language Processing, pages388?395, Barcelona, Spain.
ACL.S.
Kumar and W. J. Byrne.
2004.
Minimum Bayes-risk decoding for statistical machine translation.
InHLT-NAACL, pages 169?176, Boston, MA.
ACL.C.
Monz.
2011.
Statistical machine translation with lo-cal language models.
In Proc.
of the Conference onEmpirical Methods in Natural Language Process-ing, pages 869?879, Edniburgh, Scotland.
ACL.NIST.
2002.
Automatic Evaluation of Machine Trans-lation Quality Using N-gram Co-Occurrence Statis-tics.
Technical report.E.
W. Noreen.
1989.
Computer intensive methods fortesting hypotheses.
Wiley, New York City, NY.F.
J. Och.
2003.
Minimum error rate training in statis-tical machine translation.
In Proc.
41st Ann.
Meet-ing of the Assoc.
Computational Linguistics, pages160?167, Sapporo, Japan.
ACL.273K.
Papineni, S. Roukos, T. Ward, and W. J. Zhu.2002.
A method for automatic evaluation of ma-chine translation.
In Proc.
40th Ann.
Meeting of theAssoc.
Computational Linguistics, pages 311?318,Philadelphia, PA. ACL.S.
Riezler and J. T. Maxwell.
2005.
On some pitfallsin automatic evaluation and significance testing formt.
In Proc.
of the ACL Workshop on Intrinsic andExtrinsic Evaluation Measures for Machine Trans-lation and/or Summarization, pages 57?64, Ann Ar-bor, MI.
ACL.S.
Riezler and J. T. Maxwell.
2006.
Grammaticalmachine translation.
In Proc.
of the Main Confer-ence on Human Language Technology Conference ofthe North American Chapter of the Assoc.
Computa-tional Linguistics, pages 248?255, New York City,NY.
ACL.M.
Smucker, J. Allan, and B. Carterette.
2007.
A com-parison of statistical significance tests for informa-tion retrieval evaluation.
In Proc.
of the SixteenthACM Conference on Information and KnowledgeManagement (CIKM 2007), pages 623?632, Lisbon,Portugal.
ACM.M.
Snover, B. Dorr, R. Scwartz, J. Makhoul, andL.
Micciula.
2006.
A study of translation error ratewith targeted human annotation.
In Proc.
7th Bien-nial Conf.
of the Assoc.
Machine Translaiton in theAmericas, pages 223?231, Boston, MA.
ACL.274
