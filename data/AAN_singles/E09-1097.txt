Proceedings of the 12th Conference of the European Chapter of the ACL, pages 852?860,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsImproving Grammaticality in Statistical Sentence Generation:Introducing a Dependency Spanning Tree Algorithm with an ArgumentSatisfaction ModelStephen Wan??
Mark Dras?
Robert Dale?
?Centre for Language TechnologyDepartment of ComputingMacquarie UniversitySydney, NSW 2113swan,madras,rdale@ics.mq.edu.auCe?cile Paris?
?ICT CentreCSIROSydney, AustraliaCecile.Paris@csiro.auAbstractAbstract-like text summarisation requiresa means of producing novel summary sen-tences.
In order to improve the grammati-cality of the generated sentence, we modela global (sentence) level syntactic struc-ture.
We couch statistical sentence genera-tion as a spanning tree problem in order tosearch for the best dependency tree span-ning a set of chosen words.
We also intro-duce a new search algorithm for this taskthat models argument satisfaction to im-prove the linguistic validity of the gener-ated tree.
We treat the allocation of modi-fiers to heads as a weighted bipartite graphmatching (or assignment) problem, a wellstudied problem in graph theory.
UsingBLEU to measure performance on a stringregeneration task, we found an improve-ment, illustrating the benefit of the span-ning tree approach armed with an argu-ment satisfaction model.1 IntroductionResearch in statistical novel sentence generationhas the potential to extend the current capabili-ties of automatic text summarisation technology,moving from sentence extraction to abstract-likesummarisation.
In this paper, we describe a newalgorithm that improves upon the grammaticalityof statistically generated sentences, evaluated on astring regeneration task, which was first proposedas a surrogate for a grammaticality test by Ban-galore et al (2000).
In this task, a system mustregenerate the original sentence which has had itsword order scrambled.As an evaluation task, string regeneration re-flects the issues that challenge the sentence gen-eration components of machine translation, para-phrase generation, and summarisation systems(Soricut and Marcu, 2005).
Our research in sum-marisation utilises the statistical generation algo-rithms described in this paper to generate novelsummary sentences.The goal of the string regeneration task is to re-cover a sentence once its words have been ran-domly ordered.
Similarly, for a text-to-text gen-eration scenario, the goal is to generate a sen-tence given an unordered list of words, typicallyusing an n-gram language model to select the bestword ordering.
N-gram language models appearto do well at a local level when examining wordsequences smaller than n. However, beyond thiswindow size, the sequence is often ungrammati-cal.
This is not surprising as these methods are un-able to model grammaticality at the sentence level,unless the size of n is sufficiently large.
In prac-tice, the lack of sufficient training data means thatn is often smaller than the average sentence length.Even if data exists, increasing the size of n corre-sponds to a higher degree polynomial complexitysearch for the best word sequence.In response, we introduce an algorithm forsearching for the best word sequence in a waythat attempts to model grammaticality at the sen-tence level.
Mirroring the use of spanning tree al-gorithms in parsing (McDonald et al, 2005), wepresent an approach to statistical sentence genera-tion.
Given a set of scrambled words, the approachsearches for the most probable dependency tree, asdefined by some corpus, such that it contains eachword of the input set.
The tree is then traversed toobtain the final word ordering.In particular, we present two spanning tree al-gorithms.
We first adapt the Chu-Liu-Edmonds(CLE) algorithm (see Chu and Liu (1965) and Ed-monds (1967)), used in McDonald et al (2005),to include a basic argument model, added to keeptrack of linear precedence between heads andmodifiers.
While our adapted version of the CLEalgorithm finds an optimal spanning tree, this does852not always correspond with a linguistically validdependency tree, primarily because it does not at-tempt to ensure that words in the tree have plausi-ble numbers of arguments.We propose an alternative dependency-spanning tree algorithm which uses a morefine-grained argument model representing argu-ment positions.
To find the best modifiers forargument positions, we treat the attachment ofedges to the spanning tree as a weighted bipartitegraph matching problem (or the assignmentproblem), a standard problem in graph theory.The remainder of this paper is as follows.
Sec-tion 2 outlines the graph representation of thespanning tree problem.
We describe a standardspanning tree algorithm in Section 3.
Section 4 de-fines a finer-grained argument model and presentsa new dependency spanning tree search algorithm.We experiment to determine whether a global de-pendency structure, as found by our algorithm,improves performance on the string regenerationproblem, presenting results in Section 5.
Relatedwork is presented in Section 6.
Section 7 con-cludes that an argument model improves the lin-guistic plausibility of the generated trees, thus im-proving grammaticality in text generation.2 A Graph Representation ofDependenciesIn couching statistical generation as a spanningtree problem, this work is the generation analogof the parsing work by McDonald et al (2005).Given a bag of words with no additional con-straints, the aim is to produce a dependency treecontaining the given words.
Informally, as all de-pendency relations between each pair of words arepossible, the set of all possible dependencies canbe represented as a graph, as noted by McDon-ald et al (2005).
Our goal is to find the subset ofthese edges corresponding to a tree with maximumprobability such that each vertex in the graph isvisited once, thus including each word once.
Theresulting tree is a spanning tree, an acyclic graphwhich spans all vertices.
The best tree is the onewith an optimal overall score.
We use negative logprobabilities so that edge weights will correspondto costs.
The overall score is the sum of the costsof the edges in the spanning tree, which we wantto minimise.
Hence, our problem is the minimumspanning tree (MST) problem.We define a directed graph (digraph) in a stan-dard way, G = (V,E) where V is a set of verticesand E ?
{(u, v)|u, v ?
V } is a set of directededges.
For each sentence w = w1 .
.
.
wn, we de-fine the digraph Gw = (Vw, Ew) where Vw ={w0, w1, .
.
.
, wn}, with w0 a dummy root vertex,and Ew = {(u, v)|u ?
Vw, v ?
Vw \ {w0}}.The graph is fully connected (except for the rootvertex w0 which is only fully connected outwards)and is a representation of possible dependencies.For an edge (u, v), we refer to u as the head and vas the modifier.We extend the original formulation of McDon-ald et al (2005) by adding a notion of argumentpositions for a word, providing points to attachmodifiers.
Adopting an approach similar to John-son (2007), we look at the direction (left or right)of the head with respect to the modifier; we con-sequently define a set D = {l, r} to representthis.
Set D represents the linear precedence of thewords in the dependency relation; consequently,it partially approximates the distinction betweensyntactic roles like subject and object.Each edge has a pair of associated weights, onefor each direction, defined by the function s :E?D ?
R, based on a probabilistic model of de-pendency relations.
To calculate the edge weights,we adapt the definition of Collins (1996) to use di-rection rather than relation type (represented in theoriginal as triples of non-terminals).
Given a cor-pus, for some edge e = (u, v) ?
E and directiond ?
D, we calculate the edge weight as:s((u, v), d) = ?log probdep(u, v, d) (1)We define the set of part-of-speech (PoS) tags Pand a function pos : V ?
P , which maps vertices(representing words) to their PoS, to calculate theprobability of a dependency relation, defined as:probdep(u, v, d)= cnt((u, pos(u)), (v, pos(v)), d)co-occurs((u, pos(u)), (v, pos(v))) (2)where cnt((u, pos(u)), (v, pos(v)), d) is the num-ber of times where (v, pos(v)) and (u, pos(u))are seen in a sentence in the training data, and(v, pos(v)) modifies (u, pos(u)) in direction d.The function co-occurs((u, pos(u)), (v, pos(v)))returns the number of times that (v, pos(v)) and(u, pos(u)) are seen in a sentence in the trainingdata.
We adopt the same smoothing strategy asCollins (1996), which backs off to PoS for unseendependency events.8533 Generation via Spanning Trees3.1 The Chu-Liu Edmonds AlgorithmGiven the graph Gw = (Vw, Ew), the Chu-LiuEdmonds (CLE) algorithm finds a rooted directedspanning tree, specified by Tw, which is an acyclicset of edges in Ew minimising?e?Tw,d?D s(e, d).The algorithm is presented as Algorithm 1.1There are two stages to the algorithm.
The firststage finds the best edge for each vertex, connect-ing it to another vertex.
To do so, all outgoingedges of v, that is edges where v is a modifier, areconsidered, and the one with the best edge weightis chosen, where best is defined as the smallestcost.
This minimisation step is used to ensure thateach modifier has only one head.If the chosen edges Tw produce a strongly con-nected subgraph Gmw = (Vw, Tw), then this is theMST.
If not, a cycle amongst some subset of Vwmust be handled in the second stage.
Essentially,one edge in the cycle is removed to produce a sub-tree.
This is done by finding the best edge to joinsome vertex in the cycle to the main tree.
This hasthe effect of finding an alternative head for someword in the cycle.
The edge to the original headis discarded (to maintain one head per modifier),turning the cycle into a subtree.
When all cycleshave been handled, applying a greedy edge selec-tion once more will then yield the MST.3.2 Generating a Word SequenceOnce the tree has been generated, all that remainsis to obtain an ordering of words based upon it.Because dependency relations in the tree are eitherof leftward or rightward direction, it becomes rel-atively trivial to order child vertices with respectto a parent vertex.
The only difficulty lies in find-ing a relative ordering for the leftward (to the par-ent) children, and similarly for the rightward (tothe parent) children.We traverse Gmw using a greedy algorithm to or-der the siblings using an n-gram language model.Algorithm 2 describes the traversal in pseudo-code.
The generated sentence is obtained by call-ing the algorithm with w0 and Tw as parameters.The algorithm operates recursively if called on an1Adapted from (McDonald et al, 2005) andhttp://www.ce.rit.edu/?
sjyeec/dmst.html .
The dif-ference concerns the direction of the edge and the edgeweight function.
We have also folded the function ?contract?in McDonald et al (2005) into the main algorithm.
Againfollowing that work, we treat the function s as a datastructure permitting storage of updated edge weights./* initialisation */Discard the edges exiting the w0 if any.1/* Chu-Liu/Edmonds Algorithm */begin2Tw ?
(u, v) ?
E : ?v?V,d?Darg min(u,v)s((u, v), d)3if Mw = (Vw, Tw) has no cycles then return Mw4forall C ?
Tw : C is a cycle in Mw do5(e, d)?
arg mine?,d?s(e?, d?)
: e ?
C6forall c = (vh, vm, ) ?
C and dc ?
D do7forall e?
= (vi, vm) ?
E and d?
?
D do8s(e?, d?)?
s(e?, d?)?
s(c, dc)?
s(e, d)9end10end11s(e, d)?
s(e, d) + 112end13Tw ?
(u, v) ?
E : ?v?V,d?Darg min(u,v)s((u, v), d)14return Mw15end16Algorithm 1: The pseudo-code for the Chu-LiuEdmonds algorithm with our adaptation to in-clude linear precedence.inner node.
If a vertex v is a leaf in the dependencytree, its string realisation realise(v) is returned.We keep track of ordered siblings with two lists,one for each direction.
If the sibling set is left-wards, the ordered list, Rl, is initialised to be thesingleton set containing a dummy start token withan empty realisation.
If the sibling set is right-wards then the ordered list, Rr is initialised to bethe realisation of the parent.For some sibling set C ?
Vw to be ordered, thealgorithm chooses the next vertex, v ?
C, to insertinto the appropriate ordered list, Rx, x ?
D, bymaximising the probability of the string of wordsthat would result if the realisation, realise(v), wereconcatenated with Rx.The probability of the concatenation is calcu-lated based on a window of words around the join.This window length is defined to be 2?floor(n/2),for some n, in this case, 4.If the siblings are leftwards, the window con-sists of the last min(n ?
1, |Rl|) previously cho-sen words concatenated with the first min(n ?1, |realise(v)|).
If the siblings are rightwards, thewindow consists of the last min(n?1, |realise(v)|)previously chosen words concatenated with thefirst min(n ?
1, |Rr|).
The probability of a win-dow of words, w0 .
.
.
wj , of length j+1 is definedby the following equation:probLMO(w0 .
.
.
wj)=j?k?1?i=0probMLE(wi+k|wi .
.
.
wi+k?1)(3)854/* LMO Algorithm */input : v, Tw where v ?
Vwoutput: R ?
Vwbegin1if isLeaf(v) then2return {realise(v)}3end4else5Cl ?
getLeftChildren(v, Tw)6Cr ?
getRightChildren(v, Tw)7Rl ?
{start}8Rr ?
{realise(v)}9while Cl 6= {} do10c?
arg maxc?Clprobngram(LMO(c, Tw) ?
Rl)11Rl ?
realise(c, Tw) ?
Rl12Cl ?
Cl \ {c}13end14while Cr 6= {} do15c?
arg maxc?Crprobngram(Rr ?
LMO(c, Tw))16Rr ?
Rr ?
realise(c, Tw)17Cr ?
Cr \ {c}18end19return Rl ?
Rr20end21end22Algorithm 2: The Language Model Ordering al-gorithm for linearising an Tw.where k = min(n?
1, j ?
1), and,probMLE(wi+k|wi .
.
.
wi+k?1)= cnt(wi .
.
.
wi+k)cnt(wi .
.
.
wi+k?1)(4)where probMLE(wi+k|wi .
.
.
wi+k?1) is the max-imum likelihood estimate n-gram probability.
Werefer to this tree linearisation method as the Lan-guage Model Ordering (LMO).4 Using an Argument Satisfaction Model4.1 Assigning Words to Argument PositionsOne limitation of using the CLE algorithm forgeneration is that the resulting tree, though max-imal in probability, may not conform to basic lin-guistic properties of a dependency tree.
In partic-ular, it may not have the correct number of argu-ments for each head word.
That is, a word mayhave too few or too many modifiers.To address this problem, we can take into ac-count the argument position when assigning aweight to an edge.
When attaching an edge con-necting a modifier to a head to the spanning tree,we count how many modifiers the head alreadyhas.
An edge is penalised if it is improbable thatthe head takes on yet another modifier, say in theexample of an attachment to a preposition whoseargument position has already been filled.However, accounting for argument positionsmakes an edge weight dynamic and dependent onsurrounding tree context.
This makes the searchfor an optimal tree an NP-hard problem (McDon-ald and Satta, 2007) as all possible trees must beconsidered to find an optimal solution.Consequently, we must choose a heuristicsearch algorithm for finding the locally optimumspanning tree.
By representing argument positionsthat can be filled only once, we allow modifiersto compete for argument positions and vice versa.The CLE algorithm only considers this competi-tion in one direction.
In line 3 of Algorithm 1,only heads compete for modifiers, and thus the so-lution will be sub-optimal.
In Wan et al (2007),we showed that introducing a model of argumentpositions into a greedy spanning tree algorithmhad little effect on performance.
Thus, to considerboth directions of competition, we design a newalgorithm for constructing (dependency) spanningtrees that casts edge selection as a weighted bipar-tite graph matching (or assignment) problem.This problem is to find a weighted alignmentsbetween objects of two distinct sets, where an ob-ject from one set is uniquely aligned to some ob-ject in the other set.
The optimal alignment is onewhere the sum of alignment costs is minimal.
Thegraph of all possible assignments is a weighted bi-partite graph.
Here, to discuss bipartite graphs, wewill extend our notation in a fairly standard way,to write Gp = (U, V,Ep), where U, V are the dis-joint sets of vertices and Ep the set of edges.In our paper, we treat the assignment betweenattachment positions and words as an assignmentproblem.
The standard polynomial-time solutionto the assignment problem is the Kuhn-Munkres(or Hungarian) algorithm (Kuhn, 1955).24.2 A Dependency-Spanning Tree AlgorithmOur alternative dependency-spanning tree algo-rithm, presented as Algorithm 3, incrementallyadds vertices to a growing spanning tree.
Ateach iteration, the Kuhn-Munkres method assignswords that are as yet unattached to argument posi-tions already available in the tree.
We focus on thebipartite graph in Section 4.3.Let the sentence w have the dependency graphGw = (Vw, Ew).
At some arbitrary iteration of thealgorithm (see Figure 1), we have the following:?
Tw ?
Ew, the set of edges in the spanningtree constructed so far;2GPL code: http://sites.google.com/site/garybaker/hungarian-algorithm/assignment855Partially determined spanning tree:w0madejohn?
l0?
r1 cupsof?
l0?
l1for?
l0?
l3johnl0 mader1 ofl0 cupsl1 forl0 madel3Hw1 Hw2 Hw3 Hw4 Hw5 Hw6Mw1 Mw2 Mw3 Mw4 Mw5 Mw6coffee everyone yesterday ?1 ?2 ?3Figure 1: A snapshot of the generation process.Each word in the tree has argument positions towhich we can assign remaining words.
PaddingMw with ?
is described in Section 4.3.?
Hw = {u, v | (u, v) ?
Tw}, the set of ver-tices in Tw, or ?attached vertices?, and there-fore potential heads; and?
Mw = Vw\Hw, the set of ?unattached ver-tices?, and therefore potential modifiers.For the potential heads, we want to define the setof possible attachment positions available in thespanning tree where the potential modifiers can at-tach.
To talk about these attachment positions, wedefine the set of labels L = {(d, j)|d ?
D, j ?N}, an element (d, j) representing an attachmentpoint in direction d, position j.
Valid attachmentpositions must be in sequential order and not miss-ing any intermediate positions (e.g.
if position 2on the right is specified, position 1 must be also):so we define for some i ?
N, 0 ?
i < N , a setAi ?
L such that if the label (d, j) ?
Ai then thelabel (d, k) ?
Ai for 0 ?
k < j.
Collecting these,we define A = {Ai | 0 ?
i < N}.To map a potential head onto the set of attach-ment positions, we define a function q : Hw ?
A.So, given some v ?
Hw, q(v) = Ai for some0 ?
i < N .
In talking about an individual attach-ment point (d, j) ?
q(v) for potential head v, we/* initialisation */Hw ?
{w0}1Mw ?
V ?2Uw ?
{w0R1}3U ?w ?
{}4Tw ?
{}5/* The Assignment-based Algorithm */begin6while Mw 6= {} and U ?w 6= Uw do7U ?w ?
Uw8foreach ?u, (d, j)), v?
?
Kuhn-Munkres(Gpw =9(Uw,M?w, Epw)) doTw ?
Tw ?
{(u, v)}10if u ?
Hw then11Uw ?
Uw \ {u}12end13Uw ?
Uw ?
next(q(u))14Uw ?
Uw ?
next(q(m))15q(m)?
q(m) \ next(q(m))16q(h)?
q(h) \ next(q(h))17Mw ?Mw \ {m}18Hw ?
Hw ?
{m}19end20end21end22Algorithm 3: The Assignment-based Depen-dency Tree Building algorithm.use the notation vdj .
For example, when referringto the second argument position on the right withrespect to v, we use vr2.For the implementation of the algorithm, wehave defined q, to specify attachment points, asfollows, given some v ?
Hw:q(v) =???????
{vr1} if v = w0, the root{vl1} if pos(v) is a prepositionL if pos(v) is a verb{vlj |j ?
N} otherwiseDefining q allows one to optionally incorporatelinguistic information if desired.We define the function next : q(v) ?
A, v ?Hw that returns the position (d, j) with the small-est value of j for direction d. Finally, we write theset of available attachment positions in the span-ning tree as U ?
{(v, l) | v ?
Hw, l ?
q(v)}.4.3 Finding an AssignmentTo construct the bipartite graph used for the as-signment problem at line 9 of Algorithm 3, givenour original dependency graph Gw = (Vw, Ew),and the variables defined from it above in Sec-tion 4.2, we do the following.
The first set ofvertices, of possible heads and their attachmentpoints, is the set Uw.
The second set of ver-tices is the set of possible modifiers augmentedby dummy vertices ?i (indicating no modifica-tion) such that this set is at least as large as Uw :M ?w = Mw?
{?0, .
.
.
, ?max(0,|Uw|?|Mw|)}.
The bi-856partite graph is then Gpw = (Uw,M ?w, Epw), whereEpw = {(u, v) |u ?
Uw, v ?
M ?w}.The weights on the edges of this graph incor-porate a model of argument counts.
The weightfunction is of the form sap : Ep ?
R. Weconsider some e ?
Epw: e = (v?, v) for somev?
?
Uw, v ?
M ?w; and v?
= (u, (d, j)) for someu ?
Vw, d ?
D, j ?
N. s(u,M ?w) is defined to re-turn the maximum cost so that the dummy leavesare only attached as a last resort.
We then define:sap(e)= ?log(probdep(u, v, d) ?
probarg(u, d, j))(5)where probdep(u, v, d) is as in equation 2, usingthe original dependency graph defined in Section2; and probarg(u, d, j), an estimate of the prob-ability that a word u with i arguments assignedalready can take on more arguments, is defined as:probarg(u, d, j)=?
?i=j+1 cntarg(u, d, i)cnt(u, d) (6)where cntarg(u, d, i) is the number of times wordu has been seen with i arguments in directiond; and cnt(u, d) = ?i?N cntarg(u, d, i).
As theprobability of argument positions beyond a certainvalue for i in a given direction will be extremelysmall, we approximate this sum by calculating theprobability density up to a fixed maximum, in thiscase 7 argument positions, and assume zero prob-ability beyond that.5 Evaluation5.1 String Generation TaskThe best-performing word ordering algorithm isone that makes fewest grammatical errors.
As asurrogate measurement of grammaticality, we usethe string regeneration task.
Beginning with ahuman-authored sentence with its word order ran-domised, the goal is to regenerate the original sen-tence.
Success is indicated by the proportion of theoriginal sentence regenerated, as measured by anystring comparison method: in our case, using theBLEU metric (Papineni et al, 2002).
One benefitto this evaluation is that content selection, as a fac-tor, is held constant.
Specifically, the probabilityof word selection is uniform for all words.The string comparison task and its associatedmetrics like BLEU are not perfect.3 The evalu-ation can be seen as being overly strict.
It as-sumes that the only grammatical order is that of theoriginal human authored sentence, referred to asthe ?gold standard?
sentence.
Should an approachchance upon an alternative grammatical ordering,it would penalised.
However, all algorithms andbaselines compared would suffer equally in thisrespect, and so this will be less problematic whenaveraging across multiple test cases.5.2 Data Sets and Training ProceduresThe Penn Treebank corpus (PTB) was used to pro-vide a model of dependency relations and argu-ment counts.
It contains about 3 million wordsof text from the Wall Street Journal (WSJ) withhuman annotations of syntactic structures.
Depen-dency events were sourced from the events file ofthe Collins parser package, which contains the de-pendency events found in training sections 2-22 ofthe corpus.
Development was done on section 00and testing was performed on section 23.A 4-gram language model (LM) was also ob-tained from the PTB training data, referred to asPTB-LM.
Additionally, a 4-gram language modelwas obtained from a subsection of the BLLIP?99Corpus (LDC number: LDC2000T43) containingthree years of WSJ data from 1987 to 1989 (Char-niak et al, 1999).
As in Collins et al (2004),the 1987 portion of the BLLIP corpus containing20 million words was also used to create a lan-guage model, referred to here as BLLIP-LM.
N-gram models were smoothed using Katz?s method,backing off to smaller values of n.For this evaluation, tokenisation was based onthat provided by the PTB data set.
This dataset alo delimits base noun phrases (noun phraseswithout nested constituents).
Base noun phraseswere treated as single tokens, and the rightmostword assumed to be the head.
For the algorithmstested, the input set for any test case consisted ofthe single tokens identified by the PTB tokenisa-tion.
Additionally, the heads of base noun phraseswere included in this input set.
That is, we do notregenerate the base noun phrases.43Alternative grammaticality measures have been devel-oped recently (Mutton et al, 2007).
We are currently explor-ing the use of this and other metrics.4This would correspond to the use of a chunking algo-rithm or a named-entity recogniser to find noun phrases thatcould be re-used for sentence generation.857Algorithms PTB-LM BLLIP-LMViterbi baseline 14.9 18.0LMO baseline 24.3 26.0CLE 26.4 26.8AB 33.6 33.7Figure 2: String regeneration as measured inBLEU points (maximum 100)5.3 Algorithms and BaselinesWe compare the baselines against the Chu-LiuEdmonds (CLE) algorithm to see if spanningtree algorithms do indeed improve upon conven-tional language modelling.
We also comparethe Assignment-based (AB) algorithm against thebaselines and CLE to see if, additionally, mod-elling argument assignments improves the re-sulting tree and thus the generated word se-quence.
Two baseline generators based on n-gram language-models were used, representingapproaches that optimise word sequences based onthe local context of the n-grams.The first baseline re-uses the LMO greedy se-quence algorithm on the same set of input wordspresented to the CLE and AB algorithms.
We ap-ply LMO in a rightward manner beginning witha start-of-sentence token.
Note that this baselinegenerator, like the two spanning tree algorithms,will score favourably using BLEU since, mini-mally, the word order of the base noun phrases willbe correct when each is reinserted.Since the LMO baseline reduces to bigram gen-eration when concatenating single words, we testa second language model baseline which alwaysuses a 4-gram window size.
A Viterbi-like gen-erator with a 4-gram model and a beam of 100 isused to generate a sequence.
For this baseline, re-ferred to as the Viterbi baseline, base noun phraseswere separated into their constituent words and in-cluded in the input word set.5.4 ResultsThe results are presented in Table 2.
Significancewas measured using the sign test and the samplingmethod outlined in (Collins et al, 2005).
We willexamine the results in the PTB-LM column first.The gain of 10 BLEU points by the LMO baselineover the Viterbi baseline shows the performanceimprovement that can be gained when reinsertingthe base noun phrases.AB: the dow at this point was down about 35 pointsCLE: was down about this point 35 points the dow atLMO: was this point about at down the down 35 pointsViterbi: the down 35 points at was about this point downOriginal: at this point, the dow was down about 35 pointsFigure 3: Example generated sentences using theBLLIP-LM.The CLE algorithm significantly out-performedthe LMO baseline by 2 BLEU points, from whichwe conclude that incorporating a model for globalsyntactic structure and treating the search for adependency tree as a spanning problem helps fornovel sentence generation.
However, the real im-provement can be seen in the performance of theAB system which significantly out-performs allother methods, beating the CLE algorithm by 7BLEU points, illustrating the benefits of a modelfor argument counts and of couching tree buildingas an iterative set of argument assignments.One might reasonably ask if more n-gram datawould narrow the gap between the tree algorithmsand the baselines, which encode global and lo-cal information respectively.
Examining results inthe BLLIP-LM column, all approaches improvewith the better language model.
Unsurprisingly,the improvements are most evident in the base-lines which rely heavily on the language model.The margin narrows between the CLE algorithmand the LMO baseline.
However, the AB algo-rithm still out-performs all other approaches by7 BLEU points, highlighting the benefit in mod-elling dependency relations.
Even with a languagemodel that is one order of magnitude larger thanthe PTB-LM, the AB still maintains a sizeable leadin performance.
Figure 3 presents sample gener-ated strings.6 Related Work6.1 Statistical Surface RealisersThe work in this paper is similar to research instatistical surface realisation (for example, Langk-ilde and Knight (1998); Bangalore and Rambow(2000); Filippova and Strube (2008)).
These startwith a semantic representation for which a specificrendering, an ordering of words, must be deter-mined, often using language models to govern treetraversal.
The task in this paper is different as it isa text-to-text scenario and does not begin with arepresentation of semantics.858The dependency model and the LMO lineari-sation algorithm are based heavily on word orderstatistics.
As such, the utility of this approach islimited to human languages with minimal use ofinflections, such as English.
Approaches for otherlanguage types, for example German, have beenexplored (Filippova and Strube, 2007).6.2 Text-to-Text GenerationAs a text-to-text approach, our work is more sim-ilar to work on Information Fusion (Barzilay etal., 1999), a sub-problem in multi-document sum-marisation.
In this work, sentences presenting thesame information, for example multiple news arti-cles describing the same event, are merged to forma single summary by aligning repeated words andphrases across sentences.Other text-to-text approaches for generatingnovel sentences also aim to recycle sentence frag-ments where possible, as we do.
Work on phrase-based statistical machine translation has beenapplied to paraphrase generation (Bannard andCallison-Burch, 2005) and multi-sentence align-ment in summarisation (Daume?
III and Marcu,2004).
These approaches typically use n-grammodels to find the best word sequence.The WIDL formalism (Soricut and Marcu,2005) was proposed to efficiently encode con-straints that restricted possible word sequences,for example dependency information.
Thoughsimilar, our work here does not explicitly repre-sent the word lattice.For these text-to-text systems, the order of ele-ments in the generated sentence is heavily basedon the original order of words and phrases in theinput sentences from which lattices are built.
Ourapproach has the benefit of considering all possi-ble orderings of words, corresponding to a widerrange of paraphrases, provided with a suitable de-pendency model is available.6.3 Parsing and Semantic Role LabellingThis paper presents work closely related to parsingwork by McDonald et al (2005) which searchesfor the best parse tree.
Our work can be thought ofas generating projective dependency trees (that is,without crossing dependencies).The key difference between parsing and gener-ation is that, in parsing, the word order is fixed,whereas for generation, this must be determined.In this paper, we search across all possible treestructures whilst searching for the best word or-dering.
As a result, an argument model is neededto identify linguistically plausible spanning trees.We treated the alignment of modifiers to headwords as a bipartite graph matching problem.
Thisis similar to work in semantic role labelling byPado?
and Lapata (2006).
The alignment of an-swers to question types as a semantic role labellingtask using similar methods was explored by Shenand Lapata (2007).Our work is also strongly related to that ofWong and Mooney (2007) which constructs sym-bolic semantic structures via an assignment pro-cess in order to provide surface realisers with in-put.
Our approach differs in that we do not be-gin with a fixed set of semantic labels.
Addition-ally, our end goal is a dependency tree that encodesword precedence order, bypassing the surface re-alisation stage.7 ConclusionsIn this paper, we presented a new use of spanningtree algorithms for generating sentences from aninput set of words, a task common to many text-to-text scenarios.
The algorithm finds the best de-pendency trees in order to ensure that the result-ing string has grammaticality modelled at a global(sentence) level.
Our algorithm incorporates amodel of argument satisfaction which is treated asan assignment problem, using the Kuhn-Munkresassignment algorithm.
We found a significant im-provement using BLEU to measure improvementson the string regeneration task.
We conclude thatour new algorithm based on the assignment prob-lem and an argument model finds trees that are lin-guistically more plausible, thereby improving thegrammaticality of the generated word sequence.ReferencesSrinivas Bangalore and Owen Rambow.
2000.
Ex-ploiting a probabilistic hierarchical model for gen-eration.
In Proceedings of the 18th Conference onComputational Linguistics, Saarbru?cken, Germany.Srinivas Bangalore, Owen Rambow, and Steve Whit-taker.
2000.
Evaluation metrics for generation.In Proceedings of the first international conferenceon Natural language generation, Morristown, NJ,USA.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Pro-ceedings of the 43rd Annual Meeting of the Asso-859ciation for Computational Linguistics, Ann Arbor,Michigan.Regina Barzilay, Kathleen R. McKeown, and MichaelElhadad.
1999.
Information fusion in the contextof multi-document summarization.
In Proceedingsof the 37th conference on Association for Computa-tional Linguistics, Morristown, NJ, USA.Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,John Hale, and Mark Johnson.
1999.
Bllip 1987-89wsj corpus release 1.
Technical report, LinguisticData Consortium.Y.
J. Chu and T. H. Liu.
1965.
On the shortestarborescence of a directed graph.
Science Sinica,v.14:1396?1400.Christopher Collins, Bob Carpenter, and Gerald Penn.2004.
Head-driven parsing for word lattices.
In Pro-ceedings of the 42nd Annual Meeting on Associa-tion for Computational Linguistics, Morristown, NJ,USA.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of the 43rd AnnualMeeting on Association for Computational Linguis-tics, Morristown, NJ, USA.Michael John Collins.
1996.
A new statistical parserbased on bigram lexical dependencies.
In Proceed-ings of the Thirty-Fourth Annual Meeting of the As-sociation for Computational Linguistics, San Fran-cisco.Hal Daume?
III and Daniel Marcu.
2004.
A phrase-based hmm approach to document/abstract align-ment.
In Proceedings of EMNLP 2004, Barcelona,Spain..J. Edmonds.
1967.
Optimum branchings.
J. Researchof the National Bureau of Standards, 71B:233?240.Katja Filippova and Michael Strube.
2007.
Generatingconstituent order in german clauses.
In Proceedingsof the 45th Annual Meeting on Association for Com-putational Linguistics.
Prague, Czech Republic.Katja Filippova and Michael Strube.
2008.
Sentencefusion via dependency graph compression.
In Con-ference on Empirical Methods in Natural LanguageProcessing, Waikiki, Honolulu, Hawaii.Mark Johnson.
2007.
Transforming projective bilex-ical dependency grammars into efficiently-parsablecfgs with unfold-fold.
In Proceedings of the 45thAnnual Meeting on Association for ComputationalLinguistics.
Prague, Czech Republic.H.W.
Kuhn.
1955.
The hungarian method for the as-signment problem.
Naval Research Logistics Quar-terly, 219552:83?97 83?97.Irene Langkilde and Kevin Knight.
1998.
The practi-cal value of N-grams in derivation.
In Proceedingsof the Ninth International Workshop on Natural Lan-guage Generation, New Brunswick, New Jersey.Ryan McDonald and Giorgio Satta.
2007.
On the com-plexity of non-projective data-driven dependencyparsing.
In Proceedings of the Tenth InternationalConference on Parsing Technologies, Prague, CzechRepublic.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedingsof the conference on Human Language Technologyand Empirical Methods in Natural Language Pro-cessing, Morristown, NJ, USA.Andrew Mutton, Mark Dras, Stephen Wan, and RobertDale.
2007.
Gleu: Automatic evaluation ofsentence-level fluency.
In Proceedings of the 45thAnnual Meeting of the Association of ComputationalLinguistics, Prague, Czech Republic.Sebastian Pado?
and Mirella Lapata.
2006.
Optimalconstituent alignment with edge covers for seman-tic projection.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics andthe 44th annual meeting of the Association for Com-putational Linguistics, Morristown, NJ, USA.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics, Philadelphia, July.Dan Shen and Mirella Lapata.
2007.
Using seman-tic roles to improve question answering.
In Pro-ceedings of the 2007 Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning, Prague,Czech Republic.Radu Soricut and Daniel Marcu.
2005.
Towards devel-oping generation algorithms for text-to-text applica-tions.
In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics, AnnArbor, Michigan.Stephen Wan, Robert Dale, Mark Dras, and Ce?cileParis.
2007.
Global revision in summarisation:Generating novel sentences with prim?s algorithm.In Proceedings of 10th Conference of the Pacific As-sociation for Computational Linguistic, Melbourne,Australia.Yuk Wah Wong and Raymond Mooney.
2007.
Genera-tion by inverting a semantic parser that uses statisti-cal machine translation.
In Human Language Tech-nologies 2007: The Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics, Rochester, New York.860
